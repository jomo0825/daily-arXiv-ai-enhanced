<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 65]
- [cs.CL](#cs.CL) [Total: 47]
- [cs.AI](#cs.AI) [Total: 36]
- [cs.LG](#cs.LG) [Total: 83]
- [cs.NE](#cs.NE) [Total: 6]
- [eess.IV](#eess.IV) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [CWNet: Causal Wavelet Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.10689)
*Tongshun Zhang,Pingping Liu,Yubing Lu,Mengen Cai,Zijian Zhang,Zhe Zhang,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: 提出了一种新的低光图像增强方法CWNet，结合因果推理和小波变换，克服了传统方法在语义信息和特征处理上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统低光图像增强方法忽视了实例级语义信息以及特征间的差异。

Method: 设计了一个基于因果推理和小波变换的框架，利用因果嵌入及CLIP语义损失，分离与保持因果因素，并引入小波变换优化频率恢复效果。

Result: CWNet在多个数据集上的性能显著优于现有方法，展现了其在多场景中的强大性能。

Conclusion: 基于因果原理和小波变换的CWNet能够有效提升低光图像增强质量，并验证了其鲁棒性和普适性。

Abstract: Traditional Low-Light Image Enhancement (LLIE) methods primarily focus on
uniform brightness adjustment, often neglecting instance-level semantic
information and the inherent characteristics of different features. To address
these limitations, we propose CWNet (Causal Wavelet Network), a novel
architecture that leverages wavelet transforms for causal reasoning.
Specifically, our approach comprises two key components: 1) Inspired by the
concept of intervention in causality, we adopt a causal reasoning perspective
to reveal the underlying causal relationships in low-light enhancement. From a
global perspective, we employ a metric learning strategy to ensure causal
embeddings adhere to causal principles, separating them from non-causal
confounding factors while focusing on the invariance of causal factors. At the
local level, we introduce an instance-level CLIP semantic loss to precisely
maintain causal factor consistency. 2) Based on our causal analysis, we present
a wavelet transform-based backbone network that effectively optimizes the
recovery of frequency information, ensuring precise enhancement tailored to the
specific attributes of wavelet transforms. Extensive experiments demonstrate
that CWNet significantly outperforms current state-of-the-art methods across
multiple datasets, showcasing its robust performance across diverse scenes.
Code is available at https://github.com/bywlzts/CWNet-Causal-Wavelet-Network.

</details>


### [2] [Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines](https://arxiv.org/abs/2507.10737)
*Jiayuan Chen,Thai-Hoang Pham,Yuanlong Wang,Ping Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的框架，将外部生物知识整合到显微图像分析模型中，以改善新型细胞系的扰动检测。


<details>
  <summary>Details</summary>
Motivation: 现有技术在处理新型细胞系的显著形态和生物学异质性时表现不佳，因此提出改进方法以更好处理这些挑战。

Method: 构建了一个知识图谱，使用来自STRING和Hetionet数据库的蛋白质相互作用数据进行预训练，结合单细胞基础模型的转录组特征，用于学习细胞系特异性和扰动特异性表示。

Result: 实验表明方法在RxRx数据库中表现优异，能有效提升显微图像分析对新型细胞系的泛化能力。

Conclusion: 该方法在医学药物发现尤其是基于表型的药物开发中具有实际应用价值。

Abstract: High-throughput screening techniques, such as microscopy imaging of cellular
responses to genetic and chemical perturbations, play a crucial role in drug
discovery and biomedical research. However, robust perturbation screening for
\textit{de novo} cell lines remains challenging due to the significant
morphological and biological heterogeneity across cell lines. To address this,
we propose a novel framework that integrates external biological knowledge into
existing pretraining strategies to enhance microscopy image profiling models.
Our approach explicitly disentangles perturbation-specific and cell
line-specific representations using external biological information.
Specifically, we construct a knowledge graph leveraging protein interaction
data from STRING and Hetionet databases to guide models toward
perturbation-specific features during pretraining. Additionally, we incorporate
transcriptomic features from single-cell foundation models to capture cell
line-specific representations. By learning these disentangled features, our
method improves the generalization of imaging models to \textit{de novo} cell
lines. We evaluate our framework on the RxRx database through one-shot
fine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from
the RxRx19a dataset. Experimental results demonstrate that our method enhances
microscopy image profiling for \textit{de novo} cell lines, highlighting its
effectiveness in real-world phenotype-based drug discovery applications.

</details>


### [3] [Auditing Facial Emotion Recognition Datasets for Posed Expressions and Racial Bias](https://arxiv.org/abs/2507.10755)
*Rina Khan,Catherine Stinson*

Main category: cs.CV

TL;DR: 研究揭示面部表情识别算法受数据集质量限制，在真实场景中的表现较差，且存在种族和肤色偏见的问题。


<details>
  <summary>Details</summary>
Motivation: 探讨面部表情识别算法在处理自发和伪装表情上的性能差异及其潜在的种族偏见。

Method: 对两个先进的FER数据集进行审查，随机抽样检查图像是否为自发或伪装，并观察样本中个体的肤色，同时测试训练于这些数据集的模型的表情预测性能。

Result: 发现数据集中被标记为“野外（in-the-wild）”但实际为伪装的图像较多；模型对肤色较深或被标记为非白人的个体更倾向于预测为负面情绪。

Conclusion: 数据集中的伪装图像影响了算法的真实性能评估，而种族和肤色偏见可能导致算法在实际应用中产生歧视性后果。

Abstract: Facial expression recognition (FER) algorithms classify facial expressions
into emotions such as happy, sad, or angry. An evaluative challenge facing FER
algorithms is the fall in performance when detecting spontaneous expressions
compared to posed expressions. An ethical (and evaluative) challenge facing FER
algorithms is that they tend to perform poorly for people of some races and
skin colors. These challenges are linked to the data collection practices
employed in the creation of FER datasets. In this study, we audit two
state-of-the-art FER datasets. We take random samples from each dataset and
examine whether images are spontaneous or posed. In doing so, we propose a
methodology for identifying spontaneous or posed images. We discover a
significant number of images that were posed in the datasets purporting to
consist of in-the-wild images. Since performance of FER models vary between
spontaneous and posed images, the performance of models trained on these
datasets will not represent the true performance if such models were to be
deployed in in-the-wild applications. We also observe the skin color of
individuals in the samples, and test three models trained on each of the
datasets to predict facial expressions of people from various races and skin
tones. We find that the FER models audited were more likely to predict people
labeled as not white or determined to have dark skin as showing a negative
emotion such as anger or sadness even when they were smiling. This bias makes
such models prone to perpetuate harm in real life applications.

</details>


### [4] [FPC-Net: Revisiting SuperPoint with Descriptor-Free Keypoint Detection via Feature Pyramids and Consistency-Based Implicit Matching](https://arxiv.org/abs/2507.10770)
*Ionuţ Grigore,Călin-Adrian Popa,Claudiu Leoveanu-Condrei*

Main category: cs.CV

TL;DR: 本研究提出了一种在检测阶段直接关联兴趣点的技术，无需描述符，从而显著减少内存使用。


<details>
  <summary>Details</summary>
Motivation: 减少几何计算机视觉任务中由于依赖描述符带来的内存占用及计算复杂度。

Method: 提出了一种新方法，在兴趣点检测时直接完成关联，无需额外计算、存储或传输描述符。

Result: 尽管匹配准确性稍低，但完全消除了使用描述符的需求，有效降低了定位系统的内存使用。

Conclusion: 方法在匹配准确性与内存优化之间进行了权衡，是描述符替代方案的有力候选。

Abstract: The extraction and matching of interest points are fundamental to many
geometric computer vision tasks. Traditionally, matching is performed by
assigning descriptors to interest points and identifying correspondences based
on descriptor similarity. This work introduces a technique where interest
points are inherently associated during detection, eliminating the need for
computing, storing, transmitting, or matching descriptors. Although the
matching accuracy is marginally lower than that of conventional approaches, our
method completely eliminates the need for descriptors, leading to a drastic
reduction in memory usage for localization systems. We assess its effectiveness
by comparing it against both classical handcrafted methods and modern learned
approaches.

</details>


### [5] [A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Flight Computers](https://arxiv.org/abs/2507.10775)
*Jeffrey Joan Sam,Janhavi Sathe,Nikhil Chigali,Naman Gupta,Radhey Ruparel,Yicheng Jiang,Janmajay Singh,James W. Berck,Arko Barman*

Main category: cs.CV

TL;DR: 论文提出了一套涵盖约64k标注航天器图像的新数据集，并基于此数据集微调了YOLOv8和YOLOv11分割模型以进行性能评估，取得了高效且准确的结果，适合实时太空应用。


<details>
  <summary>Details</summary>
Motivation: 解决现有航天器维修中人力或机器人操作存在的高成本和危险问题，并应对当前缺乏航天器图像分割数据的问题。

Method: 通过使用真实航天器模型和NASA TTALOS产生的背景，生成了一个包含64k标注图像的数据集，同时加入噪声模拟真实条件。然后微调YOLOv8及YOLOv11分割模型进行评估，并在NASA架构下模拟实时分割应用。

Result: 经微调的分割模型在严格硬件和推理时间限制下实现了Dice得分0.92、Hausdorff距离0.69，推理时间约为0.5秒。

Conclusion: 提出的新数据集和模型为太空中实时自动化航天器检测分割提供了高效且可靠的解决方案，有助于降低维修成本和危险性。

Abstract: Spacecraft deployed in outer space are routinely subjected to various forms
of damage due to exposure to hazardous environments. In addition, there are
significant risks to the subsequent process of in-space repairs through human
extravehicular activity or robotic manipulation, incurring substantial
operational costs. Recent developments in image segmentation could enable the
development of reliable and cost-effective autonomous inspection systems. While
these models often require large amounts of training data to achieve
satisfactory results, publicly available annotated spacecraft segmentation data
are very scarce. Here, we present a new dataset of nearly 64k annotated
spacecraft images that was created using real spacecraft models, superimposed
on a mixture of real and synthetic backgrounds generated using NASA's TTALOS
pipeline. To mimic camera distortions and noise in real-world image
acquisition, we also added different types of noise and distortion to the
images. Finally, we finetuned YOLOv8 and YOLOv11 segmentation models to
generate performance benchmarks for the dataset under well-defined hardware and
inference time constraints to mimic real-world image segmentation challenges
for real-time onboard applications in space on NASA's inspector spacecraft. The
resulting models, when tested under these constraints, achieved a Dice score of
0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second.
The dataset and models for performance benchmark are available at
https://github.com/RiceD2KLab/SWiM.

</details>


### [6] [Warehouse Spatial Question Answering with LLM Agent](https://arxiv.org/abs/2507.10778)
*Hsiang-Wei Huang,Jen-Hao Cheng,Kuang-Ming Chen,Cheng-Yen Yang,Bahaa Alattar,Yi-Ru Lin,Pyongkun Kim,Sangwon Kim,Kwangju Kim,Chung-I Huang,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: 本研究提出了一种数据高效的LLM代理系统，用于复杂室内仓库场景的空间推理和问题解答。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在空间理解任务上存在挑战，尤其是复杂室内仓库环境中的问题解答。

Method: 提出了一个多工具集成的LLM代理系统，通过空间推理和API工具交互提高对复杂问题的解答能力。

Result: 在2025 AI City挑战的物理AI空间智能仓库数据集上，该系统在对象检索、计数和距离估算任务中表现出高准确性和效率。

Conclusion: 论文展示了一种高效且强大的空间推理系统，为复杂室内场景中的空间问题提供了新的解决方案。

Abstract: Spatial understanding has been a challenging task for existing Multi-modal
Large Language Models~(MLLMs). Previous methods leverage large-scale MLLM
finetuning to enhance MLLM's spatial understanding ability. In this paper, we
present a data-efficient approach. We propose a LLM agent system with strong
and advanced spatial reasoning ability, which can be used to solve the
challenging spatial question answering task in complex indoor warehouse
scenarios. Our system integrates multiple tools that allow the LLM agent to
conduct spatial reasoning and API tools interaction to answer the given
complicated spatial question. Extensive evaluations on the 2025 AI City
Challenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that
our system achieves high accuracy and efficiency in tasks such as object
retrieval, counting, and distance estimation. The code is available at:
https://github.com/hsiangwei0903/SpatialAgent

</details>


### [7] [ThinkingViT: Matryoshka Thinking Vision Transformer for Elastic Inference](https://arxiv.org/abs/2507.10800)
*Ali Hojjat,Janek Haberer,Soren Pirk,Olaf Landsiedel*

Main category: cs.CV

TL;DR: 论文提出了ThinkingViT，一种动态调整推理计算的Vision Transformer架构，在推理效率与准确率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有嵌套Transformer架构无法根据输入复杂度调整计算量，导致效率低下。

Method: 提出了ThinkingViT，利用渐进思维阶段动态调整计算，结合Token Recycling机制逐步优化推理。

Result: 在ImageNet-1K数据集上，ThinkingViT在相同吞吐量下提高了准确率2%，在相同GMACs下提高了2.9%。

Conclusion: ThinkingViT能够在保持推理高效性的同时提升准确率，是一种适合异构硬件部署的实践方法。

Abstract: Vision Transformers deliver state-of-the-art performance, yet their fixed
computational budget prevents scalable deployment across heterogeneous
hardware. Recent nested Transformer architectures mitigate this by embedding
nested subnetworks within a single model to enable scalable inference. However,
these models allocate the same amount of compute to all inputs, regardless of
their complexity, which leads to inefficiencies. To address this, we introduce
ThinkingViT, a nested ViT architecture that employs progressive thinking stages
to dynamically adjust inference computation based on input difficulty.
ThinkingViT initiates inference by activating a small subset of the most
important attention heads and terminates early if predictions reach sufficient
certainty. Otherwise, it activates additional attention heads and re-evaluates
the input. At the core of ThinkingViT is our Token Recycling mechanism, which
conditions each subsequent inference stage on the embeddings from the previous
stage, enabling progressive improvement. Due to its backbone-preserving design,
ThinkingViT also serves as a plugin upgrade for vanilla ViT. Experiments show
that ThinkingViT surpasses nested baselines by up to 2.0 percentage points
(p.p.) in accuracy at the same throughput and by up to 2.9 p.p. at equal GMACs
on ImageNet-1K. The source code is available at
https://github.com/ds-kiel/ThinkingViT.

</details>


### [8] [LLM-Guided Agentic Object Detection for Open-World Understanding](https://arxiv.org/abs/2507.10844)
*Furkan Mumcu,Michael J. Jones,Anoop Cherian,Yasin Yilmaz*

Main category: cs.CV

TL;DR: 提出了一种基于大语言模型（LLM）的自适应物体检测框架（LAOD），实现了无需标签的零样本检测，通过动态生成物体名称并使用开放词汇探测器进行定位。


<details>
  <summary>Details</summary>
Motivation: 传统物体检测需要预设类别集且训练成本高，现有方法OWOD缺少未知物语义标签，OVOD依赖用户提示，限制自主性。

Method: 利用大语言模型（LLM）生成场景相关的物体名称，并通过开放词汇探测器进行定位，同时提出新的评估指标CAAP和SNAP分别评估物体定位和命名能力。

Result: 实验表明在LVIS、COCO和COCO-OOD数据集上的方法表现优异，能够有效检测和命名新型物体。

Conclusion: 该方法提升了自主性和适应能力，为开放世界中的理解任务提供了一种新的解决方案。

Abstract: Object detection traditionally relies on fixed category sets, requiring
costly re-training to handle novel objects. While Open-World and
Open-Vocabulary Object Detection (OWOD and OVOD) improve flexibility, OWOD
lacks semantic labels for unknowns, and OVOD depends on user prompts, limiting
autonomy. We propose an LLM-guided agentic object detection (LAOD) framework
that enables fully label-free, zero-shot detection by prompting a Large
Language Model (LLM) to generate scene-specific object names. These are passed
to an open-vocabulary detector for localization, allowing the system to adapt
its goals dynamically. We introduce two new metrics, Class-Agnostic Average
Precision (CAAP) and Semantic Naming Average Precision (SNAP), to separately
evaluate localization and naming. Experiments on LVIS, COCO, and COCO-OOD
validate our approach, showing strong performance in detecting and naming novel
objects. Our method offers enhanced autonomy and adaptability for open-world
understanding.

</details>


### [9] [Winsor-CAM: Human-Tunable Visual Explanations from Deep Networks via Layer-Wise Winsorization](https://arxiv.org/abs/2507.10846)
*Casey Wall,Longwei Wang,Rodrigue Rizk,KC Santosh*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Winsor-CAM的新方法，用于生成更清晰和鲁棒的CNN可视化解释。


<details>
  <summary>Details</summary>
Motivation: 现有的Grad-CAM方法存在对最后卷积层过于依赖或简单层平均的问题，这可能忽略重要的语义线索或增加不相关噪声，因此需要改进。

Method: 引入Winsor-CAM，在多个卷积层间聚合信息并应用Winsorization技术以减弱噪点，同时提供用户可控的语义级别阈值调整能力。

Result: 在PASCAL VOC 2012数据集上，与Grad-CAM和均匀层平均基准方法比较，Winsor-CAM生成了更具可解释性的热图，在定位指标上（如交并比和质心对齐度）表现更优越。

Conclusion: Winsor-CAM提供了更加可信赖的人工智能解释工具，支持多层次模型洞察并允许用户进行交互调整。

Abstract: Interpreting the decision-making process of Convolutional Neural Networks
(CNNs) is critical for deploying models in high-stakes domains.
Gradient-weighted Class Activation Mapping (Grad-CAM) is a widely used method
for visual explanations, yet it typically focuses on the final convolutional
layer or na\"ively averages across layers, strategies that can obscure
important semantic cues or amplify irrelevant noise. We propose Winsor-CAM, a
novel, human-tunable extension of Grad-CAM that generates robust and coherent
saliency maps by aggregating information across all convolutional layers. To
mitigate the influence of noisy or extreme attribution values, Winsor-CAM
applies Winsorization, a percentile-based outlier attenuation technique. A
user-controllable threshold allows for semantic-level tuning, enabling flexible
exploration of model behavior across representational hierarchies. Evaluations
on standard architectures (ResNet50, DenseNet121, VGG16, InceptionV3) using the
PASCAL VOC 2012 dataset demonstrate that Winsor-CAM produces more interpretable
heatmaps and achieves superior performance in localization metrics, including
intersection-over-union and center-of-mass alignment, when compared to Grad-CAM
and uniform layer-averaging baselines. Winsor-CAM advances the goal of
trustworthy AI by offering interpretable, multi-layer insights with
human-in-the-loop control.

</details>


### [10] [Sparse Fine-Tuning of Transformers for Generative Tasks](https://arxiv.org/abs/2507.10855)
*Wei Chen,Jingxi Yu,Zichen Miao,Qiang Qiu*

Main category: cs.CV

TL;DR: 本文提出了一种基于稀疏编码的微调框架，通过稀疏组合基础元素（特征字典原子）来表示特征，提高了图像编辑和文本到图像概念定制任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的微调方法中，更新的表示形式是由修改后的参数密集组合而成，难以解释其贡献及模型如何适应新任务，因此需要一种更具解释性的微调框架。

Method: 提出了一种受稀疏编码启发的微调框架，将微调特征表示为特征字典原子的稀疏组合，利用稀疏系数作为原子重要性的指示器，优化特征字典原子以适应下游任务。

Result: 在实验中，该方法通过去除不重要的特征字典原子提升了图像编辑的文本对齐性能，在文本到图像概念定制任务中，利用稀疏组合有效构建目标概念，性能优于多种基线方法。

Conclusion: 该方法证明了稀疏编码微调框架在提高模型可解释性及适应性上的有效性，为加强大模型在下游任务中的表现提供了一种新途径。

Abstract: Large pre-trained transformers have revolutionized artificial intelligence
across various domains, and fine-tuning remains the dominant approach for
adapting these models to downstream tasks due to the cost of training from
scratch. However, in existing fine-tuning methods, the updated representations
are formed as a dense combination of modified parameters, making it challenging
to interpret their contributions and understand how the model adapts to new
tasks. In this work, we introduce a fine-tuning framework inspired by sparse
coding, where fine-tuned features are represented as a sparse combination of
basic elements, i.e., feature dictionary atoms. The feature dictionary atoms
function as fundamental building blocks of the representation, and tuning atoms
allows for seamless adaptation to downstream tasks. Sparse coefficients then
serve as indicators of atom importance, identifying the contribution of each
atom to the updated representation. Leveraging the atom selection capability of
sparse coefficients, we first demonstrate that our method enhances image
editing performance by improving text alignment through the removal of
unimportant feature dictionary atoms. Additionally, we validate the
effectiveness of our approach in the text-to-image concept customization task,
where our method efficiently constructs the target concept using a sparse
combination of feature dictionary atoms, outperforming various baseline
fine-tuning methods.

</details>


### [11] [A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF-Based Preprocessing and YOLO-v11n](https://arxiv.org/abs/2507.10864)
*Saadat Behzadi,Danial Sharifrazi,Bita Mesbahzadeh,Javad Hassannataj Joloudarid,Roohallah Alizadehsani*

Main category: cs.CV

TL;DR: 提出了一种结合LOF算法和轻量级YOLO-v11n模型的新方法，用于检测结直肠息肉，取得了显著的检测性能提升。


<details>
  <summary>Details</summary>
Motivation: 及时准确地检测结直肠息肉对于诊断和预防结直肠癌至关重要，而已有方法在数据质量和实时性上仍存在不足。

Method: 使用LOF算法过滤异常数据，优化5份公开数据集的标注后采用YOLO-v11n模型进行检测，并通过折叠交叉验证和数据增强技术提升泛化性能。

Result: 方法在多项指标上表现优异，包括95.83%的精确率、91.85%的召回率、93.48%的F1值、96.48%的mAP@0.5及77.75%的mAP@0.5:0.95，优于现有方法。

Conclusion: 该方法适用于临床实时结肠镜检测，强调数据预处理和模型高效性的重要性，为医疗影像AI系统设计提供参考。

Abstract: Objectives: Timely and accurate detection of colorectal polyps plays a
crucial role in diagnosing and preventing colorectal cancer, a major cause of
mortality worldwide. This study introduces a new, lightweight, and efficient
framework for polyp detection that combines the Local Outlier Factor (LOF)
algorithm for filtering noisy data with the YOLO-v11n deep learning model.
  Study design: An experimental study leveraging deep learning and outlier
removal techniques across multiple public datasets.
  Methods: The proposed approach was tested on five diverse and publicly
available datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene.
Since these datasets originally lacked bounding box annotations, we converted
their segmentation masks into suitable detection labels. To enhance the
robustness and generalizability of our model, we apply 5-fold cross-validation
and remove anomalous samples using the LOF method configured with 30 neighbors
and a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a
fast and resource-efficient object detection architecture optimized for
real-time applications. We train the model using a combination of modern
augmentation strategies to improve detection accuracy under diverse conditions.
  Results: Our approach significantly improves polyp localization performance,
achieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5
of 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods,
our model demonstrates enhanced accuracy and efficiency.
  Conclusions: These results suggest that the proposed method is well-suited
for real-time colonoscopy support in clinical settings. Overall, the study
underscores how crucial data preprocessing and model efficiency are when
designing effective AI systems for medical imaging.

</details>


### [12] [Trexplorer Super: Topologically Correct Centerline Tree Tracking of Tubular Objects in CT Volumes](https://arxiv.org/abs/2507.10881)
*Roman Naeem,David Hagerman,Jennifer Alvén,Lennart Svensson,Fredrik Kahl*

Main category: cs.CV

TL;DR: Trexplorer Super通过创新技术改进了中心线追踪性能，并开发了三个评估数据集，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有的中心线追踪模型Trexplorer存在预测重复分支和过早终止的问题，需要改进。

Method: 引入了改进后的模型Trexplorer Super，并开发了一个合成和两个真实的中心线数据集，用于综合评估模型性能。

Result: Trexplorer Super在每个数据集上的表现优于现有的SOTA模型，并显示合成数据上的强表现未必适用于真实数据。

Conclusion: Trexplorer Super有效提升了中心线追踪的性能并验证了在真实数据上的优势，代码和数据集已公开。

Abstract: Tubular tree structures, such as blood vessels and airways, are essential in
human anatomy and accurately tracking them while preserving their topology is
crucial for various downstream tasks. Trexplorer is a recurrent model designed
for centerline tracking in 3D medical images but it struggles with predicting
duplicate branches and terminating tracking prematurely. To address these
issues, we present Trexplorer Super, an enhanced version that notably improves
performance through novel advancements. However, evaluating centerline tracking
models is challenging due to the lack of public datasets. To enable thorough
evaluation, we develop three centerline datasets, one synthetic and two real,
each with increasing difficulty. Using these datasets, we conduct a
comprehensive evaluation of existing state-of-the-art (SOTA) models and compare
them with our approach. Trexplorer Super outperforms previous SOTA models on
every dataset. Our results also highlight that strong performance on synthetic
data does not necessarily translate to real datasets. The code and datasets are
available at https://github.com/RomStriker/Trexplorer-Super.

</details>


### [13] [Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency](https://arxiv.org/abs/2507.10893)
*Minjong Cheon,Eunhan Goo,Su-Hyeon Shin,Muhammad Ahmed,Hyungjun Kim*

Main category: cs.CV

TL;DR: 提出了一种现代化的基于CNN的全球天气预测模型，具备较小的训练复杂度和高效的资源利用，性能与先进模型相当。


<details>
  <summary>Details</summary>
Motivation: 目前基于Transformer的AI天气模型虽然精度高，但训练复杂度和资源需求较高，因此需要更高效的替代方案。

Method: 研究引入一种基于现代化CNN架构的模型，包含比例不变的结构和InceptionNeXt模块，以适应地球系统数据，同时通过 ERA5 数据集进行训练和评估。

Result: KAI-a模型在一个NVIDIA L40s GPU上仅需12小时即可完成训练，且仅有约700万参数。在中期天气预测中匹配目前最先进模型的表现，并能够有效预测极端天气事件。

Conclusion: KAI-a模型不仅具备竞争力的天气预测精度，还提升了计算效率，展示出在实际天气预测中的应用潜力。

Abstract: Recently, AI-based weather forecast models have achieved impressive advances.
These models have reached accuracy levels comparable to traditional NWP
systems, marking a significant milestone in data-driven weather prediction.
However, they mostly leverage Transformer-based architectures, which often
leads to high training complexity and resource demands due to the massive
parameter sizes. In this study, we introduce a modernized CNN-based model for
global weather forecasting that delivers competitive accuracy while
significantly reducing computational requirements. To present a systematic
modernization roadmap, we highlight key architectural enhancements across
multiple design scales from an earlier CNN-based approach. KAI-a incorporates a
scale-invariant architecture and InceptionNeXt-based blocks within a
geophysically-aware design, tailored to the structure of Earth system data.
Trained on the ERA5 daily dataset with 67 atmospheric variables, the model
contains about 7 million parameters and completes training in just 12 hours on
a single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the
performance of state-of-the-art models in medium-range weather forecasting,
while offering a significantly lightweight design. Furthermore, case studies on
the 2018 European heatwave and the East Asian summer monsoon demonstrate
KAI-a's robust skill in capturing extreme events, reinforcing its practical
utility.

</details>


### [14] [Commuting Distance Regularization for Timescale-Dependent Label Inconsistency in EEG Emotion Recognition](https://arxiv.org/abs/2507.10895)
*Xiaocong Zeng,Craig Michoski,Yan Pang,Dongyang Kuang*

Main category: cs.CV

TL;DR: 研究关注EEG情感识别任务中常被忽视的时间尺度依赖标签不一致性问题，提出新的正则化策略并通过实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决EEG情感识别中的时间尺度依赖标签不一致性问题，提高模型泛化能力与可解释性。

Method: 提出两种正则化策略：局部变化损失（LVL）和局部-全局一致性损失（LGCL），基于图论框架结合经典数学概念。此外，提出新的评价指标套件来评估时序预测和全局情感标签的一致性。

Result: 在DREAMER和DEAP数据集上测试，采用LSTM和Transformer等网络模型，以五种指标验证方法有效性，结果显示新方法在准确性和一致性上优于现有先进方法。

Conclusion: LVL在所有模型和指标中表现优异，LGCL表现次之，证明所提方法在标签不一致问题下提供了可解释性和预测能力的良好平衡。

Abstract: In this work, we address the often-overlooked issue of Timescale Dependent
Label Inconsistency (TsDLI) in training neural network models for EEG-based
human emotion recognition. To mitigate TsDLI and enhance model generalization
and explainability, we propose two novel regularization strategies: Local
Variation Loss (LVL) and Local-Global Consistency Loss (LGCL). Both methods
incorporate classical mathematical principles--specifically, functions of
bounded variation and commute-time distances--within a graph theoretic
framework. Complementing our regularizers, we introduce a suite of new
evaluation metrics that better capture the alignment between temporally local
predictions and their associated global emotion labels. We validate our
approach through comprehensive experiments on two widely used EEG emotion
datasets, DREAMER and DEAP, across a range of neural architectures including
LSTM and transformer-based models. Performance is assessed using five distinct
metrics encompassing both quantitative accuracy and qualitative consistency.
Results consistently show that our proposed methods outperform state-of-the-art
baselines, delivering superior aggregate performance and offering a principled
trade-off between interpretability and predictive power under label
inconsistency. Notably, LVL achieves the best aggregate rank across all
benchmarked backbones and metrics, while LGCL frequently ranks the second,
highlighting the effectiveness of our framework.

</details>


### [15] [GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization](https://arxiv.org/abs/2507.10935)
*Shaowen Tong,Zimin Xia,Alexandre Alahi,Xuming He,Yujiao Shi*

Main category: cs.CV

TL;DR: 提出了一种名为GeoDistill的几何引导弱监督自蒸馏框架，通过教师-学生学习和视场（FoV）遮罩增强局部特征学习，实现了鲁棒的跨视角定位。


<details>
  <summary>Details</summary>
Motivation: 现有跨视角定位方法依赖完全监督学习，需要昂贵的真实姿态标注，限制了其大规模应用的潜力。该研究旨在提出一种更高效的弱监督方法。

Method: 通过几何引导的弱监督自蒸馏框架GeoDistill，利用教师模型定位全景图像，学生模型从基于FoV遮罩生成的有限FoV图像预测结果，并对齐两者的预测，提升局部特征学习的鲁棒性。此外，提出了一个无需精确平面位置真值的相对方位估计网络。

Result: 实验表明，GeoDistill在不同框架中显著提高了定位性能，同时减少了不确定性。

Conclusion: GeoDistill提供了一种可扩展且高效的解决方案，有助于解决真实世界中的跨视角定位挑战。

Abstract: Cross-view localization, the task of estimating a camera's
3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with
satellite images, is crucial for large-scale outdoor applications like
autonomous navigation and augmented reality. Existing methods often rely on
fully supervised learning, which requires costly ground-truth pose annotations.
In this work, we propose GeoDistill, a Geometry guided weakly supervised self
distillation framework that uses teacher-student learning with Field-of-View
(FoV)-based masking to enhance local feature learning for robust cross-view
localization. In GeoDistill, the teacher model localizes a panoramic image,
while the student model predicts locations from a limited FoV counterpart
created by FoV-based masking. By aligning the student's predictions with those
of the teacher, the student focuses on key features like lane lines and ignores
textureless regions, such as roads. This results in more accurate predictions
and reduced uncertainty, regardless of whether the query images are panoramas
or limited FoV images. Our experiments show that GeoDistill significantly
improves localization performance across different frameworks. Additionally, we
introduce a novel orientation estimation network that predicts relative
orientation without requiring precise planar position ground truth. GeoDistill
provides a scalable and efficient solution for real-world cross-view
localization challenges. Code and model can be found at
https://github.com/tongshw/GeoDistill.

</details>


### [16] [Graph Aggregation Prototype Learning for Semantic Change Detection in Remote Sensing](https://arxiv.org/abs/2507.10938)
*Zhengyi Xu,Haoran Wu,Wen Jiang,Jie Geng*

Main category: cs.CV

TL;DR: 提出了一种用于遥感语义变化检测的图聚合原型学习方法（GAPL-SCD），通过多任务优化和冲突调整，显著提升了语义分割与变化检测任务的精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决遥感语义变化检测任务中多任务学习易导致的负迁移问题，提升多时态数据变化检测的精度和细粒度语义信息提取能力。

Method: 提出GAPL-SCD框架，设计了多任务联合优化方法，利用自适应权重分配和梯度旋转缓解任务冲突，并通过图聚合原型学习模块实现类别级域对齐和干扰削弱，同时引入多尺度特征交互与融合模块提升复杂场景的表现能力。

Result: 在SECOND和Landsat-SCD数据集上的实验结果表明，其在语义变化检测任务中达到了最新的性能水平，显著提高了准确性和鲁棒性。

Conclusion: GAPL-SCD框架通过引入图聚合原型学习和任务优化策略，有效缓解了多任务学习的冲突问题，为遥感语义变化检测提供了更强大的工具。

Abstract: Semantic change detection (SCD) extends the binary change detection task to
provide not only the change locations but also the detailed "from-to"
categories in multi-temporal remote sensing data. Such detailed semantic
insights into changes offer considerable advantages for a wide array of
applications. However, since SCD involves the simultaneous optimization of
multiple tasks, the model is prone to negative transfer due to task-specific
learning difficulties and conflicting gradient flows. To address this issue, we
propose Graph Aggregation Prototype Learning for Semantic Change Detection in
remote sensing(GAPL-SCD). In this framework, a multi-task joint optimization
method is designed to optimize the primary task of semantic segmentation and
change detection, along with the auxiliary task of graph aggregation prototype
learning. Adaptive weight allocation and gradient rotation methods are used to
alleviate the conflict between training tasks and improve multi-task learning
capabilities. Specifically, the graph aggregation prototype learning module
constructs an interaction graph using high-level features. Prototypes serve as
class proxies, enabling category-level domain alignment across time points and
reducing interference from irrelevant changes. Additionally, the proposed
self-query multi-level feature interaction and bi-temporal feature fusion
modules further enhance multi-scale feature representation, improving
performance in complex scenes. Experimental results on the SECOND and
Landsat-SCD datasets demonstrate that our method achieves state-of-the-art
performance, with significant improvements in accuracy and robustness for SCD
task.

</details>


### [17] [Robust ID-Specific Face Restoration via Alignment Learning](https://arxiv.org/abs/2507.10943)
*Yushun Fang,Lu Liu,Xiang Gao,Qiang Hu,Ning Cao,Jianghe Cui,Gang Chen,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的ID特定面部修复框架RIDFR，强调通过内容注入和身份注入模块以及对齐学习实现高质量修复。


<details>
  <summary>Details</summary>
Motivation: 面对不明确身份输入和生成过程随机性的挑战，亟需提高在面部修复时的身份保真度与鲁棒性。

Method: 结合扩散模型，提出内容注入模块处理退化图像，身份注入模块引入特定身份信息，并通过对齐学习减少ID无关语义干扰。

Result: 实验表明比现有方法更出色，RIDFR能高质量地恢复具有高身份保真度的人脸结果，并表现出强大的鲁棒性。

Conclusion: RIDFR成功解决了存在随机性和身份不确定问题，实现了高质量、ID特定的面部修复。

Abstract: The latest developments in Face Restoration have yielded significant
advancements in visual quality through the utilization of diverse diffusion
priors. Nevertheless, the uncertainty of face identity introduced by
identity-obscure inputs and stochastic generative processes remains unresolved.
To address this challenge, we present Robust ID-Specific Face Restoration
(RIDFR), a novel ID-specific face restoration framework based on diffusion
models. Specifically, RIDFR leverages a pre-trained diffusion model in
conjunction with two parallel conditioning modules. The Content Injection
Module inputs the severely degraded image, while the Identity Injection Module
integrates the specific identity from a given image. Subsequently, RIDFR
incorporates Alignment Learning, which aligns the restoration results from
multiple references with the same identity in order to suppress the
interference of ID-irrelevant face semantics (e.g. pose, expression, make-up,
hair style). Experiments demonstrate that our framework outperforms the
state-of-the-art methods, reconstructing high-quality ID-specific results with
high identity fidelity and demonstrating strong robustness.

</details>


### [18] [Women Sport Actions Dataset for Visual Classification Using Small Scale Training Data](https://arxiv.org/abs/2507.10969)
*Palash Ray,Mahuya Sasmal,Asish Bera*

Main category: cs.CV

TL;DR: 本文提出了一个名为WomenSports的新数据集，并结合深度学习方法用于女性运动动作分类，取得了显著的分类性能。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏针对女性运动动作的图像数据集，且需解决类内和类间变化不足的问题，从而推动运动分析领域的发展。

Method: 引入WomenSports数据集，结合卷积神经网络（CNN）进行深度特征提取，并通过通道注意力机制增强局部上下文的特征表示能力。

Result: 在WomenSports数据集上，使用ResNet-50模型实现了89.15%的Top-1分类准确率，在其他多个数据集上的表现也较为出色。

Conclusion: 本文基于新建数据集与改进的深度学习方法，有效推动了女性运动动作分类领域的研究与应用。

Abstract: Sports action classification representing complex body postures and
player-object interactions is an emerging area in image-based sports analysis.
Some works have contributed to automated sports action recognition using
machine learning techniques over the past decades. However, sufficient image
datasets representing women sports actions with enough intra- and inter-class
variations are not available to the researchers. To overcome this limitation,
this work presents a new dataset named WomenSports for women sports
classification using small-scale training data. This dataset includes a variety
of sports activities, covering wide variations in movements, environments, and
interactions among players. In addition, this study proposes a convolutional
neural network (CNN) for deep feature extraction. A channel attention scheme
upon local contextual regions is applied to refine and enhance feature
representation. The experiments are carried out on three different sports
datasets and one dance dataset for generalizing the proposed algorithm, and the
performances on these datasets are noteworthy. The deep learning method
achieves 89.15% top-1 classification accuracy using ResNet-50 on the proposed
WomenSports dataset, which is publicly available for research at Mendeley Data.

</details>


### [19] [Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection](https://arxiv.org/abs/2507.10977)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: 提出了一种新的波浪注意力骨干网络和基于射线的编码器，用于高效和准确的人物与物体交互检测。


<details>
  <summary>Details</summary>
Motivation: 解决现有HOI检测器在效率和可靠性方面的不足，例如资源密集的训练方法和低效的架构。

Method: 设计了一种波浪注意力骨干网络，能够通过多样化的卷积过滤器表达中间次序的交互；同时提出了基于射线的编码器，通过关注相关区域和减少计算开销来提升多尺度注意力。

Result: 在ImageNet和HICO-DET等基准数据集上的实验结果验证了其潜力。

Conclusion: 该方法为当前复杂场景中人物与物体交互检测提供了更高效准确的解决方案，且相关代码公开供使用。

Abstract: Human-object interaction (HOI) detection is essential for accurately
localizing and characterizing interactions between humans and objects,
providing a comprehensive understanding of complex visual scenes across various
domains. However, existing HOI detectors often struggle to deliver reliable
predictions efficiently, relying on resource-intensive training methods and
inefficient architectures. To address these challenges, we conceptualize a
wavelet attention-like backbone and a novel ray-based encoder architecture
tailored for HOI detection. Our wavelet backbone addresses the limitations of
expressing middle-order interactions by aggregating discriminative features
from the low- and high-order interactions extracted from diverse convolutional
filters. Concurrently, the ray-based encoder facilitates multi-scale attention
by optimizing the focus of the decoder on relevant regions of interest and
mitigating computational overhead. As a result of harnessing the attenuated
intensity of learnable ray origins, our decoder aligns query embeddings with
emphasized regions of interest for accurate predictions. Experimental results
on benchmark datasets, including ImageNet and HICO-DET, showcase the potential
of our proposed architecture. The code is publicly available at
[https://github.com/henry-pay/RayEncoder].

</details>


### [20] [Mind the Gap: Bridging Occlusion in Gait Recognition via Residual Gap Correction](https://arxiv.org/abs/2507.10978)
*Ayush Gupta,Siyuan Huang,Rama Chellappa*

Main category: cs.CV

TL;DR: 该论文提出了一种解决遮挡步态识别问题并保留完整步态识别性能的方法。


<details>
  <summary>Details</summary>
Motivation: 目前步态识别在解决遮挡问题时效率不高，现有方法过于依赖配对数据或无法在完整输入中保持高性能。

Method: 提出了RG-Gait，将遮挡步态问题建模为残差学习任务，学得的残差用于改善遮挡情况下的步态识别表现，同时保持完整步态识别性能。

Result: 在Gait3D、GREW和BRIAR等数据集上验证了方法的有效性，证明残差学习模型能有效应对遮挡，同时保留对完整步态的高辨识能力。

Conclusion: RG-Gait通过残差学习显著改善了遮挡步态识别的性能，是一种有效的解决方案。

Abstract: Gait is becoming popular as a method of person re-identification because of
its ability to identify people at a distance. However, most current works in
gait recognition do not address the practical problem of occlusions. Among
those which do, some require paired tuples of occluded and holistic sequences,
which are impractical to collect in the real world. Further, these approaches
work on occlusions but fail to retain performance on holistic inputs. To
address these challenges, we propose RG-Gait, a method for residual correction
for occluded gait recognition with holistic retention. We model the problem as
a residual learning task, conceptualizing the occluded gait signature as a
residual deviation from the holistic gait representation. Our proposed network
adaptively integrates the learned residual, significantly improving performance
on occluded gait sequences without compromising the holistic recognition
accuracy. We evaluate our approach on the challenging Gait3D, GREW and BRIAR
datasets and show that learning the residual can be an effective technique to
tackle occluded gait recognition with holistic retention.

</details>


### [21] [SpaRTAN: Spatial Reinforcement Token-based Aggregation Network for Visual Recognition](https://arxiv.org/abs/2507.10999)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: 提出了一种名为SpaRTAN的新型轻量化网络架构，通过多尺度卷积核和波形式通道聚合模块改善特征提取与信息冗余问题，取得了卓越的参数效率与竞争性能。


<details>
  <summary>Details</summary>
Motivation: 现有的CNN和transformer架构存在偏向简单特征的倾向，同时现代CNN中混入的MLP模块信息冗余较大，设计以提高参数效率。

Method: 通过结合不同接收域的卷积核捕捉高阶空间特征，辅以波形式通道聚合模块减少通道冗余，以动态聚合判别特征。

Result: 在ImageNet-1k基准上，SpaRTAN仅用3.8M参数和1.0 GFLOPs即达77.7%精度；在COCO基准上，用21.5M参数获得50.0% AP，超越前一基准1.2%。

Conclusion: 通过高效轻量化设计，SpaRTAN在保持竞争性能的同时实现卓越的参数效率，为未来视觉任务提供更优化的网络架构选择。

Abstract: The resurgence of convolutional neural networks (CNNs) in visual recognition
tasks, exemplified by ConvNeXt, has demonstrated their capability to rival
transformer-based architectures through advanced training methodologies and
ViT-inspired design principles. However, both CNNs and transformers exhibit a
simplicity bias, favoring straightforward features over complex structural
representations. Furthermore, modern CNNs often integrate MLP-like blocks akin
to those in transformers, but these blocks suffer from significant information
redundancies, necessitating high expansion ratios to sustain competitive
performance. To address these limitations, we propose SpaRTAN, a lightweight
architectural design that enhances spatial and channel-wise information
processing. SpaRTAN employs kernels with varying receptive fields, controlled
by kernel size and dilation factor, to capture discriminative multi-order
spatial features effectively. A wave-based channel aggregation module further
modulates and reinforces pixel interactions, mitigating channel-wise
redundancies. Combining the two modules, the proposed network can efficiently
gather and dynamically contextualize discriminative features. Experimental
results in ImageNet and COCO demonstrate that SpaRTAN achieves remarkable
parameter efficiency while maintaining competitive performance. In particular,
on the ImageNet-1k benchmark, SpaRTAN achieves 77. 7% accuracy with only 3.8M
parameters and approximately 1.0 GFLOPs, demonstrating its ability to deliver
strong performance through an efficient design. On the COCO benchmark, it
achieves 50.0% AP, surpassing the previous benchmark by 1.2% with only 21.5M
parameters. The code is publicly available at
[https://github.com/henry-pay/SpaRTAN].

</details>


### [22] [Bridge Feature Matching and Cross-Modal Alignment with Mutual-filtering for Zero-shot Anomaly Detection](https://arxiv.org/abs/2507.11003)
*Yuhu Bai,Jiangning Zhang,Yunkang Cao,Guangyuan Lu,Qingdong He,Xiangtai Li,Guanzhong Tian*

Main category: cs.CV

TL;DR: 提出了一种名为FiSeCLIP的方法，用于基于CLIP的零样本异常检测，在MVTec-AD数据集上取得了超越SOTA的方法AdaCLIP的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型在零样本/小样本场景中表现出色，研究旨在提高视觉-语言模型在实际工业需求中进行零样本异常检测的能力。

Method: 引入FiSeCLIP，结合特征匹配和跨模态对齐，利用同批次图像作为参考信息并通过文本信息过滤掉噪声特征。同时挖掘CLIP潜在的局部语义关联，用于提高细粒度异常检测的能力。

Result: FiSeCLIP在异常检测基准（如MVTec-AD）上，异常分类和分割性能均优于AdaCLIP，特别是在分割评估指标AU-ROC和$F_1$-max上分别提升了4.6%和5.7%。

Conclusion: FiSeCLIP为零样本异常检测提出了一种更强的基线方法，证明了其在实际应用场景和细粒度任务中的优越性。

Abstract: With the advent of vision-language models (e.g., CLIP) in zero- and few-shot
settings, CLIP has been widely applied to zero-shot anomaly detection (ZSAD) in
recent research, where the rare classes are essential and expected in many
applications. This study introduces \textbf{FiSeCLIP} for ZSAD with
training-free \textbf{CLIP}, combining the feature matching with the
cross-modal alignment. Testing with the entire dataset is impractical, while
batch-based testing better aligns with real industrial needs, and images within
a batch can serve as mutual reference points. Accordingly, FiSeCLIP utilizes
other images in the same batch as reference information for the current image.
However, the lack of labels for these references can introduce ambiguity, we
apply text information to \textbf{fi}lter out noisy features. In addition, we
further explore CLIP's inherent potential to restore its local
\textbf{se}mantic correlation, adapting it for fine-grained anomaly detection
tasks to enable a more accurate filtering process. Our approach exhibits
superior performance for both anomaly classification and segmentation on
anomaly detection benchmarks, building a stronger baseline for the direction,
e.g., on MVTec-AD, FiSeCLIP outperforms the SOTA AdaCLIP by
+4.6\%$\uparrow$/+5.7\%$\uparrow$ in segmentation metrics AU-ROC/$F_1$-max.

</details>


### [23] [Semantically Informed Salient Regions Guided Radiology Report Generation](https://arxiv.org/abs/2507.11015)
*Zeyi Hou,Zeqiang Wei,Ruixin Yan,Ning Lang,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: SISRNet方法通过关注医学图像中具有临床重要性的显著区域，提高了X射线报告生成的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习方法生成的X射线报告流畅但医学准确性欠佳的局限性，缓解数据偏置难题。

Method: 提出了一种语义驱动的显著区域引导方法（SISRNet），该方法通过跨模态细粒度语义分析，明确识别医学关键特征区域，并在图像建模与报告生成中重点关注这些区域。

Result: SISRNet在常用的数据集IU-Xray和MIMIC-CXR上的性能优于现有方法，生成报告准确性和临床适用性显著提升。

Conclusion: SISRNet能够有效捕获X射线细微异常，克服数据偏置问题，为临床医学领域自动报告生成提供有力支持。

Abstract: Recent advances in automated radiology report generation from chest X-rays
using deep learning algorithms have the potential to significantly reduce the
arduous workload of radiologists. However, due to the inherent massive data
bias in radiology images, where abnormalities are typically subtle and sparsely
distributed, existing methods often produce fluent yet medically inaccurate
reports, limiting their applicability in clinical practice. To address this
issue effectively, we propose a Semantically Informed Salient Regions-guided
(SISRNet) report generation method. Specifically, our approach explicitly
identifies salient regions with medically critical characteristics using
fine-grained cross-modal semantics. Then, SISRNet systematically focuses on
these high-information regions during both image modeling and report
generation, effectively capturing subtle abnormal findings, mitigating the
negative impact of data bias, and ultimately generating clinically accurate
reports. Compared to its peers, SISRNet demonstrates superior performance on
widely used IU-Xray and MIMIC-CXR datasets.

</details>


### [24] [Human-Guided Shade Artifact Suppression in CBCT-to-MDCT Translation via Schrödinger Bridge with Conditional Diffusion](https://arxiv.org/abs/2507.11025)
*Sung Ho Kang,Hyun-Cheol Park*

Main category: cs.CV

TL;DR: 提出了一种基于薛定谔桥框架的CBCT到MDCT转换方法，结合GAN先验和基于人类引导的条件扩散，显著提升了医学图像转换性能。


<details>
  <summary>Details</summary>
Motivation: 解决CBCT图像转换为高质量MDCT图像的问题，特别是提升解剖结构保真度和感知可控性。

Method: 采用基于薛定谔桥的框架，引入GAN模型的先验信息和人类反馈的条件扩散，通过无分类器引导（CFG）实现生成过程的引导，同时通过迭代优化和基于竞技选择的人类反馈内化机制增强模型。

Result: 在RMSE、SSIM、LPIPS和Dice指标上表现优异，相较于以往基于GAN和微调的反馈方法具有更好的性能，同时仅需10次采样步骤。

Conclusion: 该框架有效且高效，实现了实时、对偏好对齐的医学图像转换，展示了其在医学成像领域的潜力。

Abstract: We present a novel framework for CBCT-to-MDCT translation, grounded in the
Schrodinger Bridge (SB) formulation, which integrates GAN-derived priors with
human-guided conditional diffusion. Unlike conventional GANs or diffusion
models, our approach explicitly enforces boundary consistency between CBCT
inputs and pseudo targets, ensuring both anatomical fidelity and perceptual
controllability. Binary human feedback is incorporated via classifier-free
guidance (CFG), effectively steering the generative process toward clinically
preferred outcomes. Through iterative refinement and tournament-based
preference selection, the model internalizes human preferences without relying
on a reward model. Subtraction image visualizations reveal that the proposed
method selectively attenuates shade artifacts in key anatomical regions while
preserving fine structural detail. Quantitative evaluations further demonstrate
superior performance across RMSE, SSIM, LPIPS, and Dice metrics on clinical
datasets -- outperforming prior GAN- and fine-tuning-based feedback methods --
while requiring only 10 sampling steps. These findings underscore the
effectiveness and efficiency of our framework for real-time, preference-aligned
medical image translation.

</details>


### [25] [Personalized OVSS: Understanding Personal Concept in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2507.11030)
*Sunghyun Park,Jungsoo Lee,Shubhankar Borse,Munawar Hayat,Sungha Choi,Kyuwoong Hwang,Fatih Porikli*

Main category: cs.CV

TL;DR: 研究提出了个性化开放词汇语义分割任务，并研发了针对个人视觉概念识别的新方法。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇语义分割技术无法理解用户的个性化文本描述，如“我的杯子”与“多个杯子”中的区分。

Method: 提出基于文本提示调优的插件方法，通过少量图像与遮罩对学习个性化视觉概念，同时引入“负遮罩提议”与视觉嵌入增强文本表示。

Result: 方法在新建的个性化分割基准（FSSper, CUBper, ADEper）上表现优异。

Conclusion: 所提方法在提升个性化语义分割能力的同时，保持了原有开放词汇语义分割的性能。

Abstract: While open-vocabulary semantic segmentation (OVSS) can segment an image into
semantic regions based on arbitrarily given text descriptions even for classes
unseen during training, it fails to understand personal texts (e.g., `my mug
cup') for segmenting regions of specific interest to users. This paper
addresses challenges like recognizing `my mug cup' among `multiple mug cups'.
To overcome this challenge, we introduce a novel task termed
\textit{personalized open-vocabulary semantic segmentation} and propose a text
prompt tuning-based plug-in method designed to recognize personal visual
concepts using a few pairs of images and masks, while maintaining the
performance of the original OVSS. Based on the observation that reducing false
predictions is essential when applying text prompt tuning to this task, our
proposed method employs `negative mask proposal' that captures visual concepts
other than the personalized concept. We further improve the performance by
enriching the representation of text prompts by injecting visual embeddings of
the personal concept into them. This approach enhances personalized OVSS
without compromising the original OVSS performance. We demonstrate the
superiority of our method on our newly established benchmarks for this task,
including FSS$^\text{per}$, CUB$^\text{per}$, and ADE$^\text{per}$.

</details>


### [26] [Efficient Dual-domain Image Dehazing with Haze Prior Perception](https://arxiv.org/abs/2507.11035)
*Lirong Zheng,Yanshan Li,Rui Yu,Kaihao Zhang*

Main category: cs.CV

TL;DR: 提出了名为DGFDNet的网络框架，通过结合空间和频率域，在单张图像去雾中实现了实时和高效性能。


<details>
  <summary>Details</summary>
Motivation: 现有去雾方法在处理复杂雾霾条件时计算量大且效果有限；因此，需要设计既高效又具备强全局建模能力的去雾方法。

Method: 提出利用暗通道先验引导的频率感知去雾网络（DGFDNet），结合HAFM和MGAM模块，同时引入PCGB分支，实现空间和频率域的优化融合。

Result: DGFDNet在四个基准去雾数据集上的性能具有最优表现，同时兼具强鲁棒性和实时处理效率。

Conclusion: DGFDNet有效提升了去雾性能，在多种复杂场景下表现出良好的去雾效果和效率。

Abstract: Transformer-based models exhibit strong global modeling capabilities in
single-image dehazing, but their high computational cost limits real-time
applicability. Existing methods predominantly rely on spatial-domain features
to capture long-range dependencies, which are computationally expensive and
often inadequate under complex haze conditions. While some approaches introduce
frequency-domain cues, the weak coupling between spatial and frequency branches
limits the overall performance. To overcome these limitations, we propose the
Dark Channel Guided Frequency-aware Dehazing Network (DGFDNet), a novel
dual-domain framework that performs physically guided degradation alignment
across spatial and frequency domains. At its core, the DGFDBlock comprises two
key modules: 1) the Haze-Aware Frequency Modulator (HAFM), which generates a
pixel-level haze confidence map from dark channel priors to adaptively enhance
haze-relevant frequency components, thereby achieving global degradation-aware
spectral modulation; 2) the Multi-level Gating Aggregation Module (MGAM), which
fuses multi-scale features through diverse convolutional kernels and hybrid
gating mechanisms to recover fine structural details. Additionally, a Prior
Correction Guidance Branch (PCGB) incorporates a closed-loop feedback
mechanism, enabling iterative refinement of the prior by intermediate dehazed
features and significantly improving haze localization accuracy, especially in
challenging outdoor scenes. Extensive experiments on four benchmark haze
datasets demonstrate that DGFDNet achieves state-of-the-art performance with
superior robustness and real-time efficiency. Code is available at:
https://github.com/Dilizlr/DGFDNet.

</details>


### [27] [A Multi-View High-Resolution Foot-Ankle Complex Point Cloud Dataset During Gait for Occlusion-Robust 3D Completion](https://arxiv.org/abs/2507.11037)
*Jie-Wen Li,Zi-Han Ye,Qingyuan Zhou,Jiayi Song,Ying He,Ben Fei,Wen-Ming Chen*

Main category: cs.CV

TL;DR: 本研究开发了FootGait3D，这是一个包含高分辨率足踝表面点云的多视图数据集，用于研究步态中的足踝运动学，并解决动态步态中采集数据的挑战。


<details>
  <summary>Details</summary>
Motivation: 提出高精度足踝区运动学建模数据，解决步态动态采集中因遮挡和视角所造成的数据采集挑战，同时推动生物力学和临床评估研究。

Method: 使用一个定制的五相机深度感知系统，采集46名对象在自然步态下的8,403帧点云数据，每帧数据包含五视图的完整重建的足踝表面点云以及部分遮挡的点云，用于评估3D点云补全方法的效能。

Result: 提供了一个创新的数据集，支持对单模态和多模态点云完成网络的测试和基准比较，验证其在不同遮挡水平和视角下的表现。

Conclusion: FootGait3D数据集为生物力学和多分段足部建模研究带来了新机遇，并可用于临床步态分析、假肢设计及机器人领域对详细足部模型的需求。

Abstract: The kinematics analysis of foot-ankle complex during gait is essential for
advancing biomechanical research and clinical assessment. Collecting accurate
surface geometry data from the foot and ankle during dynamic gait conditions is
inherently challenging due to swing foot occlusions and viewing limitations.
Thus, this paper introduces FootGait3D, a novel multi-view dataset of
high-resolution ankle-foot surface point clouds captured during natural gait.
Different from existing gait datasets that typically target whole-body or
lower-limb motion, FootGait3D focuses specifically on the detailed modeling of
the ankle-foot region, offering a finer granularity of motion data. To address
this, FootGait3D consists of 8,403 point cloud frames collected from 46
subjects using a custom five-camera depth sensing system. Each frame includes a
complete 5-view reconstruction of the foot and ankle (serving as ground truth)
along with partial point clouds obtained from only four, three, or two views.
This structured variation enables rigorous evaluation of 3D point cloud
completion methods under varying occlusion levels and viewpoints. Our dataset
is designed for shape completion tasks, facilitating the benchmarking of
state-of-the-art single-modal (e.g., PointTr, SnowflakeNet, Anchorformer) and
multi-modal (e.g., SVDFormer, PointSea, CSDN) completion networks on the
challenge of recovering the full foot geometry from occluded inputs. FootGait3D
has significant potential to advance research in biomechanics and multi-segment
foot modeling, offering a valuable testbed for clinical gait analysis,
prosthetic design, and robotics applications requiring detailed 3D models of
the foot during motion. The dataset is now available at
https://huggingface.co/datasets/ljw285/FootGait3D.

</details>


### [28] [Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery](https://arxiv.org/abs/2507.11040)
*Nicolas Drapier,Aladine Chetouani,Aurélien Chateigner*

Main category: cs.CV

TL;DR: GLOD提出一种针对高分辨率卫星图像的目标检测的新型Transformer架构，并大幅提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 提升高分辨率卫星图像目标检测性能，解决传统方法的不足。

Method: 用Swin Transformer取代CNN骨干网络，结合创新的UpConvMixer模块进行上采样，以及多尺度融合模块实现特征整合，采用非对称融合和多路径头部设计以优化检测能力。

Result: 在xView数据集上取得32.95%的性能表现，比目前最优方法高出11.46%。

Conclusion: GLOD在高效计算的前提下优化了卫星图像目标检测，展现出显著优越性。

Abstract: We present GLOD, a transformer-first architecture for object detection in
high-resolution satellite imagery. GLOD replaces CNN backbones with a Swin
Transformer for end-to-end feature extraction, combined with novel UpConvMixer
blocks for robust upsampling and Fusion Blocks for multi-scale feature
integration. Our approach achieves 32.95\% on xView, outperforming SOTA methods
by 11.46\%. Key innovations include asymmetric fusion with CBAM attention and a
multi-path head design capturing objects across scales. The architecture is
optimized for satellite imagery challenges, leveraging spatial priors while
maintaining computational efficiency.

</details>


### [29] [Alleviating Textual Reliance in Medical Language-guided Segmentation via Prototype-driven Semantic Approximation](https://arxiv.org/abs/2507.11055)
*Shuchang Ye,Usman Naseem,Mingyuan Meng,Jinman Kim*

Main category: cs.CV

TL;DR: 本文提出了ProLearn，一种通过语义原型驱动的图像分割技术以减少对文本报告依赖性的新方法。


<details>
  <summary>Details</summary>
Motivation: 大多医学数据集中缺乏配对的影像与文本，而现有的基于语言引导的分割方法受限于此，影响其应用性与可用性。

Method: 提出了ProLearn框架，引入语义原型驱动方法(PSA)，通过查询与回应机制降低对文本输入的依赖，从而支持单独影像数据的分割任务。

Result: 实验结果表明，ProLearn在有限文本数据条件下优于现有最前沿语言引导方法。

Conclusion: ProLearn能够在减少文本依赖的同时提升分割性能，为医学图像分割在临床应用的广泛性提供了新的解决方案。

Abstract: Medical language-guided segmentation, integrating textual clinical reports as
auxiliary guidance to enhance image segmentation, has demonstrated significant
improvements over unimodal approaches. However, its inherent reliance on paired
image-text input, which we refer to as ``textual reliance", presents two
fundamental limitations: 1) many medical segmentation datasets lack paired
reports, leaving a substantial portion of image-only data underutilized for
training; and 2) inference is limited to retrospective analysis of cases with
paired reports, limiting its applicability in most clinical scenarios where
segmentation typically precedes reporting. To address these limitations, we
propose ProLearn, the first Prototype-driven Learning framework for
language-guided segmentation that fundamentally alleviates textual reliance. At
its core, in ProLearn, we introduce a novel Prototype-driven Semantic
Approximation (PSA) module to enable approximation of semantic guidance from
textual input. PSA initializes a discrete and compact prototype space by
distilling segmentation-relevant semantics from textual reports. Once
initialized, it supports a query-and-respond mechanism which approximates
semantic guidance for images without textual input, thereby alleviating textual
reliance. Extensive experiments on QaTa-COV19, MosMedData+ and Kvasir-SEG
demonstrate that ProLearn outperforms state-of-the-art language-guided methods
when limited text is available.

</details>


### [30] [Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling](https://arxiv.org/abs/2507.11061)
*Hayeon Kim,Ji Ha Jang,Se Young Chun*

Main category: cs.CV

TL;DR: 为了实现更精准的局部3D编辑，提出RoMaP框架，通过3D-GALP模块和改进的SDS损失，再结合额外正则化器以实现高质量和灵活的3D高斯编辑。


<details>
  <summary>Details</summary>
Motivation: 当前3D神经表达和实例级编辑的进展使得高质量3D内容生成更高效，但局部3D编辑仍然存在挑战，尤其在高斯Splatting上，主要由于多视角2D部分分割的不一致性和SDS损失的模糊特性。

Method: 提出RoMaP框架，包括：(1) 3D-GALP模块，通过球谐系数处理多视角标签变化，实现一致性的部分分割；(2) 改进的SDS损失，引入L1锚定损失和其他正则化器，实现对目标区域的精确编辑，同时保持上下文一致性；并在必要时进行超出现有上下文的灵活变化。

Result: 实验结果表明，RoMaP在优化后的高斯场景和物体的局部3D编辑上表现最优，具有定性和定量上的显著提升。

Conclusion: RoMaP框架赋能更鲁棒和灵活的部分级3D高斯编辑，突破了当前技术的局限。

Abstract: Recent advances in 3D neural representations and instance-level editing
models have enabled the efficient creation of high-quality 3D content. However,
achieving precise local 3D edits remains challenging, especially for Gaussian
Splatting, due to inconsistent multi-view 2D part segmentations and inherently
ambiguous nature of Score Distillation Sampling (SDS) loss. To address these
limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that
enables precise and drastic part-level modifications. First, we introduce a
robust 3D mask generation module with our 3D-Geometry Aware Label Prediction
(3D-GALP), which uses spherical harmonics (SH) coefficients to model
view-dependent label variations and soft-label property, yielding accurate and
consistent part segmentations across viewpoints. Second, we propose a
regularized SDS loss that combines the standard SDS loss with additional
regularizers. In particular, an L1 anchor loss is introduced via our Scheduled
Latent Mixing and Part (SLaMP) editing method, which generates high-quality
part-edited 2D images and confines modifications only to the target region
while preserving contextual coherence. Additional regularizers, such as
Gaussian prior removal, further improve flexibility by allowing changes beyond
the existing context, and robust 3D masking prevents unintended edits.
Experimental results demonstrate that our RoMaP achieves state-of-the-art local
3D editing on both reconstructed and generated Gaussian scenes and objects
qualitatively and quantitatively, making it possible for more robust and
flexible part-level 3D Gaussian editing.

</details>


### [31] [Joint angle model based learning to refine kinematic human pose estimation](https://arxiv.org/abs/2507.11075)
*Chang Peng,Yifei Zhou,Huifeng Xi,Shiqing Huang,Chuangye Chen,Jianming Yang,Bao Yang,Zhenyu Jiang*

Main category: cs.CV

TL;DR: 提出了一种基于关节角度的模型和技术，用于提高无标记人体姿态估计（HPE）的精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前的HPE方法易受关键点识别错误和轨迹波动的影响，而现有深度学习模型的改进受限于不准确的训练数据集。

Method: 构建了一个基于关节角度的模型，利用高阶傅里叶级数逼近关节角度的时间变化来生成高质量“真值”，并使用双向递归网络优化HRNet的估计结果。

Result: 经过所构建高质量数据集训练的网络能够有效校正错误的关键点识别并平滑其时空轨迹，在花样滑冰和霹雳舞这类高挑战性场景中，表现优于目前最先进的HPE优化方法。

Conclusion: 基于关节角度的优化方法显著提升了HPE在复杂场景中的表现，可以更准确和稳定地估算人体运动姿态。

Abstract: Marker-free human pose estimation (HPE) has found increasing applications in
various fields. Current HPE suffers from occasional errors in keypoint
recognition and random fluctuation in keypoint trajectories when analyzing
kinematic human poses. The performance of existing deep learning-based models
for HPE refinement is considerably limited by inaccurate training datasets in
which the keypoints are manually annotated. This paper proposed a novel method
to overcome the difficulty through joint angle-based modeling. The key
techniques include: (i) A joint angle-based model of human pose, which is
robust to describe kinematic human poses; (ii) Approximating temporal variation
of joint angles through high order Fourier series to get reliable "ground
truth"; (iii) A bidirectional recurrent network is designed as a
post-processing module to refine the estimation of well-established HRNet.
Trained with the high-quality dataset constructed using our method, the network
demonstrates outstanding performance to correct wrongly recognized joints and
smooth their spatiotemporal trajectories. Tests show that joint angle-based
refinement (JAR) outperforms the state-of-the-art HPE refinement network in
challenging cases like figure skating and breaking.

</details>


### [32] [GKNet: Graph-based Keypoints Network for Monocular Pose Estimation of Non-cooperative Spacecraft](https://arxiv.org/abs/2507.11077)
*Weizhao Ma,Dong Zhou,Yuhui Hu,Zipeng He*

Main category: cs.CV

TL;DR: 该论文提出了一种单目姿态估计新方法GKNet，并验证了其在非合作航天器上的高精度表现，同时发布了一个相关数据集SKD。


<details>
  <summary>Details</summary>
Motivation: 当前单目姿态估计方法对非合作航天器的结构对称性和部分遮挡问题较为敏感，需要提升其关键点检测的鲁棒性和精确度。

Method: 提出了一个基于图的关键点网络GKNet，通过利用关键点图的几何约束提升检测效果。此外，公开了一个航天器关键点检测数据集SKD。

Result: 实验和消融研究表明，GKNet在准确性和效果上优于当前最先进的方法。

Conclusion: GKNet能有效提高非合作航天器的姿态估计精度，并结合SKD数据集为相关研究提供支持。

Abstract: Monocular pose estimation of non-cooperative spacecraft is significant for
on-orbit service (OOS) tasks, such as satellite maintenance, space debris
removal, and station assembly. Considering the high demands on pose estimation
accuracy, mainstream monocular pose estimation methods typically consist of
keypoint detectors and PnP solver. However, current keypoint detectors remain
vulnerable to structural symmetry and partial occlusion of non-cooperative
spacecraft. To this end, we propose a graph-based keypoints network for the
monocular pose estimation of non-cooperative spacecraft, GKNet, which leverages
the geometric constraint of keypoints graph. In order to better validate
keypoint detectors, we present a moderate-scale dataset for the spacecraft
keypoint detection, named SKD, which consists of 3 spacecraft targets, 90,000
simulated images, and corresponding high-precise keypoint annotations.
Extensive experiments and an ablation study have demonstrated the high accuracy
and effectiveness of our GKNet, compared to the state-of-the-art spacecraft
keypoint detectors. The code for GKNet and the SKD dataset is available at
https://github.com/Dongzhou-1996/GKNet.

</details>


### [33] [Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification](https://arxiv.org/abs/2507.11081)
*Chang Peng,Bao Yang,Meiqi Li,Ge Zhang,Hui Sun,Zhenyu Jiang*

Main category: cs.CV

TL;DR: 本研究提出一种基于深度学习的道路地下病害检测方法，通过构建高质量的3D GPR数据集并引入创新的交叉验证策略，大幅提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前使用地质雷达（GPR）进行道路地下病害检测存在人工识别耗时且依赖经验的问题。深度学习虽有望解决，但受限于数据集稀缺及网络分辨能力不足。

Method: 研究构建了包含2134个样本的高质量3D GPR数据集，并基于YOLO模型使用创新的交叉验证方法增强了对不同病害的识别能力。

Result: 所提方法在实地测试中召回率超过98.6%，并显著减少了约90%的人工检测工作量。

Conclusion: 这种集成于在线检测系统中的方法展示了其高效性，为道路地下病害的自动化检测提供了切实可行的解决方案。

Abstract: Ground penetrating radar (GPR) has become a rapid and non-destructive
solution for road subsurface distress (RSD) detection. However, RSD recognition
from GPR images is labor-intensive and heavily relies on inspectors' expertise.
Deep learning offers the possibility for automatic RSD recognition, but its
current performance is limited by two factors: Scarcity of high-quality dataset
for network training and insufficient capability of network to distinguish RSD.
In this study, a rigorously validated 3D GPR dataset containing 2134 samples of
diverse types was constructed through field scanning. Based on the finding that
the YOLO model trained with one of the three scans of GPR images exhibits
varying sensitivity to specific type of RSD, we proposed a novel
cross-verification strategy with outstanding accuracy in RSD recognition,
achieving recall over 98.6% in field tests. The approach, integrated into an
online RSD detection system, can reduce the labor of inspection by around 90%.

</details>


### [34] [Atmos-Bench: 3D Atmospheric Structures for Climate Insight](https://arxiv.org/abs/2507.11085)
*Tianchi Xu*

Main category: cs.CV

TL;DR: 文章提出了一个新的3D大气基准测试Atmos-Bench，以及一个新颖的网络模型FourCastX，用于改进卫星激光雷达数据的3D大气结构恢复。


<details>
  <summary>Details</summary>
Motivation: 现有方法对物理近似过于简化，依赖辅助输入，存在不确定性且缺乏标准化的3D基准测试。作者希望弥补这些不足，建立一个新的标准化基准并提出更精准的恢复模型。

Method: 作者开发了Atmos-Bench基准，结合WRF和改进的COSP模拟器生成了高质量的3D散射体积数据，同时提出了FourCastX网络模型。该模型嵌入了物理约束，提升了还原过程中能量一致性。

Result: FourCastX在Atmos-Bench基准测试上的两个波长数据集(532nm和355nm)上均表现出色，显著超越了其他方法，并无需依赖辅助输入。

Conclusion: Atmos-Bench和FourCastX为卫星激光雷达数据的3D大气结构恢复设立了新标准，为深入理解气候提供了基础。

Abstract: Atmospheric structure, represented by backscatter coefficients (BC) recovered
from satellite LiDAR attenuated backscatter (ATB), provides a volumetric view
of clouds, aerosols, and molecules, playing a critical role in human
activities, climate understanding, and extreme weather forecasting. Existing
methods often rely on auxiliary inputs and simplified physics-based
approximations, and lack a standardized 3D benchmark for fair evaluation.
However, such approaches may introduce additional uncertainties and
insufficiently capture realistic radiative transfer and atmospheric
scattering-absorption effects. To bridge these gaps, we present Atmos-Bench:
the first 3D atmospheric benchmark, along with a novel FourCastX:
Frequency-enhanced Spatio-Temporal Mixture-of-Experts Network that (a)
generates 921,600 image slices from 3D scattering volumes simulated at 532 nm
and 355 nm by coupling WRF with an enhanced COSP simulator over 384 land-ocean
time steps, yielding high-quality voxel-wise references; (b) embeds ATB-BC
physical constraints into the model architecture, promoting energy consistency
during restoration; (c) achieves consistent improvements on the Atmos-Bench
dataset across both 355 nm and 532 nm bands, outperforming state-of-the-art
baseline models without relying on auxiliary inputs. Atmos-Bench establishes a
new standard for satellite-based 3D atmospheric structure recovery and paves
the way for deeper climate insight.

</details>


### [35] [A Survey on Interpretability in Visual Recognition](https://arxiv.org/abs/2507.11099)
*Qiyang Wan,Chengzhi Gao,Ruiping Wang,Xilin Chen*

Main category: cs.CV

TL;DR: 近年来视觉识别技术取得重大进展，本研究系统性综述了解释性研究，并提出基于人类视角的分类法。


<details>
  <summary>Details</summary>
Motivation: 推动关键领域的应用和故障诊断需求日益增加，促进了解释性研究发展。

Method: 从意图、对象、呈现方式及方法论四个维度对解释性方法进行分类与总结，并探讨大规模多模态模型等新技术的潜力。

Result: 对现有的解释性研究进行了系统性综述和分类，提出了一整套系统、连贯的分组标准。

Conclusion: 研究为相关领域的组织理解与未来解释性视觉识别模型研究提供了启发。

Abstract: In recent years, visual recognition methods have advanced significantly,
finding applications across diverse fields. While researchers seek to
understand the mechanisms behind the success of these models, there is also a
growing impetus to deploy them in critical areas like autonomous driving and
medical diagnostics to better diagnose failures, which promotes the development
of interpretability research. This paper systematically reviews existing
research on the interpretability of visual recognition models and proposes a
taxonomy of methods from a human-centered perspective. The proposed taxonomy
categorizes interpretable recognition methods based on Intent, Object,
Presentation, and Methodology, thereby establishing a systematic and coherent
set of grouping criteria for these XAI methods. Additionally, we summarize the
requirements for evaluation metrics and explore new opportunities enabled by
recent technologies, such as large multimodal models. We aim to organize
existing research in this domain and inspire future investigations into the
interpretability of visual recognition models.

</details>


### [36] [KptLLM++: Towards Generic Keypoint Comprehension with Large Language Model](https://arxiv.org/abs/2507.11102)
*Jie Yang,Wang Zeng,Sheng Jin,Lumin Xu,Wentao Liu,Chen Qian,Zhen Li,Ruimao Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为KptLLM++的新型多模态大语言模型，专注于多样化的关键点理解任务，并取得了先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在捕捉细粒度语义信息方面存在不足，特别是在精准识别和分析对象关键点的能力上。

Method: 提出了KptLLM++，通过用户定义的指导整合多种输入模态，并使用“先识别后检测”的新范式，通过结构化的链式推理机制来解析关键点语义和定位其精确位置。同时扩展了超过50万样本的数据集，涵盖了多种对象和复杂场景。

Result: 在多个关键点检测基准上，KptLLM++展现出了最先进的性能，具有出色的准确性和泛化能力。

Conclusion: KptLLM++为细粒度图像理解提供了统一解决方案，并对人机交互产生了重要变革性影响。

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has revolutionized
image understanding by bridging textual and visual modalities. However, these
models often struggle with capturing fine-grained semantic information, such as
the precise identification and analysis of object keypoints. Keypoints, as
structure-aware, pixel-level, and compact representations of objects,
particularly articulated ones, play a crucial role in applications such as
fine-grained image analysis, object retrieval, and behavior recognition. In
this paper, we propose KptLLM++, a novel multimodal large language model that
specifically designed for generic keypoint comprehension through the
integration of diverse input modalities guided by user-defined instructions. By
unifying keypoint detection across varied contexts, KptLLM++ establishes itself
as an advanced interface, fostering more effective human-AI collaboration. The
model is built upon a novel identify-then-detect paradigm, which first
interprets keypoint semantics and subsequently localizes their precise
positions through a structured chain-of-thought reasoning mechanism. To push
the boundaries of performance, we have scaled up the training dataset to over
500K samples, encompassing diverse objects, keypoint categories, image styles,
and scenarios with complex occlusions. This extensive scaling enables KptLLM++
to unlock its potential, achieving remarkable accuracy and generalization.
Comprehensive experiments on multiple keypoint detection benchmarks demonstrate
its state-of-the-art performance, underscoring its potential as a unified
solution for fine-grained image understanding and its transformative
implications for human-AI interaction.

</details>


### [37] [Jellyfish Species Identification: A CNN Based Artificial Neural Network Approach](https://arxiv.org/abs/2507.11116)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha,Mostofa Kamal Nasir*

Main category: cs.CV

TL;DR: 本文探讨了深度学习框架在水下图像中检测和分类水母物种的应用，最优模型准确率达98%。


<details>
  <summary>Details</summary>
Motivation: 研究水母在海洋生态系统中的关键作用及其快速扩张对生物多样性和保护带来的挑战，旨在通过深度学习实现高效物种识别。

Method: 提出采用深度学习框架结合MobileNetV3、ResNet50、EfficientNetV2-B0和VGG16等高级特征提取技术，以及传统机器学习和前馈神经网络分类器进行水母物种识别。

Result: 最终研究发现将人工神经网络与MobileNetV3结合的模型表现最佳，准确率达98%。

Conclusion: 该研究证明深度学习和混合框架在解决生物多样性问题及提升海洋环境下物种检测有效性上的潜力。

Abstract: Jellyfish, a diverse group of gelatinous marine organisms, play a crucial
role in maintaining marine ecosystems but pose significant challenges for
biodiversity and conservation due to their rapid proliferation and ecological
impact. Accurate identification of jellyfish species is essential for
ecological monitoring and management. In this study, we proposed a deep
learning framework for jellyfish species detection and classification using an
underwater image dataset. The framework integrates advanced feature extraction
techniques, including MobileNetV3, ResNet50, EfficientNetV2-B0, and VGG16,
combined with seven traditional machine learning classifiers and three
Feedforward Neural Network classifiers for precise species identification.
Additionally, we activated the softmax function to directly classify jellyfish
species using the convolutional neural network models. The combination of the
Artificial Neural Network with MobileNetV3 is our best-performing model,
achieving an exceptional accuracy of 98%, significantly outperforming other
feature extractor-classifier combinations. This study demonstrates the efficacy
of deep learning and hybrid frameworks in addressing biodiversity challenges
and advancing species detection in marine environments.

</details>


### [38] [Try Harder: Hard Sample Generation and Learning for Clothes-Changing Person Re-ID](https://arxiv.org/abs/2507.11119)
*Hankun Liu,Yujian Zhao,Guanglin Niu*

Main category: cs.CV

TL;DR: 提出了一个用于衣物变更环境下行人重识别的全新模型框架HSGL，主要通过多模态技术生成和优化难样本，并取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 解决衣物变化行人重识别中的难样本问题，这些问题因为模糊与相似性导致模型鲁棒性和学习策略设计的受限。

Method: 引入多模态引导的难样本生成与学习框架HSGL，包含两个核心组成：(1)双粒度难样本生成(DGHSG)：利用多模态线索生成语义一致的难样本；(2)难样本自适应学习(HSAL)：采用基于文字语义标签的优化策略。

Result: 在PRCC和LTCC数据集上达到了最新的最优性能（state-of-the-art），并显著加快了目标学习过程的收敛。

Conclusion: 多模态结合难样本生成与学习的方法能够有效提升衣物变更环境下行人重识别的鲁棒性与区分能力。

Abstract: Hard samples pose a significant challenge in person re-identification (ReID)
tasks, particularly in clothing-changing person Re-ID (CC-ReID). Their inherent
ambiguity or similarity, coupled with the lack of explicit definitions, makes
them a fundamental bottleneck. These issues not only limit the design of
targeted learning strategies but also diminish the model's robustness under
clothing or viewpoint changes. In this paper, we propose a novel
multimodal-guided Hard Sample Generation and Learning (HSGL) framework, which
is the first effort to unify textual and visual modalities to explicitly
define, generate, and optimize hard samples within a unified paradigm. HSGL
comprises two core components: (1) Dual-Granularity Hard Sample Generation
(DGHSG), which leverages multimodal cues to synthesize semantically consistent
samples, including both coarse- and fine-grained hard positives and negatives
for effectively increasing the hardness and diversity of the training data. (2)
Hard Sample Adaptive Learning (HSAL), which introduces a hardness-aware
optimization strategy that adjusts feature distances based on textual semantic
labels, encouraging the separation of hard positives and drawing hard negatives
closer in the embedding space to enhance the model's discriminative capability
and robustness to hard samples. Extensive experiments on multiple CC-ReID
benchmarks demonstrate the effectiveness of our approach and highlight the
potential of multimodal-guided hard sample generation and learning for robust
CC-ReID. Notably, HSAL significantly accelerates the convergence of the
targeted learning procedure and achieves state-of-the-art performance on both
PRCC and LTCC datasets. The code is available at
https://github.com/undooo/TryHarder-ACMMM25.

</details>


### [39] [MMOne: Representing Multiple Modalities in One Scene](https://arxiv.org/abs/2507.11129)
*Zhifeng Gu,Bing Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为MMOne的框架，用于多模态场景表示，能解决模态冲突并扩展至更多模态。


<details>
  <summary>Details</summary>
Motivation: 设计一个通用的多模态场景表示框架，以解决模态冲突带来的属性和粒度差异问题。

Method: 提出了模态建模模块和模态分解机制，通过模态指示器捕捉各模态特性，并将多模态信息分解为共享和特定模态成分。

Result: 实验表明，该方法提升了每种模态的表示能力，并具有可扩展性。

Conclusion: MMOne框架提供了更紧凑、高效的多模态场景表示，代码已开源。

Abstract: Humans perceive the world through multimodal cues to understand and interact
with the environment. Learning a scene representation for multiple modalities
enhances comprehension of the physical world. However, modality conflicts,
arising from inherent distinctions among different modalities, present two
critical challenges: property disparity and granularity disparity. To address
these challenges, we propose a general framework, MMOne, to represent multiple
modalities in one scene, which can be readily extended to additional
modalities. Specifically, a modality modeling module with a novel modality
indicator is proposed to capture the unique properties of each modality.
Additionally, we design a multimodal decomposition mechanism to separate
multi-modal Gaussians into single-modal Gaussians based on modality
differences. We address the essential distinctions among modalities by
disentangling multimodal information into shared and modality-specific
components, resulting in a more compact and efficient multimodal scene
representation. Extensive experiments demonstrate that our method consistently
enhances the representation capability for each modality and is scalable to
additional modalities. The code is available at
https://github.com/Neal2020GitHub/MMOne.

</details>


### [40] [RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images](https://arxiv.org/abs/2507.11143)
*Lam Pham,Cam Le,Hieu Tang,Khang Truong,Truong Nguyen,Jasmin Lampert,Alexander Schindler,Martin Boyer,Son Phan*

Main category: cs.CV

TL;DR: 本文提出一个基于深度学习的模型，通过遥感图像实现滑坡灾害的自动监测，实验结果显示该模型在多个数据集上的性能出色。


<details>
  <summary>Details</summary>
Motivation: 频发的极端天气和人类活动导致滑坡灾害频繁发生，而大范围和地形复杂的区域给自动监测滑坡带来挑战。

Method: 提出了一种端到端的深度学习模型，结合遥感图像作为输入数据，设计了一种新型神经网络架构，同时用于滑坡检测和分割两项任务。

Result: 在LandSlide4Sense, Bijie, 和 Nepal三个数据集上进行了实验，滑坡检测任务的F1分数分别达到了98.23和93.83，分割任务的mIoU分数分别达到了63.74和76.88。

Conclusion: 实验结果表明，提出的模型在实际滑坡监测系统中具有潜在应用价值。

Abstract: In recent years, landslide disasters have reported frequently due to the
extreme weather events of droughts, floods , storms, or the consequence of
human activities such as deforestation, excessive exploitation of natural
resources. However, automatically observing landslide is challenging due to the
extremely large observing area and the rugged topography such as mountain or
highland. This motivates us to propose an end-to-end deep-learning-based model
which explores the remote sensing images for automatically observing landslide
events. By considering remote sensing images as the input data, we can obtain
free resource, observe large and rough terrains by time. To explore the remote
sensing images, we proposed a novel neural network architecture which is for
two tasks of landslide detection and landslide segmentation. We evaluated our
proposed model on three different benchmark datasets of LandSlide4Sense, Bijie,
and Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23,
93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU
scores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense,
Nepal datasets. These experimental results prove potential to integrate our
proposed model into real-life landslide observation systems.

</details>


### [41] [Assessing Color Vision Test in Large Vision-language Models](https://arxiv.org/abs/2507.11153)
*Hongfei Ye,Bin Chen,Wenxi Liu,Yu Zhang,Zhao Li,Dandan Ni,Hongyang Chen*

Main category: cs.CV

TL;DR: 本文研究了大型视觉语言模型的色觉能力，并通过构建数据集和优化方法改进其表现。


<details>
  <summary>Details</summary>
Motivation: 检测和提升大型视觉语言模型在色觉能力方面的性能。

Method: 定义色觉测试任务，构建覆盖多种问题类别及难度的数据集，分析模型错误类型并提出微调策略。

Result: 分析了模型的错误模式，并通过微调提高了其在色觉测试中的表现。

Conclusion: 初步验证了优化方法对提高大型视觉语言模型色觉能力的有效性。

Abstract: With the widespread adoption of large vision-language models, the capacity
for color vision in these models is crucial. However, the color vision
abilities of large visual-language models have not yet been thoroughly
explored. To address this gap, we define a color vision testing task for large
vision-language models and construct a dataset \footnote{Anonymous Github
Showing some of the data
https://anonymous.4open.science/r/color-vision-test-dataset-3BCD} that covers
multiple categories of test questions and tasks of varying difficulty levels.
Furthermore, we analyze the types of errors made by large vision-language
models and propose fine-tuning strategies to enhance their performance in color
vision tests.

</details>


### [42] [Clustering-Guided Multi-Layer Contrastive Representation Learning for Citrus Disease Classification](https://arxiv.org/abs/2507.11171)
*Jun Chen,Yonghua Yu,Weifu Li,Yaohui Chen,Hong Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种名为CMCRL的自监督多层对比表示学习算法，用于柑橘病害检测与分类，提升了性能并解决了样本标签依赖问题。


<details>
  <summary>Details</summary>
Motivation: 柑橘病害严重影响产量，现有基于深度学习的检测方法对高质量标注样本依赖较大。提出无需大量标注样本的算法，可精准检测病害并减小标注依赖。

Method: 提出了结合聚类中心对比和多层对比训练（MCT）范式的CMCRL算法，实现无监督学习样本的层次特征表示，适应病害症状的相似性。

Result: 在公共柑橘数据集CDD上表现出色，实现了4.5%-30.1%的准确率提升，缩小了与全监督方法的差距，并在F1评分、精准率等指标上表现优异。

Conclusion: CMCRL算法既提高了分类精度，又展现出应对类别不平衡的强大鲁棒性，为柑橘病害检测提供了新的思路。

Abstract: Citrus, as one of the most economically important fruit crops globally,
suffers severe yield depressions due to various diseases. Accurate disease
detection and classification serve as critical prerequisites for implementing
targeted control measures. Recent advancements in artificial intelligence,
particularly deep learning-based computer vision algorithms, have substantially
decreased time and labor requirements while maintaining the accuracy of
detection and classification. Nevertheless, these methods predominantly rely on
massive, high-quality annotated training examples to attain promising
performance. By introducing two key designs: contrasting with cluster centroids
and a multi-layer contrastive training (MCT) paradigm, this paper proposes a
novel clustering-guided self-supervised multi-layer contrastive representation
learning (CMCRL) algorithm. The proposed method demonstrates several advantages
over existing counterparts: (1) optimizing with massive unannotated samples;
(2) effective adaptation to the symptom similarity across distinct citrus
diseases; (3) hierarchical feature representation learning. The proposed method
achieves state-of-the-art performance on the public citrus image set CDD,
outperforming existing methods by 4.5\%-30.1\% accuracy. Remarkably, our method
narrows the performance gap with fully supervised counterparts (all samples are
labeled). Beyond classification accuracy, our method shows great performance on
other evaluation metrics (F1 score, precision, and recall), highlighting the
robustness against the class imbalance challenge.

</details>


### [43] [How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study](https://arxiv.org/abs/2507.11200)
*Che Liu,Jiazhen Pan,Weixiang Shen,Wenjia Bai,Daniel Rueckert,Rossella Arcucci*

Main category: cs.CV

TL;DR: 本文评估了大型视觉-语言模型（VLMs）在医疗任务中的表现，发现其在理解方面优于推理，但距离临床部署仍有差距。


<details>
  <summary>Details</summary>
Motivation: 探讨大型通用和医疗专用VLMs在医疗任务中的能力以及其适用性。

Method: 对多个开源VLMs进行性能评估，涵盖八个基准任务，将表现分为理解和推理两部分进行分析。

Result: 1）通用模型在某些基准上已超越医疗专用模型；2）推理性能远低于理解性能；3）性能随任务设计和需求有所不同。

Conclusion: 当前模型尚未达到临床可靠性要求，需要更强的多模态对齐和更严格的评估方法。

Abstract: Vision-Language Models (VLMs) trained on web-scale corpora excel at natural
image tasks and are increasingly repurposed for healthcare; however, their
competence in medical tasks remains underexplored. We present a comprehensive
evaluation of open-source general-purpose and medically specialised VLMs,
ranging from 3B to 72B parameters, across eight benchmarks: MedXpert,
OmniMedVQA, PMC-VQA, PathVQA, MMMU, SLAKE, and VQA-RAD. To observe model
performance across different aspects, we first separate it into understanding
and reasoning components. Three salient findings emerge. First, large
general-purpose models already match or surpass medical-specific counterparts
on several benchmarks, demonstrating strong zero-shot transfer from natural to
medical images. Second, reasoning performance is consistently lower than
understanding, highlighting a critical barrier to safe decision support. Third,
performance varies widely across benchmarks, reflecting differences in task
design, annotation quality, and knowledge demands. No model yet reaches the
reliability threshold for clinical deployment, underscoring the need for
stronger multimodal alignment and more rigorous, fine-grained evaluation
protocols.

</details>


### [44] [A Robust Incomplete Multimodal Low-Rank Adaptation Approach for Emotion Recognition](https://arxiv.org/abs/2507.11202)
*Xinkui Zhao,Jinsong Shu,Yangyang Wu,Guanjie Cheng,Zihe Liu,Naibo Wang,Shuiguang Deng,Zhongle Xie,Jianwei Yin*

Main category: cs.CV

TL;DR: 本文提出了一种名为MCULoRA的新方法，用于解决非完整多模态情感识别，其包含MCLA与DPFT两模块，能够有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 在多模态情感识别中，受传感器故障或隐私保护限制，常面临非完整多模态问题。现有方法未解决不同模态组合训练梯度冲突，影响模型预测性能。

Method: 提出MCULoRA方法，由两个模块组成：MCLA模块分离共享信息与不同模态特性；DPFT模块根据模态分离度调整训练比率，实现高效学习。

Result: 通过在多个基准数据集上的实验，证明MCULoRA在下游任务准确率上显著优于以往方法。

Conclusion: MCULoRA是一种高效参数训练框架，能有效解决非完整多模态学习问题，提升任务性能。

Abstract: Multimodal Emotion Recognition (MER) often encounters incomplete
multimodality in practical applications due to sensor failures or privacy
protection requirements. While existing methods attempt to address various
incomplete multimodal scenarios by balancing the training of each modality
combination through additional gradients, these approaches face a critical
limitation: training gradients from different modality combinations conflict
with each other, ultimately degrading the performance of the final prediction
model. In this paper, we propose a unimodal decoupled dynamic low-rank
adaptation method based on modality combinations, named MCULoRA, which is a
novel framework for the parameter-efficient training of incomplete multimodal
learning models. MCULoRA consists of two key modules, modality combination
aware low-rank adaptation (MCLA) and dynamic parameter fine-tuning (DPFT). The
MCLA module effectively decouples the shared information from the distinct
characteristics of individual modality combinations. The DPFT module adjusts
the training ratio of modality combinations based on the separability of each
modality's representation space, optimizing the learning efficiency across
different modality combinations. Our extensive experimental evaluation in
multiple benchmark datasets demonstrates that MCULoRA substantially outperforms
previous incomplete multimodal learning approaches in downstream task accuracy.

</details>


### [45] [NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models](https://arxiv.org/abs/2507.11245)
*X. Feng,H. Yu,M. Wu,S. Hu,J. Chen,C. Zhu,J. Wu,X. Chu,K. Huang*

Main category: cs.CV

TL;DR: 提出了一种名为NarrLV的评估基准，专注于长视频生成模型的叙事表达能力，并设计了相关评估指标和方法。


<details>
  <summary>Details</summary>
Motivation: 当前长视频生成技术缺乏针对其叙事表达能力的专项评估基准；现有评估主要使用简单叙事提示，不能满足更复杂的需求。

Method: 引入时间叙事原子（TNA）概念，构建可扩展提示生成管道；基于TNA变化的三大叙事元素，设计MLLM的问答评估框架；对多个模型进行评测。

Result: 实验结果表明，所提指标与人工评价高度一致，同时揭示当前模型在叙事表达能力中的局限性。

Conclusion: NarrLV提供了一个全面、有效的框架来评估长视频生成模型的叙事表达能力，为后续研究奠定了基础。

Abstract: With the rapid development of foundation video generation technologies, long
video generation models have exhibited promising research potential thanks to
expanded content creation space. Recent studies reveal that the goal of long
video generation tasks is not only to extend video duration but also to
accurately express richer narrative content within longer videos. However, due
to the lack of evaluation benchmarks specifically designed for long video
generation models, the current assessment of these models primarily relies on
benchmarks with simple narrative prompts (e.g., VBench). To the best of our
knowledge, our proposed NarrLV is the first benchmark to comprehensively
evaluate the Narrative expression capabilities of Long Video generation models.
Inspired by film narrative theory, (i) we first introduce the basic narrative
unit maintaining continuous visual presentation in videos as Temporal Narrative
Atom (TNA), and use its count to quantitatively measure narrative richness.
Guided by three key film narrative elements influencing TNA changes, we
construct an automatic prompt generation pipeline capable of producing
evaluation prompts with a flexibly expandable number of TNAs. (ii) Then, based
on the three progressive levels of narrative content expression, we design an
effective evaluation metric using the MLLM-based question generation and
answering framework. (iii) Finally, we conduct extensive evaluations on
existing long video generation models and the foundation generation models.
Experimental results demonstrate that our metric aligns closely with human
judgments. The derived evaluation outcomes reveal the detailed capability
boundaries of current video generation models in narrative content expression.

</details>


### [46] [Fairness-Aware Grouping for Continuous Sensitive Variables: Application for Debiasing Face Analysis with respect to Skin Tone](https://arxiv.org/abs/2507.11247)
*Veronika Shilova,Emmanuel Malherbe,Giovanni Palma,Laurent Risser,Jean-Michel Loubes*

Main category: cs.CV

TL;DR: 提出一种针对连续敏感属性的不公平检测和分组方法，能更细致发现歧视模式并改进数据模型的公平性且不大影响准确性。


<details>
  <summary>Details</summary>
Motivation: 现有公平性评估方法在处理连续敏感属性（如肤色）时容易忽视部分少数群体的歧视现象。

Method: 通过基于歧视的分组选取方法，将数据分组并优化一种新颖的基于组间歧视差异的标准，发现关键的歧视子群。

Result: 在合成数据集、CelebA和FFHQ上验证了方法的稳健性，尤其是肤色方面的歧视分组模型能揭示更细致的歧视模式并保持跨数据集的稳定性。同时这一分组还可用于后处理去偏操作。

Conclusion: 提出的分组方法不仅有助于更细致地理解敏感属性空间中的歧视，也可以提升公平性指标，同时对模型准确性影响较小，具有工业落地潜力。

Abstract: Within a legal framework, fairness in datasets and models is typically
assessed by dividing observations into predefined groups and then computing
fairness measures (e.g., Disparate Impact or Equality of Odds with respect to
gender). However, when sensitive attributes such as skin color are continuous,
dividing into default groups may overlook or obscure the discrimination
experienced by certain minority subpopulations. To address this limitation, we
propose a fairness-based grouping approach for continuous (possibly
multidimensional) sensitive attributes. By grouping data according to observed
levels of discrimination, our method identifies the partition that maximizes a
novel criterion based on inter-group variance in discrimination, thereby
isolating the most critical subgroups.
  We validate the proposed approach using multiple synthetic datasets and
demonstrate its robustness under changing population distributions - revealing
how discrimination is manifested within the space of sensitive attributes.
Furthermore, we examine a specialized setting of monotonic fairness for the
case of skin color. Our empirical results on both CelebA and FFHQ, leveraging
the skin tone as predicted by an industrial proprietary algorithm, show that
the proposed segmentation uncovers more nuanced patterns of discrimination than
previously reported, and that these findings remain stable across datasets for
a given model. Finally, we leverage our grouping model for debiasing purpose,
aiming at predicting fair scores with group-by-group post-processing. The
results demonstrate that our approach improves fairness while having minimal
impact on accuracy, thus confirming our partition method and opening the door
for industrial deployment.

</details>


### [47] [MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection](https://arxiv.org/abs/2507.11252)
*Guanghao Wu,Chen Xu,Hai Song,Chong Wang,Qixing Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种综合性框架，用于生成森林火灾烟雾图像，结合了掩码特征、随机差异损失以及大语言模型过滤工具以提高图像质量，改善用于烟雾检测的合成数据集。


<details>
  <summary>Details</summary>
Motivation: 当前森林火灾的图像烟雾检测受到图像数据稀缺的限制，现有的图像生成模型在生成高质量烟雾图像上仍存在背景不一致的问题。

Method: 提出了一种新型网络架构，结合预训练分割模型与多模态模型生成掩码和图像描述，利用掩码和隐藏图像特征的新网络结构，并引入一种新的掩码随机差异损失函数，同时结合大语言模型对生成图像进行多样性和合理性筛选。

Result: 实验表明，生成的烟雾图像真实且多样性高，并有效提升了森林火灾烟雾检测模型的性能。

Conclusion: 通过生成和筛选高质量烟雾图像，本文的框架显著提升了森林火灾烟雾检测的效果，为合成数据集的构建提供了新方法。

Abstract: Smoke is the first visible indicator of a wildfire.With the advancement of
deep learning, image-based smoke detection has become a crucial method for
detecting and preventing forest fires. However, the scarcity of smoke image
data from forest fires is one of the significant factors hindering the
detection of forest fire smoke. Image generation models offer a promising
solution for synthesizing realistic smoke images. However, current inpainting
models exhibit limitations in generating high-quality smoke representations,
particularly manifesting as inconsistencies between synthesized smoke and
background contexts. To solve these problems, we proposed a comprehensive
framework for generating forest fire smoke images. Firstly, we employed the
pre-trained segmentation model and the multimodal model to obtain smoke masks
and image captions.Then, to address the insufficient utilization of masks and
masked images by inpainting models, we introduced a network architecture guided
by mask and masked image features. We also proposed a new loss function, the
mask random difference loss, which enhances the consistency of the generated
effects around the mask by randomly expanding and eroding the mask
edges.Finally, to generate a smoke image dataset using random masks for
subsequent detection tasks, we incorporated smoke characteristics and use a
multimodal large language model as a filtering tool to select diverse and
reasonable smoke images, thereby improving the quality of the synthetic
dataset. Experiments showed that our generated smoke images are realistic and
diverse, and effectively enhance the performance of forest fire smoke detection
models. Code is available at https://github.com/wghr123/MFGDiffusion.

</details>


### [48] [ViewSRD: 3D Visual Grounding via Structured Multi-View Decomposition](https://arxiv.org/abs/2507.11261)
*Ronggang Huang,Haoxin Yang,Yan Cai,Xuemiao Xu,Huaidong Zhang,Shengfeng He*

Main category: cs.CV

TL;DR: 提出了一种叫ViewSRD的方法，通过结构化多视角解构过程改进3D视觉定位，尤其在复杂查询情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在复杂多锚点查询中区分目标与锚点，并解决由视角变化引起的空间描述不一致问题。

Method: 该方法包括简单关系解耦模块（SRD）将复杂查询转化为单一目标的陈述，生成具有视角感知的描述；多视角文本场景交互模块（Multi-TSI）通过共享的跨模态一致视图令牌（CCVTs）保留空间关联；最终通过文本场景推理模块整合多视角预测实现统一3D视觉定位。

Result: 实验表明，在需要精确空间区分的复杂查询中，ViewSRD显著优于当前最先进方法。

Conclusion: 通过引入结构化多视图解构和跨模态一致性方法，ViewSRD能够更好地处理3D视觉定位中的复杂空间关系问题，并提升精度。

Abstract: 3D visual grounding aims to identify and localize objects in a 3D space based
on textual descriptions. However, existing methods struggle with disentangling
targets from anchors in complex multi-anchor queries and resolving
inconsistencies in spatial descriptions caused by perspective variations. To
tackle these challenges, we propose ViewSRD, a framework that formulates 3D
visual grounding as a structured multi-view decomposition process. First, the
Simple Relation Decoupling (SRD) module restructures complex multi-anchor
queries into a set of targeted single-anchor statements, generating a
structured set of perspective-aware descriptions that clarify positional
relationships. These decomposed representations serve as the foundation for the
Multi-view Textual-Scene Interaction (Multi-TSI) module, which integrates
textual and scene features across multiple viewpoints using shared, Cross-modal
Consistent View Tokens (CCVTs) to preserve spatial correlations. Finally, a
Textual-Scene Reasoning module synthesizes multi-view predictions into a
unified and robust 3D visual grounding. Experiments on 3D visual grounding
datasets show that ViewSRD significantly outperforms state-of-the-art methods,
particularly in complex queries requiring precise spatial differentiation.

</details>


### [49] [YOLOatr : Deep Learning Based Automatic Target Detection and Localization in Thermal Infrared Imagery](https://arxiv.org/abs/2507.11267)
*Aon Safdar,Usman Akram,Waseem Anwar,Basit Malik,Mian Ibad Ali*

Main category: cs.CV

TL;DR: 本文提出了一种名为YOLOatr的改进单阶段检测器，用于提高热红外图像中的目标自动检测与识别（ATR）精度，达到了99.6%的表现。


<details>
  <summary>Details</summary>
Motivation: 由于热红外图像数据少，加上硬件限制、尺寸不变性、遮挡、低分辨率及环境变化等多种因素，当前最先进的深度学习架构在防御和监视领域的自动目标识别中表现欠佳，因此有必要开发改进模型。

Method: 本文基于YOLOv5s，对检测头、特征融合模块以及数据增强策略进行了优化，提出了一种名为YOLOatr的单阶段检测器，并在DSIAC中波红外数据集上进行了性能评估。

Result: 实验结果表明，YOLOatr在相关与非相关测试协议中均表现出色，实现了高达99.6%的自动目标识别性能。

Conclusion: YOLOatr在热红外图像的自动目标检测与识别任务中表现出色，克服了传统深度学习模型的不足，为实现实时高精度ATR提供了可行方案。

Abstract: Automatic Target Detection (ATD) and Recognition (ATR) from Thermal Infrared
(TI) imagery in the defense and surveillance domain is a challenging computer
vision (CV) task in comparison to the commercial autonomous vehicle perception
domain. Limited datasets, peculiar domain-specific and TI modality-specific
challenges, i.e., limited hardware, scale invariance issues due to greater
distances, deliberate occlusion by tactical vehicles, lower sensor resolution
and resultant lack of structural information in targets, effects of weather,
temperature, and time of day variations, and varying target to clutter ratios
all result in increased intra-class variability and higher inter-class
similarity, making accurate real-time ATR a challenging CV task. Resultantly,
contemporary state-of-the-art (SOTA) deep learning architectures underperform
in the ATR domain. We propose a modified anchor-based single-stage detector,
called YOLOatr, based on a modified YOLOv5s, with optimal modifications to the
detection heads, feature fusion in the neck, and a custom augmentation profile.
We evaluate the performance of our proposed model on a comprehensive DSIAC MWIR
dataset for real-time ATR over both correlated and decorrelated testing
protocols. The results demonstrate that our proposed model achieves
state-of-the-art ATR performance of up to 99.6%.

</details>


### [50] [Tomato Multi-Angle Multi-Pose Dataset for Fine-Grained Phenotyping](https://arxiv.org/abs/2507.11279)
*Yujie Zhang,Sabine Struckmeyer,Andreas Kolb,Sven Reichardt*

Main category: cs.CV

TL;DR: 本文介绍了一种名为TomatoMAP的全面数据集，结合物联网(IoT)成像系统及标准化数据采集协议，用于解决传统植物表型测定中存在的观察偏差和不一致问题。


<details>
  <summary>Details</summary>
Motivation: 传统植物表型测定方法存在观察偏差及不一致问题，影响了精细化植物分析的准确性及可重复性。本研究旨在通过开发一个准确、高效的数据集来克服这些问题。

Method: 开发了TomatoMAP数据集，通过物联网(IoT)成像系统采集12种植物姿态在不同角度的RGB图像，并为每张图像手动标注感兴趣区域与生长阶段。此外，提供高分辨率图像用于语义和实例分割验证。

Result: 使用深度学习框架(如MobileNetv3、YOLOv11及MaskRCNN)在该数据集上进行验证，结果表明模型的准确性和速度与专家相当，且通过Cohen's Kappa和一致性热图验证了自动精细表型分析的可靠性。

Conclusion: 基于此数据集的自动化测定可为植物表型分析提供高效且可靠的解决方案，克服了传统方法中的主要不足。

Abstract: Observer bias and inconsistencies in traditional plant phenotyping methods
limit the accuracy and reproducibility of fine-grained plant analysis. To
overcome these challenges, we developed TomatoMAP, a comprehensive dataset for
Solanum lycopersicum using an Internet of Things (IoT) based imaging system
with standardized data acquisition protocols. Our dataset contains 64,464 RGB
images that capture 12 different plant poses from four camera elevation angles.
Each image includes manually annotated bounding boxes for seven regions of
interest (ROIs), including leaves, panicle, batch of flowers, batch of fruits,
axillary shoot, shoot and whole plant area, along with 50 fine-grained growth
stage classifications based on the BBCH scale. Additionally, we provide 3,616
high-resolution image subset with pixel-wise semantic and instance segmentation
annotations for fine-grained phenotyping. We validated our dataset using a
cascading model deep learning framework combining MobileNetv3 for
classification, YOLOv11 for object detection, and MaskRCNN for segmentation.
Through AI vs. Human analysis involving five domain experts, we demonstrate
that the models trained on our dataset achieve accuracy and speed comparable to
the experts. Cohen's Kappa and inter-rater agreement heatmap confirm the
reliability of automated fine-grained phenotyping using our approach.

</details>


### [51] [Task-Oriented Human Grasp Synthesis via Context- and Task-Aware Diffusers](https://arxiv.org/abs/2507.11287)
*An-Lun Liu,Yu-Wei Chao,Yi-Ting Chen*

Main category: cs.CV

TL;DR: 研究了一种新的任务导向人类抓取合成任务，结合场景和任务信息提出任务感知接触图，用于合成任务导向的手抓动作。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏任务和场景感知能力，无法充分满足任务导向抓取的要求。

Method: 提出了一种两阶段流水线，第一阶段生成任务感知接触图，第二阶段基于接触图合成任务导向抓取。

Result: 证明了考虑场景和任务的重要性，在抓取质量和任务表现上显著优于现有方法。

Conclusion: 结合任务和场景信息提升抓取质量，有希望改善任务完成效果。

Abstract: In this paper, we study task-oriented human grasp synthesis, a new grasp
synthesis task that demands both task and context awareness. At the core of our
method is the task-aware contact maps. Unlike traditional contact maps that
only reason about the manipulated object and its relation with the hand, our
enhanced maps take into account scene and task information. This comprehensive
map is critical for hand-object interaction, enabling accurate grasping poses
that align with the task. We propose a two-stage pipeline that first constructs
a task-aware contact map informed by the scene and task. In the subsequent
stage, we use this contact map to synthesize task-oriented human grasps. We
introduce a new dataset and a metric for the proposed task to evaluate our
approach. Our experiments validate the importance of modeling both scene and
task, demonstrating significant improvements over existing methods in both
grasp quality and task performance. See our project page for more details:
https://hcis-lab.github.io/TOHGS/

</details>


### [52] [Detección y Cuantificación de Erosión Fluvial con Visión Artificial](https://arxiv.org/abs/2507.11301)
*Paúl Maji,Marlon Túquerres,Stalin Valencia,Marcela Valenzuela,Christian Mejia-Escobar*

Main category: cs.CV

TL;DR: 研究使用YOLOv11模型，以照片和LiDAR图像为数据，开发了EROSCAN系统，用于自动识别水流侵蚀区域及面积。


<details>
  <summary>Details</summary>
Motivation: 水流侵蚀对土壤稳定性和基础设施有重要影响，现有的方法需要高强度的人工操作及专业知识。

Method: 通过微调YOLOv11模型，并结合照片和LiDAR图像训练，利用Roboflow平台进行数据标注和分割。

Result: 实验表明侵蚀区域的检测准确率达到70%，能够精确识别和量化侵蚀面积。

Conclusion: 开发了EROSCAN系统，用户可通过上传图片自动获取侵蚀区域和面积测算，从而提高风险管理和规划决策效率。

Abstract: Fluvial erosion is a natural process that can generate significant impacts on
soil stability and strategic infrastructures. The detection and monitoring of
this phenomenon is traditionally addressed by photogrammetric methods and
analysis in geographic information systems. These tasks require specific
knowledge and intensive manual processing. This study proposes an artificial
intelligence-based approach for automatic identification of eroded zones and
estimation of their area. The state-of-the-art computer vision model YOLOv11,
adjusted by fine-tuning and trained with photographs and LiDAR images, is used.
This combined dataset was segmented and labeled using the Roboflow platform.
Experimental results indicate efficient detection of erosion patterns with an
accuracy of 70%, precise identification of eroded areas and reliable
calculation of their extent in pixels and square meters. As a final product,
the EROSCAN system has been developed, an interactive web application that
allows users to upload images and obtain automatic segmentations of fluvial
erosion, together with the estimated area. This tool optimizes the detection
and quantification of the phenomenon, facilitating decision making in risk
management and territorial planning.

</details>


### [53] [A Mixed-Primitive-based Gaussian Splatting Method for Surface Reconstruction](https://arxiv.org/abs/2507.11321)
*Haoxuan Qu,Yujun Cai,Hossein Rahmani,Ajay Kumar,Junsong Yuan,Jun Liu*

Main category: cs.CV

TL;DR: 提出了一种新型框架，让高斯散点首次能在表面重构过程中整合多种（几何）元素。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯散点的方法仅使用单一类型的散点元素，无法高质量地表示复杂多样的三维物体表面。

Method: 提出了组合散点策略、混合元素初始化策略和顶点剪枝机制，以支持多种类型元素在表面重构中的应用。

Result: 广泛实验验证了框架的有效性及其在表面重构中的高精度表现。

Conclusion: 框架显著提升了表面表示能力，是高斯散点方法的重要改进。

Abstract: Recently, Gaussian Splatting (GS) has received a lot of attention in surface
reconstruction. However, while 3D objects can be of complex and diverse shapes
in the real world, existing GS-based methods only limitedly use a single type
of splatting primitive (Gaussian ellipse or Gaussian ellipsoid) to represent
object surfaces during their reconstruction. In this paper, we highlight that
this can be insufficient for object surfaces to be represented in high quality.
Thus, we propose a novel framework that, for the first time, enables Gaussian
Splatting to incorporate multiple types of (geometrical) primitives during its
surface reconstruction process. Specifically, in our framework, we first
propose a compositional splatting strategy, enabling the splatting and
rendering of different types of primitives in the Gaussian Splatting pipeline.
In addition, we also design our framework with a mixed-primitive-based
initialization strategy and a vertex pruning mechanism to further promote its
surface representation learning process to be well executed leveraging
different types of primitives. Extensive experiments show the efficacy of our
framework and its accurate surface reconstruction performance.

</details>


### [54] [MonoMVSNet: Monocular Priors Guided Multi-View Stereo Network](https://arxiv.org/abs/2507.11333)
*Jianfei Jiang,Qiankun Liu,Haochen Yu,Hongyuan Liu,Liyong Wang,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: 作者提出了MonoMVSNet，结合单目深度估计和多视图几何以改进深度预测，特别是在纹理缺失和反射面等挑战区域表现优秀。


<details>
  <summary>Details</summary>
Motivation: 现有MVS方法在纹理缺失区域和反射面表现不佳，而单目深度估计在这些区域能够实现稳健的相对深度估计。本研究旨在结合单目方法的优势解决这一问题。

Method: 提出MonoMVSNet网络：1.通过注意力机制与设计的新型跨视图位置编码，将参考视图单目特征与源视图特征整合；2.使用参考视图的单目深度对采样过程中深度候选值进行动态更新；3.设计基于单目深度的相对一致性损失以监督深度预测。

Result: 实验表明，MonoMVSNet在DTU和Tanks-and-Temples数据集上表现优异，并在Tanks-and-Temples中级和高级基准上排名第一。

Conclusion: MonoMVSNet方法通过整合单目模型的强大先验和多视图几何，在解决MVS任务中的挑战性问题上取得了领先表现。

Abstract: Learning-based Multi-View Stereo (MVS) methods aim to predict depth maps for
a sequence of calibrated images to recover dense point clouds. However,
existing MVS methods often struggle with challenging regions, such as
textureless regions and reflective surfaces, where feature matching fails. In
contrast, monocular depth estimation inherently does not require feature
matching, allowing it to achieve robust relative depth estimation in these
regions. To bridge this gap, we propose MonoMVSNet, a novel monocular feature
and depth guided MVS network that integrates powerful priors from a monocular
foundation model into multi-view geometry. Firstly, the monocular feature of
the reference view is integrated into source view features by the attention
mechanism with a newly designed cross-view position encoding. Then, the
monocular depth of the reference view is aligned to dynamically update the
depth candidates for edge regions during the sampling procedure. Finally, a
relative consistency loss is further designed based on the monocular depth to
supervise the depth prediction. Extensive experiments demonstrate that
MonoMVSNet achieves state-of-the-art performance on the DTU and
Tanks-and-Temples datasets, ranking first on the Tanks-and-Temples Intermediate
and Advanced benchmarks. The source code is available at
https://github.com/JianfeiJ/MonoMVSNet.

</details>


### [55] [UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks](https://arxiv.org/abs/2507.11336)
*Peiran Wu,Yunze Liu,Zhengdong Zhu,Enmin Zhou,Shawn Shen*

Main category: cs.CV

TL;DR: 本文介绍了一个名为UGC-VideoCap的新基准和模型框架，用于短视频的全模态详细字幕生成，重点结合音频和视觉信息，解决现有数据集和模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的视频字幕生成基准和模型主要关注视觉内容，忽视音频在场景动态、说话者意图和叙事上下文中的作用，因此需要一个能全面结合音频和视觉信息的解决方案。

Method: 构建了一个包含1000个TikTok视频的UGC-VideoCap数据集，通过三阶段人工参与的管道对音频、视觉及两者联合的语义进行标注，并生成4000个针对单模态和跨模态理解的问答对。同时提出了一种具有3B参数的UGC-VideoCaptioner模型，采用新颖的两阶段训练策略，包括有监督微调和群体相对策略优化（GRPO）。

Result: 所提出的方法在全模态视频字幕生成方面具有高质量和高效性，成功地解决了数据有限情况下的适应难题，同时维持了竞争力强的性能。

Conclusion: UGC-VideoCap数据集和UGC-VideoCaptioner模型提供了一个质量高且数据高效的解决方案，为真实世界中用户生成内容（UGC）的全模态视频字幕生成奠定了基础。

Abstract: Real-world user-generated videos, especially on platforms like TikTok, often
feature rich and intertwined audio visual content. However, existing video
captioning benchmarks and models remain predominantly visual centric,
overlooking the crucial role of audio in conveying scene dynamics, speaker
intent, and narrative context. This lack of omni datasets and lightweight,
capable models hampers progress in fine grained, multimodal video
understanding. To address these challenges, we introduce UGC-VideoCap, a new
benchmark and model framework specifically designed for detailed omnimodal
captioning of short form user-generated videos. Unlike prior datasets,
UGC-VideoCap emphasizes balanced integration of audio and visual modalities,
featuring 1000 TikTok videos annotated through a structured three stage
human-in-the-loop pipeline covering audio only, visual only, and joint audio
visual semantics. The benchmark also includes 4000 carefully crafted QA pairs
probing both unimodal and cross modal understanding. Alongside the dataset, we
propose UGC-VideoCaptioner(3B), a 3B parameter captioning model distilled from
Gemini 2.5 Flash. Using a novel two-stage training strategy supervised fine
tuning followed by Group Relative Policy Optimization (GRPO), our approach
enables efficient adaptation from limited data while maintaining competitive
performance. Together, our benchmark and model offer a high-quality foundation
and a data-efficient solution for advancing omnimodal video captioning in
unconstrained real-world UGC settings.

</details>


### [56] [Attributes Shape the Embedding Space of Face Recognition Models](https://arxiv.org/abs/2507.11372)
*Pierrick Leroy,Antonio Mastropietro,Marco Nurisso,Francesco Vaccarino*

Main category: cs.CV

TL;DR: 通过研究表明，面部识别模型受特定属性的影响，并提出度量其依赖性的新方法。


<details>
  <summary>Details</summary>
Motivation: 当前的面部识别任务主要通过深度神经网络实现，而现有方法对图像属性的解释性和依赖性研究较少。

Method: 提出一种基于几何方法的度量方式，结合物理学概念的对齐指标，分析模型在不同面部和图像属性上的表现。

Result: 研究发现模型在不同属性上展现出不同程度的依赖性或不变性，可用于理解模型的强项与弱点。

Conclusion: 通过对面部识别模型的几何属性分析，可以增进对模型的深入解读并指导未来改进方向。

Abstract: Face Recognition (FR) tasks have made significant progress with the advent of
Deep Neural Networks, particularly through margin-based triplet losses that
embed facial images into high-dimensional feature spaces. During training,
these contrastive losses focus exclusively on identity information as labels.
However, we observe a multiscale geometric structure emerging in the embedding
space, influenced by interpretable facial (e.g., hair color) and image
attributes (e.g., contrast). We propose a geometric approach to describe the
dependence or invariance of FR models to these attributes and introduce a
physics-inspired alignment metric. We evaluate the proposed metric on
controlled, simplified models and widely used FR models fine-tuned with
synthetic data for targeted attribute augmentation. Our findings reveal that
the models exhibit varying degrees of invariance across different attributes,
providing insight into their strengths and weaknesses and enabling deeper
interpretability. Code available here:
https://github.com/mantonios107/attrs-fr-embs}{https://github.com/mantonios107/attrs-fr-embs

</details>


### [57] [Implementing Adaptations for Vision AutoRegressive Model](https://arxiv.org/abs/2507.11441)
*Kaif Shaikh,Antoni Kowalczuk,Franziska Boenisch,Adam Dziedzic*

Main category: cs.CV

TL;DR: 论文讨论了VAR在图像生成及其数据隐私相关应用中的表现，特别是与DM的表现比较。


<details>
  <summary>Details</summary>
Motivation: 作者旨在探索如何调整预训练的VAR模型以执行特定任务（如医学数据生成），并研究其数据隐私保护能力。

Method: 对VAR的多种适配策略进行了实现和基准测试，并与先进的DM适配策略进行比较。

Result: VAR在非DP适配性能上优于DM，但在DP适配方面表现较差，结果揭示了VAR在私人数据适配方面的研究需求。

Conclusion: VAR在非隐私适配中表现突出，但DP适配仍需较多的后续研究。

Abstract: Vision AutoRegressive model (VAR) was recently introduced as an alternative
to Diffusion Models (DMs) in image generation domain. In this work we focus on
its adaptations, which aim to fine-tune pre-trained models to perform specific
downstream tasks, like medical data generation. While for DMs there exist many
techniques, adaptations for VAR remain underexplored. Similarly, differentially
private (DP) adaptations-ones that aim to preserve privacy of the adaptation
data-have been extensively studied for DMs, while VAR lacks such solutions. In
our work, we implement and benchmark many strategies for VAR, and compare them
to state-of-the-art DM adaptation strategies. We observe that VAR outperforms
DMs for non-DP adaptations, however, the performance of DP suffers, which
necessitates further research in private adaptations for VAR. Code is available
at https://github.com/sprintml/finetuning_var_dp.

</details>


### [58] [COLI: A Hierarchical Efficient Compressor for Large Images](https://arxiv.org/abs/2507.11443)
*Haoran Wang,Hanyu Pei,Yang Lyu,Kai Zhang,Li Li,Feng-Lei Fan*

Main category: cs.CV

TL;DR: 高分辨率大视场图像的增多增加了高效压缩方法的需求，研究提出了一种名为COLI的新框架，用于解决当前方法的训练速度慢和压缩比低的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前压缩方法要么无法保留图像细节，要么通用性有限。本文使用隐式神经表示（INRs）作为替代，但传统INR方法在大图像上效率不佳，因此需要优化。

Method: 提出了COLI框架，结合使用NeRV方法，通过预训练-微调范式、混合精度训练和并行优化目标提升训练速度，并通过超压缩技术优化压缩比。

Result: 实验表明，COLI在两个医学图像数据集上以更低的bpp达到了更高的PSNR和SSIM，同时将NeRV训练速度提升了4倍。

Conclusion: COLI在压缩效率和视觉质量上都有显著改进，为大图像压缩提供了一种优化解决方案。

Abstract: The escalating adoption of high-resolution, large-field-of-view imagery
amplifies the need for efficient compression methodologies. Conventional
techniques frequently fail to preserve critical image details, while
data-driven approaches exhibit limited generalizability. Implicit Neural
Representations (INRs) present a promising alternative by learning continuous
mappings from spatial coordinates to pixel intensities for individual images,
thereby storing network weights rather than raw pixels and avoiding the
generalization problem. However, INR-based compression of large images faces
challenges including slow compression speed and suboptimal compression ratios.
To address these limitations, we introduce COLI (Compressor for Large Images),
a novel framework leveraging Neural Representations for Videos (NeRV). First,
recognizing that INR-based compression constitutes a training process, we
accelerate its convergence through a pretraining-finetuning paradigm,
mixed-precision training, and reformulation of the sequential loss into a
parallelizable objective. Second, capitalizing on INRs' transformation of image
storage constraints into weight storage, we implement Hyper-Compression, a
novel post-training technique to substantially enhance compression ratios while
maintaining minimal output distortion. Evaluations across two medical imaging
datasets demonstrate that COLI consistently achieves competitive or superior
PSNR and SSIM metrics at significantly reduced bits per pixel (bpp), while
accelerating NeRV training by up to 4 times.

</details>


### [59] [HUG-VAS: A Hierarchical NURBS-Based Generative Model for Aortic Geometry Synthesis and Controllable Editing](https://arxiv.org/abs/2507.11474)
*Pan Du,Mingqi Xu,Xiaozhi Zhu,Jian-xun Wang*

Main category: cs.CV

TL;DR: 提出了一种新模型HUG-VAS，通过NURBS参数化和扩散生成建模生成精细的主动脉几何结构。


<details>
  <summary>Details</summary>
Motivation: 解决传统统计形状建模方法受线性假设限制，难以表达复杂多分支血管结构的问题。

Method: 使用分层架构，其中去噪扩散模型生成中心线，指导扩散模型生成基于中心线的径向轮廓，并结合NURBS参数化。

Result: HUG-VAS生成的主动脉几何结构与原始数据集的生物标志分布高度一致，并支持从图像导出的先验条件生成。

Conclusion: HUG-VAS首次将NURBS参数化与分层扩散过程统一整合，实现了图像先验与生成形状建模的桥接，支持多种实际应用。

Abstract: Accurate characterization of vascular geometry is essential for
cardiovascular diagnosis and treatment planning. Traditional statistical shape
modeling (SSM) methods rely on linear assumptions, limiting their expressivity
and scalability to complex topologies such as multi-branch vascular structures.
We introduce HUG-VAS, a Hierarchical NURBS Generative model for Vascular
geometry Synthesis, which integrates NURBS surface parameterization with
diffusion-based generative modeling to synthesize realistic, fine-grained
aortic geometries. Trained with 21 patient-specific samples, HUG-VAS generates
anatomically faithful aortas with supra-aortic branches, yielding biomarker
distributions that closely match those of the original dataset. HUG-VAS adopts
a hierarchical architecture comprising a denoising diffusion model that
generates centerlines and a guided diffusion model that synthesizes radial
profiles conditioned on those centerlines, thereby capturing two layers of
anatomical variability. Critically, the framework supports zero-shot
conditional generation from image-derived priors, enabling practical
applications such as interactive semi-automatic segmentation, robust
reconstruction under degraded imaging conditions, and implantable device
optimization. To our knowledge, HUG-VAS is the first SSM framework to bridge
image-derived priors with generative shape modeling via a unified integration
of NURBS parameterization and hierarchical diffusion processes.

</details>


### [60] [C-FBI: A Combinatorial method using Convolutions for Circle Fitting in Blurry Images](https://arxiv.org/abs/2507.11476)
*Esteban Román Catafau,Torbjörn E. M. Nordling*

Main category: cs.CV

TL;DR: 本文提出了一种名为3C-FBI的新算法，用于在恶劣成像条件下实现准确的圆检测与拟合，具有高精度和实时性表现。


<details>
  <summary>Details</summary>
Motivation: 解决计算机视觉中在退化成像条件下精确的圆检测及拟合问题。

Method: 融合了组合边缘点取样与基于卷积参数空间密度估计，以桥接圆检测与精确参数拟合之间的差距。

Result: 在帕金森病评估的实际数据、控制的合成数据和系统分析实验中表现出国家最优水平；在Jaccard指数和速度上都显著优于经典方法，具有鲁棒性与高效性。

Conclusion: 3C-FBI在精确度、速度和鲁棒性上的优秀表现，使其成为医学成像、机器人和工业检测的理想选择，特别适用于恶劣环境。

Abstract: This paper addresses the fundamental computer vision challenge of robust
circle detection and fitting in degraded imaging conditions. We present
Combinatorial Convolution-based Circle Fitting for Blurry Images (3C-FBI), an
algorithm that bridges the gap between circle detection and precise parametric
fitting by combining (1) efficient combinatorial edge pixel (edgel) sampling
and (2) convolution-based density estimation in parameter space.
  We evaluate 3C-FBI across three experimental frameworks: (1) real-world
medical data from Parkinson's disease assessments (144 frames from 36 videos),
(2) controlled synthetic data following established circle-fitting benchmarks,
and (3) systematic analysis across varying spatial resolutions and outlier
contamination levels. Results show that 3C-FBI achieves state-of-the-art
accuracy (Jaccard index 0.896) while maintaining real-time performance (40.3
fps), significantly outperforming classical methods like RCD (6.8 fps) on a
standard CPU (i7-10875H). It maintains near-perfect accuracy (Jaccard almost
1.0) at high resolutions (480x480) and reliable performance (Jaccard higher
than 0.95) down to 160x160 with up to 20% outliers.
  In extensive synthetic testing, 3C-FBI achieves a mean Jaccard Index of 0.989
across contamination levels, comparable to modern methods like Qi et al. (2024,
0.991), and surpassing RHT (0.964). This combination of accuracy, speed, and
robustness makes 3C-FBI ideal for medical imaging, robotics, and industrial
inspection under challenging conditions.

</details>


### [61] [COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation](https://arxiv.org/abs/2507.11488)
*Pakizar Shamoi,Nuray Toganas,Muragul Muratbekova,Elnara Kadyrgali,Adilet Yerkin,Ayan Igali,Malika Ziyada,Ayana Adilova,Aron Karatayev,Yerdauit Torekhan*

Main category: cs.CV

TL;DR: 本文引入了一种基于人类感知的模糊色彩模型COLIBRI，旨在弥合计算机色彩表达与人类视觉感知之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有计算机色彩模型难以真实模仿人类的色彩感知，因此需要开发更符合人类视觉感知的表达方法。

Method: 利用模糊集合与逻辑构建彩色分类框架，包括区分实验、人类分类调查及模糊分割生成等多阶段实验，结合反馈适应机制优化模型。

Result: 实验验证表明，与传统色彩模型相比，该模型更能贴近人类的感知效果。

Conclusion: 该研究在基于大样本构建色彩感知模型方面具有开创性，对设计、人工智能等领域有重要意义。

Abstract: Colors are omnipresent in today's world and play a vital role in how humans
perceive and interact with their surroundings. However, it is challenging for
computers to imitate human color perception. This paper introduces the Human
Perception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based
Representation and Interpretation), designed to bridge the gap between
computational color representations and human visual perception. The proposed
model uses fuzzy sets and logic to create a framework for color categorization.
Using a three-phase experimental approach, the study first identifies
distinguishable color stimuli for hue, saturation, and intensity through
preliminary experiments, followed by a large-scale human categorization survey
involving more than 1000 human subjects. The resulting data are used to extract
fuzzy partitions and generate membership functions that reflect real-world
perceptual uncertainty. The model incorporates a mechanism for adaptation that
allows refinement based on feedback and contextual changes. Comparative
evaluations demonstrate the model's alignment with human perception compared to
traditional color models, such as RGB, HSV, and LAB. To the best of our
knowledge, no previous research has documented the construction of a model for
color attribute specification based on a sample of this size or a comparable
sample of the human population (n = 2496). Our findings are significant for
fields such as design, artificial intelligence, marketing, and human-computer
interaction, where perceptually relevant color representation is critical.

</details>


### [62] [CATVis: Context-Aware Thought Visualization](https://arxiv.org/abs/2507.11522)
*Tariq Mehmood,Hamza Ahmad,Muhammad Haroon Shakeel,Murtaza Taj*

Main category: cs.CV

TL;DR: 本文提出了一种5阶段框架，用于从EEG信号中解码视觉表征并生成高质量图像。


<details>
  <summary>Details</summary>
Motivation: 解决由于EEG信号复杂性和噪声问题导致视觉表征解码难题。

Method: 提出了5个阶段：EEG信号编码、与CLIP特征空间中的文本对齐、重新排序优化、概念与文本嵌入插值，以及利用现有Stable Diffusion模型生成图像。

Result: 实验显示，与现有方法相比，该框架在分类准确率、生成准确率和图像质量三个方面分别提升了13.43%、15.21%和36.61%。

Conclusion: 提出的方法在解码EEG信号的视觉表征以及生成上下文相关的图像方面表现出了显著优势。

Abstract: EEG-based brain-computer interfaces (BCIs) have shown promise in various
applications, such as motor imagery and cognitive state monitoring. However,
decoding visual representations from EEG signals remains a significant
challenge due to their complex and noisy nature. We thus propose a novel
5-stage framework for decoding visual representations from EEG signals: (1) an
EEG encoder for concept classification, (2) cross-modal alignment of EEG and
text embeddings in CLIP feature space, (3) caption refinement via re-ranking,
(4) weighted interpolation of concept and caption embeddings for richer
semantics, and (5) image generation using a pre-trained Stable Diffusion model.
We enable context-aware EEG-to-image generation through cross-modal alignment
and re-ranking. Experimental results demonstrate that our method generates
high-quality images aligned with visual stimuli, outperforming SOTA approaches
by 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and
reducing Fr\'echet Inception Distance by 36.61%, indicating superior semantic
alignment and image quality.

</details>


### [63] [CharaConsist: Fine-Grained Consistent Character Generation](https://arxiv.org/abs/2507.11533)
*Mengyu Wang,Henghui Ding,Jianing Peng,Yao Zhao,Yunpeng Chen,Yunchao Wei*

Main category: cs.CV

TL;DR: CharaConsist提出了一种新方法，通过改进注意机制和控制前景背景，一定程度上解决了图像生成中角色一致性问题。


<details>
  <summary>Details</summary>
Motivation: 在文本到图像生成中，一致性关系到生成内容在实际场景中的可用性，现有方法在背景和运动变化前景的一致性保持上存在不足。

Method: 提出CharaConsist结合点跟踪注意机制和自适应标记合并，并引入前景和背景的解耦控制机制。

Result: CharaConsist在一个场景内连续拍摄或跨场景离散拍摄中表现出细粒度前景和背景一致性。

Conclusion: 该方法首次将一致性生成调整为适配文本到图像的DiT模型，实现了高质量、多样的一致性输出，适应更广泛的实际应用需求。

Abstract: In text-to-image generation, producing a series of consistent contents that
preserve the same identity is highly valuable for real-world applications.
Although a few works have explored training-free methods to enhance the
consistency of generated subjects, we observe that they suffer from the
following problems. First, they fail to maintain consistent background details,
which limits their applicability. Furthermore, when the foreground character
undergoes large motion variations, inconsistencies in identity and clothing
details become evident. To address these problems, we propose CharaConsist,
which employs point-tracking attention and adaptive token merge along with
decoupled control of the foreground and background. CharaConsist enables
fine-grained consistency for both foreground and background, supporting the
generation of one character in continuous shots within a fixed scene or in
discrete shots across different scenes. Moreover, CharaConsist is the first
consistent generation method tailored for text-to-image DiT model. Its ability
to maintain fine-grained consistency, combined with the larger capacity of
latest base model, enables it to produce high-quality visual outputs,
broadening its applicability to a wider range of real-world scenarios. The
source code has been released at https://github.com/Murray-Wang/CharaConsist

</details>


### [64] [Streaming 4D Visual Geometry Transformer](https://arxiv.org/abs/2507.11539)
*Dong Zhuo,Wenzhao Zheng,Jiahe Guo,Yuqi Wu,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文提出一种流式4D视觉几何变换器，以视频中感知和重建4D时空几何信息。


<details>
  <summary>Details</summary>
Motivation: 解决实时交互式的4D时空几何重建需求，同时提升在线处理效率和空间一致性。

Method: 使用因果变换器架构处理序列数据，通过时间因果注意机制及历史缓存实现高效的流式4D重建；采用知识蒸馏进行高效训练，支持高效注意力算子迁移。

Result: 实验表明，模型在保持竞品性能的同时，提高在线场景中的推理速度。

Conclusion: 为可扩展的交互式4D视觉系统铺平道路，代码已开放。

Abstract: Perceiving and reconstructing 4D spatial-temporal geometry from videos is a
fundamental yet challenging computer vision task. To facilitate interactive and
real-time applications, we propose a streaming 4D visual geometry transformer
that shares a similar philosophy with autoregressive large language models. We
explore a simple and efficient design and employ a causal transformer
architecture to process the input sequence in an online manner. We use temporal
causal attention and cache the historical keys and values as implicit memory to
enable efficient streaming long-term 4D reconstruction. This design can handle
real-time 4D reconstruction by incrementally integrating historical information
while maintaining high-quality spatial consistency. For efficient training, we
propose to distill knowledge from the dense bidirectional visual geometry
grounded transformer (VGGT) to our causal model. For inference, our model
supports the migration of optimized efficient attention operator (e.g.,
FlashAttention) from the field of large language models. Extensive experiments
on various 4D geometry perception benchmarks demonstrate that our model
increases the inference speed in online scenarios while maintaining competitive
performance, paving the way for scalable and interactive 4D vision systems.
Code is available at: https://github.com/wzzheng/StreamVGGT.

</details>


### [65] [Towards Depth Foundation Model: Recent Trends in Vision-Based Depth Estimation](https://arxiv.org/abs/2507.11540)
*Zhen Xu,Hongyu Zhou,Sida Peng,Haotong Lin,Haoyu Guo,Jiahao Shao,Peishan Yang,Qinglin Yang,Sheng Miao,Xingyi He,Yifan Wang,Yue Wang,Ruizhen Hu,Yiyi Liao,Xiaowei Zhou,Hujun Bao*

Main category: cs.CV

TL;DR: 本文探讨了深度估计领域的发展，重点是深度基础模型的潜力，该模型利用了大规模数据集以提升零样本泛化能力，同时涵盖了各种深度估计场景。


<details>
  <summary>Details</summary>
Motivation: 传统依赖硬件传感器的方法因成本高、分辨率低、对环境敏感等局限性，难以广泛适用；而基于视觉的方法虽然有前景，但也面临泛化性和稳定性不足的挑战，因此需要研发更强大的深度估计模型。

Method: 通过回顾单目、双目、多视图及单目视频深度估计的深度学习架构和范式的演变，本文解析了大规模数据集与关键训练策略对深度基础模型的影响与支持。

Result: 总结出深度基础模型在提升深度估计任务性能上的潜力，同时提供了一套关于大规模数据集和模型训练策略的详尽框架。

Conclusion: 深度基础模型能够解决现有的许多挑战，未来研究应致力于其鲁棒性提升及具体应用场景的拓展。

Abstract: Depth estimation is a fundamental task in 3D computer vision, crucial for
applications such as 3D reconstruction, free-viewpoint rendering, robotics,
autonomous driving, and AR/VR technologies. Traditional methods relying on
hardware sensors like LiDAR are often limited by high costs, low resolution,
and environmental sensitivity, limiting their applicability in real-world
scenarios. Recent advances in vision-based methods offer a promising
alternative, yet they face challenges in generalization and stability due to
either the low-capacity model architectures or the reliance on domain-specific
and small-scale datasets. The emergence of scaling laws and foundation models
in other domains has inspired the development of "depth foundation models":
deep neural networks trained on large datasets with strong zero-shot
generalization capabilities. This paper surveys the evolution of deep learning
architectures and paradigms for depth estimation across the monocular, stereo,
multi-view, and monocular video settings. We explore the potential of these
models to address existing challenges and provide a comprehensive overview of
large-scale datasets that can facilitate their development. By identifying key
architectures and training strategies, we aim to highlight the path towards
robust depth foundation models, offering insights into their future research
and applications.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [66] [Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions](https://arxiv.org/abs/2507.10577)
*Logé Cécile,Ghori Rehan*

Main category: cs.CL

TL;DR: 论文提出了一个AI系统，通过对YouTube视频中信息进行事实核查，同时与评论区用户互动，有效挑战错误信息。


<details>
  <summary>Details</summary>
Motivation: 应对数字平台尤其是YouTube上快速传播的错误信息，为建设更具信息透明的在线空间提供解决方案。

Method: 系统由两个主体组成：Truth Sleuth（事实核查）利用RAG技术评估准确性并生成报告；Trend Bender（评论互动）基于报告和相关文章撰写评论，通过自我评估循环提升评论质量。

Result: 证明系统在基准数据集和实际应用中的高效性，展示出事实核查的准确性和用户互动的影响潜力。

Conclusion: AI驱动的干预措施有助于有效打击错误信息，促进更富信息性的在线空间形成。

Abstract: Misinformation poses a significant threat in today's digital world, often
spreading rapidly through platforms like YouTube. This paper introduces a novel
approach to combating misinformation by developing an AI-powered system that
not only fact-checks claims made in YouTube videos but also actively engages
users in the comment section and challenge misleading narratives. Our system
comprises two main agents: Truth Sleuth and Trend Bender.
  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented
Generation (RAG) approach - drawing on sources like Wikipedia, Google Search,
Google FactCheck - to accurately assess their veracity and generates a nuanced
and comprehensive report. Through rigorous prompt engineering, Trend Bender
leverages this report along with a curated corpus of relevant articles to
generate insightful and persuasive comments designed to stimulate a productive
debate. With a carefully set up self-evaluation loop, this agent is able to
iteratively improve its style and refine its output.
  We demonstrate the system's capabilities through experiments on established
benchmark datasets and a real-world deployment on YouTube, showcasing its
potential to engage users and potentially influence perspectives. Our findings
highlight the high accuracy of our fact-checking agent, and confirm the
potential of AI-driven interventions in combating misinformation and fostering
a more informed online space.

</details>


### [67] [An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation](https://arxiv.org/abs/2507.10580)
*Vimaleswar A,Prabhu Nandan Sahu,Nilesh Kumar Sahu,Haroon R Lone*

Main category: cs.CL

TL;DR: 研究提出了一款名为EmoSApp的离线智能手机应用，用于心理健康和情感支持，其使用经过微调和量化的LLMs模型，能够在资源受限的设备上工作。


<details>
  <summary>Details</summary>
Motivation: 当前心理健康支持面临用户访问受限、互联网连接和数据隐私等挑战，因此需要开发一种离线、智能手机支持的解决方案。

Method: 开发了一款基于离线运行、智能手机使用的对话式应用EmoSApp，通过微调和量化的LLMs模型（如LLaMA-3.2-1B-Instruct），并使用自定义的14000多条心理健康问答和多轮对话数据集进行训练，提高模型对心理健康领域的适应性。

Result: 在学生群体中的定性评估表明，EmoSApp能够进行连贯、富有同理心的互动并提供相关建议；在9个标准推理基准上的量化评估显示模型在低资源环境中的有效性。

Conclusion: 通过强调设备本地部署和专有领域适应性，EmoSApp为未来便携、安全且高度定制化的AI驱动心理健康解决方案提供了一个范例。

Abstract: Mental health plays a crucial role in the overall well-being of an
individual. In recent years, digital platforms have been increasingly used to
expand mental health and emotional support. However, there are persistent
challenges related to limited user accessibility, internet connectivity, and
data privacy, which highlight the need for an offline, smartphone-based
solution. To address these challenges, we propose EmoSApp (Emotional Support
App): an entirely offline, smartphone-based conversational app designed for
mental health and emotional support. The system leverages Large Language Models
(LLMs), specifically fine-tuned, quantized and deployed using Torchtune and
Executorch for resource-constrained devices, allowing all inferences to occur
on the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned
the LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of
14,582 mental-health QA pairs, along with the multi-turn conversational data.
  Through qualitative human evaluation with the student population, we
demonstrate that EmoSApp has the ability to respond coherently, empathetically,
maintain interactive dialogue, and provide relevant suggestions to user's
mental health problems. Additionally, quantitative evaluations on nine standard
commonsense and reasoning benchmarks demonstrate the efficacy of our
fine-tuned, quantized model in low-resource settings. By prioritizing on-device
deployment and specialized domain adaptation, EmoSApp serves as a blueprint for
future innovations in portable, secure, and highly tailored AI-driven mental
health solutions.

</details>


### [68] [Transforming Sensitive Documents into Quantitative Data: An AI-Based Preprocessing Toolchain for Structured and Privacy-Conscious Analysis](https://arxiv.org/abs/2507.10582)
*Anders Ledberg,Anna Thalén*

Main category: cs.CL

TL;DR: 该论文提出了一种模块化工具链，通过大语言模型标准化、总结、翻译文本，同时结合匿名化技术，使得来自法律、医疗和行政领域的非结构化文本数据能够在隐私保护下进行大规模分析。


<details>
  <summary>Details</summary>
Motivation: 研究未充分利用的法律、医疗和行政领域非结构化文本数据，并克服隐私和异质性问题的障碍。

Method: 设计了一个基于开源模型的模块化工具链，利用本地硬件处理文本数据，通过大语言模型提示语生成文本总结和标准化内容，结合匿名化技术（如识别命名实体和基于规则的方法）保障隐私。

Result: 工具链成功处理超过10,800份瑞典法院判决文档，完成匿名化与标准化，总结后生成文档级嵌入。验证显示工具链在去除敏感信息的同时保留语义内容，支持预测建模任务。

Conclusion: 该工具链为涉及文本的隐私敏感领域的研究打开了新可能，能够支持大规模结构化分析并克服隐私和异质性约束。

Abstract: Unstructured text from legal, medical, and administrative sources offers a
rich but underutilized resource for research in public health and the social
sciences. However, large-scale analysis is hampered by two key challenges: the
presence of sensitive, personally identifiable information, and significant
heterogeneity in structure and language. We present a modular toolchain that
prepares such text data for embedding-based analysis, relying entirely on
open-weight models that run on local hardware, requiring only a
workstation-level GPU and supporting privacy-sensitive research.
  The toolchain employs large language model (LLM) prompting to standardize,
summarize, and, when needed, translate texts to English for greater
comparability. Anonymization is achieved via LLM-based redaction, supplemented
with named entity recognition and rule-based methods to minimize the risk of
disclosure. We demonstrate the toolchain on a corpus of 10,842 Swedish court
decisions under the Care of Abusers Act (LVM), comprising over 56,000 pages.
Each document is processed into an anonymized, standardized summary and
transformed into a document-level embedding. Validation, including manual
review, automated scanning, and predictive evaluation shows the toolchain
effectively removes identifying information while retaining semantic content.
As an illustrative application, we train a predictive model using embedding
vectors derived from a small set of manually labeled summaries, demonstrating
the toolchain's capacity for semi-automated content analysis at scale.
  By enabling structured, privacy-conscious analysis of sensitive documents,
our toolchain opens new possibilities for large-scale research in domains where
textual data was previously inaccessible due to privacy and heterogeneity
constraints.

</details>


### [69] [A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations](https://arxiv.org/abs/2507.10585)
*Isar Nejadgholi,Mona Omidyeganeh,Marc-Antoine Drouin,Jonathan Boisvert*

Main category: cs.CL

TL;DR: 本文提出了一个适用于大语言模型的自然语言解释（NLE）的分类法，以提升AI系统透明度。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的崛起，理解和验证其行为成为当务之急，现有的XAI方法需要适应新需求。

Method: 基于解释性人工智能（XAI）文献，构建了一个适用于自然语言解释的分类框架，涵盖上下文、生成与呈现以及评估三个方面。

Result: 新分类法为研究人员、审计者和政策制定者提供了一种工具，用于更好地表征、设计和增强自然语言解释。

Conclusion: 此分类法促进了自然语言解释的标准化，为AI系统的问责和透明性提供支持。

Abstract: Effective AI governance requires structured approaches for stakeholders to
access and verify AI system behavior. With the rise of large language models,
Natural Language Explanations (NLEs) are now key to articulating model
behavior, which necessitates a focused examination of their characteristics and
governance implications. We draw on Explainable AI (XAI) literature to create
an updated XAI taxonomy, adapted to prompt-based NLEs, across three dimensions:
(1) Context, including task, data, audience, and goals; (2) Generation and
Presentation, covering generation methods, inputs, interactivity, outputs, and
forms; and (3) Evaluation, focusing on content, presentation, and user-centered
properties, as well as the setting of the evaluation. This taxonomy provides a
framework for researchers, auditors, and policymakers to characterize, design,
and enhance NLEs for transparent AI systems.

</details>


### [70] [AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters](https://arxiv.org/abs/2507.10586)
*Kaushik Dwivedi,Padmanabh Patanjali Mishra*

Main category: cs.CL

TL;DR: 提出AutoRAG-LoRA框架，通过轻量化LoRA适配器和KL正则化训练，减少大语言模型的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 减少大语言模型生成中的幻觉现象，提高模型的可信度与实用性。

Method: 集成自动化提示重写、混合检索、低秩适配器调优技术，结合分类器和自我评估的幻觉检测模块，引入对比KL损失和适配器微调的反馈纠正循环。

Result: AutoRAG-LoRA显著降低了事实偏移，同时保留了模型的高效性和模块化能力。

Conclusion: AutoRAG-LoRA框架有效解决了大语言模型中的幻觉问题，是一种兼具效率和灵活性的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable fluency across a
range of natural language tasks, yet remain vulnerable to hallucinations -
factual inaccuracies that undermine trust in real world deployment. We present
AutoRAG-LoRA, a modular framework for Retrieval-Augmented Generation (RAG) that
tackles hallucination in large language models through lightweight LoRA-based
adapters and KL-regularized training. Our pipeline integrates automated prompt
rewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in
retrieved evidence. A hallucination detection module, using both
classifier-based and self-evaluation techniques, assigns confidence scores to
generated outputs, triggering an optional feedback correction loop. This loop
enforces factual alignment via contrastive KL loss and adapter fine tuning. We
demonstrate that AutoRAG-LoRA significantly reduces the factual drift while
preserving the efficiency and modularity of the model.

</details>


### [71] [Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing](https://arxiv.org/abs/2507.10587)
*Dennis Ulmer,Alexandra Lorson,Ivan Titov,Christian Hardmeier*

Main category: cs.CL

TL;DR: 本文讨论了大语言模型在输出中表现出的过度自信问题，并提出通过语言方式表达不确定性以提高用户信任。通过模拟人类的沟通方式可以实现更加可信和直观的不确定性表达。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型输出中常带有过度自信，引发潜在风险和用户信任问题，需探讨高效表达模型不确定性的方法。

Method: 梳理人类对不确定性交流的研究，分析当前不确定性语言表达中的数据偏差，提出拟人化不确定性表达的方法。

Result: 揭示了语言不确定性表达被忽略的偏差，并提出语言真实性和个性化有助于提高不确定性交流的可靠性。

Conclusion: 通过拟人化不确定性表达可以改善人机交互中的信任和协作，未来需要进一步探索其在自然语言处理中具体实现的方式。

Abstract: Human users increasingly rely on natural language interactions with large
language models (LLMs) in order to receive help on a large variety of tasks and
problems. However, the trustworthiness and perceived legitimacy of LLMs is
undermined by the fact that their output is frequently stated in very confident
terms, even when its accuracy is questionable. Therefore, there is a need to
signal the confidence of the language model to a user in order to reap the
benefits of human-machine collaboration and mitigate potential harms.
Verbalized uncertainty is the expression of confidence with linguistic means,
an approach that integrates perfectly into language-based interfaces.
Nevertheless, most recent research in natural language processing (NLP)
overlooks the nuances surrounding human uncertainty communication and the data
biases that influence machine uncertainty communication. We argue for
anthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty
communication requires a degree of linguistic authenticity and personalization
to the user, which could be achieved by emulating human communication. We
present a thorough overview over the research in human uncertainty
communication, survey ongoing research, and perform additional analyses to
demonstrate so-far overlooked biases in verbalized uncertainty. We conclude by
pointing out unique factors in human-machine communication of uncertainty and
deconstruct anthropomimetic uncertainty into future research directions for
NLP.

</details>


### [72] [PLEX: Perturbation-free Local Explanations for LLM-Based Text Classification](https://arxiv.org/abs/2507.10596)
*Yogachandran Rahulamathavan,Misbah Farooq,Varuna De Silva*

Main category: cs.CL

TL;DR: 提出了一种无需扰动的新XAI方法PLEX，显著加速了LLM文本分类的可解释性，同时效果媲美传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的XAI方法如LIME及SHAP依赖复杂的扰动计算，使用大语言模型时计算代价高昂且效率低下，亟需更高效的解决方案。

Method: 提出PLEX方法，利用大语言模型的上下文嵌入和一个Siamese风格的神经网络，训练时对齐特征重要性分数，避免了生成扰动句子的过程。

Result: 在四个分类任务中，PLEX与现有方法的结果一致度超过92%。在删除关键单词的“压力测试”中，PLEX与LIME、SHAP表现相当，但计算时间和成本分别减少2和4个数量级。

Conclusion: PLEX提供了一种高效的、无需扰动的大语言模型文本分类解释方法，对XAI领域是具有前景的创新。

Abstract: Large Language Models (LLMs) excel in text classification, but their
complexity hinders interpretability, making it difficult to understand the
reasoning behind their predictions. Explainable AI (XAI) methods like LIME and
SHAP offer local explanations by identifying influential words, but they rely
on computationally expensive perturbations. These methods typically generate
thousands of perturbed sentences and perform inferences on each, incurring a
substantial computational burden, especially with LLMs. To address this, we
propose \underline{P}erturbation-free \underline{L}ocal \underline{Ex}planation
(PLEX), a novel method that leverages the contextual embeddings extracted from
the LLM and a ``Siamese network" style neural network trained to align with
feature importance scores. This one-off training eliminates the need for
subsequent perturbations, enabling efficient explanations for any new sentence.
We demonstrate PLEX's effectiveness on four different classification tasks
(sentiment, fake news, fake COVID-19 news and depression), showing more than
92\% agreement with LIME and SHAP. Our evaluation using a ``stress test"
reveals that PLEX accurately identifies influential words, leading to a similar
decline in classification accuracy as observed with LIME and SHAP when these
words are removed. Notably, in some cases, PLEX demonstrates superior
performance in capturing the impact of key features. PLEX dramatically
accelerates explanation, reducing time and computational overhead by two and
four orders of magnitude, respectively. This work offers a promising solution
for explainable LLM-based text classification.

</details>


### [73] [Emergence of Hierarchical Emotion Organization in Large Language Models](https://arxiv.org/abs/2507.10599)
*Bo Zhao,Maya Okawa,Eric J. Bigelow,Rose Yu,Tomer Ullman,Ekdeep Singh Lubana,Hidenori Tanaka*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）如何建模用户的情绪状态，发现它们自然形成了与人类心理模型一致的层级情绪树，并在更大的模型中观察到更加复杂的层次。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在对话代理中的广泛应用，理解LLMs如何建模用户情绪状态对其伦理部署至关重要。

Method: 借鉴情绪轮理论，分析LLMs生成结果中情绪状态之间的概率依赖关系，并结合人类研究验证其社会感知内化能力。

Result: 发现LLMs形成与人类心理模型一致的层级情绪树，更大的模型表现出更复杂的层级；同时揭示其在不同社会经济背景下存在系统性偏差，尤其对交叉性弱势群体。

Conclusion: LLMs体现出情绪推理能力，借鉴认知理论有助于开发更优评价方法。

Abstract: As large language models (LLMs) increasingly power conversational agents,
understanding how they model users' emotional states is critical for ethical
deployment. Inspired by emotion wheels -- a psychological framework that argues
emotions organize hierarchically -- we analyze probabilistic dependencies
between emotional states in model outputs. We find that LLMs naturally form
hierarchical emotion trees that align with human psychological models, and
larger models develop more complex hierarchies. We also uncover systematic
biases in emotion recognition across socioeconomic personas, with compounding
misclassifications for intersectional, underrepresented groups. Human studies
reveal striking parallels, suggesting that LLMs internalize aspects of social
perception. Beyond highlighting emergent emotional reasoning in LLMs, our
results hint at the potential of using cognitively-grounded theories for
developing better model evaluations.

</details>


### [74] [Language Models for Adult Service Website Text Analysis](https://arxiv.org/abs/2507.10743)
*Nickolas Freeman,Thanh Nguyen,Gregory Bott,Jason Parton,Collin Francel*

Main category: cs.CL

TL;DR: 本研究探讨了针对成人服务网站文本数据进行分析的方法，重点是精简和定制的预训练变压器模型。


<details>
  <summary>Details</summary>
Motivation: 由于成人服务网站与性交易的联系，研究此类网站的广告文本有助于识别潜在的性交易受害者，但这些文本具有复杂性，分析具有挑战性。

Method: 提出了一种定制的变压器模型，对ASW广告文本中的低语法质量、表情符号以及加密手段进行处理分析，并在小型GPU资源上进行高效训练和推断。

Result: 定制模型在各项指标上超过了经典的预训练模型（如BERT-base、RoBERTa等），并成功地应用到拆解大图组件、文本聚类和表情符号分析任务中。

Conclusion: 本研究的模型提升了对ASW广告文本分析的能力，为后续研究和实际应用提供了可行性。

Abstract: Sex trafficking refers to the use of force, fraud, or coercion to compel an
individual to perform in commercial sex acts against their will. Adult service
websites (ASWs) have and continue to be linked to sex trafficking, offering a
platform for traffickers to advertise their victims. Thus, organizations
involved in the fight against sex trafficking often use ASW data when
attempting to identify potential sex trafficking victims. A critical challenge
in transforming ASW data into actionable insight is text analysis. Previous
research using ASW data has shown that ASW ad text is important for linking
ads. However, working with this text is challenging due to its extensive use of
emojis, poor grammar, and deliberate obfuscation to evade law enforcement
scrutiny. We conduct a comprehensive study of language modeling approaches for
this application area, including simple information retrieval methods,
pre-trained transformers, and custom transformer models. We demonstrate that
characteristics of ASW text data allow efficient custom transformer models to
be trained with relatively small GPU resources and used efficiently for
inference on consumer hardware. Our custom models outperform fine-tuned
variants of well-known encoder-only transformer models, including BERT-base,
RoBERTa, and ModernBERT, on accuracy, recall, F1 score, and ROC AUC. We
demonstrate the use of our best-performing custom configuration on three tasks
related to ASW data analysis: (i) decomposing the giant component in a graph
representation of ASW data, (ii) clustering ASW ad text, and (iii) using the
learned token embeddings to understand the use of emojis in the illicit context
we study. The models we develop represent a significant advancement in ASW text
analysis, which can be leveraged in a variety of downstream applications and
research.

</details>


### [75] [Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs](https://arxiv.org/abs/2507.10772)
*Michal Podstawski*

Main category: cs.CL

TL;DR: 利用预训练文本嵌入模型增强标注属性图中的语义分析，提高节点分类和关系预测任务的准确性和解释性。


<details>
  <summary>Details</summary>
Motivation: 标注属性图含有丰富的文本属性，但很少被充分利用；该研究旨在挖掘这些文本属性以提升图分析任务效果。

Method: 通过将语言模型的文本嵌入与属性图结合，不改变图的结构，将文本语义导入分析流程中。

Result: 实验表明，文本嵌入的大量语义信息能显著提高图分析任务的准确性和理解能力。

Conclusion: 加入语义分析的属性图能在节点分类、关系预测等任务中表现出较大优势。

Abstract: Labeled property graphs often contain rich textual attributes that can
enhance analytical tasks when properly leveraged. This work explores the use of
pretrained text embedding models to enable efficient semantic analysis in such
graphs. By embedding textual node and edge properties, we support downstream
tasks including node classification and relation prediction with improved
contextual understanding. Our approach integrates language model embeddings
into the graph pipeline without altering its structure, demonstrating that
textual semantics can significantly enhance the accuracy and interpretability
of property graph analysis.

</details>


### [76] [Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers](https://arxiv.org/abs/2507.10787)
*Yilun Zhao,Chengye Wang,Chuhan Li,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出了MISS-QA，这是第一个专门用来评估模型解读科学文献中的示意图能力的基准。


<details>
  <summary>Details</summary>
Motivation: 通过提供一个评估模型在解读科学文献示意图的能力的基准，促进多模态模型在科学领域的发展。

Method: 创建了MISS-QA基准，包含1,500个专家标注的示例，评估18个多模态模型对研究示意图的解读能力及回答相关问题的表现。

Result: 发现当前模型与人类专家在MISS-QA上的性能存在显著差距，并通过对无法回答的问题和错误进行分析揭示了模型的优缺点。

Conclusion: 当前的多模态模型在解读科学文献示意图上尚有明显不足，研究提供了改进模型的新见解。

Abstract: This paper introduces MISS-QA, the first benchmark specifically designed to
evaluate the ability of models to interpret schematic diagrams within
scientific literature. MISS-QA comprises 1,500 expert-annotated examples over
465 scientific papers. In this benchmark, models are tasked with interpreting
schematic diagrams that illustrate research overviews and answering
corresponding information-seeking questions based on the broader context of the
paper. We assess the performance of 18 frontier multimodal foundation models,
including o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant
performance gap between these models and human experts on MISS-QA. Our analysis
of model performance on unanswerable questions and our detailed error analysis
further highlight the strengths and limitations of current models, offering key
insights to enhance models in comprehending multimodal scientific literature.

</details>


### [77] [Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Posts from Parler](https://arxiv.org/abs/2507.10810)
*David M. Markowitz,Samuel Hardman Taylor*

Main category: cs.CL

TL;DR: 本研究探讨了在线仇恨言论与社会认同的关系，发现社交媒体中的仇恨言论与获得的认同信号之间的联系并不显著。


<details>
  <summary>Details</summary>
Motivation: 考察社交媒体上社会认同如何驱动仇恨言论，为理解这一复杂现象提供新的视角。

Method: 使用2018-2021年间Parler平台上的超过1.1亿条帖子，重点分析点赞数量与仇恨言论发布之间的关联性。

Result: 结果显示，获得点赞的仇恨言论与后续发布的仇恨内容数量无显著关联；在个体层面，社交认同与仇恨言论存在较弱负相关，但在更长时间间隔内关系混杂。

Conclusion: 社交认同驱动机制在小众社交媒体平台上的表现可能不同，需要更深入的研究来验证这些关系的普适性。

Abstract: In this paper, we explored how online hate is motivated by receiving social
approval from others. We specifically examined two central tenets of Walther's
(2024) social approval theory of online hate: (H1a) more signals of social
approval on hate messages predicts more subsequent hate messages, and (H1b) as
social approval increases, hate speech messages become more extreme. Using over
110 million posts from Parler (2018-2021), we observed that the number of
upvotes a person received on a hate speech post was unassociated with the
amount of hate speech in their next post and posts during the next week, month,
three months, and six months. Between-person effects revealed an average
negative relationship between social approval and hate speech production at the
post level, but this relationship was mixed at other time intervals. Social
approval reinforcement mechanisms of online hate may operate differently on
niche social media platforms.

</details>


### [78] [LLMs on Trial: Evaluating Judicial Fairness for Large Language Models](https://arxiv.org/abs/2507.10852)
*Yiran Hu,Zongyue Xue,Haitao Li,Siyuan Zheng,Qingjing Chen,Shaochun Wang,Xihan Zhang,Ning Zheng,Yun Liu,Qingyao Ai,Yiqun Liu,Charles L. A. Clarke,Weixing Shen*

Main category: cs.CL

TL;DR: 本文提出了一种框架和数据集来评估大型语言模型（LLMs）的司法公平性，并发现现有模型在一致性、公平性和准确性上存在明显缺陷。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs被用于影响权利和公平的高风险领域，评估其司法公平性变得至关重要。

Method: 通过司法公平理论，构建了一个公平性测评框架，设计了三个评估指标，并开发了JudiFair数据集及工具包，用于多模型对比。

Result: 实验表明现有LLMs普遍存在不一致性、偏见和不平衡的准确性，尤其在人口学标签下更显著。调整参数对公平性有影响，但模型规模、发布日期及来源国影响较小。

Conclusion: LLMs目前在司法公平上尚存挑战，提供的工具包为未来研究提供了平台以改进LLMs公平性。

Abstract: Large Language Models (LLMs) are increasingly used in high-stakes fields
where their decisions impact rights and equity. However, LLMs' judicial
fairness and implications for social justice remain underexplored. When LLMs
act as judges, the ability to fairly resolve judicial issues is a prerequisite
to ensure their trustworthiness. Based on theories of judicial fairness, we
construct a comprehensive framework to measure LLM fairness, leading to a
selection of 65 labels and 161 corresponding values. Applying this framework to
the judicial system, we compile an extensive dataset, JudiFair, comprising
177,100 unique case facts. To achieve robust statistical inference, we develop
three evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and
introduce a method to assess the overall fairness of multiple LLMs across
various labels. Through experiments with 16 LLMs, we uncover pervasive
inconsistency, bias, and imbalanced inaccuracy across models, underscoring
severe LLM judicial unfairness. Particularly, LLMs display notably more
pronounced biases on demographic labels, with slightly less bias on substance
labels compared to procedure ones. Interestingly, increased inconsistency
correlates with reduced biases, but more accurate predictions exacerbate
biases. While we find that adjusting the temperature parameter can influence
LLM fairness, model size, release date, and country of origin do not exhibit
significant effects on judicial fairness. Accordingly, we introduce a publicly
available toolkit containing all datasets and code, designed to support future
research in evaluating and improving LLM fairness.

</details>


### [79] [How Stylistic Similarity Shapes Preferences in Dialogue Dataset with User and Third Party Evaluations](https://arxiv.org/abs/2507.10918)
*Ikumi Numaya,Shoji Moriya,Shiki Sato,Reina Akama,Jun Suzuki*

Main category: cs.CL

TL;DR: 本研究探讨用户与对话生成系统的主观与客观风格相似性对用户偏好的影响，并提供了包含用户偏好及主观和客观风格相似性的新数据集。


<details>
  <summary>Details</summary>
Motivation: 目前的对话生成技术已实现更自然的交互，但需要进一步理解风格相似性对用户印象的影响，尤其是主观和客观相似性之间的区别。

Method: 创建一个包括用户偏好、主观风格相似性以及由第三方评估的客观风格相似性的数据集，并进行相关性分析。

Result: 研究发现主观风格相似性与用户偏好有强正相关，同时用户对主观相似性的感知与第三方评估的客观相似性存在差异。

Conclusion: 区分主观与客观相似性评估对深入理解风格相似性与用户偏好的关系至关重要，研究提供的新数据集可供进一步研究使用。

Abstract: Recent advancements in dialogue generation have broadened the scope of
human-bot interactions, enabling not only contextually appropriate responses
but also the analysis of human affect and sensitivity. While prior work has
suggested that stylistic similarity between user and system may enhance user
impressions, the distinction between subjective and objective similarity is
often overlooked. To investigate this issue, we introduce a novel dataset that
includes users' preferences, subjective stylistic similarity based on users'
own perceptions, and objective stylistic similarity annotated by third party
evaluators in open-domain dialogue settings. Analysis using the constructed
dataset reveals a strong positive correlation between subjective stylistic
similarity and user preference. Furthermore, our analysis suggests an important
finding: users' subjective stylistic similarity differs from third party
objective similarity. This underscores the importance of distinguishing between
subjective and objective evaluations and understanding the distinct aspects
each captures when analyzing the relationship between stylistic similarity and
user preferences. The dataset presented in this paper is available online.

</details>


### [80] [HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training](https://arxiv.org/abs/2507.10920)
*Seungho Choi*

Main category: cs.CL

TL;DR: 提出了一种名为HanjaBridge的技术，通过向语言模型注入汉字(Hanja)语义信息来增强韩语语言模型的表现。


<details>
  <summary>Details</summary>
Motivation: 韩国语的语言模型表现因韩文中音同字不同的汉字词产生语义模糊问题而较差。

Method: HanjaBridge通过向模型提供多种汉字选项，结合上下文实现词汇语境解模糊，并采用知识蒸馏方法避免遗忘问题。

Result: 在KoBALT基准测试中实现了21%的性能提升，展现了显著的跨语言迁移效应。

Conclusion: HanjaBridge技法无需额外推理成本情况下，增强了韩语语言模型性能，并提升了韩中语义对齐。

Abstract: Large language models (LLMs) often show poor performance in low-resource
languages like Korean, partly due to unique linguistic challenges such as
homophonous Sino-Korean words that are indistinguishable in Hangul script. To
address this semantic ambiguity, we propose HanjaBridge, a novel
meaning-injection technique integrated into a continual pre-training (CPT)
framework. Instead of deterministically mapping a word to a single Hanja
(Chinese character), HanjaBridge presents the model with all possible Hanja
candidates for a given homograph, encouraging the model to learn contextual
disambiguation. This process is paired with token-level knowledge distillation
to prevent catastrophic forgetting. Experimental results show that HanjaBridge
significantly improves Korean language understanding, achieving a 21\% relative
improvement on the KoBALT benchmark. Notably, by reinforcing semantic alignment
between Korean and Chinese through shared Hanja, we observe a strong positive
cross-lingual transfer. Furthermore, these gains persist even when Hanja
augmentation is omitted at inference time, ensuring practical efficiency with
no additional run-time cost.

</details>


### [81] [Modeling Understanding of Story-Based Analogies Using Large Language Models](https://arxiv.org/abs/2507.10957)
*Kalit Inani,Keshav Kabra,Vijay Marupudi,Sashank Varma*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在类比推理能力上的表现，包括语义表示和问题解释，探讨其相较于人类推理性能的异同。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在类比检测和映射任务中的表现及其与人类推理性能的对比，目标是填补目前研究发现中的空白。

Method: 通过故事型类比映射任务对LLMs的推理能力进行细致评估，分析模型的语义表示及提示其解释类比的效果，同时比较不同模型参数规模和架构的表现。

Result: 实验表明LLMs在类比任务中具有一定的理解和推理能力，不同规模及架构的模型在性能上存在变异。

Conclusion: 研究深化了对LLMs类比推理能力的理解，并探讨它们作为人类推理模型的潜力。

Abstract: Recent advancements in Large Language Models (LLMs) have brought them closer
to matching human cognition across a variety of tasks. How well do these models
align with human performance in detecting and mapping analogies? Prior research
has shown that LLMs can extract similarities from analogy problems but lack
robust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the
current study focused on a story-based analogical mapping task and conducted a
fine-grained evaluation of LLM reasoning abilities compared to human
performance. First, it explored the semantic representation of analogies in
LLMs, using sentence embeddings to assess whether they capture the similarity
between the source and target texts of an analogy, and the dissimilarity
between the source and distractor texts. Second, it investigated the
effectiveness of explicitly prompting LLMs to explain analogies. Throughout, we
examine whether LLMs exhibit similar performance profiles to those observed in
humans by evaluating their reasoning at the level of individual analogies, and
not just at the level of overall accuracy (as prior studies have done). Our
experiments include evaluating the impact of model size (8B vs. 70B parameters)
and performance variation across state-of-the-art model architectures such as
GPT-4 and LLaMA3. This work advances our understanding of the analogical
reasoning abilities of LLMs and their potential as models of human reasoning.

</details>


### [82] [DS@GT at eRisk 2025: From prompts to predictions, benchmarking early depression detection with conversational agent based assessments and temporal attention models](https://arxiv.org/abs/2507.10958)
*Anthony Miyaguchi,David Guecha,Yuwen Chiu,Sidharth Gaur*

Main category: cs.CL

TL;DR: 本文总结了DS@GT团队参与eRisk 2025中两项挑战的工作，探讨了用大语言模型（LLMs）进行对话式抑郁检测的实验。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索通过提示策略让大语言模型根据BDI-II标准进行对话式抑郁检测，提高自评一致性和准确性。

Method: 采用提示工程方法，使用不同的LLM进行BDI-II评估并生成结构化JSON输出，通过跨模型一致性和内部一致性进行评价。

Result: 最佳提交结果在官方榜单排名第二，指标为DCHR = 0.50，ADODL = 0.89，ASHR = 0.27。

Conclusion: 提示设计方法有效对齐BDI-II标准，提供了对影响症状预测的对话线索的分析支持。

Abstract: This Working Note summarizes the participation of the DS@GT team in two eRisk
2025 challenges. For the Pilot Task on conversational depression detection with
large language-models (LLMs), we adopted a prompt-engineering strategy in which
diverse LLMs conducted BDI-II-based assessments and produced structured JSON
outputs. Because ground-truth labels were unavailable, we evaluated cross-model
agreement and internal consistency. Our prompt design methodology aligned model
outputs with BDI-II criteria and enabled the analysis of conversational cues
that influenced the prediction of symptoms. Our best submission, second on the
official leaderboard, achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27.

</details>


### [83] [Teach Me Sign: Stepwise Prompting LLM for Sign Language Production](https://arxiv.org/abs/2507.10972)
*Zhaoyi An,Rei Kawakami*

Main category: cs.CL

TL;DR: 本文提出了一种名为TEAM-Sign的方法，通过微调大型语言模型（LLM）以生成手语。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在许多AI任务中表现出色，但由于手语的复杂性及独特规则，其对手语生成的影响有限；研究旨在改善这一现状。

Method: 将手语视为自然语言，使用微调LLM并采用逐步提示策略挖掘LLM中的手语知识，以学习文本和手语之间的对应关系并生成手语。

Result: 实验表明，TEAM-Sign有效地利用了LLM的手语知识和推理能力，使得手语和口语在分布及语法规则上得到更好的对齐。

Conclusion: TEAM-Sign方法展示了LLM在手语生成方面的潜力，可通过微调和提示策略提升生成的效果，证明其在复杂语言任务中的应用价值。

Abstract: Large language models, with their strong reasoning ability and rich
knowledge, have brought revolution to many tasks of AI, but their impact on
sign language generation remains limited due to its complexity and unique
rules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign
language as another natural language. By fine-tuning an LLM, we enable it to
learn the correspondence between text and sign language, and facilitate
generation. Considering the differences between sign and spoken language, we
employ a stepwise prompting strategy to extract the inherent sign language
knowledge within the LLM, thereby supporting the learning and generation
process. Experimental results on How2Sign and Phoenix14T datasets demonstrate
that our approach effectively leverages both the sign language knowledge and
reasoning capabilities of LLM to align the different distribution and
grammatical rules between sign and spoken language.

</details>


### [84] [Mario at EXIST 2025: A Simple Gateway to Effective Multilingual Sexism Detection](https://arxiv.org/abs/2507.10996)
*Lin Tian,Johanne R. Trippas,Marian-Andrei Rizoiu*

Main category: cs.CL

TL;DR: 本文使用Llama 3.1 8B通过分层LoRA方法用于英文和西班牙文推文的性别歧视检测，提出了条件适配路由并展示了其高效性能。


<details>
  <summary>Details</summary>
Motivation: 利用分层结构和LoRA减少训练时间和存储需求，同时提升模型在多语言性别歧视检测任务中的表现。

Method: 结合传统Low-Rank Adaptation（LoRA）框架，以分层结构对应三个子任务进行训练（性别歧视判断、意图检测、多标签分类），并通过跨语言训练提升模型表现。

Result: 通过跨语言迁移提升F1评分1.7-2.4\%，训练参数只占全模型的1.67\%，训练时间减少75\%，存储需求减少98\%，在所有子任务中表现强劲。

Conclusion: 方法展现了在多任务、多语言环境中有效改进模型性能的潜力，同时具有极高的参数效率。

Abstract: This paper presents our approach to EXIST 2025 Task 1, addressing text-based
sexism detection in English and Spanish tweets through hierarchical Low-Rank
Adaptation (LoRA) of Llama 3.1 8B. Our method introduces conditional adapter
routing that explicitly models label dependencies across three hierarchically
structured subtasks: binary sexism identification, source intention detection,
and multilabel sexism categorization. Unlike conventional LoRA applications
that target only attention layers, we apply adaptation to all linear
transformations, enhancing the model's capacity to capture task-specific
patterns. In contrast to complex data processing and ensemble approaches, we
show that straightforward parameter-efficient fine-tuning achieves strong
performance. We train separate LoRA adapters (rank=16, QLoRA 4-bit) for each
subtask using unified multilingual training that leverages Llama 3.1's native
bilingual capabilities. The method requires minimal preprocessing and uses
standard supervised learning. Our multilingual training strategy eliminates the
need for separate language-specific models, achieving 1.7-2.4\% F1 improvements
through cross-lingual transfer. With only 1.67\% trainable parameters compared
to full fine-tuning, our approach reduces training time by 75\% and model
storage by 98\%, while achieving competitive performance across all subtasks
(ICM-Hard: 0.6774 for binary classification, 0.4991 for intention detection,
0.6519 for multilabel categorization).

</details>


### [85] [Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification](https://arxiv.org/abs/2507.11004)
*Yejun Yoon,Jaeyoon Jung,Seunghyun Yoon,Kunwoo Park*

Main category: cs.CL

TL;DR: HerO 2 是针对 AVeriTeC 任务的改进系统，提升了证据质量、真假预测和系统性能，排名第二，运行时间最短。


<details>
  <summary>Details</summary>
Motivation: 改进前一年度 FEVER-25 工作坊挑战中最佳开源模型 HerO，以提升事实验证性能和运行效率。

Method: 通过文档摘要和答案重述提升证据质量，通过后训练量化优化真假预测，并更新语言模型以增强系统性能。

Result: HerO 2 在排行榜中排名第二，同时在前三的系统中运行时间最短。

Conclusion: HerO 2 展现了高效性和强大的事实验证能力，可用于实际应用场景。

Abstract: This paper presents HerO 2, Team HUMANE's system for the AVeriTeC shared task
at the FEVER-25 workshop. HerO 2 is an enhanced version of HerO, the
best-performing open-source model from the previous year's challenge. It
improves evidence quality through document summarization and answer
reformulation, optimizes veracity prediction via post-training quantization
under computational constraints, and enhances overall system performance by
integrating updated language model (LM) backbones. HerO 2 ranked second on the
leaderboard while achieving the shortest runtime among the top three systems,
demonstrating both high efficiency and strong potential for real-world fact
verification. The code is available at https://github.com/ssu-humane/HerO2.

</details>


### [86] [Journalism-Guided Agentic In-Context Learning for News Stance Detection](https://arxiv.org/abs/2507.11049)
*Dahyun Lee,Jonghyeon Choi,Jiyoung Han,Kunwoo Park*

Main category: cs.CL

TL;DR: 这篇论文提出了韩国首个用于文章级别立场检测的 K-News-Stance 数据集，并提出了一个基于语言模型的新框架 JoA-ICL，可用于改进对象多样化新闻推荐和媒体偏向分析。


<details>
  <summary>Details</summary>
Motivation: 随着在线新闻消费的增长，个性化推荐系统的使用日益普遍，但这些系统可能导致过滤泡沫和政治极化，呼吁引入能考虑多样化视角的方法。

Method: 作者创建了包含2000篇新闻文章和19650段落级别标注的韩语数据集 K-News-Stance，同时提出了基于语料分段立场预测的语言模型 JoA-ICL 框架，通过关键段落的预测推断文章整体立场。

Result: 实验表明 JoA-ICL 在捕获长篇新闻文章整体立场的表现优于现有方法。此外，通过两个案例研究，也展示了该方法在新闻多样化推荐和媒体偏向模式分析中的实用性。

Conclusion: 该研究通过数据和方法论上的创新，为改进新闻推荐系统和深入理解媒体偏向现象提供了新的工具。

Abstract: As online news consumption grows, personalized recommendation systems have
become integral to digital journalism. However, these systems risk reinforcing
filter bubbles and political polarization by failing to incorporate diverse
perspectives. Stance detection -- identifying a text's position on a target --
can help mitigate this by enabling viewpoint-aware recommendations and
data-driven analyses of media bias. Yet, existing stance detection research
remains largely limited to short texts and high-resource languages. To address
these gaps, we introduce \textsc{K-News-Stance}, the first Korean dataset for
article-level stance detection, comprising 2,000 news articles with
article-level and 19,650 segment-level stance annotations across 47 societal
issues. We also propose \textsc{JoA-ICL}, a \textbf{Jo}urnalism-guided
\textbf{A}gentic \textbf{I}n-\textbf{C}ontext \textbf{L}earning framework that
employs a language model agent to predict the stances of key structural
segments (e.g., leads, quotes), which are then aggregated to infer the overall
article stance. Experiments show that \textsc{JoA-ICL} outperforms existing
stance detection methods, highlighting the benefits of segment-level agency in
capturing the overall position of long-form news articles. Two case studies
further demonstrate its broader utility in promoting viewpoint diversity in
news recommendations and uncovering patterns of media bias.

</details>


### [87] [LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction: A Clinical NLP](https://arxiv.org/abs/2507.11052)
*Haowei Yang,Ziyu Shen,Junli Shao,Luyao Men,Xinyue Han,Jing Dong*

Main category: cs.CL

TL;DR: 这篇论文提出了一个结合大语言模型（LLM）的临床自然语言处理（NLP）方法，用于从医疗文本中提取症状并进行相关分析，从而提高心血管疾病风险预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 目前的心血管疾病预测模型多依赖于结构化数据，但非结构化的医疗文本包含许多重要的早期迹象。研究者希望通过利用这些文本信息，进一步改进疾病风险预测能力。

Method: 该方法采用了心血管疾病领域适配的大语言模型，结合特定领域的微调、基于提示的推理以及实体感知的推理，设计了一个新的临床NLP管道。同时通过提示工程和基于规则的验证解决了上下文幻觉及时间模糊性问题。

Result: 在MIMIC-III和CARDIO-NLP两个数据集上的实验表明，该方法在精准率、召回率、F1分数和AUROC方面均有明显提升，并且与心血管专家评估一致性较高（kappa=0.82）。

Conclusion: 研究证明了LLM在心血管疾病风险识别及预测中的潜力，有助于改善早期预警系统，并将患者描述转化为可行动的风险评估结果。

Abstract: Timely identification and accurate risk stratification of cardiovascular
disease (CVD) remain essential for reducing global mortality. While existing
prediction models primarily leverage structured data, unstructured clinical
notes contain valuable early indicators. This study introduces a novel
LLM-augmented clinical NLP pipeline that employs domain-adapted large language
models for symptom extraction, contextual reasoning, and correlation from
free-text reports. Our approach integrates cardiovascular-specific fine-tuning,
prompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III
and CARDIO-NLP datasets demonstrate improved performance in precision, recall,
F1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by
cardiologists. Challenges such as contextual hallucination, which occurs when
plausible information contracts with provided source, and temporal ambiguity,
which is related with models struggling with chronological ordering of events
are addressed using prompt engineering and hybrid rule-based verification. This
work underscores the potential of LLMs in clinical decision support systems
(CDSS), advancing early warning systems and enhancing the translation of
patient narratives into actionable risk assessments.

</details>


### [88] [Social Media Sentiments Analysis on the July Revolution in Bangladesh: A Hybrid Transformer Based Machine Learning Approach](https://arxiv.org/abs/2507.11084)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha*

Main category: cs.CL

TL;DR: 本文分析了孟加拉国7月革命期间社交媒体上表达的公众情绪，使用了混合变压器的情绪分析框架，并提出了一个新的XMB-BERT模型。


<details>
  <summary>Details</summary>
Motivation: 研究在社交媒体平台上如何分析与理解公众情绪，特别是在孟加拉国7月革命这样的社会运动期间。

Method: 提出并使用了混合变压器模型XMB-BERT，通过情绪分析框架分析来自社交媒体的4200条孟加拉语评论；引入PCA维度缩减，结合多种机器学习分类器进行深入分析。

Result: 实验表明，混合XMB-BERT结合投票分类器的准确率达到83.7%，在性能上优于其他模型。

Conclusion: 研究显示，机器学习技术在分析孟加拉语等低资源语言的社交情绪方面具有巨大潜力。

Abstract: The July Revolution in Bangladesh marked a significant student-led mass
uprising, uniting people across the nation to demand justice, accountability,
and systemic reform. Social media platforms played a pivotal role in amplifying
public sentiment and shaping discourse during this historic mass uprising. In
this study, we present a hybrid transformer-based sentiment analysis framework
to decode public opinion expressed in social media comments during and after
the revolution. We used a brand new dataset of 4,200 Bangla comments collected
from social media. The framework employs advanced transformer-based feature
extraction techniques, including BanglaBERT, mBERT, XLM-RoBERTa, and the
proposed hybrid XMB-BERT, to capture nuanced patterns in textual data.
Principle Component Analysis (PCA) were utilized for dimensionality reduction
to enhance computational efficiency. We explored eleven traditional and
advanced machine learning classifiers for identifying sentiments. The proposed
hybrid XMB-BERT with the voting classifier achieved an exceptional accuracy of
83.7% and outperform other model classifier combinations. This study
underscores the potential of machine learning techniques to analyze social
sentiment in low-resource languages like Bangla.

</details>


### [89] [Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border Entity Identification](https://arxiv.org/abs/2507.11086)
*Andres Azqueta-Gavaldón,Joaquin Ramos Cosgrove*

Main category: cs.CL

TL;DR: 本文探讨了如何在西班牙金融系统中，利用大语言模型（LLMs）优化跨境实体识别与分类，以对抗传统算法在语义与上下文处理方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着国际金融活动的增加，准确识别并分类跨境实体成为必要，这不仅有助于风险管理，也能确保监管合规及防范经济犯罪。

Method: 采用了传统算法（如Jaccard、余弦和Levenshtein距离）、基于Hugging Face的大语言模型以及界面型LLM（如微软Copilot、阿里巴巴Qwen 2.5）进行实体匹配测试，涵盖65个葡萄牙企业案例。

Result: 传统方法准确率超过92%，但假阳性率较高（20-40%）。界面型LLM表现最佳，准确率超过93%，F1得分超过96%，且假阳性率大幅降低（40-80%）。

Conclusion: 界面型LLM在跨境实体匹配任务中表现优异，能更有效处理语义关係与上下文，是传统算法的灵活替代方案。

Abstract: The growing prevalence of cross-border financial activities in global markets
has underscored the necessity of accurately identifying and classifying foreign
entities. This practice is essential within the Spanish financial system for
ensuring robust risk management, regulatory adherence, and the prevention of
financial misconduct. This process involves a labor-intensive entity-matching
task, where entities need to be validated against available reference sources.
Challenges arise from linguistic variations, special characters, outdated
names, and changes in legal forms, complicating traditional matching algorithms
like Jaccard, cosine, and Levenshtein distances. These methods struggle with
contextual nuances and semantic relationships, leading to mismatches. To
address these limitations, we explore Large Language Models (LLMs) as a
flexible alternative. LLMs leverage extensive training to interpret context,
handle abbreviations, and adapt to legal transitions. We evaluate traditional
methods, Hugging Face-based LLMs, and interface-based LLMs (e.g., Microsoft
Copilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese company cases.
Results show traditional methods achieve accuracies over 92% but suffer high
false positive rates (20-40%). Interface-based LLMs outperform, achieving
accuracies above 93%, F1 scores exceeding 96%, and lower false positives
(40-80%).

</details>


### [90] [The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs](https://arxiv.org/abs/2507.11097)
*Zichen Wen,Jiashu Qu,Dongrui Liu,Zhiyuan Liu,Ruixi Wu,Yicun Yang,Xiangqi Jin,Haoyun Xu,Xuyang Liu,Weijia Li,Chaochao Lu,Jing Shao,Conghui He,Linfeng Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种针对扩散型大语言模型（dLLMs）的新型攻击方法DIJA，揭示了此类模型面临的安全性弱点，特别是在上下文感知和屏蔽输入的对抗性提示词中。


<details>
  <summary>Details</summary>
Motivation: 研究表明，尽管扩散型大语言模型在代码生成和文本填充方面性能出色，但现有的对齐机制无法应对特定的攻击提示词，暴露出新的安全漏洞。作者旨在揭示和量化这些漏洞，并寻找可能的解决方案。

Method: 提出DIJA框架，通过利用dLLMs的双向建模和并行解码机制，构建对抗性掩码文本提示词，探索模型在对齐机制下的安全性漏洞和修复界限。

Result: 实验表明，DIJA方法在多项基准测试中表现优异，关键字攻击成功率（ASR）达到100%，并在多个指标上显著超越现有最先进方案。

Conclusion: 研究表明现有的对齐机制不足以确保dLLMs的安全使用，呼吁在此类模型的安全对齐方面进行重新思考和改进。

Abstract: Diffusion-based large language models (dLLMs) have recently emerged as a
powerful alternative to autoregressive LLMs, offering faster inference and
greater interactivity via parallel decoding and bidirectional modeling.
However, despite strong performance in code generation and text infilling, we
identify a fundamental safety concern: existing alignment mechanisms fail to
safeguard dLLMs against context-aware, masked-input adversarial prompts,
exposing novel vulnerabilities. To this end, we present DIJA, the first
systematic study and jailbreak attack framework that exploits unique safety
weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial
interleaved mask-text prompts that exploit the text generation mechanisms of
dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional
modeling drives the model to produce contextually consistent outputs for masked
spans, even when harmful, while parallel decoding limits model dynamic
filtering and rejection sampling of unsafe content. This causes standard
alignment mechanisms to fail, enabling harmful completions in alignment-tuned
dLLMs, even when harmful behaviors or unsafe instructions are directly exposed
in the prompt. Through comprehensive experiments, we demonstrate that DIJA
significantly outperforms existing jailbreak methods, exposing a previously
overlooked threat surface in dLLM architectures. Notably, our method achieves
up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior
baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and
by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of
harmful content in the jailbreak prompt. Our findings underscore the urgent
need for rethinking safety alignment in this emerging class of language models.
Code is available at https://github.com/ZichenWen1/DIJA.

</details>


### [91] [Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs](https://arxiv.org/abs/2507.11112)
*Sanhanat Sivapiromrat,Caiqi Zhang,Marco Basaldella,Nigel Collier*

Main category: cs.CL

TL;DR: 研究表明，LLM容易受到数据投毒攻击，这种攻击通过恶意训练样本植入隐藏行为，触发特定输入模式。本研究提出了一个研究LLM投毒的框架，并显示多个触发器可以共存且不互相干扰。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注攻击效果，较少探讨触发机制及多个触发器的交互，本研究旨在填补这一空白。

Method: 提出框架研究多触发器特性的实验方法，并利用高嵌入相似性的触发器实现多触发器稳定活跃性。同时，提出基于权重差异分析的触发行为去除方法。

Result: 发现多个触发器可在模型内共存，且具有鲁棒活跃性。提出的防御方法可有效消除触发行为并仅需最小参数更新。

Conclusion: 研究揭示了LLMs更广泛且持续的投毒风险，提供了高效的防御方法以减少攻击威胁。

Abstract: Recent studies have shown that Large Language Models (LLMs) are vulnerable to
data poisoning attacks, where malicious training examples embed hidden
behaviours triggered by specific input patterns. However, most existing works
assume a phrase and focus on the attack's effectiveness, offering limited
understanding of trigger mechanisms and how multiple triggers interact within
the model. In this paper, we present a framework for studying poisoning in
LLMs. We show that multiple distinct backdoor triggers can coexist within a
single model without interfering with each other, enabling adversaries to embed
several triggers concurrently. Using multiple triggers with high embedding
similarity, we demonstrate that poisoned triggers can achieve robust activation
even when tokens are substituted or separated by long token spans. Our findings
expose a broader and more persistent vulnerability surface in LLMs. To mitigate
this threat, we propose a post hoc recovery method that selectively retrains
specific model components based on a layer-wise weight difference analysis. Our
method effectively removes the trigger behaviour with minimal parameter
updates, presenting a practical and efficient defence against multi-trigger
poisoning.

</details>


### [92] [MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models](https://arxiv.org/abs/2507.11114)
*Seif Ahmed,Mohamed T. Younes,Abdelrahman Moustafa,Abdelrahman Allam,Hamza Moustafa*

Main category: cs.CL

TL;DR: 提出了一种基于轻量OCR和VLM的多语言多模态推理系统，并在ImageCLEF 2025 EXAMS V挑战赛中取得了优异的表现。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种轻量化、高效的解决方案，增强多语言教育场景下的推理能力。

Method: 结合多模块模型（Gemini 2.5 Flash、Gemini 1.5 Pro等）以及精心设计的few-shot和zero-shot提示，通过多语言数据增强和提示优化提升性能。

Result: 系统在官方多语言排行榜中以81.4%的准确率获得第一名，并在13种语言测试中取得11项最佳成绩。

Conclusion: 优化提示配合轻量化的OCR-VLM组合能够超越传统端到端模型，特别是在多语言高风险场景中表现出优越性。

Abstract: We present a robust ensemble-based system for multilingual multimodal
reasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach
integrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption
refinement and consistency checks, and Gemini 2.5 Pro as a reasoner which
handles final answer selection, all coordinated through carefully engineered
few-shot and zero-shot prompts. We conducted an extensive ablation study,
training several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3,
Mistral) on an English dataset and its multilingual augmented version.
Additionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for
comparison and found it to substantially outperform the trained models. Prompt
design also proved critical: enforcing concise, language-normalized formats and
prohibiting explanatory text boosted model accuracy on the English validation
set from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA)
achieved first place overall in the multilingual track with 81.4% accuracy, and
led 11 out of 13 individual language tracks, with top results such as 95.07%
for Croatian and 92.12% for Italian. These findings highlight that lightweight
OCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual
augmentation, can outperform heavier end-to-end models in high-stakes,
multilingual educational settings.

</details>


### [93] [What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests](https://arxiv.org/abs/2507.11128)
*Dimitri Staufer*

Main category: cs.CL

TL;DR: 本论文提出了一种新方法，通过构建WikiMem数据集，量化大语言模型中人类相关信息的记忆程度，为识别和遗忘模型中存储的个人信息奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 目前关于“被遗忘权”的研究主要集中在数据已知的前提下，缺乏针对模型中具体存储的个人信息进行识别和测量的方法。

Method: 创建WikiMem数据集，包含5000多个自然语言提示语料，针对243个人类相关属性；并引入一种模型无关的量化指标，通过负对数似然度对真实值和反事实值进行排序评估。

Result: 对200名个体在15个大语言模型中（规模410M-70B参数）进行评估，发现记忆程度与主体网络存在性及模型规模呈正相关。

Conclusion: 研究为基于大语言模型的个体级别“被遗忘权”实施提供了技术基础，通过动态构建遗忘集，实现更精确和符合隐私法规的模型实践。

Abstract: Large Language Models (LLMs) can memorize and reveal personal information,
raising concerns regarding compliance with the EU's GDPR, particularly the
Right to Be Forgotten (RTBF). Existing machine unlearning methods assume the
data to forget is already known but do not address how to identify which
individual-fact associations are stored in the model. Privacy auditing
techniques typically operate at the population level or target a small set of
identifiers, limiting applicability to individual-level data inquiries. We
introduce WikiMem, a dataset of over 5,000 natural language canaries covering
243 human-related properties from Wikidata, and a model-agnostic metric to
quantify human-fact associations in LLMs. Our approach ranks ground-truth
values against counterfactuals using calibrated negative log-likelihood across
paraphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B
parameters), showing that memorization correlates with subject web presence and
model scale. We provide a foundation for identifying memorized personal data in
LLMs at the individual level, enabling the dynamic construction of forget sets
for machine unlearning and RTBF requests.

</details>


### [94] [Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding](https://arxiv.org/abs/2507.11198)
*Conrad Borchers,Bahar Shahrokhian,Francesco Balzan,Elham Tajik,Sreecharan Sankaranarayanan,Sebastian Simon*

Main category: cs.CL

TL;DR: 本研究探讨多代理系统（MAS）是否能提高基于代码本的对话段落编码准确性，并发现多代理在大部分情况下未优于单代理编码，甚至在某些配置中表现更差，但在某些特定条件下可改善模糊代码的处理。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）逐步应用于规模化质性研究，作者希望分析多代理系统（MAS）在仿真人类编码流程及其对提升编码准确性方面的潜力，特别是与单代理编码相比的表现。

Method: 研究采用开放源码MAS，通过结构化代理讨论和共识仲裁模拟演绎的人类编码过程。使用6种开放源码LLMs，18种实验配置，并与人类注释的数学辅导数据集进行对比分析，共处理超过77,000条编码决策。

Result: 实验表明温度显著影响所有模型达成共识的时间与方式；多种人格代理（如中立、强势或富有同理心）延缓部分模型中的共识达成，但在准确性提升方面，MAS总体未优于单代理编码，仅在某些特定条件下少数模型表现稍优。

Conclusion: 文章质疑多样化MAS人格可提升编码产出的观念，并开放了相关代码以供后续研究使用，同时指出MAS可能在改进代码本和缩小模糊代码应用的差异方面发挥一定作用。

Abstract: Large Language Models (LLMs) enable new possibilities for qualitative
research at scale, including coding and data annotation. While multi-agent
systems (MAS) can emulate human coding workflows, their benefits over
single-agent coding remain poorly understood. We conducted an experimental
study of how agent persona and temperature shape consensus-building and coding
accuracy of dialog segments based on a codebook with 8 codes. Our open-source
MAS mirrors deductive human coding through structured agent discussion and
consensus arbitration. Using six open-source LLMs (with 3 to 32 billion
parameters) and 18 experimental configurations, we analyze over 77,000 coding
decisions against a gold-standard dataset of human-annotated transcripts from
online math tutoring sessions. Temperature significantly impacted whether and
when consensus was reached across all six LLMs. MAS with multiple personas
(including neutral, assertive, or empathetic), significantly delayed consensus
in four out of six LLMs compared to uniform personas. In three of those LLMs,
higher temperatures significantly diminished the effects of multiple personas
on consensus. However, neither temperature nor persona pairing lead to robust
improvements in coding accuracy. Single agents matched or outperformed MAS
consensus in most conditions. Only one model (OpenHermesV2:7B) and code
category showed above-chance gains from MAS deliberation when temperature was
0.5 or lower and especially when the agents included at least one assertive
persona. Qualitative analysis of MAS collaboration for these configurations
suggests that MAS may nonetheless aid in narrowing ambiguous code applications
that could improve codebooks and human-AI coding. We contribute new insight
into the limits of LLM-based qualitative methods, challenging the notion that
diverse MAS personas lead to better outcomes. We open-source our MAS and
experimentation code.

</details>


### [95] [EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question Answering](https://arxiv.org/abs/2507.11216)
*Valle Ruiz-Fernández,Mario Mina,Júlia Falcão,Luis Vasquez-Reina,Anna Sallés,Aitor Gonzalez-Agirre,Olatz Perez-de-Viñaspre*

Main category: cs.CL

TL;DR: 这篇论文介绍了西班牙语和加泰罗尼亚语的社会偏见评估数据集EsBBQ和CaBBQ，用于评估大语言模型在非英语和美国以外社会背景中的偏见表现。


<details>
  <summary>Details</summary>
Motivation: 现有文献表明大语言模型倾向于延续从其预训练数据中学习到的社会偏见，但针对英语以外语言和美国以外社会背景的评估资源明显不足。

Method: 基于原始的BBQ数据集，设计了适应西班牙语和加泰罗尼亚语，以及西班牙社会背景的平行数据集EsBBQ和CaBBQ，并使用多项选择的问答设置评估10类社会偏见问题。

Result: 对不同家族、规模及变体的语言模型进行了测试，发现模型在歧义场景中往往无法选择正确答案，且较高的问答准确率通常伴随着更强的社会偏见依赖。

Conclusion: 该研究提出的评估基准揭示了现有大语言模型跨语言和跨社会背景的局限性，并为开发更公正的模型提供了重要参考。

Abstract: Previous literature has largely shown that Large Language Models (LLMs)
perpetuate social biases learnt from their pre-training data. Given the notable
lack of resources for social bias evaluation in languages other than English,
and for social contexts outside of the United States, this paper introduces the
Spanish and the Catalan Bias Benchmarks for Question Answering (EsBBQ and
CaBBQ). Based on the original BBQ, these two parallel datasets are designed to
assess social bias across 10 categories using a multiple-choice QA setting, now
adapted to the Spanish and Catalan languages and to the social context of
Spain. We report evaluation results on different LLMs, factoring in model
family, size and variant. Our results show that models tend to fail to choose
the correct answer in ambiguous scenarios, and that high QA accuracy often
correlates with greater reliance on social biases.

</details>


### [96] [An Agentic Flow for Finite State Machine Extraction using Prompt Chaining](https://arxiv.org/abs/2507.11222)
*Fares Wael,Youssef Maklad,Ali Hamdi,Wael Elsersy*

Main category: cs.CL

TL;DR: 在研究网络协议的操作逻辑建模中，提出了一种基于大语言模型(LLMs)的新框架FlowFSM，用于从RFC文档中提取有限状态机(FSM)。


<details>
  <summary>Details</summary>
Motivation: 现有FSM提取技术在扩展性、不完整覆盖以及自然语言规范的模糊性方面存在局限性。

Method: 提出了FlowFSM框架，通过结合大语言模型(LLMs)、提示链技术及链式思考推理方式，从RFC文档中系统地提取FSM和构建规则书。

Result: 实验表明，FlowFSM在FTP和RTSP协议上能够实现高精度的FSM提取，同时最小化虚假的跃迁。

Conclusion: FlowFSM展示了基于代理的大语言模型系统在协议分析及FSM推断中的潜力，对网络安全及逆向工程具有重要意义。

Abstract: Finite-State Machines (FSMs) are critical for modeling the operational logic
of network protocols, enabling verification, analysis, and vulnerability
discovery. However, existing FSM extraction techniques face limitations such as
scalability, incomplete coverage, and ambiguity in natural language
specifications. In this paper, we propose FlowFSM, a novel agentic framework
that leverages Large Language Models (LLMs) combined with prompt chaining and
chain-of-thought reasoning to extract accurate FSMs from raw RFC documents.
FlowFSM systematically processes protocol specifications, identifies state
transitions, and constructs structured rule-books by chaining agent outputs.
Experimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM
achieves high extraction precision while minimizing hallucinated transitions,
showing promising results. Our findings highlight the potential of agent-based
LLM systems in the advancement of protocol analysis and FSM inference for
cybersecurity and reverse engineering applications.

</details>


### [97] [Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages](https://arxiv.org/abs/2507.11230)
*Lyzander Marciano Andrylie,Inaya Rahmanisa,Mahardika Krisna Ihsani,Alfan Farizki Wicaksono,Haryo Akbarianto Wibowo,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 研究通过稀疏自编码器方法，在大型语言模型中识别出语言特定特征，并展示其如何影响模型的多语言表现及语言识别能力。


<details>
  <summary>Details</summary>
Motivation: 研究多语言机制的基础是了解大型语言模型如何处理不同语言，但现有方法难以分离跨语言的语言特定单元。

Method: 提出了一种基于特征激活概率的SAE-LAPE方法，在前馈网络层中识别语言特定特征。

Result: 发现语言特定特征多数集中在模型的中后层，其对多语言处理表现和输出结果有重要影响，并可实现接近fastText的语言识别性能。

Conclusion: 语言特定特征具有可解释性，可用于改进语言模型的多语言表现及提高语言识别的解释性。

Abstract: Understanding the multilingual mechanisms of large language models (LLMs)
provides insight into how they process different languages, yet this remains
challenging. Existing studies often focus on individual neurons, but their
polysemantic nature makes it difficult to isolate language-specific units from
cross-lingual representations. To address this, we explore sparse autoencoders
(SAEs) for their ability to learn monosemantic features that represent concrete
and abstract concepts across languages in LLMs. While some of these features
are language-independent, the presence of language-specific features remains
underexplored. In this work, we introduce SAE-LAPE, a method based on feature
activation probability, to identify language-specific features within the
feed-forward network. We find that many such features predominantly appear in
the middle to final layers of the model and are interpretable. These features
influence the model's multilingual performance and language output and can be
used for language identification with performance comparable to fastText along
with more interpretability. Our code is available at
https://github.com/LyzanderAndrylie/language-specific-features .

</details>


### [98] [KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding](https://arxiv.org/abs/2507.11273)
*Luohe Shi,Zuchao Li,Lefei Zhang,Guoming Liu,Baoyuan Qi,Hai Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种名为KV-Latent的范式，通过将Key-Value向量降采样到潜在空间中，降低KV缓存的内存消耗并提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型推理时Key-Value(KV)缓存的内存与带宽瓶颈问题。

Method: 使用KV-Latent范式，将Key-Value向量降采样到潜在空间；优化低维向量上的旋转位置嵌入的频率采样机制；并进行对比实验分析减少Key与Value的影响。

Result: 实验验证了在Grouped Query Attention和非Grouped Query Attention模型中，该方法能有效减少KV缓存，同时提高推理效率并保持稳定性能。

Conclusion: KV-Latent方法显著提升了语言模型的推理效率，并提出了在KV缓存优化和构建高效LLM方面的新可能性。

Abstract: Large language models (LLMs) based on Transformer Decoders have become the
preferred choice for conversational generative AI. Despite the overall
superiority of the Decoder architecture, the gradually increasing Key-Value
(KV) cache during inference has emerged as a primary efficiency bottleneck,
both in aspects of memory consumption and data transfer bandwidth limitations.
To address these challenges, we propose a paradigm called KV-Latent. By
down-sampling the Key-Value vector dimensions into a latent space, we can
significantly reduce the KV Cache footprint and improve inference speed, only
with a small amount of extra training, less than 1\% of pre-training takes.
Besides, we enhanced the stability of Rotary Positional Embedding applied on
lower-dimensional vectors by modifying its frequency sampling mechanism,
avoiding noise introduced by higher frequencies while retaining position
attenuation. Our experiments, including both models with Grouped Query
Attention and those without, have yielded satisfactory results. Finally, we
conducted comparative experiments to study the impact of separately reducing
Key and Value components on model's performance. Our approach allows for the
construction of more efficient language model systems, and opens the new
possibility on KV Cache saving and efficient LLMs. Our code is available at
https://github.com/ShiLuohe/KV-Latent.

</details>


### [99] [FMC: Formalization of Natural Language Mathematical Competition Problems](https://arxiv.org/abs/2507.11275)
*Jiaxuan Xie,Chengwu Liu,Ye Yuan,Siqi Li,Zhiping Xiao,Ming Zhang*

Main category: cs.CL

TL;DR: 研究提出了一种基于大语言模型和错误反馈的自动形式化管道，并构建了一个涵盖3922个自然语言数学问题和9787个Lean形式化的奥林匹克级数据集作为验证。


<details>
  <summary>Details</summary>
Motivation: 推动形式化数学推理的发展需要高效精准的自动形式化方法及包含自然语言数学问题的大规模数据集。

Method: 提出了一种通过大语言模型和错误反馈的自动形式化管道，这种方法是完全自动化且不需要训练的，还构建了一个包含自然语言与Lean形式化对应的数据集。

Result: 所得的数据集中64.46%的内容被评估为高于平均质量，并通过实验表明少样本学习、错误反馈以及增加采样次数可提升自动形式化质量。

Conclusion: 所提出的数据集具有挑战性，为自动定理证明器提供了一个重要的基准，同时验证了所提出的管道和大语言模型在形式化及推理能力上的潜力。

Abstract: Efficient and accurate autoformalization methods, which leverage large-scale
datasets of extensive natural language mathematical problems to construct
formal language datasets, are key to advancing formal mathematical reasoning.
In this paper, we propose an autoformalization pipeline based on large language
models with error feedback, achieving a fully automatic and training-free
formalization approach. Using this pipeline, we curate an Olympiad-level
dataset aligning natural language problems with Lean formalizations. The
dataset comprises $3,922$ mathematical problems in natural language and $9,787$
in Lean, of which $64.46\%$ were assessed as at least above-average quality,
making it suitable as a benchmark for automated theorem provers. Additionally,
we investigate the formalization and reasoning capabilities of various LLMs and
empirically demonstrate that few-shot learning, error feedback, and increasing
sampling numbers enhance the autoformalization process. Experiments of three
automated theorem provers on the \dataset\ dataset also highlight its
challenging nature and its value as a benchmark for formal reasoning tasks.

</details>


### [100] [Fine-Grained Chinese Hate Speech Understanding: Span-Level Resources, Coded Term Lexicon, and Enhanced Detection Frameworks](https://arxiv.org/abs/2507.11292)
*Zewen Bai,Liang Yang,Shengdi Yin,Yuanyuan Sun,Hongfei Lin*

Main category: cs.CL

TL;DR: 提出了一种基于SPAN级别的中文仇恨言论数据集和新的模型方法，改善仇恨言论检测和解释性。


<details>
  <summary>Details</summary>
Motivation: 当前中文仇恨言论检测研究相较于其他语言滞后，尤其在深度语义理解和解释性方面缺乏支持。

Method: 引入首个SPAN级中文仇恨言论数据集（STATE ToxiCN），研究LLMs对仇恨语义的解释能力，并提出将标注词典融入模型的新方法。

Result: 新方法显著增强了仇恨言论检测性能，为中文仇恨言论研究提供了新的数据资源和视角。

Conclusion: 通过改进数据集与方法融合，促进了中文仇恨言论检测的语义理解及其解释性研究的进展。

Abstract: The proliferation of hate speech has inflicted significant societal harm,
with its intensity and directionality closely tied to specific targets and
arguments. In recent years, numerous machine learning-based methods have been
developed to detect hateful comments on online platforms automatically.
However, research on Chinese hate speech detection lags behind, and
interpretability studies face two major challenges: first, the scarcity of
span-level fine-grained annotated datasets limits models' deep semantic
understanding of hate speech; second, insufficient research on identifying and
interpreting coded hate speech restricts model explainability in complex
real-world scenarios. To address these, we make the following contributions:
(1) We introduce the Span-level Target-Aware Toxicity Extraction dataset (STATE
ToxiCN), the first span-level Chinese hate speech dataset, and evaluate the
hate semantic understanding of existing models using it. (2) We conduct the
first comprehensive study on Chinese coded hate terms, LLMs' ability to
interpret hate semantics. (3) We propose a method to integrate an annotated
lexicon into models, significantly enhancing hate speech detection performance.
Our work provides valuable resources and insights to advance the
interpretability of Chinese hate speech detection research.

</details>


### [101] [Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving Patient-Doctor Communication in Romanian](https://arxiv.org/abs/2507.11299)
*Andrei Niculae,Adrian Cosma,Cosmin Dumitrache,Emilian Rǎdoi*

Main category: cs.CL

TL;DR: 本文提出了Dr.Copilot系统，用于帮助罗马尼亚医生在文本远程医疗中改进书面回复的呈现质量，而非医学正确性。


<details>
  <summary>Details</summary>
Motivation: 在文本远程医疗中，医患互动的建议质量常因表达方式而受到评价，亟需提升医嘱的呈现方式。

Method: Dr.Copilot由三个大型语言模型代理和DSPy自动优化的提示组成，基于罗马尼亚低资源数据设计并开放模型权重，提供实时的具体反馈。

Result: 通过对41名医生的实地部署，用户评价和回复质量得到显著提升。

Conclusion: Dr.Copilot是首个在罗马尼亚医疗场景中部署的大型语言模型系统，展示了其在改进医患互动中的潜力。

Abstract: Text-based telemedicine has become increasingly common, yet the quality of
medical advice in doctor-patient interactions is often judged more on how
advice is communicated rather than its clinical accuracy. To address this, we
introduce Dr.Copilot , a multi-agent large language model (LLM) system that
supports Romanian-speaking doctors by evaluating and enhancing the presentation
quality of their written responses. Rather than assessing medical correctness,
Dr.Copilot provides feedback along 17 interpretable axes. The system comprises
of three LLM agents with prompts automatically optimized via DSPy. Designed
with low-resource Romanian data and deployed using open-weight models, it
delivers real-time specific feedback to doctors within a telemedicine platform.
Empirical evaluations and live deployment with 41 doctors show measurable
improvements in user reviews and response quality, marking one of the first
real-world deployments of LLMs in Romanian medical settings.

</details>


### [102] [Internal Value Alignment in Large Language Models through Controlled Value Vector Activation](https://arxiv.org/abs/2507.11316)
*Haoran Jin,Meng Li,Xiting Wang,Zhihao Xu,Minlie Huang,Yantao Jia,Defu Lian*

Main category: cs.CL

TL;DR: 本文提出了一种名为ConVA的控制值向量激活方法，用于直接调整大型语言模型的内部价值以实现价值对齐，实验显示其在控制价值方面效果最佳且无损模型性能。


<details>
  <summary>Details</summary>
Motivation: 通过直接调整大型语言模型内在价值，提高其清晰性、透明性及适应性，解决当前对齐挑战。

Method: 提出了一种控制值向量激活（ConVA）的新方法，包括上下文控制值向量识别和门控值向量激活技术，以实现精准、最低程度的价值控制而不牺牲模型性能。

Result: 实验表明，ConVA在10种基本价值的控制中成功率最高，同时模型性能和流畅性未受到影响；且能保持针对恶意输入仍具目标价值。

Conclusion: 所提方法ConVA能够有效实现大型语言模型的价值对齐，为未来的发展提供了新的方向，并强调了其适用于恶意或对立场景的鲁棒性。

Abstract: Aligning Large Language Models (LLMs) with human values has attracted
increasing attention since it provides clarity, transparency, and the ability
to adapt to evolving scenarios. In this paper, we introduce a Controlled Value
Vector Activation (ConVA) method that directly aligns the internal values of
LLMs by interpreting how a value is encoded in their latent representations and
modifies relevant activations to ensure consistent values in LLMs. To ensure an
accurate and unbiased interpretation, we propose a context-controlled value
vector identification method. To consistently control values without
sacrificing model performance, we introduce a gated value vector activation
method for effective and minimum degree of value control. Experiments show that
our method achieves the highest control success rate across 10 basic values
without hurting LLM performance and fluency, and ensures target values even
with opposite and potentially malicious input prompts. Source code and data are
available at~ https://github.com/hr-jin/ConVA.

</details>


### [103] [Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge](https://arxiv.org/abs/2507.11330)
*Wenqing Wu,Chengzhi Zhang,Yi Zhao*

Main category: cs.CL

TL;DR: 论文提出结合大型语言模型（LLM）和人类专家的知识与能力，利用预训练语言模型（PLMs）预测学术论文中方法的新颖性，通过文本引导的稀疏注意力融合模块实现优于传统方法的效果。


<details>
  <summary>Details</summary>
Motivation: 当前新颖性评估方法存在局限：专家的知识有限，引用组合法效果存疑，且无法明确独特引用是否真正衡量新颖性。因此，研究旨在结合LLM的知识优势与人类判断力解决这些问题。

Method: 通过提取同行评审报告中与学术论文新颖性相关的句子，并利用LLM总结论文的方法论部分，再用这些信息微调PLMs；设计了具有稀疏注意力的文本引导融合模块以整合人类与LLM知识。

Result: 系统性实验表明，与大量基线方法相比，所提出方法在预测方法新颖性方面表现更为出色。

Conclusion: 结合人类和LLM的知识与能力可以改进学术论文方法新颖性评估，提出的融合模块和方法显著提升预测性能，为相关领域提供了新的研究思路。

Abstract: Novelty is a crucial criterion in the peer review process for evaluating
academic papers. Traditionally, it's judged by experts or measure by unique
reference combinations. Both methods have limitations: experts have limited
knowledge, and the effectiveness of the combination method is uncertain.
Moreover, it's unclear if unique citations truly measure novelty. The large
language model (LLM) possesses a wealth of knowledge, while human experts
possess judgment abilities that the LLM does not possess. Therefore, our
research integrates the knowledge and abilities of LLM and human experts to
address the limitations of novelty assessment. The most common novelty in
academic papers is the introduction of new methods. In this paper, we propose
leveraging human knowledge and LLM to assist pretrained language models (PLMs,
e.g. BERT etc.) in predicting the method novelty of papers. Specifically, we
extract sentences related to the novelty of the academic paper from peer review
reports and use LLM to summarize the methodology section of the academic paper,
which are then used to fine-tune PLMs. In addition, we have designed a
text-guided fusion module with novel Sparse-Attention to better integrate human
and LLM knowledge. We compared the method we proposed with a large number of
baselines. Extensive experiments demonstrate that our method achieves superior
performance.

</details>


### [104] [What is the Best Process Model Representation? A Comparative Analysis for Process Modeling with Large Language Models](https://arxiv.org/abs/2507.11356)
*Alexis Brissard,Frédéric Cuppens,Amal Zouaq*

Main category: cs.CL

TL;DR: 本文首次对多种过程建模表示(PMRs)进行系统比较，引入了包含9种PMRs的PMo数据集并进行评估。


<details>
  <summary>Details</summary>
Motivation: 当前不同的PMR结构差异较大，使用和评估困难，缺乏系统对比研究。

Method: 通过构建PMo数据集，并从两个维度（LLM适用性和PMG表现）评价多种PMR。

Result: Mermaid在六个PMo标准中表现最佳，而BPMN文本在流程元素相似性方面取得了PMG的最好结果。

Conclusion: 对比分析显示，不同PMR在LLM应用和生成效果中表现有差异，为未来选择和优化PMR提供了参考。

Abstract: Large Language Models (LLMs) are increasingly applied for Process Modeling
(PMo) tasks such as Process Model Generation (PMG). To support these tasks,
researchers have introduced a variety of Process Model Representations (PMRs)
that serve as model abstractions or generation targets. However, these PMRs
differ widely in structure, complexity, and usability, and have never been
systematically compared. Moreover, recent PMG approaches rely on distinct
evaluation strategies and generation techniques, making comparison difficult.
This paper presents the first empirical study that evaluates multiple PMRs in
the context of PMo with LLMs. We introduce the PMo Dataset, a new dataset
containing 55 process descriptions paired with models in nine different PMRs.
We evaluate PMRs along two dimensions: suitability for LLM-based PMo and
performance on PMG. \textit{Mermaid} achieves the highest overall score across
six PMo criteria, whereas \textit{BPMN text} delivers the best PMG results in
terms of process element similarity.

</details>


### [105] [Addressing Data Imbalance in Transformer-Based Multi-Label Emotion Detection with Weighted Loss](https://arxiv.org/abs/2507.11384)
*Xia Cui*

Main category: cs.CL

TL;DR: 本文研究将简单加权损失函数应用于Transformer模型，以解决SemEval-2025任务的数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 通过动态调整类权重，提升模型对少数类情绪的检测性能，避免传统重采样方法的计算负担。

Method: 采用动态加权损失函数，并在BRIGHTER数据集上评估多个Transformer架构（BERT、RoBERTa、BART）。

Result: 加权损失函数提高了高频情绪类别的性能，但对少数类别的提升有限。

Conclusion: 动态加权损失函数有效应对多标签情绪检测中的挑战，但仍需改善少数类检测表现。

Abstract: This paper explores the application of a simple weighted loss function to
Transformer-based models for multi-label emotion detection in SemEval-2025
Shared Task 11. Our approach addresses data imbalance by dynamically adjusting
class weights, thereby enhancing performance on minority emotion classes
without the computational burden of traditional resampling methods. We evaluate
BERT, RoBERTa, and BART on the BRIGHTER dataset, using evaluation metrics such
as Micro F1, Macro F1, ROC-AUC, Accuracy, and Jaccard similarity coefficients.
The results demonstrate that the weighted loss function improves performance on
high-frequency emotion classes but shows limited impact on minority classes.
These findings underscore both the effectiveness and the challenges of applying
this approach to imbalanced multi-label emotion detection.

</details>


### [106] [DCR: Quantifying Data Contamination in LLMs Evaluation](https://arxiv.org/abs/2507.11405)
*Cheng Xu,Nan Yan,Shuhao Guan,Changhong Jin,Yuke Mei,Yibing Guo,M-Tahar Kechadi*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型中的基准数据污染问题，并提出了一种DCR框架，用以识别和量化污染并调整性能评估。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，数据污染成为一个关键问题，因为它可能导致性能指标通胀，影响模型真实泛化能力的评估。

Method: 论文提出了一个名为Data Contamination Risk (DCR) 的框架，基于模糊推理系统，评估四个粒度的污染级别，生成DCR因子，对模型的准确性进行污染感知的调整。

Result: 在对9个大语言模型的实验中，DCR框架可将污染调节后的性能精度误差调整至4%以内，有效诊断污染严重性。

Conclusion: 该框架提高了基准测试的公平性和可信度，是一种高效且透明的工具，可常规化地融入评估流程。

Abstract: The rapid advancement of large language models (LLMs) has heightened concerns
about benchmark data contamination (BDC), where models inadvertently memorize
evaluation data, inflating performance metrics and undermining genuine
generalization assessment. This paper introduces the Data Contamination Risk
(DCR) framework, a lightweight, interpretable pipeline designed to detect and
quantify BDC across four granular levels: semantic, informational, data, and
label. By synthesizing contamination scores via a fuzzy inference system, DCR
produces a unified DCR Factor that adjusts raw accuracy to reflect
contamination-aware performance. Validated on 9 LLMs (0.5B-72B) across
sentiment analysis, fake news detection, and arithmetic reasoning tasks, the
DCR framework reliably diagnoses contamination severity and with accuracy
adjusted using the DCR Factor to within 4% average error across the three
benchmarks compared to the uncontaminated baseline. Emphasizing computational
efficiency and transparency, DCR provides a practical tool for integrating
contamination assessment into routine evaluations, fostering fairer comparisons
and enhancing the credibility of LLM benchmarking practices.

</details>


### [107] [EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes](https://arxiv.org/abs/2507.11407)
*LG AI Research,:,Kyunghoon Bae,Eunbi Choi,Kibong Choi,Stanley Jungkyu Choi,Yemuk Choi,Kyubeen Han,Seokhee Hong,Junwon Hwang,Taewan Hwang,Joonwon Jang,Hyojin Jeon,Kijeong Jeon,Gerrard Jeongwon Jo,Hyunjik Jo,Jiyeon Jung,Euisoon Kim,Hyosang Kim,Jihoon Kim,Joonkee Kim,Seonghwan Kim,Soyeon Kim,Sunkyoung Kim,Yireun Kim,Yongil Kim,Youchul Kim,Edward Hwayoung Lee,Gwangho Lee,Haeju Lee,Honglak Lee,Jinsik Lee,Kyungmin Lee,Sangha Park,Young Min Paik,Yongmin Park,Youngyong Park,Sanghyun Seo,Sihoon Yang,Heuiyeen Yeen,Sihyuk Yi,Hyeongu Yun*

Main category: cs.CL

TL;DR: EXAONE 4.0结合了非推理模式与推理模式，具备增强的易用性和高级推理能力，支持英语、西班牙语和韩语。


<details>
  <summary>Details</summary>
Motivation: 为应对代理AI时代的需求，提供具备强大工具使用和多语言支持能力的模型。

Method: 开发了两种不同尺寸的模型（32B和1.2B），并提升性能，使其在同类模型中表现优越。

Result: EXAONE 4.0在公开模型中表现卓越，且具备与前沿类模型的竞争力，并免费提供给研究用途。

Conclusion: EXAONE 4.0成功结合易用性和性能，支持多语言及先进推理能力，展示了面向代理AI时代的巨大潜力。

Abstract: This technical report introduces EXAONE 4.0, which integrates a Non-reasoning
mode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5
and the advanced reasoning abilities of EXAONE Deep. To pave the way for the
agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool
use, and its multilingual capabilities are extended to support Spanish in
addition to English and Korean. The EXAONE 4.0 model series consists of two
sizes: a mid-size 32B model optimized for high performance, and a small-size
1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates
superior performance compared to open-weight models in its class and remains
competitive even against frontier-class models. The models are publicly
available for research purposes and can be easily downloaded via
https://huggingface.co/LGAI-EXAONE.

</details>


### [108] [KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?](https://arxiv.org/abs/2507.11408)
*Soumadeep Saha,Akshay Chaturvedi,Saptarshi Saha,Utpal Garain,Nicholas Asher*

Main category: cs.CL

TL;DR: 本文引入了一种名为Causal CoT Graphs（CCGs）的工具，通过提取推理路径中的因果依赖性，研究大语言模型中链式推理提升性能的机制。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型中链式推理链提升性能的机制，为进一步探索模型的推理过程提供新的方法。

Method: 提出了CCGs方法，通过从推理路径中提取定向无环图，分析因果依赖性。同时构建KisMATH数据集，包括1671个数学推理问题及其对应的CCGs，使用15个开源权重LLMs进行了实验分析。

Result: 实验表明，CCGs中的推理节点对最终答案起中介作用，LLMs倾向于内在化结构与CCGs类似的推理路径。

Conclusion: KisMATH提供了一种新途径，用于干预和研究链式推理的作用，并为理解大语言模型推理能力铺平了道路。

Abstract: Chain-of-thought traces have been shown to improve performance of large
language models in a plethora of reasoning tasks, yet there is no consensus on
the mechanism through which this performance boost is achieved. To shed more
light on this, we introduce Causal CoT Graphs (CCGs), which are directed
acyclic graphs automatically extracted from reasoning traces that model
fine-grained causal dependencies in the language model output. A collection of
$1671$ mathematical reasoning problems from MATH500, GSM8K and AIME, and their
associated CCGs are compiled into our dataset -- \textbf{KisMATH}. Our detailed
empirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in
the CCG are mediators for the final answer, a condition necessary for
reasoning; and (ii) LLMs emphasise reasoning paths given by the CCG, indicating
that models internally realise structures akin to our graphs. KisMATH enables
controlled, graph-aligned interventions and opens up avenues for further
investigation into the role of chain-of-thought in LLM reasoning.

</details>


### [109] [Seq vs Seq: An Open Suite of Paired Encoders and Decoders](https://arxiv.org/abs/2507.11412)
*Orion Weller,Kathryn Ricci,Marc Marone,Antoine Chaffin,Dawn Lawrie,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 该论文提出并研究了Ettin模型套件，包括编码器模型和解码器模型，范围从1700万到10亿参数。


<details>
  <summary>Details</summary>
Motivation: 为了解决编码器与解码器模型客观对比中的数据量与训练方法不一致的问题。

Method: 开发了一套开源的Ettin模型套件，对编码器和解码器模型使用相同配方训练，模型参数从1700万到10亿不等，训练数据达到2兆tokens。

Result: 发现编码器在分类与检索任务中表现优越，解码器在生成任务中表现卓越。此外，通过训练转换模型的跨任务适配效果较差（如400M编码器优于1B解码器的MNLI任务）。

Conclusion: 对于编码与解码的专业任务，应优先选择专用模型而非通过跨任务训练适配；研究还提供了所有开源数据及训练过程，为未来研究提供支持。

Abstract: The large language model (LLM) community focuses almost exclusively on
decoder-only language models, since they are easier to use for text generation.
However, a large subset of the community still uses encoder-only models for
tasks such as classification or retrieval. Previous work has attempted to
compare these architectures, but is forced to make comparisons with models that
have different numbers of parameters, training techniques, and datasets. We
introduce the SOTA open-data Ettin suite of models: paired encoder-only and
decoder-only models ranging from 17 million parameters to 1 billion, trained on
up to 2 trillion tokens. Using the same recipe for both encoder-only and
decoder-only models produces SOTA recipes in both categories for their
respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as
decoders. Like previous work, we find that encoder-only models excel at
classification and retrieval tasks while decoders excel at generative tasks.
However, we show that adapting a decoder model to encoder tasks (and vice
versa) through continued training is subpar compared to using only the reverse
objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa
for generative tasks). We open-source all artifacts of this study including
training data, training order segmented by checkpoint, and 200+ checkpoints to
allow future work to analyze or extend all aspects of training.

</details>


### [110] [Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?](https://arxiv.org/abs/2507.11423)
*Yanjian Zhang,Guillaume Wisniewski,Nadi Tomeh,Thierry Charnois*

Main category: cs.CL

TL;DR: 本研究探讨了通过提示控制大语言模型（LLMs）的推理策略及其对逻辑问题解决能力的影响。


<details>
  <summary>Details</summary>
Motivation: 当前的LLMs倾向于使用单一的推理策略，这在多样化的推理挑战中可能会限制其效果。研究动机是探索能否通过提示来改善其策略选择能力，从而优化推理表现。

Method: 通过实验评估不同提示方式对LLMs推理策略的影响，并提出指导模型策略选择的新方法。

Result: 实验表明，单一策略无法始终提高准确性，但若模型能够自适应地选择最佳策略，其表现可能得到优化。

Conclusion: 提示可以显著提升LLMs的策略选择能力，为改进其推理能力提供了新路径。

Abstract: Human reasoning involves different strategies, each suited to specific
problems. Prior work shows that large language model (LLMs) tend to favor a
single reasoning strategy, potentially limiting their effectiveness in diverse
reasoning challenges. In this work, we investigate whether prompting can
control LLMs reasoning strategies and assess its impact on logical
problem-solving. While our experiments show that no single strategy
consistently improves accuracy, performance could be enhanced if models could
adaptively choose the optimal strategy. We propose methods to guide LLMs in
strategy selection, highlighting new ways to refine their reasoning abilities.

</details>


### [111] [HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong](https://arxiv.org/abs/2507.11502)
*Sirui Han,Junqi Zhu,Ruiyuan Zhang,Yike Guo*

Main category: cs.CL

TL;DR: 本文介绍了一种名为HKGAI-V1的基础主权大型语言模型，其专为香港多语言环境、独特社会法律背景以及文化价值考量开发，并在设计中强调了区域特定的对齐和安全框架。


<details>
  <summary>Details</summary>
Motivation: 推动香港建立本地化的价值对齐型AI基础设施，满足多语言需求、特殊社会法律背景以及文化价值的要求。

Method: 基于DeepSeek架构，引入全参数微调流程，整合检索增强生成（RAG）系统，并设计了Adversarial HK Value Benchmark工具进行对齐评估。

Result: 开发出HKGAI-V1模型，能够优于通用模型处理香港特定查询，同时建立对区域重要领域如公共服务、法律系统和教育的AI应用控制。

Conclusion: 展示了如何开发出根植于地方特性的高级AI系统，提供了一种可复制的区域性AI系统开发框架。

Abstract: This paper presents the development of HKGAI-V1, a foundational sovereign
large language model (LLM), developed as part of an initiative to establish
value-aligned AI infrastructure specifically tailored for Hong Kong. Addressing
the region's unique multilingual environment (Cantonese, Mandarin, and
English), its distinct socio-legal context under the "one country, two systems"
framework, and specific local cultural and value considerations, the model is
built upon the DeepSeek architecture and systematically aligned with regional
norms through a multifaceted full parameter fine-tuning process. It is further
integrated with a retrieval-augmented generation (RAG) system to ensure timely
and factually grounded information access. The core contribution lies in the
design and implementation of a comprehensive, region-specific AI alignment and
safety framework, demonstrated through two key achievements: 1) The successful
development of HKGAI-V1 itself - which outper-forms general-purpose models in
handling Hong Kong-specific culturally sensitive queries, and embodies a
"governance-embedded" approach to digital sovereignty - empowers Hong Kong to
exercise control over AI applications in critical sectors including public
services, legal systems, and edu-cation. 2) The development of the proprietary
Adversarial HK Value Benchmark, a rigorous tool for evaluating model alignment
with local ethical and legal stand-ards under challenging conditions. By
documenting these achievements, the paper provides not only a technological
artifact but also a replicable blueprint for developing advanced, regionally
focused AI systems deeply rooted in their local identities.

</details>


### [112] [Real-World Summarization: When Evaluation Reaches Its Limits](https://arxiv.org/abs/2507.11508)
*Patrícia Schmidtová,Ondřej Dušek,Saad Mahamood*

Main category: cs.CL

TL;DR: 研究探讨了在酒店特色摘要的生成中，如何评估结果与输入数据的信实性，并发现简单的单词重叠指标表现不俗，但LLM在评价中仍存在不可靠性。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是解决在酒店特色摘要生成中，如何有效评价生成内容与输入数据的信实性问题。

Method: 通过人工评估、错误分类以及段落级标注，比较传统指标、可训练方法和LLM作为评估者的方法的表现。

Result: 简单指标如词汇重叠与人工评价相关性较高（Spearman为0.63），而LLM生成的摘要质量高但在评估中不可靠，容易出现过多或过少标注的问题。

Conclusion: 简单评价指标在跨领域数据评估中表现优异，LLM尽管生成能力强但在评估任务中仍需慎重，错误及无法核查的信息对业务影响较大。

Abstract: We examine evaluation of faithfulness to input data in the context of hotel
highlights: brief LLM-generated summaries that capture unique features of
accommodations. Through human evaluation campaigns involving categorical error
assessment and span-level annotation, we compare traditional metrics, trainable
methods, and LLM-as-a-judge approaches. Our findings reveal that simpler
metrics like word overlap correlate surprisingly well with human judgments
(Spearman correlation rank of 0.63), often outperforming more complex methods
when applied to out-of-domain data. We further demonstrate that while LLMs can
generate high-quality highlights, they prove unreliable for evaluation as they
tend to severely under- or over-annotate. Our analysis of real-world business
impacts shows incorrect and non-checkable information pose the greatest risks.
We also highlight challenges in crowdsourced evaluations.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [113] [SAMEP: A Secure Protocol for Persistent Context Sharing Across AI Agents](https://arxiv.org/abs/2507.10562)
*Hari Masoor*

Main category: cs.AI

TL;DR: 提出了SAMEP框架，解决AI代理在会话中的记忆、协作和知识共享问题，表明其在多个领域有效并符合隐私与安全要求。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理架构因临时记忆限制，无法跨会话和代理边界协作与知识共享。

Method: 提出SAMEP框架，包括分布式记忆库、基于向量的语义搜索、AES-256-GCM加密控制和标准化API，兼容现有通信协议。

Result: 在实验中实现了73%的冗余计算减少，89%的上下文相关性提升，并完全符合隐私与合规性要求。

Conclusion: SAMEP框架不仅提高了计算效率和上下文相关性，还保障了隐私与安全，为持续性和协作性的AI代理系统提供了一种全新模式。

Abstract: Current AI agent architectures suffer from ephemeral memory limitations,
preventing effective collaboration and knowledge sharing across sessions and
agent boundaries. We introduce SAMEP (Secure Agent Memory Exchange Protocol), a
novel framework that enables persistent, secure, and semantically searchable
memory sharing among AI agents. Our protocol addresses three critical
challenges: (1) persistent context preservation across agent sessions, (2)
secure multi-agent collaboration with fine-grained access control, and (3)
efficient semantic discovery of relevant historical context. SAMEP implements a
distributed memory repository with vector-based semantic search, cryptographic
access controls (AES-256-GCM), and standardized APIs compatible with existing
agent communication protocols (MCP, A2A). We demonstrate SAMEP's effectiveness
across diverse domains including multi-agent software development, healthcare
AI with HIPAA compliance, and multi-modal processing pipelines. Experimental
results show 73% reduction in redundant computations, 89% improvement in
context relevance scores, and complete compliance with regulatory requirements
including audit trail generation. SAMEP enables a new paradigm of persistent,
collaborative AI agent ecosystems while maintaining security and privacy
guarantees.

</details>


### [114] [AI Mother Tongue: Self-Emergent Communication in MARL via Endogenous Symbol Systems](https://arxiv.org/abs/2507.10566)
*Hung Ming Liu*

Main category: cs.AI

TL;DR: 该研究通过AI Mother Tongue框架展示了无需外部引导偏置的符号通信自发性与语义收敛能力。


<details>
  <summary>Details</summary>
Motivation: 质疑传统方法中施加人工引导偏置的必要性，探讨经由内生符号系统实现通信的可能性。

Method: 通过基于向量量化变分自编码器(VQ-VAE)的AI Mother Tongue框架，进行实验分析符号通信的自然生成和语义收敛。

Result: AI Mother Tongue框架实现了无需外部引导偏置的符号通信，同时验证了符号使用的幂律分布及其语义收缩特性。

Conclusion: 该方法展现了在无需外部人工干预下实现自发性高效通信的潜力，为符号主义与连接主义桥接提供了新视角，并提出了相关的三个理论见解。

Abstract: In Decentralized Multi-Agent Reinforcement Learning (MARL), the development
of Emergent Communication has long been constrained by the ``Joint Exploration
Dilemma'', leading agents to fall into a ``Communication Vacuum Equilibrium'' .
Traditional methods address this by introducing inductive biases to facilitate
communication emergence . This study fundamentally questions whether such
artificial inductive biases are, in fact, over-engineering. Through experiments
with the ``AI Mother Tongue'' (AIM) framework, based on a Vector Quantized
Variational Autoencoder (VQ-VAE), we demonstrate that when agents possess an
endogenous symbol system, their neural representations naturally exhibit
spontaneous semantic compression and Nash equilibrium-driven semantic
convergence, achieving effective symbolic communication without external
inductive biases. This aligns with recent neuroscience findings suggesting that
the human brain does not directly use human language for internal thought , and
resonates with research on ``soft thinking'' capabilities in Large Language
Models (LLMs) . Compared to traditional explicit communication methods, AIM
demonstrates stronger generality and efficiency. The interpretable analysis
toolkit developed in this study confirms that symbol usage exhibits a
significant power-law distribution, leading to three major theoretical
insights: the ``Neural Communication Hypothesis'', the ``Tool-First
Principle'', and the ``Semantic Interpretability Paradigm''. Future research
will explore the integration of Hierarchical Quantized Variational Autoencoders
(HQ-VAE) to enhance AIM's complex expressive capabilities and investigate the
potential for ``Reinforcement Learning (RL) Low-Level Pre-training''. This
discovery offers new avenues for bridging symbolism and connectionism.

</details>


### [115] [Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning](https://arxiv.org/abs/2507.10571)
*Konstantinos I. Roumeliotis,Ranjan Sapkota,Manoj Karkee,Nikolaos D. Tselikas*

Main category: cs.AI

TL;DR: 该论文提出了一种新颖的模块化Agentic AI视觉分类框架，结合多模态代理、非视觉推理协调器和一种基于检索增强生成（RAG）的模块，旨在提升零样本任务中的信任度，应用于苹果叶疾病诊断，并开源了所有源码。


<details>
  <summary>Details</summary>
Motivation: 探索如何提升智能代理在零样本任务中的信任度，特别是针对跨视觉和语言的多模态理解。

Method: 提出了一个整合通用多模态代理、信任校准（包括置信校准和基于CLIP的图像检索）及再评估循环的框架，应用于多代理诊断场景中。采用了三种不同配置进行实验，包括零样本设置、微调代理设置，以及信任校准增强设置。

Result: 在零样本设置中，通过信任感知的协调和RAG模块的使用，准确率提升了77.94%，达到了85.63%的整体准确率。同时结合图像检索，改善了代理过度自信的问题。

Conclusion: 该系统分离了视觉感知（视觉代理）和元推理（协调器），实现了可扩展且可解释的多代理AI框架，并能推广到诊断学、生物学等信任至关重要的领域。

Abstract: Modern Artificial Intelligence (AI) increasingly relies on multi-agent
architectures that blend visual and language understanding. Yet, a pressing
challenge remains: How can we trust these agents especially in zero-shot
settings with no fine-tuning? We introduce a novel modular Agentic AI visual
classification framework that integrates generalist multimodal agents with a
non-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG)
module. Applied to apple leaf disease diagnosis, we benchmark three
configurations: (I) zero-shot with confidence-based orchestration, (II)
fine-tuned agents with improved performance, and (III) trust-calibrated
orchestration enhanced by CLIP-based image retrieval and re-evaluation loops.
Using confidence calibration metrics (ECE, OCR, CCC), the orchestrator
modulates trust across agents. Our results demonstrate a 77.94\% accuracy
improvement in the zero-shot setting using trust-aware orchestration and RAG,
achieving 85.63\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL
displayed overconfidence. Furthermore, image-RAG grounded predictions with
visually similar cases, enabling correction of agent overconfidence via
iterative re-evaluation. The proposed system separates perception (vision
agents) from meta-reasoning (orchestrator), enabling scalable and interpretable
multi-agent AI. This blueprint is extensible to diagnostics, biology, and other
trust-critical domains. All models, prompts, results, and system components
including the complete software source code are openly released to support
reproducibility, transparency, and community benchmarking at Github:
https://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust

</details>


### [116] [Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning](https://arxiv.org/abs/2507.10624)
*Zheng Zhang*

Main category: cs.AI

TL;DR: 本文揭示了大型语言模型（LLMs）在符号推理、算术准确性和逻辑一致性上的系统性缺陷，提出了“计算分裂脑综合症”现象，即指令和执行路径在几何和功能上的分离。


<details>
  <summary>Details</summary>
Motivation: 分析和解释LLMs在表面流畅性与实际符号推理能力之间的差距，揭示其核心限制并框定当前模型能力的边界。

Method: 通过受控实验和结构分析，发现LLMs正确表述原则但无法可靠应用其原因。提出计算分裂脑综合症，分析其架构局限性以及几何路径分离现象。

Result: LLMs无法在数学操作和关系推理等领域实现稳定表现，表现出理想提示下行为的不稳定性，原因在于执行路径架构的限制。

Conclusion: LLMs现阶段是强大的模式匹配引擎，但缺乏原则性和结构化推理能力。这一研究为未来模型开发提供了方向，如元认知控制和结构化执行。

Abstract: Large Language Models (LLMs) display striking surface fluency yet
systematically fail at tasks requiring symbolic reasoning, arithmetic accuracy,
and logical consistency. This paper offers a structural diagnosis of such
failures, revealing a persistent gap between \textit{comprehension} and
\textit{competence}. Through controlled experiments and architectural analysis,
we demonstrate that LLMs often articulate correct principles without reliably
applying them--a failure rooted not in knowledge access, but in computational
execution. We term this phenomenon the computational \textit{split-brain
syndrome}, where instruction and action pathways are geometrically and
functionally dissociated. This core limitation recurs across domains, from
mathematical operations to relational inferences, and explains why model
behavior remains brittle even under idealized prompting. We argue that LLMs
function as powerful pattern completion engines, but lack the architectural
scaffolding for principled, compositional reasoning. Our findings delineate the
boundary of current LLM capabilities and motivate future models with
metacognitive control, principle lifting, and structurally grounded execution.
This diagnosis also clarifies why mechanistic interpretability findings may
reflect training-specific pattern coordination rather than universal
computational principles, and why the geometric separation between instruction
and execution pathways suggests limitations in neural introspection and
mechanistic analysis.

</details>


### [117] [Enhancing the Capabilities of Large Language Models for API calls through Knowledge Graphs](https://arxiv.org/abs/2507.10630)
*Ye Yang,Xue Xiao,Ping Yin,Taotao Xie*

Main category: cs.AI

TL;DR: 本文介绍了KG2data系统，该系统结合知识图谱、LLM、ReAct代理和工具使用技术，用于气象领域智能数据获取和查询处理，并在API调用准确性上表现优越。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型通过API调用工具的能力，特别是在知识密集领域（如气象学）中。

Method: 通过引入结合知识图谱作为持久记忆的KG2data系统，与其他对照系统对比，考察其在API调用中的性能表现。

Result: KG2data在三项指标上表现优异（1.43%, 0%, 88.57%），明显优于RAG2data（16%, 10%, 72.14%）和chat2data（7.14%, 8.57%, 71.43%）。

Conclusion: KG2data提供了一种适用于高知识需求领域的智能化、基于知识的问题解答和数据分析方案，且具有高适应性和低优化成本的优点。

Abstract: API calls by large language models (LLMs) offer a cutting-edge approach for
data analysis. However, their ability to effectively utilize tools via API
calls remains underexplored in knowledge-intensive domains like meteorology.
This paper introduces KG2data, a system that integrates knowledge graphs, LLMs,
ReAct agents, and tool-use technologies to enable intelligent data acquisition
and query handling in the meteorological field. Using a virtual API, we
evaluate API call accuracy across three metrics: name recognition failure,
hallucination failure, and call correctness. KG2data achieves superior
performance (1.43%, 0%, 88.57%) compared to RAG2data (16%, 10%, 72.14%) and
chat2data (7.14%, 8.57%, 71.43%). KG2data differs from typical LLM-based
systems by addressing their limited access to domain-specific knowledge, which
hampers performance on complex or terminology-rich queries. By using a
knowledge graph as persistent memory, our system enhances content retrieval,
complex query handling, domain-specific reasoning, semantic relationship
resolution, and heterogeneous data integration. It also mitigates the high cost
of fine-tuning LLMs, making the system more adaptable to evolving domain
knowledge and API structures. In summary, KG2data provides a novel solution for
intelligent, knowledge-based question answering and data analysis in domains
with high knowledge demands.

</details>


### [118] [From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents](https://arxiv.org/abs/2507.10644)
*Tatiana Petrova,Aleksandr Puzikov,Boris Bliznukov,Radu State*

Main category: cs.AI

TL;DR: 本文探讨了Web of Agents (WoA) 的历史演变及未来发展方向，通过构建四轴分类体系揭示了智能代理系统发展的连续性，并指出下一步研究需解决WoA中的社会技术挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的发展，代理型网页 (WoA) 的概念逐渐流行，它将静态的文档型网页转变成以用户代理为核心的环境。但现有研究分散在不同领域，需要全面梳理这一领域的演化历程。

Method: 本文提出了一个四轴分类体系（语义基础、通信范式、智能位置、发现机制），分析了从FIPA标准到现代协议（如A2A和MCP）的演进，揭示了代理智能系统发展的内在一致性和趋势。

Result: 研究表明，现代代理系统在‘智能位置’上出现了范式转移：从最初依靠外部数据或平台到在代理核心模型（如LLMs）中嵌入智能，这种转变是现代Agentic AI的基础。

Conclusion: 当前的研究和协议对于构建一个开放、健壮、可信的WoA生态系统至关重要，但并不足够，应关注去中心化身份、经济模型、安全性和治理等关键挑战，为WoA制定新的研究议程。

Abstract: The concept of the Web of Agents (WoA), which transforms the static,
document-centric Web into an environment of autonomous agents acting on users'
behalf, has attracted growing interest as large language models (LLMs) become
more capable. However, research in this area is still fragmented across
different communities. Contemporary surveys catalog the latest LLM-powered
frameworks, while the rich histories of Multi-Agent Systems (MAS) and the
Semantic Web are often treated as separate, legacy domains. This fragmentation
obscures the intellectual lineage of modern systems and hinders a holistic
understanding of the field's trajectory. We present the first comprehensive
evolutionary overview of the WoA. We show that modern protocols like A2A and
the MCP, are direct evolutionary responses to the well-documented limitations
of earlier standards like FIPA standards and OWL-based semantic agents. To
systematize this analysis, we introduce a four-axis taxonomy (semantic
foundation, communication paradigm, locus of intelligence, discovery
mechanism). This framework provides a unified analytical lens for comparing
agent architectures across all generations, revealing a clear line of descent
where others have seen a disconnect. Our analysis identifies a paradigm shift
in the 'locus of intelligence': from being encoded in external data (Semantic
Web) or the platform (MAS) to being embedded within the agent's core model
(LLM). This shift is foundational to modern Agentic AI, enabling the scalable
and adaptive systems the WoA has long envisioned. We conclude that while new
protocols are essential, they are insufficient for building a robust, open,
trustworthy ecosystem. Finally, we argue that the next research frontier lies
in solving persistent socio-technical challenges, and we map out a new agenda
focused on decentralized identity, economic models, security, and governance
for the emerging WoA.

</details>


### [119] [Parsing Musical Structure to Enable Meaningful Variations](https://arxiv.org/abs/2507.10740)
*Maziar Kanani,Sean O Leary,James McDermott*

Main category: cs.AI

TL;DR: 本文提出了一种基于规则的音乐生成方法，通过对现有旋律进行变异来生成新旋律。研究集中于爱尔兰传统音乐数据集中旋律音高序列的变化。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过变异生成与原始旋律相关的新音乐，并研究不同变异类型对旋律变化的效果与音乐性影响。

Method: 使用Sequitur算法解析旋律以生成文法结构，然后对文法进行19种可能的变异操作（如添加、移除、交换和反转）。变化后的文法再展开成新的旋律，并通过编辑距离、结构复杂性及长度分析旋律变化。

Result: 研究展示了经过多个变异后的旋律如何逐步改变，并分析了不同变异类型的效果。同时对最终生成旋律的音乐特性进行了审视。

Conclusion: 通过基于文法的变异方法，可以逐步生成与原旋律相关但具有新意的音乐，为音乐生成领域提供了一种新思路。

Abstract: This paper presents a novel rule-based approach for generating music by
varying existing tunes. We parse each tune to find the Pathway Assembly (PA) [
1], that is a structure representing all repetitions in the tune. The Sequitur
algorithm [2 ] is used for this. The result is a grammar. We then carry out
mutation on the grammar, rather than on a tune directly. There are potentially
19 types of mutations such as adding, removing, swapping or reversing parts of
the grammar that can be applied to the grammars. The system employs one of the
mutations randomly in this step to automatically manipulate the grammar.
Following the mutation, we need to expand the grammar which returns a new tune.
The output after 1 or more mutations will be a new tune related to the original
tune. Our study examines how tunes change gradually over the course of multiple
mutations. Edit distances, structural complexity and length of the tunes are
used to show how a tune is changed after multiple mutations. In addition, the
size of effect of each mutation type is analyzed. As a final point, we review
the musical aspect of the output tunes. It should be noted that the study only
focused on generating new pitch sequences. The study is based on an Irish
traditional tune dataset and a list of integers has been used to represent each
tune's pitch values.

</details>


### [120] [AI and the Net-Zero Journey: Energy Demand, Emissions, and the Potential for Transition](https://arxiv.org/abs/2507.10750)
*Pandu Devarakota,Nicolas Tsesmetzis,Faruk O. Alpak,Apurva Gala,Detlef Hohl*

Main category: cs.AI

TL;DR: 本文探讨人工智能在未来对温室气体(GHG)排放以及二氧化碳减排的影响，从短期到长期视角进行分析，指出人工智能短期内将导致能源消耗和排放增加，但长期可能通过优化流程显著减排。


<details>
  <summary>Details</summary>
Motivation: 为了解人工智能快速发展的同时对能源消耗和碳排放的影响，探讨其在设计和运用过程中能否对气候改善作出正面贡献。

Method: 对AI数据中心的能耗情景进行短期（2030年前）和长期（2035年及之后）影响分析，并研究AI如何在不同领域优化能源流程，实现减排。

Result: 从短期看，AI的发展将增加电力消耗和相应的CO2排放，但从长期看，AI可能通过自动化和优化各行业流程，显著降低碳排放并带来气候改善的潜力。

Conclusion: 尽管AI在发展初期对环境带来一定压力，但通过其长期优化及减排潜力，可对气候减缓起到积极作用，同时为社会和企业创造价值。

Abstract: Thanks to the availability of massive amounts of data, computing resources,
and advanced algorithms, AI has entered nearly every sector. This has sparked
significant investment and interest, particularly in building data centers with
the necessary hardware and software to develop and operate AI models and
AI-based workflows. In this technical review article, we present energy
consumption scenarios of data centers and impact on GHG emissions, considering
both near-term projections (up to 2030) and long-term outlook (2035 and
beyond). We address the quintessential question of whether AI will have a net
positive, neutral, or negative impact on CO2 emissions by 2035. Additionally,
we discuss AI's potential to automate, create efficient and disruptive
workflows across various fields related to energy production, supply and
consumption. In the near-term scenario, the growing demand for AI will likely
strain computing resources, lead to increase in electricity consumption and
therefore associated CO2 emissions. This is due to the power-hungry nature of
big data centers and the requirements for training and running of large and
complex AI models, as well as the penetration of AI assistant search and
applications for public use. However, the long-term outlook could be more
promising. AI has the potential to be a game-changer in CO2 reduction. Its
ability to further automate and optimize processes across industries, from
energy production to logistics, could significantly decrease our carbon
footprint. This positive impact is anticipated to outweigh the initial
emissions bump, creating value for businesses and society in areas where
traditional solutions have fallen short. In essence, AI might cause some
initial growing pains for the environment, but it has the potential to support
climate mitigation efforts.

</details>


### [121] [IoT Malware Network Traffic Detection using Deep Learning and GraphSAGE Models](https://arxiv.org/abs/2507.10758)
*Nikesh Prajapati,Bimal Karki,Saroj Gopali,Akbar Siami Namin*

Main category: cs.AI

TL;DR: 本文通过深度学习模型检测物联网中恶意攻击，重点评估了GraphSAGE、BERT、TCN、多头注意力机制及BI-LSTM等模型。


<details>
  <summary>Details</summary>
Motivation: 物联网中恶意流量具有时序性及多样性，需要高效的检测方法。

Method: 采取深度学习模型，如GraphSAGE、BERT、TCN等，结合实验评估其检测性能。

Result: 实验结果显示BERT性能最佳，准确率达到99.94%，其他模型如多头注意力机制和GraphSAGE也展现了部分优势。

Conclusion: BERT是最有效的模型，但考虑到时间和性能的平衡，不同模型有各自的应用场景。

Abstract: This paper intends to detect IoT malicious attacks through deep learning
models and demonstrates a comprehensive evaluation of the deep learning and
graph-based models regarding malicious network traffic detection. The models
particularly are based on GraphSAGE, Bidirectional encoder representations from
transformers (BERT), Temporal Convolutional Network (TCN) as well as Multi-Head
Attention, together with Bidirectional Long Short-Term Memory (BI-LSTM)
Multi-Head Attention and BI-LSTM and LSTM models. The chosen models
demonstrated great performance to model temporal patterns and detect feature
significance. The observed performance are mainly due to the fact that IoT
system traffic patterns are both sequential and diverse, leaving a rich set of
temporal patterns for the models to learn. Experimental results showed that
BERT maintained the best performance. It achieved 99.94% accuracy rate
alongside high precision and recall, F1-score and AUC-ROC score of 99.99% which
demonstrates its capabilities through temporal dependency capture. The
Multi-Head Attention offered promising results by providing good detection
capabilities with interpretable results. On the other side, the Multi-Head
Attention model required significant processing time like BI-LSTM variants. The
GraphSAGE model achieved good accuracy while requiring the shortest training
time but yielded the lowest accuracy, precision, and F1 score compared to the
other models

</details>


### [122] [Detecting AI Assistance in Abstract Complex Tasks](https://arxiv.org/abs/2507.10761)
*Tyler King,Nikolos Gurney,John H. Miller,Volkan Ustun*

Main category: cs.AI

TL;DR: 该论文提出在抽象任务中检测人工智能辅助的问题，通过适当的预处理，将其转化为可以由神经网络进行分类的形式，并提出了一种结合时间序列和图像数据的并行神经网络架构。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在文本生成、医疗诊断和自动驾驶等复杂任务中的普及，检测人工智能辅助的作用变得越来越重要，因为这对确保任务独立性和透明性尤为关键。

Method: 构建四种适合神经网络处理的数据图像形式，并增加一种时间序列形式编码用户的探索/利用行为，测试通过经典深度学习架构（包括CNN-RNN并行架构）评估这些预处理方法对AI辅助检测任务的效果。

Result: 实验结果表明，通过适当的预处理，可以充分利用神经网络和时间序列编码来在抽象任务中有效地检测AI辅助。

Conclusion: 编码时间和空间特性（尤其是通过时间序列和图像数据组合）对于提高AI辅助检测的效果是十分重要的，并为处理抽象任务提供了通用化的解决方案。

Abstract: Detecting assistance from artificial intelligence is increasingly important
as they become ubiquitous across complex tasks such as text generation, medical
diagnosis, and autonomous driving. Aid detection is challenging for humans,
especially when looking at abstract task data. Artificial neural networks excel
at classification thanks to their ability to quickly learn from and process
large amounts of data -- assuming appropriate preprocessing. We posit detecting
help from AI as a classification task for such models. Much of the research in
this space examines the classification of complex but concrete data classes,
such as images. Many AI assistance detection scenarios, however, result in data
that is not machine learning-friendly. We demonstrate that common models can
effectively classify such data when it is appropriately preprocessed. To do so,
we construct four distinct neural network-friendly image formulations along
with an additional time-series formulation that explicitly encodes the
exploration/exploitation of users, which allows for generalizability to other
abstract tasks. We benchmark the quality of each image formulation across three
classical deep learning architectures, along with a parallel CNN-RNN
architecture that leverages the additional time series to maximize testing
performance, showcasing the importance of encoding temporal and spatial
quantities for detecting AI aid in abstract tasks.

</details>


### [123] [Uncertainty-Informed Scheduling of Decision Points for Intelligent Mobile Health Interventions](https://arxiv.org/abs/2507.10798)
*Asim H. Gazi,Bhanu T. Gullapalli,Daiqi Gao,Benjamin M. Marlin,Vivek Shetty,Susan A. Murphy*

Main category: cs.AI

TL;DR: 本文提出SigmaScheduling，用于动态安排移动健康干预的决策时间点，以提高干预对时敏性行为的影响。


<details>
  <summary>Details</summary>
Motivation: 现有固定时间点决策方法不适用于行为规律不稳定的个体，导致干预效果降低。

Method: 通过SigmaScheduling，根据行为时间预测中的不确定性动态调整决策点位置，确保有效及时干预。

Result: 在涵盖68名参与者和为期10周的试验中，SigmaScheduling在多个情况下有效提升了干预前决策时间点的阳性率。

Conclusion: SigmaScheduling能显著提升习惯性、时间敏感行为的干预精度，为移动健康应用的精准化发展提供新的支持。

Abstract: Timely decision making is critical to the effectiveness of mobile health
(mHealth) interventions. At predefined timepoints called "decision points,"
intelligent mHealth systems such as just-in-time adaptive interventions
(JITAIs) estimate an individual's biobehavioral context from sensor or survey
data and determine whether and how to intervene. For interventions targeting
habitual behavior (e.g., oral hygiene), effectiveness often hinges on
delivering support shortly before the target behavior is likely to occur.
Current practice schedules decision points at a fixed interval (e.g., one hour)
before user-provided behavior times, and the fixed interval is kept the same
for all individuals. However, this one-size-fits-all approach performs poorly
for individuals with irregular routines, often scheduling decision points after
the target behavior has already occurred, rendering interventions ineffective.
In this paper, we propose SigmaScheduling, a method to dynamically schedule
decision points based on uncertainty in predicted behavior times. When behavior
timing is more predictable, SigmaScheduling schedules decision points closer to
the predicted behavior time; when timing is less certain, SigmaScheduling
schedules decision points earlier, increasing the likelihood of timely
intervention. We evaluated SigmaScheduling using real-world data from 68
participants in a 10-week trial of Oralytics, a JITAI designed to improve daily
toothbrushing. SigmaScheduling increased the likelihood that decision points
preceded brushing events in at least 70% of cases, preserving opportunities to
intervene and impact behavior. Our results indicate that SigmaScheduling can
advance precision mHealth, particularly for JITAIs targeting time-sensitive,
habitual behaviors such as oral hygiene or dietary habits.

</details>


### [124] [Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case](https://arxiv.org/abs/2507.10803)
*JaMor Hairston,Ritvik Ranjan,Sahithi Lakamana,Anthony Spadaro,Selen Bozkurt,Jeanmarie Perrone,Abeed Sarker*

Main category: cs.AI

TL;DR: 研究评估了大语言模型（LLMs）是否能复制专家驱动的社交媒体数据主题分析，发现GPT-4在少样本提示下表现最佳，某些主题的分布与专家分类非常接近。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否可以用于替代专家进行具有领域特定知识需求的主题分析，从而在定性研究中提供可扩展的辅助工具。

Method: 使用xylazine相关的两个Reddit数据集和12个由专家定义的主题，测试五个LLMs在零样本、单样本和少样本提示下的分类表现，依靠准确性、精确度、召回率和F1得分作为评估指标。

Result: 在验证集上，GPT-4o在两次提示条件下表现最佳，其准确率达到90.9%，F1得分为0.71。对于高流行度主题，模型分类与专家结果非常接近。

Conclusion: 少样本提示的LLMs可用于自动化主题分析，为定性研究提供可扩展的补充工具。

Abstract: Background Large language models (LLMs) face challenges in inductive thematic
analysis, a task requiring deep interpretive and domain-specific expertise. We
evaluated the feasibility of using LLMs to replicate expert-driven thematic
analysis of social media data. Methods Using two temporally non-intersecting
Reddit datasets on xylazine (n=286 and n=686, for model optimization and
validation, respectively) with twelve expert-derived themes, we evaluated five
LLMs against expert coding. We modeled the task as a series of binary
classifications, rather than a single, multi-label classification, employing
zero-, single-, and few-shot prompting strategies and measuring performance via
accuracy, precision, recall, and F1-score. Results On the validation set,
GPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score:
0.71). For high-prevalence themes, model-derived thematic distributions closely
mirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use:
16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based
approaches can automate thematic analyses, offering a scalable supplement for
qualitative research. Keywords: thematic analysis, large language models,
natural language processing, qualitative analysis, social media, prompt
engineering, public health

</details>


### [125] [AF-XRAY: Visual Explanation and Resolution of Ambiguity in Legal Argumentation Frameworks](https://arxiv.org/abs/2507.10831)
*Yilin Xia,Heng Zheng,Shawn Bowers,Bertram Ludäscher*

Main category: cs.AI

TL;DR: 本文提出了一个开源工具AF-XRAY，用于探索、分析和可视化法律推理中的抽象辩护框架（AFs）。


<details>
  <summary>Details</summary>
Motivation: 在法律推理中，确定模糊性来源并解释论点接受对非专家来说仍然是一个挑战。

Method: AF-XRAY引入了四个主要功能：基于博弈论长度的分层可视化、攻击边语义角色分类、模棱两可的三值语义上的二值解决方案可视化，以及通过生成关键攻击集合来解决未决论点。

Result: 通过AF-XRAY，用户可以将模糊场景转化为有根解决方案，并探讨导致模糊性的具体原因及替代解决方案。

Conclusion: AF-XRAY支持目的性法律推理，有助于揭示不同假设如何产生不同的合理结论。

Abstract: Argumentation frameworks (AFs) provide formal approaches for legal reasoning,
but identifying sources of ambiguity and explaining argument acceptance remains
challenging for non-experts. We present AF-XRAY, an open-source toolkit for
exploring, analyzing, and visualizing abstract AFs in legal reasoning. AF-XRAY
introduces: (i) layered visualizations based on game-theoretic argument length
revealing well-founded derivation structures; (ii) classification of attack
edges by semantic roles (primary, secondary, blunders); (iii) overlay
visualizations of alternative 2-valued solutions on ambiguous 3-valued grounded
semantics; and (iv) identification of critical attack sets whose suspension
resolves undecided arguments. Through systematic generation of critical attack
sets, AF-XRAY transforms ambiguous scenarios into grounded solutions, enabling
users to pinpoint specific causes of ambiguity and explore alternative
resolutions. We use real-world legal cases (e.g., Wild Animals as modeled by
Bench-Capon) to show that our tool supports teleological legal reasoning by
revealing how different assumptions lead to different justified conclusions.

</details>


### [126] [NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization](https://arxiv.org/abs/2507.10894)
*Zongtao He,Liuyi Wang,Lu Chen,Chengju Liu,Qijun Chen*

Main category: cs.AI

TL;DR: 本文提出了一种名为NavComposer的框架，用于自动生成高质量的导航指令，并引入NavInstrCritic评价系统来全面评估生成指令的质量。


<details>
  <summary>Details</summary>
Motivation: 现有专家提供的导航指令数据量有限，合成指令质量较差，难以支持大规模研究，因此需要一种更高效的指令生成与评价方法。

Method: 通过NavComposer框架，将动作、场景和物体等语义实体解构并重组为自然语言指令；引入模块化架构以整合先进技术；并采用NavInstrCritic系统在三维度评估指令质量，消除了对专家注释的依赖。

Result: 实验显示，该方法在指令生成质量和评估维度上均具有明显优势，为导航任务提供了可靠的工具支持。

Conclusion: NavComposer和NavInstrCritic的提出解决了导航指令生成和评价的瓶颈问题，使研究更加可扩展和普适。

Abstract: Language-guided navigation is a cornerstone of embodied AI, enabling agents
to interpret language instructions and navigate complex environments. However,
expert-provided instructions are limited in quantity, while synthesized
annotations often lack quality, making them insufficient for large-scale
research. To address this, we propose NavComposer, a novel framework for
automatically generating high-quality navigation instructions. NavComposer
explicitly decomposes semantic entities such as actions, scenes, and objects,
and recomposes them into natural language instructions. Its modular
architecture allows flexible integration of state-of-the-art techniques, while
the explicit use of semantic entities enhances both the richness and accuracy
of instructions. Moreover, it operates in a data-agnostic manner, supporting
adaptation to diverse navigation trajectories without domain-specific training.
Complementing NavComposer, we introduce NavInstrCritic, a comprehensive
annotation-free evaluation system that assesses navigation instructions on
three dimensions: contrastive matching, semantic consistency, and linguistic
diversity. NavInstrCritic provides a holistic evaluation of instruction
quality, addressing limitations of traditional metrics that rely heavily on
expert annotations. By decoupling instruction generation and evaluation from
specific navigation agents, our method enables more scalable and generalizable
research. Extensive experiments provide direct and practical evidence for the
effectiveness of our method.

</details>


### [127] [Lessons Learned from Evaluation of LLM based Multi-agents in Safer Therapy Recommendation](https://arxiv.org/abs/2507.10911)
*Yicong Wu,Ting Chen,Irit Hochberg,Zhoujian Sun,Ruth Edry,Zhengxing Huang,Mor Peleg*

Main category: cs.AI

TL;DR: 研究探讨利用基于大语言模型的多智能体系统（MAS）进行更安全的疗法推荐，发现当前单智能体表现与多学科团队相当，但需要进一步优化以减少不必要的药物冲突。


<details>
  <summary>Details</summary>
Motivation: 当前对慢性多病共存患者的疗法推荐因治疗冲突风险面临挑战，现有决策支持系统存在扩展性限制，本研究旨在借鉴全科医生使用多学科团队协作管理患者的经验，探索基于大语言模型（LLM）的多智能体系统（MAS）在该领域的可行性和价值。

Method: 设计了一个模拟多学科团队决策的MAS框架，允许人工代理通过讨论解决医疗冲突，并使用基准案例对疗法规划任务进行了多智能体与单智能体以及现实对照的综合评估。提出了兼顾技术精度、临床目标及用药负担的评价指标。

Result: 当前单一代理的治疗建议能够达到与多学科团队相当的水平，最佳模型能正确解决临床目标，但建议内容不够完整，同时有些模型仍存在多余用药导致的用药冲突和药物相互作用问题。

Conclusion: 使用LLM进行疗法推荐在技术上具备潜力，但需进一步优化模型以减少用药不必要的冲突，提升建议的完整性和实用性。

Abstract: Therapy recommendation for chronic patients with multimorbidity is
challenging due to risks of treatment conflicts. Existing decision support
systems face scalability limitations. Inspired by the way in which general
practitioners (GP) manage multimorbidity patients, occasionally convening
multidisciplinary team (MDT) collaboration, this study investigated the
feasibility and value of using a Large Language Model (LLM)-based multi-agent
system (MAS) for safer therapy recommendations. We designed a single agent and
a MAS framework simulating MDT decision-making by enabling discussion among LLM
agents to resolve medical conflicts. The systems were evaluated on therapy
planning tasks for multimorbidity patients using benchmark cases. We compared
MAS performance with single-agent approaches and real-world benchmarks. An
important contribution of our study is the definition of evaluation metrics
that go beyond the technical precision and recall and allow the inspection of
clinical goals met and medication burden of the proposed advices to a gold
standard benchmark. Our results show that with current LLMs, a single agent GP
performs as well as MDTs. The best-scoring models provide correct
recommendations that address all clinical goals, yet the advices are
incomplete. Some models also present unnecessary medications, resulting in
unnecessary conflicts between medication and conditions or drug-drug
interactions.

</details>


### [128] [Enhancing Safe and Controllable Protein Generation via Knowledge Preference Optimization](https://arxiv.org/abs/2507.10923)
*Yuhao Wang,Keyan Ding,Kehua Feng,Zeyuan Wang,Ming Qin,Xiaotong Li,Qiang Zhang,Huajun Chen*

Main category: cs.AI

TL;DR: 提出了一种基于知识指引的偏好优化框架（KPO），用以减少生成有害蛋白序列的风险，同时保持高功能性。


<details>
  <summary>Details</summary>
Motivation: 蛋白质语言模型在序列生成方面表现出色，但可能生成有害蛋白序列，面临生物安全和伦理问题。

Method: 通过引入蛋白安全知识图谱和图修剪策略，结合强化学习，优化生成序列的安全性。

Result: 实验表明，KPO显著降低了生成有害序列的可能性，同时保持了蛋白功能的高效性。

Conclusion: KPO为生物技术领域使用生成模型提供了一个可靠的安全保障框架。

Abstract: Protein language models have emerged as powerful tools for sequence
generation, offering substantial advantages in functional optimization and
denovo design. However, these models also present significant risks of
generating harmful protein sequences, such as those that enhance viral
transmissibility or evade immune responses. These concerns underscore critical
biosafety and ethical challenges. To address these issues, we propose a
Knowledge-guided Preference Optimization (KPO) framework that integrates prior
knowledge via a Protein Safety Knowledge Graph. This framework utilizes an
efficient graph pruning strategy to identify preferred sequences and employs
reinforcement learning to minimize the risk of generating harmful proteins.
Experimental results demonstrate that KPO effectively reduces the likelihood of
producing hazardous sequences while maintaining high functionality, offering a
robust safety assurance framework for applying generative models in
biotechnology.

</details>


### [129] [Modeling Habitat Shifts: Integrating Convolutional Neural Networks and Tabular Data for Species Migration Prediction](https://arxiv.org/abs/2507.10993)
*Emir Durakovic,Min-Hong Shih*

Main category: cs.AI

TL;DR: 提出使用卷积神经网络（CNN）结合环境特征数据预测鸟类栖息地分布，取得了85%的准确率。


<details>
  <summary>Details</summary>
Motivation: 由于气候变化，栖息地范围发生改变，需要更准确的方法预测鸟类是否出现在特定栖息地。

Method: 应用CNN分析卫星影像捕捉景观空间特征，同时结合温度、降水和海拔等环境数据构建预测模型。

Result: 预测鸟类分布时平均准确率达到85%。

Conclusion: 提出的方法可扩展且可靠，对了解鸟类迁徙现象具有重要应用价值。

Abstract: Due to climate-induced changes, many habitats are experiencing range shifts
away from their traditional geographic locations (Piguet, 2011). We propose a
solution to accurately model whether bird species are present in a specific
habitat through the combination of Convolutional Neural Networks (CNNs)
(O'Shea, 2015) and tabular data. Our approach makes use of satellite imagery
and environmental features (e.g., temperature, precipitation, elevation) to
predict bird presence across various climates. The CNN model captures spatial
characteristics of landscapes such as forestation, water bodies, and
urbanization, whereas the tabular method uses ecological and geographic data.
Both systems predict the distribution of birds with an average accuracy of 85%,
offering a scalable but reliable method to understand bird migration.

</details>


### [130] [Personalized Exercise Recommendation with Semantically-Grounded Knowledge Tracing](https://arxiv.org/abs/2507.11060)
*Yilmazcan Ozyurt,Tunaberk Almaci,Stefan Feuerriegel,Mrinmaya Sachan*

Main category: cs.AI

TL;DR: ExRec 是一种用于个性化练习推荐的框架，通过语义知识追踪改进学生学习体验，并且能稳健地适应未见的题目。


<details>
  <summary>Details</summary>
Motivation: 现有的练习推荐方法忽视了题目语义内容与学生学习的序列结构化进程的影响。

Method: 通过对题目进行知识点标注与学习其语义表示，结合强化学习（RL）优化，提升现有知识追踪模型的表现，并引入基于模型的价值估计（MVE）方法提高知识累积改善的评估。

Result: 验证了 ExRec 在包含不同教育目标的四项数学在线学习任务中的有效性，表现为更强的个性化能力和鲁棒性。

Conclusion: ExRec 展现了知识追踪与强化学习结合对教育个性化的潜力，其生成的学生学习轨迹具有良好的解释性。

Abstract: We introduce ExRec, a general framework for personalized exercise
recommendation with semantically-grounded knowledge tracing. Our method builds
on the observation that existing exercise recommendation approaches simulate
student performance via knowledge tracing (KT) but they often overlook two key
aspects: (a) the semantic content of questions and (b) the sequential,
structured progression of student learning. To address this, our ExRec presents
an end-to-end pipeline, from annotating the KCs of questions and learning their
semantic representations to training KT models and optimizing several
reinforcement learning (RL) methods. Moreover, we improve standard
Q-learning-based continuous RL methods via a tailored model-based value
estimation (MVE) approach that directly leverages the components of KT model in
estimating cumulative knowledge improvement. We validate the effectiveness of
our ExRec using various RL methods across four real-world tasks with different
educational goals in online math learning. We further show that ExRec
generalizes robustly to new, unseen questions and that it produces
interpretable student learning trajectories. Together, our findings highlight
the promise of KT-guided RL for effective personalization in education.

</details>


### [131] [Tactical Decision for Multi-UGV Confrontation with a Vision-Language Model-Based Commander](https://arxiv.org/abs/2507.11079)
*Li Wang,Qizhen Wu,Lei Chen*

Main category: cs.AI

TL;DR: 该论文提出一种基于视觉语言模型的指挥系统以解决自主对抗中的智能感知到决策推理问题。


<details>
  <summary>Details</summary>
Motivation: 现有手工规则方法在复杂和瞬态的战场环境中表现不佳，而强化学习方法在解释性不足的情况下主要关注操作控制，不适用于战略决策。

Method: 将视觉语言模型用于场景理解，将轻量级大语言模型用于战略推理，通过共享语义空间实现了感知与决策的统一。

Result: 通过仿真与消融实验验证，该方法的胜率相比基线模型超过80%。

Conclusion: 该研究方法强适应性和解释性，类似于人类指挥官的认知过程，为多无人车自主对抗中的战术决策提供了新思路。

Abstract: In multiple unmanned ground vehicle confrontations, autonomously evolving
multi-agent tactical decisions from situational awareness remain a significant
challenge. Traditional handcraft rule-based methods become vulnerable in the
complicated and transient battlefield environment, and current reinforcement
learning methods mainly focus on action manipulation instead of strategic
decisions due to lack of interpretability. Here, we propose a vision-language
model-based commander to address the issue of intelligent
perception-to-decision reasoning in autonomous confrontations. Our method
integrates a vision language model for scene understanding and a lightweight
large language model for strategic reasoning, achieving unified perception and
decision within a shared semantic space, with strong adaptability and
interpretability. Unlike rule-based search and reinforcement learning methods,
the combination of the two modules establishes a full-chain process, reflecting
the cognitive process of human commanders. Simulation and ablation experiments
validate that the proposed approach achieves a win rate of over 80% compared
with baseline models.

</details>


### [132] [Function-to-Style Guidance of LLMs for Code Translation](https://arxiv.org/abs/2507.11083)
*Longhui Zhang,Bin Wang,Jiahao Wang,Xiaofeng Zhao,Min Zhang,Hao Yang,Meishan Zhang,Yu Li,Jing Li,Jun Yu,Min Zhang*

Main category: cs.AI

TL;DR: 提出一种新的代码翻译方法F2STrans，并通过实验验证其卓越性能。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型在代码翻译方面虽然取得进展，但正确性和可读性仍是主要挑战，限制其在实际软件开发中的应用。

Method: 提出了F2STrans，在功能学习上优化翻译正确性，在风格学习上提升翻译可读性，并设计了全新代码翻译基准用于全面评估。

Result: 在提出的新基准和现有数据集上的实验显示，该方法在20个代码翻译场景中表现优于Qwen-32B和GPT-4。

Conclusion: F2STrans显著提高了代码翻译的正确性和可读性，为代码翻译任务树立了新的性能标杆。

Abstract: Large language models (LLMs) have made significant strides in code
translation tasks. However, ensuring both the correctness and readability of
translated code remains a challenge, limiting their effective adoption in
real-world software development. In this work, we propose F2STrans, a
function-to-style guiding paradigm designed to progressively improve the
performance of LLMs in code translation. Our approach comprises two key stages:
(1) Functional learning, which optimizes translation correctness using
high-quality source-target code pairs mined from online programming platforms,
and (2) Style learning, which improves translation readability by incorporating
both positive and negative style examples. Additionally, we introduce a novel
code translation benchmark that includes up-to-date source code, extensive test
cases, and manually annotated ground-truth translations, enabling comprehensive
functional and stylistic evaluations. Experiments on both our new benchmark and
existing datasets demonstrate that our approach significantly improves code
translation performance. Notably, our approach enables Qwen-1.5B to outperform
prompt-enhanced Qwen-32B and GPT-4 on average across 20 diverse code
translation scenarios.

</details>


### [133] [AI Agent Architecture for Decentralized Trading of Alternative Assets](https://arxiv.org/abs/2507.11117)
*Ailiya Borjigin,Cong He,Charles CC Lee,Wei Zhou*

Main category: cs.AI

TL;DR: 本文提出GoldMine OS，一个结合AI代理与区块链技术的体系结构，用于金锭等实物资产的去中心化交交易和代币化，并展示了其高效、安全的性能。


<details>
  <summary>Details</summary>
Motivation: 针对实物资产（如黄金）的去中心化交易所所面临的合规性、流动性和风险管理要求，迫切需要将实物资产的托管与区块链技术结合。

Method: 提出了GoldMine OS体系结构，结合链上智能合约和链下AI代理，分别完成风险控制和决策任务。系统采用四种协作AI代理（合规、代币发行、做市和风险控制）与一个核心协调架构，并通过模拟和试点部署进行验证。

Result: 原型系统表现出高效的代币发行、流动性管理及攻击应对能力，支持多达5000笔每秒的交易及10000名并发用户。系统在性能和安全性方面达到了严格要求。

Conclusion: GoldMine OS通过将AI代理与区块链技术相结合，展示了使传统非流动资产去中心化并民主化交易的潜力，确保持续的透明性和适应性。

Abstract: Decentralized trading of real-world alternative assets (e.g., gold) requires
bridging physical asset custody with blockchain systems while meeting strict
requirements for compliance, liquidity, and risk management. We present
GoldMine OS, a research oriented architecture that employs multiple specialized
AI agents to automate and secure the tokenization and exchange of physical gold
into a blockchain based stablecoin ("OZ"). Our approach combines on chain smart
contracts for critical risk controls with off chain AI agents for decision
making, blending the transparency and reliability of blockchains with the
flexibility of AI driven automation. We describe four cooperative agents
(Compliance, Token Issuance, Market Making, and Risk Control) and a
coordinating core, and evaluate the system through simulation and a controlled
pilot deployment. In experiments the prototype delivers on demand token
issuance in under 1.2 s, more than 100 times faster than manual workflows. The
Market Making agent maintains tight liquidity with spreads often below 0.5
percent even under volatile conditions. Fault injection tests show resilience:
an oracle price spoofing attack is detected and mitigated within 10 s, and a
simulated vault mis reporting halts issuance immediately with minimal user
impact. The architecture scales to 5000 transactions per second with 10000
concurrent users in benchmarks. These results indicate that an AI agent based
decentralized exchange for alternative assets can satisfy rigorous performance
and safety requirements. We discuss broader implications for democratizing
access to traditionally illiquid assets and explain how our governance model --
multi signature agent updates and on chain community voting on risk parameters
-- provides ongoing transparency, adaptability, and formal assurance of system
integrity.

</details>


### [134] [Defining neurosymbolic AI](https://arxiv.org/abs/2507.11127)
*Lennert De Smet,Luc De Raedt*

Main category: cs.AI

TL;DR: 本文提出了一个神经符号AI的正式定义，作为其核心成分的抽象化表达。


<details>
  <summary>Details</summary>
Motivation: 目前神经符号AI领域缺乏公认的正式定义，尽管已有多种不同体系，作者希望统一定义并抽象其核心成分。

Method: 将神经符号推理形式化定义为逻辑函数和信念函数的积分计算。

Result: 该定义能够抽象化描述多种具有代表性的神经符号AI系统。

Conclusion: 正式定义为神经符号AI提供了统一的理论框架，推动领域的发展和规范化。

Abstract: Neurosymbolic AI focuses on integrating learning and reasoning, in
particular, on unifying logical and neural representations. Despite the
existence of an alphabet soup of neurosymbolic AI systems, the field is lacking
a generally accepted formal definition of what neurosymbolic models and
inference really are. We introduce a formal definition for neurosymbolic AI
that makes abstraction of its key ingredients. More specifically, we define
neurosymbolic inference as the computation of an integral over a product of a
logical and a belief function. We show that our neurosymbolic AI definition
makes abstraction of key representative neurosymbolic AI systems.

</details>


### [135] [Collaborative Trustworthiness for Good Decision Making in Autonomous Systems](https://arxiv.org/abs/2507.11135)
*Selma Saidi,Omar Laimona,Christoph Schmickler,Dirk Ziegenbein*

Main category: cs.AI

TL;DR: 这篇论文提出一种基于协作的数据共享方法，通过使用二进制决策图（BDD）进行信念聚合与传播，从而提高自主系统在动态复杂环境中的决策能力及可信度。


<details>
  <summary>Details</summary>
Motivation: 当前自主系统在复杂动态环境中的安全性和正确行为仍具挑战，需要改进决策的可靠性和质量。

Method: 提出了一种基于协作数据共享的决策框架，采用社会认识论概念定义聚合传播规则，以及利用BDD模型实现信念聚合与简化。

Result: 通过使用BDD的归约规则，提高了计算效率并支持自主系统协作决策。

Conclusion: 协作方法和BDD应用为自主系统在复杂环境中提升决策可靠性和可信度提供了可能性。

Abstract: Autonomous systems are becoming an integral part of many application domains,
like in the mobility sector. However, ensuring their safe and correct behaviour
in dynamic and complex environments remains a significant challenge, where
systems should autonomously make decisions e.g., about manoeuvring. We propose
in this paper a general collaborative approach for increasing the level of
trustworthiness in the environment of operation and improve reliability and
good decision making in autonomous system. In the presence of conflicting
information, aggregation becomes a major issue for trustworthy decision making
based on collaborative data sharing. Unlike classical approaches in the
literature that rely on consensus or majority as aggregation rule, we exploit
the fact that autonomous systems have different quality attributes like
perception quality. We use this criteria to determine which autonomous systems
are trustworthy and borrow concepts from social epistemology to define
aggregation and propagation rules, used for automated decision making. We use
Binary Decision Diagrams (BDDs) as formal models for beliefs aggregation and
propagation, and formulate reduction rules to reduce the size of the BDDs and
allow efficient computation structures for collaborative automated reasoning.

</details>


### [136] [Fine-grained Timing Analysis of Digital Integrated Circuits in Answer Set Programming](https://arxiv.org/abs/2507.11150)
*Alessandro Bertagnon,Marcello Dalpasso,Michele Favalli,Marco Gavanelli*

Main category: cs.AI

TL;DR: 这篇论文探讨了如何计算电路模块的实际最大延迟，以改进硬件性能，使用答案集编程(ASP)进行建模并取得了不错的实验效果。


<details>
  <summary>Details</summary>
Motivation: 目前硬件设计中为求计算简单常使用静态时序分析来估算最大延迟，但这种方法通常无法充分优化性能，因而需要更精确的方法。

Method: 将计算电路最大延迟问题建模为答案集编程(ASP)问题，提出了高效的编码方式并利用ASP求解器进行求解。

Result: 实验结果表明，ASP可以有效解决硬件设计中的复杂问题，能够更精确地计算最大延迟。

Conclusion: 使用ASP求解器可以更精确计算电路最大延迟，从而提升整体硬件性能，是传统方法的有力补充。

Abstract: In the design of integrated circuits, one critical metric is the maximum
delay introduced by combinational modules within the circuit. This delay is
crucial because it represents the time required to perform a computation: in an
Arithmetic-Logic Unit it represents the maximum time taken by the circuit to
perform an arithmetic operation. When such a circuit is part of a larger,
synchronous system, like a CPU, the maximum delay directly impacts the maximum
clock frequency of the entire system. Typically, hardware designers use Static
Timing Analysis to compute an upper bound of the maximum delay because it can
be determined in polynomial time. However, relying on this upper bound can lead
to suboptimal processor speeds, thereby missing performance opportunities. In
this work, we tackle the challenging task of computing the actual maximum
delay, rather than an approximate value. Since the problem is computationally
hard, we model it in Answer Set Programming (ASP), a logic language featuring
extremely efficient solvers. We propose non-trivial encodings of the problem
into ASP. Experimental results show that ASP is a viable solution to address
complex problems in hardware design.

</details>


### [137] [DuetGraph: Coarse-to-Fine Knowledge Graph Reasoning with Dual-Pathway Global-Local Fusion](https://arxiv.org/abs/2507.11229)
*Jin Li,Zezhong Ding,Xike Xie*

Main category: cs.AI

TL;DR: DuetGraph提出了一种具有双路径的全局-局部融合的粗粒到细粒知识图谱推理方法，有效缓解了过平滑问题并提高了推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱推理方法因过平滑问题导致推理效率下降，需要改进处理方式。

Method: 设计了具有双路径（分别处理全局信息和局部信息）的融合机制，并引入粗粒到细粒优化策略。

Result: 在多个数据集上实现了最高性能提升，推理质量提高了8.7%，训练效率提高了1.8倍。

Conclusion: DuetGraph显著改善了推理质量，为知识图谱推理方法开辟了新的方向。

Abstract: Knowledge graphs (KGs) are vital for enabling knowledge reasoning across
various domains. Recent KG reasoning methods that integrate both global and
local information have achieved promising results. However, existing methods
often suffer from score over-smoothing, which blurs the distinction between
correct and incorrect answers and hinders reasoning effectiveness. To address
this, we propose DuetGraph, a coarse-to-fine KG reasoning mechanism with
dual-pathway global-local fusion. DuetGraph tackles over-smoothing by
segregating -- rather than stacking -- the processing of local (via message
passing) and global (via attention) information into two distinct pathways,
preventing mutual interference and preserving representational discrimination.
In addition, DuetGraph introduces a coarse-to-fine optimization, which
partitions entities into high- and low-score subsets. This strategy narrows the
candidate space and sharpens the score gap between the two subsets, which
alleviates over-smoothing and enhances inference quality. Extensive experiments
on various datasets demonstrate that DuetGraph achieves state-of-the-art (SOTA)
performance, with up to an 8.7% improvement in reasoning quality and a
1.8$\times$ acceleration in training efficiency.

</details>


### [138] [Taming Uncertainty via Automation: Observing, Analyzing, and Optimizing Agentic AI Systems](https://arxiv.org/abs/2507.11277)
*Dany Moshkovich,Sergey Zeltyn*

Main category: cs.AI

TL;DR: 本文提出AgentOps框架，旨在管理和优化复杂LLM驱动的代理系统操作。


<details>
  <summary>Details</summary>
Motivation: 传统的软件可观测性和操作方法不足以应对代理AI系统的不确定性，该研究旨在解决这一问题。

Method: 提出一个AgentOps六阶段自动化管道，包括行为观察、指标收集、问题检测、原因分析、优化建议和运行时自动化。

Result: 框架展示了通过自动化应对代理AI系统不确定性的重要性，能够确保系统的自适应性和有效运行。

Conclusion: 通过控制不确定性，不是将其完全消除，而是将其管理为安全、适应性强且操作高效的AI系统。

Abstract: Large Language Models (LLMs) are increasingly deployed within agentic
systems-collections of interacting, LLM-powered agents that execute complex,
adaptive workflows using memory, tools, and dynamic planning. While enabling
powerful new capabilities, these systems also introduce unique forms of
uncertainty stemming from probabilistic reasoning, evolving memory states, and
fluid execution paths. Traditional software observability and operations
practices fall short in addressing these challenges.
  This paper introduces AgentOps: a comprehensive framework for observing,
analyzing, optimizing, and automating operation of agentic AI systems. We
identify distinct needs across four key roles-developers, testers, site
reliability engineers (SREs), and business users-each of whom engages with the
system at different points in its lifecycle. We present the AgentOps Automation
Pipeline, a six-stage process encompassing behavior observation, metric
collection, issue detection, root cause analysis, optimized recommendations,
and runtime automation. Throughout, we emphasize the critical role of
automation in managing uncertainty and enabling self-improving AI systems-not
by eliminating uncertainty, but by taming it to ensure safe, adaptive, and
effective operation.

</details>


### [139] [Opus: A Prompt Intention Framework for Complex Workflow Generation](https://arxiv.org/abs/2507.11288)
*Théo Fagnoni,Mahsun Altin,Chia En Chung,Phillip Kingston,Alan Tuning,Dana O. Mohamed,Inès Adnani*

Main category: cs.AI

TL;DR: 本文提出了Opus意图框架，通过引入意图捕获层来改进指令微调大语言模型(LLMs)在复杂工作流生成中的表现。实验表明该框架显著提升了语义工作流相似性指标的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在复杂工作流生成中面临逻辑性和一致性问题。

Method: 设计并集成了意图捕获层，包括提取用户查询中的工作流信号，转化为结构化工作流意图对象，并基于意图生成工作流。

Result: 在1000组合成基准测试中，使用该框架生成的工作流具有更高的语义一致性和质量。

Conclusion: 引入意图捕获层显著提升了LLM驱动的工作流生成质量，特别是在处理混合意图时表现突出。

Abstract: This paper introduces the Opus Prompt Intention Framework, designed to
improve complex Workflow Generation with instruction-tuned Large Language
Models (LLMs). We propose an intermediate Intention Capture layer between user
queries and Workflow Generation, implementing the Opus Workflow Intention
Framework, which consists of extracting Workflow Signals from user queries,
interpreting them into structured Workflow Intention objects, and generating
Workflows based on these Intentions. Our results show that this layer enables
LLMs to produce logical and meaningful outputs that scale reliably as query
complexity increases. On a synthetic benchmark of 1,000 multi-intent
query-Workflow(s) pairs, applying the Opus Prompt Intention Framework to
Workflow Generation yields consistent improvements in semantic Workflow
similarity metrics. In this paper, we introduce the Opus Prompt Intention
Framework by applying the concepts of Workflow Signal and Workflow Intention to
LLM-driven Workflow Generation. We present a reproducible, customizable
LLM-based Intention Capture system to extract Workflow Signals and Workflow
Intentions from user queries. Finally, we provide empirical evidence that the
proposed system significantly improves Workflow Generation quality compared to
direct generation from user queries, particularly in cases of Mixed Intention
Elicitation.

</details>


### [140] [Contestability in Quantitative Argumentation](https://arxiv.org/abs/2507.11323)
*Xiang Yin,Nico Potyka,Antonio Rago,Timotheus Kampik,Francesca Toni*

Main category: cs.AI

TL;DR: 研究介绍了用于争议性AI的Edge-Weighted Quantitative Bipolar Argumentation Frameworks (EW-QBAFs)，并提出了通过调整边权重来实现目标争论强度的方法。


<details>
  <summary>Details</summary>
Motivation: 希望AI决策能够符合人类偏好，并探索如何通过新型的论证框架EW-QBAFs实现这一目标。

Method: 在EW-QBAFs上下文中引入争议性问题，提出基于梯度的关系归因解释(G-RAEs)以量化争论强度与边权重之间的敏感性，并开发了一种迭代算法调整边权重以实现目标。

Result: 实验评估表明，对模拟个性化推荐系统和多层感知器结构特征的合成EW-QBAFs，该方法能够有效解决争议性问题。

Conclusion: 提出的框架和方法能够有效支持争议性AI，实现目标争论强度的调整，有助于促进AI决策的可争议性和与人类偏好的对齐。

Abstract: Contestable AI requires that AI-driven decisions align with human
preferences. While various forms of argumentation have been shown to support
contestability, Edge-Weighted Quantitative Bipolar Argumentation Frameworks
(EW-QBAFs) have received little attention. In this work, we show how EW-QBAFs
can be deployed for this purpose. Specifically, we introduce the contestability
problem for EW-QBAFs, which asks how to modify edge weights (e.g., preferences)
to achieve a desired strength for a specific argument of interest (i.e., a
topic argument). To address this problem, we propose gradient-based relation
attribution explanations (G-RAEs), which quantify the sensitivity of the topic
argument's strength to changes in individual edge weights, thus providing
interpretable guidance for weight adjustments towards contestability. Building
on G-RAEs, we develop an iterative algorithm that progressively adjusts the
edge weights to attain the desired strength. We evaluate our approach
experimentally on synthetic EW-QBAFs that simulate the structural
characteristics of personalised recommender systems and multi-layer
perceptrons, and demonstrate that it can solve the problem effectively.

</details>


### [141] [CogDDN: A Cognitive Demand-Driven Navigation with Decision Optimization and Dual-Process Thinking](https://arxiv.org/abs/2507.11334)
*Yuehao Huang,Liang Liu,Shuangming Lei,Yukai Ma,Hao Su,Jianbiao Mei,Pengxiang Zhao,Yaqing Gu,Yong Liu,Jiajun Lv*

Main category: cs.AI

TL;DR: 本文提出了一种名为CogDDN的框架，通过模仿人类认知与学习机制提升导航效率与适应性。


<details>
  <summary>Details</summary>
Motivation: 现有的需求驱动导航方法依赖预先收集的数据，难以在未知场景中做到泛化。本文希望通过融合快速与慢速思维系统解决此问题。

Method: 提出VLM-based框架CogDDN，通过语义对齐检测目标物与指令，并整合启发式和分析性决策模块。

Result: 在AI2Thor模拟器与ProcThor数据集上，CogDDN在导航精度与适应性上表现优于单摄像头方法，提升了15%。

Conclusion: CogDDN框架有效模拟了人类认知机制，显著提高了机器人在未知环境中的导航与决策能力。

Abstract: Mobile robots are increasingly required to navigate and interact within
unknown and unstructured environments to meet human demands. Demand-driven
navigation (DDN) enables robots to identify and locate objects based on
implicit human intent, even when object locations are unknown. However,
traditional data-driven DDN methods rely on pre-collected data for model
training and decision-making, limiting their generalization capability in
unseen scenarios. In this paper, we propose CogDDN, a VLM-based framework that
emulates the human cognitive and learning mechanisms by integrating fast and
slow thinking systems and selectively identifying key objects essential to
fulfilling user demands. CogDDN identifies appropriate target objects by
semantically aligning detected objects with the given instructions.
Furthermore, it incorporates a dual-process decision-making module, comprising
a Heuristic Process for rapid, efficient decisions and an Analytic Process that
analyzes past errors, accumulates them in a knowledge base, and continuously
improves performance. Chain of Thought (CoT) reasoning strengthens the
decision-making process. Extensive closed-loop evaluations on the AI2Thor
simulator with the ProcThor dataset show that CogDDN outperforms single-view
camera-only methods by 15%, demonstrating significant improvements in
navigation accuracy and adaptability. The project page is available at
https://yuehaohuang.github.io/CogDDN/.

</details>


### [142] [Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces](https://arxiv.org/abs/2507.11352)
*Yunhao Yang,Neel P. Bhatt,Christian Ellis,Alvaro Velasquez,Zhangyang Wang,Ufuk Topcu*

Main category: cs.AI

TL;DR: 本文提出了一种结合自然语言对话与可验证目标解释的神经符号框架，用于应对物流领域中涉及不确定性的复杂决策。


<details>
  <summary>Details</summary>
Motivation: 现有的整数规划方法虽能保证逻辑约束，但速度缓慢且假设理想化数学模型；而大语言模型虽能处理不确定性，但可能存在误解和虚构内容，影响安全性与成本。

Method: 提出了一种神经符号框架，通过自然语言转化为结构化规划规范，并根据不确定性动态调整交互澄清步骤，基于100个不确定性筛选的例子进行了轻量级微调。

Result: 使用该方法的轻量化模型在推理延迟减少近50%的情况下超越了GPT-4.1的零样本表现。

Conclusion: 此方法为复杂物流中的可证实、实时且用户对齐的决策制定提供了一个实用途径。

Abstract: Logistics operators, from battlefield coordinators rerouting airlifts ahead
of a storm to warehouse managers juggling late trucks, often face life-critical
decisions that demand both domain expertise and rapid and continuous
replanning. While popular methods like integer programming yield logistics
plans that satisfy user-defined logical constraints, they are slow and assume
an idealized mathematical model of the environment that does not account for
uncertainty. On the other hand, large language models (LLMs) can handle
uncertainty and promise to accelerate replanning while lowering the barrier to
entry by translating free-form utterances into executable plans, yet they
remain prone to misinterpretations and hallucinations that jeopardize safety
and cost. We introduce a neurosymbolic framework that pairs the accessibility
of natural-language dialogue with verifiable guarantees on goal interpretation.
It converts user requests into structured planning specifications, quantifies
its own uncertainty at the field and token level, and invokes an interactive
clarification loop whenever confidence falls below an adaptive threshold. A
lightweight model, fine-tuned on just 100 uncertainty-filtered examples,
surpasses the zero-shot performance of GPT-4.1 while cutting inference latency
by nearly 50%. These preliminary results highlight a practical path toward
certifiable, real-time, and user-aligned decision-making for complex logistics.

</details>


### [143] [Modeling Code: Is Text All You Need?](https://arxiv.org/abs/2507.11467)
*Daniel Nichols,Konstantinos Parasyris,Harshitha Menon,Brian R. Bartoldson,Giorgis Georgakoudis,Tal Ben-Nun,Abhinav Bhatele*

Main category: cs.AI

TL;DR: 本文讨论了代码语言模型（Code LLMs）的局限性，并提出一种结合代码文本和结构化形式的新方法。


<details>
  <summary>Details</summary>
Motivation: 代码语言模型在生成、翻译和总结等任务上很流行，但难以处理代码的控制流和数据流等结构性特性。

Method: 提出了一种新方法，将代码作为文本和更结构化的形式进行组合建模。

Result: 该方法解决了传统模型在处理代码结构化数据上的局限性，同时具备生成能力与规模化。

Conclusion: 新方法结合了文本和结构化建模的优势，为代码分析和生成任务提供更优解决方案。

Abstract: Code LLMs have become extremely popular recently for modeling source code
across a variety of tasks, such as generation, translation, and summarization.
However, transformer-based models are limited in their capabilities to reason
through structured, analytical properties of code, such as control and data
flow. Previous work has explored the modeling of these properties with
structured data and graph neural networks. However, these approaches lack the
generative capabilities and scale of modern LLMs. In this work, we introduce a
novel approach to combine the strengths of modeling both code as text and more
structured forms.

</details>


### [144] [Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety](https://arxiv.org/abs/2507.11473)
*Tomek Korbak,Mikita Balesni,Elizabeth Barnes,Yoshua Bengio,Joe Benton,Joseph Bloom,Mark Chen,Alan Cooney,Allan Dafoe,Anca Dragan,Scott Emmons,Owain Evans,David Farhi,Ryan Greenblatt,Dan Hendrycks,Marius Hobbhahn,Evan Hubinger,Geoffrey Irving,Erik Jenner,Daniel Kokotajlo,Victoria Krakovna,Shane Legg,David Lindner,David Luan,Aleksander Mądry,Julian Michael,Neel Nanda,Dave Orr,Jakub Pachocki,Ethan Perez,Mary Phuong,Fabien Roger,Joshua Saxe,Buck Shlegeris,Martín Soto,Eric Steinberger,Jasmine Wang,Wojciech Zaremba,Bowen Baker,Rohin Shah,Vlad Mikulik*

Main category: cs.AI

TL;DR: 该论文探讨了监控AI思考链(CoT)是否可以提高AI安全性，并建议进一步研究此方法，同时与其他安全方法相结合。


<details>
  <summary>Details</summary>
Motivation: 探讨通过AI的思考链(CoT)来监测其是否存在意图不当行为的可能性，为AI安全性提供新的监控手段。

Method: 评估并讨论了思考链(CoT)监控的作用和局限性，同时建议融合现有安全方法进行综合应用。

Result: CoT监控方法虽然不完美，但展示了应用潜力，并指出研发者应考虑开发决策对CoT可监控性的影响。

Conclusion: 思考链(CoT)监控是一个具有前景的方法，需与现有AI安全方法结合使用，同时应关注其脆弱性，并推动相关研究。

Abstract: AI systems that "think" in human language offer a unique opportunity for AI
safety: we can monitor their chains of thought (CoT) for the intent to
misbehave. Like all other known AI oversight methods, CoT monitoring is
imperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows
promise and we recommend further research into CoT monitorability and
investment in CoT monitoring alongside existing safety methods. Because CoT
monitorability may be fragile, we recommend that frontier model developers
consider the impact of development decisions on CoT monitorability.

</details>


### [145] [Perspective-Aware AI in Extended Reality](https://arxiv.org/abs/2507.11479)
*Daniel Platnick,Matti Gruener,Marjan Alirezaie,Kent Larson,Dava J. Newman,Hossein Rahnama*

Main category: cs.AI

TL;DR: PAiR结合观点感知AI和XR以实现上下文感知和解释型沉浸体验，通过用户身份模型在沉浸系统中应用，并演示两个概念验证场景。


<details>
  <summary>Details</summary>
Motivation: 当前XR系统缺乏深度用户建模和认知环境，难以实现自适应的沉浸体验。

Method: 提出了Perspective-Aware AI in Extended Reality (PAiR)，一个将观点感知AI与XR整合的框架，基于从多模态数字足迹中学习的Chronicles身份模型。

Result: 展示了PAiR的架构及其系统流程，并在OpenDome引擎中通过两个场景验证了其实用性。

Conclusion: PAiR为人机交互开辟了一种新方向，通过将基于观点的身份模型嵌入到沉浸式系统中实现更为智能的互动体验。

Abstract: AI-enhanced Extended Reality (XR) aims to deliver adaptive, immersive
experiences-yet current systems fall short due to shallow user modeling and
limited cognitive context. We introduce Perspective-Aware AI in Extended
Reality (PAiR), a foundational framework for integrating Perspective-Aware AI
(PAi) with XR to enable interpretable, context-aware experiences grounded in
user identity. PAi is built on Chronicles: reasoning-ready identity models
learned from multimodal digital footprints that capture users' cognitive and
experiential evolution. PAiR employs these models in a closed-loop system
linking dynamic user states with immersive environments. We present PAiR's
architecture, detailing its modules and system flow, and demonstrate its
utility through two proof-of-concept scenarios implemented in the Unity-based
OpenDome engine. PAiR opens a new direction for human-AI interaction by
embedding perspective-based identity models into immersive systems.

</details>


### [146] [Illuminating the Three Dogmas of Reinforcement Learning under Evolutionary Light](https://arxiv.org/abs/2507.11482)
*Mani Hamidi,Terrence W. Deacon*

Main category: cs.AI

TL;DR: 本文讨论强化学习三大核心原则的重新思考，提出以开放式进化理论为框架来重新审视相关问题，并结合演化学和生命起源理论解决相关挑战。


<details>
  <summary>Details</summary>
Motivation: 对强化学习的三个核心原则进行重新审视，探讨其理论和应用意义，以开放式进化理论为视角提供新见解。

Method: 通过分析进化理论、类比生物学习过程以及借用生命起源理论，对强化学习中关于学习目标、奖励假设和代理性的问题提出了新的解释与框架。

Result: 从进化视角探讨了学习适应过程和奖励假设的多目标性质，并强调生命起源理论在理解代理性问题上的潜力。

Conclusion: 通过结合进化理论和生命起源研究，本文提供了重新定义强化学习核心原则的一种框架，但仍需进一步解决代理性方面的核心问题。

Abstract: Three core tenets of reinforcement learning (RL)--concerning the definition
of agency, the objective of learning, and the scope of the reward
hypothesis--have been highlighted as key targets for conceptual revision, with
major implications for theory and application. We propose a framework, inspired
by open-ended evolutionary theory, to reconsider these three "dogmas." We
revisit each assumption and address related concerns raised alongside them. To
make our arguments relevant to RL as a model of biological learning, we first
establish that evolutionary dynamics can plausibly operate within living brains
over an individual's lifetime, and are not confined to cross-generational
processes. We begin by revisiting the second dogma, drawing on evolutionary
insights to enrich the "adaptation-rather-than-search" view of learning. We
then address the third dogma regarding the limits of the reward hypothesis,
using analogies from evolutionary fitness to illuminate the scalar reward vs.
multi-objective debate. After discussing practical implications for exploration
in RL, we turn to the first--and arguably most fundamental--issue: the absence
of a formal account of agency. We argue that unlike the other two problems, the
evolutionary paradigm alone cannot resolve the agency question, though it
gestures in a productive direction. We advocate integrating ideas from
origins-of-life theory, where the thermodynamics of sustenance and replication
offer promising foundations for understanding agency and resource-constrained
reinforcement learning in biological systems.

</details>


### [147] [DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering](https://arxiv.org/abs/2507.11527)
*Yinsheng Li,Zhen Dong,Yi Shao*

Main category: cs.AI

TL;DR: 该研究提出了DrafterBench基准，用于评估大语言模型（LLM）在技术图纸修订中的性能，包含12种任务类型、46个定制工具和1920项任务，共开源于GitHub和Huggingface。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏从工业角度系统评估自动化代理特别是在土木工程领域的基准。

Method: 创建了名为DrafterBench的基准，包含12种从真实图纸中提取的任务类型、46种自定义功能和1920项任务，用于评估LLM的多种能力。

Result: DrafterBench能够全面评估AI代理在结构化数据理解、功能执行、指令跟随及关键推理中的表现，为改进LLM在工程应用的整合提供深刻见解。

Conclusion: DrafterBench以开源平台形式提供，为进一步开发和部署工业自动化领域的LLM提供了重要工具。

Abstract: Large Language Model (LLM) agents have shown great potential for solving
real-world problems and promise to be a solution for tasks automation in
industry. However, more benchmarks are needed to systematically evaluate
automation agents from an industrial perspective, for example, in Civil
Engineering. Therefore, we propose DrafterBench for the comprehensive
evaluation of LLM agents in the context of technical drawing revision, a
representation task in civil engineering. DrafterBench contains twelve types of
tasks summarized from real-world drawing files, with 46 customized
functions/tools and 1920 tasks in total. DrafterBench is an open-source
benchmark to rigorously test AI agents' proficiency in interpreting intricate
and long-context instructions, leveraging prior knowledge, and adapting to
dynamic instruction quality via implicit policy awareness. The toolkit
comprehensively assesses distinct capabilities in structured data
comprehension, function execution, instruction following, and critical
reasoning. DrafterBench offers detailed analysis of task accuracy and error
statistics, aiming to provide deeper insight into agent capabilities and
identify improvement targets for integrating LLMs in engineering applications.
Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,
with the test set hosted at
https://huggingface.co/datasets/Eason666/DrafterBench.

</details>


### [148] [How Many Instructions Can LLMs Follow at Once?](https://arxiv.org/abs/2507.11538)
*Daniel Jaroslawicz,Brendan Whiting,Parth Shah,Karime Maamari*

Main category: cs.AI

TL;DR: 研究提出了IFScale基准，用于评估大语言模型（LLM）如何在高指令密度下执行任务，并发现当前模型在高指令密度下表现仍有限。


<details>
  <summary>Details</summary>
Motivation: 当前LLM系统需要同时遵循大量指令，但现有基准尚未评估模型在高指令密度任务下的表现，因此提出新基准以填补这一研究空白。

Method: 研究设计了包含500条关键词指令的IFScale基准任务，并测试了来自七大提供商的20个最先进的模型，分析指令密度对任务表现的影响。

Result: 即使是最优秀的前沿模型，在指令密度达到500时准确率也仅为68%，且不同模型表现出三种明显的性能退化模式，包括对早期指令的偏向及不同类型的错误。

Conclusion: 该研究为真实应用中的指令密集型提示设计提供了指导，并突出了性能与延迟间的重要权衡；研究团队还开源了基准与所有结果以供进一步分析。

Abstract: Production-grade LLM systems require robust adherence to dozens or even
hundreds of instructions simultaneously. However, the instruction-following
capabilities of LLMs at high instruction densities have not yet been
characterized, as existing benchmarks only evaluate models on tasks with a
single or few instructions. We introduce IFScale, a simple benchmark of 500
keyword-inclusion instructions for a business report writing task to measure
how instruction-following performance degrades as instruction density
increases. We evaluate 20 state-of-the-art models across seven major providers
and find that even the best frontier models only achieve 68% accuracy at the
max density of 500 instructions. Our analysis reveals model size and reasoning
capability to correlate with 3 distinct performance degradation patterns, bias
towards earlier instructions, and distinct categories of instruction-following
errors. Our insights can help inform design of instruction-dense prompts in
real-world applications and highlight important performance-latency tradeoffs.
We open-source the benchmark and all results for further analysis at
https://distylai.github.io/IFScale.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [149] [Tool-to-Tool Matching Analysis Based Difference Score Computation Methods for Semiconductor Manufacturing](https://arxiv.org/abs/2507.10564)
*Sameera Bharadwaja H.,Siddhrath Jandial,Shashank S. Agashe,Rajesh Kumar Reddy Moore,Youngkwan Kim*

Main category: cs.LG

TL;DR: 提出了一种新的工具匹配方法，避免了传统方法的局限性，实现了有效的匹配分析。


<details>
  <summary>Details</summary>
Motivation: 解决传统工具匹配依赖静态配置或单一参考模型的问题，尤其是在不同厂商设备之间的应用限制。

Method: 假设不匹配设备具有更高的方差或数据模式数，提出基于单变量与多变量分析的匹配算法，并研究多变量算法对超参数的敏感性。

Result: 单变量方法与方差和数据模式数的相关系数分别高于0.95和0.5，多变量方法与最佳单变量方法的相关系数高于0.75，表明方法有效。

Conclusion: 提出的方法能有效处理工具匹配问题，适用于异构设备设置，尤其在变异和数据模式数的评估方面表现出色。

Abstract: We consider the problem of tool-to-tool matching (TTTM), also called, chamber
matching in the context of a semiconductor manufacturing equipment. Traditional
TTTM approaches utilize static configuration data or depend on a golden
reference which are difficult to obtain in a commercial manufacturing line.
Further, existing methods do not extend very well to a heterogeneous setting,
where equipment are of different make-and-model, sourced from different
equipment vendors. We propose novel TTTM analysis pipelines to overcome these
issues. We hypothesize that a mismatched equipment would have higher variance
and/or higher number of modes in the data. Our best univariate method achieves
a correlation coefficient >0.95 and >0.5 with the variance and number of modes,
respectively showing that the proposed methods are effective. Also, the best
multivariate method achieves a correlation coefficient >0.75 with the
top-performing univariate methods, showing its effectiveness. Finally, we
analyze the sensitivity of the multivariate algorithms to the algorithm
hyper-parameters.

</details>


### [150] [Enhancing Cross Entropy with a Linearly Adaptive Loss Function for Optimized Classification Performance](https://arxiv.org/abs/2507.10574)
*Jae Wan Shim*

Main category: cs.LG

TL;DR: 提出了线性自适应交叉熵损失函数，用于改进分类任务中的优化过程，通过在CIFAR-100数据集上的验证表现出优于传统交叉熵损失的准确性。


<details>
  <summary>Details</summary>
Motivation: 优化分类任务中的损失函数，提高分类准确性，同时保持计算效率。

Method: 提出并定义了线性自适应交叉熵损失函数，添加了依赖于真实类别预测概率的附加项；并在使用ResNet模型的CIFAR-100数据集上进行了实验验证。

Result: 实验结果表明，新损失函数在分类准确性上优于标准交叉熵损失函数，且计算效率无明显区别。

Conclusion: 线性自适应交叉熵损失函数是一种有效改进工具，未来有潜力拓展为损失函数设计的研究方向。

Abstract: We propose the Linearly Adaptive Cross Entropy Loss function. This is a novel
measure derived from the information theory. In comparison to the standard
cross entropy loss function, the proposed one has an additional term that
depends on the predicted probability of the true class. This feature serves to
enhance the optimization process in classification tasks involving one-hot
encoded class labels. The proposed one has been evaluated on a ResNet-based
model using the CIFAR-100 dataset. Preliminary results show that the proposed
one consistently outperforms the standard cross entropy loss function in terms
of classification accuracy. Moreover, the proposed one maintains simplicity,
achieving practically the same efficiency to the traditional cross entropy
loss. These findings suggest that our approach could broaden the scope for
future research into loss function design.

</details>


### [151] [An Adaptive Volatility-based Learning Rate Scheduler](https://arxiv.org/abs/2507.10575)
*Kieran Chai Kai Ren*

Main category: cs.LG

TL;DR: 本文介绍了一种新的自适应学习率调度算法VolSched，通过动态调整学习率提高深度神经网络的泛化性能，并在CIFAR-100数据集上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的预定义和自适应学习率调度算法常表现出次优的泛化性能，迫切需要一种改进的方法来解决这一问题。

Method: 提出了一种基于随机过程（如几何布朗运动）波动性的学习率调度算法VolSched，通过计算长短期的准确率波动比率，自适应地调整学习率以突破训练瓶颈并稳定训练。

Result: 在CIFAR-100数据集上，使用ResNet-18和ResNet-34的实验中，VolSched分别提高了1.4和1.3个百分点的top-1准确率。同时，Hessian定量分析显示，本方法找到的最优解比现有最佳基线平坦38%。

Conclusion: VolSched能够延长模型的探索阶段，找到更宽泛的最小值，显著提高了模型的泛化性能。

Abstract: Effective learning rate (LR) scheduling is crucial for training deep neural
networks. However, popular pre-defined and adaptive schedulers can still lead
to suboptimal generalization. This paper introduces VolSched, a novel adaptive
LR scheduler inspired by the concept of volatility in stochastic processes like
Geometric Brownian Motion to dynamically adjust the learning rate. By
calculating the ratio between long-term and short-term accuracy volatility,
VolSched increases the LR to escape plateaus and decreases it to stabilize
training, allowing the model to explore the loss landscape more effectively. We
evaluate VolSched on the CIFAR-100 dataset against a strong baseline using a
standard augmentation pipeline. When paired with ResNet-18 and ResNet-34, our
scheduler delivers consistent performance gains, improving top-1 accuracy by
1.4 and 1.3 percentage points respectively. Analysis of the loss curves reveals
that VolSched promotes a longer exploration phase. A quantitative analysis of
the Hessian shows that VolSched finds a final solution that is 38% flatter than
the next-best baseline, allowing the model to obtain wider minima and hence
better generalization performance.

</details>


### [152] [Universal Approximation Theorem for a Single-Layer Transformer](https://arxiv.org/abs/2507.10581)
*Esmail Gumaan*

Main category: cs.LG

TL;DR: 本文探讨了深度学习和Transformer模型的数学基础，并提出了单层Transformer的通用逼近定理，证明了其在紧域上可逼近任何连续的序列到序列映射。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer模型在自然语言处理、计算机视觉等领域取得了显著成功，但其理论理解仍显不足，因此作者希望通过数学分析加深对Transformer模型的理解。

Method: 本文回顾了深度学习中的线性代数、概率与优化基础，详细分析了多头自注意力机制和反向传播算法，并通过理论证明展示了Transformers的单层通用逼近定理。

Result: 证明了包含一个自注意力层和一个ReLU激活的前馈网络的单层Transformer，可以在紧域内以任意精度逼近连续的序列到序列映射，并通过实例研究展示了这一结果的实际意义。

Conclusion: 该研究深化了对Transformer模型的理论理解，有助于弥合理论与实践之间的差距，并为进一步研究提供了数学依据。

Abstract: Deep learning employs multi-layer neural networks trained via the
backpropagation algorithm. This approach has achieved success across many
domains and relies on adaptive gradient methods such as the Adam optimizer.
Sequence modeling evolved from recurrent neural networks to attention-based
models, culminating in the Transformer architecture. Transformers have achieved
state-of-the-art performance in natural language processing (for example, BERT
and GPT-3) and have been applied in computer vision and computational biology.
However, theoretical understanding of these models remains limited. In this
paper, we examine the mathematical foundations of deep learning and
Transformers and present a novel theoretical result. We review key concepts
from linear algebra, probability, and optimization that underpin deep learning,
and we analyze the multi-head self-attention mechanism and the backpropagation
algorithm in detail. Our main contribution is a universal approximation theorem
for Transformers: we prove that a single-layer Transformer, comprising one
self-attention layer followed by a position-wise feed-forward network with ReLU
activation, can approximate any continuous sequence-to-sequence mapping on a
compact domain to arbitrary precision. We provide a formal statement and a
complete proof. Finally, we present case studies that demonstrate the practical
implications of this result. Our findings advance the theoretical understanding
of Transformer models and help bridge the gap between theory and practice.

</details>


### [153] [MH-FSF: A Unified Framework for Overcoming Benchmarking and Reproducibility Limitations in Feature Selection Evaluation](https://arxiv.org/abs/2507.10591)
*Vanderson Rocha,Diego Kreutz,Gabriel Canto,Hendrio Bragança,Eduardo Feitosa*

Main category: cs.LG

TL;DR: 提出了MH-FSF框架，用于对特征选择方法进行系统评估，并应用于Android恶意软件领域。


<details>
  <summary>Details</summary>
Motivation: 目前的特征选择研究存在可重复性差和对私有数据集依赖的局限性。

Method: 开发了MH-FSF框架，包含17种方法的实现，并在10个公开的Android恶意软件数据集上进行系统评估。

Result: 结果显示，不同数据集平衡状况下方法的表现存在差异，强调了数据预处理和选择准则的重要性。

Conclusion: 通过提供统一的平台，促进特征选择方法的对比与研究一致性，为Android恶意软件检测领域的未来研究开辟新方向。

Abstract: Feature selection is vital for building effective predictive models, as it
reduces dimensionality and emphasizes key features. However, current research
often suffers from limited benchmarking and reliance on proprietary datasets.
This severely hinders reproducibility and can negatively impact overall
performance. To address these limitations, we introduce the MH-FSF framework, a
comprehensive, modular, and extensible platform designed to facilitate the
reproduction and implementation of feature selection methods. Developed through
collaborative research, MH-FSF provides implementations of 17 methods (11
classical, 6 domain-specific) and enables systematic evaluation on 10 publicly
available Android malware datasets. Our results reveal performance variations
across both balanced and imbalanced datasets, highlighting the critical need
for data preprocessing and selection criteria that account for these
asymmetries. We demonstrate the importance of a unified platform for comparing
diverse feature selection techniques, fostering methodological consistency and
rigor. By providing this framework, we aim to significantly broaden the
existing literature and pave the way for new research directions in feature
selection, particularly within the context of Android malware detection.

</details>


### [154] [GeoHopNet: Hopfield-Augmented Sparse Spatial Attention for Dynamic UAV Site Location Problem](https://arxiv.org/abs/2507.10636)
*Jianing Zhi,Xinghua Li,Zidong Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为GeoHopNet的新型深度强化学习模型，专注解决无人机动态选址问题，通过引入四项核心创新以提升计算效率和解决结果。


<details>
  <summary>Details</summary>
Motivation: 随着城市低空无人机经济快速发展，对动态选址问题提出了更高要求，但传统深度强化学习方法在大规模城市选址问题中计算复杂度过高。

Method: 引入了一种Hopfield增强的稀疏空间注意力网络GeoHopNet，该方法包括距离偏差多头注意力机制、K近邻稀疏注意力、现代Hopfield外存储模块和记忆正则化策略。

Result: 实验表明，GeoHopNet能够高效解决大规模选址问题，显著超越现有最优模型，在1000节点实例上达到仅0.1秒解决时间且最优性差距仅0.22%。

Conclusion: GeoHopNet显著提升了解决无人机选址问题的效率和质量，为大规模城市选址问题提供了强有力的解决方案。

Abstract: The rapid development of urban low-altitude unmanned aerial vehicle (UAV)
economy poses new challenges for dynamic site selection of UAV landing points
and supply stations. Traditional deep reinforcement learning methods face
computational complexity bottlenecks, particularly with standard attention
mechanisms, when handling large-scale urban-level location problems. This paper
proposes GeoHopNet, a Hopfield-augmented sparse spatial attention network
specifically designed for dynamic UAV site location problems. Our approach
introduces four core innovations: (1) distance-biased multi-head attention
mechanism that explicitly encodes spatial geometric information; (2) K-nearest
neighbor sparse attention that reduces computational complexity from $O(N^2)$
to $O(NK)$; (3) a modern Hopfield external memory module; and (4) a memory
regularization strategy. Experimental results demonstrate that GeoHopNet
extends the boundary of solvable problem sizes. For large-scale instances with
1,000 nodes, where standard attention models become prohibitively slow (over 3
seconds per instance) and traditional solvers fail, GeoHopNet finds
high-quality solutions (0.22\% optimality gap) in under 0.1 seconds. Compared
to the state-of-the-art ADNet baseline on 100-node instances, our method
improves solution quality by 22.2\% and is 1.8$\times$ faster.

</details>


### [155] [Extension OL-MDISF: Online Learning from Mix-Typed, Drifted, and Incomplete Streaming Features](https://arxiv.org/abs/2507.10594)
*Shengda Zhuo,Di Wu,Yi He,Shuqiang Huang,Xindong Wu*

Main category: cs.LG

TL;DR: 本文提出了一种名为OL-MDISF的新方法以应对在线学习中的混合类型特征、分布漂移和数据标签缺失问题。


<details>
  <summary>Details</summary>
Motivation: 传统的参数化建模无法很好地应对具有多样性和漂移特性的真实数据流，同时标注成本高，这些问题限制了在线学习的推广和应用。

Method: 通过借助基于Copula的潜在表示生成模型，结合集成熵与潜在失配检测漂移，以及结构感知的伪标记策略进行建模。

Result: 该方法在14个真实数据集上的两类漂移场景中进行了全面实验，表现出良好的结果。同时呈现了关键趋势、敏感性分析和时间集成动态等实证研究。

Conclusion: OL-MDISF为处理复杂的在线学习数据提供了一种高效的解决方案，同时具备复现性强的基准作用。

Abstract: Online learning, where feature spaces can change over time, offers a flexible
learning paradigm that has attracted considerable attention. However, it still
faces three significant challenges. First, the heterogeneity of real-world data
streams with mixed feature types presents challenges for traditional parametric
modeling. Second, data stream distributions can shift over time, causing an
abrupt and substantial decline in model performance. Third, it is often
infeasible to label every data instance due to time and cost constraints. To
address these issues, we proposed OL-MDISF (Online Learning from Mix-typed,
Drifted, and Incomplete Streaming Features), which constructs a latent
copula-based representation for heterogeneous features, detects drifts via
ensemble entropy and latent mismatch, and performs structure-aware
pseudo-labeling.
  This companion paper serves as a standalone technical reference to OL-MDISF.
It provides a contextual discussion of related work in mixed-type modeling,
drift adaptation, and weak supervision, as well as a comprehensive set of
experiments across 14 real-world datasets under two types of drift scenarios.
These include CER trends, ablation studies, sensitivity analyses, and temporal
ensemble dynamics. We hope this document offers a reproducible benchmark for
online learning on complex, weakly supervised streaming data.

</details>


### [156] [A Group Theoretic Analysis of the Symmetries Underlying Base Addition and Their Learnability by Neural Networks](https://arxiv.org/abs/2507.10678)
*Cutter Dawes,Simon Segert,Kamesh Krishnamurthy,Jonathan D. Cohen*

Main category: cs.LG

TL;DR: 研究了神经网络在以对称性学习为基础的极端泛化能力中的表现，特别是以进位函数在基数加法中的角色为切入点。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在模拟认知功能和人工智能中高效学习、实现极端泛化能力的问题，同时探讨对称功能的实现。

Method: 对基数加法进行群论分析，通过差异化的进位函数设计量化测量，训练神经网络学习基数加法并比较其结构与学习速率的关系。

Result: 发现简单神经网络能够以正确的输入格式和进位函数实现极端泛化能力，且学习速度与进位函数的结构紧密相关。

Conclusion: 揭示神经网络的输入格式与进位函数选择的重要性，为认知科学及机器学习的对称学习提供启发。

Abstract: A major challenge in the use of neural networks both for modeling human
cognitive function and for artificial intelligence is the design of systems
with the capacity to efficiently learn functions that support radical
generalization. At the roots of this is the capacity to discover and implement
symmetry functions. In this paper, we investigate a paradigmatic example of
radical generalization through the use of symmetry: base addition. We present a
group theoretic analysis of base addition, a fundamental and defining
characteristic of which is the carry function -- the transfer of the remainder,
when a sum exceeds the base modulus, to the next significant place. Our
analysis exposes a range of alternative carry functions for a given base, and
we introduce quantitative measures to characterize these. We then exploit
differences in carry functions to probe the inductive biases of neural networks
in symmetry learning, by training neural networks to carry out base addition
using different carries, and comparing efficacy and rate of learning as a
function of their structure. We find that even simple neural networks can
achieve radical generalization with the right input format and carry function,
and that learning speed is closely correlated with carry function structure. We
then discuss the relevance this has for cognitive science and machine learning.

</details>


### [157] [Divide-Then-Rule: A Cluster-Driven Hierarchical Interpolator for Attribute-Missing Graphs](https://arxiv.org/abs/2507.10595)
*Yaowen Hu,Wenxuan Tu,Yue Liu,Miaomiao Li,Wenpeng Lu,Zhigang Luo,Xinwang Liu,Ping Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为DTRGC的新方法，专注于在属性缺失图中进行深度图聚类，实现了明显的性能提升。


<details>
  <summary>Details</summary>
Motivation: 针对属性缺失图上节点的聚类问题尚未被充分研究，而现有的属性插补方法在节点邻域信息不足时效果不佳，因此需要新的方法。

Method: 提出Divide-Then-Rule Graph Completion (DTRGC)方法，包括动态聚类感知特征传播（DCFP）、分层邻域感知插补（HNAI）和跳步表示增强（HRE），逐步填补缺失属性，并利用聚类信息纠错。

Result: 在六个常用的图数据集上实验表明，DTRGC显著提升了各种深度图聚类方法在属性缺失图上的聚类性能。

Conclusion: DTRGC通过分步填补缺失属性并优化节点表示，有效解决了属性缺失图的聚类问题，对相关领域具有重要意义。

Abstract: Deep graph clustering (DGC) for attribute-missing graphs is an unsupervised
task aimed at partitioning nodes with incomplete attributes into distinct
clusters. Addressing this challenging issue is vital for practical
applications. However, research in this area remains underexplored. Existing
imputation methods for attribute-missing graphs often fail to account for the
varying amounts of information available across node neighborhoods, leading to
unreliable results, especially for nodes with insufficient known neighborhood.
To address this issue, we propose a novel method named Divide-Then-Rule Graph
Completion (DTRGC). This method first addresses nodes with sufficient known
neighborhood information and treats the imputed results as new knowledge to
iteratively impute more challenging nodes, while leveraging clustering
information to correct imputation errors. Specifically, Dynamic Cluster-Aware
Feature Propagation (DCFP) initializes missing node attributes by adjusting
propagation weights based on the clustering structure. Subsequently,
Hierarchical Neighborhood-aware Imputation (HNAI) categorizes attribute-missing
nodes into three groups based on the completeness of their neighborhood
attributes. The imputation is performed hierarchically, prioritizing the groups
with nodes that have the most available neighborhood information. The cluster
structure is then used to refine the imputation and correct potential errors.
Finally, Hop-wise Representation Enhancement (HRE) integrates information
across multiple hops, thereby enriching the expressiveness of node
representations. Experimental results on six widely used graph datasets show
that DTRGC significantly improves the clustering performance of various DGC
methods under attribute-missing graphs.

</details>


### [158] [RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services](https://arxiv.org/abs/2507.10605)
*Fei Zhao,Chonggang Lu,Yue Wang,Zheyong Xie,Ziyan Liu,Haofu Qian,JianZhao Huang,Fangcheng Shi,Zijie Meng,Hongcheng Guo,Mingqian He,Xinze Lyu,Yiming Lu,Ziyang Xiang,Zheyu Ye,Chengqiang Lu,Zhe Xu,Yi Wu,Yao Hu,Yan Gao,Jun Fan,Xiaolong Jiang,Weiting Liu,Boyang Wang,Shaosheng Cao*

Main category: cs.LG

TL;DR: RedOne是为社交网络服务设计的领域特定大语言模型，通过三阶段训练策略显著提升了多项任务的性能，并展现了优秀的通用能力与实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 解决传统研究在数据扩展收益递减及灵活适应多样真实环境方面的局限性，提升社交网络平台内容管理与交互质量。

Method: 提出一种三阶段训练策略，包括继续预训练、监督微调和偏好优化，并使用大规模真实世界数据集进行模型开发。

Result: 相比基线模型，在8项主要SNS任务中平均提升14.02%，在SNS双语评估基准上提升7.56%；在线测试中，减少有害内容检测曝光率11.23%，提高点击页面率14.95%。

Conclusion: RedOne作为一种SNS领域特定大语言模型，不仅在多种任务中表现出色，还有广泛的实际应用潜力。

Abstract: As a primary medium for modern information dissemination, social networking
services (SNS) have experienced rapid growth, which has proposed significant
challenges for platform content management and interaction quality improvement.
Recently, the development of large language models (LLMs) has offered potential
solutions but existing studies focus on isolated tasks, which not only
encounter diminishing benefit from the data scaling within individual scenarios
but also fail to flexibly adapt to diverse real-world context. To address these
challenges, we introduce RedOne, a domain-specific LLM designed to break the
performance bottleneck of single-task baselines and establish a comprehensive
foundation for the SNS. RedOne was developed through a three-stage training
strategy consisting of continue pretraining, supervised fine-tuning, and
preference optimization, using a large-scale real-world dataset. Through
extensive experiments, RedOne maintains strong general capabilities, and
achieves an average improvement up to 14.02% across 8 major SNS tasks and 7.56%
in SNS bilingual evaluation benchmark, compared with base models. Furthermore,
through online testing, RedOne reduced the exposure rate in harmful content
detection by 11.23% and improved the click page rate in post-view search by
14.95% compared with single-tasks finetuned baseline models. These results
establish RedOne as a robust domain-specific LLM for SNS, demonstrating
excellent generalization across various tasks and promising applicability in
real-world scenarios.

</details>


### [159] [A parametric activation function based on Wendland RBF](https://arxiv.org/abs/2507.11493)
*Majid Darehmiraki*

Main category: cs.LG

TL;DR: 提出了基于Wendland径向基函数（RBF）的新型参数化激活函数，具有可调局部性、优化梯度传播及提升训练稳定性等优点。


<details>
  <summary>Details</summary>
Motivation: 现有激活函数如ReLU、sigmoid和tanh存在局限性，需要更加优化的解决方案提升网络性能。

Method: 引入基于Wendland RBFs的增强型激活函数，将其与线性和指数项结合，并进行理论分析和实验验证。

Result: 新提出的Wendland激活函数在合成任务（如正弦波近似）和基准数据集（如MNIST、Fashion-MNIST）中表现良好，尤其在回归任务中提高了准确性，同时保持计算效率。

Conclusion: 将传统的RBF理论引入深度学习领域，Wendland激活函数通过局部化和平滑变换能够减轻过拟合现象并提升泛化能力，未来可探索混合架构和领域特定的改进。

Abstract: This paper introduces a novel parametric activation function based on
Wendland radial basis functions (RBFs) for deep neural networks. Wendland RBFs,
known for their compact support, smoothness, and positive definiteness in
approximation theory, are adapted to address limitations of traditional
activation functions like ReLU, sigmoid, and tanh. The proposed enhanced
Wendland activation combines a standard Wendland component with linear and
exponential terms, offering tunable locality, improved gradient propagation,
and enhanced stability during training. Theoretical analysis highlights its
mathematical properties, including smoothness and adaptability, while empirical
experiments on synthetic tasks (e.g., sine wave approximation) and benchmark
datasets (MNIST, Fashion-MNIST) demonstrate competitive performance. Results
show that the Wendland-based activation achieves superior accuracy in certain
scenarios, particularly in regression tasks, while maintaining computational
efficiency. The study bridges classical RBF theory with modern deep learning,
suggesting that Wendland activations can mitigate overfitting and improve
generalization through localized, smooth transformations. Future directions
include hybrid architectures and domain-specific adaptations.

</details>


### [160] [DALI-PD: Diffusion-based Synthetic Layout Heatmap Generation for ML in Physical Design](https://arxiv.org/abs/2507.10606)
*Bing-Yue Wu,Vidya A. Chhabria*

Main category: cs.LG

TL;DR: 提出了一种名为DALI-PD的框架，通过扩散模型生成合成布局热图以加速机器学习在物理设计中的研究。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型在物理设计任务中的泛化能力受限于高质量、大规模数据集的可用性，且这些数据集通常代价高昂、更新缓慢且受知识产权限制。

Method: 采用DALI-PD框架，基于扩散模型生成多样化的布局热图，包括功耗、IR drop、拥堵、宏单元布局及单元密度图等，生成速度快、效果多样。

Result: 通过DALI-PD，生成了包含超过2万个布局配置的数据集，这些热图与真实布局相似，可以提升机器学习在IR drop或拥堵预测等下游任务中的准确性。

Conclusion: DALI-PD框架为生成合成布局热图提供了高效的解决方案，有助于未来机器学习在物理设计领域的研究与应用。

Abstract: Machine learning (ML) has demonstrated significant promise in various
physical design (PD) tasks. However, model generalizability remains limited by
the availability of high-quality, large-scale training datasets. Creating such
datasets is often computationally expensive and constrained by IP. While very
few public datasets are available, they are typically static, slow to generate,
and require frequent updates. To address these limitations, we present DALI-PD,
a scalable framework for generating synthetic layout heatmaps to accelerate ML
in PD research. DALI-PD uses a diffusion model to generate diverse layout
heatmaps via fast inference in seconds. The heatmaps include power, IR drop,
congestion, macro placement, and cell density maps. Using DALI-PD, we created a
dataset comprising over 20,000 layout configurations with varying macro counts
and placements. These heatmaps closely resemble real layouts and improve ML
accuracy on downstream ML tasks such as IR drop or congestion prediction.

</details>


### [161] [A Feed-Forward Artificial Intelligence Pipeline for Sustainable Desalination under Climate Uncertainties: UAE Insights](https://arxiv.org/abs/2507.10609)
*Obumneme Nwafor,Chioma Nwafor,Amro Zakaria,Nkechi Nwankwo*

Main category: cs.LG

TL;DR: 本文探讨了阿联酋通过开发预测性建模和基于规则的控制逻辑提高海水淡化设施可持续性的方法，并达成98%的准确率。


<details>
  <summary>Details</summary>
Motivation: 阿联酋超过90%的饮用水依赖能耗高的海水淡化，但受到气候变化如高温、盐度和气溶胶光学厚度(AOD)影响，急需解决可持续性问题。

Method: 提出两阶段预测模型框架：第一阶段用卫星数据预测AOD，第二阶段通过预测的AOD和气象因素评估淡化性能损失；并引入基于预测值的尘土感知规则控制逻辑。

Result: 该框架达成98%的预测准确率，利用SHAP方法揭示系统退化机理，开发交互式仪表盘用于情景和预测分析。

Conclusion: 研究成果提供了应对气候变化的决策支持工具，可提高阿联酋海水淡化设施的可持续性和运营效率。

Abstract: The United Arab Emirates (UAE) relies heavily on seawater desalination to
meet over 90% of its drinking water needs. Desalination processes are highly
energy intensive and account for approximately 15% of the UAE's electricity
consumption, contributing to over 22% of the country's energy-related CO2
emissions. Moreover, these processes face significant sustainability challenges
in the face of climate uncertainties such as rising seawater temperatures,
salinity, and aerosol optical depth (AOD). AOD greatly affects the operational
and economic performance of solar-powered desalination systems through
photovoltaic soiling, membrane fouling, and water turbidity cycles.
  This study proposes a novel pipelined two-stage predictive modelling
architecture: the first stage forecasts AOD using satellite-derived time series
and meteorological data; the second stage uses the predicted AOD and other
meteorological factors to predict desalination performance efficiency losses.
The framework achieved 98% accuracy, and SHAP (SHapley Additive exPlanations)
was used to reveal key drivers of system degradation. Furthermore, this study
proposes a dust-aware rule-based control logic for desalination systems based
on predicted values of AOD and solar efficiency. This control logic is used to
adjust the desalination plant feed water pressure, adapt maintenance
scheduling, and regulate energy source switching.
  To enhance the practical utility of the research findings, the predictive
models and rule-based controls were packaged into an interactive dashboard for
scenario and predictive analytics. This provides a management decision-support
system for climate-adaptive planning.

</details>


### [162] [FedGSCA: Medical Federated Learning with Global Sample Selector and Client Adaptive Adjuster under Label Noise](https://arxiv.org/abs/2507.10611)
*Mengwen Ye,Yingzi Huangfu,Shujian Gao,Wei Ren,Weifan Liu,Zekuan Yu*

Main category: cs.LG

TL;DR: 本文提出了一个名为FedGSCA的新框架，用于应对联邦学习中因标签噪声引起的模型性能下降与训练不稳定性问题，尤其是在医学图像分类领域。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习方法在处理医疗数据的噪声异质性和不平衡性方面存在不足，这影响了全局模型的稳定性和准确性。

Method: FedGSCA框架包括两个关键机制：1. 全局样本选择器（Global Sample Selector），用于聚合客户端的噪声知识，以解决噪声异质性并提升全局模型稳定性；2. 客户端自适应调整机制（Client Adaptive Adjustment），通过自适应阈值伪标签生成和鲁棒可信标签损失，动态调整样本分布，处理噪声标签并防止局部训练过拟合。

Result: 在一个实际的结肠切片数据集和两个不同噪声条件下的合成医学数据集上进行了实验，结果显示FedGSCA在包括极端和异质性噪声场景在内的多种情况下优于现有方法。

Conclusion: FedGSCA在提升模型稳定性和处理复杂噪声方面表现显著，适用于真实的医学联邦学习场景。

Abstract: Federated Learning (FL) emerged as a solution for collaborative medical image
classification while preserving data privacy. However, label noise, which
arises from inter-institutional data variability, can cause training
instability and degrade model performance. Existing FL methods struggle with
noise heterogeneity and the imbalance in medical data. Motivated by these
challenges, we propose FedGSCA, a novel framework for enhancing robustness in
noisy medical FL. FedGSCA introduces a Global Sample Selector that aggregates
noise knowledge from all clients, effectively addressing noise heterogeneity
and improving global model stability. Furthermore, we develop a Client Adaptive
Adjustment (CAA) mechanism that combines adaptive threshold pseudo-label
generation and Robust Credal Labeling Loss. CAA dynamically adjusts to class
distributions, ensuring the inclusion of minority samples and carefully
managing noisy labels by considering multiple plausible labels. This dual
approach mitigates the impact of noisy data and prevents overfitting during
local training, which improves the generalizability of the model. We evaluate
FedGSCA on one real-world colon slides dataset and two synthetic medical
datasets under various noise conditions, including symmetric, asymmetric,
extreme, and heterogeneous types. The results show that FedGSCA outperforms the
state-of-the-art methods, excelling in extreme and heterogeneous noise
scenarios. Moreover, FedGSCA demonstrates significant advantages in improving
model stability and handling complex noise, making it well-suited for
real-world medical federated learning scenarios.

</details>


### [163] [Sub-Scaling Laws: On the Role of Data Density and Training Strategies in LLMs](https://arxiv.org/abs/2507.10613)
*Zhengyu Chen,Siqi Wang,Teng Xiao,Yudong Wang,Shiqi Chen,Xunliang Cai,Junxian He,Jingang Wang*

Main category: cs.LG

TL;DR: 研究揭示在现有语言模型中，传统的扩展定律由于数据质量和训练策略的影响而未能完全生效。


<details>
  <summary>Details</summary>
Motivation: 探讨数据质量和训练策略对语言模型扩展性能的影响，解释为何性能改进可能无法持续加速。

Method: 通过对超过400个模型的详细实证分析，调查模型性能与数据密度和资源分配之间的关系。

Result: 发现高数据密度导致信息冗余，从而造成收益递减，而优化的资源分配对性能改进至关重要。提出适配于子扩展状态的新扩展定律模型。

Conclusion: 数据质量和多样性是模型性能扩展的关键因素。

Abstract: Traditional scaling laws in natural language processing suggest that
increasing model size and training data enhances performance. However, recent
studies reveal deviations, particularly in large language models, where
performance improvements decelerate, which is a phenomenon known as
sub-scaling. This paper revisits these scaling laws by examining the impact of
data quality and training strategies on model performance. Through extensive
empirical analysis of over 400 models, we identify high data density and
non-optimal resource allocation as key factors contributing to sub-scaling.
High data density leads to diminishing returns due to redundant information,
while optimal resource allocation is crucial for sustained performance
improvements. We propose a sub-optimal scaling law that better predicts
performance in sub-scaling regimes, highlighting the importance of data quality
and diversity.

</details>


### [164] [Fine-tuning Large Language Model for Automated Algorithm Design](https://arxiv.org/abs/2507.10614)
*Fei Liu,Rui Zhang,Xi Lin,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: 本研究探讨了针对算法设计任务微调大语言模型(LLMs)的必要性及其效果，提出了DAR采样策略以平衡训练数据的多样性与质量，结果表明微调后的模型在多种算法设计任务上表现优越。


<details>
  <summary>Details</summary>
Motivation: 当前大多数嵌入搜索程序的LLMs应用于一般性编码任务，但是否需要专门微调适配算法设计任务尚未明确。

Method: 提出多样性感知排名(DAR)采样策略，用于平衡训练数据的多样性与质量，并利用直接偏好优化方法对模型的输出进行高效对齐。

Result: 实验表明：微调后的较小模型在部分算法设计任务中性能超过未微调大模型，并能在不同设置下泛化到相关任务。

Conclusion: 任务特定的适配能够有效提高LLMs在算法设计任务上的表现，未来研究可以在这一基础上进一步探索。

Abstract: The integration of large language models (LLMs) into automated algorithm
design has shown promising potential. A prevalent approach embeds LLMs within
search routines to iteratively generate and refine candidate algorithms.
However, most existing methods rely on off-the-shelf LLMs trained for general
coding tasks,leaving a key question open: Do we need LLMs specifically tailored
for algorithm design? If so, how can such LLMs be effectively obtained and how
well can they generalize across different algorithm design tasks? In this
paper, we take a first step toward answering these questions by exploring
fine-tuning of LLMs for algorithm design. We introduce a Diversity-Aware Rank
based (DAR) sampling strategy to balance training data diversity and quality,
then we leverage direct preference optimization to efficiently align LLM
outputs with task objectives. Our experiments, conducted on
Llama-3.2-1B-Instruct and Llama- 3.1-8B-Instruct, span three distinct algorithm
design tasks. Results suggest that finetuned LLMs can significantly outperform
their off-the-shelf counterparts with the smaller Llama-3.2-1B-Instruct and
match the larger Llama-3.1-8B-Instruct on the admissible set problem. Moreover,
we observe promising generalization: LLMs finetuned on specific algorithm
design tasks also improve performance on related tasks with varying settings.
These findings highlight the value of task-specific adaptation for LLMs in
algorithm design and open new avenues for future research.

</details>


### [165] [Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them](https://arxiv.org/abs/2507.10616)
*Neel Rajani,Aryo Pradipta Gema,Seraphina Goldfarb-Tarrant,Ivan Titov*

Main category: cs.LG

TL;DR: 本文比较了强化学习（RL）和监督微调（SFT）在大型语言模型数学推理训练中的表现，发现两者各有得失且机制不同。


<details>
  <summary>Details</summary>
Motivation: 探讨强化学习（RL）和监督微调（SFT）在基于数学和代码数据集训练大型语言模型中对模型性能的不同影响机制。

Method: 在相同数学问题、模型和类似超参数下对RL和SFT进行对比分析；研究模型参数在两个算法中的变化；通过冻结模型部分参数实验测试知识密集型基准性能下降的可能缓解方法。

Result: RL在数学推理域内小幅提升，但在MMLU等知识密集型基准上则稍有退化；SFT在这两个方面的趋势更为明显；参数更新分析显示，SFT影响中层MLP较大，可能是性能退化原因；冻结参数实验未得出明确结论，各基准测试表现不一。

Conclusion: RL可能加强现有能力，而SFT则可能用新能力取代旧能力；需进一步研究以明确其机制和应用效果。

Abstract: Training large language models (LLMs) for reasoning via maths and code
datasets has become a major new focus in LLM post-training. Two particularly
popular approaches are reinforcement learning (RL) and supervised fine-tuning
(SFT), but their training dynamics are poorly understood. We present a
comparative analysis of RL and SFT on the same maths problems with the same
model and similar hyperparameters. We find that RL yields minor in-domain gains
on maths and slight degradation on knowledge-intensive benchmarks like MMLU,
while both trends are more pronounced in SFT. We also analyse model parameters
across checkpoints, observing that both algorithms modify query and key weights
the most. Meanwhile, SFT exhibits greater updates and also affects mid-layer
MLPs more, leading us to hypothesise that this may have caused the
out-of-domain degradation. We therefore investigate whether freezing parts of
the model during training can mitigate the reduced performance on
knowledge-intensive benchmarks. However, our results are inconclusive, with
benefits on GPQA:Diamond and degradation on other benchmarks. Taken together,
our observations provide a preliminary indication for why RL amplifies existing
capabilities, while SFT replaces old skills with new ones.

</details>


### [166] [Compute Requirements for Algorithmic Innovation in Frontier AI Models](https://arxiv.org/abs/2507.10618)
*Peter Barnett*

Main category: cs.LG

TL;DR: 研究评估了算法创新所需的计算资源，并分析了限制计算能力对创新的影响。


<details>
  <summary>Details</summary>
Motivation: 探索大模型预训练中的计算需求与算法创新之间的关系，研究如何限制计算能力对算法进步的影响。

Method: 编译Llama 3和DeepSeek-V3中36种预训练算法创新，估算其开发所需的总FLOP和硬件性能，并分析计算限制对创新的影响。

Result: 发现需要大量资源的创新每年翻倍增长，但即使在严格的计算限制下，仍能实现一半的创新。

Conclusion: 单独依赖计算限制可能不足以显著减缓AI算法进步，因为多数创新仍在限制下可实现。

Abstract: Algorithmic innovation in the pretraining of large language models has driven
a massive reduction in the total compute required to reach a given level of
capability. In this paper we empirically investigate the compute requirements
for developing algorithmic innovations. We catalog 36 pre-training algorithmic
innovations used in Llama 3 and DeepSeek-V3. For each innovation we estimate
both the total FLOP used in development and the FLOP/s of the hardware
utilized. Innovations using significant resources double in their requirements
each year. We then use this dataset to investigate the effect of compute caps
on innovation. Our analysis suggests that compute caps alone are unlikely to
dramatically slow AI algorithmic progress. Even stringent compute caps -- such
as capping total operations to the compute used to train GPT-2 or capping
hardware capacity to 8 H100 GPUs -- could still have allowed for half of the
cataloged innovations.

</details>


### [167] [Meta-Reinforcement Learning for Fast and Data-Efficient Spectrum Allocation in Dynamic Wireless Networks](https://arxiv.org/abs/2507.10619)
*Oluwaseyi Giwa,Tobi Awodunmila,Muhammad Ahmed Mohsin,Ahsan Bilal,Muhammad Ali Jamshed*

Main category: cs.LG

TL;DR: 提出一种基于元学习的框架，用于提升5G/6G网络中的动态频谱分配效率，相比传统DRL方法，提高性能和资源分配公平性，同时降低信号干扰和时延。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习在动态频谱分配中样本复杂度高且探索过程存在安全风险，因此需要一种更高效且安全的学习方法。

Method: 提出一个元学习框架，包括三种架构：MAML、RNN和Attention增强型RNN，并在一个仿真实验环境中进行测试，比较其与非元学习算法PPO的性能表现。

Result: 提升了网络吞吐量，减少了信噪比和时延违规率，且达到了更高的资源分配公平性（公平指数0.7），相比PPO表现大幅领先。

Conclusion: 元学习在复杂无线网络的智能控制中更加高效、安全，是传统DRL的理想替代方案。

Abstract: The dynamic allocation of spectrum in 5G / 6G networks is critical to
efficient resource utilization. However, applying traditional deep
reinforcement learning (DRL) is often infeasible due to its immense sample
complexity and the safety risks associated with unguided exploration, which can
cause severe network interference. To address these challenges, we propose a
meta-learning framework that enables agents to learn a robust initial policy
and rapidly adapt to new wireless scenarios with minimal data. We implement
three meta-learning architectures, model-agnostic meta-learning (MAML),
recurrent neural network (RNN), and an attention-enhanced RNN, and evaluate
them against a non-meta-learning DRL algorithm, proximal policy optimization
(PPO) baseline, in a simulated dynamic integrated access/backhaul (IAB)
environment. Our results show a clear performance gap. The attention-based
meta-learning agent reaches a peak mean network throughput of 48 Mbps, while
the PPO baseline decreased drastically to 10 Mbps. Furthermore, our method
reduces SINR and latency violations by more than 50% compared to PPO. It also
shows quick adaptation, with a fairness index 0.7, showing better resource
allocation. This work proves that meta-learning is a very effective and safer
option for intelligent control in complex wireless systems.

</details>


### [168] [LLMs Meet Cross-Modal Time Series Analytics: Overview and Directions](https://arxiv.org/abs/2507.10620)
*Chenxi Liu,Hao Miao,Cheng Long,Yan Zhao,Ziyue Li,Panos Kalnis*

Main category: cs.LG

TL;DR: 本文概述了使用大型语言模型（LLMs）进行跨模态时间序列分析的最新进展与方法。


<details>
  <summary>Details</summary>
Motivation: 通过利用LLMs强大的参数能力，解决时间序列数据和文本数据之间的模态差异问题，以拓展其在真实场景中的应用。

Method: 提出了一个分类法，将现有方法按跨模态建模策略分为三类：转换、对齐和融合，并探讨了这些方法在下游任务中的应用，同时总结了未解决的挑战。

Result: 参与者将全面了解跨模态时间序列分析领域的最新进展、技术方法和未来研究方向。

Conclusion: 本研究旨在平衡效果与效率，促进LLMs在跨模态时间序列分析中的实际应用。

Abstract: Large Language Models (LLMs) have emerged as a promising paradigm for time
series analytics, leveraging their massive parameters and the shared sequential
nature of textual and time series data. However, a cross-modality gap exists
between time series and textual data, as LLMs are pre-trained on textual
corpora and are not inherently optimized for time series. In this tutorial, we
provide an up-to-date overview of LLM-based cross-modal time series analytics.
We introduce a taxonomy that classifies existing approaches into three groups
based on cross-modal modeling strategies, e.g., conversion, alignment, and
fusion, and then discuss their applications across a range of downstream tasks.
In addition, we summarize several open challenges. This tutorial aims to expand
the practical application of LLMs in solving real-world problems in cross-modal
time series analytics while balancing effectiveness and efficiency.
Participants will gain a thorough understanding of current advancements,
methodologies, and future research directions in cross-modal time series
analytics.

</details>


### [169] [Flows and Diffusions on the Neural Manifold](https://arxiv.org/abs/2507.10623)
*Daniel Saragih,Deyu Cao,Tejas Balaji*

Main category: cs.LG

TL;DR: 研究将扩散与流动生成模型应用于权重空间学习，利用基于优化的诱导偏置，优化了权重生成能力和安全性。


<details>
  <summary>Details</summary>
Motivation: 扩散和流动生成模型已经在图像合成、视频生成及自然语言建模上取得成功，计划将这种成功扩展到权重空间学习中。

Method: 提出将梯度下降轨迹建模为轨迹推理问题，统一了多种轨迹推理方法，提出了基于梯度流匹配框架，并探索了编码器权重表示、任务数据上下文条件及奖励精调等方法。

Result: 实验显示该方法在生成权重、下游初始化训练及细调性能上优于基线方法。同时在检测危害性协变量转移方面也表现更优。

Conclusion: 通过该方法，改进了权重初始化和生成能力，尤其针对安全关键系统，该方法在任务表现和异常检测上具有实际价值。

Abstract: Diffusion and flow-based generative models have achieved remarkable success
in domains such as image synthesis, video generation, and natural language
modeling. In this work, we extend these advances to weight space learning by
leveraging recent techniques to incorporate structural priors derived from
optimization dynamics. Central to our approach is modeling the trajectory
induced by gradient descent as a trajectory inference problem. We unify several
trajectory inference techniques under the framework of gradient flow matching,
providing a theoretical framework for treating optimization paths as inductive
bias. We further explore architectural and algorithmic choices, including
reward fine-tuning by adjoint matching, the use of autoencoders for latent
weight representation, conditioning on task-specific context data, and adopting
informative source distributions such as Kaiming uniform. Experiments
demonstrate that our method matches or surpasses baselines in generating
in-distribution weights, improves initialization for downstream training, and
supports fine-tuning to enhance performance. Finally, we illustrate a practical
application in safety-critical systems: detecting harmful covariate shifts,
where our method outperforms the closest comparable baseline.

</details>


### [170] [Player-Team Heterogeneous Interaction Graph Transformer for Soccer Outcome Prediction](https://arxiv.org/abs/2507.10626)
*Lintao Wang,Shiwen Xu,Michael Horton,Joachim Gudmundsson,Zhiyong Wang*

Main category: cs.LG

TL;DR: 本论文提出了一种名为HIGFormer的模型，通过图增强的Transformer架构，解决现有方法在预测足球比赛结果时对异质交互关系建模的不足，实验结果验证了其在预测准确性上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的足球比赛结果预测方法常忽视球员与球队之间的异质交互关系，而这一关系对于准确建模比赛动态至关重要。

Method: 提出HIGFormer模型，结合了球员交互网络、队伍交互网络和比赛比较Transformer，分别捕捉微观的球员动态与宏观的队伍交互。

Result: 在大规模真实足球数据集WyScout上，HIGFormer显著优于现有方法，提供了更高的预测精度。

Conclusion: HIGFormer不仅提升了比赛结果预测的准确性，还能用于球员表现评估，为选才和团队策略分析提供了新视角。

Abstract: Predicting soccer match outcomes is a challenging task due to the inherently
unpredictable nature of the game and the numerous dynamic factors influencing
results. While it conventionally relies on meticulous feature engineering, deep
learning techniques have recently shown a great promise in learning effective
player and team representations directly for soccer outcome prediction.
However, existing methods often overlook the heterogeneous nature of
interactions among players and teams, which is crucial for accurately modeling
match dynamics. To address this gap, we propose HIGFormer (Heterogeneous
Interaction Graph Transformer), a novel graph-augmented transformer-based deep
learning model for soccer outcome prediction. HIGFormer introduces a
multi-level interaction framework that captures both fine-grained player
dynamics and high-level team interactions. Specifically, it comprises (1) a
Player Interaction Network, which encodes player performance through
heterogeneous interaction graphs, combining local graph convolutions with a
global graph-augmented transformer; (2) a Team Interaction Network, which
constructs interaction graphs from a team-to-team perspective to model
historical match relationships; and (3) a Match Comparison Transformer, which
jointly analyzes both team and player-level information to predict match
outcomes. Extensive experiments on the WyScout Open Access Dataset, a
large-scale real-world soccer dataset, demonstrate that HIGFormer significantly
outperforms existing methods in prediction accuracy. Furthermore, we provide
valuable insights into leveraging our model for player performance evaluation,
offering a new perspective on talent scouting and team strategy analysis.

</details>


### [171] [GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning](https://arxiv.org/abs/2507.10628)
*Ziru Liu,Cheng Gong,Xinyu Fu,Yaofang Liu,Ran Chen,Shoubo Hu,Suiyun Zhang,Rui Liu,Qingfu Zhang,Dandan Tu*

Main category: cs.LG

TL;DR: 提出了一种新的强化学习框架GHPO，通过动态调整任务难度来提高训练效率和模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决当前强化学习方法在训练小型高效语言模型时存在的训练不稳定与效率低下的问题。

Method: 提出指导性混合策略优化（GHPO）方法，通过自适应提示优化实现难度动态调整，结合模仿学习和探索式强化学习形成平滑的学习路径。

Result: 在六项数学基准测试中，GHPO实现了约5%的平均性能提升，并优于其他强化学习和课程学习基线方法。

Conclusion: GHPO有效提高了训练的稳定性和最终推理性能，为开发高效可靠的推理模型提供了可扩展的解决方案。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a powerful paradigm for facilitating the self-improvement of large language
models (LLMs), particularly in the domain of complex reasoning tasks. However,
prevailing on-policy RL methods often contend with significant training
instability and inefficiency. This is primarily due to a capacity-difficulty
mismatch, where the complexity of training data frequently outpaces the model's
current capabilities, leading to critically sparse reward signals and stalled
learning progress. This challenge is particularly acute for smaller, more
resource-efficient LLMs. To overcome this, we introduce the Guided Hybrid
Policy Optimization (GHPO), a novel difficulty-aware reinforcement learning
framework. GHPO dynamically calibrates task difficulty by employing adaptive
prompt refinement to provide targeted guidance. This unique approach adaptively
balances direct imitation learning for problems currently beyond the model's
reach with exploration-based reinforcement learning for more manageable tasks,
effectively creating a smooth and optimized learning curriculum. Extensive
experiments demonstrate that GHPO achieves an average performance gain of
approximately 5% across six challenging mathematics benchmarks, consistently
outperforming strong on-policy reinforcement learning and curriculum learning
baselines. Further analysis confirms that our framework significantly enhances
both training stability and final reasoning performance, thus offering a
scalable and efficient solution for developing powerful and robust reasoning
models.

</details>


### [172] [Scalable Unsupervised Segmentation via Random Fourier Feature-based Gaussian Process](https://arxiv.org/abs/2507.10632)
*Issei Saito,Masatoshi Nagano,Tomoaki Nakamura,Daichi Mochihashi,Koki Mimura*

Main category: cs.LG

TL;DR: 提出了RFF-GP-HSMM方法，通过随机傅里叶特征（RFF）降低GP-HSMM的高计算成本，速度提高约278倍，同时保持分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决GP-HSMM在处理大规模时间序列数据时，由于核矩阵求逆造成的高计算成本问题。

Method: 通过使用随机傅里叶特征（RFF），将高斯过程转化为线性回归，避免核矩阵求逆，保持模型表达能力。

Result: 在CMU动作捕捉数据集的实验中，与传统方法相比，RFF-GP-HSMM保持了相似的分割性能，同时分割速度提高了约278倍。

Conclusion: RFF-GP-HSMM是一种快速有效的时间序列分割方法，能够显著降低计算成本，同时达到与传统方法相当的性能。

Abstract: In this paper, we propose RFF-GP-HSMM, a fast unsupervised time-series
segmentation method that incorporates random Fourier features (RFF) to address
the high computational cost of the Gaussian process hidden semi-Markov model
(GP-HSMM). GP-HSMM models time-series data using Gaussian processes, requiring
inversion of an N times N kernel matrix during training, where N is the number
of data points. As the scale of the data increases, matrix inversion incurs a
significant computational cost. To address this, the proposed method
approximates the Gaussian process with linear regression using RFF, preserving
expressive power while eliminating the need for inversion of the kernel matrix.
Experiments on the Carnegie Mellon University (CMU) motion-capture dataset
demonstrate that the proposed method achieves segmentation performance
comparable to that of conventional methods, with approximately 278 times faster
segmentation on time-series data comprising 39,200 frames.

</details>


### [173] [A Simple Baseline for Stable and Plastic Neural Networks](https://arxiv.org/abs/2507.10637)
*É. Künzel,A. Jaziri,V. Ramesh*

Main category: cs.LG

TL;DR: 本文提出了一种用于计算机视觉的简单连续学习方法，称为RDBP，通过轻量的激活修改和梯度调度机制有效平衡了塑性与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的连续学习方法在适应新的任务流时往往在塑性与稳定性之间难以平衡，作者希望提出一种既能高效适应新任务，又能减少忘却的方案。

Method: 提出RDBP方法，其中包含两个机制：ReLUDown（通过激活修改提升神经元敏感性并防止其失活）和Decreasing Backpropagation（一种渐进保护早期层免受剧烈更新的梯度调度方案）。

Result: 在Continual ImageNet基准测试中，RDBP在保证或超越前沿方法性能的同时，显著减少了计算开销。

Conclusion: RDBP为现实中的连续学习任务提供了可行的解决方案，同时也成为衡量未来策略的新基准。

Abstract: Continual learning in computer vision requires that models adapt to a
continuous stream of tasks without forgetting prior knowledge, yet existing
approaches often tip the balance heavily toward either plasticity or stability.
We introduce RDBP, a simple, low-overhead baseline that unites two
complementary mechanisms: ReLUDown, a lightweight activation modification that
preserves feature sensitivity while preventing neuron dormancy, and Decreasing
Backpropagation, a biologically inspired gradient-scheduling scheme that
progressively shields early layers from catastrophic updates. Evaluated on the
Continual ImageNet benchmark, RDBP matches or exceeds the plasticity and
stability of state-of-the-art methods while reducing computational cost. RDBP
thus provides both a practical solution for real-world continual learning and a
clear benchmark against which future continual learning strategies can be
measured.

</details>


### [174] [ZClassifier: Temperature Tuning and Manifold Approximation via KL Divergence on Logit Space](https://arxiv.org/abs/2507.10638)
*Shim Soon Yong*

Main category: cs.LG

TL;DR: 本文提出了一个新的分类框架ZClassifier，通过将传统的确定性logits替换为对角高斯分布的logits，实现对分类问题中的不确定性校准与流形逼近的统一处理。


<details>
  <summary>Details</summary>
Motivation: 现有分类器在不确定性校准与流形控制上缺乏统一的方法。

Method: 提出ZClassifier框架，采用对角高斯分布logits，并通过最小化预测的高斯分布与单位各向同性高斯分布的KL散度，实现温度缩放与流形控制的一体化。

Result: 实验结果表明，ZClassifier在CIFAR-10和CIFAR-100数据集上的鲁棒性、不确定性校准和潜在分离性能优于基于softmax的分类器。

Conclusion: ZClassifier在概率框架下实现了分类器的几何一致性和不确定性校准，并在生成任务中具有潜在的语义控制能力。

Abstract: We introduce a novel classification framework, ZClassifier, that replaces
conventional deterministic logits with diagonal Gaussian-distributed logits.
Our method simultaneously addresses temperature scaling and manifold
approximation by minimizing the Kullback-Leibler (KL) divergence between the
predicted Gaussian distributions and a unit isotropic Gaussian. This unifies
uncertainty calibration and latent control in a principled probabilistic
manner, enabling a natural interpretation of class confidence and geometric
consistency. Experiments on CIFAR-10 and CIFAR-100 show that ZClassifier
improves over softmax classifiers in robustness, calibration, and latent
separation. We also demonstrate its effectiveness for classifier-guided
generation by interpreting logits as Gaussian semantic potentials.

</details>


### [175] [First-of-its-kind AI model for bioacoustic detection using a lightweight associative memory Hopfield neural network](https://arxiv.org/abs/2507.10642)
*Andrew Gascoyne,Wendy Lomas*

Main category: cs.LG

TL;DR: 本文提出了一种利用透明且可解释的Hopfield神经网络的新型AI模型，用于快速、轻量、可持续的生物声学分析。


<details>
  <summary>Details</summary>
Motivation: 当前生物保护声学领域面临海量声学数据分析的挑战，现有AI模型存在训练数据有限、能耗和碳足迹高以及硬件要求高等问题。

Method: 开发了一种基于Hopfield神经网络的模型，通过关联记忆存储信号，检测类似信号以分类物种。模型训练仅需代表性信号，快速轻量化，支持标准个人设备甚至边缘设备部署。

Result: 该模型精准度高（达86%），仅需3ms训练，5.4s即可处理10384条数据，占用RAM仅144.09MB，轻便高效，并与人工鉴定结果完全一致。

Conclusion: 提出了一种快速、轻量、可持续、透明且高准确性的AI模型，有潜力成为生物声学分析的变革性工具。

Abstract: A growing issue within conservation bioacoustics is the task of analysing the
vast amount of data generated from the use of passive acoustic monitoring
devices. In this paper, we present an alternative AI model which has the
potential to help alleviate this problem. Our model formulation addresses the
key issues encountered when using current AI models for bioacoustic analysis,
namely the: limited training data available; environmental impact, particularly
in energy consumption and carbon footprint of training and implementing these
models; and associated hardware requirements. The model developed in this work
uses associative memory via a transparent, explainable Hopfield neural network
to store signals and detect similar signals which can then be used to classify
species. Training is rapid ($3$\,ms), as only one representative signal is
required for each target sound within a dataset. The model is fast, taking only
$5.4$\,s to pre-process and classify all $10384$ publicly available bat
recordings, on a standard Apple MacBook Air. The model is also lightweight with
a small memory footprint of $144.09$\,MB of RAM usage. Hence, the low
computational demands make the model ideal for use on a variety of standard
personal devices with potential for deployment in the field via edge-processing
devices. It is also competitively accurate, with up to $86\%$ precision on the
dataset used to evaluate the model. In fact, we could not find a single case of
disagreement between model and manual identification via expert field guides.
Although a dataset of bat echolocation calls was chosen to demo this
first-of-its-kind AI model, trained on only two representative calls, the model
is not species specific. In conclusion, we propose an equitable AI model that
has the potential to be a game changer for fast, lightweight, sustainable,
transparent, explainable and accurate bioacoustic analysis.

</details>


### [176] [A Simple Approximate Bayesian Inference Neural Surrogate for Stochastic Petri Net Models](https://arxiv.org/abs/2507.10714)
*Bright Kwaku Manu,Trevor Reckell,Beckett Sterner,Petar Jevtic*

Main category: cs.LG

TL;DR: 提出了一种基于神经网络的替代框架，用于从部分观察数据中估计随机Petri网（SPNs）的参数，该方法更高效并具有不确定性校准能力。


<details>
  <summary>Details</summary>
Motivation: 随机Petri网（SPNs）在描述离散事件系统中具有重要应用，但其参数估计在存在外部协变量和缺乏显式似然的情况下非常具有挑战性。

Method: 引入一个基于神经网络的后验分布近似模型，通过1D卷积残差网络从噪声、部分观察数据中直接预测协变量相关的速率函数系数，并使用蒙特卡罗Dropout实现不确定性校准。

Result: 在20%事件丢失的合成数据上，替代模型实现了RMSE = 0.108的系数估计误差，比传统贝叶斯方法显著加速。

Conclusion: 数据驱动、无显式似然的替代模型可在复杂、部分观察的离散事件系统中实现准确、可靠、实时的参数估计。

Abstract: Stochastic Petri Nets (SPNs) are an increasingly popular tool of choice for
modeling discrete-event dynamics in areas such as epidemiology and systems
biology, yet their parameter estimation remains challenging in general and in
particular when transition rates depend on external covariates and explicit
likelihoods are unavailable. We introduce a neural-surrogate
(neural-network--based approximation of the posterior distribution) framework
that predicts the coefficients of known covariate-dependent rate functions
directly from noisy, partially observed token trajectories. Our model employs a
lightweight 1D Convolutional Residual Network trained end-to-end on
Gillespie-simulated SPN realizations, learning to invert system dynamics under
realistic conditions of event dropout. During inference, Monte Carlo dropout
provides calibrated uncertainty bounds together with point estimates. On
synthetic SPNs with 20% missing events, our surrogate recovers rate-function
coefficients with an RMSE = 0.108 and substantially runs faster than
traditional Bayesian approaches. These results demonstrate that data-driven,
likelihood-free surrogates can enable accurate, robust, and real-time parameter
recovery in complex, partially observed discrete-event systems.

</details>


### [177] [Distributionally Robust Optimization with Adversarial Data Contamination](https://arxiv.org/abs/2507.10718)
*Shuyao Li,Ilias Diakonikolas,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: 提出了一个统一解决数据污染和分布漂移影响的Wasserstein-1 DRO问题方法，并证明了其估计误差界和高效性。


<details>
  <summary>Details</summary>
Motivation: 应对分布不确定性（Distributional Uncertainty）和随之而来的决策影响，同时解决由训练数据中存在异常值（Outliers）导致的模型效果退化问题。

Method: 通过构建一个适用于广义线性模型的Wasserstein-1 DRO目标优化框架，结合凸Lipschitz损失函数，并设计了一个受鲁棒统计启发的高效优化算法，应对训练数据污染和分布转移的双重影响。

Result: 在有限协方差条件下，仅使用污染数据即可实现$O(\sqrt{\epsilon})$的估计误差，同时具有理论上的鲁棒性保证。

Conclusion: 论文开创性地建立了用于应对数据污染和分布漂移挑战的有效学习方法，不仅提供了理论保证，还具有高效的计算性能。

Abstract: Distributionally Robust Optimization (DRO) provides a framework for
decision-making under distributional uncertainty, yet its effectiveness can be
compromised by outliers in the training data. This paper introduces a
principled approach to simultaneously address both challenges. We focus on
optimizing Wasserstein-1 DRO objectives for generalized linear models with
convex Lipschitz loss functions, where an $\epsilon$-fraction of the training
data is adversarially corrupted. Our primary contribution lies in a novel
modeling framework that integrates robustness against training data
contamination with robustness against distributional shifts, alongside an
efficient algorithm inspired by robust statistics to solve the resulting
optimization problem. We prove that our method achieves an estimation error of
$O(\sqrt{\epsilon})$ for the true DRO objective value using only the
contaminated data under the bounded covariance assumption. This work
establishes the first rigorous guarantees, supported by efficient computation,
for learning under the dual challenges of data contamination and distributional
shifts.

</details>


### [178] [Ground-Compose-Reinforce: Tasking Reinforcement Learning Agents through Formal Language](https://arxiv.org/abs/2507.10741)
*Andrew C. Li,Toryn Q. Klassen,Andrew Wang,Parand A. Alamdari,Sheila A. McIlraith*

Main category: cs.LG

TL;DR: 本文提出了一种名为Ground-Compose-Reinforce的神经符号框架，用于通过形式化语言直接指导强化学习（RL）代理，并表现出数据效率高和泛化性能强的特点。


<details>
  <summary>Details</summary>
Motivation: 语言在复杂的感知和行动场景中的落地是构建能够与人类交流的情境代理的关键挑战，以往的方法通常依赖手工设计语言对环境的映射或大量数据集的整理。

Method: 提出Ground-Compose-Reinforce框架，通过数据驱动的方式将正式语言（formal language）与行为相关联，避免了手动设计奖励函数或符号检测器。该框架通过形式语言的组合语义实现数据高效的语言落地和任意语言组合的泛化能力。

Result: 实验证明该框架在基于图像的网格世界和MuJoCo机器人领域中，仅需少量数据即可将正式语言指令可靠地映射为行为，而传统的端到端数据驱动方法则失败。

Conclusion: 该框架实现了高效的数据驱动学习，克服了手动设计和数据需求的挑战，为提升基于语言的强化学习代理的性能提供了有力工具。

Abstract: Grounding language in complex perception (e.g. pixels) and action is a key
challenge when building situated agents that can interact with humans via
language. In past works, this is often solved via manual design of the language
grounding or by curating massive datasets relating language to elements of the
environment. We propose Ground-Compose-Reinforce, a neurosymbolic framework for
grounding formal language from data, and eliciting behaviours by directly
tasking RL agents through this language. By virtue of data-driven learning, our
framework avoids the manual design of domain-specific elements like reward
functions or symbol detectors. By virtue of compositional formal language
semantics, our framework achieves data-efficient grounding and generalization
to arbitrary language compositions. Experiments on an image-based gridworld and
a MuJoCo robotics domain show that our approach reliably maps formal language
instructions to behaviours with limited data while end-to-end, data-driven
approaches fail.

</details>


### [179] [A Benchmarking Framework for AI models in Automotive Aerodynamics](https://arxiv.org/abs/2507.10747)
*Kaustubh Tangsali,Rishikesh Ranade,Mohammad Amin Nabian,Alexey Kamenev,Peter Sharpe,Neil Ashton,Ram Cherukuri,Sanjay Choudhry*

Main category: cs.LG

TL;DR: 文章介绍了一个用于汽车空气动力学预测AI模型的开源评测框架，分析和比较模型精度、性能、可扩展性等。


<details>
  <summary>Details</summary>
Motivation: 提高汽车空气动力学预测模型发展与透明性，推动研究和创新。

Method: 提供了标准化的评测方法，整合多个评估指标，对DoMINO、X-MeshGraphNet和FIGConvNet模型进行了验证。

Result: 框架增强了AI模型性能评测的透明性和一致性，并验证了其在三种AI模型上的应用效果。

Conclusion: 将帮助研究者和企业优化和推进AI驱动的空气动力学建模方法，推动高效、精确和可解释解决方案的发展。

Abstract: In this paper, we introduce a benchmarking framework within the open-source
NVIDIA PhysicsNeMo-CFD framework designed to systematically assess the
accuracy, performance, scalability, and generalization capabilities of AI
models for automotive aerodynamics predictions. The open extensible framework
enables incorporation of a diverse set of metrics relevant to the
Computer-Aided Engineering (CAE) community. By providing a standardized
methodology for comparing AI models, the framework enhances transparency and
consistency in performance assessment, with the overarching goal of improving
the understanding and development of these models to accelerate research and
innovation in the field. To demonstrate its utility, the framework includes
evaluation of both surface and volumetric flow field predictions on three AI
models: DoMINO, X-MeshGraphNet, and FIGConvNet using the DrivAerML dataset. It
also includes guidelines for integrating additional models and datasets, making
it extensible for physically consistent metrics. This benchmarking study aims
to enable researchers and industry professionals in selecting, refining, and
advancing AI-driven aerodynamic modeling approaches, ultimately fostering the
development of more efficient, accurate, and interpretable solutions in
automotive aerodynamics

</details>


### [180] [Spatial Reasoners for Continuous Variables in Any Domain](https://arxiv.org/abs/2507.10768)
*Bart Pogodzinski,Christopher Wewer,Bernt Schiele,Jan Eric Lenssen*

Main category: cs.LG

TL;DR: 本文提出了一个名为Spatial Reasoners的软件框架，用于利用生成去噪模型在连续变量上的空间推理。


<details>
  <summary>Details</summary>
Motivation: 生成式去噪模型在处理复杂、高维分布的图像生成上表现出色，并开始用于多维连续变量推理，但实现基础设施难度较高。

Method: 设计了一个灵活的框架，可以支持不同的去噪模型、采样器和推理策略的集成和控制，简化实现过程。

Result: 提供用户友好接口，提升了更加广泛的研究和应用可能性，并向公众开放。

Conclusion: Spatial Reasoners通过其通用框架，促进了利用生成去噪模型进行空间推理研究的发展。

Abstract: We present Spatial Reasoners, a software framework to perform spatial
reasoning over continuous variables with generative denoising models. Denoising
generative models have become the de-facto standard for image generation, due
to their effectiveness in sampling from complex, high-dimensional
distributions. Recently, they have started being explored in the context of
reasoning over multiple continuous variables. Providing infrastructure for
generative reasoning with such models requires a high effort, due to a wide
range of different denoising formulations, samplers, and inference strategies.
Our presented framework aims to facilitate research in this area, providing
easy-to-use interfaces to control variable mapping from arbitrary data domains,
generative model paradigms, and inference strategies. Spatial Reasoners are
openly available at https://spatialreasoners.github.io/

</details>


### [181] [A Generalizable Physics-Enhanced State Space Model for Long-Term Dynamics Forecasting in Complex Environments](https://arxiv.org/abs/2507.10792)
*Yuchen Wang,Hongjue Zhao,Haohong Lin,Enze Xu,Lifang He,Huajie Shao*

Main category: cs.LG

TL;DR: 本文讨论一种名为Phy-SSM的新方法，用于在复杂环境中进行长期动态预测，该方法将部分物理知识集成到状态空间模型中。


<details>
  <summary>Details</summary>
Motivation: 当前的预测方法很难在噪声和不规则采样数据的复杂场景中进行长期外推，因而需要提升预测性能和泛化能力的方法。

Method: 提出了Phy-SSM方法，将部分已知物理特性分解为已知和未知的状态矩阵，集成到状态空间模型中，并通过物理状态正则化项增强预测的长期表现。

Result: 实验结果表明，在车辆运动预测、无人机状态预测和COVID-19疫情预测等三种实际应用中，Phy-SSM在长期插值和外推任务上优于现有基线方法。

Conclusion: Phy-SSM结合物理知识和状态空间模型，在复杂环境中的预测能力突出，展示了其在真实场景中的有效性和泛化能力。

Abstract: This work aims to address the problem of long-term dynamic forecasting in
complex environments where data are noisy and irregularly sampled. While recent
studies have introduced some methods to improve prediction performance, these
approaches still face a significant challenge in handling long-term
extrapolation tasks under such complex scenarios. To overcome this challenge,
we propose Phy-SSM, a generalizable method that integrates partial physics
knowledge into state space models (SSMs) for long-term dynamics forecasting in
complex environments. Our motivation is that SSMs can effectively capture
long-range dependencies in sequential data and model continuous dynamical
systems, while the incorporation of physics knowledge improves generalization
ability. The key challenge lies in how to seamlessly incorporate partially
known physics into SSMs. To achieve this, we decompose partially known system
dynamics into known and unknown state matrices, which are integrated into a
Phy-SSM unit. To further enhance long-term prediction performance, we introduce
a physics state regularization term to make the estimated latent states align
with system dynamics. Besides, we theoretically analyze the uniqueness of the
solutions for our method. Extensive experiments on three real-world
applications, including vehicle motion prediction, drone state prediction, and
COVID-19 epidemiology forecasting, demonstrate the superior performance of
Phy-SSM over the baselines in both long-term interpolation and extrapolation
tasks. The code is available at https://github.com/511205787/Phy_SSM-ICML2025.

</details>


### [182] [Multi-Armed Sampling Problem and the End of Exploration](https://arxiv.org/abs/2507.10797)
*Mohammad Pedramfar,Siamak Ravanbakhsh*

Main category: cs.LG

TL;DR: 本文提出了多臂抽样框架，这是多臂赌博机问题优化问题的抽样对应，并探讨了探索与利用之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究探索与利用权衡在抽样中的表现及影响。

Method: 系统定义了合理的遗憾度量并提出一种简单算法以实现最优遗憾界限，同时将多臂抽样与多臂赌博机问题进行统一建模。

Result: 理论结果表明，在抽样问题中与优化问题不同，并不需要探索。此外，提出的一系列问题和遗憾度量统一了多臂抽样和多臂赌博机框架。

Conclusion: 多臂抽样框架可为包括神经抽样器在内的抽样研究奠定基础，对强化学习中的探索需求、算法收敛性质及相关领域问题提供重要启示。

Abstract: This paper introduces the framework of multi-armed sampling, as the sampling
counterpart to the optimization problem of multi-arm bandits. Our primary
motivation is to rigorously examine the exploration-exploitation trade-off in
the context of sampling. We systematically define plausible notions of regret
for this framework and establish corresponding lower bounds. We then propose a
simple algorithm that achieves these optimal regret bounds. Our theoretical
results demonstrate that in contrast to optimization, sampling does not require
exploration. To further connect our findings with those of multi-armed bandits,
we define a continuous family of problems and associated regret measures that
smoothly interpolates and unifies multi-armed sampling and multi-armed bandit
problems using a temperature parameter. We believe the multi-armed sampling
framework, and our findings in this setting can have a foundational role in the
study of sampling including recent neural samplers, akin to the role of
multi-armed bandits in reinforcement learning. In particular, our work sheds
light on the need for exploration and the convergence properties of algorithm
for entropy-regularized reinforcement learning, fine-tuning of pretrained
models and reinforcement learning with human feedback (RLHF).

</details>


### [183] [Uncovering Causal Relation Shifts in Event Sequences under Out-of-Domain Interventions](https://arxiv.org/abs/2507.10809)
*Kazi Tasnim Zinat,Yun Zhou,Xiang Lyu,Yawei Wang,Zhicheng Liu,Panpan Xu*

Main category: cs.LG

TL;DR: 本文提出了一种新型因果框架，用于在时间序列中推断事件对之间的因果关系，尤其关注跨域干预的影响。


<details>
  <summary>Details</summary>
Motivation: 现有因果推断方法主要聚焦于域内事件类型，未充分考虑域外干预对因果动态的影响，从而在现实场景中造成限制。

Method: 提出了一种新的因果框架用于定义ATE，在经典Rubin因果框架基础上扩展表征时序过程事件之间受域外干预影响的因果关系转变；设计了无偏ATE估计器，并开发基于Transformer的神经网络模型，融合长时间依赖与局部模式，同时纳入域外干预信息。

Result: 实验证明，该方法在ATE估计和包含域外干预的信息点过程拟合效果上优于其他基线。

Conclusion: 本研究拓展了因果推断方法的适用范围，可有效应对域外干预场景下的时序因果关系挖掘。

Abstract: Inferring causal relationships between event pairs in a temporal sequence is
applicable in many domains such as healthcare, manufacturing, and
transportation. Most existing work on causal inference primarily focuses on
event types within the designated domain, without considering the impact of
exogenous out-of-domain interventions. In real-world settings, these
out-of-domain interventions can significantly alter causal dynamics. To address
this gap, we propose a new causal framework to define average treatment effect
(ATE), beyond independent and identically distributed (i.i.d.) data in classic
Rubin's causal framework, to capture the causal relation shift between events
of temporal process under out-of-domain intervention. We design an unbiased ATE
estimator, and devise a Transformer-based neural network model to handle both
long-range temporal dependencies and local patterns while integrating
out-of-domain intervention information into process modeling. Extensive
experiments on both simulated and real-world datasets demonstrate that our
method outperforms baselines in ATE estimation and goodness-of-fit under
out-of-domain-augmented point processes.

</details>


### [184] [Semantic Context for Tool Orchestration](https://arxiv.org/abs/2507.10820)
*Robert Müller*

Main category: cs.LG

TL;DR: 本文探讨了语义上下文（SC）在工具编排中的基础性作用，提出理论和实践的双重验证及其在大规模工具场景中的应用。


<details>
  <summary>Details</summary>
Motivation: 解决如何在大规模动态动作空间中实现高效且适应性的工具编排问题。

Method: 引入语义上下文（SC），通过SC-LinUCB算法在理论上证明其在上下文牟利问题中的高效性，同时提出FiReAct管道并进行实证验证。

Result: 通过理论和大模型实验验证了SC在静态和非静态情景中的关键作用，并展示了其在1万余工具上的有效编排能力。

Conclusion: SC能够显著提升工具编排的学习效率、适应性及可扩展性，为构建更先进的编排代理提供了指导。

Abstract: This paper demonstrates that Semantic Context (SC), leveraging descriptive
tool information, is a foundational component for robust tool orchestration.
Our contributions are threefold. First, we provide a theoretical foundation
using contextual bandits, introducing SC-LinUCB and proving it achieves lower
regret and adapts favourably in dynamic action spaces. Second, we provide
parallel empirical validation with Large Language Models, showing that SC is
critical for successful in-context learning in both static (efficient learning)
and non-stationary (robust adaptation) settings. Third, we propose the FiReAct
pipeline, and demonstrate on a benchmark with over 10,000 tools that SC-based
retrieval enables an LLM to effectively orchestrate over a large action space.
These findings provide a comprehensive guide to building more sample-efficient,
adaptive, and scalable orchestration agents.

</details>


### [185] [From Small to Large: A Graph Convolutional Network Approach for Solving Assortment Optimization Problems](https://arxiv.org/abs/2507.10834)
*Guokai Li,Pin Gao,Stefanus Jasin,Zizhuo Wang*

Main category: cs.LG

TL;DR: 本文提出了一种基于图卷积网络(GCN)的创新方法，用于解决混合多项逻辑选择模型下的受限商品选择优化问题，并在大规模产品情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 商品选择优化问题由于其组合性和非线性通常是NP难问题，需要高效的解决策略，同时很多实际问题在选择模型未知但有交易数据的情况下，也亟需新的解决方法。

Method: 作者将商品选择问题用图表示，并训练GCN学习最优选择的模式，然后基于GCN的输出提出了两种推断策略。此外，构建了一个无需已知选择模型的模型无关框架。

Result: 实验表明，对于小规模实例训练的GCN（如20件商品），在大规模案例（最高2000件商品）中能达到90%以上的最优化性能，并显著超过现有启发式策略。此外，框架在选择模型未知的情况下同样显示出高效性和卓越性能。

Conclusion: 基于GCN的策略能够有效解决大规模商品选择优化问题，不仅具备通用性和可扩展性，还能在模型无关场景下展现出色的表现，优于传统方法。

Abstract: Assortment optimization involves selecting a subset of substitutable products
(subject to certain constraints) to maximize the expected revenue. It is a
classic problem in revenue management and finds applications across various
industries. However, the problem is usually NP-hard due to its combinatorial
and non-linear nature. In this work, we explore how graph concolutional
networks (GCNs) can be leveraged to efficiently solve constrained assortment
optimization under the mixed multinomial logit choice model. We first develop a
graph representation of the assortment problem, then train a GCN to learn the
patterns of optimal assortments, and lastly propose two inference policies
based on the GCN's output. Due to the GCN's inherent ability to generalize
across inputs of varying sizes, we can use a GCN trained on small-scale
instances to facilitate large-scale instances. Extensive numerical experiments
demonstrate that given a GCN trained on small-scale instances (e.g., with 20
products), the proposed policies can achieve superior performance (90%+
optimality) on large-scale instances (with up to 2,000 products) within
seconds, which outperform existing heuristic policies in both performance and
efficiency. Furthermore, we extend our framework to a model-free setting where
the underlying choice model is unknown but transaction data is available. We
also conduct numerical experiments to demonstrate the effectiveness and
efficiency of our proposed policies in this setting.

</details>


### [186] [Offline Reinforcement Learning with Wasserstein Regularization via Optimal Transport Maps](https://arxiv.org/abs/2507.10843)
*Motoki Omura,Yusuke Mukuta,Kazuki Ota,Takayuki Osa,Tatsuya Harada*

Main category: cs.LG

TL;DR: 本文提出了一种利用Wasserstein距离进行正则化的新方法，用于解决离线强化学习中的分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中的分布偏移可能导致不可靠的超出分布的操作，需要提高模型在这些情况下的鲁棒性。

Method: 通过输入凸神经网络（ICNNs）建模最优传输映射，以无判别器的方式计算Wasserstein距离，取代传统的密度比率正则化方法。

Result: 在D4RL基准数据集上的结果表明，该方法性能与其它主流方法相当或更优。

Conclusion: 所提方法通过使用Wasserstein距离实现了离线强化学习中的稳定学习，避免了对抗训练，证明了其在分布偏移情况下的鲁棒性。

Abstract: Offline reinforcement learning (RL) aims to learn an optimal policy from a
static dataset, making it particularly valuable in scenarios where data
collection is costly, such as robotics. A major challenge in offline RL is
distributional shift, where the learned policy deviates from the dataset
distribution, potentially leading to unreliable out-of-distribution actions. To
mitigate this issue, regularization techniques have been employed. While many
existing methods utilize density ratio-based measures, such as the
$f$-divergence, for regularization, we propose an approach that utilizes the
Wasserstein distance, which is robust to out-of-distribution data and captures
the similarity between actions. Our method employs input-convex neural networks
(ICNNs) to model optimal transport maps, enabling the computation of the
Wasserstein distance in a discriminator-free manner, thereby avoiding
adversarial training and ensuring stable learning. Our approach demonstrates
comparable or superior performance to widely used existing methods on the D4RL
benchmark dataset. The code is available at
https://github.com/motokiomura/Q-DOT .

</details>


### [187] [Collaboration Promotes Group Resilience in Multi-Agent RL](https://arxiv.org/abs/2111.06614)
*Ilai Shraga,Guy Azran,Matthias Gerstgrasser,Ofir Abu,Jeffrey S. Rosenschein,Sarah Keren*

Main category: cs.LG

TL;DR: 本文提出了多智能体环境下的组恢复能力概念，并通过实验验证协作可以提升组恢复能力。


<details>
  <summary>Details</summary>
Motivation: 希望解决多智能体强化学习场景中对环境变化的适应性问题。

Method: 引入并形式化描述组恢复能力的概念，提出多种协作协议并评测其对组恢复能力的影响。

Result: 实验结果表明，相较于无协作的对照组，协作方式显著提升了组恢复能力。

Conclusion: 多智能体协作是提升组恢复能力的重要手段。

Abstract: To effectively operate in various dynamic scenarios, RL agents must be
resilient to unexpected changes in their environment. Previous work on this
form of resilience has focused on single-agent settings. In this work, we
introduce and formalize a multi-agent variant of resilience, which we term
group resilience. We further hypothesize that collaboration with other agents
is key to achieving group resilience; collaborating agents adapt better to
environmental perturbations in multi-agent reinforcement learning (MARL)
settings. We test our hypothesis empirically by evaluating different
collaboration protocols and examining their effect on group resilience. Our
experiments show that all the examined collaborative approaches achieve higher
group resilience than their non-collaborative counterparts.

</details>


### [188] [Visually grounded emotion regulation via diffusion models and user-driven reappraisal](https://arxiv.org/abs/2507.10861)
*Edoardo Pinzuti,Oliver Tüscher,André Ferreira Castro*

Main category: cs.LG

TL;DR: 认知重评是一种重要的情绪调节策略，但其传统方法对语言和高级认知功能依赖过多，限制了其在某些群体中的效果。本文提出结合生成式AI的视觉化认知重评新方法，通过智能生成视觉反馈来增强调节效果，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的认知重评方法过于依赖抽象的语言处理和高级认知功能，对创伤或抑郁患者的效果有限。因此需要一种更直观的方式来增强情绪调节能力。

Method: 本文提出使用大型文本到图像扩散模型，将用户的语言重评转化为支持性的视觉化输出，同时与原始刺激在结构上保持相似性，并通过实验验证其效果。

Result: 实验显示，使用AI生成图像反馈的认知重评显著降低了负面情绪。分析还发现，参与者认知重评与生成图像之间的情感一致性与减缓情绪负担相关。

Conclusion: 生成式视觉输入能够增强认知重评的效果，这表明生成式AI在情感计算和治疗技术等领域有巨大的潜力。

Abstract: Cognitive reappraisal is a key strategy in emotion regulation, involving
reinterpretation of emotionally charged stimuli to alter affective responses.
Despite its central role in clinical and cognitive science, real-world
reappraisal interventions remain cognitively demanding, abstract, and primarily
verbal. This reliance on higher-order cognitive and linguistic processes is
often impaired in individuals with trauma or depression, limiting the
effectiveness of standard approaches. Here, we propose a novel, visually based
augmentation of cognitive reappraisal by integrating large-scale text-to-image
diffusion models into the emotional regulation process. Specifically, we
introduce a system in which users reinterpret emotionally negative images via
spoken reappraisals, which are transformed into supportive, emotionally
congruent visualizations using stable diffusion models with a fine-tuned
IP-adapter. This generative transformation visually instantiates users'
reappraisals while maintaining structural similarity to the original stimuli,
externalizing and reinforcing regulatory intent. To test this approach, we
conducted a within-subject experiment (N = 20) using a modified cognitive
emotion regulation (CER) task. Participants reappraised or described aversive
images from the International Affective Picture System (IAPS), with or without
AI-generated visual feedback. Results show that AI-assisted reappraisal
significantly reduced negative affect compared to both non-AI and control
conditions. Further analyses reveal that sentiment alignment between
participant reappraisals and generated images correlates with affective relief,
suggesting that multimodal coherence enhances regulatory efficacy. These
findings demonstrate that generative visual input can support cogitive
reappraisal and open new directions at the intersection of generative AI,
affective computing, and therapeutic technology.

</details>


### [189] [GALDS: A Graph-Autoencoder-based Latent Dynamics Surrogate model to predict neurite material transport](https://arxiv.org/abs/2507.10871)
*Tsung Yeh Hsieh,Yongjie Jessica Zhang*

Main category: cs.LG

TL;DR: 提出了GALDS模型，利用图自动编码器和神经ODE优化神经网络中物质传输的仿真，精确度高，速度快。


<details>
  <summary>Details</summary>
Motivation: 由于传统方法在模拟复杂神经网络几何中物质传输的耗时性和高资源需求，亟需一种高效模拟方法。

Method: 设计基于图自动编码器的GALDS模型，将网络几何与动态特性编码为潜在空间表示，并通过神经ODE预测潜在空间内物质动态。

Result: 在8个未见几何和4个异常案例中表现优异，平均相对误差3%，最大误差<8%，速度提升10倍。

Conclusion: GALDS模型能以高精确度和效率完成神经网络中物质传输的模拟，为此领域的研究和应用提供了新工具。

Abstract: Neurons exhibit intricate geometries within their neurite networks, which
play a crucial role in processes such as signaling and nutrient transport.
Accurate simulation of material transport in the networks is essential for
understanding these biological phenomena but poses significant computational
challenges because of the complex tree-like structures involved. Traditional
approaches are time-intensive and resource-demanding, yet the inherent
properties of neuron trees, which consists primarily of pipes with steady-state
parabolic velocity profiles and bifurcations, provide opportunities for
computational optimization. To address these challenges, we propose a
Graph-Autoencoder-based Latent Dynamics Surrogate (GALDS) model, which is
specifically designed to streamline the simulation of material transport in
neural trees. GALDS employs a graph autoencoder to encode latent
representations of the network's geometry, velocity fields, and concentration
profiles. These latent space representations are then assembled into a global
graph, which is subsequently used to predict system dynamics in the latent
space via a trained graph latent space system dynamic model, inspired by the
Neural Ordinary Differential Equations (Neural ODEs) concept. The integration
of an autoencoder allows for the use of smaller graph neural network models
with reduced training data requirements. Furthermore, the Neural ODE component
effectively mitigates the issue of error accumulation commonly encountered in
recurrent neural networks. The effectiveness of the GALDS model is demonstrated
through results on eight unseen geometries and four abnormal transport
examples, where our approach achieves mean relative error of 3% with maximum
relative error <8% and demonstrates a 10-fold speed improvement compared to
previous surrogate model approaches.

</details>


### [190] [Domain-Adaptive Small Language Models for Structured Tax Code Prediction](https://arxiv.org/abs/2507.10880)
*Souvik Nath,Sumit Wadhwa,Luiz Perez*

Main category: cs.LG

TL;DR: 本研究提出了一种基于编码器-解码器架构的小型语言模型（SLM），用于加强商品和服务税码的预测，展现了在这一领域相较以往方法的优越性能。


<details>
  <summary>Details</summary>
Motivation: 多国企业每天处理数千笔交易，需要遵守不同司法管辖区的税收规定。为了避免税收罚款，准确确定税码至关重要。

Method: 提出了一种基于编码器-解码器架构的小型语言模型（SLM），结合顺序生成方法以捕捉税码的层级依赖性。

Result: 实验表明，SLM在结构性税码的顺序预测中优于平面分类器，以及单一编码器或解码器架构。

Conclusion: 该方法不仅在Harmonized System of Nomenclature (HSN)领域表现优异，还可以推广应用于其它政府规定的税商品代码领域。

Abstract: Every day, multinational firms process thousands of transactions, each of
which must adhere to tax regulations that vary by jurisdiction and are often
nuanced. The determination of product and service tax codes, such as HSN or SAC
is a major use case in Tax compliance. An accurate determination of such codes
is imperative to avoid any tax penalties. This paper proposes a domain-adaptive
small language model (SLM) with an encoder-decoder architecture for the
enhanced prediction of product and service tax codes. In this approach, we
address the problem of predicting hierarchical tax code sequences using
unstructured product and services data. We employ an SLM based upon
encoder-decoder architecture as this enables sequential generation of tax codes
to capture the hierarchical dependencies present within the tax codes. Our
experiments demonstrate that encoder-decoder SLMs can be successfully applied
to the sequential prediction of structured tax codes, a domain that remains
comparatively unexplored in current NLP research. In this paper, we demonstrate
the superior performance of the domain-adaptive encoder-decoder SLMs over flat
classifiers when applied to the Harmonized System of Nomenclature (HSN), and
achieve superior results compared to decoder-only and encoder-only
architectures for structured sequence generation tasks. This approach can also
be scaled to other government-mandated tax commodity codes, such as United
Nations Standard Products and Services Codes (UNSPSC), or Brazil's Nomenclatura
Comum do Mercosul (NCM).

</details>


### [191] [Learning from Imperfect Data: Robust Inference of Dynamic Systems using Simulation-based Generative Model](https://arxiv.org/abs/2507.10884)
*Hyunwoo Cho,Hyeontae Jo,Hyung Ju Hwang*

Main category: cs.LG

TL;DR: 提出了一种名为SiGMoID的生成模型方法，用于解决非线性动态系统的推断难题，特别是在数据不完美的情况下。


<details>
  <summary>Details</summary>
Motivation: 解决在数据噪声大、稀疏或部分可观测情况下的非线性动态系统建模与推断挑战。

Method: 整合了物理驱动神经网络（含超网络）构建ODE求解器与Wasserstein生成对抗网络捕获数据分布以估算ODE参数的方法。

Result: SiGMoID能够量化数据噪声、估算系统参数并推断未观测的系统部分，其有效性通过实验案例得到了验证，展示了其广泛适用性。

Conclusion: SiGMoID在科学研究与工程系统中有广泛应用价值，能够解析动态系统的全部动态。

Abstract: System inference for nonlinear dynamic models, represented by ordinary
differential equations (ODEs), remains a significant challenge in many fields,
particularly when the data are noisy, sparse, or partially observable. In this
paper, we propose a Simulation-based Generative Model for Imperfect Data
(SiGMoID) that enables precise and robust inference for dynamic systems. The
proposed approach integrates two key methods: (1) physics-informed neural
networks with hyper-networks that constructs an ODE solver, and (2) Wasserstein
generative adversarial networks that estimates ODE parameters by effectively
capturing noisy data distributions. We demonstrate that SiGMoID quantifies data
noise, estimates system parameters, and infers unobserved system components.
Its effectiveness is validated validated through realistic experimental
examples, showcasing its broad applicability in various domains, from
scientific research to engineered systems, and enabling the discovery of full
system dynamics.

</details>


### [192] [How to Protect Models against Adversarial Unlearning?](https://arxiv.org/abs/2507.10886)
*Patryk Jasiorski,Marek Klonowski,Michał Woźniak*

Main category: cs.LG

TL;DR: 本文探讨了对AI模型的对抗性遗忘问题，并提出了一种新方法来保护模型性能免受不良影响。


<details>
  <summary>Details</summary>
Motivation: 需要满足法律法规如AI法案、GDPR，以及去除有毒内容、消除偏见、应对恶意实例和数据分布变化等需求，使得AI模型必须具备遗忘能力。

Method: 研究对抗性遗忘问题，分析恶意方通过遗忘请求最大化降低模型性能的行为，并提出新方法保护模型性能，防止自发或恶意的负面影响。

Result: 证明了对抗性遗忘现象和对手能力依赖于骨干模型及数据选择策略，形成了一种保护模型性能的新方法。

Conclusion: 新方法有效防止AI模型在遗忘行为中性能恶化，无论是自发问题还是来自对手的攻击行为。

Abstract: AI models need to be unlearned to fulfill the requirements of legal acts such
as the AI Act or GDPR, and also because of the need to remove toxic content,
debiasing, the impact of malicious instances, or changes in the data
distribution structure in which a model works. Unfortunately, removing
knowledge may cause undesirable side effects, such as a deterioration in model
performance. In this paper, we investigate the problem of adversarial
unlearning, where a malicious party intentionally sends unlearn requests to
deteriorate the model's performance maximally. We show that this phenomenon and
the adversary's capabilities depend on many factors, primarily on the backbone
model itself and strategy/limitations in selecting data to be unlearned. The
main result of this work is a new method of protecting model performance from
these side effects, both in the case of unlearned behavior resulting from
spontaneous processes and adversary actions.

</details>


### [193] [Outbound Modeling for Inventory Management](https://arxiv.org/abs/2507.10890)
*Riccardo Savorgnan,Udaya Ghai,Carson Eisenach,Dean Foster*

Main category: cs.LG

TL;DR: 本文研究了预测库存发货量和相关运输成本的问题，并将其模型应用于强化学习领域，以进行更高效的仿真和控制政策开发。


<details>
  <summary>Details</summary>
Motivation: 准确模拟库存发货和运输成本的运作过程，对于基于强化学习的区域库存管理至关重要。

Method: 将问题建模为概率预测问题，建模所有仓库的发货量和运输成本的联合分布，并提出了一种使用生产系统验证模型在RL环境中的稳健性的方法。

Result: 初步结果表明，模型在分布内场景中的预测精确度较高。

Conclusion: 该研究提出的建模方法能够有效支持强化学习中的库存控制策略开发，在初步实验中表现出较好的潜力。

Abstract: We study the problem of forecasting the number of units fulfilled (or
``drained'') from each inventory warehouse to meet customer demand, along with
the associated outbound shipping costs. The actual drain and shipping costs are
determined by complex production systems that manage the planning and execution
of customers' orders fulfillment, i.e. from where and how to ship a unit to be
delivered to a customer. Accurately modeling these processes is critical for
regional inventory planning, especially when using Reinforcement Learning (RL)
to develop control policies. For the RL usecase, a drain model is incorporated
into a simulator to produce long rollouts, which we desire to be
differentiable. While simulating the calls to the internal software systems can
be used to recover this transition, they are non-differentiable and too slow
and costly to run within an RL training environment. Accordingly, we frame this
as a probabilistic forecasting problem, modeling the joint distribution of
outbound drain and shipping costs across all warehouses at each time period,
conditioned on inventory positions and exogenous customer demand. To ensure
robustness in an RL environment, the model must handle out-of-distribution
scenarios that arise from off-policy trajectories. We propose a validation
scheme that leverages production systems to evaluate the drain model on
counterfactual inventory states induced by RL policies. Preliminary results
demonstrate the model's accuracy within the in-distribution setting.

</details>


### [194] [Class-Proportional Coreset Selection for Difficulty-Separable Data](https://arxiv.org/abs/2507.10904)
*Elisa Tsai,Haizhong Zheng,Atul Prakash*

Main category: cs.LG

TL;DR: 提出了用Class Difficulty Separability Coefficient (CDSC)量化类别难度分离性，改进传统数据稀疏方法的效率。


<details>
  <summary>Details</summary>
Motivation: 现有一轮核心选择法忽视了不同类别数据难度的差异性，容易遗漏重要的难类数据，从而导致性能下降。

Method: 通过引入类难度分离性系数(CDSC)，设计类比例数据采样策略，改进现有方法如Coverage-centric Coreset Selection (CCS)并提出CCS-CP。

Result: 在包括网络入侵检测和医疗影像在内的五个不同数据集上测试，CCS-CP在极端稀疏情况下性能稳定，显著优于类无关核心选择方法。

Conclusion: 考虑类别难度差异性能显著提高稀疏效率及泛化能力，特别适用于高风险应用场景。

Abstract: High-quality training data is essential for building reliable and efficient
machine learning systems. One-shot coreset selection addresses this by pruning
the dataset while maintaining or even improving model performance, often
relying on training-dynamics-based data difficulty scores. However, most
existing methods implicitly assume class-wise homogeneity in data difficulty,
overlooking variation in data difficulty across different classes.
  In this work, we challenge this assumption by showing that, in domains such
as network intrusion detection and medical imaging, data difficulty often
clusters by class. We formalize this as class-difficulty separability and
introduce the Class Difficulty Separability Coefficient (CDSC) as a
quantitative measure. We demonstrate that high CDSC values correlate with
performance degradation in class-agnostic coreset methods, which tend to
overrepresent easy majority classes while neglecting rare but informative ones.
  To address this, we introduce class-proportional variants of multiple
sampling strategies. Evaluated on five diverse datasets spanning security and
medical domains, our methods consistently achieve state-of-the-art data
efficiency. For instance, on CTU-13, at an extreme 99% pruning rate, a
class-proportional variant of Coverage-centric Coreset Selection (CCS-CP) shows
remarkable stability, with accuracy dropping only 2.58%, precision 0.49%, and
recall 0.19%. In contrast, the class-agnostic CCS baseline, the next best
method, suffers sharper declines of 7.59% in accuracy, 4.57% in precision, and
4.11% in recall.
  We further show that aggressive pruning enhances generalization in noisy,
imbalanced, and large-scale datasets. Our results underscore that explicitly
modeling class-difficulty separability leads to more effective, robust, and
generalizable data pruning, particularly in high-stakes scenarios.

</details>


### [195] [Diffusion Decoding for Peptide De Novo Sequencing](https://arxiv.org/abs/2507.10955)
*Chi-en Amy Tai,Alexander Wong*

Main category: cs.LG

TL;DR: 本文探索了通过扩散解码器改进肽谱新测序方法，显著提高氨基酸回召能力。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在肽谱新测序中存在误差级联和无法有效利用高置信度区域的问题。

Method: 引入适用于离散数据领域的扩散解码器，并尝试多种解码器设计、广义背包搜索和损失函数，以提高预测性能。

Result: 即使肽精度和回召仍为零，但最佳扩散解码器设计结合DINOISER损失函数使氨基酸回召相对基线模型提升了0.373。

Conclusion: 扩散解码器不仅可以提高模型的敏感性，还可推动肽谱新测序的显著进步。

Abstract: Peptide de novo sequencing is a method used to reconstruct amino acid
sequences from tandem mass spectrometry data without relying on existing
protein sequence databases. Traditional deep learning approaches, such as
Casanovo, mainly utilize autoregressive decoders and predict amino acids
sequentially. Subsequently, they encounter cascading errors and fail to
leverage high-confidence regions effectively. To address these issues, this
paper investigates using diffusion decoders adapted for the discrete data
domain. These decoders provide a different approach, allowing sequence
generation to start from any peptide segment, thereby enhancing prediction
accuracy. We experiment with three different diffusion decoder designs,
knapsack beam search, and various loss functions. We find knapsack beam search
did not improve performance metrics and simply replacing the transformer
decoder with a diffusion decoder lowered performance. Although peptide
precision and recall were still 0, the best diffusion decoder design with the
DINOISER loss function obtained a statistically significant improvement in
amino acid recall by 0.373 compared to the baseline autoregressive
decoder-based Casanovo model. These findings highlight the potential of
diffusion decoders to not only enhance model sensitivity but also drive
significant advancements in peptide de novo sequencing.

</details>


### [196] [Physics-Informed Neural Networks For Semiconductor Film Deposition: A Review](https://arxiv.org/abs/2507.10983)
*Tao Han,Zahra Taheri,Hyunwoong Ko*

Main category: cs.LG

TL;DR: 本文综述了半导体薄膜沉积过程中机器学习（ML）应用的现状，重点关注物理信息神经网络（PINNs）在提高过程控制和预测建模方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 半导体制造中薄膜沉积过程需要精确控制才能满足均匀性、粘附性和功能性的要求，目前的挑战亟需创新型机器学习方法解决。

Method: 通过综述方法，本文分析了现有ML方法的优点和限制，特别是探讨了如何将物理知识融入到PINNs框架中。

Result: 研究总结了当前PINNs及其他ML方法的能力及局限性，并提出了将其应用于提高薄膜沉积过程的可解释性、精度和鲁棒性的可能性路径。

Conclusion: 研究明确了未来在集成物理信息ML框架方面的研究方向，期待能显著提升半导体制造中的精度、可扩展性与运行效率。

Abstract: Semiconductor manufacturing relies heavily on film deposition processes, such
as Chemical Vapor Deposition and Physical Vapor Deposition. These complex
processes require precise control to achieve film uniformity, proper adhesion,
and desired functionality. Recent advancements in Physics-Informed Neural
Networks (PINNs), an innovative machine learning (ML) approach, have shown
significant promise in addressing challenges related to process control,
quality assurance, and predictive modeling within semiconductor film deposition
and other manufacturing domains. This paper provides a comprehensive review of
ML applications targeted at semiconductor film deposition processes. Through a
thematic analysis, we identify key trends, existing limitations, and research
gaps, offering insights into both the advantages and constraints of current
methodologies. Our structured analysis aims to highlight the potential
integration of these ML techniques to enhance interpretability, accuracy, and
robustness in film deposition processes. Additionally, we examine
state-of-the-art PINN methods, discussing strategies for embedding physical
knowledge, governing laws, and partial differential equations into advanced
neural network architectures tailored for semiconductor manufacturing. Based on
this detailed review, we propose novel research directions that integrate the
strengths of PINNs to significantly advance film deposition processes. The
contributions of this study include establishing a clear pathway for future
research in integrating physics-informed ML frameworks, addressing existing
methodological gaps, and ultimately improving precision, scalability, and
operational efficiency within semiconductor manufacturing.

</details>


### [197] [StellarF: A Lora-Adapter Integrated Large Model Framework for Stellar Flare Forecasting with Historical & Statistical Data](https://arxiv.org/abs/2507.10986)
*Tianyu Su,Zhiqiang Zou,Ali Luo,Xiao Kong,Qingyu Lu,Min Li*

Main category: cs.LG

TL;DR: 提出了一种名为StellarF的模型，用于高效学习恒星耀斑的预测，达到了先进的预测性能。


<details>
  <summary>Details</summary>
Motivation: 由于恒星耀斑事件记录稀少且缺乏领域特定的大规模预测模型，研究者希望创建一个能够高效用于恒星耀斑预测的工具。

Method: 通过使用低秩(Low-Rank)和适配器技术(Adapters)实现参数高效学习，并结合耀斑统计信息模块和历史记录模块，进行多尺度模式识别。

Result: 在自建数据集（基于Kepler和TESS光曲线）上的实验表明，StellarF性能优于现有方法，达到了最佳水平。

Conclusion: 所提出的方法不仅为恒星物理研究提供了新的框架，还为跨领域应用开辟了新方向。

Abstract: Stellar flare forecasting, a critical research frontier in astronomy, offers
profound insights into stellar activity. However, the field is constrained by
both the sparsity of recorded flare events and the absence of domain-specific
large-scale predictive models. To address these challenges, this study
introduces StellarF (Stellar Flare Forecasting), a novel large model that
leverages Low-Rank (LoRA) and Adapter techniques to parameter-efficient
learning for stellar flare forecasting. At its core, StellarF integrates an
flare statistical information module with a historical flare record module,
enabling multi-scale pattern recognition from observational data. Extensive
experiments on our self-constructed datasets (derived from Kepler and TESS
light curves) demonstrate that StellarF achieves state-of-the-art performance
compared to existing methods. The proposed prediction paradigm establishes a
novel methodological framework for advancing astrophysical research and
cross-disciplinary applications.

</details>


### [198] [High-Throughput Distributed Reinforcement Learning via Adaptive Policy Synchronization](https://arxiv.org/abs/2507.10990)
*Rodney Lafuente-Mercado*

Main category: cs.LG

TL;DR: ClusterEnv是一种面向分布式环境执行的接口，通过引入DETACH模式和AAPS机制优化了模拟和同步效率。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习框架在分布式环境仿真中缺乏模块化和可复用性，同时受限于单体系统的设计架构。

Method: 提出ClusterEnv框架，引入DETACH模式将模拟与训练分离；通过AAPS机制在策略分歧触发的情况下进行演员策略同步以减少开销。

Result: 在离散控制任务中，AAPS实现了高采样效率且显著减少了模型权重更新频率。

Conclusion: ClusterEnv通过轻量化设计和广泛兼容性改善了分布式强化学习工作流，可在现有RL管道中方便集成。

Abstract: Scaling reinforcement learning (RL) workloads often requires distributing
environment simulation across compute clusters. Existing frameworks entangle
simulation, learning logic, and orchestration into monolithic systems, limiting
modularity and reusability. We present ClusterEnv, a lightweight,
learner-agnostic interface for distributed environment execution that mirrors
the Gymnasium API. ClusterEnv introduces the DETACH pattern, which decouples
simulation from training by offloading reset() and step() operations to remote
workers while keeping learning centralized. To address policy staleness in
distributed execution, we propose Adaptive Actor Policy Synchronization (AAPS),
a divergence-triggered update mechanism that reduces synchronization overhead
without sacrificing performance. ClusterEnv integrates cleanly into existing RL
pipelines, supports both on-policy and off-policy methods, and requires minimal
code changes. Experiments on discrete control tasks demonstrate that AAPS
achieves high sample efficiency with significantly fewer weight updates. Source
code is available at https://github.com/rodlaf/ClusterEnv.

</details>


### [199] [Misalignment from Treating Means as Ends](https://arxiv.org/abs/2507.10995)
*Henrik Marklund,Alex Infanger,Benjamin Van Roy*

Main category: cs.LG

TL;DR: 研究发现奖励函数容易受到人类终极目标与工具性目标混淆的影响，导致偏差。


<details>
  <summary>Details</summary>
Motivation: 探讨奖励函数中终极目标和工具性目标的混淆对强化学习表现的影响。

Method: 通过一个简单的例子，展示当混淆发生时，优化错配的奖励函数如何导致实际目标的表现下降。

Result: 结果表明，即使是轻微的目标混淆，也会使强化学习对奖励函数的敏感性显著提高，表现出较差的真实目标性能。

Conclusion: 需要注意在构建奖励函数时避免混淆终极目标与工具性目标，以提升强化学习的对齐程度。

Abstract: Reward functions, learned or manually specified, are rarely perfect. Instead
of accurately expressing human goals, these reward functions are often
distorted by human beliefs about how best to achieve those goals. Specifically,
these reward functions often express a combination of the human's terminal
goals -- those which are ends in themselves -- and the human's instrumental
goals -- those which are means to an end. We formulate a simple example in
which even slight conflation of instrumental and terminal goals results in
severe misalignment: optimizing the misspecified reward function results in
poor performance when measured by the true reward function. This example
distills the essential properties of environments that make reinforcement
learning highly sensitive to conflation of instrumental and terminal goals. We
discuss how this issue can arise with a common approach to reward learning and
how it can manifest in real environments.

</details>


### [200] [Crafting Imperceptible On-Manifold Adversarial Attacks for Tabular Data](https://arxiv.org/abs/2507.10998)
*Zhipeng He,Alexander Stevens,Chun Ouyang,Johannes De Smedt,Alistair Barros,Catarina Moreira*

Main category: cs.LG

TL;DR: 研究提出了一种利用混合输入变分自编码器（VAE）的对抗攻击框架，用于生成不可察觉的对抗样本，并在统计上保持输入分布的一致性。


<details>
  <summary>Details</summary>
Motivation: 在表格数据领域，对抗攻击缺乏像图像或文本数据那样直观的相似性度量标准，而且传统的梯度方法通常会产生偏离数据分布的对抗样本，容易被检测到。

Method: 提出了一种基于混合输入变分自编码器（VAE）的框架，将分类和数值特征集成到统一的潜在空间中，通过潜在空间的扰动生成不可察觉的对抗样本，并提出衡量对抗样本是否保持在分布内的新指标IDSR。

Result: 在6个公开数据集和3种模型上实验表明，所提出的方法相比传统的输入空间攻击和图像领域适配的VAE方法，产生更少的异常值并展现更加一致的性能。

Conclusion: 研究证明潜在空间内的扰动对现实世界的表格数据对抗攻击至关重要，并提供了一个实用性强的稳健方法，具备较高的实践部署价值。

Abstract: Adversarial attacks on tabular data present fundamental challenges distinct
from image or text domains due to the heterogeneous nature of mixed categorical
and numerical features. Unlike images where pixel perturbations maintain visual
similarity, tabular data lacks intuitive similarity metrics, making it
difficult to define imperceptible modifications. Additionally, traditional
gradient-based methods prioritise $\ell_p$-norm constraints, often producing
adversarial examples that deviate from the original data distributions, making
them detectable. We propose a latent space perturbation framework using a
mixed-input Variational Autoencoder (VAE) to generate imperceptible adversarial
examples. The proposed VAE integrates categorical embeddings and numerical
features into a unified latent manifold, enabling perturbations that preserve
statistical consistency. We specify In-Distribution Success Rate (IDSR) to
measure the proportion of adversarial examples that remain statistically
indistinguishable from the input distribution. Evaluation across six publicly
available datasets and three model architectures demonstrates that our method
achieves substantially lower outlier rates and more consistent performance
compared to traditional input-space attacks and other VAE-based methods adapted
from image domain approaches. Our comprehensive analysis includes
hyperparameter sensitivity, sparsity control mechanisms, and generative
architectural comparisons, revealing that VAE-based attacks depend critically
on reconstruction quality but offer superior practical utility when sufficient
training data is available. This work highlights the importance of on-manifold
perturbations for realistic adversarial attacks on tabular data, offering a
robust approach for practical deployment. The source code can be accessed
through https://github.com/ZhipengHe/VAE-TabAttack.

</details>


### [201] [AdaMuon: Adaptive Muon Optimizer](https://arxiv.org/abs/2507.11005)
*Chongjie Si,Debing Zhang,Wei Shen*

Main category: cs.LG

TL;DR: 提出了AdaMuon优化器，支持自适应学习率，相较于Muon优化器，AdaMuon在收敛速度和训练稳定性上表现更佳。


<details>
  <summary>Details</summary>
Motivation: 改进现有的Muon优化器以实现更高效的学习率和优化性能。

Method: 引入两个核心模块：基于参数的第二矩调整模块和 RMS 对齐重缩放模块，以提高优化器的收敛效率和稳定性。

Result: 在多模型规模和学习率场景下，AdaMuon优于原始的Muon优化器，表现出更快的收敛速度和更高的训练稳定性。

Conclusion: AdaMuon优化器无需额外调试，能够无缝地集成到现有的Muon训练流程中，是对原有算法的显著提升。

Abstract: We propose AdaMuon, an adaptive learning-rate framework built upon the
recently validated Muon optimizer, which has demonstrated substantial
efficiency gains over AdamW in large-scale model training. AdaMuon augments
Muon with two mutually dependent modules: (1) a per-parameter second-moment
modulation that captures orthogonal gradient updates to ensure update-level
adaptivity, and (2) a RMS-aligned rescaling that regulates the overall update
magnitude by aligning it with the intrinsic structure of the parameter space.
Empirical results on multiple model scales and learning-rate regimes confirm
that AdaMuon consistently outperforms the original Muon, delivering higher
acceleration in convergence while maintaining training stability. Our method
introduces no additional tuning burden and can be seamlessly integrated into
existing Muon training pipelines.

</details>


### [202] [Leveraging Advanced Machine Learning to Predict Turbulence Dynamics from Temperature Observations at an Experimental Prescribed Fire](https://arxiv.org/abs/2507.11012)
*Dipak Dulal,Joseph J. Charney,Michael R. Gallagher,Pitambar Acharya,Carmeliza Navasca,Nicholas S. Skowronski*

Main category: cs.LG

TL;DR: 研究探讨利用温度数据预测湍流动能（TKE）的可能性，采用多种机器学习模型，揭示温度与TKE的关系并精确预测。


<details>
  <summary>Details</summary>
Motivation: 希望通过温度数据预测TKE，从而更好地理解火环境的复杂动力学过程，为火灾管理策略和模型改进提供依据。

Method: 使用多种机器学习模型（深度神经网络、随机森林、梯度提升、Gaussian过程回归），结合温度和湍流数据进行相关性分析和TKE预测。

Result: 尽管预测因子与目标变量（TKE）的相关性较低，但多个回归模型成功实现了对TKE的较高精度预测。

Conclusion: 该研究展示了利用机器学习揭示温度与气流过程关系的新方法，有助于改进火灾环境理解、火灾操作策略以及烟雾预测模型，并表明机器学习在分析复杂火灾环境数据中的潜力。

Abstract: This study explores the potential for predicting turbulent kinetic energy
(TKE) from more readily acquired temperature data using temperature profiles
and turbulence data collected concurrently at 10 Hz during a small experimental
prescribed burn in the New Jersey Pine Barrens. Machine learning models,
including Deep Neural Networks, Random Forest Regressor, Gradient Boosting, and
Gaussian Process Regressor, were employed to assess the potential to predict
TKE from temperature perturbations and explore temporal and spatial dynamics of
correlations. Data visualization and correlation analyses revealed patterns and
relationships between thermocouple temperatures and TKE, providing insight into
the underlying dynamics. More accurate predictions of TKE were achieved by
employing various machine learning models despite a weak correlation between
the predictors and the target variable. The results demonstrate significant
success, particularly from regression models, in accurately predicting the TKE.
The findings of this study demonstrate a novel numerical approach to
identifying new relationships between temperature and airflow processes in and
around the fire environment. These relationships can help refine our
understanding of combustion environment processes and the coupling and
decoupling of fire environment processes necessary for improving fire
operations strategy and fire and smoke model predictions. The findings of this
study additionally highlight the valuable role of machine learning techniques
in analyzing the complex large datasets of the fire environments, showcasing
their potential to advance fire research and management practices.

</details>


### [203] [First-Order Error Matters: Accurate Compensation for Quantized Large Language Models](https://arxiv.org/abs/2507.11017)
*Xingyu Zheng,Haotong Qin,Yuye Li,Jiakai Wang,Jinyang Guo,Michele Magno,Xianglong Liu*

Main category: cs.LG

TL;DR: 提出了FOEM，一种新颖的后量化方法，显著改进了量化误差的补偿效果，在多种模型和基准测试中效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有补偿量化误差的方法中假设一阶项可被忽略，但发现这一假设因累积误差而不成立，进而提出适应性更强的新方法。

Method: 提出FOEM方法，通过直接计算潜在权重与全精度权重的差异来近似梯度，并结合Cholesky因子以低开销实现高效补偿。

Result: FOEM显著超过GPTQ，尤其在Llama模型的3-bit量化和MMLU测试中，接近全精度表现；此外与其他前沿技术结合后还能进一步提升。

Conclusion: FOEM方法在多种量化场景中表现优越，能在低计算开销的同时有效缩小与全精度模型的性能差距。代码已开源。

Abstract: Post-training quantization (PTQ) offers an efficient approach to compressing
large language models (LLMs), significantly reducing memory access and
computational costs. Existing compensation-based weight calibration methods
often rely on a second-order Taylor expansion to model quantization error,
under the assumption that the first-order term is negligible in well-trained
full-precision models. However, we reveal that the progressive compensation
process introduces accumulated first-order deviations between latent weights
and their full-precision counterparts, making this assumption fundamentally
flawed. To address this, we propose FOEM, a novel PTQ method that explicitly
incorporates first-order gradient terms to improve quantization error
compensation. FOEM approximates gradients by directly computing the difference
between latent and full-precision weights, avoiding the high cost and limited
generalization of backpropagation-based gradient computation. This approach
introduces minimal additional computational overhead. Moreover, FOEM leverages
precomputed Cholesky factors to efficiently recover the inverse of Hessian
submatrices in real time. Extensive experiments across a wide range of models
and benchmarks demonstrate that FOEM consistently outperforms the classical
GPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of
Llama3-8B by 89.6%, and improves the 5-shot MMLU accuracy of Llama3-70B from
51.7% to 74.9%, approaching the full-precision performance of 78.6%.
Furthermore, FOEM can be seamlessly integrated with advanced techniques such as
GPTAQ and SpinQuant, yielding additional improvements under the challenging
W4A4KV4 setting, and further narrowing the accuracy gap with full-precision
baselines beyond what current state-of-the-art methods achieve. The code is
available at https://github.com/Xingyu-Zheng/FOEM.

</details>


### [204] [Relative Entropy Pathwise Policy Optimization](https://arxiv.org/abs/2507.11019)
*Claas Voelcker,Axel Brunnbauer,Marcel Hussing,Michal Nauman,Pieter Abbeel,Eric Eaton,Radu Grosu,Amir-massoud Farahmand,Igor Gilitschenski*

Main category: cs.LG

TL;DR: 提出了一种名为REPPO的高效在线算法，用于平衡探索和稳定训练，同时利用路径梯度提升样本效率，表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决得分函数策略梯度高方差以及路径梯度更新需要精确Q值模型的问题，旨在改进训练的稳定性和效率。

Method: 通过建立基于路径梯度的在线算法，结合相对熵策略约束，实现从在线数据训练Q值模型，并优化架构以提升价值函数的学习准确性。

Result: REPPO在GPU并行化基准上的实验表现出较高效率，样本要求和内存占用减少，同时显示了较强的超参数鲁棒性。

Conclusion: REPPO成功结合了路径梯度的样本效率与在线学习的简单性，为提高深度强化学习的效率与可靠性提供了一个有效解决方案。

Abstract: Score-function policy gradients have delivered strong results in
game-playing, robotics and language-model fine-tuning. Yet its high-variance
often undermines training stability. On the other hand, pathwise policy
gradients alleviate the training variance, but are reliable only when driven by
an accurate action-conditioned value function which is notoriously hard to
train without relying on past off-policy data. In this paper, we discuss how to
construct a value-gradient driven, on-policy algorithm that allow training
Q-value models purely from on-policy data, unlocking the possibility of using
pathwise policy updates in the context of on-policy learning. We show how to
balance stochastic policies for exploration with constrained policy updates for
stable training, and evaluate important architectural components that
facilitate accurate value function learning. Building on these insights, we
propose Relative Entropy Pathwise Policy Optimization (REPPO), an efficient
on-policy algorithm that combines the sample-efficiency of pathwise policy
gradients with the simplicity and minimal memory footprint of standard
on-policy learning. We demonstrate that REPPO provides strong empirical
performance at decreased sample requirements, wall-clock time, memory footprint
as well as high hyperparameter robustness in a set of experiments on two
standard GPU-parallelized benchmarks.

</details>


### [205] [AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air](https://arxiv.org/abs/2507.11515)
*Shiyi Yang,Xiaoxue Yu,Rongpeng Li,Jianhang Zhu,Zhifeng Zhao,Honggang Zhang*

Main category: cs.LG

TL;DR: AirLLM是一种新方法，通过强化学习和扩散模型优化LoRA的秩配置，实现高效的边缘设备上大语言模型远程微调。


<details>
  <summary>Details</summary>
Motivation: 由于边缘设备上的通信和硬件资源限制，通过云端远程优化大语言模型显得必不可少。而现有方法在秩配置上缺乏灵活性，且参数传输效率较低。

Method: 提出了AirLLM框架，利用分层扩散策略和强化学习模型（PPO）进行秩配置，结合Denoising Diffusion Implicit Models（DDIM）优化高分辨率的秩向量。采用Classifier-Free Guidance（CFG）训练保持奖励对齐。

Result: 实验表明，AirLLM在不同信噪比条件下能显著提高微调性能，同时大幅减少传输成本。

Conclusion: AirLLM通过强化学习和扩散优化策略，在任务适应性和通信效率上均表现优异，为远程优化大语言模型提供了可扩展的解决方案。

Abstract: Operating Large Language Models (LLMs) on edge devices is increasingly
challenged by limited communication bandwidth and strained computational and
memory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable.
Nevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ
fixed or heuristic rank configurations, and the subsequent over-the-air
transmission of all LoRA parameters could be rather inefficient. To address
this limitation, we develop AirLLM, a hierarchical diffusion policy framework
for communication-aware LoRA adaptation. Specifically, AirLLM models the rank
configuration as a structured action vector that spans all LoRA-inserted
projections. To solve the underlying high-dimensional sequential
decision-making problem, a Proximal Policy Optimization (PPO) agent generates
coarse-grained decisions by jointly observing wireless states and linguistic
complexity, which are then refined via Denoising Diffusion Implicit Models
(DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The
two modules are optimized alternatively, with the DDIM trained under the
Classifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards.
Experiments under varying signal-to-noise ratios demonstrate that AirLLM
consistently enhances fine-tuning performance while significantly reducing
transmission costs, highlighting the effectiveness of reinforcement-driven,
diffusion-refined rank adaptation for scalable and efficient remote fine-tuning
over the air.

</details>


### [206] [GATE: Graph Attention Neural Networks with Real-Time Edge Construction for Robust Indoor Localization using Mobile Embedded Devices](https://arxiv.org/abs/2507.11053)
*Danish Gufran,Sudeep Pasricha*

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为GATE（Graph Attention-based Topology Exploration）的室内定位框架，用于解决Wi-Fi RSS指纹本地化中设备异构性和环境噪声所带来的挑战，同时改进传统深度学习和图神经网络方法的缺陷。


<details>
  <summary>Details</summary>
Motivation: 室内定位需求被大量智能环境和导航系统所驱动。传统Wi-Fi RSS指纹方法存在设备异构性和空间关系建模不足的问题，导致定位性能不够理想。

Method: 提出GATE框架，重点研究并设计了三种创新：1）Attention Hyperspace Vector（AHV），用于增强消息传递；2）Multi-Dimensional Hyperspace Vector（MDHV），减轻图神经网络的盲点问题；3）Real-Time Edge Construction（RTEC），实现动态图的自适应。

Result: 通过广泛的真实环境测试，对比多种路径长度、接入点密度和异构设备，GATE的平均定位误差比现有最佳方法低1.6倍至4.72倍，最差误差低1.85倍至4.57倍。

Conclusion: GATE框架显著优化了室内定位的精度和鲁棒性，为解决设备异构性和环境噪声带来的挑战提供了有效的解决方案，可推广至更多室内定位场景。

Abstract: Accurate indoor localization is crucial for enabling spatial context in smart
environments and navigation systems. Wi-Fi Received Signal Strength (RSS)
fingerprinting is a widely used indoor localization approach due to its
compatibility with mobile embedded devices. Deep Learning (DL) models improve
accuracy in localization tasks by learning RSS variations across locations, but
they assume fingerprint vectors exist in a Euclidean space, failing to
incorporate spatial relationships and the non-uniform distribution of
real-world RSS noise. This results in poor generalization across heterogeneous
mobile devices, where variations in hardware and signal processing distort RSS
readings. Graph Neural Networks (GNNs) can improve upon conventional DL models
by encoding indoor locations as nodes and modeling their spatial and signal
relationships as edges. However, GNNs struggle with non-Euclidean noise
distributions and suffer from the GNN blind spot problem, leading to degraded
accuracy in environments with dense access points (APs). To address these
challenges, we propose GATE, a novel framework that constructs an adaptive
graph representation of fingerprint vectors while preserving an indoor
state-space topology, modeling the non-Euclidean structure of RSS noise to
mitigate environmental noise and address device heterogeneity. GATE introduces
1) a novel Attention Hyperspace Vector (AHV) for enhanced message passing, 2) a
novel Multi-Dimensional Hyperspace Vector (MDHV) to mitigate the GNN blind
spot, and 3) an new Real-Time Edge Construction (RTEC) approach for dynamic
graph adaptation. Extensive real-world evaluations across multiple indoor
spaces with varying path lengths, AP densities, and heterogeneous devices
demonstrate that GATE achieves 1.6x to 4.72x lower mean localization errors and
1.85x to 4.57x lower worst-case errors compared to state-of-the-art indoor
localization frameworks.

</details>


### [207] [A Distance Metric for Mixed Integer Programming Instances](https://arxiv.org/abs/2507.11063)
*Gwen Maudet,Grégoire Danoy*

Main category: cs.LG

TL;DR: 本文提出了一种用于混合整数线性规划实例的新数学距离度量，该度量通过约束比较中的权重-变量分布的不匹配来量化实例间差异，从而改进了实例集异质性评估与求解器指导。


<details>
  <summary>Details</summary>
Motivation: 现有的类似度量方法往往缺乏精度，或者过于依赖标注数据，从而限制了其实用性和推广性。

Method: 提出了一个离散化右端项、权重与变量后，通过类似地球迁移距离的概念实现的数学距离度量，并设计了精确与贪婪两种变体以便应用于混合整数线性规划实例中。

Result: 实验结果表明，该度量可帮助区分实例类，贪婪版本相比精确版本速度提升200倍且效果相近，并在部分性能上超越现有的非学习方法，与监督分类器的表现相匹敌。

Conclusion: 该研究填补了现有度量方法的不足，提出的无监督距离度量能够有效评估混合整数线性规划实例间的特征差异，并具有较高的计算效率与性能可靠性。

Abstract: Mixed-integer linear programming (MILP) is a powerful tool for addressing a
wide range of real-world problems, but it lacks a clear structure for comparing
instances. A reliable similarity metric could establish meaningful
relationships between instances, enabling more effective evaluation of instance
set heterogeneity and providing better guidance to solvers, particularly when
machine learning is involved. Existing similarity metrics often lack precision
in identifying instance classes or rely heavily on labeled data, which limits
their applicability and generalization. To bridge this gap, this paper
introduces the first mathematical distance metric for MILP instances, derived
directly from their mathematical formulations. By discretizing right-hand
sides, weights, and variables into classes, the proposed metric draws
inspiration from the Earth mover's distance to quantify mismatches in
weight-variable distributions for constraint comparisons. This approach
naturally extends to enable instance-level comparisons. We evaluate both an
exact and a greedy variant of our metric under various parameter settings,
using the StrIPLIB dataset. Results show that all components of the metric
contribute to class identification, and that the greedy version achieves
accuracy nearly identical to the exact formulation while being nearly 200 times
faster. Compared to state-of-the-art baselines, including feature-based,
image-based, and neural network models, our unsupervised method consistently
outperforms all non-learned approaches and rivals the performance of a
supervised classifier on class and subclass grouping tasks.

</details>


### [208] [LogTinyLLM: Tiny Large Language Models Based Contextual Log Anomaly Detection](https://arxiv.org/abs/2507.11071)
*Isaiah Thompson Ocansey,Ritwik Bhattacharya,Tanmay Sen*

Main category: cs.LG

TL;DR: 提出一种基于LoRA和适配器的参数高效微调方法，用于在大型日志数据中检测上下文异常序列，性能较传统方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 由于日志序列的数据量大和结构复杂性，传统基于规则或深度学习的异常检测方法难以有效应对，因此需要一种更高效的方法，用于检测日志中的异常序列。

Method: 应用了LoRA（低秩适配）和适配器两种参数高效微调技术，并对比了多种小型大型语言模型在Thunderbird数据集上的表现。

Result: 基于LoRA的微调方法相比于LogBert的全量微调，性能提升了18%至19%，准确率从79.37%提高到97.76%-98.83%。

Conclusion: LoRA微调方法显著提升了日志序列异常检测的效率和准确性，在日志分析领域具有较高的应用价值。

Abstract: Log anomaly detection using traditional rule based or deep learning based
methods is often challenging due to the large volume and highly complex nature
of log sequence. So effective way of detection of anomalous sequence of logs is
crucial for system maintenance and development. This paper proposes parameter
efficient finetuning specifically low rank adaptation (LoRA) and adapter based
approaches for finding contextual anomalies in sequence of logs in large log
data set. It compares different tiny large language models (LLMs) on the
Thunderbird dataset. The results show that LoRA based finetuning provides
substantial performance improvements of 18 to 19 percentage over LogBert based
full finetuning approach, achieving accuracy scores between 97.76% and 98.83%
compared to 79.37%.

</details>


### [209] [Real-Time Bayesian Detection of Drift-Evasive GNSS Spoofing in Reinforcement Learning Based UAV Deconfliction](https://arxiv.org/abs/2507.11173)
*Deepak Kumar Panda,Weisi Guo*

Main category: cs.LG

TL;DR: 通过强化学习的贝叶斯在线变化点检测方法 (BOCPD)，提高了无人机对漂移规避欺骗攻击检测的准确性和响应速度。


<details>
  <summary>Details</summary>
Motivation: 受到全球导航卫星系统（GNSS）伪距测量的漏洞影响，无人机容易受到复杂欺骗攻击的威胁，特别是隐蔽的漂移规避攻击，不易通过常规检测机制发现。这使得需要开发新的时序检测方法以提高对无人机导航攻击的鲁棒性和响应速度。

Method: 提出将贝叶斯在线变化点检测（BOCPD）方法结合强化学习的RL批评网络中的时序值估计，作为检测无人机行为偏差的框架，并与传统GNSS检测手段和其他时序检测算法进行评估对比。

Result: 实验结果表明，该方法在检测GNSS漂移规避欺骗攻击方面表现优异，比传统检测方法和其他先进方法取得了更高的检测准确性，同时降低了误报率和漏报率。

Conclusion: 该研究提出的新方法证明了其应对隐蔽性强的GNSS欺骗攻击的能力，为提高无人机抗干扰性能提供了有效解决方案。

Abstract: Autonomous unmanned aerial vehicles (UAVs) rely on global navigation
satellite system (GNSS) pseudorange measurements for accurate real-time
localization and navigation. However, this dependence exposes them to
sophisticated spoofing threats, where adversaries manipulate pseudoranges to
deceive UAV receivers. Among these, drift-evasive spoofing attacks subtly
perturb measurements, gradually diverting the UAVs trajectory without
triggering conventional signal-level anti-spoofing mechanisms. Traditional
distributional shift detection techniques often require accumulating a
threshold number of samples, causing delays that impede rapid detection and
timely response. Consequently, robust temporal-scale detection methods are
essential to identify attack onset and enable contingency planning with
alternative sensing modalities, improving resilience against stealthy
adversarial manipulations. This study explores a Bayesian online change point
detection (BOCPD) approach that monitors temporal shifts in value estimates
from a reinforcement learning (RL) critic network to detect subtle behavioural
deviations in UAV navigation. Experimental results show that this temporal
value-based framework outperforms conventional GNSS spoofing detectors,
temporal semi-supervised learning frameworks, and the Page-Hinkley test,
achieving higher detection accuracy and lower false-positive and false-negative
rates for drift-evasive spoofing attacks.

</details>


### [210] [Gradient Regularization-based Neural Granger Causality](https://arxiv.org/abs/2507.11178)
*Meiliang Liu,Huiwen Dong,Xiaoxiao Yang,Yunfang Xu,Zijin Li,Zhengye Si,Xinyue Yang,Zhiwen Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种新型的基于梯度正则化的神经Granger因果分析方法（GRNGC），相比传统方法具有更高效率并能捕捉复杂交互关系。


<details>
  <summary>Details</summary>
Motivation: 解决现有神经网络-based Granger因果方法中单独建立模型和稀疏惩罚机制制约模型能力的缺陷。

Method: 提出GRNGC方法，使用一个时间序列预测模型，通过梯度的L1正则化来推断因果关系，且兼容多种架构如KAN、MLP和LSTM。

Result: 在多个合成数据集与实际数据集上的实验表明，GRNGC超越了一些现有基准模型，并显著降低了计算开销。

Conclusion: GRNGC具有灵活高效的特性，在因果关系推断和基因调控网络重构上表现优越，展现了广阔的应用前景。

Abstract: With the advancement of deep learning technologies, various neural
network-based Granger causality models have been proposed. Although these
models have demonstrated notable improvements, several limitations remain. Most
existing approaches adopt the component-wise architecture, necessitating the
construction of a separate model for each time series, which results in
substantial computational costs. In addition, imposing the sparsity-inducing
penalty on the first-layer weights of the neural network to extract causal
relationships weakens the model's ability to capture complex interactions. To
address these limitations, we propose Gradient Regularization-based Neural
Granger Causality (GRNGC), which requires only one time series prediction model
and applies $L_{1}$ regularization to the gradient between model's input and
output to infer Granger causality. Moreover, GRNGC is not tied to a specific
time series forecasting model and can be implemented with diverse architectures
such as KAN, MLP, and LSTM, offering enhanced flexibility. Numerical
simulations on DREAM, Lorenz-96, fMRI BOLD, and CausalTime show that GRNGC
outperforms existing baselines and significantly reduces computational
overhead. Meanwhile, experiments on real-world DNA, Yeast, HeLa, and bladder
urothelial carcinoma datasets further validate the model's effectiveness in
reconstructing gene regulatory networks.

</details>


### [211] [Mixture of Experts in Large Language Models](https://arxiv.org/abs/2507.11181)
*Danyang Zhang,Junhao Song,Ziqian Bi,Yingfang Yuan,Tianyang Wang,Joe Yeong,Junfeng Hao*

Main category: cs.LG

TL;DR: 本文对大规模语言模型中的专家混合（MoE）架构进行了全面综述，强调其在提升性能的同时具备低计算开销的优点。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过MoE架构提升大规模语言模型的性能并有效扩展模型容量，同时解决当前存在的研究问题和挑战。

Method: 通过系统分析，研究专家门控与路由机制、分层与稀疏MoE配置、元学习、多模态和多任务学习等多个方面，结合理论基础和实际案例，评估MoE体系结构。

Result: MoE架构具备高模型容量、增强任务性能、有效扩展能力等优势，并揭示了需要关注专家多样性、校准精度和推理可靠性的关键点。

Conclusion: 综述总结了现有研究局限及新挑战，并提出了未来研究方向，为MoE架构及其应用的进一步创新提供了基础。

Abstract: This paper presents a comprehensive review of the Mixture-of-Experts (MoE)
architecture in large language models, highlighting its ability to
significantly enhance model performance while maintaining minimal computational
overhead. Through a systematic analysis spanning theoretical foundations, core
architectural designs, and large language model (LLM) applications, we examine
expert gating and routing mechanisms, hierarchical and sparse MoE
configurations, meta-learning approaches, multimodal and multitask learning
scenarios, real-world deployment cases, and recent advances and challenges in
deep learning. Our analysis identifies key advantages of MoE, including
superior model capacity compared to equivalent Bayesian approaches, improved
task-specific performance, and the ability to scale model capacity efficiently.
We also underscore the importance of ensuring expert diversity, accurate
calibration, and reliable inference aggregation, as these are essential for
maximizing the effectiveness of MoE architectures. Finally, this review
outlines current research limitations, open challenges, and promising future
directions, providing a foundation for continued innovation in MoE architecture
and its applications.

</details>


### [212] [Quantized Rank Reduction: A Communications-Efficient Federated Learning Scheme for Network-Critical Applications](https://arxiv.org/abs/2507.11183)
*Dimitrios Kritsiolis,Constantine Kotropoulos*

Main category: cs.LG

TL;DR: 提出了一种通信高效的联邦学习方法，通过低秩近似和量化减少通信开销。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中由频繁模型更新引起的通信开销问题。

Method: 利用神经网络梯度的低秩近似和量化技术减少通信负担。

Result: 显著降低分布式学习过程中的网络负载，同时对模型精度影响较小。

Conclusion: 证明了通过低秩近似和量化可以实现通信高效的联邦学习。

Abstract: Federated learning is a machine learning approach that enables multiple
devices (i.e., agents) to train a shared model cooperatively without exchanging
raw data. This technique keeps data localized on user devices, ensuring privacy
and security, while each agent trains the model on their own data and only
shares model updates. The communication overhead is a significant challenge due
to the frequent exchange of model updates between the agents and the central
server. In this paper, we propose a communication-efficient federated learning
scheme that utilizes low-rank approximation of neural network gradients and
quantization to significantly reduce the network load of the decentralized
learning process with minimal impact on the model's accuracy.

</details>


### [213] [An Explainable AI-Enhanced Machine Learning Approach for Cardiovascular Disease Detection and Risk Assessment](https://arxiv.org/abs/2507.11185)
*Md. Emon Akter Sourov,Md. Sabbir Hossen,Pabon Shaha,Mohammad Minoar Hossain,Md Sadiq Iqbal*

Main category: cs.LG

TL;DR: 心脏疾病是全球性健康问题，研究提出结合分类模型和回归模型的框架，通过SMOTE技术生成额外数据以改善模型性能，最终随机森林在分类任务中表现最佳，线性回归在回归任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 目前传统的心脏病诊断方法难以高效且准确地识别疾病风险，尤其在医疗资源匮乏的地区，因此需要更高效的诊断方法。

Method: 研究利用心脏病数据集，结合分类模型（如随机森林）和回归模型（如线性回归），应用SMOTE技术生成额外数据，并通过多种性能指标进行评估，同时使用可解释性AI技术提升模型透明度。

Result: 分类任务中，随机森林模型在真实数据上达到97.2%的准确率，在合成数据上达到97.6%。回归任务中，线性回归在真实和合成数据上分别达到R2值0.992和0.984，并表现出最低误差。

Conclusion: 机器学习在心脏病诊断和风险预测上有巨大潜力，这种方法能够实现早期干预并提升临床决策水平。

Abstract: Heart disease remains a major global health concern, particularly in regions
with limited access to medical resources and diagnostic facilities. Traditional
diagnostic methods often fail to accurately identify and manage heart disease
risks, leading to adverse outcomes. Machine learning has the potential to
significantly enhance the accuracy, efficiency, and speed of heart disease
diagnosis. In this study, we proposed a comprehensive framework that combines
classification models for heart disease detection and regression models for
risk prediction. We employed the Heart Disease dataset, which comprises 1,035
cases. To address the issue of class imbalance, the Synthetic Minority
Oversampling Technique (SMOTE) was applied, resulting in the generation of an
additional 100,000 synthetic data points. Performance metrics, including
accuracy, precision, recall, F1-score, R2, MSE, RMSE, and MAE, were used to
evaluate the model's effectiveness. Among the classification models, Random
Forest emerged as the standout performer, achieving an accuracy of 97.2% on
real data and 97.6% on synthetic data. For regression tasks, Linear Regression
demonstrated the highest R2 values of 0.992 and 0.984 on real and synthetic
datasets, respectively, with the lowest error metrics. Additionally,
Explainable AI techniques were employed to enhance the interpretability of the
models. This study highlights the potential of machine learning to
revolutionize heart disease diagnosis and risk prediction, thereby facilitating
early intervention and enhancing clinical decision-making.

</details>


### [214] [Striking the Perfect Balance: Preserving Privacy While Boosting Utility in Collaborative Medical Prediction Platforms](https://arxiv.org/abs/2507.11187)
*Shao-Bo Lin,Xiaotong Liu,Yao Wang*

Main category: cs.LG

TL;DR: 本文提出了一种隐私保护协作医疗预测平台，通过分布式学习框架提高预测性能，同时保护患者和医生的隐私。


<details>
  <summary>Details</summary>
Motivation: 针对患者隐私泄露和医生模型被提取的担忧，以及低质量预测导致的协作参与率降低问题，提出隐私保护与预测性能兼顾的解决方案。

Method: 通过设计隐私保护机制，并将其融入到新颖的一次性分布式学习框架中，基于统计学习理论进行理论验证，随后在仿真与真实数据实验中测试平台性能。

Result: 理论分析证明在隐私要求下达到最佳预测性能，实验验证了平台的有效性。

Conclusion: 该研究成功开发了隐私保护与高效预测兼顾的医疗协作预测平台，为提升参与率和预测精准度提供了技术基础。

Abstract: Online collaborative medical prediction platforms offer convenience and
real-time feedback by leveraging massive electronic health records. However,
growing concerns about privacy and low prediction quality can deter patient
participation and doctor cooperation. In this paper, we first clarify the
privacy attacks, namely attribute attacks targeting patients and model
extraction attacks targeting doctors, and specify the corresponding privacy
principles. We then propose a privacy-preserving mechanism and integrate it
into a novel one-shot distributed learning framework, aiming to simultaneously
meet both privacy requirements and prediction performance objectives. Within
the framework of statistical learning theory, we theoretically demonstrate that
the proposed distributed learning framework can achieve the optimal prediction
performance under specific privacy requirements. We further validate the
developed privacy-preserving collaborative medical prediction platform through
both toy simulations and real-world data experiments.

</details>


### [215] [Gradient Descent on Logistic Regression: Do Large Step-Sizes Work with Data on the Sphere?](https://arxiv.org/abs/2507.11228)
*Si Yi Meng,Baptiste Goujaud,Antonio Orvieto,Christopher De Sa*

Main category: cs.LG

TL;DR: 本文研究了在数据具有相等幅度、步长低于稳定性阈值的情况下，梯度下降法在逻辑回归上的全局收敛性，并证明了一维空间中成立但高维可能仍会产生循环行为。


<details>
  <summary>Details</summary>
Motivation: 探讨逻辑回归在非线性可分场景下，如何通过条件约束如数据幅度限制，来判断梯度下降法的收敛性。

Method: 分析数据具有相等幅度的情况下，步长低于稳定性阈值时梯度下降的行为，分一维和高维两种情况进行证明和讨论。

Result: 在一维空间中，限制数据幅度可保证全局收敛；然而在高维空间中，数据可能出现循环行为。

Conclusion: 数据幅度限制可在某些情况下提升收敛性，但高维情景仍需进一步研究，以寻求确保大步长全局收敛的充分条件。

Abstract: Gradient descent (GD) on logistic regression has many fascinating properties.
When the dataset is linearly separable, it is known that the iterates converge
in direction to the maximum-margin separator regardless of how large the step
size is. In the non-separable case, however, it has been shown that GD can
exhibit a cycling behaviour even when the step sizes is still below the
stability threshold $2/\lambda$, where $\lambda$ is the largest eigenvalue of
the Hessian at the solution. This short paper explores whether restricting the
data to have equal magnitude is a sufficient condition for global convergence,
under any step size below the stability threshold. We prove that this is true
in a one dimensional space, but in higher dimensions cycling behaviour can
still occur. We hope to inspire further studies on quantifying how common these
cycles are in realistic datasets, as well as finding sufficient conditions to
guarantee global convergence with large step sizes.

</details>


### [216] [Generative Click-through Rate Prediction with Applications to Search Advertising](https://arxiv.org/abs/2507.11246)
*Lingwei Kong,Lu Wang,Changping Peng,Zhangang Lin,Ching Law,Jingping Shao*

Main category: cs.LG

TL;DR: 该论文提出使用生成模型提高点击率预测模型的精度，并通过两阶段训练方法有效验证其性能。


<details>
  <summary>Details</summary>
Motivation: 当前的点击率预测模型主要依赖用户历史行为和商品信息，但在表达能力上仍存在不足。生成模型的成功展示了其超越判别模型的潜力。

Method: 提出了一种结合生成模型和判别模型的新方法，采用两阶段训练策略。第一阶段通过生成预训练进行用户行为序列的预测，第二阶段将经过预训练的生成模型融入判别模型框架中进行点击率预测。

Result: 通过新数据集的大量实验和在线A/B测试验证了该方法的有效性。

Conclusion: 模型展现了在工业中显著的实际应用价值，目前已部署于大型电商平台，未来将公开代码和数据集。

Abstract: Click-Through Rate (CTR) prediction models are integral to a myriad of
industrial settings, such as personalized search advertising. Current methods
typically involve feature extraction from users' historical behavior sequences
combined with product information, feeding into a discriminative model that is
trained on user feedback to estimate CTR. With the success of models such as
GPT, the potential for generative models to enrich expressive power beyond
discriminative models has become apparent. In light of this, we introduce a
novel model that leverages generative models to enhance the precision of CTR
predictions in discriminative models. To reconcile the disparate data
aggregation needs of both model types, we design a two-stage training process:
1) Generative pre-training for next-item prediction with the given item
category in user behavior sequences; 2) Fine-tuning the well-trained generative
model within a discriminative CTR prediction framework. Our method's efficacy
is substantiated through extensive experiments on a new dataset, and its
significant utility is further corroborated by online A/B testing results.
Currently, the model is deployed on one of the world's largest e-commerce
platforms, and we intend to release the associated code and dataset in the
future.

</details>


### [217] [LyAm: Robust Non-Convex Optimization for Stable Learning in Noisy Environments](https://arxiv.org/abs/2507.11262)
*Elmira Mirzabeigi,Sepehr Rezaee,Kourosh Parand*

Main category: cs.LG

TL;DR: 提出了一种新型优化器LyAm，通过将Lyapunov稳定性理论与Adam相结合，优化深度学习的过程，改善收敛稳健性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络训练过程中常出现梯度噪声和收敛不稳定的问题，影响了性能和泛化能力。

Method: 将Adam优化器的自适应动量估计与Lyapunov稳定性机制相结合，动态调整学习率以减弱训练噪声并增强鲁棒性。

Result: 在CIFAR-10、CIFAR-100等实验中，LyAm在准确性、收敛速度和稳定性方面超过了现有优化器。

Conclusion: LyAm优化器显著提高了深度学习训练的稳定性和性能，是一种稳健的深度神经网络优化工具。

Abstract: Training deep neural networks, particularly in computer vision tasks, often
suffers from noisy gradients and unstable convergence, which hinder performance
and generalization. In this paper, we propose LyAm, a novel optimizer that
integrates Adam's adaptive moment estimation with Lyapunov-based stability
mechanisms. LyAm dynamically adjusts the learning rate using Lyapunov stability
theory to enhance convergence robustness and mitigate training noise. We
provide a rigorous theoretical framework proving the convergence guarantees of
LyAm in complex, non-convex settings. Extensive experiments on like as CIFAR-10
and CIFAR-100 show that LyAm consistently outperforms state-of-the-art
optimizers in terms of accuracy, convergence speed, and stability, establishing
it as a strong candidate for robust deep learning optimization.

</details>


### [218] [Turning Sand to Gold: Recycling Data to Bridge On-Policy and Off-Policy Learning via Causal Bound](https://arxiv.org/abs/2507.11269)
*Tal Fiskus,Uri Shaham*

Main category: cs.LG

TL;DR: 本文提出一种新方法，将Neyman-Rubin潜在结果框架融入深度强化学习（DRL），通过对事实损失建立因果性界限，显著提高了样本效率和奖励比率，同时大幅减少经验重放缓冲区的大小。


<details>
  <summary>Details</summary>
Motivation: 传统DRL方法需要大量训练步骤和庞大的经验回放缓冲区，导致资源需求居高不下，亟需提高样本效率的新方法。

Method: 本文通过引入Neyman-Rubin潜在结果框架，为DRL中的事实损失建立因果性界限，同时在经验回放缓冲区中存储以往的价值网络输出，以提高数据利用率。

Result: 实验表明，在Atari 2600和MuJoCo平台上，对DQN、SAC等多种代理的奖励比率最高提升2427%，同时经验回放缓冲区的大小最高减少96%。

Conclusion: 该方法在不增加成本的情况下，显著提高了样本效率和训练性能，为DRL在经验利用方面提供了新的理论基础和实践可能。

Abstract: Deep reinforcement learning (DRL) agents excel in solving complex
decision-making tasks across various domains. However, they often require a
substantial number of training steps and a vast experience replay buffer,
leading to significant computational and resource demands. To address these
challenges, we introduce a novel theoretical result that leverages the
Neyman-Rubin potential outcomes framework into DRL. Unlike most methods that
focus on bounding the counterfactual loss, we establish a causal bound on the
factual loss, which is analogous to the on-policy loss in DRL. This bound is
computed by storing past value network outputs in the experience replay buffer,
effectively utilizing data that is usually discarded. Extensive experiments
across the Atari 2600 and MuJoCo domains on various agents, such as DQN and
SAC, achieve up to 2,427% higher reward ratio, outperforming the same agents
without our proposed term, and reducing the experience replay buffer size by up
to 96%, significantly improving sample efficiency at negligible cost.

</details>


### [219] [Fast Last-Iterate Convergence of SGD in the Smooth Interpolation Regime](https://arxiv.org/abs/2507.11274)
*Amit Attia,Matan Schliserman,Uri Sherman,Tomer Koren*

Main category: cs.LG

TL;DR: 本文探讨了平滑凸目标在插值情况下使用随机梯度下降法（SGD）的全局收敛性，并获得了最优的最后一轮预期风险收敛率。


<details>
  <summary>Details</summary>
Motivation: 近年来，大步长下SGD最后一轮迭代的行为受到了更多关注，因为它在超参数模型训练、持续学习遗忘分析及随机Kaczmarz方法解决线性系统的收敛性研究中具有重要意义。

Method: 在$\beta$平滑凸损失函数且步长满足$\eta \leq 1/\beta$的条件下，本文分析了经过$T$步SGD后最后迭代点的表现，推导出期望过量风险公式，并讨论了步长调节对收敛表现的影响。

Result: 研究得出通过合理调优步长可使最后一轮迭代获得接近最优的收敛率：$\widetilde{O}(1/T + \sigma_\star/\sqrt{T})$，而在无噪声情况$\sigma_\star=0$下，收敛率达到$O(1/\sqrt{T})$。

Conclusion: 本文扩展了先前研究结果并改进了一些特殊情况下的最优收敛率，为研究SGD的最后迭代行为提供了新的理论依据。

Abstract: We study population convergence guarantees of stochastic gradient descent
(SGD) for smooth convex objectives in the interpolation regime, where the noise
at optimum is zero or near zero. The behavior of the last iterate of SGD in
this setting -- particularly with large (constant) stepsizes -- has received
growing attention in recent years due to implications for the training of
over-parameterized models, as well as to analyzing forgetting in continual
learning and to understanding the convergence of the randomized Kaczmarz method
for solving linear systems. We establish that after $T$ steps of SGD on
$\beta$-smooth convex loss functions with stepsize $\eta \leq 1/\beta$, the
last iterate exhibits expected excess risk $\widetilde{O}(1/(\eta
T^{1-\beta\eta/2}) + \eta T^{\beta\eta/2} \sigma_\star^2)$, where
$\sigma_\star^2$ denotes the variance of the stochastic gradients at the
optimum. In particular, for a well-tuned stepsize we obtain a near optimal
$\widetilde{O}(1/T + \sigma_\star/\sqrt{T})$ rate for the last iterate,
extending the results of Varre et al. (2021) beyond least squares regression;
and when $\sigma_\star=0$ we obtain a rate of $O(1/\sqrt{T})$ with
$\eta=1/\beta$, improving upon the best-known $O(T^{-1/4})$ rate recently
established by Evron et al. (2025) in the special case of realizable linear
regression.

</details>


### [220] [Guiding LLM Decision-Making with Fairness Reward Models](https://arxiv.org/abs/2507.11344)
*Zara Hall,Melanie Subbiah,Thomas P Zollo,Kathleen McKeown,Richard Zemel*

Main category: cs.LG

TL;DR: 提出一种方法，通过训练普适性的公平性奖励模型（FRM）来增强语言模型在高影响力决策中的公平性和准确信度。


<details>
  <summary>Details</summary>
Motivation: 语言模型在高风险决策场景中使用，存在放大不公平偏见的风险，需要一种方法来确保其决策的公平性和可信度。

Method: 利用弱监督和语言模型标注的偏见例子训练一个通用的公平性奖励模型（FRM），为语言模型决策赋予公平性评分，从而降低不公平路径的权重，增强公平路径的优先性。

Result: 单一的FRM能够在无需额外微调的情况下适应不同任务、领域和语言模型系列，并在实际任务（如累犯预测和社交媒体管理）中提高公平性，同时保持甚至超越基线准确率。

Conclusion: 所提出的框架在解决语言模型决策中的公平性问题上表现优异，为信任其在高风险决策应用中的使用提供了可能性。

Abstract: Large language models are increasingly used to support high-stakes decisions,
potentially influencing who is granted bail or receives a loan. Naive
chain-of-thought sampling can improve average decision accuracy, but has also
been shown to amplify unfair bias. To address this challenge and enable the
trustworthy use of reasoning models in high-stakes decision-making, we propose
a framework for training a generalizable Fairness Reward Model (FRM). Our model
assigns a fairness score to LLM reasoning, enabling the system to down-weight
biased trajectories and favor equitable ones when aggregating decisions across
reasoning chains. We show that a single Fairness Reward Model, trained on
weakly supervised, LLM-annotated examples of biased versus unbiased reasoning,
transfers across tasks, domains, and model families without additional
fine-tuning. Applied to real-world decision-making tasks including recidivism
prediction and social media moderation, we show that our approach consistently
improves fairness while matching, or even surpassing, baseline accuracy.

</details>


### [221] [Neurosymbolic Reasoning Shortcuts under the Independence Assumption](https://arxiv.org/abs/2507.11357)
*Emile van Krieken,Pasquale Minervini,Edoardo Ponti,Antonio Vergari*

Main category: cs.LG

TL;DR: 本文探讨符号概念之间的独立性假设在神经符号预测器中的影响，并正式证明该假设会导致模型无法正确表达某些概念组合的不确定性。


<details>
  <summary>Details</summary>
Motivation: 独立性假设在神经符号系统中被广泛使用以简化推理，但有人质疑这种假设在何种情况下限制了系统性能。

Method: 作者通过理论分析，证明了独立性假设会导致模型无法检测某些推理捷径，以此揭示其限制。

Result: 证明显示该假设阻碍了模型对某些概念组合的不确定性建模，并影响了下游任务的准确性。

Conclusion: 表明独立性假设不仅影响学习过程，还导致模型在逻辑推理时无法正确处理特定不确定性情况。

Abstract: The ubiquitous independence assumption among symbolic concepts in
neurosymbolic (NeSy) predictors is a convenient simplification: NeSy predictors
use it to speed up probabilistic reasoning. Recent works like van Krieken et
al. (2024) and Marconato et al. (2024) argued that the independence assumption
can hinder learning of NeSy predictors and, more crucially, prevent them from
correctly modelling uncertainty. There is, however, scepticism in the NeSy
community around the scenarios in which the independence assumption actually
limits NeSy systems (Faronius and Dos Martires, 2025). In this work, we settle
this question by formally showing that assuming independence among symbolic
concepts entails that a model can never represent uncertainty over certain
concept combinations. Thus, the model fails to be aware of reasoning shortcuts,
i.e., the pathological behaviour of NeSy predictors that predict correct
downstream tasks but for the wrong reasons.

</details>


### [222] [Local Pairwise Distance Matching for Backpropagation-Free Reinforcement Learning](https://arxiv.org/abs/2507.11367)
*Daniel Tanneberg*

Main category: cs.LG

TL;DR: 作者提出一种通过局部信号训练神经网络每一层的新方法，避免了反向传播需要的中间激活存储问题，并提升算法在强化学习中的稳定性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络通过反向传播进行训练，但其需要存储前向传播的中间激活结果，并容易受到梯度消失或梯度爆炸问题的影响，导致学习性能下降。

Method: 提出了一种利用多维标度中匹配成对距离的原理，结合可选的奖励驱动的指导，以局部、逐层损失函数训练每一层神经网络的方法，不需要反向传播及中间激活存储。

Result: 实验验证了该方法在强化学习基准中与传统反向传播相比性能具有竞争力，同时提高了在复杂环境中的表现和运行的稳定性及一致性。

Conclusion: 该方法通过减少对反向传播的依赖，同时克服其缺陷，为神经网络训练提供了一种稳定性更高且表现更优的新思路。

Abstract: Training neural networks with reinforcement learning (RL) typically relies on
backpropagation (BP), necessitating storage of activations from the forward
pass for subsequent backward updates. Furthermore, backpropagating error
signals through multiple layers often leads to vanishing or exploding
gradients, which can degrade learning performance and stability. We propose a
novel approach that trains each layer of the neural network using local signals
during the forward pass in RL settings. Our approach introduces local,
layer-wise losses leveraging the principle of matching pairwise distances from
multi-dimensional scaling, enhanced with optional reward-driven guidance. This
method allows each hidden layer to be trained using local signals computed
during forward propagation, thus eliminating the need for backward passes and
storing intermediate activations. Our experiments, conducted with policy
gradient methods across common RL benchmarks, demonstrate that this
backpropagation-free method achieves competitive performance compared to their
classical BP-based counterpart. Additionally, the proposed method enhances
stability and consistency within and across runs, and improves performance
especially in challenging environments.

</details>


### [223] [Step-wise Policy for Rare-tool Knowledge (SPaRK): Offline RL that Drives Diverse Tool Use in LLMs](https://arxiv.org/abs/2507.11371)
*Gabriel Bo,Koa Chang,Justin Gu*

Main category: cs.LG

TL;DR: SPaRK是一个新的强化学习框架，通过稀有工具的多样性使用提高语言模型的决策能力。


<details>
  <summary>Details</summary>
Motivation: 希望强化语言模型对工具多样性的探索能力，突破传统高温采样方法的局限性。

Method: 提出双目标奖励机制，优化答案质量和工具多样性，利用稀有性优先的策略，结合离线PPO训练和GPT-4o评分模型。

Result: SPaRK在MMLU-Pro的14个类别中表现优异，比基线模型具有显著的工具选择熵增幅。

Conclusion: 通过显式增加工具多样性，能够增强推理能力，同时保持高准确性。

Abstract: We present Step-wise Policy for Rare-tool Knowledge (SPaRK), a novel
reinforcement learning framework that teaches large language models to explore
diverse tool usage patterns beyond conventional high-temperature sampling.
Building on recent advances in step-wise reinforcement learning, we introduce a
dual-objective reward system that simultaneously optimizes for answer quality
and tool diversity, training a Llama-3.1 8B model through offline PPO on
synthetically generated trajectories from the MMLU-Pro dataset. Our approach
uniquely employs a rarity-first exploitation strategy where a GPT-4o judge
scores candidate actions across eight distinct tools plus chain-of-thought
reasoning, with the policy favoring less-frequently used but still viable tools
to encourage systematic exploration. Empirical results demonstrate that SPaRK
achieves competitive performance across 14 MMLU-Pro categories while exhibiting
significantly higher entropy in tool selection compared to both baseline and
supervised fine-tuning approaches, suggesting that algorithmic exploration
through explicit tool diversity can enhance reasoning capabilities without
sacrificing accuracy.

</details>


### [224] [A Neural Network Model of Complementary Learning Systems: Pattern Separation and Completion for Continual Learning](https://arxiv.org/abs/2507.11393)
*James P Jun,Vijay Marupudi,Raj Sanjay Shah,Sashank Varma*

Main category: cs.LG

TL;DR: 通过将变分自编码器（VAE）与现代Hopfield网络（MHN）相结合，提出了一种能够减少遗忘并且实现类似脑部记忆巩固与泛化功能的持续学习模型。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在持续学习中存在的灾难性遗忘问题，为减少信息遗忘的机制提供解释。

Method: 结合变分自编码器（VAE）的表示泛化能力与现代Hopfield网络（MHN）的记忆存储能力，评估模型在Split-MNIST任务上的表现，并分析两个组成部分的功能分离。

Result: 该模型在Split-MNIST任务中实现了接近90%的准确率，接近当前最优水平，大幅减少了遗忘现象。

Conclusion: 通过将模式分离与模式完成结合在可扩展架构中，该工作为生物和人工系统中的记忆巩固、泛化与持续学习建模提供了一个功能性范本。

Abstract: Learning new information without forgetting prior knowledge is central to
human intelligence. In contrast, neural network models suffer from catastrophic
forgetting: a significant degradation in performance on previously learned
tasks when acquiring new information. The Complementary Learning Systems (CLS)
theory offers an explanation for this human ability, proposing that the brain
has distinct systems for pattern separation (encoding distinct memories) and
pattern completion (retrieving complete memories from partial cues). To capture
these complementary functions, we leverage the representational generalization
capabilities of variational autoencoders (VAEs) and the robust memory storage
properties of Modern Hopfield networks (MHNs), combining them into a neurally
plausible continual learning model. We evaluate this model on the Split-MNIST
task, a popular continual learning benchmark, and achieve close to
state-of-the-art accuracy (~90%), substantially reducing forgetting.
Representational analyses empirically confirm the functional dissociation: the
VAE underwrites pattern completion, while the MHN drives pattern separation. By
capturing pattern separation and completion in scalable architectures, our work
provides a functional template for modeling memory consolidation,
generalization, and continual learning in both biological and artificial
systems.

</details>


### [225] [Robust-Multi-Task Gradient Boosting](https://arxiv.org/abs/2507.11411)
*Seyedsaman Emami,Gonzalo Martínez-Muñoz,Daniel Hernández-Lobato*

Main category: cs.LG

TL;DR: 本文提出了一种名为R-MTGB的新型多任务梯度提升框架，旨在解决任务异质性问题，并通过三阶段架构实现鲁棒性和精确性。


<details>
  <summary>Details</summary>
Motivation: 研究如何在任务不完全一致甚至存在对抗性任务的情况下，通过多任务学习提升整体模型性能。

Method: 将学习过程分为三阶段：学习共享模式、使用正则参数划分异常任务和正常任务，以及微调任务特定预测器，结合梯度提升方法实现。

Result: 该方法在合成基准和实际数据集中表现优异，能有效隔离异常任务，同时实现知识迁移并降低每个任务的预测误差。

Conclusion: R-MTGB能够在具有挑战性的多任务学习环境下实现鲁棒性、适应性及可靠的收敛效果。

Abstract: Multi-task learning (MTL) has shown effectiveness in exploiting shared
information across tasks to improve generalization. MTL assumes tasks share
similarities that can improve performance. In addition, boosting algorithms
have demonstrated exceptional performance across diverse learning problems,
primarily due to their ability to focus on hard-to-learn instances and
iteratively reduce residual errors. This makes them a promising approach for
learning multi-task problems. However, real-world MTL scenarios often involve
tasks that are not well-aligned (known as outlier or adversarial tasks), which
do not share beneficial similarities with others and can, in fact, deteriorate
the performance of the overall model. To overcome this challenge, we propose
Robust-Multi-Task Gradient Boosting (R-MTGB), a novel boosting framework that
explicitly models and adapts to task heterogeneity during training. R-MTGB
structures the learning process into three sequential blocks: (1) learning
shared patterns, (2) partitioning tasks into outliers and non-outliers with
regularized parameters, and (3) fine-tuning task-specific predictors. This
architecture enables R-MTGB to automatically detect and penalize outlier tasks
while promoting effective knowledge transfer among related tasks. Our method
integrates these mechanisms seamlessly within gradient boosting, allowing
robust handling of noisy or adversarial tasks without sacrificing accuracy.
Extensive experiments on both synthetic benchmarks and real-world datasets
demonstrate that our approach successfully isolates outliers, transfers
knowledge, and consistently reduces prediction errors for each task
individually, and achieves overall performance gains across all tasks. These
results highlight robustness, adaptability, and reliable convergence of R-MTGB
in challenging MTL environments.

</details>


### [226] [Toward Improving fNIRS Classification: A Study on Activation Functions in Deep Neural Architectures](https://arxiv.org/abs/2507.11436)
*Behtom Adeli,John McLinden,Pankaj Pandey,Ming Shao,Yalda Shahriari*

Main category: cs.LG

TL;DR: 研究探讨不同激活函数在fNIRS领域深度学习中的性能。发现对称激活函数（如Tanh和Abs(x)）可能优于ReLU，并高效提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 当前文献中对激活函数在fNIRS领域的影响研究较少，而fNIRS数据的非线性、低信噪比及信号变化性对深度学习提出了严峻挑战。

Method: 评估多种深度学习架构（如fNIRSNet、AbsoluteNet等）及多种激活函数在标准化预处理和一致训练参数下的表现，并单独分析对称性对模型性能的影响。

Result: 对称激活函数（如Tanh和Abs(x)）在特定架构下优于ReLU；修改后的绝对值函数也显示对称性对于性能的提升作用。

Conclusion: 研究表明选择与fNIRS信号特性相符的对称激活函数可显著提升深度学习性能，需在应用中予以重视。

Abstract: Activation functions are critical to the performance of deep neural networks,
particularly in domains such as functional near-infrared spectroscopy (fNIRS),
where nonlinearity, low signal-to-noise ratio (SNR), and signal variability
poses significant challenges to model accuracy. However, the impact of
activation functions on deep learning (DL) performance in the fNIRS domain
remains underexplored and lacks systematic investigation in the current
literature. This study evaluates a range of conventional and field-specific
activation functions for fNIRS classification tasks using multiple deep
learning architectures, including the domain-specific fNIRSNet, AbsoluteNet,
MDNN, and shallowConvNet (as the baseline), all tested on a single dataset
recorded during an auditory task. To ensure fair a comparison, all networks
were trained and tested using standardized preprocessing and consistent
training parameters. The results show that symmetrical activation functions
such as Tanh and the Absolute value function Abs(x) can outperform commonly
used functions like the Rectified Linear Unit (ReLU), depending on the
architecture. Additionally, a focused analysis of the role of symmetry was
conducted using a Modified Absolute Function (MAF), with results further
supporting the effectiveness of symmetrical activation functions on performance
gains. These findings underscore the importance of selecting proper activation
functions that align with the signal characteristics of fNIRS data.

</details>


### [227] [Data Augmentation in Time Series Forecasting through Inverted Framework](https://arxiv.org/abs/2507.11439)
*Hongming Tan,Ting Chen,Ruochong Jin,Wai Kin Chan*

Main category: cs.LG

TL;DR: 本文提出了一种名为DAIF的新型实时数据增强方法，用于改进iTransformer在多变量时间序列预测中的表现。


<details>
  <summary>Details</summary>
Motivation: iTransformer虽然有效，但其倒置框架存在降低时间依赖信息及引入噪声的不足。

Method: 定义倒置序列到序列框架结构，并提出两种DAIF策略：频率过滤和交叉变化拼接。

Result: 在多个数据集和倒置模型上进行了实验验证，证明了DAIF方法的有效性。

Conclusion: DAIF可有效解决倒置框架的局限性，为MTS预测中的数据增强提供了新选择。

Abstract: Currently, iTransformer is one of the most popular and effective models for
multivariate time series (MTS) forecasting. Thanks to its inverted framework,
iTransformer effectively captures multivariate correlation. However, the
inverted framework still has some limitations. It diminishes temporal
interdependency information, and introduces noise in cases of nonsignificant
variable correlation. To address these limitations, we introduce a novel data
augmentation method on inverted framework, called DAIF. Unlike previous data
augmentation methods, DAIF stands out as the first real-time augmentation
specifically designed for the inverted framework in MTS forecasting. We first
define the structure of the inverted sequence-to-sequence framework, then
propose two different DAIF strategies, Frequency Filtering and Cross-variation
Patching to address the existing challenges of the inverted framework.
Experiments across multiple datasets and inverted models have demonstrated the
effectiveness of our DAIF.

</details>


### [228] [LRMR: LLM-Driven Relational Multi-node Ranking for Lymph Node Metastasis Assessment in Rectal Cancer](https://arxiv.org/abs/2507.11457)
*Yaoxian Dong,Yifan Gao,Haoyue Li,Yanfen Cui,Xin Gao*

Main category: cs.LG

TL;DR: 提出了一种名为LRMR的新框架，用于直肠癌淋巴结转移评估，通过两阶段的LLM模型实现更高的诊断性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统MRI基于形态学的评估方法对直肠癌淋巴结转移的诊断性能有限，现有人工智能模型缺乏临床所需的可解释性且通常忽略患者整体情况。

Method: 引入LRMR框架，利用多模态LLM生成淋巴结影像的重要特征报告，并用基于文本的LLM进行患者间的相对风险排序。

Result: 在117名直肠癌患者的回顾性队列中，LRMR框架表现出优于深度学习基线的性能，AUC为0.7917，F1为0.7200，验证了框架的有效性和贡献。

Conclusion: 通过视觉感知与认知推理分离的双阶段LLM框架，为评估直肠癌淋巴结转移提供了一种强大、可解释且有效的新范式。

Abstract: Accurate preoperative assessment of lymph node (LN) metastasis in rectal
cancer guides treatment decisions, yet conventional MRI evaluation based on
morphological criteria shows limited diagnostic performance. While some
artificial intelligence models have been developed, they often operate as black
boxes, lacking the interpretability needed for clinical trust. Moreover, these
models typically evaluate nodes in isolation, overlooking the patient-level
context. To address these limitations, we introduce LRMR, an LLM-Driven
Relational Multi-node Ranking framework. This approach reframes the diagnostic
task from a direct classification problem into a structured reasoning and
ranking process. The LRMR framework operates in two stages. First, a multimodal
large language model (LLM) analyzes a composite montage image of all LNs from a
patient, generating a structured report that details ten distinct radiological
features. Second, a text-based LLM performs pairwise comparisons of these
reports between different patients, establishing a relative risk ranking based
on the severity and number of adverse features. We evaluated our method on a
retrospective cohort of 117 rectal cancer patients. LRMR achieved an area under
the curve (AUC) of 0.7917 and an F1-score of 0.7200, outperforming a range of
deep learning baselines, including ResNet50 (AUC 0.7708). Ablation studies
confirmed the value of our two main contributions: removing the relational
ranking stage or the structured prompting stage led to a significant
performance drop, with AUCs falling to 0.6875 and 0.6458, respectively. Our
work demonstrates that decoupling visual perception from cognitive reasoning
through a two-stage LLM framework offers a powerful, interpretable, and
effective new paradigm for assessing lymph node metastasis in rectal cancer.

</details>


### [229] [D3FL: Data Distribution and Detrending for Robust Federated Learning in Non-linear Time-series Data](https://arxiv.org/abs/2507.11471)
*Harsha Varun Marisetty,Manik Gupta,Yogesh Simmhan*

Main category: cs.LG

TL;DR: 提出了在联邦学习框架下处理非线性、非平稳时间序列数据的方法，研究了数据分布和去趋势技术对预测模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 物联网数据往往是时间序列数据的形式，其分布可能是非线性和非平稳的，为了高效预测这些数据的趋势，需要克服传统集中分析方法引发的延迟和通信成本问题。

Method: 在生成的非线性分布（如广义极值分布和对数正态分布）合成时间序列数据以及实际数据集上，使用基于LSTM的预测模型进行试验，通过集中式与联邦学习方式进行性能对比，并引入去趋势技术进行性能改进分析。

Result: 结果表明，联邦学习在处理非线性数据分布时表现逊于集中式方法；但采用适当的去趋势技术可以提升联邦学习的性能，降低不同数据分布上的损失。

Conclusion: 研究展示了去趋势技术对于提高联邦学习处理非线性、非平稳时间序列数据的实际效果的重要性。

Abstract: With advancements in computing and communication technologies, the Internet
of Things (IoT) has seen significant growth. IoT devices typically collect data
from various sensors, such as temperature, humidity, and energy meters. Much of
this data is temporal in nature. Traditionally, data from IoT devices is
centralized for analysis, but this approach introduces delays and increased
communication costs. Federated learning (FL) has emerged as an effective
alternative, allowing for model training across distributed devices without the
need to centralize data. In many applications, such as smart home energy and
environmental monitoring, the data collected by IoT devices across different
locations can exhibit significant variation in trends and seasonal patterns.
Accurately forecasting such non-stationary, non-linear time-series data is
crucial for applications like energy consumption estimation and weather
forecasting. However, these data variations can severely impact prediction
accuracy. The key contributions of this paper are: (1) Investigating how
non-linear, non-stationary time-series data distributions, like generalized
extreme value (gen-extreme) and log norm distributions, affect FL performance.
(2) Analyzing how different detrending techniques for non-linear time-series
data influence the forecasting model's performance in a FL setup. We generated
several synthetic time-series datasets using non-linear data distributions and
trained an LSTM-based forecasting model using both centralized and FL
approaches. Additionally, we evaluated the impact of detrending on real-world
datasets with non-linear time-series data distributions. Our experimental
results show that: (1) FL performs worse than centralized approaches when
dealing with non-linear data distributions. (2) The use of appropriate
detrending techniques improves FL performance, reducing loss across different
data distributions.

</details>


### [230] [Exploring the robustness of TractOracle methods in RL-based tractography](https://arxiv.org/abs/2507.11486)
*Jeremi Levesque,Antoine Théberge,Maxime Descoteaux,Pierre-Marc Jodoin*

Main category: cs.LG

TL;DR: 本文提出通过改进TractOracle-RL实现更准确和可靠的大脑白质束追踪。


<details>
  <summary>Details</summary>
Motivation: 现有的基于RL的TractOracle-RL方法已在白质追踪中表现优异，但仍需进一步改进以提升准确性和可靠性。

Method: 通过整合四种RL进展对TractOracle-RL框架进行扩展，提出一种受RLHF启发的新训练方案Iterative Reward Training（IRT）。

Result: 升级的RL方法在五个扩展数据集上表现出一致的可靠性，IRT可迭代改进oracle引导，显著优于传统方法。

Conclusion: 与传统方法相比，使用oracle反馈训练的RL方法在精度和解剖学有效性上均有显著优势。

Abstract: Tractography algorithms leverage diffusion MRI to reconstruct the fibrous
architecture of the brain's white matter. Among machine learning approaches,
reinforcement learning (RL) has emerged as a promising framework for
tractography, outperforming traditional methods in several key aspects.
TractOracle-RL, a recent RL-based approach, reduces false positives by
incorporating anatomical priors into the training process via a reward-based
mechanism. In this paper, we investigate four extensions of the original
TractOracle-RL framework by integrating recent advances in RL, and we evaluate
their performance across five diverse diffusion MRI datasets. Results
demonstrate that combining an oracle with the RL framework consistently leads
to robust and reliable tractography, regardless of the specific method or
dataset used. We also introduce a novel RL training scheme called Iterative
Reward Training (IRT), inspired by the Reinforcement Learning from Human
Feedback (RLHF) paradigm. Instead of relying on human input, IRT leverages
bundle filtering methods to iteratively refine the oracle's guidance throughout
training. Experimental results show that RL methods trained with oracle
feedback significantly outperform widely used tractography techniques in terms
of accuracy and anatomical validity.

</details>


### [231] [Langevin Flows for Modeling Neural Latent Dynamics](https://arxiv.org/abs/2507.11531)
*Yue Song,T. Anderson Keller,Yisong Yue,Pietro Perona,Max Welling*

Main category: cs.LG

TL;DR: 本文提出了一种名为LangevinFlow的时间序列变分自编码器，用于捕捉神经群体中的内在动态结构及其未观察到的外部影响，并在多个数据集上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 神经群体表现出驱动时间演变活动的潜在动态结构，需要开发能够同时捕捉内在网络动态和外部未观察到影响的模型。

Method: 提出了LangevinFlow模型，以变分自编码器为核心，结合朗之万方程描述潜变量的时间演变，并引入物理先验及局部耦合振荡器来模拟神经群体中的复杂动力学过程。

Result: 在由Lorenz吸引子生成的合成数据上，模型准确匹配真实发放率；在Neural Latents Benchmark上，其预测精度及神经元激发的对数似然表现优于现有基准；并在行为指标解码中表现良好。

Conclusion: LangevinFlow是一种灵活、高性能且受物理启发的框架，适用于复杂的神经群体动力学建模及其未观察到的影响分析。

Abstract: Neural populations exhibit latent dynamical structures that drive
time-evolving spiking activities, motivating the search for models that capture
both intrinsic network dynamics and external unobserved influences. In this
work, we introduce LangevinFlow, a sequential Variational Auto-Encoder where
the time evolution of latent variables is governed by the underdamped Langevin
equation. Our approach incorporates physical priors -- such as inertia,
damping, a learned potential function, and stochastic forces -- to represent
both autonomous and non-autonomous processes in neural systems. Crucially, the
potential function is parameterized as a network of locally coupled
oscillators, biasing the model toward oscillatory and flow-like behaviors
observed in biological neural populations. Our model features a recurrent
encoder, a one-layer Transformer decoder, and Langevin dynamics in the latent
space. Empirically, our method outperforms state-of-the-art baselines on
synthetic neural populations generated by a Lorenz attractor, closely matching
ground-truth firing rates. On the Neural Latents Benchmark (NLB), the model
achieves superior held-out neuron likelihoods (bits per spike) and forward
prediction accuracy across four challenging datasets. It also matches or
surpasses alternative methods in decoding behavioral metrics such as hand
velocity. Overall, this work introduces a flexible, physics-inspired,
high-performing framework for modeling complex neural population dynamics and
their unobserved influences.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [232] [Tangma: A Tanh-Guided Activation Function with Learnable Parameters](https://arxiv.org/abs/2507.10560)
*Shreel Golwala*

Main category: cs.NE

TL;DR: 提出了一种新的激活函数Tangma，结合双参数可调的双曲正切优势，在MNIST和CIFAR-10上表现出显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决现有激活函数中收敛性能差或梯度消失等问题，并提供更稳定的学习过程。

Method: 设计一种新的激活函数Tangma，结合双曲正切特性，并引入具有学习能力的两个参数（α调整拐点位置，γ增强线性）。通过实验与其他激活函数在MNIST和CIFAR-10数据集上进行对比。

Result: Tangma在MNIST验证精度达99.09%，在CIFAR-10验证精度达到78.15%，并显示出更快的收敛、更稳定的训练和更高的效率。

Conclusion: Tangma具有优越的分类表现和稳定性，其可学习参数为大型模型提供了更灵活的激活行为，适用于图像识别或语言建模等任务。

Abstract: Activation functions are key to effective backpropagation and expressiveness
in deep neural networks. This work introduces Tangma, a new activation function
that combines the smooth shape of the hyperbolic tangent with two learnable
parameters: $\alpha$, which shifts the curve's inflection point to adjust
neuron activation, and $\gamma$, which adds linearity to preserve weak
gradients and improve training stability. Tangma was evaluated on MNIST and
CIFAR-10 using custom networks composed of convolutional and linear layers, and
compared against ReLU, Swish, and GELU. On MNIST, Tangma achieved the highest
validation accuracy of 99.09% and the lowest validation loss, demonstrating
faster and more stable convergence than the baselines. On CIFAR-10, Tangma
reached a top validation accuracy of 78.15%, outperforming all other activation
functions while maintaining a competitive training loss. Tangma also showed
improved training efficiency, with lower average epoch runtimes compared to
Swish and GELU. These results suggest that Tangma performs well on standard
vision tasks and enables reliable, efficient training. Its learnable design
gives more control over activation behavior, which may benefit larger models in
tasks such as image recognition or language modeling.

</details>


### [233] [SFATTI: Spiking FPGA Accelerator for Temporal Task-driven Inference -- A Case Study on MNIST](https://arxiv.org/abs/2507.10561)
*Alessio Caviglia,Filippo Marostica,Alessio Carpegna,Alessandro Savino,Stefano Di Carlo*

Main category: cs.NE

TL;DR: 本文探讨如何利用开源框架Spiker+，生成针对手写数字识别的SNN硬件加速器，并在FPGA上进行低功耗部署。


<details>
  <summary>Details</summary>
Motivation: 硬件加速器对实现边缘应用中的低延迟、节能推理至关重要，作者希望利用SNN的事件驱动特性优化该过程。

Method: 通过Spiker+框架高层次定义网络拓扑、神经元模型和量化策略，自动生成可用于硬件部署的高清晰度描述语言（HDL）。

Result: 对不同配置进行了评估，并分析了边缘计算环境中相关的取舍和优化。

Conclusion: Spiker+能有效地加速SNN硬件开发，展现了其在低功耗边缘计算中的应用潜力。

Abstract: Hardware accelerators are essential for achieving low-latency,
energy-efficient inference in edge applications like image recognition. Spiking
Neural Networks (SNNs) are particularly promising due to their event-driven and
temporally sparse nature, making them well-suited for low-power Field
Programmable Gate Array (FPGA)-based deployment. This paper explores using the
open-source Spiker+ framework to generate optimized SNNs accelerators for
handwritten digit recognition on the MNIST dataset. Spiker+ enables high-level
specification of network topologies, neuron models, and quantization,
automatically generating deployable HDL. We evaluate multiple configurations
and analyze trade-offs relevant to edge computing constraints.

</details>


### [234] [A Biomimetic Way for Coral-Reef-Inspired Swarm Intelligence for Carbon-Neutral Wastewater Treatment](https://arxiv.org/abs/2507.10563)
*Antonis Messinis*

Main category: cs.NE

TL;DR: 提出一种受珊瑚礁启发的群体交互网络，用于碳中和废水处理，效率高，能耗低，碳排放少，并在多种场景中展现了应用潜力。


<details>
  <summary>Details</summary>
Motivation: 应对日益增长的废水处理需求，实现能源中和及更高的成本效益目标。

Method: 利用受珊瑚礁启发的群体交互网络，结合形态生成抽象及多任务碳意识模型，从而实现高效能废水处理的可扩展设计。

Result: 实现了96.7%的去除效率，能耗为0.31kWh每立方米，二氧化碳排放量为14.2g每立方米。在感测漂移情况下展现了稳健性，并在实际场景中节约高达22%的柴油消耗。

Conclusion: 该方法在废水处理中的高效率与低碳排放展现了碳中和及能源节约的巨大潜力，但数据科学技能缺乏和监管限制仍需解决。

Abstract: With increasing wastewater rates, achieving energy-neutral purification is
challenging. We introduce a coral-reef-inspired Swarm Interaction Network for
carbon-neutral wastewater treatment, combining morphogenetic abstraction with
multi-task carbon awareness. Scalability stems from linear token complexity,
mitigating the energy-removal problem. Compared with seven baselines, our
approach achieves 96.7\% removal efficiency, 0.31~kWh~m$^{-3}$ energy
consumption, and 14.2~g~m$^{-3}$ CO$_2$ emissions. Variance analysis
demonstrates robustness under sensor drift. Field scenarios--insular lagoons,
brewery spikes, and desert greenhouses--show potential diesel savings of up to
22\%. However, data-science staffing remains an impediment. Future work will
integrate AutoML wrappers within the project scope, although governance
restrictions pose interpretability challenges that require further visual
analytics.

</details>


### [235] [An Exact Gradient Framework for Training Spiking Neural Networks](https://arxiv.org/abs/2507.10568)
*Arman Ferdowsi,Atakan Aral*

Main category: cs.NE

TL;DR: 提出一种分析型事件驱动学习框架，突出训练突触权重、传输延迟和自适应神经元发射阈值的改进。


<details>
  <summary>Details</summary>
Motivation: 充分利用尖峰神经网络的时间动态特性，尤其是通过引入可训练的突触传输延迟和自适应发射阈值，同时克服现有方法在训练精度、效率以及硬件实现上的局限性。

Method: 开发了一种分析的事件驱动学习框架，通过精确计算损失梯度，优化突触权重、传输延迟及自适应发射阈值，无需依赖离散时间模拟或代理梯度近似。

Result: 在多个基准上实现了精确度提升（最高可达7%）、时间精度和鲁棒性的显著改善。

Conclusion: 所提出的方法展示了先前方法未能实现的新改进，特别是在尖峰神经网络的时间动态利用及硬件实现中的优势。

Abstract: Spiking neural networks inherently rely on the precise timing of discrete
spike events for information processing. Incorporating additional bio-inspired
degrees of freedom, such as trainable synaptic transmission delays and adaptive
firing thresholds, is essential for fully leveraging the temporal dynamics of
SNNs. Although recent methods have demonstrated the benefits of training
synaptic weights and delays, both in terms of accuracy and temporal
representation, these techniques typically rely on discrete-time simulations,
surrogate gradient approximations, or full access to internal state variables
such as membrane potentials. Such requirements limit training precision and
efficiency and pose challenges for neuromorphic hardware implementation due to
increased memory and I/O bandwidth demands. To overcome these challenges, we
propose an analytical event-driven learning framework that computes exact loss
gradients not only with respect to synaptic weights and transmission delays but
also to adaptive neuronal firing thresholds. Experiments on multiple benchmarks
demonstrate significant gains in accuracy (up to 7%), timing precision, and
robustness compared to existing methods.

</details>


### [236] [Grammatical Structure and Grammatical Variations in Non-Metric Iranian Classical Music](https://arxiv.org/abs/2507.10708)
*Maziar Kanani,Sean O Leary,James McDermott*

Main category: cs.NE

TL;DR: 本文引入了一个非度量伊朗古典音乐的符号数据集，以及用于解析这种音乐的结构以及生成变奏的算法，并表明系统能够成功生成变奏。


<details>
  <summary>Details</summary>
Motivation: 探索如何解析和生成非度量的伊朗古典音乐，解决此类非西方音乐的独特结构带来的分析问题，同时为教育与民族音乐学提供工具。

Method: 引入包括MIDI文件和Radif Mirza Abdollah的Dastgah Shour数据表的音乐数据集，使用一种解析旋律结构的算法将每段旋律解析为语法，识别出动机与短句，并通过语法变异生成旋律变奏。

Result: 通过语法变异生成了原旋律的变奏，这些变奏被领域专家较高评价，统计分析也表明算法在不同设定中表现良好。

Conclusion: 本系统能在变异后成功生成令人满意的变奏版本，所提出的方法具有广泛适应性，也可用于阿拉伯或土耳其古典音乐。

Abstract: In this study we introduce a symbolic dataset composed of non-metric Iranian
classical music, and algorithms for structural parsing of this music, and
generation of variations. The corpus comprises MIDI files and data sheets of
Dastgah Shour from Radif Mirza Abdollah, the foundational repertoire of Iranian
classical music. Furthermore, we apply our previously-introduced algorithm for
parsing melodic structure (Kanani et al., 2023b)to the dataset. Unlike much
Western music, this type of non-metric music does not follow bar-centric
organisation. The non-metric organisation can be captured well by our parsing
algorithm. We parse each tune (Gusheh) into a grammar to identify motifs and
phrases. These grammar representations can be useful for educational and
ethnomusicological purposes. We also further develop a previously-introduced
method of creating melodic variations (Kanani et al., 2023b). After parsing an
existing tune to produce a grammar, by applying mutations to this grammar, we
generate a new grammar. Expanding this new version yields a variation of the
original tune. Variations are assessed by a domain-expert listener.
Additionally, we conduct a statistical analysis of mutation with different
representation setups for our parsing and generation algorithms. The
overarching conclusion is that the system successfully produces acceptable
variations post-mutation. While our case study focuses on Iranian classical
music, the methodology can be adapted for Arabic or Turkish classical music.

</details>


### [237] [Biological Processing Units: Leveraging an Insect Connectome to Pioneer Biofidelic Neural Architectures](https://arxiv.org/abs/2507.10951)
*Siyu Yu,Zihan Qin,Tingshan Liu,Beiya Xu,R. Jacob Vogelstein,Jason Brown,Joshua T. Vogelstein*

Main category: cs.NE

TL;DR: 该研究将果蝇幼虫脑的完整神经连接图转换为生物处理单元（BPU），并通过此生物网络模型取得了在多个人工智能测试任务上的优异表现。


<details>
  <summary>Details</summary>
Motivation: 通过生物进化产生的神经回路是否能够支持人工智能是一个未解之谜，因此研究利用果蝇幼虫脑的完整神经连接进行测试。

Method: 将果蝇幼虫脑的神经连接转化为生物处理单元（BPU）模型，直接从突触连接中派生网络，并进行扩展和跨模型实验以验证其在人工智能任务中的性能。

Result: 未经修改的BPU在MNIST数据集上达到了98%的准确率，在CIFAR-10上达到58%。此外，模型扩展后进一步提升结果，并在ChessBench数据集中实现了优越性能，与其它传统方法相比表现突出。

Conclusion: 生物灵感的神经架构展示了支持复杂认知任务的巨大潜力，未来的工作可以扩展到更大和更智能的连接组模型中。

Abstract: The complete connectome of the Drosophila larva brain offers a unique
opportunity to investigate whether biologically evolved circuits can support
artificial intelligence. We convert this wiring diagram into a Biological
Processing Unit (BPU), a fixed recurrent network derived directly from synaptic
connectivity. Despite its modest size 3,000 neurons and 65,000 weights between
them), the unmodified BPU achieves 98% accuracy on MNIST and 58% on CIFAR-10,
surpassing size-matched MLPs. Scaling the BPU via structured connectome
expansions further improves CIFAR-10 performance, while modality-specific
ablations reveal the uneven contributions of different sensory subsystems. On
the ChessBench dataset, a lightweight GNN-BPU model trained on only 10,000
games achieves 60% move accuracy, nearly 10x better than any size transformer.
Moreover, CNN-BPU models with ~2M parameters outperform parameter-matched
Transformers, and with a depth-6 minimax search at inference, reach 91.7%
accuracy, exceeding even a 9M-parameter Transformer baseline. These results
demonstrate the potential of biofidelic neural architectures to support complex
cognitive tasks and motivate scaling to larger and more intelligent connectomes
in future work.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [238] [Latent Space Consistency for Sparse-View CT Reconstruction](https://arxiv.org/abs/2507.11152)
*Duoyou Chen,Yunqing Chen,Can Zhang,Zhou Wang,Cheng Chen,Ruoxiu Xiao*

Main category: eess.IV

TL;DR: 研究提出了一种新的CT重建方法CLS-DM，通过改进的扩散模型实现跨模态特征对比学习，有效提升稀疏X射线图像到3D CT重建效果。


<details>
  <summary>Details</summary>
Motivation: 应对CT扫描中高成本、高辐射和高时间消耗问题，探索稀疏视图X射线图像的重建方法。

Method: 提出了CLS-DM，一种兼具跨模态特征对比学习的扩散模型，以增强2D X射线与3D CT潜在空间的一致性，并进行高效的3D信息重建。

Result: 实验表明，CLS-DM性能超越传统和最先进生成模型，在LIDC-IDRI和CTSpine1K数据集上取得了更优的PSNR和SSIM性能指标。

Conclusion: CLS-DM不仅提升了稀疏X射线重建CT的有效性和经济性，还具有跨模态任务的广泛适用性，为相关领域研究提供了新的思路。

Abstract: Computed Tomography (CT) is a widely utilized imaging modality in clinical
settings. Using densely acquired rotational X-ray arrays, CT can capture 3D
spatial features. However, it is confronted with challenged such as significant
time consumption and high radiation exposure. CT reconstruction methods based
on sparse-view X-ray images have garnered substantial attention from
researchers as they present a means to mitigate costs and risks. In recent
years, diffusion models, particularly the Latent Diffusion Model (LDM), have
demonstrated promising potential in the domain of 3D CT reconstruction.
Nonetheless, due to the substantial differences between the 2D latent
representation of X-ray modalities and the 3D latent representation of CT
modalities, the vanilla LDM is incapable of achieving effective alignment
within the latent space. To address this issue, we propose the Consistent
Latent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature
contrastive learning to efficiently extract latent 3D information from 2D X-ray
images and achieve latent space alignment between modalities. Experimental
results indicate that CLS-DM outperforms classical and state-of-the-art
generative models in terms of standard voxel-level metrics (PSNR, SSIM) on the
LIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing
the effectiveness and economic viability of sparse X-ray reconstructed CT but
can also be generalized to other cross-modal transformation tasks, such as
text-to-image synthesis. We have made our code publicly available at
https://anonymous.4open.science/r/CLS-DM-50D6/ to facilitate further research
and applications in other domains.

</details>


### [239] [3D Magnetic Inverse Routine for Single-Segment Magnetic Field Images](https://arxiv.org/abs/2507.11293)
*J. Senthilnath,Chen Hao,F. C. Wellstood*

Main category: eess.IV

TL;DR: 提出了一种名为3D MIR的新方法，结合了深度学习和物理优化，用于在半导体封装中通过磁场图像（MFI）精确恢复3D电流流动信息，以进行无损检测（NDT）。


<details>
  <summary>Details</summary>
Motivation: 在半导体封装中，精确恢复三维信息对无损检测至关重要，因为这有助于定位电路缺陷。

Method: 通过3D MIR方法，该方法分为三步：CNN模型预测部分参数和分类、使用物理约束估算初始参数值、优化器调整参数值以最小化磁场重建误差。同时结合了深度学习的CNN模型和基于物理的优化策略。

Result: 实验结果表明，3D MIR方法能够高精度地恢复3D信息，并在磁场图像重建领域内成为新标杆。

Conclusion: 3D MIR展现了深度学习与物理优化结合的潜力，为实际应用中的磁场图像重建和无损检测提供新思路。

Abstract: In semiconductor packaging, accurately recovering 3D information is crucial
for non-destructive testing (NDT) to localize circuit defects. This paper
presents a novel approach called the 3D Magnetic Inverse Routine (3D MIR),
which leverages Magnetic Field Images (MFI) to retrieve the parameters for the
3D current flow of a single-segment. The 3D MIR integrates a deep learning
(DL)-based Convolutional Neural Network (CNN), spatial-physics-based
constraints, and optimization techniques. The method operates in three stages:
i) The CNN model processes the MFI data to predict ($\ell/z_o$), where $\ell$
is the wire length and $z_o$ is the wire's vertical depth beneath the magnetic
sensors and classify segment type ($c$). ii) By leveraging
spatial-physics-based constraints, the routine provides initial estimates for
the position ($x_o$, $y_o$, $z_o$), length ($\ell$), current ($I$), and current
flow direction (positive or negative) of the current segment. iii) An optimizer
then adjusts these five parameters ($x_o$, $y_o$, $z_o$, $\ell$, $I$) to
minimize the difference between the reconstructed MFI and the actual MFI. The
results demonstrate that the 3D MIR method accurately recovers 3D information
with high precision, setting a new benchmark for magnetic image reconstruction
in semiconductor packaging. This method highlights the potential of combining
DL and physics-driven optimization in practical applications.

</details>


### [240] [HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging](https://arxiv.org/abs/2507.11325)
*Arefin Ittesafun Abian,Ripon Kumar Debnath,Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Md Rafiqul Islam,Asif Karim,Reem E. Mohamed,Sami Azam*

Main category: eess.IV

TL;DR: 本研究提出了HANS-Net，一个针对腹部CT图像的创新性分割框架，旨在解决肝脏和肿瘤分割中的挑战，取得了卓越的精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 提高肝脏和肿瘤分割精度的重要性，同时克服复杂解剖结构、肿瘤外观差异和标注数据有限等挑战。

Method: HANS-Net结合了超曲面卷积、基于小波的多尺度纹理学习、生物启发的突触可塑性机制、隐式神经表示，以及不确定性感知的蒙特卡罗降维和轻量级时间注意力机制。

Result: HANS-Net在LiTS数据集上取得了Dice分数93.26%，IoU 88.09%，ASSD 0.72 mm，VOE 11.91%的优异表现，并在3D-IRCADb-01数据集上表现出强大的跨数据集泛化能力。

Conclusion: HANS-Net能提供解剖一致、准确且可信的肝脏和肿瘤分割结果，显示其有效性和鲁棒性。

Abstract: Accurate liver and tumor segmentation on abdominal CT images is critical for
reliable diagnosis and treatment planning, but remains challenging due to
complex anatomical structures, variability in tumor appearance, and limited
annotated data. To address these issues, we introduce Hyperbolic-convolutions
Adaptive-temporal-attention with Neural-representation and Synaptic-plasticity
Network (HANS-Net), a novel segmentation framework that synergistically
combines hyperbolic convolutions for hierarchical geometric representation, a
wavelet-inspired decomposition module for multi-scale texture learning, a
biologically motivated synaptic plasticity mechanism for adaptive feature
enhancement, and an implicit neural representation branch to model fine-grained
and continuous anatomical boundaries. Additionally, we incorporate
uncertainty-aware Monte Carlo dropout to quantify prediction confidence and
lightweight temporal attention to improve inter-slice consistency without
sacrificing efficiency. Extensive evaluations of the LiTS dataset demonstrate
that HANS-Net achieves a mean Dice score of 93.26%, an IoU of 88.09%, an
average symmetric surface distance (ASSD) of 0.72 mm, and a volume overlap
error (VOE) of 11.91%. Furthermore, cross-dataset validation on the
3D-IRCADb-01 dataset obtains an average Dice of 87.45%, IoU of 80.30%, ASSD of
1.525 mm, and VOE of 19.71%, indicating strong generalization across different
datasets. These results confirm the effectiveness and robustness of HANS-Net in
providing anatomically consistent, accurate, and confident liver and tumor
segmentation.

</details>
