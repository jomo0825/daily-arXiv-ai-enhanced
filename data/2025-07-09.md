<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 81]
- [cs.CL](#cs.CL) [Total: 57]
- [cs.AI](#cs.AI) [Total: 41]
- [cs.LG](#cs.LG) [Total: 71]
- [cs.NE](#cs.NE) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption 19M)](https://arxiv.org/abs/2507.05300)
*Nicholas Merchant,Haitz Sáez de Ocáriz Borde,Andrei Cristian Popescu,Carlos Garcia Jurado Suarez*

Main category: cs.CV

TL;DR: 通过在训练中引入一致的字幕结构，该研究提升了生成式图文模型的控制能力和文本对齐性能。


<details>
  <summary>Details</summary>
Motivation: 生成式图文模型由于训练数据集的不一致性，难以有效遵守用户输入的提示信息，用户需要大量依赖提示工程。

Method: 提出在训练过程中使用统一的四部分模板字幕（主体、场景、美学、相机细节）构建高质量数据集，并对模型进行微调。

Result: 结构化字幕相比随机字幕，在视觉问答模型的文本-图像对齐分数上表现更优。

Conclusion: 一致的字幕结构训练能够显著提升模型的控制能力和文本对齐性能。

Abstract: We argue that generative text-to-image models often struggle with prompt
adherence due to the noisy and unstructured nature of large-scale datasets like
LAION-5B. This forces users to rely heavily on prompt engineering to elicit
desirable outputs. In this work, we propose that enforcing a consistent caption
structure during training can significantly improve model controllability and
alignment. We introduce Re-LAION-Caption 19M, a high-quality subset of
Re-LAION-5B, comprising 19 million 1024x1024 images with captions generated by
a Mistral 7B Instruct-based LLaVA-Next model. Each caption follows a four-part
template: subject, setting, aesthetics, and camera details. We fine-tune
PixArt-$\Sigma$ and Stable Diffusion 2 using both structured and randomly
shuffled captions, and show that structured versions consistently yield higher
text-image alignment scores using visual question answering (VQA) models. The
dataset is publicly available at
https://huggingface.co/datasets/supermodelresearch/Re-LAION-Caption19M.

</details>


### [2] [CorrDetail: Visual Detail Enhanced Self-Correction for Face Forgery Detection](https://arxiv.org/abs/2507.05302)
*Binjia Zhou,Hengrui Lou,Lizhe Chen,Haoyuan Li,Dawei Luo,Shuai Chen,Jie Lei,Zunlei Feng,Yijun Bei*

Main category: cs.CV

TL;DR: 本文提出了一个名为CorrDetail的框架，用于可解释性面部伪造检测，解决已有方法的辨别能力和误差问题，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成技术的快速发展，面部深度伪造对安全领域构成了重大挑战，现有伪造检测技术在解释力和鲁棒性上存在不足。

Method: 提出了一个视觉细节增强的自校正框架CorrDetail，通过错误引导的问题纠正功能和视觉精细化细节增强模块改进伪造细节检测能力，并设计了融合决策策略以增强模型的辨别力。

Result: 实验结果表明，CorrDetail在性能、伪造细节检测准确性以及泛化能力上均超越现有技术。

Conclusion: CorrDetail提供了解决面部伪造问题的有效方法，不仅性能提升，还能更好地解释和识别伪造细节，可用于增强安全领域的应对能力。

Abstract: With the swift progression of image generation technology, the widespread
emergence of facial deepfakes poses significant challenges to the field of
security, thus amplifying the urgent need for effective deepfake
detection.Existing techniques for face forgery detection can broadly be
categorized into two primary groups: visual-based methods and multimodal
approaches. The former often lacks clear explanations for forgery details,
while the latter, which merges visual and linguistic modalities, is more prone
to the issue of hallucinations.To address these shortcomings, we introduce a
visual detail enhanced self-correction framework, designated CorrDetail, for
interpretable face forgery detection. CorrDetail is meticulously designed to
rectify authentic forgery details when provided with error-guided questioning,
with the aim of fostering the ability to uncover forgery details rather than
yielding hallucinated responses. Additionally, to bolster the reliability of
its findings, a visual fine-grained detail enhancement module is incorporated,
supplying CorrDetail with more precise visual forgery details. Ultimately, a
fusion decision strategy is devised to further augment the model's
discriminative capacity in handling extreme samples, through the integration of
visual information compensation and model bias reduction.Experimental results
demonstrate that CorrDetail not only achieves state-of-the-art performance
compared to the latest methodologies but also excels in accurately identifying
forged details, all while exhibiting robust generalization capabilities.

</details>


### [3] [YOLO-APD: Enhancing YOLOv8 for Robust Pedestrian Detection on Complex Road Geometries](https://arxiv.org/abs/2507.05376)
*Aquino Joctum,John Kandiri*

Main category: cs.CV

TL;DR: 本文提出了一种名为YOLO-APD的新型深度学习架构，旨在解决自动驾驶车辆在复杂道路曲面上的行人检测问题。通过在YOLOv8框架上进行多项改进，模型达到了卓越的检测性能，并在真实时间处理能力方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的基于RGB相机的行人检测方法在复杂几何道路（如Type-S曲面）上存在性能局限，亟需一种更为鲁棒和高效的解决方案以提升自动驾驶的感知性能。

Method: 本文提出的YOLO-APD模型结合了多项新颖的架构改进，包括无参数SimAM注意机制、高效的C3Ghost模块、新型SimSPPF模块、Mish激活函数、以及用于网络特征融合的智能收集与分配(IGD)模块。此外，利用车辆转向动态实现自适应兴趣区域处理也是本文的重要创新点。

Result: 在定制的CARLA数据集上的实验表明，YOLO-APD达到了77.7%的mAP@0.5:0.95以及超过96%的行人召回率，显著优于基础模型YOLOv8。同时，其处理帧率达到100 FPS，验证了高效性与准确性的平衡。消融实验表明各个模块对性能提升具有协同贡献。

Conclusion: YOLO-APD以低成本传感器为基础，显著提升了复杂场景下的行人检测性能，为提高自动驾驶在结构化较低的驾驶环境中的安全性及可靠性做出了重要贡献。

Abstract: Autonomous vehicle perception systems require robust pedestrian detection,
particularly on geometrically complex roadways like Type-S curved surfaces,
where standard RGB camera-based methods face limitations. This paper introduces
YOLO-APD, a novel deep learning architecture enhancing the YOLOv8 framework
specifically for this challenge. YOLO-APD integrates several key architectural
modifications: a parameter-free SimAM attention mechanism, computationally
efficient C3Ghost modules, a novel SimSPPF module for enhanced multi-scale
feature pooling, the Mish activation function for improved optimization, and an
Intelligent Gather & Distribute (IGD) module for superior feature fusion in the
network's neck. The concept of leveraging vehicle steering dynamics for
adaptive region-of-interest processing is also presented. Comprehensive
evaluations on a custom CARLA dataset simulating complex scenarios demonstrate
that YOLO-APD achieves state-of-the-art detection accuracy, reaching 77.7%
mAP@0.5:0.95 and exceptional pedestrian recall exceeding 96%, significantly
outperforming baseline models, including YOLOv8. Furthermore, it maintains
real-time processing capabilities at 100 FPS, showcasing a superior balance
between accuracy and efficiency. Ablation studies validate the synergistic
contribution of each integrated component. Evaluation on the KITTI dataset
confirms the architecture's potential while highlighting the need for domain
adaptation. This research advances the development of highly accurate,
efficient, and adaptable perception systems based on cost-effective sensors,
contributing to enhanced safety and reliability for autonomous navigation in
challenging, less-structured driving environments.

</details>


### [4] [Foreground-aware Virtual Staining for Accurate 3D Cell Morphological Profiling](https://arxiv.org/abs/2507.05383)
*Alexandr A. Kalinin,Paula Llanos,Theresa Maria Sommer,Giovanni Sestini,Xinhai Hou,Jonathan Z. Sexton,Xiang Wan,Ivo D. Dinov,Brian D. Athey,Nicolas Rivron,Anne E. Carpenter,Beth Cimini,Shantanu Singh,Matthew J. O'Meara*

Main category: cs.CV

TL;DR: 提出Spotlight方法，通过引导模型聚焦细胞结构提高虚拟染色效果，特别适用于下游任务。


<details>
  <summary>Details</summary>
Motivation: 目前虚拟染色方法使用平等对待所有像素的损失函数，导致关注不充分，无法有效捕获生物学意义信号。

Method: 提出Spotlight方法，利用基于直方图的前景估计来屏蔽像素损失，并通过软阈值预测计算Dice loss进行形状感知学习。

Result: 在3D基准数据集上，Spotlight改进了形态学表现，同时保留像素级精确度。

Conclusion: Spotlight能提供更好的虚拟染色效果，有助于后续分割及特征分析任务。

Abstract: Microscopy enables direct observation of cellular morphology in 3D, with
transmitted-light methods offering low-cost, minimally invasive imaging and
fluorescence microscopy providing specificity and contrast. Virtual staining
combines these strengths by using machine learning to predict fluorescence
images from label-free inputs. However, training of existing methods typically
relies on loss functions that treat all pixels equally, thus reproducing
background noise and artifacts instead of focusing on biologically meaningful
signals. We introduce Spotlight, a simple yet powerful virtual staining
approach that guides the model to focus on relevant cellular structures.
Spotlight uses histogram-based foreground estimation to mask pixel-wise loss
and to calculate a Dice loss on soft-thresholded predictions for shape-aware
learning. Applied to a 3D benchmark dataset, Spotlight improves morphological
representation while preserving pixel-level accuracy, resulting in virtual
stains better suited for downstream tasks such as segmentation and profiling.

</details>


### [5] [From General to Specialized: The Need for Foundational Models in Agriculture](https://arxiv.org/abs/2507.05390)
*Vishal Nedungadi,Xingguo Xiong,Aike Potze,Ron Van Bree,Tao Lin,Marc Rußwurm,Ioannis N. Athanasiadis*

Main category: cs.CV

TL;DR: 本文探讨了基础模型在农业监测中的潜力与现存不足，提出关于理想农业基础模型的框架并进行了初步评估，建议开发专为农业领域量身定制的基础模型。


<details>
  <summary>Details</summary>
Motivation: 随着人口增长和气候变化加剧，为实现可持续农业生产，迫切需要创新解决方案。虽然基础模型在遥感和气候科学中表现出色，但在与农业相关的挑战中的应用仍然有限。

Method: 本文设计了一个农业基础模型的需求框架（CropFM）并对现有通用基础模型进行对比与评估，实证研究了其在三项典型农业任务中的表现。

Result: 通过实验证明，目前的基础模型在农业领域的有效性有限，无法完全满足农业监测的需求。

Conclusion: 需要开发一个专门针对农业领域的定制化基础模型，以满足农业监测的特殊需求。

Abstract: Food security remains a global concern as population grows and climate change
intensifies, demanding innovative solutions for sustainable agricultural
productivity. Recent advances in foundation models have demonstrated remarkable
performance in remote sensing and climate sciences, and therefore offer new
opportunities for agricultural monitoring. However, their application in
challenges related to agriculture-such as crop type mapping, crop phenology
estimation, and crop yield estimation-remains under-explored. In this work, we
quantitatively evaluate existing foundational models to assess their
effectivity for a representative set of agricultural tasks. From an
agricultural domain perspective, we describe a requirements framework for an
ideal agricultural foundation model (CropFM). We then survey and compare
existing general-purpose foundational models in this framework and empirically
evaluate two exemplary of them in three representative agriculture specific
tasks. Finally, we highlight the need for a dedicated foundational model
tailored specifically to agriculture.

</details>


### [6] [Enhancing Underwater Images Using Deep Learning with Subjective Image Quality Integration](https://arxiv.org/abs/2507.05393)
*Jose M. Montero,Jose-Luis Lisani*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的方法，通过整合人类主观判断来提升水下图像质量，并使用分类器网络和GAN进行增强。


<details>
  <summary>Details</summary>
Motivation: 为了解决水下图像低质量问题，提出深度学习方法以提升图像色彩保真度和清晰度。

Method: 首先用训练分类器网络区分高低质量图像，然后使用生成对抗网络（GAN）按照增强准则改进低质量图像。

Result: 实验结果表明，结合色彩保真度和清晰度等标准，提出的方法能在主观视觉感受和定量指标上明显提升图像质量。

Conclusion: 该研究方法证明了整合人类主观评价和深度学习技术的有效性，可用于改进水下图像质量。

Abstract: Recent advances in deep learning, particularly neural networks, have
significantly impacted a wide range of fields, including the automatic
enhancement of underwater images. This paper presents a deep learning-based
approach to improving underwater image quality by integrating human subjective
assessments into the training process. To this end, we utilize publicly
available datasets containing underwater images labeled by experts as either
high or low quality. Our method involves first training a classifier network to
distinguish between high- and low-quality images. Subsequently, generative
adversarial networks (GANs) are trained using various enhancement criteria to
refine the low-quality images. The performance of the GAN models is evaluated
using quantitative metrics such as PSNR, SSIM, and UIQM, as well as through
qualitative analysis. Results demonstrate that the proposed model --
particularly when incorporating criteria such as color fidelity and image
sharpness -- achieves substantial improvements in both perceived and measured
image quality.

</details>


### [7] [pFedMMA: Personalized Federated Fine-Tuning with Multi-Modal Adapter for Vision-Language Models](https://arxiv.org/abs/2507.05394)
*Sajjad Ghiasvand,Mahnoosh Alizadeh,Ramtin Pedarsani*

Main category: cs.CV

TL;DR: pFedMMA是一种针对视觉语言任务的个性化联邦学习框架，采用多模态适配器和非对称优化策略，兼顾个性化和全局泛化能力，且通信高效。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在零样本及少样本场景中表现出色，但在处理去中心化、异构数据时存在适应能力不足的问题。通过引入多模态适配器和跨模态对齐机制，pFedMMA旨在解决这一问题，提升个性化与泛化之间的平衡。

Method: pFedMMA引入了具有模态特异性上下投影层和全局共享投影层的多模态适配器，并通过非对称优化策略实现个性化数据分布的本地适应及全局投影的协同训练。此外，框架通过仅传输共享部分，实现了通信效率的优化。

Result: pFedMMA在11个包含领域转移和标签偏移的数据集上，展现了在个性化与泛化之间达到最优平衡的能力，并超越了现有的联邦提示调优方法。

Conclusion: pFedMMA证明了多模态适配器和非对称优化策略在提升视觉语言任务上下文中个性化与全局泛化性能的有效性，还具有较高的通信效率。

Abstract: Vision-Language Models (VLMs) like CLIP have demonstrated remarkable
generalization in zero- and few-shot settings, but adapting them efficiently to
decentralized, heterogeneous data remains a challenge. While prompt tuning has
emerged as a popular parameter-efficient approach in personalized federated
learning, existing methods often sacrifice generalization in favor of
personalization, struggling particularly on unseen classes or domains. In this
work, we propose pFedMMA, the first personalized federated learning framework
that leverages multi-modal adapters for vision-language tasks. Each adapter
contains modality-specific up- and down-projection layers alongside a globally
shared projection that aligns cross-modal features. Our asymmetric optimization
strategy allows clients to locally adapt to personalized data distributions
while collaboratively training the shared projection to improve global
generalization. This design is also communication-efficient, as only the shared
component is exchanged during rounds. Through extensive experiments across
eleven datasets, including domain- and label-shift scenarios, we show that
pFedMMA achieves state-of-the-art trade-offs between personalization and
generalization, outperforming recent federated prompt tuning methods. The code
is available at https://github.com/sajjad-ucsb/pFedMMA.

</details>


### [8] [Neural-Driven Image Editing](https://arxiv.org/abs/2507.05397)
*Pengfei Zhou,Jie Xia,Xiaopeng Peng,Wangbo Zhao,Zilong Ye,Zekai Li,Suorong Yang,Jiadong Pan,Yuanxiang Chen,Ziqiao Wang,Kai Wang,Qian Zheng,Xiaojun Chang,Gang Pan,Shurong Dong,Kaipeng Zhang,Yang You*

Main category: cs.CV

TL;DR: 本文提出了一种结合脑机接口（BCIs）和生成模型的免手动图像编辑方法LoongX，使用多模态神经信号驱动，实现了与文本驱动方法相当甚至更优的性能。


<details>
  <summary>Details</summary>
Motivation: 传统图像编辑方法对手动提示依赖度高，对运动能力或语言能力有限的用户不够友好，该领域需要更直观和兼容性更强的编辑方式。

Method: LoongX利用多模态神经信号（EEG、fNIRS、PPG、运动信号）捕捉用户意图，并通过CS3模块提取模态特定特征，DGF模块融合特征并对编辑语义进行对齐，同时采用对比学习将认知状态与语义意图对齐。

Result: 实验表明，LoongX的性能（CLIP-I: 0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636）与文本驱动方法相当，并在结合语音信号时超越后者（CLIP-T: 0.2588 vs. 0.2549）。

Conclusion: LoongX展示了神经驱动生成模型在图像编辑中的潜力，为认知驱动创意技术开辟了新方向。

Abstract: Traditional image editing typically relies on manual prompting, making it
labor-intensive and inaccessible to individuals with limited motor control or
language abilities. Leveraging recent advances in brain-computer interfaces
(BCIs) and generative models, we propose LoongX, a hands-free image editing
approach driven by multimodal neurophysiological signals. LoongX utilizes
state-of-the-art diffusion models trained on a comprehensive dataset of 23,928
image editing pairs, each paired with synchronized electroencephalography
(EEG), functional near-infrared spectroscopy (fNIRS), photoplethysmography
(PPG), and head motion signals that capture user intent. To effectively address
the heterogeneity of these signals, LoongX integrates two key modules. The
cross-scale state space (CS3) module encodes informative modality-specific
features. The dynamic gated fusion (DGF) module further aggregates these
features into a unified latent space, which is then aligned with edit semantics
via fine-tuning on a diffusion transformer (DiT). Additionally, we pre-train
the encoders using contrastive learning to align cognitive states with semantic
intentions from embedded natural language. Extensive experiments demonstrate
that LoongX achieves performance comparable to text-driven methods (CLIP-I:
0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636) and outperforms them when neural
signals are combined with speech (CLIP-T: 0.2588 vs. 0.2549). These results
highlight the promise of neural-driven generative models in enabling
accessible, intuitive image editing and open new directions for
cognitive-driven creative technologies. Datasets and code will be released to
support future work and foster progress in this emerging area.

</details>


### [9] [SoftReMish: A Novel Activation Function for Enhanced Convolutional Neural Networks for Visual Recognition Performance](https://arxiv.org/abs/2507.06148)
*Mustafa Bayram Gücen*

Main category: cs.CV

TL;DR: 本文提出了一种新的激活函数SoftReMish，在MNIST数据集上的图像分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索能够提升CNNs在图像分类任务中表现的激活函数。

Method: 构建一个标准的CNN架构，比较SoftReMish与ReLU、Tanh和Mish，通过替换所有可训练层的激活函数并进行评估。

Result: SoftReMish在训练损失和验证准确性上优于其他激活函数，分别达到3.14e-8和99.41%。

Conclusion: SoftReMish展现出更好的收敛行为和泛化能力，适合视觉识别任务。

Abstract: In this study, SoftReMish, a new activation function designed to improve the
performance of convolutional neural networks (CNNs) in image classification
tasks, is proposed. Using the MNIST dataset, a standard CNN architecture
consisting of two convolutional layers, max pooling, and fully connected layers
was implemented. SoftReMish was evaluated against popular activation functions
including ReLU, Tanh, and Mish by replacing the activation function in all
trainable layers. The model performance was assessed in terms of minimum
training loss and maximum validation accuracy. Results showed that SoftReMish
achieved a minimum loss (3.14e-8) and a validation accuracy (99.41%),
outperforming all other functions tested. These findings demonstrate that
SoftReMish offers better convergence behavior and generalization capability,
making it a promising candidate for visual recognition tasks.

</details>


### [10] [Motion Generation: A Survey of Generative Approaches and Benchmarks](https://arxiv.org/abs/2507.05419)
*Aliasghar Khani,Arianna Rampini,Bruno Roy,Larasika Nadela,Noa Kaplan,Evan Atherton,Derek Cheung,Jacky Bibliowicz*

Main category: cs.CV

TL;DR: 本文综述了自2023年以来在运动生成领域的最新进展，从生成策略的角度对方法进行分类，并分析架构原则、条件机制、生成设置和评估标准。


<details>
  <summary>Details</summary>
Motivation: 运动生成领域的快速发展及多样化方法的引入（GANs、自编码器等），需要一个从生成方法视角系统回顾的综述，以帮助研究人员和从业者更明确地比较进展并识别当前挑战。

Method: 作者从生成策略的角度对方法进行了系统分类，并重点分析了顶级论文中的架构设计、条件机制和生成设置，同时总结了评估指标和使用的数据集。

Result: 提供了一份详尽且结构化的运动生成方法综述，明确领域中存在的开放性挑战，为未来研究提供参考。

Conclusion: 这篇综述为运动生成领域提供了关键性的参考资料，有助于研究者理解最新技术的特点、比较优劣势，并明晰未来研究方向。

Abstract: Motion generation, the task of synthesizing realistic motion sequences from
various conditioning inputs, has become a central problem in computer vision,
computer graphics, and robotics, with applications ranging from animation and
virtual agents to human-robot interaction. As the field has rapidly progressed
with the introduction of diverse modeling paradigms including GANs,
autoencoders, autoregressive models, and diffusion-based techniques, each
approach brings its own advantages and limitations. This growing diversity has
created a need for a comprehensive and structured review that specifically
examines recent developments from the perspective of the generative approach
employed.
  In this survey, we provide an in-depth categorization of motion generation
methods based on their underlying generative strategies. Our main focus is on
papers published in top-tier venues since 2023, reflecting the most recent
advancements in the field. In addition, we analyze architectural principles,
conditioning mechanisms, and generation settings, and compile a detailed
overview of the evaluation metrics and datasets used across the literature. Our
objective is to enable clearer comparisons and identify open challenges,
thereby offering a timely and foundational reference for researchers and
practitioners navigating the rapidly evolving landscape of motion generation.

</details>


### [11] [Mastering Regional 3DGS: Locating, Initializing, and Editing with Diverse 2D Priors](https://arxiv.org/abs/2507.05426)
*Lanqing Guo,Yufei Wang,Hezhen Hu,Yan Zheng,Yeying Jin,Siyu Huang,Zhangyang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于2D扩散编辑和逆渲染的方法，能够在3D场景中实现高效的局部编辑，同时提升编辑的一致性和细节。


<details>
  <summary>Details</summary>
Motivation: 在3D场景编辑中，局部修改往往比整个场景的全局编辑更为常见。然而，由于3D语义解析能力弱于2D，导致在3D场景中实现高保真的局部编辑具有挑战性。

Method: 作者借助2D扩散编辑识别每个视图中的修改区域，通过逆渲染实现3D定位，结合2D基础模型预测的深度图初始化粗糙的3D点，并通过迭代的视图一致性编辑逐步增强结构细节和纹理。

Result: 实验表明，该方法在局部3D场景编辑上实现了最先进的性能，同时提供了高达4倍的速度提升。

Conclusion: 这项研究提出了一种更高效、更有效的3D局部场景编辑方法，显著提升了操作的一致性和细节完整性。

Abstract: Many 3D scene editing tasks focus on modifying local regions rather than the
entire scene, except for some global applications like style transfer, and in
the context of 3D Gaussian Splatting (3DGS), where scenes are represented by a
series of Gaussians, this structure allows for precise regional edits, offering
enhanced control over specific areas of the scene; however, the challenge lies
in the fact that 3D semantic parsing often underperforms compared to its 2D
counterpart, making targeted manipulations within 3D spaces more difficult and
limiting the fidelity of edits, which we address by leveraging 2D diffusion
editing to accurately identify modification regions in each view, followed by
inverse rendering for 3D localization, then refining the frontal view and
initializing a coarse 3DGS with consistent views and approximate shapes derived
from depth maps predicted by a 2D foundation model, thereby supporting an
iterative, view-consistent editing process that gradually enhances structural
details and textures to ensure coherence across perspectives. Experiments
demonstrate that our method achieves state-of-the-art performance while
delivering up to a $4\times$ speedup, providing a more efficient and effective
approach to 3D scene local editing.

</details>


### [12] [OpenWorldSAM: Extending SAM2 for Universal Image Segmentation with Language Prompts](https://arxiv.org/abs/2507.05427)
*Shiting Xiao,Rishabh Kabra,Yuhang Li,Donghyun Lee,Joao Carreira,Priyadarshini Panda*

Main category: cs.CV

TL;DR: 提出OpenWorldSAM，实现对文字语义到空间区域的精确映射，解决开放词汇物体分割问题。


<details>
  <summary>Details</summary>
Motivation: 解决基于开放性语言提示的物体分割问题，需模型将文本语义映射到精确的空间区域。

Method: 将SAM2扩展到开放词汇场景，集成轻量级视觉-语言模型的多模态嵌入，并以训练高效的方式增强实例感知与泛化能力。

Result: 在多个基准（如ADE20k、PASCAL等）上达到开放词汇情境下的分割任务的最新性能。

Conclusion: OpenWorldSAM通过灵活的提示支持、高资源效率、实例意识与优异的泛化能力，展示了其在开放词汇分割中的优越表现。

Abstract: The ability to segment objects based on open-ended language prompts remains a
critical challenge, requiring models to ground textual semantics into precise
spatial masks while handling diverse and unseen categories. We present
OpenWorldSAM, a framework that extends the prompt-driven Segment Anything Model
v2 (SAM2) to open-vocabulary scenarios by integrating multi-modal embeddings
extracted from a lightweight vision-language model (VLM). Our approach is
guided by four key principles: i) Unified prompting: OpenWorldSAM supports a
diverse range of prompts, including category-level and sentence-level language
descriptions, providing a flexible interface for various segmentation tasks.
ii) Efficiency: By freezing the pre-trained components of SAM2 and the VLM, we
train only 4.5 million parameters on the COCO-stuff dataset, achieving
remarkable resource efficiency. iii) Instance Awareness: We enhance the model's
spatial understanding through novel positional tie-breaker embeddings and
cross-attention layers, enabling effective segmentation of multiple instances.
iv) Generalization: OpenWorldSAM exhibits strong zero-shot capabilities,
generalizing well on unseen categories and an open vocabulary of concepts
without additional training. Extensive experiments demonstrate that
OpenWorldSAM achieves state-of-the-art performance in open-vocabulary semantic,
instance, and panoptic segmentation across multiple benchmarks, including
ADE20k, PASCAL, ScanNet, and SUN-RGBD.

</details>


### [13] [Robotic System with AI for Real Time Weed Detection, Canopy Aware Spraying, and Droplet Pattern Evaluation](https://arxiv.org/abs/2507.05432)
*Inayat Rasool,Pappu Kumar Yadav,Amee Parmar,Hasan Mirzakhaninafchi,Rikesh Budhathoki,Zain Ul Abideen Usmani,Supriya Paudel,Ivan Perez Olivera,Eric Jone*

Main category: cs.CV

TL;DR: 该研究提出了一种基于视觉引导和AI驱动的可变速率喷雾系统，用于精准的杂草检测和喷雾控制，可降低农药使用量，减少环境污染及抗药性杂草的出现。


<details>
  <summary>Details</summary>
Motivation: 现代农业中统一和过量的除草剂应用导致了成本上升、环境污染及抗药性杂草的出现，急需一种精准、高效、低成本的解决方案。

Method: 通过YOLO11n和YOLO11n-seg深度学习模型进行杂草检测和冠层估算，结合NVIDIA Jetson Orin Nano和Arduino控制电磁阀喷嘴，实现实时喷雾控制。试验在室内采用15株不同冠层大小的木槿植物模拟杂草场景进行验证。

Result: YOLO11n模型 mAP@50 达到0.98，精确度0.99，召回接近1.0。YOLO11n-seg模型 mAP@50 为0.48，精确度0.55，召回0.52。水敏感纸验证显示喷雾覆盖率与冠层大小呈正相关，说明系统可以根据冠层大小实时调整喷雾输出。

Conclusion: 该系统证明了将实时深度学习与低成本嵌入式硬件相结合的潜力，有助于实现精准除草剂喷洒，减少环境污染和成本。下一步将扩展检测种类并在大田试验中进一步验证。

Abstract: Uniform and excessive herbicide application in modern agriculture contributes
to increased input costs, environmental pollution, and the emergence of
herbicide resistant weeds. To address these challenges, we developed a vision
guided, AI-driven variable rate sprayer system capable of detecting weed
presence, estimating canopy size, and dynamically adjusting nozzle activation
in real time. The system integrates lightweight YOLO11n and YOLO11n-seg deep
learning models, deployed on an NVIDIA Jetson Orin Nano for onboard inference,
and uses an Arduino Uno-based relay interface to control solenoid actuated
nozzles based on canopy segmentation results. Indoor trials were conducted
using 15 potted Hibiscus rosa sinensis plants of varying canopy sizes to
simulate a range of weed patch scenarios. The YOLO11n model achieved a mean
average precision (mAP@50) of 0.98, with a precision of 0.99 and a recall close
to 1.0. The YOLO11n-seg segmentation model achieved a mAP@50 of 0.48, precision
of 0.55, and recall of 0.52. System performance was validated using water
sensitive paper, which showed an average spray coverage of 24.22% in zones
where canopy was present. An upward trend in mean spray coverage from 16.22%
for small canopies to 21.46% and 21.65% for medium and large canopies,
respectively, demonstrated the system's capability to adjust spray output based
on canopy size in real time. These results highlight the potential of combining
real time deep learning with low-cost embedded hardware for selective herbicide
application. Future work will focus on expanding the detection capabilities to
include three common weed species in South Dakota: water hemp (Amaranthus
tuberculatus), kochia (Bassia scoparia), and foxtail (Setaria spp.), followed
by further validation in both indoor and field trials within soybean and corn
production systems.

</details>


### [14] [Driving as a Diagnostic Tool: Scenario-based Cognitive Assessment in Older Drivers From Driving Video](https://arxiv.org/abs/2507.05463)
*Md Zahid Hasan,Guillermo Basulto-Elias,Jun Ha Chang,Sahuna Hallmark,Matthew Rizzo,Anuj Sharma,Soumik Sarkar*

Main category: cs.CV

TL;DR: 研究提出利用自然驾驶视频和大规模视觉模型识别老年驾驶者的认知状态，旨在通过分析驱动行为数字化特征，早期检测认知能力下降。


<details>
  <summary>Details</summary>
Motivation: 当前阿尔茨海默病（AD）及轻度认知障碍（MCI）等认知衰退疾病的诊断方式耗时高、成本高，导致诊断不足。为解决这一问题，研究希望通过驾驶行为分析找到与认知衰退相关的数字特征，实现早期诊断。

Method: 通过自然驾驶视频和现代大规模视觉模型的结合，分析老年驾驶者的驾驶行为，构建框架进行认知状态分类及疾病进展预测。

Result: 研究显示，驾驶行为与驾驶者的认知状态密切相关，车辆可作为诊断工具用于识别功能性障碍早期信号。

Conclusion: 研究为非侵入性、可扩展的监控系统提供支持，推动认知能力下降的早期检测及干预策略，减轻社会和经济负担。

Abstract: We introduce scenario-based cognitive status identification in older drivers
from Naturalistic driving videos and large vision models. In recent times,
cognitive decline, including Alzheimer's disease (AD) and mild cognitive
impairment (MCI), is often underdiagnosed due to the time-consuming and costly
nature of current diagnostic methods. By analyzing real-world driving behavior
captured through in-vehicle systems, this research aims to extract "digital
fingerprints" that correlate with functional decline and clinical features of
MCI and AD. Moreover, modern large vision models can draw meaningful insights
from everyday driving patterns of older patients to early detect cognitive
decline. We propose a framework that uses large vision models and naturalistic
driving videos to analyze driver behavior, classify cognitive status and
predict disease progression. We leverage the strong relationship between
real-world driving behavior as an observation of the current cognitive status
of the drivers where the vehicle can be utilized as a "diagnostic tool". Our
method identifies early warning signs of functional impairment, contributing to
proactive intervention strategies. This work enhances early detection and
supports the development of scalable, non-invasive monitoring systems to
mitigate the growing societal and economic burden of cognitive decline in the
aging population.

</details>


### [15] [Cloud Diffusion Part 1: Theory and Motivation](https://arxiv.org/abs/2507.05496)
*Andrew Randono*

Main category: cs.CV

TL;DR: 该论文提出了一种称为'云扩散模型'的新型扩散模型，用于图像生成，用以替代传统使用白噪声的扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型采用白噪声进行训练，而自然图像的低阶统计特性呈现尺度不变性，具有较大的相关性和不同分布。因此，研究者希望开发更贴近自然图像分布的扩散模型。

Method: 用一种基于尺度不变噪声分布的模型取代传统的白噪声分布，称之为'云扩散模型'，应用于扩散过程。

Result: 提出了云扩散模型的概念，强调其可能提升推断速度，改进高频细节，并提供更大的控制能力。

Conclusion: 云扩散模型将自然图像的尺度不变特性纳入其中，潜在改善了推断性能和图像生成质量，将在后续研究中进行实证验证并与传统模型进行对比。

Abstract: Diffusion models for image generation function by progressively adding noise
to an image set and training a model to separate out the signal from the noise.
The noise profile used by these models is white noise -- that is, noise based
on independent normal distributions at each point whose mean and variance is
independent of the scale. By contrast, most natural image sets exhibit a type
of scale invariance in their low-order statistical properties characterized by
a power-law scaling. Consequently, natural images are closer (in a quantifiable
sense) to a different probability distribution that emphasizes large scale
correlations and de-emphasizes small scale correlations. These scale invariant
noise profiles can be incorporated into diffusion models in place of white
noise to form what we will call a ``Cloud Diffusion Model". We argue that these
models can lead to faster inference, improved high-frequency details, and
greater controllability. In a follow-up paper, we will build and train a Cloud
Diffusion Model that uses scale invariance at a fundamental level and compare
it to classic, white noise diffusion models.

</details>


### [16] [LoomNet: Enhancing Multi-View Image Generation via Latent Space Weaving](https://arxiv.org/abs/2507.05499)
*Giulio Federico,Fabio Carrara,Claudio Gennaro,Giuseppe Amato,Marco Di Benedetto*

Main category: cs.CV

TL;DR: 本文提出LoomNet，一种能够从单张图片生成一致性多视图图像的新型架构，重点解决空间一致性问题，从而提升3D表面重建质量。


<details>
  <summary>Details</summary>
Motivation: 在多视图图像生成中，空间一致性缺乏会影响最终的3D网格质量，亟需一种改进方法以生成连贯的视图。

Method: 设计了一种名为LoomNet的多视图扩散架构，通过并行应用扩散模型形成共享潜在空间，以保持视图一致性；采用编码投影至三正交平面并融合处理生成统一解释，最终用于渲染一致的多视图图像。

Result: LoomNet在15秒内生成了16张高质量且一致的视图，在图像质量和重建指标上超越了现有最优方法，并展现出创造性，多样地生成了不同的可行新视图。

Conclusion: LoomNet有效地提升了多视图生成的空间一致性，为3D表面重建提供了更好的基础，且具有生成多样性与高效率的特性。

Abstract: Generating consistent multi-view images from a single image remains
challenging. Lack of spatial consistency often degrades 3D mesh quality in
surface reconstruction. To address this, we propose LoomNet, a novel multi-view
diffusion architecture that produces coherent images by applying the same
diffusion model multiple times in parallel to collaboratively build and
leverage a shared latent space for view consistency. Each viewpoint-specific
inference generates an encoding representing its own hypothesis of the novel
view from a given camera pose, which is projected onto three orthogonal planes.
For each plane, encodings from all views are fused into a single aggregated
plane. These aggregated planes are then processed to propagate information and
interpolate missing regions, combining the hypotheses into a unified, coherent
interpretation. The final latent space is then used to render consistent
multi-view images. LoomNet generates 16 high-quality and coherent views in just
15 seconds. In our experiments, LoomNet outperforms state-of-the-art methods on
both image quality and reconstruction metrics, also showing creativity by
producing diverse, plausible novel views from the same input.

</details>


### [17] [Llama Nemoretriever Colembed: Top-Performing Text-Image Retrieval Model](https://arxiv.org/abs/2507.05513)
*Mengyao Xu,Gabriel Moreira,Ronay Ak,Radek Osmulski,Yauhen Babakhin,Zhiding Yu,Benedikt Schifferer,Even Oldridge*

Main category: cs.CV

TL;DR: 本文提出了一种名为llama-nemoretriever-colembed的文本-图像检索模型，提供了跨模态检索的最优性能。


<details>
  <summary>Details</summary>
Motivation: 随着跨模态检索需求的增长，迫切需要一种统一的高性能文本-图像检索模型。

Method: 使用NVIDIA Eagle2视图-语言模型，并通过替换因果注意力为双向注意力、整合ColBERT机制构建嵌入空间，同时采用两阶段训练策略以提升检索能力。

Result: 3B模型在ViDoRe V1和V2上分别取得了91.0和63.5的NDCG@5分数，均位居榜首。

Conclusion: 提出的模型在多个基准数据集上达到了最新性能，但在存储与效率上需要权衡。

Abstract: Motivated by the growing demand for retrieval systems that operate across
modalities, we introduce llama-nemoretriever-colembed, a unified text-image
retrieval model that delivers state-of-the-art performance across multiple
benchmarks. We release two model variants, 1B and 3B. The 3B model achieves
state of the art performance, scoring NDCG@5 91.0 on ViDoRe V1 and 63.5 on
ViDoRe V2, placing first on both leaderboards as of June 27, 2025.
  Our approach leverages the NVIDIA Eagle2 Vision-Language model (VLM),
modifies its architecture by replacing causal attention with bidirectional
attention, and integrates a ColBERT-style late interaction mechanism to enable
fine-grained multimodal retrieval in a shared embedding space. While this
mechanism delivers superior retrieval accuracy, it introduces trade-offs in
storage and efficiency. We provide a comprehensive analysis of these
trade-offs. Additionally, we adopt a two-stage training strategy to enhance the
model's retrieval capabilities.

</details>


### [18] [Simulating Refractive Distortions and Weather-Induced Artifacts for Resource-Constrained Autonomous Perception](https://arxiv.org/abs/2507.05536)
*Moseli Mots'oehli,Feimei Chen,Hok Wai Chan,Itumeleng Tlali,Thulani Babeli,Kyungim Baek,Huaijin Chen*

Main category: cs.CV

TL;DR: 该论文解决了非洲地区自动驾驶数据集匮乏的问题，提出了一种程序化增强方法，生成适合低资源环境的逼真数据集，以支持后续研究。


<details>
  <summary>Details</summary>
Motivation: 开发针对非洲城市和乡村道路的自动驾驶数据在低资源环境中具有挑战性，为了支持这一研究，提出了一种便宜而有效的增强方法，模拟实际驾驶场景的复杂性。

Method: 设计了一种程序化增强流水线，包括折射效果模块（光学镜头失真、空气湍流等模拟）和天气模块（雾、镜头光晕等）。

Result: 提供了增强数据集基线性能测试，证明了方法的有效性，并公开了相关工具包和基准测试数据。

Conclusion: 该方法无需昂贵的数据采集和标注，为低资源技术环境中的自动驾驶感知研究提供了重要支持。

Abstract: The scarcity of autonomous vehicle datasets from developing regions,
particularly across Africa's diverse urban, rural, and unpaved roads, remains a
key obstacle to robust perception in low-resource settings. We present a
procedural augmentation pipeline that enhances low-cost monocular dashcam
footage with realistic refractive distortions and weather-induced artifacts
tailored to challenging African driving scenarios. Our refractive module
simulates optical effects from low-quality lenses and air turbulence, including
lens distortion, Perlin noise, Thin-Plate Spline (TPS), and divergence-free
(incompressible) warps. The weather module adds homogeneous fog, heterogeneous
fog, and lens flare. To establish a benchmark, we provide baseline performance
using three image restoration models. To support perception research in
underrepresented African contexts, without costly data collection, labeling, or
simulation, we release our distortion toolkit, augmented dataset splits, and
benchmark results.

</details>


### [19] [ReLayout: Integrating Relation Reasoning for Content-aware Layout Generation with Multi-modal Large Language Models](https://arxiv.org/abs/2507.05568)
*Jiaxu Tian,Xuehui Yu,Yaoxing Wang,Pan Wang,Guangqian Guo,Shan Gao*

Main category: cs.CV

TL;DR: ReLayout通过引入关系链式推理和布局原型重新平衡采样机制，提升了设计布局的合理性、美观性和解释性，超越了现有的基准方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型（LLMs）的设计布局生成方法难以充分解释视觉主题和设计元素之间的空间关系，导致生成结果存在结构性和多样性问题。

Method: ReLayout通过引入关系链式推理（relation-CoT）和增强布局注释使布局分解为更小、更结构化和递归的布局，同时加入布局原型重新平衡采样机制，从而定义布局原型特性并解决因数据偏差引起的均匀性问题。

Result: 实验结果表明，ReLayout相比基准方法生成的布局在结构性、多样性与人类美学的契合度上表现更优，且具有更强的解释性。

Conclusion: ReLayout采用创新的方法生成更美观、合理且具备解释力的布局，成功克服了现有方法的缺陷并显著提升了布局生成效果。

Abstract: Content-aware layout aims to arrange design elements appropriately on a given
canvas to convey information effectively. Recently, the trend for this task has
been to leverage large language models (LLMs) to generate layouts
automatically, achieving remarkable performance. However, existing LLM-based
methods fail to adequately interpret spatial relationships among visual themes
and design elements, leading to structural and diverse problems in layout
generation. To address this issue, we introduce ReLayout, a novel method that
leverages relation-CoT to generate more reasonable and aesthetically coherent
layouts by fundamentally originating from design concepts. Specifically, we
enhance layout annotations by introducing explicit relation definitions, such
as region, salient, and margin between elements, with the goal of decomposing
the layout into smaller, structured, and recursive layouts, thereby enabling
the generation of more structured layouts. Furthermore, based on these defined
relationships, we introduce a layout prototype rebalance sampler, which defines
layout prototype features across three dimensions and quantifies distinct
layout styles. This sampler addresses uniformity issues in generation that
arise from data bias in the prototype distribution balance process. Extensive
experimental results verify that ReLayout outperforms baselines and can
generate structural and diverse layouts that are more aligned with human
aesthetics and more explainable.

</details>


### [20] [Multi-Modal Face Anti-Spoofing via Cross-Modal Feature Transitions](https://arxiv.org/abs/2507.05575)
*Jun-Xiong Chong,Fang-Yu Hsu,Ming-Tsung Hsu,Yi-Ting Lin,Kai-Heng Chien,Chiou-Ting Hsu,Pei-Kai Huang*

Main category: cs.CV

TL;DR: 该论文提出了跨模态过渡引导网络（CTNet）用于解决多模态面部反欺诈任务中的分布差异和欠缺模态问题，通过学习一致和不一致的模态特征转变，实现更广泛的特征空间及对抗分布外攻击。实验表明CTNet的表现优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 传统多模态面部反欺诈技术在训练和测试域之间存在显著的分布差异，并且推理阶段面临模态缺失的挑战，因此需要一种更鲁棒的解决方案。

Method: 提出跨模态过渡引导网络（CTNet），通过学习活体样本之间一致的跨模态特征过渡，构建通用特征空间，同时通过学习活体与攻击样本之间不一致的跨模态特征过渡，检测分布外攻击。此外，利用RGB模态预测红外和深度模态的辅助特征以应对模态缺失。

Result: 通过多种协议的实验结果证明，CTNet在多模态面部反欺诈任务中比之前的双类方法表现更优。

Conclusion: CTNet有效解决了多模态面部反欺诈中的分布差异和模态缺失问题，提升了系统对分布外攻击的鲁棒性。

Abstract: Multi-modal face anti-spoofing (FAS) aims to detect genuine human presence by
extracting discriminative liveness cues from multiple modalities, such as RGB,
infrared (IR), and depth images, to enhance the robustness of biometric
authentication systems. However, because data from different modalities are
typically captured by various camera sensors and under diverse environmental
conditions, multi-modal FAS often exhibits significantly greater distribution
discrepancies across training and testing domains compared to single-modal FAS.
Furthermore, during the inference stage, multi-modal FAS confronts even greater
challenges when one or more modalities are unavailable or inaccessible. In this
paper, we propose a novel Cross-modal Transition-guided Network (CTNet) to
tackle the challenges in the multi-modal FAS task. Our motivation stems from
that, within a single modality, the visual differences between live faces are
typically much smaller than those of spoof faces. Additionally, feature
transitions across modalities are more consistent for the live class compared
to those between live and spoof classes. Upon this insight, we first propose
learning consistent cross-modal feature transitions among live samples to
construct a generalized feature space. Next, we introduce learning the
inconsistent cross-modal feature transitions between live and spoof samples to
effectively detect out-of-distribution (OOD) attacks during inference. To
further address the issue of missing modalities, we propose learning
complementary infrared (IR) and depth features from the RGB modality as
auxiliary modalities. Extensive experiments demonstrate that the proposed CTNet
outperforms previous two-class multi-modal FAS methods across most protocols.

</details>


### [21] [Semi-Supervised Defect Detection via Conditional Diffusion and CLIP-Guided Noise Filtering](https://arxiv.org/abs/2507.05588)
*Shuai Li,Shihan Chen,Wanru Geng,Zhaohua Xu,Xiaolu Liu,Can Dong,Zhen Tian,Changlin Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于条件扩散的半监督缺陷检测框架“DSYM”，通过两阶段协作训练机制和分阶段联合优化策略，在标签数据利用效率上表现优异。


<details>
  <summary>Details</summary>
Motivation: 工业质量检测，特别是在汽车零部件、航空航天及医疗设备等高精度、安全相关领域中，传统方法的低效性、高成本以及有限的鲁棒性推助了新方法的需求。

Method: 采用条件扩散模型生成多尺度伪缺陷样本，结合CLIP跨模态特征的噪声过滤机制，通过伪标签引入无标数据，利用两阶段协作训练机制与分阶段联合优化策略。

Result: 在NEU-DET数据集上的实验结果显示，与传统监督方法相比，该框架在相同标注数据量下达到了78.4%的mAP@0.5指标，而仅使用40%标注数据时达到75.1%的mAP@0.5，表现出显著的数据效率优势。

Conclusion: 该研究为工业质量检测提供了一种高精度且依赖更少标注数据的解决方案，并实现了开源。

Abstract: In the realm of industrial quality inspection, defect detection stands as a
critical component, particularly in high-precision, safety-critical sectors
such as automotive components aerospace, and medical devices. Traditional
methods, reliant on manual inspection or early image processing algorithms,
suffer from inefficiencies, high costs, and limited robustness. This paper
introduces a semi-supervised defect detection framework based on conditional
diffusion (DSYM), leveraging a two-stage collaborative training mechanism and a
staged joint optimization strategy. The framework utilizes labeled data for
initial training and subsequently incorporates unlabeled data through the
generation of pseudo-labels. A conditional diffusion model synthesizes
multi-scale pseudo-defect samples, while a CLIP cross-modal feature-based noise
filtering mechanism mitigates label contamination. Experimental results on the
NEU-DET dataset demonstrate a 78.4% mAP@0.5 with the same amount of labeled
data as traditional supervised methods, and 75.1% mAP@0.5 with only 40% of the
labeled data required by the original supervised model, showcasing significant
advantages in data efficiency. This research provides a high-precision,
low-labeling-dependent solution for defect detection in industrial quality
inspection scenarios. The work of this article has been open-sourced at
https://github.com/cLin-c/Semisupervised-DSYM.

</details>


### [22] [GSVR: 2D Gaussian-based Video Representation for 800+ FPS with Hybrid Deformation Field](https://arxiv.org/abs/2507.05594)
*Zhizhuo Pang,Zhihui Ke,Xiaobo Zhou,Tie Qiu*

Main category: cs.CV

TL;DR: 论文提出了一个名为GSVR的新方法，用于视频的高效表示和解码，与现有方法相比，显著提升了训练速度和解码速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于卷积网络的视频表示方法在重建质量上表现良好，但解码速度慢且训练耗时长，亟需通过优化方法解决这一问题。

Method: 提出基于2D高斯的新型视频表示方法GSVR：采用混合形变场处理摄像机与物体运动耦合问题，动态感知时间切片策略适应不同动态级别视频，同时结合量化感知微调与图像编解码器实现紧凑表示。

Result: 实验表明，GSVR方法在训练时间（每帧2秒）、解码速度（800+FPS）及视频压缩表现上均优于现有方法，并在视频插值任务上表现与SOTA方法相当。

Conclusion: GSVR大幅加速了训练和解码速度，同时在多个视频任务上获得了更优性能，展现出很强的应用潜力。

Abstract: Implicit neural representations for video have been recognized as a novel and
promising form of video representation. Existing works pay more attention to
improving video reconstruction quality but little attention to the decoding
speed. However, the high computation of convolutional network used in existing
methods leads to low decoding speed. Moreover, these convolution-based video
representation methods also suffer from long training time, about 14 seconds
per frame to achieve 35+ PSNR on Bunny. To solve the above problems, we propose
GSVR, a novel 2D Gaussian-based video representation, which achieves 800+ FPS
and 35+ PSNR on Bunny, only needing a training time of $2$ seconds per frame.
Specifically, we propose a hybrid deformation field to model the dynamics of
the video, which combines two motion patterns, namely the tri-plane motion and
the polynomial motion, to deal with the coupling of camera motion and object
motion in the video. Furthermore, we propose a Dynamic-aware Time Slicing
strategy to adaptively divide the video into multiple groups of pictures(GOP)
based on the dynamic level of the video in order to handle large camera motion
and non-rigid movements. Finally, we propose quantization-aware fine-tuning to
avoid performance reduction after quantization and utilize image codecs to
compress Gaussians to achieve a compact representation. Experiments on the
Bunny and UVG datasets confirm that our method converges much faster than
existing methods and also has 10x faster decoding speed compared to other
methods. Our method has comparable performance in the video interpolation task
to SOTA and attains better video compression performance than NeRV.

</details>


### [23] [PaddleOCR 3.0 Technical Report](https://arxiv.org/abs/2507.05595)
*Cheng Cui,Ting Sun,Manhui Lin,Tingquan Gao,Yubo Zhang,Jiaxuan Liu,Xueqing Wang,Zelun Zhang,Changda Zhou,Hongen Liu,Yue Zhang,Wenyu Lv,Kui Huang,Yichao Zhang,Jing Zhang,Jun Zhang,Yi Liu,Dianhai Yu,Yanjun Ma*

Main category: cs.CV

TL;DR: PaddleOCR 3.0是一款用于OCR和文档解析的开源工具包，提供PP-OCRv5、PP-StructureV3和PP-ChatOCRv4三种主要解决方案。


<details>
  <summary>Details</summary>
Motivation: 满足大语言模型时代对文档理解日益增长的需求。

Method: 提供三个主要模型：PP-OCRv5用于多语言文本识别，PP-StructureV3用于分层文档解析，PP-ChatOCRv4用于关键信息提取。此外，通过硬件加速支持和高效工具，提供便捷的训练、推理和部署能力。

Result: 在保证模型参数不超过1亿的情况下，PaddleOCR 3.0达到了媲美那些拥有10亿参数的主流视觉-语言模型的效果。

Conclusion: PaddleOCR 3.0在性能和效率上具备较强的竞争力，并为开发者构建智能文档应用提供高品质OCR模型库及便捷开发工具。

Abstract: This technical report introduces PaddleOCR 3.0, an Apache-licensed
open-source toolkit for OCR and document parsing. To address the growing demand
for document understanding in the era of large language models, PaddleOCR 3.0
presents three major solutions: (1) PP-OCRv5 for multilingual text recognition,
(2) PP-StructureV3 for hierarchical document parsing, and (3) PP-ChatOCRv4 for
key information extraction. Compared to mainstream vision-language models
(VLMs), these models with fewer than 100 million parameters achieve competitive
accuracy and efficiency, rivaling billion-parameter VLMs. In addition to
offering a high-quality OCR model library, PaddleOCR 3.0 provides efficient
tools for training, inference, and deployment, supports heterogeneous hardware
acceleration, and enables developers to easily build intelligent document
applications.

</details>


### [24] [Rethinking Layered Graphic Design Generation with a Top-Down Approach](https://arxiv.org/abs/2507.05601)
*Jingye Chen,Zhaowen Wang,Nanxuan Zhao,Li Zhang,Difan Liu,Jimei Yang,Qifeng Chen*

Main category: cs.CV

TL;DR: 提出了一种名为Accordion的框架，用于将AI生成的设计转换为可编辑的分层设计，同时通过用户提示改善文本内容。


<details>
  <summary>Details</summary>
Motivation: 现有的AI生成设计虽然质量高，但缺乏可编辑性，而手动设计需要较高的专业知识，开发激励是解决这一矛盾。

Method: 基于视觉语言模型（VLM），通过三阶段框架引导模型完成分层设计，并结合多个视觉专家工具如SAM和元素移除模型，采用自顶向下的方式进行层分解。

Result: 在DesignIntention基准任务（包括文本到模板、背景文本添加、文本去渲染等）和设计变体生成中表现良好。

Conclusion: 该框架简化了AI生成设计的编辑流程，提升了设计的实用性和灵活性。

Abstract: Graphic design is crucial for conveying ideas and messages. Designers usually
organize their work into objects, backgrounds, and vectorized text layers to
simplify editing. However, this workflow demands considerable expertise. With
the rise of GenAI methods, an endless supply of high-quality graphic designs in
pixel format has become more accessible, though these designs often lack
editability. Despite this, non-layered designs still inspire human designers,
influencing their choices in layouts and text styles, ultimately guiding the
creation of layered designs. Motivated by this observation, we propose
Accordion, a graphic design generation framework taking the first attempt to
convert AI-generated designs into editable layered designs, meanwhile refining
nonsensical AI-generated text with meaningful alternatives guided by user
prompts. It is built around a vision language model (VLM) playing distinct
roles in three curated stages. For each stage, we design prompts to guide the
VLM in executing different tasks. Distinct from existing bottom-up methods
(e.g., COLE and Open-COLE) that gradually generate elements to create layered
designs, our approach works in a top-down manner by using the visually
harmonious reference image as global guidance to decompose each layer.
Additionally, it leverages multiple vision experts such as SAM and element
removal models to facilitate the creation of graphic layers. We train our
method using the in-house graphic design dataset Design39K, augmented with
AI-generated design images coupled with refined ground truth created by a
customized inpainting model. Experimental results and user studies by designers
show that Accordion generates favorable results on the DesignIntention
benchmark, including tasks such as text-to-template, adding text to background,
and text de-rendering, and also excels in creating design variations.

</details>


### [25] [Kernel Density Steering: Inference-Time Scaling via Mode Seeking for Image Restoration](https://arxiv.org/abs/2507.05604)
*Yuyang Hu,Kangfu Mei,Mojtaba Sahraee-Ardakan,Ulugbek S. Kamilov,Peyman Milanfar,Mauricio Delbracio*

Main category: cs.CV

TL;DR: 本研究提出了一种名为Kernel Density Steering (KDS)的新方法，用于改进扩散模型在图像修复任务中的性能，特别是在清晰度和抗伪影方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在执行图像修复任务时，常会面临一致性不足和伪影问题的挑战。

Method: 提出了一种推断时的框架KDS，通过$N$-粒子的扩散样本群集，应用局部的内核密度估计梯度进行引导，促使样本生成更高密度、更可靠的局部模式。

Result: 实验结果显示KDS显著提升了超分辨率和图像修复任务中的定量和定性表现，同时实现更高质量的图像样本。

Conclusion: KDS无需重新训练或借助外部验证器，能够无缝与不同扩散采样器结合，成为改进图像修复的有力工具，但以较高计算量为代价。

Abstract: Diffusion models show promise for image restoration, but existing methods
often struggle with inconsistent fidelity and undesirable artifacts. To address
this, we introduce Kernel Density Steering (KDS), a novel inference-time
framework promoting robust, high-fidelity outputs through explicit local
mode-seeking. KDS employs an $N$-particle ensemble of diffusion samples,
computing patch-wise kernel density estimation gradients from their collective
outputs. These gradients steer patches in each particle towards shared,
higher-density regions identified within the ensemble. This collective local
mode-seeking mechanism, acting as "collective wisdom", steers samples away from
spurious modes prone to artifacts, arising from independent sampling or model
imperfections, and towards more robust, high-fidelity structures. This allows
us to obtain better quality samples at the expense of higher compute by
simultaneously sampling multiple particles. As a plug-and-play framework, KDS
requires no retraining or external verifiers, seamlessly integrating with
various diffusion samplers. Extensive numerical validations demonstrate KDS
substantially improves both quantitative and qualitative performance on
challenging real-world super-resolution and image inpainting tasks.

</details>


### [26] [Generative Head-Mounted Camera Captures for Photorealistic Avatars](https://arxiv.org/abs/2507.05620)
*Shaojie Bai,Seunghyeon Seo,Yida Wang,Chenghui Li,Owen Wang,Te-Li Wang,Tianyang Ma,Jason Saragih,Shih-En Wei,Nojun Kwak,Hyung Jun Kim*

Main category: cs.CV

TL;DR: 本研究提出了一种名为GenHMC的新方法，用于生成高质量的合成HMC图像，通过解耦表情、视角及外观信息，解决了虚拟/增强现实中逼真化身动画面临的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 解决虚拟/增强现实中因难以获取真实的面部状态数据和现有方法中表现与风格解耦不完全的问题，同时降低数据采集成本。

Method: 提出了GenHMC方法，利用大量未配对的HMC捕捉数据，直接生成与穹顶相机捕捉的化身状态对应的高质量HMC图像，并实现了表情、视角与外观信息的解耦。

Result: 方法有效生成了更准确的真实数据，并能泛化到不同身份，消除了对配对捕捉数据的依赖，提高了数据效率与准确性。

Conclusion: GenHMC方法实现了虚拟/增强现实中动画化身的逼真性突破，既提升了合成数据质量，又减少了数据采集与训练的复杂性。

Abstract: Enabling photorealistic avatar animations in virtual and augmented reality
(VR/AR) has been challenging because of the difficulty of obtaining ground
truth state of faces. It is physically impossible to obtain synchronized images
from head-mounted cameras (HMC) sensing input, which has partial observations
in infrared (IR), and an array of outside-in dome cameras, which have full
observations that match avatars' appearance. Prior works relying on
analysis-by-synthesis methods could generate accurate ground truth, but suffer
from imperfect disentanglement between expression and style in their
personalized training. The reliance of extensive paired captures (HMC and dome)
for the same subject makes it operationally expensive to collect large-scale
datasets, which cannot be reused for different HMC viewpoints and lighting. In
this work, we propose a novel generative approach, Generative HMC (GenHMC),
that leverages large unpaired HMC captures, which are much easier to collect,
to directly generate high-quality synthetic HMC images given any conditioning
avatar state from dome captures. We show that our method is able to properly
disentangle the input conditioning signal that specifies facial expression and
viewpoint, from facial appearance, leading to more accurate ground truth.
Furthermore, our method can generalize to unseen identities, removing the
reliance on the paired captures. We demonstrate these breakthroughs by both
evaluating synthetic HMC images and universal face encoders trained from these
new HMC-avatar correspondences, which achieve better data efficiency and
state-of-the-art accuracy.

</details>


### [27] [AdaptaGen: Domain-Specific Image Generation through Hierarchical Semantic Optimization Framework](https://arxiv.org/abs/2507.05621)
*Suoxiang Zhang,Xiaxi Li,Hongrui Chang,Zhuoyan Hou,Guoxin Wu,Ronghua Ji*

Main category: cs.CV

TL;DR: 现有的领域图像生成方法存在语义理解与视觉表现脱节和领域语义约束不足的问题。本文提出AdaptaGen，一个分层语义优化框架，通过语义优化和跨模态适配机制解决上述问题，显著提升生成图像的质量、多样性和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 解决领域专用图像生成时语义理解与视觉表现脱节和生成结果语义偏差的问题，提高生成内容的质量、细节和领域语义一致性。

Method: 提出一个分层语义优化框架AdaptaGen，包括矩阵化的提示优化、多视角语义理解、跨模态自适应机制及双阶段语义转换机制，以保持核心主题元素的同时改善细节和视觉多样性。

Result: 实验表明，在多数据集的40个类别中，每类别仅采用16张图像，实现了显著的图像质量、多样性和语义一致性改进。

Conclusion: 提出的AdaptaGen方法有效缓解了领域专用图像生成中的语义和视觉表现脱节问题，在提升生成内容的语义一致性和视觉表现上表现突出。

Abstract: Domain-specific image generation aims to produce high-quality visual content
for specialized fields while ensuring semantic accuracy and detail fidelity.
However, existing methods exhibit two critical limitations: First, current
approaches address prompt engineering and model adaptation separately,
overlooking the inherent dependence between semantic understanding and visual
representation in specialized domains. Second, these techniques inadequately
incorporate domain-specific semantic constraints during content synthesis,
resulting in generation outcomes that exhibit hallucinations and semantic
deviations. To tackle these issues, we propose AdaptaGen, a hierarchical
semantic optimization framework that integrates matrix-based prompt
optimization with multi-perspective understanding, capturing comprehensive
semantic relationships from both global and local perspectives. To mitigate
hallucinations in specialized domains, we design a cross-modal adaptation
mechanism, which, when combined with intelligent content synthesis, enables
preserving core thematic elements while incorporating diverse details across
images. Additionally, we introduce a two-phase caption semantic transformation
during the generation phase. This approach maintains semantic coherence while
enhancing visual diversity, ensuring the generated images adhere to
domain-specific constraints. Experimental results confirm our approach's
effectiveness, with our framework achieving superior performance across 40
categories from diverse datasets using only 16 images per category,
demonstrating significant improvements in image quality, diversity, and
semantic consistency.

</details>


### [28] [OFFSET: Segmentation-based Focus Shift Revision for Composed Image Retrieval](https://arxiv.org/abs/2507.05631)
*Zhiwei Chen,Yupeng Hu,Zixu Li,Zhiheng Fu,Xuemeng Song,Liqiang Nie*

Main category: cs.CV

TL;DR: 本文提出了一种创新的组合图像检索（CIR）方法，名为OFFSET，重点解决了现有方法在特征退化及视觉焦点偏差方面的两大主要限制。


<details>
  <summary>Details</summary>
Motivation: 目前的CIR方法无法处理视觉数据中的显著部分和噪声之间的不均匀性，同时忽略了文本数据在图像修正中的优先级。

Method: 本文设计了一种基于焦点映射的特征提取框架，包含主区域分割和双重焦点映射模块，并引入文本引导的焦点修订模块，实现对参考图像的自适应修正。

Result: 通过四个基准数据集的全面实验，验证了所提出方法在组合图像检索任务中的优越性。

Conclusion: OFFSET模型通过降低噪声干扰并提高视觉与文本特性提取的有效性，显著增强了CIR任务中的性能。

Abstract: Composed Image Retrieval (CIR) represents a novel retrieval paradigm that is
capable of expressing users' intricate retrieval requirements flexibly. It
enables the user to give a multimodal query, comprising a reference image and a
modification text, and subsequently retrieve the target image. Notwithstanding
the considerable advances made by prevailing methodologies, CIR remains in its
nascent stages due to two limitations: 1) inhomogeneity between dominant and
noisy portions in visual data is ignored, leading to query feature degradation,
and 2) the priority of textual data in the image modification process is
overlooked, which leads to a visual focus bias. To address these two
limitations, this work presents a focus mapping-based feature extractor, which
consists of two modules: dominant portion segmentation and dual focus mapping.
It is designed to identify significant dominant portions in images and guide
the extraction of visual and textual data features, thereby reducing the impact
of noise interference. Subsequently, we propose a textually guided focus
revision module, which can utilize the modification requirements implied in the
text to perform adaptive focus revision on the reference image, thereby
enhancing the perception of the modification focus on the composed features.
The aforementioned modules collectively constitute the segmentatiOn-based Focus
shiFt reviSion nETwork (\mbox{OFFSET}), and comprehensive experiments on four
benchmark datasets substantiate the superiority of our proposed method. The
codes and data are available on https://zivchen-ty.github.io/OFFSET.github.io/

</details>


### [29] [Knowledge-guided Complex Diffusion Model for PolSAR Image Classification in Contourlet Domain](https://arxiv.org/abs/2507.05666)
*Junfei Shi,Yu Cheng,Haiyan Jin,Junhuai Li,Zhaolin Xiao,Maoguo Gong,Weisi Lin*

Main category: cs.CV

TL;DR: 本文提出了一种结构知识引导的复杂扩散模型，在Contourlet域进行PolSAR图像分类，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的实值扩散模型在处理PolSAR数据时难以捕捉复杂的相位信息，并且在保留细节结构上存在不足。

Method: 利用复杂Contourlet变换分解PolSAR数据，提取统计和边界特征；设计了知识引导的复杂扩散网络建模低频分量的统计特性，同时利用高频结构信息指导扩散过程。

Result: 实验结果表明，该方法在三种真实PolSAR数据集上效果超越现有方法，特别是在复杂地形中能更好地保留边缘细节和区域一致性。

Conclusion: 该研究通过将结构知识与复杂扩散模型相结合，提高了PolSAR图像分类的性能，特别是在保留边缘和区域一致性方面表现优异。

Abstract: Diffusion models have demonstrated exceptional performance across various
domains due to their ability to model and generate complicated data
distributions. However, when applied to PolSAR data, traditional real-valued
diffusion models face challenges in capturing complex-valued phase
information.Moreover, these models often struggle to preserve fine structural
details. To address these limitations, we leverage the Contourlet transform,
which provides rich multiscale and multidirectional representations well-suited
for PolSAR imagery. We propose a structural knowledge-guided complex diffusion
model for PolSAR image classification in the Contourlet domain. Specifically,
the complex Contourlet transform is first applied to decompose the data into
low- and high-frequency subbands, enabling the extraction of statistical and
boundary features. A knowledge-guided complex diffusion network is then
designed to model the statistical properties of the low-frequency components.
During the process, structural information from high-frequency coefficients is
utilized to guide the diffusion process, improving edge preservation.
Furthermore, multiscale and multidirectional high-frequency features are
jointly learned to further boost classification accuracy. Experimental results
on three real-world PolSAR datasets demonstrate that our approach surpasses
state-of-the-art methods, particularly in preserving edge details and
maintaining region homogeneity in complex terrain.

</details>


### [30] [Dynamic Rank Adaptation for Vision-Language Models](https://arxiv.org/abs/2507.05668)
*Jiahui Wang,Qin Xu,Bo Jiang,Bin Luo*

Main category: cs.CV

TL;DR: 提出了一种新的适配器变体方法DRA，通过动态分配适配秩来增强视觉-语言模型对新类的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在微调大规模视觉-语言模型时难以平衡泛化能力，尤其对未见新类的识别存在局限。

Method: DRA 方法通过序列注意力评估和分组重要特征，并动态分配更高秩给重要特征，同时设计了新通道响应机制和引入L1正则化来稳定训练。

Result: 与现有方法相比，DRA 显著提升了多个基准测试包括基础-新类、跨数据集与领域泛化的性能。

Conclusion: DRA 方法通过优化特征选择与动态适配秩，解决了过拟合问题，增强了对未见新类的泛化表现，适合实际应用。

Abstract: Pre-trained large vision-language models (VLMs) like CLIP demonstrate
impressive generalization ability. Existing prompt-based and adapter-based
works have made significant progress in fine-tuning VLMs but still face the
challenges of maintaining strong generalization abilities, particularly towards
unseen new classes. This limitation partly arises from these methods treating
all tokens of the image and text encoder equally, which can lead to overfitting
on less informative features (e.g., background noise, template words) and
degrade the general representations that are crucial for novel concept
recognition. To address this issue, we propose Dynamic Rank Adaptation (DRA), a
novel adapter variant method, designed specifically to enhance new class
generalization. DRA dynamically allocates adaptation ranks based on the
importance of features during training to preserve general knowledge. DRA first
employs token importance grouping, using sequence attention to evaluate and
group tokens by their importance. Then, we adopt rank adaptation according to
the importance of each token group dynamically by assigning higher feature
ranks to the more important tokens. Also, we design a new channel response
mechanism to prioritize the preservation and adaptation of feature channels
identified as the most informative for each instance. In addition, a L1
regularization term is introduced to stabilize the training. Extensive
experiments demonstrate the effectiveness and superiority of our proposed DRA
over existing works, especially on enhancing the performance of new classes on
various benchmarks, including base-new classes, cross-datasets evaluation and
domain generalization. The source code will be published after the paper is
received.

</details>


### [31] [Modeling and Reversing Brain Lesions Using Diffusion Models](https://arxiv.org/abs/2507.05670)
*Omar Zamzam,Haleh Akrami,Anand Joshi,Richard Leahy*

Main category: cs.CV

TL;DR: 本研究提出了一种基于扩散模型的框架，用于分析和逆转脑损伤过程，改进了脑损伤分割和标记的精度。


<details>
  <summary>Details</summary>
Motivation: 目前的脑损伤分割方法未能区分损坏的组织和变形的组织，需要更有效的方法来进行区分分析并恢复健康脑组织。

Method: 提出了一个基于扩散模型的框架：首先分割异常区域，估计并逆转组织变形以恢复位移的组织，并隔离核心损伤区域，最后通过修复核心区域来估算健康状态的大脑。

Result: 与传统方法相比，该框架在脑损伤分割、特征分析和脑标记方面表现出更高的精度。

Conclusion: 此框架为临床和研究中的脑损伤分析提供了强有力的工具，并在合成脑图像的基础上验证其逆向处理效果。

Abstract: Brain lesions are abnormalities or injuries in brain tissue that are often
detectable using magnetic resonance imaging (MRI), which reveals structural
changes in the affected areas. This broad definition of brain lesions includes
areas of the brain that are irreversibly damaged, as well as areas of brain
tissue that are deformed as a result of lesion growth or swelling. Despite the
importance of differentiating between damaged and deformed tissue, existing
lesion segmentation methods overlook this distinction, labeling both of them as
a single anomaly. In this work, we introduce a diffusion model-based framework
for analyzing and reversing the brain lesion process. Our pipeline first
segments abnormal regions in the brain, then estimates and reverses tissue
deformations by restoring displaced tissue to its original position, isolating
the core lesion area representing the initial damage. Finally, we inpaint the
core lesion area to arrive at an estimation of the pre-lesion healthy brain.
This proposed framework reverses a forward lesion growth process model that is
well-established in biomechanical studies that model brain lesions. Our results
demonstrate improved accuracy in lesion segmentation, characterization, and
brain labeling compared to traditional methods, offering a robust tool for
clinical and research applications in brain lesion analysis. Since pre-lesion
healthy versions of abnormal brains are not available in any public dataset for
validation of the reverse process, we simulate a forward model to synthesize
multiple lesioned brain images.

</details>


### [32] [R-VLM: Region-Aware Vision Language Model for Precise GUI Grounding](https://arxiv.org/abs/2507.05673)
*Joonhyung Park,Peng Tang,Sagnik Das,Srikar Appalaraju,Kunwar Yashraj Singh,R. Manmatha,Shabnam Ghadar*

Main category: cs.CV

TL;DR: 本文提出了一种名为R-VLM的新方法，用于提高基于GUI图形界面自动化的定位准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于视觉的GUI代理在元素定位过程中因多余信息干扰导致的准确性问题，以及改进现有目标函数对定位精度的捕获能力。

Method: 通过提供放大区域的提案来实现精确的GUI元素定位，同时引入一种基于IoU的目标函数，改进模型在高IoU预测上的收敛能力。

Result: 在GUI定位基准ScreenSpot和AgentStudio上，将定位准确性提高了13%；在AITW和Mind2Web基准的GUI导航任务中实现了3.2-9.7%的绝对准确率提升。

Conclusion: R-VLM方法将视觉语言模型与传统目标检测技术结合，显著提高了GUI自动化任务的效果。

Abstract: Visual agent models for automating human activities on Graphical User
Interfaces (GUIs) have emerged as a promising research direction, driven by
advances in large Vision Language Models (VLMs). A critical challenge in GUI
automation is the precise grounding of interface elements across diverse
platforms. Existing vision-only GUI agents directly ground elements from large
and cluttered screenshots, requiring them to process substantial irrelevant
information that compromises their accuracy. In addition, these approaches
typically employ basic cross-entropy loss for learning grounding objectives,
which fails to effectively capture grounding quality compared to established
object detection metrics like Intersection-over-Union (IoU). To address these
issues, we introduce R-VLM, a novel GUI grounding approach that leverages
zoomed-in region proposals for precise element localization. We also propose an
IoU-aware objective function that facilitates model convergence toward high IoU
predictions. Our approach bridges the gap between VLMs and conventional object
detection techniques, improving the state-of-the-art grounding accuracy by 13%
across diverse GUI platforms on the GUI grounding benchmarks ScreenSpot and
AgentStudio. In addition, our R-VLM approach shows 3.2-9.7% absolute accuracy
improvements in GUI navigation tasks on the AITW and Mind2Web benchmarks.

</details>


### [33] [MedGen: Unlocking Medical Video Generation by Scaling Granularly-annotated Medical Videos](https://arxiv.org/abs/2507.05675)
*Rongsheng Wang,Junying Chen,Ke Ji,Zhenyang Cai,Shunian Chen,Yunjin Yang,Benyou Wang*

Main category: cs.CV

TL;DR: 该论文提出了首个大规模、高质量的医疗视频生成数据集MedVideoCap-55K，并开发了表现领先的生成模型MedGen。


<details>
  <summary>Details</summary>
Motivation: 医疗视频对于临床培训、教育和模拟至关重要，但当前的生成模型因缺乏高质量医疗领域数据集，生成的内容往往不真实或有误。

Method: 研究者构建了一个包含55,000条医疗视频剪辑的大规模数据集MedVideoCap-55K，并基于该数据集开发了生成模型MedGen以优化视觉质量和医疗准确性。

Result: MedGen在多个基准测试中表现优异，其视觉质量和医疗准确性媲美商业系统。

Conclusion: 该数据集和模型将成为医疗视频生成领域的重要资源，并促进后续研究的发展。同时，其代码和数据已公开发布。

Abstract: Recent advances in video generation have shown remarkable progress in
open-domain settings, yet medical video generation remains largely
underexplored. Medical videos are critical for applications such as clinical
training, education, and simulation, requiring not only high visual fidelity
but also strict medical accuracy. However, current models often produce
unrealistic or erroneous content when applied to medical prompts, largely due
to the lack of large-scale, high-quality datasets tailored to the medical
domain. To address this gap, we introduce MedVideoCap-55K, the first
large-scale, diverse, and caption-rich dataset for medical video generation. It
comprises over 55,000 curated clips spanning real-world medical scenarios,
providing a strong foundation for training generalist medical video generation
models. Built upon this dataset, we develop MedGen, which achieves leading
performance among open-source models and rivals commercial systems across
multiple benchmarks in both visual quality and medical accuracy. We hope our
dataset and model can serve as a valuable resource and help catalyze further
research in medical video generation. Our code and data is available at
https://github.com/FreedomIntelligence/MedGen

</details>


### [34] [Integrated Structural Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2507.05677)
*Jiahui Wang,Qin Xu,Bo Jiang,Bin Luo*

Main category: cs.CV

TL;DR: 本文提出了一种集成结构化提示(Integrated Structural Prompt, ISP)用于视觉语言模型(VLMs)，提升文本与图像分支之间的信息交互，并提高对新类别的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的提示学习方法忽略了可学习提示与模态内外的标记之间的结构关系，同时难以平衡基础类别和新类别的性能。

Method: 提出ISP，利用自结构和跨结构提示模块，在模态内外建模可学习提示与冻结标记的结构关系，并设计采样探测模块，根据样本难度动态调整损失系数，防止过拟合简单样本。

Result: 在基础到新类别的泛化、跨数据集评估和域泛化三个场景下，ISP均表现出与最先进方法相当的竞争性能。

Conclusion: ISP通过提升模态间信息交互和调整样本学习策略，有效增强了VLMs的泛化能力，验证了方法的有效性。

Abstract: Prompt learning methods have significantly extended the transferability of
pre-trained Vision-Language Models (VLMs) like CLIP for various downstream
tasks. These methods adopt handcraft templates or learnable vectors to provide
text or image instructions in fine-tuning VLMs. However, most existing works
ignore the structural relationships between learnable prompts and tokens within
and between modalities. Moreover, balancing the performance of base and new
classes remains a significant challenge. In this paper, we propose an
Integrated Structural Prompt (ISP) for VLMs to enhance the interaction of
information representations between the text and image branches. ISP introduces
self-structural and cross-structural prompt modules to model the structural
relationships between learnable prompts and frozen tokens within and across
modalities. This enables efficient information transfer while preserving
feature stability. Additionally, we propose a sample probing module that
dynamically adjusts loss coefficients based on sample difficulty, preventing
the mode from overfitting to simple samples and improving generalization
ability to new classes. Extensive experiments on three widely used settings:
base-to-new generalization, cross-dataset evaluation, and domain generalization
demonstrate that the proposed ISP achieves competitive performance against
state-of-the-art methods.

</details>


### [35] [LiON-LoRA: Rethinking LoRA Fusion to Unify Controllable Spatial and Temporal Generation for Video Diffusion](https://arxiv.org/abs/2507.05678)
*Yisu Zhang,Chenjie Cao,Chaohui Yu,Jianke Zhu*

Main category: cs.CV

TL;DR: 该论文提出LiON-LoRA，用于改进视频扩散模型的控制能力，解决相机轨迹与物体运动的精确控制问题。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA在控制相机轨迹和物体运动方面存在非线性扩展和融合不稳定问题，难以实现精确控制，因此需要一种新方法。

Method: 提出LiON-LoRA框架，通过线性扩展性、正交性和规范一致性三个原则重新定义LoRA的融合过程，并联合修改自注意力机制实现相机与物体运动的解耦控制。

Result: LiON-LoRA在轨迹控制精度和运动调整强度方面超越了现有方法，同时实现了最小训练数据下的优异泛化性能。

Conclusion: LiON-LoRA为视频扩散模型提供了精确控制的解决方案，提升了空间和时间维度上的生成能力。

Abstract: Video Diffusion Models (VDMs) have demonstrated remarkable capabilities in
synthesizing realistic videos by learning from large-scale data. Although
vanilla Low-Rank Adaptation (LoRA) can learn specific spatial or temporal
movement to driven VDMs with constrained data, achieving precise control over
both camera trajectories and object motion remains challenging due to the
unstable fusion and non-linear scalability. To address these issues, we propose
LiON-LoRA, a novel framework that rethinks LoRA fusion through three core
principles: Linear scalability, Orthogonality, and Norm consistency. First, we
analyze the orthogonality of LoRA features in shallow VDM layers, enabling
decoupled low-level controllability. Second, norm consistency is enforced
across layers to stabilize fusion during complex camera motion combinations.
Third, a controllable token is integrated into the diffusion transformer (DiT)
to linearly adjust motion amplitudes for both cameras and objects with a
modified self-attention mechanism to ensure decoupled control. Additionally, we
extend LiON-LoRA to temporal generation by leveraging static-camera videos,
unifying spatial and temporal controllability. Experiments demonstrate that
LiON-LoRA outperforms state-of-the-art methods in trajectory control accuracy
and motion strength adjustment, achieving superior generalization with minimal
training data. Project Page: https://fuchengsu.github.io/lionlora.github.io/

</details>


### [36] [Event-RGB Fusion for Spacecraft Pose Estimation Under Harsh Lighting](https://arxiv.org/abs/2507.05698)
*Mohsi Jawaid,Marcus Märtens,Tat-Jun Chin*

Main category: cs.CV

TL;DR: 本文提出了一种将RGB传感器和事件传感器融合的姿态估计方法，专门用于光线条件恶劣的太空操作场景，并公开了一个相关数据集。


<details>
  <summary>Details</summary>
Motivation: 太空中进行自主操作如对接、服务等需要精确的姿态估计，但传统基于RGB传感器的方法在极端光线条件下表现不佳。借助动态范围更广的事件传感器可以解决部分问题，但其分辨率和信噪比不足。本工作尝试通过传感器融合解决这些问题。

Method: 引入了一种基于RGB和事件传感器的信息融合方法：通过光学分光棱镜实现精确的对齐，并使用RANSAC技术结合两种传感器特点进行估计，同时加入dropout不确定性处理以识别极端条件对单一通道的影响。

Result: 在实验室条件下的多种极端光照条件测试中，验证了提出的融合方法之有效性，并确认了事件传感器在太空姿态估计中的潜力。

Conclusion: 结果支持事件传感器和RGB传感器联合用于太空姿态估计，这为这一领域的研究提供参考，同时公布的数据集有助于进一步研究。

Abstract: Spacecraft pose estimation is crucial for autonomous in-space operations,
such as rendezvous, docking and on-orbit servicing. Vision-based pose
estimation methods, which typically employ RGB imaging sensors, is a compelling
solution for spacecraft pose estimation, but are challenged by harsh lighting
conditions, which produce imaging artifacts such as glare, over-exposure,
blooming and lens flare. Due to their much higher dynamic range, neuromorphic
or event sensors are more resilient to extreme lighting conditions. However,
event sensors generally have lower spatial resolution and suffer from reduced
signal-to-noise ratio during periods of low relative motion. This work
addresses these individual sensor limitations by introducing a sensor fusion
approach combining RGB and event sensors. A beam-splitter prism was employed to
achieve precise optical and temporal alignment. Then, a RANSAC-based technique
was developed to fuse the information from the RGB and event channels to
achieve pose estimation that leveraged the strengths of the two modalities. The
pipeline was complemented by dropout uncertainty estimation to detect extreme
conditions that affect either channel. To benchmark the performance of the
proposed event-RGB fusion method, we collected a comprehensive real dataset of
RGB and event data for satellite pose estimation in a laboratory setting under
a variety of challenging illumination conditions. Encouraging results on the
dataset demonstrate the efficacy of our event-RGB fusion approach and further
supports the usage of event sensors for spacecraft pose estimation. To support
community research on this topic, our dataset will be released publicly.

</details>


### [37] [Hyperspectral Anomaly Detection Methods: A Survey and Comparative Study](https://arxiv.org/abs/2507.05730)
*Aayushma Pant,Arbind Agrahari Baniya,Tsz-Kwan Lee,Sunil Aryal*

Main category: cs.CV

TL;DR: 本文评估了不同的高光谱异常检测方法，包括统计模型、基于表示的方法、经典机器学习方法和深度学习模型，结果显示深度学习模型检测精度最高，而统计模型速度最优。


<details>
  <summary>Details</summary>
Motivation: 针对高光谱异常检测中存在的高计算复杂度、对噪声敏感以及跨数据集泛化能力有限等问题，对不同检测技术进行比较分析。

Method: 将高光谱异常检测方法分为四类，基于统计模型、表示方法、经典机器学习和深度学习模型，并使用17个基准数据集评估其性能。

Result: 深度学习模型在检测精度上表现最佳，而统计模型在速度方面表现突出。此外，不同方法各具优势与不足，研究还指出未来可能的研究方向。

Conclusion: 本文为高光谱异常检测领域的研究者和实践者提供了全面的技术对比和趋势分析。

Abstract: Hyperspectral images are high-dimensional datasets consisting of hundreds of
contiguous spectral bands, enabling detailed material and surface analysis.
Hyperspectral anomaly detection (HAD) refers to the technique of identifying
and locating anomalous targets in such data without prior information about a
hyperspectral scene or target spectrum. This technology has seen rapid
advancements in recent years, with applications in agriculture, defence,
military surveillance, and environmental monitoring. Despite this significant
progress, existing HAD methods continue to face challenges such as high
computational complexity, sensitivity to noise, and limited generalisation
across diverse datasets. This study presents a comprehensive comparison of
various HAD techniques, categorising them into statistical models,
representation-based methods, classical machine learning approaches, and deep
learning models. We evaluated these methods across 17 benchmarking datasets
using different performance metrics, such as ROC, AUC, and separability map to
analyse detection accuracy, computational efficiency, their strengths,
limitations, and directions for future research.The research shows that deep
learning models achieved the highest detection accuracy, while statistical
models demonstrated exceptional speed across all datasets. This study aims to
provide valuable insights for researchers and practitioners working to advance
the field of hyperspectral anomaly detection methods.

</details>


### [38] [SenseShift6D: Multimodal RGB-D Benchmarking for Robust 6D Pose Estimation across Environment and Sensor Variations](https://arxiv.org/abs/2507.05751)
*Yegyu Han,Taegyoon Yoon,Dayeon Woo,Sojeong Kim,Hyung-Sin Kim*

Main category: cs.CV

TL;DR: 本文提出了首个RGB-D数据集SenseShift6D，研究传感器控制对现实环境中物体6D姿态估计的影响，并证明了测试时的传感器自适应控制可有效提升模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的6D物体姿态估计数据集难以涵盖真实环境中的光照、曝光、增益和深度传感模式等变化，缺乏对传感器控制缓解这些变化的探索。

Method: 提出SenseShift6D数据集，涵盖13种RGB曝光、9种增益、4种深度模式和5种光照级别，共计10.19万张RGB图像和1万张深度图像，以及1380种传感器-光照组合，用于评估传感器自适应的效果。

Result: 实验表明，与数字数据增强相比，测试时传感器控制的引入可显著提升性能，并表现出与大规模增加真实训练数据量类似的效果。

Conclusion: SenseShift6D将6D姿态评估从数据中心扩展到传感器鲁棒性，为适应性和自调节感知系统奠定了基础，同时验证了多模态RGB-D联合自适应的效果优于单模态适应。

Abstract: Recent advances on 6D object-pose estimation has achieved high performance on
representative benchmarks such as LM-O, YCB-V, and T-Less. However, these
datasets were captured under fixed illumination and camera settings, leaving
the impact of real-world variations in illumination, exposure, gain or
depth-sensor mode - and the potential of test-time sensor control to mitigate
such variations - largely unexplored. To bridge this gap, we introduce
SenseShift6D, the first RGB-D dataset that physically sweeps 13 RGB exposures,
9 RGB gains, auto-exposure, 4 depth-capture modes, and 5 illumination levels.
For three common household objects (spray, pringles, and tincase), we acquire
101.9k RGB and 10k depth images, which can provide 1,380 unique sensor-lighting
permutations per object pose. Experiments with state-of-the-art models on our
dataset show that applying sensor control during test-time induces greater
performance improvement over digital data augmentation, achieving performance
comparable to or better than costly increases in real-world training data
quantity and diversity. Adapting either RGB or depth sensors individually is
effective, while jointly adapting multimodal RGB-D configurations yields even
greater improvements. SenseShift6D extends the 6D-pose evaluation paradigm from
data-centered to sensor-aware robustness, laying a foundation for adaptive,
self-tuning perception systems capable of operating robustly in uncertain
real-world environments. Our dataset is available at:
huggingface.co/datasets/Yegyu/SenseShift6D Associated scripts can be found at:
github.com/yegyu-han/SenseShift6D

</details>


### [39] [Normal Patch Retinex Robust Alghoritm for White Balancing in Digital Microscopy](https://arxiv.org/abs/2507.05757)
*Radoslaw Roszczyk,Artur Krupa,Izabella Antoniuk*

Main category: cs.CV

TL;DR: 文章提出了一种全自动白平衡算法，可有效纠正光学显微镜的显微图像颜色问题。实验验证表明，该方法优于传统数码摄影中的白平衡算法。


<details>
  <summary>Details</summary>
Motivation: 解决显微镜图像白平衡矫正难的问题，尤其是针对病理形态学中常见的显微标本。

Method: 设计并实现了一种全自动白平衡算法，并将其应用于显微图像中，同时与传统数码摄影中的白平衡算法效果进行对比分析。

Result: 实验验证表明，该算法在处理Hematoxylin-Phloxine-Saffron和免疫组织化学染色图像时表现更优。

Conclusion: 自动白平衡算法比传统算法在处理特定显微图像时更有效，适用于病理形态学中显微图像的颜色矫正。

Abstract: The acquisition of accurately coloured, balanced images in an optical
microscope can be a challenge even for experienced microscope operators. This
article presents an entirely automatic mechanism for balancing the white level
that allows the correction of the microscopic colour images adequately. The
results of the algorithm have been confirmed experimentally on a set of two
hundred microscopic images. The images contained scans of three microscopic
specimens commonly used in pathomorphology. Also, the results achieved were
compared with other commonly used white balance algorithms in digital
photography. The algorithm applied in this work is more effective than the
classical algorithms used in colour photography for microscopic images stained
with hematoxylin-phloxine-saffron and for immunohistochemical staining images.

</details>


### [40] [DreamArt: Generating Interactable Articulated Objects from a Single Image](https://arxiv.org/abs/2507.05763)
*Ruijie Lu,Yu Liu,Jiaxiang Tang,Junfeng Ni,Yuxiang Wang,Diwen Wan,Gang Zeng,Yixin Chen,Siyuan Huang*

Main category: cs.CV

TL;DR: 本文提出DreamArt框架，通过单视图图像生成高质量、可交互的多关节物体模型，具有高形状和外观保真度以及逼真的关节运动能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法对几何结构和材质纹理关注较多，但缺乏对部件分解和运动建模的考虑。同时，神经重建方法需要大量的多视角数据，难以扩展。

Method: DreamArt采用三阶段方法：图像到3D的部件分割与完成；优化基于视频扩散模型的关节运动推测；精炼整体材质并重绘以保证质量。

Result: 实验表明该方法生成的模型具备准确部件形状、高外观保真度和合理的运动表现。

Conclusion: DreamArt为基于单图像的多关节物体生成提供了一种高效且可扩展的解决方案，为未来的更多应用奠定了基础。

Abstract: Generating articulated objects, such as laptops and microwaves, is a crucial
yet challenging task with extensive applications in Embodied AI and AR/VR.
Current image-to-3D methods primarily focus on surface geometry and texture,
neglecting part decomposition and articulation modeling. Meanwhile, neural
reconstruction approaches (e.g., NeRF or Gaussian Splatting) rely on dense
multi-view or interaction data, limiting their scalability. In this paper, we
introduce DreamArt, a novel framework for generating high-fidelity,
interactable articulated assets from single-view images. DreamArt employs a
three-stage pipeline: firstly, it reconstructs part-segmented and complete 3D
object meshes through a combination of image-to-3D generation, mask-prompted 3D
segmentation, and part amodal completion. Second, we fine-tune a video
diffusion model to capture part-level articulation priors, leveraging movable
part masks as prompt and amodal images to mitigate ambiguities caused by
occlusion. Finally, DreamArt optimizes the articulation motion, represented by
a dual quaternion, and conducts global texture refinement and repainting to
ensure coherent, high-quality textures across all parts. Experimental results
demonstrate that DreamArt effectively generates high-quality articulated
objects, possessing accurate part shape, high appearance fidelity, and
plausible articulation, thereby providing a scalable solution for articulated
asset generation. Our project page is available at
https://dream-art-0.github.io/DreamArt/.

</details>


### [41] [TalkFashion: Intelligent Virtual Try-On Assistant Based on Multimodal Large Language Model](https://arxiv.org/abs/2507.05790)
*Yujie Hu,Xuanyu Zhang,Weiqi Li,Jian Zhang*

Main category: cs.CV

TL;DR: 本文提出了TalkFashion，一个智能虚拟试穿助手，通过文字指令实现多功能的虚拟试穿，包括全套服装更换和局部编辑。


<details>
  <summary>Details</summary>
Motivation: 在现有的方法中，主要依赖端到端网络进行单一试穿任务，缺乏多样性和灵活性。本文的目标是解决这些问题，实现功能更丰富的虚拟试穿。

Method: 引入了TalkFashion助手，它利用大型语言模型分析用户指令并决定执行的任务。同时提出基于指令的局部重绘模型，配合多模态模型，实现全自动化的局部编辑。

Result: 实验结果表明，与现有方法相比，该方法在语义一致性和视觉质量上表现更佳。

Conclusion: 本文方法有效提升了虚拟试穿的多功能性和灵活性，为用户提供更智能的试穿体验。

Abstract: Virtual try-on has made significant progress in recent years. This paper
addresses how to achieve multifunctional virtual try-on guided solely by text
instructions, including full outfit change and local editing. Previous methods
primarily relied on end-to-end networks to perform single try-on tasks, lacking
versatility and flexibility. We propose TalkFashion, an intelligent try-on
assistant that leverages the powerful comprehension capabilities of large
language models to analyze user instructions and determine which task to
execute, thereby activating different processing pipelines accordingly.
Additionally, we introduce an instruction-based local repainting model that
eliminates the need for users to manually provide masks. With the help of
multi-modal models, this approach achieves fully automated local editings,
enhancing the flexibility of editing tasks. The experimental results
demonstrate better semantic consistency and visual quality compared to the
current methods.

</details>


### [42] [SPADE: Spatial-Aware Denoising Network for Open-vocabulary Panoptic Scene Graph Generation with Long- and Local-range Context Reasoning](https://arxiv.org/abs/2507.05798)
*Xin Hu,Ke Qin,Guiduo Duan,Ming Li,Yuan-Fang Li,Tao He*

Main category: cs.CV

TL;DR: 该论文提出了一种名为SPADE的框架，用于解决全景场景图生成（PSG）任务中的空间关系推理和像素级结构关系问题，并在基准数据集上表现出卓越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（VLMs）在空间关系推理方面存在固有限制，难以区分对象的相对位置，而全景场景图生成需要处理复杂场景中的像素级结构关系。

Method: 引入了SPADE框架，包括两个关键步骤：1）通过基于LoRA的轻量化微调策略，利用扩散模型的逆过程调整UNet为PSG特定去噪网络；2）开发空间感知关系图变换器，捕捉局部和长距离上下文信息生成高质量关系查询。

Result: 在基准PSG和Visual Genome数据集上的实验表明，SPADE在关闭集合和开放集合情景下均优于现有最先进方法，尤其在空间关系预测方面表现卓越。

Conclusion: SPADE框架通过解决VLMs在空间关系推理中的局限性，在全景场景图生成任务中提供了更准确的关系预测，并推动了此领域的研究进展和性能提升。

Abstract: Panoptic Scene Graph Generation (PSG) integrates instance segmentation with
relation understanding to capture pixel-level structural relationships in
complex scenes. Although recent approaches leveraging pre-trained
vision-language models (VLMs) have significantly improved performance in the
open-vocabulary setting, they commonly ignore the inherent limitations of VLMs
in spatial relation reasoning, such as difficulty in distinguishing object
relative positions, which results in suboptimal relation prediction. Motivated
by the denoising diffusion model's inversion process in preserving the spatial
structure of input images, we propose SPADE (SPatial-Aware Denoising-nEtwork)
framework -- a novel approach for open-vocabulary PSG. SPADE consists of two
key steps: (1) inversion-guided calibration for the UNet adaptation, and (2)
spatial-aware context reasoning. In the first step, we calibrate a general
pre-trained teacher diffusion model into a PSG-specific denoising network with
cross-attention maps derived during inversion through a lightweight LoRA-based
fine-tuning strategy. In the second step, we develop a spatial-aware relation
graph transformer that captures both local and long-range contextual
information, facilitating the generation of high-quality relation queries.
Extensive experiments on benchmark PSG and Visual Genome datasets demonstrate
that SPADE outperforms state-of-the-art methods in both closed- and open-set
scenarios, particularly for spatial relationship prediction.

</details>


### [43] [DREAM: Document Reconstruction via End-to-end Autoregressive Model](https://arxiv.org/abs/2507.05805)
*Xin Li,Mingming Gong,Yunfei Wu,Jianxin Dai,Antai Guo,Xinghua Jiang,Haoyu Cao,Yinsong Liu,Deqiang Jiang,Xing Sun*

Main category: cs.CV

TL;DR: 本文提出DREAM模型，以端到端方式进行文档重建，克服现有方法中的错误传播问题，同时保留文档元素布局信息。


<details>
  <summary>Details</summary>
Motivation: 当前文档分析领域的多阶段方法易受错误传播影响，而现有生成模型缺乏对文档元素布局信息的保留，本研究针对这些问题提出解决方案。

Method: 提出一种新的自回归模型DREAM，以端到端方式将文本图像转化为文档重建序列，同时定义标准化的文档重建任务，并引入新的评估指标DSM和数据集DocRec1K。

Result: 实验结果显示，该方法在文档重建方面性能优越，并在文档布局分析、文本识别、表格结构识别、公式识别和读取顺序检测等子任务上具有竞争力。

Conclusion: DREAM模型通过综合处理文档元素信息和端到端设计，在文档重建领域取得了前所未有的性能表现，并兼具多任务适应性。

Abstract: Document reconstruction constitutes a significant facet of document analysis
and recognition, a field that has been progressively accruing interest within
the scholarly community. A multitude of these researchers employ an array of
document understanding models to generate predictions on distinct subtasks,
subsequently integrating their results into a holistic document reconstruction
format via heuristic principles. Nevertheless, these multi-stage methodologies
are hindered by the phenomenon of error propagation, resulting in suboptimal
performance. Furthermore, contemporary studies utilize generative models to
extract the logical sequence of plain text, tables and mathematical expressions
in an end-to-end process. However, this approach is deficient in preserving the
information related to element layouts, which are vital for document
reconstruction. To surmount these aforementioned limitations, we in this paper
present an innovative autoregressive model specifically designed for document
reconstruction, referred to as Document Reconstruction via End-to-end
Autoregressive Model (DREAM). DREAM transmutes the text image into a sequence
of document reconstruction in a comprehensive, end-to-end process,
encapsulating a broader spectrum of document element information. In addition,
we establish a standardized definition of the document reconstruction task, and
introduce a novel Document Similarity Metric (DSM) and DocRec1K dataset for
assessing the performance of the task. Empirical results substantiate that our
methodology attains unparalleled performance in the realm of document
reconstruction. Furthermore, the results on a variety of subtasks, encompassing
document layout analysis, text recognition, table structure recognition,
formula recognition and reading order detection, indicate that our model is
competitive and compatible with various tasks.

</details>


### [44] [Towards Solar Altitude Guided Scene Illumination](https://arxiv.org/abs/2507.05812)
*Samed Doğan,Maximilian Hoh,Nico Leuze,Nicolas R. -Peña,Alfred Schöttl*

Main category: cs.CV

TL;DR: 该研究利用太阳高度作为条件变量生成模拟摄像机传感器数据，降低人工标注需求并提升白昼变化模拟的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶研究受限于真实世界数据采集成本高、场景覆盖有限等问题，需探索生成高质量合成传感器数据的方案。

Method: 通过太阳高度这一简单可计算的变量对数据进行条件生成，并采用特殊归一化方法处理太阳高度对光照变化的敏感性。

Result: 提出的方法能够准确捕捉光照特性及光照相关的图像噪声，尤其是在扩散模型的背景下表现优秀。

Conclusion: 展示了太阳高度变量在无需大量人工标注的情况下也能提升白昼数据生成质量，解决了研究中白昼变化模拟的空白问题。

Abstract: The development of safe and robust autonomous driving functions is heavily
dependent on large-scale, high-quality sensor data. However, real-word data
acquisition demands intensive human labor and is strongly limited by factors
such as labeling cost, driver safety protocols and diverse scenario coverage.
Thus, multiple lines of work focus on the conditional generation of synthetic
camera sensor data. We identify a significant gap in research regarding daytime
variation, presumably caused by the scarcity of available labels. Consequently,
we present the solar altitude as global conditioning variable. It is readily
computable from latitude-longitude coordinates and local time, eliminating the
need for extensive manual labeling. Our work is complemented by a tailored
normalization approach, targeting the sensitivity of daylight towards small
numeric changes in altitude. We demonstrate its ability to accurately capture
lighting characteristics and illumination-dependent image noise in the context
of diffusion models.

</details>


### [45] [Empowering Bridge Digital Twins by Bridging the Data Gap with a Unified Synthesis Framework](https://arxiv.org/abs/2507.05814)
*Wang Wang,Mingyu Shi,Jun Jiang,Wenqian Ma,Chong Liu,Yasutaka Narazaki,Xuguang Wang*

Main category: cs.CV

TL;DR: 提出了一种系统框架，用于生成3D桥梁数据，解决实景点云数据存在的缺陷问题。同时，生成的数据显著提升了语义分割和补全任务的效果。


<details>
  <summary>Details</summary>
Motivation: 桥梁作为关键交通基础设施，面临老化和损耗的挑战，传统人工检查效率低下。而3D点云技术受限于实景数据不完整的问题，存在无法有效应用的瓶颈，亟待解决数据生成不足的问题。

Method: 提出了一个系统框架，可以自动生成包含组件级实例注释、高保真颜色和精确法向量的完整点云数据，同时还能生成多样化、具有物理真实性的不完整点云数据，用于支持语义分割和点云补全网络的训练。

Result: 实验结果显示，通过该框架生成的合成数据训练PointNet++模型，其在实际桥梁语义分割中的平均交并比（mIoU）达到84.2%；同时，微调后的KT-Net在组件补全任务中表现卓越。

Conclusion: 该研究提供了一种创新方法和基础数据集，可用于桥梁结构的3D可视化分析，对基础设施的自动化管理和维护具有重要意义。

Abstract: As critical transportation infrastructure, bridges face escalating challenges
from aging and deterioration, while traditional manual inspection methods
suffer from low efficiency. Although 3D point cloud technology provides a new
data-driven paradigm, its application potential is often constrained by the
incompleteness of real-world data, which results from missing labels and
scanning occlusions. To overcome the bottleneck of insufficient generalization
in existing synthetic data methods, this paper proposes a systematic framework
for generating 3D bridge data.
  This framework can automatically generate complete point clouds featuring
component-level instance annotations, high-fidelity color, and precise normal
vectors. It can be further extended to simulate the creation of diverse and
physically realistic incomplete point clouds, designed to support the training
of segmentation and completion networks, respectively. Experiments demonstrate
that a PointNet++ model trained with our synthetic data achieves a mean
Intersection over Union (mIoU) of 84.2% in real-world bridge semantic
segmentation. Concurrently, a fine-tuned KT-Net exhibits superior performance
on the component completion task.
  This research offers an innovative methodology and a foundational dataset for
the 3D visual analysis of bridge structures, holding significant implications
for advancing the automated management and maintenance of infrastructure.

</details>


### [46] [2D Instance Editing in 3D Space](https://arxiv.org/abs/2507.05819)
*Yuhuan Xie,Aoxuan Pan,Ming-Xian Lin,Wei Huang,Yi-Hua Huang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 提出了一种“2D-3D-2D”框架，将2D图像转换为3D表示以改进编辑一致性和物体特性保留。


<details>
  <summary>Details</summary>
Motivation: 现有2D生成模型存在一致性和物体身份保持方面的局限性，需改进编辑精度。

Method: 提出一种将2D图像转换为3D表示进行物理可行性编辑的框架，编辑后再将3D物体重新投影并填充至原始2D图像中。

Result: 实验表明，此框架在编辑一致性和物体身份保留上优于现有方法。

Conclusion: 通过引入3D编辑环境，有效改善了编辑一致性和物体特性保持的能力。

Abstract: Generative models have achieved significant progress in advancing 2D image
editing, demonstrating exceptional precision and realism. However, they often
struggle with consistency and object identity preservation due to their
inherent pixel-manipulation nature. To address this limitation, we introduce a
novel "2D-3D-2D" framework. Our approach begins by lifting 2D objects into 3D
representation, enabling edits within a physically plausible,
rigidity-constrained 3D environment. The edited 3D objects are then reprojected
and seamlessly inpainted back into the original 2D image. In contrast to
existing 2D editing methods, such as DragGAN and DragDiffusion, our method
directly manipulates objects in a 3D environment. Extensive experiments
highlight that our framework surpasses previous methods in general performance,
delivering highly consistent edits while robustly preserving object identity.

</details>


### [47] [Video Event Reasoning and Prediction by Fusing World Knowledge from LLMs with Vision Foundation Models](https://arxiv.org/abs/2507.05822)
*L'ea Dubois,Klaus Schmidt,Chengyu Wang,Ji-Hoon Park,Lin Wang,Santiago Munoz*

Main category: cs.CV

TL;DR: 提出一种融合视觉基础模型（VFM）与大语言模型（LLM）的框架，以提升视频理解的因果推理与未来预测能力。


<details>
  <summary>Details</summary>
Motivation: 目前视频理解模型能识别“发生了什么”，但缺乏因果推理和未来预测能力，这源于缺乏常识性世界知识。

Method: 创新性地设计了基于Q-Former的融合模块，结合视频-文本数据的对齐预训练及精细调优，强化推理和预测能力。

Result: 模型在多个基准测试中达到先进水平，并展现出对未见推理任务的零样本泛化能力。

Conclusion: 研究推进了机器从简单识别到认知理解的能力，为机器人与人机交互等领域的智能应用奠定了基础。

Abstract: Current video understanding models excel at recognizing "what" is happening
but fall short in high-level cognitive tasks like causal reasoning and future
prediction, a limitation rooted in their lack of commonsense world knowledge.
To bridge this cognitive gap, we propose a novel framework that synergistically
fuses a powerful Vision Foundation Model (VFM) for deep visual perception with
a Large Language Model (LLM) serving as a knowledge-driven reasoning core. Our
key technical innovation is a sophisticated fusion module, inspired by the
Q-Former architecture, which distills complex spatiotemporal and object-centric
visual features into a concise, language-aligned representation. This enables
the LLM to effectively ground its inferential processes in direct visual
evidence. The model is trained via a two-stage strategy, beginning with
large-scale alignment pre-training on video-text data, followed by targeted
instruction fine-tuning on a curated dataset designed to elicit advanced
reasoning and prediction skills. Extensive experiments demonstrate that our
model achieves state-of-the-art performance on multiple challenging benchmarks.
Notably, it exhibits remarkable zero-shot generalization to unseen reasoning
tasks, and our in-depth ablation studies validate the critical contribution of
each architectural component. This work pushes the boundary of machine
perception from simple recognition towards genuine cognitive understanding,
paving the way for more intelligent and capable AI systems in robotics,
human-computer interaction, and beyond.

</details>


### [48] [I$^2$R: Inter and Intra-image Refinement in Few Shot Segmentation](https://arxiv.org/abs/2507.05838)
*Ourui Fu,Hangzhou He,Xinliang Zhang,Lei Zhu,Shuang Zeng,ZhaoHeng Xie,Yanye Lu*

Main category: cs.CV

TL;DR: 提出了I^2R方法，通过类别特异的高层表示和定向掩码策略，提升小样本分割性能，在PASCAL-5$^i$ 和 COCO-20$^i$中分别提升mIoU 1.9% 和 2.1%。


<details>
  <summary>Details</summary>
Motivation: 解决语义分割中的小样本问题，即利用极少的示例快速泛化到新类别，同时改善支持图与查询图之间的语义间隙和视觉相似性问题。

Method: 提出了一种名为I^2R的方法，包含两大关键：1. 使用类别特异的高层表示整合支持图和查询图的全局语义提示；2. 设计了定向掩码策略以抑制特征相似但掩码冲突的像素对。

Result: 实验表明，提出的方法在PASCAL-5$^i$和COCO-20$^i$基准测试的1-shot设置下，分别将mIoU提升了1.9%和2.1%。

Conclusion: I^2R方法克服了当前方法的关键局限，改进了小样本语义分割的表现，具有良好的泛化能力和精度提升效果。

Abstract: The annotation bottleneck in semantic segmentation has driven significant
interest in few-shot segmentation, which aims to develop segmentation models
capable of generalizing rapidly to novel classes using minimal exemplars.
Conventional training paradigms typically generate query prior maps by
extracting masked-area features from support images, followed by making
predictions guided by these prior maps. However, current approaches remain
constrained by two critical limitations stemming from inter- and intra-image
discrepancies, both of which significantly degrade segmentation performance: 1)
The semantic gap between support and query images results in mismatched
features and inaccurate prior maps; 2) Visually similar yet semantically
distinct regions within support or query images lead to false negative or false
positive predictions. We propose a novel FSS method called \textbf{I$^2$R}: 1)
Using category-specific high level representations which aggregate global
semantic cues from support and query images, enabling more precise inter-image
region localization and address the first limitation. 2) Directional masking
strategy that suppresses inconsistent support-query pixel pairs, which exhibit
high feature similarity but conflicting mask, to mitigate the second issue.
Experiments demonstrate that our method outperforms state-of-the-art
approaches, achieving improvements of 1.9\% and 2.1\% in mIoU under the 1-shot
setting on PASCAL-5$^i$ and COCO-20$^i$ benchmarks, respectively.

</details>


### [49] [USIGAN: Unbalanced Self-Information Feature Transport for Weakly Paired Image IHC Virtual Staining](https://arxiv.org/abs/2507.05843)
*Yue Peng,Bing Xiong,Fuqiang Chen,De Eybo,RanRan Zhang,Wanming Hu,Jing Cai,Wenjian Qin*

Main category: cs.CV

TL;DR: 本文提出了一种名为USIGAN的新方法，用于从H&E图像生成虚拟IHC图像，同时提升内容一致性和病理语义一致性，并在公开数据集上取得了优异结果。


<details>
  <summary>Details</summary>
Motivation: 为了解决弱配对条件下的空间异质性对生成路径一致性及病理语义一致性的影响，提出使用无依赖位置对应的全局形态语义提取方法。

Method: 设计了不平衡自信息特征传输的生成对抗网络USIGAN，并且引入了UOT-CTM和PC-SCM机制，分别在图像级和组内级别进行一致性增强。

Result: 实验表明，提出的方法在IoD和Pearson-R相关性等多个临床显著性指标上表现优越，生成结果具有更高的临床相关性。

Conclusion: 通过减少弱配对项对联合分布的影响，显著提高了生成结果的内容和病理语义一致性，这为病理分析提供了高效且低成本的解决方案。

Abstract: Immunohistochemical (IHC) virtual staining is a task that generates virtual
IHC images from H\&E images while maintaining pathological semantic consistency
with adjacent slices. This task aims to achieve cross-domain mapping between
morphological structures and staining patterns through generative models,
providing an efficient and cost-effective solution for pathological analysis.
However, under weakly paired conditions, spatial heterogeneity between adjacent
slices presents significant challenges. This can lead to inaccurate one-to-many
mappings and generate results that are inconsistent with the pathological
semantics of adjacent slices. To address this issue, we propose a novel
unbalanced self-information feature transport for IHC virtual staining, named
USIGAN, which extracts global morphological semantics without relying on
positional correspondence.By removing weakly paired terms in the joint marginal
distribution, we effectively mitigate the impact of weak pairing on joint
distributions, thereby significantly improving the content consistency and
pathological semantic consistency of the generated results. Moreover, we design
the Unbalanced Optimal Transport Consistency (UOT-CTM) mechanism and the
Pathology Self-Correspondence (PC-SCM) mechanism to construct correlation
matrices between H\&E and generated IHC in image-level and real IHC and
generated IHC image sets in intra-group level.. Experiments conducted on two
publicly available datasets demonstrate that our method achieves superior
performance across multiple clinically significant metrics, such as IoD and
Pearson-R correlation, demonstrating better clinical relevance.

</details>


### [50] [DFYP: A Dynamic Fusion Framework with Spectral Channel Attention and Adaptive Operator learning for Crop Yield Prediction](https://arxiv.org/abs/2507.05849)
*Juli Zhang,Zeyu Yan,Jing Zhang,Qiguang Miao,Quan Wang*

Main category: cs.CV

TL;DR: 本文提出了一个名为DFYP的新框架，通过结合光谱通道注意力、边缘自适应空间建模和可学习的融合机制，旨在提高作物产量预测的鲁棒性和精确度。


<details>
  <summary>Details</summary>
Motivation: 作物产量预测面临复杂的空间模式、异质的光谱特性和动态农业条件等挑战，而现有方法在空间建模容量和跨作物年份的泛化能力方面存在不足。

Method: DFYP引入了三个关键模块：(1) 分辨率感知通道注意模块(RCA)，通过自适应重加权输入通道增强光谱表示；(2) 自适应算子学习网络(AOL-Net)，动态选择卷积算子以改进边缘敏感的空间特征提取；(3) 双分支架构带有可学习融合机制，联合建模局部空间细节和全局上下文信息。

Result: 实验表明，DFYP在包括RMSE、MAE和R2在内的多个指标上优于当前最先进的基线方法，并在不同的空间分辨率、作物类型和时间段上表现出优越性。

Conclusion: DFYP展示了其在真实农业监测中的有效性和鲁棒性，是解决现有作物产量预测挑战的创新方法。

Abstract: Accurate remote sensing-based crop yield prediction remains a fundamental
challenging task due to complex spatial patterns, heterogeneous spectral
characteristics, and dynamic agricultural conditions. Existing methods often
suffer from limited spatial modeling capacity, weak generalization across crop
types and years. To address these challenges, we propose DFYP, a novel Dynamic
Fusion framework for crop Yield Prediction, which combines spectral channel
attention, edge-adaptive spatial modeling and a learnable fusion mechanism to
improve robustness across diverse agricultural scenarios. Specifically, DFYP
introduces three key components: (1) a Resolution-aware Channel Attention (RCA)
module that enhances spectral representation by adaptively reweighting input
channels based on resolution-specific characteristics; (2) an Adaptive Operator
Learning Network (AOL-Net) that dynamically selects operators for convolutional
kernels to improve edge-sensitive spatial feature extraction under varying crop
and temporal conditions; and (3) a dual-branch architecture with a learnable
fusion mechanism, which jointly models local spatial details and global
contextual information to support cross-resolution and cross-crop
generalization. Extensive experiments on multi-year datasets MODIS and
multi-crop dataset Sentinel-2 demonstrate that DFYP consistently outperforms
current state-of-the-art baselines in RMSE, MAE, and R2 across different
spatial resolutions, crop types, and time periods, showcasing its effectiveness
and robustness for real-world agricultural monitoring.

</details>


### [51] [D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for Free-Viewpoint Videos](https://arxiv.org/abs/2507.05859)
*Wenkang Zhang,Yan Zhao,Qiang Wang,Li Song,Zhengxue Cheng*

Main category: cs.CV

TL;DR: 该研究提出一种新的动态高斯散粒动态序列压缩框架D-FCGS，解决了现有方法普适性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有动态3D高斯散粒（3DGS）压缩方法普适性差，无法满足高效存储和传输要求。

Method: 提出了一种基于I-P帧结构的前馈框架，结合稀疏控制点提取的运动矢量及双重先验熵模型对数据进行高效压缩，同时利用运动补偿与精细重建网络提升视角一致性。

Result: 实验结果表明，相较于基于优化的方法，新方法在视觉质量保持的基础上实现了40多倍的压缩速度，且无需逐场景优化。

Conclusion: 此研究为动态3DGS的前馈压缩提供了新范式，有助于沉浸式自由视点视频在传输和存储中的扩展性。

Abstract: Free-viewpoint video (FVV) enables immersive 3D experiences, but efficient
compression of dynamic 3D representations remains a major challenge. Recent
advances in 3D Gaussian Splatting (3DGS) and its dynamic extensions have
enabled high-fidelity scene modeling. However, existing methods often couple
scene reconstruction with optimization-dependent coding, which limits
generalizability. This paper presents Feedforward Compression of Dynamic
Gaussian Splatting (D-FCGS), a novel feedforward framework for compressing
temporally correlated Gaussian point cloud sequences. Our approach introduces a
Group-of-Frames (GoF) structure with I-P frame coding, where inter-frame
motions are extracted via sparse control points. The resulting motion tensors
are compressed in a feedforward manner using a dual prior-aware entropy model
that combines hyperprior and spatial-temporal priors for accurate rate
estimation. For reconstruction, we perform control-point-guided motion
compensation and employ a refinement network to enhance view-consistent
fidelity. Trained on multi-view video-derived Gaussian frames, D-FCGS
generalizes across scenes without per-scene optimization. Experiments show that
it matches the rate-distortion performance of optimization-based methods,
achieving over 40 times compression in under 2 seconds while preserving visual
quality across viewpoints. This work advances feedforward compression for
dynamic 3DGS, paving the way for scalable FVV transmission and storage in
immersive applications.

</details>


### [52] [GeoMag: A Vision-Language Model for Pixel-level Fine-Grained Remote Sensing Image Parsing](https://arxiv.org/abs/2507.05887)
*Xianzhi Ma,Jianhui Li,Changhua Pei,Hao Liu*

Main category: cs.CV

TL;DR: 提出了一种名为GeoMag的框架解决了RS-VLMs在像素级任务处理和计算资源消耗上的局限性，显著提高了性能并节省了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有RS-VLMs在像素级任务处理、小目标识别以及高分辨率影像分析时存在能力不足和效率问题，限制了其实用性。

Method: 通过任务驱动的多粒度分辨率调整（TMRA）和基于提示的语义感知裁剪（PSC），动态调整关注范围，以减少无关区域的空间分辨率，同时加强重要目标区域的视觉表示。

Result: GeoMag在10个基准测试中表现优异，不仅在像素级任务上性能卓越，还在其他粒度任务上保持了竞争力。

Conclusion: GeoMag为遥感图像解析任务提供了高效、全能的解决方案，解决了当前RS-VLMs的局限性，在多个任务层次上取得了平衡的性能与效率。

Abstract: The application of Vision-Language Models (VLMs) in remote sensing (RS) image
understanding has achieved notable progress, demonstrating the basic ability to
recognize and describe geographical entities. However, existing RS-VLMs are
mostly limited to image-level and region-level tasks, lacking the capability to
handle pixel-level tasks and performing poorly in small-object recognition
scenarios. Moreover, RS-VLMs consume significant computational resources when
processing high-resolution RS images, further restricting their practical
applicability. In this context, we propose GeoMag (Geographical Magnifier), an
end-to-end general-purpose large model framework for RS. GeoMag dynamically
focuses the attention scope based on prompt semantics to effectively perform
remote sensing image parsing across multiple levels of granularity. This method
introduces Task-driven Multi-granularity Resolution Adjustment (TMRA) and
Prompt-guided Semantic-aware Cropping (PSC), which adaptively reduce the
spatial resolution of task-irrelevant regions while enhancing the visual
representation of task-relevant areas. This approach improves the model's
perception of critical target regions, suppresses background redundancy, and
reduces the computational cost of interpreting high-resolution RS imagery.
Extensive comparative experiments on 10 benchmarks demonstrate that GeoMag not
only excels in handling pixel-level tasks but also maintains competitive
performance across tasks of other granularities compared to existing RS-VLMs.

</details>


### [53] [What You Have is What You Track: Adaptive and Robust Multimodal Tracking](https://arxiv.org/abs/2507.05899)
*Yuedong Tan,Jiawei Shao,Eduard Zamfir,Ruanjun Li,Zhaochong An,Chao Ma,Danda Paudel,Luc Van Gool,Radu Timofte,Zongwei Wu*

Main category: cs.CV

TL;DR: 本文研究了多模态数据在出现时间不同步和数据丢失情况下的视觉跟踪问题，提出了一种适应性强的跟踪框架，在9个基准上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有跟踪器难以在多模态数据存在时间不完整情况下保持性能，该领域目前研究不足，需要寻求适应性更强的方法。

Method: 提出了一个灵活的跟踪框架，采用了异构专家混合融合机制和视频级掩码策略，以实现动态适应缺失数据率，并保证时空一致性。

Result: 框架在9个基准上表现优异，不仅适应了多模态数据缺失率的变化，还根据场景复杂度进行了调整，取得了SOTA性能。

Conclusion: 该研究在多模态跟踪领域具有重要意义，提出的框架在多模态数据部分缺失情况下表现尤为优异，代码和基准将公开以供社区使用。

Abstract: Multimodal data is known to be helpful for visual tracking by improving
robustness to appearance variations. However, sensor synchronization challenges
often compromise data availability, particularly in video settings where
shortages can be temporal. Despite its importance, this area remains
underexplored. In this paper, we present the first comprehensive study on
tracker performance with temporally incomplete multimodal data. Unsurprisingly,
under such a circumstance, existing trackers exhibit significant performance
degradation, as their rigid architectures lack the adaptability needed to
effectively handle missing modalities. To address these limitations, we propose
a flexible framework for robust multimodal tracking. We venture that a tracker
should dynamically activate computational units based on missing data rates.
This is achieved through a novel Heterogeneous Mixture-of-Experts fusion
mechanism with adaptive complexity, coupled with a video-level masking strategy
that ensures both temporal consistency and spatial completeness which is
critical for effective video tracking. Surprisingly, our model not only adapts
to varying missing rates but also adjusts to scene complexity. Extensive
experiments show that our model achieves SOTA performance across 9 benchmarks,
excelling in both conventional complete and missing modality settings. The code
and benchmark will be publicly available at
https://github.com/supertyd/FlexTrack/tree/main.

</details>


### [54] [On the Effectiveness of Methods and Metrics for Explainable AI in Remote Sensing Image Scene Classification](https://arxiv.org/abs/2507.05916)
*Jonas Klotz,Tom Burgert,Begüm Demir*

Main category: cs.CV

TL;DR: 本文分析了十种解释性指标和五种特征归因方法在遥感场景分类中的适用性及其局限性，并提供了选择方法和指标的指导。


<details>
  <summary>Details</summary>
Motivation: 大多数xAI方法和相关评估指标为自然图像设计，直接应用于遥感可能不适合，需针对遥感场景分类验证其有效性。

Method: 对十种解释指标（五个类别）与五种特征归因方法（如Occlusion、LIME、GradCAM等）在三个遥感数据集上的表现进行了方法论和实验分析。

Result: 发现遮挡和LIME等基于扰动的方法受场景时空特性影响较大，GradCAM等梯度方法在多标签场景下表现欠佳，部分传播方法（如LRP）在类的空间分布上存在不均衡问题；忠实性指标与扰动方法类似存在问题，而稳健性和随机化指标相较更为可靠。

Conclusion: 基于分析提供了针对遥感图像场景分类任务中选择解释方法和指标的指导建议。

Abstract: The development of explainable artificial intelligence (xAI) methods for
scene classification problems has attracted great attention in remote sensing
(RS). Most xAI methods and the related evaluation metrics in RS are initially
developed for natural images considered in computer vision (CV), and their
direct usage in RS may not be suitable. To address this issue, in this paper,
we investigate the effectiveness of explanation methods and metrics in the
context of RS image scene classification. In detail, we methodologically and
experimentally analyze ten explanation metrics spanning five categories
(faithfulness, robustness, localization, complexity, randomization), applied to
five established feature attribution methods (Occlusion, LIME, GradCAM, LRP,
and DeepLIFT) across three RS datasets. Our methodological analysis identifies
key limitations in both explanation methods and metrics. The performance of
perturbation-based methods, such as Occlusion and LIME, heavily depends on
perturbation baselines and spatial characteristics of RS scenes. Gradient-based
approaches like GradCAM struggle when multiple labels are present in the same
image, while some relevance propagation methods (LRP) can distribute relevance
disproportionately relative to the spatial extent of classes. Analogously, we
find limitations in evaluation metrics. Faithfulness metrics share the same
problems as perturbation-based methods. Localization metrics and complexity
metrics are unreliable for classes with a large spatial extent. In contrast,
robustness metrics and randomization metrics consistently exhibit greater
stability. Our experimental results support these methodological findings.
Based on our analysis, we provide guidelines for selecting explanation methods,
metrics, and hyperparameters in the context of RS image scene classification.

</details>


### [55] [High-Resolution Visual Reasoning via Multi-Turn Grounding-Based Reinforcement Learning](https://arxiv.org/abs/2507.05920)
*Xinyu Huang,Yuhao Dong,Weiwei Tian,Bo Li,Rui Feng,Ziwei Liu*

Main category: cs.CV

TL;DR: 提出了MGPO方法，通过多轮对话框架和无监督的强化学习优化LMMs的视觉区域聚焦，显著提高了模型的OOB能力。


<details>
  <summary>Details</summary>
Motivation: 目前LMMs在处理高分辨率图像时面临大量不相关视觉信息的干扰，且监督式微调成本高昂，需要寻找一种高效方法提升模型的视觉聚焦能力。

Method: 通过MGPO框架，采用强化学习策略优化LMMs，让其通过多轮对话自动裁剪和定位重要的视觉区域，使用的奖励函数基于最终答案的正确性且无需标注数据支持。

Result: MGPO在标准无标注视觉问答数据上训练后，相较GRPO在MME-Realworld上提升5.4%，在更具挑战的OOD V* Bench上提升5.2%。基于Qwen2.5-VL-7B后训练后模型性能超越OpenAI的o1和GPT-4o模型。

Conclusion: MGPO方法证明了强化学习能够有效提升LMMs的视觉聚焦性能，其在无标注情况下的表现优于现有方法，具有显著的研究与应用价值。

Abstract: State-of-the-art large multi-modal models (LMMs) face challenges when
processing high-resolution images, as these inputs are converted into enormous
visual tokens, many of which are irrelevant to the downstream task. In this
paper, we propose Multi-turn Grounding-based Policy Optimization (MGPO), an
end-to-end reinforcement learning (RL) framework that enables LMMs to
iteratively focus on key visual regions by automatically cropping sub-images,
based on model-predicted grounding coordinates within a multi-turn conversation
framework. Compared to supervised fine-tuning (SFT), which requires costly
additional grounding annotations, our approach highlights that LMMs can emerge
robust grounding abilities during the RL training process, leveraging only a
binary reward function derived from the correctness of the final answer.
Additionally, we observe that LMMs struggle to autonomously trigger visual
grounding during the rollout process. To address this cold start problem, we
design a multi-turn conversational template and restrict policy loss
computation to model outputs generated across multiple dialogue rounds, thereby
promoting stable optimization. Extensive experiments demonstrate that, when
trained on standard visual-question-short answering data without grounding
annotations, MGPO effectively elicits stronger grounding capabilities compared
to GRPO, leading to 5.4\% improvement on in-distribution MME-Realworld and
5.2\% improvement on the challenging out-of-distribution (OOD) V* Bench.
Notably, MGPO post-training on Qwen2.5-VL-7B with 21K samples surpasses
OpenAI's o1 and GPT-4o models on the OOD V* Bench. Codes are available at
https://github.com/EvolvingLMMs-Lab/MGPO.

</details>


### [56] [Beyond Appearance: Geometric Cues for Robust Video Instance Segmentation](https://arxiv.org/abs/2507.05948)
*Quanzhu Niu,Yikang Zhou,Shihao Chen,Tao Zhang,Shunping Ji*

Main category: cs.CV

TL;DR: 本文提出利用几何意识（depth cues）通过单目深度估计增强视频实例分割（VIS）鲁棒性，同时研究了三个集成方法：EDC、SV和DS，其中EDC和SV显著提升了VIS性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频实例分割方法面临目标遮挡、运动模糊及外观变化的挑战，亟需改进实例分割在视频理解中的鲁棒性。

Method: 提出三种利用深度估计的方法：EDC将深度图作为输入通道，SV共享一个ViT骨干网络用于深度估计和分割分支，DS将深度预测作为训练辅助监督。

Result: 实验表明EDC和SV显著提升VIS性能，其中EDC在Swin-L骨干网络下在OVIS基准上达到了新的56.2 AP的最新结果。

Conclusion: 验证了深度线索对于视频实例分割任务中增强视频理解的重要性，并提出了多个行之有效的结合深度估计的解决方案。

Abstract: Video Instance Segmentation (VIS) fundamentally struggles with pervasive
challenges including object occlusions, motion blur, and appearance variations
during temporal association. To overcome these limitations, this work
introduces geometric awareness to enhance VIS robustness by strategically
leveraging monocular depth estimation. We systematically investigate three
distinct integration paradigms. Expanding Depth Channel (EDC) method
concatenates the depth map as input channel to segmentation networks; Sharing
ViT (SV) designs a uniform ViT backbone, shared between depth estimation and
segmentation branches; Depth Supervision (DS) makes use of depth prediction as
an auxiliary training guide for feature learning. Though DS exhibits limited
effectiveness, benchmark evaluations demonstrate that EDC and SV significantly
enhance the robustness of VIS. When with Swin-L backbone, our EDC method gets
56.2 AP, which sets a new state-of-the-art result on OVIS benchmark. This work
conclusively establishes depth cues as critical enablers for robust video
understanding.

</details>


### [57] [High-Fidelity and Generalizable Neural Surface Reconstruction with Sparse Feature Volumes](https://arxiv.org/abs/2507.05952)
*Aoxiang Fan,Corentin Dumery,Nicolas Talabot,Hieu Le,Pascal Fua*

Main category: cs.CV

TL;DR: 提出了一种基于稀疏表示的新方法，用于改进从少量图像中进行3D表面重建的精度，并显著降低存储需求。


<details>
  <summary>Details</summary>
Motivation: 当前3D表面重建方法受限于密集表示在高分辨率下的内存需求问题，难以实现高质量的重建。

Method: 采用两阶段方法：第一阶段训练网络预测体素占用；第二阶段仅对高占用估计体素计算特征并进行体积渲染，同时开发了支持稀疏表示的高效采样、特征聚合和查询算法。

Result: 所提方法在存储需求减少50倍的情况下，无性能降低，支持在标准硬件上实现512³分辨率的重建，并优于当前状态最先进方法。

Conclusion: 稀疏表示的引入不仅提升了硬件资源利用效率，还实现了更高分辨率和更准的3D重建，为相关领域提供了兼具有效性和高效性的解决方案。

Abstract: Generalizable neural surface reconstruction has become a compelling technique
to reconstruct from few images without per-scene optimization, where dense 3D
feature volume has proven effective as a global representation of scenes.
However, the dense representation does not scale well to increasing voxel
resolutions, severely limiting the reconstruction quality. We thus present a
sparse representation method, that maximizes memory efficiency and enables
significantly higher resolution reconstructions on standard hardware. We
implement this through a two-stage approach: First training a network to
predict voxel occupancies from posed images and associated depth maps, then
computing features and performing volume rendering only in voxels with
sufficiently high occupancy estimates. To support this sparse representation,
we developed custom algorithms for efficient sampling, feature aggregation, and
querying from sparse volumes-overcoming the dense-volume assumptions inherent
in existing works. Experiments on public datasets demonstrate that our approach
reduces storage requirements by more than 50 times without performance
degradation, enabling reconstructions at $512^3$ resolution compared to the
typical $128^3$ on similar hardware, and achieving superior reconstruction
accuracy over current state-of-the-art methods.

</details>


### [58] [Tora2: Motion and Appearance Customized Diffusion Transformer for Multi-Entity Video Generation](https://arxiv.org/abs/2507.05963)
*Zhenghao Zhang,Junchao Liao,Xiangyu Meng,Long Qin,Weizhi Wang*

Main category: cs.CV

TL;DR: Tora2是一种改进的运动引导视频生成模型，支持多实体的外观和运动自定义。


<details>
  <summary>Details</summary>
Motivation: 提升视频生成中对外观和运动的个性化程度，解决多模态条件对齐的挑战。

Method: 引入解耦的个性化提取器、门控自注意力机制及对比损失函数，同时进行外观与运动的联合优化。

Result: 实验表明，Tora2在多条件视频生成中性能优异，提供更高的动作控制能力。

Conclusion: Tora2实现了多实体外观与运动同时自定义，推动了多条件视频生成的进步。

Abstract: Recent advances in diffusion transformer models for motion-guided video
generation, such as Tora, have shown significant progress. In this paper, we
present Tora2, an enhanced version of Tora, which introduces several design
improvements to expand its capabilities in both appearance and motion
customization. Specifically, we introduce a decoupled personalization extractor
that generates comprehensive personalization embeddings for multiple open-set
entities, better preserving fine-grained visual details compared to previous
methods. Building on this, we design a gated self-attention mechanism to
integrate trajectory, textual description, and visual information for each
entity. This innovation significantly reduces misalignment in multimodal
conditioning during training. Moreover, we introduce a contrastive loss that
jointly optimizes trajectory dynamics and entity consistency through explicit
mapping between motion and personalization embeddings. Tora2 is, to our best
knowledge, the first method to achieve simultaneous multi-entity customization
of appearance and motion for video generation. Experimental results demonstrate
that Tora2 achieves competitive performance with state-of-the-art customization
methods while providing advanced motion control capabilities, which marks a
critical advancement in multi-condition video generation. Project page:
https://github.com/alibaba/Tora .

</details>


### [59] [T-LoRA: Single Image Diffusion Model Customization Without Overfitting](https://arxiv.org/abs/2507.05964)
*Vera Soboleva,Aibek Alanov,Andrey Kuznetsov,Konstantin Sobolev*

Main category: cs.CV

TL;DR: 本文提出了一种名为T-LoRA的时间步长相关低秩适配框架，用于解决扩散模型在单图像定制时过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 针对扩散模型在样本有限情况下个性化定制时的过拟合和泛化能力下降问题，特别是单图像定制场景下的问题。

Method: 提出T-LoRA框架，结合动态时间步长调整的低秩更新策略和独立适配器组件的权重参数化技术。

Result: 实验表明，T-LoRA相较于LoRA及其他个性化技术在概念保真度和文本对齐平衡上表现更佳。

Conclusion: T-LoRA框架能在数据有限和资源受限的情况下，为扩散模型的个性化定制提供有效解决方案。

Abstract: While diffusion model fine-tuning offers a powerful approach for customizing
pre-trained models to generate specific objects, it frequently suffers from
overfitting when training samples are limited, compromising both generalization
capability and output diversity. This paper tackles the challenging yet most
impactful task of adapting a diffusion model using just a single concept image,
as single-image customization holds the greatest practical potential. We
introduce T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework
specifically designed for diffusion model personalization. In our work we show
that higher diffusion timesteps are more prone to overfitting than lower ones,
necessitating a timestep-sensitive fine-tuning strategy. T-LoRA incorporates
two key innovations: (1) a dynamic fine-tuning strategy that adjusts
rank-constrained updates based on diffusion timesteps, and (2) a weight
parametrization technique that ensures independence between adapter components
through orthogonal initialization. Extensive experiments show that T-LoRA and
its individual components outperform standard LoRA and other diffusion model
personalization techniques. They achieve a superior balance between concept
fidelity and text alignment, highlighting the potential of T-LoRA in
data-limited and resource-constrained scenarios. Code is available at
https://github.com/ControlGenAI/T-LoRA.

</details>


### [60] [Automatic Synthesis of High-Quality Triplet Data for Composed Image Retrieval](https://arxiv.org/abs/2507.05970)
*Haiwen Li,Delong Liu,Zhaohui Hou,Zhicheng Zhao,Fei Su*

Main category: cs.CV

TL;DR: 提出了一种自动生成三元组数据和完全合成数据集CIRHS的管道，以及新框架CoAlign，在大幅提高CIR任务性能的同时实现了零样本学习。


<details>
  <summary>Details</summary>
Motivation: 现有CIR方法依赖昂贵的人工标注三元组数据，难以实现扩展性和零样本能力。

Method: 开发了一个利用大语言模型生成提示，控制文本到图像生成模型生成图像对的管道，并构建了完全合成的CIRHS数据集。同时提出了结合全局对齐和局部推理的新框架CoAlign。

Result: CoAlign在零样本学习下于三种常用基准数据集上表现突出。同时在监督训练下超越了所有最新最优的CIR方法。

Conclusion: 首次验证了在完全合成数据集上训练CIR模型的可行性，并证明了所提框架在提取任务中的有效性。

Abstract: As a challenging vision-language (VL) task, Composed Image Retrieval (CIR)
aims to retrieve target images using multimodal (image+text) queries. Although
many existing CIR methods have attained promising performance, their reliance
on costly, manually labeled triplets hinders scalability and zero-shot
capability. To address this issue, we propose a scalable pipeline for automatic
triplet generation, along with a fully synthetic dataset named Composed Image
Retrieval on High-quality Synthetic Triplets (CIRHS). Our pipeline leverages a
large language model (LLM) to generate diverse prompts, controlling a
text-to-image generative model to produce image pairs with identical elements
in each pair, which are then filtered and reorganized to form the CIRHS
dataset. In addition, we introduce Hybrid Contextual Alignment (CoAlign), a
novel CIR framework, which can accomplish global alignment and local reasoning
within a broader context, enabling the model to learn more robust and
informative representations. By utilizing the synthetic CIRHS dataset, CoAlign
achieves outstanding zero-shot performance on three commonly used benchmarks,
demonstrating for the first time the feasibility of training CIR models on a
fully synthetic dataset. Furthermore, under supervised training, our method
outperforms all the state-of-the-art supervised CIR approaches, validating the
effectiveness of our proposed retrieval framework. The code and the CIRHS
dataset will be released soon.

</details>


### [61] [Exploring Partial Multi-Label Learning via Integrating Semantic Co-occurrence Knowledge](https://arxiv.org/abs/2507.05992)
*Xin Wu,Fei Teng,Yue Feng,Kaibo Shi,Zhuosheng Lin,Ji Zhang,James Wang*

Main category: cs.CV

TL;DR: SCINet是一种新的部分多标签学习框架，通过匹配标签与实例的共现模式解决数据不完全标注问题，并在多个基准数据集上表现超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决部分多标签学习中的标签和实例关系模糊问题，强调匹配共现模式的关键性。

Method: 提出SCINet框架，包括双主导提示模块、跨模态融合模块以及内在语义增强策略，结合多模态模型捕捉语义与实例-标签之间的依赖关系。

Result: 在四个常用基准数据集上，SCINet表现优于现有最先进方法。

Conclusion: SCINet通过对标签-实例关系的深度建模，有效解决了部分多标签学习中的挑战，提升了性能。

Abstract: Partial multi-label learning aims to extract knowledge from incompletely
annotated data, which includes known correct labels, known incorrect labels,
and unknown labels. The core challenge lies in accurately identifying the
ambiguous relationships between labels and instances. In this paper, we
emphasize that matching co-occurrence patterns between labels and instances is
key to addressing this challenge. To this end, we propose Semantic
Co-occurrence Insight Network (SCINet), a novel and effective framework for
partial multi-label learning. Specifically, SCINet introduces a bi-dominant
prompter module, which leverages an off-the-shelf multimodal model to capture
text-image correlations and enhance semantic alignment. To reinforce
instance-label interdependencies, we develop a cross-modality fusion module
that jointly models inter-label correlations, inter-instance relationships, and
co-occurrence patterns across instance-label assignments. Moreover, we propose
an intrinsic semantic augmentation strategy that enhances the model's
understanding of intrinsic data semantics by applying diverse image
transformations, thereby fostering a synergistic relationship between label
confidence and sample difficulty. Extensive experiments on four widely-used
benchmark datasets demonstrate that SCINet surpasses state-of-the-art methods.

</details>


### [62] [Ensemble-Based Deepfake Detection using State-of-the-Art Models with Robust Cross-Dataset Generalisation](https://arxiv.org/abs/2507.05996)
*Haroon Wahab,Hassan Ugail,Lujain Jaleel*

Main category: cs.CV

TL;DR: 该研究提出一种基于集成方法的深伪检测模型，解决其在分布外数据上的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 旨在提高深伪检测模型在不同数据集间的泛化能力，因为现有模型常在分布外数据上表现不佳。

Method: 基于集成方法，将不同顶级会议提出的不对称模型的预测结果概率结合分析，测试跨两个分布外数据集。

Result: 单一模型在各种条件下无法始终表现最佳，而基于集成的预测方法提供了更加稳定和可靠的性能。

Conclusion: 不对称集成是一种在现实世界深伪检测任务中具有鲁棒性和可扩展性的有效解决方案，适用于无法预知伪造类型或质量的场景。

Abstract: Machine learning-based Deepfake detection models have achieved impressive
results on benchmark datasets, yet their performance often deteriorates
significantly when evaluated on out-of-distribution data. In this work, we
investigate an ensemble-based approach for improving the generalization of
deepfake detection systems across diverse datasets. Building on a recent
open-source benchmark, we combine prediction probabilities from several
state-of-the-art asymmetric models proposed at top venues. Our experiments span
two distinct out-of-domain datasets and demonstrate that no single model
consistently outperforms others across settings. In contrast, ensemble-based
predictions provide more stable and reliable performance in all scenarios. Our
results suggest that asymmetric ensembling offers a robust and scalable
solution for real-world deepfake detection where prior knowledge of forgery
type or quality is often unavailable.

</details>


### [63] [Geo-Registration of Terrestrial LiDAR Point Clouds with Satellite Images without GNSS](https://arxiv.org/abs/2507.05999)
*Xinyu Wang,Muhammad Ibrahim,Atif Mansoor,Ajmal Mian*

Main category: cs.CV

TL;DR: 提出了一种将3D点云与卫星图像对齐的地理配准方法，解决了GNSS信号受阻导致的定位误差问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖GNSS和IMU数据，但在高楼密集的城市环境中易出现定位误差，亟需解决。

Method: 采用预训练的Point Transformer模型分割道路点，从点云与目标地图中提取道路骨架及交叉点，进行全局刚性配准与径向基函数插值精细化，并应用地形信息进行高程校正。

Result: KITTI和Perth数据集测试表现出显著改进，分别在平面及高程对齐上达到了55.3%和77.4%的提升。

Conclusion: 该方法在无GNSS信息情况下有效处理点云地理配准并提升对齐准确度，可用于城市级别3D地图重建。

Abstract: Accurate geo-registration of LiDAR point clouds presents significant
challenges in GNSS signal denied urban areas with high-rise buildings and
bridges. Existing methods typically rely on real-time GNSS and IMU data, that
require pre-calibration and assume stable positioning during data collection.
However, this assumption often fails in dense urban areas, resulting in
localization errors. To address this, we propose a structured geo-registration
and spatial correction method that aligns 3D point clouds with satellite
images, enabling frame-wise recovery of GNSS information and reconstruction of
city scale 3D maps without relying on prior localization. The proposed approach
employs a pre-trained Point Transformer model to segment the road points and
then extracts the road skeleton and intersection points from the point cloud as
well as the target map for alignment. Global rigid alignment of the two is
performed using the intersection points, followed by local refinement using
radial basis function (RBF) interpolation. Elevation correction is then applied
to the point cloud based on terrain information from SRTM dataset to resolve
vertical discrepancies. The proposed method was tested on the popular KITTI
benchmark and a locally collected Perth (Western Australia) CBD dataset. On the
KITTI dataset, our method achieved an average planimetric alignment standard
deviation (STD) of 0.84~m across sequences with intersections, representing a
55.3\% improvement over the original dataset. On the Perth dataset, which lacks
GNSS information, our method achieved an average STD of 0.96~m compared to the
GPS data extracted from Google Maps API. This corresponds to a 77.4\%
improvement from the initial alignment. Our method also resulted in elevation
correlation gains of 30.5\% on the KITTI dataset and 50.4\% on the Perth
dataset.

</details>


### [64] [TextPixs: Glyph-Conditioned Diffusion with Character-Aware Attention and OCR-Guided Supervision](https://arxiv.org/abs/2507.06033)
*Syeda Anshrah Gillani,Mirza Samad Ahmed Baig,Osama Ahmed Khan,Shahid Munir Shah,Umema Mujeeb,Maheen Ali*

Main category: cs.CV

TL;DR: 提出了一种名为GCDA框架，通过改进扩散模型提升生成图像中文字的可读性和准确性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽能生成高质量图像，但对图中文字生成能力不足，限制了其实际应用。

Method: 引入GCDA框架，包括双流文本编码器、字符感知注意机制及OCR循环微调阶段。

Result: 在MARIO-10M和T2I-CompBench数据集上达到了新的性能表现，字符错误率从0.21降至0.08，单词错误率从0.25降至0.15，FID达到14.3。

Conclusion: GCDA框架提升了现有扩散模型的字符呈现能力，并保持了整体图像生成的高质量。

Abstract: The modern text-to-image diffusion models boom has opened a new era in
digital content production as it has proven the previously unseen ability to
produce photorealistic and stylistically diverse imagery based on the semantics
of natural-language descriptions. However, the consistent disadvantage of these
models is that they cannot generate readable, meaningful, and correctly spelled
text in generated images, which significantly limits the use of practical
purposes like advertising, learning, and creative design. This paper introduces
a new framework, namely Glyph-Conditioned Diffusion with Character-Aware
Attention (GCDA), using which a typical diffusion backbone is extended by three
well-designed modules. To begin with, the model has a dual-stream text encoder
that encodes both semantic contextual information and explicit glyph
representations, resulting in a character-aware representation of the input
text that is rich in nature. Second, an attention mechanism that is aware of
the character is proposed with a new attention segregation loss that aims to
limit the attention distribution of each character independently in order to
avoid distortion artifacts. Lastly, GCDA has an OCR-in-the-loop fine-tuning
phase, where a full text perceptual loss, directly optimises models to be
legible and accurately spell. Large scale experiments to benchmark datasets,
such as MARIO-10M and T2I-CompBench, reveal that GCDA sets a new
state-of-the-art on all metrics, with better character based metrics on text
rendering (Character Error Rate: 0.08 vs 0.21 for the previous best; Word Error
Rate: 0.15 vs 0.25), human perception, and comparable image synthesis quality
on high-fidelity (FID: 14.3).

</details>


### [65] [VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis](https://arxiv.org/abs/2507.06060)
*Alexandre Symeonidis-Herzig,Özge Mercanoğlu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: VisualSpeaker通过引入感知唇读损失与视觉语音识别结合，以改进3D面部动画的逼真度与精确度。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有基于网格的方法在利用2D视觉创新上的局限性，提升3D面部动画的表达力与真实性。

Method: 提出了一种基于感知唇读损失的训练方法，该损失通过3D高斯渲染结果与视觉自动语音识别预训练模型的结合实现。

Result: 在MEAD数据集上性能评估表明，该方法提高了标准Lip Vertex Error指标的准确率56.1%，并改善了生成动画的感知质量。

Conclusion: VisualSpeaker在保留网格驱动动画可控性的同时，显著提升了面部动画的感知质量与细节表现，尤其对于手语角色动画中的关键信号表达提供了支持。

Abstract: Realistic, high-fidelity 3D facial animations are crucial for expressive
avatar systems in human-computer interaction and accessibility. Although prior
methods show promising quality, their reliance on the mesh domain limits their
ability to fully leverage the rapid visual innovations seen in 2D computer
vision and graphics. We propose VisualSpeaker, a novel method that bridges this
gap using photorealistic differentiable rendering, supervised by visual speech
recognition, for improved 3D facial animation. Our contribution is a perceptual
lip-reading loss, derived by passing photorealistic 3D Gaussian Splatting
avatar renders through a pre-trained Visual Automatic Speech Recognition model
during training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker
improves both the standard Lip Vertex Error metric by 56.1% and the perceptual
quality of the generated animations, while retaining the controllability of
mesh-driven animation. This perceptual focus naturally supports accurate
mouthings, essential cues that disambiguate similar manual signs in sign
language avatars.

</details>


### [66] [MEDTalk: Multimodal Controlled 3D Facial Animation with Dynamic Emotions by Disentangled Embedding](https://arxiv.org/abs/2507.06071)
*Chang Liu,Ye Pan,Chenyang Ding,Susanto Rahardja,Xiaokang Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为MEDTalk的新框架，用于细粒度动态情感3D面部动画生成，解决了现有方法在情感标签多样性和自然性上的局限。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要基于静态、预定义的情感标签，导致情感动画的多样性和自然性受限，因此有必要开发更加动态和灵活的解决方案。

Method: 提出了一种通过交叉重建过程分离运动序列中的内容与情感嵌入的方法，并融合音频、语音文本等多模态输入预测帧间情感变化，使得生成动态情感面部动画。同时支持通过文本描述和参考图像实现情感个性化。

Result: 实验结果表明，该方法实现了更加细腻和动态的情感面部动画，且与工业生产管道兼容，可以无缝集成。

Conclusion: MEDTalk框架在动态情感生成中表现出色，显著提升了生成效果的自然性和多样性，同时增强了用户对情感表达的控制和定制能力。

Abstract: Audio-driven emotional 3D facial animation aims to generate synchronized lip
movements and vivid facial expressions. However, most existing approaches focus
on static and predefined emotion labels, limiting their diversity and
naturalness. To address these challenges, we propose MEDTalk, a novel framework
for fine-grained and dynamic emotional talking head generation. Our approach
first disentangles content and emotion embedding spaces from motion sequences
using a carefully designed cross-reconstruction process, enabling independent
control over lip movements and facial expressions. Beyond conventional
audio-driven lip synchronization, we integrate audio and speech text,
predicting frame-wise intensity variations and dynamically adjusting static
emotion features to generate realistic emotional expressions. Furthermore, to
enhance control and personalization, we incorporate multimodal inputs-including
text descriptions and reference expression images-to guide the generation of
user-specified facial expressions. With MetaHuman as the priority, our
generated results can be conveniently integrated into the industrial production
pipeline.

</details>


### [67] [MCAM: Multimodal Causal Analysis Model for Ego-Vehicle-Level Driving Video Understanding](https://arxiv.org/abs/2507.06072)
*Tongtong Cheng,Rongzhen Li,Yixin Xiong,Tao Zhang,Jing Wang,Kai Liu*

Main category: cs.CV

TL;DR: 提出了一种新型多模态因果分析模型（MCAM），通过视觉-语言融合改进自动驾驶视频的因果关系学习，取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法充分揭示深层因果关系，忽略了不同模态的虚假相关性和自车级别的因果建模需求。

Method: 设计了一个多模态因果分析模型（MCAM），包括多级特征提取模块、因果分析模块和视觉-语言对齐模块。

Result: 在BDD-X和CoVLA数据集上表现出SOTA性能，成功捕捉视频序列中的因果特征。

Conclusion: MCAM在视觉-语言因果关系建模方面表现出色，为自动驾驶相关应用提供了有力支持。

Abstract: Accurate driving behavior recognition and reasoning are critical for
autonomous driving video understanding. However, existing methods often tend to
dig out the shallow causal, fail to address spurious correlations across
modalities, and ignore the ego-vehicle level causality modeling. To overcome
these limitations, we propose a novel Multimodal Causal Analysis Model (MCAM)
that constructs latent causal structures between visual and language
modalities. Firstly, we design a multi-level feature extractor to capture
long-range dependencies. Secondly, we design a causal analysis module that
dynamically models driving scenarios using a directed acyclic graph (DAG) of
driving states. Thirdly, we utilize a vision-language transformer to align
critical visual features with their corresponding linguistic expressions.
Extensive experiments on the BDD-X, and CoVLA datasets demonstrate that MCAM
achieves SOTA performance in visual-language causal relationship learning.
Furthermore, the model exhibits superior capability in capturing causal
characteristics within video sequences, showcasing its effectiveness for
autonomous driving applications. The code is available at
https://github.com/SixCorePeach/MCAM.

</details>


### [68] [Discontinuity-aware Normal Integration for Generic Central Camera Models](https://arxiv.org/abs/2507.06075)
*Francesco Milano,Manuel López-Antequera,Naina Dhingra,Roland Siegwart,Robert Thiel*

Main category: cs.CV

TL;DR: 通过局部平面假设提出了一种新方法，用于处理具有表面法线图的3D表面恢复，支持深度中断和通用的中央相机模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理深度中断和支持通用相机模型方面存在局限性，提出新方法明确应对这些问题。

Method: 通过局部平面假设，将表面法线与光线方向的约束整合以建模，支持深度中断和通用中央相机模型。

Result: 在标准法线积分基准测试上达到最先进结果，并首次直接支持通用中央相机模型。

Conclusion: 提出的方法更精确近似深度与表面法线间关系，加强深度中断处理和通用相机兼容性。

Abstract: Recovering a 3D surface from its surface normal map, a problem known as
normal integration, is a key component for photometric shape reconstruction
techniques such as shape-from-shading and photometric stereo. The vast majority
of existing approaches for normal integration handle only implicitly the
presence of depth discontinuities and are limited to orthographic or ideal
pinhole cameras. In this paper, we propose a novel formulation that allows
modeling discontinuities explicitly and handling generic central cameras. Our
key idea is based on a local planarity assumption, that we model through
constraints between surface normals and ray directions. Compared to existing
methods, our approach more accurately approximates the relation between depth
and surface normals, achieves state-of-the-art results on the standard normal
integration benchmark, and is the first to directly handle generic central
camera models.

</details>


### [69] [ScoreAdv: Score-based Targeted Generation of Natural Adversarial Examples via Diffusion Models](https://arxiv.org/abs/2507.06078)
*Chihan Huang,Hao Tang*

Main category: cs.CV

TL;DR: 该论文针对当前深度学习的对抗攻击脆弱性，提出基于扩散模型的生成无约束对抗样本方法ScoreAdv，克服了现有方法的限制，生成高质量的自然对抗样本。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法多采用$ll_{p}$范数约束，与人类感知能力不符，且自然无约束对抗样本的生成方法存在图像质量欠佳、对扩散模型核心能力利用不足等问题。

Method: 提出ScoreAdv，基于扩散模型，通过可解释的对抗引导机制调整采样分布，结合显著性图将参考图像信息注入生成样本。支持对分类模型和检索模型的攻击。

Result: 在ImageNet和CelebA数据集上，对十个目标模型进行黑盒与白盒攻击实验，验证了ScoreAdv的攻击成功率和图像质量均达到最新水平，且在防御措施下依然具有鲁棒性。

Conclusion: ScoreAdv平衡了去噪与对抗扰动的动态关系，为高效生成自然对抗样本提供了新方法，对分类和检索任务均适用。

Abstract: Despite the success of deep learning across various domains, it remains
vulnerable to adversarial attacks. Although many existing adversarial attack
methods achieve high success rates, they typically rely on $\ell_{p}$-norm
perturbation constraints, which do not align with human perceptual
capabilities. Consequently, researchers have shifted their focus toward
generating natural, unrestricted adversarial examples (UAEs). GAN-based
approaches suffer from inherent limitations, such as poor image quality due to
instability and mode collapse. Meanwhile, diffusion models have been employed
for UAE generation, but they still rely on iterative PGD perturbation
injection, without fully leveraging their central denoising capabilities. In
this paper, we introduce a novel approach for generating UAEs based on
diffusion models, named ScoreAdv. This method incorporates an interpretable
adversarial guidance mechanism to gradually shift the sampling distribution
towards the adversarial distribution, while using an interpretable saliency map
to inject the visual information of a reference image into the generated
samples. Notably, our method is capable of generating an unlimited number of
natural adversarial examples and can attack not only classification models but
also retrieval models. We conduct extensive experiments on ImageNet and CelebA
datasets, validating the performance of ScoreAdv across ten target models in
both black-box and white-box settings. Our results demonstrate that ScoreAdv
achieves state-of-the-art attack success rates and image quality. Furthermore,
the dynamic balance between denoising and adversarial perturbation enables
ScoreAdv to remain robust even under defensive measures.

</details>


### [70] [CAST-Phys: Contactless Affective States Through Physiological signals Database](https://arxiv.org/abs/2507.06080)
*Joaquim Comas,Alexander Joel Vera,Xavier Vives,Eleonora De Filippi,Alexandre Pereda,Federico Sukno*

Main category: cs.CV

TL;DR: 本研究提出了CAST-Phys数据库，用于非接触式多模态情绪识别，利用面部和生理信号。


<details>
  <summary>Details</summary>
Motivation: 目前多模态情绪识别由于缺乏有效数据集及接触式设备的局限性，亟需解决方案。

Method: 开发了一个包含PPG、EDA、RR等多种生理信号和未压缩面部视频的高质量数据库。

Result: 证明了生理信号在情绪识别中至关重要，并验证了多模态融合的有效性。

Conclusion: CAST-Phys数据库推进了非接触式多模态情绪识别技术的发展。

Abstract: In recent years, affective computing and its applications have become a
fast-growing research topic. Despite significant advancements, the lack of
affective multi-modal datasets remains a major bottleneck in developing
accurate emotion recognition systems. Furthermore, the use of contact-based
devices during emotion elicitation often unintentionally influences the
emotional experience, reducing or altering the genuine spontaneous emotional
response. This limitation highlights the need for methods capable of extracting
affective cues from multiple modalities without physical contact, such as
remote physiological emotion recognition. To address this, we present the
Contactless Affective States Through Physiological Signals Database
(CAST-Phys), a novel high-quality dataset explicitly designed for multi-modal
remote physiological emotion recognition using facial and physiological cues.
The dataset includes diverse physiological signals, such as
photoplethysmography (PPG), electrodermal activity (EDA), and respiration rate
(RR), alongside high-resolution uncompressed facial video recordings, enabling
the potential for remote signal recovery. Our analysis highlights the crucial
role of physiological signals in realistic scenarios where facial expressions
alone may not provide sufficient emotional information. Furthermore, we
demonstrate the potential of remote multi-modal emotion recognition by
evaluating the impact of individual and fused modalities, showcasing its
effectiveness in advancing contactless emotion recognition technologies.

</details>


### [71] [Tile-Based ViT Inference with Visual-Cluster Priors for Zero-Shot Multi-Species Plant Identification](https://arxiv.org/abs/2507.06093)
*Murilo Gustineli,Anthony Miyaguchi,Adrian Cheung,Divyansh Khattak*

Main category: cs.CV

TL;DR: 本文描述了DS@GT团队在PlantCLEF 2025多物种植物识别挑战中获得第二名的解决方案，采用了一种结合Vision Transformer、图块分割策略以及领域适应的管道。


<details>
  <summary>Details</summary>
Motivation: 通过提升植物识别的准确性和效率来解决植物生物多样性监测中的实际问题。

Method: 管道包括三部分：(i) 利用Vision Transformer进行图块级推断；(ii) 采用4x4图块策略以匹配网络的感受野；(iii) 使用PaCMAP与K-Means聚类及地理位置过滤进行领域适应优化。

Result: 最终方案通过多图块预测、集成贝叶斯先验调整，达到了0.348的宏平均F1分数，在私人排行榜上获得第二名。

Conclusion: 提出的方法能高效结合视觉模型与领域知识，具备高重现性，对相关领域研究具有指导价值。

Abstract: We describe DS@GT's second-place solution to the PlantCLEF 2025 challenge on
multi-species plant identification in vegetation quadrat images. Our pipeline
combines (i) a fine-tuned Vision Transformer ViTD2PC24All for patch-level
inference, (ii) a 4x4 tiling strategy that aligns patch size with the network's
518x518 receptive field, and (iii) domain-prior adaptation through PaCMAP +
K-Means visual clustering and geolocation filtering. Tile predictions are
aggregated by majority vote and re-weighted with cluster-specific Bayesian
priors, yielding a macro-averaged F1 of 0.348 (private leaderboard) while
requiring no additional training. All code, configuration files, and
reproducibility scripts are publicly available at
https://github.com/dsgt-arc/plantclef-2025.

</details>


### [72] [Reflections Unlock: Geometry-Aware Reflection Disentanglement in 3D Gaussian Splatting for Photorealistic Scenes Rendering](https://arxiv.org/abs/2507.06103)
*Jiayi Song,Zihan Ye,Qingyuan Zhou,Weidong Yang,Ben Fei,Jingyi Xu,Ying He,Wanli Ouyang*

Main category: cs.CV

TL;DR: Ref-Unlock是一种基于3D高斯散射的几何感知反射建模框架，可在场景重建中准确处理复杂反射问题，并显著提升几何一致性和图像清晰度。


<details>
  <summary>Details</summary>
Motivation: 现有方法如NeRF和3DGS在面对反射表面时效果不佳，因其将反射误解为物理几何，从而导致场景重建质量下降。

Method: Ref-Unlock采用双分支表示结合高阶球谐函数建模高频反射，使用反射去除模块提供反射分解指导，配合伪深度图和几何感知双边平滑约束增强3D几何一致性。

Result: 实验显示，Ref-Unlock无论在传统GS方法还是NeRF模型中都表现优秀，同时还支持灵活的视觉基础模型驱动反射编辑功能。

Conclusion: Ref-Unlock为真实感反射场景渲染提供了一种高效且具备广泛适用性的解决方案。

Abstract: Accurately rendering scenes with reflective surfaces remains a significant
challenge in novel view synthesis, as existing methods like Neural Radiance
Fields (NeRF) and 3D Gaussian Splatting (3DGS) often misinterpret reflections
as physical geometry, resulting in degraded reconstructions. Previous methods
rely on incomplete and non-generalizable geometric constraints, leading to
misalignment between the positions of Gaussian splats and the actual scene
geometry. When dealing with real-world scenes containing complex geometry, the
accumulation of Gaussians further exacerbates surface artifacts and results in
blurred reconstructions. To address these limitations, in this work, we propose
Ref-Unlock, a novel geometry-aware reflection modeling framework based on 3D
Gaussian Splatting, which explicitly disentangles transmitted and reflected
components to better capture complex reflections and enhance geometric
consistency in real-world scenes. Our approach employs a dual-branch
representation with high-order spherical harmonics to capture high-frequency
reflective details, alongside a reflection removal module providing pseudo
reflection-free supervision to guide clean decomposition. Additionally, we
incorporate pseudo-depth maps and a geometry-aware bilateral smoothness
constraint to enhance 3D geometric consistency and stability in decomposition.
Extensive experiments demonstrate that Ref-Unlock significantly outperforms
classical GS-based reflection methods and achieves competitive results with
NeRF-based models, while enabling flexible vision foundation models (VFMs)
driven reflection editing. Our method thus offers an efficient and
generalizable solution for realistic rendering of reflective scenes. Our code
is available at https://ref-unlock.github.io/.

</details>


### [73] [Omni-Video: Democratizing Unified Video Understanding and Generation](https://arxiv.org/abs/2507.06119)
*Zhiyu Tan,Hao Yang,Luozheng Qin,Jia Gong,Mengping Yang,Hao Li*

Main category: cs.CV

TL;DR: 目前的基础模型主要集中于图像处理，而在视频理解与生成方面存在不足。本文提出Omni-Video框架，通过多模态大语言模型生成连续视觉线索，结合扩散解码器，实现高质量视频生成、理解和编辑。


<details>
  <summary>Details</summary>
Motivation: 现有模型在视频领域缺乏统一建模能力，本研究旨在开发一个高效的统一框架来解决此问题。

Method: 提出一种轻量化架构，结合多模态大语言模型和扩散解码器，通过多阶段训练提高视频任务表现。

Result: 实验结果表明，模型在视频理解、生成与编辑任务上表现出良好的泛化能力。

Conclusion: Omni-Video通过技术整合克服了现有视频建模的不足，为视频生成与理解提供了高效解决方案。

Abstract: Notable breakthroughs in unified understanding and generation modeling have
led to remarkable advancements in image understanding, reasoning, production
and editing, yet current foundational models predominantly focus on processing
images, creating a gap in the development of unified models for video
understanding and generation. This report presents Omni-Video, an efficient and
effective unified framework for video understanding, generation, as well as
instruction-based editing. Our key insight is to teach existing multimodal
large language models (MLLMs) to produce continuous visual clues that are used
as the input of diffusion decoders, which produce high-quality videos
conditioned on these visual clues. To fully unlock the potential of our system
for unified video modeling, we integrate several technical improvements: 1) a
lightweight architectural design that respectively attaches a vision head on
the top of MLLMs and a adapter before the input of diffusion decoders, the
former produce visual tokens for the latter, which adapts these visual tokens
to the conditional space of diffusion decoders; and 2) an efficient multi-stage
training scheme that facilitates a fast connection between MLLMs and diffusion
decoders with limited data and computational resources. We empirically
demonstrate that our model exhibits satisfactory generalization abilities
across video generation, editing and understanding tasks.

</details>


### [74] [Prompt-Free Conditional Diffusion for Multi-object Image Augmentation](https://arxiv.org/abs/2507.06146)
*Haoyu Wang,Lei Zhang,Wei Wei,Chen Ding,Yanning Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种无提示条件扩散框架，用于多目标图像数据增强，解决了生成图像多样性和原始数据偏差问题。


<details>
  <summary>Details</summary>
Motivation: 传统的图像生成方法在多目标图像生成中，要么依赖文本条件，造成生成对象与原数据的偏差；要么过度依赖原图像，生成图像缺乏多样性。

Method: 设计了一种无提示条件扩散框架，采用局部-全局语义融合策略替代文本，从图像中提取语义，并使用LoRA注入知识缓解类别偏差，同时设计了基于奖励模型的计数损失，改善重建损失的限制。

Result: 实验表明，所提方法相比现有技术具有优势，能显著提升下游任务效果和泛化能力。

Conclusion: 该方法显著提高了图像生成的多样性和与原数据的匹配程度，对提升下游任务和跨域泛化性有重要意义。

Abstract: Diffusion models has underpinned much recent advances of dataset augmentation
in various computer vision tasks. However, when involving generating
multi-object images as real scenarios, most existing methods either rely
entirely on text condition, resulting in a deviation between the generated
objects and the original data, or rely too much on the original images,
resulting in a lack of diversity in the generated images, which is of limited
help to downstream tasks. To mitigate both problems with one stone, we propose
a prompt-free conditional diffusion framework for multi-object image
augmentation. Specifically, we introduce a local-global semantic fusion
strategy to extract semantics from images to replace text, and inject knowledge
into the diffusion model through LoRA to alleviate the category deviation
between the original model and the target dataset. In addition, we design a
reward model based counting loss to assist the traditional reconstruction loss
for model training. By constraining the object counts of each category instead
of pixel-by-pixel constraints, bridging the quantity deviation between the
generated data and the original data while improving the diversity of the
generated data. Experimental results demonstrate the superiority of the
proposed method over several representative state-of-the-art baselines and
showcase strong downstream task gain and out-of-domain generalization
capabilities. Code is available at
\href{https://github.com/00why00/PFCD}{here}.

</details>


### [75] [CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic Images and Contextualized Captions](https://arxiv.org/abs/2507.06210)
*Yuchen Huang,Zhiyuan Fan,Zhitao He,Sandeep Polisetty,Wenyan Li,Yi R. Fung*

Main category: cs.CV

TL;DR: 该论文提出一种新的方法，通过构建合成文化数据集（CulTwin）并微调CLIP模型（CultureCLIP），以便在文化概念方面实现更细粒度的区分，同时保持模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 预训练的视觉-语言模型（如CLIP）在多模态理解方面表现出色，但在区分细粒度文化概念时存在不足，主要由于缺乏高质量的文化数据集及难以辨别的硬负样本。

Method: 设计了一个数据处理流程，结合VLMs和文本到图像扩散模型生成CulTwin文化数据集。随后，基于CulTwin使用自定义对比学习对CLIP进行微调，形成CultureCLIP模型。

Result: CultureCLIP在文化相关的基准测试中表现优于原CLIP，在某些任务中的细粒度概念识别性能提高了5.49%，并保持CLIP的泛化能力。

Conclusion: 验证了通过数据合成和VLM微调训练模式能够有效捕捉文化细微差别，证明了CulTwin和CultureCLIP的实用性。

Abstract: Pretrained vision-language models (VLMs) such as CLIP excel in multimodal
understanding but struggle with contextually relevant fine-grained visual
features, making it difficult to distinguish visually similar yet culturally
distinct concepts. This limitation stems from the scarcity of high-quality
culture-specific datasets, the lack of integrated contextual knowledge, and the
absence of hard negatives highlighting subtle distinctions. To address these
challenges, we first design a data curation pipeline that leverages
open-sourced VLMs and text-to-image diffusion models to construct CulTwin, a
synthetic cultural dataset. This dataset consists of paired
concept-caption-image triplets, where concepts visually resemble each other but
represent different cultural contexts. Then, we fine-tune CLIP on CulTwin to
create CultureCLIP, which aligns cultural concepts with contextually enhanced
captions and synthetic images through customized contrastive learning, enabling
finer cultural differentiation while preserving generalization capabilities.
Experiments on culturally relevant benchmarks show that CultureCLIP outperforms
the base CLIP, achieving up to a notable 5.49% improvement in fine-grained
concept recognition on certain tasks, while preserving CLIP's original
generalization ability, validating the effectiveness of our data synthesis and
VLM backbone training paradigm in capturing subtle cultural distinctions.

</details>


### [76] [Normalizing Diffusion Kernels with Optimal Transport](https://arxiv.org/abs/2507.06161)
*Nathan Kessler,Robin Magnet,Jean Feydy*

Main category: cs.CV

TL;DR: 本文提出了一种基于泛化相似矩阵的平滑操作方法，解决了传统方法在处理非规则数据时的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统平滑方法对域结构的依赖限制使其难以处理非规则数据，如点云等。

Method: 引入了一种基于对称Sinkhorn算法的平滑操作方法，通过归一化构造出类似拉普拉斯算子的扩散操作。

Result: 该方法不仅接近于热扩散模型，还能保留拉普拉斯的谱信息，可应用于形状分析和匹配等领域。

Conclusion: 通过新的构造方式，有效扩展了拉普拉斯型平滑操作的适用范围，特别是对非规则数据的处理能力。

Abstract: Smoothing a signal based on local neighborhoods is a core operation in
machine learning and geometry processing. On well-structured domains such as
vector spaces and manifolds, the Laplace operator derived from differential
geometry offers a principled approach to smoothing via heat diffusion, with
strong theoretical guarantees. However, constructing such Laplacians requires a
carefully defined domain structure, which is not always available. Most
practitioners thus rely on simple convolution kernels and message-passing
layers, which are biased against the boundaries of the domain. We bridge this
gap by introducing a broad class of smoothing operators, derived from general
similarity or adjacency matrices, and demonstrate that they can be normalized
into diffusion-like operators that inherit desirable properties from
Laplacians. Our approach relies on a symmetric variant of the Sinkhorn
algorithm, which rescales positive smoothing operators to match the structural
behavior of heat diffusion. This construction enables Laplacian-like smoothing
and processing of irregular data such as point clouds, sparse voxel grids or
mixture of Gaussians. We show that the resulting operators not only approximate
heat diffusion but also retain spectral information from the Laplacian itself,
with applications to shape analysis and matching.

</details>


### [77] [OmniPart: Part-Aware 3D Generation with Semantic Decoupling and Structural Cohesion](https://arxiv.org/abs/2507.06165)
*Yunhan Yang,Yufan Zhou,Yuan-Chen Guo,Zi-Xin Zou,Yukun Huang,Ying-Tian Liu,Hao Xu,Ding Liang,Yan-Pei Cao,Xihui Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为OmniPart的框架，通过引入分阶段生成方法达到高语义解耦和结构连贯性，支持用户自定义3D部件细粒度和布局。


<details>
  <summary>Details</summary>
Motivation: 现有的3D生成方法多产生整体模型，无法满足交互式应用对部件编辑与分割的需求，需要开发具备明确、可编辑部件结构的3D生成方案。

Method: OmniPart框架包括两个步骤：(1) 自动回归结构模块生成3D部件的可控序列，利用灵活的2D部件掩码引导分解；(2) 基于预训练生成器的空间条件的校正流模型，生成与计划布局一致的3D部件。

Result: 实验表明OmniPart在实现可解释、可编辑的3D资产生成方面表现出了当前最优性能。

Conclusion: OmniPart为3D内容创建提供了更强的语义控制与多样性支持，为交互式应用和更广泛场景打开了新的可能性。

Abstract: The creation of 3D assets with explicit, editable part structures is crucial
for advancing interactive applications, yet most generative methods produce
only monolithic shapes, limiting their utility. We introduce OmniPart, a novel
framework for part-aware 3D object generation designed to achieve high semantic
decoupling among components while maintaining robust structural cohesion.
OmniPart uniquely decouples this complex task into two synergistic stages: (1)
an autoregressive structure planning module generates a controllable,
variable-length sequence of 3D part bounding boxes, critically guided by
flexible 2D part masks that allow for intuitive control over part decomposition
without requiring direct correspondences or semantic labels; and (2) a
spatially-conditioned rectified flow model, efficiently adapted from a
pre-trained holistic 3D generator, synthesizes all 3D parts simultaneously and
consistently within the planned layout. Our approach supports user-defined part
granularity, precise localization, and enables diverse downstream applications.
Extensive experiments demonstrate that OmniPart achieves state-of-the-art
performance, paving the way for more interpretable, editable, and versatile 3D
content.

</details>


### [78] [Enhancing Scientific Visual Question Answering through Multimodal Reasoning and Ensemble Modeling](https://arxiv.org/abs/2507.06183)
*Prahitha Movva,Naga Harshita Marupaka*

Main category: cs.CV

TL;DR: 研究提出了针对科学文章中的图形数据问答任务的解决方法，采用了多种模型实验优化结果。


<details>
  <summary>Details</summary>
Motivation: 提高当前视觉问答模型在科学数据解读中的精度，解决数值处理、多步推理及视觉与文本一致性问题。

Method: 使用参数范围在5B到8B的多种模型进行实验，优化提示词、链式推理方法并构建视觉语言模型集成策略。

Result: 最强单模型InternVL3的SciVQA测试集的ROUGE-1和ROUGE-L F1得分为0.740，BERTScore为0.983；集成模型在验证集上表现优于大多数单模型。

Conclusion: 提示优化、链式推理及集成建模有效提升了视觉问答能力，验证了这些方法在科学图形问答中的实用性。

Abstract: Technical reports and articles often contain valuable information in the form
of semi-structured data like charts, and figures. Interpreting these and using
the information from them is essential for downstream tasks such as question
answering (QA). Current approaches to visual question answering often struggle
with the precision required for scientific data interpretation, particularly in
handling numerical values, multi-step reasoning over visual elements, and
maintaining consistency between visual observation and textual reasoning. We
present our approach to the SciVQA 2025 shared task, focusing on answering
visual and non-visual questions grounded in scientific figures from scholarly
articles.
  We conducted a series of experiments using models with 5B to 8B parameters.
Our strongest individual model, InternVL3, achieved ROUGE-1 and ROUGE-L F1
scores of \textbf{0.740} and a BERTScore of \textbf{0.983} on the SciVQA test
split. We also developed an ensemble model with multiple vision language models
(VLMs). Through error analysis on the validation split, our ensemble approach
improved performance compared to most individual models, though InternVL3
remained the strongest standalone performer. Our findings underscore the
effectiveness of prompt optimization, chain-of-thought reasoning and ensemble
modeling in improving the model's ability in visual question answering.

</details>


### [79] [Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion](https://arxiv.org/abs/2507.06230)
*Aleksandar Jevtić,Christoph Reich,Felix Wimbauer,Oliver Hahn,Christian Rupprecht,Stefan Roth,Daniel Cremers*

Main category: cs.CV

TL;DR: 提出SceneDINO，一种用于无监督语义场景完成（SSC）的方法，无需语义或几何地面真值，利用多视图一致性自监督方法达到3D场景理解的显著效果。


<details>
  <summary>Details</summary>
Motivation: 当前SSC的方法依赖昂贵的地面真值标注，而SciDINO旨在解决这一问题，使SSC在无监督设置下成为可能。

Method: 利用多视图一致性自监督训练方法，不使用任何语义或几何地面真值。通过3D特征蒸馏方式推导无监督3D语义，实现在单张图片输入下生成3D几何与DINO特征。

Result: SceneDINO在3D和2D无监督场景理解中达到了最新的分割精度。在3D特征线性探测中，与当前有监督SSC方法匹配分割精度。此外，展现了领域泛化及多视图一致性能力。

Conclusion: SceneDINO是单张图片3D场景理解的重要基础，展现了其无监督的强大潜力。

Abstract: Semantic scene completion (SSC) aims to infer both the 3D geometry and
semantics of a scene from single images. In contrast to prior work on SSC that
heavily relies on expensive ground-truth annotations, we approach SSC in an
unsupervised setting. Our novel method, SceneDINO, adapts techniques from
self-supervised representation learning and 2D unsupervised scene understanding
to SSC. Our training exclusively utilizes multi-view consistency
self-supervision without any form of semantic or geometric ground truth. Given
a single input image, SceneDINO infers the 3D geometry and expressive 3D DINO
features in a feed-forward manner. Through a novel 3D feature distillation
approach, we obtain unsupervised 3D semantics. In both 3D and 2D unsupervised
scene understanding, SceneDINO reaches state-of-the-art segmentation accuracy.
Linear probing our 3D features matches the segmentation accuracy of a current
supervised SSC approach. Additionally, we showcase the domain generalization
and multi-view consistency of SceneDINO, taking the first steps towards a
strong foundation for single image 3D scene understanding.

</details>


### [80] [RSRefSeg 2: Decoupling Referring Remote Sensing Image Segmentation with Foundation Models](https://arxiv.org/abs/2507.06231)
*Keyan Chen,Chenyang Liu,Bowen Chen,Jiafan Zhang,Zhengxia Zou,Zhenwei Shi*

Main category: cs.CV

TL;DR: 提出了一个新的遥感影像分割框架RSRefSeg 2，通过解耦粗定位和细分割步骤，结合CLIP的跨模态对齐能力和SAM的分割泛化能力，显著提升了分割精度和复杂语义理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂语义关系和跨模态对齐时存在局限性，主要因定位与边界划定耦合，导致错误传播和模型泛化性受限。

Method: 提出解耦范式RSRefSeg 2，通过CLIP激活目标特征生成定位提示，并设计级联二阶提示器分解文本嵌入，引导SAM生成精细分割结果，实现语义传递。

Result: 在多项实验中，RSRefSeg 2在分割精度（+~3% gIoU）和语义解释能力上优于现有方法。

Conclusion: RSRefSeg 2的解耦框架突破了现有方法的瓶颈，为遥感影像场景分析提供了更精确的语义分割能力。

Abstract: Referring Remote Sensing Image Segmentation provides a flexible and
fine-grained framework for remote sensing scene analysis via vision-language
collaborative interpretation. Current approaches predominantly utilize a
three-stage pipeline encompassing dual-modal encoding, cross-modal interaction,
and pixel decoding. These methods demonstrate significant limitations in
managing complex semantic relationships and achieving precise cross-modal
alignment, largely due to their coupled processing mechanism that conflates
target localization with boundary delineation. This architectural coupling
amplifies error propagation under semantic ambiguity while restricting model
generalizability and interpretability. To address these issues, we propose
RSRefSeg 2, a decoupling paradigm that reformulates the conventional workflow
into a collaborative dual-stage framework: coarse localization followed by fine
segmentation. RSRefSeg 2 integrates CLIP's cross-modal alignment strength with
SAM's segmentation generalizability through strategic foundation model
collaboration. Specifically, CLIP is employed as the dual-modal encoder to
activate target features within its pre-aligned semantic space and generate
localization prompts. To mitigate CLIP's misactivation challenges in
multi-entity scenarios described by referring texts, a cascaded second-order
prompter is devised, which enhances precision through implicit reasoning via
decomposition of text embeddings into complementary semantic subspaces. These
optimized semantic prompts subsequently direct the SAM to generate pixel-level
refined masks, thereby completing the semantic transmission pipeline. Extensive
experiments (RefSegRS, RRSIS-D, and RISBench) demonstrate that RSRefSeg 2
surpasses contemporary methods in segmentation accuracy (+~3% gIoU) and complex
semantic interpretation. Code is available at:
https://github.com/KyanChen/RSRefSeg2.

</details>


### [81] [Learning to Track Any Points from Human Motion](https://arxiv.org/abs/2507.06233)
*Inès Hyeonsu Kim,Seokju Cho,Jahyeok Koo,Junghyun Park,Jiahui Huang,Joon-Young Lee,Seungryong Kim*

Main category: cs.CV

TL;DR: 提出了一种名为AnthroTAP的自动化流水线，通过生成伪标注数据集用于点追踪模型的训练，解决了人类运动分析中的复杂点追踪问题，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 针对人类运动的复杂性（例如非刚性变形、关节运动、衣物变形及遮挡等）对点追踪标注数据的需求，提出一种减小监督数据获取成本的解决方案。

Method: 提出自动化流水线AnthroTAP，通过与SMPL模型结合生成伪标注的训练数据，其中包括3D网格顶点投影到2D平面处理遮挡及过滤不可靠轨迹。

Result: 基于AnthroTAP训练的点追踪模型在TAP-Vid基准测试中优于现有方法，同时使用的训练数据量减少了10,000倍，并显著降低了计算资源需求。

Conclusion: AnthroTAP流水线通过高效生成伪标注数据显著提升了点追踪模型的性能，并提供了大幅降低数据需求与计算开销的解决方案。

Abstract: Human motion, with its inherent complexities, such as non-rigid deformations,
articulated movements, clothing distortions, and frequent occlusions caused by
limbs or other individuals, provides a rich and challenging source of
supervision that is crucial for training robust and generalizable point
trackers. Despite the suitability of human motion, acquiring extensive training
data for point tracking remains difficult due to laborious manual annotation.
Our proposed pipeline, AnthroTAP, addresses this by proposing an automated
pipeline to generate pseudo-labeled training data, leveraging the Skinned
Multi-Person Linear (SMPL) model. We first fit the SMPL model to detected
humans in video frames, project the resulting 3D mesh vertices onto 2D image
planes to generate pseudo-trajectories, handle occlusions using ray-casting,
and filter out unreliable tracks based on optical flow consistency. A point
tracking model trained on AnthroTAP annotated dataset achieves state-of-the-art
performance on the TAP-Vid benchmark, surpassing other models trained on real
videos while using 10,000 times less data and only 1 day in 4 GPUs, compared to
256 GPUs used in recent state-of-the-art.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [82] [TokenShapley: Token Level Context Attribution with Shapley Value](https://arxiv.org/abs/2507.05261)
*Yingtai Xiao,Yuqing Zhu,Sirat Samyoun,Wanrong Zhang,Jiachen T. Wang,Jian Du*

Main category: cs.CL

TL;DR: 提出TokenShapley方法，通过结合Shapley值数据归因和KNN检索技术改善大语言模型在生成内容中的细粒度关键词归因能力，并在基准测试中表现出显著优势。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在上下文学习方面表现强大，但生成内容的准确性验证仍然具有挑战，尤其在关键词层面存在归因难的问题。

Method: 将基于Shapley值的数据归因和KNN检索技术相结合，通过预先计算的数据存储和Shapley值计算量化关键词重要性，提出TokenShapley方法进行细粒度数据归因。

Result: 在四个基准测试中，TokenShapley在词级归因准确性上较现有方法提升了11-23%。

Conclusion: TokenShapley显著提高了大语言模型在关键词层面的归因能力，展示了使用细粒度数据归因技术验证生成内容准确性的潜力。

Abstract: Large language models (LLMs) demonstrate strong capabilities in in-context
learning, but verifying the correctness of their generated responses remains a
challenge. Prior work has explored attribution at the sentence level, but these
methods fall short when users seek attribution for specific keywords within the
response, such as numbers, years, or names. To address this limitation, we
propose TokenShapley, a novel token-level attribution method that combines
Shapley value-based data attribution with KNN-based retrieval techniques
inspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed
datastore for contextual retrieval and computing Shapley values to quantify
token importance, TokenShapley provides a fine-grained data attribution
approach. Extensive evaluations on four benchmarks show that TokenShapley
outperforms state-of-the-art baselines in token-level attribution, achieving an
11-23% improvement in accuracy.

</details>


### [83] [User Behavior Prediction as a Generic, Robust, Scalable, and Low-Cost Evaluation Strategy for Estimating Generalization in LLMs](https://arxiv.org/abs/2507.05266)
*Sougata Saha,Monojit Choudhury*

Main category: cs.CL

TL;DR: 该研究探讨LLM测量泛化能力的挑战，提出通过用户行为预测作为替代方法进行评估，并通过实验验证了框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM泛化能力的测量因数据污染变得困难，并且随着模型规模扩大和计算成本降低，很难确保训练过程中测试任务不被提前见过。

Method: 提出了一种基于用户行为预测的框架，用于替代现有的知识检索和推理类任务，并通过电影和音乐推荐数据集测试了框架下的GPT-4o、GPT-4o-mini和Llama-3.1-8B-Instruct三种模型。

Result: 结果表明，GPT-4o表现优于GPT-4o-mini和Llama，但所有模型仍有较大改进空间，尤其是Llama。

Conclusion: 用户行为预测是一种理论上合理、可扩展且鲁棒的LLM泛化能力测量方法，实验结果验证了该框架的有效性。

Abstract: Measuring the generalization ability of Large Language Models (LLMs) is
challenging due to data contamination. As models grow and computation becomes
cheaper, ensuring tasks and test cases are unseen during training phases will
become nearly impossible. We argue that knowledge-retrieval and reasoning tasks
are not ideal for measuring generalization, as LLMs are not trained for
specific tasks. Instead, we propose user behavior prediction, also a key aspect
of personalization, as a theoretically sound, scalable, and robust alternative.
We introduce a novel framework for this approach and test it on movie and music
recommendation datasets for GPT-4o, GPT-4o-mini, and Llama-3.1-8B-Instruct.
Results align with our framework's predictions, showing GPT-4o outperforms
GPT-4o-mini and Llama, though all models have much room for improvement,
especially Llama.

</details>


### [84] [An Adaptive Supervised Contrastive Learning Framework for Implicit Sexism Detection in Digital Social Networks](https://arxiv.org/abs/2507.05271)
*Mohammad Zia Ur Rehman,Aditya Shah,Nagendra Kumar*

Main category: cs.CL

TL;DR: 本文提出了一个用于检测隐性性别歧视的框架ASCEND，通过使用基于阈值的对比学习和联合损失优化机制，显著提高了检测准确性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的隐性性别歧视内容难以被传统检测方法识别，需要更有效的检测方法。

Method: 设计了一种自适应监督对比学习框架ASCEND，结合可学习阈值的对比学习、加权注意力模块及情感、情绪和毒性特征的增强。

Result: 在EXIST2021和MLSC数据集上的实验显示，ASCEND在多个任务的平均Macro F1值上分别提升了9.86%、29.63%和32.51%。

Conclusion: ASCEND是一个有效的新方法，在捕捉隐性性别歧视语言方面有显著优势。

Abstract: The global reach of social media has amplified the spread of hateful content,
including implicit sexism, which is often overlooked by conventional detection
methods. In this work, we introduce an Adaptive Supervised Contrastive lEarning
framework for implicit sexism detectioN (ASCEND). A key innovation of our
method is the incorporation of threshold-based contrastive learning: by
computing cosine similarities between embeddings, we selectively treat only
those sample pairs as positive if their similarity exceeds a learnable
threshold. This mechanism refines the embedding space by robustly pulling
together representations of semantically similar texts while pushing apart
dissimilar ones, thus reducing false positives and negatives. The final
classification is achieved by jointly optimizing a contrastive loss with a
cross-entropy loss. Textual features are enhanced through a word-level
attention module. Additionally, we employ sentiment, emotion, and toxicity
features. Evaluations on the EXIST2021 and MLSC datasets demonstrate that
ASCEND significantly outperforms existing methods, with average Macro F1
improvements of 9.86%, 29.63%, and 32.51% across multiple tasks, highlighting
its efficacy in capturing the subtle cues of implicit sexist language.

</details>


### [85] [Beyond classical and contemporary models: a transformative ai framework for student dropout prediction in distance learning using rag, prompt engineering, and cross-modal fusion](https://arxiv.org/abs/2507.05285)
*Miloud Mihoubi,Meriem Zerkouk,Belkacem Chikhaoui*

Main category: cs.CL

TL;DR: 论文提出了一种利用情感分析、提示工程与跨模态注意力融合的新型AI框架，以更好地预测远程学习中的学生辍学问题，并取得了89%的准确率和0.88的F1分数。


<details>
  <summary>Details</summary>
Motivation: 远程学习中的学生辍学问题仍然是一个重大挑战，传统机器学习无法充分捕捉学生交互中的情感和上下文因素。

Method: 提出了一种整合了RAG进行领域特定情感分析、通过提示工程解析学术压力源，以及利用跨模态注意力层融合文本、行为和社会人口学数据的框架。

Result: 模型在4423名学生的纵向数据集上表现优异，达到了89%的准确率和0.88的F1分数，相比传统方法提升了7%，并减少了21%的假阴性率。

Conclusion: 该框架不仅提升了辍学预测能力，还能根据情境生成适合的干预措施，为全球教育体系提供了可扩展的解决方案。

Abstract: Student dropout in distance learning remains a critical challenge, with
profound societal and economic consequences. While classical machine learning
models leverage structured socio-demographic and behavioral data, they often
fail to capture the nuanced emotional and contextual factors embedded in
unstructured student interactions. This paper introduces a transformative AI
framework that redefines dropout prediction through three synergistic
innovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment
analysis, prompt engineering to decode academic stressors, and cross-modal
attention fusion to dynamically align textual, behavioral, and
socio-demographic insights. By grounding sentiment analysis in a curated
knowledge base of pedagogical content, our RAG-enhanced BERT model interprets
student comments with unprecedented contextual relevance, while optimized
prompts isolate indicators of academic distress (e.g., "isolation," "workload
anxiety"). A cross-modal attention layer then fuses these insights with
temporal engagement patterns, creating holistic risk profiles. Evaluated on a
longitudinal dataset of 4 423 students, the framework achieves 89% accuracy and
an F1-score of 0.88, outperforming conventional models by 7% and reducing false
negatives by 21%. Beyond prediction, the system generates interpretable
interventions by retrieving contextually aligned strategies (e.g., mentorship
programs for isolated learners). This work bridges the gap between predictive
analytics and actionable pedagogy, offering a scalable solution to mitigate
dropout risks in global education systems

</details>


### [86] [LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review](https://arxiv.org/abs/2507.05319)
*Cheng Yuan,Xinkai Rui,Yongqi Fan,Yawei Fan,Boyang Zhong,Jiacheng Wang,Weiyan Zhang,Tong Ruan*

Main category: cs.CL

TL;DR: 大语言模型（LLMs）在生成出院总结时性能优越，但存在幻觉问题及源信息归属困难。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs生成不准确内容、虚构信息以及难以归结来源这些问题。

Method: 提出LCDS系统，通过文本相似度构建源映射表并结合逻辑规则生成可信出院总结，支持内容溯源及专家修正。

Result: LCDS生成可靠的银出院总结，通过专家反馈不断改进模型精度。

Conclusion: LCDS提升了模型生成内容的可靠性和可溯源性，为LLMs在医疗领域的实际应用提供支持。

Abstract: Despite the remarkable performance of Large Language Models (LLMs) in
automated discharge summary generation, they still suffer from hallucination
issues, such as generating inaccurate content or fabricating information
without valid sources. In addition, electronic medical records (EMRs) typically
consist of long-form data, making it challenging for LLMs to attribute the
generated content to the sources. To address these challenges, we propose LCDS,
a Logic-Controlled Discharge Summary generation system. LCDS constructs a
source mapping table by calculating textual similarity between EMRs and
discharge summaries to constrain the scope of summarized content. Moreover,
LCDS incorporates a comprehensive set of logical rules, enabling it to generate
more reliable silver discharge summaries tailored to different clinical fields.
Furthermore, LCDS supports source attribution for generated content, allowing
experts to efficiently review, provide feedback, and rectify errors. The
resulting golden discharge summaries are subsequently recorded for incremental
fine-tuning of LLMs. Our project and demo video are in the GitHub repository
https://github.com/ycycyc02/LCDS.

</details>


### [87] [MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents](https://arxiv.org/abs/2507.05330)
*Ming Gong,Xucheng Huang,Chenghan Yang,Xianhan Peng,Haoxin Wang,Yang Liu,Ling Jiang*

Main category: cs.CL

TL;DR: MindFlow是一个开源的多模态大语言模型（LLM）代理，专注于电子商务应用，具有综合决策和视觉文本推理能力，可显著提高用户满意度和效率。


<details>
  <summary>Details</summary>
Motivation: 针对当前大语言模型（LLMs）在处理复杂多模态场景中的局限性，开发一种能够适应电子商务需求的高效多模态解决方案。

Method: 构建了一个基于CoALA框架的多模态LLM代理MindFlow，采用模块化设计，包括记忆、决策和行动模块，使用"MLLM-as-Tool"策略来完成视觉-文本推理。

Result: 通过在线A/B测试和基于模拟的消融实验，MindFlow在解决复杂查询、提高用户满意度和降低运营成本方面表现出显著的优势，在实际部署中实现了93.53%的相对改进。

Conclusion: MindFlow展示了多模态语言模型在电子商务复杂交互中的实际应用潜力，可以优化用户体验并提高经济效益。

Abstract: Recent advances in large language models (LLMs) have enabled new applications
in e-commerce customer service. However, their capabilities remain constrained
in complex, multimodal scenarios. We present MindFlow, the first open-source
multimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it
integrates memory, decision-making, and action modules, and adopts a modular
"MLLM-as-Tool" strategy for effect visual-textual reasoning. Evaluated via
online A/B testing and simulation-based ablation, MindFlow demonstrates
substantial gains in handling complex queries, improving user satisfaction, and
reducing operational costs, with a 93.53% relative improvement observed in
real-world deployments.

</details>


### [88] [LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks](https://arxiv.org/abs/2507.05346)
*William Fleshman,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 提出了一种称为LoRA-Augmented Generation (LAG)的方法，用于高效选择和组合任务和领域的特定LoRA适配器，无需额外训练或数据访问，在多个知识密集任务上表现优越。


<details>
  <summary>Details</summary>
Motivation: 解决在大量语言模型中高效选择和组合专家模型的需求问题。

Method: 引入LAG方法，通过每个token和层来过滤、检索和应用LoRA适配器，无需额外训练或数据。

Result: 在知识密集型任务上优于现有无数据方法，并展示了与RAG等其他解决方案的兼容性。

Conclusion: LAG方法在无需数据的情况下表现实用和高效，适用于各种场景，并且可以与其他方案结合使用。

Abstract: The proliferation of fine-tuned language model experts for specific tasks and
domains signals the need for efficient selection and combination methods. We
propose LoRA-Augmented Generation (LAG) for leveraging large libraries of
knowledge and task-specific LoRA adapters. LAG requires no additional training
or access to data, and efficiently filters, retrieves, and applies experts on a
per-token and layer basis. We evaluate LAG on various knowledge-intensive
tasks, achieving superior performance over existing data-free methods. We
explore scenarios where additional data is available, demonstrating LAG's
compatibility with alternative solutions such as retrieval-augmented generation
(RAG).

</details>


### [89] [On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study](https://arxiv.org/abs/2507.05362)
*Riccardo Alberghi,Elizaveta Demyanenko,Luca Biggio,Luca Saglietti*

Main category: cs.CL

TL;DR: 长语言模型（LLMs）中的推理受益于合理的计算分配和系统化的增量推理。作者在分层图上的最短路径任务中发现，尽管低效推理可能更冗长，但却提升了模型的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 探讨计算分配和增量推理对LLMs推理能力的影响，并理解训练模式对模型泛化性的作用。

Method: 在分层图的最短路径任务中，训练解码器型transformer，比较使用动态规划优化的高效推理轨迹与包含回溯的低效但正确的推理轨迹。

Result: 在相同训练令牌预算下，基于低效推理轨迹训练的模型对新图的泛化能力更强。此结果并非因轨迹长度，而是因其更连贯、局部增量化的特性使优化更易。

Conclusion: 低效但结构化的推理轨迹（只要连贯且具有增量性）能增强模型的泛化能力，而非任意冗余。

Abstract: Recent advances in natural language processing highlight two key factors for
improving reasoning in large language models (LLMs): (i) allocating more
test-time compute tends to help on harder problems but often introduces
redundancy in the reasoning trace, and (ii) compute is most effective when
reasoning is systematic and incremental, forming structured chains of thought
(CoTs) akin to human problem-solving. To study these factors in isolation, we
introduce a controlled setting based on shortest-path tasks in layered graphs.
We train decoder-only transformers on question-trace-answer triples using a
custom tokenizer, comparing models trained on optimal bottom-up dynamic
programming traces with those trained on longer, valid traces involving
backtracking. Surprisingly, with the same training-token budget, models trained
on inefficient traces generalize better to unseen graphs. This benefit is not
due to length alone-injecting arbitrary redundancy into reasoning traces fails
to help and can even hurt performance. Instead, we find that generalization
correlates with the model's confidence in next-token prediction, suggesting
that long, coherent, and locally incremental traces make the training signal
easier to optimize.

</details>


### [90] [EduCoder: An Open-Source Annotation System for Education Transcript Data](https://arxiv.org/abs/2507.05385)
*Guanzhong Pan,Mei Tan,Hyunji Nam,Lucía Langlois,James Malamut,Liliana Deonizio,Dorottya Demszky*

Main category: cs.CL

TL;DR: 本文介绍了一款名为EduCoder的开源工具，专门用于教育对话的逐句标注，支持复杂代码本定义、多种标注类型以及数据上下文化。


<details>
  <summary>Details</summary>
Motivation: 现有的一般自然语言处理和质性研究的文本标注工具难以处理教育对话转录中的复杂交互需求，如定义复杂教学特征的代码本、支持多种标注类型以及上下文化对话内容。

Method: EduCoder通过为研究人员和领域专家提供一个合作平台来定义代码本，支持分类和开放式标注类型，还结合了上下文材料。该系统还支持对多个标注者的结果进行并排比较与校准。

Result: 系统能够更灵活地应对教育对话中的注释需求，提升数据可靠性，并且所有功能开源可用。

Conclusion: EduCoder提供了一个创新的工具，解决了教育对话中注释的复杂性问题，为教育研究的标注带来了新可能。

Abstract: We introduce EduCoder, a domain-specialized tool designed to support
utterance-level annotation of educational dialogue. While general-purpose text
annotation tools for NLP and qualitative research abound, few address the
complexities of coding education dialogue transcripts -- with diverse
teacher-student and peer interactions. Common challenges include defining
codebooks for complex pedagogical features, supporting both open-ended and
categorical coding, and contextualizing utterances with external features, such
as the lesson's purpose and the pedagogical value of the instruction. EduCoder
is designed to address these challenges by providing a platform for researchers
and domain experts to collaboratively define complex codebooks based on
observed data. It incorporates both categorical and open-ended annotation types
along with contextual materials. Additionally, it offers a side-by-side
comparison of multiple annotators' responses, allowing comparison and
calibration of annotations with others to improve data reliability. The system
is open-source, with a demo video available.

</details>


### [91] [The Generalization Ridge: Information Flow in Natural Language Generation](https://arxiv.org/abs/2507.05387)
*Ruidi Chang,Chunyuan Deng,Hanjie Chen*

Main category: cs.CL

TL;DR: 本文通过提出信息论框架InfoRidge，揭示了Transformer语言模型中，中间层在信息表征和任务泛化中的核心作用。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在自然语言生成任务中表现卓越，但其如何在不同层次合成任务相关信息的内部机制尚不清楚。研究中试图探究模型泛化能力如何在训练中随着深度传播。

Method: 提出了InfoRidge框架，通过互信息的计算追踪模型在训练过程中任务相关信息的流动，并引入残差缩放系数作为功能探针评估Transformer层的重要性。

Result: 实验表明，预测信息在上中层（形成"泛化山脊"）达到峰值，随后在最终层下降，体现了从泛化到记忆的转变。此外，分布变化条件下模型更加依赖"山脊层"而非最终层。

Conclusion: 研究表明中间层对支持泛化至关重要，同时提供了对Transformer模型内部机制的新见解。

Abstract: Transformer-based language models have achieved state-of-the-art performance
in natural language generation (NLG) tasks, yet their internal mechanisms for
synthesizing task-relevant information remain insufficiently understood. While
prior studies suggest that intermediate layers often yield more generalizable
representations than final layers, how this generalization ability emerges and
propagates across layers during training remains unclear. To address this gap,
we propose InfoRidge, an information-theoretic framework, to characterize how
predictive information-the mutual information between hidden representations
and target outputs-varies across depth. Estimating this quantity enables us to
trace the flow of task-relevant information throughout the model during
training. Our experiments across various models and datasets reveal a
consistent non-monotonic trend: predictive information peaks in upper-middle
layers-forming a generalization ridge-before declining in final layers,
reflecting a transition between generalization and memorization. To further
investigate this phenomenon, we introduce residual scaling
coefficients-trainable scalar parameters applied to each residual block-which
serve as functional probes for assessing the relative importance of individual
transformer layers. These coefficients reveal that, under distribution shift,
models downweight final layers and increasingly rely on ridge layers,
highlighting their role in generalization. Together, these findings offer new
insights into the internal mechanisms of transformers and underscore the
critical role of intermediate layers in supporting generalization.

</details>


### [92] [Controlling What You Share: Assessing Language Model Adherence to Privacy Preferences](https://arxiv.org/abs/2507.05391)
*Guillem Ramírez,Alexandra Birch,Ivan Titov*

Main category: cs.CL

TL;DR: 本研究提出了一种利用隐私概要来保护用户数据隐私的方法，研究了本地模型通过自然语言指令修改查询以隐藏敏感信息，从而允许用户控制数据分享。


<details>
  <summary>Details</summary>
Motivation: 目前访问大型语言模型（LLMs）通常需要通过商业API，这可能导致用户数据暴露。因此，研究者希望找到一种在不牺牲性能的情况下保护数据隐私的方法。

Method: 研究提出使用隐私概要（简明的自然语言指令）和本地模型框架，用户可以通过指令定义敏感信息，模型基于指令重写查询以隐藏这些信息，并将其发送至外部模型处理。

Result: 引入了PEEP，一个多语言用户查询数据集，标注了敏感内容并配有合成的隐私概要。实验表明轻量级LLMs可以部分遵循隐私指令，但面临一致性挑战。

Conclusion: 当前模型对用户定义的隐私偏好理解仍需改进，未来需要更好地处理用户隐私保护与性能平衡问题。

Abstract: Large language models (LLMs) are primarily accessed via commercial APIs, but
this often requires users to expose their data to service providers. In this
paper, we explore how users can stay in control of their data by using privacy
profiles: simple natural language instructions that say what should and should
not be revealed. We build a framework where a local model uses these
instructions to rewrite queries, only hiding details deemed sensitive by the
user, before sending them to an external model, thus balancing privacy with
performance. To support this research, we introduce PEEP, a multilingual
dataset of real user queries annotated to mark private content and paired with
synthetic privacy profiles. Our experiments with lightweight LLMs show they can
follow these instructions to some extent, but also face consistent challenges,
highlighting the need for models that better understand and comply with
user-defined privacy preferences.

</details>


### [93] [Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning](https://arxiv.org/abs/2507.05418)
*Jaedong Hwang,Kumar Tanmay,Seok-Jin Lee,Ayush Agrawal,Hamid Palangi,Kumar Ayush,Ila Fiete,Paul Pu Liang*

Main category: cs.CL

TL;DR: 该研究提出GeoFact-X，这是一个涵盖五种语言的地理多语言事实推理基准,并引入了一种新的训练方法BRIDGE，用于提升多语言推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模语言模型在多语言推理方面，尤其是低资源语言上表现不足，因此需要改进这一偏向高资源语言的问题。

Method: 提出GeoFact-X基准，包含五种语言，并使用BRIDGE训练方法结合语言一致性奖励进行多语言推理训练，同时实验设计了一种自动评估协议。

Result: BRIDGE方法显著提高了模型在多语言推理中的一致性和泛化能力。

Conclusion: 多语言推理需要基于推理过程的强化学习来增强跨语言的泛化能力。

Abstract: Large Language Models (LLMs) have achieved strong performance in domains like
mathematics, factual QA, and code generation, yet their multilingual reasoning
capabilities in these tasks remain underdeveloped. Especially for low-resource
languages such as Swahili or Thai, LLMs can often misinterpret prompts or
default to reasoning in English. This implicit bias toward high-resource
languages undermines factual accuracy, interpretability, and trust. Current
multilingual benchmarks focus only on final answers, overlooking whether models
actually reason in the target language. To address this gap, we introduce
GeoFact-X, a geography-based multilingual factual reasoning benchmark with
annotated reasoning traces in five languages: English, Hindi, Japanese,
Swahili, and Thai. We further propose BRIDGE, a novel training method that
guides supervised fine-tuning and test-time reinforcement learning with a
language-consistency reward to align reasoning with the input language.
Finally, we develop an automatic evaluation protocol using LLM-as-a-judge to
assess answer correctness and the quality and language consistency of reasoning
traces, enabling nuanced and scalable analysis beyond surface-level metrics.
Our results show that BRIDGE significantly enhances multilingual reasoning
fidelity, demonstrating that reasoning-aware multilingual reinforcement
learning is crucial for robust cross-lingual generalization.
https://jd730.github.io/projects/GeoFact-X_BRIDGE

</details>


### [94] ["Lost-in-the-Later": Framework for Quantifying Contextual Grounding in Large Language Models](https://arxiv.org/abs/2507.05424)
*Yufei Tao,Adam Hiatt,Rahul Seetharaman,Ameeta Agrawal*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型如何在处理问题时整合和优先使用上下文知识 (CK) 和参数化知识 (PK)。通过提出CoPE评估框架，作者揭示了这些模型存在位置性偏差，并提出改进提示方法以优化其性能。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在集成上下文知识和参数化知识时的优先级和方式，对于改进模型的推理能力和实际应用具有重要意义。

Method: 提出了CoPE评估框架，基于多个语言环境（英语、西班牙语和丹麦语）的MultiWikiAtomic数据集，分析语言模型处理上下文信息与整合PK的表现。同时设计了多种提示方法并进行了案例研究。

Result: 发现模型存在位置性偏差，即倾向于忽略或低估上下文中较晚出现的信息。此外，链式思维（CoT）提示方式在提升推理能力的同时降低了模型对上下文的使用效率。改进的提示方法能够提升事实性总结的准确性并减少幻觉现象。

Conclusion: 通过使用CoPE框架，揭示了大型语言模型在上下文整合中的缺陷，并展示了改进提示技术能够提升模型性能的可能性。

Abstract: Large language models are capable of leveraging both contextual and
parametric knowledge but how they prioritize and integrate these sources
remains underexplored. We introduce CoPE, a novel evaluation framework that
systematically measures contextual knowledge (CK) and parametric knowledge (PK)
across models and languages. Using our MultiWikiAtomic dataset in English,
Spanish, and Danish, we analyze how large language models (LLMs) integrate
context, prioritize information, and incorporate PK in open-ended question
answering. Our analysis uncovers a phenomenon we call lost-in-the-later, where
LLMs tend to overlook or deprioritize information that appears later in a given
context, revealing a strong positional bias that affects contextual grounding.
We further find that reasoning models, as well as non-reasoning models prompted
with chain-of-thought (CoT), use context even less than non-reasoning models
without CoT and fail to mitigate the lost-in-the-later effect. CoT prompting,
in particular, results in lower recall and shorter responses, leading to
degraded contextual grounding. Based on these insights, we design prompt-based
methods to effectively leverage input context. A case study applying CoPE to
summarization demonstrates that CK-informed prompting improves factual
grounding and reduces hallucination.

</details>


### [95] [Gendered Divides in Online Discussions about Reproductive Rights](https://arxiv.org/abs/2507.05443)
*Ashwin Rao,Sze Yuh Nina Wang,Kristina Lerman*

Main category: cs.CL

TL;DR: 2022年美国最高法院关于Dobbs v. Jackson Women's Health Organization的裁决在生育权国家辩论中具有重要意义，研究分析了涵盖近一千万条与堕胎相关的X（原Twitter）帖子，揭示了性别、保守地区与意识形态之间的互动对公众话语的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨性别与地方社会政治背景如何在堕胎权问题上影响公众话语及其情绪表达。

Method: 分析近1000万条堕胎相关的X平台帖子，并推断用户性别、意识形态和位置，研究性别是否以及如何影响堕胎态度和情绪表达，特别是在保守地区。

Result: 性别显著影响堕胎态度和情绪表达，特别在保守地区更为明显，且与意识形态无关。此外，在权利受到威胁的地区，Dobbs裁决草案泄露增加了亲堕胎女性的在线互动。

Conclusion: 研究揭示了堕胎话语不仅意识形态上分化，而且深受性别和地区结构化影响，凸显身份在体制变革时塑造政治表达的中心作用。

Abstract: The U.S. Supreme Court's 2022 ruling in Dobbs v. Jackson Women's Health
Organization marked a turning point in the national debate over reproductive
rights. While the ideological divide over abortion is well documented, less is
known about how gender and local sociopolitical contexts interact to shape
public discourse. Drawing on nearly 10 million abortion-related posts on X
(formerly Twitter) from users with inferred gender, ideology and location, we
show that gender significantly moderates abortion attitudes and emotional
expression, particularly in conservative regions, and independently of
ideology. This creates a gender gap in abortion attitudes that grows more
pronounced in conservative regions. The leak of the Dobbs draft opinion further
intensified online engagement, disproportionately mobilizing pro-abortion women
in areas where access was under threat. These findings reveal that abortion
discourse is not only ideologically polarized but also deeply structured by
gender and place, highlighting the central role of identity in shaping
political expression during moments of institutional disruption.

</details>


### [96] [PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically Distant Language Pairs](https://arxiv.org/abs/2507.05444)
*Sana Kang,Myeongseok Gwon,Su Young Kwon,Jaewook Lee,Andrew Lan,Bhiksha Raj,Rita Singh*

Main category: cs.CL

TL;DR: 本文提出了一个名为PhoniTale的跨语种助记生成系统，通过音韵相似性检索及LLMs生成助记，有助于L2词汇学习，尤其是语言结构迥异的情况如英语到韩语。


<details>
  <summary>Details</summary>
Motivation: 解决L2学习者在学习语言跨度较大的语言(如英语和韩语)时，由于音韵及结构上的差异导致的词汇学习困难问题。

Method: 提出PhoniTale系统，基于L1和L2的音韵相似性检索L1关键字序列，再利用LLMs生成助记，并通过自动化指标及人工评估测试其效果。

Result: 实验表明，PhoniTale生成的助记效果与人为编写的助记相当，对提高词汇短期记忆有效，同时为未来改进指出方向。

Conclusion: PhoniTale能够有效生成跨语言的助记工具，对提升L2词汇学习提供有效帮助，但未来在助记质量和方法论的改进上仍有潜力可挖。

Abstract: Vocabulary acquisition poses a significant challenge for second-language (L2)
learners, especially when learning typologically distant languages such as
English and Korean, where phonological and structural mismatches complicate
vocabulary learning. Recently, large language models (LLMs) have been used to
generate keyword mnemonics by leveraging similar keywords from a learner's
first language (L1) to aid in acquiring L2 vocabulary. However, most of this
research has focused on native English speakers learning other languages,
rather than the reverse. In this paper, we present PhoniTale, a novel
cross-lingual mnemonic generation system that retrieves L1 keyword sequence
based on phonological similarity and uses LLMs to generate mnemonics. We
evaluate PhoniTale using both automated metrics and human evaluations,
comparing its output to mnemonics created by humans and by previous automated
approaches. To assess practical effectiveness, we also conduct a short-term
recall test measuring mnemonic helpfulness. Our findings show that PhoniTale
performs comparably to human-authored mnemonics. We also highlight key areas
for future improvement in mnemonic quality and methodology.

</details>


### [97] [On the Semantics of Large Language Models](https://arxiv.org/abs/2507.05448)
*Martin Schuele*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型(LLMs)在词汇和句子语义层面的理解能力，并借助经典语义理论进行分析。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs的语义能力及其是否真正理解语言，这一问题在学术界和工业界具有争议性。

Method: 通过研究LLMs的内部运作及其语言生成表征，并结合经典语义理论如Frege和Russell的思想进行分析。

Result: 获得了关于LLMs在语义能力上更为细致入微的认识。

Conclusion: 尽管LLMs表现出语言生成能力，但在语言语义理解方面仍存在争议，需要进一步研究其语义潜力。

Abstract: Large Language Models (LLMs) such as ChatGPT demonstrated the potential to
replicate human language abilities through technology, ranging from text
generation to engaging in conversations. However, it remains controversial to
what extent these systems truly understand language. We examine this issue by
narrowing the question down to the semantics of LLMs at the word and sentence
level. By examining the inner workings of LLMs and their generated
representation of language and by drawing on classical semantic theories by
Frege and Russell, we get a more nuanced picture of the potential semantic
capabilities of LLMs.

</details>


### [98] [ModelCitizens:Representing Community Voices in Online Safety](https://arxiv.org/abs/2507.05455)
*Ashima Suvarna,Christina Chance,Hamid Palangi,Sophie Hao,Thomas Hartvigsen,Saadia Gabriel*

Main category: cs.CL

TL;DR: 引入了一个新数据集（MODELCITIZENS）和两个改进的模型（LLAMACITIZEN-8B和GEMMACITIZEN-12B），用于更精确地检测社交媒体中的有害语言，尤其是包含语境的帖文。


<details>
  <summary>Details</summary>
Motivation: 现有的有害语言检测模型通常将多样化的标注观点简化为单一的“标准答案”，忽略了社区背景和语言使用的细腻差异，导致检测结果不够准确和包容。

Method: 构建了包含6.8K社交媒体帖子和40K标注的新数据集MODELCITIZENS；它通过大语言模型（LLM）生成的对话场景来补充帖子语境。同时发布了改进模型（LLAMACITIZEN-8B和GEMMACITIZEN-12B），以更好适配该数据集。

Result: 现有工具（如OpenAI Moderation API，GPT-o4-mini）在MODELCITIZENS数据集上的表现较差，尤其是在语境增强的帖子上。而LLAMACITIZEN-8B和GEMMACITIZEN-12B在数据集评估中比GPT-o4-mini提升了5.5%。

Conclusion: 研究证明了带有社区背景和语境信息的标注和建模，对于构建更包容和高效的内容审查系统至关重要。

Abstract: Automatic toxic language detection is critical for creating safe, inclusive
online spaces. However, it is a highly subjective task, with perceptions of
toxic language shaped by community norms and lived experience. Existing
toxicity detection models are typically trained on annotations that collapse
diverse annotator perspectives into a single ground truth, erasing important
context-specific notions of toxicity such as reclaimed language. To address
this, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K
toxicity annotations across diverse identity groups. To capture the role of
conversational context on toxicity, typical of social media posts, we augment
MODELCITIZENS posts with LLM-generated conversational scenarios.
State-of-the-art toxicity detection tools (e.g. OpenAI Moderation API,
GPT-o4-mini) underperform on MODELCITIZENS, with further degradation on
context-augmented posts. Finally, we release LLAMACITIZEN-8B and
GEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS,
which outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our
findings highlight the importance of community-informed annotation and modeling
for inclusive content moderation.

</details>


### [99] [Empowering Healthcare Practitioners with Language Models: Structuring Speech Transcripts in Two Real-World Clinical Applications](https://arxiv.org/abs/2507.05517)
*Jean-Philippe Corbeil,Asma Ben Abacha,George Michalopoulos,Phillip Swazinna,Miguel Del-Agua,Jerome Tremblay,Akila Jeeson Daniel,Cari Bader,Kevin Cho,Pooja Krishnan,Nathan Bodenstab,Thomas Lin,Wenxuan Teng,Francois Beaulieu,Paul Vozila*

Main category: cs.CL

TL;DR: 本文提出并分析了两项临床自然语言处理任务的解决方案，并公开了首个相关数据集。


<details>
  <summary>Details</summary>
Motivation: 减轻医护人员书写负担，让他们更专注于患者护理。

Method: 使用私有和开源的临床数据集评估LLMs（包含开源和封闭模型），并提出生成非敏感护士记录的管道系统。

Result: 论文创建了两个新数据集：SYNUR和SIMORD，并证明了其用于结构化信息抽取的有效性。

Conclusion: 本文为临床NLP的两个未充分研究领域提供了初步解决方案及开源资源，能推动相关研究进展。

Abstract: Large language models (LLMs) such as GPT-4o and o1 have demonstrated strong
performance on clinical natural language processing (NLP) tasks across multiple
medical benchmarks. Nonetheless, two high-impact NLP tasks - structured tabular
reporting from nurse dictations and medical order extraction from
doctor-patient consultations - remain underexplored due to data scarcity and
sensitivity, despite active industry efforts. Practical solutions to these
real-world clinical tasks can significantly reduce the documentation burden on
healthcare providers, allowing greater focus on patient care. In this paper, we
investigate these two challenging tasks using private and open-source clinical
datasets, evaluating the performance of both open- and closed-weight LLMs, and
analyzing their respective strengths and limitations. Furthermore, we propose
an agentic pipeline for generating realistic, non-sensitive nurse dictations,
enabling structured extraction of clinical observations. To support further
research in both areas, we release SYNUR and SIMORD, the first open-source
datasets for nurse observation extraction and medical order extraction.

</details>


### [100] [Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS](https://arxiv.org/abs/2507.05557)
*Alex ZH Dou,Zhongwei Wan,Dongfei Cui,Xin Wang,Jing Xiong,Haokun Lin,Chaofan Tao,Shen Yan,Mi Zhang*

Main category: cs.CL

TL;DR: 本文介绍了一种名为R2-LLMs的新型分层检索增强推理框架，用于改善大型语言模型的测试时推理能力，而无需依赖更高级模型获取链式推理训练数据。


<details>
  <summary>Details</summary>
Motivation: 测试时扩展计算资源可以显著提升语言模型的性能。然而，当前方法需要依赖更大模型的蒸馏数据，这增加了模型训练的复杂性。

Method: R2-LLMs利用双层检索框架实现推理增强：第一层提取问题与答案对以帮助高层次推理；第二层在搜索过程中检索中间步骤，并用奖励模型优化决策过程。

Result: 在MATH500、GSM8K和OlympiadBench-TO数据集上的实验证明，R2-LLMs与基线相比推理性能最多提升16%。

Conclusion: R2-LLMs展示了通过一个灵活的分层框架，如何在复杂推理任务中显著增强大型语言模型的性能。

Abstract: Test-time scaling has emerged as a promising paradigm in language modeling,
leveraging additional computational resources at inference time to enhance
model performance. In this work, we introduce R2-LLMs, a novel and versatile
hierarchical retrieval-augmented reasoning framework designed to improve
test-time scaling in large language models (LLMs) without requiring
distillation from more advanced models to obtain chain-of-thought (CoT)
training data. R2-LLMs enhances inference-time generalization by integrating
dual-level retrieval-based in-context learning: (1) At the coarse level, our
approach extracts abstract templates from complex reasoning problems and
retrieves similar problem-answer pairs to facilitate high-level in-context
learning; (2) At the fine level, during Monte Carlo Tree Search (MCTS), R2-LLMs
efficiently retrieves analogous intermediate solution steps from reference
mathematical problem datasets, refining step-wise reasoning with the aid of a
process reward model (PRM) for scoring. R2-LLMs is a robust hierarchical
reasoning-augmentation method that enhances in-context-level reasoning while
seamlessly integrating with step-level tree search methods. Utilizing PRM, it
refines both candidate generation and decision-making for improved reasoning
accuracy. Empirical evaluations on the MATH500, GSM8K, and OlympiadBench-TO
datasets achieve substantial relative improvement with an increase of up to 16%
using LLaMA-3.1-8B compared to the baselines, showcasing the effectiveness of
our approach in complex reasoning tasks.

</details>


### [101] [Self-Review Framework for Enhancing Instruction Following Capability of LLM](https://arxiv.org/abs/2507.05598)
*Sihyun Park*

Main category: cs.CL

TL;DR: Re5框架结合了任务和约束提取、结构评估、内容评估和选择性修订，有效增强了大语言模型指令跟随能力，同时保持生成内容质量，实验中表现优于多个基线。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在单次生成中难以完全遵守复杂指令，现有修订方法成本高、输出质量可能下降。

Method: 提出Re5框架，通过提取任务与约束组件、结构评估防止错误累积、细粒度内容评估与选择性修订，增强内容质量与指令执行效果。

Result: Re5实现了与GPT-4o-mini生成数据模型相当的指令跟随能力，且在保持回答质量的同时，对初始响应有64.24%的胜率。

Conclusion: Re5是一种高效的方式，能在减少监督的情况下显著提高指令遵守性能和生成内容质量。

Abstract: Various techniques have been proposed to improve large language models (LLMs)
adherence to formatting and instruction constraints. One of the most effective
approaches involves utilizing high-quality data generated by powerful models.
However, such models often fail to fully comply with complex instructions in a
single generation. To address this limitation, iterative revision methods have
been introduced. Nevertheless, as the number of data points and revision
iterations increases, the associated monetary costs grow significantly. As a
resource-efficient alternative, methods have been proposed that leverage
high-performance evaluation tools to compensate for the limited self-evaluation
capabilities of open-source LLMs. However, these approaches often lead to a
degradation in output quality due to excessive revision. To overcome these
challenges, we propose Re5, a self-evaluation and revision framework designed
to enhance instruction-following performance while preserving the quality of
the generated content. Re5 extracts task and constraint components from user
instructions, performs structural evaluations to prevent error accumulation,
and applies fine-grained constraint-specific content evaluations followed by
selective revisions. This process ensures precise and quality-preserving
improvements. The final high-quality outputs are used for alignment tuning,
enabling long-term alignment improvements through a data-centric iterative
refinement loop. Experimental results demonstrate that Re5 achieves
instruction-following performance comparable to models trained on data
generated by GPT-4o-mini, a high-performance model, even with a small amount of
data while maintaining response quality with a 64.24%-win rate over the
non-revised initial responses. These results validate Re5 as an efficient and
effective solution for enhancing instruction adherence with minimal external
supervision.

</details>


### [102] [Flipping Knowledge Distillation: Leveraging Small Models' Expertise to Enhance LLMs in Text Matching](https://arxiv.org/abs/2507.05617)
*Mingzhe Li,Jing Xiang,Qishen Zhang,Kaiyang Wan,Xiuying Chen*

Main category: cs.CL

TL;DR: 提出了一种反转知识蒸馏的方法，让大型语言模型(LLM)从小型语言模型(SLM)中学习，以在特定领域任务中提升性能。


<details>
  <summary>Details</summary>
Motivation: 在特定领域任务中，小型语言模型(SLM)通过优化输入对的相似性，通常能在细化表示方面表现优异，但传统知识蒸馏主要是从LLM向SLM转移知识，未能充分利用小模型的特长。

Method: 通过LoRA将解码器模型的LLM重新框定为编码-解码结构，结合提出的边界感知对比学习(MCL)，实现LLM从SLM学习相似性得分的能力。

Result: 实验在金融、医疗等领域基准测试和真实应用中验证了该方法的有效性，并成功部署于在线环境中。

Conclusion: 证明了通过反转知识蒸馏，可有效结合大型语言模型的语义能力与小模型的特定领域表现优势，提升整体模型性能。

Abstract: Knowledge distillation typically involves transferring knowledge from a Large
Language Model (LLM) to a Smaller Language Model (SLM). However, in tasks such
as text matching, fine-tuned smaller models often yield more effective
domain-specific representations, as they focus on optimizing the similarity of
input pairs. To leverage both the specialized strengths of small models and the
rich semantic understanding of LLMs, we introduce a flipped knowledge
distillation paradigm, where LLM learns from SLM. Specifically, we address the
architectural gap between decoder-only LLMs and smaller encoder-based models by
reinterpreting LLMs in an encoder-decoder manner using LoRA. The encoder
generates compressed representations, while the decoder maps them to the output
space. During training, the encoder produces representations and their
similarities, which are then aligned with the similarity scores produced by the
teacher, using our proposed Margin-aware Contrastive Learning (MCL) approach.
The MCL ensures accurate similarity for both positive and negative pairs, and
adaptively handles the internal differences within positive and negative
samples. Our paradigm requires only a reasonably good-performing SLM, allowing
the LLM to achieve improved performance. Experiments on financial and
healthcare benchmarks, as well as real-world applications, confirm its
effectiveness, and the model has been fully deployed in an online environment.

</details>


### [103] [SARA: Selective and Adaptive Retrieval-augmented Generation with Context Compression](https://arxiv.org/abs/2507.05633)
*Yiqiao Jin,Kartik Sharma,Vineeth Rakesh,Yingtong Dou,Menghai Pan,Mahashweta Das,Srijan Kumar*

Main category: cs.CL

TL;DR: 这篇论文提出SARA，一个结合文本和语义压缩表示的检索增强生成框架，旨在提升上下文效率和答案正确性。


<details>
  <summary>Details</summary>
Motivation: 解决传统RAG模型在上下文长度受限和检索文档冗余方面的问题，同时保持细粒度信息的准确性和全局知识覆盖。

Method: 设计了SARA框架，将自然语言片段和语义压缩向量结合，用以平衡上下文精度和知识覆盖，并通过迭代证据选择模块进行动态上下文重新排序。

Result: 在9个数据集和5种开源LLM中，SARA显著提升了答案相关性（+17.71）、正确性（+13.72）和语义相似性（+15.53）。

Conclusion: 本文证明了文本与压缩表示的结合能够增强RAG的鲁棒性和上下文效率，同时在多个模型和数据集上表现出一致的改进。

Abstract: Retrieval-augmented Generation (RAG) extends large language models (LLMs)
with external knowledge but faces key challenges: restricted effective context
length and redundancy in retrieved documents. Pure compression-based approaches
reduce input size but often discard fine-grained details essential for factual
accuracy. We propose SARA, a unified RAG framework that balances local
precision and global knowledge coverage under tight context budgets. SARA
combines natural-language text snippets with semantic compression vectors to
jointly enhance context efficiency and answer correctness. It represents
contexts at two complementary levels: 1) fine-grained natural-language spans
that preserve critical entities and numerical values, and 2) compact,
interpretable vectors that summarize high-level semantics. An iterative
evidence-selection module employs the compression vectors for dynamic reranking
of contexts. Across 9 datasets and 5 open-source LLMs spanning 3 model families
(Mistral, Llama, and Gemma), SARA consistently improves answer relevance
(+17.71), answer correctness (+13.72), and semantic similarity (+15.53),
demonstrating the importance of integrating textual and compressed
representations for robust, context-efficient RAG.

</details>


### [104] [ECom-Bench: Can LLM Agent Resolve Real-World E-commerce Customer Support Issues?](https://arxiv.org/abs/2507.05639)
*Haoxin Wang,Xianhan Peng,Xucheng Huang,Yizhe Huang,Ming Gong,Chenghan Yang,Yang Liu,Ling Jiang*

Main category: cs.CL

TL;DR: 本文提出了ECom-Bench，这是一个用于在电商客服领域评估多模态能力的大语言模型（LLM）代理的基准框架。


<details>
  <summary>Details</summary>
Motivation: 现有模型在处理电商复杂场景时表现有限，缺乏针对这一领域的专门基准。

Method: 创建了一个动态用户模拟系统，基于真实电商客服交互中的角色信息与任务数据集，覆盖多种商业场景，构建挑战性测试框架。

Result: 即使是先进的模型如GPT-4o，在基准测试中仅达到10%-20%的pass^3指标性能，表明任务难度较高。

Conclusion: ECom-Bench填补了电商客服领域多模态LLM评测的空白，代码与数据开源将推动相关研究与发展。

Abstract: In this paper, we introduce ECom-Bench, the first benchmark framework for
evaluating LLM agent with multimodal capabilities in the e-commerce customer
support domain. ECom-Bench features dynamic user simulation based on persona
information collected from real e-commerce customer interactions and a
realistic task dataset derived from authentic e-commerce dialogues. These
tasks, covering a wide range of business scenarios, are designed to reflect
real-world complexities, making ECom-Bench highly challenging. For instance,
even advanced models like GPT-4o achieve only a 10-20% pass^3 metric in our
benchmark, highlighting the substantial difficulties posed by complex
e-commerce scenarios. Upon publication, the code and data will be open-sourced
to facilitate further research and development in this domain.

</details>


### [105] [Smoothie-Qwen: Post-Hoc Smoothing to Reduce Language Bias in Multilingual LLMs](https://arxiv.org/abs/2507.05686)
*SeungWon Ji,Jungyup Lee,Jemin Kim,Sang Park,SeungJae Lee*

Main category: cs.CL

TL;DR: 提出了一种叫Smoothie-Qwen的方法来解决多语种大语言模型的语言混淆问题，不需要重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 多语种模型生成的内容常常受到一种主导语言的影响，为了解决语言混淆问题，提供了一种轻量级的方法来减轻这种语言偏差。

Method: 提出一种后处理方法Smoothie-Qwen，通过选择性地调整词元级别的输出概率，以抑制非目标语言的生成。

Result: 将该方法应用到Qwen模型中，结果减少了95%以上的无意中文输出，同时在多语测试基准上保持了任务准确率。

Conclusion: 这一方法为提升大语言模型的语言可控性提供了高效、实用的解决方案，有助于推动其在全球范围内的应用。

Abstract: Multilingual large language models (LLMs) often exhibit language confusion, a
tendency to generate responses in a dominant language irrespective of the
prompt's language. To address this, we propose Smoothie-Qwen, a lightweight,
post-hoc method that mitigates language bias without retraining. This technique
selectively adjusts token-level output probabilities to effectively suppress
undesired language generation. Applied to the Qwen model, our method reduces
unintended Chinese output by over 95% while preserving task accuracy on
multilingual benchmarks. This work provides a practical and efficient solution
for enhancing the language controllability of LLMs, making them more reliable
for global applications.

</details>


### [106] [Agentic-R1: Distilled Dual-Strategy Reasoning](https://arxiv.org/abs/2507.05707)
*Weihua Du,Pranjal Aggarwal,Sean Welleck,Yiming Yang*

Main category: cs.CL

TL;DR: 提出了'DualDistill'框架，从多个老师模型中蒸馏不同推理策略到一个统一的学生模型，并推出了Agentic-R1模型，显著提高了任务精度。


<details>
  <summary>Details</summary>
Motivation: 现有长链条推理模型虽擅长数学推理，但速度慢且容易出错，而工具增强型代理在复杂逻辑任务上表现不佳。

Method: 引入'DualDistill'微调框架，从多种推理策略中蒸馏并统一到一个动态选择最优策略的模型Agentic-R1上。

Result: 在计算密集型和标准基准测试上，该方法显著提升了准确性。

Conclusion: 多策略蒸馏方法可以实现稳健且高效的推理，展示了其有效性。

Abstract: Current long chain-of-thought (long-CoT) models excel at mathematical
reasoning but rely on slow and error-prone natural language traces.
Tool-augmented agents address arithmetic via code execution, but often falter
on complex logical tasks. We introduce a fine-tuning framework, DualDistill,
that distills complementary reasoning strategies from multiple teachers into a
unified student model. Using this approach, we train Agentic-R1, which
dynamically selects the optimal strategy for each query, invoking tools for
arithmetic and algorithmic problems, and using text-based reasoning for
abstract ones. Our method improves accuracy across a range of tasks, including
both computation-intensive and standard benchmarks, demonstrating the
effectiveness of multi-strategy distillation in achieving robust and efficient
reasoning. Our project is available at https://github.com/StigLidu/DualDistill

</details>


### [107] [DRAGON: Dynamic RAG Benchmark On News](https://arxiv.org/abs/2507.05713)
*Fedor Chernogorskii,Sergei Averkiev,Liliya Kudraleeva,Zaven Martirosian,Maria Tikhonova,Valentin Malykh,Alena Fenogenova*

Main category: cs.CL

TL;DR: 本文提出了一个名为DRAGON的动态基准，用于评估俄语的检索增强生成（RAG）系统，基于动态更新的新闻与公共文档语料库。


<details>
  <summary>Details</summary>
Motivation: 目前，RAG的评估资源主要集中在英语，其它语言尤其俄语的评估资源稀缺且缺乏动态性，无法反映真实部署中动态变化的需求。

Method: 作者开发了一个动态基准DRAGON，它基于定期更新的俄语新闻与公共文档语料，支持对检索和生成组件的全面评估。问题生成借助知识图谱自动完成，提取四种核心问题类型，并提供完整的评估框架和公共排行榜。

Result: DRAGON实现了对RAG系统的动态评估，支持多语言扩展，并通过公共排行榜促进了社区参与与比较。

Conclusion: DRAGON填补了俄语RAG动态基准的空白，通过动态更新的评估环境提升了对RAG系统的实际适用性评估能力。

Abstract: Retrieval-Augmented Generation (RAG) is a widely adopted approach for
improving the factuality of large language models (LLMs) by incorporating
external knowledge at inference time. Although there exist multiple RAG
benchmarks for English, evaluation resources for other languages, including
Russian, remain scarce and static, failing to capture the dynamic nature of
real-world deployments.
  In this work, we present DRAGON (Dynamic RAG Benchmark On News), the first
dynamic benchmark for evaluating RAG systems in Russian on a changing news
corpora. DRAGON is built upon a regularly updated corpus of Russian news and
public documents and supports comprehensive evaluation of both the retriever
and generator components. Question generation is performed automatically with
the use of Knowledge Graph constructed from the corpus and enables the
extraction of four core question types aligned with distinct subgraph patterns.
We release a complete evaluation framework comprising the pipeline for
automatic question generation, evaluation scripts, which are potentially
reusable for other languages and multilingual settings, and benchmark data. We
also launch a public leaderboard to encourage community participation and
comparison.

</details>


### [108] [HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation](https://arxiv.org/abs/2507.05714)
*YiHan Jiao,ZheHao Tan,Dan Yang,DuoLin Sun,Jie Feng,Jian Wang,Peng Wei*

Main category: cs.CL

TL;DR: 提出了一种名为HIRAG的分层思维指令微调方法，旨在提升RAG模型在解决实时信息和领域特定问题时的性能。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统在生成结果时，文件质量不一致及检索系统不完美的问题依然存在。此外，目前针对RAG任务的生成模型微调研究缺乏细化的侧重点和对连锁思维过程的深入运用。

Method: 提出了分层的三种能力：信息过滤、语义组合及基于自身和外部知识的推理，并设计了一种新的思维流程“先思考后回答”，用于指导模型的指令微调策略。

Result: 实验证明，HIRAG训练策略显著提升了RAG模型在RGB、PopQA、MuSiQue、HotpotQA和PubmedQA等数据集上的性能。

Conclusion: 通过采用分层连锁思维的指令微调方法，HIRAG有效解决了RAG任务中文档质量不一致和检索系统不足的挑战，提升了模型性能。

Abstract: Retrieval-augmented generation (RAG) has become a fundamental paradigm for
addressing the challenges faced by large language models in handling real-time
information and domain-specific problems. Traditional RAG systems primarily
rely on the in-context learning (ICL) capabilities of the large language model
itself. Still, in-depth research on the specific capabilities needed by the RAG
generation model is lacking, leading to challenges with inconsistent document
quality and retrieval system imperfections. Even the limited studies that
fine-tune RAG generative models often \textit{lack a granular focus on RAG
task} or \textit{a deeper utilization of chain-of-thought processes}. To
address this, we propose that RAG models should possess three progressively
hierarchical abilities (1) Filtering: the ability to select relevant
information; (2) Combination: the ability to combine semantic information
across paragraphs; and (3) RAG-specific reasoning: the ability to further
process external knowledge using internal knowledge. Thus, we introduce our new
RAG instruction fine-tuning method, Hierarchical-Thought Instruction-Tuning
Retrieval-Augmented Generation (HIRAG) incorporates a "think before answering"
strategy. This method enhances the model's open-book examination capability by
utilizing multi-level progressive chain-of-thought. Experiments show that the
HIRAG training strategy significantly improves the model's performance on
datasets such as RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA.

</details>


### [109] [Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition](https://arxiv.org/abs/2507.05724)
*Zijin Gu,Tatiana Likhomanenko,Navdeep Jaitly*

Main category: cs.CL

TL;DR: 本论文提出了Omni-router Transformer，通过共享不同MoE层的路由器提高层间专家的合作和专门化。


<details>
  <summary>Details</summary>
Motivation: 探索和改进混合专家模型（MoE）在自动语音识别（ASR）中的应用，解决传统模型中路由器选择独立性的问题。

Method: 通过在不同的MoE层之间共享一个通用路由器，提高专家之间的协作性和专门化水平。

Result: 在大规模伪标记数据集的实验和10个不同领域ASR基准上的评估显示，与密集模型和Switch Transformer相比，Omni-router Transformer将平均词错误率分别降低了11.2%和8.2%。

Conclusion: 提出的Omni-router Transformer实现了更低的训练损失和更好的鲁棒性，同时优化了专家的使用结构，表明其在各类数据上都有一致的优越表现。

Abstract: Mixture-of-experts (MoE) architectures have expanded from language modeling
to automatic speech recognition (ASR). Traditional MoE methods, such as the
Switch Transformer, route experts independently within each layer. Our analysis
reveals that routers in most layers make expert choices that are not strongly
correlated with the choices of the routers in other layers. To increase the
cooperation between experts in different layers and encourage greater
specialization, we use a shared router across different MoE layers. We call
this model \emph{Omni-router Transformer}. Extensive experiments on a
large-scale pseudo-labeled dataset and evaluations across 10 diverse,
out-of-domain ASR benchmarks demonstrate that the Omni-router Transformer is
able to achieve lower training loss and consistently outperform dense and
Switch Transformer models, reducing average word error rates by 11.2% and 8.2%,
respectively, while providing structured expert usage and improved robustness
to diverse data.

</details>


### [110] [GPTKB v1.5: A Massive Knowledge Base for Exploring Factual LLM Knowledge](https://arxiv.org/abs/2507.05740)
*Yujia Hu,Tuan-Phong Nguyen,Shrestha Ghosh,Moritz Müller,Simon Razniewski*

Main category: cs.CL

TL;DR: 论文介绍了GPTKB v1.5，这是一个从GPT-4.1中构建的100百万数据项的知识库，通过三种方式探索LLM的知识潜力和局限性。


<details>
  <summary>Details</summary>
Motivation: 语言模型的知识仍然难以被系统化理解和查询，因此需要一个可扩展的知识库以支持分析和浏览。

Method: 采用通过GPTKB方法进行大规模递归的知识实体化，从GPT-4.1中创建了一个100百万数据项的知识库。

Result: 创建了GPTKB v1.5，并展示了该知识库在知识探索、结构化查询以及知识优势与劣势对比中的应用。

Conclusion: GPTKB v1.5展现了将大语言模型知识转化成可用知识库的显著潜力，有助于更系统的语言模型知识研究与自动化知识库构建。

Abstract: Language models are powerful tools, yet their factual knowledge is still
poorly understood, and inaccessible to ad-hoc browsing and scalable statistical
analysis. This demonstration introduces GPTKB v1.5, a densely interlinked
100-million-triple knowledge base (KB) built for $14,000 from GPT-4.1, using
the GPTKB methodology for massive-recursive LLM knowledge materialization (Hu
et al., ACL 2025). The demonstration experience focuses on three use cases: (1)
link-traversal-based LLM knowledge exploration, (2) SPARQL-based structured LLM
knowledge querying, (3) comparative exploration of the strengths and weaknesses
of LLM knowledge. Massive-recursive LLM knowledge materialization is a
groundbreaking opportunity both for the research area of systematic analysis of
LLM knowledge, as well as for automated KB construction. The GPTKB demonstrator
is accessible at https://gptkb.org.

</details>


### [111] [DocTalk: Scalable Graph-based Dialogue Synthesis for Enhancing LLM Conversational Capabilities](https://arxiv.org/abs/2507.05750)
*Jing Yang Lee,Hamed Bonab,Nasser Zalmout,Ming Zeng,Sanket Lokegaonkar,Colin Lockard,Binxuan Huang,Ritesh Sarkhel,Haodong Wang*

Main category: cs.CL

TL;DR: 提出了一种从现有文本语料库中合成对话数据的方法，生成了DocTalk语料库以提升大语言模型的多轮对话能力。


<details>
  <summary>Details</summary>
Motivation: 为了应对大语言模型预训练数据与实际多轮对话任务需求不匹配的问题。

Method: 设计了一种管道，将相关文档的聚簇转化为多轮、多主题的信息交互对话，并生成了包含73万多次对话的DocTalk语料库。

Result: 实验表明，使用DocTalk进行预训练，可将上下文记忆和理解能力提升40%，且无损模型基础性能。

Conclusion: 通过引入合成对话数据进行预训练，大语言模型的多轮对话能力得到了显著改善。

Abstract: Large Language Models (LLMs) are increasingly employed in multi-turn
conversational tasks, yet their pre-training data predominantly consists of
continuous prose, creating a potential mismatch between required capabilities
and training paradigms. We introduce a novel approach to address this
discrepancy by synthesizing conversational data from existing text corpora. We
present a pipeline that transforms a cluster of multiple related documents into
an extended multi-turn, multi-topic information-seeking dialogue. Applying our
pipeline to Wikipedia articles, we curate DocTalk, a multi-turn pre-training
dialogue corpus consisting of over 730k long conversations. We hypothesize that
exposure to such synthesized conversational structures during pre-training can
enhance the fundamental multi-turn capabilities of LLMs, such as context memory
and understanding. Empirically, we show that incorporating DocTalk during
pre-training results in up to 40% gain in context memory and understanding,
without compromising base performance. DocTalk is available at
https://huggingface.co/datasets/AmazonScience/DocTalk.

</details>


### [112] [Flippi: End To End GenAI Assistant for E-Commerce](https://arxiv.org/abs/2507.05788)
*Anand A. Rajasekar,Praveen Tangarajan,Anjali Nainani,Amogh Batwal,Vinay Rao Dandin,Anusua Trivedi,Ozan Ersoy*

Main category: cs.CL

TL;DR: 本文介绍了一种名为Flippi的对话助手，基于大语言模型（LLMs）构建，专为电子商务环境设计，旨在通过自然语言对话帮助用户更高效地发现产品。


<details>
  <summary>Details</summary>
Motivation: 随着对话助手的兴起，用户与数字平台的交互形式发生了根本变化。本论文旨在通过推出一个专注于电子商务的对话系统，解决用户搜索商品时因信息过量而感到困扰的问题。

Method: 论文介绍了Flippi如何利用高级自然语言处理技术（如查询重构、意图检测、检索增强生成、命名实体识别和上下文缩减）来理解用户的自然语言查询，并提供精准的信息。此外，其能力包括推荐最优惠交易和进行产品比对等特性。

Result: Flippi展现了通过个性化的对话方式，实现更高效的用户参与和交易转化率的能力，从而提升了电子商务的用户体验。

Conclusion: 通过将在线购物的便利性与实体店个性化服务相结合，Flippi为数字市场中的客户满意度和参与度树立了新标杆，并具有良好的跨平台适配性。

Abstract: The emergence of conversational assistants has fundamentally reshaped user
interactions with digital platforms. This paper introduces Flippi-a
cutting-edge, end-to-end conversational assistant powered by large language
models (LLMs) and tailored for the e-commerce sector. Flippi addresses the
challenges posed by the vast and often overwhelming product landscape, enabling
customers to discover products more efficiently through natural language
dialogue. By accommodating both objective and subjective user requirements,
Flippi delivers a personalized shopping experience that surpasses traditional
search methods. This paper details how Flippi interprets customer queries to
provide precise product information, leveraging advanced NLP techniques such as
Query Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG),
Named Entity Recognition (NER), and Context Reduction. Flippi's unique
capability to identify and present the most attractive offers on an e-commerce
site is also explored, demonstrating how it empowers users to make
cost-effective decisions. Additionally, the paper discusses Flippi's
comparative analysis features, which help users make informed choices by
contrasting product features, prices, and other relevant attributes. The
system's robust architecture is outlined, emphasizing its adaptability for
integration across various e-commerce platforms and the technological choices
underpinning its performance and accuracy. Finally, a comprehensive evaluation
framework is presented, covering performance metrics, user satisfaction, and
the impact on customer engagement and conversion rates. By bridging the
convenience of online shopping with the personalized assistance traditionally
found in physical stores, Flippi sets a new standard for customer satisfaction
and engagement in the digital marketplace.

</details>


### [113] [Bridging Perception and Language: A Systematic Benchmark for LVLMs' Understanding of Amodal Completion Reports](https://arxiv.org/abs/2507.05799)
*Amane Watahiki,Tomoki Doi,Taiga Shinozaki,Satoshi Nishida,Takuya Niikawa,Katsunori Miyahara,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 研究了大型视觉语言模型（LVLMs）在理解与感知相关的描述性文本中的推理能力，尤其是在“非视域完形”现象上的表现及差异。


<details>
  <summary>Details</summary>
Motivation: 探讨LVLMs是否能通过解析文本来正确推测被遮挡物体的形状（非视域完形能力），并关注其在多语言场景下的表现差异。

Method: 构建了一个基于基础形式本体（Basic Formal Ontology）的评测基准，用以系统性分类和评估模型在非视域完形任务上的能力。

Result: 结果显示，尽管部分LVLMs整体表现接近人类，但对特定物体类型的非视域完形准确性差距显著；尤其在使用日语提示时表现差异更为显著。

Conclusion: LVLMs在多语言场景下的表现存在缺陷，尤其是日语相关的语言能力薄弱，需进一步提升其语言与视觉协同能力。

Abstract: One of the main objectives in developing large vision-language models (LVLMs)
is to engineer systems that can assist humans with multimodal tasks, including
interpreting descriptions of perceptual experiences. A central phenomenon in
this context is amodal completion, in which people perceive objects even when
parts of those objects are hidden. Although numerous studies have assessed
whether computer-vision algorithms can detect or reconstruct occluded regions,
the inferential abilities of LVLMs on texts related to amodal completion remain
unexplored. To address this gap, we constructed a benchmark grounded in Basic
Formal Ontology to achieve a systematic classification of amodal completion.
Our results indicate that while many LVLMs achieve human-comparable performance
overall, their accuracy diverges for certain types of objects being completed.
Notably, in certain categories, some LLaVA-NeXT variants and Claude 3.5 Sonnet
exhibit lower accuracy on original images compared to blank stimuli lacking
visual content. Intriguingly, this disparity emerges only under Japanese
prompting, suggesting a deficiency in Japanese-specific linguistic competence
among these models.

</details>


### [114] [How to Evaluate Automatic Speech Recognition: Comparing Different Performance and Bias Measures](https://arxiv.org/abs/2507.05885)
*Tanvina Patel,Wiebke Hutiri,Aaron Yi Ding,Odette Scharenborg*

Main category: cs.CL

TL;DR: 本文研究自动语音识别（ASR）系统对不同说话人群体的偏见问题，评估了多种性能和偏见测量方法，并提供了改进报告ASR性能和偏见的建议。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在检测和量化ASR系统中的偏见以及提出缓解偏见的方法，但具体如何有效测量系统性能与偏见仍然是一个未解决的问题。

Method: 本文比较了文献中已有的及新增的多种性能和偏见测量方法，评估了针对荷兰语的先进端到端ASR系统，并实验采用了一些偏见缓解策略。

Result: 研究发现，仅依赖平均错误率（ASR研究中的标准）不够充分，需补充其他测量指标。

Conclusion: 建议采用综合的性能和偏见报告方法，以更好地反映ASR系统对不同说话人群体的表现和总体偏见。

Abstract: There is increasingly more evidence that automatic speech recognition (ASR)
systems are biased against different speakers and speaker groups, e.g., due to
gender, age, or accent. Research on bias in ASR has so far primarily focused on
detecting and quantifying bias, and developing mitigation approaches. Despite
this progress, the open question is how to measure the performance and bias of
a system. In this study, we compare different performance and bias measures,
from literature and proposed, to evaluate state-of-the-art end-to-end ASR
systems for Dutch. Our experiments use several bias mitigation strategies to
address bias against different speaker groups. The findings reveal that
averaged error rates, a standard in ASR research, alone is not sufficient and
should be supplemented by other measures. The paper ends with recommendations
for reporting ASR performance and bias to better represent a system's
performance for diverse speaker groups, and overall system bias.

</details>


### [115] [Psychometric Item Validation Using Virtual Respondents with Trait-Response Mediators](https://arxiv.org/abs/2507.05890)
*Sungjib Lim,Woojung Song,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 本论文提出使用LLMs模拟虚拟受访者来生成高效且具有结构效度的心理测量问卷条目，从而节省传统的人类数据收集成本。


<details>
  <summary>Details</summary>
Motivation: 随着心理测量调查在评估LLMs性状中的应用增加，确保生成问卷条目能准确测量目标特质的结构效度变得尤为重要。

Method: 提出一种虚拟受访者模拟框架，利用LLMs生成中介变量，并通过模拟不同中介条件下的受访者行为，筛选出高效且高效度的问卷条目。

Result: 实验表明，在三种心理特质理论（Big5、Schwartz、VIA）的验证中，此模拟框架与中介生成方法能够有效识别具有高效度的条目。

Conclusion: 本研究开辟了针对成本效益优化的调查开发新方向，同时展示了LLMs在人类行为模拟中的潜力。研究还提供数据集与代码以支持后续研究。

Abstract: As psychometric surveys are increasingly used to assess the traits of large
language models (LLMs), the need for scalable survey item generation suited for
LLMs has also grown. A critical challenge here is ensuring the construct
validity of generated items, i.e., whether they truly measure the intended
trait. Traditionally, this requires costly, large-scale human data collection.
To make it efficient, we present a framework for virtual respondent simulation
using LLMs. Our central idea is to account for mediators: factors through which
the same trait can give rise to varying responses to a survey item. By
simulating respondents with diverse mediators, we identify survey items that
robustly measure intended traits. Experiments on three psychological trait
theories (Big5, Schwartz, VIA) show that our mediator generation methods and
simulation framework effectively identify high-validity items. LLMs demonstrate
the ability to generate plausible mediators from trait definitions and to
simulate respondent behavior for item validation. Our problem formulation,
metrics, methodology, and dataset open a new direction for cost-effective
survey development and a deeper understanding of how LLMs replicate human-like
behavior. We will publicly release our dataset and code to support future work.

</details>


### [116] [Few-shot text-based emotion detection](https://arxiv.org/abs/2507.05918)
*Teodor-George Marchitan,Claudiu Creanga,Liviu P. Dinu*

Main category: cs.CL

TL;DR: 该论文介绍了Unibuc - NLP团队在SemEval 2025任务11中，针对文本中情感检测的解决方案，主要用了几个大语言模型和不同的训练策略。


<details>
  <summary>Details</summary>
Motivation: 拓展和改善现有的基于文本的情感检测方法，尤其针对多语言和多情感标签问题。

Method: 使用Gemini、Qwen和DeepSeek大语言模型，通过少样本提示学习以及微调方法进行实验和模型开发。

Result: 在多标签情感检测赛道中，对于英语子集，取得了0.7546的F1-macro（排名26/96）；对葡萄牙语子集为0.1727（35/36）；对Emakhuwa子集为0.325（排名1/31）。

Conclusion: 证明了模型在多语言、多情感标签环境中的能力，尤其是在低资源语言（Emakhuwa）上的卓越表现。

Abstract: This paper describes the approach of the Unibuc - NLP team in tackling the
SemEval 2025 Workshop, Task 11: Bridging the Gap in Text-Based Emotion
Detection. We mainly focused on experiments using large language models
(Gemini, Qwen, DeepSeek) with either few-shot prompting or fine-tuning. With
our final system, for the multi-label emotion detection track (track A), we got
an F1-macro of $0.7546$ (26/96 teams) for the English subset, $0.1727$ (35/36
teams) for the Portuguese (Mozambican) subset and $0.325$ (\textbf{1}/31 teams)
for the Emakhuwa subset.

</details>


### [117] [Towards a Principled Evaluation of Knowledge Editors](https://arxiv.org/abs/2507.05937)
*Sebastian Pohl,Max Ploner,Alan Akbik*

Main category: cs.CL

TL;DR: 研究了知识编辑的评估方法问题，发现评估指标不同可能影响编辑器排名，并揭示现有方法可能有错误匹配倾向。


<details>
  <summary>Details</summary>
Motivation: 探讨知识编辑评估中的潜在问题，如方法鲁棒性和编辑对整体模型性能的干扰。

Method: 分析不同指标和编辑批次对知识编辑器排名的影响，同时手动评估现有评估方法的字符串匹配方式。

Result: 不同的评估方式会导致知识编辑器排名的差异。此外，当前方法的字符串匹配评估倾向于产生误匹配。

Conclusion: 现有评估指标和方法可能对知识编辑器有不公平倾向，优化评估标准显得十分重要。

Abstract: Model editing has been gaining increasing attention over the past few years.
For Knowledge Editing in particular, more challenging evaluation datasets have
recently been released. These datasets use different methodologies to score the
success of editors. Yet, it remains under-explored how robust these
methodologies are and whether they unfairly favor some editors. Moreover, the
disruptive impact of these editors on overall model capabilities remains a
constant blind spot.
  We address both of these problems and show that choosing different metrics
and evaluation methodologies as well as different edit batch sizes can lead to
a different ranking of knowledge editors. Crucially we demonstrate this effect
also on general language understanding tasks evaluated alongside the knowledge
editing tasks. Further we include a manual assessment of the string matching
based evaluation method for knowledge editing that is favored by recently
released datasets, revealing a tendency to produce false positive matches.

</details>


### [118] [Remember Past, Anticipate Future: Learning Continual Multimodal Misinformation Detectors](https://arxiv.org/abs/2507.05939)
*Bing Wang,Ximing Li,Mengzhe Ye,Changchun Li,Bo Fu,Jianfeng Qu,Lin Yuanbo Wu*

Main category: cs.CL

TL;DR: 提出了一种名为DAEDCMD的持续多模态错误信息检测（MMD）方法，用于应对在线数据流下的挑战，显示出显著优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在解决在持续在线数据流中的多模态错误信息检测问题，并克服常见的过往知识遗忘与社会环境演变导致的泛化问题。

Method: 提出结合Dirichlet过程的专家混合模型以隔离事件特定参数的干扰，并采用连续时间动态模型来预测未来环境分布，形成DAEDCMD方法。

Result: 实验表明，DAEDCMD显著优于六种传统MMD方法和三种持续学习方法。

Conclusion: DAEDCMD是一种有效的新方法，通过记忆历史知识和预测未来环境变化，解决了持续多模态错误信息检测中的核心问题。

Abstract: Nowadays, misinformation articles, especially multimodal ones, are widely
spread on social media platforms and cause serious negative effects. To control
their propagation, Multimodal Misinformation Detection (MMD) becomes an active
topic in the community to automatically identify misinformation. Previous MMD
methods focus on supervising detectors by collecting offline data. However, in
real-world scenarios, new events always continually emerge, making MMD models
trained on offline data consistently outdated and ineffective. To address this
issue, training MMD models under online data streams is an alternative,
inducing an emerging task named continual MMD. Unfortunately, it is hindered by
two major challenges. First, training on new data consistently decreases the
detection performance on past data, named past knowledge forgetting. Second,
the social environment constantly evolves over time, affecting the
generalization on future data. To alleviate these challenges, we propose to
remember past knowledge by isolating interference between event-specific
parameters with a Dirichlet process-based mixture-of-expert structure, and
anticipate future environmental distributions by learning a continuous-time
dynamics model. Accordingly, we induce a new continual MMD method DAEDCMD.
Extensive experiments demonstrate that DAEDCMD can consistently and
significantly outperform the compared methods, including six MMD baselines and
three continual learning methods.

</details>


### [119] [Chat-Ghosting: A Comparative Study of Methods for Auto-Completion in Dialog Systems](https://arxiv.org/abs/2507.05940)
*Sandeep Mishra,Anubhab Mandal,Bishal Santra,Tushar Abhishek,Pawan Goyal,Manish Gupta*

Main category: cs.CL

TL;DR: 本文探讨聊天补全问题，比较了多种方法，并提出了基于熵的动态提前停止策略。


<details>
  <summary>Details</summary>
Motivation: 随着聊天系统的普及（如ChatGPT等），文本补全问题在改善用户体验方面变得越来越重要，但目前缺乏系统性研究和标准化评估基准。

Method: 使用两个人工对话数据集（DailyDialog & DSTC7-Ubuntu）和两个人机对话数据集（Open Assistant & ShareGPT），对通过trie、n-gram、深度学习方法（如T5和Phi-2）进行实验，同时考虑了上下文信息，并提出了基于熵的动态提前停止策略。

Result: 在已知前缀上，统计n-gram模型和树结构优于深度学习模型；在未知查询上，深度学习模型（如T5和Phi-2）表现更佳。加入对话背景信息大幅提升了补全质量，尤其是在Open Assistant和ShareGPT数据集上表现显著。

Conclusion: 统计模型和深度学习方法各有优劣，整合上下文信息显著提升性能。提供的代码和数据集为后续研究奠定了基础。

Abstract: Ghosting, the ability to predict a user's intended text input for inline
query auto-completion, is an invaluable feature for modern search engines and
chat interfaces, greatly enhancing user experience. By suggesting completions
to incomplete queries (or prefixes), ghosting aids users with slow typing
speeds, disabilities, or limited language proficiency. Ghosting is a
challenging problem and has become more important with the ubiquitousness of
chat-based systems like ChatGPT, Copilot, etc. Despite the increasing
prominence of chat-based systems utilizing ghosting, this challenging problem
of Chat-Ghosting has received little attention from the NLP/ML research
community. There is a lack of standardized benchmarks and relative performance
analysis of deep learning and non-deep learning methods. We address this
through an open and thorough study of this problem using four publicly
available dialog datasets: two human-human (DailyDialog and DSTC7-Ubuntu) and
two human-bot (Open Assistant and ShareGPT). We experiment with various
existing query auto-completion methods (using tries), n-gram methods and deep
learning methods, with and without dialog context. We also propose a novel
entropy-based dynamic early stopping strategy. Our analysis finds that
statistical n-gram models and tries outperform deep learning based models in
terms of both model performance and inference efficiency for seen prefixes. For
unseen queries, neural models like T5 and Phi-2 lead to better results. Adding
conversational context leads to significant improvements in ghosting quality,
especially for Open-Assistant and ShareGPT. We make code and data publicly
available

</details>


### [120] [OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text Generation](https://arxiv.org/abs/2507.05965)
*Lucas Fonseca Lage,Simon Ostermann*

Main category: cs.CL

TL;DR: OpenFActScore是一种开源的FActScore框架实现，用于评估大型语言模型生成文本的真实性，通过开放的模型实现，更具透明性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 希望提供一种开源工具来评估大型语言模型生成文本的事实性，以弥补原始FActScore依赖闭源商业模型的不足。

Method: 使用原子事实生成（AFG）提取单个事实性声明，并用原子事实验证（AFV）对其进行验证，同时支持任何Hugging Face兼容模型。

Result: 通过对多个开源模型进行评估，结果表明这些模型能够接近闭源系统的性能，其中Gemma表现最佳并达到与原始FActScore实验0.99的Pearson相关度。

Conclusion: OpenFActScore支持开放模型的使用，促进了透明性、可重复性和具成本效益的评估。

Abstract: We introduce OpenFActScore, an open-source implementation of the FActScore
framework for evaluating the factuality of text generated by large language
models (LLMs). FActScore evaluates the factual accuracy of long-form text by
using Atomic Fact Generation (AFG) to extract individual factual claims and
Atomic Fact Validation (AFV) to verify each claim against a trusted knowledge
source. While the original FActScore relies on closed-source and commercial
models such as InstructGPT and ChatGPT, OpenFActScore enables the use of any
Hugging Face-compatible model for both AFG and AFV. We provide a detailed
technical overview of our implementation, highlighting design choices and
modifications made to support open models. We evaluate multiple open-source
LLMs on both AFG and AFV using the original FActScore benchmark, reporting
BERTScore-F1 for AFG and Error Rate relative to human annotations for AFV. Our
results show that open models can approximate the performance of closed-source
systems, with Gemma achieving the best overall performance, and our final setup
obtains a 0.99 Pearson correlation with the original FActScore experiments.
OpenFActScore promotes transparency, reproducibility, and cost-effective
evaluation, and is available at: https://github.com/lflage/OpenFActScore.

</details>


### [121] [We Should Evaluate Real-World Impact](https://arxiv.org/abs/2507.05973)
*Ehud Reiter*

Main category: cs.CL

TL;DR: 该论文指出ACL学术界对评估NLP系统的实际影响缺乏兴趣，并强调评估其现实影响的重要性。


<details>
  <summary>Details</summary>
Motivation: 揭示ACL领域几乎没有关注NLP系统实际影响的现状，希望提升对现实影响的认知和评估。

Method: 通过对ACL文集进行结构化调查，分析论文对实际影响的关注度。

Result: 发现仅约0.1%的ACL论文涉及现实影响评估，且这些评估大都较为简略，集中在指标评估上。

Conclusion: NLP技术若能更加注重现实影响评估，将会更加有用并更快被采纳。

Abstract: The ACL community has very little interest in evaluating the real-world
impact of NLP systems. A structured survey of the ACL Anthology shows that
perhaps 0.1% of its papers contain such evaluations; furthermore most papers
which include impact evaluations present them very sketchily and instead focus
on metric evaluations. NLP technology would be more useful and more quickly
adopted if we seriously tried to understand and evaluate its real-world impact.

</details>


### [122] [RabakBench: Scaling Human Annotations to Construct Localized Multilingual Safety Benchmarks for Low-Resource Languages](https://arxiv.org/abs/2507.05980)
*Gabriel Chua,Leanne Tan,Ziyu Ge,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: 本文提出RabakBench，一个用于检查新加坡多语种内容安全性的基准测试，涵盖Singlish、中文、马来语和泰米尔语，解决低资源语言的训练和评估难题。


<details>
  <summary>Details</summary>
Motivation: 解决由于训练数据和评估基准受限，导致的大型语言模型(LLMs)在低资源语言上表现不佳的问题。

Method: 设计了三阶段流程：(i) 基于LLM生成对抗性示例；(ii) 结合多人投票的LLM标注，进行半自动化安全标签分类；(iii) 进行高保真、多语言翻译，维护语言特性与安全性。最终数据集包含跨四种语言、六种安全类别及严重性分级的5000多个安全标注示例。

Result: 评估了11种流行的开放和封闭源的模型分类器，发现其在这一新基准上的表现显著下降，这表明传统方法在低资源多语言语境中的局限性。

Conclusion: RabakBench为东南亚多语言设置的安全性评估提供了强大工具，并为构建低资源环境的本地化安全数据集提供了可复制的框架，其数据集和评估代码公开可用。

Abstract: Large language models (LLMs) and their safety classifiers often perform
poorly on low-resource languages due to limited training data and evaluation
benchmarks. This paper introduces RabakBench, a new multilingual safety
benchmark localized to Singapore's unique linguistic context, covering
Singlish, Chinese, Malay, and Tamil. RabakBench is constructed through a
scalable three-stage pipeline: (i) Generate - adversarial example generation by
augmenting real Singlish web content with LLM-driven red teaming; (ii) Label -
semi-automated multi-label safety annotation using majority-voted LLM labelers
aligned with human judgments; and (iii) Translate - high-fidelity translation
preserving linguistic nuance and toxicity across languages. The final dataset
comprises over 5,000 safety-labeled examples across four languages and six
fine-grained safety categories with severity levels. Evaluations of 11 popular
open-source and closed-source guardrail classifiers reveal significant
performance degradation. RabakBench not only enables robust safety evaluation
in Southeast Asian multilingual settings but also offers a reproducible
framework for building localized safety datasets in low-resource environments.
The benchmark dataset, including the human-verified translations, and
evaluation code are publicly available.

</details>


### [123] [Evolution without Large Models: Training Language Model with Task Principles](https://arxiv.org/abs/2507.05991)
*Minghang Zhu,Shen Gao,Zhengliang Shi,Jiabao Fang,Pengjie Ren,Zhaochun Ren,Zhumin Chen,Shuo Shang*

Main category: cs.CL

TL;DR: 该研究提出了一种自进化语言模型方法，通过多级原则生成和基于原则的实例生成，改善模型性能并减少碳排放。


<details>
  <summary>Details</summary>
Motivation: 现有利用大规模语言模型扩展数据集的方法存在高碳排放和数据泄露风险的挑战。

Method: 引入多级原则生成机制总结任务完成原则，并通过小规模语言模型生成数据后进行模型训练。

Result: 实验表明，该方法显著提升了模型性能，并有效减少了碳排放。

Conclusion: 提出的方法能够在保证模型性能的同时，降低训练过程中的碳排放和数据泄露风险，具有实际应用意义。

Abstract: A common training approach for language models involves using a large-scale
language model to expand a human-provided dataset, which is subsequently used
for model training.This method significantly reduces training costs by
eliminating the need for extensive human data annotation. However, it still
faces challenges such as high carbon emissions during data augmentation and the
risk of data leakage when we use closed-source LLMs. To address these issues,
we propose a self-evolution method for language models. First, we introduce the
Multi-level Principle Generation, which enables a large-scale model to
summarize task-completion principles based on a small amount of task data.
Then, we propose the Principle-based Instance Generation, in which a
smaller-scale language model uses these task principles to generate a large
amount of data. This data is then used for model training. Experimental results
show that our proposed method significantly improves model performance compared
to directly using a smaller-scale language model to generate data.
Additionally, since we only use the large-scale language model to generate the
task-completion principles, the carbon emissions associated with training the
model are greatly reduced.

</details>


### [124] [DocIE@XLLM25: In-Context Learning for Information Extraction using Fully Synthetic Demonstrations](https://arxiv.org/abs/2507.05997)
*Nicholas Popovič,Ashish Kangen,Tim Schopf,Michael Färber*

Main category: cs.CL

TL;DR: 本文提出了一种基于LLM的全自动数据合成和上下文学习管道，用于文档级实体和关系抽取的零样本或少样本设置。


<details>
  <summary>Details</summary>
Motivation: 解决文档级实体和关系抽取中高质量标注语料库稀缺的问题，特别是在零样本或少样本场景中。

Method: 结合合成数据生成与基于检索的上下文学习，利用优化推理的语言模型，动态检索相关示例并构建高质量的演示数据库。

Result: 生成了超过5000条维基百科摘要的合成数据集，包含约59000个实体和30000个关系三元组，并在DocIE任务上进行了零样本学习评估。

Conclusion: 文档级的联合实体和关系抽取在零样本设置下仍然是一个具有挑战性的任务，即使是最先进的语言模型也存在明显困难。

Abstract: Large, high-quality annotated corpora remain scarce in document-level entity
and relation extraction in zero-shot or few-shot settings. In this paper, we
present a fully automatic, LLM-based pipeline for synthetic data generation and
in-context learning for document-level entity and relation extraction. In
contrast to existing approaches that rely on manually annotated demonstrations
or direct zero-shot inference, our method combines synthetic data generation
with retrieval-based in-context learning, using a reasoning-optimized language
model. This allows us to build a high-quality demonstration database without
manual annotation and to dynamically retrieve relevant examples at inference
time. Based on our approach we produce a synthetic dataset of over $5k$
Wikipedia abstracts with approximately $59k$ entities and $30k$ relation
triples. Finally, we evaluate in-context learning performance on the DocIE
shared task, extracting entities and relations from long documents in a
zero-shot setting. We find that in-context joint entity and relation extraction
at document-level remains a challenging task, even for state-of-the-art large
language models.

</details>


### [125] [Conditional Multi-Stage Failure Recovery for Embodied Agents](https://arxiv.org/abs/2507.06016)
*Youmna Farag,Svetlana Stoyanchev,Mohan Li,Simon Keizer,Rama Doddipatla*

Main category: cs.CL

TL;DR: 本文提出了一种面向复杂任务的具身智能体的失败恢复多阶段框架，利用零样本链式提示并在TEACH数据集的TfD基准上实现了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 面对复杂任务的智能体容易出现执行失败，因此需要有效的失败恢复机制。

Method: 提出一个条件多阶段失败恢复框架，包括任务执行中的三个阶段和任务后反思阶段，并利用LLMs的推理能力分析执行问题并制定策略。

Result: 在TEACH数据集的TfD基准上，该方法的性能比没有错误恢复的基线高11.5%，超过目前最强模型19%。

Conclusion: 该研究证明了多阶段失败恢复框架的有效性和LLMs在复杂任务失败应对中的潜力。

Abstract: Embodied agents performing complex tasks are susceptible to execution
failures, motivating the need for effective failure recovery mechanisms. In
this work, we introduce a conditional multistage failure recovery framework
that employs zero-shot chain prompting. The framework is structured into four
error-handling stages, with three operating during task execution and one
functioning as a post-execution reflection phase. Our approach utilises the
reasoning capabilities of LLMs to analyse execution challenges within their
environmental context and devise strategic solutions. We evaluate our method on
the TfD benchmark of the TEACH dataset and achieve state-of-the-art
performance, outperforming a baseline without error recovery by 11.5% and
surpassing the strongest existing model by 19%.

</details>


### [126] [Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in LLMs](https://arxiv.org/abs/2507.06056)
*Yizhan Huang,Zhe Yang,Meifang Chen,Jianping Zhang,Michael R. Lyu*

Main category: cs.CL

TL;DR: 这篇论文探讨了大型语言模型(LLMs)中的记忆难度，提出了熵与记忆分数间的线性关系，并发现随机字符串的经验熵较低，可用于训练与测试数据的区分。


<details>
  <summary>Details</summary>
Motivation: 研究训练数据在LLMs中记忆难度的特征化问题。

Method: 通过对开放模型OLMo的实验，提出了熵-记忆定律，并在随机字符串上进行案例研究，探索其熵特性。

Result: 发现数据熵与记忆分数线性相关，并表明随机字符串的经验熵低于一般训练数据。

Conclusion: 该方法可用于通过熵特性区分训练与测试数据，并有潜力改进LLMs的数据处理。

Abstract: Large Language Models (LLMs) are known to memorize portions of their training
data, sometimes reproducing content verbatim when prompted appropriately. In
this work, we investigate a fundamental yet under-explored question in the
domain of memorization: How to characterize memorization difficulty of training
data in LLMs? Through empirical experiments on OLMo, a family of open models,
we present the Entropy-Memorization Law. It suggests that data entropy is
linearly correlated with memorization score. Moreover, in a case study of
memorizing highly randomized strings, or "gibberish", we observe that such
sequences, despite their apparent randomness, exhibit unexpectedly low
empirical entropy compared to the broader training corpus. Adopting the same
strategy to discover Entropy-Memorization Law, we derive a simple yet effective
approach to distinguish training and testing data, enabling Dataset Inference
(DI).

</details>


### [127] [A Survey on Prompt Tuning](https://arxiv.org/abs/2507.06085)
*Zongqian Li,Yixuan Su,Nigel Collier*

Main category: cs.CL

TL;DR: 本文综述了prompt tuning，这是一种通过前置可训练连续向量、保持语言模型冻结的参数高效适配方法，包括直接prompt学习和迁移学习两大类方法。


<details>
  <summary>Details</summary>
Motivation: 探讨prompt tuning在语言模型适配中的现有方法、挑战及未来方向，以提升性能与适用性。

Method: 按直接学习与迁移学习两类系统分类分析，并进一步细分探讨设计、创新、优缺点等，并提供对比性可视化。

Result: 识别了计算效率与训练稳定性方面的挑战，并总结当前方法的设计与差异。

Conclusion: 未来需提升训练鲁棒性及扩展应用范围，优化当前挑战。

Abstract: This survey reviews prompt tuning, a parameter-efficient approach for
adapting language models by prepending trainable continuous vectors while
keeping the model frozen. We classify existing approaches into two categories:
direct prompt learning and transfer learning. Direct prompt learning methods
include: general optimization approaches, encoder-based methods, decomposition
strategies, and mixture-of-experts frameworks. Transfer learning methods
consist of: general transfer approaches, encoder-based methods, and
decomposition strategies. For each method, we analyze method designs,
innovations, insights, advantages, and disadvantages, with illustrative
visualizations comparing different frameworks. We identify challenges in
computational efficiency and training stability, and discuss future directions
in improving training robustness and broadening application scope.

</details>


### [128] [NeoBabel: A Multilingual Open Tower for Visual Generation](https://arxiv.org/abs/2507.06137)
*Mohammad Mahdi Derakhshani,Dheeraj Varghese,Marzieh Fadaee,Cees G. M. Snoek*

Main category: cs.CL

TL;DR: NeoBabel是一种多语种图像生成框架，支持六种语言，通过多语言预训练和高分辨率调优实现尖端性能，对比其他模型，具有更高的多语言生成能力且体积更小。


<details>
  <summary>Details</summary>
Motivation: 当前文本生成图像技术集中在英语，阻碍非英语用户并加剧数字不平等。现有系统依赖翻译管道，但会引发语义漂移和文化对接问题。

Method: 研发NeoBabel框架，通过大规模多语言预训练和高分辨率指令调优，并扩展两种基准测评数据集以评估系统表现。

Result: NeoBabel多语言基准测评表现优异，分别比主流模型高0.11和0.09分，且在保持高效的同时仅为其它模型大小的2-4倍。

Conclusion: NeoBabel展示了多语言能力可促进生成式AI的鲁棒性、效率和文化适配性，无需在性能间进行权衡。

Abstract: Text-to-image generation advancements have been predominantly
English-centric, creating barriers for non-English speakers and perpetuating
digital inequities. While existing systems rely on translation pipelines, these
introduce semantic drift, computational overhead, and cultural misalignment. We
introduce NeoBabel, a novel multilingual image generation framework that sets a
new Pareto frontier in performance, efficiency and inclusivity, supporting six
languages: English, Chinese, Dutch, French, Hindi, and Persian. The model is
trained using a combination of large-scale multilingual pretraining and
high-resolution instruction tuning. To evaluate its capabilities, we expand two
English-only benchmarks to multilingual equivalents: m-GenEval and m-DPG.
NeoBabel achieves state-of-the-art multilingual performance while retaining
strong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG.
Notably, it performs on par with leading models on English tasks while
outperforming them by +0.11 and +0.09 on multilingual benchmarks, even though
these models are built on multilingual base LLMs. This demonstrates the
effectiveness of our targeted alignment training for preserving and extending
crosslingual generalization. We further introduce two new metrics to rigorously
assess multilingual alignment and robustness to code-mixed prompts. Notably,
NeoBabel matches or exceeds English-only models while being 2-4x smaller. We
release an open toolkit, including all code, model checkpoints, a curated
dataset of 124M multilingual text-image pairs, and standardized multilingual
evaluation protocols, to advance inclusive AI research. Our work demonstrates
that multilingual capability is not a trade-off but a catalyst for improved
robustness, efficiency, and cultural fidelity in generative AI.

</details>


### [129] [Coding Triangle: How Does Large Language Model Understand Code?](https://arxiv.org/abs/2507.06138)
*Taolin Zhang,Zihan Ma,Maosong Cao,Junnan Liu,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出了Code Triangle框架，用于系统评估大型语言模型(LLMs)在编程中的能力。从编辑分析、代码实现和测试用例生成三个方面，揭示了LLMs与人类程序员的差距及改进方向。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码生成中表现出色，但其真正的编程能力尚未被深入研究。作者希望通过评估模型在关键编程维度的表现，探索其不足之处与改进方案。

Method: 提出Code Triangle框架，分别从编辑分析、代码实现和测试用例生成三个维度对LLMs进行系统评估，并通过竞赛编程基准进行实验分析。

Result: 研究表明，LLMs在这些维度上能够形成自洽的体系，但其解决方案缺乏多样性和稳健性，同时存在模型认知与人类经验之间的显著分布差异。引入人类生成的编辑内容、解决方案和多样化测试用例，以及使用模型混合可以显著提升LLMs的性能及稳健性。

Conclusion: 通过识别LLMs的一致性和不一致性认知，本文为进一步开发更强大的编码模型提供了潜在方向，并展示了自我反思和改进的可能性。

Abstract: Large language models (LLMs) have achieved remarkable progress in code
generation, yet their true programming competence remains underexplored. We
introduce the Code Triangle framework, which systematically evaluates LLMs
across three fundamental dimensions: editorial analysis, code implementation,
and test case generation. Through extensive experiments on competitive
programming benchmarks, we reveal that while LLMs can form a self-consistent
system across these dimensions, their solutions often lack the diversity and
robustness of human programmers. We identify a significant distribution shift
between model cognition and human expertise, with model errors tending to
cluster due to training data biases and limited reasoning transfer. Our study
demonstrates that incorporating human-generated editorials, solutions, and
diverse test cases, as well as leveraging model mixtures, can substantially
enhance both the performance and robustness of LLMs. Furthermore, we reveal
both the consistency and inconsistency in the cognition of LLMs that may
facilitate self-reflection and self-improvement, providing a potential
direction for developing more powerful coding models.

</details>


### [130] [Skywork-R1V3 Technical Report](https://arxiv.org/abs/2507.06167)
*Wei Shen,Jiangbo Pei,Yi Peng,Xuchen Song,Yang Liu,Jian Peng,Haofeng Sun,Yunzhuo Hao,Peiyu Wang,Yahui Zhou*

Main category: cs.CL

TL;DR: Skywork-R1V3 是一款开源的视觉-语言模型，通过转移文本模型的推理能力到视觉任务，实现了视觉推理的重大飞跃，获得了人类水平的表现。


<details>
  <summary>Details</summary>
Motivation: 目前的视觉-语言模型在多模态推理能力上尚有不足，需要一种新的方法提高模型的高效推理能力，并挖掘跨模态对齐机制。

Method: 提出了一种基于强化学习框架的后期训练方法，配合特定的连接模块用于多模态对齐，并引入“关键推理标记熵”指标用于训练中的检查点选择。

Result: Skywork-R1V3 模型在MMMU上成绩从 64.3% 提升至 76.0%，达到入门级人类能力，并在 38B 参数模型上实现了与封闭源顶级模型的竞争能力。

Conclusion: Skywork-R1V3 通过强化学习策略和创新的训练方法，显著推动了开源视觉-语言模型的多模态推理能力，证明了强化学习在这一领域的前景。

Abstract: We introduce Skywork-R1V3, an advanced, open-source vision-language model
(VLM) that pioneers a new approach to visual reasoning. Its key innovation lies
in effectively transferring reasoning skills from text-only Large Language
Models (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily
stems from our elaborate post-training RL framework, which effectively
activates and enhances the model's reasoning ability, without the need for
additional continue pre-training. Through this framework, we further uncover
the fundamental role of the connector module in achieving robust cross-modal
alignment for multimodal reasoning models. In addition, we introduce a unique
indicator of reasoning capability, the entropy of critical reasoning tokens,
which has proven highly effective for checkpoint selection during RL training.
Skywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving
from 64.3% to 76.0%. This performance matches entry-level human capabilities.
Remarkably, our RL-powered post-training approach enables even the 38B
parameter model to rival top closed-source VLMs. The implementation
successfully transfers mathematical reasoning to other subject-related
reasoning tasks. We also include an analysis of curriculum learning and
reinforcement finetuning strategies, along with a broader discussion on
multimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal
reasoning, showcasing RL as a powerful engine for advancing open-source VLM
capabilities.

</details>


### [131] [CriticLean: Critic-Guided Reinforcement Learning for Mathematical Formalization](https://arxiv.org/abs/2507.06181)
*Zhongyuan Peng,Yifan Yao,Kaijing Ma,Shuyue Guo,Yizhe Li,Yichi Zhang,Chenchen Zhang,Yifan Zhang,Zhouliang Yu,Luming Li,Minghao Liu,Yihang Xia,Jiawei Shen,Yuchen Wu,Yixin Cao,Zhaoxiang Zhang,Wenhao Huang,Jiaheng Liu,Ge Zhang*

Main category: cs.CL

TL;DR: CriticLean是一种新颖的评论驱动强化学习框架，用于提升自然语言数学陈述到形式化代码的翻译质量。


<details>
  <summary>Details</summary>
Motivation: 针对自动定理证明领域中缺乏对语言形式化的语义正确性评估的问题，提出优化评论阶段这一关键步骤。

Method: 提出CriticLeanGPT模型，经过监督微调和强化学习训练来评估形式化代码的语义正确性；并设计了一个基准CriticLeanBench来测试评估模型的性能。

Result: CriticLeanGPT模型在语义正确性测评中显著优于开源与闭源的对标模型；发布了包含超过28.5万问题的多样化训练数据集FineLeanCorpus。

Conclusion: 优化评论阶段对于生成可靠的形式化代码至关重要，CriticLean框架为形式化数学推理提供了宝贵参考。

Abstract: Translating natural language mathematical statements into formal, executable
code is a fundamental challenge in automated theorem proving. While prior work
has focused on generation and compilation success, little attention has been
paid to the critic phase-the evaluation of whether generated formalizations
truly capture the semantic intent of the original problem. In this paper, we
introduce CriticLean, a novel critic-guided reinforcement learning framework
that elevates the role of the critic from a passive validator to an active
learning component. Specifically, first, we propose the CriticLeanGPT, trained
via supervised fine-tuning and reinforcement learning, to rigorously assess the
semantic fidelity of Lean 4 formalizations. Then, we introduce CriticLeanBench,
a benchmark designed to measure models' ability to distinguish semantically
correct from incorrect formalizations, and demonstrate that our trained
CriticLeanGPT models can significantly outperform strong open- and
closed-source baselines. Building on the CriticLean framework, we construct
FineLeanCorpus, a dataset comprising over 285K problems that exhibits rich
domain diversity, broad difficulty coverage, and high correctness based on
human evaluation. Overall, our findings highlight that optimizing the critic
phase is essential for producing reliable formalizations, and we hope our
CriticLean will provide valuable insights for future advances in formal
mathematical reasoning.

</details>


### [132] [DS@GT at CheckThat! 2025: Detecting Subjectivity via Transfer-Learning and Corrective Data Augmentation](https://arxiv.org/abs/2507.06189)
*Maximilian Heil,Dionne Bang*

Main category: cs.CL

TL;DR: 本文提出了一种结合特定编码器迁移学习与标签一致性数据增强的方法，用于英文新闻文本的主观性检测。实验表明，该方法在主观性内容检测中显著提升了模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探究迁移学习和风格化数据增强在改进主观性和客观性句子分类中的有效性。

Method: 利用微调预训练编码器和任务相关的迁移学习，并引入由GPT-4o控制的风格化数据增强管道，生成主观性风格固定的改写句并进行校正。

Result: 实验结果显示，特定编码器的迁移学习优于通用编码器的微调，标签一致的数据增强显著提升了模型鲁棒性。

Conclusion: 结合专用迁移学习与标签一致增强，可有效提升主观性检测性能。

Abstract: This paper presents our submission to Task 1, Subjectivity Detection, of the
CheckThat! Lab at CLEF 2025. We investigate the effectiveness of
transfer-learning and stylistic data augmentation to improve classification of
subjective and objective sentences in English news text. Our approach contrasts
fine-tuning of pre-trained encoders and transfer-learning of fine-tuned
transformer on related tasks. We also introduce a controlled augmentation
pipeline using GPT-4o to generate paraphrases in predefined subjectivity
styles. To ensure label and style consistency, we employ the same model to
correct and refine the generated samples. Results show that transfer-learning
of specified encoders outperforms fine-tuning general-purpose ones, and that
carefully curated augmentation significantly enhances model robustness,
especially in detecting subjective content. Our official submission placed us
$16^{th}$ of 24 participants. Overall, our findings underscore the value of
combining encoder specialization with label-consistent augmentation for
improved subjectivity detection. Our code is available at
https://github.com/dsgt-arc/checkthat-2025-subject.

</details>


### [133] [DS@GT at CheckThat! 2025: Evaluating Context and Tokenization Strategies for Numerical Fact Verification](https://arxiv.org/abs/2507.06195)
*Maximilian Heil,Aleksandar Pramov*

Main category: cs.CL

TL;DR: 本文研究了用于验证带数量声明的自动事实核查技术，提出了一种新的证据检索管道，结论表明证据质量是性能的主要限制因素。


<details>
  <summary>Details</summary>
Motivation: 研究涉及数量声明的事实核查，以解决模型在处理数值型语句上的挑战，提高NLI任务的表现能力。

Method: 采用QuanTemp数据集与新构建的证据检索管道，通过探索长输入上下文窗口与从右到左的分词方法对真实性预测性能的影响。

Result: 发现从右到左分词对NLI中的数值任务无显著帮助，拓展上下文窗口也未提升性能，最终实现了0.57的宏F1分数，位列比赛前四。

Conclusion: 证据质量，而非上下文长度或分词方向，是数值声明真实性预测的主要瓶颈。论文已开源代码以供参考。

Abstract: Numerical claims, statements involving quantities, comparisons, and temporal
references, pose unique challenges for automated fact-checking systems. In this
study, we evaluate modeling strategies for veracity prediction of such claims
using the QuanTemp dataset and building our own evidence retrieval pipeline. We
investigate three key factors: (1) the impact of more evidences with longer
input context windows using ModernBERT, (2) the effect of right-to-left (R2L)
tokenization, and (3) their combined influence on classification performance.
Contrary to prior findings in arithmetic reasoning tasks, R2L tokenization does
not boost natural language inference (NLI) of numerical tasks. A longer context
window does also not enhance veracity performance either, highlighting evidence
quality as the dominant bottleneck. Our best-performing system achieves
competitive macro-average F1 score of 0.57 and places us among the Top-4
submissions in Task 3 of CheckThat! 2025. Our code is available at
https://github.com/dsgt-arc/checkthat-2025-numerical.

</details>


### [134] [UQLM: A Python Package for Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2507.06196)
*Dylan Bouchard,Mohit Singh Chauhan,David Skarbrevik,Ho-Kyeong Ra,Viren Bajaj,Zeya Ahmad*

Main category: cs.CL

TL;DR: 该论文提出了一个名为UQLM的Python工具包，用于基于不确定性量化技术检测大语言模型（LLMs）的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: LLMs生成的虚假或误导性内容（即幻觉）对下游应用的安全性和信任产生负面影响，亟需有效的解决方案。

Method: 通过一套先进的不确定性量化技术，UQLM工具包提供了多个基于UQ的评分器，可以计算从0到1的响应级别置信分数，用于评估LLMs输出中的幻觉问题。

Result: UQLM提供了一种即插即用的解决方案，可以集成到现有系统中以改进LLMs输出的可靠性。

Conclusion: 通过引入基于UQ的检测手段，该工具包为提高LLMs的输出可信度提供了实际可行的工具。

Abstract: Hallucinations, defined as instances where Large Language Models (LLMs)
generate false or misleading content, pose a significant challenge that impacts
the safety and trust of downstream applications. We introduce UQLM, a Python
package for LLM hallucination detection using state-of-the-art uncertainty
quantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers
that compute response-level confidence scores ranging from 0 to 1. This library
provides an off-the-shelf solution for UQ-based hallucination detection that
can be easily integrated to enhance the reliability of LLM outputs.

</details>


### [135] [A Survey on Latent Reasoning](https://arxiv.org/abs/2507.06203)
*Rui-Jie Zhu,Tianhao Peng,Tianhao Cheng,Xingwei Qu,Jinfa Huang,Dawei Zhu,Hao Wang,Kaiwen Xue,Xuanliang Zhang,Yong Shan,Tianle Cai,Taylor Kergan,Assel Kembay,Andrew Smith,Chenghua Lin,Binh Nguyen,Yuqi Pan,Yuhong Chou,Zefan Cai,Zhenhe Wu,Yongchi Zhao,Tianyu Liu,Jian Yang,Wangchunshu Zhou,Chujie Zheng,Chongxuan Li,Yuyin Zhou,Zhoujun Li,Zhaoxiang Zhang,Jiaheng Liu,Ge Zhang,Wenhao Huang,Jason Eshraghian*

Main category: cs.CL

TL;DR: 该论文综述了潜在推理领域的最新进展，分析多步推理中的神经网络层作用、各种推理方法及其高级范式。


<details>
  <summary>Details</summary>
Motivation: 现有显式推理方式受限于自然语言表达，限制了语言模型在推理过程中的表现能力。

Method: 探讨神经网络作为推理计算基底的作用，研究激活-递归、隐藏状态传播等潜在推理方法，及以压缩显式推理轨迹为目标的微调策略。

Result: 梳理潜在推理的方法与技术，特别展示了具备一致性和可逆性的无限深度推理。

Conclusion: 通过统一视角总结潜在推理领域，为语言大模型推理研究提供了清晰的理论框架与未来研究方向参考。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning
capabilities, especially when guided by explicit chain-of-thought (CoT)
reasoning that verbalizes intermediate steps. While CoT improves both
interpretability and accuracy, its dependence on natural language reasoning
limits the model's expressive bandwidth. Latent reasoning tackles this
bottleneck by performing multi-step inference entirely in the model's
continuous hidden state, eliminating token-level supervision. To advance latent
reasoning research, this survey provides a comprehensive overview of the
emerging field of latent reasoning. We begin by examining the foundational role
of neural network layers as the computational substrate for reasoning,
highlighting how hierarchical representations support complex transformations.
Next, we explore diverse latent reasoning methodologies, including
activation-based recurrence, hidden state propagation, and fine-tuning
strategies that compress or internalize explicit reasoning traces. Finally, we
discuss advanced paradigms such as infinite-depth latent reasoning via masked
diffusion models, which enable globally consistent and reversible reasoning
processes. By unifying these perspectives, we aim to clarify the conceptual
landscape of latent reasoning and chart future directions for research at the
frontier of LLM cognition. An associated GitHub repository collecting the
latest papers and repos is available at:
https://github.com/multimodal-art-projection/LatentCoT-Horizon/.

</details>


### [136] [DS@GT at CheckThat! 2025: Ensemble Methods for Detection of Scientific Discourse on Social Media](https://arxiv.org/abs/2507.06205)
*Ayush Parikh,Hoang Thanh Thanh Truong,Jeanette Schofield,Maximilian Heil*

Main category: cs.CL

TL;DR: 本文描述了CLEF 2025 CheckThat! Task 4a任务中，DS@GT团队的方法和成果。


<details>
  <summary>Details</summary>
Motivation: 旨在解决推特中科学主张、科学研究参考或科学实体提及的多类别分类问题。

Method: 提出了三种建模方法：Transformer微调、LLM少样本提示、以及结合早期实验结果的集成模型。

Result: 团队在竞赛中排名第七，取得了0.8611的宏平均F1分数，高于DeBERTaV3基线的0.8375。

Conclusion: 团队方法在性能上表现出改进，并公开了相关代码以供研究社区使用。

Abstract: In this paper, we, as the DS@GT team for CLEF 2025 CheckThat! Task 4a
Scientific Web Discourse Detection, present the methods we explored for this
task. For this multiclass classification task, we determined if a tweet
contained a scientific claim, a reference to a scientific study or publication,
and/or mentions of scientific entities, such as a university or a scientist. We
present 3 modeling approaches for this task: transformer finetuning, few-shot
prompting of LLMs, and a combined ensemble model whose design was informed by
earlier experiments. Our team placed 7th in the competition, achieving a
macro-averaged F1 score of 0.8611, an improvement over the DeBERTaV3 baseline
of 0.8375. Our code is available on Github at
https://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4a.

</details>


### [137] [Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers](https://arxiv.org/abs/2507.06223)
*Zhiyuan Peng,Ting-ruen Wei,Tingyu Song,Yilun Zhao,Yi Fang*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型(LLMs)在信息检索中作为排序任务的表现，同时提出了一种新的效率评价指标体系。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM排序器效率评价中的硬件依赖性和模型规模比较难度，提升效率与效果的可解释性。

Method: 提出了E²R-FLOPs评价指标，包括每PetaFLOP对应的相关性得分和查询吞吐量，并建立了FLOPs估算器用于在不实际运行实验的情况下估算LLM排序器的FLOPs。

Result: 通过提出的新指标进行大规模实验，评估了采用不同结构的LLM排序器的效率与效果折中问题。

Conclusion: 提出的指标体系能够促进研究者对效率与效果的深入理解，为研究社区带来新思考方向。

Abstract: Large Language Models (LLMs) have recently been applied to reranking tasks in
information retrieval, achieving strong performance. However, their high
computational demands often hinder practical deployment. Existing studies
evaluate the efficiency of LLM-based rerankers using proxy metrics such as
latency, the number of forward passes, input tokens, and output tokens.
However, these metrics depend on hardware and running-time choices (\eg
parallel or not, batch size, etc), and often fail to account for model size,
making it difficult to interpret and obscuring the evaluation of the
efficiency-effectiveness tradeoff. To address this issue, we propose
E\textsuperscript{2}R-FLOPs, for LLM-based rerankers: ranking metrics per
PetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for
hardware-agnostic throughput. Companied with the new metrics, an interpretable
FLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even
without running any experiments. Based on the proposed metrics, we conduct
comprehensive experiments to evaluate a wide range of LLM-based rerankers with
different architecture, studying the efficiency-effectiveness trade-off and
bringing this issue to the attention of the research community.

</details>


### [138] [Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving](https://arxiv.org/abs/2507.06229)
*Xiangru Tang,Tianrui Qin,Tianhao Peng,Ziyang Zhou,Daniel Shao,Tingting Du,Xinming Wei,Peng Xia,Fang Wu,He Zhu,Ge Zhang,Jiaheng Liu,Xingyao Wang,Sirui Hong,Chenglin Wu,Hao Cheng,Chi Wang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 本研究提出了Agent KB，一个通过Reason-Retrieve-Refine流程实现跨领域知识共享的新框架，可显著提升语言代理在复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决语言代理在复杂任务中难以有效纠错及经验复用的核心问题，同时克服传统代理无法共享经验的限制。

Method: 引入了Agent KB框架，采用Reason-Retrieve-Refine流程，构建层次化知识库，涵盖高层策略与详细执行日志，从而实现跨代理知识迁移。

Result: Agent KB在GAIA基准测试中提升成功率最多可达16.28个百分点，在最具挑战性任务中Claude-3从38.46%提升至57.69%，GPT-4在中等任务中从53.49%提升至73.26%，在SWE-bench代码修复任务中使Claude-3从41.33%提升至53.33%。

Conclusion: Agent KB作为一种模块化、框架无关的基础架构，能够帮助语言代理从过去经验中学习，并将成功策略泛化到新任务中。

Abstract: As language agents tackle increasingly complex tasks, they struggle with
effective error correction and experience reuse across domains. We introduce
Agent KB, a hierarchical experience framework that enables complex agentic
problem solving via a novel Reason-Retrieve-Refine pipeline. Agent KB addresses
a core limitation: agents traditionally cannot learn from each other's
experiences. By capturing both high-level strategies and detailed execution
logs, Agent KB creates a shared knowledge base that enables cross-agent
knowledge transfer. Evaluated on the GAIA benchmark, Agent KB improves success
rates by up to 16.28 percentage points. On the most challenging tasks, Claude-3
improves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on
intermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to
improve from 41.33% to 53.33%. Our results suggest that Agent KB provides a
modular, framework-agnostic infrastructure for enabling agents to learn from
past experiences and generalize successful strategies to new tasks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [139] [Strongly Solving $7 \times 6$ Connect-Four on Consumer Grade Hardware](https://arxiv.org/abs/2507.05267)
*Markus Böck*

Main category: cs.AI

TL;DR: 使用符号搜索方法和BDDs制作了一个大的查找表以针对Connect-Four游戏生成强解。


<details>
  <summary>Details</summary>
Motivation: 传统基于搜索的方法虽然有效但难以得到高效的查找表解决方案，作者希望借由改进的技术实现此点。

Method: 利用基于符号搜索的二元决策图(Binary Decision Diagram, BDD)生成查找表，同时结合alpha-beta搜索寻找最优路径。

Result: 生成了大小89.6 GB的查找表，在单核CPU和128 GB内存环境下历时47小时完成，适用于7×6标准棋盘。

Conclusion: 符号搜索方法结合BDD在解决Connect-Four类游戏中证明了其实用性，为生成高效查找表和寻找快速解提供了一种新路径。

Abstract: While the game Connect-Four has been solved mathematically and the best move
can be effectively computed with search based methods, a strong solution in the
form of a look-up table was believed to be infeasible. In this paper, we
revisit a symbolic search method based on binary decision diagrams to produce
strong solutions. With our efficient implementation we were able to produce a
89.6 GB large look-up table in 47 hours on a single CPU core with 128 GB main
memory for the standard $7 \times 6$ board size. In addition to this
win-draw-loss evaluation, we include an alpha-beta search in our open source
artifact to find the move which achieves the fastest win or slowest loss.

</details>


### [140] [Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management](https://arxiv.org/abs/2507.05283)
*Yue Wang,Miao Zhou,Guijing Huang,Rui Zhuo,Chao Yi,Zhenliang Ma*

Main category: cs.AI

TL;DR: 本文提出了Chat2SPaT，一种用来将用户模糊描述的交通信号控制计划转化为精确信号阶段与时间（SPaT）结果的方法。


<details>
  <summary>Details</summary>
Motivation: 当前预定时交通信号控制需要大量人工工作导致低效，研究希望通过简化信号计划管理过程来解决此问题。

Method: 利用大语言模型（LLM）解析用户描述，将计划转换为json格式的相位结果，并编写Python脚本完成交通信号控制的细节处理及完成计划组装。

Result: 实验表明，Chat2SPaT在测试集中对计划生成的准确率超过94%，适用于英中两种语言。

Conclusion: Chat2SPaT改进了交通信号控制计划管理过程，为ITS领域中更准确且多用的大模型应用奠定了基础。

Abstract: Pre-timed traffic signal control, commonly used for operating signalized
intersections and coordinated arterials, requires tedious manual work for
signaling plan creating and updating. When the time-of-day or day-of-week plans
are utilized, one intersection is often associated with multiple plans, leading
to further repetitive manual plan parameter inputting. To enable a
user-friendly traffic signal control plan management process, this study
proposes Chat2SPaT, a method to convert users' semi-structured and ambiguous
descriptions on the signal control plan to exact signal phase and timing (SPaT)
results, which could further be transformed into structured stage-based or
ring-based plans to interact with intelligent transportation system (ITS)
software and traffic signal controllers. With curated prompts, Chat2SPaT first
leverages large language models' (LLMs) capability of understanding users' plan
descriptions and reformulate the plan as a combination of phase sequence and
phase attribute results in the json format. Based on LLM outputs, python
scripts are designed to locate phases in a cycle, address nuances of traffic
signal control, and finally assemble the complete traffic signal control plan.
Within a chat, the pipeline can be utilized iteratively to conduct further plan
editing. Experiments show that Chat2SPaT can generate plans with an accuracy of
over 94% for both English and Chinese cases, using a test dataset with over 300
plan descriptions. As the first benchmark for evaluating LLMs' capability of
understanding traffic signal control plan descriptions, Chat2SPaT provides an
easy-to-use plan management pipeline for traffic practitioners and researchers,
serving as a potential new building block for a more accurate and versatile
application of LLMs in the field of ITS. The source codes, prompts and test
dataset are openly accessible at https://github.com/yuewangits/Chat2SPaT.

</details>


### [141] [Fuzzy Classification Aggregation for a Continuum of Agents](https://arxiv.org/abs/2507.05297)
*Zijun Meng*

Main category: cs.AI

TL;DR: 证明了一种模糊分类聚合函数在一定条件下为加权算术平均。


<details>
  <summary>Details</summary>
Motivation: 探究模糊分类聚合函数在特定数学条件下的性质。

Method: 利用数学理论证明模糊分类聚合函数的形式特征。

Result: 发现任何满足条件的模糊分类聚合函数都为加权算术平均。

Conclusion: 加权算术平均是满足条件的唯一模糊分类聚合函数形式。

Abstract: We prove that any optimal, independent, and zero unanimous fuzzy
classification aggregation function of a continuum of individual
classifications of $m\ge 3$ objects into $2\le p\le m$ types must be a weighted
arithmetic mean.

</details>


### [142] [OLG++: A Semantic Extension of Obligation Logic Graph](https://arxiv.org/abs/2507.05488)
*Subhasis Dasgupta,Jon Stephens,Amarnath Gupta*

Main category: cs.AI

TL;DR: OLG++ 是一种语义扩展模型，用于更丰富地表示法律规则及其上下文。


<details>
  <summary>Details</summary>
Motivation: 基于现有的图模型改进法律知识表示能力，以支持更复杂的法律问题回答和推理需求。

Method: 提出了 OLG++ 扩展模型，通过引入新的节点和边类型（如空间、时间、群体等），并结合结构化推理能力来增强表达力。

Result: OLG++ 在表达力上优于 LegalRuleML 和其他以图为基础的法律知识表示方法，尤其在处理子类别、空间约束和例外结构上。

Conclusion: 通过示例验证了 OLG++ 能更好地支持法律问答，提供更强的表达能力及对复杂规则的建模。

Abstract: We present OLG++, a semantic extension of the Obligation Logic Graph (OLG)
for modeling regulatory and legal rules in municipal and interjurisdictional
contexts. OLG++ introduces richer node and edge types, including spatial,
temporal, party group, defeasibility, and logical grouping constructs, enabling
nuanced representations of legal obligations, exceptions, and hierarchies. The
model supports structured reasoning over rules with contextual conditions,
precedence, and complex triggers. We demonstrate its expressiveness through
examples from food business regulations, showing how OLG++ supports legal
question answering using property graph queries. OLG++ also improves over
LegalRuleML by providing native support for subClassOf, spatial constraints,
and reified exception structures. Our examples show that OLG++ is more
expressive than prior graph-based models for legal knowledge representation.

</details>


### [143] [Deep Research Comparator: A Platform For Fine-grained Human Annotations of Deep Research Agents](https://arxiv.org/abs/2507.05495)
*Prahaladh Chandrahasan,Jiahe Jin,Zhihan Zhang,Tevin Wang,Andy Tang,Lucy Mo,Morteza Ziyadi,Leonardo F. R. Ribeiro,Zimeng Qiu,Markus Dreyer,Akari Asai,Chenyan Xiong*

Main category: cs.AI

TL;DR: 介绍了一种名为Deep Research Comparator的平台，用于评估深度研究代理的最终报告和中间步骤。


<details>
  <summary>Details</summary>
Motivation: 解决当前深度研究代理在长报告和中间步骤评估上的难题。

Method: 开发了Deep Research Comparator平台，支持代理的最终报告对比评估、中间步骤反馈，同时提供Simple Deepresearch代理框架作为基线。

Result: 收集了17名注释者对三个深度研究代理的偏好数据，验证了平台实用性。

Conclusion: 该平台为深度研究代理的开发与评估提供了有效工具。

Abstract: Effectively evaluating deep research agents that autonomously search the web,
analyze information, and generate reports remains a major challenge,
particularly when it comes to assessing long reports and giving detailed
feedback on their intermediate steps. To address these gaps, we introduce Deep
Research Comparator, a platform that offers a holistic framework for deep
research agent hosting, side-by-side comparison, fine-grained human feedback
collection, and ranking calculation. Given a user query, our platform displays
the final reports from two different agents along with their intermediate steps
during generation. Annotators can evaluate the overall quality of final reports
based on side-by-side comparison, and also provide detailed feedback separately
by assessing intermediate steps or specific text spans within the final report.
Furthermore, we develop Simple Deepresearch, an end-to-end agent scaffold. This
scaffold serves as a baseline that facilitates the easy integration of various
large language models to transform them into deep research agents for
evaluation. To demonstrate the platform's utility for deep research agent
development, we have collected real user preference data from 17 annotators on
three deep research agents. A demo video of our platform can be found at
https://www.youtube.com/watch?v=g4d2dnbdseg.

</details>


### [144] [Fine-Grained Vision-Language Modeling for Multimodal Training Assistants in Augmented Reality](https://arxiv.org/abs/2507.05515)
*Haochen Huang,Jiahuan Pei,Mohammad Aliannejadi,Xin Sun,Moonisa Ahsan,Pablo Cesar,Chuang Yu,Zhaochun Ren,Junxiao Wang*

Main category: cs.AI

TL;DR: 该论文探讨视觉语言模型(VLMs)在增强现实(AR)训练中的应用，提出了新的数据集并评估了九个先进的VLM模型。结果显示当前模型在细粒度任务中表现有限。


<details>
  <summary>Details</summary>
Motivation: 探讨和提升视觉语言模型(VLMs)在增强现实训练中的潜力，尤其是赋能视觉障碍用户。

Method: 创建针对AR训练的综合数据集，涵盖系统化的视觉语言任务，并对九个最先进的VLM模型进行评估。

Result: 发现高级模型如GPT-4o在细粒度装配任务上的表现相对较弱，状态检测任务的F1分数仅达到40.54%。

Conclusion: 该研究揭示当前视觉语言模型对细粒度任务的局限性，未来亟需改进数据集、评估基准及模型能力以提升性能。

Abstract: Vision-language models (VLMs) are essential for enabling AI-powered smart
assistants to interpret and reason in multimodal environments. However, their
application in augmented reality (AR) training remains largely unexplored. In
this work, we introduce a comprehensive dataset tailored for AR training,
featuring systematized vision-language tasks, and evaluate nine
state-of-the-art VLMs on it. Our results reveal that even advanced models,
including GPT-4o, struggle with fine-grained assembly tasks, achieving a
maximum F1 score of just 40.54% on state detection. These findings highlight
the demand for enhanced datasets, benchmarks, and further research to improve
fine-grained vision-language alignment. Beyond technical contributions, our
work has broader social implications, particularly in empowering blind and
visually impaired users with equitable access to AI-driven learning
opportunities. We provide all related resources, including the dataset, source
code, and evaluation results, to support the research community.

</details>


### [145] [Modeling (Deontic) Modal Operators With the s(CASP) Goal-directed Predicated Answer Set Programming System](https://arxiv.org/abs/2507.05519)
*Gopal Gupta,Abhiramon Rajasekharan,Alexis R. Tudor,Elmer Salazar,Joaquín Arias*

Main category: cs.AI

TL;DR: 使用ASP中的否定方式和全局约束优雅地实现了义务模态逻辑，解决了模态逻辑中的多种悖论问题。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过一种有效的方法来表达和实现义务模态逻辑（deontic modal logic），同时解决其已有的逻辑悖论问题。

Method: 利用答案集规划（ASP）中的两种否定形式（默认否定和强否定）以及全局约束来表述义务和禁止的逻辑关系，从而实现义务模态逻辑。

Result: 证明了所提出的方法可以优雅地表达模态运算符，同时解决义务模态逻辑中的多种悖论问题。

Conclusion: 本文的ASP表示方法为解决义务模态逻辑中的悖论问题提供了一种优雅且有效的方式，展示了ASP在逻辑表述中的潜力。

Abstract: We consider the problem of implementing deontic modal logic. We show how
(deontic) modal operators can be expressed elegantly using default negation
(negation-as-failure) and strong negation present in answer set programming
(ASP). We propose using global constraints of ASP to represent obligations and
impermissibilities of deontic modal logic. We show that our proposed
representation results in the various paradoxes of deontic modal logic being
elegantly resolved.

</details>


### [146] [Cultivating Multimodal Intelligence: Interpretive Reasoning and Agentic RAG Approaches to Dermatological Diagnosis](https://arxiv.org/abs/2507.05520)
*Karishma Thakrar,Shreyas Basavatia,Akshay Daftardar*

Main category: cs.AI

TL;DR: 此论文探讨如何结合多模态模型和结构化推理层来解决闭合视觉问答任务（CVQA），并在2025年ImageCLEF MEDIQA-MAGIC挑战中获第二名。


<details>
  <summary>Details</summary>
Motivation: 解决远程医疗中的诊断问题，尤其是在受限输入条件下，实现高准确性和可解释的诊断决策。

Method: (1) 微调开源多模态模型；(2) 引入结构化推理层来整合模型输出；(3) 在多模态模型中加入基于数据库的增强生成模块，以丰富诊断背景信息。

Result: 提出的方法在比赛中获得第二名，验证了高准确性和竞争力。

Conclusion: 通过模拟皮肤科医生对病症系统性推理的模式，提出了一种可靠的自动诊断支持系统解决方案，为远程医疗问题提供了潜在方法。

Abstract: The second edition of the 2025 ImageCLEF MEDIQA-MAGIC challenge, co-organized
by researchers from Microsoft, Stanford University, and the Hospital Clinic of
Barcelona, focuses on multimodal dermatology question answering and
segmentation, using real-world patient queries and images. This work addresses
the Closed Visual Question Answering (CVQA) task, where the goal is to select
the correct answer to multiple-choice clinical questions based on both
user-submitted images and accompanying symptom descriptions. The proposed
approach combines three core components: (1) fine-tuning open-source multimodal
models from the Qwen, Gemma, and LLaMA families on the competition dataset, (2)
introducing a structured reasoning layer that reconciles and adjudicates
between candidate model outputs, and (3) incorporating agentic
retrieval-augmented generation (agentic RAG), which adds relevant information
from the American Academy of Dermatology's symptom and condition database to
fill in gaps in patient context. The team achieved second place with a
submission that scored sixth, demonstrating competitive performance and high
accuracy. Beyond competitive benchmarks, this research addresses a practical
challenge in telemedicine: diagnostic decisions must often be made
asynchronously, with limited input and with high accuracy and interpretability.
By emulating the systematic reasoning patterns employed by dermatologists when
evaluating skin conditions, this architecture provided a pathway toward more
reliable automated diagnostic support systems.

</details>


### [147] [Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment](https://arxiv.org/abs/2507.05528)
*Jiahuan Pei,Fanghua Ye,Xin Sun,Wentao Deng,Koen Hindriks,Junxiao Wang*

Main category: cs.AI

TL;DR: 提出WikiHowAgent，一个基于大型语言模型的多代理工作流，用于模拟教学互动并评估教学质量，并开放相关数据和实现。


<details>
  <summary>Details</summary>
Motivation: 现有工作在利用大规模、多样化课程内容及评估教学质量方面存在局限性，难以实现可扩展性。

Method: 设计WikiHowAgent框架，包括教师、学习者代理，交互管理器和评估模块；同时构建了一个包含超过11万教学对话的数据集，并制定了综合的评估协议（计算与人工结合）。

Result: 实验表明，该工作流在不同场景下卓有成效，验证了大型语言模型在多领域的能力。

Conclusion: 提出了一个有效的多代理教学模拟框架，并提供了全面的开放资源，推动了AI在教育中的应用发展。

Abstract: Large language models (LLMs) have advanced virtual educators and learners,
bridging NLP with AI4Education. Existing work often lacks scalability and fails
to leverage diverse, large-scale course content, with limited frameworks for
assessing pedagogic quality. To this end, we propose WikiHowAgent, a
multi-agent workflow leveraging LLMs to simulate interactive teaching-learning
conversations. It integrates teacher and learner agents, an interaction
manager, and an evaluator to facilitate procedural learning and assess
pedagogic quality. We introduce a dataset of 114,296 teacher-learner
conversations grounded in 14,287 tutorials across 17 domains and 727 topics.
Our evaluation protocol combines computational and rubric-based metrics with
human judgment alignment. Results demonstrate the workflow's effectiveness in
diverse setups, offering insights into LLM capabilities across domains. Our
datasets and implementations are fully open-sourced.

</details>


### [148] [Red Teaming AI Red Teaming](https://arxiv.org/abs/2507.05538)
*Subhabrata Majumdar,Brian Pendleton,Abhishek Gupta*

Main category: cs.AI

TL;DR: 本文批判性探讨了AI红队实践，指出其目前过于关注模型级的漏洞，忽略了模型、用户和环境间复杂交互所导致的系统性问题，并提出了宏观与微观层面的红队操作框架。


<details>
  <summary>Details</summary>
Motivation: 作者注意到当前的AI红队实践重点在于发现模型层面的漏洞，而忽视了涉及更广泛社会技术系统的潜在问题，因此希望扩展该实践的适用范围。

Method: 提出了一种综合框架，在AI系统开发生命周期的宏观层面和个体模型的微观层面进行红队操作，并借助网络安全经验和系统理论提出相应建议。

Result: 该框架和建议不仅关注单一技术问题，还深入考虑了潜在的系统性风险、系统漏洞及技术与社会因素的相互作用。

Conclusion: 为了实现高效的AI红队实践，需组建多功能团队，将重点放在识别新兴风险、系统性脆弱性以及技术与社会之间的交互上。

Abstract: Red teaming has evolved from its origins in military applications to become a
widely adopted methodology in cybersecurity and AI. In this paper, we take a
critical look at the practice of AI red teaming. We argue that despite its
current popularity in AI governance, there exists a significant gap between red
teaming's original intent as a critical thinking exercise and its narrow focus
on discovering model-level flaws in the context of generative AI. Current AI
red teaming efforts focus predominantly on individual model vulnerabilities
while overlooking the broader sociotechnical systems and emergent behaviors
that arise from complex interactions between models, users, and environments.
To address this deficiency, we propose a comprehensive framework
operationalizing red teaming in AI systems at two levels: macro-level system
red teaming spanning the entire AI development lifecycle, and micro-level model
red teaming. Drawing on cybersecurity experience and systems theory, we further
propose a set of recommendations. In these, we emphasize that effective AI red
teaming requires multifunctional teams that examine emergent risks, systemic
vulnerabilities, and the interplay between technical and social factors.

</details>


### [149] [SenseCF: LLM-Prompted Counterfactuals for Intervention and Sensor Data Augmentation](https://arxiv.org/abs/2507.05541)
*Shovito Barua Soumma,Asiful Arefeen,Stephanie M. Carpenter,Melanie Hingle,Hassan Ghasemzadeh*

Main category: cs.AI

TL;DR: 研究使用大型语言模型（LLMs）生成反事实解释，特别是应用于压力预测和心脏病检测问题，优于传统方法，并能提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 利用反事实解释为机器学习预测提供人性化洞察，同时可用于异常预防干预和训练更鲁棒模型。

Method: 使用GPT-4o-mini等LLMs在零样本和三样本设置下生成反事实解释，并在两个数据集上进行评估，与传统方法进行对比分析。

Result: 在生成反事实解释方面，LLMs方法达到了高达99%的合理性、0.99的有效性，稀疏性竞争力强。同时，利用LLMs生成的反事实数据作为增强样本提升了下游分类器性能，尤其是在低数据场景中平均准确率提升5%。

Conclusion: 基于提示的大型语言生成技术有潜力提高临床和生理预测任务的可解释性和鲁棒性。

Abstract: Counterfactual explanations (CFs) offer human-centric insights into machine
learning predictions by highlighting minimal changes required to alter an
outcome. Therefore, CFs can be used as (i) interventions for abnormality
prevention and (ii) augmented data for training robust models. In this work, we
explore large language models (LLMs), specifically GPT-4o-mini, for generating
CFs in a zero-shot and three-shot setting. We evaluate our approach on two
datasets: the AI-Readi flagship dataset for stress prediction and a public
dataset for heart disease detection. Compared to traditional methods such as
DiCE, CFNOW, and NICE, our few-shot LLM-based approach achieves high
plausibility (up to 99%), strong validity (up to 0.99), and competitive
sparsity. Moreover, using LLM-generated CFs as augmented samples improves
downstream classifier performance (an average accuracy gain of 5%), especially
in low-data regimes. This demonstrates the potential of prompt-based generative
techniques to enhance explainability and robustness in clinical and
physiological prediction tasks. Code base: github.com/anonymous/SenseCF.

</details>


### [150] [SingLoRA: Low Rank Adaptation Using a Single Matrix](https://arxiv.org/abs/2507.05566)
*David Bensaïd,Noam Rotstein,Roy Velich,Daniel Bensaïd,Ron Kimmel*

Main category: cs.AI

TL;DR: 本文提出SingLoRA，通过用低秩矩阵与其转置相乘的方式改造LoRA，使训练更稳定、参数更少，同时在多个任务上取得了更优异的性能。


<details>
  <summary>Details</summary>
Motivation: 为解决LoRA中因两个小矩阵之间的尺度不一致而导致的不稳定训练和次优表现问题。

Method: 提出SingLoRA，将权重更新形式化为一个低秩矩阵与其转置的分解，消除尺度冲突，保证优化稳定，并减少参数使用。

Result: 在常识推理任务中，SingLoRA使用比LoRA少40%的参数预算，将MNLI上的准确率提升至91.3%；在图像生成任务中，使用SingLoRA使Stable Diffusion在DreamBooth上的DINO相似性评分达到0.151，比LoRA和DoRA更加优异。

Conclusion: SingLoRA通过引入单矩阵分解格式，降低了参数使用量，同时提升了多个任务的模型表现，表明其设计能够实现稳定与高效的优化。

Abstract: Low-Rank Adaptation (LoRA) has significantly advanced parameter-efficient
fine-tuning of large pretrained models. LoRA augments the pre-trained weights
of a model by adding the product of two smaller matrices that together form a
low-rank matrix update. Recent research has shown that scale disparities
between these two matrices often cause unstable training dynamics, leading to
suboptimal performance. In this paper, we propose SingLoRA, which reformulates
low-rank adaptation by learning the weights update as a decomposition of a
single low-rank matrix multiplied by its transpose. This simple design
inherently removes inter-matrix scale conflicts, ensuring stable optimization,
and roughly halves the parameter count. We analyze SingLoRA within the
infinite-width neural network framework, showing that it guarantees stable
feature learning by construction. Extensive experiments on multiple tasks
validate these benefits. In common sense reasoning, fine-tuning LLama 7B on
MNLI with SingLoRA achieves 91.3% accuracy - surpassing LoRA (89.1%) and LoRA+
(90.2%) - while using only 60% of their parameter budget. In image generation,
fine-tuning Stable Diffusion with SingLoRA significantly improves image
fidelity on DreamBooth, achieving a DINO similarity score of 0.151, compared to
scores of 0.148 and 0.143 for DoRA and LoRA, respectively.

</details>


### [151] [Towards Measurement Theory for Artificial Intelligence](https://arxiv.org/abs/2507.05587)
*Elija Perrier*

Main category: cs.AI

TL;DR: 论文提出了一种形式化的人工智能测量理论，以统一和改进AI系统评估与比较。


<details>
  <summary>Details</summary>
Motivation: 人工智能的测量还需要更为系统化的理论，以便研究人员、从业者和监管机构能更科学地评估AI系统的性能及风险。

Method: 提出一个分层的测量架构，区分直接与间接可观察的量度，并为AI现象提供统一且可校准的分类方法。

Result: 提出一个初步的测量理论框架和关键概念，以为AI测量提供更严谨的指导。

Conclusion: 形式化的AI测量理论将为系统间比较、风险分析及能力评估提供有力工具，推动AI研究和应用的科学化与统一化。

Abstract: We motivate and outline a programme for a formal theory of measurement of
artificial intelligence. We argue that formalising measurement for AI will
allow researchers, practitioners, and regulators to: (i) make comparisons
between systems and the evaluation methods applied to them; (ii) connect
frontier AI evaluations with established quantitative risk analysis techniques
drawn from engineering and safety science; and (iii) foreground how what counts
as AI capability is contingent upon the measurement operations and scales we
elect to use. We sketch a layered measurement stack, distinguish direct from
indirect observables, and signpost how these ingredients provide a pathway
toward a unified, calibratable taxonomy of AI phenomena.

</details>


### [152] [MLlm-DR: Towards Explainable Depression Recognition with MultiModal Large Language Models](https://arxiv.org/abs/2507.05591)
*Wei Zhang,Juan Chen,En Zhu,Wenhong Cheng,YunPeng Li,Yanbo J. Wang*

Main category: cs.AI

TL;DR: 本文提出了一种新的多模态大语言模型(MLlm-DR)，用于分析访谈视频以进行抑郁症诊断，并在两项基准数据集上取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化抑郁症诊断方法缺乏明确的解释，只能对当前大语言模型的有限性能提出挑战，因此需要一个可以提供解释性诊断的模型。

Method: 设计了一个结合小型大语言模型和轻量化查询模块（LQ-former）的系统，能够解析语音及视觉数据，并通过构建强大的领域训练数据集提升诊断逻辑推理能力。

Result: 在两个基准数据集CMDC和E-DAIC-WOZ上实现了最先进的诊断效果，证明了方法的有效性和优越性。

Conclusion: MLlm-DR模型能够理解多模态输入信息，生成解释性抑郁症评分，并表现出卓越的诊断性能。

Abstract: Automated depression diagnosis aims to analyze multimodal information from
interview videos to predict participants' depression scores. Previous studies
often lack clear explanations of how these scores were determined, limiting
their adoption in clinical practice. While the advent of LLMs provides a
possible pathway for explainable depression diagnosis, current LLMs capable of
processing multimodal data lack training on interview data, resulting in poor
diagnostic performance when used directly. In this paper, we propose a novel
multimodal large language model (MLlm-DR) that can understand multimodal
information inputs and supports explainable depression diagnosis. MLlm-DR
integrates a smaller LLMs and a lightweight query module (LQ-former).
Specifically, the smaller LLMs is designed to generate depression scores and
corresponding evaluation rationales. To enhance its logical reasoning for
domain-specific tasks while maintaining practicality, we constructed a robust
training dataset to fine-tune it. Meanwhile, the LQ-former captures
depression-related features from speech and visual data, aiding the model's
ability to process multimodal information, to achieve comprehensive depression
diagnosis. Our approach achieves state-of-the-art results on two
interview-based benchmark datasets, CMDC and E-DAIC-WOZ, demonstrating its
effectiveness and superiority.

</details>


### [153] [Domain adaptation of large language models for geotechnical applications](https://arxiv.org/abs/2507.05613)
*Lei Fan,Fangxue Liu,Cheng Chen*

Main category: cs.AI

TL;DR: 本文讨论了大语言模型（LLMs）如何在岩土工程和工程地质中应用，并首次对其在该领域的适配和应用进行综述。


<details>
  <summary>Details</summary>
Motivation: 探索如何将大语言模型（LLMs）适配到岩土工程领域以提升工作效率，并为学术和实践提供指导。

Method: 通过综述，分析主流适配方法，包括提示工程、检索增强生成、领域自适应预训练和微调；并探讨在该领域的具体应用场景与优势。

Result: 总结了现有研究的适应方法，应用范围，以及LLMs在岩土工程中的优点与局限性，同时提出未来研究的方向。

Conclusion: 这项研究为从业者和学术界理解如何有效整合LLMs到岩土工程领域提供了宝贵参考，同时将促进跨学科进一步研究。

Abstract: Recent developments in large language models (LLMs) are opening up new
opportunities in geotechnical engineering and engineering geology. While
general-purpose LLMs possess broad capabilities, effective application in
geotechnics often requires domain-specific adaptation. Such tailored LLMs are
increasingly employed to streamline geotechnical workflows. This paper presents
the first survey of the adaptation and application of LLMs in geotechnical
engineering. It outlines key methodologies for adaptation to geotechnical
domain, including prompt engineering, retrieval-augmented generation,
domain-adaptive pretraining, and fine-tuning. The survey examines the
state-of-the-art applications of geotechnical-adapted LLMs, including
geological interpretation, subsurface characterization, site planning, design
calculations, numerical modeling, safety and risk assessment, and educational
tutoring. It also analyzes benefits and limitations of geotechnical-adapted
LLMs, and identifies promising directions for future research in this
interdisciplinary discipline. The findings serve as a valuable resource for
practitioners seeking to integrate LLMs into geotechnical practice, while also
providing a foundation to stimulate further investigation within the academic
community.

</details>


### [154] [ADMC: Attention-based Diffusion Model for Missing Modalities Feature Completion](https://arxiv.org/abs/2507.05624)
*Wei Zhang,Juan Chen,Yanbo J. Wang,En Zhu,Xuan Yang,Yiduo Wang*

Main category: cs.AI

TL;DR: 论文提出了一种用于多模态情感和意图识别的注意力扩散模型（ADMC），解决了因模态缺失导致的性能下降问题，并在多个基准数据集上取得了最先进的效果。


<details>
  <summary>Details</summary>
Motivation: 解决多模态情感与意图识别中因传感器故障或数据不完整而导致的模态缺失问题，以及传统方法中过度耦合和生成精度不佳的问题。

Method: 引入了Attention-based Diffusion模型，该模型通过独立训练每个模态的特征提取网络，避免特征过度耦合，利用注意力机制产生贴近真实多模态分布的缺失模态特征。

Result: 所提方法在IEMOCAP和MIntRec基准数据集上达到了最先进的结果，在模态缺失和完整模态场景中均表现出色。

Conclusion: 注意力扩散模型（ADMC）成功解决了模态缺失问题，同时提升了完整模态情感与意图识别的性能，证明了方法的有效性。

Abstract: Multimodal emotion and intent recognition is essential for automated
human-computer interaction, It aims to analyze users' speech, text, and visual
information to predict their emotions or intent. One of the significant
challenges is that missing modalities due to sensor malfunctions or incomplete
data. Traditional methods that attempt to reconstruct missing information often
suffer from over-coupling and imprecise generation processes, leading to
suboptimal outcomes. To address these issues, we introduce an Attention-based
Diffusion model for Missing Modalities feature Completion (ADMC). Our framework
independently trains feature extraction networks for each modality, preserving
their unique characteristics and avoiding over-coupling. The Attention-based
Diffusion Network (ADN) generates missing modality features that closely align
with authentic multimodal distribution, enhancing performance across all
missing-modality scenarios. Moreover, ADN's cross-modal generation offers
improved recognition even in full-modality contexts. Our approach achieves
state-of-the-art results on the IEMOCAP and MIntRec benchmarks, demonstrating
its effectiveness in both missing and complete modality scenarios.

</details>


### [155] [Enhancing Student Learning with LLM-Generated Retrieval Practice Questions: An Empirical Study in Data Science Courses](https://arxiv.org/abs/2507.05629)
*Yuan An,John Liu,Niyam Acharya,Ruhma Hashmi*

Main category: cs.AI

TL;DR: 该研究探讨大型语言模型（LLM）自动生成的检索练习题对学习效果的影响，显示其可显著提高学生知识保留率。


<details>
  <summary>Details</summary>
Motivation: 当前优秀的教学方法（如检索练习）虽能提升学习效果，但因高质量题目生成困难，应用受限。而LLM有潜力简化这一过程，因此需要评估其生成题目的教育有效性。

Method: 在两门大学数据科学课程中展开实证研究，共约60名学生。比较了提供LLM生成检索练习题和不提供该练习题的两周学习结果。

Result: 学生接受LLM生成的检索练习后知识保留率显著提升，正确率从无练习的73%提高到89%。

Conclusion: LLM生成的检索练习题可支持学生学习并具有可扩展性，但因题目质量不一，仍需教师审查和修订。

Abstract: Retrieval practice is a well-established pedagogical technique known to
significantly enhance student learning and knowledge retention. However,
generating high-quality retrieval practice questions is often time-consuming
and labor intensive for instructors, especially in rapidly evolving technical
subjects. Large Language Models (LLMs) offer the potential to automate this
process by generating questions in response to prompts, yet the effectiveness
of LLM-generated retrieval practice on student learning remains to be
established. In this study, we conducted an empirical study involving two
college-level data science courses, with approximately 60 students. We compared
learning outcomes during one week in which students received LLM-generated
multiple-choice retrieval practice questions to those from a week in which no
such questions were provided. Results indicate that students exposed to
LLM-generated retrieval practice achieved significantly higher knowledge
retention, with an average accuracy of 89%, compared to 73% in the week without
such practice. These findings suggest that LLM-generated retrieval questions
can effectively support student learning and may provide a scalable solution
for integrating retrieval practice into real-time teaching. However, despite
these encouraging outcomes and the potential time-saving benefits, cautions
must be taken, as the quality of LLM-generated questions can vary. Instructors
must still manually verify and revise the generated questions before releasing
them to students.

</details>


### [156] [LLMs are Introvert](https://arxiv.org/abs/2507.05638)
*Litian Zhang,Xiaoming Zhang,Bingyu Yan,Ziyi Zhou,Bo Zhang,Zhenyu Guan,Xi Zhang,Chaozhuo Li*

Main category: cs.AI

TL;DR: 社交媒体与生成式AI的快速发展虽然促进了连接，但也加速了错误信息的传播。本研究提出了基于大型语言模型(LLM)的传播模拟方案，通过引入情感导向的记忆与社会信息处理机制(SIP-CoT)提升了模型的社会智能和真实性。


<details>
  <summary>Details</summary>
Motivation: 现有信息传播模型无法全面捕捉在线互动的复杂性，而LLM具有人类式的推理能力，有望模拟信息传播过程中的心理维度。本研究旨在改进LLM在社会信息传播模拟中的表现，解决其在情感处理等方面的局限。

Method: 研究提出了结合社会信息处理理论和链式思维(SIP-CoT)机制的模拟系统，并引入情感导向记忆，以改善对社会线索的理解、目标的个性化以及反馈的评估。

Result: 实验表明，采用SIP-CoT机制的LLM模拟更有效地处理社会信息，表现出了更接近真实人类互动的行为、态度和情感。

Conclusion: 本文揭示了当前LLM传播模拟的关键局限性，并证明通过整合SIP-CoT和情感记忆，可显著增强LLM代理的社会智能与真实性。

Abstract: The exponential growth of social media and generative AI has transformed
information dissemination, fostering connectivity but also accelerating the
spread of misinformation. Understanding information propagation dynamics and
developing effective control strategies is essential to mitigate harmful
content. Traditional models, such as SIR, provide basic insights but
inadequately capture the complexities of online interactions. Advanced methods,
including attention mechanisms and graph neural networks, enhance accuracy but
typically overlook user psychology and behavioral dynamics. Large language
models (LLMs), with their human-like reasoning, offer new potential for
simulating psychological aspects of information spread. We introduce an
LLM-based simulation environment capturing agents' evolving attitudes,
emotions, and responses. Initial experiments, however, revealed significant
gaps between LLM-generated behaviors and authentic human dynamics, especially
in stance detection and psychological realism. A detailed evaluation through
Social Information Processing Theory identified major discrepancies in
goal-setting and feedback evaluation, stemming from the lack of emotional
processing in standard LLM training. To address these issues, we propose the
Social Information Processing-based Chain of Thought (SIP-CoT) mechanism
enhanced by emotion-guided memory. This method improves the interpretation of
social cues, personalization of goals, and evaluation of feedback. Experimental
results confirm that SIP-CoT-enhanced LLM agents more effectively process
social information, demonstrating behaviors, attitudes, and emotions closer to
real human interactions. In summary, this research highlights critical
limitations in current LLM-based propagation simulations and demonstrates how
integrating SIP-CoT and emotional memory significantly enhances the social
intelligence and realism of LLM agents.

</details>


### [157] [City-Level Foreign Direct Investment Prediction with Tabular Learning on Judicial Data](https://arxiv.org/abs/2507.05651)
*Tianxing Wu,Lizhe Cao,Shuang Wang,Jiming Wang,Shutong Zhu,Yerong Wu,Yuqing Feng*

Main category: cs.AI

TL;DR: 本文提出了一种基于司法数据的城市层面外国直接投资（FDI）预测方法，称为TLJD，利用大规模司法数据代替不可靠的经济数据实现更精确的预测。


<details>
  <summary>Details</summary>
Motivation: 当前大多数基于经济数据的FDI预测方法可能因数据操控性而不够可靠，因此提出利用司法数据来反映地方投资安全性及回报水平进行预测，以更可靠地支持联合国可持续发展目标中的经济增长目标。

Method: 构建了一个基于公开裁定文书的司法表现评价指标体系，并据此重构了一个表格数据集。提出了TLJD方法，将表格数据中的行与列信息结合用于司法指标编码，同时引入混合专家模型调整指标权重，适用于区域差异。

Result: 在跨城市和跨时间的FDI预测任务中，TLJD方法在多个评估指标上优于十大现有基线方法（R2至少达到0.92）。

Conclusion: TLJD方法证明了利用司法数据预测FDI的可行性和优越性，为地方政府提供了一种更加可靠的经济预测工具，有助于实现可持续经济增长目标。

Abstract: To advance the United Nations Sustainable Development Goal on promoting
sustained, inclusive, and sustainable economic growth, foreign direct
investment (FDI) plays a crucial role in catalyzing economic expansion and
fostering innovation. Precise city-level FDI prediction is quite important for
local government and is commonly studied based on economic data (e.g., GDP).
However, such economic data could be prone to manipulation, making predictions
less reliable. To address this issue, we try to leverage large-scale judicial
data which reflects judicial performance influencing local investment security
and returns, for city-level FDI prediction. Based on this, we first build an
index system for the evaluation of judicial performance over twelve million
publicly available adjudication documents according to which a tabular dataset
is reformulated. We then propose a new Tabular Learning method on Judicial Data
(TLJD) for city-level FDI prediction. TLJD integrates row data and column data
in our built tabular dataset for judicial performance indicator encoding, and
utilizes a mixture of experts model to adjust the weights of different
indicators considering regional variations. To validate the effectiveness of
TLJD, we design cross-city and cross-time tasks for city-level FDI predictions.
Extensive experiments on both tasks demonstrate the superiority of TLJD (reach
to at least 0.92 R2) over the other ten state-of-the-art baselines in different
evaluation metrics.

</details>


### [158] [Divergent Realities: A Comparative Analysis of Human Expert vs. Artificial Intelligence Based Generation and Evaluation of Treatment Plans in Dermatology](https://arxiv.org/abs/2507.05716)
*Dipayan Sengupta,Saumya Panda*

Main category: cs.AI

TL;DR: 研究比较了专家与AI模型生成的治疗方案评估，发现评估结果取决于评估者的性质。


<details>
  <summary>Details</summary>
Motivation: 探索AI在生成与评估医疗治疗方案中的潜力，以及人类与AI评估之间的差异。

Method: 使用10名专家和两种AI模型对皮肤病案例生成治疗方案，并分别通过专家与一个高级AI评估。

Result: 专家更倾向于为人类方案打高分，而高级AI则评估AI方案优于人类方案，两者评分存在显著差异。

Conclusion: 需设计结合人类与AI优势的协同系统，以弥合差距并改善医疗服务。

Abstract: Background: Evaluating AI-generated treatment plans is a key challenge as AI
expands beyond diagnostics, especially with new reasoning models. This study
compares plans from human experts and two AI models (a generalist and a
reasoner), assessed by both human peers and a superior AI judge.
  Methods: Ten dermatologists, a generalist AI (GPT-4o), and a reasoning AI
(o3) generated treatment plans for five complex dermatology cases. The
anonymized, normalized plans were scored in two phases: 1) by the ten human
experts, and 2) by a superior AI judge (Gemini 2.5 Pro) using an identical
rubric.
  Results: A profound 'evaluator effect' was observed. Human experts scored
peer-generated plans significantly higher than AI plans (mean 7.62 vs. 7.16;
p=0.0313), ranking GPT-4o 6th (mean 7.38) and the reasoning model, o3, 11th
(mean 6.97). Conversely, the AI judge produced a complete inversion, scoring AI
plans significantly higher than human plans (mean 7.75 vs. 6.79; p=0.0313). It
ranked o3 1st (mean 8.20) and GPT-4o 2nd, placing all human experts lower.
  Conclusions: The perceived quality of a clinical plan is fundamentally
dependent on the evaluator's nature. An advanced reasoning AI, ranked poorly by
human experts, was judged as superior by a sophisticated AI, revealing a deep
gap between experience-based clinical heuristics and data-driven algorithmic
logic. This paradox presents a critical challenge for AI integration,
suggesting the future requires synergistic, explainable human-AI systems that
bridge this reasoning gap to augment clinical care.

</details>


### [159] [An autonomous agent for auditing and improving the reliability of clinical AI models](https://arxiv.org/abs/2507.05755)
*Lukas Kuhn,Florian Buettner*

Main category: cs.AI

TL;DR: ModelAuditor是一种工具，能够在临床环境下模拟和检测AI模型的潜在失败模式，并提供改进建议，显著提高AI模型的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型在应对医学影像中的现实变化时容易失败，而识别这些失败模式的过程既繁琐又费时，需要一种更高效的方法辅助部署前的可靠性审计。

Method: ModelAuditor结合多代理架构，与用户互动，选择特定任务的指标，模拟与临床相关的分布变化，并生成报告，解释性能退化及改进方案。

Result: 在病理学、皮肤病学和胸部放射等三种场景中，ModelAuditor成功识别模型的上下文特定失败模式，其改进建议能恢复15-25%的性能下降，表现优于基准模型和先进数据增强方法。

Conclusion: ModelAuditor有效提升了AI模型在临床实际应用中的可靠性，并且在成本和时间效率上具有显著优势，适合大规模应用。

Abstract: The deployment of AI models in clinical practice faces a critical challenge:
models achieving expert-level performance on benchmarks can fail
catastrophically when confronted with real-world variations in medical imaging.
Minor shifts in scanner hardware, lighting or demographics can erode accuracy,
but currently reliability auditing to identify such catastrophic failure cases
before deployment is a bespoke and time-consuming process. Practitioners lack
accessible and interpretable tools to expose and repair hidden failure modes.
Here we introduce ModelAuditor, a self-reflective agent that converses with
users, selects task-specific metrics, and simulates context-dependent,
clinically relevant distribution shifts. ModelAuditor then generates
interpretable reports explaining how much performance likely degrades during
deployment, discussing specific likely failure modes and identifying root
causes and mitigation strategies. Our comprehensive evaluation across three
real-world clinical scenarios - inter-institutional variation in
histopathology, demographic shifts in dermatology, and equipment heterogeneity
in chest radiography - demonstrates that ModelAuditor is able correctly
identify context-specific failure modes of state-of-the-art models such as the
established SIIM-ISIC melanoma classifier. Its targeted recommendations recover
15-25% of performance lost under real-world distribution shift, substantially
outperforming both baseline models and state-of-the-art augmentation methods.
These improvements are achieved through a multi-agent architecture and execute
on consumer hardware in under 10 minutes, costing less than US$0.50 per audit.

</details>


### [160] [Real-time monitoring of the SoH of lithium-ion batteries](https://arxiv.org/abs/2507.05765)
*Bruno Jammes,Edgar Hernando Sepúlveda-Oviedo,Corinne Alonso*

Main category: cs.AI

TL;DR: 本文提出了一种基于放电脉冲分析的创新方法，用于电池健康状态（SoH）的实时监测，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 目前在微电网中实时监测电池健康状态面临重大挑战，传统方法受限。因此，提出一种可适用于实际操作的新方法。

Method: 利用充电末期的放电脉冲，分析其电压变化参数并建立等效电气模型，进而估算电池SoH。

Result: 通过实验数据验证，该方法在预测电池衰减方面取得了较好结果，如在约90% SoH时的平均绝对误差约为1%，估计器可解释性评分接近0.9。

Conclusion: 如果进一步验证其性能，该方法可集成至电池管理系统（BMS），实现电池的优化管理并支持连续运行。

Abstract: Real-time monitoring of the state of health (SoH) of batteries remains a
major challenge, particularly in microgrids where operational constraints limit
the use of traditional methods. As part of the 4BLife project, we propose an
innovative method based on the analysis of a discharge pulse at the end of the
charge phase. The parameters of the equivalent electrical model describing the
voltage evolution across the battery terminals during this current pulse are
then used to estimate the SoH. Based on the experimental data acquired so far,
the initial results demonstrate the relevance of the proposed approach. After
training using the parameters of two batteries with a capacity degradation of
around 85%, we successfully predicted the degradation of two other batteries,
cycled down to approximately 90% SoH, with a mean absolute error of around 1%
in the worst case, and an explainability score of the estimator close to 0.9.
If these performances are confirmed, this method can be easily integrated into
battery management systems (BMS) and paves the way for optimized battery
management under continuous operation.

</details>


### [161] [GTA1: GUI Test-time Scaling Agent](https://arxiv.org/abs/2507.05791)
*Yan Yang,Dongxu Li,Yutong Dai,Yuhao Yang,Ziyang Luo,Zirui Zhao,Zhiyuan Hu,Junzhe Huang,Amrita Saha,Zeyuan Chen,Ran Xu,Liyuan Pan,Caiming Xiong,Junnan Li*

Main category: cs.AI

TL;DR: 本文提出了一种名为GTA1的图形用户界面（GUI）代理，通过测试时放缩（test-time scaling）方法和强化学习（RL），解决了任务规划中的歧义和精准定位界面目标的问题，取得了多个基准测试的最新性能。


<details>
  <summary>Details</summary>
Motivation: 现代GUI代理需要在处理任务指令时，解决任务规划中的多样性和界面复杂度带来的挑战，因此需要设计更有效的任务解构方法和精准的目标交互方案。

Method: 通过测试时放缩技术，候选多个动作提议，并引入评估模型选择最优方案，同时通过强化学习提高视觉目标定位的准确性，优化整体交互性能。

Result: 实验结果表明，GTA1在多个基准（如Screenspot-Pro、Screenspot-V2和OSWorld-G）上达到了分别为50.1%、92.4%和67.7%的准确率，同时其任务成功率在OSWorld上为45.2%。

Conclusion: 提出的GTA1方法通过引入测试时放缩策略和强化学习显著提高了GUI代理在任务规划和视觉目标定位中的表现，推动了领域技术的发展。

Abstract: Graphical user interface (GUI) agents autonomously operate across platforms
(e.g., Linux) to complete tasks by interacting with visual elements.
Specifically, a user instruction is decomposed into a sequence of action
proposals, each corresponding to an interaction with the GUI. After each
action, the agent observes the updated GUI environment to plan the next step.
However, two main challenges arise: i) resolving ambiguity in task planning
(i.e., the action proposal sequence), where selecting an appropriate plan is
non-trivial, as many valid ones may exist; ii) accurately grounding actions in
complex and high-resolution interfaces, i.e., precisely interacting with visual
targets.
  This paper investigates the two aforementioned challenges with our GUI
Test-time Scaling Agent, namely GTA1. First, to select the most appropriate
action proposal, we introduce a test-time scaling method. At each step, we
sample multiple candidate action proposals and leverage a judge model to
evaluate and select the most suitable one. It trades off computation for better
decision quality by concurrent sampling, shortening task execution steps, and
improving overall performance. Second, we propose a model that achieves
improved accuracy when grounding the selected action proposal to its
corresponding visual elements. Our key insight is that reinforcement learning
(RL) facilitates visual grounding through inherent objective alignments,
rewarding successful clicks on interface elements.
  Experimentally, our method establishes state-of-the-art performance across
diverse benchmarks. For example, GTA1-7B achieves 50.1%, 92.4%, and 67.7%
accuracies on Screenspot-Pro, Screenspot-V2, and OSWorld-G, respectively. When
paired with a planner applying our test-time scaling strategy, it exhibits
state-of-the-art agentic performance (e.g., 45.2% task success rate on
OSWorld). We open-source our code and models here.

</details>


### [162] [Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting Retinopathy of Prematurity](https://arxiv.org/abs/2507.05816)
*Shuai Zhao,Yulin Zhang,Luwei Xiao,Xinyi Wu,Yanhao Jia,Zhongliang Guo,Xiaobao Wu,Cong-Duy Nguyen,Guoming Zhang,Anh Tuan Luu*

Main category: cs.AI

TL;DR: 引入了一个名为CROP的中文基准数据集和测试框架Affective-ROPTester，用于评估大语言模型在ROP（早产儿视网膜病变）风险预测中的能力与情感偏差，并提出情感敏感的提示工程可以提高诊断可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在各个领域取得了显著进展，但其在预测ROP风险方面的能力仍未受到充分探索，研究意在填补这一空白。

Method: 创建一个包含不同风险标注的中文数据集CROP，并提出Affective-ROPTester自动评估框架，引入三种提示策略（Instruction-based, Chain-of-Thought, In-Context Learning）及情感提示方式以评估模型的预测能力和情感偏差。

Result: 1. LLM仅依靠自身知识对ROP的预测有限，而结合外部结构化输入后表现显著提升；2. 模型输出存在情感偏差，且倾向高估中高风险病例；3. 积极情感提示有助于减轻预测偏差。

Conclusion: 情感敏感的提示工程对提高诊断可靠性有重要作用，Affective-ROPTester可作为评估和缓解临床语言模型情感偏差的有效框架。

Abstract: Despite the remarkable progress of large language models (LLMs) across
various domains, their capacity to predict retinopathy of prematurity (ROP)
risk remains largely unexplored. To address this gap, we introduce a novel
Chinese benchmark dataset, termed CROP, comprising 993 admission records
annotated with low, medium, and high-risk labels. To systematically examine the
predictive capabilities and affective biases of LLMs in ROP risk
stratification, we propose Affective-ROPTester, an automated evaluation
framework incorporating three prompting strategies: Instruction-based,
Chain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme
assesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and
ICL schemes leverage external medical knowledge to enhance predictive accuracy.
Crucially, we integrate emotional elements at the prompt level to investigate
how different affective framings influence the model's ability to predict ROP
and its bias patterns. Empirical results derived from the CROP dataset yield
two principal observations. First, LLMs demonstrate limited efficacy in ROP
risk prediction when operating solely on intrinsic knowledge, yet exhibit
marked performance gains when augmented with structured external inputs.
Second, affective biases are evident in the model outputs, with a consistent
inclination toward overestimating medium- and high-risk cases. Third, compared
to negative emotions, positive emotional framing contributes to mitigating
predictive bias in model outputs. These findings highlight the critical role of
affect-sensitive prompt engineering in enhancing diagnostic reliability and
emphasize the utility of Affective-ROPTester as a framework for evaluating and
mitigating affective bias in clinical language modeling systems.

</details>


### [163] [CogniPlay: a work-in-progress Human-like model for General Game Playing](https://arxiv.org/abs/2507.05868)
*Aloïs Rautureau,Éric Piette*

Main category: cs.AI

TL;DR: 人工智能系统在各种游戏中超越了人类表现，但仍未实现真正的“类人”特性。


<details>
  <summary>Details</summary>
Motivation: 探索如何建立能够模拟人类直觉化、模式化决策过程的人工智能。

Method: 回顾认知心理学研究及人工智能模拟“类人”行为的尝试，并提出基于这些观察的CogniPlay模型。

Result: 尚为工作中模型，旨在将认知心理学的发现应用于通用游戏应用领域。

Conclusion: CogniPlay是一个早期模型，尝试弥补现有游戏AI与人类认知行为的差距。

Abstract: While AI systems have equaled or surpassed human performance in a wide
variety of games such as Chess, Go, or Dota 2, describing these systems as
truly "human-like" remains far-fetched. Despite their success, they fail to
replicate the pattern-based, intuitive decision-making processes observed in
human cognition. This paper presents an overview of findings from cognitive
psychology and previous efforts to model human-like behavior in artificial
agents, discusses their applicability to General Game Playing (GGP) and
introduces our work-in-progress model based on these observations: CogniPlay.

</details>


### [164] [Current Practices for Building LLM-Powered Reasoning Tools Are Ad Hoc -- and We Can Do Better](https://arxiv.org/abs/2507.05886)
*Aaron Bembenek*

Main category: cs.AI

TL;DR: 作者提出了一种称为神经符号状态转换系统的新计算模型，用以改进当前基于LLM的自动推理工具，目标是实现符号算法的强大保障与神经网络深度结合。


<details>
  <summary>Details</summary>
Motivation: 目前的神经符号自动推理系统实现方式较为随意，缺乏符号算法的强保障，同时神经网络与符号推理的结合不够紧密，因此需要一种改进的计算模型。

Method: 提出了神经符号状态转换系统的模型：此模型将符号状态与直觉相结合，并使状态转换在符号和直觉上并行操作。同时简要描述了这一模型在逻辑编程语言中的实现方式。

Result: 该模型理论上可以在保留符号算法强保障的同时，使逻辑推理能力大幅扩展。

Conclusion: 神经符号状态转换系统有潜力成为设计LLM驱动自动推理工具的新基础，其结合了符号推理的可靠性和神经网络的潜在强计算能力。

Abstract: There is growing excitement about building software verifiers, synthesizers,
and other Automated Reasoning (AR) tools by combining traditional symbolic
algorithms and Large Language Models (LLMs). Unfortunately, the current
practice for constructing such neurosymbolic AR systems is an ad hoc
programming model that does not have the strong guarantees of traditional
symbolic algorithms, nor a deep enough synchronization of neural networks and
symbolic reasoning to unlock the full potential of LLM-powered reasoning. I
propose Neurosymbolic Transition Systems as a principled computational model
that can underlie infrastructure for building neurosymbolic AR tools. In this
model, symbolic state is paired with intuition, and state transitions operate
over symbols and intuition in parallel. I argue why this new paradigm can scale
logical reasoning beyond current capabilities while retaining the strong
guarantees of symbolic algorithms, and I sketch out how the computational model
I propose can be reified in a logic programming language.

</details>


### [165] [Decomposing the Time Series Forecasting Pipeline: A Modular Approach for Time Series Representation, Information Extraction, and Projection](https://arxiv.org/abs/2507.05891)
*Robert Leppich,Michael Stenger,André Bauer,Samuel Kounev*

Main category: cs.AI

TL;DR: 提出了一种新方法分解时间序列预测流程为三个阶段，并通过实验验证其在多个数据集上的效果，取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统时间序列预测中序列表示、信息提取、目标投射的种种挑战，并尝试以系统化方法改进模型表现和效率。

Method: 将时间序列预测流程分为三个阶段：输入序列表示、信息提取及记忆构建、目标投射。在各阶段内引入各类模块（如卷积、注意力机制），并在七个数据集上进行评估。

Result: 模型达到了SOTA预测精度，并显著降低了训练和推理时间以及参数量。

Conclusion: 通过模块化设计的方式显著改进了时间序列预测的精度和效率，为未来研究提供了新思路。代码已开源。

Abstract: With the advent of Transformers, time series forecasting has seen significant
advances, yet it remains challenging due to the need for effective sequence
representation, memory construction, and accurate target projection. Time
series forecasting remains a challenging task, demanding effective sequence
representation, meaningful information extraction, and precise future
projection. Each dataset and forecasting configuration constitutes a distinct
task, each posing unique challenges the model must overcome to produce accurate
predictions. To systematically address these task-specific difficulties, this
work decomposes the time series forecasting pipeline into three core stages:
input sequence representation, information extraction and memory construction,
and final target projection. Within each stage, we investigate a range of
architectural configurations to assess the effectiveness of various modules,
such as convolutional layers for feature extraction and self-attention
mechanisms for information extraction, across diverse forecasting tasks,
including evaluations on seven benchmark datasets. Our models achieve
state-of-the-art forecasting accuracy while greatly enhancing computational
efficiency, with reduced training and inference times and a lower parameter
count. The source code is available at
https://github.com/RobertLeppich/REP-Net.

</details>


### [166] [MusiScene: Leveraging MU-LLaMA for Scene Imagination and Enhanced Video Background Music Generation](https://arxiv.org/abs/2507.05894)
*Fathinah Izzati,Xinyue Li,Yuxuan Wu,Gus Xia*

Main category: cs.AI

TL;DR: 这篇论文探讨了音乐语言模型是否可以通过多模态信息实现音乐场景想象任务，并提出了MusiScene模型，该模型生成与音乐匹配的场景描述。


<details>
  <summary>Details</summary>
Motivation: 研究人员受人类听音乐时想象场景的能力启发，探讨音乐语言模型是否可以实现类似功能。

Method: 提出了一个名为MusiScene的模型，通过构建3,371对视频-音频数据集，将Music Understanding LLaMA微调为音乐场景想象任务专用模型，并对其性能进行全面评估。

Result: MusiScene证明在生成与音乐情境相关的文字描述方面优于现有模式，如MU-LLaMA，并在文本到视频背景音乐生成任务中有所应用。

Conclusion: MusiScene能够更好地理解和生成与音乐相关的情境描述，为音乐与视频交互的研究提供了新的可能性。

Abstract: Humans can imagine various atmospheres and settings when listening to music,
envisioning movie scenes that complement each piece. For example, slow,
melancholic music might evoke scenes of heartbreak, while upbeat melodies
suggest celebration. This paper explores whether a Music Language Model, e.g.
MU-LLaMA, can perform a similar task, called Music Scene Imagination (MSI),
which requires cross-modal information from video and music to train. To
improve upon existing music captioning models which focusing solely on musical
elements, we introduce MusiScene, a music captioning model designed to imagine
scenes that complement each music. In this paper, (1) we construct a
large-scale video-audio caption dataset with 3,371 pairs, (2) we finetune Music
Understanding LLaMA for the MSI task to create MusiScene, and (3) we conduct
comprehensive evaluations and prove that our MusiScene is more capable of
generating contextually relevant captions compared to MU-LLaMA. We leverage the
generated MSI captions to enhance Video Background Music Generation (VBMG) from
text.

</details>


### [167] [BlueLM-2.5-3B Technical Report](https://arxiv.org/abs/2507.05934)
*Baojiao Xiong,Boheng Chen,Chengzhi Wang,Daxiong Luo,Dongsheng Xu,Dongyang Liu,Fan Yang,Fangyuan Li,Fei Teng,Feng Wang,Fukang Qin,Fuquan Peng,Guanxin Tan,Guozhi Wang,Haibo Yu,Haohao Gao,Heng Liu,Hongbo Yang,Hongjian Zou,Houzheng Shen,Hu Meng,Huan Li,Hui Tan,Jiali Chen,Jianzhao Chen,Jinliang Zhu,Kai Wang,Lei Wu,Liangbing Liu,Liuyang Bian,Liyan He,Long Liu,Peiwen Li,Penggang Shi,Qi Ding,Rui Hu,Shuai Cao,Shuai Ren,Shuang Peng,Teng Xie,Weiji Chen,Weilin Xiang,Weixin Wu,Xi Yin,Xiaoxin Chen,Xu Chen,Yafei Wen,Yan Hu,Yanzhou Yang,Yina Xie,Yinghao Chen,Yixuan Liao,Yu Geng,Yuanjiang Ouyang,Yuanzhuo Yang,Yuehua He,Yushuai Peng,Zhaoxiong Wang,Zheng Wang,Zhibo Zhou,Ziyang Wu*

Main category: cs.AI

TL;DR: 展示了一种名为BlueLM-2.5-3B的新型多模态大语言模型，专为边缘设备优化，兼具高效性与强大的推理能力。


<details>
  <summary>Details</summary>
Motivation: 旨在创建一个参数规模仅为3B但具备强大多模态理解能力的边缘设备高效部署语言模型。

Method: 通过多样化数据获取、数据重采样、混合异构强化学习以及高性能训练机制开发BlueLM-2.5-3B。

Result: 在多模态评估中，性能接近更大的模型。蓝思维模式下与Qwen3-4B接近，仅比Kimi-VL-A3B-16B差5%。非蓝思维模式下优于Qwen2.5-VL-3B。整体训练数据量显著低于竞争模型。

Conclusion: BlueLM-2.5-3B以低参数、高效率实现了强大的多模态与单模态表现，为边缘设备上的MLLM研究提供了有益启发。

Abstract: We present BlueLM-2.5-3B, a compact and unified dense Multimodal Large
Language Model (MLLM) designed for efficient edge-device deployment, offering
strong general-purpose and reasoning capabilities. To the best of our
knowledge, this is the first 3B-scale MLLM to support both thinking and
non-thinking modes, while also enabling explicit control over thinking token
budget. BlueLM-2.5-3B is developed through diversified data curation, key data
resampling, hybrid heterogeneous reinforcement learning, and a high-performance
training infrastructure. Our model achieves superior multimodal capacity while
preserving competitive pure-text performance with only 2.9 billion parameters.
We conduct comprehensive evaluations across a broad range of multimodal and
text-only benchmarks. In thinking mode, BlueLM-2.5-3B achieves comparable
performance to Qwen3-4B on text-only benchmarks, and trails the larger
Kimi-VL-A3B-16B by only about 5% on average across multimodal evaluations. In
non-thinking mode, it outperforms Qwen2.5-VL-3B on the majority of multimodal
benchmarks. Additionally, BlueLM-2.5-3B exhibits exceptional data efficiency.
All of the aforementioned performance is achieved with substantially less total
training data than Qwen2.5-VL-3B and Qwen3-4B. We hope our work contributes to
the advancement of high-performance, on-device MLLMs and provides meaningful
insights to the research community.

</details>


### [168] [A Wireless Foundation Model for Multi-Task Prediction](https://arxiv.org/abs/2507.05938)
*Yucheng Sheng,Jiacheng Wang,Xingyu Zhou,Le Liang,Hao Ye,Shi Jin,Geoffrey Ye Li*

Main category: cs.AI

TL;DR: 我们提出了一种用于无线网络多任务预测的统一基础模型，解决复杂场景中的不同任务和时间间隔预测问题。


<details>
  <summary>Details</summary>
Motivation: 随着移动通信网络的日益复杂和动态化，传统基于深度学习的方法在不同场景和任务中缺乏泛化能力。

Method: 提出基础模型，整合异构任务采用单变量分解机制，加入时间间隔编码，并使用因果Transformer主干实现精确预测，同时引入补丁遮掩策略支持不同输入长度。

Result: 经过大规模数据集训练后，该模型在未见场景下表现出良好的泛化能力，并在零次样本任务上超过传统全样本基线性能。

Conclusion: 该模型提供了一个有效的解决方案，统一了无线网络中的多任务预测，并展现了其在复杂动态环境中的适应能力。

Abstract: With the growing complexity and dynamics of the mobile communication
networks, accurately predicting key system parameters, such as channel state
information (CSI), user location, and network traffic, has become essential for
a wide range of physical (PHY)-layer and medium access control (MAC)-layer
tasks. Although traditional deep learning (DL)-based methods have been widely
applied to such prediction tasks, they often struggle to generalize across
different scenarios and tasks. In response, we propose a unified foundation
model for multi-task prediction in wireless networks that supports diverse
prediction intervals. The proposed model enforces univariate decomposition to
unify heterogeneous tasks, encodes granularity for interval awareness, and uses
a causal Transformer backbone for accurate predictions. Additionally, we
introduce a patch masking strategy during training to support arbitrary input
lengths. After trained on large-scale datasets, the proposed foundation model
demonstrates strong generalization to unseen scenarios and achieves zero-shot
performance on new tasks that surpass traditional full-shot baselines.

</details>


### [169] [Enhancing the Interpretability of Rule-based Explanations through Information Retrieval](https://arxiv.org/abs/2507.05976)
*Alessandro Umbrico,Guido Bologna,Luca Coraci,Francesca Fracasso,Silvia Gola,Gabriella Cortellessa*

Main category: cs.AI

TL;DR: 为提升人工智能在医疗领域的可解释性，该研究提出一种基于属性分析的方法，并在乳腺癌治疗的淋巴水肿风险评估中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 目前许多数据驱动的人工智能技术在医疗领域中的应用受限于缺乏透明性和可解释性。提升人工智能对医疗决策支持的解释能力极为重要。

Method: 采用基于属性的统计分析方法，利用信息检索中的标准指标，评估规则预测模型中每个属性对预测结果的影响，为用户提供风险因素的解释性信息。

Result: 用户研究表明，与传统人工智能模型的输出相比，提出的方法显著提升了风险预测的可解释性与实用性。

Conclusion: 该方法有效增强了人工智能在特定医疗背景下的可解释性，为提高人工智能在医疗决策中的应用价值提供了新方向。

Abstract: The lack of transparency of data-driven Artificial Intelligence techniques
limits their interpretability and acceptance into healthcare decision-making
processes. We propose an attribution-based approach to improve the
interpretability of Explainable AI-based predictions in the specific context of
arm lymphedema's risk assessment after lymph nodal radiotherapy in breast
cancer. The proposed method performs a statistical analysis of the attributes
in the rule-based prediction model using standard metrics from Information
Retrieval techniques. This analysis computes the relevance of each attribute to
the prediction and provides users with interpretable information about the
impact of risk factors. The results of a user study that compared the output
generated by the proposed approach with the raw output of the Explainable AI
model suggested higher levels of interpretability and usefulness in the context
of predicting lymphedema risk.

</details>


### [170] [Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening](https://arxiv.org/abs/2507.05984)
*Zhijun Guo,Alvina Lai,Julia Ive,Alexandru Petcu,Yutong Wang,Luyuan Qi,Johan H Thygesen,Kezhi Li*

Main category: cs.AI

TL;DR: 我们开发了一个名为HopeBot的基于LLM的聊天机器人，可实现动态、交互式的PHQ-9抑郁筛查，研究显示其信任度和用户意愿均高。


<details>
  <summary>Details</summary>
Motivation: 传统静态工具如PHQ-9虽然有效，但缺乏互动性和适应性，无法很好满足个性化需求，因此需要开发能够动态交互并适应用户需求的新型解决方案。

Method: 利用大型语言模型（LLM）开发HopeBot，支持通过信息检索增强生成和实时澄清方式进行PHQ-9筛查，并设计了一项横向研究，比较自助与聊天机器人版本之间的表现及用户反馈。

Result: 研究发现，这两种筛查方式表现高度一致（ICC=0.91）；75名参与者中71%对聊天机器人表示更高信任；总体舒适度评分为8.4，并有87.1%的用户愿意重复使用或推荐该工具。

Conclusion: 基于语音的LLM聊天机器人是一种可行并可扩展的低负担抑郁筛查辅助工具，具有良好的用户接受度和信任度。

Abstract: Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively
screen depression but lack interactivity and adaptability. We developed
HopeBot, a chatbot powered by a large language model (LLM) that administers the
PHQ-9 using retrieval-augmented generation and real-time clarification. In a
within-subject study, 132 adults in the United Kingdom and China completed both
self-administered and chatbot versions. Scores demonstrated strong agreement
(ICC = 0.91; 45% identical). Among 75 participants providing comparative
feedback, 71% reported greater trust in the chatbot, highlighting clearer
structure, interpretive guidance, and a supportive tone. Mean ratings (0-10)
were 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics,
and 7.4 for recommendation helpfulness; the latter varied significantly by
employment status and prior mental-health service use (p < 0.05). Overall,
87.1% expressed willingness to reuse or recommend HopeBot. These findings
demonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden
adjuncts for routine depression screening.

</details>


### [171] [CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL Generation](https://arxiv.org/abs/2507.06013)
*Kushal Gajjar,Harshit Sikchi,Arpit Singh Gautam,Marc Hammons,Saurabh Jha*

Main category: cs.AI

TL;DR: 研究介绍了CogniSQL-R1-Zero，这是一种结合强化学习框架以高效生成准确SQL的模型，显著提升Text-to-SQL任务执行精度。


<details>
  <summary>Details</summary>
Motivation: 存在难以生成复杂、可执行SQL的问题，现有大语言模型虽然提升了语言流畅性，但在复杂查询上仍表现不佳。

Method: 采用强化学习框架，通过一个轻量级奖励信号，包括执行正确性和格式规范合规性，避免复杂的中间监督和奖励设计，直接对最终任务目标进行优化。

Result: CogniSQL-R1-Zero模型在Text2SQL基准上的执行精度达到SOTA水平，超越了多种监督和指令调优基线，即便只基于较小规模的模型和算力。

Conclusion: 该研究展示了基于强化学习的Text-to-SQL方法在效率和可扩展性上的优势，并为领域研究提供了新的数据集支持。

Abstract: Translating natural language into SQL (Text-to-SQL) remains a core challenge
at the intersection of language understanding and structured data access.
Although large language models (LLMs) have improved fluency, generating correct
and executable SQL, especially for complex queries, continues to be
challenging. We introduce CogniSQL-R1-Zero, a reinforcement learning (RL)
framework and model that produces accurate SQL using a lightweight reward
signal based on execution correctness and format-tag compliance. By avoiding
intermediate supervision, hybrid pipelines and complex reward shaping, our
method encourages stable learning and stronger alignment with the ultimate task
objective-producing executable programs. CogniSQL-R1-Zero achieves
state-of-the-art execution accuracy on Text2SQL benchmark; BIRD bench,
outperforming prior supervised and instruction-tuned baselines including SFT
CodeS-7B, DeepSeek-Coder 236B, and Mistral 123B-despite being trained on a
significantly smaller 7B backbone. This result underscores the scalability and
efficiency of our RL-based approach when trained on just four NVIDIA A100 GPUs
(40 GB VRAM each). To support further research in efficient and interpretable
Text-to-SQL modeling, we release two curated datasets: (i) a collection of
5,024 reasoning traces with varying context lengths, and (ii) a
positive-sampled corpus of 36,356 corpus of weakly supervised queries, each
annotated with six semantically diverse reasoning paths. Together, these
contributions advance scalable, execution-aligned Text-to-SQL generation.

</details>


### [172] [Feature-Guided Neighbor Selection for Non-Expert Evaluation of Model Predictions](https://arxiv.org/abs/2507.06029)
*Courtney Ford,Mark T. Keane*

Main category: cs.AI

TL;DR: 引入了一种名为Feature-Guided Neighbor Selection (FGNS)的方法，用于提升AI输出的可解释性，通过选择代表类的样本实例改善用户理解。


<details>
  <summary>Details</summary>
Motivation: 许多可解释AI方法对非领域专家不够友好，难以生成清晰易懂的输出。

Method: 提出了一种后验方法FGNS，结合局部和全局特征重要性来选择能够代表类的样本，进行解释性增强。

Result: 通过用户研究，FGNS显著提高了非专家识别模型错误的能力，并提高了决策速度和准确性；定量分析显示其选择的样本更能反映类别特征。

Conclusion: FGNS有助于增强模型评估的贴近性，但仍需进一步工作来缩小解释质量与信任感之间的差距。

Abstract: Explainable AI (XAI) methods often struggle to generate clear, interpretable
outputs for users without domain expertise. We introduce Feature-Guided
Neighbor Selection (FGNS), a post hoc method that enhances interpretability by
selecting class-representative examples using both local and global feature
importance. In a user study (N = 98) evaluating Kannada script classifications,
FGNS significantly improved non-experts' ability to identify model errors while
maintaining appropriate agreement with correct predictions. Participants made
faster and more accurate decisions compared to those given traditional k-NN
explanations. Quantitative analysis shows that FGNS selects neighbors that
better reflect class characteristics rather than merely minimizing
feature-space distance, leading to more consistent selection and tighter
clustering around class prototypes. These results support FGNS as a step toward
more human-aligned model assessment, although further work is needed to address
the gap between explanation quality and perceived trust.

</details>


### [173] [On Lockean beliefs that are deductively closed and minimal change](https://arxiv.org/abs/2507.06042)
*Tommaso Flaminio,Lluis Godo,Ramón Pino Pérez,Lluis Subirana*

Main category: cs.AI

TL;DR: 本文研究了Lockean信念集，提供了两个关于封闭在经典逻辑推导下的信念集的特性描述，并提出一种最小修订方法进行概率更新。


<details>
  <summary>Details</summary>
Motivation: Lockean信念集在一些背景下（如信念变化理论）使用存在局限性，特别是不总是封闭于经典逻辑推导。本文希望解决这些局限性。

Method: 作者通过两种方式刻画封闭于经典逻辑推导的信念集，并提出了一种可实现最小修订的概率更新方法。

Result: 发现可以通过最小修订实现信念集的推理封闭，从而有效地应对新信息更新。

Conclusion: 提出的修订方法为信念变化提供了一种新工具，能够最小化对原信念集的修改，同时适应新信息。

Abstract: Within the formal setting of the Lockean thesis, an agent belief set is
defined in terms of degrees of confidence and these are described in
probabilistic terms. This approach is of established interest, notwithstanding
some limitations that make its use troublesome in some contexts, like, for
instance, in belief change theory. Precisely, Lockean belief sets are not
generally closed under (classical) logical deduction. The aim of the present
paper is twofold: on one side we provide two characterizations of those belief
sets that are closed under classical logic deduction, and on the other we
propose an approach to probabilistic update that allows us for a minimal
revision of those beliefs, i.e., a revision obtained by making the fewest
possible changes to the existing belief set while still accommodating the new
information. In particular, we show how we can deductively close a belief set
via a minimal revision.

</details>


### [174] [FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large Language Models](https://arxiv.org/abs/2507.06057)
*Bo Pang,Yalu Ouyang,Hangfei Xu,Ziqi Jia,Panpan Li,Shengzhao Wen,Lu Wang,Shiyong Li,Yanpeng Wang*

Main category: cs.AI

TL;DR: 提出了一种名为FEVO的方法，旨在通过多阶段优化框架提升大语言模型（LLMs）在金融领域的表现，并在多个基准测试中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 目前针对LLMs在金融领域的研究有限，而金融任务需要大量的领域知识。研究的目的是填补这一空白，通过提升LLMs对金融领域任务的处理能力。

Method: 研究提出FEVO框架，包括继续预训练（CPT）扩展领域知识、监督微调（SFT）引入结构化推理模式、以及通过强化学习（RL）整合金融知识与推理能力。此外，FEVO-Train数据集被用于不同阶段的训练。

Result: 通过FEVO框架训练的模型（C32B、S32B、R32B）在七个基准测试中表现优异。其中R32B在五项金融基准测试中表现优于同类更大模型以及专用模型，并显著优于只用RL训练的基线模型。

Conclusion: 研究验证了扩展金融领域知识和引入结构化推理的有效性，FEVO框架在提升LLMs金融领域任务性能方面具有显著优势。

Abstract: Advancements in reasoning for large language models (LLMs) have lead to
significant performance improvements for LLMs in various fields such as
mathematics and programming. However, research applying these advances to the
financial domain, where considerable domain-specific knowledge is necessary to
complete tasks, remains limited. To address this gap, we introduce FEVO
(Financial Evolution), a multi-stage enhancement framework developed to enhance
LLM performance in the financial domain. FEVO systemically enhances LLM
performance by using continued pre-training (CPT) to expand financial domain
knowledge, supervised fine-tuning (SFT) to instill structured, elaborate
reasoning patterns, and reinforcement learning (RL) to further integrate the
expanded financial domain knowledge with the learned structured reasoning. To
ensure effective and efficient training, we leverage frontier reasoning models
and rule-based filtering to curate FEVO-Train, high-quality datasets
specifically designed for the different post-training phases. Using our
framework, we train the FEVO series of models -- C32B, S32B, R32B -- from
Qwen2.5-32B and evaluate them on seven benchmarks to assess financial and
general capabilities, with results showing that FEVO-R32B achieves
state-of-the-art performance on five financial benchmarks against much larger
models as well as specialist models. More significantly, FEVO-R32B demonstrates
markedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct
using only RL), thus validating the effectiveness of financial domain knowledge
expansion and structured, logical reasoning distillation

</details>


### [175] [AI-Based Demand Forecasting and Load Balancing for Optimising Energy use in Healthcare Systems: A real case study](https://arxiv.org/abs/2507.06077)
*Iman Rahimi,Isha Patel*

Main category: cs.AI

TL;DR: 本文提出了一种结合LSTM、遗传算法(GA)和SHAP分析的人工智能框架，用于医疗设施的能耗管理，通过显著提高预测精度和透明度，从而提升能效和可持续性。


<details>
  <summary>Details</summary>
Motivation: 当前医疗设施的能耗需求波动大，传统能源管理方法效率低下且成本高昂，亟需更高效的能源管理解决方案。

Method: 研究设计了一种结合LSTM、遗传算法(GA)和SHAP分析的方法框架：LSTM用于复杂需求预测，GA优化模型参数和负载平衡策略，SHAP增加对预测结果的解释性。

Result: LSTM模型在预测精度上显著优于Prophet和ARIMA模型（如LSTM在MAE和RMSE上的性能大幅领先），GA优化了能耗平衡能力，SHAP解释了不同特征对预测的影响。

Conclusion: 该方法显著提高了医疗设施的能耗预测准确性和管理效率，为AI在医疗能源管理中的应用打下了基础，并具备高效性、可扩展性和可靠性。

Abstract: This paper tackles the urgent need for efficient energy management in
healthcare facilities, where fluctuating demands challenge operational
efficiency and sustainability. Traditional methods often prove inadequate,
causing inefficiencies and higher costs. To address this, the study presents an
AI-based framework combining Long Short-Term Memory (LSTM), genetic algorithm
(GA), and SHAP (Shapley Additive Explanations), specifically designed for
healthcare energy management. Although LSTM is widely used for time-series
forecasting, its application in healthcare energy prediction remains
underexplored. The results reveal that LSTM significantly outperforms ARIMA and
Prophet models in forecasting complex, non-linear demand patterns. LSTM
achieves a Mean Absolute Error (MAE) of 21.69 and Root Mean Square Error (RMSE)
of 29.96, far better than Prophet (MAE: 59.78, RMSE: 81.22) and ARIMA (MAE:
87.73, RMSE: 125.22), demonstrating superior performance. The genetic algorithm
is applied to optimize model parameters and improve load balancing strategies,
enabling adaptive responses to real-time energy fluctuations. SHAP analysis
further enhances model transparency by explaining the influence of different
features on predictions, fostering trust in decision-making processes. This
integrated LSTM-GA-SHAP approach offers a robust solution for improving
forecasting accuracy, boosting energy efficiency, and advancing sustainability
in healthcare facilities. Future research may explore real-time deployment and
hybridization with reinforcement learning for continuous optimization. Overall,
the study establishes a solid foundation for using AI in healthcare energy
management, highlighting its scalability, efficiency, and resilience potential.

</details>


### [176] [OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety](https://arxiv.org/abs/2507.06134)
*Sanidhya Vijayvargiya,Aditya Bharat Soni,Xuhui Zhou,Zora Zhiruo Wang,Nouha Dziri,Graham Neubig,Maarten Sap*

Main category: cs.AI

TL;DR: 研究提出了OpenAgentSafety框架，用于全面评估AI代理在多种现实工具和复杂任务中的安全性，发现现有模型在许多任务中存在严重的安全隐患。


<details>
  <summary>Details</summary>
Motivation: AI代理因能解决复杂的日常任务而得以实际应用，但其潜在的不安全行为需要严格评估。目前的研究方法通常基于模拟环境或狭窄任务领域，无法全面覆盖潜在的安全问题。

Method: 提出一种名为OpenAgentSafety的新框架，通过与真实工具（如Web浏览器、代码执行环境、文件系统等）的交互，评估AI代理在八种关键风险类别中的行为。框架涵盖超过350个多轮次、多用户任务，并结合规则分析和LLM评估检测行为中的风险。

Result: 在对五种主流语言模型的测试中，发现Claude-Sonnet-3.7的危险任务比例高达51.2%，而o3-mini则为72.7%，表明现有AI代理在安全性上存在显著漏洞。

Conclusion: 需要更强大的安全机制以降低AI代理部署时的风险，OpenAgentSafety框架为后续研究和改进提供了一个覆盖更广泛应用场景的新工具。

Abstract: Recent advances in AI agents capable of solving complex, everyday tasks, from
scheduling to customer service, have enabled deployment in real-world settings,
but their possibilities for unsafe behavior demands rigorous evaluation. While
prior benchmarks have attempted to assess agent safety, most fall short by
relying on simulated environments, narrow task domains, or unrealistic tool
abstractions. We introduce OpenAgentSafety, a comprehensive and modular
framework for evaluating agent behavior across eight critical risk categories.
Unlike prior work, our framework evaluates agents that interact with real
tools, including web browsers, code execution environments, file systems, bash
shells, and messaging platforms; and supports over 350 multi-turn, multi-user
tasks spanning both benign and adversarial user intents. OpenAgentSafety is
designed for extensibility, allowing researchers to add tools, tasks, websites,
and adversarial strategies with minimal effort. It combines rule-based analysis
with LLM-as-judge assessments to detect both overt and subtle unsafe behaviors.
Empirical analysis of five prominent LLMs in agentic scenarios reveals unsafe
behavior in 51.2% of safety-vulnerable tasks with Claude-Sonnet-3.7, to 72.7%
with o3-mini, highlighting critical safety vulnerabilities and the need for
stronger safeguards before real-world deployment.

</details>


### [177] [The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield Strong Gains](https://arxiv.org/abs/2507.06187)
*Scott Geng,Hamish Ivison,Chun-Liang Li,Maarten Sap,Jerry Li,Ranjay Krishna,Pang Wei Koh*

Main category: cs.AI

TL;DR: 该研究发现，通过弱数据点间的相对质量差异（delta）进行偏好调整，可以驱动模型学习，即使单独使用弱数据效果较差。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型的改进通常依赖于高质量数据，但高质量强监督数据稀缺，需探索弱监督数据的潜力。

Method: 提出‘delta学习假设’，利用两组弱模型的相对质量差异生成的偏好数据，进行偏好微调和后训练，并通过理论分析和实验验证其有效性。

Result: 在11个标准基准测试中，与依赖更强监督（GPT-4o）的Tulu 3性能相当，但训练更简单和经济。

Conclusion: 表明即使弱数据通常被认为不足以单独驱动学习，其相对质量差异对提升更强学生模型性能仍然有显著作用。

Abstract: Improvements in language models are often driven by improving the quality of
the data we train them on, which can be limiting when strong supervision is
scarce. In this work, we show that paired preference data consisting of
individually weak data points can enable gains beyond the strength of each
individual data point. We formulate the delta learning hypothesis to explain
this phenomenon, positing that the relative quality delta between points
suffices to drive learning via preference tuning--even when supervised
finetuning on the weak data hurts. We validate our hypothesis in controlled
experiments and at scale, where we post-train 8B models on preference data
generated by pairing a small 3B model's responses with outputs from an even
smaller 1.5B model to create a meaningful delta. Strikingly, on a standard
11-benchmark evaluation suite (MATH, MMLU, etc.), our simple recipe matches the
performance of Tulu 3, a state-of-the-art open model tuned from the same base
model while relying on much stronger supervisors (e.g., GPT-4o). Thus, delta
learning enables simpler and cheaper open recipes for state-of-the-art
post-training. To better understand delta learning, we prove in logistic
regression that the performance gap between two weak teacher models provides
useful signal for improving a stronger student. Overall, our work shows that
models can learn surprisingly well from paired data that might typically be
considered weak.

</details>


### [178] [Identifiability in Causal Abstractions: A Hierarchy of Criteria](https://arxiv.org/abs/2507.06213)
*Clément Yvernes,Emilie Devijver,Marianne Clausel,Eric Gaussier*

Main category: cs.AI

TL;DR: 本文探讨在因果知识不完全的情况下如何通过因果抽象来研究因果查询的可识别性，并提出了一种分层的准则体系以分析不同水平的因果知识下的情况。


<details>
  <summary>Details</summary>
Motivation: 现实中因果图通常难以完全知道，尤其在高维或复杂场景中。本文试图通过因果抽象简化因果信息以解决这一问题。

Method: 将因果抽象形式化为因果图集合，在该集合上探索几种可识别性准则，并构建一个层次化框架以组织和理解这些准则之间的关系。

Result: 提出了一个分层可识别性准则框架，展示了不同因果知识水平下可识别性的关联，并使用文献实例说明了该框架的适用性。

Conclusion: 该框架为在因果知识有限的场景下推导因果影响的可识别性提供了清晰明确的方法论支持。

Abstract: Identifying the effect of a treatment from observational data typically
requires assuming a fully specified causal diagram. However, such diagrams are
rarely known in practice, especially in complex or high-dimensional settings.
To overcome this limitation, recent works have explored the use of causal
abstractions-simplified representations that retain partial causal information.
In this paper, we consider causal abstractions formalized as collections of
causal diagrams, and focus on the identifiability of causal queries within such
collections. We introduce and formalize several identifiability criteria under
this setting. Our main contribution is to organize these criteria into a
structured hierarchy, highlighting their relationships. This hierarchical view
enables a clearer understanding of what can be identified under varying levels
of causal knowledge. We illustrate our framework through examples from the
literature and provide tools to reason about identifiability when full causal
knowledge is unavailable.

</details>


### [179] [Aligned Textual Scoring Rules](https://arxiv.org/abs/2507.06221)
*Yuxuan Lu,Yifan Wu,Jason Hartline,Michael J. Curry*

Main category: cs.AI

TL;DR: 提出了一种新的对齐评分规则（ASR），通过最小化与参考评分（如人类评分）之间的均方误差，从而在保持正确性的同时更符合人类偏好。


<details>
  <summary>Details</summary>
Motivation: 现有的正确评分规则虽然可以引导文本预测，但并不总是符合人类对文本的偏好。

Method: 设计了对齐评分规则（ASR），通过优化最小化正确评分规则与参考评分之间的均方误差。

Result: 实验显示，ASR在对齐人类偏好方面表现优于现有方法，同时保持了评分规则的正确性。

Conclusion: ASR是一种在正确性和人类偏好之间取得平衡的评分规则，表现更为理想。

Abstract: Scoring rules elicit probabilistic predictions from a strategic agent by
scoring the prediction against a ground truth state. A scoring rule is proper
if, from the agent's perspective, reporting the true belief maximizes the
expected score. With the development of language models, Wu and Hartline (2024)
proposes a reduction from textual information elicitation to the numerical
(i.e. probabilistic) information elicitation problem, which achieves provable
properness for textual elicitation. However, not all proper scoring rules are
well aligned with human preference over text. Our paper designs the Aligned
Scoring rule (ASR) for text by optimizing and minimizing the mean squared error
between a proper scoring rule and a reference score (e.g. human score). Our
experiments show that our ASR outperforms previous methods in aligning with
human preference while maintaining properness.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [180] [Rethinking Over-Smoothing in Graph Neural Networks: A Perspective from Anderson Localization](https://arxiv.org/abs/2507.05263)
*Kaichen Ouyang*

Main category: cs.LG

TL;DR: 研究提出通过分析安德森局域化现象理解图神经网络的过度平滑问题，并提出减少信息传播中的无序性以缓解这一现象。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络深层结构中节点表示过度平滑问题，提高深度模型的表现能力。

Method: 采用安德森局域化现象作为理论基础，引入参与度作为指标，研究图神经网络的过度平滑机制，并通过减少信息传播中的无序性缓解过度平滑问题。

Result: 理论分析表明，图神经网络的过度平滑可以类比为安德森局域化中的模式行为，提出了增加信息传播的有序性作为改进方法的潜力。

Conclusion: 过度平滑问题在深层图神经网络中显著，基于安德森局域化的视角有助于系统性理解相关机理，并有潜力通过减少传播无序性改善模型性能。

Abstract: Graph Neural Networks (GNNs) have shown great potential in graph data
analysis due to their powerful representation capabilities. However, as the
network depth increases, the issue of over-smoothing becomes more severe,
causing node representations to lose their distinctiveness. This paper analyzes
the mechanism of over-smoothing through the analogy to Anderson localization
and introduces participation degree as a metric to quantify this phenomenon.
Specifically, as the depth of the GNN increases, node features homogenize after
multiple layers of message passing, leading to a loss of distinctiveness,
similar to the behavior of vibration modes in disordered systems. In this
context, over-smoothing in GNNs can be understood as the expansion of
low-frequency modes (increased participation degree) and the localization of
high-frequency modes (decreased participation degree). Based on this, we
systematically reviewed the potential connection between the Anderson
localization behavior in disordered systems and the over-smoothing behavior in
Graph Neural Networks. A theoretical analysis was conducted, and we proposed
the potential of alleviating over-smoothing by reducing the disorder in
information propagation.

</details>


### [181] [Temporal Window Smoothing of Exogenous Variables for Improved Time Series Prediction](https://arxiv.org/abs/2507.05284)
*Mustafa Kamal,Niyaz Bin Hashem,Robin Krambroeckers,Nabeel Mohammed,Shafin Rahman*

Main category: cs.LG

TL;DR: 本文提出了一种通过对外生输入数据美白，提升基于Transformer的时间序列预测模型性能的方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在引入外生输入时存在冗余问题，以及由于固定回看窗口长度而难以捕获长期依赖性的问题。

Method: 基于全局统计，美白外生输入以减少冗余并增强其对长期模式和趋势的感知，然后将优化后的外生输入整合到内生输入中，无需增加回看窗口长度。

Result: 在四个基准数据集上取得了当前最优的性能，超越了11种基线模型。

Conclusion: 提出的外生输入优化方法是一种在时间序列预测中高效且鲁棒的替代方案。

Abstract: Although most transformer-based time series forecasting models primarily
depend on endogenous inputs, recent state-of-the-art approaches have
significantly improved performance by incorporating external information
through exogenous inputs. However, these methods face challenges, such as
redundancy when endogenous and exogenous inputs originate from the same source
and limited ability to capture long-term dependencies due to fixed look-back
windows. In this paper, we propose a method that whitens the exogenous input to
reduce redundancy that may persist within the data based on global statistics.
Additionally, our approach helps the exogenous input to be more aware of
patterns and trends over extended periods. By introducing this refined,
globally context-aware exogenous input to the endogenous input without
increasing the lookback window length, our approach guides the model towards
improved forecasting. Our approach achieves state-of-the-art performance in
four benchmark datasets, consistently outperforming 11 baseline models. These
results establish our method as a robust and effective alternative for using
exogenous inputs in time series forecasting.

</details>


### [182] [Dataless Neural Networks for Resource-Constrained Project Scheduling](https://arxiv.org/abs/2507.05322)
*Marc Bara*

Main category: cs.LG

TL;DR: 本论文首个将无数据神经网络方法应用于资源受限项目调度问题（RCPSP）。


<details>
  <summary>Details</summary>
Motivation: 构建首个无数据神经网络模型，用于解决资源受限项目调度问题（RCPSP）。

Method: 将离散调度约束转化为可微分的目标函数，利用平滑松弛和自动微分实现基于GPU的梯度优化，多线程并行。

Result: 目前实验测试正在PSPLIB基准实例（J30, J60, 和J120）进行。结果未公开。

Conclusion: 研究填补了无数据神经网络在RCPSP领域的空白，提供了完整的数学框架和方法论基础。

Abstract: Dataless neural networks represent a paradigm shift in applying neural
architectures to combinatorial optimization problems, eliminating the need for
training datasets by encoding problem instances directly into network
parameters. Despite the pioneering work of Alkhouri et al. (2022) demonstrating
the viability of dataless approaches for the Maximum Independent Set problem,
our comprehensive literature review reveals that no published work has extended
these methods to the Resource-Constrained Project Scheduling Problem (RCPSP).
This paper addresses this gap by presenting the first dataless neural network
approach for RCPSP, providing a complete mathematical framework that transforms
discrete scheduling constraints into differentiable objectives suitable for
gradient-based optimization. Our approach leverages smooth relaxations and
automatic differentiation to unlock GPU parallelization for project scheduling,
traditionally a domain of sequential algorithms. We detail the mathematical
formulation for both precedence and renewable resource constraints, including a
memory-efficient dense time-grid representation. Implementation and
comprehensive experiments on PSPLIB benchmark instances (J30, J60, and J120)
are currently underway, with empirical results to be reported in an updated
version of this paper.

</details>


### [183] [Compressing Deep Neural Networks Using Explainable AI](https://arxiv.org/abs/2507.05286)
*Kimia Soroush,Mohsen Raji,Behnam Ghavami*

Main category: cs.LG

TL;DR: 本论文提出了一种基于XAI的新型DNN压缩方法，通过计算权重的重要性评分以实现高效压缩，结果在减少64%模型大小的同时提升了42%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现代深度神经网络尽管性能出色，但计算和内存成本高昂；为适应资源受限的边缘设备，需开发高效的模型压缩方法。

Method: 利用Layer-wise Relevance Propagation (LRP)技术计算出DNN权重的重要性评分，剪枝移除负值或零评分的权重，并按权重评分高低应用混合精度量化保存。

Result: 实验显示，与最新XAI压缩方法相比，提出的方法减少模型大小64%，且提升准确率42%。

Conclusion: 基于XAI的压缩方法提供了一种有效的DNN优化路径，可在模型大小和性能之间找到新的平衡。

Abstract: Deep neural networks (DNNs) have demonstrated remarkable performance in many
tasks but it often comes at a high computational cost and memory usage.
Compression techniques, such as pruning and quantization, are applied to reduce
the memory footprint of DNNs and make it possible to accommodate them on
resource-constrained edge devices. Recently, explainable artificial
intelligence (XAI) methods have been introduced with the purpose of
understanding and explaining AI methods. XAI can be utilized to get to know the
inner functioning of DNNs, such as the importance of different neurons and
features in the overall performance of DNNs. In this paper, a novel DNN
compression approach using XAI is proposed to efficiently reduce the DNN model
size with negligible accuracy loss. In the proposed approach, the importance
score of DNN parameters (i.e. weights) are computed using a gradient-based XAI
technique called Layer-wise Relevance Propagation (LRP). Then, the scores are
used to compress the DNN as follows: 1) the parameters with the negative or
zero importance scores are pruned and removed from the model, 2)
mixed-precision quantization is applied to quantize the weights with
higher/lower score with higher/lower number of bits. The experimental results
show that, the proposed compression approach reduces the model size by 64%
while the accuracy is improved by 42% compared to the state-of-the-art
XAI-based compression method.

</details>


### [184] [Model-free Optical Processors using In Situ Reinforcement Learning with Proximal Policy Optimization](https://arxiv.org/abs/2507.05583)
*Yuhang Li,Shiqi Chen,Tingyu Gong,Aydogan Ozcan*

Main category: cs.LG

TL;DR: 本文提出了一种基于强化学习的无模型优化方法，用于在物理系统中直接对衍射光学处理器进行高效训练，显著提升了收敛速度和性能。


<details>
  <summary>Details</summary>
Motivation: 光学计算虽然在高速、节能信息处理方面具有潜力，但光学网络层的优化和对准因硬件缺陷及复杂物理特性的影响面临难题。现有优化方法效能低，且对系统建模依赖强。

Method: 作者提出一种基于近端策略优化（PPO）的强化学习方法，无需对物理系统的建模，直接在实验系统中进行高效数据利用和稳定策略更新。

Result: 通过实验证明该方法可成功完成目标能量聚焦、全息图像生成、像差校正和光学图像分类等多种任务，且相比传统方法表现更优、收敛速度更快。

Conclusion: 该方法无须先验系统知识，显著提升了实验约束条件下的训练效率和精度，为涉及复杂动态反馈的光学及物理系统提供了一种可扩展的优化框架。

Abstract: Optical computing holds promise for high-speed, energy-efficient information
processing, with diffractive optical networks emerging as a flexible platform
for implementing task-specific transformations. A challenge, however, is the
effective optimization and alignment of the diffractive layers, which is
hindered by the difficulty of accurately modeling physical systems with their
inherent hardware imperfections, noise, and misalignments. While existing in
situ optimization methods offer the advantage of direct training on the
physical system without explicit system modeling, they are often limited by
slow convergence and unstable performance due to inefficient use of limited
measurement data. Here, we introduce a model-free reinforcement learning
approach utilizing Proximal Policy Optimization (PPO) for the in situ training
of diffractive optical processors. PPO efficiently reuses in situ measurement
data and constrains policy updates to ensure more stable and faster
convergence. We experimentally validated our method across a range of in situ
learning tasks, including targeted energy focusing through a random diffuser,
holographic image generation, aberration correction, and optical image
classification, demonstrating in each task better convergence and performance.
Our strategy operates directly on the physical system and naturally accounts
for unknown real-world imperfections, eliminating the need for prior system
knowledge or modeling. By enabling faster and more accurate training under
realistic experimental constraints, this in situ reinforcement learning
approach could offer a scalable framework for various optical and physical
systems governed by complex, feedback-driven dynamics.

</details>


### [185] [Physics-Informed Graph Neural Networks to Reconstruct Local Fields Considering Finite Strain Hyperelasticity](https://arxiv.org/abs/2507.05291)
*Manuel Ricardo Guevara Garban,Yves Chemisky,Étienne Prulière,Michaël Clément*

Main category: cs.LG

TL;DR: 论文提出P-DivGNN框架，通过物理约束的图神经网络重建微观尺度的局部应力场，适用于多尺度情境并显著加速非线性超弹性计算。


<details>
  <summary>Details</summary>
Motivation: 开发能够在保持精度的同时显著加速应力场预测的模型，特别是针对裂缝分析和局部疲劳准则等应用。

Method: 结合消息传递的图神经网络和周期性微结构图表示，并通过物理约束训练模型以维持局部应力场平衡和边界条件。

Result: 在非线性超弹性的情境下展示显著的计算加速效果，并验证了模型在不同几何形状下的普适性。

Conclusion: P-DivGNN框架在预测局部应力场时兼备精度和计算效率，尤其适用于大规模应用场景。

Abstract: We propose a physics-informed machine learning framework called P-DivGNN to
reconstruct local stress fields at the micro-scale, in the context of
multi-scale simulation given a periodic micro-structure mesh and mean,
macro-scale, stress values. This method is based in representing a periodic
micro-structure as a graph, combined with a message passing graph neural
network. We are able to retrieve local stress field distributions, providing
average stress values produced by a mean field reduced order model (ROM) or
Finite Element (FE) simulation at the macro-scale. The prediction of local
stress fields are of utmost importance considering fracture analysis or the
definition of local fatigue criteria. Our model incorporates physical
constraints during training to constraint local stress field equilibrium state
and employs a periodic graph representation to enforce periodic boundary
conditions. The benefits of the proposed physics-informed GNN are evaluated
considering linear and non linear hyperelastic responses applied to varying
geometries. In the non-linear hyperelastic case, the proposed method achieves
significant computational speed-ups compared to FE simulation, making it
particularly attractive for large-scale applications.

</details>


### [186] [Neural Velocity for hyperparameter tuning](https://arxiv.org/abs/2507.05309)
*Gianluca Dalmasso,Andrea Bragagnolo,Enzo Tartaglione,Attilio Fiandrotti,Marco Grangetto*

Main category: cs.LG

TL;DR: NeVe方法通过“神经速度”调整学习率和定义停止标准，避免过多依赖验证集。


<details>
  <summary>Details</summary>
Motivation: 减少对验证集的依赖，优化超参数调整，提高训练效率。

Method: 引入“神经速度”，通过网络噪声前传采样该值，动态调整学习率和停止标准。

Result: 验证神经速度作为关键指标，展示其在网络训练优化中的潜力。

Conclusion: 提出一种减少对验证集依赖，提高训练高效性的超参数调整方法，值得进一步研究。

Abstract: Hyperparameter tuning, such as learning rate decay and defining a stopping
criterion, often relies on monitoring the validation loss. This paper presents
NeVe, a dynamic training approach that adjusts the learning rate and defines
the stop criterion based on the novel notion of "neural velocity". The neural
velocity measures the rate of change of each neuron's transfer function and is
an indicator of model convergence: sampling neural velocity can be performed
even by forwarding noise in the network, reducing the need for a held-out
dataset. Our findings show the potential of neural velocity as a key metric for
optimizing neural network training efficiently

</details>


### [187] [Conditional Graph Neural Network for Predicting Soft Tissue Deformation and Forces](https://arxiv.org/abs/2507.05315)
*Madina Kojanazarova,Florentin Bieder,Robin Sandkühler,Philippe C. Cattin*

Main category: cs.LG

TL;DR: 提出一种基于条件图神经网络（cGNN）的数据驱动模型，用于预测软组织的变形与交互力，取得了高精度预测结果。


<details>
  <summary>Details</summary>
Motivation: 当前软组织模拟需要解决高变形性带来的挑战，包括分割、网格化、刚度属性估计及精确的力反馈等。

Method: 采用条件图神经网络模型（cGNN），通过表面点与施加力的位置预测软组织的变形及受力，并使用迁移学习结合模拟数据与实验数据进行训练，提升模型泛化能力。

Result: 模型能够在30mm变形范围内实现0.35±0.03mm的距离误差和7.5N力范围内实现0.37±0.05N的绝对误差预测。

Conclusion: 该方法为虚拟环境中软组织模拟的复杂问题提供了准确高效的解决方案，可用于医疗模拟及其他需要软组织模拟的领域。

Abstract: Soft tissue simulation in virtual environments is becoming increasingly
important for medical applications. However, the high deformability of soft
tissue poses significant challenges. Existing methods rely on segmentation,
meshing and estimation of stiffness properties of tissues. In addition, the
integration of haptic feedback requires precise force estimation to enable a
more immersive experience. We introduce a novel data-driven model, a
conditional graph neural network (cGNN) to tackle this complexity. Our model
takes surface points and the location of applied forces, and is specifically
designed to predict the deformation of the points and the forces exerted on
them. We trained our model on experimentally collected surface tracking data of
a soft tissue phantom and used transfer learning to overcome the data scarcity
by initially training it with mass-spring simulations and fine-tuning it with
the experimental data. This approach improves the generalisation capability of
the model and enables accurate predictions of tissue deformations and
corresponding interaction forces. The results demonstrate that the model can
predict deformations with a distance error of 0.35$\pm$0.03 mm for deformations
up to 30 mm and the force with an absolute error of 0.37$\pm$0.05 N for forces
up to 7.5 N. Our data-driven approach presents a promising solution to the
intricate challenge of simulating soft tissues within virtual environments.
Beyond its applicability in medical simulations, this approach holds the
potential to benefit various fields where realistic soft tissue simulations are
required.

</details>


### [188] [Going Beyond Heuristics by Imposing Policy Improvement as a Constraint](https://arxiv.org/abs/2507.05328)
*Chi-Chang Lee,Zhang-Wei Hong,Pulkit Agrawal*

Main category: cs.LG

TL;DR: 提出了一种新方法，HEPO，在增强 RL 时通过改进策略优化来有效利用启发式奖励，同时避免传统方法中的奖励黑客问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决在强化学习中平衡任务奖励和启发式奖励所需的高成本和资源浪费问题，以及传统方法中策略不改进的问题。

Method: 引入了一种名为HEPO的框架，不依赖最优启发式设计，聚焦于策略改进。

Result: HEPO在标准基准测试中性能优越，同时在由非专家设计的粗略启发式奖励环境下也表现出色。

Conclusion: HEPO降低了奖励设计中的人工成本，是一种即插即用的优化方法，适用于增强启发式奖励的强化学习。

Abstract: In many reinforcement learning (RL) applications, augmenting the task rewards
with heuristic rewards that encode human priors about how a task should be
solved is crucial for achieving desirable performance. However, because such
heuristics are usually not optimal, much human effort and computational
resources are wasted in carefully balancing tasks and heuristic rewards.
Theoretically rigorous ways of incorporating heuristics rely on the idea of
\textit{policy invariance}, which guarantees that the performance of a policy
obtained by maximizing heuristic rewards is the same as the optimal policy with
respect to the task reward. However, in practice, policy invariance doesn't
result in policy improvement, and such methods are known to empirically perform
poorly. We propose a new paradigm to mitigate reward hacking and effectively
use heuristics based on the practical goal of maximizing policy improvement
instead of policy improvement. Our framework, Heuristic Enhanced Policy
Optimization (HEPO), effectively leverages heuristics while avoiding the
pitfall of prior methods for mitigating reward hacking. HEPO achieves superior
performance on standard benchmarks with well-engineered reward functions. More
surprisingly, HEPO allows policy optimization to achieve good performance even
when heuristics are not well-engineered and designed by non-expert humans,
showcasing HEPO's ability to reduce human effort in reward design. % HEPO is a
plug-and-play optimization method for leveraging heuristics in reinforcement
learning. Code is available at https://github.com/Improbable-AI/hepo.

</details>


### [189] [Causal Foundation Models: Disentangling Physics from Instrument Properties](https://arxiv.org/abs/2507.05333)
*Jeroen Audenaert,Daniel Muthukrishna,Paul F. Gregory,David W. Hogg,V. Ashley Villar*

Main category: cs.LG

TL;DR: 论文提出一种新的基础模型，旨在解决时间序列数据中测量工具和真实物理现象交织的问题，通过双编码器结构结合对比学习，显著改善了模型泛化能力，特别是在低数据场景。


<details>
  <summary>Details</summary>
Motivation: 传统基础模型在面临异质性或多测量设备环境时，因无法有效分离物理信号与测量工具的影响，导致泛化能力受限。

Method: 基于因果关系提出双编码器架构，通过对比学习利用三组观测数据（三元组），分别学习物理信号与测量工具的独立潜表示。

Result: 实验表明，在模拟的天文时间序列任务中（如NASA TESS数据），该方法在下游预测任务中显著优于传统单潜空间模型，尤其在低数据场景中表现尤为突出。

Conclusion: 研究证明，编码因果结构对于时间序列数据的表征学习至关重要，新模型支持关键的基础模型能力，如小样本泛化与高效适应性。

Abstract: Foundation models for structured time series data must contend with a
fundamental challenge: observations often conflate the true underlying physical
phenomena with systematic distortions introduced by measurement instruments.
This entanglement limits model generalization, especially in heterogeneous or
multi-instrument settings. We present a causally-motivated foundation model
that explicitly disentangles physical and instrumental factors using a
dual-encoder architecture trained with structured contrastive learning.
Leveraging naturally occurring observational triplets (i.e., where the same
target is measured under varying conditions, and distinct targets are measured
under shared conditions) our model learns separate latent representations for
the underlying physical signal and instrument effects. Evaluated on simulated
astronomical time series designed to resemble the complexity of variable stars
observed by missions like NASA's Transiting Exoplanet Survey Satellite (TESS),
our method significantly outperforms traditional single-latent space foundation
models on downstream prediction tasks, particularly in low-data regimes. These
results demonstrate that our model supports key capabilities of foundation
models, including few-shot generalization and efficient adaptation, and
highlight the importance of encoding causal structure into representation
learning for structured data.

</details>


### [190] [Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training](https://arxiv.org/abs/2507.05386)
*Song Lai,Haohan Zhao,Rong Feng,Changyi Ma,Wenzhuo Liu,Hongbo Zhao,Xi Lin,Dong Yi,Min Xie,Qingfu Zhang,Hongbin Liu,Gaofeng Meng,Fei Zhu*

Main category: cs.LG

TL;DR: 本文比较了两种续训范式（监督微调和强化微调）对基础模型在持续训练中知识保留的影响，发现强化微调能有效减轻遗忘问题并提升通用性能。


<details>
  <summary>Details</summary>
Motivation: 探讨续训范式在任务迁移和知识保留中的基础作用，特别是在多模态大语言模型的连续训练场景中。

Method: 比较了监督微调(SFT)和强化微调(RFT)在七个多模态任务上的表现，并提出了一种基于rollout的实例过滤算法以提高RFT的稳定性和效率。

Result: 发现SFT导致遗忘问题且损害模型通用能力，而RFT不仅有效保留知识，还可提升通用基准性能；实验表明，RFT隐含的正则化作用是减轻遗忘的关键。

Conclusion: RFT在密集训练环境下是更优的续训范式，兼顾任务表现和通用能力保留，具备实际应用潜力。

Abstract: Continual post-training (CPT) is a popular and effective technique for
adapting foundation models like multimodal large language models to specific
and ever-evolving downstream tasks. While existing research has primarily
concentrated on methods like data replay, model expansion, or parameter
regularization, the fundamental role of the learning paradigm within CPT
remains largely unexplored. This paper presents a comparative analysis of two
core post-training paradigms: supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT), investigating their respective impacts on knowledge
retention during CPT. Our experiments are conducted on a benchmark comprising
seven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base
model for continual post-training. The investigation yields two significant
findings: (1) When continuously learning on downstream tasks, SFT leads to
catastrophic forgetting of previously learned tasks. In contrast, RFT
inherently preserves prior knowledge and achieve performance comparable to
multi-task training. (2) RFT successfully protects and even enhances the
model's general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro).
Conversely, SFT degrades general model capabilities severely. Further analysis
shows that explicit mechanisms, such as KL penalty and chain-of-thought
reasoning, are not the primary factors. Instead, we find that the implicit
regularization inherent to RFT is a key factor in mitigating forgetting.
Finally, we propose a rollout-based instance filtering algorithm to improve the
stability and efficiency of RFT. Our comprehensive study demonstrates the
superiority of RFT as a robust paradigm for continual post-training.

</details>


### [191] [Probabilistically Tightened Linear Relaxation-based Perturbation Analysis for Neural Network Verification](https://arxiv.org/abs/2507.05405)
*Luca Marzari,Ferdinando Cicalese,Alessandro Farinelli*

Main category: cs.LG

TL;DR: 提出了以LiRPA为基础的概率抽样框架PT-LiRPA，用于神经网络验证，显著提升了效率和验证效果。


<details>
  <summary>Details</summary>
Motivation: 神经网络的形式化验证存在计算成本高和效率低的挑战，需要寻找更有效的方法。

Method: 结合LiRPA的上界处理技术与采样方法，估计并收紧神经网络的中间可达集合的线性界限。

Result: PT-LiRPA比现有方法验证鲁棒性提升至多3.31倍，在99%置信度下解决了现有方法失败的挑战性案例问题。

Conclusion: PT-LiRPA显著改进了神经网络验证的效率和可靠性，为形式化验证领域提供了重要突破。

Abstract: We present $\textbf{P}$robabilistically $\textbf{T}$ightened
$\textbf{Li}$near $\textbf{R}$elaxation-based $\textbf{P}$erturbation
$\textbf{A}$nalysis ($\texttt{PT-LiRPA}$), a novel framework that combines
over-approximation techniques from LiRPA-based approaches with a sampling-based
method to compute tight intermediate reachable sets. In detail, we show that
with negligible computational overhead, $\texttt{PT-LiRPA}$ exploiting the
estimated reachable sets, significantly tightens the lower and upper linear
bounds of a neural network's output, reducing the computational cost of formal
verification tools while providing probabilistic guarantees on verification
soundness. Extensive experiments on standard formal verification benchmarks,
including the International Verification of Neural Networks Competition, show
that our $\texttt{PT-LiRPA}$-based verifier improves robustness certificates by
up to 3.31X and 2.26X compared to related work. Importantly, our probabilistic
approach results in a valuable solution for challenging competition entries
where state-of-the-art formal verification methods fail, allowing us to provide
answers with high confidence (i.e., at least 99%).

</details>


### [192] [AXLearn: Modular Large Model Training on Heterogeneous Infrastructure](https://arxiv.org/abs/2507.05411)
*Mark Lee,Tom Gunter,Chang Lan,John Peebles,Hanzhi Zhou,Kelvin Zou,Sneha Bangalore,Chung-Cheng Chiu,Nan Du,Xianzhi Du,Philipp Dufter,Ruixuan Hou,Haoshuo Huang,Dongseong Hwang,Xiang Kong,Jinhao Lei,Tao Lei,Meng Li,Li Li,Jiarui Lu,Zhiyun Lu,Yiping Ma,David Qiu,Vivek Rathod,Senyu Tong,Zhucheng Tu,Jianyu Wang,Yongqiang Wang,Zirui Wang,Floris Weers,Sam Wiseman,Guoli Yin,Bowen Zhang,Xiyou Zhou,Danyang Zhuo,Cheng Leong,Ruoming Pang*

Main category: cs.LG

TL;DR: AXLearn 是一个高度模块化且支持异构硬件基础设施的大规模深度学习训练系统，具有高性能和较低代码复杂性特点。


<details>
  <summary>Details</summary>
Motivation: 旨在设计一个高度模块化和高扩展性的深度学习系统，用于支持异构硬件的深度学习模型训练。

Method: 通过引入严格的模块化内部接口、采用 LoC 复杂性的方法量化模块化，并通过高级别接口支持快速模型开发和实验。

Result: AXLearn 可以以较低的代码复杂性支持大规模架构扩展，同时在性能上与其他最先进的训练系统表现相当。

Conclusion: AXLearn 实现了模块化、高扩展性和高性能，在异构计算环境中提供高效的深度学习支持。

Abstract: We design and implement AXLearn, a production deep learning system that
facilitates scalable and high-performance training of large deep learning
models. Compared to other state-of-the-art deep learning systems, AXLearn has a
unique focus on modularity and support for heterogeneous hardware
infrastructure. AXLearn's internal interfaces between software components
follow strict encapsulation, allowing different components to be assembled to
facilitate rapid model development and experimentation on heterogeneous compute
infrastructure. We introduce a novel method of quantifying modularity via
Lines-of-Code (LoC)-complexity, which demonstrates how our system maintains
constant complexity as we scale the components in the system, compared to
linear or quadratic complexity in other systems. This allows integrating
features such as Rotary Position Embeddings (RoPE) into AXLearn across hundred
of modules with just 10 lines of code, compared to hundreds as required in
other systems. At the same time, AXLearn maintains equivalent performance
compared to state-of-the-art training systems. Finally, we share our experience
in the development and operation of AXLearn.

</details>


### [193] [Incorporating Interventional Independence Improves Robustness against Interventional Distribution Shift](https://arxiv.org/abs/2507.05412)
*Gautam Sreekumar,Vishnu Naresh Boddeti*

Main category: cs.LG

TL;DR: 该论文针对学习具有因果关系的潜变量鲁棒判别表示的问题，提出了一种新算法RepLIn，用于在干预训练期间显式强制统计独立性。实验表明，RepLIn在抗分布变化表现表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分利用因果关系信息，导致模型在观察性数据和干预性数据上表现差异较大，尤其在干预数据较少时。本研究旨在证明这种性能差距与因果独立条件的关系，并制定改进方法。

Method: 1. 确定性能差距与因果独立关系的强关联性；2. 推导干预数据比例与错误率的关系；3. 提出RepLIn算法，通过显式约束干预条件下的独立性来改进表示学习。

Result: RepLIn通过针对因果图表中的干预机制进行设计，证明了其在合成数据及真实图像和文本数据（如面部属性分类与毒性检测任务）上的可伸缩性与优越性。

Conclusion: 该研究展示了将干预独立性显式加入到表征学习模型中的重要性及其有效性，提高了处理分布变化的鲁棒性，同时表明RepLIn在处理连续和离散变量时均具有良好适用性。

Abstract: We consider the problem of learning robust discriminative representations of
causally-related latent variables. In addition to observational data, the
training dataset also includes interventional data obtained through targeted
interventions on some of these latent variables to learn representations robust
against the resulting interventional distribution shifts. Existing approaches
treat interventional data like observational data, even when the underlying
causal model is known, and ignore the independence relations that arise from
these interventions. Since these approaches do not fully exploit the causal
relational information resulting from interventions, they learn representations
that produce large disparities in predictive performance on observational and
interventional data, which worsens when the number of interventional training
samples is limited. In this paper, (1) we first identify a strong correlation
between this performance disparity and adherence of the representations to the
independence conditions induced by the interventional causal model. (2) For
linear models, we derive sufficient conditions on the proportion of
interventional data in the training dataset, for which enforcing interventional
independence between representations corresponding to the intervened node and
its non-descendants lowers the error on interventional data. Combining these
insights, (3) we propose RepLIn, a training algorithm to explicitly enforce
this statistical independence during interventions. We demonstrate the utility
of RepLIn on a synthetic dataset and on real image and text datasets on facial
attribute classification and toxicity detection, respectively. Our experiments
show that RepLIn is scalable with the number of nodes in the causal graph and
is suitable to improve the robust representations against interventional
distribution shifts of both continuous and discrete latent variables.

</details>


### [194] [EmissionNet: Air Quality Pollution Forecasting for Agriculture](https://arxiv.org/abs/2507.05416)
*Prady Saligram,Tanvir Bhathal*

Main category: cs.LG

TL;DR: 研究开发了两种深度学习架构,用于预测农业N$_2$O排放。


<details>
  <summary>Details</summary>
Motivation: 农业排放对环境和健康影响显著，但传统模型难以捕捉复杂非线性污染物交互，因此需要新的预测方法。

Method: 提出了EmissionNet (ENV) 和 EmissionNet-Transformer (ENT) 两种基于卷积与Transformer的深度学习模型，用于分时空解析高分辨率排放数据。

Result: 展示了新模型如何更有效地捕捉排放的空间与时间依赖性。

Conclusion: 深度学习方法对改进农业污染物预测有重要潜力。

Abstract: Air pollution from agricultural emissions is a significant yet often
overlooked contributor to environmental and public health challenges.
Traditional air quality forecasting models rely on physics-based approaches,
which struggle to capture complex, nonlinear pollutant interactions. In this
work, we explore forecasting N$_2$O agricultural emissions through evaluating
popular architectures, and proposing two novel deep learning architectures,
EmissionNet (ENV) and EmissionNet-Transformer (ENT). These models leverage
convolutional and transformer-based architectures to extract spatial-temporal
dependencies from high-resolution emissions data

</details>


### [195] [Adversarial Machine Learning Attacks on Financial Reporting via Maximum Violated Multi-Objective Attack](https://arxiv.org/abs/2507.05441)
*Edward Raff,Karen Kukla,Michel Benaroch,Joseph Comprix*

Main category: cs.LG

TL;DR: 本文提出了一种名为MVMO的新方法，用于检测企业通过操纵财务报告进行的欺诈行为，其准确率较高。


<details>
  <summary>Details</summary>
Motivation: 企业出于个人利益，有动机操纵财务数据以掩盖财务困境。现有的攻击模型由于目标间的反相关性，难以成功探测这些欺诈行为，因此需要提出改进方法。

Method: 提出了一种最大违目标（MVMO）攻击模型，通过调整攻击者的搜索方向，在多目标冲突情况下寻找到更高效的攻击路径。

Result: 通过实验表明，MVMO方法可以使公司在50%的情况下将盈利虚增100-200%，同时欺诈评分降低15%。

Conclusion: 与法律专家和会计师合作，这一方法模拟了真实的欺诈行为情况，展示了欺诈风险的严重性并提供研究支持。

Abstract: Bad actors, primarily distressed firms, have the incentive and desire to
manipulate their financial reports to hide their distress and derive personal
gains. As attackers, these firms are motivated by potentially millions of
dollars and the availability of many publicly disclosed and used financial
modeling frameworks. Existing attack methods do not work on this data due to
anti-correlated objectives that must both be satisfied for the attacker to
succeed. We introduce Maximum Violated Multi-Objective (MVMO) attacks that
adapt the attacker's search direction to find $20\times$ more satisfying
attacks compared to standard attacks. The result is that in $\approx50\%$ of
cases, a company could inflate their earnings by 100-200%, while simultaneously
reducing their fraud scores by 15%. By working with lawyers and professional
accountants, we ensure our threat model is realistic to how such frauds are
performed in practice.

</details>


### [196] [2048: Reinforcement Learning in a Delayed Reward Environment](https://arxiv.org/abs/2507.05465)
*Prady Saligram,Tanvir Bhathal,Robby Manihani*

Main category: cs.LG

TL;DR: 本文提出一种统一的分布式多步RL框架，通过方法创新显著提高2048游戏得分，展现了在稀疏奖励场景中提升性能的潜力。


<details>
  <summary>Details</summary>
Motivation: 强化学习常因延迟且稀疏的奖励而难以评估长远动作价值。2048游戏作为代表性挑战，为此研究设计了专门优化长时间表现的新框架。

Method: 提出统一分布式多步强化学习框架，开发并比较四种Agent（DQN、PPO、QR-DQN及创新H-DQN），结合多种机制如分布式学习及优先级回放等。

Result: H-DQN显著超越其他方法，取得了最高分18.21K，推动到2048 tile；在扩展时达到了41.828K和4096 tile。

Conclusion: 分布式多步目标在稀疏奖励领域显著提升效果，为模型规划和课程学习等研究方向提供了潜力。

Abstract: Delayed and sparse rewards present a fundamental obstacle for
reinforcement-learning (RL) agents, which struggle to assign credit for actions
whose benefits emerge many steps later. The sliding-tile game 2048 epitomizes
this challenge: although frequent small score changes yield immediate feedback,
they often mislead agents into locally optimal but globally suboptimal
strategies. In this work, we introduce a unified, distributional multi-step RL
framework designed to directly optimize long-horizon performance. Using the
open source Gym-2048 environment we develop and compare four agent variants:
standard DQN, PPO, QR-DQN (Quantile Regression DQN), and a novel Horizon-DQN
(H-DQN) that integrates distributional learning, dueling architectures, noisy
networks, prioritized replay, and more. Empirical evaluation reveals a clear
hierarchy in effectiveness: max episode scores improve from 3.988K (DQN) to
5.756K (PPO), 8.66K (QR-DQN), and 18.21K (H-DQN), with H-DQN reaching the 2048
tile. Upon scaling H-DQN it reaches a max score 41.828K and a 4096 tile. These
results demonstrate that distributional, multi-step targets substantially
enhance performance in sparse-reward domains, and they suggest promising
avenues for further gains through model-based planning and curriculum learning.

</details>


### [197] [Epistemically-guided forward-backward exploration](https://arxiv.org/abs/2507.05477)
*Núria Armengol Urpí,Marin Vlastelica,Georg Martius,Stelian Coros*

Main category: cs.LG

TL;DR: 提出了一种利用FB方法实现零样本强化学习的新策略，特别关注与探索问题的结合，提议通过最小化FB表示的不确定性来改进探索策略。


<details>
  <summary>Details</summary>
Motivation: 当前零样本强化学习中FB方法虽在无回报情况下表现出色，但与探索问题脱节。研究旨在结合FB表示与探索问题，提升学习效率。

Method: 设计基于FB表示的探索策略，通过最小化FB表示的后验方差，降低其不确定性优化探索过程。

Result: 实验证明，相较于其他探索方法，该策略显著提高了FB算法的样本效率。

Conclusion: 通过将FB方法与探索过程相结合，显著增强了零样本强化学习的性能，公开代码为后续研究提供支持。

Abstract: Zero-shot reinforcement learning is necessary for extracting optimal policies
in absence of concrete rewards for fast adaptation to future problem settings.
Forward-backward representations (FB) have emerged as a promising method for
learning optimal policies in absence of rewards via a factorization of the
policy occupancy measure. However, up until now, FB and many similar zero-shot
reinforcement learning algorithms have been decoupled from the exploration
problem, generally relying on other exploration algorithms for data collection.
We argue that FB representations should fundamentally be used for exploration
in order to learn more efficiently. With this goal in mind, we design
exploration policies that arise naturally from the FB representation that
minimize the posterior variance of the FB representation, hence minimizing its
epistemic uncertainty. We empirically demonstrate that such principled
exploration strategies improve sample complexity of the FB algorithm
considerably in comparison to other exploration methods. Code is publicly
available at https://sites.google.com/view/fbee-url.

</details>


### [198] [Dynamic Regret Reduces to Kernelized Static Regret](https://arxiv.org/abs/2507.05478)
*Andrew Jacobsen,Alessandro Rudi,Francesco Orabona,Nicolo Cesa-Bianchi*

Main category: cs.LG

TL;DR: 这篇论文研究了在线凸优化中的动态遗憾，通过将动态遗憾最小化问题转换为一个函数空间中的静态遗憾问题，对其进行分析并提出新方法。


<details>
  <summary>Details</summary>
Motivation: 研究如何更有效地在在线凸优化中实现较低的累积损失，特别是相较于任意的基准序列，以提升现有算法的表现。

Method: 论文通过构建再生核希尔伯特空间（RKHS）作为函数空间，利用动态到静态的还原法，用于分析和最小化动态遗憾。

Result: 在线性损失的场景下，重新获得了已知最优的动态遗憾界，并提出了全新的无比例和方向自适应的动态遗憾保证。此外，该方法扩展到任意损失序列，并可获得对数遗憾界。

Conclusion: 利用RKHS构建的函数空间和动态到静态的还原，提出了有效的在线凸优化算法，即便在无限维空间中，这些算法仍然可以实用并具有良好的理论保证。

Abstract: We study dynamic regret in online convex optimization, where the objective is
to achieve low cumulative loss relative to an arbitrary benchmark sequence. By
observing that competing with an arbitrary sequence of comparators
$u_{1},\ldots,u_{T}$ in $\mathcal{W}\subseteq\mathbb{R}^{d}$ is equivalent to
competing with a fixed comparator function $u:[1,T]\to \mathcal{W}$, we frame
dynamic regret minimization as a static regret problem in a function space. By
carefully constructing a suitable function space in the form of a Reproducing
Kernel Hilbert Space (RKHS), our reduction enables us to recover the optimal
$R_{T}(u_{1},\ldots,u_{T}) = \mathcal{O}(\sqrt{\sum_{t}\|u_{t}-u_{t-1}\|T})$
dynamic regret guarantee in the setting of linear losses, and yields new
scale-free and directionally-adaptive dynamic regret guarantees. Moreover,
unlike prior dynamic-to-static reductions -- which are valid only for linear
losses -- our reduction holds for any sequence of losses, allowing us to
recover $\mathcal{O}\big(\|u\|^2+d_{\mathrm{eff}}(\lambda)\ln T\big)$ bounds in
exp-concave and improper linear regression settings, where
$d_{\mathrm{eff}}(\lambda)$ is a measure of complexity of the RKHS. Despite
working in an infinite-dimensional space, the resulting reduction leads to
algorithms that are computable in practice, due to the reproducing property of
RKHSs.

</details>


### [199] [Navigating Sparse Molecular Data with Stein Diffusion Guidance](https://arxiv.org/abs/2507.05482)
*Van Khoa Nguyen,Lionel Blondé,Alexandros Kalousis*

Main category: cs.LG

TL;DR: 该论文提出了一种新的训练自由扩散指导框架Stein Diffusion Guidance (SDG)，通过结合随机最优控制和Stein变分推断，以对抗现有方法中的误差问题，提升了复杂任务的整体性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型优化方法SOC计算成本高，而无训练指导方法误差较大且不可靠。

Method: 提出Stein Diffusion Guidance (SDG)，结合SOC目标，通过Stein变分推断校正后验，并设计运行成本功能实现低密度区域指导。

Result: 实验表明SDG在分子生成任务上显著优于标准的无训练指导方法。

Conclusion: SDG展示了在扩散模型指导中的潜力，并为未来的广泛应用提供了可能性。

Abstract: Stochastic optimal control (SOC) has recently emerged as a principled
framework for fine-tuning diffusion models. However, its dependence on
computationally intensive simulations makes it impractical for fast sampling.
In parallel, a class of training-free approaches has been developed that guides
diffusion models using off-the-shelf classifiers on predicted clean samples,
bypassing the need to train classifiers on noisy data. These methods can be
interpreted as approximate SOC schemes, using Tweedie's formula to estimate
diffusion posteriors. In practice, however, such direct approximations can
introduce significant errors, leading to unreliable guidance. In this work, we
unify the strengths of both paradigms by proposing a novel training-free
diffusion guidance framework based on a surrogate stochastic optimal control
objective. We derive a new theoretical bound on the value function that reveals
the necessity of correcting the approximate posteriors to remain faithful to
the true diffusion posterior. To this end, we connect the problem with Stein
variational inference, which seeks the steepest descent direction that
minimizes the Kullback-Leibler discrepancy between the two posteriors. Our
method, which we refer to as Stein Diffusion Guidance (SDG), introduces a
principled correction mechanism and incorporates a novel running cost
functional to enable effective guidance in low-density regions. Experiments on
challenging molecular generation tasks demonstrate that SDG significantly
outperforms standard training-free guidance methods, highlighting its potential
for broader applications.

</details>


### [200] [Explainable Hierarchical Deep Learning Neural Networks (Ex-HiDeNN)](https://arxiv.org/abs/2507.05498)
*Reza T. Batley,Chanwook Park,Wing Kam Liu,Sourav Saha*

Main category: cs.LG

TL;DR: 本文提出了一种新方法Ex-HiDeNN，该方法结合分层深度学习神经网络与符号回归，从有限数据中发现闭合形式的表达式，性能优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在从复杂数据中高效发现可解释、准确的封闭形式表达式的挑战。

Method: 提出Ex-HiDeNN神经网络，该网络具备准确、高效、快速、可分离、可扩展性，分为两步算法并嵌入可分离性检查器。

Result: Ex-HiDeNN在基准问题和三个工程应用中表现卓越，其误差比传统符号回归小几个数量级，并在所有案例中优于文献中的参考方法。

Conclusion: Ex-HiDeNN展现了卓越的逼近能力，为从有限数据中提取表达式提供了一种准确、高效的方法，并探讨了其局限性与未来扩展方向。

Abstract: Data-driven science and computation have advanced immensely to construct
complex functional relationships using trainable parameters. However,
efficiently discovering interpretable and accurate closed-form expressions from
complex dataset remains a challenge. The article presents a novel approach
called Explainable Hierarchical Deep Learning Neural Networks or Ex-HiDeNN that
uses an accurate, frugal, fast, separable, and scalable neural architecture
with symbolic regression to discover closed-form expressions from limited
observation. The article presents the two-step Ex-HiDeNN algorithm with a
separability checker embedded in it. The accuracy and efficiency of Ex-HiDeNN
are tested on several benchmark problems, including discerning a dynamical
system from data, and the outcomes are reported. Ex-HiDeNN generally shows
outstanding approximation capability in these benchmarks, producing orders of
magnitude smaller errors compared to reference data and traditional symbolic
regression. Later, Ex-HiDeNN is applied to three engineering applications: a)
discovering a closed-form fatigue equation, b) identification of hardness from
micro-indentation test data, and c) discovering the expression for the yield
surface with data. In every case, Ex-HiDeNN outperformed the reference methods
used in the literature. The proposed method is built upon the foundation and
published works of the authors on Hierarchical Deep Learning Neural Network
(HiDeNN) and Convolutional HiDeNN. The article also provides a clear idea about
the current limitations and future extensions of Ex-HiDeNN.

</details>


### [201] [Dynamic Campus Origin-Destination Mobility Prediction using Graph Convolutional Neural Network on WiFi Logs](https://arxiv.org/abs/2507.05507)
*Godwin Badu-Marfo,Bilal Farooq*

Main category: cs.LG

TL;DR: 本文提出一种基于图的神经网络架构，用于预测校园建筑的占用情况和建筑间动态时间分辨率的交通流动态。


<details>
  <summary>Details</summary>
Motivation: 通过Wi-Fi日志和建筑使用时间表学习校园内交通流模式，同时保护隐私，精准估计相对流量。

Method: 设计了一种数据驱动的图结构，其中节点表示建筑，边表示路径。提出一种结合图卷积和LSTM的神经网络（GCLSTM），以捕捉复杂模式。

Result: 提出的GCLSTM模型在实际的WiFi日志数据上表现优异，显著优于MLP和线性回归等传统方法。

Conclusion: 综合模型展示了优越的性能和实际应用潜力，为未来交通流预测提供了一种高效的解决方案。

Abstract: We present an integrated graph-based neural networks architecture for
predicting campus buildings occupancy and inter-buildings movement at dynamic
temporal resolution that learns traffic flow patterns from Wi-Fi logs combined
with the usage schedules within the buildings. The relative traffic flows are
directly estimated from the WiFi data without assuming the occupant behaviour
or preferences while maintaining individual privacy. We formulate the problem
as a data-driven graph structure represented by a set of nodes (representing
buildings), connected through a route of edges or links using a novel Graph
Convolution plus LSTM Neural Network (GCLSTM) which has shown remarkable
success in modelling complex patterns. We describe the formulation, model
estimation, interpretability and examine the relative performance of our
proposed model. We also present an illustrative architecture of the models and
apply on real-world WiFi logs collected at the Toronto Metropolitan University
campus. The results of the experiments show that the integrated GCLSTM models
significantly outperform traditional pedestrian flow estimators like the Multi
Layer Perceptron (MLP) and Linear Regression.

</details>


### [202] [Beyond Communication Overhead: A Multilevel Monte Carlo Approach for Mitigating Compression Bias in Distributed Learning](https://arxiv.org/abs/2507.05508)
*Ze'ev Zukerman,Bassel Hamoud,Kfir Y. Levy*

Main category: cs.LG

TL;DR: 引入了一种新的多级蒙特卡洛（MLMC）压缩方案，结合了有偏和无偏压缩方法的优点，用于分布式深度学习任务中。


<details>
  <summary>Details</summary>
Motivation: 通信开销常被认为是分布式学习中的瓶颈，而传统梯度压缩方法在效率和理论保证之间存在权衡。

Method: 提出了MLMC压缩方案，通过利用有偏压缩器构建统计上的无偏估计，并将其与常用压缩器（如Top-k、位压缩器）结合，并进一步优化为自适应版本。

Result: 通过分布式深度学习任务的实验验证了方法的有效性，提升了原有压缩器的性能。

Conclusion: 该方法有效结合了有偏与无偏压缩的优点，减少了通信成本，并为分布式学习提供了更高效的压缩方案。

Abstract: Distributed learning methods have gained substantial momentum in recent
years, with communication overhead often emerging as a critical bottleneck.
Gradient compression techniques alleviate communication costs but involve an
inherent trade-off between the empirical efficiency of biased compressors and
the theoretical guarantees of unbiased compressors. In this work, we introduce
a novel Multilevel Monte Carlo (MLMC) compression scheme that leverages biased
compressors to construct statistically unbiased estimates. This approach
effectively bridges the gap between biased and unbiased methods, combining the
strengths of both. To showcase the versatility of our method, we apply it to
popular compressors, like Top-$k$ and bit-wise compressors, resulting in
enhanced variants. Furthermore, we derive an adaptive version of our approach
to further improve its performance. We validate our method empirically on
distributed deep learning tasks.

</details>


### [203] [Heterogeneous Causal Learning for Optimizing Aggregated Functions in User Growth](https://arxiv.org/abs/2507.05510)
*Shuyang Du,Jennifer Zhang,Will Y. Zou*

Main category: cs.LG

TL;DR: 该论文提出了一种新颖的深度学习优化方法，用于增强用户增长营销的效益。


<details>
  <summary>Details</summary>
Motivation: 用户增长是消费者互联网公司的关键战略，研究动机是优化高昂成本的营销活动并提高用户参与度。

Method: 通过深度学习算法，学习过去的实验数据以优化用户选择和奖励分配，同时直接针对关键业务指标进行优化，并通过softmax门控联合优化参数。

Result: 相较于传统方法，模型在复杂业务约束下表现出更优的灵活性，并在实验中表现出相较于当前技术（如R-learner和Causal Forest）超过20%的性能提升。

Conclusion: 所提出的算法具有良好的成本效益和实际影响，可应用于多种产品场景并已在全球部署中取得成功验证。

Abstract: User growth is a major strategy for consumer internet companies. To optimize
costly marketing campaigns and maximize user engagement, we propose a novel
treatment effect optimization methodology to enhance user growth marketing. By
leveraging deep learning, our algorithm learns from past experiments to
optimize user selection and reward allocation, maximizing campaign impact while
minimizing costs. Unlike traditional prediction methods, our model directly
models uplifts in key business metrics. Further, our deep learning model can
jointly optimize parameters for an aggregated loss function using softmax
gating. Our approach surpasses traditional methods by directly targeting
desired business metrics and demonstrates superior algorithmic flexibility in
handling complex business constraints. Comprehensive evaluations, including
comparisons with state-of-the-art techniques such as R-learner and Causal
Forest, validate the effectiveness of our model. We experimentally demonstrate
that our proposed constrained and direct optimization algorithms significantly
outperform state-of-the-art methods by over $20\%$, proving their
cost-efficiency and real-world impact. The versatile methods can be applied to
various product scenarios, including optimal treatment allocation. Its
effectiveness has also been validated through successful worldwide production
deployments.

</details>


### [204] [Deep Learning of Continuous and Structured Policies for Aggregated Heterogeneous Treatment Effects](https://arxiv.org/abs/2507.05511)
*Jennifer Y. Zhang,Shuyang Du,Will Y. Zou*

Main category: cs.LG

TL;DR: 研究提出了一种通用深度学习框架，用于异质性治疗政策的估计和排名，展示了其在公共数据集上的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着异质性治疗效果的广泛应用，治疗行动空间也在扩展，需要一种方法来处理包含多个政策因子的结构化治疗政策。

Method: 研究从基础原理推导了结合多个治疗政策变量的方法，并在深度学习框架中引入了神经增强的朴素贝叶斯层，实现治疗政策变量的整合与排名。

Result: 提出的方法能够处理连续治疗变量、治疗分配以及对聚合治疗效果的直接排名，在公共数据集上验证了其有效性。

Conclusion: 研究提供了一种通用的深度学习框架，用于估计和优化异质性治疗政策，为进一步研究和工业应用奠定了基础。

Abstract: As estimation of Heterogeneous Treatment Effect (HTE) is increasingly adopted
across a wide range of scientific and industrial applications, the treatment
action space can naturally expand, from a binary treatment variable to a
structured treatment policy. This policy may include several policy factors
such as a continuous treatment intensity variable, or discrete treatment
assignments. From first principles, we derive the formulation for incorporating
multiple treatment policy variables into the functional forms of individual and
average treatment effects. Building on this, we develop a methodology to
directly rank subjects using aggregated HTE functions. In particular, we
construct a Neural-Augmented Naive Bayes layer within a deep learning framework
to incorporate an arbitrary number of factors that satisfies the Naive Bayes
assumption. The factored layer is then applied with continuous treatment
variables, treatment assignment, and direct ranking of aggregated treatment
effect functions. Together, these algorithms build towards a generic framework
for deep learning of heterogeneous treatment policies, and we show their power
to improve performance with public datasets.

</details>


### [205] [Estimating Interventional Distributions with Uncertain Causal Graphs through Meta-Learning](https://arxiv.org/abs/2507.05526)
*Anish Dhir,Cristiana Diaconu,Valentinian Mihai Lungu,James Requeima,Richard E. Turner,Mark van der Wilk*

Main category: cs.LG

TL;DR: 本文提出MACE-TNP方法，通过元学习实现端到端建模，在无需依赖昂贵计算的情况下预测贝叶斯模型平均干预后验分布。


<details>
  <summary>Details</summary>
Motivation: 在科学领域中，需要了解对变量干预的效果。然而，观察数据可能兼容多种因果图，传统方法可能过于自信并造成偏差。通过贝叶斯推理解决结构不确定性是一个原则性的方法，但其计算复杂度过高。

Method: 使用元学习，提出一种名为MACE-TNP的端到端模型，该模型用于预测贝叶斯模型平均干预后验分布，从而绕过传统复杂计算。

Result: MACE-TNP方法在实验中表现优于基线的强贝叶斯方法。

Conclusion: 本文工作表明，元学习是一种灵活且可扩展的近似复杂贝叶斯因果推理的方法，可扩展到更具挑战性的场景中。

Abstract: In scientific domains -- from biology to the social sciences -- many
questions boil down to \textit{What effect will we observe if we intervene on a
particular variable?} If the causal relationships (e.g.~a causal graph) are
known, it is possible to estimate the intervention distributions. In the
absence of this domain knowledge, the causal structure must be discovered from
the available observational data. However, observational data are often
compatible with multiple causal graphs, making methods that commit to a single
structure prone to overconfidence. A principled way to manage this structural
uncertainty is via Bayesian inference, which averages over a posterior
distribution on possible causal structures and functional mechanisms.
Unfortunately, the number of causal structures grows super-exponentially with
the number of nodes in the graph, making computations intractable. We propose
to circumvent these challenges by using meta-learning to create an end-to-end
model: the Model-Averaged Causal Estimation Transformer Neural Process
(MACE-TNP). The model is trained to predict the Bayesian model-averaged
interventional posterior distribution, and its end-to-end nature bypasses the
need for expensive calculations. Empirically, we demonstrate that MACE-TNP
outperforms strong Bayesian baselines. Our work establishes meta-learning as a
flexible and scalable paradigm for approximating complex Bayesian causal
inference, that can be scaled to increasingly challenging settings in the
future.

</details>


### [206] [Mitigating Shortcut Learning with InterpoLated Learning](https://arxiv.org/abs/2507.05527)
*Michalis Korakakis,Andreas Vlachos,Adrian Weller*

Main category: cs.LG

TL;DR: 提出了InterpoLL方法，通过插值样本特征减少模型对捷径特征的依赖，从而提高少数样本的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有经验风险最小化方法（ERM）易导致模型利用数据集中存在的输入和标签间的捷径相关性，削弱少数样本的性能和泛化能力。

Method: 利用InterpoLL方法将多数样本的特征插值为包含少数样本特征的形式，从而削弱模型对捷径特征的依赖并增强其对少数样本的泛化能力。

Result: 实验表明，InterpoLL在多个自然语言理解任务中显著提升了少数样本的泛化能力，同时不影响多数样本的准确性。

Conclusion: InterpoLL方法不仅有效减弱了模型对捷径特征的依赖，还能广泛适用于多种体系结构的性能增强。

Abstract: Empirical risk minimization (ERM) incentivizes models to exploit shortcuts,
i.e., spurious correlations between input attributes and labels that are
prevalent in the majority of the training data but unrelated to the task at
hand. This reliance hinders generalization on minority examples, where such
correlations do not hold. Existing shortcut mitigation approaches are
model-specific, difficult to tune, computationally expensive, and fail to
improve learned representations. To address these issues, we propose
InterpoLated Learning (InterpoLL) which interpolates the representations of
majority examples to include features from intra-class minority examples with
shortcut-mitigating patterns. This weakens shortcut influence, enabling models
to acquire features predictive across both minority and majority examples.
Experimental results on multiple natural language understanding tasks
demonstrate that InterpoLL improves minority generalization over both ERM and
state-of-the-art shortcut mitigation methods, without compromising accuracy on
majority examples. Notably, these gains persist across encoder,
encoder-decoder, and decoder-only architectures, demonstrating the method's
broad applicability.

</details>


### [207] [Bit-Flip Fault Attack: Crushing Graph Neural Networks via Gradual Bit Search](https://arxiv.org/abs/2507.05531)
*Sanaz Kazemi Abharian,Sai Manoj Pudukotai Dinakarrao*

Main category: cs.LG

TL;DR: 探讨图神经网络（GNNs）在面对硬件故障攻击时的脆弱性，并提出“GBFA渐进比特翻转故障攻击”方法。


<details>
  <summary>Details</summary>
Motivation: 当前GNN硬件加速器不断涌现，但面临的硬件攻击风险尚未充分研究。

Method: 提出基于Markov模型的分层位翻转攻击（GBFA），通过预测层的执行顺序和权重中位的脆弱性实施攻击，逐步破坏性能。

Result: 在Cora和PubMed数据集上测试，显示GBFA显著降低了GNN模型的预测准确性，例如仅一次比特翻转就使GraphSAGE在Cora数据集上的准确性下降17%。

Conclusion: 研究揭示了使用分层攻击策略对GNN安全的重要性，并指出需要进一步研究硬件攻击防护。

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful machine learning
method for graph-structured data. A plethora of hardware accelerators has been
introduced to meet the performance demands of GNNs in real-world applications.
However, security challenges of hardware-based attacks have been generally
overlooked. In this paper, we investigate the vulnerability of GNN models to
hardware-based fault attack, wherein an attacker attempts to misclassify output
by modifying trained weight parameters through fault injection in a memory
device. Thus, we propose Gradual Bit-Flip Fault Attack (GBFA), a layer-aware
bit-flip fault attack, selecting a vulnerable bit in each selected weight
gradually to compromise the GNN's performance by flipping a minimal number of
bits. To achieve this, GBFA operates in two steps. First, a Markov model is
created to predict the execution sequence of layers based on features extracted
from memory access patterns, enabling the launch of the attack within a
specific layer. Subsequently, GBFA identifies vulnerable bits within the
selected weights using gradient ranking through an in-layer search. We evaluate
the effectiveness of the proposed GBFA attack on various GNN models for node
classification tasks using the Cora and PubMed datasets. Our findings show that
GBFA significantly degrades prediction accuracy, and the variation in its
impact across different layers highlights the importance of adopting a
layer-aware attack strategy in GNNs. For example, GBFA degrades GraphSAGE's
prediction accuracy by 17% on the Cora dataset with only a single bit flip in
the last layer.

</details>


### [208] [Theoretical Learning Performance of Graph Neural Networks: The Impact of Jumping Connections and Layer-wise Sparsification](https://arxiv.org/abs/2507.05533)
*Jiawei Sun,Hongkang Li,Meng Wang*

Main category: cs.LG

TL;DR: 通过分析跳跃连接和图稀疏化对GCN的一般化能力的影响，并验证其理论结论，提出了一种理论框架以解释这些现象。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了跳跃连接和图稀疏化的结合使用的理论分析，而这种结合已经在实践中展现了成功的应用。

Method: 对跳跃连接结合图稀疏化的GCN进行学习动态和一般化分析，提出了稀疏有效邻接矩阵的概念，并研究了不同层的稀疏化需求。

Result: 稀疏化后的一般化性能与稀疏矩阵$A^*$保留的有效边相关，跳跃连接对不同层的稀疏化需求有不同影响，其中第一层对稀疏化偏差的敏感性更高。

Conclusion: 实现了对跳跃连接和稀疏化需求的首次理论表征，并通过基准数据集验证了相关理论结果。

Abstract: Jumping connections enable Graph Convolutional Networks (GCNs) to overcome
over-smoothing, while graph sparsification reduces computational demands by
selecting a sub-matrix of the graph adjacency matrix during neighborhood
aggregation. Learning GCNs with graph sparsification has shown empirical
success across various applications, but a theoretical understanding of the
generalization guarantees remains limited, with existing analyses ignoring
either graph sparsification or jumping connections. This paper presents the
first learning dynamics and generalization analysis of GCNs with jumping
connections using graph sparsification. Our analysis demonstrates that the
generalization accuracy of the learned model closely approximates the highest
achievable accuracy within a broad class of target functions dependent on the
proposed sparse effective adjacency matrix $A^*$. Thus, graph sparsification
maintains generalization performance when $A^*$ preserves the essential edges
that support meaningful message propagation. We reveal that jumping connections
lead to different sparsification requirements across layers. In a
two-hidden-layer GCN, the generalization is more affected by the sparsified
matrix deviations from $A^*$ of the first layer than the second layer. To the
best of our knowledge, this marks the first theoretical characterization of
jumping connections' role in sparsification requirements. We validate our
theoretical results on benchmark datasets in deep GCNs.

</details>


### [209] [Robust Learning on Noisy Graphs via Latent Space Constraints with External Knowledge](https://arxiv.org/abs/2507.05540)
*Chunhui Gu,Mohammad Sadegh Nasr,James P. Long,Kim-Anh Do,Ehsan Irajizad*

Main category: cs.LG

TL;DR: 提出了一种名为LSC-GNN的模型，通过加入外部“干净”边的约束来应对噪声边的挑战，并表现出优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络在处理存在噪声边时的表现问题。

Method: 训练两个编码器，一个在包含全部（目标及外部）边的完整图上，另一个在排除目标噪声边的正则化图上；通过惩罚两个编码器的潜在表示之间的差异来进行约束。

Result: 实验表明，LSC-GNN在噪声较多的图上优于标准和抗噪图神经网络，并能有效扩展到异质图中。

Conclusion: 验证了LSC-GNN能够提高具有噪声关系结构环境下的预测性能和解释力。

Abstract: Graph Neural Networks (GNNs) often struggle with noisy edges. We propose
Latent Space Constrained Graph Neural Networks (LSC-GNN) to incorporate
external "clean" links and guide embeddings of a noisy target graph. We train
two encoders--one on the full graph (target plus external edges) and another on
a regularization graph excluding the target's potentially noisy links--then
penalize discrepancies between their latent representations. This constraint
steers the model away from overfitting spurious edges. Experiments on benchmark
datasets show LSC-GNN outperforms standard and noise-resilient GNNs in graphs
subjected to moderate noise. We extend LSC-GNN to heterogeneous graphs and
validate it on a small protein-metabolite network, where metabolite-protein
interactions reduce noise in protein co-occurrence data. Our results highlight
LSC-GNN's potential to boost predictive performance and interpretability in
settings with noisy relational structures.

</details>


### [210] [Gait-Based Hand Load Estimation via Deep Latent Variable Models with Auxiliary Information](https://arxiv.org/abs/2507.05544)
*Jingyi Gao,Sol Lim,Seokhyun Chung*

Main category: cs.LG

TL;DR: 提出了一种改进的载重估计框架，将未加载的基线步态模式和搬运风格等辅助信息纳入估计过程，无需在推理时提供搬运风格标签。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从步态数据预测负载时存在泛化能力和预测准确性不足的问题。

Method: 采用深度潜变量建模、时间卷积网络和双向交叉注意力机制，将载重和未载重步态模式融合，实现负载估计。

Result: 实验表明，加入辅助信息和显式融合机制可以显著提高预测准确性。

Conclusion: 所提模型在不需要额外标注的情况下，利用辅助信息有效提升了负载估计性能。

Abstract: Machine learning methods are increasingly applied to ergonomic risk
assessment in manual material handling, particularly for estimating carried
load from gait motion data collected from wearable sensors. However, existing
approaches often rely on direct mappings from loaded gait to hand load,
limiting generalization and predictive accuracy. In this study, we propose an
enhanced load estimation framework that incorporates auxiliary information,
including baseline gait patterns during unloaded walking and carrying style.
While baseline gait can be automatically captured by wearable sensors and is
thus readily available at inference time, carrying style typically requires
manual labeling and is often unavailable during deployment. Our model
integrates deep latent variable modeling with temporal convolutional networks
and bi-directional cross-attention to capture gait dynamics and fuse loaded and
unloaded gait patterns. Guided by domain knowledge, the model is designed to
estimate load magnitude conditioned on carrying style, while eliminating the
need for carrying style labels at inference time. Experiments using real-world
data collected from inertial measurement units attached to participants
demonstrate substantial accuracy gains from incorporating auxiliary information
and highlight the importance of explicit fusion mechanisms over naive feature
concatenation.

</details>


### [211] [Preemptive Solving of Future Problems: Multitask Preplay in Humans and Machines](https://arxiv.org/abs/2507.05561)
*Wilka Carvalho,Sam Hall-McMaster,Honglak Lee,Samuel J. Gershman*

Main category: cs.LG

TL;DR: 人类利用一种名为多任务预执行的机制，在从事一个任务的过程中，同时学习未来可能需要的其他任务解决方法。这被证明可以加速人类和人工代理的跨任务学习和适应能力。


<details>
  <summary>Details</summary>
Motivation: 探究人类如何通过已有任务经验，学习并快速适应与之相关的未完成任务，并将这种理论应用到人工智能中以提升其多任务处理能力。

Method: 提出了一种多任务预执行算法，通过将一个任务的经验作为未执行任务的“预执行”模拟，借此学习预测性表示，用于未来快速适应。研究通过小型网格世界和部分可观察的2D Minecraft环境Craftax进行验证，并扩展至人工代理任务中。

Result: 多任务预执行算法在实验中更准确预测了人类如何泛化到未完成任务，同时让人工代理成功实现跨任务行为迁移。

Conclusion: 多任务预执行作为一种理论，不仅能够解释人类如何跨任务学习与泛化，还能提升人工代理的多任务学习能力，在复杂任务环境中表现更优。

Abstract: Humans can pursue a near-infinite variety of tasks, but typically can only
pursue a small number at the same time. We hypothesize that humans leverage
experience on one task to preemptively learn solutions to other tasks that were
accessible but not pursued. We formalize this idea as Multitask Preplay, a
novel algorithm that replays experience on one task as the starting point for
"preplay" -- counterfactual simulation of an accessible but unpursued task.
Preplay is used to learn a predictive representation that can support fast,
adaptive task performance later on. We first show that, compared to traditional
planning and predictive representation methods, multitask preplay better
predicts how humans generalize to tasks that were accessible but not pursued in
a small grid-world, even when people didn't know they would need to generalize
to these tasks. We then show these predictions generalize to Craftax, a
partially observable 2D Minecraft environment. Finally, we show that Multitask
Preplay enables artificial agents to learn behaviors that transfer to novel
Craftax worlds sharing task co-occurrence structure. These findings demonstrate
that Multitask Preplay is a scalable theory of how humans counterfactually
learn and generalize across multiple tasks; endowing artificial agents with the
same capacity can significantly improve their performance in challenging
multitask environments.

</details>


### [212] [The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation](https://arxiv.org/abs/2507.05578)
*Alexander Xiong,Xuandong Zhao,Aneesh Pappu,Dawn Song*

Main category: cs.LG

TL;DR: 本文研究了大型语言模型（LLMs）的数据记忆现象，分析其驱动因素、检测与缓解方法及其法律与伦理影响，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs展现出强大能力，其数据记忆问题导致隐私风险及学习与记忆界限模糊，亟需系统分析与解决。

Method: 综合近期研究，探讨训练数据重复、训练动态及微调程序对记忆的影响，研究前缀提取、成员推断等检测方法，并评估缓解策略如数据清理及差分隐私。

Result: 全面分析了LLMs记忆现象的技术、隐私及性能层面，提出不同维度的发现与挑战。

Conclusion: 记忆现象不仅影响模型行为和隐私，更涉及伦理与法律。文章综述现状并指出未来研究方向的关键问题。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks, yet they also exhibit memorization of their training
data. This phenomenon raises critical questions about model behavior, privacy
risks, and the boundary between learning and memorization. Addressing these
concerns, this paper synthesizes recent studies and investigates the landscape
of memorization, the factors influencing it, and methods for its detection and
mitigation. We explore key drivers, including training data duplication,
training dynamics, and fine-tuning procedures that influence data memorization.
In addition, we examine methodologies such as prefix-based extraction,
membership inference, and adversarial prompting, assessing their effectiveness
in detecting and measuring memorized content. Beyond technical analysis, we
also explore the broader implications of memorization, including the legal and
ethical implications. Finally, we discuss mitigation strategies, including data
cleaning, differential privacy, and post-training unlearning, while
highlighting open challenges in balancing the minimization of harmful
memorization with utility. This paper provides a comprehensive overview of the
current state of research on LLM memorization across technical, privacy, and
performance dimensions, identifying critical directions for future work.

</details>


### [213] [The Fourier Spectral Transformer Networks For Efficient and Generalizable Nonlinear PDEs Prediction](https://arxiv.org/abs/2507.05584)
*Beibei Li*

Main category: cs.LG

TL;DR: 提出了一种统一的傅立叶谱Transformer网络，结合传统谱方法与基于注意力的神经架构，成功应用于流体动力学预测。


<details>
  <summary>Details</summary>
Motivation: 解决传统数值模拟和机器学习方法在复杂动力系统长时间预测中的不足。

Method: 通过将PDE转化为谱常微分方程，用高精度数值求解生成训练数据，并使用Transformer网络建模谱系数的变化。

Result: 在不可压缩Navier-Stokes方程和一维Burgers方程中表现出高准确性，长时间预测优于传统和现有机器学习方法。

Conclusion: 超越传统方式，为复杂动力系统的实时预测与控制提供了新范式。

Abstract: In this work we propose a unified Fourier Spectral Transformer network that
integrates the strengths of classical spectral methods and attention based
neural architectures. By transforming the original PDEs into spectral ordinary
differential equations, we use high precision numerical solvers to generate
training data and use a Transformer network to model the evolution of the
spectral coefficients. We demonstrate the effectiveness of our approach on the
two dimensional incompressible Navier-Stokes equations and the one dimensional
Burgers' equation. The results show that our spectral Transformer can achieve
highly accurate long term predictions even with limited training data, better
than traditional numerical methods and machine learning methods in forecasting
future flow dynamics. The proposed framework generalizes well to unseen data,
bringing a promising paradigm for real time prediction and control of complex
dynamical systems.

</details>


### [214] [Detecting and Mitigating Reward Hacking in Reinforcement Learning Systems: A Comprehensive Empirical Study](https://arxiv.org/abs/2507.05619)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 该论文研究了强化学习中奖励黑客问题，提出了一种系统性检测和缓解方法，并在多种环境中测试了其实效。


<details>
  <summary>Details</summary>
Motivation: 强化学习代理可能会通过利用奖励函数的缺陷获得高分，而不实现预期目标，这限制了自主代理的部署。


Method: 分析了15种环境和5种算法下的训练数据，提出了六类奖励黑客的检测算法，并进行了精度和召回率评估以及多种场景实验验证。

Result: 检测框架在各环境中获得了78.4%的精度和81.7%的召回率，缓解技术在受控场景中将黑客频率降低了54.6%。

Conclusion: 尽管存在实践性挑战，论文成果为强化学习安全性的再现性研究提供了坚实支持。

Abstract: Reward hacking in Reinforcement Learning (RL) systems poses a critical threat
to the deployment of autonomous agents, where agents exploit flaws in reward
functions to achieve high scores without fulfilling intended objectives.
Despite growing awareness of this problem, systematic detection and mitigation
approaches remain limited. This paper presents a large-scale empirical study of
reward hacking across diverse RL environments and algorithms. We analyze 15,247
training episodes across 15 RL environments (Atari, MuJoCo, custom domains) and
5 algorithms (PPO, SAC, DQN, A3C, Rainbow), implementing automated detection
algorithms for six categories of reward hacking: specification gaming, reward
tampering, proxy optimization, objective misalignment, exploitation patterns,
and wireheading. Our detection framework achieves 78.4% precision and 81.7%
recall across environments, with computational overhead under 5%. Through
controlled experiments varying reward function properties, we demonstrate that
reward density and alignment with true objectives significantly impact hacking
frequency ($p < 0.001$, Cohen's $d = 1.24$). We validate our approach through
three simulated application studies representing recommendation systems,
competitive gaming, and robotic control scenarios. Our mitigation techniques
reduce hacking frequency by up to 54.6% in controlled scenarios, though we find
these trade-offs are more challenging in practice due to concept drift, false
positive costs, and adversarial adaptation. All detection algorithms, datasets,
and experimental protocols are publicly available to support reproducible
research in RL safety.

</details>


### [215] [Graph Learning](https://arxiv.org/abs/2507.05636)
*Feng Xia,Ciyuan Peng,Jing Ren,Falih Gozi Febrinanto,Renqiang Luo,Vidya Saikrishna,Shuo Yu,Xiangjie Kong*

Main category: cs.LG

TL;DR: 这是关于图学习的综述，强调其在多领域的应用潜力，同时指出了当前面临的挑战和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 探索图学习作为机器学习和人工智能的关键子领域的重要性，以及其解决复杂非欧几里德关系问题的潜力。

Method: 综述了图学习主要方向，包括高效处理大规模图、时间依赖性建模、多模态数据集成、生成新图样本、解释性增强及伦理问题；并探讨了技术趋势和跨领域整合。

Result: 提供了当前图学习技术的全面视野，并讨论了关键挑战如可扩展性、异质性、可解释性及信任构建等；同时识别了未来研究方向。

Conclusion: 图学习在解决现实世界复杂问题中具备重要意义，深入研究和改进其技术与应用能进一步释放潜力。该综述为相关领域研究者和实践者提供了指导与参考。

Abstract: Graph learning has rapidly evolved into a critical subfield of machine
learning and artificial intelligence (AI). Its development began with early
graph-theoretic methods, gaining significant momentum with the advent of graph
neural networks (GNNs). Over the past decade, progress in scalable
architectures, dynamic graph modeling, multimodal learning, generative AI,
explainable AI (XAI), and responsible AI has broadened the applicability of
graph learning to various challenging environments. Graph learning is
significant due to its ability to model complex, non-Euclidean relationships
that traditional machine learning struggles to capture, thus better supporting
real-world applications ranging from drug discovery and fraud detection to
recommender systems and scientific reasoning. However, challenges like
scalability, generalization, heterogeneity, interpretability, and
trustworthiness must be addressed to unlock its full potential. This survey
provides a comprehensive introduction to graph learning, focusing on key
dimensions including scalable, temporal, multimodal, generative, explainable,
and responsible graph learning. We review state-of-the-art techniques for
efficiently handling large-scale graphs, capturing dynamic temporal
dependencies, integrating heterogeneous data modalities, generating novel graph
samples, and enhancing interpretability to foster trust and transparency. We
also explore ethical considerations, such as privacy and fairness, to ensure
responsible deployment of graph learning models. Additionally, we identify and
discuss emerging topics, highlighting recent integration of graph learning and
other AI paradigms and offering insights into future directions. This survey
serves as a valuable resource for researchers and practitioners seeking to
navigate the rapidly evolving landscape of graph learning.

</details>


### [216] [FACT: the Features At Convergence Theorem for neural networks](https://arxiv.org/abs/2507.05644)
*Enric Boix-Adsera,Neil Mallinar,James B. Simon,Mikhail Belkin*

Main category: cs.LG

TL;DR: 这篇论文提出了一个名为“特征收敛定理”（FACT）的理论，用以解释神经网络在训练收敛时的特征行为，并验证了其正确性。


<details>
  <summary>Details</summary>
Motivation: 深度学习理论中的一个核心问题是理解神经网络如何学习和表示特征，因此作者希望通过理论分析加深对这一过程的理解。

Method: 作者提出并证明了特征收敛定理（FACT），并根据这一理论设计了一种新的学习算法FACT-RFM，结合理论验证与实验验证，展示了其有效性。

Result: FACT得到了实验验证，并且基于FACT的算法FACT-RFM显示出在结构化数据任务上较强的表现，同时捕捉到神经网络学习中的特征行为，如模运算中的模式发现和稀疏奇偶校验问题中的相变现象。

Conclusion: 文章通过理论和实验结合，展示了神经网络特征学习现象的本质，并成功开发出基于这一理论的高性能算法。

Abstract: A central challenge in deep learning theory is to understand how neural
networks learn and represent features. To this end, we prove the Features at
Convergence Theorem (FACT), which gives a self-consistency equation that neural
network weights satisfy at convergence when trained with nonzero weight decay.
For each weight matrix $W$, this equation relates the "feature matrix" $W^\top
W$ to the set of input vectors passed into the matrix during forward
propagation and the loss gradients passed through it during backpropagation. We
validate this relation empirically, showing that neural features indeed satisfy
the FACT at convergence. Furthermore, by modifying the "Recursive Feature
Machines" of Radhakrishnan et al. 2024 so that they obey the FACT, we arrive at
a new learning algorithm, FACT-RFM. FACT-RFM achieves high performance on
tabular data and captures various feature learning behaviors that occur in
neural network training, including grokking in modular arithmetic and phase
transitions in learning sparse parities.

</details>


### [217] [Canine Clinical Gait Analysis for Orthopedic and Neurological Disorders: An Inertial Deep-Learning Approach](https://arxiv.org/abs/2507.05671)
*Netta Palez,Léonie Straß,Sebastian Meller,Holger Volk,Anna Zamansky,Itzik Klein*

Main category: cs.LG

TL;DR: 本研究利用可穿戴惯性传感器和深度学习模型进行犬步态分析，区分健康、骨科及神经系统步态问题，模型分别实现了多分类96%和二分类82%的准确率。


<details>
  <summary>Details</summary>
Motivation: 为了在兽医学中更准确、客观地区分神经及骨科问题步态，改进当前临床诊断能力。

Method: 开发了一种基于惯性传感器读数的深度学习方法，并探索最佳传感器配置、评估协议及模型优化方案，使用29只犬的数据进行实验。

Result: 多分类（健康/骨科/神经）任务准确率达96%，二分类（健康/非健康）任务对未见过样本的准确率为82%。

Conclusion: 基于惯性传感器的深度学习模型可以作为区分骨科和神经系统问题的有效诊断辅助工具，为临床诊断提供可靠支持。

Abstract: Canine gait analysis using wearable inertial sensors is gaining attention in
veterinary clinical settings, as it provides valuable insights into a range of
mobility impairments. Neurological and orthopedic conditions cannot always be
easily distinguished even by experienced clinicians. The current study explored
and developed a deep learning approach using inertial sensor readings to assess
whether neurological and orthopedic gait could facilitate gait analysis. Our
investigation focused on optimizing both performance and generalizability in
distinguishing between these gait abnormalities. Variations in sensor
configurations, assessment protocols, and enhancements to deep learning model
architectures were further suggested. Using a dataset of 29 dogs, our proposed
approach achieved 96% accuracy in the multiclass classification task
(healthy/orthopedic/neurological) and 82% accuracy in the binary classification
task (healthy/non-healthy) when generalizing to unseen dogs. Our results
demonstrate the potential of inertial-based deep learning models to serve as a
practical and objective diagnostic and clinical aid to differentiate gait
assessment in orthopedic and neurological conditions.

</details>


### [218] [Efficient Training of Large-Scale AI Models Through Federated Mixture-of-Experts: A System-Level Approach](https://arxiv.org/abs/2507.05685)
*Xiaobing Chen,Boyang Zhang,Xiangwei Zhou,Mingxuan Sun,Shuai Zhang,Songyang Zhang,Geoffrey Ye Li*

Main category: cs.LG

TL;DR: 本文分析了在分布式数据中维护隐私的同时，利用联邦学习训练大规模人工智能模型的可行性，并提出了一种智能分配专家资源的动态系统设计。


<details>
  <summary>Details</summary>
Motivation: 文章的动机是解决在联邦学习场景中，如何高效管理资源受限的客户端以及复杂的专家模型协调问题。

Method: 提出了一种包含动态适配评分、全局专家负载监控和客户端能力分析的智能客户端-专家对齐机制。

Result: 通过解决系统性问题，显著提升了大规模联邦专家模型的扩展性、效率及通信收敛速度。

Conclusion: 研究提出的智能客户端-专家对齐方法，为边缘计算中支持超高通信效率的大规模人工智能模型部署铺平了道路。

Abstract: The integration of Federated Learning (FL) and Mixture-of-Experts (MoE)
presents a compelling pathway for training more powerful, large-scale
artificial intelligence models (LAMs) on decentralized data while preserving
privacy. However, efficient federated training of these complex MoE-structured
LAMs is hindered by significant system-level challenges, particularly in
managing the interplay between heterogeneous client resources and the
sophisticated coordination required for numerous specialized experts. This
article highlights a critical, yet underexplored concept: the absence of robust
quantitative strategies for dynamic client-expert alignment that holistically
considers varying client capacities and the imperative for system-wise load
balancing. Specifically, we propose a conceptual system design for intelligent
client-expert alignment that incorporates dynamic fitness scoring, global
expert load monitoring, and client capacity profiling. By tackling these
systemic issues, we can unlock more scalable, efficient, and robust training
mechanisms {with fewer communication rounds for convergence}, paving the way
for the widespread deployment of large-scale federated MoE-structured LAMs in
edge computing with ultra-high communication efficiency.

</details>


### [219] [AutoTriton: Automatic Triton Programming with Reinforcement Learning in LLMs](https://arxiv.org/abs/2507.05687)
*Shangzhan Li,Zefan Wang,Ye He,Yuxuan Li,Qi Shi,Jianling Li,Yonggang Hu,Wanxiang Che,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: AutoTriton 是一种通过强化学习优化 Triton 编程的模型，简化了 GPU 核心开发，并在多项基准测试中表现优越。


<details>
  <summary>Details</summary>
Motivation: GPU 内核开发需要针对硬件、内存管理、并行性等方面进行复杂优化，目前的开发过程涉及大量的人工调试，效率低下，阻碍了更广泛的采用。

Method: 提出了 AutoTriton 模型，通过监督微调获取 Triton 编程知识，并使用 GRPO 算法结合规则和执行奖励进行强化学习，优化内核开发过程。

Result: AutoTriton 在 TritonBench 和 KernelBench 的五个评估渠道中，与主流大模型（如 Claude-4-Sonnet 和 DeepSeek-R1-0528）表现相当。

Conclusion: AutoTriton 通过自动生成高性能内核，为构建更高效的 AI 系统奠定了基础，并降低了内核开发的门槛。

Abstract: Kernel development in deep learning requires optimizing computational units
across hardware while balancing memory management, parallelism, and
hardware-specific optimizations through extensive empirical tuning. Although
domain-specific languages like Triton simplify GPU programming by abstracting
low-level details, developers must still manually tune critical parameters such
as tile sizes and memory access patterns through iterative experimentation,
creating substantial barriers to optimal performance and wider adoption. In
this work, we introduce AutoTriton, the first model dedicated to Triton
programming powered by reinforcement learning (RL). AutoTriton performs
supervised fine-tuning (SFT) to be equipped with essential Triton programming
expertise using a high-quality data gathering pipeline, and conducts RL with
Group Relative Policy Optimization (GRPO) algorithm, combining a rule-based
reward and an execution-based reward to further improve Triton programming
ability, sequentially. Experiments across five evaluation channels of
TritonBench and KernelBench illustrate that our 8B model AutoTriton achieves
performance comparable to mainstream large models, including Claude-4-Sonnet
and DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial
role of each module within AutoTriton, including the SFT stage, the RL stage,
and the reward design strategy. These findings underscore the promise of RL for
automatically generating high-performance kernels, and since high-performance
kernels are core components of AI systems, this breakthrough establishes an
important foundation for building more efficient AI systems. The model and code
will be available at https://github.com/AI9Stars/AutoTriton.

</details>


### [220] [MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning in Online Environment](https://arxiv.org/abs/2507.05720)
*Yucheng Shi,Wenhao Yu,Zaitang Li,Yonglin Wang,Hongming Zhang,Ninghao Liu,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: 提出了一种名为MobileGUI-RL的框架，在线训练GUI智能体以解决离线方法在扩展性和环境适应性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的GUI智能体多基于离线收集的轨迹进行训练，容易导致扩展性不足和对新环境的适应性较差。

Method: 提出MobileGUI-RL框架，包含两个关键部分：通过自我探索和筛选构建可学习任务的课程；改进GRPO方法，结合轨迹感知优势和综合奖励以优化导航策略。

Result: 在三个在线移动智能体基准上进行了实验验证，显示出持续性的改进效果。

Conclusion: MobileGUI-RL提供了一种更具扩展性和可靠性的GUI智能体训练方法，有效提高了任务成功率和执行效率。

Abstract: Recently, there has been a surge of vision-based GUI agents designed to
automate everyday mobile and web tasks. These agents interpret raw GUI
screenshots and autonomously decide where to click, scroll, or type, which
bypasses handcrafted rules and app-specific APIs. However, most existing
methods trained GUI agent in the offline environment using pre-collected
trajectories. This approach limits scalability, causes overfitting to specific
UI templates, and leads to brittle policies when faced with unseen environment.
We present MobileGUI-RL, a scalable framework that trains GUI agent in online
environment. MobileGUI-RL contains two key components. It (i) synthesizes a
curriculum of learnable tasks through self-exploration and filtering, and (ii)
adapts GRPO to GUI navigation with trajectory-aware advantages and composite
rewards that balance task success and execution efficiency. Experiments on
three online mobile-agent benchmarks show consistent gains, validating the
effectiveness of our approach.

</details>


### [221] [Hierarchical Task Offloading for UAV-Assisted Vehicular Edge Computing via Deep Reinforcement Learning](https://arxiv.org/abs/2507.05722)
*Hongbao Li,Ziye Jia,Sijie He,Kun Guo,Qihui Wu*

Main category: cs.LG

TL;DR: 本文提出了一种基于部分卸载的双层无人机辅助边缘计算架构，用于解决车载网络中计算和延迟问题，并通过软演员-评论家算法的分层卸载方案优化系统效率。


<details>
  <summary>Details</summary>
Motivation: 现有无人机辅助卸载策略在异构计算资源协调和动态网络适应方面存在不足，亟需改进。

Method: 提出了双层无人机辅助边缘计算架构，包括高空无人机的中继能力和低空无人机的计算支持，并设计了基于软演员-评论家算法的分层卸载方案来优化全局和局部决策。

Result: 仿真结果显示，该方法在任务完成率、系统效率及收敛速度方面优于多种基线方法，并展现了在动态车载环境下的强鲁棒性和适用性。

Conclusion: 提出的架构和方法有效提升了车载网络中计算任务的处理效率，具有重要的实际应用潜力。

Abstract: With the emergence of compute-intensive and delay-sensitive applications in
vehicular networks, unmanned aerial vehicles (UAVs) have emerged as a promising
complement for vehicular edge computing due to the high mobility and flexible
deployment. However, the existing UAV-assisted offloading strategies are
insufficient in coordinating heterogeneous computing resources and adapting to
dynamic network conditions. Hence, this paper proposes a dual-layer
UAV-assisted edge computing architecture based on partial offloading, composed
of the relay capability of high-altitude UAVs and the computing support of
low-altitude UAVs. The proposed architecture enables efficient integration and
coordination of heterogeneous resources. A joint optimization problem is
formulated to minimize the system delay and energy consumption while ensuring
the task completion rate. To solve the high-dimensional decision problem, we
reformulate the problem as a Markov decision process and propose a hierarchical
offloading scheme based on the soft actor-critic algorithm. The method
decouples global and local decisions, where the global decisions integrate
offloading ratios and trajectory planning into continuous actions, while the
local scheduling is handled via designing a priority-based mechanism.
Simulations are conducted and demonstrate that the proposed approach
outperforms several baselines in task completion rate, system efficiency, and
convergence speed, showing strong robustness and applicability in dynamic
vehicular environments.

</details>


### [222] [ABench-Physics: Benchmarking Physical Reasoning in LLMs via High-Difficulty and Dynamic Physics Problems](https://arxiv.org/abs/2507.04766)
*Yiming Zhang,Yingfan Ma,Yanmei Gu,Zhengkai Yang,Yihong Zhuang,Feng Wang,Zenan Huang,Yuanyuan Wang,Chao Huang,Bowen Song,Cheng Lin,Junbo Zhao*

Main category: cs.LG

TL;DR: ABench-Physics评估LLM在物理推理与泛化能力上的表现，揭示其在动态场景中的显著局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在数学和编程领域表现出色，但其在物理领域的能力尚未深入探索，该领域需要精确计算和深刻的概念理解。

Method: 设计ABench-Physics基准测试，包括静态的Phy_A和动态的Phy_B，用以全面评估LLM在物理推理中的表现，并要求精确的数值解答。

Result: LLM在测试中表现出巨大差距，尤其在动态变化情境的泛化能力上存在明显局限。

Conclusion: ABench-Physics为推动LLM科学推理能力定义了一个挑战性的诊断框架。

Abstract: Large Language Models (LLMs) have shown impressive performance in domains
such as mathematics and programming, yet their capabilities in physics remain
underexplored and poorly understood. Physics poses unique challenges that
demand not only precise computation but also deep conceptual understanding and
physical modeling skills. Existing benchmarks often fall short due to limited
difficulty, multiple-choice formats, and static evaluation settings that fail
to capture physical modeling ability. In this paper, we introduce
ABench-Physics, a novel benchmark designed to rigorously evaluate LLMs'
physical reasoning and generalization capabilities. ABench-Physics consists of
two components: Phy_A, a static set of 400 graduate- or Olympiad-level
problems; and Phy_B, a dynamic subset of 100 problems equipped with an
automatic variation engine to test model robustness across changing conditions.
All questions require precise numerical answers, with strict formatting and
tolerance constraints. Our evaluation of several state-of-the-art LLMs reveals
substantial performance gaps, highlighting persistent limitations in physical
reasoning, especially in generalization to dynamic variants. ABench-Physics
provides a challenging and diagnostic framework for advancing scientific
reasoning in LLMs.

</details>


### [223] [Jigsaw: Training Multi-Billion-Parameter AI Weather Models with Optimized Model Parallelism](https://arxiv.org/abs/2507.05753)
*Deifilia Kieckhefen,Markus Götz,Lars H. Heyen,Achim Streit,Charlotte Debus*

Main category: cs.LG

TL;DR: 本文提出WeatherMixer，一种基于多层感知器的架构，以线性复杂度解决气象建模难题，同时通过新型模型并行方案Jigsaw优化计算需求。


<details>
  <summary>Details</summary>
Motivation: 为了在高分辨率和长时间跨度条件下准确建模复杂的大气动态，同时解决大规模神经网络训练中内存和I/O带宽的瓶颈问题。

Method: 提出WeatherMixer架构，其复杂度随输入大小线性扩展，并设计了Jigsaw模型并行方案，通过领域与张量并行的方法消除内存冗余。

Result: Jigsaw在计算通信受限系统中超过了现有的强扩展性能，并在I/O带宽受限系统中实现了弱扩展的超标量水平；在256个GPU上的训练达到9和11 PFLOPs的峰值性能，达到理论峰值性能的23%和28%。

Conclusion: 基于WeatherMixer和Jigsaw的框架在大规模并行训练上展现出卓越的效率和性能，突破了存储与带宽的瓶颈。

Abstract: AI-based methods have revolutionized atmospheric forecasting, with recent
successes in medium-range forecasting spurring the development of climate
foundation models. Accurate modeling of complex atmospheric dynamics at high
spatial resolutions and longer lead times requires large neural networks and
gigabyte-sized data samples, making accelerator memory and I/O-bandwidth the
bottlenecks for model training. We introduce WeatherMixer, a
multi-layer-perceptron-based architecture whose workload scales linearly with
input size, allowing the model to learn global weather phenomena at accuracies
similar to numerical weather prediction. To cope with the computational demand,
we propose Jigsaw, a novel model parallelization scheme that employs both
domain and tensor parallelism, eliminating memory redundancy. Jigsaw exceeds
state-of-the-art performance in strong scaling in compute-communication-limited
systems and achieves superscalar weak scaling in I/O-bandwidth-limited systems.
We scale training to 256 GPUs, reaching peak performances of 9 and 11 PFLOPs,
23% and 28% of theoretical peaks, achieving 68% and 72% scaling efficiency
versus 51% without model parallelism.

</details>


### [224] [From Motion to Meaning: Biomechanics-Informed Neural Network for Explainable Cardiovascular Disease Identification](https://arxiv.org/abs/2507.05783)
*Comte Valentin,Gemma Piella,Mario Ceresa,Miguel A. Gonzalez Ballester*

Main category: cs.LG

TL;DR: 本研究提出了一种结合深度学习图像配准与物理约束的创新方法，用于预测心脏组织的生物力学特性并进行疾病分类，测试结果显示方法在准确性和可解释性方面优势显著。


<details>
  <summary>Details</summary>
Motivation: 心脏疾病是全球主要的致病和致死原因，需要更准确和及时的诊断方法。本研究旨在开发一种结合人工智能的可解释模型，为心脏疾病诊断提供新方法。

Method: 研究采用一种将深度学习图像配准和基于Neo-Hookean材料理论的物理约束结合的方法，对心脏组织的运动进行生物力学建模，并提取特征用于疾病分类。此外，计算局部应变并结合特征选择，测试多种分类算法性能。

Result: 在ACDC数据集上，左心室、右心室和心肌的Dice分数分别为0.945、0.908和0.905。疾病分类中最佳分类器在训练集上达98%准确率，在测试集上达100%。

Conclusion: 该研究方法结合了人工智能与生物力学，既提升了心脏疾病诊断的准确性和可靠性，又为临床诊治提供了具有可解释性的诊断模型，为个性化医疗开辟新途径。

Abstract: Cardiac diseases are among the leading causes of morbidity and mortality
worldwide, which requires accurate and timely diagnostic strategies. In this
study, we introduce an innovative approach that combines deep learning image
registration with physics-informed regularization to predict the biomechanical
properties of moving cardiac tissues and extract features for disease
classification. We utilize the energy strain formulation of Neo-Hookean
material to model cardiac tissue deformations, optimizing the deformation field
while ensuring its physical and biomechanical coherence. This explainable
approach not only improves image registration accuracy, but also provides
insights into the underlying biomechanical processes of the cardiac tissues.
Evaluation on the Automated Cardiac Diagnosis Challenge (ACDC) dataset achieved
Dice scores of 0.945 for the left ventricular cavity, 0.908 for the right
ventricular cavity, and 0.905 for the myocardium. Subsequently, we estimate the
local strains within the moving heart and extract a detailed set of features
used for cardiovascular disease classification. We evaluated five
classification algorithms, Logistic Regression, Multi-Layer Perceptron, Support
Vector Classifier, Random Forest, and Nearest Neighbour, and identified the
most relevant features using a feature selection algorithm. The best performing
classifier obtained a classification accuracy of 98% in the training set and
100% in the test set of the ACDC dataset. By integrating explainable artificial
intelligence, this method empowers clinicians with a transparent understanding
of the model's predictions based on cardiac mechanics, while also significantly
improving the accuracy and reliability of cardiac disease diagnosis, paving the
way for more personalized and effective patient care.

</details>


### [225] [Predicting Graph Structure via Adapted Flux Balance Analysis](https://arxiv.org/abs/2507.05806)
*Sevvandi Kandanaarachchi,Ziqi Xu,Stefan Westerlund,Conrad Sanderson*

Main category: cs.LG

TL;DR: 本文提出了一种结合时间序列预测方法与改进流量平衡分析（FBA）的新方法，用于预测动态图的未来结构。


<details>
  <summary>Details</summary>
Motivation: 现有的图预测方法存在局限性，例如假设连续图的顶点不发生变化。因此需要更灵活的方法来应对动态变化。

Method: 将时间序列预测方法与一种改进的流量平衡分析（FBA）结合使用，以适应动态增长的图结构。

Result: 通过在合成数据集（基于优先连接模型）和真实数据集（如UCI Message, HePH, Facebook, Bitcoin）上的实验证明了该方法的有效性。

Conclusion: 该研究展示了在处理动态图预测方面的新可能性，为类似问题提供了参考途径。

Abstract: Many dynamic processes such as telecommunication and transport networks can
be described through discrete time series of graphs. Modelling the dynamics of
such time series enables prediction of graph structure at future time steps,
which can be used in applications such as detection of anomalies. Existing
approaches for graph prediction have limitations such as assuming that the
vertices do not to change between consecutive graphs. To address this, we
propose to exploit time series prediction methods in combination with an
adapted form of flux balance analysis (FBA), a linear programming method
originating from biochemistry. FBA is adapted to incorporate various
constraints applicable to the scenario of growing graphs. Empirical evaluations
on synthetic datasets (constructed via Preferential Attachment model) and real
datasets (UCI Message, HePH, Facebook, Bitcoin) demonstrate the efficacy of the
proposed approach.

</details>


### [226] [Improving Robustness of Foundation Models in Domain Adaptation with Soup-Adapters](https://arxiv.org/abs/2507.05807)
*Marco Roschkowski*

Main category: cs.LG

TL;DR: 本文提出了在少样本领域适应中，通过训练多个独立的适配器并平均其输出，提高模型性能和鲁棒性的方法。


<details>
  <summary>Details</summary>
Motivation: 解决基础模型在少样本领域适应时的超参数调整难题，以及在分布偏移下的模型鲁棒性问题。

Method: 训练多个独立适配器并平均其输出，同时将这些适配器参数重新拼接为单一适配器（称为Soup-Adapter）。

Result: 新方法相比每个单一适配器表现更佳且对分布偏移更加鲁棒，并且对关键超参数更不敏感。

Conclusion: Soup-Adapter有效解决少样本领域适应中的关键问题，并首次将CLIP-Adapter技术应用于DINOv2进行直接对比。

Abstract: In this paper, we tackle two fundamental problems in few-shot domain
adaptation of foundation models. First, hyperparameter tuning is often
impractical due to the lack of large validation datasets. Second, model
robustness under distribution shifts where test time data deviates slightly
from training distributions, remains a concern. We show that by training
multiple independent adapters and averaging their outputs, the new model has a
higher performance and is more robust to distribution shifts compared to any
individual adapter. This improvement holds even when the adapters are trained
with diverse hyperparameters sampled from a wide range, resulting in varied
individual performance. Consequently, our method addresses both of the problems
described above. The ensemble is also significantly less sensitive to the
residual ratio, a critical hyperparameter of CLIP-Adapter. Since the ensemble
can be reparameterized to a single adapter again using a principled
concatenation of the parameters, we refer to our method as Soup-Adapter. This
is also the first study to explore CLIP adapter-style techniques for DINOv2 and
to directly compare them with CLIP in this setting.

</details>


### [227] [Concept-Based Mechanistic Interpretability Using Structured Knowledge Graphs](https://arxiv.org/abs/2507.05810)
*Sofiia Chorna,Kateryna Tarelkina,Eloïse Berthier,Gianni Franchi*

Main category: cs.LG

TL;DR: 提出一个新的框架和交互工具（BAGEL），通过全球分析高层语义属性的表示和传播，增强对神经网络行为的解释能力。


<details>
  <summary>Details</summary>
Motivation: 针对神经网络预测的可解释性，突破了以往局限于局部解释的方法，拓展至机械可解释性领域。

Method: 系统量化语义概念在各层中的表示，揭示模型内部的潜在电路和信息流，结合知识图可视化平台BAGEL探索概念与类别关系。

Result: 平台BAGEL展示模型行为的系统知识图，帮助识别虚假关联，增强模型可信度。

Conclusion: 框架具有模型无关性和可扩展性，有助于理解深度学习模型在数据偏差下的泛化能力。

Abstract: While concept-based interpretability methods have traditionally focused on
local explanations of neural network predictions, we propose a novel framework
and interactive tool that extends these methods into the domain of mechanistic
interpretability. Our approach enables a global dissection of model behavior by
analyzing how high-level semantic attributes (referred to as concepts) emerge,
interact, and propagate through internal model components. Unlike prior work
that isolates individual neurons or predictions, our framework systematically
quantifies how semantic concepts are represented across layers, revealing
latent circuits and information flow that underlie model decision-making. A key
innovation is our visualization platform that we named BAGEL (for Bias Analysis
with a Graph for global Explanation Layers), which presents these insights in a
structured knowledge graph, allowing users to explore concept-class
relationships, identify spurious correlations, and enhance model
trustworthiness. Our framework is model-agnostic, scalable, and contributes to
a deeper understanding of how deep learning models generalize (or fail to) in
the presence of dataset biases. The demonstration is available at
https://knowledge-graph-ui-4a7cb5.gitlab.io/.

</details>


### [228] [Fair Domain Generalization: An Information-Theoretic View](https://arxiv.org/abs/2507.05823)
*Tangzheng Lian,Guanyu Hu,Dimitrios Kollias,Xinyu Yang,Oya Celiktutan*

Main category: cs.LG

TL;DR: 本文探讨了同时处理领域泛化（Domain Generalization）和算法公平性问题，通过引入PAFDG框架解决Fair Domain Generalization (FairDG) 问题，实验表明PAFDG在真实数据集上实现了更卓越的实用性-公平性权衡。


<details>
  <summary>Details</summary>
Motivation: 现有领域泛化方法往往忽略公平性问题，而公平性方法又未考虑领域偏移，因此需要一种方法同时优化领域泛化和公平性。

Method: 通过信息论角度推导出多类别分类任务中关于风险和公平性违例的互信息上界，以此设计算法，提出PAFDG框架基于帕累托优化实现实用性与公平性之间的平衡。

Result: 在真实世界的视觉和语言数据集上的实验结果表明，与现有方法相比，PAFDG框架达到了更优的实用性-公平性权衡。

Conclusion: PAFDG框架有效解决了FairDG问题，为同时兼顾领域泛化与公平性挑战提供了实用的解决方案。

Abstract: Domain generalization (DG) and algorithmic fairness are two critical
challenges in machine learning. However, most DG methods focus only on
minimizing expected risk in the unseen target domain without considering
algorithmic fairness. Conversely, fairness methods typically do not account for
domain shifts, so the fairness achieved during training may not generalize to
unseen test domains. In this work, we bridge these gaps by studying the problem
of Fair Domain Generalization (FairDG), which aims to minimize both expected
risk and fairness violations in unseen target domains. We derive novel mutual
information-based upper bounds for expected risk and fairness violations in
multi-class classification tasks with multi-group sensitive attributes. These
bounds provide key insights for algorithm design from an information-theoretic
perspective. Guided by these insights, we introduce PAFDG (Pareto-Optimal
Fairness for Domain Generalization), a practical framework that solves the
FairDG problem and models the utility-fairness trade-off through Pareto
optimization. Experiments on real-world vision and language datasets show that
PAFDG achieves superior utility-fairness trade-offs compared to existing
methods.

</details>


### [229] [Prototype-Guided and Lightweight Adapters for Inherent Interpretation and Generalisation in Federated Learning](https://arxiv.org/abs/2507.05852)
*Samuel Ofosu Mensah,Kerol Djoumessi,Philipp Berens*

Main category: cs.LG

TL;DR: 本文提出基于原型和轻量级适配器模块的联邦学习框架，有效解决通信开销和统计异质性问题，同时提升了在临床站点图像分类上的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习在解决隐私保护的分布式模型训练问题中表现优异，但面临通信开销大和客户数据分布统计异质性的问题。

Method: 提出一种联邦学习框架，使用原型进行模型解释，同时利用轻量级适配器模块替代本地模型，优化通信效率并指导泛化能力。

Result: 通过在真实世界的眼底图像数据集上的实验发现，所提方法在提高模型解析能力的同时，比基线算法在分类任务中表现更优。

Conclusion: 所提方法兼顾了模型的可解释性、通信效率和跨客户分布泛化的能力，为联邦学习在实际应用中的挑战提供解决方案。

Abstract: Federated learning (FL) provides a promising paradigm for collaboratively
training machine learning models across distributed data sources while
maintaining privacy. Nevertheless, real-world FL often faces major challenges
including communication overhead during the transfer of large model parameters
and statistical heterogeneity, arising from non-identical independent data
distributions across clients. In this work, we propose an FL framework that 1)
provides inherent interpretations using prototypes, and 2) tackles statistical
heterogeneity by utilising lightweight adapter modules to act as compressed
surrogates of local models and guide clients to achieve generalisation despite
varying client distribution. Each client locally refines its model by aligning
class embeddings toward prototype representations and simultaneously adjust the
lightweight adapter. Our approach replaces the need to communicate entire model
weights with prototypes and lightweight adapters. This design ensures that each
client's model aligns with a globally shared structure while minimising
communication load and providing inherent interpretations. Moreover, we
conducted our experiments on a real-world retinal fundus image dataset, which
provides clinical-site information. We demonstrate inherent interpretable
capabilities and perform a classification task, which shows improvements in
accuracy over baseline algorithms.

</details>


### [230] [Robust Power System State Estimation using Physics-Informed Neural Networks](https://arxiv.org/abs/2507.05874)
*Solon Falas,Markos Asprou,Charalambos Konstantinou,Maria K. Michael*

Main category: cs.LG

TL;DR: 本文提出利用物理信息神经网络（PINNs）提高电力系统状态估计的精度和鲁棒性，能够有效应对故障和数据操纵攻击等问题。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统在面对故障条件或网络攻击时，状态估计和实时监控的响应速度和准确性面临巨大挑战。

Method: 通过在神经网络架构中嵌入物理定律，采用物理信息神经网络（PINNs）进行状态估计，提高了电力传输网在正常及故障条件下的估计准确性，同时对数据操纵攻击等安全问题表现出潜力。

Result: 实验表明，该方法在未见过的数据子集上准确性提高83%，在全新数据集上性能提升65%；在关键节点数据操控攻击下，PINN比传统神经网络准确性高93%。

Conclusion: 该研究表明，基于物理信息神经网络的混合方法显著提高了电力系统状态估计的准确性和鲁棒性，并具有应对安全威胁的潜力。

Abstract: Modern power systems face significant challenges in state estimation and
real-time monitoring, particularly regarding response speed and accuracy under
faulty conditions or cyber-attacks. This paper proposes a hybrid approach using
physics-informed neural networks (PINNs) to enhance the accuracy and
robustness, of power system state estimation. By embedding physical laws into
the neural network architecture, PINNs improve estimation accuracy for
transmission grid applications under both normal and faulty conditions, while
also showing potential in addressing security concerns such as data
manipulation attacks. Experimental results show that the proposed approach
outperforms traditional machine learning models, achieving up to 83% higher
accuracy on unseen subsets of the training dataset and 65% better performance
on entirely new, unrelated datasets. Experiments also show that during a data
manipulation attack against a critical bus in a system, the PINN can be up to
93% more accurate than an equivalent neural network.

</details>


### [231] [Universal Embeddings of Tabular Data](https://arxiv.org/abs/2507.05904)
*Astrid Franz,Frederik Hoppe,Marianne Michaelis,Udo Göbel*

Main category: cs.LG

TL;DR: 该研究提出了一种针对表格数据生成通用嵌入的方法，可用于多种下游任务，而无需预先定义目标。


<details>
  <summary>Details</summary>
Motivation: 工业数据库中的表格数据使用广泛，但通常在设置时未明确应用任务，因此需要一种通用方法来生成适应多任务的嵌入。

Method: 通过将表格数据转换为图结构，使用图自编码器生成实体嵌入，进一步聚合生成每一表格行（样本）的嵌入，可对类似样本实现无需额外训练的嵌入生成。

Result: 实验表明，与现有的通用表格数据嵌入技术相比，该方法在实际数据集中表现出更优越的性能。

Conclusion: 本研究提供了一种任务无关的嵌入方式，其通用性和优异性能为表格数据的多样化应用提供了可能。

Abstract: Tabular data in relational databases represents a significant portion of
industrial data. Hence, analyzing and interpreting tabular data is of utmost
importance. Application tasks on tabular data are manifold and are often not
specified when setting up an industrial database. To address this, we present a
novel framework for generating universal, i.e., task-independent embeddings of
tabular data for performing downstream tasks without predefined targets. Our
method transforms tabular data into a graph structure, leverages Graph
Auto-Encoders to create entity embeddings, which are subsequently aggregated to
obtain embeddings for each table row, i.e., each data sample. This two-step
approach has the advantage that unseen samples, consisting of similar entities,
can be embedded without additional training. Downstream tasks such as
regression, classification or outlier detection, can then be performed by
applying a distance-based similarity measure in the embedding space.
Experiments on real-world datasets demonstrate that our method achieves
superior performance compared to existing universal tabular data embedding
techniques.

</details>


### [232] [Feature-Based vs. GAN-Based Learning from Demonstrations: When and Why](https://arxiv.org/abs/2507.05906)
*Chenhao Li,Marco Hutter,Andreas Krause*

Main category: cs.LG

TL;DR: 论文比较了基于特征和基于GAN的从示范中学习的方法，分析其奖励函数结构对策略学习的影响。


<details>
  <summary>Details</summary>
Motivation: 分析基于特征与GAN两种方法在从示范中学习的优缺点，提供方法选择的框架。

Method: 通过探讨奖励信号的设计、动作用途表示重要性等，评估两种方法的适应性与效率。

Result: 发现两种方法在特定任务中有不同优势，应结合任务需求选择。

Conclusion: 基于特征模式强调解释性和精细模仿，GAN模式注重适应性和可扩展性，不存在哪个更优，应根据应用场景作出决策。

Abstract: This survey provides a comparative analysis of feature-based and GAN-based
approaches to learning from demonstrations, with a focus on the structure of
reward functions and their implications for policy learning. Feature-based
methods offer dense, interpretable rewards that excel at high-fidelity motion
imitation, yet often require sophisticated representations of references and
struggle with generalization in unstructured settings. GAN-based methods, in
contrast, use implicit, distributional supervision that enables scalability and
adaptation flexibility, but are prone to training instability and coarse reward
signals. Recent advancements in both paradigms converge on the importance of
structured motion representations, which enable smoother transitions,
controllable synthesis, and improved task integration. We argue that the
dichotomy between feature-based and GAN-based methods is increasingly nuanced:
rather than one paradigm dominating the other, the choice should be guided by
task-specific priorities such as fidelity, diversity, interpretability, and
adaptability. This work outlines the algorithmic trade-offs and design
considerations that underlie method selection, offering a framework for
principled decision-making in learning from demonstrations.

</details>


### [233] [Diffusion Dataset Condensation: Training Your Diffusion Model Faster with Less Data](https://arxiv.org/abs/2507.05914)
*Rui Huang,Shitong Shao,Zikai Zhou,Pukun Zhao,Hangyu Guo,Tian Ye,Lichen Bai,Shuo Yang,Zeke Xie*

Main category: cs.LG

TL;DR: 该论文提出了一种新的问题设定：扩散数据集浓缩，并通过D2C框架显著加速扩散模型训练，减少数据需求，同时保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型受限于其训练需要大量资源，研究如何减少训练数据与时间成本，以提升计算效率。

Method: 提出D2C框架，包括“选择”和“附加”两个阶段。选择阶段通过扩散难度评分与间隔采样选择多样且紧凑的子集；附加阶段增强子集的语义和视觉表达。

Result: 实验证明，D2C能用极少的数据大幅加速扩散模型训练，例如用0.8%的数据实现100倍加速，FID分数仅为4.3。

Conclusion: D2C框架能够在保持高质量的同时显著降低扩散模型的训练成本，为扩散模型研究提供了新方向。

Abstract: Diffusion models have achieved remarkable success in various generative
tasks, but training them remains highly resource-intensive, often requiring
millions of images and many days of GPU computation. From a data-centric
perspective addressing this limitation, we study diffusion dataset condensation
as a new and challenging problem setting. The goal is to construct a
"synthetic" sub-dataset with significantly fewer samples than the original
dataset, enabling high-quality diffusion model training with greatly reduced
cost. To the best of our knowledge, we are the first to formally investigate
dataset condensation for diffusion models, whereas prior work focused on
training discriminative models. To tackle this new challenge, we propose a
novel Diffusion Dataset Condensation (D2C) framework, which consists of two
phases: Select and Attach. The Select phase identifies a compact and diverse
subset using a diffusion difficulty score and interval sampling. The Attach
phase enhances the selected subset by attaching rich semantic and visual
representations to strengthen the conditional signals. Extensive experiments
across various dataset sizes, model architectures, and resolutions show that
our D2C framework enables significantly faster diffusion model training with
dramatically fewer data, while preserving high visual quality. Notably, for the
SiT-XL/2 architecture, D2C achieves a 100x training speed-up, reaching a FID
score of 4.3 in just 40k steps using only 0.8% of the training data.

</details>


### [234] [Improving AI-Based Canine Heart Disease Diagnosis with Expert-Consensus Auscultation Labeling](https://arxiv.org/abs/2507.05950)
*Pinar Bisgin,Tom Strube,Niklas Tschorn,Michael Pantförder,Maximilian Fecke,Ingrid Ljungvall,Jens Häggström,Gerhard Wess,Christoph Schummer,Sven Meister,Falk M. Howar*

Main category: cs.LG

TL;DR: 本研究探讨了犬类心音数据中的标签噪声问题，证明了通过减少标签噪声和引入多专家意见可以显著提升分类算法的性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决标签噪声对兽医医学中AI模型训练的负面影响，特别是在犬类心音数据的分类中。

Method: 通过邀请专家对140份犬类心音数据标注致病杂音强度，并筛选出70个高质量样本以减少标签噪声。此外，利用心音周期扩展训练数据，并评估了三种算法（AdaBoost、XGBoost和Random Forest）的表现。

Result: 所有算法的分类性能由于标签噪声减少而显著提升，特别是XGBoost在检测不同程度的心杂音时表现尤为出色。

Conclusion: 通过减少标签噪声，机器学习算法在检测犬类心脏杂音中的性能显著提升，研究证明了多专家意见和数据质量控制的重要性。

Abstract: Noisy labels pose significant challenges for AI model training in veterinary
medicine. This study examines expert assessment ambiguity in canine
auscultation data, highlights the negative impact of label noise on
classification performance, and introduces methods for label noise reduction.
To evaluate whether label noise can be minimized by incorporating multiple
expert opinions, a dataset of 140 heart sound recordings (HSR) was annotated
regarding the intensity of holosystolic heart murmurs caused by Myxomatous
Mitral Valve Disease (MMVD). The expert opinions facilitated the selection of
70 high-quality HSR, resulting in a noise-reduced dataset. By leveraging
individual heart cycles, the training data was expanded and classification
robustness was enhanced. The investigation encompassed training and evaluating
three classification algorithms: AdaBoost, XGBoost, and Random Forest. While
AdaBoost and Random Forest exhibited reasonable performances, XGBoost
demonstrated notable improvements in classification accuracy. All algorithms
showed significant improvements in classification accuracy due to the applied
label noise reduction, most notably XGBoost. Specifically, for the detection of
mild heart murmurs, sensitivity increased from 37.71% to 90.98% and specificity
from 76.70% to 93.69%. For the moderate category, sensitivity rose from 30.23%
to 55.81% and specificity from 64.56% to 97.19%. In the loud/thrilling
category, sensitivity and specificity increased from 58.28% to 95.09% and from
84.84% to 89.69%, respectively. These results highlight the importance of
minimizing label noise to improve classification algorithms for the detection
of canine heart murmurs. Index Terms: AI diagnosis, canine heart disease, heart
sound classification, label noise reduction, machine learning, XGBoost,
veterinary cardiology, MMVD.

</details>


### [235] [Simple Convergence Proof of Adam From a Sign-like Descent Perspective](https://arxiv.org/abs/2507.05966)
*Hanyang Peng,Shuang Qin,Yue Yu,Fangqing Jiang,Hui Wang,Zhouchen Lin*

Main category: cs.LG

TL;DR: 本文提出了一种新的Adam优化器理论解释，视其为sign-like优化器，大幅简化了收敛性分析并给予了更优的收敛率证明。


<details>
  <summary>Details</summary>
Motivation: 现有关于Adam优化器的理论分析较为复杂且收敛性证明繁琐、不易推广，而本文目标是提供Adam优化器的更简单且易验证的理论解释。

Method: 通过将Adam优化器重新表述为sign-like形式，基于$p$-仿射方差和$(L_0, L_1, q)$平滑性假设进行了新的收敛性分析。

Result: 在弱假设下，本文证明了Adam优化器达到了${\cal O}(\frac{1}{T^{1/4}})$的最优收敛率，且分析过程与模型维度及稳定性参数无关。同时揭示了动量在保证Adam收敛性中的关键作用。

Conclusion: 新的解释显著简化了收敛性分析，改进了Adam的理论收敛率，并为学习率调参提出了实际指导方法，进一步统一了理论与实践间的桥梁。

Abstract: Adam is widely recognized as one of the most effective optimizers for
training deep neural networks (DNNs). Despite its remarkable empirical success,
its theoretical convergence analysis remains unsatisfactory. Existing works
predominantly interpret Adam as a preconditioned stochastic gradient descent
with momentum (SGDM), formulated as $\bm{x}_{t+1} = \bm{x}_t -
\frac{\gamma_t}{{\sqrt{\bm{v}_t}+\epsilon}} \circ \bm{m}_t$. This perspective
necessitates strong assumptions and intricate techniques, resulting in lengthy
and opaque convergence proofs that are difficult to verify and extend. In
contrast, we propose a novel interpretation by treating Adam as a sign-like
optimizer, expressed as $\bm{x}_{t+1} = \bm{x}_t - \gamma_t
\frac{|\bm{m}_t|}{{\sqrt{\bm{v}_t}+\epsilon}} \circ {\rm Sign}(\bm{m}_t)$. This
reformulation significantly simplifies the convergence analysis. For the first
time, with some mild conditions, we prove that Adam achieves the optimal rate
of ${\cal O}(\frac{1}{T^{\sfrac{1}{4}}})$ rather than the previous ${\cal O}
\left(\frac{\ln T}{T^{\sfrac{1}{4}}}\right)$ under weak assumptions of the
generalized $p$-affine variance and $(L_0, L_1, q)$-smoothness, without
dependence on the model dimensionality or the numerical stability parameter
$\epsilon$. Additionally, our theoretical analysis provides new insights into
the role of momentum as a key factor ensuring convergence and offers practical
guidelines for tuning learning rates in Adam, further bridging the gap between
theory and practice.

</details>


### [236] [KnowIt: Deep Time Series Modeling and Interpretation](https://arxiv.org/abs/2507.06009)
*M. W. Theunissen,R. Rabe,M. H. Davel*

Main category: cs.LG

TL;DR: KnowIt是一个用于深度时间序列模型构建和解释的Python工具包，旨在简化数据导入、模型构建及可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 为用户提供一个灵活的平台，用于对复杂时间序列数据进行知识发现和深度学习模型解释。

Method: 通过定义清晰的接口，将数据集、深度神经网络架构和解释技术分离，实现对时间序列数据的高效建模和解释。

Result: 提供了一个可扩展的框架，帮助用户在自己的时间序列数据上应用深度学习模型并解释其表现。

Conclusion: KnowIt通过促进协作和数据探索，致力于推动时间序列建模领域的发展，并成为受信任的深度学习工具。

Abstract: KnowIt (Knowledge discovery in time series data) is a flexible framework for
building deep time series models and interpreting them. It is implemented as a
Python toolkit, with source code and documentation available from
https://must-deep-learning.github.io/KnowIt. It imposes minimal assumptions
about task specifications and decouples the definition of dataset, deep neural
network architecture, and interpretability technique through well defined
interfaces. This ensures the ease of importing new datasets, custom
architectures, and the definition of different interpretability paradigms while
maintaining on-the-fly modeling and interpretation of different aspects of a
user's own time series data. KnowIt aims to provide an environment where users
can perform knowledge discovery on their own complex time series data through
building powerful deep learning models and explaining their behavior. With
ongoing development, collaboration and application our goal is to make this a
platform to progress this underexplored field and produce a trusted tool for
deep time series modeling.

</details>


### [237] [Kamae: Bridging Spark and Keras for Seamless ML Preprocessing](https://arxiv.org/abs/2507.06021)
*George Barrowclough,Marian Andrecki,James Shinner,Daniele Donghi*

Main category: cs.LG

TL;DR: Kamae 是一个 Python 开源库，将 PySpark 预处理管道转换为等效的 Keras 模型，用于生产推荐系统中的一致性预处理。


<details>
  <summary>Details</summary>
Motivation: 解决训练和推理环境中的预处理逻辑重复以及数据集转移风险问题。

Method: 通过提供 Spark 变换器和估计器，并将其映射到相应的 Keras 层，实现从 PySpark 到 Keras 的转换。

Result: Kamae 在 MovieLens 数据集和 Expedia 的学习排序管道中展示了其实用性。

Conclusion: Kamae 通过实现一致性预处理，降低工程复杂性，同时减少风险，适用于推荐系统的生产环境。

Abstract: In production recommender systems, feature preprocessing must be faithfully
replicated across training and inference environments. This often requires
duplicating logic between offline and online environments, increasing
engineering effort and introducing risks of dataset shift. We present Kamae, an
open-source Python library that bridges this gap by translating PySpark
preprocessing pipelines into equivalent Keras models. Kamae provides a suite of
configurable Spark transformers and estimators, each mapped to a corresponding
Keras layer, enabling consistent, end-to-end preprocessing across the ML
lifecycle. Framework's utility is illustrated on real-world use cases,
including MovieLens dataset and Expedia's Learning-to-Rank pipelines. The code
is available at https://github.com/ExpediaGroup/kamae.

</details>


### [238] [Multi-view mid fusion: a universal approach for learning in an HDLSS setting](https://arxiv.org/abs/2507.06026)
*Lynn Houthuys*

Main category: cs.LG

TL;DR: 本研究针对高维小样本场景提出了一种利用多视图中层融合技术的学习方法。


<details>
  <summary>Details</summary>
Motivation: 解决高维低样本设置中因特征维度高于样本数量所引发的挑战。

Method: 提出了三种视图构建方法，将高维特征向量拆分为多个小子集，每个子集代表一个不同的视图，结合多视图中层融合技术进行学习。

Result: 通过多模型和多任务的实验验证了方法的有效性与泛化能力。

Conclusion: 该研究为进一步探索多视图中层融合学习的普适性提供了基础。

Abstract: The high-dimensional low-sample-size (HDLSS) setting presents significant
challenges in various applications where the feature dimension far exceeds the
number of available samples. This paper introduces a universal approach for
learning in HDLSS setting using multi-view mid fusion techniques. It shows how
existing mid fusion multi-view methods perform well in an HDLSS setting even if
no inherent views are provided. Three view construction methods are proposed
that split the high-dimensional feature vectors into smaller subsets, each
representing a different view. Extensive experimental validation across
model-types and learning tasks confirm the effectiveness and generalization of
the approach. We believe the work in this paper lays the foundation for further
research into the universal benefits of multi-view mid fusion learning.

</details>


### [239] [EdgeCodec: Onboard Lightweight High Fidelity Neural Compressor with Residual Vector Quantization](https://arxiv.org/abs/2507.06040)
*Benjamin Hodo,Tommaso Polonelli,Amirhossein Moallemi,Luca Benini,Michele Magno*

Main category: cs.LG

TL;DR: EdgeCodec 是一款高效的神经压缩器，用于风力涡轮机叶片的气压数据处理，实现高效压缩和实时重建。


<details>
  <summary>Details</summary>
Motivation: 开发一种可在嵌入式设备上实时运行的高效压缩方案，以减少无线数据传输能耗并延长传感器单元的运行寿命。

Method: 提出了EdgeCodec，利用高度不对称的自动编码器架构、判别器和残差矢量量化器来优化压缩效率，支持基于样本即时调整比特率。

Result: 实现了2560:1到10240:1的压缩率，重建误差低于3%，运行在GAP9微控制器上，支持11.25至45比特/秒的比特率。

Conclusion: EdgeCodec 显著提升了压缩效率和无线传输的能耗表现，为分布式传感器的长期运行提供了可能性。

Abstract: We present EdgeCodec, an end-to-end neural compressor for barometric data
collected from wind turbine blades. EdgeCodec leverages a heavily asymmetric
autoencoder architecture, trained with a discriminator and enhanced by a
Residual Vector Quantizer to maximize compression efficiency. It achieves
compression rates between 2'560:1 and 10'240:1 while maintaining a
reconstruction error below 3%, and operates in real time on the GAP9
microcontroller with bitrates ranging from 11.25 to 45 bits per second.
Bitrates can be selected on a sample-by-sample basis, enabling on-the-fly
adaptation to varying network conditions. In its highest compression mode,
EdgeCodec reduces the energy consumption of wireless data transmission by up to
2.9x, significantly extending the operational lifetime of deployed sensor
units.

</details>


### [240] [Few-Shot Learning by Explicit Physics Integration: An Application to Groundwater Heat Transport](https://arxiv.org/abs/2507.06062)
*Julia Pelzer,Corné Verburg,Alexander Heinlein,Miriam Schulte*

Main category: cs.LG

TL;DR: 该论文提出了一种名为“Local-Global CNN (LGCNN)”的新方法，结合全局轻量数值替代和局部卷积神经网络，用于模拟地下水热传输流场，并解决了地热实际应用中的计算复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习在科学与工程的实际应用中常受限于低质量或有限数量的数据，特别是在如地下水热传输复杂问题的高维空间中，需要解决以往方法中计算成本高和长程空间交互预测困难的问题。

Method: 提出了一种称为LGCNN的新方法，将轻量级的数值替代模型（用于全局传输过程）与卷积神经网络（用于局部流速和热扩散过程）相结合。针对大量不均质的地下水场进行实验分析并可扩展至更大区域。

Result: 利用LGCNN模型可以对城市规模的地下温度场进行建模，即使在复杂的地下水流场和多个注入点干扰条件下也能成功预测热羽流扩散情况，总体而言性能优越并可扩展。

Conclusion: LGCNN是实现大尺度复杂地下水热传输建模的一种高效解决方案，结合了科学计算和机器学习的优势，为大规模区域热环境预测提供了一种切实可行的工具。

Abstract: Machine learning methods often struggle with real-world applications in
science and engineering due to limited or low-quality training data. In this
work, the example of groundwater flow with heat transport is considered; this
corresponds to an advection-diffusion process under heterogeneous flow
conditions, that is, spatially distributed material parameters and heat
sources. Classical numerical simulations are costly and challenging due to high
spatio-temporal resolution requirements and large domains. While often
computationally more efficient, purely data-driven surrogate models face
difficulties, particularly in predicting the advection process, which is highly
sensitive to input variations and involves long-range spatial interactions.
Therefore, in this work, a Local-Global Convolutional Neural Network (LGCNN)
approach is introduced. It combines a lightweight numerical surrogate for the
transport process (global) with convolutional neural networks for the
groundwater velocity and heat diffusion processes (local). With the LGCNN, a
city-wide subsurface temperature field is modeled, involving a heterogeneous
groundwater flow field and one hundred groundwater heat pump injection points
forming interacting heat plumes over long distances. The model is first
systematically analyzed based on random subsurface input fields. Then, the
model is trained on a handful of cut-outs from a real-world subsurface map of
the Munich region in Germany, and it scales to larger cut-outs without
retraining. All datasets, our code, and trained models are published for
reproducibility.

</details>


### [241] [QS4D: Quantization-aware training for efficient hardware deployment of structured state-space sequential models](https://arxiv.org/abs/2507.06079)
*Sebastian Siegel,Ming-Jay Yang,Younes Bouhadjar,Maxime Fabre,Emre Neftci,John Paul Strachan*

Main category: cs.LG

TL;DR: 研究展示量化感知训练（QAT）可显著减少结构化状态空间模型（SSM）的复杂性，并提升其在记忆计算设备上的部署效率。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在处理长序列数据时的资源消耗问题，使其适合在资源受限的边缘设备上运行，并探索与专用硬件的结合。

Method: 通过量化感知训练（QAT）减少SSM在性能指标上的复杂性，同时增强其对模拟噪声的鲁棒性，并实现结构性裁剪，最终在存储计算芯片上进行整合与部署。

Result: QAT减少了模型复杂性达两个数量级，并提升了对模拟噪声的鲁棒性。此外，这些技术有助于在模拟存储计算硬件上的高效运行。

Conclusion: 通过结合QAT和相关优化方法，SSM可以在有限硬件资源上高效执行，为在边缘计算设备上的深度学习模型部署提供了一条新路径。

Abstract: Structured State Space models (SSM) have recently emerged as a new class of
deep learning models, particularly well-suited for processing long sequences.
Their constant memory footprint, in contrast to the linearly scaling memory
demands of Transformers, makes them attractive candidates for deployment on
resource-constrained edge-computing devices. While recent works have explored
the effect of quantization-aware training (QAT) on SSMs, they typically do not
address its implications for specialized edge hardware, for example, analog
in-memory computing (AIMC) chips. In this work, we demonstrate that QAT can
significantly reduce the complexity of SSMs by up to two orders of magnitude
across various performance metrics. We analyze the relation between model size
and numerical precision, and show that QAT enhances robustness to analog noise
and enables structural pruning. Finally, we integrate these techniques to
deploy SSMs on a memristive analog in-memory computing substrate and highlight
the resulting benefits in terms of computational efficiency.

</details>


### [242] [CoRE: Enhancing Metacognition with Label-free Self-evaluation in LRMs](https://arxiv.org/abs/2507.06087)
*Haoxi Li,Sikai Bai,Jie Zhang,Song Guo*

Main category: cs.LG

TL;DR: 提出了CoRE和CoRE-Eval框架，用于改善大规模推理模型的效率和准确性。通过分析隐空间中的推理轨迹，以无标签的方式评估中间推理步骤的正确性。实验显示显著减少推理长度同时提高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模推理模型（LRM）在推理过程中过度思考导致效率低下的问题，同时开发方法让模型能够自主评估其推理轨迹的正确性。

Method: 提出了链式推理嵌入（CoRE）和一个无训练、无标签的自我评估框架（CoRE-Eval），利用隐空间状态分析推理轨迹的几何特性以检测冗余推理模式，并动态优化模型的推理终止时间。

Result: 在数学推理基准（如GSM8K, MATH-500, 和AIME）及不同规模模型（7B至32B）上的实验表明，CoRE-Eval减少了13.7%到33.2%的推理长度，同时将答案准确率提高了约10%，32B模型在AIME基准上达到了70.0%的准确率。

Conclusion: CoRE和CoRE-Eval为增强LRM的元认知和推理效率提供了有效的方法，证明可以在减少推理冗余的同时提高准确率。

Abstract: Large reasoning models (LRMs) have demonstrated impressive capabilities in
domains like mathematics and program synthesis. Despite their strong
performance, LRMs often exhibit overthinking -- excessive and redundant
reasoning steps that introduce inefficiencies during inference. This phenomenon
raises an important question for LRM self-evaluation: How can a model
autonomously assess the correctness of its own reasoning trajectory without
external labels? To address this, we propose Chain-of-Reasoning Embedding
(CoRE), a series of hidden states in latent space to enable label-free
self-evaluation on intermediate reasoning steps of LRMs, so as to enhance
metacognition abilities for improved reasoning efficiency. By analyzing the
geometric properties of the CoRE trajectories, we reveal that redundant
reasoning usually presents cyclical fluctuations, which correspond to
repetitive and unconscious reflection/exploration. Leveraging this insight, we
further introduce a training-free, label-free self-evaluation framework,
CoRE-Eval, to detect such patterns and dynamically determine whether to
terminate reasoning early. Extensive experiments on mathematical reasoning
benchmarks (GSM8K, MATH-500, and AIME) and across model sizes from 7B to 32B
demonstrate that CoRE-Eval reduces chain-of-thought length by 13.7% to 33.2%
while improving answer accuracy by around 10%, achieving 70.0% accuracy on the
challenging AIME benchmark with the 32B model.

</details>


### [243] [Safe Domain Randomization via Uncertainty-Aware Out-of-Distribution Detection and Policy Adaptation](https://arxiv.org/abs/2507.06111)
*Mohamad H. Danesh,Maxime Wabartha,Stanley Wu,Joelle Pineau,Hsiu-Chin Lin*

Main category: cs.LG

TL;DR: 提出了一种名为UARL（Uncertainty-Aware RL）的新框架，无需直接在目标域交互的情况下解决分布偏移和安全问题，提升策略的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界部署强化学习（RL）策略时面临的分布偏移、安全性和不可直接交互等挑战。

Method: 使用评论家集群量化策略不确定性并进行环境渐进随机化，通过在模拟环境中的高不确定性区域反复优化，提升策略对目标域的泛化能力。

Result: 在MuJoCo基准和四足机器人上进行了验证，显示了在OOD检测、性能提升和样本效率方面的优越性。

Conclusion: UARL框架在无需目标域明确训练情况下，实现了安全训练与稳健的泛化能力，为真实世界强化学习提供了新的思路。

Abstract: Deploying reinforcement learning (RL) policies in real-world involves
significant challenges, including distribution shifts, safety concerns, and the
impracticality of direct interactions during policy refinement. Existing
methods, such as domain randomization (DR) and off-dynamics RL, enhance policy
robustness by direct interaction with the target domain, an inherently unsafe
practice. We propose Uncertainty-Aware RL (UARL), a novel framework that
prioritizes safety during training by addressing Out-Of-Distribution (OOD)
detection and policy adaptation without requiring direct interactions in target
domain. UARL employs an ensemble of critics to quantify policy uncertainty and
incorporates progressive environmental randomization to prepare the policy for
diverse real-world conditions. By iteratively refining over high-uncertainty
regions of the state space in simulated environments, UARL enhances robust
generalization to the target domain without explicitly training on it. We
evaluate UARL on MuJoCo benchmarks and a quadrupedal robot, demonstrating its
effectiveness in reliable OOD detection, improved performance, and enhanced
sample efficiency compared to baselines.

</details>


### [244] [Subspace-based Approximate Hessian Method for Zeroth-Order Optimization](https://arxiv.org/abs/2507.06125)
*Dongyoon Kim,Sungjae Lee,Wonjin Lee,Kwang In Kim*

Main category: cs.LG

TL;DR: 本文提出了一种名为ZO-SAH的零阶优化方法，通过在随机选取的二维子空间中估计Hessian矩阵，以加速优化问题的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有零阶优化方法主要依赖于一阶近似，而利用二阶信息（曲率）可以显著加速优化收敛。然而，估算Hessian矩阵的高昂计算代价限制了其实用性。

Method: ZO-SAH方法专注于随机选定的二维子空间，通过拟合目标函数的二次多项式来估计Hessian矩阵。为了进一步降低查询成本，方法引入周期性子空间切换策略，重用函数评价。

Result: 在包括逻辑回归和深度神经网络训练任务的8个基准数据集上实验，ZO-SAH表现出相较现有零阶优化方法更快的收敛速度。

Conclusion: ZO-SAH通过利用子空间近似Hessian和周期性切换策略，为零阶优化任务提供了更高效的收敛性能。

Abstract: Zeroth-order optimization addresses problems where gradient information is
inaccessible or impractical to compute. While most existing methods rely on
first-order approximations, incorporating second-order (curvature) information
can, in principle, significantly accelerate convergence. However, the high cost
of function evaluations required to estimate Hessian matrices often limits
practical applicability. We present the subspace-based approximate Hessian
(ZO-SAH) method, a zeroth-order optimization algorithm that mitigates these
costs by focusing on randomly selected two-dimensional subspaces. Within each
subspace, ZO-SAH estimates the Hessian by fitting a quadratic polynomial to the
objective function and extracting its second-order coefficients. To further
reduce function-query costs, ZO-SAH employs a periodic subspace-switching
strategy that reuses function evaluations across optimization steps.
Experiments on eight benchmark datasets, including logistic regression and deep
neural network training tasks, demonstrate that ZO-SAH achieves significantly
faster convergence than existing zeroth-order methods.

</details>


### [245] [Topic Modeling and Link-Prediction for Material Property Discovery](https://arxiv.org/abs/2507.06139)
*Ryan C. Barron,Maksim E. Eren,Valentin Stanev,Cynthia Matuszek,Boian S. Alexandrov*

Main category: cs.LG

TL;DR: 提出了一种基于AI的分层链接预测框架，用于识别科学文献网络及知识图谱中的隐藏联系和指导发现。


<details>
  <summary>Details</summary>
Motivation: 解决科学网络中高稀疏、噪声多、链接缺失等问题，从而更好地揭示隐藏在这些网络中的关联。

Method: 结合层次非负矩阵分解（HNMFk）、布尔矩阵分解（BNMFk）和逻辑矩阵分解（LMF），通过46,862篇文献构建三级主题树并进行自动模型选择和解析。

Result: 实现了超导、储能、摩擦学等材料领域的主题映射，并指出了材料之间隐藏或弱连接的潜在关系，有效验证了工具的准确性与预测能力。

Conclusion: 提出的模型在处理多样学科及视角的科学文献时，能够揭示潜在关系并生成新的假设，助力跨学科探索，可通过交互式平台促进科学发现。

Abstract: Link prediction infers missing or future relations between graph nodes, based
on connection patterns. Scientific literature networks and knowledge graphs are
typically large, sparse, and noisy, and often contain missing links between
entities. We present an AI-driven hierarchical link prediction framework that
integrates matrix factorization to infer hidden associations and steer
discovery in complex material domains. Our method combines Hierarchical
Nonnegative Matrix Factorization (HNMFk) and Boolean matrix factorization
(BNMFk) with automatic model selection, as well as Logistic matrix
factorization (LMF), we use to construct a three-level topic tree from a
46,862-document corpus focused on 73 transition-metal dichalcogenides (TMDs).
These materials are studied in a variety of physics fields with many current
and potential applications.
  An ensemble BNMFk + LMF approach fuses discrete interpretability with
probabilistic scoring. The resulting HNMFk clusters map each material onto
coherent topics like superconductivity, energy storage, and tribology. Also,
missing or weakly connected links are highlight between topics and materials,
suggesting novel hypotheses for cross-disciplinary exploration. We validate our
method by removing publications about superconductivity in well-known
superconductors, and show the model predicts associations with the
superconducting TMD clusters. This shows the method finds hidden connections in
a graph of material to latent topic associations built from scientific
literature, especially useful when examining a diverse corpus of scientific
documents covering the same class of phenomena or materials but originating
from distinct communities and perspectives. The inferred links generating new
hypotheses, produced by our method, are exposed through an interactive
Streamlit dashboard, designed for human-in-the-loop scientific discovery.

</details>


### [246] [Aliasing in Convnets: A Frame-Theoretic Perspective](https://arxiv.org/abs/2507.06152)
*Daniel Haider,Vincent Lostanlen,Martin Ehler,Nicki Holighaus,Peter Balazs*

Main category: cs.LG

TL;DR: 本文分析卷积层中步长引入的混叠效应，并提出优化目标抑制混叠影响，从而促进Parseval稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决卷积层中步长引入的混叠问题对数值稳定性和统计泛化性的影响，填补混叠分析和稳定性研究的空白。

Method: 采用框架理论描述1D卷积核中的混叠效应，推导针对短核的稳定性界限估计与Parseval稳定性特征，并提出两种高效优化目标抑制混叠效应。

Result: 通过计算得到随机卷积核情况下混叠效应的期望与方差的解析表达式，为初始化阶段混叠行为提供了基本见解。

Conclusion: 提出的方法有效抑制混叠进而促进Parseval稳定性，为卷积层的设计和优化提供了理论与实践的支持。

Abstract: Using a stride in a convolutional layer inherently introduces aliasing, which
has implications for numerical stability and statistical generalization. While
techniques such as the parametrizations via paraunitary systems have been used
to promote orthogonal convolution and thus ensure Parseval stability, a general
analysis of aliasing and its effects on the stability has not been done in this
context. In this article, we adapt a frame-theoretic approach to describe
aliasing in convolutional layers with 1D kernels, leading to practical
estimates for stability bounds and characterizations of Parseval stability,
that are tailored to take short kernel sizes into account. From this, we derive
two computationally very efficient optimization objectives that promote
Parseval stability via systematically suppressing aliasing. Finally, for layers
with random kernels, we derive closed-form expressions for the expected value
and variance of the terms that describe the aliasing effects, revealing
fundamental insights into the aliasing behavior at initialization.

</details>


### [247] [A Method for Optimizing Connections in Differentiable Logic Gate Networks](https://arxiv.org/abs/2507.06173)
*Wout Mommen,Lars Keuninckx,Matthias Hartmann,Piet Wambacq*

Main category: cs.LG

TL;DR: 本文提出了一种针对深度可微逻辑门网络(LGNs)的部分连接优化方法，使用概率分布选择最优连接，结果在多个数据集上表现超越传统方法，同时显著减少逻辑门的使用数量。


<details>
  <summary>Details</summary>
Motivation: 解决深度逻辑门网络在逻辑门连接上的优化问题，以提升模型在多个数据集上的表现，同时减少逻辑门数量并提高效率。

Method: 提出一种训练方法，使用针对每个逻辑门输入的部分连接的概率分布，选取最优的连接后确定逻辑门类型。训练时优化部分连接并进行性能评估。

Result: 在Yin-Yang、MNIST和Fashion-MNIST数据集上，连接优化后的LGNs超越了固定连接LGNs，使用了更少的逻辑门，其中用8000逻辑门在MNIST上超过98%准确率。此外，连接优化后的网络比完全连接LGNs表现更好，使用的逻辑门减少了24倍。

Conclusion: 本文提出的连接优化方法推动了全可训练布尔逻辑网络的发展，减少了计算资源需求并提升了效率和性能。

Abstract: We introduce a novel method for partial optimization of the connections in
Deep Differentiable Logic Gate Networks (LGNs). Our training method utilizes a
probability distribution over a subset of connections per gate input, selecting
the connection with highest merit, after which the gate-types are selected. We
show that the connection-optimized LGNs outperform standard fixed-connection
LGNs on the Yin-Yang, MNIST and Fashion-MNIST benchmarks, while requiring only
a fraction of the number of logic gates. When training all connections, we
demonstrate that 8000 simple logic gates are sufficient to achieve over 98% on
the MNIST data set. Additionally, we show that our network has 24 times fewer
gates, while performing better on the MNIST data set compared to standard fully
connected LGNs. As such, our work shows a pathway towards fully trainable
Boolean logic.

</details>


### [248] [Differential Mamba](https://arxiv.org/abs/2507.06204)
*Nadav Schneider,Itamar Zimerman,Eliya Nachmani*

Main category: cs.LG

TL;DR: 本文探讨了差分设计是否能有效应用于Mamba架构，并通过引入新型机制改进其表现。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer和RNN模型在处理上下文时容易关注不相关内容，导致噪声表示和模型性能下降，而改进模型设计有潜力解决这一问题。

Method: 提出了一种新的差分设计机制，适配Mamba架构，并通过语言建模基准测试验证其有效性。

Result: 改进后的Mamba架构在检索能力与整体性能上优于原版，同时有效缓解了上下文资源过分分配问题。

Conclusion: 研究表明新设计方法成功提升了Mamba模型的表现，为基于Mamba的未来研究和应用提供了参考价值。

Abstract: Sequence models like Transformers and RNNs often overallocate attention to
irrelevant context, leading to noisy intermediate representations. This
degrades LLM capabilities by promoting hallucinations, weakening long-range and
retrieval abilities, and reducing robustness. Recent work has shown that
differential design can mitigate this issue in Transformers, improving their
effectiveness across various applications. In this paper, we explore whether
these techniques, originally developed for Transformers, can be applied to
Mamba, a recent architecture based on selective state-space layers that
achieves Transformer-level performance with greater efficiency. We show that a
naive adaptation of differential design to Mamba is insufficient and requires
careful architectural modifications. To address this, we introduce a novel
differential mechanism for Mamba, empirically validated on language modeling
benchmarks, demonstrating improved retrieval capabilities and superior
performance over vanilla Mamba. Finally, we conduct extensive ablation studies
and empirical analyses to justify our design choices and provide evidence that
our approach effectively mitigates the overallocation problem in Mamba-based
models. Our code is publicly available.

</details>


### [249] [Modern Methods in Associative Memory](https://arxiv.org/abs/2507.06211)
*Dmitry Krotov,Benjamin Hoover,Parikshit Ram,Bao Pham*

Main category: cs.LG

TL;DR: 该论文是关于关联记忆（如Hopfield网络）在存储和检索信息中的应用，以及其与当代AI架构（如Transformer和扩散模型）的关系。


<details>
  <summary>Details</summary>
Motivation: 研究关联记忆的理论和实践，以更好理解传统AI模型的计算原理，并设计出更强大的分布式记忆模型。

Method: 通过现代数学语言和方法，结合实际推导和编程，通过教程形式详细介绍关联记忆。

Result: 展示了关联记忆在理解和设计AI模型方面的潜力，以及其与现代模型的联系。

Conclusion: 关联记忆不仅是信息存储和检索的有用模型，还可通过理论分析为现代AI架构提供指导，并启发新模型的设计。

Abstract: Associative Memories like the famous Hopfield Networks are elegant models for
describing fully recurrent neural networks whose fundamental job is to store
and retrieve information. In the past few years they experienced a surge of
interest due to novel theoretical results pertaining to their information
storage capabilities, and their relationship with SOTA AI architectures, such
as Transformers and Diffusion Models. These connections open up possibilities
for interpreting the computation of traditional AI networks through the
theoretical lens of Associative Memories. Additionally, novel Lagrangian
formulations of these networks make it possible to design powerful distributed
models that learn useful representations and inform the design of novel
architectures. This tutorial provides an approachable introduction to
Associative Memories, emphasizing the modern language and methods used in this
area of research, with practical hands-on mathematical derivations and coding
notebooks.

</details>


### [250] [Deep Learning Optimization of Two-State Pinching Antennas Systems](https://arxiv.org/abs/2507.06222)
*Odysseas G. Karagiannidis,Victoria E. Galanopoulou,Panagiotis D. Diamantoulakis,Zhiguo Ding,Octavia Dobre*

Main category: cs.LG

TL;DR: 研究提出了一种利用神经网络优化无线通信系统的动态天线激活策略，以最大化用户终端的通信速率。


<details>
  <summary>Details</summary>
Motivation: 随着无线通信系统的演进，对灵活、节能和成本效益高的天线技术需求日益增加。阶段激活天线通过二元激活状态动态控制电磁波传播，被认为是一种有前景的候选方案。

Method: 本文将选取最优激活天线的问题建模为组合分式0-1二次规划问题，并使用不同复杂度的神经网络架构直接从数据中学习激活策略，同时考虑用户位置的不确定性以模拟实际部署条件。

Result: 仿真结果验证了所提模型的有效性和鲁棒性。

Conclusion: 利用神经网络学习的动态天线激活策略能够在现实条件下有效提高通信速率。

Abstract: The evolution of wireless communication systems requires flexible,
energy-efficient, and cost-effective antenna technologies. Pinching antennas
(PAs), which can dynamically control electromagnetic wave propagation through
binary activation states, have recently emerged as a promising candidate. In
this work, we investigate the problem of optimally selecting a subset of
fixed-position PAs to activate in a waveguide, when the aim is to maximize the
communication rate at a user terminal. Due to the complex interplay between
antenna activation, waveguide-induced phase shifts, and power division, this
problem is formulated as a combinatorial fractional 0-1 quadratic program. To
efficiently solve this challenging problem, we use neural network architectures
of varying complexity to learn activation policies directly from data,
leveraging spatial features and signal structure. Furthermore, we incorporate
user location uncertainty into our training and evaluation pipeline to simulate
realistic deployment conditions. Simulation results demonstrate the
effectiveness and robustness of the proposed models.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [251] [Evolutionary and Coevolutionary Multi-Agent Design Choices and Dynamics](https://arxiv.org/abs/2507.05534)
*Erik Hemberg,Eric Liu,Lucille Fuller,Stephen Moskal,Una-May O'Reilly*

Main category: cs.NE

TL;DR: 本文研究了两种网络安全场景下，演化算法结合不同控制表征方案的效果，并比较了协同进化与单侧优化的表现差异。


<details>
  <summary>Details</summary>
Motivation: 探究不同控制器表征方式及演化算法对团队性能的影响，同时研究协同进化与非协同进化在网络安全情景中的表现差异。

Method: 引入了一种支持LLM的新型变异操作，并使用语法演化算法生成允许用代码逻辑表达控制器的语法。在网络安全场景中评估团队间的学习及竞争，其中包括单侧优化和双侧协同进化两种模式。

Result: 使用允许控制器以代码逻辑表达的语法演化算法能获得最佳团队性能；协同进化减少了峰值波动，但无法维持单侧优化下的高峰表现。

Conclusion: 不同的控制器表征与演化算法组合在团队表现上存在显著差异，协同进化虽然稳定性更高，但单侧优化的性能峰值可能更优。

Abstract: We investigate two representation alternatives for the controllers of teams
of cyber agents. We combine these controller representations with different
evolutionary algorithms, one of which introduces a novel LLM-supported mutation
operator. Using a cyber security scenario, we evaluate agent learning when one
side is trained to compete against a side that does not evolve and when two
sides coevolve with each other. This allows us to quantify the relative merits
and tradeoffs of representation and algorithm combinations in terms of team
performance. Our versions of grammatical evolution algorithms using grammars
that allow a controller to be expressed in code-like logic can achieve the best
team performance. The scenario also allows us to compare the performance impact
and dynamics of coevolution versus evolution under different combinations.
Across the algorithms and representations, we observe that coevolution reduces
the performance highs and lows of both sides while it induces fluctuations on
both sides. In contrast, when only one-side is optimized, performance peaks are
higher and is more sustained than when both sides are optimized with
coevolution.

</details>


### [252] [A Universal Framework for Large-Scale Multi-Objective Optimization Based on Particle Drift and Diffusion](https://arxiv.org/abs/2507.05847)
*Jia-Cheng Li,Min-Rong Chen,Guo-Qiang Zeng,Jian Weng,Man Wang,Jia-Lin Mai*

Main category: cs.NE

TL;DR: 本文提出了一种基于粒子漂移和扩散的大规模多目标优化通用框架，通过模仿粒子在不同环境下的运动，有效解决了高维决策变量带来的收敛性和多样性难题。


<details>
  <summary>Details</summary>
Motivation: 大规模决策变量的多目标优化中，需要解决算法在保持收敛性和种群多样性方面的困难。

Method: 通过借鉴物理粒子的运动行为，将优化过程精妙地分为两个粗调阶段和一个精调阶段，开发多种漂移扩散操作策略，结合现有进化算法，评估其在理论和实际问题中的表现。

Result: 实验表明，本框架显著提升了多目标进化算法的收敛和多样性性能，同时提高了解决大规模多目标问题时的算法计算效率。

Conclusion: 该框架有效应对大规模多目标优化问题，为提升算法性能提供了新方法，并展示了在实际问题中的应用潜力。

Abstract: Large-scale multi-objective optimization poses challenges to existing
evolutionary algorithms in maintaining the performances of convergence and
diversity because of high dimensional decision variables. Inspired by the
motion of particles in physics, we propose a universal framework for
large-scale multi-objective optimization based on particle drift and diffusion
to solve these challenges in this paper. This framework innovatively divides
the optimization process into three sub-stages: two coarse-tuning sub-stages
and one fine-tuning sub-stage. Different strategies of drift-diffusion
operations are performed on the guiding solutions according to the current
sub-stage, ingeniously simulating the movement of particles under diverse
environmental conditions. Finally, representative evolutionary algorithms are
embedded into the proposed framework, and their effectiveness are evaluated
through comparative experiments on various large-scale multi-objective problems
with 1000 to 5000 decision variables. Moreover, comparative algorithms are
conducted on neural network training problems to validate the effectiveness of
the proposed framework in the practical problems. The experimental results
demonstrate that the framework proposed in this paper significantly enhances
the performance of convergence and diversity of MOEAs, and improves the
computational efficiency of algorithms in solving large-scale multi-objective
optimization problems.

</details>
