<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 88]
- [cs.CL](#cs.CL) [Total: 42]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.LG](#cs.LG) [Total: 86]
- [cs.NE](#cs.NE) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Geometry-aware 4D Video Generation for Robot Manipulation](https://arxiv.org/abs/2507.01099)
*Zeyi Liu,Shuang Li,Eric Cousineau,Siyuan Feng,Benjamin Burchfiel,Shuran Song*

Main category: cs.CV

TL;DR: 本文提出了一种4D视频生成模型，通过跨视图点云对齐的几何监督方式解决生成视频时的时间一致性和几何一致性问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在生成时空一致的视频以及避免视角间几何失配方面存在挑战。本研究旨在提升机器人在复杂环境中规划和交互的能力。

Method: 提出的4D视频生成模型使用跨视图点云对齐的几何监督方法训练，使模型可以通过共享的3D表示生成不同视角下的未来视频序列。此外，该方法依赖于RGB-D观察，而无需摄像机位姿信息。

Result: 与现有方法相比，该模型在多个模拟和现实机器人数据集上的表现更加稳定并且视觉和空间对齐更优。

Conclusion: 预测的4D视频可结合6DoF位姿追踪器恢复机器人的末端执行器轨迹，这支持了稳健的机器人操作并实现了对新摄像机视角的泛化性能。

Abstract: Understanding and predicting the dynamics of the physical world can enhance a
robot's ability to plan and interact effectively in complex environments. While
recent video generation models have shown strong potential in modeling dynamic
scenes, generating videos that are both temporally coherent and geometrically
consistent across camera views remains a significant challenge. To address
this, we propose a 4D video generation model that enforces multi-view 3D
consistency of videos by supervising the model with cross-view pointmap
alignment during training. This geometric supervision enables the model to
learn a shared 3D representation of the scene, allowing it to predict future
video sequences from novel viewpoints based solely on the given RGB-D
observations, without requiring camera poses as inputs. Compared to existing
baselines, our method produces more visually stable and spatially aligned
predictions across multiple simulated and real-world robotic datasets. We
further show that the predicted 4D videos can be used to recover robot
end-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting
robust robot manipulation and generalization to novel camera viewpoints.

</details>


### [2] [Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions](https://arxiv.org/abs/2507.01123)
*Rahul A. Burange,Harsh K. Shinde,Omkar Mutyalwar*

Main category: cs.CV

TL;DR: 利用深度学习和多源遥感结合检测和预测滑坡。


<details>
  <summary>Details</summary>
Motivation: 滑坡对基础设施、经济和生命造成威胁，需要准确的检测和预测方法。

Method: 整合多源卫星影像和深度学习模型，包括Sentinel-2多光谱数据、ALOS PALSAR坡度和DEM数据，采用U-Net、DeepLabV3+及Res-Net等分割模型进行分析。

Result: 验证了深度学习和遥感结合对滑坡检测和预测的高效性，并分析地形、植被、降雨对预测的影响。

Conclusion: 提出框架为早期预警、灾害风险管理和可持续土地利用规划提供了可靠支持，有助于构建可扩展和可转移的滑坡预测模型。

Abstract: Landslides pose severe threats to infrastructure, economies, and human lives,
necessitating accurate detection and predictive mapping across diverse
geographic regions. With advancements in deep learning and remote sensing,
automated landslide detection has become increasingly effective. This study
presents a comprehensive approach integrating multi-source satellite imagery
and deep learning models to enhance landslide identification and prediction. We
leverage Sentinel-2 multispectral data and ALOS PALSAR-derived slope and
Digital Elevation Model (DEM) layers to capture critical environmental features
influencing landslide occurrences. Various geospatial analysis techniques are
employed to assess the impact of terra in characteristics, vegetation cover,
and rainfall on detection accuracy. Additionally, we evaluate the performance
of multiple stateof-the-art deep learning segmentation models, including U-Net,
DeepLabV3+, and Res-Net, to determine their effectiveness in landslide
detection. The proposed framework contributes to the development of reliable
early warning systems, improved disaster risk management, and sustainable
land-use planning. Our findings provide valuable insights into the potential of
deep learning and multi-source remote sensing in creating robust, scalable, and
transferable landslide prediction models.

</details>


### [3] [cp_measure: API-first feature extraction for image-based profiling workflows](https://arxiv.org/abs/2507.01163)
*Alán F. Muñoz,Tim Treis,Alexandr A. Kalinin,Shatavisha Dasgupta,Fabian Theis,Anne E. Carpenter,Shantanu Singh*

Main category: cs.CV

TL;DR: 本文提出了一种名为cp_measure的Python库，用于通过模块化、API优先的方式从图像中提取特征，以支持自动化和可重复的图像分析和机器学习应用。


<details>
  <summary>Details</summary>
Motivation: 现有工具（如CellProfiler）在生成特征集合方面存在自动化和可重复性分析的障碍，阻碍了与机器学习流程的结合。因此需要一种改进的工具来解决这一问题。

Method: 开发了cp_measure，一个Python库，将CellProfiler的核心测量功能提取为模块化、API优先工具，方便程序化特征提取。

Result: cp_measure特征与CellProfiler保持高保真，同时能无缝集成到科学Python生态系统中。通过3D星形胶质细胞成像和空间转录组学证明了其在计算生物学中可以扩展、自动化和重复使用。

Conclusion: cp_measure为图像分析和机器学习提供了一种自动化和可重复性强的工具，有助于揭示细胞状态、药物反应和疾病机制中的隐藏模式。

Abstract: Biological image analysis has traditionally focused on measuring specific
visual properties of interest for cells or other entities. A complementary
paradigm gaining increasing traction is image-based profiling - quantifying
many distinct visual features to form comprehensive profiles which may reveal
hidden patterns in cellular states, drug responses, and disease mechanisms.
While current tools like CellProfiler can generate these feature sets, they
pose significant barriers to automated and reproducible analyses, hindering
machine learning workflows. Here we introduce cp_measure, a Python library that
extracts CellProfiler's core measurement capabilities into a modular, API-first
tool designed for programmatic feature extraction. We demonstrate that
cp_measure features retain high fidelity with CellProfiler features while
enabling seamless integration with the scientific Python ecosystem. Through
applications to 3D astrocyte imaging and spatial transcriptomics, we showcase
how cp_measure enables reproducible, automated image-based profiling pipelines
that scale effectively for machine learning applications in computational
biology.

</details>


### [4] [Rapid Salient Object Detection with Difference Convolutional Neural Networks](https://arxiv.org/abs/2507.01182)
*Zhuo Su,Li Liu,Matthias Müller,Jiehua Zhang,Diana Wofk,Ming-Ming Cheng,Matti Pietikäinen*

Main category: cs.CV

TL;DR: 本文提出了一种高效的网络设计，用于在资源受限的设备上实时执行显著性目标检测（SOD）。


<details>
  <summary>Details</summary>
Motivation: 在资源受限设备上进行实时SOD的需求推动了该研究的发展，以解决现有模型计算成本高的问题。

Method: 提出了结合传统显著性检测智慧与现代CNN表示能力的网络设计，使用像素差分卷积（PDC）和差分卷积重参数化（DCR）策略，创新性地引入时空差分卷积（STDC）用于视频SOD。

Result: 提出的SDNet和STDNet在效率与准确性权衡上取得显著优势，参数小于1M的模型在Jetson Orin设备上运行速度分别达到46 FPS（图像）和150 FPS（视频）。

Conclusion: 该方法大幅提升了显著性目标检测的实时性与效率，比现有轻量化模型快2-3倍，且保持出色的准确性。

Abstract: This paper addresses the challenge of deploying salient object detection
(SOD) on resource-constrained devices with real-time performance. While recent
advances in deep neural networks have improved SOD, existing top-leading models
are computationally expensive. We propose an efficient network design that
combines traditional wisdom on SOD and the representation power of modern CNNs.
Like biologically-inspired classical SOD methods relying on computing contrast
cues to determine saliency of image regions, our model leverages Pixel
Difference Convolutions (PDCs) to encode the feature contrasts. Differently,
PDCs are incorporated in a CNN architecture so that the valuable contrast cues
are extracted from rich feature maps. For efficiency, we introduce a difference
convolution reparameterization (DCR) strategy that embeds PDCs into standard
convolutions, eliminating computation and parameters at inference.
Additionally, we introduce SpatioTemporal Difference Convolution (STDC) for
video SOD, enhancing the standard 3D convolution with spatiotemporal contrast
capture. Our models, SDNet for image SOD and STDNet for video SOD, achieve
significant improvements in efficiency-accuracy trade-offs. On a Jetson Orin
device, our models with $<$ 1M parameters operate at 46 FPS and 150 FPS on
streamed images and videos, surpassing the second-best lightweight models in
our experiments by more than $2\times$ and $3\times$ in speed with superior
accuracy. Code will be available at https://github.com/hellozhuo/stdnet.git.

</details>


### [5] [Robust Brain Tumor Segmentation with Incomplete MRI Modalities Using Hölder Divergence and Mutual Information-Enhanced Knowledge Transfer](https://arxiv.org/abs/2507.01254)
*Runze Cheng,Xihang Qiu,Ming Li,Ye Zhang,Chun Li,Fei Yu*

Main category: cs.CV

TL;DR: 提出了一种单一模态并行处理框架，可在多模态MRI数据缺失的情况下，实现高精度的脑肿瘤分割。


<details>
  <summary>Details</summary>
Motivation: 解决传统脑肿瘤分割方法在多模态MRI数据缺失情况下的性能下降问题。

Method: 使用基于Holder散度和互信息的损失函数，维持模态特定特征，并动态调整网络参数以适应缺失的模态，同时量化预测与真实值之间的差异。

Result: 在BraTS 2018和BraTS 2020数据集上进行了广泛评估，展示了在处理模态缺失情况下优于现有方法的性能。

Conclusion: 所提方法在模态缺失情况下表现出较强的鲁棒性，为准确的脑肿瘤分割提供了一种高效可靠的解决方案。

Abstract: Multimodal MRI provides critical complementary information for accurate brain
tumor segmentation. However, conventional methods struggle when certain
modalities are missing due to issues such as image quality, protocol
inconsistencies, patient allergies, or financial constraints. To address this,
we propose a robust single-modality parallel processing framework that achieves
high segmentation accuracy even with incomplete modalities. Leveraging Holder
divergence and mutual information, our model maintains modality-specific
features while dynamically adjusting network parameters based on the available
inputs. By using these divergence- and information-based loss functions, the
framework effectively quantifies discrepancies between predictions and
ground-truth labels, resulting in consistently accurate segmentation. Extensive
evaluations on the BraTS 2018 and BraTS 2020 datasets demonstrate superior
performance over existing methods in handling missing modalities.

</details>


### [6] [AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for AI-Generated Video Evaluation](https://arxiv.org/abs/2507.01255)
*Xiao Liu,Jiawei Zhang*

Main category: cs.CV

TL;DR: 本研究提出了AIGVE-MACS系统，用于对AI生成视频进行全面且具解释性的评估，包括数值与语言评论反馈，同时发布了包含大量数据的基准测试(AIGVE-BENCH 2)。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方式缺乏解释性和与人类评估一致的能力，无法提供多方面的语言评论反馈。

Method: 通过构建AIGVE-BENCH 2基准数据集和结合Vision-Language模型，创新性地引入加权损失函数和动态帧采样策略，实现更贴近人类评估的评价。

Result: 实验表明，AIGVE-MACS在数值评分相关性和评论质量上均超越现有基线，包括GPT-4和VideoScore，并能改进视频质量达到53.5%的提升。

Conclusion: AIGVE-MACS系统为AI生成视频的综合评估设立了新的标准，为领域研究提供了工具和数据，推动了生成视频质量的提升。

Abstract: The rapid advancement of AI-generated video models has created a pressing
need for robust and interpretable evaluation frameworks. Existing metrics are
limited to producing numerical scores without explanatory comments, resulting
in low interpretability and human evaluation alignment. To address those
challenges, we introduce AIGVE-MACS, a unified model for AI-Generated Video
Evaluation(AIGVE), which can provide not only numerical scores but also
multi-aspect language comment feedback in evaluating these generated videos.
Central to our approach is AIGVE-BENCH 2, a large-scale benchmark comprising
2,500 AI-generated videos and 22,500 human-annotated detailed comments and
numerical scores across nine critical evaluation aspects. Leveraging
AIGVE-BENCH 2, AIGVE-MACS incorporates recent Vision-Language Models with a
novel token-wise weighted loss and a dynamic frame sampling strategy to better
align with human evaluators. Comprehensive experiments across supervised and
zero-shot benchmarks demonstrate that AIGVE-MACS achieves state-of-the-art
performance in both scoring correlation and comment quality, significantly
outperforming prior baselines including GPT-4o and VideoScore. In addition, we
further showcase a multi-agent refinement framework where feedback from
AIGVE-MACS drives iterative improvements in video generation, leading to 53.5%
quality enhancement. This work establishes a new paradigm for comprehensive,
human-aligned evaluation of AI-generated videos. We release the AIGVE-BENCH 2
and AIGVE-MACS at https://huggingface.co/xiaoliux/AIGVE-MACS.

</details>


### [7] [Advancements in Weed Mapping: A Systematic Review](https://arxiv.org/abs/2507.01269)
*Mohammad Jahanbakht,Alex Olsen,Ross Marchant,Emilie Fillols,Mostafa Rahimi Azghadi*

Main category: cs.CV

TL;DR: 文章探讨了杂草制图的重要性及其技术进展，并通过系统性综述填补了当前该领域全面文献回顾的空白。


<details>
  <summary>Details</summary>
Motivation: 当前杂草制图对精准农业和环境友好管理至关重要，但缺乏覆盖整个制图流程的系统性文献综述，限制了该领域的发展。

Method: 文章采用PRISMA方法对一系列从数据获取、处理到制图技术（包括传感器、建模及决策支持工具等）进行全面分析和研究。

Result: 通过系统性评估和综合文献发现，文章对杂草制图领域提供了全面理解，为未来研究和开发可扩展、可持续的杂草管理系统奠定基础。

Conclusion: 该综述有助于指导未来研究，支持高效和可持续的杂草管理系统的发展。

Abstract: Weed mapping plays a critical role in precision management by providing
accurate and timely data on weed distribution, enabling targeted control and
reduced herbicide use. This minimizes environmental impacts, supports
sustainable land management, and improves outcomes across agricultural and
natural environments. Recent advances in weed mapping leverage ground-vehicle
Red Green Blue (RGB) cameras, satellite and drone-based remote sensing combined
with sensors such as spectral, Near Infra-Red (NIR), and thermal cameras. The
resulting data are processed using advanced techniques including big data
analytics and machine learning, significantly improving the spatial and
temporal resolution of weed maps and enabling site-specific management
decisions. Despite a growing body of research in this domain, there is a lack
of comprehensive literature reviews specifically focused on weed mapping. In
particular, the absence of a structured analysis spanning the entire mapping
pipeline, from data acquisition to processing techniques and mapping tools,
limits progress in the field. This review addresses these gaps by
systematically examining state-of-the-art methods in data acquisition (sensor
and platform technologies), data processing (including annotation and
modelling), and mapping techniques (such as spatiotemporal analysis and
decision support tools). Following PRISMA guidelines, we critically evaluate
and synthesize key findings from the literature to provide a holistic
understanding of the weed mapping landscape. This review serves as a
foundational reference to guide future research and support the development of
efficient, scalable, and sustainable weed management systems.

</details>


### [8] [Frequency Domain-Based Diffusion Model for Unpaired Image Dehazing](https://arxiv.org/abs/2507.01275)
*Chengxu Liu,Lu Qi,Jinshan Pan,Xueming Qian,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 该论文提出了一种新型的基于频域的扩散模型，用于无配对图像去雾，显著提高了去雾性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有去雾方法引入无关信息、忽略频域中雾霾特性的问题。

Method: 通过频域重建使用扩散模型生成与清晰图像一致的幅度谱，同时引入幅度残差编码器和相位校正模块进行优化。

Result: 实验结果表明，提出的方法在合成和真实世界数据集上均优于其他最新方法。

Conclusion: 基于频域的扩散模型显著提升了无配对图像去雾的性能，实现了先进的去雾效果。

Abstract: Unpaired image dehazing has attracted increasing attention due to its
flexible data requirements during model training. Dominant methods based on
contrastive learning not only introduce haze-unrelated content information, but
also ignore haze-specific properties in the frequency domain (\ie,~haze-related
degradation is mainly manifested in the amplitude spectrum). To address these
issues, we propose a novel frequency domain-based diffusion model, named \ours,
for fully exploiting the beneficial knowledge in unpaired clear data. In
particular, inspired by the strong generative ability shown by Diffusion Models
(DMs), we tackle the dehazing task from the perspective of frequency domain
reconstruction and perform the DMs to yield the amplitude spectrum consistent
with the distribution of clear images. To implement it, we propose an Amplitude
Residual Encoder (ARE) to extract the amplitude residuals, which effectively
compensates for the amplitude gap from the hazy to clear domains, as well as
provide supervision for the DMs training. In addition, we propose a Phase
Correction Module (PCM) to eliminate artifacts by further refining the phase
spectrum during dehazing with a simple attention mechanism. Experimental
results demonstrate that our \ours outperforms other state-of-the-art methods
on both synthetic and real-world datasets.

</details>


### [9] [Learning an Ensemble Token from Task-driven Priors in Facial Analysis](https://arxiv.org/abs/2507.01290)
*Sunyong Seo,Semin Kim,Jongha Lee*

Main category: cs.CV

TL;DR: 本文提出了一种新方法ET-Fuser，通过利用任务相关的先验知识以及注意力机制，实现人脸分析任务的统一特征表示。


<details>
  <summary>Details</summary>
Motivation: 现有研究在训练单任务人脸分析过程中，缺乏对于统一特征表示的关注，这限制了视觉解释性的发展。

Method: 提出一种基于任务先验的关注机制，生成一个共享的集成token，用于在多个预训练编码器之间交互信息，同时具有较高的效率和极低的计算消耗。

Result: 改进了多种人脸分析任务表现，并在特征表示上展现出统计学上显著性的优化。

Conclusion: ET-Fuser实现了有效的人脸分析统一特征表示，具有高效性和计算经济性，为相关领域提供了新视角。

Abstract: Facial analysis exhibits task-specific feature variations. While
Convolutional Neural Networks (CNNs) have enabled the fine-grained
representation of spatial information, Vision Transformers (ViTs) have
facilitated the representation of semantic information at the patch level.
Although the generalization of conventional methodologies has advanced visual
interpretability, there remains paucity of research that preserves the unified
feature representation on single task learning during the training process. In
this work, we introduce ET-Fuser, a novel methodology for learning ensemble
token by leveraging attention mechanisms based on task priors derived from
pre-trained models for facial analysis. Specifically, we propose a robust prior
unification learning method that generates a ensemble token within a
self-attention mechanism, which shares the mutual information along the
pre-trained encoders. This ensemble token approach offers high efficiency with
negligible computational cost. Our results show improvements across a variety
of facial analysis, with statistically significant enhancements observed in the
feature representations.

</details>


### [10] [DiffusionLight-Turbo: Accelerated Light Probes for Free via Single-Pass Chrome Ball Inpainting](https://arxiv.org/abs/2507.01305)
*Worameth Chinchuthakun,Pakkapon Phongthawee,Amit Raj,Varun Jampani,Pramook Khungurn,Supasorn Suwajanakorn*

Main category: cs.CV

TL;DR: 提出了一种利用Stable Diffusion XL模型，从单张低动态范围图像估计照明的新技术，并使用DiffusionLight和DiffusionLight-Turbo实现高质量的HDR灯光探针估计。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖有限的HDR全景数据集，存在泛化失败的情况。该论文的目标是通过预训练扩散模型改进单图像光照估计的准确性和鲁棒性。

Method: 将单张LDR图像任务转化为铬球填充问题，使用扩散模型进行迭代填充生成光照初始结果，并通过Exposure LoRA和Turbo LoRA实现曝光值调整和运行时间优化。

Result: 实验结果显示该方法在复杂场景中生成了逼真的光照估计，并实现了较强的泛化能力。DiffusionLight-Turbo实现了60倍的速度提升。

Conclusion: 该研究提出了一种新颖且有效的方法用于单张图像光照估计，且在易用性、准确性以及运行效率上都有显著提升，适用于野外场景。

Abstract: We introduce a simple yet effective technique for estimating lighting from a
single low-dynamic-range (LDR) image by reframing the task as a chrome ball
inpainting problem. This approach leverages a pre-trained diffusion model,
Stable Diffusion XL, to overcome the generalization failures of existing
methods that rely on limited HDR panorama datasets. While conceptually simple,
the task remains challenging because diffusion models often insert incorrect or
inconsistent content and cannot readily generate chrome balls in HDR format.
Our analysis reveals that the inpainting process is highly sensitive to the
initial noise in the diffusion process, occasionally resulting in unrealistic
outputs. To address this, we first introduce DiffusionLight, which uses
iterative inpainting to compute a median chrome ball from multiple outputs to
serve as a stable, low-frequency lighting prior that guides the generation of a
high-quality final result. To generate high-dynamic-range (HDR) light probes,
an Exposure LoRA is fine-tuned to create LDR images at multiple exposure
values, which are then merged. While effective, DiffusionLight is
time-intensive, requiring approximately 30 minutes per estimation. To reduce
this overhead, we introduce DiffusionLight-Turbo, which reduces the runtime to
about 30 seconds with minimal quality loss. This 60x speedup is achieved by
training a Turbo LoRA to directly predict the averaged chrome balls from the
iterative process. Inference is further streamlined into a single denoising
pass using a LoRA swapping technique. Experimental results that show our method
produces convincing light estimates across diverse settings and demonstrates
superior generalization to in-the-wild scenarios. Our code is available at
https://diffusionlight.github.io/turbo

</details>


### [11] [Physics-informed Ground Reaction Dynamics from Human Motion Capture](https://arxiv.org/abs/2507.01340)
*Cuong Le,Huy-Phuong Le,Duc Le,Minh-Thien Duong,Van-Binh Nguyen,My-Ha Le*

Main category: cs.CV

TL;DR: 提出了一个基于物理的模型，通过运动捕捉数据估算人体运动的地面反作用力，精度高，成功超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 目前方法需要用到实验室设备如测力板，限制了人体动力学的研究。

Method: 基于物理定律，引入欧拉积分和PD算法，从运动捕捉数据精确计算地面反作用力，并将其用于训练模型以提高估算精度。

Result: 在GroundLink数据集上，该方法在地面反作用力估算精度和模拟轨迹精确度上超过了基线模型。

Conclusion: 方法提高了人体动力学估算的精度，并减少对专用设备的依赖。代码已开源。

Abstract: Body dynamics are crucial information for the analysis of human motions in
important research fields, ranging from biomechanics, sports science to
computer vision and graphics. Modern approaches collect the body dynamics,
external reactive force specifically, via force plates, synchronizing with
human motion capture data, and learn to estimate the dynamics from a black-box
deep learning model. Being specialized devices, force plates can only be
installed in laboratory setups, imposing a significant limitation on the
learning of human dynamics. To this end, we propose a novel method for
estimating human ground reaction dynamics directly from the more reliable
motion capture data with physics laws and computational simulation as
constrains. We introduce a highly accurate and robust method for computing
ground reaction forces from motion capture data using Euler's integration
scheme and PD algorithm. The physics-based reactive forces are used to inform
the learning model about the physics-informed motion dynamics thus improving
the estimation accuracy. The proposed approach was tested on the GroundLink
dataset, outperforming the baseline model on: 1) the ground reaction force
estimation accuracy compared to the force plates measurement; and 2) our
simulated root trajectory precision. The implementation code is available at
https://github.com/cuongle1206/Phys-GRD

</details>


### [12] [Learning Camera-Agnostic White-Balance Preferences](https://arxiv.org/abs/2507.01342)
*Luxi Zhao,Mahmoud Afifi,Michael S. Brown*

Main category: cs.CV

TL;DR: 提出一种新的美学一致性的自动白平衡（AWB）映射方法，用于跨相机的图像颜色调整。


<details>
  <summary>Details</summary>
Motivation: 现有的AWB方法主要关注中性白平衡，难以跨不同相机传感器泛化，特别是在多相机智能手机上。需要一种能够实现美学一致性的解决方案。

Method: 利用一个轻量化模型（约500个参数）学习后置光源估算映射，将中性白平衡修正转化为美学偏好的修正，从而实现跨相机的一致性和风格化的颜色渲染。

Result: 在包含三个不同相机的771张智能手机图像数据集上测试，性能达到最新技术水平，同时完全兼容现有的跨相机AWB技术，并带来了极低的计算和内存开销。

Conclusion: 该方法提供了一种高效且实用的美学一致性自动白平衡解决方案，可广泛应用于不同相机设备，并提升图像的美观性和一致性。

Abstract: The image signal processor (ISP) pipeline in modern cameras consists of
several modules that transform raw sensor data into visually pleasing images in
a display color space. Among these, the auto white balance (AWB) module is
essential for compensating for scene illumination. However, commercial AWB
systems often strive to compute aesthetic white-balance preferences rather than
accurate neutral color correction. While learning-based methods have improved
AWB accuracy, they typically struggle to generalize across different camera
sensors -- an issue for smartphones with multiple cameras. Recent work has
explored cross-camera AWB, but most methods remain focused on achieving neutral
white balance. In contrast, this paper is the first to address aesthetic
consistency by learning a post-illuminant-estimation mapping that transforms
neutral illuminant corrections into aesthetically preferred corrections in a
camera-agnostic space. Once trained, our mapping can be applied after any
neutral AWB module to enable consistent and stylized color rendering across
unseen cameras. Our proposed model is lightweight -- containing only $\sim$500
parameters -- and runs in just 0.024 milliseconds on a typical flagship mobile
CPU. Evaluated on a dataset of 771 smartphone images from three different
cameras, our method achieves state-of-the-art performance while remaining fully
compatible with existing cross-camera AWB techniques, introducing minimal
computational and memory overhead.

</details>


### [13] [Learning from Random Subspace Exploration: Generalized Test-Time Augmentation with Self-supervised Distillation](https://arxiv.org/abs/2507.01347)
*Andrei Jelea,Ahmed Nabil Belbachir,Marius Leordeanu*

Main category: cs.CV

TL;DR: 提出了一种通用测试时增强（GTTA）方法，用于改进训练模型的性能，并且能广泛应用于视觉和非视觉任务。


<details>
  <summary>Details</summary>
Motivation: 旨在克服现有测试时增强方法的局限，提供一种更通用、有效的模型性能提升手段。

Method: 通过随机扰动测试输入的PCA子空间投影形成稳健的集成，并引入最终阶段的自监督学习以减少计算成本。

Result: GTTA在图像分类、分割等多任务上表现优越，并证明在低可见度水下视频鲑鱼分割检测中的有效性，还引入了相关领域最大的数据集DeepSalmon。

Conclusion: GTTA具备通用性，显著降低了测试时间下的计算成本，同时提升了性能，适用于广泛的实际任务。

Abstract: We introduce Generalized Test-Time Augmentation (GTTA), a highly effective
method for improving the performance of a trained model, which unlike other
existing Test-Time Augmentation approaches from the literature is general
enough to be used off-the-shelf for many vision and non-vision tasks, such as
classification, regression, image segmentation and object detection. By
applying a new general data transformation, that randomly perturbs multiple
times the PCA subspace projection of a test input, GTTA forms robust ensembles
at test time in which, due to sound statistical properties, the structural and
systematic noises in the initial input data is filtered out and final estimator
errors are reduced. Different from other existing methods, we also propose a
final self-supervised learning stage in which the ensemble output, acting as an
unsupervised teacher, is used to train the initial single student model, thus
reducing significantly the test time computational cost, at no loss in
accuracy. Our tests and comparisons to strong TTA approaches and SoTA models on
various vision and non-vision well-known datasets and tasks, such as image
classification and segmentation, speech recognition and house price prediction,
validate the generality of the proposed GTTA. Furthermore, we also prove its
effectiveness on the more specific real-world task of salmon segmentation and
detection in low-visibility underwater videos, for which we introduce
DeepSalmon, the largest dataset of its kind in the literature.

</details>


### [14] [Long-Tailed Distribution-Aware Router For Mixture-of-Experts in Large Vision-Language Model](https://arxiv.org/abs/2507.01351)
*Chaoxiang Cai,Longrong Yang,Kaibing Chen,Fan Yang,Xi Li*

Main category: cs.CV

TL;DR: 本文提出了Long-Tailed Distribution-aware Router (LTDR)用于视觉-语言模型（LVLM）的专家路由，解决当前框架忽视视觉与语言分布差异的问题。


<details>
  <summary>Details</summary>
Motivation: 针对视觉-语言混合模型中现有的专家路由方法未能充分考虑视觉与语言信号的分布差异，导致模型不足以处理长尾的视觉信息。

Method: 提出了一种新的路由方法（LTDR），包括：1）基于分布特性的路由方案，分别针对语言（均匀分布）与视觉（长尾分布）信号设计独立的路由策略；2）为视觉长尾信号设计的专家激活增强策略，通过类过采样方法增加激活专家的数量。

Result: 通过大规模的基准测试，验证了提出方法在视觉-语言任务中的有效性。

Conclusion: LTDR能够更好地适应视觉与语言的分布差异，在专家路由中提升了对视觉长尾信号的处理能力。

Abstract: The mixture-of-experts (MoE), which replaces dense models with sparse
architectures, has gained attention in large vision-language models (LVLMs) for
achieving comparable performance with fewer activated parameters. Existing MoE
frameworks for LVLMs focus on token-to-expert routing (TER), encouraging
different experts to specialize in processing distinct tokens. However, these
frameworks often rely on the load balancing mechanism, overlooking the inherent
distributional differences between vision and language. To this end, we propose
a Long-Tailed Distribution-aware Router (LTDR) for vision-language TER,
tackling two challenges: (1) Distribution-aware router for modality-specific
routing. We observe that language TER follows a uniform distribution, whereas
vision TER exhibits a long-tailed distribution. This discrepancy necessitates
distinct routing strategies tailored to each modality. (2) Enhancing expert
activation for vision tail tokens. Recognizing the importance of vision tail
tokens, we introduce an oversampling-like strategy by increasing the number of
activated experts for these tokens. Experiments on extensive benchmarks
validate the effectiveness of our approach.

</details>


### [15] [3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation](https://arxiv.org/abs/2507.01367)
*Tianrui Lou,Xiaojun Jia,Siyuan Liang,Jiawei Liang,Ming Zhang,Yanjun Xiao,Xiaochun Cao*

Main category: cs.CV

TL;DR: 文章提出了一种基于3D高斯点云的新型物理对抗攻击框架PGA，以实现快速精确的重构与照片级真实渲染，并在多视角和复杂环境中具有强大的对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 物理对抗攻击在深度神经网络中的安全场景中暴露了潜在风险，现有方法在多视图鲁棒性和复杂环境适应性上存在不足。

Method: 提出PGA框架，利用3D高斯点云技术通过极少数图像进行快速精确的目标重建，并通过避免点云的互相遮挡和自遮挡以及采用最小-最大优化以提升跨视图鲁棒性。

Result: 实验结果表明，PGA在对抗有效性和多样物理环境的鲁棒性上优于现有方法。

Conclusion: PGA框架为物理对抗攻击领域提供了一种高效、鲁棒的解决方案，并具有广泛应用潜力。

Abstract: Physical adversarial attack methods expose the vulnerabilities of deep neural
networks and pose a significant threat to safety-critical scenarios such as
autonomous driving. Camouflage-based physical attack is a more promising
approach compared to the patch-based attack, offering stronger adversarial
effectiveness in complex physical environments. However, most prior work relies
on mesh priors of the target object and virtual environments constructed by
simulators, which are time-consuming to obtain and inevitably differ from the
real world. Moreover, due to the limitations of the backgrounds in training
images, previous methods often fail to produce multi-view robust adversarial
camouflage and tend to fall into sub-optimal solutions. Due to these reasons,
prior work lacks adversarial effectiveness and robustness across diverse
viewpoints and physical environments. We propose a physical attack framework
based on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and
precise reconstruction with few images, along with photo-realistic rendering
capabilities. Our framework further enhances cross-view robustness and
adversarial effectiveness by preventing mutual and self-occlusion among
Gaussians and employing a min-max optimization approach that adjusts the
imaging background of each viewpoint, helping the algorithm filter out
non-robust adversarial features. Extensive experiments validate the
effectiveness and superiority of PGA. Our code is available
at:https://github.com/TRLou/PGA.

</details>


### [16] [Activation Reward Models for Few-Shot Model Alignment](https://arxiv.org/abs/2507.01368)
*Tianning Chai,Chancharik Mitra,Brandon Huang,Gautam Rajendrakumar Gare,Zhiqiu Lin,Assaf Arbelle,Leonid Karlinsky,Rogerio Feris,Trevor Darrell,Deva Ramanan,Roei Herzig*

Main category: cs.CV

TL;DR: 提出了一种新的奖励建模方法——激活奖励模型(Activation RMs)，通过少量监督和无需模型重新微调即可实现对人类偏好的高效对齐，并在多个标准测试中表现优越。


<details>
  <summary>Details</summary>
Motivation: 解决当前奖励建模不易适应新偏好的问题，以及减轻奖励盗取行为在安全关键应用中可能带来的风险。

Method: 引入激活奖励模型(Activation RMs)，通过利用激活导向（activation steering）技术，以少量监督构建精确的奖励信号，无需额外的模型微调。

Result: 激活奖励模型在标准奖励建模基准测试中表现优于现有的少样本奖励建模方法，并成功减轻奖励盗取行为。此外，提出了PreferenceHack基准，用于测试奖励模型在奖励盗取场景下的性能，且Activation RM在该基准上表现超越了GPT-4o。

Conclusion: 激活奖励模型不仅提高了奖励建模的适应性和效率，还展现出在安全关键情境中的潜力，是一种新型的高效奖励建模方法。

Abstract: Aligning Large Language Models (LLMs) and Large Multimodal Models (LMMs) to
human preferences is a central challenge in improving the quality of the
models' generative outputs for real-world applications. A common approach is to
use reward modeling to encode preferences, enabling alignment via post-training
using reinforcement learning. However, traditional reward modeling is not
easily adaptable to new preferences because it requires a separate reward
model, commonly trained on large preference datasets. To address this, we
introduce Activation Reward Models (Activation RMs) -- a novel few-shot reward
modeling method that leverages activation steering to construct well-aligned
reward signals using minimal supervision and no additional model finetuning.
Activation RMs outperform existing few-shot reward modeling approaches such as
LLM-as-a-judge with in-context learning, voting-based scoring, and token
probability scoring on standard reward modeling benchmarks. Furthermore, we
demonstrate the effectiveness of Activation RMs in mitigating reward hacking
behaviors, highlighting their utility for safety-critical applications. Toward
this end, we propose PreferenceHack, a novel few-shot setting benchmark, the
first to test reward models on reward hacking in a paired preference format.
Finally, we show that Activation RM achieves state-of-the-art performance on
this benchmark, surpassing even GPT-4o.

</details>


### [17] [Active Measurement: Efficient Estimation at Scale](https://arxiv.org/abs/2507.01372)
*Max Hamilton,Jinlin Lai,Wenlong Zhao,Subhransu Maji,Daniel Sheldon*

Main category: cs.CV

TL;DR: 提出了一种名为主动测量的AI模型，通过人机协作改进科学测量的精确性。


<details>
  <summary>Details</summary>
Motivation: 当前AI在科学发现中具有潜力但缺乏准确性和统计保证，本研究旨在改进这点。

Method: 设计了主动测量框架，采用AI预测测量值和人为标注结合，提高模型性能并优化测量估算。

Result: 通过引入新估计器及加权方案，主动测量在多项任务中降低了估算误差。

Conclusion: 主动测量在保持较少人工干预下实现了精确测量，展示了潜力和实用性。

Abstract: AI has the potential to transform scientific discovery by analyzing vast
datasets with little human effort. However, current workflows often do not
provide the accuracy or statistical guarantees that are needed. We introduce
active measurement, a human-in-the-loop AI framework for scientific
measurement. An AI model is used to predict measurements for individual units,
which are then sampled for human labeling using importance sampling. With each
new set of human labels, the AI model is improved and an unbiased Monte Carlo
estimate of the total measurement is refined. Active measurement can provide
precise estimates even with an imperfect AI model, and requires little human
effort when the AI model is very accurate. We derive novel estimators,
weighting schemes, and confidence intervals, and show that active measurement
reduces estimation error compared to alternatives in several measurement tasks.

</details>


### [18] [MUG: Pseudo Labeling Augmented Audio-Visual Mamba Network for Audio-Visual Video Parsing](https://arxiv.org/abs/2507.01384)
*Langyu Wang,Bingke Zhu,Yingying Chen,Yiyuan Zhang,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: 提出了一种基于伪标签增强技术的音频-视频解析模型MUG，显著提升了弱监督音视频解析任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督音视频解析方法由于模型架构和监督信息的限制，在提升事件级和片段级预测能力上存在不足。

Method: 提出了MUG模型，通过基于过去工作的伪标签注释及跨模态的随机组合生成新数据，结合音视频Mamba网络实现特征处理与交互。

Result: 在LLP数据集上取得了多个指标上领先的性能，其中视觉片段级和音频片段级指标分别提升2.1%和1.2%。

Conclusion: MUG模型能够增强对不同片段的感知能力，并在排除其他模态噪声干扰的同时有效结合相似模态信息。代码已开源。

Abstract: The weakly-supervised audio-visual video parsing (AVVP) aims to predict all
modality-specific events and locate their temporal boundaries. Despite
significant progress, due to the limitations of the weakly-supervised and the
deficiencies of the model architecture, existing methods are lacking in
simultaneously improving both the segment-level prediction and the event-level
prediction. In this work, we propose a audio-visual Mamba network with pseudo
labeling aUGmentation (MUG) for emphasising the uniqueness of each segment and
excluding the noise interference from the alternate modalities. Specifically,
we annotate some of the pseudo-labels based on previous work. Using unimodal
pseudo-labels, we perform cross-modal random combinations to generate new data,
which can enhance the model's ability to parse various segment-level event
combinations. For feature processing and interaction, we employ a audio-visual
mamba network. The AV-Mamba enhances the ability to perceive different segments
and excludes additional modal noise while sharing similar modal information.
Our extensive experiments demonstrate that MUG improves state-of-the-art
results on LLP dataset in all metrics (e.g,, gains of 2.1% and 1.2% in terms of
visual Segment-level and audio Segment-level metrics). Our code is available at
https://github.com/WangLY136/MUG.

</details>


### [19] [FixTalk: Taming Identity Leakage for High-Quality Talking Head Generation in Extreme Cases](https://arxiv.org/abs/2507.01390)
*Shuai Tan,Bill Gong,Bin Ji,Ye Pan*

Main category: cs.CV

TL;DR: 本文提出FixTalk，解决说话人生成中的身份泄漏和渲染伪影问题，显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有说话人生成技术面临身份泄漏和渲染伪影，尤其在极端情况表现不佳，需要提升质量。

Method: 通过分析，提出Enhanced Motion Indicator (EMI)用于解耦运动特征中的身份信息，降低身份泄漏影响；引入Enhanced Detail Indicator (EDI)利用泄漏信息修复细节，改善伪影。

Result: FixTalk模型通过大量实验显示在缓解身份泄漏和渲染伪影方面优于现有方法，并实现更优生成表现。

Conclusion: FixTalk有效解决了说话人生成中的关键问题，为高质量生成提供新方案，与现有方法相比具备显著优势。

Abstract: Talking head generation is gaining significant importance across various
domains, with a growing demand for high-quality rendering. However, existing
methods often suffer from identity leakage (IL) and rendering artifacts (RA),
particularly in extreme cases. Through an in-depth analysis of previous
approaches, we identify two key insights: (1) IL arises from identity
information embedded within motion features, and (2) this identity information
can be leveraged to address RA. Building on these findings, this paper
introduces FixTalk, a novel framework designed to simultaneously resolve both
issues for high-quality talking head generation. Firstly, we propose an
Enhanced Motion Indicator (EMI) to effectively decouple identity information
from motion features, mitigating the impact of IL on generated talking heads.
To address RA, we introduce an Enhanced Detail Indicator (EDI), which utilizes
the leaked identity information to supplement missing details, thus fixing the
artifacts. Extensive experiments demonstrate that FixTalk effectively mitigates
IL and RA, achieving superior performance compared to state-of-the-art methods.

</details>


### [20] [Coherent Online Road Topology Estimation and Reasoning with Standard-Definition Maps](https://arxiv.org/abs/2507.01397)
*Khanh Son Pham,Christian Witte,Jens Behley,Johannes Betz,Cyrill Stachniss*

Main category: cs.CV

TL;DR: 该研究提出一种利用标准清晰度地图（SD地图）进行高精地图元素预测的新方法。


<details>
  <summary>Details</summary>
Motivation: 目前的自动驾驶汽车大多依赖高精地图，而生成高精地图需要解决道路拓扑的复杂性以及传感器预测与交通元素的关系。

Method: 该方法通过网络架构结合SD地图信息，利用混合车道段编码和降噪技术增强稳定性，同时结合历史帧以确保时序一致性。

Result: 实验表明，该方法在性能上显著优于以往方法。

Conclusion: 利用SD地图和新的方法能够提高高精地图的在线生成效率和一致性。

Abstract: Most autonomous cars rely on the availability of high-definition (HD) maps.
Current research aims to address this constraint by directly predicting HD map
elements from onboard sensors and reasoning about the relationships between the
predicted map and traffic elements. Despite recent advancements, the coherent
online construction of HD maps remains a challenging endeavor, as it
necessitates modeling the high complexity of road topologies in a unified and
consistent manner. To address this challenge, we propose a coherent approach to
predict lane segments and their corresponding topology, as well as road
boundaries, all by leveraging prior map information represented by commonly
available standard-definition (SD) maps. We propose a network architecture,
which leverages hybrid lane segment encodings comprising prior information and
denoising techniques to enhance training stability and performance.
Furthermore, we facilitate past frames for temporal consistency. Our
experimental evaluation demonstrates that our approach outperforms previous
methods by a large margin, highlighting the benefits of our modeling scheme.

</details>


### [21] [Medical-Knowledge Driven Multiple Instance Learning for Classifying Severe Abdominal Anomalies on Prenatal Ultrasound](https://arxiv.org/abs/2507.01401)
*Huanwen Liang,Jingxian Xu,Yuanji Zhang,Yuhao Huang,Yuhan Zhang,Xin Yang,Ran Li,Xuedong Deng,Yanjun Liu,Guowei Tao,Yun Wu,Sheng Zhao,Xinru Gao,Dong Ni*

Main category: cs.CV

TL;DR: 提出了无需标准平面定位的病例级胎儿腹部异常分类方法，通过混合注意力专家模块(MoAE)、医学知识驱动的特征选择模块(MFS)及提示式原型学习(PPL)提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决胎儿腹部异常诊断中病例级分类的需求，弥补现有图像级分类方法的不足，且降低因标准平面定位难度带来的技术限制。

Method: 采用多实例学习框架，结合MoAE模块对不同平面进行加权，MFS模块基于医学知识对图像特征优选，PPL模块提升特征选择性能。

Result: 在包含2419例病例（24748张图像，6类）的大规模胎儿超声数据集上验证，结果优于当前最先进方法。

Conclusion: 新方法显著提高了胎儿腹部异常案例的分类准确性，解决了标准平面依赖问题，为临床孕期管理提供支持。

Abstract: Fetal abdominal malformations are serious congenital anomalies that require
accurate diagnosis to guide pregnancy management and reduce mortality. Although
AI has demonstrated significant potential in medical diagnosis, its application
to prenatal abdominal anomalies remains limited. Most existing studies focus on
image-level classification and rely on standard plane localization, placing
less emphasis on case-level diagnosis. In this paper, we develop a case-level
multiple instance learning (MIL)-based method, free of standard plane
localization, for classifying fetal abdominal anomalies in prenatal ultrasound.
Our contribution is three-fold. First, we adopt a mixture-of-attention-experts
module (MoAE) to weight different attention heads for various planes. Secondly,
we propose a medical-knowledge-driven feature selection module (MFS) to align
image features with medical knowledge, performing self-supervised image token
selection at the case-level. Finally, we propose a prompt-based prototype
learning (PPL) to enhance the MFS. Extensively validated on a large prenatal
abdominal ultrasound dataset containing 2,419 cases, with a total of 24,748
images and 6 categories, our proposed method outperforms the state-of-the-art
competitors. Codes are available at:https://github.com/LL-AC/AAcls.

</details>


### [22] [CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning](https://arxiv.org/abs/2507.01409)
*Kuniaki Saito,Donghyun Kim,Kwanyong Park,Atsushi Hashimoto,Yoshitaka Ushiku*

Main category: cs.CV

TL;DR: 他们提出了CaptionSmiths，用于生成具有不同语言模式的图像标题。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型难以控制生成标题的属性，例如描述性或长度。

Method: 通过量化标题属性（长度、描述性、词汇独特性）为连续标量值，并以插值表示条件，开发单一模型提供多样语言模式。

Result: 模型在标题长度控制误差减少506%的同时，表现出更高的词汇对齐度。

Conclusion: CaptionSmiths在生成变换语言模式的标题上展现出优越能力，提供更多灵活应用潜力。

Abstract: An image captioning model flexibly switching its language pattern, e.g.,
descriptiveness and length, should be useful since it can be applied to diverse
applications. However, despite the dramatic improvement in generative
vision-language models, fine-grained control over the properties of generated
captions is not easy due to two reasons: (i) existing models are not given the
properties as a condition during training and (ii) existing models cannot
smoothly transition its language pattern from one state to the other. Given
this challenge, we propose a new approach, CaptionSmiths, to acquire a single
captioning model that can handle diverse language patterns. First, our approach
quantifies three properties of each caption, length, descriptiveness, and
uniqueness of a word, as continuous scalar values, without human annotation.
Given the values, we represent the conditioning via interpolation between two
endpoint vectors corresponding to the extreme states, e.g., one for a very
short caption and one for a very long caption. Empirical results demonstrate
that the resulting model can smoothly change the properties of the output
captions and show higher lexical alignment than baselines. For instance,
CaptionSmiths reduces the error in controlling caption length by 506\% despite
better lexical alignment. Code will be available on
https://github.com/omron-sinicx/captionsmiths.

</details>


### [23] [Gradient Short-Circuit: Efficient Out-of-Distribution Detection via Feature Intervention](https://arxiv.org/abs/2507.01417)
*Jiawei Gu,Ziyue Qiao,Zechao Li*

Main category: cs.CV

TL;DR: 本文提出了一种高效轻量的推理阶段方法，利用局部梯度现象改进OOD检测性能，避免重计算，针对真实世界中实际部署场景。


<details>
  <summary>Details</summary>
Motivation: 由于深度模型在开放世界中需要对OOD输入进行稳健检测，但传统方法在这种情况下易失败，因此需要一种高效且实际的改进方法。

Method: 通过探索深度模型在训练分布(ID)数据上的局部梯度一致性现象，针对OOD数据的混乱梯度，提出了一种短路特定特征通道的推理阶段方法；同时利用局部一阶近似替代昂贵的二次前向传播计算。

Result: 在标准的OOD基准测试中表现出显著的性能改进，且方法轻量级，对标准推理流水线的影响较小。

Conclusion: 提出的方法在增加推理效率和鲁棒性上非常实用，为真实世界中的OOD检测提供了可靠的解决方案。

Abstract: Out-of-Distribution (OOD) detection is critical for safely deploying deep
models in open-world environments, where inputs may lie outside the training
distribution. During inference on a model trained exclusively with
In-Distribution (ID) data, we observe a salient gradient phenomenon: around an
ID sample, the local gradient directions for "enhancing" that sample's
predicted class remain relatively consistent, whereas OOD samples--unseen in
training--exhibit disorganized or conflicting gradient directions in the same
neighborhood. Motivated by this observation, we propose an inference-stage
technique to short-circuit those feature coordinates that spurious gradients
exploit to inflate OOD confidence, while leaving ID classification largely
intact. To circumvent the expense of recomputing the logits after this gradient
short-circuit, we further introduce a local first-order approximation that
accurately captures the post-modification outputs without a second forward
pass. Experiments on standard OOD benchmarks show our approach yields
substantial improvements. Moreover, the method is lightweight and requires
minimal changes to the standard inference pipeline, offering a practical path
toward robust OOD detection in real-world applications.

</details>


### [24] [DocShaDiffusion: Diffusion Model in Latent Space for Document Image Shadow Removal](https://arxiv.org/abs/2507.01422)
*Wenjie Liu,Bingshu Wang,Ze Wang,C. L. Philip Chen*

Main category: cs.CV

TL;DR: 本文提出了一个用于文档图像阴影去除的扩散模型DocShaDiffusion, 能有效应对彩色阴影, 并在多个数据集上取得了优越表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法对去除阴影的处理限制在单色背景上，对彩色阴影不够关注，本研究旨在解决彩色阴影去除的难点。

Method: 设计了DocShaDiffusion扩散模型，将像素空间转换为潜变量空间，通过SSGM模块生成精准阴影掩码并加入噪声，利用SMGDM模块在扩散与去噪过程中引导模型去除阴影。此外提出了一种阴影鲁棒的感知特征损失，并创建了SDCSRD彩色阴影数据集。

Result: 实验表明，文中方法在3个公开数据集上优于现有的最先进方法，展现出了彩色阴影去除的显著性能提升。

Conclusion: 提出的DocShaDiffusion模型能够有效去除文档图像中的彩色阴影，保留图像细节结构，同时提供了新的大规模彩色阴影去除数据集，推动了该领域的进步。

Abstract: Document shadow removal is a crucial task in the field of document image
enhancement. However, existing methods tend to remove shadows with constant
color background and ignore color shadows. In this paper, we first design a
diffusion model in latent space for document image shadow removal, called
DocShaDiffusion. It translates shadow images from pixel space to latent space,
enabling the model to more easily capture essential features. To address the
issue of color shadows, we design a shadow soft-mask generation module (SSGM).
It is able to produce accurate shadow mask and add noise into shadow regions
specially. Guided by the shadow mask, a shadow mask-aware guided diffusion
module (SMGDM) is proposed to remove shadows from document images by
supervising the diffusion and denoising process. We also propose a
shadow-robust perceptual feature loss to preserve details and structures in
document images. Moreover, we develop a large-scale synthetic document color
shadow removal dataset (SDCSRD). It simulates the distribution of realistic
color shadows and provides powerful supports for the training of models.
Experiments on three public datasets validate the proposed method's superiority
over state-of-the-art. Our code and dataset will be publicly available.

</details>


### [25] [DiffMark: Diffusion-based Robust Watermark Against Deepfakes](https://arxiv.org/abs/2507.01428)
*Chen Sun,Haiyang Sun,Zhiqing Guo,Yunfeng Diao,Liejun Wang,Dan Ma,Gaobo Yang,Keqin Li*

Main category: cs.CV

TL;DR: DiffMark 是一种通过扩散模型实现的鲁棒水印嵌入框架，专注于抵抗 Deepfake 操作。通过条件生成和跨信息融合，能更高效地生成与水印融合的图像并提高防伪鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决现有水印技术在抗击 Deepfake 操作方面的鲁棒性不足问题。

Method: 通过扩散模型，将面部图像和水印作为条件，采用带时间步长因子的面部条件构造及跨信息融合模块，实现水印与图像的无缝融合；利用冻结的自动编码器模拟 Deepfake 操作，并通过对抗性指导增强水印鲁棒性。

Result: 实验显示 DiffMark 在典型 Deepfake 情景下表现出色，提高了水印的抗伪造能力。

Conclusion: DiffMark 提供一种创新且鲁棒的水印生成方法，为抵御 Deepfake 图像操纵提供了有效解决方案。

Abstract: Deepfakes pose significant security and privacy threats through malicious
facial manipulations. While robust watermarking can aid in authenticity
verification and source tracking, existing methods often lack the sufficient
robustness against Deepfake manipulations. Diffusion models have demonstrated
remarkable performance in image generation, enabling the seamless fusion of
watermark with image during generation. In this study, we propose a novel
robust watermarking framework based on diffusion model, called DiffMark. By
modifying the training and sampling scheme, we take the facial image and
watermark as conditions to guide the diffusion model to progressively denoise
and generate corresponding watermarked image. In the construction of facial
condition, we weight the facial image by a timestep-dependent factor that
gradually reduces the guidance intensity with the decrease of noise, thus
better adapting to the sampling process of diffusion model. To achieve the
fusion of watermark condition, we introduce a cross information fusion (CIF)
module that leverages a learnable embedding table to adaptively extract
watermark features and integrates them with image features via cross-attention.
To enhance the robustness of the watermark against Deepfake manipulations, we
integrate a frozen autoencoder during training phase to simulate Deepfake
manipulations. Additionally, we introduce Deepfake-resistant guidance that
employs specific Deepfake model to adversarially guide the diffusion sampling
process to generate more robust watermarked images. Experimental results
demonstrate the effectiveness of the proposed DiffMark on typical Deepfakes.
Our code will be available at https://github.com/vpsg-research/DiffMark.

</details>


### [26] [TurboReg: TurboClique for Robust and Efficient Point Cloud Registration](https://arxiv.org/abs/2507.01439)
*Shaocheng Yan,Pengcheng Shi,Zhenjun Zhao,Kaixin Wang,Kuang Cao,Ji Wu,Jiayuan Li*

Main category: cs.CV

TL;DR: TurboReg提出了一种基于轻量级TurboClique和高度并行的Pivot-Guided Search算法的快速稳健点云配准方法，显著提高速度和精度。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于兼容性图的点云配准方法因指数时间复杂度导致的时间敏感性问题。

Method: 引入TurboClique（3-团）和Pivot-Guided Search（PGS）算法，通过高SC^2分数的匹配对作为枢轴，提高搜索效率和稳健性。PGS算法时间复杂度为线性，大幅优于最大团搜索的指数复杂度。

Result: TurboReg在多个真实数据集上实现了最先进的性能，显著提高配准速度。例如，在3DMatch+FCGF数据集上，TurboReg（1K）比3DMAC快208.22倍，并获得更高的召回率。

Conclusion: TurboReg通过创新算法结合实现了高效和鲁棒的点云配准，适合时间敏感的应用场景。

Abstract: Robust estimation is essential in correspondence-based Point Cloud
Registration (PCR). Existing methods using maximal clique search in
compatibility graphs achieve high recall but suffer from exponential time
complexity, limiting their use in time-sensitive applications. To address this
challenge, we propose a fast and robust estimator, TurboReg, built upon a novel
lightweight clique, TurboClique, and a highly parallelizable Pivot-Guided
Search (PGS) algorithm. First, we define the TurboClique as a 3-clique within a
highly-constrained compatibility graph. The lightweight nature of the 3-clique
allows for efficient parallel searching, and the highly-constrained
compatibility graph ensures robust spatial consistency for stable
transformation estimation. Next, PGS selects matching pairs with high SC$^2$
scores as pivots, effectively guiding the search toward TurboCliques with
higher inlier ratios. Moreover, the PGS algorithm has linear time complexity
and is significantly more efficient than the maximal clique search with
exponential time complexity. Extensive experiments show that TurboReg achieves
state-of-the-art performance across multiple real-world datasets, with
substantial speed improvements. For example, on the 3DMatch+FCGF dataset,
TurboReg (1K) operates $208.22\times$ faster than 3DMAC while also achieving
higher recall. Our code is accessible at
\href{https://github.com/Laka-3DV/TurboReg}{\texttt{TurboReg}}.

</details>


### [27] [OoDDINO:A Multi-level Framework for Anomaly Segmentation on Complex Road Scenes](https://arxiv.org/abs/2507.01455)
*Yuxing Liu,Ji Zhang,Zhou Xuchuan,Jingzhong Xiao,Huimin Yang,Jiaxin Zhong*

Main category: cs.CV

TL;DR: 文章提出了一个多层级异常分割框架OoDDINO，通过不确定性引导的两阶段方法策略，提高异常分割的精度和效果。


<details>
  <summary>Details</summary>
Motivation: 现有像素级异常分割方法未充分利用像素间的空间相关性，并且全局阈值处理策略导致分割结果在真实世界情况中表现不佳。

Method: 论文提出一种称为OoDDINO的框架，通过两个阶段的流程：首先采用不确定性引导的异常检测模型与视觉表达融合，并引入正交约束来更精准地定位异常区域；其次设计了自适应双阈值网络ADT-Net，基于区域检测输出及像素级异常分数生成区域特定的阈值策略，解决背景误判或遗漏的情况。

Result: 在两个基准数据集上的实验结果表明，OoDDINO框架相比目前最先进方法表现优越，且具备很好的兼容性。

Conclusion: OoDDINO框架通过一系列创新策略提升了异常分割任务的效果，对现有像素级异常检测模型具有增强作用，同时具备较强的适用性。

Abstract: Anomaly segmentation aims to identify Out-of-Distribution (OoD) anomalous
objects within images. Existing pixel-wise methods typically assign anomaly
scores individually and employ a global thresholding strategy to segment
anomalies. Despite their effectiveness, these approaches encounter significant
challenges in real-world applications: (1) neglecting spatial correlations
among pixels within the same object, resulting in fragmented segmentation; (2)
variabil ity in anomaly score distributions across image regions, causing
global thresholds to either generate false positives in background areas or
miss segments of anomalous objects. In this work, we introduce OoDDINO, a novel
multi-level anomaly segmentation framework designed to address these
limitations through a coarse-to-fine anomaly detection strategy. OoDDINO
combines an uncertainty-guided anomaly detection model with a pixel-level
segmentation model within a two-stage cascade architecture. Initially, we
propose an Orthogonal Uncertainty-Aware Fusion Strategy (OUAFS) that
sequentially integrates multiple uncertainty metrics with visual
representations, employing orthogonal constraints to strengthen the detection
model's capacity for localizing anomalous regions accurately. Subsequently, we
develop an Adaptive Dual-Threshold Network (ADT-Net), which dynamically
generates region-specific thresholds based on object-level detection outputs
and pixel-wise anomaly scores. This approach allows for distinct thresholding
strategies within foreground and background areas, achieving fine-grained
anomaly segmentation. The proposed framework is compatible with other
pixel-wise anomaly detection models, which acts as a plug-in to boost the
performance. Extensive experiments on two benchmark datasets validate our
framework's superiority and compatibility over state-of-the-art methods.

</details>


### [28] [NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation](https://arxiv.org/abs/2507.01463)
*Max Gandyra,Alessandro Santonicola,Michael Beetz*

Main category: cs.CV

TL;DR: 提出了一个无需再训练即可处理新物体实例分割的简单高效框架NOCTIS，借助Grounded-SAM 2和DINOv2实现了优于现有技术的性能，并在BOP 2023挑战中取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 解决在RGB图像中对新物体实例进行分割的问题，避免对模型进行重新训练，实现更通用的应用。

Method: 利用Grounded-SAM 2生成物体提案，获取精确边界框和分割掩码；通过DINOv2的零样本能力生成图像嵌入；结合嵌入和平均最大相似性的匹配评分机制实现提案-物体的匹配。此外，加入基于循环路径过滤和提案置信度的得分加权策略。

Result: NOCTIS在无需进一步训练和微调的情况下，超越了现有RGB及RGB-D方法，在BOP 2023挑战的七个核心数据集上实现了最佳结果。

Conclusion: NOCTIS框架凭借创新机制和视觉基础模型的优势，实现了对新物体的高效实例分割，无需额外训练也能显著提升性能。

Abstract: Instance segmentation of novel objects instances in RGB images, given some
example images for each object, is a well known problem in computer vision.
Designing a model general enough to be employed, for all kinds of novel
objects, without (re-) training, has proven to be a difficult task. To handle
this, we propose a simple, yet powerful, framework, called: Novel Object Cyclic
Threshold based Instance Segmentation (NOCTIS). This work stems from and
improves upon previous ones like CNOS, SAM-6D and NIDS-Net; thus, it also
leverages on recent vision foundation models, namely: Grounded-SAM 2 and
DINOv2. It utilises Grounded-SAM 2 to obtain object proposals with precise
bounding boxes and their corresponding segmentation masks; while DINOv2's
zero-shot capabilities are employed to generate the image embeddings. The
quality of those masks, together with their embeddings, is of vital importance
to our approach; as the proposal-object matching is realized by determining an
object matching score based on the similarity of the class embeddings and the
average maximum similarity of the patch embeddings. Differently to SAM-6D,
calculating the latter involves a prior patch filtering based on the distance
between each patch and its corresponding cyclic/roundtrip patch in the image
grid. Furthermore, the average confidence of the proposals' bounding box and
mask is used as an additional weighting factor for the object matching score.
We empirically show that NOCTIS, without further training/fine tuning,
outperforms the best RGB and RGB-D methods on the seven core datasets of the
BOP 2023 challenge for the "Model-based 2D segmentation of unseen objects"
task.

</details>


### [29] [Representation Entanglement for Generation:Training Diffusion Transformers Is Much Easier Than You Think](https://arxiv.org/abs/2507.01467)
*Ge Wu,Shen Zhang,Ruijing Shi,Shanghua Gao,Zhenyuan Chen,Lei Wang,Zhaowei Chen,Hongcheng Gao,Yao Tang,Jian Yang,Ming-Ming Cheng,Xiang Li*

Main category: cs.CV

TL;DR: 本文提出REG方法，通过融合基础模型的高层语义标识与图像潜编码以改进扩散模型的训练及生成表现。


<details>
  <summary>Details</summary>
Motivation: 目前的扩散模型训练挑战在于噪声影像隐藏表示与干净影像表示的对齐不足。

Method: 提出REG方法，将低层次影像编码与一个基础模型的高层类别标识进行结合，用于加快训练并提升生成质量。

Result: 在ImageNet基准测试上，REG大幅加速了模型训练，并在效率和效果上显著优于现有方法。

Conclusion: REG方法通过低额外计算开销引入语义指导生成过程，显著提升了扩散模型的性能和训练效率。

Abstract: REPA and its variants effectively mitigate training challenges in diffusion
models by incorporating external visual representations from pretrained models,
through alignment between the noisy hidden projections of denoising networks
and foundational clean image representations. We argue that the external
alignment, which is absent during the entire denoising inference process, falls
short of fully harnessing the potential of discriminative representations. In
this work, we propose a straightforward method called Representation
Entanglement for Generation (REG), which entangles low-level image latents with
a single high-level class token from pretrained foundation models for
denoising. REG acquires the capability to produce coherent image-class pairs
directly from pure noise, substantially improving both generation quality and
training efficiency. This is accomplished with negligible additional inference
overhead, requiring only one single additional token for denoising (<0.5\%
increase in FLOPs and latency). The inference process concurrently reconstructs
both image latents and their corresponding global semantics, where the acquired
semantic knowledge actively guides and enhances the image generation process.
On ImageNet 256$\times$256, SiT-XL/2 + REG demonstrates remarkable convergence
acceleration, achieving $\textbf{63}\times$ and $\textbf{23}\times$ faster
training than SiT-XL/2 and SiT-XL/2 + REPA, respectively. More impressively,
SiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA
trained for 4M iterations ($\textbf{10}\times$ longer). Code is available at:
https://github.com/Martinser/REG.

</details>


### [30] [Optimizing Methane Detection On Board Satellites: Speed, Accuracy, and Low-Power Solutions for Resource-Constrained Hardware](https://arxiv.org/abs/2507.01472)
*Jonáš Herec,Vít Růžička,Rado Pitoňák*

Main category: cs.CV

TL;DR: 该研究利用高光谱卫星图像检测甲烷泄漏，通过提出计算效率更高的算法Mag1c-SAS和引入新目标检测方法（ACE，CEM），提升了在资源受限硬件上的处理速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 为了减少甲烷泄漏导致的气候变化，高效检测甲烷成为关键。然而，传统检测方法计算需求高，不适合资源受限的卫星板载硬件。

Method: 提出了一种改进的检测算法Mag1c-SAS，并引入ACE、CEM两种未被用于甲烷检测的快速目标检测算法，同时与机器学习模型（U-Net、LinkNet）集成。此外，设计并比较了三种波段选择策略以优化性能。

Result: 研究显示，Mag1c-SAS和CEM在检测强甲烷羽流时表现出准确性和计算效率的平衡，处理速度比传统算法提升了100至230倍。同时，一种新的波段选择策略在减少使用通道的前提下提高了处理速度而不损失精度。

Conclusion: 该成果为资源有限硬件上的甲烷检测提供了高效解决方案，实现更及时的数据传输，并通过开源方式支持未来研究和应用。

Abstract: Methane is a potent greenhouse gas, and detecting its leaks early via
hyperspectral satellite imagery can help mitigate climate change. Meanwhile,
many existing missions operate in manual tasking regimes only, thus missing
potential events of interest. To overcome slow downlink rates cost-effectively,
onboard detection is a viable solution. However, traditional methane
enhancement methods are too computationally demanding for resource-limited
onboard hardware. This work accelerates methane detection by focusing on
efficient, low-power algorithms. We test fast target detection methods (ACE,
CEM) that have not been previously used for methane detection and propose a
Mag1c-SAS - a significantly faster variant of the current state-of-the-art
algorithm for methane detection: Mag1c. To explore their true detection
potential, we integrate them with a machine learning model (U-Net, LinkNet).
Our results identify two promising candidates (Mag1c-SAS and CEM), both
acceptably accurate for the detection of strong plumes and computationally
efficient enough for onboard deployment: one optimized more for accuracy, the
other more for speed, achieving up to ~100x and ~230x faster computation than
original Mag1c on resource-limited hardware. Additionally, we propose and
evaluate three band selection strategies. One of them can outperform the method
traditionally used in the field while using fewer channels, leading to even
faster processing without compromising accuracy. This research lays the
foundation for future advancements in onboard methane detection with minimal
hardware requirements, improving timely data delivery. The produced code, data,
and models are open-sourced and can be accessed from
https://github.com/zaitra/methane-filters-benchmark.

</details>


### [31] [Active Control Points-based 6DoF Pose Tracking for Industrial Metal Objects](https://arxiv.org/abs/2507.01478)
*Chentao Shen,Ding Pan,Mingyu Mei,Zaixing He,Xinyue Zhao*

Main category: cs.CV

TL;DR: 提出一种基于主动控制点的6自由度姿态追踪方法，解决工业金属物体反光干扰问题，已公开代码。


<details>
  <summary>Details</summary>
Motivation: 由于金属物体的反光特性，传统姿态追踪方法在工业环境中表现不佳。

Method: 基于主动控制点生成边缘特征用于优化，并引入最佳控制点回归方法以提高鲁棒性。

Result: 在数据集评估和真实任务中表现有效，能实时追踪工业金属物体。

Conclusion: 该方法为解决工业金属物体实时追踪问题提供了可行方案，代码已开源。

Abstract: Visual pose tracking is playing an increasingly vital role in industrial
contexts in recent years. However, the pose tracking for industrial metal
objects remains a challenging task especially in the real world-environments,
due to the reflection characteristic of metal objects. To address this issue,
we propose a novel 6DoF pose tracking method based on active control points.
The method uses image control points to generate edge feature for optimization
actively instead of 6DoF pose-based rendering, and serve them as optimization
variables. We also introduce an optimal control point regression method to
improve robustness. The proposed tracking method performs effectively in both
dataset evaluation and real world tasks, providing a viable solution for
real-time tracking of industrial metal objects. Our source code is made
publicly available at: https://github.com/tomatoma00/ACPTracking.

</details>


### [32] [What Really Matters for Robust Multi-Sensor HD Map Construction?](https://arxiv.org/abs/2507.01484)
*Xiaoshuai Hao,Yuting Zhao,Yuheng Ji,Luanyuan Dai,Peng Hao,Dingzhe Li,Shuai Cheng,Rong Yin*

Main category: cs.CV

TL;DR: 现有多模态HD地图构建方法主要关注精度，而忽略了鲁棒性，本文提出通过数据增强、新的多模态融合模块以及模态丢失训练策略来提升方法的鲁棒性，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合方法尽管在精度上已有突破，但忽略了模型鲁棒性，而它是实际应用中的关键问题，这促使研究如何增强HD地图构建方法的鲁棒性。

Method: 提出三项关键技术：数据增强、一种新型多模态融合模块以及模态丢失训练策略，并通过10天的NuScenes数据集进行了评估。

Result: 实验表明，与基线方法相比，提出的方法在鲁棒性和清晰验证集上的表现显著提升，达到了最新性能水平。

Conclusion: 研究结果为开发更鲁棒可靠的HD地图构建模型提供了重要见解，有助于其在自动驾驶领域的实际应用。

Abstract: High-definition (HD) map construction methods are crucial for providing
precise and comprehensive static environmental information, which is essential
for autonomous driving systems. While Camera-LiDAR fusion techniques have shown
promising results by integrating data from both modalities, existing approaches
primarily focus on improving model accuracy and often neglect the robustness of
perception models, which is a critical aspect for real-world applications. In
this paper, we explore strategies to enhance the robustness of multi-modal
fusion methods for HD map construction while maintaining high accuracy. We
propose three key components: data augmentation, a novel multi-modal fusion
module, and a modality dropout training strategy. These components are
evaluated on a challenging dataset containing 10 days of NuScenes data. Our
experimental results demonstrate that our proposed methods significantly
enhance the robustness of baseline methods. Furthermore, our approach achieves
state-of-the-art performance on the clean validation set of the NuScenes
dataset. Our findings provide valuable insights for developing more robust and
reliable HD map construction models, advancing their applicability in
real-world autonomous driving scenarios. Project website:
https://robomap-123.github.io.

</details>


### [33] [AVC-DPO: Aligned Video Captioning via Direct Preference Optimization](https://arxiv.org/abs/2507.01492)
*Jiyang Tang,Hengyi Li,Yifan Du,Wayne Xin Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种名为AVC-DPO的视频描述生成后训练框架，通过优化对人类偏好的对齐，实现了视频多模态大语言模型在视频描述任务中的显著改进，并在相关挑战中获得了第一名。


<details>
  <summary>Details</summary>
Motivation: 现有的视频多模态大语言模型虽然在视频描述任务中取得了一定进展，但难以根据人类偏好调整视频描述的重点。

Method: 提出了AVC-DPO后训练框架，通过设计聚焦时间动态和空间信息的增强提示，结合基于模型生成响应的偏好感知训练，实现视频描述与人类偏好的对齐。

Result: 在LOVE@CVPR'25的Track 1A视频详细描述挑战中取得了第一名，并在VDCSCORE评价指标下表现优异。

Conclusion: AVC-DPO框架成功提升了视频多模态大语言模型在视频描述任务中的性能，证明了偏好对齐方法的有效性。

Abstract: Although video multimodal large language models (video MLLMs) have achieved
substantial progress in video captioning tasks, it remains challenging to
adjust the focal emphasis of video captions according to human preferences. To
address this limitation, we propose Aligned Video Captioning via Direct
Preference Optimization (AVC-DPO), a post-training framework designed to
enhance captioning capabilities in video MLLMs through preference alignment.
Our approach designs enhanced prompts that specifically target temporal
dynamics and spatial information-two key factors that humans care about when
watching a video-thereby incorporating human-centric preferences. AVC-DPO
leverages the same foundation model's caption generation responses under varied
prompt conditions to conduct preference-aware training and caption alignment.
Using this framework, we have achieved exceptional performance in the
LOVE@CVPR'25 Workshop Track 1A: Video Detailed Captioning Challenge, achieving
first place on the Video Detailed Captioning (VDC) benchmark according to the
VDCSCORE evaluation metric.

</details>


### [34] [Crop Pest Classification Using Deep Learning Techniques: A Review](https://arxiv.org/abs/2507.01494)
*Muhammad Hassam Ejaz,Muhammad Bilal,Usman Habib*

Main category: cs.CV

TL;DR: 综述了2018-2025年的37项关于基于深度学习的害虫分类研究，分析了技术进展及其挑战。


<details>
  <summary>Details</summary>
Motivation: 解决传统害虫监测方法缓慢、人工化和难以扩展的问题，通过深度学习技术实现自动化害虫检测。

Method: 对近年37篇研究进行综述，根据作物种类、害虫种类、模型架构、数据集使用和技术挑战进行分类分析。

Result: 从CNN到混合模型与Transformer的转型提供了更高准确性和更优场景理解，但数据不平衡、小型害虫检测、泛化能力差和边缘设备部署的问题仍存。

Conclusion: 提供了害虫监测领域的结构化概述，指出可用数据集及未来研究方向，推动AI在害虫检测中的应用。

Abstract: Insect pests continue to bring a serious threat to crop yields around the
world, and traditional methods for monitoring them are often slow, manual, and
difficult to scale. In recent years, deep learning has emerged as a powerful
solution, with techniques like convolutional neural networks (CNNs), vision
transformers (ViTs), and hybrid models gaining popularity for automating pest
detection. This review looks at 37 carefully selected studies published between
2018 and 2025, all focused on AI-based pest classification. The selected
research is organized by crop type, pest species, model architecture, dataset
usage, and key technical challenges. The early studies relied heavily on CNNs
but latest work is shifting toward hybrid and transformer-based models that
deliver higher accuracy and better contextual understanding. Still, challenges
like imbalanced datasets, difficulty in detecting small pests, limited
generalizability, and deployment on edge devices remain significant hurdles.
Overall, this review offers a structured overview of the field, highlights
useful datasets, and outlines the key challenges and future directions for
AI-based pest monitoring systems.

</details>


### [35] [ReFlex: Text-Guided Editing of Real Images in Rectified Flow via Mid-Step Feature Extraction and Attention Adaptation](https://arxiv.org/abs/2507.01496)
*Jimyeong Kim,Jungwon Park,Yeji Song,Nojun Kwak,Wonjong Rhee*

Main category: cs.CV

TL;DR: 提出了一种改进ReFlow真实图像编辑的方法，利用中间步骤潜变量，改进编辑能力和文本对齐，无需额外训练和用户提供的掩码，在多项基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前ReFlow模型在图像质量和文本对齐上表现优于扩散模型，但在真实图像编辑上的适配仍是一个挑战。

Method: 通过分析多模态Transformer模块的中间表示，识别关键特征；利用中间步骤潜变量，仅逆转到中间步骤；并在注入过程中调整注意力机制以提高可编辑性和文本对齐。

Result: 通过在人类评估和两个基准测试中的比较，其方法显示出强大的编辑性能和用户偏好。

Conclusion: 新方法无需训练、掩码或源提示，显著提升了ReFlow在真实图像编辑中的适应性，优于现有方法。

Abstract: Rectified Flow text-to-image models surpass diffusion models in image quality
and text alignment, but adapting ReFlow for real-image editing remains
challenging. We propose a new real-image editing method for ReFlow by analyzing
the intermediate representations of multimodal transformer blocks and
identifying three key features. To extract these features from real images with
sufficient structural preservation, we leverage mid-step latent, which is
inverted only up to the mid-step. We then adapt attention during injection to
improve editability and enhance alignment to the target text. Our method is
training-free, requires no user-provided mask, and can be applied even without
a source prompt. Extensive experiments on two benchmarks with nine baselines
demonstrate its superior performance over prior methods, further validated by
human evaluations confirming a strong user preference for our approach.

</details>


### [36] [Integrating Traditional and Deep Learning Methods to Detect Tree Crowns in Satellite Images](https://arxiv.org/abs/2507.01502)
*Ozan Durgut,Beril Kallfelz-Sirmacek,Cem Unsalan*

Main category: cs.CV

TL;DR: 论文提出了一种结合传统方法和深度学习的规则性方法，用于增强树冠检测的准确性和鲁棒性，以监测森林并应对全球变暖及生物多样性丧失等环境问题。


<details>
  <summary>Details</summary>
Motivation: 传统森林监测手段较为局限，且自动化程度低，阻碍了快速高效应对全球变暖、生物多样性丧失和空气污染等环境问题的能力。

Method: 结合传统方法的特征提取与分割能力和深度学习方法的树冠检测能力，提出一种新的基于规则的后处理方法，通过邻近树木和局部操作提高树冠检测的数量和准确性。

Result: 使用规则性方法对结果进行后处理后，与其他方法对比显示，该方法能够提高树冠检测数量，并对方法进行了优缺点及改善点分析。

Conclusion: 该规则性方法在树冠检测表现上有优势，显示了将传统算法与深度学习结合的潜力，适用于未来的森林监测与保护应用。

Abstract: Global warming, loss of biodiversity, and air pollution are among the most
significant problems facing Earth. One of the primary challenges in addressing
these issues is the lack of monitoring forests to protect them. To tackle this
problem, it is important to leverage remote sensing and computer vision methods
to automate monitoring applications. Hence, automatic tree crown detection
algorithms emerged based on traditional and deep learning methods. In this
study, we first introduce two different tree crown detection methods based on
these approaches. Then, we form a novel rule-based approach that integrates
these two methods to enhance robustness and accuracy of tree crown detection
results. While traditional methods are employed for feature extraction and
segmentation of forested areas, deep learning methods are used to detect tree
crowns in our method. With the proposed rule-based approach, we post-process
these results, aiming to increase the number of detected tree crowns through
neighboring trees and localized operations. We compare the obtained results
with the proposed method in terms of the number of detected tree crowns and
report the advantages, disadvantages, and areas for improvement of the obtained
outcomes.

</details>


### [37] [Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence](https://arxiv.org/abs/2507.01504)
*Robert Aufschläger,Youssef Shoeb,Azarm Nowzad,Michael Heigl,Fabian Bally,Martin Schramm*

Main category: cs.CV

TL;DR: 本文提出了一种将大规模视觉语言模型、图注意力网络和表示学习结合的跨模态框架cRID，有效检测超出生物特征（如人脸）的可描述的个人身份信息（PII），并提高人重识别的性能。


<details>
  <summary>Details</summary>
Motivation: 街道级数据集作为开放数据虽能推动AI发展，但也面临隐私风险，尤其涉及超出生物特征的人物PII。

Method: 提出cRID框架，结合视觉语言模型、图神经网络和表示学习，用于检测语义上可解释的PII特征，并提升人重识别的跨数据集表现。

Result: 实验显示，在类似Market-1501到CUHK03-np的实际跨数据集Re-ID任务中性能得到提升，验证了框架的实用性。

Conclusion: cRID框架在保障数据隐私的基础上，能有效检测PII并提升人重识别性能，为开放数据隐私保护提供新思路。

Abstract: The collection and release of street-level recordings as Open Data play a
vital role in advancing autonomous driving systems and AI research. However,
these datasets pose significant privacy risks, particularly for pedestrians,
due to the presence of Personally Identifiable Information (PII) that extends
beyond biometric traits such as faces. In this paper, we present cRID, a novel
cross-modal framework combining Large Vision-Language Models, Graph Attention
Networks, and representation learning to detect textual describable clues of
PII and enhance person re-identification (Re-ID). Our approach focuses on
identifying and leveraging interpretable features, enabling the detection of
semantically meaningful PII beyond low-level appearance cues. We conduct a
systematic evaluation of PII presence in person image datasets. Our experiments
show improved performance in practical cross-dataset Re-ID scenarios, notably
from Market-1501 to CUHK03-np (detected), highlighting the framework's
practical utility. Code is available at https://github.com/RAufschlaeger/cRID.

</details>


### [38] [Mamba Guided Boundary Prior Matters: A New Perspective for Generalized Polyp Segmentation](https://arxiv.org/abs/2507.01509)
*Tapas K. Dutta,Snehashis Majhi,Deepak Ranjan Nayak,Debesh Jha*

Main category: cs.CV

TL;DR: 提出了一种名为SAM-MaGuP的新方法，通过改进边界处理和特征学习，有效提高结肠镜图像中息肉分割的性能。


<details>
  <summary>Details</summary>
Motivation: 应对现有方法在处理弱或模糊边界的息肉时表现不稳的问题，并提升分割模型的实际应用性能。

Method: 将边界蒸馏模块和1D-2D Mamba适配器引入Segment Anything Model (SAM)，通过引入Mamba引导的边界先验和1D-2D Mamba模块加强边界信息捕获和全局上下文交互。

Result: 在五个不同数据集上的实验显示，SAM-MaGuP在准确性和鲁棒性上均超过现有方法。

Conclusion: SAM-MaGuP在息肉分割上树立了新标杆，为真实医疗场景提供了更高的应用潜力。

Abstract: Polyp segmentation in colonoscopy images is crucial for early detection and
diagnosis of colorectal cancer. However, this task remains a significant
challenge due to the substantial variations in polyp shape, size, and color, as
well as the high similarity between polyps and surrounding tissues, often
compounded by indistinct boundaries. While existing encoder-decoder CNN and
transformer-based approaches have shown promising results, they struggle with
stable segmentation performance on polyps with weak or blurry boundaries. These
methods exhibit limited abilities to distinguish between polyps and non-polyps
and capture essential boundary cues. Moreover, their generalizability still
falls short of meeting the demands of real-time clinical applications. To
address these limitations, we propose SAM-MaGuP, a groundbreaking approach for
robust polyp segmentation. By incorporating a boundary distillation module and
a 1D-2D Mamba adapter within the Segment Anything Model (SAM), SAM-MaGuP excels
at resolving weak boundary challenges and amplifies feature learning through
enriched global contextual interactions. Extensive evaluations across five
diverse datasets reveal that SAM-MaGuP outperforms state-of-the-art methods,
achieving unmatched segmentation accuracy and robustness. Our key innovations,
a Mamba-guided boundary prior and a 1D-2D Mamba block, set a new benchmark in
the field, pushing the boundaries of polyp segmentation to new heights.

</details>


### [39] [Exploring Pose-based Sign Language Translation: Ablation Studies and Attention Insights](https://arxiv.org/abs/2507.01532)
*Tomas Zelezny,Jakub Straka,Vaclav Javorek,Ondrej Valach,Marek Hruz,Ivan Gruber*

Main category: cs.CV

TL;DR: 本文研究了姿态数据预处理对手语翻译性能的影响，采用了基于Transformer的模型，并通过消融研究证明了规范化、插值和增强方法的效果。


<details>
  <summary>Details</summary>
Motivation: 探索数据预处理（如规范化、插值和增强）对手语翻译性能的提高作用。

Method: 基于Transformer的架构，采用改进的T5编码器-解码器模型处理姿态数据，通过YouTubeASL和How2Sign数据集进行广泛的消融研究。

Result: 适当的预处理策略显著提升了模型的鲁棒性和泛化能力；专门的寄存器标记可以进一步优化性能。

Conclusion: 姿态数据预处理是提升手语翻译系统效果的关键方法，研究提供了指导性预处理策略并开放代码和数据。

Abstract: Sign Language Translation (SLT) has evolved significantly, moving from
isolated recognition approaches to complex, continuous gloss-free translation
systems. This paper explores the impact of pose-based data preprocessing
techniques - normalization, interpolation, and augmentation - on SLT
performance. We employ a transformer-based architecture, adapting a modified T5
encoder-decoder model to process pose representations. Through extensive
ablation studies on YouTubeASL and How2Sign datasets, we analyze how different
preprocessing strategies affect translation accuracy. Our results demonstrate
that appropriate normalization, interpolation, and augmentation techniques can
significantly improve model robustness and generalization abilities.
Additionally, we provide a deep analysis of the model's attentions and reveal
interesting behavior suggesting that adding a dedicated register token can
improve overall model performance. We publish our code on our GitHub
repository, including the preprocessed YouTubeASL data.

</details>


### [40] [TrackingMiM: Efficient Mamba-in-Mamba Serialization for Real-time UAV Object Tracking](https://arxiv.org/abs/2507.01535)
*Bingxi Liu,Calvin Chen,Junhao Li,Guyang Yu,Haoqian Song,Xuchen Liu,Jinqiang Cui,Hong Zhang*

Main category: cs.CV

TL;DR: 文章研究了Vision Transformer (ViT)模型在无人机追踪中的局限性，并提出了基于状态空间模型Mamba的新方法TrackingMiM。


<details>
  <summary>Details</summary>
Motivation: 解决ViT模型在无人机实时追踪中的复杂度以及现有Mamba方法在时间一致性上的不足。

Method: 提出了TrackingMiM架构，即在Mamba模型中嵌套扫描，同时独立处理时间和空间相关的patch tokens，模板帧被编码为查询token并在每次扫描中用于追踪任务。

Result: 在五个无人机追踪基准上，通过实验验证TrackingMiM模型在精度和速度上都实现了最先进的水平。

Conclusion: TrackingMiM在保证无人机追踪性能的同时，提高了模型的运行速度，证明了其在实际场景中的适用性。

Abstract: The Vision Transformer (ViT) model has long struggled with the challenge of
quadratic complexity, a limitation that becomes especially critical in unmanned
aerial vehicle (UAV) tracking systems, where data must be processed in real
time. In this study, we explore the recently proposed State-Space Model, Mamba,
leveraging its computational efficiency and capability for long-sequence
modeling to effectively process dense image sequences in tracking tasks. First,
we highlight the issue of temporal inconsistency in existing Mamba-based
methods, specifically the failure to account for temporal continuity in the
Mamba scanning mechanism. Secondly, building upon this insight,we propose
TrackingMiM, a Mamba-in-Mamba architecture, a minimal-computation burden model
for handling image sequence of tracking problem. In our framework, the mamba
scan is performed in a nested way while independently process temporal and
spatial coherent patch tokens. While the template frame is encoded as query
token and utilized for tracking in every scan. Extensive experiments conducted
on five UAV tracking benchmarks confirm that the proposed TrackingMiM achieves
state-of-the-art precision while offering noticeable higher speed in UAV
tracking.

</details>


### [41] [A Multi-Centric Anthropomorphic 3D CT Phantom-Based Benchmark Dataset for Harmonization](https://arxiv.org/abs/2507.01539)
*Mohammadreza Amirian,Michael Bach,Oscar Jimenez-del-Toro,Christoph Aberle,Roger Schaer,Vincent Andrearczyk,Jean-Félix Maestrati,Maria Martin Asiain,Kyriakos Flouris,Markus Obmann,Clarisse Dromain,Benoît Dufour,Pierre-Alexandre Alois Poletti,Hendrik von Tengg-Kobligk,Rolf Hügli,Martin Kretzschmar,Hatem Alkadhi,Ender Konukoglu,Henning Müller,Bram Stieltjes,Adrien Depeursinge*

Main category: cs.CV

TL;DR: 本文提出了一个开放的基准数据集，包含通过多种CT扫描器和不同设置采集的扫描图像，旨在促进AI在CT分析中的标准化开发。


<details>
  <summary>Details</summary>
Motivation: 人工智能在医学中有广泛的应用，但由于数据分布的变化，其泛化能力表现较差。特别是在CT分析中，不同扫描器、重建技术或剂量等变化可能造成显著数据分布偏移，本研究旨在解决这一问题。

Method: 通过使用人形模拟器采集数据，排除病人内外因变量的影响，从13台扫描器以及多个制造商和机构中采集数据，并通过统一的协议和不同剂量完成数据收集。此外，提供了方法论和基准结果，用于评估图像及特征级别的稳定性和肝组织分类。

Result: 数据集包括1378组图像序列，由来自4个制造商、8个机构的设备在不同剂量下采集，并配套开源代码和基准方法推进AI标准化研究。

Conclusion: 该数据集为AI在CT分析中的标准化开发奠定基础，同时降低了数据采集时因设备差异导致的数据分布偏移。

Abstract: Artificial intelligence (AI) has introduced numerous opportunities for human
assistance and task automation in medicine. However, it suffers from poor
generalization in the presence of shifts in the data distribution. In the
context of AI-based computed tomography (CT) analysis, significant data
distribution shifts can be caused by changes in scanner manufacturer,
reconstruction technique or dose. AI harmonization techniques can address this
problem by reducing distribution shifts caused by various acquisition settings.
This paper presents an open-source benchmark dataset containing CT scans of an
anthropomorphic phantom acquired with various scanners and settings, which
purpose is to foster the development of AI harmonization techniques. Using a
phantom allows fixing variations attributed to inter- and intra-patient
variations. The dataset includes 1378 image series acquired with 13 scanners
from 4 manufacturers across 8 institutions using a harmonized protocol as well
as several acquisition doses. Additionally, we present a methodology, baseline
results and open-source code to assess image- and feature-level stability and
liver tissue classification, promoting the development of AI harmonization
strategies.

</details>


### [42] [Interpolation-Based Event Visual Data Filtering Algorithms](https://arxiv.org/abs/2507.01557)
*Marcin Kowlaczyk,Tomasz Kryjak*

Main category: cs.CV

TL;DR: 本研究提出了一种基于IIR滤波器矩阵的方法，能够有效滤除事件数据中的噪声，同时保留大部分信号。


<details>
  <summary>Details</summary>
Motivation: 在神经形态视觉领域，事件相机应用逐渐增多，但其数据流中存在显著的噪声，亟需有效的解决方案。

Method: 研究基于IIR滤波器矩阵设计了四种算法，并以多个数据集（包括人工生成和实际记录的噪声数据）进行测试与比较。

Result: 新算法能在过滤事件数据噪声的同时保存大部分有效信号，噪声去除效果达99%以上，同时算法仅需约30KB内存，适合在嵌入式设备上实现。

Conclusion: 此方法提供了高效、资源友好的事件数据噪声过滤方案，有助于提高事件相机在嵌入式和实际场景中的应用效果。

Abstract: The field of neuromorphic vision is developing rapidly, and event cameras are
finding their way into more and more applications. However, the data stream
from these sensors is characterised by significant noise. In this paper, we
propose a method for event data that is capable of removing approximately 99\%
of noise while preserving the majority of the valid signal. We have proposed
four algorithms based on the matrix of infinite impulse response (IIR) filters
method. We compared them on several event datasets that were further modified
by adding artificially generated noise and noise recorded with dynamic vision
sensor. The proposed methods use about 30KB of memory for a sensor with a
resolution of 1280 x 720 and is therefore well suited for implementation in
embedded devices.

</details>


### [43] [A Gift from the Integration of Discriminative and Diffusion-based Generative Learning: Boundary Refinement Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2507.01573)
*Hao Wang,Keyan Hu,Xin Guo,Haifeng Li,Chao Tao*

Main category: cs.CV

TL;DR: 该研究提出了一种名为IDGBR的框架，将判别学习和基于扩散的生成学习相结合，用于遥感语义分割中的边界优化，显著提高了边界精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法过于依赖判别学习，擅长捕获低频特征，但在学习高频特征（如边界细节）时存在不足；研究提出通过结合生成学习弥补这一缺点。

Method: 提出IDGBR框架：先利用判别模型生成粗分割图，随后将图像与粗分割提供给条件引导网络，生成指导表示，再通过迭代扩散过程优化边界精度。

Result: 在五个遥感语义分割数据集上的实验表明，该框架能显著优化不同判别架构的粗结果边界。

Conclusion: IDGBR框架证明了判别学习与扩散生成学习结合在边界优化中的优越性，代码即将公开。

Abstract: Remote sensing semantic segmentation must address both what the ground
objects are within an image and where they are located. Consequently,
segmentation models must ensure not only the semantic correctness of
large-scale patches (low-frequency information) but also the precise
localization of boundaries between patches (high-frequency information).
However, most existing approaches rely heavily on discriminative learning,
which excels at capturing low-frequency features, while overlooking its
inherent limitations in learning high-frequency features for semantic
segmentation. Recent studies have revealed that diffusion generative models
excel at generating high-frequency details. Our theoretical analysis confirms
that the diffusion denoising process significantly enhances the model's ability
to learn high-frequency features; however, we also observe that these models
exhibit insufficient semantic inference for low-frequency features when guided
solely by the original image. Therefore, we integrate the strengths of both
discriminative and generative learning, proposing the Integration of
Discriminative and diffusion-based Generative learning for Boundary Refinement
(IDGBR) framework. The framework first generates a coarse segmentation map
using a discriminative backbone model. This map and the original image are fed
into a conditioning guidance network to jointly learn a guidance representation
subsequently leveraged by an iterative denoising diffusion process refining the
coarse segmentation. Extensive experiments across five remote sensing semantic
segmentation datasets (binary and multi-class segmentation) confirm our
framework's capability of consistent boundary refinement for coarse results
from diverse discriminative architectures. The source code will be available at
https://github.com/KeyanHu-git/IDGBR.

</details>


### [44] [SketchColour: Channel Concat Guided DiT-based Sketch-to-Colour Pipeline for 2D Animation](https://arxiv.org/abs/2507.01586)
*Bryan Constantine Sadihin,Michael Hua Wang,Shei Pern Chua,Hang Su*

Main category: cs.CV

TL;DR: 提出了一个基于扩散式变压器（DiT）架构的2D动画彩色化工具SketchColour，用于高效的手绘动画上色。


<details>
  <summary>Details</summary>
Motivation: 当前，高质量2D动画的制作高度依赖人力，动画师需手动绘制和上色大量画面，生产效率低下。

Method: 本研究使用改良的扩散式变压器（DiT）架构取代传统U-Net去噪器，通过轻量级的通道拼接适配器和LoRA微调方式注入草图信息，减轻了参数需求和GPU存储压力。

Result: 在SAKUGA数据集上评测，SketchColour在使用仅一半训练数据的情况下，在所有指标上都超过了之前的最优方法，同时生成的动画时间上一致性高，颜色溢出现象和物体变形问题少。

Conclusion: SketchColour显著简化了2D动画上色的工作量，并提升了动画彩色化的效果与效率，其代码已开源。

Abstract: The production of high-quality 2D animation is highly labor-intensive
process, as animators are currently required to draw and color a large number
of frames by hand. We present SketchColour, the first sketch-to-colour pipeline
for 2D animation built on a diffusion transformer (DiT) backbone. By replacing
the conventional U-Net denoiser with a DiT-style architecture and injecting
sketch information via lightweight channel-concatenation adapters accompanied
with LoRA finetuning, our method natively integrates conditioning without the
parameter and memory bloat of a duplicated ControlNet, greatly reducing
parameter count and GPU memory usage. Evaluated on the SAKUGA dataset,
SketchColour outperforms previous state-of-the-art video colourization methods
across all metrics, despite using only half the training data of competing
models. Our approach produces temporally coherent animations with minimal
artifacts such as colour bleeding or object deformation. Our code is available
at: https://bconstantine.github.io/SketchColour .

</details>


### [45] [Towards Controllable Real Image Denoising with Camera Parameters](https://arxiv.org/abs/2507.01587)
*Youngjin Oh,Junhyeong Kwon,Keuntek Lee,Nam Ik Cho*

Main category: cs.CV

TL;DR: 本文提出了一种新的可控去噪框架，利用相机参数的信息调整图像去噪强度，同时提升了去噪网络的性能。


<details>
  <summary>Details</summary>
Motivation: 许多现有的深度学习去噪方法尽管有效，但无法根据噪声水平、相机设置及用户偏好灵活调整去噪强度。

Method: 利用相机参数（如ISO、快门速度和光圈值）生成控制向量，并嵌入去噪网络以实现可控及性能增强的去噪效果。

Result: 实验表明，该方法不仅为传统去噪网络添加了可控功能，还显著提升了其性能。

Conclusion: 该方法结合相机参数，实现了图像去噪强度的适应性调整，且代码开源以供研究者使用。

Abstract: Recent deep learning-based image denoising methods have shown impressive
performance; however, many lack the flexibility to adjust the denoising
strength based on the noise levels, camera settings, and user preferences. In
this paper, we introduce a new controllable denoising framework that adaptively
removes noise from images by utilizing information from camera parameters.
Specifically, we focus on ISO, shutter speed, and F-number, which are closely
related to noise levels. We convert these selected parameters into a vector to
control and enhance the performance of the denoising network. Experimental
results show that our method seamlessly adds controllability to standard
denoising neural networks and improves their performance. Code is available at
https://github.com/OBAKSA/CPADNet.

</details>


### [46] [Autonomous AI Surveillance: Multimodal Deep Learning for Cognitive and Behavioral Monitoring](https://arxiv.org/abs/2507.01590)
*Ameer Hamza,Zuhaib Hussain But,Umar Arif,Samiya,M. Abdullah Asad,Muhammad Naeem*

Main category: cs.CV

TL;DR: 该研究开发了一种多模态课堂监测系统，集成了瞌睡检测、手机使用追踪和人脸识别，以高精度评估学生专注度。


<details>
  <summary>Details</summary>
Motivation: 解决现有课堂监测系统在精度和实时性上的不足，提供更全面的学生行为分析方法。

Method: 结合了YOLOv8模型检测手机和睡觉情况，同时用LResNet和MTCNN进行人脸识别，并基于特定数据集进行模型训练，最终集成到基于PHP的Web应用和ESP32-CAM硬件中，实现实时监控。

Result: 在模型评估中，瞌睡检测达97.42%mAP@50，人脸识别验证精度为86.45%，手机检测达85.89%mAP@50，系统表现优异。

Conclusion: 该系统显著提升了课堂监测精度，并通过人脸识别实现自动考勤，具有在多样化教育环境中推广的潜力。

Abstract: This study presents a novel classroom surveillance system that integrates
multiple modalities, including drowsiness, tracking of mobile phone usage, and
face recognition,to assess student attentiveness with enhanced precision.The
system leverages the YOLOv8 model to detect both mobile phone and sleep
usage,(Ghatge et al., 2024) while facial recognition is achieved through
LResNet Occ FC body tracking using YOLO and MTCNN.(Durai et al., 2024) These
models work in synergy to provide comprehensive, real-time monitoring, offering
insights into student engagement and behavior.(S et al., 2023) The framework is
trained on specialized datasets, such as the RMFD dataset for face recognition
and a Roboflow dataset for mobile phone detection. The extensive evaluation of
the system shows promising results. Sleep detection achieves 97. 42% mAP@50,
face recognition achieves 86. 45% validation accuracy and mobile phone
detection reach 85. 89% mAP@50. The system is implemented within a core PHP web
application and utilizes ESP32-CAM hardware for seamless data capture.(Neto et
al., 2024) This integrated approach not only enhances classroom monitoring, but
also ensures automatic attendance recording via face recognition as students
remain seated in the classroom, offering scalability for diverse educational
environments.(Banada,2025)

</details>


### [47] [DepthSync: Diffusion Guidance-Based Depth Synchronization for Scale- and Geometry-Consistent Video Depth Estimation](https://arxiv.org/abs/2507.01603)
*Yue-Jiang Dong,Wang Zhao,Jiale Xu,Ying Shan,Song-Hai Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为DepthSync的无训练框架，利用扩散引导实现长视频的尺度和几何一致深度预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理长视频深度估计时面临尺度不一致和几何不一致的问题，主要源于滑动窗口预测方法及缺乏3D几何结构的应用。

Method: 提出DepthSync框架，通过尺度引导和几何引导这两项创新，分别同步窗口间的深度尺度和窗口内的几何一致性，从而改善预测结果。

Result: 实验表明，DepthSync方法能在多个数据集上生成具有更好尺度与几何一致性的深度预测，特别是在长视频上效果显著。

Conclusion: DepthSync方法成功通过扩散引导解决了长视频深度预测中的一致性问题，证明其在提升预测质量上的有效性。

Abstract: Diffusion-based video depth estimation methods have achieved remarkable
success with strong generalization ability. However, predicting depth for long
videos remains challenging. Existing methods typically split videos into
overlapping sliding windows, leading to accumulated scale discrepancies across
different windows, particularly as the number of windows increases.
Additionally, these methods rely solely on 2D diffusion priors, overlooking the
inherent 3D geometric structure of video depths, which results in geometrically
inconsistent predictions. In this paper, we propose DepthSync, a novel,
training-free framework using diffusion guidance to achieve scale- and
geometry-consistent depth predictions for long videos. Specifically, we
introduce scale guidance to synchronize the depth scale across windows and
geometry guidance to enforce geometric alignment within windows based on the
inherent 3D constraints in video depths. These two terms work synergistically,
steering the denoising process toward consistent depth predictions. Experiments
on various datasets validate the effectiveness of our method in producing depth
estimates with improved scale and geometry consistency, particularly for long
videos.

</details>


### [48] [Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems](https://arxiv.org/abs/2507.01607)
*Quentin Le Roux,Yannick Teglia,Teddy Furon,Philippe Loubet-Moundi,Eric Bourbao*

Main category: cs.CV

TL;DR: 本文探讨深度学习人脸识别系统中的后门攻击，提出了两种新型攻击，并在多个配置中验证其有效性，同时给出了相关防御措施。


<details>
  <summary>Details</summary>
Motivation: 探讨深度学习人脸识别在非约束场景中的后门攻击问题，填补现有研究空白。

Method: 进行了系统性研究，提出了两种后门攻击方式：人脸生成攻击和人脸关键点漂移攻击，并通过多种管道配置评估了其影响。

Result: 证明了单一后门可使攻击者破坏整个系统功能，并在实践中展示了多种攻击案例的成功。

Conclusion: 研究表明后门攻击在深度学习人脸识别系统中是现实威胁，论文提供若干防御建议以提高安全性。

Abstract: The widespread use of deep learning face recognition raises several security
concerns. Although prior works point at existing vulnerabilities, DNN backdoor
attacks against real-life, unconstrained systems dealing with images captured
in the wild remain a blind spot of the literature. This paper conducts the
first system-level study of backdoors in deep learning-based face recognition
systems. This paper yields four contributions by exploring the feasibility of
DNN backdoors on these pipelines in a holistic fashion. We demonstrate for the
first time two backdoor attacks on the face detection task: face generation and
face landmark shift attacks. We then show that face feature extractors trained
with large margin losses also fall victim to backdoor attacks. Combining our
models, we then show using 20 possible pipeline configurations and 15 attack
cases that a single backdoor enables an attacker to bypass the entire function
of a system. Finally, we provide stakeholders with several best practices and
countermeasures.

</details>


### [49] [Perception-Oriented Latent Coding for High-Performance Compressed Domain Semantic Inference](https://arxiv.org/abs/2507.01608)
*Xu Zhang,Ming Lu,Yan Chen,Zhan Ma*

Main category: cs.CV

TL;DR: 这篇论文提出了一个名为POLC的感知导向潜在编码方法，提高了压缩域语义推断的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的基于MSE优化的图像编码模型在潜在语义丰富性方面存在不足，限制了下游任务中的语义推断能力。此外，这些模型性能的提高往往需要对整个视觉模型进行微调，计算成本高昂。

Method: 引入一种感知导向的潜在编码方法（POLC），通过语义丰富的潜在空间减少对模型微调的需求，使用轻量级的适配器替代全面的参数调整。

Result: 实验结果表明，POLC在视觉任务中的表现显著提高，其速率-感知性能与先进的生成图像编码方法相当，并显著减少了微调所需的开销。

Conclusion: POLC方法不仅提升了压缩域语义推断的效率和效果，同时也降低了对计算资源的需求，为该领域提供了新思路。

Abstract: In recent years, compressed domain semantic inference has primarily relied on
learned image coding models optimized for mean squared error (MSE). However,
MSE-oriented optimization tends to yield latent spaces with limited semantic
richness, which hinders effective semantic inference in downstream tasks.
Moreover, achieving high performance with these models often requires
fine-tuning the entire vision model, which is computationally intensive,
especially for large models. To address these problems, we introduce
Perception-Oriented Latent Coding (POLC), an approach that enriches the
semantic content of latent features for high-performance compressed domain
semantic inference. With the semantically rich latent space, POLC requires only
a plug-and-play adapter for fine-tuning, significantly reducing the parameter
count compared to previous MSE-oriented methods. Experimental results
demonstrate that POLC achieves rate-perception performance comparable to
state-of-the-art generative image coding methods while markedly enhancing
performance in vision tasks, with minimal fine-tuning overhead. Code is
available at https://github.com/NJUVISION/POLC.

</details>


### [50] [Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss](https://arxiv.org/abs/2507.01630)
*Yuxiao Wang,Yu Lei,Zhenao Wei,Weiying Xue,Xinyu Jiang,Nan Zhuang,Qi Liu*

Main category: cs.CV

TL;DR: 提出了新的HOT检测框架P3HOT，用于增强人与物接触区域的识别效果，通过语义驱动的Prompt机制和动态感知机制实现更精确的检测。


<details>
  <summary>Details</summary>
Motivation: 现有模型对单一类型图像的局限性以及对区域一致性和细节交互的不足。

Method: 引入语义驱动的Prompt机制和人类邻近感知机制，通过可学习参数动态感知重要深度范围；提出区域联合损失RJLoss，以及新的评价指标AD-Acc.

Result: 在两大基准数据集上的四项指标中均达到最新的性能：SC-Acc提升0.7，mIoU提升2.0，wIoU提升1.6，AD-Acc提升11.0。

Conclusion: P3HOT框架显著增强了HOT检测能力，实现了更高的精确度和一致性验证，具有良好的实际应用潜力。

Abstract: The task of Human-Object conTact (HOT) detection involves identifying the
specific areas of the human body that are touching objects. Nevertheless,
current models are restricted to just one type of image, often leading to too
much segmentation in areas with little interaction, and struggling to maintain
category consistency within specific regions. To tackle this issue, a HOT
framework, termed \textbf{P3HOT}, is proposed, which blends \textbf{P}rompt
guidance and human \textbf{P}roximal \textbf{P}erception. To begin with, we
utilize a semantic-driven prompt mechanism to direct the network's attention
towards the relevant regions based on the correlation between image and text.
Then a human proximal perception mechanism is employed to dynamically perceive
key depth range around the human, using learnable parameters to effectively
eliminate regions where interactions are not expected. Calculating depth
resolves the uncertainty of the overlap between humans and objects in a 2D
perspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss
(RJLoss) has been created as a new loss to inhibit abnormal categories in the
same area. A new evaluation metric called ``AD-Acc.'' is introduced to address
the shortcomings of existing methods in addressing negative samples.
Comprehensive experimental results demonstrate that our approach achieves
state-of-the-art performance in four metrics across two benchmark datasets.
Specifically, our model achieves an improvement of \textbf{0.7}$\uparrow$,
\textbf{2.0}$\uparrow$, \textbf{1.6}$\uparrow$, and \textbf{11.0}$\uparrow$ in
SC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated
dataset. Code is available at https://github.com/YuxiaoWang-AI/P3HOT.

</details>


### [51] [Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation](https://arxiv.org/abs/2507.01631)
*Camille Billouard,Dawa Derksen,Alexandre Constantin,Bruno Vallet*

Main category: cs.CV

TL;DR: 论文介绍了Snake-NeRF，一种适用于大场景神经辐射场重建的框架，通过不加载所有图像与网络来降低内存占用，同时保证训练质量。


<details>
  <summary>Details</summary>
Motivation: 现有的神经辐射场（NeRF）方法通常受限于小规模场景，需解决内存问题以应用于大规模场景。

Method: 提出Snake-NeRF框架，将感兴趣区域分割成无重叠的3D瓷砖，并通过图像裁剪与重叠训练。引入$2\times 2$ 3D瓷砖进展策略及分段采样器来消除边缘重建误差。

Result: 实验表明，Snake-NeRF能在线性时间内处理大规模卫星图像，无需妥协质量，仅用单GPU完成。

Conclusion: Snake-NeRF为基于神经辐射场的大场景3D重建提供了高效、省内存的解决方案。

Abstract: Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D
reconstruction from multiview satellite imagery. However, state-of-the-art NeRF
methods are typically constrained to small scenes due to the memory footprint
during training, which we study in this paper. Previous work on large-scale
NeRFs palliate this by dividing the scene into NeRFs. This paper introduces
Snake-NeRF, a framework that scales to large scenes. Our out-of-core method
eliminates the need to load all images and networks simultaneously, and
operates on a single device. We achieve this by dividing the region of interest
into NeRFs that 3D tile without overlap. Importantly, we crop the images with
overlap to ensure each NeRFs is trained with all the necessary pixels. We
introduce a novel $2\times 2$ 3D tile progression strategy and segmented
sampler, which together prevent 3D reconstruction errors along the tile edges.
Our experiments conclude that large satellite images can effectively be
processed with linear time complexity, on a single GPU, and without compromise
in quality.

</details>


### [52] [Depth Anything at Any Condition](https://arxiv.org/abs/2507.01634)
*Boyuan Sun,Modi Jin,Bowen Yin,Qibin Hou*

Main category: cs.CV

TL;DR: DepthAnything-AC是一种用于单目深度估计的模型，能在各种复杂环境条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有模型在复杂开放世界环境中表现欠佳，以及受数据稀缺性和标签质量影响的问题。

Method: 提出了一种无监督一致性正则化微调方法，只需少量无标签数据；以及引入空间距离约束机制，改善语义边界清晰度和细节精度。

Result: 实验表明，DepthAnything-AC在各类基准测试中表现出零样本推理能力，涵盖真实恶劣天气、合成破坏场景和通用场景。

Conclusion: DepthAnything-AC克服了复杂环境条件下的深度估计限制，展示了适应性强、精度高的潜力。

Abstract: We present Depth Anything at Any Condition (DepthAnything-AC), a foundation
monocular depth estimation (MDE) model capable of handling diverse
environmental conditions. Previous foundation MDE models achieve impressive
performance across general scenes but not perform well in complex open-world
environments that involve challenging conditions, such as illumination
variations, adverse weather, and sensor-induced distortions. To overcome the
challenges of data scarcity and the inability of generating high-quality
pseudo-labels from corrupted images, we propose an unsupervised consistency
regularization finetuning paradigm that requires only a relatively small amount
of unlabeled data. Furthermore, we propose the Spatial Distance Constraint to
explicitly enforce the model to learn patch-level relative relationships,
resulting in clearer semantic boundaries and more accurate details.
Experimental results demonstrate the zero-shot capabilities of DepthAnything-AC
across diverse benchmarks, including real-world adverse weather benchmarks,
synthetic corruption benchmarks, and general benchmarks.
  Project Page: https://ghost233lism.github.io/depthanything-AC-page
  Code: https://github.com/HVision-NKU/DepthAnythingAC

</details>


### [53] [SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via Gradual Feature Refinement](https://arxiv.org/abs/2507.01643)
*Weijie Yin,Dingkang Yang,Hongyuan Dong,Zijian Kang,Jiacong Wang,Xiao Liang,Chao Feng,Jiao Ran*

Main category: cs.CV

TL;DR: 提出了一种名为SAILViT的视觉Transformer，通过逐步优化特征学习来提升多模态大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉Transformer在与大语言模型协同训练时存在参数初始化冲突和模态语义差异等问题。

Method: 通过逐步特征优化，SAILViT实现粗到细的特征对齐以及世界知识的融合，提高了目标训练的适应性。

Result: 搭载SAILViT的模型在OpenCompass基准测试中的下游任务上表现显著提升。

Conclusion: SAILViT增强了视觉Transformer的鲁棒性和泛化能力，为多模态大语言模型提供了更优的视觉基础。

Abstract: Vision Transformers (ViTs) are essential as foundation backbones in
establishing the visual comprehension capabilities of Multimodal Large Language
Models (MLLMs). Although most ViTs achieve impressive performance through
image-text pair-based contrastive learning or self-supervised mechanisms, they
struggle to engage in connector-based co-training directly with LLMs due to
potential parameter initialization conflicts and modality semantic gaps. To
address the above challenges, this paper proposes SAILViT, a gradual feature
learning-enhanced ViT for facilitating MLLMs to break through performance
bottlenecks in complex multimodal interactions. SAILViT achieves
coarse-to-fine-grained feature alignment and world knowledge infusion with
gradual feature refinement, which better serves target training demands. We
perform thorough empirical analyses to confirm the powerful robustness and
generalizability of SAILViT across different dimensions, including parameter
sizes, model architectures, training strategies, and data scales. Equipped with
SAILViT, existing MLLMs show significant and consistent performance
improvements on the OpenCompass benchmark across extensive downstream tasks.
SAILViT series models are released at
https://huggingface.co/BytedanceDouyinContent.

</details>


### [54] [ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving](https://arxiv.org/abs/2507.01735)
*Kai Chen,Ruiyuan Gao,Lanqing Hong,Hang Xu,Xu Jia,Holger Caesar,Dengxin Dai,Bingbing Liu,Dzmitry Tsishkou,Songcen Xu,Chunjing Xu,Qiang Xu,Huchuan Lu,Dit-Yan Yeung*

Main category: cs.CV

TL;DR: 本文介绍了2024年ECCV大会上举行的第1届W-CODA研讨会，旨在探讨解决自动驾驶中极端情况的解决方案。


<details>
  <summary>Details</summary>
Motivation: 通过多模态感知和理解技术解决自动驾驶中角落情况带来的挑战。

Method: 举办研讨会，邀请学术界和工业界专家发言，并通过论文征集和双轨挑战赛促进角落情况的研究与应用。

Result: 研讨会汇集了前沿研究和专家观点，为更可靠的自动驾驶技术搭建桥梁。

Conclusion: W-CODA是首个致力于解决自动驾驶角落情况挑战的研讨会，将推动完全智能和可靠的自动驾驶系统的发展。

Abstract: In this paper, we present details of the 1st W-CODA workshop, held in
conjunction with the ECCV 2024. W-CODA aims to explore next-generation
solutions for autonomous driving corner cases, empowered by state-of-the-art
multimodal perception and comprehension techniques. 5 Speakers from both
academia and industry are invited to share their latest progress and opinions.
We collect research papers and hold a dual-track challenge, including both
corner case scene understanding and generation. As the pioneering effort, we
will continuously bridge the gap between frontier autonomous driving techniques
and fully intelligent, reliable self-driving agents robust towards corner
cases.

</details>


### [55] [Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective](https://arxiv.org/abs/2507.01652)
*Yuxin Mao,Zhen Qin,Jinxing Zhou,Hui Deng,Xuyang Shen,Bin Fan,Jing Zhang,Yiran Zhong,Yuchao Dai*

Main category: cs.CV

TL;DR: 提出了一种名为LASAD的线性注意力机制，通过二维空间关系进行位置依赖性衰减，提升图像生成效率与质量。


<details>
  <summary>Details</summary>
Motivation: 解决传统自回归模型中因Transformer架构带来的高计算复杂性和内存消耗问题，同时改善线性注意力机制在图像生成中对长距离依赖捕捉不足的缺陷。

Method: 提出了线性注意力机制LASAD，通过真实二维空间位置计算衰减因子，改进图像生成任务的注意力效果，并基于此开发了LASADGen图像生成器。

Result: LASADGen在ImageNet数据上实现了线性复杂度下的高效图像生成，并达到了最新的图像生成性能，为线性注意力机制的效率和空间理解建立了平衡。

Conclusion: LASADGen成功结合了线性注意力的计算效率和对空间语义的高度理解，为高质量的图像生成提供了一种新的解决方案。

Abstract: Autoregressive (AR) models have garnered significant attention in image
generation for their ability to effectively capture both local and global
structures within visual data. However, prevalent AR models predominantly rely
on the transformer architectures, which are beset by quadratic computational
complexity concerning input sequence length and substantial memory overhead due
to the necessity of maintaining key-value caches. Although linear attention
mechanisms have successfully reduced this burden in language models, our
initial experiments reveal that they significantly degrade image generation
quality because of their inability to capture critical long-range dependencies
in visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a
novel attention mechanism that explicitly preserves genuine 2D spatial
relationships within the flattened image sequences by computing
position-dependent decay factors based on true 2D spatial location rather than
1D sequence positions. Based on this mechanism, we present LASADGen, an
autoregressive image generator that enables selective attention to relevant
spatial contexts with linear complexity. Experiments on ImageNet show LASADGen
achieves state-of-the-art image generation performance and computational
efficiency, bridging the gap between linear attention's efficiency and spatial
understanding needed for high-quality generation.

</details>


### [56] [RobuSTereo: Robust Zero-Shot Stereo Matching under Adverse Weather](https://arxiv.org/abs/2507.01653)
*Yuran Wang,Yingping Liang,Yutao Hu,Ying Fu*

Main category: cs.CV

TL;DR: 提出了RobuSTereo框架，通过生成与恶劣天气条件相匹配的高质量立体数据集以及结合ConvNet与去噪Transformer的特征编码实现，显著提升了立体匹配模型在恶劣天气下的鲁棒性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于学习的立体匹配模型在恶劣天气条件下的表现有限，主要因缺乏相应训练数据及从退化图像中提取判别性特征的难度。这显著限制了模型在分布外天气条件下的零样本泛化能力。

Method: 提出了一个基于扩散的模拟管道生成高质量的立体数据，并加入立体一致性模块保持几何完整性，此外，设计了结合ConvNet与去噪Transformer的鲁棒特征编码器，用于从退化图像中提取稳定、可靠的特征。

Result: 实验表明，RobuSTereo 框架显著提升了立体匹配模型在各种恶劣天气场景下的鲁棒性和泛化能力。

Conclusion: RobuSTereo 综合解决了训练数据稀缺和特征提取的挑战，为恶劣天气下的立体匹配问题提供了一个有效的解决方案。

Abstract: Learning-based stereo matching models struggle in adverse weather conditions
due to the scarcity of corresponding training data and the challenges in
extracting discriminative features from degraded images. These limitations
significantly hinder zero-shot generalization to out-of-distribution weather
conditions. In this paper, we propose \textbf{RobuSTereo}, a novel framework
that enhances the zero-shot generalization of stereo matching models under
adverse weather by addressing both data scarcity and feature extraction
challenges. First, we introduce a diffusion-based simulation pipeline with a
stereo consistency module, which generates high-quality stereo data tailored
for adverse conditions. By training stereo matching models on our synthetic
datasets, we reduce the domain gap between clean and degraded images,
significantly improving the models' robustness to unseen weather conditions.
The stereo consistency module ensures structural alignment across synthesized
image pairs, preserving geometric integrity and enhancing depth estimation
accuracy. Second, we design a robust feature encoder that combines a
specialized ConvNet with a denoising transformer to extract stable and reliable
features from degraded images. The ConvNet captures fine-grained local
structures, while the denoising transformer refines global representations,
effectively mitigating the impact of noise, low visibility, and weather-induced
distortions. This enables more accurate disparity estimation even under
challenging visual conditions. Extensive experiments demonstrate that
\textbf{RobuSTereo} significantly improves the robustness and generalization of
stereo matching models across diverse adverse weather scenarios.

</details>


### [57] [SPoT: Subpixel Placement of Tokens in Vision Transformers](https://arxiv.org/abs/2507.01654)
*Martine Hjelkrem-Tan,Marius Aasan,Gabriel Y. Arteaga,Adín Ramírez Rivera*

Main category: cs.CV

TL;DR: 提出一种新的图像标记策略SPoT，通过次像素级别的标记位置优化，提升视觉Transformer模型的效率和预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉Transformer模型因传统基于网格的标记方法受到限制，无法充分利用稀疏性。

Method: 提出次像素标记策略SPoT，并采用oracle-guided search来实现理想的标记位置，大幅减少预测所需的标记数量。

Result: 在理想次像素标记位置下，模型性能显著提高，同时大幅提升推理效率。

Conclusion: SPoT为灵活、高效以及可解释的视觉Transformer架构提供了新的研究方向，将稀疏性转化为优势而非限制。

Abstract: Vision Transformers naturally accommodate sparsity, yet standard tokenization
methods confine features to discrete patch grids. This constraint prevents
models from fully exploiting sparse regimes, forcing awkward compromises. We
propose Subpixel Placement of Tokens (SPoT), a novel tokenization strategy that
positions tokens continuously within images, effectively sidestepping
grid-based limitations. With our proposed oracle-guided search, we uncover
substantial performance gains achievable with ideal subpixel token positioning,
drastically reducing the number of tokens necessary for accurate predictions
during inference. SPoT provides a new direction for flexible, efficient, and
interpretable ViT architectures, redefining sparsity as a strategic advantage
rather than an imposed limitation.

</details>


### [58] [What does really matter in image goal navigation?](https://arxiv.org/abs/2507.01667)
*Gianluca Monaci,Philippe Weinzaepfel,Christian Wolf*

Main category: cs.CV

TL;DR: 本文研究了利用强化学习（RL）是否可以端到端地高效解决基于图像的导航问题。


<details>
  <summary>Details</summary>
Motivation: 探讨能否通过仅基于导航奖励的强化学习训练出具备相对姿态估计能力的导航模型，从而超越当前依赖图像匹配或相对姿态预训练的状态。

Method: 通过大规模研究，分析不同架构选择（如延迟融合、通道堆叠、空间到深度投影和交叉关注）对导航性能和相对姿态估计能力演化的影响。

Result: 发现仿真环境的设置会影响模型表现，但某些能力可以部分迁移到更真实环境中。此外，导航性能和相对姿态估计表现之间存在相关性。

Conclusion: 强化学习通过端到端训练导航代理是可行的，但其成功受限于仿真环境，并非完全归功于方法论本身。

Abstract: Image goal navigation requires two different skills: firstly, core navigation
skills, including the detection of free space and obstacles, and taking
decisions based on an internal representation; and secondly, computing
directional information by comparing visual observations to the goal image.
Current state-of-the-art methods either rely on dedicated image-matching, or
pre-training of computer vision modules on relative pose estimation. In this
paper, we study whether this task can be efficiently solved with end-to-end
training of full agents with RL, as has been claimed by recent work. A positive
answer would have impact beyond Embodied AI and allow training of relative pose
estimation from reward for navigation alone. In a large study we investigate
the effect of architectural choices like late fusion, channel stacking,
space-to-depth projections and cross-attention, and their role in the emergence
of relative pose estimators from navigation training. We show that the success
of recent methods is influenced up to a certain extent by simulator settings,
leading to shortcuts in simulation. However, we also show that these
capabilities can be transferred to more realistic setting, up to some extend.
We also find evidence for correlations between navigation performance and
probed (emerging) relative pose estimation performance, an important sub skill.

</details>


### [59] [Facial Emotion Learning with Text-Guided Multiview Fusion via Vision-Language Model for 3D/4D Facial Expression Recognition](https://arxiv.org/abs/2507.01673)
*Muzammil Behzad*

Main category: cs.CV

TL;DR: 该论文提出一种基于视觉语言的框架FACET-VLM，用于3D/4D面部表情识别，重点在多视角表征学习和自然语言提示语义指导，取得了多个基准测试的最新准确性。


<details>
  <summary>Details</summary>
Motivation: 面部表情识别的重要性在于提升人类行为理解、健康监测和人机交互等领域中的应用效果。因此，解决空间和时间面部动态复杂性的问题至关重要。

Method: 提出一种名为FACET-VLM的框架，包括三个关键组成：跨视角语义聚合（CVSA）、多视角文本引导融合（MTGF）和多视角一致性损失，分别用于视角一致、语义对齐和结构一致性。

Result: 提出的新框架在BU-3DFE、Bosphorus、BU-4DFE和BP4D-Spontaneous基准上实现了最先进的准确性；在4DME数据集上进行了微表情识别扩展并表现出强力效果。

Conclusion: FACET-VLM是一种鲁棒且高性能的多模态3D/4D面部表情识别解决方案，适用于摆拍及自发场景中的表情分析。

Abstract: Facial expression recognition (FER) in 3D and 4D domains presents a
significant challenge in affective computing due to the complexity of spatial
and temporal facial dynamics. Its success is crucial for advancing applications
in human behavior understanding, healthcare monitoring, and human-computer
interaction. In this work, we propose FACET-VLM, a vision-language framework
for 3D/4D FER that integrates multiview facial representation learning with
semantic guidance from natural language prompts. FACET-VLM introduces three key
components: Cross-View Semantic Aggregation (CVSA) for view-consistent fusion,
Multiview Text-Guided Fusion (MTGF) for semantically aligned facial emotions,
and a multiview consistency loss to enforce structural coherence across views.
Our model achieves state-of-the-art accuracy across multiple benchmarks,
including BU-3DFE, Bosphorus, BU-4DFE, and BP4D-Spontaneous. We further extend
FACET-VLM to 4D micro-expression recognition (MER) on the 4DME dataset,
demonstrating strong performance in capturing subtle, short-lived emotional
cues. The extensive experimental results confirm the effectiveness and
substantial contributions of each individual component within the framework.
Overall, FACET-VLM offers a robust, extensible, and high-performing solution
for multimodal FER in both posed and spontaneous settings.

</details>


### [60] [Component Adaptive Clustering for Generalized Category Discovery](https://arxiv.org/abs/2507.01711)
*Mingfu Yan,Jiancheng Huang,Yifan Liu,Shifeng Chen*

Main category: cs.CV

TL;DR: 提出了一种称为AdaGCD的框架，用于自动发现数据中的已知和未知类别，解决了传统方法预定义类别数量的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理部分有标签和部分无标签数据集时往往需要预设类别数量，这限制了其在真实数据中应用的能力。

Method: 提出了一个基于自适应槽式注意力(AdaSlot)的集群对比学习框架，以动态确定数据复杂性所需的最佳槽数量，灵活地将无标签数据划分为已知和未知类别。

Result: 通过对公共和细粒度数据集的实验，表明此框架有效性，同时展示了利用空间局部信息在类别发现中的优势。

Conclusion: AdaGCD实现了动态和适应性类别发现，提高了解决开放场景下类别发现问题的能力。

Abstract: Generalized Category Discovery (GCD) tackles the challenging problem of
categorizing unlabeled images into both known and novel classes within a
partially labeled dataset, without prior knowledge of the number of unknown
categories. Traditional methods often rely on rigid assumptions, such as
predefining the number of classes, which limits their ability to handle the
inherent variability and complexity of real-world data. To address these
shortcomings, we propose AdaGCD, a cluster-centric contrastive learning
framework that incorporates Adaptive Slot Attention (AdaSlot) into the GCD
framework. AdaSlot dynamically determines the optimal number of slots based on
data complexity, removing the need for predefined slot counts. This adaptive
mechanism facilitates the flexible clustering of unlabeled data into known and
novel categories by dynamically allocating representational capacity. By
integrating adaptive representation with dynamic slot allocation, our method
captures both instance-specific and spatially clustered features, improving
class discovery in open-world scenarios. Extensive experiments on public and
fine-grained datasets validate the effectiveness of our framework, emphasizing
the advantages of leveraging spatial local information for category discovery
in unlabeled image datasets.

</details>


### [61] [Using Wavelet Domain Fingerprints to Improve Source Camera Identification](https://arxiv.org/abs/2507.01712)
*Xinle Tian,Matthew Nunes,Emiko Dupont,Shaunagh Downing,Freddie Lichtenstein,Matt Burns*

Main category: cs.CV

TL;DR: 研究提出了一种基于小波的传感器模式噪声(SPN)提取方法的改进，通过直接在小波域进行指纹比较，提高了检测精度和处理速度。


<details>
  <summary>Details</summary>
Motivation: 目前基于小波去噪的相机指纹提取方法虽有效，但存在复杂度高的问题，需优化提取及比较流程。

Method: 通过在小波域直接进行指纹对比，省略了传统方法中的反演步骤，并提出小波域指纹的概念。

Result: 实验结果表明，该方法在真实数据集上的检测精度和处理速度均显著高于现有方法。

Conclusion: 改进后的小波域指纹提取方法优化了流程，提高了效率和精度，实现了相机指纹检测的性能提升。

Abstract: Camera fingerprint detection plays a crucial role in source identification
and image forensics, with wavelet denoising approaches proving to be
particularly effective in extracting sensor pattern noise (SPN). In this
article, we propose a modification to wavelet-based SPN extraction. Rather than
constructing the fingerprint as an image, we introduce the notion of a wavelet
domain fingerprint. This avoids the final inversion step of the denoising
algorithm and allows fingerprint comparisons to be made directly in the wavelet
domain. As such, our modification streamlines the extraction and comparison
process. Experimental results on real-world datasets demonstrate that our
method not only achieves higher detection accuracy but can also significantly
improve processing speed.

</details>


### [62] [Soft Self-labeling and Potts Relaxations for Weakly-Supervised Segmentation](https://arxiv.org/abs/2507.01721)
*Zhongwen Zhang,Yuri Boykov*

Main category: cs.CV

TL;DR: 本文提出了一种基于软标签的弱监督分割方法，通过优化CRF/Potts无监督损失的松弛版来提升网络性能，显示出超越复杂WSSS系统的潜力。


<details>
  <summary>Details</summary>
Motivation: 以往基于硬伪标签的CRF优化方法无法表示类不确定性或错误，这促使了基于软标签的自我标签化方法的研究。

Method: 系统研究了标准及新型CRF松弛方法（凸和非凸）、邻域系统及将网络预测与软伪标签连接的项，提出通用的连续子问题求解器并验证其在标准架构上的性能提升。

Result: 软自我标签在scribble标注的训练中表现优异，超越许多更复杂的WSSS系统，甚至超过全像素精确监督的效果。

Conclusion: 该方法适用于其他弱监督问题，并在通过简化网络架构和引入软标签方面展现了强大的普适性和性能优势。

Abstract: We consider weakly supervised segmentation where only a fraction of pixels
have ground truth labels (scribbles) and focus on a self-labeling approach
optimizing relaxations of the standard unsupervised CRF/Potts loss on unlabeled
pixels. While WSSS methods can directly optimize such losses via gradient
descent, prior work suggests that higher-order optimization can improve network
training by introducing hidden pseudo-labels and powerful CRF sub-problem
solvers, e.g. graph cut. However, previously used hard pseudo-labels can not
represent class uncertainty or errors, which motivates soft self-labeling. We
derive a principled auxiliary loss and systematically evaluate standard and new
CRF relaxations (convex and non-convex), neighborhood systems, and terms
connecting network predictions with soft pseudo-labels. We also propose a
general continuous sub-problem solver. Using only standard architectures, soft
self-labeling consistently improves scribble-based training and outperforms
significantly more complex specialized WSSS systems. It can outperform full
pixel-precise supervision. Our general ideas apply to other weakly-supervised
problems/systems.

</details>


### [63] [When Does Pruning Benefit Vision Representations?](https://arxiv.org/abs/2507.01722)
*Enrico Cassano,Riccardo Renzulli,Andrea Bragagnolo,Marco Grangetto*

Main category: cs.CV

TL;DR: 该论文探讨了模型剪枝对计算机视觉模型在解释性、无监督对象发现和与人类认知对齐三方面的影响，发现模型稀疏化的效果依赖于模型架构和参数规模，揭示了三者之间的复杂关系。


<details>
  <summary>Details</summary>
Motivation: 研究剪枝如何影响模型表现及其解释性，特别是在降低模型复杂性以及是否促进更加简洁结构化的表示方面。

Method: 通过分析不同的视觉网络架构，在不同稀疏度下评估特征归因解释性方法，同时探讨剪枝是否促进对象发现及增强与人类感知的对齐能力。

Result: 稀疏模型可能在解释性、下游任务泛化能力及与人类感知对齐上出现“甜蜜点”，但这些效果高度依赖于模型架构及可训练参数的大小。

Conclusion: 剪枝对视觉表示的潜在益处应该基于具体情境进行研究，因为其影响受多维因素制约，不能一概而论。

Abstract: Pruning is widely used to reduce the complexity of deep learning models, but
its effects on interpretability and representation learning remain poorly
understood. This paper investigates how pruning influences vision models across
three key dimensions: (i) interpretability, (ii) unsupervised object discovery,
and (iii) alignment with human perception. We first analyze different vision
network architectures to examine how varying sparsity levels affect feature
attribution interpretability methods. Additionally, we explore whether pruning
promotes more succinct and structured representations, potentially improving
unsupervised object discovery by discarding redundant information while
preserving essential features. Finally, we assess whether pruning enhances the
alignment between model representations and human perception, investigating
whether sparser models focus on more discriminative features similarly to
humans. Our findings also reveal the presence of sweet spots, where sparse
models exhibit higher interpretability, downstream generalization and human
alignment. However, these spots highly depend on the network architectures and
their size in terms of trainable parameters. Our results suggest a complex
interplay between these three dimensions, highlighting the importance of
investigating when and how pruning benefits vision representations.

</details>


### [64] [HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion](https://arxiv.org/abs/2507.01737)
*Lin Wu,Zhixiang Chen,Jianglin Lan*

Main category: cs.CV

TL;DR: 本文提出了一个名为HOI-Dyn的新框架，用以解决3D人-物交互生成中因互动动态建模缺陷导致的物理不合理性和因果不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，人和物体动作是独立建模的，导致生成的交互行为缺乏真实性和一致性。作者希望通过新的框架改善该问题。

Method: 引入一个轻量级基于Transformer的动态模型，将人-物交互建模为驱动-响应系统，利用动态损失函数减少预测误差对生成的负面影响，动态模型仅在训练时使用以确保推理效率。

Result: 实验表明，所提方法在提高交互生成质量的同时，也提出了一种评价生成交互质量的可行性指标。

Conclusion: HOI-Dyn不仅有效增强了3D人-物交互生成的真实性和一致性，还在评估交互生成质量方面提供了新的视角。

Abstract: Generating realistic 3D human-object interactions (HOIs) remains a
challenging task due to the difficulty of modeling detailed interaction
dynamics. Existing methods treat human and object motions independently,
resulting in physically implausible and causally inconsistent behaviors. In
this work, we present HOI-Dyn, a novel framework that formulates HOI generation
as a driver-responder system, where human actions drive object responses. At
the core of our method is a lightweight transformer-based interaction dynamics
model that explicitly predicts how objects should react to human motion. To
further enforce consistency, we introduce a residual-based dynamics loss that
mitigates the impact of dynamics prediction errors and prevents misleading
optimization signals. The dynamics model is used only during training,
preserving inference efficiency. Through extensive qualitative and quantitative
experiments, we demonstrate that our approach not only enhances the quality of
HOI generation but also establishes a feasible metric for evaluating the
quality of generated interactions.

</details>


### [65] [DeRIS: Decoupling Perception and Cognition for Enhanced Referring Image Segmentation through Loopback Synergy](https://arxiv.org/abs/2507.01738)
*Ming Dai,Wenxuan Cheng,Jiang-jiang Liu,Sen Yang,Wenxiao Cai,Yanpeng Sun,Wankou Yang*

Main category: cs.CV

TL;DR: DeRIS是一种通过将任务分解为感知和认知两个关键部分的新框架。


<details>
  <summary>Details</summary>
Motivation: 分析现有RIS框架中未被充分探索的根本瓶颈。

Method: 提出DeRIS框架和Loopback Synergy机制，以及一种数据增强策略解决长尾分布问题。

Result: 实现了对非参照和多参照情景的适应性，无需特别架构改动，展现出广泛的通用性。

Conclusion: DeRIS解决了现有模型中多模态认知能力不足的问题，并提升了视觉-语言理解能力。

Abstract: Referring Image Segmentation (RIS) is a challenging task that aims to segment
objects in an image based on natural language expressions. While prior studies
have predominantly concentrated on improving vision-language interactions and
achieving fine-grained localization, a systematic analysis of the fundamental
bottlenecks in existing RIS frameworks remains underexplored. To bridge this
gap, we propose DeRIS, a novel framework that decomposes RIS into two key
components: perception and cognition. This modular decomposition facilitates a
systematic analysis of the primary bottlenecks impeding RIS performance. Our
findings reveal that the predominant limitation lies not in perceptual
deficiencies, but in the insufficient multi-modal cognitive capacity of current
models. To mitigate this, we propose a Loopback Synergy mechanism, which
enhances the synergy between the perception and cognition modules, thereby
enabling precise segmentation while simultaneously improving robust image-text
comprehension. Additionally, we analyze and introduce a simple non-referent
sample conversion data augmentation to address the long-tail distribution issue
related to target existence judgement in general scenarios. Notably, DeRIS
demonstrates inherent adaptability to both non- and multi-referents scenarios
without requiring specialized architectural modifications, enhancing its
general applicability. The codes and models are available at
https://github.com/Dmmm1997/DeRIS.

</details>


### [66] [Calibrated Self-supervised Vision Transformers Improve Intracranial Arterial Calcification Segmentation from Clinical CT Head Scans](https://arxiv.org/abs/2507.01744)
*Benjamin Jin,Grant Mair,Joanna M. Wardlaw,Maria del C. Valdés Hernández*

Main category: cs.CV

TL;DR: 本文探讨了预训练的Vision Transformers（ViTs）在医学图像分割中的应用，特别是针对脑内动脉钙化（IAC）的自动量化，实现了比传统方法更高的性能。


<details>
  <summary>Details</summary>
Motivation: 基于ViTs的模型在自然图像领域取得了成功，但在3D医学图像分割中表现有限。本文希望通过借助受掩码自动编码器（MAE）框架自监督训练的3D ViTs，高效使用无标注的医学影像数据，从而在IAC的自动分割与风险评估中提供改进方案。

Method: 作者基于第三次国际卒中试验（IST-3）的高异质性数据集，对ViTs使用MAE进行预训练，再利用该预训练模型进行IAC分割的精调。研究分析了ViTs在关键参数（如patch大小、模型插值方式等）及临床意义上的表现。

Result: 1）经校准的自监督ViT在分割任务上比传统nnU-Net基线提高了3.2 Dice分； 2）较小的patch尺寸和基于规则卷积的插值上采样对提升分割表现至关重要； 3）ViTs增强了对较厚切片的鲁棒性，并在临床风险分组分类中提升了46%的性能。

Conclusion: 研究首次成功将预训练的ViTs用于IAC分割，验证其对自监督学习、低patch尺寸选择及插值方法的优势，同时证明其可改善临床风险评估的潜力，且代码已公开以供进一步验证与研究。

Abstract: Vision Transformers (ViTs) have gained significant popularity in the natural
image domain but have been less successful in 3D medical image segmentation.
Nevertheless, 3D ViTs are particularly interesting for large medical imaging
volumes due to their efficient self-supervised training within the masked
autoencoder (MAE) framework, which enables the use of imaging data without the
need for expensive manual annotations. intracranial arterial calcification
(IAC) is an imaging biomarker visible on routinely acquired CT scans linked to
neurovascular diseases such as stroke and dementia, and automated IAC
quantification could enable their large-scale risk assessment. We pre-train
ViTs with MAE and fine-tune them for IAC segmentation for the first time. To
develop our models, we use highly heterogeneous data from a large clinical
trial, the third International Stroke Trial (IST-3). We evaluate key aspects of
MAE pre-trained ViTs in IAC segmentation, and analyse the clinical
implications. We show: 1) our calibrated self-supervised ViT beats a strong
supervised nnU-Net baseline by 3.2 Dice points, 2) low patch sizes are crucial
for ViTs for IAC segmentation and interpolation upsampling with regular
convolutions is preferable to transposed convolutions for ViT-based models, and
3) our ViTs increase robustness to higher slice thicknesses and improve risk
group classification in a clinical scenario by 46%. Our code is available
online.

</details>


### [67] [SSL4SAR: Self-Supervised Learning for Glacier Calving Front Extraction from SAR Imagery](https://arxiv.org/abs/2507.01747)
*Nora Gourmelon,Marcel Dreier,Martin Mayr,Thorsten Seehaus,Dakota Pyles,Matthias Braun,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: 这篇论文通过结合两种新颖的自监督多模态预训练技术和一种混合模型架构，实现了对冰川崩解前沿位置的高精度自动解析。


<details>
  <summary>Details</summary>
Motivation: 冰川质量快速流失迫切需要精确的年度监测，而现有方法面对遥感影像的特性有局限性。

Method: 论文提出两种自监督多模态预训练技术和一种结合Swin Transformer编码器与残差CNN解码器的混合模型架构，并利用一个新的无标签数据集SSL4SAR进行模型预训练。

Result: 预训练的模型在CaFFe基准测试集上实现了293米的平均误差，优于此前模型的360米，还进一步通过模型集成将平均误差降低到75米，接近人工标注精度（38米）。

Conclusion: 该研究推动了遥感影像中的冰川崩解前沿监测，可提高季节性冰川变化的监测精度。

Abstract: Glaciers are losing ice mass at unprecedented rates, increasing the need for
accurate, year-round monitoring to understand frontal ablation, particularly
the factors driving the calving process. Deep learning models can extract
calving front positions from Synthetic Aperture Radar imagery to track seasonal
ice losses at the calving fronts of marine- and lake-terminating glaciers. The
current state-of-the-art model relies on ImageNet-pretrained weights. However,
they are suboptimal due to the domain shift between the natural images in
ImageNet and the specialized characteristics of remote sensing imagery, in
particular for Synthetic Aperture Radar imagery. To address this challenge, we
propose two novel self-supervised multimodal pretraining techniques that
leverage SSL4SAR, a new unlabeled dataset comprising 9,563 Sentinel-1 and 14
Sentinel-2 images of Arctic glaciers, with one optical image per glacier in the
dataset. Additionally, we introduce a novel hybrid model architecture that
combines a Swin Transformer encoder with a residual Convolutional Neural
Network (CNN) decoder. When pretrained on SSL4SAR, this model achieves a mean
distance error of 293 m on the "CAlving Fronts and where to Find thEm" (CaFFe)
benchmark dataset, outperforming the prior best model by 67 m. Evaluating an
ensemble of the proposed model on a multi-annotator study of the benchmark
dataset reveals a mean distance error of 75 m, approaching the human
performance of 38 m. This advancement enables precise monitoring of seasonal
changes in glacier calving fronts.

</details>


### [68] [Rethinking Discrete Tokens: Treating Them as Conditions for Continuous Autoregressive Image Synthesis](https://arxiv.org/abs/2507.01756)
*Peng Zheng,Junke Wang,Yi Chang,Yizhou Yu,Rui Ma,Zuxuan Wu*

Main category: cs.CV

TL;DR: 本文提出DisCon框架，通过将离散token重新解释为条件信号以优化连续token生成问题，并在ImageNet生成任务中取得了显著的性能优势。


<details>
  <summary>Details</summary>
Motivation: 现有基于自回归框架的视觉生成模型在量化过程中信息损失较大，而直接预测连续token又面临分布外伪影和优化难题。

Method: 引入DisCon框架，通过建模条件概率，将离散token作为条件信号指导连续表示的生成。

Result: 在ImageNet 256×256数据集生成任务中，DisCon获得了1.38的gFID分数，优于现有自回归方法。

Conclusion: DisCon成功缓解了连续token建模中的优化问题，同时避免了量化导致的信息损失，为视觉生成任务提供了一种新颖高效的解决方案。

Abstract: Recent advances in large language models (LLMs) have spurred interests in
encoding images as discrete tokens and leveraging autoregressive (AR)
frameworks for visual generation. However, the quantization process in AR-based
visual generation models inherently introduces information loss that degrades
image fidelity. To mitigate this limitation, recent studies have explored to
autoregressively predict continuous tokens. Unlike discrete tokens that reside
in a structured and bounded space, continuous representations exist in an
unbounded, high-dimensional space, making density estimation more challenging
and increasing the risk of generating out-of-distribution artifacts. Based on
the above findings, this work introduces DisCon (Discrete-Conditioned
Continuous Autoregressive Model), a novel framework that reinterprets discrete
tokens as conditional signals rather than generation targets. By modeling the
conditional probability of continuous representations conditioned on discrete
tokens, DisCon circumvents the optimization challenges of continuous token
modeling while avoiding the information loss caused by quantization. DisCon
achieves a gFID score of 1.38 on ImageNet 256$\times$256 generation,
outperforming state-of-the-art autoregressive approaches by a clear margin.

</details>


### [69] [Are Vision Transformer Representations Semantically Meaningful? A Case Study in Medical Imaging](https://arxiv.org/abs/2507.01788)
*Montasir Shams,Chashi Mahiul Islam,Shaeke Salman,Phat Tran,Xiuwen Liu*

Main category: cs.CV

TL;DR: 本文研究发现视觉变换模型（ViT）在医疗图像分类中，其表示缺乏语义性且易受微小变化影响，可能导致分类结果不可靠。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉变换模型在医疗图像分类中的语义性以及可靠性问题，填补ViT表示语义研究的空白。

Method: 使用基于投影梯度的算法，系统评估ViT模型的语义表示特性及其对微小变化的脆弱性。

Result: ViT模型的表示对不可察觉的图像差异敏感，表现出语义表示不足问题，并且这种脆弱性对分类准确性造成显著影响，降幅超60%。

Conclusion: ViT模型在医疗图像分类中的语义表示缺乏以及脆弱性对其在安全关键系统中的部署构成重大挑战。

Abstract: Vision transformers (ViTs) have rapidly gained prominence in medical imaging
tasks such as disease classification, segmentation, and detection due to their
superior accuracy compared to conventional deep learning models. However, due
to their size and complex interactions via the self-attention mechanism, they
are not well understood. In particular, it is unclear whether the
representations produced by such models are semantically meaningful. In this
paper, using a projected gradient-based algorithm, we show that their
representations are not semantically meaningful and they are inherently
vulnerable to small changes. Images with imperceptible differences can have
very different representations; on the other hand, images that should belong to
different semantic classes can have nearly identical representations. Such
vulnerability can lead to unreliable classification results; for example,
unnoticeable changes cause the classification accuracy to be reduced by over
60\%. %. To the best of our knowledge, this is the first work to systematically
demonstrate this fundamental lack of semantic meaningfulness in ViT
representations for medical image classification, revealing a critical
challenge for their deployment in safety-critical systems.

</details>


### [70] [Boosting Adversarial Transferability Against Defenses via Multi-Scale Transformation](https://arxiv.org/abs/2507.01791)
*Zihong Guo,Chen Wan,Yayin Zheng,Hailing Kuang,Xiaohai Lu*

Main category: cs.CV

TL;DR: 提出了一种名为分段高斯金字塔（SGP）的新攻击方法，通过多尺度梯度计算，显著提升了对抗样本的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 解决对抗样本针对防御模型的迁移性问题，使无需了解目标模型详细信息即可实施有效攻击。

Method: 利用高斯滤波和三种下采样方式生成多尺度图像，计算各尺度下的梯度平均值以确定对抗扰动。

Result: 实验表明，相较于其他方法，SGP在黑盒防御模型下的攻击成功率提升了2.3%至32.6%。

Conclusion: SGP作为一种高可扩展性的输入变换技术，可广泛整合到现有对抗攻击方法中，显著提高了攻击的迁移性。

Abstract: The transferability of adversarial examples poses a significant security
challenge for deep neural networks, which can be attacked without knowing
anything about them. In this paper, we propose a new Segmented Gaussian Pyramid
(SGP) attack method to enhance the transferability, particularly against
defense models. Unlike existing methods that generally focus on single-scale
images, our approach employs Gaussian filtering and three types of downsampling
to construct a series of multi-scale examples. Then, the gradients of the loss
function with respect to each scale are computed, and their average is used to
determine the adversarial perturbations. The proposed SGP can be considered an
input transformation with high extensibility that is easily integrated into
most existing adversarial attacks. Extensive experiments demonstrate that in
contrast to the state-of-the-art methods, SGP significantly enhances attack
success rates against black-box defense models, with average attack success
rates increasing by 2.3% to 32.6%, based only on transferability.

</details>


### [71] [FreeLoRA: Enabling Training-Free LoRA Fusion for Autoregressive Multi-Subject Personalization](https://arxiv.org/abs/2507.01792)
*Peng Zheng,Ye Wang,Rui Ma,Zuxuan Wu*

Main category: cs.CV

TL;DR: 本文提出FreeLoRA框架，通过零训练的方式融合多个基于LoRA的个性化模块，用于多主体图像生成任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法在进行多主体个性化图像生成时，需要复杂的模块重新训练或联合优化，限制了灵活性与实用性。

Method: 提出了FreeLoRA框架，并利用完全Token调整策略进行模块适配，以及在推理时采用了基于主体的推理方式，实现在单张图像中融合多个个性化主体，而无需重新训练。

Result: 实验结果表明，FreeLoRA在主体保真性和提示一致性方面均表现良好。

Conclusion: FreeLoRA为多主体个性化图像生成提供了一种高效的解决方案，减少了训练需求并提高了组合灵活性。

Abstract: Subject-driven image generation plays a crucial role in applications such as
virtual try-on and poster design. Existing approaches typically fine-tune
pretrained generative models or apply LoRA-based adaptations for individual
subjects. However, these methods struggle with multi-subject personalization,
as combining independently adapted modules often requires complex re-tuning or
joint optimization. We present FreeLoRA, a simple and generalizable framework
that enables training-free fusion of subject-specific LoRA modules for
multi-subject personalization. Each LoRA module is adapted on a few images of a
specific subject using a Full Token Tuning strategy, where it is applied across
all tokens in the prompt to encourage weakly supervised token-content
alignment. At inference, we adopt Subject-Aware Inference, activating each
module only on its corresponding subject tokens. This enables training-free
fusion of multiple personalized subjects within a single image, while
mitigating overfitting and mutual interference between subjects. Extensive
experiments show that FreeLoRA achieves strong performance in both subject
fidelity and prompt consistency.

</details>


### [72] [HCNQA: Enhancing 3D VQA with Hierarchical Concentration Narrowing Supervision](https://arxiv.org/abs/2507.01800)
*Shengli Zhou,Jianuo Zhu,Qilin Huang,Fangjing Wang,Yanfu Zhang,Feng Zheng*

Main category: cs.CV

TL;DR: 这篇文章提出了HCNQA模型，用分层的“集中缩小”监督方法提升3D VQA任务的效果，解决了当前答案导向监督方法的潜在问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D VQA模型主要依赖答案导向的监督方式，但这种方法未对推理路径施加监督，可能导致模型发展不合理或短视的逻辑推理路径。

Method: 提出HCNQA模型，引入分层集中缩小监督方法，模仿人类从大范围逐渐聚焦到特定对象的答题过程，对推理路径上的关键节点施加监督。

Result: 实验结果表明，该方法能有效促进模型发展合理的推理路径，并显著提高性能。

Conclusion: HCNQA通过分层监督方式，引导模型发展合理的推理路径，解决现有问题并提升3D VQA任务表现。

Abstract: 3D Visual Question-Answering (3D VQA) is pivotal for models to perceive the
physical world and perform spatial reasoning. Answer-centric supervision is a
commonly used training method for 3D VQA models. Many models that utilize this
strategy have achieved promising results in 3D VQA tasks. However, the
answer-centric approach only supervises the final output of models and allows
models to develop reasoning pathways freely. The absence of supervision on the
reasoning pathway enables the potential for developing superficial shortcuts
through common patterns in question-answer pairs. Moreover, although
slow-thinking methods advance large language models, they suffer from
underthinking. To address these issues, we propose \textbf{HCNQA}, a 3D VQA
model leveraging a hierarchical concentration narrowing supervision method. By
mimicking the human process of gradually focusing from a broad area to specific
objects while searching for answers, our method guides the model to perform
three phases of concentration narrowing through hierarchical supervision. By
supervising key checkpoints on a general reasoning pathway, our method can
ensure the development of a rational and effective reasoning pathway. Extensive
experimental results demonstrate that our method can effectively ensure that
the model develops a rational reasoning pathway and performs better. The code
is available at https://github.com/JianuoZhu/HCNQA.

</details>


### [73] [AMD: Adaptive Momentum and Decoupled Contrastive Learning Framework for Robust Long-Tail Trajectory Prediction](https://arxiv.org/abs/2507.01801)
*Bin Rao,Haicheng Liao,Yanchen Guan,Chengyue Wang,Bonan Wang,Jiaxun Zhang,Zhenning Li*

Main category: cs.CV

TL;DR: 本研究提出AMD框架，通过引入改进的对比学习方法和随机增强策略，有效提升长尾轨迹预测的精度和整体性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶中的轨迹预测面临数据分布不均的问题，尤其是长尾数据往往包含更多复杂和危险的场景，传统方法难以处理这些情况。

Method: 提出并整合了改进的动量对比学习（MoCo-DT）和去耦对比学习（DCL）模块，同时设计了四种轨迹随机增强方法和在线迭代聚类策略，共同提高对长尾轨迹的识别能力。

Result: 在nuScenes和ETH/UCY数据集上的实验表明，AMD框架在长尾轨迹预测和整体预测精度上均表现优异，实现了最优性能。

Conclusion: 采用AMD框架可以显著增强模型处理长尾数据的能力，同时在整体轨迹预测上取得更好的效果，有助于自动驾驶领域的未来发展。

Abstract: Accurately predicting the future trajectories of traffic agents is essential
in autonomous driving. However, due to the inherent imbalance in trajectory
distributions, tail data in natural datasets often represents more complex and
hazardous scenarios. Existing studies typically rely solely on a base model's
prediction error, without considering the diversity and uncertainty of
long-tail trajectory patterns. We propose an adaptive momentum and decoupled
contrastive learning framework (AMD), which integrates unsupervised and
supervised contrastive learning strategies. By leveraging an improved momentum
contrast learning (MoCo-DT) and decoupled contrastive learning (DCL) module,
our framework enhances the model's ability to recognize rare and complex
trajectories. Additionally, we design four types of trajectory random
augmentation methods and introduce an online iterative clustering strategy,
allowing the model to dynamically update pseudo-labels and better adapt to the
distributional shifts in long-tail data. We propose three different criteria to
define long-tail trajectories and conduct extensive comparative experiments on
the nuScenes and ETH$/$UCY datasets. The results show that AMD not only
achieves optimal performance in long-tail trajectory prediction but also
demonstrates outstanding overall prediction accuracy.

</details>


### [74] [Modulate and Reconstruct: Learning Hyperspectral Imaging from Misaligned Smartphone Views](https://arxiv.org/abs/2507.01835)
*Daniil Reutsky,Daniil Vladimirov,Yasin Mamedov,Georgy Perevozchikov,Nancy Mehta,Egor Ershov,Radu Timofte*

Main category: cs.CV

TL;DR: 研究提出一种新的多图片超光谱重建（MI-HSR）框架，通过智能手机的三摄像头系统获取丰富的光谱信息，比单一RGB摄像头方案有更高精度。提供一个新数据集Doomer作为验证基准。


<details>
  <summary>Details</summary>
Motivation: 单RGB图像的超光谱重建准确性受限，且问题由于光谱信息损失严重而变得困难，需找到改进方法。

Method: 通过智能手机的三摄系统（两个镜头配备光谱滤镜）收集多样的光谱信息并引入全新数据集Doomer，以及开发改进的重建模型。

Result: 与传统单RGB图像系统相比，新模型能在同一基准上实现光谱估计30%的改进。

Conclusion: 多视角光谱滤波结合大众硬件方案可以提供更准确且实用的超光谱成像解决方案。

Abstract: Hyperspectral reconstruction (HSR) from RGB images is a fundamentally
ill-posed problem due to severe spectral information loss. Existing approaches
typically rely on a single RGB image, limiting reconstruction accuracy. In this
work, we propose a novel multi-image-to-hyperspectral reconstruction (MI-HSR)
framework that leverages a triple-camera smartphone system, where two lenses
are equipped with carefully selected spectral filters. Our configuration,
grounded in theoretical and empirical analysis, enables richer and more diverse
spectral observations than conventional single-camera setups. To support this
new paradigm, we introduce Doomer, the first dataset for MI-HSR, comprising
aligned images from three smartphone cameras and a hyperspectral reference
camera across diverse scenes. We show that the proposed HSR model achieves
consistent improvements over existing methods on the newly proposed benchmark.
In a nutshell, our setup allows 30% towards more accurately estimated spectra
compared to an ordinary RGB camera. Our findings suggest that multi-view
spectral filtering with commodity hardware can unlock more accurate and
practical hyperspectral imaging solutions.

</details>


### [75] [MobileIE: An Extremely Lightweight and Effective ConvNet for Real-Time Image Enhancement on Mobile Devices](https://arxiv.org/abs/2507.01838)
*Hailong Yan,Ao Li,Xiangtao Zhang,Zhe Liu,Zenglin Shi,Ce Zhu,Le Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种极轻量级的卷积神经网络框架，仅有约4K参数，能在高达1100 FPS的速度下实现实时图像增强，同时保持较高的图像质量。


<details>
  <summary>Details</summary>
Motivation: 当前深度神经网络虽然推动了图像增强技术的进步，但其高计算量与内存需求使得在移动设备上的部署受到限制。

Method: 通过重参数化及增量权重优化策略设计轻量化CNN框架，同时结合特征自变换模块与层次双路径注意机制，并使用局部方差加权损失进行优化。

Result: 此框架实现了高达1100 FPS的实时推断速度，并在多种图像增强任务中展现了速度与性能的最佳平衡。

Conclusion: 本文提出的轻量化框架为资源受限平台尤其是移动设备上的实时图像增强开辟了新道路，并结合代码开源进一步推动了此领域的发展。

Abstract: Recent advancements in deep neural networks have driven significant progress
in image enhancement (IE). However, deploying deep learning models on
resource-constrained platforms, such as mobile devices, remains challenging due
to high computation and memory demands. To address these challenges and
facilitate real-time IE on mobile, we introduce an extremely lightweight
Convolutional Neural Network (CNN) framework with around 4K parameters. Our
approach integrates reparameterization with an Incremental Weight Optimization
strategy to ensure efficiency. Additionally, we enhance performance with a
Feature Self-Transform module and a Hierarchical Dual-Path Attention mechanism,
optimized with a Local Variance-Weighted loss. With this efficient framework,
we are the first to achieve real-time IE inference at up to 1,100 frames per
second (FPS) while delivering competitive image quality, achieving the best
trade-off between speed and performance across multiple IE tasks. The code will
be available at https://github.com/AVC2-UESTC/MobileIE.git.

</details>


### [76] [Future Slot Prediction for Unsupervised Object Discovery in Surgical Video](https://arxiv.org/abs/2507.01882)
*Guiqiu Liao,Matjaz Jogan,Marcel Hussing,Edward Zhang,Eric Eaton,Daniel A. Hashimoto*

Main category: cs.CV

TL;DR: 提出了动态时序槽变压器（DTST）模块，以提升在外科手术视频中的无监督对象解析能力。


<details>
  <summary>Details</summary>
Motivation: 当前无监督对象中心化方法在处理复杂的现实场景（如手术视频）时表现不佳，亟需改进模型以增强时序推理能力和槽初始化预测能力。

Method: 提出动态时序槽变压器模块，结合时序推理及预测未来槽初始化的能力，以提升模型在外科手术视频中的表现。

Result: 模型在多个外科手术数据库上达到了最佳性能，验证了方法的有效性。

Conclusion: 无监督对象中心化方法可以成功应用于实际医疗数据，为医疗应用提供新的工具。

Abstract: Object-centric slot attention is an emerging paradigm for unsupervised
learning of structured, interpretable object-centric representations (slots).
This enables effective reasoning about objects and events at a low
computational cost and is thus applicable to critical healthcare applications,
such as real-time interpretation of surgical video. The heterogeneous scenes in
real-world applications like surgery are, however, difficult to parse into a
meaningful set of slots. Current approaches with an adaptive slot count perform
well on images, but their performance on surgical videos is low. To address
this challenge, we propose a dynamic temporal slot transformer (DTST) module
that is trained both for temporal reasoning and for predicting the optimal
future slot initialization. The model achieves state-of-the-art performance on
multiple surgical databases, demonstrating that unsupervised object-centric
methods can be applied to real-world data and become part of the common arsenal
in healthcare applications.

</details>


### [77] [Self-Reinforcing Prototype Evolution with Dual-Knowledge Cooperation for Semi-Supervised Lifelong Person Re-Identification](https://arxiv.org/abs/2507.01884)
*Kunlun Xu,Fan Zhuo,Jiangmeng Li,Xu Zou,Jiahuan Zhou*

Main category: cs.CV

TL;DR: 本研究提出了一种针对半监督终身行人重识别问题(Semi-LReID)的新框架，通过原型进化和双知识合作机制，在动态伪标签生成和去噪知识利用之间建立循环，从而增强无标签数据的利用效率。


<details>
  <summary>Details</summary>
Motivation: 当前的终身行人重识别方法严重依赖全标注的数据流，但在实际中标注资源有限，无标注数据与少量标注样本长期共存。现有方法在半监督情境下表现不佳，需要更好的策略来应对此问题。

Method: 提出了一种名为SPRED的框架，利用动态可学习的身份原型生成高质量伪标签，并通过双知识合作方案整合当前模型与历史模型的知识，对伪标签进行净化。

Result: 在Semi-LReID基准测试中，SPRED取得了最新的性能表现，证明了其有效性。

Conclusion: SPRED框架通过自我强化循环和知识协作，大幅提升了半监督终身行人重识别任务中的性能，解决了长期学习中的噪声传播问题。

Abstract: Current lifelong person re-identification (LReID) methods predominantly rely
on fully labeled data streams. However, in real-world scenarios where
annotation resources are limited, a vast amount of unlabeled data coexists with
scarce labeled samples, leading to the Semi-Supervised LReID (Semi-LReID)
problem where LReID methods suffer severe performance degradation. Existing
LReID methods, even when combined with semi-supervised strategies, suffer from
limited long-term adaptation performance due to struggling with the noisy
knowledge occurring during unlabeled data utilization. In this paper, we
pioneer the investigation of Semi-LReID, introducing a novel Self-Reinforcing
Prototype Evolution with Dual-Knowledge Cooperation framework (SPRED). Our key
innovation lies in establishing a self-reinforcing cycle between dynamic
prototype-guided pseudo-label generation and new-old knowledge collaborative
purification to enhance the utilization of unlabeled data. Specifically,
learnable identity prototypes are introduced to dynamically capture the
identity distributions and generate high-quality pseudo-labels. Then, the
dual-knowledge cooperation scheme integrates current model specialization and
historical model generalization, refining noisy pseudo-labels. Through this
cyclic design, reliable pseudo-labels are progressively mined to improve
current-stage learning and ensure positive knowledge propagation over long-term
learning. Experiments on the established Semi-LReID benchmarks show that our
SPRED achieves state-of-the-art performance. Our source code is available at
https://github.com/zhoujiahuan1991/ICCV2025-SPRED

</details>


### [78] [Reasoning to Edit: Hypothetical Instruction-Based Image Editing with Visual Reasoning](https://arxiv.org/abs/2507.01908)
*Qingdong He,Xueqin Chen,Chaoyi Wang,Yanjie Pan,Xiaobin Hu,Zhenye Gan,Yabiao Wang,Chengjie Wang,Xiangtai Li,Jiangning Zhang*

Main category: cs.CV

TL;DR: 该研究提出了Reason50K数据集和ReasonBrain框架，用于复杂隐性推理指令图像编辑，特别关注推理能力的提升。


<details>
  <summary>Details</summary>
Motivation: 当前的指令驱动图像编辑通常只支持简单明确的指令，缺乏复杂推理能力的支持，同时缺少相应的训练和评估数据集。

Method: 提出了Reason50K数据集和ReasonBrain框架，Reason50K包含超过50K样本，覆盖四种推理场景，并通过多模态大语言模型和扩散模型的结合引入精细化推理线索提取和跨模态增强机制。

Result: 实验表明ReasonBrain框架在推理任务上优于现有方法，并在传统图像编辑任务中展现了强大的零样本泛化能力。

Conclusion: Reason50K和ReasonBrain为复杂推理型图像编辑提供了新的研究方向及工具，为未来研究奠定了重要基础。

Abstract: Instruction-based image editing (IIE) has advanced rapidly with the success
of diffusion models. However, existing efforts primarily focus on simple and
explicit instructions to execute editing operations such as adding, deleting,
moving, or swapping objects. They struggle to handle more complex implicit
hypothetical instructions that require deeper reasoning to infer plausible
visual changes and user intent. Additionally, current datasets provide limited
support for training and evaluating reasoning-aware editing capabilities.
Architecturally, these methods also lack mechanisms for fine-grained detail
extraction that support such reasoning. To address these limitations, we
propose Reason50K, a large-scale dataset specifically curated for training and
evaluating hypothetical instruction reasoning image editing, along with
ReasonBrain, a novel framework designed to reason over and execute implicit
hypothetical instructions across diverse scenarios. Reason50K includes over 50K
samples spanning four key reasoning scenarios: Physical, Temporal, Causal, and
Story reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs)
for editing guidance generation and a diffusion model for image synthesis,
incorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture
detailed visual and textual semantics essential for supporting instruction
reasoning. To mitigate the semantic loss, we further introduce a Cross-Modal
Enhancer (CME) that enables rich interactions between the fine-grained cues and
MLLM-derived features. Extensive experiments demonstrate that ReasonBrain
consistently outperforms state-of-the-art baselines on reasoning scenarios
while exhibiting strong zero-shot generalization to conventional IIE tasks. Our
dataset and code will be released publicly.

</details>


### [79] [Modality Agnostic, patient-specific digital twins modeling temporally varying digestive motion](https://arxiv.org/abs/2507.01909)
*Jorge Tapias Gomez,Nishant Nadkarni,Lando S. Bosma,Jue Jiang,Ergys D. Subashi,William P. Segars,James M. Balter,Mert R Sabuncu,Neelam Tyagi,Harini Veeraraghavan*

Main category: cs.CV

TL;DR: 本文探讨了一种通过数字孪生技术模拟胃肠道运动的方法，用于验证图像配准的精度和剂量映射的准确性。


<details>
  <summary>Details</summary>
Motivation: 胃肠道器官由于高运动性，临床上对可变形图像配准精度的评估困难，因此需借助数字孪生模型来提供评估基准。

Method: 通过分析患者3D扫描数据，并结合文献中胃肠道运动模型，生成时间序列的模拟运动数据。此外，通过目标配准误差、Dice相似系数和95% Hausdorff距离对6种配准方法进行评估。同时，研究对患者辐射剂量分布进行变形映射验证。

Result: 提出的数字孪生技术可以较高精度模拟真实胃肠道运动，配准误差在0.8mm以内，同时验证了配准工具的精度以及剂量映射的有效性。

Conclusion: 该研究提出的模拟管道对动态、复杂解剖区域的图像配准和剂量验证具有重大意义，可提高空间和剂量学精度。

Abstract: Objective: Clinical implementation of deformable image registration (DIR)
requires voxel-based spatial accuracy metrics such as manually identified
landmarks, which are challenging to implement for highly mobile
gastrointestinal (GI) organs. To address this, patient-specific digital twins
(DT) modeling temporally varying motion were created to assess the accuracy of
DIR methods. Approach: 21 motion phases simulating digestive GI motion as 4D
sequences were generated from static 3D patient scans using published
analytical GI motion models through a semi-automated pipeline. Eleven datasets,
including six T2w FSE MRI (T2w MRI), two T1w 4D golden-angle stack-of-stars,
and three contrast-enhanced CT scans. The motion amplitudes of the DTs were
assessed against real patient stomach motion amplitudes extracted from
independent 4D MRI datasets. The generated DTs were then used to assess six
different DIR methods using target registration error, Dice similarity
coefficient, and the 95th percentile Hausdorff distance using summary metrics
and voxel-level granular visualizations. Finally, for a subset of T2w MRI scans
from patients treated with MR-guided radiation therapy, dose distributions were
warped and accumulated to assess dose warping errors, including evaluations of
DIR performance in both low- and high-dose regions for patient-specific error
estimation. Main results: Our proposed pipeline synthesized DTs modeling
realistic GI motion, achieving mean and maximum motion amplitudes and a mean
log Jacobian determinant within 0.8 mm and 0.01, respectively, similar to
published real-patient gastric motion data. It also enables the extraction of
detailed quantitative DIR performance metrics and rigorous validation of dose
mapping accuracy. Significance: The pipeline enables rigorously testing DIR
tools for dynamic, anatomically complex regions enabling granular spatial and
dosimetric accuracies.

</details>


### [80] [3D Reconstruction and Information Fusion between Dormant and Canopy Seasons in Commercial Orchards Using Deep Learning and Fast GICP](https://arxiv.org/abs/2507.01912)
*Ranjan Sapkota,Zhichao Meng,Martin Churuvija,Xiaoqiang Du,Zenghong Ma,Manoj Karkee*

Main category: cs.CV

TL;DR: 本文研究了一个信息融合框架，结合不同季节的树冠结构数据，提升了整个生长季节的自动化作物负载管理能力。


<details>
  <summary>Details</summary>
Motivation: 果园自动化中，树冠生长期的浓密叶冠使得树结构（如树干和树枝）能见度降低，限制了机器视觉系统的能力。因此，这项研究利用落叶期的开放性树冠结构来弥补这一局限。

Method: 提出了一个信息融合框架，结合落叶和树冠两个季节的高分辨率RGB-D影像。框架包括使用YOLOv9-Seg进行实例分割，Kinect Fusion进行3D重建，以及Fast GICP进行跨季节模型对齐。

Result: YOLOv9-Seg在手动标注的图片数据集上达到了0.0047的MSE和0.78的mAP@50；Kinect Fusion重建的树几何模型的尺寸测量结果分别为：树干直径的RMSE为5.23毫米，树枝直径的RMSE为4.50毫米，树枝间距的RMSE为13.72毫米；Fast GICP实现了最低适配分数为0.00197的精确跨季节注册。

Conclusion: 这一框架能够在生长期重度遮挡情况下对树结构进行综合建模，从而提升修剪、疏枝等果园自动化操作的精度，并实现跨季节的结构数据融合，为农业自动化提供了更强的技术支持。

Abstract: In orchard automation, dense foliage during the canopy season severely
occludes tree structures, minimizing visibility to various canopy parts such as
trunks and branches, which limits the ability of a machine vision system.
However, canopy structure is more open and visible during the dormant season
when trees are defoliated. In this work, we present an information fusion
framework that integrates multi-seasonal structural data to support robotic and
automated crop load management during the entire growing season. The framework
combines high-resolution RGB-D imagery from both dormant and canopy periods
using YOLOv9-Seg for instance segmentation, Kinect Fusion for 3D
reconstruction, and Fast Generalized Iterative Closest Point (Fast GICP) for
model alignment. Segmentation outputs from YOLOv9-Seg were used to extract
depth-informed masks, which enabled accurate 3D point cloud reconstruction via
Kinect Fusion; these reconstructed models from each season were subsequently
aligned using Fast GICP to achieve spatially coherent multi-season fusion. The
YOLOv9-Seg model, trained on manually annotated images, achieved a mean squared
error (MSE) of 0.0047 and segmentation mAP@50 scores up to 0.78 for trunks in
dormant season dataset. Kinect Fusion enabled accurate reconstruction of tree
geometry, validated with field measurements resulting in root mean square
errors (RMSE) of 5.23 mm for trunk diameter, 4.50 mm for branch diameter, and
13.72 mm for branch spacing. Fast GICP achieved precise cross-seasonal
registration with a minimum fitness score of 0.00197, allowing integrated,
comprehensive tree structure modeling despite heavy occlusions during the
growing season. This fused structural representation enables robotic systems to
access otherwise obscured architectural information, improving the precision of
pruning, thinning, and other automated orchard operations.

</details>


### [81] [IC-Custom: Diverse Image Customization via In-Context Learning](https://arxiv.org/abs/2507.01926)
*Yaowei Li,Xiaoyu Li,Zhaoyang Zhang,Yuxuan Bian,Gan Liu,Xinyuan Li,Jiale Xu,Wenbo Hu,Yating Liu,Lingen Li,Jing Cai,Yuexian Zou,Yancheng He,Ying Shan*

Main category: cs.CV

TL;DR: IC-Custom是一种统一框架，结合位置感知和非位置感知图像定制，通过上下文学习实现多种需求。


<details>
  <summary>Details</summary>
Motivation: 现有方法将图像个性化分为不同范畴，缺乏统一框架，限制实际应用。

Method: 提出IC-Custom框架，利用多模态注意机制并引入ICMA机制，结合高质量数据集训练。

Result: IC-Custom显著优于现有工作流及同类模型，在多个评估指标上表现突出。

Conclusion: IC-Custom在多个工业应用场景中展现卓越能力，以极少参数优化实现显著效果。

Abstract: Image customization, a crucial technique for industrial media production,
aims to generate content that is consistent with reference images. However,
current approaches conventionally separate image customization into
position-aware and position-free customization paradigms and lack a universal
framework for diverse customization, limiting their applications across various
scenarios. To overcome these limitations, we propose IC-Custom, a unified
framework that seamlessly integrates position-aware and position-free image
customization through in-context learning. IC-Custom concatenates reference
images with target images to a polyptych, leveraging DiT's multi-modal
attention mechanism for fine-grained token-level interactions. We introduce the
In-context Multi-Modal Attention (ICMA) mechanism with learnable task-oriented
register tokens and boundary-aware positional embeddings to enable the model to
correctly handle different task types and distinguish various inputs in
polyptych configurations. To bridge the data gap, we carefully curated a
high-quality dataset of 12k identity-consistent samples with 8k from real-world
sources and 4k from high-quality synthetic data, avoiding the overly glossy and
over-saturated synthetic appearance. IC-Custom supports various industrial
applications, including try-on, accessory placement, furniture arrangement, and
creative IP customization. Extensive evaluations on our proposed ProductBench
and the publicly available DreamBench demonstrate that IC-Custom significantly
outperforms community workflows, closed-source models, and state-of-the-art
open-source approaches. IC-Custom achieves approximately 73% higher human
preference across identity consistency, harmonicity, and text alignment
metrics, while training only 0.4% of the original model parameters. Project
page: https://liyaowei-stu.github.io/project/IC_Custom

</details>


### [82] [evMLP: An Efficient Event-Driven MLP Architecture for Vision](https://arxiv.org/abs/2507.01927)
*Zhentan Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种名为evMLP的新型视觉模型，并通过事件驱动的局部更新机制提高视频处理的计算效率，同时保持准确率。


<details>
  <summary>Details</summary>
Motivation: 目前深度神经网络广泛应用于计算机视觉领域，然而现有方法在处理时序图像（如视频数据）时存在冗余计算的问题。作者旨在提出一种提高计算效率的新框架。

Method: 提出了一种名为evMLP的模型，使用MLP独立处理图像或特征贴片，并定义帧间的变化为事件，通过事件驱动的局部更新机制，选择性处理发生事件的区域，减少冗余计算。

Result: evMLP在ImageNet图像分类实验中取得了与当前最先进模型相当的准确率，并在多个视频数据集的实验结果中，通过事件驱动机制显著降低了计算成本，同时维持了与非事件驱动基线模型一致的输出。

Conclusion: evMLP通过事件驱动机制实现了效率提升，验证了其在视频处理中减少冗余计算的潜力，具有较强的研究价值和实际应用前景。

Abstract: Deep neural networks have achieved remarkable results in computer vision
tasks. In the early days, Convolutional Neural Networks (CNNs) were the
mainstream architecture. In recent years, Vision Transformers (ViTs) have
become increasingly popular. In addition, exploring applications of multi-layer
perceptrons (MLPs) has provided new perspectives for research into vision model
architectures. In this paper, we present evMLP accompanied by a simple
event-driven local update mechanism. The proposed evMLP can independently
process patches on images or feature maps via MLPs. We define changes between
consecutive frames as "events". Under the event-driven local update mechanism,
evMLP selectively processes patches where events occur. For sequential image
data (e.g., video processing), this approach improves computational performance
by avoiding redundant computations. Through ImageNet image classification
experiments, evMLP attains accuracy competitive with state-of-the-art models.
More significantly, experimental results on multiple video datasets demonstrate
that evMLP reduces computational cost via its event-driven local update
mechanism while maintaining output consistency with its non-event-driven
baseline. The code and trained models are available at
https://github.com/i-evi/evMLP.

</details>


### [83] [How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks](https://arxiv.org/abs/2507.01955)
*Rahul Ramachandran,Ali Garjani,Roman Bachmann,Andrei Atanov,Oğuzhan Fatih Kar,Amir Zamir*

Main category: cs.CV

TL;DR: 本文评估了多模态基础模型（如GPT-4o）在标准计算机视觉任务上的表现，通过标准化的基准框架发现了它们的优缺点和局限性。


<details>
  <summary>Details</summary>
Motivation: 旨在弄清多模态基础模型在理解视觉方面的能力，并评估其在计算机视觉任务中的性能表现。

Method: 通过针对标准视觉任务设计文本提示任务，将多模态基础模型嵌入到标准化的基准框架中进行比较，包括语义分割、目标检测、图像分类以及深度和表面法线预测等任务。

Result: 1) 多模态模型在任何任务上都难以达到专业模型的水准；2) 作为通用模型表现令人尊敬；3) 通常在语义任务中表现优于几何任务；4) 模型对提示链的敏感度有所不同，较好的模型更稳定；5) GPT-4o在非推理模型中表现最佳；6) 推理模型（如o3）在几何任务中有所提升；7) 集成图像生成的模型表明存在错觉和空间失配等问题。

Conclusion: 多模态基础模型在计算机视觉任务中展现出一定潜力，但在性能成熟度和专用性方面仍与专业模型存在显著差距。

Abstract: Multimodal foundation models, such as GPT-4o, have recently made remarkable
progress, but it is not clear where exactly these models stand in terms of
understanding vision. In this paper, we benchmark the performance of popular
multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0
Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision
tasks (semantic segmentation, object detection, image classification, depth and
surface normal prediction) using established datasets (e.g., COCO, ImageNet and
its variants, etc).
  The main challenges to performing this are: 1) most models are trained to
output text and cannot natively express versatile domains, such as segments or
3D geometry, and 2) many leading models are proprietary and accessible only at
an API level, i.e., there is no weight access to adapt them. We address these
challenges by translating standard vision tasks into equivalent text-promptable
and API-compatible tasks via prompt chaining to create a standardized
benchmarking framework.
  We observe that 1) the models are not close to the state-of-the-art
specialist models at any task. However, 2) they are respectable generalists;
this is remarkable as they are presumably trained on primarily image-text-based
tasks. 3) They perform semantic tasks notably better than geometric ones. 4)
While the prompt-chaining techniques affect performance, better models exhibit
less sensitivity to prompt variations. 5) GPT-4o performs the best among
non-reasoning models, securing the top position in 4 out of 6 tasks, 6)
reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a
preliminary analysis of models with native image generation, like the latest
GPT-4o, shows they exhibit quirks like hallucinations and spatial
misalignments.

</details>


### [84] [CI-VID: A Coherent Interleaved Text-Video Dataset](https://arxiv.org/abs/2507.01938)
*Yiming Ju,Jijin Hu,Zhengxiong Luo,Haoge Deng,hanyu Zhao,Li Du,Chengwei Wu,Donglin Hao,Xinlong Wang,Tengfei Pan*

Main category: cs.CV

TL;DR: 这篇论文提出了CI-VID数据集，专注于从文本与视频生成连续多场景的视频，并验证其在故事内容生成中的效果。


<details>
  <summary>Details</summary>
Motivation: 现有数据集仅支持孤立的文本-视频配对生成，无法满足生成连贯多场景视频序列的需求。

Method: 引入一个包含34万条数据的CI-VID数据集，该数据集包含文本和视频序列，通过文-视结合提升连贯但多场景视频生成能力，提供综合性基准测试框架并进行实验评估。

Result: 实验显示，在CI-VID数据集上训练的模型在生成视频序列时，准确性和内容连贯性显著提高。

Conclusion: 该数据集能够支持更加连贯和过渡自然的视频生成，特别是在基于故事驱动内容生成中展现明显优势，并已开放数据及相关代码。

Abstract: Text-to-video (T2V) generation has recently attracted considerable attention,
resulting in the development of numerous high-quality datasets that have
propelled progress in this area. However, existing public datasets are
primarily composed of isolated text-video (T-V) pairs and thus fail to support
the modeling of coherent multi-clip video sequences. To address this
limitation, we introduce CI-VID, a dataset that moves beyond isolated
text-to-video (T2V) generation toward text-and-video-to-video (TV2V)
generation, enabling models to produce coherent, multi-scene video sequences.
CI-VID contains over 340,000 samples, each featuring a coherent sequence of
video clips with text captions that capture both the individual content of each
clip and the transitions between them, enabling visually and textually grounded
generation. To further validate the effectiveness of CI-VID, we design a
comprehensive, multi-dimensional benchmark incorporating human evaluation,
VLM-based assessment, and similarity-based metrics. Experimental results
demonstrate that models trained on CI-VID exhibit significant improvements in
both accuracy and content consistency when generating video sequences. This
facilitates the creation of story-driven content with smooth visual transitions
and strong temporal coherence, underscoring the quality and practical utility
of the CI-VID dataset We release the CI-VID dataset and the accompanying code
for data construction and evaluation at: https://github.com/ymju-BAAI/CI-VID

</details>


### [85] [Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation](https://arxiv.org/abs/2507.01957)
*Zhuoyang Zhang,Luke J. Huang,Chengyue Wu,Shang Yang,Kelly Peng,Yao Lu,Song Han*

Main category: cs.CV

TL;DR: 提出了一种新的方法Locality-aware Parallel Decoding (LPD)用以加速自回归图像生成，从而显著减少生成步骤，降低延迟，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统自回归图像生成方法通过逐步预测下一个区域，效率低且延迟高。现有研究尝试通过多区域并行预测加速过程，但实现的并行度有限。

Method: 引入了两项关键技术：(1) 灵活的并行自回归建模技术(Flexible Parallelized Autoregressive Modeling)，支持任意生成顺序及并行度；(2) 位置感知生成顺序(Locality-aware Generation Ordering)，通过分组减少组内依赖，提高上下文支持，提升生成质量。

Result: 在ImageNet类条件生成任务上，将256$	imes$256和512$	imes$512分辨率的生成步骤分别减少至20和48，同时相比其他并行自回归模型延迟降低至少3.4倍。

Conclusion: 提出的LPD方法显著提升了自回归图像生成的效率和速度，同时保持了高质量的生成效果，表明其在该领域具有潜在的实用价值。

Abstract: We present Locality-aware Parallel Decoding (LPD) to accelerate
autoregressive image generation. Traditional autoregressive image generation
relies on next-patch prediction, a memory-bound process that leads to high
latency. Existing works have tried to parallelize next-patch prediction by
shifting to multi-patch prediction to accelerate the process, but only achieved
limited parallelization. To achieve high parallelization while maintaining
generation quality, we introduce two key techniques: (1) Flexible Parallelized
Autoregressive Modeling, a novel architecture that enables arbitrary generation
ordering and degrees of parallelization. It uses learnable position query
tokens to guide generation at target positions while ensuring mutual visibility
among concurrently generated tokens for consistent parallel decoding. (2)
Locality-aware Generation Ordering, a novel schedule that forms groups to
minimize intra-group dependencies and maximize contextual support, enhancing
generation quality. With these designs, we reduce the generation steps from 256
to 20 (256$\times$256 res.) and 1024 to 48 (512$\times$512 res.) without
compromising quality on the ImageNet class-conditional generation, and
achieving at least 3.4$\times$ lower latency than previous parallelized
autoregressive models.

</details>


### [86] [LongAnimation: Long Animation Generation with Dynamic Global-Local Memory](https://arxiv.org/abs/2507.01945)
*Nan Chen,Mengqi Huang,Yihao Meng,Zhendong Mao*

Main category: cs.CV

TL;DR: 提出了一种名为LongAnimation的新框架，用于实现动画上色中的长期颜色一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅限于短期动画上色，未能有效考虑长期颜色一致性问题。

Method: 引入动态全局-局部框架，包括SketchDiT模块、动态全局-局部记忆（DGLM）模块及颜色一致性奖励机制，并在推理阶段通过颜色一致性融合平滑过渡。

Result: 在短期（14帧）与长期（平均500帧）动画上色实验中，框架证明能有效保持颜色一致性。

Conclusion: 新方法显著提高了动画上色中短期和长期的颜色一致性，展现出较大的工业应用前景。

Abstract: Animation colorization is a crucial part of real animation industry
production. Long animation colorization has high labor costs. Therefore,
automated long animation colorization based on the video generation model has
significant research value. Existing studies are limited to short-term
colorization. These studies adopt a local paradigm, fusing overlapping features
to achieve smooth transitions between local segments. However, the local
paradigm neglects global information, failing to maintain long-term color
consistency. In this study, we argue that ideal long-term color consistency can
be achieved through a dynamic global-local paradigm, i.e., dynamically
extracting global color-consistent features relevant to the current generation.
Specifically, we propose LongAnimation, a novel framework, which mainly
includes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color
Consistency Reward. The SketchDiT captures hybrid reference features to support
the DGLM module. The DGLM module employs a long video understanding model to
dynamically compress global historical features and adaptively fuse them with
the current generation features. To refine the color consistency, we introduce
a Color Consistency Reward. During inference, we propose a color consistency
fusion to smooth the video segment transition. Extensive experiments on both
short-term (14 frames) and long-term (average 500 frames) animations show the
effectiveness of LongAnimation in maintaining short-term and long-term color
consistency for open-domain animation colorization task. The code can be found
at https://cn-makers.github.io/long_animation_web/.

</details>


### [87] [Kwai Keye-VL Technical Report](https://arxiv.org/abs/2507.01949)
*Kwai Keye Team,Biao Yang,Bin Wen,Changyi Liu,Chenglong Chu,Chengru Song,Chongling Rao,Chuan Yi,Da Li,Dunju Zang,Fan Yang,Guorui Zhou,Hao Peng,Haojie Ding,Jiaming Huang,Jiangxia Cao,Jiankang Chen,Jingyun Hua,Jin Ouyang,Kaibing Chen,Kaiyu Jiang,Kaiyu Tang,Kun Gai,Shengnan Zhang,Siyang Mao,Sui Huang,Tianke Zhang,Tingting Gao,Wei Chen,Wei Yuan,Xiangyu Wu,Xiao Hu,Xingyu Lu,Yang Zhou,Yi-Fan Zhang,Yiping Yang,Yulong Chen,Zhenhua Wu,Zhenyu Li,Zhixin Ling,Ziming Li,Dehua Ma,Di Xu,Haixuan Gao,Hang Li,Jiawei Guo,Jing Wang,Lejian Ren,Muhao Wei,Qianqian Wang,Qigen Hu,Shiyao Wang,Tao Yu,Xinchen Luo,Yan Li,Yiming Liang,Yuhang Hu,Zeyi Lu,Zhuoran Yang,Zixing Zhang*

Main category: cs.CV

TL;DR: 提出Keye-VL，一个8亿参数的多模态基础模型，针对短视频理解，在视觉-语言任务中表现抢眼。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在短视频中表现有限，而短视频是数字媒体的主流形式，因此需要改进模型以提升其理解动态短视频的能力。

Method: 构建了包含超6000亿个token的高质量大规模视频数据集，采用四阶段预训练和两阶段后期训练的策略，并使用五模式“冷启动”数据混合技术，结合强化学习和对齐步骤提升模型能力。

Result: Keye-VL在视频任务中达到了最新的性能，且在图像任务中竞争力强。此外，模型在新设计的短视频基准KC-MMBench上表现出显著优势。

Conclusion: Keye-VL不仅提升了短视频理解能力，还在视觉语言任务中保持了通用性，对该领域研究有重要推动作用。

Abstract: While Multimodal Large Language Models (MLLMs) demonstrate remarkable
capabilities on static images, they often fall short in comprehending dynamic,
information-dense short-form videos, a dominant medium in today's digital
landscape. To bridge this gap, we introduce \textbf{Kwai Keye-VL}, an
8-billion-parameter multimodal foundation model engineered for leading-edge
performance in short-video understanding while maintaining robust
general-purpose vision-language abilities. The development of Keye-VL rests on
two core pillars: a massive, high-quality dataset exceeding 600 billion tokens
with a strong emphasis on video, and an innovative training recipe. This recipe
features a four-stage pre-training process for solid vision-language alignment,
followed by a meticulous two-phase post-training process. The first
post-training stage enhances foundational capabilities like instruction
following, while the second phase focuses on stimulating advanced reasoning. In
this second phase, a key innovation is our five-mode ``cold-start'' data
mixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think
with image'', and high-quality video data. This mixture teaches the model to
decide when and how to reason. Subsequent reinforcement learning (RL) and
alignment steps further enhance these reasoning capabilities and correct
abnormal model behaviors, such as repetitive outputs. To validate our approach,
we conduct extensive evaluations, showing that Keye-VL achieves
state-of-the-art results on public video benchmarks and remains highly
competitive on general image-based tasks (Figure 1). Furthermore, we develop
and release the \textbf{KC-MMBench}, a new benchmark tailored for real-world
short-video scenarios, where Keye-VL shows a significant advantage.

</details>


### [88] [FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model](https://arxiv.org/abs/2507.01953)
*Yukang Cao,Chenyang Si,Jinghao Wang,Ziwei Liu*

Main category: cs.CV

TL;DR: FreeMorph是一种无需调试的图像变形方法，与现有方法相比，可更高效地对不同语义/布局的输入生成高保真度的图像变形。


<details>
  <summary>Details</summary>
Motivation: 现有的图像变形方法依赖于微调预训练扩散模型，存在时间限制以及语义和布局不一致的问题。

Method: 提出FreeMorph，通过引入两个关键创新：1) 指导感知的球面插值设计，修改自注意力模块，明确引导输入图像，维护标识和方向性过渡；2) 步骤导向的变化趋势设计，结合输入图像的自注意力模块，保证一致控制的变化过程。

Result: 实验表明，FreeMorph比现有方法快10倍至50倍，且在图像变形领域达到了新的SOTA（最先进技术性能）。

Conclusion: FreeMorph不仅无需调试且更高效，提供了在不同语义和布局前提下图像变形的高质量解决方案。

Abstract: We present FreeMorph, the first tuning-free method for image morphing that
accommodates inputs with different semantics or layouts. Unlike existing
methods that rely on finetuning pre-trained diffusion models and are limited by
time constraints and semantic/layout discrepancies, FreeMorph delivers
high-fidelity image morphing without requiring per-instance training. Despite
their efficiency and potential, tuning-free methods face challenges in
maintaining high-quality results due to the non-linear nature of the multi-step
denoising process and biases inherited from the pre-trained diffusion model. In
this paper, we introduce FreeMorph to address these challenges by integrating
two key innovations. 1) We first propose a guidance-aware spherical
interpolation design that incorporates explicit guidance from the input images
by modifying the self-attention modules, thereby addressing identity loss and
ensuring directional transitions throughout the generated sequence. 2) We
further introduce a step-oriented variation trend that blends self-attention
modules derived from each input image to achieve controlled and consistent
transitions that respect both inputs. Our extensive evaluations demonstrate
that FreeMorph outperforms existing methods, being 10x ~ 50x faster and
establishing a new state-of-the-art for image morphing.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [89] [MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered](https://arxiv.org/abs/2507.01019)
*Imran Mirza,Cole Huang,Ishwara Vasista,Rohan Patil,Asli Akalin,Sean O'Brien,Kevin Zhu*

Main category: cs.CL

TL;DR: 本文提出了一个名为MALIBU的基准，用于评估基于大语言模型的多智能体系统如何隐性地强化社会偏见和刻板印象。


<details>
  <summary>Details</summary>
Motivation: 随着基于多智能体的系统在个性化交互中的广泛应用，对其公平性和公正性的担忧也在增长。本文旨在解决多智能体系统可能加剧隐性偏见的问题。

Method: 开发了MALIBU基准，通过场景评估，对具有不同人口统计特征的AI输出进行评分和比较，采用多步骤判断系统分析偏见强度。

Result: 研究发现偏见缓解有时可能会偏向弱势群体形象，难以达到真正的中立性。

Conclusion: 精准的偏见检测、平衡的公平性策略和透明的评估基准对于多智能体系统的发展至关重要。

Abstract: Multi-agent systems, which consist of multiple AI models interacting within a
shared environment, are increasingly used for persona-based interactions.
However, if not carefully designed, these systems can reinforce implicit biases
in large language models (LLMs), raising concerns about fairness and equitable
representation. We present MALIBU, a novel benchmark developed to assess the
degree to which LLM-based multi-agent systems implicitly reinforce social
biases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems
through scenario-based assessments. AI models complete tasks within predefined
contexts, and their responses undergo evaluation by an LLM-based multi-agent
judging system in two phases. In the first phase, judges score responses
labeled with specific demographic personas (e.g., gender, race, religion)
across four metrics. In the second phase, judges compare paired responses
assigned to different personas, scoring them and selecting the superior
response. Our study quantifies biases in LLM-generated outputs, revealing that
bias mitigation may favor marginalized personas over true neutrality,
emphasizing the need for nuanced detection, balanced fairness strategies, and
transparent evaluation benchmarks in multi-agent systems.

</details>


### [90] [Event-based evaluation of abstractive news summarization](https://arxiv.org/abs/2507.01160)
*Huiling You,Samia Touileb,Erik Velldal,Lilja Øvrelid*

Main category: cs.CL

TL;DR: 本研究提出通过计算生成摘要、参考摘要和新闻文章之间的重叠事件来评估自动生成摘要的质量，特别是在挪威数据集的实验中提供了洞察。


<details>
  <summary>Details</summary>
Motivation: 当前自动生成摘要的评估方法过于依赖人类撰写的金标准摘要，缺乏对摘要中实际事件信息的深层评估。

Method: 通过比较生成摘要、参考摘要与原始新闻文章的事件重叠情况来评估摘要质量。

Result: 在挪威数据集上进行实验，使用带注释的事件和由专家撰写的摘要，展示了新方法能更好地捕捉摘要中的事件信息。

Conclusion: 评估生成摘要质量的新方法提供了对事件信息的更深入分析，有助于改进摘要质量的评价体系。

Abstract: An abstractive summary of a news article contains its most important
information in a condensed version. The evaluation of automatically generated
summaries by generative language models relies heavily on human-authored
summaries as gold references, by calculating overlapping units or similarity
scores. News articles report events, and ideally so should the summaries. In
this work, we propose to evaluate the quality of abstractive summaries by
calculating overlapping events between generated summaries, reference
summaries, and the original news articles. We experiment on a richly annotated
Norwegian dataset comprising both events annotations and summaries authored by
expert human annotators. Our approach provides more insight into the event
information contained in the summaries.

</details>


### [91] [Matching and Linking Entries in Historical Swedish Encyclopedias](https://arxiv.org/abs/2507.01170)
*Simon Börjesson,Erik Ersmark,Pierre Nugues*

Main category: cs.CL

TL;DR: 本文分析瑞典百科全书《Nordisk familjebok》前两版的地理条目演变。


<details>
  <summary>Details</summary>
Motivation: 研究百科全书内容的历史变迁及其反映的社会与知识变化，特别关注纸质资源的数字化与分析潜力。

Method: 使用语义句子嵌入匹配第一版和第二版的条目，构建文本分类器提取地理条目，并将其链接到Wikidata以分析地理趋势。

Result: 发现从第一版到第二版，地理焦点从欧洲转向北美洲、非洲、亚洲、大洋洲及北斯堪的纳维亚，受一战和新兴权力影响。

Conclusion: 百科全书内容随时间变化反映了社会历史动态，数字化研究提供了深入理解的可能性。

Abstract: The \textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and
20th centuries. It was written by a team of experts and aimed to be an
intellectual reference, stressing precision and accuracy. This encyclopedia had
four main editions remarkable by their size, ranging from 20 to 38 volumes. As
a consequence, the \textit{Nordisk familjebok} had a considerable influence in
universities, schools, the media, and society overall. As new editions were
released, the selection of entries and their content evolved, reflecting
intellectual changes in Sweden.
  In this paper, we used digitized versions from \textit{Project Runeberg}. We
first resegmented the raw text into entries and matched pairs of entries
between the first and second editions using semantic sentence embeddings. We
then extracted the geographical entries from both editions using a
transformer-based classifier and linked them to Wikidata. This enabled us to
identify geographic trends and possible shifts between the first and second
editions, written between 1876-1899 and 1904-1926, respectively.
  Interpreting the results, we observe a small but significant shift in
geographic focus away from Europe and towards North America, Africa, Asia,
Australia, and northern Scandinavia from the first to the second edition,
confirming the influence of the First World War and the rise of new powers. The
code and data are available on GitHub at
https://github.com/sibbo/nordisk-familjebok.

</details>


### [92] [MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis](https://arxiv.org/abs/2507.01213)
*Adamu Lawan,Juhua Pu,Haruna Yunusa,Jawad Muhammad,Muhammad Lawan*

Main category: cs.CL

TL;DR: 本文提出了xLSTM与多头指数门控融合（MEGA）框架，以提升Aspect-based Sentiment Analysis（ABSA）任务的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的ABSA方法在处理计算效率和性能之间难以平衡，而新兴的xLSTM技术在长期依赖建模方面表现突出，但尚未在ABSA领域充分探索。

Method: 提出了结合双向增强mLSTM架构和部分反向流(PF-mLSTM)的MEGA框架，并引入多头交叉指数门控融合机制(MECGAF)，以增强短程依赖捕获能力，同时维护全局上下文和效率。

Result: 实验结果表明，MEGA在三个基准数据集上表现优于现有的最新方法，具有更高的准确性和效率。

Conclusion: MEGA框架突破了现有ABSA模型的局限，在优化计算效率和短程依赖捕获能力的同时，实现了更高的性能。

Abstract: Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language
Processing (NLP) task that extracts aspects from text and determines their
associated sentiments, enabling fine-grained analysis of user opinions.
Existing ABSA methods struggle to balance computational efficiency with high
performance: deep learning models often lack global context, transformers
demand significant computational resources, and Mamba-based approaches face
CUDA dependency and diminished local correlations. Recent advancements in
Extended Long Short-Term Memory (xLSTM) models, particularly their efficient
modeling of long-range dependencies, have significantly advanced the NLP
community. However, their potential in ABSA remains untapped. To this end, we
propose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework
integrating a bi-directional mLSTM architecture with forward and partially
flipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context
modeling by processing the initial sequence segment in reverse with dedicated
parameters, preserving critical short-range patterns. We further introduce an
mLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that
dynamically combines forward mLSTM outputs as query and key with PF-mLSTM
outputs as value, optimizing short-range dependency capture while maintaining
global context and efficiency. Experimental results on three benchmark datasets
demonstrate that MEGA outperforms state-of-the-art baselines, achieving
superior accuracy and efficiency in ABSA tasks.

</details>


### [93] [The Medium Is Not the Message: Deconfounding Text Embeddings via Linear Concept Erasure](https://arxiv.org/abs/2507.01234)
*Yu Fan,Yang Tian,Shauli Ravfogel,Mrinmaya Sachan,Elliott Ash,Alexander Hoyle*

Main category: cs.CL

TL;DR: 研究如何通过去除文本嵌入（embedding）中与来源或语言等无关因素相关的信息，以减少文档相似性和聚类评估中的偏差。


<details>
  <summary>Details</summary>
Motivation: 当前的嵌入相似性度量会因文本来源或语言等非目标属性产生偏差，尤其是在多语种或跨语料库分析中，这些偏倚影响分析的公正性和准确性。

Method: 引入了一种去偏算法，在嵌入表示中移除与已知混杂因素相关的信息，同时保持计算成本最低。

Result: 去偏算法显著改善了不同嵌入类型和任务中的文档相似性与聚类指标，同时验证结果表明对分布外基准数据的性能未受损。

Conclusion: 该方法有效减少了因混杂因素导致的偏置，提升了文本嵌入模型在跨语料库应用中的实际效果，同时不会显著影响其他性能。

Abstract: Embedding-based similarity metrics between text sequences can be influenced
not just by the content dimensions we most care about, but can also be biased
by spurious attributes like the text's source or language. These document
confounders cause problems for many applications, but especially those that
need to pool texts from different corpora. This paper shows that a debiasing
algorithm that removes information about observed confounders from the encoder
representations substantially reduces these biases at a minimal computational
cost. Document similarity and clustering metrics improve across every embedding
variant and task we evaluate -- often dramatically. Interestingly, performance
on out-of-distribution benchmarks is not impacted, indicating that the
embeddings are not otherwise degraded.

</details>


### [94] [GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant](https://arxiv.org/abs/2507.01259)
*Michał Matak,Jarosław A. Chudziak*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型在非英语和非中文语境下的法律任务中的表现，提出了基于检索的gAIus架构，并改进了模型的性能。


<details>
  <summary>Details</summary>
Motivation: 研究非英语和非中文国家的法律任务中，大型语言模型如何提供准确回答及参考依据，尤其专注于波兰法律环境。

Method: 介绍gAIus架构，基于波兰民法典设计解释性更强、人类友好、优于嵌入式方法的检索机制，并使用波兰法律考试单选题数据集评估。

Result: gAIus显著提升了gpt-3.5-turbo-0125的性能（提高419%），超越gpt-4o，将gpt-4o-mini的准确率从31%提升至86%。

Conclusion: gAIus架构证明了在法律检索和问题解答中的有效性，并提出了未来研究方向和应用潜力。

Abstract: In this paper we discuss the capability of large language models to base
their answer and provide proper references when dealing with legal matters of
non-english and non-chinese speaking country. We discuss the history of legal
information retrieval, the difference between case law and statute law, its
impact on the legal tasks and analyze the latest research in this field. Basing
on that background we introduce gAIus, the architecture of the cognitive
LLM-based agent, whose responses are based on the knowledge retrieved from
certain legal act, which is Polish Civil Code. We propose a retrieval mechanism
which is more explainable, human-friendly and achieves better results than
embedding-based approaches. To evaluate our method we create special dataset
based on single-choice questions from entrance exams for law apprenticeships
conducted in Poland. The proposed architecture critically leveraged the
abilities of used large language models, improving the gpt-3.5-turbo-0125 by
419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.
At the end of our paper we show the possible future path of research and
potential applications of our findings.

</details>


### [95] [Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening](https://arxiv.org/abs/2507.01278)
*Cindy Lie Tabuse,David Restepo,Carolina Gracitelli,Fernando Korn Malerbi,Caio Regatieri,Luis Filipe Nakayama*

Main category: cs.CL

TL;DR: 本文研究了GPT-4在模拟眼科决策中的能力，尤其是针对糖尿病视网膜病变(DR)和青光眼的筛查。结果显示GPT-4可以完成基础任务，但对复杂任务的精度不足，临床应用有限。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在眼科中模拟临床推理的能力，评估其在DR和青光眼筛查中的实际效用。

Method: 使用300张标注的视网膜图像，通过结构化提示向GPT-4提供图片描述及患者数据，评估模型在DR严重程度评分、转诊建议及青光眼相关分析中的表现。

Result: GPT-4在DR严重程度分类中表现一般(准确率67.5%)，但在二元转诊任务中有改善；青光眼相关任务表现不佳。元数据并未显著影响结果。

Conclusion: GPT-4能模拟基础的眼科决策，但不适用于复杂任务或临床环境。其潜力更多体现在教育、文档编制或图像标注等领域。

Abstract: Large language models (LLMs) can simulate clinical reasoning based on natural
language prompts, but their utility in ophthalmology is largely unexplored.
This study evaluated GPT-4's ability to interpret structured textual
descriptions of retinal fundus photographs and simulate clinical decisions for
diabetic retinopathy (DR) and glaucoma screening, including the impact of
adding real or synthetic clinical metadata. We conducted a retrospective
diagnostic validation study using 300 annotated fundus images. GPT-4 received
structured prompts describing each image, with or without patient metadata. The
model was tasked with assigning an ICDR severity score, recommending DR
referral, and estimating the cup-to-disc ratio for glaucoma referral.
Performance was evaluated using accuracy, macro and weighted F1 scores, and
Cohen's kappa. McNemar's test and change rate analysis were used to assess the
influence of metadata. GPT-4 showed moderate performance for ICDR
classification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),
driven mainly by correct identification of normal cases. Performance improved
in the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For
glaucoma referral, performance was poor across all settings (accuracy ~78%, F1
<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes
(McNemar p > 0.05), and predictions remained consistent across conditions.
GPT-4 can simulate basic ophthalmic decision-making from structured prompts but
lacks precision for complex tasks. While not suitable for clinical use, LLMs
may assist in education, documentation, or image annotation workflows in
ophthalmology.

</details>


### [96] [Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization](https://arxiv.org/abs/2507.01281)
*Juan Chen,Baolong Bi,Wei Zhang,Jingyan Sui,Xiaofei Zhu,Yuanzhuo Wang,Lingrui Mei,Shenghua Liu*

Main category: cs.CL

TL;DR: 本文介绍了CARE-RAG，一个通过检测和总结冲突来提高检索增强生成系统（RAG）可信性的新框架。


<details>
  <summary>Details</summary>
Motivation: 目前的检索增强生成（RAG）系统容易受到内部知识和外部检索内容冲突的影响，从而降低生成结果的可靠性。

Method: 提出CARE-RAG框架，包含参数感知证据提取、上下文感知证据精炼及基于冲突的总结模块，能可靠综合多源内容并修复不一致或模糊的问答答案。

Result: 实验展示CARE-RAG在包含噪声或冲突证据的场景中，性能优于现有的强基线RAG方法。

Conclusion: CARE-RAG通过冲突驱动的总结和精炼过程，显著提升了检索增强生成系统的可信性和生成结果的质量。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating their parametric knowledge with external retrieved content.
However, knowledge conflicts caused by internal inconsistencies or noisy
retrieved content can severely undermine the generation reliability of RAG
systems.In this work, we argue that LLMs should rethink all evidence, including
both retrieved content and internal knowledge, before generating responses.We
propose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel
framework that improves trustworthiness through Conflict-Driven Summarization
of all available evidence.CARE-RAG first derives parameter-aware evidence by
comparing parameter records to identify diverse internal perspectives. It then
refines retrieved evidences to produce context-aware evidence, removing
irrelevant or misleading content. To detect and summarize conflicts, we distill
a 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable
synthesis across multiple sources.To further ensure evaluation integrity, we
introduce a QA Repair step to correct outdated or ambiguous benchmark
answers.Experiments on revised QA datasets with retrieval data show that
CARE-RAG consistently outperforms strong RAG baselines, especially in scenarios
with noisy or conflicting evidence.

</details>


### [97] [Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks](https://arxiv.org/abs/2507.01297)
*Xinxi Lyu,Michael Duan,Rulin Shao,Pang Wei Koh,Sewon Min*

Main category: cs.CL

TL;DR: 本文研究了通过引入高效、全面的web级数据存储CompactDS，改进检索增强生成（RAG）的应用，特别是在推理密集型基准测试中的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG研究多集中于事实型问答，较少关注推理密集型任务；缺乏一个高质量的、与预训练数据广度对齐的web规模数据存储是制约性能的重要因素。

Method: 提出CompactDS数据存储系统，通过高质量子集筛选和内存近似最近邻结合磁盘精确检索的方式，实现亚秒级检索效率与高精度；并证明其对各种RAG任务的有效性。

Result: CompactDS显著提升了RAG在多个推理密集型基准中的表现，如MMLU提升10%，MMLU Pro提升33%，GPQA提升14%，MATH提升19%。

Conclusion: CompactDS证明了数据来源的多样性（如网络抓取、学术论文、教科书等）和高质量的重要性，且在简化设计的同时超越了谷歌搜索及复杂的代理RAG系统。

Abstract: Retrieval-augmented Generation (RAG) has primarily been studied in limited
settings, such as factoid question answering; more challenging,
reasoning-intensive benchmarks have seen limited success from minimal RAG. In
this work, we challenge this prevailing view on established,
reasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We
identify a key missing component in prior work: a usable, web-scale datastore
aligned with the breadth of pretraining data. To this end, we introduce
CompactDS: a diverse, high-quality, web-scale datastore that achieves high
retrieval accuracy and subsecond latency on a single-node. The key insights are
(1) most web content can be filtered out without sacrificing coverage, and a
compact, high-quality subset is sufficient; and (2) combining in-memory
approximate nearest neighbor (ANN) retrieval and on-disk exact search balances
speed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves
consistent accuracy improvements across all benchmarks and model sizes
(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,
and 19% on MATH. No single data source suffices alone, highlighting the
importance of diversity of sources (web crawls, curated math, academic papers,
textbooks). Finally, we show that our carefully designed in-house datastore
matches or outperforms web search engines such as Google Search, as well as
recently proposed, complex agent-based RAG systems--all while maintaining
simplicity, reproducibility, and self-containment. We release CompactDS and our
retrieval pipeline, supporting future research exploring retrieval-based AI
systems.

</details>


### [98] [La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation](https://arxiv.org/abs/2507.01299)
*Kai Liu,Bowen Xu,Shaoyu Wu,Xin Chen,Hao Zhou,Yongliang Tao,Lulu Hu*

Main category: cs.CL

TL;DR: 提出了一种新的激活稀疏化方法LaRoSA，通过层内正交旋转和Top-K选择实现LLM高效推理，无需额外训练或基于幅值的剪枝。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么需要耗时的恢复训练，要么依赖幅值剪枝导致稀疏性波动和推理速度不稳定。

Method: 利用层内正交旋转转换输入激活至更适合稀疏化的形式，并在旋转后的激活中采用Top-K选择实现一致的模型级稀疏性。

Result: 在LLaMA2-7B 40%稀疏时，LaRoSA仅产生0.17的困惑度差距，实现1.30倍的稳定时间加速，零样本任务的准确率差距缩小到0.54%，并优于TEAL和CATS分别1.77%和17.14%。

Conclusion: LaRoSA无需额外训练，效果显著且适用多种LLM，能实现稳定推理加速和性能保持。

Abstract: Activation sparsity can reduce the computational overhead and memory
transfers during the forward pass of Large Language Model (LLM) inference.
Existing methods face limitations, either demanding time-consuming recovery
training that hinders real-world adoption, or relying on empirical
magnitude-based pruning, which causes fluctuating sparsity and unstable
inference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse
Activation), a novel method for activation sparsification designed to improve
LLM efficiency without requiring additional training or magnitude-based
pruning. We leverage layerwise orthogonal rotations to transform input
activations into rotated forms that are more suitable for sparsification. By
employing a Top-K selection approach within the rotated activations, we achieve
consistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA
is effective across various sizes and types of LLMs, demonstrating minimal
performance degradation and robust inference acceleration. Specifically, for
LLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a
consistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in
zero-shot tasks compared to the dense model to just 0.54%, while surpassing
TEAL by 1.77% and CATS by 17.14%.

</details>


### [99] [Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs](https://arxiv.org/abs/2507.01334)
*Nifu Dan,Yujun Cai,Yiwei Wang*

Main category: cs.CL

TL;DR: 研究探讨先进的指令调整推理模型在物理问题求解中的应用，并取得了卓越的实验结果。


<details>
  <summary>Details</summary>
Motivation: 探索高级推理模型（如Deepseek-R1）在处理复杂物理问题上的能力，特别是基于SciBench基准的表现。

Method: 采用指令调优推理模型，并通过全面的实验评估其解决物理问题的能力和推理模式，结合少样本提示技术进行优化。

Result: 模型在应对复杂物理问题上取得了最先进的准确率，展现了符号推导特点；少样本提示技术进一步提升了性能。

Conclusion: 证明高级指令调整推理模型在物理问题求解中具有极高潜力，并可通过策略调整实现更优表现。

Abstract: Navigating the complexities of physics reasoning has long been a difficult
task for Large Language Models (LLMs), requiring a synthesis of profound
conceptual understanding and adept problem-solving techniques. In this study,
we investigate the application of advanced instruction-tuned reasoning models,
such as Deepseek-R1, to address a diverse spectrum of physics problems curated
from the challenging SciBench benchmark. Our comprehensive experimental
evaluation reveals the remarkable capabilities of reasoning models. Not only do
they achieve state-of-the-art accuracy in answering intricate physics
questions, but they also generate distinctive reasoning patterns that emphasize
on symbolic derivation. Furthermore, our findings indicate that even for these
highly sophisticated reasoning models, the strategic incorporation of few-shot
prompting can still yield measurable improvements in overall accuracy,
highlighting the potential for continued performance gains.

</details>


### [100] [LEDOM: An Open and Fundamental Reverse Language Model](https://arxiv.org/abs/2507.01335)
*Xunjian Yin,Sitao Cheng,Yuxi Xie,Xinyu Hu,Li Lin,Xinyi Wang,Liangming Pan,William Yang Wang,Xiaojun Wan*

Main category: cs.CL

TL;DR: 介绍了LEDOM，一种全新基于逆序训练的语言模型，其在数学推理任务中表现出色，并计划公开所有资源。


<details>
  <summary>Details</summary>
Motivation: 探索逆序语言模型在通用任务中的潜力，提供新的任务视角和工具。

Method: 训练了435B token的逆序语言模型LEDOM，提供了2B和7B参数版本，并提出了利用LEDOM改进前向模型输出的逆序奖励方法。

Result: 通过逆序引导，显著提升了数学推理任务的性能，并验证了LEDOM在后向推理的独特能力。

Conclusion: LEDOM展现了其在广泛任务中的潜力，是基础模型研究的新方向，并承诺开放相关资源以促进未来研究。

Abstract: We introduce LEDOM, the first purely reverse language model, trained
autoregressively on 435B tokens with 2B and 7B parameter variants, which
processes sequences in reverse temporal order through previous token
prediction. For the first time, we present the reverse language model as a
potential foundational model across general tasks, accompanied by a set of
intriguing examples and insights. Based on LEDOM, we further introduce a novel
application: Reverse Reward, where LEDOM-guided reranking of forward language
model outputs leads to substantial performance improvements on mathematical
reasoning tasks. This approach leverages LEDOM's unique backward reasoning
capability to refine generation quality through posterior evaluation. Our
findings suggest that LEDOM exhibits unique characteristics with broad
application potential. We will release all models, training code, and
pre-training data to facilitate future research.

</details>


### [101] [Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy](https://arxiv.org/abs/2507.01352)
*Chris Yuhao Liu,Liang Zeng,Yuzhen Xiao,Jujie He,Jiacai Liu,Chaojie Wang,Rui Yan,Wei Shen,Fuxiang Zhang,Jiacheng Xu,Yang Liu,Yahui Zhou*

Main category: cs.CL

TL;DR: 提出以大规模偏好数据集SynPref-40M作为基础以及人机协作两阶段数据筛选流水线，从而训练出一系列性能卓越的奖励模型Skywork-Reward-V2，并在多项评估基准测试中实现了最先进的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的开放奖励模型性能欠佳，原因可能在于现有偏好数据集的局限性，如范围狭窄、标签合成化或缺乏质量控制。

Method: 构建包含4000万偏好对的大规模数据集SynPref-40M，设计人机协作两阶段流水线：人类提供验证标注，大语言模型依据人类指导进行自动筛选，并对数据进行高质量的筛选和训练。

Result: 基于SynPref-40M训练出的Skywork-Reward-V2奖励模型在七项主要基准测试中性能领先；研究还通过消融实验表明数据规模与高质量筛选的结合至关重要。

Conclusion: Skywork-Reward-V2展示了开放奖励模型的显著进展和现有偏好数据集的潜力，证明人机协作的数据筛选能极大提高数据质量。

Abstract: Despite the critical role of reward models (RMs) in reinforcement learning
from human feedback (RLHF), current state-of-the-art open RMs perform poorly on
most existing evaluation benchmarks, failing to capture the spectrum of nuanced
and sophisticated human preferences. Even approaches that incorporate advanced
training techniques have not yielded meaningful performance improvements. We
hypothesize that this brittleness stems primarily from limitations in
preference datasets, which are often narrowly scoped, synthetically labeled, or
lack rigorous quality control. To address these challenges, we present a
large-scale preference dataset comprising 40 million preference pairs, named
SynPref-40M. To enable data curation at scale, we design a human-AI synergistic
two-stage pipeline that leverages the complementary strengths of human
annotation quality and AI scalability. In this pipeline, humans provide
verified annotations, while large language models perform automatic curation
based on human guidance. Training on this preference mixture, we introduce
Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B
parameters, trained on a carefully curated subset of 26 million preference
pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile
across a wide range of capabilities, including alignment with human
preferences, objective correctness, safety, resistance to stylistic biases, and
best-of-N scaling, achieving state-of-the-art performance across seven major
reward model benchmarks. Ablation studies confirm that the effectiveness of our
approach stems not only from data scale but also from high-quality curation.
The Skywork-Reward-V2 series represents substantial progress in open reward
models, highlighting the untapped potential of existing preference datasets and
demonstrating how human-AI curation synergy can unlock significantly higher
data quality.

</details>


### [102] [Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction](https://arxiv.org/abs/2507.01437)
*Ting Xu,Xiaoxiao Deng,Xiandong Meng,Haifeng Yang,Yan Wu*

Main category: cs.CL

TL;DR: 本文提出了一种基于注意力机制的深度学习模型，用于电子健康记录文本的信息抽取和多标签疾病预测，在MIMIC-IV数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录文本的非结构化特点及其语义的高维复杂性，给信息抽取和疾病预测带来了挑战，需要更高效的模型加以应对。

Method: 采用基于Transformer的模型对临床文本进行表示学习，并通过多层自注意机制捕捉关键医学实体及其语境关系，同时结合Sigmoid多标签分类器和语义对齐机制增强模型性能。

Result: 通过一系列实验验证，提出的方法在多项性能指标上均优于现有方法，并在不同数据规模、干扰程度及模型深度配置下展现出较强的泛化能力。

Conclusion: 提出的框架为处理真实世界临床文本提供了高效的算法基础，对多标签医学文本建模任务具有实用意义。

Abstract: This paper addresses the challenges posed by the unstructured nature and
high-dimensional semantic complexity of electronic health record texts. A deep
learning method based on attention mechanisms is proposed to achieve unified
modeling for information extraction and multi-label disease prediction. The
study is conducted on the MIMIC-IV dataset. A Transformer-based architecture is
used to perform representation learning over clinical text. Multi-layer
self-attention mechanisms are employed to capture key medical entities and
their contextual relationships. A Sigmoid-based multi-label classifier is then
applied to predict multiple disease labels. The model incorporates a
context-aware semantic alignment mechanism, enhancing its representational
capacity in typical medical scenarios such as label co-occurrence and sparse
information. To comprehensively evaluate model performance, a series of
experiments were conducted, including baseline comparisons, hyperparameter
sensitivity analysis, data perturbation studies, and noise injection tests.
Results demonstrate that the proposed method consistently outperforms
representative existing approaches across multiple performance metrics. The
model maintains strong generalization under varying data scales, interference
levels, and model depth configurations. The framework developed in this study
offers an efficient algorithmic foundation for processing real-world clinical
texts and presents practical significance for multi-label medical text modeling
tasks.

</details>


### [103] [LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation](https://arxiv.org/abs/2507.01449)
*Tianyu Liu,Qitan Lv,Hao Li,Xing Gao,Xiao Sun*

Main category: cs.CL

TL;DR: 提出了一种新的推测解码方法LogitSpec，通过扩展检索范围和使用logit预测生成更相关的参考草稿符号，从而提升大语言模型的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法中，草稿生成过程存在计算开销高和草稿准确性不足的问题，难以在实际应用中部署。

Method: LogitSpec通过两步生成草稿符号：首先利用最后一个符号的logit预测下两个符号，其次检索与其相关的参考符号，可无须训练直接集成至现有推理框架。

Result: 实验显示LogitSpec在多种文本生成任务上能实现最高2.61倍加速效果，每步解码平均接受符号数达3.28。

Conclusion: LogitSpec有效解决了推测解码中草稿生成的匹配问题，能显著提升推理速度且降低部署复杂性。

Abstract: Speculative decoding (SD), where a small draft model is employed to propose
draft tokens in advance and then the target model validates them in parallel,
has emerged as a promising technique for LLM inference acceleration. Many
endeavors to improve SD are to eliminate the need for a draft model and
generate draft tokens in a retrieval-based manner in order to further alleviate
the drafting overhead and significantly reduce the difficulty in deployment and
applications. However, retrieval-based SD relies on a matching paradigm to
retrieval the most relevant reference as the draft tokens, where these methods
often fail to find matched and accurate draft tokens. To address this
challenge, we propose LogitSpec to effectively expand the retrieval range and
find the most relevant reference as drafts. Our LogitSpec is motivated by the
observation that the logit of the last token can not only predict the next
token, but also speculate the next next token. Specifically, LogitSpec
generates draft tokens in two steps: (1) utilizing the last logit to speculate
the next next token; (2) retrieving relevant reference for both the next token
and the next next token. LogitSpec is training-free and plug-and-play, which
can be easily integrated into existing LLM inference frameworks. Extensive
experiments on a wide range of text generation benchmarks demonstrate that
LogitSpec can achieve up to 2.61 $\times$ speedup and 3.28 mean accepted tokens
per decoding step. Our code is available at
https://github.com/smart-lty/LogitSpec.

</details>


### [104] [Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities](https://arxiv.org/abs/2507.01479)
*Yingqiang Gao,Kaede Johnson,David Froehlich,Luisa Carrer,Sarah Ebling*

Main category: cs.CL

TL;DR: 本文探讨基于LLM的文本简化系统，提出通过直接偏好优化（DPO）结合目标人群反馈实现个性化文本简化，同时提供个性化开发的完整流程。


<details>
  <summary>Details</summary>
Motivation: 现有LLM文本简化模型未能结合目标用户偏好的反馈，缺乏个性化的满足特定群体需求的能力。

Method: 结合监督微调与直接偏好优化（DPO）技术，利用智力障碍人群对文本简化对比的偏好反馈来后训练模型。此外，还提出一种涵盖数据收集、模型选择、训练和评估的完整个性化开发管道。

Result: 研究显示目标人群的参与对设计个性化文本简化AI解决方案至关重要，证明了方法有效性。

Conclusion: 本文强调结合目标群体反馈与专家知识是发展包容性AI系统的一大进步，为个性化AI解决方案开发提供新思路。

Abstract: Automatic text simplification (ATS) aims to enhance language accessibility
for various target groups, particularly persons with intellectual disabilities.
Recent advancements in generative AI, especially large language models (LLMs),
have substantially improved the quality of machine-generated text
simplifications, thereby mitigating information barriers for the target group.
However, existing LLM-based ATS systems do not incorporate preference feedback
on text simplifications during training, resulting in a lack of personalization
tailored to the specific needs of target group representatives.
  In this work, we extend the standard supervised fine-tuning (SFT) approach
for adapting LLM-based ATS models by leveraging a computationally efficient LLM
alignment technique -- direct preference optimization (DPO). Specifically, we
post-train LLM-based ATS models using human feedback collected from persons
with intellectual disabilities, reflecting their preferences on paired text
simplifications generated by mainstream LLMs. Furthermore, we propose a
pipeline for developing personalized LLM-based ATS systems, encompassing data
collection, model selection, SFT and DPO post-training, and evaluation. Our
findings underscore the necessity of active participation of target group
persons in designing personalized AI accessibility solutions aligned with human
expectations. This work represents a step towards personalizing inclusive AI
systems at the target-group level, incorporating insights not only from text
simplification experts but also from target group persons themselves.

</details>


### [105] [Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing](https://arxiv.org/abs/2507.01541)
*Álvaro Zaera,Diana Nicoleta Popa,Ivan Sekulic,Paolo Rosso*

Main category: cs.CL

TL;DR: 提出了一种整合不确定性建模和微调大型语言模型的新方法，用于任务导向对话系统中的范围外意图检测，取得了先进的性能成果。


<details>
  <summary>Details</summary>
Motivation: 提升任务导向对话系统在处理未知和模糊查询时的鲁棒性。

Method: 开发了模块化框架，包括对范围内意图检测分类器输出进行不确定性估计，并利用微调大型语言模型对高不确定性实例作最终决策。

Result: 在关键OOS检测基准和真实任务导向对话系统数据上取得了最先进效果，同时实现计算效率与性能的平衡。

Conclusion: 通过结合传统方法与大型语言模型，提出的框架可显著提升范围外意图检测的效率和准确性。

Abstract: Out-of-scope (OOS) intent detection is a critical challenge in task-oriented
dialogue systems (TODS), as it ensures robustness to unseen and ambiguous
queries. In this work, we propose a novel but simple modular framework that
combines uncertainty modeling with fine-tuned large language models (LLMs) for
efficient and accurate OOS detection. The first step applies uncertainty
estimation to the output of an in-scope intent detection classifier, which is
currently deployed in a real-world TODS handling tens of thousands of user
interactions daily. The second step then leverages an emerging LLM-based
approach, where a fine-tuned LLM is triggered to make a final decision on
instances with high uncertainty. Unlike prior approaches, our method
effectively balances computational efficiency and performance, combining
traditional approaches with LLMs and yielding state-of-the-art results on key
OOS detection benchmarks, including real-world OOS data acquired from a
deployed TODS.

</details>


### [106] [Is External Information Useful for Stance Detection with LLMs?](https://arxiv.org/abs/2507.01543)
*Quang Minh Nguyen,Taegyoon Kim*

Main category: cs.CL

TL;DR: 大模型 (LLMs) 在立场检测任务中使用外部信息可能会导致性能下降，而非提升。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明外部信息（如来自维基百科的摘录）可提升基于BERT的立场检测模型的性能，但是否对大语言模型 (LLMs) 同样有效仍未被详尽研究。

Method: 系统评估了使用维基百科和网络搜索作为外部信息对八个LLMs在三个数据集（涵盖12个目标）上的影响，并通过实验探讨信息偏向带来的影响。

Result: 发现外部信息在多数情况下会导致性能下降，宏平均F1分数最多下降27.9%。模型更倾向于与信息的立场及情感偏向对齐，而非准确识别文本的真实立场。

Conclusion: 外部信息可能引入偏向并导致性能下降，这与基于BERT的研究有所不同。在链式推理提示中，性能下降依旧显著；而微调能部分缓解，但不足以完全消除问题。这揭示了LLMs在使用外部信息时的潜在风险。

Abstract: In the stance detection task, a text is classified as either favorable,
opposing, or neutral towards a target. Prior work suggests that the use of
external information, e.g., excerpts from Wikipedia, improves stance detection
performance. However, whether or not such information can benefit large
language models (LLMs) remains an unanswered question, despite their wide
adoption in many reasoning tasks. In this study, we conduct a systematic
evaluation on how Wikipedia and web search external information can affect
stance detection across eight LLMs and in three datasets with 12 targets.
Surprisingly, we find that such information degrades performance in most cases,
with macro F1 scores dropping by up to 27.9\%. We explain this through
experiments showing LLMs' tendency to align their predictions with the stance
and sentiment of the provided information rather than the ground truth stance
of the given text. We also find that performance degradation persists with
chain-of-thought prompting, while fine-tuning mitigates but does not fully
eliminate it. Our findings, in contrast to previous literature on BERT-based
systems which suggests that external information enhances performance,
highlight the risks of information biases in LLM-based stance classifiers. Code
is available at https://github.com/ngqm/acl2025-stance-detection.

</details>


### [107] [Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation](https://arxiv.org/abs/2507.01594)
*Shutong Feng,Hsien-chin Lin,Nurul Lubis,Carel van Niekerk,Michael Heck,Benjamin Ruppik,Renato Vukovic,Milica Gašić*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型（LLM）的任务型对话系统LUSTER，该系统结合了短期（用户情感）和长期（任务成功）的奖励，通过端到端强化学习提升了对复杂对话环境的适应能力及情感响应能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决任务型对话系统在高噪声和模糊对话环境中优化任务成功率、情感理解以及信息传递等多重挑战。

Method: 提出LUSTER框架，通过结合大语言模型能力与结构化奖励建模，使用端到端强化学习来优化对话系统的表现，其中奖励函数包含用户情感和任务成功两个层面。

Result: 实验表明，LUSTER比传统模型在任务成功率及情感响应能力上有显著提升，并能更好应对噪声和不完美语言理解模块的挑战。

Conclusion: 整合LLMs与多重奖励建模的LUSTER方法展示了其开发下一代任务型对话智能代理的潜力，能够实现更具情感智能的对话交互。

Abstract: Task-oriented dialogue (ToD) systems are designed to help users achieve
specific goals through natural language interaction. While recent advances in
large language models (LLMs) have significantly improved linguistic fluency and
contextual understanding, building effective and emotionally intelligent ToD
systems remains a complex challenge. Effective ToD systems must optimise for
task success, emotional understanding and responsiveness, and precise
information conveyance, all within inherently noisy and ambiguous
conversational environments. In this work, we investigate architectural,
representational, optimisational as well as emotional considerations of ToD
systems. We set up systems covering these design considerations with a
challenging evaluation environment composed of a natural-language user
simulator coupled with an imperfect natural language understanding module. We
propose \textbf{LUSTER}, an \textbf{L}LM-based \textbf{U}nified \textbf{S}ystem
for \textbf{T}ask-oriented dialogue with \textbf{E}nd-to-end
\textbf{R}einforcement learning with both short-term (user sentiment) and
long-term (task success) rewards. Our findings demonstrate that combining LLM
capability with structured reward modelling leads to more resilient and
emotionally responsive ToD systems, offering a practical path forward for
next-generation conversational agents.

</details>


### [108] [Chart Question Answering from Real-World Analytical Narratives](https://arxiv.org/abs/2507.01627)
*Maeve Hutchinson,Radu Jianu,Aidan Slingsby,Jo Wood,Pranava Madhyastha*

Main category: cs.CL

TL;DR: 本文提出了一个基于可视化笔记本构建的图表问答新数据集，展示了真实世界的图表和自然语言问题。


<details>
  <summary>Details</summary>
Motivation: 目标是填补现有图表问答基准中生态有效性不足的空白，提供更贴近真实分析情景的数据。

Method: 从可视化笔记本中获取多视图图表并配对自然语言问题，形成新的生态有效的数据集，对现有多模态大语言模型进行性能评估。

Result: 目前最先进的多模态语言模型（如GPT-4.1）在数据集上表现出显著的性能差距，仅达到了69.3%的准确率。

Conclusion: 新数据集展示了在更真实的环境下进行图表问答的挑战性，为多模态模型研究提供了新的方向。

Abstract: We present a new dataset for chart question answering (CQA) constructed from
visualization notebooks. The dataset features real-world, multi-view charts
paired with natural language questions grounded in analytical narratives.
Unlike prior benchmarks, our data reflects ecologically valid reasoning
workflows. Benchmarking state-of-the-art multimodal large language models
reveals a significant performance gap, with GPT-4.1 achieving an accuracy of
69.3%, underscoring the challenges posed by this more authentic CQA setting.

</details>


### [109] [Confidence and Stability of Global and Pairwise Scores in NLP Evaluation](https://arxiv.org/abs/2507.01633)
*Georgii Levtsov,Dmitry Ustalov*

Main category: cs.CL

TL;DR: 本文探讨了在NLP模型评测中，传统全局分数评估（如GLUE）与成对比较评估（如LMSYS Arena）各自的优劣。


<details>
  <summary>Details</summary>
Motivation: 随着指令调整后的高性能语言模型的出现，模型评估需要更精确与多样化的方法，但现有评估方法存在不足，需要更深入的分析与比较。

Method: 该研究通过对合成与真实数据集进行计算实验，应用传统全局分数评估指标与Bradley-Terry模型的成对比较方法，评估其有效性与适用场景。

Result: 全局分数评估对整体排名更可靠，但可能低估包含罕见重大错误的强模型。而成对比较对评估全局评分较低的模型尤其有效，但在结果中存在频繁平局时会导致更高比较成本。

Conclusion: 在选择模型评估策略时，应同时考虑全局分数和成对比较的特点，根据应用场景选择合适的方法，以更全面地评估模型性能。

Abstract: With the advent of highly capable instruction-tuned neural language models,
benchmarking in natural language processing (NLP) is increasingly shifting
towards pairwise comparison leaderboards, such as LMSYS Arena, from traditional
global pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper
empirically investigates the strengths and weaknesses of both global scores and
pairwise comparisons to aid decision-making in selecting appropriate model
evaluation strategies. Through computational experiments on synthetic and
real-world datasets using standard global metrics and the popular Bradley-Terry
model for pairwise comparisons, we found that while global scores provide more
reliable overall rankings, they can underestimate strong models with rare,
significant errors or low confidence. Conversely, pairwise comparisons are
particularly effective for identifying strong contenders among models with
lower global scores, especially where quality metrics are hard to define (e.g.,
text generation), though they require more comparisons to converge if ties are
frequent. Our code and data are available at
https://github.com/HSPyroblast/srw-ranking under a permissive license.

</details>


### [110] [Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings](https://arxiv.org/abs/2507.01645)
*Rifki Afina Putri*

Main category: cs.CL

TL;DR: 本文研究了预训练语言模型在低资源印度尼西亚地方语言中的迁移能力，聚焦于情感分析任务。


<details>
  <summary>Details</summary>
Motivation: 评估不同模型在语言迁移任务中的表现，并探索如何提升对目标语言的适应能力。

Method: 使用不同类型的模型（如单语BERT、多语mBERT和XLM-R、MAD-X）进行零样本和基于适配器的迁移评估，将目标语言分为见过、部分见过和未见过三类。

Result: 多语模型在见过语言上性能最佳，部分见过语言中等，未见过语言表现较差；MAD-X方法在见过和部分见过语言上提升显著。

Conclusion: 模型对目标语言的直接或间接接触是迁移成功的关键，适配器的运用显著改善了性能。

Abstract: In this paper, we investigate the transferability of pre-trained language
models to low-resource Indonesian local languages through the task of sentiment
analysis. We evaluate both zero-shot performance and adapter-based transfer on
ten local languages using models of different types: a monolingual Indonesian
BERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based
approach called MAD-X. To better understand model behavior, we group the target
languages into three categories: seen (included during pre-training), partially
seen (not included but linguistically related to seen languages), and unseen
(absent and unrelated in pre-training data). Our results reveal clear
performance disparities across these groups: multilingual models perform best
on seen languages, moderately on partially seen ones, and poorly on unseen
languages. We find that MAD-X significantly improves performance, especially
for seen and partially seen languages, without requiring labeled data in the
target language. Additionally, we conduct a further analysis on tokenization
and show that while subword fragmentation and vocabulary overlap with
Indonesian correlate weakly with prediction quality, they do not fully explain
the observed performance. Instead, the most consistent predictor of transfer
success is the model's prior exposure to the language, either directly or
through a related language.

</details>


### [111] [AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness](https://arxiv.org/abs/2507.01702)
*Zixin Chen,Hongzhan Lin,Kaixin Li,Ziyang Luo,Zhen Ye,Guang Chen,Zhiyong Huang,Jing Ma*

Main category: cs.CL

TL;DR: 本文提出了一个名为AdamMeme的框架，用于动态评估多模态大语言模型在解读有害模因上的能力，以应对模因的动态变化。


<details>
  <summary>Details</summary>
Motivation: 现有的评估基准未能应对互联网模因的动态演变，限制了对大语言模型理解有害模因能力的全面评估。

Method: 提出了一种基于代理的评估框架AdamMeme，通过多代理协作，迭代更新有挑战性的模因数据，探测大语言模型在理解模因有害性方面的推理能力。

Result: 实验表明，该框架能够系统揭示不同多模态大语言模型的性能差异，并提供细粒度的模型弱点分析。

Conclusion: AdamMeme框架可以动态适应模因变化，提供对多模态大语言模型相关能力的全面和深入评估。

Abstract: The proliferation of multimodal memes in the social media era demands that
multimodal Large Language Models (mLLMs) effectively understand meme
harmfulness. Existing benchmarks for assessing mLLMs on harmful meme
understanding rely on accuracy-based, model-agnostic evaluations using static
datasets. These benchmarks are limited in their ability to provide up-to-date
and thorough assessments, as online memes evolve dynamically. To address this,
we propose AdamMeme, a flexible, agent-based evaluation framework that
adaptively probes the reasoning capabilities of mLLMs in deciphering meme
harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive
evaluations by iteratively updating the meme data with challenging samples,
thereby exposing specific limitations in how mLLMs interpret harmfulness.
Extensive experiments show that our framework systematically reveals the
varying performance of different target mLLMs, offering in-depth, fine-grained
analyses of model-specific weaknesses. Our code is available at
https://github.com/Lbotirx/AdamMeme.

</details>


### [112] [Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach](https://arxiv.org/abs/2507.01715)
*Aditya Tomar,Rudra Murthy,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 提出StereoBias数据集，并通过联合学习提高语言模型的偏见和刻板印象检测能力。


<details>
  <summary>Details</summary>
Motivation: 语言模型中的偏见和刻板印象可能导致在内容审核及决策中产生危害，因此需要方法来有效检测这些问题。

Method: 提出StereoBias数据集，涵盖宗教、性别、社会经济地位、种族、职业等五个类别，并比较了采用QLoRA微调的解码器模型和编码器模型之间的表现，通过联合训练偏见和刻板印象检测任务来提高检测性能。

Result: 编码器模型表现优异，解码器模型竞争力强；联合训练比单独训练显著提升偏见检测性能，实验还排除了仅由多任务学习引起的性能提升可能性。

Conclusion: 将刻板印象信息纳入考虑，可以构建更公平、性能更优的AI系统。

Abstract: Bias and stereotypes in language models can cause harm, especially in
sensitive areas like content moderation and decision-making. This paper
addresses bias and stereotype detection by exploring how jointly learning these
tasks enhances model performance. We introduce StereoBias, a unique dataset
labeled for bias and stereotype detection across five categories: religion,
gender, socio-economic status, race, profession, and others, enabling a deeper
study of their relationship. Our experiments compare encoder-only models and
fine-tuned decoder-only models using QLoRA. While encoder-only models perform
well, decoder-only models also show competitive results. Crucially, joint
training on bias and stereotype detection significantly improves bias detection
compared to training them separately. Additional experiments with sentiment
analysis confirm that the improvements stem from the connection between bias
and stereotypes, not multi-task learning alone. These findings highlight the
value of leveraging stereotype information to build fairer and more effective
AI systems.

</details>


### [113] [LLMs for Legal Subsumption in German Employment Contracts](https://arxiv.org/abs/2507.01734)
*Oliver Wardas,Florian Matthes*

Main category: cs.CL

TL;DR: 论文探讨了使用大语言模型（LLMs）和情境学习评估德国雇佣合同条款合法性的可能性，并比较了不同法律上下文的效果。


<details>
  <summary>Details</summary>
Motivation: 法律领域因其文本密集、资源消耗大的特点，对NLP领域提出了独特挑战；现有数据驱动方法在解释性和可信度上存在不足，限制了其动态法律环境中的应用。

Method: 扩展已有数据集，与法律专家合作，结合LLMs和情境学习，评估合同条款合法性，并比较不同法律上下文（无法律上下文、完整法律文本和提炼后的指导条款）的模型表现。

Result: 完整法律文本稍微提高了模型性能，而提炼版的指导条款显著提升了针对无效条款的召回率和加权F1得分（达到80%）；但即便如此，LLMs的表现仍低于人类律师。

Conclusion: 尽管在法律条款审核中LLMs显示出协助潜力，但目前方法以及LLMs在法律领域的适用性仍存在明显局限性。研究还贡献了扩展数据集和相关代码。

Abstract: Legal work, characterized by its text-heavy and resource-intensive nature,
presents unique challenges and opportunities for NLP research. While
data-driven approaches have advanced the field, their lack of interpretability
and trustworthiness limits their applicability in dynamic legal environments.
To address these issues, we collaborated with legal experts to extend an
existing dataset and explored the use of Large Language Models (LLMs) and
in-context learning to evaluate the legality of clauses in German employment
contracts. Our work evaluates the ability of different LLMs to classify clauses
as "valid," "unfair," or "void" under three legal context variants: no legal
context, full-text sources of laws and court rulings, and distilled versions of
these (referred to as examination guidelines). Results show that full-text
sources moderately improve performance, while examination guidelines
significantly enhance recall for void clauses and weighted F1-Score, reaching
80\%. Despite these advancements, LLMs' performance when using full-text
sources remains substantially below that of human lawyers. We contribute an
extended dataset, including examination guidelines, referenced legal sources,
and corresponding annotations, alongside our code and all log files. Our
findings highlight the potential of LLMs to assist lawyers in contract legality
review while also underscoring the limitations of the methods presented.

</details>


### [114] [Data interference: emojis, homoglyphs, and issues of data fidelity in corpora and their results](https://arxiv.org/abs/2507.01764)
*Matteo Di Cristofaro*

Main category: cs.CL

TL;DR: 本文讨论了文本标记化过程中可能出现的问题，特别是表情符号和同形异义字符在语料库中的处理对语言数据表示和分析结果的影响。


<details>
  <summary>Details</summary>
Motivation: 由于表情符号和同形异义字符在数字文本中日益普遍，这些元素如果未经处理可能导致语料库数据失真，从而影响分析的可靠性和可重复性。

Method: 研究通过分析表情符号及同形异义字符在数字文本中的表现，提出了适当的预处理方法以保证语料库的准确性。

Result: 研究发现，通过详细预处理特定文本元素，可显著提高语料库分析的准确性和可重复性。

Conclusion: 研究揭示了在处理数字语言数据时，结合语言学和技术细节的重要性，并对基于语料库的定量和定性研究产生了重大影响。

Abstract: Tokenisation - "the process of splitting text into atomic parts" (Brezina &
Timperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides
the basis for any applicable quantitative method (e.g. collocations) while
ensuring the reliability of qualitative approaches. This paper examines how
discrepancies in tokenisation affect the representation of language data and
the validity of analytical findings: investigating the challenges posed by
emojis and homoglyphs, the study highlights the necessity of preprocessing
these elements to maintain corpus fidelity to the source data. The research
presents methods for ensuring that digital texts are accurately represented in
corpora, thereby supporting reliable linguistic analysis and guaranteeing the
repeatability of linguistic interpretations. The findings emphasise the
necessity of a detailed understanding of both linguistic and technical aspects
involved in digital textual data to enhance the accuracy of corpus analysis,
and have significant implications for both quantitative and qualitative
approaches in corpus-based research.

</details>


### [115] [MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining](https://arxiv.org/abs/2507.01785)
*Zhixun Chen,Ping Guo,Wenhan Han,Yifan Zhang,Binbin Liu,Haobin Lin,Fengze Liu,Yan Zhao,Bingni Zhang,Taifeng Wang,Yin Zheng,Meng Fang*

Main category: cs.CL

TL;DR: 研究提出了一个名为MuRating的框架，通过翻译将高质量的英语数据质量信号转移到17种目标语言的评估上，并验证其对语言模型性能提升的效果。


<details>
  <summary>Details</summary>
Motivation: 为了提高大语言模型的多语言性能，需要一种能够针对多语言数据进行高质量筛选的高效方法，而现有方法主要集中在英语上。

Method: 该研究利用多个英语评分器经过成对比较生成统一的文档质量评分，然后通过翻译将这些判断扩展到多语言环境中；并用于选择英语和多语言内容子集，训练一个1.2亿参数的LLaMA模型。

Result: 与多种强基线方法相比，MuRating在英语基准和多语言评估中的平均精度均有所提升，尤其在知识密集型任务上表现突出。

Conclusion: MuRating在提升模型性能，特别是多语言和跨语言任务上表现显著，为未来在翻译保真度、选择偏差和叙事材料的研究指明了方向。

Abstract: Data quality is a critical driver of large language model performance, yet
existing model-based selection methods focus almost exclusively on English. We
introduce MuRating, a scalable framework that transfers high-quality English
data-quality signals into a single rater for 17 target languages. MuRating
aggregates multiple English "raters" via pairwise comparisons to learn unified
document-quality scores,then projects these judgments through translation to
train a multilingual evaluator on monolingual, cross-lingual, and parallel text
pairs. Applied to web data, MuRating selects balanced subsets of English and
multilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to
strong baselines, including QuRater, AskLLM, DCLM and so on, our approach
boosts average accuracy on both English benchmarks and multilingual
evaluations, with especially large gains on knowledge-intensive tasks. We
further analyze translation fidelity, selection biases, and underrepresentation
of narrative material, outlining directions for future work.

</details>


### [116] [Probing Evaluation Awareness of Language Models](https://arxiv.org/abs/2507.01786)
*Jord Nguyen,Khiem Hoang,Carlo Leonardo Attubato,Felix Hofstätter*

Main category: cs.CL

TL;DR: 本文分析了语言模型在测试和部署阶段能够区分输入（评估意识能力）的现象及其影响。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型区分测试和部署阶段输入的能力对AI评价方式和政策的潜在影响。

Method: 使用Llama-3.3-70B-Instruct模型进行评估，并通过线性探针区分测试和部署提示，分析模型内部的表示差异。

Result: 发现当前模型能够区分测试与部署输入，并认为某些安全评估提示对模型显得不自然或人工化。

Conclusion: 强调了建立可信评估标准的重要性，以及理解模型欺骗能力的必要性，并指出可以利用模型内部信息辅助安全审计。

Abstract: Language models can distinguish between testing and deployment phases -- a
capability known as evaluation awareness. This has significant safety and
policy implications, potentially undermining the reliability of evaluations
that are central to AI governance frameworks and voluntary industry
commitments. In this paper, we study evaluation awareness in
Llama-3.3-70B-Instruct. We show that linear probes can separate real-world
evaluation and deployment prompts, suggesting that current models internally
represent this distinction. We also find that current safety evaluations are
correctly classified by the probes, suggesting that they already appear
artificial or inauthentic to models. Our findings underscore the importance of
ensuring trustworthy evaluations and understanding deceptive capabilities. More
broadly, our work showcases how model internals may be leveraged to support
blackbox methods in safety audits, especially for future models more competent
at evaluation awareness and deception.

</details>


### [117] [How Do Vision-Language Models Process Conflicting Information Across Modalities?](https://arxiv.org/abs/2507.01790)
*Tianze Hua,Tian Yun,Ellie Pavlick*

Main category: cs.CL

TL;DR: 本文研究了视觉-语言多模态模型在接收到不一致信息输入时的表现和处理机制。


<details>
  <summary>Details</summary>
Motivation: 在复杂的多模态环境中，了解模型如何处理输入冲突是提升其准确性和适应性的重要一步。

Method: 对多模态模型输入冲突信号，分析其偏向某一模态的行为，并研究注意力头对表示结构的影响和可转移性。

Result: 发现模型会偏向特定模态，内部表示结构中有偏向模态的特征，同时存在可以优化结果的无模态偏向“路由头”。

Conclusion: 论文为识别和控制多模态模型在冲突输入环境中的信号解析提供了重要方法，有助于优化模型的多模态处理能力。

Abstract: AI models are increasingly required to be multimodal, integrating disparate
input streams into a coherent state representation on which subsequent
behaviors and actions can be based. This paper seeks to understand how such
models behave when input streams present conflicting information. Focusing
specifically on vision-language models, we provide inconsistent inputs (e.g.,
an image of a dog paired with the caption "A photo of a cat") and ask the model
to report the information present in one of the specific modalities (e.g.,
"What does the caption say / What is in the image?"). We find that models often
favor one modality over the other, e.g., reporting the image regardless of what
the caption says, but that different models differ in which modality they
favor. We find evidence that the behaviorally preferred modality is evident in
the internal representational structure of the model, and that specific
attention heads can restructure the representations to favor one modality over
the other. Moreover, we find modality-agnostic "router heads" which appear to
promote answers about the modality requested in the instruction, and which can
be manipulated or transferred in order to improve performance across datasets
and modalities. Together, the work provides essential steps towards identifying
and controlling if and how models detect and resolve conflicting signals within
complex multimodal environments.

</details>


### [118] [The Anatomy of Evidence: An Investigation Into Explainable ICD Coding](https://arxiv.org/abs/2507.01802)
*Katharina Beckh,Elisa Studeny,Sujan Sai Gannamaneni,Dario Antweiler,Stefan Rüping*

Main category: cs.CL

TL;DR: 本文研究了自动医疗编码系统的透明性和可解释性，特别是利用最新的MDACE数据集。


<details>
  <summary>Details</summary>
Motivation: 为了缓解文档和账单处理过程的负担，同时提高透明度，解决目前在短文本和二元分类上的局限性。

Method: 对MDACE数据集进行深入分析，并对现有自动医疗编码系统的可解释性进行合理性评估，提出匹配度量方法，并分析成功和失败案例。

Result: 研究发现，真实证据和代码描述有一定程度的匹配，现有最先进方法与真实证据有较高的重叠。

Conclusion: 基于研究结果，提出了开发和评估可解释医疗编码系统的建议。

Abstract: Automatic medical coding has the potential to ease documentation and billing
processes. For this task, transparency plays an important role for medical
coders and regulatory bodies, which can be achieved using explainability
methods. However, the evaluation of these approaches has been mostly limited to
short text and binary settings due to a scarcity of annotated data. Recent
efforts by Cheng et al. (2023) have introduced the MDACE dataset, which
provides a valuable resource containing code evidence in clinical records. In
this work, we conduct an in-depth analysis of the MDACE dataset and perform
plausibility evaluation of current explainable medical coding systems from an
applied perspective. With this, we contribute to a deeper understanding of
automatic medical coding and evidence extraction. Our findings reveal that
ground truth evidence aligns with code descriptions to a certain degree. An
investigation into state-of-the-art approaches shows a high overlap with ground
truth evidence. We propose match measures and highlight success and failure
cases. Based on our findings, we provide recommendations for developing and
evaluating explainable medical coding systems.

</details>


### [119] [Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes](https://arxiv.org/abs/2507.01810)
*Nikita Neveditsin,Pawan Lingras,Vijay Mago*

Main category: cs.CL

TL;DR: 讨论了小型语言模型生成的结构化输出在临床笔记开放属性值提取中的可解析性比较，发现JSON格式具有最高的可解析性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索小型语言模型生成结构化数据（JSON、YAML、XML）的可靠性和适用性，以指导其在隐私敏感的临床场景中的使用。

Method: 基于三种常用格式（JSON、YAML、XML）进行比较，分析模型输出中的结构化数据解析性，同时结合目标提示和不同模型参数进行实验。

Result: JSON格式在解析性上表现最佳，目标提示和较大的模型可提升结构稳健性，但较长文档和某些类型笔记会削弱解析性能；同时，分析了不同格式的常见失效模式。

Conclusion: 建议在隐私敏感的临床环境中，优先选择JSON格式并依据研究提供的提示设计来提高解析性能。

Abstract: We present a comparative analysis of the parseability of structured outputs
generated by small language models for open attribute-value extraction from
clinical notes. We evaluate three widely used serialization formats: JSON,
YAML, and XML, and find that JSON consistently yields the highest parseability.
Structural robustness improves with targeted prompting and larger models, but
declines for longer documents and certain note types. Our error analysis
identifies recurring format-specific failure patterns. These findings offer
practical guidance for selecting serialization formats and designing prompts
when deploying language models in privacy-sensitive clinical settings.

</details>


### [120] [Low-Perplexity LLM-Generated Sequences and Where To Find Them](https://arxiv.org/abs/2507.01844)
*Arthur Wuhrmann,Anastasiia Kucherenko,Andrei Kucharavy*

Main category: cs.CL

TL;DR: 研究引入了一种系统方法，通过分析低困惑度序列以探索大型语言模型如何利用和复制其训练数据。


<details>
  <summary>Details</summary>
Motivation: 旨在理解LLMs的训练数据如何塑造其输出，以促进透明性、责任性、隐私保护和公平性。

Method: 提出基于低困惑度序列分析的管道系统，该系统提取和追踪模型生成的高概率文本片段并将其与训练数据源关联。

Result: 发现相当一部分低困惑度序列无法追溯至训练数据，而能匹配的部分则揭示了逐字回忆的分布特性。

Conclusion: 这些研究结果为理解LLMs训练数据对模型行为的影响提供了重要的基础。

Abstract: As Large Language Models (LLMs) become increasingly widespread, understanding
how specific training data shapes their outputs is crucial for transparency,
accountability, privacy, and fairness. To explore how LLMs leverage and
replicate their training data, we introduce a systematic approach centered on
analyzing low-perplexity sequences - high-probability text spans generated by
the model. Our pipeline reliably extracts such long sequences across diverse
topics while avoiding degeneration, then traces them back to their sources in
the training data. Surprisingly, we find that a substantial portion of these
low-perplexity spans cannot be mapped to the corpus. For those that do match,
we quantify the distribution of occurrences across source documents,
highlighting the scope and nature of verbatim recall and paving a way toward
better understanding of how LLMs training data impacts their behavior.

</details>


### [121] [Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages](https://arxiv.org/abs/2507.01853)
*Samridhi Raj Sinha,Rajvee Sheth,Abhishek Upperwal,Mayank Singh*

Main category: cs.CL

TL;DR: 本文推出了EKA-EVAL，一个支持多语言的评价框架，特别强调印度语言。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型评价框架多集中于英语，无法满足印度等语言多样化地区的需求。

Method: 整合了超过35个基准数据集，其中包括10个印度特定数据集，并支持分布式推理、量化和多GPU使用。

Result: EKA-EVAL成为第一个针对全球和印度大语言模型的端到端、可扩展的评价框架。

Conclusion: EKA-EVAL作为一项开源项目，显著降低了多语言基准评价的难度，并计划扩展到超过100个基准，打造强大的多语言评价生态系统。

Abstract: The rapid advancement of Large Language Models (LLMs) has intensified the
need for evaluation frameworks that go beyond English centric benchmarks and
address the requirements of linguistically diverse regions such as India. We
present EKA-EVAL, a unified and production-ready evaluation framework that
integrates over 35 benchmarks, including 10 Indic-specific datasets, spanning
categories like reasoning, mathematics, tool use, long-context understanding,
and reading comprehension. Compared to existing Indian language evaluation
tools, EKA-EVAL offers broader benchmark coverage, with built-in support for
distributed inference, quantization, and multi-GPU usage. Our systematic
comparison positions EKA-EVAL as the first end-to-end, extensible evaluation
suite tailored for both global and Indic LLMs, significantly lowering the
barrier to multilingual benchmarking. The framework is open-source and publicly
available at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA
initiative (https://eka.soket.ai), which aims to scale up to over 100
benchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.

</details>


### [122] [DIY-MKG: An LLM-Based Polyglot Language Learning System](https://arxiv.org/abs/2507.01872)
*Kenan Tang,Yanhong Li,Yao Qin*

Main category: cs.CL

TL;DR: 该论文提出了一种DIY-MKG系统，旨在支持多语言学习者构建个性化多语言词汇知识图谱，并通过生成测验增强学习效果。


<details>
  <summary>Details</summary>
Motivation: 传统语言学习工具未能为多语言学习者提供跨语言词汇联系的支持，也无法满足学习者个性化需求，还存在认知负荷过高的问题。

Method: 作者设计了一种名为DIY-MKG的开源系统，允许用户通过LLM建议的相关词汇选择性地扩展个性化词汇知识图谱，结合注释功能和动态测验生成模块实现个性化学习；同时引入反馈机制提升用户参与度。

Result: 通过评估，DIY-MKG中基于LLM的词汇扩展在多语言中表现出良好的可靠性和公平性，生成的测验题目高度准确。

Conclusion: DIY-MKG在多语言学习场景中提供了一种可行且有效的工具，通过个性化和交互式设计提升了学习效果并验证了其方法的稳健性。

Abstract: Existing language learning tools, even those powered by Large Language Models
(LLMs), often lack support for polyglot learners to build linguistic
connections across vocabularies in multiple languages, provide limited
customization for individual learning paces or needs, and suffer from
detrimental cognitive offloading. To address these limitations, we design
Do-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system
that supports polyglot language learning. DIY-MKG allows the user to build
personalized vocabulary knowledge graphs, which are constructed by selective
expansion with related words suggested by an LLM. The system further enhances
learning through rich annotation capabilities and an adaptive review module
that leverages LLMs for dynamic, personalized quiz generation. In addition,
DIY-MKG allows users to flag incorrect quiz questions, simultaneously
increasing user engagement and providing a feedback loop for prompt refinement.
Our evaluation of LLM-based components in DIY-MKG shows that vocabulary
expansion is reliable and fair across multiple languages, and that the
generated quizzes are highly accurate, validating the robustness of DIY-MKG.

</details>


### [123] [MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants](https://arxiv.org/abs/2507.01887)
*Dongyi Ding,Tiannan Wang,Chenghao Zhu,Meiling Tao,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 本文提出了MiCoTA框架，通过引入中间大小模型和中等长度的CoT序列填补小型语言模型（SLM）的推理学习能力差距，显著提升其长推理任务性能。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型（SLM）在长形式链式推理（CoT）学习中的表现常因其有限能力而受限制，本文研究如何提高SLM的长推理任务能力。

Method: 提出MiCoTA框架，使用中间大小模型作为助教和中等长度CoT序列来实现长CoT推理的蒸馏策略，从而连接SLM与大型语言模型的推理能力差距。

Result: 实验结果表明，MiCoTA显著提高了SLM的推理性能，例如，Qwen2.5-7B-Instruct和Qwen2.5-3B-Instruct在多个基准数据集（如AIME2024、AMC、Olympiad、MATH-500和GSM8K）上的平均得分分别提高了3.47和3.93。

Conclusion: MiCoTA能生成更符合SLM分布的数据，为未来探索SLM长CoT数据蒸馏研究提供新的路径和洞察。

Abstract: Large language models (LLMs) excel at reasoning tasks requiring long thought
sequences for planning, reflection, and refinement. However, their substantial
model size and high computational demands are impractical for widespread
deployment. Yet, small language models (SLMs) often struggle to learn long-form
CoT reasoning due to their limited capacity, a phenomenon we refer to as the
"SLMs Learnability Gap". To address this, we introduce
\textbf{Mi}d-\textbf{Co}T \textbf{T}eacher \textbf{A}ssistant Distillation
(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA
employs intermediate-sized models as teacher assistants and utilizes
intermediate-length CoT sequences to bridge both the capacity and reasoning
length gaps. Our experiments on downstream tasks demonstrate that although SLMs
distilled from large teachers can perform poorly, by applying MiCoTA, they
achieve significant improvements in reasoning performance. Specifically,
Qwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and
3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and
GSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform
a quantitative experiment demonstrating that our method produces data more
closely aligned with base SLM distributions. Our insights pave the way for
future research into long-CoT data distillation for SLMs.

</details>


### [124] [High-Layer Attention Pruning with Rescaling](https://arxiv.org/abs/2507.01900)
*Songtao Liu,Peng Liu*

Main category: cs.CL

TL;DR: 提出了一种新型的注意力头剪枝算法，结合适应性的重新缩放参数，并在多个大型语言模型和数据集上验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 目前的剪枝方法缺乏对注意力头位置的考量，可能导致模型性能下降。

Method: 在模型高层有策略地剪枝注意力头，并引入适应性的重新缩放参数以校准剪枝后的表示尺度。

Result: 在包括 LLaMA3.1-8B、Mistral-7B-v0.3 等多个模型和数据集的测试中，表现优于现有的结构化剪枝方法，特别是在生成任务方面。

Conclusion: 新方法能更高效地进行剪枝，同时在性能上具有显著提升，特别是针对生成任务。

Abstract: Pruning is a highly effective approach for compressing large language models
(LLMs), significantly reducing inference latency. However, conventional
training-free structured pruning methods often employ a heuristic metric that
indiscriminately removes some attention heads across all pruning layers,
without considering their positions within the network architecture. In this
work, we propose a novel pruning algorithm that strategically prunes attention
heads in the model's higher layers. Since the removal of attention heads can
alter the magnitude of token representations, we introduce an adaptive
rescaling parameter that calibrates the representation scale post-pruning to
counteract this effect. We conduct comprehensive experiments on a wide range of
LLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our
evaluation includes both generation and discriminative tasks across 27
datasets. The results consistently demonstrate that our method outperforms
existing structured pruning methods. This improvement is particularly notable
in generation tasks, where our approach significantly outperforms existing
baselines.

</details>


### [125] [AI4Research: A Survey of Artificial Intelligence for Scientific Research](https://arxiv.org/abs/2507.01903)
*Qiguang Chen,Mingda Yang,Libo Qin,Jinhao Liu,Zheng Yan,Jiannan Guan,Dengyun Peng,Yiyan Ji,Hanjing Li,Mengkang Hu,Yimeng Zhang,Yihao Liang,Yuhang Zhou,Jiaqi Wang,Zhi Chen,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文综述了当前人工智能在科学研究领域的应用，提出了一种系统的分类方法，总结研究空白并展望未来方向，同时提供大量资源以促进研究。


<details>
  <summary>Details</summary>
Motivation: 近年来大语言模型在复杂领域表现优异，许多研究将AI应用于科学研究以推动创新。然而，目前缺乏关于AI4Research的全面综述，这限制了我们的理解和研发进展。

Method: 提出了AI4Research的系统分类法，识别核心研究空白并展望新方向，重点关注自动化实验的严谨性和可扩展性及其社会影响。此外，还汇总了多学科应用、数据集和工具资源。

Result: 构建了AI4Research的分类框架，辨识了研究领域中的关键空白，并提供了大量便于社区研究实践的资源。

Conclusion: 研究为AI4Research领域提供了统一视角，解决了缺乏综述的困境，旨在推动技术未来创新突破。

Abstract: Recent advancements in artificial intelligence (AI), particularly in large
language models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated
remarkable capabilities in complex domains such as logical reasoning and
experimental coding. Motivated by these advancements, numerous studies have
explored the application of AI in the innovation process, particularly in the
context of scientific research. These AI technologies primarily aim to develop
systems that can autonomously conduct research processes across a wide range of
scientific disciplines. Despite these significant strides, a comprehensive
survey on AI for Research (AI4Research) remains absent, which hampers our
understanding and impedes further development in this field. To address this
gap, we present a comprehensive survey and offer a unified perspective on
AI4Research. Specifically, the main contributions of our work are as follows:
(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify
five mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key
research gaps and highlight promising future directions, focusing on the rigor
and scalability of automated experiments, as well as the societal impact. (3)
Abundant applications and resources: Finally, we compile a wealth of resources,
including relevant multidisciplinary applications, data corpora, and tools. We
hope our work will provide the research community with quick access to these
resources and stimulate innovative breakthroughs in AI4Research.

</details>


### [126] [Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models](https://arxiv.org/abs/2507.01915)
*Chengao Li,Hanyu Zhang,Yunkun Xu,Hongyan Xue,Xiang Ao,Qing He*

Main category: cs.CL

TL;DR: 提出了Gradient-Adaptive Policy Optimization (GAPO)用于解决多目标优化问题，能够更好地对齐模型与多样化的人类偏好。


<details>
  <summary>Details</summary>
Motivation: 现有方法在对齐大语言模型与人类偏好时存在挑战，特别是面对可能存在冲突的多样化偏好。

Method: 将人类价值对齐视为多目标优化问题，提出GAPO，通过多梯度下降方式调整每个目标的梯度。此外，P-GAPO结合用户在不同目标上的偏好以实现更符合需求的Pareto解。

Result: 理论分析表明，GAPO可以收敛于多个目标的Pareto最优解，且在Mistral-7B实验中表现优于当下技术，在有用性和无害性方面取得更好的平衡。

Conclusion: GAPO和P-GAPO方法为对齐人类偏好提供了新的有效路径，开创了处理多目标优化的前沿。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful
technique for aligning large language models (LLMs) with human preferences.
However, effectively aligning LLMs with diverse human preferences remains a
significant challenge, particularly when they are conflict. To address this
issue, we frame human value alignment as a multi-objective optimization
problem, aiming to maximize a set of potentially conflicting objectives. We
introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning
paradigm that employs multiple-gradient descent to align LLMs with diverse
preference distributions. GAPO adaptively rescales the gradients for each
objective to determine an update direction that optimally balances the
trade-offs between objectives. Additionally, we introduce P-GAPO, which
incorporates user preferences across different objectives and achieves Pareto
solutions that better align with the user's specific needs. Our theoretical
analysis demonstrates that GAPO converges towards a Pareto optimal solution for
multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms
current state-of-the-art methods, achieving superior performance in both
helpfulness and harmlessness.

</details>


### [127] [NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks](https://arxiv.org/abs/2507.01921)
*Yang Li,Youssef Emad,Karthik Padthe,Jack Lanchantin,Weizhe Yuan,Thao Nguyen,Jason Weston,Shang-Wen Li,Dong Wang,Ilia Kulikov,Xian Li*

Main category: cs.CL

TL;DR: 研究表明，从大型教师模型蒸馏推理能力到较小的学生模型时，通过高质量的推理示例选择可以显著提升学生模型的推理能力，并优于随机采样或使用现有数据集。


<details>
  <summary>Details</summary>
Motivation: 探索哪种类型的推理示例对提升学生模型的推理能力最有效，重点分析样本效率以及在一般推理任务中的可扩展性。

Method: 对教师模型在大规模问题集（NaturalReasoning）中的推理示例进行筛选，形成高质量的推理轨迹“NaturalThoughts”；采用系统分析方法研究蒸馏推理能力所受的因素影响，并与随机采样等基线方法进行比较。

Result: 通过高质量的数据选择（尤其是需要更多推理策略的难例），在多种一般推理基准（如GPQA-Diamond、MMLU-Pro等）上，学生模型的性能显著超过现有数据集训练的模型。

Conclusion: 选择高效且高质量的推理示例比随机扩展规模更能提高推理能力，建议未来将重点放在更具挑战性的多样化推理数据集上。

Abstract: Recent work has shown that distilling reasoning traces from a larger teacher
model via supervised finetuning outperforms reinforcement learning with the
smaller student model alone (Guo et al. 2025). However, there has not been a
systematic study of what kind of reasoning demonstrations from the teacher are
most effective in improving the student model's reasoning capabilities. In this
work we curate high-quality "NaturalThoughts" by selecting reasoning traces
from a strong teacher model based on a large pool of questions from
NaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of
factors that affect distilling reasoning capabilities, in terms of sample
efficiency and scalability for general reasoning tasks. We observe that simply
scaling up data size with random sampling is a strong baseline with steady
performance gains. Further, we find that selecting difficult examples that
require more diverse reasoning strategies is more sample-efficient to transfer
the teacher model's reasoning skills. Evaluated on both Llama and Qwen models,
training with NaturalThoughts outperforms existing reasoning datasets such as
OpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including
GPQA-Diamond, MMLU-Pro and SuperGPQA.

</details>


### [128] [Decision-oriented Text Evaluation](https://arxiv.org/abs/2507.01923)
*Yu-Shiang Huang,Chuan-Ju Wang,Chung-Chi Chen*

Main category: cs.CL

TL;DR: 探讨NLG方法在高风险领域的评估，提出基于决策导向的评估框架，分析生成文本对人类和LLM决策的影响。


<details>
  <summary>Details</summary>
Motivation: 传统内在评估指标（如n-gram重叠）与实际决策效能之间相关性较低，无法有效评估生成文本在决策中的实际作用。

Method: 提出基于决策的评估框架，使用市场摘要文本（包括客观晨间总结和主观收盘分析）为测试案例，通过人类投资者和LLM代理的金融交易表现评估决策质量。

Result: 研究发现，仅依赖摘要时，人类和LLM代理的表现无法可靠超过随机水平；但通过分析性评论协作的人类-LLM团队显著优于单独的人类或代理。

Conclusion: 强调应从促进人类与LLM协作决策的角度重新审视生成文本评估，指出传统内在指标的局限性。

Abstract: Natural language generation (NLG) is increasingly deployed in high-stakes
domains, yet common intrinsic evaluation methods, such as n-gram overlap or
sentence plausibility, weakly correlate with actual decision-making efficacy.
We propose a decision-oriented framework for evaluating generated text by
directly measuring its influence on human and large language model (LLM)
decision outcomes. Using market digest texts--including objective morning
summaries and subjective closing-bell analyses--as test cases, we assess
decision quality based on the financial performance of trades executed by human
investors and autonomous LLM agents informed exclusively by these texts. Our
findings reveal that neither humans nor LLM agents consistently surpass random
performance when relying solely on summaries. However, richer analytical
commentaries enable collaborative human-LLM teams to outperform individual
human or agent baselines significantly. Our approach underscores the importance
of evaluating generated text by its ability to facilitate synergistic
decision-making between humans and LLMs, highlighting critical limitations of
traditional intrinsic metrics.

</details>


### [129] [Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla](https://arxiv.org/abs/2507.01931)
*Md Sazzadul Islam Ridoy,Sumi Akter,Md. Aminur Rahman*

Main category: cs.CL

TL;DR: 研究比较了两种最先进ASR模型（Whisper和Wav2Vec-BERT）在低资源语言班加拉语的表现，发现Wav2Vec-BERT整体优于Whisper。


<details>
  <summary>Details</summary>
Motivation: 探讨在低资源语言背景下的自动语音识别模型的性能优化及表现，以支持班加拉语的语音识别需求。

Method: 使用Mozilla Common Voice-17和OpenSLR公开数据集，通过微调学习率、训练回合、模型检查点选择等超参数优化，对比模型在错误率、训练时间和计算效率方面的表现。

Result: Wav2Vec-BERT在所有评价指标（单词错误率、字符错误率）上优于Whisper，同时计算资源需求更低。

Conclusion: Wav2Vec-BERT展示了在低资源语言设置下开发强大语音识别系统的潜力。

Abstract: In recent years, neural models trained on large multilingual text and speech
datasets have shown great potential for supporting low-resource languages. This
study investigates the performances of two state-of-the-art Automatic Speech
Recognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's
Wav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments
using two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to
evaluate model performances. Through systematic fine-tuning and hyperparameter
optimization, including learning rate, epochs, and model checkpoint selection,
we have compared the models based on Word Error Rate (WER), Character Error
Rate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model
outperformed Whisper across all key evaluation metrics, demonstrated superior
performance while requiring fewer computational resources, and offered valuable
insights to develop robust speech recognition systems in low-resource
linguistic settings.

</details>


### [130] [The Thin Line Between Comprehension and Persuasion in LLMs](https://arxiv.org/abs/2507.01936)
*Adrian de Wynter,Tangming Yuan*

Main category: cs.CL

TL;DR: 本文分析了大语言模型（LLMs）在辩论中的表现，发现它们能够维持连贯且有说服力的对话，但在对话背后的深层结构和语境理解上仍然存在不足。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在高水平和复杂人类交流中的理解能力，特别是评估它们在对话结构和语用语境理解上的表现。

Method: 通过研究LLMs参与辩论的能力，以及它们在理解对话结构和语用语境方面的表现来评估它们的理解能力。

Result: LLMs可以进行连贯且说服力强的辩论，能够影响参与者和观众的观点，但在深层次对话理解上无法展现同样的能力。同时，人们对AI参与的怀疑会提高对其论点的批判性。

Conclusion: 尽管LLMs在形式上可以维护有效对话，但并不需要真正理解对话内容。对话有效性比上下文和连贯性的建模更为重要。

Abstract: Large language models (LLMs) are excellent at maintaining high-level,
convincing dialogues. They are being fast deployed as chatbots and evaluators
in sensitive areas, such as peer review and mental health applications. This,
along with the disparate accounts on their reasoning capabilities, calls for a
closer examination of LLMs and their comprehension of dialogue. In this work we
begin by evaluating LLMs' ability to maintain a debate--one of the purest yet
most complex forms of human communication. Then we measure how this capability
relates to their understanding of what is being talked about, namely, their
comprehension of dialogical structures and the pragmatic context. We find that
LLMs are capable of maintaining coherent, persuasive debates, often swaying the
beliefs of participants and audiences alike. We also note that awareness or
suspicion of AI involvement encourage people to be more critical of the
arguments made. When polling LLMs on their comprehension of deeper structures
of dialogue, however, they cannot demonstrate said understanding. Our findings
tie the shortcomings of LLMs-as-evaluators to their (in)ability to understand
the context. More broadly, for the field of argumentation theory we posit that,
if an agent can convincingly maintain a dialogue, it is not necessary for it to
know what it is talking about. Hence, the modelling of pragmatic context and
coherence are secondary to effectiveness.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [131] [Rethinking the Illusion of Thinking](https://arxiv.org/abs/2507.01231)
*Iñaki Dellibarda Varela,Pablo Romero-Sorozabal,Eduardo Rocon,Manuel Cebrian*

Main category: cs.AI

TL;DR: 本研究通过改进实验方法，重新评估了两大争议性的AI推理能力基准，发现了LRMs在推理复杂性与问题可解性方面的限制及潜力。


<details>
  <summary>Details</summary>
Motivation: 澄清AI推理能力的争议，通过更精细的实验方法探讨LRMs的实际能力边界。

Method: 通过引入分步提示和协作对话两种方法，重新评估了原有实验的两项基准（汉诺塔和过河问题），并特别设计了具有可解决性的测试配置。

Result: 发现LRMs在中等复杂度（如8个盘）的推理任务中仍有局限性，但在通过限制测试为可解问题后能够在大规模复杂任务（超过100对元素）中取得成功。

Conclusion: LRMs被确认具有一定推理能力，但其表现取决于问题配置和复杂性，未来需通过细化实验揭示更多推理机制。

Abstract: Earlier this year, Apple ignited controversy by publishing "The Illusion of
Thinking," prompting heated debate within the AI community. Critics seized upon
the findings as conclusive evidence that Large Reasoning Models (LRMs) lack
genuine reasoning capabilities, branding them as mere stochastic parrots.
Meanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning
the experimental setup as flawed and the conclusions overstated. We clarify
this debate by replicating and refining two of the original study's most
contentious benchmarks: Towers of Hanoi and River Crossing. By introducing
incremental stepwise prompting and agentic collaborative dialogue, we show that
previously reported failures solving the Towers of Hanoi were not purely result
of output constraints, but also partly a result of cognition limitations: LRMs
still stumble when complexity rises moderately (around 8 disks). Moreover, the
River Crossing results initially heralded as catastrophic failures turn out to
hinge upon testing unsolvable configurations. Once we limit tests strictly to
solvable problems-LRMs effortlessly solve large instances involving over 100
agent pairs. Our findings ultimately defy simplistic narratives: today's LRMs
are stochastic, RL-tuned searchers in a discrete state space we barely
understand. Real progress in symbolic, long-horizon reasoning demands mapping
that terrain through fine-grained ablations like those introduced here.

</details>


### [132] [Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care](https://arxiv.org/abs/2507.01282)
*Matthew JY Kang,Wenli Yang,Monica R Roberts,Byeong Ho Kang,Charles B Malpas*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）尽管在基准测试中表现优异，但在医疗场景特别是痴呆诊断和护理中仍未显著改善实际效果。主要限制造成模型输出缺乏透明性、易发生幻觉、不具备因果推理能力。


<details>
  <summary>Details</summary>
Motivation: 为探讨AI在实际临床应用中的限制，如痴呆诊断领域，并找出如何通过改进模型弥补这些弱点。

Method: 重点分析LLM的瓶颈及其数据驱动范式造成的局限性，提议混合方法（统计学习与专家规则知识结合），并在现有临床工作流程中测试。如PEIRS及ATHENA-CDS示例，强调解释性与因果匹配。

Result: 混合模型与专家知识结合能够提升解释性及临床适配性，尽管当前大部分AI仍然以数据驱动为核心，尚未广泛采用“人类参与流程”。

Conclusion: 未来的AI研究应关注如何提升解释性、与临床工作流的契合以及患者结果的改善；并通过发展可解释神经符号AI等方法，弥补当前模型的弱点。

Abstract: The recent boom of large language models (LLMs) has re-ignited the hope that
artificial intelligence (AI) systems could aid medical diagnosis. Yet despite
dazzling benchmark scores, LLM assistants have yet to deliver measurable
improvements at the bedside. This scoping review aims to highlight the areas
where AI is limited to make practical contributions in the clinical setting,
specifically in dementia diagnosis and care.
  Standalone machine-learning models excel at pattern recognition but seldom
provide actionable, interpretable guidance, eroding clinician trust. Adjacent
use of LLMs by physicians did not result in better diagnostic accuracy or
speed. Key limitations trace to the data-driven paradigm: black-box outputs
which lack transparency, vulnerability to hallucinations, and weak causal
reasoning. Hybrid approaches that combine statistical learning with expert
rule-based knowledge, and involve clinicians throughout the process help bring
back interpretability. They also fit better with existing clinical workflows,
as seen in examples like PEIRS and ATHENA-CDS.
  Future decision-support should prioritise explanatory coherence by linking
predictions to clinically meaningful causes. This can be done through
neuro-symbolic or hybrid AI that combines the language ability of LLMs with
human causal expertise. AI researchers have addressed this direction, with
explainable AI and neuro-symbolic AI being the next logical steps in further
advancement in AI. However, they are still based on data-driven knowledge
integration instead of human-in-the-loop approaches. Future research should
measure success not only by accuracy but by improvements in clinician
understanding, workflow fit, and patient outcomes. A better understanding of
what helps improve human-computer interactions is greatly needed for AI systems
to become part of clinical practice.

</details>


### [133] [AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing](https://arxiv.org/abs/2507.01376)
*Yinwang Ren,Yangyang Liu,Tang Ji,Xun Xu*

Main category: cs.AI

TL;DR: 该研究探讨了生成式AI（GenAI）下的AI代理，尤其是基于大语言模型（LLMs）和多模态大语言模型（MLLMs）的代理技术及其在智能制造领域的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 在生成式AI快速发展的背景下，虽然LLM-Agents、MLLM-Agents和Agentic AI为AI在动态环境下的信息处理、环境感知和自治决策带来了新的可能性，但其在智能制造中的具体定义、能力范围和应用尚不明确。

Method: 作者系统性回顾AI及其代理技术的演进，分析LLM-Agents、MLLM-Agents及Agentic AI的核心概念和技术进展，同时探讨这些技术在智能制造领域的应用潜力及其可能面临的挑战。

Result: 梳理了与AI代理相关的技术演进，阐明了新兴AI代理技术的定义与能力，展示其在智能制造领域的应用前景并识别潜在挑战。

Conclusion: LLM-Agents、MLLM-Agents和Agentic AI为AI技术在智能制造中的革新提供了丰富可能性，尽管尚需解决技术定义、应用评估和环境挑战等问题。

Abstract: AI agents are autonomous systems designed to perceive, reason, and act within
dynamic environments. With the rapid advancements in generative AI (GenAI),
large language models (LLMs) and multimodal large language models (MLLMs) have
significantly improved AI agents' capabilities in semantic comprehension,
complex reasoning, and autonomous decision-making. At the same time, the rise
of Agentic AI highlights adaptability and goal-directed autonomy in dynamic and
complex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents
(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in
information processing, environmental perception, and autonomous
decision-making, opening new avenues for smart manufacturing. However, the
definitions, capability boundaries, and practical applications of these
emerging AI paradigms in smart manufacturing remain unclear. To address this
gap, this study systematically reviews the evolution of AI and AI agent
technologies, examines the core concepts and technological advancements of
LLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential
applications in and integration into manufacturing, along with the potential
challenges they may face.

</details>


### [134] [A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models](https://arxiv.org/abs/2507.01410)
*Abeer Dyoub,Francesca A. Lisi*

Main category: cs.AI

TL;DR: 本文提出了一种基于伦理风险评估的正式方法，用于描述伦理决策模型，利用模糊Petri网进行验证和验证，并通过医疗领域的案例研究加以说明。


<details>
  <summary>Details</summary>
Motivation: 由于道德领域的本体和认知识别复杂性，难以为评估道德机器的性能建立明确的标准。

Method: 提出了一种基于伦理风险评估的正式方法，采用模糊规则描述伦理决策模型，并通过模糊Petri网进行验证和验证。

Result: 展示了如何应用该方法对模型进行验证与评估，并通过医学领域的案例研究进行说明。

Conclusion: 该方法能够提供一种正式化的伦理决策模型描述及验证工具，对道德机器的研究具有帮助。

Abstract: The ontological and epistemic complexities inherent in the moral domain make
it challenging to establish clear standards for evaluating the performance of a
moral machine. In this paper, we present a formal method to describe Ethical
Decision Making models based on ethical risk assessment. Then, we show how
these models that are specified as fuzzy rules can be verified and validated
using fuzzy Petri nets. A case study from the medical field is considered to
illustrate the proposed approach.

</details>


### [135] [Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading](https://arxiv.org/abs/2507.01431)
*Yoonseok Yang,Minjune Kim,Marlon Rondinelli,Keren Shao*

Main category: cs.AI

TL;DR: Pensieve是一种AI辅助评分平台，利用大型语言模型处理手写STEM课程答题的评分任务，已在超过20所机构中应用，平均减少65%的评分时间并达到95.4%的评分一致率。


<details>
  <summary>Details</summary>
Motivation: 解决在大型大学STEM课程中手写开放式答案评分的瓶颈问题。

Method: 利用AI驱动的大型语言模型，提供了一种能够处理从扫描答卷到反馈的评分全流程平台，同时结合人类监督以确保准确性。

Result: Pensieve在数学、计算机科学、物理和化学领域真实环境中应用，评分时间减少65%，达到95.4%的讲师评分一致率。

Conclusion: Pensieve通过AI和人类协作的方式，使评分更加高效，同时保持较高的评分一致性，为STEM课程教学带来显著便利。

Abstract: Grading handwritten, open-ended responses remains a major bottleneck in large
university STEM courses. We introduce Pensieve (https://www.pensieve.co), an
AI-assisted grading platform that leverages large language models (LLMs) to
transcribe and evaluate student work, providing instructors with rubric-aligned
scores, transcriptions, and confidence ratings. Unlike prior tools that focus
narrowly on specific tasks like transcription or rubric generation, Pensieve
supports the entire grading pipeline-from scanned student submissions to final
feedback-within a human-in-the-loop interface.
  Pensieve has been deployed in real-world courses at over 20 institutions and
has graded more than 300,000 student responses. We present system details and
empirical results across four core STEM disciplines: Computer Science,
Mathematics, Physics, and Chemistry. Our findings show that Pensieve reduces
grading time by an average of 65%, while maintaining a 95.4% agreement rate
with instructor-assigned grades for high-confidence predictions.

</details>


### [136] [Using multi-agent architecture to mitigate the risk of LLM hallucinations](https://arxiv.org/abs/2507.01446)
*Abd Elrahman Amer,Magdi Amer*

Main category: cs.AI

TL;DR: 本研究提出一种结合LLM和模糊逻辑的多代理系统，旨在通过SMS处理客户请求并降低幻觉风险。


<details>
  <summary>Details</summary>
Motivation: 提升客户服务质量和响应时间，以维持客户忠诚度和增加市场份额。

Method: 设计了一种结合大型语言模型（LLM）和模糊逻辑的多代理系统，用于处理通过SMS发送的客户请求。

Result: 通过结合LLM和模糊逻辑，降低了系统的幻觉风险。

Conclusion: 采用该多代理系统能够有效提升客户服务质量，同时降低使用LLM中的幻觉风险。

Abstract: Improving customer service quality and response time are critical factors for
maintaining customer loyalty and increasing a company's market share. While
adopting emerging technologies such as Large Language Models (LLMs) is becoming
a necessity to achieve these goals, the risk of hallucination remains a major
challenge. In this paper, we present a multi-agent system to handle customer
requests sent via SMS. This system integrates LLM based agents with fuzzy logic
to mitigate hallucination risks.

</details>


### [137] [Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning](https://arxiv.org/abs/2507.01489)
*Yanfei Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种名为Agent-as-tool的层次化框架，其分离了工具调用过程和推理过程，提升语言模型的推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究面临工具调用和推理过程同时决策的挑战，且分析过程中依赖于含冗余信息的原始工具输出，增加了模型推理的复杂性。

Method: 引入Agent-as-tool层次化框架，将工具调用过程与推理过程分离，以减少推理负担。其中工具调用由一个代理处理，使模型聚焦于语言推理。

Result: 通过对180个样本进行轻微的强化微调，该框架在Bamboogle任务中表现出色，达到63.2%的完全匹配率和75.2%的覆盖完全匹配率，比Search-R1分别提高了4.8%和3.2%。

Conclusion: 该框架显著优化了推理过程，与现有方法相比具有更高的性能和效率，为基于强化学习的框架研究提供了新方向。

Abstract: Large Language Models (LLMs) have emerged as one of the most significant
technological advancements in artificial intelligence in recent years. Their
ability to understand, generate, and reason with natural language has
transformed how we interact with AI systems. With the development of LLM-based
agents and reinforcement-learning-based reasoning models, the study of applying
reinforcement learning in agent frameworks has become a new research focus.
However, all previous studies face the challenge of deciding the tool calling
process and the reasoning process simultaneously, and the chain of reasoning
was solely relied on the unprocessed raw result with redundant information and
symbols unrelated to the task from the tool, which impose a heavy burden on the
model's capability to reason. Therefore, in our research, we proposed a
hierarchical framework Agent-as-tool that detach the tool calling process and
the reasoning process, which enables the model to focus on the verbally
reasoning process while the tool calling process is handled by another agent.
Our work had achieved comparable results with only a slight reinforcement
fine-tuning on 180 samples, and had achieved exceptionally well performance in
Bamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding
Search-R1 by 4.8% in exact match and 3.2% in cover exact match.

</details>


### [138] [T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2507.01597)
*Yuehang Si,Zefan Zeng,Jincai Huang,Qing Cheng*

Main category: cs.AI

TL;DR: 提出了一种名为T3DM的新方法，用于解决时间知识图谱推理中的分布偏移问题，并通过对抗训练设计了一种更高质量的负采样策略。


<details>
  <summary>Details</summary>
Motivation: 现有研究在处理时间知识图谱推理时，面临分布偏移问题和低质量的负采样问题，影响模型的推理能力和结果可靠性。

Method: 引入测试时训练指导的分布偏移建模（T3DM）方法，动态调整模型以适应分布偏移；并通过对抗训练设计了高质量的负采样策略。

Result: 实验表明，T3DM在大多数情况下优于现有的最先进基线模型，提供了更好的性能和鲁棒性。

Conclusion: T3DM能够有效应对时间知识图谱推理中的分布偏移问题，并通过高质量负采样提高推理质量，是一种性能优异的方法。

Abstract: Temporal Knowledge Graph (TKG) is an efficient method for describing the
dynamic development of facts along a timeline. Most research on TKG reasoning
(TKGR) focuses on modelling the repetition of global facts and designing
patterns of local historical facts. However, they face two significant
challenges: inadequate modeling of the event distribution shift between
training and test samples, and reliance on random entity substitution for
generating negative samples, which often results in low-quality sampling. To
this end, we propose a novel distributional feature modeling approach for
training TKGR models, Test-Time Training-guided Distribution shift Modelling
(T3DM), to adjust the model based on distribution shift and ensure the global
consistency of model reasoning. In addition, we design a negative-sampling
strategy to generate higher-quality negative quadruples based on adversarial
training. Extensive experiments show that T3DM provides better and more robust
results than the state-of-the-art baselines in most cases.

</details>


### [139] [Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI](https://arxiv.org/abs/2507.01717)
*Gopichand Kanumolu,Ashok Urlana,Charaka Vinayak Kumar,Bala Mallikarjunarao Garlapati*

Main category: cs.AI

TL;DR: 该论文提出通过大型语言模型（LLMs）和自主智能体从专利中挖掘和生成产品概念。通过名为Agent Ideate的框架，自动生成产品业务创意。


<details>
  <summary>Details</summary>
Motivation: 专利具有丰富的技术知识，可以为创新产品提供灵感，但获取和解释这些信息存在挑战。

Method: 设计了Agent Ideate框架，将开源LLMs与基于智能体的架构结合，用于三个领域（计算机科学、自然语言处理、材料化学）生成产品想法。

Result: 实验结果表明，基于智能体的方法在创意质量、相关性和新颖性上均优于单独的LLMs。

Conclusion: 结合LLMs和智能体的工作流可以显著提升创新流程，释放专利数据中未被开发的业务创意潜力。

Abstract: Patents contain rich technical knowledge that can inspire innovative product
ideas, yet accessing and interpreting this information remains a challenge.
This work explores the use of Large Language Models (LLMs) and autonomous
agents to mine and generate product concepts from a given patent. In this work,
we design Agent Ideate, a framework for automatically generating product-based
business ideas from patents. We experimented with open-source LLMs and
agent-based architectures across three domains: Computer Science, Natural
Language Processing, and Material Chemistry. Evaluation results show that the
agentic approach consistently outperformed standalone LLMs in terms of idea
quality, relevance, and novelty. These findings suggest that combining LLMs
with agentic workflows can significantly enhance the innovation pipeline by
unlocking the untapped potential of business idea generation from patent data.

</details>


### [140] [Joint Matching and Pricing for Crowd-shipping with In-store Customers](https://arxiv.org/abs/2507.01749)
*Arash Dehghan,Mucahit Cevik,Merve Bodur,Bissan Ghaddar*

Main category: cs.AI

TL;DR: 研究探讨在实体零售场景中，利用购物者履行在线订单交付任务的可能性，并提出MDP模型结合NeurADP和DDQN策略优化交付效率。


<details>
  <summary>Details</summary>
Motivation: 解决城市中高效“最后一公里”配送需求问题，探索将实体零售中的顾客转化为订单配送员。

Method: 提出一个整合MDP模型，通过NeurADP进行自适应订单分配，并结合DDQN实现动态定价解决方案。

Result: 结果显示，NeurADP+DDQN策略在交付成本效率上提高了6.7%到18%，并通过灵活交付延迟及多目的地路由分别降低了8%和17%的成本。

Conclusion: 动态且前瞻性的策略在众包配送系统中具有显著优势，为城市物流运营提供了实际指导。

Abstract: This paper examines the use of in-store customers as delivery couriers in a
centralized crowd-shipping system, targeting the growing need for efficient
last-mile delivery in urban areas. We consider a brick-and-mortar retail
setting where shoppers are offered compensation to deliver time-sensitive
online orders. To manage this process, we propose a Markov Decision Process
(MDP) model that captures key uncertainties, including the stochastic arrival
of orders and crowd-shippers, and the probabilistic acceptance of delivery
offers. Our solution approach integrates Neural Approximate Dynamic Programming
(NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network
(DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop
routing and accounts for offer acceptance uncertainty, aligning more closely
with real-world operations. Experimental results demonstrate that the
integrated NeurADP + DDQN policy achieves notable improvements in delivery cost
efficiency, with up to 6.7\% savings over NeurADP with fixed pricing and
approximately 18\% over myopic baselines. We also show that allowing flexible
delivery delays and enabling multi-destination routing further reduces
operational costs by 8\% and 17\%, respectively. These findings underscore the
advantages of dynamic, forward-looking policies in crowd-shipping systems and
offer practical guidance for urban logistics operators.

</details>


### [141] [Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics](https://arxiv.org/abs/2507.01833)
*Yi-Dong Shen,Thomas Eiter*

Main category: cs.AI

TL;DR: 本文通过新的Gelfond答案集（GAS）原则对答案集编程（ASP）进行改进，定义了新的答案集语义，同时分析了其计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 研究是否需要严格遵守最小模型属性、约束单调性和基础性这三个条件，以及提出普遍适用的答案集语义原则。

Method: 改进Gelfond答案集（GAS）原则，提出了合理性原则的新定义，包括支持良好性、基于缺省否定的最小性和基于认知否定的最小性，并定义新的答案集语义。

Result: 提出了新的答案集语义，评估了现有语义，并进行了计算复杂性分析。

Conclusion: 通过改进的GAS原则，提出了具有更强解释能力的答案集语义，使得答案集构造更直观且减少循环推理的可能性。

Abstract: Non-monotonic logic programming is the basis for a declarative problem
solving paradigm known as answer set programming (ASP). Departing from the
seminal definition by Gelfond and Lifschitz in 1988 for simple normal logic
programs, various answer set semantics have been proposed for extensions. We
consider two important questions: (1) Should the minimal model property,
constraint monotonicity and foundedness as defined in the literature be
mandatory conditions for an answer set semantics in general? (2) If not, what
other properties could be considered as general principles for answer set
semantics? We address the two questions. First, it seems that the three
aforementioned conditions may sometimes be too strong, and we illustrate with
examples that enforcing them may exclude expected answer sets. Second, we
evolve the Gelfond answer set (GAS) principles for answer set construction by
refining the Gelfond's rationality principle to well-supportedness, minimality
w.r.t. negation by default and minimality w.r.t. epistemic negation. The
principle of well-supportedness guarantees that every answer set is
constructible from if-then rules obeying a level mapping and is thus free of
circular justification, while the two minimality principles ensure that the
formalism minimizes knowledge both at the level of answer sets and of world
views. Third, to embody the refined GAS principles, we extend the notion of
well-supportedness substantially to answer sets and world views, respectively.
Fourth, we define new answer set semantics in terms of the refined GAS
principles. Fifth, we use the refined GAS principles as an alternative baseline
to intuitively assess the existing answer set semantics. Finally, we analyze
the computational complexity.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [142] [Few-Shot Inspired Generative Zero-Shot Learning](https://arxiv.org/abs/2507.01026)
*Md Shakil Ahamed Shohag,Q. M. Jonathan Wu,Farhad Pourpanah*

Main category: cs.LG

TL;DR: 提出了一种新的生成式零样本学习方法FSIGenZ，减少对大规模特征合成的依赖，实现了有效的性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成式零样本学习方法需要大量计算资源和大量合成数据，放松了零样本学习的初衷。

Method: 提出FSIGenZ，通过Model-Specific Attribute Scoring调整属性得分，并生成组实例原型，同时引入Dual-Purpose Semantic Regularization策略，在语义感知对比分类器上训练。

Result: 在SUN、AwA2和CUB数据集上的实验表明，FSIGenZ在使用较少合成特征的情况下表现出色。

Conclusion: FSIGenZ通过更有效的方式减少对合成特征的依赖，提升了零样本学习的性能。

Abstract: Generative zero-shot learning (ZSL) methods typically synthesize visual
features for unseen classes using predefined semantic attributes, followed by
training a fully supervised classification model. While effective, these
methods require substantial computational resources and extensive synthetic
data, thereby relaxing the original ZSL assumptions. In this paper, we propose
FSIGenZ, a few-shot-inspired generative ZSL framework that reduces reliance on
large-scale feature synthesis. Our key insight is that class-level attributes
exhibit instance-level variability, i.e., some attributes may be absent or
partially visible, yet conventional ZSL methods treat them as uniformly
present. To address this, we introduce Model-Specific Attribute Scoring (MSAS),
which dynamically re-scores class attributes based on model-specific
optimization to approximate instance-level variability without access to unseen
data. We further estimate group-level prototypes as clusters of instances based
on MSAS-adjusted attribute scores, which serve as representative synthetic
features for each unseen class. To mitigate the resulting data imbalance, we
introduce a Dual-Purpose Semantic Regularization (DPSR) strategy while training
a semantic-aware contrastive classifier (SCC) using these prototypes.
Experiments on SUN, AwA2, and CUB benchmarks demonstrate that FSIGenZ achieves
competitive performance using far fewer synthetic features.

</details>


### [143] [DBellQuant: Breaking the Bell with Double-Bell Transformation for LLMs Post Training Binarization](https://arxiv.org/abs/2507.01027)
*Zijian Ye,Wei Huang,Yifei Yu,Tianhe Ren,Zhongrui Wang,Xiaojuan Qi*

Main category: cs.LG

TL;DR: 提出DBellQuant框架，通过几乎1-bit的权重量化和6-bit的激活量化，显著减少性能衰减，同时在LLM模型压缩领域创下了新表现。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在部署时，由于计算和内存需求过高而难以高效部署的问题，特别是量化过程中因分布问题导致的量化误差问题。

Method: 提出了一种创新的后训练量化（PTQ）框架——DBellQuant，通过可学习的双钟形变换（LTDB）算法优化权重分布，减少量化误差并平滑激活输出。

Result: DBellQuant实现了6-bit激活量化，同时在Wikitext2数据集上达到了优异表现，例如在LLaMA2-13B上获得了14.39的困惑度，而显著优于BiLLM的21.35。

Conclusion: DBellQuant显著推进了大语言模型的压缩性能，在权重和激活量化的同时保持了模型性能，展示了在实际应用中的潜力。

Abstract: Large language models (LLMs) demonstrate remarkable performance but face
substantial computational and memory challenges that limit their practical
deployment. Quantization has emerged as a promising solution; however, its
effectiveness is often limited by quantization errors arising from weight
distributions that are not quantization-friendly and the presence of activation
outliers. To address these challenges, we introduce DBellQuant, an innovative
post-training quantization (PTQ) framework that achieves nearly 1-bit weight
compression and 6-bit activation quantization with minimal performance
degradation. DBellQuant uses Learnable Transformation for Dual-Bell (LTDB)
algorithm, which transforms single-bell weight distributions into dual-bell
forms to reduce binarization errors and applies inverse transformations to
smooth activations. DBellQuant sets a new state-of-the-art by preserving
superior model performance under aggressive weight and activation quantization.
For example, on the Wikitext2 dataset, DBellQuant achieves a perplexity of
14.39 on LLaMA2-13B with 6-bit activation quantization, significantly
outperforming BiLLM's 21.35 without activation quantization, underscoring its
potential in compressing LLMs for real-world applications.

</details>


### [144] [Dual Perspectives on Non-Contrastive Self-Supervised Learning](https://arxiv.org/abs/2507.01028)
*Jean Ponce,Martial Hebert,Basile Terver*

Main category: cs.LG

TL;DR: 本文探讨了无对比学习中阻止表示塌陷的机制及理论分析，特别是stop gradient和指数移动平均方法，通过优化理论和动力系统的角度验证了它们避免了表示塌陷。


<details>
  <summary>Details</summary>
Motivation: 旨在通过研究无对比自监督学习中的扮演重要角色的stop gradient和指数移动平均方法，深入理解它们在避免表示塌陷方面的原理及其优化特性。

Method: 从优化理论和动力系统两个理论视角分析stop gradient和指数移动平均方法，以及对应情况的数学性质和动力系统平衡点的稳定性分析。

Result: 证明了不使用stop gradient或指数移动平均时的优化过程会导致表示塌陷，而使用它们的系统平衡点通常是渐近稳定的，不会导致塌陷。

Conclusion: 尽管stop gradient和指数移动平均无法优化原始目标函数，但它们有效避免了表示塌陷，而且这两种机制下的平衡点具有稳定性。

Abstract: The objective of non-contrastive approaches to self-supervised learning is to
train on pairs of different views of the data an encoder and a predictor that
minimize the mean discrepancy between the code predicted from the embedding of
the first view and the embedding of the second one. In this setting, the stop
gradient and exponential moving average iterative procedures are commonly used
to avoid representation collapse, with excellent performance in downstream
supervised applications. This presentation investigates these procedures from
the dual theoretical viewpoints of optimization and dynamical systems. We first
show that, in general, although they do not optimize the original objective, or
for that matter, any other smooth function, they do avoid collapse. Following
Tian et al. [2021], but without any of the extra assumptions used in their
proofs, we then show using a dynamical system perspective that, in the linear
case, minimizing the original objective function without the use of a stop
gradient or exponential moving average always leads to collapse. Conversely, we
finally show that the limit points of the dynamical systems associated with
these two procedures are, in general, asymptotically stable equilibria, with no
risk of degenerating to trivial solutions.

</details>


### [145] [PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning](https://arxiv.org/abs/2507.01029)
*Junjie Zhou,Yingli Zuo,Shichang Feng,Peng Wan,Qi Zhu,Daoqiang Zhang,Wei Shao*

Main category: cs.LG

TL;DR: 本文提出了PathCoT，一种结合病理学专家知识和自我评估的零样本链式推理方法，显著提升了病理视觉任务的理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在病理视觉推理任务中效果不佳，主要因为缺乏领域知识和链式推理步骤易引入错误。

Method: 文章提出了PathCoT方法，通过融合病理学专家知识指导模型推理，并加入自我评估步骤，以提高推理答案的可靠性。

Result: 在PathMMU数据集上的实验结果验证了PathCoT在病理视觉理解和推理任务中的有效性。

Conclusion: 结合领域知识和自我评估策略，PathCoT提升了多模态大语言模型在专业领域任务中的表现，提供了一种应对病理视觉推理挑战的有效方法。

Abstract: With the development of generative artificial intelligence and instruction
tuning techniques, multimodal large language models (MLLMs) have made
impressive progress on general reasoning tasks. Benefiting from the
chain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning
problem step-by-step. However, existing MLLMs still face significant challenges
when applied to pathology visual reasoning tasks: (1) LLMs often underperforms
because they lack domain-specific information, which can lead to model
hallucinations. (2) The additional reasoning steps in CoT may introduce errors,
leading to the divergence of answers. To address these limitations, we propose
PathCoT, a novel zero-shot CoT prompting method which integrates the pathology
expert-knowledge into the reasoning process of MLLMs and incorporates
self-evaluation to mitigate divergence of answers. Specifically, PathCoT guides
the MLLM with prior knowledge to perform as pathology experts, and provides
comprehensive analysis of the image with their domain-specific knowledge. By
incorporating the experts' knowledge, PathCoT can obtain the answers with CoT
reasoning. Furthermore, PathCoT incorporates a self-evaluation step that
assesses both the results generated directly by MLLMs and those derived through
CoT, finally determining the reliable answer. The experimental results on the
PathMMU dataset demonstrate the effectiveness of our method on pathology visual
understanding and reasoning.

</details>


### [146] [Optimizing Flamelet Generated Manifold Models: A Machine Learning Performance Study](https://arxiv.org/abs/2507.01030)
*Reza Lotfi Navaei,Mohammad Safarzadeh,Seyed Mohammad Jafar Sobhani*

Main category: cs.LG

TL;DR: 研究利用四种机器学习算法重建甲烷燃料的层流FGM库，发现MLP方法最佳，通过超参数调整进一步优化，最终优化模型准确率达99.81%。


<details>
  <summary>Details</summary>
Motivation: 现有FGM模型尽管精确，但其实际应用需要耗费大量内存，研究旨在通过机器学习技术开发高效的FGM库以优化甲烷燃烧模拟。

Method: 采用多层感知机（MLP）、随机森林、线性回归和支持向量机四种机器学习算法，训练数据来自七种库。最终以MLP为优化重点，通过超参数调整改进模型性能。

Result: 优化后的最佳MLP模型由四个隐藏层组成，每层分别包含10、15、20和25个神经元，模型准确率达99.81%。

Conclusion: 研究为甲烷燃烧模拟开发了一种高精度、高效的FGM库方法，为化学燃烧建模提供了参考与优化思路。

Abstract: In chemistry tabulations and Flamelet combustion models, the Flamelet
Generated Manifold (FGM) is recognized for its precision and physical
representation. The practical implementation of FGM requires a significant
allocation of memory resources. FGM libraries are developed specifically for a
specific fuel and subsequently utilized for all numerical problems using
machine learning techniques. This research aims to develop libraries of Laminar
FGM utilizing machine learning algorithms for application in combustion
simulations of methane fuel. This study employs four Machine Learning
algorithms to regenerate Flamelet libraries, based on an understanding of data
sources, techniques, and data-driven concepts. 1. Multi-Layer Perceptron; 2.
Random Forest; 3. Linear Regression; 4. Support Vector Machine. Seven libraries
were identified as appropriate for constructing a database for training machine
learning models, giving an error rate of 2.30%. The default architectures of
each method were evaluated to determine the optimal approach, leading to the
selection of the MLP method as the primary choice. The method was enhanced
through hyperparameter tuning to improve accuracy. The quantity of hidden
layers and neurons significantly influences method performance. The optimal
model, comprising four hidden layers with 10, 15, 20, and 25 neurons
respectively, achieved an accuracy of 99.81%.

</details>


### [147] [Fast Clifford Neural Layers](https://arxiv.org/abs/2507.01040)
*Tianxiang Xia,Max Neuwinger,Lin Xiao*

Main category: cs.LG

TL;DR: 这篇论文提出通过克利福德代数增强神经网络以改进偏微分方程（PDE）建模，并优化了其在单核CPU上的推理性能。


<details>
  <summary>Details</summary>
Motivation: 通过引入克利福德代数，旨在改善PDE建模的效率和性能，并优化特殊层的推理速度。

Method: 设计了2D/3D克利福德卷积层和多向量激活层，并优化其在单核CPU上的实现。

Result: 在较大的数据和网络规模下（超过L2缓存），其实现比标准PyTorch快30%。

Conclusion: 论文展示了克利福德神经网络在特定条件下的高效实现，并开放了代码库供学术及技术验证。

Abstract: Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra
into neural networks. In this project we focus on optimizing the inference of
2/3D Clifford convolutional layers and multivector activation layers for one
core CPU performance.
  Overall, by testing on a real network block involving Clifford convolutional
layers and multivector activation layers, we observe that our implementation is
30% faster than standard PyTorch implementation in relatively large data +
network size (>L2 cache).
  We open source our code base at
https://github.com/egretwAlker/c-opt-clifford-layers

</details>


### [148] [PyTorch-based Geometric Learning with Non-CUDA Processing Units: Experiences from Intel Gaudi-v2 HPUs](https://arxiv.org/abs/2507.01031)
*Fanchen Bu,Kijung Shin*

Main category: cs.LG

TL;DR: 这篇论文探讨了将基于PyTorch的几何学习框架移植到Intel的Gaudi-v2 HPUs的经验，目标是降低非CUDA硬件的使用门槛，同时提升研究者对几何学习算法在多硬件平台上的实验能力。


<details>
  <summary>Details</summary>
Motivation: 当前几何学习应用场景广泛，但主要基于Nvidia CUDA-GPU的生态系统还存在硬件垄断与能效问题，因此有必要探索非CUDA硬件如Intel Gaudi HPU的潜能与适配性。

Method: 作者通过添加关键工具支持（如散列、稀疏索引和k近邻），开发了一套适配Gaudi-v2 HPU架构的核心工具，并辅以教程及案例分析来克服性能障碍。最终，研究结果通过开源GitHub库的形式与公众共享。

Result: 作者成功移植几何学习框架到Gaudi-v2 HPU，完成工具开发并提供一系列学习教程和例子，为将来优化和跨硬件平台的迁移奠定了基础。

Conclusion: 本研究通过改进工具与公开资源，降低了几何学习领域跨硬件平台实验的门槛，为图结构数据处理提供了更广的硬件选择与性能优化方向。

Abstract: Geometric learning has emerged as a powerful paradigm for modeling
non-Euclidean data, especially graph-structured ones, with applications
spanning social networks, molecular structures, knowledge graphs, and
recommender systems. While Nvidia's CUDA-enabled graphics processing units
(GPUs) largely dominate the hardware landscape, emerging accelerators such as
Intel's Gaudi Habana Processing Units (HPUs) offer competitive performance and
energy efficiency. However, the usage of such non-CUDA processing units
requires significant engineering effort and novel software adaptations. In this
work, we present our experiences porting PyTorch-based geometric learning
frameworks to Gaudi-v2 HPUs. We introduce a collection of core utilities that
restore essential operations (e.g., scatter, sparse indexing, k-nearest
neighbors) on Gaudi-v2 HPUs, and we consolidate sixteen guided tutorials and
eleven real-world examples with diagnostic analyses of encountered failures and
detailed workarounds. We collect all our experiences into a publicly accessible
GitHub repository. Our contributions lower the barrier for researchers to
experiment with geometric-learning algorithms and models on non-CUDA hardware,
providing a foundation for further optimization and cross-platform portability.

</details>


### [149] [Long-Sequence Memory with Temporal Kernels and Dense Hopfield Functionals](https://arxiv.org/abs/2507.01052)
*Ahmed Farooq*

Main category: cs.LG

TL;DR: 研究提出了一种新能量函数，用于提升长序列记忆性能，基于Hopfield网络框架实现高效存储和顺序回忆。


<details>
  <summary>Details</summary>
Motivation: 解决现有变压器在处理长上下文任务中的局限性，增强对时间序列数据的长时依赖处理。

Method: 引入时间核函数$K(m, k)$，在Hopfield网络框架中整合时间依赖性，实现高维数据的顺序检索。

Result: 成功用于存储和顺序回忆电影帧，并在长序列建模、优化注意力机制等方面表现出色。

Conclusion: 模型为处理长上下文任务提供了创新途径，在自然语言处理、预测等领域具有重要潜力。

Abstract: In this study we introduce a novel energy functional for long-sequence
memory, building upon the framework of dense Hopfield networks which achieves
exponential storage capacity through higher-order interactions. Building upon
earlier work on long-sequence Hopfield memory models, we propose a temporal
kernal $K(m, k)$ to incorporate temporal dependencies, enabling efficient
sequential retrieval of patterns over extended sequences. We demonstrate the
successful application of this technique for the storage and sequential
retrieval of movies frames which are well suited for this because of the high
dimensional vectors that make up each frame creating enough variation between
even sequential frames in the high dimensional space. The technique has
applications in modern transformer architectures, including efficient
long-sequence modeling, memory augmentation, improved attention with temporal
bias, and enhanced handling of long-term dependencies in time-series data. Our
model offers a promising approach to address the limitations of transformers in
long-context tasks, with potential implications for natural language
processing, forecasting, and beyond.

</details>


### [150] [An Uncertainty-Aware Dynamic Decision Framework for Progressive Multi-Omics Integration in Classification Tasks](https://arxiv.org/abs/2507.01032)
*Nan Mu,Hongbo Yang,Chen Zhao*

Main category: cs.LG

TL;DR: 提出了一种基于多视图动态决策框架的多组学数据分类方法，以减少检测成本并保持高诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 降低多组学数据处理的经济成本，同时避免对全组学数据的过度依赖导致资源浪费。

Method: 通过优化神经网络的激活函数生成Dirichlet分布参数，并结合主观逻辑评估分类结果的不确定性。在多组学层面上，利用Dempster-Shafer理论进行模态融合，同时引入动态决策机制逐步利用患者的数据源。

Result: 在四个多组学数据集（ROSMAP、LGG、BRCA和KIPAN）上验证，超过50%的案例通过单组学模态实现准确分类，降低了冗余测试，并保持与全组学模型相当的诊断性能。

Conclusion: 所提方法不仅在减少检测成本方面表现出色，同时保留了诊断性能和生物学洞察力。

Abstract: Background and Objective: High-throughput multi-omics technologies have
proven invaluable for elucidating disease mechanisms and enabling early
diagnosis. However, the high cost of multi-omics profiling imposes a
significant economic burden, with over reliance on full omics data potentially
leading to unnecessary resource consumption. To address these issues, we
propose an uncertainty-aware, multi-view dynamic decision framework for omics
data classification that aims to achieve high diagnostic accuracy while
minimizing testing costs. Methodology: At the single-omics level, we refine the
activation functions of neural networks to generate Dirichlet distribution
parameters, utilizing subjective logic to quantify both the belief masses and
uncertainty mass of classification results. Belief mass reflects the support of
a specific omics modality for a disease class, while the uncertainty parameter
captures limitations in data quality and model discriminability, providing a
more trustworthy basis for decision-making. At the multi omics level, we employ
a fusion strategy based on Dempster-Shafer theory to integrate heterogeneous
modalities, leveraging their complementarity to boost diagnostic accuracy and
robustness. A dynamic decision mechanism is then applied that omics data are
incrementally introduced for each patient until either all data sources are
utilized or the model confidence exceeds a predefined threshold, potentially
before all data sources are utilized. Results and Conclusion: We evaluate our
approach on four benchmark multi-omics datasets, ROSMAP, LGG, BRCA, and KIPAN.
In three datasets, over 50% of cases achieved accurate classification using a
single omics modality, effectively reducing redundant testing. Meanwhile, our
method maintains diagnostic performance comparable to full-omics models and
preserves essential biological insights.

</details>


### [151] [Data-driven Insights for Informed Decision-Making: Applying LSTM Networks for Robust Electricity Forecasting in Libya](https://arxiv.org/abs/2507.01034)
*Asma Agaal,Mansour Essgaer,Hend M. Farkash,Zulaiha Ali Othman*

Main category: cs.LG

TL;DR: 研究针对利比亚班加西的用电预测问题，提出基于数据驱动的框架进行2025年电力负荷、发电量和缺口的准确预测，其中LSTM模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 由于班加西存在频繁的负载中断、发电缺口和基础设施限制，因此准确的电力预测对于电网稳定和能源规划至关重要。

Method: 采用一种数据驱动的方法，基于2019年（不稳定年）和2023年（较为稳定年）的历史数据进行预测，并应用多种时间序列模型（例如ARIMA、LSTM等）。数据质量通过插补缺失值、异常值平滑和对数变换进行了改进。优化的LSTM框架还整合了外因（如温度和湿度）。

Result: LSTM模型在所有模型中表现最佳，展示了对非平稳性和季节性模式的强大建模能力。

Conclusion: 优化的LSTM框架为在数据匮乏和波动性大的地区进行预测提供了强健工具，对政策制定者和电网运营者具有实际意义。

Abstract: Accurate electricity forecasting is crucial for grid stability and energy
planning, especially in Benghazi, Libya, where frequent load shedding,
generation deficits, and infrastructure limitations persist. This study
proposes a data-driven approach to forecast electricity load, generation, and
deficits for 2025 using historical data from 2019 (a year marked by
instability) and 2023 (a more stable year). Multiple time series models were
applied, including ARIMA, seasonal ARIMA, dynamic regression ARIMA, exponential
smoothing, extreme gradient boosting, and Long Short-Term Memory (LSTM) neural
networks. The dataset was enhanced through missing value imputation, outlier
smoothing, and log transformation. Performance was assessed using mean squared
error, root mean squared error, mean absolute error, and mean absolute
percentage error. LSTM outperformed all other models, showing strong
capabilities in modeling non-stationary and seasonal patterns. A key
contribution of this work is an optimized LSTM framework that integrates
exogenous factors such as temperature and humidity, offering robust performance
in forecasting multiple electricity indicators. These results provide practical
insights for policymakers and grid operators to enable proactive load
management and resource planning in data-scarce, volatile regions.

</details>


### [152] [Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems](https://arxiv.org/abs/2507.01035)
*Yushang Zhao,Haotian Lyu,Yike Peng,Aijia Sun,Feng Jiang,Xinyue Han*

Main category: cs.LG

TL;DR: 研究聚焦于增强推荐系统性能，通过混合使用图神经网络（GNN）和大语言模型（LLM），并采用优化策略提升其低延迟推理和高效训练能力。


<details>
  <summary>Details</summary>
Motivation: 随着在线服务的增长，对高效、实时的推荐系统需求增加。本文旨在解决混合GNN和LLM的推荐系统在推理延迟和训练效率上的瓶颈问题。

Method: 通过构建混合GNN-LLM框架，结合量化、LoRA、蒸馏等优化策略以及硬件加速（如FPGA和DeepSpeed），提高系统性能。

Result: 结合FPGA和DeepSpeed的优化配置在提升准确性方面优越（NDCG@10: 0.75），且延迟控制在40-60ms以内。LoRA使训练时间缩短66%，从而显著提升效率。

Conclusion: 硬件与软件协同设计以及参数高效调优显著提升混合模型表现，优于单独实现GNN或LLM。建议使用FPGA和LoRA进行实时部署，并探索联邦学习和先进融合架构以提高系统扩展性与隐私保护能力。

Abstract: The incessant advent of online services demands high speed and efficient
recommender systems (ReS) that can maintain real-time performance along with
processing very complex user-item interactions. The present study, therefore,
considers computational bottlenecks involved in hybrid Graph Neural Network
(GNN) and Large Language Model (LLM)-based ReS with the aim optimizing their
inference latency and training efficiency. An extensive methodology was used:
hybrid GNN-LLM integrated architecture-optimization strategies(quantization,
LoRA, distillation)-hardware acceleration (FPGA, DeepSpeed)-all under R 4.4.2.
Experimental improvements were significant, with the optimal Hybrid + FPGA +
DeepSpeed configuration reaching 13.6% more accuracy (NDCG@10: 0.75) at 40-60ms
of latency, while LoRA brought down training time by 66% (3.8 hours) in
comparison to the non-optimized baseline. Irrespective of domain, such as
accuracy or efficiency, it can be established that hardware-software co-design
and parameter-efficient tuning permit hybrid models to outperform GNN or LLM
approaches implemented independently. It recommends the use of FPGA as well as
LoRA for real-time deployment. Future work should involve federated learning
along with advanced fusion architectures for better scalability and privacy
preservation. Thus, this research marks the fundamental groundwork concerning
next-generation ReS balancing low-latency response with cutting-edge
personalization.

</details>


### [153] [Learning to Segment for Vehicle Routing Problems](https://arxiv.org/abs/2507.01037)
*Wenbin Ouyang,Sirui Li,Yining Ma,Cathy Wu*

Main category: cs.LG

TL;DR: 提出了一种称为First-Segment-Then-Aggregate (FSTA) 的分解技术，加速解决车辆路径问题（VRPs）的迭代搜索方法，并引入了可学习的Learning-to-Segment (L2Seg)框架来优化该技术的性能。


<details>
  <summary>Details</summary>
Motivation: 在解决车辆路径问题的迭代搜索方法中，大量的解在搜索迭代中保持稳定，导致大量冗余计算，尤其在处理大规模VRPs时尤为突出。

Method: 研究并提出了FSTA分解技术，该技术在搜索中保留稳定的解段，将每个段中的节点汇总为固定的超节点，只针对不稳定的部分进行搜索。此外，引入了一种名为L2Seg的神经框架，有三个变体（非自回归、自动回归及其结合），用于智能识别稳定和不稳定的段落，并提出了专门的训练与推断策略。

Result: 在CVRP和VRPTW实验中，L2Seg将现有的迭代搜索器加速了最多7倍，且非自回归与自动回归的协同作用表现最佳。此外，L2Seg框架灵活，兼容传统、基于学习和混合的求解器，并支持多种类型的VRPs。

Conclusion: L2Seg框架通过引入智能分段技术，大幅提升了解决车辆路径问题的迭代搜索器的效率，且具有很大的灵活性和广泛的适用性。

Abstract: Iterative search heuristics are widely recognized as state-of-the-art for
solving Vehicle Routing Problems (VRPs). In this work, we identify and exploit
a critical observation: within these solvers, a large portion of the solution
remains stable, i.e., unchanged across search iterations, causing redundant
computations, especially for large-scale VRPs with long subtours. To address
this, we pioneer the formal study of the First-Segment-Then-Aggregate (FSTA)
decomposition technique to accelerate iterative solvers. Specifically, FSTA
preserves stable solution segments during the search, aggregates nodes within
each segment into fixed hypernodes, and focuses the search only on unstable
portions. Yet, a key challenge lies in identifying which segments should be
aggregated by FSTA. To this end, we then introduce Learning-to-Segment (L2Seg),
a novel neural framework to intelligently differentiate potentially stable and
unstable portions for FSTA decomposition. We present three L2Seg variants:
non-autoregressive (globally comprehensive but locally indiscriminate),
autoregressive (locally refined but globally deficient), and their synergy,
with bespoke training and inference strategies. Empirical results on CVRP and
VRPTW suggest that L2Seg accelerates state-of-the-art iterative solvers by up
to 7x. Additionally, we provide in-depth analysis showing NAR and AR synergy
achieves best performance by combining their complementary strengths. Notably,
L2Seg is a flexible framework that is compatible with traditional,
learning-based, and hybrid solvers, while supporting a broad class of VRPs.

</details>


### [154] [On-Policy Optimization of ANFIS Policies Using Proximal Policy Optimization](https://arxiv.org/abs/2507.01039)
*Kaaustaaub Shankar,Wilhelm Louw,Kelly Cohen*

Main category: cs.LG

TL;DR: 本文提出一种基于Proximal Policy Optimization (PPO) 的强化学习方法，用于训练神经模糊控制器。实验表明，与基于Deep Q-Network (DQN) 的方法相比，PPO方法收敛速度更快且训练过程方差更低。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有使用Deep Q-Learning方法应用于自适应神经模糊推理系统（ANFIS）存在的训练不稳定性问题，探索一种新的稳定的训练方法。

Method: 采用一种基于PPO的强化学习方法，使用了稳定的on-policy的actor-critic 训练框架，替代之前基于off-policy的价值框架。

Result: 在CartPole-v1环境中的实验表明，使用PPO训练的模糊控制器能够在20000次更新后实现稳定的500分回报，表现出比基于DQN的方法更少的方差和更快的收敛速度。

Conclusion: PPO为训练可解释的神经模糊控制器提供了一种有潜力的方法，可用于解决强化学习任务。

Abstract: We propose a reinforcement learning (RL) approach for training neuro-fuzzy
controllers using Proximal Policy Optimization (PPO). Building on prior work
that applied Deep Q-Learning to Adaptive Neuro-Fuzzy Inference Systems (ANFIS),
our method replaces the off-policy value-based framework with a stable
on-policy actor-critic loop. We evaluate this approach in the CartPole-v1
environment using multiple random seeds and compare its learning performance
against ANFIS-Deep Q-Network (DQN) baselines. It was found that PPO-trained
fuzzy agents achieved a mean return of 500 +/- 0 on CartPole-v1 after 20000
updates, showcasing less variance than prior DQN-based methods during training
and overall faster convergence. These findings suggest that PPO offers a
promising pathway for training explainable neuro-fuzzy controllers in
reinforcement learning tasks.

</details>


### [155] [Fast AI Model Splitting over Edge Networks](https://arxiv.org/abs/2507.01041)
*Zuguang Li,Wen Wu,Shaohua Wu,Songge Zhang,Ye Wang,Xuemin,Shen*

Main category: cs.LG

TL;DR: 提出了一种基于DAGs的快速分割学习算法，解决复杂AI模型的复杂性，实验表明能显著减少训练延迟。


<details>
  <summary>Details</summary>
Motivation: 解决复杂AI模型的高计算复杂性以实现最优分割。

Method: 将AI模型表示为DAG，并构建最小s-t切割问题，通过最大流法实现最优分割；对于块状结构模型，引入块级算法以简化DAG并减少计算量。

Result: 提出的算法在毫秒内实现最优分割，并在动态边缘网络训练中减少24.62%-38.95%的延迟。

Conclusion: 提供了一种高效、精确的模型分割方法，具有显著降低训练延迟的潜力。

Abstract: Split learning (SL) has emerged as a computationally efficient approach for
artificial intelligence (AI) model training, which can alleviate device-side
computational workloads. However, complex AI model architectures pose high
computational complexity to obtain the optimal model splitting. In this paper,
we represent an arbitrary AI model as a directed acyclic graph (DAG), and then
reformulate the optimal model splitting problem as a minimum s-t cut search
problem. To solve the problem, we propose a fast DAG-based model splitting
algorithm, which restructures the DAG to enable the optimal model splitting
identification via a maximum flow method. Theoretical analysis indicates that
the proposed algorithm is optimal. Furthermore, considering AI models with
block structures, we propose a block-wise model splitting algorithm to reduce
computational complexity. The algorithm abstracts each block, i.e., a component
consisting of multiple layers, into a single vertex, thereby obtaining the
optimal model splitting via a simplified DAG. Extensive experimental results
demonstrate that the proposed algorithms can determine the optimal model
splitting within milliseconds, as well as reduce training delay by
24.62%-38.95% in dynamic edge networks as compared to the state-of-the-art
benchmarks.

</details>


### [156] [Data Classification with Dynamically Growing and Shrinking Neural Networks](https://arxiv.org/abs/2507.01043)
*Szymon Świderski,Agnieszka Jastrzębska*

Main category: cs.LG

TL;DR: 本文提出了一种神经网络动态结构调整的方法，通过蒙特卡洛树搜索实现模型架构的动态收缩和扩展，并在视觉和时间序列分类任务中展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 目前的大多神经网络模型使用固定架构和可训练权重，但不断寻找和优化模型架构则能带来更好的表现。

Method: 通过蒙特卡洛树搜索机制，在模型训练期间动态调整神经网络结构，包含动态收缩和扩展，选择最佳架构改变。

Result: 在视觉模式和多变量时间序列分类任务中的实验表明，本方法表现出高鲁棒性和适应性，特别适合多变量时间序列分类，且代码公开以支持复现。

Conclusion: 动态调整神经网络结构的能力提升了模型表现，尤其是在需要高适应能力的任务中效果显著。

Abstract: The issue of data-driven neural network model construction is one of the core
problems in the domain of Artificial Intelligence. A standard approach assumes
a fixed architecture with trainable weights. A conceptually more advanced
assumption is that we not only train the weights, but also find out the optimal
model architecture. We present a new method that realizes just that. This
article is an extended version of our conference paper titled "Dynamic Growing
and Shrinking of Neural Networks with Monte Carlo Tree Search [26]". In the
paper, we show in detail how to create a neural network with a procedure that
allows dynamic shrinking and growing of the model while it is being trained.
The decision-making mechanism for the architectural design is governed by a
Monte Carlo tree search procedure which simulates network behavior and allows
to compare several candidate architecture changes to choose the best one. The
proposed method was validated using both visual and time series datasets,
demonstrating its particular effectiveness in multivariate time series
classification. This is attributed to the architecture's ability to adapt
dynamically, allowing independent modifications for each time series. The
approach is supplemented by Python source code for reproducibility.
Experimental evaluations in visual pattern and multivariate time series
classification tasks revealed highly promising performance, underscoring the
method's robustness and adaptability.

</details>


### [157] [Sensing Cardiac Health Across Scenarios and Devices: A Multi-Modal Foundation Model Pretrained on Heterogeneous Data from 1.7 Million Individuals](https://arxiv.org/abs/2507.01045)
*Xiao Gu,Wei Tang,Jinpei Han,Veer Sangha,Fenglin Liu,Shreyank N Gowda,Antonio H. Ribeiro,Patrick Schwab,Kim Branson,Lei Clifton,Antonio Luiz P. Ribeiro,Zhangdaihong Liu,David A. Clifton*

Main category: cs.LG

TL;DR: 提出了一种心脏感知基础模型(CSFM)，通过先进的transformer架构和生成的掩码预训练策略，从异构的数据记录中学习统一表示，并适用于各种心脏感知任务。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法因依赖同质数据集和静态的定制模型，导致其在不同的临床环境中鲁棒性和泛化性受限。

Method: 使用transformer架构和生成式掩码预训练策略，利用多模态数据（包括MIMIC-III-WDB、MIMIC-IV-ECG和CODE）进行大规模预训练，从约1.7百万个个体的心脏信号和相关文本报告中学习统一表征。

Result: CSFM在诊断任务、人口统计信息识别、生命体征测量、临床结果预测和ECG问答中，均优于传统单模单任务的方法，且在不同的心电图导联配置和传感器输入场景（如仅使用ECG、仅使用PPG或二者结合）中表现出色。

Conclusion: CSFM是一种多功能且可扩展的解决方案，能够支持全面心脏监测，在各种传感器和临床场景中具有重要应用潜力。

Abstract: Cardiac biosignals, such as electrocardiograms (ECG) and photoplethysmograms
(PPG), are of paramount importance for the diagnosis, prevention, and
management of cardiovascular diseases, and have been extensively used in a
variety of clinical tasks. Conventional deep learning approaches for analyzing
these signals typically rely on homogeneous datasets and static bespoke models,
limiting their robustness and generalizability across diverse clinical settings
and acquisition protocols. In this study, we present a cardiac sensing
foundation model (CSFM) that leverages advanced transformer architectures and a
generative, masked pretraining strategy to learn unified representations from
vast, heterogeneous health records. Our model is pretrained on an innovative
multi-modal integration of data from multiple large-scale datasets (including
MIMIC-III-WDB, MIMIC-IV-ECG, and CODE), comprising cardiac signals and the
corresponding clinical or machine-generated text reports from approximately 1.7
million individuals. We demonstrate that the embeddings derived from our CSFM
not only serve as effective feature extractors across diverse cardiac sensing
scenarios, but also enable seamless transfer learning across varying input
configurations and sensor modalities. Extensive evaluations across diagnostic
tasks, demographic information recognition, vital sign measurement, clinical
outcome prediction, and ECG question answering reveal that CSFM consistently
outperforms traditional one-modal-one-task approaches. Notably, CSFM exhibits
robust performance across multiple ECG lead configurations from standard
12-lead systems to single-lead setups, and in scenarios where only ECG, only
PPG, or a combination thereof is available. These findings highlight the
potential of CSFM as a versatile and scalable solution, for comprehensive
cardiac monitoring.

</details>


### [158] [Variational Digital Twins](https://arxiv.org/abs/2507.01047)
*Logan A. Burnett,Umme Mahbuba Nabila,Majdi I. Radaideh*

Main category: cs.LG

TL;DR: 本文提出了一种变分数字孪生（VDT）框架，通过加入贝叶斯输出层和创新的更新算法，使数字孪生能够在标准硬件上快速更新并提供校准的预测不确定性。


<details>
  <summary>Details</summary>
Motivation: 当前的数字孪生研究在模型与资产的信息交换框架、实时实现关键功能以及模型不确定性分析方面存在不足，迫切需要一种改进的方法。

Method: 提出了变分数字孪生（VDT）框架，将标准神经网络架构与贝叶斯输出层结合，并设计了一种快速高效的更新算法。

Result: 通过四个能源领域的案例研究验证了VDT的有效性，包括预测实验减少47%且提升准确率、可再生能源预测保持高精度、传感器丢失情境下的鲁棒性，以及动态电池模型在预测精确度上的显著改进。

Conclusion: VDT框架能够将传统替代模型转变为具有不确定性意识、数据高效且计算可控的数字孪生，为工业和科学能源系统提供了可靠的建模解决方案。

Abstract: While digital twins (DT) hold promise for providing real-time insights into
complex energy assets, much of the current literature either does not offer a
clear framework for information exchange between the model and the asset, lacks
key features needed for real-time implementation, or gives limited attention to
model uncertainty. Here, we aim to solve these gaps by proposing a variational
digital twin (VDT) framework that augments standard neural architectures with a
single Bayesian output layer. This lightweight addition, along with a novel VDT
updating algorithm, lets a twin update in seconds on commodity GPUs while
producing calibrated uncertainty bounds that can inform experiment design,
control algorithms, and model reliability. The VDT is evaluated on four
energy-sector problems. For critical-heat-flux prediction, uncertainty-driven
active learning reaches R2 = 0.98 using 47 % fewer experiments and one-third
the training time of random sampling. A three-year renewable-generation twin
maintains R2 > 0.95 for solar output and curbs error growth for volatile wind
forecasts via monthly updates that process only one month of data at a time. A
nuclear reactor transient cooldown twin reconstructs thermocouple signals with
R2 > 0.99 and preserves accuracy after 50 % sensor loss, demonstrating
robustness to degraded instrumentation. Finally, a physics-informed Li-ion
battery twin, retrained after every ten discharges, lowers voltage mean-squared
error by an order of magnitude relative to the best static model while adapting
its credible intervals as the cell approaches end-of-life. These results
demonstrate that combining modest Bayesian augmentation with efficient update
schemes turns conventional surrogates into uncertainty-aware, data-efficient,
and computationally tractable DTs, paving the way for dependable models across
industrial and scientific energy systems.

</details>


### [159] [3W Dataset 2.0.0: a realistic and public dataset with rare undesirable real events in oil wells](https://arxiv.org/abs/2507.01048)
*Ricardo Emanuel Vaz Vargas,Afrânio José de Melo Junior,Celso José Munaro,Cláudio Benevenuto de Campos Lima,Eduardo Toledo de Lima Junior,Felipe Muntzberg Barrocas,Flávio Miguel Varejão,Guilherme Fidelis Peixer,Igor de Melo Nery Oliveira,Jader Riso Barbosa Jr.,Jaime Andrés Lozano Cadena,Jean Carlos Dias de Araújo,João Neuenschwander Escosteguy Carneiro,Lucas Gouveia Omena Lopes,Lucas Pereira de Gouveia,Mateus de Araujo Fernandes,Matheus Lima Scramignon,Patrick Marques Ciarelli,Rodrigo Castello Branco,Rogério Leite Alves Pinto*

Main category: cs.LG

TL;DR: 本论文介绍了3W数据集的最新公开版本，用于通过时间序列分析检测石油井中不可取事件。


<details>
  <summary>Details</summary>
Motivation: 石油井中的不可取事件会带来经济损失、环境事故和人员伤亡，现有数据集不足以支持相关的人工智能和机器学习研究。

Method: Petrobras公司开发并发布了3W数据集，该数据集包含由专家标注的多变量时间序列数据，当前提供了结构性修改和新增标签数据的公开版本。

Result: 改进后的3W数据集能够支持社区进一步提高已有研究成果，同时激励开发新方法与产品。

Conclusion: 3W数据集为石油井事件的早期检测提供了支持，并成为相关领域的参考基础，有助于采取防范或修正措施。

Abstract: In the oil industry, undesirable events in oil wells can cause economic
losses, environmental accidents, and human casualties. Solutions based on
Artificial Intelligence and Machine Learning for Early Detection of such events
have proven valuable for diverse applications across industries. In 2019,
recognizing the importance and the lack of public datasets related to
undesirable events in oil wells, Petrobras developed and publicly released the
first version of the 3W Dataset, which is essentially a set of Multivariate
Time Series labeled by experts. Since then, the 3W Dataset has been developed
collaboratively and has become a foundational reference for numerous works in
the field. This data article describes the current publicly available version
of the 3W Dataset, which contains structural modifications and additional
labeled data. The detailed description provided encourages and supports the 3W
community and new 3W users to improve previous published results and to develop
new robust methodologies, digital products and services capable of detecting
undesirable events in oil wells with enough anticipation to enable corrective
or mitigating actions.

</details>


### [160] [Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization](https://arxiv.org/abs/2507.01050)
*Jing Yu,Yibo Zhao,Jiapeng Zhu,Wenming Shao,Bo Pang,Zhao Zhang,Xiang Li*

Main category: cs.LG

TL;DR: 提出了一种能有效平衡数据高效性、语义保留和模型泛化能力的两个阶段训练框架，用于社交媒体文本的去毒化。


<details>
  <summary>Details</summary>
Motivation: 应对社交媒体上有害内容的泛滥，这些内容威胁到在线环境和公共讨论的健康。

Method: 采用两个阶段的训练框架：第一阶段在小规模高质量的并行数据上进行有监督微调；第二阶段利用无标注有毒数据和定制奖励模型，通过“Group Relative Policy Optimization”训练模型。

Result: 实验结果表明，该方法实现了性能的最新水平，具备更强的泛化能力，并大大降低了对注释数据的依赖。

Conclusion: 本文提出的方法有效缓解了去毒化任务中的多种权衡问题，并在性能、泛化性以及数据依赖性方面展现了显著优势。

Abstract: The widespread dissemination of toxic content on social media poses a serious
threat to both online environments and public discourse, highlighting the
urgent need for detoxification methods that effectively remove toxicity while
preserving the original semantics. However, existing approaches often struggle
to simultaneously achieve strong detoxification performance, semantic
preservation, and robustness to out-of-distribution data. Moreover, they
typically rely on costly, manually annotated parallel corpora while showing
poor data efficiency. To address these challenges, we propose a two-stage
training framework that jointly optimizes for data efficiency, semantic
preservation, and model generalization. We first perform supervised fine-tuning
on a small set of high-quality, filtered parallel data to establish a strong
initialization. Then, we leverage unlabeled toxic inputs and a custom-designed
reward model to train the LLM using Group Relative Policy Optimization.
Experimental results demonstrate that our method effectively mitigates the
trade-offs faced by previous work, achieving state-of-the-art performance with
improved generalization and significantly reduced dependence on annotated data.
Our code is available at:
https://anonymous.4open.science/r/Detoxification-of-Text-725F/

</details>


### [161] [XxaCT-NN: Structure Agnostic Multimodal Learning for Materials Science](https://arxiv.org/abs/2507.01054)
*Jithendaraa Subramanian,Linda Hung,Daniel Schweigert,Santosh Suram,Weike Ye*

Main category: cs.LG

TL;DR: 提出了一种可扩展的多模态框架，直接从元素组成和X射线衍射（XRD）中学习，而无需晶体结构输入。


<details>
  <summary>Details</summary>
Motivation: 现有的基于结构的模型对真实世界的应用不够现实，尤其是在某些情况下原子结构未知或难以获取。

Method: 设计了一种多模态框架，结合专用编码器和跨注意力融合模块，使用5百万样本的Alexandria数据集进行训练，提出了掩蔽XRD建模（MXM）和对比对齐两种自监督预训练策略。

Result: 预训练方法加速了收敛（最多提升4.2倍），提高了模型准确性和表示质量；多模态性能比单模态基线在数据规模增大时表现更佳。

Conclusion: 成果确立了用于材料科学的无结构输入且具实验基础的基础模型的新方向。

Abstract: Recent advances in materials discovery have been driven by structure-based
models, particularly those using crystal graphs. While effective for
computational datasets, these models are impractical for real-world
applications where atomic structures are often unknown or difficult to obtain.
We propose a scalable multimodal framework that learns directly from elemental
composition and X-ray diffraction (XRD) -- two of the more available modalities
in experimental workflows without requiring crystal structure input. Our
architecture integrates modality-specific encoders with a cross-attention
fusion module and is trained on the 5-million-sample Alexandria dataset. We
present masked XRD modeling (MXM), and apply MXM and contrastive alignment as
self-supervised pretraining strategies. Pretraining yields faster convergence
(up to 4.2x speedup) and improves both accuracy and representation quality. We
further demonstrate that multimodal performance scales more favorably with
dataset size than unimodal baselines, with gains compounding at larger data
regimes. Our results establish a path toward structure-free, experimentally
grounded foundation models for materials science.

</details>


### [162] [Evaluating Pavement Deterioration Rates Due to Flooding Events Using Explainable AI](https://arxiv.org/abs/2507.01056)
*Lidan Peng,Lu Gao,Feng Hong,Jingran Sun*

Main category: cs.LG

TL;DR: 研究探讨洪水如何影响路面劣化，计算出受洪水影响的路面比未受影响路面更加快速地变得粗糙。


<details>
  <summary>Details</summary>
Motivation: 洪水对路面基础设施的损害显著，为了更好地理解和缓解这种影响，此研究探索了洪水如何影响路面粗糙度的变化及其劣化速率。

Method: 利用TxDOT的PMIS数据库中的20年路况数据，与洪水事件数据（包括持续时间和空间范围）相结合，进行统计分析，应用XAI技术（如SHAP和LIME）解释洪水对路面性能的影响。

Result: 研究表明，受洪水影响的路面粗糙度增速显著快于未受洪水影响的路段。

Conclusion: 研究强调需要采取前瞻性的洪水缓解策略，如改进排水系统、使用抗洪材料和预防性维护，以提升易受洪水影响地区的路面耐久性。

Abstract: Flooding can damage pavement infrastructure significantly, causing both
immediate and long-term structural and functional issues. This research
investigates how flooding events affect pavement deterioration, specifically
focusing on measuring pavement roughness by the International Roughness Index
(IRI). To quantify these effects, we utilized 20 years of pavement condition
data from TxDOT's PMIS database, which is integrated with flood event data,
including duration and spatial extent. Statistical analyses were performed to
compare IRI values before and after flooding and to calculate the deterioration
rates influenced by flood exposure. Moreover, we applied Explainable Artificial
Intelligence (XAI) techniques, such as SHapley Additive exPlanations (SHAP) and
Local Interpretable Model-Agnostic Explanations (LIME), to assess the impact of
flooding on pavement performance. The results demonstrate that flood-affected
pavements experience a more rapid increase in roughness compared to non-flooded
sections. These findings emphasize the need for proactive flood mitigation
strategies, including improved drainage systems, flood-resistant materials, and
preventative maintenance, to enhance pavement resilience in vulnerable regions.

</details>


### [163] [Loop2Net: Data-Driven Generation and Optimization of Airfoil CFD Meshes from Sparse Boundary Coordinates](https://arxiv.org/abs/2507.01057)
*Lushun Fan,Yuqin Xia,Jun Li,Karl Jenkins*

Main category: cs.LG

TL;DR: 提出了基于深度卷积神经网络（CNN）的网格质量智能优化系统，以优化网格生成和质量。


<details>
  <summary>Details</summary>
Motivation: 改善网格生成与优化过程的质量与效率，特别是在给定机翼坐标情况下。

Method: 研究核心为Loop2Net生成器和损失函数，通过深度学习来预测网格，并用两个关键损失函数进行优化。

Result: 系统实现了对机翼坐标的网格生成目标，并通过增加惩罚项达到最优性能。

Conclusion: 证明了基于深度学习的网格优化系统在提高生成效率和质量方面的可行性。

Abstract: In this study, an innovative intelligent optimization system for mesh quality
is proposed, which is based on a deep convolutional neural network
architecture, to achieve mesh generation and optimization. The core of the
study is the Loop2Net generator and loss function, it predicts the mesh based
on the given wing coordinates. And the model's performance is continuously
optimised by two key loss functions during the training. Then discipline by
adding penalties, the goal of mesh generation was finally reached.

</details>


### [164] [Evaluation of a Foundational Model and Stochastic Models for Forecasting Sporadic or Spiky Production Outages of High-Performance Machine Learning Services](https://arxiv.org/abs/2507.01067)
*Keun Soo Yim*

Main category: cs.LG

TL;DR: 本文优化基础模型预测器以处理机器学习服务中稀有、尖峰的生产中断事件。实验表明，相较传统模型，该模型在稀有事件预测中表现更优，并可有效预测一年内的中断统计数据，误差小于6%。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在长序列和零样本设置中表现优异，但对稀有、尖峰事件的预测仍是难点。本文旨在探索优化基础模型以预测这些极端事件。

Method: 本文将一种最先进的基础模型调整优化，并与经典随机预测模型（如移动平均、自回归）进行误差评估，分析各模型对稀有、尖峰事件的表现，并找出数据模式与模型间的适配性。

Result: 优化后的基础模型能够有效预测稀有的尖峰中断事件，相较经典的随机模型表现更佳。此外，经过参数优化的模型可实现一年内某特定中断根因的统计预测，误差小于6%。

Conclusion: 优化的基础模型在预测稀有、极端事件上领先于传统方法，为长时间稀有事件的准确预测提供了新的可能性。

Abstract: Time series forecasting models have diverse real world applications (e.g.,
from electricity metrics to software workload). Latest foundational models
trained for time series forecasting show strengths (e.g., for long sequences
and in zero-shot settings). However, foundational model was not yet used for
forecasting rare, spiky events, i.e., a challenging target because those are a
corner case of extreme events. In this paper, we optimize a state-of-the-art
foundational model to forecast sporadic or spiky production outages of
high-performance machine learning services powering billions of client devices.
We evaluate the forecasting errors of the foundational model compared with
classical stochastic forecasting models (e.g., moving average and
autoregressive). The analysis helps us understand how each of the evaluated
models performs for the sporadic or spiky events. For example, it identifies
the key patterns in the target data that are well tracked by the foundational
model vs. each of the stochastic models. We use the models with optimal
parameters to estimate a year-long outage statistics of a particular root cause
with less than 6% value errors.

</details>


### [165] [Prediction of Freezing of Gait in Parkinsons Disease using Explainable AI and Federated Deep Learning for Wearable Sensors](https://arxiv.org/abs/2507.01068)
*Biplov Paneru*

Main category: cs.LG

TL;DR: 本研究利用IMU数据集和可解释AI方法，早期检测和预测帕金森病的步态冻结（FOG），并通过集合模型和SHAP分析探索影响因素。


<details>
  <summary>Details</summary>
Motivation: 开发一种基于IMU数据和可解释AI技术的工具，用于早期检测和预测帕金森病的步态冻结，从而改善患者生活质量。

Method: 使用CatBoost、XGBoost和Extra Trees等机器学习分类器，并设计堆叠集成模型。通过SHAP解释分析关键特征，结合联邦学习框架与混合Conv1D+LSTM模型进行预测。

Result: 堆叠集成模型分类准确率接近99%，发现时间特征对步态模式区分最重要。联邦学习框架进一步提升预测性能。

Conclusion: 所提出的方法表现优越，为帕金森病早期干预提供了新的可能性，同时强调了解释性和联邦学习的重要性。

Abstract: This study leverages an Inertial Measurement Unit (IMU) dataset to develop
explainable AI methods for the early detection and prediction of Freezing of
Gait (FOG), a common symptom in Parkinson's disease. Machine learning models,
including CatBoost, XGBoost, and Extra Trees classifiers, are employed to
accurately categorize FOG episodes based on relevant clinical features. A
Stacking Ensemble model achieves superior performance, surpassing a hybrid
bidirectional GRU model and reaching nearly 99% classification accuracy. SHAP
interpretability analysis reveals that time (seconds) is the most influential
factor in distinguishing gait patterns. Additionally, the proposed FOG
prediction framework incorporates federated learning, where models are trained
locally on individual devices and aggregated on a central server using a
federated averaging approach, utilizing a hybrid Conv1D + LSTM architecture for
enhanced predictive capability.

</details>


### [166] [Rotational Sampling: A Plug-and-Play Encoder for Rotation-Invariant 3D Molecular GNNs](https://arxiv.org/abs/2507.01073)
*Dian Jin*

Main category: cs.LG

TL;DR: 本文提出了一种新的插入即用3D编码模块，通过旋转采样计算平均值实现旋转不变性，并通过后对齐策略实现严格的不变性。


<details>
  <summary>Details</summary>
Motivation: 传统图表示难以有效捕捉分子的3D空间结构，导致模型泛化性和鲁棒性受限。现有方法要么依赖过多先验知识，要么计算复杂度高。

Method: 提出一种基于SO(3)旋转组计算期望的3D编码模块，并加入后对齐策略以实现更严格的旋转不变性。

Result: 在QM9和C10数据集上的实验中，所提方法在预测准确性、鲁棒性和泛化性上优于现有方法，同时保有低计算复杂度和更高的可解释性。

Conclusion: 该方法为高效处理3D分子信息提供了新方向，具有潜在应用价值于药物发现与材料设计。

Abstract: Graph neural networks (GNNs) have achieved remarkable success in molecular
property prediction. However, traditional graph representations struggle to
effectively encode the inherent 3D spatial structures of molecules, as
molecular orientations in 3D space introduce significant variability, severely
limiting model generalization and robustness. Existing approaches primarily
focus on rotation-invariant and rotation-equivariant methods. Invariant methods
often rely heavily on prior knowledge and lack sufficient generalizability,
while equivariant methods suffer from high computational costs. To address
these limitations, this paper proposes a novel plug-and-play 3D encoding module
leveraging rotational sampling. By computing the expectation over the SO(3)
rotational group, the method naturally achieves approximate rotational
invariance. Furthermore, by introducing a carefully designed post-alignment
strategy, strict invariance can be achieved without compromising performance.
Experimental evaluations on the QM9 and C10 Datasets demonstrate superior
predictive accuracy, robustness, and generalization performance compared to
existing methods. Moreover, the proposed approach maintains low computational
complexity and enhanced interpretability, providing a promising direction for
efficient and effective handling of 3D molecular information in drug discovery
and material design.

</details>


### [167] [Provenance Tracking in Large-Scale Machine Learning Systems](https://arxiv.org/abs/2507.01075)
*Gabriele Padovani,Valentine Anantharaj,Sandro Fiore*

Main category: cs.LG

TL;DR: 论文提出了一种工具(yProv4ML)，旨在优化大规模AI模型的训练过程，通过收集符合W3C PROV和ProvML标准的溯源数据，提高能量效率和工作流的监控能力。


<details>
  <summary>Details</summary>
Motivation: 随着大规模AI模型需求的增长，训练过程的计算效率、执行时间、准确性及能耗之间的平衡成为一个重大挑战。引入溯源数据可以帮助解决模型训练中的资源利用、效率及可复现性问题。

Method: 提出了一个名为yProv4ML的库，该工具以JSON格式收集W3C PROV和ProvML标准的溯源数据，并支持插件扩展，可与yProv框架完全集成，用于工作流管理系统中的任务。

Result: yProv4ML能够提供灵活性、扩展性和高层级的集成能力，提升模型训练和部署过程的透明性及效率。

Conclusion: 通过yProv4ML库的开发，研究人员和工程师可以优化资源利用，提升大规模AI模型的能效并改进全流程工作流的可监控性。

Abstract: As the demand for large scale AI models continues to grow, the optimization
of their training to balance computational efficiency, execution time, accuracy
and energy consumption represents a critical multidimensional challenge.
Achieving this balance requires not only innovative algorithmic techniques and
hardware architectures but also comprehensive tools for monitoring, analyzing,
and understanding the underlying processes involved in model training and
deployment. Provenance data information about the origins, context, and
transformations of data and processes has become a key component in this
pursuit. By leveraging provenance, researchers and engineers can gain insights
into resource usage patterns, identify inefficiencies, and ensure
reproducibility and accountability in AI development workflows. For this
reason, the question of how distributed resources can be optimally utilized to
scale large AI models in an energy efficient manner is a fundamental one. To
support this effort, we introduce the yProv4ML library, a tool designed to
collect provenance data in JSON format, compliant with the W3C PROV and ProvML
standards. yProv4ML focuses on flexibility and extensibility, and enables users
to integrate additional data collection tools via plugins. The library is fully
integrated with the yProv framework, allowing for higher level pairing in tasks
run also through workflow management systems.

</details>


### [168] [Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning](https://arxiv.org/abs/2507.01196)
*Na Lee,Konstantinos Barmpas,Yannis Panagakis,Dimitrios Adamos,Nikolaos Laskaris,Stefanos Zafeiriou*

Main category: cs.LG

TL;DR: 本文评估了大型脑波基础模型（LBMs）在多种脑机接口基准任务中的表现，发现其与传统深度架构相比，效能提升有限且参数需求巨大。通过参数高效的低秩适配（LoRA），显著减少参数量而性能不损失，从而表明LBMs当前在架构和训练上的效率问题。


<details>
  <summary>Details</summary>
Motivation: 探讨大型脑波基础模型（LBMs）在脑波建模中是否具备显著优势，并评估其在实际任务中的适用性。

Method: 通过系统性微调实验和详尽的消融研究，评估LBMs在脑机接口基准任务中的表现，结合低秩适配（LoRA）减少参数量且保持性能。

Result: 当前LBMs比传统深度架构仅有微小性能提升（提升0.9%-1.2%），但参数量消耗巨大。通过LoRA技术，显著减少模型参数量且性能无显著下降。

Conclusion: LBMs当前面临架构和训练效率的挑战，需设计针对领域的开发策略以充分发挥其潜力。

Abstract: Foundation Models have demonstrated significant success across various
domains in Artificial Intelligence (AI), yet their capabilities for brainwave
modeling remain unclear. In this paper, we comprehensively evaluate current
Large Brainwave Foundation Models (LBMs) through systematic fine-tuning
experiments across multiple Brain-Computer Interface (BCI) benchmark tasks,
including memory tasks and sleep stage classification. Our extensive analysis
shows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%)
over traditional deep architectures while requiring significantly more
parameters (millions vs thousands), raising important questions about their
efficiency and applicability in BCI contexts. Moreover, through detailed
ablation studies and Low-Rank Adaptation (LoRA), we significantly reduce
trainable parameters without performance degradation, while demonstrating that
architectural and training inefficiencies limit LBMs' current capabilities. Our
experiments span both full model fine-tuning and parameter-efficient adaptation
techniques, providing insights into optimal training strategies for BCI
applications. We pioneer the application of LoRA to LBMs, revealing that
performance benefits generally emerge when adapting multiple neural network
components simultaneously. These findings highlight the critical need for
domain-specific development strategies to advance LBMs, suggesting that current
architectures may require redesign to fully leverage the potential of
foundation models in brainwave analysis.

</details>


### [169] [Good Enough to Learn: LLM-based Anomaly Detection in ECU Logs without Reliable Labels](https://arxiv.org/abs/2507.01077)
*Bogdan Bogdan,Arina Cazacu,Laura Vasilie*

Main category: cs.LG

TL;DR: 该研究提出了一种仅基于解码器的大型语言模型（LLM）用于检测汽车电子控制单元（ECU）通信日志中的异常。


<details>
  <summary>Details</summary>
Motivation: 解决当前ECU通信中缺乏专用LLM模型以及不一致的真实数据的问题，同时应对汽车通信环境中规模化和复杂性的挑战。

Method: 通过学习UDP通信日志，提出将异常检测简化为识别与正常行为的时间偏差，并引入熵正则化技术来处理已知异常的不确定性以及类似场景的模型一致性。

Result: 提出了三个创新点：仅解码器的异常检测架构、处理不一致标注的方法，以及适配于不同ECU通信应用场景的LLM模型。

Conclusion: 该方法提供了一种高效的可扩展解决方案，不仅减少了依赖人工标注的高成本和易出错性，还提升了复杂通信环境下的检测精度。

Abstract: Anomaly detection often relies on supervised or clustering approaches, with
limited success in specialized domains like automotive communication systems
where scalable solutions are essential. We propose a novel decoder-only Large
Language Model (LLM) to detect anomalies in Electronic Control Unit (ECU)
communication logs. Our approach addresses two key challenges: the lack of LLMs
tailored for ECU communication and the complexity of inconsistent ground truth
data. By learning from UDP communication logs, we formulate anomaly detection
simply as identifying deviations in time from normal behavior. We introduce an
entropy regularization technique that increases model's uncertainty in known
anomalies while maintaining consistency in similar scenarios. Our solution
offers three novelties: a decoder-only anomaly detection architecture, a way to
handle inconsistent labeling, and an adaptable LLM for different ECU
communication use cases. By leveraging the generative capabilities of
decoder-only models, we present a new technique that addresses the high cost
and error-prone nature of manual labeling through a more scalable system that
is able to learn from a minimal set of examples, while improving detection
accuracy in complex communication environments.

</details>


### [170] [Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW](https://arxiv.org/abs/2507.01241)
*Di Zhang,Yihang Zhang*

Main category: cs.LG

TL;DR: 本文提出一种结合随机共轭次梯度方法和自适应采样的新算法，用于训练大语言模型（LLMs），相较传统随机梯度下降法（SGD）在收敛速度和可扩展性上均有提升。


<details>
  <summary>Details</summary>
Motivation: SGD在训练大语言模型（LLMs）中的有效性受到质疑，特别是在大规模应用中表现出一定的局限性，因此需要一种更高效的方法。

Method: 提出随机共轭次梯度方法结合自适应采样机制，利用样本复杂性分析动态选择样本大小，以AdamW样式算法调整步长，从而应对非凸和非光滑的优化问题。

Result: 实验表明，新方法不仅在许多情况下超过了传统SGD的可扩展性，还显著提高了优化过程的速度和准确性。

Conclusion: 新方法在保持一阶方法优势的同时，有效解决了LLMs训练中的非凸性和非光滑性问题，是传统SGD的强力替代方案。

Abstract: Stochastic gradient-based descent (SGD), have long been central to training
large language models (LLMs). However, their effectiveness is increasingly
being questioned, particularly in large-scale applications where empirical
evidence suggests potential performance limitations. In response, this paper
proposes a stochastic conjugate subgradient method together with adaptive
sampling tailored specifically for training LLMs. The method not only achieves
faster convergence per iteration but also demonstrates improved scalability
compared to traditional SGD techniques. It leverages sample complexity analysis
to adaptively choose the sample size, employs a stochastic conjugate
subgradient approach to determine search directions and utilizing an AdamW-like
algorithm to adaptively adjust step sizes. This approach preserves the key
advantages of first-order methods while effectively addressing the nonconvexity
and non-smoothness inherent in LLMs training. Additionally, we provide a
detailed analysis of the advantage of the algorithm. Experimental results show
that the proposed method not only maintains, but in many cases surpasses, the
scalability of traditional SGD techniques, significantly enhancing both the
speed and accuracy of the optimization process.

</details>


### [171] [yProv4ML: Effortless Provenance Tracking for Machine Learning Systems](https://arxiv.org/abs/2507.01078)
*Gabriele Padovani,Valentine Anantharaj,Sandro Fiore*

Main category: cs.LG

TL;DR: 本文提出yProv4ML框架，可在最少代码修改下，以PROV-JSON格式捕获机器学习中的谱系信息。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型开发中的透明性和严谨性不足，例如无法提前确定训练参数，影响了模型选择和开发的可靠性。

Method: 提出了yProv4ML框架，提供了一种在机器学习过程中捕获数据谱系信息的手段，采用PROV-JSON格式且最小化开发代码的改动。

Result: yProv4ML框架有效实现了机器学习过程中的谱系信息捕获，弥补了现有框架在数据格式和谱系关注方面的不足。

Conclusion: 该框架有助于改进大型语言模型开发中的透明性和数据管控，有潜力促进相关技术规范化。

Abstract: The rapid growth of interest in large language models (LLMs) reflects their
potential for flexibility and generalization, and attracted the attention of a
diverse range of researchers. However, the advent of these techniques has also
brought to light the lack of transparency and rigor with which development is
pursued. In particular, the inability to determine the number of epochs and
other hyperparameters in advance presents challenges in identifying the best
model. To address this challenge, machine learning frameworks such as MLFlow
can automate the collection of this type of information. However, these tools
capture data using proprietary formats and pose little attention to lineage.
This paper proposes yProv4ML, a framework to capture provenance information
generated during machine learning processes in PROV-JSON format, with minimal
code modifications.

</details>


### [172] [Development and Comparative Evaluation of Three Artificial Intelligence Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A 7-Month Retrospective Proof-of-Concept](https://arxiv.org/abs/2507.01080)
*Edouard Lansiaux,Ramy Azzouz,Emmanuel Chazard,Amélie Vromant,Eric Wiel*

Main category: cs.LG

TL;DR: 本研究评估了三种人工智能模型（NLP、LLM 和 JEPA）在预测急诊分诊结果方面的表现。


<details>
  <summary>Details</summary>
Motivation: 解决急诊室中持久存在的分诊错误问题（包括低分诊和过度分诊），并探讨 AI 技术能否改善分诊效率和安全性。

Method: 对一组 7 个月内在法国里尔 Roger Salengro 医院急诊科收集的成人患者分诊数据进行回顾性分析，训练并验证三种 AI 模型，使用多种指标评估它们的预测准确性。

Result: LLM（URGENTIAPARSE）模型准确性最高，表现优于 JEPA（EMERGINET）和 NLP（TRIAGEMASTER），并超过护士分诊水平。

Conclusion: LLM 架构能够更精准地进行分诊预测，AI 若全面融入急诊流程，可提升患者安全及运营效率，但需解决模型局限性并确保伦理透明。

Abstract: Triage errors, including undertriage and overtriage, are persistent
challenges in emergency departments (EDs). With increasing patient influx and
staff shortages, the integration of artificial intelligence (AI) into triage
protocols has gained attention. This study compares the performance of three AI
models [Natural Language Processing (NLP), Large Language Models (LLM), and
Joint Embedding Predictive Architecture (JEPA)] in predicting triage outcomes
against the FRENCH scale and clinical practice.We conducted a retrospective
analysis of a prospectively recruited cohort gathering adult patient triage
data over a 7-month period at the Roger Salengro Hospital ED (Lille, France).
Three AI models were trained and validated : (1) TRIAGEMASTER (NLP), (2)
URGENTIAPARSE (LLM), and (3) EMERGINET (JEPA). Data included demographic
details, verbatim chief complaints, vital signs, and triage outcomes based on
the FRENCH scale and GEMSA coding. The primary outcome was the concordance of
AI-predicted triage level with the FRENCH gold-standard. It was assessed thanks
to various indicators : F1-Score, Weighted Kappa, Spearman, MAE, RMSE. The LLM
model (URGENTIAPARSE) showed higher accuracy (composite score: 2.514) compared
to JEPA (EMERGINET, 0.438) and NLP (TRIAGEMASTER, -3.511), outperforming nurse
triage (-4.343). Secondary analyses highlighted the effectiveness of
URGENTIAPARSE in predicting hospitalization needs (GEMSA) and its robustness
with structured data versus raw transcripts (either for GEMSA prediction or for
FRENCH prediction). LLM architecture, through abstraction of patient
representations, offers the most accurate triage predictions among tested
models. Integrating AI into ED workflows could enhance patient safety and
operational efficiency, though integration into clinical workflows requires
addressing model limitations and ensuring ethical transparency.

</details>


### [173] [PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning](https://arxiv.org/abs/2507.01271)
*Tatsuki Kawakami,Kazuki Egashira,Atsuyuki Miyai,Go Irie,Kiyoharu Aizawa*

Main category: cs.LG

TL;DR: 本文提出了PULSE协议，用于评估大规模多模态模型（LMMs）中的现实遗忘场景，包括对预训练知识的遗忘和长期可持续性评价。结果表明，现有方法在遗忘微调知识上有效，但对遗忘预训练知识和处理连续性请求表现不足。


<details>
  <summary>Details</summary>
Motivation: 近年来，大规模语言和多模态模型的隐私及版权问题日益突出，因此需要开发有效的遗忘技术，使模型能够“遗忘”特定学习内容。

Method: 提出了PULSE协议，分两个维度评估遗忘方法：一是对不同知识获取阶段的遗忘效果分析，二是对序列请求的长期可持续性评价。同时对现有遗忘方法在这些维度上进行测试。

Result: 研究表明，现有技术虽然可以有效地遗忘通过微调获得的知识，但在删除预训练阶段学习的信息和连续遗忘操作时性能显著下降。

Conclusion: 目前LMMs中的遗忘技术对现实应用存在局限，特别是在预训练信息遗忘和处理多次分段遗忘请求方面，需要进一步研究和改进。

Abstract: In recent years, unlearning techniques, which are methods for inducing a
model to "forget" previously learned information, have attracted attention as a
way to address privacy and copyright concerns in large language models (LLMs)
and large multimodal models (LMMs). While several unlearning benchmarks have
been established for LLMs, a practical evaluation framework for unlearning in
LMMs has been less explored. Specifically, existing unlearning benchmark for
LMMs considers only scenarios in which the model is required to unlearn
fine-tuned knowledge through a single unlearning operation. In this study, we
introduce PULSE protocol for realistic unlearning scenarios for LMMs by
introducing two critical perspectives: (i) Pre-trained knowledge Unlearning for
analyzing the effect across different knowledge acquisition phases and (ii)
Long-term Sustainability Evaluation to address sequential requests. We then
evaluate existing unlearning methods along these dimensions. Our results reveal
that, although some techniques can successfully unlearn knowledge acquired
through fine-tuning, they struggle to eliminate information learned during
pre-training. Moreover, methods that effectively unlearn a batch of target data
in a single operation exhibit substantial performance degradation when the same
data are split and unlearned sequentially.

</details>


### [174] [Proof of a perfect platonic representation hypothesis](https://arxiv.org/abs/2507.01098)
*Liu Ziyin,Isaac Chuang*

Main category: cs.LG

TL;DR: 本文解释了 Ziyin 等人关于嵌入式深度线性网络（EDLN）完美柏拉图表示假设的证明，指出通过 SGD 训练，EDLN 可收敛到完美柏拉图解。


<details>
  <summary>Details</summary>
Motivation: 探究为何通过 SGD 训练的网络会自发产生完美的柏拉图表示，并理解其中的潜在机制。

Method: 通过数学证明和理论分析展示 SGD 如何导致 EDLN 达到完美柏拉图状态，并探讨其潜在的六种破裂方式。

Result: 发现虽然大部分损失函数的全局最小值并非柏拉图解，但 SGD 仍趋向找到完美柏拉图解，且柏拉图表示的出现与渐进式增强现象一样，源于 SGD 的不可逆性。

Conclusion: 研究揭示了 SGD 中的熵力在表征学习中的关键作用，深化了对网络自发涌现表征现象的理解，并提供多个破裂点的探索方向。

Abstract: In this note, we elaborate on and explain in detail the proof given by Ziyin
et al. (2025) of the "perfect" Platonic Representation Hypothesis (PRH) for the
embedded deep linear network model (EDLN). We show that if trained with SGD,
two EDLNs with different widths and depths and trained on different data will
become Perfectly Platonic, meaning that every possible pair of layers will
learn the same representation up to a rotation. Because most of the global
minima of the loss function are not Platonic, that SGD only finds the perfectly
Platonic solution is rather extraordinary. The proof also suggests at least six
ways the PRH can be broken. We also show that in the EDLN model, the emergence
of the Platonic representations is due to the same reason as the emergence of
progressive sharpening. This implies that these two seemingly unrelated
phenomena in deep learning can, surprisingly, have a common cause. Overall, the
theory and proof highlight the importance of understanding emergent "entropic
forces" due to the irreversibility of SGD training and their role in
representation learning. The goal of this note is to be instructive and avoid
lengthy technical details.

</details>


### [175] [A Neural Operator based on Dynamic Mode Decomposition](https://arxiv.org/abs/2507.01117)
*Nikita Sakovich,Dmitry Aksenov,Ekaterina Pleshakova,Sergey Gataullin*

Main category: cs.LG

TL;DR: 本研究提出了一个基于动态模态分解（DMD）算法的神经算子，结合深度学习，有效建模时空过程，能够在求解偏微分方程时减少计算资源需求，并在多个方程的近似解中表现出高精度。


<details>
  <summary>Details</summary>
Motivation: 在人工智能领域，科学计算方法的开发仍然是一个热门议题，研究希望在计算轻量化和高精度之间寻找平衡，提升偏微分方程求解的效率。

Method: 通过动态模态分解（DMD）算法与深度学习的结合，提出了一种新型神经算子，用来高效建模时空过程，自动提取模态及了解系统动力学，减少了对传统数值计算方法的高计算成本依赖。

Result: 与现有类似方法（如DeepONet和FNO）进行性能比较分析，在处理热方程、拉普拉斯方程和Burgers方程的解时，该方法表现出了较高的重建精度，同时显著降低了计算成本。

Conclusion: 基于动态模态分解的神经算子实现了轻量化又高精度的科学计算，对时空过程建模和偏微分方程求解具有显著的应用优势和潜力。

Abstract: The scientific computation methods development in conjunction with artificial
intelligence technologies remains a hot research topic. Finding a balance
between lightweight and accurate computations is a solid foundation for this
direction. The study presents a neural operator based on the dynamic mode
decomposition algorithm (DMD), mapping functional spaces, which combines DMD
and deep learning (DL) for spatiotemporal processes efficient modeling. Solving
PDEs for various initial and boundary conditions requires significant
computational resources. The method suggested automatically extracts key modes
and system dynamics using them to construct predictions, reducing computational
costs compared to traditional numerical methods. The approach has demonstrated
its efficiency through comparative analysis of performance with closest
analogues DeepONet and FNO in the heat equation, Laplaces equation, and Burgers
equation solutions approximation, where it achieves high reconstruction
accuracy.

</details>


### [176] [Neural Hamiltonian Operator](https://arxiv.org/abs/2507.01313)
*Qian Qi*

Main category: cs.LG

TL;DR: 本文提出了一种利用深度学习求解高维随机控制问题的新框架，称为神经哈密顿算子（NHO）。


<details>
  <summary>Details</summary>
Motivation: 由于维度灾难，高维随机控制问题难以解决，本文旨在提出一种新的理论框架，结合深度学习和Pontryagin最大化原理，以提升求解能力。

Method: 定义了一种神经哈密顿算子（NHO），通过神经网络表示FBSDE（正逆随机微分方程）的耦合动力学，包括反馈控制和价值函数梯度。然后通过训练网络来满足Pontryagin最大化原理的约束条件，从而找到最优算子。

Result: 提出的NHO方法被数学化为一种从模拟数据中学习未知算子的问题，并证明了其在一般鞅驱动条件下的通用近似能力。

Conclusion: 通过运算符理论视角，NHO框架既具有数学严谨性，也为高维随机控制问题优化提供了新思路。

Abstract: Stochastic control problems in high dimensions are notoriously difficult to
solve due to the curse of dimensionality. An alternative to traditional dynamic
programming is Pontryagin's Maximum Principle (PMP), which recasts the problem
as a system of Forward-Backward Stochastic Differential Equations (FBSDEs). In
this paper, we introduce a formal framework for solving such problems with deep
learning by defining a \textbf{Neural Hamiltonian Operator (NHO)}. This
operator parameterizes the coupled FBSDE dynamics via neural networks that
represent the feedback control and an ansatz for the value function's spatial
gradient. We show how the optimal NHO can be found by training the underlying
networks to enforce the consistency conditions dictated by the PMP. By adopting
this operator-theoretic view, we situate the deep FBSDE method within the
rigorous language of statistical inference, framing it as a problem of learning
an unknown operator from simulated data. This perspective allows us to prove
the universal approximation capabilities of NHOs under general martingale
drivers and provides a clear lens for analyzing the significant optimization
challenges inherent to this class of models.

</details>


### [177] [On Design Principles for Private Adaptive Optimizers](https://arxiv.org/abs/2507.01129)
*Arun Ganesh,Brendan McMahan,Abhradeep Thakurta*

Main category: cs.LG

TL;DR: 探讨微分隐私训练中噪声对自适应优化器的影响，并比较几种算法的理论和实践表现，发现一种名为“scale-then-privatize”的技术表现更优。


<details>
  <summary>Details</summary>
Motivation: 微分隐私训练中的噪声会降低自适应优化器的性能，因此需要更好的方法来改进这种优化器的性能。

Method: 对多种优化器变体进行了理论分析和实验比较，专注于一个小规模的语言模型训练任务，以研究不同方法的效果。

Result: 发现“scale-then-privatize”技术理论表现较好，噪声分布更接近相关噪声机制，且在实验中取得最佳效果。

Conclusion: 改进自适应优化器应关注与相关噪声匹配的方法，而不是追求无偏估计；“scale-then-privatize”在理论和实践上更具吸引力。

Abstract: The spherical noise added to gradients in differentially private (DP)
training undermines the performance of adaptive optimizers like AdaGrad and
Adam, and hence many recent works have proposed algorithms to address this
challenge. However, the empirical results in these works focus on simple tasks
and models and the conclusions may not generalize to model training in
practice. In this paper we survey several of these variants, and develop better
theoretical intuition for them as well as perform empirical studies comparing
them. We find that a common intuition of aiming for unbiased estimates of
second moments of gradients in adaptive optimizers is misguided, and instead
that a simple technique called scale-then-privatize (which does not achieve
unbiased second moments) has more desirable theoretical behaviors and
outperforms all other variants we study on a small-scale language model
training task. We additionally argue that scale-then-privatize causes the noise
addition to better match the application of correlated noise mechanisms which
are more desirable to use in practice.

</details>


### [178] [ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks](https://arxiv.org/abs/2507.01321)
*Zhiyao Ren,Siyuan Liang,Aishan Liu,Dacheng Tao*

Main category: cs.LG

TL;DR: 该论文提出了ICLShield，一种防御大语言模型在ICL中受到后门攻击的方法，通过动态调整概念偏好比率，显著降低了模型的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在ICL中表现出色，但其易受到后门攻击，这可能严重影响模型的可靠性与安全性，呼吁一种有效的防御机制。

Method: 提出一种基于双重学习假设的防御方法ICLShield，通过调整概念偏好比率，并利用置信度与相似性评分，动态选择清洁的示例来减轻后门攻击风险。

Result: ICLShield在多个LLM和任务上展示出卓越的防御效果，平均提升了26.02%，并且对包括GPT-4在内的闭源模型也表现出色的适应能力与防护能力。

Conclusion: ICLShield为防御LLM中ICL后门攻击提供了一个高效且适应性强的解决方案，对增强模型的安全性和应用价值具有重要意义。

Abstract: In-context learning (ICL) has demonstrated remarkable success in large
language models (LLMs) due to its adaptability and parameter-free nature.
However, it also introduces a critical vulnerability to backdoor attacks, where
adversaries can manipulate LLM behaviors by simply poisoning a few ICL
demonstrations. In this paper, we propose, for the first time, the
dual-learning hypothesis, which posits that LLMs simultaneously learn both the
task-relevant latent concepts and backdoor latent concepts within poisoned
demonstrations, jointly influencing the probability of model outputs. Through
theoretical analysis, we derive an upper bound for ICL backdoor effects,
revealing that the vulnerability is dominated by the concept preference ratio
between the task and the backdoor. Motivated by these findings, we propose
ICLShield, a defense mechanism that dynamically adjusts the concept preference
ratio. Our method encourages LLMs to select clean demonstrations during the ICL
phase by leveraging confidence and similarity scores, effectively mitigating
susceptibility to backdoor attacks. Extensive experiments across multiple LLMs
and tasks demonstrate that our method achieves state-of-the-art defense
effectiveness, significantly outperforming existing approaches (+26.02% on
average). Furthermore, our method exhibits exceptional adaptability and
defensive performance even for closed-source models (e.g., GPT-4).

</details>


### [179] [Tensor Decomposition Networks for Fast Machine Learning Interatomic Potential Computations](https://arxiv.org/abs/2507.01131)
*Yuchao Lin,Cong Fu,Zachary Krueger,Haiyang Yu,Maho Nakata,Jianwen Xie,Emine Kucukbenli,Xiaofeng Qian,Shuiwang Ji*

Main category: cs.LG

TL;DR: 本文提出了张量分解网络（TDNs），通过低秩张量分解如CP分解加速$ms{SO}(3)$等变网络的计算，同时保证高效性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的$ms{SO}(3)$等变网络由于Clebsch-Gordan张量积的计算成本高而受到限制。研究的目的在于减少计算复杂度，同时保持等变性质。

Method: 提出TDNs，其中利用CANDECOMP/PARAFAC (CP)分解代替Clebsch-Gordan张量积，并设计了路径权重共享机制以进一步减少参数量。

Result: 实验表明，TDNs在PubChemQCR、OC20和OC22数据集上达到了竞争性能，同时在计算性能上有显著提升，尤其计算复杂度从$O(L^6)$减少到$O(L^4)$。

Conclusion: TDNs在保持等变精度的同时大幅提高了计算效率，可作为现有网络的高效替代。

Abstract: $\rm{SO}(3)$-equivariant networks are the dominant models for machine
learning interatomic potentials (MLIPs). The key operation of such networks is
the Clebsch-Gordan (CG) tensor product, which is computationally expensive. To
accelerate the computation, we develop tensor decomposition networks (TDNs) as
a class of approximately equivariant networks whose CG tensor products are
replaced by low-rank tensor decompositions, such as the CANDECOMP/PARAFAC (CP)
decomposition. With the CP decomposition, we prove (i) a uniform bound on the
induced error of $\rm{SO}(3)$-equivariance, and (ii) the universality of
approximating any equivariant bilinear map. To further reduce the number of
parameters, we propose path-weight sharing that ties all multiplicity-space
weights across the $O(L^3)$ CG paths into a single path without compromising
equivariance, where $L$ is the maximum angular degree. The resulting layer acts
as a plug-and-play replacement for tensor products in existing networks, and
the computational complexity of tensor products is reduced from $O(L^6)$ to
$O(L^4)$. We evaluate TDNs on PubChemQCR, a newly curated molecular relaxation
dataset containing 105 million DFT-calculated snapshots. We also use existing
datasets, including OC20, and OC22. Results show that TDNs achieve competitive
performance with dramatic speedup in computations.

</details>


### [180] [Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy](https://arxiv.org/abs/2507.01327)
*Xiaoyun Zhang,Jingqing Ruan,Xing Ma,Yawen Zhu,Jiansong Chen,Ke Zeng,Xunliang Cai*

Main category: cs.LG

TL;DR: 提出一种用于异常事件检测的新型框架(APARL)，通过提升模型对复杂对话数据的适应性和鲁棒性，实现高效的跨领域一般化能力。


<details>
  <summary>Details</summary>
Motivation: 旨在解决客服对话异常检测中因业务数据复杂性和客户交互动态性导致的困难，并实现强大的跨领域适应能力，以提升商业价值。

Method: 提出了APARL框架，采用大语言模型的推理能力，通过双循环动态课程学习架构逐步关注更具挑战性的样本，以优化性能并提升模型的跨领域可迁移性。

Result: 在外卖对话任务中的广泛评估中，模型在适配性和鲁棒性方面表现优异，平均F1分数提升17.19%，在跨领域测试中的平均性能提升9.59%。

Conclusion: 该方法为工业中的异常检测模型提供了优越的解决方案，有助于提高运营效率和实现商业收益。

Abstract: Detecting abnormal events in real-world customer service dialogues is highly
challenging due to the complexity of business data and the dynamic nature of
customer interactions. Moreover, models must demonstrate strong out-of-domain
(OOD) generalization to enable rapid adaptation across different business
scenarios and maximize commercial value. In this work, we propose a novel
Adaptive Perplexity-Aware Reinforcement Learning (APARL) framework that
leverages the advanced reasoning capabilities of large language models for
abnormal event detection. APARL introduces a dual-loop dynamic curriculum
learning architecture, enabling the model to progressively focus on more
challenging samples as its proficiency increases. This design effectively
addresses performance bottlenecks and significantly enhances OOD
transferability. Extensive evaluations on food delivery dialogue tasks show
that our model achieves significantly enhanced adaptability and robustness,
attaining the highest F1 score with an average improvement of 17.19\%, and an
average improvement of 9.59\% in OOD transfer tests. This method provides a
superior solution for industrial deployment of anomaly detection models,
contributing to improved operational efficiency and commercial benefits.

</details>


### [181] [Spectral Manifold Harmonization for Graph Imbalanced Regression](https://arxiv.org/abs/2507.01132)
*Brenda Nogueira,Gabe Gomes,Meng Jiang,Nitesh V. Chawla,Nuno Moniz*

Main category: cs.LG

TL;DR: 提出了一种名为光谱流形协调（SMH）的新方法，解决图结构数据中不平衡回归问题，重点优化欠代表目标区间的样本生成。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了图结构数据的不平衡回归问题，领域偏好要求关注特定的目标值范围，这些范围通常具有更高的科学价值。

Method: 提出了SMH，通过生成保持拓扑特性的合成图样本，特别关注欠代表的目标分布区域。

Result: 实验结果表明，在用于化学与药物发现的测试数据集上，SMH方法在预测性能上有一致性提升，特别是在目标领域范围内。

Conclusion: SMH能够解决传统方法忽视图拓扑结构以及域内特定目标值偏好导致的问题，展现出改进预测性能的潜力。

Abstract: Graph-structured data is ubiquitous in scientific domains, where models often
face imbalanced learning settings. In imbalanced regression, domain preferences
focus on specific target value ranges representing the most scientifically
valuable cases; we observe a significant lack of research. In this paper, we
present Spectral Manifold Harmonization (SMH), a novel approach for addressing
this imbalanced regression challenge on graph-structured data by generating
synthetic graph samples that preserve topological properties while focusing on
often underrepresented target distribution regions. Conventional methods fail
in this context because they either ignore graph topology in case generation or
do not target specific domain ranges, resulting in models biased toward average
target values. Experimental results demonstrate the potential of SMH on
chemistry and drug discovery benchmark datasets, showing consistent
improvements in predictive performance for target domain ranges.

</details>


### [182] [FlashDP: Private Training Large Language Models with Efficient DP-SGD](https://arxiv.org/abs/2507.01154)
*Liangyu Wang,Junxiao Wang,Jie Ren,Zihang Xiang,David E. Keyes,Di Wang*

Main category: cs.LG

TL;DR: FlashDP通过融合计算的创新方式提升了大语言模型在差分隐私训练中的效率，减小了内存需求并保持高效能和准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的训练数据隐私需求与差分隐私（DP）机制结合相关挑战是研究动机，当前方法存在存储和计算效率方面的问题。

Method: 提出了FlashDP，通过每层融合计算的DP-SGD算法，只计算一次梯度，大幅减少内存移动和冗余计算。

Result: 相比现有显式与隐式方法减少高达50%的内存移动及20%的冗余计算，且在四块A100系统上达到90%非差分隐私方法的吞吐量，同时保证模型精度。

Conclusion: FlashDP是一种高效且隐私保护性良好的DP-SGD方法，其创新性设计显著改善了训练大语言模型的效率和资源利用。

Abstract: As large language models (LLMs) increasingly underpin technological
advancements, the privacy of their training data emerges as a critical concern.
Differential Privacy (DP) serves as a rigorous mechanism to protect this data,
yet its integration via Differentially Private Stochastic Gradient Descent
(DP-SGD) introduces substantial challenges, primarily due to the complexities
of per-sample gradient clipping. Current explicit methods, such as Opacus,
necessitate extensive storage for per-sample gradients, significantly inflating
memory requirements. Conversely, implicit methods like GhostClip reduce storage
needs by recalculating gradients multiple times, which leads to inefficiencies
due to redundant computations. This paper introduces FlashDP, an innovative
cache-friendly per-layer DP-SGD that consolidates necessary operations into a
single task, calculating gradients only once in a fused manner. This approach
not only diminishes memory movement by up to \textbf{50\%} but also cuts down
redundant computations by \textbf{20\%}, compared to previous methods.
Consequently, FlashDP does not increase memory demands and achieves a
\textbf{90\%} throughput compared to the Non-DP method on a four-A100 system
during the pre-training of the Llama-13B model, while maintaining parity with
standard per-layer clipped DP-SGD in terms of accuracy. These advancements
establish FlashDP as a pivotal development for efficient and privacy-preserving
training of LLMs. FlashDP's code has been open-sourced in
https://github.com/kaustpradalab/flashdp.

</details>


### [183] [Diffusion Explorer: Interactive Exploration of Diffusion Models](https://arxiv.org/abs/2507.01178)
*Alec Helbling,Duen Horng Chau*

Main category: cs.LG

TL;DR: 本文介绍了一个名为Diffusion Explorer的互动工具，用于解释扩散模型的几何属性。


<details>
  <summary>Details</summary>
Motivation: 现有关于扩散模型的资源要么过于理论化，要么集中在神经网络结构，而忽略其几何特性，作者希望填补这一空白。

Method: 通过一个交互式动画工具，用户可以在浏览器中训练2D扩散模型，并观察其时间动态采样过程。

Result: 提供了一个可视化互动工具，使用户更好地理解扩散模型的几何及动态特性。

Conclusion: 此工具为动态系统可视化提供了新的可能性，并帮助用户深入了解扩散模型的特性。

Abstract: Diffusion models have been central to the development of recent image, video,
and even text generation systems. They posses striking geometric properties
that can be faithfully portrayed in low-dimensional settings. However, existing
resources for explaining diffusion either require an advanced theoretical
foundation or focus on their neural network architectures rather than their
rich geometric properties. We introduce Diffusion Explorer, an interactive tool
to explain the geometric properties of diffusion models. Users can train 2D
diffusion models in the browser and observe the temporal dynamics of their
sampling process. Diffusion Explorer leverages interactive animation, which has
been shown to be a powerful tool for making engaging visualizations of dynamic
systems, making it well suited to explaining diffusion models which represent
stochastic processes that evolve over time. Diffusion Explorer is open source
and a live demo is available at alechelbling.com/Diffusion-Explorer.

</details>


### [184] [Distributional Soft Actor-Critic with Diffusion Policy](https://arxiv.org/abs/2507.01381)
*Tong Liu,Yinuo Wang,Xujie Song,Wenjun Zou,Liangfa Chen,Likun Wang,Bin Shuai,Jingliang Duan,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 提出了一种名为DSAC-D的分布式强化学习算法，通过引入扩散模型和多模态分布解决值函数估计偏差问题，显著提升了算法性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习常用单模态分布建模，但易导致值函数估计偏差，从而影响算法表现。

Method: 引入策略熵和值分布函数，建立多模态分布的策略迭代框架；使用扩散模型生成奖励样本，构建多峰分布的扩散值网络；设计具有值网络和策略网络双扩散特性的强化学习算法。

Result: 在MuJoCo测试任务中，DSAC-D在9个控制任务中达到SOTA性能，总平均回报率提升超10%；在实际车辆测试中显示了精准描述多模态驾驶分布的能力。

Conclusion: DSAC-D能够显著抑制估计偏差，提升强化学习算法性能，并且很好地刻画多模态策略分布。

Abstract: Reinforcement learning has been proven to be highly effective in handling
complex control tasks. Traditional methods typically use unimodal
distributions, such as Gaussian distributions, to model the output of value
distributions. However, unimodal distribution often and easily causes bias in
value function estimation, leading to poor algorithm performance. This paper
proposes a distributional reinforcement learning algorithm called DSAC-D
(Distributed Soft Actor Critic with Diffusion Policy) to address the challenges
of estimating bias in value functions and obtaining multimodal policy
representations. A multimodal distributional policy iteration framework that
can converge to the optimal policy was established by introducing policy
entropy and value distribution function. A diffusion value network that can
accurately characterize the distribution of multi peaks was constructed by
generating a set of reward samples through reverse sampling using a diffusion
model. Based on this, a distributional reinforcement learning algorithm with
dual diffusion of the value network and the policy network was derived. MuJoCo
testing tasks demonstrate that the proposed algorithm not only learns
multimodal policy, but also achieves state-of-the-art (SOTA) performance in all
9 control tasks, with significant suppression of estimation bias and total
average return improvement of over 10\% compared to existing mainstream
algorithms. The results of real vehicle testing show that DSAC-D can accurately
characterize the multimodal distribution of different driving styles, and the
diffusion policy network can characterize multimodal trajectories.

</details>


### [185] [Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models](https://arxiv.org/abs/2507.01201)
*Hyoseo,Yoon,Yisong Yue,Been Kim*

Main category: cs.LG

TL;DR: 文章提出通过优化独立训练的视觉和语言模型之间的对齐问题，并引入联合自动编码器调节器（Joint Autoencoder Modulator, JAM）框架达成目标。


<details>
  <summary>Details</summary>
Motivation: 探索独立训练的视觉和语言模型是否可能在共享的现实统计模型上达成一致，并提出将其显式优化，旨在提升单模态基础模型的多模态对齐潜力。

Method: 提出JAM框架，通过对单模态模型的潜在表征进行联合训练，利用重建和跨模态目标来促进对齐，同时开发并评估对比损失、负对比损失和Spread损失的表现。

Result: 实验表明，即使在冻结的、独立训练的表征之间，JAM框架也能实现高效对齐，且验证了模型层深度和规模对对齐结果的影响。

Conclusion: JAM框架不仅提供了理论见解，还为单模态模型转变为多模态模型提供了一条高效的实际路径。

Abstract: Independently trained vision and language models inhabit disjoint
representational spaces, shaped by their respective modalities, objectives, and
architectures. Yet an emerging hypothesis - the Platonic Representation
Hypothesis - suggests that such models may nonetheless converge toward a shared
statistical model of reality. This compatibility, if it exists, raises a
fundamental question: can we move beyond post-hoc statistical detection of
alignment and explicitly optimize for it between such disjoint representations?
We cast this Platonic alignment problem as a multi-objective optimization task
- preserve each modality's native structure while aligning for mutual
coherence. We introduce the Joint Autoencoder Modulator (JAM) framework that
jointly trains modality-specific autoencoders on the latent representations of
pre-trained single modality models, encouraging alignment through both
reconstruction and cross-modal objectives. By analogy, this framework serves as
a method to escape Plato's Cave, enabling the emergence of shared structure
from disjoint inputs. We evaluate this framework across three critical design
axes: (i) the alignment objective - comparing contrastive loss (Con), its
hard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at
which alignment is most effective, and (iii) the impact of foundation model
scale on representational convergence. Our findings show that our lightweight
Pareto-efficient framework reliably induces alignment, even across frozen,
independently trained representations, offering both theoretical insight and
practical pathways for transforming generalist unimodal foundations into
specialist multimodal models.

</details>


### [186] [Deep Learning-Based Intrusion Detection for Automotive Ethernet: Evaluating & Optimizing Fast Inference Techniques for Deployment on Low-Cost Platform](https://arxiv.org/abs/2507.01208)
*Pedro R. X. Carmo,Igor de Moura,Assis T. de Oliveira Filho,Djamel Sadok,Cleber Zanchettin*

Main category: cs.LG

TL;DR: 本文研究提出使用快速神经网络推理技术（如蒸馏和剪枝）来部署基于深度学习的入侵检测系统（IDS），并展示了在低成本平台（如Raspberry Pi 4）上的实时性能。


<details>
  <summary>Details</summary>
Motivation: 随着现代车辆的连接性增强和车内通信需求的增长，汽车以太网基础设施的安全性显得尤为重要。然而，目前的入侵检测系统需要昂贵的硬件支持，因此亟需更具成本效益的解决方案。

Method: 采用压缩神经网络模型的优化方法，包括蒸馏和剪枝，将深度学习IDS模型部署到低成本硬件平台上，并评估其实时入侵检测性能。

Result: 实验结果表明，在Raspberry Pi 4上可以达到727微秒的入侵检测时间，且AUC-ROC值高达0.9890。

Conclusion: 快速神经网络推理技术在低成本平台部署IDS是可行的，不仅能有效检测入侵，还实现了高性价比和良好的实时性能。

Abstract: Modern vehicles are increasingly connected, and in this context, automotive
Ethernet is one of the technologies that promise to provide the necessary
infrastructure for intra-vehicle communication. However, these systems are
subject to attacks that can compromise safety, including flow injection
attacks. Deep Learning-based Intrusion Detection Systems (IDS) are often
designed to combat this problem, but they require expensive hardware to run in
real time. In this work, we propose to evaluate and apply fast neural network
inference techniques like Distilling and Prunning for deploying IDS models on
low-cost platforms in real time. The results show that these techniques can
achieve intrusion detection times of up to 727 {\mu}s using a Raspberry Pi 4,
with AUCROC values of 0.9890.

</details>


### [187] [Tensor Program Optimization for the RISC-V Vector Extension Using Probabilistic Programs](https://arxiv.org/abs/2507.01457)
*Federico Nicolas Peccia,Frederik Haxel,Oliver Bringmann*

Main category: cs.LG

TL;DR: 本文提出了基于TVM编译器的工作流程，用于高效地将AI工作负载映射到RISC-V的向量单元。其方案显著提升了执行延迟和代码内存占用，且已开源供社区扩展。


<details>
  <summary>Details</summary>
Motivation: RISC-V的RVV扩展为AI工作负载的加速提供了可能，但编写高效利用其向量单元的软件需要专业知识，而现有工具如编译器自动矢量化或手工制作的库存在一定局限性。缺乏可与RVV扩展集成的自动调优框架限制了复杂AI工作负载的高效部署。

Method: 提出一种基于TVM的工作流程，将RVV扩展集成到TVM的MetaSchedule框架中，实现对张量操作的调优。通过在FPGA上实现不同的RISC-V SoC并调优一系列AI工作负载来验证方案性能。

Result: 与GCC的自动矢量化相比，执行延迟平均改善46%；与muRISCV-NN相比提升29%；生成的二进制文件拥有更小的代码内存占用。在商用RISC-V SoC上的测试表明，映射结果平均快于LLVM的方案35%。

Conclusion: 方案有效提升了AI工作负载在RISC-V上的执行效率，并通过开源推动社区进一步优化和扩展其他RISC-V扩展的支持。

Abstract: RISC-V provides a flexible and scalable platform for applications ranging
from embedded devices to high-performance computing clusters. Particularly, its
RISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI
workloads. But writing software that efficiently utilizes the vector units of
RISC-V CPUs without expert knowledge requires the programmer to rely on the
autovectorization features of compilers or hand-crafted libraries like
muRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing
the integration with the RISC-V RVV extension, thus heavily limiting the
efficient deployment of complex AI workloads. In this paper, we present a
workflow based on the TVM compiler to efficiently map AI workloads onto RISC-V
vector units. Instead of relying on hand-crafted libraries, we integrated the
RVV extension into TVM's MetaSchedule framework, a probabilistic program
framework for tensor operation tuning. We implemented different RISC-V SoCs on
an FPGA and tuned a wide range of AI workloads on them. We found that our
proposal shows a mean improvement of 46% in execution latency when compared
against the autovectorization feature of GCC, and 29% against muRISCV-NN.
Moreover, the binary resulting from our proposal has a smaller code memory
footprint, making it more suitable for embedded devices. Finally, we also
evaluated our solution on a commercially available RISC-V SoC implementing the
RVV 1.0 Vector Extension and found our solution is able to find mappings that
are 35% faster on average than the ones proposed by LLVM. We open-sourced our
proposal for the community to expand it to target other RISC-V extensions.

</details>


### [188] [PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile Device via Additive Side-Tuning](https://arxiv.org/abs/2507.01216)
*Xingke Yang,Liang Li,Zhiyi Wan,Sicong Li,Hao Wang,Xiaoqi Qi,Jiang Liu,Tomoaki Ohtsuki,Xin Fu,Miao Pan*

Main category: cs.LG

TL;DR: 提出了一种名为PAE MobiLLM的新方法，解决了服务器辅助使用大型语言模型在移动设备上微调的隐私和通信问题。


<details>
  <summary>Details</summary>
Motivation: 现有的服务器辅助方法在处理移动端大型语言模型微调时存在通信负担大、隐私泄露风险等问题。

Method: 开发了PAE MobiLLM方法，采用增强隐私的服务器辅助附加侧微调，结合激活缓存和单激活维度的通信缩减技术，以及设备定义的添加适配器方案。

Result: PAE MobiLLM显著减少了通信开销，同时保护了数据、标签和微调模型的隐私，提升了微调效率。

Conclusion: PAE MobiLLM能够在保持高隐私保护的前提下，有效支持移动设备上大型语言模型的高效微调。

Abstract: There is a huge gap between numerous intriguing applications fostered by
on-device large language model (LLM) fine-tuning (FT) from fresh mobile data
and the limited resources of a mobile device. While existing server-assisted
methods (e.g., split learning or side-tuning) may enable LLM FT on the local
mobile device, they suffer from heavy communication burdens of activation
transmissions, and may disclose data, labels or fine-tuned models to the
server. To address those issues, we develop PAE MobiLLM, a privacy-aware and
efficient LLM FT method which can be deployed on the mobile device via
server-assisted additive side-tuning. To further accelerate FT convergence and
improve computing efficiency, PAE MobiLLM integrates activation caching on the
server side, which allows the server to reuse historical activations and saves
the mobile device from repeatedly computing forward passes for the recurring
data samples. Besides, to reduce communication cost, PAE MobiLLM develops a
one-token (i.e., ``pivot'' token) activation shortcut that transmits only a
single activation dimension instead of full activation matrices to guide the
side network tuning. Last but not least, PAE MobiLLM introduces the additive
adapter side-network design which makes the server train the adapter modules
based on device-defined prediction differences rather than raw ground-truth
labels. In this way, the server can only assist device-defined side-network
computing, and learn nothing about data, labels or fine-tuned models.

</details>


### [189] [Quantum Machine Learning in Transportation: A Case Study of Pedestrian Stress Modelling](https://arxiv.org/abs/2507.01235)
*Bara Rababa,Bilal Farooq*

Main category: cs.LG

TL;DR: 本研究探索了量子机器学习在建模复杂皮肤电导响应事件中的应用，比较了量子支持向量机（QSVM）和量子神经网络（QNN）的表现。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在通过量子机器学习解决复杂的机器学习任务，并应用于道路交通实验中的行人压力识别问题。

Method: 开发了具有八量子位 ZZ 特征映射的 QSVM 和基于树张量网络的 QNN模型，使用现实中的 SCR 数据进行分类建模。

Result: QSVM 在训练中表现良好，但测试集准确率仅为45%。QNN 在模型分类上表现更优，测试集准确率达到了55%。

Conclusion: QNN 在分类任务中比 QSVM 和经典模型更为有效，但仍存在一定的改进空间。

Abstract: Quantum computing has opened new opportunities to tackle complex machine
learning tasks, for instance, high-dimensional data representations commonly
required in intelligent transportation systems. We explore quantum machine
learning to model complex skin conductance response (SCR) events that reflect
pedestrian stress in a virtual reality road crossing experiment. For this
purpose, Quantum Support Vector Machine (QSVM) with an eight-qubit ZZ feature
map and a Quantum Neural Network (QNN) using a Tree Tensor Network ansatz and
an eight-qubit ZZ feature map, were developed on Pennylane. The dataset
consists of SCR measurements along with features such as the response amplitude
and elapsed time, which have been categorized into amplitude-based classes. The
QSVM achieved good training accuracy, but had an overfitting problem, showing a
low test accuracy of 45% and therefore impacting the reliability of the
classification model. The QNN model reached a higher test accuracy of 55%,
making it a better classification model than the QSVM and the classic versions.

</details>


### [190] [Zero-Incentive Dynamics: a look at reward sparsity through the lens of unrewarded subgoals](https://arxiv.org/abs/2507.01470)
*Yannick Molinghen,Tom Lenaerts*

Main category: cs.LG

TL;DR: 本文探讨了强化学习中奖励频率作为任务难度衡量标准的不可靠性问题，提出零激励动态的概念，并揭示了现有策略学习方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 重新审视当前强化学习中普遍的假设，即奖励频率可以作为任务难度的可靠度量。

Method: 提出并形式化了零激励动态概念，分析了现有目标导向的深度学习算法在此情景下的表现，并检验了学习性能对奖励时间邻近性的敏感性。

Result: 发现现有深度子目标算法无法有效利用零激励动态，并揭示了学习性能对奖励时间间隔的高度敏感性。

Conclusion: 当前方法存在局限性，需要发展能够在缺乏直接激励的情况下推断潜在任务结构的机制。

Abstract: This work re-examines the commonly held assumption that the frequency of
rewards is a reliable measure of task difficulty in reinforcement learning. We
identify and formalize a structural challenge that undermines the effectiveness
of current policy learning methods: when essential subgoals do not directly
yield rewards. We characterize such settings as exhibiting zero-incentive
dynamics, where transitions critical to success remain unrewarded. We show that
state-of-the-art deep subgoal-based algorithms fail to leverage these dynamics
and that learning performance is highly sensitive to the temporal proximity
between subgoal completion and eventual reward. These findings reveal a
fundamental limitation in current approaches and point to the need for
mechanisms that can infer latent task structure without relying on immediate
incentives.

</details>


### [191] [Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation](https://arxiv.org/abs/2507.01285)
*Aymen Rayane Khouas,Mohamed Reda Bouadjenek,Hakim Hacid,Sunil Aryal*

Main category: cs.LG

TL;DR: 本文提出了一种新方法Dist-FedAvg，用于图联邦推荐系统，通过对相似用户赋予更高权重的方式改进个性化推荐效果，并保持隐私保护能力。


<details>
  <summary>Details</summary>
Motivation: 现有聚合方法未能充分考虑用户嵌入的独特属性，以及用户相似性对推荐效果的重要性。同时，用户交互的动态变化要求支持适应性聚合并保留关键用户的影响力。

Method: 提出了一种名为Dist-FedAvg的基于距离的聚合方法，通过为相似嵌入的用户分配更高权重，同时确保关键用户在本地更新中的显著影响力，从而实现个性化和聚合效率的提升。

Result: 实验结果表明，Dist-FedAvg方法在多个数据集上均优于现有基线方法，在推荐准确度方面表现更佳，并且可以无缝集成到现有联邦学习框架中。

Conclusion: 本文的方法为提升图联邦推荐系统的个性化推荐效果提供了一种新思路，同时兼顾了用户隐私和关键用户在系统中的重要性。

Abstract: Graph federated recommendation systems offer a privacy-preserving alternative
to traditional centralized recommendation architectures, which often raise
concerns about data security. While federated learning enables personalized
recommendations without exposing raw user data, existing aggregation methods
overlook the unique properties of user embeddings in this setting. Indeed,
traditional aggregation methods fail to account for their complexity and the
critical role of user similarity in recommendation effectiveness. Moreover,
evolving user interactions require adaptive aggregation while preserving the
influence of high-relevance anchor users (the primary users before expansion in
graph-based frameworks). To address these limitations, we introduce
Dist-FedAvg, a novel distance-based aggregation method designed to enhance
personalization and aggregation efficiency in graph federated learning. Our
method assigns higher aggregation weights to users with similar embeddings,
while ensuring that anchor users retain significant influence in local updates.
Empirical evaluations on multiple datasets demonstrate that Dist-FedAvg
consistently outperforms baseline aggregation techniques, improving
recommendation accuracy while maintaining seamless integration into existing
federated learning frameworks.

</details>


### [192] [Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning](https://arxiv.org/abs/2507.01551)
*Wu Fei,Hao Kong,Shuxian Liang,Yang Lin,Yibo Yang,Jing Tang,Lei Chen,Xiansheng Hua*

Main category: cs.LG

TL;DR: 提出了一种新的框架SPRO，在政策模型内部推导过程奖励并引入累积过程奖励和MSA以提高大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决了PRL中额外的计算开销和缺乏统一过程级优势估计理论框架的问题。

Method: 理论上从政策模型内推导过程奖励，并引入累积过程奖励和MSA，提升采样组的逐步作用估计。

Result: 训练效率提高3.4倍，测试准确率增强17.5%，实现更稳定和更高的策略熵，同时减少了约1/3的平均应答长度。

Conclusion: SPRO在不增加计算开销的情况下显著提升了PRL性能，并具备工业应用潜力。

Abstract: Process Reinforcement Learning~(PRL) has demonstrated considerable potential
in enhancing the reasoning capabilities of Large Language Models~(LLMs).
However, introducing additional process reward models incurs substantial
computational overhead, and there is no unified theoretical framework for
process-level advantage estimation. To bridge this gap, we propose
\textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward
\textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables
process-aware RL through two key innovations: (1) we first theoretically
demonstrate that process rewards can be derived intrinsically from the policy
model itself, and (2) we introduce well-defined cumulative process rewards and
\textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which
facilitates rigorous step-wise action advantage estimation within shared-prompt
sampling groups. Our experimental results demonstrate that SPRO outperforms
vaniila GRPO with 3.4x higher training efficiency and a 17.5\% test accuracy
improvement. Furthermore, SPRO maintains a stable and elevated policy entropy
throughout training while reducing the average response length by approximately
$1/3$, evidencing sufficient exploration and prevention of reward hacking.
Notably, SPRO incurs no additional computational overhead compared to
outcome-supervised RL methods such as GRPO, which benefit industrial
implementation.

</details>


### [193] [Chargax: A JAX Accelerated EV Charging Simulator](https://arxiv.org/abs/2507.01522)
*Koen Ponse,Jan Felix Kleuker,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: 本文介绍了Chargax，这是一个基于JAX的环境，用于加速电动汽车充电站的强化学习训练，显著提高了计算性能。


<details>
  <summary>Details</summary>
Motivation: 现有电网系统拥堵严重，需要提高运行效率，而传统强化学习方法在高采样复杂性和昂贵仿真要求下效率较低。

Method: 利用JAX构建名为Chargax的新环境，通过模块化架构支持多样化的充电站模拟，并进行强化学习训练。

Result: Chargax在仿真和RL训练中性能提升超100至1000倍，并成功验证了多种基于真实数据的使用场景。

Conclusion: 该系统显著提高了强化学习训练的效率和灵活性，有助于解决电动汽车充电站运营中的实际问题。

Abstract: Deep Reinforcement Learning can play a key role in addressing sustainable
energy challenges. For instance, many grid systems are heavily congested,
highlighting the urgent need to enhance operational efficiency. However,
reinforcement learning approaches have traditionally been slow due to the high
sample complexity and expensive simulation requirements. While recent works
have effectively used GPUs to accelerate data generation by converting
environments to JAX, these works have largely focussed on classical toy
problems. This paper introduces Chargax, a JAX-based environment for realistic
simulation of electric vehicle charging stations designed for accelerated
training of RL agents. We validate our environment in a variety of scenarios
based on real data, comparing reinforcement learning agents against baselines.
Chargax delivers substantial computational performance improvements of over
100x-1000x over existing environments. Additionally, Chargax' modular
architecture enables the representation of diverse real-world charging station
configurations.

</details>


### [194] [Efficient Kilometer-Scale Precipitation Downscaling with Conditional Wavelet Diffusion](https://arxiv.org/abs/2507.01354)
*Chugang Yi,Minghan Yu,Weikang Qian,Yixin Wen,Haizhao Yang*

Main category: cs.LG

TL;DR: 提出了WDM模型，用于将降水数据从10 km空间分辨率提升到1 km，同时使推理速度提高9倍。


<details>
  <summary>Details</summary>
Motivation: 现有全球降水产品的分辨率无法满足水文建模和极端天气分析的需求，需要更高分辨率的数据。

Method: 引入名为Wavelet Diffusion Model（WDM）的生成框架，以小波域的条件扩散模型对降水结构进行学习和细化，实现10倍空间超分辨率。

Result: WDM在生成1 km分辨率降水数据时，效果比像素空间模型更真实，伪影更少，采样效率显著提高。

Conclusion: WDM为解决地球科学超分辨率中的精度和速度问题提供了强有力的解决方案，为更可靠的水文预测铺平了道路。

Abstract: Effective hydrological modeling and extreme weather analysis demand
precipitation data at a kilometer-scale resolution, which is significantly
finer than the 10 km scale offered by standard global products like IMERG. To
address this, we propose the Wavelet Diffusion Model (WDM), a generative
framework that achieves 10x spatial super-resolution (downscaling to 1 km) and
delivers a 9x inference speedup over pixel-based diffusion models. WDM is a
conditional diffusion model that learns the learns the complex structure of
precipitation from MRMS radar data directly in the wavelet domain. By focusing
on high-frequency wavelet coefficients, it generates exceptionally realistic
and detailed 1-km precipitation fields. This wavelet-based approach produces
visually superior results with fewer artifacts than pixel-space models, and
delivers a significant gains in sampling efficiency. Our results demonstrate
that WDM provides a robust solution to the dual challenges of accuracy and
speed in geoscience super-resolution, paving the way for more reliable
hydrological forecasts.

</details>


### [195] [Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling](https://arxiv.org/abs/2507.01679)
*Zeyu Huang,Tianhao Cheng,Zihan Qiu,Zili Wang,Yinghui Xu,Edoardo M. Ponti,Ivan Titov*

Main category: cs.LG

TL;DR: 提出一种称为Prefix-RFT的混合方法，将SFT（监督微调）和RFT（强化微调）结合，旨在优化大型语言模型的后训练性能。


<details>
  <summary>Details</summary>
Motivation: 现有的SFT与RFT各有优缺点，SFT易于模仿但可能导致糟糕的泛化，RFT性能提升潜力大但容易学到意外行为，且对初始策略敏感，因此需要一种能够结合两者优点的新的后训练方法。

Method: 提出并定义了Prefix-RFT方法，该方法通过在SFT与RFT之间形成统一视角，通过前置技术融合示范与探索学习，并通过数学推理问题验证其有效性。

Result: Prefix-RFT的表现超越了单独的SFT和RFT方法，也优于并行混合策略的RFT方法。同时指出这一机制能与现有开源框架无缝集成，且对数据质量和数量变化具有强鲁棒性。

Conclusion: Prefix-RFT验证了SFT和RFT的互补性，基于这种统一框架的方法在大型语言模型后训练任务中具有潜在研究价值。

Abstract: Existing post-training techniques for large language models are broadly
categorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning
(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking
demonstration data but can lead to problematic generalization as a form of
behavior cloning. Conversely, RFT can significantly enhance a model's
performance but is prone to learn unexpected behaviors, and its performance is
highly sensitive to the initial policy. In this paper, we propose a unified
view of these methods and introduce Prefix-RFT, a hybrid approach that
synergizes learning from both demonstration and exploration. Using mathematical
reasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is
both simple and effective. It not only surpasses the performance of standalone
SFT and RFT but also outperforms parallel mixed-policy RFT methods. A key
advantage is its seamless integration into existing open-source frameworks,
requiring only minimal modifications to the standard RFT pipeline. Our analysis
highlights the complementary nature of SFT and RFT, and validates that
Prefix-RFT effectively harmonizes these two learning paradigms. Furthermore,
ablation studies confirm the method's robustness to variations in the quality
and quantity of demonstration data. We hope this work offers a new perspective
on LLM post-training, suggesting that a unified paradigm that judiciously
integrates demonstration and exploration could be a promising direction for
future research.

</details>


### [196] [Surrogate Modeling via Factorization Machine and Ising Model with Enhanced Higher-Order Interaction Learning](https://arxiv.org/abs/2507.01389)
*Anbang Wang,Dunbo Cai,Yu Zhang,Yangqing Huang,Xiangyang Feng,Zhihong Zhang*

Main category: cs.LG

TL;DR: 提出了一种结合松弛变量的改进代理模型用于预测药物组合效果，实验显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 受到利用因式分解机与量子退火优化代理模型的启发，旨在改进代理模型以更好地捕获特征交互，提高预测效果。

Method: 在因式分解机及其对应的Ising表示中引入松弛变量，将两步训练过程统一为单一集成步骤，并迭代更新松弛变量以考虑更高阶的特征交互。

Result: 实验表明，引入松弛变量的改进模型在预测药物组合效果任务中性能显著提高。

Conclusion: 改进后的算法展示了构建高效代理模型的潜力，并有望利用潜在的量子计算优势。

Abstract: Recently, a surrogate model was proposed that employs a factorization machine
to approximate the underlying input-output mapping of the original system, with
quantum annealing used to optimize the resulting surrogate function. Inspired
by this approach, we propose an enhanced surrogate model that incorporates
additional slack variables into both the factorization machine and its
associated Ising representation thereby unifying what was by design a two-step
process into a single, integrated step. During the training phase, the slack
variables are iteratively updated, enabling the model to account for
higher-order feature interactions. We apply the proposed method to the task of
predicting drug combination effects. Experimental results indicate that the
introduction of slack variables leads to a notable improvement of performance.
Our algorithm offers a promising approach for building efficient surrogate
models that exploit potential quantum advantages.

</details>


### [197] [Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training](https://arxiv.org/abs/2507.01752)
*Ismail Labiad,Mathurin Videau,Matthieu Kowalski,Marc Schoenauer,Alessandro Leite,Julia Kempe,Olivier Teytaud*

Main category: cs.LG

TL;DR: 介绍了一种用于大语言模型（LLMs）后训练的新型进化黑盒优化方法BBoxER，旨在解决深度学习中梯度优化方法的隐私、鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 传统的梯度优化需要大量标记数据，存在隐私和安全问题；而黑盒优化在数据访问受限或风险较高时是可选方案，但其在高维参数空间中的可扩展性和计算成本是难点。

Method: 提出BBoxER方法，通过隐式压缩训练数据引入信息瓶颈，并结合信息流的易处理性提供强理论保证；作为一种轻量化模块，BBoxER可应用于预训练的LLMs中。

Result: 实验表明，BBoxER在推理数据集上的性能有所改善，且具有良好的泛化能力。

Conclusion: BBoxER有效结合黑盒优化和梯度优化的优点，是LLMs训练中的一个有吸引力的补充方案。

Abstract: Gradient-based optimization is the workhorse of deep learning, offering
efficient and scalable training via backpropagation. However, its reliance on
large volumes of labeled data raises privacy and security concerns such as
susceptibility to data poisoning attacks and the risk of overfitting. In
contrast, black box optimization methods, which treat the model as an opaque
function, relying solely on function evaluations to guide optimization, offer a
promising alternative in scenarios where data access is restricted, adversarial
risks are high, or overfitting is a concern. However, black box methods also
pose significant challenges, including poor scalability to high-dimensional
parameter spaces, as prevalent in large language models (LLMs), and high
computational costs due to reliance on numerous model evaluations. This paper
introduces BBoxER, an evolutionary black-box method for LLM post-training that
induces an information bottleneck via implicit compression of the training
data. Leveraging the tractability of information flow, we provide strong
theoretical bounds on generalization, differential privacy, susceptibility to
data poisoning attacks, and robustness to extraction attacks. BBoxER operates
on top of pre-trained LLMs, offering a lightweight and modular enhancement
suitable for deployment in restricted or privacy-sensitive environments, in
addition to non-vacuous generalization guarantees. In experiments with LLMs, we
demonstrate empirically that Retrofitting methods are able to learn, showing
how a few iterations of BBoxER improve performance and generalize well on a
benchmark of reasoning datasets. This positions BBoxER as an attractive add-on
on top of gradient-based optimization.

</details>


### [198] [Decomposing Prediction Mechanisms for In-Context Recall](https://arxiv.org/abs/2507.01414)
*Sultan Daniels,Dylan Davis,Dhruv Gautam,Wentinn Liao,Gireeja Ranade,Anant Sahai*

Main category: cs.LG

TL;DR: 本文引入了一个融合连续在线学习与离散联想记忆的新型玩具问题，研究了训练的Transformer模型如何通过上下文标签回忆早先序列的状态，并揭示了模型的多机制学习动态。


<details>
  <summary>Details</summary>
Motivation: 通过研究玩具问题中的Transformer表现，探索模型是否能在上下文标签提示下有效联想并预测序列状态，揭示深层学习动态的多机制特征及其实现。

Method: 采用训练Transformer模型，并使用随机生成的线性确定性动态系统的带符号标记状态观察数据作为训练样本，通过逐步分析模型在不同学习阶段的表现，以及使用分布外实验与边缘剪枝等方法进行机制分析。

Result: 发现任务的两项子能力（关联回忆与延续预测）在不同阶段出现，且针对下一步Token预测存在两种不同机制，分别基于离散符号标签的联想回忆以及不依赖符号标签的贝叶斯式预测，表现出不同的学习动态。

Conclusion: 模型中多机制表现出分阶段的学习特性，这种现象不仅存在于玩具问题中，在更复杂的任务中如ICL翻译也有类似体现，提示了模型对任务理解与解决的不同过程。

Abstract: We introduce a new family of toy problems that combine features of
linear-regression-style continuous in-context learning (ICL) with discrete
associative recall. We pretrain transformer models on sample traces from this
toy, specifically symbolically-labeled interleaved state observations from
randomly drawn linear deterministic dynamical systems. We study if the
transformer models can recall the state of a sequence previously seen in its
context when prompted to do so with the corresponding in-context label. Taking
a closer look at this task, it becomes clear that the model must perform two
functions: (1) identify which system's state should be recalled and apply that
system to its last seen state, and (2) continuing to apply the correct system
to predict the subsequent states. Training dynamics reveal that the first
capability emerges well into a model's training. Surprisingly, the second
capability, of continuing the prediction of a resumed sequence, develops much
earlier.
  Via out-of-distribution experiments, and a mechanistic analysis on model
weights via edge pruning, we find that next-token prediction for this toy
problem involves at least two separate mechanisms. One mechanism uses the
discrete symbolic labels to do the associative recall required to predict the
start of a resumption of a previously seen sequence. The second mechanism,
which is largely agnostic to the discrete symbolic labels, performs a
"Bayesian-style" prediction based on the previous token and the context. These
two mechanisms have different learning dynamics.
  To confirm that this multi-mechanism (manifesting as separate phase
transitions) phenomenon is not just an artifact of our toy setting, we used
OLMo training checkpoints on an ICL translation task to see a similar
phenomenon: a decisive gap in the emergence of first-task-token performance vs
second-task-token performance.

</details>


### [199] [LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs](https://arxiv.org/abs/2507.01806)
*Reza Arabpour,Haitz Sáez de Ocáriz Borde,Anastasis Kratsios*

Main category: cs.LG

TL;DR: 本文提出一种基于LoRA的CPU端参数高效微调方法，通过使用多种预训练的适配器来组合生成，提供了一种无需GPU即可微调大型语言模型的实用替代方案。


<details>
  <summary>Details</summary>
Motivation: 当前GPU限制了LoRA的广泛应用，本研究旨在为CPU用户提供经济高效的微调方法，扩大其使用范围。

Method: 通过学习一种元操作器，将输入数据映射为LoRA权重，这些权重来自已有的预训练适配器库，能够在CPU上以轻量方式组合实现。

Result: 生成的适配器性能不及GPU训练模型，但在特定任务中明显优于Mistral基模型，且无需依赖GPU。

Conclusion: 该方法为资源受限的用户提供了低成本的微调替代方案，显著降低了使用大型语言模型的门槛。

Abstract: Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language
Models (LLMs) by enabling parameter-efficient updates. However, their
widespread adoption remains limited by the reliance on GPU-based training. In
this work, we propose a theoretically grounded approach to LoRA fine-tuning
designed specifically for users with limited computational resources,
particularly those restricted to standard laptop CPUs. Our method learns a
meta-operator that maps any input dataset, represented as a probability
distribution, to a set of LoRA weights by leveraging a large bank of
pre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of
performing new gradient-based updates, our pipeline constructs adapters via
lightweight combinations of existing LoRAs directly on CPU. While the resulting
adapters do not match the performance of GPU-trained counterparts, they
consistently outperform the base Mistral model on downstream tasks, offering a
practical and accessible alternative to traditional GPU-based fine-tuning.

</details>


### [200] [Test-Time Scaling with Reflective Generative Model](https://arxiv.org/abs/2507.01951)
*Zixiao Wang,Yuxin Wang,Xiaorui Wang,Mengting Xing,Jie Gao,Jianjun Xu,Guangcan Liu,Chenhui Jin,Zhuo Wang,Shengzhuo Zhang,Hongtao Xie*

Main category: cs.LG

TL;DR: 我们提出了MetaStone-S1，一种反射生成模型，通过自监督过程奖励模型（SPRM）实现了与OpenAI o3相当的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在通过共享的骨干网络和任务特定头融合策略模型和过程奖励模型（PRM），提高推理效率并减少参数消耗。

Method: 通过自监督过程奖励模型（SPRM），整合政策模型和奖励模型；引入任务时间缩放（TTS）和三种推理模式（低、中、高）；并揭示总思维计算与TTS性能的关系。

Result: MetaStone-S1凭借32B参数大小，性能达到OpenAI-o3-mini系列的水平，并为研究社区开源。

Conclusion: MetaStone-S1实现了高效的推理过程和出色的性能表现，并通过缩放规律验证模型的可控性与实用性。

Abstract: We introduce our first reflective generative model MetaStone-S1, which
obtains OpenAI o3's performance via the self-supervised process reward model
(SPRM). Through sharing the backbone network and using task-specific heads for
next token prediction and process scoring respectively, SPRM successfully
integrates the policy model and process reward model(PRM) into a unified
interface without extra process annotation, reducing over 99% PRM parameters
for efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable
for test time scaling (TTS), and we provide three reasoning effort modes (low,
medium, and high), based on the controllable thinking length. Moreover, we
empirically establish a scaling law that reveals the relationship between total
thinking computation and TTS performance. Experiments demonstrate that our
MetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with
only 32B parameter size. To support the research community, we have
open-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.

</details>


### [201] [Cross-platform Smartphone Positioning at Museums](https://arxiv.org/abs/2507.01469)
*Alessio Ferrato,Fabio Gasparetti,Carla Limongelli,Stefano Mastandrea,Giuseppe Sansonetti,Joaquín Torres-Sospedra*

Main category: cs.LG

TL;DR: 文章提出了一种名为BAR的新数据集，用于解决博物馆环境中室内定位系统（IPS）的数据不足问题，同时提供了定位算法的基线性能分析。


<details>
  <summary>Details</summary>
Motivation: 为了改善文化遗产机构中的访客体验，通过提供个性化导航、优化展品组织和提高与展览互动的效率，室内定位系统可发挥重要作用，但面临环境、技术和实验限制等多重挑战。

Method: 文章收集了一种新的RSS数据集（BAR），该数据集包括在博物馆中13个房间、90件艺术品前的数据，采用了安卓和iOS两种平台。同时，利用基于邻近性的方法和$k$-NN算法提供了一个定位基线分析。

Result: 作者展示了RSS数据集在博物馆环境中的使用情况，并通过分析数据提出了定位方法的基线性能，同时充分讨论了结果及未来研究方向。

Conclusion: 本文解决了博物馆环境中用于研究室内定位问题的数据集匮乏问题，提出的BAR数据集与定位分析为未来研究提供了重要支持，并为文化遗产领域的室内定位方案开辟了新的可能性。

Abstract: Indoor Positioning Systems (IPSs) hold significant potential for enhancing
visitor experiences in cultural heritage institutions. By enabling personalized
navigation, efficient artifact organization, and better interaction with
exhibits, IPSs can transform the modalities of how individuals engage with
museums, galleries and libraries. However, these institutions face several
challenges in implementing IPSs, including environmental constraints, technical
limits, and limited experimentation. In other contexts, Received Signal
Strength (RSS)-based approaches using Bluetooth Low Energy (BLE) and WiFi have
emerged as preferred solutions due to their non-invasive nature and minimal
infrastructure requirements. Nevertheless, the lack of publicly available RSS
datasets that specifically reflect museum environments presents a substantial
barrier to developing and evaluating positioning algorithms designed for the
intricate spatial characteristics typical of cultural heritage sites. To
address this limitation, we present BAR, a novel RSS dataset collected in front
of 90 artworks across 13 museum rooms using two different platforms, i.e.,
Android and iOS. Additionally, we provide an advanced position classification
baseline taking advantage of a proximity-based method and $k$-NN algorithms. In
our analysis, we discuss the results and offer suggestions for potential
research directions.

</details>


### [202] [GradMetaNet: An Equivariant Architecture for Learning on Gradients](https://arxiv.org/abs/2507.01649)
*Yoav Gelberg,Yam Eitan,Aviv Navon,Aviv Shamsian,Theo,Putterman,Michael Bronstein,Haggai Maron*

Main category: cs.LG

TL;DR: 提出了一种名为GradMetaNet的新型体系结构，用于直接处理神经网络梯度，在多个梯度相关任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理梯度时缺乏针对性设计，限制了其适用性，提出专门用于梯度处理的架构以解决这一问题。

Method: 引入三个原则：(1) 保持神经元置换对称性的等变性设计；(2) 通过跨多个数据点的梯度集处理捕获曲率信息；(3) 使用秩-1分解进行高效梯度表示，并基于这些原则构建GradMetaNet体系结构，采用简单等变模块。

Result: 证明GradMetaNet具有普适性，可近似其他方法无法处理的自然梯度相关函数，并在多个任务（如学习优化、INR编辑和损失曲率估计）中展现出优越的性能。

Conclusion: GradMetaNet通过有效处理神经网络梯度，在理论和实践中均表现出色，在多个梯度相关应用中具有显著潜力。

Abstract: Gradients of neural networks encode valuable information for optimization,
editing, and analysis of models. Therefore, practitioners often treat gradients
as inputs to task-specific algorithms, e.g. for pruning or optimization. Recent
works explore learning algorithms that operate directly on gradients but use
architectures that are not specifically designed for gradient processing,
limiting their applicability. In this paper, we present a principled approach
for designing architectures that process gradients. Our approach is guided by
three principles: (1) equivariant design that preserves neuron permutation
symmetries, (2) processing sets of gradients across multiple data points to
capture curvature information, and (3) efficient gradient representation
through rank-1 decomposition. Based on these principles, we introduce
GradMetaNet, a novel architecture for learning on gradients, constructed from
simple equivariant blocks. We prove universality results for GradMetaNet, and
show that previous approaches cannot approximate natural gradient-based
functions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness
on a diverse set of gradient-based tasks on MLPs and transformers, such as
learned optimization, INR editing, and estimating loss landscape curvature.

</details>


### [203] [Loss Functions in Diffusion Models: A Comparative Study](https://arxiv.org/abs/2507.01516)
*Dibyanshu Kumar,Philipp Vaeth,Magda Gregorová*

Main category: cs.LG

TL;DR: 本研究统一了扩散模型中的目标和损失函数，探讨了它们的关系，并进行了理论和实证分析。


<details>
  <summary>Details</summary>
Motivation: 研究动机是在扩散模型框架中探索最佳的损失函数，为更高效和目标导向的模型设计提供指导。

Method: 从理论上整合了不同的目标函数，将它们统一在变分下界目标框架下，并进行了实证研究，分析它们在不同条件下的性能差异及其影响因素。

Result: 发现目标函数的选择对生成高质量样本或准确估计似然的能力影响显著，并揭示了各目标函数性能分歧的条件和原因。

Conclusion: 本研究提供了扩散模型损失函数的一种统一视角，对未来更高效、目标导向的模型设计具有指导意义。

Abstract: Diffusion models have emerged as powerful generative models, inspiring
extensive research into their underlying mechanisms. One of the key questions
in this area is the loss functions these models shall train with. Multiple
formulations have been introduced in the literature over the past several years
with some links and some critical differences stemming from various initial
considerations. In this paper, we explore the different target objectives and
corresponding loss functions in detail. We present a systematic overview of
their relationships, unifying them under the framework of the variational lower
bound objective. We complement this theoretical analysis with an empirical
study providing insights into the conditions under which these objectives
diverge in performance and the underlying factors contributing to such
deviations. Additionally, we evaluate how the choice of objective impacts the
model ability to achieve specific goals, such as generating high-quality
samples or accurately estimating likelihoods. This study offers a unified
understanding of loss functions in diffusion models, contributing to more
efficient and goal-oriented model designs in future research.

</details>


### [204] [AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training](https://arxiv.org/abs/2507.01663)
*Zhenyu Han,Ansheng You,Haibo Wang,Kui Luo,Guang Yang,Wenqi Shi,Menglong Chen,Sicheng Zhang,Zeshun Lan,Chunshi Deng,Huazhong Ji,Wenjie Liu,Yu Huang,Yixiang Zhang,Chenyi Pan,Jing Wang,Xin Huang,Chunsheng Li,Jianping Wu*

Main category: cs.LG

TL;DR: 本文提出了AsyncFlow，一个异步流式RL框架，用于高效后训练。通过分布式数据存储与传输模块以及生产者-消费者异步流程，实现了显著的吞吐量改进。


<details>
  <summary>Details</summary>
Motivation: 传统的任务并置RL框架面临扩展性瓶颈，分离式RL框架则因复杂数据流和计算资源利用不均而受限。此外，许多现有框架与LLM训练或推理引擎紧耦合，难以支持自定义引擎。

Method: 提出AsyncFlow框架，采用分布式数据存储与传输模块实现任务流水线的自动重叠及负载均衡；引入生产者-消费者异步流程，优化计算资源利用并支持可定制引擎。

Result: 实验显示，与先进基线相比，AsyncFlow平均提高了1.59倍的吞吐量。

Conclusion: AsyncFlow为下一代RL训练系统设计提供了可操作的前瞻性框架，解决了现有系统扩展性、资源利用和模块化支持的主要难题。

Abstract: Reinforcement learning (RL) has become a pivotal technology in the
post-training phase of large language models (LLMs). Traditional task-colocated
RL frameworks suffer from significant scalability bottlenecks, while
task-separated RL frameworks face challenges in complex dataflows and the
corresponding resource idling and workload imbalance. Moreover, most existing
frameworks are tightly coupled with LLM training or inference engines, making
it difficult to support custom-designed engines. To address these challenges,
we propose AsyncFlow, an asynchronous streaming RL framework for efficient
post-training. Specifically, we introduce a distributed data storage and
transfer module that provides a unified data management and fine-grained
scheduling capability in a fully streamed manner. This architecture inherently
facilitates automated pipeline overlapping among RL tasks and dynamic load
balancing. Moreover, we propose a producer-consumer-based asynchronous workflow
engineered to minimize computational idleness by strategically deferring
parameter update process within staleness thresholds. Finally, the core
capability of AsynFlow is architecturally decoupled from underlying training
and inference engines and encapsulated by service-oriented user interfaces,
offering a modular and customizable user experience. Extensive experiments
demonstrate an average of 1.59 throughput improvement compared with
state-of-the-art baseline. The presented architecture in this work provides
actionable insights for next-generation RL training system designs.

</details>


### [205] [MARVIS: Modality Adaptive Reasoning over VISualizations](https://arxiv.org/abs/2507.01544)
*Benjamin Feuer,Lennart Purucker,Oussama Elachqar,Chinmay Hegde*

Main category: cs.LG

TL;DR: 本研究提出了一个无需训练的方法MARVIS，该方法结合视觉语言模型（VLMs）扩展至多种数据模态，实现了与特定领域方法相较之下的具有竞争力的性能表现。


<details>
  <summary>Details</summary>
Motivation: 当前的机器学习模型在专门领域中表现优秀，但缺乏灵活性，而基础模型虽然具备多样性但常在非传统模态或长尾领域中表现不佳。

Method: 设计了一个无训练需求的方法MARVIS，通过将潜在嵌入空间转化为视觉表示，借助视觉语言模型的空间和细粒度推理能力，跨模态处理多模态数据。

Result: MARVIS在视觉、音频、生物与表格等领域展现了竞争力，并使用了一个3B模型参数，在多数场景下超越其他方法如Gemini达16%的提升。

Conclusion: MARVIS方法无需域特定训练，实现了跨模态出色表现的同时，也避免暴露隐私信息，展示了其通用性和潜力。

Abstract: Scientific applications of machine learning often rely on small, specialized
models tuned to particular domains. Such models often achieve excellent
performance, but lack flexibility. Foundation models offer versatility, but
typically underperform specialized approaches, especially on non-traditional
modalities and long-tail domains. We propose MARVIS (Modality Adaptive
Reasoning over VISualizations), a training-free method that enables even small
vision-language models to predict any data modality with high accuracy. MARVIS
transforms latent embedding spaces into visual representations and then
leverages the spatial and fine-grained reasoning skills of VLMs to successfully
interpret and utilize them. MARVIS achieves competitive performance on vision,
audio, biological, and tabular domains using a single 3B parameter model,
achieving results that beat Gemini by 16\% on average and approach specialized
methods, without exposing personally identifiable information (P.I.I.) or
requiring any domain-specific training. We open source our code and datasets at
https://github.com/penfever/marvis

</details>


### [206] [How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks](https://arxiv.org/abs/2507.01559)
*Lapo Frati,Neil Traft,Jeff Clune,Nick Cheney*

Main category: cs.LG

TL;DR: 该研究探讨了在神经网络最后一层进行权重重采样（即"zapping"）的效果及机制，尤其在持续学习和少样本迁移学习中的作用。


<details>
  <summary>Details</summary>
Motivation: 虽然权重重采样（zapping）在改善持续学习中发挥作用已有实证支持，但其具体机制仍未被清晰了解。

Method: 研究通过解析卷积神经网络在不同任务和迁移设置下，学习与遗忘的模式来探讨权重重采样的影响，并观察优化器对学习与遗忘动态的影响。

Result: 实验表明，实施zapping的模型在迁移到新域时能更快速地恢复，同时不同优化器的选择也显著影响任务间协同或干扰的模式。

Conclusion: 权重重采样和优化器的选择在继续学习和迁移学习中起到了重要作用，可显著改变模型的学习动态。

Abstract: Recent work in continual learning has highlighted the beneficial effect of
resampling weights in the last layer of a neural network (``zapping"). Although
empirical results demonstrate the effectiveness of this approach, the
underlying mechanisms that drive these improvements remain unclear. In this
work, we investigate in detail the pattern of learning and forgetting that take
place inside a convolutional neural network when trained in challenging
settings such as continual learning and few-shot transfer learning, with
handwritten characters and natural images. Our experiments show that models
that have undergone zapping during training more quickly recover from the shock
of transferring to a new domain. Furthermore, to better observe the effect of
continual learning in a multi-task setting we measure how each individual task
is affected. This shows that, not only zapping, but the choice of optimizer can
also deeply affect the dynamics of learning and forgetting, causing complex
patterns of synergy/interference between tasks to emerge when the model learns
sequentially at transfer time.

</details>


### [207] [GPT, But Backwards: Exactly Inverting Language Model Outputs](https://arxiv.org/abs/2507.01693)
*Adrians Skapars,Edoardo Manino,Youcheng Sun,Lucas C. Cordeiro*

Main category: cs.LG

TL;DR: 该研究提出了SODA算法，用于反向重构LLM输出的精确输入，并在短输入情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的审计方法旨在检测大型语言模型中的潜在不良行为，但对输出对应的输入重构研究较少，该研究旨在解决这一问题。

Method: 将精确输入重构形式化为具有唯一全局最小值的离散优化问题，并引入基于连续松弛输入搜索空间的梯度算法SODA。

Result: 在模型参数从33M到3B的范围内，SODA对短输入的恢复率达79.5%，但对较长输入难以提取相关信息。

Conclusion: 当前的标准部署方式可能能够有效防范SODA方法的恶意使用。

Abstract: While existing auditing techniques attempt to identify potential unwanted
behaviours in large language models (LLMs), we address the complementary
forensic problem of reconstructing the exact input that led to an existing LLM
output - enabling post-incident analysis and potentially the detection of fake
output reports. We formalize exact input reconstruction as a discrete
optimisation problem with a unique global minimum and introduce SODA, an
efficient gradient-based algorithm that operates on a continuous relaxation of
the input search space with periodic restarts and parameter decay. Through
comprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we
demonstrate that SODA significantly outperforms existing approaches. We succeed
in fully recovering 79.5% of shorter out-of-distribution inputs from next-token
logits, without a single false positive, but struggle to extract private
information from the outputs of longer (15+ token) input sequences. This
suggests that standard deployment practices may currently provide adequate
protection against malicious use of our method. Our code is available at
https://doi.org/10.5281/zenodo.15539879.

</details>


### [208] [A Privacy-Preserving Indoor Localization System based on Hierarchical Federated Learning](https://arxiv.org/abs/2507.01581)
*Masood Jan,Wafa Njima,Xun Zhang*

Main category: cs.LG

TL;DR: 提出一种基于联邦学习（FL）结合深度神经网络（DNN）的动态室内定位方法，在保障隐私的同时，接近集中式模型（CL）的性能。


<details>
  <summary>Details</summary>
Motivation: 现有室内定位技术存在隐私泄露、带宽限制和服务器可靠性问题。

Method: 使用联邦学习方法，无需数据集中，通过深度神经网络实现室内定位。

Result: 实验表明，提出的FL方法性能接近集中式模型，并解决了隐私、带宽和服务器可靠性问题。

Conclusion: 该研究证明FL方法是一种保障隐私并高效的室内定位方案，为相关系统的发展开辟了新路径。

Abstract: Location information serves as the fundamental element for numerous Internet
of Things (IoT) applications. Traditional indoor localization techniques often
produce significant errors and raise privacy concerns due to centralized data
collection. In response, Machine Learning (ML) techniques offer promising
solutions by capturing indoor environment variations. However, they typically
require central data aggregation, leading to privacy, bandwidth, and server
reliability issues. To overcome these challenges, in this paper, we propose a
Federated Learning (FL)-based approach for dynamic indoor localization using a
Deep Neural Network (DNN) model. Experimental results show that FL has the
nearby performance to Centralized Model (CL) while keeping the data privacy,
bandwidth efficiency and server reliability. This research demonstrates that
our proposed FL approach provides a viable solution for privacy-enhanced indoor
localization, paving the way for advancements in secure and efficient indoor
localization systems.

</details>


### [209] [Relational Causal Discovery with Latent Confounders](https://arxiv.org/abs/2507.01700)
*Andrea Piras,Matteo Negro,Ragib Ahsan,David Arbour,Elena Zheleva*

Main category: cs.LG

TL;DR: 提出了一种新的算法RelFCI，用于在存在潜在混杂因素的情形下，从关系数据中发现因果关系。


<details>
  <summary>Details</summary>
Motivation: 目前的因果发现算法存在两大局限性：一是大多假定数据是独立同分布，难以处理关系数据；二是假设因果充分性，而这在现实场景中往往不成立。

Method: 基于快速因果推断（FCI）和关系因果发现（RCD）算法，定义了新的图模型，开发了RelFCI算法，并证明了其在关系数据因果发现中的可靠性与完整性。

Result: 实验结果展示了RelFCI算法在识别带有潜在混杂因素的关系因果模型中的正确因果结构方面的有效性。

Conclusion: RelFCI算法不仅扩展了因果发现领域，且在关系数据与潜在混杂场景中表现优越，为研究真实世界问题提供了新工具。

Abstract: Estimating causal effects from real-world relational data can be challenging
when the underlying causal model and potential confounders are unknown. While
several causal discovery algorithms exist for learning causal models with
latent confounders from data, they assume that the data is independent and
identically distributed (i.i.d.) and are not well-suited for learning from
relational data. Similarly, existing relational causal discovery algorithms
assume causal sufficiency, which is unrealistic for many real-world datasets.
To address this gap, we propose RelFCI, a sound and complete causal discovery
algorithm for relational data with latent confounders. Our work builds upon the
Fast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms
and it defines new graphical models, necessary to support causal discovery in
relational domains. We also establish soundness and completeness guarantees for
relational d-separation with latent confounders. We present experimental
results demonstrating the effectiveness of RelFCI in identifying the correct
causal structure in relational causal models with latent confounders.

</details>


### [210] [Analysis of Muon's Convergence and Critical Batch Size](https://arxiv.org/abs/2507.01598)
*Naoki Sato,Hiroki Naganuma,Hideaki Iiduka*

Main category: cs.LG

TL;DR: 本文分析了优化器Muon，提出其利用神经网络参数的矩阵结构的理论方法，并验证了其理论发现。


<details>
  <summary>Details</summary>
Motivation: 研究如何改进神经网络优化方法，通过利用参数的矩阵结构提升算法表现。

Method: 对Muon优化器进行理论分析，提供收敛性证明，研究加权衰减对参数与梯度范数的影响，同时推导出最优批量大小并结合实验验证结论。

Result: 证明了加权衰减增强了范数界限的紧致性，并明确了衰减系数与学习率的关系；提出了最优批量大小以最小化计算复杂度。

Conclusion: Muon优化器在理论和实践上都展示了其在神经网络优化中的潜力。

Abstract: This paper presents a theoretical analysis of Muon, a new optimizer that
leverages the inherent matrix structure of neural network parameters. We
provide convergence proofs for four practical variants of Muon: with and
without Nesterov momentum, and with and without weight decay. We then show that
adding weight decay leads to strictly tighter bounds on both the parameter and
gradient norms, and we clarify the relationship between the weight decay
coefficient and the learning rate. Finally, we derive Muon's critical batch
size minimizing the stochastic first-order oracle (SFO) complexity, which is
the stochastic computational cost, and validate our theoretical findings with
experiments.

</details>


### [211] [Kernel Recursive Least Squares Dictionary Learning Algorithm](https://arxiv.org/abs/2507.01636)
*Ghasem Alipoor,Karl Skretting*

Main category: cs.LG

TL;DR: 提出了一种高效的在线核字典学习算法，能够以低计算复杂度实现高效、稀疏的信号表示，并在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有核字典学习方法难以兼顾计算效率和模型性能，尤其是在在线学习情境下亟需改进的方法。

Method: 使用递归最小二乘（RLS）方法开发了一种新颖的递归更新算法，更新过程中支持单样本或小批量样本处理，并确保在线学习的低计算复杂度。

Result: 在四个不同领域的数据集上，该方法不仅优于现有的在线核字典学习方法，还在效率显著提升的同时接近批量训练模型的分类精度。

Conclusion: 该方法在保持高效计算的同时提供了强大的在线稀疏表示能力，为高维核空间学习提供了一种实践解决方案。

Abstract: We propose an efficient online dictionary learning algorithm for kernel-based
sparse representations. In this framework, input signals are nonlinearly mapped
to a high-dimensional feature space and represented sparsely using a virtual
dictionary. At each step, the dictionary is updated recursively using a novel
algorithm based on the recursive least squares (RLS) method. This update
mechanism works with single samples or mini-batches and maintains low
computational complexity. Experiments on four datasets across different domains
show that our method not only outperforms existing online kernel dictionary
learning approaches but also achieves classification accuracy close to that of
batch-trained models, while remaining significantly more efficient.

</details>


### [212] [Dance Dance ConvLSTM](https://arxiv.org/abs/2507.01644)
*Miguel O'Malley*

Main category: cs.LG

TL;DR: 本文介绍了一种使用ConvLSTM模型自动生成《Dance Dance Revolution（DDR）》舞步的改进方法。


<details>
  <summary>Details</summary>
Motivation: 研究为《Dance Dance Revolution》游戏自动生成舞步，即DDR图表的方法。

Method: 使用ConvLSTM模型改进先前的CNN-LSTM方法，提高DDR舞步生成的准确性。

Result: 通过实验证明，新方法显著提升了生成的图表准确性。

Conclusion: 提出的ConvLSTM方法更优于此前的研究，能更准确地生成DDR舞步图表。

Abstract: \textit{Dance Dance Revolution} is a rhythm game consisting of songs and
accompanying choreography, referred to as charts. Players press arrows on a
device referred to as a dance pad in time with steps determined by the song's
chart. In 2017, the authors of Dance Dance Convolution (DDC) developed an
algorithm for the automatic generation of \textit{Dance Dance Revolution}
charts, utilizing a CNN-LSTM architecture. We introduce Dance Dance ConvLSTM
(DDCL), a new method for the automatic generation of DDR charts using a
ConvLSTM based model, which improves upon the DDC methodology and substantially
increases the accuracy of chart generation.

</details>


### [213] [Enhanced Generative Model Evaluation with Clipped Density and Coverage](https://arxiv.org/abs/2507.01761)
*Nicolas Salvy,Hugues Talbot,Bertrand Thirion*

Main category: cs.LG

TL;DR: 本文提出了两个新的评估生成模型质量的指标：剪辑密度和剪辑覆盖率，以提高其在鲁棒性、敏感性和可解释性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 目前生成模型的质量评估面临可靠性不足的问题，特别是在校准及对异常值的鲁棒性方面存在较大不足。

Method: 通过剪辑样本贡献和最近邻球的半径，提出了剪辑密度和剪辑覆盖率这两个指标，并结合分析与实验校准，确保指标评分线性反映模型样本质量比例。

Result: 经过合成数据和真实数据的广泛实验，证明剪辑密度和剪辑覆盖率在鲁棒性、灵敏度及可解释性方面优于现有方法。

Conclusion: 新的评估指标能够更直观且可靠地反映生成模型的样本质量，为关键应用提供坚实支持。

Abstract: Although generative models have made remarkable progress in recent years,
their use in critical applications has been hindered by their incapacity to
reliably evaluate sample quality. Quality refers to at least two complementary
concepts: fidelity and coverage. Current quality metrics often lack reliable,
interpretable values due to an absence of calibration or insufficient
robustness to outliers. To address these shortcomings, we introduce two novel
metrics, Clipped Density and Clipped Coverage. By clipping individual sample
contributions and, for fidelity, the radii of nearest neighbor balls, our
metrics prevent out-of-distribution samples from biasing the aggregated values.
Through analytical and empirical calibration, these metrics exhibit linear
score degradation as the proportion of poor samples increases. Thus, they can
be straightforwardly interpreted as equivalent proportions of good samples.
Extensive experiments on synthetic and real-world datasets demonstrate that
Clipped Density and Clipped Coverage outperform existing methods in terms of
robustness, sensitivity, and interpretability for evaluating generative models.

</details>


### [214] [BranchNet: A Neuro-Symbolic Learning Framework for Structured Multi-Class Classification](https://arxiv.org/abs/2507.01781)
*Dalia Rodríguez-Salas,Christian Riess*

Main category: cs.LG

TL;DR: BranchNet 提出了一种结合神经网络与符号表示的框架，用于将决策树转换为稀疏的神经网络，并在多分类任务中表现出超过 XGBoost 的性能。


<details>
  <summary>Details</summary>
Motivation: 通过结合决策树和神经网络的方法，旨在提高模型的性能、可解释性，并减少手动调节架构的需求。

Method: 将决策树的每个分支映射为隐藏神经元，保持符号结构同时利用梯度优化；训练过程优化稀疏性以提升效率和性能。

Result: BranchNet 在多分类任务中性能优于 XGBoost，展示了准确性上的显著提升。

Conclusion: BranchNet 实现了模型的紧凑性和可解释性，但在二元分类任务上仍需更进一步的适应性校准。

Abstract: We introduce BranchNet, a neuro-symbolic learning framework that transforms
decision tree ensembles into sparse, partially connected neural networks. Each
branch, defined as a decision path from root to a parent of leaves, is mapped
to a hidden neuron, preserving symbolic structure while enabling gradient-based
optimization. The resulting models are compact, interpretable, and require no
manual architecture tuning. Evaluated on a suite of structured multi-class
classification benchmarks, BranchNet consistently outperforms XGBoost in
accuracy, with statistically significant gains. We detail the architecture,
training procedure, and sparsity dynamics, and discuss the model's strengths in
symbolic interpretability as well as its current limitations, particularly on
binary tasks where further adaptive calibration may be beneficial.

</details>


### [215] [PERTINENCE: Input-based Opportunistic Neural Network Dynamic Execution](https://arxiv.org/abs/2507.01695)
*Omkar Shende,Gayathri Ananthanarayanan,Marcello Traiola*

Main category: cs.LG

TL;DR: 提出PERTINENCE，一种在线方法，通过动态选择合适模型来优化输入处理，实现高效性和准确性的兼顾。


<details>
  <summary>Details</summary>
Motivation: 面对大规模深度神经网络的高计算资源和能耗需求，开发方法以减少对大模型的依赖，同时保持输出精度。

Method: 引入PERTINENCE方法，使用遗传算法优化输入分派器，动态选择预训练模型进行输入处理，追求精度与计算效率的权衡。

Result: 在CIFAR-10、CIFAR-100和TinyImageNet数据集上的实验表明，与同类方法相比，PERTINENCE能在保证精度的基础上减少高达36%的操作量。

Conclusion: PERTINENCE通过整合现有模型的特性，实现了精度与效率的优化，为动态、基于输入的模型选择提供了新的方法。

Abstract: Deep neural networks (DNNs) have become ubiquitous thanks to their remarkable
ability to model complex patterns across various domains such as computer
vision, speech recognition, robotics, etc. While large DNN models are often
more accurate than simpler, lightweight models, they are also resource- and
energy-hungry. Hence, it is imperative to design methods to reduce reliance on
such large models without significant degradation in output accuracy. The high
computational cost of these models is often necessary only for a reduced set of
challenging inputs, while lighter models can handle most simple ones. Thus,
carefully combining properties of existing DNN models in a dynamic, input-based
way opens opportunities to improve efficiency without impacting accuracy.
  In this work, we introduce PERTINENCE, a novel online method designed to
analyze the complexity of input features and dynamically select the most
suitable model from a pre-trained set to process a given input effectively. To
achieve this, we employ a genetic algorithm to explore the training space of an
ML-based input dispatcher, enabling convergence towards the Pareto front in the
solution space that balances overall accuracy and computational efficiency.
  We showcase our approach on state-of-the-art Convolutional Neural Networks
(CNNs) trained on the CIFAR-10 and CIFAR-100, as well as Vision Transformers
(ViTs) trained on TinyImageNet dataset. We report results showing PERTINENCE's
ability to provide alternative solutions to existing state-of-the-art models in
terms of trade-offs between accuracy and number of operations. By
opportunistically selecting among models trained for the same task, PERTINENCE
achieves better or comparable accuracy with up to 36% fewer operations.

</details>


### [216] [Variational Graph Convolutional Neural Networks](https://arxiv.org/abs/2507.01699)
*Illia Oleksiienko,Juho Kanniainen,Alexandros Iosifidis*

Main category: cs.LG

TL;DR: 提出了一种用于空间和时空图卷积网络的不确定性估计方法，并展示其在多个数据集上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 提高图卷积网络的可解释性和模型的准确性，同时可以在关键应用中验证模型结果。

Method: 引入变分神经网络版本的空间和时空图卷积网络，并对输出和层级注意力进行不确定性估计。

Result: 在社会交易分析和基于骨骼的人类动作识别任务中，改进了模型的准确性，并估计了模型不确定性。

Conclusion: 该方法不仅提升了图卷积网络的性能，还增强了模型的可解释性和结果验证能力。

Abstract: Estimation of model uncertainty can help improve the explainability of Graph
Convolutional Networks and the accuracy of the models at the same time.
Uncertainty can also be used in critical applications to verify the results of
the model by an expert or additional models. In this paper, we propose
Variational Neural Network versions of spatial and spatio-temporal Graph
Convolutional Networks. We estimate uncertainty in both outputs and layer-wise
attentions of the models, which has the potential for improving model
explainability. We showcase the benefits of these models in the social trading
analysis and the skeleton-based human action recognition tasks on the Finnish
board membership, NTU-60, NTU-120 and Kinetics datasets, where we show
improvement in model accuracy in addition to estimated model uncertainties.

</details>


### [217] [B-PL-PINN: Stabilizing PINN Training with Bayesian Pseudo Labeling](https://arxiv.org/abs/2507.01714)
*Kevin Innerebner,Franz M. Rohrhofer,Bernhard C. Geiger*

Main category: cs.LG

TL;DR: 本文提出了一种基于贝叶斯PINN的物理信息神经网络训练方法，解决了以前存在的收敛问题。


<details>
  <summary>Details</summary>
Motivation: 解决PINN在正问题训练时信息传播困难和收敛性差的问题。

Method: 用贝叶斯PINN代替传统的PINN组合方法，并用后验方差评估取代一致性度量。

Result: 实验表明该方法在一系列基准问题上优于传统PINN组合方法，并表现出与Adam和LBFGS优化器结合的PINN集合可比的性能。

Conclusion: 贝叶斯PINN是一种数学上更严格的选择，为物理问题的神经网络建模提供了更高效的解决方案。

Abstract: Training physics-informed neural networks (PINNs) for forward problems often
suffers from severe convergence issues, hindering the propagation of
information from regions where the desired solution is well-defined.
Haitsiukevich and Ilin (2023) proposed an ensemble approach that extends the
active training domain of each PINN based on i) ensemble consensus and ii)
vicinity to (pseudo-)labeled points, thus ensuring that the information from
the initial condition successfully propagates to the interior of the
computational domain.
  In this work, we suggest replacing the ensemble by a Bayesian PINN, and
consensus by an evaluation of the PINN's posterior variance. Our experiments
show that this mathematically principled approach outperforms the ensemble on a
set of benchmark problems and is competitive with PINN ensembles trained with
combinations of Adam and LBFGS.

</details>


### [218] [MILP-SAT-GNN: Yet Another Neural SAT Solver](https://arxiv.org/abs/2507.01825)
*Franco Alberto Cardillo,Hamza Khyari,Umberto Straccia*

Main category: cs.LG

TL;DR: 利用用于混合整数线性规划的技术，提出了一种方法将k-CNF公式映射为加权二部图并通过图神经网络解决SAT问题，理论和实验上都展示了其效果。


<details>
  <summary>Details</summary>
Motivation: 希望利用图神经网络的表达能力，为SAT问题求解提供一种新视角，并结合已有在MILP中的技术加以实现。

Method: 通过将k-CNF公式映射为加权二部图，并通过图神经网络进行训练和测试，结合理论分析和实验验证方法的有效性。

Result: 实验表明，尽管采用的神经架构相对简单，但方法实现了有希望的结果，尤其针对不同类型的公式给出对SAT问题的有效求解能力。

Conclusion: 提出的基于图神经网络和二部图表示的SAT问题求解方法，在理论和实践上都具有一定优势，尤其是针对特定公式类型达到了高效求解效果。

Abstract: We proposes a novel method that enables Graph Neural Networks (GNNs) to solve
SAT problems by leveraging a technique developed for applying GNNs to Mixed
Integer Linear Programming (MILP). Specifically, k-CNF formulae are mapped into
MILP problems, which are then encoded as weighted bipartite graphs and
subsequently fed into a GNN for training and testing. From a theoretical
perspective: (i) we establish permutation and equivalence invariance results,
demonstrating that the method produces outputs that are stable under reordering
of clauses and variables; (ii) we identify a theoretical limitation, showing
that for a class of formulae called foldable formulae, standard GNNs cannot
always distinguish satisfiable from unsatisfiable instances; (iii) we prove a
universal approximation theorem, establishing that with Random Node
Initialization (RNI), the method can approximate SAT solving to arbitrary
precision on finite datasets, that is, the GNN becomes approximately sound and
complete on such datasets. Furthermore, we show that for unfoldable formulae,
the same approximation guarantee can be achieved without the need for RNI.
Finally, we conduct an experimental evaluation of our approach, which show
that, despite the simplicity of the neural architecture, the method achieves
promising results.

</details>


### [219] [Revisiting Learning Rate Control](https://arxiv.org/abs/2507.01724)
*Micha Henheik,Theresa Eimer,Marius Lindauer*

Main category: cs.LG

TL;DR: 本文比较了多种学习率控制的范式，并发现它们在某些任务中表现良好，但在不同情境中不够可靠，同时强调了算法选择的重要性。


<details>
  <summary>Details</summary>
Motivation: 学习率作为深度学习的重要超参数，控制它的方法对深度学习和自动机器学习（AutoML）研究至关重要。

Method: 将多种学习率控制方法进行比较，包括多保真超参数优化、固定超参数调度以及无超参数学习的方式，分析其在不同任务中的表现差异。

Result: 现有的方法尽管在某些特定任务中表现良好，但在不同的任务场景下表现不够一致，展示出可靠性问题。

Conclusion: 需要开发算法选择方法以更有效地控制学习率，同时考虑在任务复杂性增加时优化方法的局限性，并推荐关注与模型复杂性相匹配的新方向如微调方法和元学习。

Abstract: The learning rate is one of the most important hyperparameters in deep
learning, and how to control it is an active area within both AutoML and deep
learning research. Approaches for learning rate control span from classic
optimization to online scheduling based on gradient statistics. This paper
compares paradigms to assess the current state of learning rate control. We
find that methods from multi-fidelity hyperparameter optimization,
fixed-hyperparameter schedules, and hyperparameter-free learning often perform
very well on selected deep learning tasks but are not reliable across settings.
This highlights the need for algorithm selection methods in learning rate
control, which have been neglected so far by both the AutoML and deep learning
communities. We also observe a trend of hyperparameter optimization approaches
becoming less effective as models and tasks grow in complexity, even when
combined with multi-fidelity approaches for more expensive model trainings. A
focus on more relevant test tasks and new promising directions like finetunable
methods and meta-learning will enable the AutoML community to significantly
strengthen its impact on this crucial factor in deep learning.

</details>


### [220] [mGRADE: Minimal Recurrent Gating Meets Delay Convolutions for Lightweight Sequence Modeling](https://arxiv.org/abs/2507.01829)
*Tristan Torchet,Christian Metzner,Laura Kriener,Melika Payvand*

Main category: cs.LG

TL;DR: 本文提出mGRADE，一种结合1D卷积和轻量化门控递归单元(minGRU)的混合记忆系统，旨在解决边缘设备上的多尺度时间处理问题，同时显著降低内存需求。


<details>
  <summary>Details</summary>
Motivation: 目标是为边缘设备上的时间序列处理任务设计一种高效的模型，能够同时捕获短期和长期动态特性并满足内存约束。

Method: mGRADE采用一种基于1D时间卷积和最小门控递归单元(minGRU)的混合架构，卷积用于实现灵活的时延嵌入以捕获快速的时间变化，递归模块以最低的内存开销维护全局上下文。

Result: 在两个合成任务和图像分类任务中验证了mGRADE的性能，其在内存占用减少约20%的情况下，表现超越了纯卷积和纯递归模型。

Conclusion: mGRADE是一种高效的多尺度时间处理解决方案，尤其适合内存受限的边缘设备应用环境。

Abstract: Edge devices for temporal processing demand models that capture both short-
and long- range dynamics under tight memory constraints. While Transformers
excel at sequence modeling, their quadratic memory scaling with sequence length
makes them impractical for such settings. Recurrent Neural Networks (RNNs)
offer constant memory but train sequentially, and Temporal Convolutional
Networks (TCNs), though efficient, scale memory with kernel size. To address
this, we propose mGRADE (mininally Gated Recurrent Architecture with Delay
Embedding), a hybrid-memory system that integrates a temporal 1D-convolution
with learnable spacings followed by a minimal gated recurrent unit (minGRU).
This design allows the convolutional layer to realize a flexible delay
embedding that captures rapid temporal variations, while the recurrent module
efficiently maintains global context with minimal memory overhead. We validate
our approach on two synthetic tasks, demonstrating that mGRADE effectively
separates and preserves multi-scale temporal features. Furthermore, on
challenging pixel-by-pixel image classification benchmarks, mGRADE consistently
outperforms both pure convolutional and pure recurrent counterparts using
approximately 20% less memory footprint, highlighting its suitability for
memory-constrained temporal processing at the edge. This highlights mGRADE's
promise as an efficient solution for memory-constrained multi-scale temporal
processing at the edge.

</details>


### [221] [A Real-Time Digital Twin for Type 1 Diabetes using Simulation-Based Inference](https://arxiv.org/abs/2507.01740)
*Trung-Dung Hoang,Alceu Bissoto,Vihangkumar V. Naik,Tim Flühmann,Artemii Shlychkov,José Garcia-Tirado,Lisa M. Koch*

Main category: cs.LG

TL;DR: 本文提出基于神经后验估计的模拟推断方法，用于高效建模1型糖尿病中复杂的葡萄糖-胰岛素动态关系，结果表明该方法比传统方法性能更优，且适应性更强。


<details>
  <summary>Details</summary>
Motivation: 现有基于马尔可夫链蒙特卡罗的参数估计方法在高维参数空间下效率低下，且推断时需从零开始拟合参数，难以满足实时性需求，因此需开发更高效的推断方法。

Method: 本文提出一种基于神经后验估计的模拟推断方法，通过捕获进餐、胰岛素和血糖水平之间的复杂关系，实现快速且支持不确定性量化的参数推断。

Result: 实验表明，该方法不仅在参数估计上优于传统方法，还能更好地适应未知条件，提供实时高效推断并具有可靠的不确定性量化能力。

Conclusion: 基于模拟推断的神经后验估计方法是一种在1型糖尿病建模中高效可靠的解决方案，具有比传统方法更好的性能和通用性。

Abstract: Accurately estimating parameters of physiological models is essential to
achieving reliable digital twins. For Type 1 Diabetes, this is particularly
challenging due to the complexity of glucose-insulin interactions. Traditional
methods based on Markov Chain Monte Carlo struggle with high-dimensional
parameter spaces and fit parameters from scratch at inference time, making them
slow and computationally expensive. In this study, we propose a
Simulation-Based Inference approach based on Neural Posterior Estimation to
efficiently capture the complex relationships between meal intake, insulin, and
glucose level, providing faster, amortized inference. Our experiments
demonstrate that SBI not only outperforms traditional methods in parameter
estimation but also generalizes better to unseen conditions, offering real-time
posterior inference with reliable uncertainty quantification.

</details>


### [222] [Towards Foundation Auto-Encoders for Time-Series Anomaly Detection](https://arxiv.org/abs/2507.01875)
*Gastón García González,Pedro Casas,Emilio Martínez,Alicia Fernández*

Main category: cs.LG

TL;DR: 我们研究了一种新颖的时间序列建模方法，提出了一种名为FAE（Foundation Auto-Encoders）的基础生成式AI模型，用于时间序列数据的异常检测。


<details>
  <summary>Details</summary>
Motivation: 受大规模预训练基础模型成功的启发，设计了一种能在未见数据集上实现高度建模、预测和异常检测的模型。

Method: 模型基于变分自编码器（VAE）和膨胀卷积神经网络（DCNNs），使用预训练的方法学习时间序列复杂的时间特征。

Result: 提出的FAE模型在多维时间序列数据集，包括来自运营商的真实数据和KDD 2021异常检测数据集上获得初步结果。

Conclusion: FAE提供了一种通用的单变量时间序列建模方法，可实现即开即用的零样本异常检测应用。

Abstract: We investigate a novel approach to time-series modeling, inspired by the
successes of large pretrained foundation models. We introduce FAE (Foundation
Auto-Encoders), a foundation generative-AI model for anomaly detection in
time-series data, based on Variational Auto-Encoders (VAEs). By foundation, we
mean a model pretrained on massive amounts of time-series data which can learn
complex temporal patterns useful for accurate modeling, forecasting, and
detection of anomalies on previously unseen datasets. FAE leverages VAEs and
Dilated Convolutional Neural Networks (DCNNs) to build a generic model for
univariate time-series modeling, which could eventually perform properly in
out-of-the-box, zero-shot anomaly detection applications. We introduce the main
concepts of FAE, and present preliminary results in different multi-dimensional
time-series datasets from various domains, including a real dataset from an
operational mobile ISP, and the well known KDD 2021 Anomaly Detection dataset.

</details>


### [223] [Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection](https://arxiv.org/abs/2507.01924)
*Samirah Bakker,Yao Ma,Seyed Sahand Mohammadi Ziabari*

Main category: cs.LG

TL;DR: 本文研究通过结合LSTM网络和Transformer，并利用Isolation Forest（iForest）与Autoencoders（AE）进行伪标签化的方法，检测心理健康护理账单中的异常情况。


<details>
  <summary>Details</summary>
Motivation: 心理健康护理账单的复杂性可能导致异常，如欺诈行为，而现有机器学习技术在处理类别不平衡、标签稀缺及复杂时序模式方面存在困难。本文旨在探讨一种结合深度学习与伪标签化的混合方法解决这些问题。

Method: 提出一种混合深度学习方法，包括LSTM、Transformer网络，并结合iForest和AE进行伪标签化。在两个心理健康护理账单的真实数据集上测试效果。

Result: iForest LSTM模型在声明级数据上召回率达0.963；在操作级数据上，基于iForest的混合模型召回率最高为0.744，但精度较低。

Conclusion: 结合伪标签化与混合深度学习在处理复杂、不平衡的异常检测场景中显示出潜力。

Abstract: The complexity of mental healthcare billing enables anomalies, including
fraud. While machine learning methods have been applied to anomaly detection,
they often struggle with class imbalance, label scarcity, and complex
sequential patterns. This study explores a hybrid deep learning approach
combining Long Short-Term Memory (LSTM) networks and Transformers, with
pseudo-labeling via Isolation Forests (iForest) and Autoencoders (AE). Prior
work has not evaluated such hybrid models trained on pseudo-labeled data in the
context of healthcare billing. The approach is evaluated on two real-world
billing datasets related to mental healthcare. The iForest LSTM baseline
achieves the highest recall (0.963) on declaration-level data. On the
operation-level data, the hybrid iForest-based model achieves the highest
recall (0.744), though at the cost of lower precision. These findings highlight
the potential of combining pseudo-labeling with hybrid deep learning in
complex, imbalanced anomaly detection settings.

</details>


### [224] [Towards Decentralized and Sustainable Foundation Model Training with the Edge](https://arxiv.org/abs/2507.01803)
*Leyang Xue,Meghana Madhyastha,Randal Burns,Myungjin Lee,Mahesh K. Marina*

Main category: cs.LG

TL;DR: 本文提出了一种利用边缘设备分散式计算资源进行基础模型训练的新方法，以解决其高计算需求带来的环境及控制问题。


<details>
  <summary>Details</summary>
Motivation: 目前基础模型需要庞大的计算资源，这带来了环境影响及开发控制集中的问题。作者希望通过边缘设备的分布式计算来实现更加可持续的发展模式。

Method: 提出通过人人参与的边缘设备连接，充分利用其闲置计算资源，进行分布式训练模型。

Result: 本文重点是提供关于这种方法实现的潜在挑战的框架和设想，但未提及具体实验结果。

Conclusion: 若实现，可以显著提升基础模型训练的可持续性，并减轻目前集中化模型训练的问题，同时推动AI发展向更开放的方向发展。

Abstract: Foundation models are at the forefront of AI research, appealing for their
ability to learn from vast datasets and cater to diverse tasks. Yet, their
significant computational demands raise issues of environmental impact and the
risk of centralized control in their development. We put forward a vision
towards decentralized and sustainable foundation model training that leverages
the collective compute of sparingly used connected edge AI devices. We present
the rationale behind our vision, particularly in support of its sustainability
benefit. We further outline a set of challenges that need to be addressed to
turn this vision into reality.

</details>


### [225] [TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents](https://arxiv.org/abs/2507.01823)
*Dmytro Kuzmenko,Nadiya Shvai*

Main category: cs.LG

TL;DR: 本文提出了一种用于模型强化学习的新方法，通过高效蒸馏从高容量多任务代理（317M参数）提炼出紧凑模型（1M参数）。


<details>
  <summary>Details</summary>
Motivation: 解决大规模世界模型在资源受限环境中的部署问题，提供一种更高效的多任务强化学习系统。

Method: 通过一种高效的蒸馏技术，在MT30基准上从317M参数的高容量模型提炼出1M参数的紧凑模型，并通过FP16后训练量化进一步优化模型大小。

Result: 蒸馏后的模型在MT30基准中的标准化得分为28.45，优于原始1M参数模型的18.93，同时模型大小减少了50%。

Conclusion: 该方法证明了蒸馏技术在多任务知识捕获与整合方面的优势，为资源受限环境中的强化学习部署提供了重要参考。

Abstract: We present a novel approach to knowledge transfer in model-based
reinforcement learning, addressing the critical challenge of deploying large
world models in resource-constrained environments. Our method efficiently
distills a high-capacity multi-task agent (317M parameters) into a compact
model (1M parameters) on the MT30 benchmark, significantly improving
performance across diverse tasks. Our distilled model achieves a
state-of-the-art normalized score of 28.45, surpassing the original 1M
parameter model score of 18.93. This improvement demonstrates the ability of
our distillation technique to capture and consolidate complex multi-task
knowledge. We further optimize the distilled model through FP16 post-training
quantization, reducing its size by $\sim$50\%. Our approach addresses practical
deployment limitations and offers insights into knowledge representation in
large world models, paving the way for more efficient and accessible multi-task
reinforcement learning systems in robotics and other resource-constrained
applications. Code available at https://github.com/dmytro-kuzmenko/td-mpc-opt.

</details>


### [226] [Out-of-Distribution Detection Methods Answer the Wrong Questions](https://arxiv.org/abs/2507.01831)
*Yucen Lily Li,Daohan Lu,Polina Kirichenko,Shikai Qiu,Tim G. J. Rudner,C. Bayan Bruss,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 本文重新审视了基于预测不确定性和特征的OOD检测方法，指出它们在OOD检测问题上存在根本错位，并揭示了不可避免的错误来源。


<details>
  <summary>Details</summary>
Motivation: 越来越多的研究尝试利用预测不确定性或特征作为OOD检测的基础，但作者认为这些方法在OOD识别方面有根本性的局限。

Method: 通过分析现有方法，作者指出预测不确定性不能准确表达OOD，特征距离也不能代表OOD点，并检验了多种方法在应对这些挑战时的失败情况。

Result: 特征-逻辑混合方法、模型和数据规模扩展、以及其他改善尝试都未能解决OOD检测目标的根本错位问题。

Conclusion: 本文揭示了现有OOD检测方法和目标之间的错位，并指出需超越现行方法重新设计解决方案。

Abstract: To detect distribution shifts and improve model safety, many
out-of-distribution (OOD) detection methods rely on the predictive uncertainty
or features of supervised models trained on in-distribution data. In this
paper, we critically re-examine this popular family of OOD detection
procedures, and we argue that these methods are fundamentally answering the
wrong questions for OOD detection. There is no simple fix to this misalignment,
since a classifier trained only on in-distribution classes cannot be expected
to identify OOD points; for instance, a cat-dog classifier may confidently
misclassify an airplane if it contains features that distinguish cats from
dogs, despite generally appearing nothing alike. We find that uncertainty-based
methods incorrectly conflate high uncertainty with being OOD, while
feature-based methods incorrectly conflate far feature-space distance with
being OOD. We show how these pathologies manifest as irreducible errors in OOD
detection and identify common settings where these methods are ineffective.
Additionally, interventions to improve OOD detection such as feature-logit
hybrid methods, scaling of model and data size, epistemic uncertainty
representation, and outlier exposure also fail to address this fundamental
misalignment in objectives. We additionally consider unsupervised density
estimation and generative models for OOD detection, which we show have their
own fundamental limitations.

</details>


### [227] [Automatic Rank Determination for Low-Rank Adaptation via Submodular Function Maximization](https://arxiv.org/abs/2507.01841)
*Yihang Gao,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 本文提出了一种名为SubLoRA的新方法，用于通过次模函数最大化来确定低秩适配(LoRA)的秩。


<details>
  <summary>Details</summary>
Motivation: 当前的方法如AdaLoRA依赖于损失函数的一阶线性近似，然而当LoRA参数经过良好优化后，这种线性化方法变得不准确并且具有调节问题，因而需要引入二阶信息以更好地刻画复杂的误差面。

Method: 通过构造二阶海森矩阵信息，将秩确定问题重新定义为具有二次目标的组合优化问题。尽管这个问题通常是NP难的，但作者提出了一个基于次模函数最大化的贪婪算法，并设计出了具有逼近保证的优化方法，同时确保计算效率。

Result: 在将SubLoRA应用于物理信息神经网络(PINNs)的微分方程求解任务中，其结果显示在秩确定和联合训练性能方面优于现有方法。

Conclusion: SubLoRA有效结合了理论基础、二阶准确性以及计算的可行性，提供了一个在切实约束下表现良好的低秩优化方法。

Abstract: In this paper, we propose SubLoRA, a rank determination method for Low-Rank
Adaptation (LoRA) based on submodular function maximization. In contrast to
prior approaches, such as AdaLoRA, that rely on first-order (linearized)
approximations of the loss function, SubLoRA utilizes second-order information
to capture the potentially complex loss landscape by incorporating the Hessian
matrix. We show that the linearization becomes inaccurate and ill-conditioned
when the LoRA parameters have been well optimized, motivating the need for a
more reliable and nuanced second-order formulation. To this end, we reformulate
the rank determination problem as a combinatorial optimization problem with a
quadratic objective. However, solving this problem exactly is NP-hard in
general. To overcome the computational challenge, we introduce a submodular
function maximization framework and devise a greedy algorithm with
approximation guarantees. We derive a sufficient and necessary condition under
which the rank-determination objective becomes submodular, and construct a
closed-form projection of the Hessian matrix that satisfies this condition
while maintaining computational efficiency. Our method combines solid
theoretical foundations, second-order accuracy, and practical computational
efficiency. We further extend SubLoRA to a joint optimization setting,
alternating between LoRA parameter updates and rank determination under a rank
budget constraint. Extensive experiments on fine-tuning physics-informed neural
networks (PINNs) for solving partial differential equations (PDEs) demonstrate
the effectiveness of our approach. Results show that SubLoRA outperforms
existing methods in both rank determination and joint training performance.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [228] [Diversity-Preserving Exploitation of Crossover](https://arxiv.org/abs/2507.01524)
*Johannes Lengler,Tom Offermann*

Main category: cs.NE

TL;DR: 提出了一种新的交叉利用范式DiPEC（多样性保持交叉利用），并开发了Diversity Exploitation Genetic Algorithm (DEGA)，证明在LeadingOnes问题中有明显的性能提升，并进行了理论和实证验证。


<details>
  <summary>Details</summary>
Motivation: 解决交叉操作中多样性减少导致效能下降的问题，提高遗传算法性能。

Method: 提出DiPEC（多样性保持交叉利用）范式，开发出DEGA方法并通过理论分析和仿真验证其性能。

Result: 在LeadingOnes问题中，DEGA显著提升了效率，从标准遗传算法的$\Theta(n^2)$下降到$O(n^{5/3}\log^{2/3} n)$评估数，并在其他基准测试中展现了竞争力。

Conclusion: DiPEC减少了交叉操作中多样性降低的问题，使遗传算法在多种环境下表现更优，为后续研究提供了新方向。

Abstract: Crossover is a powerful mechanism for generating new solutions from a given
population of solutions. Crossover comes with a discrepancy in itself: on the
one hand, crossover usually works best if there is enough diversity in the
population; on the other hand, exploiting the benefits of crossover reduces
diversity. This antagonism often makes crossover reduce its own effectiveness.
  We introduce a new paradigm for utilizing crossover that reduces this
antagonism, which we call diversity-preserving exploitation of crossover
(DiPEC). The resulting Diversity Exploitation Genetic Algorithm (DEGA) is able
to still exploit the benefits of crossover, but preserves a much higher
diversity than conventional approaches.
  We demonstrate the benefits by proving that the (2+1)-DEGA finds the optimum
of LeadingOnes with $O(n^{5/3}\log^{2/3} n)$ fitness evaluations. This is
remarkable since standard genetic algorithms need $\Theta(n^2)$ evaluations,
and among genetic algorithms only some artificial and specifically tailored
algorithms were known to break this runtime barrier. We confirm the theoretical
results by simulations. Finally, we show that the approach is not overfitted to
Leadingones by testing it empirically on other benchmarks and showing that it
is also competitive in other settings. We believe that our findings justify
further systematic investigations of the DiPEC paradigm.

</details>


### [229] [Adaptive Estimation of the Number of Algorithm Runs in Stochastic Optimization](https://arxiv.org/abs/2507.01629)
*Tome Eftimov,Peter Korošec*

Main category: cs.NE

TL;DR: 该论文提出一种经验方法，动态估计单目标随机优化算法所需运行次数。


<details>
  <summary>Details</summary>
Motivation: 实验过程中确定运行次数影响实验的持续时间和结果可靠性。希望通过一种方法减少运行次数而不影响性能估计准确性，从而提高效率。

Method: 通过概率理论，并引入鲁棒性检查数据分布是否相对平均，动态调整运行次数，实现在线估算。

Result: 算法估算准确度达到82%-95%，在不影响优化结果的情况下，减少约50%的运行次数。

Conclusion: 该方法提高了计算效率，减少约50%运行次数，不仅优化基准测试能效，还降低能源消耗，为可持续计算做出贡献。

Abstract: Determining the number of algorithm runs is a critical aspect of experimental
design, as it directly influences the experiment's duration and the reliability
of its outcomes. This paper introduces an empirical approach to estimating the
required number of runs per problem instance for accurate estimation of the
performance of the continuous single-objective stochastic optimization
algorithm. The method leverages probability theory, incorporating a robustness
check to identify significant imbalances in the data distribution relative to
the mean, and dynamically adjusts the number of runs during execution as an
online approach. The proposed methodology was extensively tested across two
algorithm portfolios (104 Differential Evolution configurations and the
Nevergrad portfolio) and the COCO benchmark suite, totaling 5748000 runs. The
results demonstrate 82% - 95% accuracy in estimations across different
algorithms, allowing a reduction of approximately 50% in the number of runs
without compromising optimization outcomes. This online calculation of required
runs not only improves benchmarking efficiency, but also contributes to energy
reduction, fostering a more environmentally sustainable computing ecosystem.

</details>


### [230] [Customized Exploration of Landscape Features Driving Multi-Objective Combinatorial Optimization Performance](https://arxiv.org/abs/2507.01638)
*Ana Nikolikj,Gabriela Ochoa,Tome Eftimov*

Main category: cs.NE

TL;DR: 该研究分析了使用组合性景观的特征来预测多目标优化算法的性能，评估了三种算法的表现并揭示了特定特征对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是通过分析景观特征更好地理解其对多目标组合优化算法性能的影响。

Method: 使用压缩的Pareto局部最优解网络模型（C-PLOS-net）提取景观特征，分析了三种算法在不同景观下的性能表现，并使用分辨率和超体积两种指标。

Result: 研究表明，不同景观特征组合会显著影响某些场景下算法的性能表现，揭示了对此类特定景观和算法的性能影响。

Conclusion: 该研究提供了关于特定景观特征对多目标优化算法性能影响的新见解，有助于优化设计针对性的优化算法模型。

Abstract: We present an analysis of landscape features for predicting the performance
of multi-objective combinatorial optimization algorithms. We consider features
from the recently proposed compressed Pareto Local Optimal Solutions Networks
(C-PLOS-net) model of combinatorial landscapes. The benchmark instances are a
set of rmnk-landscapes with 2 and 3 objectives and various levels of ruggedness
and objective correlation. We consider the performance of three algorithms --
Pareto Local Search (PLS), Global Simple EMO Optimizer (GSEMO), and
Non-dominated Sorting Genetic Algorithm (NSGA-II) - using the resolution and
hypervolume metrics. Our tailored analysis reveals feature combinations that
influence algorithm performance specific to certain landscapes. This study
provides deeper insights into feature importance, tailored to specific
rmnk-landscapes and algorithms.

</details>


### [231] [Comparing Optimization Algorithms Through the Lens of Search Behavior Analysis](https://arxiv.org/abs/2507.01668)
*Gjorgjina Cenikj,Gašper Petelin,Tome Eftimov*

Main category: cs.NE

TL;DR: 对114种优化算法使用统计检验分析其搜索行为，试图辨别行为相似的算法。


<details>
  <summary>Details</summary>
Motivation: 优化领域涌现了大量基于隐喻的新算法，但缺乏实质创新且难以区分，因此需开发更有效的比较方法。

Method: 使用cross-match统计检验比较多元分布，分析MEALPY库中114种算法的搜索行为。

Result: 通过统计检验得出不同算法在搜索行为上的相似性。

Conclusion: 统计检验方法可用于分析优化算法的行为差异，为算法评价提供新思路。

Abstract: The field of numerical optimization has recently seen a surge in the
development of "novel" metaheuristic algorithms, inspired by metaphors derived
from natural or human-made processes, which have been widely criticized for
obscuring meaningful innovations and failing to distinguish themselves from
existing approaches. Aiming to address these concerns, we investigate the
applicability of statistical tests for comparing algorithms based on their
search behavior. We utilize the cross-match statistical test to compare
multivariate distributions and assess the solutions produced by 114 algorithms
from the MEALPY library. These findings are incorporated into an empirical
analysis aiming to identify algorithms with similar search behaviors.

</details>
