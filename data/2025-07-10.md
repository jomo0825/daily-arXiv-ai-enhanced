<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 85]
- [cs.CL](#cs.CL) [Total: 44]
- [cs.AI](#cs.AI) [Total: 8]
- [cs.LG](#cs.LG) [Total: 72]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Unveiling the Underwater World: CLIP Perception Model-Guided Underwater Image Enhancement](https://arxiv.org/abs/2507.06234)
*Jiangzhong Cao,Zekai Zeng,Xu Zhang,Huan Zhang,Chunling Fan,Gangyi Jiang,Weisi Lin*

Main category: cs.CV

TL;DR: 提出了一种融合CLIP感知损失模块和课程对比正则化的水下图像增强方法，有效提升增强图像的感知质量和内容还原能力。


<details>
  <summary>Details</summary>
Motivation: 现有水下图像增强的深度学习方法忽视了对人类视觉感知的考虑，缺乏对解空间的约束，导致增强后的图像感知质量差或内容还原效果不佳。

Method: 通过引入CLIP感知损失模块和课程对比正则化，利用CLIP模型的视觉语义特征提取能力开发水下图像感知模型，并将其作为感知损失模块集成到增强网络中；结合课程对比正则化，改进对增强图像在CLIP感知空间内的约束能力。

Result: 实验表明，所提出的方法在视觉质量和泛化能力方面优于其他最先进方法。

Conclusion: 该方法有效提升了水下图像增强的感知质量和细节还原效果，为相关领域研究提供了新的方向。

Abstract: High-quality underwater images are essential for both machine vision tasks
and viewers with their aesthetic appeal.However, the quality of underwater
images is severely affected by light absorption and scattering. Deep
learning-based methods for Underwater Image Enhancement (UIE) have achieved
good performance. However, these methods often overlook considering human
perception and lack sufficient constraints within the solution space.
Consequently, the enhanced images often suffer from diminished perceptual
quality or poor content restoration.To address these issues, we propose a UIE
method with a Contrastive Language-Image Pre-Training (CLIP) perception loss
module and curriculum contrastive regularization. Above all, to develop a
perception model for underwater images that more aligns with human visual
perception, the visual semantic feature extraction capability of the CLIP model
is leveraged to learn an appropriate prompt pair to map and evaluate the
quality of underwater images. This CLIP perception model is then incorporated
as a perception loss module into the enhancement network to improve the
perceptual quality of enhanced images. Furthermore, the CLIP perception model
is integrated with the curriculum contrastive regularization to enhance the
constraints imposed on the enhanced images within the CLIP perceptual space,
mitigating the risk of both under-enhancement and over-enhancement.
Specifically, the CLIP perception model is employed to assess and categorize
the learning difficulty level of negatives in the regularization process,
ensuring comprehensive and nuanced utilization of distorted images and
negatives with varied quality levels. Extensive experiments demonstrate that
our method outperforms state-of-the-art methods in terms of visual quality and
generalization ability.

</details>


### [2] [SPARC: Concept-Aligned Sparse Autoencoders for Cross-Model and Cross-Modal Interpretability](https://arxiv.org/abs/2507.06265)
*Ali Nasiri-Sarvi,Hassan Rivaz,Mahdi S. Hosseini*

Main category: cs.CV

TL;DR: 提出了SPARC框架，通过学习统一的稀疏潜在空间，跨架构和模态对齐高层概念表征，大幅提高模型间概念一致性。


<details>
  <summary>Details</summary>
Motivation: 现有模型表征高层概念不统一，缺乏跨模型解释性方法。

Method: 利用SPARC框架中的全局TopK稀疏机制与跨重构损失，实现跨架构统一的潜在概念表征空间。

Result: 在Open Images上，SPARC的概念对齐度Jaccard相似性达到0.80，比之前方法提高三倍以上。

Conclusion: SPARC对齐了不同模型的潜在空间，无需人工对齐或特定模型分析，应用于文本引导空间定位及跨模型检索。

Abstract: Understanding how different AI models encode the same high-level concepts,
such as objects or attributes, remains challenging because each model typically
produces its own isolated representation. Existing interpretability methods
like Sparse Autoencoders (SAEs) produce latent concepts individually for each
model, resulting in incompatible concept spaces and limiting cross-model
interpretability. To address this, we introduce SPARC (Sparse Autoencoders for
Aligned Representation of Concepts), a new framework that learns a single,
unified latent space shared across diverse architectures and modalities (e.g.,
vision models like DINO, and multimodal models like CLIP). SPARC's alignment is
enforced through two key innovations: (1) a Global TopK sparsity mechanism,
ensuring all input streams activate identical latent dimensions for a given
concept; and (2) a Cross-Reconstruction Loss, which explicitly encourages
semantic consistency between models. On Open Images, SPARC dramatically
improves concept alignment, achieving a Jaccard similarity of 0.80, more than
tripling the alignment compared to previous methods. SPARC creates a shared
sparse latent space where individual dimensions often correspond to similar
high-level concepts across models and modalities, enabling direct comparison of
how different architectures represent identical concepts without requiring
manual alignment or model-specific analysis. As a consequence of this aligned
representation, SPARC also enables practical applications such as text-guided
spatial localization in vision-only models and cross-model/cross-modal
retrieval. Code and models are available at
https://github.com/AtlasAnalyticsLab/SPARC.

</details>


### [3] [A Probabilistic Approach to Uncertainty Quantification Leveraging 3D Geometry](https://arxiv.org/abs/2507.06269)
*Rushil Desai,Frederik Warburg,Trevor Darrell,Marissa Ramirez de Chanlatte*

Main category: cs.CV

TL;DR: BayesSDF是一种用于神经隐式SDF模型的不确定性量化新框架，在科学仿真应用中表现优异，具有高效的表面意识的不确定性估算能力。


<details>
  <summary>Details</summary>
Motivation: 提出BayesSDF以解决当前SDF不确定性建模中的计算效率低、可扩展性差和几何不一致等问题，特别针对于科学模拟环境中对高保真和表面几何的不确定性需求。

Method: 使用Laplace近似及基于Hessian的度量方法，量化局部表面不稳定性，实现高效的、不依赖辐射模型的表面不确定性估计框架。

Result: 在综合的合成和实际数据集评估中，BayesSDF在校准和几何一致性方面优于现有方法。

Conclusion: BayesSDF建立了不确定性感知的3D场景重建、模拟及机器人决策的坚实基础。

Abstract: Quantifying uncertainty in neural implicit 3D representations, particularly
those utilizing Signed Distance Functions (SDFs), remains a substantial
challenge due to computational inefficiencies, scalability issues, and
geometric inconsistencies. Existing methods typically neglect direct geometric
integration, leading to poorly calibrated uncertainty maps. We introduce
BayesSDF, a novel probabilistic framework for uncertainty quantification in
neural implicit SDF models, motivated by scientific simulation applications
with 3D environments (e.g., forests) such as modeling fluid flow through
forests, where precise surface geometry and awareness of fidelity surface
geometric uncertainty are essential. Unlike radiance-based models such as NeRF
or 3D Gaussian splatting, which lack explicit surface formulations, SDFs define
continuous and differentiable geometry, making them better suited for physical
modeling and analysis. BayesSDF leverages a Laplace approximation to quantify
local surface instability via Hessian-based metrics, enabling computationally
efficient, surface-aware uncertainty estimation. Our method shows that
uncertainty predictions correspond closely with poorly reconstructed geometry,
providing actionable confidence measures for downstream use. Extensive
evaluations on synthetic and real-world datasets demonstrate that BayesSDF
outperforms existing methods in both calibration and geometric consistency,
establishing a strong foundation for uncertainty-aware 3D scene reconstruction,
simulation, and robotic decision-making.

</details>


### [4] [LIRA: Inferring Segmentation in Large Multi-modal Models with Local Interleaved Region Assistance](https://arxiv.org/abs/2507.06272)
*Zhang Li,Biao Yang,Qiang Liu,Shuo Zhang,Zhiyin Ma,Shuo Zhang,Liang Yin,Linger Deng,Yabo Sun,Yuliang Liu,Xiang Bai*

Main category: cs.CV

TL;DR: 提出LIRA框架，通过语义增强特征提取器（SEFE）和交错式局部视觉耦合（ILVC）解决LMMs的分割不精确和理解偏差问题，提升分割和理解任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的LMMs在分割和理解方面存在精准度和偏差问题，这来源于视觉理解能力不足和缺乏细粒度感知。

Method: 通过LIRA框架，包括语义增强特征提取器融合语义与像素特征提高分割精度，以及交错式局部视觉耦合生成局部描述以减少偏差，并引入Attributes Evaluation数据集量化分割与语义推断能力关系。

Result: 实验表明，LIRA在分割和理解任务中取得了最先进的性能表现。

Conclusion: LIRA框架通过结合视觉理解和分割，显著提升LMMs在相关任务中的性能，证明其对解决现有模型局限性的有效性。

Abstract: While large multi-modal models (LMMs) demonstrate promising capabilities in
segmentation and comprehension, they still struggle with two limitations:
inaccurate segmentation and hallucinated comprehension. These challenges stem
primarily from constraints in weak visual comprehension and a lack of
fine-grained perception. To alleviate these limitations, we propose LIRA, a
framework that capitalizes on the complementary relationship between visual
comprehension and segmentation via two key components: (1) Semantic-Enhanced
Feature Extractor (SEFE) improves object attribute inference by fusing semantic
and pixel-level features, leading to more accurate segmentation; (2)
Interleaved Local Visual Coupling (ILVC) autoregressively generates local
descriptions after extracting local features based on segmentation masks,
offering fine-grained supervision to mitigate hallucinations. Furthermore, we
find that the precision of object segmentation is positively correlated with
the latent related semantics of the <seg> token. To quantify this relationship
and the model's potential semantic inferring ability, we introduce the
Attributes Evaluation (AttrEval) dataset. Our experiments show that LIRA
achieves state-of-the-art performance in both segmentation and comprehension
tasks. Code will be available at https://github.com/echo840/LIRA.

</details>


### [5] [Advancing Offline Handwritten Text Recognition: A Systematic Review of Data Augmentation and Generation Techniques](https://arxiv.org/abs/2507.06275)
*Yassin Hussein Rassul,Aram M. Ahmed,Polla Fattah,Bryar A. Hassan,Arwaa W. Abdulkareem,Tarik A. Rashid,Joan Lu*

Main category: cs.CV

TL;DR: 本文调查了离线手写文本识别中的数据增强和生成技术，重点关注数据稀缺问题和跨语言/脚本的挑战。


<details>
  <summary>Details</summary>
Motivation: 旨在解决手写文本识别系统因训练数据不足尤其是低资源语言和复杂脚本数据导致的性能瓶颈。

Method: 系统回顾了传统增强方法与深度学习新进展（包括GANs、扩散模型和Transformer），基于PRISMA方法从1302篇文献筛选出848篇，分析数据集、评估指标等。

Result: 总结出当前研究的不足，明确关键研究空白，并为手写生成技术的未来发展提出建议。

Conclusion: 本研究为提升手写文本识别系统的精度与鲁棒性提供了全面的技术支持，并强调了多语言、多风格领域的潜在研究方向。

Abstract: Offline Handwritten Text Recognition (HTR) systems play a crucial role in
applications such as historical document digitization, automatic form
processing, and biometric authentication. However, their performance is often
hindered by the limited availability of annotated training data, particularly
for low-resource languages and complex scripts. This paper presents a
comprehensive survey of offline handwritten data augmentation and generation
techniques designed to improve the accuracy and robustness of HTR systems. We
systematically examine traditional augmentation methods alongside recent
advances in deep learning, including Generative Adversarial Networks (GANs),
diffusion models, and transformer-based approaches. Furthermore, we explore the
challenges associated with generating diverse and realistic handwriting
samples, particularly in preserving script authenticity and addressing data
scarcity. This survey follows the PRISMA methodology, ensuring a structured and
rigorous selection process. Our analysis began with 1,302 primary studies,
which were filtered down to 848 after removing duplicates, drawing from key
academic sources such as IEEE Digital Library, Springer Link, Science Direct,
and ACM Digital Library. By evaluating existing datasets, assessment metrics,
and state-of-the-art methodologies, this survey identifies key research gaps
and proposes future directions to advance the field of handwritten text
generation across diverse linguistic and stylistic landscapes.

</details>


### [6] [Centralized Copy-Paste: Enhanced Data Augmentation Strategy for Wildland Fire Semantic Segmentation](https://arxiv.org/abs/2507.06321)
*Joon Tai Kim,Tianle Chen,Ziyu Dong,Nishanth Kunchala,Alexander Guller,Daniel Ospina Acero,Roger Williams,Mrinal Kumar*

Main category: cs.CV

TL;DR: 提出了一种专注于改进火灾目标分割性能的集中式拷贝粘贴数据增强(CCPDA)方法，通过三步提取和增强火灾特征来提高数据集多样性，并在实验中验证了方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 通过深度学习解析火灾图像受限于缺乏标注数据，且构建和标注训练数据非常昂贵，故需要有效的数据增强技术来弥补这一不足。

Method: 提出了一种CCPDA方法，包含三个步骤：(i) 识别图像中的火灾簇，(ii) 中心化火灾区域，(iii) 将处理后的火灾簇粘贴到目标图像上，从而增强数据集的多样性和类特征保存性。

Result: 通过数值分析与对比验证，CCPDA在提升火灾类分割性能上优于其他增强方法，并在小型标注数据集下展现出其效果。

Conclusion: CCPDA作为创新的数据增强策略，成功解决了火灾目标分割中标注数据稀缺的问题，并在改进火灾类别的重要分割性能指标上表现突出。

Abstract: Collecting and annotating images for the purpose of training segmentation
models is often cost prohibitive. In the domain of wildland fire science, this
challenge is further compounded by the scarcity of reliable public datasets
with labeled ground truth. This paper presents the Centralized Copy-Paste Data
Augmentation (CCPDA) method, for the purpose of assisting with the training of
deep-learning multiclass segmentation models, with special focus on improving
segmentation outcomes for the fire-class. CCPDA has three main steps: (i)
identify fire clusters in the source image, (ii) apply a centralization
technique to focus on the core of the fire area, and (iii) paste the refined
fire clusters onto a target image. This method increases dataset diversity
while preserving the essential characteristics of the fire class. The
effectiveness of this augmentation technique is demonstrated via numerical
analysis and comparison against various other augmentation methods using a
weighted sum-based multi-objective optimization approach. This approach helps
elevate segmentation performance metrics specific to the fire class, which
carries significantly more operational significance than other classes (fuel,
ash, or background). Numerical performance assessment validates the efficacy of
the presented CCPDA method in alleviating the difficulties associated with
small, manually labeled training datasets. It also illustrates that CCPDA
outperforms other augmentation strategies in the application scenario
considered, particularly in improving fire-class segmentation performance.

</details>


### [7] [AR2: Attention-Guided Repair for the Robustness of CNNs Against Common Corruptions](https://arxiv.org/abs/2507.06332)
*Fuyuan Zhang,Qichen Wang,Jianjun Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种名为AR2（注意力引导修复以增强鲁棒性）的方法，通过对清洁图像和受损图像的类激活图（CAMs）进行对齐，从而提高预训练CNN模型在常见图像损坏情况下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前深度神经网络在存在嘈杂、模糊、天气等常见图像损坏时会出现显著性能降低，这限制了其在实际应用中的可靠性。

Method: 提出AR2方法，通过在清洁图像和受损图像之间对类激活图（CAMs）进行对齐，并采用迭代修复策略交替进行CAM引导的修复和标准微调，且无需改变模型架构。

Result: AR2在常见腐败数据集（如CIFAR-10-C、CIFAR-100-C和ImageNet-C）上超越了当前最先进方法，能在清洁数据的准确性和损坏鲁棒性之间实现平衡。

Conclusion: AR2方法提供了一种可扩展且具有鲁棒性的解决方案，能够在具有多种损坏的真实环境中提升模型的可靠性。

Abstract: Deep neural networks suffer from significant performance degradation when
exposed to common corruptions such as noise, blur, weather, and digital
distortions, limiting their reliability in real-world applications. In this
paper, we propose AR2 (Attention-Guided Repair for Robustness), a simple yet
effective method to enhance the corruption robustness of pretrained CNNs. AR2
operates by explicitly aligning the class activation maps (CAMs) between clean
and corrupted images, encouraging the model to maintain consistent attention
even under input perturbations. Our approach follows an iterative repair
strategy that alternates between CAM-guided refinement and standard
fine-tuning, without requiring architectural changes. Extensive experiments
show that AR2 consistently outperforms existing state-of-the-art methods in
restoring robustness on standard corruption benchmarks (CIFAR-10-C, CIFAR-100-C
and ImageNet-C), achieving a favorable balance between accuracy on clean data
and corruption robustness. These results demonstrate that AR2 provides a robust
and scalable solution for enhancing model reliability in real-world
environments with diverse corruptions.

</details>


### [8] [VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting](https://arxiv.org/abs/2507.05116)
*Juyi Lin,Amir Taherin,Arash Akbari,Arman Akbari,Lei Lu,Guangyu Chen,Taskin Padir,Xiaomeng Yang,Weiwei Chen,Yiqian Li,Xue Lin,David Kaeli,Pu Zhao,Yanzhi Wang*

Main category: cs.CV

TL;DR: 提出了VOTE框架，通过优化和加速视觉-语言-动作（VLA）模型以达到更高效的动作预测，展示了显著的性能和推理速度提升。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在处理自然语言引导的机器人操控任务中表现出色，但在处理新对象或不熟悉环境时泛化能力有限。为了解决这一问题，现有方法通常引入额外组件，例如深度估计或分割，但这些方法增加了计算成本和效率问题，因此作者探索独立于这些额外方法的高效动作预测。

Method: 提出了VOTE框架，包括一种无标记器的微调方法来实现并行精准的动作预测，从而降低计算开销和提高推理速度，此外，采用了集成投票策略以改进动作采样，提升泛化能力。

Result: 实验结果显示，VOTE方法达到了较先进水平，推理速度提高了35倍，吞吐量达到145 Hz。

Conclusion: VOTE框架在不依赖高层视觉表示或扩散技术的情况下，显著提升了VLA模型的泛化能力、性能和效率，并计划开源其细节和代码。

Abstract: Recent large-scale Vision Language Action (VLA) models have shown superior
performance in robotic manipulation tasks guided by natural language. However,
their generalization remains limited when applied to novel objects or
unfamiliar environments that lie outside the training distribution. To address
this, many existing approaches integrate additional components such as depth
estimation, segmentation, or even diffusion to improve generalization, at the
cost of adding significant computation overhead, resulting in low efficiency.
This motivates the exploration of efficient action prediction methods, which
are independent of additional high-level visual representations or diffusion
techniques. In this work, we propose VOTE, an efficient and general framework
for the optimization and acceleration of VLA models. In details, we propose a
novel tokenizer-free fine-tuning approach for parallel accurate action
prediction, which reduces computational overhead and accelerates inference
speed. Additionally, we adopt an ensemble voting strategy for the action
sampling, which significantly improves model performance and enhances
generalization. Experimental results show that our method achieves
state-of-the-art performance with 35$\times$ faster inference and 145 Hz
throughput. All the details and codes will be open-sourced.

</details>


### [9] [When Trackers Date Fish: A Benchmark and Framework for Underwater Multiple Fish Tracking](https://arxiv.org/abs/2507.06400)
*Weiran Li,Yeqiang Liu,Qiannan Guo,Yijie Wei,Hwa Liang Leo,Zhenbo Li*

Main category: cs.CV

TL;DR: 提出了首个专为水下多鱼跟踪设计的数据集MFT25，并开发了一种优化跟踪框架SU-T，展现了鱼类跟踪与陆地多目标跟踪的不同之处。


<details>
  <summary>Details</summary>
Motivation: 与陆地多目标跟踪的快速发展相比，水下多目标跟踪的研究仍然欠缺，但其对海洋生态学及水产养殖至关重要。

Method: 设计了一个数据集MFT25，包含408,578个边界框和48,066帧，并开发了SU-T框架，结合无迹卡尔曼滤波器和FishIoU匹配机制，专注于解决鱼类游动的非线性特征和形态差异。

Result: 在MFT25数据集上的基准测试中，SU-T框架达到了34.1 HOTA和44.6 IDF1的性能，同时揭示了鱼类跟踪与陆地跟踪的显著差异。

Conclusion: MFT25和SU-T为水下多目标跟踪研究奠定了坚实基础，对海洋生物学、水产养殖监控和生态保护具有广泛的应用价值。

Abstract: Multiple object tracking (MOT) technology has made significant progress in
terrestrial applications, but underwater tracking scenarios remain
underexplored despite their importance to marine ecology and aquaculture. We
present Multiple Fish Tracking Dataset 2025 (MFT25), the first comprehensive
dataset specifically designed for underwater multiple fish tracking, featuring
15 diverse video sequences with 408,578 meticulously annotated bounding boxes
across 48,066 frames. Our dataset captures various underwater environments,
fish species, and challenging conditions including occlusions, similar
appearances, and erratic motion patterns. Additionally, we introduce
Scale-aware and Unscented Tracker (SU-T), a specialized tracking framework
featuring an Unscented Kalman Filter (UKF) optimized for non-linear fish
swimming patterns and a novel Fish-Intersection-over-Union (FishIoU) matching
that accounts for the unique morphological characteristics of aquatic species.
Extensive experiments demonstrate that our SU-T baseline achieves
state-of-the-art performance on MFT25, with 34.1 HOTA and 44.6 IDF1, while
revealing fundamental differences between fish tracking and terrestrial object
tracking scenarios. MFT25 establishes a robust foundation for advancing
research in underwater tracking systems with important applications in marine
biology, aquaculture monitoring, and ecological conservation. The dataset and
codes are released at https://vranlee.github.io/SU-T/.

</details>


### [10] [SImpHAR: Advancing impedance-based human activity recognition using 3D simulation and text-to-motion models](https://arxiv.org/abs/2507.06405)
*Lala Shakti Swarup Ray,Mengxi Liu,Deepika Gurung,Bo Zhou,Sungho Suh,Paul Lukowicz*

Main category: cs.CV

TL;DR: SImpHAR 是一个框架，通过模拟与两阶段训练策略改进基于阻抗传感的人体动作识别，显著提升准确性和宏观F1得分。


<details>
  <summary>Details</summary>
Motivation: 解决因标注数据稀缺而阻碍生物阻抗传感在人类活动识别中的应用问题。

Method: 提出模拟管道生成真实的生物阻抗信号，同时设计两阶段训练策略，用于训练无需完全对齐标注的模型。

Result: 在 ImpAct 数据集和两个公共基准的性能测试中，准确率和宏观 F1 分数分别提高了高达 22.3% 和 21.8%。

Conclusion: 模拟驱动的增强技术和模块化训练方法在基于阻抗的活动识别中具有很大潜力。

Abstract: Human Activity Recognition (HAR) with wearable sensors is essential for
applications in healthcare, fitness, and human-computer interaction.
Bio-impedance sensing offers unique advantages for fine-grained motion capture
but remains underutilized due to the scarcity of labeled data. We introduce
SImpHAR, a novel framework addressing this limitation through two core
contributions. First, we propose a simulation pipeline that generates realistic
bio-impedance signals from 3D human meshes using shortest-path estimation,
soft-body physics, and text-to-motion generation serving as a digital twin for
data augmentation. Second, we design a two-stage training strategy with
decoupled approach that enables broader activity coverage without requiring
label-aligned synthetic data. We evaluate SImpHAR on our collected ImpAct
dataset and two public benchmarks, showing consistent improvements over
state-of-the-art methods, with gains of up to 22.3% and 21.8%, in terms of
accuracy and macro F1 score, respectively. Our results highlight the promise of
simulation-driven augmentation and modular training for impedance-based HAR.

</details>


### [11] [Hierarchical Multi-Stage Transformer Architecture for Context-Aware Temporal Action Localization](https://arxiv.org/abs/2507.06411)
*Hayat Ullah,Arslan Munir,Oliver Nina*

Main category: cs.CV

TL;DR: 本研究提出了一种名为PCL-Former的多阶段Transformer架构，有效提升了时间动作定位任务的性能，在多个基准数据集上超越了现有的方法。


<details>
  <summary>Details</summary>
Motivation: 近年来Transformer和多阶段架构在视频识别和目标检测领域取得成功，促使作者探索其在时间动作定位任务中的潜力。

Method: 研究提出PCL-Former架构，将时间动作定位任务分解为候选片段检测、动作分类和边界定位三个子任务，每个子任务由单独的Transformer模块完成并辅以专门的损失函数。

Result: 在THUMOS-14、ActivityNet-1.3和HACS Segments数据集上分别比最先进方法提高了2.8%、1.2%和4.8%的性能。

Conclusion: 通过将多阶段体系结构与Transformer结合，PCL-Former实现了时间动作定位任务的显著性能提升，并证实了该方法的有效性。

Abstract: Inspired by the recent success of transformers and multi-stage architectures
in video recognition and object detection domains. We thoroughly explore the
rich spatio-temporal properties of transformers within a multi-stage
architecture paradigm for the temporal action localization (TAL) task. This
exploration led to the development of a hierarchical multi-stage transformer
architecture called PCL-Former, where each subtask is handled by a dedicated
transformer module with a specialized loss function. Specifically, the
Proposal-Former identifies candidate segments in an untrimmed video that may
contain actions, the Classification-Former classifies the action categories
within those segments, and the Localization-Former precisely predicts the
temporal boundaries (i.e., start and end) of the action instances. To evaluate
the performance of our method, we have conducted extensive experiments on three
challenging benchmark datasets: THUMOS-14, ActivityNet-1.3, and HACS Segments.
We also conducted detailed ablation experiments to assess the impact of each
individual module of our PCL-Former. The obtained quantitative results validate
the effectiveness of the proposed PCL-Former, outperforming state-of-the-art
TAL approaches by 2.8%, 1.2%, and 4.8% on THUMOS14, ActivityNet-1.3, and HACS
datasets, respectively.

</details>


### [12] [THOR: Thermal-guided Hand-Object Reasoning via Adaptive Vision Sampling](https://arxiv.org/abs/2507.06442)
*Soroush Shahi,Farzad Shahabi,Rama Nabulsi,Glenn Fernandes,Aggelos Katsaggelos,Nabil Alshurafa*

Main category: cs.CV

TL;DR: 提出了一种名为THOR的实时自适应时空RGB帧采样方法，通过热感摄像头提升可穿戴设备对手部活动分析的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: RGB图像处理消耗高功耗、大量不必要数据、隐私问题及计算资源需求高等问题。

Method: 利用低分辨率热感摄像头数据识别活动转换时刻，调整RGB帧采样率；同时通过热感线索定位交互区域，精简图像处理范围。

Result: 方法仅使用3%的RGB视频数据即可识别所有活动段，且活动识别准确率达到95%，与全量RGB视频数据（94%）相当。

Conclusion: THOR提供了一种高效、实用的手部活动与健康行为监测方法，特别适用于可穿戴设备的持续应用。

Abstract: Wearable cameras are increasingly used as an observational and interventional
tool for human behaviors by providing detailed visual data of hand-related
activities. This data can be leveraged to facilitate memory recall for logging
of behavior or timely interventions aimed at improving health. However,
continuous processing of RGB images from these cameras consumes significant
power impacting battery lifetime, generates a large volume of unnecessary video
data for post-processing, raises privacy concerns, and requires substantial
computational resources for real-time analysis. We introduce THOR, a real-time
adaptive spatio-temporal RGB frame sampling method that leverages thermal
sensing to capture hand-object patches and classify them in real-time. We use
low-resolution thermal camera data to identify moments when a person switches
from one hand-related activity to another, and adjust the RGB frame sampling
rate by increasing it during activity transitions and reducing it during
periods of sustained activity. Additionally, we use the thermal cues from the
hand to localize the region of interest (i.e., the hand-object interaction) in
each RGB frame, allowing the system to crop and process only the necessary part
of the image for activity recognition. We develop a wearable device to validate
our method through an in-the-wild study with 14 participants and over 30
activities, and further evaluate it on Ego4D (923 participants across 9
countries, totaling 3,670 hours of video). Our results show that using only 3%
of the original RGB video data, our method captures all the activity segments,
and achieves hand-related activity recognition F1-score (95%) comparable to
using the entire RGB video (94%). Our work provides a more practical path for
the longitudinal use of wearable cameras to monitor hand-related activities and
health-risk behaviors in real time.

</details>


### [13] [EA: An Event Autoencoder for High-Speed Vision Sensing](https://arxiv.org/abs/2507.06459)
*Riadul Islam,Joey Mulé,Dhandeep Challagundla,Shahmir Rizvi,Sean Carson*

Main category: cs.CV

TL;DR: 研究提出了一个事件自编码器架构，用于实时应用中的高效事件数据压缩和分类，显著提高了性能，适合低功耗和高速场景。


<details>
  <summary>Details</summary>
Motivation: 传统视觉系统在动态环境中的感知能力有限，而事件相机虽然优秀但因事件流稀疏和噪声问题面临挑战。

Method: 提出基于卷积编码的事件自编码器架构，结合自适应阈值选择和轻量级分类器，增强了分类精度并降低了计算复杂度。

Result: 实验表明模型在现有数据集上的准确率与YOLO-v4相当，但参数量减少了35.5倍；在边缘设备上实现了高达44.8 FPS的帧率，并在性能上远超现有技术。

Conclusion: 该方法显著提升了事件相机在实时边缘计算中的性能，适用于低功耗和高速高效应用场景。

Abstract: High-speed vision sensing is essential for real-time perception in
applications such as robotics, autonomous vehicles, and industrial automation.
Traditional frame-based vision systems suffer from motion blur, high latency,
and redundant data processing, limiting their performance in dynamic
environments. Event cameras, which capture asynchronous brightness changes at
the pixel level, offer a promising alternative but pose challenges in object
detection due to sparse and noisy event streams. To address this, we propose an
event autoencoder architecture that efficiently compresses and reconstructs
event data while preserving critical spatial and temporal features. The
proposed model employs convolutional encoding and incorporates adaptive
threshold selection and a lightweight classifier to enhance recognition
accuracy while reducing computational complexity. Experimental results on the
existing Smart Event Face Dataset (SEFD) demonstrate that our approach achieves
comparable accuracy to the YOLO-v4 model while utilizing up to $35.5\times$
fewer parameters. Implementations on embedded platforms, including Raspberry Pi
4B and NVIDIA Jetson Nano, show high frame rates ranging from 8 FPS up to 44.8
FPS. The proposed classifier exhibits up to 87.84x better FPS than the
state-of-the-art and significantly improves event-based vision performance,
making it ideal for low-power, high-speed applications in real-time edge
computing.

</details>


### [14] [Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning](https://arxiv.org/abs/2507.06485)
*Ziyang Wang,Jaehong Yoon,Shoubin Yu,Md Mohaiminul Islam,Gedas Bertasius,Mohit Bansal*

Main category: cs.CV

TL;DR: 本文提出了一种视频推理新方法Video-RTS，以提高数据效率和视频推理能力，无需大规模监督微调和额外注释。


<details>
  <summary>Details</summary>
Motivation: 现有视频推理方法依赖大规模数据监督微调，成本高且难以扩展。

Method: 提出了Video-RTS方法，结合数据高效强化学习（RL）和视频自适应测试时缩放（TTS）策略，采用输出奖励的纯RL训练，并引入稀疏到密集的TTS策略优化推理。

Result: 在多个视频推理基准测试中，Video-RTS平均提高视频推理准确率2.4%，使用的训练样本仅为3.6%，在Video-Holmes和MMVU等数据集上分别提升4.2%和2.6%。

Conclusion: Video-RTS通过纯RL训练与自适应TTS策略的结合，在数据效率和推理性能上优于现有模型，具有显著的优势。

Abstract: Despite advances in reinforcement learning (RL)-based video reasoning with
large language models (LLMs), data collection and finetuning remain significant
challenges. These methods often rely on large-scale supervised fine-tuning
(SFT) with extensive video data and long Chain-of-Thought (CoT) annotations,
making them costly and hard to scale. To address this, we present Video-RTS, a
new approach to improve video reasoning capability with drastically improved
data efficiency by combining data-efficient RL with a video-adaptive test-time
scaling (TTS) strategy. Based on observations about the data scaling of RL
samples, we skip the resource-intensive SFT step and employ efficient pure-RL
training with output-based rewards, requiring no additional annotations or
extensive fine-tuning. Furthermore, to utilize computational resources more
efficiently, we introduce a sparse-to-dense video TTS strategy that improves
inference by iteratively adding frames based on output consistency. We validate
our approach on multiple video reasoning benchmarks, showing that Video-RTS
surpasses existing video reasoning models by an average of 2.4% in accuracy
using only 3.6% training samples. For example, Video-RTS achieves a 4.2%
improvement on Video-Holmes, a recent and challenging video reasoning
benchmark, and a 2.6% improvement on MMVU. Notably, our pure RL training and
adaptive video TTS offer complementary strengths, enabling Video-RTS's strong
reasoning performance.

</details>


### [15] [Mask6D: Masked Pose Priors For 6D Object Pose Estimation](https://arxiv.org/abs/2507.06486)
*Yuechen Xie,Haobo Jiang,Jin Xie*

Main category: cs.CV

TL;DR: 本文提出了一种名为Mask6D的新预训练策略，以改进基于RGB图像的6D物体姿态估计，尤其是在复杂遮挡情况下。


<details>
  <summary>Details</summary>
Motivation: 现有的姿态估计网络在使用2D特征骨干网络提取具有区分性和姿态感知特征时表现欠佳，尤其是在因目标遮挡RGB信息有限的场景中。

Method: 提出Mask6D预训练策略，利用姿态感知的2D-3D对应图与可见性掩码图，结合RGB图像进行基于重建的模型预训练；并优化了聚焦于目标的预训练损失函数。此外，通过传统的姿态训练策略对预训练网络进行微调，以实现可靠的姿态预测。

Result: 实验表明，该方法性能优于之前的端到端姿态估计方法。

Conclusion: 利用Mask6D预训练策略的姿态感知网络能够在复杂背景或遮挡情况下有效提升6D物体姿态估计性能。

Abstract: Robust 6D object pose estimation in cluttered or occluded conditions using
monocular RGB images remains a challenging task. One reason is that current
pose estimation networks struggle to extract discriminative, pose-aware
features using 2D feature backbones, especially when the available RGB
information is limited due to target occlusion in cluttered scenes. To mitigate
this, we propose a novel pose estimation-specific pre-training strategy named
Mask6D. Our approach incorporates pose-aware 2D-3D correspondence maps and
visible mask maps as additional modal information, which is combined with RGB
images for the reconstruction-based model pre-training. Essentially, this 2D-3D
correspondence maps a transformed 3D object model to 2D pixels, reflecting the
pose information of the target in camera coordinate system. Meanwhile, the
integrated visible mask map can effectively guide our model to disregard
cluttered background information. In addition, an object-focused pre-training
loss function is designed to further facilitate our network to remove the
background interference. Finally, we fine-tune our pre-trained pose prior-aware
network via conventional pose training strategy to realize the reliable pose
prediction. Extensive experiments verify that our method outperforms previous
end-to-end pose estimation methods.

</details>


### [16] [Bilateral Collaboration with Large Vision-Language Models for Open Vocabulary Human-Object Interaction Detection](https://arxiv.org/abs/2507.06510)
*Yupeng Hu,Changxing Ding,Chang Sun,Shaoli Huang,Xiangmin Xu*

Main category: cs.CV

TL;DR: 该研究提出了一种新方法BC-HOI，用于开放词汇的人物-物体交互(HOI)检测，构建了双向合作框架，通过改进VLM生成更精细的交互特征，提升检测表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，基于大型视觉语言模型(VLM)的特征缺乏精细性，与检测任务需求不符，需开发更能捕获精细交互特征的方法。

Method: 提出双向合作框架BC-HOI，包括注意偏差引导(ABG)和语言模型监督引导(LSG)模块，分别优化VLM的特征细粒度度和检测器的监督信息质量。

Result: 在HICO-DET和V-COCO数据集上进行实验，无论开放还是封闭词汇情况下均表现优异。

Conclusion: BC-HOI结合VLM和LLM优势，通过双向协作显著提升了人-动词-物检测任务的表现，展示了跨领域模型的潜力及实用性。

Abstract: Open vocabulary Human-Object Interaction (HOI) detection is a challenging
task that detects all <human, verb, object> triplets of interest in an image,
even those that are not pre-defined in the training set. Existing approaches
typically rely on output features generated by large Vision-Language Models
(VLMs) to enhance the generalization ability of interaction representations.
However, the visual features produced by VLMs are holistic and coarse-grained,
which contradicts the nature of detection tasks. To address this issue, we
propose a novel Bilateral Collaboration framework for open vocabulary HOI
detection (BC-HOI). This framework includes an Attention Bias Guidance (ABG)
component, which guides the VLM to produce fine-grained instance-level
interaction features according to the attention bias provided by the HOI
detector. It also includes a Large Language Model (LLM)-based Supervision
Guidance (LSG) component, which provides fine-grained token-level supervision
for the HOI detector by the LLM component of the VLM. LSG enhances the ability
of ABG to generate high-quality attention bias. We conduct extensive
experiments on two popular benchmarks: HICO-DET and V-COCO, consistently
achieving superior performance in the open vocabulary and closed settings. The
code will be released in Github.

</details>


### [17] [What Demands Attention in Urban Street Scenes? From Scene Understanding towards Road Safety: A Survey of Vision-driven Datasets and Studies](https://arxiv.org/abs/2507.06513)
*Yaoqi Huang,Julie Stephany Berrio,Mao Shan,Stewart Worrall*

Main category: cs.CV

TL;DR: 本文通过综合分析、分类探讨交通情景关键元素、视觉驱动任务及数据集现状，为视觉传感器与计算机视觉的道路安全应用提供了统一框架与研究指引。


<details>
  <summary>Details</summary>
Motivation: 推动视觉传感器与计算机视觉算法的改进在道路安全中的使用，并系统性分析当前交通情景中的关键元素及相关数据集和任务。

Method: 提出一个统一的分类学，将交通中的异常和正常但关键实体分为十个类别和二十个子类，并对35个视觉驱动任务与73个数据集进行跨领域分析。

Result: 该研究系统探讨了视觉驱动任务和相关数据集的优缺点，并提供了标准化和资源优化的建议。

Conclusion: 研究通过提出分类框架和表格总结，为研究人员提供了系统的研究综述，指导资源选择，指出研究空白，推动领域发展。

Abstract: Advances in vision-based sensors and computer vision algorithms have
significantly improved the analysis and understanding of traffic scenarios. To
facilitate the use of these improvements for road safety, this survey
systematically categorizes the critical elements that demand attention in
traffic scenarios and comprehensively analyzes available vision-driven tasks
and datasets. Compared to existing surveys that focus on isolated domains, our
taxonomy categorizes attention-worthy traffic entities into two main groups
that are anomalies and normal but critical entities, integrating ten categories
and twenty subclasses. It establishes connections between inherently related
fields and provides a unified analytical framework. Our survey highlights the
analysis of 35 vision-driven tasks and comprehensive examinations and
visualizations of 73 available datasets based on the proposed taxonomy. The
cross-domain investigation covers the pros and cons of each benchmark with the
aim of providing information on standards unification and resource
optimization. Our article concludes with a systematic discussion of the
existing weaknesses, underlining the potential effects and promising solutions
from various perspectives. The integrated taxonomy, comprehensive analysis, and
recapitulatory tables serve as valuable contributions to this rapidly evolving
field by providing researchers with a holistic overview, guiding strategic
resource selection, and highlighting critical research gaps.

</details>


### [18] [FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and Video-to-Text Generation](https://arxiv.org/abs/2507.06523)
*Liqiang Jing,Viet Lai,Seunghyun Yoon,Trung Bui,Xinya Du*

Main category: cs.CV

TL;DR: 提出了一种名为FIFA的统一验证框架，用于评估视频多模态大语言模型的内容真实性，并引入后修正技术来增强生成内容的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法在处理视频多模态大语言模型的开放式生成内容时存在局限性，尤其是在检测其潜在的幻觉输出方面。

Method: 提出FIFA框架，以时空语义依赖图建模描述性事实并通过VideoQA模型验证，同时引入后修正工具以改进生成内容。

Result: 实验证明，FIFA与人工评估更为一致，后修正技术有效提高了文本和视频生成中的事实一致性。

Conclusion: FIFA框架和后修正方法在评估与改进视频多模态模型生成内容的真实性上表现出显著优势。

Abstract: Video Multimodal Large Language Models (VideoMLLMs) have achieved remarkable
progress in both Video-to-Text and Text-to-Video tasks. However, they often
suffer fro hallucinations, generating content that contradicts the visual
input. Existing evaluation methods are limited to one task (e.g., V2T) and also
fail to assess hallucinations in open-ended, free-form responses. To address
this gap, we propose FIFA, a unified FaIthFulness evAluation framework that
extracts comprehensive descriptive facts, models their semantic dependencies
via a Spatio-Temporal Semantic Dependency Graph, and verifies them using
VideoQA models. We further introduce Post-Correction, a tool-based correction
framework that revises hallucinated content. Extensive experiments demonstrate
that FIFA aligns more closely with human judgment than existing evaluation
methods, and that Post-Correction effectively improves factual consistency in
both text and video generation.

</details>


### [19] [Concept Unlearning by Modeling Key Steps of Diffusion Process](https://arxiv.org/abs/2507.06526)
*Chaoshuo Zhang,Chenhao Lin,Zhengyu Zhao,Le Yang,Qian Wang,Chao Shen*

Main category: cs.CV

TL;DR: 该论文提出了一种名为KSCU（关键步骤概念遗忘）的方法，用于在确保生成模型保留能力的同时，有效防止文本到图像扩散模型生成不良图像。


<details>
  <summary>Details</summary>
Motivation: 现有概念遗忘方法在平衡遗忘有效性和生成可保留性方面存在挑战，该研究旨在解决这一问题。

Method: 提出了一种名为KSCU的新方法，利用扩散模型的逐步采样特性，通过关键步骤针对性地调整模型，而非对所有降噪步骤均一对待。

Result: 通过实验表明，KSCU在有效防止生成不良图像的同时，更好地保留了生成模型的能力。

Conclusion: KSCU是一种高效且有效的概念遗忘方法，为文本到图像模型的安全性和性能提供了新的解决方案。

Abstract: Text-to-image diffusion models (T2I DMs), represented by Stable Diffusion,
which generate highly realistic images based on textual input, have been widely
used. However, their misuse poses serious security risks. While existing
concept unlearning methods aim to mitigate these risks, they struggle to
balance unlearning effectiveness with generative retainability.To overcome this
limitation, we innovatively propose the Key Step Concept Unlearning (KSCU)
method, which ingeniously capitalizes on the unique stepwise sampling
characteristic inherent in diffusion models during the image generation
process. Unlike conventional approaches that treat all denoising steps equally,
KSCU strategically focuses on pivotal steps with the most influence over the
final outcome by dividing key steps for different concept unlearning tasks and
fine-tuning the model only at those steps. This targeted approach reduces the
number of parameter updates needed for effective unlearning, while maximizing
the retention of the model's generative capabilities.Through extensive
benchmark experiments, we demonstrate that KSCU effectively prevents T2I DMs
from generating undesirable images while better retaining the model's
generative capabilities.Our code will be released.

</details>


### [20] [Speak2Sign3D: A Multi-modal Pipeline for English Speech to American Sign Language Animation](https://arxiv.org/abs/2507.06530)
*Kazi Mahathir Rahman,Naveed Imtiaz Nafis,Md. Farhan Sadik,Mohammad Al Rafi,Mehedi Hasan Shahed*

Main category: cs.CV

TL;DR: 本研究提出了一套完整系统，可以将英语语音转译为自然流畅的3D手语动画，覆盖了语音识别、翻译及动画生成等多个步骤。


<details>
  <summary>Details</summary>
Motivation: 当前手语自动翻译研究多集中在手语转文字，且对将语音转为手语动画的研究较少，因其中涉及多个复杂阶段。

Method: 提出联合语音识别（Whisper）、文本翻译（MarianMT转为ASL Gloss）和3D手语动画生成的完整流水线系统，并使用新数据集和词嵌入技术增强精度。

Result: 系统可以准确地将英语语音转为流畅的3D手语动画，在BLEU分数评估中表现优异（达到0.7714和0.8923）。

Conclusion: 该研究首次将语音、文本和3D动画结合，将语音转译为真实感强的手语动作，为促进聋哑人之间的交流提供了更完整的解决方案。

Abstract: Helping deaf and hard-of-hearing people communicate more easily is the main
goal of Automatic Sign Language Translation. Although most past research has
focused on turning sign language into text, doing the reverse, turning spoken
English into sign language animations, has been largely overlooked. That's
because it involves multiple steps, such as understanding speech, translating
it into sign-friendly grammar, and generating natural human motion. In this
work, we introduce a complete pipeline that converts English speech into
smooth, realistic 3D sign language animations. Our system starts with Whisper
to translate spoken English into text. Then, we use a MarianMT machine
translation model to translate that text into American Sign Language (ASL)
gloss, a simplified version of sign language that captures meaning without
grammar. This model performs well, reaching BLEU scores of 0.7714 and 0.8923.
To make the gloss translation more accurate, we also use word embeddings such
as Word2Vec and FastText to understand word meanings. Finally, we animate the
translated gloss using a 3D keypoint-based motion system trained on
Sign3D-WLASL, a dataset we created by extracting body, hand, and face key
points from real ASL videos in the WLASL dataset. To support the gloss
translation stage, we also built a new dataset called BookGlossCorpus-CG, which
turns everyday English sentences from the BookCorpus dataset into ASL gloss
using grammar rules. Our system stitches everything together by smoothly
interpolating between signs to create natural, continuous animations. Unlike
previous works like How2Sign and Phoenix-2014T that focus on recognition or use
only one type of data, our pipeline brings together audio, text, and motion in
a single framework that goes all the way from spoken English to lifelike 3D
sign language animation.

</details>


### [21] [ILNet: Trajectory Prediction with Inverse Learning Attention for Enhancing Intention Capture](https://arxiv.org/abs/2507.06531)
*Mingjin Zeng,Nan Ouyang,Wenkang Wan,Lei Ao,Qing Cai,Kai Sheng*

Main category: cs.CV

TL;DR: 该论文提出了一种名为ILNet的多代理轨迹预测方法，以解决多代理交互场景中的轨迹预测挑战，并基于先进的注意力机制和动态锚点选择模块提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 受到人类驾驶行为的启发，该研究试图解决现有轨迹预测方法无法充分捕捉复杂交互模式和适应未来环境的问题。

Method: 提出的ILNet包括两个核心组件：利用逆学习(IL)注意力机制对交互时空模式进行动态编码；以及通过动态锚点选择(DAS)模块提取关键轨迹点，增强轨迹预测能力。

Result: 实验结果表明，ILNet在INTERACTION和Argoverse数据集上达到了当前最优性能，尤其在挑战性的交互场景中表现出更高的预测精度和多模态能力。

Conclusion: ILNet利用逆学习注意力和动态锚点选择机制，更有效地捕捉交互场景中的复杂行为，并在参数较少的情况下提升了轨迹预测性能。

Abstract: Trajectory prediction for multi-agent interaction scenarios is a crucial
challenge. Most advanced methods model agent interactions by efficiently
factorized attention based on the temporal and agent axes. However, this static
and foward modeling lacks explicit interactive spatio-temporal coordination,
capturing only obvious and immediate behavioral intentions. Alternatively, the
modern trajectory prediction framework refines the successive predictions by a
fixed-anchor selection strategy, which is difficult to adapt in different
future environments. It is acknowledged that human drivers dynamically adjust
initial driving decisions based on further assumptions about the intentions of
surrounding vehicles. Motivated by human driving behaviors, this paper proposes
ILNet, a multi-agent trajectory prediction method with Inverse Learning (IL)
attention and Dynamic Anchor Selection (DAS) module. IL Attention employs an
inverse learning paradigm to model interactions at neighboring moments,
introducing proposed intentions to dynamically encode the spatio-temporal
coordination of interactions, thereby enhancing the model's ability to capture
complex interaction patterns. Then, the learnable DAS module is proposed to
extract multiple trajectory change keypoints as anchors in parallel with almost
no increase in parameters. Experimental results show that the ILNet achieves
state-of-the-art performance on the INTERACTION and Argoverse motion
forecasting datasets. Particularly, in challenged interaction scenarios, ILNet
achieves higher accuracy and more multimodal distributions of trajectories over
fewer parameters. Our codes are available at https://github.com/mjZeng11/ILNet.

</details>


### [22] [A model-agnostic active learning approach for animal detection from camera traps](https://arxiv.org/abs/2507.06537)
*Thi Thu Thuy Nguyen,Duc Thanh Nguyen*

Main category: cs.CV

TL;DR: 该研究提出了一种与模型无关的主动学习方法，通过结合目标与图像层面的不确定性和多样性来优化采样选择，在动物检测实验中显著减少了所需的标注数据量。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要完全访问模型，这种限制阻碍了主动学习技术在数据繁多的动物监测领域的应用。

Method: 提出了一种结合不确定性和样本多样性的主动学习方法，并在目标和图像两层面选择训练样本。

Result: 实验结果表明，仅使用30%的训练数据，利用新方法的动物检测性能可达到或超过使用完整训练数据的水平。

Conclusion: 该方法在实现高效数据采样和动物检测方面具有显著效果，有助于改善野生动物监测与保护。

Abstract: Smart data selection is becoming increasingly important in data-driven
machine learning. Active learning offers a promising solution by allowing
machine learning models to be effectively trained with optimal data including
the most informative samples from large datasets. Wildlife data captured by
camera traps are excessive in volume, requiring tremendous effort in data
labelling and animal detection models training. Therefore, applying active
learning to optimise the amount of labelled data would be a great aid in
enabling automated wildlife monitoring and conservation. However, existing
active learning techniques require that a machine learning model (i.e., an
object detector) be fully accessible, limiting the applicability of the
techniques. In this paper, we propose a model-agnostic active learning approach
for detection of animals captured by camera traps. Our approach integrates
uncertainty and diversity quantities of samples at both the object-based and
image-based levels into the active learning sample selection process. We
validate our approach in a benchmark animal dataset. Experimental results
demonstrate that, using only 30% of the training data selected by our approach,
a state-of-the-art animal detector can achieve a performance of equal or
greater than that with the use of the complete training dataset.

</details>


### [23] [Token Bottleneck: One Token to Remember Dynamics](https://arxiv.org/abs/2507.06543)
*Taekyung Kim,Dongyoon Han,Byeongho Heo,Jeongeun Park,Sangdoo Yun*

Main category: cs.CV

TL;DR: 提出了一种名为Token Bottleneck (ToBo)的自监督学习方法，旨在通过压缩场景为瓶颈标记并利用少量补丁预测后续场景，从而高效提取动态场景中的时间相关视觉表示。


<details>
  <summary>Details</summary>
Motivation: 解决动态场景中时序场景理解任务（如视觉跟踪和机器人操作）的紧凑和时间感知视觉表示提取问题。

Method: 设计了一个称为Token Bottleneck (ToBo)的管道，通过将参考场景编码为紧凑的瓶颈标记，并使用少量目标补丁作为提示进行扩展预测，学习序列场景表示。

Result: ToBo在视频标签传播和机器人操作等多个任务中表现优于基线方法。在物理机器人上的实验也验证了其鲁棒性和有效性，同时方法在不同模型规模上具有可扩展性。

Conclusion: ToBo方法有效地嵌入了时间依赖性和动态场景变化的理解，为动态场景中的视觉表示学习提供了新的解决方案。

Abstract: Deriving compact and temporally aware visual representations from dynamic
scenes is essential for successful execution of sequential scene understanding
tasks such as visual tracking and robotic manipulation. In this paper, we
introduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised
learning pipeline that squeezes a scene into a bottleneck token and predicts
the subsequent scene using minimal patches as hints. The ToBo pipeline
facilitates the learning of sequential scene representations by conservatively
encoding the reference scene into a compact bottleneck token during the squeeze
step. In the expansion step, we guide the model to capture temporal dynamics by
predicting the target scene using the bottleneck token along with few target
patches as hints. This design encourages the vision backbone to embed temporal
dependencies, thereby enabling understanding of dynamic transitions across
scenes. Extensive experiments in diverse sequential tasks, including video
label propagation and robot manipulation in simulated environments demonstrate
the superiority of ToBo over baselines. Moreover, deploying our pre-trained
model on physical robots confirms its robustness and effectiveness in
real-world environments. We further validate the scalability of ToBo across
different model scales.

</details>


### [24] [Concept-TRAK: Understanding how diffusion models learn concepts through concept-level attribution](https://arxiv.org/abs/2507.06547)
*Yonghyun Park,Chieh-Hsin Lai,Satoshi Hayakawa,Yuhta Takida,Naoki Murata,Wei-Hsiang Liao,Woosung Choi,Kin Wai Cheuk,Junghyun Koo,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 本文提出了一种名为Concept-TRAK的新方法，通过概念级归因解决了现有方法在特定元素归因上的不足。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在生成图像时无法细化至具体元素（如风格或对象）的归因问题，提供对模型的透明化和责任型AI发展的支持。

Method: 方法包括两个创新：(1) 基于扩散后验采样的重新定义训练损失；(2) 引入概念感知奖励函数以强化语义相关性。

Result: 在AbC基准测试中，Concept-TRAK展现了显著的效果提升，并在多个案例中证明了其实用性。

Conclusion: 概念级归因方法Concept-TRAK为生成式AI的责任开发和治理提供了有价值的洞察及工具。

Abstract: While diffusion models excel at image generation, their growing adoption
raises critical concerns around copyright issues and model transparency.
Existing attribution methods identify training examples influencing an entire
image, but fall short in isolating contributions to specific elements, such as
styles or objects, that matter most to stakeholders. To bridge this gap, we
introduce \emph{concept-level attribution} via a novel method called
\emph{Concept-TRAK}. Concept-TRAK extends influence functions with two key
innovations: (1) a reformulated diffusion training loss based on diffusion
posterior sampling, enabling robust, sample-specific attribution; and (2) a
concept-aware reward function that emphasizes semantic relevance. We evaluate
Concept-TRAK on the AbC benchmark, showing substantial improvements over prior
methods. Through diverse case studies--ranging from identifying IP-protected
and unsafe content to analyzing prompt engineering and compositional
learning--we demonstrate how concept-level attribution yields actionable
insights for responsible generative AI development and governance.

</details>


### [25] [Divergence-Based Similarity Function for Multi-View Contrastive Learning](https://arxiv.org/abs/2507.06560)
*Jae Hyoung Jeon,Cheolsu Lim,Myungjoo Kang*

Main category: cs.CV

TL;DR: 提出了一种新的相似性函数（DSF），通过测量分布之间的发散性显式捕捉多视图间的联合结构，在多任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法主要捕捉成对关系，未能充分建模多个增广视图间的联合结构。

Method: 通过将多个增广视图建模为分布，并使用基于发散性的新相似性函数DSF来衡量这些分布之间的相似性。

Result: 在kNN分类和线性评估等任务中表现出色，效率高于现有多视图方法；同时与余弦相似度建立了理论联系，并摆脱了温度超参数的需求。

Conclusion: DSF能够有效地捕捉多视图的联合结构，提高了对比学习的性能和效率。

Abstract: Recent success in contrastive learning has sparked growing interest in more
effectively leveraging multiple augmented views of an instance. While prior
methods incorporate multiple views at the loss or feature level, they primarily
capture pairwise relationships and fail to model the joint structure across all
views. In this work, we propose a divergence-based similarity function (DSF)
that explicitly captures the joint structure by representing each set of
augmented views as a distribution and measuring similarity as the divergence
between distributions. Extensive experiments demonstrate that DSF consistently
improves performance across various tasks, including kNN classification and
linear evaluation, while also offering greater efficiency compared to other
multi-view methods. Furthermore, we establish a theoretical connection between
DSF and cosine similarity, and show that, unlike cosine similarity, DSF
operates effectively without requiring a temperature hyperparameter.

</details>


### [26] [Edge-Boundary-Texture Loss: A Tri-Class Generalization of Weighted Binary Cross-Entropy for Enhanced Edge Detection](https://arxiv.org/abs/2507.06569)
*Hao Shu*

Main category: cs.CV

TL;DR: 提出一种新的损失函数EBT，通过区分像素类型提升边缘检测效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法对边界区域非边缘像素处理不准确的问题，改善边缘细节预测质量。

Method: 设计EBT损失函数，将像素分为边缘、边界和纹理三类，根据具体类别赋予不同权重，实现更为结构化的学习。

Result: 在多个基准上验证了优越性，无论定量还是感知效果都优于现有方法。此外，超参调节需求低，实用性强。

Conclusion: EBT在泛化性、适用性以及边缘检测性能上均显现出显著优势，推动了该领域的发展。

Abstract: Edge detection (ED) remains a fundamental task in computer vision, yet its
performance is often hindered by the ambiguous nature of non-edge pixels near
object boundaries. The widely adopted Weighted Binary Cross-Entropy (WBCE) loss
treats all non-edge pixels uniformly, overlooking the structural nuances around
edges and often resulting in blurred predictions. In this paper, we propose the
Edge-Boundary-Texture (EBT) loss, a novel objective that explicitly divides
pixels into three categories, edge, boundary, and texture, and assigns each a
distinct supervisory weight. This tri-class formulation enables more structured
learning by guiding the model to focus on both edge precision and contextual
boundary localization. We theoretically show that the EBT loss generalizes the
WBCE loss, with the latter becoming a limit case. Extensive experiments across
multiple benchmarks demonstrate the superiority of the EBT loss both
quantitatively and perceptually. Furthermore, the consistent use of unified
hyperparameters across all models and datasets, along with robustness to their
moderate variations, indicates that the EBT loss requires minimal fine-tuning
and is easily deployable in practice.

</details>


### [27] [MOST: Motion Diffusion Model for Rare Text via Temporal Clip Banzhaf Interaction](https://arxiv.org/abs/2507.06590)
*Yin Wang,Mu li,Zhiying Leng,Frederick W. B. Li,Xiaohui Liang*

Main category: cs.CV

TL;DR: MOST 提出了一种基于时间片段 Banzhaf 交互的运动扩散模型，解决了从稀有语言提示生成人体运动的问题。


<details>
  <summary>Details</summary>
Motivation: 解决从稀有语言提示生成人体运动时的粗粒度匹配和语义线索忽略问题。

Method: 采用时间片段 Banzhaf 交互来量化文本和运动之间的细粒度一致性，并通过运动提示模块生成一致的动作。

Result: MOST 在文本到运动检索和生成上表现出色，特别是在处理稀有提示时。

Conclusion: 通过综合处理以往挑战，MOST 展现了卓越的性能，证明了其方法的有效性。

Abstract: We introduce MOST, a novel motion diffusion model via temporal clip Banzhaf
interaction, aimed at addressing the persistent challenge of generating human
motion from rare language prompts. While previous approaches struggle with
coarse-grained matching and overlook important semantic cues due to motion
redundancy, our key insight lies in leveraging fine-grained clip relationships
to mitigate these issues. MOST's retrieval stage presents the first formulation
of its kind - temporal clip Banzhaf interaction - which precisely quantifies
textual-motion coherence at the clip level. This facilitates direct,
fine-grained text-to-motion clip matching and eliminates prevalent redundancy.
In the generation stage, a motion prompt module effectively utilizes retrieved
motion clips to produce semantically consistent movements. Extensive
evaluations confirm that MOST achieves state-of-the-art text-to-motion
retrieval and generation performance by comprehensively addressing previous
challenges, as demonstrated through quantitative and qualitative results
highlighting its effectiveness, especially for rare prompts.

</details>


### [28] [Ambiguity-aware Point Cloud Segmentation by Adaptive Margin Contrastive Learning](https://arxiv.org/abs/2507.06592)
*Yang Chen,Yueqi Duan,Haowen Sun,Jiwen Lu,Yap-Peng Tan*

Main category: cs.CV

TL;DR: 本文提出了一种针对点云的3D语义分割的自适应边距对比学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法未考虑每个点模糊性，尤其是过渡区域，导致分类错误或模型次优化。

Method: 设计了AMContrast3D，通过结合对比学习的模糊性估计框架，为每个点依据模糊性调节目标，并进阶为AMContrast3D++，提出模糊性预测模块以改进性能。

Result: 在3D室内场景数据集S3DIS和ScanNet上，论文方法验证了其有效性。

Conclusion: 该方法提高了语义分割效果和鲁棒性，并整合了代码在GitHub供研究使用。

Abstract: This paper proposes an adaptive margin contrastive learning method for 3D
semantic segmentation on point clouds. Most existing methods use equally
penalized objectives, which ignore the per-point ambiguities and less
discriminated features stemming from transition regions. However, as highly
ambiguous points may be indistinguishable even for humans, their manually
annotated labels are less reliable, and hard constraints over these points
would lead to sub-optimal models. To address this, we first design
AMContrast3D, a method comprising contrastive learning into an ambiguity
estimation framework, tailored to adaptive objectives for individual points
based on ambiguity levels. As a result, our method promotes model training,
which ensures the correctness of low-ambiguity points while allowing mistakes
for high-ambiguity points. As ambiguities are formulated based on position
discrepancies across labels, optimization during inference is constrained by
the assumption that all unlabeled points are uniformly unambiguous, lacking
ambiguity awareness. Inspired by the insight of joint training, we further
propose AMContrast3D++ integrating with two branches trained in parallel, where
a novel ambiguity prediction module concurrently learns point ambiguities from
generated embeddings. To this end, we design a masked refinement mechanism that
leverages predicted ambiguities to enable the ambiguous embeddings to be more
reliable, thereby boosting segmentation performance and enhancing robustness.
Experimental results on 3D indoor scene datasets, S3DIS and ScanNet,
demonstrate the effectiveness of the proposed method. Code is available at
https://github.com/YangChenApril/AMContrast3D.

</details>


### [29] [Capturing Stable HDR Videos Using a Dual-Camera System](https://arxiv.org/abs/2507.06593)
*Qianyu Zhang,Bolun Zheng,Hangjia Pan,Lingyu Zhu,Zunjie Zhu,Zongpeng Li,Shiqi Wang*

Main category: cs.CV

TL;DR: 提出了一种名为DCS的双摄像机系统以及EAFNet融合网络，用于解决HDR视频中的闪烁问题，并在多项实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决HDR视频中传统交替曝光所导致的闪烁问题，提升HDR视频质量。

Method: 通过双摄像机系统分别捕捉参考序列与非参考序列，再结合具有预对齐和跨特征融合功能的EAFNet网络进行图像融合与重建。

Result: 提出的方法在多个数据集上实现了最先进的性能，验证了DCS在HDR视频重建中的潜力。

Conclusion: DCS系统及其融合网络方法能有效抑制HDR视频中的闪烁，同时显著提升视频质量。

Abstract: In HDR video reconstruction, exposure fluctuations in reference images from
alternating exposure methods often result in flickering. To address this issue,
we propose a dual-camera system (DCS) for HDR video acquisition, where one
camera is assigned to capture consistent reference sequences, while the other
is assigned to capture non-reference sequences for information supplementation.
To tackle the challenges posed by video data, we introduce an exposure-adaptive
fusion network (EAFNet) to achieve more robust results. EAFNet introduced a
pre-alignment subnetwork to explore the influence of exposure, selectively
emphasizing the valuable features across different exposure levels. Then, the
enhanced features are fused by the asymmetric cross-feature fusion subnetwork,
which explores reference-dominated attention maps to improve image fusion by
aligning cross-scale features and performing cross-feature fusion. Finally, the
reconstruction subnetwork adopts a DWT-based multiscale architecture to reduce
ghosting artifacts and refine features at different resolutions. Extensive
experimental evaluations demonstrate that the proposed method achieves
state-of-the-art performance on different datasets, validating the great
potential of the DCS in HDR video reconstruction. The codes and data captured
by DCS will be available at https://github.com/zqqqyu/DCS.

</details>


### [30] [Cross-Modal Dual-Causal Learning for Long-Term Action Recognition](https://arxiv.org/abs/2507.06603)
*Xu Shaowu,Jia Xibin,Gao Junyu,Sun Qianmei,Chang Jing,Fan Chao*

Main category: cs.CV

TL;DR: 本文提出了一种新方法CMDCL，通过结构因果模型分析视频和文本标签间的因果关系，应对长时动作识别中的跨模态偏差和视觉误导问题。


<details>
  <summary>Details</summary>
Motivation: 当前长时动作识别中存在复杂的动作关联与视觉干扰问题，而现有视觉-语言模型方法依赖统计相关性，缺乏因果机制。此外，已有因果方法缺少跨模态因果建模，限制了其在长时动作识别中的应用。

Method: CMDCL通过跨模态双因果学习，使用结构因果模型发现视频与文本标签间的因果关系。具体来说，文本因果干预调整了文本嵌入的跨模态偏差，视觉因果干预通过结合调整后的文本消除视觉内在混淆因素。

Result: 实验在三个基准数据集Charades、Breakfast和COIN上进行了验证，结果证明了该模型的有效性。

Conclusion: CMDCL通过双因果干预有效应对长时动作识别中的挑战，显著提升了模型的鲁棒性与表现。

Abstract: Long-term action recognition (LTAR) is challenging due to extended temporal
spans with complex atomic action correlations and visual confounders. Although
vision-language models (VLMs) have shown promise, they often rely on
statistical correlations instead of causal mechanisms. Moreover, existing
causality-based methods address modal-specific biases but lack cross-modal
causal modeling, limiting their utility in VLM-based LTAR. This paper proposes
\textbf{C}ross-\textbf{M}odal \textbf{D}ual-\textbf{C}ausal \textbf{L}earning
(CMDCL), which introduces a structural causal model to uncover causal
relationships between videos and label texts.
  CMDCL addresses cross-modal biases in text embeddings via textual causal
intervention and removes confounders inherent in the visual modality through
visual causal intervention guided by the debiased text.
  These dual-causal interventions enable robust action representations to
address LTAR challenges. Experimental results on three benchmarks including
Charades, Breakfast and COIN, demonstrate the effectiveness of the proposed
model. Our code is available at https://github.com/xushaowu/CMDCL.

</details>


### [31] [Omni-Fusion of Spatial and Spectral for Hyperspectral Image Segmentation](https://arxiv.org/abs/2507.06606)
*Qing Zhang,Guoquan Pei,Yan Wang*

Main category: cs.CV

TL;DR: 本文提出一种名为Omni-Fuse的空间光谱全维融合网络，以解决医学超光谱成像（MHSI）在分割任务中的高维特性和光谱冗余问题。


<details>
  <summary>Details</summary>
Motivation: MHSI在计算病理学等领域因其丰富的光谱信息成为疾病诊断的有力工具，但如何有效融合其空间和光谱信息一直是一个挑战。

Method: 本文设计了全新的空间-光谱融合框架，包括跨维增强模块、光谱引导的空间查询选择以及两阶段跨维解码器，结合双向注意力机制实现高效的特征融合。

Result: 在两个显微超光谱数据集上的实验证明，Omni-Fuse在分割性能上优于现有方法，DSC提升超过5.73%。

Conclusion: Omni-Fuse通过全新的融合机制显著提升了MHSI分割任务的效果，展示了在医学影像分割领域的潜力。

Abstract: Medical Hyperspectral Imaging (MHSI) has emerged as a promising tool for
enhanced disease diagnosis, particularly in computational pathology, offering
rich spectral information that aids in identifying subtle biochemical
properties of tissues. Despite these advantages, effectively fusing both
spatial-dimensional and spectral-dimensional information from MHSIs remains
challenging due to its high dimensionality and spectral redundancy inherent
characteristics. To solve the above challenges, we propose a novel
spatial-spectral omni-fusion network for hyperspectral image segmentation,
named as Omni-Fuse. Here, we introduce abundant cross-dimensional feature
fusion operations, including a cross-dimensional enhancement module that
refines both spatial and spectral features through bidirectional attention
mechanisms, a spectral-guided spatial query selection to select the most
spectral-related spatial feature as the query, and a two-stage
cross-dimensional decoder which dynamically guide the model to focus on the
selected spatial query. Despite of numerous attention blocks, Omni-Fuse remains
efficient in execution. Experiments on two microscopic hyperspectral image
datasets show that our approach can significantly improve the segmentation
performance compared with the state-of-the-art methods, with over 5.73 percent
improvement in DSC. Code available at:
https://github.com/DeepMed-Lab-ECNU/Omni-Fuse.

</details>


### [32] [PointVDP: Learning View-Dependent Projection by Fireworks Rays for 3D Point Cloud Segmentation](https://arxiv.org/abs/2507.06618)
*Yang Chen,Yueqi Duan,Haowen Sun,Ziwei Wang,Jiwen Lu,Yap-Peng Tan*

Main category: cs.CV

TL;DR: 本文提出了一种名为视图依赖投影（VDP）的方法，用于改进点云分割，通过动态适应视图变化的空间几何设计高效的3D到2D映射，并优化计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于投影的方法依赖于视图独立的投影，这种方法的预定义参数限制了点的感知能力，同时由于投影冗余导致了图像处理中的计算效率低下。需要一种能动态适应3D点分布并优化投影计算资源的方法。

Method: 设计了一种VDP框架，通过模拟烟花的自适应行为从3D点分布生成数据驱动的投影，同时引入颜色正则化以优化框架，突出语义像素中的关键特征并抑制非语义特征。

Result: 在S3DIS和ScanNet基准测试中的实验表明，PointVDP以较低的计算资源需求达到了有竞争力的结果。

Conclusion: PointVDP提供了一种资源高效的语义理解解决方案，通过轻量化的投影显著改进了点云分割的效率和效果。

Abstract: In this paper, we propose view-dependent projection (VDP) to facilitate point
cloud segmentation, designing efficient 3D-to-2D mapping that dynamically
adapts to the spatial geometry from view variations. Existing projection-based
methods leverage view-independent projection in complex scenes, relying on
straight lines to generate direct rays or upward curves to reduce occlusions.
However, their view independence provides projection rays that are limited to
pre-defined parameters by human settings, restricting point awareness and
failing to capture sufficient projection diversity across different view
planes. Although multiple projections per view plane are commonly used to
enhance spatial variety, the projected redundancy leads to excessive
computational overhead and inefficiency in image processing. To address these
limitations, we design a framework of VDP to generate data-driven projections
from 3D point distributions, producing highly informative single-image inputs
by predicting rays inspired by the adaptive behavior of fireworks. In addition,
we construct color regularization to optimize the framework, which emphasizes
essential features within semantic pixels and suppresses the non-semantic
features within black pixels, thereby maximizing 2D space utilization in a
projected image. As a result, our approach, PointVDP, develops lightweight
projections in marginal computation costs. Experiments on S3DIS and ScanNet
benchmarks show that our approach achieves competitive results, offering a
resource-efficient solution for semantic understanding.

</details>


### [33] [EXAONE Path 2.0: Pathology Foundation Model with End-to-End Supervision](https://arxiv.org/abs/2507.06639)
*Myungjang Pyeon,Janghyeon Lee,Minsoo Lee,Juseung Yun,Hwanil Choi,Jonghyun Kim,Jiwon Kim,Yi Hu,Jongseong Jang,Soonyoung Lee*

Main category: cs.CV

TL;DR: EXAONE Path 2.0是一种病理模型，通过直接滑动级监督学习补丁表示，在37k WSIs上达到了10项生物标志物预测任务中的最新性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法在处理数字病理的gigapixel全景图像时效率低下，且使用自监督学习可能忽略重要的领域特定特征，存在数据效率与计算资源需求问题。

Method: 通过直接使用滑动级监督训练补丁表示，开发了一种称为EXAONE Path 2.0的病理基础模型，取代传统的补丁级自监督学习与多个实例学习。

Result: EXAONE Path 2.0在仅利用37k全片图像训练情况下，在10个生物标志物预测任务中表现出色，且具备出色的数据效率。

Conclusion: EXAONE Path 2.0有效解决了当前补丁级表示学习的局限性，为病理学领域的生物标志物预测任务提供了高效且具有竞争力的解决方案。

Abstract: In digital pathology, whole-slide images (WSIs) are often difficult to handle
due to their gigapixel scale, so most approaches train patch encoders via
self-supervised learning (SSL) and then aggregate the patch-level embeddings
via multiple instance learning (MIL) or slide encoders for downstream tasks.
However, patch-level SSL may overlook complex domain-specific features that are
essential for biomarker prediction, such as mutation status and molecular
characteristics, as SSL methods rely only on basic augmentations selected for
natural image domains on small patch-level area. Moreover, SSL methods remain
less data efficient than fully supervised approaches, requiring extensive
computational resources and datasets to achieve competitive performance. To
address these limitations, we present EXAONE Path 2.0, a pathology foundation
model that learns patch-level representations under direct slide-level
supervision. Using only 37k WSIs for training, EXAONE Path 2.0 achieves
state-of-the-art average performance across 10 biomarker prediction tasks,
demonstrating remarkable data efficiency.

</details>


### [34] [Learning from Sparse Point Labels for Dense Carcinosis Localization in Advanced Ovarian Cancer Assessment](https://arxiv.org/abs/2507.06643)
*Farahdiba Zarin,Riccardo Oliva,Vinkle Srivastav,Armine Vardazaryan,Andrea Rosati,Alice Zampolini Faustini,Giovanni Scambia,Anna Fagotti,Pietro Mascagni,Nicolas Padoy*

Main category: cs.CV

TL;DR: 本研究提出了一种通过稀疏像素级注释学习关键点定位的方法，特别应用于二维癌症关键点定位任务。


<details>
  <summary>Details</summary>
Motivation: 医疗领域存在稀疏标签学习的挑战，尤其在需要密集像素级注释的新任务中很困难。对仅有少量像素级注释的任务进行学习可推动那些缺乏完美标注研究的发展。

Method: 研究将问题建模为稀疏热图回归任务，提出了一种新的损失函数（Crag and Tail loss），以有效学习稀疏像素级注解，并减少误标注的影响。

Result: 通过广泛的消融实验，这种方法能够实现高效且精准的癌症关键点密集定位。

Conclusion: 该方法有助于解决密集标注难题，推动相关研究的进步。

Abstract: Learning from sparse labels is a challenge commonplace in the medical domain.
This is due to numerous factors, such as annotation cost, and is especially
true for newly introduced tasks. When dense pixel-level annotations are needed,
this becomes even more unfeasible. However, being able to learn from just a few
annotations at the pixel-level, while extremely difficult and underutilized,
can drive progress in studies where perfect annotations are not immediately
available. This work tackles the challenge of learning the dense prediction
task of keypoint localization from a few point annotations in the context of 2d
carcinosis keypoint localization from laparoscopic video frames for diagnostic
planning of advanced ovarian cancer patients. To enable this, we formulate the
problem as a sparse heatmap regression from a few point annotations per image
and propose a new loss function, called Crag and Tail loss, for efficient
learning. Our proposed loss function effectively leverages positive sparse
labels while minimizing the impact of false negatives or missed annotations.
Through an extensive ablation study, we demonstrate the effectiveness of our
approach in achieving accurate dense localization of carcinosis keypoints,
highlighting its potential to advance research in scenarios where dense
annotations are challenging to obtain.

</details>


### [35] [ClipGS: Clippable Gaussian Splatting for Interactive Cinematic Visualization of Volumetric Medical Data](https://arxiv.org/abs/2507.06647)
*Chengkun Li,Yuqi Tong,Kai Chen,Zhenya Yang,Ruiyang Li,Shi Qiu,Jason Ying-Kuen Chan,Pheng-Ann Heng,Qi Dou*

Main category: cs.CV

TL;DR: 本文介绍了一种名为ClipGS的高斯溅射框架，支持剪切面，用于交互式电影级可视化医学体数据，解决了高计算成本和渲染速度低的问题。


<details>
  <summary>Details</summary>
Motivation: 当前电影级渲染技术虽可丰富医学数据可视化，但计算成本高、渲染速度低，限制了实际应用中的交互需求。

Method: 提出了ClipGS框架并引入可学习截断方案随着剪切面动态调整可见性，同时设计了自适应调整模型优化高斯变形和渲染性能。

Result: 在五种医学体数据集验证下，达到36.635 PSNR的渲染质量，156 FPS速度，模型尺寸仅16.1 MB，优于现有方法。

Conclusion: ClipGS实现了高效高质量的医学体数据交互式可视化，有望促进医学诊断与教学应用。

Abstract: The visualization of volumetric medical data is crucial for enhancing
diagnostic accuracy and improving surgical planning and education. Cinematic
rendering techniques significantly enrich this process by providing
high-quality visualizations that convey intricate anatomical details, thereby
facilitating better understanding and decision-making in medical contexts.
However, the high computing cost and low rendering speed limit the requirement
of interactive visualization in practical applications. In this paper, we
introduce ClipGS, an innovative Gaussian splatting framework with the clipping
plane supported, for interactive cinematic visualization of volumetric medical
data. To address the challenges posed by dynamic interactions, we propose a
learnable truncation scheme that automatically adjusts the visibility of
Gaussian primitives in response to the clipping plane. Besides, we also design
an adaptive adjustment model to dynamically adjust the deformation of Gaussians
and refine the rendering performance. We validate our method on five volumetric
medical data (including CT and anatomical slice data), and reach an average
36.635 PSNR rendering quality with 156 FPS and 16.1 MB model size,
outperforming state-of-the-art methods in rendering quality and efficiency.

</details>


### [36] [Diff$^2$I2P: Differentiable Image-to-Point Cloud Registration with Diffusion Prior](https://arxiv.org/abs/2507.06651)
*Juncheng Mu,Chengwei Ren,Weixiang Zhang,Liang Pan,Xiao-Ping Zhang,Yue Gao*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的完全可微的图像到点云注册框架Diff$^2$I2P，显著提升了跨模态特征对齐的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了图像与点云数据之间的模态差异，无法保证准确的跨模态对应性。

Method: 提出了Control-Side Score Distillation (CSD) 技术结合可微PnP求解器，实现了可微的对应性优化，并通过Deformable Correspondence Tuning (DCT)模块估计对应关系。

Result: 在7-Scenes基准测试中，注册召回率提高了7%以上，性能显著优于现有方法。

Conclusion: Diffusion模型在跨模态特征学习中起到关键作用，有效提升图像到点云注册的鲁棒性。

Abstract: Learning cross-modal correspondences is essential for image-to-point cloud
(I2P) registration. Existing methods achieve this mostly by utilizing metric
learning to enforce feature alignment across modalities, disregarding the
inherent modality gap between image and point data. Consequently, this paradigm
struggles to ensure accurate cross-modal correspondences. To this end, inspired
by the cross-modal generation success of recent large diffusion models, we
propose Diff$^2$I2P, a fully Differentiable I2P registration framework,
leveraging a novel and effective Diffusion prior for bridging the modality gap.
Specifically, we propose a Control-Side Score Distillation (CSD) technique to
distill knowledge from a depth-conditioned diffusion model to directly optimize
the predicted transformation. However, the gradients on the transformation fail
to backpropagate onto the cross-modal features due to the non-differentiability
of correspondence retrieval and PnP solver. To this end, we further propose a
Deformable Correspondence Tuning (DCT) module to estimate the correspondences
in a differentiable way, followed by the transformation estimation using a
differentiable PnP solver. With these two designs, the Diffusion model serves
as a strong prior to guide the cross-modal feature learning of image and point
cloud for forming robust correspondences, which significantly improves the
registration. Extensive experimental results demonstrate that Diff$^2$I2P
consistently outperforms SoTA I2P registration methods, achieving over 7%
improvement in registration recall on the 7-Scenes benchmark.

</details>


### [37] [MS-DPPs: Multi-Source Determinantal Point Processes for Contextual Diversity Refinement of Composite Attributes in Text to Image Retrieval](https://arxiv.org/abs/2507.06654)
*Naoya Sogi,Takashi Shibata,Makoto Terao,Masanori Suganuma,Takayuki Okatani*

Main category: cs.CV

TL;DR: 本文提出了一种新的任务CDR-CA，致力于根据应用场景优化文本到图像检索的多属性多样性。


<details>
  <summary>Details</summary>
Motivation: 传统的结果多样化方法只关注图像展示的多样性，但不同的应用场景对于多样性的需求不同，需要一种能够根据上下文优化多样性的策略。

Method: 提出了Multi-Source DPPs，将传统的DPP扩展到多源，并使用统一的相似度矩阵进行建模，结合了切线归一化以表现场景上下文。

Result: 通过大量实验，验证了所提出方法在优化多属性多样性方面的有效性。

Conclusion: 提出了一种新颖的解决方法，有助于在文本到图像检索任务中更好地满足多样性需求，同时代码已开源，供进一步研究。

Abstract: Result diversification (RD) is a crucial technique in Text-to-Image Retrieval
for enhancing the efficiency of a practical application. Conventional methods
focus solely on increasing the diversity metric of image appearances. However,
the diversity metric and its desired value vary depending on the application,
which limits the applications of RD. This paper proposes a novel task called
CDR-CA (Contextual Diversity Refinement of Composite Attributes). CDR-CA aims
to refine the diversities of multiple attributes, according to the
application's context. To address this task, we propose Multi-Source DPPs, a
simple yet strong baseline that extends the Determinantal Point Process (DPP)
to multi-sources. We model MS-DPP as a single DPP model with a unified
similarity matrix based on a manifold representation. We also introduce Tangent
Normalization to reflect contexts. Extensive experiments demonstrate the
effectiveness of the proposed method. Our code is publicly available at
https://github.com/NEC-N-SOGI/msdpp.

</details>


### [38] [Enhancing Diffusion Model Stability for Image Restoration via Gradient Management](https://arxiv.org/abs/2507.06656)
*Hongjie Wu,Mingqin Zhang,Linchao He,Ji-Zhe Zhou,Jiancheng Lv*

Main category: cs.CV

TL;DR: 扩散模型在图像恢复中展现出巨大的潜力，但其贝叶斯推断框架中的去噪与似然引导交互仍未被充分探索。本文提出SPGD技术，显著改善生成稳定性，达到顶尖性能。


<details>
  <summary>Details</summary>
Motivation: 目前扩散模型的图像恢复虽然有前景，但其贝叶斯推断框架中去噪与似然之间的相互作用存在显著不稳定性，需要进行深入分析和解决。

Method: 提出了一种新型的渐进梯度管理技术——SPGD，包括(1)渐进式似然预热策略以缓解梯度冲突；(2)自适应方向动量平滑以减少梯度波动。

Result: SPGD通过在多种修复任务中的实验，显著提升了生成稳定性，在量化指标上达到最先进水平，同时视觉表现优异。

Conclusion: SPGD有效解决了扩散模型中的梯度不稳定问题，为图像恢复提供了性能更优的生成方法。

Abstract: Diffusion models have shown remarkable promise for image restoration by
leveraging powerful priors. Prominent methods typically frame the restoration
problem within a Bayesian inference framework, which iteratively combines a
denoising step with a likelihood guidance step. However, the interactions
between these two components in the generation process remain underexplored. In
this paper, we analyze the underlying gradient dynamics of these components and
identify significant instabilities. Specifically, we demonstrate conflicts
between the prior and likelihood gradient directions, alongside temporal
fluctuations in the likelihood gradient itself. We show that these
instabilities disrupt the generative process and compromise restoration
performance. To address these issues, we propose Stabilized Progressive
Gradient Diffusion (SPGD), a novel gradient management technique. SPGD
integrates two synergistic components: (1) a progressive likelihood warm-up
strategy to mitigate gradient conflicts; and (2) adaptive directional momentum
(ADM) smoothing to reduce fluctuations in the likelihood gradient. Extensive
experiments across diverse restoration tasks demonstrate that SPGD
significantly enhances generation stability, leading to state-of-the-art
performance in quantitative metrics and visually superior results. Code is
available at \href{https://github.com/74587887/SPGD}{here}.

</details>


### [39] [MK-Pose: Category-Level Object Pose Estimation via Multimodal-Based Keypoint Learning](https://arxiv.org/abs/2507.06662)
*Yifan Yang,Peili Song,Enfan Lan,Dong Liu,Jingtai Liu*

Main category: cs.CV

TL;DR: 本论文提出MK-Pose模型，通过结合RGB图像、点云和类别文本描述，解决类别级物体姿态估计中的遮挡与泛化问题，在多个数据集上超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 类别级物体姿态估计在仓库自动化和制造业中非常重要，但现有基于RGB或点云的方法在物体遮挡及实例和类别间泛化上表现不佳。

Method: 提出MK-Pose多模态关键点学习框架，使用自监督关键点检测模块、基于注意力的查询生成、软热图匹配和图结构关系建模。同时设计了图增强特征融合模块，结合局部几何信息与全局上下文。

Result: 在CAMERA25和REAL275数据集上评估，跨数据集测试于HouseCat6D，结果表明MK-Pose在IoU和平均精度上超过最新方法，且无需形状先验。

Conclusion: MK-Pose通过创新性多模态方法，显著提升了类别级物体姿态估计的性能，具有较好的跨类别和跨数据集泛化能力。

Abstract: Category-level object pose estimation, which predicts the pose of objects
within a known category without prior knowledge of individual instances, is
essential in applications like warehouse automation and manufacturing. Existing
methods relying on RGB images or point cloud data often struggle with object
occlusion and generalization across different instances and categories. This
paper proposes a multimodal-based keypoint learning framework (MK-Pose) that
integrates RGB images, point clouds, and category-level textual descriptions.
The model uses a self-supervised keypoint detection module enhanced with
attention-based query generation, soft heatmap matching and graph-based
relational modeling. Additionally, a graph-enhanced feature fusion module is
designed to integrate local geometric information and global context. MK-Pose
is evaluated on CAMERA25 and REAL275 dataset, and is further tested for
cross-dataset capability on HouseCat6D dataset. The results demonstrate that
MK-Pose outperforms existing state-of-the-art methods in both IoU and average
precision without shape priors. Codes will be released at
\href{https://github.com/yangyifanYYF/MK-Pose}{https://github.com/yangyifanYYF/MK-Pose}.

</details>


### [40] [FlexGaussian: Flexible and Cost-Effective Training-Free Compression for 3D Gaussian Splatting](https://arxiv.org/abs/2507.06671)
*Boyuan Tian,Qizhe Gao,Siran Xianyu,Xiaotong Cui,Minjia Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为FlexGaussian的方法，通过混合精度量化和属性区分修剪实现3D高斯数据的无训练压缩。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯压缩方法需求重新训练或微调，无法灵活适应多样化的压缩需求。提出一种无需训练的方法，以满足灵活性和低成本的目标。

Method: 结合混合精度量化和属性区分修剪技术，实现一种无需重新训练的压缩方法，适应不同的压缩需求。

Result: FlexGaussian实现了高达96.4%的压缩率，同时渲染质量几乎不下降（PSNR下降小于1 dB），并适用于移动设备，速度显著快于现有方法。

Conclusion: FlexGaussian是一种有效且灵活的解决方案，能够在秒级时间范围内实现高效率压缩，其表现优于现有方法并兼顾了质量与速度。

Abstract: 3D Gaussian splatting has become a prominent technique for representing and
rendering complex 3D scenes, due to its high fidelity and speed advantages.
However, the growing demand for large-scale models calls for effective
compression to reduce memory and computation costs, especially on mobile and
edge devices with limited resources. Existing compression methods effectively
reduce 3D Gaussian parameters but often require extensive retraining or
fine-tuning, lacking flexibility under varying compression constraints.
  In this paper, we introduce FlexGaussian, a flexible and cost-effective
method that combines mixed-precision quantization with attribute-discriminative
pruning for training-free 3D Gaussian compression. FlexGaussian eliminates the
need for retraining and adapts easily to diverse compression targets.
Evaluation results show that FlexGaussian achieves up to 96.4% compression
while maintaining high rendering quality (<1 dB drop in PSNR), and is
deployable on mobile devices. FlexGaussian delivers high compression ratios
within seconds, being 1.7-2.1x faster than state-of-the-art training-free
methods and 10-100x faster than training-involved approaches. The code is being
prepared and will be released soon at:
https://github.com/Supercomputing-System-AI-Lab/FlexGaussian

</details>


### [41] [Text-promptable Object Counting via Quantity Awareness Enhancement](https://arxiv.org/abs/2507.06679)
*Miaojing Shi,Xiaowen Zhang,Zijie Yue,Yong Luo,Cairong Zhao,Li Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为QUANet的新方法，用以解决文本提示对象计数问题。


<details>
  <summary>Details</summary>
Motivation: 目前的视觉语言模型在对象计数任务中表现有限，无法准确区分对象数量，需要改进模型的数量感知能力。

Method: 提出具有数量导向的文本提示和视觉-文本数量对齐损失，同时设计了一种双流自适应计数解码器，将Transformer流与CNN流通过T2C适配器联通，并引入跨流数量排序损失。

Result: 在FSC-147、CARPK、PUCPR+和ShanghaiTech等多项基准测试中表现出色，证明了其零样本类无关计数的强泛化能力。

Conclusion: QUANet通过增强数量感知和引入双流解码器设计，有效提升了对象计数任务的准确性和泛化能力。

Abstract: Recent advances in large vision-language models (VLMs) have shown remarkable
progress in solving the text-promptable object counting problem. Representative
methods typically specify text prompts with object category information in
images. This however is insufficient for training the model to accurately
distinguish the number of objects in the counting task. To this end, we propose
QUANet, which introduces novel quantity-oriented text prompts with a
vision-text quantity alignment loss to enhance the model's quantity awareness.
Moreover, we propose a dual-stream adaptive counting decoder consisting of a
Transformer stream, a CNN stream, and a number of Transformer-to-CNN
enhancement adapters (T2C-adapters) for density map prediction. The
T2C-adapters facilitate the effective knowledge communication and aggregation
between the Transformer and CNN streams. A cross-stream quantity ranking loss
is proposed in the end to optimize the ranking orders of predictions from the
two streams. Extensive experiments on standard benchmarks such as FSC-147,
CARPK, PUCPR+, and ShanghaiTech demonstrate our model's strong generalizability
for zero-shot class-agnostic counting. Code is available at
https://github.com/viscom-tongji/QUANet

</details>


### [42] [StixelNExT++: Lightweight Monocular Scene Segmentation and Representation for Collective Perception](https://arxiv.org/abs/2507.06687)
*Marcel Vosshans,Omar Ait-Aider,Youcef Mezouar,Markus Enzweiler*

Main category: cs.CV

TL;DR: 本文提出了一种新的单目感知系统场景表示方法StixelNExT++。它通过聚类更小的3D Stixel单元增强了物体分割，同时支持高效场景信息压缩。


<details>
  <summary>Details</summary>
Motivation: 为了解决单目感知系统中场景表示和物体分割的效率与精度问题。

Method: 利用轻量级神经网络推断3D Stixel，并基于自动生成的LiDAR地面实况进行训练。其方法还能转换为点云和鸟瞰视角表示。

Result: 实验在Waymo数据集上验证了其性能，在30米范围内表现优异，同时实现了10ms每帧的实时计算。

Conclusion: StixelNExT++为单目感知系统提供了一种高效的集体感知新方案，在自动驾驶等领域具有很大的应用潜力。

Abstract: This paper presents StixelNExT++, a novel approach to scene representation
for monocular perception systems. Building on the established Stixel
representation, our method infers 3D Stixels and enhances object segmentation
by clustering smaller 3D Stixel units. The approach achieves high compression
of scene information while remaining adaptable to point cloud and
bird's-eye-view representations. Our lightweight neural network, trained on
automatically generated LiDAR-based ground truth, achieves real-time
performance with computation times as low as 10 ms per frame. Experimental
results on the Waymo dataset demonstrate competitive performance within a
30-meter range, highlighting the potential of StixelNExT++ for collective
perception in autonomous systems.

</details>


### [43] [DIFFUMA: High-Fidelity Spatio-Temporal Video Prediction via Dual-Path Mamba and Diffusion Enhancement](https://arxiv.org/abs/2507.06738)
*Xinyu Xie,Weifeng Cao,Jun Shi,Yangyang Hu,Hui Liang,Wanyong Liang,Xiaoliang Qian*

Main category: cs.CV

TL;DR: 本文提出用于视频时空预测的CHDL数据集和DIFFUMA模型，在工业细化制造中表现卓越，同时改进了传统指标MSE和SSIM。


<details>
  <summary>Details</summary>
Motivation: 在半导体制造的高精度场景中，缺乏专用基准数据集，阻碍了复杂过程的建模和预测研究。

Method: 构建并发布CHDL数据集，该数据集专注于半导体切割过程，并设计了DIFFUMA模型，通过Mamba模块获取全局时序信息及扩散模块增强细节。

Result: DIFFUMA模型在CHDL数据集上显著优于现有方法，MSE降低39%，SSIM提升至0.988，并在自然现象数据集上展现了良好的泛化性。

Conclusion: 该研究不仅引入最新的SOTA模型，还为工业AI研究提供了宝贵的数据资源。

Abstract: Spatio-temporal video prediction plays a pivotal role in critical domains,
ranging from weather forecasting to industrial automation. However, in
high-precision industrial scenarios such as semiconductor manufacturing, the
absence of specialized benchmark datasets severely hampers research on modeling
and predicting complex processes. To address this challenge, we make a twofold
contribution.First, we construct and release the Chip Dicing Lane Dataset
(CHDL), the first public temporal image dataset dedicated to the semiconductor
wafer dicing process. Captured via an industrial-grade vision system, CHDL
provides a much-needed and challenging benchmark for high-fidelity process
modeling, defect detection, and digital twin development.Second, we propose
DIFFUMA, an innovative dual-path prediction architecture specifically designed
for such fine-grained dynamics. The model captures global long-range temporal
context through a parallel Mamba module, while simultaneously leveraging a
diffusion module, guided by temporal features, to restore and enhance
fine-grained spatial details, effectively combating feature degradation.
Experiments demonstrate that on our CHDL benchmark, DIFFUMA significantly
outperforms existing methods, reducing the Mean Squared Error (MSE) by 39% and
improving the Structural Similarity (SSIM) from 0.926 to a near-perfect 0.988.
This superior performance also generalizes to natural phenomena datasets. Our
work not only delivers a new state-of-the-art (SOTA) model but, more
importantly, provides the community with an invaluable data resource to drive
future research in industrial AI.

</details>


### [44] [Spatial-Temporal Graph Mamba for Music-Guided Dance Video Synthesis](https://arxiv.org/abs/2507.06689)
*Hao Tang,Ling Shao,Zhenyu Zhang,Luc Van Gool,Nicu Sebe*

Main category: cs.CV

TL;DR: 本文提出了一个名为STG-Mamba的新方法，用于基于音乐生成舞蹈视频，分为音乐到骨架的转译和骨架到视频的转译两个步骤，并通过实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决基于音乐生成舞蹈视频任务中的空间和时间依赖性，提升生成的舞蹈视频的质量和真实性。

Method: 提出了一种新颖的时空图 Mamba（STGM）模块，用于从输入音乐生成骨架序列，捕捉关节间的时空依赖关系。然后通过一种自监督正则化网络将生成的骨架序列和条件图像转译为舞蹈视频。此外，收集了一个新的骨架到视频的转译数据集包含54,944段视频片段。

Result: 实验结果表明，STG-Mamba在基于音乐生成舞蹈视频的任务上显著优于现有方法。

Conclusion: STG-Mamba有效地增强了基于音乐生成舞蹈视频的表现，展示了其在时空依赖处理和视频生成任务中的潜力。

Abstract: We propose a novel spatial-temporal graph Mamba (STG-Mamba) for the
music-guided dance video synthesis task, i.e., to translate the input music to
a dance video. STG-Mamba consists of two translation mappings:
music-to-skeleton translation and skeleton-to-video translation. In the
music-to-skeleton translation, we introduce a novel spatial-temporal graph
Mamba (STGM) block to effectively construct skeleton sequences from the input
music, capturing dependencies between joints in both the spatial and temporal
dimensions. For the skeleton-to-video translation, we propose a novel
self-supervised regularization network to translate the generated skeletons,
along with a conditional image, into a dance video. Lastly, we collect a new
skeleton-to-video translation dataset from the Internet, containing 54,944
video clips. Extensive experiments demonstrate that STG-Mamba achieves
significantly better results than existing methods.

</details>


### [45] [A Neural Representation Framework with LLM-Driven Spatial Reasoning for Open-Vocabulary 3D Visual Grounding](https://arxiv.org/abs/2507.06719)
*Zhenyang Liu,Sixiao Zheng,Siyu Chen,Cairong Zhao,Longfei Liang,Xiangyang Xue,Yanwei Fu*

Main category: cs.CV

TL;DR: 提出了一种名为SpatialReasoner的新框架，通过语言模型和视觉特性增强，用于解决开放词汇3D视觉定位中的空间关系推理问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理语言查询中包含的空间关系（如“椅子上的书”），这是因为对语言和3D场景中的空间关系推理能力不足。

Method: SpatialReasoner通过微调语言模型以捕捉空间关系，并结合视觉属性（如不透明度和颜色），构建增强特性的分层特征场，从而实现更好的目标定位。

Result: 实验表明所提框架能够无缝集成至多种神经表示中，提升3D视觉定位性能，同时增强空间推理能力。

Conclusion: SpatialReasoner有效解决了语言查询与3D场景内空间关系推理不足的问题，为增强型AI应用提供了支持。

Abstract: Open-vocabulary 3D visual grounding aims to localize target objects based on
free-form language queries, which is crucial for embodied AI applications such
as autonomous navigation, robotics, and augmented reality. Learning 3D language
fields through neural representations enables accurate understanding of 3D
scenes from limited viewpoints and facilitates the localization of target
objects in complex environments. However, existing language field methods
struggle to accurately localize instances using spatial relations in language
queries, such as ``the book on the chair.'' This limitation mainly arises from
inadequate reasoning about spatial relations in both language queries and 3D
scenes. In this work, we propose SpatialReasoner, a novel neural
representation-based framework with large language model (LLM)-driven spatial
reasoning that constructs a visual properties-enhanced hierarchical feature
field for open-vocabulary 3D visual grounding. To enable spatial reasoning in
language queries, SpatialReasoner fine-tunes an LLM to capture spatial
relations and explicitly infer instructions for the target, anchor, and spatial
relation. To enable spatial reasoning in 3D scenes, SpatialReasoner
incorporates visual properties (opacity and color) to construct a hierarchical
feature field. This field represents language and instance features using
distilled CLIP features and masks extracted via the Segment Anything Model
(SAM). The field is then queried using the inferred instructions in a
hierarchical manner to localize the target 3D instance based on the spatial
relation in the language query. Extensive experiments show that our framework
can be seamlessly integrated into different neural representations,
outperforming baseline models in 3D visual grounding while empowering their
spatial reasoning capability.

</details>


### [46] [FOLC-Net: A Federated-Optimized Lightweight Architecture for Enhanced MRI Disease Diagnosis across Axial, Coronal, and Sagittal Views](https://arxiv.org/abs/2507.06763)
*Saif Ur Rehman Khan,Muhammad Nabeel Asim,Sebastian Vollmer,Andreas Dengel*

Main category: cs.CV

TL;DR: 该论文提出了FOLC-Net框架，优化了MRI疾病诊断中单一和组合解剖视角分析的性能，其存储需求仅为0.9MB。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA模型在处理轴状、冠状和矢状解剖平面时表现下降，需要一种提升适应性的解决方案。

Method: 通过引入FOLC-Net，该框架基于1.217百万参数的轻量级架构，结合Manta-ray觅食优化机制(MRFO)、全局模型克隆和ConvNeXt组件。

Result: 实验表明FOLC-Net在包括困难的矢状角度在内的单一视角和多视角情况下均优于现有模型，达到92.44%的准确率。

Conclusion: FOLC-Net框架为分布式医疗影像分析提供了更具可靠性和适应性的解决方案，克服了SOTA模型的局限性，展示了强大的实际应用前景。

Abstract: The framework is designed to improve performance in the analysis of combined
as well as single anatomical perspectives for MRI disease diagnosis. It
specifically addresses the performance degradation observed in state-of-the-art
(SOTA) models, particularly when processing axial, coronal, and sagittal
anatomical planes. The paper introduces the FOLC-Net framework, which
incorporates a novel federated-optimized lightweight architecture with
approximately 1.217 million parameters and a storage requirement of only 0.9
MB. FOLC-Net integrates Manta-ray foraging optimization (MRFO) mechanisms for
efficient model structure generation, global model cloning for scalable
training, and ConvNeXt for enhanced client adaptability. The model was
evaluated on combined multi-view data as well as individual views, such as
axial, coronal, and sagittal, to assess its robustness in various medical
imaging scenarios. Moreover, FOLC-Net tests a ShallowFed model on different
data to evaluate its ability to generalize beyond the training dataset. The
results show that FOLC-Net outperforms existing models, particularly in the
challenging sagittal view. For instance, FOLC-Net achieved an accuracy of
92.44% on the sagittal view, significantly higher than the 88.37% accuracy of
study method (DL + Residual Learning) and 88.95% of DL models. Additionally,
FOLC-Net demonstrated improved accuracy across all individual views, providing
a more reliable and robust solution for medical image analysis in decentralized
environments. FOLC-Net addresses the limitations of existing SOTA models by
providing a framework that ensures better adaptability to individual views
while maintaining strong performance in multi-view settings. The incorporation
of MRFO, global model cloning, and ConvNeXt ensures that FOLC-Net performs
better in real-world medical applications.

</details>


### [47] [Hierarchical Feature Alignment for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2507.06732)
*Sobhan Asasi,Mohamed Ilyes Lakhal,Richard Bowden*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新颖的分层预训练策略，通过伪手语符号与对比视频语言对齐来改进手语翻译的质效。


<details>
  <summary>Details</summary>
Motivation: 解决当前手语翻译中视觉与文本表征的差距问题，同时避免手语注释的繁琐，提出更加灵活有效的无手语符号对齐方法。

Method: 设计了一种分层预训练策略，结合伪手语符号与对比视频-语言对齐，同时在帧、片段和视频层次上提取特征以优化翻译效果。

Result: 实验表明，该方法在BLEU-4和ROUGE分数上有显著提升，同时保持了效率。

Conclusion: 该方法有效缓解了视觉与文本的表征差距，证明了伪手语符号与分层对齐策略在提升手语翻译质量中的潜力。

Abstract: Sign Language Translation (SLT) attempts to convert sign language videos into
spoken sentences. However, many existing methods struggle with the disparity
between visual and textual representations during end-to-end learning.
Gloss-based approaches help to bridge this gap by leveraging structured
linguistic information. While, gloss-free methods offer greater flexibility and
remove the burden of annotation, they require effective alignment strategies.
Recent advances in Large Language Models (LLMs) have enabled gloss-free SLT by
generating text-like representations from sign videos. In this work, we
introduce a novel hierarchical pre-training strategy inspired by the structure
of sign language, incorporating pseudo-glosses and contrastive video-language
alignment. Our method hierarchically extracts features at frame, segment, and
video levels, aligning them with pseudo-glosses and the spoken sentence to
enhance translation quality. Experiments demonstrate that our approach improves
BLEU-4 and ROUGE scores while maintaining efficiency.

</details>


### [48] [MADPOT: Medical Anomaly Detection with CLIP Adaptation and Partial Optimal Transport](https://arxiv.org/abs/2507.06733)
*Mahshid Shiri,Cigdem Beyan,Vittorio Murino*

Main category: cs.CV

TL;DR: 该论文提出了一种结合视觉适配器和提示学习的新方法，以改进CLIP在医学图像异常检测中的适应性，并取得了前沿的结果。


<details>
  <summary>Details</summary>
Motivation: 由于医学图像的多样性、解剖结构变化及标注数据有限，现有异常检测方法存在较大挑战，因此需要开发更强适应性的技术。

Method: 提出结合部分最优传输（POT）和对比学习（CL）的提示学习，通过辅助多种提示和局部特征对齐捕捉微小异常，并通过CL提升类内凝聚性及类间分离性。

Result: 在少样本、零样本和跨数据集场景中均实现了先进的性能，且不依赖于合成数据或记忆库。

Conclusion: 此方法显著提升了医学图像异常检测的表现，展示了其在多种场景的潜力。

Abstract: Medical anomaly detection (AD) is challenging due to diverse imaging
modalities, anatomical variations, and limited labeled data. We propose a novel
approach combining visual adapters and prompt learning with Partial Optimal
Transport (POT) and contrastive learning (CL) to improve CLIP's adaptability to
medical images, particularly for AD. Unlike standard prompt learning, which
often yields a single representation, our method employs multiple prompts
aligned with local features via POT to capture subtle abnormalities. CL further
enforces intra-class cohesion and inter-class separation. Our method achieves
state-of-the-art results in few-shot, zero-shot, and cross-dataset scenarios
without synthetic data or memory banks. The code is available at
https://github.com/mahshid1998/MADPOT.

</details>


### [49] [Residual Prior-driven Frequency-aware Network for Image Fusion](https://arxiv.org/abs/2507.06735)
*Guan Zheng,Xue Wang,Wenhua Qian,Peng Liu,Runzhuo Ma*

Main category: cs.CV

TL;DR: 提出了一种名为RPFNet的网络，采用频率域融合、残差先验和跨促进模块，有效融合多模态图像信息，增强视觉任务表现。


<details>
  <summary>Details</summary>
Motivation: 解决多模态图像融合中长期依赖计算成本高和无地面真值导致的特征提取困难问题。

Method: 构建RPFNet网络，包括残差先验模块、频率域融合模块和跨促进模块，结合辅助解码器和多种损失函数进行高效特征建模和融合。

Result: 实验表明RPFNet能有效整合判别特征，增强纹理细节和显著对象，对高阶视觉任务具有显著促进作用。

Conclusion: RPFNet通过多模块结合与精细设计达成了高效、准确的多模态信息融合，提供了对后续视觉任务极大的支持。

Abstract: Image fusion aims to integrate complementary information across modalities to
generate high-quality fused images, thereby enhancing the performance of
high-level vision tasks. While global spatial modeling mechanisms show
promising results, constructing long-range feature dependencies in the spatial
domain incurs substantial computational costs. Additionally, the absence of
ground-truth exacerbates the difficulty of capturing complementary features
effectively. To tackle these challenges, we propose a Residual Prior-driven
Frequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a
dual-branch feature extraction framework: the Residual Prior Module (RPM)
extracts modality-specific difference information from residual maps, thereby
providing complementary priors for fusion; the Frequency Domain Fusion Module
(FDFM) achieves efficient global feature modeling and integration through
frequency-domain convolution. Additionally, the Cross Promotion Module (CPM)
enhances the synergistic perception of local details and global structures
through bidirectional feature interaction. During training, we incorporate an
auxiliary decoder and saliency structure loss to strengthen the model's
sensitivity to modality-specific differences. Furthermore, a combination of
adaptive weight-based frequency contrastive loss and SSIM loss effectively
constrains the solution space, facilitating the joint capture of local details
and global features while ensuring the retention of complementary information.
Extensive experiments validate the fusion performance of RPFNet, which
effectively integrates discriminative features, enhances texture details and
salient objects, and can effectively facilitate the deployment of the
high-level vision task.

</details>


### [50] [Democratizing High-Fidelity Co-Speech Gesture Video Generation](https://arxiv.org/abs/2507.06812)
*Xu Yang,Shaoli Huang,Shenbo Xie,Xuelin Chen,Yifei Liu,Changxing Ding*

Main category: cs.CV

TL;DR: 本论文提出了一种轻量级框架，通过融合音频和2D全身骨架特征生成高质量音频同步的共语手势视频，并引入了公共数据集CSG-405。


<details>
  <summary>Details</summary>
Motivation: 针对共语手势视频生成的音频和视觉内容间的映射多样性、大规模公共数据集的稀缺以及高计算需求的难点，作者提出了解决方案。

Method: 利用一个条件扩散模型，通过音频片段和从参考图像中提取的骨架特征融合，预测骨骼运动以确保音频协调性，最终结合人体视频生成模型生成视频。

Result: 实验表明，新方法在视觉质量和同步性方面超过了现有方法，并能广泛适配不同的说话者和上下文。

Conclusion: 提出的新方法通过结合轻量级框架与大规模CSG-405数据集，为共语手势生成提供了新方向，并显著提高了生成结果的质量。

Abstract: Co-speech gesture video generation aims to synthesize realistic,
audio-aligned videos of speakers, complete with synchronized facial expressions
and body gestures. This task presents challenges due to the significant
one-to-many mapping between audio and visual content, further complicated by
the scarcity of large-scale public datasets and high computational demands. We
propose a lightweight framework that utilizes 2D full-body skeletons as an
efficient auxiliary condition to bridge audio signals with visual outputs. Our
approach introduces a diffusion model conditioned on fine-grained audio
segments and a skeleton extracted from the speaker's reference image,
predicting skeletal motions through skeleton-audio feature fusion to ensure
strict audio coordination and body shape consistency. The generated skeletons
are then fed into an off-the-shelf human video generation model with the
speaker's reference image to synthesize high-fidelity videos. To democratize
research, we present CSG-405-the first public dataset with 405 hours of
high-resolution videos across 71 speech types, annotated with 2D skeletons and
diverse speaker demographics. Experiments show that our method exceeds
state-of-the-art approaches in visual quality and synchronization while
generalizing across speakers and contexts.

</details>


### [51] [PromptTea: Let Prompts Tell TeaCache the Optimal Threshold](https://arxiv.org/abs/2507.06739)
*Zishen Huang,Chunyu Yang,Mengyuan Ren*

Main category: cs.CV

TL;DR: 本文提出了一种名为Prompt-Complexity-Aware (PCA) 的缓存方法，提高视频生成的推断速度并保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成方法在推断速度上存在瓶颈，固定间隔缓存方法的效果在复杂场景中表现不佳。

Method: 提出了PCA缓存，依据输入提示估计场景复杂度动态调整缓存阈值，并用DynCFGCache替换静态CFGCache以提高计算效率。

Result: 在Wan2.1模型上实现2.79倍速提升，同时在多种场景中保持高质量视觉效果。

Conclusion: PCA缓存显著提高了推断速度和视觉保真度，解决了现有方法的固定频率再利用问题。

Abstract: Despite recent progress in video generation, inference speed remains a major
bottleneck. A common acceleration strategy involves reusing model outputs via
caching mechanisms at fixed intervals. However, we find that such
fixed-frequency reuse significantly degrades quality in complex scenes, while
manually tuning reuse thresholds is inefficient and lacks robustness. To
address this, we propose Prompt-Complexity-Aware (PCA) caching, a method that
automatically adjusts reuse thresholds based on scene complexity estimated
directly from the input prompt. By incorporating prompt-derived semantic cues,
PCA enables more adaptive and informed reuse decisions than conventional
caching methods. We also revisit the assumptions behind TeaCache and identify a
key limitation: it suffers from poor input-output relationship modeling due to
an oversimplified prior. To overcome this, we decouple the noisy input, enhance
the contribution of meaningful textual information, and improve the model's
predictive accuracy through multivariate polynomial feature expansion. To
further reduce computational cost, we replace the static CFGCache with
DynCFGCache, a dynamic mechanism that selectively reuses classifier-free
guidance (CFG) outputs based on estimated output variations. This allows for
more flexible reuse without compromising output quality. Extensive experiments
demonstrate that our approach achieves significant acceleration-for example,
2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across
a range of scenes.

</details>


### [52] [Dual-Granularity Cross-Modal Identity Association for Weakly-Supervised Text-to-Person Image Matching](https://arxiv.org/abs/2507.06744)
*Yafei Zhang,Yongle Shang,Huafeng Li*

Main category: cs.CV

TL;DR: 本文提出了一种局部与全局双粒度身份关联机制，用于弱监督文本到人物图像匹配，显著提升了跨模态匹配的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂的多对一身份关系时表现受限，因此需要创新方法提升弱监督文本到人物图像匹配的性能。

Method: 引入局部与全局双粒度身份关联机制。局部上，在batch内建立跨模态身份关系；全局上，以视觉模态为锚点构建动态跨模态身份网络，并引入信任度动态调整机制。同时，采用信息不对称样本对构建和一致性学习方法挖掘难样本。

Result: 实验结果显示，所提出的方法显著提高了跨模态匹配的准确性和鲁棒性。

Conclusion: 该方法为文本到人物图像匹配提供了一种高效且可行的解决方案，并在性能上具有显著提升。

Abstract: Weakly supervised text-to-person image matching, as a crucial approach to
reducing models' reliance on large-scale manually labeled samples, holds
significant research value. However, existing methods struggle to predict
complex one-to-many identity relationships, severely limiting performance
improvements. To address this challenge, we propose a local-and-global
dual-granularity identity association mechanism. Specifically, at the local
level, we explicitly establish cross-modal identity relationships within a
batch, reinforcing identity constraints across different modalities and
enabling the model to better capture subtle differences and correlations. At
the global level, we construct a dynamic cross-modal identity association
network with the visual modality as the anchor and introduce a confidence-based
dynamic adjustment mechanism, effectively enhancing the model's ability to
identify weakly associated samples while improving overall sensitivity.
Additionally, we propose an information-asymmetric sample pair construction
method combined with consistency learning to tackle hard sample mining and
enhance model robustness. Experimental results demonstrate that the proposed
method substantially boosts cross-modal matching accuracy, providing an
efficient and practical solution for text-to-person image matching.

</details>


### [53] [Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning in Multimodal LLMs](https://arxiv.org/abs/2507.06999)
*Yahan Yu,Yuyang Dong,Masafumi Oyamada*

Main category: cs.CV

TL;DR: 提出了一种名为D2I的框架，旨在通过训练中的规则奖励和无额外标注，提升多模态大模型的理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决多模态推理中模态对齐和高训练成本的挑战，尤其是现有方法依赖额外数据标注和规则奖励的问题。

Method: 引入Deliberate-to-Intuitive推理框架，训练时使用规则奖励进行深度推理，测试时转变为直觉风格以评估所学推理能力。

Result: D2I在域内和跨域基准测试中均优于对照方法。

Conclusion: 研究强调了格式奖励对多模态大模型传递推理技能的作用，同时为推理深度与测试灵活性解耦提供了新方向。

Abstract: Reasoning is a key capability for large language models (LLMs), particularly
when applied to complex tasks such as mathematical problem solving. However,
multimodal reasoning research still requires further exploration of modality
alignment and training costs. Many of these approaches rely on additional data
annotation and relevant rule-based rewards to enhance the understanding and
reasoning ability, which significantly increases training costs and limits
scalability. To address these challenges, we propose the
Deliberate-to-Intuitive reasoning framework (D2I) that improves the
understanding and reasoning ability of multimodal LLMs (MLLMs) without extra
annotations and complex rewards. Specifically, our method sets deliberate
reasoning strategies to enhance modality alignment only through the rule-based
format reward during training. While evaluating, the reasoning style shifts to
intuitive, which removes deliberate reasoning strategies during training and
implicitly reflects the model's acquired abilities in the response. D2I
outperforms baselines across both in-domain and out-of-domain benchmarks. Our
findings highlight the role of format reward in fostering transferable
reasoning skills in MLLMs, and inspire directions for decoupling training-time
reasoning depth from test-time response flexibility.

</details>


### [54] [Finetuning Vision-Language Models as OCR Systems for Low-Resource Languages: A Case Study of Manchu](https://arxiv.org/abs/2507.06761)
*Yan Hon Michael Chung,Donghyeok Choi*

Main category: cs.CV

TL;DR: 研究针对濒危满语历史文档开发高效OCR系统，通过微调三种视觉-语言模型，在合成数据上表现优异，并实现从合成到真实数据的有效迁移。


<details>
  <summary>Details</summary>
Motivation: 满语是理解近现代东欧亚历史的关键，但缺乏能处理真实历史文档的OCR系统。

Method: 微调三个开源视觉-语言模型（LLaMA-3.2-11B、Qwen2.5-VL-7B、Qwen2.5-VL-3B），利用参数高效训练，在6万张合成满语字词图像上进行训练。

Result: LLaMA-3.2-11B在合成数据上达到98.3%的词准确率和0.0024的字符错误率，在真实手写文档中达到93.1%的准确率。相比传统方法，其CRNN基线在合成数据上的准确率为99.8%，但在真实文档中的准确率仅72.5%。

Conclusion: 本研究提供了一种具成本效益且技术门槛低的濒危语言OCR框架，促进数字人文学科的发展，使历史学家和语言学家无需专业设备即可处理历史档案。

Abstract: Manchu, a critically endangered language essential for understanding early
modern Eastern Eurasian history, lacks effective OCR systems that can handle
real-world historical documents. This study develops high-performing OCR
systems by fine-tuning three open-source vision-language models (LLaMA-3.2-11B,
Qwen2.5-VL-7B, Qwen2.5-VL-3B) on 60,000 synthetic Manchu word images using
parameter-efficient training. LLaMA-3.2-11B achieved exceptional performance
with 98.3\% word accuracy and 0.0024 character error rate on synthetic data,
while crucially maintaining 93.1\% accuracy on real-world handwritten
documents. Comparative evaluation reveals substantial advantages over
traditional approaches: while a CRNN baseline achieved 99.8\% synthetic
accuracy, it suffered severe degradation to 72.5\% on real documents. Our
approach demonstrates effective synthetic-to-real domain transfer, providing a
cost-effective solution deployable on accessible infrastructure. This work
establishes a transferable framework for endangered language OCR that removes
technical and financial barriers in digital humanities, enabling historians and
linguists to process historical archives without specialized computing
resources. Code and model weights are available at
https://github.com/mic7ch1/ManchuAI-OCR.

</details>


### [55] [Physics-Grounded Motion Forecasting via Equation Discovery for Trajectory-Guided Image-to-Video Generation](https://arxiv.org/abs/2507.06830)
*Tao Feng,Xianbing Zhao,Zhenhua Chen,Tien Tsin Wong,Hamid Rezatofighi,Gholamreza Haffari,Lizhen Qu*

Main category: cs.CV

TL;DR: 新框架整合符号回归和图像到视频模型，以改进物理对齐的视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型缺乏物理对齐性，难以表现真实动态。

Method: 利用符号回归提取运动轨迹并发现运动方程，结合轨迹指导生成物理准确的视频。

Result: 在经典力学场景中验证可恢复真实运动方程，生成的物理对齐视频优于基准方法。

Conclusion: 新方法改进了视频生成中的物理对齐性，提出了一种物理驱动的视频预测方法。

Abstract: Recent advances in diffusion-based and autoregressive video generation models
have achieved remarkable visual realism. However, these models typically lack
accurate physical alignment, failing to replicate real-world dynamics in object
motion. This limitation arises primarily from their reliance on learned
statistical correlations rather than capturing mechanisms adhering to physical
laws. To address this issue, we introduce a novel framework that integrates
symbolic regression (SR) and trajectory-guided image-to-video (I2V) models for
physics-grounded video forecasting. Our approach extracts motion trajectories
from input videos, uses a retrieval-based pre-training mechanism to enhance
symbolic regression, and discovers equations of motion to forecast physically
accurate future trajectories. These trajectories then guide video generation
without requiring fine-tuning of existing models. Evaluated on scenarios in
Classical Mechanics, including spring-mass, pendulums, and projectile motions,
our method successfully recovers ground-truth analytical equations and improves
the physical alignment of generated videos over baseline methods.

</details>


### [56] [Unlocking Thermal Aerial Imaging: Synthetic Enhancement of UAV Datasets](https://arxiv.org/abs/2507.06797)
*Antonella Barisic Kulas,Andreja Jurasovic,Stjepan Bogdan*

Main category: cs.CV

TL;DR: 本文提出了一种生成合成热成像图像的程序化方法，用于无人机的搜索与救援、野生动物监测及应急响应等领域。


<details>
  <summary>Details</summary>
Motivation: 解决热成像数据采集困难及高成本带来的大规模、多样化数据集缺乏问题。

Method: 提出一种生成航空视角合成热成像图像的程序化生成管道，能控制物体在背景中的位置、比例和方向，并与背景视角对齐。同时，增强现有热成像数据集，引入新目标类别。

Result: 本文通过实验验证了为现有数据集新增类别后，在目标检测任务中的性能显著提升。同时证实热成像检测器在热成像环境下优于可见光训练的检测器。

Conclusion: 提供了高效且可扩展的热成像数据生成方法，扩展了目标检测应用领域的重要性，为无人机的热成像任务提供了潜在解决方案。

Abstract: Thermal imaging from unmanned aerial vehicles (UAVs) holds significant
potential for applications in search and rescue, wildlife monitoring, and
emergency response, especially under low-light or obscured conditions. However,
the scarcity of large-scale, diverse thermal aerial datasets limits the
advancement of deep learning models in this domain, primarily due to the high
cost and logistical challenges of collecting thermal data. In this work, we
introduce a novel procedural pipeline for generating synthetic thermal images
from an aerial perspective. Our method integrates arbitrary object classes into
existing thermal backgrounds by providing control over the position, scale, and
orientation of the new objects, while aligning them with the viewpoints of the
background. We enhance existing thermal datasets by introducing new object
categories, specifically adding a drone class in urban environments to the
HIT-UAV dataset and an animal category to the MONET dataset. In evaluating
these datasets for object detection task, we showcase strong performance across
both new and existing classes, validating the successful expansion into new
applications. Through comparative analysis, we show that thermal detectors
outperform their visible-light-trained counterparts and highlight the
importance of replicating aerial viewing angles. Project page:
https://github.com/larics/thermal_aerial_synthetic.

</details>


### [57] [GreenHyperSpectra: A multi-source hyperspectral dataset for global vegetation trait prediction](https://arxiv.org/abs/2507.06806)
*Eya Cherif,Arthur Ouaknine,Luke A. Brown,Phuong D. Dao,Kyle R. Kovach,Bing Lu,Daniel Mederer,Hannes Feilhauer,Teja Kattenborn,David Rolnick*

Main category: cs.CV

TL;DR: 该研究以GreenHyperSpectra数据集来预训练机器学习模型，以提升基于光谱数据的植物性状预测能力，超越了传统监督方法。


<details>
  <summary>Details</summary>
Motivation: 传统的实地采样方法难以在空间尺度上获取生态意义明显的植物性状变化数据，研究目的在于借助光谱数据和机器学习方法提升植物性状预测能力。

Method: 提出了一个名为GreenHyperSpectra的预训练数据集，并通过半监督和自监督方法，结合多输出回归模型，进行跨域植物性状预测的研究。

Result: 利用GreenHyperSpectra数据集，研究构建的模型在性状预测中超越了传统监督方法，表现了显著的改进。

Conclusion: 本文为植物性状的光谱表示学习奠定了新的方法学框架，加速了代表性学习和植物功能性状评估研究的有机融合。

Abstract: Plant traits such as leaf carbon content and leaf mass are essential
variables in the study of biodiversity and climate change. However,
conventional field sampling cannot feasibly cover trait variation at
ecologically meaningful spatial scales. Machine learning represents a valuable
solution for plant trait prediction across ecosystems, leveraging hyperspectral
data from remote sensing. Nevertheless, trait prediction from hyperspectral
data is challenged by label scarcity and substantial domain shifts (\eg across
sensors, ecological distributions), requiring robust cross-domain methods.
Here, we present GreenHyperSpectra, a pretraining dataset encompassing
real-world cross-sensor and cross-ecosystem samples designed to benchmark trait
prediction with semi- and self-supervised methods. We adopt an evaluation
framework encompassing in-distribution and out-of-distribution scenarios. We
successfully leverage GreenHyperSpectra to pretrain label-efficient
multi-output regression models that outperform the state-of-the-art supervised
baseline. Our empirical analyses demonstrate substantial improvements in
learning spectral representations for trait prediction, establishing a
comprehensive methodological framework to catalyze research at the intersection
of representation learning and plant functional traits assessment. All code and
data are available at: https://github.com/echerif18/HyspectraSSL.

</details>


### [58] [IAP: Invisible Adversarial Patch Attack through Perceptibility-Aware Localization and Perturbation Optimization](https://arxiv.org/abs/2507.06856)
*Subrat Kishore Dutta,Xiao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的攻击框架IAP，用于生成具有高度隐形的对抗性补丁，并在目标攻击情景中实现较高的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗性补丁方法难以在目标攻击场景中表现良好，同时缺乏上下文一致性且易被人类和自动补丁防御系统检测。

Method: 提出了IAP框架，通过基于感知性的局部化和扰动优化方法，利用类级定位和敏感度图进行补丁位置优化，以及感知性正则化的对抗性损失和颜色一致性的梯度更新规则来优化隐形扰动。

Result: 实验表明IAP在多种图像基准和模型架构上，显著提高了补丁的隐形性，同时在目标攻击中表现出了有竞争力的成功率，并使多个先进的补丁防御方法失效。

Conclusion: IAP框架生成的对抗性补丁在隐蔽性和攻击效果两方面均优于现有方法，是目标攻击的有效解决方案。

Abstract: Despite modifying only a small localized input region, adversarial patches
can drastically change the prediction of computer vision models. However, prior
methods either cannot perform satisfactorily under targeted attack scenarios or
fail to produce contextually coherent adversarial patches, causing them to be
easily noticeable by human examiners and insufficiently stealthy against
automatic patch defenses. In this paper, we introduce IAP, a novel attack
framework that generates highly invisible adversarial patches based on
perceptibility-aware localization and perturbation optimization schemes.
Specifically, IAP first searches for a proper location to place the patch by
leveraging classwise localization and sensitivity maps, balancing the
susceptibility of patch location to both victim model prediction and human
visual system, then employs a perceptibility-regularized adversarial loss and a
gradient update rule that prioritizes color constancy for optimizing invisible
perturbations. Comprehensive experiments across various image benchmarks and
model architectures demonstrate that IAP consistently achieves competitive
attack success rates in targeted settings with significantly improved patch
invisibility compared to existing baselines. In addition to being highly
imperceptible to humans, IAP is shown to be stealthy enough to render several
state-of-the-art patch defenses ineffective.

</details>


### [59] [HVI-CIDNet+: Beyond Extreme Darkness for Low-Light Image Enhancement](https://arxiv.org/abs/2507.06814)
*Qingsen Yan,Kangbiao Shi,Yixu Feng,Tao Hu,Peng Wu,Guansong Pang,Yanning Zhang*

Main category: cs.CV

TL;DR: 提出了一种新的低光图像增强方法，利用HVI颜色空间和HVI-CIDNet+网络，可显著提升低光图像的质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于sRGB和HSV颜色空间的低光图像增强方法容易出现颜色偏差或噪声伪影，需要改进。

Method: 提出了HVI颜色空间和基于该颜色空间的HVI-CIDNet+网络，分别通过颜色地图与可学习的强度去除红色和黑色噪声，同时利用先验引导注意力块结合语义先验与退化信息改进极暗区域的恢复效果。

Result: 在10个数据集上的基准实验表明，所提出的方法在低光图像增强质量上优于现有方法。

Conclusion: HVI颜色空间和HVI-CIDNet+网络有效缓解了低光图像的颜色失真与亮度问题，为低光图像增强提供了一种新的解决方案。

Abstract: Low-Light Image Enhancement (LLIE) aims to restore vivid content and details
from corrupted low-light images. However, existing standard RGB (sRGB) color
space-based LLIE methods often produce color bias and brightness artifacts due
to the inherent high color sensitivity. While Hue, Saturation, and Value (HSV)
color space can decouple brightness and color, it introduces significant red
and black noise artifacts. To address this problem, we propose a new color
space for LLIE, namely Horizontal/Vertical-Intensity (HVI), defined by the HV
color map and learnable intensity. The HV color map enforces small distances
for the red coordinates to remove red noise artifacts, while the learnable
intensity compresses the low-light regions to remove black noise artifacts.
Additionally, we introduce the Color and Intensity Decoupling Network+
(HVI-CIDNet+), built upon the HVI color space, to restore damaged content and
mitigate color distortion in extremely dark regions. Specifically, HVI-CIDNet+
leverages abundant contextual and degraded knowledge extracted from low-light
images using pre-trained vision-language models, integrated via a novel
Prior-guided Attention Block (PAB). Within the PAB, latent semantic priors can
promote content restoration, while degraded representations guide precise color
correction, both particularly in extremely dark regions through the
meticulously designed cross-attention fusion mechanism. Furthermore, we
construct a Region Refinement Block that employs convolution for
information-rich regions and self-attention for information-scarce regions,
ensuring accurate brightness adjustments. Comprehensive results from benchmark
experiments demonstrate that the proposed HVI-CIDNet+ outperforms the
state-of-the-art methods on 10 datasets.

</details>


### [60] [Know Your Attention Maps: Class-specific Token Masking for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2507.06848)
*Joelle Hanna,Damian Borth*

Main category: cs.CV

TL;DR: 提出了一种基于ViT的端到端方法，通过稀疏ViT和随机掩码策略生成高质量伪分割掩码，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统WSSS对外部模块依赖的问题，提出一种直接利用ViT自注意力机制的端到端方法，提高伪分割掩码的质量与效率。

Method: 设计一种稀疏ViT，使用多个与类别相关的[CLS]标记，并采用随机掩码策略进行训练；推理时根据预测标签聚合各[CLS]标记的自注意力图生成伪分割掩码。

Result: 在两个标准基准和三个专用数据集上的实验表明，该方法生成的伪分割掩码准确性高，优于相关工作。

Conclusion: 提出的方法不仅能提升自注意力图的可解释性和分类准确性，还能在无需细粒度标注数据的情况下接近完全监督模型的性能。

Abstract: Weakly Supervised Semantic Segmentation (WSSS) is a challenging problem that
has been extensively studied in recent years. Traditional approaches often rely
on external modules like Class Activation Maps to highlight regions of interest
and generate pseudo segmentation masks. In this work, we propose an end-to-end
method that directly utilizes the attention maps learned by a Vision
Transformer (ViT) for WSSS. We propose training a sparse ViT with multiple
[CLS] tokens (one for each class), using a random masking strategy to promote
[CLS] token - class assignment. At inference time, we aggregate the different
self-attention maps of each [CLS] token corresponding to the predicted labels
to generate pseudo segmentation masks. Our proposed approach enhances the
interpretability of self-attention maps and ensures accurate class assignments.
Extensive experiments on two standard benchmarks and three specialized datasets
demonstrate that our method generates accurate pseudo-masks, outperforming
related works. Those pseudo-masks can be used to train a segmentation model
which achieves results comparable to fully-supervised models, significantly
reducing the need for fine-grained labeled data.

</details>


### [61] [Longitudinal Study of Facial Biometrics at the BEZ: Temporal Variance Analysis](https://arxiv.org/abs/2507.06858)
*Mathias Schulz,Alexander Spenke,Pia Funk,Florian Blümel,Markus Rohde,Ralph Breithaupt,Gerd Nolden,Norbert Jung,Robert Lange*

Main category: cs.CV

TL;DR: 本研究通过对超过400名不同族群、性别和年龄的参与者的长期生物识别评估，发现短期波动比长期波动更显著。


<details>
  <summary>Details</summary>
Motivation: 通过长期生物识别数据分析，探索生物识别特征的稳定性与变化。

Method: 使用包含238,000多个数据集的GDPR合规数据库，应用先进的面部识别算法对长期对比分数进行分析。

Result: 发现同一测量环境中，个体间隔日波动大于长期随时间变化的波动。

Conclusion: 研究强调了在受控环境中长时间测试同一人群生物特征的重要性，为未来的生物识别数据分析奠定基础。

Abstract: This study presents findings from long-term biometric evaluations conducted
at the Biometric Evaluation Center (bez). Over the course of two and a half
years, our ongoing research with over 400 participants representing diverse
ethnicities, genders, and age groups were regularly assessed using a variety of
biometric tools and techniques at the controlled testing facilities. Our
findings are based on the General Data Protection Regulation-compliant local
bez database with more than 238.000 biometric data sets categorized into
multiple biometric modalities such as face and finger. We used state-of-the-art
face recognition algorithms to analyze long-term comparison scores. Our results
show that these scores fluctuate more significantly between individual days
than over the entire measurement period. These findings highlight the
importance of testing biometric characteristics of the same individuals over a
longer period of time in a controlled measurement environment and lays the
groundwork for future advancements in biometric data analysis.

</details>


### [62] [SemRaFiner: Panoptic Segmentation in Sparse and Noisy Radar Point Clouds](https://arxiv.org/abs/2507.06906)
*Matthias Zeller,Daniel Casado Herraez,Bengisu Ayan,Jens Behley,Michael Heidingsfeld,Cyrill Stachniss*

Main category: cs.CV

TL;DR: 本文提出了SemRaFiner方法，针对稀疏雷达点云进行全景分割以增强场景理解，并通过优化特征提取和训练过程提高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有相机和LiDAR在恶劣天气下的局限性，并改善雷达稀疏点云中运动体检测与语义分割的性能。

Method: 提出SemRaFiner方法，优化稀疏雷达点云的特征提取，并通过新增数据增强方法改进实例分配的训练过程。

Result: 实验表明，提出的方法在基于雷达的全景分割任务中优于当前先进方法。

Conclusion: SemRaFiner可以增强稀疏雷达点云的场景理解性能，尤其是在运动体检测的全景分割方面。

Abstract: Semantic scene understanding, including the perception and classification of
moving agents, is essential to enabling safe and robust driving behaviours of
autonomous vehicles. Cameras and LiDARs are commonly used for semantic scene
understanding. However, both sensor modalities face limitations in adverse
weather and usually do not provide motion information. Radar sensors overcome
these limitations and directly offer information about moving agents by
measuring the Doppler velocity, but the measurements are comparably sparse and
noisy. In this paper, we address the problem of panoptic segmentation in sparse
radar point clouds to enhance scene understanding. Our approach, called
SemRaFiner, accounts for changing density in sparse radar point clouds and
optimizes the feature extraction to improve accuracy. Furthermore, we propose
an optimized training procedure to refine instance assignments by incorporating
a dedicated data augmentation. Our experiments suggest that our approach
outperforms state-of-the-art methods for radar-based panoptic segmentation.

</details>


### [63] [Adaptive Part Learning for Fine-Grained Generalized Category Discovery: A Plug-and-Play Enhancement](https://arxiv.org/abs/2507.06928)
*Qiyuan Dai,Hanzhuo Huang,Yu Wu,Sibei Yang*

Main category: cs.CV

TL;DR: 提出一种新的方法APL，通过发现和学习自适应的对象部分来改进通用类别发现任务中的表征和知识转移。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法中仅依赖全局表征导致的区分能力与泛化能力的权衡问题，同时改进细粒度数据集上的表现。

Method: 引入一种自适应部分发现与学习的方法（APL），基于共享可学习的部分查询和DINO部分先验生成一致的对象部分及其对应关系，辅以新的all-min对比损失，适应性地高亮区分性对象部分并共享其它部分实现知识转移。

Result: 在不同的GCD框架中替换CLS标记特征为部分表征后，在细粒度数据集上表现显著提升。

Conclusion: APL方法有效提高了分类的区分性和泛化性，为通用类别发现任务提供了一种新的解决方案。

Abstract: Generalized Category Discovery (GCD) aims to recognize unlabeled images from
known and novel classes by distinguishing novel classes from known ones, while
also transferring knowledge from another set of labeled images with known
classes. Existing GCD methods rely on self-supervised vision transformers such
as DINO for representation learning. However, focusing solely on the global
representation of the DINO CLS token introduces an inherent trade-off between
discriminability and generalization. In this paper, we introduce an adaptive
part discovery and learning method, called APL, which generates consistent
object parts and their correspondences across different similar images using a
set of shared learnable part queries and DINO part priors, without requiring
any additional annotations. More importantly, we propose a novel all-min
contrastive loss to learn discriminative yet generalizable part representation,
which adaptively highlights discriminative object parts to distinguish similar
categories for enhanced discriminability while simultaneously sharing other
parts to facilitate knowledge transfer for improved generalization. Our APL can
easily be incorporated into different GCD frameworks by replacing their CLS
token feature with our part representations, showing significant enhancements
on fine-grained datasets.

</details>


### [64] [CheXPO: Preference Optimization for Chest X-ray VLMs with Counterfactual Rationale](https://arxiv.org/abs/2507.06959)
*Xiao Liang,Jiawei Hu,Di Wang,Zhi Ma,Lin Zhao,Ronghan Li,Bo Wan,Quan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为CheXPO的胸部X光偏好优化策略，结合信心相似性联合挖掘和对抗事实推理，以解决现有视觉语言模型(VLMs)的幻觉问题，并减少对专家标注的依赖。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在医学应用中存在的幻觉问题，这些问题会显著影响模型的可靠性，同时克服临床上不相关样本、数据分布失衡以及专家标注成本高的障碍。

Method: 提出CheXPO策略：1. 构建统一的精细多任务胸部X光视觉指令数据集用于监督微调(SFT)；2. 通过SFT失败的Token级信心分析识别困难样本，并用基于相似性的检索方法扩展困难样本；3. 使用生成的对抗事实推理提供精细的临床偏好，无需额外的专家输入。

Result: CheXPO仅需要5%的SFT样本，即获得了8.93%的相对性能提升，并在多种临床任务中达到了最新水平。

Conclusion: CheXPO在提高胸部X光VLM可靠性的同时提供了可扩展和可解释的解决方案，具有实际的放射学应用价值。

Abstract: Vision-language models (VLMs) are prone to hallucinations that critically
compromise reliability in medical applications. While preference optimization
can mitigate these hallucinations through clinical feedback, its implementation
faces challenges such as clinically irrelevant training samples, imbalanced
data distributions, and prohibitive expert annotation costs. To address these
challenges, we introduce CheXPO, a Chest X-ray Preference Optimization strategy
that combines confidence-similarity joint mining with counterfactual rationale.
Our approach begins by synthesizing a unified, fine-grained multi-task chest
X-ray visual instruction dataset across different question types for supervised
fine-tuning (SFT). We then identify hard examples through token-level
confidence analysis of SFT failures and use similarity-based retrieval to
expand hard examples for balancing preference sample distributions, while
synthetic counterfactual rationales provide fine-grained clinical preferences,
eliminating the need for additional expert input. Experiments show that CheXPO
achieves 8.93% relative performance gain using only 5% of SFT samples, reaching
state-of-the-art performance across diverse clinical tasks and providing a
scalable, interpretable solution for real-world radiology applications.

</details>


### [65] [MCCD: A Multi-Attribute Chinese Calligraphy Character Dataset Annotated with Script Styles, Dynasties, and Calligraphers](https://arxiv.org/abs/2507.06948)
*Yixin Zhao,Yuyi Zhang,Lianwen Jin*

Main category: cs.CV

TL;DR: 本文提出了一个名为MCCD的全新多属性中国书法字符数据集，包含种类丰富的书法属性信息，并建立了基准测试平台。


<details>
  <summary>Details</summary>
Motivation: 研究书法的风格、朝代和书法家等属性信息具有重要的文化与历史价值，但由于书法风格演变及独特性，准确识别其属性极具挑战性。同时，目前书法数据集稀缺且多为单一属性标注，这限制了深入研究。

Method: 通过创建一个多属性书法字符数据集MCCD，涵盖7,765类共329,715张样本图像，并基于书体风格、朝代和书法家三种属性构建子集，同时进行单任务和多任务识别实验以建立基准性能。

Result: 实验结果表明，书法字符的笔画结构复杂性及其不同属性间的相互作用，显著增加了准确识别的难度。该数据集为书法字符识别、书法家识别以及汉字演变研究提供了新的资源。

Conclusion: MCCD数据集不仅填补了书法数据集的空白，还为书法研究乃至其他相关领域的进步铺平了道路。

Abstract: Research on the attribute information of calligraphy, such as styles,
dynasties, and calligraphers, holds significant cultural and historical value.
However, the styles of Chinese calligraphy characters have evolved dramatically
through different dynasties and the unique touches of calligraphers, making it
highly challenging to accurately recognize these different characters and their
attributes. Furthermore, existing calligraphic datasets are extremely scarce,
and most provide only character-level annotations without additional attribute
information. This limitation has significantly hindered the in-depth study of
Chinese calligraphy. To fill this gap, we present a novel Multi-Attribute
Chinese Calligraphy Character Dataset (MCCD). The dataset encompasses 7,765
categories with a total of 329,715 isolated image samples of Chinese
calligraphy characters, and three additional subsets were extracted based on
the attribute labeling of the three types of script styles (10 types),
dynasties (15 periods) and calligraphers (142 individuals). The rich
multi-attribute annotations render MCCD well-suited diverse research tasks,
including calligraphic character recognition, writer identification, and
evolutionary studies of Chinese characters. We establish benchmark performance
through single-task and multi-task recognition experiments across MCCD and all
of its subsets. The experimental results demonstrate that the complexity of the
stroke structure of the calligraphic characters, and the interplay between
their different attributes, leading to a substantial increase in the difficulty
of accurate recognition. MCCD not only fills a void in the availability of
detailed calligraphy datasets but also provides valuable resources for
advancing research in Chinese calligraphy and fostering advancements in
multiple fields. The dataset is available at
https://github.com/SCUT-DLVCLab/MCCD.

</details>


### [66] [Pre-Columbian Settlements Shaped Palm Clusters in the Sierra Nevada de Santa Marta, Colombia](https://arxiv.org/abs/2507.06949)
*Sebastian Fajardo,Sina Mohammadi,Jonas Gregorio de Souza,César Ardila,Alan Tapscott Baltar,Shaddai Heidgen,Maria Isabel Mayorga Hernández,Sylvia Mota de Oliveira,Fernando Montejo,Marco Moderato,Vinicius Peripato,Katy Puche,Carlos Reina,Juan Carlos Vargas,Frank W. Takes,Marco Madella*

Main category: cs.CV

TL;DR: 研究利用深度学习和卫星影像分析了解古代人类对新热带森林长期影响，发现当地棕榈树分布与考古遗址密切相关。


<details>
  <summary>Details</summary>
Motivation: 探讨古代人类在新热带森林中的长期管理对当地生态系统的高分辨率影响，并弥补传统考古证据的局限性。

Method: 通过深度学习模型识别卫星图像中的棕榈树分布，并结合聚类算法估算古代管理区域，同时整合考古遗址数据和手工标注的数据集。

Result: 研究发现，棕榈树在考古遗址附近的分布显著更密集，反映了古代人类活动对植物分布的影响，最大棕榈聚集区域表明古代管理区域可能比考古证据表明的规模大两个数量级。

Conclusion: 前哥伦布时期的人类活动显著影响了当地植被分布，为基础设施建设创造条件，研究表明结合人工智能方法可以揭示古代人类与环境的精细互动。

Abstract: Ancient populations markedly transformed Neotropical forests, yet
understanding the long-term effects of ancient human management, particularly
at high-resolution scales, remains challenging. In this work we propose a new
approach to investigate archaeological areas of influence based on vegetation
signatures. It consists of a deep learning model trained on satellite imagery
to identify palm trees, followed by a clustering algorithm to identify palm
clusters, which are then used to estimate ancient management areas. To assess
the palm distribution in relation to past human activity, we applied the
proposed approach to unique high-resolution satellite imagery data covering 765
km2 of the Sierra Nevada de Santa Marta, Colombia. With this work, we also
release a manually annotated palm tree dataset along with estimated locations
of archaeological sites from ground-surveys and legacy records. Results
demonstrate how palms were significantly more abundant near archaeological
sites showing large infrastructure investment. The extent of the largest palm
cluster indicates that ancient human-managed areas linked to major
infrastructure sites may be up to two orders of magnitude bigger than indicated
by archaeological evidence alone. Our findings suggest that pre-Columbian
populations influenced local vegetation fostering conditions conducive to palm
proliferation, leaving a lasting ecological footprint. This may have lowered
the logistical costs of establishing infrastructure-heavy settlements in
otherwise less accessible locations. Overall, this study demonstrates the
potential of integrating artificial intelligence approaches with new ecological
and archaeological data to identify archaeological areas of interest through
vegetation patterns, revealing fine-scale human-environment interactions.

</details>


### [67] [MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology Report Generation](https://arxiv.org/abs/2507.06992)
*Qilong Xing,Zikai Song,Youjia Zhang,Na Feng,Junqing Yu,Wei Yang*

Main category: cs.CV

TL;DR: 提出了一种名为MCA-RG的新框架，通过对视觉特征和医学概念的匹配，优化放射学报告生成过程，在两个公共数据库上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型难以准确映射病理与解剖特征到文本描述，造成临床应用困难。

Method: 利用两种医学概念库（病理库和解剖库），结合对比学习、匹配损失和低质量特征过滤等方法，增强视觉特征并优化与医学概念的结合。

Result: 在MIMIC-CXR和CheXpert Plus公开数据集上表现优越，优于现有方法。

Conclusion: 通过显式对齐视觉特征与医学概念，能有效提升放射学报告生成的准确性和临床适用性。

Abstract: Despite significant advancements in adapting Large Language Models (LLMs) for
radiology report generation (RRG), clinical adoption remains challenging due to
difficulties in accurately mapping pathological and anatomical features to
their corresponding text descriptions. Additionally, semantic agnostic feature
extraction further hampers the generation of accurate diagnostic reports. To
address these challenges, we introduce Medical Concept Aligned Radiology Report
Generation (MCA-RG), a knowledge-driven framework that explicitly aligns visual
features with distinct medical concepts to enhance the report generation
process. MCA-RG utilizes two curated concept banks: a pathology bank containing
lesion-related knowledge, and an anatomy bank with anatomical descriptions. The
visual features are aligned with these medical concepts and undergo tailored
enhancement. We further propose an anatomy-based contrastive learning procedure
to improve the generalization of anatomical features, coupled with a matching
loss for pathological features to prioritize clinically relevant regions.
Additionally, a feature gating mechanism is employed to filter out low-quality
concept features. Finally, the visual features are corresponding to individual
medical concepts, and are leveraged to guide the report generation process.
Experiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate
that MCA-RG achieves superior performance, highlighting its effectiveness in
radiology report generation.

</details>


### [68] [Segmentation Regularized Training for Multi-Domain Deep Learning Registration applied to MR-Guided Prostate Cancer Radiotherapy](https://arxiv.org/abs/2507.06966)
*Sudharsan Madhavan,Chengcheng Gui,Lando Bosma,Josiah Simeth,Jue Jiang,Nicolas Cote,Nima Hassan Rezaeian,Himanshu Nagar,Victoria Brennan,Neelam Tyagi,Harini Veeraraghavan*

Main category: cs.CV

TL;DR: 研究介绍了一种名为ProRSeg的深度学习变形图像配准方法，适用于多域MR-MR配准，用于MRgART中的轮廓传播和剂量累积验证。


<details>
  <summary>Details</summary>
Motivation: 在MR引导的自适应放疗中，需要精确的变形图像配准（DIR）来进行轮廓传播和剂量累积。然而，目前在域不变的MR-MR配准中存在挑战。

Method: 采用ProRSeg方法，这是一种进阶的配准和分割技术，基于加权分割一致性损失，通过262对3T MR扫描图像进行训练，并在不同域的数据集上进行测试，进行轮廓传播和剂量累积分析。

Result: ProRSeg在膀胱的配准中表现出了跨域的泛化能力，对直肠和临床靶体积（CTV）的性能依赖于特定域。对于剂量累积，研究发现大多数患者满足CTV覆盖和膀胱保护的临床限制，但目标剂量上限制的达标率较低。

Conclusion: ProRSeg在多域MR-MR配准中表现出合理的性能，在前列腺癌患者中显示了初步的可行性，并可用于评估治疗是否符合临床限制。

Abstract: Background: Accurate deformable image registration (DIR) is required for
contour propagation and dose accumulation in MR-guided adaptive radiotherapy
(MRgART). This study trained and evaluated a deep learning DIR method for
domain invariant MR-MR registration. Methods: A progressively refined
registration and segmentation (ProRSeg) method was trained with 262 pairs of 3T
MR simulation scans from prostate cancer patients using weighted segmentation
consistency loss. ProRSeg was tested on same- (58 pairs), cross- (72 1.5T MR
Linac pairs), and mixed-domain (42 MRSim-MRL pairs) datasets for contour
propagation accuracy of clinical target volume (CTV), bladder, and rectum. Dose
accumulation was performed for 42 patients undergoing 5-fraction MRgART.
Results: ProRSeg demonstrated generalization for bladder with similar Dice
Similarity Coefficients across domains (0.88, 0.87, 0.86). For rectum and CTV,
performance was domain-dependent with higher accuracy on cross-domain MRL
dataset (DSCs 0.89) versus same-domain data. The model's strong cross-domain
performance prompted us to study the feasibility of using it for dose
accumulation. Dose accumulation showed 83.3% of patients met CTV coverage (D95
>= 40.0 Gy) and bladder sparing (D50 <= 20.0 Gy) constraints. All patients
achieved minimum mean target dose (>40.4 Gy), but only 9.5% remained under
upper limit (<42.0 Gy). Conclusions: ProRSeg showed reasonable multi-domain
MR-MR registration performance for prostate cancer patients with preliminary
feasibility for evaluating treatment compliance to clinical constraints.

</details>


### [69] [Cross-Modality Masked Learning for Survival Prediction in ICI Treated NSCLC Patients](https://arxiv.org/abs/2507.06994)
*Qilong Xing,Zikai Song,Bingxin Gong,Lian Yang,Junqing Yu,Wei Yang*

Main category: cs.CV

TL;DR: 该研究提出了一种新框架，通过多模态特征融合，提高非小细胞肺癌（NSCLC）患者生存预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 为实现个性化治疗规划，需要准确预测接受免疫治疗的NSCLC患者的预后。然而，缺乏大规模相关数据集和有效的多模态特征融合策略成为主要障碍。

Method: 研究提供了一个大规模的数据集，包括NSCLC患者的3D CT影像、临床记录、无进展生存期（PFS）和总体生存期（OS）数据，并提出了一种交叉模态掩码学习方法。该方法包含两个分支：针对CT影像的Slice-Depth Transformer，以及针对临床变量的基于图的Transformer，利用掩码模态学习策略融合模态特征。

Result: 在NSCLC生存预测的多模态集成中表现优于现有方法，设定了新的预测基准。

Conclusion: 所提出的方法改进了模态特异性特征的整合，显著提升了NSCLC患者预后的预测性能。

Abstract: Accurate prognosis of non-small cell lung cancer (NSCLC) patients undergoing
immunotherapy is essential for personalized treatment planning, enabling
informed patient decisions, and improving both treatment outcomes and quality
of life. However, the lack of large, relevant datasets and effective
multi-modal feature fusion strategies pose significant challenges in this
domain. To address these challenges, we present a large-scale dataset and
introduce a novel framework for multi-modal feature fusion aimed at enhancing
the accuracy of survival prediction. The dataset comprises 3D CT images and
corresponding clinical records from NSCLC patients treated with immune
checkpoint inhibitors (ICI), along with progression-free survival (PFS) and
overall survival (OS) data. We further propose a cross-modality masked learning
approach for medical feature fusion, consisting of two distinct branches, each
tailored to its respective modality: a Slice-Depth Transformer for extracting
3D features from CT images and a graph-based Transformer for learning node
features and relationships among clinical variables in tabular data. The fusion
process is guided by a masked modality learning strategy, wherein the model
utilizes the intact modality to reconstruct missing components. This mechanism
improves the integration of modality-specific features, fostering more
effective inter-modality relationships and feature interactions. Our approach
demonstrates superior performance in multi-modal integration for NSCLC survival
prediction, surpassing existing methods and setting a new benchmark for
prognostic models in this context.

</details>


### [70] [Hallucinating 360°: Panoramic Street-View Generation via Local Scenes Diffusion and Probabilistic Prompting](https://arxiv.org/abs/2507.06971)
*Fei Teng,Kai Luo,Sheng Wu,Siyu Li,Pujun Guo,Jiale Wei,Kunyu Peng,Jiaming Zhang,Kailun Yang*

Main category: cs.CV

TL;DR: 提出了一种名为Percep360的全景生成方法，专注于自主驾驶任务中数据的一致性与可控制性，提升了图像质量并优化了下游感知模型性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决自主驾驶中全景数据获取复杂、耗时及仅依赖现有数据分布的限制，研究了一种高质量、可控的全景数据生成方法。

Method: 设计了局部场景扩散方法 (LSDM) 用于解决全景生成中的信息丢失问题，并提出概率提示方法 (PPM) 以实现全景图像的可控生成。

Result: 生成的数据在无参照质量指标上优于原始拼接图像，并且提升了下游Bird's Eye View (BEV)感知任务的表现。

Conclusion: Percep360方法有效实现了高质量和可控的全景数据生成，同时提高了其在实际自主驾驶场景中的实用性。

Abstract: Panoramic perception holds significant potential for autonomous driving,
enabling vehicles to acquire a comprehensive 360{\deg} surround view in a
single shot. However, autonomous driving is a data-driven task. Complete
panoramic data acquisition requires complex sampling systems and annotation
pipelines, which are time-consuming and labor-intensive. Although existing
street view generation models have demonstrated strong data regeneration
capabilities, they can only learn from the fixed data distribution of existing
datasets and cannot achieve high-quality, controllable panoramic generation. In
this paper, we propose the first panoramic generation method Percep360 for
autonomous driving. Percep360 enables coherent generation of panoramic data
with control signals based on the stitched panoramic data. Percep360 focuses on
two key aspects: coherence and controllability. Specifically, to overcome the
inherent information loss caused by the pinhole sampling process, we propose
the Local Scenes Diffusion Method (LSDM). LSDM reformulates the panorama
generation as a spatially continuous diffusion process, bridging the gaps
between different data distributions. Additionally, to achieve the controllable
generation of panoramic images, we propose a Probabilistic Prompting Method
(PPM). PPM dynamically selects the most relevant control cues, enabling
controllable panoramic image generation. We evaluate the effectiveness of the
generated images from three perspectives: image quality assessment (i.e.,
no-reference and with reference), controllability, and their utility in
real-world Bird's Eye View (BEV) segmentation. Notably, the generated data
consistently outperforms the original stitched images in no-reference quality
metrics and enhances downstream perception models. The source code will be
publicly available at https://github.com/Bryant-Teng/Percep360.

</details>


### [71] [A multi-modal dataset for insect biodiversity with imagery and DNA at the trap and individual level](https://arxiv.org/abs/2507.06972)
*Johanna Orsholm,John Quinto,Hannu Autto,Gaia Banelyte,Nicolas Chazot,Jeremy deWaard,Stephanie deWaard,Arielle Farrell,Brendan Furneaux,Bess Hardwick,Nao Ito,Amlan Kar,Oula Kalttopää,Deirdre Kerdraon,Erik Kristensen,Jaclyn McKeown,Tommi Mononen,Ellen Nein,Hanna Rogers,Tomas Roslin,Paula Schmitz,Jayme Sones,Maija Sujala,Amy Thompson,Evgeny V. Zakharov,Iuliia Zarubiieva,Akshita Gupta,Scott C. Lowe,Graham W. Taylor*

Main category: cs.CV

TL;DR: 研究提出MassID45数据集，用于自动分类昆虫的混合样本，结合了分子数据、影像数据和人工标注。其目标是加速对昆虫多样性的理解，并推动生态学和机器学习领域的创新。


<details>
  <summary>Details</summary>
Motivation: 昆虫种类繁多，但许多种群面临由于环境和栖息地变化导致的严重下降，亟需高效的分类和多样性理解工具。

Method: 提出MassID45数据集，通过结合DNA条形码信息和高分辨图像，同时在人类标注和AI工具辅助下，对散装昆虫样本进行分割和分类操作，实现自动化分类。

Result: MassID45提供超过17000个实例的分类标签和分割蒙版，将DNA条形码的分类精度与图像数据的丰富信息结合，实现了对昆虫群落的快速表征。

Conclusion: 该数据集为微小目标检测和实例分割提供了新的挑战和创新空间，推动生态学研究与机器学习发展。

Abstract: Insects comprise millions of species, many experiencing severe population
declines under environmental and habitat changes. High-throughput approaches
are crucial for accelerating our understanding of insect diversity, with DNA
barcoding and high-resolution imaging showing strong potential for automatic
taxonomic classification. However, most image-based approaches rely on
individual specimen data, unlike the unsorted bulk samples collected in
large-scale ecological surveys. We present the Mixed Arthropod Sample
Segmentation and Identification (MassID45) dataset for training automatic
classifiers of bulk insect samples. It uniquely combines molecular and imaging
data at both the unsorted sample level and the full set of individual
specimens. Human annotators, supported by an AI-assisted tool, performed two
tasks on bulk images: creating segmentation masks around each individual
arthropod and assigning taxonomic labels to over 17 000 specimens. Combining
the taxonomic resolution of DNA barcodes with precise abundance estimates of
bulk images holds great potential for rapid, large-scale characterization of
insect communities. This dataset pushes the boundaries of tiny object detection
and instance segmentation, fostering innovation in both ecological and machine
learning research.

</details>


### [72] [Free on the Fly: Enhancing Flexibility in Test-Time Adaptation with Online EM](https://arxiv.org/abs/2507.06973)
*Qiyuan Dai,Sibei Yang*

Main category: cs.CV

TL;DR: 研究提出了一种名为FreeTTA的训练无关测试时间适应方法，通过在线EM算法改进了单样本预测，显著提升效果。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型(VLMs)在开放世界目标识别中表现优异，但由于数据分布的域偏移和变化，其实际应用效果受到限制。研究希望提出一种更灵活的测试时间适应（TTA）方法以解决该问题。

Method: 研究提出FreeTTA方法，无需训练和先验假设，通过将VLMs的零样本预测用作先验，结合在线EM算法迭代计算后验概率并更新参数，明确建模测试数据分布，并利用测试样本间内在关联提升预测。

Result: 实验结果表明，FreeTTA在跨域和分布外测试条件下，与当前最先进的多种方法相比，在15个数据集上均实现了稳定且显著的性能提升。

Conclusion: FreeTTA实现了无需历史数据或额外训练的灵活测试时间适应方法，填补了现有方法在建模测试数据分布上的空白，并展现了与现有方法相比的显著优势。

Abstract: Vision-Language Models (VLMs) have become prominent in open-world image
recognition for their strong generalization abilities. Yet, their effectiveness
in practical applications is compromised by domain shifts and distributional
changes, especially when test data distributions diverge from training data.
Therefore, the paradigm of test-time adaptation (TTA) has emerged, enabling the
use of online off-the-shelf data at test time, supporting independent sample
predictions, and eliminating reliance on test annotations. Traditional TTA
methods, however, often rely on costly training or optimization processes, or
make unrealistic assumptions about accessing or storing historical training and
test data. Instead, this study proposes FreeTTA, a training-free and
universally available method that makes no assumptions, to enhance the
flexibility of TTA. More importantly, FreeTTA is the first to explicitly model
the test data distribution, enabling the use of intrinsic relationships among
test samples to enhance predictions of individual samples without simultaneous
access--a direction not previously explored. FreeTTA achieves these advantages
by introducing an online EM algorithm that utilizes zero-shot predictions from
VLMs as priors to iteratively compute the posterior probabilities of each
online test sample and update parameters. Experiments demonstrate that FreeTTA
achieves stable and significant improvements compared to state-of-the-art
methods across 15 datasets in both cross-domain and out-of-distribution
settings.

</details>


### [73] [Design and Implementation of an OCR-Powered Pipeline for Table Extraction from Invoices](https://arxiv.org/abs/2507.07029)
*Parshva Dhilankumar Patel*

Main category: cs.CV

TL;DR: 本论文展示了一种基于OCR的流水线，用于从发票中高效提取表格信息。


<details>
  <summary>Details</summary>
Motivation: 旨在解决在嘈杂和非标准发票格式中提取结构化表格数据的挑战，并支持自动化财务工作流与数字存档。

Method: 利用Tesseract OCR进行文本识别，并辅以自定义后处理逻辑实现动态预处理、表格边界检测和行列映射的功能。

Result: 所设计的流水线显著提高了数据提取的准确性和一致性。

Conclusion: 提出的流水线在处理真实场景中展示了强大的实用性，特别是在自动化财务和存档应用中。

Abstract: This paper presents the design and development of an OCR-powered pipeline for
efficient table extraction from invoices. The system leverages Tesseract OCR
for text recognition and custom post-processing logic to detect, align, and
extract structured tabular data from scanned invoice documents. Our approach
includes dynamic preprocessing, table boundary detection, and row-column
mapping, optimized for noisy and non-standard invoice formats. The resulting
pipeline significantly improves data extraction accuracy and consistency,
supporting real-world use cases such as automated financial workflows and
digital archiving.

</details>


### [74] [DenoiseCP-Net: Efficient Collective Perception in Adverse Weather via Joint LiDAR-Based 3D Object Detection and Denoising](https://arxiv.org/abs/2507.06976)
*Sven Teufel,Dominique Mayer,Jörg Gamerdinger,Oliver Bringmann*

Main category: cs.CV

TL;DR: 自动驾驶汽车的感知系统易受恶劣天气影响。本文提出一种名为DenoiseCP-Net的新型多任务架构，改善LiDAR感知在恶劣天气下的性能，减少通信带宽需求和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆感知系统容易因恶劣天气和环境遮挡而性能下降，集体感知技术能通过信息共享缓解这一问题，但其在恶劣天气下的应用尚未被充分研究。

Method: 设计了DenoiseCP-Net，一个结合体素级噪声过滤与目标检测的多任务架构，集成为稀疏卷积主干网络，减少冗余计算、通信开销及推理延迟，并扩展了OPV2V数据集，模拟雨、雪、雾等现实天气条件。

Result: DenoiseCP-Net在恶劣天气中实现了接近完美的降噪精度，将带宽需求减少最高23.6%，且在保持相同检测精度的同时降低了协作车辆的推理延迟。

Conclusion: DenoiseCP-Net有效提升了自动驾驶车辆在恶劣天气下的感知性能，降低了通信开销和延迟，为集体感知技术的进一步应用提供了可能性。

Abstract: While automated vehicles hold the potential to significantly reduce traffic
accidents, their perception systems remain vulnerable to sensor degradation
caused by adverse weather and environmental occlusions. Collective perception,
which enables vehicles to share information, offers a promising approach to
overcoming these limitations. However, to this date collective perception in
adverse weather is mostly unstudied. Therefore, we conduct the first study of
LiDAR-based collective perception under diverse weather conditions and present
a novel multi-task architecture for LiDAR-based collective perception under
adverse weather. Adverse weather conditions can not only degrade perception
capabilities, but also negatively affect bandwidth requirements and latency due
to the introduced noise that is also transmitted and processed. Denoising prior
to communication can effectively mitigate these issues. Therefore, we propose
DenoiseCP-Net, a novel multi-task architecture for LiDAR-based collective
perception under adverse weather conditions. DenoiseCP-Net integrates
voxel-level noise filtering and object detection into a unified sparse
convolution backbone, eliminating redundant computations associated with
two-stage pipelines. This design not only reduces inference latency and
computational cost but also minimizes communication overhead by removing
non-informative noise. We extended the well-known OPV2V dataset by simulating
rain, snow, and fog using our realistic weather simulation models. We
demonstrate that DenoiseCP-Net achieves near-perfect denoising accuracy in
adverse weather, reduces the bandwidth requirements by up to 23.6% while
maintaining the same detection accuracy and reducing the inference latency for
cooperative vehicles.

</details>


### [75] [An AI Approach for Learning the Spectrum of the Laplace-Beltrami Operator](https://arxiv.org/abs/2507.07073)
*Yulin An,Enrique del Castillo*

Main category: cs.CV

TL;DR: 该论文提出了一种基于几何深度学习框架的方法，用于预测拉普拉斯-贝尔特拉米算子的谱，实现显著的计算节约同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 传统有限元法估算拉普拉斯-贝尔特拉米算子谱的计算复杂度较高，无法有效应对需要快速频繁决策的场景，如CAD零件数据库以及质量控制应用。本研究旨在提供一种高效的替代方案。

Method: 提出了一种基于图神经网络的新方法，利用CAD网格的几何特性（如高斯曲率、平均曲率、主曲率）来预测谱值，并进行了大规模公开数据集的训练和测试。

Result: 与传统线性有限元法相比，提出的方法使计算时间缩短了约5倍，同时保持了竞争力的精度。

Conclusion: 实验表明，拉普拉斯-贝尔特拉米算子谱是可学习的，所提方法显著提升了计算效率，为几何深度学习任务提供了新的解决方案。

Abstract: The spectrum of the Laplace-Beltrami (LB) operator is central in geometric
deep learning tasks, capturing intrinsic properties of the shape of the object
under consideration. The best established method for its estimation, from a
triangulated mesh of the object, is based on the Finite Element Method (FEM),
and computes the top k LB eigenvalues with a complexity of O(Nk), where N is
the number of points. This can render the FEM method inefficient when
repeatedly applied to databases of CAD mechanical parts, or in quality control
applications where part metrology is acquired as large meshes and decisions
about the quality of each part are needed quickly and frequently. As a solution
to this problem, we present a geometric deep learning framework to predict the
LB spectrum efficiently given the CAD mesh of a part, achieving significant
computational savings without sacrificing accuracy, demonstrating that the LB
spectrum is learnable. The proposed Graph Neural Network architecture uses a
rich set of part mesh features - including Gaussian curvature, mean curvature,
and principal curvatures. In addition to our trained network, we make
available, for repeatability, a large curated dataset of real-world mechanical
CAD models derived from the publicly available ABC dataset used for training
and testing. Experimental results show that our method reduces computation time
of the LB spectrum by approximately 5 times over linear FEM while delivering
competitive accuracy.

</details>


### [76] [GNN-ViTCap: GNN-Enhanced Multiple Instance Learning with Vision Transformers for Whole Slide Image Classification and Captioning](https://arxiv.org/abs/2507.07006)
*S M Taslim Uddin Raju,Md. Milon Islam,Md Rezwanul Haque,Hamdi Altaheri,Fakhri Karray*

Main category: cs.CV

TL;DR: 本文提出GNN-ViTCap框架，用于从显微镜病理图像中进行分类和生成描述。这一框架有效解决了冗余补丁和自动生成病理描述的挑战，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的Whole Slide Image (WSI) 分类与描述生成存在补丁冗余及路径主观性问题，同时自动病理描述生成难度较大。

Method: 利用GNN-ViTCap框架，提取补丁特征，动态聚类过滤冗余补丁，构建相似矩阵并通过图神经网络结合全局与局部信息。将图像嵌入投射至语言模型空间，结合描述令牌进行微调生成描述。

Result: 在BreakHis和PatchGastric数据集中，分类任务中F1得分0.934，AUC得分0.963；描述生成任务中BLEU-4得分0.811，METEOR得分0.569，均优于现有方法。

Conclusion: GNN-ViTCap框架提供了可靠高效的病理诊断分类与描述解决方案，证明其在显微图像分析中的卓越性能。

Abstract: Microscopic assessment of histopathology images is vital for accurate cancer
diagnosis and treatment. Whole Slide Image (WSI) classification and captioning
have become crucial tasks in computer-aided pathology. However, microscopic WSI
face challenges such as redundant patches and unknown patch positions due to
subjective pathologist captures. Moreover, generating automatic pathology
captions remains a significant challenge. To address these issues, we introduce
a novel GNN-ViTCap framework for classification and caption generation from
histopathological microscopic images. First, a visual feature extractor
generates patch embeddings. Redundant patches are then removed by dynamically
clustering these embeddings using deep embedded clustering and selecting
representative patches via a scalar dot attention mechanism. We build a graph
by connecting each node to its nearest neighbors in the similarity matrix and
apply a graph neural network to capture both local and global context. The
aggregated image embeddings are projected into the language model's input space
through a linear layer and combined with caption tokens to fine-tune a large
language model. We validate our method on the BreakHis and PatchGastric
datasets. GNN-ViTCap achieves an F1 score of 0.934 and an AUC of 0.963 for
classification, along with a BLEU-4 score of 0.811 and a METEOR score of 0.569
for captioning. Experimental results demonstrate that GNN-ViTCap outperforms
state of the art approaches, offering a reliable and efficient solution for
microscopy based patient diagnosis.

</details>


### [77] [Integrating Pathology Foundation Models and Spatial Transcriptomics for Cellular Decomposition from Histology Images](https://arxiv.org/abs/2507.07013)
*Yutong Sun,Sichen Zhu,Peng Qiu*

Main category: cs.CV

TL;DR: 本研究提出一种轻量化且训练高效的方法，通过利用预训练病理基础模型中提取的信息丰富的特征嵌入，从HE染色的组织学图像中直接预测细胞组成。


<details>
  <summary>Details</summary>
Motivation: 数字病理学和深度学习迅速发展，病理基础模型有望解决各种疾病条件下的一般病理问题；同时空间转录组学为深入研究HE组织学图像提供了前所未有的机会。

Method: 利用预训练的病理基础模型提取信息丰富的特征嵌入，并训练一个轻量化多层感知机回归器（MLP），预测细胞类型组成。

Result: 该方法能够以较低的计算复杂性准确预测组织图像中的细胞类型组成，与现有方法如Hist2Cell相比表现出竞争力。

Conclusion: 本方法通过有效提取和利用基础病理模型中的知识，无需实际进行昂贵的空间转录组学操作，即可实现对细胞类型组成的预测，具有较高的应用潜力。

Abstract: The rapid development of digital pathology and modern deep learning has
facilitated the emergence of pathology foundation models that are expected to
solve general pathology problems under various disease conditions in one
unified model, with or without fine-tuning. In parallel, spatial
transcriptomics has emerged as a transformative technology that enables the
profiling of gene expression on hematoxylin and eosin (H&E) stained histology
images. Spatial transcriptomics unlocks the unprecedented opportunity to dive
into existing histology images at a more granular, cellular level. In this
work, we propose a lightweight and training-efficient approach to predict
cellular composition directly from H&E-stained histology images by leveraging
information-enriched feature embeddings extracted from pre-trained pathology
foundation models. By training a lightweight multi-layer perceptron (MLP)
regressor on cell-type abundances derived via cell2location, our method
efficiently distills knowledge from pathology foundation models and
demonstrates the ability to accurately predict cell-type compositions from
histology images, without physically performing the costly spatial
transcriptomics. Our method demonstrates competitive performance compared to
existing methods such as Hist2Cell, while significantly reducing computational
complexity.

</details>


### [78] [MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge Distillation](https://arxiv.org/abs/2507.07015)
*Hui Li,Pengfei Yang,Juanyang Chen,Le Dong,Yanxin Chen,Quan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为MST-Distill的新型跨模态知识蒸馏框架，利用专门教师模型的混合体系改进跨模态知识转移，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法在跨模态环境中因数据和统计异质性问题，难以利用跨模态教师模型中的互补性先验知识。

Method: 提出MST-Distill框架，结合跨模态和多模态的多样化教师模型，利用动态实例级路由网络进行自适应蒸馏，并加入独立训练的掩码模块减少模态差异。

Result: 在五个多模态数据集（视觉、音频和文本）上的实验表明，MST-Distill显著优于现有跨模态知识蒸馏方法。

Conclusion: MST-Distill有效解决了蒸馏路径选择和知识漂移问题，提高了跨模态知识转移的效果，具备实际推广性。

Abstract: Knowledge distillation as an efficient knowledge transfer technique, has
achieved remarkable success in unimodal scenarios. However, in cross-modal
settings, conventional distillation methods encounter significant challenges
due to data and statistical heterogeneities, failing to leverage the
complementary prior knowledge embedded in cross-modal teacher models. This
paper empirically reveals two critical issues in existing approaches:
distillation path selection and knowledge drift. To address these limitations,
we propose MST-Distill, a novel cross-modal knowledge distillation framework
featuring a mixture of specialized teachers. Our approach employs a diverse
ensemble of teacher models across both cross-modal and multimodal
configurations, integrated with an instance-level routing network that
facilitates adaptive and dynamic distillation. This architecture effectively
transcends the constraints of traditional methods that rely on monotonous and
static teacher models. Additionally, we introduce a plug-in masking module,
independently trained to suppress modality-specific discrepancies and
reconstruct teacher representations, thereby mitigating knowledge drift and
enhancing transfer effectiveness. Extensive experiments across five diverse
multimodal datasets, spanning visual, audio, and text, demonstrate that our
method significantly outperforms existing state-of-the-art knowledge
distillation methods in cross-modal distillation tasks. The source code is
available at https://github.com/Gray-OREO/MST-Distill.

</details>


### [79] [Evaluating Large Multimodal Models for Nutrition Analysis: A Benchmark Enriched with Contextual Metadata](https://arxiv.org/abs/2507.07048)
*Bruce Coburn,Jiangpeng He,Megan E. Rollo,Satvinder S. Dhaliwal,Deborah A. Kerr,Fengqing Zhu*

Main category: cs.CV

TL;DR: 提出了一种利用上下文元数据增强大型多模态模型(LMMs)在营养分析领域表现的方法，并发布了一个公开的营养分析数据集ACETADA。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要评估专有模型，对开源LLMs的探索较少。同时，上下文元数据与推理方法交互对营养分析的影响尚不明确。作者希望通过研究这一领域填补空白。

Method: 提出通过将GPS坐标（转换为位置/场所类型）、时间戳（转换为餐/日类型）以及食品项上下文元数据集成到LMMs中，结合直观提示策略和推理修饰符方法（如Chain-of-Thought等），提升营养值估计准确性。

Result: 结合上下文元数据的LMMs显著降低了预测营养值的平均绝对误差（MAE）和平均百分比误差（MAPE），展示上下文信息对模型性能的提升作用。

Conclusion: 上下文感知的多模态模型在改进营养分析方面有巨大的潜力。这一研究不仅验证了元数据整合对模型表现的提升，还公开了新的食品图像数据集ACETADA，供未来研究使用。

Abstract: Large Multimodal Models (LMMs) are increasingly applied to meal images for
nutrition analysis. However, existing work primarily evaluates proprietary
models, such as GPT-4. This leaves the broad range of LLMs underexplored.
Additionally, the influence of integrating contextual metadata and its
interaction with various reasoning modifiers remains largely uncharted. This
work investigates how interpreting contextual metadata derived from GPS
coordinates (converted to location/venue type), timestamps (transformed into
meal/day type), and the food items present can enhance LMM performance in
estimating key nutritional values. These values include calories,
macronutrients (protein, carbohydrates, fat), and portion sizes. We also
introduce ACETADA, a new food-image dataset slated for public release. This
open dataset provides nutrition information verified by the dietitian and
serves as the foundation for our analysis. Our evaluation across eight LMMs
(four open-weight and four closed-weight) first establishes the benefit of
contextual metadata integration over straightforward prompting with images
alone. We then demonstrate how this incorporation of contextual information
enhances the efficacy of reasoning modifiers, such as Chain-of-Thought,
Multimodal Chain-of-Thought, Scale Hint, Few-Shot, and Expert Persona.
Empirical results show that integrating metadata intelligently, when applied
through straightforward prompting strategies, can significantly reduce the Mean
Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) in predicted
nutritional values. This work highlights the potential of context-aware LMMs
for improved nutrition analysis.

</details>


### [80] [Reading a Ruler in the Wild](https://arxiv.org/abs/2507.07077)
*Yimu Pan,Manas Mehta,Gwen Sincerbeaux,Jeffery A. Goldstein,Alison D. Gernand,James Z. Wang*

Main category: cs.CV

TL;DR: RulerNet 是一个能够在复杂环境下鲁棒估计比例尺度的深度学习框架，通过将尺子阅读问题转化为统一的关键点检测问题，并用几何级数参数表示尺子，达到了精确且高效的测量结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在复杂环境中准确将像素转化为真实尺寸，影响了关键领域如生物医学、法医、营养分析和电商的发展。

Method: 提出了RulerNet框架，利用几何级数参数表示尺子，并结合生成合成数据的图形生成和ControlNet技术，训练轻量化网络DeepGP以增强网络鲁棒性，同时能在移动设备进行实时尺度估计。

Result: RulerNet 在挑战性条件下实现了准确、一致且高效的尺度估计，证明其为一款通用的测量工具，并展示了其在高影响领域与其他视觉组件集成的潜力。

Conclusion: RulerNet 具备广泛的适用性和良好的性能，能满足多种实际应用需求，具有进一步推广和应用价值。

Abstract: Accurately converting pixel measurements into absolute real-world dimensions
remains a fundamental challenge in computer vision and limits progress in key
applications such as biomedicine, forensics, nutritional analysis, and
e-commerce. We introduce RulerNet, a deep learning framework that robustly
infers scale "in the wild" by reformulating ruler reading as a unified
keypoint-detection problem and by representing the ruler with
geometric-progression parameters that are invariant to perspective
transformations. Unlike traditional methods that rely on handcrafted thresholds
or rigid, ruler-specific pipelines, RulerNet directly localizes centimeter
marks using a distortion-invariant annotation and training strategy, enabling
strong generalization across diverse ruler types and imaging conditions while
mitigating data scarcity. We also present a scalable synthetic-data pipeline
that combines graphics-based ruler generation with ControlNet to add
photorealistic context, greatly increasing training diversity and improving
performance. To further enhance robustness and efficiency, we propose DeepGP, a
lightweight feed-forward network that regresses geometric-progression
parameters from noisy marks and eliminates iterative optimization, enabling
real-time scale estimation on mobile or edge devices. Experiments show that
RulerNet delivers accurate, consistent, and efficient scale estimates under
challenging real-world conditions. These results underscore its utility as a
generalizable measurement tool and its potential for integration with other
vision components for automated, scale-aware analysis in high-impact domains. A
live demo is available at https://huggingface.co/spaces/ymp5078/RulerNet-Demo.

</details>


### [81] [Evaluating Attribute Confusion in Fashion Text-to-Image Generation](https://arxiv.org/abs/2507.07079)
*Ziyue Liu,Federico Girella,Yiming Wang,Davide Talon*

Main category: cs.CV

TL;DR: 提出了一种名为L-VQAScore的自动化评价指标，用于更精确地评估文本到图像生成模型中的实体-属性语义对齐问题，尤其是在复杂的时尚领域。


<details>
  <summary>Details</summary>
Motivation: 当前T2I生成模型在复杂的组合生成领域（如时尚）无法完全解决实体-属性多样语义评估的问题，现有基于预训练视觉-语言模型的方法在处理属性混淆时存在局限性。

Method: 提出了基于视觉问答（VQA）的定位策略，通过专注于单一实体的评估，结合视觉定位与VQA探测，设计了一种新的自动化评价指标L-VQAScore，以及一个对应的人类评估协议。

Result: 在一个新建立的数据集上，L-VQAScore在准确反映复杂实体-属性对应关系上优于现有T2I评估方法，并与人类评价结果更为一致。

Conclusion: L-VQAScore为现有主观评价提供了一种可靠且可扩展的替代方法，填补了现有评估方法在精细语义层面的不足。

Abstract: Despite the rapid advances in Text-to-Image (T2I) generation models, their
evaluation remains challenging in domains like fashion, involving complex
compositional generation. Recent automated T2I evaluation methods leverage
pre-trained vision-language models to measure cross-modal alignment. However,
our preliminary study reveals that they are still limited in assessing rich
entity-attribute semantics, facing challenges in attribute confusion, i.e.,
when attributes are correctly depicted but associated to the wrong entities. To
address this, we build on a Visual Question Answering (VQA) localization
strategy targeting one single entity at a time across both visual and textual
modalities. We propose a localized human evaluation protocol and introduce a
novel automatic metric, Localized VQAScore (L-VQAScore), that combines visual
localization with VQA probing both correct (reflection) and miss-localized
(leakage) attribute generation. On a newly curated dataset featuring
challenging compositional alignment scenarios, L-VQAScore outperforms
state-of-the-art T2I evaluation methods in terms of correlation with human
judgments, demonstrating its strength in capturing fine-grained
entity-attribute associations. We believe L-VQAScore can be a reliable and
scalable alternative to subjective evaluations.

</details>


### [82] [Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data](https://arxiv.org/abs/2507.07095)
*Ke Fan,Shunlin Lu,Minyue Dai,Runyi Yu,Lixing Xiao,Zhiyang Dou,Junting Dong,Lizhuang Ma,Jingbo Wang*

Main category: cs.CV

TL;DR: 本研究提出MotionMillion数据集和评测框架，提升文本生成动作任务的零样本泛化能力，实现高质量动作序列生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在零样本泛化方面表现有限，主要源于训练数据集规模不足且缺乏全面评估框架来指引改进方向。

Method: 开发高效的标注流程，构建包含2000小时、200万高质量动作序列的MotionMillion数据集，并提出全面的MotionMillion-Eval评估工具，同时利用可扩展架构构建了具有70亿参数的模型。

Result: 验证表明，该模型在MotionMillion-Eval上的表现良好，能够广泛推广至领域外和复杂组合动作。

Conclusion: 本研究显著提升了零样本动作生成能力，为该领域的研究与应用奠定了新基础。

Abstract: Generating diverse and natural human motion sequences based on textual
descriptions constitutes a fundamental and challenging research area within the
domains of computer vision, graphics, and robotics. Despite significant
advancements in this field, current methodologies often face challenges
regarding zero-shot generalization capabilities, largely attributable to the
limited size of training datasets. Moreover, the lack of a comprehensive
evaluation framework impedes the advancement of this task by failing to
identify directions for improvement. In this work, we aim to push
text-to-motion into a new era, that is, to achieve the generalization ability
of zero-shot. To this end, firstly, we develop an efficient annotation pipeline
and introduce MotionMillion-the largest human motion dataset to date, featuring
over 2,000 hours and 2 million high-quality motion sequences. Additionally, we
propose MotionMillion-Eval, the most comprehensive benchmark for evaluating
zero-shot motion generation. Leveraging a scalable architecture, we scale our
model to 7B parameters and validate its performance on MotionMillion-Eval. Our
results demonstrate strong generalization to out-of-domain and complex
compositional motions, marking a significant step toward zero-shot human motion
generation. The code is available at
https://github.com/VankouF/MotionMillion-Codes.

</details>


### [83] [Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation from Diffusion Models](https://arxiv.org/abs/2507.07104)
*Tiezheng Zhang,Yitong Li,Yu-cheng Chou,Jieneng Chen,Alan Yuille,Chen Wei,Junfei Xiao*

Main category: cs.CV

TL;DR: 提出了一种名为Vision-Language-Vision (VLV)的自动编码器框架，用于构建具有强大图文生成能力的模型，大幅降低了训练成本和数据需求。


<details>
  <summary>Details</summary>
Motivation: 现有的图文生成模型训练通常需要海量的图文配对数据和极高的计算成本，因此需要一种更高效的训练方法。

Method: 该方法利用了预训练的视觉编码器、文本-图像扩散模型的解码器以及大型语言模型（LLM），通过冻结扩散模型解码器以建立信息瓶颈，并微调LLM以生成详细的图文描述。

Result: 实验结果表明，该方法不仅能够通过单模态图像实现高质量的语义生成，还能生成更好的图文描述，与最先进的模型（如GPT-4o和Gemini 2.0 Flash）具有可比性。

Conclusion: VLV框架在成本效率和数据利用率方面表现极佳，通过充分利用单模态图像和现有的预训练模型，实现了在不到1000美元的预算内构建出新的卓越图文生成模型。

Abstract: Building state-of-the-art Vision-Language Models (VLMs) with strong
captioning capabilities typically necessitates training on billions of
high-quality image-text pairs, requiring millions of GPU hours. This paper
introduces the Vision-Language-Vision (VLV) auto-encoder framework, which
strategically leverages key pretrained components: a vision encoder, the
decoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large
Language Model (LLM). Specifically, we establish an information bottleneck by
regularizing the language representation space, achieved through freezing the
pretrained T2I diffusion decoder. Our VLV pipeline effectively distills
knowledge from the text-conditioned diffusion model using continuous
embeddings, demonstrating comprehensive semantic understanding via high-quality
reconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the
intermediate language representations into detailed descriptions, we construct
a state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o
and Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and
significantly reduces data requirements; by primarily utilizing single-modal
images for training and maximizing the utility of existing pretrained models
(image encoder, T2I diffusion model, and LLM), it circumvents the need for
massive paired image-text datasets, keeping the total training expenditure
under $1,000 USD.

</details>


### [84] [4KAgent: Agentic Any Image to 4K Super-Resolution](https://arxiv.org/abs/2507.07105)
*Yushen Zuo,Qi Zheng,Mingyang Wu,Xinrui Jiang,Renjie Li,Jian Wang,Yide Zhang,Gengchen Mai,Lihong V. Wang,James Zou,Xiaoyu Wang,Ming-Hsuan Yang,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 4KAgent是一个统一的超分辨率系统，可以将低分辨率甚至极度退化的图像转化为清晰的4K图像，同时支持多种成像领域任务，并表现出卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图像超分辨率方法在处理极低分辨率图像以及多领域任务时仍存在局限性，迫切需要一种通用且高性能的解决方案。

Method: 4KAgent由三部分组成：1) 定制化分析模块Profiling；2) 感知Agent，通过视觉-语言模型和图像质量评估专家制定恢复计划；3) 恢复Agent，利用专家混合策略递归优化图像质量。此外，专门的面部恢复管道提升了人像细节。

Result: 在包括自然图像、医学影像等11类任务和26个基准测试上，4KAgent达到了最先进的性能，在感知及保真度等多个指标上表现出色。

Conclusion: 4KAgent通过创新的Agentic范式，实现了多领域超分辨率任务的统一解决，推动了视觉中心自治Agent领域的研究发展。

Abstract: We present 4KAgent, a unified agentic super-resolution generalist system
designed to universally upscale any image to 4K resolution (and even higher, if
applied iteratively). Our system can transform images from extremely low
resolutions with severe degradations, for example, highly distorted inputs at
256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three
core components: (1) Profiling, a module that customizes the 4KAgent pipeline
based on bespoke use cases; (2) A Perception Agent, which leverages
vision-language models alongside image quality assessment experts to analyze
the input image and make a tailored restoration plan; and (3) A Restoration
Agent, which executes the plan, following a recursive execution-reflection
paradigm, guided by a quality-driven mixture-of-expert policy to select the
optimal output for each step. Additionally, 4KAgent embeds a specialized face
restoration pipeline, significantly enhancing facial details in portrait and
selfie photos. We rigorously evaluate our 4KAgent across 11 distinct task
categories encompassing a total of 26 diverse benchmarks, setting new
state-of-the-art on a broad spectrum of imaging domains. Our evaluations cover
natural images, portrait photos, AI-generated content, satellite imagery,
fluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and
X-ray, demonstrating superior performance in terms of both perceptual (e.g.,
NIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic
paradigm for low-level vision tasks, we aim to catalyze broader interest and
innovation within vision-centric autonomous agents across diverse research
communities. We will release all the code, models, and results at:
https://4kagent.github.io.

</details>


### [85] [Towards Multimodal Understanding via Stable Diffusion as a Task-Aware Feature Extractor](https://arxiv.org/abs/2507.07106)
*Vatsal Agarwal,Matthew Gwilliam,Gefen Kohavi,Eshan Verma,Daniel Ulbricht,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 该论文探讨预训练的文本到图像扩散模型是否可以作为具有指令感知能力的视觉编码器，研究其内部表示及与CLIP的融合策略，并评估在多种多模态任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大语言模型的图片问答功能受CLIP视觉编码器局限，无法处理细节信息，研究旨在探讨如何通过扩散模型改进。

Method: 分析扩散模型的特征表示与文本对齐性，发现可以通过条件文本聚焦相关区域；提出一种将CLIP与扩散特征结合的简单融合策略。

Result: 在通用VQA和专用多模态基准测试上评估了方法的有效性，扩散模型在需要空间与组合推理的视觉任务中表现出潜力。

Conclusion: 研究表明，扩散模型的特性能够弥补现有方法的不足，为多模态任务的视觉理解提供新的可能性。

Abstract: Recent advances in multimodal large language models (MLLMs) have enabled
image-based question-answering capabilities. However, a key limitation is the
use of CLIP as the visual encoder; while it can capture coarse global
information, it often can miss fine-grained details that are relevant to the
input query. To address these shortcomings, this work studies whether
pre-trained text-to-image diffusion models can serve as instruction-aware
visual encoders. Through an analysis of their internal representations, we find
diffusion features are both rich in semantics and can encode strong image-text
alignment. Moreover, we find that we can leverage text conditioning to focus
the model on regions relevant to the input question. We then investigate how to
align these features with large language models and uncover a leakage
phenomenon, where the LLM can inadvertently recover information from the
original diffusion prompt. We analyze the causes of this leakage and propose a
mitigation strategy. Based on these insights, we explore a simple fusion
strategy that utilizes both CLIP and conditional diffusion features. We
evaluate our approach on both general VQA and specialized MLLM benchmarks,
demonstrating the promise of diffusion models for visual understanding,
particularly in vision-centric tasks that require spatial and compositional
reasoning. Our project page can be found
https://vatsalag99.github.io/mustafar/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [86] [Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities](https://arxiv.org/abs/2507.06261)
*Gheorghe Comanici,Eric Bieber,Mike Schaekermann,Ice Pasupat,Noveen Sachdeva,Inderjit Dhillon,Marcel Blistein,Ori Ram,Dan Zhang,Evan Rosen,Luke Marris,Sam Petulla,Colin Gaffney,Asaf Aharoni,Nathan Lintz,Tiago Cardal Pais,Henrik Jacobsson,Idan Szpektor,Nan-Jiang Jiang,Krishna Haridasan,Ahmed Omran,Nikunj Saunshi,Dara Bahri,Gaurav Mishra,Eric Chu,Toby Boyd,Brad Hekman,Aaron Parisi,Chaoyi Zhang,Kornraphop Kawintiranon,Tania Bedrax-Weiss,Oliver Wang,Ya Xu,Ollie Purkiss,Uri Mendlovic,Ilaï Deutel,Nam Nguyen,Adam Langley,Flip Korn,Lucia Rossazza,Alexandre Ramé,Sagar Waghmare,Helen Miller,Vaishakh Keshava,Ying Jian,Xiaofan Zhang,Raluca Ada Popa,Kedar Dhamdhere,Blaž Bratanič,Kyuyeun Kim,Terry Koo,Ferran Alet,Yi-ting Chen,Arsha Nagrani,Hannah Muckenhirn,Zhiyuan Zhang,Corbin Quick,Filip Pavetić,Duc Dung Nguyen,Joao Carreira,Michael Elabd,Haroon Qureshi,Fabian Mentzer,Yao-Yuan Yang,Danielle Eisenbud,Anmol Gulati,Ellie Talius,Eric Ni,Sahra Ghalebikesabi,Edouard Yvinec,Alaa Saade,Thatcher Ulrich,Lorenzo Blanco,Dan A. Calian,Muhuan Huang,Aäron van den Oord,Naman Goyal,Terry Chen,Praynaa Rawlani,Christian Schallhart,Swachhand Lokhande,Xianghong Luo,Jyn Shan,Ceslee Montgomery,Victoria Krakovna,Federico Piccinini,Omer Barak,Jingyu Cui,Yiling Jia,Mikhail Dektiarev,Alexey Kolganov,Shiyu Huang,Zhe Chen,Xingyu Wang,Jessica Austin,Peter de Boursac,Evgeny Sluzhaev,Frank Ding,Huijian Li,Surya Bhupatiraju,Mohit Agarwal,Sławek Kwasiborski,Paramjit Sandhu,Patrick Siegler,Ahmet Iscen,Eyal Ben-David,Shiraz Butt,Miltos Allamanis,Seth Benjamin,Robert Busa-Fekete,Felix Hernandez-Campos,Sasha Goldshtein,Matt Dibb,Weiyang Zhang,Annie Marsden,Carey Radebaugh,Stephen Roller,Abhishek Nayyar,Jacob Austin,Tayfun Terzi,Bhargav Kanagal Shamanna,Pete Shaw,Aayush Singh,Florian Luisier,Artur Mendonça,Vaibhav Aggarwal,Larisa Markeeva,Claudio Fantacci,Sergey Brin,HyunJeong Choe,Guanyu Wang,Hartwig Adam,Avigail Dabush,Tatsuya Kiyono,Eyal Marcus,Jeremy Cole,Theophane Weber,Hongrae Lee,Ronny Huang,Alex Muzio,Leandro Kieliger,Maigo Le,Courtney Biles,Long Le,Archit Sharma,Chengrun Yang,Avery Lamp,Dave Dopson,Nate Hurley,Katrina,Xu,Zhihao Shan,Shuang Song,Jiewen Tan,Alexandre Senges,George Zhang,Chong You,Yennie Jun,David Raposo,Susanna Ricco,Xuan Yang,Weijie Chen,Prakhar Gupta,Arthur Szlam,Kevin Villela,Chun-Sung Ferng,Daniel Kasenberg,Chen Liang,Rui Zhu,Arunachalam Narayanaswamy,Florence Perot,Paul Pucciarelli,Anna Shekhawat,Alexey Stern,Rishikesh Ingale,Stefani Karp,Sanaz Bahargam,Adrian Goedeckemeyer,Jie Han,Sicheng Li,Andrea Tacchetti,Dian Yu,Abhishek Chakladar,Zhiying Zhang,Mona El Mahdy,Xu Gao,Dale Johnson,Samrat Phatale,AJ Piergiovanni,Hyeontaek Lim,Clement Farabet,Carl Lebsack,Theo Guidroz,John Blitzer,Nico Duduta,David Madras,Steve Li,Daniel von Dincklage,Xin Li,Mahdis Mahdieh,George Tucker,Ganesh Jawahar,Owen Xiao,Danny Tarlow,Robert Geirhos,Noam Velan,Daniel Vlasic,Kalesha Bullard,SK Park,Nishesh Gupta,Kellie Webster,Ayal Hitron,Jieming Mao,Julian Eisenschlos,Laurel Prince,Nina D'Souza,Kelvin Zheng,Sara Nasso,Gabriela Botea,Carl Doersch,Caglar Unlu,Chris Alberti,Alexey Svyatkovskiy,Ankita Goel,Krzysztof Choromanski,Pan-Pan Jiang,Richard Nguyen,Four Flynn,Daria Ćurko,Peter Chen,Nicholas Roth,Kieran Milan,Caleb Habtegebriel,Shashi Narayan,Michael Moffitt,Jake Marcus,Thomas Anthony,Brendan McMahan,Gowoon Cheon,Ruibo Liu,Megan Barnes,Lukasz Lew,Rebeca Santamaria-Fernandez,Mayank Upadhyay,Arjun Akula,Arnar Mar Hrafnkelsson,Alvaro Caceres,Andrew Bunner,Michal Sokolik,Subha Puttagunta,Lawrence Moore,Berivan Isik,Weilun Chen,Jay Hartford,Lawrence Chan,Pradeep Shenoy,Dan Holtmann-Rice,Jane Park,Fabio Viola,Alex Salcianu,Sujeevan Rajayogam,Ian Stewart-Binks,Zelin Wu,Richard Everett,Xi Xiong,Pierre-Antoine Manzagol,Gary Leung,Carl Saroufim,Bo Pang,Dawid Wegner,George Papamakarios,Jennimaria Palomaki,Helena Pankov,Guangda Lai,Guilherme Tubone,Shubin Zhao,Theofilos Strinopoulos,Seth Neel,Mingqiu Wang,Joe Kelley,Li Li,Pingmei Xu,Anitha Vijayakumar,Andrea D'olimpio,Omer Levy,Massimo Nicosia,Grigory Rozhdestvenskiy,Ni Lao,Sirui Xie,Yash Katariya,Jon Simon,Sanjiv Kumar,Florian Hartmann,Michael Kilgore,Jinhyuk Lee,Aroma Mahendru,Roman Ring,Tom Hennigan,Fiona Lang,Colin Cherry,David Steiner,Dawsen Hwang,Ray Smith,Pidong Wang,Jeremy Chen,Ming-Hsuan Yang,Sam Kwei,Philippe Schlattner,Donnie Kim,Ganesh Poomal Girirajan,Nikola Momchev,Ayushi Agarwal,Xingyi Zhou,Ilkin Safarli,Zachary Garrett,AJ Pierigiovanni,Sarthak Jauhari,Alif Raditya Rochman,Shikhar Vashishth,Quan Yuan,Christof Angermueller,Jon Blanton,Xinying Song,Nitesh Bharadwaj Gundavarapu,Thi Avrahami,Maxine Deines,Subhrajit Roy,Manish Gupta,Christopher Semturs,Shobha Vasudevan,Aditya Srikanth Veerubhotla,Shriya Sharma,Josh Jacob,Zhen Yang,Andreas Terzis,Dan Karliner,Auriel Wright,Tania Rojas-Esponda,Ashley Brown,Abhijit Guha Roy,Pawan Dogra,Andrei Kapishnikov,Peter Young,Wendy Kan,Vinodh Kumar Rajendran,Maria Ivanova,Salil Deshmukh,Chia-Hua Ho,Mike Kwong,Stav Ginzburg,Annie Louis,KP Sawhney,Slav Petrov,Jing Xie,Yunfei Bai,Georgi Stoyanov,Alex Fabrikant,Rajesh Jayaram,Yuqi Li,Joe Heyward,Justin Gilmer,Yaqing Wang,Radu Soricut,Luyang Liu,Qingnan Duan,Jamie Hayes,Maura O'Brien,Gaurav Singh Tomar,Sivan Eiger,Bahar Fatemi,Jeffrey Hui,Catarina Barros,Adaeze Chukwuka,Alena Butryna,Saksham Thakur,Austin Huang,Zhufeng Pan,Haotian Tang,Serkan Cabi,Tulsee Doshi,Michiel Bakker,Sumit Bagri,Ruy Ley-Wild,Adam Lelkes,Jennie Lees,Patrick Kane,David Greene,Shimu Wu,Jörg Bornschein,Gabriela Surita,Sarah Hodkinson,Fangtao Li,Chris Hidey,Sébastien Pereira,Sean Ammirati,Phillip Lippe,Adam Kraft,Pu Han,Sebastian Gerlach,Zifeng Wang,Liviu Panait,Feng Han,Brian Farris,Yingying Bi,Hannah DeBalsi,Miaosen Wang,Gladys Tyen,James Cohan,Susan Zhang,Jarred Barber,Da-Woon Chung,Jaeyoun Kim,Markus Kunesch,Steven Pecht,Nami Akazawa,Abe Friesen,James Lyon,Ali Eslami,Junru Wu,Jie Tan,Yue Song,Ravi Kumar,Chris Welty,Ilia Akolzin,Gena Gibson,Sean Augenstein,Arjun Pillai,Nancy Yuen,Du Phan,Xin Wang,Iain Barr,Heiga Zen,Nan Hua,Casper Liu,Jilei,Wang,Tanuj Bhatia,Hao Xu,Oded Elyada,Pushmeet Kohli,Mirek Olšák,Ke Chen,Azalia Mirhoseini,Noam Shazeer,Shoshana Jakobovits,Maggie Tran,Nolan Ramsden,Tarun Bharti,Fred Alcober,Yunjie Li,Shilpa Shetty,Jing Chen,Dmitry Kalashnikov,Megha Nawhal,Sercan Arik,Hanwen Chen,Michiel Blokzijl,Shubham Gupta,James Rubin,Rigel Swavely,Sophie Bridgers,Ian Gemp,Chen Su,Arun Suggala,Juliette Pluto,Mary Cassin,Alain Vaucher,Kaiyang Ji,Jiahao Cai,Andrew Audibert,Animesh Sinha,David Tian,Efrat Farkash,Amy Hua,Jilin Chen,Duc-Hieu Tran,Edward Loper,Nicole Brichtova,Lara McConnaughey,Ballie Sandhu,Robert Leland,Doug DeCarlo,Andrew Over,James Huang,Xing Wu,Connie Fan,Eric Li,Yun Lei,Deepak Sharma,Cosmin Paduraru,Luo Yu,Matko Bošnjak,Phuong Dao,Min Choi,Sneha Kudugunta,Jakub Adamek,Carlos Guía,Ali Khodaei,Jie Feng,Wenjun Zeng,David Welling,Sandeep Tata,Christina Butterfield,Andrey Vlasov,Seliem El-Sayed,Swaroop Mishra,Tara Sainath,Shentao Yang,RJ Skerry-Ryan,Jeremy Shar,Robert Berry,Arunkumar Rajendran,Arun Kandoor,Andrea Burns,Deepali Jain,Tom Stone,Wonpyo Park,Shibo Wang,Albin Cassirer,Guohui Wang,Hayato Kobayashi,Sergey Rogulenko,Vineetha Govindaraj,Mikołaj Rybiński,Nadav Olmert,Colin Evans,Po-Sen Huang,Kelvin Xu,Premal Shah,Terry Thurk,Caitlin Sikora,Mu Cai,Jin Xie,Elahe Dabir,Saloni Shah,Norbert Kalb,Carrie Zhang,Shruthi Prabhakara,Amit Sabne,Artiom Myaskovsky,Vikas Raunak,Blanca Huergo,Behnam Neyshabur,Jon Clark,Ye Zhang,Shankar Krishnan,Eden Cohen,Dinesh Tewari,James Lottes,Yumeya Yamamori,Hui,Li,Mohamed Elhawaty,Ada Maksutaj Oflazer,Adrià Recasens,Sheryl Luo,Duy Nguyen,Taylor Bos,Kalyan Andra,Ana Salazar,Ed Chi,Jeongwoo Ko,Matt Ginsberg,Anders Andreassen,Anian Ruoss,Todor Davchev,Elnaz Davoodi,Chenxi Liu,Min Kim,Santiago Ontanon,Chi Ming To,Dawei Jia,Rosemary Ke,Jing Wang,Anna Korsun,Moran Ambar,Ilya Kornakov,Irene Giannoumis,Toni Creswell,Denny Zhou,Yi Su,Ishaan Watts,Aleksandr Zaks,Evgenii Eltyshev,Ziqiang Feng,Sidharth Mudgal,Alex Kaskasoli,Juliette Love,Kingshuk Dasgupta,Sam Shleifer,Richard Green,Sungyong Seo,Chansoo Lee,Dale Webster,Prakash Shroff,Ganna Raboshchuk,Isabel Leal,James Manyika,Sofia Erell,Daniel Murphy,Zhisheng Xiao,Anton Bulyenov,Julian Walker,Mark Collier,Matej Kastelic,Nelson George,Sushant Prakash,Sailesh Sidhwani,Alexey Frolov,Steven Hansen,Petko Georgiev,Tiberiu Sosea,Chris Apps,Aishwarya Kamath,David Reid,Emma Cooney,Charlotte Magister,Oriana Riva,Alec Go,Pu-Chin Chen,Sebastian Krause,Nir Levine,Marco Fornoni,Ilya Figotin,Nick Roy,Parsa Mahmoudieh,Vladimir Magay,Mukundan Madhavan,Jin Miao,Jianmo Ni,Yasuhisa Fujii,Ian Chou,George Scrivener,Zak Tsai,Siobhan Mcloughlin,Jeremy Selier,Sandra Lefdal,Jeffrey Zhao,Abhijit Karmarkar,Kushal Chauhan,Shivanker Goel,Zhaoyi Zhang,Vihan Jain,Parisa Haghani,Mostafa Dehghani,Jacob Scott,Erin Farnese,Anastasija Ilić,Steven Baker,Julia Pawar,Li Zhong,Josh Camp,Yoel Zeldes,Shravya Shetty,Anand Iyer,Vít Listík,Jiaxian Guo,Luming Tang,Mark Geller,Simon Bucher,Yifan Ding,Hongzhi Shi,Carrie Muir,Dominik Grewe,Ramy Eskander,Octavio Ponce,Boqing Gong,Derek Gasaway,Samira Khan,Umang Gupta,Angelos Filos,Weicheng Kuo,Klemen Kloboves,Jennifer Beattie,Christian Wright,Leon Li,Alicia Jin,Sandeep Mariserla,Miteyan Patel,Jens Heitkaemper,Dilip Krishnan,Vivek Sharma,David Bieber,Christian Frank,John Lambert,Paul Caron,Martin Polacek,Mai Giménez,Himadri Choudhury,Xing Yu,Sasan Tavakkol,Arun Ahuja,Franz Och,Rodolphe Jenatton,Wojtek Skut,Bryan Richter,David Gaddy,Andy Ly,Misha Bilenko,Megh Umekar,Ethan Liang,Martin Sevenich,Mandar Joshi,Hassan Mansoor,Rebecca Lin,Sumit Sanghai,Abhimanyu Singh,Xiaowei Li,Sudheendra Vijayanarasimhan,Zaheer Abbas,Yonatan Bitton,Hansa Srinivasan,Manish Reddy Vuyyuru,Alexander Frömmgen,Yanhua Sun,Ralph Leith,Alfonso Castaño,DJ Strouse,Le Yan,Austin Kyker,Satish Kambala,Mary Jasarevic,Thibault Sellam,Chao Jia,Alexander Pritzel,Raghavender R,Huizhong Chen,Natalie Clay,Sudeep Gandhe,Sean Kirmani,Sayna Ebrahimi,Hannah Kirkwood,Jonathan Mallinson,Chao Wang,Adnan Ozturel,Kuo Lin,Shyam Upadhyay,Vincent Cohen-Addad,Sean Purser-haskell,Yichong Xu,Ebrahim Songhori,Babi Seal,Alberto Magni,Almog Gueta,Tingting Zou,Guru Guruganesh,Thais Kagohara,Hung Nguyen,Khalid Salama,Alejandro Cruzado Ruiz,Justin Frye,Zhenkai Zhu,Matthias Lochbrunner,Simon Osindero,Wentao Yuan,Lisa Lee,Aman Prasad,Lam Nguyen Thiet,Daniele Calandriello,Victor Stone,Qixuan Feng,Han Ke,Maria Voitovich,Geta Sampemane,Lewis Chiang,Ling Wu,Alexander Bykovsky,Matt Young,Luke Vilnis,Ishita Dasgupta,Aditya Chawla,Qin Cao,Bowen Liang,Daniel Toyama,Szabolcs Payrits,Anca Stefanoiu,Dimitrios Vytiniotis,Ankesh Anand,Tianxiao Shen,Blagoj Mitrevski,Michael Tschannen,Sreenivas Gollapudi,Aishwarya P S,José Leal,Zhe Shen,Han Fu,Wei Wang,Arvind Kannan,Doron Kukliansky,Sergey Yaroshenko,Svetlana Grant,Umesh Telang,David Wood,Alexandra Chronopoulou,Alexandru Ţifrea,Tao Zhou,Tony,Nguy\~ên,Muge Ersoy,Anima Singh,Meiyan Xie,Emanuel Taropa,Woohyun Han,Eirikur Agustsson,Andrei Sozanschi,Hui Peng,Alex Chen,Yoel Drori,Efren Robles,Yang Gao,Xerxes Dotiwalla,Ying Chen,Anudhyan Boral,Alexei Bendebury,John Nham,Chris Tar,Luis Castro,Jiepu Jiang,Canoee Liu,Felix Halim,Jinoo Baek,Andy Wan,Jeremiah Liu,Yuan Cao,Shengyang Dai,Trilok Acharya,Ruoxi Sun,Fuzhao Xue,Saket Joshi,Morgane Lustman,Yongqin Xian,Rishabh Joshi,Deep Karkhanis,Nora Kassner,Jamie Hall,Xiangzhuo Ding,Gan Song,Gang Li,Chen Zhu,Yana Kulizhskaya,Bin Ni,Alexey Vlaskin,Solomon Demmessie,Lucio Dery,Salah Zaiem,Yanping Huang,Cindy Fan,Felix Gimeno,Ananth Balashankar,Koji Kojima,Hagai Taitelbaum,Maya Meng,Dero Gharibian,Sahil Singla,Wei Chen,Ambrose Slone,Guanjie Chen,Sujee Rajayogam,Max Schumacher,Suyog Kotecha,Rory Blevins,Qifei Wang,Mor Hazan Taege,Alex Morris,Xin Liu,Fayaz Jamil,Richard Zhang,Pratik Joshi,Ben Ingram,Tyler Liechty,Ahmed Eleryan,Scott Baird,Alex Grills,Gagan Bansal,Shan Han,Kiran Yalasangi,Shawn Xu,Majd Al Merey,Isabel Gao,Felix Weissenberger,Igor Karpov,Robert Riachi,Ankit Anand,Gautam Prasad,Kay Lamerigts,Reid Hayes,Jamie Rogers,Mandy Guo,Ashish Shenoy,Qiong,Hu,Kyle He,Yuchen Liu,Polina Zablotskaia,Sagar Gubbi,Yifan Chang,Jay Pavagadhi,Kristian Kjems,Archita Vadali,Diego Machado,Yeqing Li,Renshen Wang,Dipankar Ghosh,Aahil Mehta,Dana Alon,George Polovets,Alessio Tonioni,Nate Kushman,Joel D'sa,Lin Zhuo,Allen Wu,Rohin Shah,John Youssef,Jiayu Ye,Justin Snyder,Karel Lenc,Senaka Buthpitiya,Matthew Tung,Jichuan Chang,Tao Chen,David Saxton,Jenny Lee,Lydia Lihui Zhang,James Qin,Prabakar Radhakrishnan,Maxwell Chen,Piotr Ambroszczyk,Metin Toksoz-Exley,Yan Zhong,Nitzan Katz,Brendan O'Donoghue,Tamara von Glehn,Adi Gerzi Rosenthal,Aga Świetlik,Xiaokai Zhao,Nick Fernando,Jinliang Wei,Jieru Mei,Sergei Vassilvitskii,Diego Cedillo,Pranjal Awasthi,Hui Zheng,Koray Kavukcuoglu,Itay Laish,Joseph Pagadora,Marc Brockschmidt,Christopher A. Choquette-Choo,Arunkumar Byravan,Yifeng Lu,Xu Chen,Mia Chen,Kenton Lee,Rama Pasumarthi,Sijal Bhatnagar,Aditya Shah,Qiyin Wu,Zhuoyuan Chen,Zack Nado,Bartek Perz,Zixuan Jiang,David Kao,Ganesh Mallya,Nino Vieillard,Lantao Mei,Sertan Girgin,Mandy Jordan,Yeongil Ko,Alekh Agarwal,Yaxin Liu,Yasemin Altun,Raoul de Liedekerke,Anastasios Kementsietsidis,Daiyi Peng,Dangyi Liu,Utku Evci,Peter Humphreys,Austin Tarango,Xiang Deng,Yoad Lewenberg,Kevin Aydin,Chengda Wu,Bhavishya Mittal,Tsendsuren Munkhdalai,Kleopatra Chatziprimou,Rodrigo Benenson,Uri First,Xiao Ma,Jinning Li,Armand Joulin,Hamish Tomlinson,Tingnan Zhang,Milad Nasr,Zhi Hong,Michaël Sander,Lisa Anne Hendricks,Anuj Sharma,Andrew Bolt,Eszter Vértes,Jiri Simsa,Tomer Levinboim,Olcan Sercinoglu,Divyansh Shukla,Austin Wu,Craig Swanson,Danny Vainstein,Fan Bu,Bo Wang,Ryan Julian,Charles Yoon,Sergei Lebedev,Antonious Girgis,Bernd Bandemer,David Du,Todd Wang,Xi Chen,Ying Xiao,Peggy Lu,Natalie Ha,Vlad Ionescu,Simon Rowe,Josip Matak,Federico Lebron,Andreas Steiner,Lalit Jain,Manaal Faruqui,Nicolas Lacasse,Georgie Evans,Neesha Subramaniam,Dean Reich,Giulia Vezzani,Aditya Pandey,Joe Stanton,Tianhao Zhou,Liam McCafferty,Henry Griffiths,Verena Rieser,Soheil Hassas Yeganeh,Eleftheria Briakou,Lu Huang,Zichuan Wei,Liangchen Luo,Erik Jue,Gabby Wang,Victor Cotruta,Myriam Khan,Jongbin Park,Qiuchen Guo,Peiran Li,Rong Rong,Diego Antognini,Anastasia Petrushkina,Chetan Tekur,Eli Collins,Parul Bhatia,Chester Kwak,Wenhu Chen,Arvind Neelakantan,Immanuel Odisho,Sheng Peng,Vincent Nallatamby,Vaibhav Tulsyan,Fabian Pedregosa,Peng Xu,Raymond Lin,Yulong Wang,Emma Wang,Sholto Douglas,Reut Tsarfaty,Elena Gribovskaya,Renga Aravamudhan,Manu Agarwal,Mara Finkelstein,Qiao Zhang,Elizabeth Cole,Phil Crone,Sarmishta Velury,Anil Das,Chris Sauer,Luyao Xu,Danfeng Qin,Chenjie Gu,Dror Marcus,CJ Zheng,Wouter Van Gansbeke,Sobhan Miryoosefi,Haitian Sun,YaGuang Li,Charlie Chen,Jae Yoo,Pavel Dubov,Alex Tomala,Adams Yu,Paweł Wesołowski,Alok Gunjan,Eddie Cao,Jiaming Luo,Nikhil Sethi,Arkadiusz Socala,Laura Graesser,Tomas Kocisky,Arturo BC,Minmin Chen,Edward Lee,Sophie Wang,Weize Kong,Qiantong Xu,Nilesh Tripuraneni,Yiming Li,Xinxin Yu,Allen Porter,Paul Voigtlaender,Biao Zhang,Arpi Vezer,Sarah York,Qing Wei,Geoffrey Cideron,Mark Kurzeja,Seungyeon Kim,Benny Li,Angéline Pouget,Hyo Lee,Kaspar Daugaard,Yang Li,Dave Uthus,Aditya Siddhant,Paul Cavallaro,Sriram Ganapathy,Maulik Shah,Rolf Jagerman,Jeff Stanway,Piermaria Mendolicchio,Li Xiao,Kayi Lee,Tara Thompson,Shubham Milind Phal,Jason Chase,Sun Jae Lee,Adrian N Reyes,Disha Shrivastava,Zhen Qin,Roykrong Sukkerd,Seth Odoom,Lior Madmoni,John Aslanides,Jonathan Herzig,Elena Pochernina,Sheng Zhang,Parker Barnes,Daisuke Ikeda,Qiujia Li,Shuo-yiin Chang,Shakir Mohamed,Jim Sproch,Richard Powell,Bidisha Samanta,Domagoj Ćevid,Anton Kovsharov,Shrestha Basu Mallick,Srinivas Tadepalli,Anne Zheng,Kareem Ayoub,Andreas Noever,Christian Reisswig,Zhuo Xu,Junhyuk Oh,Martin Matysiak,Tim Blyth,Shereen Ashraf,Julien Amelot,Boone Severson,Michele Bevilacqua,Motoki Sano,Ethan Dyer,Ofir Roval,Anu Sinha,Yin Zhong,Sagi Perel,Tea Sabolić,Johannes Mauerer,Willi Gierke,Mauro Verzetti,Rodrigo Cabrera,Alvin Abdagic,Steven Hemingray,Austin Stone,Jong Lee,Farooq Ahmad,Karthik Raman,Lior Shani,Jonathan Lai,Orhan Firat,Nathan Waters,Eric Ge,Mo Shomrat,Himanshu Gupta,Rajeev Aggarwal,Tom Hudson,Bill Jia,Simon Baumgartner,Palak Jain,Joe Kovac,Junehyuk Jung,Ante Žužul,Will Truong,Morteza Zadimoghaddam,Songyou Peng,Marco Liang,Rachel Sterneck,Balaji Lakshminarayanan,Machel Reid,Oliver Woodman,Tong Zhou,Jianling Wang,Vincent Coriou,Arjun Narayanan,Jay Hoover,Yenai Ma,Apoorv Jindal,Clayton Sanford,Doug Reid,Swaroop Ramaswamy,Alex Kurakin,Roland Zimmermann,Yana Lunts,Dragos Dena,Zalán Borsos,Vered Cohen,Shujian Zhang,Will Grathwohl,Robert Dadashi,Morgan Redshaw,Joshua Kessinger,Julian Odell,Silvano Bonacina,Zihang Dai,Grace Chen,Ayush Dubey,Pablo Sprechmann,Mantas Pajarskas,Wenxuan Zhou,Niharika Ahuja,Tara Thomas,Martin Nikoltchev,Matija Kecman,Bharath Mankalale,Andrey Ryabtsev,Jennifer She,Christian Walder,Jiaming Shen,Lu Li,Carolina Parada,Sheena Panthaplackel,Okwan Kwon,Matt Lawlor,Utsav Prabhu,Yannick Schroecker,Marc'aurelio Ranzato,Pete Blois,Iurii Kemaev,Ting Yu,Dmitry,Lepikhin,Hao Xiong,Sahand Sharifzadeh,Oleaser Johnson,Jeremiah Willcock,Rui Yao,Greg Farquhar,Sujoy Basu,Hidetoshi Shimokawa,Nina Anderson,Haiguang Li,Khiem Pham,Yizhong Liang,Sebastian Borgeaud,Alexandre Moufarek,Hideto Kazawa,Blair Kutzman,Marcin Sieniek,Sara Smoot,Ruth Wang,Natalie Axelsson,Nova Fallen,Prasha Sundaram,Yuexiang Zhai,Varun Godbole,Petros Maniatis,Alek Wang,Ilia Shumailov,Santhosh Thangaraj,Remi Crocker,Nikita Gupta,Gang Wu,Phil Chen,Gellért Weisz,Celine Smith,Mojtaba Seyedhosseini,Boya Fang,Xiyang Luo,Roey Yogev,Zeynep Cankara,Andrew Hard,Helen Ran,Rahul Sukthankar,George Necula,Gaël Liu,Honglong Cai,Praseem Banzal,Daniel Keysers,Sanjay Ghemawat,Connie Tao,Emma Dunleavy,Aditi Chaudhary,Wei Li,Maciej Mikuła,Chen-Yu Lee,Tiziana Refice,Krishna Somandepalli,Alexandre Fréchette,Dan Bahir,John Karro,Keith Rush,Sarah Perrin,Bill Rosgen,Xiaomeng Yang,Clara Huiyi Hu,Mahmoud Alnahlawi,Justin Mao-Jones,Roopal Garg,Hoang Nguyen,Bat-Orgil Batsaikhan,Iñaki Iturrate,Anselm Levskaya,Avi Singh,Ashyana Kachra,Tony Lu,Denis Petek,Zheng Xu,Mark Graham,Lukas Zilka,Yael Karov,Marija Kostelac,Fangyu Liu,Yaohui Guo,Weiyue Wang,Bernd Bohnet,Emily Pitler,Tony Bruguier,Keisuke Kinoshita,Chrysovalantis Anastasiou,Nilpa Jha,Ting Liu,Jerome Connor,Phil Wallis,Philip Pham,Eric Bailey,Shixin Li,Heng-Tze Cheng,Sally Ma,Haiqiong Li,Akanksha Maurya,Kate Olszewska,Manfred Warmuth,Christy Koh,Dominik Paulus,Siddhartha Reddy Jonnalagadda,Enrique Piqueras,Ali Elqursh,Geoff Brown,Hadar Shemtov,Loren Maggiore,Fei Xia,Ryan Foley,Beka Westberg,George van den Driessche,Livio Baldini Soares,Arjun Kar,Michael Quinn,Siqi Zuo,Jialin Wu,Kyle Kastner,Anna Bortsova,Aijun Bai,Ales Mikhalap,Luowei Zhou,Jennifer Brennan,Vinay Ramasesh,Honglei Zhuang,John Maggs,Johan Schalkwyk,Yuntao Xu,Hui Huang,Andrew Howard,Sasha Brown,Linting Xue,Gloria Shen,Brian Albert,Neha Jha,Daniel Zheng,Varvara Krayvanova,Spurthi Amba Hombaiah,Olivier Lacombe,Gautam Vasudevan,Dan Graur,Tian Xie,Meet Gandhi,Bangju Wang,Dustin Zelle,Harman Singh,Dahun Kim,Sébastien Cevey,Victor Ungureanu,Natasha Noy,Fei Liu,Annie Xie,Fangxiaoyu Feng,Katerina Tsihlas,Daniel Formoso,Neera Vats,Quentin Wellens,Yinan Wang,Niket Kumar Bhumihar,Samrat Ghosh,Matt Hoffman,Tom Lieber,Oran Lang,Kush Bhatia,Tom Paine,Aroonalok Pyne,Ronny Votel,Madeleine Clare Elish,Benoit Schillings,Alex Panagopoulos,Haichuan Yang,Adam Raveret,Zohar Yahav,Shuang Liu,Warren Chen,Dalia El Badawy,Nishant Agrawal,Mohammed Badawi,Mahdi Mirzazadeh,Carla Bromberg,Fan Ye,Chang Liu,Tatiana Sholokhova,George-Cristian Muraru,Gargi Balasubramaniam,Jonathan Malmaud,Alen Carin,Danilo Martins,Irina Jurenka,Pankil Botadra,Dave Lacey,Richa Singh,Mariano Schain,Dan Zheng,Isabelle Guyon,Victor Lavrenko,Seungji Lee,Xiang Zhou,Demis Hassabis,Jeshwanth Challagundla,Derek Cheng,Nikhil Mehta,Matthew Mauger,Michela Paganini,Pushkar Mishra,Kate Lee,Zhang Li,Lexi Baugher,Ondrej Skopek,Max Chang,Amir Zait,Gaurav Menghani,Lizzetth Bellot,Guangxing Han,Jean-Michel Sarr,Sharat Chikkerur,Himanshu Sahni,Rohan Anil,Arun Narayanan,Chandu Thekkath,Daniele Pighin,Hana Strejček,Marko Velic,Fred Bertsch,Manuel Tragut,Keran Rong,Alicia Parrish,Kai Bailey,Jiho Park,Isabela Albuquerque,Abhishek Bapna,Rajesh Venkataraman,Alec Kosik,Johannes Griesser,Zhiwei Deng,Alek Andreev,Qingyun Dou,Kevin Hui,Fanny Wei,Xiaobin Yu,Lei Shu,Avia Aharon,David Barker,Badih Ghazi,Sebastian Flennerhag,Chris Breaux,Yuchuan Liu,Matthew Bilotti,Josh Woodward,Uri Alon,Stephanie Winkler,Tzu-Kuo Huang,Kostas Andriopoulos,João Gabriel Oliveira,Penporn Koanantakool,Berkin Akin,Michael Wunder,Cicero Nogueira dos Santos,Mohammad Hossein Bateni,Lin Yang,Dan Horgan,Beer Changpinyo,Keyvan Amiri,Min Ma,Dayeong Lee,Lihao Liang,Anirudh Baddepudi,Tejasi Latkar,Raia Hadsell,Jun Xu,Hairong Mu,Michael Han,Aedan Pope,Snchit Grover,Frank Kim,Ankit Bhagatwala,Guan Sun,Yamini Bansal,Amir Globerson,Alireza Nazari,Samira Daruki,Hagen Soltau,Jane Labanowski,Laurent El Shafey,Matt Harvey,Yanif Ahmad,Elan Rosenfeld,William Kong,Etienne Pot,Yi-Xuan Tan,Aurora Wei,Victoria Langston,Marcel Prasetya,Petar Veličković,Richard Killam,Robin Strudel,Darren Ni,Zhenhai Zhu,Aaron Archer,Kavya Kopparapu,Lynn Nguyen,Emilio Parisotto,Hussain Masoom,Sravanti Addepalli,Jordan Grimstad,Hexiang Hu,Joss Moore,Avinatan Hassidim,Le Hou,Mukund Raghavachari,Jared Lichtarge,Adam R. Brown,Hilal Dib,Natalia Ponomareva,Justin Fu,Yujing Zhang,Altaf Rahman,Joana Iljazi,Edouard Leurent,Gabriel Dulac-Arnold,Cosmo Du,Chulayuth Asawaroengchai,Larry Jin,Ela Gruzewska,Ziwei Ji,Benigno Uria,Daniel De Freitas,Paul Barham,Lauren Beltrone,Víctor Campos,Jun Yan,Neel Kovelamudi,Arthur Nguyen,Elinor Davies,Zhichun Wu,Zoltan Egyed,Kristina Toutanova,Nithya Attaluri,Hongliang Fei,Peter Stys,Siddhartha Brahma,Martin Izzard,Siva Velusamy,Scott Lundberg,Vincent Zhuang,Kevin Sequeira,Adam Santoro,Ehsan Amid,Ophir Aharoni,Shuai Ye,Mukund Sundararajan,Lijun Yu,Yu-Cheng Ling,Stephen Spencer,Hugo Song,Josip Djolonga,Christo Kirov,Sonal Gupta,Alessandro Bissacco,Clemens Meyer,Mukul Bhutani,Andrew Dai,Weiyi Wang,Siqi Liu,Ashwin Sreevatsa,Qijun Tan,Maria Wang,Lucy Kim,Yicheng Wang,Alex Irpan,Yang Xiao,Stanislav Fort,Yifan He,Alex Gurney,Bryan Gale,Yue Ma,Monica Roy,Viorica Patraucean,Taylan Bilal,Golnaz Ghiasi,Anahita Hosseini,Melvin Johnson,Zhuowan Li,Yi Tay,Benjamin Beyret,Katie Millican,Josef Broder,Mayank Lunayach,Danny Swisher,Eugen Vušak,David Parkinson,MH Tessler,Adi Mayrav Gilady,Richard Song,Allan Dafoe,Yves Raimond,Masa Yamaguchi,Itay Karo,Elizabeth Nielsen,Kevin Kilgour,Mike Dusenberry,Rajiv Mathews,Jiho Choi,Siyuan Qiao,Harsh Mehta,Sahitya Potluri,Chris Knutsen,Jialu Liu,Tat Tan,Kuntal Sengupta,Keerthana Gopalakrishnan,Abodunrinwa Toki,Mencher Chiang,Mike Burrows,Grace Vesom,Zafarali Ahmed,Ilia Labzovsky,Siddharth Vashishtha,Preeti Singh,Ankur Sharma,Ada Ma,Jinyu Xie,Pranav Talluri,Hannah Forbes-Pollard,Aarush Selvan,Joel Wee,Loic Matthey,Tom Funkhouser,Parthasarathy Gopavarapu,Lev Proleev,Cheng Li,Matt Thomas,Kashyap Kolipaka,Zhipeng Jia,Ashwin Kakarla,Srinivas Sunkara,Joan Puigcerver,Suraj Satishkumar Sheth,Emily Graves,Chen Wang,Sadh MNM Khan,Kai Kang,Shyamal Buch,Fred Zhang,Omkar Savant,David Soergel,Kevin Lee,Linda Friso,Xuanyi Dong,Rahul Arya,Shreyas Chandrakaladharan,Connor Schenck,Greg Billock,Tejas Iyer,Anton Bakalov,Leslie Baker,Alex Ruiz,Angad Chandorkar,Trieu Trinh,Matt Miecnikowski,Yanqi Zhou,Yangsibo Huang,Jiazhong Nie,Ali Shah,Ashish Thapliyal,Sam Haves,Lun Wang,Uri Shaham,Patrick Morris-Suzuki,Soroush Radpour,Leonard Berrada,Thomas Strohmann,Chaochao Yan,Jingwei Shen,Sonam Goenka,Tris Warkentin,Petar Dević,Dan Belov,Albert Webson,Madhavi Yenugula,Puranjay Datta,Jerry Chang,Nimesh Ghelani,Aviral Kumar,Vincent Perot,Jessica Lo,Yang Song,Herman Schmit,Jianmin Chen,Vasilisa Bashlovkina,Xiaoyue Pan,Diana Mincu,Paul Roit,Isabel Edkins,Andy Davis,Yujia Li,Ben Horn,Xinjian Li,Pradeep Kumar S,Eric Doi,Wanzheng Zhu,Sri Gayatri Sundara Padmanabhan,Siddharth Verma,Jasmine Liu,Heng Chen,Mihajlo Velimirović,Malcolm Reynolds,Priyanka Agrawal,Nick Sukhanov,Abhinit Modi,Siddharth Goyal,John Palowitch,Nima Khajehnouri,Wing Lowe,David Klinghoffer,Sharon Silver,Vinh Tran,Candice Schumann,Francesco Piccinno,Xi Liu,Mario Lučić,Xiaochen Yang,Sandeep Kumar,Ajay Kannan,Ragha Kotikalapudi,Mudit Bansal,Fabian Fuchs,Javad Hosseini,Abdelrahman Abdelhamed,Dawn Bloxwich,Tianhe Yu,Ruoxin Sang,Gregory Thornton,Karan Gill,Yuchi Liu,Virat Shejwalkar,Jason Lin,Zhipeng Yan,Kehang Han,Thomas Buschmann,Michael Pliskin,Zhi Xing,Susheel Tatineni,Junlin Zhang,Sissie Hsiao,Gavin Buttimore,Marcus Wu,Zefei Li,Geza Kovacs,Legg Yeung,Tao Huang,Aaron Cohen,Bethanie Brownfield,Averi Nowak,Mikel Rodriguez,Tianze Shi,Hado van Hasselt,Kevin Cen,Deepanway Ghoshal,Kushal Majmundar,Weiren Yu,Warren,Chen,Danila Sinopalnikov,Hao Zhang,Vlado Galić,Di Lu,Zeyu Zheng,Maggie Song,Gary Wang,Gui Citovsky,Swapnil Gawde,Isaac Galatzer-Levy,David Silver,Ivana Balazevic,Dipanjan Das,Kingshuk Majumder,Yale Cong,Praneet Dutta,Dustin Tran,Hui Wan,Junwei Yuan,Daniel Eppens,Alanna Walton,Been Kim,Harry Ragan,James Cobon-Kerr,Lu Liu,Weijun Wang,Bryce Petrini,Jack Rae,Rakesh Shivanna,Yan Xiong,Chace Lee,Pauline Coquinot,Yiming Gu,Lisa Patel,Blake Hechtman,Aviel Boag,Orion Jankowski,Alex Wertheim,Alex Lee,Paul Covington,Hila Noga,Sam Sobell,Shanthal Vasanth,William Bono,Chirag Nagpal,Wei Fan,Xavier Garcia,Kedar Soparkar,Aybuke Turker,Nathan Howard,Sachit Menon,Yuankai Chen,Vikas Verma,Vladimir Pchelin,Harish Rajamani,Valentin Dalibard,Ana Ramalho,Yang Guo,Kartikeya Badola,Seojin Bang,Nathalie Rauschmayr,Julia Proskurnia,Sudeep Dasari,Xinyun Chen,Mikhail Sushkov,Anja Hauth,Pauline Sho,Abhinav Singh,Bilva Chandra,Allie Culp,Max Dylla,Olivier Bachem,James Besley,Heri Zhao,Timothy Lillicrap,Wei Wei,Wael Al Jishi,Ning Niu,Alban Rrustemi,Raphaël Lopez Kaufman,Ryan Poplin,Jewel Zhao,Minh Truong,Shikhar Bharadwaj,Ester Hlavnova,Eli Stickgold,Cordelia Schmid,Georgi Stephanov,Zhaoqi Leng,Frederick Liu,Léonard Hussenot,Shenil Dodhia,Juliana Vicente Franco,Lesley Katzen,Abhanshu Sharma,Sarah Cogan,Zuguang Yang,Aniket Ray,Sergi Caelles,Shen Yan,Ravin Kumar,Daniel Gillick,Renee Wong,Joshua Ainslie,Jonathan Hoech,Séb Arnold,Dan Abolafia,Anca Dragan,Ben Hora,Grace Hu,Alexey Guseynov,Yang Lu,Chas Leichner,Jinmeng Rao,Abhimanyu Goyal,Nagabhushan Baddi,Daniel Hernandez Diaz,Tim McConnell,Max Bain,Jake Abernethy,Qiqi Yan,Rylan Schaeffer,Paul Vicol,Will Thompson,Montse Gonzalez Arenas,Mathias Bellaiche,Pablo Barrio,Stefan Zinke,Riccardo Patana,Pulkit Mehta,JK Kearns,Avraham Ruderman,Scott Pollom,David D'Ambrosio,Cath Hope,Yang Yu,Andrea Gesmundo,Kuang-Huei Lee,Aviv Rosenberg,Yiqian Zhou,Yaoyiran Li,Drew Garmon,Yonghui Wu,Safeen Huda,Gil Fidel,Martin Baeuml,Jian Li,Phoebe Kirk,Rhys May,Tao Tu,Sara Mc Carthy,Toshiyuki Fukuzawa,Miranda Aperghis,Chih-Kuan Yeh,Toshihiro Yoshino,Bo Li,Austin Myers,Kaisheng Yao,Ben Limonchik,Changwan Ryu,Rohun Saxena,Alex Goldin,Ruizhe Zhao,Rocky Rhodes,Tao Zhu,Divya Tyam,Heidi Howard,Nathan Byrd,Hongxu Ma,Yan Wu,Ryan Mullins,Qingze Wang,Aida Amini,Sebastien Baur,Yiran Mao,Subhashini Venugopalan,Will Song,Wen Ding,Paul Collins,Sashank Reddi,Megan Shum,Andrei Rusu,Luisa Zintgraf,Kelvin Chan,Sheela Goenka,Mathieu Blondel,Michael Collins,Renke Pan,Marissa Giustina,Nikolai Chinaev,Christian Schuler,Ce Zheng,Jonas Valfridsson,Alyssa Loo,Alex Yakubovich,Jamie Smith,Tao Jiang,Rich Munoz,Gabriel Barcik,Rishabh Bansal,Mingyao Yang,Yilun Du,Pablo Duque,Mary Phuong,Alexandra Belias,Kunal Lad,Zeyu Liu,Tal Schuster,Karthik Duddu,Jieru Hu,Paige Kunkle,Matthew Watson,Jackson Tolins,Josh Smith,Denis Teplyashin,Garrett Bingham,Marvin Ritter,Marco Andreetto,Divya Pitta,Mohak Patel,Shashank Viswanadha,Trevor Strohman,Catalin Ionescu,Jincheng Luo,Yogesh Kalley,Jeremy Wiesner,Dan Deutsch,Derek Lockhart,Peter Choy,Rumen Dangovski,Chawin Sitawarin,Cat Graves,Tanya Lando,Joost van Amersfoort,Ndidi Elue,Zhouyuan Huo,Pooya Moradi,Jean Tarbouriech,Henryk Michalewski,Wenting Ye,Eunyoung Kim,Alex Druinsky,Florent Altché,Xinyi Chen,Artur Dwornik,Da-Cheng Juan,Rivka Moroshko,Horia Toma,Jarrod Kahn,Hai Qian,Maximilian Sieb,Irene Cai,Roman Goldenberg,Praneeth Netrapalli,Sindhu Raghuram,Yuan Gong,Lijie Fan,Evan Palmer,Yossi Matias,Valentin Gabeur,Shreya Pathak,Tom Ouyang,Don Metzler,Geoff Bacon,Srinivasan Venkatachary,Sridhar Thiagarajan,Alex Cullum,Eran Ofek,Vytenis Sakenas,Mohamed Hammad,Cesar Magalhaes,Mayank Daswani,Oscar Chang,Ashok Popat,Ruichao Li,Komal Jalan,Yanhan Hou,Josh Lipschultz,Antoine He,Wenhao Jia,Pier Giuseppe Sessa,Prateek Kolhar,William Wong,Sumeet Singh,Lukas Haas,Jay Whang,Hanna Klimczak-Plucińska,Georges Rotival,Grace Chung,Yiqing Hua,Anfal Siddiqui,Nicolas Serrano,Dongkai Chen,Billy Porter,Libin Bai,Keshav Shivam,Sho Arora,Partha Talukdar,Tom Cobley,Sangnie Bhardwaj,Evgeny Gladchenko,Simon Green,Kelvin Guu,Felix Fischer,Xiao Wu,Eric Wang,Achintya Singhal,Tatiana Matejovicova,James Martens,Hongji Li,Roma Patel,Elizabeth Kemp,Jiaqi Pan,Lily Wang,Blake JianHang Chen,Jean-Baptiste Alayrac,Navneet Potti,Erika Gemzer,Eugene Ie,Kay McKinney,Takaaki Saeki,Edward Chou,Pascal Lamblin,SQ Mah,Zach Fisher,Martin Chadwick,Jon Stritar,Obaid Sarvana,Andrew Hogue,Artem Shtefan,Hadi Hashemi,Yang Xu,Jindong Gu,Sharad Vikram,Chung-Ching Chang,Sabela Ramos,Logan Kilpatrick,Weijuan Xi,Jenny Brennan,Yinghao Sun,Abhishek Jindal,Ionel Gog,Dawn Chen,Felix Wu,Jason Lee,Sudhindra Kopalle,Srinadh Bhojanapalli,Oriol Vinyals,Natan Potikha,Burcu Karagol Ayan,Yuan Yuan,Michael Riley,Piotr Stanczyk,Sergey Kishchenko,Bing Wang,Dan Garrette,Antoine Yang,Vlad Feinberg,CJ Carey,Javad Azizi,Viral Shah,Erica Moreira,Chongyang Shi,Josh Feldman,Elizabeth Salesky,Thomas Lampe,Aneesh Pappu,Duhyeon Kim,Jonas Adler,Avi Caciularu,Brian Walker,Yunhan Xu,Yochai Blau,Dylan Scandinaro,Terry Huang,Sam El-Husseini,Abhishek Sinha,Lijie Ren,Taylor Tobin,Patrik Sundberg,Tim Sohn,Vikas Yadav,Mimi Ly,Emily Xue,Jing Xiong,Afzal Shama Soudagar,Sneha Mondal,Nikhil Khadke,Qingchun Ren,Ben Vargas,Stan Bileschi,Sarah Chakera,Cindy Wang,Boyu Wang,Yoni Halpern,Joe Jiang,Vikas Sindhwani,Petre Petrov,Pranavaraj Ponnuramu,Sanket Vaibhav Mehta,Yu Watanabe,Betty Chan,Matheus Wisniewski,Trang Pham,Jingwei Zhang,Conglong Li,Dario de Cesare,Art Khurshudov,Alex Vasiloff,Melissa Tan,Zoe Ashwood,Bobak Shahriari,Maryam Majzoubi,Garrett Tanzer,Olga Kozlova,Robin Alazard,James Lee-Thorp,Nguyet Minh Phu,Isaac Tian,Junwhan Ahn,Andy Crawford,Lauren Lax,Yuan,Shangguan,Iftekhar Naim,David Ross,Oleksandr Ferludin,Tongfei Guo,Andrea Banino,Hubert Soyer,Xiaoen Ju,Dominika Rogozińska,Ishaan Malhi,Marcella Valentine,Daniel Balle,Apoorv Kulshreshtha,Maciej Kula,Yiwen Song,Sophia Austin,John Schultz,Roy Hirsch,Arthur Douillard,Apoorv Reddy,Michael Fink,Summer Yue,Khyatti Gupta,Adam Zhang,Norman Rink,Daniel McDuff,Lei Meng,András György,Yasaman Razeghi,Ricky Liang,Kazuki Osawa,Aviel Atias,Matan Eyal,Tyrone Hill,Nikolai Grigorev,Zhengdong Wang,Nitish Kulkarni,Rachel Soh,Ivan Lobov,Zachary Charles,Sid Lall,Kazuma Hashimoto,Ido Kessler,Victor Gomes,Zelda Mariet,Danny Driess,Alessandro Agostini,Canfer Akbulut,Jingcao Hu,Marissa Ikonomidis,Emily Caveness,Kartik Audhkhasi,Saurabh Agrawal,Ioana Bica,Evan Senter,Jayaram Mudigonda,Kelly Chen,Jingchen Ye,Xuanhui Wang,James Svensson,Philipp Fränken,Josh Newlan,Li Lao,Eva Schnider,Sami Alabed,Joseph Kready,Jesse Emond,Afief Halumi,Tim Zaman,Chengxi Ye,Naina Raisinghani,Vilobh Meshram,Bo Chang,Ankit Singh Rawat,Axel Stjerngren,Sergey Levi,Rui Wang,Xiangzhu Long,Mitchelle Rasquinha,Steven Hand,Aditi Mavalankar,Lauren Agubuzu,Sudeshna Roy,Junquan Chen,Jarek Wilkiewicz,Hao Zhou,Michal Jastrzebski,Qiong Hu,Agustin Dal Lago,Ramya Sree Boppana,Wei-Jen Ko,Jennifer Prendki,Yao Su,Zhi Li,Eliza Rutherford,Girish Ramchandra Rao,Ramona Comanescu,Adrià Puigdomènech,Qihang Chen,Dessie Petrova,Christine Chan,Vedrana Milutinovic,Felipe Tiengo Ferreira,Chin-Yi Cheng,Ming Zhang,Tapomay Dey,Sherry Yang,Ramesh Sampath,Quoc Le,Howard Zhou,Chu-Cheng Lin,Hoi Lam,Christine Kaeser-Chen,Kai Hui,Dean Hirsch,Tom Eccles,Basil Mustafa,Shruti Rijhwani,Morgane Rivière,Yuanzhong Xu,Junjie Wang,Xinyang Geng,Xiance Si,Arjun Khare,Cheolmin Kim,Vahab Mirrokni,Kamyu Lee,Khuslen Baatarsukh,Nathaniel Braun,Lisa Wang,Pallavi LV,Richard Tanburn,Yuvein,Zhu,Fangda Li,Setareh Ariafar,Dan Goldberg,Ken Burke,Daniil Mirylenka,Meiqi Guo,Olaf Ronneberger,Hadas Natalie Vogel,Liqun Cheng,Nishita Shetty,Johnson Jia,Thomas Jimma,Corey Fry,Ted Xiao,Martin Sundermeyer,Ryan Burnell,Yannis Assael,Mario Pinto,JD Chen,Rohit Sathyanarayana,Donghyun Cho,Jing Lu,Rishabh Agarwal,Sugato Basu,Lucas Gonzalez,Dhruv Shah,Meng Wei,Dre Mahaarachchi,Rohan Agrawal,Tero Rissa,Yani Donchev,Ramiro Leal-Cavazos,Adrian Hutter,Markus Mircea,Alon Jacovi,Faruk Ahmed,Jiageng Zhang,Shuguang Hu,Bo-Juen Chen,Jonni Kanerva,Guillaume Desjardins,Andrew Lee,Nikos Parotsidis,Asier Mujika,Tobias Weyand,Jasper Snoek,Jo Chick,Kai Chen,Paul Chang,Ethan Mahintorabi,Zi Wang,Tolly Powell,Orgad Keller,Abhirut Gupta,Claire Sha,Kanav Garg,Nicolas Heess,Ágoston Weisz,Cassidy Hardin,Bartek Wydrowski,Ben Coleman,Karina Zainullina,Pankaj Joshi,Alessandro Epasto,Terry Spitz,Binbin Xiong,Kai Zhao,Arseniy Klimovskiy,Ivy Zheng,Johan Ferret,Itay Yona,Waleed Khawaja,Jean-Baptiste Lespiau,Maxim Krikun,Siamak Shakeri,Timothee Cour,Bonnie Li,Igor Krivokon,Dan Suh,Alex Hofer,Jad Al Abdallah,Nikita Putikhin,Oscar Akerlund,Silvio Lattanzi,Anurag Kumar,Shane Settle,Himanshu Srivastava,Folawiyo Campbell-Ajala,Edouard Rosseel,Mihai Dorin Istin,Nishanth Dikkala,Anand Rao,Nick Young,Kate Lin,Dhruva Bhaswar,Yiming Wang,Jaume Sanchez Elias,Kritika Muralidharan,James Keeling,Dayou Du,Siddharth Gopal,Gregory Dibb,Charles Blundell,Manolis Delakis,Jacky Liang,Marco Tulio Ribeiro,Georgi Karadzhov,Guillermo Garrido,Ankur Bapna,Jiawei Cao,Adam Sadovsky,Pouya Tafti,Arthur Guez,Coline Devin,Yixian Di,Jinwei Xing,Chuqiao,Xu,Hanzhao Lin,Chun-Te Chu,Sameera Ponda,Wesley Helmholz,Fan Yang,Yue Gao,Sara Javanmardi,Wael Farhan,Alex Ramirez,Ricardo Figueira,Khe Chai Sim,Yuval Bahat,Ashwin Vaswani,Liangzhe Yuan,Gufeng Zhang,Leland Rechis,Hanjun Dai,Tayo Oguntebi,Alexandra Cordell,Eugénie Rives,Kaan Tekelioglu,Naveen Kumar,Bing Zhang,Aurick Zhou,Nikolay Savinov,Andrew Leach,Alex Tudor,Sanjay Ganapathy,Yanyan Zheng,Mirko Rossini,Vera Axelrod,Arnaud Autef,Yukun Zhu,Zheng Zheng,Mingda Zhang,Baochen Sun,Jie Ren,Nenad Tomasev,Nithish Kannan,Amer Sinha,Charles Chen,Louis O'Bryan,Alex Pak,Aditya Kusupati,Weel Yang,Deepak Ramachandran,Patrick Griffin,Seokhwan Kim,Philipp Neubeck,Craig Schiff,Tammo Spalink,Mingyang Ling,Arun Nair,Ga-Young Joung,Linda Deng,Avishkar Bhoopchand,Lora Aroyo,Tom Duerig,Jordan Griffith,Gabe Barth-Maron,Jake Ades,Alex Haig,Ankur Taly,Yunting Song,Paul Michel,Dave Orr,Dean Weesner,Corentin Tallec,Carrie Grimes Bostock,Paul Niemczyk,Andy Twigg,Mudit Verma,Rohith Vallu,Henry Wang,Marco Gelmi,Kiranbir Sodhia,Aleksandr Chuklin,Omer Goldman,Jasmine George,Liang Bai,Kelvin Zhang,Petar Sirkovic,Efrat Nehoran,Golan Pundak,Jiaqi Mu,Alice Chen,Alex Greve,Paulo Zacchello,David Amos,Heming Ge,Eric Noland,Colton Bishop,Jeffrey Dudek,Youhei Namiki,Elena Buchatskaya,Jing Li,Dorsa Sadigh,Masha Samsikova,Dan Malkin,Damien Vincent,Robert David,Rob Willoughby,Phoenix Meadowlark,Shawn Gao,Yan Li,Raj Apte,Amit Jhindal,Stein Xudong Lin,Alex Polozov,Zhicheng Wang,Tomas Mery,Anirudh GP,Varun Yerram,Sage Stevens,Tianqi Liu,Noah Fiedel,Charles Sutton,Matthew Johnson,Xiaodan Song,Kate Baumli,Nir Shabat,Muqthar Mohammad,Hao Liu,Marco Selvi,Yichao Zhou,Mehdi Hafezi Manshadi,Chu-ling Ko,Anthony Chen,Michael Bendersky,Jorge Gonzalez Mendez,Nisarg Kothari,Amir Zandieh,Yiling Huang,Daniel Andor,Ellie Pavlick,Idan Brusilovsky,Jitendra Harlalka,Sally Goldman,Andrew Lampinen,Guowang Li,Asahi Ushio,Somit Gupta,Lei Zhang,Chuyuan Kelly Fu,Madhavi Sewak,Timo Denk,Jed Borovik,Brendan Jou,Avital Zipori,Prateek Jain,Junwen Bai,Thang Luong,Jonathan Tompson,Alice Li,Li Liu,George Powell,Jiajun Shen,Alex Feng,Grishma Chole,Da Yu,Yinlam Chow,Tongxin Yin,Eric Malmi,Kefan Xiao,Yash Pande,Shachi Paul,Niccolò Dal Santo,Adil Dostmohamed,Sergio Guadarrama,Aaron Phillips,Thanumalayan Sankaranarayana Pillai,Gal Yona,Amin Ghafouri,Preethi Lahoti,Benjamin Lee,Dhruv Madeka,Eren Sezener,Simon Tokumine,Adrian Collister,Nicola De Cao,Richard Shin,Uday Kalra,Parker Beak,Emily Nottage,Ryo Nakashima,Ivan Jurin,Vikash Sehwag,Meenu Gaba,Junhao Zeng,Kevin R. McKee,Fernando Pereira,Tamar Yakar,Amayika Panda,Arka Dhar,Peilin Zhong,Daniel Sohn,Mark Brand,Lars Lowe Sjoesund,Viral Carpenter,Sharon Lin,Shantanu Thakoor,Marcus Wainwright,Ashwin Chaugule,Pranesh Srinivasan,Muye Zhu,Bernett Orlando,Jack Weber,Ayzaan Wahid,Gilles Baechler,Apurv Suman,Jovana Mitrović,Gabe Taubman,Honglin Yu,Helen King,Josh Dillon,Cathy Yip,Dhriti Varma,Tomas Izo,Levent Bolelli,Borja De Balle Pigem,Julia Di Trapani,Fotis Iliopoulos,Adam Paszke,Nishant Ranka,Joe Zou,Francesco Pongetti,Jed McGiffin,Alex Siegman,Rich Galt,Ross Hemsley,Goran Žužić,Victor Carbune,Tao Li,Myle Ott,Félix de Chaumont Quitry,David Vilar Torres,Yuri Chervonyi,Tomy Tsai,Prem Eruvbetine,Samuel Yang,Matthew Denton,Jake Walker,Slavica Andačić,Idan Heimlich Shtacher,Vittal Premachandran,Harshal Tushar Lehri,Cip Baetu,Damion Yates,Lampros Lamprou,Mariko Iinuma,Ioana Mihailescu,Ben Albrecht,Shachi Dave,Susie Sargsyan,Bryan Perozzi,Lucas Manning,Chiyuan Zhang,Denis Vnukov,Igor Mordatch,Raia Hadsell Wolfgang Macherey,Ryan Kappedal,Jim Stephan,Aditya Tripathi,Klaus Macherey,Jun Qian,Abhishek Bhowmick,Shekoofeh Azizi,Rémi Leblond,Shiva Mohan Reddy Garlapati,Timothy Knight,Matthew Wiethoff,Wei-Chih Hung,Anelia Angelova,Georgios Evangelopoulos,Pawel Janus,Dimitris Paparas,Matthew Rahtz,Ken Caluwaerts,Vivek Sampathkumar,Daniel Jarrett,Shadi Noghabi,Antoine Miech,Chak Yeung,Geoff Clark,Henry Prior,Fei Zheng,Jean Pouget-Abadie,Indro Bhattacharya,Kalpesh Krishna,Will Bishop,Zhe Yuan,Yunxiao Deng,Ashutosh Sathe,Kacper Krasowiak,Ciprian Chelba,Cho-Jui Hsieh,Kiran Vodrahalli,Buhuang Liu,Thomas Köppe,Amr Khalifa,Lubo Litchev,Pichi Charoenpanit,Reed Roberts,Sachin Yadav,Yasumasa Onoe,Desi Ivanov,Megha Mohabey,Vighnesh Birodkar,Nemanja Rakićević,Pierre Sermanet,Vaibhav Mehta,Krishan Subudhi,Travis Choma,Will Ng,Luheng He,Kathie Wang,Tasos Kementsietsidis,Shane Gu,Mansi Gupta,Andrew Nystrom,Mehran Kazemi,Timothy Chung,Nacho Cano,Nikhil Dhawan,Yufei Wang,Jiawei Xia,Trevor Yacovone,Eric Jia,Mingqing Chen,Simeon Ivanov,Ashrith Sheshan,Sid Dalmia,Paweł Stradomski,Pengcheng Yin,Salem Haykal,Congchao Wang,Dennis Duan,Neslihan Bulut,Greg Kochanski,Liam MacDermed,Namrata Godbole,Shitao Weng,Jingjing Chen,Rachana Fellinger,Ramin Mehran,Daniel Suo,Hisham Husain,Tong He,Kaushal Patel,Joshua Howland,Randall Parker,Kelvin Nguyen,Sharath Maddineni,Chris Rawles,Mina Khan,Shlomi Cohen-Ganor,Amol Mandhane,Xinyi Wu,Chenkai Kuang,Iulia Comşa,Ramya Ganeshan,Hanie Sedghi,Adam Bloniarz,Nuo Wang Pierse,Anton Briukhov,Petr Mitrichev,Anita Gergely,Serena Zhan,Allan Zhou,Nikita Saxena,Eva Lu,Josef Dean,Ashish Gupta,Nicolas Perez-Nieves,Renjie Wu,Cory McLean,Wei Liang,Disha Jindal,Anton Tsitsulin,Wenhao Yu,Kaiz Alarakyia,Tom Schaul,Piyush Patil,Peter Sung,Elijah Peake,Hongkun Yu,Feryal Behbahani,JD Co-Reyes,Alan Ansell,Sean Sun,Clara Barbu,Jonathan Lee,Seb Noury,James Allingham,Bilal Piot,Mohit Sharma,Christopher Yew,Ivan Korotkov,Bibo Xu,Demetra Brady,Goran Petrovic,Shibl Mourad,Claire Cui,Aditya Gupta,Parker Schuh,Saarthak Khanna,Anna Goldie,Abhinav Arora,Vadim Zubov,Amy Stuart,Mark Epstein,Yun Zhu,Jianqiao Liu,Yury Stuken,Ziyue Wang,Karolis Misiunas,Dee Guo,Ashleah Gill,Ale Hartman,Zaid Nabulsi,Aurko Roy,Aleksandra Faust,Jason Riesa,Ben Withbroe,Mengchao Wang,Marco Tagliasacchi,Andreea Marzoca,James Noraky,Serge Toropov,Malika Mehrotra,Bahram Raad,Sanja Deur,Steve Xu,Marianne Monteiro,Zhongru Wu,Yi Luan,Sam Ritter,Nick Li,Håvard Garnes,Yanzhang He,Martin Zlocha,Jifan Zhu,Matteo Hessel,Will Wu,Spandana Raj Babbula,Chizu Kawamoto,Yuanzhen Li,Mehadi Hassen,Yan Wang,Brian Wieder,James Freedman,Yin Zhang,Xinyi Bai,Tianli Yu,David Reitter,XiangHai Sheng,Mateo Wirth,Aditya Kini,Dima Damen,Mingcen Gao,Rachel Hornung,Michael Voznesensky,Brian Roark,Adhi Kuncoro,Yuxiang Zhou,Rushin Shah,Anthony Brohan,Kuangyuan Chen,James Wendt,David Rim,Paul Kishan Rubenstein,Jonathan Halcrow,Michelle Liu,Ty Geri,Yunhsuan Sung,Jane Shapiro,Shaan Bijwadia,Chris Duvarney,Christina Sorokin,Paul Natsev,Reeve Ingle,Pramod Gupta,Young Maeng,Ndaba Ndebele,Kexin Zhu,Valentin Anklin,Katherine Lee,Yuan Liu,Yaroslav Akulov,Shaleen Gupta,Guolong Su,Flavien Prost,Tianlin Liu,Vitaly Kovalev,Pol Moreno,Martin Scholz,Sam Redmond,Zongwei Zhou,Alex Castro-Ros,André Susano Pinto,Dia Kharrat,Michal Yarom,Rachel Saputro,Jannis Bulian,Ben Caine,Ji Liu,Abbas Abdolmaleki,Shariq Iqbal,Tautvydas Misiunas,Mikhail Sirotenko,Shefali Garg,Guy Bensky,Huan Gui,Xuezhi Wang,Raphael Koster,Mike Bernico,Da Huang,Romal Thoppilan,Trevor Cohn,Ben Golan,Wenlei Zhou,Andrew Rosenberg,Markus Freitag,Tynan Gangwani,Vincent Tsang,Anand Shukla,Xiaoqi Ren,Minh Giang,Chi Zou,Andre Elisseeff,Charline Le Lan,Dheeru Dua,Shuba Lall,Pranav Shyam,Frankie Garcia,Sarah Nguyen,Michael Guzman,AJ Maschinot,Marcello Maggioni,Ming-Wei Chang,Karol Gregor,Lotte Weerts,Kumaran Venkatesan,Bogdan Damoc,Leon Liu,Jan Wassenberg,Lewis Ho,Becca Roelofs,Majid Hadian,François-Xavier Aubet,Yu Liang,Sami Lachgar,Danny Karmon,Yong Cheng,Amelio Vázquez-Reina,Angie Chen,Zhuyun Dai,Andy Brock,Shubham Agrawal,Chenxi Pang,Peter Garst,Mariella Sanchez-Vargas,Ivor Rendulic,Aditya Ayyar,Andrija Ražnatović,Olivia Ma,Roopali Vij,Neha Sharma,Ashwin Balakrishna,Bingyuan Liu,Ian Mackinnon,Sorin Baltateanu,Petra Poklukar,Gabriel Ibagon,Colin Ji,Hongyang Jiao,Isaac Noble,Wojciech Stokowiec,Zhihao Li,Jeff Dean,David Lindner,Mark Omernick,Kristen Chiafullo,Mason Dimarco,Vitor Rodrigues,Vittorio Selo,Garrett Honke,Xintian,Wu,Wei He,Adam Hillier,Anhad Mohananey,Vihari Piratla,Chang Ye,Chase Malik,Sebastian Riedel,Samuel Albanie,Zi Yang,Kenny Vassigh,Maria Bauza,Sheng Li,Yiqing Tao,Nevan Wichers,Andrii Maksai,Abe Ittycheriah,Ross Mcilroy,Bryan Seybold,Noah Goodman,Romina Datta,Steven M. Hernandez,Tian Shi,Yony Kochinski,Anna Bulanova,Ken Franko,Mikita Sazanovich,Nicholas FitzGerald,Praneeth Kacham,Shubha Srinivas Raghvendra,Vincent Hellendoorn,Alexander Grushetsky,Julian Salazar,Angeliki Lazaridou,Jason Chang,Jan-Thorsten Peter,Sushant Kafle,Yann Dauphin,Abhishek Rao,Filippo Graziano,Izhak Shafran,Yuguo Liao,Tianli Ding,Geng Yan,Grace Chu,Zhao Fu,Vincent Roulet,Gabriel Rasskin,Duncan Williams,Shahar Drath,Alex Mossin,Raphael Hoffmann,Jordi Orbay,Francesco Bertolini,Hila Sheftel,Justin Chiu,Siyang Xue,Yuheng Kuang,Ferjad Naeem,Swaroop Nath,Nana Nti,Phil Culliton,Kashyap Krishnakumar,Michael Isard,Pei Sun,Ayan Chakrabarti,Nathan Clement,Regev Cohen,Arissa Wongpanich,GS Oh,Ashwin Murthy,Hao Zheng,Jessica Hamrick,Oskar Bunyan,Suhas Ganesh,Nitish Gupta,Roy Frostig,John Wieting,Yury Malkov,Pierre Marcenac,Zhixin,Lai,Xiaodan Tang,Mohammad Saleh,Fedir Zubach,Chinmay Kulkarni,Huanjie Zhou,Vicky Zayats,Nan Ding,Anshuman Tripathi,Arijit Pramanik,Patrik Zochbauer,Harish Ganapathy,Vedant Misra,Zach Behrman,Hugo Vallet,Mingyang Zhang,Mukund Sridhar,Ye Jin,Mohammad Babaeizadeh,Siim Põder,Megha Goel,Divya Jain,Tajwar Nasir,Shubham Mittal,Tim Dozat,Diego Ardila,Aliaksei Severyn,Fabio Pardo,Sammy Jerome,Siyang Qin,Louis Rouillard,Amir Yazdanbakhsh,Zizhao Zhang,Shivani Agrawal,Kaushik Shivakumar,Caden Lu,Praveen Kallakuri,Rachita Chhaparia,Kanishka Rao,Charles Kwong,Asya Fadeeva,Shitij Nigam,Yan Virin,Yuan Zhang,Balaji Venkatraman,Beliz Gunel,Marc Wilson,Huiyu Wang,Abhinav Gupta,Xiaowei Xu,Adrien Ali Taïga,Kareem Mohamed,Doug Fritz,Daniel Rodriguez,Zoubin Ghahramani,Harry Askham,Lior Belenki,James Zhao,Rahul Gupta,Krzysztof Jastrzębski,Takahiro Kosakai,Kaan Katircioglu,Jon Schneider,Rina Panigrahy,Konstantinos Bousmalis,Peter Grabowski,Prajit Ramachandran,Chaitra Hegde,Mihaela Rosca,Angelo Scorza Scarpati,Kyriakos Axiotis,Ying Xu,Zach Gleicher,Assaf Hurwitz Michaely,Mandar Sharma,Sanil Jain,Christoph Hirnschall,Tal Marian,Xuhui Jia,Kevin Mather,Kilol Gupta,Linhai Qiu,Nigamaa Nayakanti,Lucian Ionita,Steven Zheng,Lucia Loher,Kurt Shuster,Igor Petrovski,Roshan Sharma,Rahma Chaabouni,Angel Yeh,James An,Arushi Gupta,Steven Schwarcz,Seher Ellis,Sam Conway-Rahman,Javier Snaider,Alex Zhai,James Atwood,Daniel Golovin,Liqian Peng,Te I,Vivian Xia,Salvatore Scellato,Mahan Malihi,Arthur Bražinskas,Vlad-Doru Ion,Younghoon Jun,James Swirhun,Soroosh Mariooryad,Jiao Sun,Steve Chien,Rey Coaguila,Ariel Brand,Yi Gao,Tom Kwiatkowski,Roee Aharoni,Cheng-Chun Lee,Mislav Žanić,Yichi Zhang,Dan Ethier,Vitaly Nikolaev,Pranav Nair,Yoav Ben Shalom,Hen Fitoussi,Jai Gupta,Hongbin Liu,Dee Cattle,Tolga Bolukbasi,Ben Murdoch,Fantine Huot,Yin Li,Chris Hahn*

Main category: cs.CL

TL;DR: 探讨了Gemini 2.X模型家族的特点与性能，包括Gemini 2.5 Pro和Flash系列模型，在多模态处理、长上下文应用及推理性能上表现优异。


<details>
  <summary>Details</summary>
Motivation: 设计出可在复杂决策和问题解决中表现优越的多模态模型，同时追求性能与成本的平衡。

Method: 提出了一系列模型版本（Gemini 2.5 Pro，Flash和Flash-Lite），每种版本在推理、多模态处理和计算效率等方面采用不同优化目标。

Result: Gemini 2.5 Pro在编码和推理基准测试中达到了最先进的性能，支持长达3小时的视频处理；Flash等版本在性能和计算开销上也表现出色，涵盖不同的应用需求。

Conclusion: Gemini 2.X模型家族成功在性能与成本之间达成Pareto前沿平衡，推动多模态推理与复杂智能代理任务的实现可能性。

Abstract: In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and
Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite
models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA
performance on frontier coding and reasoning benchmarks. In addition to its
incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that
excels at multimodal understanding and it is now able to process up to 3 hours
of video content. Its unique combination of long context, multimodal and
reasoning capabilities can be combined to unlock new agentic workflows. Gemini
2.5 Flash provides excellent reasoning abilities at a fraction of the compute
and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high
performance at low latency and cost. Taken together, the Gemini 2.X model
generation spans the full Pareto frontier of model capability vs cost, allowing
users to explore the boundaries of what is possible with complex agentic
problem solving.

</details>


### [87] [Humans overrely on overconfident language models, across languages](https://arxiv.org/abs/2507.06306)
*Neil Rathi,Dan Jurafsky,Kaitlyn Zhou*

Main category: cs.CL

TL;DR: 本研究分析了大型语言模型（LLMs）在多语言环境中的语言校准问题，指出其在不同语言下存在过度自信和用户过度依赖的风险。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs在全球范围内的部署，确保其在多语言环境中的回应准确传达不确定性和局限性至关重要，同时降低用户过度依赖的风险。

Method: 研究了五种语言中LLMs生成的语言标记的分布，比较了不同语言的用户依赖行为；分析了模型对语言差异的敏感性及其生成的不确定性和确定性标记。

Result: 发现所有语言用户均倾向于依赖自信的生成内容。模型在日语中倾向于更多生成不确定性标记，而在德语和普通话中生成的确定性标记最多。语言间的依赖行为差异显著，例如，用户在日语中更依赖表达的不确定性。

Conclusion: LLMs在多语言环境中存在高风险的过度依赖问题，指出了多语言语言校准的挑战，并强调了基于文化和语言情境的模型安全性评估的重要性。

Abstract: As large language models (LLMs) are deployed globally, it is crucial that
their responses are calibrated across languages to accurately convey
uncertainty and limitations. Previous work has shown that LLMs are
linguistically overconfident in English, leading users to overrely on confident
generations. However, the usage and interpretation of epistemic markers (e.g.,
'It's definitely,' 'I think') can differ sharply across languages. Here, we
study the risks of multilingual linguistic (mis)calibration, overconfidence,
and overreliance across five languages to evaluate the safety of LLMs in a
global context.
  We find that overreliance risks are high across all languages. We first
analyze the distribution of LLM-generated epistemic markers, and observe that
while LLMs are cross-linguistically overconfident, they are also sensitive to
documented linguistic variation. For example, models generate the most markers
of uncertainty in Japanese and the most markers of certainty in German and
Mandarin. We then measure human reliance rates across languages, finding that
while users strongly rely on confident LLM generations in all languages,
reliance behaviors differ cross-linguistically: for example, users rely
significantly more on expressions of uncertainty in Japanese than in English.
Taken together, these results indicate high risk of reliance on overconfident
model generations across languages. Our findings highlight the challenges of
multilingual linguistic calibration and stress the importance of culturally and
linguistically contextualized model safety evaluations.

</details>


### [88] [ETT: Expanding the Long Context Understanding Capability of LLMs at Test-Time](https://arxiv.org/abs/2507.06313)
*Kiarash Zahirnia,Zahra Golpayegani,Walid Ahmad,Yang Liu*

Main category: cs.CL

TL;DR: 提出了一种名为ETT的方法，用于在测试时扩展Transformer模型上下文长度，且具有恒定的内存需求和线性计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer语言模型在处理长序列时因计算和内存开销高增加而造成的挑战。

Method: 通过将输入上下文分割成小的重叠子序列进行高效微调，以在测试阶段扩展短上下文Transformer LLM的上下文长度，同时保持恒定的内存需求和线性计算成本。

Result: 模型在LongBench评估中，将上下文长度从1k扩展到32k标记，准确率提高了30%。

Conclusion: 微调FFN第二层比全面微调更有效，进一步提高了模型的准确性。

Abstract: Transformer-based Language Models' computation and memory overhead increase
quadratically as a function of sequence length. The quadratic cost poses
challenges when employing LLMs for processing long sequences. In this work, we
introduce \ourmodelacronym~(Extend at Test-Time), method for extending the
context length of short context Transformer-based LLMs, with constant memory
requirement and linear computation overhead. ETT enable the extension of the
context length at test-time by efficient fine-tuning the model's parameters on
the input context, chunked into overlapping small subsequences. We evaluate ETT
on LongBench by extending the context length of GPT-Large and Phi-2 up to 32
times, increasing from 1k to 32k tokens. This results in up to a 30 percent
improvement in the model's accuracy. We also study how context can be stored in
LLM's weights effectively and efficiently. Through a detailed ablation study,
we examine which Transformer modules are most beneficial to fine-tune at
test-time. Interestingly, we find that fine-tuning the second layer of the FFNs
is more effective than full fine-tuning, leading to a further improvement in
the models' accuracy.

</details>


### [89] [Could the Road to Grounded, Neuro-symbolic AI be Paved with Words-as-Classifiers?](https://arxiv.org/abs/2507.06335)
*Casey Kennington,David Schlangen*

Main category: cs.CL

TL;DR: 本文探讨了结合形式、分布式和语义基础理论的方法，通过‘词语即分类器’模型实现语义统一。


<details>
  <summary>Details</summary>
Motivation: 许多语言模型需要结合符号化方法和视觉知识，以在形式、分布式和语义基础理论中取得平衡。

Method: 作者回顾相关文献，以认知科学依据为动力支持‘词语即分类器’模式，并设计了一个小型实验进行探讨。

Result: ‘词语即分类器’模型在互动对话背景下表现良好，并且可以促进语义统一。

Conclusion: ‘词语即分类器’是一条实现三大语义理论统一的潜在路径。

Abstract: Formal, Distributional, and Grounded theories of computational semantics each
have their uses and their drawbacks. There has been a shift to ground models of
language by adding visual knowledge, and there has been a call to enrich models
of language with symbolic methods to gain the benefits from formal,
distributional, and grounded theories. In this paper, we attempt to make the
case that one potential path forward in unifying all three semantic fields is
paved with the words-as-classifier model, a model of word-level grounded
semantics that has been incorporated into formalisms and distributional
language models in the literature, and it has been well-tested within
interactive dialogue settings. We review that literature, motivate the
words-as-classifiers model with an appeal to recent work in cognitive science,
and describe a small experiment. Finally, we sketch a model of semantics
unified through words-as-classifiers.

</details>


### [90] [Evaluating Morphological Alignment of Tokenizers in 70 Languages](https://arxiv.org/abs/2507.06378)
*Catherine Arnett,Marisa Hudspeth,Brendan O'Connor*

Main category: cs.CL

TL;DR: 作者扩展MorphScore工具用于评估分词器质量，从22种语言扩展到70种语言, 并探讨其对模型性能的影响，但发现形态对齐与模型性能的相关性较低。


<details>
  <summary>Details</summary>
Motivation: 分词是语言建模的重要步骤，但如何有效评估分词器质量尚不清楚。研究希望通过形态学对齐质量来提高评估的科学性。

Method: 扩展原有的MorphScore评估工具，将覆盖范围从22种语言增加到70种。随后分析评估得分与五种预训练语言模型在七个下游任务上的性能之间的关系。

Result: 通过分析，发现形态对齐得分与模型性能的相关性较低，表明形态对齐无法完全反映分词器质量对模型性能的影响。

Conclusion: 形态学对齐分数并不能有效解释分词器质量对模型性能的影响，暗示模型性能依赖于更多维度的分词器质量。

Abstract: While tokenization is a key step in language modeling, with effects on model
training and performance, it remains unclear how to effectively evaluate
tokenizer quality. One proposed dimension of tokenizer quality is the extent to
which tokenizers preserve linguistically meaningful subwords, aligning token
boundaries with morphological boundaries within a word. We expand MorphScore
(Arnett & Bergen, 2025), which previously covered 22 languages, to support a
total of 70 languages. The updated MorphScore offers more flexibility in
evaluation and addresses some of the limitations of the original version. We
then correlate our alignment scores with downstream task performance for five
pre-trained languages models on seven tasks, with at least one task in each of
the languages in our sample. We find that morphological alignment does not
explain very much variance in model performance, suggesting that morphological
alignment alone does not measure dimensions of tokenization quality relevant to
model performance.

</details>


### [91] [Hypermagmas and Colored Operads: Heads, Phases, and Theta Roles](https://arxiv.org/abs/2507.06393)
*Matilde Marcolli,Riny Huijbregts,Richard K. Larson*

Main category: cs.CL

TL;DR: 本文提出了一种基于超幺半群和有色操作代数的新方法，以解释语法结构中的各种关系和规则。


<details>
  <summary>Details</summary>
Motivation: 研究语法结构如何通过超幺半群和有色操作代数进行统一描述。

Method: 将头部功能扩展为超幺半群结构，并利用有色操作代数和过滤规则解释语法现象。

Result: 语法结构的各种规则，如内部合并、移动规则等，可以统一纳入有色操作代数的框架内。

Conclusion: 通过有色操作代数与超幺半群的结合，提供了一种统一描述语法规则的新理论框架。

Abstract: We show that head functions on syntactic objects extend the magma structure
to a hypermagma, with the c-command relation compatible with the magma
operation and the m-command relation with the hypermagma. We then show that the
structure of head and complement and specifier, additional modifier positions,
and the structure of phases in the Extended Projection can be formulated as a
bud generating system of a colored operad, in a form similar to the structure
of theta roles. We also show that, due to the special form of the colored
operad generators, the filtering of freely generated syntactic objects by these
coloring rules can be equivalently formulated as a filtering in the course of
structure formation via a colored Merge, which can in turn be related to the
hypermagma structure. The rules on movement by Internal Merge with respect to
phases, the Extended Projection Principle, Empty Category Principle, and Phase
Impenetrability Condition are all subsumed into the form of the colored operad
generators. Movement compatibilities between the phase structure and the theta
roles assignments can then be formulated in terms of the respective colored
operads and a transduction of colored operads.

</details>


### [92] [PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning](https://arxiv.org/abs/2507.06415)
*Zeming Chen,Angelika Romanou,Gail Weiss,Antoine Bosselut*

Main category: cs.CL

TL;DR: 本文提出了一种名为PERK的参数高效记忆模块，能够在测试时通过轻量级模型适配器进行长上下文推理。


<details>
  <summary>Details</summary>
Motivation: 长上下文推理需要从冗长且噪声较多的输入中准确提取相关信息，以此为基础提供高效、易扩展的推理方法。

Method: 提出基于LoRA的双重优化循环方法：内环快速将上下文编码为低秩适配器，外环学习利用更新后的适配器从长上下文中推理关键信息。

Result: PERK相比传统的基于prompt的推理方式有显著优势，小模型（GPT-2）性能提升最高可达90%，大模型（Qwen-2.5-0.5B）提升达27%。

Conclusion: PERK在推理复杂性、长度外推及上下文信息定位上更具鲁棒性，且在推理阶段较节省内存，具有实际应用潜力。

Abstract: Long-context reasoning requires accurately identifying relevant information
in extensive, noisy input contexts. Previous research shows that using
test-time learning to encode context directly into model parameters can
effectively enable reasoning over noisy information. However, meta-learning
methods for enabling test-time learning are prohibitively memory-intensive,
preventing their application to long context settings. In this work, we propose
PERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for
learning to encode long input contexts using gradient updates to a lightweight
model adapter at test time. Specifically, PERK employs two nested optimization
loops in a meta-training phase. The inner loop rapidly encodes contexts into a
low-rank adapter (LoRA) that serves as a parameter-efficient memory module for
the base model. Concurrently, the outer loop learns to use the updated adapter
to accurately recall and reason over relevant information from the encoded long
context. Our evaluations on several long-context reasoning tasks show that PERK
significantly outperforms the standard prompt-based long-context baseline,
achieving average absolute performance gains of up to 90% for smaller models
(GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In
general, PERK is more robust to reasoning complexity, length extrapolation, and
the locations of relevant information in contexts. Finally, we show that while
PERK is memory-intensive during training, it scales more efficiently at
inference time than prompt-based long-context inference.

</details>


### [93] [Reward Models Can Improve Themselves: Reward-Guided Adversarial Failure Mode Discovery for Robust Reward Modeling](https://arxiv.org/abs/2507.06419)
*Pankayaraj Pathmanathan,Furong Huang*

Main category: cs.CL

TL;DR: 提出了一种名为REFORM的自我改进框架，用于通过奖励指导生成错误响应来增强奖励模型的鲁棒性，并在两组数据集上证明了其实用性。


<details>
  <summary>Details</summary>
Motivation: 奖励模型在处理人类偏好对齐任务时容易因分布变化或对抗性扰动失败，因此需要一种不用依赖偏好分布或失败属性知识的鲁棒性改进方法。

Method: 引入了奖励引导的控制解码方法，同时设计了REFORM框架，通过奖励模型生成对抗性示例以扩充训练数据并修正模型行为。

Result: REFORM在HH和PKU Beavertails数据集上提高了奖励模型的鲁棒性，保留了直接评估和下游训练的性能，并改进了对齐质量。

Conclusion: REFORM框架在增强奖励模型鲁棒性和实现更高质量对齐的同时，不依赖于先验知识，可用于实际设定。

Abstract: Reward modeling (RM), which captures human preferences to align large
language models (LLMs), is increasingly employed in tasks such as model
finetuning, response filtering, and ranking. However, due to the inherent
complexity of human preferences and the limited coverage of available datasets,
reward models often fail under distributional shifts or adversarial
perturbations. Existing approaches for identifying such failure modes typically
rely on prior knowledge about preference distributions or failure attributes,
limiting their practicality in real-world settings where such information is
unavailable. In this work, we propose a tractable, preference-distribution
agnostic method for discovering reward model failure modes via reward guided
controlled decoding. Building on this, we introduce REFORM, a self-improving
reward modeling framework that enhances robustness by using the reward model
itself to guide the generation of falsely scored responses. These adversarial
examples are then used to augment the training data and patch the reward
model's misaligned behavior. We evaluate REFORM on two widely used preference
datasets Anthropic Helpful Harmless (HH) and PKU Beavertails and demonstrate
that it significantly improves robustness without sacrificing reward quality.
Notably, REFORM preserves performance both in direct evaluation and in
downstream policy training, and further improves alignment quality by removing
spurious correlations.

</details>


### [94] [Exploring Task Performance with Interpretable Models via Sparse Auto-Encoders](https://arxiv.org/abs/2507.06427)
*Shun Wang,Tyler Loakman,Youbo Lei,Yi Liu,Bohao Yang,Yuting Zhao,Dong Yang,Chenghua Lin*

Main category: cs.CL

TL;DR: 研究通过字典学习和稀疏自编码器分解LLM，提取单义特征，提高下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型（LLMs）被视为黑盒，降低了可信性，也难以提升性能。

Method: 采用字典学习与稀疏自编码器对LLM进行分解，从多义神经元中提取单义特征。

Result: 改进了模型解释，自动重新调整提示，显著提高数学推理和隐喻检测等下游任务性能。

Conclusion: 提出的分解方法不仅提高了LLM透明性，也改善了其性能，有助于未来研究和应用。

Abstract: Large Language Models (LLMs) are traditionally viewed as black-box
algorithms, therefore reducing trustworthiness and obscuring potential
approaches to increasing performance on downstream tasks. In this work, we
apply an effective LLM decomposition method using a dictionary-learning
approach with sparse autoencoders. This helps extract monosemantic features
from polysemantic LLM neurons. Remarkably, our work identifies model-internal
misunderstanding, allowing the automatic reformulation of the prompts with
additional annotations to improve the interpretation by LLMs. Moreover, this
approach demonstrates a significant performance improvement in downstream
tasks, such as mathematical reasoning and metaphor detection.

</details>


### [95] [Temporal Analysis of Climate Policy Discourse: Insights from Dynamic Embedded Topic Modeling](https://arxiv.org/abs/2507.06435)
*Rafiu Adekoya Badekale,Adewale Akinfaderin*

Main category: cs.CL

TL;DR: 本文探讨了动态嵌入主题模型（DETM）在分析全球气候政策话语演变中的应用，发现此方法可以有效捕捉其话语动态变化。


<details>
  <summary>Details</summary>
Motivation: 现有针对全球性挑战的政策话语分析方法耗时且难以捕捉复杂的相互关系，亟需更高效的分析方法。

Method: 采用动态嵌入主题模型（DETM），分析1995至2023年间联合国气候变化框架公约的政策决策文本，排除2020年数据，并展示了数据预处理、模型训练和话语变化的可视化过程。

Result: 结果揭示了政策话语从早期重点的温室气体和国际公约讨论，逐步转向执行、技术合作、能力建设、资金支持和全球协议等主题。

Conclusion: DETM模型是分析政策话语演变的有效工具，未来可将其扩展至其他政策领域。

Abstract: Understanding how policy language evolves over time is critical for assessing
global responses to complex challenges such as climate change. Temporal
analysis helps stakeholders, including policymakers and researchers, to
evaluate past priorities, identify emerging themes, design governance
strategies, and develop mitigation measures. Traditional approaches, such as
manual thematic coding, are time-consuming and limited in capturing the
complex, interconnected nature of global policy discourse. With the increasing
relevance of unsupervised machine learning, these limitations can be addressed,
particularly under high-volume, complex, and high-dimensional data conditions.
In this work, we explore a novel approach that applies the dynamic embedded
topic model (DETM) to analyze the evolution of global climate policy discourse.
A probabilistic model designed to capture the temporal dynamics of topics over
time. We collected a corpus of United Nations Framework Convention on Climate
Change (UNFCCC) policy decisions from 1995 to 2023, excluding 2020 due to the
postponement of COP26 as a result of the COVID-19 pandemic. The model reveals
shifts from early emphases on greenhouse gases and international conventions to
recent focuses on implementation, technical collaboration, capacity building,
finance, and global agreements. Section 3 presents the modeling pipeline,
including preprocessing, model training, and visualization of temporal word
distributions. Our results show that DETM is a scalable and effective tool for
analyzing the evolution of global policy discourse. Section 4 discusses the
implications of these findings and we concluded with future directions and
refinements to extend this approach to other policy domains.

</details>


### [96] [Perception-Aware Policy Optimization for Multimodal Reasoning](https://arxiv.org/abs/2507.06448)
*Zhenhailong Wang,Xuehang Guo,Sofia Stoica,Haiyang Xu,Hongru Wang,Hyeonjeong Ha,Xiusi Chen,Yangyi Chen,Ming Yan,Fei Huang,Heng Ji*

Main category: cs.CL

TL;DR: 提出了一种改进GRPO的策略优化方法PAPO，增强大语言模型在多模态推理中的表现，尤其对高视觉依赖任务有显著提升。


<details>
  <summary>Details</summary>
Motivation: 通过观察，发现当前多模态推理的主要问题是对视觉输入的感知能力较差，亟需一种方法改进感知能力。

Method: 提出了感知感知政策优化（PAPO），在GRPO目标中引入隐式感知损失作为KL散度项，并结合双熵损失解决潜在的损失黑客问题。

Result: 在多模态基准测试中提升总体性能约4.4%，对高视觉依赖任务的性能提升接近8.0%，且感知错误减少约30.5%。

Conclusion: 证明感知感知监督可有效融入RLVR，奠定了鼓励视觉推理的新型强化学习框架基础。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a
highly effective strategy for endowing Large Language Models (LLMs) with robust
multi-step reasoning abilities. However, its design and optimizations remain
tailored to purely textual domains, resulting in suboptimal performance when
applied to multimodal reasoning tasks. In particular, we observe that a major
source of error in current multimodal reasoning lies in the perception of
visual inputs. To address this bottleneck, we propose Perception-Aware Policy
Optimization (PAPO), a simple yet effective extension of GRPO that encourages
the model to learn to perceive while learning to reason, entirely from internal
supervision signals. Notably, PAPO does not rely on additional data curation,
external reward models, or proprietary models. Specifically, we introduce the
Implicit Perception Loss in the form of a KL divergence term to the GRPO
objective, which, despite its simplicity, yields significant overall
improvements (4.4%) on diverse multimodal benchmarks. The improvements are more
pronounced, approaching 8.0%, on tasks with high vision dependency. We also
observe a substantial reduction (30.5%) in perception errors, indicating
improved perceptual capabilities with PAPO. We conduct comprehensive analysis
of PAPO and identify a unique loss hacking issue, which we rigorously analyze
and mitigate through a Double Entropy Loss. Overall, our work introduces a
deeper integration of perception-aware supervision into RLVR learning
objectives and lays the groundwork for a new RL framework that encourages
visually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO.

</details>


### [97] [A Semantic Parsing Framework for End-to-End Time Normalization](https://arxiv.org/abs/2507.06450)
*Xin Su,Sungduk Yu,Phillip Howard,Steven Bethard*

Main category: cs.CL

TL;DR: 时间归一化任务将自然语言中的时间表达转换为机器可读形式。本文提出基于SCATE框架的新方法，实现了可执行的SCATE Python库并通过LLM生成代码，结果表明小型模型也能实现高效准确的归一化。


<details>
  <summary>Details</summary>
Motivation: 解决传统ISO-TimeML方法难以处理复杂时间表达的问题，例如组合型、事件相关型和跨范围时间表达。

Method: 提出一种基于SCATE框架的时间归一化方法，通过LLM生成可执行的Python代码，并进行了数据增强以训练更小的模型。

Result: 实验显示，小型、本地化的模型在使用增强数据训练后表现优异，甚至超过了自身的LLM模型。

Conclusion: 新方法实现了高效准确且易于解释的时间归一化，为信息检索、问答和临床决策等应用提供了支持。

Abstract: Time normalization is the task of converting natural language temporal
expressions into machine-readable representations. It underpins many downstream
applications in information retrieval, question answering, and clinical
decision-making. Traditional systems based on the ISO-TimeML schema limit
expressivity and struggle with complex constructs such as compositional,
event-relative, and multi-span time expressions. In this work, we introduce a
novel formulation of time normalization as a code generation task grounded in
the SCATE framework, which defines temporal semantics through symbolic and
compositional operators. We implement a fully executable SCATE Python library
and demonstrate that large language models (LLMs) can generate executable SCATE
code. Leveraging this capability, we develop an automatic data augmentation
pipeline using LLMs to synthesize large-scale annotated data with code-level
validation. Our experiments show that small, locally deployable models trained
on this augmented data can achieve strong performance, outperforming even their
LLM parents and enabling practical, accurate, and interpretable time
normalization.

</details>


### [98] [A Systematic Analysis of Hybrid Linear Attention](https://arxiv.org/abs/2507.06457)
*Dustin Wang,Rui-Jie Zhu,Steven Abreu,Yong Shan,Taylor Kergan,Yuqi Pan,Yuhong Chou,Zheng Li,Ge Zhang,Wenhao Huang,Jason Eshraghian*

Main category: cs.CL

TL;DR: 本文结合全注意力和线性注意力层，解决长序列的Transformer计算复杂度问题。测试72个模型，分析线性注意力机制表现和混合比率对语言建模与回忆任务的影响。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer因长序列带来的计算和内存开销问题，驱动研究线性注意力机制；但线性注意力回忆性能往往较低，因此探索混合架构的有效性变得重要。

Method: 文章系统性评估六种线性注意力模型，在不同参数规模和混合比例下，运行语言建模和回忆任务。

Result: 优秀的单一线性模式在混合模型中未必表现最佳；线性路径的语言建模相对稳定，而回忆任务在3:1以下的全注意力比例改善显著。

Conclusion: 通过混合架构结合线性与全注意力模型，推荐HGRN-2或GatedDeltaNet，并选取3:1至6:1的线性替代比例可高效达到Transformer水平回忆表现。

Abstract: Transformers face quadratic complexity and memory issues with long sequences,
prompting the adoption of linear attention mechanisms using fixed-size hidden
states. However, linear models often suffer from limited recall performance,
leading to hybrid architectures that combine linear and full attention layers.
Despite extensive hybrid architecture research, the choice of linear attention
component has not been deeply explored. We systematically evaluate various
linear attention models across generations - vector recurrences to advanced
gating mechanisms - both standalone and hybridized. To enable this
comprehensive analysis, we trained and open-sourced 72 models: 36 at 340M
parameters (20B tokens) and 36 at 1.3B parameters (100B tokens), covering six
linear attention variants across five hybridization ratios. Benchmarking on
standard language modeling and recall tasks reveals that superior standalone
linear models do not necessarily excel in hybrids. While language modeling
remains stable across linear-to-full attention ratios, recall significantly
improves with increased full attention layers, particularly below a 3:1 ratio.
Our study highlights selective gating, hierarchical recurrence, and controlled
forgetting as critical for effective hybrid models. We recommend architectures
such as HGRN-2 or GatedDeltaNet with a linear-to-full ratio between 3:1 and 6:1
to achieve Transformer-level recall efficiently. Our models are open-sourced at
https://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e.

</details>


### [99] [On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks](https://arxiv.org/abs/2507.06489)
*Stephen Obadinma,Xiaodan Zhu*

Main category: cs.CL

TL;DR: 该论文研究了大语言模型（LLMs）在对抗性攻击下的信心表达的鲁棒性，提出新的攻击框架并揭示了现有方法的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高风险应用场景中的部署需保证透明性、信任与安全性，而可靠的语言信心表达对此至关重要。

Method: 提出了一种新的框架，通过扰动和越狱方法攻击语言信心分数，并测试了不同的提示策略、模型规模及应用场景。

Result: 发现当前的信心提取方法容易受到对抗性攻击，常用防御技术无效甚至适得其反。

Conclusion: 现有机制需要改进设计，因小幅的语义保留修改即可导致误导性的信心表达。

Abstract: Robust verbal confidence generated by large language models (LLMs) is crucial
for the deployment of LLMs to ensure transparency, trust, and safety in
human-AI interactions across many high-stakes applications. In this paper, we
present the first comprehensive study on the robustness of verbal confidence
under adversarial attacks. We introduce a novel framework for attacking verbal
confidence scores through both perturbation and jailbreak-based methods, and
show that these attacks can significantly jeopardize verbal confidence
estimates and lead to frequent answer changes. We examine a variety of
prompting strategies, model sizes, and application domains, revealing that
current confidence elicitation methods are vulnerable and that commonly used
defence techniques are largely ineffective or counterproductive. Our findings
underscore the urgent need to design more robust mechanisms for confidence
expression in LLMs, as even subtle semantic-preserving modifications can lead
to misleading confidence in responses.

</details>


### [100] [Pun Intended: Multi-Agent Translation of Wordplay with Contrastive Learning and Phonetic-Semantic Embeddings](https://arxiv.org/abs/2507.06506)
*Russell Taylor,Benjamin Herbert,Michael Sana*

Main category: cs.CL

TL;DR: 本研究提出了一种结合最先进语言模型和语言游戏生成技术的方法，将英语中的双关语翻译成法语，并成功应用在CLEF JOKER 2025比赛中。


<details>
  <summary>Details</summary>
Motivation: 为了解决单纯词汇翻译无法捕捉源语言文本创造性和幽默的问题，提出了一种新方法处理双关语翻译的复杂性。

Method: 采用三阶段方法：1) 基于新的对比学习数据集，建立多种语言模型基线；2) 实施结合语义-语音嵌入的引导式链式思维流程；3) 使用多代理生成-判别框架对双关语进行评估与再生。

Result: 提出的方法不仅超越字面翻译，成功捕捉语言创造性和幽默，且获得CLEF JOKER 2025竞赛手动评估前两名。

Conclusion: 通过引入语言学知情的翻译技术，该研究填补了翻译学与计算语言学间的空白，表明语言模型可有效处理语义模糊、语音相似度及文化语言的复杂性。

Abstract: Translating wordplay across languages presents unique challenges that have
long confounded both professional human translators and machine translation
systems. This research proposes a novel approach for translating puns from
English to French by combining state-of-the-art large language models with
specialized techniques for wordplay generation.
  Our methodology employs a three-stage approach. First, we establish a
baseline using multiple frontier large language models with feedback based on a
new contrastive learning dataset. Second, we implement a guided
chain-of-thought pipeline with combined phonetic-semantic embeddings. Third, we
implement a multi-agent generator-discriminator framework for evaluating and
regenerating puns with feedback.
  Moving beyond the limitations of literal translation, our methodology's
primary objective is to capture the linguistic creativity and humor of the
source text wordplay, rather than simply duplicating its vocabulary. Our best
runs earned first and second place in the CLEF JOKER 2025 Task 2 competition
where they were evaluated manually by expert native French speakers.
  This research addresses a gap between translation studies and computational
linguistics by implementing linguistically-informed techniques for wordplay
translation, advancing our understanding of how language models can be
leveraged to handle the complex interplay between semantic ambiguity, phonetic
similarity, and the implicit cultural and linguistic awareness needed for
successful humor.

</details>


### [101] [SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers](https://arxiv.org/abs/2507.06517)
*Zicong Tang,Shi Luohe,Zuchao Li,Baoyuan Qi,Guoming Liu,Lefei Zhang,Ping Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的KV缓存减少方法SpindleKV，用于平衡浅层和深层缓存的削减，同时解决了其他方法面临的GQA问题。


<details>
  <summary>Details</summary>
Motivation: KV缓存的增加导致推理系统内存消耗上升，需通过减少缓存来解决此挑战。

Method: 深层：基于注意力权重的移除方法；浅层：基于相似度和合并策略的码书替换方法。

Result: 实验表明，与基线方法相比，SpindleKV提高了缓存减少效果，同时保持甚至提升了模型性能。

Conclusion: SpindleKV在缓存减少与模型性能之间达到了更好的平衡，具有重要价值。

Abstract: Large Language Models (LLMs) have achieved impressive accomplishments in
recent years. However, the increasing memory consumption of KV cache has
possessed a significant challenge to the inference system. Eviction methods
have revealed the inherent redundancy within the KV cache, demonstrating its
potential for reduction, particularly in deeper layers. However, KV cache
reduction for shallower layers has been found to be insufficient. Based on our
observation that, the KV cache exhibits a high degree of similarity. Based on
this observation, we proposed a novel KV cache reduction method, SpindleKV,
which balances both shallow and deep layers. For deep layers, we employ an
attention weight based eviction method, while for shallow layers, we apply a
codebook based replacement approach which is learnt by similarity and merging
policy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma
faced by other attention based eviction methods. Experiments on two common
benchmarks with three different LLMs shown that SpindleKV obtained better KV
cache reduction effect compared to baseline methods, while preserving similar
or even better model performance.

</details>


### [102] [InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes under Herd Behavior](https://arxiv.org/abs/2507.06528)
*Huisheng Wang,Zhuoshi Pan,Hangjing Zhang,Mingxiao Liu,Hanqing Gao,H. Vicky Zhao*

Main category: cs.CL

TL;DR: 提出了InvestAlign框架，通过理论化的投资问题生成高质量训练数据，以提升大语言模型（LLM）在投资决策中的对齐能力。


<details>
  <summary>Details</summary>
Motivation: 解决行为金融学中投资决策的LLM对齐问题，尤其在面对数据稀缺、大规模数据收集成本和隐私风险时的挑战。

Method: 提出InvestAlign框架，利用理论化投资问题生成高质量训练数据，并开发了InvestAgent模型，基于这些数据进行微调。

Result: InvestAgent在简单与复杂投资问题中的表现较未调整模型更接近真实用户数据，并显示出更高的学习效率。

Conclusion: InvestAlign框架能够在复杂投资决策场景下有效对齐LLM与投资者决策行为，具有广泛应用潜力。

Abstract: Aligning Large Language Models (LLMs) with investor decision-making processes
under herd behavior is a critical challenge in behavioral finance, which
grapples with a fundamental limitation: the scarcity of real-user data needed
for Supervised Fine-Tuning (SFT). While SFT can bridge the gap between LLM
outputs and human behavioral patterns, its reliance on massive authentic data
imposes substantial collection costs and privacy risks. We propose InvestAlign,
a novel framework that constructs high-quality SFT datasets by leveraging
theoretical solutions to similar and simple optimal investment problems rather
than complex scenarios. Our theoretical analysis demonstrates that training
LLMs with InvestAlign-generated data achieves faster parameter convergence than
using real-user data, suggesting superior learning efficiency. Furthermore, we
develop InvestAgent, an LLM agent fine-tuned with InvestAlign, which
demonstrates significantly closer alignment to real-user data than pre-SFT
models in both simple and complex investment problems. This highlights our
proposed InvestAlign as a promising approach with the potential to address
complex optimal investment problems and align LLMs with investor
decision-making processes under herd behavior. Our code is publicly available
at https://github.com/thu-social-network-research-group/InvestAlign.

</details>


### [103] [Large Language Model for Extracting Complex Contract Information in Industrial Scenes](https://arxiv.org/abs/2507.06539)
*Yunyang Cao,Yanjun Li,Silong Dai*

Main category: cs.CL

TL;DR: 本文提出了一种用于工业场景中复杂合同信息提取任务的高质量数据集构建方法，并基于此数据集微调大规模语言模型。实验结果表明，模型在场景召回率、准确率和解析效率上表现出色。


<details>
  <summary>Details</summary>
Motivation: 希望解决工业场景中的合同信息提取问题，提高模型的准确性和鲁棒性，并提供高效的解决方案。

Method: 通过聚类分析合同文本，利用GPT-4和GPT-3.5提取关键信息，获取高质量标注；通过构造新文本和关键词组合生成非结构化合同文本进行数据增强；微调大规模语言模型。

Result: 模型在保证高召回率和精度的同时，拥有优秀的整体性能，并兼顾了解析效率。LoRA、数据平衡和数据增强显著提高模型的准确性和鲁棒性。

Conclusion: 该方法为工业合同信息提取任务提供了新颖高效的解决方案，并在实验中表现优良。

Abstract: This paper proposes a high-quality dataset construction method for complex
contract information extraction tasks in industrial scenarios and fine-tunes a
large language model based on this dataset. Firstly, cluster analysis is
performed on industrial contract texts, and GPT-4 and GPT-3.5 are used to
extract key information from the original contract data, obtaining high-quality
data annotations. Secondly, data augmentation is achieved by constructing new
texts, and GPT-3.5 generates unstructured contract texts from randomly combined
keywords, improving model robustness. Finally, the large language model is
fine-tuned based on the high-quality dataset. Experimental results show that
the model achieves excellent overall performance while ensuring high field
recall and precision and considering parsing efficiency. LoRA, data balancing,
and data augmentation effectively enhance model accuracy and robustness. The
proposed method provides a novel and efficient solution for industrial contract
information extraction tasks.

</details>


### [104] [The Flaws of Others: An LLM-driven Framework for Scientific Knowledge Production](https://arxiv.org/abs/2507.06565)
*Juan B. Gutiérrez*

Main category: cs.CL

TL;DR: 文章提出了一个话语网络模型，将人类和大型语言模型视为平等节点，以追踪其声明的流动，并探索无效性问题及其危害，通过数学建模展示解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究如何在人与大型语言模型（LLM）交互中，通过网络建模的方法评估与提升可靠性。

Method: 提出了一种通用的话语网络数学模型，并开发了一个名为FOO算法的开源工具，用于在节点间实现互相批评及协调结果。

Result: 发现无效性问题源于事实漂移、自我修复、新生成及外部检测，并提出通过网络化的节点交互可以转变系统的真实性主导状态。

Conclusion: 提升新型交互媒体可靠性的方法是利用多个不完美模型的网络化协作，而非单一模型的完美性能。

Abstract: Large-language models turn writing into a live exchange between humans and
software. We capture this new medium with a discursive-network model that
treats people and LLMs as equal nodes and tracks how their statements
circulate. Broadening the focus from isolated hallucinations, we define
invalidation (any factual, logical, or structural breach) and show it follows
four hazards: drift from truth, self-repair, fresh fabrication, and external
detection. A general mathematical model of discursive networks is developed to
provide valuable insights: A network governed only by drift and self-repair
stabilizes at a modest error rate; adding fabrication reproduces the high rates
seen in current LLMs. Giving each false claim even a small chance of peer
review shifts the system to a truth-dominant state. We operationalize peer
review with the open-source \emph{Flaws-of-Others (FOO) algorithm}: a
configurable loop in which any set of agents critique one another while a
harmoniser merges their verdicts. The takeaway is practical and cultural:
reliability in this new medium comes not from perfecting single models but from
wiring imperfect ones into networks that keep each other honest.

</details>


### [105] [Enhancing Food-Domain Question Answering with a Multimodal Knowledge Graph: Hybrid QA Generation and Diversity Analysis](https://arxiv.org/abs/2507.06571)
*Srihari K B,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 提出了一个结合多模态知识图谱和生成式AI的食品领域问答框架，通过LLaVA/DeepSeek生成了4万对问答数据，并大幅提升了生成质量和准确性。


<details>
  <summary>Details</summary>
Motivation: 在食品领域的问答中，现有方法难以同时优化信息的可靠性和多样性，因此需要一种结合结构化知识和生成式AI的解决方案。

Method: 使用包含13,000道菜谱、3,000种食材和14,000张图片的大规模多模态知识图谱，结合LLaVA/DeepSeek生成问答训练数据，并通过联合微调Meta LLaMA与Stable Diffusion模型；诊断分析包括CLIP模式匹配检测和LLaVA幻觉检查。

Result: BERTScore提高16.2%，FID降低37.8%，CLIP对齐性提升31.1%，图像复用准确性达到94.1%，生成的充分性为85%。

Conclusion: 结构化知识与多模态生成的结合显著提高了食品问答的可靠性和多样性。

Abstract: We propose a unified food-domain QA framework that combines a large-scale
multimodal knowledge graph (MMKG) with generative AI. Our MMKG links 13,000
recipes, 3,000 ingredients, 140,000 relations, and 14,000 images. We generate
40,000 QA pairs using 40 templates and LLaVA/DeepSeek augmentation. Joint
fine-tuning of Meta LLaMA 3.1-8B and Stable Diffusion 3.5-Large improves
BERTScore by 16.2\%, reduces FID by 37.8\%, and boosts CLIP alignment by
31.1\%. Diagnostic analyses-CLIP-based mismatch detection (35.2\% to 7.3\%) and
LLaVA-driven hallucination checks-ensure factual and visual fidelity. A hybrid
retrieval-generation strategy achieves 94.1\% accurate image reuse and 85\%
adequacy in synthesis. Our results demonstrate that structured knowledge and
multimodal generation together enhance reliability and diversity in food QA.

</details>


### [106] [Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long Generation](https://arxiv.org/abs/2507.06607)
*Liliang Ren,Congcong Chen,Haoran Xu,Young Jin Kim,Adam Atkinson,Zheng Zhan,Jiankai Sun,Baolin Peng,Liyuan Liu,Shuohang Wang,Hao Cheng,Jianfeng Gao,Weizhu Chen,Yelong Shen*

Main category: cs.CL

TL;DR: 该研究引入了一种称为Gated Memory Unit (GMU) 的机制，用于跨层高效内存共享，并开发出一种新型模型SambaY，提高了解码效率和长上下文性能，同时简化了位置编码需求。


<details>
  <summary>Details</summary>
Motivation: 现有的混合架构在序列建模中虽有提升，但尚未探索在层间共享表示的效率潜力。

Method: 提出了Gated Memory Unit (GMU) 机制，通过与Samba架构结合，设计了一种新型decoder-hybrid-decoder架构SambaY，用于共享内存读出状态，并增强解码效率。

Result: SambaY 在长上下文任务中表现优异，缩短了不可降低的损失，并在推理任务中显示出卓越的性能力。

Conclusion: SambaY 很好地平衡了高解码性能和效率，展示了在大规模计算下的可扩展性，其优秀表现表明在未来可能成为一种更高效的序列建模范式。

Abstract: Recent advances in language modeling have demonstrated the effectiveness of
State Space Models (SSMs) for efficient sequence modeling. While hybrid
architectures such as Samba and the decoder-decoder architecture, YOCO, have
shown promising performance gains over Transformers, prior works have not
investigated the efficiency potential of representation sharing between SSM
layers. In this paper, we introduce the Gated Memory Unit (GMU), a simple yet
effective mechanism for efficient memory sharing across layers. We apply it to
create SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in
the cross-decoder to share memory readout states from a Samba-based
self-decoder. SambaY significantly enhances decoding efficiency, preserves
linear pre-filling time complexity, and boosts long-context performance, all
while eliminating the need for explicit positional encoding. Through extensive
scaling experiments, we demonstrate that our model exhibits a significantly
lower irreducible loss compared to a strong YOCO baseline, indicating superior
performance scalability under large-scale compute regimes. Our largest model
enhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves
significantly better performance than Phi4-mini-Reasoning on reasoning tasks
such as Math500, AIME24/25, and GPQA Diamond without any reinforcement
learning, while delivering up to 10x higher decoding throughput on 2K-length
prompts with 32K generation length under the vLLM inference framework. We
release our training codebase on open-source data at
https://github.com/microsoft/ArchScale.

</details>


### [107] [FuDoBa: Fusing Document and Knowledge Graph-based Representations with Bayesian Optimisation](https://arxiv.org/abs/2507.06622)
*Boshko Koloski,Senja Pollak,Roberto Navigli,Blaž Škrlj*

Main category: cs.CL

TL;DR: 该论文提出FuDoBa方法，通过贝叶斯优化将LLM嵌入与领域知识融合，生成低维、高效的文档表示，同时降低训练复杂性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM嵌入在领域特定任务中效率和表现较差。

Method: 使用贝叶斯优化，将LLM生成的嵌入与领域结构化知识（包括本地与外部知识如WikiData）融合，生成解释性早融合权重和任务相关的表示。

Result: 在六个数据集两个领域的实验中，FuDoBa的表现与或优于仅使用LLM嵌入的基线。

Conclusion: FuDoBa在生成低维任务相关表示的同时，提升分类性能，简化计算复杂性，证明是高效的领域知识集成方法。

Abstract: Building on the success of Large Language Models (LLMs), LLM-based
representations have dominated the document representation landscape, achieving
great performance on the document embedding benchmarks. However, the
high-dimensional, computationally expensive embeddings from LLMs tend to be
either too generic or inefficient for domain-specific applications. To address
these limitations, we introduce FuDoBa a Bayesian optimisation-based method
that integrates LLM-based embeddings with domain-specific structured knowledge,
sourced both locally and from external repositories like WikiData. This fusion
produces low-dimensional, task-relevant representations while reducing training
complexity and yielding interpretable early-fusion weights for enhanced
classification performance. We demonstrate the effectiveness of our approach on
six datasets in two domains, showing that when paired with robust AutoML-based
classifiers, our proposed representation learning approach performs on par
with, or surpasses, those produced solely by the proprietary LLM-based
embedding baselines.

</details>


### [108] [Expediting data extraction using a large language model (LLM) and scoping review protocol: a methodological study within a complex scoping review](https://arxiv.org/abs/2507.06623)
*James Stewart-Evans,Emma Wilson,Tessa Langley,Andrew Prayle,Angela Hands,Karen Exley,Jo Leonardi-Bee*

Main category: cs.CL

TL;DR: 研究测试了Claude 3.5 Sonnet在基于协议的数据提取中的表现，结果显示对简单数据的提取准确性高，但对复杂数据表现欠佳。需要更全面的性能评估并与传统方法对比。


<details>
  <summary>Details</summary>
Motivation: 数据提取阶段耗时，研究者尝试通过大语言模型（LLM）和审查协议来加速数据提取过程。

Method: 利用Claude 3.5 Sonnet测试了两种基于协议的数据提取方法，并评估其提取10个证据来源数据的表现，包含简单和复杂数据项。

Result: 对于简单、定义明确的数据，准确性高达83.3%到100%；但对于复杂、主观的数据，准确性仅为9.6%到15.8%。总的来说，精确率>90%，但召回率<25%，F1分数<40%。

Conclusion: 现有方法对复杂数据的表现受限，建议研究者在报告和评估LLM的性能时进行更全面的测试，并结合传统方法进行对比。此外，LLM建议在协议草拟中具有一定价值。

Abstract: The data extraction stages of reviews are resource-intensive, and researchers
may seek to expediate data extraction using online (large language models) LLMs
and review protocols. Claude 3.5 Sonnet was used to trial two approaches that
used a review protocol to prompt data extraction from 10 evidence sources
included in a case study scoping review. A protocol-based approach was also
used to review extracted data. Limited performance evaluation was undertaken
which found high accuracy for the two extraction approaches (83.3% and 100%)
when extracting simple, well-defined citation details; accuracy was lower (9.6%
and 15.8%) when extracting more complex, subjective data items. Considering all
data items, both approaches had precision >90% but low recall (<25%) and F1
scores (<40%). The context of a complex scoping review, open response types and
methodological approach likely impacted performance due to missed and
misattributed data. LLM feedback considered the baseline extraction accurate
and suggested minor amendments: four of 15 (26.7%) to citation details and 8 of
38 (21.1%) to key findings data items were considered to potentially add value.
However, when repeating the process with a dataset featuring deliberate errors,
only 2 of 39 (5%) errors were detected. Review-protocol-based methods used for
expediency require more robust performance evaluation across a range of LLMs
and review contexts with comparison to conventional prompt engineering
approaches. We recommend researchers evaluate and report LLM performance if
using them similarly to conduct data extraction or review extracted data. LLM
feedback contributed to protocol adaptation and may assist future review
protocol drafting.

</details>


### [109] [Elite Polarization in European Parliamentary Speeches: a Novel Measurement Approach Using Large Language Models](https://arxiv.org/abs/2507.06658)
*Gennadii Iakovlev*

Main category: cs.CL

TL;DR: 本研究提出了一种利用人工智能进行行为者与受众检测的新方法，旨在通过分析议会演讲中政治人物提及他人的情况来衡量精英极化。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种新的方式来量化和理解政治精英间的极化现象，特別是彼此间的敌对情感。

Method: 通过AI检测政治家在议会演讲中提到他人时的情感温度与情绪评价，构建相应的极化指数，并分析了英国、匈牙利和意大利数据。

Result: 生成了一个具有良好面效度的极化指数，该指数能跟随选举、危机等事件动态变化。

Conclusion: 这项研究不仅能用于已有国家数据分析，还为未来欧盟范围内精英极化的长期数据集奠定了基础。

Abstract: This project introduces a new measure of elite polarization via actor and
subject detection using artificial intelligence. I identify when politicians
mention one another in parliamentary speeches, note who is speaking and who is
being addressed, and assess the emotional temperature behind these evaluations.
This maps how elites evaluate their various out-parties, allowing us to create
an index of mutual out-party hostility, that is, elite polarization. While I
analyzed polarization data over the past four decades for the UK, and two
decades for Hungary and Italy, my approach lays the groundwork for a
twenty-year, EU-wide time-series dataset on elite polarization. I obtain the
results that can be aggregated by party and quarter. The resulting index
demonstrates a good face validity: it reacts to events such as electoral
campaigns, country- and party-level crises, and to parties losing and assuming
power.

</details>


### [110] [CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and Context Aware Text Generation with LLMs](https://arxiv.org/abs/2507.06715)
*Garapati Keerthana,Manik Gupta*

Main category: cs.CL

TL;DR: 本研究提出CLI-RAG，一种用于生成结构化和临床关联文本的新框架，解决了临床数据结构不一及长文本生成的挑战。


<details>
  <summary>Details</summary>
Motivation: 临床文本生成面临患者数据非结构化、异质性及文档过长导致相关信息省略的问题。

Method: 引入CLI-RAG框架，结合分层分块策略和双阶段检索机制，针对临床文档的结构和任务需求优化生成流程。

Result: 在MIMIC-III数据集上测试显示，该框架生成的进度记录在时间语义对齐上优于基线，由80.7%提升至87.7%。

Conclusion: CLI-RAG通过增强结构和语义一致性，展示了在临床文本生成中的潜在价值，为可靠且可重复的临床应用奠定了基础。

Abstract: Large language models (LLMs), including zero-shot and few-shot paradigms,
have shown promising capabilities in clinical text generation. However,
real-world applications face two key challenges: (1) patient data is highly
unstructured, heterogeneous, and scattered across multiple note types and (2)
clinical notes are often long and semantically dense, making naive prompting
infeasible due to context length constraints and the risk of omitting
clinically relevant information.
  We introduce CLI-RAG (Clinically Informed Retrieval-Augmented Generation), a
domain-specific framework for structured and clinically grounded text
generation using LLMs. It incorporates a novel hierarchical chunking strategy
that respects clinical document structure and introduces a task-specific
dual-stage retrieval mechanism. The global stage identifies relevant note types
using evidence-based queries, while the local stage extracts high-value content
within those notes creating relevance at both document and section levels.
  We apply the system to generate structured progress notes for individual
hospital visits using 15 clinical note types from the MIMIC-III dataset.
Experiments show that it preserves temporal and semantic alignment across
visits, achieving an average alignment score of 87.7%, surpassing the 80.7%
baseline from real clinician-authored notes. The generated outputs also
demonstrate high consistency across LLMs, reinforcing deterministic behavior
essential for reproducibility, reliability, and clinical trust.

</details>


### [111] [On the Effect of Uncertainty on Layer-wise Inference Dynamics](https://arxiv.org/abs/2507.06722)
*Sunwoo Kim,Haneul Yoo,Alice Oh*

Main category: cs.CL

TL;DR: 本文分析了LLMs中隐藏层的概率轨迹，发现无论输出是否有不确定性，其概率动态变化规律是一致的，表明不确定性没有显著影响推理动态。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注LLMs对不确定性的编码，而鲜有探讨其对隐藏状态处理的影响，作者希望揭示不确定性与推理动态之间的联系。

Method: 采用一种名为Tuned Lens的工具，分析模型在11个数据集和5个模型上的逐层概率轨迹，并使用错误预测来衡量高不确定性表现。

Result: 无论是确定性输出还是不确定性输出，两者的概率动态在层间大体一致，且高不确定性预测同样在类似层次出现信心骤增现象。而更高性能的模型对不确定性的处理方式可能会不同。

Conclusion: 简化方法检测推理时的不确定性可能不可靠，作者建议利用可解释性工具进一步研究不确定性如何影响推理。

Abstract: Understanding how large language models (LLMs) internally represent and
process their predictions is central to detecting uncertainty and preventing
hallucinations. While several studies have shown that models encode uncertainty
in their hidden states, it is underexplored how this affects the way they
process such hidden states. In this work, we demonstrate that the dynamics of
output token probabilities across layers for certain and uncertain outputs are
largely aligned, revealing that uncertainty does not seem to affect inference
dynamics. Specifically, we use the Tuned Lens, a variant of the Logit Lens, to
analyze the layer-wise probability trajectories of final prediction tokens
across 11 datasets and 5 models. Using incorrect predictions as those with
higher epistemic uncertainty, our results show aligned trajectories for certain
and uncertain predictions that both observe abrupt increases in confidence at
similar layers. We balance this finding by showing evidence that more competent
models may learn to process uncertainty differently. Our findings challenge the
feasibility of leveraging simplistic methods for detecting uncertainty at
inference. More broadly, our work demonstrates how interpretability methods may
be used to investigate the way uncertainty affects inference.

</details>


### [112] [KAConvText: Novel Approach to Burmese Sentence Classification using Kolmogorov-Arnold Convolution](https://arxiv.org/abs/2507.06753)
*Ye Kyaw Thu,Thura Aung,Thazin Myint Oo,Thepchai Supnithi*

Main category: cs.CL

TL;DR: 本文首次将Kolmogorov-Arnold卷积(KAConvText)应用于句子分类，研究了三个任务，并与多个基准模型进行比较，取得了显著的分类性能。


<details>
  <summary>Details</summary>
Motivation: 探索KAConvText在处理不平衡和多类句子分类任务中是否具有优势，并研究其不同嵌入配置的效果及可解释性。

Method: 使用KAConvText进行句子分类，采用随机与fastText静态和微调嵌入，比较100与300维CBOW和Skip-gram模型，并结合不同分类头（MLP与KAN）进行实验。

Result: KAConvText-MLP结合微调的fastText嵌入在所有任务中均取得最佳效果：仇恨言论检测91.23%准确率(F1=0.9109)，新闻分类92.66%准确率(F1=0.9267)，语言识别99.82%准确率(F1=0.9982)。

Conclusion: KAConvText对不平衡二元分类、多类分类和语言检测任务表现出出色的性能，尤其结合微调fastText和MLP分类头时表现最佳，同时支持增强的可解释性。

Abstract: This paper presents the first application of Kolmogorov-Arnold Convolution
for Text (KAConvText) in sentence classification, addressing three tasks:
imbalanced binary hate speech detection, balanced multiclass news
classification, and imbalanced multiclass ethnic language identification. We
investigate various embedding configurations, comparing random to fastText
embeddings in both static and fine-tuned settings, with embedding dimensions of
100 and 300 using CBOW and Skip-gram models. Baselines include standard CNNs
and CNNs augmented with a Kolmogorov-Arnold Network (CNN-KAN). In addition, we
investigated KAConvText with different classification heads - MLP and KAN,
where using KAN head supports enhanced interpretability. Results show that
KAConvText-MLP with fine-tuned fastText embeddings achieves the best
performance of 91.23% accuracy (F1-score = 0.9109) for hate speech detection,
92.66% accuracy (F1-score = 0.9267) for news classification, and 99.82%
accuracy (F1-score = 0.9982) for language identification.

</details>


### [113] [Checklist Engineering Empowers Multilingual LLM Judges](https://arxiv.org/abs/2507.06774)
*Mohammad Ghiasvand Mohammadkhani,Hamid Beigy*

Main category: cs.CL

TL;DR: 提出了CE-Judge，一个基于Checklist直觉的多语言评价框架，通过开源模型实现无需训练的跨语言自然语言处理文本评估，性能相比基准表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注多语言评价，且通常依赖于昂贵的专有模型或大量训练数据，需要在效率和成本之间权衡。

Method: 通过结合Checklist直觉提出CE-Judge框架，采用无需训练的开源模型进行点对点和对对对比的多语言文本评价。

Result: 在多种语言和三个基准数据集上的实验表明，CE-Judge方法总体优于基准表现，并与GPT-4o模型性能相当。

Conclusion: CE-Judge为多语言自然语言处理中的自动文本评价提供了一种高效、无训练、且成本较低的解决方案。

Abstract: Automated text evaluation has long been a central issue in Natural Language
Processing (NLP). Recently, the field has shifted toward using Large Language
Models (LLMs) as evaluators-a trend known as the LLM-as-a-Judge paradigm. While
promising and easily adaptable across tasks, this approach has seen limited
exploration in multilingual contexts. Existing multilingual studies often rely
on proprietary models or require extensive training data for fine-tuning,
raising concerns about cost, time, and efficiency. In this paper, we propose
Checklist Engineering based LLM-as-a-Judge (CE-Judge), a training-free
framework that uses checklist intuition for multilingual evaluation with an
open-source model. Experiments across multiple languages and three benchmark
datasets, under both pointwise and pairwise settings, show that our method
generally surpasses the baselines and performs on par with the GPT-4o model.

</details>


### [114] [Efficient Industrial sLLMs through Domain Adaptive Continual Pretraining: Method, Evaluation and Applications](https://arxiv.org/abs/2507.06795)
*Seonwu Kim,Yohan Na,Kihun Kim,Hanhee Cho,Geun Lim,Mintae Kim,Seongik Park,Ki Hyun Kim,Youngsub Han,Byoung-Ki Jeon*

Main category: cs.CL

TL;DR: 研究验证了在小型语言模型(sLLMs)上应用基于领域自适应持续预训练(DACP) 的方法的有效性，为企业级部署提供了成本高效且可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现存许多组织尚缺乏部署和维护大规模语言模型(LLMs)的基础设施，因此研究小型语言模型(sLLMs)成为了更为实用的替代品。

Method: 采用领域自适应持续预训练(DACP)的方法，对不同的基础模型和服务领域进行实验和真实场景验证。

Result: 实验表明，DACP 策略应用于 sLLMs 后，目标领域性能显著提高，同时保留了通用能力。

Conclusion: DACP 提供了一种经济且可扩展的商业化小型语言模型应用方案。

Abstract: The emergence of open-source large language models (LLMs) has expanded
opportunities for enterprise applications; however, many organizations still
lack the infrastructure to deploy and maintain large-scale models. As a result,
small LLMs (sLLMs) have become a practical alternative, despite their inherent
performance limitations. While Domain Adaptive Continual Pretraining (DACP) has
been previously explored as a method for domain adaptation, its utility in
commercial applications remains under-examined. In this study, we validate the
effectiveness of applying a DACP-based recipe across diverse foundation models
and service domains. Through extensive experiments and real-world evaluations,
we demonstrate that DACP-applied sLLMs achieve substantial gains in target
domain performance while preserving general capabilities, offering a
cost-efficient and scalable solution for enterprise-level deployment.

</details>


### [115] [Text to model via SysML: Automated generation of dynamical system computational models from unstructured natural language text via enhanced System Modeling Language diagrams](https://arxiv.org/abs/2507.06803)
*Matthew Anderson Hendricks,Alice Cicirello*

Main category: cs.CL

TL;DR: 本文提出了一种利用领域和专业知识自动生成动态系统计算模型的方法，并通过示例展示了其在工程动态系统设计中的有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 加速工程动态系统的设计和部署，通过自动化模型生成减少人为介入，提高设计精度。

Method: 策略包括五个步骤，核心是利用SysML图表提取组件的依赖关系、属性和操作，并通过NLP和LLMs改进信息提取和生成模型。

Result: 展示了从文本到简单摆模型的案例，证明该方法比仅使用LLMs性能更优秀。

Conclusion: 该方法适用于不同系统、领域和计算软件，为动态系统建模提供了一种更快、更精确的解决方案。

Abstract: This paper contributes to speeding up the design and deployment of
engineering dynamical systems by proposing a strategy for exploiting domain and
expert knowledge for the automated generation of dynamical system computational
model starting from a corpus of document relevant to the dynamical system of
interest and an input document describing the specific system. This strategy is
implemented in five steps and, crucially, it uses system modeling language
diagrams (SysML) to extract accurate information about the dependencies,
attributes, and operations of components. Natural Language Processing (NLP)
strategies and Large Language Models (LLMs) are employed in specific tasks to
improve intermediate outputs of the SySML diagrams automated generation, such
as: list of key nouns; list of extracted relationships; list of key phrases and
key relationships; block attribute values; block relationships; and BDD diagram
generation. The applicability of automated SysML diagram generation is
illustrated with different case studies. The computational models of complex
dynamical systems from SysML diagrams are then obtained via code generation and
computational model generation steps. In the code generation step, NLP
strategies are used for summarization, while LLMs are used for validation only.
The proposed approach is not limited to a specific system, domain, or
computational software. The applicability of the proposed approach is shown via
an end-to-end example from text to model of a simple pendulum, showing improved
performance compared to results yielded by LLMs only.

</details>


### [116] [Adaptive Termination for Multi-round Parallel Reasoning: An Universal Semantic Entropy-Guided Framework](https://arxiv.org/abs/2507.06829)
*Zenan Xu,Zexuan Qiu,Guanhua Huang,Kun Li,Siheng Li,Chenchen Zhang,Kejiao Li,Qi Yi,Yuhao Jiang,Bo Zhou,Fengzong Lian,Zhanhui Kang*

Main category: cs.CL

TL;DR: 提出一种结合序列推理和并行推理的推理框架，并引入语义熵(SE)指标。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在推理时面临效率低下或缺乏协调性的问题，亟需将序列和并行推理的优势结合起来设计更灵活的推理框架。

Method: 提出一种在推理时协作的框架，并引入了语义熵(SE)作为质量指标，用于评估模型响应，动态控制推理过程。

Result: 语义熵(SE)被证明与推理精度有强负相关性，表现出对推理质量的有效评估能力。

Conclusion: 新框架结合了序列和并行推理的优点，通过语义熵实现高效、准确的推理评估和动态控制。

Abstract: Recent advances in large language models (LLMs) have accelerated progress
toward artificial general intelligence, with inference-time scaling emerging as
a key technique. Contemporary approaches leverage either sequential reasoning
(iteratively extending chains of thought) or parallel reasoning (generating
multiple solutions simultaneously) to scale inference. However, both paradigms
face fundamental limitations: sequential scaling typically relies on arbitrary
token budgets for termination, leading to inefficiency or premature cutoff;
while parallel scaling often lacks coordination among parallel branches and
requires intrusive fine-tuning to perform effectively. In light of these
challenges, we aim to design a flexible test-time collaborative inference
framework that exploits the complementary strengths of both sequential and
parallel reasoning paradigms. Towards this goal, the core challenge lies in
developing an efficient and accurate intrinsic quality metric to assess model
responses during collaborative inference, enabling dynamic control and early
termination of the reasoning trace. To address this challenge, we introduce
semantic entropy (SE), which quantifies the semantic diversity of parallel
model responses and serves as a robust indicator of reasoning quality due to
its strong negative correlation with accuracy...

</details>


### [117] [Shifting from Ranking to Set Selection for Retrieval Augmented Generation](https://arxiv.org/abs/2507.06838)
*Dahyun Lee,Yongrae Jo,Haeju Park,Moontae Lee*

Main category: cs.CL

TL;DR: 本论文提出了一种名为SETR的方法，通过链式思维推理进行集合式文档选择，提升了多跳问题回答系统中的答案正确性和检索质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要基于单个文档相关性重新排序top-k文档，难以满足复杂多跳问题的需求，亟需一种方法能够集合地满足查询信息需求。

Method: 提出了一个集合式文档选择方法SETR，通过链式思维推理识别查询的信息需求，并选择能满足这些需求的文档集合。

Result: 在多跳RAG评测中，SETR优于专有LLM reranker和开源基线方法，在答案正确性和检索质量上均表现突出。

Conclusion: SETR为传统的RAG系统reranker提供了一个有效且高效的替代方案，并公开了相关代码。

Abstract: Retrieval in Retrieval-Augmented Generation(RAG) must ensure that retrieved
passages are not only individually relevant but also collectively form a
comprehensive set. Existing approaches primarily rerank top-k passages based on
their individual relevance, often failing to meet the information needs of
complex queries in multi-hop question answering. In this work, we propose a
set-wise passage selection approach and introduce SETR, which explicitly
identifies the information requirements of a query through Chain-of-Thought
reasoning and selects an optimal set of passages that collectively satisfy
those requirements. Experiments on multi-hop RAG benchmarks show that SETR
outperforms both proprietary LLM-based rerankers and open-source baselines in
terms of answer correctness and retrieval quality, providing an effective and
efficient alternative to traditional rerankers in RAG systems. The code is
available at https://github.com/LGAI-Research/SetR

</details>


### [118] [Developing and Maintaining an Open-Source Repository of AI Evaluations: Challenges and Insights](https://arxiv.org/abs/2507.06893)
*Alexandra Abbas,Celia Waggoner,Justin Olive*

Main category: cs.CL

TL;DR: 该论文分析了维护一个包含70多个社区贡献的AI评价开源库的实践经验，提出应对AI评价挑战的方法。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型的能力和安全性评估变得至关重要，研究者希望通过共享的社区资源来改善AI评估方法。

Method: 论文提出了三种解决方案：1. 结构化的社区管理框架以扩大社区贡献；2. 用于最优重采样和跨模型比较的统计方法，包括不确定性量化；3. 系统化的质量控制流程以提高结果的可重复性。

Result: 分析表明，AI评估需要比传统软件开发实践更多的专业基础设施、统计严谨性和社区协调。

Conclusion: AI评估需要新颖且协调化的基础设施和方法，这些超越了传统开发范畴，并显现了社区协作的重要性。

Abstract: AI evaluations have become critical tools for assessing large language model
capabilities and safety. This paper presents practical insights from eight
months of maintaining $inspect\_evals$, an open-source repository of 70+
community-contributed AI evaluations. We identify key challenges in
implementing and maintaining AI evaluations and develop solutions including:
(1) a structured cohort management framework for scaling community
contributions, (2) statistical methodologies for optimal resampling and
cross-model comparison with uncertainty quantification, and (3) systematic
quality control processes for reproducibility. Our analysis reveals that AI
evaluation requires specialized infrastructure, statistical rigor, and
community coordination beyond traditional software development practices.

</details>


### [119] [SCoRE: Streamlined Corpus-based Relation Extraction using Multi-Label Contrastive Learning and Bayesian kNN](https://arxiv.org/abs/2507.06895)
*Luca Mariotti,Veronica Guidetti,Federica Mandreoli*

Main category: cs.CL

TL;DR: 本文提出了一个名为SCoRE的模块化、高效的句子级关系提取系统，能够在不需要微调的情况下，与预训练大语言模型集成，并适应不同语料和知识图谱。


<details>
  <summary>Details</summary>
Motivation: 满足在知识图谱扩展中对低监督、高适应性和抗噪性关系提取方法的需求。

Method: 引入SCoRE，将监督对比学习与贝叶斯k近邻分类器相结合，进行多标签分类，并设计两个新评估指标：相关性结构距离（CSD）和R处的精度（P@R）。

Result: SCoRE在五个基准测试中匹配或超越现有方法，同时显著降低了能耗，并展示了复杂设计可能损害性能的优势。

Conclusion: SCoRE以其高效性、模块化和可扩展性成为现实世界中关系提取应用的最佳选择。

Abstract: The growing demand for efficient knowledge graph (KG) enrichment leveraging
external corpora has intensified interest in relation extraction (RE),
particularly under low-supervision settings. To address the need for adaptable
and noise-resilient RE solutions that integrate seamlessly with pre-trained
large language models (PLMs), we introduce SCoRE, a modular and cost-effective
sentence-level RE system. SCoRE enables easy PLM switching, requires no
finetuning, and adapts smoothly to diverse corpora and KGs. By combining
supervised contrastive learning with a Bayesian k-Nearest Neighbors (kNN)
classifier for multi-label classification, it delivers robust performance
despite the noisy annotations of distantly supervised corpora. To improve RE
evaluation, we propose two novel metrics: Correlation Structure Distance (CSD),
measuring the alignment between learned relational patterns and KG structures,
and Precision at R (P@R), assessing utility as a recommender system. We also
release Wiki20d, a benchmark dataset replicating real-world RE conditions where
only KG-derived annotations are available. Experiments on five benchmarks show
that SCoRE matches or surpasses state-of-the-art methods while significantly
reducing energy consumption. Further analyses reveal that increasing model
complexity, as seen in prior work, degrades performance, highlighting the
advantages of SCoRE's minimal design. Combining efficiency, modularity, and
scalability, SCoRE stands as an optimal choice for real-world RE applications.

</details>


### [120] [VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation](https://arxiv.org/abs/2507.06899)
*Ziang Ye,Yang Zhang,Wentao Shi,Xiaoyu You,Fuli Feng,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 研究发现，将大规模视觉语言模型（LVLMs）与图形用户界面(GUI)代理整合存在安全漏洞，可能受到后门攻击，提出了VisualTrap方法验证这种漏洞。


<details>
  <summary>Details</summary>
Motivation: 图形用户界面代理结合LVLMs带来便捷，却因其与个人设备深度集成引发安全问题，尤其是后门攻击问题尚未深入研究。

Method: 通过提出VisualTrap方法，利用预训练阶段注入少量的有毒数据，并设计高度隐蔽的视觉触发机制，验证后门攻击的可行性及跨环境通用性。

Result: 实验显示，即使在不存在清晰可见的视觉触发的情况下，利用5%有毒数据，VisualTrap方法能有效攻破视觉对位功能，且被攻击点可跨不同GUI环境通用。

Conclusion: 这项工作揭示了GUI代理系统在视觉对位方面存在的后门攻击隐患，强调了对解决这些安全风险的迫切需求。

Abstract: Graphical User Interface (GUI) agents powered by Large Vision-Language Models
(LVLMs) have emerged as a revolutionary approach to automating human-machine
interactions, capable of autonomously operating personal devices (e.g., mobile
phones) or applications within the device to perform complex real-world tasks
in a human-like manner. However, their close integration with personal devices
raises significant security concerns, with many threats, including backdoor
attacks, remaining largely unexplored. This work reveals that the visual
grounding of GUI agent-mapping textual plans to GUI elements-can introduce
vulnerabilities, enabling new types of backdoor attacks. With backdoor attack
targeting visual grounding, the agent's behavior can be compromised even when
given correct task-solving plans. To validate this vulnerability, we propose
VisualTrap, a method that can hijack the grounding by misleading the agent to
locate textual plans to trigger locations instead of the intended targets.
VisualTrap uses the common method of injecting poisoned data for attacks, and
does so during the pre-training of visual grounding to ensure practical
feasibility of attacking. Empirical results show that VisualTrap can
effectively hijack visual grounding with as little as 5% poisoned data and
highly stealthy visual triggers (invisible to the human eye); and the attack
can be generalized to downstream tasks, even after clean fine-tuning. Moreover,
the injected trigger can remain effective across different GUI environments,
e.g., being trained on mobile/web and generalizing to desktop environments.
These findings underscore the urgent need for further research on backdoor
attack risks in GUI agents.

</details>


### [121] [MIND: A Multi-agent Framework for Zero-shot Harmful Meme Detection](https://arxiv.org/abs/2507.06908)
*Ziyan Liu,Chunxiao Fan,Haoran Lou,Yuexin Wu,Kaiwei Deng*

Main category: cs.CL

TL;DR: MIND 是一个无需标注数据的零样本恶意表情包检测框架，使用相似表情包检索、双向洞察推导和多Agent 辩论机制来提升检测效果，其性能超越现有方法且具备良好的通用性。


<details>
  <summary>Details</summary>
Motivation: 传统方法无法有效检测新型表情包，因为表情包迅速演变且缺乏最新标注数据。

Method: 提出了 MIND 框架，包括：1) 从未标注的参考集检索相似表情包，提供上下文信息；2) 双向洞察推导机制全面理解相似表情包；3) 多代理辩论机制通过推理裁决确保决策稳健性。

Result: MIND 在三个表情包数据集上的实验中表现优异，超越现有零样本方法，并对不同模型架构和参数规模展示了强大的泛化能力，具备扩展性。

Conclusion: MIND 提供了一种无需依赖标注数据且可扩展的恶意表情包检测解决方案，对快速演变的问题具有意义。

Abstract: The rapid expansion of memes on social media has highlighted the urgent need
for effective approaches to detect harmful content. However, traditional
data-driven approaches struggle to detect new memes due to their evolving
nature and the lack of up-to-date annotated data. To address this issue, we
propose MIND, a multi-agent framework for zero-shot harmful meme detection that
does not rely on annotated data. MIND implements three key strategies: 1) We
retrieve similar memes from an unannotated reference set to provide contextual
information. 2) We propose a bi-directional insight derivation mechanism to
extract a comprehensive understanding of similar memes. 3) We then employ a
multi-agent debate mechanism to ensure robust decision-making through reasoned
arbitration. Extensive experiments on three meme datasets demonstrate that our
proposed framework not only outperforms existing zero-shot approaches but also
shows strong generalization across different model architectures and parameter
scales, providing a scalable solution for harmful meme detection. The code is
available at https://github.com/destroy-lonely/MIND.

</details>


### [122] [MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal Prediction](https://arxiv.org/abs/2507.06909)
*Xiao Wang,Jiahuan Pei,Diancheng Shui,Zhiguang Han,Xin Sun,Dawei Zhu,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 提出了一个新的法律判决预测数据集MPMCP，用于探讨是否应单独处理多被告和多罪名问题，实验发现多被告多罪名场景（S4）具有最大挑战性。


<details>
  <summary>Details</summary>
Motivation: 探索法律判决预测中单独处理多被告及多罪名的必要性，并创建相关数据集进行验证。

Method: 引入了新的多被告多罪名预测数据集（MPMCP），并使用多种法律大语言模型分别对四种法律场景进行评估，比较其在罪名预测及刑期预测任务中的表现。

Result: 实验发现，S4（多被告、多罪名）的模型表现最差，影响程度根据模型不同而显著变化，例如在S4与S1的比较中，InternLM2的F1分数降低4.5%，而Lawformer降低19.7%。

Conclusion: 在法律判决预测中，多被告多罪名需要特别对待，不同模型的表现差异揭示了进一步改进的必要性。

Abstract: Legal judgment prediction offers a compelling method to aid legal
practitioners and researchers. However, the research question remains
relatively under-explored: Should multiple defendants and charges be treated
separately in LJP? To address this, we introduce a new dataset namely
multi-person multi-charge prediction (MPMCP), and seek the answer by evaluating
the performance of several prevailing legal large language models (LLMs) on
four practical legal judgment scenarios: (S1) single defendant with a single
charge, (S2) single defendant with multiple charges, (S3) multiple defendants
with a single charge, and (S4) multiple defendants with multiple charges. We
evaluate the dataset across two LJP tasks, i.e., charge prediction and penalty
term prediction. We have conducted extensive experiments and found that the
scenario involving multiple defendants and multiple charges (S4) poses the
greatest challenges, followed by S2, S3, and S1. The impact varies
significantly depending on the model. For example, in S4 compared to S1,
InternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD,
while Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD.
Our dataset and code are available at
https://github.com/lololo-xiao/MultiJustice-MPMCP.

</details>


### [123] [Exploring LLMs for Predicting Tutor Strategy and Student Outcomes in Dialogues](https://arxiv.org/abs/2507.06910)
*Fareya Ikram,Alexander Scarlatos,Andrew Lan*

Main category: cs.CL

TL;DR: 研究探讨了现代大语言模型（LLMs）预测教育对话中导师策略和学生表现的能力，发现当前LLMs在预测未来导师策略上表现较差，而导师策略对学生结果的重要性很高。


<details>
  <summary>Details</summary>
Motivation: 由于在线学习兴起及AI辅导能力的发展，理解导师策略对学生成绩的影响变得重要，但目前缺乏对导师策略预测的研究。

Method: 分析了两个数学辅导对话数据集，使用Llama 3和GPT-4o等现代LLMs预测教育对话中导师策略和学生表现的能力。

Result: 即使是最前沿的LLMs在预测未来导师策略上表现较差，且导师策略对学生结果具有重要参考意义。

Conclusion: 需要更强大的方法来改进对教育对话中导师策略和学生表现实验的预测能力。

Abstract: Tutoring dialogues have gained significant attention in recent years, given
the prominence of online learning and the emerging tutoring abilities of
artificial intelligence (AI) agents powered by large language models (LLMs).
Recent studies have shown that the strategies used by tutors can have
significant effects on student outcomes, necessitating methods to predict how
tutors will behave and how their actions impact students. However, few works
have studied predicting tutor strategy in dialogues. Therefore, in this work we
investigate the ability of modern LLMs, particularly Llama 3 and GPT-4o, to
predict both future tutor moves and student outcomes in dialogues, using two
math tutoring dialogue datasets. We find that even state-of-the-art LLMs
struggle to predict future tutor strategy while tutor strategy is highly
indicative of student outcomes, outlining a need for more powerful methods to
approach this task.

</details>


### [124] [Rethinking Verification for LLM Code Generation: From Generation to Testing](https://arxiv.org/abs/2507.06920)
*Zihan Ma,Taolin Zhang,Maosong Cao,Wenwei Zhang,Minnan Luo,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 该论文研究了大语言模型在代码生成评估中的测试用例不足问题，并提出了TCG任务解决方案SAGA，以提高覆盖率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有的代码生成评估中测试套件样本单一，隐藏误差未被发现，影响了相应系统的性能评估和奖励函数准确性。

Method: 提出了新的多维测试评估指标，以及一种名为SAGA的人类与LLM协作方法，并开发了TCGBench工具框架。

Result: 实验表明SAGA在TCGBench上的检测率和验证器准确率分别达到了90.62%和32.58%；其生成的基准表现较现有方法高10.78%。

Conclusion: SAGA显著提升了测试覆盖率与评估方法的可靠性，推动了RLVR在代码生成中的发展，为更复杂且自适应的基准构建提供了基础。

Abstract: Large language models (LLMs) have recently achieved notable success in
code-generation benchmarks such as HumanEval and LiveCodeBench. However, a
detailed examination reveals that these evaluation suites often comprise only a
limited number of homogeneous test cases, resulting in subtle faults going
undetected. This not only artificially inflates measured performance but also
compromises accurate reward estimation in reinforcement learning frameworks
utilizing verifiable rewards (RLVR). To address these critical shortcomings, we
systematically investigate the test-case generation (TCG) task by proposing
multi-dimensional metrics designed to rigorously quantify test-suite
thoroughness. Furthermore, we introduce a human-LLM collaborative method
(SAGA), leveraging human programming expertise with LLM reasoning capability,
aimed at significantly enhancing both the coverage and the quality of generated
test cases. In addition, we develop a TCGBench to facilitate the study of the
TCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a
verifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc)
of the code generation evaluation benchmark synthesized by SAGA is 10.78%
higher than that of LiveCodeBench-v6. These results demonstrate the
effectiveness of our proposed method. We hope this work contributes to building
a scalable foundation for reliable LLM code evaluation, further advancing RLVR
in code generation, and paving the way for automated adversarial test synthesis
and adaptive benchmark integration.

</details>


### [125] [Investigating the Robustness of Retrieval-Augmented Generation at the Query Level](https://arxiv.org/abs/2507.06956)
*Sezen Perçin,Xin Su,Qutub Sha Syed,Phillip Howard,Aleksei Kuvshinov,Leo Schwinn,Kay-Ulrich Scholl*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）存在更新效率低的问题，检索增强生成（RAG）为其提供了解决方案，但面临输入查询质量的挑战。本文分析RAG系统对查询扰动的敏感性，提出评价框架并给出实际建议。


<details>
  <summary>Details</summary>
Motivation: LLMs更新成本高且效率低，通过RAG可以引入外部知识提升一致性，减少幻觉现象，但RAG系统依赖输入查询质量，需研究其鲁棒性问题。

Method: 对RAG的各模块进行单独和结合分析，基于多种数据集开展1092次实验，评估查询扰动对性能的影响，并提出系统评价框架。

Result: 发现常见的检索模型在面对轻微查询变化时性能显著下降，提出了一个系统化的框架和实践建议来提升RAG管道的查询级鲁棒性。

Conclusion: RAG管道对输入查询质量非常敏感，本文通过深入分析及实验证明其不足，并为实践者提供了提升系统鲁棒性的实际指导。

Abstract: Large language models (LLMs) are very costly and inefficient to update with
new information. To address this limitation, retrieval-augmented generation
(RAG) has been proposed as a solution that dynamically incorporates external
knowledge during inference, improving factual consistency and reducing
hallucinations. Despite its promise, RAG systems face practical challenges-most
notably, a strong dependence on the quality of the input query for accurate
retrieval. In this paper, we investigate the sensitivity of different
components in the RAG pipeline to various types of query perturbations. Our
analysis reveals that the performance of commonly used retrievers can degrade
significantly even under minor query variations. We study each module in
isolation as well as their combined effect in an end-to-end question answering
setting, using both general-domain and domain-specific datasets. Additionally,
we propose an evaluation framework to systematically assess the query-level
robustness of RAG pipelines and offer actionable recommendations for
practitioners based on the results of more than 1092 experiments we performed.

</details>


### [126] [FRaN-X: FRaming and Narratives-eXplorer](https://arxiv.org/abs/2507.06974)
*Artur Muratov,Hana Fatima Shaikh,Vanshikaa Jani,Tarek Mahmoud,Zhuohan Xie,Daniil Orel,Aaryamonvikram Singh,Yuxia Wang,Aadi Joshi,Hasan Iqbal,Ming Shan Hee,Dhruv Sahnan,Nikolaos Nikolaidis,Purificação Silvano,Dimitar Dimitrov,Roman Yangarber,Ricardo Campos,Alípio Jorge,Nuno Guimarães,Elisa Sartori,Nicolas Stefanovitch,Giovanni Da San Martino,Jakub Piskorski,Preslav Nakov*

Main category: cs.CL

TL;DR: 本文展示了FRaN-X，一个用于自动检测实体提及并分类其叙述角色的系统。


<details>
  <summary>Details</summary>
Motivation: 解决如何自动检测和标注实体在叙述中被如何框架呈现的问题。

Method: 提出一个两阶段系统，结合序列标注与细粒度角色分类，支持五种语言和两个领域，并提供交互式网络界面进行分析。

Result: 系统能生成直观图表，展示文章群体的叙述结构，并提供角色转变的时间轴视图。

Conclusion: FRaN-X 有助于媒体分析，允许用户跨文章和时间维度进行角色框架的全面探索。

Abstract: We present FRaN-X, a Framing and Narratives Explorer that automatically
detects entity mentions and classifies their narrative roles directly from raw
text. FRaN-X comprises a two-stage system that combines sequence labeling with
fine-grained role classification to reveal how entities are portrayed as
protagonists, antagonists, or innocents, using a unique taxonomy of 22
fine-grained roles nested under these three main categories. The system
supports five languages (Bulgarian, English, Hindi, Russian, and Portuguese)
and two domains (the Russia-Ukraine Conflict and Climate Change). It provides
an interactive web interface for media analysts to explore and compare framing
across different sources, tackling the challenge of automatically detecting and
labeling how entities are framed. Our system allows end users to focus on a
single article as well as analyze up to four articles simultaneously. We
provide aggregate level analysis including an intuitive graph visualization
that highlights the narrative a group of articles are pushing. Our system
includes a search feature for users to look up entities of interest, along with
a timeline view that allows analysts to track an entity's role transitions
across different contexts within the article. The FRaN-X system and the trained
models are licensed under an MIT License. FRaN-X is publicly accessible at
https://fran-x.streamlit.app/ and a video demonstration is available at
https://youtu.be/VZVi-1B6yYk.

</details>


### [127] [FlexOlmo: Open Language Models for Flexible Data Use](https://arxiv.org/abs/2507.07024)
*Weijia Shi,Akshita Bhagia,Kevin Farhat,Niklas Muennighoff,Pete Walsh,Jacob Morrison,Dustin Schwenk,Shayne Longpre,Jake Poznanski,Allyson Ettinger,Daogao Liu,Margaret Li,Dirk Groeneveld,Mike Lewis,Wen-tau Yih,Luca Soldaini,Kyle Lo,Noah A. Smith,Luke Zettlemoyer,Pang Wei Koh,Hannaneh Hajishirzi,Ali Farhadi,Sewon Min*

Main category: cs.CL

TL;DR: FlexOlmo 提出了一种支持无数据共享分布式训练和灵活数据推断的新型语言模型，通过独立专家模块和领域相关路由设计来实现对封闭数据的利用，同时尊重数据所有者的权限要求和隐私需求。


<details>
  <summary>Details</summary>
Motivation: 当前大量模型训练依赖联合大数据集，但针对封闭数据或敏感数据使用时，存在数据授权和隐私保护挑战，因此需要创新性方法在保障数据独立的基础上提高性能。

Method: FlexOlmo 采用了独立训练的专家模型(MoE架构)，每个专家针对各自封闭数据集训练，并通过领域知情的路由机制在推断时灵活整合这些专家，避免了传统需要联合训练的需求。

Result: 使用FlexOlmo，我们实现了模型性能的显著提升：结合通用与专用专家时，性能相较单独训练提高41%；并且针对手动合并或其他MoE方法分别提升了10.1%和超过标准MoE效果，尤其适合封闭或受限数据应用场景。

Conclusion: FlexOlmo 提供了一种可以在闭环数据环境下兼顾数据可控性与模型性能的新途径，不仅对保护数据所有者权力有积极意义，且推动在隐私敏感行业中的语言模型研究应用。

Abstract: We introduce FlexOlmo, a new class of language models (LMs) that supports (1)
distributed training without data sharing, where different model parameters are
independently trained on closed datasets, and (2) data-flexible inference,
where these parameters along with their associated data can be flexibly
included or excluded from model inferences with no further training. FlexOlmo
employs a mixture-of-experts (MoE) architecture where each expert is trained
independently on closed datasets and later integrated through a new
domain-informed routing without any joint training. FlexOlmo is trained on
FlexMix, a corpus we curate comprising publicly available datasets alongside
seven domain-specific sets, representing realistic approximations of closed
sets. We evaluate models with up to 37 billion parameters (20 billion active)
on 31 diverse downstream tasks. We show that a general expert trained on public
data can be effectively combined with independently trained experts from other
data owners, leading to an average 41% relative improvement while allowing
users to opt out of certain data based on data licensing or permission
requirements. Our approach also outperforms prior model merging methods by
10.1% on average and surpasses the standard MoE trained without data
restrictions using the same training FLOPs. Altogether, this research presents
a solution for both data owners and researchers in regulated industries with
sensitive or protected data. FlexOlmo enables benefiting from closed data while
respecting data owners' preferences by keeping their data local and supporting
fine-grained control of data access during inference.

</details>


### [128] [UniConv: Unifying Retrieval and Response Generation for Large Language Models in Conversations](https://arxiv.org/abs/2507.07030)
*Fengran Mo,Yifan Gao,Chuan Meng,Xin Liu,Zhuofeng Wu,Kelong Mao,Zhengyang Wang,Pei Chen,Zheng Li,Xian Li,Bing Yin,Meng Jiang*

Main category: cs.CL

TL;DR: 本研究提出一个统一模型，同时支持对话检索和生成任务，并通过联合微调和特定机制提升这些任务的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的对话搜索系统通常将检索和生成分为两个模型，限制了同时利用两者知识的能力。因此，研究如何将这两个任务统一在一个模型中以改进性能。

Method: 采用联合微调策略，针对不同任务设计目标，并引入两种机制减少不一致性风险和数据差异影响。

Result: 通过在五个对话搜索数据集上的评估，证明了统一模型可以互相促进两个任务的表现，并超过了现有的基线模型。

Conclusion: 统一模型能够有效地将对话检索和生成结合起来，改进了各自的效果，展示了优于现有方法的性能。

Abstract: The rapid advancement of conversational search systems revolutionizes how
information is accessed by enabling the multi-turn interaction between the user
and the system. Existing conversational search systems are usually built with
two different models. This separation restricts the system from leveraging the
intrinsic knowledge of the models simultaneously, which cannot ensure the
effectiveness of retrieval benefiting the generation. The existing studies for
developing unified models cannot fully address the aspects of understanding
conversational context, managing retrieval independently, and generating
responses. In this paper, we explore how to unify dense retrieval and response
generation for large language models in conversation. We conduct joint
fine-tuning with different objectives and design two mechanisms to reduce the
inconsistency risks while mitigating data discrepancy. The evaluations on five
conversational search datasets demonstrate that our unified model can mutually
improve both tasks and outperform the existing baselines.

</details>


### [129] [Discrete Diffusion Models for Language Generation](https://arxiv.org/abs/2507.07050)
*Ashen Weligalle*

Main category: cs.CL

TL;DR: 本文探讨了扩散模型在离散数据（如自然语言生成）中的应用，将离散扩散模型（D3PM）与传统自回归模型进行了对比，分析其生成质量及效率的权衡。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散模型在连续数据生成中表现优异，但在离散数据（自然语言生成）中应用面临挑战，如token依赖复杂性及生成顺序的未定义性。因此作者研究了离散扩散模型的可行性和性能。

Method: 通过实验对比离散扩散模型（D3PM）和传统自回归语言模型，评估其生成性能（BPT、NLL、PPL）和处理速度。所有测试均在固定条件下进行，以确保公平对比。

Result: D3PM模型BPT平均值为8.05，性能最佳的为5.72，而AR模型BPT平均值较低（4.59），说明其压缩效果更好。但D3PM在处理速度上领先，达到每秒3.97批次，显示了并行生成的潜力。

Conclusion: 研究表明离散扩散模型在非自回归语言生成中具有潜力和局限性，提供了扩展相关研究的基础。

Abstract: Diffusion models have emerged as a powerful class of generative models,
achieving state-of-the-art results in continuous data domains such as image and
video generation. Their core mechanism involves a forward diffusion process
that gradually transforms structured data into a Gaussian-like distribution,
followed by a learned reverse process to reconstruct the data. While successful
in continuous modalities, applying this framework to discrete data-particularly
natural language-remains challenging due to token dependency complexities and
the lack of a defined generation order.This thesis investigates the feasibility
and performance of discrete diffusion models for natural language generation.
Specifically, we evaluate the Discrete Denoising Diffusion Probabilistic Model
(D3PM) and compare it with traditional autoregressive (AR) language models. To
assess generative performance, we use Bits Per Token (BPT), Negative
Log-Likelihood (NLL), Perplexity (PPL), and Batch Processing Speed.
  Results show the best-performing D3PM model achieves a BPT of 5.72, with a
mean of 8.05. The AR model outperforms in compression with a lower mean BPT of
4.59, but D3PM achieves higher processing speed, reaching up to 3.97 batches
per sec., indicating potential for parallel generation.All evaluations were
conducted under consistent conditions-generating 100,000 tokens per model with
a fixed batch size of four-for fair comparison. This research presents a
detailed analysis of diffusion-based vs. autoregressive models, highlighting
trade-offs in generative quality and efficiency. Findings emphasize both the
promise and limitations of diffusion models for discrete data, supporting
future work in non-autoregressive language generation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [130] [Digital Wargames to Enhance Military Medical Evacuation Decision-Making](https://arxiv.org/abs/2507.06373)
*Jeremy Fischer,Ram Krishnamoorthy,Vishal Kumar,Mahdi Al-Husseini*

Main category: cs.AI

TL;DR: 本文介绍了一种名为医疗疏散兵棋推演(MEWI)的三维多人模拟工具，旨在提高军事医疗疏散规划和决策训练的效果。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够模拟复杂医疗疏散网络的训练工具，而医疗疏散任务对于军队至关重要。本文的目的是开发一个高保真训练工具来填补这一空白。

Method: 使用Unity开发了一个三维多人模拟环境(MEWI)，模拟战场约束条件和不确定性，并设计了两种作战场景供学员参与，即太平洋岛屿两栖攻击和欧亚大陆冲突。

Result: 数据和调查结果显示，使用MEWI显著提升了学员对医疗疏散相关知识的掌握以及协作决策能力。

Conclusion: MEWI是医疗教育高保真训练工具领域的重大进步，能为提升医疗疏散教育与联合部队行动提供关键见解。

Abstract: Medical evacuation is one of the United States Army's most storied and
critical mission sets, responsible for efficiently and expediently evacuating
the battlefield ill and injured. Medical evacuation planning involves designing
a robust network of medical platforms and facilities capable of moving and
treating large numbers of casualties. Until now, there has not been a medium to
simulate these networks in a classroom setting and evaluate both offline
planning and online decision-making performance. This work describes the
Medical Evacuation Wargaming Initiative (MEWI), a three-dimensional multiplayer
simulation developed in Unity that replicates battlefield constraints and
uncertainties. MEWI accurately models patient interactions at casualty
collection points, ambulance exchange points, medical treatment facilities, and
evacuation platforms. Two operational scenarios are introduced: an amphibious
island assault in the Pacific and a Eurasian conflict across a sprawling road
and river network. These scenarios pit students against the clock to save as
many casualties as possible while adhering to doctrinal lessons learned during
didactic training. We visualize performance data collected from two iterations
of the MEWI Pacific scenario executed in the United States Army's Medical
Evacuation Doctrine Course. We consider post-wargame Likert survey data from
student participants and external observer notes to identify key planning
decision points, document medical evacuation lessons learned, and quantify
general utility. Results indicate that MEWI participation substantially
improves uptake of medical evacuation lessons learned and co-operative
decision-making. MEWI is a substantial step forward in the field of
high-fidelity training tools for medical education, and our study findings
offer critical insights into improving medical evacuation education and
operations across the joint force.

</details>


### [131] [Representing Prompting Patterns with PDL: Compliance Agent Case Study](https://arxiv.org/abs/2507.06396)
*Mandana Vaziri,Louis Mandel,Yuji Watanabe,Hirokuni Kitahara,Martin Hirzel,Anca Sailer*

Main category: cs.AI

TL;DR: 论文提出了一个名为Prompt Declaration Language（PDL）的新方法，用于简化复杂的提示工程问题，并显著提升提示调整的灵活性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的提示工程框架过于复杂，或者隐藏在限制性API后，或者使用不可自定义的固定模式，阻碍了复杂的编程实现。本研究希望通过改进提示表示方式解决这些问题。

Method: 提出了一种新型的提示表示方式——Prompt Declaration Language（PDL），该语言将LLM调用、规则代码和外部工具的组合进行了统一表示，同时支持手动和自动提示调优，并具备面向优化的声明式特性。

Result: 通过对一个实际合规代理的案例研究，调整PDL中的提示模式相比使用固定代理和提示模式可实现性能提升高达4倍。

Conclusion: PDL改进了提示表示和工程流程，不仅提升编程生产力，还通过优化提示模式实现显著性能提升。

Abstract: Prompt engineering for LLMs remains complex, with existing frameworks either
hiding complexity behind restrictive APIs or providing inflexible canned
patterns that resist customization -- making sophisticated agentic programming
challenging. We present the Prompt Declaration Language (PDL), a novel approach
to prompt representation that tackles this fundamental complexity by bringing
prompts to the forefront, enabling manual and automatic prompt tuning while
capturing the composition of LLM calls together with rule-based code and
external tools. By abstracting away the plumbing for such compositions, PDL
aims at improving programmer productivity while providing a declarative
representation that is amenable to optimization. This paper demonstrates PDL's
utility through a real-world case study of a compliance agent. Tuning the
prompting pattern of this agent yielded up to 4x performance improvement
compared to using a canned agent and prompt pattern.

</details>


### [132] [Jolting Technologies: Superexponential Acceleration in AI Capabilities and Implications for AGI](https://arxiv.org/abs/2507.06398)
*David Orban*

Main category: cs.AI

TL;DR: 该论文探讨了“震荡技术假设”，即AI能力发展的超指数增长，并通过蒙特卡洛模拟验证检测方法，旨在为未来研究和政策提供见解。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在理解AI能力是否以正三阶导数（加速度增加）的方式增长，为未来人工智能发展和AGI出现可能带来的影响提供理论支持。

Method: 作者提出了一个理论框架，并用蒙特卡洛模拟验证假设检测方法，尽管尚未获得适当的纵向数据进行实证验证。

Result: 研究着重创建稳健的工具用于未来的实证研究，并探讨缩短的创意至行动周期和迭代的AI改进如何驱动超指数增长模式。

Conclusion: 研究通过形式化震荡动态和验证检测方法，为理解AI发展路径和其对AGI出现的影响提供了必要的数学基础。

Abstract: This paper investigates the Jolting Technologies Hypothesis, which posits
superexponential growth (increasing acceleration, or a positive third
derivative) in the development of AI capabilities. We develop a theoretical
framework and validate detection methodologies through Monte Carlo simulations,
while acknowledging that empirical validation awaits suitable longitudinal
data. Our analysis focuses on creating robust tools for future empirical
studies and exploring the potential implications should the hypothesis prove
valid. The study examines how factors such as shrinking idea-to-action
intervals and compounding iterative AI improvements drive this jolting pattern.
By formalizing jolt dynamics and validating detection methods through
simulation, this work provides the mathematical foundation necessary for
understanding potential AI trajectories and their consequences for AGI
emergence, offering insights for research and policy.

</details>


### [133] [Comparing Dialectical Systems: Contradiction and Counterexample in Belief Change (Extended Version)](https://arxiv.org/abs/2507.06798)
*Uri Andrews,Luca San Mauro*

Main category: cs.AI

TL;DR: 研究证明了q-辩证系统比p-辩证系统更强大，而p-辩证系统又比d-辩证系统更强大。


<details>
  <summary>Details</summary>
Motivation: 探究和比较不同类型辩证系统的能力，以阐明其在自动化信念修正和知识动态管理中的作用。

Method: 通过数学证明阐明三种辩证系统之间的能力关系，并特别关注q-辩证系统的相对优势。

Result: 证明了q-辩证系统具有严格更高的能力，且p-辩证系统能力高于d-辩证系统。

Conclusion: 补充并解决了文献中未解的问题，突出了反例和矛盾在信念修订与数学推理过程中的重要性。

Abstract: Dialectical systems are a mathematical formalism for modeling an agent
updating a knowledge base seeking consistency. Introduced in the 1970s by
Roberto Magari, they were originally conceived to capture how a working
mathematician or a research community refines beliefs in the pursuit of truth.
Dialectical systems also serve as natural models for the belief change of an
automated agent, offering a unifying, computable framework for dynamic belief
management.
  The literature distinguishes three main models of dialectical systems:
(d-)dialectical systems based on revising beliefs when they are seen to be
inconsistent, p-dialectical systems based on revising beliefs based on finding
a counterexample, and q-dialectical systems which can do both. We answer an
open problem in the literature by proving that q-dialectical systems are
strictly more powerful than p-dialectical systems, which are themselves known
to be strictly stronger than (d-)dialectical systems. This result highlights
the complementary roles of counterexample and contradiction in automated belief
revision, and thus also in the reasoning processes of mathematicians and
research communities.

</details>


### [134] [SCC-recursiveness in infinite argumentation (extended version)](https://arxiv.org/abs/2507.06852)
*Uri Andrews,Luca San Mauro*

Main category: cs.AI

TL;DR: 本文探讨了将SCC递归语义方法扩展到无限论证框架中的可能性。


<details>
  <summary>Details</summary>
Motivation: 为解决现有SCC递归语义在无限论证框架中存在的可靠性问题。

Method: 提出两种扩展SCC递归性的方法，并通过Baroni和Giacomin的标准系统性评估这些方法。

Result: 发现方向性在无限论证框架中普遍失败，但在有限框架中部分语义能够满足方向性条件。

Conclusion: 该研究推动了无限论证理论的发展，并为处理无界或动态领域的推理系统奠定基础。

Abstract: Argumentation frameworks (AFs) are a foundational tool in artificial
intelligence for modeling structured reasoning and conflict. SCC-recursiveness
is a well-known design principle in which the evaluation of arguments is
decomposed according to the strongly connected components (SCCs) of the attack
graph, proceeding recursively from "higher" to "lower" components. While
SCC-recursive semantics such as \cft and \stgt have proven effective for finite
AFs, Baumann and Spanring showed the failure of SCC-recursive semantics to
generalize reliably to infinite AFs due to issues with well-foundedness.
  We propose two approaches to extending SCC-recursiveness to the infinite
setting. We systematically evaluate these semantics using Baroni and Giacomin's
established criteria, showing in particular that directionality fails in
general. We then examine these semantics' behavior in finitary frameworks,
where we find some of our semantics satisfy directionality. These results
advance the theory of infinite argumentation and lay the groundwork for
reasoning systems capable of handling unbounded or evolving domains.

</details>


### [135] [Scaling Towards the Information Boundary of Instruction Set: InfinityInstruct-Subject Technical Report](https://arxiv.org/abs/2507.06968)
*Li Du,Hanyu Zhao,Yiming Ju,Tengfei Pan*

Main category: cs.AI

TL;DR: 本文提出了一个系统化的指令数据构建框架，并基于此框架构建了高质量的指令数据集InfinityInstruct-Subject (~150万条指令)，实验结果表明可提升模型的指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现有的指令数据集仍难以处理复杂指令以及稀有领域任务，需提高其任务类型和知识领域覆盖率（coverage）以及指令复杂度（depth）。

Method: 提出了一个闭环的指令数据构建框架，包括分层标签系统、信息性种子选择算法、演化的数据合成过程，以及基于模型缺陷诊断的定向数据生成。

Result: 构建了含约150万指令的高质量数据集InfinityInstruct-Subject，在多种模型和基准任务上实验表明其提高了模型的指令遵循能力。

Conclusion: 理论与实践上推动了指令数据从数量扩展向质量提升的高效、持续演化，为未来相关研究奠定了基础。

Abstract: Instruction tuning has become a foundation for unlocking the capabilities of
large-scale pretrained models and improving their performance on complex tasks.
Thus, the construction of high-quality instruction datasets is crucial for
enhancing model performance and generalizability. Although current instruction
datasets have reached tens of millions of samples, models finetuned on them may
still struggle with complex instruction following and tasks in rare domains.
This is primarily due to limited expansion in both ``coverage'' (coverage of
task types and knowledge areas) and ``depth'' (instruction complexity) of the
instruction set. To address this issue, we propose a systematic instruction
data construction framework, which integrates a hierarchical labeling system,
an informative seed selection algorithm, an evolutionary data synthesis
process, and a model deficiency diagnosis with targeted data generation. These
components form an iterative closed-loop to continuously enhance the coverage
and depth of instruction data. Based on this framework, we construct
InfinityInstruct-Subject, a high-quality dataset containing ~1.5 million
instructions. Experiments on multiple foundation models and benchmark tasks
demonstrate its effectiveness in improving instruction-following capabilities.
Further analyses suggest that InfinityInstruct-Subject shows enlarged coverage
and depth compared to comparable synthesized instruction datasets. Our work
lays a theoretical and practical foundation for the efficient, continuous
evolution of instruction datasets, moving from data quantity expansion to
qualitative improvement.

</details>


### [136] [The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced Planning, Navigation, and Dynamic Adaptation](https://arxiv.org/abs/2507.06993)
*Jieren Deng,Aleksandar Cvetkovic,Pak Kiu Chung,Dragomir Yankov,Chiqun Zhang*

Main category: cs.AI

TL;DR: 提出了一种包含三个协作代理的新系统，用于提升旅行规划、导航和行程调整能力，有效应对动态和复杂的旅行场景。


<details>
  <summary>Details</summary>
Motivation: 传统旅行规划系统在处理复杂的现实场景（如环境变化和行程中断）上表现不足，导致用户体验受挫。

Method: 引入三个协作代理：旅行规划代理（提供复杂多模式查询处理）、终点导航代理（细化最后100米导航）、本地发现代理（利用图像嵌入和检索增强生成应对行程中断）。

Result: 新系统显著提升了查询解释、导航准确性和行程中断应变能力。

Conclusion: 该系统展示了从城市探索到应急响应等领域的广阔应用前景，同时改善了用户体验。

Abstract: Traditional travel-planning systems are often static and fragmented, leaving
them ill-equipped to handle real-world complexities such as evolving
environmental conditions and unexpected itinerary disruptions. In this paper,
we identify three gaps between existing service providers causing frustrating
user experience: intelligent trip planning, precision "last-100-meter"
navigation, and dynamic itinerary adaptation. We propose three cooperative
agents: a Travel Planning Agent that employs grid-based spatial grounding and
map analysis to help resolve complex multi-modal user queries; a Destination
Assistant Agent that provides fine-grained guidance for the final navigation
leg of each journey; and a Local Discovery Agent that leverages image
embeddings and Retrieval-Augmented Generation (RAG) to detect and respond to
trip plan disruptions. With evaluations and experiments, our system
demonstrates substantial improvements in query interpretation, navigation
accuracy, and disruption resilience, underscoring its promise for applications
from urban exploration to emergency response.

</details>


### [137] [First Return, Entropy-Eliciting Explore](https://arxiv.org/abs/2507.07017)
*Tianyu Zheng,Tianshun Xing,Qingshui Gu,Taoran Liang,Xingwei Qu,Xin Zhou,Yizhi Li,Zhoufutu Wen,Chenghua Lin,Wenhao Huang,Qian Liu,Ge Zhang,Zejun Ma*

Main category: cs.AI

TL;DR: 提出了FR3E框架，主要针对现有强化学习在探索阶段的不稳定性，通过结构化方法改善LLMs推理能力，实验效果显著。


<details>
  <summary>Details</summary>
Motivation: 强化学习在改进LLMs推理能力中效果显著，但探索阶段存在不稳定性，需要更好的探索方法。

Method: 提出FR3E框架，辨别推理轨迹中的高不确定性决策点，进行针对性rollouts，从而生成语义基础反馈，无需依赖稠密监督。

Result: 实验表明，FR3E框架改善训练稳定性，生成更长且连贯的回应，提升了完全正确轨迹占比。

Conclusion: FR3E通过结构化探索框架显著提高了大型语言模型的推理能力，验证了方法的有效性。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning
abilities of Large Language Models (LLMs) but it struggles with unstable
exploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a
structured exploration framework that identifies high-uncertainty decision
points in reasoning trajectories and performs targeted rollouts to construct
semantically grounded intermediate feedback. Our method provides targeted
guidance without relying on dense supervision. Empirical results on
mathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable
training, produces longer and more coherent responses, and increases the
proportion of fully correct trajectories. These results highlight the
framework's effectiveness in improving LLM reasoning through more robust and
structured exploration.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [138] [Energy-Efficient Supervised Learning with a Binary Stochastic Forward-Forward Algorithm](https://arxiv.org/abs/2507.06461)
*Risi Jaiswal,Supriyo Datta,Joseph G. Makin*

Main category: cs.LG

TL;DR: 本文提出了一种新算法，旨在提高神经网络的能效，特别是替代反向传播以克服定制硬件加速器中的若干挑战。


<details>
  <summary>Details</summary>
Motivation: 随着现代机器学习需要更大的神经网络，能耗迅速增加，而目前的训练方法如反向传播难以高效支持硬件加速。因此需要探索新的高能效多样性算法。

Method: 提出一种针对二值随机单元的前向-前向算法，利用索引操作替代矩阵乘法，同时结合随机性和相同权值策略突破信息瓶颈。此外，采用不稳定磁体制成的p-bit快速实现二值采样以降低硬件复杂度。

Result: 算法在MNIST、Fashion-MNIST和CIFAR-10数据集上进行了评估，性能接近实数前向-前向算法，同时实现了大约一个数量级的能量节省。

Conclusion: 无论在传统硬件还是新颖硬件上，所提出的算法展示了大幅度节省能量的潜力，同时在性能上保持竞争力，对能效优化具有重要意义。

Abstract: Reducing energy consumption has become a pressing need for modern machine
learning, which has achieved many of its most impressive results by scaling to
larger and more energy-consumptive neural networks. Unfortunately, the main
algorithm for training such networks, backpropagation, poses significant
challenges for custom hardware accelerators, due to both its serial
dependencies and the memory footprint needed to store forward activations for
the backward pass. Alternatives to backprop, although less effective, do exist;
here the main computational bottleneck becomes matrix multiplication. In this
study, we derive forward-forward algorithms for binary, stochastic units.
Binarization of the activations transforms matrix multiplications into indexing
operations, which can be executed efficiently in hardware. Stochasticity,
combined with tied weights across units with different biases, bypasses the
information bottleneck imposed by binary units. Furthermore, although slow and
expensive in traditional hardware, binary sampling that is very fast can be
implemented cheaply with p-bits (probabilistic bits), novel devices made up of
unstable magnets. We evaluate our proposed algorithms on the MNIST,
Fashion-MNIST, and CIFAR-10 datasets, showing that its performance is close to
real-valued forward-forward, but with an estimated energy savings of about one
order of magnitude.

</details>


### [139] [Neural Network-Based Parameter Estimation for Non-Autonomous Differential Equations with Discontinuous Signals](https://arxiv.org/abs/2507.06267)
*Hyeontae Jo,Krešimir Josić,Jae Kyoung Kim*

Main category: cs.LG

TL;DR: HADES-NN 提出了一种利用神经网络对不连续外部信号进行光滑化，然后估算模型参数的两阶段方法，可精准拟合复杂环境下的非自治微分方程模型。


<details>
  <summary>Details</summary>
Motivation: 针对不连续外部信号导致数据拟合困难的问题，提出解决方案。

Method: 利用神经网络通过两阶段方法：先将不连续信号光滑化，再用其估算模型参数。

Result: 在实际应用中，包括昼夜节律系统和酵母对信息素响应等，HADES-NN 展示了高精确性与高精密性。

Conclusion: 该方法显著扩展了能够拟合实际测量数据的模型系统范围。

Abstract: Non-autonomous differential equations are crucial for modeling systems
influenced by external signals, yet fitting these models to data becomes
particularly challenging when the signals change abruptly. To address this
problem, we propose a novel parameter estimation method utilizing functional
approximations with artificial neural networks. Our approach, termed Harmonic
Approximation of Discontinuous External Signals using Neural Networks
(HADES-NN), operates in two iterated stages. In the first stage, the algorithm
employs a neural network to approximate the discontinuous signal with a smooth
function. In the second stage, it uses this smooth approximate signal to
estimate model parameters. HADES-NN gives highly accurate and precise parameter
estimates across various applications, including circadian clock systems
regulated by external light inputs measured via wearable devices and the mating
response of yeast to external pheromone signals. HADES-NN greatly extends the
range of model systems that can be fit to real-world measurements.

</details>


### [140] [Sample-Efficient Reinforcement Learning Controller for Deep Brain Stimulation in Parkinson's Disease](https://arxiv.org/abs/2507.06326)
*Harsh Ravivarapu,Gaurav Bagwe,Xiaoyong Yuan,Chunxiu Yu,Lan Zhang*

Main category: cs.LG

TL;DR: 提出了SEA-DBS，一个用于自适应深脑刺激（aDBS）的高效强化学习框架，解决现有方法在样本效率、探索稳定性和硬件兼容性方面的问题。


<details>
  <summary>Details</summary>
Motivation: 传统开放回路的深脑刺激系统在帕金森病治疗中缺乏适应性、能效低且个性化不足，而自适应深脑刺激（aDBS）需要更高效和稳定的控制方法。

Method: 提出了一种基于强化学习的SEA-DBS框架，结合了预测奖励模型减少实时反馈依赖，以及基于Gumbel Softmax的探索方法，提升样本效率和在限制性硬件上的可部署性。

Result: 在帕金森基底神经节生物仿真上，SEA-DBS实现了更快的收敛速度，更有效的病理性β波功率抑制及对FP16量化后的稳健性。

Conclusion: SEA-DBS为实时且资源受限的自适应深脑刺激提供了一个实用高效的框架。

Abstract: Deep brain stimulation (DBS) is an established intervention for Parkinson's
disease (PD), but conventional open-loop systems lack adaptability, are
energy-inefficient due to continuous stimulation, and provide limited
personalization to individual neural dynamics. Adaptive DBS (aDBS) offers a
closed-loop alternative, using biomarkers such as beta-band oscillations to
dynamically modulate stimulation. While reinforcement learning (RL) holds
promise for personalized aDBS control, existing methods suffer from high sample
complexity, unstable exploration in binary action spaces, and limited
deployability on resource-constrained hardware.
  We propose SEA-DBS, a sample-efficient actor-critic framework that addresses
the core challenges of RL-based adaptive neurostimulation. SEA-DBS integrates a
predictive reward model to reduce reliance on real-time feedback and employs
Gumbel Softmax-based exploration for stable, differentiable policy updates in
binary action spaces. Together, these components improve sample efficiency,
exploration robustness, and compatibility with resource-constrained
neuromodulatory hardware. We evaluate SEA-DBS on a biologically realistic
simulation of Parkinsonian basal ganglia activity, demonstrating faster
convergence, stronger suppression of pathological beta-band power, and
resilience to post-training FP16 quantization. Our results show that SEA-DBS
offers a practical and effective RL-based aDBS framework for real-time,
resource-constrained neuromodulation.

</details>


### [141] [SymFlux: deep symbolic regression of Hamiltonian vector fields](https://arxiv.org/abs/2507.06342)
*M. A. Evangelista-Alvarado,P. Suárez-Serrato*

Main category: cs.LG

TL;DR: SymFlux 是一种通过符号回归从矢量场中识别哈密顿函数的深度学习框架。


<details>
  <summary>Details</summary>
Motivation: 旨在通过深度学习框架简化并自动化哈密顿力学中哈密顿函数的发现过程。

Method: 结合使用混合 CNN-LSTM 架构训练模型，从哈密顿矢量场数据中提取符号表达式。

Result: 模型成功准确地恢复了哈密顿函数的符号表达式，展现了卓越的效果。

Conclusion: SymFlux 推进了哈密顿力学中自动化发现的能力。

Abstract: We present SymFlux, a novel deep learning framework that performs symbolic
regression to identify Hamiltonian functions from their corresponding vector
fields on the standard symplectic plane. SymFlux models utilize hybrid CNN-LSTM
architectures to learn and output the symbolic mathematical expression of the
underlying Hamiltonian. Training and validation are conducted on newly
developed datasets of Hamiltonian vector fields, a key contribution of this
work. Our results demonstrate the model's effectiveness in accurately
recovering these symbolic expressions, advancing automated discovery in
Hamiltonian mechanics.

</details>


### [142] [DecoyDB: A Dataset for Graph Contrastive Learning in Protein-Ligand Binding Affinity Prediction](https://arxiv.org/abs/2507.06366)
*Yupu Zhang,Zelin Xu,Tingsong Xiao,Gustavo Seabra,Yanjun Li,Chenglong Li,Zhe Jiang*

Main category: cs.LG

TL;DR: 该论文提出一种新的大型数据集DecoyDB以及相应的图对比学习（GCL）框架，用于预测蛋白质-配体结合亲和力，并显著提高了模型精度与泛化性能。


<details>
  <summary>Details</summary>
Motivation: 当前蛋白质-配体结合亲和力预测缺乏大规模高质量的标注数据集，如PDBbind的样本量小于2万，亟需一种高效利用未标注数据的方法。

Method: 提出了DecoyDB数据集，包含高分辨率的真实结构和多种计算生成的虚假绑定位点，同时开发定制的图对比学习（GCL）框架进行模型预训练和微调。

Result: 通过DecoyDB预训练的模型，在精度、标签效率及泛化能力等多方面都优于基线模型。

Conclusion: DecoyDB和定制GCL框架为蛋白质-配体结合亲和力预测提供了新的方法，特别解决了数据稀缺的关键问题。

Abstract: Predicting the binding affinity of protein-ligand complexes plays a vital
role in drug discovery. Unfortunately, progress has been hindered by the lack
of large-scale and high-quality binding affinity labels. The widely used
PDBbind dataset has fewer than 20K labeled complexes. Self-supervised learning,
especially graph contrastive learning (GCL), provides a unique opportunity to
break the barrier by pre-training graph neural network models based on vast
unlabeled complexes and fine-tuning the models on much fewer labeled complexes.
However, the problem faces unique challenges, including a lack of a
comprehensive unlabeled dataset with well-defined positive/negative complex
pairs and the need to design GCL algorithms that incorporate the unique
characteristics of such data. To fill the gap, we propose DecoyDB, a
large-scale, structure-aware dataset specifically designed for self-supervised
GCL on protein-ligand complexes. DecoyDB consists of high-resolution ground
truth complexes (less than 2.5 Angstrom) and diverse decoy structures with
computationally generated binding poses that range from realistic to suboptimal
(negative pairs). Each decoy is annotated with a Root Mean Squared Deviation
(RMSD) from the native pose. We further design a customized GCL framework to
pre-train graph neural networks based on DecoyDB and fine-tune the models with
labels from PDBbind. Extensive experiments confirm that models pre-trained with
DecoyDB achieve superior accuracy, label efficiency, and generalizability.

</details>


### [143] [The Riemannian Geometry associated to Gradient Flows of Linear Convolutional Networks](https://arxiv.org/abs/2507.06367)
*El Mehdi Achour,Kathlén Kohn,Holger Rauhut*

Main category: cs.LG

TL;DR: 研究表明线性卷积网络的渐变流在参数空间中总能被表述为函数空间上的黎曼渐变流。


<details>
  <summary>Details</summary>
Motivation: 探索深度线性卷积网络学习中渐变流的几何性质，尤其是其在参数空间和函数空间的关系。

Method: 利用数学分析，将参数空间中的梯度流表示为函数空间中的黎曼梯度流，并探讨初始条件的影响。

Result: 证明对于二维及以上维的卷积网络，无论初始条件如何，梯度流均可转化为函数空间上的黎曼梯度流。一维情况下，需满足步长大于1的条件。

Conclusion: 初始条件虽然影响黎曼度量，但不会改变参数空间到函数空间映射的本质特性。

Abstract: We study geometric properties of the gradient flow for learning deep linear
convolutional networks. For linear fully connected networks, it has been shown
recently that the corresponding gradient flow on parameter space can be written
as a Riemannian gradient flow on function space (i.e., on the product of weight
matrices) if the initialization satisfies a so-called balancedness condition.
We establish that the gradient flow on parameter space for learning linear
convolutional networks can be written as a Riemannian gradient flow on function
space regardless of the initialization. This result holds for $D$-dimensional
convolutions with $D \geq 2$, and for $D =1$ it holds if all so-called strides
of the convolutions are greater than one. The corresponding Riemannian metric
depends on the initialization.

</details>


### [144] [Secure and Storage-Efficient Deep Learning Models for Edge AI Using Automatic Weight Generation](https://arxiv.org/abs/2507.06380)
*Habibur Rahaman,Atri Chatterjee,Swarup Bhunia*

Main category: cs.LG

TL;DR: WINGs框架通过动态生成神经网络权重和压缩CNN权重，大幅降低存储需求，且几乎无精度损失，同时提高安全性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂神经网络权重存储需求大问题，适配资源有限的边缘设备。

Method: 使用主成分分析（PCA）进行降维，轻量级支持向量回归（SVR）模型预测全连接网络权重，并结合敏感性分析进行CNN权重的选择性压缩与安全性提升。

Result: 实现全连接层53倍、AlexNet在MNIST数据集28倍、CIFAR-10数据集18倍的压缩率，精度损失仅1-2%。

Conclusion: WINGs框架显著降低了DNN推理的内存需求，提高了吞吐量和能效，为边缘设备提供了有吸引力的解决方案。

Abstract: Complex neural networks require substantial memory to store a large number of
synaptic weights. This work introduces WINGs (Automatic Weight Generator for
Secure and Storage-Efficient Deep Learning Models), a novel framework that
dynamically generates layer weights in a fully connected neural network (FC)
and compresses the weights in convolutional neural networks (CNNs) during
inference, significantly reducing memory requirements without sacrificing
accuracy. WINGs framework uses principal component analysis (PCA) for
dimensionality reduction and lightweight support vector regression (SVR) models
to predict layer weights in the FC networks, removing the need for storing
full-weight matrices and achieving substantial memory savings. It also
preferentially compresses the weights in low-sensitivity layers of CNNs using
PCA and SVR with sensitivity analysis. The sensitivity-aware design also offers
an added level of security, as any bit-flip attack with weights in compressed
layers has an amplified and readily detectable effect on accuracy. WINGs
achieves 53x compression for the FC layers and 28x for AlexNet with MNIST
dataset, and 18x for Alexnet with CIFAR-10 dataset with 1-2% accuracy loss.
This significant reduction in memory results in higher throughput and lower
energy for DNN inference, making it attractive for resource-constrained edge
applications.

</details>


### [145] [KPFlow: An Operator Perspective on Dynamic Collapse Under Gradient Descent Training of Recurrent Networks](https://arxiv.org/abs/2507.06381)
*James Hazelden,Laura Driscoll,Eli Shlizerman,Eric Shea-Brown*

Main category: cs.LG

TL;DR: 本文探讨了梯度下降在非线性循环模型（如RNNs和GRUs）中的作用，并提供了一种KP分解方式来分析其学习表现。


<details>
  <summary>Details</summary>
Motivation: 需要理论工具来理解训练过程中学习的表示在非线性、有限模型中的形成机制。

Method: 提出一种基于梯度流的分解方法，将其分解为参数算子K和线性化流传播算子P，并分析其对模型动态的影响。

Result: 证明了低维潜在动态是由网络结构驱动的，并且提出了度量多任务训练问题对齐度的方法。还设计了一个Pytorch工具包支持这一分析。

Conclusion: 研究拓展了对于梯度下降在非线性循环模型中学习机制的理解，为构建下一阶段理论工具奠定了基础。

Abstract: Gradient Descent (GD) and its variants are the primary tool for enabling
efficient training of recurrent dynamical systems such as Recurrent Neural
Networks (RNNs), Neural ODEs and Gated Recurrent units (GRUs). The dynamics
that are formed in these models exhibit features such as neural collapse and
emergence of latent representations that may support the remarkable
generalization properties of networks. In neuroscience, qualitative features of
these representations are used to compare learning in biological and artificial
systems. Despite recent progress, there remains a need for theoretical tools to
rigorously understand the mechanisms shaping learned representations,
especially in finite, non-linear models. Here, we show that the gradient flow,
which describes how the model's dynamics evolve over GD, can be decomposed into
a product that involves two operators: a Parameter Operator, K, and a
Linearized Flow Propagator, P. K mirrors the Neural Tangent Kernel in
feed-forward neural networks, while P appears in Lyapunov stability and optimal
control theory. We demonstrate two applications of our decomposition. First, we
show how their interplay gives rise to low-dimensional latent dynamics under
GD, and, specifically, how the collapse is a result of the network structure,
over and above the nature of the underlying task. Second, for multi-task
training, we show that the operators can be used to measure how objectives
relevant to individual sub-tasks align. We experimentally and theoretically
validate these findings, providing an efficient Pytorch package, \emph{KPFlow},
implementing robust analysis tools for general recurrent architectures. Taken
together, our work moves towards building a next stage of understanding of GD
learning in non-linear recurrent models.

</details>


### [146] [Detection of Intelligent Tampering in Wireless Electrocardiogram Signals Using Hybrid Machine Learning](https://arxiv.org/abs/2507.06402)
*Siddhant Deshpande,Yalemzerf Getnet,Waltenegus Dargie*

Main category: cs.LG

TL;DR: 研究对无线ECG系统的信号篡改检测和身份验证，提出了不同的模型分析和验证方法，部分模型达到了极高的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着无线ECG系统的普及，保护信号完整性免受篡改变得尤为重要，因此需要研究针对信号篡改的检测方法及身份验证技术的有效性。

Method: 采用不同深度学习模型（CNN、ResNet、混合Transformer-CNN）检测信号篡改，并使用基于Siamese网络的模型进行身份验证，同时通过CWT将一维信号转化为二维时频域表示。

Result: 实验显示在复杂的篡改场景下，CNN和混合模型准确率超过99.5%；在较细微的篡改下，FeatCNN-TranCNN平均达98%准确率；而纯Transformer-Siamese网络在身份验证上达98.30%平均准确率，混合模型实现100%验证准确率。

Conclusion: 提出的模型对ECG信号篡改检测和身份验证展现了卓越性能，为未来非临床环境下的健康监控系统提供了重要技术支持。

Abstract: With the proliferation of wireless electrocardiogram (ECG) systems for health
monitoring and authentication, protecting signal integrity against tampering is
becoming increasingly important. This paper analyzes the performance of CNN,
ResNet, and hybrid Transformer-CNN models for tamper detection. It also
evaluates the performance of a Siamese network for ECG based identity
verification. Six tampering strategies, including structured segment
substitutions and random insertions, are emulated to mimic real world attacks.
The one-dimensional ECG signals are transformed into a two dimensional
representation in the time frequency domain using the continuous wavelet
transform (CWT). The models are trained and evaluated using ECG data from 54
subjects recorded in four sessions 2019 to 2025 outside of clinical settings
while the subjects performed seven different daily activities. Experimental
results show that in highly fragmented manipulation scenarios, CNN,
FeatCNN-TranCNN, FeatCNN-Tran and ResNet models achieved an accuracy exceeding
99.5 percent . Similarly, for subtle manipulations (for example, 50 percent
from A and 50 percent from B and, 75 percent from A and 25 percent from B
substitutions) our FeatCNN-TranCNN model demonstrated consistently reliable
performance, achieving an average accuracy of 98 percent . For identity
verification, the pure Transformer-Siamese network achieved an average accuracy
of 98.30 percent . In contrast, the hybrid CNN-Transformer Siamese model
delivered perfect verification performance with 100 percent accuracy.

</details>


### [147] [Bridging Data Gaps of Rare Conditions in ICU: A Multi-Disease Adaptation Approach for Clinical Prediction](https://arxiv.org/abs/2507.06432)
*Mingcheng Zhu,Yu Liu,Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: 本文介绍了KnowRare，这是一种基于领域适配的深度学习框架，旨在解决ICU中罕见疾病临床预测中的数据稀缺和异质性问题，显示了其在五个任务上的卓越预测表现。


<details>
  <summary>Details</summary>
Motivation: 罕见疾病和ICU低患病率条件因数据稀缺和异质性问题，导致在临床决策中的支持不足，亟需有效解决方案来提升预测性能。

Method: KnowRare通过自监督预训练学习病症无关的表征，结合病情知识图谱选择性地从相似临床条件中引入知识来应对数据稀缺和条件内异质性问题。

Result: 在两个ICU数据集的五个任务上，KnowRare超越了现有的最新模型和传统ICU评分系统（如APACHE IV等），且在各种情况下表现优良。

Conclusion: KnowRare在支持临床决策和提高ICU罕见病护理质量方面潜力巨大，是一种可行且有力的解决方案。

Abstract: Artificial Intelligence has revolutionised critical care for common
conditions. Yet, rare conditions in the intensive care unit (ICU), including
recognised rare diseases and low-prevalence conditions in the ICU, remain
underserved due to data scarcity and intra-condition heterogeneity. To bridge
such gaps, we developed KnowRare, a domain adaptation-based deep learning
framework for predicting clinical outcomes for rare conditions in the ICU.
KnowRare mitigates data scarcity by initially learning condition-agnostic
representations from diverse electronic health records through self-supervised
pre-training. It addresses intra-condition heterogeneity by selectively
adapting knowledge from clinically similar conditions with a developed
condition knowledge graph. Evaluated on two ICU datasets across five clinical
prediction tasks (90-day mortality, 30-day readmission, ICU mortality,
remaining length of stay, and phenotyping), KnowRare consistently outperformed
existing state-of-the-art models. Additionally, KnowRare demonstrated superior
predictive performance compared to established ICU scoring systems, including
APACHE IV and IV-a. Case studies further demonstrated KnowRare's flexibility in
adapting its parameters to accommodate dataset-specific and task-specific
characteristics, its generalisation to common conditions under limited data
scenarios, and its rationality in selecting source conditions. These findings
highlight KnowRare's potential as a robust and practical solution for
supporting clinical decision-making and improving care for rare conditions in
the ICU.

</details>


### [148] [eegFloss: A Python package for refining sleep EEG recordings using machine learning models](https://arxiv.org/abs/2507.06433)
*Niloy Sikder,Paul Zerr,Mahdad Jafarzadeh Esfahani,Martin Dresler,Matthias Krauledat*

Main category: cs.LG

TL;DR: 本文提出了一种名为eegFloss的开源Python工具包，用于检测睡眠脑电图中的伪影，提高睡眠分期的准确性。


<details>
  <summary>Details</summary>
Motivation: 脑电图是睡眠研究的重要工具，但易受伪影干扰，影响自动睡眠分期的准确性，引出对伪影检测和数据可用性分析工具的需求。

Method: 提出eegFloss工具包，整合eegUsability和eegMobility两种机器学习模型，分别检测伪影和自动分析就寝时间，并利用手动标注数据验证性能。

Result: eegUsability展示出了优秀的分类性能（F1分数约为0.85，Cohen's kappa为0.78），伪影检测召回率高达94%，显著提高了EEG数据的可用性分析能力。

Conclusion: eegFloss工具为睡眠研究提供了有效解决方案，提升了分析精度和结果的可靠性，为改善大规模研究中的数据质量铺平了道路。

Abstract: Electroencephalography (EEG) allows monitoring of brain activity, providing
insights into the functional dynamics of various brain regions and their roles
in cognitive processes. EEG is a cornerstone in sleep research, serving as the
primary modality of polysomnography, the gold standard in the field. However,
EEG signals are prone to artifacts caused by both internal (device-specific)
factors and external (environmental) interferences. As sleep studies are
becoming larger, most rely on automatic sleep staging, a process highly
susceptible to artifacts, leading to erroneous sleep scores. This paper
addresses this challenge by introducing eegFloss, an open-source Python package
to utilize eegUsability, a novel machine learning (ML) model designed to detect
segments with artifacts in sleep EEG recordings. eegUsability has been trained
and evaluated on manually artifact-labeled EEG data collected from 15
participants over 127 nights using the Zmax headband. It demonstrates solid
overall classification performance (F1-score is approximately 0.85, Cohens
kappa is 0.78), achieving a high recall rate of approximately 94% in
identifying channel-wise usable EEG data, and extends beyond Zmax.
Additionally, eegFloss offers features such as automatic time-in-bed detection
using another ML model named eegMobility, filtering out certain artifacts, and
generating hypnograms and sleep statistics. By addressing a fundamental
challenge faced by most sleep studies, eegFloss can enhance the precision and
rigor of their analysis as well as the accuracy and reliability of their
outcomes.

</details>


### [149] [Can Interpretation Predict Behavior on Unseen Data?](https://arxiv.org/abs/2507.06445)
*Victoria R. Li,Jenny Kaufmann,Martin Wattenberg,David Alvarez-Melis,Naomi Saphra*

Main category: cs.LG

TL;DR: 本文探讨了解释性作为预测分布外模式行为工具的潜力与挑战，研究了注意力模式与分布外泛化的关系。


<details>
  <summary>Details</summary>
Motivation: 旨在解决传统解释性研究主要针对模型特定机制，较少预测模型在未见数据上的行为的问题。

Method: 使用数百个Transformer模型进行合成分类任务，分析其注意力模式与分布外泛化性能的关系，并通过相关分析与消融实验验证。

Result: 发现简单的观察性工具可以预测分布外表现，尤其是分布内层次化注意力模式可暗示分布外的层次化泛化行为。

Conclusion: 提供了一个概念验证，证明解释性工具可以在预测未见模型行为方面有潜力，推动更多相关研究。

Abstract: Interpretability research often aims to predict how a model will respond to
targeted interventions on specific mechanisms. However, it rarely predicts how
a model will respond to unseen input data. This paper explores the promises and
challenges of interpretability as a tool for predicting out-of-distribution
(OOD) model behavior. Specifically, we investigate the correspondence between
attention patterns and OOD generalization in hundreds of Transformer models
independently trained on a synthetic classification task. These models exhibit
several distinct systematic generalization rules OOD, forming a diverse
population for correlational analysis. In this setting, we find that simple
observational tools from interpretability can predict OOD performance. In
particular, when in-distribution attention exhibits hierarchical patterns, the
model is likely to generalize hierarchically on OOD data -- even when the
rule's implementation does not rely on these hierarchical patterns, according
to ablation tests. Our findings offer a proof-of-concept to motivate further
interpretability work on predicting unseen model behavior.

</details>


### [150] [FedPhD: Federated Pruning with Hierarchical Learning of Diffusion Models](https://arxiv.org/abs/2507.06449)
*Qianyu Long,Qiyuan Wang,Christos Anagnostopoulos,Daning Bi*

Main category: cs.LG

TL;DR: 提出了一种新的联邦学习方法FedPhD，用于高效训练扩散模型，有效解决了数据异质性和通信成本高的问题，并显著提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习环境中，扩散模型的训练面临数据异质性和高通信成本等挑战，且针对这些问题的研究有限。

Method: 提出FedPhD，利用分层联邦学习、同质性感知的模型聚合与选择策略，以及分布式结构化剪枝，解决数据异质性并降低通信成本。

Result: 实验结果表明，FedPhD在多个数据集上实现了更高性能的FID得分，通信成本降低了88%，并超越基准方法，FID提升了至少34%，资源消耗降低至56%。

Conclusion: FedPhD有效提高了联邦学习中扩散模型的训练效率及性能，同时显著降低了计算和通信资源的消耗。

Abstract: Federated Learning (FL), as a distributed learning paradigm, trains models
over distributed clients' data. FL is particularly beneficial for distributed
training of Diffusion Models (DMs), which are high-quality image generators
that require diverse data. However, challenges such as high communication costs
and data heterogeneity persist in training DMs similar to training Transformers
and Convolutional Neural Networks. Limited research has addressed these issues
in FL environments. To address this gap and challenges, we introduce a novel
approach, FedPhD, designed to efficiently train DMs in FL environments. FedPhD
leverages Hierarchical FL with homogeneity-aware model aggregation and
selection policy to tackle data heterogeneity while reducing communication
costs. The distributed structured pruning of FedPhD enhances computational
efficiency and reduces model storage requirements in clients. Our experiments
across multiple datasets demonstrate that FedPhD achieves high model
performance regarding Fr\'echet Inception Distance (FID) scores while reducing
communication costs by up to $88\%$. FedPhD outperforms baseline methods
achieving at least a $34\%$ improvement in FID, while utilizing only $56\%$ of
the total computation and communication resources.

</details>


### [151] [Automated Neuron Labelling Enables Generative Steering and Interpretability in Protein Language Models](https://arxiv.org/abs/2507.06458)
*Arjun Banerjee,David Martinez,Camille Dang,Ethan Tam*

Main category: cs.LG

TL;DR: 本研究提出了一种自动框架，为蛋白质语言模型（PLMs）的每个神经元提供生物学相关的自然语言描述，并揭示模型中的神经元对多种生化和结构特性有选择性敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有的蛋白质语言模型虽然能编码丰富的生物学信息，但其内部神经元表示的功能仍不清晰，需要一种高效且可扩展的方法来揭示这些神经元的生物学意义。

Method: 提出了一种自动化框架，为模型的每个神经元提供自然语言标签，并进一步开发基于神经元激活指导的方法来引导生成具备特定生化特性和结构的蛋白质。

Result: 证明了神经元对多样的生化和结构特性具有选择性敏感性，同时实现了生成具有目标生化性质和结构的蛋白质。此外，分析了不同规模模型中的神经元标记，发现了PLM的扩展规律及结构化的神经元空间分布。

Conclusion: 该方法不仅揭示了蛋白质语言模型的内部神经元功能，还开辟了基于模型神经元的理论分析和功能设计的新途径。

Abstract: Protein language models (PLMs) encode rich biological information, yet their
internal neuron representations are poorly understood. We introduce the first
automated framework for labeling every neuron in a PLM with biologically
grounded natural language descriptions. Unlike prior approaches relying on
sparse autoencoders or manual annotation, our method scales to hundreds of
thousands of neurons, revealing individual neurons are selectively sensitive to
diverse biochemical and structural properties. We then develop a novel neuron
activation-guided steering method to generate proteins with desired traits,
enabling convergence to target biochemical properties like molecular weight and
instability index as well as secondary and tertiary structural motifs,
including alpha helices and canonical Zinc Fingers. We finally show that
analysis of labeled neurons in different model sizes reveals PLM scaling laws
and a structured neuron space distribution.

</details>


### [152] [SoftSignSGD(S3): An Enhanced Optimizer for Practical DNN Training and Loss Spikes Minimization Beyond Adam](https://arxiv.org/abs/2507.06464)
*Hanyang Peng,Shuang Qin,Yue Yu,Fangqing Jiang,Hui Wang,Wen Gao*

Main category: cs.LG

TL;DR: 作者提出了一种新型优化器SignSoftSGD (S3)，通过改进Adam优化器的机制，提高了训练稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: Adam在处理神经网络训练中的梯度波动表现出色，但也存在因更新尺度失控导致的不稳定性。

Method: 设计了基于$p$阶动量的通用更新方式，减少损失尖峰的机制，并引入等效的Nesterov加速梯度模块。

Result: 在多种任务中，S3实现了更快收敛、更高性能，且即使使用10倍学习率，损失尖峰现象也很少发生。

Conclusion: S3优化器在效率和最终性能方面均优于AdamW，可在更少训练步骤下实现相当或更好的效果。

Abstract: Adam has proven remarkable successful in training deep neural networks, but
the mechanisms underlying its empirical successes and limitations remain
underexplored. In this study, we demonstrate that the effectiveness of Adam
stems largely from its similarity to SignSGD in robustly handling large
gradient fluctuations, yet it is also vulnerable to destabilizing loss spikes
due to its uncontrolled update scaling. To enhance the advantage of Adam and
mitigate its limitation, we propose SignSoftSGD (S3), a novel optimizer with
three key innovations. \emph{First}, S3 generalizes the sign-like update by
employing a flexible $p$-th order momentum ($p \geq 1$) in the denominator,
departing from the conventional second-order momentum (variance)
preconditioning. This design enables enhanced performance while achieving
stable training even with aggressive learning rates. \emph{Second}, S3
minimizes the occurrences of loss spikes through unified exponential moving
average coefficients for numerator and denominator momenta, which inherently
bound updates to $[-1, 1]$ and simplify hyperparameter tuning. \emph{Third}, S3
incorporates an equivalent Nesterov's accelerated gradient(NAG) module,
accelerating convergence without memory overhead. Theoretically, we prove that
S3 achieves the optimal convergence rate of
$O\left(\frac{1}{T^{\sfrac{1}{4}}}\right)$ for general nonconvex stochastic
optimization under weak assumptions. Extensive experiments across a range of
vision and language tasks show that \textsf{\small S3} not only converges more
rapidly and improves performance but also rarely experiences loss spikes, even
with a \textbf{$\bm{10 \times}$} larger learning rate. In fact, S3 delivers
performance comparable to or better than AdamW with \textbf{$2 \times$} the
training steps, establishing its efficacy in both efficiency and final task
performance.

</details>


### [153] [Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models](https://arxiv.org/abs/2507.06466)
*Aaron Dharna,Cong Lu,Jeff Clune*

Main category: cs.LG

TL;DR: 本文提出了一种新方法FMSP，通过利用基础模型来改进自博弈算法，解决了自博弈中缺乏多样性和局部最优的问题，开辟了策略探索的新方向。


<details>
  <summary>Details</summary>
Motivation: 传统自博弈常陷入局部最优，解决方案的多样性有限，需要一种方法来扩展解决方案的空间和质量。

Method: 提出了FMSP框架，包括三种方法：vFMSP、NSSP和QDSP，并在Car Tag和Gandalf模拟环境下验证其性能。

Result: FMSP方法展现了卓越的策略探索能力，超越了人类设计的策略，并成功破解和修复了LLM的防御漏洞。

Conclusion: FMSP充分利用基础模型的潜力，使自博弈过程更具创新性和开放性，代表了该领域一项重要进展。

Abstract: Multi-agent interactions have long fueled innovation, from natural
predator-prey dynamics to the space race. Self-play (SP) algorithms try to
harness these dynamics by pitting agents against ever-improving opponents,
thereby creating an implicit curriculum toward learning high-quality solutions.
However, SP often fails to produce diverse solutions and can get stuck in
locally optimal behaviors. We introduce Foundation-Model Self-Play (FMSP), a
new direction that leverages the code-generation capabilities and vast
knowledge of foundation models (FMs) to overcome these challenges by leaping
across local optima in policy space. We propose a family of approaches: (1)
\textbf{Vanilla Foundation-Model Self-Play (vFMSP)} continually refines agent
policies via competitive self-play; (2) \textbf{Novelty-Search Self-Play
(NSSP)} builds a diverse population of strategies, ignoring performance; and
(3) the most promising variant, \textbf{Quality-Diveristy Self-Play (QDSP)},
creates a diverse set of high-quality policies by combining the diversity of
NSSP and refinement of vFMSP. We evaluate FMSPs in Car Tag, a
continuous-control pursuer-evader setting, and in Gandalf, a simple AI safety
simulation in which an attacker tries to jailbreak an LLM's defenses. In Car
Tag, FMSPs explore a wide variety of reinforcement learning, tree search, and
heuristic-based methods, to name just a few. In terms of discovered policy
quality, \ouralgo and vFMSP surpass strong human-designed strategies. In
Gandalf, FMSPs can successfully automatically red-team an LLM, breaking through
and jailbreaking six different, progressively stronger levels of defense.
Furthermore, FMSPs can automatically proceed to patch the discovered
vulnerabilities. Overall, FMSPs represent a promising new research frontier of
improving self-play with foundation models, opening fresh paths toward more
creative and open-ended strategy discovery

</details>


### [154] [Mitigating Message Imbalance in Fraud Detection with Dual-View Graph Representation Learning](https://arxiv.org/abs/2507.06469)
*Yudan Song,Yuecen Wei,Yuhang Lu,Qingyun Sun,Minglai Shao,Li-e Wang,Chunming Hu,Xianxian Li,Xingcheng Fu*

Main category: cs.LG

TL;DR: 提出一种名为MimbFD的双视图图表示学习方法，用于解决图神经网络在欺诈检测中存在的监督信息不平衡问题，实验表明其性能卓越。


<details>
  <summary>Details</summary>
Motivation: 现有图表示学习方法主要侧重于局部交互，导致全局拓扑信息传递不平衡，特别是在欺诈检测任务中，欺诈与正常节点的不平衡性增加了节点信息丢失风险。

Method: 提出了一种双视图图表示学习方法：包括拓扑消息可达性模块以改善节点表征学习，以及局部混杂去偏模块以调整节点表征平衡不同类别的影响。

Result: 在三个公开的欺诈数据集上进行实验，结果表明MimbFD方法在欺诈检测中表现出色。

Conclusion: MimbFD通过缓解图神经网络监督消息的不平衡问题，有效提高了欺诈检测的表现。

Abstract: Graph representation learning has become a mainstream method for fraud
detection due to its strong expressive power, which focuses on enhancing node
representations through improved neighborhood knowledge capture. However, the
focus on local interactions leads to imbalanced transmission of global
topological information and increased risk of node-specific information being
overwhelmed during aggregation due to the imbalance between fraud and benign
nodes. In this paper, we first summarize the impact of topology and class
imbalance on downstream tasks in GNN-based fraud detection, as the problem of
imbalanced supervisory messages is caused by fraudsters' topological behavior
obfuscation and identity feature concealment. Based on statistical validation,
we propose a novel dual-view graph representation learning method to mitigate
Message imbalance in Fraud Detection(MimbFD). Specifically, we design a
topological message reachability module for high-quality node representation
learning to penetrate fraudsters' camouflage and alleviate insufficient
propagation. Then, we introduce a local confounding debiasing module to adjust
node representations, enhancing the stable association between node
representations and labels to balance the influence of different classes.
Finally, we conducted experiments on three public fraud datasets, and the
results demonstrate that MimbFD exhibits outstanding performance in fraud
detection.

</details>


### [155] [FedDifRC: Unlocking the Potential of Text-to-Image Diffusion Models in Heterogeneous Federated Learning](https://arxiv.org/abs/2507.06482)
*Huan Wang,Haoran Li,Huaming Chen,Jun Yan,Jiahua Shi,Jun Shen*

Main category: cs.LG

TL;DR: 本论文提出了一种新的联邦学习范式FedDifRC，通过引入扩散模型缓解数据异质性问题，并在多种场景下验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习受数据异质性问题的限制，作者希望通过引入扩散模型解决这一问题，从而提高模型的收敛性和性能。

Method: 提出了FedDifRC，将扩散模型中的语义引导融入联邦训练中，主要通过文字驱动的对比学习策略和噪声驱动的一致性正则化来缓解数据异质性，并支持无监督学习。

Result: 实验验证表明，FedDifRC在多种场景下表现出色，关键组件高效且方法有效。

Conclusion: FedDifRC能有效缓解联邦学习中的数据异质性问题，扩散模型提供了有意义的指导，同时理论分析保证了其在非凸目标下的收敛性。

Abstract: Federated learning aims at training models collaboratively across
participants while protecting privacy. However, one major challenge for this
paradigm is the data heterogeneity issue, where biased data preferences across
multiple clients, harming the model's convergence and performance. In this
paper, we first introduce powerful diffusion models into the federated learning
paradigm and show that diffusion representations are effective steers during
federated training. To explore the possibility of using diffusion
representations in handling data heterogeneity, we propose a novel
diffusion-inspired Federated paradigm with Diffusion Representation
Collaboration, termed FedDifRC, leveraging meaningful guidance of diffusion
models to mitigate data heterogeneity. The key idea is to construct text-driven
diffusion contrasting and noise-driven diffusion regularization, aiming to
provide abundant class-related semantic information and consistent convergence
signals. On the one hand, we exploit the conditional feedback from the
diffusion model for different text prompts to build a text-driven contrastive
learning strategy. On the other hand, we introduce a noise-driven consistency
regularization to align local instances with diffusion denoising
representations, constraining the optimization region in the feature space. In
addition, FedDifRC can be extended to a self-supervised scheme without relying
on any labeled data. We also provide a theoretical analysis for FedDifRC to
ensure convergence under non-convex objectives. The experiments on different
scenarios validate the effectiveness of FedDifRC and the efficiency of crucial
components.

</details>


### [156] [MoFE-Time: Mixture of Frequency Domain Experts for Time-Series Forecasting Models](https://arxiv.org/abs/2507.06502)
*Yiwen Liu,Chenyu Zhang,Junjie Song,Siqi Chen,Sun Yin,Zihan Wang,Lingming Zeng,Yuji Cao,Junming Jiao*

Main category: cs.LG

TL;DR: 本文提出了MoFE-Time模型，将时间和频率特征集成到专家网络中解决复杂时间序列预测问题，取得了显著优于现有方法的结果。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测模型未能有效结合时间和频率特征，尤其是在预训练-微调范式下表现不佳，导致复杂时间序列预测结果不理想。

Method: 提出了一种名为MoFE-Time的新模型，在混合专家网络中整合时间与频率域特征，并采用预训练-微调范式来有效迁移不同周期分布数据集之间的先验模式知识。

Result: 在六个公开基准测试中，MoFE-Time在MSE和MAE指标上分别降低了6.95%和6.02%，同时在自定义的真实场景数据集NEV-sales中也展现了卓越性能。

Conclusion: MoFE-Time模型有效提高了时间序列预测的准确性，特别是在复杂场景中的应用潜力显著，验证了整合时间和频率特征以及预训练-微调范式的有效性。

Abstract: As a prominent data modality task, time series forecasting plays a pivotal
role in diverse applications. With the remarkable advancements in Large
Language Models (LLMs), the adoption of LLMs as the foundational architecture
for time series modeling has gained significant attention. Although existing
models achieve some success, they rarely both model time and frequency
characteristics in a pretraining-finetuning paradigm leading to suboptimal
performance in predictions of complex time series, which requires both modeling
periodicity and prior pattern knowledge of signals. We propose MoFE-Time, an
innovative time series forecasting model that integrates time and frequency
domain features within a Mixture of Experts (MoE) network. Moreover, we use the
pretraining-finetuning paradigm as our training framework to effectively
transfer prior pattern knowledge across pretraining and finetuning datasets
with different periodicity distributions. Our method introduces both frequency
and time cells as experts after attention modules and leverages the MoE routing
mechanism to construct multidimensional sparse representations of input
signals. In experiments on six public benchmarks, MoFE-Time has achieved new
state-of-the-art performance, reducing MSE and MAE by 6.95% and 6.02% compared
to the representative methods Time-MoE. Beyond the existing evaluation
benchmarks, we have developed a proprietary dataset, NEV-sales, derived from
real-world business scenarios. Our method achieves outstanding results on this
dataset, underscoring the effectiveness of the MoFE-Time model in practical
commercial applications.

</details>


### [157] [Instance-Wise Monotonic Calibration by Constrained Transformation](https://arxiv.org/abs/2507.06516)
*Yunrui Zhang,Gustavo Batista,Salil S. Kanhere*

Main category: cs.LG

TL;DR: 本文提出了一种新型方法来改进深度神经网络的概率校准，解决了现有方法的非单调性和可解释性不足的问题，取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的预测概率往往过于自信，当前的大多数后处理校准方法无法保证单调性或具有表达能力限制。

Method: 论文提出了一种基于约束优化问题的新型后处理校准方法，利用线性参数化校准映射，保证单调性并兼顾表达能力和可解释性。

Result: 该方法在多种数据集和模型上均实现了最先进的性能，表现优于现有校准方法，同时更加高效。

Conclusion: 所提出的方法在保持单调性及解释性的同时，改进了深度神经网络的校准效果，具有实际意义。

Abstract: Deep neural networks often produce miscalibrated probability estimates,
leading to overconfident predictions. A common approach for calibration is
fitting a post-hoc calibration map on unseen validation data that transforms
predicted probabilities. A key desirable property of the calibration map is
instance-wise monotonicity (i.e., preserving the ranking of probability
outputs). However, most existing post-hoc calibration methods do not guarantee
monotonicity. Previous monotonic approaches either use an under-parameterized
calibration map with limited expressive ability or rely on black-box neural
networks, which lack interpretability and robustness. In this paper, we propose
a family of novel monotonic post-hoc calibration methods, which employs a
constrained calibration map parameterized linearly with respect to the number
of classes. Our proposed approach ensures expressiveness, robustness, and
interpretability while preserving the relative ordering of the probability
output by formulating the proposed calibration map as a constrained
optimization problem. Our proposed methods achieve state-of-the-art performance
across datasets with different deep neural network models, outperforming
existing calibration methods while being data and computation-efficient. Our
code is available at
https://github.com/YunruiZhang/Calibration-by-Constrained-Transformation

</details>


### [158] [AdaDPIGU: Differentially Private SGD with Adaptive Clipping and Importance-Based Gradient Updates for Deep Neural Networks](https://arxiv.org/abs/2507.06525)
*Huiqi Zhang,Fang Xie*

Main category: cs.LG

TL;DR: 研究提出了一种新框架AdaDPIGU，通过自适应稀疏化和重要性更新机制，在保持差分隐私的同时优化了高维度设置下的模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私SGD方法在高维条件下表现受限，因此需要开发一种既能保护隐私又能有效提升模型性能的方法。

Method: 提出AdaDPIGU框架，在预训练阶段使用差分隐私高斯机制估计参数重要性，梯度更新时采用低重要性参数裁剪与自适应裁剪机制实现稀疏化和高效更新，并证明其满足$(\varepsilon, \delta)$-差分隐私与收敛性。

Result: 在固定60%保持比率下，MNIST数据集精度可达99.12%（$\epsilon = 8$），几乎与非隐私模型一致；在CIFAR-10数据集上，精度为73.21% ($\varepsilon = 4$)，优于71.12%的非隐私基线。

Conclusion: 此研究表明，自适应稀疏化可以在提升隐私的同时改善模型性能，验证了对深度神经网络中差分隐私的实际应用价值。

Abstract: Differential privacy has been proven effective for stochastic gradient
descent; however, existing methods often suffer from performance degradation in
high-dimensional settings, as the scale of injected noise increases with
dimensionality. To tackle this challenge, we propose AdaDPIGU--a new
differentially private SGD framework with importance-based gradient updates
tailored for deep neural networks. In the pretraining stage, we apply a
differentially private Gaussian mechanism to estimate the importance of each
parameter while preserving privacy. During the gradient update phase, we prune
low-importance coordinates and introduce a coordinate-wise adaptive clipping
mechanism, enabling sparse and noise-efficient gradient updates. Theoretically,
we prove that AdaDPIGU satisfies $(\varepsilon, \delta)$-differential privacy
and retains convergence guarantees. Extensive experiments on standard
benchmarks validate the effectiveness of AdaDPIGU. All results are reported
under a fixed retention ratio of 60%. On MNIST, our method achieves a test
accuracy of 99.12% under a privacy budget of $\epsilon = 8$, nearly matching
the non-private model. Remarkably, on CIFAR-10, it attains 73.21% accuracy at
$\epsilon = 4$, outperforming the non-private baseline of 71.12%, demonstrating
that adaptive sparsification can enhance both privacy and utility.

</details>


### [159] [Direct Regret Optimization in Bayesian Optimization](https://arxiv.org/abs/2507.06529)
*Fengxue Zhang,Yuxin Chen*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的直接遗憾优化方法，用于联合学习最优模型和非短视的采集策略，从而优化昂贵的黑箱函数。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化方法常依赖手工设计的采集函数和代理模型，且具有短视特性，因此需要改进其性能和探索能力。

Method: 通过一种直接遗憾优化框架，使用了包含多元高斯过程（GP）的集成模型生成模拟的贝叶斯优化轨迹，训练一个决策变压器直接学习如何选择下一次查询点。采用了密集训练和稀疏学习相结合的策略。

Result: 实验表明，该方法在合成和真实基准测试中显著优于传统方法，表现出在高维或噪声环境下更为稳健的探索性能和更低的简单遗憾值。

Conclusion: 新方法在优化性能和解决实际问题中展现了明显优势，可作为贝叶斯优化领域的新方向。

Abstract: Bayesian optimization (BO) is a powerful paradigm for optimizing expensive
black-box functions. Traditional BO methods typically rely on separate
hand-crafted acquisition functions and surrogate models for the underlying
function, and often operate in a myopic manner. In this paper, we propose a
novel direct regret optimization approach that jointly learns the optimal model
and non-myopic acquisition by distilling from a set of candidate models and
acquisitions, and explicitly targets minimizing the multi-step regret. Our
framework leverages an ensemble of Gaussian Processes (GPs) with varying
hyperparameters to generate simulated BO trajectories, each guided by an
acquisition function chosen from a pool of conventional choices, until a
Bayesian early stop criterion is met. These simulated trajectories, capturing
multi-step exploration strategies, are used to train an end-to-end decision
transformer that directly learns to select next query points aimed at improving
the ultimate objective. We further adopt a dense training--sparse learning
paradigm: The decision transformer is trained offline with abundant simulated
data sampled from ensemble GPs and acquisitions, while a limited number of real
evaluations refine the GPs online. Experimental results on synthetic and
real-world benchmarks suggest that our method consistently outperforms BO
baselines, achieving lower simple regret and demonstrating more robust
exploration in high-dimensional or noisy settings.

</details>


### [160] [Transferable Parasitic Estimation via Graph Contrastive Learning and Label Rebalancing in AMS Circuits](https://arxiv.org/abs/2507.06535)
*Shan Shen,Shenglu Hua,Jiajun Zou,Jiawei Liu,Jianwang Zhai,Chuan Shi,Wenjian Yu*

Main category: cs.LG

TL;DR: 提出一种用于AMS电路的图对比学习框架CircuitGCL，通过无监督策略学习拓扑不变的节点嵌入和平衡的损失函数解决数据稀缺以及标签分布不均的问题，在TSMC 28nm AMS设计上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决AMS电路设计数据稀缺、标签分布不平衡和电路实现多样性对鲁棒和可迁移表示学习的挑战。

Method: 提出CircuitGCL框架，包含超球体表示散射用于无监督学习拓扑不变节点嵌入，以及平衡的MSE和bsmCE损失减少标签分布差异。

Result: 在TSMC 28nm AMS电路上，边回归任务的R²提升了33.64%到44.20%，节点分类任务的F1得分提高了0.9到2.1倍，优于所有SOTA方法。

Conclusion: CircuitGCL成功解决了AMS电路图的代表性学习问题，提升了迁移性和性能，适用于多种电路任务。

Abstract: Graph representation learning on Analog-Mixed Signal (AMS) circuits is
crucial for various downstream tasks, e.g., parasitic estimation. However, the
scarcity of design data, the unbalanced distribution of labels, and the
inherent diversity of circuit implementations pose significant challenges to
learning robust and transferable circuit representations. To address these
limitations, we propose CircuitGCL, a novel graph contrastive learning
framework that integrates representation scattering and label rebalancing to
enhance transferability across heterogeneous circuit graphs. CircuitGCL employs
a self-supervised strategy to learn topology-invariant node embeddings through
hyperspherical representation scattering, eliminating dependency on large-scale
data. Simultaneously, balanced mean squared error (MSE) and softmax
cross-entropy (bsmCE) losses are introduced to mitigate label distribution
disparities between circuits, enabling robust and transferable parasitic
estimation. Evaluated on parasitic capacitance estimation (edge-level task) and
ground capacitance classification (node-level task) across TSMC 28nm AMS
designs, CircuitGCL outperforms all state-of-the-art (SOTA) methods, with the
$R^2$ improvement of $33.64\% \sim 44.20\%$ for edge regression and F1-score
gain of $0.9\times \sim 2.1\times$ for node classification. Our code is
available at
\href{https://anonymous.4open.science/r/CircuitGCL-099B/README.md}{here}.

</details>


### [161] [Few-shot Learning on AMS Circuits and Its Application to Parasitic Capacitance Prediction](https://arxiv.org/abs/2507.06538)
*Shan Shen,Yibin Zhang,Hector Rodriguez Rodriguez,Wenjian Yu*

Main category: cs.LG

TL;DR: 提出CircuitGPS，一种用于AMS电路中寄生效应预测的少样本学习方法，通过图表示学习显著提升耦合存在性预测准确性并降低电容估计误差。


<details>
  <summary>Details</summary>
Motivation: 为解决AMS电路设计中集成电路设计数据稀缺性的问题，提高深度学习模型在AMS设计中的有效性。

Method: 将电路网表表示为异构图，耦合电容建模为连接，采用预训练与微调相结合的方式，利用小跳跃采样技术创建子图，并通过混合图Transformer学习子图嵌入。此外，集成低成本位置编码以增强结构与位置信息表达能力。

Result: CircuitGPS提高了耦合存在性预测准确性至少20%，并将电容估计的MAE减少至少0.067，展示了强大的可扩展性及零样本学习能力。

Conclusion: CircuitGPS显著提升了AMS电路中寄生效应预测的性能，同时其零样本学习能力和图模型方面的洞见具有重要价值。

Abstract: Graph representation learning is a powerful method to extract features from
graph-structured data, such as analog/mixed-signal (AMS) circuits. However,
training deep learning models for AMS designs is severely limited by the
scarcity of integrated circuit design data. In this work, we present
CircuitGPS, a few-shot learning method for parasitic effect prediction in AMS
circuits. The circuit netlist is represented as a heterogeneous graph, with the
coupling capacitance modeled as a link. CircuitGPS is pre-trained on link
prediction and fine-tuned on edge regression. The proposed method starts with a
small-hop sampling technique that converts a link or a node into a subgraph.
Then, the subgraph embeddings are learned with a hybrid graph Transformer.
Additionally, CircuitGPS integrates a low-cost positional encoding that
summarizes the positional and structural information of the sampled subgraph.
CircuitGPS improves the accuracy of coupling existence by at least 20\% and
reduces the MAE of capacitance estimation by at least 0.067 compared to
existing methods. Our method demonstrates strong inherent scalability, enabling
direct application to diverse AMS circuit designs through zero-shot learning.
Furthermore, the ablation studies provide valuable insights into graph models
for representation learning.

</details>


### [162] [A Single Merging Suffices: Recovering Server-based Learning Performance in Decentralized Learning](https://arxiv.org/abs/2507.06542)
*Tongtian Zhu,Tianyu Zhang,Mingze Wang,Zhanpeng Zhou,Can Wang*

Main category: cs.LG

TL;DR: 该研究探讨了如何优化去中心化学习中的通信安排，通过实验和理论证明在训练后期集中通信预算可显著提高模型性能，并通过单次全局合并实现效果匹配服务器式训练。


<details>
  <summary>Details</summary>
Motivation: 去中心化学习因设备间通信受限常导致性能不佳，需优化通信策略提升其全球泛化能力。

Method: 通过实验研究通信时机和频率对去中心化学习效果的影响，并重新定义本地模型间差异从而解释其对收敛的正向作用。

Result: 实验表明，在训练后期集中通信预算改善效果显著，单次全局合并通信便可匹配中心化训练性能，此外低通信量保留了本地模型的可合并性。

Conclusion: 去中心化学习在数据异质和通信受限情况下仍可达到优良性能，模型合并与网络损失景观研究提供了新见解。

Abstract: Decentralized learning provides a scalable alternative to traditional
parameter-server-based training, yet its performance is often hindered by
limited peer-to-peer communication. In this paper, we study how communication
should be scheduled over time, including determining when and how frequently
devices synchronize. Our empirical results show that concentrating
communication budgets in the later stages of decentralized training markedly
improves global generalization. Surprisingly, we uncover that fully connected
communication at the final step, implemented by a single global merging, is
sufficient to match the performance of server-based training. We further show
that low communication in decentralized learning preserves the
\textit{mergeability} of local models throughout training. Our theoretical
contributions, which explains these phenomena, are first to establish that the
globally merged model of decentralized SGD can converge faster than centralized
mini-batch SGD. Technically, we novelly reinterpret part of the discrepancy
among local models, which were previously considered as detrimental noise, as
constructive components that accelerate convergence. This work challenges the
common belief that decentralized learning generalizes poorly under data
heterogeneity and limited communication, while offering new insights into model
merging and neural network loss landscapes.

</details>


### [163] [Deep-Learning-Based Pre-Layout Parasitic Capacitance Prediction on SRAM Designs](https://arxiv.org/abs/2507.06549)
*Shan Shen,Dingcheng Yang,Yuyang Xie,Chunyan Pei,Wenjian Yu,Bei Yu*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的两阶段模型，用于提高SRAM电路中布局前寄生参数预测的准确性，从而加速仿真。


<details>
  <summary>Details</summary>
Motivation: 解决SRAM中布局前与布局后电路仿真之间寄生效应差异大、设计参数难以收敛及设计迭代过多的问题，旨在进行寄生感知的布局前仿真。

Method: 提出一个结合图神经网络（GNN）分类器和多层感知机（MLP）回归器的深度学习模型，利用Focal Loss处理类别不平衡，并将子电路信息集成到图中抽象层次结构。

Result: 在4个真实SRAM设计上的实验显示，该方法在寄生参数预测上比当今最先进的模型误差减少最多达19倍，并将仿真过程速度提升最多598倍。

Conclusion: 该方法通过提高寄生参数预测准确性和加速仿真，显著提升了SRAM设计的效率。

Abstract: To achieve higher system energy efficiency, SRAM in SoCs is often customized.
The parasitic effects cause notable discrepancies between pre-layout and
post-layout circuit simulations, leading to difficulty in converging design
parameters and excessive design iterations. Is it possible to well predict the
parasitics based on the pre-layout circuit, so as to perform parasitic-aware
pre-layout simulation? In this work, we propose a deep-learning-based 2-stage
model to accurately predict these parasitics in pre-layout stages. The model
combines a Graph Neural Network (GNN) classifier and Multi-Layer Perceptron
(MLP) regressors, effectively managing class imbalance of the net parasitics in
SRAM circuits. We also employ Focal Loss to mitigate the impact of abundant
internal net samples and integrate subcircuit information into the graph to
abstract the hierarchical structure of schematics. Experiments on 4 real SRAM
designs show that our approach not only surpasses the state-of-the-art model in
parasitic prediction by a maximum of 19X reduction of error but also
significantly boosts the simulation process by up to 598X speedup.

</details>


### [164] [The Primacy of Magnitude in Low-Rank Adaptation](https://arxiv.org/abs/2507.06558)
*Zicheng Zhang,Haoran Li,Yifeng Zhang,Guoqiang Gong,Jiaxing Wang,Pengzhang Liu,Qixia Jiang,Junxing Hu*

Main category: cs.LG

TL;DR: 研究提出了LoRAM，一种通过更新幅度驱动的初始化方法来改进LoRA的性能，同时保持操作高效性和精简性。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA方法在改进大模型调优的效率方面表现出色，但现有的谱初始化方法会带来额外的计算和存储成本，制约了效率提升。

Method: 提出LoRAM，通过基于幅度驱动的“Basis & Basis”初始化方案，用预训练权重的幅度来缩放确定性正交基，模拟谱初始化的效果，同时消除其低效性。

Result: LoRAM以更低的计算复杂度实现了与谱初始化方法相当甚至更优的性能，在多个基准测试中展现了出色的表现。

Conclusion: LoRAM提供了一种替代谱初始化的强大基准方法，既保持了LoRA的效率优势，又有效改善了模型调优的性能。

Abstract: Low-Rank Adaptation (LoRA) offers a parameter-efficient paradigm for tuning
large models. While recent spectral initialization methods improve convergence
and performance over the naive "Noise & Zeros" scheme, their extra
computational and storage overhead undermines efficiency. In this paper, we
establish update magnitude as the fundamental driver of LoRA performance and
propose LoRAM, a magnitude-driven "Basis & Basis" initialization scheme that
matches spectral methods without their inefficiencies. Our key contributions
are threefold: (i) Magnitude of weight updates determines convergence. We prove
low-rank structures intrinsically bound update magnitudes, unifying
hyperparameter tuning in learning rate, scaling factor, and initialization as
mechanisms to optimize magnitude regulation. (ii) Spectral initialization
succeeds via magnitude amplification. We demystify that the presumed
knowledge-driven benefit of the spectral component essentially arises from the
boost in the weight update magnitude. (iii) A novel and compact initialization
strategy, LoRAM, scales deterministic orthogonal bases using pretrained weight
magnitudes to simulate spectral gains. Extensive experiments show that LoRAM
serves as a strong baseline, retaining the full efficiency of LoRA while
matching or outperforming spectral initialization across benchmarks.

</details>


### [165] [SlimCaching: Edge Caching of Mixture-of-Experts for Distributed Inference](https://arxiv.org/abs/2507.06567)
*Qian Chen,Xianhao Chen,Kaibin Huang*

Main category: cs.LG

TL;DR: 提出一种通过优化边缘设备上的专家缓存以最小化延迟的算法，用于分布式Mixture-of-Experts (MoE)模型推理。


<details>
  <summary>Details</summary>
Motivation: 解决由于大量专家网络引起的存储负担问题，特别是在边缘设备资源有限的情况下，目标是通过优化专家缓存分配降低推理延迟。

Method: 针对K=1和K≥1的情况，分别设计了基于贪心算法和连续分解动态规划的方法，同时利用最大卷积技术加速计算，提供可证明保证的近似解。

Result: 通过各种MoE模型的模拟实验，证明所提方法能显著减少推理延迟，与现有基线方法相比表现更优。

Conclusion: 所提出的优化方法有效降低了分布式MoE模型推理的延迟，为边缘设备上的大规模语言模型应用提供了实际可行的解决方案。

Abstract: Mixture-of-Experts (MoE) models improve the scalability of large language
models (LLMs) by activating only a small subset of relevant experts per input.
However, the sheer number of expert networks in an MoE model introduces a
significant storage burden for an edge device. To address this challenge, we
consider a scenario where experts are dispersed within an edge network for
distributed inference. Based on the popular Top-$K$ expert selection strategy,
we formulate a latency minimization problem by optimizing expert caching on
edge servers under storage constraints. When $K=1$, the problem reduces to a
monotone submodular maximization problem with knapsack constraints, for which
we design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.
For the general case where $K\geq1$, expert co-activation within the same MoE
layer introduces non-submodularity, causing greedy methods to be ineffective.
To tackle this issue, we propose a successive greedy decomposition method to
decompose the original problem into a series of subproblems, with each being
solved by a dynamic programming approach. Furthermore, we design an accelerated
algorithm based on the max-convolution technique to obtain the approximate
solution with a provable guarantee in polynomial time. Simulation results on
various MoE models demonstrate that our method significantly reduces inference
latency compared to existing baselines.

</details>


### [166] [From Data-Centric to Sample-Centric: Enhancing LLM Reasoning via Progressive Optimization](https://arxiv.org/abs/2507.06573)
*Xinjie Chen,Minpeng Liao,Guoxin Chen,Chengxi Li,Biao Fu,Kai Fan,Xinggao Liu*

Main category: cs.LG

TL;DR: 提出了一种名为LPPO的新方法改进大语言模型的强化学习能力，尤其在有限高质量样本下表现出色。


<details>
  <summary>Details</summary>
Motivation: 探讨在只有少量可信高质量示范数据的情况下，如何最大化强化学习模型的效率和性能，而非单纯依赖数据规模扩展。

Method: 提出了LPPO框架：1. 前缀引导采样，通过从专家示范中抽取部分前缀解决问题难点；2. 学习进展加权，根据模型进展动态调整样本权重，避免过度依赖未能显著提升学习的样本。

Result: 在数学推理基准测试中，LPPO方法优于现有强化学习基线，表现出更快的收敛速度和更高的模型性能极限。

Conclusion: LPPO框架通过灵活利用有限、可信的高质量示范数据，将强化学习模型的收敛速度和性能提升到了新高度。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently advanced
the reasoning capabilities of large language models (LLMs). While prior work
has emphasized algorithmic design, data curation, and reward shaping, we
investigate RLVR from a sample-centric perspective and introduce LPPO
(Learning-Progress and Prefix-guided Optimization), a framework of progressive
optimization techniques. Our work addresses a critical question: how to best
leverage a small set of trusted, high-quality demonstrations, rather than
simply scaling up data volume. First, motivated by how hints aid human
problem-solving, we propose prefix-guided sampling, an online data augmentation
method that incorporates partial solution prefixes from expert demonstrations
to guide the policy, particularly for challenging instances. Second, inspired
by how humans focus on important questions aligned with their current
capabilities, we introduce learning-progress weighting, a dynamic strategy that
adjusts each training sample's influence based on model progression. We
estimate sample-level learning progress via an exponential moving average of
per-sample pass rates, promoting samples that foster learning and
de-emphasizing stagnant ones. Experiments on mathematical-reasoning benchmarks
demonstrate that our methods outperform strong baselines, yielding faster
convergence and a higher performance ceiling.

</details>


### [167] [Learning controllable dynamics through informative exploration](https://arxiv.org/abs/2507.06582)
*Peter N. Loxley,Friedrich T. Sommer*

Main category: cs.LG

TL;DR: 研究通过信息增益的计算来指导环境探索，得到了可靠的环境动力学估算。


<details>
  <summary>Details</summary>
Motivation: 研究在明确动力学模型不可用的情况下，如何通过对环境的探索学习环境的可控动力学。

Method: 通过预测信息增益，利用深度强化学习方法找到探索环境的次优策略，并估算其动力学。

Result: 证明通过信息增益为导向的探索策略相比其他探索方式更为可靠。

Conclusion: 信息增益导向的策略对于学习和估算环境动力学是有效的，能够提供可靠的指导。

Abstract: Environments with controllable dynamics are usually understood in terms of
explicit models. However, such models are not always available, but may
sometimes be learned by exploring an environment. In this work, we investigate
using an information measure called "predicted information gain" to determine
the most informative regions of an environment to explore next. Applying
methods from reinforcement learning allows good suboptimal exploring policies
to be found, and leads to reliable estimates of the underlying controllable
dynamics. This approach is demonstrated by comparing with several myopic
exploration approaches.

</details>


### [168] [Generalization in Reinforcement Learning for Radio Access Networks](https://arxiv.org/abs/2507.06602)
*Burak Demirel,Yu Wang,Cristian Tatino,Pablo Soldati*

Main category: cs.LG

TL;DR: 本文提出了一种面向现代RAN控制的广义强化学习框架，通过注意力图表示、域随机化和分布式数据生成等方法，提升传统算法的性能并实现泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于现代无线接入网的动态性和异质性，传统规则型RRM算法往往表现不足；同时，强化学习在应对多样化部署和不可预测无线条件时存在泛化挑战。

Method: 设计了一个以泛化能力为核心的强化学习框架，包括基于注意力的图表示、通过域随机化扩展训练分布、以及分布式数据生成与云端集中训练的架构。

Result: 在多个5G基准测试中，策略平均吞吐量和频谱效率提升约10%，高移动场景下超过20%，多种模型比较中显著优于MLP基线。

Conclusion: 提出的分布式设计和架构为实现具有单一、可泛化RL代理的AI原生6G RAN铺平了道路。

Abstract: Modern RAN operate in highly dynamic and heterogeneous environments, where
hand-tuned, rule-based RRM algorithms often underperform. While RL can surpass
such heuristics in constrained settings, the diversity of deployments and
unpredictable radio conditions introduce major generalization challenges.
Data-driven policies frequently overfit to training conditions, degrading
performance in unseen scenarios. To address this, we propose a
generalization-centered RL framework for RAN control that: (i) encodes cell
topology and node attributes via attention-based graph representations; (ii)
applies domain randomization to broaden the training distribution; and (iii)
distributes data generation across multiple actors while centralizing training
in a cloud-compatible architecture aligned with O-RAN principles. Although
generalization increases computational and data-management complexity, our
distributed design mitigates this by scaling data collection and training
across diverse network conditions. Applied to downlink link adaptation in five
5G benchmarks, our policy improves average throughput and spectral efficiency
by ~10% over an OLLA baseline (10% BLER target) in full-buffer MIMO/mMIMO and
by >20% under high mobility. It matches specialized RL in full-buffer traffic
and achieves up to 4- and 2-fold gains in eMBB and mixed-traffic benchmarks,
respectively. In nine-cell deployments, GAT models offer 30% higher throughput
over MLP baselines. These results, combined with our scalable architecture,
offer a path toward AI-native 6G RAN using a single, generalizable RL agent.

</details>


### [169] [Denoising Multi-Beta VAE: Representation Learning for Disentanglement and Generation](https://arxiv.org/abs/2507.06613)
*Anshuk Uppal,Yuhta Takida,Chieh-Hsin Lai,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 提出一种新框架，结合不同的$\beta$值实现既具备解耦性又保持生成质量的潜在表示，利用非线性扩散模型实现平滑过渡，并支持无输入的样本生成。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在解耦性和生成质量之间存在权衡，需要更好地平衡二者。

Method: 通过新损失函数训练单个变分自编码器（VAE），结合多种$\beta$值学习对应的潜在表示，并引入非线性扩散模型实现潜在表示间的平滑过渡。

Result: 实现了生成模型在高解耦性和高生成质量间的平衡，其潜在空间随$\beta$变化具备平滑的可操作性。

Conclusion: 该框架在解耦性和生成质量上取得显著提升，并支持无输入图片的样本生成，为生成模型结合更好性能和解释性提供了新思路。

Abstract: Disentangled and interpretable latent representations in generative models
typically come at the cost of generation quality. The $\beta$-VAE framework
introduces a hyperparameter $\beta$ to balance disentanglement and
reconstruction quality, where setting $\beta > 1$ introduces an information
bottleneck that favors disentanglement over sharp, accurate reconstructions. To
address this trade-off, we propose a novel generative modeling framework that
leverages a range of $\beta$ values to learn multiple corresponding latent
representations. First, we obtain a slew of representations by training a
single variational autoencoder (VAE), with a new loss function that controls
the information retained in each latent representation such that the higher
$\beta$ value prioritize disentanglement over reconstruction fidelity. We then,
introduce a non-linear diffusion model that smoothly transitions latent
representations corresponding to different $\beta$ values. This model denoises
towards less disentangled and more informative representations, ultimately
leading to (almost) lossless representations, enabling sharp reconstructions.
Furthermore, our model supports sample generation without input images,
functioning as a standalone generative model. We evaluate our framework in
terms of both disentanglement and generation quality. Additionally, we observe
smooth transitions in the latent spaces with respect to changes in $\beta$,
facilitating consistent manipulation of generated outputs.

</details>


### [170] [Efficient Multi-Task Reinforcement Learning with Cross-Task Policy Guidance](https://arxiv.org/abs/2507.06615)
*Jinmin He,Kai Li,Yifan Zang,Haobo Fu,Qiang Fu,Junliang Xing,Jian Cheng*

Main category: cs.LG

TL;DR: CTPG是一种用于多任务强化学习的框架，通过指南策略在不同任务的控制策略之间选择，以改善训练轨迹，提高性能。


<details>
  <summary>Details</summary>
Motivation: 当前多任务强化学习方法忽略了利用熟练任务的控制策略直接指导未掌握任务的潜力。

Method: 提出跨任务策略指导框架（CTPG），通过指南策略为每个任务选择最佳控制策略并采用两种门控机制优化学习效率。

Result: 在操作和运动基准测试中，结合CTPG的参数共享方法表现显著提升。

Conclusion: CTPG框架可增强多任务学习性能，并成功应用于多任务强化学习的多种场景中。

Abstract: Multi-task reinforcement learning endeavors to efficiently leverage shared
information across various tasks, facilitating the simultaneous learning of
multiple tasks. Existing approaches primarily focus on parameter sharing with
carefully designed network structures or tailored optimization procedures.
However, they overlook a direct and complementary way to exploit cross-task
similarities: the control policies of tasks already proficient in some skills
can provide explicit guidance for unmastered tasks to accelerate skills
acquisition. To this end, we present a novel framework called Cross-Task Policy
Guidance (CTPG), which trains a guide policy for each task to select the
behavior policy interacting with the environment from all tasks' control
policies, generating better training trajectories. In addition, we propose two
gating mechanisms to improve the learning efficiency of CTPG: one gate filters
out control policies that are not beneficial for guidance, while the other gate
blocks tasks that do not necessitate guidance. CTPG is a general framework
adaptable to existing parameter sharing approaches. Empirical evaluations
demonstrate that incorporating CTPG with these approaches significantly
enhances performance in manipulation and locomotion benchmarks.

</details>


### [171] [Steps Adaptive Decay DPSGD: Enhancing Performance on Imbalanced Datasets with Differential Privacy with HAM10000](https://arxiv.org/abs/2507.06619)
*Xiaobo Huang,Fang Xie*

Main category: cs.LG

TL;DR: 提出了一种适用于小型、不平衡医疗数据集的新方法SAD-DPSGD，解决了现有方法因数据不平衡导致模型性能下降的问题，并在HAM10000数据集上提升了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的差分隐私方法在处理小型、不平衡的医疗图像数据集时性能较差，主要因数据不平衡导致梯度信息丢失，影响模型的学习。

Method: 引入SAD-DPSGD，采用线性衰减机制对噪声和裁剪阈值进行调整，在初始训练阶段分配更多的隐私预算和更高的裁剪阈值，以避免早期陷入次优解。

Result: 在HAM10000数据集的实验中，SAD-DPSGD在$\epsilon = 3.0$，$\delta = 10^{-3}$的情况下将准确率提高了2.15%，优于Auto-DPSGD。

Conclusion: SAD-DPSGD能有效解决因数据不平衡导致的模型性能下降问题，特别是在小型医疗数据集上的应用中具有显著优势。

Abstract: When applying machine learning to medical image classification, data leakage
is a critical issue. Previous methods, such as adding noise to gradients for
differential privacy, work well on large datasets like MNIST and CIFAR-100, but
fail on small, imbalanced medical datasets like HAM10000. This is because the
imbalanced distribution causes gradients from minority classes to be clipped
and lose crucial information, while majority classes dominate. This leads the
model to fall into suboptimal solutions early. To address this, we propose
SAD-DPSGD, which uses a linear decaying mechanism for noise and clipping
thresholds. By allocating more privacy budget and using higher clipping
thresholds in the initial training phases, the model avoids suboptimal
solutions and enhances performance. Experiments show that SAD-DPSGD outperforms
Auto-DPSGD on HAM10000, improving accuracy by 2.15% under $\epsilon = 3.0$ ,
$\delta = 10^{-3}$.

</details>


### [172] [UniOD: A Universal Model for Outlier Detection across Diverse Domains](https://arxiv.org/abs/2507.06624)
*Dazhi Fu,Jicong Fan*

Main category: cs.LG

TL;DR: 提出了UniOD，一个无需特定数据集调试的通用异常检测框架，通过图与节点分类问题实现跨域检测。


<details>
  <summary>Details</summary>
Motivation: 现有OD方法需复杂的超参数调试与模型训练，难以满足实际需求。

Method: UniOD将数据集转化为图结构，并将异常检测建模为节点分类任务，借助已有的标注数据训练通用模型。

Result: UniOD在15个基准数据集与15个先进基线算法对比中表现出色。

Conclusion: UniOD解决了多域场景下异常检测的现实需求，提升了便利性和准确性。

Abstract: Outlier detection (OD) seeks to distinguish inliers and outliers in
completely unlabeled datasets and plays a vital role in science and
engineering. Most existing OD methods require troublesome dataset-specific
hyperparameter tuning and costly model training before they can be deployed to
identify outliers. In this work, we propose UniOD, a universal OD framework
that leverages labeled datasets to train a single model capable of detecting
outliers of datasets from diverse domains. Specifically, UniOD converts each
dataset into multiple graphs, produces consistent node features, and frames
outlier detection as a node-classification task, and is able to generalize to
unseen domains. As a result, UniOD avoids effort on model selection and
hyperparameter tuning, reduces computational cost, and effectively utilizes the
knowledge from historical datasets, which improves the convenience and accuracy
in real applications. We evaluate UniOD on 15 benchmark OD datasets against 15
state-of-the-art baselines, demonstrating its effectiveness.

</details>


### [173] [Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement Learning](https://arxiv.org/abs/2507.06628)
*Jinmin He,Kai Li,Yifan Zang,Haobo Fu,Qiang Fu,Junliang Xing,Jian Cheng*

Main category: cs.LG

TL;DR: 本文提出了一种称为GO-Skill的方法，用于离线多任务强化学习，通过抽取可重复使用的目标导向技能来提高知识共享和任务表现，并在MetaWorld基准中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 离线多任务强化学习需要在无在线互动场景中学习解决多个任务的统一策略，但知识共享效果不佳，这促使研究如何更高效地抽象知识。

Method: 提出GO-Skill方法，包含目标导向的技能提取，向量量化构建离散技能库，技能改进阶段，以及层次化策略学习来动态调配技能完成任务。

Result: 在MetaWorld基准中，实验表明GO-Skill在各种机器人操作任务中表现出有效性和多功能性。

Conclusion: GO-Skill方法设计合理，能够通过技能抽象和整合显著提高多任务强化学习的知识传递效率。

Abstract: Offline multi-task reinforcement learning aims to learn a unified policy
capable of solving multiple tasks using only pre-collected task-mixed datasets,
without requiring any online interaction with the environment. However, it
faces significant challenges in effectively sharing knowledge across tasks.
Inspired by the efficient knowledge abstraction observed in human learning, we
propose Goal-Oriented Skill Abstraction (GO-Skill), a novel approach designed
to extract and utilize reusable skills to enhance knowledge transfer and task
performance. Our approach uncovers reusable skills through a goal-oriented
skill extraction process and leverages vector quantization to construct a
discrete skill library. To mitigate class imbalances between broadly applicable
and task-specific skills, we introduce a skill enhancement phase to refine the
extracted skills. Furthermore, we integrate these skills using hierarchical
policy learning, enabling the construction of a high-level policy that
dynamically orchestrates discrete skills to accomplish specific tasks.
Extensive experiments on diverse robotic manipulation tasks within the
MetaWorld benchmark demonstrate the effectiveness and versatility of GO-Skill.

</details>


### [174] [Prevention of Overfitting on Mesh-Structured Data Regressions with a Modified Laplace Operator](https://arxiv.org/abs/2507.06631)
*Enda D. V. Bigarella*

Main category: cs.LG

TL;DR: 研究了一种基于网格数据结构的方法，通过利用拉普拉斯算子导数的损失来检测和防止过拟合。


<details>
  <summary>Details</summary>
Motivation: 为了解决数据回归中的过拟合问题，尤其是在网格形状数据上的应用。

Method: 通过使用拉普拉斯算子的二阶导数与交错网格构建的方法，来计算训练数据和模型的导数差异，并以此优化超参数。

Result: 该方法通过最小化训练模型的熵，减少了不必要的振荡。在不需要分离测试点的情况下，直接对训练数据进行优化。

Conclusion: 拉普拉斯算子的导数损失用于优化，提供了针对过拟合问题有效的解决方案，并提出了一种基于扩散特性的测试度量方法。

Abstract: This document reports on a method for detecting and preventing overfitting on
data regressions, herein applied to mesh-like data structures. The mesh
structure allows for the straightforward computation of the Laplace-operator
second-order derivatives in a finite-difference fashion for noiseless data.
Derivatives of the training data are computed on the original training mesh to
serve as a true label of the entropy of the training data. Derivatives of the
trained data are computed on a staggered mesh to identify oscillations in the
interior of the original training mesh cells. The loss of the Laplace-operator
derivatives is used for hyperparameter optimisation, achieving a reduction of
unwanted oscillation through the minimisation of the entropy of the trained
model. In this setup, testing does not require the splitting of points from the
training data, and training is thus directly performed on all available
training points. The Laplace operator applied to the trained data on a
staggered mesh serves as a surrogate testing metric based on diffusion
properties.

</details>


### [175] [Deep Disentangled Representation Network for Treatment Effect Estimation](https://arxiv.org/abs/2507.06650)
*Hui Meng,Keping Yang,Xuyu Peng,Bo Zheng*

Main category: cs.LG

TL;DR: 本文提出了一种新的算法，通过引入多头注意力的专家混合模型和线性正交正则化器，柔性解构预处理变量，并结合重要性采样重新加权技术消除选择偏差，从而提升了个体治疗效果的估计精度。


<details>
  <summary>Details</summary>
Motivation: 个体治疗效应估计是观察性数据中的关键问题，特别是在教育、医疗与公共政策领域。尽管此前研究表明解耦表示方法效果理想，但现有生成模型或硬分解方法难以保证精确解耦，这需要更有效的方式处理因果关系。

Method: 作者引入结合多头注意力的专家混合模型以柔性解构预处理变量，与线性正交正则化器配合，同时结合重要性采样重新加权技术以降低选择偏差，实现个体治疗效果的优化估计。

Result: 实验显示，在公共半合成和现实生产数据集上的大量实验中，该算法在个体治疗效果估计算法中表现出了超越现有最优方法的结果。

Conclusion: 提出的算法创新结合解耦与选择偏差去除技术，在多种数据集上表现优异，为更好地估计治疗效果提供了有效工具。

Abstract: Estimating individual-level treatment effect from observational data is a
fundamental problem in causal inference and has attracted increasing attention
in the fields of education, healthcare, and public policy.In this work, we
concentrate on the study of disentangled representation methods that have shown
promising outcomes by decomposing observed covariates into instrumental,
confounding, and adjustment factors. However, most of the previous work has
primarily revolved around generative models or hard decomposition methods for
covariates, which often struggle to guarantee the attainment of precisely
disentangled factors. In order to effectively model different causal
relationships, we propose a novel treatment effect estimation algorithm that
incorporates a mixture of experts with multi-head attention and a linear
orthogonal regularizer to softly decompose the pre-treatment variables, and
simultaneously eliminates selection bias via importance sampling re-weighting
techniques. We conduct extensive experiments on both public semi-synthetic and
real-world production datasets. The experimental results clearly demonstrate
that our algorithm outperforms the state-of-the-art methods focused on
individual treatment effects.

</details>


### [176] [Federated Learning Inspired Fuzzy Systems: Decentralized Rule Updating for Privacy and Scalable Decision Making](https://arxiv.org/abs/2507.06652)
*Arthur Alexander Lim,Zhen Bin It,Jovan Bowen Heng,Tee Hui Teo*

Main category: cs.LG

TL;DR: 本文讨论模糊系统如何通过机器学习和联邦学习的启发进行改进，包括更新模糊规则等方法。尽管需要进一步研究，其潜力可观。


<details>
  <summary>Details</summary>
Motivation: 研究如何改进现有的模糊系统，使其更好地处理不确定性和其他局限性。

Method: 借鉴机器学习和联邦学习技术，通过更新模糊规则等手段，改善模糊系统的性能和效率。

Result: 提出了改进模糊系统的框架和理念，尽管未提供具体实验，指出了研究方向和潜在巨大改进空间。

Conclusion: 尽管这些改进方案还需深入研究，但其为模糊系统的优化提供了可观的潜力和方向。

Abstract: Fuzzy systems are a way to allow machines, systems and frameworks to deal
with uncertainty, which is not possible in binary systems that most computers
use. These systems have already been deployed for certain use cases, and fuzzy
systems could be further improved as proposed in this paper. Such technologies
to draw inspiration from include machine learning and federated learning.
Machine learning is one of the recent breakthroughs of technology and could be
applied to fuzzy systems to further improve the results it produces. Federated
learning is also one of the recent technologies that have huge potential, which
allows machine learning training to improve by reducing privacy risk, reducing
burden on networking infrastructure, and reducing latency of the latest model.
Aspects from federated learning could be used to improve federated learning,
such as applying the idea of updating the fuzzy rules that make up a key part
of fuzzy systems, to further improve it over time. This paper discusses how
these improvements would be implemented in fuzzy systems, and how it would
improve fuzzy systems. It also discusses certain limitations on the potential
improvements. It concludes that these proposed ideas and improvements require
further investigation to see how far the improvements are, but the potential is
there to improve fuzzy systems.

</details>


### [177] [Heterogeneous Graph Neural Networks for Short-term State Forecasting in Power Systems across Domains and Time Scales: A Hydroelectric Power Plant Case Study](https://arxiv.org/abs/2507.06694)
*Raffael Theiler,Olga Fink*

Main category: cs.LG

TL;DR: 研究提出了一种基于异构图注意网络的短期电力系统状态预测方法，可处理多物理域复杂动态，准确率显著提升。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源和分布式能源的提升，电力系统的变化性增加，因此需要高效可靠地进行短期状态预测以保障电网稳定性和支持控制决策。传统方法难以处理多物理域和异构传感器数据的问题。

Method: 提出一种基于异构图注意网络（Heterogeneous Graph Attention Networks, HGATs）的方法，利用传感器网络的拓扑，模型化同质域内及异质域间的传感器关系，从而整合液压和电气领域的数据进行预测。

Result: 实验表明，与传统基线相比，所提方法在正则化均方根误差（Normalized RMSE）上平均提升35.5%，验证了其在多域、多速率电力系统状态预测中的有效性。

Conclusion: 该研究证明HGATs对多域复杂电力系统的短期状态预测具有显著改善，可为现代电网的运行和决策提供支持。

Abstract: Accurate short-term state forecasting is essential for efficient and stable
operation of modern power systems, especially in the context of increasing
variability introduced by renewable and distributed energy resources. As these
systems evolve rapidly, it becomes increasingly important to reliably predict
their states in the short term to ensure operational stability, support control
decisions, and enable interpretable monitoring of sensor and machine behavior.
Modern power systems often span multiple physical domains - including
electrical, mechanical, hydraulic, and thermal - posing significant challenges
for modeling and prediction. Graph Neural Networks (GNNs) have emerged as a
promising data-driven framework for system state estimation and state
forecasting in such settings. By leveraging the topological structure of sensor
networks, GNNs can implicitly learn inter-sensor relationships and propagate
information across the network. However, most existing GNN-based methods are
designed under the assumption of homogeneous sensor relationships and are
typically constrained to a single physical domain. This limitation restricts
their ability to integrate and reason over heterogeneous sensor data commonly
encountered in real-world energy systems, such as those used in energy
conversion infrastructure. In this work, we propose the use of Heterogeneous
Graph Attention Networks to address these limitations. Our approach models both
homogeneous intra-domain and heterogeneous inter-domain relationships among
sensor data from two distinct physical domains - hydraulic and electrical -
which exhibit fundamentally different temporal dynamics. Experimental results
demonstrate that our method significantly outperforms conventional baselines on
average by 35.5% in terms of normalized root mean square error, confirming its
effectiveness in multi-domain, multi-rate power system state forecasting.

</details>


### [178] [Value from Observations: Towards Large-Scale Imitation Learning via Self-Improvement](https://arxiv.org/abs/2507.06701)
*Michael Bloesch,Markus Wulfmeier,Philemon Brakel,Todor Davchev,Martina Zambelli,Jost Tobias Springenberg,Abbas Abdolmaleki,William F Whitney,Nicolas Heess,Roland Hafner,Martin Riedmiller*

Main category: cs.LG

TL;DR: 本文提出了从不包含动作信息的示范数据中进行模仿学习的新方法，特别适用于更复杂的数据分布场景。


<details>
  <summary>Details</summary>
Motivation: 当前模仿学习主要依赖操作标签或奖励函数，但这些方法在大规模应用中成本高昂，因此需要研究基于无操作信息示范的学习方法。

Method: 该方法基于强化学习思想，利用价值函数在专家与非专家数据之间传递信息，从而实现从无操作示范中学习。

Result: 通过全面的实验评估，作者揭示了不同数据分布下算法的适用性，并指出了现有方法的局限性。

Conclusion: 研究结果为开发更鲁棒和实用的模仿学习技术提供了宝贵见解，为可扩展的行为学习铺平了道路。

Abstract: Imitation Learning from Observation (IfO) offers a powerful way to learn
behaviors at large-scale: Unlike behavior cloning or offline reinforcement
learning, IfO can leverage action-free demonstrations and thus circumvents the
need for costly action-labeled demonstrations or reward functions. However,
current IfO research focuses on idealized scenarios with mostly bimodal-quality
data distributions, restricting the meaningfulness of the results. In contrast,
this paper investigates more nuanced distributions and introduces a method to
learn from such data, moving closer to a paradigm in which imitation learning
can be performed iteratively via self-improvement. Our method adapts RL-based
imitation learning to action-free demonstrations, using a value function to
transfer information between expert and non-expert data. Through comprehensive
evaluation, we delineate the relation between different data distributions and
the applicability of algorithms and highlight the limitations of established
methods. Our findings provide valuable insights for developing more robust and
practical IfO techniques on a path to scalable behaviour learning.

</details>


### [179] [PINN-Obs: Physics-Informed Neural Network-Based Observer for Nonlinear Dynamical Systems](https://arxiv.org/abs/2507.06712)
*Ayoub Farkane,Mohamed Boutayeb,Mustapha Oudani,Mounir Ghogho*

Main category: cs.LG

TL;DR: 介绍了一种基于自适应物理信息神经网络的观测器（PINN-Obs），用于非线性系统的状态估计，显示出其在准确性、鲁棒性和适应性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 解决非线性动力系统中仅有部分且含噪声测量下的状态估计问题，克服传统基于模型的观测器对系统转换或线性化的依赖。

Method: 利用物理信息神经网络，大胆创新地将系统动力学和传感器数据直接集成到学习过程中，并通过自适应学习优化增益矩阵，实现估计状态的收敛。

Result: 在多种非线性系统上进行了广泛的数值仿真，验证了PINN-Obs的有效性。实验研究表明，与现有观测器设计相比，PINN-Obs在准确性、鲁棒性和适应性方面具有明显优势。

Conclusion: 提出了一种新的PINN-Obs方法，并通过理论与实验双重验证展示了其在非线性系统状态估计中的有效性和实用性。

Abstract: State estimation for nonlinear dynamical systems is a critical challenge in
control and engineering applications, particularly when only partial and noisy
measurements are available. This paper introduces a novel Adaptive
Physics-Informed Neural Network-based Observer (PINN-Obs) for accurate state
estimation in nonlinear systems. Unlike traditional model-based observers,
which require explicit system transformations or linearization, the proposed
framework directly integrates system dynamics and sensor data into a
physics-informed learning process. The observer adaptively learns an optimal
gain matrix, ensuring convergence of the estimated states to the true system
states. A rigorous theoretical analysis establishes formal convergence
guarantees, demonstrating that the proposed approach achieves uniform error
minimization under mild observability conditions. The effectiveness of PINN-Obs
is validated through extensive numerical simulations on diverse nonlinear
systems, including an induction motor model, a satellite motion system, and
benchmark academic examples. Comparative experimental studies against existing
observer designs highlight its superior accuracy, robustness, and adaptability.

</details>


### [180] [Mathematical artificial data for operator learning](https://arxiv.org/abs/2507.06752)
*Heng Wu,Benzhuo Lu*

Main category: cs.LG

TL;DR: 本文提出了一种名为MAD（Mathematical Artificial Data）的框架，通过结合物理定律和数据驱动学习，有效改进了微分方程求解的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的微分方程机器学习方法存在两大局限：数据驱动方法依赖高成本的标注数据集，而模型驱动方法面临效率与精度的权衡。

Method: 引入MAD框架，通过利用微分方程的数学结构生成嵌入物理规律的解析解及合成数据，消除了对实验或模拟训练数据的依赖。

Result: 通过数值实验展示了在多参数系统的高效算子学习能力以及在不同微分方程场景中的优越性和普适性。

Conclusion: MAD框架展现了其通用性和潜力，有望成为科学计算领域中物理信息机器智能的通用范式。

Abstract: Machine learning has emerged as a transformative tool for solving
differential equations (DEs), yet prevailing methodologies remain constrained
by dual limitations: data-driven methods demand costly labeled datasets while
model-driven techniques face efficiency-accuracy trade-offs. We present the
Mathematical Artificial Data (MAD) framework, a new paradigm that integrates
physical laws with data-driven learning to facilitate large-scale operator
discovery. By exploiting DEs' intrinsic mathematical structure to generate
physics-embedded analytical solutions and associated synthetic data, MAD
fundamentally eliminates dependence on experimental or simulated training data.
This enables computationally efficient operator learning across multi-parameter
systems while maintaining mathematical rigor. Through numerical demonstrations
spanning 2D parametric problems where both the boundary values and source term
are functions, we showcase MAD's generalizability and superior
efficiency/accuracy across various DE scenarios. This
physics-embedded-data-driven framework and its capacity to handle complex
parameter spaces gives it the potential to become a universal paradigm for
physics-informed machine intelligence in scientific computing.

</details>


### [181] [Robust Deep Network Learning of Nonlinear Regression Tasks by Parametric Leaky Exponential Linear Units (LELUs) and a Diffusion Metric](https://arxiv.org/abs/2507.06765)
*Enda D. V. Bigarella*

Main category: cs.LG

TL;DR: 本文提出了一种参数化激活函数，旨在提升大规模神经网络进行多维非线性数据回归的性能。


<details>
  <summary>Details</summary>
Motivation: 非线性数据集的学习需要使用非线性激活函数，但现有的激活函数在平滑性和梯度特性上存在不足，影响了神经网络的性能。

Method: 引入一种平滑且具有非零梯度的激活函数“Leaky Exponential Linear Unit”，并提出一种新的扩散损失度量用于评估模型的过拟合性能。

Result: 相比传统激活函数，新提出的激活函数实验证明提高了模型性能，减轻了过拟合。

Conclusion: 所提出的激活函数及评估方法有效改善了大规模神经网络的训练表现并减少敏感性。

Abstract: This document proposes a parametric activation function (ac.f.) aimed at
improving multidimensional nonlinear data regression. It is a established
knowledge that nonlinear ac.f.'s are required for learning nonlinear datasets.
This work shows that smoothness and gradient properties of the ac.f. further
impact the performance of large neural networks in terms of overfitting and
sensitivity to model parameters. Smooth but vanishing-gradient ac.f.'s such as
ELU or SiLU have limited performance and non-smooth ac.f.'s such as RELU and
Leaky-RELU further impart discontinuity in the trained model. Improved
performance is demonstrated with a smooth "Leaky Exponential Linear Unit", with
non-zero gradient that can be trained. A novel diffusion-loss metric is also
proposed to gauge the performance of the trained models in terms of
overfitting.

</details>


### [182] [Mutual Information Free Topological Generalization Bounds via Stability](https://arxiv.org/abs/2507.06775)
*Mario Tuci,Lennart Bastian,Benjamin Dupuis,Nassir Navab,Tolga Birdal,Umut Şimşekli*

Main category: cs.LG

TL;DR: 提出了一种基于轨迹稳定性的新框架，用于无互信息项的可解释拓扑泛化界，使其更适用于实际优化算法。


<details>
  <summary>Details</summary>
Motivation: 现有拓扑泛化界依赖复杂的信息论术语，难以应用于实际算法，亟需一种更具解释性和适用性的泛化界。

Method: 通过扩展假设集稳定性概念为轨迹稳定性，提出了一种结合轨迹稳定性和拓扑数据分析的新颖学习理论框架，用以推导泛化界。

Result: 证明了轨迹稳定算法的泛化界可以由优化器轨迹复杂性(TDA指标)和算法的轨迹稳定性参数表述，并通过实验验证了TDA项的关键作用。

Conclusion: 该方法不仅解释了拓扑泛化界的实践成功，还为复杂数据分析提供了理论基础，同时增强了实际优化算法的适用性。

Abstract: Providing generalization guarantees for stochastic optimization algorithms is
a major challenge in modern learning theory. Recently, several studies
highlighted the impact of the geometry of training trajectories on the
generalization error, both theoretically and empirically. Among these works, a
series of topological generalization bounds have been proposed, relating the
generalization error to notions of topological complexity that stem from
topological data analysis (TDA). Despite their empirical success, these bounds
rely on intricate information-theoretic (IT) terms that can be bounded in
specific cases but remain intractable for practical algorithms (such as ADAM),
potentially reducing the relevance of the derived bounds. In this paper, we
seek to formulate comprehensive and interpretable topological generalization
bounds free of intractable mutual information terms. To this end, we introduce
a novel learning theoretic framework that departs from the existing strategies
via proof techniques rooted in algorithmic stability. By extending an existing
notion of \textit{hypothesis set stability}, to \textit{trajectory stability},
we prove that the generalization error of trajectory-stable algorithms can be
upper bounded in terms of (i) TDA quantities describing the complexity of the
trajectory of the optimizer in the parameter space, and (ii) the trajectory
stability parameter of the algorithm. Through a series of experimental
evaluations, we demonstrate that the TDA terms in the bound are of great
importance, especially as the number of training samples grows. This ultimately
forms an explanation of the empirical success of the topological generalization
bounds.

</details>


### [183] [Learning safe, constrained policies via imitation learning: Connection to Probabilistic Inference and a Naive Algorithm](https://arxiv.org/abs/2507.06780)
*George Papadopoulos,George A. Vouros*

Main category: cs.LG

TL;DR: 提出了一种模仿学习方法，用于在多个约束条件下学习顺应专家轨迹的最大熵策略。


<details>
  <summary>Details</summary>
Motivation: 旨在解决如何在强化学习中学习符合约束的策略，同时最大化策略熵的问题。

Method: 通过KL散度将策略学习与验证约束相结合，基于概率推理框架整合强化学习目标并使用双梯度下降优化学习目标。

Result: 实验表明，该方法可以有效地学习满足多重约束的策略模型并具有良好的泛化能力。

Conclusion: 该算法对于学习符合约束的行为策略在理论和实践上均显示出了有效性和稳定性。

Abstract: This article introduces an imitation learning method for learning maximum
entropy policies that comply with constraints demonstrated by expert
trajectories executing a task. The formulation of the method takes advantage of
results connecting performance to bounds for the KL-divergence between
demonstrated and learned policies, and its objective is rigorously justified
through a connection to a probabilistic inference framework for reinforcement
learning, incorporating the reinforcement learning objective and the objective
to abide by constraints in an entropy maximization setting. The proposed
algorithm optimizes the learning objective with dual gradient descent,
supporting effective and stable training. Experiments show that the proposed
method can learn effective policy models for constraints-abiding behaviour, in
settings with multiple constraints of different types, accommodating different
modalities of demonstrated behaviour, and with abilities to generalize.

</details>


### [184] [Speech Tokenizer is Key to Consistent Representation](https://arxiv.org/abs/2507.06802)
*Wonjin Jung,Sungil Kang,Dong-Yeon Cho*

Main category: cs.LG

TL;DR: 本文提出了一种新型语音分词方法，同时编码语言学和声学信息，提升语音表示的保真度，可适用于多种下游任务。


<details>
  <summary>Details</summary>
Motivation: 现有的残差矢量量化（RVQ）模型虽然引入了语义元素，但常忽略关键的声学特征，有必要开发一种能同时编码语言和声学信息的新方法。

Method: 提出一种新型的语音分词方法，该方法通过保留韵律和情感内容，同时对语言学和声学信息进行编码。

Result: 实验表明，该方法在语音编码、语音转换、情感识别和多模态语言建模等任务中表现优异，无需额外训练。

Conclusion: 新型语音分词方法具有广泛的适用性和优越性，有潜力推动AI驱动语音处理的发展。

Abstract: Speech tokenization is crucial in digital speech processing, converting
continuous speech signals into discrete units for various computational tasks.
This paper introduces a novel speech tokenizer with broad applicability across
downstream tasks. While recent advances in residual vector quantization (RVQ)
have incorporated semantic elements, they often neglect critical acoustic
features. We propose an advanced approach that simultaneously encodes both
linguistic and acoustic information, preserving prosodic and emotional content.
Our method significantly enhances speech representation fidelity across diverse
applications. Empirical evaluations demonstrate its effectiveness in speech
coding, voice conversion, emotion recognition, and multimodal language
modeling, without requiring additional training. This versatility underscores
its potential as a key tool for advancing AI-driven speech processing.

</details>


### [185] [Intrinsic Training Signals for Federated Learning Aggregation](https://arxiv.org/abs/2507.06813)
*Cosimo Fiorini,Matteo Mosconi,Pietro Buzzega,Riccardo Salami,Simone Calderara*

Main category: cs.LG

TL;DR: 提出了一种无需修改架构和损失函数的新型联邦学习方法LIVAR，通过现有的训练信号实现高效的模型集成。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习需要对分类头或骨干参数的聚合进行更改，但作者欲开发一种方法，无需架构或损失函数改动即可提升模型表现。

Method: 提出了LIVAR方法，包括基于自然生成特征统计的方差加权分类器聚合方案，以及基于SHAP分析的解释性驱动参数合并技术。

Result: LIVAR在多项基准测试中实现了与现有方法无缝集成的同时达到最先进性能。

Conclusion: 展示了仅用现有训练信号即可实现模型高效合并，为联邦学习模型聚合提供了一种新范式。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients while preserving data privacy. While existing approaches
for aggregating client-specific classification heads and adapted backbone
parameters require architectural modifications or loss function changes, our
method uniquely leverages intrinsic training signals already available during
standard optimization. We present LIVAR (Layer Importance and VARiance-based
merging), which introduces: i) a variance-weighted classifier aggregation
scheme using naturally emergent feature statistics, and ii) an
explainability-driven LoRA merging technique based on SHAP analysis of existing
update parameter patterns. Without any architectural overhead, LIVAR achieves
state-of-the-art performance on multiple benchmarks while maintaining seamless
integration with existing FL methods. This work demonstrates that effective
model merging can be achieved solely through existing training signals,
establishing a new paradigm for efficient federated model aggregation. The code
will be made publicly available upon acceptance.

</details>


### [186] [Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for Large Language Model](https://arxiv.org/abs/2507.06892)
*Jing Liang,Hongyao Tang,Yi Ma,Jinyi Liu,Yan Zheng,Shuyue Hu,Lei Bai,Jianye Hao*

Main category: cs.LG

TL;DR: ReMix方法改进了当前的强化学习（RL）模型训练效率，尤其适用于大语言模型（LLMs）的推理能力提升。


<details>
  <summary>Details</summary>
Motivation: 目前许多强化训练方法使用策略内训练(on-policy)，导致无法充分利用过往数据，增加了计算和时间成本，并阻碍了更高效的扩展。

Method: ReMix引入了混合策略近端策略梯度、KL凸政策约束和策略重生等三大技术，实现了从高效初期学习到稳态后期改进的转变。

Result: 实验表明，ReMix在多个数学推理基准上达到了当前最优性能（SOTA），同时显著降低了训练成本，数据量减少30倍到450倍。

Conclusion: 通过引入ReMix方法，不仅提升了训练效率与性能，还深入揭示了偏离策略带来的隐性现象，推动了RL在大语言模型中的应用。

Abstract: Reinforcement Learning (RL) has demonstrated its potential to improve the
reasoning ability of Large Language Models (LLMs). One major limitation of most
existing Reinforcement Finetuning (RFT) methods is that they are on-policy RL
in nature, i.e., data generated during the past learning process is not fully
utilized. This inevitably comes at a significant cost of compute and time,
posing a stringent bottleneck on continuing economic and efficient scaling. To
this end, we launch the renaissance of off-policy RL and propose Reincarnating
Mix-policy Proximal Policy Gradient (ReMix), a general approach to enable
on-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix
consists of three major components: (1) Mix-policy proximal policy gradient
with an increased Update-To-Data (UTD) ratio for efficient training; (2)
KL-Convex policy constraint to balance the trade-off between stability and
flexibility; (3) Policy reincarnation to achieve a seamless transition from
efficient early-stage learning to steady asymptotic improvement. In our
experiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base
models. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with
0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B
model) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math
reasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and
MATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level
performance with an over 30x to 450x reduction in training cost in terms of
rollout data volume. In addition, we reveal insightful findings via
multifaceted analysis, including the implicit preference for shorter responses
due to the Whipping Effect of off-policy discrepancy, the collapse mode of
self-reflection behavior under the presence of severe off-policyness, etc.

</details>


### [187] [Comprehensive Evaluation of Prototype Neural Networks](https://arxiv.org/abs/2507.06819)
*Philipp Schlinge,Steffen Meinert,Martin Atzmueller*

Main category: cs.LG

TL;DR: 本研究深入分析了一组重要的原型模型（包括ProtoPNet、ProtoPool和PIPNet），提出了一些新的评估指标，并在多样化的数据集上测试，最终并开源了相关代码库。


<details>
  <summary>Details</summary>
Motivation: 近年来，解释性人工智能（XAI）和可解释机器学习的需求日益增加，原型模型作为重要方法，其性能和解释性需进一步分析和提升。

Method: 通过采用一组全面的评估指标（包括标准和新增指标），在多种数据集上（如细粒度分类、非独立同分布和多标签分类）测试原型模型，同时公开代码以便于其他研究人员扩展使用。

Result: 验证了现有和新提出的指标在不同情境下的有效性，对比了不同原型模型在多样化数据集上的性能。

Conclusion: 原型模型在多种领域具有潜力，新开发的评估框架和代码工具进一步推动了该领域的研究和应用。

Abstract: Prototype models are an important method for explainable artificial
intelligence (XAI) and interpretable machine learning. In this paper, we
perform an in-depth analysis of a set of prominent prototype models including
ProtoPNet, ProtoPool and PIPNet. For their assessment, we apply a comprehensive
set of metrics. In addition to applying standard metrics from literature, we
propose several new metrics to further complement the analysis of model
interpretability. In our experimentation, we apply the set of prototype models
on a diverse set of datasets including fine-grained classification, Non-IID
settings and multi-label classification to further contrast the performance.
Furthermore, we also provide our code as an open-source library, which
facilitates simple application of the metrics itself, as well as extensibility
- providing the option for easily adding new metrics and models.
https://github.com/uos-sis/quanproto

</details>


### [188] [HeLo: Heterogeneous Multi-Modal Fusion with Label Correlation for Emotion Distribution Learning](https://arxiv.org/abs/2507.06821)
*Chuhang Zheng,Chunwei Tian,Jie Wen,Daoqiang Zhang,Qi Zhu*

Main category: cs.LG

TL;DR: 提出了HeLo框架，用于多模态情绪分布学习，通过异质性挖掘和标签相关性优化提高识别性能，在公开数据集上表现卓越。


<details>
  <summary>Details</summary>
Motivation: 为解决多模态情绪分布学习中存在的模态间异质性挖掘和标签相关性利用不足问题。

Method: 提出HeLo框架，包括基于跨注意力的融合、生理与行为表征的最优传输模块、可学习标签嵌入优化及整合标签相关性驱动的跨注意机制。

Result: 在两个公开数据集上实验验证，所提方法在情绪分布学习 tasks上表现优于现有方法。

Conclusion: HeLo框架有效挖掘多模态情绪数据中的补充信息和异质性，并优化标签相关性，推动多模态情绪分布学习的研究。

Abstract: Multi-modal emotion recognition has garnered increasing attention as it plays
a significant role in human-computer interaction (HCI) in recent years. Since
different discrete emotions may exist at the same time, compared with
single-class emotion recognition, emotion distribution learning (EDL) that
identifies a mixture of basic emotions has gradually emerged as a trend.
However, existing EDL methods face challenges in mining the heterogeneity among
multiple modalities. Besides, rich semantic correlations across arbitrary basic
emotions are not fully exploited. In this paper, we propose a multi-modal
emotion distribution learning framework, named HeLo, aimed at fully exploring
the heterogeneity and complementary information in multi-modal emotional data
and label correlation within mixed basic emotions. Specifically, we first adopt
cross-attention to effectively fuse the physiological data. Then, an optimal
transport (OT)-based heterogeneity mining module is devised to mine the
interaction and heterogeneity between the physiological and behavioral
representations. To facilitate label correlation learning, we introduce a
learnable label embedding optimized by correlation matrix alignment. Finally,
the learnable label embeddings and label correlation matrices are integrated
with the multi-modal representations through a novel label correlation-driven
cross-attention mechanism for accurate emotion distribution learning.
Experimental results on two publicly available datasets demonstrate the
superiority of our proposed method in emotion distribution learning.

</details>


### [189] [Artificial Generals Intelligence: Mastering Generals.io with Reinforcement Learning](https://arxiv.org/abs/2507.06825)
*Matej Straka,Martin Schmid*

Main category: cs.LG

TL;DR: 本文介绍了一个构建在Generals.io之上的实时策略游戏环境，提供了一个与Gymnasium和PettingZoo兼容的平台，用于多智能体强化学习研究。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体强化学习研究需要具有挑战性和可访问的平台，而Generals.io提供了合适的基础。

Method: 开发了兼容Gymnasium和PettingZoo的实时策略游戏环境，支持快速运行。参考智能体通过监督预训练和自我博弈训练，仅用36小时达到了人类1v1排行榜的0.003%。

Result: 环境运行效率高，参考智能体性能卓越，并采用了奖励塑造和记忆特性以加速学习。

Conclusion: 该实时策略游戏环境和卓越的基线智能体为多智能体强化学习提供了新的研究机会，既具有挑战性又易于使用。

Abstract: We introduce a real-time strategy game environment built on Generals.io, a
game that hosts thousands of active players each week across multiple game
formats. Our environment is fully compatible with Gymnasium and PettingZoo,
capable of running thousands of frames per second on commodity hardware. Our
reference agent -- trained with supervised pre-training and self-play -- hits
the top 0.003\% of the 1v1 human leaderboard after just 36 hours on a single
H100 GPU. To accelerate learning, we incorporate potential-based reward shaping
and memory features. Our contributions -- a modular RTS benchmark and a
competitive, state-of-the-art baseline agent -- provide an accessible yet
challenging platform for advancing multi-agent reinforcement learning research.

</details>


### [190] [Scalable Gaussian Processes: Advances in Iterative Methods and Pathwise Conditioning](https://arxiv.org/abs/2507.06839)
*Jihao Andreas Lin*

Main category: cs.LG

TL;DR: 本论文通过结合迭代方法和路径条件改善高斯过程的可扩展性，实现对大规模数据的支持。


<details>
  <summary>Details</summary>
Motivation: 经典高斯过程计算框架对大规模数据和现代硬件难以扩展，亟需改进其可扩展性。

Method: 结合迭代方法和路径条件，将高昂的计算转化为解决线性方程组的问题，并通过迭代线性系统求解器实现，从而优化内存需求和硬件兼容性。

Result: 提出的方法大幅降低了内存需求，并引入矩阵乘法作为主要操作，使其适用于现代硬件和大规模数据。

Conclusion: 这一改进方法实现了高斯过程在现代大规模数据场景中的高效应用，为大数据场景中的不确定性建模提供了新的解决方案。

Abstract: Gaussian processes are a powerful framework for uncertainty-aware function
approximation and sequential decision-making. Unfortunately, their classical
formulation does not scale gracefully to large amounts of data and modern
hardware for massively-parallel computation, prompting many researchers to
develop techniques which improve their scalability. This dissertation focuses
on the powerful combination of iterative methods and pathwise conditioning to
develop methodological contributions which facilitate the use of Gaussian
processes in modern large-scale settings. By combining these two techniques
synergistically, expensive computations are expressed as solutions to systems
of linear equations and obtained by leveraging iterative linear system solvers.
This drastically reduces memory requirements, facilitating application to
significantly larger amounts of data, and introduces matrix multiplication as
the main computational operation, which is ideal for modern hardware.

</details>


### [191] [DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models](https://arxiv.org/abs/2507.06853)
*Liang Wang,Yu Rong,Tingyang Xu,Zhenyi Zhong,Zhiyuan Liu,Pengju Wang,Deli Zhao,Qiang Liu,Shu Wu,Liang Wang*

Main category: cs.LG

TL;DR: DiffSpectra通过多模态谱数据以及扩散模型直接推断分子的2D和3D结构，并利用SE(3)-等变架构和多模态谱编码器SpecFormer进行条件生成。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决传统方法对专家解释的依赖性和机器学习方法对有限库的依赖性，并探索结合2D和3D生成以及多模态谱数据的新方法。

Method: 提出DiffSpectra，作为基于扩散模型的生成框架，结合SE(3)-等变扩散分子变压器和谱编码器SpecFormer，从多模态谱数据中条件生成分子结构。

Result: DiffSpectra在结构解析中实现了16.01%的一次准确率以及96.86%的前20次准确率，其中3D几何建模和SpecFormer的预训练显著提高了性能。

Conclusion: DiffSpectra开创性地统一了多模态谱推理和2D/3D联合生成建模，为分子结构解析提供了一种有效的新方法。这是首次在谱数据中实现此类方法。

Abstract: Molecular structure elucidation from spectra is a foundational problem in
chemistry, with profound implications for compound identification, synthesis,
and drug development. Traditional methods rely heavily on expert interpretation
and lack scalability. Pioneering machine learning methods have introduced
retrieval-based strategies, but their reliance on finite libraries limits
generalization to novel molecules. Generative models offer a promising
alternative, yet most adopt autoregressive SMILES-based architectures that
overlook 3D geometry and struggle to integrate diverse spectral modalities. In
this work, we present DiffSpectra, a generative framework that directly infers
both 2D and 3D molecular structures from multi-modal spectral data using
diffusion models. DiffSpectra formulates structure elucidation as a conditional
generation process. Its denoising network is parameterized by Diffusion
Molecule Transformer, an SE(3)-equivariant architecture that integrates
topological and geometric information. Conditioning is provided by SpecFormer,
a transformer-based spectral encoder that captures intra- and inter-spectral
dependencies from multi-modal spectra. Extensive experiments demonstrate that
DiffSpectra achieves high accuracy in structure elucidation, recovering exact
structures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through
sampling. The model benefits significantly from 3D geometric modeling,
SpecFormer pre-training, and multi-modal conditioning. These results highlight
the effectiveness of spectrum-conditioned diffusion modeling in addressing the
challenge of molecular structure elucidation. To our knowledge, DiffSpectra is
the first framework to unify multi-modal spectral reasoning and joint 2D/3D
generative modeling for de novo molecular structure elucidation.

</details>


### [192] [Episodic Contextual Bandits with Knapsacks under Conversion Models](https://arxiv.org/abs/2507.06859)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 本研究针对在不同情境下的带背包问题提出了一种新的在线算法，实现了次线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 探索在非平稳情境和资源约束下如何优化决策，应用包括可耗资源动态定价和首次价格拍卖。

Method: 提出了一种基于信心界定Oracle的在线算法，同时考虑非平稳情境和无限状态空间的问题。

Result: 该算法能达到次线性遗憾，并在提供未标注特征数据时表现更优。

Conclusion: 本研究扩展了上下文带背包问题的理论框架，通过结合现有算法提供了更好的遗憾界限。

Abstract: We study an online setting, where a decision maker (DM) interacts with
contextual bandit-with-knapsack (BwK) instances in repeated episodes. These
episodes start with different resource amounts, and the contexts' probability
distributions are non-stationary in an episode. All episodes share the same
latent conversion model, which governs the random outcome contingent upon a
request's context and an allocation decision. Our model captures applications
such as dynamic pricing on perishable resources with episodic replenishment,
and first price auctions in repeated episodes with different starting budgets.
We design an online algorithm that achieves a regret sub-linear in $T$, the
number of episodes, assuming access to a \emph{confidence bound oracle} that
achieves an $o(T)$-regret. Such an oracle is readily available from existing
contextual bandit literature. We overcome the technical challenge with
arbitrarily many possible contexts, which leads to a reinforcement learning
problem with an unbounded state space. Our framework provides improved regret
bounds in certain settings when the DM is provided with unlabeled feature data,
which is novel to the contextual BwK literature.

</details>


### [193] [Horizontal and Vertical Federated Causal Structure Learning via Higher-order Cumulants](https://arxiv.org/abs/2507.06888)
*Wei Chen,Wanyang Gu,Linjun Peng,Ruichu Cai,Zhifeng Hao,Kun Zhang*

Main category: cs.LG

TL;DR: 提出了一种用于水平和垂直联邦场景下的因果结构学习方法，使用高阶累积量实现全局因果强度矩阵的构造，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决联邦因果发现中因变量缺失而导致的虚假因果关系问题，同时平衡数据隐私保护和因果关系揭示的需求。

Method: 利用各客户端的高阶累积量信息，构建全局估计，并通过递归源识别得到全局因果强度矩阵，实现因果图的重构和因果强度系数的估测。

Result: 在合成数据和真实数据实验中，提出的方法表现优于现有方法。

Conclusion: 该方法不仅有效地解决了不同类型联邦场景的因果结构学习问题，还在保护数据隐私的同时，提升了因果发现的准确性。

Abstract: Federated causal discovery aims to uncover the causal relationships between
entities while protecting data privacy, which has significant importance and
numerous applications in real-world scenarios. Existing federated causal
structure learning methods primarily focus on horizontal federated settings.
However, in practical situations, different clients may not necessarily contain
data on the same variables. In a single client, the incomplete set of variables
can easily lead to spurious causal relationships, thereby affecting the
information transmitted to other clients. To address this issue, we
comprehensively consider causal structure learning methods under both
horizontal and vertical federated settings. We provide the identification
theories and methods for learning causal structure in the horizontal and
vertical federal setting via higher-order cumulants. Specifically, we first
aggregate higher-order cumulant information from all participating clients to
construct global cumulant estimates. These global estimates are then used for
recursive source identification, ultimately yielding a global causal strength
matrix. Our approach not only enables the reconstruction of causal graphs but
also facilitates the estimation of causal strength coefficients. Our algorithm
demonstrates superior performance in experiments conducted on both synthetic
data and real-world data.

</details>


### [194] [Designing Adaptive Algorithms Based on Reinforcement Learning for Dynamic Optimization of Sliding Window Size in Multi-Dimensional Data Streams](https://arxiv.org/abs/2507.06901)
*Abolfazl Zarghani,Sadegh Abedi*

Main category: cs.LG

TL;DR: 本文提出了一种基于强化学习的滑动窗口优化方法，旨在动态调整多维数据流的窗口大小，并通过对比验证表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了处理多维数据流中的高速度、无限性及维度间复杂性，解决固定窗口大小难以适应概念漂移及突增模式的问题。

Method: 将动态滑动窗口大小选择建模为强化学习问题，并用Dueling DQN与经验回放机制优化策略。

Result: 实验结果表明，该方法在分类精度、漂移鲁棒性和计算效率上优于当前最先进方法（如ADWIN和CNN-Adaptive）。

Conclusion: RL-Window方法通过提升适应性和稳定性，证明其在实时应用中的优势。

Abstract: Multi-dimensional data streams, prevalent in applications like IoT, financial
markets, and real-time analytics, pose significant challenges due to their high
velocity, unbounded nature, and complex inter-dimensional dependencies. Sliding
window techniques are critical for processing such streams, but fixed-size
windows struggle to adapt to dynamic changes like concept drift or bursty
patterns. This paper proposes a novel reinforcement learning (RL)-based
approach to dynamically optimize sliding window sizes for multi-dimensional
data streams. By formulating window size selection as an RL problem, we enable
an agent to learn an adaptive policy based on stream characteristics, such as
variance, correlations, and temporal trends. Our method, RL-Window, leverages a
Dueling Deep Q-Network (DQN) with prioritized experience replay to handle
non-stationarity and high-dimensionality. Evaluations on benchmark datasets
(UCI HAR, PAMAP2, Yahoo! Finance Stream) demonstrate that RL-Window outperforms
state-of-the-art methods like ADWIN and CNN-Adaptive in classification
accuracy, drift robustness, and computational efficiency. Additional
qualitative analyses, extended metrics (e.g., energy efficiency, latency), and
a comprehensive dataset characterization further highlight its adaptability and
stability, making it suitable for real-time applications.

</details>


### [195] [Robust and Safe Traffic Sign Recognition using N-version with Weighted Voting](https://arxiv.org/abs/2507.06907)
*Linyun Gao,Qiang Wen,Fumio Machida*

Main category: cs.LG

TL;DR: 提出了一种基于N版本机器学习的框架，通过加权投票提高交通标志识别系统的安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 针对自动驾驶中的交通标志识别易受对抗攻击的问题，提出保护驾驶安全的方案。

Method: 基于Failure Mode and Effects Analysis（FMEA），通过安全感知权重的软投票机制，优化N版本机器学习的性能。

Result: 实验表明，该方法能有效提高交通标志识别系统应对对抗攻击的鲁棒性和安全性。

Conclusion: NVML框架在提升自动驾驶中交通标志识别系统安全性方面具有显著效果，尤其在对抗条件下表现突出。

Abstract: Autonomous driving is rapidly advancing as a key application of machine
learning, yet ensuring the safety of these systems remains a critical
challenge. Traffic sign recognition, an essential component of autonomous
vehicles, is particularly vulnerable to adversarial attacks that can compromise
driving safety. In this paper, we propose an N-version machine learning (NVML)
framework that integrates a safety-aware weighted soft voting mechanism. Our
approach utilizes Failure Mode and Effects Analysis (FMEA) to assess potential
safety risks and assign dynamic, safety-aware weights to the ensemble outputs.
We evaluate the robustness of three-version NVML systems employing various
voting mechanisms against adversarial samples generated using the Fast Gradient
Sign Method (FGSM) and Projected Gradient Descent (PGD) attacks. Experimental
results demonstrate that our NVML approach significantly enhances the
robustness and safety of traffic sign recognition systems under adversarial
conditions.

</details>


### [196] [DICE: Data Influence Cascade in Decentralized Learning](https://arxiv.org/abs/2507.06931)
*Tongtian Zhu,Wenhao Li,Can Wang,Fengxiang He*

Main category: cs.LG

TL;DR: 提出了DICE方法以解决去中心化学习中公平激励的问题，通过估算数据影响级联为节点贡献提供公平评估。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化学习中因缺乏公平激励机制导致参与度低的问题，通过公平评估节点贡献设计出激励机制。

Method: 设计了DICE方法，能够在去中心化环境中估算数据影响的级联，理论上利用邻居节点间的影响级联近似计算，考虑数据、通信拓扑与损失曲率的相互作用。

Result: DICE可以估算任意邻居层数的影响级联，为选择合适的协作者和识别恶意行为提供了基础。

Conclusion: DICE作为第一个估算数据影响级联的方法，对去中心化学习公平激励及实际应用具有重要意义。

Abstract: Decentralized learning offers a promising approach to crowdsource data
consumptions and computational workloads across geographically distributed
compute interconnected through peer-to-peer networks, accommodating the
exponentially increasing demands. However, proper incentives are still in
absence, considerably discouraging participation. Our vision is that a fair
incentive mechanism relies on fair attribution of contributions to
participating nodes, which faces non-trivial challenges arising from the
localized connections making influence ``cascade'' in a decentralized network.
To overcome this, we design the first method to estimate \textbf{D}ata
\textbf{I}nfluence \textbf{C}ascad\textbf{E} (DICE) in a decentralized
environment. Theoretically, the framework derives tractable approximations of
influence cascade over arbitrary neighbor hops, suggesting the influence
cascade is determined by an interplay of data, communication topology, and the
curvature of loss landscape. DICE also lays the foundations for applications
including selecting suitable collaborators and identifying malicious behaviors.
Project page is available at https://raiden-zhu.github.io/blog/2025/DICE/.

</details>


### [197] [What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models](https://arxiv.org/abs/2507.06952)
*Keyon Vafa,Peter G. Chang,Ashesh Rambachan,Sendhil Mullainathan*

Main category: cs.LG

TL;DR: 研究探讨基础模型能否仅通过序列预测理解更深层次领域结构，并提出了一种评估模型归纳偏差的新方法，实验表明这些模型在适应新任务时可能无法很好地概括。


<details>
  <summary>Details</summary>
Motivation: 探讨基础模型是否能够通过训练任务获得对真实世界模型的深刻理解，尤其是在适应新任务时其归纳偏差是否符合真实世界模型。

Method: 提出一种称为归纳偏差探针的方法，通过让模型适应由假定的世界模型生成的合成数据集，评估其归纳偏差与世界模型的契合程度。

Result: 实验发现，在多个领域中，尽管基础模型能很好完成训练任务，但在适应新任务时未能生成符合底层世界模型的归纳偏差。例如，训练于轨道运动的模型在新物理任务中未能应用牛顿力学。

Conclusion: 基础模型可能倾向于开发任务特定的启发式方法，而非普遍适用的理论，从而在跨任务泛化能力上表现不足。

Abstract: Foundation models are premised on the idea that sequence prediction can
uncover deeper domain understanding, much like how Kepler's predictions of
planetary motion later led to the discovery of Newtonian mechanics. However,
evaluating whether these models truly capture deeper structure remains a
challenge. We develop a technique for evaluating foundation models that
examines how they adapt to synthetic datasets generated from some postulated
world model. Our technique measures whether the foundation model's inductive
bias aligns with the world model, and so we refer to it as an inductive bias
probe. Across multiple domains, we find that foundation models can excel at
their training tasks yet fail to develop inductive biases towards the
underlying world model when adapted to new tasks. We particularly find that
foundation models trained on orbital trajectories consistently fail to apply
Newtonian mechanics when adapted to new physics tasks. Further analysis reveals
that these models behave as if they develop task-specific heuristics that fail
to generalize.

</details>


### [198] [Noisy PDE Training Requires Bigger PINNs](https://arxiv.org/abs/2507.06967)
*Sebastien Andre-Sloan,Anirbit Mukherjee,Matthew Colbrook*

Main category: cs.LG

TL;DR: 本研究研究了在有噪声的数据情况下，物理信息神经网络 (PINNs) 的有效性，并推导了其参数需求的下界条件。


<details>
  <summary>Details</summary>
Motivation: 研究在实际应用中数据样本有噪声的情况下，PINNs 仍然能够有效实现低经验风险的条件。

Method: 推导了在监督和非监督 PINN 场景下，经验风险低于噪声方差水平时神经网络所需参数的下界条件，并通过案例研究基于 Hamilton–Jacobi–Bellman (HJB) PDE 验证了以上推论。

Result: 证明了PINNs若要在噪声监督标签下实现低经验风险，需要满足条件 $d_N\log d_N\gtrsim N_s \eta^2$，并通过实验验证该结论。同时表明增加噪声监督标签数量本身并不能有效降低经验风险。

Conclusion: 本文为在噪声数据环境下训练PINNs的参数需求提供了定量化理解的基础，有助于指导实际应用中的PINNs设计。

Abstract: Physics-Informed Neural Networks (PINNs) are increasingly used to approximate
solutions of partial differential equations (PDEs), especially in high
dimensions. In real-world applications, data samples are noisy, so it is
important to know when a predictor can still achieve low empirical risk.
However, little is known about the conditions under which a PINN can do so
effectively. We prove a lower bound on the size of neural networks required for
the supervised PINN empirical risk to fall below the variance of noisy
supervision labels. Specifically, if a predictor achieves an empirical risk
$O(\eta)$ below $\sigma^2$ (variance of supervision data), then necessarily
$d_N\log d_N\gtrsim N_s \eta^2$, where $N_s$ is the number of samples and $d_N$
is the number of trainable parameters of the PINN. A similar constraint applies
to the fully unsupervised PINN setting when boundary labels are sampled
noisily. Consequently, increasing the number of noisy supervision labels alone
does not provide a ``free lunch'' in reducing empirical risk. We also show
empirically that PINNs can indeed achieve empirical risks below $\sigma^2$
under such conditions. As a case study, we investigate PINNs applied to the
Hamilton--Jacobi--Bellman (HJB) PDE. Our findings lay the groundwork for
quantitatively understanding the parameter requirements for training PINNs in
the presence of noise.

</details>


### [199] [Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy](https://arxiv.org/abs/2507.06969)
*Bogdan Kulynych,Juan Felipe Gomez,Georgios Kaissis,Jamie Hayes,Borja Balle,Flavio du Pin Calmon,Jean Louis Raisaro*

Main category: cs.LG

TL;DR: 该论文提出了一个统一框架，根据假设检验的解释，改进了现有微分隐私机制的风险评估和噪声调整。


<details>
  <summary>Details</summary>
Motivation: 目前的微分隐私机制难以解释，也缺乏一致性，特别是在实际隐私威胁中的风险评估上，例如重新识别、属性推断以及数据重构。

Method: 作者采用了基于假设检验解释的$f$-DP方法，通过统一的形式来绑定这些风险，并提出可调整的边界来更好地评估风险。

Result: 实验结果表明，其结果比现有的$\varepsilon$-DP、R\'enyi DP和集中DP方法更加紧密，减少20%噪声需求，同时提高了任务准确性。

Conclusion: 本文的统一框架为微分隐私在重新识别、属性推断和数据重构风险下提供了一个更为一致和可调整的评估和校准方法。

Abstract: Differentially private (DP) mechanisms are difficult to interpret and
calibrate because existing methods for mapping standard privacy parameters to
concrete privacy risks -- re-identification, attribute inference, and data
reconstruction -- are both overly pessimistic and inconsistent. In this work,
we use the hypothesis-testing interpretation of DP ($f$-DP), and determine that
bounds on attack success can take the same unified form across
re-identification, attribute inference, and data reconstruction risks. Our
unified bounds are (1) consistent across a multitude of attack settings, and
(2) tunable, enabling practitioners to evaluate risk with respect to arbitrary
(including worst-case) levels of baseline risk. Empirically, our results are
tighter than prior methods using $\varepsilon$-DP, R\'enyi DP, and concentrated
DP. As a result, calibrating noise using our bounds can reduce the required
noise by 20% at the same risk level, which yields, e.g., more than 15pp
accuracy increase in a text classification task. Overall, this unifying
perspective provides a principled framework for interpreting and calibrating
the degree of protection in DP against specific levels of re-identification,
attribute inference, or data reconstruction risk.

</details>


### [200] [A Principled Framework for Multi-View Contrastive Learning](https://arxiv.org/abs/2507.06979)
*Panagiotis Koromilas,Efthymios Georgiou,Giorgos Bouritsas,Theodoros Giannakopoulos,Mihalis A. Nicolaou,Yannis Panagakis*

Main category: cs.LG

TL;DR: 本论文提出两种新型损失函数MV-InfoNCE和MV-DHEL，以优化多视角对比学习中现存方法的缺陷，并在多个数据集上显示了其显著性能提升与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的多视角对比学习方法通过简单地聚合不同的双视角目标来处理额外视角，但此方法存在多个优化目标冲突、未建模所有视角交互等问题。

Method: 提出两种新方法：MV-InfoNCE将所有可能的视角交互整合到一个优化项中，MV-DHEL将视角的对齐与均匀性分离且与视角的多样性扩展相一致。两种方法都有理论证明支持，它们优化多视角对齐与均匀性的目标。

Result: 实验证明，在ImageNet1K等多个数据集和多模态数据中，两种方法均优于现有多视角对比学习方法，且随视角数量增加表现出良好的扩展性。

Conclusion: 论文提出的方法解决了多视角对比学习的关键问题，使其能够更好地利用多视角的潜力，并有效缓解了维度塌缩问题。

Abstract: Contrastive Learning (CL), a leading paradigm in Self-Supervised Learning
(SSL), typically relies on pairs of data views generated through augmentation.
While multiple augmentations per instance (more than two) improve
generalization in supervised learning, current CL methods handle additional
views suboptimally by simply aggregating different pairwise objectives. This
approach suffers from four critical limitations: (L1) it utilizes multiple
optimization terms per data point resulting to conflicting objectives, (L2) it
fails to model all interactions across views and data points, (L3) it inherits
fundamental limitations (e.g. alignment-uniformity coupling) from pairwise CL
losses, and (L4) it prevents fully realizing the benefits of increased view
multiplicity observed in supervised settings. We address these limitations
through two novel loss functions: MV-InfoNCE, which extends InfoNCE to
incorporate all possible view interactions simultaneously in one term per data
point, and MV-DHEL, which decouples alignment from uniformity across views
while scaling interaction complexity with view multiplicity. Both approaches
are theoretically grounded - we prove they asymptotically optimize for
alignment of all views and uniformity, providing principled extensions to
multi-view contrastive learning. Our empirical results on ImageNet1K and three
other datasets demonstrate that our methods consistently outperform existing
multi-view approaches and effectively scale with increasing view multiplicity.
We also apply our objectives to multimodal data and show that, in contrast to
other contrastive objectives, they can scale beyond just two modalities. Most
significantly, ablation studies reveal that MV-DHEL with five or more views
effectively mitigates dimensionality collapse by fully utilizing the embedding
space, thereby delivering multi-view benefits observed in supervised learning.

</details>


### [201] [Generating Multi-Table Time Series EHR from Latent Space with Minimal Preprocessing](https://arxiv.org/abs/2507.06996)
*Eunbyeol Cho,Jiyoun Kim,Minjae Lee,Sungjin Park,Edward Choi*

Main category: cs.LG

TL;DR: 本文提出了RawMed，这是第一个用于合成多表时间序列EHR数据的框架，能够生成与原始EHR相似的数据，同时保证隐私保护。


<details>
  <summary>Details</summary>
Motivation: 由于隐私问题和法规限制，共享和使用敏感的EHR数据受到限制，因此需要开发合成EHR数据集的工具。

Method: 本文提出了RawMed框架，利用基于文本的表示和压缩技术，来捕获复杂结构和时间动态。并开发了一个新的评估框架，考察分布相似性、表间关系、时间动态和隐私性。

Result: 在两个开源EHR数据集上的验证结果显示，RawMed在保真性和实用性方面优于基线模型。

Conclusion: RawMed成功地实现了生成与原始EHR数据相似的多表时间序列数据，同时提供了更高的隐私保护和数据真实性。

Abstract: Electronic Health Records (EHR) are time-series relational databases that
record patient interactions and medical events over time, serving as a critical
resource for healthcare research and applications. However, privacy concerns
and regulatory restrictions limit the sharing and utilization of such sensitive
data, necessitating the generation of synthetic EHR datasets. Unlike previous
EHR synthesis methods, which typically generate medical records consisting of
expert-chosen features (e.g. a few vital signs or structured codes only), we
introduce RawMed, the first framework to synthesize multi-table, time-series
EHR data that closely resembles raw EHRs. Using text-based representation and
compression techniques, RawMed captures complex structures and temporal
dynamics with minimal preprocessing. We also propose a new evaluation framework
for multi-table time-series synthetic EHRs, assessing distributional
similarity, inter-table relationships, temporal dynamics, and privacy.
Validated on two open-source EHR datasets, RawMed outperforms baseline models
in fidelity and utility. The code is available at
https://github.com/eunbyeol-cho/RawMed.

</details>


### [202] [Exact Evaluation of the Accuracy of Diffusion Models for Inverse Problems with Gaussian Data Distributions](https://arxiv.org/abs/2507.07008)
*Emile Pierret,Bruno Galerne*

Main category: cs.LG

TL;DR: 本文探讨了扩散模型作为贝叶斯反问题中的先验模型，其在去模糊任务中的性能进行评估。


<details>
  <summary>Details</summary>
Motivation: 研究如何更好地利用扩散模型解决非确定逆问题中的多解问题，并评估其准确性。

Method: 在受限的高斯数据分布和去模糊任务中，计算Wasserstein距离，分析扩散模型采样的分布与理想解分布之间的差异。

Result: 揭示了理论逆问题分辨率与扩散模型实际分辨率之间的差距。

Conclusion: 通过精确地计算分布差异，为文献中不同算法的比较提供了依据。

Abstract: Used as priors for Bayesian inverse problems, diffusion models have recently
attracted considerable attention in the literature. Their flexibility and high
variance enable them to generate multiple solutions for a given task, such as
inpainting, super-resolution, and deblurring. However, several unresolved
questions remain about how well they perform. In this article, we investigate
the accuracy of these models when applied to a Gaussian data distribution for
deblurring. Within this constrained context, we are able to precisely analyze
the discrepancy between the theoretical resolution of inverse problems and
their resolution obtained using diffusion models by computing the exact
Wasserstein distance between the distribution of the diffusion model sampler
and the ideal distribution of solutions to the inverse problem. Our findings
allow for the comparison of different algorithms from the literature.

</details>


### [203] [On-Device Training of PV Power Forecasting Models in a Smart Meter for Grid Edge Intelligence](https://arxiv.org/abs/2507.07016)
*Jian Huang,Yongli Zhu,Linna Xu,Zhe Zheng,Wenpeng Cui,Mingyang Sun*

Main category: cs.LG

TL;DR: 对资源有限的智能电表进行边缘侧模型训练研究，提出网格边缘智能的动机和设备端训练概念，进行技术准备，展示光伏功率预测案例，验证其经济可行性。


<details>
  <summary>Details</summary>
Motivation: 为解决资源受限环境中实现网格边缘智能的需求，提出设备端训练以更有效利用资源。

Method: 对智能电表设备进行技术改造，提出混合精度和降低精度训练方案，使用梯度提升树和循环神经网络模型进行光伏功率预测实验。

Result: 实验结果表明，通过现有的先进计量基础设施可以经济地实现网格边缘智能训练。

Conclusion: 设备端训练方案在智能电表上实现了光伏功率预测任务，展示了资源有限环境下经济地实现边缘智能的潜力。

Abstract: In this paper, an edge-side model training study is conducted on a
resource-limited smart meter. The motivation of grid-edge intelligence and the
concept of on-device training are introduced. Then, the technical preparation
steps for on-device training are described. A case study on the task of
photovoltaic power forecasting is presented, where two representative machine
learning models are investigated: a gradient boosting tree model and a
recurrent neural network model. To adapt to the resource-limited situation in
the smart meter, "mixed"- and "reduced"-precision training schemes are also
devised. Experiment results demonstrate the feasibility of economically
achieving grid-edge intelligence via the existing advanced metering
infrastructures.

</details>


### [204] [PLAME: Leveraging Pretrained Language Models to Generate Enhanced Protein Multiple Sequence Alignments](https://arxiv.org/abs/2507.07032)
*Hanqun Cao,Xinyi Zhou,Zijun Gao,Chenyu Wang,Xin Gao,Zhi Zhang,Chunbin Gu,Ge Liu,Pheng-Ann Heng*

Main category: cs.LG

TL;DR: 提出了PLAME，一种依赖于预训练蛋白质语言模型的新的MSA设计模型，解决了低同源性和孤儿蛋白质预测中的问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型对多序列比对（MSA）依赖性强，导致对低同源性和孤儿蛋白预测效果不佳。

Method: 开发了PLAME模型，利用预训练的蛋白质语言模型嵌入并引入保守-多样性损失提升生成质量，还提出了新的MSA筛选和序列质量评估方法。

Result: 在AlphaFold2的低同源性和孤儿蛋白基准测试中，PLAME在折叠增强和序列质量评估方面达到了最新性能。

Conclusion: 通过实验验证了MSA筛选方法的有效性，展示了PLAME对蛋白质预测质量的提升，并证明其可以在具有较快推理速度的模型中达到与AlphaFold2相当的准确性。

Abstract: Protein structure prediction is essential for drug discovery and
understanding biological functions. While recent advancements like AlphaFold
have achieved remarkable accuracy, most folding models rely heavily on multiple
sequence alignments (MSAs) to boost prediction performance. This dependency
limits their effectiveness on low-homology proteins and orphan proteins, where
MSA information is sparse or unavailable. To address this limitation, we
propose PLAME, a novel MSA design model that leverages evolutionary embeddings
from pretrained protein language models. Unlike existing methods, PLAME
introduces pretrained representations to enhance evolutionary information and
employs a conservation-diversity loss to enhance generation quality.
Additionally, we propose a novel MSA selection method to effectively screen
high-quality MSAs and improve folding performance. We also propose a sequence
quality assessment metric that provides an orthogonal perspective to evaluate
MSA quality. On the AlphaFold2 benchmark of low-homology and orphan proteins,
PLAME achieves state-of-the-art performance in folding enhancement and sequence
quality assessment, with consistent improvements demonstrated on AlphaFold3.
Ablation studies validate the effectiveness of the MSA selection method, while
extensive case studies on various protein types provide insights into the
relationship between AlphaFold's prediction quality and MSA characteristics.
Furthermore, we demonstrate that PLAME can serve as an adapter achieving
AlphaFold2-level accuracy with the ESMFold's inference speed.

</details>


### [205] [Self-Supervised Learning at the Edge: The Cost of Labeling](https://arxiv.org/abs/2507.07033)
*Roberto Pereira,Fernanda Famá,Asal Rangrazi,Marco Miozzo,Charalampos Kalalas,Paolo Dini*

Main category: cs.LG

TL;DR: 对比学习（CL）和自监督学习（SSL）在资源受限的边缘设备上的可行性和效率被探讨，研究了性能与能效之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上资源有限（计算能力、数据、能量），因此探索SSL技术的潜力以符合这些限制，同时降低能耗。

Method: 分析并评估不同SSL技术在资源受限条件下的适应能力和性能，研究半监督学习如何降低CL模型的训练能耗并进行广泛实验验证。

Result: 量身定制的SSL策略在性能和资源消耗之间取得显著平衡，资源消耗降低了4倍，同时维持竞争力的表现。

Conclusion: SSL技术在边缘设备上具备潜力，通过适当的优化方案，可实现能效高的学习。

Abstract: Contrastive learning (CL) has recently emerged as an alternative to
traditional supervised machine learning solutions by enabling rich
representations from unstructured and unlabeled data. However, CL and, more
broadly, self-supervised learning (SSL) methods often demand a large amount of
data and computational resources, posing challenges for deployment on
resource-constrained edge devices. In this work, we explore the feasibility and
efficiency of SSL techniques for edge-based learning, focusing on trade-offs
between model performance and energy efficiency. In particular, we analyze how
different SSL techniques adapt to limited computational, data, and energy
budgets, evaluating their effectiveness in learning robust representations
under resource-constrained settings. Moreover, we also consider the energy
costs involved in labeling data and assess how semi-supervised learning may
assist in reducing the overall energy consumed to train CL models. Through
extensive experiments, we demonstrate that tailored SSL strategies can achieve
competitive performance while reducing resource consumption by up to 4X,
underscoring their potential for energy-efficient learning at the edge.

</details>


### [206] [An Ensemble Embedding Approach for Improving Semantic Caching Performance in LLM-based Systems](https://arxiv.org/abs/2507.07061)
*Shervin Ghaffari,Zohre Bahranifard,Mohammad Akbari*

Main category: cs.LG

TL;DR: 本文提出了一种通过训练的元编码器结合多种嵌入模型的集成嵌入方法，用于改进大语言模型(LLM)的语义缓存系统，其显著提高了缓存命中率和准确性能，同时降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有语义缓存框架多依赖单一嵌入模型来表示查询，限制了其在处理实际查询分布中多样语义关系的能力，因此亟需一种改进方法提升语义相似性检测的表现。

Method: 通过引入多个嵌入模型并用训练的元编码器集成它们，以提升语义相似性识别性能。

Result: 在Quora问题对(QQP)数据集上进行评估，方法实现了92%的语义等价查询缓存命中率，同时正确拒绝非等价查询作为缓存未命中的准确率为85%。

Conclusion: 集成嵌入方法在区分语义相似和不相似查询上显著优于单模型方法，由此优化了语义缓存性能并减少了LLM系统的计算开销。

Abstract: Semantic caching enhances the efficiency of large language model (LLM)
systems by identifying semantically similar queries, storing responses once,
and serving them for subsequent equivalent requests. However, existing semantic
caching frameworks rely on single embedding models for query representation,
which limits their ability to capture the diverse semantic relationships
present in real-world query distributions. This paper presents an ensemble
embedding approach that combines multiple embedding models through a trained
meta-encoder to improve semantic similarity detection in LLM caching systems.
We evaluate our method using the Quora Question Pairs (QQP) dataset, measuring
cache hit ratios, cache miss ratios, token savings, and response times. Our
ensemble approach achieves a 92\% cache hit ratio for semantically equivalent
queries while maintaining an 85\% accuracy in correctly rejecting
non-equivalent queries as cache misses. These results demonstrate that ensemble
embedding methods significantly outperform single-model approaches in
distinguishing between semantically similar and dissimilar queries, leading to
more effective caching performance and reduced computational overhead in
LLM-based systems.

</details>


### [207] [Addressing Imbalanced Domain-Incremental Learning through Dual-Balance Collaborative Experts](https://arxiv.org/abs/2507.07100)
*Lan Li,Da-Wei Zhou,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.LG

TL;DR: 本文提出了一个名为Dual-Balance Collaborative Experts (DCE)的框架，来解决领域增量学习（DIL）中因数据不平衡问题引起的模型性能退化。


<details>
  <summary>Details</summary>
Motivation: 当前的领域增量学习需要应对非平稳环境中不断演进的领域数据，同时面临着严重的数据不平衡问题（包括域内类别不平衡和跨域类别分布偏移），这些问题严重影响了模型的性能。

Method: 提出的DCE框架基于频率感知专家组，利用特定损失函数设计专家学习特定频率组特征，并通过平衡的高斯采样生成伪特征，从而动态选择专家，同时保留过去任务的多样样本信息并提升针对稀有样本的表现。

Result: DCE在四个基准数据集上取得了最先进的结果，显示出其优越的性能。

Conclusion: 该研究表明，通过专门设计的频率感知专家组以及动态专家选择机制，DCE框架有效应对了领域增量学习中因数据不平衡导致的问题，实现了优秀的跨域知识迁移与保留能力。

Abstract: Domain-Incremental Learning (DIL) focuses on continual learning in
non-stationary environments, requiring models to adjust to evolving domains
while preserving historical knowledge. DIL faces two critical challenges in the
context of imbalanced data: intra-domain class imbalance and cross-domain class
distribution shifts. These challenges significantly hinder model performance,
as intra-domain imbalance leads to underfitting of few-shot classes, while
cross-domain shifts require maintaining well-learned many-shot classes and
transferring knowledge to improve few-shot class performance in old domains. To
overcome these challenges, we introduce the Dual-Balance Collaborative Experts
(DCE) framework. DCE employs a frequency-aware expert group, where each expert
is guided by specialized loss functions to learn features for specific
frequency groups, effectively addressing intra-domain class imbalance.
Subsequently, a dynamic expert selector is learned by synthesizing
pseudo-features through balanced Gaussian sampling from historical class
statistics. This mechanism navigates the trade-off between preserving many-shot
knowledge of previous domains and leveraging new data to improve few-shot class
performance in earlier tasks. Extensive experimental results on four benchmark
datasets demonstrate DCE's state-of-the-art performance.

</details>


### [208] [Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful](https://arxiv.org/abs/2507.07101)
*Martin Marek,Sanae Lotfi,Aditya Somasundaram,Andrew Gordon Wilson,Micah Goldblum*

Main category: cs.LG

TL;DR: 该研究探讨了小批量大小在语言模型预训练和微调时的稳定性，提出了一种针对Adam优化器超参数的调整规则，并发现小批量大小可以训练稳定，且性能优于较大批量。


<details>
  <summary>Details</summary>
Motivation: 挑战传统观念，通过研究批量大小对语言模型性能的影响，验证小批量是否不稳定并提出优化策略。

Method: 分析批量大小对Adam优化器超参数的影响，提出适用于小批量的超参数调整规则，并进行了实验验证其对训练稳定性和性能的影响。

Result: 发现小批量大小训练稳定性高，对超参数选择更具鲁棒性，性能优于大批量，且无需动量的SGD也能带来稳定的训练性能。

Conclusion: 小批量训练效果良好，无需梯度累积且优化性能更高，因此建议根据具体设置选择合适的批量大小及优化器超参数。

Abstract: Conventional wisdom dictates that small batch sizes make language model
pretraining and fine-tuning unstable, motivating gradient accumulation, which
trades off the number of optimizer steps for a proportional increase in batch
size. While it is common to decrease the learning rate for smaller batch sizes,
other hyperparameters are often held fixed. In this work, we revisit small
batch sizes all the way down to batch size one, and we propose a rule for
scaling Adam hyperparameters to small batch sizes. We find that small batch
sizes (1) train stably, (2) are consistently more robust to hyperparameter
choices, (3) achieve equal or better per-FLOP performance than larger batch
sizes, and (4) notably enable stable language model training with vanilla SGD,
even without momentum, despite storing no optimizer state. Building on these
results, we provide practical recommendations for selecting a batch size and
setting optimizer hyperparameters. We further recommend against gradient
accumulation unless training on multiple devices with multiple model replicas,
bottlenecked by inter-device bandwidth.

</details>


### [209] [Does Data Scaling Lead to Visual Compositional Generalization?](https://arxiv.org/abs/2507.07102)
*Arnas Uselis,Andrea Dittadi,Seong Joon Oh*

Main category: cs.LG

TL;DR: 研究评估了视觉模型的组成理解能力，发现数据多样性比数据规模对组成概括更关键，并提出线性分解表示结构对于高效学习的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前主流的机器学习假设认为扩大数据和模型规模可以提升跨分布性能，但是否适用于组成概括尚不清楚，研究旨在检验这一假设。

Method: 设计受控实验系统性地改变数据规模、概念多样性和组合覆盖率，并评估DINO、CLIP等预训练模型的性能。

Result: 发现组成概括受数据多样性驱动，而非数据规模；提升组合覆盖率会促使模型发现线性分解表示结构。

Conclusion: 研究强调应更加关注多样化数据集的构建，以及对高效组成学习有重要意义的表示结构。

Abstract: Compositional understanding is crucial for human intelligence, yet it remains
unclear whether contemporary vision models exhibit it. The dominant machine
learning paradigm is built on the premise that scaling data and model sizes
will improve out-of-distribution performance, including compositional
generalization. We test this premise through controlled experiments that
systematically vary data scale, concept diversity, and combination coverage. We
find that compositional generalization is driven by data diversity, not mere
data scale. Increased combinatorial coverage forces models to discover a
linearly factored representational structure, where concepts decompose into
additive components. We prove this structure is key to efficiency, enabling
perfect generalization from few observed combinations. Evaluating pretrained
models (DINO, CLIP), we find above-random yet imperfect performance, suggesting
partial presence of this structure. Our work motivates stronger emphasis on
constructing diverse datasets for compositional generalization, and considering
the importance of representational structure that enables efficient
compositional learning. Code available at
https://github.com/oshapio/visual-compositional-generalization.

</details>
