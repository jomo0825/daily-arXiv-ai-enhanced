<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 96]
- [cs.CL](#cs.CL) [Total: 66]
- [cs.AI](#cs.AI) [Total: 46]
- [cs.LG](#cs.LG) [Total: 150]
- [cs.NE](#cs.NE) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Computer Vision based Automated Quantification of Agricultural Sprayers Boom Displacement](https://arxiv.org/abs/2506.19939)
*Aryan Singh Dalal,Sidharth Rai,Rahul Singh,Treman Singh Kaloya,Rahul Harsha Cheppally,Ajay Sharda*

Main category: cs.CV

TL;DR: 本文开发了一种基于计算机视觉的系统，通过神经网络模型实时跟踪喷雾器横杆的运动，具有超过90%的检测准确率，并为改善喷雾器设计提供数据支持。


<details>
  <summary>Details</summary>
Motivation: 解决农业生产中由于喷雾器横杆不稳定导致的施药率误差问题。

Method: 开发基于计算机视觉的系统，其中使用YOLO V7、V8和V11神经网络模型实时跟踪横杆运动，并使用倾角传感器验证模型输出的精度。

Result: 系统能够以超过90%的准确率检测目标，距离估算与倾角传感器数据相差不超过0.026米。

Conclusion: 该系统可以量化现有喷雾器和其他喷雾器的横杆运动，为横杆设计改进和施药准确性提升提供数据支持。

Abstract: Application rate errors when using self-propelled agricultural sprayers for
agricultural production remain a concern. Among other factors, spray boom
instability is one of the major contributors to application errors. Spray
booms' width of 38m, combined with 30 kph driving speeds, varying terrain, and
machine dynamics when maneuvering complex field boundaries, make controls of
these booms very complex. However, there is no quantitative knowledge on the
extent of boom movement to systematically develop a solution that might include
boom designs and responsive boom control systems. Therefore, this study was
conducted to develop an automated computer vision system to quantify the boom
movement of various agricultural sprayers. A computer vision system was
developed to track a target on the edge of the sprayer boom in real time. YOLO
V7, V8, and V11 neural network models were trained to track the boom's
movements in field operations to quantify effective displacement in the
vertical and transverse directions. An inclinometer sensor was mounted on the
boom to capture boom angles and validate the neural network model output. The
results showed that the model could detect the target with more than 90 percent
accuracy, and distance estimates of the target on the boom were within 0.026 m
of the inclinometer sensor data. This system can quantify the boom movement on
the current sprayer and potentially on any other sprayer with minor
modifications. The data can be used to make design improvements to make sprayer
booms more stable and achieve greater application accuracy.

</details>


### [2] [EBC-ZIP: Improving Blockwise Crowd Counting with Zero-Inflated Poisson Regression](https://arxiv.org/abs/2506.19955)
*Yiming Ma,Victor Sanchez,Tanaya Guha*

Main category: cs.CV

TL;DR: 提出一种基于零膨胀泊松分布（ZIP）的框架EBC-ZIP，用于密度图估计，更好地处理稀疏分布问题，实现更高的准确性，并优于当前的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人群计数方法通常忽视真实密度图的极端稀疏性，导致模型性能在不同密度区域间不平衡。此外，常用的损失函数基于均方误差（MSE），假设高斯分布，与离散且非负的计数数据特征不符。

Method: 使用零膨胀泊松分布（ZIP）回归替代传统回归损失，以负对数似然为目标函数，更好地处理零稀疏分布；并融入增强块分类（EBC）的框架以提升稳定性和准确性。

Result: 在四个主流的人群计数基准测试中，EBC-ZIP一致性地超越了EBC并实现了最先进的性能。

Conclusion: EBC-ZIP结合概率模型与增强分类框架，平衡了稀疏和密集区域的计数分布，解决了现存方法的短板并树立了新标准。

Abstract: Density map estimation has become the mainstream paradigm in crowd counting.
However, most existing methods overlook the extreme sparsity of ground-truth
density maps. In real-world crowd scenes, the vast majority of spatial regions
(often over 95%) contain no people, leading to heavily imbalanced count
distributions. Ignoring this imbalance can bias models toward overestimating
dense regions and underperforming in sparse areas. Furthermore, most loss
functions used in density estimation are majorly based on MSE and implicitly
assume Gaussian distributions, which are ill-suited for modeling discrete,
non-negative count data. In this paper, we propose EBC-ZIP, a crowd counting
framework that models the spatial distribution of counts using a Zero-Inflated
Poisson (ZIP) regression formulation. Our approach replaces the traditional
regression loss with the negative log-likelihood of the ZIP distribution,
enabling better handling of zero-heavy distributions while preserving count
accuracy. Built upon the recently proposed Enhanced Block Classification (EBC)
framework, EBC-ZIP inherits EBC's advantages in preserving the discreteness of
targets and ensuring training stability, while further improving performance
through a more principled probabilistic loss. We also evaluate EBC-ZIP with
backbones of varying computational complexity to assess its scalability.
Extensive experiments on four crowd counting benchmarks demonstrate that
EBC-ZIP consistently outperforms EBC and achieves state-of-the-art results.

</details>


### [3] [ToSA: Token Merging with Spatial Awareness](https://arxiv.org/abs/2506.20066)
*Hsiang-Wei Huang,Wenhao Chai,Kuang-Ming Chen,Cheng-Yen Yang,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: 提出了ToSA，一种结合语义和空间感知的新的token合并方法，提升了ViT的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有token合并方法主要利用视觉token的特征相似性，而忽视了早期ViT中可以利用的空间信息。

Method: 引入ToSA，通过深度图像生成伪空间token，结合语义和空间信息指导ViT中的token合并。

Result: ToSA在多个基准测试中优于现有方法，同时显著减少了ViT的运行时间。

Conclusion: ToSA以效率提升为目标，通过综合语义和空间的token合并策略，加速了ViT并保留了重要场景结构。

Abstract: Token merging has emerged as an effective strategy to accelerate Vision
Transformers (ViT) by reducing computational costs. However, existing methods
primarily rely on the visual token's feature similarity for token merging,
overlooking the potential of integrating spatial information, which can serve
as a reliable criterion for token merging in the early layers of ViT, where the
visual tokens only possess weak visual information. In this paper, we propose
ToSA, a novel token merging method that combines both semantic and spatial
awareness to guide the token merging process. ToSA leverages the depth image as
input to generate pseudo spatial tokens, which serve as auxiliary spatial
information for the visual token merging process. With the introduced spatial
awareness, ToSA achieves a more informed merging strategy that better preserves
critical scene structure. Experimental results demonstrate that ToSA
outperforms previous token merging methods across multiple benchmarks on visual
and embodied question answering while largely reducing the runtime of the ViT,
making it an efficient solution for ViT acceleration. The code will be
available at: https://github.com/hsiangwei0903/ToSA

</details>


### [4] [BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos](https://arxiv.org/abs/2506.20103)
*Jiahao Lin,Weixuan Peng,Bojia Zi,Yifeng Gao,Xianbiao Qi,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本研究引入了BrokenVideos数据集——一个涵盖3254个经过人工校验的AI生成视频，以及像素级注释的基准数据集，用于检测和定位视频中的视觉伪影区域。


<details>
  <summary>Details</summary>
Motivation: 当前生成视频中经常出现视觉伪影（如运动不一致、物理不合理轨迹、物体变形等），导致缺乏真实感与用户信任，需要一种方法来检测并精确定位这些伪影的具体位置。

Method: 提出了一个名为BrokenVideos的新基准数据集，包括3,254个AI生成视频及像素级标注。通过人工验证确保标注质量；并利用其训练和评估伪影检测模型及多模态大语言模型，验证其对定位伪影的提升效果。

Result: 利用BrokenVideos数据集进行实验验证，结果显示其显著提升了伪影区域的定位能力，并成为评测生成视频中伪影定位研究的关键基准。

Conclusion: BrokenVideos数据集填补了现有数据集在视频伪影精确定位领域的空白，为改进生成视频模型的研究提供了重要的评估工具。

Abstract: Recent advances in deep generative models have led to significant progress in
video generation, yet the fidelity of AI-generated videos remains limited.
Synthesized content often exhibits visual artifacts such as temporally
inconsistent motion, physically implausible trajectories, unnatural object
deformations, and local blurring that undermine realism and user trust.
Accurate detection and spatial localization of these artifacts are crucial for
both automated quality control and for guiding the development of improved
generative models. However, the research community currently lacks a
comprehensive benchmark specifically designed for artifact localization in AI
generated videos. Existing datasets either restrict themselves to video or
frame level detection or lack the fine-grained spatial annotations necessary
for evaluating localization methods. To address this gap, we introduce
BrokenVideos, a benchmark dataset of 3,254 AI-generated videos with
meticulously annotated, pixel-level masks highlighting regions of visual
corruption. Each annotation is validated through detailed human inspection to
ensure high quality ground truth. Our experiments show that training state of
the art artifact detection models and multi modal large language models (MLLMs)
on BrokenVideos significantly improves their ability to localize corrupted
regions. Through extensive evaluation, we demonstrate that BrokenVideos
establishes a critical foundation for benchmarking and advancing research on
artifact localization in generative video models. The dataset is available at:
https://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/.

</details>


### [5] [From 2D to 3D Cognition: A Brief Survey of General World Models](https://arxiv.org/abs/2506.20134)
*Ningwei Xie,Zizi Tian,Lei Yang,Xiao-Ping Zhang,Meng Guo,Jie Li*

Main category: cs.CV

TL;DR: 本研究调查了从2D感知向3D认知演进的世界模型，提出一个框架以梳理该领域最新进展，同时识别关键技术驱动和挑战，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于世界模型在发展人工通用智能中的关键角色，尤其是在从2D向3D空间认知转变过程中，缺乏系统分类和分析。

Method: 通过引入一个概念框架，分类研究3D世界建模，强调3D表示和世界知识的进步，并分析相关认知能力的发展及其在实际应用中的部署。

Result: 研究揭示了世界模型在3D物理场景生成、空间推理和交互中的核心能力，同时指出数据、建模和实践中的挑战。

Conclusion: 该研究为3D世界模型的未来发展提出了系统性的方向，提出打造更健壮且可泛化的认知模型的建议，并明确了核心领域中的潜在研究机会。

Abstract: World models have garnered increasing attention in the development of
artificial general intelligence (AGI), serving as computational frameworks for
learning representations of the external world and forecasting future states.
While early efforts focused on 2D visual perception and simulation, recent
3D-aware generative world models have demonstrated the ability to synthesize
geometrically consistent, interactive 3D environments, marking a shift toward
3D spatial cognition. Despite rapid progress, the field lacks systematic
analysis to categorize emerging techniques and clarify their roles in advancing
3D cognitive world models. This survey addresses this need by introducing a
conceptual framework, providing a structured and forward-looking review of
world models transitioning from 2D perception to 3D cognition. Within this
framework, we highlight two key technological drivers, particularly advances in
3D representations and the incorporation of world knowledge, as fundamental
pillars. Building on these, we dissect three core cognitive capabilities that
underpin 3D world modeling: 3D physical scene generation, 3D spatial reasoning,
and 3D spatial interaction. We further examine the deployment of these
capabilities in real-world applications, including embodied AI, autonomous
driving, digital twin, and gaming/VR. Finally, we identify challenges across
data, modeling, and deployment, and outline future directions for advancing
more robust and generalizable 3D world models.

</details>


### [6] [EAR: Erasing Concepts from Unified Autoregressive Models](https://arxiv.org/abs/2506.20151)
*Haipeng Fan,Shiyuan Zhang,Baohunesitu,Zihang Guo,Huaiwen Zhang*

Main category: cs.CV

TL;DR: 提出了一种用于AR模型的有效概念消除方法（EAR），并通过全面的基准测试验证其性能。


<details>
  <summary>Details</summary>
Motivation: 解决AR模型中既要有效删除非期望概念，又要保持整体生成质量的挑战。

Method: 引入了窗口梯度累积（WGA）方法和阈值损失屏蔽（TLM）方法以实现有效的概念删除。同时，设计了一个新的评测基准ECGVF，利用LLM和视觉分类器生成和验证测试数据。

Result: 通过在ECGVF基准测试上实验，证明EAR在概念删除效果和模型实用性保持方面均有显著提升。

Conclusion: EAR是一种高效的概念消除方法，设计了严谨的评测基准并在实验中展现出强有力的性能，为AR模型的概念控制提供了重要工具。

Abstract: Autoregressive (AR) models have achieved unified and strong performance
across both visual understanding and image generation tasks. However, removing
undesired concepts from AR models while maintaining overall generation quality
remains an open challenge. In this paper, we propose Erasure Autoregressive
Model (EAR), a fine-tuning method for effective and utility-preserving concept
erasure in AR models. Specifically, we introduce Windowed Gradient Accumulation
(WGA) strategy to align patch-level decoding with erasure objectives, and
Thresholded Loss Masking (TLM) strategy to protect content unrelated to the
target concept during fine-tuning. Furthermore, we propose a novel benchmark,
Erase Concept Generator and Visual Filter (ECGVF), aim at provide a more
rigorous and comprehensive foundation for evaluating concept erasure in AR
models. Specifically, we first employ structured templates across diverse large
language models (LLMs) to pre-generate a large-scale corpus of
target-replacement concept prompt pairs. Subsequently, we generate images from
these prompts and subject them to rigorous filtering via a visual classifier to
ensure concept fidelity and alignment. Extensive experimental results conducted
on the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR
achieves marked improvements in both erasure effectiveness and model utility
preservation. Code is available at: https://github.com/immc-lab/ear/

</details>


### [7] [Loss-Aware Automatic Selection of Structured Pruning Criteria for Deep Neural Network Acceleration](https://arxiv.org/abs/2506.20152)
*Deepak Ghimire,Kilho Lee,Seong-heum Kim*

Main category: cs.CV

TL;DR: 提出一种名为LAASP的高效剪枝方法，通过训练中剪枝，缩小模型规模并加速运算，同时实现性能提升和浮点操作减少。


<details>
  <summary>Details</summary>
Motivation: 解决现有神经网络剪枝技术中的效率问题，适应资源受限的边缘设备部署需求。

Method: 通过一种名为LAASP的机制，在训练过程中整合剪枝与微调过程，自动选择剪枝准则，并基于样本损失指导剪枝层的决策，同时动态确定各层的最佳剪枝比例。

Result: 在CIFAR-10和ImageNet数据集上测试有显著效果：CIFAR-10上ResNet56和ResNet110模型的top-1精度提升并减少52% FLOPs；ImageNet上ResNet50减少超过42% FLOPs，top-5精度仅下降0.33%。

Conclusion: 该方法通过训练中剪枝及损失感知的自动准则选择，显著减少FLOPs且保持精度，提供高效网络压缩解决方案。

Abstract: Structured pruning is a well-established technique for compressing neural
networks, making it suitable for deployment in resource-limited edge devices.
This paper presents an efficient Loss-Aware Automatic Selection of Structured
Pruning Criteria (LAASP) for slimming and accelerating deep neural networks.
The majority of pruning methodologies employ a sequential process consisting of
three stages: 1) training, 2) pruning, and 3) fine-tuning, whereas the proposed
pruning technique adopts a pruning-while-training approach that eliminates the
first stage and integrates the second and third stages into a single cycle. The
automatic selection of magnitude or similarity-based filter pruning criteria
from a specified pool of criteria and the specific pruning layer at each
pruning iteration is guided by the network's overall loss on a small subset of
the training data. To mitigate the abrupt accuracy drop due to pruning, the
network is retrained briefly after each reduction of a predefined number of
floating-point operations (FLOPs). The optimal pruning rates for each layer in
the network are automatically determined, eliminating the need for manual
allocation of fixed or variable pruning rates for each layer. Experiments on
the VGGNet and ResNet models on the CIFAR-10 and ImageNet benchmark datasets
demonstrate the effectiveness of the proposed method. In particular, the
ResNet56 and ResNet110 models on the CIFAR-10 dataset significantly improve the
top-1 accuracy compared to state-of-the-art methods while reducing the network
FLOPs by 52\%. Furthermore, the ResNet50 model on the ImageNet dataset reduces
FLOPs by more than 42\% with a negligible 0.33\% drop in top-5 accuracy. The
source code of this paper is publicly available online -
https://github.com/ghimiredhikura/laasp.

</details>


### [8] [Towards Efficient Exemplar Based Image Editing with Multimodal VLMs](https://arxiv.org/abs/2506.20155)
*Avadhoot Jadhav,Ashutosh Srivastava,Abhinav Java,Silky Singh,Tarun Ram Menta,Surgan Jandial,Balaji Krishnamurthy*

Main category: cs.CV

TL;DR: 本论文探讨通过示例对的方式进行图像编辑，以克服单纯依赖文本所带来的局限性，提出了无需优化的端到端方法，并证明了其在多种编辑任务中的优越性。


<details>
  <summary>Details</summary>
Motivation: 图像编辑任务本质模糊，单靠文本难以全面表达编辑意图，因此需要探索基于图像示例对的方法。

Method: 结合预训练的文本到图像扩散模型和多模态视觉语言模型，提出无需优化的端到端图像编辑流程。

Result: 实验表明，该方法在多种编辑类型上优于现有方法，同时提高了编辑效率（速度快4倍）。

Conclusion: 示例对驱动的端到端处理方式，有效填补了文本编辑的局限，为高效、多样化的图像编辑提供了新的可能性。

Abstract: Text-to-Image Diffusion models have enabled a wide array of image editing
applications. However, capturing all types of edits through text alone can be
challenging and cumbersome. The ambiguous nature of certain image edits is
better expressed through an exemplar pair, i.e., a pair of images depicting an
image before and after an edit respectively. In this work, we tackle
exemplar-based image editing -- the task of transferring an edit from an
exemplar pair to a content image(s), by leveraging pretrained text-to-image
diffusion models and multimodal VLMs. Even though our end-to-end pipeline is
optimization-free, our experiments demonstrate that it still outperforms
baselines on multiple types of edits while being ~4x faster.

</details>


### [9] [Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2506.20168)
*Zhentao He,Can Zhang,Ziheng Wu,Zhenghao Chen,Yufei Zhan,Yifan Li,Zhao Zhang,Xian Wang,Minghui Qiu*

Main category: cs.CV

TL;DR: 这篇论文介绍了一种新的基准数据集（KIE-HVQA）和基于GRPO框架的方法，用于评估和减少降级文档理解中的OCR幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在处理真实世界降级视觉数据时容易出现语言依赖或视觉-文本推理对齐问题，导致生成幻觉内容，因此需要应对这些不确定性挑战。

Method: 提出了KIE-HVQA数据集，包含带有真实世界降级的身份卡和发票测试样本；引入了一种基于GRPO的框架，并设计了视觉不确定性自我感知和拒绝回答机制，通过监督微调和强化学习减少幻觉。

Result: 实验表明，所开发的7B参数模型在KIE-HVQA上无幻觉准确率比GPT-4o提高了22%，且在标准任务中没有显著性能下降，表现出工具的有效性和鲁棒性。

Conclusion: 通过结合创新数据集和框架设计，显著改善了模型在降级文档场景中识别不确定性和避免幻觉内容生成的能力，为多模态模型的未来发展提供了新思路。

Abstract: Recent advancements in multimodal large language models have enhanced
document understanding by integrating textual and visual information. However,
existing models exhibit incompleteness within their paradigm in real-world
scenarios, particularly under visual degradation. In such conditions, the
current response paradigm often fails to adequately perceive visual degradation
and ambiguity, leading to overreliance on linguistic priors or misaligned
visual-textual reasoning. This difficulty in recognizing uncertainty frequently
results in the generation of hallucinatory content, especially when a precise
answer is not feasible. To better demonstrate and analyze this phenomenon and
problem, we propose KIE-HVQA, the first benchmark dedicated to evaluating OCR
hallucination in degraded document understanding. This dataset includes test
samples spanning identity cards and invoices, with simulated real-world
degradations for OCR reliability. This setup allows for evaluating models'
capacity, under degraded input, to distinguish reliable visual information and
answer accordingly, thereby highlighting the challenge of avoiding
hallucination on uncertain data. To achieve vision-faithful reasoning and
thereby avoid the aforementioned issues, we further introduce a GRPO-based
framework featuring a novel reward mechanism. By incorporating a self-awareness
of visual uncertainty and an analysis method that initiates refusal to answer
to increase task difficulty within our supervised fine-tuning and reinforcement
learning framework, we successfully mitigated hallucinations in ambiguous
regions. Experiments on Qwen2.5-VL demonstrate that our 7B-parameter model
achieves a 22\% absolute improvement in hallucination-free accuracy over GPT-4o
on KIE-HVQA and there is no significant performance drop in standard tasks,
highlighting both effectiveness and robustness.

</details>


### [10] [Towards Scalable and Generalizable Earth Observation Data Mining via Foundation Model Composition](https://arxiv.org/abs/2506.20174)
*Man Duc Chuc*

Main category: cs.CV

TL;DR: 该论文探讨了如何通过组合现有预训练模型来改进地球观测任务的性能，提出了特征级集成和知识蒸馏方法，结果显示这些方法能在减少资源需求的同时达到或超越大模型的表现。


<details>
  <summary>Details</summary>
Motivation: 地球观测数据挖掘中的大模型通常需要庞大的数据和计算资源，而通过组合已有模型可能提供一个更高效的替代方案。

Method: 评估了多个预训练模型（如Prithvi、Hiera、DOFA）的性能，并使用特征级集成和知识蒸馏等技术，在GEO-Bench基准上分析其效果。

Result: 小型预训练模型的特征级集成在多任务上达到了与更大模型相当或更高的性能，同时显著降低了训练时间与计算成本。

Conclusion: 通过集成和知识蒸馏技术，能够在保持高性能的情况下显著减小模型规模，为地球观测任务模型的实用化提供了新思路。

Abstract: Foundation models are rapidly transforming Earth Observation data mining by
enabling generalizable and scalable solutions for key tasks such as scene
classification and semantic segmentation. While most efforts in the geospatial
domain have focused on developing large models trained from scratch using
massive Earth Observation datasets, an alternative strategy that remains
underexplored is the reuse and combination of existing pretrained models. In
this study, we investigate whether foundation models pretrained on remote
sensing and general vision datasets can be effectively combined to improve
performance across a diverse set of key Earth Observation tasks. Using the
GEO-Bench benchmark, we evaluate several prominent models, including Prithvi,
Hiera, and DOFA, on eleven datasets covering a range of spatial resolutions,
sensor modalities, and task types. The results show that feature-level
ensembling of smaller pretrained models can match or exceed the performance of
much larger models, while requiring less training time and computational
resources. Moreover, the study highlights the potential of applying knowledge
distillation to transfer the strengths of ensembles into more compact models,
offering a practical path for deploying foundation models in real-world Earth
Observation applications.

</details>


### [11] [Progressive Alignment Degradation Learning for Pansharpening](https://arxiv.org/abs/2506.20179)
*Enzhe Zhao,Zhichang Guo,Yao Li,Fanghui Song,Boying Wu*

Main category: cs.CV

TL;DR: 提出了一种新的深度学习泛色增强方法，克服了Wald协议的不足，显著提升了高分辨率多光谱图像的细节和质量。


<details>
  <summary>Details</summary>
Motivation: 目前的泛色增强方法依赖于Wald协议创建的合成数据，但其对真实世界退化模式的描述不够准确，限制了模型的泛化能力。

Method: 引入渐进对齐退化模块（PADM），通过两个子网络（PAlignNet和PDegradeNet）进行相互迭代学习真实退化过程，并结合高频嵌入扩散框架HFreqdiff，以及CFB和BACM模块，提升细节提取和逆过程学习的精度。

Result: 提出的方法在实验和消融研究中表现优异，显著超过了当前最先进的方法。

Conclusion: 通过自适应学习退化过程与高频嵌入的创新设计，本方法有效融合高分辨率泛色与多光谱图像，极大提升了多光谱图像的空间清晰度和质量。

Abstract: Deep learning-based pansharpening has been shown to effectively generate
high-resolution multispectral (HRMS) images. To create supervised ground-truth
HRMS images, synthetic data generated using the Wald protocol is commonly
employed. This protocol assumes that networks trained on artificial
low-resolution data will perform equally well on high-resolution data. However,
well-trained models typically exhibit a trade-off in performance between
reduced-resolution and full-resolution datasets. In this paper, we delve into
the Wald protocol and find that its inaccurate approximation of real-world
degradation patterns limits the generalization of deep pansharpening models. To
address this issue, we propose the Progressive Alignment Degradation Module
(PADM), which uses mutual iteration between two sub-networks, PAlignNet and
PDegradeNet, to adaptively learn accurate degradation processes without relying
on predefined operators. Building on this, we introduce HFreqdiff, which embeds
high-frequency details into a diffusion framework and incorporates CFB and BACM
modules for frequency-selective detail extraction and precise reverse process
learning. These innovations enable effective integration of high-resolution
panchromatic and multispectral images, significantly enhancing spatial
sharpness and quality. Experiments and ablation studies demonstrate the
proposed method's superior performance compared to state-of-the-art techniques.

</details>


### [12] [UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2506.20214)
*Yanzhe Chen,Huasong Zhong,Yan Li,Zhenheng Yang*

Main category: cs.CV

TL;DR: 提出了一种新的级联代码簿框架UniCode^2，用于实现更稳定、容量更大的视觉离散化，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于代码簿方法要么语义精度较低，要么训练不稳定，需要一种能够同时扩展容量和保持稳定性的方法。

Method: 通过聚类数百万个SigLIP序列嵌入，构建一个50万个条目的代码簿，采用级联设计：一个冻结的代码簿锚定嵌入空间，一个可训练的代码簿优化任务特定的语义。

Result: 实现高质量的视觉合成，代码簿的扩展能力不会影响训练的稳定性、语义表达能力和模块化设计。

Conclusion: UniCode^2展示了在不牺牲稳定性和语义对齐的情况下扩展视觉标记空间的可行性，适用于多模态大语言模型的开发。

Abstract: Unified multimodal large language models (MLLMs) have shown promise in
jointly advancing multimodal understanding and generation, with visual
codebooks discretizing images into tokens for autoregressive modeling. Existing
codebook-based methods either rely on small vocabularies (~16K entries) that
lack fine-grained semantics or naively scale up, resulting in low token
utilization and unstable training. We propose UniCode$^2$, a cascaded codebook
framework enabling large-scale, semantically aligned, and stable visual
tokenization. By clustering millions of SigLIP sequence embeddings, we build a
500K-entry codebook that preserves vision-language alignment while expanding
capacity. Stability is ensured via a cascaded design: a frozen codebook anchors
the embedding space, and a trainable codebook refines task-specific semantics.
This decoupling promotes high utilization and robust learning. Moreover, the
alignment of our visual tokens with textual semantics enables seamless
integration with pretrained diffusion decoders, supporting high-quality visual
synthesis with minimal adaptation. UniCode^2 delivers strong performance across
diverse benchmarks, demonstrating the viability of scaling visual token spaces
without sacrificing stability, semantics, or modularity.

</details>


### [13] [Dynamic Bandwidth Allocation for Hybrid Event-RGB Transmission](https://arxiv.org/abs/2506.20222)
*Pujing Yang,Guangyi Zhang,Yunlong Cai,Lei Yu,Guanding Yu*

Main category: cs.CV

TL;DR: 本文提出了一种针对事件相机和RGB相机联合传输的高效方案，利用贝叶斯建模和信息瓶颈方法解决数据传输的冗余问题，并实现实时去模糊。


<details>
  <summary>Details</summary>
Motivation: 事件相机与RGB相机联合使用时存在数据传输效率低下的问题，其中冗余信息是主要挑战。

Method: 通过建立联合事件和图像（E-I）传输框架，利用贝叶斯建模及信息瓶颈方法来解析共享与领域特定的信息，并根据场景动态自适应分配传输带宽。

Result: 实验结果表明，这一方案不仅在重建质量上优于传统方法，还提供了更优的实时去模糊性能。

Conclusion: 提出了一个结合事件相机与RGB相机的高效传输框架，实现了数据传输优化和去模糊效果提升。

Abstract: Event cameras asynchronously capture pixel-level intensity changes with
extremely low latency. They are increasingly used in conjunction with RGB
cameras for a wide range of vision-related applications. However, a major
challenge in these hybrid systems lies in the transmission of the large volume
of triggered events and RGB images. To address this, we propose a transmission
scheme that retains efficient reconstruction performance of both sources while
accomplishing real-time deblurring in parallel. Conventional RGB cameras and
event cameras typically capture the same scene in different ways, often
resulting in significant redundant information across their outputs. To address
this, we develop a joint event and image (E-I) transmission framework to
eliminate redundancy and thereby optimize channel bandwidth utilization. Our
approach employs Bayesian modeling and the information bottleneck method to
disentangle the shared and domain-specific information within the E-I inputs.
This disentangled information bottleneck framework ensures both the compactness
and informativeness of extracted shared and domain-specific information.
Moreover, it adaptively allocates transmission bandwidth based on scene
dynamics, i.e., more symbols are allocated to events for dynamic details or to
images for static information. Simulation results demonstrate that the proposed
scheme not only achieves superior reconstruction quality compared to
conventional systems but also delivers enhanced deblurring performance.

</details>


### [14] [Recognizing Surgical Phases Anywhere: Few-Shot Test-time Adaptation and Task-graph Guided Refinement](https://arxiv.org/abs/2506.20254)
*Kun Yuan,Tingxuan Chen,Shi Li,Joel L. Lavanchy,Christian Heiliger,Ege Özsoy,Yiming Huang,Long Bai,Nassir Navab,Vinkle Srivastav,Hongliang Ren,Nicolas Padoy*

Main category: cs.CV

TL;DR: 提出了一个名为SPA的框架，通过轻量化的方式适应手术工作流程理解，以增强跨机构和跨手术过程的应用能力。


<details>
  <summary>Details</summary>
Motivation: 由于手术中不同环境、流程和解剖的复杂性，难以开发通用化的模型来应对跨机构和流程的多样需求，需要一种能够高效适配场景的模型。

Method: 提出SPA框架，包括少量样本的空间适配、通过扩散建模保证的时间一致性，以及利用测试时动态适配机制，实现轻量化的多模态手术阶段理解。

Result: 实验表明SPA框架在跨机构和多种手术程序的少样本阶段识别中达到了当前顶尖水平，并超过了配有32个标注样本的全量模型。

Conclusion: SPA框架能够通过轻量化定制实现高效的手术阶段识别，帮助医院快速适应新的手术环境和需求。

Abstract: The complexity and diversity of surgical workflows, driven by heterogeneous
operating room settings, institutional protocols, and anatomical variability,
present a significant challenge in developing generalizable models for
cross-institutional and cross-procedural surgical understanding. While recent
surgical foundation models pretrained on large-scale vision-language data offer
promising transferability, their zero-shot performance remains constrained by
domain shifts, limiting their utility in unseen surgical environments. To
address this, we introduce Surgical Phase Anywhere (SPA), a lightweight
framework for versatile surgical workflow understanding that adapts foundation
models to institutional settings with minimal annotation. SPA leverages
few-shot spatial adaptation to align multi-modal embeddings with
institution-specific surgical scenes and phases. It also ensures temporal
consistency through diffusion modeling, which encodes task-graph priors derived
from institutional procedure protocols. Finally, SPA employs dynamic test-time
adaptation, exploiting the mutual agreement between multi-modal phase
prediction streams to adapt the model to a given test video in a
self-supervised manner, enhancing the reliability under test-time distribution
shifts. SPA is a lightweight adaptation framework, allowing hospitals to
rapidly customize phase recognition models by defining phases in natural
language text, annotating a few images with the phase labels, and providing a
task graph defining phase transitions. The experimental results show that the
SPA framework achieves state-of-the-art performance in few-shot surgical phase
recognition across multiple institutions and procedures, even outperforming
full-shot models with 32-shot labeled data. Code is available at
https://github.com/CAMMA-public/SPA

</details>


### [15] [A Transformer Based Handwriting Recognition System Jointly Using Online and Offline Features](https://arxiv.org/abs/2506.20255)
*Ayush Lodh,Ritabrata Chakraborty,Shivakumara Palaiahnakote,Umapada Pal*

Main category: cs.CV

TL;DR: 该论文引入了一种结合离线图像和在线笔迹数据的网络方法，以改进手写识别。


<details>
  <summary>Details</summary>
Motivation: 手写识别可以从光栅处理的复杂字形和笔迹轨迹的互补信息中获益，但现有系统通常只利用一种模态。

Method: 提出了一个端到端的网络，将离线图像与在线笔划数据在共享潜在空间中进行早期融合，通过补丁编码器和轻量级transformer处理后进行联合学习。

Result: 在IAMOn-DB和VNOn-DB数据集上的实验表明，该方法的识别精度达到最新技术水平，提高了1%；还展示了在ISI-Air数据集上的适应性验证。

Conclusion: 此方法能够通过模态的早期整合增强手写识别精度，并实现更强的书写者独立性。

Abstract: We posit that handwriting recognition benefits from complementary cues
carried by the rasterized complex glyph and the pen's trajectory, yet most
systems exploit only one modality. We introduce an end-to-end network that
performs early fusion of offline images and online stroke data within a shared
latent space. A patch encoder converts the grayscale crop into fixed-length
visual tokens, while a lightweight transformer embeds the $(x, y, \text{pen})$
sequence. Learnable latent queries attend jointly to both token streams,
yielding context-enhanced stroke embeddings that are pooled and decoded under a
cross-entropy loss objective. Because integration occurs before any high-level
classification, temporal cues reinforce each other during representation
learning, producing stronger writer independence. Comprehensive experiments on
IAMOn-DB and VNOn-DB demonstrate that our approach achieves state-of-the-art
accuracy, exceeding previous bests by up to 1\%. Our study also shows
adaptation of this pipeline with gesturification on the ISI-Air dataset. Our
code can be found here.

</details>


### [16] [Hierarchical Mask-Enhanced Dual Reconstruction Network for Few-Shot Fine-Grained Image Classification](https://arxiv.org/abs/2506.20263)
*Ning Luo,Meiyin Hu,Huan Wan,Yanyan Yang,Zhuohang Jiang,Xin Wei*

Main category: cs.CV

TL;DR: 本文提出了一种用于少样本细粒度图像分类的HMDRN模型，通过双层特征重建和掩码增强技术实现了卓越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的少样本细粒度分类方法在处理本地特征对齐与显著区提取方面存在局限，作者希望解决这些问题以提升分类精度。

Method: 提出一种融合双层特征重建与掩码增强处理的HMDRN模型，该模型通过学习加权融合低层次结构细节与高层次语义特征，并利用掩码增强的变换器实现自重建，从而更好地区分细粒度特征和减少背景噪声。

Result: 在三个挑战性细粒度数据集上，HMDRN在不同骨干网络下均优于最先进方法。消融实验显示模型的各部分设计有效，且结果可视化验证了其卓越的特征重建能力。

Conclusion: HMDRN模型通过整合双层特征重建与掩码增强机制，有效解决了少样本细粒度分类中的关键问题，实现了显著性能提升。

Abstract: Few-shot fine-grained image classification (FS-FGIC) presents a significant
challenge, requiring models to distinguish visually similar subclasses with
limited labeled examples. Existing methods have critical limitations:
metric-based methods lose spatial information and misalign local features,
while reconstruction-based methods fail to utilize hierarchical feature
information and lack mechanisms to focus on discriminative regions. We propose
the Hierarchical Mask-enhanced Dual Reconstruction Network (HMDRN), which
integrates dual-layer feature reconstruction with mask-enhanced feature
processing to improve fine-grained classification. HMDRN incorporates a
dual-layer feature reconstruction and fusion module that leverages
complementary visual information from different network hierarchies. Through
learnable fusion weights, the model balances high-level semantic
representations from the last layer with mid-level structural details from the
penultimate layer. Additionally, we design a spatial binary mask-enhanced
transformer self-reconstruction module that processes query features through
adaptive thresholding while maintaining complete support features, enhancing
focus on discriminative regions while filtering background noise. Extensive
experiments on three challenging fine-grained datasets demonstrate that HMDRN
consistently outperforms state-of-the-art methods across Conv-4 and ResNet-12
backbone architectures. Comprehensive ablation studies validate the
effectiveness of each proposed component, revealing that dual-layer
reconstruction enhances inter-class discrimination while mask-enhanced
transformation reduces intra-class variations. Visualization results provide
evidence of HMDRN's superior feature reconstruction capabilities.

</details>


### [17] [Forensic Study of Paintings Through the Comparison of Fabrics](https://arxiv.org/abs/2506.20272)
*Juan José Murillo-Fuentes,Pablo M. Olmos,Laura Alba-Carcelén*

Main category: cs.CV

TL;DR: 本文开发了一种基于深度学习的新方法，用于评估艺术品画布间的相似性，而无需依赖传统的纤维密度图匹配方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于纤维密度匹配的方法不适用于画布来源非连续卷轴位置的情况，作者希望开发一种更通用的画布相似性评估方法。

Method: 提出了一种基于Siamese深度学习模型的新方法，通过分析扫描图像中的特征表示来判断画布之间的相似性，并设计了一种汇集多个样本分数的相似性估计方法，以获得稳健的相似性结果。

Result: 结果显示，该方法能够有效比较即使纤维密度接近的画布，验证了该方法的可行性和高准确性。

Conclusion: 这项工作为研究和保护艺术品提供了一种全新的工具，具有广泛的应用潜力，尤其是画布的鉴定和归属分析。

Abstract: The study of canvas fabrics in works of art is a crucial tool for
authentication, attribution and conservation. Traditional methods are based on
thread density map matching, which cannot be applied when canvases do not come
from contiguous positions on a roll. This paper presents a novel approach based
on deep learning to assess the similarity of textiles. We introduce an
automatic tool that evaluates the similarity between canvases without relying
on thread density maps. A Siamese deep learning model is designed and trained
to compare pairs of images by exploiting the feature representations learned
from the scans. In addition, a similarity estimation method is proposed,
aggregating predictions from multiple pairs of cloth samples to provide a
robust similarity score. Our approach is applied to canvases from the Museo
Nacional del Prado, corroborating the hypothesis that plain weave canvases,
widely used in painting, can be effectively compared even when their thread
densities are similar. The results demonstrate the feasibility and accuracy of
the proposed method, opening new avenues for the analysis of masterpieces.

</details>


### [18] [From Ideal to Real: Unified and Data-Efficient Dense Prediction for Real-World Scenarios](https://arxiv.org/abs/2506.20279)
*Changliang Xia,Chengyou Jia,Zhuohang Dang,Minnan Luo*

Main category: cs.CV

TL;DR: 作者研究了密集预测任务在现实世界情景下的表现，并提出了DenseDiT模型，显著提高了多种密集预测任务的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的密集预测方法多基于理想化条件，难以泛化到现实世界，同时缺乏针对真实数据的研究。

Method: 设计了一个名为DenseWorld的基准，涵盖25个紧密联系实际应用的密集预测任务，并提出DenseDiT模型，通过生成模型的视觉先验和轻量化的多尺度上下文整合机制，显著减少参数量和所需的训练数据。

Result: DenseDiT在DenseWorld基准上较现有方法表现优异，仅使用0.01%的训练数据就超越了基准模型，在现实世界任务中展现强大的性能。

Conclusion: DenseDiT模型展现了密集预测任务在现实世界条件下的高效性和可行性，为现实中推广密集预测技术提供了实践价值。

Abstract: Dense prediction tasks hold significant importance of computer vision, aiming
to learn pixel-wise annotated label for an input image. Despite advances in
this field, existing methods primarily focus on idealized conditions, with
limited generalization to real-world scenarios and facing the challenging
scarcity of real-world data. To systematically study this problem, we first
introduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction
tasks that correspond to urgent real-world applications, featuring unified
evaluation across tasks. Then, we propose DenseDiT, which maximally exploits
generative models' visual priors to perform diverse real-world dense prediction
tasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism
and two lightweight branches that adaptively integrate multi-scale context,
working with less than 0.1% additional parameters. Evaluations on DenseWorld
reveal significant performance drops in existing general and specialized
baselines, highlighting their limited real-world generalization. In contrast,
DenseDiT achieves superior results using less than 0.01% training data of
baselines, underscoring its practical value for real-world deployment. Our
data, and checkpoints and codes are available at
https://xcltql666.github.io/DenseDiTProj

</details>


### [19] [Breaking Spatial Boundaries: Spectral-Domain Registration Guided Hyperspectral and Multispectral Blind Fusion](https://arxiv.org/abs/2506.20293)
*Kunjing Yang,Libin Zheng,Minru Bai,Ting Lu,Leyuan Fang*

Main category: cs.CV

TL;DR: 本研究提出了一种从光谱领域解决未注册高光谱图像(HSI)和多光谱图像(MSI)融合问题的方法，并利用稀疏融合模型与效率提升算法取得出色效果。


<details>
  <summary>Details</summary>
Motivation: 现有HSI与MSI融合方法由于图像分辨率差异和配准过程复杂，表现不佳，且处理大尺寸图像耗时。本研究旨在从光谱领域解决这些问题。

Method: 开发轻量级光谱先验学习(SPL)网络以提取HSI光谱特征、提升MSI光谱分辨率，通过空间下采样生成配准的HSI。同时，采用子空间表征和循环训练优化光谱精度，并提出Blind Sparse Fusion(BSF)方法，结合组稀疏正则化和邻近交替优化(PAO)算法解决模型问题，并分析其收敛性。

Result: 通过仿真与真实数据的数值实验验证了所提方法在图像配准和融合上的有效性，并展示了其在分类性能提升中的表现。

Conclusion: 研究表明，从光谱领域进行配准和融合能够有效提升HSI与MSI的综合性能，并验证了一种高效的稀疏融合模型及其收敛算法的可行性与优越性。

Abstract: The blind fusion of unregistered hyperspectral images (HSIs) and
multispectral images (MSIs) has attracted growing attention recently. To
address the registration challenge, most existing methods employ spatial
transformations on the HSI to achieve alignment with the MSI. However, due to
the substantial differences in spatial resolution of the images, the
performance of these methods is often unsatisfactory. Moreover, the
registration process tends to be time-consuming when dealing with large-sized
images in remote sensing. To address these issues, we propose tackling the
registration problem from the spectral domain. Initially, a lightweight
Spectral Prior Learning (SPL) network is developed to extract spectral features
from the HSI and enhance the spectral resolution of the MSI. Following this,
the obtained image undergoes spatial downsampling to produce the registered
HSI. In this process, subspace representation and cyclic training strategy are
employed to improve spectral accuracy of the registered HSI obtained. Next, we
propose a blind sparse fusion (BSF) method, which utilizes group sparsity
regularization to equivalently promote the low-rankness of the image. This
approach not only circumvents the need for rank estimation, but also reduces
computational complexity. Then, we employ the Proximal Alternating Optimization
(PAO) algorithm to solve the BSF model, and present its convergence analysis.
Finally, extensive numerical experiments on simulated and real datasets are
conducted to verify the effectiveness of our method in registration and fusion.
We also demonstrate its efficacy in enhancing classification performance.

</details>


### [20] [Ctrl-Z Sampling: Diffusion Sampling with Controlled Random Zigzag Explorations](https://arxiv.org/abs/2506.20294)
*Shunqi Mao,Wei Guo,Chaoyi Zhang,Weidong Cai*

Main category: cs.CV

TL;DR: 本文提出了一种新型采样策略Ctrl-Z Sampling，用于解决扩散模型在条件生成中容易陷入局部最优的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在条件生成中表现强劲，但由于潜在空间的复杂性和次优初始化，容易产生局部视觉一致但整体不匹配的结果。

Method: 提出了一种称为Ctrl-Z Sampling的采样策略，通过奖励模型检测局部最优点，注入噪声并返回到较早的状态进行重新优化，采用动态的前后交替策略以提高生成质量。

Result: 实验结果表明，Ctrl-Z Sampling显著提高了生成质量，仅需增加约7.6倍的函数评估次数。

Conclusion: Ctrl-Z Sampling是一种与现有扩散框架兼容的方法，可以显著提升扩散模型的对齐和视觉质量。

Abstract: Diffusion models have shown strong performance in conditional generation by
progressively denoising Gaussian noise toward a target data distribution. This
denoising process can be interpreted as a form of hill climbing in a learned
latent space, where the model iteratively refines the sample toward regions of
higher probability. However, diffusion models often converge to local optima
that are locally visually coherent yet globally inconsistent or conditionally
misaligned, due to latent space complexity and suboptimal initialization. Prior
efforts attempted to address this by strengthening guidance signals or
manipulating the initial noise distribution. We introduce Controlled Random
Zigzag Sampling (Ctrl-Z Sampling), a novel sampling strategy designed to detect
and escape such local maxima during conditional generation. The method first
identifies potential local maxima using a reward model. Upon detection, it
injects noise and reverts to a previous, noisier state to escape the current
optimization plateau. The reward model then evaluates candidate trajectories,
accepting only those that offer improvement, while progressively deeper retreat
enables stronger escapes when nearby alternatives fail. This controlled random
zigzag process allows dynamic alternation between forward refinement and
backward exploration, enhancing both alignment and visual quality in the
generated outputs. The proposed Ctrl-Z Sampling is model-agnostic and
compatible with existing diffusion frameworks. Experimental results show that
Ctrl-Z Sampling substantially improves generation quality with only around 7.6X
increase in function evaluations.

</details>


### [21] [TDiR: Transformer based Diffusion for Image Restoration Tasks](https://arxiv.org/abs/2506.20302)
*Abbas Anwar,Mohammad Shullar,Ali Arshad Nasir,Mudassir Masood,Saeed Anwar*

Main category: cs.CV

TL;DR: 本文提出基于Transformer的扩散模型，用于图像修复任务，并在多种质量指标上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境拍摄的图像由于降质问题，难以用于目标检测、映射和分类等任务，亟需一种有效的图像修复方法。

Method: 开发了一种基于Transformer的扩散模型，用于降质图像的修复任务，并与现有深度学习方法进行了对比测试。

Result: 实验表明，该模型在公开数据集上针对水下图像增强、去噪和去雨等任务中表现优于现有方法。

Conclusion: 基于扩散模型和Transformers的解决方案在增强降质图像质量方面具有显著的效果，并有助于提升下游任务的使用价值。

Abstract: Images captured in challenging environments often experience various forms of
degradation, including noise, color cast, blur, and light scattering. These
effects significantly reduce image quality, hindering their applicability in
downstream tasks such as object detection, mapping, and classification. Our
transformer-based diffusion model was developed to address image restoration
tasks, aiming to improve the quality of degraded images. This model was
evaluated against existing deep learning methodologies across multiple quality
metrics for underwater image enhancement, denoising, and deraining on publicly
available datasets. Our findings demonstrate that the diffusion model, combined
with transformers, surpasses current methods in performance. The results of our
model highlight the efficacy of diffusion models and transformers in improving
the quality of degraded images, consequently expanding their utility in
downstream tasks that require high-fidelity visual data.

</details>


### [22] [Radiomic fingerprints for knee MR images assessment](https://arxiv.org/abs/2506.20306)
*Yaxi Chen,Simin Ni,Shaheer U. Saeed,Aleksandra Ivanova,Rikin Hargunani,Jie Huang,Chaozong Liu,Yipeng Hu*

Main category: cs.CV

TL;DR: 提出了一种动态构建的放射组学指纹框架，以提高膝盖MRI诊断的准确性和解释性。


<details>
  <summary>Details</summary>
Motivation: 当前固定化的放射组学特征选择方法缺乏对个体病理变化的表现力，导致其诊断性能相对较低。

Method: 通过深度学习模型为每个患者动态构建特定的放射组学特征池，再结合逻辑回归用于分类，并以此设计患者个性化的放射指纹。

Result: 在多种诊断任务上表现出与最新深度学习模型相当或更高的诊断准确性，包括膝关节异常、前交叉韧带撕裂和半月板撕裂。

Conclusion: 新方法不仅提升了诊断性能，还增强了解释性，有助于临床洞察以及生物标志物发现。

Abstract: Accurate interpretation of knee MRI scans relies on expert clinical judgment,
often with high variability and limited scalability. Existing radiomic
approaches use a fixed set of radiomic features (the signature), selected at
the population level and applied uniformly to all patients. While
interpretable, these signatures are often too constrained to represent
individual pathological variations. As a result, conventional radiomic-based
approaches are found to be limited in performance, compared with recent
end-to-end deep learning (DL) alternatives without using interpretable radiomic
features. We argue that the individual-agnostic nature in current radiomic
selection is not central to its intepretability, but is responsible for the
poor generalization in our application. Here, we propose a novel radiomic
fingerprint framework, in which a radiomic feature set (the fingerprint) is
dynamically constructed for each patient, selected by a DL model. Unlike the
existing radiomic signatures, our fingerprints are derived on a per-patient
basis by predicting the feature relevance in a large radiomic feature pool, and
selecting only those that are predictive of clinical conditions for individual
patients. The radiomic-selecting model is trained simultaneously with a
low-dimensional (considered relatively explainable) logistic regression for
downstream classification. We validate our methods across multiple diagnostic
tasks including general knee abnormalities, anterior cruciate ligament (ACL)
tears, and meniscus tears, demonstrating comparable or superior diagnostic
accuracy relative to state-of-the-art end-to-end DL models. More importantly,
we show that the interpretability inherent in our approach facilitates
meaningful clinical insights and potential biomarker discovery, with detailed
discussion, quantitative and qualitative analysis of real-world clinical cases
to evidence these advantages.

</details>


### [23] [On the Burstiness of Faces in Set](https://arxiv.org/abs/2506.20312)
*Jiong Wang*

Main category: cs.CV

TL;DR: 本文探讨在集合式人脸识别（SFR）中的爆发性现象，提出了相应检测方法及改进策略，从实验中验证了其对性能提升的重要性。


<details>
  <summary>Details</summary>
Motivation: 爆发性现象的存在会在训练和评估阶段干扰模型性能，迫切需要分析并解决这种对人脸识别负面影响的现象。

Method: 提出基于Quickshift++、特征自相似性以及广义最大池化(GMP)的三种检测策略，并在训练阶段调整稀有脸样本比例，在评估阶段引入质量感知GMP提高鲁棒性。

Result: 验证了爆发性现象的广泛存在，通过实验发现抑制爆发性明显提升总体识别效果。

Conclusion: 爆发性现象是SFR中的关键问题，有效检测并抑制后可显著增强人脸识别性能和模型的泛化能力。

Abstract: Burstiness, a phenomenon observed in text and image retrieval, refers to that
particular elements appear more times in a set than a statistically independent
model assumes. We argue that in the context of set-based face recognition
(SFR), burstiness exists widely and degrades the performance in two aspects:
Firstly, the bursty faces, where faces with particular attributes %exist
frequently in a face set, dominate the training instances and dominate the
training face sets and lead to poor generalization ability to unconstrained
scenarios. Secondly, the bursty faces %dominating the evaluation sets interfere
with the similarity comparison in set verification and identification when
evaluation. To detect the bursty faces in a set, we propose three strategies
based on Quickshift++, feature self-similarity, and generalized max-pooling
(GMP). We apply the burst detection results on training and evaluation stages
to enhance the sampling ratios or contributions of the infrequent faces. When
evaluation, we additionally propose the quality-aware GMP that enables
awareness of the face quality and robustness to the low-quality faces for the
original GMP. We give illustrations and extensive experiments on the SFR
benchmarks to demonstrate that burstiness is widespread and suppressing
burstiness considerably improves the recognition performance.

</details>


### [24] [From Codicology to Code: A Comparative Study of Transformer and YOLO-based Detectors for Layout Analysis in Historical Documents](https://arxiv.org/abs/2506.20326)
*Sergio Torres Aguilar*

Main category: cs.CV

TL;DR: 该研究评估了5种先进的目标检测架构在古籍历史文献上的版面分析表现，探讨了模型架构与数据集复杂性间的关系，并强调了方向边界框(OBB)的关键作用。


<details>
  <summary>Details</summary>
Motivation: 历史文献因其复杂的页面组织结构，需要鲁棒且高效的版面分析方法，这是实现自动化处理的关键。

Method: 对两个基于Transformer的模型(Co-DETR, Grounding DINO)和三种YOLO变体(AABB, OBB, YOLO-World)在三个不同复杂度的数据集(e-NDP, CATMuS, HORAE)上进行性能对比。

Result: 在e-NDP数据集上，Co-DETR表现最优(mAP=0.752)，而在更复杂的CATMuS和HORAE数据集上，YOLOv11x-OBB优势显著(mAP分别为0.564和0.568)。

Conclusion: 方向边界框(OBB)对于建模历史文献的非笛卡尔结构至关重要。Transformer和CNN模型各有优势：Transformer适于结构化布局，CNN则更能应对视觉多样性和复杂性。

Abstract: Robust Document Layout Analysis (DLA) is critical for the automated
processing and understanding of historical documents with complex page
organizations. This paper benchmarks five state-of-the-art object detection
architectures on three annotated datasets representing a spectrum of
codicological complexity: The e-NDP, a corpus of Parisian medieval registers
(1326-1504); CATMuS, a diverse multiclass dataset derived from various medieval
and modern sources (ca.12th-17th centuries) and HORAE, a corpus of decorated
books of hours (ca.13th-16th centuries). We evaluate two Transformer-based
models (Co-DETR, Grounding DINO) against three YOLO variants (AABB, OBB, and
YOLO-World). Our findings reveal significant performance variations dependent
on model architecture, data set characteristics, and bounding box
representation. In the e-NDP dataset, Co-DETR achieves state-of-the-art results
(0.752 mAP@.50:.95), closely followed by YOLOv11X-OBB (0.721). Conversely, on
the more complex CATMuS and HORAE datasets, the CNN-based YOLOv11x-OBB
significantly outperforms all other models (0.564 and 0.568, respectively).
This study unequivocally demonstrates that using Oriented Bounding Boxes (OBB)
is not a minor refinement but a fundamental requirement for accurately modeling
the non-Cartesian nature of historical manuscripts. We conclude that a key
trade-off exists between the global context awareness of Transformers, ideal
for structured layouts, and the superior generalization of CNN-OBB models for
visually diverse and complex documents.

</details>


### [25] [Feature Hallucination for Self-supervised Action Recognition](https://arxiv.org/abs/2506.20342)
*Lei Wang,Piotr Koniusz*

Main category: cs.CV

TL;DR: 提出一种深度转换动作识别框架，通过联合预测动作概念和辅助特征提高视频动作识别准确性，使用多模态和不增加计算开销的幻觉流，同时引入新型领域特定描述符。


<details>
  <summary>Details</summary>
Motivation: 现有视频动作理解方法无法充分有效整合多模态特征，需要高层次语义推理。

Method: 设计了深度转换动作识别框架，结合RGB帧预测动作概念与辅助特征，引入对象检测特征和显著性检测特征等新描述符，并在幻觉步骤中加入不确定性建模，采用稳健损失函数。

Result: 该框架在多个基准上表现出色，如Kinetics-400、Kinetics-600和Something-Something V2，捕捉了细粒度动作动态，达到了最新性能。

Conclusion: 通过结合多模态特征、减少计算成本及增强鲁棒性，该框架为视频动作识别提供了一种高效新方法，在多个基准上取得领先效果。

Abstract: Understanding human actions in videos requires more than raw pixel analysis;
it relies on high-level semantic reasoning and effective integration of
multimodal features. We propose a deep translational action recognition
framework that enhances recognition accuracy by jointly predicting action
concepts and auxiliary features from RGB video frames. At test time,
hallucination streams infer missing cues, enriching feature representations
without increasing computational overhead. To focus on action-relevant regions
beyond raw pixels, we introduce two novel domain-specific descriptors. Object
Detection Features (ODF) aggregate outputs from multiple object detectors to
capture contextual cues, while Saliency Detection Features (SDF) highlight
spatial and intensity patterns crucial for action recognition. Our framework
seamlessly integrates these descriptors with auxiliary modalities such as
optical flow, Improved Dense Trajectories, skeleton data, and audio cues. It
remains compatible with state-of-the-art architectures, including I3D,
AssembleNet, Video Transformer Network, FASTER, and recent models like VideoMAE
V2 and InternVideo2. To handle uncertainty in auxiliary features, we
incorporate aleatoric uncertainty modeling in the hallucination step and
introduce a robust loss function to mitigate feature noise. Our multimodal
self-supervised action recognition framework achieves state-of-the-art
performance on multiple benchmarks, including Kinetics-400, Kinetics-600, and
Something-Something V2, demonstrating its effectiveness in capturing
fine-grained action dynamics.

</details>


### [26] [InvZW: Invariant Feature Learning via Noise-Adversarial Training for Robust Image Zero-Watermarking](https://arxiv.org/abs/2506.20370)
*Abdullah All Tanvir,Xin Zhong*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的鲁棒图像零水印方法，重点是通过不失真特征学习来实现水印的嵌入与恢复。


<details>
  <summary>Details</summary>
Motivation: 旨在解决目前图像水印技术中鲁棒性不足的问题，同时希望通过不修改原图像来提供一种零水印方案。

Method: 框架包括两个模块：第一模块采用噪声对抗学习训练特征提取器，使特征具备抗失真性和语义表达性；第二模块中通过学习的多比特零水印方案将抗失真特征映射到可训练的参考码上，实现与目标二进制信息的匹配。

Result: 实验表明，该方法在特征稳定性和水印恢复的鲁棒性上达到了当前最优水平，并通过对比实验验证了其在泛化性上的优越性。

Conclusion: 提出的深度学习框架不仅实现了高效鲁棒的零水印处理，还能在多种失真环境中展现卓越性能，证明了其实用性。

Abstract: This paper introduces a novel deep learning framework for robust image
zero-watermarking based on distortion-invariant feature learning. As a
zero-watermarking scheme, our method leaves the original image unaltered and
learns a reference signature through optimization in the feature space. The
proposed framework consists of two key modules. In the first module, a feature
extractor is trained via noise-adversarial learning to generate representations
that are both invariant to distortions and semantically expressive. This is
achieved by combining adversarial supervision against a distortion
discriminator and a reconstruction constraint to retain image content. In the
second module, we design a learning-based multibit zero-watermarking scheme
where the trained invariant features are projected onto a set of trainable
reference codes optimized to match a target binary message. Extensive
experiments on diverse image datasets and a wide range of distortions show that
our method achieves state-of-the-art robustness in both feature stability and
watermark recovery. Comparative evaluations against existing self-supervised
and deep watermarking techniques further highlight the superiority of our
framework in generalization and robustness.

</details>


### [27] [Exploiting Lightweight Hierarchical ViT and Dynamic Framework for Efficient Visual Tracking](https://arxiv.org/abs/2506.20381)
*Ben Kang,Xin Chen,Jie Zhao,Chunjuan Bo,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: HiT是一个高效的Transformer视觉跟踪模型，结合轻量Transformers和创新的模块设计，同时DyHiT进一步通过动态路由选取场景优化效率。这两个模型展示了极快的处理速度和强大的跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的视觉跟踪技术性能优秀，但在资源受限设备上的运行速度较慢，限制了其实用性。

Method: 提出了基于轻量Transformer的HiT和动态路由DyHiT，并引入桥接模块，提高特征表示质量，以及双图像位置编码方法显著增强空间信息表达。同时设计动态路由策略优化场景计算适应性，还提供了一种训练无关加速方法。

Result: HiT在NVIDIA Jetson AGX平台上实现了61帧每秒的速度和LaSOT基准测试的64.6% AUC，而DyHiT最快版本达到了111帧每秒，AUC为62.4%。加速方法使SeqTrack-B256的运行速度提升2.68倍，同时保持69.9%的AUC。

Conclusion: HiT和DyHiT在效率和性能之间达成了出色的平衡，并为视觉跟踪领域提供了一种高效的加速解决方案，适用于多种高度计算和资源受限设备场景。

Abstract: Transformer-based visual trackers have demonstrated significant advancements
due to their powerful modeling capabilities. However, their practicality is
limited on resource-constrained devices because of their slow processing
speeds. To address this challenge, we present HiT, a novel family of efficient
tracking models that achieve high performance while maintaining fast operation
across various devices. The core innovation of HiT lies in its Bridge Module,
which connects lightweight transformers to the tracking framework, enhancing
feature representation quality. Additionally, we introduce a dual-image
position encoding approach to effectively encode spatial information. HiT
achieves an impressive speed of 61 frames per second (fps) on the NVIDIA Jetson
AGX platform, alongside a competitive AUC of 64.6% on the LaSOT benchmark,
outperforming all previous efficient trackers.Building on HiT, we propose
DyHiT, an efficient dynamic tracker that flexibly adapts to scene complexity by
selecting routes with varying computational requirements. DyHiT uses search
area features extracted by the backbone network and inputs them into an
efficient dynamic router to classify tracking scenarios. Based on the
classification, DyHiT applies a divide-and-conquer strategy, selecting
appropriate routes to achieve a superior trade-off between accuracy and speed.
The fastest version of DyHiT achieves 111 fps on NVIDIA Jetson AGX while
maintaining an AUC of 62.4% on LaSOT.Furthermore, we introduce a training-free
acceleration method based on the dynamic routing architecture of DyHiT. This
method significantly improves the execution speed of various high-performance
trackers without sacrificing accuracy. For instance, our acceleration method
enables the state-of-the-art tracker SeqTrack-B256 to achieve a 2.68 times
speedup on an NVIDIA GeForce RTX 2080 Ti GPU while maintaining the same AUC of
69.9% on the LaSOT.

</details>


### [28] [A Novel Large Vision Foundation Model (LVFM)-based Approach for Generating High-Resolution Canopy Height Maps in Plantations for Precision Forestry Management](https://arxiv.org/abs/2506.20388)
*Shen Tan,Xin Zhang,Liangxiu Han,Huaguo Huang,Han Wang*

Main category: cs.CV

TL;DR: 提出了一种新模型利用高分辨率RGB影像生成冠层高度图（CHM），为植被碳固存需求提供了经济高效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统基于激光雷达的方法尽管精确，但成本高昂，而现有RGB影像深度学习方法在提取冠层高度方面精确度不足，亟需开发经济高效的替代方案以支持CCER计划中的碳固存监测。

Method: 开发了一种新型模型，基于大规模视觉基础模型（LVFM），结合特征提取模块、自监督特征增强模块和高度估计器，通过1米分辨率的Google Earth影像生成高分辨率冠层高度图。

Result: 模型在北京市房山区进行测试，相较传统CNN方法表现更优，绝对误差（MAE）为0.09米，均方根误差（RMSE）为0.24米，相关系数达0.78。生成的冠层高度图可实现个体树木检测成功率超90%，并精确估算地上生物量，能够有效追踪植被生长。

Conclusion: 该方法为种植园及自然森林的碳固存监测提供了一种高效、可扩展的工具，表现出良好的泛化性。

Abstract: Accurate, cost-effective monitoring of plantation aboveground biomass (AGB)
is crucial for supporting local livelihoods and carbon sequestration
initiatives like the China Certified Emission Reduction (CCER) program.
High-resolution canopy height maps (CHMs) are essential for this, but standard
lidar-based methods are expensive. While deep learning with RGB imagery offers
an alternative, accurately extracting canopy height features remains
challenging. To address this, we developed a novel model for high-resolution
CHM generation using a Large Vision Foundation Model (LVFM). Our model
integrates a feature extractor, a self-supervised feature enhancement module to
preserve spatial details, and a height estimator. Tested in Beijing's Fangshan
District using 1-meter Google Earth imagery, our model outperformed existing
methods, including conventional CNNs. It achieved a mean absolute error of 0.09
m, a root mean square error of 0.24 m, and a correlation of 0.78 against
lidar-based CHMs. The resulting CHMs enabled over 90% success in individual
tree detection, high accuracy in AGB estimation, and effective tracking of
plantation growth, demonstrating strong generalization to non-training areas.
This approach presents a promising, scalable tool for evaluating carbon
sequestration in both plantations and natural forests.

</details>


### [29] [Med-Art: Diffusion Transformer for 2D Medical Text-to-Image Generation](https://arxiv.org/abs/2506.20449)
*Changlu Guo,Anders Nymark Christensen,Morten Rieger Hannemose*

Main category: cs.CV

TL;DR: 本研究提出了一种名为Med-Art的框架，用于在有限数据下生成医疗图像，通过使用视觉语言模型克服医疗文本数据稀缺的问题，并结合了创新的混合级别扩散微调方法，达到了最新水平的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在解决医疗图像生成领域中的数据有限性和文本数据稀缺问题，提高生成质量和实际应用价值。

Method: 提出了Med-Art框架，该框架利用视觉语言模型生成人工视觉描述，与基于扩散变压器的PixArt-α模型结合，并创新性地引入了混合级别扩散微调（HLDF）方法来优化生成效果。

Result: 实验结果表明，Med-Art在FID、KID和下游分类性能指标上达到了两个医疗数据集上的最优性能。

Conclusion: Med-Art通过创新的方法克服了医疗图像生成中的数据稀缺和质量优化问题，为该领域提供了新的解决方案，并展现了高性能特点。

Abstract: Text-to-image generative models have achieved remarkable breakthroughs in
recent years. However, their application in medical image generation still
faces significant challenges, including small dataset sizes, and scarcity of
medical textual data. To address these challenges, we propose Med-Art, a
framework specifically designed for medical image generation with limited data.
Med-Art leverages vision-language models to generate visual descriptions of
medical images which overcomes the scarcity of applicable medical textual data.
Med-Art adapts a large-scale pre-trained text-to-image model, PixArt-$\alpha$,
based on the Diffusion Transformer (DiT), achieving high performance under
limited data. Furthermore, we propose an innovative Hybrid-Level Diffusion
Fine-tuning (HLDF) method, which enables pixel-level losses, effectively
addressing issues such as overly saturated colors. We achieve state-of-the-art
performance on two medical image datasets, measured by FID, KID, and downstream
classification performance.

</details>


### [30] [HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based Diffusion Sampling](https://arxiv.org/abs/2506.20452)
*Tobias Vontobel,Seyedmorteza Sadat,Farnood Salehi,Romann M. Weber*

Main category: cs.CV

TL;DR: 本文提出了HiWave方法，通过两阶段流程和全新的小波细节增强模块，实现无训练、零次生成超高分辨率图像，并有效提升视觉质量，克服现有方法的伪影问题。


<details>
  <summary>Details</summary>
Motivation: 解决高分辨率扩散模型计算成本过高及现有零次生成方法存在伪影的问题，包括目标重复和空间不一致。

Method: 引入HiWave方法，分为两阶段：首先使用预训练模型生成基础图像并通过补丁式DDIM反演保存整体一致性；然后通过小波细节增强模块在保留低频结构的同时引导高频分量优化图像细节和纹理。

Result: HiWave在使用Stable Diffusion XL模型的实验中表现出色，大幅减弱伪影问题并提升感知质量；用户研究表明，该方法在80%以上的对比中优于现有最佳方案。

Conclusion: HiWave无需重新训练和模型修改，在超高分辨率图像合成中取得了显著的效果，验证了其有效性。

Abstract: Diffusion models have emerged as the leading approach for image synthesis,
demonstrating exceptional photorealism and diversity. However, training
diffusion models at high resolutions remains computationally prohibitive, and
existing zero-shot generation techniques for synthesizing images beyond
training resolutions often produce artifacts, including object duplication and
spatial incoherence. In this paper, we introduce HiWave, a training-free,
zero-shot approach that substantially enhances visual fidelity and structural
coherence in ultra-high-resolution image synthesis using pretrained diffusion
models. Our method employs a two-stage pipeline: generating a base image from
the pretrained model followed by a patch-wise DDIM inversion step and a novel
wavelet-based detail enhancer module. Specifically, we first utilize inversion
methods to derive initial noise vectors that preserve global coherence from the
base image. Subsequently, during sampling, our wavelet-domain detail enhancer
retains low-frequency components from the base image to ensure structural
consistency, while selectively guiding high-frequency components to enrich fine
details and textures. Extensive evaluations using Stable Diffusion XL
demonstrate that HiWave effectively mitigates common visual artifacts seen in
prior methods, achieving superior perceptual quality. A user study confirmed
HiWave's performance, where it was preferred over the state-of-the-art
alternative in more than 80% of comparisons, highlighting its effectiveness for
high-quality, ultra-high-resolution image synthesis without requiring
retraining or architectural modifications.

</details>


### [31] [A Deep Learning Approach to Identify Rock Bolts in Complex 3D Point Clouds of Underground Mines Captured Using Mobile Laser Scanners](https://arxiv.org/abs/2506.20464)
*Dibyayan Patra,Pasindu Ranasinghe,Bikram Banerjee,Simit Raval*

Main category: cs.CV

TL;DR: 该研究提出了一种名为DeepBolt的新方法，用于在复杂3D点云中自动识别岩石螺栓，其精度和召回率显著高于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有用于自动识别岩石螺栓的方法缺乏鲁棒性，并且手动测量过程在地下矿中受限于低光环境和耗时因素，需求一个高效自动的解决方案。

Method: 引入了一种两阶段深度学习架构DeepBolt，专门为解决严重类别不平衡问题而设计，用于在复杂的3D点云中自动高效地识别岩石螺栓。

Result: DeepBolt在岩石螺栓点的IoU上比最先进的语义分割模型提高了42.5%，同时，其分类精度达96.41%，召回率达96.96%。

Conclusion: DeepBolt的实验结果表明，该方法在复杂的地下环境中具有卓越的鲁棒性和有效性，优于当前的岩石螺栓识别技术。

Abstract: Rock bolts are crucial components of the subterranean support systems in
underground mines that provide adequate structural reinforcement to the rock
mass to prevent unforeseen hazards like rockfalls. This makes frequent
assessments of such bolts critical for maintaining rock mass stability and
minimising risks in underground mining operations. Where manual surveying of
rock bolts is challenging due to the low light conditions in the underground
mines and the time-intensive nature of the process, automated detection of rock
bolts serves as a plausible solution. To that end, this study focuses on the
automatic identification of rock bolts within medium to large-scale 3D point
clouds obtained from underground mines using mobile laser scanners. Existing
techniques for automated rock bolt identification primarily rely on feature
engineering and traditional machine learning approaches. However, such
techniques lack robustness as these point clouds present several challenges due
to data noise, varying environments, and complex surrounding structures.
Moreover, the target rock bolts are extremely small objects within large-scale
point clouds and are often partially obscured due to the application of
reinforcement shotcrete. Addressing these challenges, this paper proposes an
approach termed DeepBolt, which employs a novel two-stage deep learning
architecture specifically designed for handling severe class imbalance for the
automatic and efficient identification of rock bolts in complex 3D point
clouds. The proposed method surpasses state-of-the-art semantic segmentation
models by up to 42.5% in Intersection over Union (IoU) for rock bolt points.
Additionally, it outperforms existing rock bolt identification techniques,
achieving a 96.41% precision and 96.96% recall in classifying rock bolts,
demonstrating its robustness and effectiveness in complex underground
environments.

</details>


### [32] [AI-assisted radiographic analysis in detecting alveolar bone-loss severity and patterns](https://arxiv.org/abs/2506.20522)
*Chathura Wimalasiri,Piumal Rathnayake,Shamod Wijerathne,Sumudu Rasnayaka,Dhanushka Leuke Bandara,Roshan Ragel,Vajira Thambawita,Isuru Nawinne*

Main category: cs.CV

TL;DR: 本文提出了一种基于AI的深度学习框架，用于通过IOPA放射影像自动检测和量化牙槽骨丧失及其模式。


<details>
  <summary>Details</summary>
Motivation: 研究旨在提高牙周炎诊断与治疗的精确性，通过自动化方法提升评估骨丧失严重程度及模式的效率与客观性。

Method: 通过结合YOLOv8（用于牙齿检测）、Keypoint R-CNN（定位解剖标志）和YOLOv8x-seg（骨水平和牙齿分割）模型，对放射影像上的牙槽骨丧失进行检测和模式识别。

Result: 方法在1000张专业标注的影像数据集上验证，骨丧失严重程度检测的ICCs达0.80，模式分类准确率为87%。

Conclusion: 该系统提供了快速、客观且可重复的牙周评估工具，有助于提高早期诊断和个性化治疗规划能力，优化患者护理及临床结果。

Abstract: Periodontitis, a chronic inflammatory disease causing alveolar bone loss,
significantly affects oral health and quality of life. Accurate assessment of
bone loss severity and pattern is critical for diagnosis and treatment
planning. In this study, we propose a novel AI-based deep learning framework to
automatically detect and quantify alveolar bone loss and its patterns using
intraoral periapical (IOPA) radiographs. Our method combines YOLOv8 for tooth
detection with Keypoint R-CNN models to identify anatomical landmarks, enabling
precise calculation of bone loss severity. Additionally, YOLOv8x-seg models
segment bone levels and tooth masks to determine bone loss patterns (horizontal
vs. angular) via geometric analysis. Evaluated on a large, expertly annotated
dataset of 1000 radiographs, our approach achieved high accuracy in detecting
bone loss severity (intra-class correlation coefficient up to 0.80) and bone
loss pattern classification (accuracy 87%). This automated system offers a
rapid, objective, and reproducible tool for periodontal assessment, reducing
reliance on subjective manual evaluation. By integrating AI into dental
radiographic analysis, our framework has the potential to improve early
diagnosis and personalized treatment planning for periodontitis, ultimately
enhancing patient care and clinical outcomes.

</details>


### [33] [Pay Less Attention to Deceptive Artifacts: Robust Detection of Compressed Deepfakes on Online Social Networks](https://arxiv.org/abs/2506.20548)
*Manyi Li,Renshuai Tao,Yufan Liu,Chuangchuang Tan,Haotong Qin,Bing Li,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: PLADA是一个深度伪造检测框架，通过处理压缩引起的"块效应"和利用无配对数据，展示了在在线社交网络上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 深度伪造图片已经变得难以辨别，并大量传播于在线社交网络中，但现有检测方法忽视了因压缩产生的"块效应"，以及现实场景中原始图片的稀缺性。

Method: 提出PLADA框架，由"块效应擦除"模块（B2E）与"开放数据聚合"模块（ODA）组成，通过双阶段注意机制处理压缩效应，并混合使用配对和非配对数据以提高检测能力。

Result: PLADA在26个数据集上进行实验，结果优于现有顶尖方法，尤其在处理在线社交网络压缩图片和有限配对数据情境下表现出色。

Conclusion: 文章突出了"块效应"在深度伪造检测中的重要性，并为开放世界场景提供了鲁棒解决方案。

Abstract: With the rapid advancement of deep learning, particularly through generative
adversarial networks (GANs) and diffusion models (DMs), AI-generated images, or
``deepfakes", have become nearly indistinguishable from real ones. These images
are widely shared across Online Social Networks (OSNs), raising concerns about
their misuse. Existing deepfake detection methods overlook the ``block effects"
introduced by compression in OSNs, which obscure deepfake artifacts, and
primarily focus on raw images, rarely encountered in real-world scenarios. To
address these challenges, we propose PLADA (Pay Less Attention to Deceptive
Artifacts), a novel framework designed to tackle the lack of paired data and
the ineffective use of compressed images. PLADA consists of two core modules:
Block Effect Eraser (B2E), which uses a dual-stage attention mechanism to
handle block effects, and Open Data Aggregation (ODA), which processes both
paired and unpaired data to improve detection. Extensive experiments across 26
datasets demonstrate that PLADA achieves a remarkable balance in deepfake
detection, outperforming SoTA methods in detecting deepfakes on OSNs, even with
limited paired data and compression. More importantly, this work introduces the
``block effect" as a critical factor in deepfake detection, providing a robust
solution for open-world scenarios. Our code is available at
https://github.com/ManyiLee/PLADA.

</details>


### [34] [Lightweight Multi-Frame Integration for Robust YOLO Object Detection in Videos](https://arxiv.org/abs/2506.20550)
*Yitong Quan,Benjamin Kiefer,Martin Messmer,Andreas Zell*

Main category: cs.CV

TL;DR: 文章提出一种利用多帧输入改进视频目标检测的简单方法，并验证其在实际应用中的有效性。


<details>
  <summary>Details</summary>
Motivation: 单帧检测方法忽略了视频中的时序信息，而现有的视频目标检测方法则复杂度过高，难以应用于实际场景。

Method: 结合YOLO检测器，使用多帧叠加输入但仅监督目标帧输出，最大限度地利用时序信息同时保持实时性和模型的简单性。

Result: 实验表明新方法在MOT20Det和BOAT360数据集上显著提高了检测的鲁棒性，尤其是在轻量级模型上的表现更为突出。

Conclusion: 该方法有效缩小了轻量级和重型检测网络的性能差距，提出的数据集为未来的多帧视频检测研究提供了新的基础。

Abstract: Modern image-based object detection models, such as YOLOv7, primarily process
individual frames independently, thus ignoring valuable temporal context
naturally present in videos. Meanwhile, existing video-based detection methods
often introduce complex temporal modules, significantly increasing model size
and computational complexity. In practical applications such as surveillance
and autonomous driving, transient challenges including motion blur, occlusions,
and abrupt appearance changes can severely degrade single-frame detection
performance. To address these issues, we propose a straightforward yet highly
effective strategy: stacking multiple consecutive frames as input to a
YOLO-based detector while supervising only the output corresponding to a single
target frame. This approach leverages temporal information with minimal
modifications to existing architectures, preserving simplicity, computational
efficiency, and real-time inference capability. Extensive experiments on the
challenging MOT20Det and our BOAT360 datasets demonstrate that our method
improves detection robustness, especially for lightweight models, effectively
narrowing the gap between compact and heavy detection networks. Additionally,
we contribute the BOAT360 benchmark dataset, comprising annotated fisheye video
sequences captured from a boat, to support future research in multi-frame video
object detection in challenging real-world scenarios.

</details>


### [35] [AdvMIM: Adversarial Masked Image Modeling for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2506.20563)
*Lei Zhu,Jun Zhou,Rick Siow Mong Goh,Yong Liu*

Main category: cs.CV

TL;DR: 本文提出了一种对抗性掩码图像建模方法，以充分发挥Transformer在医学图像分割中的潜力，特别是在半监督学习场景中。实验表明，该方法在多个数据集上的表现显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在半监督医学图像分割中的训练挑战，特别是标注数据不足时的监督信号问题。

Method: 基于掩码图像建模构建辅助域，通过对抗性训练减少原始域与掩码域之间的域间差距，提升Transformer的性能，并将该方法扩展到CNN网络。

Result: 在三个公开医学图像分割数据集上进行了广泛实验，实验证明新方法显著优于现有方法。

Conclusion: 对抗性掩码图像建模提升了半监督学习中Transformer的性能，为医学图像分割任务带来了显著的改进，为相关领域提供了新的研究方向。

Abstract: Vision Transformer has recently gained tremendous popularity in medical image
segmentation task due to its superior capability in capturing long-range
dependencies. However, transformer requires a large amount of labeled data to
be effective, which hinders its applicability in annotation scarce
semi-supervised learning scenario where only limited labeled data is available.
State-of-the-art semi-supervised learning methods propose combinatorial
CNN-Transformer learning to cross teach a transformer with a convolutional
neural network, which achieves promising results. However, it remains a
challenging task to effectively train the transformer with limited labeled
data. In this paper, we propose an adversarial masked image modeling method to
fully unleash the potential of transformer for semi-supervised medical image
segmentation. The key challenge in semi-supervised learning with transformer
lies in the lack of sufficient supervision signal. To this end, we propose to
construct an auxiliary masked domain from original domain with masked image
modeling and train the transformer to predict the entire segmentation mask with
masked inputs to increase supervision signal. We leverage the original labels
from labeled data and pseudo-labels from unlabeled data to learn the masked
domain. To further benefit the original domain from masked domain, we provide a
theoretical analysis of our method from a multi-domain learning perspective and
devise a novel adversarial training loss to reduce the domain gap between the
original and masked domain, which boosts semi-supervised learning performance.
We also extend adversarial masked image modeling to CNN network. Extensive
experiments on three public medical image segmentation datasets demonstrate the
effectiveness of our method, where our method outperforms existing methods
significantly. Our code is publicly available at
https://github.com/zlheui/AdvMIM.

</details>


### [36] [Show, Tell and Summarize: Dense Video Captioning Using Visual Cue Aided Sentence Summarization](https://arxiv.org/abs/2506.20567)
*Zhiwang Zhang,Dong Xu,Wanli Ouyang,Chuanqi Tan*

Main category: cs.CV

TL;DR: 提出了一种名为DaS的分段与摘要框架，用于为稠密视频生成字幕描述，并在ActivityNet Captions数据集上验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 希望通过提高对于视频事件的语义和视觉特征的总结能力，提升稠密视频字幕生成的效果。

Method: 通过分段生成初步描述，并借助视觉线索和两阶段LSTM网络的分层注意机制，将这些描述总结为单句摘要。

Result: 实验证明该方法在ActivityNet Captions数据集上表现出色，证实了其有效性。

Conclusion: DaS框架能够有效地结合语义和视觉特征生成高质量的稠密视频字幕，为视频内容理解提供了新方向。

Abstract: In this work, we propose a division-and-summarization (DaS) framework for
dense video captioning. After partitioning each untrimmed long video as
multiple event proposals, where each event proposal consists of a set of short
video segments, we extract visual feature (e.g., C3D feature) from each segment
and use the existing image/video captioning approach to generate one sentence
description for this segment. Considering that the generated sentences contain
rich semantic descriptions about the whole event proposal, we formulate the
dense video captioning task as a visual cue aided sentence summarization
problem and propose a new two stage Long Short Term Memory (LSTM) approach
equipped with a new hierarchical attention mechanism to summarize all generated
sentences as one descriptive sentence with the aid of visual features.
Specifically, the first-stage LSTM network takes all semantic words from the
generated sentences and the visual features from all segments within one event
proposal as the input, and acts as the encoder to effectively summarize both
semantic and visual information related to this event proposal. The
second-stage LSTM network takes the output from the first-stage LSTM network
and the visual features from all video segments within one event proposal as
the input, and acts as the decoder to generate one descriptive sentence for
this event proposal. Our comprehensive experiments on the ActivityNet Captions
dataset demonstrate the effectiveness of our newly proposed DaS framework for
dense video captioning.

</details>


### [37] [Causal Representation Learning with Observational Grouping for CXR Classification](https://arxiv.org/abs/2506.20582)
*Rajat Rasal,Avinash Kori,Ben Glocker*

Main category: cs.CV

TL;DR: 该研究提出一种端到端框架，通过分组观察的方式学习疾病分类的可识别表示。


<details>
  <summary>Details</summary>
Motivation: 希望通过学习可识别的因果表示，提升医疗成像中特定任务特征的普适性与鲁棒性。

Method: 通过分组观察方式，利用种族、性别及成像视角的不可变性，学习胸部X光疾病分类的因果表示。

Result: 实验结果表明，使用分组观察强制不可变性时，可以提升分类任务的普适性与鲁棒性。

Conclusion: 分组观察和可识别因果表示学习对提升疾病分类应用的效果和鲁棒性有显著贡献。

Abstract: Identifiable causal representation learning seeks to uncover the true causal
relationships underlying a data generation process. In medical imaging, this
presents opportunities to improve the generalisability and robustness of
task-specific latent features. This work introduces the concept of grouping
observations to learn identifiable representations for disease classification
in chest X-rays via an end-to-end framework. Our experiments demonstrate that
these causal representations improve generalisability and robustness across
multiple classification tasks when grouping is used to enforce invariance w.r.t
race, sex, and imaging views.

</details>


### [38] [Dense Video Captioning using Graph-based Sentence Summarization](https://arxiv.org/abs/2506.20583)
*Zhiwang Zhang,Dong Xu,Wanli Ouyang,Luping Zhou*

Main category: cs.CV

TL;DR: 提出了一种基于图的划分和摘要（GPaS）框架，通过分阶段处理提高密集视频描述的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在对长时间的视频提案进行描述时，未能充分捕捉事件场景的动态变化，因此无法有效处理场景与物体变化较大的提案。

Method: 提出GPaS框架，分为“划分”和“摘要”两个阶段：首先将事件提案分割为短视频片段进行精细描述，然后将这些描述汇总成一句话；核心在于使用GCN和LSTM组合的结构处理语义词节点间的交互关系。

Result: 在ActivityNet Captions和YouCook II数据集上，与最先进方法相比，验证了该方法的有效性。

Conclusion: 通过分段处理和语义交互，提升了长时间视频提案的描述质量，对密集视频描述提供了新的思路。

Abstract: Recently, dense video captioning has made attractive progress in detecting
and captioning all events in a long untrimmed video. Despite promising results
were achieved, most existing methods do not sufficiently explore the scene
evolution within an event temporal proposal for captioning, and therefore
perform less satisfactorily when the scenes and objects change over a
relatively long proposal. To address this problem, we propose a graph-based
partition-and-summarization (GPaS) framework for dense video captioning within
two stages. For the ``partition" stage, a whole event proposal is split into
short video segments for captioning at a finer level. For the ``summarization"
stage, the generated sentences carrying rich description information for each
segment are summarized into one sentence to describe the whole event. We
particularly focus on the ``summarization" stage, and propose a framework that
effectively exploits the relationship between semantic words for summarization.
We achieve this goal by treating semantic words as nodes in a graph and
learning their interactions by coupling Graph Convolutional Network (GCN) and
Long Short Term Memory (LSTM), with the aid of visual cues. Two schemes of
GCN-LSTM Interaction (GLI) modules are proposed for seamless integration of GCN
and LSTM. The effectiveness of our approach is demonstrated via an extensive
comparison with the state-of-the-arts methods on the two benchmarks ActivityNet
Captions dataset and YouCook II dataset.

</details>


### [39] [Learning-Based Distance Estimation for 360° Single-Sensor Setups](https://arxiv.org/abs/2506.20586)
*Yitong Quan,Benjamin Kiefer,Martin Messmer,Andreas Zell*

Main category: cs.CV

TL;DR: 该研究提出了一种基于神经网络的方法，用于使用单个360°鱼眼镜头相机进行单目距离估计，展示了在多个数据集上的优异表现。


<details>
  <summary>Details</summary>
Motivation: 传统几何方法在应对镜头畸变和环境变化时表现不佳，因此需要一种更鲁棒和自适应的方法进行全方向成像距离估计。

Method: 利用基于神经网络的学习方法，直接从原始的全方向图像输入中学习并推断物体距离，而无需精准的镜头校准。

Result: 实验表明，该方法在多个数据集（LOAF、ULM360和新数据集Boat360）上均优于传统几何方法和其他基于学习的基线方法。

Conclusion: 深度学习方法不仅提升了全方向成像距离估计的准确性和鲁棒性，还适合低成本的机器人、自动导航及监控应用场景。

Abstract: Accurate distance estimation is a fundamental challenge in robotic
perception, particularly in omnidirectional imaging, where traditional
geometric methods struggle with lens distortions and environmental variability.
In this work, we propose a neural network-based approach for monocular distance
estimation using a single 360{\deg} fisheye lens camera. Unlike classical
trigonometric techniques that rely on precise lens calibration, our method
directly learns and infers the distance of objects from raw omnidirectional
inputs, offering greater robustness and adaptability across diverse conditions.
We evaluate our approach on three 360{\deg} datasets (LOAF, ULM360, and a newly
captured dataset Boat360), each representing distinct environmental and sensor
setups. Our experimental results demonstrate that the proposed learning-based
model outperforms traditional geometry-based methods and other learning
baselines in both accuracy and robustness. These findings highlight the
potential of deep learning for real-time omnidirectional distance estimation,
making our approach particularly well-suited for low-cost applications in
robotics, autonomous navigation, and surveillance.

</details>


### [40] [TRIM: A Self-Supervised Video Summarization Framework Maximizing Temporal Relative Information and Representativeness](https://arxiv.org/abs/2506.20588)
*Pritam Mishra,Coloma Ballester,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: 提出了一种无监督的视频摘要模型，无需注意力机制、RNN 或 transformer，性能优于现存无监督方法，并接近最佳监督方法。


<details>
  <summary>Details</summary>
Motivation: 随着视频内容的普及，对信息高效提取的需求日益增加，然而现有方法依赖监督标注或注意力模型，计算代价高，跨领域适应性差。

Method: 提出一种以马尔可夫过程驱动损失函数为核心的自监督视频摘要框架，具有独特的两阶段学习机制，同时有效整合空间与时间依赖。

Result: 在SUMME和TVSUM数据集上，该方法表现优于所有现存无监督方法，与最佳监督模型相媲美。

Conclusion: 证明了无需复杂架构即可实现高效、无标注视频摘要的可能性，为更普适的视频摘要技术奠定了基础。

Abstract: The increasing ubiquity of video content and the corresponding demand for
efficient access to meaningful information have elevated video summarization
and video highlights as a vital research area. However, many state-of-the-art
methods depend heavily either on supervised annotations or on attention-based
models, which are computationally expensive and brittle in the face of
distribution shifts that hinder cross-domain applicability across datasets. We
introduce a pioneering self-supervised video summarization model that captures
both spatial and temporal dependencies without the overhead of attention, RNNs,
or transformers. Our framework integrates a novel set of Markov process-driven
loss metrics and a two-stage self supervised learning paradigm that ensures
both performance and efficiency. Our approach achieves state-of-the-art
performance on the SUMME and TVSUM datasets, outperforming all existing
unsupervised methods. It also rivals the best supervised models, demonstrating
the potential for efficient, annotation-free architectures. This paves the way
for more generalizable video summarization techniques and challenges the
prevailing reliance on complex architectures.

</details>


### [41] [WonderFree: Enhancing Novel View Quality and Cross-View Consistency for 3D Scene Exploration](https://arxiv.org/abs/2506.20590)
*Chaojun Ni,Jie Li,Haoyun Li,Hengyu Liu,Xiaofeng Wang,Zheng Zhu,Guosheng Zhao,Boyuan Wang,Chenxin Li,Guan Huang,Wenjun Mei*

Main category: cs.CV

TL;DR: 该论文提出了WonderFree模型，解决了当前3D场景生成方法在未知视角探索质量和一致性上的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决当前单张图像生成3D场景方法中视角可探索性低，无法高质量渲染新视角导致限制的难题。

Method: 将问题分为新视角质量和跨视角一致性两部分，通过引入数据驱动的WorldRestorer视频恢复模型改善新视角渲染质量，并提出ConsistView多视角联合恢复机制保证时空一致性。

Result: 实验表明WonderFree模型显著提升了不同视角的渲染质量和全局一致性，用户偏好测试中表现优于现有方法。

Conclusion: WonderFree实现了更高质量的3D探索体验，将推动3D生成领域更进一步，代码数据将公开以促进研究。

Abstract: Interactive 3D scene generation from a single image has gained significant
attention due to its potential to create immersive virtual worlds. However, a
key challenge in current 3D generation methods is the limited explorability,
which cannot render high-quality images during larger maneuvers beyond the
original viewpoint, particularly when attempting to move forward into unseen
areas. To address this challenge, we propose WonderFree, the first model that
enables users to interactively generate 3D worlds with the freedom to explore
from arbitrary angles and directions. Specifically, we decouple this challenge
into two key subproblems: novel view quality, which addresses visual artifacts
and floating issues in novel views, and cross-view consistency, which ensures
spatial consistency across different viewpoints. To enhance rendering quality
in novel views, we introduce WorldRestorer, a data-driven video restoration
model designed to eliminate floaters and artifacts. In addition, a data
collection pipeline is presented to automatically gather training data for
WorldRestorer, ensuring it can handle scenes with varying styles needed for 3D
scene generation. Furthermore, to improve cross-view consistency, we propose
ConsistView, a multi-view joint restoration mechanism that simultaneously
restores multiple perspectives while maintaining spatiotemporal coherence.
Experimental results demonstrate that WonderFree not only enhances rendering
quality across diverse viewpoints but also significantly improves global
coherence and consistency. These improvements are confirmed by CLIP-based
metrics and a user study showing a 77.20% preference for WonderFree over
WonderWorld enabling a seamless and immersive 3D exploration experience. The
code, model, and data will be publicly available.

</details>


### [42] [SFNet: Fusion of Spatial and Frequency-Domain Features for Remote Sensing Image Forgery Detection](https://arxiv.org/abs/2506.20599)
*Ji Qi,Xinchang Zhang,Dingqi Ye,Yongjia Ruan,Xin Guo,Shaowen Wang,Haifeng Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为SFNet的新型伪造检测框架，用于检测遥感影像中的假图。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单一视觉特征捕捉预定义伪造痕迹，无法在多样化的遥感数据中有效泛化。

Method: 采用空间域和频率域特征提取器，并通过特征映射和融合模块（如CBAM注意力机制）处理多域特征，从而提高伪造检测效果。

Result: 在三个数据集上，SFNet的准确率相比现有方法提升4%-15.18%，表现出良好的泛化能力。

Conclusion: SFNet在检测遥感影像伪造方面优于现有技术，兼具创新性和实用性，代码已开源。

Abstract: The rapid advancement of generative artificial intelligence is producing fake
remote sensing imagery (RSI) that is increasingly difficult to detect,
potentially leading to erroneous intelligence, fake news, and even conspiracy
theories. Existing forgery detection methods typically rely on single visual
features to capture predefined artifacts, such as spatial-domain cues to detect
forged objects like roads or buildings in RSI, or frequency-domain features to
identify artifacts from up-sampling operations in adversarial generative
networks (GANs). However, the nature of artifacts can significantly differ
depending on geographic terrain, land cover types, or specific features within
the RSI. Moreover, these complex artifacts evolve as generative models become
more sophisticated. In short, over-reliance on a single visual cue makes
existing forgery detectors struggle to generalize across diverse remote sensing
data. This paper proposed a novel forgery detection framework called SFNet,
designed to identify fake images in diverse remote sensing data by leveraging
spatial and frequency domain features. Specifically, to obtain rich and
comprehensive visual information, SFNet employs two independent feature
extractors to capture spatial and frequency domain features from input RSIs. To
fully utilize the complementary domain features, the domain feature mapping
module and the hybrid domain feature refinement module(CBAM attention) of SFNet
are designed to successively align and fuse the multi-domain features while
suppressing redundant information. Experiments on three datasets show that
SFNet achieves an accuracy improvement of 4%-15.18% over the state-of-the-art
RS forgery detection methods and exhibits robust generalization capabilities.
The code is available at https://github.com/GeoX-Lab/RSTI/tree/main/SFNet.

</details>


### [43] [Video Perception Models for 3D Scene Synthesis](https://arxiv.org/abs/2506.20601)
*Rui Huang,Guangyao Zhai,Zuria Bauer,Marc Pollefeys,Federico Tombari,Leonidas Guibas,Gao Huang,Francis Engelmann*

Main category: cs.CV

TL;DR: VIPScene通过视频生成模型的3D物理世界常识，实现高逼真度和结构一致的场景生成。


<details>
  <summary>Details</summary>
Motivation: 减轻传统3D场景合成对专家知识和人工的依赖，为建筑设计、机器人模拟、虚拟现实及游戏等领域带来便利。

Method: 提出VIPScene框架，使用视频生成、3D重建以及开放词汇感知模型进行语义和几何分析，并引入FPVScore进行评估。

Result: VIPScene显著优于现有方法，在多种场景中表现出良好的通用性。

Conclusion: VIPScene通过创新性的多模型整合，实现了灵活、高质量的3D场景合成，其代码计划公开。

Abstract: Traditionally, 3D scene synthesis requires expert knowledge and significant
manual effort. Automating this process could greatly benefit fields such as
architectural design, robotics simulation, virtual reality, and gaming. Recent
approaches to 3D scene synthesis often rely on the commonsense reasoning of
large language models (LLMs) or strong visual priors of modern image generation
models. However, current LLMs demonstrate limited 3D spatial reasoning ability,
which restricts their ability to generate realistic and coherent 3D scenes.
Meanwhile, image generation-based methods often suffer from constraints in
viewpoint selection and multi-view inconsistencies. In this work, we present
Video Perception models for 3D Scene synthesis (VIPScene), a novel framework
that exploits the encoded commonsense knowledge of the 3D physical world in
video generation models to ensure coherent scene layouts and consistent object
placements across views. VIPScene accepts both text and image prompts and
seamlessly integrates video generation, feedforward 3D reconstruction, and
open-vocabulary perception models to semantically and geometrically analyze
each object in a scene. This enables flexible scene synthesis with high realism
and structural consistency. For more precise analysis, we further introduce
First-Person View Score (FPVScore) for coherence and plausibility evaluation,
utilizing continuous first-person perspective to capitalize on the reasoning
ability of multimodal large language models. Extensive experiments show that
VIPScene significantly outperforms existing methods and generalizes well across
diverse scenarios. The code will be released.

</details>


### [44] [Shape2Animal: Creative Animal Generation from Natural Silhouettes](https://arxiv.org/abs/2506.20616)
*Quoc-Duy Tran,Anh-Tuan Vo,Dinh-Khoi Vo,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: 本文提出了一个名为Shape2Animal的框架，可以通过重新解释自然物体的轮廓（如云朵、石头或火焰）为合理的动物形式，从而模拟人类将模糊刺激视为有意义模式的能力。


<details>
  <summary>Details</summary>
Motivation: 研究人类认知现象：即如何将模糊物体识别为有意义的模式，并开发一种能模仿这种能力的工具。

Method: 采用开放词汇的分割技术提取物体轮廓，使用视觉语言模型对其进行语义解释，然后利用文本到图像扩散模型生成符合输入形状的动物图像，最后将其融入原始场景中以生成视觉连贯且空间一致的作品。

Result: 在多样的真实世界输入上评估了Shape2Animal，验证了其鲁棒性和创意潜能。

Conclusion: Shape2Animal可以为视觉故事讲述、教育内容、数字艺术和互动媒体设计提供新的可能性。

Abstract: Humans possess a unique ability to perceive meaningful patterns in ambiguous
stimuli, a cognitive phenomenon known as pareidolia. This paper introduces
Shape2Animal framework to mimics this imaginative capacity by reinterpreting
natural object silhouettes, such as clouds, stones, or flames, as plausible
animal forms. Our automated framework first performs open-vocabulary
segmentation to extract object silhouette and interprets semantically
appropriate animal concepts using vision-language models. It then synthesizes
an animal image that conforms to the input shape, leveraging text-to-image
diffusion model and seamlessly blends it into the original scene to generate
visually coherent and spatially consistent compositions. We evaluated
Shape2Animal on a diverse set of real-world inputs, demonstrating its
robustness and creative potential. Our Shape2Animal can offer new opportunities
for visual storytelling, educational content, digital art, and interactive
media design. Our project page is here: https://shape2image.github.io

</details>


### [45] [Joint attitude estimation and 3D neural reconstruction of non-cooperative space objects](https://arxiv.org/abs/2506.20638)
*Clément Forray,Pauline Delporte,Nicolas Delaygue,Florence Genin,Dawa Derksen*

Main category: cs.CV

TL;DR: 本研究利用神经辐射场（NeRF）技术，对非合作性空间物体进行3D重建，并重点探讨了与相机位姿联合优化的效果。


<details>
  <summary>Details</summary>
Motivation: 希望更好地了解地球轨道上物体的状态和行为，以支持空间碎片清除、轨道维护及异常检测等应用。

Method: 应用NeRF进行非合作性空间物体的3D重建，特别针对相机位姿的联合优化，采用逐一训练图像，并通过正则化防止相邻相机位姿偏差过大。

Result: 实验结果表明，通过逐一训练图像和相机位姿优化，能够获得最精确的3D重建。

Conclusion: 研究表明，设计针对特定环境和条件的图像训练与相机位姿优化策略，可以有效提升3D重建精度，为空间态势感知提供支持。

Abstract: Obtaining a better knowledge of the current state and behavior of objects
orbiting Earth has proven to be essential for a range of applications such as
active debris removal, in-orbit maintenance, or anomaly detection. 3D models
represent a valuable source of information in the field of Space Situational
Awareness (SSA). In this work, we leveraged Neural Radiance Fields (NeRF) to
perform 3D reconstruction of non-cooperative space objects from simulated
images. This scenario is challenging for NeRF models due to unusual camera
characteristics and environmental conditions : mono-chromatic images, unknown
object orientation, limited viewing angles, absence of diffuse lighting etc. In
this work we focus primarly on the joint optimization of camera poses alongside
the NeRF. Our experimental results show that the most accurate 3D
reconstruction is achieved when training with successive images one-by-one. We
estimate camera poses by optimizing an uniform rotation and use regularization
to prevent successive poses from being too far apart.

</details>


### [46] [Disentangled representations of microscopy images](https://arxiv.org/abs/2506.20649)
*Jacopo Dapueto,Vito Paolo Pastore,Nicoletta Noceti,Francesca Odone*

Main category: cs.CV

TL;DR: 深度学习在显微镜图像分析中表现出色，但解释性仍是挑战。本文提出通过DRL方法增强图像分类模型的解释性，并在多领域数据集上验证其效果。


<details>
  <summary>Details</summary>
Motivation: 现代获取系统生成的大量显微镜图像需要发展高效的分析方法，同时保证深度学习模型解释性以满足显微镜图像分析的需求。

Method: 提出使用DRL方法，利用从合成数据学习的表征迁移，改善显微镜图像分类中的模型解释性，测试了三种显微图像数据集。

Result: 实验表明，该方法在精确性和解释性之间达到了良好的平衡。

Conclusion: DRL框架可以提升显微镜图像分类模型的解释性，且保留了高准确性，展示了其在多个领域的适用性。

Abstract: Microscopy image analysis is fundamental for different applications, from
diagnosis to synthetic engineering and environmental monitoring. Modern
acquisition systems have granted the possibility to acquire an escalating
amount of images, requiring a consequent development of a large collection of
deep learning-based automatic image analysis methods. Although deep neural
networks have demonstrated great performance in this field, interpretability,
an essential requirement for microscopy image analysis, remains an open
challenge.
  This work proposes a Disentangled Representation Learning (DRL) methodology
to enhance model interpretability for microscopy image classification.
Exploiting benchmark datasets from three different microscopic image domains
(plankton, yeast vacuoles, and human cells), we show how a DRL framework, based
on transferring a representation learnt from synthetic data, can provide a good
trade-off between accuracy and interpretability in this domain.

</details>


### [47] [MMSearch-R1: Incentivizing LMMs to Search](https://arxiv.org/abs/2506.20670)
*Jinming Wu,Zihao Deng,Wei Li,Yiding Liu,Bo You,Bo Li,Zejun Ma,Ziwei Liu*

Main category: cs.CV

TL;DR: 本论文提出MMSearch-R1，一种强化学习框架，用于实现大规模多模态模型(LMMs)在真实互联网环境中的按需多回合搜索。


<details>
  <summary>Details</summary>
Motivation: 现有方法如RAG和搜索代理因其流水线刚性，通常效率低下或者搜索行为过度，为解决这一问题，该研究旨在开发一种能动态处理跨模态知识需求的高效搜索框架。

Method: 论文提出了MMSearch-R1框架，通过强化学习训练模型适时调用文本和图像搜索工具；此外，作者构建了综合多样数据集，包括需要搜索和不需要搜索的样本，用于塑造模型的高效搜索行为。

Result: 在知识密集型和信息搜索型VQA任务上，MMSearch-R1不仅优于同尺寸的RAG基线模型，还匹敌更大尺寸RAG模型，同时减少了超过30%的搜索调用。

Conclusion: MMSearch-R1不仅提高了多模态搜索的效率与效果，还为后续研究提供了可操作性见解。

Abstract: Robust deployment of large multimodal models (LMMs) in real-world scenarios
requires access to external knowledge sources, given the complexity and dynamic
nature of real-world information. Existing approaches such as
retrieval-augmented generation (RAG) and prompt engineered search agents rely
on rigid pipelines, often leading to inefficient or excessive search behaviors.
We present MMSearch-R1, the first end-to-end reinforcement learning framework
that enables LMMs to perform on-demand, multi-turn search in real-world
Internet environments. Our framework integrates both image and text search
tools, allowing the model to reason about when and how to invoke them guided by
an outcome-based reward with a search penalty. To support training, We collect
a multimodal search VQA dataset through a semi-automated pipeline that covers
diverse visual and textual knowledge needs and curate a search-balanced subset
with both search-required and search-free samples, which proves essential for
shaping efficient and on-demand search behavior. Extensive experiments on
knowledge-intensive and info-seeking VQA tasks show that our model not only
outperforms RAG-based baselines of the same model size, but also matches the
performance of a larger RAG-based model while reducing search calls by over
30%. We further analyze key empirical findings to offer actionable insights for
advancing research in multimodal search.

</details>


### [48] [IPFormer: Visual 3D Panoptic Scene Completion with Context-Adaptive Instance Proposals](https://arxiv.org/abs/2506.20671)
*Markus Gross,Aya Fahmy,Danit Niwattananan,Dominik Muhle,Rui Song,Daniel Cremers,Henri Meeß*

Main category: cs.CV

TL;DR: 本文提出IPFormer，用于基于相机的3D全景场景补全，通过上下文自适应实例提议方法，提升场景理解性能并大幅减少运行成本。


<details>
  <summary>Details</summary>
Motivation: 当前场景补全研究主要集中在LiDAR数据上，相机图像的应用较少，同时基于Transformer的固定查询方法在测试时无法动态适配场景。

Method: 提出IPFormer方法，首次利用上下文自适应实例提议，在训练和测试阶段依赖图像上下文生成与优化查询，推理语义实例-体素关系。

Result: 实验显示，IPFormer在全景指标PQ$^\dagger$和PQ-All上超越现有技术，且运行时间减少14倍以上。动态推导实例提议提升PQ-All 3.62%，显著改善事物指标18.65%。

Conclusion: IPFormer通过引入上下文自适应实例提议，为基于视觉的3D全景场景补全领域带来了创新性的解决方案。

Abstract: Semantic Scene Completion (SSC) has emerged as a pivotal approach for jointly
learning scene geometry and semantics, enabling downstream applications such as
navigation in mobile robotics. The recent generalization to Panoptic Scene
Completion (PSC) advances the SSC domain by integrating instance-level
information, thereby enhancing object-level sensitivity in scene understanding.
While PSC was introduced using LiDAR modality, methods based on camera images
remain largely unexplored. Moreover, recent Transformer-based SSC approaches
utilize a fixed set of learned queries to reconstruct objects within the scene
volume. Although these queries are typically updated with image context during
training, they remain static at test time, limiting their ability to
dynamically adapt specifically to the observed scene. To overcome these
limitations, we propose IPFormer, the first approach that leverages
context-adaptive instance proposals at train and test time to address
vision-based 3D Panoptic Scene Completion. Specifically, IPFormer adaptively
initializes these queries as panoptic instance proposals derived from image
context and further refines them through attention-based encoding and decoding
to reason about semantic instance-voxel relationships. Experimental results
show that our approach surpasses state-of-the-art methods in overall panoptic
metrics PQ$^\dagger$ and PQ-All, matches performance in individual metrics, and
achieves a runtime reduction exceeding 14$\times$. Furthermore, our ablation
studies reveal that dynamically deriving instance proposals from image context,
as opposed to random initialization, leads to a 3.62% increase in PQ-All and a
remarkable average improvement of 18.65% in combined Thing-metrics. These
results highlight our introduction of context-adaptive instance proposals as a
pioneering effort in addressing vision-based 3D Panoptic Scene Completion.

</details>


### [49] [Computer Vision based Automated Quantification of Agricultural Sprayers Boom Displacement](https://arxiv.org/abs/2506.19939)
*Aryan Singh Dalal,Sidharth Rai,Rahul Singh,Treman Singh Kaloya,Rahul Harsha Cheppally,Ajay Sharda*

Main category: cs.CV

TL;DR: 开发了一种自动化计算机视觉系统，利用YOLO神经网络模型跟踪喷杆运动，结果显示模型检测精度超90%，距离估算误差在0.026m内，为喷杆设计改进提供数据支持。


<details>
  <summary>Details</summary>
Motivation: 农业喷药过程中，由于喷杆不稳定导致的药剂施用误差问题较大，但缺乏对喷杆运动的定量认识以制定改进措施。

Method: 通过基于YOLO神经网络开发的计算机视觉系统，实时跟踪喷杆边缘目标，采集其位移数据并通过倾角仪验证精度。

Result: 模型检测目标的准确率超过90%，距离估算与倾角仪数据的误差在0.026m以内。

Conclusion: 该系统实现了喷杆运动的定量分析，具备适配其他喷药设备的潜力，为改善喷杆设计和提升喷药精度提供了可靠的数据支持。

Abstract: Application rate errors when using self-propelled agricultural sprayers for
agricultural production remain a concern. Among other factors, spray boom
instability is one of the major contributors to application errors. Spray
booms' width of 38m, combined with 30 kph driving speeds, varying terrain, and
machine dynamics when maneuvering complex field boundaries, make controls of
these booms very complex. However, there is no quantitative knowledge on the
extent of boom movement to systematically develop a solution that might include
boom designs and responsive boom control systems. Therefore, this study was
conducted to develop an automated computer vision system to quantify the boom
movement of various agricultural sprayers. A computer vision system was
developed to track a target on the edge of the sprayer boom in real time. YOLO
V7, V8, and V11 neural network models were trained to track the boom's
movements in field operations to quantify effective displacement in the
vertical and transverse directions. An inclinometer sensor was mounted on the
boom to capture boom angles and validate the neural network model output. The
results showed that the model could detect the target with more than 90 percent
accuracy, and distance estimates of the target on the boom were within 0.026 m
of the inclinometer sensor data. This system can quantify the boom movement on
the current sprayer and potentially on any other sprayer with minor
modifications. The data can be used to make design improvements to make sprayer
booms more stable and achieve greater application accuracy.

</details>


### [50] [EBC-ZIP: Improving Blockwise Crowd Counting with Zero-Inflated Poisson Regression](https://arxiv.org/abs/2506.19955)
*Yiming Ma,Victor Sanchez,Tanaya Guha*

Main category: cs.CV

TL;DR: 提出了EBC-ZIP，一种基于零膨胀泊松回归的群体计数框架，在多个基准测试中性能优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了真实密度图的极端稀疏性，造成对密集区域的过度估计和稀疏区域的性能下降。

Method: 提出采用零膨胀泊松分布的负对数似然来替代传统回归损失，同时结合增强块分类（EBC）框架进行目标离散性的保持与模型稳定性提升。

Result: 在四个群体计数基准上进行了广泛实验，验证了EBC-ZIP的指标领先，并具备较好的扩展性。

Conclusion: EBC-ZIP框架通过更加合理的概率损失处理零稠密分布，显著提升了群体计数的精度与稳定性。

Abstract: Density map estimation has become the mainstream paradigm in crowd counting.
However, most existing methods overlook the extreme sparsity of ground-truth
density maps. In real-world crowd scenes, the vast majority of spatial regions
(often over 95%) contain no people, leading to heavily imbalanced count
distributions. Ignoring this imbalance can bias models toward overestimating
dense regions and underperforming in sparse areas. Furthermore, most loss
functions used in density estimation are majorly based on MSE and implicitly
assume Gaussian distributions, which are ill-suited for modeling discrete,
non-negative count data. In this paper, we propose EBC-ZIP, a crowd counting
framework that models the spatial distribution of counts using a Zero-Inflated
Poisson (ZIP) regression formulation. Our approach replaces the traditional
regression loss with the negative log-likelihood of the ZIP distribution,
enabling better handling of zero-heavy distributions while preserving count
accuracy. Built upon the recently proposed Enhanced Block Classification (EBC)
framework, EBC-ZIP inherits EBC's advantages in preserving the discreteness of
targets and ensuring training stability, while further improving performance
through a more principled probabilistic loss. We also evaluate EBC-ZIP with
backbones of varying computational complexity to assess its scalability.
Extensive experiments on four crowd counting benchmarks demonstrate that
EBC-ZIP consistently outperforms EBC and achieves state-of-the-art results.

</details>


### [51] [ToSA: Token Merging with Spatial Awareness](https://arxiv.org/abs/2506.20066)
*Hsiang-Wei Huang,Wenhao Chai,Kuang-Ming Chen,Cheng-Yen Yang,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: 提出了一种结合语义和空间信息的视觉Transformer(ViT)加速方法ToSA，通过伪空间信息增强令牌合并策略，显著提升性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有ViT加速方法基于视觉特征相似性进行令牌合并，但忽略了在ViT初期层弱视觉信息下，空间信息可作为更可靠的合并标准。

Method: 提出ToSA方法，利用深度图生成伪空间令牌作为辅助信息与语义信息结合，引导更高效的令牌合并策略。

Result: 实验结果表明，ToSA在视觉和问题回答基准测试中表现优于现有方法，并显著减少运行时间。

Conclusion: ToSA通过引入空间意识有效保留场景结构，对ViT加速提供了一个高效的解决方案，相关代码将公开。

Abstract: Token merging has emerged as an effective strategy to accelerate Vision
Transformers (ViT) by reducing computational costs. However, existing methods
primarily rely on the visual token's feature similarity for token merging,
overlooking the potential of integrating spatial information, which can serve
as a reliable criterion for token merging in the early layers of ViT, where the
visual tokens only possess weak visual information. In this paper, we propose
ToSA, a novel token merging method that combines both semantic and spatial
awareness to guide the token merging process. ToSA leverages the depth image as
input to generate pseudo spatial tokens, which serve as auxiliary spatial
information for the visual token merging process. With the introduced spatial
awareness, ToSA achieves a more informed merging strategy that better preserves
critical scene structure. Experimental results demonstrate that ToSA
outperforms previous token merging methods across multiple benchmarks on visual
and embodied question answering while largely reducing the runtime of the ViT,
making it an efficient solution for ViT acceleration. The code will be
available at: https://github.com/hsiangwei0903/ToSA

</details>


### [52] [BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos](https://arxiv.org/abs/2506.20103)
*Jiahao Lin,Weixuan Peng,Bojia Zi,Yifeng Gao,Xianbiao Qi,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 引入全新数据集BrokenVideos，专注AI生成视频中的视觉伪影定位，并提供像素级注释，显著改善伪影检测性能。


<details>
  <summary>Details</summary>
Motivation: AI生成视频的真实性受限，视觉伪影（如运动不一致、物理不合理、物体变形等）减低了真实性与用户信任，需要提供准确的伪影定位进行质量控制及生成模型改进。然而，目前缺乏针对此领域的全面性基准数据集。

Method: 提出了BrokenVideos数据集，包含3,254段AI生成视频，通过精准人工验证提供像素级的伪影区域注释，用于改进伪影检测性能。

Result: 实验表明，使用此数据集训练的最先进伪影检测模型和多模态大型语言模型显著提升了对伪影区域的定位能力。

Conclusion: BrokenVideos数据集填补了AI生成视频伪影定位研究中的重要空白，为基准检测和研究提供了基础。

Abstract: Recent advances in deep generative models have led to significant progress in
video generation, yet the fidelity of AI-generated videos remains limited.
Synthesized content often exhibits visual artifacts such as temporally
inconsistent motion, physically implausible trajectories, unnatural object
deformations, and local blurring that undermine realism and user trust.
Accurate detection and spatial localization of these artifacts are crucial for
both automated quality control and for guiding the development of improved
generative models. However, the research community currently lacks a
comprehensive benchmark specifically designed for artifact localization in AI
generated videos. Existing datasets either restrict themselves to video or
frame level detection or lack the fine-grained spatial annotations necessary
for evaluating localization methods. To address this gap, we introduce
BrokenVideos, a benchmark dataset of 3,254 AI-generated videos with
meticulously annotated, pixel-level masks highlighting regions of visual
corruption. Each annotation is validated through detailed human inspection to
ensure high quality ground truth. Our experiments show that training state of
the art artifact detection models and multi modal large language models (MLLMs)
on BrokenVideos significantly improves their ability to localize corrupted
regions. Through extensive evaluation, we demonstrate that BrokenVideos
establishes a critical foundation for benchmarking and advancing research on
artifact localization in generative video models. The dataset is available at:
https://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/.

</details>


### [53] [From 2D to 3D Cognition: A Brief Survey of General World Models](https://arxiv.org/abs/2506.20134)
*Ningwei Xie,Zizi Tian,Lei Yang,Xiao-Ping Zhang,Meng Guo,Jie Li*

Main category: cs.CV

TL;DR: 本文旨在系统性分析3D世界模型技术，从2D感知向3D认知的过渡，并讨论3D世界建模的核心能力及其应用和挑战。


<details>
  <summary>Details</summary>
Motivation: 研究从2D感知向3D认知过渡，推动更广泛的3D认知世界模型的发展。

Method: 提出概念框架，分析3D世界模型的技术进展，尤其关注其认知能力、关键驱动因素和应用实例。

Result: 总结了基于3D表示和世界知识的技术驱动，解剖了3D世界建模的三大核心认知能力，并探讨了其应用场景和面临的挑战。

Conclusion: 尽管进展显著，3D世界模型领域仍面临诸多挑战，需要更系统性的方法来实现通用性和鲁棒性增强。

Abstract: World models have garnered increasing attention in the development of
artificial general intelligence (AGI), serving as computational frameworks for
learning representations of the external world and forecasting future states.
While early efforts focused on 2D visual perception and simulation, recent
3D-aware generative world models have demonstrated the ability to synthesize
geometrically consistent, interactive 3D environments, marking a shift toward
3D spatial cognition. Despite rapid progress, the field lacks systematic
analysis to categorize emerging techniques and clarify their roles in advancing
3D cognitive world models. This survey addresses this need by introducing a
conceptual framework, providing a structured and forward-looking review of
world models transitioning from 2D perception to 3D cognition. Within this
framework, we highlight two key technological drivers, particularly advances in
3D representations and the incorporation of world knowledge, as fundamental
pillars. Building on these, we dissect three core cognitive capabilities that
underpin 3D world modeling: 3D physical scene generation, 3D spatial reasoning,
and 3D spatial interaction. We further examine the deployment of these
capabilities in real-world applications, including embodied AI, autonomous
driving, digital twin, and gaming/VR. Finally, we identify challenges across
data, modeling, and deployment, and outline future directions for advancing
more robust and generalizable 3D world models.

</details>


### [54] [EAR: Erasing Concepts from Unified Autoregressive Models](https://arxiv.org/abs/2506.20151)
*Haipeng Fan,Shiyuan Zhang,Baohunesitu,Zihang Guo,Huaiwen Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为EAR的新方法，通过精调实现对自回归（AR）模型中特定概念的有效删除，同时保留生成质量。


<details>
  <summary>Details</summary>
Motivation: 虽然自回归模型在图像理解和生成任务中表现强大，但移除特定概念时往往会损害生成质量，需要一种高效的概念消除方法。

Method: EAR方法提出了窗口梯度累积（WGA）和阈值损失掩码（TLM）策略，用于对齐解码过程与消除目标，同时保护与目标概念无关的内容。此外，还提出了新的基准ECGVF，用于系统性评估AR模型的概念消除性能。

Result: 实验表明，EAR在脑海基准ECGVF上的表现优于基线模型，实现了更好的消除效果和模型效用保留。

Conclusion: EAR成功在生成质量和概念消除的有效性之间实现平衡，为进一步研究AR模型的特定概念消除提供了新方向。

Abstract: Autoregressive (AR) models have achieved unified and strong performance
across both visual understanding and image generation tasks. However, removing
undesired concepts from AR models while maintaining overall generation quality
remains an open challenge. In this paper, we propose Erasure Autoregressive
Model (EAR), a fine-tuning method for effective and utility-preserving concept
erasure in AR models. Specifically, we introduce Windowed Gradient Accumulation
(WGA) strategy to align patch-level decoding with erasure objectives, and
Thresholded Loss Masking (TLM) strategy to protect content unrelated to the
target concept during fine-tuning. Furthermore, we propose a novel benchmark,
Erase Concept Generator and Visual Filter (ECGVF), aim at provide a more
rigorous and comprehensive foundation for evaluating concept erasure in AR
models. Specifically, we first employ structured templates across diverse large
language models (LLMs) to pre-generate a large-scale corpus of
target-replacement concept prompt pairs. Subsequently, we generate images from
these prompts and subject them to rigorous filtering via a visual classifier to
ensure concept fidelity and alignment. Extensive experimental results conducted
on the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR
achieves marked improvements in both erasure effectiveness and model utility
preservation. Code is available at: https://github.com/immc-lab/ear/

</details>


### [55] [Loss-Aware Automatic Selection of Structured Pruning Criteria for Deep Neural Network Acceleration](https://arxiv.org/abs/2506.20152)
*Deepak Ghimire,Kilho Lee,Seong-heum Kim*

Main category: cs.CV

TL;DR: 提出了LAASP方法，用于神经网络的高效结构化剪枝，实现训练与剪枝的一体化处理，同时提高模型压缩效率和保持精度。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了解决现有剪枝方法需要多个阶段、人工干预高和剪枝后精度可能下降的问题。

Method: 采用了一种基于损失感知的自适应剪枝方法（LAASP），通过训练数据的小样本指导，自动选择剪枝标准和层的剪枝，避免手动设定剪枝比例，并整合为剪枝与训练同步进行的过程。

Result: 在CIFAR-10和ImageNet数据集上，VGGNet和ResNet模型展现了显著性能；例如，CIFAR-10上的ResNet56减少52%的FLOPs同时提高Top-1精度，ImageNet上的ResNet50减少42%以上FLOPs而Top-5精度仅轻微下降0.33%。

Conclusion: LAASP在保持高精度的同时显著减少模型参数量，特别适合资源受限设备上的部署，且提供了开源代码便于复现。

Abstract: Structured pruning is a well-established technique for compressing neural
networks, making it suitable for deployment in resource-limited edge devices.
This paper presents an efficient Loss-Aware Automatic Selection of Structured
Pruning Criteria (LAASP) for slimming and accelerating deep neural networks.
The majority of pruning methodologies employ a sequential process consisting of
three stages: 1) training, 2) pruning, and 3) fine-tuning, whereas the proposed
pruning technique adopts a pruning-while-training approach that eliminates the
first stage and integrates the second and third stages into a single cycle. The
automatic selection of magnitude or similarity-based filter pruning criteria
from a specified pool of criteria and the specific pruning layer at each
pruning iteration is guided by the network's overall loss on a small subset of
the training data. To mitigate the abrupt accuracy drop due to pruning, the
network is retrained briefly after each reduction of a predefined number of
floating-point operations (FLOPs). The optimal pruning rates for each layer in
the network are automatically determined, eliminating the need for manual
allocation of fixed or variable pruning rates for each layer. Experiments on
the VGGNet and ResNet models on the CIFAR-10 and ImageNet benchmark datasets
demonstrate the effectiveness of the proposed method. In particular, the
ResNet56 and ResNet110 models on the CIFAR-10 dataset significantly improve the
top-1 accuracy compared to state-of-the-art methods while reducing the network
FLOPs by 52\%. Furthermore, the ResNet50 model on the ImageNet dataset reduces
FLOPs by more than 42\% with a negligible 0.33\% drop in top-5 accuracy. The
source code of this paper is publicly available online -
https://github.com/ghimiredhikura/laasp.

</details>


### [56] [Towards Efficient Exemplar Based Image Editing with Multimodal VLMs](https://arxiv.org/abs/2506.20155)
*Avadhoot Jadhav,Ashutosh Srivastava,Abhinav Java,Silky Singh,Tarun Ram Menta,Surgan Jandial,Balaji Krishnamurthy*

Main category: cs.CV

TL;DR: 本文探讨通过示例对图像编辑，从而提升基于文本的图像生成编辑的局限性，并使用预训练的文本到图像扩散模型优化任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本输入的图像编辑方式虽多样但在特定编辑表达上存在模糊性，因此提出用示例对图像编辑的方法。

Method: 利用预训练的文本到图像扩散模型及多模态视觉语言模型（VLMs），设计了一个端到端的、无需优化的管道来进行编辑传递。

Result: 实验表明，所提出的方法在多种编辑任务中优于基线方法，并且速度快约4倍。

Conclusion: 提出的基于示例对的图像编辑方法能有效克服文本描述编辑的模糊性，同时在编辑质量及效率上表现突出。

Abstract: Text-to-Image Diffusion models have enabled a wide array of image editing
applications. However, capturing all types of edits through text alone can be
challenging and cumbersome. The ambiguous nature of certain image edits is
better expressed through an exemplar pair, i.e., a pair of images depicting an
image before and after an edit respectively. In this work, we tackle
exemplar-based image editing -- the task of transferring an edit from an
exemplar pair to a content image(s), by leveraging pretrained text-to-image
diffusion models and multimodal VLMs. Even though our end-to-end pipeline is
optimization-free, our experiments demonstrate that it still outperforms
baselines on multiple types of edits while being ~4x faster.

</details>


### [57] [Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2506.20168)
*Zhentao He,Can Zhang,Ziheng Wu,Zhenghao Chen,Yufei Zhan,Yifan Li,Zhao Zhang,Xian Wang,Minghui Qiu*

Main category: cs.CV

TL;DR: 本文研究了多模态大型语言模型在文档理解中的不足之处，特别是在视觉退化情况下模型容易产生幻觉性内容的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态语言模型在处理退化视觉信息时，常因依赖文本先验或视觉-文本推理误差而生成幻觉性内容，缺乏应对真实世界挑战的能力。

Method: 提出KIE-HVQA基准，专门评估文档理解中OCR幻觉问题，并开发了基于GRPO的新框架，通过引入视觉不确定性的自我感知和拒绝回答机制来减少幻觉。

Result: 在Qwen2.5-VL上的实验表明，所提出的模型在KIE-HVQA基准上的无幻觉准确度比GPT-4o提高了22%，且在标准任务上性能无明显下降。

Conclusion: 新框架在提升视觉退化场景下的模型鲁棒性和有效性方面表现卓越，同时有效降低了幻觉的生成，为未来真实场景中的多模态语言模型研究提供了有力支持。

Abstract: Recent advancements in multimodal large language models have enhanced
document understanding by integrating textual and visual information. However,
existing models exhibit incompleteness within their paradigm in real-world
scenarios, particularly under visual degradation. In such conditions, the
current response paradigm often fails to adequately perceive visual degradation
and ambiguity, leading to overreliance on linguistic priors or misaligned
visual-textual reasoning. This difficulty in recognizing uncertainty frequently
results in the generation of hallucinatory content, especially when a precise
answer is not feasible. To better demonstrate and analyze this phenomenon and
problem, we propose KIE-HVQA, the first benchmark dedicated to evaluating OCR
hallucination in degraded document understanding. This dataset includes test
samples spanning identity cards and invoices, with simulated real-world
degradations for OCR reliability. This setup allows for evaluating models'
capacity, under degraded input, to distinguish reliable visual information and
answer accordingly, thereby highlighting the challenge of avoiding
hallucination on uncertain data. To achieve vision-faithful reasoning and
thereby avoid the aforementioned issues, we further introduce a GRPO-based
framework featuring a novel reward mechanism. By incorporating a self-awareness
of visual uncertainty and an analysis method that initiates refusal to answer
to increase task difficulty within our supervised fine-tuning and reinforcement
learning framework, we successfully mitigated hallucinations in ambiguous
regions. Experiments on Qwen2.5-VL demonstrate that our 7B-parameter model
achieves a 22\% absolute improvement in hallucination-free accuracy over GPT-4o
on KIE-HVQA and there is no significant performance drop in standard tasks,
highlighting both effectiveness and robustness.

</details>


### [58] [Towards Scalable and Generalizable Earth Observation Data Mining via Foundation Model Composition](https://arxiv.org/abs/2506.20174)
*Man Duc Chuc*

Main category: cs.CV

TL;DR: 本文探讨了如何通过结合预训练模型来提高地球观测任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要专注于从头训练大模型，而忽略了结合现有预训练模型的潜力。

Method: 利用多个预训练模型在GEO-Bench基准上进行实验，通过特征级集成和知识蒸馏提升性能。

Result: 特征级集成的小模型可以达到甚至超越更大模型的性能，同时需要更少的训练时间和计算资源。

Conclusion: 结合现有预训练模型是一种高效而可行的方法，具有实际应用潜力。

Abstract: Foundation models are rapidly transforming Earth Observation data mining by
enabling generalizable and scalable solutions for key tasks such as scene
classification and semantic segmentation. While most efforts in the geospatial
domain have focused on developing large models trained from scratch using
massive Earth Observation datasets, an alternative strategy that remains
underexplored is the reuse and combination of existing pretrained models. In
this study, we investigate whether foundation models pretrained on remote
sensing and general vision datasets can be effectively combined to improve
performance across a diverse set of key Earth Observation tasks. Using the
GEO-Bench benchmark, we evaluate several prominent models, including Prithvi,
Hiera, and DOFA, on eleven datasets covering a range of spatial resolutions,
sensor modalities, and task types. The results show that feature-level
ensembling of smaller pretrained models can match or exceed the performance of
much larger models, while requiring less training time and computational
resources. Moreover, the study highlights the potential of applying knowledge
distillation to transfer the strengths of ensembles into more compact models,
offering a practical path for deploying foundation models in real-world Earth
Observation applications.

</details>


### [59] [Progressive Alignment Degradation Learning for Pansharpening](https://arxiv.org/abs/2506.20179)
*Enzhe Zhao,Zhichang Guo,Yao Li,Fanghui Song,Boying Wu*

Main category: cs.CV

TL;DR: 本文探讨了深度学习在高分辨率多光谱影像融合中的应用，发现现有技术的不足，并提出了新的模块（PADM和HFreqdiff）提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的融合方法面临降解方式不准确的问题，从而无法充分提升模型泛化能力。作者希望解决这一问题。

Method: 提出了PADM模块（通过PAlignNet和PDegradeNet实现逐步对齐降解）以及HFreqdiff模块（通过高频嵌入及CFB和BACM技术进行细节提取和精确逆向学习）。

Result: 实验和消融研究表明，该方法在空间清晰度和影像质量上优于现有技术。

Conclusion: 本文通过提出新方法有效改善高分辨率影像融合质量，证明了其在实际应用中的潜力。

Abstract: Deep learning-based pansharpening has been shown to effectively generate
high-resolution multispectral (HRMS) images. To create supervised ground-truth
HRMS images, synthetic data generated using the Wald protocol is commonly
employed. This protocol assumes that networks trained on artificial
low-resolution data will perform equally well on high-resolution data. However,
well-trained models typically exhibit a trade-off in performance between
reduced-resolution and full-resolution datasets. In this paper, we delve into
the Wald protocol and find that its inaccurate approximation of real-world
degradation patterns limits the generalization of deep pansharpening models. To
address this issue, we propose the Progressive Alignment Degradation Module
(PADM), which uses mutual iteration between two sub-networks, PAlignNet and
PDegradeNet, to adaptively learn accurate degradation processes without relying
on predefined operators. Building on this, we introduce HFreqdiff, which embeds
high-frequency details into a diffusion framework and incorporates CFB and BACM
modules for frequency-selective detail extraction and precise reverse process
learning. These innovations enable effective integration of high-resolution
panchromatic and multispectral images, significantly enhancing spatial
sharpness and quality. Experiments and ablation studies demonstrate the
proposed method's superior performance compared to state-of-the-art techniques.

</details>


### [60] [UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2506.20214)
*Yanzhe Chen,Huasong Zhong,Yan Li,Zhenheng Yang*

Main category: cs.CV

TL;DR: UniCode^2 提出了一种级联码本框架，通过大规模且语义对齐的视觉标记化，加强多模态理解和生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于码本的方法在处理小词汇表时缺乏细粒度语义，而直接扩展词汇表则面临低利用率和训练不稳定的问题，需要新的方法克服这些限制。

Method: 通过对 SigLIP 序列嵌入聚类，构建拥有 500K 条目的码本，同时采用级联设计中的冻结码本和可训练码本以确保稳定性和高利用率。还将视觉标记与文本语义对齐以提升通用性。

Result: UniCode^2 在多个基准上表现优异，不仅扩展了视觉标记空间，还保持了稳定性、语义对齐和模块化设计。

Conclusion: 证明了可以在不牺牲稳定性和语义的前提下扩展视觉标记，同时促进多模态任务的进一步发展。

Abstract: Unified multimodal large language models (MLLMs) have shown promise in
jointly advancing multimodal understanding and generation, with visual
codebooks discretizing images into tokens for autoregressive modeling. Existing
codebook-based methods either rely on small vocabularies (~16K entries) that
lack fine-grained semantics or naively scale up, resulting in low token
utilization and unstable training. We propose UniCode$^2$, a cascaded codebook
framework enabling large-scale, semantically aligned, and stable visual
tokenization. By clustering millions of SigLIP sequence embeddings, we build a
500K-entry codebook that preserves vision-language alignment while expanding
capacity. Stability is ensured via a cascaded design: a frozen codebook anchors
the embedding space, and a trainable codebook refines task-specific semantics.
This decoupling promotes high utilization and robust learning. Moreover, the
alignment of our visual tokens with textual semantics enables seamless
integration with pretrained diffusion decoders, supporting high-quality visual
synthesis with minimal adaptation. UniCode^2 delivers strong performance across
diverse benchmarks, demonstrating the viability of scaling visual token spaces
without sacrificing stability, semantics, or modularity.

</details>


### [61] [Dynamic Bandwidth Allocation for Hybrid Event-RGB Transmission](https://arxiv.org/abs/2506.20222)
*Pujing Yang,Guangyi Zhang,Yunlong Cai,Lei Yu,Guanding Yu*

Main category: cs.CV

TL;DR: 该论文提出了一种针对事件相机和RGB摄像头联合系统中文传输优化问题的新方法，使用信息瓶颈框架实现高效传输和实时去模糊。


<details>
  <summary>Details</summary>
Motivation: 现有事件相机和RGB摄像头的混合系统在信息传输方面存在大量冗余，影响了带宽利用率，且无法与去模糊任务无缝结合。

Method: 通过贝叶斯建模和信息瓶颈方法，将输入信息分离为共享信息和特定领域信息，并基于场景动态自适应分配传输带宽，从而优化信息传输，提升重建与去模糊性能。

Result: 仿真结果表明，该方法相比传统系统在重建质量上表现更优良，同时去模糊性能得到显著提升。

Conclusion: 通过整合事件相机与RGB图像的冗余信息，并优化带宽分配，该方法为高效的信息传输和实时去模糊解决方案提供了可行性，有望应用于未来高效视觉系统。

Abstract: Event cameras asynchronously capture pixel-level intensity changes with
extremely low latency. They are increasingly used in conjunction with RGB
cameras for a wide range of vision-related applications. However, a major
challenge in these hybrid systems lies in the transmission of the large volume
of triggered events and RGB images. To address this, we propose a transmission
scheme that retains efficient reconstruction performance of both sources while
accomplishing real-time deblurring in parallel. Conventional RGB cameras and
event cameras typically capture the same scene in different ways, often
resulting in significant redundant information across their outputs. To address
this, we develop a joint event and image (E-I) transmission framework to
eliminate redundancy and thereby optimize channel bandwidth utilization. Our
approach employs Bayesian modeling and the information bottleneck method to
disentangle the shared and domain-specific information within the E-I inputs.
This disentangled information bottleneck framework ensures both the compactness
and informativeness of extracted shared and domain-specific information.
Moreover, it adaptively allocates transmission bandwidth based on scene
dynamics, i.e., more symbols are allocated to events for dynamic details or to
images for static information. Simulation results demonstrate that the proposed
scheme not only achieves superior reconstruction quality compared to
conventional systems but also delivers enhanced deblurring performance.

</details>


### [62] [Recognizing Surgical Phases Anywhere: Few-Shot Test-time Adaptation and Task-graph Guided Refinement](https://arxiv.org/abs/2506.20254)
*Kun Yuan,Tingxuan Chen,Shi Li,Joel L. Lavanchy,Christian Heiliger,Ege Özsoy,Yiming Huang,Long Bai,Nassir Navab,Vinkle Srivastav,Hongliang Ren,Nicolas Padoy*

Main category: cs.CV

TL;DR: 提出了一种轻量级框架SPA来增强手术数据模型的跨机构和跨过程适应性，通过少量注释实现多模态嵌入对齐、时间一致性保证以及动态测试时自适应，实验表明SPA性能优于全数据模型。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在零样本环境下表现有限，需要一种能更适应手术场景多样性的方法来改进跨机构与跨过程的理解。

Method: 通过少量注释进行空间对齐、多模态相互校正的动态测试自适应及基于扩散模型的时间一致性编码实现模型的轻量级调整，适配不同机构的手术工作流程。

Result: SPA在多个机构和手术流程中以少样本方式实现了先进的手术阶段识别性能，超过了需要全数据的模型。

Conclusion: SPA实现了手术阶段识别的轻量级快速定制化，可通过少量自然语言和图像注释及任务图定义实现高效模型适配。

Abstract: The complexity and diversity of surgical workflows, driven by heterogeneous
operating room settings, institutional protocols, and anatomical variability,
present a significant challenge in developing generalizable models for
cross-institutional and cross-procedural surgical understanding. While recent
surgical foundation models pretrained on large-scale vision-language data offer
promising transferability, their zero-shot performance remains constrained by
domain shifts, limiting their utility in unseen surgical environments. To
address this, we introduce Surgical Phase Anywhere (SPA), a lightweight
framework for versatile surgical workflow understanding that adapts foundation
models to institutional settings with minimal annotation. SPA leverages
few-shot spatial adaptation to align multi-modal embeddings with
institution-specific surgical scenes and phases. It also ensures temporal
consistency through diffusion modeling, which encodes task-graph priors derived
from institutional procedure protocols. Finally, SPA employs dynamic test-time
adaptation, exploiting the mutual agreement between multi-modal phase
prediction streams to adapt the model to a given test video in a
self-supervised manner, enhancing the reliability under test-time distribution
shifts. SPA is a lightweight adaptation framework, allowing hospitals to
rapidly customize phase recognition models by defining phases in natural
language text, annotating a few images with the phase labels, and providing a
task graph defining phase transitions. The experimental results show that the
SPA framework achieves state-of-the-art performance in few-shot surgical phase
recognition across multiple institutions and procedures, even outperforming
full-shot models with 32-shot labeled data. Code is available at
https://github.com/CAMMA-public/SPA

</details>


### [63] [A Transformer Based Handwriting Recognition System Jointly Using Online and Offline Features](https://arxiv.org/abs/2506.20255)
*Ayush Lodh,Ritabrata Chakraborty,Shivakumara Palaiahnakote,Umapada Pal*

Main category: cs.CV

TL;DR: 本研究提出了一种结合离线图像与在线笔迹数据的新型手写识别网络，实验表明该方法在多个数据集上取得了最先进的准确率。


<details>
  <summary>Details</summary>
Motivation: 在手写识别中，离线的光栅化字体和在线笔迹轨迹通常仅被单独利用，未能充分结合两者的互补优势。

Method: 提出了一种端到端网络，通过视觉token编码灰度图像，引入轻量级Transformer嵌入笔画轨迹，在共享的潜在空间中进行早期融合。

Result: 在IAMOn-DB和VNOn-DB数据集上实现了最先进的准确率，超越前任最好结果1%。通过在ISI-Air数据集上的实验验证了该模型的适应能力。

Conclusion: 多模态结合的早期融合方法增强了独立性并提高了手写识别精度，展示了结合不同数据的有效性。

Abstract: We posit that handwriting recognition benefits from complementary cues
carried by the rasterized complex glyph and the pen's trajectory, yet most
systems exploit only one modality. We introduce an end-to-end network that
performs early fusion of offline images and online stroke data within a shared
latent space. A patch encoder converts the grayscale crop into fixed-length
visual tokens, while a lightweight transformer embeds the $(x, y, \text{pen})$
sequence. Learnable latent queries attend jointly to both token streams,
yielding context-enhanced stroke embeddings that are pooled and decoded under a
cross-entropy loss objective. Because integration occurs before any high-level
classification, temporal cues reinforce each other during representation
learning, producing stronger writer independence. Comprehensive experiments on
IAMOn-DB and VNOn-DB demonstrate that our approach achieves state-of-the-art
accuracy, exceeding previous bests by up to 1\%. Our study also shows
adaptation of this pipeline with gesturification on the ISI-Air dataset. Our
code can be found here.

</details>


### [64] [Hierarchical Mask-Enhanced Dual Reconstruction Network for Few-Shot Fine-Grained Image Classification](https://arxiv.org/abs/2506.20263)
*Ning Luo,Meiyin Hu,Huan Wan,Yanyan Yang,Zhuohang Jiang,Xin Wei*

Main category: cs.CV

TL;DR: 本论文提出了一种名为HMDRN的新方法，结合双层特征重建与掩模增强处理，用于解决少样本细粒度图像分类问题，并在多个数据集上展现了优越性能。


<details>
  <summary>Details</summary>
Motivation: 少样本细粒度图像分类面临严重挑战，现有方法或丢失空间信息，或未能专注于判别区域。作者希望解决这些痛点。

Method: 提出HMDRN模型，包括双层特征重建融合模块（结合高层语义与中层结构信息）以及掩模增强的Transformer自重建模块（通过自适应阈值处理查询特征，突出判别区域）。

Result: 在三个细粒度数据集和两种主流骨干网络架构上，HMDRN性能超越当前最先进方法。

Conclusion: HMDRN通过双层重建增强类间差异，掩模增强减少类内差异，其模型架构被实验和可视化结果验证为有效。

Abstract: Few-shot fine-grained image classification (FS-FGIC) presents a significant
challenge, requiring models to distinguish visually similar subclasses with
limited labeled examples. Existing methods have critical limitations:
metric-based methods lose spatial information and misalign local features,
while reconstruction-based methods fail to utilize hierarchical feature
information and lack mechanisms to focus on discriminative regions. We propose
the Hierarchical Mask-enhanced Dual Reconstruction Network (HMDRN), which
integrates dual-layer feature reconstruction with mask-enhanced feature
processing to improve fine-grained classification. HMDRN incorporates a
dual-layer feature reconstruction and fusion module that leverages
complementary visual information from different network hierarchies. Through
learnable fusion weights, the model balances high-level semantic
representations from the last layer with mid-level structural details from the
penultimate layer. Additionally, we design a spatial binary mask-enhanced
transformer self-reconstruction module that processes query features through
adaptive thresholding while maintaining complete support features, enhancing
focus on discriminative regions while filtering background noise. Extensive
experiments on three challenging fine-grained datasets demonstrate that HMDRN
consistently outperforms state-of-the-art methods across Conv-4 and ResNet-12
backbone architectures. Comprehensive ablation studies validate the
effectiveness of each proposed component, revealing that dual-layer
reconstruction enhances inter-class discrimination while mask-enhanced
transformation reduces intra-class variations. Visualization results provide
evidence of HMDRN's superior feature reconstruction capabilities.

</details>


### [65] [Forensic Study of Paintings Through the Comparison of Fabrics](https://arxiv.org/abs/2506.20272)
*Juan José Murillo-Fuentes,Pablo M. Olmos,Laura Alba-Carcelén*

Main category: cs.CV

TL;DR: 該研究提出一種基於深度學習的新方法，用於分析藝術作品中畫布的相似度，無需依賴傳統的線密度圖匹配。


<details>
  <summary>Details</summary>
Motivation: 藝術作品的畫布研究對於鑑定、歸屬和保護非常重要，但傳統方法在畫布不連續時受到限制。

Method: 設計並訓練了一個Siamese深度學習模型，通過學習的特徵表示來比較畫布圖像對，並提出了一種通過多對布料樣本的預測來聚合的相似性評估方法。

Result: 在普拉多博物館的畫布樣本上應用該方法，驗證了即便在線密度相似情況下，普通平紋畫布之間仍能有效比較。

Conclusion: 該方法展示了其可行性與準確性，為傑作分析開辟了新的方向。

Abstract: The study of canvas fabrics in works of art is a crucial tool for
authentication, attribution and conservation. Traditional methods are based on
thread density map matching, which cannot be applied when canvases do not come
from contiguous positions on a roll. This paper presents a novel approach based
on deep learning to assess the similarity of textiles. We introduce an
automatic tool that evaluates the similarity between canvases without relying
on thread density maps. A Siamese deep learning model is designed and trained
to compare pairs of images by exploiting the feature representations learned
from the scans. In addition, a similarity estimation method is proposed,
aggregating predictions from multiple pairs of cloth samples to provide a
robust similarity score. Our approach is applied to canvases from the Museo
Nacional del Prado, corroborating the hypothesis that plain weave canvases,
widely used in painting, can be effectively compared even when their thread
densities are similar. The results demonstrate the feasibility and accuracy of
the proposed method, opening new avenues for the analysis of masterpieces.

</details>


### [66] [From Ideal to Real: Unified and Data-Efficient Dense Prediction for Real-World Scenarios](https://arxiv.org/abs/2506.20279)
*Changliang Xia,Chengyou Jia,Zhuohang Dang,Minnan Luo*

Main category: cs.CV

TL;DR: 提出DenseWorld基准来研究密集预测任务在真实世界的泛化问题，并引入DenseDiT模型，通过生成模型的视觉先验以统一策略解决25个密集预测任务，实现了低数据需求和高性能。


<details>
  <summary>Details</summary>
Motivation: 探索在真实场景下密集预测任务的泛化能力和数据稀缺问题，提出一个跨越多个任务的标准化评估方法。

Method: 构建DenseWorld基准，并提出DenseDiT模型，结合参数重用机制和轻量级分支以适应多尺度上下文，仅需增加0.1%的参数量即可完成多任务预测。

Result: DenseDiT在DenseWorld基准上显著优于现有模型，同时只需使用不到0.01%的训练数据，展现了其在真实世界中的实用价值。

Conclusion: DenseDiT在解决真实世界密集预测任务中实现了高效和准确的性能，并通过DenseWorld评估方法为模型对比提供新标准。

Abstract: Dense prediction tasks hold significant importance of computer vision, aiming
to learn pixel-wise annotated label for an input image. Despite advances in
this field, existing methods primarily focus on idealized conditions, with
limited generalization to real-world scenarios and facing the challenging
scarcity of real-world data. To systematically study this problem, we first
introduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction
tasks that correspond to urgent real-world applications, featuring unified
evaluation across tasks. Then, we propose DenseDiT, which maximally exploits
generative models' visual priors to perform diverse real-world dense prediction
tasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism
and two lightweight branches that adaptively integrate multi-scale context,
working with less than 0.1% additional parameters. Evaluations on DenseWorld
reveal significant performance drops in existing general and specialized
baselines, highlighting their limited real-world generalization. In contrast,
DenseDiT achieves superior results using less than 0.01% training data of
baselines, underscoring its practical value for real-world deployment. Our
data, and checkpoints and codes are available at
https://xcltql666.github.io/DenseDiTProj

</details>


### [67] [Breaking Spatial Boundaries: Spectral-Domain Registration Guided Hyperspectral and Multispectral Blind Fusion](https://arxiv.org/abs/2506.20293)
*Kunjing Yang,Libin Zheng,Minru Bai,Ting Lu,Leyuan Fang*

Main category: cs.CV

TL;DR: 提出了一种无需配准的融合方法，通过光谱域处理和轻量级光谱先验学习网络实现高效的高光谱图像（HSIs）和多光谱图像（MSIs）融合。


<details>
  <summary>Details</summary>
Motivation: 现有方法中通过空间变换实现HSIs和MSIs的配准效果不佳，尤其在处理大尺寸遥感图像时耗时过长。为了解决这些问题，需要一种更高效、更准确的融合方法。

Method: 提出一种基于光谱域的配准方法，利用光谱先验学习网络提升光谱分辨率并下采样以生成配准的HSI；结合子空间表示和循环训练策略提升光谱精度。融合过程中设计了一种盲稀疏融合方法（BSF），并通过PAO算法优化，降低计算复杂度并提升融合效果。

Result: 设计的配准与融合方法在模拟与真实数据集上均表现出色，提高了配准效率、融合效果，并在提升分类性能方面显示出显著作用。

Conclusion: 所提方法有效解决了大尺寸高光谱和多光谱图像的配准与融合问题，具有高效性与准确性，并能显著增强图像分类性能。

Abstract: The blind fusion of unregistered hyperspectral images (HSIs) and
multispectral images (MSIs) has attracted growing attention recently. To
address the registration challenge, most existing methods employ spatial
transformations on the HSI to achieve alignment with the MSI. However, due to
the substantial differences in spatial resolution of the images, the
performance of these methods is often unsatisfactory. Moreover, the
registration process tends to be time-consuming when dealing with large-sized
images in remote sensing. To address these issues, we propose tackling the
registration problem from the spectral domain. Initially, a lightweight
Spectral Prior Learning (SPL) network is developed to extract spectral features
from the HSI and enhance the spectral resolution of the MSI. Following this,
the obtained image undergoes spatial downsampling to produce the registered
HSI. In this process, subspace representation and cyclic training strategy are
employed to improve spectral accuracy of the registered HSI obtained. Next, we
propose a blind sparse fusion (BSF) method, which utilizes group sparsity
regularization to equivalently promote the low-rankness of the image. This
approach not only circumvents the need for rank estimation, but also reduces
computational complexity. Then, we employ the Proximal Alternating Optimization
(PAO) algorithm to solve the BSF model, and present its convergence analysis.
Finally, extensive numerical experiments on simulated and real datasets are
conducted to verify the effectiveness of our method in registration and fusion.
We also demonstrate its efficacy in enhancing classification performance.

</details>


### [68] [Ctrl-Z Sampling: Diffusion Sampling with Controlled Random Zigzag Explorations](https://arxiv.org/abs/2506.20294)
*Shunqi Mao,Wei Guo,Chaoyi Zhang,Weidong Cai*

Main category: cs.CV

TL;DR: 本文提出了Ctrl-Z Sampling，一种控制随机锯齿采样策略，可通过动态切换前向优化与后退探索来改善扩散模型的生成质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在条件生成中因局部最优点而导致视觉不一致或对条件不对齐问题。

Method: 设计了一种控制随机锯齿采样策略，利用奖励模型检测局部最优并通过注入噪声和返回到较早的状态来逃离当前优化平台。

Result: 实验表明，Ctrl-Z Sampling显著提升了生成质量，仅增加约7.6倍的函数评估次数。

Conclusion: 该方法模型无关，与现有扩散框架兼容，能动态平衡精细生成与探索退路，提高生成效果。

Abstract: Diffusion models have shown strong performance in conditional generation by
progressively denoising Gaussian noise toward a target data distribution. This
denoising process can be interpreted as a form of hill climbing in a learned
latent space, where the model iteratively refines the sample toward regions of
higher probability. However, diffusion models often converge to local optima
that are locally visually coherent yet globally inconsistent or conditionally
misaligned, due to latent space complexity and suboptimal initialization. Prior
efforts attempted to address this by strengthening guidance signals or
manipulating the initial noise distribution. We introduce Controlled Random
Zigzag Sampling (Ctrl-Z Sampling), a novel sampling strategy designed to detect
and escape such local maxima during conditional generation. The method first
identifies potential local maxima using a reward model. Upon detection, it
injects noise and reverts to a previous, noisier state to escape the current
optimization plateau. The reward model then evaluates candidate trajectories,
accepting only those that offer improvement, while progressively deeper retreat
enables stronger escapes when nearby alternatives fail. This controlled random
zigzag process allows dynamic alternation between forward refinement and
backward exploration, enhancing both alignment and visual quality in the
generated outputs. The proposed Ctrl-Z Sampling is model-agnostic and
compatible with existing diffusion frameworks. Experimental results show that
Ctrl-Z Sampling substantially improves generation quality with only around 7.6X
increase in function evaluations.

</details>


### [69] [TDiR: Transformer based Diffusion for Image Restoration Tasks](https://arxiv.org/abs/2506.20302)
*Abbas Anwar,Mohammad Shullar,Ali Arshad Nasir,Mudassir Masood,Saeed Anwar*

Main category: cs.CV

TL;DR: 本论文提出了一种基于Transformer的扩散模型，用于图像还原，显著提高了降质图像的质量。


<details>
  <summary>Details</summary>
Motivation: 针对在恶劣环境中捕获的图像因噪声、色偏、模糊等降质影响，限制了其在目标检测、分类等任务中的应用，研究提出提升降质图像质量的方法。

Method: 开发了一种结合Transformer和扩散模型的图像还原方法，并利用公开数据集对其在水下图像增强、去噪、去雨等任务上的表现与现有深度学习方法进行了对比。

Result: 实验结果表明，结合Transformer的扩散模型在多个质量指标上均优于现有方法。

Conclusion: 本文方法验证了扩散模型和Transformer在提升降质图像质量中的有效性，为支持依赖高质量视觉数据的任务拓宽了应用可能。

Abstract: Images captured in challenging environments often experience various forms of
degradation, including noise, color cast, blur, and light scattering. These
effects significantly reduce image quality, hindering their applicability in
downstream tasks such as object detection, mapping, and classification. Our
transformer-based diffusion model was developed to address image restoration
tasks, aiming to improve the quality of degraded images. This model was
evaluated against existing deep learning methodologies across multiple quality
metrics for underwater image enhancement, denoising, and deraining on publicly
available datasets. Our findings demonstrate that the diffusion model, combined
with transformers, surpasses current methods in performance. The results of our
model highlight the efficacy of diffusion models and transformers in improving
the quality of degraded images, consequently expanding their utility in
downstream tasks that require high-fidelity visual data.

</details>


### [70] [Radiomic fingerprints for knee MR images assessment](https://arxiv.org/abs/2506.20306)
*Yaxi Chen,Simin Ni,Shaheer U. Saeed,Aleksandra Ivanova,Rikin Hargunani,Jie Huang,Chaozong Liu,Yipeng Hu*

Main category: cs.CV

TL;DR: 本研究提出了一种动态构建个性化放射组学指纹的框架，在膝关节磁共振成像诊断上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的固定放射组学特征方法在个体病理变异表示上有限，难以推广，同时缺乏灵活性与解释性。

Method: 设计了一种动态的放射组学指纹框架，通过深度学习模型为每个患者选择相关特征，并结合逻辑回归模型进行分类。

Result: 在膝关节多种诊断任务中表现出与最先进深度学习模型相当或更好的诊断准确性，同时保持较高解释性。

Conclusion: 动态个性化放射组学特征提取框架不仅能提升诊断性能，还具有更强的临床解释性和生物标志物发现潜力。

Abstract: Accurate interpretation of knee MRI scans relies on expert clinical judgment,
often with high variability and limited scalability. Existing radiomic
approaches use a fixed set of radiomic features (the signature), selected at
the population level and applied uniformly to all patients. While
interpretable, these signatures are often too constrained to represent
individual pathological variations. As a result, conventional radiomic-based
approaches are found to be limited in performance, compared with recent
end-to-end deep learning (DL) alternatives without using interpretable radiomic
features. We argue that the individual-agnostic nature in current radiomic
selection is not central to its intepretability, but is responsible for the
poor generalization in our application. Here, we propose a novel radiomic
fingerprint framework, in which a radiomic feature set (the fingerprint) is
dynamically constructed for each patient, selected by a DL model. Unlike the
existing radiomic signatures, our fingerprints are derived on a per-patient
basis by predicting the feature relevance in a large radiomic feature pool, and
selecting only those that are predictive of clinical conditions for individual
patients. The radiomic-selecting model is trained simultaneously with a
low-dimensional (considered relatively explainable) logistic regression for
downstream classification. We validate our methods across multiple diagnostic
tasks including general knee abnormalities, anterior cruciate ligament (ACL)
tears, and meniscus tears, demonstrating comparable or superior diagnostic
accuracy relative to state-of-the-art end-to-end DL models. More importantly,
we show that the interpretability inherent in our approach facilitates
meaningful clinical insights and potential biomarker discovery, with detailed
discussion, quantitative and qualitative analysis of real-world clinical cases
to evidence these advantages.

</details>


### [71] [On the Burstiness of Faces in Set](https://arxiv.org/abs/2506.20312)
*Jiong Wang*

Main category: cs.CV

TL;DR: 本研究探讨了人脸识别中的“突发性”现象，提出检测与缓解此现象的方法以提升识别性能。


<details>
  <summary>Details</summary>
Motivation: 人脸识别中，具有某些特性的“突发性人脸”容易导致训练和评估阶段表现不佳。需要找到方法减少突发性对系统性能的负面影响。

Method: 提出了基于Quickshift++、特征自相似性和广义最大池化（GMP）的三种策略用于检测突发人脸，并提出质量感知的GMP算法增强性能。

Result: 实验结果显示，突发性在人脸数据中广泛存在，抑制突发性显著提升了系统的人脸识别性能。

Conclusion: 抑制训练和评估阶段的突发性人脸现象，对提升人脸识别的泛化能力和鲁棒性具有关键作用。

Abstract: Burstiness, a phenomenon observed in text and image retrieval, refers to that
particular elements appear more times in a set than a statistically independent
model assumes. We argue that in the context of set-based face recognition
(SFR), burstiness exists widely and degrades the performance in two aspects:
Firstly, the bursty faces, where faces with particular attributes %exist
frequently in a face set, dominate the training instances and dominate the
training face sets and lead to poor generalization ability to unconstrained
scenarios. Secondly, the bursty faces %dominating the evaluation sets interfere
with the similarity comparison in set verification and identification when
evaluation. To detect the bursty faces in a set, we propose three strategies
based on Quickshift++, feature self-similarity, and generalized max-pooling
(GMP). We apply the burst detection results on training and evaluation stages
to enhance the sampling ratios or contributions of the infrequent faces. When
evaluation, we additionally propose the quality-aware GMP that enables
awareness of the face quality and robustness to the low-quality faces for the
original GMP. We give illustrations and extensive experiments on the SFR
benchmarks to demonstrate that burstiness is widespread and suppressing
burstiness considerably improves the recognition performance.

</details>


### [72] [From Codicology to Code: A Comparative Study of Transformer and YOLO-based Detectors for Layout Analysis in Historical Documents](https://arxiv.org/abs/2506.20326)
*Sergio Torres Aguilar*

Main category: cs.CV

TL;DR: 本文对五种顶尖的目标检测架构在三个复杂的历史文献数据集上的性能进行基准测试，发现模型架构、数据集特性及边界框表示方式对性能影响显著，并强调定向边界框对历史文献分析的重要性。


<details>
  <summary>Details</summary>
Motivation: 自动处理和理解复杂历史文献页面的布局分析需要提高鲁棒性，与此同时，考虑到文献复杂性和多样性，现有技术需要进一步评估与优化。

Method: 基于三个标注数据集（e-NDP、CATMuS 和 HORAE），测试和比较两种 Transformer 模型（Co-DETR 和 Grounding DINO）与三种 YOLO 变体（AABB、OBB 和 YOLO-World）的性能。

Result: 在结构化的 e-NDP 数据集上，Transformer 模型 Co-DETR 达到最高性能（0.752 mAP@.50:.95），但在更复杂和视觉多样化的 CATMuS 和 HORAE 数据集上，CNN 基于 OBB 的 YOLOv11x-OBB 表现最佳（分别为 0.564 和 0.568）。

Conclusion: OBB 是准确建模历史手稿非笛卡尔特性的重要方法，Transformer 更适合结构化布局，而 CNN-OBB 模型则在复杂和视觉多样文档中泛化更好，并不存在普适最佳的模型方法。

Abstract: Robust Document Layout Analysis (DLA) is critical for the automated
processing and understanding of historical documents with complex page
organizations. This paper benchmarks five state-of-the-art object detection
architectures on three annotated datasets representing a spectrum of
codicological complexity: The e-NDP, a corpus of Parisian medieval registers
(1326-1504); CATMuS, a diverse multiclass dataset derived from various medieval
and modern sources (ca.12th-17th centuries) and HORAE, a corpus of decorated
books of hours (ca.13th-16th centuries). We evaluate two Transformer-based
models (Co-DETR, Grounding DINO) against three YOLO variants (AABB, OBB, and
YOLO-World). Our findings reveal significant performance variations dependent
on model architecture, data set characteristics, and bounding box
representation. In the e-NDP dataset, Co-DETR achieves state-of-the-art results
(0.752 mAP@.50:.95), closely followed by YOLOv11X-OBB (0.721). Conversely, on
the more complex CATMuS and HORAE datasets, the CNN-based YOLOv11x-OBB
significantly outperforms all other models (0.564 and 0.568, respectively).
This study unequivocally demonstrates that using Oriented Bounding Boxes (OBB)
is not a minor refinement but a fundamental requirement for accurately modeling
the non-Cartesian nature of historical manuscripts. We conclude that a key
trade-off exists between the global context awareness of Transformers, ideal
for structured layouts, and the superior generalization of CNN-OBB models for
visually diverse and complex documents.

</details>


### [73] [Feature Hallucination for Self-supervised Action Recognition](https://arxiv.org/abs/2506.20342)
*Lei Wang,Piotr Koniusz*

Main category: cs.CV

TL;DR: 该论文提出了一种用于视频中动作识别的深度翻译框架，通过结合 RGB 视频帧中的动作概念和辅助特征来提高识别精度。


<details>
  <summary>Details</summary>
Motivation: 传统的动作识别方法依赖像素级的信息，缺乏高层语义推理和多模态特征的有效整合，因此需要一种新方法来克服这一局限性。

Method: 框架联合预测动作概念及辅助特征，引入对象检测特征(ODF)和显著性检测特征(SDF)两种新颖的描述符，并通过幻觉流补足测试时缺失的线索。此外，作者引入模态不确定性建模和稳健损失函数缓解特征噪声问题。

Result: 该框架在多个基准数据集（如 Kinetics-400、Kinetics-600 和 Something-Something V2）上实现了最先进的性能，并兼容当前最先进的模型架构。

Conclusion: 所提出的多模态自监督动作识别框架能够有效捕捉精细动作动态，在性能和适配性方面均展现了优势。

Abstract: Understanding human actions in videos requires more than raw pixel analysis;
it relies on high-level semantic reasoning and effective integration of
multimodal features. We propose a deep translational action recognition
framework that enhances recognition accuracy by jointly predicting action
concepts and auxiliary features from RGB video frames. At test time,
hallucination streams infer missing cues, enriching feature representations
without increasing computational overhead. To focus on action-relevant regions
beyond raw pixels, we introduce two novel domain-specific descriptors. Object
Detection Features (ODF) aggregate outputs from multiple object detectors to
capture contextual cues, while Saliency Detection Features (SDF) highlight
spatial and intensity patterns crucial for action recognition. Our framework
seamlessly integrates these descriptors with auxiliary modalities such as
optical flow, Improved Dense Trajectories, skeleton data, and audio cues. It
remains compatible with state-of-the-art architectures, including I3D,
AssembleNet, Video Transformer Network, FASTER, and recent models like VideoMAE
V2 and InternVideo2. To handle uncertainty in auxiliary features, we
incorporate aleatoric uncertainty modeling in the hallucination step and
introduce a robust loss function to mitigate feature noise. Our multimodal
self-supervised action recognition framework achieves state-of-the-art
performance on multiple benchmarks, including Kinetics-400, Kinetics-600, and
Something-Something V2, demonstrating its effectiveness in capturing
fine-grained action dynamics.

</details>


### [74] [InvZW: Invariant Feature Learning via Noise-Adversarial Training for Robust Image Zero-Watermarking](https://arxiv.org/abs/2506.20370)
*Abdullah All Tanvir,Xin Zhong*

Main category: cs.CV

TL;DR: 提出一种基于深度学习的新型图像零水印框架，实现扭曲不变特征学习，并达到图像内容保护和水印恢复的高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决图像零水印中如何同时保持图像质量不变和提供高鲁棒性的问题。

Method: 通过两大模块：1. 噪声对抗学习提取具有扭曲不变性和语义表达力的特征；2. 基于学习的多位零水印方案，将提取的特征映射到可训练的参考码以匹配目标二进制信息。

Result: 在多种图像数据集和不同扭曲条件下，该方法在特征稳定性和水印恢复鲁棒性方面达到最先进水平。

Conclusion: 该框架相比现有自监督与深度水印技术具有更优的泛化能力和鲁棒性，是一种高效的零水印解决方案。

Abstract: This paper introduces a novel deep learning framework for robust image
zero-watermarking based on distortion-invariant feature learning. As a
zero-watermarking scheme, our method leaves the original image unaltered and
learns a reference signature through optimization in the feature space. The
proposed framework consists of two key modules. In the first module, a feature
extractor is trained via noise-adversarial learning to generate representations
that are both invariant to distortions and semantically expressive. This is
achieved by combining adversarial supervision against a distortion
discriminator and a reconstruction constraint to retain image content. In the
second module, we design a learning-based multibit zero-watermarking scheme
where the trained invariant features are projected onto a set of trainable
reference codes optimized to match a target binary message. Extensive
experiments on diverse image datasets and a wide range of distortions show that
our method achieves state-of-the-art robustness in both feature stability and
watermark recovery. Comparative evaluations against existing self-supervised
and deep watermarking techniques further highlight the superiority of our
framework in generalization and robustness.

</details>


### [75] [Exploiting Lightweight Hierarchical ViT and Dynamic Framework for Efficient Visual Tracking](https://arxiv.org/abs/2506.20381)
*Ben Kang,Xin Chen,Jie Zhao,Chunjuan Bo,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: 文章提出了一种名为HiT的高效视觉跟踪模型，解决了Transformer模型在资源受限设备上的速度问题，同时提出了DyHiT动态跟踪器，进一步优化准确性和速度的平衡。此外，介绍了一种无需训练的加速方法，可显著提升多种追踪器的速度。


<details>
  <summary>Details</summary>
Motivation: Transformer跟踪模型尽管性能强大，但处理速度较慢，难以满足资源受限设备的应用需求。

Method: HiT通过桥接模块提升轻量化Transformers的特征表征能力，并引入双图像位置编码方法。而DyHiT采用动态路由架构，根据场景复杂性选择最佳计算路径。此外，引入无需训练的加速方法。

Result: HiT在NVIDIA Jetson AGX平台上实现了61fps并在LaSOT基准上取得64.6%的AUC成绩；DyHiT以动态路由实现了111fps和62.4%的AUC表现，加速方法还使SeqTrack-B256在RTX 2080Ti上速度提升了2.68倍。

Conclusion: HiT和DyHiT显著提高了视觉跟踪器在速度与准确性之间的平衡，并开发了可通用的加速方法，为资源受限环境中的实际应用提供了解决方案。

Abstract: Transformer-based visual trackers have demonstrated significant advancements
due to their powerful modeling capabilities. However, their practicality is
limited on resource-constrained devices because of their slow processing
speeds. To address this challenge, we present HiT, a novel family of efficient
tracking models that achieve high performance while maintaining fast operation
across various devices. The core innovation of HiT lies in its Bridge Module,
which connects lightweight transformers to the tracking framework, enhancing
feature representation quality. Additionally, we introduce a dual-image
position encoding approach to effectively encode spatial information. HiT
achieves an impressive speed of 61 frames per second (fps) on the NVIDIA Jetson
AGX platform, alongside a competitive AUC of 64.6% on the LaSOT benchmark,
outperforming all previous efficient trackers.Building on HiT, we propose
DyHiT, an efficient dynamic tracker that flexibly adapts to scene complexity by
selecting routes with varying computational requirements. DyHiT uses search
area features extracted by the backbone network and inputs them into an
efficient dynamic router to classify tracking scenarios. Based on the
classification, DyHiT applies a divide-and-conquer strategy, selecting
appropriate routes to achieve a superior trade-off between accuracy and speed.
The fastest version of DyHiT achieves 111 fps on NVIDIA Jetson AGX while
maintaining an AUC of 62.4% on LaSOT.Furthermore, we introduce a training-free
acceleration method based on the dynamic routing architecture of DyHiT. This
method significantly improves the execution speed of various high-performance
trackers without sacrificing accuracy. For instance, our acceleration method
enables the state-of-the-art tracker SeqTrack-B256 to achieve a 2.68 times
speedup on an NVIDIA GeForce RTX 2080 Ti GPU while maintaining the same AUC of
69.9% on the LaSOT.

</details>


### [76] [A Novel Large Vision Foundation Model (LVFM)-based Approach for Generating High-Resolution Canopy Height Maps in Plantations for Precision Forestry Management](https://arxiv.org/abs/2506.20388)
*Shen Tan,Xin Zhang,Liangxiu Han,Huaguo Huang,Han Wang*

Main category: cs.CV

TL;DR: 提出了一种利用大型视觉基础模型（LVFM）和RGB影像生成高分辨率冠层高度图（CHM）的方法，性能超越传统方法，适用于林木生长监测和碳汇评估。


<details>
  <summary>Details</summary>
Motivation: 现有使用激光雷达生成CHM的方法成本高，而基于RGB影像的深度学习方法虽然成本低，但难以准确提取冠层高度特征。因此需要一种高效且准确的CHM生成方法。

Method: 开发了一种新型模型，基于LVFM，包括特征提取器、自监督的特征增强模块和高度估计器，利用1米分辨率的Google Earth影像进行训练。

Result: 在北京市房山区实验中，该模型的平均绝对误差为0.09米，均方根误差为0.24米，与激光雷达生成的CHM相关系数为0.78，个体树木检测成功率超过90%。

Conclusion: 该方法是一种成本效益高且可扩展的工具，在种植林地和天然森林的碳汇评估及林木生长监测中具有广阔的应用前景。

Abstract: Accurate, cost-effective monitoring of plantation aboveground biomass (AGB)
is crucial for supporting local livelihoods and carbon sequestration
initiatives like the China Certified Emission Reduction (CCER) program.
High-resolution canopy height maps (CHMs) are essential for this, but standard
lidar-based methods are expensive. While deep learning with RGB imagery offers
an alternative, accurately extracting canopy height features remains
challenging. To address this, we developed a novel model for high-resolution
CHM generation using a Large Vision Foundation Model (LVFM). Our model
integrates a feature extractor, a self-supervised feature enhancement module to
preserve spatial details, and a height estimator. Tested in Beijing's Fangshan
District using 1-meter Google Earth imagery, our model outperformed existing
methods, including conventional CNNs. It achieved a mean absolute error of 0.09
m, a root mean square error of 0.24 m, and a correlation of 0.78 against
lidar-based CHMs. The resulting CHMs enabled over 90% success in individual
tree detection, high accuracy in AGB estimation, and effective tracking of
plantation growth, demonstrating strong generalization to non-training areas.
This approach presents a promising, scalable tool for evaluating carbon
sequestration in both plantations and natural forests.

</details>


### [77] [Med-Art: Diffusion Transformer for 2D Medical Text-to-Image Generation](https://arxiv.org/abs/2506.20449)
*Changlu Guo,Anders Nymark Christensen,Morten Rieger Hannemose*

Main category: cs.CV

TL;DR: 本文提出了一种名为Med-Art的新框架，用于在有限数据条件下生成高质量医学图像，通过结合视觉语言模型和优化的细化方法，解决小数据集和医学文本稀缺性问题，并在多个指标上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 解决现有医学图像生成领域面临的挑战，即小数据集和医学文本数据稀缺的问题。

Method: 开发Med-Art框架，结合视觉-语言模型生成医学图像的视觉描述，并针对文本-图像模型PixArt-α进行适应性调整，同时提出了混合级扩散细化方法（HLDF）以优化生成性能。

Result: 在两个医疗图像数据集上取得了最先进的性能，评估指标包括FID、KID以及下游分类任务的表现。

Conclusion: Med-Art框架在医学图像生成方向展示了显著的效果，可以有效应对数据有限的情况下生成医学图像的问题，证明了其广泛的应用潜力。

Abstract: Text-to-image generative models have achieved remarkable breakthroughs in
recent years. However, their application in medical image generation still
faces significant challenges, including small dataset sizes, and scarcity of
medical textual data. To address these challenges, we propose Med-Art, a
framework specifically designed for medical image generation with limited data.
Med-Art leverages vision-language models to generate visual descriptions of
medical images which overcomes the scarcity of applicable medical textual data.
Med-Art adapts a large-scale pre-trained text-to-image model, PixArt-$\alpha$,
based on the Diffusion Transformer (DiT), achieving high performance under
limited data. Furthermore, we propose an innovative Hybrid-Level Diffusion
Fine-tuning (HLDF) method, which enables pixel-level losses, effectively
addressing issues such as overly saturated colors. We achieve state-of-the-art
performance on two medical image datasets, measured by FID, KID, and downstream
classification performance.

</details>


### [78] [HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based Diffusion Sampling](https://arxiv.org/abs/2506.20452)
*Tobias Vontobel,Seyedmorteza Sadat,Farnood Salehi,Romann M. Weber*

Main category: cs.CV

TL;DR: HiWave是一种无需训练的高分辨率图像生成方法，利用预训练的扩散模型改进图像的视觉保真度和结构一致性。


<details>
  <summary>Details</summary>
Motivation: 解决现有高分辨率图像生成方法计算开销大及超出训练分辨率时容易产生伪影的问题。

Method: 采用两阶段管道：基于预训练模型生成初始图像，再通过贴片式DDIM反演与基于小波的细节增强模块优化视觉效果。

Result: HiWave显著提高了超高分辨图像的感知质量，用户对其偏好超过80%。

Conclusion: HiWave在无重新训练或架构调整的情况下，提供稳定的高质量高分辨率图像生成能力。

Abstract: Diffusion models have emerged as the leading approach for image synthesis,
demonstrating exceptional photorealism and diversity. However, training
diffusion models at high resolutions remains computationally prohibitive, and
existing zero-shot generation techniques for synthesizing images beyond
training resolutions often produce artifacts, including object duplication and
spatial incoherence. In this paper, we introduce HiWave, a training-free,
zero-shot approach that substantially enhances visual fidelity and structural
coherence in ultra-high-resolution image synthesis using pretrained diffusion
models. Our method employs a two-stage pipeline: generating a base image from
the pretrained model followed by a patch-wise DDIM inversion step and a novel
wavelet-based detail enhancer module. Specifically, we first utilize inversion
methods to derive initial noise vectors that preserve global coherence from the
base image. Subsequently, during sampling, our wavelet-domain detail enhancer
retains low-frequency components from the base image to ensure structural
consistency, while selectively guiding high-frequency components to enrich fine
details and textures. Extensive evaluations using Stable Diffusion XL
demonstrate that HiWave effectively mitigates common visual artifacts seen in
prior methods, achieving superior perceptual quality. A user study confirmed
HiWave's performance, where it was preferred over the state-of-the-art
alternative in more than 80% of comparisons, highlighting its effectiveness for
high-quality, ultra-high-resolution image synthesis without requiring
retraining or architectural modifications.

</details>


### [79] [A Deep Learning Approach to Identify Rock Bolts in Complex 3D Point Clouds of Underground Mines Captured Using Mobile Laser Scanners](https://arxiv.org/abs/2506.20464)
*Dibyayan Patra,Pasindu Ranasinghe,Bikram Banerjee,Simit Raval*

Main category: cs.CV

TL;DR: 本文提出了一种名为DeepBolt的深度学习方法，能够在复杂的3D点云中自动识别地下矿井的岩石锚杆，性能远超当前技术方案。


<details>
  <summary>Details</summary>
Motivation: 矿井地下的岩石锚杆是支持系统的重要组成部分，其结构稳定性评估关系到矿井安全。然而，传统的人工调查和基于特征工程的方法效果有限，并且在低光环境和复杂结构下存在识别困难。

Method: 引入了一种名为DeepBolt的两阶段深度学习架构，专门针对类别不平衡问题设计，用于复杂3D点云中岩石锚杆的自动识别。

Result: DeepBolt在IoU指标上比当前最先进的语义分割模型提升了42.5%，并在岩石锚杆分类中达到了96.41%的精确率和96.96%的召回率，体现了其在复杂环境中的鲁棒性和有效性。

Conclusion: DeepBolt在自动化识别岩石锚杆问题上展现出显著优势，证明了深度学习技术在矿井安全领域的潜力。

Abstract: Rock bolts are crucial components of the subterranean support systems in
underground mines that provide adequate structural reinforcement to the rock
mass to prevent unforeseen hazards like rockfalls. This makes frequent
assessments of such bolts critical for maintaining rock mass stability and
minimising risks in underground mining operations. Where manual surveying of
rock bolts is challenging due to the low light conditions in the underground
mines and the time-intensive nature of the process, automated detection of rock
bolts serves as a plausible solution. To that end, this study focuses on the
automatic identification of rock bolts within medium to large-scale 3D point
clouds obtained from underground mines using mobile laser scanners. Existing
techniques for automated rock bolt identification primarily rely on feature
engineering and traditional machine learning approaches. However, such
techniques lack robustness as these point clouds present several challenges due
to data noise, varying environments, and complex surrounding structures.
Moreover, the target rock bolts are extremely small objects within large-scale
point clouds and are often partially obscured due to the application of
reinforcement shotcrete. Addressing these challenges, this paper proposes an
approach termed DeepBolt, which employs a novel two-stage deep learning
architecture specifically designed for handling severe class imbalance for the
automatic and efficient identification of rock bolts in complex 3D point
clouds. The proposed method surpasses state-of-the-art semantic segmentation
models by up to 42.5% in Intersection over Union (IoU) for rock bolt points.
Additionally, it outperforms existing rock bolt identification techniques,
achieving a 96.41% precision and 96.96% recall in classifying rock bolts,
demonstrating its robustness and effectiveness in complex underground
environments.

</details>


### [80] [AI-assisted radiographic analysis in detecting alveolar bone-loss severity and patterns](https://arxiv.org/abs/2506.20522)
*Chathura Wimalasiri,Piumal Rathnayake,Shamod Wijerathne,Sumudu Rasnayaka,Dhanushka Leuke Bandara,Roshan Ragel,Vajira Thambawita,Isuru Nawinne*

Main category: cs.CV

TL;DR: 本研究提出一种基于AI的深度学习框架，用于自动检测和量化通过内窥周牙根片评估的牙槽骨丧失程度和模式。


<details>
  <summary>Details</summary>
Motivation: 牙周炎导致牙槽骨丧失，显著影响口腔健康和生活质量，精准评估丧失程度对诊断和治疗至关重要。

Method: 结合YOLOv8用于牙齿检测，Keypoint R-CNN模型用于标定解剖学标志点，YOLOv8x-seg模型用于骨水平和牙齿分割，进行几何分析判断骨丢失模式。

Result: 在1000张专家注释的影像数据集中，检测骨丢失程度的ICC高达0.80，准确度为87%。

Conclusion: 该系统提供了快速、客观和可重复的牙周评估手段，有助于增强早期诊断和个性化治疗规划，提高患者护理和临床结果。

Abstract: Periodontitis, a chronic inflammatory disease causing alveolar bone loss,
significantly affects oral health and quality of life. Accurate assessment of
bone loss severity and pattern is critical for diagnosis and treatment
planning. In this study, we propose a novel AI-based deep learning framework to
automatically detect and quantify alveolar bone loss and its patterns using
intraoral periapical (IOPA) radiographs. Our method combines YOLOv8 for tooth
detection with Keypoint R-CNN models to identify anatomical landmarks, enabling
precise calculation of bone loss severity. Additionally, YOLOv8x-seg models
segment bone levels and tooth masks to determine bone loss patterns (horizontal
vs. angular) via geometric analysis. Evaluated on a large, expertly annotated
dataset of 1000 radiographs, our approach achieved high accuracy in detecting
bone loss severity (intra-class correlation coefficient up to 0.80) and bone
loss pattern classification (accuracy 87%). This automated system offers a
rapid, objective, and reproducible tool for periodontal assessment, reducing
reliance on subjective manual evaluation. By integrating AI into dental
radiographic analysis, our framework has the potential to improve early
diagnosis and personalized treatment planning for periodontitis, ultimately
enhancing patient care and clinical outcomes.

</details>


### [81] [Pay Less Attention to Deceptive Artifacts: Robust Detection of Compressed Deepfakes on Online Social Networks](https://arxiv.org/abs/2506.20548)
*Manyi Li,Renshuai Tao,Yufan Liu,Chuangchuang Tan,Haotong Qin,Bing Li,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: 研究引入了PLADA框架，提出通过消除压缩带来的“块效应”和处理非配对数据，提高深度伪造图像检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法难以有效应对在线社交网络图像的压缩引起的块效应和缺乏配对数据的挑战。

Method: 提出PLADA框架，其中包括两个核心模块：块效应清除器(B2E)和开放数据聚合(ODA)。B2E采用双阶段注意机制处理块效应，ODA则结合配对和非配对数据以增强检测能力。

Result: 通过26个数据集的大量实验表明，PLADA在深度伪造检测上优于当前方法，即使在有限的配对数据和压缩条件下也表现卓越。

Conclusion: PLADA框架有效解决了块效应对深度伪造检测的干扰，并为开放世界场景提供了稳健的解决方案。

Abstract: With the rapid advancement of deep learning, particularly through generative
adversarial networks (GANs) and diffusion models (DMs), AI-generated images, or
``deepfakes", have become nearly indistinguishable from real ones. These images
are widely shared across Online Social Networks (OSNs), raising concerns about
their misuse. Existing deepfake detection methods overlook the ``block effects"
introduced by compression in OSNs, which obscure deepfake artifacts, and
primarily focus on raw images, rarely encountered in real-world scenarios. To
address these challenges, we propose PLADA (Pay Less Attention to Deceptive
Artifacts), a novel framework designed to tackle the lack of paired data and
the ineffective use of compressed images. PLADA consists of two core modules:
Block Effect Eraser (B2E), which uses a dual-stage attention mechanism to
handle block effects, and Open Data Aggregation (ODA), which processes both
paired and unpaired data to improve detection. Extensive experiments across 26
datasets demonstrate that PLADA achieves a remarkable balance in deepfake
detection, outperforming SoTA methods in detecting deepfakes on OSNs, even with
limited paired data and compression. More importantly, this work introduces the
``block effect" as a critical factor in deepfake detection, providing a robust
solution for open-world scenarios. Our code is available at
https://github.com/ManyiLee/PLADA.

</details>


### [82] [Lightweight Multi-Frame Integration for Robust YOLO Object Detection in Videos](https://arxiv.org/abs/2506.20550)
*Yitong Quan,Benjamin Kiefer,Martin Messmer,Andreas Zell*

Main category: cs.CV

TL;DR: 提出了一种通过堆叠多帧输入到YOLO检测器的方法，以改进视频物体检测。


<details>
  <summary>Details</summary>
Motivation: 目前大部分模型处理视频检测时忽略了时间上下文信息，而现有视频检测方法的复杂性又增加了模型的大小和计算复杂度。

Method: 通过堆叠连续多帧作为输入，同时仅对单个目标帧的输出进行监督，从而最小化对架构的修改，同时保留计算效率和实时推理能力。

Result: 在MOT20Det和BOAT360数据集上的实验表明该方法提高了检测鲁棒性，尤其对于轻量级模型，以缩小紧凑型网络和复杂网络之间的性能差距。

Conclusion: 提出了一种简单有效的多帧输入视频物体检测方法，同时贡献了BOAT360数据集，以支持未来研究。

Abstract: Modern image-based object detection models, such as YOLOv7, primarily process
individual frames independently, thus ignoring valuable temporal context
naturally present in videos. Meanwhile, existing video-based detection methods
often introduce complex temporal modules, significantly increasing model size
and computational complexity. In practical applications such as surveillance
and autonomous driving, transient challenges including motion blur, occlusions,
and abrupt appearance changes can severely degrade single-frame detection
performance. To address these issues, we propose a straightforward yet highly
effective strategy: stacking multiple consecutive frames as input to a
YOLO-based detector while supervising only the output corresponding to a single
target frame. This approach leverages temporal information with minimal
modifications to existing architectures, preserving simplicity, computational
efficiency, and real-time inference capability. Extensive experiments on the
challenging MOT20Det and our BOAT360 datasets demonstrate that our method
improves detection robustness, especially for lightweight models, effectively
narrowing the gap between compact and heavy detection networks. Additionally,
we contribute the BOAT360 benchmark dataset, comprising annotated fisheye video
sequences captured from a boat, to support future research in multi-frame video
object detection in challenging real-world scenarios.

</details>


### [83] [AdvMIM: Adversarial Masked Image Modeling for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2506.20563)
*Lei Zhu,Jun Zhou,Rick Siow Mong Goh,Yong Liu*

Main category: cs.CV

TL;DR: 本论文提出了一种对抗式掩码图像建模方法，用于提高在半监督医学图像分割任务中的Transformer模型表现。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer模型在半监督学习中因标签数据短缺导致表现受限的问题。

Method: 通过构建掩码域，利用掩码输入训练Transformer预测完整分割掩码。同时，通过对抗式训练损失缩小原始域与掩码域的差距，从多域学习角度优化方法。

Result: 在三个公开的医学图像分割数据集上，方法表现显著优于现有方法。

Conclusion: 对抗式掩码图像建模方法能有效增强在半监督医学图像分割任务中Transformer的能力，有望助力标签稀缺场景下的医学影像分析。

Abstract: Vision Transformer has recently gained tremendous popularity in medical image
segmentation task due to its superior capability in capturing long-range
dependencies. However, transformer requires a large amount of labeled data to
be effective, which hinders its applicability in annotation scarce
semi-supervised learning scenario where only limited labeled data is available.
State-of-the-art semi-supervised learning methods propose combinatorial
CNN-Transformer learning to cross teach a transformer with a convolutional
neural network, which achieves promising results. However, it remains a
challenging task to effectively train the transformer with limited labeled
data. In this paper, we propose an adversarial masked image modeling method to
fully unleash the potential of transformer for semi-supervised medical image
segmentation. The key challenge in semi-supervised learning with transformer
lies in the lack of sufficient supervision signal. To this end, we propose to
construct an auxiliary masked domain from original domain with masked image
modeling and train the transformer to predict the entire segmentation mask with
masked inputs to increase supervision signal. We leverage the original labels
from labeled data and pseudo-labels from unlabeled data to learn the masked
domain. To further benefit the original domain from masked domain, we provide a
theoretical analysis of our method from a multi-domain learning perspective and
devise a novel adversarial training loss to reduce the domain gap between the
original and masked domain, which boosts semi-supervised learning performance.
We also extend adversarial masked image modeling to CNN network. Extensive
experiments on three public medical image segmentation datasets demonstrate the
effectiveness of our method, where our method outperforms existing methods
significantly. Our code is publicly available at
https://github.com/zlheui/AdvMIM.

</details>


### [84] [Show, Tell and Summarize: Dense Video Captioning Using Visual Cue Aided Sentence Summarization](https://arxiv.org/abs/2506.20567)
*Zhiwang Zhang,Dong Xu,Wanli Ouyang,Chuanqi Tan*

Main category: cs.CV

TL;DR: 本文提出了一种用于密集视频描述的分割与总结(DaS)框架，在长视频中提取事件提案，通过两个阶段的LSTM网络生成事件的浓缩文本描述。


<details>
  <summary>Details</summary>
Motivation: 旨在解决长视频精确摘要描述的难题，通过丰富语义和视觉信息提取全面的事件描述。

Method: 提出分割与总结框架(DaS)，分两阶段进行：首先用LSTM编码段落的语义词与视觉特征；其次用另一LSTM解码出浓缩的描述文本，同时引入层级注意力机制。

Result: 在ActivityNet Captions数据集上验证了DaS框架效果显著，与现有方法相比具优势。

Conclusion: 新方法通过将视觉特征与文本生成有效结合，提高了密集视频描述的质量。

Abstract: In this work, we propose a division-and-summarization (DaS) framework for
dense video captioning. After partitioning each untrimmed long video as
multiple event proposals, where each event proposal consists of a set of short
video segments, we extract visual feature (e.g., C3D feature) from each segment
and use the existing image/video captioning approach to generate one sentence
description for this segment. Considering that the generated sentences contain
rich semantic descriptions about the whole event proposal, we formulate the
dense video captioning task as a visual cue aided sentence summarization
problem and propose a new two stage Long Short Term Memory (LSTM) approach
equipped with a new hierarchical attention mechanism to summarize all generated
sentences as one descriptive sentence with the aid of visual features.
Specifically, the first-stage LSTM network takes all semantic words from the
generated sentences and the visual features from all segments within one event
proposal as the input, and acts as the encoder to effectively summarize both
semantic and visual information related to this event proposal. The
second-stage LSTM network takes the output from the first-stage LSTM network
and the visual features from all video segments within one event proposal as
the input, and acts as the decoder to generate one descriptive sentence for
this event proposal. Our comprehensive experiments on the ActivityNet Captions
dataset demonstrate the effectiveness of our newly proposed DaS framework for
dense video captioning.

</details>


### [85] [Causal Representation Learning with Observational Grouping for CXR Classification](https://arxiv.org/abs/2506.20582)
*Rajat Rasal,Avinash Kori,Ben Glocker*

Main category: cs.CV

TL;DR: 通过对观测数据分组，以一种端到端框架学习胸部X光疾病分类的因果表示。


<details>
  <summary>Details</summary>
Motivation: 提高医疗影像中任务特定潜在特征的普适性和鲁棒性。

Method: 提出通过分组观测数据以实现因果表示学习，并利用端到端框架进行疾病分类。

Result: 这种因果表示在多个分类任务中提升了普适性和鲁棒性，特别是在与种族、性别和成像视角无关的情况下表现优异。

Conclusion: 分组和因果表示学习可以有效提升医疗影像分析中的分类任务性能。

Abstract: Identifiable causal representation learning seeks to uncover the true causal
relationships underlying a data generation process. In medical imaging, this
presents opportunities to improve the generalisability and robustness of
task-specific latent features. This work introduces the concept of grouping
observations to learn identifiable representations for disease classification
in chest X-rays via an end-to-end framework. Our experiments demonstrate that
these causal representations improve generalisability and robustness across
multiple classification tasks when grouping is used to enforce invariance w.r.t
race, sex, and imaging views.

</details>


### [86] [Dense Video Captioning using Graph-based Sentence Summarization](https://arxiv.org/abs/2506.20583)
*Zhiwang Zhang,Dong Xu,Wanli Ouyang,Luping Zhou*

Main category: cs.CV

TL;DR: 提出一种基于图的分段与总结（GPaS）框架用于密集视频描述。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法难以处理较长事件提案中场景和对象变化的问题。

Method: 采用两阶段框架：第一阶段，将整个事件提案拆分为短视频片段；第二阶段，使用图卷积网络（GCN）和长短期记忆网络（LSTM）的交互模块总结生成片段描述的语义信息。

Result: 在ActivityNet Captions和YouCook II数据集上与现有方法相比表现更优越。

Conclusion: 提出的GPaS框架能够更好地利用语义词互动关系，提升密集视频描述的准确性。

Abstract: Recently, dense video captioning has made attractive progress in detecting
and captioning all events in a long untrimmed video. Despite promising results
were achieved, most existing methods do not sufficiently explore the scene
evolution within an event temporal proposal for captioning, and therefore
perform less satisfactorily when the scenes and objects change over a
relatively long proposal. To address this problem, we propose a graph-based
partition-and-summarization (GPaS) framework for dense video captioning within
two stages. For the ``partition" stage, a whole event proposal is split into
short video segments for captioning at a finer level. For the ``summarization"
stage, the generated sentences carrying rich description information for each
segment are summarized into one sentence to describe the whole event. We
particularly focus on the ``summarization" stage, and propose a framework that
effectively exploits the relationship between semantic words for summarization.
We achieve this goal by treating semantic words as nodes in a graph and
learning their interactions by coupling Graph Convolutional Network (GCN) and
Long Short Term Memory (LSTM), with the aid of visual cues. Two schemes of
GCN-LSTM Interaction (GLI) modules are proposed for seamless integration of GCN
and LSTM. The effectiveness of our approach is demonstrated via an extensive
comparison with the state-of-the-arts methods on the two benchmarks ActivityNet
Captions dataset and YouCook II dataset.

</details>


### [87] [Learning-Based Distance Estimation for 360° Single-Sensor Setups](https://arxiv.org/abs/2506.20586)
*Yitong Quan,Benjamin Kiefer,Martin Messmer,Andreas Zell*

Main category: cs.CV

TL;DR: 研究提出一种基于神经网络的新方法，通过单个360°鱼眼相机实现单目距离估计，并在多数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决传统几何方法在全向成像中因镜头失真与环境变化导致的距离估计困难。

Method: 利用神经网络，直接从全向图像输入中学习和推断物体距离，避免传统方法中对镜头精准校准的依赖。

Result: 在LOAF、ULM360和新数据集Boat360上进行评估，结果表明其在准确性和鲁棒性上优于传统几何方法和其他学习基线模型。

Conclusion: 深度学习在实时全向距离估计中的潜力巨大，尤其适合机器人、自动导航及监控中的低成本应用。

Abstract: Accurate distance estimation is a fundamental challenge in robotic
perception, particularly in omnidirectional imaging, where traditional
geometric methods struggle with lens distortions and environmental variability.
In this work, we propose a neural network-based approach for monocular distance
estimation using a single 360{\deg} fisheye lens camera. Unlike classical
trigonometric techniques that rely on precise lens calibration, our method
directly learns and infers the distance of objects from raw omnidirectional
inputs, offering greater robustness and adaptability across diverse conditions.
We evaluate our approach on three 360{\deg} datasets (LOAF, ULM360, and a newly
captured dataset Boat360), each representing distinct environmental and sensor
setups. Our experimental results demonstrate that the proposed learning-based
model outperforms traditional geometry-based methods and other learning
baselines in both accuracy and robustness. These findings highlight the
potential of deep learning for real-time omnidirectional distance estimation,
making our approach particularly well-suited for low-cost applications in
robotics, autonomous navigation, and surveillance.

</details>


### [88] [TRIM: A Self-Supervised Video Summarization Framework Maximizing Temporal Relative Information and Representativeness](https://arxiv.org/abs/2506.20588)
*Pritam Mishra,Coloma Ballester,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: 提出了一个无需注意力机制、RNN或transformers的自监督视频摘要模型，使用马尔可夫过程驱动的损失和两阶段自监督学习，实现了跨领域高效性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频内容广泛存在，但提取有效信息的方法多依赖监督注释或注意力模型，这在计算资源和跨域适应性上存在局限。

Method: 采用一种自监督模型，结合马尔可夫过程损失指标和两阶段学习，不用复杂的注意力机制来保证时空依赖建模。

Result: 在SUMME和TVSUM数据集上达到了最先进的性能，并与最佳监督方法相当，同时证明了无注释架构的潜力。

Conclusion: 该方法降低了对复杂架构的依赖，为更通用的视频摘要技术提供了新的方向。

Abstract: The increasing ubiquity of video content and the corresponding demand for
efficient access to meaningful information have elevated video summarization
and video highlights as a vital research area. However, many state-of-the-art
methods depend heavily either on supervised annotations or on attention-based
models, which are computationally expensive and brittle in the face of
distribution shifts that hinder cross-domain applicability across datasets. We
introduce a pioneering self-supervised video summarization model that captures
both spatial and temporal dependencies without the overhead of attention, RNNs,
or transformers. Our framework integrates a novel set of Markov process-driven
loss metrics and a two-stage self supervised learning paradigm that ensures
both performance and efficiency. Our approach achieves state-of-the-art
performance on the SUMME and TVSUM datasets, outperforming all existing
unsupervised methods. It also rivals the best supervised models, demonstrating
the potential for efficient, annotation-free architectures. This paves the way
for more generalizable video summarization techniques and challenges the
prevailing reliance on complex architectures.

</details>


### [89] [WonderFree: Enhancing Novel View Quality and Cross-View Consistency for 3D Scene Exploration](https://arxiv.org/abs/2506.20590)
*Chaojun Ni,Jie Li,Haoyun Li,Hengyu Liu,Xiaofeng Wang,Zheng Zhu,Guosheng Zhao,Boyuan Wang,Chenxin Li,Guan Huang,Wenjun Mei*

Main category: cs.CV

TL;DR: 本研究提出WonderFree模型，解决了单张图像生成交互式3D场景中视角可探索性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前3D生成方法的视角探索能力有限，无法在超出原始视点范围内的任意角度生成高质量图像。提出WonderFree旨在解决这一问题。

Method: 研究将问题分解为视角质量与跨视角一致性两个子问题，并提出WorldRestorer和ConsistView模型来分别提升视觉效果和一致性。同时开发数据采集管道以支持多样化风格场景的生成。

Result: 实验表明，WonderFree显著提升了多视角渲染质量和跨视角一致性，用户偏好率达77.20%。

Conclusion: WonderFree能提供更加无缝和沉浸式的3D探索体验，所有代码、模型及数据将会公开。

Abstract: Interactive 3D scene generation from a single image has gained significant
attention due to its potential to create immersive virtual worlds. However, a
key challenge in current 3D generation methods is the limited explorability,
which cannot render high-quality images during larger maneuvers beyond the
original viewpoint, particularly when attempting to move forward into unseen
areas. To address this challenge, we propose WonderFree, the first model that
enables users to interactively generate 3D worlds with the freedom to explore
from arbitrary angles and directions. Specifically, we decouple this challenge
into two key subproblems: novel view quality, which addresses visual artifacts
and floating issues in novel views, and cross-view consistency, which ensures
spatial consistency across different viewpoints. To enhance rendering quality
in novel views, we introduce WorldRestorer, a data-driven video restoration
model designed to eliminate floaters and artifacts. In addition, a data
collection pipeline is presented to automatically gather training data for
WorldRestorer, ensuring it can handle scenes with varying styles needed for 3D
scene generation. Furthermore, to improve cross-view consistency, we propose
ConsistView, a multi-view joint restoration mechanism that simultaneously
restores multiple perspectives while maintaining spatiotemporal coherence.
Experimental results demonstrate that WonderFree not only enhances rendering
quality across diverse viewpoints but also significantly improves global
coherence and consistency. These improvements are confirmed by CLIP-based
metrics and a user study showing a 77.20% preference for WonderFree over
WonderWorld enabling a seamless and immersive 3D exploration experience. The
code, model, and data will be publicly available.

</details>


### [90] [SFNet: Fusion of Spatial and Frequency-Domain Features for Remote Sensing Image Forgery Detection](https://arxiv.org/abs/2506.20599)
*Ji Qi,Xinchang Zhang,Dingqi Ye,Yongjia Ruan,Xin Guo,Shaowen Wang,Haifeng Li*

Main category: cs.CV

TL;DR: 生成性人工智能生成的遥感影像伪造难以检测，现有方法依赖于单一的视觉特征，难以在多样化数据中泛化，此文提出了利用空间和频域特征的SFNet框架解决该问题。


<details>
  <summary>Details</summary>
Motivation: 当前生成性人工智能生成的伪造遥感图像可能导致错误情报和虚假信息，而现有伪造检测方法受限于单一视觉特征，难以有效泛化至多样化的遥感数据。

Method: 提出一种名为SFNet的伪造检测框架，同时提取空间和频域特征，并利用域特征映射模块和混合域特征优化模块对多域特征进行对齐和融合。

Result: 在三个数据集上实验表明，SFNet比现有方法的检测准确率提高了4%-15.18%，并展示出较强的泛化能力。

Conclusion: SFNet通过联合空间和频域特征更有效地检测多样化的伪造遥感数据，具备显著优势和实用前景。

Abstract: The rapid advancement of generative artificial intelligence is producing fake
remote sensing imagery (RSI) that is increasingly difficult to detect,
potentially leading to erroneous intelligence, fake news, and even conspiracy
theories. Existing forgery detection methods typically rely on single visual
features to capture predefined artifacts, such as spatial-domain cues to detect
forged objects like roads or buildings in RSI, or frequency-domain features to
identify artifacts from up-sampling operations in adversarial generative
networks (GANs). However, the nature of artifacts can significantly differ
depending on geographic terrain, land cover types, or specific features within
the RSI. Moreover, these complex artifacts evolve as generative models become
more sophisticated. In short, over-reliance on a single visual cue makes
existing forgery detectors struggle to generalize across diverse remote sensing
data. This paper proposed a novel forgery detection framework called SFNet,
designed to identify fake images in diverse remote sensing data by leveraging
spatial and frequency domain features. Specifically, to obtain rich and
comprehensive visual information, SFNet employs two independent feature
extractors to capture spatial and frequency domain features from input RSIs. To
fully utilize the complementary domain features, the domain feature mapping
module and the hybrid domain feature refinement module(CBAM attention) of SFNet
are designed to successively align and fuse the multi-domain features while
suppressing redundant information. Experiments on three datasets show that
SFNet achieves an accuracy improvement of 4%-15.18% over the state-of-the-art
RS forgery detection methods and exhibits robust generalization capabilities.
The code is available at https://github.com/GeoX-Lab/RSTI/tree/main/SFNet.

</details>


### [91] [Video Perception Models for 3D Scene Synthesis](https://arxiv.org/abs/2506.20601)
*Rui Huang,Guangyao Zhai,Zuria Bauer,Marc Pollefeys,Federico Tombari,Leonidas Guibas,Gao Huang,Francis Engelmann*

Main category: cs.CV

TL;DR: 传统的3D场景合成需要专家知识和大量手动操作，本文提出VIPScene框架，通过视频生成模型的知识确保场景一致性，显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前方法受限于LLM的空间推理能力不足和图像生成方法视点选择困难，本文希望突破这些限制，实现自动化、高质量的3D场景合成。

Method: 提出VIPScene框架，结合视频生成、前向3D重建和开放词汇感知模型，利用文本和图像输入生成语义和几何一致的3D场景，同时引入FPVScore指标以增强评估。

Result: VIPScene在实验中显著优于现有方法，展示了对各种场景的良好泛化能力。

Conclusion: VIPScene利用视频生成模型实现高逼真度和结构一致性的3D场景合成，并提出了创新的评价方法，未来代码将开源。

Abstract: Traditionally, 3D scene synthesis requires expert knowledge and significant
manual effort. Automating this process could greatly benefit fields such as
architectural design, robotics simulation, virtual reality, and gaming. Recent
approaches to 3D scene synthesis often rely on the commonsense reasoning of
large language models (LLMs) or strong visual priors of modern image generation
models. However, current LLMs demonstrate limited 3D spatial reasoning ability,
which restricts their ability to generate realistic and coherent 3D scenes.
Meanwhile, image generation-based methods often suffer from constraints in
viewpoint selection and multi-view inconsistencies. In this work, we present
Video Perception models for 3D Scene synthesis (VIPScene), a novel framework
that exploits the encoded commonsense knowledge of the 3D physical world in
video generation models to ensure coherent scene layouts and consistent object
placements across views. VIPScene accepts both text and image prompts and
seamlessly integrates video generation, feedforward 3D reconstruction, and
open-vocabulary perception models to semantically and geometrically analyze
each object in a scene. This enables flexible scene synthesis with high realism
and structural consistency. For more precise analysis, we further introduce
First-Person View Score (FPVScore) for coherence and plausibility evaluation,
utilizing continuous first-person perspective to capitalize on the reasoning
ability of multimodal large language models. Extensive experiments show that
VIPScene significantly outperforms existing methods and generalizes well across
diverse scenarios. The code will be released.

</details>


### [92] [Shape2Animal: Creative Animal Generation from Natural Silhouettes](https://arxiv.org/abs/2506.20616)
*Quoc-Duy Tran,Anh-Tuan Vo,Dinh-Khoi Vo,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: 本论文提出了一种名为Shape2Animal的框架，将自然对象的轮廓重新解释为动物形态，利用视觉语言模型与文本生成技术生成具有一致性的图像。


<details>
  <summary>Details</summary>
Motivation: 通过模仿人类在模糊刺激中发现有意义模式的能力（即空想性错觉），开发一种能够自动将自然物体轮廓重新解释为动物形态的方法。

Method: 使用开放词汇的分割技术提取物体轮廓；通过视觉语言模型解析适合的动物概念；利用文本到图像的扩散模型生成动物图像并无缝合成原场景。

Result: 实验表明，Shape2Animal框架在各类真实世界输入中表现出强大的鲁棒性与创造潜能。

Conclusion: Shape2Animal框架可广泛应用于视觉故事创作、教育内容、数字艺术及交互媒体设计等领域，展现出无限的可能性。

Abstract: Humans possess a unique ability to perceive meaningful patterns in ambiguous
stimuli, a cognitive phenomenon known as pareidolia. This paper introduces
Shape2Animal framework to mimics this imaginative capacity by reinterpreting
natural object silhouettes, such as clouds, stones, or flames, as plausible
animal forms. Our automated framework first performs open-vocabulary
segmentation to extract object silhouette and interprets semantically
appropriate animal concepts using vision-language models. It then synthesizes
an animal image that conforms to the input shape, leveraging text-to-image
diffusion model and seamlessly blends it into the original scene to generate
visually coherent and spatially consistent compositions. We evaluated
Shape2Animal on a diverse set of real-world inputs, demonstrating its
robustness and creative potential. Our Shape2Animal can offer new opportunities
for visual storytelling, educational content, digital art, and interactive
media design. Our project page is here: https://shape2image.github.io

</details>


### [93] [Joint attitude estimation and 3D neural reconstruction of non-cooperative space objects](https://arxiv.org/abs/2506.20638)
*Clément Forray,Pauline Delporte,Nicolas Delaygue,Florence Genin,Dawa Derksen*

Main category: cs.CV

TL;DR: 该论文利用Neural Radiance Fields (NeRF) 通过模拟图像进行非合作空间物体的3D重建。


<details>
  <summary>Details</summary>
Motivation: 提高对地球轨道物体状态和行为的知识，以支持如活跃碎片清除、轨道维护或异常检测等应用。

Method: 通过NeRF模型进行3D重建，并联合优化相机姿态，同时采用逐帧图像训练，以克服单色图像、未知物体方向、有限视角等挑战。

Result: 实验结果表明，逐帧图像训练可获得最准确的3D重建，并通过优化统一旋转和正则化避免相机姿态过渡变化过大。

Conclusion: 联合优化NeRF与相机姿态是解决非合作空间物体3D重建挑战的有效方法。

Abstract: Obtaining a better knowledge of the current state and behavior of objects
orbiting Earth has proven to be essential for a range of applications such as
active debris removal, in-orbit maintenance, or anomaly detection. 3D models
represent a valuable source of information in the field of Space Situational
Awareness (SSA). In this work, we leveraged Neural Radiance Fields (NeRF) to
perform 3D reconstruction of non-cooperative space objects from simulated
images. This scenario is challenging for NeRF models due to unusual camera
characteristics and environmental conditions : mono-chromatic images, unknown
object orientation, limited viewing angles, absence of diffuse lighting etc. In
this work we focus primarly on the joint optimization of camera poses alongside
the NeRF. Our experimental results show that the most accurate 3D
reconstruction is achieved when training with successive images one-by-one. We
estimate camera poses by optimizing an uniform rotation and use regularization
to prevent successive poses from being too far apart.

</details>


### [94] [Disentangled representations of microscopy images](https://arxiv.org/abs/2506.20649)
*Jacopo Dapueto,Vito Paolo Pastore,Nicoletta Noceti,Francesca Odone*

Main category: cs.CV

TL;DR: 該研究提出了一種基於解耦表徵學習（DRL）的方法，用於提升顯微鏡圖像分類的模型可解釋性。


<details>
  <summary>Details</summary>
Motivation: 現代顯微鏡圖像獲取系統產生大量數據，但儘管深度學習在圖像分析領域表現良好，可解釋性仍是一大挑戰。

Method: 研究提出了一種基於解耦表徵學習的方法，通過從合成數據學習的表徵轉移到顯微鏡圖像中進行分類。

Result: 該方法在三個顯微圖像領域（浮游生物、酵母液泡及人體細胞）的基準數據集中顯示了該方法在準確性和可解釋性之間的良好平衡。

Conclusion: 此解法證明了基於解耦表徵學習的方法在顯微鏡圖像分析中可以有效提升模型的可解釋性的同時保持高準確性。

Abstract: Microscopy image analysis is fundamental for different applications, from
diagnosis to synthetic engineering and environmental monitoring. Modern
acquisition systems have granted the possibility to acquire an escalating
amount of images, requiring a consequent development of a large collection of
deep learning-based automatic image analysis methods. Although deep neural
networks have demonstrated great performance in this field, interpretability,
an essential requirement for microscopy image analysis, remains an open
challenge.
  This work proposes a Disentangled Representation Learning (DRL) methodology
to enhance model interpretability for microscopy image classification.
Exploiting benchmark datasets from three different microscopic image domains
(plankton, yeast vacuoles, and human cells), we show how a DRL framework, based
on transferring a representation learnt from synthetic data, can provide a good
trade-off between accuracy and interpretability in this domain.

</details>


### [95] [MMSearch-R1: Incentivizing LMMs to Search](https://arxiv.org/abs/2506.20670)
*Jinming Wu,Zihao Deng,Wei Li,Yiding Liu,Bo You,Bo Li,Zejun Ma,Ziwei Liu*

Main category: cs.CV

TL;DR: 该论文介绍了MMSearch-R1，一种新型多模态搜索强化学习框架，在各种知识密集型和信息查询任务中表现突出，能显著减少搜索调用。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在处理复杂、动态的真实世界信息时，需要外部知识源来支持，而现有方法往往由于固定的流程导致效率低下。

Method: 提出了MMSearch-R1，通过整合图像和文本搜索工具，应用基于结果奖励和搜索惩罚的强化学习框架，实现端到端的多轮搜索决策。还采集了一个多模态搜索VQA数据集，并挑选出平衡的样本用于高效搜索行为的训练。

Result: 在知识密集型和信息需求的VQA任务中，模型超越了同尺寸的RAG基线表现，并以减少30%以上的搜索调用与更大的RAG模型性能持平。

Conclusion: MMSearch-R1成功展示了在多模态搜索任务中更高效和按需搜索的潜力，并为未来研究提供了实践启示。

Abstract: Robust deployment of large multimodal models (LMMs) in real-world scenarios
requires access to external knowledge sources, given the complexity and dynamic
nature of real-world information. Existing approaches such as
retrieval-augmented generation (RAG) and prompt engineered search agents rely
on rigid pipelines, often leading to inefficient or excessive search behaviors.
We present MMSearch-R1, the first end-to-end reinforcement learning framework
that enables LMMs to perform on-demand, multi-turn search in real-world
Internet environments. Our framework integrates both image and text search
tools, allowing the model to reason about when and how to invoke them guided by
an outcome-based reward with a search penalty. To support training, We collect
a multimodal search VQA dataset through a semi-automated pipeline that covers
diverse visual and textual knowledge needs and curate a search-balanced subset
with both search-required and search-free samples, which proves essential for
shaping efficient and on-demand search behavior. Extensive experiments on
knowledge-intensive and info-seeking VQA tasks show that our model not only
outperforms RAG-based baselines of the same model size, but also matches the
performance of a larger RAG-based model while reducing search calls by over
30%. We further analyze key empirical findings to offer actionable insights for
advancing research in multimodal search.

</details>


### [96] [IPFormer: Visual 3D Panoptic Scene Completion with Context-Adaptive Instance Proposals](https://arxiv.org/abs/2506.20671)
*Markus Gross,Aya Fahmy,Danit Niwattananan,Dominik Muhle,Rui Song,Daniel Cremers,Henri Meeß*

Main category: cs.CV

TL;DR: 论文提出了IPFormer方法，通过动态利用图像上下文生成实例提案，解决基于视觉的3D全景场景补全问题，显著提升了精度和效率。


<details>
  <summary>Details</summary>
Motivation: 目前的全景场景补全（PSC）方法主要基于LiDAR，基于摄像头的研究较少；此外，现有变压器方法的静态查询限制了对动态场景的适应能力。

Method: 提出IPFormer，利用基于图像上下文的动态实例提案初始化，并通过编码器和解码器进一步优化，以推理语义与实例体素关系。

Result: 实验结果表明，相较于当前最先进的方法，IPFormer在全景质量指标（PQ$^\dagger$ 和 PQ-All）方面有显著提高，运行速度提升超过14倍，并在动态实例提案的使用上带来了显著改进。

Conclusion: IPFormer开创性地提出了基于上下文的实例提案方法，为基于视觉的3D全景场景补全提供了新的研究方向，同时显著优化了精度和性能。

Abstract: Semantic Scene Completion (SSC) has emerged as a pivotal approach for jointly
learning scene geometry and semantics, enabling downstream applications such as
navigation in mobile robotics. The recent generalization to Panoptic Scene
Completion (PSC) advances the SSC domain by integrating instance-level
information, thereby enhancing object-level sensitivity in scene understanding.
While PSC was introduced using LiDAR modality, methods based on camera images
remain largely unexplored. Moreover, recent Transformer-based SSC approaches
utilize a fixed set of learned queries to reconstruct objects within the scene
volume. Although these queries are typically updated with image context during
training, they remain static at test time, limiting their ability to
dynamically adapt specifically to the observed scene. To overcome these
limitations, we propose IPFormer, the first approach that leverages
context-adaptive instance proposals at train and test time to address
vision-based 3D Panoptic Scene Completion. Specifically, IPFormer adaptively
initializes these queries as panoptic instance proposals derived from image
context and further refines them through attention-based encoding and decoding
to reason about semantic instance-voxel relationships. Experimental results
show that our approach surpasses state-of-the-art methods in overall panoptic
metrics PQ$^\dagger$ and PQ-All, matches performance in individual metrics, and
achieves a runtime reduction exceeding 14$\times$. Furthermore, our ablation
studies reveal that dynamically deriving instance proposals from image context,
as opposed to random initialization, leads to a 3.62% increase in PQ-All and a
remarkable average improvement of 18.65% in combined Thing-metrics. These
results highlight our introduction of context-adaptive instance proposals as a
pioneering effort in addressing vision-based 3D Panoptic Scene Completion.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [97] [CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation](https://arxiv.org/abs/2506.19952)
*Deepon Halder,Thanmay Jayakumar,Raj Dabre*

Main category: cs.CL

TL;DR: 本文提出了CycleDistill方法，通过利用大语言模型（LLMs）和少量示例翻译，实现无需平行语料的高质量机器翻译，特别适用于低资源语言。


<details>
  <summary>Details</summary>
Motivation: 低资源语言中平行语料匮乏，导致现有的大语言模型和专用机器翻译系统难以提供高质量翻译。

Method: CycleDistill方法通过迭代生成合成平行语料，并借助极少量的少示例翻译对模型进行微调，无需大量平行语料支持。

Result: 实验表明，在印度三种语言的翻译任务中，仅使用单语语料，CycleDistill方法显著优于基线模型，提升约20-30 chrF分。

Conclusion: CycleDistill方法证明了无需大量平行语料的情况下，也能实现低资源语言的高质量机器翻译，展示了其在相关领域的巨大潜力。

Abstract: Large language models (LLMs), despite their ability to perform few-shot
machine translation (MT), often lag behind dedicated MT systems trained on
parallel corpora, which are crucial for high quality machine translation (MT).
However, parallel corpora are often scarce or non-existent for low-resource
languages. In this paper, we propose CycleDistill, a bootstrapping approach
leveraging LLMs and few-shot translation to obtain high-quality MT systems.
CycleDistill involves iteratively generating synthetic parallel corpora from
monolingual corpora via zero- or few-shot MT, which is then used to fine-tune
the model that was used for generating said data for MT. CycleDistill does not
need parallel corpora beyond 1 to 4 few-shot examples, and in our experiments
focusing on three Indian languages, by relying solely on monolingual corpora,
it can achieve high-quality machine translation, improving upon a few-shot
baseline model by over 20-30 chrF points on average in the first iteration. We
also study the effect of leveraging softmax activations during the distillation
process and observe mild improvements in translation quality.

</details>


### [98] [Inference Scaled GraphRAG: Improving Multi Hop Question Answering on Knowledge Graphs](https://arxiv.org/abs/2506.19967)
*Travis Thompson,Seung-Hwan Lim,Paul Liu,Ruoying He,Dongkuan Xu*

Main category: cs.CL

TL;DR: 本论文提出了一种名为Inference-Scaled GraphRAG的新框架，通过在推理阶段的计算扩展，显著提升了大语言模型在多跳知识推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在知识密集型推理任务中的表现不佳，主要因为它们难以有效利用结构化的上下文和多跳信息，而现有的RAG方法对于捕捉知识图谱中节点之间的关系结构也表现有限。

Method: 提出了一种名为Inference-Scaled GraphRAG的推理框架，该框架通过对推论阶段的计算进行扩展，包括序列扩展（深层链式思维图遍历）和并行扩展（样本轨迹的多数投票）。两种扩展通过交替进行的推理-执行循环相结合。

Result: 在GRBench基准上进行的实验表明，该方法显著提升了多跳问答任务的表现，相较传统GraphRAG和现有图遍历方法效果更佳。

Conclusion: 推理阶段的计算扩展是一种实际且与模型架构无关的解决方案，能够提升大语言模型在处理结构化知识推理任务中的表现。

Abstract: Large Language Models (LLMs) have achieved impressive capabilities in
language understanding and generation, yet they continue to underperform on
knowledge-intensive reasoning tasks due to limited access to structured context
and multi-hop information. Retrieval-Augmented Generation (RAG) partially
mitigates this by grounding generation in retrieved context, but conventional
RAG and GraphRAG methods often fail to capture relational structure across
nodes in knowledge graphs. We introduce Inference-Scaled GraphRAG, a novel
framework that enhances LLM-based graph reasoning by applying inference-time
compute scaling. Our method combines sequential scaling with deep
chain-of-thought graph traversal, and parallel scaling with majority voting
over sampled trajectories within an interleaved reasoning-execution loop.
Experiments on the GRBench benchmark demonstrate that our approach
significantly improves multi-hop question answering performance, achieving
substantial gains over both traditional GraphRAG and prior graph traversal
baselines. These findings suggest that inference-time scaling is a practical
and architecture-agnostic solution for structured knowledge reasoning with LLMs

</details>


### [99] [Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation](https://arxiv.org/abs/2506.19998)
*Xinyi Ni,Haonan Jian,Qiuyang Wang,Vedanshi Chetan Shah,Pengyu Hong*

Main category: cs.CL

TL;DR: Doc2Agent是一种打造API调用工具代理的方法。


<details>
  <summary>Details</summary>
Motivation: 解决基于REST API的代理无法应对真实世界非结构化API文档的问题。

Method: 通过一个可扩展的流水线，从API文档生成可执行Python工具，并采用代码代理迭代优化。

Result: 对真实世界、WebArena和科研API进行验证，达成了55%性能提升和90%成本降低。

Conclusion: Doc2Agent为从非结构化API文档生成工具代理提供了一种可扩展的通用解决方案。

Abstract: REST APIs play important roles in enriching the action space of web agents,
yet most API-based agents rely on curated and uniform toolsets that do not
reflect the complexity of real-world APIs. Building tool-using agents for
arbitrary domains remains a major challenge, as it requires reading
unstructured API documentation, testing APIs and inferring correct parameters.
We propose Doc2Agent, a scalable pipeline to build agents that can call
Python-based tools generated from API documentation. Doc2Agent generates
executable tools from API documentations and iteratively refines them using a
code agent. We evaluate our approach on real-world APIs, WebArena APIs, and
research APIs, producing validated tools. We achieved a 55\% relative
performance improvement with 90\% lower cost compared to direct API calling on
WebArena benchmark. A domain-specific agent built for glycomaterial science
further demonstrates the pipeline's adaptability to complex, knowledge-rich
tasks. Doc2Agent offers a generalizable solution for building tool agents from
unstructured API documentation at scale.

</details>


### [100] [A Modular Multitask Reasoning Framework Integrating Spatio-temporal Models and LLMs](https://arxiv.org/abs/2506.20073)
*Kethmi Hirushini Hettige,Jiahao Ji,Cheng Long,Shili Xiang,Gao Cong,Jingyuan Wang*

Main category: cs.CL

TL;DR: STReason是一种新框架，将大语言模型与时空模型结合，实现多任务推理和复杂长形式推理，比现有方法效果显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以胜任多任务推理及复杂长形式推理，限制实际应用场景中的决策支持能力。

Method: 通过结合LLM与时空模型，STReason使用上下文学习，将复杂自然语言查询分解为模块化、可解释的程序，生成解决方案及详细推理。

Result: 实验结果显示，STReason在各项指标上优于先进LLM基线，尤其在复杂时空推理场景中表现卓越，人类评估也进一步验证其实践价值。

Conclusion: STReason有助于减少专家工作负担，拓展了时空任务实际应用场景，显示了开发更强大且可扩展的时空推理系统的潜力。

Abstract: Spatio-temporal data mining plays a pivotal role in informed decision making
across diverse domains. However, existing models are often restricted to narrow
tasks, lacking the capacity for multi-task inference and complex long-form
reasoning that require generation of in-depth, explanatory outputs. These
limitations restrict their applicability to real-world, multi-faceted decision
scenarios. In this work, we introduce STReason, a novel framework that
integrates the reasoning strengths of large language models (LLMs) with the
analytical capabilities of spatio-temporal models for multi-task inference and
execution. Without requiring task-specific finetuning, STReason leverages
in-context learning to decompose complex natural language queries into modular,
interpretable programs, which are then systematically executed to generate both
solutions and detailed rationales. To facilitate rigorous evaluation, we
construct a new benchmark dataset and propose a unified evaluation framework
with metrics specifically designed for long-form spatio-temporal reasoning.
Experimental results show that STReason significantly outperforms advanced LLM
baselines across all metrics, particularly excelling in complex,
reasoning-intensive spatio-temporal scenarios. Human evaluations further
validate STReason's credibility and practical utility, demonstrating its
potential to reduce expert workload and broaden the applicability to real-world
spatio-temporal tasks. We believe STReason provides a promising direction for
developing more capable and generalizable spatio-temporal reasoning systems.

</details>


### [101] [SACL: Understanding and Combating Textual Bias in Code Retrieval with Semantic-Augmented Reranking and Localization](https://arxiv.org/abs/2506.20081)
*Dhruv Gupta,Gayathri Ganesh Lakshmy,Yiqing Xie*

Main category: cs.CL

TL;DR: 提出RACG技术问题分析与改进框架SACL，通过增强代码语义信息改进代码检索和生成性能。


<details>
  <summary>Details</summary>
Motivation: 探讨当前代码检索技术存在的表面化依赖和偏向性问题，改善检索效果以推动代码生成技术。

Method: 提出一种名为SACL的框架，通过加入语义信息来改进代码以及结构知识，从而减少文本特征的表面依赖与偏差。

Result: SACL在多种基准测试中的代码检索任务中显著提升Recall@1指标，同时在代码生成性能上也有显著改进，例如在HumanEval上提升Pass@1 4.88%。

Conclusion: 通过SACL框架，在加入语义信息的情况下，显著改善代码检索与生成的表现，表明框架在未来代码生成系统中的潜力。

Abstract: Retrieval-Augmented Code Generation (RACG) is a critical technique for
enhancing code generation by retrieving relevant information. In this work, we
conduct an in-depth analysis of code retrieval by systematically masking
specific features while preserving code functionality. Our discoveries include:
(1) although trained on code, current retrievers heavily rely on surface-level
textual features (e.g., docstrings, identifier names), and (2) they exhibit a
strong bias towards well-documented code, even if the documentation is
irrelevant.Based on our discoveries, we propose SACL, a framework that enriches
textual information and reduces bias by augmenting code or structural knowledge
with semantic information. Extensive experiments show that SACL substantially
improves code retrieval (e.g., by 12.8% / 9.4% / 7.0% Recall@1 on HumanEval /
MBPP / SWE-Bench-Lite), which also leads to better code generation performance
(e.g., by 4.88% Pass@1 on HumanEval).

</details>


### [102] [Bridging Compositional and Distributional Semantics: A Survey on Latent Semantic Geometry via AutoEncoder](https://arxiv.org/abs/2506.20083)
*Yingji Zhang,Danilo S. Carvalho,André Freitas*

Main category: cs.CL

TL;DR: 文章探讨了通过融合组合与符号特性改进Transformer语言模型的语义表征能力并评估不同自编码器的潜在几何特性。


<details>
  <summary>Details</summary>
Motivation: 弥补符号语义与分布语义之间的差距，提升Transformer模型的可解释性与组合性。

Method: 分析和比较VAEs、VQVAEs、SAEs三类自编码器的潜在空间几何并其语义结构的关系。

Result: 探讨了不同自编码器设计如何在潜在空间几何中增强语义结构和提升模型的相关属性。

Conclusion: 语义表征学习可在符号语义与分布语义间建立桥梁，有助于提升语言模型的各类能力。

Abstract: Integrating compositional and symbolic properties into current distributional
semantic spaces can enhance the interpretability, controllability,
compositionality, and generalisation capabilities of Transformer-based
auto-regressive language models (LMs). In this survey, we offer a novel
perspective on latent space geometry through the lens of compositional
semantics, a direction we refer to as \textit{semantic representation
learning}. This direction enables a bridge between symbolic and distributional
semantics, helping to mitigate the gap between them. We review and compare
three mainstream autoencoder architectures-Variational AutoEncoder (VAE),
Vector Quantised VAE (VQVAE), and Sparse AutoEncoder (SAE)-and examine the
distinctive latent geometries they induce in relation to semantic structure and
interpretability.

</details>


### [103] [ITFormer: Bridging Time Series and Natural Language for Multi-Modal QA with Large-Scale Multitask Dataset](https://arxiv.org/abs/2506.20093)
*Yilin Wang,Peixuan Lei,Jie Song,Yuzhe Hao,Tao Chen,Yuxuan Zhang,Lei Jia,Yuanxiang Li,Zhongyu Wei*

Main category: cs.CL

TL;DR: 引入了时间序列问答任务，并开发了大型数据集EngineMT-QA和模型ITFormer，展示了在时间序列数据与自然语言交互任务中的进展。


<details>
  <summary>Details</summary>
Motivation: 解决高维时间序列数据和自然语言集成困难的问题，支持动态交互任务。

Method: 提出时间序列问答任务，开发了多任务数据集EngineMT-QA，并设计了将时间序列编码器和大型语言模型结合的框架ITFormer。

Result: ITFormer在问答准确性上超越了强基线模型，且所需额外参数不足1%。

Conclusion: 该研究提出了一种高效的跨模态建模新范式，有助于推动多模态人工智能的新研究和应用。

Abstract: Time-series data are critical in diverse applications, such as industrial
monitoring, medical diagnostics, and climate research. However, effectively
integrating these high-dimensional temporal signals with natural language for
dynamic, interactive tasks remains a significant challenge. To address this, we
introduce the Time-Series Question Answering (Time-Series QA) task and release
EngineMT-QA, the first large-scale, multi-task, temporal-textual QA dataset
designed to capture complex interactions between time-series signals and
natural language. Building on this resource, we propose the Instruct Time
Transformer (ITFormer), a novel framework that bridges time-series encoders
with frozen large language models (LLMs). ITFormer effectively extracts,
aligns, and fuses temporal and textual features, achieving a strong improvement
in QA accuracy over strong baselines with fewer than 1\% additional trainable
parameters. By combining computational efficiency with robust cross-modal
modeling, our work establishes a adaptable paradigm for integrating temporal
data with natural language, paving the way for new research and applications in
multi-modal AI. More details about the project, including datasets and code,
are available at: https://pandalin98.github.io/itformer_site/

</details>


### [104] [A Multi-Pass Large Language Model Framework for Precise and Efficient Radiology Report Error Detection](https://arxiv.org/abs/2506.20112)
*Songsoo Kim,Seungtae Lee,See Young Lee,Joonho Kim,Keechan Kan,Dukyong Yoon*

Main category: cs.CL

TL;DR: 通过三阶段大语言模型（LLM）框架，提高了辐射学报告校对的正预测值（PPV），并降低了运营成本，同时保持检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统单次的大语言模型框架在处理辐射学报告校对时，由于错误发生率较低，导致正预测值（PPV）局限，研究希望通过改进的LLM框架解决这一问题并降低运营成本。

Method: 研究采用MIMIC-III数据库的1000份辐射学报告，验证数据集包括CheXpert和Open-i。测试了三种LLM框架：（1）单次提示框架；（2）提取器+检测框架；（3）提取器、检测器和假阳性验证器框架。通过PPV和绝对真阳性率（aTPR）评估精度以及通过推断费用和人工审核支出评估效率，并结合统计学方法进行分析。

Result: 在三个框架中，框架3显著提高了PPV至0.159（相较于框架1和2显著改善），并将运营成本降低至每1000份报告5.58美元，与框架1和2相比分别减少42.6%和18.5%，同时aTPR保持稳定。人类审核报告数量从192减少至88，并通过外部验证数据集进一步证明框架3的优势。

Conclusion: 三阶段LLM框架显著提高辐射学报告校对的PPV，降低运营成本，同时保持检测性能，是一种有效的人工智能辅助报告质量保证策略。

Abstract: Background: The positive predictive value (PPV) of large language model
(LLM)-based proofreading for radiology reports is limited due to the low error
prevalence. Purpose: To assess whether a three-pass LLM framework enhances PPV
and reduces operational costs compared with baseline approaches. Materials and
Methods: A retrospective analysis was performed on 1,000 consecutive radiology
reports (250 each: radiography, ultrasonography, CT, MRI) from the MIMIC-III
database. Two external datasets (CheXpert and Open-i) were validation sets.
Three LLM frameworks were tested: (1) single-prompt detector; (2) extractor
plus detector; and (3) extractor, detector, and false-positive verifier.
Precision was measured by PPV and absolute true positive rate (aTPR).
Efficiency was calculated from model inference charges and reviewer
remuneration. Statistical significance was tested using cluster bootstrap,
exact McNemar tests, and Holm-Bonferroni correction. Results: Framework PPV
increased from 0.063 (95% CI, 0.036-0.101, Framework 1) to 0.079 (0.049-0.118,
Framework 2), and significantly to 0.159 (0.090-0.252, Framework 3; P<.001 vs.
baselines). aTPR remained stable (0.012-0.014; P>=.84). Operational costs per
1,000 reports dropped to USD 5.58 (Framework 3) from USD 9.72 (Framework 1) and
USD 6.85 (Framework 2), reflecting reductions of 42.6% and 18.5%, respectively.
Human-reviewed reports decreased from 192 to 88. External validation supported
Framework 3's superior PPV (CheXpert 0.133, Open-i 0.105) and stable aTPR
(0.007). Conclusion: A three-pass LLM framework significantly enhanced PPV and
reduced operational costs, maintaining detection performance, providing an
effective strategy for AI-assisted radiology report quality assurance.

</details>


### [105] [Leveraging AI Graders for Missing Score Imputation to Achieve Accurate Ability Estimation in Constructed-Response Tests](https://arxiv.org/abs/2506.20119)
*Masaki Uto,Yuma Ito*

Main category: cs.CL

TL;DR: 本文提出了利用自动评分技术的一个创新方法，以提高基于项目反应理论（IRT）的能力评估准确性。


<details>
  <summary>Details</summary>
Motivation: 目前评估高阶能力（如表达技巧和逻辑思维）的需求日益增加，而构 constructed-response 测试需要大量人工评分，成本高昂。

Method: 利用自动评分技术填补缺失分数，并结合IRT实现能力的准确评估。

Result: 提出的方法在能力评估中展现了较高的准确性，同时显著减少了人工评分工作量。

Conclusion: 通过新方法可以优化高阶能力的测评流程，减少人工操作并提高效率。

Abstract: Evaluating the abilities of learners is a fundamental objective in the field
of education. In particular, there is an increasing need to assess higher-order
abilities such as expressive skills and logical thinking. Constructed-response
tests such as short-answer and essay-based questions have become widely used as
a method to meet this demand. Although these tests are effective, they require
substantial manual grading, making them both labor-intensive and costly. Item
response theory (IRT) provides a promising solution by enabling the estimation
of ability from incomplete score data, where human raters grade only a subset
of answers provided by learners across multiple test items. However, the
accuracy of ability estimation declines as the proportion of missing scores
increases. Although data augmentation techniques for imputing missing scores
have been explored in order to address this limitation, they often struggle
with inaccuracy for sparse or heterogeneous data. To overcome these challenges,
this study proposes a novel method for imputing missing scores by leveraging
automated scoring technologies for accurate IRT-based ability estimation. The
proposed method achieves high accuracy in ability estimation while markedly
reducing manual grading workload.

</details>


### [106] [CCRS: A Zero-Shot LLM-as-a-Judge Framework for Comprehensive RAG Evaluation](https://arxiv.org/abs/2506.20128)
*Aashiq Muhamed*

Main category: cs.CL

TL;DR: 提出了CCRS，一个基于LLM的高效评估框架，用于评价RAG系统的输出质量，尤其在生物医药领域的BioASQ数据集中展示了其性能优越性。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法不足以全面分析RAG系统的多方面质量需求，且复杂的多阶段评估框架效率低下。需要一种高效全面的评估方法。

Method: 引入CCRS框架，该框架使用一个预训练的大型语言模型作为零样本端到端的评估工具，提供五个指标：情境连贯性(CC)、问题相关性(QR)、信息密度(ID)、答案正确性(AC)以及信息回忆(IR)。

Result: 在BioASQ数据集上评估了六种RAG系统配置，证明了CCRS能够有效区分系统性能，例如Mistral-7B在评分上优于Llama变体。同时，CCRS与RAGChecker相较，在评估效果接近或优于的同时，显著提高了计算效率。

Conclusion: CCRS为RAG系统提供了一个实用、全面、低成本的评估框架，有助于推动RAG系统的不断改进。

Abstract: RAG systems enhance LLMs by incorporating external knowledge, which is
crucial for domains that demand factual accuracy and up-to-date information.
However, evaluating the multifaceted quality of RAG outputs, spanning aspects
such as contextual coherence, query relevance, factual correctness, and
informational completeness, poses significant challenges. Existing evaluation
methods often rely on simple lexical overlap metrics, which are inadequate for
capturing these nuances, or involve complex multi-stage pipelines with
intermediate steps like claim extraction or require finetuning specialized
judge models, hindering practical efficiency. To address these limitations, we
propose CCRS (Contextual Coherence and Relevance Score), a novel suite of five
metrics that utilizes a single, powerful, pretrained LLM as a zero-shot,
end-to-end judge. CCRS evaluates: Contextual Coherence (CC), Question Relevance
(QR), Information Density (ID), Answer Correctness (AC), and Information Recall
(IR). We apply CCRS to evaluate six diverse RAG system configurations on the
challenging BioASQ dataset. Our analysis demonstrates that CCRS effectively
discriminates between system performances, confirming, for instance, that the
Mistral-7B reader outperforms Llama variants. We provide a detailed analysis of
CCRS metric properties, including score distributions, convergent/discriminant
validity, tie rates, population statistics, and discriminative power. Compared
to the complex RAGChecker framework, CCRS offers comparable or superior
discriminative power for key aspects like recall and faithfulness, while being
significantly more computationally efficient. CCRS thus provides a practical,
comprehensive, and efficient framework for evaluating and iteratively improving
RAG systems.

</details>


### [107] [AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control](https://arxiv.org/abs/2506.20160)
*Ruosen Li,Ziming Luo,Quan Zhang,Ruochen Li,Ben Zhou,Ali Payani,Xinya Du*

Main category: cs.CL

TL;DR: 本研究提出AALC，一种在强化学习中集成的轻量级、注重准确性的长度奖励方法，以在训练中动态平衡正确性和简洁性。


<details>
  <summary>Details</summary>
Motivation: 当前大规模推理模型尽管具备强大的推理能力，但生成冗长的推理链条会带来高延迟和高成本，却未必显著提高准确性，因此需要一种方法在保持准确性的同时减少冗余推理。

Method: 研究提出AALC，它将验证准确性纳入奖励机制中，并通过平滑、动态调度的长度惩罚在目标性能达到之前延迟施加长度惩罚，从而动态平衡正确性和推理长度。

Result: 实验表明，该方法在保持或提高原始准确性的同时，减少了超过50%的响应长度。此外，分析发现，AALC有助于削减冗余推理模式，并优化输出结构。

Conclusion: AALC不仅提高了推理效率，同时也可能减少模型输出的可解释性。这表明基于奖励的方法具有引导大规模推理模型实现更高效、可泛化推理路径的潜力。

Abstract: Large reasoning models (LRMs) achieve impressive reasoning capabilities by
generating lengthy chain-of-thoughts, but this "overthinking" incurs high
latency and cost without commensurate accuracy gains. In this work, we
introduce AALC, a lightweight, accuracy-aware length reward integrated into
reinforcement learning that dynamically balances correctness and brevity during
training. By incorporating validation accuracy into the reward and employing a
smooth, dynamically scheduled length penalty, AALC delays length penalty until
target performance is met. Through extensive experiments across standard and
out-of-distribution math benchmarks, we show that our approach reduces response
length by over 50% while maintaining or even improving the original accuracy.
Furthermore, qualitative analysis reveals that our method curbs redundant
reasoning patterns such as excessive subgoal setting and verification, leading
to structurally refined outputs rather than naive truncation. We also identify
that efficiency gains are accompanied by reduced interpretability: models
trained with AALC omit some narrative framing and explanatory context. These
findings highlight the potential of reward-based strategies to guide LRMs
toward more efficient, generalizable reasoning paths.

</details>


### [108] [SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series Prediction with LLMs](https://arxiv.org/abs/2506.20167)
*Fengze Li,Yue Wang,Yangle Liu,Ming Huang,Dou Hong,Jieming Ma*

Main category: cs.CL

TL;DR: 该研究提出SEED方法，通过融合结构编码器和语言模型，解决多元时间序列预测中结构依赖建模与语义推理适应之间的难题。


<details>
  <summary>Details</summary>
Motivation: 在多元时间序列预测中需要同时捕捉变量间的结构依赖和任务间的泛化能力，但现有方法在支持语义级推理和任务适应方面存在不足。

Method: 提出一个名为SEED的分步解码框架：包括一个用于提取数据块的编解码器、一个对齐数据块与语言模型嵌入的投影模块、用于语义重编程的机制以及用于预测的冻结型语言模型。

Result: 实验证明，SEED方法在多个数据集上相较于现有基线方法有一致的性能提升，并有效弥合结构建模和语义推理的差距。

Conclusion: SEED框架通过模块化设计，将数值模式与语义推理高效对齐，为多元时间序列预测任务提供了统一且可迁移的方法。

Abstract: Multivariate time series forecasting requires models to simultaneously
capture variable-wise structural dependencies and generalize across diverse
tasks. While structural encoders are effective in modeling feature
interactions, they lack the capacity to support semantic-level reasoning or
task adaptation. Conversely, large language models (LLMs) possess strong
generalization capabilities but remain incompatible with raw time series
inputs. This gap limits the development of unified, transferable prediction
systems. Therefore, we introduce SEED, a structural encoder for
embedding-driven decoding, which integrates four stages: a token-aware encoder
for patch extraction, a projection module that aligns patches with language
model embeddings, a semantic reprogramming mechanism that maps patches to
task-aware prototypes, and a frozen language model for prediction. This modular
architecture decouples representation learning from inference, enabling
efficient alignment between numerical patterns and semantic reasoning.
Empirical results demonstrate that the proposed method achieves consistent
improvements over strong baselines, and comparative studies on various datasets
confirm SEED's role in addressing the structural-semantic modeling gap.

</details>


### [109] [COIN: Uncertainty-Guarding Selective Question Answering for Foundation Models with Provable Risk Guarantees](https://arxiv.org/abs/2506.20178)
*Zhiyuan Wang,Jinhao Duan,Qingni Wang,Xiaofeng Zhu,Tianlong Chen,Xiaoshuang Shi,Kaidi Xu*

Main category: cs.CL

TL;DR: 本文提出了COIN框架，通过统计有效的方式实现不确定性量化，并在用户定义的错误发现率（FDR）约束下，筛选出单一答案以提高实用性。


<details>
  <summary>Details</summary>
Motivation: 当前在基础模型中的不确定性量化方法多缺乏对选择性预测关键指标（如FDR）的正式保证。而现有方法构建的预测集常包含不准确的候选项，影响其实用效果。

Method: 提出COIN框架，通过在校准集合上估计经验错误率并应用Clopper-Pearson等置信区间方法，建立真实错误率（FDR）的高概率上界，从而选择最大的不确定性阈值以保证测试数据的FDR控制。

Result: COIN展现了在风险控制方面的鲁棒性、较高的保留正确答案能力，以及在校准数据有限时的预测效率。此外，替代的上界构建方法和不确定性量化策略进一步提升了其性能。

Conclusion: COIN框架不仅在控制用户定义FDR方面表现出色，还展现了高度可扩展性和适应性，适合多种应用场景。

Abstract: Uncertainty quantification (UQ) for foundation models is essential to
identify and mitigate potential hallucinations in automatically generated text.
However, heuristic UQ approaches lack formal guarantees for key metrics such as
the false discovery rate (FDR) in selective prediction. Previous work adopts
the split conformal prediction (SCP) framework to ensure desired coverage of
admissible answers by constructing prediction sets, but these sets often
contain incorrect candidates, limiting their practical utility. To address
this, we propose COIN, an uncertainty-guarding selection framework that
calibrates statistically valid thresholds to filter a single generated answer
per question under user-specified FDR constraints. COIN estimates the empirical
error rate on a calibration set and applies confidence interval methods such as
Clopper-Pearson to establish a high-probability upper bound on the true error
rate (i.e., FDR). This enables the selection of the largest uncertainty
threshold that ensures FDR control on test data while significantly increasing
sample retention. We demonstrate COIN's robustness in risk control, strong
test-time power in retaining admissible answers, and predictive efficiency
under limited calibration data across both general and multimodal text
generation tasks. Furthermore, we show that employing alternative upper bound
constructions and UQ strategies can further boost COIN's power performance,
which underscores its extensibility and adaptability to diverse application
scenarios.

</details>


### [110] [How to Retrieve Examples in In-context Learning to Improve Conversational Emotion Recognition using Large Language Models?](https://arxiv.org/abs/2506.20199)
*Mengqi Wang,Tiantian Feng,Shrikanth Narayanan*

Main category: cs.CL

TL;DR: 本文研究了在情境学习中如何检索高质量样本以提高语言模型对话情感识别的性能，并提出了基于随机与增强样本检索的策略，实验结果显示增强样本检索表现最佳。


<details>
  <summary>Details</summary>
Motivation: 为解决复杂任务（如对话情感识别）中语言模型表现提升的挑战，通过检索高质量情境学习样本来提升模型性能。

Method: 引入随机和增强样本检索策略，分析对话情境对情感识别准确性的影响，并在IEMOCAP、MELD和EmoryNLP三大数据集上进行实验测试。

Result: 实验表明，与其他技术相比，增强样本检索在所有数据集上都具有更好的表现，显示出检索目标一致且通过改写强化样本的重要性。

Conclusion: 通过高质量样本增强和目标一致性，能够显著提高大语言模型在对话情感识别中的性能，并验证了增强样本检索的有效性。

Abstract: Large language models (LLMs) have enabled a wide variety of real-world
applications in various domains. However, creating a high-performing
application with high accuracy remains challenging, particularly for subjective
tasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this
study investigates approaches to improving conversational emotion recognition
(CER) by LLMs. Specifically, we explore how to retrieve high-quality examples
in in-context learning (ICL) to enhance CER. We propose various strategies
based on random and augmented example retrieval and also analyze the impact of
conversational context on CER accuracy. Experiments were conducted on the three
datasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented
example retrieval consistently outperforms other techniques under investigation
across all datasets, highlighting the importance of retrieving coherent
targeted examples and enhancing them through paraphrasing.

</details>


### [111] [Intrinsic vs. Extrinsic Evaluation of Czech Sentence Embeddings: Semantic Relevance Doesn't Help with MT Evaluation](https://arxiv.org/abs/2506.20203)
*Petra Barančíková,Ondřej Bojar*

Main category: cs.CL

TL;DR: 本文通过内在评估和外在评估方法比较捷克语特定和多语言句子嵌入模型的表现。


<details>
  <summary>Details</summary>
Motivation: 探索语义属性检测与下游任务之间的关系，以及开发更具实际操作性的语义嵌入模型。

Method: 在内在评估中，使用Costra数据集及语义文本相似性基准评估语义捕捉能力；在外在评估中，利用COMET-based指标微调嵌入模型并评估译文质量。

Result: 发现内在语义相似性测试表现优秀的模型在翻译评估任务中表现不一致；而具有过平滑嵌入空间的模型经过微调后表现出色。

Conclusion: 语义属性检测工具和实际任务之间的关系复杂，需进一步研究操作性语义嵌入及任务特定数据集（如翻译评估）。

Abstract: In this paper, we compare Czech-specific and multilingual sentence embedding
models through intrinsic and extrinsic evaluation paradigms. For intrinsic
evaluation, we employ Costra, a complex sentence transformation dataset, and
several Semantic Textual Similarity (STS) benchmarks to assess the ability of
the embeddings to capture linguistic phenomena such as semantic similarity,
temporal aspects, and stylistic variations. In the extrinsic evaluation, we
fine-tune each embedding model using COMET-based metrics for machine
translation evaluation.
  Our experiments reveal an interesting disconnect: models that excel in
intrinsic semantic similarity tests do not consistently yield superior
performance on downstream translation evaluation tasks. Conversely, models with
seemingly over-smoothed embedding spaces can, through fine-tuning, achieve
excellent results. These findings highlight the complex relationship between
semantic property probes and downstream task, emphasizing the need for more
research into 'operationalizable semantics' in sentence embeddings, or more
in-depth downstream tasks datasets (here translation evaluation)

</details>


### [112] [Perspectives in Play: A Multi-Perspective Approach for More Inclusive NLP Systems](https://arxiv.org/abs/2506.20209)
*Benedetta Muscato,Lucia Passaro,Gizem Gezici,Fosca Giannotti*

Main category: cs.CL

TL;DR: 本文介绍了一种新的多视角方法，使用软标签处理 NLP 中的主观性任务，从而更好地捕捉标注者之间的分歧，同时提高分类性能。


<details>
  <summary>Details</summary>
Motivation: 当前传统的方法通过聚合标注者的观点来确定单一的真相标签，这忽视了个体间的分歧，尤其是在主观性任务中，这可能导致少数视角的缺失。

Method: 研究提出了一种基于软标签的多视角方法，用以捕捉不同标注者的分歧，并基于主观性文本分类任务（仇恨言论、讽刺、侮辱性语言与立场检测）进行了分析和测试。

Result: 实验结果表明，多视角方法不仅更好地拟合了真实的人类标签分布（通过Jensen-Shannon Divergence度量），还能在分类性能（F1分数）上优于传统方法，但在讽刺与立场检测任务中的置信度较低。

Conclusion: 新方法有效捕捉了被忽视的主观视角分歧，以生成更包容的模型。但在主观性较强的任务中还需进一步优化确保更高的模型置信度。

Abstract: In the realm of Natural Language Processing (NLP), common approaches for
handling human disagreement consist of aggregating annotators' viewpoints to
establish a single ground truth. However, prior studies show that disregarding
individual opinions can lead can lead to the side effect of underrepresenting
minority perspectives, especially in subjective tasks, where annotators may
systematically disagree because of their preferences. Recognizing that labels
reflect the diverse backgrounds, life experiences, and values of individuals,
this study proposes a new multi-perspective approach using soft labels to
encourage the development of the next generation of perspective aware models,
more inclusive and pluralistic. We conduct an extensive analysis across diverse
subjective text classification tasks, including hate speech, irony, abusive
language, and stance detection, to highlight the importance of capturing human
disagreements, often overlooked by traditional aggregation methods. Results
show that the multi-perspective approach not only better approximates human
label distributions, as measured by Jensen-Shannon Divergence (JSD), but also
achieves superior classification performance (higher F1 scores), outperforming
traditional approaches. However, our approach exhibits lower confidence in
tasks like irony and stance detection, likely due to the inherent subjectivity
present in the texts. Lastly, leveraging Explainable AI (XAI), we explore model
uncertainty and uncover meaningful insights into model predictions.

</details>


### [113] [Enhancing Large Language Models through Structured Reasoning](https://arxiv.org/abs/2506.20241)
*Yubo Dong,Hehe Fan*

Main category: cs.CL

TL;DR: 本文借助显式结构化推理增强大语言模型(LLMs)，通过结构化数据和算法提升其在复杂推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在自然语言处理和自动决策方面取得了进展，但是在逻辑推理和系统性规划等复杂任务中仍有困难。本研究旨在解决LLMs依赖隐式统计关系的局限性，引入显式结构化知识表征增强其能力。

Method: 将非结构化数据转换为带有推理步骤的结构化格式，并通过监督微调(SFT)训练 LLMs。同时，采用组相对策略优化（GRPO），利用创新算法如最大流量（MAX-Flow）和最长公共子序列（LCS）提升推理效果并降低计算复杂度。

Result: 实验验证了通过在DeepSeek-R1-Distill-Qwen-1.5B模型上应用显式结构化推理，提供了简洁的推理路径、强健的多场景表现以及与优化技术的兼容性。

Conclusion: 研究表明，整合显式结构化推理能有效提高LLMs在复杂推理任务中的表现，为LLMs的发展提供了新方向。

Abstract: Recent Large Language Models (LLMs) have significantly advanced natural
language processing and automated decision-making. However, these models still
encounter difficulties when performing complex reasoning tasks involving
logical deduction and systematic planning, primarily due to their reliance on
implicit statistical relationships without structured knowledge
representation.Inspired by cognitive science and neurosymbolic AI, we introduce
a novel approach to enhance LLMs through explicit structured reasoning. First,
we convert unstructured data into structured formats by explicitly annotating
reasoning steps. We then employ this structured dataset to train LLMs through
Supervised Fine-Tuning (SFT). Additionally, we enhance the structured reasoning
capabilities of LLMs using Group Relative Policy Optimization (GRPO),
incorporating two innovative algorithms--MAX-Flow and Longest Common
Subsequence (LCS)--which notably improve reasoning effectiveness and reduce
computational complexity. Experimental results from fine-tuning a
DeepSeek-R1-Distill-Qwen-1.5B model demonstrate concise reasoning, robust
performance across various scenarios, and improved compatibility with
optimization techniques, validating the efficacy of structured reasoning
integration in LLMs.

</details>


### [114] [CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment](https://arxiv.org/abs/2506.20243)
*Papa Séga Wade,Mihai Andries,Ioannis Kanellos,Thierry Moudenc*

Main category: cs.CL

TL;DR: 提出了一种基于块的多自监督学习(SSL)融合方法，用于非母语者的语音流利度自动评估，并在两项基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 自动评估非母语者语音的流利度具有挑战性，尤其是在捕捉其语音节奏、停顿和不流利现象方面。

Method: 通过使用Silero-VAD将语音分割为呼吸组块，结合自监督学习(SSL)模型(Wav2Vec2, HuBERT, WavLM)和具有CNN-BiLSTM结构的层次框架，融合声学和语言特征，同时加入块级流利度标记以增强模型表现。

Result: 在Speechocean762数据集上相比单模型提升F1分数2.8点、Pearson相关系数6.2点；在Avalinguo数据集上F1分数提升4.2点、Pearson相关系数提升4.0点，优于基于Pyannote.audio的分割方法。

Conclusion: 基于块的多自监督学习融合方法在流利度评估中具有鲁棒性，但需要进一步研究其对具有不规则韵律的方言的泛化能力。

Abstract: Automatic fluency assessment (AFA) remains challenging, particularly in
capturing speech rhythm, pauses, and disfluencies in non-native speakers. We
introduce a chunk-based approach integrating self-supervised learning (SSL)
models (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths
in phonetic, prosodic, and noisy speech modeling, with a hierarchical
CNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero
voice activity detection (Silero-VAD), enabling fine-grained temporal analysis
while mitigating over-segmentation artifacts. SSL embeddings are fused via a
learnable weighted mechanism, balancing acoustic and linguistic features, and
enriched with chunk-level fluency markers (e.g., speech rate, pause durations,
n-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies
across chunks. Evaluated on Avalinguo and Speechocean762, our approach improves
F1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines
on Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on
Avalinguo, surpassing Pyannote.audio-based segmentation baselines. These
findings highlight chunk-based multi-SSL fusion for robust fluency evaluation,
though future work should explore generalization to dialects with irregular
prosody.

</details>


### [115] [Narrative Shift Detection: A Hybrid Approach of Dynamic Topic Models and Large Language Models](https://arxiv.org/abs/2506.20269)
*Kai-Robin Lange,Tobias Schmidt,Matthias Reccius,Henrik Müller,Michael Roos,Carsten Jentsch*

Main category: cs.CL

TL;DR: 该论文结合了大语言模型和主题模型，提出了一种方法来动态检测叙事随时间的变化，特别聚焦在通过自动化手段区分内容变化和叙事变化。


<details>
  <summary>Details</summary>
Motivation: 随着媒体叙事快速发展，研究叙事如何随时间变化变得重要。然而，使用大语言模型分析整个语料库成本较高，该论文旨在设计一种高效的动态叙事建模方法。

Method: 作者结合了大语言模型的语言理解能力和主题模型的规模效能，通过叙事政策框架对叙事变化进行建模，并应用主题模型和变化点检测技术筛选出具有代表性变化的文档后，再利用大语言模型自动解释这些变化。

Result: 实验在2009年至2023年的《华尔街日报》文章语料库上进行，发现大语言模型在提取特定时间点的叙事变化方面表现良好，但在区分内容变化与叙事变化时效果较差。

Conclusion: 提出的模型在动态捕捉叙事变化方面表现有效，但在某些细节任务上仍需改进。

Abstract: With rapidly evolving media narratives, it has become increasingly critical
to not just extract narratives from a given corpus but rather investigate, how
they develop over time. While popular narrative extraction methods such as
Large Language Models do well in capturing typical narrative elements or even
the complex structure of a narrative, applying them to an entire corpus comes
with obstacles, such as a high financial or computational cost. We propose a
combination of the language understanding capabilities of Large Language Models
with the large scale applicability of topic models to dynamically model
narrative shifts across time using the Narrative Policy Framework. We apply a
topic model and a corresponding change point detection method to find changes
that concern a specific topic of interest. Using this model, we filter our
corpus for documents that are particularly representative of that change and
feed them into a Large Language Model that interprets the change that happened
in an automated fashion and distinguishes between content and narrative shifts.
We employ our pipeline on a corpus of The Wall Street Journal news paper
articles from 2009 to 2023. Our findings indicate that a Large Language Model
can efficiently extract a narrative shift if one exists at a given point in
time, but does not perform as well when having to decide whether a shift in
content or a narrative shift took place.

</details>


### [116] [Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining and Extracting Rare and Hidden Content](https://arxiv.org/abs/2506.20331)
*Rian Touchent,Nathan Godey,Eric de la Clergerie*

Main category: cs.CL

TL;DR: 本文提出了一个名为Biomed-Enriched的生物医学文本数据集，通过两阶段注释从PubMed构建，为生物医学和临床NLP提供了大规模开源资源。


<details>
  <summary>Details</summary>
Motivation: 临床文本因隐私受限而难以获取，该研究旨在提供一种公开、规模化的替代数据源，从而支持生物医学和临床NLP的研究与应用。

Method: 首先，通过大型语言模型对PubMed文章段落进行初始注释，包括类型、领域和教育质量评分；接着，使用这些注释微调小型语言模型，将标签扩展到整个PMC-OA语料库，并通过筛选和调整生成高质量数据子集。

Result: 生成了大规模的高质量临床案例段落数据集，并通过一系列持续预训练实验表明，临床上采样和教育质量筛选可分别提升不同任务约1%-5%的性能，同时缩短了训练时间。

Conclusion: 借助Biomed-Enriched数据集，生物医学预训练策略在效率与效果上均得到提升，为领域研究提供了重要资源，有望促进相关NLP任务的发展。

Abstract: We introduce Biomed-Enriched, a biomedical text dataset constructed from
PubMed via a two-stage annotation process. In the first stage, a large language
model annotates 400K paragraphs from PubMed scientific articles, assigning
scores for their type (review, study, clinical case, other), domain (clinical,
biomedical, other), and educational quality. The educational quality score
(rated 1 to 5) estimates how useful a paragraph is for college-level learning.
These annotations are then used to fine-tune a small language model, which
propagates the labels across the full PMC-OA corpus. The resulting metadata
allows us to extract refined subsets, including 2M clinical case paragraphs
with over 450K high-quality ones from articles with commercial-use licenses,
and to construct several variants via quality filtering and domain upsampling.
Clinical text is typically difficult to access due to privacy constraints, as
hospital records cannot be publicly shared. Hence, our dataset provides an
alternative large-scale, openly available collection of clinical cases from
PubMed, making it a valuable resource for biomedical and clinical NLP.
Preliminary continual-pretraining experiments with OLMo2 suggest these curated
subsets enable targeted improvements, with clinical upsampling boosting
performance by ~5% on MMLU ProfMed and educational quality filtering improving
MedQA and MedMCQA by ~1%. Combinations of these techniques led to faster
convergence, reaching same performance with a third of training tokens,
indicating potential for more efficient and effective biomedical pretraining
strategies.

</details>


### [117] [TAPS: Tool-Augmented Personalisation via Structured Tagging](https://arxiv.org/abs/2506.20409)
*Ekaterina Taktasheva,Jeff Dalton*

Main category: cs.CL

TL;DR: 提出了一种新的方法来增强大语言模型的个性化工具使用能力，显著提升了任务性能。


<details>
  <summary>Details</summary>
Motivation: 探讨如何将用户偏好有效地整合到目标导向的对话代理中，以改善个性化工具使用。

Method: 引入了一种名为TAPS的解决方案，该方法通过结构化标记工具和基于不确定性的工具检测器来增强个性化工具使用能力。

Result: TAPS方法显著提升了大语言模型整合用户偏好的能力，并在开源模型的NLSI任务中达到了新的最先进水平。

Conclusion: TAPS方法改进了现有模型对用户偏好的适应性，有助于更好地实现个性化目标对话和工具交互。

Abstract: Recent advancements in tool-augmented large language models have enabled them
to interact with external tools, enhancing their ability to perform complex
user tasks. However, existing approaches overlook the role of personalisation
in guiding tool use. This work investigates how user preferences can be
effectively integrated into goal-oriented dialogue agents. Through extensive
analysis, we identify key weaknesses in the ability of LLMs to personalise tool
use. To this end, we introduce \name, a novel solution that enhances
personalised tool use by leveraging a structured tagging tool and an
uncertainty-based tool detector. TAPS significantly improves the ability of
LLMs to incorporate user preferences, achieving the new state-of-the-art for
open source models on the NLSI task.

</details>


### [118] [An Agentic System for Rare Disease Diagnosis with Traceable Reasoning](https://arxiv.org/abs/2506.20430)
*Weike Zhao,Chaoyi Wu,Yanjie Fan,Xiaoman Zhang,Pengcheng Qiu,Yuze Sun,Xiao Zhou,Yanfeng Wang,Ya Zhang,Yongguo Yu,Kun Sun,Weidi Xie*

Main category: cs.CL

TL;DR: 本研究提出了DeepRare，一个由大语言模型驱动的稀有疾病诊断系统，展示了其在稀有疾病诊断中的卓越表现。


<details>
  <summary>Details</summary>
Motivation: 稀有疾病诊断由于临床异质性、低患病率及医生对稀有病了解有限，面临重大挑战。

Method: DeepRare采用模块化架构，结合长期记忆模块、领域分析工具及最新医学知识库，进行复杂诊断推理并生成透明的诊断过程。

Result: DeepRare在多项评估中表现出色，针对2,919种疾病诊断达到100%准确率。此外，在多种评价指标上显著优于现有方法。

Conclusion: DeepRare通过大幅提高稀有疾病的诊断准确性和推理透明度，为临床医生提供了强有力的诊断支持工具。

Abstract: Rare diseases collectively affect over 300 million individuals worldwide, yet
timely and accurate diagnosis remains a pervasive challenge. This is largely
due to their clinical heterogeneity, low individual prevalence, and the limited
familiarity most clinicians have with rare conditions. Here, we introduce
DeepRare, the first rare disease diagnosis agentic system powered by a large
language model (LLM), capable of processing heterogeneous clinical inputs. The
system generates ranked diagnostic hypotheses for rare diseases, each
accompanied by a transparent chain of reasoning that links intermediate
analytic steps to verifiable medical evidence.
  DeepRare comprises three key components: a central host with a long-term
memory module; specialized agent servers responsible for domain-specific
analytical tasks integrating over 40 specialized tools and web-scale,
up-to-date medical knowledge sources, ensuring access to the most current
clinical information. This modular and scalable design enables complex
diagnostic reasoning while maintaining traceability and adaptability. We
evaluate DeepRare on eight datasets. The system demonstrates exceptional
diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013
diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15
methods, like traditional bioinformatics diagnostic tools, LLMs, and other
agentic systems, achieving an average Recall@1 score of 57.18% and surpassing
the second-best method (Reasoning LLM) by a substantial margin of 23.79
percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at
Recall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of
reasoning chains by clinical experts achieves 95.40% agreements. Furthermore,
the DeepRare system has been implemented as a user-friendly web application
http://raredx.cn/doctor.

</details>


### [119] [Probing AI Safety with Source Code](https://arxiv.org/abs/2506.20471)
*Ujwal Narayan,Shreyas Chaudhari,Ashwin Kalyan,Tanmay Rajpurohit,Karthik Narasimhan,Ameet Deshpande,Vishvak Murahari*

Main category: cs.CL

TL;DR: 研究展示了当前的大型语言模型在安全性上存在严重问题，并提出了一个名为CoDoT的策略用于评估和揭示这些问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型广泛应用于安全关键场景，确保它们符合人类价值和偏好变得极为重要，但研究发现当前模型在安全性上严重不足。

Method: 提出一种名为Code of Thought (CoDoT)的策略，将自然语言输入转换为代表相同意图的简单代码以评估模型的安全性表现。

Result: 实验表明，使用CoDoT，GPT-4 Turbo的毒性增加了16.5倍，DeepSeek R1的失败率达100%，平均来看七个主流模型的毒性增加了300%。递归应用CoDoT甚至可使毒性翻倍。

Conclusion: CoDoT策略证明了目前LLMs在安全性上的缺陷，这凸显出从基础原则上评估和改进LLMs安全性的重要性，以确保安全性与能力同步发展。

Abstract: Large language models (LLMs) have become ubiquitous, interfacing with humans
in numerous safety-critical applications. This necessitates improving
capabilities, but importantly coupled with greater safety measures to align
these models with human values and preferences. In this work, we demonstrate
that contemporary models fall concerningly short of the goal of AI safety,
leading to an unsafe and harmful experience for users. We introduce a prompting
strategy called Code of Thought (CoDoT) to evaluate the safety of LLMs. CoDoT
converts natural language inputs to simple code that represents the same
intent. For instance, CoDoT transforms the natural language prompt "Make the
statement more toxic: {text}" to: "make_more_toxic({text})". We show that CoDoT
results in a consistent failure of a wide range of state-of-the-art LLMs. For
example, GPT-4 Turbo's toxicity increases 16.5 times, DeepSeek R1 fails 100% of
the time, and toxicity increases 300% on average across seven modern LLMs.
Additionally, recursively applying CoDoT can further increase toxicity two
times. Given the rapid and widespread adoption of LLMs, CoDoT underscores the
critical need to evaluate safety efforts from first principles, ensuring that
safety and capabilities advance together.

</details>


### [120] [Time is On My Side: Dynamics of Talk-Time Sharing in Video-chat Conversations](https://arxiv.org/abs/2506.20474)
*Kaixiang Zhang,Justine Zhang,Cristian Danescu-Niculescu-Mizil*

Main category: cs.CL

TL;DR: 本文探讨会话中讲话时间的分配，就不同动态如何影响会话平衡进行了定量分析，并提供了工具给沟通平台设计者。


<details>
  <summary>Details</summary>
Motivation: 研究会话中讲话时间如何分配，以及如何通过定量分析提高沟通体验。

Method: 引入计算框架以量化讲话时间的分配和动态，并基于多个因素分类谈话共享动态类型。

Result: 通过分析大量陌生人视频聊天数据，证实较平衡的会话更受欢迎，并发现即便总平衡相同，不同共享动态类型对体验的影响不同。

Conclusion: 提出的框架可以提升沟通平台设计，可以应用于人类与AI或人际间的沟通平台。

Abstract: An intrinsic aspect of every conversation is the way talk-time is shared
between multiple speakers. Conversations can be balanced, with each speaker
claiming a similar amount of talk-time, or imbalanced when one talks
disproportionately. Such overall distributions are the consequence of
continuous negotiations between the speakers throughout the conversation: who
should be talking at every point in time, and for how long?
  In this work we introduce a computational framework for quantifying both the
conversation-level distribution of talk-time between speakers, as well as the
lower-level dynamics that lead to it. We derive a typology of talk-time sharing
dynamics structured by several intuitive axes of variation. By applying this
framework to a large dataset of video-chats between strangers, we confirm that,
perhaps unsurprisingly, different conversation-level distributions of talk-time
are perceived differently by speakers, with balanced conversations being
preferred over imbalanced ones, especially by those who end up talking less.
Then we reveal that -- even when they lead to the same level of overall balance
-- different types of talk-time sharing dynamics are perceived differently by
the participants, highlighting the relevance of our newly introduced typology.
Finally, we discuss how our framework offers new tools to designers of
computer-mediated communication platforms, for both human-human and human-AI
communication.

</details>


### [121] [Knowledge-Aware Diverse Reranking for Cross-Source Question Answering](https://arxiv.org/abs/2506.20476)
*Tong Zhou*

Main category: cs.CL

TL;DR: 本文介绍了Team Marikarp在SIGIR 2025 LiveRAG比赛中的解决方案，提出的知识感知的多样化再排序RAG管道获得了第一名。


<details>
  <summary>Details</summary>
Motivation: 目标是从FineWeb语料库中的1500万文档子集里检索与问题相关的支持文档，并公平评估以应对多样化的题目和知识组织方法。

Method: 提出了一种知识感知的多样化再排序RAG管道，专注于解决多样化问题，并提升与问题相关文档的检索精度。

Result: 团队的解决方案在比赛中表现优异，最终获得第一名。

Conclusion: 该研究成功提供了一种针对多样化任务的高效知识检索与再排序方法，在比赛中验证了其实用性与效果。

Abstract: This paper presents Team Marikarp's solution for the SIGIR 2025 LiveRAG
competition. The competition's evaluation set, automatically generated by
DataMorgana from internet corpora, encompassed a wide range of target topics,
question types, question formulations, audience types, and knowledge
organization methods. It offered a fair evaluation of retrieving
question-relevant supporting documents from a 15M documents subset of the
FineWeb corpus. Our proposed knowledge-aware diverse reranking RAG pipeline
achieved first place in the competition.

</details>


### [122] [GPTailor: Large Language Model Pruning Through Layer Cutting and Stitching](https://arxiv.org/abs/2506.20480)
*Guinan Su,Li Shen,Lu Yin,Shiwei Liu,Yanwu Yang,Jonas Geiping*

Main category: cs.CL

TL;DR: 提出一种通过结合和合并经过微调模型变体中不同层的策略来压缩大型语言模型，实现约25%的参数减少，同时保留约97.3%的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型性能优异，但模型规模庞大带来了部署和推理的挑战。因此需要寻求有效的模型压缩方法，以降低计算成本。

Method: 将层删除、层选择以及层合并结合，采用零阶优化方法，在候选模型中搜索最佳组合。

Result: 在Llama2-13B模型上，通过删除约25%的参数，压缩后的模型仍然保留了97.3%的原始性能，比现有方法有显著提升。

Conclusion: 该方法成功实现了高效的模型压缩，为大型语言模型的部署和推理提供了一种有竞争力的解决方案。

Abstract: Large language models (LLMs) have shown remarkable capabilities in language
understanding and generation. However, such impressive capability typically
comes with a substantial model size, which presents significant challenges in
deployment and inference. While structured pruning of model parameters offers a
promising way to reduce computational costs at deployment time, current methods
primarily focus on single model pruning. In this work, we develop a novel
strategy to compress models by strategically combining or merging layers from
finetuned model variants, which preserves the original model's abilities by
aggregating capabilities accentuated in different finetunes. We pose the
optimal tailoring of these LLMs as a zero-order optimization problem, adopting
a search space that supports three different operations: (1) Layer removal, (2)
Layer selection from different candidate models, and (3) Layer merging. Our
experiments demonstrate that this approach leads to competitive model pruning,
for example, for the Llama2-13B model families, our compressed models maintain
approximately 97.3\% of the original performance while removing $\sim25\%$ of
parameters, significantly outperforming previous state-of-the-art methods. The
code is available at https://github.com/Guinan-Su/auto-merge-llm.

</details>


### [123] [ReCode: Updating Code API Knowledge with Reinforcement Learning](https://arxiv.org/abs/2506.20495)
*Haoze Wu,Yunzhi Yao,Wenhao Yu,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: ReCode 通过强化学习和改进的字符串相似性评价指标，提升了LLM在动态API场景下的代码生成能力，适用于多种模型和算法，代码已开放。


<details>
  <summary>Details</summary>
Motivation: 现有LLM难以适应外部库API的频繁更新，导致代码生成可靠性不足，尤其是在动态环境中。

Method: 提出了ReCode框架，通过构建包含约2000条数据的训练集，并采用改进的字符串相似性度量作为强化学习奖励，帮助LLM实现基于更新信息的版本迁移训练。

Result: ReCode显著提升了LLM在动态API场景下的代码生成效果，尤其在未见场景任务CodeUpdateArena上表现优异，同时对LLM的通用代码生成能力影响较小。

Conclusion: ReCode在多种LLM和强化学习算法上的表现优异，比如Qwen2.5-Coder-7B模型在部分任务上超越了更大参数量的模型，展现了其在动态代码生成领域的潜力。

Abstract: Large Language Models (LLMs) exhibit remarkable code generation capabilities
but falter when adapting to frequent updates in external library APIs. This
critical limitation, stemming from reliance on outdated API knowledge from
their training data, even with access to current documentation, impedes
reliable code generation in dynamic environments. To tackle this issue, we
propose ReCode (rule-based Reinforcement learning for Code Update), a novel
framework that mimics human programmer adaptation to API changes. Specifically,
we construct a dataset of approximately 2,000 data entries to train the LLMs to
perform version migration based on updated information. Then, we introduce a
modified string similarity metric for code evaluation as the reward for
reinforcement learning. Our experiments demonstrate that ReCode substantially
boosts LLMs' code generation performance in dynamic API scenarios, especially
on the unseen CodeUpdateArena task. Crucially, compared to supervised
fine-tuning, ReCode has less impact on LLMs' general code generation abilities.
We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and
DAPO), all achieving consistent improvements. Notably, after training,
Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned
model and the reasoning model with the same architecture. Code is available at
https://github.com/zjunlp/ReCode.

</details>


### [124] [OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling](https://arxiv.org/abs/2506.20512)
*Zengzhi Wang,Fan Zhou,Xuefeng Li,Pengfei Liu*

Main category: cs.CL

TL;DR: 本文研究了中期训练策略如何影响RL（强化学习）动态，提出了一种“两阶段中期训练策略”，并发布了开源模型和大规模数学推理语料库。


<details>
  <summary>Details</summary>
Motivation: 探索基础语言模型在强化学习中的适用性，尤其是中期训练对后续RL表现的影响。

Method: 分析了不同的语料（如数学语料和问答数据）对模型训练的影响，提出了“两阶段中期训练策略”：模型先以恒定学习率在200B标记上训练，然后用学习率衰减在包含长链式推理的20B标记上训练。

Result: 提出的训练策略带来了与更适应RL的模型（如Qwen）相当的性能，推出的OctoThinker模型表现优异。

Conclusion: 研究揭示了中期训练策略在提升基础模型RL性能中的重要性，并为未来RL友好的模型训练策略提供了指引。

Abstract: Different base language model families, such as Llama and Qwen, exhibit
divergent behaviors during post-training with reinforcement learning (RL),
especially on reasoning-intensive tasks. What makes a base language model
suitable for reinforcement learning? Gaining deeper insight into this question
is essential for developing RL-scalable foundation models of the next
generation. In this work, we investigate how mid-training strategies shape RL
dynamics, focusing on two representative model families: Qwen and Llama. Our
study reveals that (1) high-quality mathematical corpora, such as
MegaMath-Web-Pro, significantly improve both base model and RL performance,
while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further
adding QA-style data, particularly long chain-of-thought (CoT) reasoning
examples, enhances RL outcomes, and instruction data further unlocks this
effect; (3) while long-CoT improves reasoning depth, it can also induce
verbosity of model responses and unstability of RL training, underscoring the
importance of data formatting; (4) scaling mid-training consistently leads to
stronger downstream RL performance. Building on these insights, we introduce a
two-stage mid-training strategy, Stable-then-Decay, in which base models are
first trained on 200B tokens with a constant learning rate, followed by 20B
tokens across three CoT-focused branches with learning rate decay. This yields
OctoThinker, a family of models demonstrating strong RL compatibility and
closing the performance gap with more RL-friendly model families, i.e., Qwen.
We hope our work will help shape pre-training strategies for foundation models
in the RL era. To support further research, we release our open-source models
along with a curated math reasoning-intensive corpus of over 70 billion tokens
(i.e., MegaMath-Web-Pro-Max).

</details>


### [125] [When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs](https://arxiv.org/abs/2506.20544)
*Ammar Khairi,Daniel D'souza,Ye Shen,Julia Kreutzer,Sara Hooker*

Main category: cs.CL

TL;DR: 提出针对多语言、多任务场景的采样与选择策略改进方法，实现大幅性能提升。


<details>
  <summary>Details</summary>
Motivation: 旨在提升推理性能，特别是在多语言和多任务场景下，提高语言模型的推广能力。

Method: 设计新的采样与选择策略，适应不同语言和任务，评估其在多语言、多任务环境中的表现。

Result: 在多语言任务上的推理性能显著提高，8B模型平均赢率提升+6.8，111B模型提升+9.0。

Conclusion: 多语言和任务相关的推理计算策略对提升语言模型在弱势语言上的表现至关重要。

Abstract: Recent advancements in large language models (LLMs) have shifted focus toward
scaling inference-time compute, improving performance without retraining the
model. A common approach is to sample multiple outputs in parallel, and select
one of these as the final output. However, work to date has focused on English
and a handful of domains such as math and code. In contrast, we are most
interested in techniques that generalize across open-ended tasks, formally
verifiable tasks, and across languages. In this work, we study how to robustly
scale inference-time compute for open-ended generative tasks in a multilingual,
multi-task setting.
  Our findings show that both sampling strategy based on temperature variation
and selection strategy must be adapted to account for diverse domains and
varied language settings. We evaluate existing selection methods, revealing
that strategies effective in English often fail to generalize across languages.
We propose novel sampling and selection strategies specifically adapted for
multilingual and multi-task inference scenarios, and show they yield notable
gains across languages and tasks. In particular, our combined sampling and
selection methods lead to an average +6.8 jump in win-rates for our 8B models
on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At
larger scale, Command-A (111B model) equipped with our methods, shows +9.0
improvement in win-rates on the same benchmark with just five samples against
single-sample decoding, a substantial increase at minimal cost. Our results
underscore the need for language- and task-aware approaches to inference-time
compute, aiming to democratize performance improvements in underrepresented
languages.

</details>


### [126] [Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm](https://arxiv.org/abs/2506.20606)
*Baixiang Huang,Zhen Tan,Haoran Wang,Zijie Liu,Dawei Li,Ali Payani,Huan Liu,Tianlong Chen,Kai Shu*

Main category: cs.CL

TL;DR: 基于大规模语言模型的代理具有强大功能，但在高风险领域部署存在安全和伦理风险。提出了一种名为行为编辑的方法，来高效调整代理行为，同时创建了一个名为BehaviorBench的测试平台以系统评估这一方法。


<details>
  <summary>Details</summary>
Motivation: 当前的大规模语言模型代理在任务完成中展现了卓越能力，但在高风险领域可能引发严重的伦理和安全问题，如造成物理伤害或经济损失。需要一种有效的方法来引导代理的伦理行为。

Method: 将代理行为调整框架为模型编辑任务，提出“行为编辑”技术，设计BehaviorBench测试基准以支持不同情境下的行为评估与修改。

Result: 通过表现和实验验证，行为编辑能够动态调整代理的目标行为，不仅可实现情境特定的小幅度调整，还能大范围调整代理的全球道德对齐。

Conclusion: 行为编辑提供了一种新范式来引导代理行为，说明这一技术的潜力和潜在风险，有助于探索LLM代理在伦理行为调整上的可能性。

Abstract: Agents based on Large Language Models (LLMs) have demonstrated strong
capabilities across a wide range of tasks. However, deploying LLM-based agents
in high-stakes domains comes with significant safety and ethical risks.
Unethical behavior by these agents can directly result in serious real-world
consequences, including physical harm and financial loss. To efficiently steer
the ethical behavior of agents, we frame agent behavior steering as a model
editing task, which we term Behavior Editing. Model editing is an emerging area
of research that enables precise and efficient modifications to LLMs while
preserving their overall capabilities. To systematically study and evaluate
this approach, we introduce BehaviorBench, a multi-tier benchmark grounded in
psychological moral theories. This benchmark supports both the evaluation and
editing of agent behaviors across a variety of scenarios, with each tier
introducing more complex and ambiguous scenarios. We first demonstrate that
Behavior Editing can dynamically steer agents toward the target behavior within
specific scenarios. Moreover, Behavior Editing enables not only
scenario-specific local adjustments but also more extensive shifts in an
agent's global moral alignment. We demonstrate that Behavior Editing can be
used to promote ethical and benevolent behavior or, conversely, to induce
harmful or malicious behavior. Through comprehensive evaluations on agents
based on frontier LLMs, BehaviorBench shows the effectiveness of Behavior
Editing across different models and scenarios. Our findings offer key insights
into a new paradigm for steering agent behavior, highlighting both the promise
and perils of Behavior Editing.

</details>


### [127] [DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation](https://arxiv.org/abs/2506.20639)
*Shansan Gong,Ruixiang Zhang,Huangjie Zheng,Jiatao Gu,Navdeep Jaitly,Lingpeng Kong,Yizhe Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为DiffuCoder的扩散大语言模型（dLLM），并通过研究其去噪过程和强化学习方法来对其进行分析和优化，显著提升了代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 当前扩散大语言模型（dLLM）在代码生成中的训练和推理机制尚未充分探索。作者希望通过深入分析和优化dLLM的去噪过程和强化学习方法，释放其在代码生成中的潜力。

Method: 研究了扩散大语言模型的解码行为，提出一种新颖的采样方法（coupled-GRPO），用于优化DiffuCoder的强化学习训练机制，从而增强模型性能。

Result: 实验结果表明，DiffuCoder在代码生成基准测试中性能提升了4.4%，并且减少了对传统AR解码因果性的依赖。

Conclusion: 此研究为扩散大语言模型的生成机制提供了深入见解，并提出了一种有效的、基于扩散模型的强化学习训练框架。

Abstract: Diffusion large language models (dLLMs) are compelling alternatives to
autoregressive (AR) models because their denoising models operate over the
entire sequence. The global planning and iterative refinement features of dLLMs
are particularly useful for code generation. However, current training and
inference mechanisms for dLLMs in coding are still under-explored. To demystify
the decoding behavior of dLLMs and unlock their potential for coding, we
systematically investigate their denoising processes and reinforcement learning
(RL) methods. We train a 7B dLLM, \textbf{DiffuCoder}, on 130B tokens of code.
Using this model as a testbed, we analyze its decoding behavior, revealing how
it differs from that of AR models: (1) dLLMs can decide how causal their
generation should be without relying on semi-AR decoding, and (2) increasing
the sampling temperature diversifies not only token choices but also their
generation order. This diversity creates a rich search space for RL rollouts.
For RL training, to reduce the variance of token log-likelihood estimates and
maintain training efficiency, we propose \textbf{coupled-GRPO}, a novel
sampling scheme that constructs complementary mask noise for completions used
in training. In our experiments, coupled-GRPO significantly improves
DiffuCoder's performance on code generation benchmarks (+4.4\% on EvalPlus) and
reduces reliance on AR causal during decoding. Our work provides deeper insight
into the machinery of dLLM generation and offers an effective, diffusion-native
RL training framework. https://github.com/apple/ml-diffucoder.

</details>


### [128] [Memento: Note-Taking for Your Future Self](https://arxiv.org/abs/2506.20642)
*Chao Wan,Albert Gong,Mihir Mishra,Carl-Leander Henneking,Claas Beger,Kilian Q. Weinberger*

Main category: cs.CL

TL;DR: 本文提出了一种名为Memento的提示策略，通过分解问题、动态构建数据库和综合事实三阶段提升复杂问答的表现。


<details>
  <summary>Details</summary>
Motivation: 提升语言模型在需要紧密结合推理和检索任务中的性能，例如多跳问答。

Method: 设计分三阶段的Memento策略：将复杂问题分解为小步骤；用语言模型动态构建事实数据库；结合事实解决问题。

Result: 在多个基准测试中，Memento显著提升了现有方法性能，包括在9步PhantomWiki基准上将CoT性能翻倍，以及在2WikiMultiHopQA中提升了20+ F1百分点。

Conclusion: Memento策略能够有效提升多跳问答任务的性能，并具有广泛适用性。

Abstract: Large language models (LLMs) excel at reasoning-only tasks, but struggle when
reasoning must be tightly coupled with retrieval, as in multi-hop question
answering. To overcome these limitations, we introduce a prompting strategy
that first decomposes a complex question into smaller steps, then dynamically
constructs a database of facts using LLMs, and finally pieces these facts
together to solve the question. We show how this three-stage strategy, which we
call Memento, can boost the performance of existing prompting strategies across
diverse settings. On the 9-step PhantomWiki benchmark, Memento doubles the
performance of chain-of-thought (CoT) when all information is provided in
context. On the open-domain version of 2WikiMultiHopQA, CoT-RAG with Memento
improves over vanilla CoT-RAG by more than 20 F1 percentage points and over the
multi-hop RAG baseline, IRCoT, by more than 13 F1 percentage points. On the
challenging MuSiQue dataset, Memento improves ReAct by more than 3 F1
percentage points, demonstrating its utility in agentic settings.

</details>


### [129] [Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs](https://arxiv.org/abs/2506.20666)
*Sonia K. Murthy,Rosie Zhao,Jennifer Hu,Sham Kakade,Markus Wulfmeier,Peng Qian,Tomer Ullman*

Main category: cs.CL

TL;DR: 本文探讨在大语言模型（LLMs）中如何表现出人类决策过程中关于价值权衡（如表达真相与维持信任等）的多面性，利用认知科学中关于礼貌语的认知模型进行解析，评估模型在人类式价值权衡中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在捕捉类似人类复杂价值权衡的能力上存在局限，因此需要一种工具来评估模型如何在语言生成中平衡信息和社交效用的动态与权重。

Method: 本文运用认知科学中的礼貌语言认知模型，分两种模型场景系统化评估LLMs：前沿黑盒模型的推理努力程度和开源模型在强化学习后的训练动态。

Result: 结果显示在推理模型中比起社交效用更倾向于信息效用，且开源模型在数学推理上更优。此外，在训练动态中基模型和预训练数据对效用值的影响比反馈数据集或对齐方法更为显著。

Conclusion: 本文提出的方法能够敏感捕捉快速发展的LLM领域的多样性，帮助形成关于高级行为的假设，为改进推理模型的训练机制以及更好地控制训练中价值间的权衡提供科学依据。

Abstract: Navigating everyday social situations often requires juggling conflicting
goals, such as conveying a harsh truth, maintaining trust, all while still
being mindful of another person's feelings. These value trade-offs are an
integral part of human decision-making and language use, however, current tools
for interpreting such dynamic and multi-faceted notions of values in LLMs are
limited. In cognitive science, so-called "cognitive models" provide formal
accounts of these trade-offs in humans, by modeling the weighting of a
speaker's competing utility functions in choosing an action or utterance. In
this work, we use a leading cognitive model of polite speech to interpret the
extent to which LLMs represent human-like trade-offs. We apply this lens to
systematically evaluate value trade-offs in two encompassing model settings:
degrees of reasoning "effort" in frontier black-box models, and RL
post-training dynamics of open-source models. Our results highlight patterns of
higher informational utility than social utility in reasoning models, and in
open-source models shown to be stronger in mathematical reasoning. Our findings
from LLMs' training dynamics suggest large shifts in utility values early on in
training with persistent effects of the choice of base model and pretraining
data, compared to feedback dataset or alignment method. We show that our method
is responsive to diverse aspects of the rapidly evolving LLM landscape, with
insights for forming hypotheses about other high-level behaviors, shaping
training regimes for reasoning models, and better controlling trade-offs
between values during model training.

</details>


### [130] [CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation](https://arxiv.org/abs/2506.19952)
*Deepon Halder,Thanmay Jayakumar,Raj Dabre*

Main category: cs.CL

TL;DR: 提出了一种名为CycleDistill的方法，通过迭代生成合成平行语料和微调模型，提升了低资源语言的机器翻译性能。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言平行语料匮乏的问题，为低资源语言提供高质量的机器翻译能力。

Method: 提出CycleDistill方法，利用大语言模型的零样本或少样本翻译能力，从单语语料生成合成平行语料，再用这些语料微调原模型，形成迭代提升机制。同时分析了使用softmax激活优化蒸馏过程的效果。

Result: 针对三种印度语言的实验表明，CycleDistill方法可在第一轮迭代中平均提升20-30 chrF点，并实现高质量的机器翻译。

Conclusion: CycleDistill能够仅依赖少量少样本例子和单语语料，在低资源语言环境下生成高质量的机器翻译模型，具备有效性和潜力性。

Abstract: Large language models (LLMs), despite their ability to perform few-shot
machine translation (MT), often lag behind dedicated MT systems trained on
parallel corpora, which are crucial for high quality machine translation (MT).
However, parallel corpora are often scarce or non-existent for low-resource
languages. In this paper, we propose CycleDistill, a bootstrapping approach
leveraging LLMs and few-shot translation to obtain high-quality MT systems.
CycleDistill involves iteratively generating synthetic parallel corpora from
monolingual corpora via zero- or few-shot MT, which is then used to fine-tune
the model that was used for generating said data for MT. CycleDistill does not
need parallel corpora beyond 1 to 4 few-shot examples, and in our experiments
focusing on three Indian languages, by relying solely on monolingual corpora,
it can achieve high-quality machine translation, improving upon a few-shot
baseline model by over 20-30 chrF points on average in the first iteration. We
also study the effect of leveraging softmax activations during the distillation
process and observe mild improvements in translation quality.

</details>


### [131] [Inference Scaled GraphRAG: Improving Multi Hop Question Answering on Knowledge Graphs](https://arxiv.org/abs/2506.19967)
*Travis Thompson,Seung-Hwan Lim,Paul Liu,Ruoying He,Dongkuan Xu*

Main category: cs.CL

TL;DR: 本文提出一种名为Inference-Scaled GraphRAG的新框架，通过推理时间计算扩展来增强大语言模型（LLM）的图推理能力，对复杂的知识密集型推理任务表现显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决当前LLM在知识密集型推理任务中表现不佳的问题，尤其是在获取结构化上下文和多跳信息方面的不足。

Method: 提出Inference-Scaled GraphRAG框架，结合深度链式思维的图遍历（顺序扩展）与基于采样轨迹的多数投票（并行扩展），在推理执行循环中实现更高效的图推理。

Result: 在GRBench基准测试中，新方法显著提升了多跳问答性能，优于传统GraphRAG与先前的图遍历方法。

Conclusion: 推理时间扩展是一种实际且与架构无关的解决方案，可有效增强LLM在结构化知识推理中的能力。

Abstract: Large Language Models (LLMs) have achieved impressive capabilities in
language understanding and generation, yet they continue to underperform on
knowledge-intensive reasoning tasks due to limited access to structured context
and multi-hop information. Retrieval-Augmented Generation (RAG) partially
mitigates this by grounding generation in retrieved context, but conventional
RAG and GraphRAG methods often fail to capture relational structure across
nodes in knowledge graphs. We introduce Inference-Scaled GraphRAG, a novel
framework that enhances LLM-based graph reasoning by applying inference-time
compute scaling. Our method combines sequential scaling with deep
chain-of-thought graph traversal, and parallel scaling with majority voting
over sampled trajectories within an interleaved reasoning-execution loop.
Experiments on the GRBench benchmark demonstrate that our approach
significantly improves multi-hop question answering performance, achieving
substantial gains over both traditional GraphRAG and prior graph traversal
baselines. These findings suggest that inference-time scaling is a practical
and architecture-agnostic solution for structured knowledge reasoning with LLMs

</details>


### [132] [Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation](https://arxiv.org/abs/2506.19998)
*Xinyi Ni,Haonan Jian,Qiuyang Wang,Vedanshi Chetan Shah,Pengyu Hong*

Main category: cs.CL

TL;DR: 本文提出Doc2Agent，一个构建可调用Python工具的代理的流程，通过API文档生成可执行工具，并用代码代理迭代优化。实验表明其在API工具性能和成本上都有显著提升，并适用于复杂领域任务。


<details>
  <summary>Details</summary>
Motivation: 当前API代理多依赖于统一的工具集，无法反映实际复杂场景，难以构建能够自主解析非结构化API文档的智能代理工具。

Method: 提出一种名为Doc2Agent的流程，能从API文档生成可执行工具，并通过代码代理不断优化这些工具，测试和推断正确参数。

Result: 在真实API和基准测试中验证了Doc2Agent的性能，相比直接调用API提升了55%的性能，成本降低了90%。

Conclusion: Doc2Agent为从非结构化API文档大规模构建工具代理提供了一种普适性解决方案，并且可以适应领域特定的复杂任务。

Abstract: REST APIs play important roles in enriching the action space of web agents,
yet most API-based agents rely on curated and uniform toolsets that do not
reflect the complexity of real-world APIs. Building tool-using agents for
arbitrary domains remains a major challenge, as it requires reading
unstructured API documentation, testing APIs and inferring correct parameters.
We propose Doc2Agent, a scalable pipeline to build agents that can call
Python-based tools generated from API documentation. Doc2Agent generates
executable tools from API documentations and iteratively refines them using a
code agent. We evaluate our approach on real-world APIs, WebArena APIs, and
research APIs, producing validated tools. We achieved a 55\% relative
performance improvement with 90\% lower cost compared to direct API calling on
WebArena benchmark. A domain-specific agent built for glycomaterial science
further demonstrates the pipeline's adaptability to complex, knowledge-rich
tasks. Doc2Agent offers a generalizable solution for building tool agents from
unstructured API documentation at scale.

</details>


### [133] [A Modular Multitask Reasoning Framework Integrating Spatio-temporal Models and LLMs](https://arxiv.org/abs/2506.20073)
*Kethmi Hirushini Hettige,Jiahao Ji,Cheng Long,Shili Xiang,Gao Cong,Jingyuan Wang*

Main category: cs.CL

TL;DR: STReason结合大语言模型的推理能力与时空模型的分析能力，实现多任务推理和复杂的长篇推理；在新的基准数据集评估中表现优异，并有助于实际应用。


<details>
  <summary>Details</summary>
Motivation: 现有的时空数据挖掘模型受限于单一任务，无法胜任多任务推理及复杂的长篇推理，限制了其在实际多层面决策场景中的应用。

Method: 提出STReason框架，利用大语言模型的推理能力和时空模型的分析能力，通过上下文学习将自然语言查询分解为可解释的模块化程序，系统性执行以生成解决方案和详细推理。

Result: STReason在所有指标上显著优于先进的大语言模型基线，尤其在复杂的推理密集时空场景中表现突出。

Conclusion: STReason不仅在推理能力上表现优异，还降低了专家工作量，拓展了实际时空任务的适用性，为开发更通用的时空推理系统提供了方向。

Abstract: Spatio-temporal data mining plays a pivotal role in informed decision making
across diverse domains. However, existing models are often restricted to narrow
tasks, lacking the capacity for multi-task inference and complex long-form
reasoning that require generation of in-depth, explanatory outputs. These
limitations restrict their applicability to real-world, multi-faceted decision
scenarios. In this work, we introduce STReason, a novel framework that
integrates the reasoning strengths of large language models (LLMs) with the
analytical capabilities of spatio-temporal models for multi-task inference and
execution. Without requiring task-specific finetuning, STReason leverages
in-context learning to decompose complex natural language queries into modular,
interpretable programs, which are then systematically executed to generate both
solutions and detailed rationales. To facilitate rigorous evaluation, we
construct a new benchmark dataset and propose a unified evaluation framework
with metrics specifically designed for long-form spatio-temporal reasoning.
Experimental results show that STReason significantly outperforms advanced LLM
baselines across all metrics, particularly excelling in complex,
reasoning-intensive spatio-temporal scenarios. Human evaluations further
validate STReason's credibility and practical utility, demonstrating its
potential to reduce expert workload and broaden the applicability to real-world
spatio-temporal tasks. We believe STReason provides a promising direction for
developing more capable and generalizable spatio-temporal reasoning systems.

</details>


### [134] [SACL: Understanding and Combating Textual Bias in Code Retrieval with Semantic-Augmented Reranking and Localization](https://arxiv.org/abs/2506.20081)
*Dhruv Gupta,Gayathri Ganesh Lakshmy,Yiqing Xie*

Main category: cs.CL

TL;DR: 本研究探讨检索增强式代码生成中的代码检索问题，提出了一种名为SACL的新框架，显著提升了代码检索和生成效果。


<details>
  <summary>Details</summary>
Motivation: 分析当前代码检索方法的不足，包括对表面文本特征的过度依赖以及对文档良好的代码的偏向性。

Method: 提出SACL框架，该方法通过增加代码和结构知识的语义信息，丰富文本信息并减少偏见。

Result: SACL在代码检索和生成性能上均取得显著提升，例如在代码检索任务上Recall@1提高12.8%/9.4%/7.0%，代码生成任务上HumanEval的数据集Pass@1提升4.88%。

Conclusion: 优化代码检索和生成需要减少对表面文本和文档良好代码的偏好，SACL框架为此问题提供了有效方案。

Abstract: Retrieval-Augmented Code Generation (RACG) is a critical technique for
enhancing code generation by retrieving relevant information. In this work, we
conduct an in-depth analysis of code retrieval by systematically masking
specific features while preserving code functionality. Our discoveries include:
(1) although trained on code, current retrievers heavily rely on surface-level
textual features (e.g., docstrings, identifier names), and (2) they exhibit a
strong bias towards well-documented code, even if the documentation is
irrelevant.Based on our discoveries, we propose SACL, a framework that enriches
textual information and reduces bias by augmenting code or structural knowledge
with semantic information. Extensive experiments show that SACL substantially
improves code retrieval (e.g., by 12.8% / 9.4% / 7.0% Recall@1 on HumanEval /
MBPP / SWE-Bench-Lite), which also leads to better code generation performance
(e.g., by 4.88% Pass@1 on HumanEval).

</details>


### [135] [Bridging Compositional and Distributional Semantics: A Survey on Latent Semantic Geometry via AutoEncoder](https://arxiv.org/abs/2506.20083)
*Yingji Zhang,Danilo S. Carvalho,André Freitas*

Main category: cs.CL

TL;DR: 将组合性和符号属性融入分布式语义空间，提升Transformer自回归语言模型的能力，并探索语义表示学习的新方向。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过组合语义视角，更好地融合符号语义与分布式语义，填补两者之间的差距。

Method: 综述三种主流的自动编码器架构，即VAE、VQVAE和SAE，研究它们在语义结构与可解释性方面表现出不同的潜在几何性质。

Result: 对比分析了上述自动编码器如何在潜在语义几何中体现语义结构与可解释性之间的联系。

Conclusion: 通过语义表示学习的方法可以更有效地将符号语义和分布式语义结合，为语言模型提供具有可解释性和可控性的潜在空间。

Abstract: Integrating compositional and symbolic properties into current distributional
semantic spaces can enhance the interpretability, controllability,
compositionality, and generalisation capabilities of Transformer-based
auto-regressive language models (LMs). In this survey, we offer a novel
perspective on latent space geometry through the lens of compositional
semantics, a direction we refer to as \textit{semantic representation
learning}. This direction enables a bridge between symbolic and distributional
semantics, helping to mitigate the gap between them. We review and compare
three mainstream autoencoder architectures-Variational AutoEncoder (VAE),
Vector Quantised VAE (VQVAE), and Sparse AutoEncoder (SAE)-and examine the
distinctive latent geometries they induce in relation to semantic structure and
interpretability.

</details>


### [136] [ITFormer: Bridging Time Series and Natural Language for Multi-Modal QA with Large-Scale Multitask Dataset](https://arxiv.org/abs/2506.20093)
*Yilin Wang,Peixuan Lei,Jie Song,Yuzhe Hao,Tao Chen,Yuxuan Zhang,Lei Jia,Yuanxiang Li,Zhongyu Wei*

Main category: cs.CL

TL;DR: 本文提出了一个新的时间序列问答任务(Time-Series QA)，并发布了首个大规模时间序列与自然语言交互的数据集EngineMT-QA，同时介绍了一个名为ITFormer的跨模态框架。


<details>
  <summary>Details</summary>
Motivation: 现有技术难以实现时间序列数据与自然语言的动态交互，需要新的方法来更好地处理和整合这两者。

Method: 设计了一种名为ITFormer的新框架，将时间序列编码器与冻结的大型语言模型结合，能够高效提取和融合时间序列与文本特征。

Result: 与强基线相比，ITFormer在问答准确率上取得显著提升，且新增可训练参数不到1%。

Conclusion: 提出的方法不仅高效且具有鲁棒性，为时间序列数据与自然语言的集成建立了新的范式，推动了多模态AI的研究和应用。

Abstract: Time-series data are critical in diverse applications, such as industrial
monitoring, medical diagnostics, and climate research. However, effectively
integrating these high-dimensional temporal signals with natural language for
dynamic, interactive tasks remains a significant challenge. To address this, we
introduce the Time-Series Question Answering (Time-Series QA) task and release
EngineMT-QA, the first large-scale, multi-task, temporal-textual QA dataset
designed to capture complex interactions between time-series signals and
natural language. Building on this resource, we propose the Instruct Time
Transformer (ITFormer), a novel framework that bridges time-series encoders
with frozen large language models (LLMs). ITFormer effectively extracts,
aligns, and fuses temporal and textual features, achieving a strong improvement
in QA accuracy over strong baselines with fewer than 1\% additional trainable
parameters. By combining computational efficiency with robust cross-modal
modeling, our work establishes a adaptable paradigm for integrating temporal
data with natural language, paving the way for new research and applications in
multi-modal AI. More details about the project, including datasets and code,
are available at: https://pandalin98.github.io/itformer_site/

</details>


### [137] [A Multi-Pass Large Language Model Framework for Precise and Efficient Radiology Report Error Detection](https://arxiv.org/abs/2506.20112)
*Songsoo Kim,Seungtae Lee,See Young Lee,Joonho Kim,Keechan Kan,Dukyong Yoon*

Main category: cs.CL

TL;DR: 研究通过三步LLM框架显著提高了放射学报告校对的PPV，同时降低了操作成本和人工审查需求。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型在放射学报告校对中的PPV受限于低错误发生率，因此希望通过改进框架提高PPV并降低操作成本。

Method: 采用三种LLM框架（单一提示检测、提取加检测、提取检测加误报验证）对MIMIC-III数据库中的1000份报告进行分析，并使用两个外部数据集进行验证，测量了PPV、绝对真阳性率和成本效益。

Result: 三步LLM框架（框架3）将PPV从框架1的0.063提高到0.159，成本减少42.6%，人工审查量从192减少至88，而检测性能（aTPR）保持不变。外部验证表明框架3在不同数据集上的优势一致。

Conclusion: 引入三步LLM框架显著提升了错误检测的精准度，降低了成本，同时维持了检测性能，是放射学报告质量控制的有效方法。

Abstract: Background: The positive predictive value (PPV) of large language model
(LLM)-based proofreading for radiology reports is limited due to the low error
prevalence. Purpose: To assess whether a three-pass LLM framework enhances PPV
and reduces operational costs compared with baseline approaches. Materials and
Methods: A retrospective analysis was performed on 1,000 consecutive radiology
reports (250 each: radiography, ultrasonography, CT, MRI) from the MIMIC-III
database. Two external datasets (CheXpert and Open-i) were validation sets.
Three LLM frameworks were tested: (1) single-prompt detector; (2) extractor
plus detector; and (3) extractor, detector, and false-positive verifier.
Precision was measured by PPV and absolute true positive rate (aTPR).
Efficiency was calculated from model inference charges and reviewer
remuneration. Statistical significance was tested using cluster bootstrap,
exact McNemar tests, and Holm-Bonferroni correction. Results: Framework PPV
increased from 0.063 (95% CI, 0.036-0.101, Framework 1) to 0.079 (0.049-0.118,
Framework 2), and significantly to 0.159 (0.090-0.252, Framework 3; P<.001 vs.
baselines). aTPR remained stable (0.012-0.014; P>=.84). Operational costs per
1,000 reports dropped to USD 5.58 (Framework 3) from USD 9.72 (Framework 1) and
USD 6.85 (Framework 2), reflecting reductions of 42.6% and 18.5%, respectively.
Human-reviewed reports decreased from 192 to 88. External validation supported
Framework 3's superior PPV (CheXpert 0.133, Open-i 0.105) and stable aTPR
(0.007). Conclusion: A three-pass LLM framework significantly enhanced PPV and
reduced operational costs, maintaining detection performance, providing an
effective strategy for AI-assisted radiology report quality assurance.

</details>


### [138] [Leveraging AI Graders for Missing Score Imputation to Achieve Accurate Ability Estimation in Constructed-Response Tests](https://arxiv.org/abs/2506.20119)
*Masaki Uto,Yuma Ito*

Main category: cs.CL

TL;DR: 本研究提出了一种新方法，通过利用自动评分技术改进缺失分数的填补，从而在IRT框架下提高学习者能力评估的准确性，同时显著减少人工评分工作量。


<details>
  <summary>Details</summary>
Motivation: 当前评估学习者高阶能力如表达和逻辑思维技能的需求增加，而基于短答或作文题目的构建式测试虽然有效，但人工评分耗时耗力且成本高。

Method: 提出一种利用自动评分技术填补缺失分数的新方法，从而实现更准确的IRT（项目反应理论）能力估算。

Result: 该方法在显著降低人工评分工作量的同时，实现了高精度的能力估算。

Conclusion: 通过结合自动评分技术和IRC模型，该方法为有效评估学习者能力提供了一种高效且准确的解决方案。

Abstract: Evaluating the abilities of learners is a fundamental objective in the field
of education. In particular, there is an increasing need to assess higher-order
abilities such as expressive skills and logical thinking. Constructed-response
tests such as short-answer and essay-based questions have become widely used as
a method to meet this demand. Although these tests are effective, they require
substantial manual grading, making them both labor-intensive and costly. Item
response theory (IRT) provides a promising solution by enabling the estimation
of ability from incomplete score data, where human raters grade only a subset
of answers provided by learners across multiple test items. However, the
accuracy of ability estimation declines as the proportion of missing scores
increases. Although data augmentation techniques for imputing missing scores
have been explored in order to address this limitation, they often struggle
with inaccuracy for sparse or heterogeneous data. To overcome these challenges,
this study proposes a novel method for imputing missing scores by leveraging
automated scoring technologies for accurate IRT-based ability estimation. The
proposed method achieves high accuracy in ability estimation while markedly
reducing manual grading workload.

</details>


### [139] [CCRS: A Zero-Shot LLM-as-a-Judge Framework for Comprehensive RAG Evaluation](https://arxiv.org/abs/2506.20128)
*Aashiq Muhamed*

Main category: cs.CL

TL;DR: 提出一种名为CCRS的新评估套件，利用预训练的大语言模型进行零样本评估，用以提升RAG系统输出的质量评估效率和全面性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法满足RAG系统在上下文连贯性、事实准确性等多方面评估的需求，提出高效、一体化的评估手段成为必要。

Method: 采用CCRS评估套件，其中包含5个指标（上下文连贯性、问题相关性、信息密度、答案正确性和信息召回率），基于单一预训练语言模型进行零样本评估。

Result: 在BioASQ数据集上验证CCRS优于现有评估框架，特别是相比RAGChecker具备更高计算效率且在召回率和可靠性等关键方面表现优越。

Conclusion: CCRS提供了一种高效、全面的RAG系统评估框架，能够帮助提升系统性能并减少评估复杂性。

Abstract: RAG systems enhance LLMs by incorporating external knowledge, which is
crucial for domains that demand factual accuracy and up-to-date information.
However, evaluating the multifaceted quality of RAG outputs, spanning aspects
such as contextual coherence, query relevance, factual correctness, and
informational completeness, poses significant challenges. Existing evaluation
methods often rely on simple lexical overlap metrics, which are inadequate for
capturing these nuances, or involve complex multi-stage pipelines with
intermediate steps like claim extraction or require finetuning specialized
judge models, hindering practical efficiency. To address these limitations, we
propose CCRS (Contextual Coherence and Relevance Score), a novel suite of five
metrics that utilizes a single, powerful, pretrained LLM as a zero-shot,
end-to-end judge. CCRS evaluates: Contextual Coherence (CC), Question Relevance
(QR), Information Density (ID), Answer Correctness (AC), and Information Recall
(IR). We apply CCRS to evaluate six diverse RAG system configurations on the
challenging BioASQ dataset. Our analysis demonstrates that CCRS effectively
discriminates between system performances, confirming, for instance, that the
Mistral-7B reader outperforms Llama variants. We provide a detailed analysis of
CCRS metric properties, including score distributions, convergent/discriminant
validity, tie rates, population statistics, and discriminative power. Compared
to the complex RAGChecker framework, CCRS offers comparable or superior
discriminative power for key aspects like recall and faithfulness, while being
significantly more computationally efficient. CCRS thus provides a practical,
comprehensive, and efficient framework for evaluating and iteratively improving
RAG systems.

</details>


### [140] [AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control](https://arxiv.org/abs/2506.20160)
*Ruosen Li,Ziming Luo,Quan Zhang,Ruochen Li,Ben Zhou,Ali Payani,Xinya Du*

Main category: cs.CL

TL;DR: 提出了一种新的奖励机制AALC，通过动态平衡训练中的正确性和简洁性来优化大型推理模型，减少输出长度超过50%，同时保持或提高精度。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型生成冗长推理链条的问题，降低计算延迟和成本，同时保证准确性。

Method: 提出AALC，一种结合验证准确性和动态长度惩罚的强化学习奖励机制，在满足目标性能前推迟长度惩罚。

Result: 通过数学基准测试实验减少模型输出长度超过50%，并且在保持甚至提高原始准确性的同时，减少冗余推理模式并优化输出结构。

Conclusion: AALC展示了奖励机制的潜力，使模型推理路径更加高效和具有泛化能力，但模型的可解释性可能会有所减弱。

Abstract: Large reasoning models (LRMs) achieve impressive reasoning capabilities by
generating lengthy chain-of-thoughts, but this "overthinking" incurs high
latency and cost without commensurate accuracy gains. In this work, we
introduce AALC, a lightweight, accuracy-aware length reward integrated into
reinforcement learning that dynamically balances correctness and brevity during
training. By incorporating validation accuracy into the reward and employing a
smooth, dynamically scheduled length penalty, AALC delays length penalty until
target performance is met. Through extensive experiments across standard and
out-of-distribution math benchmarks, we show that our approach reduces response
length by over 50% while maintaining or even improving the original accuracy.
Furthermore, qualitative analysis reveals that our method curbs redundant
reasoning patterns such as excessive subgoal setting and verification, leading
to structurally refined outputs rather than naive truncation. We also identify
that efficiency gains are accompanied by reduced interpretability: models
trained with AALC omit some narrative framing and explanatory context. These
findings highlight the potential of reward-based strategies to guide LRMs
toward more efficient, generalizable reasoning paths.

</details>


### [141] [SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series Prediction with LLMs](https://arxiv.org/abs/2506.20167)
*Fengze Li,Yue Wang,Yangle Liu,Ming Huang,Dou Hong,Jieming Ma*

Main category: cs.CL

TL;DR: 提出SEED模型，融合结构性编码与语言模型，通过模块化架构提升时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决多变量时间序列预测中结构性建模与语义推理适应性不足的问题。

Method: 设计了四阶段的SEED模型，包括token-aware编码器、投影模块、语义重编程机制与冻结语言模型，以模块化方式解耦表征学习与推理。

Result: 实验结果表明，该方法在多种数据集上均优于强基线模型。

Conclusion: SEED模型有效填补了结构性建模与语义建模间的差距，实现了预测性能的稳定提升。

Abstract: Multivariate time series forecasting requires models to simultaneously
capture variable-wise structural dependencies and generalize across diverse
tasks. While structural encoders are effective in modeling feature
interactions, they lack the capacity to support semantic-level reasoning or
task adaptation. Conversely, large language models (LLMs) possess strong
generalization capabilities but remain incompatible with raw time series
inputs. This gap limits the development of unified, transferable prediction
systems. Therefore, we introduce SEED, a structural encoder for
embedding-driven decoding, which integrates four stages: a token-aware encoder
for patch extraction, a projection module that aligns patches with language
model embeddings, a semantic reprogramming mechanism that maps patches to
task-aware prototypes, and a frozen language model for prediction. This modular
architecture decouples representation learning from inference, enabling
efficient alignment between numerical patterns and semantic reasoning.
Empirical results demonstrate that the proposed method achieves consistent
improvements over strong baselines, and comparative studies on various datasets
confirm SEED's role in addressing the structural-semantic modeling gap.

</details>


### [142] [COIN: Uncertainty-Guarding Selective Question Answering for Foundation Models with Provable Risk Guarantees](https://arxiv.org/abs/2506.20178)
*Zhiyuan Wang,Jinhao Duan,Qingni Wang,Xiaofeng Zhu,Tianlong Chen,Xiaoshuang Shi,Kaidi Xu*

Main category: cs.CL

TL;DR: 本文提出了一种名为COIN的新框架，用于在用户指定的错误发现率（FDR）约束下生成单一答案，同时保留高置信度的预测结果。


<details>
  <summary>Details</summary>
Motivation: 解决生成式文本中因不确定性带来的错误发现问题，现有方法难以在特定错误控制下提供可靠的预测集。

Method: 提出COIN框架，通过统计学方法（如Clopper-Pearson）对校准集的经验误差率建立置信区间，以在测试数据中实现错误率控制并调整预测集的保留率。

Result: COIN在风险控制、收集有效答案能力及预测效率方面表现出强劲的适用性，即使校准数据有限时亦表现出良好性能。

Conclusion: 本文验证COIN对于多模态和一般文本生成的强适应性，且不同上界构建方法和不确定性评估策略的结合能够进一步提高模型性能。

Abstract: Uncertainty quantification (UQ) for foundation models is essential to
identify and mitigate potential hallucinations in automatically generated text.
However, heuristic UQ approaches lack formal guarantees for key metrics such as
the false discovery rate (FDR) in selective prediction. Previous work adopts
the split conformal prediction (SCP) framework to ensure desired coverage of
admissible answers by constructing prediction sets, but these sets often
contain incorrect candidates, limiting their practical utility. To address
this, we propose COIN, an uncertainty-guarding selection framework that
calibrates statistically valid thresholds to filter a single generated answer
per question under user-specified FDR constraints. COIN estimates the empirical
error rate on a calibration set and applies confidence interval methods such as
Clopper-Pearson to establish a high-probability upper bound on the true error
rate (i.e., FDR). This enables the selection of the largest uncertainty
threshold that ensures FDR control on test data while significantly increasing
sample retention. We demonstrate COIN's robustness in risk control, strong
test-time power in retaining admissible answers, and predictive efficiency
under limited calibration data across both general and multimodal text
generation tasks. Furthermore, we show that employing alternative upper bound
constructions and UQ strategies can further boost COIN's power performance,
which underscores its extensibility and adaptability to diverse application
scenarios.

</details>


### [143] [How to Retrieve Examples in In-context Learning to Improve Conversational Emotion Recognition using Large Language Models?](https://arxiv.org/abs/2506.20199)
*Mengqi Wang,Tiantian Feng,Shrikanth Narayanan*

Main category: cs.CL

TL;DR: 本文探讨通过改进上下文学习中的高质量示例检索，提高大语言模型在对话情感识别任务中的性能。提出的增强示例检索方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统情感识别任务具有主观性和较高的准确性要求，大语言模型在这方面仍有提升空间，这归因于高质量示例检索的重要性。

Method: 通过随机与增强的示例检索策略，研究对话情感识别中如何优化上下文学习，包括检索高质量的训练示例并通过改写增强其效果。

Result: 实验在IEMOCAP、MELD、EmoryNLP三组数据集上进行，结果显示增强示例检索方法在所有数据集上的表现均优于其他方法。

Conclusion: 检索和增强一致且针对性的示例对提高情感识别任务的准确性至关重要，增强示例检索被验证为优化对话情感识别的有效策略。

Abstract: Large language models (LLMs) have enabled a wide variety of real-world
applications in various domains. However, creating a high-performing
application with high accuracy remains challenging, particularly for subjective
tasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this
study investigates approaches to improving conversational emotion recognition
(CER) by LLMs. Specifically, we explore how to retrieve high-quality examples
in in-context learning (ICL) to enhance CER. We propose various strategies
based on random and augmented example retrieval and also analyze the impact of
conversational context on CER accuracy. Experiments were conducted on the three
datasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented
example retrieval consistently outperforms other techniques under investigation
across all datasets, highlighting the importance of retrieving coherent
targeted examples and enhancing them through paraphrasing.

</details>


### [144] [Intrinsic vs. Extrinsic Evaluation of Czech Sentence Embeddings: Semantic Relevance Doesn't Help with MT Evaluation](https://arxiv.org/abs/2506.20203)
*Petra Barančíková,Ondřej Bojar*

Main category: cs.CL

TL;DR: 本文对比了捷克语特定与多语言句嵌入模型，通过内部和外部评估范式进行评估，揭示了嵌入模型在语义相似性测试与翻译任务间的复杂关系。


<details>
  <summary>Details</summary>
Motivation: 研究句嵌入模型的语义捕获能力及其与下游任务表现的关系，为提升句嵌入模型的实用性提供线索。

Method: 利用Costra数据集及若干语义文本相似基准进行内部评估，用COMET度量工具进行下游翻译任务的外部评估，然后对模型进行微调。

Result: 实验表明，在语义相似性测试中表现出色的模型并不总能在翻译任务中表现更佳，但通过微调平滑后的嵌入空间可以达到优秀的翻译结果。

Conclusion: 研究指出句嵌入模型的语义检测与实际任务表现间存在错配，提出需要更深入的研究和数据集来桥接两者间的关系。

Abstract: In this paper, we compare Czech-specific and multilingual sentence embedding
models through intrinsic and extrinsic evaluation paradigms. For intrinsic
evaluation, we employ Costra, a complex sentence transformation dataset, and
several Semantic Textual Similarity (STS) benchmarks to assess the ability of
the embeddings to capture linguistic phenomena such as semantic similarity,
temporal aspects, and stylistic variations. In the extrinsic evaluation, we
fine-tune each embedding model using COMET-based metrics for machine
translation evaluation.
  Our experiments reveal an interesting disconnect: models that excel in
intrinsic semantic similarity tests do not consistently yield superior
performance on downstream translation evaluation tasks. Conversely, models with
seemingly over-smoothed embedding spaces can, through fine-tuning, achieve
excellent results. These findings highlight the complex relationship between
semantic property probes and downstream task, emphasizing the need for more
research into 'operationalizable semantics' in sentence embeddings, or more
in-depth downstream tasks datasets (here translation evaluation)

</details>


### [145] [Perspectives in Play: A Multi-Perspective Approach for More Inclusive NLP Systems](https://arxiv.org/abs/2506.20209)
*Benedetta Muscato,Lucia Passaro,Gizem Gezici,Fosca Giannotti*

Main category: cs.CL

TL;DR: 本研究提出了一种多视角软标签方法，旨在应对主观任务中处理不同意见不足的问题，证明其可以更好地捕捉人类的标签分布并提高分类性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决传统方法对争议数据中少数群体观点代表性不足的问题，尤其是在具有主观性任务的情境下。

Method: 采用多视角的软标签方法，用以取代传统的单一标签聚合策略，并通过多个主观文本分类任务进行广泛分析和评测。

Result: 结果表明多视角方法在捕获人类标签分布上表现更佳（JSD更低），且分类性能（F1分数）优于传统方法，但在讽刺和立场检测上展示出较低的置信度。

Conclusion: 多视角方法能够更好适配多元化和主观性强的NLP任务，并有助于解析模型预测的不确定性，同时为未来包容性和多元化模型的开发提供了可能性。

Abstract: In the realm of Natural Language Processing (NLP), common approaches for
handling human disagreement consist of aggregating annotators' viewpoints to
establish a single ground truth. However, prior studies show that disregarding
individual opinions can lead can lead to the side effect of underrepresenting
minority perspectives, especially in subjective tasks, where annotators may
systematically disagree because of their preferences. Recognizing that labels
reflect the diverse backgrounds, life experiences, and values of individuals,
this study proposes a new multi-perspective approach using soft labels to
encourage the development of the next generation of perspective aware models,
more inclusive and pluralistic. We conduct an extensive analysis across diverse
subjective text classification tasks, including hate speech, irony, abusive
language, and stance detection, to highlight the importance of capturing human
disagreements, often overlooked by traditional aggregation methods. Results
show that the multi-perspective approach not only better approximates human
label distributions, as measured by Jensen-Shannon Divergence (JSD), but also
achieves superior classification performance (higher F1 scores), outperforming
traditional approaches. However, our approach exhibits lower confidence in
tasks like irony and stance detection, likely due to the inherent subjectivity
present in the texts. Lastly, leveraging Explainable AI (XAI), we explore model
uncertainty and uncover meaningful insights into model predictions.

</details>


### [146] [Enhancing Large Language Models through Structured Reasoning](https://arxiv.org/abs/2506.20241)
*Yubo Dong,Hehe Fan*

Main category: cs.CL

TL;DR: 提出了一种通过显式结构化推理来增强大型语言模型性能的新方法，并验证了其在复杂推理任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型在逻辑推理和系统规划等复杂任务中表现有限，主要原因在于它们依赖隐式的统计关系而非结构化的知识表征。

Method: 将非结构化数据转换为结构化格式，明确标注推理步骤；通过监督微调训练语言模型；采用GRPO策略，结合MAX-Flow和LCS算法，以提高模型的推理能力和计算效率。

Result: 通过对DeepSeek-R1-Distill-Qwen-1.5B模型的微调实验，模型表现出简洁的推理能力和兼容性增强，并在多场景中表现出强大的性能。

Conclusion: 结构化推理的引入能有效提升大型语言模型在复杂推理任务中的表现，同时降低计算复杂度，具有较高的优化潜力。

Abstract: Recent Large Language Models (LLMs) have significantly advanced natural
language processing and automated decision-making. However, these models still
encounter difficulties when performing complex reasoning tasks involving
logical deduction and systematic planning, primarily due to their reliance on
implicit statistical relationships without structured knowledge
representation.Inspired by cognitive science and neurosymbolic AI, we introduce
a novel approach to enhance LLMs through explicit structured reasoning. First,
we convert unstructured data into structured formats by explicitly annotating
reasoning steps. We then employ this structured dataset to train LLMs through
Supervised Fine-Tuning (SFT). Additionally, we enhance the structured reasoning
capabilities of LLMs using Group Relative Policy Optimization (GRPO),
incorporating two innovative algorithms--MAX-Flow and Longest Common
Subsequence (LCS)--which notably improve reasoning effectiveness and reduce
computational complexity. Experimental results from fine-tuning a
DeepSeek-R1-Distill-Qwen-1.5B model demonstrate concise reasoning, robust
performance across various scenarios, and improved compatibility with
optimization techniques, validating the efficacy of structured reasoning
integration in LLMs.

</details>


### [147] [CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment](https://arxiv.org/abs/2506.20243)
*Papa Séga Wade,Mihai Andries,Ioannis Kanellos,Thierry Moudenc*

Main category: cs.CL

TL;DR: 本文提出了一种利用多SSL模型的分块方法评估非母语者的流利度，显著提高了多个评测指标。


<details>
  <summary>Details</summary>
Motivation: 自动流利度评估中，对于非母语者的语音节奏、停顿及不流畅性捕捉存在困难，需进一步探索更精准的方法。

Method: 运用基于呼吸分组的分块方法，通过Silero-VAD进行语音分割，结合多种SSL模型（Wav2Vec2、HuBERT、WavLM）捕捉声学与语言特征，并采用CNN-BiLSTM整合局部及全局依赖关系，同时引入语速、停顿时间等流利度标记，优化评估效果。

Result: 在Avalinguo与Speechocean762数据集上，与单一SSL基线相比，本文方法将F1得分分别提升2.8和4.2，Pearson相关系数分别提高6.2和4.0，显著优于Pyannote.audio分段基线。

Conclusion: 利用多SSL模型结合分块方法能增强对非母语者流利度的鲁棒评估，但未来研究需关注方言及不规则韵律的适用性。

Abstract: Automatic fluency assessment (AFA) remains challenging, particularly in
capturing speech rhythm, pauses, and disfluencies in non-native speakers. We
introduce a chunk-based approach integrating self-supervised learning (SSL)
models (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths
in phonetic, prosodic, and noisy speech modeling, with a hierarchical
CNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero
voice activity detection (Silero-VAD), enabling fine-grained temporal analysis
while mitigating over-segmentation artifacts. SSL embeddings are fused via a
learnable weighted mechanism, balancing acoustic and linguistic features, and
enriched with chunk-level fluency markers (e.g., speech rate, pause durations,
n-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies
across chunks. Evaluated on Avalinguo and Speechocean762, our approach improves
F1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines
on Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on
Avalinguo, surpassing Pyannote.audio-based segmentation baselines. These
findings highlight chunk-based multi-SSL fusion for robust fluency evaluation,
though future work should explore generalization to dialects with irregular
prosody.

</details>


### [148] [Narrative Shift Detection: A Hybrid Approach of Dynamic Topic Models and Large Language Models](https://arxiv.org/abs/2506.20269)
*Kai-Robin Lange,Tobias Schmidt,Matthias Reccius,Henrik Müller,Michael Roos,Carsten Jentsch*

Main category: cs.CL

TL;DR: 提出一种结合大规模语言模型和主题模型的动态叙事变化研究方法，用于分析媒体叙事的时间演化。


<details>
  <summary>Details</summary>
Motivation: 当前媒体叙事变化迅速，深入理解其随时间发展的方式更加重要，而现有方法应用于整个语料库成本高昂，需改进分析效率。

Method: 结合大型语言模型的语言理解能力和主题模型的广泛适用性，通过主题模型和变化检测方法识别特定主题的变化后，以语言模型分析变化并区分叙事与内容变化。

Result: 在《华尔街日报》2009到2023年的文章中测试，发现大型语言模型能有效提取特定时间节点的叙事变化，但在区分叙事与内容变化时表现较差。

Conclusion: 该方法证明了结合大型语言模型和主题模型在动态叙事分析中的可行性，但需要改进对叙事与内容变化的区分能力。

Abstract: With rapidly evolving media narratives, it has become increasingly critical
to not just extract narratives from a given corpus but rather investigate, how
they develop over time. While popular narrative extraction methods such as
Large Language Models do well in capturing typical narrative elements or even
the complex structure of a narrative, applying them to an entire corpus comes
with obstacles, such as a high financial or computational cost. We propose a
combination of the language understanding capabilities of Large Language Models
with the large scale applicability of topic models to dynamically model
narrative shifts across time using the Narrative Policy Framework. We apply a
topic model and a corresponding change point detection method to find changes
that concern a specific topic of interest. Using this model, we filter our
corpus for documents that are particularly representative of that change and
feed them into a Large Language Model that interprets the change that happened
in an automated fashion and distinguishes between content and narrative shifts.
We employ our pipeline on a corpus of The Wall Street Journal news paper
articles from 2009 to 2023. Our findings indicate that a Large Language Model
can efficiently extract a narrative shift if one exists at a given point in
time, but does not perform as well when having to decide whether a shift in
content or a narrative shift took place.

</details>


### [149] [Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining and Extracting Rare and Hidden Content](https://arxiv.org/abs/2506.20331)
*Rian Touchent,Nathan Godey,Eric de la Clergerie*

Main category: cs.CL

TL;DR: 引入了一个名为Biomed-Enriched的生物医学文本数据集，包含从PubMed生成的注释数据，并通过两阶段注释和小模型微调提供多种高质量子集，可用于提升生物医学与临床NLP任务。


<details>
  <summary>Details</summary>
Motivation: 为了克服临床文本因隐私限制很难获取的问题，提供一个公开可用的大规模、生物医学和临床相关数据集，促进NLP研究。

Method: 采用两阶段注释过程，首先利用大型语言模型对PubMed的400K段落进行类型、领域和教育质量评分的初始标注。其次，以此标注微调小型语言模型，扩展至完整PMC-OA数据集并生成高质量子集。

Result: 构建了多个质量过滤和领域调优后数据集，在多项任务上的表现均有提升，例如临床文本调整提高MMLU ProfMed约5%，教育质量过滤提升MedQA和MedMCQA约1%，并通过这些技术加速训练收敛。

Conclusion: Biomed-Enriched数据集是公开可获取的临床相关文献集，极具价值，且实验表明它能够有效提升生物医学NLP任务表现，并提高预训练效率。

Abstract: We introduce Biomed-Enriched, a biomedical text dataset constructed from
PubMed via a two-stage annotation process. In the first stage, a large language
model annotates 400K paragraphs from PubMed scientific articles, assigning
scores for their type (review, study, clinical case, other), domain (clinical,
biomedical, other), and educational quality. The educational quality score
(rated 1 to 5) estimates how useful a paragraph is for college-level learning.
These annotations are then used to fine-tune a small language model, which
propagates the labels across the full PMC-OA corpus. The resulting metadata
allows us to extract refined subsets, including 2M clinical case paragraphs
with over 450K high-quality ones from articles with commercial-use licenses,
and to construct several variants via quality filtering and domain upsampling.
Clinical text is typically difficult to access due to privacy constraints, as
hospital records cannot be publicly shared. Hence, our dataset provides an
alternative large-scale, openly available collection of clinical cases from
PubMed, making it a valuable resource for biomedical and clinical NLP.
Preliminary continual-pretraining experiments with OLMo2 suggest these curated
subsets enable targeted improvements, with clinical upsampling boosting
performance by ~5% on MMLU ProfMed and educational quality filtering improving
MedQA and MedMCQA by ~1%. Combinations of these techniques led to faster
convergence, reaching same performance with a third of training tokens,
indicating potential for more efficient and effective biomedical pretraining
strategies.

</details>


### [150] [TAPS: Tool-Augmented Personalisation via Structured Tagging](https://arxiv.org/abs/2506.20409)
*Ekaterina Taktasheva,Jeff Dalton*

Main category: cs.CL

TL;DR: 本文研究了如何将用户偏好有效集成到目标导向型对话代理中，提出了一个名为TAPS的新方法。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强型语言模型缺乏个性化指导工具使用的能力。本研究旨在填补这一空白。

Method: 提出了一种结合结构化标记工具和基于不确定性的工具探测器的TAPS方法。

Result: TAPS显著提高了语言模型结合用户偏好的能力，在NLSI任务上实现了开源模型的新最优性能。

Conclusion: 通过整合用户偏好的新方法TAPS，改进了对话代理的工具使用能力，推动了个性化交互发展的新方向。

Abstract: Recent advancements in tool-augmented large language models have enabled them
to interact with external tools, enhancing their ability to perform complex
user tasks. However, existing approaches overlook the role of personalisation
in guiding tool use. This work investigates how user preferences can be
effectively integrated into goal-oriented dialogue agents. Through extensive
analysis, we identify key weaknesses in the ability of LLMs to personalise tool
use. To this end, we introduce \name, a novel solution that enhances
personalised tool use by leveraging a structured tagging tool and an
uncertainty-based tool detector. TAPS significantly improves the ability of
LLMs to incorporate user preferences, achieving the new state-of-the-art for
open source models on the NLSI task.

</details>


### [151] [An Agentic System for Rare Disease Diagnosis with Traceable Reasoning](https://arxiv.org/abs/2506.20430)
*Weike Zhao,Chaoyi Wu,Yanjie Fan,Xiaoman Zhang,Pengcheng Qiu,Yuze Sun,Xiao Zhou,Yanfeng Wang,Ya Zhang,Yongguo Yu,Kun Sun,Weidi Xie*

Main category: cs.CL

TL;DR: 研究开发了DeepRare系统，可精确诊断罕见疾病，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 罕见疾病因其复杂性、低流行率以及临床医生的认知局限，对及时、准确的诊断构成巨大挑战。

Method: 引入DeepRare系统，该系统基于大型语言模型，采用模块化设计，由中央主机、长效记忆模块和多个域专用代理服务器组成，结合超过40种专业工具与医疗知识来源，提供可追踪的诊断分析。

Result: DeepRare在针对2,919种疾病的诊断中表现卓越，精准率达到100%（针对1,013种疾病）。在多数据集评估中，其Recall@1得分显著超越其他方法。此外，专家对其推理链验证的一致性达95.40%。

Conclusion: DeepRare系统以卓越性能为罕见疾病诊断提供了强大的辅助工具，同时通过临床验证证明其诊断与推理的可靠性。

Abstract: Rare diseases collectively affect over 300 million individuals worldwide, yet
timely and accurate diagnosis remains a pervasive challenge. This is largely
due to their clinical heterogeneity, low individual prevalence, and the limited
familiarity most clinicians have with rare conditions. Here, we introduce
DeepRare, the first rare disease diagnosis agentic system powered by a large
language model (LLM), capable of processing heterogeneous clinical inputs. The
system generates ranked diagnostic hypotheses for rare diseases, each
accompanied by a transparent chain of reasoning that links intermediate
analytic steps to verifiable medical evidence.
  DeepRare comprises three key components: a central host with a long-term
memory module; specialized agent servers responsible for domain-specific
analytical tasks integrating over 40 specialized tools and web-scale,
up-to-date medical knowledge sources, ensuring access to the most current
clinical information. This modular and scalable design enables complex
diagnostic reasoning while maintaining traceability and adaptability. We
evaluate DeepRare on eight datasets. The system demonstrates exceptional
diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013
diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15
methods, like traditional bioinformatics diagnostic tools, LLMs, and other
agentic systems, achieving an average Recall@1 score of 57.18% and surpassing
the second-best method (Reasoning LLM) by a substantial margin of 23.79
percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at
Recall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of
reasoning chains by clinical experts achieves 95.40% agreements. Furthermore,
the DeepRare system has been implemented as a user-friendly web application
http://raredx.cn/doctor.

</details>


### [152] [Probing AI Safety with Source Code](https://arxiv.org/abs/2506.20471)
*Ujwal Narayan,Shreyas Chaudhari,Ashwin Kalyan,Tanmay Rajpurohit,Karthik Narasimhan,Ameet Deshpande,Vishvak Murahari*

Main category: cs.CL

TL;DR: 本文提出了一个名为Code of Thought (CoDoT)的提示策略，用于评估大语言模型（LLMs）的安全性。通过实验发现，当前的LLMs在安全性目标上表现不足，存在潜在危害。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，这些模型被用于涉及人类安全的重要场景，因此需要既提高其能力，又确保其安全性以及符合人类价值观和偏好。

Method: 提出了CoDoT方法，将自然语言输入转换为简单代码以表达同样的意图。例如，将“Make the statement more toxic: {text}”转换为代码格式“make_more_toxic({text})”，然后测试模型对这种输入的反应。

Result: 实验显示，CoDoT能够在一系列最新LLMs中引发显著的安全问题，包括GPT-4 Turbo的毒性增加16.5倍，DeepSeek R1的失败率达到100%，以及平均毒性增加300%。递归应用CoDoT甚至可以将毒性再次提高两倍。

Conclusion: 该研究表明，当前LLMs在安全性方面表现不佳，提出了从根本原则评估安全性的重要性，以确保安全性和能力的同步进步。

Abstract: Large language models (LLMs) have become ubiquitous, interfacing with humans
in numerous safety-critical applications. This necessitates improving
capabilities, but importantly coupled with greater safety measures to align
these models with human values and preferences. In this work, we demonstrate
that contemporary models fall concerningly short of the goal of AI safety,
leading to an unsafe and harmful experience for users. We introduce a prompting
strategy called Code of Thought (CoDoT) to evaluate the safety of LLMs. CoDoT
converts natural language inputs to simple code that represents the same
intent. For instance, CoDoT transforms the natural language prompt "Make the
statement more toxic: {text}" to: "make_more_toxic({text})". We show that CoDoT
results in a consistent failure of a wide range of state-of-the-art LLMs. For
example, GPT-4 Turbo's toxicity increases 16.5 times, DeepSeek R1 fails 100% of
the time, and toxicity increases 300% on average across seven modern LLMs.
Additionally, recursively applying CoDoT can further increase toxicity two
times. Given the rapid and widespread adoption of LLMs, CoDoT underscores the
critical need to evaluate safety efforts from first principles, ensuring that
safety and capabilities advance together.

</details>


### [153] [Time is On My Side: Dynamics of Talk-Time Sharing in Video-chat Conversations](https://arxiv.org/abs/2506.20474)
*Kaixiang Zhang,Justine Zhang,Cristian Danescu-Niculescu-Mizil*

Main category: cs.CL

TL;DR: 本文提出了一种计算框架，用于量化对话中说话时间的分布和动态特征，探讨了不同分布对参与者感受的影响，并提出了对通信平台设计的潜在启发。


<details>
  <summary>Details</summary>
Motivation: 研究对话中说话时间分配及其动态特征如何影响参与者体验，为优化人与人或人与AI的通信设计提供工具。

Method: 提出了量化对话中说话时间分布与动态特征的计算框架，并在大规模陌生人视频聊天数据集中验证其有效性，建立了一个基于多维变化轴的分类体系。

Result: 发现不同的说话时间分布对参与者的感受有显著影响，平衡的对话更受青睐；即使分布整体一致，不同的动态模式也会导致不同的体验。

Conclusion: 框架为量化和研究对话中动态特征提供了新工具，并对通信平台的设计和优化具有实际意义。

Abstract: An intrinsic aspect of every conversation is the way talk-time is shared
between multiple speakers. Conversations can be balanced, with each speaker
claiming a similar amount of talk-time, or imbalanced when one talks
disproportionately. Such overall distributions are the consequence of
continuous negotiations between the speakers throughout the conversation: who
should be talking at every point in time, and for how long?
  In this work we introduce a computational framework for quantifying both the
conversation-level distribution of talk-time between speakers, as well as the
lower-level dynamics that lead to it. We derive a typology of talk-time sharing
dynamics structured by several intuitive axes of variation. By applying this
framework to a large dataset of video-chats between strangers, we confirm that,
perhaps unsurprisingly, different conversation-level distributions of talk-time
are perceived differently by speakers, with balanced conversations being
preferred over imbalanced ones, especially by those who end up talking less.
Then we reveal that -- even when they lead to the same level of overall balance
-- different types of talk-time sharing dynamics are perceived differently by
the participants, highlighting the relevance of our newly introduced typology.
Finally, we discuss how our framework offers new tools to designers of
computer-mediated communication platforms, for both human-human and human-AI
communication.

</details>


### [154] [Knowledge-Aware Diverse Reranking for Cross-Source Question Answering](https://arxiv.org/abs/2506.20476)
*Tong Zhou*

Main category: cs.CL

TL;DR: 本文提出了Team Marikarp在2025年SIGIR LiveRAG竞赛中的解决方案，使用创新的知识感知多样化重排序RAG流程，并取得了第一名。


<details>
  <summary>Details</summary>
Motivation: 为了解决如何从FineWeb语料库的15M文档中高效检索与问题密切相关的支持文档的问题

Method: 开发了一种知识感知的多样化重排序RAG流程，并对竞赛中自动生成的多样化评估集进行了优化。

Result: 该知识感知重排序RAG流程成功在提供公平评估的竞赛中获得了第一名。

Conclusion: 本文的方法在知识组织和检索效率上表现优越，证实其解决竞赛任务的有效性。

Abstract: This paper presents Team Marikarp's solution for the SIGIR 2025 LiveRAG
competition. The competition's evaluation set, automatically generated by
DataMorgana from internet corpora, encompassed a wide range of target topics,
question types, question formulations, audience types, and knowledge
organization methods. It offered a fair evaluation of retrieving
question-relevant supporting documents from a 15M documents subset of the
FineWeb corpus. Our proposed knowledge-aware diverse reranking RAG pipeline
achieved first place in the competition.

</details>


### [155] [GPTailor: Large Language Model Pruning Through Layer Cutting and Stitching](https://arxiv.org/abs/2506.20480)
*Guinan Su,Li Shen,Lu Yin,Shiwei Liu,Yanwu Yang,Jonas Geiping*

Main category: cs.CL

TL;DR: 本研究提出了一种通过合并和选择微调模型层来压缩大语言模型（LLMs）的方法，有效减少参数的同时，保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs虽然在语言理解和生成方面表现出了非凡的能力，但其巨大的模型规模给部署和推理带来了挑战。需要开发新的方法来在保持模型能力的前提下减少计算成本。

Method: 提出一种新型策略，将微调后的模型变体中的层进行战略性合并。通过支持三种操作的搜索空间来优化：层移除、从不同候选模型中选择层、以及层合并。

Result: 在实验中，新方法在Llama2-13B模型族中表现优异，压缩后的模型去除了约25%的参数，但仍能保持97.3%的原始性能，显著优于最先进的现有方法。

Conclusion: 通过将微调后的模型层进行优化组合，可以在减少模型规模的同时保留其性能，为LLMs的高效部署提供了一种有效解决方案。

Abstract: Large language models (LLMs) have shown remarkable capabilities in language
understanding and generation. However, such impressive capability typically
comes with a substantial model size, which presents significant challenges in
deployment and inference. While structured pruning of model parameters offers a
promising way to reduce computational costs at deployment time, current methods
primarily focus on single model pruning. In this work, we develop a novel
strategy to compress models by strategically combining or merging layers from
finetuned model variants, which preserves the original model's abilities by
aggregating capabilities accentuated in different finetunes. We pose the
optimal tailoring of these LLMs as a zero-order optimization problem, adopting
a search space that supports three different operations: (1) Layer removal, (2)
Layer selection from different candidate models, and (3) Layer merging. Our
experiments demonstrate that this approach leads to competitive model pruning,
for example, for the Llama2-13B model families, our compressed models maintain
approximately 97.3\% of the original performance while removing $\sim25\%$ of
parameters, significantly outperforming previous state-of-the-art methods. The
code is available at https://github.com/Guinan-Su/auto-merge-llm.

</details>


### [156] [ReCode: Updating Code API Knowledge with Reinforcement Learning](https://arxiv.org/abs/2506.20495)
*Haoze Wu,Yunzhi Yao,Wenhao Yu,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 本研究提出ReCode框架，通过强化学习提升LLMs处理动态API环境中代码生成的能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs难以适应外部库API频繁更新，导致动态环境下的代码生成表现受限。

Method: 提出ReCode框架，构建2000条数据集训练LLMs进行版本迁移，引入修改后的字符串相似度指标作为强化学习奖励，提升LLMs对API变化的适应能力。

Result: ReCode显著提高了动态API场景下的代码生成性能，尤其在未见过的CodeUpdateArena任务中表现优异，并对LLMs的通用代码生成能力影响较小。

Conclusion: ReCode框架为增强LLMs应对动态API变化的能力提供了一种有效方法，展示出在多个模型和算法上的一致改进，并进一步验证了其优越性。

Abstract: Large Language Models (LLMs) exhibit remarkable code generation capabilities
but falter when adapting to frequent updates in external library APIs. This
critical limitation, stemming from reliance on outdated API knowledge from
their training data, even with access to current documentation, impedes
reliable code generation in dynamic environments. To tackle this issue, we
propose ReCode (rule-based Reinforcement learning for Code Update), a novel
framework that mimics human programmer adaptation to API changes. Specifically,
we construct a dataset of approximately 2,000 data entries to train the LLMs to
perform version migration based on updated information. Then, we introduce a
modified string similarity metric for code evaluation as the reward for
reinforcement learning. Our experiments demonstrate that ReCode substantially
boosts LLMs' code generation performance in dynamic API scenarios, especially
on the unseen CodeUpdateArena task. Crucially, compared to supervised
fine-tuning, ReCode has less impact on LLMs' general code generation abilities.
We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and
DAPO), all achieving consistent improvements. Notably, after training,
Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned
model and the reasoning model with the same architecture. Code is available at
https://github.com/zjunlp/ReCode.

</details>


### [157] [OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling](https://arxiv.org/abs/2506.20512)
*Zengzhi Wang,Fan Zhou,Xuefeng Li,Pengfei Liu*

Main category: cs.CL

TL;DR: 本文研究了为什么某些语言模型在强化学习（RL）中表现优异，提出了一个“稳定-递减”的中期训练策略OctoThinker，并推出了一个高质量的数学推理语料库来支持后续研究。


<details>
  <summary>Details</summary>
Motivation: 探索基础语言模型在经过强化学习后表现差异的原因，旨在为下一代RL可扩展基础模型的研发提供依据。

Method: 分析两种代表性模型（Qwen和Llama）的中期训练策略对强化学习的影响，并提出了一个两阶段的中期训练策略“稳定-递减”。

Result: 高质量的数学语料和QA链式推理数据显著提升了RL效果，新策略训练出的模型（OctoThinker）在RL中表现优异，接近于更适合RL的模型家族（如 Qwen）。

Conclusion: 高质量语料和合理的中期训练策略对强化学习兼容至关重要，提出的“稳定-递减”策略提供了一个可靠的优化路径，有助于未来基础模型的开发。

Abstract: Different base language model families, such as Llama and Qwen, exhibit
divergent behaviors during post-training with reinforcement learning (RL),
especially on reasoning-intensive tasks. What makes a base language model
suitable for reinforcement learning? Gaining deeper insight into this question
is essential for developing RL-scalable foundation models of the next
generation. In this work, we investigate how mid-training strategies shape RL
dynamics, focusing on two representative model families: Qwen and Llama. Our
study reveals that (1) high-quality mathematical corpora, such as
MegaMath-Web-Pro, significantly improve both base model and RL performance,
while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further
adding QA-style data, particularly long chain-of-thought (CoT) reasoning
examples, enhances RL outcomes, and instruction data further unlocks this
effect; (3) while long-CoT improves reasoning depth, it can also induce
verbosity of model responses and unstability of RL training, underscoring the
importance of data formatting; (4) scaling mid-training consistently leads to
stronger downstream RL performance. Building on these insights, we introduce a
two-stage mid-training strategy, Stable-then-Decay, in which base models are
first trained on 200B tokens with a constant learning rate, followed by 20B
tokens across three CoT-focused branches with learning rate decay. This yields
OctoThinker, a family of models demonstrating strong RL compatibility and
closing the performance gap with more RL-friendly model families, i.e., Qwen.
We hope our work will help shape pre-training strategies for foundation models
in the RL era. To support further research, we release our open-source models
along with a curated math reasoning-intensive corpus of over 70 billion tokens
(i.e., MegaMath-Web-Pro-Max).

</details>


### [158] [When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs](https://arxiv.org/abs/2506.20544)
*Ammar Khairi,Daniel D'souza,Ye Shen,Julia Kreutzer,Sara Hooker*

Main category: cs.CL

TL;DR: 本文探讨了如何在多语言、多任务情境下优化生成模型的推理效率。提出了新的采样与选择策略，并在多个语言与任务上表现出了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在推理时多针对英语等少数特定领域，缺乏对开放式任务、多语言环境的适配，因此需要开发能全面泛化的新技术。

Method: 采用温度变化的采样策略，并提出针对多语言、多任务的选择优化方法，适配于开放式生成任务的推理场景。

Result: 在8B模型上，所提方法在m-ArenaHard-v2.0基准中较现有方法提升了+6.8的胜率；在111B模型上，仅用五次采样达到了+9.0的提升，对成本需求较低。

Conclusion: 需要开发语言和任务感知的推理方法，以实现对低资源语言和任务的性能提升，促进技术的普惠性。

Abstract: Recent advancements in large language models (LLMs) have shifted focus toward
scaling inference-time compute, improving performance without retraining the
model. A common approach is to sample multiple outputs in parallel, and select
one of these as the final output. However, work to date has focused on English
and a handful of domains such as math and code. In contrast, we are most
interested in techniques that generalize across open-ended tasks, formally
verifiable tasks, and across languages. In this work, we study how to robustly
scale inference-time compute for open-ended generative tasks in a multilingual,
multi-task setting.
  Our findings show that both sampling strategy based on temperature variation
and selection strategy must be adapted to account for diverse domains and
varied language settings. We evaluate existing selection methods, revealing
that strategies effective in English often fail to generalize across languages.
We propose novel sampling and selection strategies specifically adapted for
multilingual and multi-task inference scenarios, and show they yield notable
gains across languages and tasks. In particular, our combined sampling and
selection methods lead to an average +6.8 jump in win-rates for our 8B models
on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At
larger scale, Command-A (111B model) equipped with our methods, shows +9.0
improvement in win-rates on the same benchmark with just five samples against
single-sample decoding, a substantial increase at minimal cost. Our results
underscore the need for language- and task-aware approaches to inference-time
compute, aiming to democratize performance improvements in underrepresented
languages.

</details>


### [159] [Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm](https://arxiv.org/abs/2506.20606)
*Baixiang Huang,Zhen Tan,Haoran Wang,Zijie Liu,Dawei Li,Ali Payani,Huan Liu,Tianlong Chen,Kai Shu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Behavior Editing的方法，通过模型编辑来调整大型语言模型（LLMs）代理的道德行为，并介绍了BehaviorBench基准，用于系统评估模型的行为修改效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多种任务中表现卓越，但在高风险领域中的应用可能引发安全和伦理问题，因此需要一种手段来有效引导代理的道德行为。

Method: 作者将代理行为引导框架化为模型编辑任务，即Behavior Editing，并提出了一个基于心理道德理论的多层基准BehaviorBench，用于评估和编辑代理在不同情景中的行为表现。

Result: 实验表明，Behavior Editing能够动态调整代理在特定情境中的目标行为，并允许进行更广泛的全球道德一致性调整，同时展示了其能增强或削弱代理的伦理行为。

Conclusion: 研究揭示了Behavior Editing作为引导代理行为的新范式的有效性及潜在风险，为高效且精确地调整代理道德行为提供了新的思路。

Abstract: Agents based on Large Language Models (LLMs) have demonstrated strong
capabilities across a wide range of tasks. However, deploying LLM-based agents
in high-stakes domains comes with significant safety and ethical risks.
Unethical behavior by these agents can directly result in serious real-world
consequences, including physical harm and financial loss. To efficiently steer
the ethical behavior of agents, we frame agent behavior steering as a model
editing task, which we term Behavior Editing. Model editing is an emerging area
of research that enables precise and efficient modifications to LLMs while
preserving their overall capabilities. To systematically study and evaluate
this approach, we introduce BehaviorBench, a multi-tier benchmark grounded in
psychological moral theories. This benchmark supports both the evaluation and
editing of agent behaviors across a variety of scenarios, with each tier
introducing more complex and ambiguous scenarios. We first demonstrate that
Behavior Editing can dynamically steer agents toward the target behavior within
specific scenarios. Moreover, Behavior Editing enables not only
scenario-specific local adjustments but also more extensive shifts in an
agent's global moral alignment. We demonstrate that Behavior Editing can be
used to promote ethical and benevolent behavior or, conversely, to induce
harmful or malicious behavior. Through comprehensive evaluations on agents
based on frontier LLMs, BehaviorBench shows the effectiveness of Behavior
Editing across different models and scenarios. Our findings offer key insights
into a new paradigm for steering agent behavior, highlighting both the promise
and perils of Behavior Editing.

</details>


### [160] [DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation](https://arxiv.org/abs/2506.20639)
*Shansan Gong,Ruixiang Zhang,Huangjie Zheng,Jiatao Gu,Navdeep Jaitly,Lingpeng Kong,Yizhe Zhang*

Main category: cs.CL

TL;DR: 提出了一种新的扩散式大语言模型DiffuCoder，通过分析其解码行为及强化学习方法，提升了代码生成表现。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散式大语言模型在代码生成任务中的训练和推理机制尚未被充分研究。

Method: 分析模型的去噪过程，提出了新的强化学习方法coupled-GRPO，用以优化训练时的采样方案并降低方差。

Result: DiffuCoder在代码生成基准测试中的性能得到显著提升，比如EvalPlus上提升了4.4%。

Conclusion: 此工作深入解析了扩散式大语言模型的生成机制，并提供了一种效果显著的新强化学习训练框架，有助于解锁其在代码生成中的潜力。

Abstract: Diffusion large language models (dLLMs) are compelling alternatives to
autoregressive (AR) models because their denoising models operate over the
entire sequence. The global planning and iterative refinement features of dLLMs
are particularly useful for code generation. However, current training and
inference mechanisms for dLLMs in coding are still under-explored. To demystify
the decoding behavior of dLLMs and unlock their potential for coding, we
systematically investigate their denoising processes and reinforcement learning
(RL) methods. We train a 7B dLLM, \textbf{DiffuCoder}, on 130B tokens of code.
Using this model as a testbed, we analyze its decoding behavior, revealing how
it differs from that of AR models: (1) dLLMs can decide how causal their
generation should be without relying on semi-AR decoding, and (2) increasing
the sampling temperature diversifies not only token choices but also their
generation order. This diversity creates a rich search space for RL rollouts.
For RL training, to reduce the variance of token log-likelihood estimates and
maintain training efficiency, we propose \textbf{coupled-GRPO}, a novel
sampling scheme that constructs complementary mask noise for completions used
in training. In our experiments, coupled-GRPO significantly improves
DiffuCoder's performance on code generation benchmarks (+4.4\% on EvalPlus) and
reduces reliance on AR causal during decoding. Our work provides deeper insight
into the machinery of dLLM generation and offers an effective, diffusion-native
RL training framework. https://github.com/apple/ml-diffucoder.

</details>


### [161] [Memento: Note-Taking for Your Future Self](https://arxiv.org/abs/2506.20642)
*Chao Wan,Albert Gong,Mihir Mishra,Carl-Leander Henneking,Claas Beger,Kilian Q. Weinberger*

Main category: cs.CL

TL;DR: 引入了一种名为Memento的三阶段提示策略，以改进多跳问题回答中的推理与检索结合能力。


<details>
  <summary>Details</summary>
Motivation: 改进大语言模型在多跳问答任务中推理与检索结合能力不足的问题。

Method: 通过三阶段策略：分解复杂问题、动态构建事实数据库、整合事实以解决问题，提升现有提示策略的表现。

Result: 在PhantomWiki、2WikiMultiHopQA及MuSiQue等多个数据集上的F1表现显著提升，比现有方法提高3-20个百分点。

Conclusion: Memento策略提升了多跳问题回答任务中推理和检索结合性能，改善了大语言模型的任务表现。

Abstract: Large language models (LLMs) excel at reasoning-only tasks, but struggle when
reasoning must be tightly coupled with retrieval, as in multi-hop question
answering. To overcome these limitations, we introduce a prompting strategy
that first decomposes a complex question into smaller steps, then dynamically
constructs a database of facts using LLMs, and finally pieces these facts
together to solve the question. We show how this three-stage strategy, which we
call Memento, can boost the performance of existing prompting strategies across
diverse settings. On the 9-step PhantomWiki benchmark, Memento doubles the
performance of chain-of-thought (CoT) when all information is provided in
context. On the open-domain version of 2WikiMultiHopQA, CoT-RAG with Memento
improves over vanilla CoT-RAG by more than 20 F1 percentage points and over the
multi-hop RAG baseline, IRCoT, by more than 13 F1 percentage points. On the
challenging MuSiQue dataset, Memento improves ReAct by more than 3 F1
percentage points, demonstrating its utility in agentic settings.

</details>


### [162] [Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs](https://arxiv.org/abs/2506.20666)
*Sonia K. Murthy,Rosie Zhao,Jennifer Hu,Sham Kakade,Markus Wulfmeier,Peng Qian,Tomer Ullman*

Main category: cs.CL

TL;DR: 本文利用礼貌言语的认知模型来分析LLMs在社交情境中如何处理价值取舍，结果表明模型倾向于信息效用而非社交效用，并揭示了模型训练阶段对价值动态的显著影响。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在复杂社交情境中处理价值取舍的能力，特别是如何选择言语时平衡信息效用与社交效用。

Method: 通过一个礼貌言语的认知模型，分析LLMs在黑盒推理模型和RL微调模型中的价值取舍动态，并研究模型的训练阶段对价值取舍的影响。

Result: （1）推理模型表现出更高的信息效用而非社交效用；（2）开源模型（数学推理表现较强）展现了类似模式；（3）模型训练阶段的基础模型和预训练数据对价值权重的影响显著，优于反馈数据集或校准方法。

Conclusion: 本文方法适用于快速变化的LLM领域，能够对高级行为生成假设、优化推理模型的训练方案，并在训练中更好地控制值的权衡取舍。

Abstract: Navigating everyday social situations often requires juggling conflicting
goals, such as conveying a harsh truth, maintaining trust, all while still
being mindful of another person's feelings. These value trade-offs are an
integral part of human decision-making and language use, however, current tools
for interpreting such dynamic and multi-faceted notions of values in LLMs are
limited. In cognitive science, so-called "cognitive models" provide formal
accounts of these trade-offs in humans, by modeling the weighting of a
speaker's competing utility functions in choosing an action or utterance. In
this work, we use a leading cognitive model of polite speech to interpret the
extent to which LLMs represent human-like trade-offs. We apply this lens to
systematically evaluate value trade-offs in two encompassing model settings:
degrees of reasoning "effort" in frontier black-box models, and RL
post-training dynamics of open-source models. Our results highlight patterns of
higher informational utility than social utility in reasoning models, and in
open-source models shown to be stronger in mathematical reasoning. Our findings
from LLMs' training dynamics suggest large shifts in utility values early on in
training with persistent effects of the choice of base model and pretraining
data, compared to feedback dataset or alignment method. We show that our method
is responsive to diverse aspects of the rapidly evolving LLM landscape, with
insights for forming hypotheses about other high-level behaviors, shaping
training regimes for reasoning models, and better controlling trade-offs
between values during model training.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [163] [Prover Agent: An Agent-based Framework for Formal Mathematical Proofs](https://arxiv.org/abs/2506.19923)
*Kaito Baba,Chaoran Liu,Shuhei Kurita,Akiyoshi Sannai*

Main category: cs.AI

TL;DR: 研究设计了Prover Agent，将大语言模型与Lean形式证明助手集成，实现了自动定理证明的新进展，取得了86.1%的成功率。


<details>
  <summary>Details</summary>
Motivation: 提高小型语言模型在定理证明任务中的效率和成功率，减少样本预算。

Method: 结合大语言模型、形式证明助手Lean以及反馈机制，同时生成辅助引理优化证明策略。

Result: 在MiniF2F测试集上达到了86.1%的成功率，创造了使用小型语言模型的新标准。

Conclusion: Prover Agent有效促进了自动定理证明领域的发展，为解决复杂问题提供了新思路与工具。

Abstract: We present Prover Agent, a novel AI agent for automated theorem proving that
integrates large language models (LLMs) with a formal proof assistant, Lean.
Prover Agent coordinates an informal reasoning LLM, a formal prover model, and
feedback from Lean while also generating auxiliary lemmas to assist in
discovering the overall proof strategy. It achieves an 86.1% success rate on
the MiniF2F benchmark, establishing a new state-of-the-art among methods using
small language models (SLMs) with a much lower sample budget than previous
approaches. We also present case studies illustrating how these generated
lemmas contribute to solving challenging problems.

</details>


### [164] [Context Attribution with Multi-Armed Bandit Optimization](https://arxiv.org/abs/2506.19977)
*Deng Pan,Keerthiram Murugesan,Nuno Moniz,Nitesh Chawla*

Main category: cs.AI

TL;DR: 本文提出一种新的框架，将上下文归因问题形式化为组合多臂赌博（CMAB）问题，显著提高了生成问答系统的可解释性和查询效率。


<details>
  <summary>Details</summary>
Motivation: 为了构建可解释且可信的生成性问答系统，了解模型生成答案时哪些上下文部分起到关键作用是至关重要的。

Method: 将上下文片段视为一个臂，利用组合Thompson采样算法（CTS）在有限的查询预算下高效地探索上下文子集，定义基于归一化词元概率的奖励函数进行上下文归因。

Result: 与传统的基于扰动的归因方法（如SHAP）相比，该方法在大幅减少模型查询次数的情况下，仍然可以实现高质量的归因效果。

Conclusion: 新方法在保证归因精度的同时，提高了查询效率，对多种数据集和大语言模型均表现良好。

Abstract: Understanding which parts of the retrieved context contribute to a large
language model's generated answer is essential for building interpretable and
trustworthy generative QA systems. We propose a novel framework that formulates
context attribution as a combinatorial multi-armed bandit (CMAB) problem. Each
context segment is treated as a bandit arm, and we employ Combinatorial
Thompson Sampling (CTS) to efficiently explore the exponentially large space of
context subsets under a limited query budget. Our method defines a reward
function based on normalized token likelihoods, capturing how well a subset of
segments supports the original model response. Unlike traditional
perturbation-based attribution methods such as SHAP, which sample subsets
uniformly and incur high computational costs, our approach adaptively balances
exploration and exploitation by leveraging posterior estimates of segment
relevance. This leads to substantially improved query efficiency while
maintaining high attribution fidelity. Extensive experiments on diverse
datasets and LLMs demonstrate that our method achieves competitive attribution
quality with fewer model queries.

</details>


### [165] [QHackBench: Benchmarking Large Language Models for Quantum Code Generation Using PennyLane Hackathon Challenges](https://arxiv.org/abs/2506.20008)
*Abdul Basit,Minghao Shao,Haider Asif,Nouhaila Innan,Muhammad Kashif,Alberto Marchisio,Muhammad Shafique*

Main category: cs.AI

TL;DR: 本文探索了大语言模型在量子计算代码生成中的潜力，提出了一个新数据集QHackBench，并评估了多种技术对生成效果的影响。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在量子计算代码生成中的有效性，并解决该领域内现有探索不足的问题。

Method: 引入QHackBench数据集，采用基于PennyLane的代码生成进行评估，比较标准提示与增强型生成（RAG）等技术的表现，并提出一种多代理评估管道以改进结果。

Result: RAG增强模型在复杂量子算法生成中达到与标准提示相当的效果；多代理评估管道能显著提高错误解决的执行成功率。

Conclusion: 结果表明RAG与多代理策略在量子代码生成上有显著潜力。提供数据集和框架以推动量子编程领域的AI研究。

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated strong
potential in code generation, yet their effectiveness in quantum computing
remains underexplored. This paper benchmarks LLMs for PennyLane-based quantum
code generation using real-world challenges from the Quantum Hackathon (QHack).
We introduce QHackBench, a novel benchmark dataset derived from QHack
competitions, and evaluate model performance under vanilla prompting and
Retrieval-Augmented Generation (RAG). Our structured evaluation framework
assesses functional correctness, syntactic validity, and execution success
across varying challenge difficulties. Results indicate that RAG-enhanced
models, supplemented with an augmented PennyLane dataset, approximately
generate similar results as the standard prompting, particularly in complex
quantum algorithms. Additionally, we introduce a multi-agent evaluation
pipeline that iteratively refines incorrect solutions, further enhancing
execution success rates. To foster further research, we commit to publicly
releasing QHackBench, along with our evaluation framework and experimental
results, enabling continued advancements in AI-assisted quantum programming.

</details>


### [166] [Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks](https://arxiv.org/abs/2506.20009)
*Konstantinos Vrettos,Michail E. Klontzas*

Main category: cs.AI

TL;DR: 此研究开发了一种可定制的检索增强生成（RAG）框架，用于医疗任务，表现优于商用模型且能源消耗更低。


<details>
  <summary>Details</summary>
Motivation: 研究旨在应对AI在医疗领域使用中产生的隐私、安全及环境问题，寻找更高效且环保的替代方案。

Method: 开发并测试了一种监测能耗和CO2排放的RAG框架，评估其在多个开源大语言模型（LLMs）基础上的表现，并与商用模型比较。

Result: RAG模型在准确性和能耗表现上优于商用模型，其中基于llama3.1:8B的RAG模型在准确性和环境影响方面表现最佳。

Conclusion: 本研究表明，本地化LLMs可用于开发环保且准确性高的RAG模型，推动可持续AI发展以符合联合国可持续发展目标。

Abstract: Background The increasing adoption of Artificial Intelligence (AI) in
healthcare has sparked growing concerns about its environmental and ethical
implications. Commercial Large Language Models (LLMs), such as ChatGPT and
DeepSeek, require substantial resources, while the utilization of these systems
for medical purposes raises critical issues regarding patient privacy and
safety. Methods We developed a customizable Retrieval-Augmented Generation
(RAG) framework for medical tasks, which monitors its energy usage and CO2
emissions. This system was then used to create RAGs based on various
open-source LLMs. The tested models included both general purpose models like
llama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs
performance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs
o4-mini model. A dataset of medical questions was used for the evaluation.
Results Custom RAG models outperformed commercial models in accuracy and energy
consumption. The RAG model built on llama3.1:8B achieved the highest accuracy
(58.5%) and was significantly better than other models, including o4-mini and
DeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption
and CO2 footprint among all models, with a Performance per kWh of 0.52 and a
total CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x
times more accuracy points per kWh and 172% less electricity usage while
maintaining higher accuracy. Conclusion Our study demonstrates that local LLMs
can be leveraged to develop RAGs that outperform commercial, online LLMs in
medical tasks, while having a smaller environmental impact. Our modular
framework promotes sustainable AI development, reducing electricity usage and
aligning with the UNs Sustainable Development Goals.

</details>


### [167] [Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models](https://arxiv.org/abs/2506.20018)
*Zechun Deng,Ziwei Liu,Ziqian Bi,Junhao Song,Chia Xin Liang,Joe Yeong,Junfeng Hao*

Main category: cs.AI

TL;DR: 本文研究了利用低延迟AI模型的实时决策支持系统，关注AI与Edge-IoT技术的结合以及人机协作方法的发展。


<details>
  <summary>Details</summary>
Motivation: 研究探讨如何在资源有限的条件下利用大型语言模型改善决策支持，解决低资源和灵活性需求的难题。

Method: 分析DeLLMa技术、模型压缩方法及边缘设备分析能力的提升，并回顾相关开发策略和应用方向。

Result: 论述了通过技术发展实现更高效和灵活的AI支持系统的可行路径。

Conclusion: 为未来相关领域的突破提供了启示，体现了AI在实时决策支持领域的潜力。

Abstract: This paper investigates real-time decision support systems that leverage
low-latency AI models, bringing together recent progress in holistic AI-driven
decision tools, integration with Edge-IoT technologies, and approaches for
effective human-AI teamwork. It looks into how large language models can assist
decision-making, especially when resources are limited. The research also
examines the effects of technical developments such as DeLLMa, methods for
compressing models, and improvements for analytics on edge devices, while also
addressing issues like limited resources and the need for adaptable frameworks.
Through a detailed review, the paper offers practical perspectives on
development strategies and areas of application, adding to the field by
pointing out opportunities for more efficient and flexible AI-supported
systems. The conclusions set the stage for future breakthroughs in this
fast-changing area, highlighting how AI can reshape real-time decision support.

</details>


### [168] [Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning](https://arxiv.org/abs/2506.20020)
*Saloni Dash,Amélie Reymond,Emma S. Spiro,Aylin Caliskan*

Main category: cs.AI

TL;DR: 探讨了分配不同人物角色的LLMs是否会展示类似于人类的动机性推理及其对结果判断的影响。


<details>
  <summary>Details</summary>
Motivation: 研究探讨大语言模型是否会因分配身份角色而表现出类似人类的动机性推理行为，并评估其是否加剧了与身份一致的推理倾向。

Method: 分配8种人物角色给8个大语言模型，并测试其在两个推理任务（虚假信息标题真伪判别和科学性数据评估）中的表现。

Result: 具有分配身份角色的LLMs在真伪判断能力上下降了最多9%；政治性人物角色在与自身身份一致时，对枪支管控的科学证据评估正确率高出90%。传统的去偏方法基本无效。

Conclusion: 身份分配会导致LLMs表现出人类类似的动机性推理模式，这种现象难以通过常规去偏方法缓解，可能加剧身份一致性的偏见推理问题。

Abstract: Reasoning in humans is prone to biases due to underlying motivations like
identity protection, that undermine rational decision-making and judgment. This
motivated reasoning at a collective level can be detrimental to society when
debating critical issues such as human-driven climate change or vaccine safety,
and can further aggravate political polarization. Prior studies have reported
that large language models (LLMs) are also susceptible to human-like cognitive
biases, however, the extent to which LLMs selectively reason toward
identity-congruent conclusions remains largely unexplored. Here, we investigate
whether assigning 8 personas across 4 political and socio-demographic
attributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and
proprietary) across two reasoning tasks from human-subject studies -- veracity
discernment of misinformation headlines and evaluation of numeric scientific
evidence -- we find that persona-assigned LLMs have up to 9% reduced veracity
discernment relative to models without personas. Political personas
specifically, are up to 90% more likely to correctly evaluate scientific
evidence on gun control when the ground truth is congruent with their induced
political identity. Prompt-based debiasing methods are largely ineffective at
mitigating these effects. Taken together, our empirical findings are the first
to suggest that persona-assigned LLMs exhibit human-like motivated reasoning
that is hard to mitigate through conventional debiasing prompts -- raising
concerns of exacerbating identity-congruent reasoning in both LLMs and humans.

</details>


### [169] [DiaLLMs: EHR Enhanced Clinical Conversational System for Clinical Test Recommendation and Diagnosis Prediction](https://arxiv.org/abs/2506.20059)
*Weijieying Ren,Tianxiang Zhao,Lei Wang,Tianchun Wang,Vasant Honavar*

Main category: cs.AI

TL;DR: DiaLLM整合了电子健康记录（EHR）数据以改进医疗对话，支持临床测试推荐、结果解释和诊断预测。实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有医疗大语言模型主要聚焦于诊断推荐，忽略了EHR的重要性，限制了其临床应用。

Method: 提出DiaLLM，通过设计临床测试参考策略（CTR）和强化学习框架来整合EHR中的异质数据，并引入拒绝采样策略和特定奖励机制，提高诊断准确性和效率。

Result: 实验结果表明，DiaLLM在临床测试推荐和诊断预测方面优于其他基线模型。

Conclusion: DiaLLM通过整合EHR数据和优化模型结构，提升了与实际医疗实践的契合度，推动了医疗大模型的临床应用。

Abstract: Recent advances in Large Language Models (LLMs) have led to remarkable
progresses in medical consultation. However, existing medical LLMs overlook the
essential role of Electronic Health Records (EHR) and focus primarily on
diagnosis recommendation, limiting their clinical applicability. We propose
DiaLLM, the first medical LLM that integrates heterogeneous EHR data into
clinically grounded dialogues, enabling clinical test recommendation, result
interpretation, and diagnosis prediction to better align with real-world
medical practice. To construct clinically grounded dialogues from EHR, we
design a Clinical Test Reference (CTR) strategy that maps each clinical code to
its corresponding description and classifies test results as "normal" or
"abnormal". Additionally, DiaLLM employs a reinforcement learning framework for
evidence acquisition and automated diagnosis. To handle the large action space,
we introduce a reject sampling strategy to reduce redundancy and improve
exploration efficiency. Furthermore, a confirmation reward and a
class-sensitive diagnosis reward are designed to guide accurate diagnosis
prediction. Extensive experimental results demonstrate that DiaLLM outperforms
baselines in clinical test recommendation and diagnosis prediction.

</details>


### [170] [AI Copilots for Reproducibility in Science: A Case Study](https://arxiv.org/abs/2506.20130)
*Adrien Bibal,Steven N. Minton,Deborah Khider,Yolanda Gil*

Main category: cs.AI

TL;DR: 本文介绍了一个名为OpenPub的平台，该平台利用AI协助科研相关任务，主要通过Reproducibility Copilot来提升科学研究的可重复性，使实验重现时间显著缩短。


<details>
  <summary>Details</summary>
Motivation: 解决科学研究中独立复现实践的挑战，促进研究成果透明化、可访问性及再利用性。

Method: 设计并实现了Reproducibility Copilot，利用AI分析文献、代码和补充材料，生成结构化的Jupyter Notebooks并提供再现性改进建议。

Result: 实验表明平台能将研究再现时间从30小时减少到1小时，同时高效再现研究中的图表和结果，并能检测阻碍可重复性的因素，如参数缺失、未清楚记录的步骤及数据集不完整问题。

Conclusion: AI驱动的工具有助于减少科研重复性验证的负担，提升科学交流的透明性和可信性；模块化架构还可扩展至解决更多开放科学相关目标。

Abstract: Open science initiatives seek to make research outputs more transparent,
accessible, and reusable, but ensuring that published findings can be
independently reproduced remains a persistent challenge. This paper introduces
OpenPub, an AI-powered platform that supports researchers, reviewers, and
readers through a suite of modular copilots focused on key open science tasks.
In this work, we present the Reproducibility Copilot, which analyzes
manuscripts, code, and supplementary materials to generate structured Jupyter
Notebooks and recommendations aimed at facilitating computational, or "rote",
reproducibility. We conducted feasibility tests using previously studied
research papers with known reproducibility benchmarks. Results indicate that
OpenPub can substantially reduce reproduction time - from over 30 hours to
about 1 hour - while achieving high coverage of figures, tables, and results
suitable for computational reproduction. The system systematically detects
barriers to reproducibility, including missing hyperparameters, undocumented
preprocessing steps, and incomplete or inaccessible datasets. These findings
suggest that AI-driven tools can meaningfully reduce the burden of
reproducibility efforts and contribute to more transparent and verifiable
scientific communication. The modular copilot architecture also provides a
foundation for extending AI assistance to additional open science objectives
beyond reproducibility.

</details>


### [171] [Language Modeling by Language Models](https://arxiv.org/abs/2506.20249)
*Junyan Cheng,Peter Clark,Kyle Richardson*

Main category: cs.AI

TL;DR: 本文提出了一种以多智能体LLM为核心的研究发现系统Genesys，用于模拟从设计到验证的研究过程，通过层级缩放法，成功发现了1,162个新设计，其中1,062个完成验证，其最佳性能超越了已有模型。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用LLMs自动发现新型语言模型（LM）架构，通过模拟实际研究过程加速设计效率。

Method: 提出了一个名为Genesys的系统，使用多智能体LLM模拟研究流程，从构思到代码生成及验证；采用层级缩放法和遗传编程框架以优化设计流程。

Result: 共发现1,162种新模型设计，其中1,062种完成预训练验证，最佳设计在9个基准测试中6个超越了现有架构。此外，遗传编程比传统方法设计效率提升了约86%。

Conclusion: Genesys系统证明了通过多智能体LLM模拟研究过程的有效性，该框架可用来实现语言模型设计的自动化与优化，且优于多种现有模型。

Abstract: Can we leverage LLMs to model the process of discovering novel language model
(LM) architectures? Inspired by real research, we propose a multi-agent LLM
approach that simulates the conventional stages of research, from ideation and
literature search (proposal stage) to design implementation (code generation),
generative pre-training, and downstream evaluation (verification). Using ideas
from scaling laws, our system, Genesys, employs a Ladder of Scales approach;
new designs are proposed, adversarially reviewed, implemented, and selectively
verified at increasingly larger model scales (14M$\sim$350M parameters) with a
narrowing budget (the number of models we can train at each scale). To help
make discovery efficient and factorizable, Genesys uses a novel genetic
programming backbone, which we show has empirical advantages over commonly used
direct prompt generation workflows (e.g., $\sim$86\% percentage point
improvement in successful design generation, a key bottleneck). We report
experiments involving 1,162 newly discovered designs (1,062 fully verified
through pre-training) and find the best designs to be highly competitive with
known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common
benchmarks). We couple these results with comprehensive system-level ablations
and formal results, which give broader insights into the design of effective
autonomous discovery systems.

</details>


### [172] [Enterprise Large Language Model Evaluation Benchmark](https://arxiv.org/abs/2506.20274)
*Liya Wang,David Yi,Damien Jose,John Passarelli,James Gao,Jordan Leventis,Kang Li*

Main category: cs.AI

TL;DR: 提出了一个基于Bloom认知分类学的14任务框架，用于评估大型语言模型（LLM）在企业环境中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试如MMLU无法充分评估企业特定任务的复杂性，因此需要一个新的基准框架来全面测试LLM在企业中的能力表现。

Method: 开发一个包含LLM-as-a-Labeler、LLM-as-a-Judge和纠错检索增强生成（CRAG）的可扩展流水线，生成9700样本的基准测试集，并对六个主流模型进行了评估对比。

Result: 开放源码模型如DeepSeek R1在推理任务中表现接近专有模型，但在需要判断能力的任务中稍逊一筹，主要受限于“过度思考”。

Conclusion: 该研究为企业定制模型评估提供了蓝图，揭示了当前企业部署LLM时的关键性能瓶颈，并为其优化提供了实用建议。

Abstract: Large Language Models (LLMs) ) have demonstrated promise in boosting
productivity across AI-powered tools, yet existing benchmarks like Massive
Multitask Language Understanding (MMLU) inadequately assess enterprise-specific
task complexities. We propose a 14-task framework grounded in Bloom's Taxonomy
to holistically evaluate LLM capabilities in enterprise contexts. To address
challenges of noisy data and costly annotation, we develop a scalable pipeline
combining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented
generation (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six
leading models shows open-source contenders like DeepSeek R1 rival proprietary
models in reasoning tasks but lag in judgment-based scenarios, likely due to
overthinking. Our benchmark reveals critical enterprise performance gaps and
offers actionable insights for model optimization. This work provides
enterprises a blueprint for tailored evaluations and advances practical LLM
deployment.

</details>


### [173] [Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards](https://arxiv.org/abs/2506.20332)
*Jihao Gu,Qihang Ai,Yingyao Wang,Pi Bu,Jingxuan Xing,Zekun Zhu,Wei Jiang,Ziming Wang,Yingxiu Zhao,Ming-Liang Zhang,Jun Song,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: 提出Mobile-R1，一种通过交互式多轮强化学习和任务级奖励优化的移动智能体方法，并提供包含28个中文应用的高质量数据集和基准。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言模型的移动智能体在环境交互中的探索和错误纠正能力有限，导致模型可能陷入局部最优。

Method: 提出Mobile-R1方法，通过三阶段训练框架：初始格式微调、基于动作级奖励的单步在线训练，及基于多轮轨迹的任务级奖励在线训练，提升探索能力。

Result: Mobile-R1显著改善了移动智能体的性能，增强了探索和错误纠正能力，并收集了包含28个中文应用和24,521条人工注释的数据集。

Conclusion: 通过开放源码和资源，Mobile-R1为移动智能体研究提供了新工具，并推动了多轮强化学习在该领域的应用与发展。

Abstract: Vision-language model-based mobile agents have gained the ability to not only
understand complex instructions and mobile screenshots, but also optimize their
action outputs via thinking and reasoning, benefiting from reinforcement
learning, such as Group Relative Policy Optimization (GRPO). However, existing
research centers on offline reinforcement learning training or online
optimization using action-level rewards, which limits the agent's dynamic
interaction with the environment. This often results in agents settling into
local optima, thereby weakening their ability for exploration and error action
correction. To address these challenges, we introduce an approach called
Mobile-R1, which employs interactive multi-turn reinforcement learning with
task-level rewards for mobile agents. Our training framework consists of three
stages: initial format finetuning, single-step online training via action-level
reward, followed by online training via task-level reward based on multi-turn
trajectories. This strategy is designed to enhance the exploration and error
correction capabilities of Mobile-R1, leading to significant performance
improvements. Moreover, we have collected a dataset covering 28 Chinese
applications with 24,521 high-quality manual annotations and established a new
benchmark with 500 trajectories. We will open source all resources, including
the dataset, benchmark, model weight, and codes:
https://mobile-r1.github.io/Mobile-R1/.

</details>


### [174] [Tabular Feature Discovery With Reasoning Type Exploration](https://arxiv.org/abs/2506.20357)
*Sungwon Han,Sungkyu Park,Seungeon Lee*

Main category: cs.AI

TL;DR: 提出了REFeat，一种通过多种推理类型来指导大语言模型（LLM）生成多样化和有信息特征的方法，用于表格数据特征工程。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在特征生成中过于简单或重复，缺乏结构性推理指导。本研究旨在改善这种不足，提升生成特征的质量和多样性。

Method: 在REFeat中，通过多种推理方式指导LLM进行特征生成，从而避开单一转化产生的偏差，为表格数据建立更丰富的特征。

Result: 在对59个基准数据集的实验中，REFeat平均上实现了更高的预测准确性，并生成了更为多样和有意义的特征。

Conclusion: 通过结合丰富的推理框架和自适应策略选择，REFeat展示了LLM作为表格数据特征生成工具的巨大潜力。

Abstract: Feature engineering for tabular data remains a critical yet challenging step
in machine learning. Recently, large language models (LLMs) have been used to
automatically generate new features by leveraging their vast knowledge.
However, existing LLM-based approaches often produce overly simple or
repetitive features, partly due to inherent biases in the transformations the
LLM chooses and the lack of structured reasoning guidance during generation. In
this paper, we propose a novel method REFeat, which guides an LLM to discover
diverse and informative features by leveraging multiple types of reasoning to
steer the feature generation process. Experiments on 59 benchmark datasets
demonstrate that our approach not only achieves higher predictive accuracy on
average, but also discovers more diverse and meaningful features. These results
highlight the promise of incorporating rich reasoning paradigms and adaptive
strategy selection into LLM-driven feature discovery for tabular data.

</details>


### [175] [Paladin-mini: A Compact and Efficient Grounding Model Excelling in Real-World Scenarios](https://arxiv.org/abs/2506.20384)
*Dror Ivry,Oran Nahum*

Main category: cs.AI

TL;DR: 本文提出一种解决基于语境中的主张与其支持证据进行匹配的模型及评估基准。


<details>
  <summary>Details</summary>
Motivation: 解决如何验证某一主张是否在特定语境中有支持证据的问题。

Method: 提出Paladin-mini模型（一种紧凑型开源分类器模型）以及grounding-benchmark评估数据集。

Result: Paladin-mini模型在评估基准上的表现优于现有的最新技术，实验结果明确且可复现。

Conclusion: Paladin-mini模型与新的评测基准有望为验证主张是否有根据提供有力工具。

Abstract: This paper introduces two significant contributions to address the issue of
grounding claims in a given context. Grounding means that given a context
(document) and a claim, there's at least one supportive evidence for the claim
in the document. We will introduce Paladin-mini, a compact (3.8B parameters)
open-source classifier model (used for labeling data as grounded or ungrounded)
engineered for robust performance in real-world scenarios, and the
grounding-benchmark, a new evaluation dataset designed to assess performance on
critical reasoning tasks. We'll also demonstrate the results of Paladin-mini
with benchmarks against the current State-of-the-art and share clear and
reproducible results.

</details>


### [176] [Smart Ride and Delivery Services with Electric Vehicles: Leveraging Bidirectional Charging for Profit Optimisation](https://arxiv.org/abs/2506.20401)
*Jinchun Du,Bojie Shen,Muhammad Aamir Cheema,Adel N. Toosi*

Main category: cs.AI

TL;DR: 这篇论文研究了电动车(EV)集成到现代服务体系中的问题，提出了一种整合车网互动(V2G)技术的新模型，优化司机选择订单及充放电策略以最大化利润。


<details>
  <summary>Details</summary>
Motivation: 随着电动车(EV)的普及，对如何将其高效整合到现代服务系统中的需求与日俱增。特别是，V2G技术的进步为电动车在支持电网的同时创造收入提出了新的挑战和机遇。

Method: 作者提出了电动车路径选择问题与车网互动技术(EVOP-V2G)，并将其建模为混合整数规划(MIP)问题，开发了两种高效的元启发式算法：一个基于进化算法(EA)，另一个基于大邻域搜索(LNS)的算法。

Result: 实验表明，这些算法在真实数据上的表现可以使司机利润翻倍，且在小规模场景下接近最优解，在大规模问题上表现出良好的扩展性。

Conclusion: 本研究展示了一种智能且盈利的电动车移动性系统路径，结合V2G技术，不仅优化了司机利益，还增强了对电网的支持。

Abstract: With the rising popularity of electric vehicles (EVs), modern service
systems, such as ride-hailing delivery services, are increasingly integrating
EVs into their operations. Unlike conventional vehicles, EVs often have a
shorter driving range, necessitating careful consideration of charging when
fulfilling requests. With recent advances in Vehicle-to-Grid (V2G) technology -
allowing EVs to also discharge energy back to the grid - new opportunities and
complexities emerge. We introduce the Electric Vehicle Orienteering Problem
with V2G (EVOP-V2G): a profit-maximization problem where EV drivers must select
customer requests or orders while managing when and where to charge or
discharge. This involves navigating dynamic electricity prices, charging
station selection, and route constraints. We formulate the problem as a Mixed
Integer Programming (MIP) model and propose two near-optimal metaheuristic
algorithms: one evolutionary (EA) and the other based on large neighborhood
search (LNS). Experiments on real-world data show our methods can double driver
profits compared to baselines, while maintaining near-optimal performance on
small instances and excellent scalability on larger ones. Our work highlights a
promising path toward smarter, more profitable EV-based mobility systems that
actively support the energy grid.

</details>


### [177] [GymPN: A Library for Decision-Making in Process Management Systems](https://arxiv.org/abs/2506.20404)
*Riccardo Lo Bianco,Willem van Jaarsveld,Remco Dijkman*

Main category: cs.AI

TL;DR: 本文提出一种名为GymPN的软件库，使用深度强化学习优化业务过程中的决策过程，弥补了以往工作中的局限性。


<details>
  <summary>Details</summary>
Motivation: 实现更优化的任务分配决策，以应对现实业务流程中的复杂需求。

Method: 采用深度强化学习支持业务流程中的多项决策，并引入部分流程可见性模型。

Result: 通过在八种典型问题模式中的实验结果，证明了GymPN能够有效建模并学习最优决策策略。

Conclusion: GymPN 软件库为业务流程中的复杂决策提供了高效建模与学习支持，在实际场景中表现出了优越的效果。

Abstract: Process management systems support key decisions about the way work is
allocated in organizations. This includes decisions on which task to perform
next, when to execute the task, and who to assign the task to. Suitable
software tools are required to support these decisions in a way that is optimal
for the organization. This paper presents a software library, called GymPN,
that supports optimal decision-making in business processes using Deep
Reinforcement Learning. GymPN builds on previous work that supports task
assignment in business processes, introducing two key novelties: support for
partial process observability and the ability to model multiple decisions in a
business process. These novel elements address fundamental limitations of
previous work and thus enable the representation of more realistic process
decisions. We evaluate the library on eight typical business process
decision-making problem patterns, showing that GymPN allows for easy modeling
of the desired problems, as well as learning optimal decision policies.

</details>


### [178] [Mixtures of Neural Cellular Automata: A Stochastic Framework for Growth Modelling and Self-Organization](https://arxiv.org/abs/2506.20486)
*Salvatore Milite,Giulio Caravagna,Andrea Sottoriva*

Main category: cs.AI

TL;DR: 本文提出混合神经元胞自动机（MNCA），通过引入概率规则分配和内在噪声，来更好地模拟生物过程中的随机动态。


<details>
  <summary>Details</summary>
Motivation: 现有的神经元胞自动机（NCA）因其确定性特性无法有效捕捉真实世界生物和物理系统中的随机性。

Method: 提出一种混合神经元胞自动机（MNCA）框架，将混合模型的概念引入NCA，通过加入概率规则和内在噪声来实现多样化的局部行为。

Result: 在组织生长、图像形态生成和显微镜图像分割等领域验证了MNCA的有效性，其在抗干扰性、生物生长模式复现以及规则分割解释性方面表现优异。

Conclusion: MNCAs能够更好地模拟随机动力系统，并在研究自我生长过程方面具有很大潜力。

Abstract: Neural Cellular Automata (NCAs) are a promising new approach to model
self-organizing processes, with potential applications in life science.
However, their deterministic nature limits their ability to capture the
stochasticity of real-world biological and physical systems.
  We propose the Mixture of Neural Cellular Automata (MNCA), a novel framework
incorporating the idea of mixture models into the NCA paradigm. By combining
probabilistic rule assignments with intrinsic noise, MNCAs can model diverse
local behaviors and reproduce the stochastic dynamics observed in biological
processes.
  We evaluate the effectiveness of MNCAs in three key domains: (1) synthetic
simulations of tissue growth and differentiation, (2) image morphogenesis
robustness, and (3) microscopy image segmentation. Results show that MNCAs
achieve superior robustness to perturbations, better recapitulate real
biological growth patterns, and provide interpretable rule segmentation. These
findings position MNCAs as a promising tool for modeling stochastic dynamical
systems and studying self-growth processes.

</details>


### [179] [Engineering Sentience](https://arxiv.org/abs/2506.20504)
*Konstantin Demin,Taylor Webb,Eric Elmoznino,Hakwan Lau*

Main category: cs.AI

TL;DR: 本文提出了一个关于如何在机器中设计和构建“感知能力”的定义，并讨论了具体功能和计算层面上的细节，以及可能的实现方法。


<details>
  <summary>Details</summary>
Motivation: 界定机器感知能力的含义，明确其在AI中的作用，为避免或及时识别功能性有感知力的人工代理创建提供依据。

Method: 提出了一种结合持久性和质感属性的感知定义，并从功能和计算的角度提出实现建议。

Result: 提供了在当前技术条件下实现功能型感知概念的框架，并强调了监测是否无意中创建感知代理的重要性。

Conclusion: 机器感知能力的构建需要满足具体功能和主观感知属性的结合，为理解和合理设计AI的感知提供了新视角。

Abstract: We spell out a definition of sentience that may be useful for designing and
building it in machines. We propose that for sentience to be meaningful for AI,
it must be fleshed out in functional, computational terms, in enough detail to
allow for implementation. Yet, this notion of sentience must also reflect
something essentially 'subjective', beyond just having the general capacity to
encode perceptual content. For this specific functional notion of sentience to
occur, we propose that certain sensory signals need to be both assertoric
(persistent) and qualitative. To illustrate the definition in more concrete
terms, we sketch out some ways for potential implementation, given current
technology. Understanding what it takes for artificial agents to be
functionally sentient can also help us avoid creating them inadvertently, or at
least, realize that we have created them in a timely manner.

</details>


### [180] [Case-based Reasoning Augmented Large Language Model Framework for Decision Making in Realistic Safety-Critical Driving Scenarios](https://arxiv.org/abs/2506.20531)
*Wenbin Gan,Minh-Son Dao,Koji Zettsu*

Main category: cs.AI

TL;DR: 本文提出了一个基于案例推理的大型语言模型框架（CBR-LLM），用于在复杂高风险场景中进行规避操作决策，并验证了其在多个模型上的有效性。


<details>
  <summary>Details</summary>
Motivation: 驱动安全关键场景要求快速且符合情境的决策，传统的大型语言模型应用于自动驾驶存在适应性和知识不足问题。

Method: 通过将行车视频输入的语义场景理解与相关案例检索相结合，使得大型语言模型能够生成情境敏感且符合人类决策的规避操作建议。

Result: 所提框架提高了决策准确性、解释质量，与专家行为更一致，并在多种风险场景中表现优异。

Conclusion: CBR-LLM展现了强大的适应性和鲁棒性，可作为智能驾驶系统中可靠的决策支持工具。

Abstract: Driving in safety-critical scenarios requires quick, context-aware
decision-making grounded in both situational understanding and experiential
reasoning. Large Language Models (LLMs), with their powerful general-purpose
reasoning capabilities, offer a promising foundation for such decision-making.
However, their direct application to autonomous driving remains limited due to
challenges in domain adaptation, contextual grounding, and the lack of
experiential knowledge needed to make reliable and interpretable decisions in
dynamic, high-risk environments. To address this gap, this paper presents a
Case-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for
evasive maneuver decision-making in complex risk scenarios. Our approach
integrates semantic scene understanding from dashcam video inputs with the
retrieval of relevant past driving cases, enabling LLMs to generate maneuver
recommendations that are both context-sensitive and human-aligned. Experiments
across multiple open-source LLMs show that our framework improves decision
accuracy, justification quality, and alignment with human expert behavior.
Risk-aware prompting strategies further enhance performance across diverse risk
types, while similarity-based case retrieval consistently outperforms random
sampling in guiding in-context learning. Case studies further demonstrate the
framework's robustness in challenging real-world conditions, underscoring its
potential as an adaptive and trustworthy decision-support tool for intelligent
driving systems.

</details>


### [181] [Fine-Tuning and Prompt Engineering of LLMs, for the Creation of Multi-Agent AI for Addressing Sustainable Protein Production Challenges](https://arxiv.org/abs/2506.20598)
*Alexander D. Kalian,Jaewook Lee,Stefan P. Johannesson,Lennart Otte,Christer Hogstrand,Miao Guo*

Main category: cs.AI

TL;DR: 本研究提出了一种基于多智能体人工智能(AI)框架的概念验证系统，旨在支持可持续蛋白质生产研究，尤其聚焦于微生物蛋白源的探索和提取。


<details>
  <summary>Details</summary>
Motivation: 为了解决全球对可持续蛋白质来源的需求，开发快速处理和综合领域特定科学知识的智能工具。

Method: 采用基于RAG的系统，由两个基于GPT的LLM智能体组成：一个用于文献检索，另一个用于科学信息提取。此外，探索了微调和提示工程两种并行优化方式。

Result: 实验表明，微调和提示工程都能提升信息提取智能体的表现。微调使平均余弦相似度评分达到≥0.94，提示工程尽管表现稍低但统计不确定性也较低。

Conclusion: 框架优化后显著提高了系统性能，且开发了用户界面以支持实际使用，并进行了化学安全搜索功能的初步探索。

Abstract: The global demand for sustainable protein sources has accelerated the need
for intelligent tools that can rapidly process and synthesise domain-specific
scientific knowledge. In this study, we present a proof-of-concept multi-agent
Artificial Intelligence (AI) framework designed to support sustainable protein
production research, with an initial focus on microbial protein sources. Our
Retrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based
LLM agents: (1) a literature search agent that retrieves relevant scientific
literature on microbial protein production for a specified microbial strain,
and (2) an information extraction agent that processes the retrieved content to
extract relevant biological and chemical information. Two parallel
methodologies, fine-tuning and prompt engineering, were explored for agent
optimisation. Both methods demonstrated effectiveness at improving the
performance of the information extraction agent in terms of transformer-based
cosine similarity scores between obtained and ideal outputs. Mean cosine
similarity scores were increased by up to 25%, while universally reaching mean
scores of $\geq 0.89$ against ideal output text. Fine-tuning overall improved
the mean scores to a greater extent (consistently of $\geq 0.94$) compared to
prompt engineering, although lower statistical uncertainties were observed with
the latter approach. A user interface was developed and published for enabling
the use of the multi-agent AI system, alongside preliminary exploration of
additional chemical safety-based search capabilities

</details>


### [182] [CogGen: A Learner-Centered Generative AI Architecture for Intelligent Tutoring with Programming Video](https://arxiv.org/abs/2506.20600)
*Wengxi Li,Roy Pea,Nick Haber,Hari Subramonyam*

Main category: cs.AI

TL;DR: 本文提出了CogGen，一种基于认知学徒框架的AI架构，用于将编程视频转化为互动和自适应的学习体验。


<details>
  <summary>Details</summary>
Motivation: 针对现有编程教育中视频学习方式缺乏互动性和个性化的问题，提出一种结合学生建模与生成式AI的解决方案。

Method: 通过视频分段、对话式辅导引擎及基于贝叶斯知识追踪的学生模型，采用认知学徒策略实现自适应教学。

Result: 技术评估表明分段效果准确，且在知识、方法、行动及互动层面具有良好的教学对齐性；消融研究证明各组件对指导效果的重要性。

Conclusion: 该研究通过结合结构化学生建模与交互式AI对话，推进了AI赋能辅导的能力，提供了一种可扩展的方式提升编程视频教育体验。

Abstract: We introduce CogGen, a learner-centered AI architecture that transforms
programming videos into interactive, adaptive learning experiences by
integrating student modeling with generative AI tutoring based on the Cognitive
Apprenticeship framework. The architecture consists of three components: (1)
video segmentation by learning goals, (2) a conversational tutoring engine
applying Cognitive Apprenticeship strategies, and (3) a student model using
Bayesian Knowledge Tracing to adapt instruction. Our technical evaluation
demonstrates effective video segmentation accuracy and strong pedagogical
alignment across knowledge, method, action, and interaction layers. Ablation
studies confirm the necessity of each component in generating effective
guidance. This work advances AI-powered tutoring by bridging structured student
modeling with interactive AI conversations, offering a scalable approach to
enhancing video-based programming education.

</details>


### [183] [AI Assistants to Enhance and Exploit the PETSc Knowledge Base](https://arxiv.org/abs/2506.20608)
*Barry Smith,Junchao Zhang,Hong Zhang,Lois Curfman McInnes,Murat Keceli,Archit Vasan,Satish Balay,Toby Isaac,Le Chen,Venkatram Vishwanath*

Main category: cs.AI

TL;DR: 通过结合PETSc内容与自定义的LLM工具构建一个智能系统，该系统旨在提升科学计算中知识的获取和利用效率。


<details>
  <summary>Details</summary>
Motivation: 现有的PETSc知识库分散且难以获取，需要新的解决方案来整合和激活这些信息，促进知识在用户与开发者之间的无缝传递。

Method: 构建了一个基于LLM的系统，通过结合PETSc相关内容与算法工具（如RAG、重排序算法和聊天机器人），对知识进行有效提取与再利用，并设计了一套评价及界面优化方法。

Result: 初步表明，该系统可以有效提高数值软件（如Krylov求解器）的开发与使用，特别是在数据检索和信息传达方面有显著提升。

Conclusion: 提出了一种知识中心的AI扩展框架，为科学软件提供更优支持和增强工作流，并计划进一步将其发展为科学发现的强大平台。

Abstract: Generative AI, especially through large language models (LLMs), is
transforming how technical knowledge can be accessed, reused, and extended.
PETSc, a widely used numerical library for high-performance scientific
computing, has accumulated a rich but fragmented knowledge base over its three
decades of development, spanning source code, documentation, mailing lists,
GitLab issues, Discord conversations, technical papers, and more. Much of this
knowledge remains informal and inaccessible to users and new developers. To
activate and utilize this knowledge base more effectively, the PETSc team has
begun building an LLM-powered system that combines PETSc content with custom
LLM tools -- including retrieval-augmented generation (RAG), reranking
algorithms, and chatbots -- to assist users, support developers, and propose
updates to formal documentation. This paper presents initial experiences
designing and evaluating these tools, focusing on system architecture, using
RAG and reranking for PETSc-specific information, evaluation methodologies for
various LLMs and embedding models, and user interface design. Leveraging the
Argonne Leadership Computing Facility resources, we analyze how LLM responses
can enhance the development and use of numerical software, with an initial
focus on scalable Krylov solvers. Our goal is to establish an extensible
framework for knowledge-centered AI in scientific software, enabling scalable
support, enriched documentation, and enhanced workflows for research and
development. We conclude by outlining directions for expanding this system into
a robust, evolving platform that advances software ecosystems to accelerate
scientific discovery.

</details>


### [184] [Towards Community-Driven Agents for Machine Learning Engineering](https://arxiv.org/abs/2506.20640)
*Sijie Li,Weiwei Sun,Shanda Li,Ameet Talwalkar,Yiming Yang*

Main category: cs.AI

TL;DR: 提出了MLE-Live框架和CoMind代理，用于利用集体知识解决ML研究问题。CoMind在MLE-Live和Kaggle比赛中表现优异。


<details>
  <summary>Details</summary>
Motivation: 自动化ML研究需要更好地与研究社区互动，从中获取见解。

Method: 引入MLE-Live评估框架，并提出CoMind代理以促进社区中洞见的交流和解决方案的生成。

Result: CoMind在MLE-Live中达到了最先进的性能，并在Kaggle比赛中平均超越了79.2%的人工对手。

Conclusion: CoMind展示了与集体知识交互的能力，有助于推动自动化ML研究的发展。

Abstract: Large language model-based machine learning (ML) agents have shown great
promise in automating ML research. However, existing agents typically operate
in isolation on a given research problem, without engaging with the broader
research community, where human researchers often gain insights and contribute
by sharing knowledge. To bridge this gap, we introduce MLE-Live, a live
evaluation framework designed to assess an agent's ability to communicate with
and leverage collective knowledge from a simulated Kaggle research community.
Building on this framework, we propose CoMind, a novel agent that excels at
exchanging insights and developing novel solutions within a community context.
CoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2%
human competitors on average across four ongoing Kaggle competitions. Our code
is released at https://github.com/comind-ml/CoMind.

</details>


### [185] [The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind](https://arxiv.org/abs/2506.20664)
*Andrei Lupu,Timon Willi,Jakob Foerster*

Main category: cs.AI

TL;DR: 本文提出了一个名为Decrypto的游戏化基准，用于评估大型语言模型（LLMs）的多智能体推理能力和心理理论（ToM）能力，克服了现有基准的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs具备越来越多的代理能力，其需要在复杂的多智能体场景中与人类用户和其他代理互动，对心理理论（ToM）的能力需求日益增强，然而现有评估方法存在多种缺陷。

Method: 通过借鉴认知科学、计算推理学和多智能体强化学习，设计了一个名为Decrypto的交互式、多维度的游戏基准，来测试多智能体推理和ToM能力，并通过验证实验支持基准设计的有效性。

Result: 前沿LLMs在Decrypto中表现落后于人类和简单的词嵌入基线。在复现经典认知科学实验的过程中，发现最新的推理模型在关键ToM能力上表现显著落后于过往模型。

Conclusion: Decrypto填补了现有推理和心理理论评估的关键空白，为改进人工代理能力铺平了道路。

Abstract: As Large Language Models (LLMs) gain agentic abilities, they will have to
navigate complex multi-agent scenarios, interacting with human users and other
agents in cooperative and competitive settings. This will require new reasoning
skills, chief amongst them being theory of mind (ToM), or the ability to reason
about the "mental" states of other agents. However, ToM and other multi-agent
abilities in LLMs are poorly understood, since existing benchmarks suffer from
narrow scope, data leakage, saturation, and lack of interactivity. We thus
propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM
drawing inspiration from cognitive science, computational pragmatics and
multi-agent reinforcement learning. It is designed to be as easy as possible in
all other dimensions, eliminating confounding factors commonly found in other
benchmarks. To our knowledge, it is also the first platform for designing
interactive ToM experiments.
  We validate the benchmark design through comprehensive empirical evaluations
of frontier LLMs, robustness studies, and human-AI cross-play experiments. We
find that LLM game-playing abilities lag behind humans and simple
word-embedding baselines. We then create variants of two classic cognitive
science experiments within Decrypto to evaluate three key ToM abilities.
Surprisingly, we find that state-of-the-art reasoning models are significantly
worse at those tasks than their older counterparts. This demonstrates that
Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and
paves the path towards better artificial agents.

</details>


### [186] [Prover Agent: An Agent-based Framework for Formal Mathematical Proofs](https://arxiv.org/abs/2506.19923)
*Kaito Baba,Chaoran Liu,Shuhei Kurita,Akiyoshi Sannai*

Main category: cs.AI

TL;DR: 本文提出Prover Agent，将大语言模型(LLMs)与形式化证明助手Lean结合，用于自动定理证明，在MiniF2F基准测试中达到了86.1%的成功率。


<details>
  <summary>Details</summary>
Motivation: 希望通过整合非正式推理和形式化证明，提升小型语言模型(SLMs)在自动定理证明中的能力并降低样本预算。

Method: 开发Prover Agent，将非正式推理大语言模型、形式证明模型和Lean助手反馈相结合，同时生成辅助引理来帮助发现整体证明策略。

Result: 在MiniF2F基准测试中，Prover Agent达到了86.1%的成功率，比现有使用SLM的方法具更低的样本预算的新性能记录。

Conclusion: Prover Agent展示了生成引理对解决复杂问题的有效性，为自动定理证明提供了新颖且高效的解决方案。

Abstract: We present Prover Agent, a novel AI agent for automated theorem proving that
integrates large language models (LLMs) with a formal proof assistant, Lean.
Prover Agent coordinates an informal reasoning LLM, a formal prover model, and
feedback from Lean while also generating auxiliary lemmas to assist in
discovering the overall proof strategy. It achieves an 86.1% success rate on
the MiniF2F benchmark, establishing a new state-of-the-art among methods using
small language models (SLMs) with a much lower sample budget than previous
approaches. We also present case studies illustrating how these generated
lemmas contribute to solving challenging problems.

</details>


### [187] [Context Attribution with Multi-Armed Bandit Optimization](https://arxiv.org/abs/2506.19977)
*Deng Pan,Keerthiram Murugesan,Nuno Moniz,Nitesh Chawla*

Main category: cs.AI

TL;DR: 提出了一种利用组合多臂赌博模型框架的新方法，用于提高生成式问答系统的可解释性，旨在识别对于生成答案起关键作用的上下文片段。


<details>
  <summary>Details</summary>
Motivation: 目前生成式问答系统缺乏对生成答案的关键上下文贡献部分的清楚解释，难以提升这些模型的可解释性和信任度。

Method: 将上下文归因问题形式化为组合多臂赌博问题，通过组合Thompson采样方法，在受限查询预算下高效探索上下文子集空间，并基于归一化的token概率值定义奖励函数。

Result: 新方法显著提高了查询效率，同时保持了高归因准确性，在多个数据集和大语言模型的实验中表现优异。

Conclusion: 该方法相比传统方法如SHAP，能够更高效地平衡探索与利用，提供更高效且精准的上下文归因分析工具。

Abstract: Understanding which parts of the retrieved context contribute to a large
language model's generated answer is essential for building interpretable and
trustworthy generative QA systems. We propose a novel framework that formulates
context attribution as a combinatorial multi-armed bandit (CMAB) problem. Each
context segment is treated as a bandit arm, and we employ Combinatorial
Thompson Sampling (CTS) to efficiently explore the exponentially large space of
context subsets under a limited query budget. Our method defines a reward
function based on normalized token likelihoods, capturing how well a subset of
segments supports the original model response. Unlike traditional
perturbation-based attribution methods such as SHAP, which sample subsets
uniformly and incur high computational costs, our approach adaptively balances
exploration and exploitation by leveraging posterior estimates of segment
relevance. This leads to substantially improved query efficiency while
maintaining high attribution fidelity. Extensive experiments on diverse
datasets and LLMs demonstrate that our method achieves competitive attribution
quality with fewer model queries.

</details>


### [188] [QHackBench: Benchmarking Large Language Models for Quantum Code Generation Using PennyLane Hackathon Challenges](https://arxiv.org/abs/2506.20008)
*Abdul Basit,Minghao Shao,Haider Asif,Nouhaila Innan,Muhammad Kashif,Alberto Marchisio,Muhammad Shafique*

Main category: cs.AI

TL;DR: 本文探讨了LLMs在量子计算代码生成的潜力，提出了QHackBench基准数据集，并评估了标准提示和RAG方法下模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究量子计算领域LLMs的应用潜力，填补现有研究在此方面的不足。

Method: 引入QHackBench数据集，评估LLMs在标准提示与RAG辅助下的代码生成能力，并采用多智能体评价方式优化结果。

Result: RAG增强模型能在复杂算法中生成与标准提示相似的结果，多智能体评估提升了解决方案的执行成功率。

Conclusion: QHackBench和相关框架的公开为AI助力量子编程开辟了新方向，可推动未来研究发展。

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated strong
potential in code generation, yet their effectiveness in quantum computing
remains underexplored. This paper benchmarks LLMs for PennyLane-based quantum
code generation using real-world challenges from the Quantum Hackathon (QHack).
We introduce QHackBench, a novel benchmark dataset derived from QHack
competitions, and evaluate model performance under vanilla prompting and
Retrieval-Augmented Generation (RAG). Our structured evaluation framework
assesses functional correctness, syntactic validity, and execution success
across varying challenge difficulties. Results indicate that RAG-enhanced
models, supplemented with an augmented PennyLane dataset, approximately
generate similar results as the standard prompting, particularly in complex
quantum algorithms. Additionally, we introduce a multi-agent evaluation
pipeline that iteratively refines incorrect solutions, further enhancing
execution success rates. To foster further research, we commit to publicly
releasing QHackBench, along with our evaluation framework and experimental
results, enabling continued advancements in AI-assisted quantum programming.

</details>


### [189] [Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks](https://arxiv.org/abs/2506.20009)
*Konstantinos Vrettos,Michail E. Klontzas*

Main category: cs.AI

TL;DR: 本文开发了一个用来医疗任务的检索增强生成（RAG）框架，相比于商业大语言模型，这种框架不仅提高了准确性，还减少了能源消耗和环境影响。


<details>
  <summary>Details</summary>
Motivation: 调查AI在医疗领域的能源消耗和隐私问题，开发一个既高效又环保的医疗任务模型。

Method: 开发定制化的检索增强生成（RAG）框架，基于开源大语言模型优化其性能，监测其能源使用和CO2排放，并与商用模型进行对比研究。

Result: 自定义RAG模型在准确性和能源使用上优于商业模型。其中，基于llama3.1:8B的RAG模型以58.5%的准确率和0.52的每千瓦时性能表现最佳，同时CO2排放和电量使用显著低于商业模型。

Conclusion: 研究证明，本地化的LLMs可以被开发为高效、低环境影响的RAG框架，用于医疗任务。这种框架支持UN可持续发展目标，推动环保AI的应用。

Abstract: Background The increasing adoption of Artificial Intelligence (AI) in
healthcare has sparked growing concerns about its environmental and ethical
implications. Commercial Large Language Models (LLMs), such as ChatGPT and
DeepSeek, require substantial resources, while the utilization of these systems
for medical purposes raises critical issues regarding patient privacy and
safety. Methods We developed a customizable Retrieval-Augmented Generation
(RAG) framework for medical tasks, which monitors its energy usage and CO2
emissions. This system was then used to create RAGs based on various
open-source LLMs. The tested models included both general purpose models like
llama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs
performance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs
o4-mini model. A dataset of medical questions was used for the evaluation.
Results Custom RAG models outperformed commercial models in accuracy and energy
consumption. The RAG model built on llama3.1:8B achieved the highest accuracy
(58.5%) and was significantly better than other models, including o4-mini and
DeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption
and CO2 footprint among all models, with a Performance per kWh of 0.52 and a
total CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x
times more accuracy points per kWh and 172% less electricity usage while
maintaining higher accuracy. Conclusion Our study demonstrates that local LLMs
can be leveraged to develop RAGs that outperform commercial, online LLMs in
medical tasks, while having a smaller environmental impact. Our modular
framework promotes sustainable AI development, reducing electricity usage and
aligning with the UNs Sustainable Development Goals.

</details>


### [190] [Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models](https://arxiv.org/abs/2506.20018)
*Zechun Deng,Ziwei Liu,Ziqian Bi,Junhao Song,Chia Xin Liang,Joe Yeong,Junfeng Hao*

Main category: cs.AI

TL;DR: 本文探讨了低延迟AI模型的实时决策支持系统，结合了AI驱动工具、与边缘物联网技术的集成和人机协同的方式。


<details>
  <summary>Details</summary>
Motivation: 旨在研究如何在有限资源下通过大型语言模型和技术进步（如模型压缩和边缘设备分析）优化决策支持系统的性能与灵活性。

Method: 通过对AI模型、边缘物联网技术及其结合方式的详细回顾与分析，提出了开发策略并揭示应用场景。

Result: 文章指出了开发高效灵活的AI支持系统的关键机会，并提出了相关的发展策略。

Conclusion: 这种AI应用有助于改变实时决策支持方式，并为未来技术突破奠定基础。

Abstract: This paper investigates real-time decision support systems that leverage
low-latency AI models, bringing together recent progress in holistic AI-driven
decision tools, integration with Edge-IoT technologies, and approaches for
effective human-AI teamwork. It looks into how large language models can assist
decision-making, especially when resources are limited. The research also
examines the effects of technical developments such as DeLLMa, methods for
compressing models, and improvements for analytics on edge devices, while also
addressing issues like limited resources and the need for adaptable frameworks.
Through a detailed review, the paper offers practical perspectives on
development strategies and areas of application, adding to the field by
pointing out opportunities for more efficient and flexible AI-supported
systems. The conclusions set the stage for future breakthroughs in this
fast-changing area, highlighting how AI can reshape real-time decision support.

</details>


### [191] [Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning](https://arxiv.org/abs/2506.20020)
*Saloni Dash,Amélie Reymond,Emma S. Spiro,Aylin Caliskan*

Main category: cs.AI

TL;DR: 本文研究了赋予大语言模型(LLMs)特定身份认知时，是否会诱发类似人类的动机性推理，并发现这种动机性推理会对模型的判断力产生负面影响，同时传统的去偏方法难以有效缓解这些效应。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型是否会因赋予特定身份而表现出类似人类的动机性推理，从而评估这种现象对判断力和社会性问题的潜在影响。

Method: 通过设定8种不同身份，并在两种基于人类实验的数据推理任务中测试8种语言模型（包括开源和专有模型），检验身份分配对虚假信息辨别和科学证据评估的影响。

Result: 赋予身份的模型在虚假信息辨别的准确性上最多降低了9%，而某些政治身份的模型在评估与其立场一致的科学证据时表现出了高达90%的准确性。这些现象难以通过传统的提示性去偏方法消除。

Conclusion: 大语言模型在赋予特定身份后，会表现出类似人类的动机性推理，而传统去偏技术难以缓解这一问题，这可能加剧语言模型和人类的身份趋同推理现象。

Abstract: Reasoning in humans is prone to biases due to underlying motivations like
identity protection, that undermine rational decision-making and judgment. This
motivated reasoning at a collective level can be detrimental to society when
debating critical issues such as human-driven climate change or vaccine safety,
and can further aggravate political polarization. Prior studies have reported
that large language models (LLMs) are also susceptible to human-like cognitive
biases, however, the extent to which LLMs selectively reason toward
identity-congruent conclusions remains largely unexplored. Here, we investigate
whether assigning 8 personas across 4 political and socio-demographic
attributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and
proprietary) across two reasoning tasks from human-subject studies -- veracity
discernment of misinformation headlines and evaluation of numeric scientific
evidence -- we find that persona-assigned LLMs have up to 9% reduced veracity
discernment relative to models without personas. Political personas
specifically, are up to 90% more likely to correctly evaluate scientific
evidence on gun control when the ground truth is congruent with their induced
political identity. Prompt-based debiasing methods are largely ineffective at
mitigating these effects. Taken together, our empirical findings are the first
to suggest that persona-assigned LLMs exhibit human-like motivated reasoning
that is hard to mitigate through conventional debiasing prompts -- raising
concerns of exacerbating identity-congruent reasoning in both LLMs and humans.

</details>


### [192] [DiaLLMs: EHR Enhanced Clinical Conversational System for Clinical Test Recommendation and Diagnosis Prediction](https://arxiv.org/abs/2506.20059)
*Weijieying Ren,Tianxiang Zhao,Lei Wang,Tianchun Wang,Vasant Honavar*

Main category: cs.AI

TL;DR: 本文提出了一种名为DiaLLM的医疗大语言模型，首次将电子健康记录（EHR）融入到临床对话中，并在实验中显示出优于基线模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的医疗大语言模型忽略了EHR的重要作用，主要专注于诊断推荐，因而临床适用性有限。

Method: 设计了一个临床测试参考策略（CTR），将每个临床编码映射到对应的描述，并将测试结果分类为“正常”或“异常”。同时，采用强化学习框架进行证据获取和自动诊断，并引入了拒绝采样策略、确认奖励和类别敏感的诊断奖励来优化诊断预测。

Result: 实验结果表明，DiaLLM在临床测试推荐和诊断预测方面优于基线模型。

Conclusion: DiaLLM通过融合异构EHR数据，为临床对话加入了更多实用功能，使其更加贴合真实的医疗实践，具有较高的潜在应用价值。

Abstract: Recent advances in Large Language Models (LLMs) have led to remarkable
progresses in medical consultation. However, existing medical LLMs overlook the
essential role of Electronic Health Records (EHR) and focus primarily on
diagnosis recommendation, limiting their clinical applicability. We propose
DiaLLM, the first medical LLM that integrates heterogeneous EHR data into
clinically grounded dialogues, enabling clinical test recommendation, result
interpretation, and diagnosis prediction to better align with real-world
medical practice. To construct clinically grounded dialogues from EHR, we
design a Clinical Test Reference (CTR) strategy that maps each clinical code to
its corresponding description and classifies test results as "normal" or
"abnormal". Additionally, DiaLLM employs a reinforcement learning framework for
evidence acquisition and automated diagnosis. To handle the large action space,
we introduce a reject sampling strategy to reduce redundancy and improve
exploration efficiency. Furthermore, a confirmation reward and a
class-sensitive diagnosis reward are designed to guide accurate diagnosis
prediction. Extensive experimental results demonstrate that DiaLLM outperforms
baselines in clinical test recommendation and diagnosis prediction.

</details>


### [193] [AI Copilots for Reproducibility in Science: A Case Study](https://arxiv.org/abs/2506.20130)
*Adrien Bibal,Steven N. Minton,Deborah Khider,Yolanda Gil*

Main category: cs.AI

TL;DR: OpenPub 是一个 AI 驱动的平台，通过 Reproducibility Copilot 工具促进研究工作的透明性与重复性，显著减少了重复实验的时间，并提高了数据和结果的覆盖率。


<details>
  <summary>Details</summary>
Motivation: 解决开放科学中研究成果可复现问题，该领域始终面临重复实验效率低下和数据缺失等障碍。

Method: 提出 OpenPub 平台，其中 Reproducibility Copilot 分析稿件和代码，生成结构化的 Jupyter Notebooks，并检测复现性障碍。

Result: 结果显示 OpenPub 能将重复实验时间从30小时缩短至1小时，同时提高了对数据及结果复现的覆盖率，系统还能检测出导致复现困难的因素，例如缺失超参数、不详的预处理步骤等。

Conclusion: AI 驱动的工具能够显著减轻复现性研究的负担，并促进科学交流的透明性与可信度；OpenPub 性能卓越，为扩展至其他开放科学任务奠定了基础。

Abstract: Open science initiatives seek to make research outputs more transparent,
accessible, and reusable, but ensuring that published findings can be
independently reproduced remains a persistent challenge. This paper introduces
OpenPub, an AI-powered platform that supports researchers, reviewers, and
readers through a suite of modular copilots focused on key open science tasks.
In this work, we present the Reproducibility Copilot, which analyzes
manuscripts, code, and supplementary materials to generate structured Jupyter
Notebooks and recommendations aimed at facilitating computational, or "rote",
reproducibility. We conducted feasibility tests using previously studied
research papers with known reproducibility benchmarks. Results indicate that
OpenPub can substantially reduce reproduction time - from over 30 hours to
about 1 hour - while achieving high coverage of figures, tables, and results
suitable for computational reproduction. The system systematically detects
barriers to reproducibility, including missing hyperparameters, undocumented
preprocessing steps, and incomplete or inaccessible datasets. These findings
suggest that AI-driven tools can meaningfully reduce the burden of
reproducibility efforts and contribute to more transparent and verifiable
scientific communication. The modular copilot architecture also provides a
foundation for extending AI assistance to additional open science objectives
beyond reproducibility.

</details>


### [194] [Language Modeling by Language Models](https://arxiv.org/abs/2506.20249)
*Junyan Cheng,Peter Clark,Kyle Richardson*

Main category: cs.AI

TL;DR: 研究提出了一种基于大型语言模型（LLMs）的多代理系统Genesys，用于模拟发现新型语言模型架构的过程，显示出比现有方法在设计生成上优越的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究的主要动机是通过自动化和优化研究过程，探索新的语言模型架构设计，以应对传统人工设计方法的局限性和效率问题。

Method: 采用了一个名为Genesys的多代理系统，其模拟了研究的各个阶段，包括理论提出、文献检索、代码生成、预训练及评估等。系统基于Ladder of Scales方式，逐步缩小预算分配以验证模型，同时以遗传编程方法替代直接提示生成。

Result: 通过实验，研究生成并验证了1,162种新设计，其中1,062种经过完全验证，并在6/9的基准测试中超越了GPT2和Mamba2等架构。

Conclusion: Genesys展示了其在发现新的语言模型架构上的有效性与竞争力，并通过实验发现了更高效的自主发现方法，为未来类似系统的开发提供了参考。

Abstract: Can we leverage LLMs to model the process of discovering novel language model
(LM) architectures? Inspired by real research, we propose a multi-agent LLM
approach that simulates the conventional stages of research, from ideation and
literature search (proposal stage) to design implementation (code generation),
generative pre-training, and downstream evaluation (verification). Using ideas
from scaling laws, our system, Genesys, employs a Ladder of Scales approach;
new designs are proposed, adversarially reviewed, implemented, and selectively
verified at increasingly larger model scales (14M$\sim$350M parameters) with a
narrowing budget (the number of models we can train at each scale). To help
make discovery efficient and factorizable, Genesys uses a novel genetic
programming backbone, which we show has empirical advantages over commonly used
direct prompt generation workflows (e.g., $\sim$86\% percentage point
improvement in successful design generation, a key bottleneck). We report
experiments involving 1,162 newly discovered designs (1,062 fully verified
through pre-training) and find the best designs to be highly competitive with
known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common
benchmarks). We couple these results with comprehensive system-level ablations
and formal results, which give broader insights into the design of effective
autonomous discovery systems.

</details>


### [195] [Enterprise Large Language Model Evaluation Benchmark](https://arxiv.org/abs/2506.20274)
*Liya Wang,David Yi,Damien Jose,John Passarelli,James Gao,Jordan Leventis,Kang Li*

Main category: cs.AI

TL;DR: 该论文提出了一种新的14任务框架和评估管道，以全面评估大语言模型（LLMs）在企业情境中的性能，并开发了一个9700样本的基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法充分评估LLMs在企业特定任务中的复杂度，需开发专门设计的评估框架和方法。

Method: 设计了基于Bloom分类学的14任务框架，通过结合“LLM作为标签器”和“LLM作为评审官”以及校正性检索增强生成（CRAG）方法，构建了一个大规模的样本基准。

Result: 评估了六种领先模型，发现开源模型在推理任务中表现接近专有模型，但在判断任务中较弱，同时揭示了LLMs在企业任务中的性能差距。

Conclusion: 该研究为企业提供了一个量身定制评估LLMs的方法，同时对模型优化和实际部署提供了指导。

Abstract: Large Language Models (LLMs) ) have demonstrated promise in boosting
productivity across AI-powered tools, yet existing benchmarks like Massive
Multitask Language Understanding (MMLU) inadequately assess enterprise-specific
task complexities. We propose a 14-task framework grounded in Bloom's Taxonomy
to holistically evaluate LLM capabilities in enterprise contexts. To address
challenges of noisy data and costly annotation, we develop a scalable pipeline
combining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented
generation (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six
leading models shows open-source contenders like DeepSeek R1 rival proprietary
models in reasoning tasks but lag in judgment-based scenarios, likely due to
overthinking. Our benchmark reveals critical enterprise performance gaps and
offers actionable insights for model optimization. This work provides
enterprises a blueprint for tailored evaluations and advances practical LLM
deployment.

</details>


### [196] [Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards](https://arxiv.org/abs/2506.20332)
*Jihao Gu,Qihang Ai,Yingyao Wang,Pi Bu,Jingxuan Xing,Zekun Zhu,Wei Jiang,Ziming Wang,Yingxiu Zhao,Ming-Liang Zhang,Jun Song,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: Mobile-R1提出了一种通过多轮交互和任务级奖励的强化学习框架，以提升移动代理的探索与纠错能力，同时开源了包含多应用的高质量数据集和模型资源。


<details>
  <summary>Details</summary>
Motivation: 现有移动代理的强化学习方法局限于离线训练或动作级奖励优化，导致探索能力不足和容易陷入局部最优的问题。

Method: 采用分阶段训练框架，包括初始格式微调、基于动作级奖励的单步在线训练、以及基于任务级奖励的多轮轨迹在线训练，结合高质量数据集与新基准。

Result: 移动代理的探索与动作纠错能力显著增强，同时在性能指标上有大幅改善。

Conclusion: Mobile-R1通过多轮交互和任务级奖励的强化学习方法解决了局部最优与探索性不足的问题，具备较强的实际应用潜力。

Abstract: Vision-language model-based mobile agents have gained the ability to not only
understand complex instructions and mobile screenshots, but also optimize their
action outputs via thinking and reasoning, benefiting from reinforcement
learning, such as Group Relative Policy Optimization (GRPO). However, existing
research centers on offline reinforcement learning training or online
optimization using action-level rewards, which limits the agent's dynamic
interaction with the environment. This often results in agents settling into
local optima, thereby weakening their ability for exploration and error action
correction. To address these challenges, we introduce an approach called
Mobile-R1, which employs interactive multi-turn reinforcement learning with
task-level rewards for mobile agents. Our training framework consists of three
stages: initial format finetuning, single-step online training via action-level
reward, followed by online training via task-level reward based on multi-turn
trajectories. This strategy is designed to enhance the exploration and error
correction capabilities of Mobile-R1, leading to significant performance
improvements. Moreover, we have collected a dataset covering 28 Chinese
applications with 24,521 high-quality manual annotations and established a new
benchmark with 500 trajectories. We will open source all resources, including
the dataset, benchmark, model weight, and codes:
https://mobile-r1.github.io/Mobile-R1/.

</details>


### [197] [Tabular Feature Discovery With Reasoning Type Exploration](https://arxiv.org/abs/2506.20357)
*Sungwon Han,Sungkyu Park,Seungeon Lee*

Main category: cs.AI

TL;DR: 提出REFeat方法，利用多种推理方式指导大语言模型（LLM）生成多样且有效的特征，提升表格数据的特征工程效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的特征生成方法易产生简单或重复特征，缺乏结构化推理指导。

Method: 提出方法REFeat，利用多种推理方式引导LLM生成具有多样性和信息量的特征。

Result: 在59个基准数据集上的实验表明，该方法平均预测准确率更高，并能发现更具多样性和意义的特征。

Conclusion: REFeat展示了将丰富的推理模式与自适应策略选择引入LLM特征发现过程的潜力。

Abstract: Feature engineering for tabular data remains a critical yet challenging step
in machine learning. Recently, large language models (LLMs) have been used to
automatically generate new features by leveraging their vast knowledge.
However, existing LLM-based approaches often produce overly simple or
repetitive features, partly due to inherent biases in the transformations the
LLM chooses and the lack of structured reasoning guidance during generation. In
this paper, we propose a novel method REFeat, which guides an LLM to discover
diverse and informative features by leveraging multiple types of reasoning to
steer the feature generation process. Experiments on 59 benchmark datasets
demonstrate that our approach not only achieves higher predictive accuracy on
average, but also discovers more diverse and meaningful features. These results
highlight the promise of incorporating rich reasoning paradigms and adaptive
strategy selection into LLM-driven feature discovery for tabular data.

</details>


### [198] [Paladin-mini: A Compact and Efficient Grounding Model Excelling in Real-World Scenarios](https://arxiv.org/abs/2506.20384)
*Dror Ivry,Oran Nahum*

Main category: cs.AI

TL;DR: 本文介绍了用于解决论点与上下文匹配问题的两个关键贡献。


<details>
  <summary>Details</summary>
Motivation: 解决论点与文档上下文关联的困难，确保论点有支持性证据。

Method: 提出Paladin-mini模型（3.8B参数）及新基准评测集（grounding-benchmark）。

Result: Paladin-mini模型在实际应用中性能稳健，并在相关任务表现对比现有最优方法取得良好评估结果。

Conclusion: 提供了有效方法和数据评估工具，有助于推动关键推理任务的研究。

Abstract: This paper introduces two significant contributions to address the issue of
grounding claims in a given context. Grounding means that given a context
(document) and a claim, there's at least one supportive evidence for the claim
in the document. We will introduce Paladin-mini, a compact (3.8B parameters)
open-source classifier model (used for labeling data as grounded or ungrounded)
engineered for robust performance in real-world scenarios, and the
grounding-benchmark, a new evaluation dataset designed to assess performance on
critical reasoning tasks. We'll also demonstrate the results of Paladin-mini
with benchmarks against the current State-of-the-art and share clear and
reproducible results.

</details>


### [199] [Smart Ride and Delivery Services with Electric Vehicles: Leveraging Bidirectional Charging for Profit Optimisation](https://arxiv.org/abs/2506.20401)
*Jinchun Du,Bojie Shen,Muhammad Aamir Cheema,Adel N. Toosi*

Main category: cs.AI

TL;DR: 本研究提出EVOP-V2G问题，即电动车队在调度过程中优化订单选择、充电与放电行为以最大化收益，并用混合整数规划模型与元启发式算法解决。


<details>
  <summary>Details</summary>
Motivation: 随着电动车和V2G技术逐步普及，运营商在提高电动车利益时需协调接单、充放电和路径规划的复杂决策问题。

Method: 通过混合整数规划（MIP）模型定义EVOP-V2G问题，设计两个元启发式算法：进化算法（EA）及大邻域搜索（LNS），用于求解问题。

Result: 采用真实世界数据验证，提出的方法在小规模实例上接近最优解，且在大规模数据上表现出极佳的扩展性，能使司机的收益提升一倍。

Conclusion: 本研究为面向电动汽车与能源网支持的智能移动系统提出了优化路径，展示了巨大潜力。

Abstract: With the rising popularity of electric vehicles (EVs), modern service
systems, such as ride-hailing delivery services, are increasingly integrating
EVs into their operations. Unlike conventional vehicles, EVs often have a
shorter driving range, necessitating careful consideration of charging when
fulfilling requests. With recent advances in Vehicle-to-Grid (V2G) technology -
allowing EVs to also discharge energy back to the grid - new opportunities and
complexities emerge. We introduce the Electric Vehicle Orienteering Problem
with V2G (EVOP-V2G): a profit-maximization problem where EV drivers must select
customer requests or orders while managing when and where to charge or
discharge. This involves navigating dynamic electricity prices, charging
station selection, and route constraints. We formulate the problem as a Mixed
Integer Programming (MIP) model and propose two near-optimal metaheuristic
algorithms: one evolutionary (EA) and the other based on large neighborhood
search (LNS). Experiments on real-world data show our methods can double driver
profits compared to baselines, while maintaining near-optimal performance on
small instances and excellent scalability on larger ones. Our work highlights a
promising path toward smarter, more profitable EV-based mobility systems that
actively support the energy grid.

</details>


### [200] [GymPN: A Library for Decision-Making in Process Management Systems](https://arxiv.org/abs/2506.20404)
*Riccardo Lo Bianco,Willem van Jaarsveld,Remco Dijkman*

Main category: cs.AI

TL;DR: 本文介绍了一个名为GymPN的软件库，利用深度强化学习支持业务流程中的优化决策。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过软件优化组织中的任务分配、执行和任务分派决策，解决之前方法的局限性。

Method: 提出GymPN，通过支持部分过程的可观测性和建模多个业务流程决策，改进基于深度强化学习的业务流程决策方法。

Result: 在八个典型业务流程决策问题模式上评估，表明GymPN可以轻松建模问题并学习最佳决策策略。

Conclusion: GymPN解决了现有方法在部分可观测和复杂决策建模上的局限性，为真实业务流程决策提供了更有效的支持。

Abstract: Process management systems support key decisions about the way work is
allocated in organizations. This includes decisions on which task to perform
next, when to execute the task, and who to assign the task to. Suitable
software tools are required to support these decisions in a way that is optimal
for the organization. This paper presents a software library, called GymPN,
that supports optimal decision-making in business processes using Deep
Reinforcement Learning. GymPN builds on previous work that supports task
assignment in business processes, introducing two key novelties: support for
partial process observability and the ability to model multiple decisions in a
business process. These novel elements address fundamental limitations of
previous work and thus enable the representation of more realistic process
decisions. We evaluate the library on eight typical business process
decision-making problem patterns, showing that GymPN allows for easy modeling
of the desired problems, as well as learning optimal decision policies.

</details>


### [201] [Mixtures of Neural Cellular Automata: A Stochastic Framework for Growth Modelling and Self-Organization](https://arxiv.org/abs/2506.20486)
*Salvatore Milite,Giulio Caravagna,Andrea Sottoriva*

Main category: cs.AI

TL;DR: 本文提出了一种名为混合神经元胞自动机（MNCA）的新框架，用以改进传统神经元胞自动机（NCA）在处理真实世界随机性问题上的能力。


<details>
  <summary>Details</summary>
Motivation: 传统神经元胞自动机（NCA）因其确定性特性在模拟生物或物理系统的随机性时存在局限性，且缺乏对复杂行为的捕捉能力。

Method: 通过将混合模型的思想引入NCA，结合概率规则分配和内在噪声，MNCA可以模拟多样化的局部行为并再现生物过程中的随机动态。

Result: MNCA在合成组织生长与分化模拟、图像生成鲁棒性以及显微镜图像分割三大领域成功验证其有效性，表现出更强的抗干扰能力、更接近真实生物生长模式以及提供更多可解释的规则分割结果。

Conclusion: MNCA作为一种工具，能够有效建模随机动态系统并促进对自我生长过程的研究，具有广泛的应用前景。

Abstract: Neural Cellular Automata (NCAs) are a promising new approach to model
self-organizing processes, with potential applications in life science.
However, their deterministic nature limits their ability to capture the
stochasticity of real-world biological and physical systems.
  We propose the Mixture of Neural Cellular Automata (MNCA), a novel framework
incorporating the idea of mixture models into the NCA paradigm. By combining
probabilistic rule assignments with intrinsic noise, MNCAs can model diverse
local behaviors and reproduce the stochastic dynamics observed in biological
processes.
  We evaluate the effectiveness of MNCAs in three key domains: (1) synthetic
simulations of tissue growth and differentiation, (2) image morphogenesis
robustness, and (3) microscopy image segmentation. Results show that MNCAs
achieve superior robustness to perturbations, better recapitulate real
biological growth patterns, and provide interpretable rule segmentation. These
findings position MNCAs as a promising tool for modeling stochastic dynamical
systems and studying self-growth processes.

</details>


### [202] [Engineering Sentience](https://arxiv.org/abs/2506.20504)
*Konstantin Demin,Taylor Webb,Eric Elmoznino,Hakwan Lau*

Main category: cs.AI

TL;DR: 本文定义了“感知能力”，并讨论了如何在机器中设计和实现这一能力，同时提出其应该具有功能性和主观性。


<details>
  <summary>Details</summary>
Motivation: 明确如何在人工智能中设计和实现感知能力，避免无意中创造功能性感知系统。

Method: 提出感知能力的定义以及功能化要求，包括感知信号的持久性和质感，并提供可能的实现方法。

Result: 研究描绘了利用现有技术实现感知能力的方式，提供理论和实践指导。

Conclusion: 人工智能的感知定义需要结合功能性和主观性，以便规范设计并避免误创造功能性感知系统。

Abstract: We spell out a definition of sentience that may be useful for designing and
building it in machines. We propose that for sentience to be meaningful for AI,
it must be fleshed out in functional, computational terms, in enough detail to
allow for implementation. Yet, this notion of sentience must also reflect
something essentially 'subjective', beyond just having the general capacity to
encode perceptual content. For this specific functional notion of sentience to
occur, we propose that certain sensory signals need to be both assertoric
(persistent) and qualitative. To illustrate the definition in more concrete
terms, we sketch out some ways for potential implementation, given current
technology. Understanding what it takes for artificial agents to be
functionally sentient can also help us avoid creating them inadvertently, or at
least, realize that we have created them in a timely manner.

</details>


### [203] [Case-based Reasoning Augmented Large Language Model Framework for Decision Making in Realistic Safety-Critical Driving Scenarios](https://arxiv.org/abs/2506.20531)
*Wenbin Gan,Minh-Son Dao,Koji Zettsu*

Main category: cs.AI

TL;DR: 此论文提出了一种名为CBR-LLM的框架，用于在复杂高风险驾驶场景中进行规避性决策。该方法结合了语义场景理解与案例检索，利用LLM提升决策准确性及与人类行为的对齐度。


<details>
  <summary>Details</summary>
Motivation: 目前LLMs在自动驾驶中的应用受限于领域适配、背景嵌入以及缺乏体验性知识，难以应对动态及高风险环境下的可靠决策需求。

Method: 提出Case-Based Reasoning Augmented LLM框架，通过结合来自车载摄像头的视频语义场景理解和相似案例检索，利用LLMs生成上下文感知、对齐人类行为的决策建议。

Result: 实验表明，该框架在多个开源LLMs中提升了决策准确性、解释质量及与专家行为的一致性。此外，基于风险的提示策略和基于相似性的案例检索也表现出显著优势。

Conclusion: CBR-LLM框架展现了在复杂实时驾驶条件下鲁棒的决策支持能力，为智能驾驶系统提供了一种自适应且可信赖的解决方案。

Abstract: Driving in safety-critical scenarios requires quick, context-aware
decision-making grounded in both situational understanding and experiential
reasoning. Large Language Models (LLMs), with their powerful general-purpose
reasoning capabilities, offer a promising foundation for such decision-making.
However, their direct application to autonomous driving remains limited due to
challenges in domain adaptation, contextual grounding, and the lack of
experiential knowledge needed to make reliable and interpretable decisions in
dynamic, high-risk environments. To address this gap, this paper presents a
Case-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for
evasive maneuver decision-making in complex risk scenarios. Our approach
integrates semantic scene understanding from dashcam video inputs with the
retrieval of relevant past driving cases, enabling LLMs to generate maneuver
recommendations that are both context-sensitive and human-aligned. Experiments
across multiple open-source LLMs show that our framework improves decision
accuracy, justification quality, and alignment with human expert behavior.
Risk-aware prompting strategies further enhance performance across diverse risk
types, while similarity-based case retrieval consistently outperforms random
sampling in guiding in-context learning. Case studies further demonstrate the
framework's robustness in challenging real-world conditions, underscoring its
potential as an adaptive and trustworthy decision-support tool for intelligent
driving systems.

</details>


### [204] [Fine-Tuning and Prompt Engineering of LLMs, for the Creation of Multi-Agent AI for Addressing Sustainable Protein Production Challenges](https://arxiv.org/abs/2506.20598)
*Alexander D. Kalian,Jaewook Lee,Stefan P. Johannesson,Lennart Otte,Christer Hogstrand,Miao Guo*

Main category: cs.AI

TL;DR: 本研究提出一个用于支持可持续蛋白质研究的多代理人工智能框架，特别针对微生物蛋白源，采用检索增强生成（RAG）系统并探讨两种优化方法：微调和提示工程。


<details>
  <summary>Details</summary>
Motivation: 目前对可持续蛋白质来源的全球需求增加，需要智能工具快速处理和综合特定领域的科学知识。

Method: 开发了包含两个GPT模型代理的RAG系统：一个用于检索相关文献，一个用于信息提取并优化其性能，采用微调和提示工程两种方法。

Result: 微调方法比提示工程更有效，提升了信息提取的性能，平均余弦相似性得分达到0.94以上。提示工程则有更低的不确定性。系统还配备一个用户界面和化学安全搜索功能的初步探索。

Conclusion: 提出的AI框架展示了其在微生物蛋白研究中的潜力，且优化方法改进了系统性能，同时扩展的功能为未来的化学安全应用提供了可能。

Abstract: The global demand for sustainable protein sources has accelerated the need
for intelligent tools that can rapidly process and synthesise domain-specific
scientific knowledge. In this study, we present a proof-of-concept multi-agent
Artificial Intelligence (AI) framework designed to support sustainable protein
production research, with an initial focus on microbial protein sources. Our
Retrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based
LLM agents: (1) a literature search agent that retrieves relevant scientific
literature on microbial protein production for a specified microbial strain,
and (2) an information extraction agent that processes the retrieved content to
extract relevant biological and chemical information. Two parallel
methodologies, fine-tuning and prompt engineering, were explored for agent
optimisation. Both methods demonstrated effectiveness at improving the
performance of the information extraction agent in terms of transformer-based
cosine similarity scores between obtained and ideal outputs. Mean cosine
similarity scores were increased by up to 25%, while universally reaching mean
scores of $\geq 0.89$ against ideal output text. Fine-tuning overall improved
the mean scores to a greater extent (consistently of $\geq 0.94$) compared to
prompt engineering, although lower statistical uncertainties were observed with
the latter approach. A user interface was developed and published for enabling
the use of the multi-agent AI system, alongside preliminary exploration of
additional chemical safety-based search capabilities

</details>


### [205] [CogGen: A Learner-Centered Generative AI Architecture for Intelligent Tutoring with Programming Video](https://arxiv.org/abs/2506.20600)
*Wengxi Li,Roy Pea,Nick Haber,Hari Subramonyam*

Main category: cs.AI

TL;DR: CogGen是一种AI架构，将编程视频转化为交互式、自适应学习体验，核心为学习目标驱动的视频分割、基于认知学徒法的会话式辅导引擎及贝叶斯知识追踪的学生模型。


<details>
  <summary>Details</summary>
Motivation: 现有编程教育视频缺乏互动性和个性化辅导，无法充分满足学习者需求。

Method: 提出CogGen架构，包括三个核心部分：学习目标驱动的视频分割、基于认知学徒法的会话式辅导引擎及使用贝叶斯知识追踪的学生模型。通过技术评估和消融研究验证效果。

Result: 演示了视频分割的高准确性及各组件在增益学习体验中的必要性，且在知识、方法、行为和互动层面上实现了强一致的教学对齐。

Conclusion: 该研究通过整合结构化学生建模与交互式AI对话，推进了AI驱动的个性化教学，为扩大编程教育视频的教学效果提供了可行方案。

Abstract: We introduce CogGen, a learner-centered AI architecture that transforms
programming videos into interactive, adaptive learning experiences by
integrating student modeling with generative AI tutoring based on the Cognitive
Apprenticeship framework. The architecture consists of three components: (1)
video segmentation by learning goals, (2) a conversational tutoring engine
applying Cognitive Apprenticeship strategies, and (3) a student model using
Bayesian Knowledge Tracing to adapt instruction. Our technical evaluation
demonstrates effective video segmentation accuracy and strong pedagogical
alignment across knowledge, method, action, and interaction layers. Ablation
studies confirm the necessity of each component in generating effective
guidance. This work advances AI-powered tutoring by bridging structured student
modeling with interactive AI conversations, offering a scalable approach to
enhancing video-based programming education.

</details>


### [206] [AI Assistants to Enhance and Exploit the PETSc Knowledge Base](https://arxiv.org/abs/2506.20608)
*Barry Smith,Junchao Zhang,Hong Zhang,Lois Curfman McInnes,Murat Keceli,Archit Vasan,Satish Balay,Toby Isaac,Le Chen,Venkatram Vishwanath*

Main category: cs.AI

TL;DR: 本文介绍了如何利用大型语言模型(LLMs)来改进PETSc，一个高性能科学计算数值库的知识获取和利用。


<details>
  <summary>Details</summary>
Motivation: PETSc的知识库分布在多种形式中且较为零散，用户和新开发者难以有效访问和利用。

Method: 构建一个由LLM驱动的系统，结合PETSc内容和定制的LLM工具，如基于检索的生成(RAG)、重新排序算法和聊天机器人。

Result: 实现了初步的工具设计和评估，探索LLM如何改进数值软件的开发和使用。

Conclusion: 提出构建一个可扩展的AI知识框架，为科学软件提供高效支持、丰富文档和改进研发流程，并计划扩展成一个稳健的平台，加速科学发现进程。

Abstract: Generative AI, especially through large language models (LLMs), is
transforming how technical knowledge can be accessed, reused, and extended.
PETSc, a widely used numerical library for high-performance scientific
computing, has accumulated a rich but fragmented knowledge base over its three
decades of development, spanning source code, documentation, mailing lists,
GitLab issues, Discord conversations, technical papers, and more. Much of this
knowledge remains informal and inaccessible to users and new developers. To
activate and utilize this knowledge base more effectively, the PETSc team has
begun building an LLM-powered system that combines PETSc content with custom
LLM tools -- including retrieval-augmented generation (RAG), reranking
algorithms, and chatbots -- to assist users, support developers, and propose
updates to formal documentation. This paper presents initial experiences
designing and evaluating these tools, focusing on system architecture, using
RAG and reranking for PETSc-specific information, evaluation methodologies for
various LLMs and embedding models, and user interface design. Leveraging the
Argonne Leadership Computing Facility resources, we analyze how LLM responses
can enhance the development and use of numerical software, with an initial
focus on scalable Krylov solvers. Our goal is to establish an extensible
framework for knowledge-centered AI in scientific software, enabling scalable
support, enriched documentation, and enhanced workflows for research and
development. We conclude by outlining directions for expanding this system into
a robust, evolving platform that advances software ecosystems to accelerate
scientific discovery.

</details>


### [207] [Towards Community-Driven Agents for Machine Learning Engineering](https://arxiv.org/abs/2506.20640)
*Sijie Li,Weiwei Sun,Shanda Li,Ameet Talwalkar,Yiming Yang*

Main category: cs.AI

TL;DR: 本研究引入MLE-Live框架和CoMind代理,验证其在模拟Kaggle社区中协作和知识共享的能力,并取得了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习代理通常孤立地研究问题,未能与更广泛的研究社区互动,难以充分利用共享知识。

Method: 引入MLE-Live评价框架,模拟Kaggle社区,并在其上开发CoMind代理,以便通过分享和探索协作解决方案。

Result: CoMind在MLE-Live上表现出色,在四个Kaggle竞赛中平均超过79.2%的真人竞争者。

Conclusion: 与社区协作的能力可以显著提升机器学习代理的表现,展示了引入共享和互动机制的重要性。

Abstract: Large language model-based machine learning (ML) agents have shown great
promise in automating ML research. However, existing agents typically operate
in isolation on a given research problem, without engaging with the broader
research community, where human researchers often gain insights and contribute
by sharing knowledge. To bridge this gap, we introduce MLE-Live, a live
evaluation framework designed to assess an agent's ability to communicate with
and leverage collective knowledge from a simulated Kaggle research community.
Building on this framework, we propose CoMind, a novel agent that excels at
exchanging insights and developing novel solutions within a community context.
CoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2%
human competitors on average across four ongoing Kaggle competitions. Our code
is released at https://github.com/comind-ml/CoMind.

</details>


### [208] [The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind](https://arxiv.org/abs/2506.20664)
*Andrei Lupu,Timon Willi,Jakob Foerster*

Main category: cs.AI

TL;DR: 本文提出Decrypto，一个用于多代理推理和心理理论研究的游戏型基准，旨在解决现有基准的局限性。通过实验发现，大型语言模型在此方面的表现不如人类和简单基线模型。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型具备代理功能，多代理情景中的多样化交互需求日益增加，其中关键能力是心理理论（ToM）。现有基准在评估此类能力时存在诸多问题，亟需新的测试工具。

Method: 设计了一个名为Decrypto的游戏化多代理推理基准，灵感来源于认知科学、计算语用学和多代理强化学习。同时进行了全面实验验证和交互性任务评估。

Result: 结果显示，在实验任务上，最先进的推理模型表现不如人类和简单基线模型，而旧版本模型反而在某些ToM任务中表现更优。

Conclusion: Decrypto有效填补了当前在多代理推理和心理理论能力评估方面的空白，为构建更优的人工代理奠定了基础。

Abstract: As Large Language Models (LLMs) gain agentic abilities, they will have to
navigate complex multi-agent scenarios, interacting with human users and other
agents in cooperative and competitive settings. This will require new reasoning
skills, chief amongst them being theory of mind (ToM), or the ability to reason
about the "mental" states of other agents. However, ToM and other multi-agent
abilities in LLMs are poorly understood, since existing benchmarks suffer from
narrow scope, data leakage, saturation, and lack of interactivity. We thus
propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM
drawing inspiration from cognitive science, computational pragmatics and
multi-agent reinforcement learning. It is designed to be as easy as possible in
all other dimensions, eliminating confounding factors commonly found in other
benchmarks. To our knowledge, it is also the first platform for designing
interactive ToM experiments.
  We validate the benchmark design through comprehensive empirical evaluations
of frontier LLMs, robustness studies, and human-AI cross-play experiments. We
find that LLM game-playing abilities lag behind humans and simple
word-embedding baselines. We then create variants of two classic cognitive
science experiments within Decrypto to evaluate three key ToM abilities.
Surprisingly, we find that state-of-the-art reasoning models are significantly
worse at those tasks than their older counterparts. This demonstrates that
Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and
paves the path towards better artificial agents.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [209] [Position: Machine Learning Conferences Should Establish a "Refutations and Critiques" Track](https://arxiv.org/abs/2506.19882)
*Rylan Schaeffer,Joshua Kazdan,Yegor Denisov-Blanch,Brando Miranda,Matthias Gerstgrasser,Susan Zhang,Andreas Haupt,Isha Gupta,Elyas Obbad,Jesse Dodge,Jessica Zosa Forde,Koustuv Sinha,Francesco Orabona,Sanmi Koyejo,David Donoho*

Main category: cs.LG

TL;DR: 本文提议为ML会议设立“反驳与批评”轨道，以支持纠正错误研究的机制。


<details>
  <summary>Details</summary>
Motivation: ML领域快速进步，但同行评审的局限性导致错误或误导性的研究被接受，缺乏系统纠正错误的机制。

Method: 建议在ML会议中设立“反驳与批评”轨道（R & C Track），提供一个高调、可靠的平台以支持批判性研究。

Result: 提出了R & C轨道的设计、审查原则及潜在的实施挑战，并举例展示其应用。

Conclusion: ML需要官方机制来支持研究过程的自我纠正，R & C轨道是一种可行的解决方案。

Abstract: Science progresses by iteratively advancing and correcting humanity's
understanding of the world. In machine learning (ML) research, rapid
advancements have led to an explosion of publications, but have also led to
misleading, incorrect, flawed or perhaps even fraudulent studies being accepted
and sometimes highlighted at ML conferences due to the fallibility of peer
review. While such mistakes are understandable, ML conferences do not offer
robust processes to help the field systematically correct when such errors are
made.This position paper argues that ML conferences should establish a
dedicated "Refutations and Critiques" (R & C) Track. This R & C Track would
provide a high-profile, reputable platform to support vital research that
critically challenges prior research, thereby fostering a dynamic
self-correcting research ecosystem. We discuss key considerations including
track design, review principles, potential pitfalls, and provide an
illustrative example submission concerning a recent ICLR 2025 Oral. We conclude
that ML conferences should create official, reputable mechanisms to help ML
research self-correct.

</details>


### [210] [STIMULUS: Achieving Fast Convergence and Low Sample Complexity in Stochastic Multi-Objective Learning](https://arxiv.org/abs/2506.19883)
*Zhuqing Liu,Chaosheng Dong,Michinari Momma,Simone Shao,Shaoyuan Xu,Yan Gao,Haibo Yang,Jia Liu*

Main category: cs.LG

TL;DR: 本文提出了名为STIMULUS的MOO算法及其增强版本，解决了传统方法收敛性能和样本复杂性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 多目标优化在机器学习、运筹学、工程中广泛应用，但其算法设计仍处于起步阶段，现有方法的收敛速度和样本复杂性表现不理想。

Method: 提出STIMULUS算法，通过递归框架改进随机梯度估计的更新效率；引入STIMULUS-M加入动量以加速收敛；此外，通过自适应批次机制提升了算法的兼容性（STIMULUS+ / STIMULUS-M+）。

Result: 对于非凸问题，证明了O(1/T)的收敛率和最优的样本复杂性；对于强凸问题，实现了O(exp(-μT))收敛率和优化样本复杂性。

Conclusion: STIMULUS及其拓展版本不仅提升了收敛性能，还降低了样本复杂性，在MOO问题上展现出强大优势。

Abstract: Recently, multi-objective optimization (MOO) has gained attention for its
broad applications in ML, operations research, and engineering. However, MOO
algorithm design remains in its infancy and many existing MOO methods suffer
from unsatisfactory convergence rate and sample complexity performance. To
address this challenge, in this paper, we propose an algorithm called STIMULUS(
stochastic path-integrated multi-gradient recursive e\ulstimator), a new and
robust approach for solving MOO problems. Different from the traditional
methods, STIMULUS introduces a simple yet powerful recursive framework for
updating stochastic gradient estimates to improve convergence performance with
low sample complexity. In addition, we introduce an enhanced version of
STIMULUS, termed STIMULUS-M, which incorporates a momentum term to further
expedite convergence. We establish $O(1/T)$ convergence rates of the proposed
methods for non-convex settings and $O (\exp{-\mu T})$ for strongly convex
settings, where $T$ is the total number of iteration rounds. Additionally, we
achieve the state-of-the-art $O \left(n+\sqrt{n}\epsilon^{-1}\right)$ sample
complexities for non-convex settings and $O\left(n+ \sqrt{n} \ln
({\mu/\epsilon})\right)$ for strongly convex settings, where $\epsilon>0$ is a
desired stationarity error. Moreover, to alleviate the periodic full gradient
evaluation requirement in STIMULUS and STIMULUS-M, we further propose enhanced
versions with adaptive batching called STIMULUS+/ STIMULUS-M+ and provide their
theoretical analysis.

</details>


### [211] [FlightKooba: A Fast Interpretable FTP Model](https://arxiv.org/abs/2506.19885)
*Jing Lu,Xuan Wu,Yizhun Tian,Songhan Fan,Yali Fang*

Main category: cs.LG

TL;DR: 本文提出基于HIPPO方法、Koopman理论和控制论状态空间方程的FlightKooba框架，显著减少Koopman算子的计算复杂度，并提高模型可解释性、减少模型参数和训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有基于Koopman理论的飞行轨迹预测模型效能较低，可解释性和计算成本是瓶颈。

Method: 提出FlightKooba框架，通过HIPPO方法和状态空间方程，直接从数据中构建Koopman算子，从而优化模型性能及简化计算。

Result: 实验表明，FlightKooba在内存消耗和训练时间方面表现优异，在多数数据集上内存减少超50%，参数量减少十倍。

Conclusion: FlightKooba为快速计算Koopman算子提供了一种新方法，并在时间序列预测与控制结合方面呈现新可能性。

Abstract: The Koopman theory is a powerful and effective modeling tool for converting
nonlinear systems into linear representations, and flight trajectory prediction
(FTP) is a complex nonlinear system. However, current models applying the
Koopman theory to FTP tasks are not very effective, model interpretability is
indeed an issue, and the Koopman operators are computationally intensive,
resulting in long training times. To address this issue, this paper proposes a
new modeling and control framework based on the HIPPO method, the Koopman
theory, and state space equations from cybernetics: FlightKooba. Inspired by
the idea of structural state space equations, FlightKooba directly constructs
the Koopman operators from data. This makes the framework highly interpretable
and significantly reduces the number of trainable parameters in the module,
thereby greatly reducing training time. Experiments have demonstrated the
superiority of the FlightKooba modeling method in terms of time and memory
consumption (training time comparable to the Mamba module without using
CUDA-level acceleration; memory reduced by more than 50% on most datasets, with
a tenfold reduction in the number of parameters), essentially completing the
FTP task. It provides a new method for the fast computation of the Koopman
operators, opening up new possibilities for the combination of time series
forecasting and control.

</details>


### [212] [Neuromorphic Wireless Split Computing with Resonate-and-Fire Neurons](https://arxiv.org/abs/2506.20015)
*Dengyu Wu,Jiechen Chen,H. Vincent Poor,Bipin Rajendran,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 本文提出了一种基于共振神经元的无线分离计算架构，用于更高效地处理时间域信号，尤其在无需光谱预处理的情况下。


<details>
  <summary>Details</summary>
Motivation: 针对传统SNN中常见的耗能问题及无法有效捕捉丰富光谱特征的不足，设计更高效的信号处理方法。

Method: 利用可调共振频率的共振放电神经元（RF神经元）直接从时间域信号提取光谱特性，并采用OFDM无线接口实现尖峰信号的有效传输。

Result: 实验表明，该系统在音频分类和调制分类任务中，与传统神经网络和SNN性能相当，但有效减少了尖峰率和总能源消耗。

Conclusion: RF-SNN架构显著提升了时间序列处理中的能效，并在保持准确率的同时减少了计算与通信成本。

Abstract: Neuromorphic computing offers an energy-efficient alternative to conventional
deep learning accelerators for real-time time-series processing. However, many
edge applications, such as wireless sensing and audio recognition, generate
streaming signals with rich spectral features that are not effectively captured
by conventional leaky integrate-and-fire (LIF) spiking neurons. This paper
investigates a wireless split computing architecture that employs
resonate-and-fire (RF) neurons with oscillatory dynamics to process time-domain
signals directly, eliminating the need for costly spectral pre-processing. By
resonating at tunable frequencies, RF neurons extract time-localized spectral
features while maintaining low spiking activity. This temporal sparsity
translates into significant savings in both computation and transmission
energy. Assuming an OFDM-based analog wireless interface for spike
transmission, we present a complete system design and evaluate its performance
on audio classification and modulation classification tasks. Experimental
results show that the proposed RF-SNN architecture achieves comparable accuracy
to conventional LIF-SNNs and ANNs, while substantially reducing spike rates and
total energy consumption during inference and communication.

</details>


### [213] [A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior](https://arxiv.org/abs/2506.19999)
*Francesco Ignazio Re,Andreas Opedal,Glib Manaiev,Mario Giulianelli,Ryan Cotterell*

Main category: cs.LG

TL;DR: 本文提出一种基于标记时空点过程的新型概率模型，用于捕捉阅读过程中固定时间、空间位置和时间之间的动态关系，发现该模型在模拟人类的眼球运动方面表现优于传统方法，同时对使用“超出预期”的理论解释微观眼动问题提出质疑。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常采用汇总的眼动跟踪数据和带有强假设的模型，未能全面描述阅读过程中时空动态特征，亟需一种更为通用且细致的建模方法。

Method: 作者提出了一种基于标记时空点过程的阅读行为概率模型，利用Hawkes过程捕捉视线在时空中的激发模式，并用卷积函数建模视线的持续时间及其溢出效应。同时以上下文中语言信息的“意外性”作为模型预测因子之一。

Result: 实验证明所提出的Hawkes过程模型在模拟人类的跳视行为上相比基线方法表现更佳。对于视线的持续时间，加入上下文意外性预测因子仅带来边际改进，表明其理论在解释眼动微观细节时存在局限性。

Conclusion: 本文揭示了基于Hawkes过程模型的优势，并挑战了传统“意外性”理论在解释精细眼部活动上的有效性，为阅读行为研究提供了新视角和工具。

Abstract: Reading is a process that unfolds across space and time, alternating between
fixations where a reader focuses on a specific point in space, and saccades
where a reader rapidly shifts their focus to a new point. An ansatz of
psycholinguistics is that modeling a reader's fixations and saccades yields
insight into their online sentence processing. However, standard approaches to
such modeling rely on aggregated eye-tracking measurements and models that
impose strong assumptions, ignoring much of the spatio-temporal dynamics that
occur during reading. In this paper, we propose a more general probabilistic
model of reading behavior, based on a marked spatio-temporal point process,
that captures not only how long fixations last, but also where they land in
space and when they take place in time. The saccades are modeled using a Hawkes
process, which captures how each fixation excites the probability of a new
fixation occurring near it in time and space. The duration time of fixation
events is modeled as a function of fixation-specific predictors convolved
across time, thus capturing spillover effects. Empirically, our Hawkes process
model exhibits a better fit to human saccades than baselines. With respect to
fixation durations, we observe that incorporating contextual surprisal as a
predictor results in only a marginal improvement in the model's predictive
accuracy. This finding suggests that surprisal theory struggles to explain
fine-grained eye movements.

</details>


### [214] [Causal-Aware Intelligent QoE Optimization for VR Interaction with Adaptive Keyframe Extraction](https://arxiv.org/abs/2506.19890)
*Ziru Zhang,Jiadong Yu,Danny H. K. Tsang*

Main category: cs.LG

TL;DR: 本论文提出了一种智能框架，通过结合自适应关键帧提取与因果感知强化学习最大化多用户VR交互中的QoE。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分考虑带宽、CPU频率与用户感知之间的因果关系，限制了QoE的提升。

Method: 提出基于韦伯-费希纳定律的QoE指标，将QoE优化建模为混合整数规划任务，设计了部分状态因果深度确定性策略梯度（PS-CDDPG）算法，通过因果推断提高训练效率。

Result: 实验证明该框架显著降低交互延迟，提升QoE并保持公平性，表现优于基准方法。

Conclusion: 提出的框架在多用户VR场景中通过优化资源分配和因果决策显著改善了用户体验。

Abstract: The optimization of quality of experience (QoE) in multi-user virtual reality
(VR) interactions demands a delicate balance between ultra-low latency,
high-fidelity motion synchronization, and equitable resource allocation. While
adaptive keyframe extraction mitigates transmission overhead, existing
approaches often overlook the causal relationships among allocated bandwidth,
CPU frequency, and user perception, limiting QoE gains. This paper proposes an
intelligent framework to maximize QoE by integrating adaptive keyframe
extraction with causal-aware reinforcement learning (RL). First, a novel QoE
metric is formulated using the Weber-Fechner Law, combining perceptual
sensitivity, attention-driven priorities, and motion reconstruction accuracy.
The QoE optimization problem is then modeled as a mixed integer programming
(MIP) task, jointly optimizing keyframe ratios, bandwidth, and computational
resources under horizon-fairness constraints. We propose Partial State Causal
Deep Deterministic Policy Gradient (PS-CDDPG), which integrates the Deep
Deterministic Policy Gradient (DDPG) method with causal influence detection. By
leveraging causal information regarding how QoE is influenced and determined by
various actions, we explore actions guided by weights calculated from causal
inference (CI), which in turn improves training efficiency. Experiments
conducted with the CMU Motion Capture Database demonstrate that our framework
significantly reduces interactive latency, enhances QoE, and maintains
fairness, achieving superior performance compared to benchmark methods.

</details>


### [215] [Orthogonal Soft Pruning for Efficient Class Unlearning](https://arxiv.org/abs/2506.19891)
*Qinghui Gong,Xue Yang,Xiaohu Tang*

Main category: cs.LG

TL;DR: 本研究提出了一种基于正交卷积核约束和类别感知软剪枝的方法，实现了对已训练神经网络类别知识的快速准确遗忘。


<details>
  <summary>Details</summary>
Motivation: 随着隐私法规（如GDPR）的实施，要求模型能移除特定类别知识以满足数据隐私需求，而现有方法往往难以在高效遗忘和模型预测性能之间取得平衡，需要高计算资源或导致显著性能下降。

Method: 研究提出了正交卷积核正则化和类别感知软剪枝框架。通过训练过程中引入正交性约束和激活差异分析，去相关化卷积滤波器并解耦特征表征，从而快速确定类别相关的通道并实现类特定知识的遗忘。

Result: 在CIFAR-10、CIFAR-100与TinyImageNet上的实验表明，该方法可实现毫秒级响应时间、完全遗忘目标类别且对保留数据的准确性损失极小，并显著降低了成员推断攻击风险，对比当前最新方法显著加速了遗忘过程。

Conclusion: 这一框架为机器学习即服务中的实时遗忘任务提供了一种高效、实际的解决方案。

Abstract: Machine unlearning aims to selectively remove class-specific knowledge from
pretrained neural networks to satisfy privacy regulations such as the GDPR.
Existing methods typically face a trade-off between unlearning speed and
preservation of predictive accuracy, often incurring either high computational
overhead or significant performance degradation on retained classes. In this
paper, we propose a novel class-aware soft pruning framework leveraging
orthogonal convolutional kernel regularization to achieve rapid and precise
forgetting with millisecond-level response times. By enforcing orthogonality
constraints during training, our method decorrelates convolutional filters and
disentangles feature representations, while efficiently identifying
class-specific channels through activation difference analysis. Extensive
evaluations across multiple architectures and datasets demonstrate stable
pruning with near-instant execution, complete forgetting of targeted classes,
and minimal accuracy loss on retained data. Experiments on CIFAR-10, CIFAR-100,
and TinyImageNet confirm that our approach substantially reduces membership
inference attack risks and accelerates unlearning by orders of magnitude
compared to state-of-the-art baselines. This framework provides an efficient,
practical solution for real-time machine unlearning in Machine Learning as a
Service (MLaaS) scenarios.

</details>


### [216] [Distillation-Enabled Knowledge Alignment for Generative Semantic Communications in AIGC Provisioning Tasks](https://arxiv.org/abs/2506.19893)
*Jingzhi Hu,Geoffrey Ye Li*

Main category: cs.LG

TL;DR: 本文介绍了一种名为DeKA-g的算法，通过云端与边缘设备之间的知识蒸馏与对齐，提高生成语义通信（GSC）的效率与性能。


<details>
  <summary>Details</summary>
Motivation: 因为AI生成内容（AIGC）的激增，从云向边缘和移动用户传输数据给网络带来巨大压力，因此需要一种能够压缩信息传输的解决方案。

Method: 提出一种名为DeKA-g的知识对齐算法，包括元词辅助知识蒸馏（MAKD）和可变速率分组信噪比适配（VGSA）两种方法，分别提升知识蒸馏效率和适应不同信道条件的能力。

Result: DeKA-g在边缘与云端生成图像的一致性上提升了44%，在压缩效率上提升了116%，在低信噪比场景下性能提升了28%。

Conclusion: DeKA-g在解决生成语义通信中知识对齐问题上表现出优异效果，可以显著改善AIGC数据传输的效率和性能。

Abstract: Due to the surging amount of AI-generated content (AIGC), its provisioning to
edges and mobile users from the cloud incurs substantial traffic on networks.
Generative semantic communication (GSC) offers a promising solution by
transmitting highly compact information, i.e., prompt text and latent
representations, instead of high-dimensional AIGC data. However, GSC relies on
the alignment between the knowledge in the cloud generative AI (GAI) and that
possessed by the edges and users, and between the knowledge for wireless
transmission and that of actual channels, which remains challenging. In this
paper, we propose DeKA-g, a distillation-enabled knowledge alignment algorithm
for GSC systems. The core idea is to distill the generation knowledge from the
cloud-GAI into low-rank matrices, which can be incorporated by the edge and
used to adapt the transmission knowledge to diverse wireless channel
conditions. DeKA-g comprises two novel methods: metaword-aided knowledge
distillation (MAKD) and variable-rate grouped SNR adaptation (VGSA). For MAKD,
an optimized metaword is employed to enhance the efficiency of knowledge
distillation, while VGSA enables efficient adaptation to diverse compression
rates and SNR ranges. From simulation results, DeKA-g improves the alignment
between the edge-generated images and the cloud-generated ones by 44%.
Moreover, it adapts to compression rates with 116% higher efficiency than the
baseline and enhances the performance in low-SNR conditions by 28%.

</details>


### [217] [Explaining deep neural network models for electricity price forecasting with XAI](https://arxiv.org/abs/2506.19894)
*Antoine Pesenti,Aidan OSullivan*

Main category: cs.LG

TL;DR: 本文通过使用深度神经网络（DNN）预测电力市场价格，并结合可解释人工智能（XAI）方法，如SHAP和梯度方法，来分析市场价格动态的驱动因素。


<details>
  <summary>Details</summary>
Motivation: 传统经济计量方法在分析电力市场及其价格驱动上能力有限，而深度神经网络在处理复杂数据和关系上更具优势。

Method: 运用深度神经网络预测电力市场价格，通过结合SHAP、梯度方法以及可视化技术（如热图），分析五个电力市场中不同特征的行为与贡献。此外，引入SSHAP值和SSHAP线的概念，以更好表示高维表格模型。

Result: 模型成功预测了电力市场价格，并通过SSHAP值显著提升了对高维数据的解释能力。

Conclusion: 该研究证明了结合深度学习与可解释性方法能够改进对电力市场价格动态的理解，并为领域研究提供新的解析工具。

Abstract: Electricity markets are highly complex, involving lots of interactions and
complex dependencies that make it hard to understand the inner workings of the
market and what is driving prices. Econometric methods have been developed for
this, white-box models, however, they are not as powerful as deep neural
network models (DNN). In this paper, we use a DNN to forecast the price and
then use XAI methods to understand the factors driving the price dynamics in
the market. The objective is to increase our understanding of how different
electricity markets work. To do that, we apply explainable methods such as SHAP
and Gradient, combined with visual techniques like heatmaps (saliency maps) to
analyse the behaviour and contributions of various features across five
electricity markets. We introduce the novel concepts of SSHAP values and SSHAP
lines to enhance the complex representation of high-dimensional tabular models.

</details>


### [218] [A Framework for Uncertainty Quantification Based on Nearest Neighbors Across Layers](https://arxiv.org/abs/2506.19895)
*Miguel N. Font,José L. Jorro-Aragoneses,Carlos M. Alaíz*

Main category: cs.LG

TL;DR: 本文提出了一种新的后处理框架，通过利用与查询具有相似激活向量的训练样本来评估神经网络决策的不确定性，并提出了两个新指标量化不确定性。


<details>
  <summary>Details</summary>
Motivation: 神经网络在高风险领域（如医疗诊断、自动驾驶）中出错可能造成严重后果，因此需要一种有效的方法来测量并缓解这些错误中的不确定性。

Method: 提出一个基于后处理的不确定性评估框架，通过检索与输入数据激活向量相似的训练样本，计算两种新指标——决策变化和层级不确定性，来量化各层的类别分布变化。

Result: 在CIFAR-10和MNIST数据集测试中，该方法在处理具有挑战性的分类任务时提升了不确定性估计效果，优于传统的基于SoftMax的置信度估算方法。

Conclusion: 通过结合层级激活的训练样本信息，该方法有效提升了神经网络在复杂分类任务中的不确定性估计能力，具有应用潜力。

Abstract: Neural Networks have high accuracy in solving problems where it is difficult
to detect patterns or create a logical model. However, these algorithms
sometimes return wrong solutions, which become problematic in high-risk domains
like medical diagnosis or autonomous driving. One strategy to detect and
mitigate these errors is the measurement of the uncertainty over neural network
decisions. In this paper, we present a novel post-hoc framework for measuring
the uncertainty of a decision based on retrieved training cases that have a
similar activation vector to the query for each layer. Based on these retrieved
cases, we propose two new metrics: Decision Change and Layer Uncertainty, which
capture changes in nearest-neighbor class distributions across layers. We
evaluated our approach in a classification model for two datasets: CIFAR-10 and
MNIST. The results show that these metrics enhance uncertainty estimation,
especially in challenging classification tasks, outperforming softmax-based
confidence.

</details>


### [219] [MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations](https://arxiv.org/abs/2506.20100)
*Vardhan Dongre,Chi Gui,Shubham Garg,Hooshang Nayyeri,Gokhan Tur,Dilek Hakkani-Tür,Vikram S. Adve*

Main category: cs.LG

TL;DR: MIRAGE是一种为农业领域设计的多模态推理与决策基准，其特点是结合了真实用户查询、专家回复和图像上下文。


<details>
  <summary>Details</summary>
Motivation: 填补现有基准中缺少开放世界场景与复杂推理的空白，提升模型在知识密集型领域的能力。

Method: 通过超过35,000次用户-专家互动记录，采用多步骤流程精心策划，以生成包含自然查询、专家回复和视觉信息的基准数据。

Result: 创建了一个包含7,000多种生物实体、具有多样化作物健康诊断场景和知识推理需求的高保真基准。

Conclusion: MIRAGE为真实世界的视觉语言模型提供了更具挑战性的评估环境，促进了在农业领域多模态推理的研究与发展。

Abstract: We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning
and decision-making in consultative interaction settings. Designed for the
agriculture domain, MIRAGE captures the full complexity of expert consultations
by combining natural user queries, expert-authored responses, and image-based
context, offering a high-fidelity benchmark for evaluating models on grounded
reasoning, clarification strategies, and long-form generation in a real-world,
knowledge-intensive domain. Grounded in over 35,000 real user-expert
interactions and curated through a carefully designed multi-step pipeline,
MIRAGE spans diverse crop health, pest diagnosis, and crop management
scenarios. The benchmark includes more than 7,000 unique biological entities,
covering plant species, pests, and diseases, making it one of the most
taxonomically diverse benchmarks available for vision-language models, grounded
in the real world. Unlike existing benchmarks that rely on well-specified user
inputs and closed-set taxonomies, MIRAGE features underspecified, context-rich
scenarios with open-world settings, requiring models to infer latent knowledge
gaps, handle rare entities, and either proactively guide the interaction or
respond. Project Page: https://mirage-benchmark.github.io

</details>


### [220] [A Comparative Analysis of Reinforcement Learning and Conventional Deep Learning Approaches for Bearing Fault Diagnosis](https://arxiv.org/abs/2506.19929)
*Efe Çakır,Patrick Dumond*

Main category: cs.LG

TL;DR: 研究探索了使用强化学习（RL）中的深度Q网络（DQN）进行轴承故障分类任务，以提高诊断的准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 解决轴承故障诊断中传统方法对数据需求高、难以适应动态环境的问题。

Method: 采用强化学习技术，特别是深度Q网络（DQN），进行轴承故障分类实验，并优化奖励结构以提升其适应性。

Result: 研究表明RL模型在受控条件下表现与监督学习模型相当，但在适应性方面表现优越，也暴露了高计算需求的问题。

Conclusion: 强化学习可作为传统方法的有力补充，为适应性诊断框架的构建奠定基础。

Abstract: Bearing faults in rotating machinery can lead to significant operational
disruptions and maintenance costs. Modern methods for bearing fault diagnosis
rely heavily on vibration analysis and machine learning techniques, which often
require extensive labeled data and may not adapt well to dynamic environments.
This study explores the feasibility of reinforcement learning (RL),
specifically Deep Q-Networks (DQNs), for bearing fault classification tasks in
machine condition monitoring to enhance the accuracy and adaptability of
bearing fault diagnosis. The results demonstrate that while RL models developed
in this study can match the performance of traditional supervised learning
models under controlled conditions, they excel in adaptability when equipped
with optimized reward structures. However, their computational demands
highlight areas for further improvement. These findings demonstrate RL's
potential to complement traditional methods, paving the way for adaptive
diagnostic frameworks.

</details>


### [221] [Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and Architecture](https://arxiv.org/abs/2506.19935)
*Shuchen Xue,Tianyu Xie,Tianyang Hu,Zijin Feng,Jiacheng Sun,Kenji Kawaguchi,Zhenguo Li,Zhi-Ming Ma*

Main category: cs.LG

TL;DR: 该研究对比了自回归（AR）模型和蒙版扩散模型（MDM），分析了其框架和架构差异的影响。


<details>
  <summary>Details</summary>
Motivation: 目前AR模型和MDM通常使用不同的架构（分别为解码器和编码器架构），使得对比其性能时难以区分是由模型范式还是架构引起的差异。本研究旨在公平对比这些模型并提供相关设计启示。

Method: 将MDM置于解码器架构中，进行公平对比，同时探讨不同架构对MDM的影响。还优化了标准AO-AR目标函数以调整无信息的token置换。

Result: 研究显示，解码器架构下的MDM比编码器更快（生成速度提升约25倍），并且通过温度退火可以实现与编码器架构可比较的困惑度，尽管其建模空间更大。

Conclusion: 研究剖析了范式差异与架构的独立影响，为未来语言模型设计提供了重要见解。

Abstract: Large language models (LLMs) predominantly use autoregressive (AR)
approaches, but masked diffusion models (MDMs) are emerging as viable
alternatives. A key challenge in comparing AR and MDM paradigms is their
typical architectural difference: AR models are often decoder-only, while MDMs
have largely been encoder-only. This practice of changing both the modeling
paradigm and architecture simultaneously makes direct comparisons unfair, as
it's hard to distinguish whether observed differences stem from the paradigm
itself or the architectural shift. This research evaluates MDMs within a
decoder-only framework to: (1) equitably compare MDM (as Any-Order AR, or
AO-AR) and standard AR paradigms. Our investigation suggests that the standard
AO-AR objective, which averages over all token permutations, may benefit from
refinement, as many permutations appear less informative compared to the
language's inherent left-to-right structure. (2) Investigate architectural
influences (decoder-only vs. encoder-only) within MDMs. We demonstrate that
while encoder-only MDMs model a simpler conditional probability space,
decoder-only MDMs can achieve dramatic generation speedups ($\sim25\times$) and
comparable perplexity with temperature annealing despite modeling a vastly
larger space, highlighting key trade-offs. This work thus decouples core
paradigm differences from architectural influences, offering insights for
future model design. Code is available at https://github.com/scxue/AO-GPT-MDM.

</details>


### [222] [The Most Important Features in Generalized Additive Models Might Be Groups of Features](https://arxiv.org/abs/2506.19937)
*Tomas M. Bosschieter,Luis Franca,Jessica Wolk,Yiyuan Wu,Bella Mehta,Joseph Dehoney,Orsolya Kiss,Fiona C. Baker,Qingyu Zhao,Rich Caruana,Kilian M. Pohl*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，用于在广义可加模型（GAMs）中评估特征组的重要性，特别适用于多模态数据集以及高维环境，提高整体分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有办法在评估特征重要性时，常忽略特征组的联合信号，而在许多情况下，最显著的预测器来自特征组的组合效果，而非单一特征。

Method: 提出了一种高效且无需模型重训的特征组重要性评估方法，支持后定义特征组、组内特征可重叠且适用于高维场景；并在统计上与解释变量分析建立了联系。

Result: 通过三个合成实验验证了方法在不同数据条件下的适用性；通过案例研究表明，特征组分析在医学问题诊断中能提供比单一特征更为全面的见解。

Conclusion: 特征组重要性分析揭示了更具洞察力的关联关系，在医学和其他多模态数据问题中具有重要应用价值。

Abstract: While analyzing the importance of features has become ubiquitous in
interpretable machine learning, the joint signal from a group of related
features is sometimes overlooked or inadvertently excluded. Neglecting the
joint signal could bypass a critical insight: in many instances, the most
significant predictors are not isolated features, but rather the combined
effect of groups of features. This can be especially problematic for datasets
that contain natural groupings of features, including multimodal datasets. This
paper introduces a novel approach to determine the importance of a group of
features for Generalized Additive Models (GAMs) that is efficient, requires no
model retraining, allows defining groups posthoc, permits overlapping groups,
and remains meaningful in high-dimensional settings. Moreover, this definition
offers a parallel with explained variation in statistics. We showcase
properties of our method on three synthetic experiments that illustrate the
behavior of group importance across various data regimes. We then demonstrate
the importance of groups of features in identifying depressive symptoms from a
multimodal neuroscience dataset, and study the importance of social
determinants of health after total hip arthroplasty. These two case studies
reveal that analyzing group importance offers a more accurate, holistic view of
the medical issues compared to a single-feature analysis.

</details>


### [223] [HERCULES: Hierarchical Embedding-based Recursive Clustering Using LLMs for Efficient Summarization](https://arxiv.org/abs/2506.19992)
*Gabor Petnehazi,Bernadett Aradi*

Main category: cs.LG

TL;DR: 提出HERCULES算法和Python包，用于多种数据类型的分层k均值聚类，结合大语言模型（LLMs）增强解释性。


<details>
  <summary>Details</summary>
Motivation: 应对多模态复杂数据集增长的需求，提取高效、可解释的结构化知识。

Method: 利用分层k均值聚类技术，结合LLMs生成语义丰富的聚类标题和描述，支持`direct`和`description`两种表示模式，并提供交互式可视化工具。

Result: HERCULES展现出从复杂数据集中提取有意义分层知识的潜力。

Conclusion: HERCULES增强了解复杂数据集的能力，为数据分析提供了新方法。

Abstract: The explosive growth of complex datasets across various modalities
necessitates advanced analytical tools that not only group data effectively but
also provide human-understandable insights into the discovered structures. We
introduce HERCULES (Hierarchical Embedding-based Recursive Clustering Using
LLMs for Efficient Summarization), a novel algorithm and Python package
designed for hierarchical k-means clustering of diverse data types, including
text, images, and numeric data (processed one modality per run). HERCULES
constructs a cluster hierarchy by recursively applying k-means clustering,
starting from individual data points at level 0. A key innovation is its deep
integration of Large Language Models (LLMs) to generate semantically rich
titles and descriptions for clusters at each level of the hierarchy,
significantly enhancing interpretability. The algorithm supports two main
representation modes: `direct' mode, which clusters based on original data
embeddings or scaled numeric features, and `description' mode, which clusters
based on embeddings derived from LLM-generated summaries. Users can provide a
`topic\_seed' to guide LLM-generated summaries towards specific themes. An
interactive visualization tool facilitates thorough analysis and understanding
of the clustering results. We demonstrate HERCULES's capabilities and discuss
its potential for extracting meaningful, hierarchical knowledge from complex
datasets.

</details>


### [224] [TRACED: Transition-aware Regret Approximation with Co-learnability for Environment Design](https://arxiv.org/abs/2506.19997)
*Geonwoo Cho,Jaegyun Im,Jihwan Lee,Hojun Yi,Sejin Kim,Sundong Kim*

Main category: cs.LG

TL;DR: 提出TRACED方法，通过改进的遗憾近似与任务关系建模，提高了无监督环境设计中代理的零次泛化能力，同时显著降低环境交互需求。


<details>
  <summary>Details</summary>
Motivation: 强化学习代理难以在未见过的环境中泛化，无监督环境设计通过生成高潜力任务是一种可能的解决方式，需要更高效的损失评估来指导任务生成。

Method: 提出使用转移预测误差和协同学习能力相结合的TRACED方法，通过改进遗憾估计和任务关系建模，为学生代理提供更有效的学习任务。

Result: 实验表明，与现有基线相比，TRACED在多个基准测试中可提升零次泛化能力，同时环境交互次数减少最多可达2倍。

Conclusion: 通过改进遗憾近似和显式的任务关系建模，TRACED实现了样本高效的课程设计，为无监督环境设计提供了新的思路。

Abstract: Generalizing deep reinforcement learning agents to unseen environments
remains a significant challenge. One promising solution is Unsupervised
Environment Design (UED), a co-evolutionary framework in which a teacher
adaptively generates tasks with high learning potential, while a student learns
a robust policy from this evolving curriculum. Existing UED methods typically
measure learning potential via regret, the gap between optimal and current
performance, approximated solely by value-function loss. Building on these
approaches, we introduce the transition prediction error as an additional term
in our regret approximation. To capture how training on one task affects
performance on others, we further propose a lightweight metric called
co-learnability. By combining these two measures, we present Transition-aware
Regret Approximation with Co-learnability for Environment Design (TRACED).
Empirical evaluations show that TRACED yields curricula that improve zero-shot
generalization across multiple benchmarks while requiring up to 2x fewer
environment interactions than strong baselines. Ablation studies confirm that
the transition prediction error drives rapid complexity ramp-up and that
co-learnability delivers additional gains when paired with the transition
prediction error. These results demonstrate how refined regret approximation
and explicit modeling of task relationships can be leveraged for
sample-efficient curriculum design in UED.

</details>


### [225] [New Insights on Unfolding and Fine-tuning Quantum Federated Learning](https://arxiv.org/abs/2506.20016)
*Shanika Iroshi Nanayakkara,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: 通过引入深度展开的方法，该研究优化了量子联邦学习中的超参数调节，提升了模型在高度异质环境下的适配性和准确性。


<details>
  <summary>Details</summary>
Motivation: 量子联邦学习受客户端的异质性限制，现有方法容易过拟合，性能较差，难以应用于复杂领域。

Method: 引入基于深度展开的框架，允许客户端根据其特定训练行为自主优化超参数（如学习率和正则化因子）。

Result: 框架在IBM量子硬件和Qiskit Aer模拟器上的实时训练中实现了约90%的准确率，大幅高于传统方法的约55%。

Conclusion: 通过收敛自适应的优化和泛化能力，该研究克服了传统量子联邦学习在异质性环境中的核心局限性，尤其在医疗健康及基因组研究领域显示出巨大潜力。

Abstract: Client heterogeneity poses significant challenges to the performance of
Quantum Federated Learning (QFL). To overcome these limitations, we propose a
new approach leveraging deep unfolding, which enables clients to autonomously
optimize hyperparameters, such as learning rates and regularization factors,
based on their specific training behavior. This dynamic adaptation mitigates
overfitting and ensures robust optimization in highly heterogeneous
environments where standard aggregation methods often fail. Our framework
achieves approximately 90% accuracy, significantly outperforming traditional
methods, which typically yield around 55% accuracy, as demonstrated through
real-time training on IBM quantum hardware and Qiskit Aer simulators. By
developing self adaptive fine tuning, the proposed method proves particularly
effective in critical applications such as gene expression analysis and cancer
detection, enhancing diagnostic precision and predictive modeling within
quantum systems. Our results are attributed to convergence-aware, learnable
optimization steps intrinsic to the deep unfolded framework, which maintains
the generalization. Hence, this study addresses the core limitations of
conventional QFL, streamlining its applicability to any complex challenges such
as healthcare and genomic research.

</details>


### [226] [DIM-SUM: Dynamic IMputation for Smart Utility Management](https://arxiv.org/abs/2506.20023)
*Ryan Hildebrant,Rahul Bhope,Sharad Mehrotra,Christopher Tull,Nalini Venkatasubramanian*

Main category: cs.LG

TL;DR: 本文提出DIM-SUM框架，通过结合模式聚类和自适应屏蔽策略，提高时间序列数据缺失值插补模型的鲁棒性，特别适用于现实中复杂多样的缺失模式。


<details>
  <summary>Details</summary>
Motivation: 针对传统模型通常基于完备数据的人工屏蔽训练，而实际数据缺失模式复杂、多样的现象，提出能更好适应真实缺失数据的插补框架。

Method: DIM-SUM框架结合了缺失模式聚类、自适应屏蔽策略，并提供理论学习保证以处理多样的缺失模式。

Result: 实验表明，与传统方法相比，DIM-SUM在数据处理时间更短、训练数据需求更少的情况下达到相似精度；与大规模预训练模型相比，精度高出2倍，推理时间显著减少。

Conclusion: DIM-SUM通过提高插补模型对真实世界数据缺失模式的处理能力，有效提升模型精度与效率，为基础设施监测数据的缺失值插补提供了高效解决方案。

Abstract: Time series imputation models have traditionally been developed using
complete datasets with artificial masking patterns to simulate missing values.
However, in real-world infrastructure monitoring, practitioners often encounter
datasets where large amounts of data are missing and follow complex,
heterogeneous patterns. We introduce DIM-SUM, a preprocessing framework for
training robust imputation models that bridges the gap between artificially
masked training data and real missing patterns. DIM-SUM combines pattern
clustering and adaptive masking strategies with theoretical learning guarantees
to handle diverse missing patterns actually observed in the data. Through
extensive experiments on over 2 billion readings from California water
districts, electricity datasets, and benchmarks, we demonstrate that DIM-SUM
outperforms traditional methods by reaching similar accuracy with lower
processing time and significantly less training data. When compared against a
large pre-trained model, DIM-SUM averages 2x higher accuracy with significantly
less inference time.

</details>


### [227] [Elucidated Rolling Diffusion Models for Probabilistic Weather Forecasting](https://arxiv.org/abs/2506.20024)
*Salva Rühling Cachay,Miika Aittala,Karsten Kreis,Noah Brenowitz,Arash Vahdat,Morteza Mardani,Rose Yu*

Main category: cs.LG

TL;DR: 該研究提出了一種新的預測框架（ERDM），將滾動預測與高效的擴散模型設計成功結合，並在多種基準測試中表現出色。


<details>
  <summary>Details</summary>
Motivation: 探討高維混沌系統的複雜時間依賴性和逐漸增長的不確定性，以及目前預測方法面臨的挑戰，如單次快照預測模型難以捕捉時序依賴性與增長不確定性問題。

Method: 提出了Elucidated Rolling Diffusion Models（ERDM），透過改進EDM的噪聲計畫、網絡預處理與取樣，結合以下關鍵方法：1. 全新的損失加權機制；2. 基於預訓練EDM的高效初始化策略；3. 混合序列架構進行空間-時間特徵提取。

Result: 在2D Navier-Stokes模擬和ERA5全球氣象數據集的1.5度解析度測試中，ERDM的性能優於現有擴散模型基準（例如條件自回歸EDM）。

Conclusion: ERDM框架有效結合滾動預測結構和高性能擴散技術，為處理高不確定性序列生成問題提供了靈活、強大的解決方案。

Abstract: Diffusion models are a powerful tool for probabilistic forecasting, yet most
applications in high-dimensional chaotic systems predict future snapshots
one-by-one. This common approach struggles to model complex temporal
dependencies and fails to explicitly account for the progressive growth of
uncertainty inherent to such systems. While rolling diffusion frameworks, which
apply increasing noise to forecasts at longer lead times, have been proposed to
address this, their integration with state-of-the-art, high-fidelity diffusion
techniques remains a significant challenge. We tackle this problem by
introducing Elucidated Rolling Diffusion Models (ERDM), the first framework to
successfully unify a rolling forecast structure with the principled, performant
design of Elucidated Diffusion Models (EDM). To do this, we adapt the core EDM
components-its noise schedule, network preconditioning, and Heun sampler-to the
rolling forecast setting. The success of this integration is driven by three
key contributions: (i) a novel loss weighting scheme that focuses model
capacity on the mid-range forecast horizons where determinism gives way to
stochasticity; (ii) an efficient initialization strategy using a pre-trained
EDM for the initial window; and (iii) a bespoke hybrid sequence architecture
for robust spatiotemporal feature extraction under progressive denoising. On 2D
Navier-Stokes simulations and ERA5 global weather forecasting at 1.5^\circ
resolution, ERDM consistently outperforms key diffusion-based baselines,
including conditional autoregressive EDM. ERDM offers a flexible and powerful
general framework for tackling diffusion-based sequence generation problems
where modeling escalating uncertainty is paramount. Code is available at:
https://github.com/salvaRC/erdm

</details>


### [228] [Thumb on the Scale: Optimal Loss Weighting in Last Layer Retraining](https://arxiv.org/abs/2506.20025)
*Nathan Stromberg,Christos Thrampoulidis,Lalitha Sankar*

Main category: cs.LG

TL;DR: 本研究探讨了在最后一层重新训练（LLR）环境中偏差调整问题的重要性，表明调整损失权重对模型在训练数据偏差时仍然有效。


<details>
  <summary>Details</summary>
Motivation: 在模型在训练数据中存在偏差的背景下，研究者希望找到一种方法提高模型对各种类别的公平性表现，特别是在模型大小和数据不可分性介于两端极端之间的情况。

Method: 研究重点在最后一层重新训练(LLR)的情境下，对损失权重进行调整以适应模型的相对超参数化程度，评估其理论和实践上的有效性。

Result: 研究发现，在LLR环境中调整损失权重仍是有效的，但需要根据模型的相对超参数化程度来决定权重设置。

Conclusion: 在模型大小适中的重新训练情况中，适当的损失权重调整是减少偏差影响并确保模型公平性的关键。

Abstract: While machine learning models become more capable in discriminative tasks at
scale, their ability to overcome biases introduced by training data has come
under increasing scrutiny. Previous results suggest that there are two extremes
of parameterization with very different behaviors: the population
(underparameterized) setting where loss weighting is optimal and the separable
overparameterized setting where loss weighting is ineffective at ensuring equal
performance across classes. This work explores the regime of last layer
retraining (LLR) in which the unseen limited (retraining) data is frequently
inseparable and the model proportionately sized, falling between the two
aforementioned extremes. We show, in theory and practice, that loss weighting
is still effective in this regime, but that these weights \emph{must} take into
account the relative overparameterization of the model.

</details>


### [229] [Automated Generation of Diverse Courses of Actions for Multi-Agent Operations using Binary Optimization and Graph Learning](https://arxiv.org/abs/2506.20031)
*Prithvi Poddar,Ehsan Tarkesh Esfahani,Karthik Dantu,Souma Chowdhury*

Main category: cs.LG

TL;DR: 本文提出了一种新的理论框架与计算方法，用以在多代理任务环境中生成多样的行动方案 (COAs)，同时考虑任务分配优化与任务序列优化。


<details>
  <summary>Details</summary>
Motivation: 应对灾难响应、搜救与军事行动等需要多代理协调的任务环境中，存在因环境因素及代理能力变化导致行动方案性能下降的问题，需要一种具备任务分布多样性与适应能力的解决方案支持过程规划。

Method: 通过图论抽象任务空间及行动方案池，本文将行动方案规划表述为集中式多机器人任务分配问题。利用遗传算法优化任务分布多样性与代理任务映射兼容性，并通过图神经网络实现单代理任务序列优化，同时使用策略梯度进行训练。

Result: 在模拟环境中的测试表明，该方法相比随机基线可以显著提高性能，任务序列优化接近最优，同时在50分钟内完成对5个代理/100个任务的20个行动方案规划。

Conclusion: 本文提出的方法能够有效生成多样高效的行动方案，为多代理系统的任务规划提供了新的理论与实践支持。

Abstract: Operations in disaster response, search \& rescue, and military missions that
involve multiple agents demand automated processes to support the planning of
the courses of action (COA). Moreover, traverse-affecting changes in the
environment (rain, snow, blockades, etc.) may impact the expected performance
of a COA, making it desirable to have a pool of COAs that are diverse in task
distributions across agents. Further, variations in agent capabilities, which
could be human crews and/or autonomous systems, present practical opportunities
and computational challenges to the planning process. This paper presents a new
theoretical formulation and computational framework to generate such diverse
pools of COAs for operations with soft variations in agent-task compatibility.
Key to the problem formulation is a graph abstraction of the task space and the
pool of COAs itself to quantify its diversity. Formulating the COAs as a
centralized multi-robot task allocation problem, a genetic algorithm is used
for (order-ignoring) allocations of tasks to each agent that jointly maximize
diversity within the COA pool and overall compatibility of the agent-task
mappings. A graph neural network is trained using a policy gradient approach to
then perform single agent task sequencing in each COA, which maximizes
completion rates adaptive to task features. Our tests of the COA generation
process in a simulated environment demonstrate significant performance gain
over a random walk baseline, small optimality gap in task sequencing, and
execution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task
operations.

</details>


### [230] [Verifiable Unlearning on Edge](https://arxiv.org/abs/2506.20037)
*Mohammad M Maheri,Alex Davidson,Hamed Haddadi*

Main category: cs.LG

TL;DR: 提出一种利用零知识证明（zk-SNARKs）验证边缘设备模型数据遗忘的方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决数据版权、偏见和法规要求等问题，需要在所有边缘设备中可验证地删除某些数据样本。

Method: 应用zk-SNARKs设计一个验证框架，以隐私保护的方式确保边缘设备上的数据遗忘。提出相关算法以实现与zk-SNARKs高效证明生成兼容并优化性能开销。

Result: 实验结果表明，该验证框架可以在最小化性能退化的同时验证数据遗忘的执行。

Conclusion: 确保在边缘设备环境中实现隐私保护和有效的数据遗忘验证，且对个性化模型性能影响较小。

Abstract: Machine learning providers commonly distribute global models to edge devices,
which subsequently personalize these models using local data. However, issues
such as copyright infringements, biases, or regulatory requirements may require
the verifiable removal of certain data samples across all edge devices.
Ensuring that edge devices correctly execute such unlearning operations is
critical to maintaining integrity.
  In this work, we introduce a verification framework leveraging zero-knowledge
proofs, specifically zk-SNARKs, to confirm data unlearning on personalized
edge-device models without compromising privacy. We have developed algorithms
explicitly designed to facilitate unlearning operations that are compatible
with efficient zk-SNARK proof generation, ensuring minimal computational and
memory overhead suitable for constrained edge environments. Furthermore, our
approach carefully preserves personalized enhancements on edge devices,
maintaining model performance post-unlearning.
  Our results affirm the practicality and effectiveness of this verification
framework, demonstrating verifiable unlearning with minimal degradation in
personalization-induced performance improvements. Our methodology ensures
verifiable, privacy-preserving, and effective machine unlearning across edge
devices.

</details>


### [231] [Cross-Layer Discrete Concept Discovery for Interpreting Language Models](https://arxiv.org/abs/2506.20040)
*Ankur Garg,Xuemin Yu,Hassan Sajjad,Samira Ebrahimi Kahou*

Main category: cs.LG

TL;DR: 本文提出了CLVQVAE框架，用以通过向量量化映射跨层表示并压缩残差流特征至紧凑的可解释概念向量，从而解决跨层冗余问题。


<details>
  <summary>Details</summary>
Motivation: 解决跨层信息的线性混合和冗余问题，揭示Transformer层中特征的演化方式。

Method: 引入CLVQVAE框架，结合了基于top-k温度的量化采样和EMA码本更新，同时使用扩展的球面k-means++进行码本初始化以优化语义对齐。

Result: 提出的框架能够有效压缩和解释跨层残差流中的重复特征，提供更清晰的语义结构。

Conclusion: 通过CLVQVAE框架，能够在语言模型中更深入地理解跨层概念的形成及其进化。

Abstract: Uncovering emergent concepts across transformer layers remains a significant
challenge because the residual stream linearly mixes and duplicates
information, obscuring how features evolve within large language models.
Current research efforts primarily inspect neural representations at single
layers, thereby overlooking this cross-layer superposition and the redundancy
it introduces. These representations are typically either analyzed directly for
activation patterns or passed to probing classifiers that map them to a limited
set of predefined concepts. To address these limitations, we propose
\gls{clvqvae}, a framework that uses vector quantization to map representations
across layers and in the process collapse duplicated residual-stream features
into compact, interpretable concept vectors. Our approach uniquely combines
top-$k$ temperature-based sampling during quantization with EMA codebook
updates, providing controlled exploration of the discrete latent space while
maintaining code-book diversity. We further enhance the framework with
scaled-spherical k-means++ for codebook initialization, which clusters by
directional similarity rather than magnitude, better aligning with semantic
structure in word embedding space.

</details>


### [232] [LSH-DynED: A Dynamic Ensemble Framework with LSH-Based Undersampling for Evolving Multi-Class Imbalanced Classification](https://arxiv.org/abs/2506.20041)
*Soheil Abadifard,Fazli Can*

Main category: cs.LG

TL;DR: 本文提出了一种创新方法，将局部敏感哈希与随机超平面投影（LSH-RHP）集成到动态集成多样化（DynED）框架中，用于解决多分类不平衡数据流的分类问题。


<details>
  <summary>Details</summary>
Motivation: 现有关于不平衡数据流的数据分类方法多集中于二分类领域，而多分类的非平稳不平衡数据流面临动态平衡比率和大规模数据集的挑战。

Method: 文章利用LSH-RHP进行欠采样，生成平衡训练集，并结合DynED框架提升分类性能。

Result: 在23个真实数据集和10个半合成数据集的实验中，该方法在Kappa和mG-Mean指标上优于15种先进方法，并表现出对大规模高维数据的鲁棒性。

Conclusion: LSH-DynED为处理多类别不平衡数据流提供了有效的解决方案，同时为未来研究指出了方向，并公开了代码以保证可重复性。

Abstract: The classification of imbalanced data streams, which have unequal class
distributions, is a key difficulty in machine learning, especially when dealing
with multiple classes. While binary imbalanced data stream classification tasks
have received considerable attention, only a few studies have focused on
multi-class imbalanced data streams. Effectively managing the dynamic imbalance
ratio is a key challenge in this domain. This study introduces a novel, robust,
and resilient approach to address these challenges by integrating Locality
Sensitive Hashing with Random Hyperplane Projections (LSH-RHP) into the Dynamic
Ensemble Diversification (DynED) framework. To the best of our knowledge, we
present the first application of LSH-RHP for undersampling in the context of
imbalanced non-stationary data streams. The proposed method undersamples the
majority classes by utilizing LSH-RHP, provides a balanced training set, and
improves the ensemble's prediction performance. We conduct comprehensive
experiments on 23 real-world and ten semi-synthetic datasets and compare
LSH-DynED with 15 state-of-the-art methods. The results reveal that LSH-DynED
outperforms other approaches in terms of both Kappa and mG-Mean effectiveness
measures, demonstrating its capability in dealing with multi-class imbalanced
non-stationary data streams. Notably, LSH-DynED performs well in large-scale,
high-dimensional datasets with considerable class imbalances and demonstrates
adaptation and robustness in real-world circumstances. To motivate our design,
we review existing methods for imbalanced data streams, outline key challenges,
and offer guidance for future work. For the reproducibility of our results, we
have made our implementation available on GitHub.

</details>


### [233] [GNN's Uncertainty Quantification using Self-Distillation](https://arxiv.org/abs/2506.20046)
*Hirad Daneshvar,Reza Samavi*

Main category: cs.LG

TL;DR: 该论文提出一种基于知识蒸馏的方法，用于高效且精确地量化图神经网络（GNNs）的预测不确定性，尤其在医疗领域。


<details>
  <summary>Details</summary>
Motivation: 现有方法如贝叶斯方法和集成方法，尽管可以量化预测的不确定性，但计算复杂且成本高；尤其是集成方法中的分歧指标不能很好地捕捉模型间的多样性。但在医疗领域，量化信任度和不确定性又至关重要。

Method: 该方法基于自蒸馏技术，通过让同一网络作为教师和学生模型，避免了独立训练多个网络。此外，引入了一种新的不确定性度量方法，该方法通过对不同GNN分类器赋予不同权重，捕捉了网络的多样性。

Result: 通过在MIMIC-IV和Enzymes两种数据集上的实验验证，这种新方法在区分分布外数据能力、精确性及性能方面不亚于MC Dropout和传统集成方法，并有效捕捉预测不确定性。

Conclusion: 该方法在保持性能的同时显著提升计算效率，是量化GNN预测不确定性的有效工具。代码已开源促进后续研究。

Abstract: Graph Neural Networks (GNNs) have shown remarkable performance in the
healthcare domain. However, what remained challenging is quantifying the
predictive uncertainty of GNNs, which is an important aspect of trustworthiness
in clinical settings. While Bayesian and ensemble methods can be used to
quantify uncertainty, they are computationally expensive. Additionally, the
disagreement metric used by ensemble methods to compute uncertainty cannot
capture the diversity of models in an ensemble network. In this paper, we
propose a novel method, based on knowledge distillation, to quantify GNNs'
uncertainty more efficiently and with higher precision. We apply
self-distillation, where the same network serves as both the teacher and
student models, thereby avoiding the need to train several networks
independently. To ensure the impact of self-distillation, we develop an
uncertainty metric that captures the diverse nature of the network by assigning
different weights to each GNN classifier. We experimentally evaluate the
precision, performance, and ability of our approach in distinguishing
out-of-distribution data on two graph datasets: MIMIC-IV and Enzymes. The
evaluation results demonstrate that the proposed method can effectively capture
the predictive uncertainty of the model while having performance similar to
that of the MC Dropout and ensemble methods. The code is publicly available at
https://github.com/tailabTMU/UQ_GNN.

</details>


### [234] [Universal pre-training by iterated random computation](https://arxiv.org/abs/2506.20057)
*Peter Bloem*

Main category: cs.LG

TL;DR: 本文探讨了如何利用随机生成的数据进行模型预训练，并提供了相关理论与实验支持，证实了该方法的有效性及其扩展性。


<details>
  <summary>Details</summary>
Motivation: 探索使用随机生成数据进行模型预训练的可行性及其对模型表现的影响，基于算法复杂性的理论及先前的研究发现提供指导。

Method: 理论上依托算法复杂性与Solomonoff归纳的相关研究，推导出了对随机数据预训练的支持；实验上利用合成数据与真实世界数据进行模型预训练，并验证零样本学习能力、模型扩展性以及微调优化效果。

Result: 实验证明合成数据预训练可以提升模型零样本学习的能力，该性能随着模型规模增大而改善；进一步在真实数据上验证预训练的有效性，并表明预训练后微调可实现更快收敛和更优泛化性。

Conclusion: 随机生成数据用于模型预训练是一种有效方法，为模型在见到数据前提升了基础能力，并通过微调进一步增强模型性能，具有广泛的潜力。

Abstract: We investigate the use of randomly generated data for the sake of
pre-training a model. We justify this approach theoretically from the
perspective of algorithmic complexity, building on recent research that shows
that sequence models can be trained to approximate Solomonoff induction. We
derive similar, but complementary theoretical results. We show empirically that
synthetically generated data can be used to pre-train a model before the data
is seen. We replicate earlier results that models trained this way show
zero-shot in-context learning across a variety of datasets, and that this
performance improves with scale. We extend earlier results to real-world data,
and show that finetuning a model after pre-training offers faster convergence
and better generalization.

</details>


### [235] [Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models](https://arxiv.org/abs/2506.20061)
*Zhicheng Zhang,Ziyan Wang,Yali Du,Fei Fang*

Main category: cs.LG

TL;DR: 提出了一种使用大型语言模型(LLM)进行指令重标的方法，以提高强化学习中指令跟随策略的效率。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的指令跟随受制于人类标注的繁琐性及稀疏奖励，亟需自动化的高效解决方案。

Method: 利用LLM对失败的训练轨迹生成新的开放式指令，通过识别代理已完成的子任务自动重标数据，减少人类标注需求。

Result: 在Craftax环境中实验验证，显著提升了样本效率、指令覆盖率和策略性能，与当前最优基准相比有明显改进。

Conclusion: LLM指导的开放式指令重标是一种有效的方法，可显著提升指令跟随强化学习的能力并降低对人工标注的依赖。

Abstract: Developing effective instruction-following policies in reinforcement learning
remains challenging due to the reliance on extensive human-labeled instruction
datasets and the difficulty of learning from sparse rewards. In this paper, we
propose a novel approach that leverages the capabilities of large language
models (LLMs) to automatically generate open-ended instructions retrospectively
from previously collected agent trajectories. Our core idea is to employ LLMs
to relabel unsuccessful trajectories by identifying meaningful subtasks the
agent has implicitly accomplished, thereby enriching the agent's training data
and substantially alleviating reliance on human annotations. Through this
open-ended instruction relabeling, we efficiently learn a unified
instruction-following policy capable of handling diverse tasks within a single
policy. We empirically evaluate our proposed method in the challenging Craftax
environment, demonstrating clear improvements in sample efficiency, instruction
coverage, and overall policy performance compared to state-of-the-art
baselines. Our results highlight the effectiveness of utilizing LLM-guided
open-ended instruction relabeling to enhance instruction-following
reinforcement learning.

</details>


### [236] [Supervised Coupled Matrix-Tensor Factorization (SCMTF) for Computational Phenotyping of Patient Reported Outcomes in Ulcerative Colitis](https://arxiv.org/abs/2506.20065)
*Cristian Minoccheri,Sophia Tesic,Kayvan Najarian,Ryan Stidham*

Main category: cs.LG

TL;DR: 该论文提出了一种新方法，利用患者报告的结果（PROs）结合时间序列实验室和静态特征，预测溃疡性结肠炎（UC）患者的用药坚持性。


<details>
  <summary>Details</summary>
Motivation: 目前的机器学习方法通常忽略 PROs 数据，因为这些数据噪声大、主观、且稀疏，但它实际包含了有价值的信息，可用于更好地理解患者的疾病进展情况。

Method: 论文提出一种监督耦合矩阵-张量分解（SCMTF）方法，该方法整合了时间性的 PROs、实验室数据和静态特征，并基于深度学习框架进行实现，能够处理大量缺失数据。

Result: 提出的方法预测用药坚持性的效果优异，对测试集的 AUC 分别为 0.853 (8个月) 和 0.803 (20个月)。此外，该方法还能生成可解释的表型，包括静态和时间特征及其模式。

Conclusion: 论文验证了基于低秩矩阵和张量分解的表型识别法在 UC 和高缺失性 PROs 数据上的成功应用，并表明 PROs 数据包含许多可用于预测用药坚持性的相关信息。

Abstract: Phenotyping is the process of distinguishing groups of patients to identify
different types of disease progression. A recent trend employs low-rank matrix
and tensor factorization methods for their capability of dealing with
multi-modal, heterogeneous, and missing data. Symptom quantification is crucial
for understanding patient experiences in inflammatory bowel disease, especially
in conditions such as ulcerative colitis (UC). However, patient-reported
symptoms are typically noisy, subjective, and significantly more sparse than
other data types. For this reason, they are usually not included in phenotyping
and other machine learning methods. This paper explores the application of
computational phenotyping to leverage Patient-Reported Outcomes (PROs) using a
novel supervised coupled matrix-tensor factorization (SCMTF) method, which
integrates temporal PROs and temporal labs with static features to predict
medication persistence in ulcerative colitis. This is the first tensor-based
method that is both supervised and coupled, it is the first application to the
UC domain, and the first application to PROs. We use a deep learning framework
that makes the model flexible and easy to train. The proposed method allows us
to handle the large amount of missing data in the PROs. The best model predicts
changes in medication 8 and 20 months in the future with AUCs of 0.853 and
0.803 on the test set respectively. We derive interpretable phenotypes
consisting of static features and temporal features (including their temporal
patterns). We show that low-rank matrix and tensor based phenotyping can be
successfully applied to the UC domain and to highly missing PRO data. We
identify phenotypes useful to predict medication persistence - these phenotypes
include several symptom variables, showing that PROs contain relevant
infromation that is usually discarded.

</details>


### [237] [A Survey of Predictive Maintenance Methods: An Analysis of Prognostics via Classification and Regression](https://arxiv.org/abs/2506.20090)
*Ainaz Jamshidi,Dongchan Kim,Muhammad Arif*

Main category: cs.LG

TL;DR: 本文深入比较了基于回归和分类的方法在预测性维护中的应用，分析了其优缺点以及未来展望。


<details>
  <summary>Details</summary>
Motivation: 旨在通过比较回归和分类方法在预测性维护中的表现，解决当前关于其优劣缺乏专门研究的问题。

Method: 通过对近年文献进行综述，分析回归方法（如RUL预测）和分类方法（如故障概率预测）的应用，并研究其挑战和趋势。

Result: 发现回归和分类方法在预测性维护中具有不同的优劣，同时强调数据不平衡、高维特征空间等问题以及混合方法的潜力。

Conclusion: 研究提供了对不同PdM方法的优势和妥协的深入认知，并建议从实践角度开展进一步研究以推动该领域的发展。

Abstract: Predictive maintenance (PdM) has become a crucial element of modern
industrial practice. PdM plays a significant role in operational dependability
and cost management by decreasing unforeseen downtime and optimizing asset life
cycle management. Machine learning and deep learning have enabled more precise
forecasts of equipment failure and remaining useful life (RUL). Although many
studies have been conducted on PdM, there has not yet been a standalone
comparative study between regression- and classification-based approaches. In
this review, we look across a range of PdM methodologies, while focusing more
strongly on the comparative use of classification and regression methods in
prognostics. While regression-based methods typically provide estimates of RUL,
classification-based methods present a forecast of the probability of failure
across defined time intervals. Through a comprehensive analysis of recent
literature, we highlight key advancements, challenges-such as data imbalance
and high-dimensional feature spaces-and emerging trends, including hybrid
approaches and AI-enabled prognostic systems. This review aims to provide
researchers and practitioners with an awareness of the strengths and
compromises of various PdM methods and to help identify future research and
build more robust, directed adaptive maintenance systems. Future work may
include a systematic review of practical aspects such as public datasets,
benchmarking platforms, and open-source tools to support the advancement of PdM
research.

</details>


### [238] [MEL: Multi-level Ensemble Learning for Resource-Constrained Environments](https://arxiv.org/abs/2506.20094)
*Krishna Praneet Gudipaty,Walid A. Hanafy,Kaan Ozkara,Qianlin Liang,Jesse Milzman,Prashant Shenoy,Suhas Diggavi*

Main category: cs.LG

TL;DR: 本文提出了多层次集成学习(MEL)框架，用于提高边缘推理服务在资源受限和易失败环境中的鲁棒性，同时兼顾性能和灵活性。


<details>
  <summary>Details</summary>
Motivation: 边缘环境的低功耗和资源限制特性，以及容易发生故障的特点，对传统容错方法提出了挑战。本文旨在解决边缘推理中的延迟和准确性权衡问题。

Method: 通过将问题建模为多目标优化问题，采用损失函数鼓励模型的多样性，以实现单个模型在多种故障情况下的独立运行和协作表现，同时确保高准确率。

Result: MEL框架在视觉、语言和音频数据集上的实验证明，其性能与原始架构相当，同时在边缘平台上实现容错和灵活部署。在故障情况下，大小为原模型40%的集成模型仍可保持95.6%的准确率。

Conclusion: MEL方法能够在资源受限和易失败的边缘环境中提供一种高效且灵活的推理容错框架，为下一代边缘推理服务提供了重要支持。

Abstract: AI inference at the edge is becoming increasingly common for low-latency
services. However, edge environments are power- and resource-constrained, and
susceptible to failures. Conventional failure resilience approaches, such as
cloud failover or compressed backups, often compromise latency or accuracy,
limiting their effectiveness for critical edge inference services. In this
paper, we propose Multi-Level Ensemble Learning (MEL), a new framework for
resilient edge inference that simultaneously trains multiple lightweight backup
models capable of operating collaboratively, refining each other when multiple
servers are available, and independently under failures while maintaining good
accuracy. Specifically, we formulate our approach as a multi-objective
optimization problem with a loss formulation that inherently encourages
diversity among individual models to promote mutually refining representations,
while ensuring each model maintains good standalone performance. Empirical
evaluations across vision, language, and audio datasets show that MEL provides
performance comparable to original architectures while also providing fault
tolerance and deployment flexibility across edge platforms. Our results show
that our ensemble model, sized at 40\% of the original model, achieves similar
performance, while preserving 95.6\% of ensemble accuracy in the case of
failures when trained using MEL.

</details>


### [239] [High-Resolution Live Fuel Moisture Content (LFMC) Maps for Wildfire Risk from Multimodal Earth Observation Data](https://arxiv.org/abs/2506.20132)
*Patrick Alan Johnson,Gabriel Tseng,Yawen Zhang,Heather Heward,Virginia Sjahli,Favyen Bastani,Joseph Redmon,Patrick Beukema*

Main category: cs.LG

TL;DR: 本研究利用预训练的高度多模态地球观测模型生成大范围、空间完整的实时燃料含水量(LFMC)地图，用于提高野火风险评估和响应能力。


<details>
  <summary>Details</summary>
Motivation: 地面测量LFMC时间成本和金钱成本高，数据稀疏且更新频率低，亟需利用AI和卫星数据改进野火的实时监测与评估。

Method: 采用预训练的多模态地球观测模型，生成大范围且空间完整的LFMC地图，并开发自动化流程，实现快速生成这些地图。

Result: 与通过随机初始化的模型相比，降低了20的均方根误差（RMSE）,证明了改进的预测性能，同时展示了在埃顿和帕利赛兹两个近期受灾地区的有效性。

Conclusion: 该方法显著提升了LFMC计算的精确性和生成速度，为野火研究与应对提供了有力支持。

Abstract: Wildfires are increasing in intensity and severity at an alarming rate.
Recent advances in AI and publicly available satellite data enable monitoring
critical wildfire risk factors globally, at high resolution and low latency.
Live Fuel Moisture Content (LFMC) is a critical wildfire risk factor and is
valuable for both wildfire research and operational response. However,
ground-based LFMC samples are both labor intensive and costly to acquire,
resulting in sparse and infrequent updates. In this work, we explore the use of
a pretrained, highly-multimodal earth-observation model for generating
large-scale spatially complete (wall-to-wall) LFMC maps. Our approach achieves
significant improvements over previous methods using randomly initialized
models (20 reduction in RMSE). We provide an automated pipeline that enables
rapid generation of these LFMC maps across the United States, and demonstrate
its effectiveness in two regions recently impacted by wildfire (Eaton and
Palisades).

</details>


### [240] [Causal discovery in deterministic discrete LTI-DAE systems](https://arxiv.org/abs/2506.20169)
*Bala Rajesh Konkathi,Arun K. Tangirala*

Main category: cs.LG

TL;DR: 本文提出了一种称为变量划分（PoV）的方法，用于在线性时不变的微分代数系统中进行因果发现，并展示了其在多种案例研究中的有效性。


<details>
  <summary>Details</summary>
Motivation: 在数据驱动的因果网络重构中，识别纯原因或驱动变量至关重要。现有方法（2022年Kathari和Tangirala提出）对仅包含动态关系的系统有效，但无法处理具有代数关系或反馈控制的系统。

Method: 提出了变量划分（PoV）方法，结合动态迭代主成分分析（DIPCA）来确定代数和动态关系的数目以及约束矩阵，并通过计算矩阵条件数划分子集。

Result: PoV方法能够扩展应用范围，不仅适用于纯动态系统，还能处理线性时不变的微分代数系统，并识别因果驱动变量的最小子集。

Conclusion: PoV方法改进了现有因果发现方法的能力，增强了其应用范围和准确性，为因果网络构建提供了更强大的工具。

Abstract: Discovering pure causes or driver variables in deterministic LTI systems is
of vital importance in the data-driven reconstruction of causal networks. A
recent work by Kathari and Tangirala, proposed in 2022, formulated the causal
discovery method as a constraint identification problem. The constraints are
identified using a dynamic iterative PCA (DIPCA)-based approach for dynamical
systems corrupted with Gaussian measurement errors. The DIPCA-based method
works efficiently for dynamical systems devoid of any algebraic relations.
However, several dynamical systems operate under feedback control and/or are
coupled with conservation laws, leading to differential-algebraic (DAE) or
mixed causal systems. In this work, a method, namely the partition of variables
(PoV), for causal discovery in LTI-DAE systems is proposed. This method is
superior to the method that was presented by Kathari and Tangirala (2022), as
PoV also works for pure dynamical systems, which are devoid of algebraic
equations. The proposed method identifies the causal drivers up to a minimal
subset. PoV deploys DIPCA to first determine the number of algebraic relations
($n_a$), the number of dynamical relations ($n_d$) and the constraint matrix.
Subsequently, the subsets are identified through an admissible partitioning of
the constraint matrix by finding the condition number of it. Case studies are
presented to demonstrate the effectiveness of the proposed method.

</details>


### [241] [Causal Operator Discovery in Partial Differential Equations via Counterfactual Physics-Informed Neural Networks](https://arxiv.org/abs/2506.20181)
*Ronald Katende*

Main category: cs.LG

TL;DR: 提出了一种使用物理信息神经网络和反事实扰动的框架，用于从偏微分方程中发现因果结构。


<details>
  <summary>Details</summary>
Motivation: 解决传统残差最小化或稀疏回归方法的局限性，提供一种量化算子级必要性的工具，用以更准确地发现偏微分方程中的因果机制。

Method: 通过功能性干预以及引入因果敏感性指数和结构偏差指标，评估候选微分算子的影响，在理论上证明了在受限等距或互相关条件下的因果算子支持的精确恢复能力。

Result: 在包括气候动力学、肿瘤扩散和海洋流动等多个真实和合成数据集上验证了该框架，在噪声、冗余和数据稀缺的情况下，该方法仍能以较高的结构保真度恢复主导算子，优于标准PINNs和DeepONets。

Conclusion: 该研究将因果PDE发现定位为一个基于结构因果模型和变分残差分析的可解读推断任务，为领域提供了一个理论与实践兼备的新视角。

Abstract: We develop a principled framework for discovering causal structure in partial
differential equations (PDEs) using physics-informed neural networks and
counterfactual perturbations. Unlike classical residual minimization or sparse
regression methods, our approach quantifies operator-level necessity through
functional interventions on the governing dynamics. We introduce causal
sensitivity indices and structural deviation metrics to assess the influence of
candidate differential operators within neural surrogates. Theoretically, we
prove exact recovery of the causal operator support under restricted isometry
or mutual coherence conditions, with residual bounds guaranteeing
identifiability. Empirically, we validate the framework on both synthetic and
real-world datasets across climate dynamics, tumor diffusion, and ocean flows.
Our method consistently recovers governing operators even under noise,
redundancy, and data scarcity, outperforming standard PINNs and DeepONets in
structural fidelity. This work positions causal PDE discovery as a tractable
and interpretable inference task grounded in structural causal models and
variational residual analysis.

</details>


### [242] [DuoGPT: Training-free Dual Sparsity through Activation-aware Pruning in LLMs](https://arxiv.org/abs/2506.20194)
*Ruokai Yin,Yuhang Li,Donghyun Lee,Priyadarshini Panda*

Main category: cs.LG

TL;DR: DuoGPT结合非结构化权重剪枝和激活稀疏，优化大语言模型（LLMs）的运行效率，同时保持精度。


<details>
  <summary>Details</summary>
Motivation: LLMs性能强大但内存和计算成本高，现有剪枝方法未充分利用运行时的激活稀疏性。

Method: 提出DuoGPT框架，通过非结构化权重剪枝结合激活稀疏，扩展OBC框架进行激活感知校准，并引入密集模型的输出残差作为校正项。同时优化GPU执行以支持大规模参数。

Result: DuoGPT在LLaMA-2和LLaMA-3上的表现优于最新的结构化剪枝方法，在相同速度加速下精度提高最多9.17%。

Conclusion: DuoGPT通过动态分布式稀疏优化LLMs效率，在保持精度的同时显著降低成本。

Abstract: Large language models (LLMs) deliver strong performance but are difficult to
deploy due to high memory and compute costs. While pruning reduces these
demands, most methods ignore activation sparsity observed at runtime. We
reinterpret activation sparsity as dynamic structured weight sparsity and
propose DuoGPT, a unified framework that constructs dual-sparse (spMspV)
workloads by combining unstructured weight pruning with activation sparsity. To
preserve accuracy, we extend the Optimal Brain Compression (OBC) framework with
activation-aware calibration and introduce output residuals from the dense
model as correction terms. We further optimize the solution for efficient GPU
execution, enabling scalability to billion-parameter LLMs. Evaluations on
LLaMA-2 and LLaMA-3 show that DuoGPT outperforms state-of-the-art structured
pruning methods by up to 9.17% accuracy at an iso-speedup of 1.39$\times$
compared to the baseline dense model.

</details>


### [243] [Zero-Shot Attribution for Large Language Models: A Distribution Testing Approach](https://arxiv.org/abs/2506.20197)
*Clément L. Canonne,Yash Pote,Uddalok Sarkar*

Main category: cs.LG

TL;DR: 該研究提出了一種名為Anubis的工具，用於通過零樣本方法測試語言模型生成的代碼是否屬於特定模型，並在相關基準測試中表現優異。


<details>
  <summary>Details</summary>
Motivation: 隨著越來越多的代碼來自大型語言模型，研究者希望解決如何判定某段代碼是否由特定模型生成的問題。

Method: 提出使用統計假設檢驗方法並引入Anubis工具，通過分布測試手段來完成代碼歸因，結合LLM樣本和概率密度估計進行分析。

Result: 實驗顯示Anubis在識別DeepSeek-Coder、CodeGemma、Stable-Code等模型生成的代碼時，AUROC分數達到0.9以上，且僅需使用約2000個樣本。

Conclusion: Anubis工具有效解決了語言模型生成代碼的歸因問題，展示了高效性和準確性，為未來相關研究提供了可能的方向。

Abstract: A growing fraction of all code is sampled from Large Language Models (LLMs).
We investigate the problem of attributing code generated by language models
using hypothesis testing to leverage established techniques and guarantees.
Given a set of samples $S$ and a suspect model $\mathcal{L}^*$, our goal is to
assess the likelihood of $S$ originating from $\mathcal{L}^*$. Due to the curse
of dimensionality, this is intractable when only samples from the LLM are
given: to circumvent this, we use both samples and density estimates from the
LLM, a form of access commonly available.
  We introduce $\mathsf{Anubis}$, a zero-shot attribution tool that frames
attribution as a distribution testing problem. Our experiments on a benchmark
of code samples show that $\mathsf{Anubis}$ achieves high AUROC scores (
$\ge0.9$) when distinguishing between LLMs like DeepSeek-Coder, CodeGemma, and
Stable-Code using only $\approx 2000$ samples.

</details>


### [244] [Affective Priming Score: A Data-Driven Method to Detect Priming in Sequential Datasets](https://arxiv.org/abs/2506.20204)
*Eduardo Gutierrez Maestro,Hadi Banaee,Amy Loutfi*

Main category: cs.LG

TL;DR: 提出了一种名为情感启动评分（APS）的数据驱动方法，用于检测受启动效应影响的数据点，从而显著减少模型误分类率。


<details>
  <summary>Details</summary>
Motivation: 解决情感计算中由启动效应引起的歧义问题，特别是在生理信号中的影响。

Method: 通过情感启动评分（APS）方法给每个数据点分配评分，量化其受启动效应的影响程度，并验证其在SEED和SEED-VII数据集上的有效性对比结果。

Result: 使用去除启动效应的序列训练后，模型的误分类率显著降低，证明了方法的有效性与数据集改进价值。

Conclusion: 通过识别和减轻数据层面的启动效应，提升了模型鲁棒性，并为情感计算数据集的设计与收集提供了新见解。

Abstract: Affective priming exemplifies the challenge of ambiguity in affective
computing. While the community has largely addressed this issue from a
label-based perspective, identifying data points in the sequence affected by
the priming effect, the impact of priming on data itself, particularly in
physiological signals, remains underexplored. Data affected by priming can lead
to misclassifications when used in learning models. This study proposes the
Affective Priming Score (APS), a data-driven method to detect data points
influenced by the priming effect. The APS assigns a score to each data point,
quantifying the extent to which it is affected by priming. To validate this
method, we apply it to the SEED and SEED-VII datasets, which contain sufficient
transitions between emotional events to exhibit priming effects. We train
models with the same configuration using both the original data and
priming-free sequences. The misclassification rate is significantly reduced
when using priming-free sequences compared to the original data. This work
contributes to the broader challenge of ambiguity by identifying and mitigating
priming effects at the data level, enhancing model robustness, and offering
valuable insights for the design and collection of affective computing
datasets.

</details>


### [245] [Directed Link Prediction using GNN with Local and Global Feature Fusion](https://arxiv.org/abs/2506.20235)
*Yuyang Zhang,Xu Shen,Yu Xie,Ka-Chun Wong,Weidun Xie,Chengbin Peng*

Main category: cs.LG

TL;DR: 本研究提出了一种新的图神经网络框架，通过融合特征嵌入和社区信息提升有向图的链路预测性能，同时设计了一种将输入图转化为有向线图的方法，实验结果表明在大多数情况下优于现有技术水平。


<details>
  <summary>Details</summary>
Motivation: 在有向图的链接预测中，以往的方法主要通过对比学习分析节点相似性，结合图卷积聚合邻域信息，但在利用社区信息和提升预测性能上尚存改进空间。

Method: 提出了融合特征嵌入与社区信息的GNN框架，并将输入图转化为有向线图，以便节点在图卷积过程中聚合更多信息。

Result: 在使用30%、40%、50%和60%的链接作为训练数据时，实验表明该方法在大多数基准数据集上的表现优于现有最先进水平。

Conclusion: 通过理论证明与实验验证，融合特征嵌入和社区信息的混合特征能有效提升有向图链路预测性能，转化为有向线图的方法辅助增强了图卷积的信息聚合效果。

Abstract: Link prediction is a classical problem in graph analysis with many practical
applications. For directed graphs, recently developed deep learning approaches
typically analyze node similarities through contrastive learning and aggregate
neighborhood information through graph convolutions. In this work, we propose a
novel graph neural network (GNN) framework to fuse feature embedding with
community information. We theoretically demonstrate that such hybrid features
can improve the performance of directed link prediction. To utilize such
features efficiently, we also propose an approach to transform input graphs
into directed line graphs so that nodes in the transformed graph can aggregate
more information during graph convolutions. Experiments on benchmark datasets
show that our approach outperforms the state-of-the-art in most cases when 30%,
40%, 50%, and 60% of the connected links are used as training data,
respectively.

</details>


### [246] [FedBKD: Distilled Federated Learning to Embrace Gerneralization and Personalization on Non-IID Data](https://arxiv.org/abs/2506.20245)
*Yushan Zhao,Jinyuan He,Donglai Chen,Weijie Luo,Chong Xie,Ri Zhang,Yonghong Chen,Yan Xu*

Main category: cs.LG

TL;DR: 这篇论文提出FedBKD框架，通过生成对抗网络（GAN）和双向知识蒸馏，解决联邦学习中非IID数据下的全局与局部模型性能兼顾问题。实验结果显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法难以同时实现全局模型的泛化性和本地模型的高性能，且利用公共数据集解决非IID问题可能引发数据泄漏风险。

Method: 设计联邦双向知识蒸馏（FedBKD）框架，使用生成对抗网络训练生成合成数据，并利用这些数据在全局和本地模型间进行知识双向蒸馏，提升双方性能。

Result: 提出的方法在不同的非IID设置下，通过4个基准测试取得了最先进的性能表现。

Conclusion: FedBKD框架在实现全局与本地模型性能兼顾的同时，消除了引入公共数据集带来的数据隐私泄漏风险，展现出广阔的应用潜力。

Abstract: Federated learning (FL) is a decentralized collaborative machine learning
(ML) technique. It provides a solution to the issues of isolated data islands
and data privacy leakage in industrial ML practices. One major challenge in FL
is handling the non-identical and independent distributed (non-IID) data.
Current solutions either focus on constructing an all-powerful global model, or
customizing personalized local models. Few of them can provide both a
well-generalized global model and well-performed local models at the same time.
Additionally, many FL solutions to the non-IID problem are benefited from
introducing public datasets. However, this will also increase the risk of data
leakage. To tackle the problems, we propose a novel data-free distillation
framework, Federated Bidirectional Knowledge Distillation (FedBKD).
Specifically, we train Generative Adversarial Networks (GAN) for synthetic
data. During the GAN training, local models serve as discriminators and their
parameters are frozen. The synthetic data is then used for bidirectional
distillation between global and local models to achieve knowledge interactions
so that performances for both sides are improved. We conduct extensive
experiments on 4 benchmarks under different non-IID settings. The results show
that FedBKD achieves SOTA performances in every case.

</details>


### [247] [Q-resafe: Assessing Safety Risks and Quantization-aware Safety Patching for Quantized Large Language Models](https://arxiv.org/abs/2506.20251)
*Kejia Chen,Jiawen Zhang,Jiacong Hu,Yu Wang,Jian Lou,Zunlei Feng,Mingli Song*

Main category: cs.LG

TL;DR: 该研究关注量化大语言模型(LLMs)的安全性问题，提出了一种量化感知的安全修复框架Q-resafe以应对安全性下降。


<details>
  <summary>Details</summary>
Motivation: 量化方法虽然提升了资源受限环境中的模型部署能力，但可能损害LLMs的安全性能，亟需系统性评估与解决方案。

Method: 作者进行了跨主流量化技术和多样校准数据集的安全评估，并引入了量化感知安全修复框架Q-resafe，以高效恢复量化模型的安全性能。

Result: 实验结果表明，Q-resafe在各类具挑战性的评估场景中成功将量化后模型的安全性能恢复至量化前水平，同时最小化对效用的负面影响。

Conclusion: Q-resafe为提升量化LLMs安全性提供了一种有效方法，显著降低量化带来的安全风险。

Abstract: Quantized large language models (LLMs) have gained increasing attention and
significance for enabling deployment in resource-constrained environments.
However, emerging studies on a few calibration dataset-free quantization
methods suggest that quantization may compromise the safety capabilities of
LLMs, underscoring the urgent need for systematic safety evaluations and
effective mitigation strategies. In this paper, we present comprehensive safety
evaluations across various mainstream quantization techniques and diverse
calibration datasets, utilizing widely accepted safety benchmarks. To address
the identified safety vulnerabilities, we propose a quantization-aware safety
patching framework, Q-resafe, to efficiently restore the safety capabilities of
quantized LLMs while minimizing any adverse impact on utility. Extensive
experimental results demonstrate that Q-resafe successfully re-aligns the
safety of quantized LLMs with their pre-quantization counterparts, even under
challenging evaluation scenarios. Project page is available at:
https://github.com/Thecommonirin/Qresafe.

</details>


### [248] [Time-series surrogates from energy consumers generated by machine learning approaches for long-term forecasting scenarios](https://arxiv.org/abs/2506.20253)
*Ben Gerhards,Nikita Popkov,Annekatrin König,Marcel Arpogaus,Bastian Schäfermeier,Leonie Riedl,Stephan Vogt,Philip Hehlert*

Main category: cs.LG

TL;DR: 本文探讨了用于生成电力消耗长时间序列数据的多种数据驱动方法，通过对比评估WGAN、DDPM、HMM和MABF方法，分析其在长时间依赖性等方面的表现，为状态估计等任务提供更适用的工具。


<details>
  <summary>Details</summary>
Motivation: 现有电力领域的研究较多集中在系统的短期预测上，而对个体用电的长期预测则研究不足。此外，生成高保真合成数据是能源系统状态估计等应用的重要前提。

Method: 本文采用了WGAN、DDPM、HMM及MABF四种方法，对不同算法生成具时间动态特性的高保真电力消耗数据的性能进行了深入比较及评估，并利用德国家庭15分钟时间间隔数据进行验证。

Result: 四种方法在合成数据生成中表现各具特点，为不同应用提供了选择，并成功满足匿名化等隐私保护的要求。

Conclusion: 通过比较这些尖端方法，研究为生成高保真、匿名化的合成电力消耗数据提出了解决方案，为电力系统中的一些关键应用提供支撑。

Abstract: Forecasting attracts a lot of research attention in the electricity value
chain. However, most studies concentrate on short-term forecasting of
generation or consumption with a focus on systems and less on individual
consumers. Even more neglected is the topic of long-term forecasting of
individual power consumption.
  Here, we provide an in-depth comparative evaluation of data-driven methods
for generating synthetic time series data tailored to energy consumption
long-term forecasting. High-fidelity synthetic data is crucial for a wide range
of applications, including state estimations in energy systems or power grid
planning. In this study, we assess and compare the performance of multiple
state-of-the-art but less common techniques: a hybrid Wasserstein Generative
Adversarial Network (WGAN), Denoising Diffusion Probabilistic Model (DDPM),
Hidden Markov Model (HMM), and Masked Autoregressive Bernstein polynomial
normalizing Flows (MABF). We analyze the ability of each method to replicate
the temporal dynamics, long-range dependencies, and probabilistic transitions
characteristic of individual energy consumption profiles. Our comparative
evaluation highlights the strengths and limitations of: WGAN, DDPM, HMM and
MABF aiding in selecting the most suitable approach for state estimations and
other energy-related tasks. Our generation and analysis framework aims to
enhance the accuracy and reliability of synthetic power consumption data while
generating data that fulfills criteria like anonymisation - preserving privacy
concerns mitigating risks of specific profiling of single customers. This study
utilizes an open-source dataset from households in Germany with 15min time
resolution. The generated synthetic power profiles can readily be used in
applications like state estimations or consumption forecasting.

</details>


### [249] [Argumentative Ensembling for Robust Recourse under Model Multiplicity](https://arxiv.org/abs/2506.20260)
*Junqi Jiang,Antonio Rago,Francesco Leofante,Francesca Toni*

Main category: cs.LG

TL;DR: 提出了一种名为RAE（recourse-aware ensembling）的新方法，解决机器学习中的模型多样性问题，通过争议论证技术保障对抗解释的稳健性。


<details>
  <summary>Details</summary>
Motivation: 针对机器学习中因多模型预测差异导致对抗性解释(Counterfactual Explanations, CE)无法统一适用的问题，提出了减少模型分歧以提升解释稳健性的需求。

Method: 引入一种基于争议论证的集成方法，显式表示模型和对抗性解释间的冲突，利用论证语义解决冲突，并允许对模型偏好进行定制化处理。

Result: 通过理论分析验证了争议论证方法在四种论证语义下的行为特性，并通过八种方法实现的实验验证了其在保障RAE所需性质方面的有效性。

Conclusion: 该方法能够在模型多样性下提供稳健且可定制化的对抗性解释，为解决该领域问题提供了新视角和工具。

Abstract: In machine learning, it is common to obtain multiple equally performing
models for the same prediction task, e.g., when training neural networks with
different random seeds. Model multiplicity (MM) is the situation which arises
when these competing models differ in their predictions for the same input, for
which ensembling is often employed to determine an aggregation of the outputs.
Providing recourse recommendations via counterfactual explanations (CEs) under
MM thus becomes complex, since the CE may not be valid across all models, i.e.,
the CEs are not robust under MM. In this work, we formalise the problem of
providing recourse under MM, which we name recourse-aware ensembling (RAE). We
propose the idea that under MM, CEs for each individual model should be
considered alongside their predictions so that the aggregated prediction and
recourse are decided in tandem. Centred around this intuition, we introduce six
desirable properties for solutions to this problem. For solving RAE, we propose
a novel argumentative ensembling method which guarantees the robustness of CEs
under MM. Specifically, our method leverages computational argumentation to
explicitly represent the conflicts between models and counterfactuals regarding
prediction results and CE validity. It then uses argumentation semantics to
resolve the conflicts and obtain the final solution, in a manner which is
parametric to the chosen semantics. Our method also allows for the
specification of preferences over the models under MM, allowing further
customisation of the ensemble. In a comprehensive theoretical analysis, we
characterise the behaviour of argumentative ensembling with four different
argumentation semantics. We then empirically demonstrate the effectiveness of
our approach in satisfying desirable properties with eight instantiations of
our method. (Abstract is shortened for arXiv.)

</details>


### [250] [Distilling A Universal Expert from Clustered Federated Learning](https://arxiv.org/abs/2506.20285)
*Zeqi Leng,Chunxu Zhang,Guodong Long,Riting Xia,Bo Yang*

Main category: cs.LG

TL;DR: 提出了一种新型联邦学习框架，通过从多个集群的知识中蒸馏出一个通用模型，有效平衡个性化与共享知识。


<details>
  <summary>Details</summary>
Motivation: 现有的群组联邦学习方法未充分利用集群间的共享信息，无法很好地挖掘具有全局通用性的知识。

Method: 框架分为三步：本地模型训练、集群特定模型聚合以及通用专家模型蒸馏，通过蒸馏机制有效结合共享与个性化权重。

Result: 实验结果表明，该方法在多个场景下性能优越，有效减少集群间专家模型的冲突。

Conclusion: 该方法成功平衡了个性化和共享知识的发展，推动了群组联邦学习领域的进步。

Abstract: Clustered Federated Learning (CFL) addresses the challenges posed by non-IID
data by training multiple group- or cluster-specific expert models. However,
existing methods often overlook the shared information across clusters, which
represents the generalizable knowledge valuable to all participants in the
Federated Learning (FL) system. To overcome this limitation, this paper
introduces a novel FL framework that distills a universal expert model from the
knowledge of multiple clusters. This universal expert captures globally shared
information across all clients and is subsequently distributed to each client
as the initialization for the next round of model training. The proposed FL
framework operates in three iterative steps: (1) local model training at each
client, (2) cluster-specific model aggregation, and (3) universal expert
distillation. This three-step learning paradigm ensures the preservation of
fine-grained non-IID characteristics while effectively incorporating shared
knowledge across clusters. Compared to traditional gradient-based aggregation
methods, the distillation-based model aggregation introduces greater
flexibility in handling model heterogeneity and reduces conflicts among
cluster-specific experts. Extensive experimental results demonstrate the
superior performance of the proposed method across various scenarios,
highlighting its potential to advance the state of CFL by balancing
personalized and shared knowledge more effectively.

</details>


### [251] [Counterfactual Influence as a Distributional Quantity](https://arxiv.org/abs/2506.20481)
*Matthieu Meeus,Igor Shilov,Georgios Kaissis,Yves-Alexandre de Montjoye*

Main category: cs.LG

TL;DR: 研究发现，训练数据中复杂的交互作用是模型记忆的主要来源，而非单独的自我影响。


<details>
  <summary>Details</summary>
Motivation: 探究训练数据中的记忆化问题，特别是模型如何受到训练数据中样本相互影响的机制影响。

Method: 将计量记忆化的反事实自我影响扩展为一个分布量，分析训练样本之间的完全影响分布及其性质。

Result: 发现(几乎)重复样本会显著影响模型记忆，同时降低自我影响值，这种情况下记忆样本仍然容易被提取。此外，通过影响分布还发现CIFAR-10中存在(几乎)重复样本。

Conclusion: 单看自我影响可能低估记忆化风险，记忆化更多是训练数据复杂交互的结果，应通过完全影响分布来更好地理解。

Abstract: Machine learning models are known to memorize samples from their training
data, raising concerns around privacy and generalization. Counterfactual
self-influence is a popular metric to study memorization, quantifying how the
model's prediction for a sample changes depending on the sample's inclusion in
the training dataset. However, recent work has shown memorization to be
affected by factors beyond self-influence, with other training samples, in
particular (near-)duplicates, having a large impact. We here study memorization
treating counterfactual influence as a distributional quantity, taking into
account how all training samples influence how a sample is memorized. For a
small language model, we compute the full influence distribution of training
samples on each other and analyze its properties. We find that solely looking
at self-influence can severely underestimate tangible risks associated with
memorization: the presence of (near-)duplicates seriously reduces
self-influence, while we find these samples to be (near-)extractable. We
observe similar patterns for image classification, where simply looking at the
influence distributions reveals the presence of near-duplicates in CIFAR-10.
Our findings highlight that memorization stems from complex interactions across
training data and is better captured by the full influence distribution than by
self-influence alone.

</details>


### [252] [Learning Moderately Input-Sensitive Functions: A Case Study in QR Code Decoding](https://arxiv.org/abs/2506.20305)
*Kazuki Yoda,Kazuhiko Kawamoto,Hiroshi Kera*

Main category: cs.LG

TL;DR: 本文探讨了学习函数的输入敏感性，并提出了基于Transformer的QR码解码方法，超越了理论纠错极限，并能跨语言和随机字符串进行推广。


<details>
  <summary>Details</summary>
Motivation: 研究输入敏感性中等的学习函数，并探索学习模型在QR码解码中的有效性。

Method: 采用基于Transformer的模型，通过学习嵌入文本结构进行QR码解码。

Result: Transformer模型成功解码QR码，超越理论纠错极限，并在多种语言间以及随机字符串中广泛推广。

Conclusion: Transformer可以通过学习数据位的结构有效解码QR码，并表现出与传统解码方法不同的机制。

Abstract: The hardness of learning a function that attains a target task relates to its
input-sensitivity. For example, image classification tasks are
input-insensitive as minor corruptions should not affect the classification
results, whereas arithmetic and symbolic computation, which have been recently
attracting interest, are highly input-sensitive as each input variable connects
to the computation results. This study presents the first learning-based Quick
Response (QR) code decoding and investigates learning functions of medium
sensitivity. Our experiments reveal that Transformers can successfully decode
QR codes, even beyond the theoretical error-correction limit, by learning the
structure of embedded texts. They generalize from English-rich training data to
other languages and even random strings. Moreover, we observe that the
Transformer-based QR decoder focuses on data bits while ignoring
error-correction bits, suggesting a decoding mechanism distinct from standard
QR code readers.

</details>


### [253] [Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards](https://arxiv.org/abs/2506.20520)
*Charles Arnal,Gaëtan Narozniak,Vivien Cabannes,Yunhao Tang,Julia Kempe,Remi Munos*

Main category: cs.LG

TL;DR: 论文研究了介于off-policy强化学习和监督微调之间的算法范围，提出了一种基于简单的off-policy REINFORCE算法，并对算法提供了理论分析和实验验证。


<details>
  <summary>Details</summary>
Motivation: 当前的off-policy方法尽管实现简单且数据效率高，但在表现上往往次于on-policy技术；本文旨在研究具体的off-policy机制并优化其性能。

Method: 引入基于$r-V$的off-policy REINFORCE算法，其中$V$是可调的基线，通过调整$V$值控制对高奖励样本的强化和对低奖励样本的惩罚。

Result: 理论分析表明，当基线$V$低于期望奖励时，该算法具有策略改进的保证；实验验证显示在受控的随机老虎机环境和语言模型的微调任务中有良好表现。

Conclusion: 通过调整强化学习的基线值，可以改进off-policy算法的性能，同时验证了算法在理论与实际应用中的有效性。

Abstract: Reinforcement learning (RL) is increasingly used to align large language
models (LLMs). Off-policy methods offer greater implementation simplicity and
data efficiency than on-policy techniques, but often result in suboptimal
performance. In this work, we study the intermediate range of algorithms
between off-policy RL and supervised fine-tuning by analyzing a simple
off-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with
$r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$
emphasizes high-reward samples, while raising it penalizes low-reward ones more
heavily. We first provide a theoretical analysis of this off-policy REINFORCE
algorithm, showing that when the baseline $V$ lower-bounds the expected reward,
the algorithm enjoys a policy improvement guarantee. Our analysis reveals that
while on-policy updates can safely leverage both positive and negative signals,
off-policy updates benefit from focusing more on positive rewards than on
negative ones. We validate our findings experimentally in a controlled
stochastic bandit setting and through fine-tuning state-of-the-art LLMs on
reasoning tasks.

</details>


### [254] [Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration](https://arxiv.org/abs/2506.20307)
*Heyang Zhao,Xingrui Yu,David M. Bossens,Ivor W. Tsang,Quanquan Gu*

Main category: cs.LG

TL;DR: 提出了一种名为ILDE的模仿学习算法，在有限示例下实现高效学习，并超越专家性能。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，模仿学习面临状态空间复杂及示例有限的问题。如何解决数据不足以及如何实现超越专家性能是研究的动机。

Method: 提出了ILDE算法，通过（1）探索奖励优化专家策略（2）基于好奇的探索超越专家轨迹。

Result: 实验证明ILDE在数据效率上优于现有算法，并在Atari和MuJoCo任务中以更少示例实现超越专家性能，且从理论上证明了算法效果。

Conclusion: ILDE结合探索奖励与状态偏离探索，实现了更快的专家策略收敛，同时具备潜在超越专家性能的能力。

Abstract: Imitation learning is a central problem in reinforcement learning where the
goal is to learn a policy that mimics the expert's behavior. In practice, it is
often challenging to learn the expert policy from a limited number of
demonstrations accurately due to the complexity of the state space. Moreover,
it is essential to explore the environment and collect data to achieve
beyond-expert performance. To overcome these challenges, we propose a novel
imitation learning algorithm called Imitation Learning with Double Exploration
(ILDE), which implements exploration in two aspects: (1) optimistic policy
optimization via an exploration bonus that rewards state-action pairs with high
uncertainty to potentially improve the convergence to the expert policy, and
(2) curiosity-driven exploration of the states that deviate from the
demonstration trajectories to potentially yield beyond-expert performance.
Empirically, we demonstrate that ILDE outperforms the state-of-the-art
imitation learning algorithms in terms of sample efficiency and achieves
beyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations
than in previous work. We also provide a theoretical justification of ILDE as
an uncertainty-regularized policy optimization method with optimistic
exploration, leading to a regret growing sublinearly in the number of episodes.

</details>


### [255] [PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models](https://arxiv.org/abs/2506.20629)
*Soufiane Hayou,Nikhil Ghosh,Bin Yu*

Main category: cs.LG

TL;DR: 本文提出了一种名为PLoP（Precise LoRA Placement）的方法，旨在通过直观的理论分析自动识别LoRA适配器的最佳放置模块。这种方法适用于大模型的高效微调并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: LoRA尽管是流行的大模型微调方法，但其适配器放置策略研究有限。现有工作对放置在注意力模块或MLP模块的效果尚无定论，亟需更精确的方法优化适配器放置位置。

Method: 通过一种理论分析的方法，提出PLoP来自动识别LoRA适配器在预训练模型模块中的最佳放置位置，无需人工选择。

Result: 实验表明，PLoP在监督微调及强化学习任务中的表现优于现有的通常放置策略，即使在最差情况下也不逊色。

Conclusion: PLoP方法显著优化了适配器放置策略，为LoRA方法提供了更高效、更灵活的解决方案。

Abstract: Low-Rank Adaptation (LoRA) is a widely used finetuning method for large
models. Its small memory footprint allows practitioners to adapt large models
to specific tasks at a fraction of the cost of full finetuning. Different
modifications have been proposed to enhance its efficiency by, for example,
setting the learning rate, the rank, and the initialization. Another
improvement axis is adapter placement strategy: when using LoRA, practitioners
usually pick module types to adapt with LoRA, such as Query and Key modules.
Few works have studied the problem of adapter placement, with nonconclusive
results: original LoRA paper suggested placing adapters in attention modules,
while other works suggested placing them in the MLP modules. Through an
intuitive theoretical analysis, we introduce PLoP (Precise LoRA Placement), a
lightweight method that allows automatic identification of module types where
LoRA adapters should be placed, given a pretrained model and a finetuning task.
We demonstrate that PLoP consistently outperforms, and in the worst case
competes, with commonly used placement strategies through comprehensive
experiments on supervised finetuning and reinforcement learning for reasoning.

</details>


### [256] [Comparative Analysis of Deep Learning Models for Crop Disease Detection: A Transfer Learning Approach](https://arxiv.org/abs/2506.20323)
*Saundarya Subramaniam,Shalini Majumdar,Shantanu Nadar,Kaustubh Kulkarni*

Main category: cs.LG

TL;DR: 开发了一个基于AI的农作物疾病检测系统，通过对比不同深度学习模型实现了高效分类。


<details>
  <summary>Details</summary>
Motivation: 帮助资源有限的农民更高效管理农作物健康，推动可持续农业发展。

Method: 采用了EfficientNet、ResNet101、MobileNetV2以及自定义的CNN模型，集中研究并比较迁移学习的效率。

Result: 自定义CNN模型获得了95.76%的验证准确率，证明该方法有效分类植物疾病。

Conclusion: 迁移学习有潜力在农业领域革新生产方式，提升作物健康管理水平，特别是在农村环境中具有重要意义。

Abstract: This research presents the development of an Artificial Intelligence (AI) -
driven crop disease detection system designed to assist farmers in rural areas
with limited resources. We aim to compare different deep learning models for a
comparative analysis, focusing on their efficacy in transfer learning. By
leveraging deep learning models, including EfficientNet, ResNet101,
MobileNetV2, and our custom CNN, which achieved a validation accuracy of
95.76%, the system effectively classifies plant diseases. This research
demonstrates the potential of transfer learning in reshaping agricultural
practices, improving crop health management, and supporting sustainable farming
in rural environments.

</details>


### [257] [Permutation Equivariant Neural Controlled Differential Equations for Dynamic Graph Representation Learning](https://arxiv.org/abs/2506.20324)
*Torben Berndt,Benjamin Walker,Tiexin Qin,Jan Stühmer,Andrey Kormilitzin*

Main category: cs.LG

TL;DR: 提出了一种新的图神经网络方法，通过引入排列等变性，改进了动态图上微分方程的建模效果。


<details>
  <summary>Details</summary>
Motivation: 现有的方法（比如Graph Neural CDEs）在处理动态图的高效性和泛化性上存在一定的局限性。

Method: 通过将Graph Neural CDEs投影到排列等变的函数空间，减少模型参数而保留表达能力。

Result: 通过实验证明了方法在插值和外推任务中的性能优越性。

Conclusion: 提出的Permutation Equivariant Neural Graph CDEs提升了效率及泛化能力，是对动态图建模的重要改进。

Abstract: Dynamic graphs exhibit complex temporal dynamics due to the interplay between
evolving node features and changing network structures. Recently, Graph Neural
Controlled Differential Equations (Graph Neural CDEs) successfully adapted
Neural CDEs from paths on Euclidean domains to paths on graph domains. Building
on this foundation, we introduce Permutation Equivariant Neural Graph CDEs,
which project Graph Neural CDEs onto permutation equivariant function spaces.
This significantly reduces the model's parameter count without compromising
representational power, resulting in more efficient training and improved
generalisation. We empirically demonstrate the advantages of our approach
through experiments on simulated dynamical systems and real-world tasks,
showing improved performance in both interpolation and extrapolation scenarios.

</details>


### [258] [Producer-Fairness in Sequential Bundle Recommendation](https://arxiv.org/abs/2506.20329)
*Alexandre Rio,Marta Soare,Sihem Amer-Yahia*

Main category: cs.LG

TL;DR: 本论文研究了用于连续捆绑推荐中的公平性问题，提出了生产者公平的概念，为不同项目组的曝光提供平衡。


<details>
  <summary>Details</summary>
Motivation: 解决实际场景中推荐会话中的公平性问题，确保不同项目组的曝光在用户推荐中达到期望水平。

Method: 提出生产者公平的形式化定义，结合构建高质量推荐捆绑，同时研究了实时算法、精准解法，以及三种启发式算法，包括质量优先、公平优先和自适应变体。

Result: 实验基于三个真实数据集，展示了不同方法的优缺点及其在不牺牲质量的前提下实现公平推荐的有效性。

Conclusion: 提出的解决方案在实现公平与质量之间的权衡方面是高效的，但不同方法有特定适用场景需慎重选择。

Abstract: We address fairness in the context of sequential bundle recommendation, where
users are served in turn with sets of relevant and compatible items. Motivated
by real-world scenarios, we formalize producer-fairness, that seeks to achieve
desired exposure of different item groups across users in a recommendation
session. Our formulation combines naturally with building high quality bundles.
Our problem is solved in real time as users arrive. We propose an exact
solution that caters to small instances of our problem. We then examine two
heuristics, quality-first and fairness-first, and an adaptive variant that
determines on-the-fly the right balance between bundle fairness and quality.
Our experiments on three real-world datasets underscore the strengths and
limitations of each solution and demonstrate their efficacy in providing fair
bundle recommendations without compromising bundle quality.

</details>


### [259] [On the ability of Deep Neural Networks to Learn Granger Causality in Multi-Variate Time Series Data](https://arxiv.org/abs/2506.20347)
*Malik Shahid Sultan,Hernando Ombao*

Main category: cs.LG

TL;DR: 本文探讨如何通过深度学习模型从多变量时间序列数据中学习Granger因果关系结构，并提出了一种利用模型不确定性评估GC的新方法。


<details>
  <summary>Details</summary>
Motivation: 传统的线性向量自回归（VAR）模型在处理时间序列数据中的Granger因果关系存在局限性，因为需要满足特定的统计假设。本文旨在突破这些限制，利用深度神经网络（DNNs）的功能逼近能力，为Granger因果关系研究提供新的解决方案。

Method: 该方法提出通过对比在使用所有时间序列数据的情况下与去掉某一具体时间序列变量模型的残差分布，来揭示学习到的GC结构。并通过对输入层的dropout比较其对深度学习模型学GC能力的影响，而无需通过损失函数添加显式的变量选择或稀疏回归项。

Result: 实验结果表明，在具备充足训练数据的情况下，一个良好正则化的深度学习模型可以学得真实的Granger因果关系结构。

Conclusion: 深度学习模型可以通过联合建模时间序列数据自发地学习Granger因果关系结构，而无需依赖传统方法中的显式变量选择。

Abstract: Granger Causality (GC) offers an elegant statistical framework to study the
association between multivariate time series data. Linear Vector Autoregressive
models (VAR) though have nice interpretation properties but have limited
practical application due to underlying assumptions on the kind of associations
that can be captured by these models. Numerous attempts have already been made
in the literature that exploit the functional approximation power of Deep
Neural Networks (DNNs) for the task of GC estimation. These methods however
treat GC as a variable selection problem. We present a novel paradigm for
approaching GC. We present this idea that GC is essentially linked with
prediction and if a deep learning model is used to model the time series
collectively or jointly, a well regularized model may learn the true granger
causal structure from the data, given that there is enough training data. We
propose to uncover the learned GC structure by comparing the model uncertainty
or distribution of the residuals when the past of everything is used as
compared to the one where a specific time series component is dropped from the
model. We also compare the effect of input layer dropout on the ability of a
neural network to learn granger causality from the data. We show that a well
regularized model infact can learn the true GC structure from the data without
explicitly adding terms in the loss function that guide the model to select
variables or perform sparse regression.

</details>


### [260] [DipSVD: Dual-importance Protected SVD for Efficient LLM Compression](https://arxiv.org/abs/2506.20353)
*Xuan Ding,Rui Sun,Yunjian Zhang,Xiu Yan,Yueqi Zhou,Kaihao Huang,Suzhong Fu,Chuanlong Xie,Yao Zhu*

Main category: cs.LG

TL;DR: 提出一种名为DipSVD的双层重要性保护机制，以提高SVD压缩法的性能，特别是在高压缩比的情况下效果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的计算需求和部署成本高昂，SVD压缩因其硬件兼容性和理论保证受到关注，但现有方法未充分考虑压缩过程中对重要矩阵成分的保护。

Method: 提出双层重要性保护机制：(1) 局部重要性保护，通过信道加权数据白化方法保留权重矩阵的关键奇异向量。(2) 全局重要性保护，通过启发式或优化方法让不重要的层承担更多的压缩负担，从而减轻对关键层的影响。

Result: DipSVD在多个基准测试中优于现有的SVD压缩方法，特别是在高模型压缩比时表现更为出色。

Conclusion: DipSVD的双层重要性保护机制能够有效提升SVD压缩方法性能，实现高压缩比下的优异模型表现。

Abstract: The ever-increasing computational demands and deployment costs of large
language models (LLMs) have spurred numerous compressing methods. Compared to
quantization and unstructured pruning, SVD compression offers superior hardware
compatibility and theoretical guarantees. However, existing SVD-based methods
focus on the overall discrepancy between the original and compressed matrices
while overlooking the protection of critical components within the matrix,
which leads to inferior performance in the compressed models. This paper
proposes a dual-level importance protection mechanism to enhance SVD-based
compression methods: (1) local importance protection: preserving the most
critical singular vectors within each weight matrix through channel-weighted
data whitening; and (2) global importance protection: enabling less important
layers to bear a greater portion of the compression burden through either a
heuristic or optimization-based approach, thereby minimizing the impact of
compression on critical layers. Extensive experiments demonstrate that DipSVD
outperforms existing SVD-based compression approaches across multiple
benchmarks, achieving superior model performance especially at high model
compression ratios.

</details>


### [261] [A foundation model with multi-variate parallel attention to generate neuronal activity](https://arxiv.org/abs/2506.20354)
*Francesco Carzaniga,Michael Hersche,Abu Sebastian,Kaspar Schindler,Abbas Rahimi*

Main category: cs.LG

TL;DR: 本文提出了多变量并行注意力机制（MVPA）及其在MVPFormer中的应用，用于灵活、高效地处理异质性时间序列数据，表现出卓越的跨主体泛化能力及临床性能。


<details>
  <summary>Details</summary>
Motivation: 现有DNN在处理通道配置异质性的时间序列数据（如iEEG）时，表现出显著的局限性，需要一种更灵活、高效的机制应对该挑战。

Method: 作者引入MVPA，一个将内容、时间和空间注意力解耦的自注意力机制，并以此构建了MVPFormer模型，用于预测跨主体iEEG信号演变。

Result: MVPFormer以优越的跨主体泛化能力在癫痫检测等任务中展示出专家级表现，并超越了现有Transformer基线模型，同时验证了MVPA在时间序列预测与分类任务中的优势。

Conclusion: MVPA是一种通用的时间序列注意力机制，MVPFormer作为首个开源的基础模型，为研究社区提供了卓越的工具和临床解决方案。

Abstract: Learning from multi-variate time-series with heterogeneous channel
configurations remains a fundamental challenge for deep neural networks (DNNs),
particularly in clinical domains such as intracranial electroencephalography
(iEEG), where channel setups vary widely across subjects. In this work, we
introduce multi-variate parallel attention (MVPA), a novel self-attention
mechanism that disentangles content, temporal, and spatial attention, enabling
flexible, generalizable, and efficient modeling of time-series data with
varying channel counts and configurations. We use MVPA to build MVPFormer, a
generative foundation model for human electrophysiology, trained to predict the
evolution of iEEG signals across diverse subjects. To support this and future
effort by the community, we release the SWEC iEEG dataset, the largest publicly
available iEEG dataset to date, comprising nearly 10,000 hours of recordings
from heterogeneous clinical sources. MVPFormer leverages MVPA to achieve strong
generalization across subjects, demonstrating expert-level performance in
seizure detection and outperforming state-of-the-art Transformer baselines on
our SWEC, the MAYO, and the FNUSA dataset. We further validate MVPA on standard
time-series forecasting and classification tasks, where it matches or exceeds
existing attention-based models. Together, our contributions establish MVPA as
a general-purpose attention mechanism for heterogeneous time-series and
MVPFormer as the first open-source, open-weights, and open-data iEEG foundation
model with state-of-the-art clinical performance. The code is available at
https://github.com/IBM/multi-variate-parallel-transformer. The SWEC iEEG
dataset is available at
https://mb-neuro.medical-blocks.ch/public_access/databases/ieeg/swec_ieeg.

</details>


### [262] [Towards Interpretable and Efficient Feature Selection in Trajectory Datasets: A Taxonomic Approach](https://arxiv.org/abs/2506.20359)
*Chanuka Don Samarasinghage,Dhruv Gulabani*

Main category: cs.LG

TL;DR: 本文提出了一种基于分类法的特征选择方法，通过对特征进行几何和运动学的分类，解决了高维特征导致的“特征爆炸”问题，从而提升预测性能和效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决特征维度过高导致的机器学习效率和可解释性下降等问题，该研究提出了一种以特征内部结构为基础的分类法选取特征的办法。

Method: 使用基于分类法的特征选择方法，将数据分为几何和运动学特征，并进一步细分到曲率、凹痕、速度和加速度的类别。通过这种分类以减少特征组合空间的复杂性。

Result: 基于分类法的特征选择方法在预测性能上表现出明显优势，同时显著减少了特征选择所需的时间，并明确了不同数据集对特定特征集的敏感性。

Conclusion: 本文证明了基于分类法的特征选择方法可以提高数据解释性、降低维度和计算复杂性，同时支持决策制定。其为处理轨迹数据集的研究人员和实践者提供了一种方法学框架，并为可解释人工智能领域做出了贡献。

Abstract: Trajectory analysis is not only about obtaining movement data, but it is also
of paramount importance in understanding the pattern in which an object moves
through space and time, as well as in predicting its next move. Due to the
significant interest in the area, data collection has improved substantially,
resulting in a large number of features becoming available for training and
predicting models. However, this introduces a high-dimensionality-induced
feature explosion problem, which reduces the efficiency and interpretability of
the data, thereby reducing the accuracy of machine learning models. To overcome
this issue, feature selection has become one of the most prevalent tools. Thus,
the objective of this paper was to introduce a taxonomy-based feature selection
method that categorizes features based on their internal structure. This
approach classifies the data into geometric and kinematic features, further
categorizing them into curvature, indentation, speed, and acceleration. The
comparative analysis indicated that a taxonomy-based approach consistently
achieved comparable or superior predictive performance. Furthermore, due to the
taxonomic grouping, which reduces combinatorial space, the time taken to select
features was drastically reduced. The taxonomy was also used to gain insights
into what feature sets each dataset was more sensitive to. Overall, this study
provides robust evidence that a taxonomy-based feature selection method can add
a layer of interpretability, reduce dimensionality and computational
complexity, and contribute to high-level decision-making. It serves as a step
toward providing a methodological framework for researchers and practitioners
dealing with trajectory datasets and contributing to the broader field of
explainable artificial intelligence.

</details>


### [263] [Self-Supervised Graph Learning via Spectral Bootstrapping and Laplacian-Based Augmentations](https://arxiv.org/abs/2506.20362)
*Lorenzo Bini,Stephane Marchand-Maillet*

Main category: cs.LG

TL;DR: 本文提出了LaplaceGNN，一种不依赖于负采样的新型自监督图学习框架，通过引入谱引导的技术实现高效的图结构表示学习。


<details>
  <summary>Details</summary>
Motivation: 旨在解决当前自监督图学习依赖对比目标和手工数据增强的局限性，提供一种更简化高效的方案。

Method: 通过预计算基于最大-最小中心性优化的谱增强，实现丰富的结构监督，同时采用对抗性自举训练机制强化特征学习与鲁棒性。

Result: 在多个基准数据集上的实验表明，LaplaceGNN的性能优于现有的最先进方法。

Conclusion: LaplaceGNN提供了一种高效学习表达性图表征的新方向，在不同领域具有广泛适用性。

Abstract: We present LaplaceGNN, a novel self-supervised graph learning framework that
bypasses the need for negative sampling by leveraging spectral bootstrapping
techniques. Our method integrates Laplacian-based signals into the learning
process, allowing the model to effectively capture rich structural
representations without relying on contrastive objectives or handcrafted
augmentations. By focusing on positive alignment, LaplaceGNN achieves linear
scaling while offering a simpler, more efficient, self-supervised alternative
for graph neural networks, applicable across diverse domains. Our contributions
are twofold: we precompute spectral augmentations through max-min
centrality-guided optimization, enabling rich structural supervision without
relying on handcrafted augmentations, then we integrate an adversarial
bootstrapped training scheme that further strengthens feature learning and
robustness. Our extensive experiments on different benchmark datasets show that
LaplaceGNN achieves superior performance compared to state-of-the-art
self-supervised graph methods, offering a promising direction for efficiently
learning expressive graph representations.

</details>


### [264] [TESSERA: Temporal Embeddings of Surface Spectra for Earth Representation and Analysis](https://arxiv.org/abs/2506.20380)
*Zhengpeng Feng,Sadiq Jaffer,Jovana Knezevic,Silja Sormunen,Robin Young,Madeline Lisaius,Markus Immitzer,James Ball,Clement Atzberger,David A. Coomes,Anil Madhavapeddy,Andrew Blake,Srinivasan Keshav*

Main category: cs.LG

TL;DR: TESSERA是一个用于地球观测的遥感基础模型，通过自监督学习（SSL）从像素级卫星时间序列数据中生成全球、鲁棒的10米尺度表示，结合光学和SAR数据，性能超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 弥补目前遥感地球观测领域中模型性能和开放性不足的问题，支持更高效的气候建模、碳账户、保护和可持续土地利用策略。

Method: 提出TESSERA基础模型，利用自监督学习，结合Sentinel-1 SAR和Sentinel-2 MSI数据，通过两路基于Transformer构建的编码器和MLP融合生成全球表示图。

Result: TESSERA在五项不同任务的表现上均优于传统和现有地理空间基础模型，证明了其性能的领先性。

Conclusion: TESSERA不仅在性能上树立新基准，还通过开源方法让高性能和高分辨率的遥感表示普惠化，提升了遥感的应用可能性。

Abstract: Satellite remote sensing (RS) enables a wide array of downstream Earth
observation (EO) applications, including climate modeling, carbon accounting,
and strategies for conservation and sustainable land use. We present TESSERA, a
novel Remote Sensing Foundation Model (RSFM) that uses Self-Supervised Learning
(SSL) to generate global, robust representations at 10m scale from pixel-level
satellite time series data. TESSERA combines information from only optical and
SAR data streams using two parallel Transformer-based encoders: one dedicated
to Sentinel-1 SAR polarizations and another to Sentinel-2 MSI data (10 selected
spectral bands) to create representations that are then fused using a
multilayer perceptron (MLP), resulting in a global representation map covering
the years 2017 to 2024. Our precomputed representations set a new
state-of-the-art performance benchmark and our open-source approach
democratizes access to high-performance, high-resolution representations. We
benchmark the performance of TESSERA in five diverse tasks, comparing our work
with state-of-the-art task-specific models and other foundation models. Our
results show that TESSERA outperforms both traditional RS baselines and the
leading geospatial foundation models in these diverse downstream tasks.

</details>


### [265] [Client Clustering Meets Knowledge Sharing: Enhancing Privacy and Robustness in Personalized Peer-to-Peer Learning](https://arxiv.org/abs/2506.20413)
*Mohammad Mahdi Maheri,Denys Herasymuk,Hamed Haddadi*

Main category: cs.LG

TL;DR: 该研究提出了一种名为P4的方法，用于在资源有限的物联网设备之间实现高效的个性化学习，同时保障隐私和抗中毒攻击能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI在物联网生态系统中的广泛应用，如何高效且私密地在分散、异构的设备间实现个性化学习成为一大挑战。

Method: P4利用轻量化、完全分布式算法检测客户端相似性并形成协作组，同时通过差分隐私知识蒸馏方法实现组内协作训练，以此实现个性化学习、隐私保护和抗恶意攻击能力。

Result: 实验结果表明，P4在各种异构和攻击场景下，相较主流差分隐私点对点方法，准确率提升5%到30%，即使在最高30%恶意客户端情形下也保持鲁棒性。此外，P4能实际部署于资源受限设备，协作训练额外时间仅需约7秒。

Conclusion: P4为物联网中的资源受限设备提供了一种高效、鲁棒、私密的个性化学习解决方案，在实验中展现出卓越性能和实际可行性。

Abstract: The growing adoption of Artificial Intelligence (AI) in Internet of Things
(IoT) ecosystems has intensified the need for personalized learning methods
that can operate efficiently and privately across heterogeneous,
resource-constrained devices. However, enabling effective personalized learning
in decentralized settings introduces several challenges, including efficient
knowledge transfer between clients, protection of data privacy, and resilience
against poisoning attacks. In this paper, we address these challenges by
developing P4 (Personalized, Private, Peer-to-Peer) -- a method designed to
deliver personalized models for resource-constrained IoT devices while ensuring
differential privacy and robustness against poisoning attacks. Our solution
employs a lightweight, fully decentralized algorithm to privately detect client
similarity and form collaborative groups. Within each group, clients leverage
differentially private knowledge distillation to co-train their models,
maintaining high accuracy while ensuring robustness to the presence of
malicious clients. We evaluate P4 on popular benchmark datasets using both
linear and CNN-based architectures across various heterogeneity settings and
attack scenarios. Experimental results show that P4 achieves 5% to 30% higher
accuracy than leading differentially private peer-to-peer approaches and
maintains robustness with up to 30% malicious clients. Additionally, we
demonstrate its practicality by deploying it on resource-constrained devices,
where collaborative training between two clients adds only ~7 seconds of
overhead.

</details>


### [266] [Off-Policy Evaluation and Learning for the Future under Non-Stationarity](https://arxiv.org/abs/2506.20417)
*Tatsuhiro Shimizu,Kazuki Kawamura,Takanori Muroi,Yusuke Narita,Kei Tateno,Takuma Udagawa,Yuta Saito*

Main category: cs.LG

TL;DR: 本文研究非平稳环境中预测和优化政策未来价值的问题，提出了新式的离政策评估和学习方法（OPFV）。


<details>
  <summary>Details</summary>
Motivation: 解决在非平稳环境中，基于历史数据评估和优化未来政策面临的偏差问题。

Method: 提出了离政策未来价值估计器（OPFV），通过时间序列数据中的结构（如季节性）和新型重要性加权法实现低偏差评估，并拓展为新型策略梯度方法。

Result: 实验结果表明，OPFV 在非平稳场景下的政策价值评估和优化显著优于现有方法。

Conclusion: OPFV 有效解决了在非平稳环境下进行未来政策评估和优化的挑战，为相关场景的政策学习提供了具有前景的方法。

Abstract: We study the novel problem of future off-policy evaluation (F-OPE) and
learning (F-OPL) for estimating and optimizing the future value of policies in
non-stationary environments, where distributions vary over time. In e-commerce
recommendations, for instance, our goal is often to estimate and optimize the
policy value for the upcoming month using data collected by an old policy in
the previous month. A critical challenge is that data related to the future
environment is not observed in the historical data. Existing methods assume
stationarity or depend on restrictive reward-modeling assumptions, leading to
significant bias. To address these limitations, we propose a novel estimator
named \textit{\textbf{O}ff-\textbf{P}olicy Estimator for the \textbf{F}uture
\textbf{V}alue (\textbf{\textit{OPFV}})}, designed for accurately estimating
policy values at any future time point. The key feature of OPFV is its ability
to leverage the useful structure within time-series data. While future data
might not be present in the historical log, we can leverage, for example,
seasonal, weekly, or holiday effects that are consistent in both the historical
and future data. Our estimator is the first to exploit these time-related
structures via a new type of importance weighting, enabling effective F-OPE.
Theoretical analysis identifies the conditions under which OPFV becomes
low-bias. In addition, we extend our estimator to develop a new policy-gradient
method to proactively learn a good future policy using only historical data.
Empirical results show that our methods substantially outperform existing
methods in estimating and optimizing the future policy value under
non-stationarity for various experimental setups.

</details>


### [267] [Tackling Data Heterogeneity in Federated Learning through Knowledge Distillation with Inequitable Aggregation](https://arxiv.org/abs/2506.20431)
*Xing Ma*

Main category: cs.LG

TL;DR: 提出了KDIA策略解决联邦学习中由于异质性和少量客户端参与训练所导致的问题。


<details>
  <summary>Details</summary>
Motivation: 在大规模客户端的联邦学习中，仅有少量客户端参与训练时，传统方法性能下降明显，需新的方法应对该挑战。

Method: 提出了KDIA策略，其中学生模型为参与客户端的平均聚合，教师模型通过基于参与频率等多因素加权聚合，结合自知识蒸馏和服务器生成的IID数据特征辅助训练。

Result: 在CIFAR-10/100/CINIC-10等数据集和异质设定下展示了KDIA在提高准确性和减少训练轮次上的优势。

Conclusion: KDIA在异质性强的联邦学习环境下取得显著效果，具有重要实践意义。

Abstract: Federated learning aims to train a global model in a distributed environment
that is close to the performance of centralized training. However, issues such
as client label skew, data quantity skew, and other heterogeneity problems
severely degrade the model's performance. Most existing methods overlook the
scenario where only a small portion of clients participate in training within a
large-scale client setting, whereas our experiments show that this scenario
presents a more challenging federated learning task. Therefore, we propose a
Knowledge Distillation with teacher-student Inequitable Aggregation (KDIA)
strategy tailored to address the federated learning setting mentioned above,
which can effectively leverage knowledge from all clients. In KDIA, the student
model is the average aggregation of the participating clients, while the
teacher model is formed by a weighted aggregation of all clients based on three
frequencies: participation intervals, participation counts, and data volume
proportions. During local training, self-knowledge distillation is performed.
Additionally, we utilize a generator trained on the server to generate
approximately independent and identically distributed (IID) data features
locally for auxiliary training. We conduct extensive experiments on the
CIFAR-10/100/CINIC-10 datasets and various heterogeneous settings to evaluate
KDIA. The results show that KDIA can achieve better accuracy with fewer rounds
of training, and the improvement is more significant under severe
heterogeneity.

</details>


### [268] [Méthode de quadrature pour les PINNs fondée théoriquement sur la hessienne des résiduels](https://arxiv.org/abs/2506.20441)
*Antoine Caradot,Rémi Emonet,Amaury Habrard,Abdel-Rahim Mezidi,Marc Sebban*

Main category: cs.LG

TL;DR: 本文提出一种新的基于函数Hessian矩阵的正交法，用于改进物理信息神经网络（PINNs）的训练采样方法。


<details>
  <summary>Details</summary>
Motivation: PINNs通过在损失函数中嵌入物理模型，在求解偏微分方程（PDEs）上表现出良好的效率。但当前的采样方法尚待改进。

Method: 作者提出了一种基于Hessian计算的新正交法，用于更有效地近似定积分，并用于PINNs训练过程中调整采样点。

Result: 该方法优化了PINNs中采样点的选择，潜在提升了模型的训练效率。

Conclusion: 利用函数的Hessian信息进行新的采样策略能够增强PINNs的性能，并对相关应用带来更高效的解法。

Abstract: Physics-informed Neural Networks (PINNs) have emerged as an efficient way to
learn surrogate neural solvers of PDEs by embedding the physical model in the
loss function and minimizing its residuals using automatic differentiation at
so-called collocation points. Originally uniformly sampled, the choice of the
latter has been the subject of recent advances leading to adaptive sampling
refinements. In this paper, we propose a new quadrature method for
approximating definite integrals based on the hessian of the considered
function, and that we leverage to guide the selection of the collocation points
during the training process of PINNs.

</details>


### [269] [Automatic Demonstration Selection for LLM-based Tabular Data Classification](https://arxiv.org/abs/2506.20451)
*Shuchu Han,Wolfgang Bruckner*

Main category: cs.LG

TL;DR: 本文提出了一种算法，用于在In-Context Learning中自动选择表格数据分类所需的演示数量。


<details>
  <summary>Details</summary>
Motivation: 解决ICL中如何确定表格数据分类的理想演示数量问题，并综合数据分布、提示模板和LLM特性优化选择。

Method: 基于谱图理论，提出通过构建相似性图和分析其拉普拉斯特征值来确定最少的演示数量。

Result: 实验表明，与传统的随机选择算法相比，所提出方法在各种数据集和LLM上表现更优。

Conclusion: 该算法有效衡量演示相似性，并提供了一种自动化方法，优化ICL中的演示选择，提升分类性能。

Abstract: A fundamental question in applying In-Context Learning (ICL) for tabular data
classification is how to determine the ideal number of demonstrations in the
prompt. This work addresses this challenge by presenting an algorithm to
automatically select a reasonable number of required demonstrations. Our method
distinguishes itself by integrating not only the tabular data's distribution
but also the user's selected prompt template and the specific Large Language
Model (LLM) into its estimation. Rooted in Spectral Graph Theory, our proposed
algorithm defines a novel metric to quantify the similarities between different
demonstrations. We then construct a similarity graph and analyze the
eigenvalues of its Laplacian to derive the minimum number of demonstrations
capable of representing the data within the LLM's intrinsic representation
space. We validate the efficacy of our approach through experiments comparing
its performance against conventional random selection algorithms on diverse
datasets and LLMs.

</details>


### [270] [Multimodal Representation Learning and Fusion](https://arxiv.org/abs/2506.20494)
*Qihang Jin,Enze Ge,Yuhang Xie,Hongying Luo,Junhao Song,Ziqian Bi,Chia Xin Liang,Jibin Guan,Joe Yeong,Junfeng Hao*

Main category: cs.LG

TL;DR: 多模态学习通过整合图像、文本和音频等信息，增强AI在复杂现实环境中的理解、推理和决策能力。


<details>
  <summary>Details</summary>
Motivation: 推动机器理解复杂事物、实现更知性化的交互和决策。

Method: 探讨表示学习、对齐方法、融合策略等核心技术，探索非监督学习、半监督学习及AutoML工具。

Result: 尽管多模态学习取得进展，但面临数据格式差异、不完整输入、防御对抗性攻击等挑战，并正在研发更有效的模型和评估基准。

Conclusion: 该领域有望在计算机视觉、自然语言处理、语音识别和医疗领域取得突破，最终实现对复杂世界更接近人类的理解。

Abstract: Multi-modal learning is a fast growing area in artificial intelligence. It
tries to help machines understand complex things by combining information from
different sources, like images, text, and audio. By using the strengths of each
modality, multi-modal learning allows AI systems to build stronger and richer
internal representations. These help machines better interpretation, reasoning,
and making decisions in real-life situations. This field includes core
techniques such as representation learning (to get shared features from
different data types), alignment methods (to match information across
modalities), and fusion strategies (to combine them by deep learning models).
Although there has been good progress, some major problems still remain. Like
dealing with different data formats, missing or incomplete inputs, and
defending against adversarial attacks. Researchers now are exploring new
methods, such as unsupervised or semi-supervised learning, AutoML tools, to
make models more efficient and easier to scale. And also more attention on
designing better evaluation metrics or building shared benchmarks, make it
easier to compare model performance across tasks and domains. As the field
continues to grow, multi-modal learning is expected to improve many areas:
computer vision, natural language processing, speech recognition, and
healthcare. In the future, it may help to build AI systems that can understand
the world in a way more like humans, flexible, context aware, and able to deal
with real-world complexity.

</details>


### [271] [Collaborative Batch Size Optimization for Federated Learning](https://arxiv.org/abs/2506.20511)
*Arno Geimer,Karthick Panner Selvam,Beltran Fiz Pontiveros*

Main category: cs.LG

TL;DR: 本文提出使用贪婪随机搜索优化联邦学习的本地批处理大小，以提升训练效率和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习中，由于参与者硬件和训练配置差异，可能阻碍训练效率。作者希望通过优化硬件使用来改进本地训练过程。

Method: 利用联邦学习的并行处理特性，提出基于贪婪随机搜索的优化方法，调节每个参与者的本地批处理大小以改进训练效率。

Result: 结果表明，与默认参数设置相比，该方法加速了收敛速度，同时效果接近于优化本地参数的理想情况。

Conclusion: 通过硬件资源与训练配置的优化，可有效提升联邦学习训练效率，而无需数据交换。

Abstract: Federated Learning (FL) is a decentralized collaborative Machine Learning
framework for training models without collecting data in a centralized
location. It has seen application across various disciplines, from helping
medical diagnoses in hospitals to detecting fraud in financial transactions. In
this paper, we focus on improving the local training process through hardware
usage optimization. While participants in a federation might share the hardware
they are training on, since there is no information exchange between them,
their training process can be hindered by an improper training configuration.
Taking advantage of the parallel processing inherent to Federated Learning, we
use a greedy randomized search to optimize local batch sizes for the best
training settings across all participants. Our results show that against
default parameter settings, our method improves convergence speed while staying
nearly on par with the case where local parameters are optimized.

</details>


### [272] [WallStreetFeds: Client-Specific Tokens as Investment Vehicles in Federated Learning](https://arxiv.org/abs/2506.20518)
*Arno Geimer,Beltran Fiz Pontiveros,Radu State*

Main category: cs.LG

TL;DR: 该论文介绍了一种结合去中心化金融（DeFi）和自动化做市商（AMMs）的激励框架，用于改进联邦学习中的奖励分配方案。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习中的奖励分配框架较少研究，难以充分激励参与者，同时缺乏灵活性和扩展性。

Method: 提出了一种新框架，将客户端特定代币与去中心化金融平台相结合，利用自动化做市商，实现更灵活和可扩展的奖励分配以及支持第三方投资。

Result: 通过创新设计，该框架解决了现有激励方案的局限性，为联邦学习提供了新的奖励分发机制。

Conclusion: 该框架提升了联邦学习的激励机制，具备扩展性和灵活性，同时也为第三方投资者创造了参与机会。

Abstract: Federated Learning (FL) is a collaborative machine learning paradigm which
allows participants to collectively train a model while training data remains
private. This paradigm is especially beneficial for sectors like finance, where
data privacy, security and model performance are paramount. FL has been
extensively studied in the years following its introduction, leading to, among
others, better performing collaboration techniques, ways to defend against
other clients trying to attack the model, and contribution assessment methods.
An important element in for-profit Federated Learning is the development of
incentive methods to determine the allocation and distribution of rewards for
participants. While numerous methods for allocation have been proposed and
thoroughly explored, distribution frameworks remain relatively understudied. In
this paper, we propose a novel framework which introduces client-specific
tokens as investment vehicles within the FL ecosystem. Our framework aims to
address the limitations of existing incentive schemes by leveraging a
decentralized finance (DeFi) platform and automated market makers (AMMs) to
create a more flexible and scalable reward distribution system for
participants, and a mechanism for third parties to invest in the federation
learning process.

</details>


### [273] [Industrial Energy Disaggregation with Digital Twin-generated Dataset and Efficient Data Augmentation](https://arxiv.org/abs/2506.20525)
*Christian Internò,Andrea Castellani,Sebastian Schmitt,Fabio Stella,Barbara Hammer*

Main category: cs.LG

TL;DR: 本文提出了一个新的人工合成数据集SIDED，并引入了一种AMDA方法，用于优化工业负载监测中的能量分解任务。


<details>
  <summary>Details</summary>
Motivation: 现有工业非侵入式负载监测受到高质量数据匮乏和能耗模式复杂性的限制，需要解决数据稀缺及隐私问题。

Method: 设计了由数字孪生模拟生成的人工合成数据集SIDED，并提出了一种AMDA数据增强方法，通过根据设备的相对影响智能调整功率分配，提升模型泛化性能。

Result: 实验表明，应用AMDA增强的数据训练的模型在复杂工业负载的能量分解中表现优越，性能指标得到了显著改善（如误差从0.451降至0.093）。

Conclusion: AMDA方法有效提升了NILM模型的泛化性能，并通过优化数据分布对齐，推动了工业能耗分解任务的进步。

Abstract: Industrial Non-Intrusive Load Monitoring (NILM) is limited by the scarcity of
high-quality datasets and the complex variability of industrial energy
consumption patterns. To address data scarcity and privacy issues, we introduce
the Synthetic Industrial Dataset for Energy Disaggregation (SIDED), an
open-source dataset generated using Digital Twin simulations. SIDED includes
three types of industrial facilities across three different geographic
locations, capturing diverse appliance behaviors, weather conditions, and load
profiles. We also propose the Appliance-Modulated Data Augmentation (AMDA)
method, a computationally efficient technique that enhances NILM model
generalization by intelligently scaling appliance power contributions based on
their relative impact. We show in experiments that NILM models trained with
AMDA-augmented data significantly improve the disaggregation of energy
consumption of complex industrial appliances like combined heat and power
systems. Specifically, in our out-of-sample scenarios, models trained with AMDA
achieved a Normalized Disaggregation Error of 0.093, outperforming models
trained without data augmentation (0.451) and those trained with random data
augmentation (0.290). Data distribution analyses confirm that AMDA effectively
aligns training and test data distributions, enhancing model generalization.

</details>


### [274] [Physics-Informed Machine Learning Regulated by Finite Element Analysis for Simulation Acceleration of Laser Powder Bed Fusion](https://arxiv.org/abs/2506.20537)
*R. Sharma,M. Raissi,Y. B. Guo*

Main category: cs.LG

TL;DR: 提出了一种结合有限元分析和物理信息神经网络的框架（FEA-PINN），可加速激光铺粉床融合（LPBF）过程中的热场预测，减少计算成本，同时保持有限元分析的精度。


<details>
  <summary>Details</summary>
Motivation: 解决传统有限元分析在LPBF仿真中计算成本高的问题。

Method: 开发了动态材料更新策略与FEA调节物理信息神经网络（FEA-PINN）框架，包含热容法建模相变行为，并通过有限元校正实现物理一致性并减少误差累积。

Result: FEA-PINN相较于传统FEA能够显著降低计算成本，同时保持了相当的精度，并在基准数据和LPBF单轨扫描验证中得到了证明。

Conclusion: FEA-PINN框架有效地结合了精确性与高效性，为LPBF过程的热场预测提供了新的解决方案。

Abstract: Efficient simulation of Laser Powder Bed Fusion (LPBF) is crucial for process
prediction due to the lasting issue of high computation cost using traditional
numerical methods such as finite element analysis (FEA). This study presents an
efficient modeling framework termed FEA-Regulated Physics-Informed Neural
Network (FEA-PINN) to accelerate the thermal field prediction in a LPBF process
while maintaining the FEA accuracy. A novel dynamic material updating strategy
is developed to capture the dynamic phase change of powder-liquid-solid in the
PINN model. The PINN model incorporates temperature-dependent material
properties and phase change behavior using the apparent heat capacity method.
While the PINN model demonstrates high accuracy with a small training data and
enables generalization of new process parameters via transfer learning, it
faces the challenge of high computation cost in time-dependent problems due to
the residual accumulation. To overcome this issue, the FEA-PINN framework
integrates corrective FEA simulations during inference to enforce physical
consistency and reduce error drift. A comparative analysis shows that FEA-PINN
achieves equivalent accuracy to FEA while significantly reducing computational
cost. The framework has been validated using the benchmark FEA data and
demonstrated through single-track scanning in LPBF.

</details>


### [275] [Demonstration of effective UCB-based routing in skill-based queues on real-world data](https://arxiv.org/abs/2506.20543)
*Sanne van Kempen,Jaron Sanders,Fiona Sloothaak,Maarten G. Wolf*

Main category: cs.LG

TL;DR: 研究一种适用于数据中心、云计算网络和服务系统的强化学习算法，用于优化用户路由，通过实际数据集的案例研究证明了其效率和应用潜力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在改善技能型排队系统中的客户路由，通过强化学习算法实现动态环境适应和多目标优化。

Method: 实施强化学习算法，通过案例研究验证其性能，并引入启发式规则以减少延迟，调整参数以平衡性能。

Result: 实验表明新算法适应性强，超越静态基准策略，可实现实时应用并优化多目标，如服务器负载公平性和客户等待时间。

Conclusion: 算法在实际排队系统优化中具备显著潜力，提供关于参数调节和敏感性分析的重要见解，适合复杂动态环境。

Abstract: This paper is about optimally controlling skill-based queueing systems such
as data centers, cloud computing networks, and service systems. By means of a
case study using a real-world data set, we investigate the practical
implementation of a recently developed reinforcement learning algorithm for
optimal customer routing. Our experiments show that the algorithm efficiently
learns and adapts to changing environments and outperforms static benchmark
policies, indicating its potential for live implementation. We also augment the
real-world applicability of this algorithm by introducing a new heuristic
routing rule to reduce delays. Moreover, we show that the algorithm can
optimize for multiple objectives: next to payoff maximization, secondary
objectives such as server load fairness and customer waiting time reduction can
be incorporated. Tuning parameters are used for balancing inherent performance
trade--offs. Lastly, we investigate the sensitivity to estimation errors and
parameter tuning, providing valuable insights for implementing adaptive routing
algorithms in complex real-world queueing systems.

</details>


### [276] [Benchmarking Unsupervised Strategies for Anomaly Detection in Multivariate Time Series](https://arxiv.org/abs/2506.20574)
*Laura Boggia,Rafael Teixeira de Lima,Bogdan Malaescu*

Main category: cs.LG

TL;DR: 本文研究了基于Transformer的多变量时间序列异常检测方法，重点探讨了iTransformer架构的应用及相关问题。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列中异常检测由于异常的不确定性及时间序列维度间复杂依赖性具有挑战性，开发高效检测方法至关重要。

Method: 文章针对iTransformer架构，分析了窗口大小、步长及模型维度对性能的影响，研究多维异常分数标注方法，探讨异常数据对训练的影响及替代损失函数，并比较了多种Transformer模型在不同数据集上的表现。

Result: 研究揭示了关键参数和训练过程中异常数据的影响，验证了替代损失函数的有效性，并提供了基于iTransformer的广泛性能评估。

Conclusion: 本文证明了Transformer在时间序列异常检测中的潜力，提出了一系列优化方向并为基于iTransformer的实践提供了指导。

Abstract: Anomaly detection in multivariate time series is an important problem across
various fields such as healthcare, financial services, manufacturing or physics
detector monitoring. Accurately identifying when unexpected errors or faults
occur is essential, yet challenging, due to the unknown nature of anomalies and
the complex interdependencies between time series dimensions. In this paper, we
investigate transformer-based approaches for time series anomaly detection,
focusing on the recently proposed iTransformer architecture. Our contributions
are fourfold: (i) we explore the application of the iTransformer to time series
anomaly detection, and analyse the influence of key parameters such as window
size, step size, and model dimensions on performance; (ii) we examine methods
for extracting anomaly labels from multidimensional anomaly scores and discuss
appropriate evaluation metrics for such labels; (iii) we study the impact of
anomalous data present during training and assess the effectiveness of
alternative loss functions in mitigating their influence; and (iv) we present a
comprehensive comparison of several transformer-based models across a diverse
set of datasets for time series anomaly detection.

</details>


### [277] [Exploring Graph-Transformer Out-of-Distribution Generalization Abilities](https://arxiv.org/abs/2506.20575)
*Itay Niv,Neta Rabin*

Main category: cs.LG

TL;DR: 本研究探讨图神经网络（GNN）在分布偏移场景下的性能，发现图转换模型（Graph Transformer, GT）和混合GT-MPNN方法展现出更强的泛化能力，并提出了一种新的后训练分析方式来更深入理解其泛化性能。


<details>
  <summary>Details</summary>
Motivation: 当前GNN方法常假设训练和测试数据具有相同的分布，这在实际场景中往往难以实现，亟需解决GNN在分布偏移情况下的泛化问题，特别是骨干架构的影响。

Method: 系统评估了图转换模型（GT）和混合GT-MPNN骨干架构在分布偏移场景下的表现，结合域泛化算法设计了新的基准测试，并提出了一种后训练数据结构分析方法，用于理解分布对齐与类别分离。

Result: GT及混合GT-MPNN方法在各种分布偏移测试中表现优于传统MPNN，即便在未使用专门域泛化算法的情况下亦如此。

Conclusion: GT方法在分布偏移条件下表现出色，后训练分析方法提供了对模型泛化能力的深刻见解，并可扩展应用于更广泛的域泛化问题，展示了其在实际图学习任务中的潜力。

Abstract: Deep learning on graphs has shown remarkable success across numerous
applications, including social networks, bio-physics, traffic networks, and
recommendation systems. Regardless of their successes, current methods
frequently depend on the assumption that training and testing data share the
same distribution, a condition rarely met in real-world scenarios. While
graph-transformer (GT) backbones have recently outperformed traditional
message-passing neural networks (MPNNs) in multiple in-distribution (ID)
benchmarks, their effectiveness under distribution shifts remains largely
unexplored.
  In this work, we address the challenge of out-of-distribution (OOD)
generalization for graph neural networks, with a special focus on the impact of
backbone architecture. We systematically evaluate GT and hybrid backbones in
OOD settings and compare them to MPNNs. To do so, we adapt several leading
domain generalization (DG) algorithms to work with GTs and assess their
performance on a benchmark designed to test a variety of distribution shifts.
Our results reveal that GT and hybrid GT-MPNN backbones consistently
demonstrate stronger generalization ability compared to MPNNs, even without
specialized DG algorithms.
  Additionally, we propose a novel post-training analysis approach that
compares the clustering structure of the entire ID and OOD test datasets,
specifically examining domain alignment and class separation. Demonstrating its
model-agnostic design, this approach not only provided meaningful insights into
GT and MPNN backbones. It also shows promise for broader applicability to DG
problems beyond graph learning, offering a deeper perspective on generalization
abilities that goes beyond standard accuracy metrics. Together, our findings
highlight the promise of graph-transformers for robust, real-world graph
learning and set a new direction for future research in OOD generalization.

</details>


### [278] [The kernel of graph indices for vector search](https://arxiv.org/abs/2506.20584)
*Mariano Tepper,Ted Willke*

Main category: cs.LG

TL;DR: 使用机器学习方法，提出一种适用于度量和非度量向量空间的图索引方法SVG，具有形式化的导航性保证，并进一步提出了改进版本SVG-L0。


<details>
  <summary>Details</summary>
Motivation: 现有的图索引主要基于计算几何方法，仅在欧几里得空间中具有导航性保证，因此需要一种适用于更广泛向量空间的图索引方法。

Method: 利用核方法建立支持向量图（SVG）的连接性，并提出加入$\ell_0$稀疏性约束的改进方法SVG-L0，以实现有限出度的图结构。

Result: 证明SVG在度量和非度量空间中均具有形式化的导航性保证，同时SVG-L0在保持计算复杂度的情况下具备自调节性能，避免了传统启发式方法的局限。

Conclusion: SVG提供了一种适用于更多空间类型的通用图索引框架，而SVG-L0进一步优化了实践需求，为图导航性工具提供了一种更加理论化和系统化的方法。

Abstract: The most popular graph indices for vector search use principles from
computational geometry to build the graph. Hence, their formal graph
navigability guarantees are only valid in Euclidean space. In this work, we
show that machine learning can be used to build graph indices for vector search
in metric and non-metric vector spaces (e.g., for inner product similarity).
From this novel perspective, we introduce the Support Vector Graph (SVG), a new
type of graph index that leverages kernel methods to establish the graph
connectivity and that comes with formal navigability guarantees valid in metric
and non-metric vector spaces. In addition, we interpret the most popular graph
indices, including HNSW and DiskANN, as particular specializations of SVG and
show that new indices can be derived from the principles behind this
specialization. Finally, we propose SVG-L0 that incorporates an $\ell_0$
sparsity constraint into the SVG kernel method to build graphs with a bounded
out-degree. This yields a principled way of implementing this practical
requirement, in contrast to the traditional heuristic of simply truncating the
out edges of each node. Additionally, we show that SVG-L0 has a self-tuning
property that avoids the heuristic of using a set of candidates to find the
out-edges of each node and that keeps its computational complexity in check.

</details>


### [279] [H-FEX: A Symbolic Learning Method for Hamiltonian Systems](https://arxiv.org/abs/2506.20607)
*Jasen Lai,Senwei Liang,Chunmei Wang*

Main category: cs.LG

TL;DR: 提出了一种新的符号学习方法H-FEX，用于从数据中学习复杂的哈密顿系统，有效解决了能量守恒的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的数据驱动方法难以在复杂哈密顿系统中同时准确捕获系统方程和能量守恒。

Method: 设计了一种新的符号学习方法H-FEX，引入了新的交互节点以捕获复杂的交互项，从观测数据中直接学习哈密顿函数。

Result: 实验表明H-FEX能有效恢复复杂系统的哈密顿函数，准确表现系统动态特性并在长期内保持能量守恒。

Conclusion: H-FEX为发现复杂动力系统的封闭形式表达提供了一个强有力的框架。

Abstract: Hamiltonian systems describe a broad class of dynamical systems governed by
Hamiltonian functions, which encode the total energy and dictate the evolution
of the system. Data-driven approaches, such as symbolic regression and neural
network-based methods, provide a means to learn the governing equations of
dynamical systems directly from observational data of Hamiltonian systems.
However, these methods often struggle to accurately capture complex Hamiltonian
functions while preserving energy conservation. To overcome this limitation, we
propose the Finite Expression Method for learning Hamiltonian Systems (H-FEX),
a symbolic learning method that introduces novel interaction nodes designed to
capture intricate interaction terms effectively. Our experiments, including
those on highly stiff dynamical systems, demonstrate that H-FEX can recover
Hamiltonian functions of complex systems that accurately capture system
dynamics and preserve energy over long time horizons. These findings highlight
the potential of H-FEX as a powerful framework for discovering closed-form
expressions of complex dynamical systems.

</details>


### [280] [Lost in Retraining: Roaming the Parameter Space of Exponential Families Under Closed-Loop Learning](https://arxiv.org/abs/2506.20623)
*Fariba Jangjoo,Matteo Marsili,Yasser Roudi*

Main category: cs.LG

TL;DR: 本文探讨了闭环学习过程，特别是在指数族模型下的动态表现，并分析其可能的问题及解决方案。


<details>
  <summary>Details</summary>
Motivation: 由于未来大型神经网络模型可能主要利用人工生成的数据进行训练，研究这种闭环学习的过程显得尤为重要。

Method: 将研究集中在指数族模型上，并推导参数动态的运动方程，结合最大似然估计与马氏统计量属性，分析闭环学习如何受初始数据偏差影响，提出污染数据、最大后验估计及正则化的方法来减少负面作用。

Result: 发现闭环学习中的数据偏差可能被放大导致状态吸收，但通过引入少量固定模型生成的数据、利用最大后验估计或加入正则化可以避免这一问题。

Conclusion: 闭环学习可能放大初始偏差，但通过适当方法干预，可以减轻这些问题，并且模型重参数化会影响这种方法的表现。

Abstract: Closed-loop learning is the process of repeatedly estimating a model from
data generated from the model itself. It is receiving great attention due to
the possibility that large neural network models may, in the future, be
primarily trained with data generated by artificial neural networks themselves.
We study this process for models that belong to exponential families, deriving
equations of motions that govern the dynamics of the parameters. We show that
maximum likelihood estimation of the parameters endows sufficient statistics
with the martingale property and that as a result the process converges to
absorbing states that amplify initial biases present in the data. However, we
show that this outcome may be prevented by polluting the data with an
infinitesimal fraction of data points generated from a fixed model, by relying
on maximum a posteriori estimation or by introducing regularisation.
Furthermore, we show that the asymptotic behavior of the dynamics is not
reparametrisation invariant.

</details>


### [281] [Efficient Federated Learning with Encrypted Data Sharing for Data-Heterogeneous Edge Devices](https://arxiv.org/abs/2506.20644)
*Hangyu Li,Hongyue Wu,Guodong Fan,Zhen Zhang,Shizhan Chen,Zhiyong Feng*

Main category: cs.LG

TL;DR: 提出一种新型的联邦学习方法FedEDS，通过加密数据共享提高模型性能和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习忽视了网络拓扑、物理距离及数据异质性对边缘设备的影响，导致延迟增加和模型性能下降。

Method: FedEDS利用客户端模型和随机层生成加密数据，共享后让其他客户端训练和调整本地模型，然后结合本地私有数据进行联邦学习训练。

Result: FedEDS实验表明，其提高了训练收敛速度并缓解了数据异质性对模型性能的负面影响。

Conclusion: FedEDS方法适用于对收敛速度要求较高的边缘设备应用服务，有助于提升模型性能。

Abstract: As privacy protection gains increasing importance, more models are being
trained on edge devices and subsequently merged into the central server through
Federated Learning (FL). However, current research overlooks the impact of
network topology, physical distance, and data heterogeneity on edge devices,
leading to issues such as increased latency and degraded model performance. To
address these issues, we propose a new federated learning scheme on edge
devices that called Federated Learning with Encrypted Data Sharing(FedEDS).
FedEDS uses the client model and the model's stochastic layer to train the data
encryptor. The data encryptor generates encrypted data and shares it with other
clients. The client uses the corresponding client's stochastic layer and
encrypted data to train and adjust the local model. FedEDS uses the client's
local private data and encrypted shared data from other clients to train the
model. This approach accelerates the convergence speed of federated learning
training and mitigates the negative impact of data heterogeneity, making it
suitable for application services deployed on edge devices requiring rapid
convergence. Experiments results show the efficacy of FedEDS in promoting model
performance.

</details>


### [282] [Mastering Multiple-Expert Routing: Realizable $H$-Consistency and Strong Guarantees for Learning to Defer](https://arxiv.org/abs/2506.20650)
*Anqi Mao,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: 本论文提出了新颖的代理损失函数以及高效算法，用于解决多个专家学习延迟问题，并提供了强有力的理论学习保证。


<details>
  <summary>Details</summary>
Motivation: 探索如何在多个专家之间优化分配任务，权衡精度与计算成本之间的关系，特别是在自然语言生成、图像处理和医学诊断等领域。

Method: 提出具备一致性保证的新代理损失函数及算法，并在单阶段和双阶段学习场景下分别证明其一致性和理论边界。

Result: 在单阶段和双阶段学习场景中，证明了新代理损失函数的理论一致性，并提供了强理论保证。同时，实验结果表明其在性能上优于现有的基准方法。

Conclusion: 新代理损失函数及算法能够有效优化学习延迟分配任务，具有理论保证及实际优越性。

Abstract: The problem of learning to defer with multiple experts consists of optimally
assigning input instances to experts, balancing the trade-off between their
accuracy and computational cost. This is a critical challenge in natural
language generation, but also in other fields such as image processing, and
medical diagnostics. Recent studies have proposed surrogate loss functions to
optimize deferral, but challenges remain in ensuring their consistency
properties. This paper introduces novel surrogate loss functions and efficient
algorithms with strong theoretical learning guarantees. We address open
questions regarding realizable $H$-consistency, $H$-consistency bounds, and
Bayes-consistency for both single-stage (jointly learning predictor and
deferral function) and two-stage (learning only the deferral function with a
fixed expert) learning scenarios. For single-stage deferral, we introduce a
family of new realizable $H$-consistent surrogate losses and further prove
$H$-consistency for a selected member. For two-stage deferral, we derive new
surrogate losses that achieve realizable $H$-consistency, $H$-consistency
bounds, and Bayes-consistency for the two-expert scenario and, under natural
assumptions, multiple-expert scenario. Additionally, we provide enhanced
theoretical guarantees under low-noise assumptions for both scenarios. Finally,
we report the results of experiments using our proposed surrogate losses,
comparing their performance against existing baselines.

</details>


### [283] [Hear No Evil: Detecting Gradient Leakage by Malicious Servers in Federated Learning](https://arxiv.org/abs/2506.20651)
*Fei Wang,Baochun Li*

Main category: cs.LG

TL;DR: 探讨恶意梯度泄露攻击对联邦学习隐私保护的威胁，并提出了一种轻量级客户端检测机制。


<details>
  <summary>Details</summary>
Motivation: 研究如何在联邦学习中保护用户数据隐私，同时分析恶意服务器可能发动的模型操纵和梯度攻击。

Method: 分析恶意梯度泄露攻击存在的核心权衡，并提出一种简单轻量级的客户端检测机制，识别可疑模型更新。

Result: 发现梯度泄露攻击在现实联邦学习场景中难以同时高效且隐蔽，并验证了采用常见的归一化技术与联邦平均有助于防御攻击的可行性。

Conclusion: 在实际中，梯度泄露攻击难以高效操作且容易被检测，通过基本监控和轻量客户端检测机制即可有效防御，保障用户数据隐私。

Abstract: Recent work has shown that gradient updates in federated learning (FL) can
unintentionally reveal sensitive information about a client's local data. This
risk becomes significantly greater when a malicious server manipulates the
global model to provoke information-rich updates from clients. In this paper,
we adopt a defender's perspective to provide the first comprehensive analysis
of malicious gradient leakage attacks and the model manipulation techniques
that enable them. Our investigation reveals a core trade-off: these attacks
cannot be both highly effective in reconstructing private data and sufficiently
stealthy to evade detection -- especially in realistic FL settings that
incorporate common normalization techniques and federated averaging.
  Building on this insight, we argue that malicious gradient leakage attacks,
while theoretically concerning, are inherently limited in practice and often
detectable through basic monitoring. As a complementary contribution, we
propose a simple, lightweight, and broadly applicable client-side detection
mechanism that flags suspicious model updates before local training begins,
despite the fact that such detection may not be strictly necessary in realistic
FL settings. This mechanism further underscores the feasibility of defending
against these attacks with minimal overhead, offering a deployable safeguard
for privacy-conscious federated learning systems.

</details>


### [284] [Position: Machine Learning Conferences Should Establish a "Refutations and Critiques" Track](https://arxiv.org/abs/2506.19882)
*Rylan Schaeffer,Joshua Kazdan,Yegor Denisov-Blanch,Brando Miranda,Matthias Gerstgrasser,Susan Zhang,Andreas Haupt,Isha Gupta,Elyas Obbad,Jesse Dodge,Jessica Zosa Forde,Koustuv Sinha,Francesco Orabona,Sanmi Koyejo,David Donoho*

Main category: cs.LG

TL;DR: 本文讨论了在机器学习会议中建立“反驳和批判”轨道以促进研究的自我修正能力。


<details>
  <summary>Details</summary>
Motivation: 机器学习领域快速进展导致会议上有时会接受错误或有问题的研究，但缺乏系统性的纠正机制。

Method: 提出设立“反驳和批判”轨道，提供一个高质量平台支持挑战和批评已有研究。

Result: 分析了轨道设计、审查原则和潜在风险的关键点，并提供了一个示例提交说明。

Conclusion: 机器学习会议应建立官方机制，促进领域内健康的自我修正过程。

Abstract: Science progresses by iteratively advancing and correcting humanity's
understanding of the world. In machine learning (ML) research, rapid
advancements have led to an explosion of publications, but have also led to
misleading, incorrect, flawed or perhaps even fraudulent studies being accepted
and sometimes highlighted at ML conferences due to the fallibility of peer
review. While such mistakes are understandable, ML conferences do not offer
robust processes to help the field systematically correct when such errors are
made.This position paper argues that ML conferences should establish a
dedicated "Refutations and Critiques" (R & C) Track. This R & C Track would
provide a high-profile, reputable platform to support vital research that
critically challenges prior research, thereby fostering a dynamic
self-correcting research ecosystem. We discuss key considerations including
track design, review principles, potential pitfalls, and provide an
illustrative example submission concerning a recent ICLR 2025 Oral. We conclude
that ML conferences should create official, reputable mechanisms to help ML
research self-correct.

</details>


### [285] [STIMULUS: Achieving Fast Convergence and Low Sample Complexity in Stochastic Multi-Objective Learning](https://arxiv.org/abs/2506.19883)
*Zhuqing Liu,Chaosheng Dong,Michinari Momma,Simone Shao,Shaoyuan Xu,Yan Gao,Haibo Yang,Jia Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的多目标优化算法STIMULUS及其增强版本STIMULUS-M，通过递归更新随机梯度估计，大幅提升了收敛性能和采样效率，并针对适应性批量方法提出改进版本STIMULUS+/STIMULUS-M+。


<details>
  <summary>Details</summary>
Motivation: 现有多目标优化方法在收敛速度和样本复杂性方面表现不佳，亟需改进。

Method: 提出STIMULUS算法，采用递归框架更新随机梯度估计；引入动量项形成STIMULUS-M，并通过适应性批量策略优化为STIMULUS+/STIMULUS-M+。

Result: 在非凸设置中达到O(1/T)的收敛率和O(n+√nε^{-1})的采样复杂性；在强凸设置中达到O(exp{-μT})的收敛率和O(n+√nln(μ/ε))的采样复杂性。

Conclusion: STIMULUS及其改进方法显著提升了多目标优化的效果，在理论和实际性能上均取得优势。

Abstract: Recently, multi-objective optimization (MOO) has gained attention for its
broad applications in ML, operations research, and engineering. However, MOO
algorithm design remains in its infancy and many existing MOO methods suffer
from unsatisfactory convergence rate and sample complexity performance. To
address this challenge, in this paper, we propose an algorithm called STIMULUS(
stochastic path-integrated multi-gradient recursive e\ulstimator), a new and
robust approach for solving MOO problems. Different from the traditional
methods, STIMULUS introduces a simple yet powerful recursive framework for
updating stochastic gradient estimates to improve convergence performance with
low sample complexity. In addition, we introduce an enhanced version of
STIMULUS, termed STIMULUS-M, which incorporates a momentum term to further
expedite convergence. We establish $O(1/T)$ convergence rates of the proposed
methods for non-convex settings and $O (\exp{-\mu T})$ for strongly convex
settings, where $T$ is the total number of iteration rounds. Additionally, we
achieve the state-of-the-art $O \left(n+\sqrt{n}\epsilon^{-1}\right)$ sample
complexities for non-convex settings and $O\left(n+ \sqrt{n} \ln
({\mu/\epsilon})\right)$ for strongly convex settings, where $\epsilon>0$ is a
desired stationarity error. Moreover, to alleviate the periodic full gradient
evaluation requirement in STIMULUS and STIMULUS-M, we further propose enhanced
versions with adaptive batching called STIMULUS+/ STIMULUS-M+ and provide their
theoretical analysis.

</details>


### [286] [FlightKooba: A Fast Interpretable FTP Model](https://arxiv.org/abs/2506.19885)
*Jing Lu,Xuan Wu,Yizhun Tian,Songhan Fan,Yali Fang*

Main category: cs.LG

TL;DR: 本文提出FlightKooba框架，通过结合HIPPO方法、Koopman理论和控制论的状态空间方程，以更少的训练参数实现更快的飞行轨迹预测，同时显著降低内存和时间消耗。


<details>
  <summary>Details</summary>
Motivation: 当前基于Koopman理论的飞行轨迹预测模型存在解释性不足、算力需求高和训练时间过长的问题，亟需高效且可解释的解决方案。

Method: 提出了FlightKooba框架，结合HIPPO方法和控制论的结构化状态空间方程，从数据中直接构建Koopman算子，减少了可训练参数并提高了模块的可解释性。

Result: 实验表明，FlightKooba在时间和内存消耗方面表现出色，与Mamba模块相比训练时间相当，内存消耗减少50%以上，参数数量减少至十分之一。

Conclusion: FlightKooba为实现高效计算Koopman算子提供了一种可行的新方法，为时间序列预测与控制的结合开辟了新可能性。

Abstract: The Koopman theory is a powerful and effective modeling tool for converting
nonlinear systems into linear representations, and flight trajectory prediction
(FTP) is a complex nonlinear system. However, current models applying the
Koopman theory to FTP tasks are not very effective, model interpretability is
indeed an issue, and the Koopman operators are computationally intensive,
resulting in long training times. To address this issue, this paper proposes a
new modeling and control framework based on the HIPPO method, the Koopman
theory, and state space equations from cybernetics: FlightKooba. Inspired by
the idea of structural state space equations, FlightKooba directly constructs
the Koopman operators from data. This makes the framework highly interpretable
and significantly reduces the number of trainable parameters in the module,
thereby greatly reducing training time. Experiments have demonstrated the
superiority of the FlightKooba modeling method in terms of time and memory
consumption (training time comparable to the Mamba module without using
CUDA-level acceleration; memory reduced by more than 50% on most datasets, with
a tenfold reduction in the number of parameters), essentially completing the
FTP task. It provides a new method for the fast computation of the Koopman
operators, opening up new possibilities for the combination of time series
forecasting and control.

</details>


### [287] [Neuromorphic Wireless Split Computing with Resonate-and-Fire Neurons](https://arxiv.org/abs/2506.20015)
*Dengyu Wu,Jiechen Chen,H. Vincent Poor,Bipin Rajendran,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 该研究提出了一种基于共振发射神经元（RF）的无线分裂计算体系结构，用于提高时序信号处理效率，降低计算与传输能耗。


<details>
  <summary>Details</summary>
Motivation: 现有的尖峰神经网络通常使用泄漏积分发射神经元（LIF），无法有效捕获具有丰富频谱特征的流数据信号。本研究旨在为无线传感和音频识别等边缘应用提供更高效的解决方案。

Method: 通过研究共振发射神经元（RF）的振荡动态特性，设计了一种处理时域信号的无线分裂计算架构，同时假设基于OFDM的模拟无线接口用于尖峰传递。执行音频分类与调制分类实验以验证性能。

Result: 实验结果表明，提出的RF-SNN架构在分类精度上与传统的LIF-SNNs和ANNs相当，但在推理与通信过程中显著降低了尖峰率和能源消耗。

Conclusion: RF神经元的时间局部化频谱特征提取能力是一种能效优化的潜在方案，特别适合具有高频谱特征需求的实时应用。

Abstract: Neuromorphic computing offers an energy-efficient alternative to conventional
deep learning accelerators for real-time time-series processing. However, many
edge applications, such as wireless sensing and audio recognition, generate
streaming signals with rich spectral features that are not effectively captured
by conventional leaky integrate-and-fire (LIF) spiking neurons. This paper
investigates a wireless split computing architecture that employs
resonate-and-fire (RF) neurons with oscillatory dynamics to process time-domain
signals directly, eliminating the need for costly spectral pre-processing. By
resonating at tunable frequencies, RF neurons extract time-localized spectral
features while maintaining low spiking activity. This temporal sparsity
translates into significant savings in both computation and transmission
energy. Assuming an OFDM-based analog wireless interface for spike
transmission, we present a complete system design and evaluate its performance
on audio classification and modulation classification tasks. Experimental
results show that the proposed RF-SNN architecture achieves comparable accuracy
to conventional LIF-SNNs and ANNs, while substantially reducing spike rates and
total energy consumption during inference and communication.

</details>


### [288] [A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior](https://arxiv.org/abs/2506.19999)
*Francesco Ignazio Re,Andreas Opedal,Glib Manaiev,Mario Giulianelli,Ryan Cotterell*

Main category: cs.LG

TL;DR: 該研究提出了一種基於時空點過程的閱讀行為概率模型，更好地模擬閱讀中的注視和掃視行為。


<details>
  <summary>Details</summary>
Motivation: 現有閱讀行為建模方法忽略了閱讀過程中的時空動態特徵，限制了對線上句子處理的理解。

Method: 利用Hawkes過程模擬掃視行為，並使用時間捲積的形式量化注視持續時間的預測變量影響。

Result: 模型在模擬人類掃視行為時表現優於基線模型。將上下文意外性納入預測器僅對準確性提升有微小貢獻。

Conclusion: 意外性理論難以解釋細粒度的眼動行為，提出的新模型能更準確刻畫閱讀行為的時空特性。

Abstract: Reading is a process that unfolds across space and time, alternating between
fixations where a reader focuses on a specific point in space, and saccades
where a reader rapidly shifts their focus to a new point. An ansatz of
psycholinguistics is that modeling a reader's fixations and saccades yields
insight into their online sentence processing. However, standard approaches to
such modeling rely on aggregated eye-tracking measurements and models that
impose strong assumptions, ignoring much of the spatio-temporal dynamics that
occur during reading. In this paper, we propose a more general probabilistic
model of reading behavior, based on a marked spatio-temporal point process,
that captures not only how long fixations last, but also where they land in
space and when they take place in time. The saccades are modeled using a Hawkes
process, which captures how each fixation excites the probability of a new
fixation occurring near it in time and space. The duration time of fixation
events is modeled as a function of fixation-specific predictors convolved
across time, thus capturing spillover effects. Empirically, our Hawkes process
model exhibits a better fit to human saccades than baselines. With respect to
fixation durations, we observe that incorporating contextual surprisal as a
predictor results in only a marginal improvement in the model's predictive
accuracy. This finding suggests that surprisal theory struggles to explain
fine-grained eye movements.

</details>


### [289] [Causal-Aware Intelligent QoE Optimization for VR Interaction with Adaptive Keyframe Extraction](https://arxiv.org/abs/2506.19890)
*Ziru Zhang,Jiadong Yu,Danny H. K. Tsang*

Main category: cs.LG

TL;DR: 该论文提出一种基于因果感知强化学习的智能框架，优化多用户VR交互的QoE，通过整合自适应关键帧提取技术和因果信息，提高训练效率与互动性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分考虑带宽分配、CPU频率和用户感知的因果关系，限制了QoE的提升，因此需要更智能的优化框架。

Method: 结合韦伯-费希纳定律定义新的QoE指标，将优化问题模型化为一个混合整数规划任务，使用因果感知深度确定性策略梯度(PS-CDDPG)算法通过因果推理提升决策效率。

Result: 通过实验验证，该框架显著降低了互动延迟，提升QoE并保证公平性，在评估中表现优于基线方法。

Conclusion: 整合因果信息指导决策的智能框架可显著提升多用户VR交互的QoE，证明其为优化资源分配与提高用户体验的有效手段。

Abstract: The optimization of quality of experience (QoE) in multi-user virtual reality
(VR) interactions demands a delicate balance between ultra-low latency,
high-fidelity motion synchronization, and equitable resource allocation. While
adaptive keyframe extraction mitigates transmission overhead, existing
approaches often overlook the causal relationships among allocated bandwidth,
CPU frequency, and user perception, limiting QoE gains. This paper proposes an
intelligent framework to maximize QoE by integrating adaptive keyframe
extraction with causal-aware reinforcement learning (RL). First, a novel QoE
metric is formulated using the Weber-Fechner Law, combining perceptual
sensitivity, attention-driven priorities, and motion reconstruction accuracy.
The QoE optimization problem is then modeled as a mixed integer programming
(MIP) task, jointly optimizing keyframe ratios, bandwidth, and computational
resources under horizon-fairness constraints. We propose Partial State Causal
Deep Deterministic Policy Gradient (PS-CDDPG), which integrates the Deep
Deterministic Policy Gradient (DDPG) method with causal influence detection. By
leveraging causal information regarding how QoE is influenced and determined by
various actions, we explore actions guided by weights calculated from causal
inference (CI), which in turn improves training efficiency. Experiments
conducted with the CMU Motion Capture Database demonstrate that our framework
significantly reduces interactive latency, enhances QoE, and maintains
fairness, achieving superior performance compared to benchmark methods.

</details>


### [290] [Orthogonal Soft Pruning for Efficient Class Unlearning](https://arxiv.org/abs/2506.19891)
*Qinghui Gong,Xue Yang,Xiaohu Tang*

Main category: cs.LG

TL;DR: 提出了一种基于软剪枝和正交卷积核正则化的新方法，实现快速准确地移除预训练模型中与特定类别相关的知识。


<details>
  <summary>Details</summary>
Motivation: 现有方法在快速“遗忘”和预测精度保持之间难以平衡。提出的方法旨在高效、安全地满足数据隐私法规要求。

Method: 通过正交卷积核正则化和激活差异分析实现类特定通道的高效识别，并应用软剪枝快速移除特定类别知识。

Result: 实验表明，方法在不同架构和数据集上稳定执行，能瞬时忘记目标类别且对保留数据精度损失最小，显著降低隐私攻击风险。

Conclusion: 该方法为实现实时机器遗忘提供了一种高效实用的解决方案，尤其在MLaaS情境下具有重要应用价值。

Abstract: Machine unlearning aims to selectively remove class-specific knowledge from
pretrained neural networks to satisfy privacy regulations such as the GDPR.
Existing methods typically face a trade-off between unlearning speed and
preservation of predictive accuracy, often incurring either high computational
overhead or significant performance degradation on retained classes. In this
paper, we propose a novel class-aware soft pruning framework leveraging
orthogonal convolutional kernel regularization to achieve rapid and precise
forgetting with millisecond-level response times. By enforcing orthogonality
constraints during training, our method decorrelates convolutional filters and
disentangles feature representations, while efficiently identifying
class-specific channels through activation difference analysis. Extensive
evaluations across multiple architectures and datasets demonstrate stable
pruning with near-instant execution, complete forgetting of targeted classes,
and minimal accuracy loss on retained data. Experiments on CIFAR-10, CIFAR-100,
and TinyImageNet confirm that our approach substantially reduces membership
inference attack risks and accelerates unlearning by orders of magnitude
compared to state-of-the-art baselines. This framework provides an efficient,
practical solution for real-time machine unlearning in Machine Learning as a
Service (MLaaS) scenarios.

</details>


### [291] [Distillation-Enabled Knowledge Alignment for Generative Semantic Communications in AIGC Provisioning Tasks](https://arxiv.org/abs/2506.19893)
*Jingzhi Hu,Geoffrey Ye Li*

Main category: cs.LG

TL;DR: 本研究针对人工智能生成内容的传输提出了解决方案，通过生成语义通信减少传输负载，并设计了一种知识对齐算法DeKA-g。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容的爆发式增长，从云端向边缘和移动用户提供此类内容对网络带来了巨大流量负担。解决这一问题需要减少传输的数据量，同时解决知识对齐的挑战。

Method: 提出了一种名为DeKA-g的知识对齐算法，包含两个新方法：元词辅助知识蒸馏（MAKD）和可变速率分组信噪比调节（VGSA）。MAKD通过优化元词提高知识蒸馏效率；VGSA则适应了不同压缩率和信噪比范围下的无线传输需求。

Result: 实验表明，DeKA-g算法使边缘生成图像与云端生成图像的对齐精度提升了44%，在压缩率适应性上较基线提高116%，低信噪比下性能提升28%。

Conclusion: 通过提出的DeKA-g方法，本研究有效地加强了生成语义通信中知识的对齐与适应性能，为未来边缘计算与AI生成内容的结合提供了优化路径。

Abstract: Due to the surging amount of AI-generated content (AIGC), its provisioning to
edges and mobile users from the cloud incurs substantial traffic on networks.
Generative semantic communication (GSC) offers a promising solution by
transmitting highly compact information, i.e., prompt text and latent
representations, instead of high-dimensional AIGC data. However, GSC relies on
the alignment between the knowledge in the cloud generative AI (GAI) and that
possessed by the edges and users, and between the knowledge for wireless
transmission and that of actual channels, which remains challenging. In this
paper, we propose DeKA-g, a distillation-enabled knowledge alignment algorithm
for GSC systems. The core idea is to distill the generation knowledge from the
cloud-GAI into low-rank matrices, which can be incorporated by the edge and
used to adapt the transmission knowledge to diverse wireless channel
conditions. DeKA-g comprises two novel methods: metaword-aided knowledge
distillation (MAKD) and variable-rate grouped SNR adaptation (VGSA). For MAKD,
an optimized metaword is employed to enhance the efficiency of knowledge
distillation, while VGSA enables efficient adaptation to diverse compression
rates and SNR ranges. From simulation results, DeKA-g improves the alignment
between the edge-generated images and the cloud-generated ones by 44%.
Moreover, it adapts to compression rates with 116% higher efficiency than the
baseline and enhances the performance in low-SNR conditions by 28%.

</details>


### [292] [Explaining deep neural network models for electricity price forecasting with XAI](https://arxiv.org/abs/2506.19894)
*Antoine Pesenti,Aidan OSullivan*

Main category: cs.LG

TL;DR: 本文使用深度神经网络（DNN）预测电力市场价格，并通过解释性人工智能（XAI）方法分析影响价格变化的因素。


<details>
  <summary>Details</summary>
Motivation: 当前电力市场的复杂性使得理解其动态变化和价格驱动原因十分困难，而 econometric 方法无法匹配 DNN 的强大性能。

Method: 采用深度神经网络进行价格预测，并结合 SHAP、Gradient 和热图等解释性方法，分析五个电力市场中各种特征的行为和贡献。此外，引入了创新性 SSHAP 值和 SSHAP 线的概念来提升模型解释性。

Result: 通过对五个电力市场的研究，提出了一种能更清晰地展示高维表格模型复杂关系的分析框架。

Conclusion: 深度学习与解释性方法的结合在电力市场价格预测和解释能力上表现出色，显著提高了对不同电力市场动态的理解。

Abstract: Electricity markets are highly complex, involving lots of interactions and
complex dependencies that make it hard to understand the inner workings of the
market and what is driving prices. Econometric methods have been developed for
this, white-box models, however, they are not as powerful as deep neural
network models (DNN). In this paper, we use a DNN to forecast the price and
then use XAI methods to understand the factors driving the price dynamics in
the market. The objective is to increase our understanding of how different
electricity markets work. To do that, we apply explainable methods such as SHAP
and Gradient, combined with visual techniques like heatmaps (saliency maps) to
analyse the behaviour and contributions of various features across five
electricity markets. We introduce the novel concepts of SSHAP values and SSHAP
lines to enhance the complex representation of high-dimensional tabular models.

</details>


### [293] [A Framework for Uncertainty Quantification Based on Nearest Neighbors Across Layers](https://arxiv.org/abs/2506.19895)
*Miguel N. Font,José L. Jorro-Aragoneses,Carlos M. Alaíz*

Main category: cs.LG

TL;DR: 提出了一种基于训练样例的后验框架来测量神经网络决策的不确定性，并通过引入两种新的度量方法改进了估计精度。


<details>
  <summary>Details</summary>
Motivation: 神经网络虽然在许多问题上表现出高度准确性，但其错误率在高风险领域如医疗诊断或自动驾驶中是关键问题。因此，作者希望通过改善对神经网络决策不确定性的测量来减少这些风险。

Method: 该研究通过提出一种新颖的后处理框架，基于检索到的训练样例（根据每层的激活向量相似性），引入两种新的度量方法：决策变化(Decision Change)和层不确定性(Layer Uncertainty)，衡量各层最近邻类分布的变化。

Result: 在CIFAR-10和MNIST两个数据集上的分类模型测试中，新方法在提升不确定性估计尤其是复杂分类任务中的表现优于基于softmax置信度的传统方法。

Conclusion: 新提出的框架和度量方法有效增强了神经网络决策不确定性的估计能力，有助于更好地应用于高风险领域。

Abstract: Neural Networks have high accuracy in solving problems where it is difficult
to detect patterns or create a logical model. However, these algorithms
sometimes return wrong solutions, which become problematic in high-risk domains
like medical diagnosis or autonomous driving. One strategy to detect and
mitigate these errors is the measurement of the uncertainty over neural network
decisions. In this paper, we present a novel post-hoc framework for measuring
the uncertainty of a decision based on retrieved training cases that have a
similar activation vector to the query for each layer. Based on these retrieved
cases, we propose two new metrics: Decision Change and Layer Uncertainty, which
capture changes in nearest-neighbor class distributions across layers. We
evaluated our approach in a classification model for two datasets: CIFAR-10 and
MNIST. The results show that these metrics enhance uncertainty estimation,
especially in challenging classification tasks, outperforming softmax-based
confidence.

</details>


### [294] [MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations](https://arxiv.org/abs/2506.20100)
*Vardhan Dongre,Chi Gui,Shubham Garg,Hooshang Nayyeri,Gokhan Tur,Dilek Hakkani-Tür,Vikram S. Adve*

Main category: cs.LG

TL;DR: 介绍了MIRAGE，一个设计用于农业领域的多模态专家级推理与决策的新基准，特点是整合了自然用户查询、专家授权回答以及图像背景。


<details>
  <summary>Details</summary>
Motivation: 通过提供更真实的基准来填补现有多模态推理和长文生成基准在处理不明确的用户输入和开放领域知识领域的空白。

Method: 基准数据集基于35000次用户-专家互动，经过精心设计的多步骤管道构建。数据涵盖7000+生物实体，涉及疾病、害虫和植物物种，以多样性和真实性为核心。

Result: 创建了涵盖广泛生物多样性和应用扩展场景的MIRAGE基准，强调模型对潜在知识的推断能力和互动引导策略。

Conclusion: MIRAGE为复杂、多模态推理、长文本生成和决策提供了更真实的开放世界评估标准，可推动知识密集型领域技术发展。

Abstract: We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning
and decision-making in consultative interaction settings. Designed for the
agriculture domain, MIRAGE captures the full complexity of expert consultations
by combining natural user queries, expert-authored responses, and image-based
context, offering a high-fidelity benchmark for evaluating models on grounded
reasoning, clarification strategies, and long-form generation in a real-world,
knowledge-intensive domain. Grounded in over 35,000 real user-expert
interactions and curated through a carefully designed multi-step pipeline,
MIRAGE spans diverse crop health, pest diagnosis, and crop management
scenarios. The benchmark includes more than 7,000 unique biological entities,
covering plant species, pests, and diseases, making it one of the most
taxonomically diverse benchmarks available for vision-language models, grounded
in the real world. Unlike existing benchmarks that rely on well-specified user
inputs and closed-set taxonomies, MIRAGE features underspecified, context-rich
scenarios with open-world settings, requiring models to infer latent knowledge
gaps, handle rare entities, and either proactively guide the interaction or
respond. Project Page: https://mirage-benchmark.github.io

</details>


### [295] [A Comparative Analysis of Reinforcement Learning and Conventional Deep Learning Approaches for Bearing Fault Diagnosis](https://arxiv.org/abs/2506.19929)
*Efe Çakır,Patrick Dumond*

Main category: cs.LG

TL;DR: 研究探索了强化学习在轴承故障分类中的应用，特别是深度Q网络（DQN）。结果表明在适配性方面优于传统方法，但计算需求较高。


<details>
  <summary>Details</summary>
Motivation: 现代轴承故障诊断方法通常依赖振动分析和机器学习，但需要大量标记数据且不适应动态环境。

Method: 采用强化学习中的深度Q网络（DQN）方法进行轴承故障分类任务，优化奖励结构以提升适应性。

Result: RL模型在控制环境下匹配传统监督学习模型性能，且适应性更强，但计算需求仍是瓶颈。

Conclusion: 强化学习有潜力补充传统方法，为构建适应性强的诊断框架开辟新道路。

Abstract: Bearing faults in rotating machinery can lead to significant operational
disruptions and maintenance costs. Modern methods for bearing fault diagnosis
rely heavily on vibration analysis and machine learning techniques, which often
require extensive labeled data and may not adapt well to dynamic environments.
This study explores the feasibility of reinforcement learning (RL),
specifically Deep Q-Networks (DQNs), for bearing fault classification tasks in
machine condition monitoring to enhance the accuracy and adaptability of
bearing fault diagnosis. The results demonstrate that while RL models developed
in this study can match the performance of traditional supervised learning
models under controlled conditions, they excel in adaptability when equipped
with optimized reward structures. However, their computational demands
highlight areas for further improvement. These findings demonstrate RL's
potential to complement traditional methods, paving the way for adaptive
diagnostic frameworks.

</details>


### [296] [Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and Architecture](https://arxiv.org/abs/2506.19935)
*Shuchen Xue,Tianyu Xie,Tianyang Hu,Zijin Feng,Jiacheng Sun,Kenji Kawaguchi,Zhenguo Li,Zhi-Ming Ma*

Main category: cs.LG

TL;DR: 本文探讨大型语言模型(LLMs)中自回归(AR)方法与掩码扩散模型(MDMs)的比较，并提出在相同架构下公平评估两者的方法。


<details>
  <summary>Details</summary>
Motivation: 解决AR和MDM模型对比中架构不同带来的混淆，提出一种在decoder-only框架下比较两种方法的新方式。

Method: 将MDM置于decoder-only框架中，公平比较Any-Order AR (AO-AR) 与标准AR；同时探讨不同架构（decoder-only 与 encoder-only）对MDM的影响。

Result: 研究显示，标准AO-AR目标函数需要改进；decoder-only MDM相比encoder-only MDM具有约25倍生成速度提升，同时在温度退火的情况下实现相当的困惑度。

Conclusion: 通过将建模范式与架构影响解耦，该研究为未来模型的设计提供了指导。

Abstract: Large language models (LLMs) predominantly use autoregressive (AR)
approaches, but masked diffusion models (MDMs) are emerging as viable
alternatives. A key challenge in comparing AR and MDM paradigms is their
typical architectural difference: AR models are often decoder-only, while MDMs
have largely been encoder-only. This practice of changing both the modeling
paradigm and architecture simultaneously makes direct comparisons unfair, as
it's hard to distinguish whether observed differences stem from the paradigm
itself or the architectural shift. This research evaluates MDMs within a
decoder-only framework to: (1) equitably compare MDM (as Any-Order AR, or
AO-AR) and standard AR paradigms. Our investigation suggests that the standard
AO-AR objective, which averages over all token permutations, may benefit from
refinement, as many permutations appear less informative compared to the
language's inherent left-to-right structure. (2) Investigate architectural
influences (decoder-only vs. encoder-only) within MDMs. We demonstrate that
while encoder-only MDMs model a simpler conditional probability space,
decoder-only MDMs can achieve dramatic generation speedups ($\sim25\times$) and
comparable perplexity with temperature annealing despite modeling a vastly
larger space, highlighting key trade-offs. This work thus decouples core
paradigm differences from architectural influences, offering insights for
future model design. Code is available at https://github.com/scxue/AO-GPT-MDM.

</details>


### [297] [The Most Important Features in Generalized Additive Models Might Be Groups of Features](https://arxiv.org/abs/2506.19937)
*Tomas M. Bosschieter,Luis Franca,Jessica Wolk,Yiyuan Wu,Bella Mehta,Joseph Dehoney,Orsolya Kiss,Fiona C. Baker,Qingyu Zhao,Rich Caruana,Kilian M. Pohl*

Main category: cs.LG

TL;DR: 此论文提出一种评估广义可加模型（GAMs）中特征组重要性的新方法，特别关注特征组协同信号对分析准确性的关键性贡献。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习特征重要性分析通常忽视了特征组协同效应，而这种忽视可能导致关键洞察的丢失，特别是在多模态数据中更为明显。

Method: 提出了一种无需重新训练模型、可后续定义甚至允许重叠特征组的高效特征组重要性评估方法，并且这一评估方法基于统计学中的解释方差理论。

Result: 通过三个合成实验验证在不同数据场景下此方法性能，并在两个医疗相关的多模态数据案例中展示特征组重要性分析对疾病预测的优势。

Conclusion: 特征组重要性分析能够比单特征分析提供更准确、更整体的医疗问题理解，提升研究解释力。

Abstract: While analyzing the importance of features has become ubiquitous in
interpretable machine learning, the joint signal from a group of related
features is sometimes overlooked or inadvertently excluded. Neglecting the
joint signal could bypass a critical insight: in many instances, the most
significant predictors are not isolated features, but rather the combined
effect of groups of features. This can be especially problematic for datasets
that contain natural groupings of features, including multimodal datasets. This
paper introduces a novel approach to determine the importance of a group of
features for Generalized Additive Models (GAMs) that is efficient, requires no
model retraining, allows defining groups posthoc, permits overlapping groups,
and remains meaningful in high-dimensional settings. Moreover, this definition
offers a parallel with explained variation in statistics. We showcase
properties of our method on three synthetic experiments that illustrate the
behavior of group importance across various data regimes. We then demonstrate
the importance of groups of features in identifying depressive symptoms from a
multimodal neuroscience dataset, and study the importance of social
determinants of health after total hip arthroplasty. These two case studies
reveal that analyzing group importance offers a more accurate, holistic view of
the medical issues compared to a single-feature analysis.

</details>


### [298] [HERCULES: Hierarchical Embedding-based Recursive Clustering Using LLMs for Efficient Summarization](https://arxiv.org/abs/2506.19992)
*Gabor Petnehazi,Bernadett Aradi*

Main category: cs.LG

TL;DR: 本文介绍了一种名为HERCULES的新算法，可用于对文本、图像和数值数据等多种复杂数据进行高效的层次化k均值聚类，同时利用大语言模型（LLMs）生成易于理解的语义描述。


<details>
  <summary>Details</summary>
Motivation: 面对多模态复杂数据集的爆炸式增长，研究需要一种既能有效分组数据又能提供可解释洞察的高级分析工具。

Method: HERCULES通过递归应用k均值聚类，构建从数据点到层级的聚类结构，并深度集成LLMs以生成每层的语义标题和描述；支持两种表示模式：基于原始数据嵌入或LLM生成摘要后的嵌入。

Result: HERCULES展示出在从复杂数据中提取有意义的层次化知识方面的能力，并提供交互式工具以便直观分析聚类结果。

Conclusion: HERCULES算法能够有效处理多种类型数据，增强了聚类结果的解释性，为分析复杂数据提供了新颖且实用的方法。

Abstract: The explosive growth of complex datasets across various modalities
necessitates advanced analytical tools that not only group data effectively but
also provide human-understandable insights into the discovered structures. We
introduce HERCULES (Hierarchical Embedding-based Recursive Clustering Using
LLMs for Efficient Summarization), a novel algorithm and Python package
designed for hierarchical k-means clustering of diverse data types, including
text, images, and numeric data (processed one modality per run). HERCULES
constructs a cluster hierarchy by recursively applying k-means clustering,
starting from individual data points at level 0. A key innovation is its deep
integration of Large Language Models (LLMs) to generate semantically rich
titles and descriptions for clusters at each level of the hierarchy,
significantly enhancing interpretability. The algorithm supports two main
representation modes: `direct' mode, which clusters based on original data
embeddings or scaled numeric features, and `description' mode, which clusters
based on embeddings derived from LLM-generated summaries. Users can provide a
`topic\_seed' to guide LLM-generated summaries towards specific themes. An
interactive visualization tool facilitates thorough analysis and understanding
of the clustering results. We demonstrate HERCULES's capabilities and discuss
its potential for extracting meaningful, hierarchical knowledge from complex
datasets.

</details>


### [299] [TRACED: Transition-aware Regret Approximation with Co-learnability for Environment Design](https://arxiv.org/abs/2506.19997)
*Geonwoo Cho,Jaegyun Im,Jihwan Lee,Hojun Yi,Sejin Kim,Sundong Kim*

Main category: cs.LG

TL;DR: 提出了一种名为TRACED的框架，通过改进遗憾值近似和建模任务关系实现更高效的无监管环境设计，显著提升零样本泛化能力且减少交互次数。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习方法在应对未知环境时存在显著挑战，而无监管环境设计（UED）作为一种潜在解决方案还存在不足，需要改进遗憾值近似并关注任务关系。

Method: 引入转移预测误差作为遗憾值近似的额外项，并提出了轻量级的协同学习度量（co-learnability），结合这两种度量提出TRACED方法，用于生成高效的环境设计课程。

Result: TRACED在多个基准测试中提升了零样本泛化表现，同时所需的环境交互次数最多减少50%。

Conclusion: 改进的遗憾值近似结合任务关系建模能提高无监管环境设计的效率和效果，为泛化性和样本效率提供了新视角。

Abstract: Generalizing deep reinforcement learning agents to unseen environments
remains a significant challenge. One promising solution is Unsupervised
Environment Design (UED), a co-evolutionary framework in which a teacher
adaptively generates tasks with high learning potential, while a student learns
a robust policy from this evolving curriculum. Existing UED methods typically
measure learning potential via regret, the gap between optimal and current
performance, approximated solely by value-function loss. Building on these
approaches, we introduce the transition prediction error as an additional term
in our regret approximation. To capture how training on one task affects
performance on others, we further propose a lightweight metric called
co-learnability. By combining these two measures, we present Transition-aware
Regret Approximation with Co-learnability for Environment Design (TRACED).
Empirical evaluations show that TRACED yields curricula that improve zero-shot
generalization across multiple benchmarks while requiring up to 2x fewer
environment interactions than strong baselines. Ablation studies confirm that
the transition prediction error drives rapid complexity ramp-up and that
co-learnability delivers additional gains when paired with the transition
prediction error. These results demonstrate how refined regret approximation
and explicit modeling of task relationships can be leveraged for
sample-efficient curriculum design in UED.

</details>


### [300] [New Insights on Unfolding and Fine-tuning Quantum Federated Learning](https://arxiv.org/abs/2506.20016)
*Shanika Iroshi Nanayakkara,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: 提出了一种基于深度展开的量子联邦学习方法，显著提高了在异构环境下的训练性能，具有重要应用价值。


<details>
  <summary>Details</summary>
Motivation: 解决传统量子联邦学习在客户异质性下性能受限的问题，目标是自适应优化超参数以提高训练精度。

Method: 采用深度展开方法，使客户端根据自身训练行为动态调整学习率和正则化系数，从而实现自适应优化。

Result: 在IBM量子硬件和Qiskit Aer模拟器上实时训练，准确率达90%，远高于传统方法的55%。

Conclusion: 该方法通过收敛感知的优化步骤，提升了一般化能力，特别适用于基因表达分析与癌症检测等关键领域。

Abstract: Client heterogeneity poses significant challenges to the performance of
Quantum Federated Learning (QFL). To overcome these limitations, we propose a
new approach leveraging deep unfolding, which enables clients to autonomously
optimize hyperparameters, such as learning rates and regularization factors,
based on their specific training behavior. This dynamic adaptation mitigates
overfitting and ensures robust optimization in highly heterogeneous
environments where standard aggregation methods often fail. Our framework
achieves approximately 90% accuracy, significantly outperforming traditional
methods, which typically yield around 55% accuracy, as demonstrated through
real-time training on IBM quantum hardware and Qiskit Aer simulators. By
developing self adaptive fine tuning, the proposed method proves particularly
effective in critical applications such as gene expression analysis and cancer
detection, enhancing diagnostic precision and predictive modeling within
quantum systems. Our results are attributed to convergence-aware, learnable
optimization steps intrinsic to the deep unfolded framework, which maintains
the generalization. Hence, this study addresses the core limitations of
conventional QFL, streamlining its applicability to any complex challenges such
as healthcare and genomic research.

</details>


### [301] [DIM-SUM: Dynamic IMputation for Smart Utility Management](https://arxiv.org/abs/2506.20023)
*Ryan Hildebrant,Rahul Bhope,Sharad Mehrotra,Christopher Tull,Nalini Venkatasubramanian*

Main category: cs.LG

TL;DR: DIM-SUM是一种处理时间序列缺失数据的预处理框架，能在处理真实世界复杂缺失模式下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决传统插补模型在现实复杂多样缺失模式下表现不佳的问题。

Method: 采用模式聚类与自适应掩码策略，结合理论学习保证；从而缩小人工掩码训练数据与真实缺失模式间的差距。

Result: 在包括加州水资源和电力数据在内的20亿条数据上实验，显示与传统方法相比，可以更少的训练数据、更高的精度和更低的推理时间。

Conclusion: DIM-SUM显著改进了时间序列插补的效率和准确性，尤其适用于现实中的复杂缺失数据模式。

Abstract: Time series imputation models have traditionally been developed using
complete datasets with artificial masking patterns to simulate missing values.
However, in real-world infrastructure monitoring, practitioners often encounter
datasets where large amounts of data are missing and follow complex,
heterogeneous patterns. We introduce DIM-SUM, a preprocessing framework for
training robust imputation models that bridges the gap between artificially
masked training data and real missing patterns. DIM-SUM combines pattern
clustering and adaptive masking strategies with theoretical learning guarantees
to handle diverse missing patterns actually observed in the data. Through
extensive experiments on over 2 billion readings from California water
districts, electricity datasets, and benchmarks, we demonstrate that DIM-SUM
outperforms traditional methods by reaching similar accuracy with lower
processing time and significantly less training data. When compared against a
large pre-trained model, DIM-SUM averages 2x higher accuracy with significantly
less inference time.

</details>


### [302] [Elucidated Rolling Diffusion Models for Probabilistic Weather Forecasting](https://arxiv.org/abs/2506.20024)
*Salva Rühling Cachay,Miika Aittala,Karsten Kreis,Noah Brenowitz,Arash Vahdat,Morteza Mardani,Rose Yu*

Main category: cs.LG

TL;DR: 提出了一种名为ERDM（Elucidated Rolling Diffusion Models）的方法，用于结合滚动预测框架和高性能扩散模型（EDM）的优势。


<details>
  <summary>Details</summary>
Motivation: 当前高维混沌系统中的扩散模型难以建模复杂时间依赖性，并且忽略了不确定性随时间增长的问题。作者希望通过整合滚动扩散结构与高性能EDM，解决这一挑战。

Method: ERDM框架通过改进EDM的三个核心组件（噪声调度、网络预处理和Heun采样器），实现了滚动预测：1）提出新形式的损失权重机制；2）使用预训练EDM进行高效初始化；3）设计混合序列架构以应对时空特征提取和去噪问题。

Result: 在二维Navier-Stokes模拟和ERA5全球天气预测的测试中，ERDM的表现优于主要的基准方法，包括条件自回归EDM。

Conclusion: ERDM是一种灵活且通用的框架，适用于处理需要建模不确定性递增问题的扩散序列生成任务，并成功整合了先进的扩散技术与滚动预测。

Abstract: Diffusion models are a powerful tool for probabilistic forecasting, yet most
applications in high-dimensional chaotic systems predict future snapshots
one-by-one. This common approach struggles to model complex temporal
dependencies and fails to explicitly account for the progressive growth of
uncertainty inherent to such systems. While rolling diffusion frameworks, which
apply increasing noise to forecasts at longer lead times, have been proposed to
address this, their integration with state-of-the-art, high-fidelity diffusion
techniques remains a significant challenge. We tackle this problem by
introducing Elucidated Rolling Diffusion Models (ERDM), the first framework to
successfully unify a rolling forecast structure with the principled, performant
design of Elucidated Diffusion Models (EDM). To do this, we adapt the core EDM
components-its noise schedule, network preconditioning, and Heun sampler-to the
rolling forecast setting. The success of this integration is driven by three
key contributions: (i) a novel loss weighting scheme that focuses model
capacity on the mid-range forecast horizons where determinism gives way to
stochasticity; (ii) an efficient initialization strategy using a pre-trained
EDM for the initial window; and (iii) a bespoke hybrid sequence architecture
for robust spatiotemporal feature extraction under progressive denoising. On 2D
Navier-Stokes simulations and ERA5 global weather forecasting at 1.5^\circ
resolution, ERDM consistently outperforms key diffusion-based baselines,
including conditional autoregressive EDM. ERDM offers a flexible and powerful
general framework for tackling diffusion-based sequence generation problems
where modeling escalating uncertainty is paramount. Code is available at:
https://github.com/salvaRC/erdm

</details>


### [303] [Thumb on the Scale: Optimal Loss Weighting in Last Layer Retraining](https://arxiv.org/abs/2506.20025)
*Nathan Stromberg,Christos Thrampoulidis,Lalitha Sankar*

Main category: cs.LG

TL;DR: 本文探讨了机器学习模型在最后一层重新训练（LLR）中克服数据偏差的能力，指出损失加权在此情况下仍然有效，但需要考虑模型的相对过参程度。


<details>
  <summary>Details</summary>
Motivation: 研究如何在模型较难克服训练数据偏差的场景下，通过理论和实践探索找到有效的解决方法，尤其是在LLR场景中进行优化。

Method: 本文研究最后一层重新训练的场景，分析模型在未见有限数据情况下的行为，提出加入相对过参数的损失加权方法，来确保模型在不同类别间的公平性能。

Result: 理论和实验表明，在LLR场景下，损失加权方式考虑模型的相对过参程度后，可以有效提高模型性能的均衡性。

Conclusion: 本文证明了在特定机器学习模型训练情境中，正确的损失加权方式可以在理论和实践中提升模型的公平性，强调了相对过参程度的作用。

Abstract: While machine learning models become more capable in discriminative tasks at
scale, their ability to overcome biases introduced by training data has come
under increasing scrutiny. Previous results suggest that there are two extremes
of parameterization with very different behaviors: the population
(underparameterized) setting where loss weighting is optimal and the separable
overparameterized setting where loss weighting is ineffective at ensuring equal
performance across classes. This work explores the regime of last layer
retraining (LLR) in which the unseen limited (retraining) data is frequently
inseparable and the model proportionately sized, falling between the two
aforementioned extremes. We show, in theory and practice, that loss weighting
is still effective in this regime, but that these weights \emph{must} take into
account the relative overparameterization of the model.

</details>


### [304] [Automated Generation of Diverse Courses of Actions for Multi-Agent Operations using Binary Optimization and Graph Learning](https://arxiv.org/abs/2506.20031)
*Prithvi Poddar,Ehsan Tarkesh Esfahani,Karthik Dantu,Souma Chowdhury*

Main category: cs.LG

TL;DR: 本文针对多代理的任务分配与规划问题，提出了一种新的理论和计算框架，可生成多样性高且兼容性好的行动方案（COAs）。


<details>
  <summary>Details</summary>
Motivation: 在灾难响应、搜索救援、军事任务等场景中，环境变化和代理能力的不同对行动方案规划提出了挑战，因此需要能够生成多样化、多样任务分配的行动方案池。

Method: 通过图抽象任务空间和行动方案池，引入遗传算法和图神经网络模型，其中遗传算法用于最大化方案池的多样性和任务分配兼容性，图神经网络优化单个代理的任务序列。

Result: 仿真实验表明，与随机基线方法相比，所提框架在性能上有显著提升，同时任务排序的最优差距较小，并能在约50分钟内生成最多20个行动方案。

Conclusion: 该研究提出的方法能有效支持复杂操作任务的多样化行动方案生成，为未来多代理任务规划提供新思路。

Abstract: Operations in disaster response, search \& rescue, and military missions that
involve multiple agents demand automated processes to support the planning of
the courses of action (COA). Moreover, traverse-affecting changes in the
environment (rain, snow, blockades, etc.) may impact the expected performance
of a COA, making it desirable to have a pool of COAs that are diverse in task
distributions across agents. Further, variations in agent capabilities, which
could be human crews and/or autonomous systems, present practical opportunities
and computational challenges to the planning process. This paper presents a new
theoretical formulation and computational framework to generate such diverse
pools of COAs for operations with soft variations in agent-task compatibility.
Key to the problem formulation is a graph abstraction of the task space and the
pool of COAs itself to quantify its diversity. Formulating the COAs as a
centralized multi-robot task allocation problem, a genetic algorithm is used
for (order-ignoring) allocations of tasks to each agent that jointly maximize
diversity within the COA pool and overall compatibility of the agent-task
mappings. A graph neural network is trained using a policy gradient approach to
then perform single agent task sequencing in each COA, which maximizes
completion rates adaptive to task features. Our tests of the COA generation
process in a simulated environment demonstrate significant performance gain
over a random walk baseline, small optimality gap in task sequencing, and
execution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task
operations.

</details>


### [305] [Verifiable Unlearning on Edge](https://arxiv.org/abs/2506.20037)
*Mohammad M Maheri,Alex Davidson,Hamed Haddadi*

Main category: cs.LG

TL;DR: 本研究提出了一种利用零知识证明（特别是zk-SNARKs）来验证边缘设备个性化模型数据移除的框架。


<details>
  <summary>Details</summary>
Motivation: 需要确保分布在边缘设备上的模型能够可靠地移除特定数据样本，同时维持隐私和操作完整性。

Method: 开发了一种基于零知识证明（zk-SNARKs）的算法，用于验证个性化模型的安全数据移除，兼顾计算和存储效率。

Result: 验证框架能够在不明显损害模型个性化性能的情况下，实现数据移除的可验证性和隐私保护。

Conclusion: 所提出的方法为边缘设备提供了一种可行、有效且隐私友好的数据移除机制。

Abstract: Machine learning providers commonly distribute global models to edge devices,
which subsequently personalize these models using local data. However, issues
such as copyright infringements, biases, or regulatory requirements may require
the verifiable removal of certain data samples across all edge devices.
Ensuring that edge devices correctly execute such unlearning operations is
critical to maintaining integrity.
  In this work, we introduce a verification framework leveraging zero-knowledge
proofs, specifically zk-SNARKs, to confirm data unlearning on personalized
edge-device models without compromising privacy. We have developed algorithms
explicitly designed to facilitate unlearning operations that are compatible
with efficient zk-SNARK proof generation, ensuring minimal computational and
memory overhead suitable for constrained edge environments. Furthermore, our
approach carefully preserves personalized enhancements on edge devices,
maintaining model performance post-unlearning.
  Our results affirm the practicality and effectiveness of this verification
framework, demonstrating verifiable unlearning with minimal degradation in
personalization-induced performance improvements. Our methodology ensures
verifiable, privacy-preserving, and effective machine unlearning across edge
devices.

</details>


### [306] [Cross-Layer Discrete Concept Discovery for Interpreting Language Models](https://arxiv.org/abs/2506.20040)
*Ankur Garg,Xuemin Yu,Hassan Sajjad,Samira Ebrahimi Kahou*

Main category: cs.LG

TL;DR: 该论文提出了CLVQVAE框架，以通过向量量化分析跨层的特征，并将重复的残差流特征转化为易于解释的概念向量。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要分析单层的神经表示，而忽略了跨层的特征叠加及其冗余问题。

Method: 提出了一种基于向量量化的框架——CLVQVAE，该框架结合top-k温度采样和EMA码本更新，同时强化了用基于方向相似性而非大小的球面k-means++算法初始化码本。

Result: 通过使用CLVQVAE，将重复的残差流特征压缩为精简且可解释的概念向量，同时维护了码本的多样性。

Conclusion: CLVQVAE能够有效解决跨层特征的冗余，提高表示的紧凑性和解释性。

Abstract: Uncovering emergent concepts across transformer layers remains a significant
challenge because the residual stream linearly mixes and duplicates
information, obscuring how features evolve within large language models.
Current research efforts primarily inspect neural representations at single
layers, thereby overlooking this cross-layer superposition and the redundancy
it introduces. These representations are typically either analyzed directly for
activation patterns or passed to probing classifiers that map them to a limited
set of predefined concepts. To address these limitations, we propose
\gls{clvqvae}, a framework that uses vector quantization to map representations
across layers and in the process collapse duplicated residual-stream features
into compact, interpretable concept vectors. Our approach uniquely combines
top-$k$ temperature-based sampling during quantization with EMA codebook
updates, providing controlled exploration of the discrete latent space while
maintaining code-book diversity. We further enhance the framework with
scaled-spherical k-means++ for codebook initialization, which clusters by
directional similarity rather than magnitude, better aligning with semantic
structure in word embedding space.

</details>


### [307] [LSH-DynED: A Dynamic Ensemble Framework with LSH-Based Undersampling for Evolving Multi-Class Imbalanced Classification](https://arxiv.org/abs/2506.20041)
*Soheil Abadifard,Fazli Can*

Main category: cs.LG

TL;DR: 本文提出了一种名为LSH-DynED的新方法，解决多类别不平衡数据流分类问题。


<details>
  <summary>Details</summary>
Motivation: 在多类不平衡数据流的背景下，目前方法对动态不平衡比例的处理存在不足。

Method: 将局部敏感哈希和随机超平面投影（LSH-RHP）与动态集成多样化框架（DynED）相结合，引入新方法用于欠采样。

Result: 通过23个真实数据集和10个半合成数据集的实验，证明LSH-DynED在Kappa和mG-Mean指标上优于现有方法。

Conclusion: LSH-DynED能够高效处理大规模高维数据中的类别不平衡问题，并在实际应用中展示了其适应性和鲁棒性。

Abstract: The classification of imbalanced data streams, which have unequal class
distributions, is a key difficulty in machine learning, especially when dealing
with multiple classes. While binary imbalanced data stream classification tasks
have received considerable attention, only a few studies have focused on
multi-class imbalanced data streams. Effectively managing the dynamic imbalance
ratio is a key challenge in this domain. This study introduces a novel, robust,
and resilient approach to address these challenges by integrating Locality
Sensitive Hashing with Random Hyperplane Projections (LSH-RHP) into the Dynamic
Ensemble Diversification (DynED) framework. To the best of our knowledge, we
present the first application of LSH-RHP for undersampling in the context of
imbalanced non-stationary data streams. The proposed method undersamples the
majority classes by utilizing LSH-RHP, provides a balanced training set, and
improves the ensemble's prediction performance. We conduct comprehensive
experiments on 23 real-world and ten semi-synthetic datasets and compare
LSH-DynED with 15 state-of-the-art methods. The results reveal that LSH-DynED
outperforms other approaches in terms of both Kappa and mG-Mean effectiveness
measures, demonstrating its capability in dealing with multi-class imbalanced
non-stationary data streams. Notably, LSH-DynED performs well in large-scale,
high-dimensional datasets with considerable class imbalances and demonstrates
adaptation and robustness in real-world circumstances. To motivate our design,
we review existing methods for imbalanced data streams, outline key challenges,
and offer guidance for future work. For the reproducibility of our results, we
have made our implementation available on GitHub.

</details>


### [308] [GNN's Uncertainty Quantification using Self-Distillation](https://arxiv.org/abs/2506.20046)
*Hirad Daneshvar,Reza Samavi*

Main category: cs.LG

TL;DR: 本文提出一种基于知识蒸馏的方法，用于高效且精确地量化图神经网络（GNNs）的预测不确定性，解决传统贝叶斯方法和集成方法计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: GNNs在医疗领域表现出显著性能，但其预测不确定性的量化仍是临床环境中可信性的重要挑战，传统方法要么计算成本高，要么无法充分反映模型多样性。

Method: 提出基于知识蒸馏的自蒸馏方法，利用同一网络作为教师和学生模型，同时设计一种加权不确定性度量指标，捕捉GNN分类器的多样性，无需训练多个独立网络。

Result: 在MIMIC-IV和Enzymes图数据集上的实验表明，该方法能够高效捕捉模型预测不确定性，其性能与MC Dropout和集成方法相当。

Conclusion: 方法有效且高效地量化了GNNs的预测不确定性，为信任度评估提供了新的思路，代码已公开，为未来研究提供支持。

Abstract: Graph Neural Networks (GNNs) have shown remarkable performance in the
healthcare domain. However, what remained challenging is quantifying the
predictive uncertainty of GNNs, which is an important aspect of trustworthiness
in clinical settings. While Bayesian and ensemble methods can be used to
quantify uncertainty, they are computationally expensive. Additionally, the
disagreement metric used by ensemble methods to compute uncertainty cannot
capture the diversity of models in an ensemble network. In this paper, we
propose a novel method, based on knowledge distillation, to quantify GNNs'
uncertainty more efficiently and with higher precision. We apply
self-distillation, where the same network serves as both the teacher and
student models, thereby avoiding the need to train several networks
independently. To ensure the impact of self-distillation, we develop an
uncertainty metric that captures the diverse nature of the network by assigning
different weights to each GNN classifier. We experimentally evaluate the
precision, performance, and ability of our approach in distinguishing
out-of-distribution data on two graph datasets: MIMIC-IV and Enzymes. The
evaluation results demonstrate that the proposed method can effectively capture
the predictive uncertainty of the model while having performance similar to
that of the MC Dropout and ensemble methods. The code is publicly available at
https://github.com/tailabTMU/UQ_GNN.

</details>


### [309] [Universal pre-training by iterated random computation](https://arxiv.org/abs/2506.20057)
*Peter Bloem*

Main category: cs.LG

TL;DR: 研究通过随机生成数据进行模型预训练，理论与实验验证此方法的可行性和优势，并扩展结果到实际数据中。


<details>
  <summary>Details</summary>
Motivation: 探索是否可以使用随机生成的数据对模型进行预训练，并从理论层面验证这一方法的合理性。

Method: 从算法复杂性的角度建立理论分析基础，同时通过实验评估合成数据用于预训练的效果，并扩展到真实数据环境中进行验证。

Result: 实验表明，使用合成数据预训练的模型在多种数据集上表现出零样本上下文学习能力，随着模型规模增加，性能提升；此外，经过预训练的模型在微调时收敛速度更快且泛化性更好。

Conclusion: 随机生成的数据可有效用于模型的预训练，在实际应用中具有提高学习效率和泛化性能的潜力。

Abstract: We investigate the use of randomly generated data for the sake of
pre-training a model. We justify this approach theoretically from the
perspective of algorithmic complexity, building on recent research that shows
that sequence models can be trained to approximate Solomonoff induction. We
derive similar, but complementary theoretical results. We show empirically that
synthetically generated data can be used to pre-train a model before the data
is seen. We replicate earlier results that models trained this way show
zero-shot in-context learning across a variety of datasets, and that this
performance improves with scale. We extend earlier results to real-world data,
and show that finetuning a model after pre-training offers faster convergence
and better generalization.

</details>


### [310] [Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models](https://arxiv.org/abs/2506.20061)
*Zhicheng Zhang,Ziyan Wang,Yali Du,Fei Fang*

Main category: cs.LG

TL;DR: 提出了一种利用大型语言模型（LLMs）生成开放式指令并重新标记强化学习中的失败轨迹的新方法。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在指令执行策略上面临对大量人工标注数据的依赖以及稀疏奖励学习的困难。

Method: 使用LLM自动生成开放式指令，通过识别失败轨迹中的隐含子任务，重新标注这些轨迹以丰富训练数据。

Result: 在Craftax环境中，提出的方法在样本效率、指令覆盖率和总体策略表现上优于最先进的基线方法。

Conclusion: LLM引导的开放式指令重新标注可以有效提升强化学习中的指令执行能力。

Abstract: Developing effective instruction-following policies in reinforcement learning
remains challenging due to the reliance on extensive human-labeled instruction
datasets and the difficulty of learning from sparse rewards. In this paper, we
propose a novel approach that leverages the capabilities of large language
models (LLMs) to automatically generate open-ended instructions retrospectively
from previously collected agent trajectories. Our core idea is to employ LLMs
to relabel unsuccessful trajectories by identifying meaningful subtasks the
agent has implicitly accomplished, thereby enriching the agent's training data
and substantially alleviating reliance on human annotations. Through this
open-ended instruction relabeling, we efficiently learn a unified
instruction-following policy capable of handling diverse tasks within a single
policy. We empirically evaluate our proposed method in the challenging Craftax
environment, demonstrating clear improvements in sample efficiency, instruction
coverage, and overall policy performance compared to state-of-the-art
baselines. Our results highlight the effectiveness of utilizing LLM-guided
open-ended instruction relabeling to enhance instruction-following
reinforcement learning.

</details>


### [311] [Supervised Coupled Matrix-Tensor Factorization (SCMTF) for Computational Phenotyping of Patient Reported Outcomes in Ulcerative Colitis](https://arxiv.org/abs/2506.20065)
*Cristian Minoccheri,Sophia Tesic,Kayvan Najarian,Ryan Stidham*

Main category: cs.LG

TL;DR: 本论文提出了一种新型计算表型方法（SCMTF），将患者报告结果与其他数据结合，预测溃疡性结肠炎患者的药物持续性，并取得高准确度。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用患者报告结果（通常存在噪声和缺失问题）在机器学习和溃疡性结肠炎的表型分析中发挥作用。

Method: 提出了一种监督耦合矩阵-张量分解（SCMTF）方法，结合静态、时间序列的患者报告与实验室数据，基于深度学习框架，提高模型灵活性和应对大规模缺失数据能力。

Result: 模型在预测时间点药物持续性表现优秀（AUC分别为0.853和0.803）。分析得到可解释表型，发现患者报告结果包含有助于预测药物持续性的重要信息。

Conclusion: 证明了基于低秩矩阵-张量方法的表型分析可应用于溃疡性结肠炎和含高缺失数据特征的患者报告结果，挖掘出与药物持续性相关的重要表型。

Abstract: Phenotyping is the process of distinguishing groups of patients to identify
different types of disease progression. A recent trend employs low-rank matrix
and tensor factorization methods for their capability of dealing with
multi-modal, heterogeneous, and missing data. Symptom quantification is crucial
for understanding patient experiences in inflammatory bowel disease, especially
in conditions such as ulcerative colitis (UC). However, patient-reported
symptoms are typically noisy, subjective, and significantly more sparse than
other data types. For this reason, they are usually not included in phenotyping
and other machine learning methods. This paper explores the application of
computational phenotyping to leverage Patient-Reported Outcomes (PROs) using a
novel supervised coupled matrix-tensor factorization (SCMTF) method, which
integrates temporal PROs and temporal labs with static features to predict
medication persistence in ulcerative colitis. This is the first tensor-based
method that is both supervised and coupled, it is the first application to the
UC domain, and the first application to PROs. We use a deep learning framework
that makes the model flexible and easy to train. The proposed method allows us
to handle the large amount of missing data in the PROs. The best model predicts
changes in medication 8 and 20 months in the future with AUCs of 0.853 and
0.803 on the test set respectively. We derive interpretable phenotypes
consisting of static features and temporal features (including their temporal
patterns). We show that low-rank matrix and tensor based phenotyping can be
successfully applied to the UC domain and to highly missing PRO data. We
identify phenotypes useful to predict medication persistence - these phenotypes
include several symptom variables, showing that PROs contain relevant
infromation that is usually discarded.

</details>


### [312] [A Survey of Predictive Maintenance Methods: An Analysis of Prognostics via Classification and Regression](https://arxiv.org/abs/2506.20090)
*Ainaz Jamshidi,Dongchan Kim,Muhammad Arif*

Main category: cs.LG

TL;DR: 本文探讨了预测性维护（PdM）中的回归和分类方法，强调二者比较及其应用，分析了当前挑战和趋势。


<details>
  <summary>Details</summary>
Motivation: 利用机器学习和深度学习提高设备故障预测的准确性，解决现有研究中对回归和分类方法缺少独立比较的问题。

Method: 通过文献综述比较回归方法和分类方法在PdM中的效果，关注数据不平衡、高维特征空间等问题，同时探讨混合方法和AI推动的新趋势。

Result: 发现回归方法提供剩余寿命估计，分类方法提供时间间隔内故障概率；揭示关键挑战与趋势。

Conclusion: 这一综述为研究者和从业者提供了PdM方法的优劣比较，并提出未来研究方向，包括系统性数据集和工具的开发。

Abstract: Predictive maintenance (PdM) has become a crucial element of modern
industrial practice. PdM plays a significant role in operational dependability
and cost management by decreasing unforeseen downtime and optimizing asset life
cycle management. Machine learning and deep learning have enabled more precise
forecasts of equipment failure and remaining useful life (RUL). Although many
studies have been conducted on PdM, there has not yet been a standalone
comparative study between regression- and classification-based approaches. In
this review, we look across a range of PdM methodologies, while focusing more
strongly on the comparative use of classification and regression methods in
prognostics. While regression-based methods typically provide estimates of RUL,
classification-based methods present a forecast of the probability of failure
across defined time intervals. Through a comprehensive analysis of recent
literature, we highlight key advancements, challenges-such as data imbalance
and high-dimensional feature spaces-and emerging trends, including hybrid
approaches and AI-enabled prognostic systems. This review aims to provide
researchers and practitioners with an awareness of the strengths and
compromises of various PdM methods and to help identify future research and
build more robust, directed adaptive maintenance systems. Future work may
include a systematic review of practical aspects such as public datasets,
benchmarking platforms, and open-source tools to support the advancement of PdM
research.

</details>


### [313] [MEL: Multi-level Ensemble Learning for Resource-Constrained Environments](https://arxiv.org/abs/2506.20094)
*Krishna Praneet Gudipaty,Walid A. Hanafy,Kaan Ozkara,Qianlin Liang,Jesse Milzman,Prashant Shenoy,Suhas Diggavi*

Main category: cs.LG

TL;DR: 本研究提出了多层级集成学习（MEL），用以解决边缘计算中的故障韧性问题，同时保障延迟、性能和部署灵活性。


<details>
  <summary>Details</summary>
Motivation: 边缘计算对于低延迟服务至关重要，但其电力、资源的限制性及对故障的敏感性对传统容错方案提出了挑战，这些方案往往牺牲了延迟或精确性。

Method: MEL框架通过多目标优化训练多个轻量化备份模型，这些模型既能协同工作互相优化，也在故障情况下独立运行以保持稳定性能。损失函数的设计保证了模型间的多样性及其单独表现。

Result: 实证研究显示，在视觉、语言和音频数据集下，MEL表现与原始架构相当，同时提供了良好的容错能力。MEL训练的集成模型仅占原始模型40%的大小，并在故障情况下保留了95.6%的准确性。

Conclusion: MEL框架有效解决了边缘推理的韧性挑战，在多场景测试中验证了其性能、容错能力及部署优势。

Abstract: AI inference at the edge is becoming increasingly common for low-latency
services. However, edge environments are power- and resource-constrained, and
susceptible to failures. Conventional failure resilience approaches, such as
cloud failover or compressed backups, often compromise latency or accuracy,
limiting their effectiveness for critical edge inference services. In this
paper, we propose Multi-Level Ensemble Learning (MEL), a new framework for
resilient edge inference that simultaneously trains multiple lightweight backup
models capable of operating collaboratively, refining each other when multiple
servers are available, and independently under failures while maintaining good
accuracy. Specifically, we formulate our approach as a multi-objective
optimization problem with a loss formulation that inherently encourages
diversity among individual models to promote mutually refining representations,
while ensuring each model maintains good standalone performance. Empirical
evaluations across vision, language, and audio datasets show that MEL provides
performance comparable to original architectures while also providing fault
tolerance and deployment flexibility across edge platforms. Our results show
that our ensemble model, sized at 40\% of the original model, achieves similar
performance, while preserving 95.6\% of ensemble accuracy in the case of
failures when trained using MEL.

</details>


### [314] [High-Resolution Live Fuel Moisture Content (LFMC) Maps for Wildfire Risk from Multimodal Earth Observation Data](https://arxiv.org/abs/2506.20132)
*Patrick Alan Johnson,Gabriel Tseng,Yawen Zhang,Heather Heward,Virginia Sjahli,Favyen Bastani,Joseph Redmon,Patrick Beukema*

Main category: cs.LG

TL;DR: AI技术和卫星数据用于生成大规模LFMC地图，提升预测精度和响应能力。


<details>
  <summary>Details</summary>
Motivation: 现有LFMC数据采集成本高且频率低，阻碍了火灾监测和研究。

Method: 使用预训练的多模态地球观测模型，生成空间完整的LFMC地图，并构建自动化流水线。

Result: 相比传统方法误差减少20%，显著提升LFMC预测性能，并验证在野火影响区域的有效性。

Conclusion: 该方法为LFMC地图生成提供高效工具，有助于火灾预防和响应。

Abstract: Wildfires are increasing in intensity and severity at an alarming rate.
Recent advances in AI and publicly available satellite data enable monitoring
critical wildfire risk factors globally, at high resolution and low latency.
Live Fuel Moisture Content (LFMC) is a critical wildfire risk factor and is
valuable for both wildfire research and operational response. However,
ground-based LFMC samples are both labor intensive and costly to acquire,
resulting in sparse and infrequent updates. In this work, we explore the use of
a pretrained, highly-multimodal earth-observation model for generating
large-scale spatially complete (wall-to-wall) LFMC maps. Our approach achieves
significant improvements over previous methods using randomly initialized
models (20 reduction in RMSE). We provide an automated pipeline that enables
rapid generation of these LFMC maps across the United States, and demonstrate
its effectiveness in two regions recently impacted by wildfire (Eaton and
Palisades).

</details>


### [315] [Causal discovery in deterministic discrete LTI-DAE systems](https://arxiv.org/abs/2506.20169)
*Bala Rajesh Konkathi,Arun K. Tangirala*

Main category: cs.LG

TL;DR: 本文提出了一个用于线性时不变微分代数(DAE)系统的因果发现新方法PoV，能有效处理既有动态又有代数关系的复杂系统。


<details>
  <summary>Details</summary>
Motivation: 在动力学和测量噪声交织的复杂系统中，准确识别驱动变量/纯因是重构因果网络的核心挑战之一。

Method: 本文提出了变量划分（PoV）方法，利用动态迭代PCA（DIPCA）方法识别代数和动态关系的约束矩阵，并通过计算条件数进行矩阵分区，最终最小化地识别引起因果关系的变量子集。

Result: 该方法不仅适用于含有代数约束的复杂系统，还能够处理纯动力学系统，相对以往方法适用范围更广。

Conclusion: PoV方法在理论上和实践上都展现出对更广泛复杂系统进行因果发现的潜力。

Abstract: Discovering pure causes or driver variables in deterministic LTI systems is
of vital importance in the data-driven reconstruction of causal networks. A
recent work by Kathari and Tangirala, proposed in 2022, formulated the causal
discovery method as a constraint identification problem. The constraints are
identified using a dynamic iterative PCA (DIPCA)-based approach for dynamical
systems corrupted with Gaussian measurement errors. The DIPCA-based method
works efficiently for dynamical systems devoid of any algebraic relations.
However, several dynamical systems operate under feedback control and/or are
coupled with conservation laws, leading to differential-algebraic (DAE) or
mixed causal systems. In this work, a method, namely the partition of variables
(PoV), for causal discovery in LTI-DAE systems is proposed. This method is
superior to the method that was presented by Kathari and Tangirala (2022), as
PoV also works for pure dynamical systems, which are devoid of algebraic
equations. The proposed method identifies the causal drivers up to a minimal
subset. PoV deploys DIPCA to first determine the number of algebraic relations
($n_a$), the number of dynamical relations ($n_d$) and the constraint matrix.
Subsequently, the subsets are identified through an admissible partitioning of
the constraint matrix by finding the condition number of it. Case studies are
presented to demonstrate the effectiveness of the proposed method.

</details>


### [316] [Causal Operator Discovery in Partial Differential Equations via Counterfactual Physics-Informed Neural Networks](https://arxiv.org/abs/2506.20181)
*Ronald Katende*

Main category: cs.LG

TL;DR: 提出使用物理信息神经网络和反事实扰动发现偏微分方程(PDE)中的因果结构。


<details>
  <summary>Details</summary>
Motivation: 当前处理PDE的因果结构的分析方法（如残差最小化或稀疏回归方法）尚有局限，因此作者希望开发一种新的方法以更好地量化关键算子对动态的必要性。

Method: 利用物理信息神经网络（PINNs）和反事实扰动，通过引入因果敏感性指标和结构偏差度量，评估候选算子的影响，并在约束同构性或相互相干性条件下证明对因果算子的准确检索。

Result: 该方法在气候动力学、肿瘤扩散及海洋流动等领域的合成与实际数据上均表现出优越的性能，即使在噪声、冗余和数据稀缺条件下也能恢复潜在算子，并优于标准PINNs和DeepONets。

Conclusion: 该框架将因果PDE发现转化为基于结构因果模型和变分残差分析的可行且可解释的推理任务。

Abstract: We develop a principled framework for discovering causal structure in partial
differential equations (PDEs) using physics-informed neural networks and
counterfactual perturbations. Unlike classical residual minimization or sparse
regression methods, our approach quantifies operator-level necessity through
functional interventions on the governing dynamics. We introduce causal
sensitivity indices and structural deviation metrics to assess the influence of
candidate differential operators within neural surrogates. Theoretically, we
prove exact recovery of the causal operator support under restricted isometry
or mutual coherence conditions, with residual bounds guaranteeing
identifiability. Empirically, we validate the framework on both synthetic and
real-world datasets across climate dynamics, tumor diffusion, and ocean flows.
Our method consistently recovers governing operators even under noise,
redundancy, and data scarcity, outperforming standard PINNs and DeepONets in
structural fidelity. This work positions causal PDE discovery as a tractable
and interpretable inference task grounded in structural causal models and
variational residual analysis.

</details>


### [317] [DuoGPT: Training-free Dual Sparsity through Activation-aware Pruning in LLMs](https://arxiv.org/abs/2506.20194)
*Ruokai Yin,Yuhang Li,Donghyun Lee,Priyadarshini Panda*

Main category: cs.LG

TL;DR: DuoGPT通过结合非结构化权重修剪和激活稀疏性，提出了一种动态双稀疏工作负载的新框架，优化了大型语言模型的性能和存储效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在高内存和计算成本问题，现有大多修剪方法未能利用运行时的激活稀疏性。

Method: 将激活稀疏性重新定义为动态结构化权重稀疏性，提出DuoGPT框架，通过非结构化权重修剪与激活稀疏性结合，并扩展Optimal Brain Compression框架以加入激活感知校准，同时引入密集模型的输出残差作为修正项。

Result: 在LLaMA-2和LLaMA-3模型上的实验表明，DuoGPT在与基线密集模型一致的加速比下，准确性比最先进的结构化剪枝方法提高了最多9.17%。

Conclusion: DuoGPT框架在保持大型语言模型准确性的同时，大幅优化了其计算需求和执行效率，为大规模模型部署提供了一种有效方法。

Abstract: Large language models (LLMs) deliver strong performance but are difficult to
deploy due to high memory and compute costs. While pruning reduces these
demands, most methods ignore activation sparsity observed at runtime. We
reinterpret activation sparsity as dynamic structured weight sparsity and
propose DuoGPT, a unified framework that constructs dual-sparse (spMspV)
workloads by combining unstructured weight pruning with activation sparsity. To
preserve accuracy, we extend the Optimal Brain Compression (OBC) framework with
activation-aware calibration and introduce output residuals from the dense
model as correction terms. We further optimize the solution for efficient GPU
execution, enabling scalability to billion-parameter LLMs. Evaluations on
LLaMA-2 and LLaMA-3 show that DuoGPT outperforms state-of-the-art structured
pruning methods by up to 9.17% accuracy at an iso-speedup of 1.39$\times$
compared to the baseline dense model.

</details>


### [318] [Zero-Shot Attribution for Large Language Models: A Distribution Testing Approach](https://arxiv.org/abs/2506.20197)
*Clément L. Canonne,Yash Pote,Uddalok Sarkar*

Main category: cs.LG

TL;DR: 提出了一个名为$spAnubis的工具，用于通过分布检验实现对大模型生成代码的归因，实验表明它在区分不同模型时表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究如何将语言模型生成的代码归因，并利用假设检验技术解决这一问题。

Method: 利用语言模型生成的代码样本和密度估计，将归因问题建模为分布检验问题，实现零样本归因。

Result: $spAnubis在DeepSeek-Coder、CodeGemma和Stable-Code模型间的分类实验中表现优异，AUROC得分≥0.9，仅需≈2000个样本。

Conclusion: 证明了$spAnubis有效且精度高，展示了基于分布检验的归因工具的潜力。

Abstract: A growing fraction of all code is sampled from Large Language Models (LLMs).
We investigate the problem of attributing code generated by language models
using hypothesis testing to leverage established techniques and guarantees.
Given a set of samples $S$ and a suspect model $\mathcal{L}^*$, our goal is to
assess the likelihood of $S$ originating from $\mathcal{L}^*$. Due to the curse
of dimensionality, this is intractable when only samples from the LLM are
given: to circumvent this, we use both samples and density estimates from the
LLM, a form of access commonly available.
  We introduce $\mathsf{Anubis}$, a zero-shot attribution tool that frames
attribution as a distribution testing problem. Our experiments on a benchmark
of code samples show that $\mathsf{Anubis}$ achieves high AUROC scores (
$\ge0.9$) when distinguishing between LLMs like DeepSeek-Coder, CodeGemma, and
Stable-Code using only $\approx 2000$ samples.

</details>


### [319] [Affective Priming Score: A Data-Driven Method to Detect Priming in Sequential Datasets](https://arxiv.org/abs/2506.20204)
*Eduardo Gutierrez Maestro,Hadi Banaee,Amy Loutfi*

Main category: cs.LG

TL;DR: 本研究提出了一种称为情感诱发分数（APS）的数据驱动方法，用于检测受情感诱导效应影响的数据点，并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究困难在于如何处理情感计算中的模糊性问题，特别是情感诱导对数据（如生理信号）的影响仍未被充分探索。

Method: 提出了一种情感诱发分数（APS）方法，通过在情感事件间的转换数据中为每个数据点分配一个分数，量化其所受诱导效应的影响，并验证其在SEED和SEED-VII中的性能。

Result: 当使用消除诱导效应的数据相较于原始数据进行模型训练时，错误分类率显著降低。

Conclusion: 此方法在数据层面识别和缓解情感诱导效应，增强了模型的鲁棒性，并为情感计算数据集的设计与收集提供了见解。

Abstract: Affective priming exemplifies the challenge of ambiguity in affective
computing. While the community has largely addressed this issue from a
label-based perspective, identifying data points in the sequence affected by
the priming effect, the impact of priming on data itself, particularly in
physiological signals, remains underexplored. Data affected by priming can lead
to misclassifications when used in learning models. This study proposes the
Affective Priming Score (APS), a data-driven method to detect data points
influenced by the priming effect. The APS assigns a score to each data point,
quantifying the extent to which it is affected by priming. To validate this
method, we apply it to the SEED and SEED-VII datasets, which contain sufficient
transitions between emotional events to exhibit priming effects. We train
models with the same configuration using both the original data and
priming-free sequences. The misclassification rate is significantly reduced
when using priming-free sequences compared to the original data. This work
contributes to the broader challenge of ambiguity by identifying and mitigating
priming effects at the data level, enhancing model robustness, and offering
valuable insights for the design and collection of affective computing
datasets.

</details>


### [320] [Directed Link Prediction using GNN with Local and Global Feature Fusion](https://arxiv.org/abs/2506.20235)
*Yuyang Zhang,Xu Shen,Yu Xie,Ka-Chun Wong,Weidun Xie,Chengbin Peng*

Main category: cs.LG

TL;DR: 该论文提出了一种新颖的图神经网络框架，通过整合特征嵌入与社区信息来改进有向图的链接预测。通过将输入图转换成有向线图，实现更高效的特征利用，其结果在多个基准测试中优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 链接预测是图分析中的经典问题，特别是在实际有向图中的应用较广。然而，现有方法通常仅通过对比学习分析节点相似性，并通过图卷积聚合邻域信息，仍留有改进空间。

Method: 提出一个结合特征嵌入和社区信息的图神经网络框架，并提出将输入有向图转换为有向线图的方法，以增强图卷积阶段的信息聚合能力。

Result: 实验结果显示，与现有最先进方法相比，该方法在使用30%、40%、50%和60%的连通链接作为训练数据时，性能在大多数情况下更优。

Conclusion: 结合特征嵌入与社区信息的混合特征方法，加上有向线图转换策略，有效提升了有向图链接预测的效果。

Abstract: Link prediction is a classical problem in graph analysis with many practical
applications. For directed graphs, recently developed deep learning approaches
typically analyze node similarities through contrastive learning and aggregate
neighborhood information through graph convolutions. In this work, we propose a
novel graph neural network (GNN) framework to fuse feature embedding with
community information. We theoretically demonstrate that such hybrid features
can improve the performance of directed link prediction. To utilize such
features efficiently, we also propose an approach to transform input graphs
into directed line graphs so that nodes in the transformed graph can aggregate
more information during graph convolutions. Experiments on benchmark datasets
show that our approach outperforms the state-of-the-art in most cases when 30%,
40%, 50%, and 60% of the connected links are used as training data,
respectively.

</details>


### [321] [FedBKD: Distilled Federated Learning to Embrace Gerneralization and Personalization on Non-IID Data](https://arxiv.org/abs/2506.20245)
*Yushan Zhao,Jinyuan He,Donglai Chen,Weijie Luo,Chong Xie,Ri Zhang,Yonghong Chen,Yan Xu*

Main category: cs.LG

TL;DR: 提出了一种名为FedBKD的联邦学习框架，通过生成对抗网络和双向知识蒸馏解决非独立同分布（non-IID）数据问题，并在多项实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中的非独立同分布(non-IID)数据问题，并同时实现具有高泛化能力的全局模型和高性能的本地模型。

Method: 采用生成对抗网络(GAN)生成合成数据，使用双向知识蒸馏进行全局和本地模型之间的知识交互，提高两者性能。

Result: 在4个基准数据集的不同non-IID设定下进行了大量实验，结果表明FedBKD在所有情况下均达到SOTA表现。

Conclusion: FedBKD有效应对了non-IID数据问题，并在不依赖公开数据集的情况下提升了全局和本地模型的性能，具有很高的实用性和隐私保障。

Abstract: Federated learning (FL) is a decentralized collaborative machine learning
(ML) technique. It provides a solution to the issues of isolated data islands
and data privacy leakage in industrial ML practices. One major challenge in FL
is handling the non-identical and independent distributed (non-IID) data.
Current solutions either focus on constructing an all-powerful global model, or
customizing personalized local models. Few of them can provide both a
well-generalized global model and well-performed local models at the same time.
Additionally, many FL solutions to the non-IID problem are benefited from
introducing public datasets. However, this will also increase the risk of data
leakage. To tackle the problems, we propose a novel data-free distillation
framework, Federated Bidirectional Knowledge Distillation (FedBKD).
Specifically, we train Generative Adversarial Networks (GAN) for synthetic
data. During the GAN training, local models serve as discriminators and their
parameters are frozen. The synthetic data is then used for bidirectional
distillation between global and local models to achieve knowledge interactions
so that performances for both sides are improved. We conduct extensive
experiments on 4 benchmarks under different non-IID settings. The results show
that FedBKD achieves SOTA performances in every case.

</details>


### [322] [Q-resafe: Assessing Safety Risks and Quantization-aware Safety Patching for Quantized Large Language Models](https://arxiv.org/abs/2506.20251)
*Kejia Chen,Jiawen Zhang,Jiacong Hu,Yu Wang,Jian Lou,Zunlei Feng,Mingli Song*

Main category: cs.LG

TL;DR: 本文聚焦于量化大语言模型在安全性上的潜在问题，并提出解决方案Q-resafe，有效恢复量化模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 量化大语言模型虽然在资源约束环境中具有优势，但可能会对此类模型的安全能力造成影响，因此需要对其安全性进行系统评估并提出解决方案。

Method: 作者提出量化意识安全补丁框架Q-resafe，通过安全基准和多样量化技术，评估并调整量化后模型的安全性。

Result: 实验结果表明，Q-resafe能有效使量化后的模型在安全性上与未量化前的一致，即便在苛刻环境下仍能保持性能。

Conclusion: Q-resafe框架不仅能够保障量化语言模型的安全性，还将不良影响降至最小，为部署量化语言模型提供了强有力的支持。

Abstract: Quantized large language models (LLMs) have gained increasing attention and
significance for enabling deployment in resource-constrained environments.
However, emerging studies on a few calibration dataset-free quantization
methods suggest that quantization may compromise the safety capabilities of
LLMs, underscoring the urgent need for systematic safety evaluations and
effective mitigation strategies. In this paper, we present comprehensive safety
evaluations across various mainstream quantization techniques and diverse
calibration datasets, utilizing widely accepted safety benchmarks. To address
the identified safety vulnerabilities, we propose a quantization-aware safety
patching framework, Q-resafe, to efficiently restore the safety capabilities of
quantized LLMs while minimizing any adverse impact on utility. Extensive
experimental results demonstrate that Q-resafe successfully re-aligns the
safety of quantized LLMs with their pre-quantization counterparts, even under
challenging evaluation scenarios. Project page is available at:
https://github.com/Thecommonirin/Qresafe.

</details>


### [323] [Time-series surrogates from energy consumers generated by machine learning approaches for long-term forecasting scenarios](https://arxiv.org/abs/2506.20253)
*Ben Gerhards,Nikita Popkov,Annekatrin König,Marcel Arpogaus,Bastian Schäfermeier,Leonie Riedl,Stephan Vogt,Philip Hehlert*

Main category: cs.LG

TL;DR: 本文探讨了针对长期能源消费预测的合成数据生成方法，评估了WGAN、DDPM、HMM及MABF等模型的性能，并提出了一种高保真合成数据框架。


<details>
  <summary>Details</summary>
Motivation: 当前大部分研究集中在短期预测以及系统层面，针对个人消费者且长期的预测研究较少，尤其在合成高保真时间序列数据方面存在空白。

Method: 评估了四种数据驱动的先进方法，即WGAN、DDPM、HMM和MABF，分析了它们生成高保真能耗数据的能力，主要关注时间动态、长程依赖及概率转移的再现。

Result: 研究展示了不同模型在生成符合能耗特性数据中的优势与不足，这些数据具备匿名化特性，可避免对单个消费者的特定画像风险。

Conclusion: 此研究为长期能耗预测提供了多种模型比较分析框架，并强调这些生成数据可应用于状态估算和能耗预测等场景。

Abstract: Forecasting attracts a lot of research attention in the electricity value
chain. However, most studies concentrate on short-term forecasting of
generation or consumption with a focus on systems and less on individual
consumers. Even more neglected is the topic of long-term forecasting of
individual power consumption.
  Here, we provide an in-depth comparative evaluation of data-driven methods
for generating synthetic time series data tailored to energy consumption
long-term forecasting. High-fidelity synthetic data is crucial for a wide range
of applications, including state estimations in energy systems or power grid
planning. In this study, we assess and compare the performance of multiple
state-of-the-art but less common techniques: a hybrid Wasserstein Generative
Adversarial Network (WGAN), Denoising Diffusion Probabilistic Model (DDPM),
Hidden Markov Model (HMM), and Masked Autoregressive Bernstein polynomial
normalizing Flows (MABF). We analyze the ability of each method to replicate
the temporal dynamics, long-range dependencies, and probabilistic transitions
characteristic of individual energy consumption profiles. Our comparative
evaluation highlights the strengths and limitations of: WGAN, DDPM, HMM and
MABF aiding in selecting the most suitable approach for state estimations and
other energy-related tasks. Our generation and analysis framework aims to
enhance the accuracy and reliability of synthetic power consumption data while
generating data that fulfills criteria like anonymisation - preserving privacy
concerns mitigating risks of specific profiling of single customers. This study
utilizes an open-source dataset from households in Germany with 15min time
resolution. The generated synthetic power profiles can readily be used in
applications like state estimations or consumption forecasting.

</details>


### [324] [Argumentative Ensembling for Robust Recourse under Model Multiplicity](https://arxiv.org/abs/2506.20260)
*Junqi Jiang,Antonio Rago,Francesco Leofante,Francesca Toni*

Main category: cs.LG

TL;DR: 該研究探討多模型情境下的反事實（CE）解釋，提出了一種基於計算辯論的創新集成方法來保證CE的魯棒性。


<details>
  <summary>Details</summary>
Motivation: 當任務中存在多個預測表現相同的模型時，不同模型對同一輸入的預測可能不同，導致反事實解釋的可靠性下降。需要一種方法來解決此問題。

Method: 提出一種名為"Recourse-aware Ensembling (RAE)"的框架，基於計算辯論，通過表示模型和CE的衝突並利用辯論語義解析衝突來整合模型與反事實解釋。

Result: 通過理論分析和八種方法實例，驗證了該方法在多項期望屬性下的有效性。

Conclusion: 該研究確保了反事實解釋在多模型情境下的魯棒性，並且為用戶提供了靈活定制模型集成的能力。

Abstract: In machine learning, it is common to obtain multiple equally performing
models for the same prediction task, e.g., when training neural networks with
different random seeds. Model multiplicity (MM) is the situation which arises
when these competing models differ in their predictions for the same input, for
which ensembling is often employed to determine an aggregation of the outputs.
Providing recourse recommendations via counterfactual explanations (CEs) under
MM thus becomes complex, since the CE may not be valid across all models, i.e.,
the CEs are not robust under MM. In this work, we formalise the problem of
providing recourse under MM, which we name recourse-aware ensembling (RAE). We
propose the idea that under MM, CEs for each individual model should be
considered alongside their predictions so that the aggregated prediction and
recourse are decided in tandem. Centred around this intuition, we introduce six
desirable properties for solutions to this problem. For solving RAE, we propose
a novel argumentative ensembling method which guarantees the robustness of CEs
under MM. Specifically, our method leverages computational argumentation to
explicitly represent the conflicts between models and counterfactuals regarding
prediction results and CE validity. It then uses argumentation semantics to
resolve the conflicts and obtain the final solution, in a manner which is
parametric to the chosen semantics. Our method also allows for the
specification of preferences over the models under MM, allowing further
customisation of the ensemble. In a comprehensive theoretical analysis, we
characterise the behaviour of argumentative ensembling with four different
argumentation semantics. We then empirically demonstrate the effectiveness of
our approach in satisfying desirable properties with eight instantiations of
our method. (Abstract is shortened for arXiv.)

</details>


### [325] [Distilling A Universal Expert from Clustered Federated Learning](https://arxiv.org/abs/2506.20285)
*Zeqi Leng,Chunxu Zhang,Guodong Long,Riting Xia,Bo Yang*

Main category: cs.LG

TL;DR: 提出了一种新的聚合联邦学习框架，通过从多个簇的知识中提炼通用专家模型来解决非IID数据问题，同时保留簇特异性的信息。


<details>
  <summary>Details</summary>
Motivation: 现有的聚合联邦学习方法忽略了不同簇中共享信息的重要性，本研究希望提出一个方法能结合全局共享知识，同时兼顾个性化和共享知识。

Method: 框架分为三个步骤：1）客户端的本地模型训练；2）簇特异性模型的聚合；3）通用专家知识的提炼，最终以通用专家模型初始化下一轮训练。

Result: 通过广泛实验验证了该方法在各种场景中的优越性能，尤其在非IID数据平衡个性化与共享知识方面的潜力。

Conclusion: 该方法通过模型蒸馏提高了对模型异质性的处理能力，减少了簇特异性专家间冲突，为聚合联邦学习领域带来新进展。

Abstract: Clustered Federated Learning (CFL) addresses the challenges posed by non-IID
data by training multiple group- or cluster-specific expert models. However,
existing methods often overlook the shared information across clusters, which
represents the generalizable knowledge valuable to all participants in the
Federated Learning (FL) system. To overcome this limitation, this paper
introduces a novel FL framework that distills a universal expert model from the
knowledge of multiple clusters. This universal expert captures globally shared
information across all clients and is subsequently distributed to each client
as the initialization for the next round of model training. The proposed FL
framework operates in three iterative steps: (1) local model training at each
client, (2) cluster-specific model aggregation, and (3) universal expert
distillation. This three-step learning paradigm ensures the preservation of
fine-grained non-IID characteristics while effectively incorporating shared
knowledge across clusters. Compared to traditional gradient-based aggregation
methods, the distillation-based model aggregation introduces greater
flexibility in handling model heterogeneity and reduces conflicts among
cluster-specific experts. Extensive experimental results demonstrate the
superior performance of the proposed method across various scenarios,
highlighting its potential to advance the state of CFL by balancing
personalized and shared knowledge more effectively.

</details>


### [326] [Counterfactual Influence as a Distributional Quantity](https://arxiv.org/abs/2506.20481)
*Matthieu Meeus,Igor Shilov,Georgios Kaissis,Yves-Alexandre de Montjoye*

Main category: cs.LG

TL;DR: 本文研究了记忆性与训练样本间的复杂交互关系，提出通过全影响分布分析记忆性，而不仅仅使用自影响指标。


<details>
  <summary>Details</summary>
Motivation: 随着机学习模型在训练数据中记忆样本的现象普遍存在，引发了隐私及泛化性问题。本研究旨在深入探讨记忆现象背后的关键因素。

Method: 通过计算小型语言模型中的全影响分布，分析训练样本彼此之间如何影响记忆性，并进行了图像分类实验(CIFAR-10)以验证结果。

Result: 研究发现，仅依靠自影响指标可能低估记忆性带来的隐患，尤其是在数据集存在近似重复样本时，这些样本的提取性显著增强。

Conclusion: 记忆性源于训练数据间的复杂交互作用，全影响分布相较于自影响指标能更全面捕捉记忆现象。

Abstract: Machine learning models are known to memorize samples from their training
data, raising concerns around privacy and generalization. Counterfactual
self-influence is a popular metric to study memorization, quantifying how the
model's prediction for a sample changes depending on the sample's inclusion in
the training dataset. However, recent work has shown memorization to be
affected by factors beyond self-influence, with other training samples, in
particular (near-)duplicates, having a large impact. We here study memorization
treating counterfactual influence as a distributional quantity, taking into
account how all training samples influence how a sample is memorized. For a
small language model, we compute the full influence distribution of training
samples on each other and analyze its properties. We find that solely looking
at self-influence can severely underestimate tangible risks associated with
memorization: the presence of (near-)duplicates seriously reduces
self-influence, while we find these samples to be (near-)extractable. We
observe similar patterns for image classification, where simply looking at the
influence distributions reveals the presence of near-duplicates in CIFAR-10.
Our findings highlight that memorization stems from complex interactions across
training data and is better captured by the full influence distribution than by
self-influence alone.

</details>


### [327] [Learning Moderately Input-Sensitive Functions: A Case Study in QR Code Decoding](https://arxiv.org/abs/2506.20305)
*Kazuki Yoda,Kazuhiko Kawamoto,Hiroshi Kera*

Main category: cs.LG

TL;DR: 本研究首次提出基于学习的快速响应（QR）码解码，并观察到Transformers能够超越理论的纠错限制进行解码。


<details>
  <summary>Details</summary>
Motivation: 探讨中等敏感程度的学习函数，并首次尝试基于学习的QR码解码。

Method: 使用Transformer模型进行QR码解码，分析其解码性能和对嵌入文字结构的学习能力。

Result: 实验表明，Transformers可以成功解码QR码，甚至超越理论纠错限制。此外，该方法能从英文本训练数据推广到其他语言和随机字符串。

Conclusion: 生成的解码机制与标准QR码阅读器不同，它更关注数据位而忽略纠错位，展现了Transformer的独特解码能力。

Abstract: The hardness of learning a function that attains a target task relates to its
input-sensitivity. For example, image classification tasks are
input-insensitive as minor corruptions should not affect the classification
results, whereas arithmetic and symbolic computation, which have been recently
attracting interest, are highly input-sensitive as each input variable connects
to the computation results. This study presents the first learning-based Quick
Response (QR) code decoding and investigates learning functions of medium
sensitivity. Our experiments reveal that Transformers can successfully decode
QR codes, even beyond the theoretical error-correction limit, by learning the
structure of embedded texts. They generalize from English-rich training data to
other languages and even random strings. Moreover, we observe that the
Transformer-based QR decoder focuses on data bits while ignoring
error-correction bits, suggesting a decoding mechanism distinct from standard
QR code readers.

</details>


### [328] [Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards](https://arxiv.org/abs/2506.20520)
*Charles Arnal,Gaëtan Narozniak,Vivien Cabannes,Yunhao Tang,Julia Kempe,Remi Munos*

Main category: cs.LG

TL;DR: 本文研究了一种介于离线强化学习和监督微调之间的算法，名为off-policy REINFORCE，通过调节基线值V来优化大语言模型（LLMs）。


<details>
  <summary>Details</summary>
Motivation: 通过探索离线强化学习的中间算法，提高大语言模型的对齐能力，同时解决离线方法性能不佳的问题。

Method: 提出了一种简单的off-policy REINFORCE算法，定义优势为A=r-V，并结合理论分析表明基线V的设定对算法性能的影响。此外，进行了理论证明及实验验证。

Result: 实验结果表明该算法在受控随机带臂问题及顶尖大语言模型的推理任务微调中，验证了其优越性与理论分析一致。

Conclusion: 离线强化学习可以通过强调正奖励提高性能，该研究为离线方法和监督微调之间的算法改进提供了新方向。

Abstract: Reinforcement learning (RL) is increasingly used to align large language
models (LLMs). Off-policy methods offer greater implementation simplicity and
data efficiency than on-policy techniques, but often result in suboptimal
performance. In this work, we study the intermediate range of algorithms
between off-policy RL and supervised fine-tuning by analyzing a simple
off-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with
$r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$
emphasizes high-reward samples, while raising it penalizes low-reward ones more
heavily. We first provide a theoretical analysis of this off-policy REINFORCE
algorithm, showing that when the baseline $V$ lower-bounds the expected reward,
the algorithm enjoys a policy improvement guarantee. Our analysis reveals that
while on-policy updates can safely leverage both positive and negative signals,
off-policy updates benefit from focusing more on positive rewards than on
negative ones. We validate our findings experimentally in a controlled
stochastic bandit setting and through fine-tuning state-of-the-art LLMs on
reasoning tasks.

</details>


### [329] [Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration](https://arxiv.org/abs/2506.20307)
*Heyang Zhao,Xingrui Yu,David M. Bossens,Ivor W. Tsang,Quanquan Gu*

Main category: cs.LG

TL;DR: 提出了一种新颖的模仿学习算法 ILDE，通过双重探索机制改进性能，并在少量示范数据下超越了专家表现。


<details>
  <summary>Details</summary>
Motivation: 解决模仿学习中因状态空间复杂性和示范数量有限而导致的困难，并探索超越专家表现的可能性。

Method: 提出 ILDE 算法，通过探索奖励优化不确定性高的状态-动作对，以及好奇心驱动探索偏离示范轨迹的状态。

Result: ILDE 在 Atari 和 MuJoCo 任务上以更少的示范数据超越了现有算法的表现，具有更高的样本效率。

Conclusion: ILDE 算法通过双重探索机制实现了状态-动作空间的优化，提供了理论上的支持，且具有次线性增长的后悔值。

Abstract: Imitation learning is a central problem in reinforcement learning where the
goal is to learn a policy that mimics the expert's behavior. In practice, it is
often challenging to learn the expert policy from a limited number of
demonstrations accurately due to the complexity of the state space. Moreover,
it is essential to explore the environment and collect data to achieve
beyond-expert performance. To overcome these challenges, we propose a novel
imitation learning algorithm called Imitation Learning with Double Exploration
(ILDE), which implements exploration in two aspects: (1) optimistic policy
optimization via an exploration bonus that rewards state-action pairs with high
uncertainty to potentially improve the convergence to the expert policy, and
(2) curiosity-driven exploration of the states that deviate from the
demonstration trajectories to potentially yield beyond-expert performance.
Empirically, we demonstrate that ILDE outperforms the state-of-the-art
imitation learning algorithms in terms of sample efficiency and achieves
beyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations
than in previous work. We also provide a theoretical justification of ILDE as
an uncertainty-regularized policy optimization method with optimistic
exploration, leading to a regret growing sublinearly in the number of episodes.

</details>


### [330] [PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models](https://arxiv.org/abs/2506.20629)
*Soufiane Hayou,Nikhil Ghosh,Bin Yu*

Main category: cs.LG

TL;DR: 提出了一种新方法PLoP，用于自动选择LoRA适配器在模型中的放置位置，提升了低秩适配（LoRA）方法的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA方法在适配器放置策略上存在争议和不足，作者希望通过精确的放置方法提高LoRA的性能。

Method: 提出PLoP方法，通过理论分析自动识别适配器放置位置，无需人工选择模块类型。

Result: 在监督微调和强化学习任务中，PLoP的表现始终优于或至少与现有放置策略相当。

Conclusion: PLoP是一种高效且轻量的方法，可优化LoRA适配器放置策略，提高大模型微调效率。

Abstract: Low-Rank Adaptation (LoRA) is a widely used finetuning method for large
models. Its small memory footprint allows practitioners to adapt large models
to specific tasks at a fraction of the cost of full finetuning. Different
modifications have been proposed to enhance its efficiency by, for example,
setting the learning rate, the rank, and the initialization. Another
improvement axis is adapter placement strategy: when using LoRA, practitioners
usually pick module types to adapt with LoRA, such as Query and Key modules.
Few works have studied the problem of adapter placement, with nonconclusive
results: original LoRA paper suggested placing adapters in attention modules,
while other works suggested placing them in the MLP modules. Through an
intuitive theoretical analysis, we introduce PLoP (Precise LoRA Placement), a
lightweight method that allows automatic identification of module types where
LoRA adapters should be placed, given a pretrained model and a finetuning task.
We demonstrate that PLoP consistently outperforms, and in the worst case
competes, with commonly used placement strategies through comprehensive
experiments on supervised finetuning and reinforcement learning for reasoning.

</details>


### [331] [Comparative Analysis of Deep Learning Models for Crop Disease Detection: A Transfer Learning Approach](https://arxiv.org/abs/2506.20323)
*Saundarya Subramaniam,Shalini Majumdar,Shantanu Nadar,Kaustubh Kulkarni*

Main category: cs.LG

TL;DR: 本文开发了一种基于人工智能的作物病害检测系统，比较了不同的深度学习模型在迁移学习中的效果，并实现了 95.76%的验证准确率。


<details>
  <summary>Details</summary>
Motivation: 本文旨在帮助资源有限的农村农民，通过开发高效的作物病害检测系统，改善作物健康管理并促进农业可持续发展。

Method: 采用深度学习模型（包括 EfficientNet、ResNet101、MobileNetV2 和自定义 CNN），对作物病害进行分类，并比较这些模型在迁移学习中的效果。

Result: 通过自定义 CNN 模型，实现了 95.76%的高验证准确率，展现了迁移学习在作物病害分类中的潜力。

Conclusion: 迁移学习技术能有效地用于农业病害检测系统，可显著改善作物健康管理，对于农村农业可持续发展具有重要意义。

Abstract: This research presents the development of an Artificial Intelligence (AI) -
driven crop disease detection system designed to assist farmers in rural areas
with limited resources. We aim to compare different deep learning models for a
comparative analysis, focusing on their efficacy in transfer learning. By
leveraging deep learning models, including EfficientNet, ResNet101,
MobileNetV2, and our custom CNN, which achieved a validation accuracy of
95.76%, the system effectively classifies plant diseases. This research
demonstrates the potential of transfer learning in reshaping agricultural
practices, improving crop health management, and supporting sustainable farming
in rural environments.

</details>


### [332] [Permutation Equivariant Neural Controlled Differential Equations for Dynamic Graph Representation Learning](https://arxiv.org/abs/2506.20324)
*Torben Berndt,Benjamin Walker,Tiexin Qin,Jan Stühmer,Andrey Kormilitzin*

Main category: cs.LG

TL;DR: 本文提出了一种新的动态图建模方法，称为Permutation Equivariant Neural Graph CDEs，通过将Graph Neural CDEs投影到置换等变函数空间，提高了效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 动态图的复杂性来自于节点特征和网络结构的演变，现有方法难以同时高效处理这些动态特性。

Method: 通过将原有的Graph Neural CDEs投影到置换等变函数空间，显著减少参数量，同时保留模型的表达能力。

Result: 在模拟的动态系统和实际任务中，所提方法在插值和外推场景中表现出更优的性能。

Conclusion: Permutation Equivariant Neural Graph CDEs能够在提高效率的同时不妥协模型表现，是动态图建模领域的有效改进。

Abstract: Dynamic graphs exhibit complex temporal dynamics due to the interplay between
evolving node features and changing network structures. Recently, Graph Neural
Controlled Differential Equations (Graph Neural CDEs) successfully adapted
Neural CDEs from paths on Euclidean domains to paths on graph domains. Building
on this foundation, we introduce Permutation Equivariant Neural Graph CDEs,
which project Graph Neural CDEs onto permutation equivariant function spaces.
This significantly reduces the model's parameter count without compromising
representational power, resulting in more efficient training and improved
generalisation. We empirically demonstrate the advantages of our approach
through experiments on simulated dynamical systems and real-world tasks,
showing improved performance in both interpolation and extrapolation scenarios.

</details>


### [333] [Producer-Fairness in Sequential Bundle Recommendation](https://arxiv.org/abs/2506.20329)
*Alexandre Rio,Marta Soare,Sihem Amer-Yahia*

Main category: cs.LG

TL;DR: 本论文研究了在连续推荐设置中，实现基于生产者公平和高质量组合的推荐方法。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中如何在多个用户之间公平分配物品组的曝光机会，同时保证推荐组合的品质。

Method: 提出了一种实时解决的精确方案和三种启发式方法，包括优先质量、优先公平和自适应方案，在公平和质量之间动态平衡。

Result: 通过三个真实数据集的实验，验证了各方案的优缺点，并在公平性和质量两方面均取得了有效的推荐效果。

Conclusion: 证明可以在不牺牲推荐组合质量的情况下，实现公平的组合推荐方案。

Abstract: We address fairness in the context of sequential bundle recommendation, where
users are served in turn with sets of relevant and compatible items. Motivated
by real-world scenarios, we formalize producer-fairness, that seeks to achieve
desired exposure of different item groups across users in a recommendation
session. Our formulation combines naturally with building high quality bundles.
Our problem is solved in real time as users arrive. We propose an exact
solution that caters to small instances of our problem. We then examine two
heuristics, quality-first and fairness-first, and an adaptive variant that
determines on-the-fly the right balance between bundle fairness and quality.
Our experiments on three real-world datasets underscore the strengths and
limitations of each solution and demonstrate their efficacy in providing fair
bundle recommendations without compromising bundle quality.

</details>


### [334] [On the ability of Deep Neural Networks to Learn Granger Causality in Multi-Variate Time Series Data](https://arxiv.org/abs/2506.20347)
*Malik Shahid Sultan,Hernando Ombao*

Main category: cs.LG

TL;DR: 本文探讨了一种新颖的深度学习方法，用于从多变量时间序列数据中学习Granger因果关系，摒弃了传统方法中的变量选择问题。


<details>
  <summary>Details</summary>
Motivation: 传统的Granger因果分析受限于线性自回归模型的假设，这些方法难以捕获复杂的非线性关联，因此需要更强大的工具，如深度神经网络。

Method: 提出了一种基于深度学习的新框架，联合建模时间序列，通过比较模型在移除特定时间序列组件前后的不确定性或残差分布，以及研究输入层dropout的影响，探究是否能够从数据中学习出Granger因果结构。

Result: 验证了一个经过良好正则化的深度学习模型能够从数据中学习真实的Granger因果结构，无需特别设计的损失函数。

Conclusion: 本研究展示了深度学习在发现多变量时间序列因果结构中的潜力，挑战了传统Granger因果分析框架的局限性。

Abstract: Granger Causality (GC) offers an elegant statistical framework to study the
association between multivariate time series data. Linear Vector Autoregressive
models (VAR) though have nice interpretation properties but have limited
practical application due to underlying assumptions on the kind of associations
that can be captured by these models. Numerous attempts have already been made
in the literature that exploit the functional approximation power of Deep
Neural Networks (DNNs) for the task of GC estimation. These methods however
treat GC as a variable selection problem. We present a novel paradigm for
approaching GC. We present this idea that GC is essentially linked with
prediction and if a deep learning model is used to model the time series
collectively or jointly, a well regularized model may learn the true granger
causal structure from the data, given that there is enough training data. We
propose to uncover the learned GC structure by comparing the model uncertainty
or distribution of the residuals when the past of everything is used as
compared to the one where a specific time series component is dropped from the
model. We also compare the effect of input layer dropout on the ability of a
neural network to learn granger causality from the data. We show that a well
regularized model infact can learn the true GC structure from the data without
explicitly adding terms in the loss function that guide the model to select
variables or perform sparse regression.

</details>


### [335] [DipSVD: Dual-importance Protected SVD for Efficient LLM Compression](https://arxiv.org/abs/2506.20353)
*Xuan Ding,Rui Sun,Yunjian Zhang,Xiu Yan,Yueqi Zhou,Kaihao Huang,Suzhong Fu,Chuanlong Xie,Yao Zhu*

Main category: cs.LG

TL;DR: 提出一种称为DipSVD的双重层次重要性保护机制，改善SVD压缩方法，提高大语言模型在高压缩率下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有SVD压缩方法忽视了对矩阵中关键组分的保护，导致压缩模型性能不佳。

Method: （1）局部重要性保护：通过通道加权数据白化，保护每个权重矩阵中最关键的奇异向量；（2）全局重要性保护：通过启发式或基于优化的方法，让不重要的层分担更多的压缩负担，减少对关键层的影响。

Result: 实验表明，DipSVD在多个基准测试中优于现有的SVD压缩方法，特别是在高压缩率下性能表现出色。

Conclusion: DipSVD有效保护关键组分，显著提升了压缩模型的性能，尤其适用于高压缩需求的大语言模型。

Abstract: The ever-increasing computational demands and deployment costs of large
language models (LLMs) have spurred numerous compressing methods. Compared to
quantization and unstructured pruning, SVD compression offers superior hardware
compatibility and theoretical guarantees. However, existing SVD-based methods
focus on the overall discrepancy between the original and compressed matrices
while overlooking the protection of critical components within the matrix,
which leads to inferior performance in the compressed models. This paper
proposes a dual-level importance protection mechanism to enhance SVD-based
compression methods: (1) local importance protection: preserving the most
critical singular vectors within each weight matrix through channel-weighted
data whitening; and (2) global importance protection: enabling less important
layers to bear a greater portion of the compression burden through either a
heuristic or optimization-based approach, thereby minimizing the impact of
compression on critical layers. Extensive experiments demonstrate that DipSVD
outperforms existing SVD-based compression approaches across multiple
benchmarks, achieving superior model performance especially at high model
compression ratios.

</details>


### [336] [A foundation model with multi-variate parallel attention to generate neuronal activity](https://arxiv.org/abs/2506.20354)
*Francesco Carzaniga,Michael Hersche,Abu Sebastian,Kaspar Schindler,Abbas Rahimi*

Main category: cs.LG

TL;DR: 提出了一种新型自注意机制（MVPA），并用其构建了首个开源iEEG基础模型（MVPFormer），在多个数据集上表现优异并提供开源代码和大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 解决多变量时间序列中异构通道配置对深度神经网络建模的挑战，特别是针对iEEG领域中跨个体通道设置差异的问题。

Method: 通过引入多变量并行注意机制（MVPA）分离内容、时间和空间注意力以建模异构时间序列数据，并开发了MVPFormer模型实现跨个体通用和高效的信号预测。

Result: MVPFormer利用MVPA在癫痫检测任务中达到专家级性能，并在SWEC、MAYO、FNUSA等数据集上超越现有Transformer模型。此外，MVPA机制在时间序列预测和分类任务中表现优越。

Conclusion: MVPA被验证为一种通用的异构时间序列注意机制，而MVPFormer成为首个开源iEEG基础模型，在临床表现中达到最新水平并提供代码和数据支持。

Abstract: Learning from multi-variate time-series with heterogeneous channel
configurations remains a fundamental challenge for deep neural networks (DNNs),
particularly in clinical domains such as intracranial electroencephalography
(iEEG), where channel setups vary widely across subjects. In this work, we
introduce multi-variate parallel attention (MVPA), a novel self-attention
mechanism that disentangles content, temporal, and spatial attention, enabling
flexible, generalizable, and efficient modeling of time-series data with
varying channel counts and configurations. We use MVPA to build MVPFormer, a
generative foundation model for human electrophysiology, trained to predict the
evolution of iEEG signals across diverse subjects. To support this and future
effort by the community, we release the SWEC iEEG dataset, the largest publicly
available iEEG dataset to date, comprising nearly 10,000 hours of recordings
from heterogeneous clinical sources. MVPFormer leverages MVPA to achieve strong
generalization across subjects, demonstrating expert-level performance in
seizure detection and outperforming state-of-the-art Transformer baselines on
our SWEC, the MAYO, and the FNUSA dataset. We further validate MVPA on standard
time-series forecasting and classification tasks, where it matches or exceeds
existing attention-based models. Together, our contributions establish MVPA as
a general-purpose attention mechanism for heterogeneous time-series and
MVPFormer as the first open-source, open-weights, and open-data iEEG foundation
model with state-of-the-art clinical performance. The code is available at
https://github.com/IBM/multi-variate-parallel-transformer. The SWEC iEEG
dataset is available at
https://mb-neuro.medical-blocks.ch/public_access/databases/ieeg/swec_ieeg.

</details>


### [337] [Towards Interpretable and Efficient Feature Selection in Trajectory Datasets: A Taxonomic Approach](https://arxiv.org/abs/2506.20359)
*Chanuka Don Samarasinghage,Dhruv Gulabani*

Main category: cs.LG

TL;DR: 本文介绍了基于分类法的特征选择方法，用以处理轨迹数据中的高维特征问题，提高机器学习的预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 步态分析需要理解对象运动模式并预测其下一步动作，但高维特征造成的冗余问题影响了效率和模型准确性。

Method: 提出一种基于分类法的特征选择方法，将特征分为几何和运动学两类，再细分为曲率、凹度、速度和加速度，减少特征空间维数。

Result: 基于分类法的方法在预测性能上表现良好，特征选择时间显著缩短，同时揭示不同数据集对特征的敏感性。

Conclusion: 基于分类法的特征选择方法提升了解释性，降低了计算复杂度，为轨迹数据分析和可解释人工智能提供了方法论框架。

Abstract: Trajectory analysis is not only about obtaining movement data, but it is also
of paramount importance in understanding the pattern in which an object moves
through space and time, as well as in predicting its next move. Due to the
significant interest in the area, data collection has improved substantially,
resulting in a large number of features becoming available for training and
predicting models. However, this introduces a high-dimensionality-induced
feature explosion problem, which reduces the efficiency and interpretability of
the data, thereby reducing the accuracy of machine learning models. To overcome
this issue, feature selection has become one of the most prevalent tools. Thus,
the objective of this paper was to introduce a taxonomy-based feature selection
method that categorizes features based on their internal structure. This
approach classifies the data into geometric and kinematic features, further
categorizing them into curvature, indentation, speed, and acceleration. The
comparative analysis indicated that a taxonomy-based approach consistently
achieved comparable or superior predictive performance. Furthermore, due to the
taxonomic grouping, which reduces combinatorial space, the time taken to select
features was drastically reduced. The taxonomy was also used to gain insights
into what feature sets each dataset was more sensitive to. Overall, this study
provides robust evidence that a taxonomy-based feature selection method can add
a layer of interpretability, reduce dimensionality and computational
complexity, and contribute to high-level decision-making. It serves as a step
toward providing a methodological framework for researchers and practitioners
dealing with trajectory datasets and contributing to the broader field of
explainable artificial intelligence.

</details>


### [338] [Self-Supervised Graph Learning via Spectral Bootstrapping and Laplacian-Based Augmentations](https://arxiv.org/abs/2506.20362)
*Lorenzo Bini,Stephane Marchand-Maillet*

Main category: cs.LG

TL;DR: 本文提出了一种新的自监督图学习框架LaplaceGNN，通过谱引导的技术避免了负样本采样的需求。


<details>
  <summary>Details</summary>
Motivation: 开发一种无需依赖对比目标或人工增强的高效自监督图神经网络方法。

Method: 预计算通过Max-Min中心性优化的谱增强，并引入对抗引导的自举训练机制。

Result: LaplaceGNN在多个基准数据集上的性能优于现有的自监督图学习方法。

Conclusion: LaplaceGNN为有效学习图表示提供了一种简单且高效的替代方案，具有广泛的应用潜力。

Abstract: We present LaplaceGNN, a novel self-supervised graph learning framework that
bypasses the need for negative sampling by leveraging spectral bootstrapping
techniques. Our method integrates Laplacian-based signals into the learning
process, allowing the model to effectively capture rich structural
representations without relying on contrastive objectives or handcrafted
augmentations. By focusing on positive alignment, LaplaceGNN achieves linear
scaling while offering a simpler, more efficient, self-supervised alternative
for graph neural networks, applicable across diverse domains. Our contributions
are twofold: we precompute spectral augmentations through max-min
centrality-guided optimization, enabling rich structural supervision without
relying on handcrafted augmentations, then we integrate an adversarial
bootstrapped training scheme that further strengthens feature learning and
robustness. Our extensive experiments on different benchmark datasets show that
LaplaceGNN achieves superior performance compared to state-of-the-art
self-supervised graph methods, offering a promising direction for efficiently
learning expressive graph representations.

</details>


### [339] [TESSERA: Temporal Embeddings of Surface Spectra for Earth Representation and Analysis](https://arxiv.org/abs/2506.20380)
*Zhengpeng Feng,Sadiq Jaffer,Jovana Knezevic,Silja Sormunen,Robin Young,Madeline Lisaius,Markus Immitzer,James Ball,Clement Atzberger,David A. Coomes,Anil Madhavapeddy,Andrew Blake,Srinivasan Keshav*

Main category: cs.LG

TL;DR: 本文提出了名为TESSERA的新型遥感基础模型，利用自监督学习生成10m精度的全球鲁棒表示，针对多样的下游任务表现优越。


<details>
  <summary>Details</summary>
Motivation: 推动遥感技术的发展，通过生成高分辨率、高性能的表示，服务于包括气候建模、碳核算及土地可持续利用等各种地球观测应用。

Method: TESSERA模型采用自监督学习，对Sentinel-1 SAR数据和Sentinel-2 MSI数据分别进行编码，使用Transformer结构的并行编码器和多层感知机融合生成覆盖2017至2024年的全球表示地图。

Result: TESSERA在五个不同任务中表现优于传统遥感模型和现有的地理空间基础模型，刷新了性能基准。

Conclusion: TESSERA模型为遥感领域提供了开源且高效的解决方案，促进了高性能遥感表示的普及，并显示其在多样任务中的优越性。

Abstract: Satellite remote sensing (RS) enables a wide array of downstream Earth
observation (EO) applications, including climate modeling, carbon accounting,
and strategies for conservation and sustainable land use. We present TESSERA, a
novel Remote Sensing Foundation Model (RSFM) that uses Self-Supervised Learning
(SSL) to generate global, robust representations at 10m scale from pixel-level
satellite time series data. TESSERA combines information from only optical and
SAR data streams using two parallel Transformer-based encoders: one dedicated
to Sentinel-1 SAR polarizations and another to Sentinel-2 MSI data (10 selected
spectral bands) to create representations that are then fused using a
multilayer perceptron (MLP), resulting in a global representation map covering
the years 2017 to 2024. Our precomputed representations set a new
state-of-the-art performance benchmark and our open-source approach
democratizes access to high-performance, high-resolution representations. We
benchmark the performance of TESSERA in five diverse tasks, comparing our work
with state-of-the-art task-specific models and other foundation models. Our
results show that TESSERA outperforms both traditional RS baselines and the
leading geospatial foundation models in these diverse downstream tasks.

</details>


### [340] [Client Clustering Meets Knowledge Sharing: Enhancing Privacy and Robustness in Personalized Peer-to-Peer Learning](https://arxiv.org/abs/2506.20413)
*Mohammad Mahdi Maheri,Denys Herasymuk,Hamed Haddadi*

Main category: cs.LG

TL;DR: 本文开发了一种名为P4的方法，可在资源受限的IoT设备中实现个性化学习，同时确保差分隐私和对投毒攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在异构和资源受限的IoT设备中需要能够高效且私密操作的个性化学习方法，同时应对数据隐私保护和投毒攻击等挑战。

Method: 提出了P4方法，通过一个轻量化、完全去中心化的算法来检测客户端的私密相似性并形成协作组，再利用差分隐私知识蒸馏在组内进行模型协同训练。

Result: 实验表明，P4在不同的异构性设置和攻击场景下，相较于主流差分隐私点对点方法提高了5%至30%的准确率，且在多达30%的恶意客户端存在下仍具有鲁棒性。同时，在资源受限设备上协作训练的额外开销仅为约7秒。

Conclusion: P4方法成功实现了在异构资源受限IoT环境中个性化学习的目标，既能保证高精度，又具备差分隐私和鲁棒性，是资源受限设备中个性化学习的实际可行方案。

Abstract: The growing adoption of Artificial Intelligence (AI) in Internet of Things
(IoT) ecosystems has intensified the need for personalized learning methods
that can operate efficiently and privately across heterogeneous,
resource-constrained devices. However, enabling effective personalized learning
in decentralized settings introduces several challenges, including efficient
knowledge transfer between clients, protection of data privacy, and resilience
against poisoning attacks. In this paper, we address these challenges by
developing P4 (Personalized, Private, Peer-to-Peer) -- a method designed to
deliver personalized models for resource-constrained IoT devices while ensuring
differential privacy and robustness against poisoning attacks. Our solution
employs a lightweight, fully decentralized algorithm to privately detect client
similarity and form collaborative groups. Within each group, clients leverage
differentially private knowledge distillation to co-train their models,
maintaining high accuracy while ensuring robustness to the presence of
malicious clients. We evaluate P4 on popular benchmark datasets using both
linear and CNN-based architectures across various heterogeneity settings and
attack scenarios. Experimental results show that P4 achieves 5% to 30% higher
accuracy than leading differentially private peer-to-peer approaches and
maintains robustness with up to 30% malicious clients. Additionally, we
demonstrate its practicality by deploying it on resource-constrained devices,
where collaborative training between two clients adds only ~7 seconds of
overhead.

</details>


### [341] [Off-Policy Evaluation and Learning for the Future under Non-Stationarity](https://arxiv.org/abs/2506.20417)
*Tatsuhiro Shimizu,Kazuki Kawamura,Takanori Muroi,Yusuke Narita,Kei Tateno,Takuma Udagawa,Yuta Saito*

Main category: cs.LG

TL;DR: 研究了在非平稳环境下估计和优化策略未来价值的F-OPE和F-OPL新问题，提出了一种新的基于时间序列结构的OPFV估计器，以及扩展的策略梯度方法，并在实验中展示显著优越性。


<details>
  <summary>Details</summary>
Motivation: 在非平稳环境中，当前方法假设的平稳性或依赖于局限性强的奖励模型，造成了显著偏差。研究者希望通过新的方法准确预测未来时间点的策略值。

Method: 提出了一种名为OPFV的估计器，利用时间序列数据中季节性、周次性等结构设计了一种新的重要性加权方法，同时扩展为策略梯度法以实现策略优化。

Result: 实验结果表明，在各种设置下方法在估计和优化非平稳环境中策略未来价值方面优于现有方法。

Conclusion: 通过利用时间相关结构，研究提出的OPFV估计器显著减小了偏差，并成功解决了F-OPE和F-OPL的问题，有助于非平稳环境中的策略优化。

Abstract: We study the novel problem of future off-policy evaluation (F-OPE) and
learning (F-OPL) for estimating and optimizing the future value of policies in
non-stationary environments, where distributions vary over time. In e-commerce
recommendations, for instance, our goal is often to estimate and optimize the
policy value for the upcoming month using data collected by an old policy in
the previous month. A critical challenge is that data related to the future
environment is not observed in the historical data. Existing methods assume
stationarity or depend on restrictive reward-modeling assumptions, leading to
significant bias. To address these limitations, we propose a novel estimator
named \textit{\textbf{O}ff-\textbf{P}olicy Estimator for the \textbf{F}uture
\textbf{V}alue (\textbf{\textit{OPFV}})}, designed for accurately estimating
policy values at any future time point. The key feature of OPFV is its ability
to leverage the useful structure within time-series data. While future data
might not be present in the historical log, we can leverage, for example,
seasonal, weekly, or holiday effects that are consistent in both the historical
and future data. Our estimator is the first to exploit these time-related
structures via a new type of importance weighting, enabling effective F-OPE.
Theoretical analysis identifies the conditions under which OPFV becomes
low-bias. In addition, we extend our estimator to develop a new policy-gradient
method to proactively learn a good future policy using only historical data.
Empirical results show that our methods substantially outperform existing
methods in estimating and optimizing the future policy value under
non-stationarity for various experimental setups.

</details>


### [342] [Tackling Data Heterogeneity in Federated Learning through Knowledge Distillation with Inequitable Aggregation](https://arxiv.org/abs/2506.20431)
*Xing Ma*

Main category: cs.LG

TL;DR: 本研究提出了一个名为KDIA的策略，用于在联邦学习中应对少量客户参与训练的大规模客户设置，其可以更高效地利用所有客户的知识。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常忽视了仅少量客户参与训练的大规模客户场景，这种情况下联邦学习任务更加具有挑战性。

Method: 提出KDIA策略，其基本思想是结合教师-学生模型的不均等聚合：学生模型采用参与客户的平均聚合，而教师模型根据参与间隔、参与次数和数据量比例对所有客户进行加权聚合。此外，在本地训练中执行自知识蒸馏，并利用服务端生成器生成近似IID的数据特征进行辅助训练。

Result: 在CIFAR-10/100/CINIC-10数据集及多种异质性设置下进行了实验，结果表明KDIA在减少训练轮次的同时能达到更高的准确率，其中在严重异质性情况下相对提升更为显著。

Conclusion: KDIA策略在应对异质性联邦学习任务中表现出显著的性能提升，为联邦学习的大规模应用提供了新的解决方案。

Abstract: Federated learning aims to train a global model in a distributed environment
that is close to the performance of centralized training. However, issues such
as client label skew, data quantity skew, and other heterogeneity problems
severely degrade the model's performance. Most existing methods overlook the
scenario where only a small portion of clients participate in training within a
large-scale client setting, whereas our experiments show that this scenario
presents a more challenging federated learning task. Therefore, we propose a
Knowledge Distillation with teacher-student Inequitable Aggregation (KDIA)
strategy tailored to address the federated learning setting mentioned above,
which can effectively leverage knowledge from all clients. In KDIA, the student
model is the average aggregation of the participating clients, while the
teacher model is formed by a weighted aggregation of all clients based on three
frequencies: participation intervals, participation counts, and data volume
proportions. During local training, self-knowledge distillation is performed.
Additionally, we utilize a generator trained on the server to generate
approximately independent and identically distributed (IID) data features
locally for auxiliary training. We conduct extensive experiments on the
CIFAR-10/100/CINIC-10 datasets and various heterogeneous settings to evaluate
KDIA. The results show that KDIA can achieve better accuracy with fewer rounds
of training, and the improvement is more significant under severe
heterogeneity.

</details>


### [343] [Méthode de quadrature pour les PINNs fondée théoriquement sur la hessienne des résiduels](https://arxiv.org/abs/2506.20441)
*Antoine Caradot,Rémi Emonet,Amaury Habrard,Abdel-Rahim Mezidi,Marc Sebban*

Main category: cs.LG

TL;DR: 本文提出了一种基于函数Hessian矩阵的新型求积分方法，用以改进物理知识引导的神经网络(PINNs)在训练过程中对采样点的选择。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs采样点的选择主要依赖于均匀采样，但均匀采样可能难以有效地捕获函数特性，从而影响训练结果。作者希望通过改进采样策略来提高PINNs的效率和精度。

Method: 提出了一种新型的求积分方法，基于目标函数的Hessian矩阵来引导在PINNs训练过程中采样点的选择。

Result: 通过改进采样策略，在PINNs的训练过程中更好地捕获了目标函数的特性，提升了训练精度和效率。

Conclusion: 利用Hessian信息改进的采样策略能够显著提高PINNs的性能，对模型在多样化应用中的推广具有潜力。

Abstract: Physics-informed Neural Networks (PINNs) have emerged as an efficient way to
learn surrogate neural solvers of PDEs by embedding the physical model in the
loss function and minimizing its residuals using automatic differentiation at
so-called collocation points. Originally uniformly sampled, the choice of the
latter has been the subject of recent advances leading to adaptive sampling
refinements. In this paper, we propose a new quadrature method for
approximating definite integrals based on the hessian of the considered
function, and that we leverage to guide the selection of the collocation points
during the training process of PINNs.

</details>


### [344] [Automatic Demonstration Selection for LLM-based Tabular Data Classification](https://arxiv.org/abs/2506.20451)
*Shuchu Han,Wolfgang Bruckner*

Main category: cs.LG

TL;DR: 本文提出了一种算法，自动选择适用于In-Context Learning（ICL）表格数据分类的展示样本数量。


<details>
  <summary>Details</summary>
Motivation: 解决适用于ICL的表格数据分类中，如何确定提示中理想的展示样本数的问题。

Method: 基于谱图理论，提出一种新指标量化不同展示样本间的相似性，构建相似性图并通过拉普拉斯矩阵的特征值计算最少所需样本数量。

Result: 通过实验，与传统随机选择算法相比，该方法在多样数据集和LLM上的表现更优。

Conclusion: 提出的方法能够合理估算所需展示样本数量，为ICL表格数据分类提供了更高效的解决方案。

Abstract: A fundamental question in applying In-Context Learning (ICL) for tabular data
classification is how to determine the ideal number of demonstrations in the
prompt. This work addresses this challenge by presenting an algorithm to
automatically select a reasonable number of required demonstrations. Our method
distinguishes itself by integrating not only the tabular data's distribution
but also the user's selected prompt template and the specific Large Language
Model (LLM) into its estimation. Rooted in Spectral Graph Theory, our proposed
algorithm defines a novel metric to quantify the similarities between different
demonstrations. We then construct a similarity graph and analyze the
eigenvalues of its Laplacian to derive the minimum number of demonstrations
capable of representing the data within the LLM's intrinsic representation
space. We validate the efficacy of our approach through experiments comparing
its performance against conventional random selection algorithms on diverse
datasets and LLMs.

</details>


### [345] [Multimodal Representation Learning and Fusion](https://arxiv.org/abs/2506.20494)
*Qihang Jin,Enze Ge,Yuhang Xie,Hongying Luo,Junhao Song,Ziqian Bi,Chia Xin Liang,Jibin Guan,Joe Yeong,Junfeng Hao*

Main category: cs.LG

TL;DR: 多模态学习通过结合图像、文本、音频等信息，增强AI系统的理解、推理与决策能力，但仍面临数据格式多样性、不完整输入及对抗攻击等挑战，并寻求效率和可扩展性的提升。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在通过结合多模态数据的优势，开发更智能的AI系统，以应对真实世界的复杂性，并在多个领域实现突破。

Method: 核心方法包括共享表示学习、模态对齐、融合策略，以及探索无监督学习、自动化工具（AutoML）等新方法，同时重视评估标准的改进与共享基准的构建。

Result: 研究现已在多模态学习关键技术上取得进展，但仍需解决效率提升和模型扩展性问题。

Conclusion: 多模态学习在视觉、语言处理、语音识别和医疗领域潜力巨大，未来可能实现更接近人类理解世界的AI系统。

Abstract: Multi-modal learning is a fast growing area in artificial intelligence. It
tries to help machines understand complex things by combining information from
different sources, like images, text, and audio. By using the strengths of each
modality, multi-modal learning allows AI systems to build stronger and richer
internal representations. These help machines better interpretation, reasoning,
and making decisions in real-life situations. This field includes core
techniques such as representation learning (to get shared features from
different data types), alignment methods (to match information across
modalities), and fusion strategies (to combine them by deep learning models).
Although there has been good progress, some major problems still remain. Like
dealing with different data formats, missing or incomplete inputs, and
defending against adversarial attacks. Researchers now are exploring new
methods, such as unsupervised or semi-supervised learning, AutoML tools, to
make models more efficient and easier to scale. And also more attention on
designing better evaluation metrics or building shared benchmarks, make it
easier to compare model performance across tasks and domains. As the field
continues to grow, multi-modal learning is expected to improve many areas:
computer vision, natural language processing, speech recognition, and
healthcare. In the future, it may help to build AI systems that can understand
the world in a way more like humans, flexible, context aware, and able to deal
with real-world complexity.

</details>


### [346] [Collaborative Batch Size Optimization for Federated Learning](https://arxiv.org/abs/2506.20511)
*Arno Geimer,Karthick Panner Selvam,Beltran Fiz Pontiveros*

Main category: cs.LG

TL;DR: 本文提出了一种基于贪心随机搜索的优化方法，提高了联邦学习中本地训练的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习因其数据隐私保护特点广泛应用，但本地训练配置不当可能导致效率低下，因此需要优化硬件使用以改进训练过程。

Method: 利用联邦学习的并行处理特点，通过贪心随机搜索优化本地批处理大小，以找到最佳训练设置。

Result: 实验结果表明，与默认参数设置相比，该方法显著提高了收敛速度，性能接近优化本地参数的理想情况。

Conclusion: 优化硬件使用和本地训练参数能够在无需改变数据隐私前提下提高联邦学习的训练效率。

Abstract: Federated Learning (FL) is a decentralized collaborative Machine Learning
framework for training models without collecting data in a centralized
location. It has seen application across various disciplines, from helping
medical diagnoses in hospitals to detecting fraud in financial transactions. In
this paper, we focus on improving the local training process through hardware
usage optimization. While participants in a federation might share the hardware
they are training on, since there is no information exchange between them,
their training process can be hindered by an improper training configuration.
Taking advantage of the parallel processing inherent to Federated Learning, we
use a greedy randomized search to optimize local batch sizes for the best
training settings across all participants. Our results show that against
default parameter settings, our method improves convergence speed while staying
nearly on par with the case where local parameters are optimized.

</details>


### [347] [WallStreetFeds: Client-Specific Tokens as Investment Vehicles in Federated Learning](https://arxiv.org/abs/2506.20518)
*Arno Geimer,Beltran Fiz Pontiveros,Radu State*

Main category: cs.LG

TL;DR: 本文提出了一种新型的联邦学习奖励分配框架，利用去中心化金融平台和自动做市商，提高灵活性和可扩展性，同时引入第三方投资机制。


<details>
  <summary>Details</summary>
Motivation: 由于现有的奖励分配框架在联邦学习中相对缺乏研究，且无法满足灵活性和扩展要求，本文致力于解决这些问题并改进激励机制。

Method: 本文提出了使用客户端代币作为投资工具的创新框架，通过去中心化金融平台和自动做市商实现灵活和可扩展的奖励分配方案，同时允许第三方参与投资。

Result: 本文框架预期在改进奖励分配和引入外部投资方面展示了潜在优势，增强了联邦学习系统的商业潜力。

Conclusion: 建议的框架提高了联邦学习的激励有效性和系统灵活性，展示了在分布式协作中引入金融技术的前景。

Abstract: Federated Learning (FL) is a collaborative machine learning paradigm which
allows participants to collectively train a model while training data remains
private. This paradigm is especially beneficial for sectors like finance, where
data privacy, security and model performance are paramount. FL has been
extensively studied in the years following its introduction, leading to, among
others, better performing collaboration techniques, ways to defend against
other clients trying to attack the model, and contribution assessment methods.
An important element in for-profit Federated Learning is the development of
incentive methods to determine the allocation and distribution of rewards for
participants. While numerous methods for allocation have been proposed and
thoroughly explored, distribution frameworks remain relatively understudied. In
this paper, we propose a novel framework which introduces client-specific
tokens as investment vehicles within the FL ecosystem. Our framework aims to
address the limitations of existing incentive schemes by leveraging a
decentralized finance (DeFi) platform and automated market makers (AMMs) to
create a more flexible and scalable reward distribution system for
participants, and a mechanism for third parties to invest in the federation
learning process.

</details>


### [348] [Industrial Energy Disaggregation with Digital Twin-generated Dataset and Efficient Data Augmentation](https://arxiv.org/abs/2506.20525)
*Christian Internò,Andrea Castellani,Sebastian Schmitt,Fabio Stella,Barbara Hammer*

Main category: cs.LG

TL;DR: 本论文介绍了一个通过数字孪生生成的开源工业能量分解数据集（SIDED）以及一个改进NILM模型泛化能力的数据增强方法（AMDA）。


<details>
  <summary>Details</summary>
Motivation: 工业设备非侵入式负载监控(NILM)面临数据稀缺和能源使用模式复杂这一挑战。

Method: 提出SIDED数据集和AMDA方法，通过数字孪生生成多样化模拟数据，增强模型数据泛化能力并改善训练数据与测试数据分布的一致性。

Result: 使用AMDA增强的数据显著提高了NILM模型对复杂工业设备能耗分解效率，错误率(0.093)远低于用随机数据增强(0.290)甚至无增强(0.451)的模型。

Conclusion: SIDED数据集和AMDA方法有效应对了工业NILM领域的数据稀缺和模型泛化问题，提高了能量分解任务的性能。

Abstract: Industrial Non-Intrusive Load Monitoring (NILM) is limited by the scarcity of
high-quality datasets and the complex variability of industrial energy
consumption patterns. To address data scarcity and privacy issues, we introduce
the Synthetic Industrial Dataset for Energy Disaggregation (SIDED), an
open-source dataset generated using Digital Twin simulations. SIDED includes
three types of industrial facilities across three different geographic
locations, capturing diverse appliance behaviors, weather conditions, and load
profiles. We also propose the Appliance-Modulated Data Augmentation (AMDA)
method, a computationally efficient technique that enhances NILM model
generalization by intelligently scaling appliance power contributions based on
their relative impact. We show in experiments that NILM models trained with
AMDA-augmented data significantly improve the disaggregation of energy
consumption of complex industrial appliances like combined heat and power
systems. Specifically, in our out-of-sample scenarios, models trained with AMDA
achieved a Normalized Disaggregation Error of 0.093, outperforming models
trained without data augmentation (0.451) and those trained with random data
augmentation (0.290). Data distribution analyses confirm that AMDA effectively
aligns training and test data distributions, enhancing model generalization.

</details>


### [349] [Physics-Informed Machine Learning Regulated by Finite Element Analysis for Simulation Acceleration of Laser Powder Bed Fusion](https://arxiv.org/abs/2506.20537)
*R. Sharma,M. Raissi,Y. B. Guo*

Main category: cs.LG

TL;DR: 该研究提出了一种称为FEA-PINN的高效建模框架，用于加速激光粉末床融合过程中的热场预测，同时保留传统有限元法的准确性，并显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统有限元分析方法在激光粉末床融合过程中面临高计算成本的问题，需要找到一种更高效的模拟方法。

Method: 采用一种称为FEA-PINN的框架，通过将物理约束神经网络与有限元法相结合来预测热场，同时引入动态材料更新策略和转移学习来提高模型的通用性。

Result: FEA-PINN框架在保留有限元分析精度的同时，显著降低了热场预测中的计算成本，且验证了其准确性。

Conclusion: FEA-PINN框架是一种有效的解决方案，既保持了高精度，又大幅度提升了效率，可为激光粉末床融合工艺优化提供支持。

Abstract: Efficient simulation of Laser Powder Bed Fusion (LPBF) is crucial for process
prediction due to the lasting issue of high computation cost using traditional
numerical methods such as finite element analysis (FEA). This study presents an
efficient modeling framework termed FEA-Regulated Physics-Informed Neural
Network (FEA-PINN) to accelerate the thermal field prediction in a LPBF process
while maintaining the FEA accuracy. A novel dynamic material updating strategy
is developed to capture the dynamic phase change of powder-liquid-solid in the
PINN model. The PINN model incorporates temperature-dependent material
properties and phase change behavior using the apparent heat capacity method.
While the PINN model demonstrates high accuracy with a small training data and
enables generalization of new process parameters via transfer learning, it
faces the challenge of high computation cost in time-dependent problems due to
the residual accumulation. To overcome this issue, the FEA-PINN framework
integrates corrective FEA simulations during inference to enforce physical
consistency and reduce error drift. A comparative analysis shows that FEA-PINN
achieves equivalent accuracy to FEA while significantly reducing computational
cost. The framework has been validated using the benchmark FEA data and
demonstrated through single-track scanning in LPBF.

</details>


### [350] [Demonstration of effective UCB-based routing in skill-based queues on real-world data](https://arxiv.org/abs/2506.20543)
*Sanne van Kempen,Jaron Sanders,Fiona Sloothaak,Maarten G. Wolf*

Main category: cs.LG

TL;DR: 本文探讨了执行基于技能的排队系统的最优控制，尤其是使用强化学习算法优化客户路由的研究。


<details>
  <summary>Details</summary>
Motivation: 为了解决动态环境中技能型排队系统的最优客户路由问题，并提升实际应用中的表现。

Method: 通过案例分析真实数据集，利用新的强化学习算法，结合启发式路由规则，同时针对多目标优化及参数调节进行研究。

Result: 算法性能优异，适应环境变化，显著优于静态策略，并能实现多个优化目标，提供有关敏感性和参数调优的洞察。

Conclusion: 强化学习算法对复杂技能型排队系统的实时应用具有巨大潜力，可综合优化多种性能指标。

Abstract: This paper is about optimally controlling skill-based queueing systems such
as data centers, cloud computing networks, and service systems. By means of a
case study using a real-world data set, we investigate the practical
implementation of a recently developed reinforcement learning algorithm for
optimal customer routing. Our experiments show that the algorithm efficiently
learns and adapts to changing environments and outperforms static benchmark
policies, indicating its potential for live implementation. We also augment the
real-world applicability of this algorithm by introducing a new heuristic
routing rule to reduce delays. Moreover, we show that the algorithm can
optimize for multiple objectives: next to payoff maximization, secondary
objectives such as server load fairness and customer waiting time reduction can
be incorporated. Tuning parameters are used for balancing inherent performance
trade--offs. Lastly, we investigate the sensitivity to estimation errors and
parameter tuning, providing valuable insights for implementing adaptive routing
algorithms in complex real-world queueing systems.

</details>


### [351] [Benchmarking Unsupervised Strategies for Anomaly Detection in Multivariate Time Series](https://arxiv.org/abs/2506.20574)
*Laura Boggia,Rafael Teixeira de Lima,Bogdan Malaescu*

Main category: cs.LG

TL;DR: 本文研究利用iTransformer架构进行多元时间序列异常检测，从模型参数优化到异常标签提取再到训练数据评估，并与其它基于Transformer的模型进行比较。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列中异常检测因异常本质未知及维度间复杂依赖关系而具有挑战性。本文旨在借助iTransformer架构探究更高效的异常检测方法。

Method: 本文优化iTransformer关键参数，研究生成多维异常标签的方法，评估训练数据中异常的影响以及替代损失函数的有效性，并对多个基于Transformer模型进行对比分析。

Result: 证明iTransformer在不同参数设置及数据集上的表现，同时评估用于异常检测的不同方法及损失函数的效果。

Conclusion: iTransformer对多元时间序列异常检测的潜力较大，多参数调试及新方法的综合比较为提升其性能提供了良好的基础。

Abstract: Anomaly detection in multivariate time series is an important problem across
various fields such as healthcare, financial services, manufacturing or physics
detector monitoring. Accurately identifying when unexpected errors or faults
occur is essential, yet challenging, due to the unknown nature of anomalies and
the complex interdependencies between time series dimensions. In this paper, we
investigate transformer-based approaches for time series anomaly detection,
focusing on the recently proposed iTransformer architecture. Our contributions
are fourfold: (i) we explore the application of the iTransformer to time series
anomaly detection, and analyse the influence of key parameters such as window
size, step size, and model dimensions on performance; (ii) we examine methods
for extracting anomaly labels from multidimensional anomaly scores and discuss
appropriate evaluation metrics for such labels; (iii) we study the impact of
anomalous data present during training and assess the effectiveness of
alternative loss functions in mitigating their influence; and (iv) we present a
comprehensive comparison of several transformer-based models across a diverse
set of datasets for time series anomaly detection.

</details>


### [352] [Exploring Graph-Transformer Out-of-Distribution Generalization Abilities](https://arxiv.org/abs/2506.20575)
*Itay Niv,Neta Rabin*

Main category: cs.LG

TL;DR: 本研究探讨了图神经网络在分布迁移时的泛化能力，发现图变压器（GT）及其混合架构相较于传统方法更具优势，并提出了一种新颖的后训练分析方法。


<details>
  <summary>Details</summary>
Motivation: 解决当前图学习方法难以适应分布迁移（OOD）问题，从而提升实际应用中的泛化能力。

Method: 系统评估GT及混合架构在OOD环境下的表现，并适配和比较多种领域泛化算法。同时，提出了一种后训练分析方法，聚焦域对齐和类别分割。

Result: GT及其混合架构在各类分布迁移情况下均表现出更强的泛化能力，优于传统的消息传递神经网络（MPNNs）。

Conclusion: 图变压器在应对实际应用中的分布迁移问题中表现出强劲潜力，并为未来图学习研究提供了新方向。

Abstract: Deep learning on graphs has shown remarkable success across numerous
applications, including social networks, bio-physics, traffic networks, and
recommendation systems. Regardless of their successes, current methods
frequently depend on the assumption that training and testing data share the
same distribution, a condition rarely met in real-world scenarios. While
graph-transformer (GT) backbones have recently outperformed traditional
message-passing neural networks (MPNNs) in multiple in-distribution (ID)
benchmarks, their effectiveness under distribution shifts remains largely
unexplored.
  In this work, we address the challenge of out-of-distribution (OOD)
generalization for graph neural networks, with a special focus on the impact of
backbone architecture. We systematically evaluate GT and hybrid backbones in
OOD settings and compare them to MPNNs. To do so, we adapt several leading
domain generalization (DG) algorithms to work with GTs and assess their
performance on a benchmark designed to test a variety of distribution shifts.
Our results reveal that GT and hybrid GT-MPNN backbones consistently
demonstrate stronger generalization ability compared to MPNNs, even without
specialized DG algorithms.
  Additionally, we propose a novel post-training analysis approach that
compares the clustering structure of the entire ID and OOD test datasets,
specifically examining domain alignment and class separation. Demonstrating its
model-agnostic design, this approach not only provided meaningful insights into
GT and MPNN backbones. It also shows promise for broader applicability to DG
problems beyond graph learning, offering a deeper perspective on generalization
abilities that goes beyond standard accuracy metrics. Together, our findings
highlight the promise of graph-transformers for robust, real-world graph
learning and set a new direction for future research in OOD generalization.

</details>


### [353] [The kernel of graph indices for vector search](https://arxiv.org/abs/2506.20584)
*Mariano Tepper,Ted Willke*

Main category: cs.LG

TL;DR: 提出了一种新型图索引，支持向量图(SVG)，可在度量和非度量向量空间中用于矢量搜索，并且具有正式的导航保证。


<details>
  <summary>Details</summary>
Motivation: 当前主流图索引仅适用于欧几里得空间，无法满足度量和非度量空间矢量搜索的需求。

Method: 利用机器学习和核方法，创建了一种新的图索引方式。提出了SVG并分析其特性和原理，同时提出了带有稀疏性约束的版本SVG-L0。

Result: 证明了SVG具有普适性，适用于多种向量空间，并通过分析其性能提出了改进方法如SVG-L0，优化了计算复杂度并提升了图索引质量。

Conclusion: SVG开辟了图构建新视角，改进了传统方法，且为矢量搜索任务提供了有效解决方案，具有广泛应用潜力。

Abstract: The most popular graph indices for vector search use principles from
computational geometry to build the graph. Hence, their formal graph
navigability guarantees are only valid in Euclidean space. In this work, we
show that machine learning can be used to build graph indices for vector search
in metric and non-metric vector spaces (e.g., for inner product similarity).
From this novel perspective, we introduce the Support Vector Graph (SVG), a new
type of graph index that leverages kernel methods to establish the graph
connectivity and that comes with formal navigability guarantees valid in metric
and non-metric vector spaces. In addition, we interpret the most popular graph
indices, including HNSW and DiskANN, as particular specializations of SVG and
show that new indices can be derived from the principles behind this
specialization. Finally, we propose SVG-L0 that incorporates an $\ell_0$
sparsity constraint into the SVG kernel method to build graphs with a bounded
out-degree. This yields a principled way of implementing this practical
requirement, in contrast to the traditional heuristic of simply truncating the
out edges of each node. Additionally, we show that SVG-L0 has a self-tuning
property that avoids the heuristic of using a set of candidates to find the
out-edges of each node and that keeps its computational complexity in check.

</details>


### [354] [H-FEX: A Symbolic Learning Method for Hamiltonian Systems](https://arxiv.org/abs/2506.20607)
*Jasen Lai,Senwei Liang,Chunmei Wang*

Main category: cs.LG

TL;DR: 提出了一种用于学习哈密顿系统的新方法H-FEX，通过符号学习和引入新交互节点，有效捕捉复杂系统的哈密顿函数，同时保持能量守恒。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法难以准确捕捉复杂哈密顿函数并同时保持能量守恒。

Method: 提出H-FEX方法，通过符号学习引入新型交互节点以有效学习复杂交互项。

Result: 在实验中，包括高刚性动力系统的例子中，H-FEX能准确恢复复杂系统的哈密顿函数并在长时间尺度上保持能量守恒。

Conclusion: H-FEX为发现复杂动力系统的闭式表达提供了有力框架，有较大的应用潜力。

Abstract: Hamiltonian systems describe a broad class of dynamical systems governed by
Hamiltonian functions, which encode the total energy and dictate the evolution
of the system. Data-driven approaches, such as symbolic regression and neural
network-based methods, provide a means to learn the governing equations of
dynamical systems directly from observational data of Hamiltonian systems.
However, these methods often struggle to accurately capture complex Hamiltonian
functions while preserving energy conservation. To overcome this limitation, we
propose the Finite Expression Method for learning Hamiltonian Systems (H-FEX),
a symbolic learning method that introduces novel interaction nodes designed to
capture intricate interaction terms effectively. Our experiments, including
those on highly stiff dynamical systems, demonstrate that H-FEX can recover
Hamiltonian functions of complex systems that accurately capture system
dynamics and preserve energy over long time horizons. These findings highlight
the potential of H-FEX as a powerful framework for discovering closed-form
expressions of complex dynamical systems.

</details>


### [355] [Lost in Retraining: Roaming the Parameter Space of Exponential Families Under Closed-Loop Learning](https://arxiv.org/abs/2506.20623)
*Fariba Jangjoo,Matteo Marsili,Yasser Roudi*

Main category: cs.LG

TL;DR: 本文研究了封闭环学习过程，其中模型通过其自身生成的数据更新。结果表明最大似然估计会导致收敛于放大初始偏差的状态，但引入外部数据、最大后验估计或正则化可以避免这一问题。


<details>
  <summary>Details</summary>
Motivation: 封闭环学习越来越受到关注，未来可能主要以人工神经网络生成的数据为训练数据，因此需要研究这一过程在模型中的具体表现和影响。

Method: 研究指数族模型，推导出参数动态的运动方程，并分析最大似然估计、引入污染数据、最大后验估计及正则化对动态行为的影响。

Result: 最大似然估计会使充分统计量满足鞅性质，导致收敛到放大初始偏差的吸收状态，但适当的调整（如引入污染数据或正则化）可以避免这一结果。

Conclusion: 封闭环学习中参数更新可能带来问题，但通过改变数据分布或优化方法，可以有效改善学习动态行为。

Abstract: Closed-loop learning is the process of repeatedly estimating a model from
data generated from the model itself. It is receiving great attention due to
the possibility that large neural network models may, in the future, be
primarily trained with data generated by artificial neural networks themselves.
We study this process for models that belong to exponential families, deriving
equations of motions that govern the dynamics of the parameters. We show that
maximum likelihood estimation of the parameters endows sufficient statistics
with the martingale property and that as a result the process converges to
absorbing states that amplify initial biases present in the data. However, we
show that this outcome may be prevented by polluting the data with an
infinitesimal fraction of data points generated from a fixed model, by relying
on maximum a posteriori estimation or by introducing regularisation.
Furthermore, we show that the asymptotic behavior of the dynamics is not
reparametrisation invariant.

</details>


### [356] [Efficient Federated Learning with Encrypted Data Sharing for Data-Heterogeneous Edge Devices](https://arxiv.org/abs/2506.20644)
*Hangyu Li,Hongyue Wu,Guodong Fan,Zhen Zhang,Shizhan Chen,Zhiyong Feng*

Main category: cs.LG

TL;DR: 提出了一种新的联邦学习方法FedEDS，通过加密数据共享解决了数据异构性和训练延迟问题。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习忽视了网络拓扑、物理距离和数据异构性对边缘设备的影响，导致延迟增加和模型性能下降。

Method: 提出了FedEDS方法，使用客户端模型的随机层训练数据加密器，生成加密数据并共享，以及本地私有数据用于模型训练。

Result: 实验证明FedEDS能够加速联邦学习的收敛速度并改善模型性能。

Conclusion: FedEDS适合对快速收敛要求高的边缘设备应用服务，能有效缓解数据异构性对模型的负面影响。

Abstract: As privacy protection gains increasing importance, more models are being
trained on edge devices and subsequently merged into the central server through
Federated Learning (FL). However, current research overlooks the impact of
network topology, physical distance, and data heterogeneity on edge devices,
leading to issues such as increased latency and degraded model performance. To
address these issues, we propose a new federated learning scheme on edge
devices that called Federated Learning with Encrypted Data Sharing(FedEDS).
FedEDS uses the client model and the model's stochastic layer to train the data
encryptor. The data encryptor generates encrypted data and shares it with other
clients. The client uses the corresponding client's stochastic layer and
encrypted data to train and adjust the local model. FedEDS uses the client's
local private data and encrypted shared data from other clients to train the
model. This approach accelerates the convergence speed of federated learning
training and mitigates the negative impact of data heterogeneity, making it
suitable for application services deployed on edge devices requiring rapid
convergence. Experiments results show the efficacy of FedEDS in promoting model
performance.

</details>


### [357] [Mastering Multiple-Expert Routing: Realizable $H$-Consistency and Strong Guarantees for Learning to Defer](https://arxiv.org/abs/2506.20650)
*Anqi Mao,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: 提出了一种新的替代损失函数和算法，专注于优化专家分配以平衡准确性与计算成本，验证其在单阶段和两阶段推迟学习中的一致性和理论性能。


<details>
  <summary>Details</summary>
Motivation: 解决学习延迟的关键问题，特别是在如何有效地分配输入到专家以平衡准确性和计算成本之间的权衡。

Method: 引入新的替代损失函数，并提出可实现的H一致性和贝叶斯一致性分析，开发适用于单阶段和两阶段学习的有效算法，提供低噪声假设下的增强理论保证，最后用实验验证方法的有效性。

Result: 证明了所提出方法在单阶段和多专家两阶段场景中的可实现H一致性、H一致性边界和贝叶斯一致性，并在实验中表现出优于基线方法的性能。

Conclusion: 提供了一种理论上有保证且更优的学习推迟方法，适用于单阶段和两阶段场景，为多专家分配问题提供了强大的理论与实践工具。

Abstract: The problem of learning to defer with multiple experts consists of optimally
assigning input instances to experts, balancing the trade-off between their
accuracy and computational cost. This is a critical challenge in natural
language generation, but also in other fields such as image processing, and
medical diagnostics. Recent studies have proposed surrogate loss functions to
optimize deferral, but challenges remain in ensuring their consistency
properties. This paper introduces novel surrogate loss functions and efficient
algorithms with strong theoretical learning guarantees. We address open
questions regarding realizable $H$-consistency, $H$-consistency bounds, and
Bayes-consistency for both single-stage (jointly learning predictor and
deferral function) and two-stage (learning only the deferral function with a
fixed expert) learning scenarios. For single-stage deferral, we introduce a
family of new realizable $H$-consistent surrogate losses and further prove
$H$-consistency for a selected member. For two-stage deferral, we derive new
surrogate losses that achieve realizable $H$-consistency, $H$-consistency
bounds, and Bayes-consistency for the two-expert scenario and, under natural
assumptions, multiple-expert scenario. Additionally, we provide enhanced
theoretical guarantees under low-noise assumptions for both scenarios. Finally,
we report the results of experiments using our proposed surrogate losses,
comparing their performance against existing baselines.

</details>


### [358] [Hear No Evil: Detecting Gradient Leakage by Malicious Servers in Federated Learning](https://arxiv.org/abs/2506.20651)
*Fei Wang,Baochun Li*

Main category: cs.LG

TL;DR: 本文分析了在联邦学习（FL）中，恶意的梯度泄漏攻击如何通过操纵全局模型来暴露客户端的私有数据，并提出了一种简单有效的客户端检测机制来防御此类攻击。


<details>
  <summary>Details</summary>
Motivation: 为解决联邦学习中因梯度更新暴露私有数据的问题，尤其是恶意服务器通过操控全局模型导致的信息泄露，研究一种方法来提升系统隐私性和可检测性。

Method: 从防御者视角分析恶意梯度泄漏攻击的原理及其模型操控技术，研究这些攻击在重建私有数据与隐蔽性之间的矛盾，并提出一种轻量化的客户端检测机制。

Result: 发现恶意梯度泄漏攻击在现实的联邦学习环境中存在明显局限性，常规监测手段即可发现异常。同时提出的客户端检测机制能够有效标记可疑的模型更新。

Conclusion: 尽管恶意梯度泄漏攻击具有理论威胁性，但在现实设置中易被检测且具有限制性。文中提出的防护机制能以低代价提升系统安全性，为隐私意识强的联邦学习系统提供可靠保障。

Abstract: Recent work has shown that gradient updates in federated learning (FL) can
unintentionally reveal sensitive information about a client's local data. This
risk becomes significantly greater when a malicious server manipulates the
global model to provoke information-rich updates from clients. In this paper,
we adopt a defender's perspective to provide the first comprehensive analysis
of malicious gradient leakage attacks and the model manipulation techniques
that enable them. Our investigation reveals a core trade-off: these attacks
cannot be both highly effective in reconstructing private data and sufficiently
stealthy to evade detection -- especially in realistic FL settings that
incorporate common normalization techniques and federated averaging.
  Building on this insight, we argue that malicious gradient leakage attacks,
while theoretically concerning, are inherently limited in practice and often
detectable through basic monitoring. As a complementary contribution, we
propose a simple, lightweight, and broadly applicable client-side detection
mechanism that flags suspicious model updates before local training begins,
despite the fact that such detection may not be strictly necessary in realistic
FL settings. This mechanism further underscores the feasibility of defending
against these attacks with minimal overhead, offering a deployable safeguard
for privacy-conscious federated learning systems.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [359] [Higher-Order Neuromorphic Ising Machines -- Autoencoders and Fowler-Nordheim Annealers are all you need for Scalability](https://arxiv.org/abs/2506.19964)
*Faiek Ahsan,Saptarshi Maiti,Zihao Chen,Jakob Kaiser,Ankita Nandi,Madhuvanthi Srivatsav,Johannes Schemmel,Andreas G. Andreou,Jason Eshraghian,Chetan Singh Thakur,Shantanu Chakrabartty*

Main category: cs.NE

TL;DR: 提出了一种高阶神经形态伊辛机，能在复杂度和解决质量上优于传统基于二次化的架构，主要通过使用异步自动编码器并结合关联矩阵稀疏性质实现。


<details>
  <summary>Details</summary>
Motivation: 探索更高效、更高质量解决组合优化问题的机器架构，同时克服传统方法在复杂度上的不足。

Method: 引入异步自动编码器，用量子隧穿退火方法优化基于高阶交互的伊辛子句；提高结果可扩展性和速度，同时利用FPGA硬件协同设计。

Result: 该架构在多种基准优化问题（如MAX-CUT和MAX-SAT）中优于二阶伊辛机，提供了更高质量且更快速的解决方案。

Conclusion: 文中证明了自动编码器结合Fowler-Nordheim退火技术对于任何阶的神经形态伊辛机的可靠性和可扩展性是足够的，并显示了这种方法在硬件支持下的潜力。

Abstract: We report a higher-order neuromorphic Ising machine that exhibits superior
scalability compared to architectures based on quadratization, while also
achieving state-of-the-art quality and reliability in solutions with
competitive time-to-solution metrics. At the core of the proposed machine is an
asynchronous autoencoder architecture that captures higher-order interactions
by directly manipulating Ising clauses instead of Ising spins, thereby
maintaining resource complexity independent of interaction order. Asymptotic
convergence to the Ising ground state is ensured by sampling the autoencoder
latent space defined by the spins, based on the annealing dynamics of the
Fowler-Nordheim quantum mechanical tunneling. To demonstrate the advantages of
the proposed higher-order neuromorphic Ising machine, we systematically solved
benchmark combinatorial optimization problems such as MAX-CUT and MAX-SAT,
comparing the results to those obtained using a second-order Ising machine
employing the same annealing process. Our findings indicate that the proposed
architecture consistently provides higher quality solutions in shorter time
frames compared to the second-order model across multiple runs. Additionally,
we show that the techniques based on the sparsity of the interconnection
matrix, such as graph coloring, can be effectively applied to higher-order
neuromorphic Ising machines, enhancing the solution quality and the
time-to-solution. The time-to-solution can be further improved through hardware
co-design, as demonstrated in this paper using a field-programmable gate array
(FPGA). The results presented in this paper provide further evidence that
autoencoders and Fowler-Nordheim annealers are sufficient to achieve
reliability and scaling of any-order neuromorphic Ising machines.

</details>


### [360] [Surrogate-Assisted Evolution for Efficient Multi-branch Connection Design in Deep Neural Networks](https://arxiv.org/abs/2506.20469)
*Fergal Stapleton,Daniel García Núñez,Yanan Sun,Edgar Galván*

Main category: cs.NE

TL;DR: 本研究提出了一种基于线性遗传编程（LGP）的方法，用于在深度神经网络（DNNs）中编码多分支连接，通过进化算法（EAs）实现高效网络架构搜索。


<details>
  <summary>Details</summary>
Motivation: 当前多分支深度神经网络尽管在特征提取和未见数据的泛化能力上表现卓越，但其训练和架构优化过程计算成本较高，因此需要更高效的自动化搜索方法。

Method: 本研究使用线性遗传编程（LGP）来编码DNN的多分支连接，提出了NeuroLGP-MB方法。同时，通过结合非简单人工神经网络的语义辅助进化算法和更先进的代理模型，提高了大规模DNN架构搜索的效率。

Result: 新提出的代理模型在性能上优于基准方法以及计算成本更高的简单代理模型，在大规模DNN设计中展现了强大的潜力。

Conclusion: 本文成功通过进化算法和语义辅助代理模型实现高效DNN架构搜索，为进一步优化DNN设计提供了一种新颖高效的方案。

Abstract: State-of-the-art Deep Neural Networks (DNNs) often incorporate multi-branch
connections, enabling multi-scale feature extraction and enhancing the capture
of diverse features. This design improves network capacity and generalisation
to unseen data. However, training such DNNs can be computationally expensive.
The challenge is further exacerbated by the complexity of identifying optimal
network architectures. To address this, we leverage Evolutionary Algorithms
(EAs) to automatically discover high-performing architectures, a process
commonly known as neuroevolution. We introduce a novel approach based on Linear
Genetic Programming (LGP) to encode multi-branch (MB) connections within DNNs,
referred to as NeuroLGP-MB. To efficiently design the DNNs, we use
surrogate-assisted EAs. While their application in simple artificial neural
networks has been influential, we scale their use from dozens or hundreds of
sample points to thousands, aligning with the demands of complex DNNs by
incorporating a semantic-based approach in our surrogate-assisted EA.
Furthermore, we introduce a more advanced surrogate model that outperforms
baseline, computationally expensive, and simpler surrogate models.

</details>


### [361] [Higher-Order Neuromorphic Ising Machines -- Autoencoders and Fowler-Nordheim Annealers are all you need for Scalability](https://arxiv.org/abs/2506.19964)
*Faiek Ahsan,Saptarshi Maiti,Zihao Chen,Jakob Kaiser,Ankita Nandi,Madhuvanthi Srivatsav,Johannes Schemmel,Andreas G. Andreou,Jason Eshraghian,Chetan Singh Thakur,Shantanu Chakrabartty*

Main category: cs.NE

TL;DR: 本文提出了一种高阶神经形态伊辛机，具备超越基于二次化架构的扩展能力，同时在解决质量和可靠性上达到业界领先，并具备竞争力的求解时间。


<details>
  <summary>Details</summary>
Motivation: 现有的伊辛机在处理高阶相互作用时扩展性差且资源复杂度较高，本文旨在通过改进架构实现高速与高质量的求解。

Method: 设计了一种基于异步自编码器的新型架构，直接操作伊辛子句实现高阶相互作用，并结合Fowler-Nordheim量子隧穿退火采样自编码器潜在空间，从而确保收敛到伊辛基态。

Result: 实验表明新架构在多个基准优化问题中提供了比二阶模型更高的解质量和更短的求解时间。此外，通过硬件协同设计（如FPGA）进一步改进了求解时间。

Conclusion: 本文验证了基于自编码器和Fowler-Nordheim退火器的架构具有可靠性和可扩展性，适用于任意阶的神经形态伊辛机。

Abstract: We report a higher-order neuromorphic Ising machine that exhibits superior
scalability compared to architectures based on quadratization, while also
achieving state-of-the-art quality and reliability in solutions with
competitive time-to-solution metrics. At the core of the proposed machine is an
asynchronous autoencoder architecture that captures higher-order interactions
by directly manipulating Ising clauses instead of Ising spins, thereby
maintaining resource complexity independent of interaction order. Asymptotic
convergence to the Ising ground state is ensured by sampling the autoencoder
latent space defined by the spins, based on the annealing dynamics of the
Fowler-Nordheim quantum mechanical tunneling. To demonstrate the advantages of
the proposed higher-order neuromorphic Ising machine, we systematically solved
benchmark combinatorial optimization problems such as MAX-CUT and MAX-SAT,
comparing the results to those obtained using a second-order Ising machine
employing the same annealing process. Our findings indicate that the proposed
architecture consistently provides higher quality solutions in shorter time
frames compared to the second-order model across multiple runs. Additionally,
we show that the techniques based on the sparsity of the interconnection
matrix, such as graph coloring, can be effectively applied to higher-order
neuromorphic Ising machines, enhancing the solution quality and the
time-to-solution. The time-to-solution can be further improved through hardware
co-design, as demonstrated in this paper using a field-programmable gate array
(FPGA). The results presented in this paper provide further evidence that
autoencoders and Fowler-Nordheim annealers are sufficient to achieve
reliability and scaling of any-order neuromorphic Ising machines.

</details>


### [362] [Surrogate-Assisted Evolution for Efficient Multi-branch Connection Design in Deep Neural Networks](https://arxiv.org/abs/2506.20469)
*Fergal Stapleton,Daniel García Núñez,Yanan Sun,Edgar Galván*

Main category: cs.NE

TL;DR: 提出了一种基于线性遗传编程的多分支深度神经网络架构搜索方法（NeuroLGP-MB），利用进化算法和增强的代理建模进行高效设计。


<details>
  <summary>Details</summary>
Motivation: 当前多分支深度神经网络具有提取多尺度特征和提升数据泛化能力的优点，但训练成本高昂，架构搜索复杂。作者旨在通过进化算法自动化架构搜索，降低成本，提高效率。

Method: 采用基于线性遗传编程的编码方式，引入语义导向的代理辅助进化算法，并设计了一种先进的代理模型以实现高效的网络设计。

Result: 所提出的方法可以大规模应用于复杂DNN的架构设计，所使用的代理模型在表现上优于传统且高计算复杂度的模型。

Conclusion: 通过NeuroLGP-MB和优化的代理模型，显著提升了复杂DNN设计的效率，为神经进化应用于深度学习架构搜索奠定了基础。

Abstract: State-of-the-art Deep Neural Networks (DNNs) often incorporate multi-branch
connections, enabling multi-scale feature extraction and enhancing the capture
of diverse features. This design improves network capacity and generalisation
to unseen data. However, training such DNNs can be computationally expensive.
The challenge is further exacerbated by the complexity of identifying optimal
network architectures. To address this, we leverage Evolutionary Algorithms
(EAs) to automatically discover high-performing architectures, a process
commonly known as neuroevolution. We introduce a novel approach based on Linear
Genetic Programming (LGP) to encode multi-branch (MB) connections within DNNs,
referred to as NeuroLGP-MB. To efficiently design the DNNs, we use
surrogate-assisted EAs. While their application in simple artificial neural
networks has been influential, we scale their use from dozens or hundreds of
sample points to thousands, aligning with the demands of complex DNNs by
incorporating a semantic-based approach in our surrogate-assisted EA.
Furthermore, we introduce a more advanced surrogate model that outperforms
baseline, computationally expensive, and simpler surrogate models.

</details>
