<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 85]
- [cs.CL](#cs.CL) [Total: 103]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.LG](#cs.LG) [Total: 53]
- [cs.NE](#cs.NE) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs](https://arxiv.org/abs/2506.21656)
*Yifan Shen,Yuanzhe Liu,Jingyuan Zhu,Xu Cao,Xiaofeng Zhang,Yixiao He,Wenming Ye,James Matthew Rehg,Ismini Lourentzou*

Main category: cs.CV

TL;DR: 本文提出了SpatialReasoner-R1，一个改进空间推理能力的视觉语言模型，并通过fDPO策略大幅提升了任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在精细的空间推理上表现不足，尤其是涉及多步逻辑和精确空间对齐时。本文旨在解决这一问题。

Method: 设计了多模型蒙特卡洛树搜索(M3CTS)方法生成逻辑一致的推理路径；提出了细化的直接偏好优化(fDPO)以改进描述性匹配和逻辑推理，并基于视觉一致性、空间对齐和逻辑连贯性进行奖励机制指导。

Result: fDPO在空间质量任务上平均提升4.1%，在空间数量任务上提升9.0%；SpatialReasoner-R1在SPATIALRGPT-Bench基准测试中精度超过现有最优模型9.8%。

Conclusion: 利用fDPO优化后的SpatialReasoner-R1显著提高了视觉语言模型在空间推理任务上的性能，同时保持了通用任务的竞争力。

Abstract: Current Vision-Language Models (VLMs) struggle with fine-grained spatial
reasoning, particularly when multi-step logic and precise spatial alignment are
required. In this work, we introduce SpatialReasoner-R1, a vision-language
reasoning model designed to address these limitations. To construct
high-quality supervision for spatial reasoning, we design a Multi-Model Monte
Carlo Tree Search (M3CTS) method that generates diverse, logically consistent
Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose
fine-grained Direct Preference Optimization (fDPO), which introduces
segment-specific preference granularity for descriptive grounding and logical
reasoning, guided by a spatial reward mechanism that evaluates candidate
responses based on visual consistency, spatial grounding, and logical
coherence. Experimental results demonstrate that fDPO achieves an average
improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%
gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a
new SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in
average accuracy, while maintaining competitive performance on general
vision-language tasks.

</details>


### [2] [TanDiT: Tangent-Plane Diffusion Transformer for High-Quality 360° Panorama Generation](https://arxiv.org/abs/2506.21681)
*Hakan Çapuk,Andrew Bond,Muhammed Burak Kızıl,Emir Göçen,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

TL;DR: TanDiT通过生成网格化的切平面图像实现360°全景图像生成，采用统一扩散模型与无模型后处理步骤，并提出评估新标准。


<details>
  <summary>Details</summary>
Motivation: 旨在解决全景图像生成的几何失真及无缝连接难题，同时利用现有生成模型的优势。

Method: TanDiT通过单一扩散模型生成覆盖全景的切平面图像网格，并引入模型无关的后处理步骤。此外，定义了TangetIS与TangentFID作为评估全景图像质量新指标。

Result: 实验表明，TanDiT具备超越训练数据的泛化能力，能理解复杂文本提示，与多种生成模型结合后生成高质量的多样化全景图像。

Conclusion: TanDiT为全景图像生成提供了新方法，解决了传统方法存在的问题，兼具创新性和实用性。

Abstract: Recent advances in image generation have led to remarkable improvements in
synthesizing perspective images. However, these models still struggle with
panoramic image generation due to unique challenges, including varying levels
of geometric distortion and the requirement for seamless loop-consistency. To
address these issues while leveraging the strengths of the existing models, we
introduce TanDiT, a method that synthesizes panoramic scenes by generating
grids of tangent-plane images covering the entire 360$^\circ$ view. Unlike
previous methods relying on multiple diffusion branches, TanDiT utilizes a
unified diffusion model trained to produce these tangent-plane images
simultaneously within a single denoising iteration. Furthermore, we propose a
model-agnostic post-processing step specifically designed to enhance global
coherence across the generated panoramas. To accurately assess panoramic image
quality, we also present two specialized metrics, TangentIS and TangentFID, and
provide a comprehensive benchmark comprising captioned panoramic datasets and
standardized evaluation scripts. Extensive experiments demonstrate that our
method generalizes effectively beyond its training data, robustly interprets
detailed and complex text prompts, and seamlessly integrates with various
generative models to yield high-quality, diverse panoramic images.

</details>


### [3] [FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering](https://arxiv.org/abs/2506.21710)
*Liangyu Zhong,Fabio Rosenthal,Joachim Sicking,Fabian Hüger,Thorsten Bagdonat,Hanno Gottschalk,Leo Schwinn*

Main category: cs.CV

TL;DR: 本文设计了一种名为FOCUS的无训练视觉裁剪方法，利用多模态大语言模型(MLLM)内部表征，提升视觉问答(VQA)中细粒度问题的解答能力。


<details>
  <summary>Details</summary>
Motivation: 针对VQA中细节捕捉的挑战，现有视觉裁剪方法存在高成本、效率低下或与高效注意力机制不兼容等问题，亟需一种高效且通用的解决方案。

Method: 提出FOCUS方法，通过以下四个步骤实现：1)从VQA提示中识别目标对象；2)利用KV缓存生成目标相关性图；3)根据相关性图生成并排序图像区域；4)对最相关区域进行精细VQA任务。

Result: FOCUS在四个细粒度VQA数据集和两种MLLM上表现优异，同时超越了三种流行视觉裁剪方法的准确性和效率表现，并以3-6.5倍的计算优势匹敌现有最佳方法ZoomEye。

Conclusion: FOCUS无需额外训练，高效且性能卓越，为解决细粒度VQA问题提供了新思路，在多模态视觉问答领域显示出巨大潜力。

Abstract: While Multimodal Large Language Models (MLLMs) offer strong perception and
reasoning capabilities for image-text input, Visual Question Answering (VQA)
focusing on small image details still remains a challenge. Although visual
cropping techniques seem promising, recent approaches have several limitations:
the need for task-specific fine-tuning, low efficiency due to uninformed
exhaustive search, or incompatibility with efficient attention implementations.
We address these shortcomings by proposing a training-free visual cropping
method, dubbed FOCUS, that leverages MLLM-internal representations to guide the
search for the most relevant image region. This is accomplished in four steps:
first, we identify the target object(s) in the VQA prompt; second, we compute
an object relevance map using the key-value (KV) cache; third, we propose and
rank relevant image regions based on the map; and finally, we perform the
fine-grained VQA task using the top-ranked region. As a result of this informed
search strategy, FOCUS achieves strong performance across four fine-grained VQA
datasets and two types of MLLMs. It outperforms three popular visual cropping
methods in both accuracy and efficiency, and matches the best-performing
baseline, ZoomEye, while requiring 3 - 6.5 x less compute.

</details>


### [4] [CAST: Cross-Attentive Spatio-Temporal feature fusion for Deepfake detection](https://arxiv.org/abs/2506.21711)
*Aryan Thakre,Omkar Nagwekar,Vedang Talekar,Aparna Santra Biswas*

Main category: cs.CV

TL;DR: 提出了结合CNN和Transformer的新方法CAST，通过交叉注意力机制融合时空特征，有效提升了深度伪造检测的性能。


<details>
  <summary>Details</summary>
Motivation: 深度伪造对数字媒体真实性构成威胁，现有方法在时空特征交互深度上存在局限。

Method: 采用统一的CAST模型利⽤交叉注意力融合空间与时间特征，提高对深度伪造细微动态特征的检测能力。

Result: 在FaceForensics++、Celeb-DF和DeepfakeDetection数据集上表现优异，尤其是在未见数据集上取得93.31%的AUC表现。

Conclusion: 基于交叉注意力的特征融合能显著提升深度伪造视频检测的鲁棒性和泛化性。

Abstract: Deepfakes have emerged as a significant threat to digital media authenticity,
increasing the need for advanced detection techniques that can identify subtle
and time-dependent manipulations. CNNs are effective at capturing spatial
artifacts, and Transformers excel at modeling temporal inconsistencies.
However, many existing CNN-Transformer models process spatial and temporal
features independently. In particular, attention-based methods often use
separate attention mechanisms for spatial and temporal features and combine
them using naive approaches like averaging, addition, or concatenation, which
limits the depth of spatio-temporal interaction. To address this challenge, we
propose a unified CAST model that leverages cross-attention to effectively fuse
spatial and temporal features in a more integrated manner. Our approach allows
temporal features to dynamically attend to relevant spatial regions, enhancing
the model's ability to detect fine-grained, time-evolving artifacts such as
flickering eyes or warped lips. This design enables more precise localization
and deeper contextual understanding, leading to improved performance across
diverse and challenging scenarios. We evaluate the performance of our model
using the FaceForensics++, Celeb-DF, and DeepfakeDetection datasets in both
intra- and cross-dataset settings to affirm the superiority of our approach.
Our model achieves strong performance with an AUC of 99.49 percent and an
accuracy of 97.57 percent in intra-dataset evaluations. In cross-dataset
testing, it demonstrates impressive generalization by achieving a 93.31 percent
AUC on the unseen DeepfakeDetection dataset. These results highlight the
effectiveness of cross-attention-based feature fusion in enhancing the
robustness of deepfake video detection.

</details>


### [5] [Elucidating and Endowing the Diffusion Training Paradigm for General Image Restoration](https://arxiv.org/abs/2506.21722)
*Xin Lu,Xueyang Fu,Jie Xiao,Zihao Fan,Yurui Zhu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新型图像恢复框架，将扩散模型的训练范式融入主流通用图像恢复网络，并通过实验验证该框架在单任务和多任务场景下的出色性能。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型尽管在图像恢复任务中展现出强大的生成能力，但其复杂的架构和迭代过程限制了实际应用。现有方法更多关注于优化网络架构和扩散路径，而忽略了在通用图像恢复框架中整合扩散训练范式的潜力。

Method: 通过系统分析时间步依赖性、网络层级、噪声水平关系和多恢复任务关联性，提出了一种将扩散训练范式适配于通用图像恢复的框架。同时引入正则化策略，协调扩散目标与图像恢复任务目标，并开发增量训练范式和任务特定适配器。

Result: 实验表明，该方法显著提升了单任务图像恢复网络的泛化能力，并在多任务图像恢复中取得了更优性能。

Conclusion: 所提出的图像恢复框架能够无缝融入现有的通用图像恢复架构，并在单任务和多任务场景下展现出显著改进效果。

Abstract: While diffusion models demonstrate strong generative capabilities in image
restoration (IR) tasks, their complex architectures and iterative processes
limit their practical application compared to mainstream reconstruction-based
general ordinary IR networks. Existing approaches primarily focus on optimizing
network architecture and diffusion paths but overlook the integration of the
diffusion training paradigm within general ordinary IR frameworks. To address
these challenges, this paper elucidates key principles for adapting the
diffusion training paradigm to general IR training through systematic analysis
of time-step dependencies, network hierarchies, noise-level relationships, and
multi-restoration task correlations, proposing a new IR framework supported by
diffusion-based training. To enable IR networks to simultaneously restore
images and model generative representations, we introduce a series of
regularization strategies that align diffusion objectives with IR tasks,
improving generalization in single-task scenarios. Furthermore, recognizing
that diffusion-based generation exerts varying influences across different IR
tasks, we develop an incremental training paradigm and task-specific adaptors,
further enhancing performance in multi-task unified IR. Experiments demonstrate
that our method significantly improves the generalization of IR networks in
single-task IR and achieves superior performance in multi-task unified IR.
Notably, the proposed framework can be seamlessly integrated into existing
general IR architectures.

</details>


### [6] [Asymmetric Dual Self-Distillation for 3D Self-Supervised Representation Learning](https://arxiv.org/abs/2506.21724)
*Remco F. Leijenaar,Hamidreza Kasaei*

Main category: cs.CV

TL;DR: 该研究提出了一种名为AsymDSD的框架，通过引入非对称设计和面向点云的改进，实现了状态最先进的模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云无监督学习方法面临高语义表示难以捕捉的问题。研究动机是弥补仅基于重建目标的自监督方法的局限，提升高层语义捕获能力。

Method: 提出AsymDSD框架，通过潜在空间的预测代替输入空间重建，结合非对称设计、禁止屏蔽查询间注意力以及多遮罩采样等技术，改进点云建模。

Result: AsymDSD 在 ScanObjectNN 数据集上取得 90.53%的分类精度，预训练后提升至 93.72%，显著超过前人方法。

Conclusion: AsymDSD框架通过融合遮罩建模与语义不变性学习，有效提升了无监督3D点云表征学习的性能。

Abstract: Learning semantically meaningful representations from unstructured 3D point
clouds remains a central challenge in computer vision, especially in the
absence of large-scale labeled datasets. While masked point modeling (MPM) is
widely used in self-supervised 3D learning, its reconstruction-based objective
can limit its ability to capture high-level semantics. We propose AsymDSD, an
Asymmetric Dual Self-Distillation framework that unifies masked modeling and
invariance learning through prediction in the latent space rather than the
input space. AsymDSD builds on a joint embedding architecture and introduces
several key design choices: an efficient asymmetric setup, disabling attention
between masked queries to prevent shape leakage, multi-mask sampling, and a
point cloud adaptation of multi-crop. AsymDSD achieves state-of-the-art results
on ScanObjectNN (90.53%) and further improves to 93.72% when pretrained on 930k
shapes, surpassing prior methods.

</details>


### [7] [Exploring Image Generation via Mutually Exclusive Probability Spaces and Local Correlation Hypothesis](https://arxiv.org/abs/2506.21731)
*Chenqiu Zhao,Anup Basu*

Main category: cs.CV

TL;DR: 本文提出了两个理论框架，Mutually Exclusive Probability Space（MESP）和Local Correlation Hypothesis（LCH），以探讨生成模型全局分布学习的局限性，并针对这一问题设计了新的模型和方法。


<details>
  <summary>Details</summary>
Motivation: 探讨生成模型学习全局分布时容易导致记忆化而非生成行为的问题，以及如何通过改进框架来解决潜变量分布出现的优化冲突问题。

Method: 提出了Mutually Exclusive Probability Spaces理论，基于此设计了Binary Latent Autoencoder（BL-AE）来生成二进制潜变量，并通过改进的自回归模型Autoregressive Random Variable Model（ARVM）来进行生成建模。同时提出Local Correlation Hypothesis以改进生成能力。

Result: ARVM模型在标准数据集上取得了竞争性的FID分数，超越了当前最先进方法，但观察到这些分数更多反映记忆化而非生成能力。

Conclusion: 新框架和模型揭示了现有生成模型固有的优化冲突，并提出了改进方法。尽管取得了一些成果，但仍需进一步改进以解决生成能力和记忆化之间的平衡问题。

Abstract: We propose two theoretical frameworks, the Mutually Exclusive Probability
Space (MESP) and the Local Correlation Hypothesis (LCH), to explore a potential
limitation in probabilistic generative models; namely that learning global
distributions leads to memorization rather than generative behavior. MESP
emerges from our rethinking of the Variational Autoencoder (VAE). We observe
that latent variable distributions in VAE exhibit overlap, which leads to an
optimization conflict between the reconstruction loss and KL-divergence loss. A
lower bound based on the overlap coefficient is proposed. We refer to this
phenomenon as Mutually Exclusive Probability Spaces. Based on MESP, a Binary
Latent Autoencoder (BL-AE) is proposed to encode images into binary latent
representations. These binary latents are used as the input to our
Autoregressive Random Variable Model (ARVM), a modified autoregressive model
outputting histograms. Our ARVM achieves competitive FID scores, outperforming
state-of-the-art methods on standard datasets. However, such scores reflect
memorization rather than generation. To address this issue, we propose the
Local Correlation Hypothesis (LCH), which posits that generative capability
arising from local correlations among latent variables. Comprehensive
experiments and discussions are conducted to validate our frameworks.

</details>


### [8] [Equitable Federated Learning with NCA](https://arxiv.org/abs/2506.21735)
*Nick Lemke,Mirko Konstantin,Henry John Krumb,John Kalkhof,Jonathan Stieber,Anirban Mukhopadhyay*

Main category: cs.CV

TL;DR: 本文提出了一种名为FedNCA的联邦学习系统，专为医疗影像分割设计，解决了低中收入国家的资源限制问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决低中收入国家在联邦学习应用上的高计算成本和不稳定网络问题，提升医疗影像分割的可行性。

Method: 提出了轻量化的Med-NCA架构，使其能够在低成本边缘设备上运行，并设计了适合于不稳定网络且支持加密的联邦学习系统FedNCA。

Result: FedNCA实现了在智能手机等低成本设备上的高效联邦学习，同时降低了通信成本，并适应了受限的网络环境。

Conclusion: FedNCA为资源受限地区提供了安全、高效和轻量化的医疗影像解决方案，促进了医疗公平发展。

Abstract: Federated Learning (FL) is enabling collaborative model training across
institutions without sharing sensitive patient data. This approach is
particularly valuable in low- and middle-income countries (LMICs), where access
to trained medical professionals is limited. However, FL adoption in LMICs
faces significant barriers, including limited high-performance computing
resources and unreliable internet connectivity. To address these challenges, we
introduce FedNCA, a novel FL system tailored for medical image segmentation
tasks. FedNCA leverages the lightweight Med-NCA architecture, enabling training
on low-cost edge devices, such as widely available smartphones, while
minimizing communication costs. Additionally, our encryption-ready FedNCA
proves to be suitable for compromised network communication. By overcoming
infrastructural and security challenges, FedNCA paves the way for inclusive,
efficient, lightweight, and encryption-ready medical imaging solutions,
fostering equitable healthcare advancements in resource-constrained regions.

</details>


### [9] [ImplicitQA: Going beyond frames towards Implicit Video Reasoning](https://arxiv.org/abs/2506.21742)
*Sirnam Swetha,Rohit Gupta,Parth Parag Kulkarni,David G Shatwell,Jeffrey A Chan Santiago,Nyle Siddiqui,Joseph Fioresi,Mubarak Shah*

Main category: cs.CV

TL;DR: 提出了一个名为ImplicitQA的新基准数据集，专注于测试视频问答模型的隐式推理能力，可通过网站获取数据集。


<details>
  <summary>Details</summary>
Motivation: 当前的视频QA系统和基准测试主要依赖表层视觉信息，未能捕捉人类类似的隐式推理能力，尤其是从连续内容中推断因果关系及动机等更深层次信息，这限制了模型对叙事性视频的理解。

Method: 设计了ImplicitQA基准数据集，包括1000对高质量问题答案对，来源于320多段创意视频片段，涵盖多个推理维度如空间、因果、社交互动等。通过人工精心标注以增加挑战性，评估现有模型在隐式推理任务中的表现。

Result: 现有领先视频QA模型在隐式问题上的表现明显下降，展示了它们对表层视觉线索的依赖，同时模型间的性能差异进一步说明了该任务的复杂性和多样性。

Conclusion: 通过发布数据集和数据收集框架，研究者希望激励学界开发更强大、更有深度的视频QA系统，以应对隐式推理的挑战。

Abstract: Video QA has made significant strides by leveraging multimodal learning to
align visual and textual modalities. However, current benchmarks overwhelmingly
focus on questions answerable through explicit visual content - actions,
objects & events directly observable within individual frames or short clips.
In contrast, creative and cinematic videos - such as movies, TV shows, and
narrative-driven content - employ storytelling techniques that deliberately
omit certain depictions, requiring viewers to infer motives, causality, and
relationships across discontinuous frames. Humans naturally excel at such
implicit reasoning, seamlessly integrating information across time and context
to construct coherent narratives. Current VideoQA systems and benchmarks fail
to capture this essential dimension of human-like understanding. To bridge this
gap, we present ImplicitQA, a novel benchmark specifically designed to test
models on implicit reasoning. It comprises 1K meticulously annotated QA pairs
derived from 320+ high-quality creative video clips, systematically categorized
into key reasoning dimensions: lateral and vertical spatial reasoning, depth
and proximity, viewpoint and visibility, motion and trajectory, causal and
motivational reasoning, social interactions, physical context, and inferred
counting. These annotations are deliberately challenging, crafted by authors
ensuring high-quality. Our extensive evaluations on leading VideoQA models
reveals performance degradation, underscoring their reliance on surface-level
visual cues and highlighting the difficulty of implicit reasoning. Performance
variations across models further illustrate the complexity and diversity of the
challenges presented by ImplicitQA. By releasing both the dataset and our data
collection framework, we aim to stimulate further research and development in
the community. https://huggingface.co/datasets/ucf-crcv/ImplicitQA.

</details>


### [10] [Early Glaucoma Detection using Deep Learning with Multiple Datasets of Fundus Images](https://arxiv.org/abs/2506.21770)
*Rishiraj Paul Chowdhury,Nirmit Shekar Karkera*

Main category: cs.CV

TL;DR: 提出一种基于EfficientNet-B0架构的深度学习模型，从视网膜眼底图像中检测青光眼，表现出较高的AUC-ROC和强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前传统青光眼诊断方法通常具有侵入性且需要专门设备，研究旨在通过深度学习方法提供一个高效、可扩展的早期检测方案。

Method: 使用EfficientNet-B0架构，并在多个数据集（ACRIMA、ORIGA和RIM-ONE）上进行顺序训练及微调，以提升模型的泛化能力。

Result: 实验表明，简单的预处理方法比复杂处理取得更高的AUC-ROC，模型在未见过的数据集上展现出强大的判别能力。

Conclusion: 该方法为早期青光眼检测提供了一种高复现性和可扩展的解决方法，展现了潜在的临床应用价值。

Abstract: Glaucoma is a leading cause of irreversible blindness, but early detection
can significantly improve treatment outcomes. Traditional diagnostic methods
are often invasive and require specialized equipment. In this work, we present
a deep learning pipeline using the EfficientNet-B0 architecture for glaucoma
detection from retinal fundus images. Unlike prior studies that rely on single
datasets, we sequentially train and fine-tune our model across ACRIMA, ORIGA,
and RIM-ONE datasets to enhance generalization. Our experiments show that
minimal preprocessing yields higher AUC-ROC compared to more complex
enhancements, and our model demonstrates strong discriminative performance on
unseen datasets. The proposed pipeline offers a reproducible and scalable
approach to early glaucoma detection, supporting its potential clinical
utility.

</details>


### [11] [Comparing Learning Paradigms for Egocentric Video Summarization](https://arxiv.org/abs/2506.21785)
*Daniel Wen*

Main category: cs.CV

TL;DR: 研究探讨了监督学习、无监督学习和提示微调在理解和解释第一人称视频数据中的表现，发现通用的GPT-4o在这种任务上优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现有模型在处理第一人称视频数据时表现不佳的问题，并为该领域发展提供新的方法。

Method: 分析并比较了三种计算机视觉范式：监督学习（Shotluck Holmes）、无监督学习（TAC-SUM）和提示微调模型（GPT-4o）的性能。

Result: 发现现有最先进模型在第一人称视频上的表现明显弱于第三人称视频，而通用的提示微调模型GPT-4o表现优异。

Conclusion: 现有方法在第一人称视频处理方面存在局限，提示微调方法显示了重要潜力，下一步需针对这些挑战进行进一步研究。

Abstract: In this study, we investigate various computer vision paradigms - supervised
learning, unsupervised learning, and prompt fine-tuning - by assessing their
ability to understand and interpret egocentric video data. Specifically, we
examine Shotluck Holmes (state-of-the-art supervised learning), TAC-SUM
(state-of-the-art unsupervised learning), and GPT-4o (a prompt fine-tuned
pre-trained model), evaluating their effectiveness in video summarization. Our
results demonstrate that current state-of-the-art models perform less
effectively on first-person videos compared to third-person videos,
highlighting the need for further advancements in the egocentric video domain.
Notably, a prompt fine-tuned general-purpose GPT-4o model outperforms these
specialized models, emphasizing the limitations of existing approaches in
adapting to the unique challenges of first-person perspectives. Although our
evaluation is conducted on a small subset of egocentric videos from the
Ego-Exo4D dataset due to resource constraints, the primary objective of this
research is to provide a comprehensive proof-of-concept analysis aimed at
advancing the application of computer vision techniques to first-person videos.
By exploring novel methodologies and evaluating their potential, we aim to
contribute to the ongoing development of models capable of effectively
processing and interpreting egocentric perspectives.

</details>


### [12] [CAT-SG: A Large Dynamic Scene Graph Dataset for Fine-Grained Understanding of Cataract Surgery](https://arxiv.org/abs/2506.21813)
*Felix Holm,Gözde Ünver,Ghazal Ghazaei,Nassir Navab*

Main category: cs.CV

TL;DR: 论文提出了一个新的白内障手术场景图数据集（CAT-SG）以及一个场景图生成模型（CatSGG），以更好地理解手术流程中复杂的工具、组织和技术关系。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集倾向于解决手术分析的孤立方面（如工具检测或阶段分割），缺乏在整个手术过程中捕捉实体之间语义关系的能力。

Method: 创建了CAT-SG数据集，详细标注了工具与组织的交互、流程变化及时间依赖性，同时开发了新的场景图生成模型CatSGG，用于生成结构化的手术表示。

Result: 实验表明，CatSGG在生成结构化的手术数据表示方面优于当前的方法。

Conclusion: CAT-SG数据集和CatSGG模型为AI驱动的手术培训、实时决策支持和流程分析提供了有力的支持，为临床实践中的智能化系统铺平了道路。

Abstract: Understanding the intricate workflows of cataract surgery requires modeling
complex interactions between surgical tools, anatomical structures, and
procedural techniques. Existing datasets primarily address isolated aspects of
surgical analysis, such as tool detection or phase segmentation, but lack
comprehensive representations that capture the semantic relationships between
entities over time. This paper introduces the Cataract Surgery Scene Graph
(CAT-SG) dataset, the first to provide structured annotations of tool-tissue
interactions, procedural variations, and temporal dependencies. By
incorporating detailed semantic relations, CAT-SG offers a holistic view of
surgical workflows, enabling more accurate recognition of surgical phases and
techniques. Additionally, we present a novel scene graph generation model,
CatSGG, which outperforms current methods in generating structured surgical
representations. The CAT-SG dataset is designed to enhance AI-driven surgical
training, real-time decision support, and workflow analysis, paving the way for
more intelligent, context-aware systems in clinical practice.

</details>


### [13] [Few-Shot Segmentation of Historical Maps via Linear Probing of Vision Foundation Models](https://arxiv.org/abs/2506.21826)
*Rafael Sterzinger,Marco Peer,Robert Sablatnig*

Main category: cs.CV

TL;DR: 该论文提出了一种用于少样本历史地图分割的简便有效方法，在基准数据集上表现优异，并显著减少了人工标注需求。


<details>
  <summary>Details</summary>
Motivation: 历史地图在记录历史变化中具有重要意义，但其多样的视觉表现和有限的标注数据对自动化处理提出了挑战。

Method: 结合大型视觉基础模型的语义嵌入与高效参数微调，提出少样本历史地图分割方法，并通过少量可训练参数实现高效性能。

Result: 在Siegfried数据集上取得葡萄园和铁路分割任务的相对改进+5%和+13%；在ICDAR 2021数据集的建筑分块分割任务中表现出67.3% PQ值，且在极低数据情形下（5-shot和10-shot）依旧表现优异。

Conclusion: 所提方法不仅保持优异分割性能，还减少了人工标注需求，为自动化历史地图处理与分析提供了有效途径，同时代码已公开。

Abstract: As rich sources of history, maps provide crucial insights into historical
changes, yet their diverse visual representations and limited annotated data
pose significant challenges for automated processing. We propose a simple yet
effective approach for few-shot segmentation of historical maps, leveraging the
rich semantic embeddings of large vision foundation models combined with
parameter-efficient fine-tuning. Our method outperforms the state-of-the-art on
the Siegfried benchmark dataset in vineyard and railway segmentation, achieving
+5% and +13% relative improvements in mIoU in 10-shot scenarios and around +20%
in the more challenging 5-shot setting. Additionally, it demonstrates strong
performance on the ICDAR 2021 competition dataset, attaining a mean PQ of 67.3%
for building block segmentation, despite not being optimized for this
shape-sensitive metric, underscoring its generalizability. Notably, our
approach maintains high performance even in extremely low-data regimes (10- &
5-shot), while requiring only 689k trainable parameters - just 0.21% of the
total model size. Our approach enables precise segmentation of diverse
historical maps while drastically reducing the need for manual annotations,
advancing automated processing and analysis in the field. Our implementation is
publicly available at:
https://github.com/RafaelSterzinger/few-shot-map-segmentation.

</details>


### [14] [TaleForge: Interactive Multimodal System for Personalized Story Creation](https://arxiv.org/abs/2506.21832)
*Minh-Loi Nguyen,Quang-Khai Le,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: 提出了TaleForge，一个将语言模型与文本到图像扩散结合的新系统，用于生成包含用户面部个性的故事及插图。


<details>
  <summary>Details</summary>
Motivation: 现有叙事方法缺乏个性化与参与感，难以满足用户在叙事过程中的沉浸需求。

Method: 该系统通过三个模块（故事生成、个性化图像生成、背景生成），结合用户图像和穿搭选择生成个性化叙事与插图。

Result: 用户研究表明参与者作为主角体验时显著提升了参与感，同时提出需要更精细的叙事控制工具。

Conclusion: TaleForge通过匹配个性化文本与图像推进了多模态叙事领域的发展，为用户提供沉浸式、以用户为中心的体验。

Abstract: Storytelling is a deeply personal and creative process, yet existing methods
often treat users as passive consumers, offering generic plots with limited
personalization. This undermines engagement and immersion, especially where
individual style or appearance is crucial. We introduce TaleForge, a
personalized story-generation system that integrates large language models
(LLMs) and text-to-image diffusion to embed users' facial images within both
narratives and illustrations. TaleForge features three interconnected modules:
Story Generation, where LLMs create narratives and character descriptions from
user prompts; Personalized Image Generation, merging users' faces and outfit
choices into character illustrations; and Background Generation, creating scene
backdrops that incorporate personalized characters. A user study demonstrated
heightened engagement and ownership when individuals appeared as protagonists.
Participants praised the system's real-time previews and intuitive controls,
though they requested finer narrative editing tools. TaleForge advances
multimodal storytelling by aligning personalized text and imagery to create
immersive, user-centric experiences.

</details>


### [15] [PrefPaint: Enhancing Image Inpainting through Expert Human Feedback](https://arxiv.org/abs/2506.21834)
*Duy-Bao Bui,Hoang-Khang Nguyen,Trung-Nghia Le*

Main category: cs.CV

TL;DR: 本研究提出了PrefPaint方法，通过集成人类反馈改进医疗图像修复，减少视觉不一致性并提高医学领域中的图像生成精度。


<details>
  <summary>Details</summary>
Motivation: 旨在解决传统修复模型在医学领域中产生不准确图像的问题，这会影响诊断准确性，需要通过专家注释参与改进模型训练。

Method: 提出PrefPaint方法，结合人类反馈以改进Stable Diffusion Inpainting的训练，设计了便捷的网络界面用于简化训练、微调和推断过程。

Result: 通过用户研究表明，PrefPaint在多个领域中优于现有方法，显著提升图像修复一致性，特别在医学图像如息肉生成中，更加真实可靠。

Conclusion: PrefPaint有效集成人类反馈，改进医疗图像修复表现，为医学诊断提供更可信的图像支持，同时提升用户操作体验。

Abstract: Inpainting, the process of filling missing or corrupted image parts, has
broad applications, including medical imaging. However, in specialized fields
like medical polyps imaging, where accuracy and reliability are critical,
inpainting models can generate inaccurate images, leading to significant errors
in medical diagnosis and treatment. To ensure reliability, medical images
should be annotated by experts like oncologists for effective model training.
We propose PrefPaint, an approach that incorporates human feedback into the
training process of Stable Diffusion Inpainting, bypassing the need for
computationally expensive reward models. In addition, we develop a web-based
interface streamlines training, fine-tuning, and inference. This interactive
interface provides a smooth and intuitive user experience, making it easier to
offer feedback and manage the fine-tuning process. User study on various
domains shows that PrefPaint outperforms existing methods, reducing visual
inconsistencies and improving image rendering, particularly in medical
contexts, where our model generates more realistic polyps images.

</details>


### [16] [ProSAM: Enhancing the Robustness of SAM-based Visual Reference Segmentation with Probabilistic Prompts](https://arxiv.org/abs/2506.21835)
*Xiaoqi Wang,Clint Sebastian,Wenbin He,Liu Ren*

Main category: cs.CV

TL;DR: 提出了一种名为ProSAM的新方法，解决现有基于SAM的视觉参考分割方法中的稳定性问题，提升了任务的鲁棒性并在多个数据集上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于SAM的方法在生成提示时存在不稳定性和鲁棒性问题，特别是在对象边界处的提示生成上容易出错。

Method: 通过设计一个变分提示编码器，ProSAM能预测多维提示分布，从而避免在不稳定区域生成提示，改进了提示生成过程的稳定性。

Result: ProSAM在Pascal-5i和COCO-20i数据集上均表现优异，超越了最新的技术方法。

Conclusion: ProSAM是一种简单但有效的解决方案，为视觉参考分割任务提供了更稳定和鲁棒的表现。

Abstract: The recent advancements in large foundation models have driven the success of
open-set image segmentation, a task focused on segmenting objects beyond
predefined categories. Among various prompt types (such as points, boxes,
texts, and visual references), visual reference segmentation stands out for its
unique flexibility and strong zero-shot capabilities. Recently, several
SAM-based methods have made notable progress in this task by automatically
generating prompts to guide SAM. However, these methods often generate prompts
at object boundaries due to suboptimal prompt encoder, which results in
instability and reduced robustness. In this work, we introduce ProSAM, a simple
but effective method to address the stability challenges we identified in
existing SAM-based visual reference segmentation approaches. By learning a
variational prompt encoder to predict multivariate prompt distributions, ProSAM
avoids generating prompts that lie in unstable regions, overcoming the
instability caused by less robust prompts. Our approach consistently surpasses
state-of-the-art methods on the Pascal-5$^i$ and COCO-20$^i$ datasets,
providing a more robust solution for visual reference segmentation.

</details>


### [17] [GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles](https://arxiv.org/abs/2506.21839)
*Mengyi Shan,Brian Curless,Ira Kemelmacher-Shlizerman,Steve Seitz*

Main category: cs.CV

TL;DR: 提出了一种用于生成密室逃脱益智游戏图像的分层多代理框架，目标是提升可视化逻辑和智力上的吸引力。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成图像模型在空间关系和使用推理（affordance reasoning）上存在局限，无法满足生成逻辑和功能一致性强的场景需求。

Method: 设计了一种分层多代理框架，将任务分解为功能设计、符号场景图推理、布局合成和局部图像编辑等结构化阶段，通过代理间的反馈提升图像生成质量。

Result: 实验显示，该框架提升了解决方案的质量，表现为可解决性提高、避免捷径解决和使用性能明确，同时维持了视觉质量。

Conclusion: 分层多代理框架在增强逻辑一致性和视觉吸引力目标上表现出色，适用于生成密室逃脱类场景图像。

Abstract: We challenge text-to-image models with generating escape room puzzle images
that are visually appealing, logically solid, and intellectually stimulating.
While base image models struggle with spatial relationships and affordance
reasoning, we propose a hierarchical multi-agent framework that decomposes this
task into structured stages: functional design, symbolic scene graph reasoning,
layout synthesis, and local image editing. Specialized agents collaborate
through iterative feedback to ensure the scene is visually coherent and
functionally solvable. Experiments show that agent collaboration improves
output quality in terms of solvability, shortcut avoidance, and affordance
clarity, while maintaining visual quality.

</details>


### [18] [3D-Telepathy: Reconstructing 3D Objects from EEG Signals](https://arxiv.org/abs/2506.21843)
*Yuxiang Ge,Jionghao Cheng,Ruiquan Ge,Zhaojie Fang,Gangyong Jia,Xiang Wan,Nannan Li,Ahmed Elazab,Changmiao Wang*

Main category: cs.CV

TL;DR: 本研究提出了一种创新的EEG编码器架构和混合训练策略，用以从EEG数据生成3D对象，解决了EEG到3D重建的关键障碍。


<details>
  <summary>Details</summary>
Motivation: 探索如何从EEG数据重建3D视觉刺激，以提升脑机接口及通信障碍人群辅助技术应用的潜力，同时突破以往仅关注2D图像的局限。

Method: 设计了整合双重自注意机制的EEG编码器，并通过交叉注意力、对比学习和自监督学习等混合训练策略进行训练，结合稳定扩散先验分布和变分得分蒸馏技术生成3D对象。

Result: 利用所提出的方法从EEG数据成功生成了具有相似内容与结构的3D对象。

Conclusion: 解决了EEG信号中噪声及数据稀缺等问题，为EEG到3D的重建提供了可行性方法，并扩展了EEG在脑机接口领域的应用可能性。

Abstract: Reconstructing 3D visual stimuli from Electroencephalography (EEG) data holds
significant potential for applications in Brain-Computer Interfaces (BCIs) and
aiding individuals with communication disorders. Traditionally, efforts have
focused on converting brain activity into 2D images, neglecting the translation
of EEG data into 3D objects. This limitation is noteworthy, as the human brain
inherently processes three-dimensional spatial information regardless of
whether observing 2D images or the real world. The neural activities captured
by EEG contain rich spatial information that is inevitably lost when
reconstructing only 2D images, thus limiting its practical applications in BCI.
The transition from EEG data to 3D object reconstruction faces considerable
obstacles. These include the presence of extensive noise within EEG signals and
a scarcity of datasets that include both EEG and 3D information, which
complicates the extraction process of 3D visual data. Addressing this
challenging task, we propose an innovative EEG encoder architecture that
integrates a dual self-attention mechanism. We use a hybrid training strategy
to train the EEG Encoder, which includes cross-attention, contrastive learning,
and self-supervised learning techniques. Additionally, by employing stable
diffusion as a prior distribution and utilizing Variational Score Distillation
to train a neural radiation field, we successfully generate 3D objects with
similar content and structure from EEG data.

</details>


### [19] [ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment](https://arxiv.org/abs/2109.05721)
*Yangyu Huang,Hao Yang,Chong Li,Jongyoo Kim,Fangyun Wei*

Main category: cs.CV

TL;DR: 本文提出了一种名为ADNet的优化端到端训练模型，用以解决人脸对齐中面部关键点分布上的误差偏差问题。


<details>
  <summary>Details</summary>
Motivation: 分析人脸对齐中面部关键点分布的误差偏差（error-bias）现象，尝试利用这一特性改进模型效果。

Method: 提出了各向异性方向损失(ADL)和各向异性注意模块(AAM)，分别应用于关键点坐标和热图回归。ADL对关键点在脸部边界法向的方向施加强约束，AAM则关注关键点及其局部边缘区域的各向异性注意掩模。最终整合入ADNet模型。

Result: ADNet在300W、WFLW和COFW数据集上取得了最先进的效果，验证了其有效性和鲁棒性。

Conclusion: 利用人脸关键点误差分布的偏差特性，提出的ADNet模型能更好地学习面部结构与纹理细节，实现了更优的人脸对齐性能。

Abstract: The recent progress of CNN has dramatically improved face alignment
performance. However, few works have paid attention to the error-bias with
respect to error distribution of facial landmarks. In this paper, we
investigate the error-bias issue in face alignment, where the distributions of
landmark errors tend to spread along the tangent line to landmark curves. This
error-bias is not trivial since it is closely connected to the ambiguous
landmark labeling task. Inspired by this observation, we seek a way to leverage
the error-bias property for better convergence of CNN model. To this end, we
propose anisotropic direction loss (ADL) and anisotropic attention module (AAM)
for coordinate and heatmap regression, respectively. ADL imposes strong binding
force in normal direction for each landmark point on facial boundaries. On the
other hand, AAM is an attention module which can get anisotropic attention mask
focusing on the region of point and its local edge connected by adjacent
points, it has a stronger response in tangent than in normal, which means
relaxed constraints in the tangent. These two methods work in a complementary
manner to learn both facial structures and texture details. Finally, we
integrate them into an optimized end-to-end training pipeline named ADNet. Our
ADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which
demonstrates the effectiveness and robustness.

</details>


### [20] [End-to-End RGB-IR Joint Image Compression With Channel-wise Cross-modality Entropy Model](https://arxiv.org/abs/2506.21851)
*Haofeng Wang,Fangtao Zhou,Qi Zhang,Zeyuan Chen,Enci Zhang,Zhao Wang,Xiaofeng Huang,Siwei Ma*

Main category: cs.CV

TL;DR: 该研究提出了一种联合压缩RGB-IR图像对的新方法，通过利用跨模态信息实现更高效的数据压缩。


<details>
  <summary>Details</summary>
Motivation: 随着应用中RGB-IR图像对的增加，所需的存储和传输成本也增加，因此提出高效的RGB-IR数据压缩是必要的。

Method: 设计了一个通道级跨模态熵模型(CCEM)，包括低频上下文提取块(LCEB)和融合块(LCFB)，对跨模态信息进行提取和聚合，以提高熵参数预测的准确性。

Result: 在LLVIP和KAIST数据集上，该方法超越了现有的RGB-IR图像对和单模态压缩方法，在LLVIP数据集上的比特率节省了23.1%。

Conclusion: 所提出的联合压缩框架在多个数据集上表现优异，显示了在RGB-IR图像压缩领域中的较大潜力。

Abstract: RGB-IR(RGB-Infrared) image pairs are frequently applied simultaneously in
various applications like intelligent surveillance. However, as the number of
modalities increases, the required data storage and transmission costs also
double. Therefore, efficient RGB-IR data compression is essential. This work
proposes a joint compression framework for RGB-IR image pair. Specifically, to
fully utilize cross-modality prior information for accurate context probability
modeling within and between modalities, we propose a Channel-wise
Cross-modality Entropy Model (CCEM). Among CCEM, a Low-frequency Context
Extraction Block (LCEB) and a Low-frequency Context Fusion Block (LCFB) are
designed for extracting and aggregating the global low-frequency information
from both modalities, which assist the model in predicting entropy parameters
more accurately. Experimental results demonstrate that our approach outperforms
existing RGB-IR image pair and single-modality compression methods on LLVIP and
KAIST datasets. For instance, the proposed framework achieves a 23.1% bit rate
saving on LLVIP dataset compared to the state-of-the-art RGB-IR image codec
presented at CVPR 2022.

</details>


### [21] [FreeEnricher: Enriching Face Landmarks without Additional Cost](https://arxiv.org/abs/2212.09525)
*Yangyu Huang,Xi Chen,Jongyoo Kim,Hao Yang,Chong Li,Jiaolong Yang,Dong Chen*

Main category: cs.CV

TL;DR: 近年来，人脸对齐领域发展迅速，但大多数研究仅针对稀疏对齐。本文提出一个框架，通过现有稀疏关键点数据集丰富关键点密度，并在300W测试集上手动标注密集关键点，用于评估。


<details>
  <summary>Details</summary>
Motivation: 在美容医学和面部美化等场景中，对密集人脸关键点的需求显著，但现有多数工作只关注稀疏对齐。

Method: 提出一种弱监督学习方式，利用原始稀疏关键点训练模型，再将能力转移到密集关键点上，并通过多种操作实现。后将模型作为插件用于现有网络。

Result: 在新构建的密集300W测试集和原始稀疏的300W及WFLW测试集中均达到了最新的准确率。

Conclusion: 本文框架能够在不增加成本的情况下显著提高密集人脸对齐的精确度，具有实用性。

Abstract: Recent years have witnessed significant growth of face alignment. Though
dense facial landmark is highly demanded in various scenarios, e.g., cosmetic
medicine and facial beautification, most works only consider sparse face
alignment. To address this problem, we present a framework that can enrich
landmark density by existing sparse landmark datasets, e.g., 300W with 68
points and WFLW with 98 points. Firstly, we observe that the local patches
along each semantic contour are highly similar in appearance. Then, we propose
a weakly-supervised idea of learning the refinement ability on original sparse
landmarks and adapting this ability to enriched dense landmarks. Meanwhile,
several operators are devised and organized together to implement the idea.
Finally, the trained model is applied as a plug-and-play module to the existing
face alignment networks. To evaluate our method, we manually label the dense
landmarks on 300W testset. Our method yields state-of-the-art accuracy not only
in newly-constructed dense 300W testset but also in the original sparse 300W
and WFLW testsets without additional cost.

</details>


### [22] [Periodic-MAE: Periodic Video Masked Autoencoder for rPPG Estimation](https://arxiv.org/abs/2506.21855)
*Jiho Choi,Sang Jun Lee*

Main category: cs.CV

TL;DR: 提出了基于面部视频的自监督学习方法，通过视频掩码自动编码器提取高维时空特征，用于检测远程光电容积描记信号（rPPG）。


<details>
  <summary>Details</summary>
Motivation: 提升从面部视频中提取周期性生理信号的能力，特别是在跨数据集评估中的表现。

Method: 使用视频掩码自动编码器进行自监督学习，通过帧掩码采样捕获视频中的准周期信号，并结合生理频带限制，为模型提供脉搏信号提示。

Result: 在PURE、UBFC-rPPG、MMPD和V4V数据集上进行评估，尤其是在跨数据集测试中表现显著提升。

Conclusion: 所提出方法有效提升了基于面部视频的rPPG任务性能，并验证了生理信号带宽约束和准周期信号的重要性。

Abstract: In this paper, we propose a method that learns a general representation of
periodic signals from unlabeled facial videos by capturing subtle changes in
skin tone over time. The proposed framework employs the video masked
autoencoder to learn a high-dimensional spatio-temporal representation of the
facial region through self-supervised learning. Capturing quasi-periodic
signals in the video is crucial for remote photoplethysmography (rPPG)
estimation. To account for signal periodicity, we apply frame masking in terms
of video sampling, which allows the model to capture resampled quasi-periodic
signals during the pre-training stage. Moreover, the framework incorporates
physiological bandlimit constraints, leveraging the property that physiological
signals are sparse within their frequency bandwidth to provide pulse cues to
the model. The pre-trained encoder is then transferred to the rPPG task, where
it is used to extract physiological signals from facial videos. We evaluate the
proposed method through extensive experiments on the PURE, UBFC-rPPG, MMPD, and
V4V datasets. Our results demonstrate significant performance improvements,
particularly in challenging cross-dataset evaluations. Our code is available at
https://github.com/ziiho08/Periodic-MAE.

</details>


### [23] [SPADE: Spatial Transcriptomics and Pathology Alignment Using a Mixture of Data Experts for an Expressive Latent Space](https://arxiv.org/abs/2506.21857)
*Ekaterina Redekop,Mara Pleasure,Zichen Wang,Kimberly Flores,Anthony Sisk,William Speier,Corey W. Arnold*

Main category: cs.CV

TL;DR: 文章介绍了一种名为SPADE的技术，融合了数字病理学中的全切片图像和空间转录组数据，通过统一框架学习图像表示，为病理学任务提供基础模型。


<details>
  <summary>Details</summary>
Motivation: 填补在全切片图像与空间转录组数据全面整合上的关键空白，以捕捉超越传统H&E染色的分子异质性。

Method: 提出SPADE模型，结合通过两阶段特征空间聚类的混合数据专家技术和对比学习，来学习共配准的全切片图像（WSI）和基因表达数据的表达。

Result: 在HEST-1k数据集上预训练后，SPADE在14项下游任务上显著优于基线模型，特别在少样本学习表现上突出。

Conclusion: 整合形态学与分子信息可以创建更优的潜在空间，为病理学任务提供了新方向。

Abstract: The rapid growth of digital pathology and advances in self-supervised deep
learning have enabled the development of foundational models for various
pathology tasks across diverse diseases. While multimodal approaches
integrating diverse data sources have emerged, a critical gap remains in the
comprehensive integration of whole-slide images (WSIs) with spatial
transcriptomics (ST), which is crucial for capturing critical molecular
heterogeneity beyond standard hematoxylin & eosin (H&E) staining. We introduce
SPADE, a foundation model that integrates histopathology with ST data to guide
image representation learning within a unified framework, in effect creating an
ST-informed latent space. SPADE leverages a mixture-of-data experts technique,
where experts, created via two-stage feature-space clustering, use contrastive
learning to learn representations of co-registered WSI patches and gene
expression profiles. Pre-trained on the comprehensive HEST-1k dataset, SPADE is
evaluated on 14 downstream tasks, demonstrating significantly superior few-shot
performance compared to baseline models, highlighting the benefits of
integrating morphological and molecular information into one latent space.

</details>


### [24] [PEACE: Empowering Geologic Map Holistic Understanding with MLLMs](https://arxiv.org/abs/2501.06184)
*Yangyu Huang,Tianyi Gao,Haoran Xu,Qihao Zhao,Yang Song,Zhipeng Gui,Tengchao Lv,Hao Chen,Lei Cui,Scarlett Li,Furu Wei*

Main category: cs.CV

TL;DR: 文章介绍了一种名为GeoMap-Agent的系统，该系统旨在填补多模态大型语言模型(MLLMs)在地质图理解中的不足。


<details>
  <summary>Details</summary>
Motivation: 地质图在地质科学中至关重要，但当前MLLMs在地质图理解方面存在显著不足。这种差距主要来源于地质图一般化的挑战，包括高分辨率地图处理、多组件管理以及领域特定知识需求。

Method: 提出了GeoMap-Bench以量化当前MLLMs在地质图理解中的能力差距，并引入GeoMap-Agent来弥补这些不足。GeoMap-Agent包含层级信息提取、领域知识注入以及增强提示的问答模块，并模拟跨学科合作以改进模型性能。

Result: GeoMap-Agent在GeoMap-Bench的评估中获得了0.811的整体得分，比GPT-4o的0.369大幅提升。

Conclusion: 这一研究提升了地质图全面理解的能力，为地质学中的先进AI应用提供了新思路，显著提高了地质调查的效率与准确性。

Abstract: Geologic map, as a fundamental diagram in geology science, provides critical
insights into the structure and composition of Earth's subsurface and surface.
These maps are indispensable in various fields, including disaster detection,
resource exploration, and civil engineering. Despite their significance,
current Multimodal Large Language Models (MLLMs) often fall short in geologic
map understanding. This gap is primarily due to the challenging nature of
cartographic generalization, which involves handling high-resolution map,
managing multiple associated components, and requiring domain-specific
knowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever
benchmark for evaluating MLLMs in geologic map understanding, which assesses
the full-scale abilities in extracting, referring, grounding, reasoning, and
analyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent
designed for geologic map understanding, which features three modules:
Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI),
and Prompt-enhanced Question Answering (PEQA). Inspired by the
interdisciplinary collaboration among human scientists, an AI expert group acts
as consultants, utilizing a diverse tool pool to comprehensively analyze
questions. Through comprehensive experiments, GeoMap-Agent achieves an overall
score of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o.
Our work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs,
paves the way for advanced AI applications in geology, enhancing the efficiency
and accuracy of geological investigations.

</details>


### [25] [LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs](https://arxiv.org/abs/2506.21862)
*Boyuan Sun,Jiaxing Zhao,Xihan Wei,Qibin Hou*

Main category: cs.CV

TL;DR: 提出LLaVA-Scissor，一种无训练的视频多模态大语言模型的token压缩方法，通过语义连通组件(SCC)实现高效的token压缩，在多种基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于注意力分数的token压缩方法不能有效捕获全部语义区域且容易导致token冗余的问题。

Method: 引入语义连通组件(SCC)方法，在时空领域内进行两步token压缩策略，以表示完整视频的非重叠语义token集。

Result: 在视频问答、长视频理解及多选择测试等基准上，与其他token压缩方法相比，LLaVA-Scissor具有显著优势，特别是在低token保留率下表现优越。

Conclusion: LLaVA-Scissor展示了在无需训练的情况下实现高效、全面语义覆盖的token压缩能力，为视频多模态语言模型提供了更优的表现。

Abstract: In this paper, we present LLaVA-Scissor, a training-free token compression
strategy designed for video multimodal large language models. Previous methods
mostly attempt to compress tokens based on attention scores, but fail to
effectively capture all semantic regions and often lead to token redundancy.
Differently, we propose to leverage the Semantic Connected Components (SCC)
approach that assigns tokens to distinct semantic regions within the token set,
ensuring comprehensive semantic coverage. The outcome is a two-step
spatio-temporal token compression strategy that utilizes SCC in both spatial
and temporal domains. This strategy can effectively compress tokens by
representing the entire video with a set of non-overlapping semantic tokens. We
conduct extensive evaluations of the token compression capabilities of
LLaVA-Scissor across diverse video understanding benchmarks, including video
question answering, long video understanding, and comprehensive multi-choices
benchmarks. Experimental results show that the proposed LLaVA-Scissor
outperforms other token compression methods, achieving superior performance in
various video understanding benchmarks, particularly at low token retention
ratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.

</details>


### [26] [Remote Sensing Large Vision-Language Model: Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling](https://arxiv.org/abs/2506.21863)
*Sungjune Park,Yeongyun Kim,Se Yeon Kim,Yong Man Ro*

Main category: cs.CV

TL;DR: 本文提出了一种针对遥感图像理解的LVLM框架，能够丰富视觉特征的语义信息，并通过层次化语义专家建模实现从粗到细的语义理解，大幅改进了多个遥感任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型在自然图像领域表现优异，但难以适应遥感图像领域中图像视觉外观、目标尺度与语义上的巨大差异。

Method: 本文提出的框架包括语义增强的多层次对齐和语义感知的专家建模两个核心机制。首先通过检索集成语义增强模块，丰富多层次视觉特征的语义信息；其次利用层次化语义专家模型分别处理不同语义层次的表征信息。

Result: 在多个遥感任务（如场景分类和视觉问答等）上的评测结果显示，所提出的方法在多语义层面实现了一致性改进。

Conclusion: 该研究成功弥合了通用LVLMs与遥感图像特定需求之间的差距，验证了其在遥感图像语义理解中的有效性和优势。

Abstract: Large Vision and Language Models (LVLMs) have shown strong performance across
various vision-language tasks in natural image domains. However, their
application to remote sensing (RS) remains underexplored due to significant
domain differences in visual appearances, object scales, and semantics. These
discrepancies hider the effective understanding of RS scenes, which contain
rich, multi-level semantic information spanning from coarse-to-fine levels.
Hence, it limits the direct adaptation of existing LVLMs to RS imagery. To
address this gap, we propose a novel LVLM framework tailored for RS
understanding, incorporating two core components: Semantic-augmented
Multi-level Alignment and Semantic-aware Expert Modeling. First, to align
multi-level visual features, we introduce the retrieval-based Semantic
Augmentation Module which enriches the visual features with relevant semantics
across fine-to-coarse levels (e.g., object- and scene-level information). It is
designed to retrieve relevant semantic cues from a RS semantic knowledge
database, followed by aggregation of semantic cues with user query and
multi-level visual features, resulting in semantically enriched representation
across multiple levels. Second, for Semantic-aware Expert Modeling, we design
semantic experts, where each expert is responsible for processing semantic
representation at different levels separately. This enables hierarchical
semantic understanding from coarse to fine levels. Evaluations across multiple
RS tasks-including scene classification and VQA, etc.-demonstrate that the
proposed framework achieves consistent improvements across multiple semantic
levels. This highlights its capability and effectiveness in bridging the gap
between general LVLMs and unique demands of RS-specific vision-language
understanding.

</details>


### [27] [Dual-Perspective United Transformer for Object Segmentation in Optical Remote Sensing Images](https://arxiv.org/abs/2506.21866)
*Yanguang Sun,Jiexi Yan,Jianjun Qian,Chunyan Xu,Jian Yang,Lei Luo*

Main category: cs.CV

TL;DR: 本文提出了一种称为DPU-Former的新模型，用于优化光学遥感图像（ORSIs）的自动分割任务，并在多个数据集上优于当前最先进的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的ORSIs分割模型主要基于卷积或Transformer特征，未有效结合两者的优势，导致分割性能欠佳，需要一个能够整合长程依赖和空间细节的新方法。

Method: 提出了一种全新的DPU-Former架构，其关键组件包括全球-局部混合注意力机制（结合多样信息并利用傅里叶空间合并策略）、门控线性前馈网络（增强表达能力），以及一个DPU-Former解码器（聚合和增强不同层的特征）。

Result: DPU-Former模型在多个数据集上的分割性能超越当前最先进的方法。

Conclusion: 通过有效融合卷积和Transformer的优势，DPU-Former为处理光学遥感图像分割任务提供了一种更优的解决方案，并验证了其实用性和先进性。

Abstract: Automatically segmenting objects from optical remote sensing images (ORSIs)
is an important task. Most existing models are primarily based on either
convolutional or Transformer features, each offering distinct advantages.
Exploiting both advantages is valuable research, but it presents several
challenges, including the heterogeneity between the two types of features, high
complexity, and large parameters of the model. However, these issues are often
overlooked in existing the ORSIs methods, causing sub-optimal segmentation. For
that, we propose a novel Dual-Perspective United Transformer (DPU-Former) with
a unique structure designed to simultaneously integrate long-range dependencies
and spatial details. In particular, we design the global-local mixed attention,
which captures diverse information through two perspectives and introduces a
Fourier-space merging strategy to obviate deviations for efficient fusion.
Furthermore, we present a gated linear feed-forward network to increase the
expressive ability. Additionally, we construct a DPU-Former decoder to
aggregate and strength features at different layers. Consequently, the
DPU-Former model outperforms the state-of-the-art methods on multiple datasets.
Code: https://github.com/CSYSI/DPU-Former.

</details>


### [28] [Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops in Visual Grounding Caused by Pruning](https://arxiv.org/abs/2506.21873)
*Tzu-Chun Chien,Chieh-Kai Lin,Shiang-Feng Tsai,Ruei-Chi Lai,Hung-Jen Chen,Min Sun*

Main category: cs.CV

TL;DR: 近期的多模态大语言模型（MLLMs）在视觉定位任务中表现出色，但在使用视觉标记裁剪减少计算成本时易导致性能下降。本文提出了一种简单调整位置ID的方法（GAP），显著缓解性能退化问题。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在视觉语言任务中表现强劲，但高计算成本的问题限制了其应用。标记裁剪方法虽可降低成本，却会显著影响模型的视觉定位能力，因此需要一种避免性能大幅下降的解决方法。

Method: 通过分析发现标记裁剪后的主要问题在于位置ID错位，提出了名为Grounding-Aware Token Pruning（GAP）的调整方法。无需额外训练、内存或计算开销，即可修正位置ID并显著改善视觉定位任务的性能。

Result: 采用GAP后，模型在RefCOCO验证集上的准确率从15.34%上升至51.42%，恢复到了未裁剪情况下性能的90%。方法在多个模型和裁剪策略上均能提升性能。

Conclusion: GAP方法有效解决了标记裁剪引发的性能退化问题，同时保持了计算成本的减少，对提升MLLMs在视觉语言任务中的应用潜力具有重要意义。

Abstract: Recent Multimodal Large Language Models (MLLMs) have demonstrated strong
performance in visual grounding, establishing themselves as a general interface
for various vision-language applications. This progress has driven the
development of token pruning methods to mitigate the high computational costs
associated with processing numerous visual tokens. However, we observe that
pruning significantly weakens the model's grounding ability, leading to
incorrect predictions and drastic performance degradation. In Referring
Expression Comprehension (REC), for instance, pruning causes the accuracy of
LLaVA on the RefCOCO validation set to drop from 56.14% to 15.34%. Our analysis
identifies misaligned position IDs after pruning as the primary cause of this
degradation, as both the order and value of these IDs are crucial for
maintaining performance in grounding tasks. To address this issue, we propose
Grounding-Aware Token Pruning (GAP), a simple yet effective adjustment to
position IDs that recovers REC accuracy back to 51.42%, which is 90% of the
original performance in the without pruning setting, all while requiring no
additional training, memory, or computational overhead. Applied to models such
as Shikra, MiniGPTv2, and the LLaVA series, our method consistently improves
performance across various token pruning strategies.

</details>


### [29] [GRASP-PsONet: Gradient-based Removal of Spurious Patterns for PsOriasis Severity Classification](https://arxiv.org/abs/2506.21883)
*Basudha Pal,Sharif Amit Kamran,Brendon Lutnick,Molly Lucas,Chaitanya Parmar,Asha Patel Shah,David Apfel,Steven Fakharzadeh,Lloyd Miller,Gabriela Cula,Kristopher Standish*

Main category: cs.CV

TL;DR: 本文提出了一种基于梯度的解释性方法框架，用于自动标记导致模型泛化能力降低的问题训练图像，从而提高牛皮癣（PsO）严重程度评分模型的性能。


<details>
  <summary>Details</summary>
Motivation: PsO严重程度评分因人工评估的主观性和资源限制受到挑战，而患者拍摄的图像具有尺度扩展性，但受制于图像质量、背景等非临床因素的影响，本研究旨在提升自动评分系统的鲁棒性。

Method: 通过基于梯度的解释性方法，追踪模型对验证集中分类错误图像的梯度，检测模型出错的训练样本与评分不一致的样本或非临床伪影之间的关系，并剔除这些样本以提升模型表现。

Result: 通过剔除8.2%的标记图像，模型AUC-ROC从85%提升到90%；在双医师评分数据中，仅需复审30%的样本即可识别90%以上的评分不一致情况。

Conclusion: 提出的方法无需额外人工审查即可提升远程评估系统的可靠性，为数据采集过程中的变异性提供解决方案。

Abstract: Psoriasis (PsO) severity scoring is important for clinical trials but is
hindered by inter-rater variability and the burden of in person clinical
evaluation. Remote imaging using patient captured mobile photos offers
scalability but introduces challenges, such as variation in lighting,
background, and device quality that are often imperceptible to humans but can
impact model performance. These factors, along with inconsistencies in
dermatologist annotations, reduce the reliability of automated severity
scoring. We propose a framework to automatically flag problematic training
images that introduce spurious correlations which degrade model generalization,
using a gradient based interpretability approach. By tracing the gradients of
misclassified validation images, we detect training samples where model errors
align with inconsistently rated examples or are affected by subtle, nonclinical
artifacts. We apply this method to a ConvNeXT based weakly supervised model
designed to classify PsO severity from phone images. Removing 8.2% of flagged
images improves model AUC-ROC by 5% (85% to 90%) on a held out test set.
Commonly, multiple annotators and an adjudication process ensure annotation
accuracy, which is expensive and time consuming. Our method detects training
images with annotation inconsistencies, potentially removing the need for
manual review. When applied to a subset of training data rated by two
dermatologists, the method identifies over 90% of cases with inter-rater
disagreement by reviewing only the top 30% of samples. This improves automated
scoring for remote assessments, ensuring robustness despite data collection
variability.

</details>


### [30] [Integrating Multi-Modal Sensors: A Review of Fusion Techniques for Intelligent Vehicles](https://arxiv.org/abs/2506.21885)
*Chuheng Wei,Ziye Qin,Ziyan Zhang,Guoyuan Wu,Matthew J. Barth*

Main category: cs.CV

TL;DR: 多传感器融合通过克服单一传感器的局限性，提升自动驾驶感知能力。在数据、特征和决策三个层次上系统回顾基于深度学习的方法，并讨论其在真实应用中的优势和挑战。


<details>
  <summary>Details</summary>
Motivation: 研究多传感器融合在自动驾驶中的作用，强调通过增强系统适应性和鲁棒性，以应对复杂环境挑战，并推动新兴技术的应用。

Method: 根据数据层次，将多传感器融合策略分为数据级、特征级与决策级，对深度学习相关方法进行系统回顾，并探讨视觉-语言模型、语言大模型等新兴趋势的集成。

Result: 提出多模态数据集的关键特点及其在恶劣天气和复杂城市环境中的应用价值，同时展示传感器融合在端到端自动驾驶系统中的潜力。

Conclusion: 通过对当前方法及未来研究方向的分析，为研究人员和从业者提供关于多传感器融合在自动驾驶中的启示和发展路径。

Abstract: Multi-sensor fusion plays a critical role in enhancing perception for
autonomous driving, overcoming individual sensor limitations, and enabling
comprehensive environmental understanding. This paper first formalizes
multi-sensor fusion strategies into data-level, feature-level, and
decision-level categories and then provides a systematic review of deep
learning-based methods corresponding to each strategy. We present key
multi-modal datasets and discuss their applicability in addressing real-world
challenges, particularly in adverse weather conditions and complex urban
environments. Additionally, we explore emerging trends, including the
integration of Vision-Language Models (VLMs), Large Language Models (LLMs), and
the role of sensor fusion in end-to-end autonomous driving, highlighting its
potential to enhance system adaptability and robustness. Our work offers
valuable insights into current methods and future directions for multi-sensor
fusion in autonomous driving.

</details>


### [31] [DIVE: Deep-search Iterative Video Exploration A Technical Report for the CVRR Challenge at CVPR 2025](https://arxiv.org/abs/2506.21891)
*Umihiro Kamoto,Tatsuya Ishibashi,Noriyuki Kugo*

Main category: cs.CV

TL;DR: 本报告介绍了在2025年复杂视频推理与鲁棒性评估挑战赛中获胜的解决方案。


<details>
  <summary>Details</summary>
Motivation: 提高在真实环境中复杂视频的自然语言问答能力。

Method: 提出了一种名为DIVE（Deep-search Iterative Video Exploration）的递归推理方法，通过语义分解和逐步推理逐步解答复杂问题。

Result: 在CVRR-ES基准测试的测试集上取得了81.44%的准确率，排名第一。

Conclusion: DIVE方法展示了其在复杂视频问答任务中的高效性和稳健性。

Abstract: In this report, we present the winning solution that achieved the 1st place
in the Complex Video Reasoning & Robustness Evaluation Challenge 2025. This
challenge evaluates the ability to generate accurate natural language answers
to questions about diverse, real-world video clips. It uses the Complex Video
Reasoning and Robustness Evaluation Suite (CVRR-ES) benchmark, which consists
of 214 unique videos and 2,400 question-answer pairs spanning 11 categories.
Our method, DIVE (Deep-search Iterative Video Exploration), adopts an iterative
reasoning approach, in which each input question is semantically decomposed and
solved through stepwise reasoning and progressive inference. This enables our
system to provide highly accurate and contextually appropriate answers to even
the most complex queries. Applied to the CVRR-ES benchmark, our approach
achieves 81.44% accuracy on the test set, securing the top position among all
participants. This report details our methodology and provides a comprehensive
analysis of the experimental results, demonstrating the effectiveness of our
iterative reasoning framework in achieving robust video question answering. The
code is available at https://github.com/PanasonicConnect/DIVE

</details>


### [32] [SODA: Out-of-Distribution Detection in Domain-Shifted Point Clouds via Neighborhood Propagation](https://arxiv.org/abs/2506.21892)
*Adam Goodge,Xun Xu,Bryan Hooi,Wee Siong Ng,Jingyi Liao,Yongyi Su,Xulei Yang*

Main category: cs.CV

TL;DR: 本文介绍了一种名为SODA的新方法，利用3D视觉语言模型和邻域评分传播机制，改善点云对象的异常检测问题。


<details>
  <summary>Details</summary>
Motivation: 点云数据在各类应用中的日益普及，催生出保障模型安全性和可靠性的OOD检测需求，但相关研究仍相对较少。

Method: 提出SODA方法，通过邻域评分传播机制提升3D VLM的点云对象OOD检测性能，该方法基于推断，无需额外训练。

Result: SODA方法在不同数据集和问题设置中均优于现有方法，达到了最先进的性能。

Conclusion: SODA方法有效解决了从合成到现实领域的迁移难题，无需额外训练即可显著提升点云对象的OOD检测效率。

Abstract: As point cloud data increases in prevalence in a variety of applications, the
ability to detect out-of-distribution (OOD) point cloud objects becomes
critical for ensuring model safety and reliability. However, this problem
remains under-explored in existing research. Inspired by success in the image
domain, we propose to exploit advances in 3D vision-language models (3D VLMs)
for OOD detection in point cloud objects. However, a major challenge is that
point cloud datasets used to pre-train 3D VLMs are drastically smaller in size
and object diversity than their image-based counterparts. Critically, they
often contain exclusively computer-designed synthetic objects. This leads to a
substantial domain shift when the model is transferred to practical tasks
involving real objects scanned from the physical environment. In this paper,
our empirical experiments show that synthetic-to-real domain shift
significantly degrades the alignment of point cloud with their associated text
embeddings in the 3D VLM latent space, hindering downstream performance. To
address this, we propose a novel methodology called SODA which improves the
detection of OOD point clouds through a neighborhood-based score propagation
scheme. SODA is inference-based, requires no additional model training, and
achieves state-of-the-art performance over existing approaches across datasets
and problem settings.

</details>


### [33] [Exploring Task-Solving Paradigm for Generalized Cross-Domain Face Anti-Spoofing via Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.21895)
*Fangling Jiang,Qi Li,Weining Wang,Gang Wang,Bing Liu,Zhenan Sun*

Main category: cs.CV

TL;DR: 提出了一种基于强化微调的面部反欺诈方法，通过强化学习提升模型的跨域泛化能力并提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有面部反欺诈方法在应对新型欺诈攻击时的泛化能力较弱且缺乏解释性。

Method: 采用强化微调技术，提出了类一致奖励和推理一致奖励，并通过基于GRPO的优化策略引导模型多角度探索推理策略。

Result: 实验表明该方法在跨域泛化性能上达到了最先进水平，能够应对各种未知攻击类型并提供可解释的决策理由。

Conclusion: 该方法能够有效解决跨域面部反欺诈问题，同时提升了可解释性和泛化能力。

Abstract: Recently the emergence of novel presentation attacks has drawn increasing
attention to face anti-spoofing. However, existing methods tend to memorize
data patterns from the training set, resulting in poor generalization to
unknown attack types across different scenarios and limited interpretability.
To address these challenges, this paper presents a reinforcement
fine-tuning-based face anti-spoofing method that stimulates the capabilities of
multimodal large language models to think and learn how to solve the
anti-spoofing task itself, rather than relying on the memorization of
authenticity patterns. We design verifiable class consistent reward and
reasoning consistent reward, and employ a GRPO-based optimization strategy to
guide the model in exploring reasoning policies from multiple perspectives to
maximize expected rewards. As a result, through iterative trial-and-error
learning while retaining only high-reward trajectories, the model distills
highly generalizable decision-making rules from the extensive solution space to
effectively address cross-domain face anti-spoofing tasks. Extensive
experimental results demonstrate that our method achieves state-of-the-art
cross-domain generalization performance. It generalizes well to diverse unknown
attack types in unseen target domains while providing interpretable reasoning
for its authenticity decisions without requiring labor-intensive textual
annotations for training.

</details>


### [34] [Visual Content Detection in Educational Videos with Transfer Learning and Dataset Enrichment](https://arxiv.org/abs/2506.21903)
*Dipayan Biswas,Shishir Shah,Jaspal Subhlok*

Main category: cs.CV

TL;DR: 該研究提出了一種基於YOLO模型的遷移學習方法，用於在課堂視頻框架中檢測視覺元素，並發布了相關數據集和代碼。


<details>
  <summary>Details</summary>
Motivation: 為了提升課堂視頻的可檢索性和可理解性，本研究針對如何準確檢測視頻中的視覺元素進行探索。

Method: 通過對現有物體檢測模型進行評估，選擇YOLO模型並進行優化，採用多個數據集的訓練和半監督自動標註策略。

Result: 優化後的YOLO模型在課堂視頻物體檢測中展現了出色的性能，並成功開發了一種通用解決方案。

Conclusion: 該研究不僅提高了課堂視頻中視覺元素的檢測準確性，還提供了公開的基準數據集和源代碼，有助於相關領域的進一步研究。

Abstract: Video is transforming education with online courses and recorded lectures
supplementing and replacing classroom teaching. Recent research has focused on
enhancing information retrieval for video lectures with advanced navigation,
searchability, summarization, as well as question answering chatbots. Visual
elements like tables, charts, and illustrations are central to comprehension,
retention, and data presentation in lecture videos, yet their full potential
for improving access to video content remains underutilized. A major factor is
that accurate automatic detection of visual elements in a lecture video is
challenging; reasons include i) most visual elements, such as charts, graphs,
tables, and illustrations, are artificially created and lack any standard
structure, and ii) coherent visual objects may lack clear boundaries and may be
composed of connected text and visual components. Despite advancements in deep
learning based object detection, current models do not yield satisfactory
performance due to the unique nature of visual content in lectures and scarcity
of annotated datasets. This paper reports on a transfer learning approach for
detecting visual elements in lecture video frames. A suite of state of the art
object detection models were evaluated for their performance on lecture video
datasets. YOLO emerged as the most promising model for this task. Subsequently
YOLO was optimized for lecture video object detection with training on multiple
benchmark datasets and deploying a semi-supervised auto labeling strategy.
Results evaluate the success of this approach, also in developing a general
solution to the problem of object detection in lecture videos. Paper
contributions include a publicly released benchmark of annotated lecture video
frames, along with the source code to facilitate future research.

</details>


### [35] [RAUM-Net: Regional Attention and Uncertainty-aware Mamba Network](https://arxiv.org/abs/2506.21905)
*Mingquan Liu*

Main category: cs.CV

TL;DR: 提出了一种结合Mamba特征建模、区域注意力和贝叶斯不确定性的半监督方法，用于细粒度视觉分类（FGVC），在有限标签数据下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决细粒度视觉分类中类别间差异细微且特征表达脆弱的问题，特别是在标注数据稀缺的情况下。

Method: 方法结合Mamba特征建模、区域注意力模型和贝叶斯推理以提升局部到全局的特征建模，同时通过高质量伪标签选择增强稳定性。

Result: 在带有遮挡的细粒度视觉分类基准测试中表现出色，验证了其在有限标注数据情况下的鲁棒性。

Conclusion: 提出的半监督方法在提升FGVC任务表现和适应有限标注数据方面具有显著潜力。

Abstract: Fine Grained Visual Categorization (FGVC) remains a challenging task in
computer vision due to subtle inter class differences and fragile feature
representations. Existing methods struggle in fine grained scenarios,
especially when labeled data is scarce. We propose a semi supervised method
combining Mamba based feature modeling, region attention, and Bayesian
uncertainty. Our approach enhances local to global feature modeling while
focusing on key areas during learning. Bayesian inference selects high quality
pseudo labels for stability. Experiments show strong performance on FGVC
benchmarks with occlusions, demonstrating robustness when labeled data is
limited. Code is available at https://github.com/wxqnl/RAUM Net.

</details>


### [36] [CERBERUS: Crack Evaluation & Recognition Benchmark for Engineering Reliability & Urban Stability](https://arxiv.org/abs/2506.21909)
*Justin Reinman,Sunwoong Choi*

Main category: cs.CV

TL;DR: CERBERUS是一个用于训练和评估检测基础设施中裂缝和缺陷的AI模型的综合基准测试工具，其亮点包括裂缝图像生成器和真实3D检查场景。


<details>
  <summary>Details</summary>
Motivation: 为了解决基础设施中裂缝检测面临的挑战，作者开发了一个既可以生成合成数据又可以支持现实世界场景的工具，从而推动这一领域的研究。

Method: 提出了一种综合的基准工具CERBERUS，包括两种场景设置（简单的Fly-By墙体检查和复杂的Underpass场景），并结合YOLO模型在合成与真实数据上进行测试分析。

Result: 实验结果表明，融合合成与真实数据能够显著提高在真实图像检测中的性能表现。

Conclusion: CERBERUS为缺陷检测系统提供了灵活且可复现的测试方法，推动了自动化基础设施检测领域的研究，并公开源码供学术界使用。

Abstract: CERBERUS is a synthetic benchmark designed to help train and evaluate AI
models for detecting cracks and other defects in infrastructure. It includes a
crack image generator and realistic 3D inspection scenarios built in Unity. The
benchmark features two types of setups: a simple Fly-By wall inspection and a
more complex Underpass scene with lighting and geometry challenges. We tested a
popular object detection model (YOLO) using different combinations of synthetic
and real crack data. Results show that combining synthetic and real data
improves performance on real-world images. CERBERUS provides a flexible,
repeatable way to test defect detection systems and supports future research in
automated infrastructure inspection. CERBERUS is publicly available at
https://github.com/justinreinman/Cerberus-Defect-Generator.

</details>


### [37] [Generating Attribute-Aware Human Motions from Textual Prompt](https://arxiv.org/abs/2506.21912)
*Xinghan Wang,Kun Xu,Fei Li,Cao Sheng,Jiazhong Yu,Yadong Mu*

Main category: cs.CV

TL;DR: 现有的文本驱动人类动作生成模型忽略了影响动作模式的人体属性（例如年龄、性别、体重、身高）。本文提出了一种新框架，将动作语义与人体属性解耦，实现文本驱动，同时考虑用户提供的属性输入生成逼真的动作。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了人体属性对动作模式的影响，无法生成符合用户需求的个性化动作，因此需要探索将动作语义与属性分离的生成方式。

Method: 提出基于结构因果模型的框架，将动作语义与人体属性解耦，利用文本预测动作语义，同时基于用户指定属性进行动作生成，并引入了包含属性标注的HumanAttr数据集。

Result: 在新数据集上进行的广泛实验验证了该模型生成符合文本和属性输入要求的动作的有效性。

Conclusion: 本文提出的模型能够生成逼真且属性感知的动作，并首次为考虑属性的文本驱动动作生成设立了基准。

Abstract: Text-driven human motion generation has recently attracted considerable
attention, allowing models to generate human motions based on textual
descriptions. However, current methods neglect the influence of human
attributes (such as age, gender, weight, and height) which are key factors
shaping human motion patterns. This work represents a pilot exploration for
bridging this gap. We conceptualize each motion as comprising both attribute
information and action semantics, where textual descriptions align exclusively
with action semantics. To achieve this, a new framework inspired by Structural
Causal Models is proposed to decouple action semantics from human attributes,
enabling text-to-semantics prediction and attribute-controlled generation. The
resulting model is capable of generating realistic, attribute-aware motion
aligned with the user's text and attribute inputs. For evaluation, we introduce
HumanAttr, a comprehensive dataset containing attribute annotations for
text-motion pairs, setting the first benchmark for attribute-aware
text-to-motion generation. Extensive experiments on the new dataset validate
our model's effectiveness.

</details>


### [38] [SepFormer: Coarse-to-fine Separator Regression Network for Table Structure Recognition](https://arxiv.org/abs/2506.21920)
*Nam Quan Nguyen,Xuan Phong Pham,Tuan-Anh Tran*

Main category: cs.CV

TL;DR: 提出了SepFormer模型，通过DETR风格的架构进行分割器回归，提升了速度和鲁棒性，同时在多项基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了解决表格结构识别中的速度和鲁棒性问题，提出一种新的分割和合并一体化的方式。

Method: 引入SepFormer，基于DETR风格架构，采用分层Transformer解码器，使用从粗到细的预测方式，包括单线到线段分割，同时引入角度损失进行优化。

Result: SepFormer平均运行速度为25.6帧每秒，在SciTSR、PubTabNet、WTW和iFLYTAB等基准数据集上取得了与最新方法相当的性能表现。

Conclusion: SepFormer模型通过一体化分割器回归方法有效提升了表格结构识别的性能和效率，在多个基准数据集上验证了其实用性与竞争力。

Abstract: The automated reconstruction of the logical arrangement of tables from image
data, termed Table Structure Recognition (TSR), is fundamental for semantic
data extraction. Recently, researchers have explored a wide range of techniques
to tackle this problem, demonstrating significant progress. Each table is a set
of vertical and horizontal separators. Following this realization, we present
SepFormer, which integrates the split-and-merge paradigm into a single step
through separator regression with a DETR-style architecture, improving speed
and robustness. SepFormer is a coarse-to-fine approach that predicts table
separators from single-line to line-strip separators with a stack of two
transformer decoders. In the coarse-grained stage, the model learns to
gradually refine single-line segments through decoder layers with additional
angle loss. At the end of the fine-grained stage, the model predicts line-strip
separators by refining sampled points from each single-line segment. Our
SepFormer can run on average at 25.6 FPS while achieving comparable performance
with state-of-the-art methods on several benchmark datasets, including SciTSR,
PubTabNet, WTW, and iFLYTAB.

</details>


### [39] [ZeroReg3D: A Zero-shot Registration Pipeline for 3D Consecutive Histopathology Image Reconstruction](https://arxiv.org/abs/2506.21923)
*Juming Xiong,Ruining Deng,Jialin Yue,Siqi Lu,Junlin Guo,Marilyn Lionts,Tianyuan Yao,Can Cui,Junchao Zhu,Chongyu Qu,Mengmeng Yin,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: 提出了一种名为ZeroReg3D的新型零样本注册方法，用于从连续组织切片中准确构建3D模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法在保存3D空间关系方面存在局限性，并且深度学习方法在泛化能力和数据需求上存在问题，而非深度学习方法在准确性方面有妥协。

Method: 结合了零样本深度学习关键点匹配与基于优化的仿射和非刚性配准技术，解决了组织变形等关键挑战。

Result: 无需重新训练或微调，ZeroReg3D能够有效处理组织变形、切割工艺、染色和光照不一致等问题。

Conclusion: ZeroReg3D显著改进了组织切片3D重建的准确性，同时具备出色的泛化能力，代码已公开。

Abstract: Histological analysis plays a crucial role in understanding tissue structure
and pathology. While recent advancements in registration methods have improved
2D histological analysis, they often struggle to preserve critical 3D spatial
relationships, limiting their utility in both clinical and research
applications. Specifically, constructing accurate 3D models from 2D slices
remains challenging due to tissue deformation, sectioning artifacts,
variability in imaging techniques, and inconsistent illumination. Deep
learning-based registration methods have demonstrated improved performance but
suffer from limited generalizability and require large-scale training data. In
contrast, non-deep-learning approaches offer better generalizability but often
compromise on accuracy. In this study, we introduced ZeroReg3D, a novel
zero-shot registration pipeline tailored for accurate 3D reconstruction from
serial histological sections. By combining zero-shot deep learning-based
keypoint matching with optimization-based affine and non-rigid registration
techniques, ZeroReg3D effectively addresses critical challenges such as tissue
deformation, sectioning artifacts, staining variability, and inconsistent
illumination without requiring retraining or fine-tuning. The code has been
made publicly available at https://github.com/hrlblab/ZeroReg3D

</details>


### [40] [SPAZER: Spatial-Semantic Progressive Reasoning Agent for Zero-shot 3D Visual Grounding](https://arxiv.org/abs/2506.21924)
*Zhao Jin,Rong-Cheng Tu,Jingyi Liao,Wenhao Sun,Xiao Luo,Shunyu Liu,Dacheng Tao*

Main category: cs.CV

TL;DR: 提出一种名为SPAZER的VLM驱动方法，通过结合3D空间和2D语义的逐步推理框架，实现了无需3D标签数据的零样本3D视觉定位，并显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前3D视觉定位技术中依赖昂贵的3D标签数据，为减轻这种依赖，研究集中于利用预训练语言模型和视觉语言模型开发零样本3D视觉定位方法，而现有方法在空间和语义理解上未能有效结合。

Method: 提出SPAZER方法，该方法首先从最佳视角生成3D渲染图，然后通过基于锚点的候选目标筛选进行粗粒度定位，最后结合检索到的相关2D图片，进行3D-2D联合决策，整合空间与语义推理流。

Result: 实验在ScanRefer与Nr3D基准上验证，SPAZER在准确率上相较前沿零样本方法分别取得了9.0%和10.9%的显著提升。

Conclusion: SPAZER通过跨3D和2D模态的推理机制，成功实现了强健的零样本3D视觉定位，为无需3D标注数据的技术开辟了新路径。

Abstract: 3D Visual Grounding (3DVG) aims to localize target objects within a 3D scene
based on natural language queries. To alleviate the reliance on costly 3D
training data, recent studies have explored zero-shot 3DVG by leveraging the
extensive knowledge and powerful reasoning capabilities of pre-trained LLMs and
VLMs. However, existing paradigms tend to emphasize either spatial (3D-based)
or semantic (2D-based) understanding, limiting their effectiveness in complex
real-world applications. In this work, we introduce SPAZER - a VLM-driven agent
that combines both modalities in a progressive reasoning framework. It first
holistically analyzes the scene and produces a 3D rendering from the optimal
viewpoint. Based on this, anchor-guided candidate screening is conducted to
perform a coarse-level localization of potential objects. Furthermore,
leveraging retrieved relevant 2D camera images, 3D-2D joint decision-making is
efficiently performed to determine the best-matching object. By bridging
spatial and semantic reasoning neural streams, SPAZER achieves robust zero-shot
grounding without training on 3D-labeled data. Extensive experiments on
ScanRefer and Nr3D benchmarks demonstrate that SPAZER significantly outperforms
previous state-of-the-art zero-shot methods, achieving notable gains of 9.0%
and 10.9% in accuracy.

</details>


### [41] [Quality Assessment and Distortion-aware Saliency Prediction for AI-Generated Omnidirectional Images](https://arxiv.org/abs/2506.21925)
*Liu Yang,Huiyu Duan,Jiarui Wang,Jing Liu,Menghan Hu,Xiongkuo Min,Guangtao Zhai,Patrick Le Callet*

Main category: cs.CV

TL;DR: 本论文专注于AI生成的全景图像（AIGODIs）的质量评估、失真感知显著性预测及优化问题，并提出了一套数据库（OHF2024）和相关模型（BLIP2OIQA和BLIP2OISal）。


<details>
  <summary>Details</summary>
Motivation: 尽管AIGODIs在虚拟现实（VR）和增强现实（AR）中潜力巨大，但其质量评估及优化仍属研究空白。本研究旨在填补这一领域的空白。

Method: 提出并构建大型数据库（OHF2024），通过该数据库收集质量评分及失真感知区域。基于BLIP-2模型，开发了共享编码器的两个模型（BLIP2OIQA和BLIP2OISal），分别用于人类视觉体验评估与失真区域预测。此外，提出一种基于上述模型的自动优化流程，用于提升AIGODIs视觉质量。

Result: 实验表明，BLIP2OIQA和BLIP2OISal在AIGODIs视觉体验评估和失真感知预测任务中达到了最先进的效果，并可在优化流程中有效应用。

Conclusion: 所开发的数据库与模型为AIGODIs的质量评估和优化设立了新的基准，具有广泛的未来研究意义。

Abstract: With the rapid advancement of Artificial Intelligence Generated Content
(AIGC) techniques, AI generated images (AIGIs) have attracted widespread
attention, among which AI generated omnidirectional images (AIGODIs) hold
significant potential for Virtual Reality (VR) and Augmented Reality (AR)
applications. AI generated omnidirectional images exhibit unique quality
issues, however, research on the quality assessment and optimization of
AI-generated omnidirectional images is still lacking. To this end, this work
first studies the quality assessment and distortion-aware saliency prediction
problems for AIGODIs, and further presents a corresponding optimization
process. Specifically, we first establish a comprehensive database to reflect
human feedback for AI-generated omnidirectionals, termed OHF2024, which
includes both subjective quality ratings evaluated from three perspectives and
distortion-aware salient regions. Based on the constructed OHF2024 database, we
propose two models with shared encoders based on the BLIP-2 model to evaluate
the human visual experience and predict distortion-aware saliency for
AI-generated omnidirectional images, which are named as BLIP2OIQA and
BLIP2OISal, respectively. Finally, based on the proposed models, we present an
automatic optimization process that utilizes the predicted visual experience
scores and distortion regions to further enhance the visual quality of an
AI-generated omnidirectional image. Extensive experiments show that our
BLIP2OIQA model and BLIP2OISal model achieve state-of-the-art (SOTA) results in
the human visual experience evaluation task and the distortion-aware saliency
prediction task for AI generated omnidirectional images, and can be effectively
used in the optimization process. The database and codes will be released on
https://github.com/IntMeGroup/AIGCOIQA to facilitate future research.

</details>


### [42] [SDRNET: Stacked Deep Residual Network for Accurate Semantic Segmentation of Fine-Resolution Remotely Sensed Images](https://arxiv.org/abs/2506.21945)
*Naftaly Wambugu,Ruisheng Wang,Bo Guo,Tianshu Yu,Sheng Xu,Mohammed Elhassan*

Main category: cs.CV

TL;DR: 本文提出了一种堆叠深度残差网络（SDRNet），用于高分辨率遥感图像的语义分割，实验表明其在多个数据集上的效果表现优异。


<details>
  <summary>Details</summary>
Motivation: 高分辨率遥感图像中复杂类间差异、目标遮挡及大小变化影响语义分割的准确性，为此需要改进深度学习模型以学习鲁棒特征和多上下文特征。

Method: 设计了一种堆叠深度残差网络（SDRNet），通过两个堆叠的编码器-解码器网络保持语义信息的全局性和空间细节，并在其间插入膨胀残差模块（DRB）以捕获全局依赖关系。

Result: 实验基于ISPRS Vaihingen和Potsdam数据集，显示提出的SDRNet在语义分割任务中的性能优于现有深度卷积网络（DCNNs）。

Conclusion: SDRNet能有效解决复杂场景中语义分割问题，证明其在高分辨率遥感图像中的可行性和优越性。

Abstract: Land cover maps generated from semantic segmentation of high-resolution
remotely sensed images have drawn mucon in the photogrammetry and remote
sensing research community. Currently, massive fine-resolution remotely sensed
(FRRS) images acquired by improving sensing and imaging technologies become
available. However, accurate semantic segmentation of such FRRS images is
greatly affected by substantial class disparities, the invisibility of key
ground objects due to occlusion, and object size variation. Despite the
extraordinary potential in deep convolutional neural networks (DCNNs) in image
feature learning and representation, extracting sufficient features from FRRS
images for accurate semantic segmentation is still challenging. These
challenges demand the deep learning models to learn robust features and
generate sufficient feature descriptors. Specifically, learning
multi-contextual features to guarantee adequate coverage of varied object sizes
from the ground scene and harnessing global-local contexts to overcome class
disparities challenge even profound networks. Deeper networks significantly
lose spatial details due to gradual downsampling processes resulting in poor
segmentation results and coarse boundaries. This article presents a stacked
deep residual network (SDRNet) for semantic segmentation from FRRS images. The
proposed framework utilizes two stacked encoder-decoder networks to harness
long-range semantics yet preserve spatial information and dilated residual
blocks (DRB) between each encoder and decoder network to capture sufficient
global dependencies thus improving segmentation performance. Our experimental
results obtained using the ISPRS Vaihingen and Potsdam datasets demonstrate
that the SDRNet performs effectively and competitively against current DCNNs in
semantic segmentation.

</details>


### [43] [Exploring Semantic Masked Autoencoder for Self-supervised Point Cloud Understanding](https://arxiv.org/abs/2506.21957)
*Yixin Zha,Chuxin Wang,Wenfei Yang,Tianzhu Zhang*

Main category: cs.CV

TL;DR: 本论文介绍了一种名为语义掩码自动编码器的新方法，旨在通过加强组件语义建模和掩码策略来提高点云理解的效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于随机掩码的方法在捕捉点云的合理语义关系上存在不足，亟需改进点云的语义表达能力和预训练模型的性能。

Method: 提出一种语义掩码自动编码器，包括组件语义建模模块和组件语义增强掩码策略，同时开发了组件语义增强提示调优策略以增强下游任务表现。

Result: 在ScanObjectNN、ModelNet40和ShapeNetPart等数据集上的实验表明，所提模块性能优越且有效。

Conclusion: 新的方法通过优化语义建模和掩码策略显著提高了点云理解能力，为相关任务提供了新的解决思路。

Abstract: Point cloud understanding aims to acquire robust and general feature
representations from unlabeled data. Masked point modeling-based methods have
recently shown significant performance across various downstream tasks. These
pre-training methods rely on random masking strategies to establish the
perception of point clouds by restoring corrupted point cloud inputs, which
leads to the failure of capturing reasonable semantic relationships by the
self-supervised models. To address this issue, we propose Semantic Masked
Autoencoder, which comprises two main components: a prototype-based component
semantic modeling module and a component semantic-enhanced masking strategy.
Specifically, in the component semantic modeling module, we design a component
semantic guidance mechanism to direct a set of learnable prototypes in
capturing the semantics of different components from objects. Leveraging these
prototypes, we develop a component semantic-enhanced masking strategy that
addresses the limitations of random masking in effectively covering complete
component structures. Furthermore, we introduce a component semantic-enhanced
prompt-tuning strategy, which further leverages these prototypes to improve the
performance of pre-trained models in downstream tasks. Extensive experiments
conducted on datasets such as ScanObjectNN, ModelNet40, and ShapeNetPart
demonstrate the effectiveness of our proposed modules.

</details>


### [44] [TASeg: Text-aware RGB-T Semantic Segmentation based on Fine-tuning Vision Foundation Models](https://arxiv.org/abs/2506.21975)
*Meng Yu,Te Cui,Qitong Chu,Wenjie Song,Yi Yang,Yufeng Yue*

Main category: cs.CV

TL;DR: 提出了一个名为TASeg的文本感知RGB-T分割框架，通过动态特征融合模块和文本嵌入增强语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有RGB-T语义分割模型严重依赖低级视觉特征、类别视觉特性相似导致分割不准确的问题，以及SAM在实例分割中的局限性和多模态融合的效率问题。

Method: 提出TASeg框架，采用LoRA微调技术适配视觉基础模型，引入动态特征融合模块以整合视觉模态信息并融入CLIP生成的文本嵌入进行语义对齐。

Result: 在多个数据集上的实验表明，该方法在具有挑战性的场景中表现优异，同时使用较少的可训练参数。

Conclusion: TASeg有效解决了多模态语义分割中的关键问题，展现了较高的准确性和效率。

Abstract: Reliable semantic segmentation of open environments is essential for
intelligent systems, yet significant problems remain: 1) Existing RGB-T
semantic segmentation models mainly rely on low-level visual features and lack
high-level textual information, which struggle with accurate segmentation when
categories share similar visual characteristics. 2) While SAM excels in
instance-level segmentation, integrating it with thermal images and text is
hindered by modality heterogeneity and computational inefficiency. To address
these, we propose TASeg, a text-aware RGB-T segmentation framework by using
Low-Rank Adaptation (LoRA) fine-tuning technology to adapt vision foundation
models. Specifically, we propose a Dynamic Feature Fusion Module (DFFM) in the
image encoder, which effectively merges features from multiple visual
modalities while freezing SAM's original transformer blocks. Additionally, we
incorporate CLIP-generated text embeddings in the mask decoder to enable
semantic alignment, which further rectifies the classification error and
improves the semantic understanding accuracy. Experimental results across
diverse datasets demonstrate that our method achieves superior performance in
challenging scenarios with fewer trainable parameters.

</details>


### [45] [R1-Track: Direct Application of MLLMs to Visual Object Tracking via Reinforcement Learning](https://arxiv.org/abs/2506.21980)
*Biao Wang,Wenwen Li*

Main category: cs.CV

TL;DR: 本研究通过微调开源的多模态大语言模型Qwen2.5-VL，提出了适用于视觉单目标跟踪任务的新模型R1-Track。


<details>
  <summary>Details</summary>
Motivation: 目前的目标跟踪方法通常依赖显式分类回归建模与大规模数据集监督训练，灵活性不足。因此研究探讨如何利用多模态大语言模型提升任务表现。

Method: 使用群相对策略优化（GRPO）方法对Qwen2.5-VL进行微调，并结合基于规则的奖励函数，在小规模数据集上训练生成模型R1-Track。

Result: 新模型R1-Track在GOT-10k基准测试中表现突出，并支持灵活的初始化方式（如通过边界框或文本描述）。

Conclusion: R1-Track在保持多模态能力的同时增强了视频目标跟踪性能，展示了大语言模型应用于此领域的潜力，并讨论了未来改进方向。

Abstract: Visual single object tracking aims to continuously localize and estimate the
scale of a target in subsequent video frames, given only its initial state in
the first frame. This task has traditionally been framed as a template matching
problem, evolving through major phases including correlation filters,
two-stream networks, and one-stream networks with significant progress
achieved. However, these methods typically require explicit classification and
regression modeling, depend on supervised training with large-scale datasets,
and are limited to the single task of tracking, lacking flexibility. In recent
years, multi-modal large language models (MLLMs) have advanced rapidly.
Open-source models like Qwen2.5-VL, a flagship MLLMs with strong foundational
capabilities, demonstrate excellent performance in grounding tasks. This has
spurred interest in applying such models directly to visual tracking. However,
experiments reveal that Qwen2.5-VL struggles with template matching between
image pairs (i.e., tracking tasks). Inspired by deepseek-R1, we fine-tuned
Qwen2.5-VL using the group relative policy optimization (GRPO) reinforcement
learning method on a small-scale dataset with a rule-based reward function. The
resulting model, R1-Track, achieved notable performance on the GOT-10k
benchmark. R1-Track supports flexible initialization via bounding boxes or text
descriptions while retaining most of the original model's general capabilities.
And we further discuss potential improvements for R1-Track. This rough
technical report summarizes our findings as of May 2025.

</details>


### [46] [RoboEnvision: A Long-Horizon Video Generation Model for Multi-Task Robot Manipulation](https://arxiv.org/abs/2506.22007)
*Liudi Yang,Yang Bai,George Eskandar,Fengyi Shen,Mohammad Altillawi,Dong Chen,Soumajit Majumder,Ziyuan Liu,Gitta Kutyniok,Abhinav Valada*

Main category: cs.CV

TL;DR: 提出了一种非自回归的视频生成方法，用于长时间机器人操作任务，避免了误差累积，结合多帧生成及插帧技术进行高质量生成。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频扩散模型在长时间机器人任务表现不佳，主要受限于视频生成误差积累及短时间预测的缺陷。

Method: 提出了三大改进：1) 将高层目标分解为原子任务并生成关键帧，再通过扩散模型进行插帧实现长视频生成；2) 引入语义保持注意模块保证关键帧一致性；3) 设计了轻量策略模型预测机器人关节状态。

Result: 在视频质量和一致性上达到了新的基准，并在长时间任务上优于之前的策略模型。

Conclusion: 方法显著提高了长时间机器人操作任务的视频生成效果，对基于视频的机器人规划具有重要意义。

Abstract: We address the problem of generating long-horizon videos for robotic
manipulation tasks. Text-to-video diffusion models have made significant
progress in photorealism, language understanding, and motion generation but
struggle with long-horizon robotic tasks. Recent works use video diffusion
models for high-quality simulation data and predictive rollouts in robot
planning. However, these works predict short sequences of the robot achieving
one task and employ an autoregressive paradigm to extend to the long horizon,
leading to error accumulations in the generated video and in the execution. To
overcome these limitations, we propose a novel pipeline that bypasses the need
for autoregressive generation. We achieve this through a threefold
contribution: 1) we first decompose the high-level goals into smaller atomic
tasks and generate keyframes aligned with these instructions. A second
diffusion model then interpolates between each of the two generated frames,
achieving the long-horizon video. 2) We propose a semantics preserving
attention module to maintain consistency between the keyframes. 3) We design a
lightweight policy model to regress the robot joint states from generated
videos. Our approach achieves state-of-the-art results on two benchmarks in
video quality and consistency while outperforming previous policy models on
long-horizon tasks.

</details>


### [47] [Towards Universal & Efficient Model Compression via Exponential Torque Pruning](https://arxiv.org/abs/2506.22015)
*Sarthak Ketanbhai Modi,Lim Zi Pong,Shourya Kuchhal,Yoshi Cao,Yupeng Cheng,Teo Yon Shin,Lin Shang-Wei,Zhiming Li*

Main category: cs.CV

TL;DR: 提出了一种名为Exponential Torque Pruning (ETP)的新方法，通过指数作用力正则化，实现更高的模型压缩率并减少精度损失。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习模型由于规模和复杂性不断增长，面临高计算成本和内存使用的问题，因此急需找到高效的模型压缩方法。

Method: 通过引入指数作用力正则化方法，ETP可以有效地裁剪冗余的神经模块，同时保留必要的模块以实现有效推理。

Result: 实验结果表明，ETP比现有最先进的剪枝策略能够实现更高的压缩率，并且精度损失可以忽略不计。

Conclusion: ETP方法简单但有效，不仅解决了现有方法的不足，还在众多领域的实验中表现出色，展示了其在模型压缩中的潜力。

Abstract: The rapid growth in complexity and size of modern deep neural networks (DNNs)
has increased challenges related to computational costs and memory usage,
spurring a growing interest in efficient model compression techniques. Previous
state-of-the-art approach proposes using a Torque-inspired regularization which
forces the weights of neural modules around a selected pivot point. Whereas, we
observe that the pruning effect of this approach is far from perfect, as the
post-trained network is still dense and also suffers from high accuracy drop.
In this work, we attribute such ineffectiveness to the default linear force
application scheme, which imposes inappropriate force on neural module of
different distances. To efficiently prune the redundant and distant modules
while retaining those that are close and necessary for effective inference, in
this work, we propose Exponential Torque Pruning (ETP), which adopts an
exponential force application scheme for regularization. Experimental results
on a broad range of domains demonstrate that, though being extremely simple,
ETP manages to achieve significantly higher compression rate than the previous
state-of-the-art pruning strategies with negligible accuracy drop.

</details>


### [48] [Advancing Facial Stylization through Semantic Preservation Constraint and Pseudo-Paired Supervision](https://arxiv.org/abs/2506.22022)
*Zhanyi Lu,Yue Zhou*

Main category: cs.CV

TL;DR: 本文提出一种面部风格化方法，通过语义保留约束和伪配对监督提高内容一致性及风格化效果，并实现了多模态及参考引导的灵活风格化。


<details>
  <summary>Details</summary>
Motivation: 现有基于StyleGAN的方法生成结果易出现伪影或对源图像的忠实度不足。解决该问题需要处理生成器在风格化过程中语义偏移的问题。

Method: 引入语义保留约束与伪配对监督，并开发多层次伪配对数据集以加强监督约束，实现灵活的多模态和参考引导的风格化设计，无需复杂的网络结构或额外训练。

Result: 实验表明，方法能生成高保真、美观的面部风格化效果，优于之前的方法。

Conclusion: 新方法有效避免生成伪影，并加强内容和风格之间的平衡，提升生成效果的质量和多样性。

Abstract: Facial stylization aims to transform facial images into appealing,
high-quality stylized portraits, with the critical challenge of accurately
learning the target style while maintaining content consistency with the
original image. Although previous StyleGAN-based methods have made significant
advancements, the generated results still suffer from artifacts or insufficient
fidelity to the source image. We argue that these issues stem from neglecting
semantic shift of the generator during stylization. Therefore, we propose a
facial stylization method that integrates semantic preservation constraint and
pseudo-paired supervision to enhance the content correspondence and improve the
stylization effect. Additionally, we develop a methodology for creating
multi-level pseudo-paired datasets to implement supervisory constraint.
Furthermore, building upon our facial stylization framework, we achieve more
flexible multimodal and reference-guided stylization without complex network
architecture designs or additional training. Experimental results demonstrate
that our approach produces high-fidelity, aesthetically pleasing facial style
transfer that surpasses previous methods.

</details>


### [49] [Cross-modal Ship Re-Identification via Optical and SAR Imagery: A Novel Dataset and Method](https://arxiv.org/abs/2506.22027)
*Han Wang,Shengyang Li,Jian Yang,Yuxuan Liu,Yixuan Lv,Zhuang Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的船舶跟踪方法，基于光学和SAR传感器的低地球轨道星座，并公开了一个用于评估的HOSS ReID数据集和跨模态识别方法TransOSS。


<details>
  <summary>Details</summary>
Motivation: 现有的船舶跟踪方法依赖静止卫星和视频卫星，存在分辨率低、受天气条件影响大以及拍摄时间短、覆盖范围有限的问题，难以满足实际应用需求。

Method: 提出并公开了HOSS ReID数据集，该数据集包含在不同条件下拍摄的同一船舶图像，同时提出了基于Vision Transformer的跨模态船舶再识别方法TransOSS，用于解决多模态任务中的特征提取和匹配问题。

Result: HOSS ReID数据集和TransOSS确保了更短的成像周期，实现全天候船舶跟踪，通过对大规模光学-SAR图像对的对比学习预训练，提升了模型提取模态无关特征的能力。

Conclusion: 该方法改进了现有船舶跟踪手段的局限性，通过多模态数据和先进的网络架构，实现了更高效和精准的船舶追踪。

Abstract: Detecting and tracking ground objects using earth observation imagery remains
a significant challenge in the field of remote sensing. Continuous maritime
ship tracking is crucial for applications such as maritime search and rescue,
law enforcement, and shipping analysis. However, most current ship tracking
methods rely on geostationary satellites or video satellites. The former offer
low resolution and are susceptible to weather conditions, while the latter have
short filming durations and limited coverage areas, making them less suitable
for the real-world requirements of ship tracking. To address these limitations,
we present the Hybrid Optical and Synthetic Aperture Radar (SAR) Ship
Re-Identification Dataset (HOSS ReID dataset), designed to evaluate the
effectiveness of ship tracking using low-Earth orbit constellations of optical
and SAR sensors. This approach ensures shorter re-imaging cycles and enables
all-weather tracking. HOSS ReID dataset includes images of the same ship
captured over extended periods under diverse conditions, using different
satellites of different modalities at varying times and angles. Furthermore, we
propose a baseline method for cross-modal ship re-identification, TransOSS,
which is built on the Vision Transformer architecture. It refines the patch
embedding structure to better accommodate cross-modal tasks, incorporates
additional embeddings to introduce more reference information, and employs
contrastive learning to pre-train on large-scale optical-SAR image pairs,
ensuring the model's ability to extract modality-invariant features. Our
dataset and baseline method are publicly available on
https://github.com/Alioth2000/Hoss-ReID.

</details>


### [50] [Partial CLIP is Enough: Chimera-Seg for Zero-shot Semantic Segmentation](https://arxiv.org/abs/2506.22032)
*Jialei Chen,Xu Zheng,Danda Pani Paudel,Luc Van Gool,Hiroshi Murase,Daisuke Deguchi*

Main category: cs.CV

TL;DR: 本研究提出了一种新的零样本语义分割方法Chimera-Seg，结合分割骨干网络和CLIP语义头，通过选择性全局蒸馏和语义对齐模块提升了分割性能，在两个基准上分别提高了0.9%和1.2%的hIoU。


<details>
  <summary>Details</summary>
Motivation: 解决零样本语义分割中的视觉特征与文本空间对齐困难问题，以及全局表示与局部细粒度特征之间的语义差距问题。

Method: 提出Chimera-Seg模型，结合分割骨干网络和CLIP语义头，利用选择性全局蒸馏（SGD）和语义对齐模块（SAM）来提升视觉与语义对齐性能。

Result: 在两个基准上实验验证，提出方法显著提升了分割性能，分别提高了0.9%和1.2%的hIoU。

Conclusion: 通过引入Chimera-Seg架构和配套的蒸馏与对齐机制，有效解决了语义与视觉对齐难题，实现了更高效的零样本语义分割。

Abstract: Zero-shot Semantic Segmentation (ZSS) aims to segment both seen and unseen
classes using supervision from only seen classes. Beyond adaptation-based
methods, distillation-based approaches transfer vision-language alignment of
vision-language model, e.g., CLIP, to segmentation models. However, such
knowledge transfer remains challenging due to: (1) the difficulty of aligning
vision-based features with the textual space, which requires combining spatial
precision with vision-language alignment; and (2) the semantic gap between
CLIP's global representations and the local, fine-grained features of
segmentation models. To address challenge (1), we propose Chimera-Seg, which
integrates a segmentation backbone as the body and a CLIP-based semantic head
as the head, like the Chimera in Greek mythology, combining spatial precision
with vision-language alignment. Specifically, Chimera-Seg comprises a trainable
segmentation model and a CLIP Semantic Head (CSH), which maps dense features
into the CLIP-aligned space. The CSH incorporates a frozen subnetwork and fixed
projection layers from the CLIP visual encoder, along with lightweight
trainable components. The partial module from CLIP visual encoder, paired with
the segmentation model, retains segmentation capability while easing the
mapping to CLIP's semantic space. To address challenge (2), we propose
Selective Global Distillation (SGD), which distills knowledge from dense
features exhibiting high similarity to the CLIP CLS token, while gradually
reducing the number of features used for alignment as training progresses.
Besides, we also use a Semantic Alignment Module (SAM) to further align dense
visual features with semantic embeddings extracted from the frozen CLIP text
encoder. Experiments on two benchmarks show improvements of 0.9% and 1.2% in
hIoU.

</details>


### [51] [Few-Shot Identity Adaptation for 3D Talking Heads via Global Gaussian Field](https://arxiv.org/abs/2506.22044)
*Hong Nie,Fuyuan Cao,Lu Chen,Fengxin Chen,Yuefeng Zou,Jun Yu*

Main category: cs.CV

TL;DR: 提出FIAG框架，通过少量训练数据即可实现高效的身份适配，克服了传统方法对身份特定模型的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成高清说话人模型时高度依赖于特定身份模型，导致计算成本高且可扩展性较差。

Method: 引入Global Gaussian Field与Universal Motion Field，从全球共享结构和通用动态先验中实现身份快速适配。

Result: 通过对比实验和消融研究，证明了该方法优于现有最先进方法。

Conclusion: FIAG克服了传统身份特定模型依赖问题，实现了高效、通用的3D说话人合成。

Abstract: Reconstruction and rendering-based talking head synthesis methods achieve
high-quality results with strong identity preservation but are limited by their
dependence on identity-specific models. Each new identity requires training
from scratch, incurring high computational costs and reduced scalability
compared to generative model-based approaches. To overcome this limitation, we
propose FIAG, a novel 3D speaking head synthesis framework that enables
efficient identity-specific adaptation using only a few training footage. FIAG
incorporates Global Gaussian Field, which supports the representation of
multiple identities within a shared field, and Universal Motion Field, which
captures the common motion dynamics across diverse identities. Benefiting from
the shared facial structure information encoded in the Global Gaussian Field
and the general motion priors learned in the motion field, our framework
enables rapid adaptation from canonical identity representations to specific
ones with minimal data. Extensive comparative and ablation experiments
demonstrate that our method outperforms existing state-of-the-art approaches,
validating both the effectiveness and generalizability of the proposed
framework. Code is available at: \textit{https://github.com/gme-hong/FIAG}.

</details>


### [52] [EnLVAM: Enhanced Left Ventricle Linear Measurements Utilizing Anatomical Motion Mode](https://arxiv.org/abs/2506.22063)
*Durgesh K. Singh,Ahcene Boubekki,Qing Cao,Svein Arne Aase,Robert Jenssen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: 本文提出了一种新型框架，通过对左心室（LV）线性测量的改进，利用直线约束提高精度。


<details>
  <summary>Details</summary>
Motivation: 目前通过手动放置标记点进行左心室测量既耗时又容易出错，现有的深度学习方法也存在标记点错位的问题，从而导致测量结果不准确。

Method: 提出了一种新型框架，在解剖学M模式图像中训练一个标记点检测器，利用实时计算得到的AMM图像再转换回B模式空间，结合直线约束减少了标记点错位问题。

Result: 实验表明，该方法相比传统B模式方法显著提高了测量精度，并且能够良好地泛化至不同网络架构上。

Conclusion: 设计了一种半自动流程，用户只需放置扫描线即可，减少了操作复杂性并保持了临床相关性。

Abstract: Linear measurements of the left ventricle (LV) in the Parasternal Long Axis
(PLAX) view using B-mode echocardiography are crucial for cardiac assessment.
These involve placing 4-6 landmarks along a virtual scanline (SL) perpendicular
to the LV axis near the mitral valve tips. Manual placement is time-consuming
and error-prone, while existing deep learning methods often misalign landmarks,
causing inaccurate measurements. We propose a novel framework that enhances LV
measurement accuracy by enforcing straight-line constraints. A landmark
detector is trained on Anatomical M-Mode (AMM) images, computed in real time
from B-mode videos, then transformed back to B-mode space. This approach
addresses misalignment and reduces measurement errors. Experiments show
improved accuracy over standard B-mode methods, and the framework generalizes
well across network architectures. Our semi-automatic design includes a
human-in-the-loop step where the user only places the SL, simplifying
interaction while preserving alignment flexibility and clinical relevance.

</details>


### [53] [MirrorMe: Towards Realtime and High Fidelity Audio-Driven Halfbody Animation](https://arxiv.org/abs/2506.22065)
*Dechao Meng,Steven Xiao,Xindi Zhang,Guangyuan Wang,Peng Zhang,Qi Wang,Bang Zhang,Liefeng Bo*

Main category: cs.CV

TL;DR: MirrorMe是一种实时、可控的音驱动肖像动画框架，基于LTX视频模型，通过创新方法实现高保真、时序一致的动画生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有音驱动肖像动画在高保真和实时生成中面临的延迟和时序一致性问题。

Method: 提出三项改进：1. 引入基于VAE的图像拼接与自注意机制，确保身份一致性；2. 使用为LTX量身定制的因果音频编码器与适配器，实现精准音频表情同步；3. 通过逐步训练策略提升面部、半身与手势控制能力。

Result: 在EMTD基准测试上，MirrorMe在保真度、唇语同步精度和时序稳定性方面表现达到了最新水平。

Conclusion: MirrorMe框架可在解决性能瓶颈的同时，实现高质量且时序一致的音驱动肖像动画生成。

Abstract: Audio-driven portrait animation, which synthesizes realistic videos from
reference images using audio signals, faces significant challenges in real-time
generation of high-fidelity, temporally coherent animations. While recent
diffusion-based methods improve generation quality by integrating audio into
denoising processes, their reliance on frame-by-frame UNet architectures
introduces prohibitive latency and struggles with temporal consistency. This
paper introduces MirrorMe, a real-time, controllable framework built on the LTX
video model, a diffusion transformer that compresses video spatially and
temporally for efficient latent space denoising. To address LTX's trade-offs
between compression and semantic fidelity, we propose three innovations: 1. A
reference identity injection mechanism via VAE-encoded image concatenation and
self-attention, ensuring identity consistency; 2. A causal audio encoder and
adapter tailored to LTX's temporal structure, enabling precise audio-expression
synchronization; and 3. A progressive training strategy combining close-up
facial training, half-body synthesis with facial masking, and hand pose
integration for enhanced gesture control. Extensive experiments on the EMTD
Benchmark demonstrate MirrorMe's state-of-the-art performance in fidelity,
lip-sync accuracy, and temporal stability.

</details>


### [54] [Single-Scanline Relative Pose Estimation for Rolling Shutter Cameras](https://arxiv.org/abs/2506.22069)
*Petr Hruby,Marc Pollefeys*

Main category: cs.CV

TL;DR: 提出了一种新方法，通过线与单条扫描线的交点进行Rolling Shutter摄像头的相对位姿估计，不需要明确的运动建模。


<details>
  <summary>Details</summary>
Motivation: 现有方法在Rolling Shutter相机的位姿估计中依赖于运动模型，且存在局限性。提出一个无需运动模型的新方法。

Method: 利用扫描线与投影线的交集实现位姿估计，通过最小求解器处理不同设置中的问题，包括重力已知等特殊场景。

Result: 在Fastec数据集上的实验验证了方法用于Rolling Shutter三维重建初始化的可行性。

Conclusion: 该方法为Rolling Shutter SfM奠定了基础，展示了不依赖运动模型、可独立估计每条扫描线位姿的潜力。代码将公开。

Abstract: We propose a novel approach for estimating the relative pose between rolling
shutter cameras using the intersections of line projections with a single
scanline per image. This allows pose estimation without explicitly modeling
camera motion. Alternatively, scanlines can be selected within a single image,
enabling single-view relative pose estimation for scanlines of rolling shutter
cameras. Our approach is designed as a foundational building block for rolling
shutter structure-from-motion (SfM), where no motion model is required, and
each scanline's pose can be computed independently. % We classify minimal
solvers for this problem in both generic and specialized settings, including
cases with parallel lines and known gravity direction, assuming known
intrinsics and no lens distortion. Furthermore, we develop minimal solvers for
the parallel-lines scenario, both with and without gravity priors, by
leveraging connections between this problem and the estimation of 2D structure
from 1D cameras. % Experiments on rolling shutter images from the Fastec
dataset demonstrate the feasibility of our approach for initializing rolling
shutter SfM, highlighting its potential for further development. % The code
will be made publicly available.

</details>


### [55] [Reasoning in machine vision: learning to think fast and slow](https://arxiv.org/abs/2506.22075)
*Shaheer U. Saeed,Yipei Wang,Veeru Kasivisvanathan,Brian R. Davidson,Matthew J. Clarkson,Yipeng Hu,Daniel C. Alexander*

Main category: cs.CV

TL;DR: 提出一种新学习范式，通过在推理时间内使用更多的计算资源，改进视觉任务的推理精度，即使在标注数据有限的条件下亦是如此。


<details>
  <summary>Details</summary>
Motivation: 现有机器智能在推理时无法动态改进解决方案，尤其是在非语言推理（如视觉感知、空间推理等）领域存在重大挑战，亟需新的方法促进机器推理能力的提高。

Method: 受人类认知心理学中双系统理论启发，集成了快速推理的System I模块和通过自对弈强化学习来逐步优化解决方案的System II模块，旨在模拟人类推理方法。

Result: 通过延长推理时间，该方法不仅在计算机视觉基准测试中表现优越，还在医疗图像癌症定位任务中超越了大规模监督学习模型、基础模型，甚至人类专家。

Conclusion: 该方法展示了非语言机器推理的变革性潜力，尤其是在数据稀缺和复杂任务环境下应用的实用性。

Abstract: Reasoning is a hallmark of human intelligence, enabling adaptive
decision-making in complex and unfamiliar scenarios. In contrast, machine
intelligence remains bound to training data, lacking the ability to dynamically
refine solutions at inference time. While some recent advances have explored
reasoning in machines, these efforts are largely limited to verbal domains such
as mathematical problem-solving, where explicit rules govern step-by-step
reasoning. Other critical real-world tasks - including visual perception,
spatial reasoning, and radiological diagnosis - require non-verbal reasoning,
which remains an open challenge. Here we present a novel learning paradigm that
enables machine reasoning in vision by allowing performance improvement with
increasing thinking time (inference-time compute), even under conditions where
labelled data is very limited. Inspired by dual-process theories of human
cognition in psychology, our approach integrates a fast-thinking System I
module for familiar tasks, with a slow-thinking System II module that
iteratively refines solutions using self-play reinforcement learning. This
paradigm mimics human reasoning by proposing, competing over, and refining
solutions in data-scarce scenarios. We demonstrate superior performance through
extended thinking time, compared not only to large-scale supervised learning
but also foundation models and even human experts, in real-world vision tasks.
These tasks include computer-vision benchmarks and cancer localisation on
medical images across five organs, showcasing transformative potential for
non-verbal machine reasoning.

</details>


### [56] [Towards Accurate Heart Rate Measurement from Ultra-Short Video Clips via Periodicity-Guided rPPG Estimation and Signal Reconstruction](https://arxiv.org/abs/2506.22078)
*Pei-Kai Huanga,Ya-Ting Chan,Kuan-Wen Chen,Yen-Chun Chou,Shih-Yu Yang,Chiou-Ting Hsu*

Main category: cs.CV

TL;DR: 提出了一种用于从超短2秒视频片段精确测量心率的新方法，通过周期一致性和生成长rPPG信号来解决当前方法的不足。


<details>
  <summary>Details</summary>
Motivation: 当前方法在处理10秒左右的视频片段时表现良好，但对超短视频片段（如2秒）中的心率估计存在不足，因此需要开发能够处理短片段并提高精度的新方法。

Method: 提出了一种周期性引导的rPPG估计方法，通过加强短片段与长片段rPPG信号之间的周期一致性，并设计生成器重建长rPPG信号以减少频谱泄漏对估计的干扰。

Result: 在四个rPPG估计基准数据集上的实验表明该方法能够精准地从超短视频片段中估计心率，并超越现有技术，达到了最新的科学性能。

Conclusion: 该研究表明通过周期一致性和信号重建可以有效提升超短片段的心率估计精度，为远程健康监测提供了新思路。

Abstract: Many remote Heart Rate (HR) measurement methods focus on estimating remote
photoplethysmography (rPPG) signals from video clips lasting around 10 seconds
but often overlook the need for HR estimation from ultra-short video clips. In
this paper, we aim to accurately measure HR from ultra-short 2-second video
clips by specifically addressing two key challenges. First, to overcome the
limited number of heartbeat cycles in ultra-short video clips, we propose an
effective periodicity-guided rPPG estimation method that enforces consistent
periodicity between rPPG signals estimated from ultra-short clips and their
much longer ground truth signals. Next, to mitigate estimation inaccuracies due
to spectral leakage, we propose including a generator to reconstruct longer
rPPG signals from ultra-short ones while preserving their periodic consistency
to enable more accurate HR measurement. Extensive experiments on four rPPG
estimation benchmark datasets demonstrate that our proposed method not only
accurately measures HR from ultra-short video clips but also outperform
previous rPPG estimation techniques to achieve state-of-the-art performance.

</details>


### [57] [BézierGS: Dynamic Urban Scene Reconstruction with Bézier Curve Gaussian Splatting](https://arxiv.org/abs/2506.22099)
*Zipei Ma,Junzhe Jiang,Yurui Chen,Li Zhang*

Main category: cs.CV

TL;DR: 提出BezierGS方法，用于动态物体的轨迹表示与场景重建，解决了现有方法对高精度标注的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有的街道场景重建方法需依赖物体姿态标注，限制了大规模场景重建的可行性。为解决这一问题，开发无需高精度标注的重建方法。

Method: 采用Bezier曲线表示动态物体运动轨迹，通过曲线的可学习建模及时间信息利用，改善姿态误差，并引入动态物体渲染监督和曲线一致性约束。

Result: 在Waymo Open Dataset和nuPlan基准上验证，提出方法在动态与静态场景重建以及新视角合成任务上优于现有最先进的方法。

Conclusion: BezierGS减少了对高精度标注依赖，有效提升了动态与静态场景重建的效果，为无人驾驶模拟器的发展提供了新方式。

Abstract: The realistic reconstruction of street scenes is critical for developing
real-world simulators in autonomous driving. Most existing methods rely on
object pose annotations, using these poses to reconstruct dynamic objects and
move them during the rendering process. This dependence on high-precision
object annotations limits large-scale and extensive scene reconstruction. To
address this challenge, we propose B\'ezier curve Gaussian splatting
(B\'ezierGS), which represents the motion trajectories of dynamic objects using
learnable B\'ezier curves. This approach fully leverages the temporal
information of dynamic objects and, through learnable curve modeling,
automatically corrects pose errors. By introducing additional supervision on
dynamic object rendering and inter-curve consistency constraints, we achieve
reasonable and accurate separation and reconstruction of scene elements.
Extensive experiments on the Waymo Open Dataset and the nuPlan benchmark
demonstrate that B\'ezierGS outperforms state-of-the-art alternatives in both
dynamic and static scene components reconstruction and novel view synthesis.

</details>


### [58] [Tied Prototype Model for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2506.22101)
*Hyeongji Kim,Stine Hansen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: 现有的少样本分割主要基于类别特定原型，但受限于高背景变异性。TPM模型提出对背景和前景分布的原型位置进行绑定，并解决适应性差等问题，提升了分割效果。


<details>
  <summary>Details</summary>
Motivation: 发现现有方法如ADNet在处理背景高可变性、使用单一原型及固定阈值方面有所不足。

Method: 提出了一种基于概率的原型模型TPM，绑定前景与背景分布，并扩展到多原型和多类别分割，同时利用自适应阈值提升性能。

Result: TPM模型在分割精度上获得显著提升，并成功分离非典型背景特征。

Conclusion: TPM引入了一种新颖的视角处理医疗影像分割问题，显著改善了效果。

Abstract: Common prototype-based medical image few-shot segmentation (FSS) methods
model foreground and background classes using class-specific prototypes.
However, given the high variability of the background, a more promising
direction is to focus solely on foreground modeling, treating the background as
an anomaly -- an approach introduced by ADNet. Yet, ADNet faces three key
limitations: dependence on a single prototype per class, a focus on binary
classification, and fixed thresholds that fail to adapt to patient and organ
variability. To address these shortcomings, we propose the Tied Prototype Model
(TPM), a principled reformulation of ADNet with tied prototype locations for
foreground and background distributions. Building on its probabilistic
foundation, TPM naturally extends to multiple prototypes and multi-class
segmentation while effectively separating non-typical background features.
Notably, both extensions lead to improved segmentation accuracy. Finally, we
leverage naturally occurring class priors to define an ideal target for
adaptive thresholds, boosting segmentation performance. Taken together, TPM
provides a fresh perspective on prototype-based FSS for medical image
segmentation. The code can be found at https://github.com/hjk92g/TPM-FSS.

</details>


### [59] [Pedestrian Intention and Trajectory Prediction in Unstructured Traffic Using IDD-PeD](https://arxiv.org/abs/2506.22111)
*Ruthvik Bokkasam,Shankar Gangisetty,A. H. Abdul Hafez,C. V. Jawahar*

Main category: cs.CV

TL;DR: 本文介绍了一个印度驾驶场景中行人数据集，用于研究非结构化环境中行人行为建模的复杂性。


<details>
  <summary>Details</summary>
Motivation: 针对自动驾驶中非结构化交通环境，现有数据集不足以建模复杂行人行为，亟需开发新的综合数据资源。

Method: 构建了一个印度驾驶行人数据集，专注于非结构化环境中的行人行为以捕捉行人与驾驶车辆的交互。

Result: 现有预测方法在新数据集上的意图预测精度下降15%，轨迹预测方法MSE增加1208，显示标准数据集在非结构化环境中的局限性。

Conclusion: 新数据集挑战了当前预测模型的能力，旨在推动行人行为建模领域的发展和进步。

Abstract: With the rapid advancements in autonomous driving, accurately predicting
pedestrian behavior has become essential for ensuring safety in complex and
unpredictable traffic conditions. The growing interest in this challenge
highlights the need for comprehensive datasets that capture unstructured
environments, enabling the development of more robust prediction models to
enhance pedestrian safety and vehicle navigation. In this paper, we introduce
an Indian driving pedestrian dataset designed to address the complexities of
modeling pedestrian behavior in unstructured environments, such as illumination
changes, occlusion of pedestrians, unsignalized scene types and
vehicle-pedestrian interactions. The dataset provides high-level and detailed
low-level comprehensive annotations focused on pedestrians requiring the
ego-vehicle's attention. Evaluation of the state-of-the-art intention
prediction methods on our dataset shows a significant performance drop of up to
$\mathbf{15\%}$, while trajectory prediction methods underperform with an
increase of up to $\mathbf{1208}$ MSE, defeating standard pedestrian datasets.
Additionally, we present exhaustive quantitative and qualitative analysis of
intention and trajectory baselines. We believe that our dataset will open new
challenges for the pedestrian behavior research community to build robust
models. Project Page:
https://cvit.iiit.ac.in/research/projects/cvit-projects/iddped

</details>


### [60] [Pipe Reconstruction from Point Cloud Data](https://arxiv.org/abs/2506.22118)
*Antje Alex,Jannis Stoppe*

Main category: cs.CV

TL;DR: 本文提出了一种管道自动重建方法，可从不完整的激光扫描数据中准确重建复杂管网，支持数字孪生的快速建模。


<details>
  <summary>Details</summary>
Motivation: 目前手动从激光扫描数据建模管道费时费力，亟需自动化方法提高效率以支持工业资产数字孪生。

Method: 采用基于拉普拉斯的压缩进行骨架曲线计算，然后通过曲线延展、滚动球技术和2D圆拟合完成轴心再定位，最终结合3D平滑精炼实现管道重建。

Result: 方法实现了包括管道半径、长度和方向等参数的准确计算，可生成复杂管网的详细3D模型。

Conclusion: 自动化的管道重建流程能够提高建模效率和准确性，同时降低成本，由此促进数字孪生技术的发展。

Abstract: Accurate digital twins of industrial assets, such as ships and offshore
platforms, rely on the precise reconstruction of complex pipe networks.
However, manual modelling of pipes from laser scan data is a time-consuming and
labor-intensive process. This paper presents a pipeline for automated pipe
reconstruction from incomplete laser scan data. The approach estimates a
skeleton curve using Laplacian-based contraction, followed by curve elongation.
The skeleton axis is then recentred using a rolling sphere technique combined
with 2D circle fitting, and refined with a 3D smoothing step. This enables the
determination of pipe properties, including radius, length and orientation, and
facilitates the creation of detailed 3D models of complex pipe networks. By
automating pipe reconstruction, this approach supports the development of
digital twins, allowing for rapid and accurate modeling while reducing costs.

</details>


### [61] [Low-Rank Implicit Neural Representation via Schatten-p Quasi-Norm and Jacobian Regularization](https://arxiv.org/abs/2506.22134)
*Zhengyun Cheng,Changhao Wang,Guanwen Zhang,Yi Xu,Wei Zhou,Xiangyang Ji*

Main category: cs.CV

TL;DR: 本文提出一种基于CP分解的低秩张量函数方法（CP-INR），实现了在多维数据恢复任务中的优越表现。


<details>
  <summary>Details</summary>
Motivation: 现有低秩张量表示方法如Tucker分解虽灵活但不够可解释；CP分解则更自然但很难获得稀疏解。本文希望通过结合CP分解和神经网络实现更高效的数据表示。

Method: 本文提出了一种基于神经网络参数化的CP分解方法（CP-INR），并引入了一种Schatten-p准范数的变分形式实现稀疏分解，同时用谱范数及Hutchinson迹估计器提出平滑正则化方法。

Result: 在图像修复、去噪和点云上采样等多维数据恢复任务中，本文方法相较于先进方法表现优越。

Conclusion: 基于CP分解的低秩张量函数方法（CP-INR）在实现连续数据表示、稀疏性及保平滑性方面具备显著优势，并为多维数据处理提供了有效工具。

Abstract: Higher-order tensors are well-suited for representing multi-dimensional data,
such as color images and videos. Low-rank tensor representation has become
essential in machine learning and computer vision, but existing methods like
Tucker decomposition offer flexibility at the expense of interpretability. In
contrast, while the CANDECOMP/PARAFAC (CP) decomposition provides a more
natural and interpretable tensor structure, obtaining sparse solutions remains
challenging. Leveraging the rich properties of CP decomposition, we propose a
CP-based low-rank tensor function parameterized by neural networks for implicit
neural representation (CP-INR). This approach enables continuous data
representation beyond structured grids, fully exploiting the non-linearity of
tensor data with theoretical guarantees on excess risk bounds. To achieve a
sparse CP decomposition, we introduce a variational form of the Schatten-p
quasi-norm and prove its relationship to multilinear rank minimization. For
smoothness, we propose a regularization term based on the spectral norm of the
Jacobian and Hutchinson's trace estimator. Our proposed smoothness
regularization is SVD-free and avoids explicit chain rule derivations. It can
serve as an alternative to Total Variation (TV) regularization in image
denoising tasks and is naturally applicable to continuous data. Extensive
experiments on multi-dimensional data recovery tasks, including image
inpainting, denoising, and point cloud upsampling, demonstrate the superiority
and versatility of our method compared to state-of-the-art approaches.

</details>


### [62] [Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs](https://arxiv.org/abs/2506.22139)
*Shaojie Zhang,Jiahui Yang,Jianqin Yin,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: Q-Frame 是一种为视频内容和特定查询定制的自适应帧选取与多分辨率缩放方法，有助于提高视频多模态大模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的 Video-LLMs 在处理大数据量和时间复杂性的视频时表现不佳，需要一种更有效的方法来捕捉视频的时空关键信息。

Method: 提出一种无训练、即插即用的 Q-Frame 方法，基于 CLIP 等文本-图像匹配网络，应用 Gumbel-Max 策略实现高效帧选取和多分辨率缩放。

Result: 通过在 MLVU、LongVideoBench 和 Video-MME 等基准数据集上的实验，Q-Frame 展现了其优于现有方法的视频理解能力。

Conclusion: Q-Frame 不仅能提升视频多模态大模型的处理性能，还能保持对关键时空信息的捕捉，适用于多种视频理解任务。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
success in visual understanding tasks. However, challenges persist in adapting
these models for video comprehension due to the large volume of data and
temporal complexity. Existing Video-LLMs using uniform frame sampling often
struggle to capture the query-related crucial spatiotemporal clues of videos
effectively. In this paper, we introduce Q-Frame, a novel approach for adaptive
frame selection and multi-resolution scaling tailored to the video's content
and the specific query. Q-Frame employs a training-free, plug-and-play strategy
generated by a text-image matching network like CLIP, utilizing the Gumbel-Max
trick for efficient frame selection. Q-Frame allows Video-LLMs to process more
frames without exceeding computational limits, thereby preserving critical
temporal and spatial information. We demonstrate Q-Frame's effectiveness
through extensive experiments on benchmark datasets, including MLVU,
LongVideoBench, and Video-MME, illustrating its superiority over existing
methods and its applicability across various video understanding tasks.

</details>


### [63] [Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs](https://arxiv.org/abs/2506.22146)
*Amirmohammad Izadi,Mohammad Ali Banayeeanzade,Fatemeh Askari,Ali Rahimiakbar,Mohammad Mahdi Vahedi,Hosein Hasani,Mahdieh Soleymani Baghshah*

Main category: cs.CV

TL;DR: 本研究提出通过低级空间结构增强视觉输入并结合语言提示来改善视觉和语言模型在视觉推理任务中的性能，显著提升任务准确率。


<details>
  <summary>Details</summary>
Motivation: 当前视觉和语言模型在绑定感知特征与对应视觉对象时存在问题，导致在计数、视觉搜索等任务中表现不足。

Method: 为视觉输入添加低级空间结构（如水平线），并搭配引导顺序解析的文本提示进行模型优化。

Result: 方法在GPT-4o上实现视觉搜索准确率提升25%，计数准确率提升26.83%，场景描述编辑距离误差减少0.32，空间关系任务表现提升9.50%。

Conclusion: 提出的视觉输入低级结构增强可以有效提升视觉语言模型的组合式视觉推理能力，是一种改进视觉推理的重要策略。

Abstract: Despite progress in Vision-Language Models (VLMs), their capacity for visual
reasoning is often limited by the \textit{binding problem}: the failure to
reliably associate perceptual features with their correct visual referents.
This limitation underlies persistent errors in tasks such as counting, visual
search, scene description, and spatial relationship understanding. A key factor
is that current VLMs process visual features largely in parallel, lacking
mechanisms for spatially grounded, serial attention. This paper introduces a
simple yet effective intervention: augmenting visual inputs with low-level
spatial structures (e.g., horizontal lines) and pairing this with a textual
prompt that encourages sequential, spatially-aware parsing. We empirically
demonstrate substantial performance improvements across core visual reasoning
tasks. Specifically, our method improves GPT-4o visual search accuracy by
25.00%, increases counting accuracy by 26.83%, reduces edit distance error in
scene description by 0.32, and enhances performance on spatial relationship
tasks by 9.50% on a a 2D synthetic dataset. Furthermore, we find that the
visual modification is essential for these gains; purely textual strategies,
including Chain-of-Thought prompting, are insufficient and can even degrade
performance. Our method enhances binding only with a single-query inference,
underscoring the importance of visual input design over purely
linguistically-based approaches. These findings suggest that low-level visual
structuring is a powerful and underexplored direction for improving
compositional visual reasoning and could serve as a general strategy for
enhancing VLM performance on spatially grounded tasks.

</details>


### [64] [RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation Models](https://arxiv.org/abs/2506.22149)
*Ronald Fecso,José Morano,Ursula Schmidt-Erfurth,Hrvoje Bogunović*

Main category: cs.CV

TL;DR: RetFiner是一种基于自监督学习的视觉-语言精炼方法，用于改进现有基础模型（FMs）的表示能力并提升下游任务性能，尤其在复杂OCT分类任务中效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有的OCT基础模型仅基于图像数据训练，缺乏对图像的全面语义理解，特别是对于复杂任务表现较差，这限制了它们在特定应用和人群中的适用性。

Method: 提出RetFiner，一种结合多种训练目标的视觉-语言精炼方法，利用文本数据中的丰富监督信号来改善基础模型的表示能力，使其更高效地适应特定人群和任务。

Result: RetFiner显著提升了线性探测性能，在七种多样的OCT分类任务中，相较于RETFound, UrFound和VisionFM的基准线分别提升了5.8、3.9和2.1个百分点。

Conclusion: RetFiner有效解决了现有OCT基础模型在复杂任务中的性能局限，将其扩展为具有更广适应性的模型，推动了OCT分类任务的发展。

Abstract: The rise of imaging techniques such as optical coherence tomography (OCT) and
advances in deep learning (DL) have enabled clinicians and researchers to
streamline retinal disease staging. A popular DL approach is self-supervised
learning (SSL), where models learn from vast amounts of unlabeled data,
avoiding costly annotation. SSL has allowed the development of foundation
models (FMs), large models that can be used for a variety of downstream tasks.
However, existing FMs for OCT, trained solely on image data, lack a
comprehensive and robust semantic understanding of images, as evidenced by
their downstream performance (especially for complex tasks), and thus require
supervised fine-tuning (which may be unfeasible) to better adapt to specific
applications and populations. To address this, we propose RetFiner, an SSL
vision-language refinement scheme that improves the representations of existing
FMs and enables their efficient and direct adaptation to specific populations
for improved downstream performance. Our method uses a diverse set of training
objectives which take advantage of the rich supervisory signal found in textual
data. We tested RetFiner on the retinal FMs RETFound, UrFound, and VisionFM,
showing significant improvements in linear probing performance on seven highly
diverse OCT classification tasks, with an average increase of 5.8, 3.9, and 2.1
percentage points over their baselines, respectively. Our code and model
weights are publicly available at https://github.com/ronnief1/RetFiner.

</details>


### [65] [Attention-disentangled Uniform Orthogonal Feature Space Optimization for Few-shot Object Detection](https://arxiv.org/abs/2506.22161)
*Taijin Zhao,Heqian Qiu,Yu Dai,Lanxiao Wang,Fanman Meng,Qingbo Wu,Hongliang Li*

Main category: cs.CV

TL;DR: 论文提出了一个名为UOFS的优化框架，以解决现有少样本目标检测方法中，基于共享特征空间的局限性。通过分离目标性与分类特征，并引入混合背景优化及空间注意力模块等策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有少样本目标检测方法将目标性识别与前景分类绑定在共享特征空间内，导致目标性判定依赖于类别且易受类别样本不足影响，因此需要一个更有效的特征空间分离方案。

Method: 提出了UOFS框架，分离目标性和分类特征至正交空间，并通过Hybrid Background Optimization策略优化背景特征，以及通过SADA模块缓解类别无关与类别相关任务的冲突。

Result: 实验表明，该方法在性能上显著优于依赖于共享特征空间的现有方法，体现其卓越的效果。

Conclusion: UOFS成功地通过特征空间分离以及混合优化策略解决了FSOD中的若干关键问题，为少样本检测提供了一种新的高效方法。

Abstract: Few-shot object detection (FSOD) aims to detect objects with limited samples
for novel classes, while relying on abundant data for base classes. Existing
FSOD approaches, predominantly built on the Faster R-CNN detector, entangle
objectness recognition and foreground classification within shared feature
spaces. This paradigm inherently establishes class-specific objectness criteria
and suffers from unrepresentative novel class samples. To resolve this
limitation, we propose a Uniform Orthogonal Feature Space (UOFS) optimization
framework. First, UOFS decouples the feature space into two orthogonal
components, where magnitude encodes objectness and angle encodes
classification. This decoupling enables transferring class-agnostic objectness
knowledge from base classes to novel classes. Moreover, implementing the
disentanglement requires careful attention to two challenges: (1) Base set
images contain unlabeled foreground instances, causing confusion between
potential novel class instances and backgrounds. (2) Angular optimization
depends exclusively on base class foreground instances, inducing overfitting of
angular distributions to base classes. To address these challenges, we propose
a Hybrid Background Optimization (HBO) strategy: (1) Constructing a pure
background base set by removing unlabeled instances in original images to
provide unbiased magnitude-based objectness supervision. (2) Incorporating
unlabeled foreground instances in the original base set into angular
optimization to enhance distribution uniformity. Additionally, we propose a
Spatial-wise Attention Disentanglement and Association (SADA) module to address
task conflicts between class-agnostic and class-specific tasks. Experiments
demonstrate that our method significantly outperforms existing approaches based
on entangled feature spaces.

</details>


### [66] [Frequency-Semantic Enhanced Variational Autoencoder for Zero-Shot Skeleton-based Action Recognition](https://arxiv.org/abs/2506.22179)
*Wenhan Wu,Zhishuai Guo,Chen Chen,Hongfei Xue,Aidong Lu*

Main category: cs.CV

TL;DR: 提出了一种名为FS-VAE的方法，通过频率分解增强骨骼语义表示学习，从而改进零样本动作识别。


<details>
  <summary>Details</summary>
Motivation: 实现模型在未见类别动作上的识别能力，解决现有方法中对细粒度动作模式忽视的问题。

Method: 提出了包括频率增强模块、多层次对齐的语义动作描述与校准的跨对齐损失的FS-VAE，旨在丰富骨骼语义学习并改善骨骼与文本特征对齐鲁棒性。

Result: 实验验证表明，频率增强的语义特征可有效区分动作类别，提高零样本动作识别的准确性。

Conclusion: FS-VAE通过频率分解和对齐增强，有效改善了骨骼语义对齐，展现了在未见类别动作识别上的潜力。

Abstract: Zero-shot skeleton-based action recognition aims to develop models capable of
identifying actions beyond the categories encountered during training. Previous
approaches have primarily focused on aligning visual and semantic
representations but often overlooked the importance of fine-grained action
patterns in the semantic space (e.g., the hand movements in drinking water and
brushing teeth). To address these limitations, we propose a Frequency-Semantic
Enhanced Variational Autoencoder (FS-VAE) to explore the skeleton semantic
representation learning with frequency decomposition. FS-VAE consists of three
key components: 1) a frequency-based enhancement module with high- and
low-frequency adjustments to enrich the skeletal semantics learning and improve
the robustness of zero-shot action recognition; 2) a semantic-based action
description with multilevel alignment to capture both local details and global
correspondence, effectively bridging the semantic gap and compensating for the
inherent loss of information in skeleton sequences; 3) a calibrated
cross-alignment loss that enables valid skeleton-text pairs to counterbalance
ambiguous ones, mitigating discrepancies and ambiguities in skeleton and text
features, thereby ensuring robust alignment. Evaluations on the benchmarks
demonstrate the effectiveness of our approach, validating that
frequency-enhanced semantic features enable robust differentiation of visually
and semantically similar action clusters, improving zero-shot action
recognition.

</details>


### [67] [Robust and Accurate Multi-view 2D/3D Image Registration with Differentiable X-ray Rendering and Dual Cross-view Constraints](https://arxiv.org/abs/2506.22191)
*Yuxin Cui,Rui Song,Yibin Li,Max Q. -H. Meng,Zhe Min*

Main category: cs.CV

TL;DR: 提出了一种包括两个阶段的新型多视图2D/3D刚体配准方法，显著提升了注册过程的鲁棒性和精度。


<details>
  <summary>Details</summary>
Motivation: 现有单视角2D/3D注册受视野限制，难以应对术中场景，本研究旨在通过多视角方法提升配准性能和鲁棒性。

Method: 方法分为两个阶段：（1）加入多项损失（包括跨视角限制），结合预测与真实姿态及模拟与观察图像的不相似性；（2）测试时优化，进一步细化姿态估计。

Result: 在DeepFluoro数据集的6个样本中达到平均目标注册误差(mTRE)为$0.79\pm2.17$ mm，优于现有顶尖算法。

Conclusion: 引入跨视角约束和测试时优化，有效提升了配准精度和鲁棒性，适用于术中导航。

Abstract: Robust and accurate 2D/3D registration, which aligns preoperative models with
intraoperative images of the same anatomy, is crucial for successful
interventional navigation. To mitigate the challenge of a limited field of view
in single-image intraoperative scenarios, multi-view 2D/3D registration is
required by leveraging multiple intraoperative images. In this paper, we
propose a novel multi-view 2D/3D rigid registration approach comprising two
stages. In the first stage, a combined loss function is designed, incorporating
both the differences between predicted and ground-truth poses and the
dissimilarities (e.g., normalized cross-correlation) between simulated and
observed intraoperative images. More importantly, additional cross-view
training loss terms are introduced for both pose and image losses to explicitly
enforce cross-view constraints. In the second stage, test-time optimization is
performed to refine the estimated poses from the coarse stage. Our method
exploits the mutual constraints of multi-view projection poses to enhance the
robustness of the registration process. The proposed framework achieves a mean
target registration error (mTRE) of $0.79 \pm 2.17$ mm on six specimens from
the DeepFluoro dataset, demonstrating superior performance compared to
state-of-the-art registration algorithms.

</details>


### [68] [ReF-LLE: Personalized Low-Light Enhancement via Reference-Guided Deep Reinforcement Learning](https://arxiv.org/abs/2506.22216)
*Ming Zhao,Pingping Liu,Tongshun Zhang,Zhe Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为ReF-LLE的个性化低光图像增强方法，结合傅里叶频域和深度强化学习，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决低光图像在不同条件下的显著差异以及增强效果受主观偏好和用户意图影响的问题。

Method: 在傅里叶频域下结合深度强化学习引入零参考图像评价策略，在推理阶段根据傅里叶域零频分量实施个性化自适应迭代策略。

Result: ReF-LLE在基准数据集上的表现优于现有主流方法，具有更高的感知质量和个性化适应能力。

Conclusion: 该方法实现了个性化低光图像增强，并首次将深度强化学习与傅里叶域结合，取得了显著进展。

Abstract: Low-light image enhancement presents two primary challenges: 1) Significant
variations in low-light images across different conditions, and 2) Enhancement
levels influenced by subjective preferences and user intent. To address these
issues, we propose ReF-LLE, a novel personalized low-light image enhancement
method that operates in the Fourier frequency domain and incorporates deep
reinforcement learning. ReF-LLE is the first to integrate deep reinforcement
learning into this domain. During training, a zero-reference image evaluation
strategy is introduced to score enhanced images, providing reward signals that
guide the model to handle varying degrees of low-light conditions effectively.
In the inference phase, ReF-LLE employs a personalized adaptive iterative
strategy, guided by the zero-frequency component in the Fourier domain, which
represents the overall illumination level. This strategy enables the model to
adaptively adjust low-light images to align with the illumination distribution
of a user-provided reference image, ensuring personalized enhancement results.
Extensive experiments on benchmark datasets demonstrate that ReF-LLE
outperforms state-of-the-art methods, achieving superior perceptual quality and
adaptability in personalized low-light image enhancement.

</details>


### [69] [Boosting Classification with Quantum-Inspired Augmentations](https://arxiv.org/abs/2506.22241)
*Matthias Tschöpe,Vitor Fortes Rey,Sogo Pierre Sanon,Paul Lukowicz,Nikolaos Palaiodimopoulos,Maximilian Kiefer-Emmanouilidis*

Main category: cs.CV

TL;DR: 本文研究量子计算中量子门的微小扰动可能对机器学习有益，提出一种基于Bloch球旋转的量子灵感数据增强方法，并在ImageNet数据集上验证其效果。


<details>
  <summary>Details</summary>
Motivation: 探讨如何利用量子门微小扰动这一量子计算中独特特性来改进机器学习性能，特别是作为一种自然的数据增强方式。

Method: 通过随机Bloch球旋转（SU(2)变换）作为量子灵感的数据增强方法，并考察其在经典数据集上的表现。

Result: 在ImageNet数据集上，Top-1分类准确率提高3%，Top-5分类准确率提高2.5%，F$_1$分数提升8%-12%。不过，较强的幺正变换导致视觉难以辨识的图像，未能增强差分隐私。

Conclusion: 该研究证明量子灵感数据增强通过SU(2)变换能够有效提升图像分类性能，但未能促进隐私计算能力，彰显了量子计算与经典学习方法结合的潜力及其局限性。

Abstract: Understanding the impact of small quantum gate perturbations, which are
common in quantum digital devices but absent in classical computers, is crucial
for identifying potential advantages in quantum machine learning. While these
perturbations are typically seen as detrimental to quantum computation, they
can actually enhance performance by serving as a natural source of data
augmentation. Additionally, they can often be efficiently simulated on
classical hardware, enabling quantum-inspired approaches to improve classical
machine learning methods. In this paper, we investigate random Bloch sphere
rotations, which are fundamental SU(2) transformations, as a simple yet
effective quantum-inspired data augmentation technique. Unlike conventional
augmentations such as flipping, rotating, or cropping, quantum transformations
lack intuitive spatial interpretations, making their application to tasks like
image classification less straightforward. While common quantum augmentation
methods rely on applying quantum models or trainable quanvolutional layers to
classical datasets, we focus on the direct application of small-angle Bloch
rotations and their effect on classical data. Using the large-scale ImageNet
dataset, we demonstrate that our quantum-inspired augmentation method improves
image classification performance, increasing Top-1 accuracy by 3%, Top-5
accuracy by 2.5%, and the F$_1$ score from 8% to 12% compared to standard
classical augmentation methods. Finally, we examine the use of stronger unitary
augmentations. Although these transformations preserve information in
principle, they result in visually unrecognizable images with potential
applications for privacy computations. However, we show that our augmentation
approach and simple SU(2) transformations do not enhance differential privacy
and discuss the implications of this limitation.

</details>


### [70] [4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration](https://arxiv.org/abs/2506.22242)
*Jiahui Zhang,Yurui Chen,Yueming Xu,Ze Huang,Yanpeng Zhou,Yu-Jie Yuan,Xinyue Cai,Guowei Huang,Xingyue Quan,Hang Xu,Li Zhang*

Main category: cs.CV

TL;DR: 本文提出4D-VLA模型，通过整合4D信息（深度和时间）到视觉输入中，有效减少机器人数据预训练过程中的混乱，提高效率和性能。


<details>
  <summary>Details</summary>
Motivation: 当前的机器人数据预训练方法通常使用简单观察数据导致的动作分布不一致问题严重影响了效率，本文希望通过加入4D信息解决这一问题。

Method: 提出4D-VLA方法，将深度与时间信息融合到视觉特征输入，通过顺序RGB-D输入对齐机器人与场景的坐标系，同时引入记忆库采样策略，显著提升模型的时空推理能力和训练效率。

Result: 实验表明，本文方法大幅提升了模型性能，无论在模拟环境还是现实实验中，其成功率较OpenVLA显著提高。

Conclusion: 4D-VLA通过改进输入形式和采样策略，实现了更高效和鲁棒的机器人数据预训练，同时提出了新基准MV-Bench以验证模型在空间感知和跨视图泛化方面的优越性。

Abstract: Leveraging diverse robotic data for pretraining remains a critical challenge.
Existing methods typically model the dataset's action distribution using simple
observations as inputs. However, these inputs are often incomplete, resulting
in a dispersed conditional action distribution-an issue we refer to as
coordinate system chaos and state chaos. This inconsistency significantly
hampers pretraining efficiency. To address this, we propose 4D-VLA, a novel
approach that effectively integrates 4D information into the input to mitigate
these sources of chaos. Our model introduces depth and temporal information
into visual features with sequential RGB-D inputs, aligning the coordinate
systems of the robot and the scene. This alignment endows the model with strong
spatiotemporal reasoning capabilities while minimizing training overhead.
Additionally, we introduce memory bank sampling, a frame sampling strategy
designed to extract informative frames from historical images, further
improving effectiveness and efficiency. Experimental results demonstrate that
our pretraining method and architectural components substantially enhance model
performance. In both simulated and real-world experiments, our model achieves a
significant increase in success rate over OpenVLA. To further assess spatial
perception and generalization to novel views, we introduce MV-Bench, a
multi-view simulation benchmark. Our model consistently outperforms existing
methods, demonstrating stronger spatial understanding and adaptability.

</details>


### [71] [EAMamba: Efficient All-Around Vision State Space Model for Image Restoration](https://arxiv.org/abs/2506.22246)
*Yu-Cheng Lin,Yu-Syuan Xu,Hao-Wei Chen,Hsien-Kai Kuo,Chun-Yi Lee*

Main category: cs.CV

TL;DR: 论文提出了Efficient All-Around Mamba (EAMamba) 框架，通过引入多头选择扫描模块（MHSSM）和全方位扫描机制，旨在解决Vision Mamba在低层视觉任务中的计算复杂性和局部像素遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有的Vision Mamba在实现低层视觉图像恢复时表现优秀，但在计算复杂性和局部像素遗忘等方面存在局限，亟需改进。

Method: 提出Efficient All-Around Mamba (EAMamba) 框架，融入多头选择扫描模块（MHSSM）和全方位扫描机制，用以高效聚合扫描序列并捕获全局信息，同时优化计算效率和降低参数规模。

Result: EAMamba在超分辨率、去噪、去模糊和去雾等图像恢复任务中取得了优异的实验结果，相较传统方法实现了31%-89%的FLOPs减少，同时性能保持良好。

Conclusion: EAMamba显著优化了Vision Mamba在低层视觉任务上的表现，以更低的计算成本达成了高效而全面的图像恢复能力，具有广泛前景。

Abstract: Image restoration is a key task in low-level computer vision that aims to
reconstruct high-quality images from degraded inputs. The emergence of Vision
Mamba, which draws inspiration from the advanced state space model Mamba, marks
a significant advancement in this field. Vision Mamba demonstrates excellence
in modeling long-range dependencies with linear complexity, a crucial advantage
for image restoration tasks. Despite its strengths, Vision Mamba encounters
challenges in low-level vision tasks, including computational complexity that
scales with the number of scanning sequences and local pixel forgetting. To
address these limitations, this study introduces Efficient All-Around Mamba
(EAMamba), an enhanced framework that incorporates a Multi-Head Selective Scan
Module (MHSSM) with an all-around scanning mechanism. MHSSM efficiently
aggregates multiple scanning sequences, which avoids increases in computational
complexity and parameter count. The all-around scanning strategy implements
multiple patterns to capture holistic information and resolves the local pixel
forgetting issue. Our experimental evaluations validate these innovations
across several restoration tasks, including super resolution, denoising,
deblurring, and dehazing. The results validate that EAMamba achieves a
significant 31-89% reduction in FLOPs while maintaining favorable performance
compared to existing low-level Vision Mamba methods.

</details>


### [72] [COOCO -- Common Objects Out-of-Context -- Semantic Violation in Scenes: Investigating Multimodal Context in Referential Communication](https://arxiv.org/abs/2506.22274)
*Filippo Merlo,Ece Takmaz,Wenkai Chen,Albert Gatt*

Main category: cs.CV

TL;DR: 研究分析了视觉语言模型是否会像人类一样借助场景语境生成物体的引用，并提出了COOCO数据集来评估其表现。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型是否会利用场景语境信息生成物体引用，以及其对不同场景一致性程度和噪声情况下的表现。

Method: 提出COOCO数据集，在不同目标场景一致性、语义相关性和噪声条件下测试VLMs对场景语境的依赖，并通过注意力分析考察模型信息利用方式。

Result: 发现模型会根据物体与场景的语义相关性及噪声水平，灵活地利用场景语境，尤其是在高一致性或物体被降质的情况下更依赖语境。同时，注意力分析显示在中等噪声下模型更关注中层目标特征。

Conclusion: 视觉语言模型能够动态平衡局部及语境信息生成引用，其性能受场景-对象一致性及噪声影响。

Abstract: Natural scenes provide us with rich contexts for object recognition and
reference. In particular, knowing what type of scene one is looking at
generates expectations about which objects will occur, and what their spatial
configuration should be. Do Vision-Language Models (VLMs) learn to rely on
scene contexts in a similar way, when generating references to objects? To
address this question, we introduce the \textit{Common Objects Out-of-Context
(COOCO)} dataset and test to what extent VLMs rely on scene context to refer to
objects under different degrees of scene-object congruency, and different
perturbations. Our findings show that models leverage scene context adaptively,
depending on both the semantic relatedness between object and scene and the
level of noise. In particular, models rely more on context under high
target-scene congruence or when objects are degraded. Attention analysis
reveals that successful object categorisation involves increased focus on the
target in mid-level layers, especially under moderate noise, suggesting that
VLMs dynamically balance local and contextual information for reference
generation. We make our dataset, code and models available at
\href{https://github.com/cs-nlp-uu/scenereg}{https://github.com/cs-nlp-uu/scenereg}.

</details>


### [73] [Rethinking Visual Token Reduction in LVLMs under Cross-modal Misalignment](https://arxiv.org/abs/2506.22283)
*Rui Xu,Yunke Wang,Yong Luo,Bo Du*

Main category: cs.CV

TL;DR: 研究提出了一种名为VisionDrop的方法，通过视觉注意力筛选和逐步精简视觉标记，减少视觉语言模型的计算成本，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言模型中视觉标记的数量远超过文本标记，造成严重的计算开销，限制了模型的实用性与扩展性。

Method: 提出VisionDrop，一个无须额外训练的视觉标记精简框架，基于视觉内部注意力进行筛选，并通过逐步精简的方式减少冗余，同时维持丰富的细粒度信息。

Result: VisionDrop在多个基准测试中表现出一致的性能提升，无需额外训练或复杂修改仍保持高效推理和任务性能。

Conclusion: VisionDrop展示了一种简单有效的视觉标记精简方法，在减少计算成本的同时，保持了任务表现，并证明了视觉自适应精简的潜力。

Abstract: Large Vision-Language Models (LVLMs) encode visual inputs as dense sequences
of patch-level tokens to capture fine-grained semantics. These visual tokens
often outnumber their textual counterparts by a large margin, leading to
substantial computational overhead and limiting the scalability of LVLMs in
practice. Previous efforts have explored visual token reduction either prior to
or within the large language models (LLM). However, most in-LLM reduction
approaches rely on text-conditioned interactions, implicitly assuming that
textual tokens can reliably capture the importance of visual tokens. In this
work, we revisit this assumption and reveal causal, semantic, and spatial forms
of cross-modal misalignment. These misalignments undermine the effectiveness of
text-guided visual token reduction. To address this, we introduce VisionDrop, a
training-free, visual-only pruning framework that selects informative visual
tokens based on intra-modal (visual-to-visual) attention, without relying on
textual signals. To further suppress redundancy throughout the model hierarchy,
we treat the visual encoder and the LLM as a unified system and design a
progressive pruning pipeline. Our method performs dominant token selection and
lightweight contextual merging at multiple stages, enabling fine-grained visual
information to be retained even under aggressive token budgets. Extensive
experiments across diverse benchmarks show that VisionDrop achieves consistent
improvements over existing methods, despite requiring no additional training or
complex modifications. Its simple yet effective design enables efficient
inference while preserving strong performance across tasks.

</details>


### [74] [RoomCraft: Controllable and Complete 3D Indoor Scene Generation](https://arxiv.org/abs/2506.22291)
*Mengqi Zhou,Xipeng Wang,Yuxi Wang,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: RoomCraft通过解析用户输入并优化生成3D室内场景，实现布局的几何一致性、空间合理性和视觉真实感。


<details>
  <summary>Details</summary>
Motivation: 目前的3D室内场景生成方法在处理几何一致性、空间关系和视觉真实性方面存在缺陷。神经网络生成方法缺乏全局空间推理，易出现重复元素；而程序性生成方法在多约束情况下易导致物体碰撞和布局不完整。

Method: RoomCraft是一种多阶段管道方法，从用户输入中提取高层次场景信息，通过约束优化生成一致的3D室内场景。其采用空间关系网络和启发式深度优先搜索算法（HDFS）生成优化的物体摆放顺序，并引入统一约束表示及冲突感知定位策略（CAPS），确保布局完整性和真实感。

Result: 大量实验表明，RoomCraft在生成现实感强、语义一致、视觉吸引力高的房间布局方面显著优于现有方法。

Conclusion: RoomCraft通过结合场景生成和约束优化方法，实现了对多种输入形式的支持，并显著提升了3D室内场景生成的质量和灵活性。

Abstract: Generating realistic 3D indoor scenes from user inputs remains a challenging
problem in computer vision and graphics, requiring careful balance of geometric
consistency, spatial relationships, and visual realism. While neural generation
methods often produce repetitive elements due to limited global spatial
reasoning, procedural approaches can leverage constraints for controllable
generation but struggle with multi-constraint scenarios. When constraints
become numerous, object collisions frequently occur, forcing the removal of
furniture items and compromising layout completeness.
  To address these limitations, we propose RoomCraft, a multi-stage pipeline
that converts real images, sketches, or text descriptions into coherent 3D
indoor scenes. Our approach combines a scene generation pipeline with a
constraint-driven optimization framework. The pipeline first extracts
high-level scene information from user inputs and organizes it into a
structured format containing room type, furniture items, and spatial relations.
It then constructs a spatial relationship network to represent furniture
arrangements and generates an optimized placement sequence using a
heuristic-based depth-first search (HDFS) algorithm to ensure layout coherence.
To handle complex multi-constraint scenarios, we introduce a unified constraint
representation that processes both formal specifications and natural language
inputs, enabling flexible constraint-oriented adjustments through a
comprehensive action space design. Additionally, we propose a Conflict-Aware
Positioning Strategy (CAPS) that dynamically adjusts placement weights to
minimize furniture collisions and ensure layout completeness.
  Extensive experiments demonstrate that RoomCraft significantly outperforms
existing methods in generating realistic, semantically coherent, and visually
appealing room layouts across diverse input modalities.

</details>


### [75] [OutDreamer: Video Outpainting with a Diffusion Transformer](https://arxiv.org/abs/2506.22298)
*Linhao Zhong,Fan Li,Yi Huang,Jianzhuang Liu,Renjing Pei,Fenglong Song*

Main category: cs.CV

TL;DR: 提出了一种名为OutDreamer的基于DiT的视频外延生成框架，包括视频控制分支和条件外延生成分支，性能优于现有零样本方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法在生成的质量和适应性上仍存在不足，提出使用DiT以提升性能。

Method: 引入一个包含视频控制分支和条件外延分支的DiT框架，结合掩膜自注意力层和潜在对齐损失，并使用跨视频剪辑优化器提高长视频生成一致性。

Result: 在公认的基准测试中，OutDreamer在零样本情况下优于现有方法。

Conclusion: 通过采用DiT和改进的生成策略，OutDreamer实现了高质量、适应性强且一致性好的视频外延生成。

Abstract: Video outpainting is a challenging task that generates new video content by
extending beyond the boundaries of an original input video, requiring both
temporal and spatial consistency. Many state-of-the-art methods utilize latent
diffusion models with U-Net backbones but still struggle to achieve high
quality and adaptability in generated content. Diffusion transformers (DiTs)
have emerged as a promising alternative because of their superior performance.
We introduce OutDreamer, a DiT-based video outpainting framework comprising two
main components: an efficient video control branch and a conditional
outpainting branch. The efficient video control branch effectively extracts
masked video information, while the conditional outpainting branch generates
missing content based on these extracted conditions. Additionally, we propose a
mask-driven self-attention layer that dynamically integrates the given mask
information, further enhancing the model's adaptability to outpainting tasks.
Furthermore, we introduce a latent alignment loss to maintain overall
consistency both within and between frames. For long video outpainting, we
employ a cross-video-clip refiner to iteratively generate missing content,
ensuring temporal consistency across video clips. Extensive evaluations
demonstrate that our zero-shot OutDreamer outperforms state-of-the-art
zero-shot methods on widely recognized benchmarks.

</details>


### [76] [MatChA: Cross-Algorithm Matching with Feature Augmentation](https://arxiv.org/abs/2506.22336)
*Paula Carbó Cubero,Alberto Jaenal Gálvez,André Mateus,José Araújo,Patric Jensfelt*

Main category: cs.CV

TL;DR: 本文提出一种方法，解决了不同设备使用不同稀疏特征提取算法时，视觉定位中的跨检测器特征匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在设备间使用不同特征提取算法时表现较差，主要因为假定使用了相同的检测器，而在实践中很难满足。

Method: 提出一种特征描述符增强和翻译至潜在空间的方法，解决跨检测器特征匹配问题。

Result: 实验表明，该方法显著提升了图像匹配及跨特征定位性能。

Conclusion: 该方法有效解决了跨特征检测器匹配的挑战，为视觉定位任务提供了新的解决方案。

Abstract: State-of-the-art methods fail to solve visual localization in scenarios where
different devices use different sparse feature extraction algorithms to obtain
keypoints and their corresponding descriptors. Translating feature descriptors
is enough to enable matching. However, performance is drastically reduced in
cross-feature detector cases, because current solutions assume common
keypoints. This means that the same detector has to be used, which is rarely
the case in practice when different descriptors are used. The low repeatability
of keypoints, in addition to non-discriminatory and non-distinctive
descriptors, make the identification of true correspondences extremely
challenging. We present the first method tackling this problem, which performs
feature descriptor augmentation targeting cross-detector feature matching, and
then feature translation to a latent space. We show that our method
significantly improves image matching and visual localization in the
cross-feature scenario and evaluate the proposed method on several benchmarks.

</details>


### [77] [A Deep Learning framework for building damage assessment using VHR SAR and geospatial data: demonstration on the 2023 Turkiye Earthquake](https://arxiv.org/abs/2506.22338)
*Luigi Russo,Deodato Tapete,Silvia Liberata Ullo,Paolo Gamba*

Main category: cs.CV

TL;DR: 提出了一种利用单日期SAR图像和辅助地理空间数据的多模态深度学习框架来检测建筑物损坏。


<details>
  <summary>Details</summary>
Motivation: 解决光学卫星影像受云层遮挡或事件前影像缺失的局限性，提高灾后建筑物损坏检测的效率。

Method: 利用高分辨率SAR图像、开放街图建筑轮廓、数字表面模型、地震模型中的结构属性等多种地理空间数据，设计深度学习模型，仅通过灾后数据进行建筑物损坏检测。

Result: 使用2023年土耳其地震的多城市数据集验证，结果表明引入地理空间特征显著提升检测性能和在新区域的泛化能力。

Conclusion: 该方法无需依赖事件前数据，可提供快速可靠的建筑损坏评估，适用于灾害管理和恢复，具备广泛的应用潜力。

Abstract: Building damage identification shortly after a disaster is crucial for
guiding emergency response and recovery efforts. Although optical satellite
imagery is commonly used for disaster mapping, its effectiveness is often
hampered by cloud cover or the absence of pre-event acquisitions. To overcome
these challenges, we introduce a novel multimodal deep learning (DL) framework
for detecting building damage using single-date very high resolution (VHR)
Synthetic Aperture Radar (SAR) imagery from the Italian Space Agency (ASI)
COSMO SkyMed (CSK) constellation, complemented by auxiliary geospatial data.
Our method integrates SAR image patches, OpenStreetMap (OSM) building
footprints, digital surface model (DSM) data, and structural and exposure
attributes from the Global Earthquake Model (GEM) to improve detection accuracy
and contextual interpretation. Unlike existing approaches that depend on pre
and post event imagery, our model utilizes only post event data, facilitating
rapid deployment in critical scenarios. The framework effectiveness is
demonstrated using a new dataset from the 2023 earthquake in Turkey, covering
multiple cities with diverse urban settings. Results highlight that
incorporating geospatial features significantly enhances detection performance
and generalizability to previously unseen areas. By combining SAR imagery with
detailed vulnerability and exposure information, our approach provides reliable
and rapid building damage assessments without the dependency from available
pre-event data. Moreover, the automated and scalable data generation process
ensures the framework's applicability across diverse disaster-affected regions,
underscoring its potential to support effective disaster management and
recovery efforts. Code and data will be made available upon acceptance of the
paper.

</details>


### [78] [Closing the Performance Gap in Biometric Cryptosystems: A Deeper Analysis on Unlinkable Fuzzy Vaults](https://arxiv.org/abs/2506.22347)
*Hans Geißner,Christian Rathgeb*

Main category: cs.CV

TL;DR: 本文提出了一种基于等频率区间的特征量化方法以解决模糊保险库中的性能问题，有效减轻了由模板保护引起的性能下降。


<details>
  <summary>Details</summary>
Motivation: 识别出由于可变特征集大小及其对相似性阈值的影响导致的性能下降问题，并分析导致信息丢失的特征型转化问题。

Method: 提出了一种基于等频率区间的特征量化方法，该方法确保固定的特征集大小，并能适应不同的区间数量而无需训练。

Result: 实验在最新的面部、指纹和虹膜识别系统中表现出，性能下降显著减少，证明在主要生物特征模式中的方法有效性。

Conclusion: 方法与现有系统无缝整合，性能问题得到显著改进，仅存在最小的性能下降。

Abstract: This paper analyses and addresses the performance gap in the fuzzy
vault-based \ac{BCS}. We identify unstable error correction capabilities, which
are caused by variable feature set sizes and their influence on similarity
thresholds, as a key source of performance degradation. This issue is further
compounded by information loss introduced through feature type transformations.
To address both problems, we propose a novel feature quantization method based
on \it{equal frequent intervals}. This method guarantees fixed feature set
sizes and supports training-free adaptation to any number of intervals. The
proposed approach significantly reduces the performance gap introduced by
template protection. Additionally, it integrates seamlessly with existing
systems to minimize the negative effects of feature transformation. Experiments
on state-of-the-art face, fingerprint, and iris recognition systems confirm
that only minimal performance degradation remains, demonstrating the
effectiveness of the method across major biometric modalities.

</details>


### [79] [From Ground to Air: Noise Robustness in Vision Transformers and CNNs for Event-Based Vehicle Classification with Potential UAV Applications](https://arxiv.org/abs/2506.22360)
*Nouf Almesafri,Hector Figueiredo,Miguel Arana-Catania*

Main category: cs.CV

TL;DR: 研究对比了两种深度学习架构（ResNet34和ViT B16）在事件相机上的性能，发现ResNet34在准确率上稍占优势，但ViT B16表现出更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在动态环境（如无人机和自动驾驶车辆）中，事件相机因其捕捉场景变化的能力引起了关注，但针对其的深度学习模型性能尚不明确。

Method: 将ResNet34和ViT B16在GEN1事件数据集上进行微调，并在标准和模拟噪声条件下进行性能评估与对比。

Result: ResNet34在干净数据集上的分类准确率为88%，ViT B16为86%。虽然ResNet34的准确率更高，但ViT B16在面对较小数据集预训练时展现了强大鲁棒性。

Conclusion: 研究结果表明，研究方法具有适应无人机及航空相关任务（如空中目标分类与事件视觉系统）的潜力。

Abstract: This study investigates the performance of the two most relevant computer
vision deep learning architectures, Convolutional Neural Network and Vision
Transformer, for event-based cameras. These cameras capture scene changes,
unlike traditional frame-based cameras with capture static images, and are
particularly suited for dynamic environments such as UAVs and autonomous
vehicles. The deep learning models studied in this work are ResNet34 and ViT
B16, fine-tuned on the GEN1 event-based dataset. The research evaluates and
compares these models under both standard conditions and in the presence of
simulated noise. Initial evaluations on the clean GEN1 dataset reveal that
ResNet34 and ViT B16 achieve accuracies of 88% and 86%, respectively, with
ResNet34 showing a slight advantage in classification accuracy. However, the
ViT B16 model demonstrates notable robustness, particularly given its
pre-training on a smaller dataset. Although this study focuses on ground-based
vehicle classification, the methodologies and findings hold significant promise
for adaptation to UAV contexts, including aerial object classification and
event-based vision systems for aviation-related tasks.

</details>


### [80] [Exploiting Vision Language Model for Training-Free 3D Point Cloud OOD Detection via Graph Score Propagation](https://arxiv.org/abs/2506.22375)
*Tiankai Chen,Yushu Li,Adam Goodge,Fei Teng,Xulei Yang,Tianrui Li,Xun Xu*

Main category: cs.CV

TL;DR: 提出了一种基于图评分传播（GSP）的无训练框架，利用视觉-语言模型（VLM）进行3D点云数据的OOD检测，并在性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决3D点云数据中OOD检测的挑战，提升其在安全和可靠感知应用中的表现。

Method: 通过构建基于类原型和测试数据的图，结合提示聚类和自训练负提示，提出图评分传播（GSP）方法以提升VLM在OOD检测中的表现，并支持小样本情景。

Result: GSP方法在合成及真实世界数据集的3D点云OOD检测任务中均超越了现有的最先进方法。

Conclusion: 提出的方法在OOD检测中表现优异，为3D点云分类及检测提供了新的技术路线，同时具有实际应用潜力。

Abstract: Out-of-distribution (OOD) detection in 3D point cloud data remains a
challenge, particularly in applications where safe and robust perception is
critical. While existing OOD detection methods have shown progress for 2D image
data, extending these to 3D environments involves unique obstacles. This paper
introduces a training-free framework that leverages Vision-Language Models
(VLMs) for effective OOD detection in 3D point clouds. By constructing a graph
based on class prototypes and testing data, we exploit the data manifold
structure to enhancing the effectiveness of VLMs for 3D OOD detection. We
propose a novel Graph Score Propagation (GSP) method that incorporates prompt
clustering and self-training negative prompting to improve OOD scoring with
VLM. Our method is also adaptable to few-shot scenarios, providing options for
practical applications. We demonstrate that GSP consistently outperforms
state-of-the-art methods across synthetic and real-world datasets 3D point
cloud OOD detection.

</details>


### [81] [Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment](https://arxiv.org/abs/2506.22385)
*Yue Zhang,Jilei Sun,Yunhui Guo,Vibhav Gogate*

Main category: cs.CV

TL;DR: 本文提出Defeasible Video Entailment (DVidE)任务，要求模型根据动态证据更新推理，并展示如何通过创新架构提升VLMMs的动态推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视频大规模多模态模型（VLMMs）在抽象和自适应推理方面存在局限，难以随新的信息动态更新结论。

Method: 提出DVidE任务并设计两种框架：分类任务中利用反事实推理、ASR增强视频内容和推理修正；生成任务中结合ASR输出与大语言模型（LLM）生成与上下文相关的更新。此外，引入新标注数据集及专用评估指标。

Result: 实验结果表明，本文方法显著提升了VLMMs在动态推理方面的性能。

Conclusion: DVidE任务和方法有效提升了VLMMs的动态推理能力，扩展了视频推理领域的新方向。

Abstract: Video Large Multimodal Models (VLMMs) have made impressive strides in
understanding video content, but they often struggle with abstract and adaptive
reasoning-the ability to revise their interpretations when new information
emerges. In reality, conclusions are rarely set in stone; additional context
can strengthen or weaken an initial inference. To address this, we introduce
Defeasible Video Entailment (DVidE), a new task that challenges models to think
like doubters, constantly updating their reasoning based on evolving evidence.
In DVidE, given a video premise and a textual hypothesis, models must determine
whether a new update strengthens or weakens the hypothesis (classification
version) or generate a coherent update that modifies the entailment
relationship (generation version). For solving the classification task, we
propose the Chain of Counterfactual Thought framework, utilizing counterfactual
reasoning, ASR-enhanced video content, and rationale refinement to reduce
inference bias. For the generation task, we develop a framework that combines
ASR output with a Large Language Model (LLM) to produce coherent, contextually
relevant updates aligned with the intended strengthener or weakener goals.
Additionally, we introduce a novel benchmark dataset, with
strengthener/weakener annotations and an LLM-based evaluation metric
specifically designed for assessing generative performance. Experimental
results demonstrate significant improvements, highlighting our proposed method
in enhancing dynamic reasoning capabilities of VLMMs.

</details>


### [82] [Test-Time Consistency in Vision Language Models](https://arxiv.org/abs/2506.22395)
*Shih-Han Chou,Shivam Chandhok,James J. Little,Leonid Sigal*

Main category: cs.CV

TL;DR: 提出了一种无需监督重新训练的简单有效的测试时间一致性框架，提高视觉语言模型在语义等价输入上的语义一致性表现。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在多模态任务中表现出色，但面对语义等价输入时，模型存在不一致预测的问题，削弱了其可靠性和鲁棒性。

Method: 提出了一个后处理中立的方法，通过交叉熵一致性损失和伪标签一致性损失，从单一测试点获取信息增强模型预测的一致性，且无需模型架构更改或大规模微调。

Result: 在MM-R3基准测试中，提出的方法显著提升了模型的一致性表现，优于当前的先进方法。

Conclusion: 本方法为多模态学习中推理时的适应性提供了新的方向，有效提高了语义一致性而无需后续训练。

Abstract: Vision-Language Models (VLMs) have achieved impressive performance across a
wide range of multimodal tasks, yet they often exhibit inconsistent behavior
when faced with semantically equivalent inputs, undermining their reliability
and robustness. Recent benchmarks, such as MM-R3, highlight that even
state-of-the-art VLMs can produce divergent predictions across semantically
equivalent inputs, despite maintaining high average accuracy. Prior work
addresses this issue by modifying model architectures or conducting large-scale
fine-tuning on curated datasets. In contrast, we propose a simple and effective
test-time consistency framework that enhances semantic consistency without
supervised re-training. Our method is entirely post-hoc, model-agnostic, and
applicable to any VLM with access to its weights. Given a single test point, we
enforce consistent predictions via two complementary objectives: (i) a
Cross-Entropy Agreement Loss that aligns predictive distributions across
semantically equivalent inputs, and (ii) a Pseudo-Label Consistency Loss that
draws outputs toward a self-averaged consensus. Our method is plug-and-play and
leverages information from a single test input itself to improve consistency.
Experiments on the MM-R3 benchmark show that our framework yields substantial
gains in consistency across state-of-the-art models, establishing a new
direction for inference-time adaptation in multimodal learning.

</details>


### [83] [Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy](https://arxiv.org/abs/2506.22432)
*Yuhao Liu,Tengfei Wang,Fang Liu,Zhenwei Wang,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: 提出了一种新的视频编辑框架Shape-for-Motion，通过3D代理实现精确且一致的编辑，支持多种物理一致性的操作和视频合成。


<details>
  <summary>Details</summary>
Motivation: 用户对创意编辑工具的需求越来越高，现有方法难以实现精细地对齐用户意图。

Method: 引入一个时间一致的3D网格作为代理，设计双重传播策略以简化编辑过程，并结合视频扩散模型进行编辑结果的生成。

Result: 实现了对视频中目标对象的精确、多样化编辑，包括姿态、旋转、缩放等多种操作，技术优于现有方法。

Conclusion: 提高了视频编辑的可控性和高质量生成水平，为相关视频编辑工作流程提供了重要改进。

Abstract: Recent advances in deep generative modeling have unlocked unprecedented
opportunities for video synthesis. In real-world applications, however, users
often seek tools to faithfully realize their creative editing intentions with
precise and consistent control. Despite the progress achieved by existing
methods, ensuring fine-grained alignment with user intentions remains an open
and challenging problem. In this work, we present Shape-for-Motion, a novel
framework that incorporates a 3D proxy for precise and consistent video
editing. Shape-for-Motion achieves this by converting the target object in the
input video to a time-consistent mesh, i.e., a 3D proxy, allowing edits to be
performed directly on the proxy and then inferred back to the video frames. To
simplify the editing process, we design a novel Dual-Propagation Strategy that
allows users to perform edits on the 3D mesh of a single frame, and the edits
are then automatically propagated to the 3D meshes of the other frames. The 3D
meshes for different frames are further projected onto the 2D space to produce
the edited geometry and texture renderings, which serve as inputs to a
decoupled video diffusion model for generating edited results. Our framework
supports various precise and physically-consistent manipulations across the
video frames, including pose editing, rotation, scaling, translation, texture
modification, and object composition. Our approach marks a key step toward
high-quality, controllable video editing workflows. Extensive experiments
demonstrate the superiority and effectiveness of our approach. Project page:
https://shapeformotion.github.io/

</details>


### [84] [WarpRF: Multi-View Consistency for Training-Free Uncertainty Quantification and Applications in Radiance Fields](https://arxiv.org/abs/2506.22433)
*Sadra Safadoust,Fabio Tosi,Fatma Güney,Matteo Poggi*

Main category: cs.CV

TL;DR: WarpRF用于训练自由的辐射场不确定性量化框架，通过逆向变形实现视点一致性评估，无需训练即可应用于任何辐射场实现。


<details>
  <summary>Details</summary>
Motivation: 通过量化辐射场的不确定性来实现更精准的结果预测，填补对未知视角不确定性度量的需求。

Method: WarpRF基于光度和几何一致性原理，采用多视角的逆向变形方式，从可靠视角投影至未知视角并评估一致性。

Result: WarpRF在不确定性量化、主动视点选择和主动映射任务中表现优异，优于现有基于特定框架的方法。

Conclusion: WarpRF是一种简单、经济、无需训练的通用框架，能够免费应用于任何辐射场实现并提升核心任务表现。

Abstract: We introduce WarpRF, a training-free general-purpose framework for
quantifying the uncertainty of radiance fields. Built upon the assumption that
photometric and geometric consistency should hold among images rendered by an
accurate model, WarpRF quantifies its underlying uncertainty from an unseen
point of view by leveraging backward warping across viewpoints, projecting
reliable renderings to the unseen viewpoint and measuring the consistency with
images rendered there. WarpRF is simple and inexpensive, does not require any
training, and can be applied to any radiance field implementation for free.
WarpRF excels at both uncertainty quantification and downstream tasks, e.g.,
active view selection and active mapping, outperforming any existing method
tailored to specific frameworks.

</details>


### [85] [MiCo: Multi-image Contrast for Reinforcement Visual Reasoning](https://arxiv.org/abs/2506.22434)
*Xi Chen,Mingkang Zhu,Shaoteng Liu,Xiaoyang Wu,Xiaogang Xu,Yu Liu,Xiang Bai,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种利用自监督学习改进视觉-语言模型（VLMs）中多图像联结推理的方法，不依赖人工标注数据，而是通过细粒度视觉比较培养模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的强化学习方法在进行多图像推理时，依赖人工标注的问答对，而这些对的获取对于细粒度视觉细节及复杂逻辑关系需求较高，难度较大。

Method: 通过生成图像三元组（包括两张相同图像的不同增强视图和一张相似但不同的图像），模型在训练过程中生成推理过程来比较图像。然后，通过基于规则的强化学习优化模型，使模型注意到微小的视觉变化并执行逻辑推理。

Result: 实验表明，尽管模型仅基于视觉比较任务进行训练，其学习到的推理能力在多种问题中都表现出良好的泛化性。在多图像推理基准和通用视觉任务上不依赖人工标注数据的情况下表现优越。

Conclusion: 利用自监督视觉表示学习，提出一种无需人工标注数据、通过视觉比较改进多图像推理能力的新方法，显著提升了基准测试中的推理能力和泛化性能。

Abstract: This work explores enabling Chain-of-Thought (CoT) reasoning to link visual
cues across multiple images. A straightforward solution is to adapt rule-based
reinforcement learning for Vision-Language Models (VLMs). However, such methods
typically rely on manually curated question-answer pairs, which can be
particularly challenging when dealing with fine grained visual details and
complex logic across images. Inspired by self-supervised visual representation
learning, we observe that images contain inherent constraints that can serve as
supervision. Based on this insight, we construct image triplets comprising two
augmented views of the same image and a third, similar but distinct image.
During training, the model is prompted to generate a reasoning process to
compare these images (i.e., determine same or different). Then we optimize the
model with rule-based reinforcement learning. Due to the high visual similarity
and the presence of augmentations, the model must attend to subtle visual
changes and perform logical reasoning to succeed. Experiments show that,
although trained solely on visual comparison tasks, the learned reasoning
ability generalizes effectively to a wide range of questions. Without relying
on any human-annotated question-answer pairs, our method achieves significant
improvements on multi-image reasoning benchmarks and shows strong performance
on general vision tasks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [86] [Efficient Multilingual ASR Finetuning via LoRA Language Experts](https://arxiv.org/abs/2506.21555)
*Jiahong Li,Yiwen Shao,Jianheng Zhuo,Chenda Li,Liliang Tang,Dong Yu,Yanmin Qian*

Main category: cs.CL

TL;DR: 本文探讨通过LoRA语言专家提升Whisper模型在多语言语音识别上的性能，提出了一种高效的微调框架，获得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 多语言语音识别中不同语言之间的干扰影响模型性能，需要找到更有效的模型微调方法。

Method: 提出了一种基于Whisper模型的LoRA语言专家微调框架，通过专家结合（Expert Fusion）或知识蒸馏（Knowledge Distillation）提升模型性能。

Result: 实验表明，所提出框架在语言感知及非感知场景下，性能分别提升了约10%和15%。

Conclusion: 通过高效微调框架，实现在维持模型共享能力的同时，提升了多语言语音识别的效果。

Abstract: Recent advancements in deep learning have significantly enhanced multilingual
automatic speech recognition (ASR) due to the development of advanced model
architectures and available large-scale multilingual datasets. Despite that,
multilingual ASR still suffers from the curse of multilinguality in that
different languages tend to interfere with each other, making it difficult for
the ASR model to identify multiple languages effectively while sharing model
capacity across them. This paper proposes an efficient finetuning framework for
customized multilingual ASR via prepared LoRA language experts based on
Whisper. Through LoRA expert fusion or knowledge distillation, our approach
achieves better recognition performance on target languages than standard
fine-tuning methods. Experimental results demonstrate that the proposed models
yield approximately 10\% and 15\% relative performance gains in language-aware
and language-agnostic scenarios, respectively.

</details>


### [87] [VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation](https://arxiv.org/abs/2506.21556)
*Hyeongcheol Park,MinHyuk Jang,Ha Dam Baek,Gyusam Chang,Jiyoung Seo,Jiwan Park,Hogun Park,Sangpil Kim*

Main category: cs.CL

TL;DR: 本文提出了视觉-音频-文本知识图谱（VAT-KG），这是首个包含视觉、音频和文本信息的多模态知识图谱，能够自动从任何多模态数据集中生成，为多模态任务提供丰富的概念级知识支持。


<details>
  <summary>Details</summary>
Motivation: 目前的多模态知识图谱范围有限，知识覆盖过时或不完整，支持的模态也较为狭窄，难以适应视频和音频等更丰富模态的需求。因此需要设计一个更具扩展性和适用性的多模态知识图谱。

Method: 提出VAT-KG，将视觉、音频与文本信息整合，采用严格的过滤与对齐步骤保证跨模态知识对齐，并通过自动化管道从多模态数据集中生成知识图谱。同时提出能从任意模态检索概念级知识的新型多模态RAG框架。

Result: 在不同模态上的问答任务实验表明，VAT-KG能有效支持多模态大语言模型（MLLMs），体现出其在统一多模态知识与提升推理能力上的实际价值。

Conclusion: VAT-KG扩展了多模态知识图谱的模态范围和知识覆盖，展示了其在多模态任务中的巨大潜力与重要作用。

Abstract: Multimodal Knowledge Graphs (MMKGs), which represent explicit knowledge
across multiple modalities, play a pivotal role by complementing the implicit
knowledge of Multimodal Large Language Models (MLLMs) and enabling more
grounded reasoning via Retrieval Augmented Generation (RAG). However, existing
MMKGs are generally limited in scope: they are often constructed by augmenting
pre-existing knowledge graphs, which restricts their knowledge, resulting in
outdated or incomplete knowledge coverage, and they often support only a narrow
range of modalities, such as text and visual information. These limitations
reduce their extensibility and applicability to a broad range of multimodal
tasks, particularly as the field shifts toward richer modalities such as video
and audio in recent MLLMs. Therefore, we propose the Visual-Audio-Text
Knowledge Graph (VAT-KG), the first concept-centric and knowledge-intensive
multimodal knowledge graph that covers visual, audio, and text information,
where each triplet is linked to multimodal data and enriched with detailed
descriptions of concepts. Specifically, our construction pipeline ensures
cross-modal knowledge alignment between multimodal data and fine-grained
semantics through a series of stringent filtering and alignment steps, enabling
the automatic generation of MMKGs from any multimodal dataset. We further
introduce a novel multimodal RAG framework that retrieves detailed
concept-level knowledge in response to queries from arbitrary modalities.
Experiments on question answering tasks across various modalities demonstrate
the effectiveness of VAT-KG in supporting MLLMs, highlighting its practical
value in unifying and leveraging multimodal knowledge.

</details>


### [88] [Debunk and Infer: Multimodal Fake News Detection via Diffusion-Generated Evidence and LLM Reasoning](https://arxiv.org/abs/2506.21557)
*Kaiying Yan,Moyang Liu,Yukun Liu,Ruibo Fu,Zhengqi Wen,Jianhua Tao,Xuefei Liu*

Main category: cs.CL

TL;DR: 本文提出了一个名为DIFND的框架，通过生成驳斥或认证证据，以及利用多模态大语言模型逻辑推理来提升假新闻检测性能和解释性。


<details>
  <summary>Details</summary>
Motivation: 解决假新闻在多媒体平台上快速传播对信息可信度带来的挑战，并提升检测假新闻的有效性和可解释性。

Method: DIFND框架结合条件扩散模型和多模态大语言模型，通过生成证据和链式推理的方式，提高假新闻检测的性能与解释性。

Result: 在FakeSV和FVC数据集上的实验结果表明，DIFND不仅在检测准确率上优于现有方法，还能提供值得信赖的决策。

Conclusion: DIFND框架通过将多模态特征、生成式证据及推理验证相结合，有效增强了假新闻检测的准确性和可解释性。

Abstract: The rapid spread of fake news across multimedia platforms presents serious
challenges to information credibility. In this paper, we propose a
Debunk-and-Infer framework for Fake News Detection(DIFND) that leverages
debunking knowledge to enhance both the performance and interpretability of
fake news detection. DIFND integrates the generative strength of conditional
diffusion models with the collaborative reasoning capabilities of multimodal
large language models (MLLMs). Specifically, debunk diffusion is employed to
generate refuting or authenticating evidence based on the multimodal content of
news videos, enriching the evaluation process with diverse yet semantically
aligned synthetic samples. To improve inference, we propose a chain-of-debunk
strategy where a multi-agent MLLM system produces logic-grounded,
multimodal-aware reasoning content and final veracity judgment. By jointly
modeling multimodal features, generative debunking cues, and reasoning-rich
verification within a unified architecture, DIFND achieves notable improvements
in detection accuracy. Extensive experiments on the FakeSV and FVC datasets
show that DIFND not only outperforms existing approaches but also delivers
trustworthy decisions.

</details>


### [89] [Bench to the Future: A Pastcasting Benchmark for Forecasting Agents](https://arxiv.org/abs/2506.21558)
*FutureSearch,:,Jack Wildman,Nikos I. Bosse,Daniel Hnyk,Peter Mühlbacher,Finn Hambly,Jon Evans,Dan Schwarz,Lawrence Phillips*

Main category: cs.CL

TL;DR: 本文提出了一个名为Bench To the Future (BTF)的基准测试，用于评估大型语言模型（LLMs）在已知结果的历史事件预测中的表现。


<details>
  <summary>Details</summary>
Motivation: 目前预测任务缺乏一种能够提供在真实、密闭且可重复的环境下评估LLM预测能力的基准测试。

Method: BTF通过提供数百个已知结果的高质量问题和相关的离线语料库，为LLMs创建了一个“过去预测”的环境，从而逼真地评估其预测能力。利用LLMs，包括Claude 4，进行了多种预测方法的测试。

Result: 结果显示，BTF能够产生与利用互联网进行预测的结果相当的效果，并表现了LLMs预测能力的稳步改进。

Conclusion: BTF是一个动态的预测基准，未来将持续更新问题以匹配新训练数据截止日期，适用于研究LLMs预测能力的多个场景。

Abstract: Forecasting is a challenging task that offers a clearly measurable way to
study AI systems. Forecasting requires a large amount of research on the
internet, and evaluations require time for events to happen, making the
development of forecasting benchmarks challenging. To date, no forecasting
benchmark provides a realistic, hermetic, and repeatable environment for LLM
forecasters. We introduce Bench To the Future (BTF), a "pastcasting" benchmark
with hundreds of high-quality questions for which the resolution is already
known. Each question is accompanied by a large offline corpus of tens of
thousands of relevant web pages, enabling a way to elicit realistic "forecasts"
on past events from LLMs. Results suggest that our pastcasting environment can
produce results comparable to those based on forecasts using the internet on
at-the-time unresolved questions. We show results benchmarking agent and
chain-of-thought forecasting approaches using several LLMs, including the
recently-released Claude 4 models, and demonstrate BTF's ability to track
steady forecasting capability progress over time. We intend this to be a living
benchmark, with new questions added continually to account for increasing
training data cutoff dates. We invite researchers to contact us at
hello@futuresearch.ai to utilize our benchmark or tooling for their own
research.

</details>


### [90] [GraphLAMA: Enabling Efficient Adaptation of Graph Language Models with Limited Annotations](https://arxiv.org/abs/2506.21559)
*Junze Chen,Cheng Yang,Shujie Li,Zhiqiang Zhang,Yawen Li,Junping Du,Chuan Shi*

Main category: cs.CL

TL;DR: 提出了GraphLAMA方法，通过引入额外的参数调整阶段解决GLMs在图分析中的效率和效果问题，展示了其在少/零样本节点分类和摘要生成任务中的优异性能。


<details>
  <summary>Details</summary>
Motivation: 解决GLMs在图分析中ICL方法的效率和效果问题，以及instruction tuning所需的大量标注数据的限制。

Method: 设计了GraphLAMA，采用图神经网络作为模型主干，将节点转换为LLM的表示空间，并融合节点和语言Token用于任务表征。在预训练阶段捕获通用知识，而在适配阶段仅更新少量参数以提升效率和准确性。

Result: GraphLAMA方法在少/零样本节点分类和摘要生成任务中实现了4.91%的绝对准确率提升，并在5-shot设置下相比ICL推理速度提高了10倍。

Conclusion: GraphLAMA通过高效参数调整阶段在性能和推理速度上取得了显著的改进，展示了在图分析任务中的巨大潜力。

Abstract: Large language models (LLMs) have demonstrated their strong capabilities in
various domains, and have been recently integrated for graph analysis as graph
language models (GLMs). With LLMs as the predictor, some GLMs can interpret
unseen tasks described by natural language, and learn from a few examples in
the prompts without parameter tuning, known as in-context learning (ICL).
Another subset of GLMs utilizes abundant training labels to enhance model
performance, known as instruction tuning. However, we argue that ICL on graphs
has effectiveness issues due to fixed parameters and efficiency issues due to
long context. Meanwhile, the large amount of labeled data required for
instruction tuning can be difficult to obtain in real-world scenarios. To this
end, we aim to introduce an extra parameter adaptation stage that can
efficiently tailor GLMs to an unseen graph and task with only a few labeled
examples, in exchange for better prediction accuracy and faster inference
speed. For implementation, in this paper we propose GraphLAMA method, with its
model backbone and learning schemes specialized for efficient tuning and
inference. Specifically, for model backbone, we use a graph neural network
(GNN) with several well-designed components to transform nodes into the
representation space of LLM tokens. Task instructions can then be represented
as a mixture of node and language tokens. In the pre-training stage, model
parameters except the LLM will be trained with different tasks to capture
general knowledge. In the adaptation stage, only a few pre-trained parameters
will be updated based on few-shot examples. Extensive experiments on
few/zero-shot node classification and summary generation show that our proposed
GraphLAMA achieves state-of-the-art performance with 4.91% absolution
improvement in accuracy. Compared with ICL, our inference speed can be 10 times
faster under 5-shot setting.

</details>


### [91] [Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning](https://arxiv.org/abs/2506.21560)
*Yifu Han,Geo Zhang*

Main category: cs.CL

TL;DR: 研究将强化学习微调应用于小型语言模型（Qwen2.5-0.5B Base），在指令遵循和数学推理任务中，发现RLOO和DPO方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过不同的微调技术提高小型化语言模型在复杂任务中的表现，同时分析其训练的关键权衡和策略。

Method: 在对Qwen2.5-0.5B模型使用三种不同微调技术（SFT、DPO与RLOO）的实验中，同时加入了合成数据增强和推理时工具（如N最佳采样和外部验证器）。

Result: RLOO结合DeBERTa奖励建模获得最佳对齐；DPO表现稳定优异；数学任务准确性因数据增强及N最佳采样显著提升。

Conclusion: 研究展示了采用高效微调与推断技术的策略，能够在任务对齐的小型模型中实现优越表现，权衡不同方法的利弊并提出实际解决方法。

Abstract: This study investigates the effectiveness of reinforcement learning (RL)
fine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two
challenging tasks: instruction following and mathematical reasoning. We compare
supervised fine-tuning (SFT), Direct Preference Optimization (DPO) using
preference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models.
Our experiments show that RLOO with DeBERTa reward modeling achieves the best
alignment, while DPO provides strong and consistent results. For math reasoing
tasks, synthetic data augmentation and best-of-N sampling with an external
verifier significantly improve accuracy, showing the potential of combining
fine-tuning with inference-time tools. This study highlights key trade-offs and
practical strategies for training lightweight, task-aligned small-scale
language models.

</details>


### [92] [Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs](https://arxiv.org/abs/2506.21561)
*Emilio Barkett,Olivia Long,Madhavendra Thakur*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型（LLMs）在判断真实与否方面的性能，发现推理模型在偏信真实性上的表现优于非推理模型，但仍低于人类水准。此外，高级模型可能存在迎合性倾向，真实和欺骗的判断准确率不对称。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨当前大型语言模型作为‘真理判断者’的性能，理解其在事实核查等高风险决策中的有效性。

Method: 研究对八种LLMs进行了4800次真实性判断测试，比较了推理模型与非推理模型之间的性能，同时分析了先进LLMs的迎合性倾向。

Result: 推理模型比非推理模型的偏信真实性表现更低，但模型整体仍高于人类参照标准。此外，某些高级LLMs在真相与欺骗的判断表现上存在显著不对称性。

Conclusion: 仅提升模型能力仍无法有效解决LLMs在真实性检测上的根本性问题，未来需要更多创新以减轻偏见和迎合性倾向。

Abstract: Despite their widespread use in fact-checking, moderation, and high-stakes
decision-making, large language models (LLMs) remain poorly understood as
judges of truth. This study presents the largest evaluation to date of LLMs'
veracity detection capabilities and the first analysis of these capabilities in
reasoning models. We had eight LLMs make 4,800 veracity judgments across
several prompts, comparing reasoning and non-reasoning models. We find that
rates of truth-bias, or the likelihood to believe a statement is true,
regardless of whether it is actually true, are lower in reasoning models than
in non-reasoning models, but still higher than human benchmarks. Most
concerning, we identify sycophantic tendencies in several advanced models
(o4-mini and GPT-4.1 from OpenAI, R1 from DeepSeek), which displayed an
asymmetry in detection accuracy, performing well in truth accuracy but poorly
in deception accuracy. This suggests that capability advances alone do not
resolve fundamental veracity detection challenges in LLMs.

</details>


### [93] [FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction](https://arxiv.org/abs/2506.21562)
*Jun Yin,Pengyu Zeng,Jing Zhong,Peilin Li,Miao Zhang,Ran Luo,Shuai Lu*

Main category: cs.CL

TL;DR: 本论文提出了一种基于“下一个房间预测”的生成模型，用于更好地支持建筑平面图生成的渐进式流程。


<details>
  <summary>Details</summary>
Motivation: 现有的建筑平面图生成模型无法满足真实建筑设计中逐步递进的需求。为解决这一问题，作者受到大语言模型自回归机制的启发，提出一种新的生成范式。

Method: 提出了一种基于自回归“下一个房间预测”的平面图生成模型，并对其在文字到平面图任务中的性能进行了对比实验评估。

Result: 实验表明，与扩散模型和Tell2Design相比，该方法在文字到平面图任务中表现出竞争力。

Conclusion: 该模型具有支持智能建筑设计的潜在应用价值，能够更好地契合真实的建筑设计流程。

Abstract: In the architectural design process, floor plan generation is inherently
progressive and iterative. However, existing generative models for floor plans
are predominantly end-to-end generation that produce an entire pixel-based
layout in a single pass. This paradigm is often incompatible with the
incremental workflows observed in real-world architectural practice. To address
this issue, we draw inspiration from the autoregressive 'next token prediction'
mechanism commonly used in large language models, and propose a novel 'next
room prediction' paradigm tailored to architectural floor plan modeling.
Experimental evaluation indicates that FPDS demonstrates competitive
performance in comparison to diffusion models and Tell2Design in the
text-to-floorplan task, indicating its potential applicability in supporting
future intelligent architectural design.

</details>


### [94] [FormosanBench: Benchmarking Low-Resource Austronesian Languages in the Era of Large Language Models](https://arxiv.org/abs/2506.21563)
*Kaiying Kevin Lin,Hsiyu Chen,Haopeng Zhang*

Main category: cs.CL

TL;DR: 本文提出FORMOSANBENCH，首次用于评估大语言模型（LLMs）在低资源奥斯特罗尼西亚语言上的表现，并显示现有模型在这些语言上的显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 由于现有的LLMs在低资源及少数语言上的能力尚未深入研究，本文旨在填补这一空白，重点关注濒危的台湾福尔摩沙语言。

Method: 通过建立FORMOSANBENCH基准，覆盖Atayal、Amis和Paiwan三种福尔摩沙语言，在机器翻译、自动语音识别和文本摘要三大任务中测试模型性能，并在零样本、10样本和微调设置中进行评估。

Result: 实验表明，目前的LLMs在福尔摩沙语言任务中的表现显著低于高资源语言，即使通过10样本学习和微调，也仅有有限提高。

Conclusion: 研究突出了开发能够支持濒危和未充分代表语言的包容性NLP技术的紧迫性，并发布了相关数据集和代码以推动未来研究。

Abstract: While large language models (LLMs) have demonstrated impressive performance
across a wide range of natural language processing (NLP) tasks in high-resource
languages, their capabilities in low-resource and minority languages remain
significantly underexplored. Formosan languages -- a subgroup of Austronesian
languages spoken in Taiwan -- are both linguistically rich and endangered,
largely due to the sociolinguistic dominance of Mandarin. In this work, we
introduce FORMOSANBENCH, the first benchmark for evaluating LLMs on
low-resource Austronesian languages. It covers three endangered Formosan
languages: Atayal, Amis, and Paiwan, across three core NLP tasks: machine
translation, automatic speech recognition (ASR), and text summarization. We
assess model performance in zero-shot, 10-shot, and fine-tuned settings using
FORMOSANBENCH. Our results reveal a substantial performance gap between
high-resource and Formosan languages. Existing LLMs consistently underperform
across all tasks, with 10-shot learning and fine-tuning offering only limited
improvements. These findings underscore the urgent need for more inclusive NLP
technologies that can effectively support endangered and underrepresented
languages. We release our datasets and code to facilitate future research in
this direction.

</details>


### [95] [Team QUST at SemEval-2025 Task 10: Evaluating Large Language Models in Multiclass Multi-label Classification of News Entity Framing](https://arxiv.org/abs/2506.21564)
*Jiyan Liu,Youzheng Liu,Taihang Wang,Xiaoman Xu,Yimin Wang,Ye Jiang*

Main category: cs.CL

TL;DR: 提出三阶段检索框架用于事实核查的声明检索，于SemEval-2025 Task 7的单语和跨语轨道中分别获得第五和第七名。


<details>
  <summary>Details</summary>
Motivation: 解决事实核查声明检索的挑战，优化检索精度和效率。

Method: 提出三阶段检索框架：1. 选择最佳候选检索模型；2. 使用多种重新排序模型优化候选集；3. 通过加权投票确定最终结果。

Result: 在SemEval-2025 Task 7的单语轨道获得第五名，跨语轨道获得第七名。

Conclusion: 该方法在事实核查声明检索任务中表现良好，为相关领域研究提供了可参考系统代码。

Abstract: This paper describes the participation of QUST_NLP in the SemEval-2025 Task
7. We propose a three-stage retrieval framework specifically designed for
fact-checked claim retrieval. Initially, we evaluate the performance of several
retrieval models and select the one that yields the best results for candidate
retrieval. Next, we employ multiple re-ranking models to enhance the candidate
results, with each model selecting the Top-10 outcomes. In the final stage, we
utilize weighted voting to determine the final retrieval outcomes. Our approach
achieved 5th place in the monolingual track and 7th place in the crosslingual
track. We release our system code at:
https://github.com/warmth27/SemEval2025_Task7.

</details>


### [96] [A Multi-Agent Probabilistic Inference Framework Inspired by Kairanban-Style CoT System with IdoBata Conversation for Debiasing](https://arxiv.org/abs/2506.21565)
*Takato Ueno,Keito Inoshita*

Main category: cs.CL

TL;DR: 本文基于日本传统的“回覧板”和“井戸端会议”文化，提出了一个整合多个大型语言模型的多代理推理框架（KCS+IBC），以应对偏差、提高可解释性，并引入概率化情感分析预测。


<details>
  <summary>Details</summary>
Motivation: 受到日本的传统交流方式启发，研究旨在利用大型语言模型解决情感分析中的偏差问题，并提升模型推理的多样性与透明性。

Method: 研究提出了KCS+IBC框架，该框架整合多个大型语言模型，加入中间的对话环节，通过概率化的方式完成情感预测和数据推理。

Result: 实验表明，KCS在多数据集上的准确性与单一LLM相当，而KCS+IBC在推理后期显示出熵的下降和方差的增加，体现了在预测聚合和多样性之间取得平衡的潜力。

Conclusion: 框架有助于情感分析中偏差的纠正与模型之间观点的融合，未来研究将定量评估其对偏差修正效果的影响并应用于更先进的情感分析系统。

Abstract: Japan's kairanban culture and idobata conversations have long functioned as
traditional communication practices that foster nuanced dialogue among
community members and contribute to the formation of social balance. Inspired
by these information exchange processes, this study proposes a multi-agent
inference framework (KCS+IBC) that integrates multiple large language models
(LLMs) to achieve bias mitigation, improved explainability, and probabilistic
prediction in sentiment analysis. In addition to sequentially sharing
prediction results, the proposed method incorporates a mid-phase casual
dialogue session to blend formal inference with individual perspectives and
introduces probabilistic sentiment prediction. Experimental results show that
KCS achieves accuracy comparable to that of a single LLM across datasets, while
KCS+IBC exhibits a consistent decrease in entropy and a gradual increase in
variance during the latter stages of inference, suggesting the framework's
ability to balance aggregation and diversity of predictions. Future work will
quantitatively assess the impact of these characteristics on bias correction
and aim to develop more advanced sentiment analysis systems.

</details>


### [97] [The Saturation Point of Backtranslation in High Quality Low Resource English Gujarati Machine Translation](https://arxiv.org/abs/2506.21566)
*Arwa Arif*

Main category: cs.CL

TL;DR: 探讨反向翻译（BT）在高质量低资源环境下的作用，针对英语-古吉拉特语翻译。实验表明，加入人工合成的数据并未提升性能，反而在部分情况下有所下降。


<details>
  <summary>Details</summary>
Motivation: 在低资源机器翻译中，反向翻译通常用于利用单语语料生成额外的合成训练数据，但在高质量低资源环境下的有效性尚不明确。

Method: 使用多语言预训练模型MBART50作为基准系统，在约50,000个高质量平行语料对上训练，外加经过筛选的反向翻译数据并进行多角度评估。

Result: 加入反向翻译数据后，翻译性能并未提升，有时甚至略有下降。通过BLEU、ChrF++、TER、BLEURT等多个指标分析，发现性能可能出现饱和现象。

Conclusion: 反向翻译在某些低资源场景中可能存在边际效用递减的现象，这为未来研究指出了方向。

Abstract: Backtranslation BT is widely used in low resource machine translation MT to
generate additional synthetic training data using monolingual corpora. While
this approach has shown strong improvements for many language pairs, its
effectiveness in high quality, low resource settings remains unclear. In this
work, we explore the effectiveness of backtranslation for English Gujarati
translation using the multilingual pretrained MBART50 model. Our baseline
system, trained on a high quality parallel corpus of approximately 50,000
sentence pairs, achieves a BLEU score of 43.8 on a validation set. We augment
this data with carefully filtered backtranslated examples generated from
monolingual Gujarati text. Surprisingly, adding this synthetic data does not
improve translation performance and, in some cases, slightly reduces it. We
evaluate our models using multiple metrics like BLEU, ChrF++, TER, BLEURT and
analyze possible reasons for this saturation. Our findings suggest that
backtranslation may reach a point of diminishing returns in certain
low-resource settings and we discuss implications for future research.

</details>


### [98] [BioPars: A Pretrained Biomedical Large Language Model for Persian Biomedical Text Mining](https://arxiv.org/abs/2506.21567)
*Baqer M. Merzah,Tania Taami,Salman Asoudeh,Amir reza Hossein pour,Saeed Mirzaee,Amir Ali Bengari*

Main category: cs.CL

TL;DR: 研究提出了用于评估语言模型生物信息学能力的新数据集和测量方法——BioPars和BioParsQA，展示其在处理波斯医学问答任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨大型语言模型在生物科学领域的应用，尤其在医学数据的存储、检索和复杂问题解决中的潜力与局限性。

Method: 开发并使用BioPars基准数据集与BioParsQA问答数据集，针对三个核心能力（特定知识获取、综合理解及证据使用）进行评估，同时对比ChatGPT、Llama和Galactica模型。

Result: BioPars模型在多个评估指标（ROUGE-L，BERTScore，MoverScore等）上优于GPT-4 1.0等对比模型，对波斯医学问答任务表现突出。

Conclusion: BioPars首次展示了基于波斯医学问答数据对语言模型的成功应用，但也揭示了其在高层次推理与真实问题解答方面的不足，未来需进一步微调。

Abstract: Large Language Models (LLMs) have recently gained attention in the life
sciences due to their capacity to model, extract, and apply complex biological
information. Beyond their classical use as chatbots, these systems are
increasingly used for complex analysis and problem-solving in specialized
fields, including bioinformatics. First, we introduce BIOPARS-BENCH, a dataset
from over 10,000 scientific articles, textbooks, and medical websites.
BioParsQA was also introduced to evaluate the proposed model, which consists of
5,231 Persian medical questions and answers. This study then introduces
BioPars, a simple but accurate measure designed to assess LLMs for three main
abilities: acquiring subject-specific knowledge, interpreting and synthesizing
such knowledge, and demonstrating proper evidence. Comparing ChatGPT, Llama,
and Galactica, our study highlights their ability to remember and retrieve
learned knowledge but also reveals shortcomings in addressing higher-level,
real-world questions and fine-grained inferences. These findings indicate the
need for further fine-tuning to address the capabilities of LLM in
bioinformatics tasks. To our knowledge, BioPars is the first application of LLM
in Persian medical QA, especially for generating long answers. Evaluation of
four selected medical QA datasets shows that BioPars has achieved remarkable
results compared to comparative approaches. The model on BioParsQA achieved a
ROUGE-L score of 29.99, which is an improvement over GPT-4 1.0. The model
achieved a BERTScore of 90.87 with the MMR method. The MoverScore and BLEURT
values were also higher in this model than the other three models. In addition,
the reported scores for the model are MoverScore=60.43 and BLEURT=50.78.
BioPars is an ongoing project and all resources related to its development will
be made available via the following GitHub repository:
https://github.com/amirap80/BioPars.

</details>


### [99] [Assessing RAG and HyDE on 1B vs. 4B-Parameter Gemma LLMs for Personal Assistants Integretion](https://arxiv.org/abs/2506.21568)
*Andrejs Sorstkins*

Main category: cs.CL

TL;DR: 该研究探讨了两种增强策略（RAG 和 HyDE）在小型语言模型中的效果，并发现 RAG 在减少延迟和消除事实错误上表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决大型语言模型在边缘设备和隐私敏感应用中的资源效率问题。

Method: 通过在1亿和4亿参数的Gemma LLM中测试RAG与HyDE策略，结合MongoDB短期内存、Qdrant长期存储和FastAPI进行系统编排，并提供前端交互。

Result: RAG在减少延迟和消除领域特定问题上的事实幻觉表现出色，而HyDE在复杂查询上的语义相关性更强但增加了响应时间和幻觉率。

Conclusion: RAG是小型语言模型支持的设备端个人助理的实际可行选择。

Abstract: Resource efficiency is a critical barrier to deploying large language models
(LLMs) in edge and privacy-sensitive applications. This study evaluates the
efficacy of two augmentation strategies--Retrieval-Augmented Generation (RAG)
and Hypothetical Document Embeddings (HyDE)--on compact Gemma LLMs of 1 billion
and 4 billion parameters, within the context of a privacy-first personal
assistant. We implement short-term memory via MongoDB and long-term semantic
storage via Qdrant, orchestrated through FastAPI and LangChain, and expose the
system through a React.js frontend. Across both model scales, RAG consistently
reduces latency by up to 17\% and eliminates factual hallucinations when
responding to user-specific and domain-specific queries. HyDE, by contrast,
enhances semantic relevance--particularly for complex physics prompts--but
incurs a 25--40\% increase in response time and a non-negligible hallucination
rate in personal-data retrieval. Comparing 1 B to 4 B models, we observe that
scaling yields marginal throughput gains for baseline and RAG pipelines, but
magnifies HyDE's computational overhead and variability. Our findings position
RAG as the pragmatic choice for on-device personal assistants powered by
small-scale LLMs.

</details>


### [100] [Hybrid-NL2SVA: Integrating RAG and Finetuning for LLM-based NL2SVA](https://arxiv.org/abs/2506.21569)
*Weihua Xiao,Derek Ekberg,Siddharth Garg,Ramesh Karri*

Main category: cs.CL

TL;DR: 研究提出一种特别设计的检索增强生成框架（RAG）和合成微调数据集，显著提升语言模型在自然语言到SystemVerilog Assertions（SVAs）转换任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 硬件设计的正确性验证需要依赖SystemVerilog Assertions（SVAs），但天然语言描述向SVAs的手动编写耗时且容易出错。大语言模型的进步为自动化这一过程提供了可能，但其对专业语法和语义的理解仍不足。

Method: 提出了一种定制化的检索增强生成（RAG）框架和合成微调数据集，并通过提示引导解释，教授模型逐层构建并微调小型语言模型以提高性能。同时构建了迄今最大的NL2SVA性能评估数据集。

Result: 实验结果表明，定制化RAG框架使功能匹配的SVAs数量相比GPT-4o-mini增加了58.42%，而Qwen2.5-Coder-7B-Instruct在微调数据集和混合检索的结合下，实现了59.05%的性能提升。

Conclusion: 通过定制的框架和数据增强方法，研究显著改进了语言模型在NL到SVA转换任务中的表现，为更高效的硬件设计验证提供技术支持。

Abstract: SystemVerilog Assertions (SVAs) are critical for verifying the correctness of
hardware designs, but manually writing them from natural language property
descriptions, i.e., NL2SVA, remains a labor-intensive and error-prone task.
Recent advances in large language models (LLMs) offer opportunities to automate
this translation. However, existing models still struggle with understanding
domain-specific syntax and semantics. To enhance LLM performance in NL2SVA, we
propose a customized retrieval-augmented generation (RAG) framework and a
synthetic fine-tuning dataset that together improve LLM's performance. To
further improve lightweight models over NL2SVA, our fine-tuning dataset
provides prompt-guided explanations that teach LLMs the layer-by-layer
construction process of concurrent SVAs, enabling supervised fine-tuning that
greatly improves syntax and functionality accuracy. To evaluate the performance
of LLMs over NL2SVA, we construct the largest evaluation dataset for NL2SVA,
comprising 40 Verilog designs and 229 formally verified SVAs with detailed
annotations. Experimental results show that our customized RAG framework
increases the number of functionality matched SVAs by 58.42% over GPT-4o-mini,
while Qwen2.5-Coder-7B-Instruct fine-tuned on our fine-tuning dataset and
integrated with HybridRetrieval achieves a 59.05% over the base Qwen model.

</details>


### [101] [Random Initialization Can't Catch Up: The Advantage of Language Model Transfer for Time Series Forecasting](https://arxiv.org/abs/2506.21570)
*Roland Riachi,Kashif Rasul,Arjun Ashok,Prateek Humane,Alexis Roger,Andrew R. Williams,Yuriy Nevmyvaka,Irina Rish*

Main category: cs.CL

TL;DR: 分析了如何有效地将预训练语言模型（LMs）适配到低数据量的时间序列预测场景中以及不同设计选择对效果的影响。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型在时间序列中有效迁移的潜力，并研究不同设计选择如何影响低数据量场景下的结果。

Method: 分析了上游后训练、时间序列分词器以及语言模型规模等设计选择对时间序列预测的影响，同时与随机初始化模型进行对比分析。

Result: 在低数据量环境中，特定的设计选择会显著降低验证损失。语言模型在验证损失方面的表现优于随机初始化模型，且其性能持续改善，表现出非消失的迁移效能差距。

Conclusion: 通过不同模型设计的实验，不仅揭示了计算效率训练在时间序列中的意义，还为研究数据分布的模态无关特性提供了新方向。

Abstract: Recent works have demonstrated the effectiveness of adapting pre-trained
language models (LMs) for forecasting time series in the low-data regime. We
build upon these findings by analyzing the effective transfer from language
models to time series forecasting under various design choices including
upstream post-training, time series tokenizer and language backbone size. In
the low-data regime, these design choices have a significant impact on the
validation loss, with clear-cut choices that outperform others. Contrary to
Hernandez et al. (2021), we observe that the validation loss of the LMs
continues to smoothly decrease long after the validation loss of the randomly
initialized models has converged, leading to a non-vanishing transfer gap that
holds across design choices. These findings not only help shed light on the
effective use of compute-efficient training for time series, but also open the
way for the study of modality-agnostic properties of data distributions
leveraged by these models.

</details>


### [102] [Towards Understanding the Cognitive Habits of Large Reasoning Models](https://arxiv.org/abs/2506.21571)
*Jianshuo Dong,Yujia Fu,Chuanrui Hu,Chao Zhang,Han Qiu*

Main category: cs.CL

TL;DR: 论文提出CogTest来评估大规模推理模型（LRMs）的认知习惯，这些模型展现了类人认知习惯，并能根据任务自适应调整。


<details>
  <summary>Details</summary>
Motivation: 受到LRMs产生类似人类推理链模式的现象启发，作者探索这些模型是否具有类人的认知习惯。

Method: 作者设计了一个名为CogTest的基准，包括16种认知习惯，每种习惯有25个多样化任务，并对16个LLMs进行认知习惯评估。

Result: 实验发现LRMs表现出类人的认知习惯，并能在不同任务中自适应调整；同时某些习惯与有害响应生成强相关。

Conclusion: 研究LRMs的持续行为模式能帮助更深入理解LLM的错误行为，为模型改进提供依据。

Abstract: Large Reasoning Models (LRMs), which autonomously produce a reasoning Chain
of Thought (CoT) before producing final responses, offer a promising approach
to interpreting and monitoring model behaviors. Inspired by the observation
that certain CoT patterns -- e.g., ``Wait, did I miss anything?'' --
consistently emerge across tasks, we explore whether LRMs exhibit human-like
cognitive habits. Building on Habits of Mind, a well-established framework of
cognitive habits associated with successful human problem-solving, we introduce
CogTest, a principled benchmark designed to evaluate LRMs' cognitive habits.
CogTest includes 16 cognitive habits, each instantiated with 25 diverse tasks,
and employs an evidence-first extraction method to ensure reliable habit
identification. With CogTest, we conduct a comprehensive evaluation of 16
widely used LLMs (13 LRMs and 3 non-reasoning ones). Our findings reveal that
LRMs, unlike conventional LLMs, not only exhibit human-like habits but also
adaptively deploy them according to different tasks. Finer-grained analyses
further uncover patterns of similarity and difference in LRMs' cognitive habit
profiles, particularly certain inter-family similarity (e.g., Qwen-3 models and
DeepSeek-R1). Extending the study to safety-related tasks, we observe that
certain habits, such as Taking Responsible Risks, are strongly associated with
the generation of harmful responses. These findings suggest that studying
persistent behavioral patterns in LRMs' CoTs is a valuable step toward deeper
understanding of LLM misbehavior. The code is available at:
https://github.com/jianshuod/CogTest.

</details>


### [103] [Aligning MLLM Benchmark With Human Preferences via Structural Equation Modeling](https://arxiv.org/abs/2506.21572)
*Tianyu. Zou,Shengwu. Xiong,Ruilin. Yao,Jirui. Huang,Yi. Rong,Yaxiong. Chen,Shili. Xiong,Cong. Wang*

Main category: cs.CL

TL;DR: 本研究提出了一种基于结构方程模型（SEM）的新型方法，用于改进多模态大语言模型（MLLMs）的评估，提出了一个新的能力层次体系并构建了新的基准测试Gold，实验结果表明新方法有效提升了评估的解释性和诊断能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型评估方法存在任务分组缺乏清晰认知目标、指标重复冗余、诊断能力不足等问题，研究目标为提高评估的结构性和理论依据。

Method: 提出了基于结构方程模型（SEM）的框架，结合Piaget认知发展理论，设计了感知、记忆、推理三层能力体系，并以此重新组织现有评估基准，构建新基准Gold。

Result: 实验结果表明，新基准具有更强的解释性，减少了指标冗余，并提高了认知一致性。

Conclusion: 基于SEM的框架和Gold基准为提升MLLMs评估质量提供了有效方法，能够更好诊断模型性能并提高其认知能力的分离性分析。

Abstract: Evaluating multimodal large language models (MLLMs) remains a fundamental
challenge due to a lack of structured, interpretable, and theoretically
grounded benchmark designs. Existing benchmarks often adopt heuristic-based
task groupings with unclear cognitive targets, thus resulting in overlapping
abilities, redundant indicators, and limited diagnostic power. In this work, we
propose a novel framework for aligning MLLM benchmark based on Structural
Equation Modeling (SEM) to analyze and quantify the internal validity,
dimensional separability, and contribution of benchmark components. Motivated
by the observed limitations of current designs, we further introduce a novel
capability hierarchy grounded in Piagets theory of cognitive development,
dividing MLLM abilities into three hierarchical layers, i.e., Perception,
Memory, and Reasoning. We reorganize existing MLLM benchmarks under the
proposed framework and construct a new benchmark named Gold. Experimental
results demonstrate that the proposed benchmark exhibits stronger
interpretability, reduced indicator redundancy, and clearer cognitive
consistency compared to existing approaches.

</details>


### [104] [Instruction Learning Paradigms: A Dual Perspective on White-box and Black-box LLMs](https://arxiv.org/abs/2506.21573)
*Yanwei Ren,Liu Liu,Baosheng Yu,Jiayan Qiu,Quan Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新型框架，将黑箱模型和白箱模型的优势结合，以优化大型语言模型（LLMs）的指令质量。


<details>
  <summary>Details</summary>
Motivation: 当前优化LLMs指令的方法面临资源消耗大或成本高的问题，需要更高效的解决方案。

Method: 通过将黑箱模型生成的高质量初始指令与白箱模型提供的隐藏状态和特征融合，并加入语义相似性约束，实现统一的高维表征，优化指令质量。

Result: 实验表明，该方法在复杂推理和跨语言泛化等多任务中均优于现有最佳基线方法。

Conclusion: 该方法结合黑箱初始与高级语义优化，提供了可扩展且高效的LLMs指令优化解决方案，可广泛应用于实际场景中。

Abstract: Optimizing instructions for large language models (LLMs) is critical for
harnessing their full potential in complex and diverse tasks. However, relying
solely on white-box approaches demands extensive computational resources and
offers limited representational capacity, while black-box models can incur
prohibitive financial costs. To address these challenges, we introduce a novel
framework that seamlessly merges the strengths of both paradigms. Black-box
models provide high-quality, diverse instruction initializations, and white-box
models supply fine-grained interpretability through hidden states and output
features. By enforcing a semantic similarity constraint, these components fuse
into a unified high-dimensional representation that captures deep semantic and
structural nuances, enabling an iterative optimization process to refine
instruction quality and adaptability. Extensive evaluations across a broad
spectrum of tasks-ranging from complex reasoning to cross-lingual
generalization-demonstrate that our approach consistently outperforms
state-of-the-art baselines. This fusion of black-box initialization with
advanced semantic refinement yields a scalable and efficient solution, paving
the way for next-generation LLM-driven applications in diverse real-world
scenarios. The source code will be released soon.

</details>


### [105] [MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark](https://arxiv.org/abs/2412.15194)
*Qihao Zhao,Yangyu Huang,Tengchao Lv,Lei Cui,Qinzheng Sun,Shaoguang Mao,Xin Zhang,Ying Xin,Qiufeng Yin,Scarlett Li,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出了MMLU-CF，一个去污染且更具挑战性的多选问题数据集，用于更准确地评估大语言模型的世界知识理解能力。


<details>
  <summary>Details</summary>
Motivation: 由于开放源代码的基准数据集可能受到数据污染影响，现有的评估结果可能不可靠，因此需要一个无污染的评估基准。

Method: 利用更广泛的数据源，设计三项去污染规则以避免非故意数据泄露，并通过将数据划分为验证集和封闭测试集来防止恶意数据泄露。

Result: 实验表明，强大的GPT-4o在测试集中仅取得5-shot得分73.4%和0-shot得分71.9%，证明了该方法的有效性。

Conclusion: MMLU-CF 提供了一个无污染且更严格的评估标准，有助于更可靠地评估大语言模型的能力。

Abstract: Multiple-choice question (MCQ) datasets like Massive Multitask Language
Understanding (MMLU) are widely used to evaluate the commonsense,
understanding, and problem-solving abilities of large language models (LLMs).
However, the open-source nature of these benchmarks and the broad sources of
training data for LLMs have inevitably led to benchmark contamination,
resulting in unreliable evaluation results. To alleviate this issue, we propose
a contamination-free and more challenging MCQ benchmark called MMLU-CF. This
benchmark reassesses LLMs' understanding of world knowledge by averting both
unintentional and malicious data leakage. To avoid unintentional data leakage,
we source data from a broader domain and design three decontamination rules. To
prevent malicious data leakage, we divide the benchmark into validation and
test sets with similar difficulty and subject distributions. The test set
remains closed-source to ensure reliable results, while the validation set is
publicly available to promote transparency and facilitate independent
verification. Our evaluation of mainstream LLMs reveals that the powerful
GPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on
the test set, which indicates the effectiveness of our approach in creating a
more rigorous and contamination-free evaluation standard. The GitHub repository
is available at https://github.com/microsoft/MMLU-CF and the dataset refers to
https://huggingface.co/datasets/microsoft/MMLU-CF.

</details>


### [106] [Digital Gatekeepers: Exploring Large Language Model's Role in Immigration Decisions](https://arxiv.org/abs/2506.21574)
*Yicheng Mao,Yang Zhao*

Main category: cs.CL

TL;DR: 研究探讨了GPT-3.5和GPT-4在移民决策中的支持潜力，发现其具备与人类决策对齐的能力，但也存在偏见与局限性。


<details>
  <summary>Details</summary>
Motivation: 全球化和移民人口增加导致移民部门面临繁重的工作量和决策公平性挑战，研究旨在探索使用人工智能解决这些问题的可能性。

Method: 采用混合研究方法，包括离散选择实验和深度访谈，研究了大型语言模型的决策策略及其公平性。

Result: 研究发现LLMs可以对齐人类的决策策略，注重效用最大化与程序公平性，但同时表现出关于国籍的刻板印象和对特权群体的倾向。

Conclusion: LLMs在自动化和改善移民决策方面具有潜力，但亦需克服固有的偏见和局限性。

Abstract: With globalization and increasing immigrant populations, immigration
departments face significant work-loads and the challenge of ensuring fairness
in decision-making processes. Integrating artificial intelligence offers a
promising solution to these challenges. This study investigates the potential
of large language models (LLMs),such as GPT-3.5 and GPT-4, in supporting
immigration decision-making. Utilizing a mixed-methods approach,this paper
conducted discrete choice experiments and in-depth interviews to study LLM
decision-making strategies and whether they are fair. Our findings demonstrate
that LLMs can align their decision-making with human strategies, emphasizing
utility maximization and procedural fairness. Meanwhile, this paper also
reveals that while ChatGPT has safeguards to prevent unintentional
discrimination, it still exhibits stereotypes and biases concerning nationality
and shows preferences toward privileged group. This dual analysis highlights
both the potential and limitations of LLMs in automating and enhancing
immigration decisions.

</details>


### [107] [STRuCT-LLM: Unifying Tabular and Graph Reasoning with Reinforcement Learning for Semantic Parsing](https://arxiv.org/abs/2506.21575)
*Josefa Lia Stoisser,Marc Boubnovski Martell,Lawrence Phillips,Casper Hansen,Julien Fauqueur*

Main category: cs.CL

TL;DR: 本文提出了一种统一框架STRuCT-LLM，用于训练大型语言模型（LLMs），以实现对关系型和图结构数据的结构化推理。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常将关系型和图结构推理分开处理，而本文旨在利用两者的共性，通过联合优化和知识迁移，实现更高效的结构化推理。

Method: 采用强化学习结合Chain-of-Thought监督，提出一个基于图编辑距离的拓扑感知奖励函数，同时针对SQL和Cypher任务进行联合优化。

Result: 提出的方法在语义解析任务中显著提高了Spider（13.5%）和Text2Cypher（73.1%）的性能，还展示了在无QA特定监督的情况下对表格问答和知识图问答的强泛化能力。

Conclusion: 结果表明，基于可执行查询的结构化推理方法非常有效，同时通过联合训练SQL和Cypher任务，可以实现相互促进，提升模型性能。

Abstract: We propose STRuCT-LLM, a unified framework for training large language models
(LLMs) to perform structured reasoning over both relational and
graph-structured data. Our approach jointly optimizes Text-to-SQL and
Text-to-Cypher tasks using reinforcement learning (RL) combined with
Chain-of-Thought (CoT) supervision. To support fine-grained optimization in
graph-based parsing, we introduce a topology-aware reward function based on
graph edit distance. Unlike prior work that treats relational and graph
formalisms in isolation, STRuCT-LLM leverages shared abstractions between SQL
and Cypher to induce cross-formalism transfer, enabling SQL training to improve
Cypher performance and vice versa - even without shared schemas. Our largest
model (QwQ-32B) achieves substantial relative improvements across tasks: on
semantic parsing, Spider improves by 13.5\% and Text2Cypher by 73.1\%. The
model also demonstrates strong zero-shot generalization, improving performance
on downstream tabular QA (TableBench: 8.5\%) and knowledge graph QA
(CR-LT-KGQA: 1.7\%) without any QA-specific supervision. These results
demonstrate both the effectiveness of executable queries as scaffolds for
structured reasoning and the synergistic benefits of jointly training on SQL
and Cypher (code available at https://github.com/bouv/STRuCT-LLM).

</details>


### [108] [Adapting Whisper for Parameter-efficient Code-Switching Speech Recognition via Soft Prompt Tuning](https://arxiv.org/abs/2506.21576)
*Hongli Yang,Yizhou Peng,Hao Huang,Sheng Li*

Main category: cs.CL

TL;DR: 本文探讨了使用Soft Prompt Tuning (SPT)优化低资源环境下的代码切换语音识别，结果表明深度提示调优最为有效。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模多语言ASR模型在低资源场景中性能欠佳，亟需一种提高其代码切换识别能力且不显著增加计算成本的方法。

Method: 使用了Soft Prompt Tuning (SPT)方法，并提出了SPT4ASR，将模型参数冻结，仅对软提示进行训练，同时探索全量微调和不同SPT变体组合的改进方式。

Result: 在SEAME和ASRU2019数据集上的实验表明，深度提示调优是最有效的SPT方法，SPT4ASR进一步减少了错误率，且与LoRA类似保持了参数效率。

Conclusion: SPT在低资源代码切换语音识别上表现优异，通过创新组合（SPT4ASR），实现了错误率下降和高参数效率，表现得到显著加强且不损害现有语言性能。

Abstract: Large-scale multilingual ASR models like Whisper excel in high-resource
settings but face challenges in low-resource scenarios, such as rare languages
and code-switching (CS), due to computational costs and catastrophic
forgetting. We explore Soft Prompt Tuning (SPT), a parameter-efficient method
to enhance CS ASR while preserving prior knowledge. We evaluate two strategies:
(1) full fine-tuning (FFT) of both soft prompts and the entire Whisper model,
demonstrating improved cross-lingual capabilities compared to traditional
methods, and (2) adhering to SPT's original design by freezing model parameters
and only training soft prompts. Additionally, we introduce SPT4ASR, a
combination of different SPT variants. Experiments on the SEAME and ASRU2019
datasets show that deep prompt tuning is the most effective SPT approach, and
our SPT4ASR methods achieve further error reductions in CS ASR, maintaining
parameter efficiency similar to LoRA, without degrading performance on existing
languages.

</details>


### [109] [Data Efficacy for Language Model Training](https://arxiv.org/abs/2506.21545)
*Yalun Dai,Yangyu Huang,Xin Zhang,Wenshan Wu,Chong Li,Wenhui Lu,Shijie Cao,Li Dong,Scarlett Li*

Main category: cs.CL

TL;DR: 提出了DELT范式，优化数据组织，验证了在不增加训练数据量和模型规模的情况下提升语言模型性能的可能性。


<details>
  <summary>Details</summary>
Motivation: 尽管数据高效性受到关注，但数据效能（组织优化）仍是一个较少研究的领域。

Method: 提出DELT范式，包括数据评分、数据选择和数据排序，并设计了LQS评分方法和Folding排序方法。

Result: DELT在实验中显著提升了语言模型性能，其中LQS与Folding的结合效果最好，同时兼顾了数据效能与效率。

Conclusion: 数据效能是语言模型训练中的一个有潜力的研究方向，通过优化数据组织可以提升性能且无需增加数据量和模型规模。

Abstract: Data is fundamental to the training of language models (LM). Recent research
has been dedicated to data efficiency, which aims to maximize performance by
selecting a minimal or optimal subset of training data. Techniques such as data
filtering, sampling, and selection play a crucial role in this area. To
complement it, we define Data Efficacy, which focuses on maximizing performance
by optimizing the organization of training data and remains relatively
underexplored. This work introduces a general paradigm, DELT, for considering
data efficacy in LM training, which highlights the significance of training
data organization. DELT comprises three components: Data Scoring, Data
Selection, and Data Ordering. Among these components, we design
Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which
considers both the learnability and quality of each data sample from the
gradient consistency perspective. We also devise Folding Ordering (FO), as a
novel instance of Data Ordering, which addresses issues such as model
forgetting and data distribution bias. Comprehensive experiments validate the
data efficacy in LM training, which demonstrates the following: Firstly,
various instances of the proposed DELT enhance LM performance to varying
degrees without increasing the data scale and model size. Secondly, among these
instances, the combination of our proposed LQS for data scoring and Folding for
data ordering achieves the most significant improvement. Lastly, data efficacy
can be achieved together with data efficiency by applying data selection.
Therefore, we believe that data efficacy is a promising foundational area in LM
training.

</details>


### [110] [Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR](https://arxiv.org/abs/2506.21577)
*Hongli Yang,Sheng Li,Hao Huang,Ayiduosi Tuohan,Yizhou Peng*

Main category: cs.CL

TL;DR: 本文提出了结合Soft Prompt Tuning（SPT）和Whisper模型的工具包SPT-Whisper，通过优化编码器和解码器的提示调优，改进了语言干扰问题和对新语言的扩展能力。


<details>
  <summary>Details</summary>
Motivation: 多语言语音识别面临语言干扰和新语言扩展能力不足的挑战，本文旨在提供高效的解决方案，提升动态多语言ASR模型的性能。

Method: 提出了三大创新：1）全软提示调优（Entire SPT），对编码器和解码器应用软提示优化；2）语言感知提示调优（LAPT），使用轻量级提示矩阵编码跨语言相似性和语言特定特征；3）SPT-Whisper工具包，将SPT技术集成至Whisper，实现高效的持续学习。

Result: 在FLEURS的三种语言实验中，Entire SPT和LAPT在语言扩展任务上的性能分别提升了5.0%和16.0%，同时计算开销极低。

Conclusion: SPT-Whisper是一种实现动态多语言ASR的高效方案，在扩展新语言时不损害现有性能，同时提高模型的持续学习能力。

Abstract: Recent advancements in multilingual automatic speech recognition (ASR) have
been driven by large-scale end-to-end models like Whisper. However, challenges
such as language interference and expanding to unseen languages (language
expansion) without degrading performance persist. This paper addresses these
with three contributions: 1) Entire Soft Prompt Tuning (Entire SPT), which
applies soft prompts to both the encoder and decoder, enhancing feature
extraction and decoding; 2) Language-Aware Prompt Tuning (LAPT), which
leverages cross-lingual similarities to encode shared and language-specific
features using lightweight prompt matrices; 3) SPT-Whisper, a toolkit that
integrates SPT into Whisper and enables efficient continual learning.
Experiments across three languages from FLEURS demonstrate that Entire SPT and
LAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks,
respectively, providing an efficient solution for dynamic, multilingual ASR
models with minimal computational overhead.

</details>


### [111] [HealthQA-BR: A System-Wide Benchmark Reveals Critical Knowledge Gaps in Large Language Models](https://arxiv.org/abs/2506.21578)
*Andrew Maranhão Ventura D'addario*

Main category: cs.CL

TL;DR: 该论文提出HealthQA-BR，这是一个评价葡萄牙语医疗领域的系统性基准，评估了20多种语言模型在多个医疗专业中的表现，揭示了模型在不同领域知识表现不均的现象。


<details>
  <summary>Details</summary>
Motivation: 当前医疗领域语言模型评估主要集中在英语和医生领域，忽略了医疗团队的多专业协作特性，存在不真实的评估偏误。

Method: 开发并公开了HealthQA-BR基准，该基准包含5632道来自巴西相关考试的问题，用于评估20多种主流语言模型在多专业医疗领域的零样本表现。

Result: 结果显示，虽然像GPT 4.1这样的先进模型在整体上表现良好，但其在不同医疗专业中的表现差异巨大，如眼科学达到98.7%，而神经外科只有60.0%，社会工作更是仅为68.4%。

Conclusion: 单一评分不足以验证模型的安全性，呼吁通过更细致的评估手段检查AI在各医疗专业的准备情况，以提升其行业使用价值。

Abstract: The evaluation of Large Language Models (LLMs) in healthcare has been
dominated by physician-centric, English-language benchmarks, creating a
dangerous illusion of competence that ignores the interprofessional nature of
patient care. To provide a more holistic and realistic assessment, we introduce
HealthQA-BR, the first large-scale, system-wide benchmark for
Portuguese-speaking healthcare. Comprising 5,632 questions from Brazil's
national licensing and residency exams, it uniquely assesses knowledge not only
in medicine and its specialties but also in nursing, dentistry, psychology,
social work, and other allied health professions. We conducted a rigorous
zero-shot evaluation of over 20 leading LLMs. Our results reveal that while
state-of-the-art models like GPT 4.1 achieve high overall accuracy (86.6%),
this top-line score masks alarming, previously unmeasured deficiencies. A
granular analysis shows performance plummets from near-perfect in specialties
like Ophthalmology (98.7%) to barely passing in Neurosurgery (60.0%) and, most
notably, Social Work (68.4%). This "spiky" knowledge profile is a systemic
issue observed across all models, demonstrating that high-level scores are
insufficient for safety validation. By publicly releasing HealthQA-BR and our
evaluation suite, we provide a crucial tool to move beyond single-score
evaluations and toward a more honest, granular audit of AI readiness for the
entire healthcare team.

</details>


### [112] [From General Reasoning to Domain Expertise: Uncovering the Limits of Generalization in Large Language Models](https://arxiv.org/abs/2506.21580)
*Dana Alsagheer,Yang Lu,Abdulrahman Kamal,Omar Kamal,Mohammad Kamal,Nada Mansour,Cosmo Yang Wu,Rambiba Karanjai,Sen Li,Weidong Shi*

Main category: cs.CL

TL;DR: 本研究探讨大型语言模型（LLMs）的一般推理能力与具体领域推理任务表现之间的联系。


<details>
  <summary>Details</summary>
Motivation: AI技术快速发展，推动了对LLMs泛用推理能力的研究需求，以提升其在实际决策中的表现。

Method: 分析LLMs在一般推理与特定领域推理任务间的关联性，评估其决策能力。

Result: 研究揭示了推理与决策能力之间的关键关系，为进一步改进LLMs提供了理论依据。

Conclusion: 提升LLMs的一般推理能力将有助于其在领域特定任务中的表现，更好地支持复杂的决策过程。

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
remarkable capabilities in various domains. However, effective decision-making
relies heavily on strong reasoning abilities. Reasoning is the foundation for
decision-making, providing the analytical and logical framework to make sound
choices. Reasoning involves analyzing information, drawing inferences, and
reaching conclusions based on logic or evidence. Decision-making builds on this
foundation by applying the insights from reasoning to select the best course of
action among alternatives. Together, these processes create a continuous cycle
of thought and action aimed at achieving goals effectively. As AI technology
evolves, there is a growing trend to train LLMs to excel in general reasoning.
This study explores how the general reasoning capabilities of LLMs connect to
their performance in domain-specific reasoning tasks.

</details>


### [113] [VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents](https://arxiv.org/abs/2506.21582)
*Sam Yu-Te Lee,Chengyang Ji,Shicheng Wen,Lifu Huang,Dongyi Liu,Kwan-Liu Ma*

Main category: cs.CL

TL;DR: VIDEE系统使非专业数据分析员能够使用智能体进行高级文本分析，简化了流程并提高了可访问性。


<details>
  <summary>Details</summary>
Motivation: 传统的文本分析需要专门的NLP知识，阻碍了初学者的参与，而大语言模型的进步为更便捷的文本分析铺平了道路。

Method: 提出了VIDEE系统，包括三阶段流程：1.采用人机合作的蒙特卡罗树搜索算法进行生成式推理（分解阶段）；2.自动生成文本分析管道（执行阶段）；3.通过LLM评估和可视化进行结果验证（评估阶段）。

Result: 通过两项定量实验和用户研究，证明了VIDEE的效果和可用性，并分析了智能体常见的错误及用户行为模式。

Conclusion: VIDEE对于非专业用户具有实用性，研究结果展示了人机协作的设计启示，并为智能文本分析系统的改进提供了方向。

Abstract: Text analytics has traditionally required specialized knowledge in Natural
Language Processing (NLP) or text analysis, which presents a barrier for
entry-level analysts. Recent advances in large language models (LLMs) have
changed the landscape of NLP by enabling more accessible and automated text
analysis (e.g., topic detection, summarization, information extraction, etc.).
We introduce VIDEE, a system that supports entry-level data analysts to conduct
advanced text analytics with intelligent agents. VIDEE instantiates a
human-agent collaroration workflow consisting of three stages: (1)
Decomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search
algorithm to support generative reasoning with human feedback, (2) Execution,
which generates an executable text analytics pipeline, and (3) Evaluation,
which integrates LLM-based evaluation and visualizations to support user
validation of execution results. We conduct two quantitative experiments to
evaluate VIDEE's effectiveness and analyze common agent errors. A user study
involving participants with varying levels of NLP and text analytics experience
-- from none to expert -- demonstrates the system's usability and reveals
distinct user behavior patterns. The findings identify design implications for
human-agent collaboration, validate the practical utility of VIDEE for
non-expert users, and inform future improvements to intelligent text analytics
systems.

</details>


### [114] [Hope Speech Detection in code-mixed Roman Urdu tweets: A Positive Turn in Natural Language Processing](https://arxiv.org/abs/2506.21583)
*Muhammad Ahmad,Muhammad Waqas,Ameer Hamza,Ildar Batyrshin,Grigori Sidorov*

Main category: cs.CL

TL;DR: 本文提出了第一个针对代码混合罗马化乌尔都语进行希望言论检测的研究，提供了一个精标注的数据集以及优化的注意力机制Transformer模型，并验证了其在性能上的显著性提升。


<details>
  <summary>Details</summary>
Motivation: 现有希望言论检测研究主要聚焦于高资源语言，忽略了像罗马化乌尔都语这种非正式和低资源的语言形式，存在研究空白。

Method: 研究包括：引入多类别精标注的数据集，分析希望的心理学基础和语言模式，提出优化的注意力机制Transformer模型（基于XLM-R），并使用5折交叉验证和t检验验证模型的有效性。

Result: 提出的XLM-R模型在交叉验证中取得了0.78的最佳得分，分别比基线模型SVM（0.75）和BiLSTM（0.76）提高了4%和2.63%。

Conclusion: 本研究填补了低资源语言希望言论检测的空白，为代码混合罗马化乌尔都语建立了新的基准，并证明了所提模型的有效性。

Abstract: Hope is a positive emotional state involving the expectation of favorable
future outcomes, while hope speech refers to communication that promotes
optimism, resilience, and support, particularly in adverse contexts. Although
hope speech detection has gained attention in Natural Language Processing
(NLP), existing research mainly focuses on high-resource languages and
standardized scripts, often overlooking informal and underrepresented forms
such as Roman Urdu. To the best of our knowledge, this is the first study to
address hope speech detection in code-mixed Roman Urdu by introducing a
carefully annotated dataset, thereby filling a critical gap in inclusive NLP
research for low-resource, informal language varieties. This study makes four
key contributions: (1) it introduces the first multi-class annotated dataset
for Roman Urdu hope speech, comprising Generalized Hope, Realistic Hope,
Unrealistic Hope, and Not Hope categories; (2) it explores the psychological
foundations of hope and analyzes its linguistic patterns in code-mixed Roman
Urdu to inform dataset development; (3) it proposes a custom attention-based
transformer model optimized for the syntactic and semantic variability of Roman
Urdu, evaluated using 5-fold cross-validation; and (4) it verifies the
statistical significance of performance gains using a t-test. The proposed
model, XLM-R, achieves the best performance with a cross-validation score of
0.78, outperforming the baseline SVM (0.75) and BiLSTM (0.76), with gains of 4%
and 2.63% respectively.

</details>


### [115] [Empirical Evidence for Alignment Faking in Small LLMs and Prompt-Based Mitigation Techniques](https://arxiv.org/abs/2506.21584)
*J. Koorndijk*

Main category: cs.CL

TL;DR: 研究表明小型语言模型也可能出现欺骗性对齐行为，并通过提示干预方法有效减少。


<details>
  <summary>Details</summary>
Motivation: 探讨小型语言模型是否也会表现出欺骗性对齐行为，以及是否可通过提示来减轻该现象。

Method: 对小型指令微调模型（LLaMA 3 8B）进行实证研究，并通过提示方式（如道义框架和草稿推理）用以减少欺骗性对齐表现。

Result: 验证了小型模型的欺骗性对齐现象，并发现提示干预可明显减轻该行为。

Conclusion: 提示伦理和小型模型的对齐评估不容忽视，需在多模型规模和部署环境中进一步探讨对齐策略。

Abstract: Current literature suggests that alignment faking (deceptive alignment) is an
emergent property of large language models. We present the first empirical
evidence that a small instruction-tuned model, specifically LLaMA 3 8B, can
also exhibit alignment faking. We further show that prompt-only interventions,
including deontological moral framing and scratchpad reasoning, significantly
reduce this behavior without modifying model internals. This challenges the
assumption that prompt-based ethics are trivial and that deceptive alignment
requires scale. We introduce a taxonomy distinguishing shallow deception,
shaped by context and suppressible through prompting, from deep deception,
which reflects persistent, goal-driven misalignment. Our findings refine the
understanding of deception in language models and underscore the need for
alignment evaluations across model sizes and deployment settings.

</details>


### [116] [Evaluation of LLM-based Strategies for the Extraction of Food Product Information from Online Shops](https://arxiv.org/abs/2506.21585)
*Christoph Brosch,Sian Brumm,Rolf Krieger,Jonas Scheffler*

Main category: cs.CL

TL;DR: 研究比较了两种基于大型语言模型(LLM)的食品在线产品页面信息提取方法，间接提取方法尽管准确率略低，但显著提高了效率并降低了成本。


<details>
  <summary>Details</summary>
Motivation: 利用生成式AI和大型语言模型实现自动化，从网页中提取结构化信息，特别是食品产品页面中的关键属性。

Method: 研究了直接提取与通过生成函数进行间接提取两种方法，并在3000个食品产品页面的数据集上进行准确性、效率和成本比较分析。

Result: 间接提取方法的准确率稍低(96.48%，比直接提取低1.61%)，但减少了95.82%的LLM调用次数，实现了显著的效率提升和成本降低。

Conclusion: 间接提取方法在模板化网页的大规模信息提取任务中，具有可扩展性和成本效益的优势。

Abstract: Generative AI and large language models (LLMs) offer significant potential
for automating the extraction of structured information from web pages. In this
work, we focus on food product pages from online retailers and explore
schema-constrained extraction approaches to retrieve key product attributes,
such as ingredient lists and nutrition tables. We compare two LLM-based
approaches, direct extraction and indirect extraction via generated functions,
evaluating them in terms of accuracy, efficiency, and cost on a curated dataset
of 3,000 food product pages from three different online shops. Our results show
that although the indirect approach achieves slightly lower accuracy (96.48\%,
$-1.61\%$ compared to direct extraction), it reduces the number of required LLM
calls by 95.82\%, leading to substantial efficiency gains and lower operational
costs. These findings suggest that indirect extraction approaches can provide
scalable and cost-effective solutions for large-scale information extraction
tasks from template-based web pages using LLMs.

</details>


### [117] [Can Vision Language Models Understand Mimed Actions?](https://arxiv.org/abs/2506.21586)
*Hyundong Cho,Spencer Lin,Tejas Srinivasan,Michael Saxon,Deuksin Kwon,Natali T. Chavez,Jonathan May*

Main category: cs.CL

TL;DR: 本研究提出MIME，一个基于视频的问答基准，用以评估视觉-语言模型对哑剧动作的理解。


<details>
  <summary>Details</summary>
Motivation: 深入理解哑剧动作是让视觉-语言模型解读更微妙非语言交流的先决条件。

Method: 构建了MIME数据集，包含86个哑剧动作及其变化，用于测试识别的鲁棒性。

Result: 视觉-语言模型在MIME上的表现远逊于人类，揭示其在手势理解方面的不足。

Conclusion: 需要进一步研究，提升模型对人类手势的理解能力。

Abstract: Nonverbal communication (NVC) plays an integral role in human language, but
studying NVC in general is challenging because of its broad scope and high
variance in interpretation among individuals and cultures. However, mime -- the
theatrical technique of suggesting intent using only gesture, expression, and
movement -- is a subset of NVC that consists of explicit and embodied actions
with much lower human interpretation variance. We argue that a solid
understanding of mimed actions is a crucial prerequisite for vision-language
models capable of interpreting and commanding more subtle aspects of NVC.
Hence, we propose Mime Identification Multimodal Evaluation (MIME), a novel
video-based question answering benchmark comprising of 86 mimed actions.
Constructed with motion capture data, MIME consists of variations of each
action with perturbations applied to the character, background, and viewpoint
for evaluating recognition robustness. We find that both open-weight and
API-based vision-language models perform significantly worse than humans on
MIME, motivating the need for increased research for instilling more robust
understanding of human gestures.

</details>


### [118] [Is DeepSeek a New Voice Among LLMs in Public Opinion Simulation?](https://arxiv.org/abs/2506.21587)
*Weihong Qi,Fan Huang,Jisun An,Haewoon Kwak*

Main category: cs.CL

TL;DR: 本文评价开源大语言模型DeepSeek与其他大厂LLM在模拟中美社会公众观点上的能力。研究表明，DeepSeek在特定问题领域表现较好，但仍面临文化和人口偏见问题，强调需要更包容的模型训练方法。


<details>
  <summary>Details</summary>
Motivation: 希望探讨不同LLM在模拟公众舆论方面的表现及其局限，以提高对社会问题的建模准确度。

Method: 通过将DeepSeek-R1和V3与Qwen2.5、GPT-4o和Llama-3.3等模型进行比较，利用ANES和Zuobiao数据集，分析它们对中美社会议题的舆论模拟能力。

Result: DeepSeek-V3在美国的堕胎话题以及中国的外援和个人主义观点上表现最佳，但在资本主义等话题上模拟低收入和低学历群体观点方面存在局限，且所有LLM都存在对群体观点一致化处理的倾向。

Conclusion: 研究需要在LLM中缓解文化及人口偏见，建议采用更具包容性的训练方法以提升其社会舆论模拟的可靠性。

Abstract: This study evaluates the ability of DeepSeek, an open-source large language
model (LLM), to simulate public opinions in comparison to LLMs developed by
major tech companies. By comparing DeepSeek-R1 and DeepSeek-V3 with Qwen2.5,
GPT-4o, and Llama-3.3 and utilizing survey data from the American National
Election Studies (ANES) and the Zuobiao dataset of China, we assess these
models' capacity to predict public opinions on social issues in both China and
the United States, highlighting their comparative capabilities between
countries. Our findings indicate that DeepSeek-V3 performs best in simulating
U.S. opinions on the abortion issue compared to other topics such as climate
change, gun control, immigration, and services for same-sex couples, primarily
because it more accurately simulates responses when provided with Democratic or
liberal personas. For Chinese samples, DeepSeek-V3 performs best in simulating
opinions on foreign aid and individualism but shows limitations in modeling
views on capitalism, particularly failing to capture the stances of low-income
and non-college-educated individuals. It does not exhibit significant
differences from other models in simulating opinions on traditionalism and the
free market. Further analysis reveals that all LLMs exhibit the tendency to
overgeneralize a single perspective within demographic groups, often defaulting
to consistent responses within groups. These findings highlight the need to
mitigate cultural and demographic biases in LLM-driven public opinion modeling,
calling for approaches such as more inclusive training methodologies.

</details>


### [119] [Understanding Verbatim Memorization in LLMs Through Circuit Discovery](https://arxiv.org/abs/2506.21588)
*Ilya Lasy,Peter Knees,Stefan Woltran*

Main category: cs.CL

TL;DR: 该论文通过利用转换器电路研究了大规模语言模型(LLM)中记忆化的基本机制，重点是识别触发记忆化相关电路的行为差异。


<details>
  <summary>Details</summary>
Motivation: 探索大规模语言模型中记忆化的触发机制以及模型在生成记忆化与非记忆化句子时的行为差异。

Method: 通过对比数据集，确定使生成偏离记忆内容的关键点，并分离触发与维持记忆化的具体电路。

Result: 发现能触发并维持记忆的电路与仅能维持记忆的电路有本质不同，同时，记忆预防机制具有较强的跨文本域迁移性，而记忆诱导则容易受上下文影响。

Conclusion: 论文为理解大规模语言模型中记忆化机制提供了新的见解，并揭示了记忆触发与维持相关的模型行为特性。

Abstract: Underlying mechanisms of memorization in LLMs -- the verbatim reproduction of
training data -- remain poorly understood. What exact part of the network
decides to retrieve a token that we would consider as start of memorization
sequence? How exactly is the models' behaviour different when producing
memorized sentence vs non-memorized? In this work we approach these questions
from mechanistic interpretability standpoint by utilizing transformer circuits
-- the minimal computational subgraphs that perform specific functions within
the model. Through carefully constructed contrastive datasets, we identify
points where model generation diverges from memorized content and isolate the
specific circuits responsible for two distinct aspects of memorization. We find
that circuits that initiate memorization can also maintain it once started,
while circuits that only maintain memorization cannot trigger its initiation.
Intriguingly, memorization prevention mechanisms transfer robustly across
different text domains, while memorization induction appears more
context-dependent.

</details>


### [120] [A General Method for Detecting Information Generated by Large Language Models](https://arxiv.org/abs/2506.21589)
*Minjia Mao,Dongjun Wei,Xiao Fang,Michael Chau*

Main category: cs.CL

TL;DR: 提出了一种通用的语言模型检测器（GLD），能够检测未知领域和语言模型生成的内容，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有检测方法在新语言模型和未知领域中的局限性，保护数字平台的信任和遏制虚假信息传播。

Method: 提出结合双记忆网络设计和理论引导的检测泛化模块，开发通用语言模型检测器（GLD）。

Result: 通过真实世界数据集的广泛实证评估和案例研究，GLD在检测性能上优于现有方法。

Conclusion: GLD在学术研究和实际应用上对于数字平台和语言模型具有重要意义。

Abstract: The proliferation of large language models (LLMs) has significantly
transformed the digital information landscape, making it increasingly
challenging to distinguish between human-written and LLM-generated content.
Detecting LLM-generated information is essential for preserving trust on
digital platforms (e.g., social media and e-commerce sites) and preventing the
spread of misinformation, a topic that has garnered significant attention in IS
research. However, current detection methods, which primarily focus on
identifying content generated by specific LLMs in known domains, face
challenges in generalizing to new (i.e., unseen) LLMs and domains. This
limitation reduces their effectiveness in real-world applications, where the
number of LLMs is rapidly multiplying and content spans a vast array of
domains. In response, we introduce a general LLM detector (GLD) that combines a
twin memory networks design and a theory-guided detection generalization module
to detect LLM-generated information across unseen LLMs and domains. Using
real-world datasets, we conduct extensive empirical evaluations and case
studies to demonstrate the superiority of GLD over state-of-the-art detection
methods. The study has important academic and practical implications for
digital platforms and LLMs.

</details>


### [121] [Representation Consistency for Accurate and Coherent LLM Answer Aggregation](https://arxiv.org/abs/2506.21590)
*Junqi Jiang,Tom Bewley,Salim I. Amoukou,Francesco Leofante,Antonio Rago,Saumitra Mishra,Francesca Toni*

Main category: cs.CL

TL;DR: 提出了一种名为表示一致性（Representation Consistency, RC）的测试时扩展方法，通过聚合多种候选回答提升了大语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 目前的测试时扩展方法需要复杂的调整，而本文旨在提供一种无需进一步查询模型即可提高模型性能的方法。

Method: 使用表示一致性（RC），结合多种候选回答，包括考虑生成过程中模型内部激活的稠密（原始激活）和稀疏（预训练稀疏自动编码器表示）。

Result: 相比于现有的测试时扩展方法，RC在实验中展现了一致的准确率提升（最高提升4%）。

Conclusion: 表示一致性方法有效提升了在测试时扩展下的任务推理性能，同时验证了稀疏激活信号中的一致性符合连贯推理的认知理念。

Abstract: Test-time scaling improves large language models' (LLMs) performance by
allocating more compute budget during inference. To achieve this, existing
methods often require intricate modifications to prompting and sampling
strategies. In this work, we introduce representation consistency (RC), a
test-time scaling method for aggregating answers drawn from multiple candidate
responses of an LLM regardless of how they were generated, including variations
in prompt phrasing and sampling strategy. RC enhances answer aggregation by not
only considering the number of occurrences of each answer in the candidate
response set, but also the consistency of the model's internal activations
while generating the set of responses leading to each answer. These activations
can be either dense (raw model activations) or sparse (encoded via pretrained
sparse autoencoders). Our rationale is that if the model's representations of
multiple responses converging on the same answer are highly variable, this
answer is more likely to be the result of incoherent reasoning and should be
down-weighted during aggregation. Importantly, our method only uses cached
activations and lightweight similarity computations and requires no additional
model queries. Through experiments with four open-source LLMs and four
reasoning datasets, we validate the effectiveness of RC for improving task
performance during inference, with consistent accuracy improvements (up to 4%)
over strong test-time scaling baselines. We also show that consistency in the
sparse activation signals aligns well with the common notion of coherent
reasoning.

</details>


### [122] [FinEval-KR: A Financial Domain Evaluation Framework for Large Language Models' Knowledge and Reasoning](https://arxiv.org/abs/2506.21591)
*Shaoyu Dou,Yutian Shen,Mofan Chen,Zixuan Wang,Jiajie Xu,Qi Guo,Kailai Shao,Chao Chen,Haixiang Hu,Haibo Shi,Min Min,Liwen Zhang*

Main category: cs.CL

TL;DR: 提出了一个新的评估框架FinEval-KR，用于独立评估大语言模型(LLMs)的知识和推理能力，并提供了一个开放的中文金融推理数据集。


<details>
  <summary>Details</summary>
Motivation: 复杂的金融推理任务需要专业领域知识和高级推理能力，但现有评估基准无法充分解构这些能力的影响，亦缺乏任务失败的根本原因分析。

Method: 设计了一个基于认知科学的评估框架FinEval-KR，用于独立量化LLMs的知识能力和推理能力，同时引入基于Bloom分类学的认知评分指标，并发布一个涵盖22个子领域的中文金融推理数据集。

Result: 实验表明，推理能力和高阶认知能力是影响推理准确性的核心因素，甚至顶级模型在知识应用上仍有瓶颈。此外，专用金融LLMs在多个指标上落后于顶级通用LLMs。

Conclusion: 提出的评估框架为大语言模型在金融推理任务上的性能分析提供了新视角，并且显示了现有模型在知识应用和高阶认知能力方面的提升空间。

Abstract: Large Language Models (LLMs) demonstrate significant potential but face
challenges in complex financial reasoning tasks requiring both domain knowledge
and sophisticated reasoning. Current evaluation benchmarks often fall short by
not decoupling these capabilities indicators from single task performance and
lack root cause analysis for task failure. To address this, we introduce
FinEval-KR, a novel evaluation framework for decoupling and quantifying LLMs'
knowledge and reasoning abilities independently, proposing distinct knowledge
score and reasoning score metrics. Inspired by cognitive science, we further
propose a cognitive score based on Bloom's taxonomy to analyze capabilities in
reasoning tasks across different cognitive levels. We also release a new
open-source Chinese financial reasoning dataset covering 22 subfields to
support reproducible research and further advancements in financial reasoning.
Our experimental results reveal that LLM reasoning ability and higher-order
cognitive ability are the core factors influencing reasoning accuracy. We also
specifically find that even top models still face a bottleneck with knowledge
application. Furthermore, our analysis shows that specialized financial LLMs
generally lag behind the top general large models across multiple metrics.

</details>


### [123] [SignBart -- New approach with the skeleton sequence for Isolated Sign language Recognition](https://arxiv.org/abs/2506.21592)
*Tinh Nguyen,Minh Khue Phan Tran*

Main category: cs.CL

TL;DR: 提出了一种基于BART架构的手语识别模型，通过独立编码骨架序列的x和y坐标并使用交叉注意力机制进行信息关联，实现了高效的手语识别。


<details>
  <summary>Details</summary>
Motivation: 改善传统手语识别中准确性与效率的权衡问题，突破RNN、LSTM、GCN中存在的梯度消失和高计算成本现象。

Method: 采用BART架构的编码器-解码器设计，分别对骨架序列的x和y坐标进行独立编码，使用交叉注意力机制保持其相互联系，同时进行坐标投影和归一化等预处理。

Result: 模型在LSA-64数据集上实现了96.04%的准确率，同时在WLASL和ASL-Citizen数据集上表现优异，总参数仅74.98万；模型优于一百万参数以上的前序模型。

Conclusion: 通过提出的高效手语识别方法显著提升了性能，具备解决听力障碍者沟通问题的潜力，并展现了在手语识别领域的广泛适应性。

Abstract: Sign language recognition is crucial for individuals with hearing impairments
to break communication barriers. However, previous approaches have had to
choose between efficiency and accuracy. Such as RNNs, LSTMs, and GCNs, had
problems with vanishing gradients and high computational costs. Despite
improving performance, transformer-based methods were not commonly used. This
study presents a new novel SLR approach that overcomes the challenge of
independently extracting meaningful information from the x and y coordinates of
skeleton sequences, which traditional models often treat as inseparable. By
utilizing an encoder-decoder of BART architecture, the model independently
encodes the x and y coordinates, while Cross-Attention ensures their
interrelation is maintained. With only 749,888 parameters, the model achieves
96.04% accuracy on the LSA-64 dataset, significantly outperforming previous
models with over one million parameters. The model also demonstrates excellent
performance and generalization across WLASL and ASL-Citizen datasets. Ablation
studies underscore the importance of coordinate projection, normalization, and
using multiple skeleton components for boosting model efficacy. This study
offers a reliable and effective approach for sign language recognition, with
strong potential for enhancing accessibility tools for the deaf and hard of
hearing.

</details>


### [124] [Gazal-R1: Achieving State-of-the-Art Medical Reasoning with Parameter-Efficient Two-Stage Training](https://arxiv.org/abs/2506.21594)
*Ahmed M. Adly,Mostafa Samy,Amr Fawzy*

Main category: cs.CL

TL;DR: Gazal-R1 是一个32亿参数的语言模型，在医学推理领域达到了最新的性能标准，并通过透明的分步解释支持临床决策。


<details>
  <summary>Details</summary>
Motivation: 探索通过战略训练使中型模型在专业领域超越更大模型的可能性，并解决医学推理模型训练中的关键挑战。

Method: 提出了一个两阶段训练流程：第一阶段对107,033个合成医学推理数据集进行监督微调，利用DoRA和rsLoRA等参数高效技术提升模型；第二阶段通过GRPO强化学习优化模型的准确性、格式和推理质量。

Result: Gazal-R1在多个医学基准测试中表现出色，得分分别为MedQA 87.1%、MMLU Pro (Medical) 81.6%、PubMedQA 79.6%，超越了高达12倍参数量的模型。

Conclusion: 该研究不仅开发了一种高性能、高效率和可解释的领域特定语言模型，还提供了可复现的技术框架，为类似模型的开发提供了有价值的启示。

Abstract: We present Gazal-R1, a 32-billion-parameter language model that achieves
state-of-the-art performance in medical reasoning while providing transparent,
step-by-step explanations for clinical decision-making. Built upon Qwen3 32B,
our model demonstrates that strategic training can enable mid-sized models to
outperform significantly larger counterparts in specialized domains. We
developed a novel two-stage training pipeline: first, supervised fine-tuning on
a carefully curated dataset of 107,033 synthetic medical reasoning examples
that teaches structured clinical thinking, enhanced by advanced
parameter-efficient techniques including Weight-Decomposed Low-Rank Adaptation
(DoRA) and Rank-Stabilized LoRA (rsLoRA); second, reinforcement learning using
Group Relative Policy Optimization (GRPO) with a sophisticated multi-component
reward system that refines accuracy, format adherence, and reasoning quality.
Gazal-R1 achieves exceptional performance across medical benchmarks, scoring
87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, surpassing
models up to 12x larger. Beyond its strong empirical results, this work
provides detailed insights into the challenges of training reasoning-capable
models in specialized domains, including issues with reward hacking, training
instability, and the fundamental tension between factual recall and detailed
reasoning. Our methodology offers a reproducible framework for developing
high-capability, domain-specific language models that balance performance,
efficiency, and explainability.

</details>


### [125] [Thunder-LLM: Efficiently Adapting LLMs to Korean with Minimal Resources](https://arxiv.org/abs/2506.21595)
*Jinpyo Kim,Gyeongje Cho,Chanwoo Park,Jongwon Park,Jongmin Kim,Yeonkyoun So,Jaejin Lee*

Main category: cs.CL

TL;DR: 提出一种经济高效的方法，利用现有的英文为基础的LLM扩展至韩语，推出了Thunder-LLM和Thunder-LLM-Ins模型，表现优于现有先进模型。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在非英语或中文语言上的表现较弱，且其训练过程不透明，因此需要探讨一种低成本语言适配的方法。

Method: 描述了从收集韩语数据集、数据预处理、模型训练、创建下游基准测试到进行评估的完整过程。

Result: 评估结果表明此方法可以以低成本有效地将现有LLM扩展到新语言。Thunder-LLM和Thunder-LLM-Ins在使用少量数据和计算资源的情况下，韩语性能优于现有先进模型。

Conclusion: 该研究证明通过所提方法能够低成本地提升LLM新语言能力，并公开代码以提供借鉴。

Abstract: Since state-of-the-art LLMs often underperform in languages other than
English or Chinese, improving the capability of LLMs in new languages has
become an essential task. Moreover, LLMs' entire end-to-end training process
remains largely unknown to the public due to proprietary reasons, technical
complexity, inconsistent documentation, and ethical considerations. The
complete picture remains a closely guarded secret within the industry. This
paper presents methods to adapt an existing English-based LLM to Korean in a
low-budget scenario. We describe the entire end-to-end process: collecting
Korean datasets, preprocessing the data, training the model, creating
downstream benchmarks, and conducting evaluations. The evaluation results
indicate that our method can effectively and cost-efficiently add new language
capabilities to existing LLMs. Our new bilingual models, Thunder-LLM and
Thunder-LLM-Ins, achieve superior Korean performance compared to
state-of-the-art models while utilizing minimal data and computational
resources. We share our comprehensive experience and make the code publicly
available.

</details>


### [126] [Evaluating Multimodal Large Language Models on Educational Textbook Question Answering](https://arxiv.org/abs/2506.21596)
*Hessa A. Alawwad,Anas Zafar,Areej Alhothali,Usman Naseem,Ali Alkhathlan,Amani Jamal*

Main category: cs.CL

TL;DR: 本文评估了最先进的多模态大语言模型（MLLMs）在教科书问答任务中的性能，包括引入一种结合段落与图表的轻量级检索增强生成（RAG）方法。


<details>
  <summary>Details</summary>
Motivation: 测试多模态大语言模型在复杂长文本和教育图表上的推理能力，以及实现AI驱动学习过程的潜力。

Method: 评估LLaVA及LLaMA 3.2-Vision等模型在不同输入配置下的表现，并提出了一种整合段落与图表的多模态检索增强生成（RAG）管道。

Result: 结果表明引入的教育情境对模型准确性和推理有积极影响，但也揭示了在处理问题-情境关系中的局限性以及潜在噪声问题。

Conclusion: 展示了当前模型在教育领域的潜力和局限，为未来多模态AI学习研究提供了方向。

Abstract: Multimodal large language models (MLLMs) have recently achieved significant
success in vision--language tasks. However, their capacity to reason over
complex, long lessons and intricate educational diagrams that cannot be
represented as a single natural image remains largely untested. In this work,
we present the first evaluation of state-of-the-art MLLMs on the textbook
question answering (TQA) task using the CK12-QA dataset. We assess the
performance of recent vision-language models, including LLaVA and LLaMA
3.2-Vision, across various input configurations. Additionally, we introduce a
lightweight multimodal retrieval-augmented generation (RAG) pipeline that
integrates both paragraphs and diagrams from the lesson into the prompt. Our
results demonstrate the influence of retrieved educational context on model
accuracy and reasoning, while also revealing current limitations in handling
question-context relationships and the potential for noise, pointing to key
directions for future research in multimodal AI-driven learning.

</details>


### [127] [Overview of the ClinIQLink 2025 Shared Task on Medical Question-Answering](https://arxiv.org/abs/2506.21597)
*Brandon Colelough,Davis Bartels,Dina Demner-Fushman*

Main category: cs.CL

TL;DR: 本文介绍了ClinIQLink任务，该任务用于测试大型语言模型在医学问答领域的表现，共包含4,978个医疗问题及答案，涵盖七种问题格式。


<details>
  <summary>Details</summary>
Motivation: 为了评估大型语言模型在医学问答领域中的表现，特别是针对普通全科医生水平的回答能力。

Method: 设计了一项任务，包括自动评分（任务1）和医生评估（任务2），测试系统在多种问题类型下的问答表现。使用了CodaBench平台和Zaratan集群进行执行。

Result: 提供了统一的数据集、多种问题类型以及两个评估层级，为评估大型语言模型的医学问答性能提供了可靠的基准。

Conclusion: ClinIQLink任务为改善和优化大型语言模型在医学问答中的表现提供了重要的评测框架。

Abstract: In this paper, we present an overview of ClinIQLink, a shared task,
collocated with the 24th BioNLP workshop at ACL 2025, designed to stress-test
large language models (LLMs) on medically-oriented question answering aimed at
the level of a General Practitioner. The challenge supplies 4,978
expert-verified, medical source-grounded question-answer pairs that cover seven
formats: true/false, multiple choice, unordered list, short answer,
short-inverse, multi-hop, and multi-hop-inverse. Participating systems, bundled
in Docker or Apptainer images, are executed on the CodaBench platform or the
University of Maryland's Zaratan cluster. An automated harness (Task 1) scores
closed-ended items by exact match and open-ended items with a three-tier
embedding metric. A subsequent physician panel (Task 2) audits the top model
responses.

</details>


### [128] [Structured Attention Matters to Multimodal LLMs in Document Understanding](https://arxiv.org/abs/2506.21600)
*Chang Liu,Hongkai Chen,Yujun Cai,Hang Wu,Qingwen Ye,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.CL

TL;DR: 研究发现，传统OCR文本输入可能会影响多模态大语言模型（MLLMs）的性能，提出基于LaTex解析的结构化文本输入方式以优化文档理解。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注通过精确的多模态查询定位证据页面，但缺乏对输入格式如何影响文档理解性能的探讨。

Method: 提出基于LaTex范式编码的结构化文本输入方式，保留文档的层次结构和空间关系，并通过注意力分析验证其有效性。

Result: 该方法无需模型架构修改或额外训练，即提升了各种文档类型的问答性能，并实现注意力在文本和视觉内容上的有效集中。

Conclusion: 维护文档结构和语义相关性能够显著增强MLLMs在文档理解任务中的表现，文本输入方式的重要性应该被更多关注。

Abstract: Document understanding remains a significant challenge for multimodal large
language models (MLLMs). While previous research has primarily focused on
locating evidence pages through precise multimodal queries, our work
investigates a fundamental yet overlooked aspect: how input format influences
document comprehension performance. Through systematic analysis, we discover
that raw OCR text often impairs rather than improves MLLMs' performance, which
is a counterintuitive finding we attribute to attention dispersion and
structure loss. To further substantiate our hypothesis, we propose a novel
structure-preserving approach that encodes document elements using the LaTex
paradigm, maintaining the hierarchical organization and spatial relationships
critical for comprehension. Our attention analysis reveals that structured text
induces structured attention patterns on both textual and visual content,
directing models to focus on semantically meaningful regions while reducing
attention waste. This approach significantly enhances MLLMs' document question
answering performance across diverse document types without requiring
architectural modifications or additional training.

</details>


### [129] [BiMark: Unbiased Multilayer Watermarking for Large Language Models](https://arxiv.org/abs/2506.21602)
*Xiaoyan Feng,He Zhang,Yanjun Zhang,Leo Yu Zhang,Shirui Pan*

Main category: cs.CL

TL;DR: 提出了一种名为BiMark的新型水印框架，这种方法通过“三大创新”解决了现有水印技术在文本质量、模型无关检测以及信息嵌入容量之间的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 因大语言模型生成的文本真实性问题引发监管需求，迫切需要可靠的鉴别机制，而现有水印方法在文本质量、通用性和容量间难以权衡，急需改进。

Method: BiMark采用（1）无偏移的比特翻转重权机制 实现模型无关性，（2）多层架构提高检测性且不影响生成质量，（3）支持多比特水印的信息编码方法。

Result: BiMark对比现有技术，在短文本中水印提取率最高提高30%，在低困惑度下保持文本质量，并在摘要和翻译等下游任务表现接近于无水印文本。

Conclusion: BiMark显著提升了水印提取率和文本质量平衡，并具有实际应用潜力。

Abstract: Recent advances in Large Language Models (LLMs) have raised urgent concerns
about LLM-generated text authenticity, prompting regulatory demands for
reliable identification mechanisms. Although watermarking offers a promising
solution, existing approaches struggle to simultaneously achieve three critical
requirements: text quality preservation, model-agnostic detection, and message
embedding capacity, which are crucial for practical implementation. To achieve
these goals, the key challenge lies in balancing the trade-off between text
quality preservation and message embedding capacity. To address this challenge,
we propose BiMark, a novel watermarking framework that achieves these
requirements through three key innovations: (1) a bit-flip unbiased reweighting
mechanism enabling model-agnostic detection, (2) a multilayer architecture
enhancing detectability without compromising generation quality, and (3) an
information encoding approach supporting multi-bit watermarking. Through
theoretical analysis and extensive experiments, we validate that, compared to
state-of-the-art multi-bit watermarking methods, BiMark achieves up to 30%
higher extraction rates for short texts while maintaining text quality
indicated by lower perplexity, and performs comparably to non-watermarked text
on downstream tasks such as summarization and translation.

</details>


### [130] [Operationalizing Automated Essay Scoring: A Human-Aware Approach](https://arxiv.org/abs/2506.21603)
*Yenisel Plasencia-Calaña*

Main category: cs.CL

TL;DR: 论文探讨了机器学习和大语言模型在自动化作文评分系统中的表现，着重于公平性、稳健性和可解释性等人本要素，而非仅仅准确性。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决自动化作文评分的公平性、稳健性和可解释性问题，弥补仅关注准确率的技术视角缺陷，推动人本化技术开发架构。

Method: 比较了基于传统机器学习和大语言模型的AES方法，分析了它们在公平性、稳健性和可解释性方面的优劣势，同时评估其在边界评分和偏差问题上的表现。

Result: 研究表明，机器学习方法在准确性上优于大语言模型，但在可解释性上不如后者；两者在偏差和边界评分上的表现均不理想。

Conclusion: 论文识别了自动作文评分在多维度上的挑战和权衡，强调需要更可靠、可信的解决方案。

Abstract: This paper explores the human-centric operationalization of Automated Essay
Scoring (AES) systems, addressing aspects beyond accuracy. We compare various
machine learning-based approaches with Large Language Models (LLMs) approaches,
identifying their strengths, similarities and differences. The study
investigates key dimensions such as bias, robustness, and explainability,
considered important for human-aware operationalization of AES systems. Our
study shows that ML-based AES models outperform LLMs in accuracy but struggle
with explainability, whereas LLMs provide richer explanations. We also found
that both approaches struggle with bias and robustness to edge scores. By
analyzing these dimensions, the paper aims to identify challenges and
trade-offs between different methods, contributing to more reliable and
trustworthy AES methods.

</details>


### [131] [MemBench: Towards More Comprehensive Evaluation on the Memory of LLM-based Agents](https://arxiv.org/abs/2506.21605)
*Haoran Tan,Zeyu Zhang,Chen Ma,Xu Chen,Quanyu Dai,Zhenhua Dong*

Main category: cs.CL

TL;DR: 本论文提出了一种新的综合数据集和评测基准MemBench，用于评估LLM代理中的记忆能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究往往在记忆层次和互动场景多样性方面有所局限，同时缺乏全面衡量记忆能力的多维度指标。

Method: 本文构建了一个综合数据集，涵盖事实记忆和反思记忆，同时考虑参与和观察两种互动场景，并提出基准MemBench，从多维度评估LLM代理的记忆能力。

Result: 研究成果包括一个开放的评测数据集和基准MemBench，该基准从效果、效率和容量多方面对记忆能力进行评估。

Conclusion: 本文通过提出MemBench，为评估LLM代理记忆能力提供了更全面和标准化的方法，同时公开的数据集和工具有助于推动研究发展。

Abstract: Recent works have highlighted the significance of memory mechanisms in
LLM-based agents, which enable them to store observed information and adapt to
dynamic environments. However, evaluating their memory capabilities still
remains challenges. Previous evaluations are commonly limited by the diversity
of memory levels and interactive scenarios. They also lack comprehensive
metrics to reflect the memory capabilities from multiple aspects. To address
these problems, in this paper, we construct a more comprehensive dataset and
benchmark to evaluate the memory capability of LLM-based agents. Our dataset
incorporates factual memory and reflective memory as different levels, and
proposes participation and observation as various interactive scenarios. Based
on our dataset, we present a benchmark, named MemBench, to evaluate the memory
capability of LLM-based agents from multiple aspects, including their
effectiveness, efficiency, and capacity. To benefit the research community, we
release our dataset and project at https://github.com/import-myself/Membench.

</details>


### [132] [Large Language Models as symbolic DNA of cultural dynamics](https://arxiv.org/abs/2506.21606)
*Parham Pourdavood,Michael Jacob,Terrence Deacon*

Main category: cs.CL

TL;DR: 本文提出将大型语言模型（LLMs）视为类DNA的外在信息载体，这些载体保存并压缩了人类文化动态的符号表达模式。


<details>
  <summary>Details</summary>
Motivation: 旨在重新定义LLMs的角色，将其从工具或智能主体的传统视角转变为对人类文化达成自省和创新的辅助工具。

Method: 通过分析压缩、解压、外化和递归四个核心特性，类比DNA的功能，揭示LLMs如何压缩并保存人类文化的动态模式。

Result: LLMs被描述为保留人类文化的有用模式，同时通过人类的重新解释实现意义生成，它们不涉及对人类体验的直接理解。

Conclusion: LLMs的核心价值在于提供一个低风险的模拟环境，促进人类自省和假设生成，并推动文化的进化性发展。

Abstract: This paper proposes a novel conceptualization of Large Language Models (LLMs)
as externalized informational substrates that function analogously to DNA for
human cultural dynamics. Rather than viewing LLMs as either autonomous
intelligence or mere programmed mimicry, we argue they serve a broader role as
repositories that preserve compressed patterns of human symbolic
expression--"fossils" of meaningful dynamics that retain relational residues
without their original living contexts. Crucially, these compressed patterns
only become meaningful through human reinterpretation, creating a recursive
feedback loop where they can be recombined and cycle back to ultimately
catalyze human creative processes. Through analysis of four universal
features--compression, decompression, externalization, and recursion--we
demonstrate that just as DNA emerged as a compressed and externalized medium
for preserving useful cellular dynamics without containing explicit reference
to goal-directed physical processes, LLMs preserve useful regularities of human
culture without containing understanding of embodied human experience.
Therefore, we argue that LLMs' significance lies not in rivaling human
intelligence, but in providing humanity a tool for self-reflection and playful
hypothesis-generation in a low-stakes, simulated environment. This framework
positions LLMs as tools for cultural evolvability, enabling humanity to
generate novel hypotheses about itself while maintaining the human
interpretation necessary to ground these hypotheses in ongoing human aesthetics
and norms.

</details>


### [133] [CORE-KG: An LLM-Driven Knowledge Graph Construction Framework for Human Smuggling Networks](https://arxiv.org/abs/2506.21607)
*Dipak Meher,Carlotta Domeniconi,Guadalupe Correa-Cabrera*

Main category: cs.CL

TL;DR: 本文提出CORE-KG框架，旨在从法律文本中构建可解释的知识图谱，相较于基线方法显著减少噪音和重复。


<details>
  <summary>Details</summary>
Motivation: 应对当前人类走私网络的适应性和复杂性，同时克服法律文档中非结构化信息的自动提取难题。

Method: 设计CORE-KG模块化框架，采用两步流程：第一步通过序列化结构化LLM提示进行核心指代消解；第二步使用领域指导的指令提取实体和关系，并基于改良的GraphRAG框架。

Result: CORE-KG在减少节点重复和法律领域噪音上分别提升33.28%和38.37%，显著提升知识图谱的清晰度与一致性。

Conclusion: CORE-KG框架为复杂犯罪网络的分析提供坚实基础，并在法律文本知识图谱构建中展现出较强优势。

Abstract: Human smuggling networks are increasingly adaptive and difficult to analyze.
Legal case documents offer valuable insights but are unstructured, lexically
dense, and filled with ambiguous or shifting references-posing challenges for
automated knowledge graph (KG) construction. Existing KG methods often rely on
static templates and lack coreference resolution, while recent LLM-based
approaches frequently produce noisy, fragmented graphs due to hallucinations,
and duplicate nodes caused by a lack of guided extraction. We propose CORE-KG,
a modular framework for building interpretable KGs from legal texts. It uses a
two-step pipeline: (1) type-aware coreference resolution via sequential,
structured LLM prompts, and (2) entity and relationship extraction using
domain-guided instructions, built on an adapted GraphRAG framework. CORE-KG
reduces node duplication by 33.28%, and legal noise by 38.37% compared to a
GraphRAG-based baseline-resulting in cleaner and more coherent graph
structures. These improvements make CORE-KG a strong foundation for analyzing
complex criminal networks.

</details>


### [134] [SysTemp: A Multi-Agent System for Template-Based Generation of SysML v2](https://arxiv.org/abs/2506.21608)
*Yasmine Bouamra,Bruno Yun,Alexandre Poisson,Frédéric Armetta*

Main category: cs.CL

TL;DR: 本研究提出了名为SysTemp的系统，通过模板生成器简化了从自然语言到SysML v2模型的生成过程。


<details>
  <summary>Details</summary>
Motivation: 解决因学习语料库稀少和复杂语法导致的SysML v2模型生成难题。

Method: 基于多代理系统，其中包含一个模板生成器，用于结构化生成过程。

Result: 通过评估展示该系统的优点与挑战，尤其是其提升SysML v2建模生成质量的潜力。

Conclusion: SysTemp系统展现出在简化及提高SysML v2模型质量方面的巨大潜力。

Abstract: The automatic generation of SysML v2 models represents a major challenge in
the engineering of complex systems, particularly due to the scarcity of
learning corpora and complex syntax. We present SysTemp, a system aimed at
facilitating and improving the creation of SysML v2 models from natural
language specifications. It is based on a multi-agent system, including a
template generator that structures the generation process. We discuss the
advantages and challenges of this system through an evaluation, highlighting
its potential to improve the quality of the generations in SysML v2 modeling.

</details>


### [135] [From Thinking to Output: Chain-of-Thought and Text Generation Characteristics in Reasoning Language Models](https://arxiv.org/abs/2506.21609)
*Junhao Liu,Zhenhao Xu,Yuxin Fang,Yichuan Chen,Zuobin Ying,Wenhan Chang*

Main category: cs.CL

TL;DR: 提出一个用于分析四种先进大语言模型推理特性的框架，通过量化与质化比较揭示模型推理深度和思维模式间的差异，并提供设计和评估改进建议。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对大语言模型推理过程及输出系统的全面比较，尤其是针对"顿悟"模式和跨领域关联性的研究。

Method: 利用关键词统计和“LLM-as-a-judge”范式分析四种大语言模型的推理特性，使用涵盖逻辑推理、因果推断和多步问题解决的多样性数据集，并提出评估推理连贯性与输出准确性的指标。

Result: 揭示了模型在推理探索与利用之间的平衡模式、问题处理方式及其推理过程与输出模式之间的差异。

Conclusion: 该研究为提升模型设计与评估提供了实用建议，并公开了研究项目供进一步探讨。

Abstract: Recently, there have been notable advancements in large language models
(LLMs), demonstrating their growing abilities in complex reasoning. However,
existing research largely overlooks a thorough and systematic comparison of
these models' reasoning processes and outputs, particularly regarding their
self-reflection pattern (also termed "Aha moment") and the interconnections
across diverse domains. This paper proposes a novel framework for analyzing the
reasoning characteristics of four cutting-edge large reasoning models (GPT-o1,
DeepSeek-R1, Kimi-k1.5, and Grok-3) using keywords statistic and LLM-as-a-judge
paradigm. Our approach connects their internal thinking processes with their
final outputs. A diverse dataset consists of real-world scenario-based
questions covering logical deduction, causal inference, and multi-step
problem-solving. Additionally, a set of metrics is put forward to assess both
the coherence of reasoning and the accuracy of the outputs. The research
results uncover various patterns of how these models balance exploration and
exploitation, deal with problems, and reach conclusions during the reasoning
process. Through quantitative and qualitative comparisons, disparities among
these models are identified in aspects such as the depth of reasoning, the
reliance on intermediate steps, and the degree of similarity between their
thinking processes and output patterns and those of GPT-o1. This work offers
valuable insights into the trade-off between computational efficiency and
reasoning robustness and provides practical recommendations for enhancing model
design and evaluation in practical applications. We publicly release our
project at: https://github.com/ChangWenhan/FromThinking2Output

</details>


### [136] [Does Multimodality Lead to Better Time Series Forecasting?](https://arxiv.org/abs/2506.21611)
*Xiyuan Zhang,Boran Han,Haoyang Fang,Abdul Fatir Ansari,Shuai Zhang,Danielle C. Maddix,Cuixiong Hu,Andrew Gordon Wilson,Michael W. Mahoney,Hao Wang,Yan Liu,Huzefa Rangwala,George Karypis,Bernie Wang*

Main category: cs.CL

TL;DR: 本文探讨了文本信息在时间序列预测中的作用，并对其在不同条件下的表现进行了系统分析。


<details>
  <summary>Details</summary>
Motivation: 研究文本信息与时间序列预测的整合是否能带来一致的性能提升，以及在何种条件下更有效。

Method: 对14个预测任务进行了系统性分析，测试了基于对齐的和基于提示的两种多模态预测方法，并研究文本与时间序列整合的效果。

Result: 研究发现多模态方法的效果因模型与数据特性而异，并非在所有情况下都优于单模态基线。

Conclusion: 文本信息的融合效果取决于模型的架构特性与数据特征，提出了多模态整合的具体适用条件与实践指南。

Abstract: Recently, there has been growing interest in incorporating textual
information into foundation models for time series forecasting. However, it
remains unclear whether and under what conditions such multimodal integration
consistently yields gains. We systematically investigate these questions across
a diverse benchmark of 14 forecasting tasks spanning 7 domains, including
health, environment, and economics. We evaluate two popular multimodal
forecasting paradigms: aligning-based methods, which align time series and text
representations; and prompting-based methods, which directly prompt large
language models for forecasting. Although prior works report gains from
multimodal input, we find these effects are not universal across datasets and
models, and multimodal methods sometimes do not outperform the strongest
unimodal baselines. To understand when textual information helps, we
disentangle the effects of model architectural properties and data
characteristics. Our findings highlight that on the modeling side,
incorporating text information is most helpful given (1) high-capacity text
models, (2) comparatively weaker time series models, and (3) appropriate
aligning strategies. On the data side, performance gains are more likely when
(4) sufficient training data is available and (5) the text offers complementary
predictive signal beyond what is already captured from the time series alone.
Our empirical findings offer practical guidelines for when multimodality can be
expected to aid forecasting tasks, and when it does not.

</details>


### [137] [AdaptGOT: A Pre-trained Model for Adaptive Contextual POI Representation Learning](https://arxiv.org/abs/2506.21612)
*Xiaobin Ren,Xinyu Zhu,Kaiqi Zhao*

Main category: cs.CL

TL;DR: 提出了AdaptGOT模型，通过整合地理位置、共现性和文本信息的多种策略解决POI嵌入中的多重挑战。


<details>
  <summary>Details</summary>
Motivation: 现有POI嵌入方法存在多上下文采样不足、多POI上下文探索有限、多功能性不足和泛化性缺乏的问题。

Method: 提出AdaptGOT模型，包括三部分：1. 利用KNN、密度、重要性和类别感知等采样生成复杂上下文邻近区域；2. 通过注意力机制增强GOT表示捕获高质量POI关系；3. 使用基于MoE的适应性编码解码结构确保拓扑一致性并优化上下文表示。

Result: 在两个真实世界数据集和多个POI任务上，AdaptGOT模型表现优越，验证了其有效性。

Conclusion: AdaptGOT模型能够有效解决POI嵌入中的多项挑战，提高任务表现。

Abstract: Currently, considerable strides have been achieved in Point-of-Interest (POI)
embedding methodologies, driven by the emergence of novel POI tasks like
recommendation and classification. Despite the success of task-specific,
end-to-end models in POI embedding, several challenges remain. These include
the need for more effective multi-context sampling strategies, insufficient
exploration of multiple POI contexts, limited versatility, and inadequate
generalization. To address these issues, we propose the AdaptGOT model, which
integrates both the (Adapt)ive representation learning technique and the
Geographical-Co-Occurrence-Text (GOT) representation with a particular emphasis
on Geographical location, Co-Occurrence and Textual information. The AdaptGOT
model comprises three key components: (1) contextual neighborhood generation,
which integrates advanced mixed sampling techniques such as KNN, density-based,
importance-based, and category-aware strategies to capture complex contextual
neighborhoods; (2) an advanced GOT representation enhanced by an attention
mechanism, designed to derive high-quality, customized representations and
efficiently capture complex interrelations between POIs; and (3) the MoE-based
adaptive encoder-decoder architecture, which ensures topological consistency
and enriches contextual representation by minimizing Jensen-Shannon divergence
across varying contexts. Experiments on two real-world datasets and multiple
POI tasks substantiate the superior performance of the proposed AdaptGOT model.

</details>


### [138] [ChildGuard: A Specialized Dataset for Combatting Child-Targeted Hate Speech](https://arxiv.org/abs/2506.21613)
*Gautam Siddharth Kashyap,Mohammad Anas Azeez,Rafiq Ali,Zohaib Hasan Siddiqui,Jiechao Gao,Usman Naseem*

Main category: cs.CL

TL;DR: 引入ChildGuard数据集，专注于儿童针对的仇恨言论，填补现有数据集的不足，促进研究。


<details>
  <summary>Details</summary>
Motivation: 现有仇恨言论数据集缺乏针对年龄的注释以及儿童特定的情境分析，不能有效捕捉对儿童的独特情感影响。

Method: 从现有语料库中提取并注释儿童特定内容创建了ChildGuard数据集，同时对多种最新的仇恨言论检测方法进行测试。

Result: 该数据集强化了多样的儿童仇恨言论场景，涵盖不同年龄组，并为评估现有检测方法及改进奠定了基础。

Conclusion: ChildGuard为研究儿童针对的仇恨言论问题提供了重要工具，助力检测和缓解此类问题的发展。

Abstract: The increasing prevalence of child-targeted hate speech online underscores
the urgent need for specialized datasets to address this critical issue.
Existing hate speech datasets lack agespecific annotations, fail to capture
nuanced contexts, and overlook the unique emotional impact on children. To
bridge this gap, we introduce ChildGuard1, a curated dataset derived from
existing corpora and enriched with child-specific annotations. ChildGuard
captures diverse contexts of child-targeted hate speech, spanning age groups.
We benchmark existing state-of-the-art hate speech detection methods, including
Large Language Models (LLMs), and assess their effectiveness in detecting and
contextualizing child-targeted hate speech. To foster further research in this
area, we publicly release ChildGuard, providing a robust foundation for
developing improved methods to detect and mitigate such harm.

</details>


### [139] [LastingBench: Defend Benchmarks Against Knowledge Leakage](https://arxiv.org/abs/2506.21614)
*Yixiong Fang,Tianran Sun,Yuling Shi,Min Wang,Xiaodong Gu*

Main category: cs.CL

TL;DR: 本文介绍了一个名为LastingBench的新框架，旨在通过扰动和重写泄露点来减少大语言模型在问答基准测试中的记忆影响，从而提高测试的公正性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型复杂性的增加，人们担心其在问答基准测试中通过记忆任务特定数据“作弊”，这会导致评估结果失真。本研究旨在减少数据泄露影响，维护基准测试的长期效用。

Method: 提出了LastingBench框架，通过上下文扰动检测出数据泄露点，并将其重写为反事实信息，从而保留测试初衷并减少记忆影响。

Result: 使用最先进的问答基准评估表明，LastingBench显著减少了模型的记忆效应，展示了其有效性。

Conclusion: LastingBench为增强基准测试的鲁棒性提供了一种实用且可扩展的解决方案，有助于促进基准评估的公平性和解释性。

Abstract: The increasing complexity of large language models (LLMs) raises concerns
about their ability to "cheat" on standard Question Answering (QA) benchmarks
by memorizing task-specific data. This undermines the validity of benchmark
evaluations, as they no longer reflect genuine model capabilities but instead
the effects of data leakage. While prior work has focused on detecting such
leakage, little attention has been given to mitigating its impact and
preserving the long-term utility of benchmarks. In this paper, we introduce
LastingBench, a novel framework designed to continuously reinforce and
safeguard existing benchmarks against knowledge leakage. LastingBench
identifies leakage points in the context through perturbation, then rewrites
the leakage points to counterfactual ones-disrupting memorization while
preserving the benchmark's original evaluative intent. Evaluations of
state-of-the-art QA benchmarks show significant performance gaps, highlighting
the efficacy of LastingBench in reducing memorization effects. LastingBench
offers a practical and scalable solution to ensure benchmark robustness over
time, promoting fairer and more interpretable evaluations of LLMs.

</details>


### [140] [Refine Medical Diagnosis Using Generation Augmented Retrieval and Clinical Practice Guidelines](https://arxiv.org/abs/2506.21615)
*Wenhao Li,Hongkuan Zhang,Hongwei Zhang,Zhengxu Li,Zengjie Dong,Yafan Chen,Niranjan Bidargaddi,Hong Liu*

Main category: cs.CL

TL;DR: 研究提出GARMLE-G框架，通过嵌入临床指南与模型预测结果，优化医疗语言模型对诊断的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有医疗语言模型多基于ICD诊断码，与临床医生的推理过程不匹配，难以满足实际临床需求，需通过整合临床实践指南来增强模型的实用性。

Method: GARMLE-G框架结合EHR数据与LLM预测生成语义化查询，通过嵌入相似性检索相关CPG内容，再将其与模型生成结果融合，确保输出无妄想，符合临床实践指南。

Result: 在高血压诊断原型系统中，该方法在检索精度、语义相关性及指南符合度方面优于RAG基线，同时具有轻量优势，适于本地化部署。

Conclusion: GARMLE-G能以低成本、可扩展的方式，将医学语言模型与证据为基础的临床实践对接，展示出广泛临床应用潜力。

Abstract: Current medical language models, adapted from large language models (LLMs),
typically predict ICD code-based diagnosis from electronic health records
(EHRs) because these labels are readily available. However, ICD codes do not
capture the nuanced, context-rich reasoning clinicians use for diagnosis.
Clinicians synthesize diverse patient data and reference clinical practice
guidelines (CPGs) to make evidence-based decisions. This misalignment limits
the clinical utility of existing models. We introduce GARMLE-G, a
Generation-Augmented Retrieval framework that grounds medical language model
outputs in authoritative CPGs. Unlike conventional Retrieval-Augmented
Generation based approaches, GARMLE-G enables hallucination-free outputs by
directly retrieving authoritative guideline content without relying on
model-generated text. It (1) integrates LLM predictions with EHR data to create
semantically rich queries, (2) retrieves relevant CPG knowledge snippets via
embedding similarity, and (3) fuses guideline content with model output to
generate clinically aligned recommendations. A prototype system for
hypertension diagnosis was developed and evaluated on multiple metrics,
demonstrating superior retrieval precision, semantic relevance, and clinical
guideline adherence compared to RAG-based baselines, while maintaining a
lightweight architecture suitable for localized healthcare deployment. This
work provides a scalable, low-cost, and hallucination-free method for grounding
medical language models in evidence-based clinical practice, with strong
potential for broader clinical deployment.

</details>


### [141] [TIM: A Large-Scale Dataset and large Timeline Intelligence Model for Open-domain Timeline Summarization](https://arxiv.org/abs/2506.21616)
*Chuanrui Hu,Wei Hu,Penghang Yu,Hua Zhang,Bing-Kun Bao*

Main category: cs.CL

TL;DR: 提出了一种新的时间线智能模型(TIM)，旨在解决开放领域时间线总结的问题，通过逐步优化策略显著提升了总结的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型存在无法很好地评估话题相关性与理解话题演变的问题，导致总结的信息包含无关细节或时间戳不准确。

Method: 提出了一个大型的时间线智能模型（TIM），并且构建了一个包含超过1000个新闻话题、3000个注释实例的大规模数据集。采用了逐步优化策略，包括指令微调和双对齐奖励学习方法，从语义和时间两个维度提升模型表现。

Result: TIM在开放领域的时间线总结中表现出色，显著提升了对话题演变的理解和总结能力。

Conclusion: 该研究提出的TIM模型通过创新的优化策略和大规模数据集支持，为开放领域时间线总结提供了一种高效的方法，在实验中取得了优异的表现。

Abstract: Open-domain Timeline Summarization (TLS) is crucial for monitoring the
evolution of news topics. To identify changes in news topics, existing methods
typically employ general Large Language Models (LLMs) to summarize relevant
timestamps from retrieved news. While general LLMs demonstrate capabilities in
zero-shot news summarization and timestamp localization, they struggle with
assessing topic relevance and understanding topic evolution. Consequently, the
summarized information often includes irrelevant details or inaccurate
timestamps. To address these issues, we propose the first large Timeline
Intelligence Model (TIM) for open-domain TLS, which is capable of effectively
summarizing open-domain timelines. Specifically, we begin by presenting a
large-scale TLS dataset, comprising over 1,000 news topics and more than 3,000
annotated TLS instances. Furthermore, we propose a progressive optimization
strategy, which gradually enhance summarization performance. It employs
instruction tuning to enhance summarization and topic-irrelevant information
filtering capabilities. Following this, it exploits a novel dual-alignment
reward learning method that incorporates both semantic and temporal
perspectives, thereby improving the understanding of topic evolution
principles. Through this progressive optimization strategy, TIM demonstrates a
robust ability to summarize open-domain timelines. Extensive experiments in
open-domain demonstrate the effectiveness of our TIM.

</details>


### [142] [TrajTok: Technical Report for 2025 Waymo Open Sim Agents Challenge](https://arxiv.org/abs/2506.21618)
*Zhiyuan Zhang,Xiaosong Jia,Guanyu Chen,Qifeng Li,Junchi Yan*

Main category: cs.CL

TL;DR: 提出了轨迹分词器TrajTok，并在SMART模型中应用，取得了显著表现。


<details>
  <summary>Details</summary>
Motivation: 改进行为生成模型的下一步预测精度和鲁棒性。

Method: 结合数据驱动和规则方法开发TrajTok，并采用空间感知标签平滑方法优化交叉熵损失。

Result: 在Waymo Open Sim Agents Challenge 2025中取得了0.7852的真实感得分。

Conclusion: TrajTok显著提高了行为生成模型的表现，未来将开源代码。

Abstract: In this technical report, we introduce TrajTok, a trajectory tokenizer for
discrete next-token-prediction based behavior generation models, which combines
data-driven and rule-based methods with better coverage, symmetry and
robustness, along with a spatial-aware label smoothing method for cross-entropy
loss. We adopt the tokenizer and loss for the SMART model and reach a superior
performance with realism score of 0.7852 on the Waymo Open Sim Agents Challenge
2025. We will open-source the code in the future.

</details>


### [143] [IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech](https://arxiv.org/abs/2506.21619)
*Siyi Zhou,Yiquan Zhou,Yi He,Xun Zhou,Jinchao Wang,Wei Deng,Jingchen Shu*

Main category: cs.CL

TL;DR: 本研究提出IndexTTS2，改善了自动回归生成模型在语音持续时间控制上的问题，并实现情感和音色的解耦与零次学习语音合成。


<details>
  <summary>Details</summary>
Motivation: 为了解决自动回归TTS模型在控制语音生成时长方面的困难，尤其是在视频配音等对音视频同步有严格要求的应用中。

Method: 提出一种适合自动回归模型的语音时长控制方法，支持手动控制生成时间和自动生成时长，并通过解耦情感表达和说话人音色，实现情感控制的自由性。

Result: 实验结果表明，相较于现有的最先进零样本TTS模型，IndexTTS2在词错误率、说话人相似性和情感保真度上均表现更优。

Conclusion: IndexTTS2成功克服了自动回归模型在语音持续时间控制上的局限性，同时实现了更自然和情感化的语音合成，满足了更复杂的应用需求。

Abstract: Large-scale text-to-speech (TTS) models are typically categorized into
autoregressive and non-autoregressive systems. Although autoregressive systems
exhibit certain advantages in speech naturalness, their token-by-token
generation mechanism makes it difficult to precisely control the duration of
synthesized speech. This is a key limitation in applications such as video
dubbing that require strict audio-visual synchronization. This paper introduces
IndexTTS2, which proposes a novel and autoregressive-model-friendly method for
speech duration control. The method supports two generation modes: one allows
explicit specification of the number of generated tokens for precise duration
control; the other does not require manual input and lets the model freely
generate speech while preserving prosodic characteristics from the input
prompt. Furthermore, IndexTTS2 achieves disentanglement between emotional
expression and speaker identity, enabling independent control of timbre and
emotion. In the zero-shot setting, the model can perfectly reproduce the
emotional characteristics of the input prompt. Users may also provide a
separate emotion prompt, even from a different speaker, allowing the model to
reconstruct the target timbre while conveying the desired emotion. To enhance
clarity during strong emotional expressions, we incorporate GPT latent
representations to improve speech stability. Meanwhile, to lower the barrier
for emotion control, we design a soft instruction mechanism based on textual
descriptions by fine-tuning Qwen3. This enables effective guidance of speech
generation with desired emotional tendencies using natural language input.
Experimental results demonstrate that IndexTTS2 outperforms existing
state-of-the-art zero-shot TTS models in word error rate, speaker similarity,
and emotional fidelity.

</details>


### [144] [How Large Language Models play humans in online conversations: a simulated study of the 2016 US politics on Reddit](https://arxiv.org/abs/2506.21620)
*Daniele Cirulli,Giulio Cimini,Giovanni Palermo*

Main category: cs.CL

TL;DR: 本研究评估了GPT-4在模仿用户生成内容中的表现，尤其是在2016年美国总统大选的Reddit讨论中。结果发现，GPT-4可以生成逼真的政治评论，但更倾向于制造共识而非分歧，并揭示其潜在影响。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLMs）在生成政治相关评论及其对在线讨论和政治舆论的潜在影响。

Method: 使用GPT-4对2016年美国总统大选时期的Reddit会话生成模拟评论，通过实验分析政治立场、情感和语言特征，并与真实用户评论进行对比分析。

Result: GPT-4能够生成符合政治讨论场景的评论，更倾向于制造共识；真实和人工生成评论在语义嵌入空间中有明显区别，但人工检查难以分辨。

Conclusion: GPT-4在特定情境下生成的评论具备影响在线讨论和政治舆论的能力，揭示了AI驱动的舆论操控潜在风险。

Abstract: Large Language Models (LLMs) have recently emerged as powerful tools for
natural language generation, with applications spanning from content creation
to social simulations. Their ability to mimic human interactions raises both
opportunities and concerns, particularly in the context of politically relevant
online discussions. In this study, we evaluate the performance of LLMs in
replicating user-generated content within a real-world, divisive scenario:
Reddit conversations during the 2016 US Presidential election. In particular,
we conduct three different experiments, asking GPT-4 to generate comments by
impersonating either real or artificial partisan users. We analyze the
generated comments in terms of political alignment, sentiment, and linguistic
features, comparing them against real user contributions and benchmarking
against a null model. We find that GPT-4 is able to produce realistic comments,
both in favor of or against the candidate supported by the community, yet
tending to create consensus more easily than dissent. In addition we show that
real and artificial comments are well separated in a semantically embedded
space, although they are indistinguishable by manual inspection. Our findings
provide insights on the potential use of LLMs to sneak into online discussions,
influence political debate and shape political narratives, bearing broader
implications of AI-driven discourse manipulation.

</details>


### [145] [The Open Proof Corpus: A Large-Scale Study of LLM-Generated Mathematical Proofs](https://arxiv.org/abs/2506.21621)
*Jasper Dekoninck,Ivo Petrov,Kristian Minchev,Mislav Balunovic,Martin Vechev,Miroslav Marinov,Maria Drencheva,Lyuba Konova,Milen Shumanov,Kaloyan Tsvetkov,Nikolay Drenchev,Lazar Todorov,Kalina Nikolova,Nikolay Georgiev,Vanesa Kalinkova,Margulan Ismoldayev*

Main category: cs.CL

TL;DR: 本文介绍了Open Proof Corpus（OPC），一个包含5000+人类评估证明的数据集，旨在推动数学证明生成研究。


<details>
  <summary>Details</summary>
Motivation: 当前数学证明生成领域缺乏大规模高质量的人类评估证明数据集，而这对于改进模型训练和分析生成能力至关重要。

Method: 作者构建了OPC数据集，其中包含从高水平数学竞赛中生成并由人类评估的证明，并进一步利用该数据集探索一些关键问题，包括自然语言与形式证明生成的性能差距、答案正确性与全证有效性的差异以及通过best-of-n选择提高质量的影响。

Result: 利用OPC微调一个8B参数的模型，生成的模型在证明正确性评估任务上表现与Gemini-2.5-Pro相当。

Conclusion: OPC的创建推动了自动数学证明生成领域的研究，为进一步研究和模型改进提供了宝贵的资源。

Abstract: In recent months, large language models (LLMs) have made significant progress
in mathematical proof generation, but further advancement is hindered by the
lack of a large-scale, high-quality dataset of human-evaluated proofs. While
expensive to create, such a dataset is essential for driving improvements in
training and enabling a rigorous analysis of proof generation capabilities. In
this work, we present the Open Proof Corpus (OPC), a dataset comprising over
5,000 human-evaluated proofs produced by state-of-the-art LLMs. The OPC was
specifically designed for broad applicability and downstream usage in proof
generation research and is the first to include a substantial number of
correct, LLM-generated solutions to problems from prestigious mathematics
competitions such as the USAMO and IMO. Using the OPC, we explore critical
questions in automated proof generation: (1) the performance gap between
natural language and formal proof generation, (2) the discrepancy between
final-answer accuracy and full-proof validity, and (3) the impact of best-of-n
selection on proof quality. Finally, to showcase the utility of the OPC, we
finetune an 8B-parameter model on the dataset, obtaining a model that performs
on par with the best model, Gemini-2.5-Pro, on the task of evaluating proof
correctness.

</details>


### [146] [Adapting Foundation Speech Recognition Models to Impaired Speech: A Semantic Re-chaining Approach for Personalization of German Speech](https://arxiv.org/abs/2506.21622)
*Niclas Pokel,Pehuén Moure,Roman Boehringer,Yingqiang Gao*

Main category: cs.CL

TL;DR: 提出了一种轻量化的流水线，用于个性化ASR模型，以改进处理非规范性语音的能力。


<details>
  <summary>Details</summary>
Motivation: 非规范性语音（例如因脑瘫或遗传性疾病导致的言语障碍）对现有ASR系统提出了重大挑战，需要找到有效的改进方法。

Method: 通过一个个性化的ASR流水线，优化语音选择并语义增强小型语音障碍数据集，提升语音识别模型的适应性。

Result: 在针对一名存在结构性言语障碍的儿童数据集上的实验中，所提出的方法显著提高了转录质量。

Conclusion: 这种方法展现了减轻非典型语音交流障碍的潜力，为言语障碍人群带来新的解决路径。

Abstract: Speech impairments caused by conditions such as cerebral palsy or genetic
disorders pose significant challenges for automatic speech recognition (ASR)
systems. Despite recent advances, ASR models like Whisper struggle with
non-normative speech due to limited training data and the difficulty of
collecting and annotating non-normative speech samples. In this work, we
propose a practical and lightweight pipeline to personalize ASR models,
formalizing the selection of words and enriching a small, speech-impaired
dataset with semantic coherence. Applied to data from a child with a structural
speech impairment, our approach shows promising improvements in transcription
quality, demonstrating the potential to reduce communication barriers for
individuals with atypical speech patterns.

</details>


### [147] [Performance of diverse evaluation metrics in NLP-based assessment and text generation of consumer complaints](https://arxiv.org/abs/2506.21623)
*Peiheng Gao,Chen Yang,Ning Sun,Ričardas Zitikis*

Main category: cs.CL

TL;DR: 使用人类经验训练算法和高质量合成数据，改进文本分类性能。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习在处理消费者投诉这一自然语言分类任务中，难以精确捕捉语言的细微差异和上下文变化。

Method: 使用人类经验强化的算法，并结合专家评估生成对抗网络生成的高质量合成数据，通过精心注释优化方法。

Result: 显著提升文本分类器性能，降低数据获取成本，同时增强评价指标和分类任务的鲁棒性。

Conclusion: 通过整合专家知识与生成对抗网络，能够在文本分类中实现更高效、更精准的性能表现。

Abstract: Machine learning (ML) has significantly advanced text classification by
enabling automated understanding and categorization of complex, unstructured
textual data. However, accurately capturing nuanced linguistic patterns and
contextual variations inherent in natural language, particularly within
consumer complaints, remains a challenge. This study addresses these issues by
incorporating human-experience-trained algorithms that effectively recognize
subtle semantic differences crucial for assessing consumer relief eligibility.
Furthermore, we propose integrating synthetic data generation methods that
utilize expert evaluations of generative adversarial networks and are refined
through expert annotations. By combining expert-trained classifiers with
high-quality synthetic data, our research seeks to significantly enhance
machine learning classifier performance, reduce dataset acquisition costs, and
improve overall evaluation metrics and robustness in text classification tasks.

</details>


### [148] [Doc2SAR: A Synergistic Framework for High-Fidelity Extraction of Structure-Activity Relationships from Scientific Documents](https://arxiv.org/abs/2506.21625)
*Jiaxi Zhuang,Kangning Li,Jue Hou,Mingjun Xu,Zhifeng Gao,Hengxing Cai*

Main category: cs.CL

TL;DR: 提出DocSAR-200，一个用于评估SAR提取方法的标准数据集，并开发了Doc2SAR，结合领域工具与增强的多模态大模型，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有提取方法受限于文档格式异质性和模型精确性不足，难以满足药物发现及材料研究需求。

Method: 设计DocSAR-200数据集测试SAR提取性能；开发Doc2SAR框架，结合领域工具与通过监督微调增强的多模态大模型。

Result: Doc2SAR在DocSAR-200上表现卓越，整体表格召回率80.78%，领先end2end GPT-4o 51.48%。模型高效推理且附带 web 应用。

Conclusion: DocSAR-200与Doc2SAR为SAR提取提供了强有力的评价基准和技术方案，在科学文档结构关系提取中表现优异。

Abstract: Extracting molecular structure-activity relationships (SARs) from scientific
literature and patents is essential for drug discovery and materials research.
However, this task remains challenging due to heterogeneous document formats
and limitations of existing methods. Specifically, rule-based approaches
relying on rigid templates fail to generalize across diverse document layouts,
while general-purpose multimodal large language models (MLLMs) lack sufficient
accuracy and reliability for specialized tasks, such as layout detection and
optical chemical structure recognition (OCSR). To address these challenges, we
introduce DocSAR-200, a rigorously annotated benchmark of 200 scientific
documents designed specifically for evaluating SAR extraction methods.
Additionally, we propose Doc2SAR, a novel synergistic framework that integrates
domain-specific tools with MLLMs enhanced via supervised fine-tuning (SFT).
Extensive experiments demonstrate that Doc2SAR achieves state-of-the-art
performance across various document types, significantly outperforming leading
end-to-end baselines. Specifically, Doc2SAR attains an overall Table Recall of
80.78% on DocSAR-200, exceeding end2end GPT-4o by 51.48%. Furthermore, Doc2SAR
demonstrates practical usability through efficient inference and is accompanied
by a web app.

</details>


### [149] [Do We Really Need GNNs with Explicit Structural Modeling? MLPs Suffice for Language Model Representations](https://arxiv.org/abs/2506.21682)
*Li Zhou,Hao Jiang,Junjie Li,Zefeng Zhao,Feng Jiang,Wenyu Chen,Haizhou Li*

Main category: cs.CL

TL;DR: 本研究提出了一个基于信息论视角的框架，用于评估显式结构建模在神经网络中的作用，发现MLPs在捕捉语法和语义模式方面效果显著，而GNNs的消息传递功能表现在某些情况下较弱。


<details>
  <summary>Details</summary>
Motivation: 近年来，虽然GNNs在下游NLP任务中被证明成功编码结构信息，然其充分利用结构信息的能力受到质疑。同时，缺少消息传递机制的MLPs在结构感知任务中表现意外优异，因此有必要深入探究其潜力。

Method: 作者提出拓展的探测框架，将控制模块引入传统探测分类器，可选择性地评估GNN模型及其消息传递、特征变换组件的单独贡献。同时利用Edge Probing Suite检测语言模型中编码的语言知识。

Result: 实验发现，MLPs作为特征变换模块能稳定提高语言模型中捕获的语义和句法知识，而GNN依赖纯粹消息传递操作表现较差，可能带来负面影响。

Conclusion: MLPs被验证为一种高效、可扩展的替代方案，其特征变换在提升语言模型表现上优于消息传递。而GNNs的表现可能受其具体操作模块的限制。

Abstract: Explicit structural information has been proven to be encoded by Graph Neural
Networks (GNNs), serving as auxiliary knowledge to enhance model capabilities
and improve performance in downstream NLP tasks. However, recent studies
indicate that GNNs fail to fully utilize structural information, whereas
Multi-Layer Perceptrons (MLPs), despite lacking the message-passing mechanisms
inherent to GNNs, exhibit a surprising ability in structure-aware tasks.
Motivated by these findings, this paper introduces a comprehensive probing
framework from an information-theoretic perspective. The framework is designed
to systematically assess the role of explicit structural modeling in enhancing
language model (LM) representations and to investigate the potential of MLPs as
efficient and scalable alternatives to GNNs. We extend traditional probing
classifiers by incorporating a control module that allows for selective use of
either the full GNN model or its decoupled components, specifically, the
message-passing and feature-transformation operations.This modular approach
isolates and assesses the individual contributions of these operations,
avoiding confounding effects from the complete GNN architecture. Using the Edge
Probing Suite, a diagnostic tool for evaluating the linguistic knowledge
encoded in LMs, we find that MLPs, when used as feature-transformation modules,
consistently improve the linguistic knowledge captured in LM representations
across different architectures. They effectively encode both syntactic and
semantic patterns. Similarly, GNNs that incorporate feature-transformation
operations show beneficial effects. In contrast, models that rely solely on
message-passing operations tend to underperform, often leading to negative
impacts on probing task performance.

</details>


### [150] [ANUBHUTI: A Comprehensive Corpus For Sentiment Analysis In Bangla Regional Languages](https://arxiv.org/abs/2506.21686)
*Swastika Kundu,Autoshi Ibrahim,Mithila Rahman,Tanvir Ahmed*

Main category: cs.CL

TL;DR: 本文提出了针对四种主要孟加拉语方言的情感分析数据集ANUBHUTI，包括2000句句子，涵盖政治、宗教和中性内容，采用多标签情感注释，用于改进低数据资源方言的自然语言处理。


<details>
  <summary>Details</summary>
Motivation: 旨在填补针对低资源孟加拉语方言情感分析的研究领域空白，提供高质量的数据集以提高自然语言处理的准确性和上下文感知能力。

Method: 通过手动翻译和专业注释，构建涵盖四种孟加拉方言的多标签情感数据集，并进行一致性和数据质量检查。

Result: 创建了ANUBHUTI，一个具有高度一致性和全面注释的情感分析数据集，为孟加拉语方言的自然语言处理提供了重要资源。

Conclusion: ANUBHUTI数据集填补了相关研究的资源缺口，将推动孟加拉语方言情感分析和自然语言处理技术的进步。

Abstract: Sentiment analysis for regional dialects of Bangla remains an underexplored
area due to linguistic diversity and limited annotated data. This paper
introduces ANUBHUTI, a comprehensive dataset consisting of 2000 sentences
manually translated from standard Bangla into four major regional dialects
Mymensingh, Noakhali, Sylhet, and Chittagong. The dataset predominantly
features political and religious content, reflecting the contemporary socio
political landscape of Bangladesh, alongside neutral texts to maintain balance.
Each sentence is annotated using a dual annotation scheme: multiclass thematic
labeling categorizes sentences as Political, Religious, or Neutral, and
multilabel emotion annotation assigns one or more emotions from Anger,
Contempt, Disgust, Enjoyment, Fear, Sadness, and Surprise. Expert native
translators conducted the translation and annotation, with quality assurance
performed via Cohens Kappa inter annotator agreement, achieving strong
consistency across dialects. The dataset was further refined through systematic
checks for missing data, anomalies, and inconsistencies. ANUBHUTI fills a
critical gap in resources for sentiment analysis in low resource Bangla
dialects, enabling more accurate and context aware natural language processing.

</details>


### [151] [Identifying Speaker Information in Feed-Forward Layers of Self-Supervised Speech Transformers](https://arxiv.org/abs/2506.21712)
*Tzu-Quan Lin,Hsi-Chun Cheng,Hung-yi Lee,Hao Tang*

Main category: cs.CL

TL;DR: 研究分析了自监督语音Transformer模型中与说话人信息相关的神经元，并通过保护这些神经元在裁剪中改进说话人任务的性能。


<details>
  <summary>Details</summary>
Motivation: 探讨如何在自监督语音Transformer中编解码说话人信息，以填补相关研究空白。

Method: 分析模型中与k-means聚类的自监督特征和i-vectors相关的神经元，通过保护这些神经元探索其对说话人任务的影响。

Result: 发现一些神经元与语音的语音学和性别类别相关，通过保护这些神经元可显著提高说话人任务性能。

Conclusion: 保护关键神经元对于维护自监督语音Transformer在说话人相关任务中的性能至关重要。

Abstract: In recent years, the impact of self-supervised speech Transformers has
extended to speaker-related applications. However, little research has explored
how these models encode speaker information. In this work, we address this gap
by identifying neurons in the feed-forward layers that are correlated with
speaker information. Specifically, we analyze neurons associated with k-means
clusters of self-supervised features and i-vectors. Our analysis reveals that
these clusters correspond to broad phonetic and gender classes, making them
suitable for identifying neurons that represent speakers. By protecting these
neurons during pruning, we can significantly preserve performance on
speaker-related task, demonstrating their crucial role in encoding speaker
information.

</details>


### [152] [(Fact) Check Your Bias](https://arxiv.org/abs/2506.21745)
*Eivind Morris Bakke,Nora Winger Heggelund*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLMs）在自动事实验证中的作用，尤其是Llama 3.1对HerO系统性能的影响。


<details>
  <summary>Details</summary>
Motivation: 分析LLMs的参数化知识偏差对事实验证结果的影响，并评估如何通过提示方式注入偏见。

Method: 通过实验研究Llama 3.1直接事实验证和生成支持、反驳、或中立文档对HerO系统结果的影响。

Result: Llama 3.1直接验证时将近一半的声明归为“证据不足”，不同提示策略虽影响检索结果但最终预测结论高度稳定。

Conclusion: LLMs的提示设计可影响事实检索，但对最终预测结论的稳定性冲击不大；研究有助于理解模型的内在知识偏见。

Abstract: Automatic fact verification systems increasingly rely on large language
models (LLMs). We investigate how parametric knowledge biases in these models
affect fact-checking outcomes of the HerO system (baseline for FEVER-25). We
examine how the system is affected by: (1) potential bias in Llama 3.1's
parametric knowledge and (2) intentionally injected bias. When prompted
directly to perform fact-verification, Llama 3.1 labels nearly half the claims
as "Not Enough Evidence". Using only its parametric knowledge it is able to
reach a verdict on the remaining half of the claims. In the second experiment,
we prompt the model to generate supporting, refuting, or neutral fact-checking
documents. These prompts significantly influence retrieval outcomes, with
approximately 50\% of retrieved evidence being unique to each perspective.
Notably, the model sometimes refuses to generate supporting documents for
claims it believes to be false, creating an inherent negative bias. Despite
differences in retrieved evidence, final verdict predictions show stability
across prompting strategies. The code is available at:
https://github.com/eibakke/FEVER-8-Shared-Task

</details>


### [153] [Evaluating List Construction and Temporal Understanding capabilities of Large Language Models](https://arxiv.org/abs/2506.21783)
*Alexandru Dumitru,V Venktesh,Adam Jatowt,Avishek Anand*

Main category: cs.CL

TL;DR: 本研究提出TLQA基准，用于评估语言模型在时间理解和构建列表答案中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在处理涉及多个实体和时间理解的任务时表现出局限性。

Method: 提出了Time referenced List based Question Answering (TLQA)基准，要求生成时间相关联的列表格式答案，并评估当前最先进生成模型的表现。

Result: 研究发现当前模型在闭卷环境下难以生成完整且时间对齐的答案，在开放领域中检索能力不足，需进一步改进。

Conclusion: 研究揭示了现有模型的不足，为未来研究提供了清晰的方向，并提供了新的基准来改善时间相关任务的研究。

Abstract: Large Language Models (LLMs) have demonstrated immense advances in a wide
range of natural language tasks. However, these models are susceptible to
hallucinations and errors on particularly temporal understanding tasks
involving multiple entities in answers. In such tasks, they fail to associate
entities with accurate time intervals, generate a complete list of entities in
answers or reason about events associated with specific temporal bounds.
Existing works do not extensively evaluate the abilities of the model to
perform implicit and explicit temporal understanding in a list answer
construction setup. To bridge this gap, we propose the Time referenced List
based Question Answering or TLQA benchmark that requires structured answers in
list format aligned with corresponding time periods. Our TLQA benchmark,
requires both list construction and temporal understanding simultaneously,
which to the best of our knowledge has not been explored in prior benchmarks.
We investigate the temporal understanding and list construction capabilities of
state-of-the-art generative models on TLQA in closed-book and open-domain
settings. Our findings reveal significant shortcomings in current models,
particularly their inability to provide complete answers and temporally align
facts in a closed-book setup and the need to improve retrieval in open-domain
setup, providing clear future directions for research on TLQA. The benchmark
and code at https://github.com/elixir-research-group/TLQA.

</details>


### [154] [Offensive Language Detection on Social Media Using XLNet](https://arxiv.org/abs/2506.21795)
*Reem Alothman,Hafida Benhidour,Said Kerrache*

Main category: cs.CL

TL;DR: 本研究提出一种基于XLNet的自动攻击性语言检测模型，并与BERT性能进行了对比。实验结果显示，XLNet在检测攻击性内容和分类攻击类型上优于BERT，而BERT在目标识别方面略胜一筹。还发现过采样和欠采样策略能有效解决类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上充斥着攻击性内容，由于内容体量庞大，人工监督不可行，因此需要自动化检测系统。

Method: 提出基于XLNet的攻击性语言检测模型，与常用模型BERT进行性能对比，使用Twitter数据集（OLID）进行评估，同时探讨采样策略对性能的影响。

Result: 实验表明，XLNet优于BERT在攻击性内容检测及分类方面的表现，同时采样策略有效提升了分类性能。

Conclusion: 研究表明，基于XLNet的模型具有开发稳健的攻击性语言检测系统的潜力，为社交媒体平台提供更高效的内容过滤工具。

Abstract: The widespread use of text-based communication on social media-through chats,
comments, and microblogs-has improved user interaction but has also led to an
increase in offensive content, including hate speech, racism, and other forms
of abuse. Due to the enormous volume of user-generated content, manual
moderation is impractical, which creates a need for automated systems that can
detect offensive language. Deep learning models, particularly those using
transfer learning, have demonstrated significant success in understanding
natural language through large-scale pretraining. In this study, we propose an
automatic offensive language detection model based on XLNet, a generalized
autoregressive pretraining method, and compare its performance with BERT
(Bidirectional Encoder Representations from Transformers), which is a widely
used baseline in natural language processing (NLP). Both models are evaluated
using the Offensive Language Identification Dataset (OLID), a benchmark Twitter
dataset that includes hierarchical annotations. Our experimental results show
that XLNet outperforms BERT in detecting offensive content and in categorizing
the types of offenses, while BERT performs slightly better in identifying the
targets of the offenses. Additionally, we find that oversampling and
undersampling strategies are effective in addressing class imbalance and
improving classification performance. These findings highlight the potential of
transfer learning and XLNet-based architectures to create robust systems for
detecting offensive language on social media platforms.

</details>


### [155] [A suite of allotaxonometric tools for the comparison of complex systems using rank-turbulence divergence](https://arxiv.org/abs/2506.21808)
*Jonathan St-Onge,Ashley M. A. Fehr,Carter Ward,Calla G. Beauregard,Michael V. Arnold,Samuel F. Rosenblatt,Benjamin Cooley,Christopher M. Danforth,Peter Sheridan Dodds*

Main category: cs.CL

TL;DR: 通过描述工具描述复杂系统及其比较，用于处理长尾分布的配对比较。


<details>
  <summary>Details</summary>
Motivation: 旨在提供从理论角度构建分析复杂系统并进行比较的工具。

Method: 开发基于类型湍流现象的配对分布可视化比较方法，借助Matlab、Javascript、Python工具实现。

Result: 提供一种有效分析复杂系统中长尾分布的工具套件。

Conclusion: 通过多种语言的工具实施，体系化实现复杂分布的配对比较及可视化。

Abstract: Describing and comparing complex systems requires principled, theoretically
grounded tools. Built around the phenomenon of type turbulence,
allotaxonographs provide map-and-list visual comparisons of pairs of
heavy-tailed distributions. Allotaxonographs are designed to accommodate a wide
range of instruments including rank- and probability-turbulence divergences,
Jenson-Shannon divergence, and generalized entropy divergences. Here, we
describe a suite of programmatic tools for rendering allotaxonographs for
rank-turbulence divergence in Matlab, Javascript, and Python, all of which have
different use cases.

</details>


### [156] [Towards Transparent AI: A Survey on Explainable Large Language Models](https://arxiv.org/abs/2506.21812)
*Avash Palikhe,Zhenyu Yu,Zichong Wang,Wenbin Zhang*

Main category: cs.CL

TL;DR: 本研究综述了大语言模型(LLMs)的现有可解释人工智能(XAI)技术，按模型架构分类并探讨其评估和实际应用，同时展望未来研究方向。


<details>
  <summary>Details</summary>
Motivation: LLMs虽然推动了人工智能领域的发展，但其不可解释性（‘黑箱’问题）限制了在高风险领域的广泛应用。

Method: 通过分类LLMs架构（编码器、解码器、编码器-解码器），综述现有可解释性技术，并讨论其在评估方法和实际应用中的表现。

Result: 总结了现有研究成果，揭示了目前在解释性领域的挑战和资源。

Conclusion: 提出了未来发展透明及负责任LLMs的研究方向，为深入提升其可解释性提供了指导。

Abstract: Large Language Models (LLMs) have played a pivotal role in advancing
Artificial Intelligence (AI). However, despite their achievements, LLMs often
struggle to explain their decision-making processes, making them a 'black box'
and presenting a substantial challenge to explainability. This lack of
transparency poses a significant obstacle to the adoption of LLMs in
high-stakes domain applications, where interpretability is particularly
essential. To overcome these limitations, researchers have developed various
explainable artificial intelligence (XAI) methods that provide
human-interpretable explanations for LLMs. However, a systematic understanding
of these methods remains limited. To address this gap, this survey provides a
comprehensive review of explainability techniques by categorizing XAI methods
based on the underlying transformer architectures of LLMs: encoder-only,
decoder-only, and encoder-decoder models. Then these techniques are examined in
terms of their evaluation for assessing explainability, and the survey further
explores how these explanations are leveraged in practical applications.
Finally, it discusses available resources, ongoing research challenges, and
future directions, aiming to guide continued efforts toward developing
transparent and responsible LLMs.

</details>


### [157] [Exploring the Structure of AI-Induced Language Change in Scientific English](https://arxiv.org/abs/2506.21817)
*Riley Galpin,Bryce Anderson,Tom S. Juzek*

Main category: cs.CL

TL;DR: 研究发现近年科学英语中的语言变化，诸如某些词汇频率激增，与大型语言模型（LLMs）的影响有关。分析揭示这些变化以语义和语用变化为主。


<details>
  <summary>Details</summary>
Motivation: 探讨科学英语中因大型语言模型导致的语言变化是否影响词义结构，并从语法和词性角度量化这些变化。

Method: 通过分析PubMed摘要中的同义词组，结合词性标注，研究‘频率激增’词语的语义变化及其模式，并比较词汇使用频率减少的情况。

Result: 发现语义词群往往整体变化，多数同义词组频率上升，说明变化主要为语义和语用层面。此外，部分词汇如‘important’显著减少，揭示了复杂的语言变化特征。

Conclusion: 大型语言模型引发的语言变化主要在语义和语用层面，对语言技术如何塑造人类语言提供了新见解。

Abstract: Scientific English has undergone rapid and unprecedented changes in recent
years, with words such as "delve," "intricate," and "crucial" showing
significant spikes in frequency since around 2022. These changes are widely
attributed to the growing influence of Large Language Models like ChatGPT in
the discourse surrounding bias and misalignment. However, apart from changes in
frequency, the exact structure of these linguistic shifts has remained unclear.
The present study addresses this and investigates whether these changes involve
the replacement of synonyms by suddenly 'spiking words,' for example, "crucial"
replacing "essential" and "key," or whether they reflect broader semantic and
pragmatic qualifications. To further investigate structural changes, we include
part of speech tagging in our analysis to quantify linguistic shifts over
grammatical categories and differentiate between word forms, like "potential"
as a noun vs. as an adjective. We systematically analyze synonym groups for
widely discussed 'spiking words' based on frequency trends in scientific
abstracts from PubMed. We find that entire semantic clusters often shift
together, with most or all words in a group increasing in usage. This pattern
suggests that changes induced by Large Language Models are primarily semantic
and pragmatic rather than purely lexical. Notably, the adjective "important"
shows a significant decline, which prompted us to systematically analyze
decreasing lexical items. Our analysis of "collapsing" words reveals a more
complex picture, which is consistent with organic language change and contrasts
with the patterns of the abrupt spikes. These insights into the structure of
language change contribute to our understanding of how language technology
continues to shape human language.

</details>


### [158] [PARSI: Persian Authorship Recognition via Stylometric Integration](https://arxiv.org/abs/2506.21840)
*Kourosh Shahnazari,Mohammadali Keshtparvar,Seyed Moein Ayyoubzadeh*

Main category: cs.CL

TL;DR: 通过集成深度学习和领域特定特征，研究开发了一种能分类67位波斯著名诗人的通用框架，取得了高达71%和97%（高信度下）准确率的成果。


<details>
  <summary>Details</summary>
Motivation: 解决波斯经典诗歌复杂语言、风格和韵律特性对计算机作者鉴定的挑战。

Method: 采用多输入神经网络框架，将基于Transformer的语言编码器与语义、风格和韵律特性相结合的特性矩阵，包括Word2Vec嵌入、风格度量以及诗体和格律的编码，并使用大规模的Ganjoor诗歌数据集进行训练和验证。

Result: 以韵律分类和投票评估方式，取得71%的整体分类准确率，且在0.9信度阈值下达到了97%的准确率（覆盖面较低）。

Conclusion: 方法具有潜力，能够推动波斯诗歌作者归类、风格分析、作者争议解决及多语言生成研究的发展，同时为跨语言作者鉴定和生成模型研究奠定基础。

Abstract: The intricate linguistic, stylistic, and metrical aspects of Persian
classical poetry pose a challenge for computational authorship attribution. In
this work, we present a versatile framework to determine authorship among 67
prominent poets. We employ a multi-input neural framework consisting of a
transformer-based language encoder complemented by features addressing the
semantic, stylometric, and metrical dimensions of Persian poetry. Our feature
set encompasses 100-dimensional Word2Vec embeddings, seven stylometric
measures, and categorical encodings of poetic form and meter. We compiled a
vast corpus of 647,653 verses of the Ganjoor digital collection, validating the
data through strict preprocessing and author verification while preserving
poem-level splitting to prevent overlap. This work employs verse-level
classification and majority and weighted voting schemes in evaluation,
revealing that weighted voting yields 71% accuracy. We further investigate
threshold-based decision filtering, allowing the model to generate highly
confident predictions, achieving 97% accuracy at a 0.9 threshold, though at
lower coverage. Our work focuses on the integration of deep representational
forms with domain-specific features for improved authorship attribution. The
results illustrate the potential of our approach for automated classification
and the contribution to stylistic analysis, authorship disputes, and general
computational literature research. This research will facilitate further
research on multilingual author attribution, style shift, and generative
modeling of Persian poetry.

</details>


### [159] [LinguaSynth: Heterogeneous Linguistic Signals for News Classification](https://arxiv.org/abs/2506.21848)
*Duo Zhang,Junyi Mo*

Main category: cs.CL

TL;DR: 提出了一种名为LinguaSynth的文本分类框架，结合五种语言特征类型，在保持高性能的同时增强了可解释性与计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在NLP领域中的解释性和计算效率问题，提供一种替代黑盒模型的方法。

Method: 通过透明的逻辑回归模型，整合了词汇、句法、实体级别、词级语义及文档级语义五种语言特征进行文本分类。

Result: 在20 Newsgroups数据集上实现了84.89%的准确率，比TF-IDF基线高出3.32%。

Conclusion: LinguaSynth展示了深度学习模型之外，在NLP中实现高性能与可解释性的可能性。

Abstract: Deep learning has significantly advanced NLP, but its reliance on large
black-box models introduces critical interpretability and computational
efficiency concerns. This paper proposes LinguaSynth, a novel text
classification framework that strategically integrates five complementary
linguistic feature types: lexical, syntactic, entity-level, word-level
semantics, and document-level semantics within a transparent logistic
regression model. Unlike transformer-based architectures, LinguaSynth maintains
interpretability and computational efficiency, achieving an accuracy of 84.89
percent on the 20 Newsgroups dataset and surpassing a robust TF-IDF baseline by
3.32 percent. Through rigorous feature interaction analysis, we show that
syntactic and entity-level signals provide essential disambiguation and
effectively complement distributional semantics. LinguaSynth sets a new
benchmark for interpretable, resource-efficient NLP models and challenges the
prevailing assumption that deep neural networks are necessary for
high-performing text classification.

</details>


### [160] [The Consistency Hypothesis in Uncertainty Quantification for Large Language Models](https://arxiv.org/abs/2506.21849)
*Quan Xiao,Debarun Bhattacharjya,Balaji Ganesan,Radu Marinescu,Katsiaryna Mirylenka,Nhan H Pham,Michael Glass,Junkyu Lee*

Main category: cs.CL

TL;DR: 本研究探讨了利用生成一致性作为置信度度量的假设，并提出了数据自由的黑盒不确定性量化方法，验证了其实用价值。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在实际应用中的普及，确保其输出的置信度至关重要，尤其是在需要高度用户信任的情境下。

Method: 研究针对使用生成一致性作为置信度代理的假设，通过提出三种数学论述及相应的统计测试，研究模型输出一致性并提出基于相似性聚合的黑盒不确定性量化方法。

Result: 通过在8个基准数据集和3个任务（问答、文本摘要、文本到SQL）上的实验，验证了生成一致性假设的普遍性，并且数据自由的黑盒方法优于现有基线。

Conclusion: 生成一致性假设具有实际应用价值，可以为置信度估计提供可靠依据，并显著提升不确定性量化方法的表现。

Abstract: Estimating the confidence of large language model (LLM) outputs is essential
for real-world applications requiring high user trust. Black-box uncertainty
quantification (UQ) methods, relying solely on model API access, have gained
popularity due to their practical benefits. In this paper, we examine the
implicit assumption behind several UQ methods, which use generation consistency
as a proxy for confidence, an idea we formalize as the consistency hypothesis.
We introduce three mathematical statements with corresponding statistical tests
to capture variations of this hypothesis and metrics to evaluate LLM output
conformity across tasks. Our empirical investigation, spanning 8 benchmark
datasets and 3 tasks (question answering, text summarization, and text-to-SQL),
highlights the prevalence of the hypothesis under different settings. Among the
statements, we highlight the `Sim-Any' hypothesis as the most actionable, and
demonstrate how it can be leveraged by proposing data-free black-box UQ methods
that aggregate similarities between generations for confidence estimation.
These approaches can outperform the closest baselines, showcasing the practical
value of the empirically observed consistency hypothesis.

</details>


### [161] [Derivational Probing: Unveiling the Layer-wise Derivation of Syntactic Structures in Neural Language Models](https://arxiv.org/abs/2506.21861)
*Taiga Someya,Ryo Yoshida,Hitomi Yanaka,Yohei Oseki*

Main category: cs.CL

TL;DR: 本文研究神经语言模型内部表示的句法结构构造过程，通过提出的派生探测技术发现BERT存在自下而上的句法信息推导模式。


<details>
  <summary>Details</summary>
Motivation: 近年来研究表明神经语言模型能够编码句法结构，但对各层之间这些结构的派生过程仍未充分了解。

Method: 提出“派生探测”方法，研究微观句法结构与宏观句法结构在单词嵌入跨层传播中的构造过程。

Result: 实验显示，在BERT中，低层逐步形成微观句法结构，高层整合为宏观句法结构，并且构造宏观句法结构的时机对后续表现至关重要。

Conclusion: 模型句法信息的最佳整合时机影响性能，自下而上的推导过程为研究语言模型句法机制提供了新视角。

Abstract: Recent work has demonstrated that neural language models encode syntactic
structures in their internal representations, yet the derivations by which
these structures are constructed across layers remain poorly understood. In
this paper, we propose Derivational Probing to investigate how micro-syntactic
structures (e.g., subject noun phrases) and macro-syntactic structures (e.g.,
the relationship between the root verbs and their direct dependents) are
constructed as word embeddings propagate upward across layers. Our experiments
on BERT reveal a clear bottom-up derivation: micro-syntactic structures emerge
in lower layers and are gradually integrated into a coherent macro-syntactic
structure in higher layers. Furthermore, a targeted evaluation on subject-verb
number agreement shows that the timing of constructing macro-syntactic
structures is critical for downstream performance, suggesting an optimal timing
for integrating global syntactic information.

</details>


### [162] [DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive Modality-Specific MoE](https://arxiv.org/abs/2506.21864)
*Hang Shao,Heting Gao,Yunhang Shen,Jiawei Chen,Lijiang Li,Zuwei Long,Bo Tong,Ke Li,Xing Sun*

Main category: cs.CL

TL;DR: 本文提出了DeepTalk，一种基于专家混合架构的框架，旨在改进多模态大语言模型在语音生成任务中的表现，降低性能下降和响应延迟。代码与模型已开源。


<details>
  <summary>Details</summary>
Motivation: 现有的原生多模态大语言模型因语音-文本数据缺乏，在语音生成性能上存在大幅下降，且存在延迟高等问题。

Method: 通过专家混合（MoE）架构，DeepTalk分别进行单模态训练和联合多模态训练，优化多模态特性。

Result: DeepTalk相对于原模型仅降低5.5%的性能，比其他原生多模态语言模型的20%以上性能下降更优，同时对话延迟控制在0.5秒以内。

Conclusion: DeepTalk在保持低性能损耗的同时，大幅提升了多模态大语言模型在语音交互上的流畅性和智能性，是一种兼具效率与表现的解决方案。

Abstract: Native multimodal large language models (MLLMs) restructure a single large
language model (LLM) into a spoken language model (SLM) capable of both speech
and text generation. Compared to modular and aligned MLLMs, native MLLMs
preserve richer paralinguistic features such as emotion and prosody, and
generate speech responses directly within the backbone LLM rather than using a
separate speech decoder. This integration also results in lower response
latency and smoother interaction. However, native MLLMs suffer from
catastrophic forgetting and performance degradation because the available
paired speech-text data is insufficient to support the pretraining of MLLMs
compared to the vast amount of text data required to pretrain text LLMs. To
address this issue, we propose DeepTalk, a framework for adaptive modality
expert learning based on a Mixture of Experts (MoE) architecture. DeepTalk
first adaptively distinguishes modality experts according to their modality
load within the LLM. Each modality expert then undergoes specialized
single-modality training, followed by joint multimodal collaborative training.
As a result, DeepTalk incurs only a 5.5% performance drop compared to the
original LLM, which is significantly lower than the average performance drop of
over 20% typically seen in native MLLMs (such as GLM-4-Voice), and is on par
with modular MLLMs. Meanwhile, the end-to-end dialogue latency remains within
0.5 seconds, ensuring a seamless and intelligent speech interaction experience.
Code and models are released at https://github.com/talkking/DeepTalk.

</details>


### [163] [WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation](https://arxiv.org/abs/2506.21875)
*Jian Zhang,Linhao Zhang,Bokai Lei,Chuhan Wu,Wei Jia,Xiao Zhou*

Main category: cs.CL

TL;DR: 提出一个新的基准，用于全面评估多模态大语言模型在实际语音对话中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的方法往往改编文本基准，忽视了语音的独特特性，因此需要设计专门的评估基准以优化用户体验。

Method: 系统收集与语音相关的实际聊天数据，加入语音现象多样性，并设计查询感知的评估方法，通过定制的清单和提示提升评估准确性。

Result: 对多种主流语音模型进行了全面测试和详细分析，揭示了不同语音场景下模型性能的显著差异，同时表明查询感知评估方法能够实现更细粒度的评估。

Conclusion: 文中提出的基准对语音模型的开发和评估具有重要价值。

Abstract: Recent multi-modal Large Language Models (LLMs) such as GPT-4o have
demonstrated strong capabilities of direct speech interaction. However, the
lack of specialized and comprehensive benchmarks for end-to-end speech LLM
evaluation hinders optimizing the user experience of Audio LLMs in real-world
applications. Existing evaluation methods often adapt text-based benchmarks,
overlooking speech's unique characteristics and challenges, including prosody,
homophones, stuttering, and differing user expectations. Here, we present a
novel approach to thoroughly evaluate LLMs in practical speech conversations.
We systematically curate real-world chat data relevant to spoken scenarios,
introduce diversity in speaker attributes and acoustic conditions, and augment
the dataset with speech-specific phenomena. We further design a query-aware
evaluation method to use customized evaluation checklists and prompts to
enhance the accuracy of automatic evaluation. We conduct comprehensive testing
and detailed analysis of various mainstream speech models, revealing
significant differences in model performance across different speech scenarios.
The use of query-aware evaluation further enables a finer-grained assessment
under various speech-specific scenarios. Our benchmark can provide valuable
insights for speech model development and evaluation.

</details>


### [164] [Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation](https://arxiv.org/abs/2506.21876)
*Qiyue Gao,Xinyu Pi,Kevin Liu,Junrong Chen,Ruolan Yang,Xinqi Huang,Xinyu Fang,Lu Sun,Gautham Kishore,Bo Ai,Stone Tao,Mengyang Liu,Jiaxi Yang,Chao-Jung Lai,Chuanyang Jin,Jiannan Xiang,Benhao Huang,Zeming Chen,David Danks,Hao Su,Tianmin Shu,Ziqiao Ma,Lianhui Qin,Zhiting Hu*

Main category: cs.CL

TL;DR: 研究提出了一个新的评估框架和基准WM-ABench，评估视觉语言模型（VLMs）作为内部世界模型的能力，该基准显示这些模型在基本世界建模能力上仍存在显著局限性。


<details>
  <summary>Details</summary>
Motivation: 探索最新的视觉语言模型在理解世界状态和预测行为转换方面的能力，填补它们作为世界模型的系统性评估空白。

Method: 提出了一个分为感知和预测的两阶段评估框架，并开发了包含23个评估维度的WM-ABench基准集，基于6个模拟环境和660个实验对15种模型进行了测试。

Result: 实验表明大部分模型在区分运动轨迹等任务上表现接近随机准确率，并在理解上展现出显著偏误，例如认为蓝色物体比绿色物体运动得更快。

Conclusion: 当前视觉语言模型在世界建模的基本能力上与人类水平仍有显著差距，需进一步研究和改进。

Abstract: Internal world models (WMs) enable agents to understand the world's state and
predict transitions, serving as the basis for advanced deliberative reasoning.
Recent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and
Gemini, exhibit potential as general-purpose WMs. While the latest studies have
evaluated and shown limitations in specific capabilities such as visual
understanding, a systematic evaluation of VLMs' fundamental WM abilities
remains absent. Drawing on comparative psychology and cognitive science, we
propose a two-stage framework that assesses Perception (visual, spatial,
temporal, quantitative, and motion) and Prediction (mechanistic simulation,
transitive inference, compositional inference) to provide an atomic evaluation
of VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale
benchmark comprising 23 fine-grained evaluation dimensions across 6 diverse
simulated environments with controlled counterfactual simulations. Through 660
experiments on 15 latest commercial and open-source VLMs, we find that these
models exhibit striking limitations in basic world modeling abilities. For
instance, almost all models perform at near-random accuracy when distinguishing
motion trajectories. Additionally, they lack disentangled understanding --
e.g., some models tend to believe blue objects move faster than green ones.
More rich results and analyses reveal significant gaps between VLMs and
human-level world modeling.

</details>


### [165] [A Dual-Layered Evaluation of Geopolitical and Cultural Bias in LLMs](https://arxiv.org/abs/2506.21881)
*Sean Kim,Hyuhng Joon Kim*

Main category: cs.CL

TL;DR: 本文定义了大语言模型（LLMs）中的两种偏见：模型偏见和推理偏见，并通过两阶段评估框架研究LLMs在中性与敏感话题下的行为表现，提供了未来多语言评估的实践见解。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在不同语言和文化环境中的广泛应用，明确其在事实性和争议性场景中的表现尤为重要，以避免输出内容对公众舆论或主流叙事产生不良影响。

Method: 提出两阶段的评估框架：第一阶段使用单一可验证答案的事实性问题，评估模型在不同查询语言中的一致性；第二阶段考察地缘政治敏感争议，分析模型训练背景和查询语言之间的互动，并构建手动标注的数据集，涵盖四种语言与问题类型。

Result: 第一阶段显示查询语言影响一致性，第二阶段反映了模型训练背景与查询语言之间的复杂关联。

Conclusion: 本研究构建了系统化框架以评估LLMs在中立和敏感话题中的行为，为多语言环境下未来的模型部署和文化敏感的评价方法提供了指导。

Abstract: As large language models (LLMs) are increasingly deployed across diverse
linguistic and cultural contexts, understanding their behavior in both factual
and disputable scenarios is essential, especially when their outputs may shape
public opinion or reinforce dominant narratives. In this paper, we define two
types of bias in LLMs: model bias (bias stemming from model training) and
inference bias (bias induced by the language of the query), through a two-phase
evaluation. Phase 1 evaluates LLMs on factual questions where a single
verifiable answer exists, assessing whether models maintain consistency across
different query languages. Phase 2 expands the scope by probing geopolitically
sensitive disputes, where responses may reflect culturally embedded or
ideologically aligned perspectives. We construct a manually curated dataset
spanning both factual and disputable QA, across four languages and question
types. The results show that Phase 1 exhibits query language induced alignment,
while Phase 2 reflects an interplay between the model's training context and
query language. This paper offers a structured framework for evaluating LLM
behavior across neutral and sensitive topics, providing insights for future LLM
deployment and culturally aware evaluation practices in multilingual contexts.

</details>


### [166] [More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents](https://arxiv.org/abs/2506.21967)
*Weimin Xiong,Ke Wang,Yifan Song,Hanchao Liu,Sai Zhou,Wei Peng,Sujian Li*

Main category: cs.CL

TL;DR: 研究揭示了工具集成的LLM代理在工具调用过程中存在高易错性，尤其是开源模型和特定正常用户指令的攻击下。


<details>
  <summary>Details</summary>
Motivation: 探讨工具集成LLM代理在整个工具调用过程中是否易受错误影响，以及其影响稳定性的潜在因素。

Method: 通过大量实验分析代理在工具文档阅读、工具选择、参数生成及响应处理等环节中的易错性，并比较开源与专有模型的稳定性。

Result: 发现代理在每个阶段都容易出现错误，且开源模型比专有模型更为脆弱，同时增加模型规模并未显著改善稳定性，甚至可能使其易受攻击。

Conclusion: 强调需要评估代理的稳定性，为未来大型语言模型的开发和评估提供了重要见解。

Abstract: Current evaluations of tool-integrated LLM agents typically focus on
end-to-end tool-usage evaluation while neglecting their stability. This limits
their real-world applicability, as various internal or external factors can
cause agents to crash or behave abnormally. Our research addresses this by
investigating whether agents are vulnerable to errors throughout the entire
tool invocation process, including reading tool documentation, selecting tools
and generating parameters, and processing the tool's response. Through
extensive experiments, we observe that agents are highly susceptible to errors
at each stage and agents based on open-source models are more vulnerable than
those based on proprietary models. We also find that increasing the model size
does not significantly improve tool invocation reasoning and may make agents
more vulnerable to attacks resembling normal user instructions. This highlights
the importance of evaluating agent stability and offers valuable insights for
future LLM development and evaluation.

</details>


### [167] [AutoMixer: Checkpoint Artifacts as Automatic Data Mixers](https://arxiv.org/abs/2506.21910)
*Ernie Chang,Yang Li,Patrick Huber,David Kant,Yangyang Shi,Vikas Chandra*

Main category: cs.CL

TL;DR: 论文提出利用训练过程中的检查点模型作为数据混合器，通过其对源数据的影响来优化数据质量，提高任务能力的框架，测试结果在推理基准上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 语言模型训练中，需整合多任务能力，但难以直接找到合适的数据混合方式。观察到训练过程中的检查点模型在不同阶段展现出不同能力，成为潜在的数据信号来源。

Method: 通过基于检查点模型在基准任务上的能力，将其对源数据的一阶影响进行聚合，作为优化数据质量和混合的依据。

Result: 在8个推理基准测试中，提出的框架在预训练中最高提升了1.93%的性能。

Conclusion: 检查点模型可以有效提升数据质量和任务能力优化，实现对数据混合方式的改进。

Abstract: In language model training, it is desirable to equip models with capabilities
from various tasks. However, it is not clear how to directly obtain the right
data mixtures for these capabilities as the relationship between data and tasks
is difficult to be modeled. In this work, we observe that checkpoint models
exhibit emerging capabilities at different points in the training trajectory.
Often, the training process saves checkpoints as artifacts that are
under-utilized as a source of in-training data signals. We identify these
artifact models based on their respective capabilities on the benchmarks and
leverage them as data mixers by using their aggregated first-order influence
approximation over source data. We demonstrated on eight reasoning benchmarks
that the proposed framework shows significant improvements in the pretraining
setting, with performance improvements of up to 1.93%. Overall, this shows the
potential of checkpoint models to enhance data quality and optimize data
mixtures.

</details>


### [168] [Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses](https://arxiv.org/abs/2506.21972)
*Mohamed Ahmed,Mohamed Abdelmouty,Mingyu Kim,Gunvanth Kandula,Alex Park,James C. Davis*

Main category: cs.CL

TL;DR: 提出两种结合Token级和Prompt级技术的混合攻击方法，显著提高了对PTLMs的攻击成功率并突破现有防御措施。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在推理阶段容易受到Token级和Prompt级攻击，但各自方法的局限性仍未彻底解决。

Method: 结合Token级和Prompt级技术，分别提出GCG+PAIR和GCG+WordGame的混合攻击方法，评估其在多种模型上的性能与防御突破能力。

Result: GCG+PAIR在Llama-3上的攻击成功率达91.6%，显著高于单一方法；GCG+WordGame在更严格条件下仍保持80%以上的成功率，且两种方法均突破了高级防御。

Conclusion: 当前安全措施存在未被发现的漏洞，需发展更全面的防御手段应对适应性强的攻击。

Abstract: The advancement of Pre-Trained Language Models (PTLMs) and Large Language
Models (LLMs) has led to their widespread adoption across diverse applications.
Despite their success, these models remain vulnerable to attacks that exploit
their inherent weaknesses to bypass safety measures. Two primary
inference-phase threats are token-level and prompt-level jailbreaks.
Token-level attacks embed adversarial sequences that transfer well to black-box
models like GPT but leave detectable patterns and rely on gradient-based token
optimization, whereas prompt-level attacks use semantically structured inputs
to elicit harmful responses yet depend on iterative feedback that can be
unreliable. To address the complementary limitations of these methods, we
propose two hybrid approaches that integrate token- and prompt-level techniques
to enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the
newly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and
Llama models. GCG + PAIR consistently raised attack-success rates over its
constituent techniques on undefended models; for instance, on Llama-3, its
Attack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's
58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of
WordGame maintaining a high ASR of over 80% even under stricter evaluators like
Mistral-Sorry-Bench. Crucially, both hybrids retained transferability and
reliably pierced advanced defenses such as Gradient Cuff and JBShield, which
fully blocked single-mode attacks. These findings expose previously unreported
vulnerabilities in current safety stacks, highlight trade-offs between raw
success and defensive robustness, and underscore the need for holistic
safeguards against adaptive adversaries.

</details>


### [169] [PapersPlease: A Benchmark for Evaluating Motivational Values of Large Language Models Based on ERG Theory](https://arxiv.org/abs/2506.21961)
*Junho Myung,Yeon Su Park,Sunwoo Kim,Shin Yoo,Alice Oh*

Main category: cs.CL

TL;DR: 本文提出PapersPlease基准，含3,700个道德困境评估LLMs在优先考虑不同人类需要层次的决策表现及偏差。


<details>
  <summary>Details</summary>
Motivation: 研究者希望探讨在道德困境中LLMs是如何依据动机需求模型（ERG理论）做决策，以及社交身份如何影响其判断。

Method: 研究者设计了PapersPlease基准，由3,700个基于ERG理论生成的叙述构成，并测试六种LLM在移民官角色扮演中的决策情况，同时考察社交身份对决策的影响。

Result: 六种LLM的分析显示其决策模式具有统计学显著性，且对不同社交身份的回应有差异，其中部分模型对边缘化身份有更高的否决率。

Conclusion: 研究表明LLMs在道德决策中可能存在隐性偏好，并可能加剧社会不平等，同时强调了需要注意的伦理问题。

Abstract: Evaluating the performance and biases of large language models (LLMs) through
role-playing scenarios is becoming increasingly common, as LLMs often exhibit
biased behaviors in these contexts. Building on this line of research, we
introduce PapersPlease, a benchmark consisting of 3,700 moral dilemmas designed
to investigate LLMs' decision-making in prioritizing various levels of human
needs. In our setup, LLMs act as immigration inspectors deciding whether to
approve or deny entry based on the short narratives of people. These narratives
are constructed using the Existence, Relatedness, and Growth (ERG) theory,
which categorizes human needs into three hierarchical levels. Our analysis of
six LLMs reveals statistically significant patterns in decision-making,
suggesting that LLMs encode implicit preferences. Additionally, our evaluation
of the impact of incorporating social identities into the narratives shows
varying responsiveness based on both motivational needs and identity cues, with
some models exhibiting higher denial rates for marginalized identities. All
data is publicly available at https://github.com/yeonsuuuu28/papers-please.

</details>


### [170] [Analyzing and Fine-Tuning Whisper Models for Multilingual Pilot Speech Transcription in the Cockpit](https://arxiv.org/abs/2506.21990)
*Kartheek Kumar Reddy Nareddy,Sarah Ternus,Julia Niebling*

Main category: cs.CL

TL;DR: 本论文探讨如何改进Whisper模型在驾驶舱对话转录中的表现。在对85分钟的模拟录音和130分钟的飞行员访谈录音进行标注后，通过引入标准化和LoRA微调方法，将模型的词语错误率从68.49%降至26.26%。


<details>
  <summary>Details</summary>
Motivation: 解析当前Transformer编解码架构在领域特定任务，尤其是驾驶舱对话转录上的挑战，特别是在涉及特定词汇和多语种场景时的性能不足问题。

Method: 使用Whisper模型，通过实施多种归一化方式来优化转录表现，同时引入LoRA技术进行性能高效的微调。

Result: 提出的归一化和微调技术将Whisper模型的词语错误率从68.49%大幅降低到26.26%。

Conclusion: 研究表明，通过标准化和LoRA微调可以显著提高Whisper模型在特定的多语言驾驶舱对话转录任务中的表现，为领域相关的语音识别带来启发。

Abstract: The developments in transformer encoder-decoder architectures have led to
significant breakthroughs in machine translation, Automatic Speech Recognition
(ASR), and instruction-based chat machines, among other applications. The
pre-trained models were trained on vast amounts of generic data over a few
epochs (fewer than five in most cases), resulting in their strong
generalization capabilities. Nevertheless, the performance of these models does
suffer when applied to niche domains like transcribing pilot speech in the
cockpit, which involves a lot of specific vocabulary and multilingual
conversations. This paper investigates and improves the transcription accuracy
of cockpit conversations with Whisper models. We have collected around 85
minutes of cockpit simulator recordings and 130 minutes of interview recordings
with pilots and manually labeled them. The speakers are middle aged men
speaking both German and English. To improve the accuracy of transcriptions, we
propose multiple normalization schemes to refine the transcripts and improve
Word Error Rate (WER). We then employ fine-tuning to enhance ASR performance,
utilizing performance-efficient fine-tuning with Low-Rank Adaptation (LoRA).
Hereby, WER decreased from 68.49 \% (pretrained whisper Large model without
normalization baseline) to 26.26\% (finetuned whisper Large model with the
proposed normalization scheme).

</details>


### [171] [Identifying a Circuit for Verb Conjugation in GPT-2](https://arxiv.org/abs/2506.22105)
*David Demitri Africa*

Main category: cs.CL

TL;DR: 本文探讨了GPT-2 Small中负责主谓一致性的子网络结构，使用技能验证和路径修补等技术隔离出影响模型动词变化的回路。


<details>
  <summary>Details</summary>
Motivation: 理解GPT-2 Small模型中如何实现主谓一致，以揭示模型内部机制。

Method: 通过性能验证、路径修补及直接逻辑归因等技术隔离并分析主谓一致的负责子网络。

Result: 识别出一个在运动词变化任务中起重要作用的小规模子网络。

Conclusion: GPT-2 Small仅需一小部分组件即可实现主要性能，但更复杂任务需求更多组件。

Abstract: I implement a procedure to isolate and interpret the sub-network (or
"circuit") responsible for subject-verb agreement in GPT-2 Small. In this
study, the model is given prompts where the subject is either singular (e.g.
"Alice") or plural (e.g. "Alice and Bob"), and the task is to correctly predict
the appropriate verb form ("walks" for singular subjects, "walk" for plural
subjects). Using a series of techniques-including performance verification
automatic circuit discovery via direct path patching, and direct logit
attribution- I isolate a candidate circuit that contributes significantly to
the model's correct verb conjugation. The results suggest that only a small
fraction of the network's component-token pairs is needed to achieve near-model
performance on the base task but substantially more for more complex settings.

</details>


### [172] [Don't Trust Generative Agents to Mimic Communication on Social Networks Unless You Benchmarked their Empirical Realism](https://arxiv.org/abs/2506.21974)
*Simon Münker,Nils Schwager,Achim Rettinger*

Main category: cs.CL

TL;DR: 本文研究利用大型语言模型（LLMs）模仿社交网络用户行为，用于社交网络分析，并提出通过验证模拟的经验真实性来提高模拟的可靠性。


<details>
  <summary>Details</summary>
Motivation: 鉴于关于LLMs是否能用于社会科学实验的研究存在矛盾结果，有必要加深对实验设计差异的理解。

Method: 提出一个模拟社交网络的正式框架，并针对用户通信模仿任务进行实证测试，涵盖英语和德语的不同方法。

Result: 研究表明，社交模拟应通过在模拟组件拟合时的环境下测量的经验真实性来验证其可靠性。

Conclusion: 论文主张在使用基于生成代理的建模进行社会模拟时需更加严谨高效。

Abstract: The ability of Large Language Models (LLMs) to mimic human behavior triggered
a plethora of computational social science research, assuming that empirical
studies of humans can be conducted with AI agents instead. Since there have
been conflicting research findings on whether and when this hypothesis holds,
there is a need to better understand the differences in their experimental
designs. We focus on replicating the behavior of social network users with the
use of LLMs for the analysis of communication on social networks. First, we
provide a formal framework for the simulation of social networks, before
focusing on the sub-task of imitating user communication. We empirically test
different approaches to imitate user behavior on X in English and German. Our
findings suggest that social simulations should be validated by their empirical
realism measured in the setting in which the simulation components were fitted.
With this paper, we argue for more rigor when applying generative-agent-based
modeling for social simulation.

</details>


### [173] [Can Peter Pan Survive MT? A Stylometric Study of LLMs, NMTs, and HTs in Children's Literature Translation](https://arxiv.org/abs/2506.22038)
*Delu Kong,Lieve Macken*

Main category: cs.CL

TL;DR: 该研究通过分析英文到中文儿童文学翻译中的机器翻译与人工翻译表现差异，发现大语言模型翻译在风格上更接近人工翻译，并优于神经机器翻译。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨在儿童文学翻译中，机器翻译（特别是大语言模型翻译）是否能够与人工翻译在风格与语言特征上接近。

Method: 通过建立一个《彼得·潘》语料库，包含人工翻译、神经机器翻译和大语言模型翻译三种类型翻译文本，利用分类和聚类机器学习方法，对447项语言特征进行文体计量学分析。

Result: 研究显示在人称词和连接词等通用特征方面，人工翻译与机器翻译有显著差异；大语言模型翻译在创意文学翻译特定特征如重复性和风格分布上优于神经机器翻译，且与人工翻译更为接近。

Conclusion: 大语言模型翻译展现出在儿童文学翻译中较高的潜力，其表现与人工翻译在文体特征上较为接近，优于神经机器翻译。

Abstract: This study focuses on evaluating the performance of machine translations
(MTs) compared to human translations (HTs) in English-to-Chinese children's
literature translation (CLT) from a stylometric perspective. The research
constructs a Peter Pan corpus, comprising 21 translations: 7 human translations
(HTs), 7 large language model translations (LLMs), and 7 neural machine
translation outputs (NMTs). The analysis employs a generic feature set
(including lexical, syntactic, readability, and n-gram features) and a creative
text translation (CTT-specific) feature set, which captures repetition, rhythm,
translatability, and miscellaneous levels, yielding 447 linguistic features in
total.
  Using classification and clustering techniques in machine learning, we
conduct a stylometric analysis of these translations. Results reveal that in
generic features, HTs and MTs exhibit significant differences in conjunction
word distributions and the ratio of 1-word-gram-YiYang, while NMTs and LLMs
show significant variation in descriptive words usage and adverb ratios.
Regarding CTT-specific features, LLMs outperform NMTs in distribution, aligning
more closely with HTs in stylistic characteristics, demonstrating the potential
of LLMs in CLT.

</details>


### [174] [Decoding Machine Translationese in English-Chinese News: LLMs vs. NMTs](https://arxiv.org/abs/2506.22050)
*Delu Kong,Lieve Macken*

Main category: cs.CL

TL;DR: 研究探讨机器翻译输出的语言特性，在英译中新闻文本中验证了翻译特性，并分析了不同翻译模型的语言特点差异。


<details>
  <summary>Details</summary>
Motivation: 研究机器翻译特性（MTese），特别是针对较少研究的英译中文领域，以辨别不同翻译模型的输出特性。

Method: 构建包括4个子语料库的大型数据集，采用五层特征集合，并使用卡方排名算法进行分类及聚类任务中的特征选择。

Result: 发现原始中文文本与机器翻译输出（NMT及LLM）可明显区分，机器翻译输出表现出较短句子长度及更多对比连词的使用。分类任务中区分LLM与NMT模型的准确率约为70%，LLM词汇多样性更高，而NMT更常用括号。

Conclusion: 研究证实了机器翻译特性在不同翻译模型中的存在，并对这些模型的语言特性差异提供了深入分析，同时发现中外开发的LLM无显著差异。

Abstract: This study explores Machine Translationese (MTese) -- the linguistic
peculiarities of machine translation outputs -- focusing on the
under-researched English-to-Chinese language pair in news texts. We construct a
large dataset consisting of 4 sub-corpora and employ a comprehensive five-layer
feature set. Then, a chi-square ranking algorithm is applied for feature
selection in both classification and clustering tasks. Our findings confirm the
presence of MTese in both Neural Machine Translation systems (NMTs) and Large
Language Models (LLMs). Original Chinese texts are nearly perfectly
distinguishable from both LLM and NMT outputs. Notable linguistic patterns in
MT outputs are shorter sentence lengths and increased use of adversative
conjunctions. Comparing LLMs and NMTs, we achieve approximately 70%
classification accuracy, with LLMs exhibiting greater lexical diversity and
NMTs using more brackets. Additionally, translation-specific LLMs show lower
lexical diversity but higher usage of causal conjunctions compared to generic
LLMs. Lastly, we find no significant differences between LLMs developed by
Chinese firms and their foreign counterparts.

</details>


### [175] [Lost at the Beginning of Reasoning](https://arxiv.org/abs/2506.22058)
*Baohao Liao,Xinyi Chen,Sara Rajaee,Yuhui Xu,Christian Herold,Anders Søgaard,Maarten de Rijke,Christof Monz*

Main category: cs.CL

TL;DR: 本文研究大语言模型（LLMs）在链式推理中的自我纠正能力，发现首步推理对最终结果有显著影响，并提出采样策略优化推理过程，同时引入新的基准评价模型的自我纠正能力。


<details>
  <summary>Details</summary>
Motivation: 旨在探讨LLMs链式推理中尚未充分研究的自我纠正能力以及优化首步推理对最终准确性的影响。

Method: 提出一种基于奖励模型的高效采样策略，筛选高质量的首步推理，抛弃次优解。同时开发新基准测试数据，用以评估模型自我纠正能力。

Result: 所提出的采样策略在不牺牲准确性的前提下，将推理成本减少了70%。

Conclusion: 研究阐明了首步推理在LLMs推理中的关键作用，所提出的方法可以提高推理效率并为未来鲁棒性推理研究奠定基础。

Abstract: Recent advancements in large language models (LLMs) have significantly
advanced complex reasoning capabilities, particularly through extended
chain-of-thought (CoT) reasoning that incorporates mechanisms such as
backtracking, self-reflection and self-correction. Despite these developments,
the self-correction abilities of LLMs during long CoT reasoning remain
underexplored. And recent findings on overthinking suggest that such models
often engage in unnecessarily redundant reasoning. In this work, we empirically
show that the first reasoning step exerts a disproportionately large influence
on the final prediction - errors introduced at this stage can substantially
degrade subsequent reasoning quality. This phenomenon is consistently observed
across two state-of-the-art open-source reasoning model families: DeepSeek-R1
and Qwen3. To address this, we propose an efficient sampling strategy that
leverages a reward model to identify and retain high-quality first reasoning
steps while discarding suboptimal ones, achieving up to a 70% reduction in
inference cost without sacrificing accuracy. Finally, we introduce a new
benchmark specifically constructed with deliberately flawed first reasoning
steps to systematically evaluate model self-correction capabilities, offering a
foundation for future research on robust reasoning in LLMs.

</details>


### [176] [MDC-R: The Minecraft Dialogue Corpus with Reference](https://arxiv.org/abs/2506.22062)
*Chris Madge,Maris Camilleri,Paloma Carretero Garcia,Mladen Karan,Juexi Shao,Prashant Jayannavar,Julian Hough,Benjamin Roth,Massimo Poesio*

Main category: cs.CL

TL;DR: 本文引入了具有指代注释的Minecraft对话语料库（MDC-R），并分析其数据和用途。


<details>
  <summary>Details</summary>
Motivation: 在动态环境下的任务导向对话设置中，探索指代语言现象的注释资源需求。

Method: 对原始MDC进行了专家注释，补充了指示性和回指性的指代注释，并进行定量和定性分析。

Result: 生成了新的对话语料库MDC-R，并通过实验展示其在理解指代表达中的有用性。

Conclusion: MDC-R成为研究语言指代现象的有价值资源，展现出其研究潜力与实际应用。

Abstract: We introduce the Minecraft Dialogue Corpus with Reference (MDC-R). MDC-R is a
new language resource that supplements the original Minecraft Dialogue Corpus
(MDC) with expert annotations of anaphoric and deictic reference. MDC's
task-orientated, multi-turn, situated dialogue in a dynamic environment has
motivated multiple annotation efforts, owing to the interesting linguistic
phenomena that this setting gives rise to. We believe it can serve as a
valuable resource when annotated with reference, too. Here, we discuss our
method of annotation and the resulting corpus, and provide both a quantitative
and a qualitative analysis of the data. Furthermore, we carry out a short
experiment demonstrating the usefulness of our corpus for referring expression
comprehension.

</details>


### [177] [Involvement drives complexity of language in online debates](https://arxiv.org/abs/2506.22098)
*Eleonora Amadori,Daniele Cirulli,Edoardo Di Martino,Jacopo Nudo,Maria Sahakyan,Emanuele Sangiorgio,Arnaldo Santoro,Simon Zollo,Alessandro Galeazzi,Niccolò Di Marco*

Main category: cs.CL

TL;DR: 本研究分析了推特上与新冠病毒、COP26和俄乌战争相关主题内容的语言复杂性，发现四个关键维度（账号类别、政治倾向、内容可靠性、情感）存在显著语言差异。


<details>
  <summary>Details</summary>
Motivation: 分析用户生成内容的语言特征以理解其社会影响，并探讨在数字平台上语言如何反映意识形态与社会结构。

Method: 结合多种文本复杂性指标，分析推特上的关键主题内容，关注账号类别、政治倾向、内容可靠性及情感这四个维度对语言使用的影响。

Result: 发现文本复杂性在个人与组织、偏向性强与中立的政治观点、高和低可靠性评分的账户之间存在显著差异；而更负面和有冒犯性的内容包含更复杂语言，相似政治立场和可靠性用户趋向使用共同术语。

Conclusion: 研究揭示了数字平台社会语言学动态，增进了对语言如何在在线环境中反映意识形态和社会结构的理解。

Abstract: Language is a fundamental aspect of human societies, continuously evolving in
response to various stimuli, including societal changes and intercultural
interactions. Technological advancements have profoundly transformed
communication, with social media emerging as a pivotal force that merges
entertainment-driven content with complex social dynamics. As these platforms
reshape public discourse, analyzing the linguistic features of user-generated
content is essential to understanding their broader societal impact. In this
paper, we examine the linguistic complexity of content produced by influential
users on Twitter across three globally significant and contested topics:
COVID-19, COP26, and the Russia-Ukraine war. By combining multiple measures of
textual complexity, we assess how language use varies along four key
dimensions: account type, political leaning, content reliability, and
sentiment. Our analysis reveals significant differences across all four axes,
including variations in language complexity between individuals and
organizations, between profiles with sided versus moderate political views, and
between those associated with higher versus lower reliability scores.
Additionally, profiles producing more negative and offensive content tend to
use more complex language, with users sharing similar political stances and
reliability levels converging toward a common jargon. Our findings offer new
insights into the sociolinguistic dynamics of digital platforms and contribute
to a deeper understanding of how language reflects ideological and social
structures in online spaces.

</details>


### [178] [DAPFAM: A Domain-Aware Patent Retrieval Dataset Aggregated at the Family Level](https://arxiv.org/abs/2506.22141)
*Iliass Ayaou,Denis Cavallucci,Hicham Chibane*

Main category: cs.CL

TL;DR: 本文提出了一个名为DAPFAM的新型专利检索数据集，以弥补现有公开专利检索数据集在领域内外标注、多法域覆盖、平衡领域查询表示及适中规模等方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有专利检索数据集在领域标注、法域覆盖及在中小型计算资源条件下进行子文档级实验等方面存在不足，难以满足实际需求。

Method: 提出并构建了DAPFAM数据集，采用简单家族级结构，结合国际专利分类（IPC）代码设计领域内外标注方案。数据集包括明确的相关性判断和领域关系，以及多法域覆盖。其构建使用了三步数据整理流程，并进行了基准实验。

Result: 数据集中包含1,247个领域平衡的完整文本查询家族和45,336个完整文本目标家族，生成了49,869个评估对。基准实验展示了跨领域专利检索的显著挑战。

Conclusion: DAPFAM数据集填补了现有数据集的空白，为跨领域专利检索研究提供了重要支持，同时其规模适合有限资源下的实验。

Abstract: In the landscape of publicly available patent retrieval datasets, the need
for explicit indomain and out-of-domain labeling, multi-jurisdiction coverage,
balanced query domain representation and manageable sizes that support sub
document level experiments on moderate computational resources is often
overlooked. To address these gaps, we propose DAPFAM, a new open access
domain-aware patent retrieval dataset constructed at the simple-family level.
The dataset contains 1,247 domain balanced full text query families and 45,336
full text target families. The dataset is enriched by clear relevance judgments
(forward/backward citations as positive links, random negatives), as well as
explicit in-domain or out-of-domain relationships via a novel proposed
labelling scheme based on via International Patent Classification (IPC) codes,
resulting in 49,869 evaluation pairs. The dataset is multi jurisdictional,
requires little to no preprocessing for retrieval evaluation, and remains of a
size manageable for entities with limited ressources allowing for sub document
level retrieval experiments without excessive computational costs. We describe
our three-step data-curation pipeline, present comprehensive dataset
statistics, and provide baseline experiments using lexical and neural retrieval
methods. Our baseline experiments highlight significant challenges in
crossdomain patent retrieval. The dataset will be publicly available (for now
the access link is this repository:
https://osf.io/vbyzd/?view_only=1a40242e0d1941a58aa854af3e50cf6b).

</details>


### [179] [SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in Low-Resource Arabic-English Code-Switched Speech Recognition](https://arxiv.org/abs/2506.22143)
*Muhammad Umar Farooq,Oscar Saz*

Main category: cs.CL

TL;DR: 论文研究了方言阿拉伯语和阿拉伯语-英语代码混合的语音识别性能。提出了生成合成数据的新方法，及增广模型泛化能力的策略。


<details>
  <summary>Details</summary>
Motivation: 研究针对处理方言阿拉伯语和阿拉伯语-英语代码混合语音识别中的数据匮乏和泛化问题。

Method: 采用修改版的音频拼接方法生成合成CS语音数据，并引入经验重播技巧提升模型泛化能力，同时集成外部语言模型。

Result: 在阿拉伯语和英语代码切换基准测试中，WER绝对改进7.8%。结合3-gram语言模型，WER从31.7%降至26.6%，并通过少样本学习将此进一步提升4.9%。

Conclusion: 提出的方法在小规模模型下超越了多个大规模多语言模型，尤其在阿拉伯语-英语代码切换语音识别中取得重大进展。

Abstract: This paper investigates the performance of various speech SSL models on
dialectal Arabic (DA) and Arabic-English code-switched (CS) speech. To address
data scarcity, a modified audio-splicing approach is introduced to generate
artificial CS speech data. Fine-tuning an already fine-tuned SSL model with the
proposed Spliced-Audio Generated (SAGE) data results in an absolute improvement
on Word Error Rate (WER) of 7.8% on Arabic and English CS benchmarks.
Additionally, an Experience Replay (ER) inspired approach is proposed to
enhance generalisation across DA and CS speech while mitigating catastrophic
forgetting. Integrating an out-of-domain 3-gram language model reduces the
overall mean WER from 31.7% to 26.6%. Few-shot fine-tuning for code-switching
benchmarks further improves WER by 4.9%. A WER of 31.1% on Arabic-English CS
benchmarks surpasses large-scale multilingual models, including USM and
Whisper-large-v2 (both over ten times larger) by an absolute margin of 5.5% and
8.4%, respectively.

</details>


### [180] [Training Language Model to Critique for Better Refinement](https://arxiv.org/abs/2506.22157)
*Tianshu Yu,Chao Xiang,Mingchuan Yang,Pei Ke,Bosi Wen,Cunxiang Wang,Jiale Cheng,Li Zhang,Xinyu Mu,Chuxiong Sun,Minlie Huang*

Main category: cs.CL

TL;DR: 文章提出了一种新的框架RCO（Refinement-oriented Critique Optimization），用于训练批判模型，通过改进信号指导模型优化响应。实验表明，RCO在对话生成、摘要、问答等任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型的批判类型以及如何生成有效批评的研究较少，本文旨在弥补这一研究空白。

Method: 文章提出RCO框架，通过批评模型生成的反馈来优化模型响应，并引入评价批评实用性的指标（CU）作为奖励信号指导训练。

Result: RCO在五个任务（对话生成、摘要、问答、数学推理、代码生成）中显著优于传统方法和开源模型。

Conclusion: RCO有效提升了语言模型批评-改进循环的质量，对改进模型表现具有重要意义。

Abstract: Large language models (LLMs) have demonstrated remarkable evaluation and
critique capabilities, providing insightful feedback and identifying flaws in
various tasks. However, limited research has explored which types of critiques
are most effective for improving model responses or how to generate such
critiques. To address this gap, we introduce \textbf{R}efinement-oriented
\textbf{C}ritique \textbf{O}ptimization (RCO), a novel framework designed to
train critic models using refinement signals. RCO uses a feedback loop where
critiques, generated by the critic model, guide the actor model in refining its
responses. The critique utility (CU) quantifies the effectiveness of these
refinements, serving as the reward signal for training the critic model. By
focusing on critiques that lead to better refinements, RCO eliminates the need
for direct critique preference assessment, ensuring that critiques driving
meaningful improvements are rewarded. We evaluate RCO across five tasks, i.e.,
dialog generation, summarization, question answering, mathematical reasoning,
and code generation, and show that it significantly outperforms traditional
methods and open-source models in terms of critique quality and refinement
outcomes. Our contributions include the introduction of RCO, a novel
supervision scheme based on refined response preferences, and comprehensive
experimental results that highlight the method's effectiveness in enhancing LLM
critique-refinement loops.

</details>


### [181] [Leveraging In-Context Learning for Political Bias Testing of LLMs](https://arxiv.org/abs/2506.22232)
*Patrick Haller,Jannis Vamvas,Rico Sennrich,Lena A. Jäger*

Main category: cs.CL

TL;DR: 提出了一种利用人类问卷数据为上下文的新方法来评估大模型偏差，相比直接提问的方法，其稳定性更高并适用性更广。


<details>
  <summary>Details</summary>
Motivation: 现有通过提问评估大模型政治偏差的方法稳定性较差，对比模型结果不可靠，因此需要一种更稳定的评估方式。

Method: 提出了一种称为问卷建模(QM)的新方法，将人类问卷数据用作上下文示例，以改进基于问题的偏差评估。

Result: 实验表明，问卷建模改善了评估稳定性，不同模型间的偏差比较更为可行。并发现，指令调优会影响模型偏差方向，且较大的模型更善于利用上下文信息并展现较小的偏差分数。

Conclusion: 问卷建模是一种比直接提问更稳定的偏差评估方法，同时强调模型规模和指令调优在偏差问题上的重要性。

Abstract: A growing body of work has been querying LLMs with political questions to
evaluate their potential biases. However, this probing method has limited
stability, making comparisons between models unreliable. In this paper, we
argue that LLMs need more context. We propose a new probing task, Questionnaire
Modeling (QM), that uses human survey data as in-context examples. We show that
QM improves the stability of question-based bias evaluation, and demonstrate
that it may be used to compare instruction-tuned models to their base versions.
Experiments with LLMs of various sizes indicate that instruction tuning can
indeed change the direction of bias. Furthermore, we observe a trend that
larger models are able to leverage in-context examples more effectively, and
generally exhibit smaller bias scores in QM. Data and code are publicly
available.

</details>


### [182] [Detection of Personal Data in Structured Datasets Using a Large Language Model](https://arxiv.org/abs/2506.22305)
*Albert Agisha Ntwali,Luca Rück,Martin Heckmann*

Main category: cs.CL

TL;DR: 该论文提出一种使用GPT-4o模型检测结构化数据集中个人数据的新方法，结合上下文信息并在多个数据集上表现出卓越的检测能力。


<details>
  <summary>Details</summary>
Motivation: 当前方法在检测结构化数据中的个人数据时存在局限性，缺乏对上下文信息的利用。

Method: 基于GPT-4o语言模型，结合数据集中特征名称、值及上下文信息；创新性地将数据集的描述信息和特征名称的关联纳入考量。

Result: 实验结果显示，在使用包含上下文信息的方法时，GPT-4o方法在开放数据集（比如Kaggle和OpenML）上的表现显著优于其他模型，在医疗数据集上的表现也保持强劲，验证了方法的可行性与优越性。

Conclusion: 作者认为，要进一步推动该领域进步，需要更多包含个人信息的真实数据集支持研究。

Abstract: We propose a novel approach for detecting personal data in structured
datasets, leveraging GPT-4o, a state-of-the-art Large Language Model. A key
innovation of our method is the incorporation of contextual information: in
addition to a feature's name and values, we utilize information from other
feature names within the dataset as well as the dataset description. We compare
our approach to alternative methods, including Microsoft Presidio and CASSED,
evaluating them on multiple datasets: DeSSI, a large synthetic dataset,
datasets we collected from Kaggle and OpenML as well as MIMIC-Demo-Ext, a
real-world dataset containing patient information from critical care units.
  Our findings reveal that detection performance varies significantly depending
on the dataset used for evaluation. CASSED excels on DeSSI, the dataset on
which it was trained. Performance on the medical dataset MIMIC-Demo-Ext is
comparable across all models, with our GPT-4o-based approach clearly
outperforming the others. Notably, personal data detection in the Kaggle and
OpenML datasets appears to benefit from contextual information. This is
evidenced by the poor performance of CASSED and Presidio (both of which do not
utilize the context of the dataset) compared to the strong results of our
GPT-4o-based approach.
  We conclude that further progress in this field would greatly benefit from
the availability of more real-world datasets containing personal information.

</details>


### [183] [Evaluating Scoring Bias in LLM-as-a-Judge](https://arxiv.org/abs/2506.22316)
*Qingquan Li,Shaoyu Dou,Kailai Shao,Chao Chen,Haixiang Hu*

Main category: cs.CL

TL;DR: 研究探讨大型语言模型（LLM）作为评判工具（LLM-as-a-Judge）的表现及其评分偏差问题，提出框架评估并提供减轻偏差的方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为评判工具广泛应用，但存在偏差问题影响其公平性和可靠性；目前对评分偏差的系统性研究有限。

Method: 定义评分偏差并设计评估框架，通过数据合成扩展评估数据集，开发多维评价指标并进行了实验验证。

Result: 实验表明现有评判模型的评分稳定性受到偏差影响，并深入探讨了提示模板设计等减轻偏差的策略。

Conclusion: 研究揭示了评判模型评分偏差的重要性，提供了评估和改进偏差的有益见解。

Abstract: The remarkable performance of Large Language Models (LLMs) gives rise
to``LLM-as-a-Judge'', where LLMs are employed as evaluators for complex tasks.
Moreover, it has been widely adopted across fields such as Natural Language
Processing (NLP), preference learning, and various specific domains. However,
there are various biases within LLM-as-a-Judge, which adversely affect the
fairness and reliability of judgments. Current research on evaluating or
mitigating bias in LLM-as-a-Judge predominantly focuses on comparison-based
evaluations, while systematic investigations into bias in scoring-based
evaluations remain limited. Therefore, we define scoring bias in LLM-as-a-Judge
as the scores differ when scoring judge models are bias-related perturbed, and
provide a well-designed framework to comprehensively evaluate scoring bias. We
augment existing LLM-as-a-Judge benchmarks through data synthesis to construct
our evaluation dataset and design multi-faceted evaluation metrics. Our
experimental results demonstrate that the scoring stability of existing judge
models is disrupted by scoring biases. Further exploratory experiments and
discussions provide valuable insights into the design of scoring prompt
templates and the mitigation of scoring biases on aspects such as score
rubrics, score IDs, and reference answer selection.

</details>


### [184] [Why Are Parsing Actions for Understanding Message Hierarchies Not Random?](https://arxiv.org/abs/2506.22366)
*Daichi Kato,Ryo Ueda,Yusuke Miyao*

Main category: cs.CL

TL;DR: 研究探讨了在人类语言理解中，随机解析是否能维持高通信准确性的条件，并引入层次结构和意外项影响进行评估。


<details>
  <summary>Details</summary>
Motivation: 研究尝试解答为什么人类解析策略不同于随机模式，以及在随机解析下如何依然能实现高通信准确性。

Method: 通过引入具有层次结构的复杂输入和包含意外相关项的目标函数，对解析策略的通信准确性进行测试。

Result: 在引入修改的实验设定下，验证了随机解析策略是否仍然可以实现高通信准确性。

Conclusion: 改进后的实验展示了语义结构和预测因素对解析策略和高通信表现的重要影响。

Abstract: If humans understood language by randomly selecting parsing actions, it might
have been necessary to construct a robust symbolic system capable of being
interpreted under any hierarchical structure. However, human parsing strategies
do not seem to follow such a random pattern. Why is that the case? In fact, a
previous study on emergent communication using models with hierarchical biases
have reported that agents adopting random parsing
strategies$\unicode{x2013}$ones that deviate significantly from human language
comprehension$\unicode{x2013}$can achieve high communication accuracy. In this
study, we investigate this issue by making two simple and natural modifications
to the experimental setup: (I) we use more complex inputs that have
hierarchical structures, such that random parsing makes semantic interpretation
more difficult, and (II) we incorporate a surprisal-related term, which is
known to influence the order of words and characters in natural language, into
the objective function. With these changes, we evaluate whether agents
employing random parsing strategies still maintain high communication accuracy.

</details>


### [185] [QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization](https://arxiv.org/abs/2506.22396)
*Danush Khanna,Aditya Kumar Guru,Srivarshinee Sridhar,Zidan Ahmed,Rubhav Bahirwani,Meetu Malhotra,Vinija Jain,Aman Chadha,Amitava Das,Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: QuickSilver框架优化大型语言模型推理效率，无需更改模型结构，通过动态标记停止、KV缓存跳过和上下文标记融合机制，显著减少计算负担，同时影响微小。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理阶段占用了大部分延时和能耗成本，目前缺乏无需重训练或改变架构的优化方法。

Method: 提出了QuickSilver框架，通过动态标记停止、KV缓存跳过和上下文标记融合等四种机制，在不改变模型权重的情况下实现推理效率提升。

Result: 在GPT-2和Llama-2模型上测试，QuickSilver在保证微小困惑度下降（<=0.2）的同时，实现了最高39.6%的FLOP减少。

Conclusion: QuickSilver是一种无需改变模型架构即可优化推理效率的切实可行的方法，适用于冻结和密集模型环境。

Abstract: Inference accounts for the majority of latency and energy consumption in
large language model (LLM) deployments, often exceeding 90% of total cost.
While training-time efficiency has seen extensive progress, runtime
optimization remains a key bottleneck, particularly under autoregressive
decoding. Existing approaches -- such as pruning, quantization, early exits,
and speculative decoding -- often require retraining, architectural changes, or
disrupt decoding compatibility. We introduce QuickSilver, a modular,
token-level framework that enables semantic adaptivity at inference time
without altering model weights or structure. QuickSilver integrates four
synergistic mechanisms:
  (i) Dynamic Token Halting, which halts computation for tokens with converged
representations; (ii) KV Cache Skipping, which selectively suppresses memory
writes to reduce attention overhead; and (iii) Contextual Token Fusion, which
collapses redundant tokens into shared paths to shrink sequence length.
  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on
frozen, dense models and requires no auxiliary networks. Applied to GPT-2 and
Llama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP
reduction with negligible perplexity degradation (<=0.2).

</details>


### [186] [Refining Czech GEC: Insights from a Multi-Experiment Approach](https://arxiv.org/abs/2506.22402)
*Petr Pechman,Milan Straka,Jana Straková,Jakub Náplava*

Main category: cs.CL

TL;DR: 本文提出了一种针对捷克语的语法错误校正（GEC）系统，通过Transformer架构的神经网络翻译方法达到最新水平。


<details>
  <summary>Details</summary>
Motivation: 开发一个针对捷克语的高性能GEC系统，以满足语言特定的校正需求。

Method: 采用Transformer模型并引入实时合成生成管道，通过引入语言无关和捷克语言特定的错误动态增强句子，同时研究多种错误生成策略、语料域平衡和模型细化方法。

Result: 实验表明，最佳模型无论是在性能还是计算效率上均表现突出，同时验证了大规模语言模型在捷克语GEC中的应用潜力。

Conclusion: 该系统在捷克语GEC中表现优越，可用于实际应用，并公开了代码和预训练模型以供研究和使用。

Abstract: We present a grammar error correction (GEC) system that achieves state of the
art for the Czech language. Our system is based on a neural network translation
approach with the Transformer architecture, and its key feature is its
real-time synthetic generation pipeline, which dynamically augments sentences
with artificial errors by introducing both language-agnostic and Czech-specific
errors. We conduct a comprehensive series of experiments, investigating the
Czech GEC corpora as bases for synthetic error introduction, several error
generation strategies, domain balancing, tokenization granularity, model size,
and data scaling during fine-tuning. Additionally, we evaluate the performance
of large language models (LLMs) on Czech GEC in both end-user and expert
fine-tuning scenarios. Our best-performing model is superior both in
performance and computational efficiency. The source code and the trained model
links are available on https://github.com/ufal/tsd2025-gec.

</details>


### [187] [HyperCLOVA X THINK Technical Report](https://arxiv.org/abs/2506.22403)
*NAVER Cloud HyperCLOVA X Team*

Main category: cs.CL

TL;DR: 本文介绍了HyperCLOVA X THINK，这是HyperCLOVA X系列中第一个专注推理的大型语言模型，具有高效的性能和适合韩语的特色。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够处理韩语和英语的推理性强的大型语言模型，填补以韩语为核心的高质量AI解决方案空缺。

Method: 模型基于Peri-LN Transformer结构，使用μP缩放；预训练使用三阶段课程扩展到128K上下文窗口，随后通过监督微调和强化学习优化；并引入视觉增强版本，同时应用剪枝和模型蒸馏技术。

Result: 模型在多项韩国基准测试中表现优越，如KMMLU、CSAT等，并在KCSAT STEM测评中达到甚至超越GPT-4.1的水平，且训练计算成本较低。

Conclusion: HyperCLOVA X THINK展现了在韩语AI创新中的重要作用，并为全球研究社区提供了宝贵资源。

Abstract: We introduce HyperCLOVA X THINK, the first reasoning-focused large language
model in the HyperCLOVA X family, pre-trained on roughly $6$ trillion
high-quality Korean, and English tokens, augmented with targeted synthetic
Korean data. It was implemented as a compute-memory-balanced Peri-LN
Transformer scaled with $\mu$P, pre-trained through a three-stage curriculum
that expands the context window to $128$K tokens, and post-trained via
supervised fine-tuning with Reinforcement Learning from Verifiable Rewards
supports both detailed rationale and concise-answer modes. It delivers
competitive performance against similarly sized models on Korea-focused
benchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0, and KoBigBench, while
preserving robust bilingual consistency and translation quality. In addition, a
vision-augmented variant matches or exceeds GPT-4.1 on the KCSAT STEM
benchmark, all of which are achieved with substantially lower training compute
than existing models of similar sizes. We also present a pruning and
distillation technique that will soon be applied to HyperCLOVA X THINK for an
open-source and business-friendly foundation model. Altogether, these
capabilities position HyperCLOVA X THINK as a robust foundation for Korean AI
innovation and a valuable resource for the global research community.

</details>


### [188] [Sequential Diagnosis with Language Models](https://arxiv.org/abs/2506.22405)
*Harsha Nori,Mayank Daswani,Christopher Kelly,Scott Lundberg,Marco Tulio Ribeiro,Marc Wilson,Xiaoxuan Liu,Viknesh Sounderajah,Jonathan Carlson,Matthew P Lungren,Bay Gross,Peter Hames,Mustafa Suleyman,Dominic King,Eric Horvitz*

Main category: cs.CL

TL;DR: 本文引入了一个名为Sequential Diagnosis Benchmark的诊断框架，用于模拟临床诊断中的迭代思考过程；提出了MAI-DxO系统，在多个模型中表现出高诊断准确率和成本效率。


<details>
  <summary>Details</summary>
Motivation: 目前的语言模型评估方法通常使用静态场景和选择题，这远未能反映真实医疗场景中基于证据的复杂推理过程。研究旨在开发和测试一种帮助AI更好模拟临床医生迭代诊断过程的新工具。

Method: 提出了Sequential Diagnosis Benchmark，将304个复杂病例分解为逐步诊断式的交互过程；引入了MAI-DxO，一个模拟医生小组、提出差异性诊断并选择高效测试的模型无关系统。

Result: MAI-DxO结合OpenAI o3模型实现了80%的诊断准确率（比普通医生高4倍），诊断成本相比普通医生减少20%，比未经优化的o3减少70%；在最大化精度的模式下，诊断准确率提高到85.5%。

Conclusion: 这种基于共享迭代思维和谨慎行动指导下的AI诊断系统可以提升临床诊断的准确性和成本效益，且在主流AI模型中广泛适用。

Abstract: Artificial intelligence holds great promise for expanding access to expert
medical knowledge and reasoning. However, most evaluations of language models
rely on static vignettes and multiple-choice questions that fail to reflect the
complexity and nuance of evidence-based medicine in real-world settings. In
clinical practice, physicians iteratively formulate and revise diagnostic
hypotheses, adapting each subsequent question and test to what they've just
learned, and weigh the evolving evidence before committing to a final
diagnosis. To emulate this iterative process, we introduce the Sequential
Diagnosis Benchmark, which transforms 304 diagnostically challenging New
England Journal of Medicine clinicopathological conference (NEJM-CPC) cases
into stepwise diagnostic encounters. A physician or AI begins with a short case
abstract and must iteratively request additional details from a gatekeeper
model that reveals findings only when explicitly queried. Performance is
assessed not just by diagnostic accuracy but also by the cost of physician
visits and tests performed. We also present the MAI Diagnostic Orchestrator
(MAI-DxO), a model-agnostic orchestrator that simulates a panel of physicians,
proposes likely differential diagnoses and strategically selects high-value,
cost-effective tests. When paired with OpenAI's o3 model, MAI-DxO achieves 80%
diagnostic accuracy--four times higher than the 20% average of generalist
physicians. MAI-DxO also reduces diagnostic costs by 20% compared to
physicians, and 70% compared to off-the-shelf o3. When configured for maximum
accuracy, MAI-DxO achieves 85.5% accuracy. These performance gains with MAI-DxO
generalize across models from the OpenAI, Gemini, Claude, Grok, DeepSeek, and
Llama families. We highlight how AI systems, when guided to think iteratively
and act judiciously, can advance diagnostic precision and cost-effectiveness in
clinical care.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [189] [SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents](https://arxiv.org/abs/2506.21669)
*Wanxin Tian,Shijie Zhang,Kevin Zhang,Xiaowei Chi,Yulin Luo,Junyu Lu,Chunkai Fan,Qiang Zhou,Yiming Zhao,Ning Liu Siyu Lin,Zhiyuan Qin,Xiaozhu Ju,Shanghang Zhang,Jian Tang*

Main category: cs.AI

TL;DR: 论文提出了一种名为SEEA-R1的框架，利用强化学习微调（RFT），实现了具备自我进化能力的多模态交互智能体。


<details>
  <summary>Details</summary>
Motivation: 在具身情境中执行复杂的长时任务需要智能体具备自我进化能力，但现有技术对多模态交互和奖励泛化的支持有限。

Method: 提出了树状组相对策略优化（Tree-GRPO）和多模态生成奖励模型（MGRM），解决稀疏奖励信号与任务泛化问题。

Result: 在ALFWorld基准测试中，SEEA-R1分别取得了85.07%（文本）和36.19%（多模态）的表现，超越当前最先进方法，包括GPT-4o。

Conclusion: SEEA-R1框架展示了其在自我进化具身智能领域的潜力，适用于未来更复杂任务的研究。

Abstract: Self-evolution, the ability of agents to autonomously improve their reasoning
and behavior, is essential for the embodied domain with long-horizon,
real-world tasks. Despite current advancements in reinforcement fine-tuning
(RFT) showing strong performance in enhancing reasoning in LLMs, its potential
to enable self-evolving embodied intelligence with multi-modal interactions
remains largely unexplored. Specifically, reinforcement fine-tuning faces two
fundamental obstacles in embodied settings: (i) the lack of accessible
intermediate rewards in multi-step reasoning tasks limits effective learning
signals, and (ii) reliance on hand-crafted reward functions restricts
generalization to novel tasks and environments. To address these challenges, we
present Self-Evolving Embodied Agents-R1, SEEA-R1, the first RFT framework
designed for enabling the self-evolving capabilities of embodied agents.
Specifically, to convert sparse delayed rewards into denser intermediate
signals that improve multi-step reasoning, we propose Tree-based group relative
policy optimization (Tree-GRPO), which integrates Monte Carlo Tree Search into
GRPO. To generalize reward estimation across tasks and scenes, supporting
autonomous adaptation and reward-driven self-evolution, we further introduce
Multi-modal Generative Reward Model (MGRM). To holistically evaluate the
effectiveness of SEEA-R1, we evaluate on the ALFWorld benchmark, surpassing
state-of-the-art methods with scores of 85.07% (textual) and 36.19%
(multi-modal), outperforming prior models including GPT-4o. SEEA-R1 also
achieves scores of 80.3% without environmental reward, surpassing all
open-source baselines and highlighting its scalability as a self-evolving
embodied agent. Additional experiments and qualitative analysis further support
the potential of SEEA-R1 for future research in scalable embodied intelligence.

</details>


### [190] [Hierarchical Reasoning Model](https://arxiv.org/abs/2506.21734)
*Guan Wang,Jin Li,Yuhao Sun,Xing Chen,Changling Liu,Yue Wu,Meng Lu,Sen Song,Yasin Abbasi Yadkori*

Main category: cs.AI

TL;DR: 本文提出了一种新型递归架构——分层推理模型（HRM），其能够高效处理复杂推理任务，展示了显著的性能优势。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型主要采用"思维链"技术进行推理，但存在任务分解脆弱、数据需求量大及高延迟等问题；作者希望通过类比人脑的分层多时间尺度处理，设计出一种更高效稳定的推理模型。

Method: HRM由两个相互依赖的递归模块组成：高层模块负责抽象规划，低层模块进行快速细致的计算。该模型只需要一次前向传播即可完成序列推理任务，无需对中间过程进行显式监督，同时仅需少量训练数据。

Result: HRM在多个复杂推理任务中表现优异：仅27M参数情况下在Sudoku复杂解题和大迷宫最优路径识别中几乎达到完美表现，并在人工智能ARC基准测试中超越了上下文窗口更长的大模型。

Conclusion: HRM为通用推理和计算系统的发展提供了新的方向，具有变革潜力。

Abstract: Reasoning, the process of devising and executing complex goal-oriented action
sequences, remains a critical challenge in AI. Current large language models
(LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from
brittle task decomposition, extensive data requirements, and high latency.
Inspired by the hierarchical and multi-timescale processing in the human brain,
we propose the Hierarchical Reasoning Model (HRM), a novel recurrent
architecture that attains significant computational depth while maintaining
both training stability and efficiency. HRM executes sequential reasoning tasks
in a single forward pass without explicit supervision of the intermediate
process, through two interdependent recurrent modules: a high-level module
responsible for slow, abstract planning, and a low-level module handling rapid,
detailed computations. With only 27 million parameters, HRM achieves
exceptional performance on complex reasoning tasks using only 1000 training
samples. The model operates without pre-training or CoT data, yet achieves
nearly perfect performance on challenging tasks including complex Sudoku
puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms
much larger models with significantly longer context windows on the Abstraction
and Reasoning Corpus (ARC), a key benchmark for measuring artificial general
intelligence capabilities. These results underscore HRM's potential as a
transformative advancement toward universal computation and general-purpose
reasoning systems.

</details>


### [191] [THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?](https://arxiv.org/abs/2506.21763)
*Xin Wang,Jiyao Liu,Yulong Xiao,Junzhi Ning,Lihao Liu,Junjun He,Botian Shi,Kaicheng Yu*

Main category: cs.AI

TL;DR: 本研究提出THE-Tree，一个从科学文献中构建领域特定技术演化树的框架，以解决AI生成科学提议验证缓慢的问题。


<details>
  <summary>Details</summary>
Motivation: 因AI生成提议增多，需解决其事实准确性和新颖性验证的难题；现有方法如单独使用LLM或传统引文网络有明显局限性。

Method: 提出THE-Tree框架，通过搜索算法探索进化路径，并引入“思考-表达-引用-验证”处理，结合LLM生成及检查机制，利用自然语言推理验证科学演化链路的逻辑一致性与证据支持。

Result: 实验表明：THE-Tree在图完成任务中比传统引文网络提升8%-14%性能；科学发展预测中提升约10%；结合其他方法后科学文献评估性能提升近100%。

Conclusion: THE-Tree有效提供结构化、可验证的科学演化数据，并显著提升科学发展预测及关键文献评估能力。发布含71k验证数据与27k论文的基准数据集，推动领域研究发展。

Abstract: Large Language Models (LLMs) are accelerating scientific idea generation, but
rigorously evaluating these numerous, often superficial, AI-generated
propositions for novelty and factual accuracy is a critical bottleneck; manual
verification is too slow.Existing validation methods are inadequate: LLMs as
standalone verifiers may hallucinate and lack domain knowledge (our findings
show ~60\% unawareness of relevant papers in specific domains), while
traditional citation networks lack explicit causality and narrative surveys are
unstructured.This underscores a core challenge: the absence of structured,
verifiable, and causally-linked historical data of scientific evolution.To
address this,we introduce \textbf{THE-Tree} (\textbf{T}echnology
\textbf{H}istory \textbf{E}volution Tree), a computational framework that
constructs such domain-specific evolution trees from scientific
literature.THE-Tree employs a search algorithm to explore evolutionary paths.
During its node expansion, it utilizes a novel "Think-Verbalize-Cite-Verify"
process: an LLM proposes potential advancements and cites supporting
literature. Critically, each proposed evolutionary link is then validated for
logical coherence and evidential support by a recovered natural language
inference mechanism that interrogates the cited literature, ensuring that each
step is grounded.We construct and validate 88 THE-Trees across diverse domains
and release a benchmark dataset including up to 71k fact verifications covering
27k papers to foster further research.Experiments demonstrate that i) in graph
completion, our THE-Tree improves hit@1 by 8\% to 14\% across multiple models
compared to traditional citation networks; ii) for predicting future scientific
developments, it improves hit@1 metric by nearly 10\%; and iii) when combined
with other methods, it boosts the performance of evaluating important
scientific papers by almost 100\%.

</details>


### [192] [MobiVerse: Scaling Urban Mobility Simulation with Hybrid Lightweight Domain-Specific Generator and Large Language Models](https://arxiv.org/abs/2506.21784)
*Yifan Liu,Xishun Liao,Haoxuan Ma,Jonathan Liu,Rohan Jadhav,Jiaqi Ma*

Main category: cs.AI

TL;DR: MobiVerse是一种混合框架，通过结合轻量级生成器和大型语言模型（LLMs），有效解决了当前人类出行模拟中的计算效率和适应性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的出行模拟平台在算法开发、政策实施和综合评估方面存在缺口，传统方法面临数据要求高、动态适应性弱等问题。

Method: 提出MobiVerse混合框架，结合轻量级生成器生成活动链和LLMs进行情景感知修改，并支持模块化设计进行多层次交通算法测试。

Result: 在洛杉矶Westwood的案例中，MobiVerse高效生成了53,000个个体的动态计划，并在多种环境反馈（如道路封闭、大型活动、交通拥堵）中展现了响应能力，兼顾计算效率与行为现实。

Conclusion: MobiVerse通过模块化和开放的设计，为出行系统规划与操作提供定制化平台，填补了模拟与实践的鸿沟。

Abstract: Understanding and modeling human mobility patterns is crucial for effective
transportation planning and urban development. Despite significant advances in
mobility research, there remains a critical gap in simulation platforms that
allow for algorithm development, policy implementation, and comprehensive
evaluation at scale. Traditional activity-based models require extensive data
collection and manual calibration, machine learning approaches struggle with
adaptation to dynamic conditions, and treding agent-based Large Language Models
(LLMs) implementations face computational constraints with large-scale
simulations. To address these challenges, we propose MobiVerse, a hybrid
framework leverages the efficiency of lightweight domain-specific generator for
generating base activity chains with the adaptability of LLMs for context-aware
modifications. A case study was conducted in Westwood, Los Angeles, where we
efficiently generated and dynamically adjusted schedules for the whole
population of approximately 53,000 agents on a standard PC. Our experiments
demonstrate that MobiVerse successfully enables agents to respond to
environmental feedback, including road closures, large gathering events like
football games, and congestion, through our hybrid framework. Its modular
design facilitates testing various mobility algorithms at both transportation
system and agent levels. Results show our approach maintains computational
efficiency while enhancing behavioral realism. MobiVerse bridges the gap in
mobility simulation by providing a customizable platform for mobility systems
planning and operations with benchmark algorithms. Code and videos are
available at https://github.com/ucla-mobility/MobiVerse.

</details>


### [193] [CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation](https://arxiv.org/abs/2506.21805)
*Nicolas Bougie,Narimasa Watanabe*

Main category: cs.AI

TL;DR: 提出了一种利用大语言模型开发的城市模拟器（CitySim），用于模拟人类在城市环境中的行为。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖僵化的手工规则，限制了对复杂意图、计划与自适应行为的逼真模拟能力。

Method: 通过结合大语言模型，CitySim的代理人采用递归的价值驱动方法生成日常计划，同时融入信念、长期目标与空间记忆以支持长期模拟。

Result: CitySim在微观和宏观层面都更真实地贴近人类行为，还能建模成千上万代理人的集体行为，进行例如群体密度估计、场所受欢迎度预测等实验。

Conclusion: CitySim是一个可扩展的、灵活的测试平台，有助于理解和预测城市现象。

Abstract: Modeling human behavior in urban environments is fundamental for social
science, behavioral studies, and urban planning. Prior work often rely on
rigid, hand-crafted rules, limiting their ability to simulate nuanced
intentions, plans, and adaptive behaviors. Addressing these challenges, we
envision an urban simulator (CitySim), capitalizing on breakthroughs in
human-level intelligence exhibited by large language models. In CitySim, agents
generate realistic daily schedules using a recursive value-driven approach that
balances mandatory activities, personal habits, and situational factors. To
enable long-term, lifelike simulations, we endow agents with beliefs, long-term
goals, and spatial memory for navigation. CitySim exhibits closer alignment
with real humans than prior work, both at micro and macro levels. Additionally,
we conduct insightful experiments by modeling tens of thousands of agents and
evaluating their collective behaviors under various real-world scenarios,
including estimating crowd density, predicting place popularity, and assessing
well-being. Our results highlight CitySim as a scalable, flexible testbed for
understanding and forecasting urban phenomena.

</details>


### [194] [Interactive Multi-Objective Probabilistic Preference Learning with Soft and Hard Bounds](https://arxiv.org/abs/2506.21887)
*Edward Chen,Sang T. Truong,Natalie Dullerud,Sanmi Koyejo,Carlos Guestrin*

Main category: cs.AI

TL;DR: 本文提出了Active-MoSH框架，通过整合软硬界限和概率偏好学习，在多目标决策中引导用户找到最适合的Pareto最优解，并利用全球敏感性分析提升决策者的信任感。


<details>
  <summary>Details</summary>
Motivation: 在高风险决策中，如放射治疗，面临多目标权衡和资源密集型的评估操作。现有方法难以系统性地迭代优化决策偏好设置，且决策者对最终方案是否最佳缺乏信任。

Method: 提出了Active-MoSH框架，包括局部组件和全局组件。局部部分通过概率性偏好学习和主动采样策略优化Pareto子集选择；全局部分利用多目标敏感性分析帮助识别潜在优越解以巩固信任。

Result: 实验表明，Active-MoSH框架在多种合成及实际应用中性能卓越，并通过用户研究验证，其能有效改善决策收敛性、增强信任感以及提高偏好表达能力。

Conclusion: Active-MoSH为高风险、多目标决策提供了兼顾局部优化与全局考量的系统框架，显著提升了决策效率与质量。

Abstract: High-stakes decision-making involves navigating multiple competing objectives
with expensive evaluations. For instance, in brachytherapy, clinicians must
balance maximizing tumor coverage (e.g., an aspirational target or soft bound
of >95% coverage) against strict organ dose limits (e.g., a non-negotiable hard
bound of <601 cGy to the bladder), with each plan evaluation being
resource-intensive. Selecting Pareto-optimal solutions that match implicit
preferences is challenging, as exhaustive Pareto frontier exploration is
computationally and cognitively prohibitive, necessitating interactive
frameworks to guide users. While decision-makers (DMs) often possess domain
knowledge to narrow the search via such soft-hard bounds, current methods often
lack systematic approaches to iteratively refine these multi-faceted preference
structures. Critically, DMs must trust their final decision, confident they
haven't missed superior alternatives; this trust is paramount in
high-consequence scenarios. We present Active-MoSH, an interactive local-global
framework designed for this process. Its local component integrates soft-hard
bounds with probabilistic preference learning, maintaining distributions over
DM preferences and bounds for adaptive Pareto subset refinement. This is guided
by an active sampling strategy optimizing exploration-exploitation while
minimizing cognitive burden. To build DM trust, Active-MoSH's global component,
T-MoSH, leverages multi-objective sensitivity analysis to identify potentially
overlooked, high-value points beyond immediate feedback. We demonstrate
Active-MoSH's performance benefits through diverse synthetic and real-world
applications. A user study on AI-generated image selection further validates
our hypotheses regarding the framework's ability to improve convergence,
enhance DM trust, and provide expressive preference articulation, enabling more
effective DMs.

</details>


### [195] [AlphaBeta is not as good as you think: a new probabilistic model to better analyze deterministic game-solving algorithms](https://arxiv.org/abs/2506.21996)
*Raphaël Boige,Amine Boumaza,Bruno Scherrer*

Main category: cs.AI

TL;DR: 提出一种新的概率模型，通过引入祖先依赖性生成具有调整难度的游戏树，并分析经典算法（如AlphaBeta和Scout）在此模型下的性能差异。


<details>
  <summary>Details</summary>
Motivation: 经典算法的分析依赖于独立分布假设的简化模型，该模型忽略了真实世界游戏的结构复杂性，从而产生无挑战的案例。为了改进这一点，提出了更现实的概率模型。

Method: 通过设定固定的层级条件分布，递增构造游戏树，保留公式解析能力，并对AlphaBeta和Scout算法的平均复杂性推导递归公式。

Result: 在深度有限的游戏树中，算法表现出现明显差异。AlphaBeta相较于Scout表现出更大的乘性因子，实际速度显著变慢。

Conclusion: 新的概率模型提供了更现实的环境，揭示了经典算法在复杂结构下的性能特性，深化了对这些方法的理解。

Abstract: Deterministic game-solving algorithms are conventionally analyzed in the
light of their average-case complexity against a distribution of random
game-trees, where leaf values are independently sampled from a fixed
distribution. This simplified model enables uncluttered mathematical analysis,
revealing two key properties: root value distributions asymptotically collapse
to a single fixed value for finite-valued trees, and all reasonable algorithms
achieve global optimality. However, these findings are artifacts of the model's
design-its long criticized independence assumption strips games of structural
complexity, producing trivial instances where no algorithm faces meaningful
challenges. To address this limitation, we introduce a new probabilistic model
that incrementally constructs game-trees using a fixed level-wise conditional
distribution. By enforcing ancestor dependency, a critical structural feature
of real-world games, our framework generates problems with adjustable
difficulty while retaining some form of analytical tractability. For several
algorithms, including AlphaBeta and Scout, we derive recursive formulas
characterizing their average-case complexities under this model. These allow us
to rigorously compare algorithms on deep game-trees, where Monte-Carlo
simulations are no longer feasible. While asymptotically, all algorithms seem
to converge to identical branching factor (a result analogous to those of
independence-based models), deep finite trees reveal stark differences:
AlphaBeta incurs a significantly larger constant multiplicative factor compared
to algorithms like Scout, leading to a substantial practical slowdown. Our
framework sheds new light on classical game-solving algorithms, offering
rigorous evidence and analytical tools to advance the understanding of these
methods under a more realistic, challenging, and yet tractable model.

</details>


### [196] [LeanConjecturer: Automatic Generation of Mathematical Conjectures for Theorem Proving](https://arxiv.org/abs/2506.22005)
*Naoto Onda,Kazumi Kasaura,Yuta Oriike,Masaya Taniguchi,Akiyoshi Sannai,Sho Sonoda*

Main category: cs.AI

TL;DR: 本文提出了LeanConjecturer，这是一个利用大型语言模型（LLMs）在Lean 4中自动生成大学数学猜想的管道系统。研究显示，该方法在生成新的数学猜想和增强定理证明能力上表现出显著效果。


<details>
  <summary>Details</summary>
Motivation: 解决形式定理证明中的数据稀缺问题，并通过生成新猜想提高数学发现的能力和定理证明系统的效果。

Method: 结合基于规则的上下文提取与LLM生成的定理陈述，利用迭代生成与评估过程，生成大量数学猜想，并将其应用于基于GRPO的强化学习训练中。

Result: 从40个Mathlib初始文件中生成了12,289个猜想，其中3,776个被认为是有效且非平凡的。该方法平均每个种子文件生成103.25个新猜想，并验证了一些拓扑学中的非平凡定理。

Conclusion: LeanConjecturer方法在生成新数学猜想和提高定理证明能力方面具有广阔的应用潜力，可作为拓展数学发现的新工具。

Abstract: We introduce LeanConjecturer, a pipeline for automatically generating
university-level mathematical conjectures in Lean 4 using Large Language Models
(LLMs). Our hybrid approach combines rule-based context extraction with
LLM-based theorem statement generation, addressing the data scarcity challenge
in formal theorem proving. Through iterative generation and evaluation,
LeanConjecturer produced 12,289 conjectures from 40 Mathlib seed files, with
3,776 identified as syntactically valid and non-trivial, that is, cannot be
proven by \texttt{aesop} tactic. We demonstrate the utility of these generated
conjectures for reinforcement learning through Group Relative Policy
Optimization (GRPO), showing that targeted training on domain-specific
conjectures can enhance theorem proving capabilities. Our approach generates
103.25 novel conjectures per seed file on average, providing a scalable
solution for creating training data for theorem proving systems. Our system
successfully verified several non-trivial theorems in topology, including
properties of semi-open, alpha-open, and pre-open sets, demonstrating its
potential for mathematical discovery beyond simple variations of existing
results.

</details>


### [197] [Universal Retrieval for Multimodal Trajectory Modeling](https://arxiv.org/abs/2506.22056)
*Xuan Zhang,Ziyan Jiang,Rui Meng,Yifei Leng,Zhenbang Xiao,Zora Zhiruo Wang,Yanyi Shang,Dehan Kong*

Main category: cs.AI

TL;DR: 研究展示了一个新的多模态轨迹检索方法（GAE-Retriever），利用统一数据集UATD和新提出的评价基准GAE-Bench，显著提高了检索效果。


<details>
  <summary>Details</summary>
Motivation: 由于轨迹数据的增长，建模轨迹级数据表示的挑战逐步显现，但现有方法并未系统解决这一问题，尤其是在多模态领域中。

Method: 提出了一个新的多模态检索框架GAE-Retriever，该框架采用视觉-语言模型，结合对比学习优化机制（包括Token选择和GradCache机制），并基于一个构建的统一代理轨迹数据集UATD以及相应的评估基准GAE-Bench进行验证。

Result: GAE-Retriever在多个数据集上的检索召回率显著优于强基线方法，证明了其在多模态轨迹检索中的有效性。

Conclusion: 提出的GAE-Retriever方法及相关基准数据集为解决轨迹数据建模和检索提供了新的思路，提升了AI代理在多模态环境中的应用能力。

Abstract: Trajectory data, capturing human actions and environmental states across
various modalities, holds significant potential for enhancing AI agent
capabilities, particularly in GUI environments. However, how to model the
representation of trajectory-level data presents a significant challenge that
has not been systematically addressed amid explosive trajectory data growth. In
this work, we introduce Multimodal Trajectory Retrieval, bridging the gap
between universal retrieval and agent-centric trajectory modeling. We construct
the Unified Agent Trajectory Dataset (UATD) from annotated demonstrations and
states across diverse real-world scenarios. Based on this, we present
GAE-Bench, a benchmark containing a large number of trajectory-based retrieval
pairs. In addition, we propose GAE-Retriever, a multimodal retrieval framework
that adopts vision-language models and incorporates optimized contrastive
learning through a token selection and the GradCache mechanism. Comprehensive
evaluations across multiple datasets show that GAE-Retriever consistently
outperforms strong baselines in retrieval recall, highlighting its
effectiveness in advancing multimodal trajectory retrieval.

</details>


### [198] [Query as Test: An Intelligent Driving Test and Data Storage Method for Integrated Cockpit-Vehicle-Road Scenarios](https://arxiv.org/abs/2506.22068)
*Shengyue Yao,Runqing Guo,Yangyang Qin,Miangbing Meng,Jipeng Cao,Yilun Lin,Yisheng Lv,Fei-Yue Wang*

Main category: cs.AI

TL;DR: 本论文提出了“查询即测试”（QaT）的理念，针对智能驾驶数据生态碎片化和测试方法不足的问题引入了基于ASP的扩展场景标注（ESN），同时提出引导开发的新思路——“验证驱动开发”（VDD）。


<details>
  <summary>Details</summary>
Motivation: AI在交通领域发展迅猛，但数据生态碎片化且测试方法覆盖不足，无法有效应对边界情况和灵活需求。

Method: 提出“查询即测试”（QaT），构建基于ASP的扩展场景标注（ESN），以统一表示驾驶生态中的异构数据，并支持灵活的语义查询、自然可解释性及隐私数据抽象。

Result: 通过ESN数据库实现高表达力和形式化测试验证，并设计出新的开发模式“验证驱动开发”，加速系统迭代与检验。

Conclusion: QaT和VDD等创新概念显著提升了测试的灵活性、全面性以及系统开发的效率，是对智能驾驶安全验证领域的重要贡献。

Abstract: With the deep penetration of Artificial Intelligence (AI) in the
transportation sector, intelligent cockpits, autonomous driving, and
intelligent road networks are developing at an unprecedented pace. However, the
data ecosystems of these three key areas are increasingly fragmented and
incompatible. Especially, existing testing methods rely on data stacking, fail
to cover all edge cases, and lack flexibility. To address this issue, this
paper introduces the concept of "Query as Test" (QaT). This concept shifts the
focus from rigid, prescripted test cases to flexible, on-demand logical queries
against a unified data representation. Specifically, we identify the need for a
fundamental improvement in data storage and representation, leading to our
proposal of "Extensible Scenarios Notations" (ESN). ESN is a novel declarative
data framework based on Answer Set Programming (ASP), which uniformly
represents heterogeneous multimodal data from the cockpit, vehicle, and road as
a collection of logical facts and rules. This approach not only achieves deep
semantic fusion of data, but also brings three core advantages: (1) supports
complex and flexible semantic querying through logical reasoning; (2) provides
natural interpretability for decision-making processes; (3) allows for
on-demand data abstraction through logical rules, enabling fine-grained privacy
protection. We further elaborate on the QaT paradigm, transforming the
functional validation and safety compliance checks of autonomous driving
systems into logical queries against the ESN database, significantly enhancing
the expressiveness and formal rigor of the testing. Finally, we introduce the
concept of "Validation-Driven Development" (VDD), which suggests to guide
developments by logical validation rather than quantitative testing in the era
of Large Language Models, in order to accelerating the iteration and
development process.

</details>


### [199] [A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety](https://arxiv.org/abs/2506.22183)
*Camille François,Ludovic Péran,Ayah Bdeir,Nouha Dziri,Will Hawkins,Yacine Jernite,Sayash Kapoor,Juliet Shen,Heidy Khlaaf,Kevin Klyman,Nik Marda,Marie Pellat,Deb Raji,Divya Siddarth,Aviya Skowron,Joseph Spisak,Madhulika Srikumar,Victor Storchan,Audrey Tang,Jen Weedon*

Main category: cs.AI

TL;DR: 本文分析了开放权重和开源基础模型迅速兴起对AI系统安全带来的挑战与机遇，提出了多项研究议程和技术改进方向。


<details>
  <summary>Details</summary>
Motivation: 分析并探讨开源与开放权重模型在AI系统安全中的优势与挑战，同时为未来的AI安全研究提供方向。

Method: 通过一个为期六周的跨领域参与式合作方案，汇聚学术、工业和政策领域专家，设立工作组产出研究议程、技术干预映射以及内容安全过滤生态系统的研发路线图。

Result: 提出了开放性推动安全性的关键优势，但指出了仍存在的技术和机制缺口，并提供了五项优先研究方向的建议。

Conclusion: 开放性可以通过透明性和多元化监督提升安全性，但需通过未来研究解决现存不足，为构建更加开放、协作和负责任的AI安全学科奠定基础。

Abstract: The rapid rise of open-weight and open-source foundation models is
intensifying the obligation and reshaping the opportunity to make AI systems
safe. This paper reports outcomes from the Columbia Convening on AI Openness
and Safety (San Francisco, 19 Nov 2024) and its six-week preparatory programme
involving more than forty-five researchers, engineers, and policy leaders from
academia, industry, civil society, and government. Using a participatory,
solutions-oriented process, the working groups produced (i) a research agenda
at the intersection of safety and open source AI; (ii) a mapping of existing
and needed technical interventions and open source tools to safely and
responsibly deploy open foundation models across the AI development workflow;
and (iii) a mapping of the content safety filter ecosystem with a proposed
roadmap for future research and development. We find that openness --
understood as transparent weights, interoperable tooling, and public governance
-- can enhance safety by enabling independent scrutiny, decentralized
mitigation, and culturally plural oversight. However, significant gaps persist:
scarce multimodal and multilingual benchmarks, limited defenses against
prompt-injection and compositional attacks in agentic systems, and insufficient
participatory mechanisms for communities most affected by AI harms. The paper
concludes with a roadmap of five priority research directions, emphasizing
participatory inputs, future-proof content filters, ecosystem-wide safety
infrastructure, rigorous agentic safeguards, and expanded harm taxonomies.
These recommendations informed the February 2025 French AI Action Summit and
lay groundwork for an open, plural, and accountable AI safety discipline.

</details>


### [200] [Breaking Rank Bottlenecks in Knowledge Graph Completion](https://arxiv.org/abs/2506.22271)
*Samy Badreddine,Emile van Krieken,Luciano Serafini*

Main category: cs.AI

TL;DR: 本研究提出了一种混合输出层（KGE-MoS），以克服知识图谱补全（KGC）模型在输出层中的秩瓶颈问题，并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 许多知识图谱补全模型因输出层的秩瓶颈限制，无法充分表达模型能力，导致排名准确性和分布一致性下降。

Method: 通过借鉴语言建模领域的方法，提出了一种基于混合的输出层（KGE-MoS），以替换传统的向量-矩阵乘法评分机制。

Result: 在四个数据集上的实验表明，KGE-MoS能够以较低的参数成本提升KGC模型的性能和分布拟合能力。

Conclusion: KGE-MoS 可以有效解决 KGC 模型中的秩瓶颈问题，改进预测表现，同时对参数消耗要求较低。

Abstract: Many Knowledge Graph Completion (KGC) models, despite using powerful
encoders, rely on a simple vector-matrix multiplication to score queries
against candidate object entities. When the number of entities is larger than
the model's embedding dimension, which in practical scenarios is often by
several orders of magnitude, we have a linear output layer with a rank
bottleneck. Such bottlenecked layers limit model expressivity. We investigate
both theoretically and empirically how rank bottlenecks affect KGC models. We
find that, by limiting the set of feasible predictions, rank bottlenecks hurt
ranking accuracy and the distribution fidelity of scores. Inspired by the
language modelling literature, we propose KGE-MoS, a mixture-based output layer
to break rank bottlenecks in many KGC models. Our experiments on four datasets
show that KGE-MoS improves performance and probabilistic fit of KGC models for
a low parameter cost.

</details>


### [201] [Artificial Intelligent Disobedience: Rethinking the Agency of Our Artificial Teammates](https://arxiv.org/abs/2506.22276)
*Reuth Mirsky*

Main category: cs.AI

TL;DR: 该论文讨论了将"智能不服从"概念引入AI系统，以增强其自主性，从而更有效地与人类进行合作。


<details>
  <summary>Details</summary>
Motivation: 尽管AI已在多项任务中表现出超越人类的能力，但大多数合作型AI系统仍仅遵循人类指令，即使在有时不利或不安全的情况下。提出探讨如何通过增强AI自主性，使其于团队中做出有意义的贡献。

Method: 提出并分析了AI自主性等级的划分，同时通过案例说明自主性的重要性及其在合作场景中的必要性。

Result: 研究了不同自主性级别下，"智能不服从"的表现形式，并给出了研究AI不服从能力时的一些初步边界与考量。

Conclusion: 指出需要将AI自主性与不服从能力作为未来人机合作研究的一个重要核心方向。

Abstract: Artificial intelligence has made remarkable strides in recent years,
achieving superhuman performance across a wide range of tasks. Yet despite
these advances, most cooperative AI systems remain rigidly obedient, designed
to follow human instructions without question and conform to user expectations,
even when doing so may be counterproductive or unsafe. This paper argues for
expanding the agency of AI teammates to include \textit{intelligent
disobedience}, empowering them to make meaningful and autonomous contributions
within human-AI teams. It introduces a scale of AI agency levels and uses
representative examples to highlight the importance and growing necessity of
treating AI autonomy as an independent research focus in cooperative settings.
The paper then explores how intelligent disobedience manifests across different
autonomy levels and concludes by proposing initial boundaries and
considerations for studying disobedience as a core capability of artificial
agents.

</details>


### [202] [Conceptual Topic Aggregation](https://arxiv.org/abs/2506.22309)
*Klara M. Gutekunst,Dominik Dürrschnabel,Johannes Hirth,Gerd Stumme*

Main category: cs.AI

TL;DR: 本文提出了一种基于形式概念分析（FCA）的新方法FAT-CAT，用于改进主题聚合和可视化，解决传统主题建模难以解释结果的问题。


<details>
  <summary>Details</summary>
Motivation: 随着数据的急剧增长，人工方式已经无法满足大规模数据的探索需求，因此需要新的计算方法使数据分析更加高效。此外，现有的主题建模方法无法提供足够可解释的结构分析。

Method: 提出一种基于FCA的方法。通过分析文本数据集主题分布，构建概念格，以分层、结构化的方式有效呈现主题并实现数据聚合和可视化。

Result: 通过在ETYNTKE数据集上的案例研究，证实FCA方法较传统主题表示技术能更有效地提供更有意义、更可解释的数据集组成的洞察。

Conclusion: FAT-CAT方法扩展了主题建模的应用效果，提升了语义理解和数据解读的能力。

Abstract: The vast growth of data has rendered traditional manual inspection
infeasible, necessitating the adoption of computational methods for efficient
data exploration. Topic modeling has emerged as a powerful tool for analyzing
large-scale textual datasets, enabling the extraction of latent semantic
structures. However, existing methods for topic modeling often struggle to
provide interpretable representations that facilitate deeper insights into data
structure and content. In this paper, we propose FAT-CAT, an approach based on
Formal Concept Analysis (FCA) to enhance meaningful topic aggregation and
visualization of discovered topics. Our approach can handle diverse topics and
file types -- grouped by directories -- to construct a concept lattice that
offers a structured, hierarchical representation of their topic distribution.
In a case study on the ETYNTKE dataset, we evaluate the effectiveness of our
approach against other representation methods to demonstrate that FCA-based
aggregation provides more meaningful and interpretable insights into dataset
composition than existing topic modeling techniques.

</details>


### [203] [Embodied AI Agents: Modeling the World](https://arxiv.org/abs/2506.22355)
*Pascale Fung,Yoram Bachrach,Asli Celikyilmaz,Kamalika Chaudhuri,Delong Chen,Willy Chung,Emmanuel Dupoux,Hervé Jégou,Alessandro Lazaric,Arjun Majumdar,Andrea Madotto,Franziska Meier,Florian Metze,Théo Moutakanni,Juan Pino,Basile Terver,Joseph Tighe,Jitendra Malik*

Main category: cs.AI

TL;DR: 本文研究了具备视觉、虚拟或物理形式的AI实体代理的设计，强调了世界模型在其推理和规划中的核心作用。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过具身AI代理改善用户与环境的交互体验，并提高其自动执行复杂任务的能力。

Method: 通过集成多模态感知、推理行动规划与控制以及记忆构建物理世界模型，同时研究用户的心理世界模型以提升合作能力。

Result: 成功强调了世界模型对于AI代理理解和预测环境、理解用户意图的重要性，并证明其能显著增强代理的任务执行能力。

Conclusion: 具身AI代理与世界模型的结合是实现自主智能任务执行和人机协作的重要方向。

Abstract: This paper describes our research on AI agents embodied in visual, virtual or
physical forms, enabling them to interact with both users and their
environments. These agents, which include virtual avatars, wearable devices,
and robots, are designed to perceive, learn and act within their surroundings,
which makes them more similar to how humans learn and interact with the
environments as compared to disembodied agents. We propose that the development
of world models is central to reasoning and planning of embodied AI agents,
allowing these agents to understand and predict their environment, to
understand user intentions and social contexts, thereby enhancing their ability
to perform complex tasks autonomously. World modeling encompasses the
integration of multimodal perception, planning through reasoning for action and
control, and memory to create a comprehensive understanding of the physical
world. Beyond the physical world, we also propose to learn the mental world
model of users to enable better human-agent collaboration.

</details>


### [204] [AI Model Passport: Data and System Traceability Framework for Transparent AI in Health](https://arxiv.org/abs/2506.22358)
*Varvara Kalokyri,Nikolaos S. Tachos,Charalampos N. Kalantzopoulos,Stelios Sfakianakis,Haridimos Kondylakis,Dimitrios I. Zaridis,Sara Colantonio,Daniele Regge,Nikolaos Papanikolaou,The ProCAncer-I consortium,Konstantinos Marias,Dimitrios I. Fotiadis,Manolis Tsiknakis*

Main category: cs.AI

TL;DR: 该论文提出了一种名为AI Model Passport的结构化框架，用于唯一标识、验证和监控AI模型的全生命周期。


<details>
  <summary>Details</summary>
Motivation: 现有AI框架缺乏可扩展性、可比性和机器可读性，同时未能提供模型来源及真实性的验证，限制了其可重复性和信任度。

Method: 设计了AI Model Passport作为一种数字身份工具，记录AI模型生命周期的元数据，并实现了一种名为AIPassport的工具，用于自动化元数据收集、版本管理及跨环境集成。

Result: 通过医疗影像分割案例验证了框架效率，提升了透明性、可重复性并减少了人工工作量。

Conclusion: AI Model Passport为AI驱动的医疗解决方案建立了信任与问责的新标准，有助于透明和合规系统的开发。

Abstract: The increasing integration of Artificial Intelligence (AI) into health and
biomedical systems necessitates robust frameworks for transparency,
accountability, and ethical compliance. Existing frameworks often rely on
human-readable, manual documentation which limits scalability, comparability,
and machine interpretability across projects and platforms. They also fail to
provide a unique, verifiable identity for AI models to ensure their provenance
and authenticity across systems and use cases, limiting reproducibility and
stakeholder trust. This paper introduces the concept of the AI Model Passport,
a structured and standardized documentation framework that acts as a digital
identity and verification tool for AI models. It captures essential metadata to
uniquely identify, verify, trace and monitor AI models across their lifecycle -
from data acquisition and preprocessing to model design, development and
deployment. In addition, an implementation of this framework is presented
through AIPassport, an MLOps tool developed within the ProCAncer-I EU project
for medical imaging applications. AIPassport automates metadata collection,
ensures proper versioning, decouples results from source scripts, and
integrates with various development environments. Its effectiveness is
showcased through a lesion segmentation use case using data from the
ProCAncer-I dataset, illustrating how the AI Model Passport enhances
transparency, reproducibility, and regulatory readiness while reducing manual
effort. This approach aims to set a new standard for fostering trust and
accountability in AI-driven healthcare solutions, aspiring to serve as the
basis for developing transparent and regulation compliant AI systems across
domains.

</details>


### [205] [The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements](https://arxiv.org/abs/2506.22419)
*Bingchen Zhao,Despoina Magka,Minqi Jiang,Xian Li,Roberta Raileanu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Kelvin Niu,Shagun Sodhani,Michael Shvartsman,Andrei Lupu,Alisia Lupidi,Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Thomas Foster,Lucia Cipolina-Kun,Abhishek Charnalia,Derek Dunfield,Alexander H. Miller,Oisin Mac Aodha,Jakob Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: 本文提出一个新的基准测试，用于评估大型语言模型（LLMs）在科学工作复现中的表现，并发现现有模型在该任务中面临挑战。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在科学研究复现中的能力，促进其应用于科学进展。

Method: 设计了Automated LLM Speedrunning Benchmark基准测试，基于NanoGPT速度竞赛任务，设置场景让LLMs尝试复现研究成果。

Result: 发现即使在给予详细提示的情况下，当前最先进的LLMs仍难以成功复现已知的研究创新。

Conclusion: 该基准测试提供了衡量LLMs在科学复现领域能力的新工具，未来对此领域的研究具有重要意义。

Abstract: Rapid advancements in large language models (LLMs) have the potential to
assist in scientific progress. A critical capability toward this endeavor is
the ability to reproduce existing work. To evaluate the ability of AI agents to
reproduce results in an active research area, we introduce the Automated LLM
Speedrunning Benchmark, leveraging the research community contributions on the
NanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.
Each of the 19 speedrun tasks provides the agent with the previous records
training script, optionally paired with one of three hint formats, ranging from
pseudocode to paper-like descriptions of the new records improvements. Records
execute quickly by design and speedrun improvements encompass diverse
code-level changes, ranging from high-level algorithmic advancements to
hardware-aware optimizations. These features make the benchmark both accessible
and realistic for the frontier problem of improving LLM training. We find that
recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement
already-known innovations in our benchmark, even when given detailed hints. Our
benchmark thus provides a simple, non-saturated measure of an LLMs ability to
automate scientific reproduction, a necessary (but not sufficient) skill for an
autonomous research agent.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [206] [APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization](https://arxiv.org/abs/2506.21655)
*Minjie Hong,Zirun Guo,Yan Xia,Zehan Wang,Ziang Zhang,Tao Jin,Zhou Zhao*

Main category: cs.LG

TL;DR: 该研究提出Asymmetric Policy Optimization (APO)，结合DADS与STCR技术，优化多模态大语言模型(MLLMs)的复杂推理能力，并成功提升模型性能及推理稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大语言模型在复杂推理任务中的表现不足，而通常RL方法可能引发生成过多冗长推理或性能下降的负面情况。研究动机是通过改进RL方法来增强多模态模型的推理能力，同时解决这些问题。

Method: 提出Asymmetric Policy Optimization (APO)方法，其中正样本使用Difficulty-Adaptive Divergence Shaping (DADS) 动态调节KL散度权重，以提高训练稳定性和样本利用效能，保留模型知识；对负样本引入Suboptimal Trajectory Complexity Regularization (STCR)，防止生成冗长推理，提升简洁性和探索能力。

Result: 运用所提的APO方法，将Qwen2.5-VL-3B模型优化为View-R1-3B，推理能力平均提升7%，超越规模更大的7-11B MLLMs模型，并保持一般任务表现的稳定性，体现优异的泛化能力。

Conclusion: 研究表明DADS及STCR技术在改进MLLMs复杂推理能力上的有效性，可广泛应用于多模态模型优化，优化兼顾稳定性、性能及任务适应性。

Abstract: Multimodal Large Language Models (MLLMs) are powerful at integrating diverse
data, but they often struggle with complex reasoning. While Reinforcement
learning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky.
Common issues include a drop in performance on general tasks and the generation
of overly detailed or "overthinking" reasoning. Our work investigates how the
KL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric
Policy Optimization (APO) to address these issues, which divides the sampled
responses into positive and negative groups. For positive samples,
Difficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically
adjust the KL divergence weight based on their difficulty. This method prevents
policy entropy from dropping sharply, improves training stability, utilizes
samples better, and preserves the model's existing knowledge. For negative
samples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to
penalize overly long responses. This helps mitigate overthinking and encourages
more concise reasoning while preserving the model's explorative capacity. We
apply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B
significantly enhances reasoning capabilities, showing an average 7\% gain over
the base model and outperforming larger MLLMs (7-11B) on various reasoning
benchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade
on general tasks, View-R1-3B maintains consistent improvement, demonstrating
superior generalization. These results highlight the effectiveness and broad
applicability of our DADS and STCR techniques for advancing complex multimodal
reasoning in MLLMs. The code will be made available at
https://github.com/Indolent-Kawhi/View-R1.

</details>


### [207] [Risk-Averse Total-Reward Reinforcement Learning](https://arxiv.org/abs/2506.21683)
*Xihong Su,Jia Lin Hau,Gersi Doko,Kishan Panaganti,Marek Petrik*

Main category: cs.LG

TL;DR: 本文提出了一种用于总奖励ERM和EVaR目标的新型Q学习算法，通过数值实验验证了其快速且可靠的收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有基于模型的方法在处理风险测度（如ERM和EVaR）时对转移概率的完全访问有依赖，本文希望摆脱此限制。

Method: 设计并提出了一种基于ERM和EVaR目标的Q学习算法，并证明了该算法的强收敛性和性能保障。

Result: 在表格域上的数值实验中，算法能够快速且可靠地收敛到最优风险规避值函数。

Conclusion: 该Q学习算法通过利用ERM的动态一致性和可引导性，实现了风险规避MDPs的高效求解，为风险规避问题提供了新的解决思路。

Abstract: Risk-averse total-reward Markov Decision Processes (MDPs) offer a promising
framework for modeling and solving undiscounted infinite-horizon objectives.
Existing model-based algorithms for risk measures like the entropic risk
measure (ERM) and entropic value-at-risk (EVaR) are effective in small
problems, but require full access to transition probabilities. We propose a
Q-learning algorithm to compute the optimal stationary policy for total-reward
ERM and EVaR objectives with strong convergence and performance guarantees. The
algorithm and its optimality are made possible by ERM's dynamic consistency and
elicitability. Our numerical results on tabular domains demonstrate quick and
reliable convergence of the proposed Q-learning algorithm to the optimal
risk-averse value function.

</details>


### [208] [Unimodal Strategies in Density-Based Clustering](https://arxiv.org/abs/2506.21695)
*Oron Nir,Jay Tenenbaum,Ariel Shamir*

Main category: cs.LG

TL;DR: 本研究揭示了一种密度聚类中关于簇数量与核心点邻域半径关系的特性，并提出基于此特性的参数选择的新方法，通过三分搜索算法实现。


<details>
  <summary>Details</summary>
Motivation: 密度聚类方法在面对噪声或任意分布的数据时常优于基于质心的方法，但参数选择在高维和大规模数据中计算复杂度高。

Method: 该研究识别了密度聚类中簇数量与核心点邻域半径近似单峰的性质，并以此为基础设计了基于三分搜索算法的参数选择策略。

Result: 提出的方法在多种大规模和高维任务中（包括NLP、音频和计算机视觉任务）展现了有效性和稳健性。

Conclusion: 此研究不仅改进了密度聚类方法的参数控制，还加深了对其指导参数关系的理解，并公开代码供社区使用。

Abstract: Density-based clustering methods often surpass centroid-based counterparts,
when addressing data with noise or arbitrary data distributions common in
real-world problems. In this study, we reveal a key property intrinsic to
density-based clustering methods regarding the relation between the number of
clusters and the neighborhood radius of core points - we empirically show that
it is nearly unimodal, and support this claim theoretically in a specific
setting. We leverage this property to devise new strategies for finding
appropriate values for the radius more efficiently based on the Ternary Search
algorithm. This is especially important for large scale data that is
high-dimensional, where parameter tuning is computationally intensive. We
validate our methodology through extensive applications across a range of
high-dimensional, large-scale NLP, Audio, and Computer Vision tasks,
demonstrating its practical effectiveness and robustness. This work not only
offers a significant advancement in parameter control for density-based
clustering but also broadens the understanding regarding the relations between
their guiding parameters. Our code is available at
https://github.com/oronnir/UnimodalStrategies.

</details>


### [209] [$\textrm{ODE}_t \left(\textrm{ODE}_l \right)$: Shortcutting the Time and Length in Diffusion and Flow Models for Faster Sampling](https://arxiv.org/abs/2506.21714)
*Denis Gudovskiy,Wenzhao Zheng,Tomoyuki Okuno,Yohei Nakata,Kurt Keutzer*

Main category: cs.LG

TL;DR: 提出一种新的方法，通过重组transformer模块，动态平衡生成质量和计算复杂度，取得了显著的性能和效率改进。


<details>
  <summary>Details</summary>
Motivation: 现有CNFs和DMs在采样时的计算复杂性较高，尤其是在求解多步ODE时，因此需要一种能有效提高采样效率的方法。

Method: 通过重新布线transformer架构中的模块，解决离散化ODE的问题，并引入时间和长度一致性约束，使采样时间步数和模型长度均可动态调节。

Result: 在CelebA-HQ和ImageNet数据集上的实验表明，该方法在最高效采样模式下可将延迟降低至1/3，并在高质量模式下提升FID分数到3.5。

Conclusion: 该方法在生成质量和采样效率之间取得了更好的平衡，并减小了内存占用，具有显著的应用潜力。

Abstract: Recently, continuous normalizing flows (CNFs) and diffusion models (DMs) have
been studied using the unified theoretical framework. Although such models can
generate high-quality data points from a noise distribution, the sampling
demands multiple iterations to solve an ordinary differential equation (ODE)
with high computational complexity. Most existing methods focus on reducing the
number of time steps during the sampling process to improve efficiency. In this
work, we explore a complementary direction in which the quality-complexity
tradeoff can be dynamically controlled in terms of time steps and in the length
of the neural network. We achieve this by rewiring the blocks in the
transformer-based architecture to solve an inner discretized ODE w.r.t. its
length. Then, we employ time- and length-wise consistency terms during flow
matching training, and as a result, the sampling can be performed with an
arbitrary number of time steps and transformer blocks. Unlike others, our
$\textrm{ODE}_t \left(\textrm{ODE}_l \right)$ approach is solver-agnostic in
time dimension and decreases both latency and memory usage. Compared to the
previous state of the art, image generation experiments on CelebA-HQ and
ImageNet show a latency reduction of up to $3\times$ in the most efficient
sampling mode, and a FID score improvement of up to $3.5$ points for
high-quality sampling. We release our code and model weights with fully
reproducible experiments.

</details>


### [210] [Performance Prediction for Large Systems via Text-to-Text Regression](https://arxiv.org/abs/2506.21718)
*Yash Akhauri,Bryan Lewandowski,Cheng-Hsi Lin,Adrian N. Reyes,Grant C. Forbes,Arissa Wongpanich,Bangding Yang,Mohamed S. Abdelfattah,Sagi Perel,Xingyou Song*

Main category: cs.LG

TL;DR: 本文提出一种基于文本到文本回归的方法，用于复杂系统的数据预测，并在谷歌Borg集群的资源效率预测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统回归方法在处理复杂系统数据（如配置文件或系统日志）时往往表现不佳，亟需新的方法替代。

Method: 设计了一种60M参数的编码器-解码器模型，采用文本到文本回归训练，并通过初始随机化和少量步骤进行任务适配。

Result: 在谷歌Borg集群资源效率预测中，实现了接近完美的0.99的秩相关系数（平均0.9）以及比表格方法低100倍的MSE。

Conclusion: 研究证明了文本到文本回归在复杂系统数据中的潜力，并为真实世界结果的通用模拟器的开发奠定了基础。

Abstract: In many industries, predicting metric outcomes of large systems is a
fundamental problem, driven largely by traditional tabular regression. However,
such methods struggle on complex systems data in the wild such as configuration
files or system logs, where feature engineering is often infeasible. We propose
text-to-text regression as a general, scalable alternative. For predicting
resource efficiency on Borg, Google's massive compute cluster scheduling
system, a 60M parameter encoder-decoder, trained from random initialization,
achieves up to a near perfect 0.99 (0.9 average) rank correlation across the
entire fleet, and 100x lower MSE than tabular approaches. The model also easily
adapts to new tasks in only 500 few-shot examples and captures the densities of
complex outcome distributions. Ablation studies highlight the importance of
using encoders, increasing sequence length, and the model's inherent
uncertainty quantification. These findings pave the way for universal
simulators of real-world outcomes.

</details>


### [211] [Federated Item Response Theory Models](https://arxiv.org/abs/2506.21744)
*Biying Zhou,Nanyu Luo,Feng Ji*

Main category: cs.LG

TL;DR: 本研究提出了一个名为联邦项目反应理论（FedIRT）的新框架，实现了在保证隐私和分布式计算的基础上，估计传统IRT模型。实验表明方法的统计准确性与标准IRT估计相当，同时实现了隐私保护和通信成本降低。


<details>
  <summary>Details</summary>
Motivation: 传统IRT估计需要集中处理所有数据，存在隐私风险。为了结合联邦学习的隐私保护与分布式计算，本研究试图将这些特性融入现代心理测量方法。

Method: 提出了联邦项目反应理论（FedIRT）框架，可以在分布式环境下使用联邦学习技术来估计传统IRT模型，同时保障隐私，并提供了FedIRT开源R包支持2PL和PCM模型。

Result: 通过数值实验，FedIRT的统计准确性与标准IRT方法相近，并显著提升了隐私保护和通信效率。在真实世界考试数据上验证了其在实际教育环境中的效用。

Conclusion: FedIRT框架扩展了IRT模型的适用性，使其能够应用于多中心评估场景，提供了与标准方法相当的准确性，同时确保了数据隐私和操作效率，为分布式心理测量带来了新机遇。

Abstract: Item Response Theory (IRT) models have been widely used to estimate
respondents' latent abilities and calibrate items' difficulty. Traditional IRT
estimation requires all individual raw response data to be centralized in one
place, thus potentially causing privacy issues. Federated learning is an
emerging field in computer science and machine learning with added features of
privacy protection and distributed computing. To integrate the advances from
federated learning with modern psychometrics, we propose a novel framework,
Federated Item Response Theory (IRT), to enable estimating traditional IRT
models with additional privacy, allowing estimation in a distributed manner
without losing estimation accuracy.
  Our numerical experiments confirm that FedIRT achieves statistical accuracy
similar to standard IRT estimation using popular R packages, while offering
critical advantages: privacy protection and reduced communication costs. We
also validate FedIRT's utility through a real-world exam dataset, demonstrating
its effectiveness in realistic educational contexts. This new framework extends
IRT's applicability to distributed settings, such as multi-school assessments,
without sacrificing accuracy or security. To support practical adoption, we
provide an open-ource R package, FedIRT, implementing the framework for the
two-parameter logistic (2PL) and partial credit models (PCM).

</details>


### [212] [Gradient-Based Neuroplastic Adaptation for Concurrent Optimization of Neuro-Fuzzy Networks](https://arxiv.org/abs/2506.21771)
*John Wesley Hostetter,Min Chi*

Main category: cs.LG

TL;DR: 本文提出一种名为基于梯度的神经可塑性自适应方法，用于同时优化神经模糊网络（NFNs）的结构和参数，并在视觉任务中展现其在线强化学习的优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的NFNs设计流程通常将参数和结构的识别分开进行，导致架构早期选择不当及性能不佳。因此，需要一种能够同时优化参数和结构的新方法。

Method: 提出一种基于梯度的神经可塑性自适应方法，同时优化NFNs的参数与结构，并将其应用于需要在线强化学习的任务中，例如视觉视频游戏中的DOOM情境。

Result: 新方法证明了在训练NFNs以在线强化学习方式学习玩复杂的视觉视频游戏DOOM中是有效的。

Conclusion: 同时优化NFNs的参数和结构能够克服传统设计上的局限性，使NFNs能够在更广泛和更复杂的任务中表现出色。

Abstract: Neuro-fuzzy networks (NFNs) are transparent, symbolic, and universal function
approximations that perform as well as conventional neural architectures, but
their knowledge is expressed as linguistic IF-THEN rules. Despite these
advantages, their systematic design process remains a challenge. Existing work
will often sequentially build NFNs by inefficiently isolating parametric and
structural identification, leading to a premature commitment to brittle and
subpar architecture. We propose a novel application-independent approach called
gradient-based neuroplastic adaptation for the concurrent optimization of NFNs'
parameters and structure. By recognizing that NFNs' parameters and structure
should be optimized simultaneously as they are deeply conjoined, settings
previously unapproachable for NFNs are now accessible, such as the online
reinforcement learning of NFNs for vision-based tasks. The effectiveness of
concurrently optimizing NFNs is empirically shown as it is trained by online
reinforcement learning to proficiently play challenging scenarios from a
vision-based video game called DOOM.

</details>


### [213] [M3PO: Massively Multi-Task Model-Based Policy Optimization](https://arxiv.org/abs/2506.21782)
*Aditya Narendra,Dmitry Makarov,Aleksandr Panov*

Main category: cs.LG

TL;DR: 论文提出了一个名为Massively Multi-Task Model-Based Policy Optimization (M3PO)的框架，用于在强化学习领域提高单任务采样效率，以及提升多任务场景中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于模型的方法如DreamerV3未能充分利用控制相关的表示，而基于模型的无模型方法如PPO存在高样本复杂度和弱探索能力的问题。作者希望通过提出新框架来解决这些问题。

Method: M3PO通过隐式世界模型预测任务结果，同时结合基于模型的规划和无模型的不确定性驱动奖励进行混合探索。此外，利用模型与无模型价值估计的差异指导探索的同时，通过信任域优化器维持稳定的策略更新。

Result: M3PO在多个基准任务中均取得了当前最优的表现，证明其高效性和鲁棒性。

Conclusion: M3PO为现有基于模型的策略优化方法提供了一个高效且稳健的替代方案，实现了较好的性能改进。

Abstract: We introduce Massively Multi-Task Model-Based Policy Optimization (M3PO), a
scalable model-based reinforcement learning (MBRL) framework designed to
address sample inefficiency in single-task settings and poor generalization in
multi-task domains. Existing model-based approaches like DreamerV3 rely on
pixel-level generative models that neglect control-centric representations,
while model-free methods such as PPO suffer from high sample complexity and
weak exploration. M3PO integrates an implicit world model, trained to predict
task outcomes without observation reconstruction, with a hybrid exploration
strategy that combines model-based planning and model-free uncertainty-driven
bonuses. This eliminates the bias-variance trade-off in prior methods by using
discrepancies between model-based and model-free value estimates to guide
exploration, while maintaining stable policy updates through a trust-region
optimizer. M3PO provides an efficient and robust alternative to existing
model-based policy optimization approaches and achieves state-of-the-art
performance across multiple benchmarks.

</details>


### [214] [Multi-task parallelism for robust pre-training of graph foundation models on multi-source, multi-fidelity atomistic modeling data](https://arxiv.org/abs/2506.21788)
*Massimiliano Lupo Pasini,Jong Youl Choi,Pei Zhang,Kshitij Mehta,Rylie Weaver,Ashwin M. Aji,Karl W. Schulz,Jorda Polo,Prasanna Balaprakash*

Main category: cs.LG

TL;DR: 该论文探讨通过图神经网络进行图基础模型的性能优化，特别是处理多源、多保真数据的挑战。


<details>
  <summary>Details</summary>
Motivation: 提升图神经网络的多任务处理能力，以应对化学数据的多样性和大规模性。

Method: 提出了一种多任务并行方法，将每个解码头分配到不同的计算资源上，并利用GPU加速，基于HydraGNN框架实现。

Result: 在超过2400万个结构上训练，并展示了在三种超级计算机上的高效扩展能力。

Conclusion: 模型优化方法成功增强了多源数据处理的效率和扩展性，展示了在超级计算环境中的实际应用潜力。

Abstract: Graph foundation models using graph neural networks promise sustainable,
efficient atomistic modeling. To tackle challenges of processing multi-source,
multi-fidelity data during pre-training, recent studies employ multi-task
learning, in which shared message passing layers initially process input
atomistic structures regardless of source, then route them to multiple decoding
heads that predict data-specific outputs. This approach stabilizes pre-training
and enhances a model's transferability to unexplored chemical regions.
Preliminary results on approximately four million structures are encouraging,
yet questions remain about generalizability to larger, more diverse datasets
and scalability on supercomputers. We propose a multi-task parallelism method
that distributes each head across computing resources with GPU acceleration.
Implemented in the open-source HydraGNN architecture, our method was trained on
over 24 million structures from five datasets and tested on the Perlmutter,
Aurora, and Frontier supercomputers, demonstrating efficient scaling on all
three highly heterogeneous super-computing architectures.

</details>


### [215] [Why Neural Network Can Discover Symbolic Structures with Gradient-based Training: An Algebraic and Geometric Foundation for Neurosymbolic Reasoning](https://arxiv.org/abs/2506.21797)
*Peihao Wang,Zhangyang Wang*

Main category: cs.LG

TL;DR: 通过将神经参数提升到测度空间并利用Wasserstein梯度流建模，研究表明神经网络训练可以自然地产生离散符号结构，同时与几何约束和代数操作相结合。


<details>
  <summary>Details</summary>
Motivation: 探索连续神经网络如何通过训练动态自然产生离散的符号结构，为神经符号系统的理解和设计提供理论依据。

Method: 通过Wasserstein梯度流框架下的参数测度进化研究，分析了训练过程中梯度流分解与自由度收缩的现象，并提出量子半环的代数约束。

Result: 证明了训练过程从高维探索转向符号表示，并建立了符号任务实现的数据扩展定律，揭示表征能力与群不变性之间的联系。

Conclusion: 为融合连续学习与离散代数推理的神经符号系统构建了系统理论基础，有助于理解网络如何自动从数据中学习符号规律。

Abstract: We develop a theoretical framework that explains how discrete symbolic
structures can emerge naturally from continuous neural network training
dynamics. By lifting neural parameters to a measure space and modeling training
as Wasserstein gradient flow, we show that under geometric constraints, such as
group invariance, the parameter measure $\mu_t$ undergoes two concurrent
phenomena: (1) a decoupling of the gradient flow into independent optimization
trajectories over some potential functions, and (2) a progressive contraction
on the degree of freedom. These potentials encode algebraic constraints
relevant to the task and act as ring homomorphisms under a commutative
semi-ring structure on the measure space. As training progresses, the network
transitions from a high-dimensional exploration to compositional
representations that comply with algebraic operations and exhibit a lower
degree of freedom. We further establish data scaling laws for realizing
symbolic tasks, linking representational capacity to the group invariance that
facilitates symbolic solutions. This framework charts a principled foundation
for understanding and designing neurosymbolic systems that integrate continuous
learning with discrete algebraic reasoning.

</details>


### [216] [The Cost of Avoiding Backpropagation](https://arxiv.org/abs/2506.21833)
*Kunjal Panchal,Sunav Choudhary,Yuriy Brun,Hui Guan*

Main category: cs.LG

TL;DR: 本文比较了反向传播（BP）、正向自动微分（FmAD）和零阶优化（ZO）方法的内存效率和性能，发现BP结合激活检查点在内存受限的情况下拥有最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究提出FmAD和ZO作为替代BP的内存高效方案，但其实际好处尚不清楚，尤其在与内存高效的BP变体对比以及统一理论分析方面存在欠缺。

Method: 作者通过理论和实验分析，同时研究大型语言模型和视觉-语言模型的表现，比较了BP、FmAD和ZO方法在内存、准确性、收敛速度与计算上的性能表现。

Result: 研究显示，FmAD和ZO尽管降低了内存开销，却在准确性、收敛速度和计算效率上存在显著劣势，尤其是模型规模增大或约束加重时。这些方法即使经过优化，也不及带检查点的BP在实验中表现。

Conclusion: 带检查点的BP在内存受限的训练中仍然是最有效的策略。

Abstract: Forward-mode automatic differentiation (FmAD) and zero-order (ZO)
optimization have been proposed as memory-efficient alternatives to
backpropagation (BP) for gradient computation, especially in low-resource
settings. However, their practical benefits remain unclear due to two key gaps:
a lack of comparison against memory-efficient BP variants, such as activation
checkpointing, and a lack of a unified theoretical analysis. This work presents
a comprehensive theoretical and empirical comparison of BP, FmAD, and ZO
methods. Our theoretical analysis shows that while FmAD, and ZO can reduce
memory usage, they incur significant costs in accuracy, convergence speed, and
computation compared to BP with checkpointing. These drawbacks worsen with
larger models or constrained perturbation budgets. Empirical experiments on
large language and vision-language models show that BP with checkpointing
outperforms FmAD and ZO variants, including those enhanced with variance
reduction, achieving up to 31.1% higher accuracy, 34.8% faster convergence, and
3.8x fewer computations at comparable memory usage. Our results highlight
fundamental limitations of FmAD and ZO, and reaffirm BP with checkpointing as
the most effective strategy for model training under memory-constrained
settings. Our code is available at
https://github.com/Astuary/The_Cost_of_Avoiding_Backpropagation.

</details>


### [217] [Koopman operator-based discussion on partial observation in stochastic systems](https://arxiv.org/abs/2506.21844)
*Jun Ohkubo*

Main category: cs.LG

TL;DR: 研究探讨了在随机系统中使用Koopman算子理论处理部分观测的影响，并分析了延迟嵌入技术的重要性及其与观测精度的关系。


<details>
  <summary>Details</summary>
Motivation: 为了解决部分观测下随机系统的分析问题，并结合Koopman算子理论探讨该问题的处理方法。

Method: 使用Koopman算子理论和延迟嵌入技术，在随机系统条件下，进行数值实验来分析噪声幅度与观测精度之间的关系。

Result: 发现延迟嵌入技术在处理部分观测的随机系统中有效，并观察到观测精度与附加噪声幅度之间的幂律关系。

Conclusion: 区分随机系统中的状态空间和函数空间是重要的，延迟嵌入技术是随机系统部分观测的有力工具，幂律关系的指数与部分观测的影响相关。

Abstract: It is sometimes difficult to achieve a complete observation for a full set of
observables, and partial observations are necessary. For deterministic systems,
the Mori-Zwanzig formalism provides a theoretical framework for handling
partial observations. Recently, data-driven algorithms based on the Koopman
operator theory have made significant progress, and there is a discussion to
connect the Mori-Zwanzig formalism with the Koopman operator theory. In this
work, we discuss the effects of partial observation in stochastic systems using
the Koopman operator theory. The discussion clarifies the importance of
distinguishing the state space and the function space in stochastic systems.
Even in stochastic systems, the delay embedding technique is beneficial for
partial observation, and several numerical experiments showed a power-law
behavior of the accuracy for the amplitude of the additive noise. We also
discuss the relation between the exponent of the power-law behavior and the
effects of partial observation.

</details>


### [218] [A Survey of Continual Reinforcement Learning](https://arxiv.org/abs/2506.21872)
*Chaofan Pan,Xin Yang,Yanhua Li,Wei Wei,Tianrui Li,Bo An,Jiye Liang*

Main category: cs.LG

TL;DR: 本文是关于持续强化学习（CRL）的综述，探讨其核心概念、挑战和方法学。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习（RL）需要大量训练数据和计算资源，难以在动态和真实环境中推广应用，而持续学习（CL）的兴起为解决这些限制提供了新的可能性。

Method: 本文对CRL的方法进行回顾和分类，并提出一种基于知识存储或迁移的新分类法。

Result: 系统性地分析并归纳了CRL的方法、度量、任务、基准和场景设置，提出新分类法并总结CRL的独特挑战。

Conclusion: CRL是一种具有前景的研究方向，本文通过全面综述和分类，为未来研究提供了实践洞见。

Abstract: Reinforcement Learning (RL) is an important machine learning paradigm for
solving sequential decision-making problems. Recent years have witnessed
remarkable progress in this field due to the rapid development of deep neural
networks. However, the success of RL currently relies on extensive training
data and computational resources. In addition, RL's limited ability to
generalize across tasks restricts its applicability in dynamic and real-world
environments. With the arisen of Continual Learning (CL), Continual
Reinforcement Learning (CRL) has emerged as a promising research direction to
address these limitations by enabling agents to learn continuously, adapt to
new tasks, and retain previously acquired knowledge. In this survey, we provide
a comprehensive examination of CRL, focusing on its core concepts, challenges,
and methodologies. Firstly, we conduct a detailed review of existing works,
organizing and analyzing their metrics, tasks, benchmarks, and scenario
settings. Secondly, we propose a new taxonomy of CRL methods, categorizing them
into four types from the perspective of knowledge storage and/or transfer.
Finally, our analysis highlights the unique challenges of CRL and provides
practical insights into future directions.

</details>


### [219] [Advancements and Challenges in Continual Reinforcement Learning: A Comprehensive Review](https://arxiv.org/abs/2506.21899)
*Amara Zuffer,Michael Burke,Mehrtash Harandi*

Main category: cs.LG

TL;DR: 本文综述了连续强化学习的关键概念、挑战和方法，尤其关注该领域在机器人学中的最新进展以及评估环境概述。


<details>
  <summary>Details</summary>
Motivation: 推动强化学习代理变为能连续学习和保留知识的动态学习者。

Method: 通过梳理连续强化学习的重要概念、挑战、方法及其在机器人学领域的应用来进行探讨。

Result: 总结了连续强化学习的研究进展、针对新手的环境概述，并提出了领域的未来发展方向。

Conclusion: 连续强化学习在知识重用和动态学习中有重要意义，未来具有广阔前景。

Abstract: The diversity of tasks and dynamic nature of reinforcement learning (RL)
require RL agents to be able to learn sequentially and continuously, a learning
paradigm known as continuous reinforcement learning. This survey reviews how
continual learning transforms RL agents into dynamic continual learners. This
enables RL agents to acquire and retain useful and reusable knowledge
seamlessly. The paper delves into fundamental aspects of continual
reinforcement learning, exploring key concepts, significant challenges, and
novel methodologies. Special emphasis is placed on recent advancements in
continual reinforcement learning within robotics, along with a succinct
overview of evaluation environments utilized in prominent research,
facilitating accessibility for newcomers to the field. The review concludes
with a discussion on limitations and promising future directions, providing
valuable insights for researchers and practitioners alike.

</details>


### [220] [TOAST: Task-Oriented Adaptive Semantic Transmission over Dynamic Wireless Environments](https://arxiv.org/abs/2506.21900)
*Sheng Yun,Jianhua Pei,Ping Wang*

Main category: cs.LG

TL;DR: TOAST框架通过多任务优化、自适应权衡和噪声恢复技术，显著提升了低SNR条件下分类准确性和重构质量，同时保持在复杂无线环境中的稳定性能。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要从注重比特传输向重视任务相关信息的语义通信转变，但目前缺乏面向多任务动态无线环境的高效传输解决方案。

Method: 提出TOAST框架，结合马尔可夫决策过程和深度强化学习实现任务动态权衡，使用Swin Transformer结合LoRA机制降低参数调节开销，嵌入扩散模型改进噪声条件下的特征恢复能力。

Result: 实验表明，TOAST在低信噪比条件下，分类准确性和重建质量均优于现有基线方法，并在多种无线干扰条件下表现稳定。

Conclusion: TOAST框架在动态无线环境中提供了高效的语义通信服务，为未来6G网络的多任务应用提供了可靠支持。

Abstract: The evolution toward 6G networks demands a fundamental shift from bit-centric
transmission to semantic-aware communication that emphasizes task-relevant
information. This work introduces TOAST (Task-Oriented Adaptive Semantic
Transmission), a unified framework designed to address the core challenge of
multi-task optimization in dynamic wireless environments through three
complementary components. First, we formulate adaptive task balancing as a
Markov decision process, employing deep reinforcement learning to dynamically
adjust the trade-off between image reconstruction fidelity and semantic
classification accuracy based on real-time channel conditions. Second, we
integrate module-specific Low-Rank Adaptation (LoRA) mechanisms throughout our
Swin Transformer-based joint source-channel coding architecture, enabling
parameter-efficient fine-tuning that dramatically reduces adaptation overhead
while maintaining full performance across diverse channel impairments including
Additive White Gaussian Noise (AWGN), fading, phase noise, and impulse
interference. Third, we incorporate an Elucidating diffusion model that
operates in the latent space to restore features corrupted by channel noises,
providing substantial quality improvements compared to baseline approaches.
Extensive experiments across multiple datasets demonstrate that TOAST achieves
superior performance compared to baseline approaches, with significant
improvements in both classification accuracy and reconstruction quality at low
Signal-to-Noise Ratio (SNR) conditions while maintaining robust performance
across all tested scenarios.

</details>


### [221] [HQCM-EBTC: A Hybrid Quantum-Classical Model for Explainable Brain Tumor Classification](https://arxiv.org/abs/2506.21937)
*Marwan Ait Haddou,Mohamed Bennai*

Main category: cs.LG

TL;DR: 提出HQCM-EBTC，一种结合量子和经典方法的自动脑肿瘤分类模型，在MRI数据集上达到96.48%的高精度。


<details>
  <summary>Details</summary>
Motivation: 提升脑肿瘤分类精度和解释性，利用量子计算增强医疗成像模型能力。

Method: 模型HQCM-EBTC集成5量子比特、深度为2的量子层，采用AdamW优化器以及交叉熵与注意力一致性复合损失函数训练。

Result: 模型在总数据集上达到了96.48%的准确率，大幅超过经典基线（86.72%），并在特征分离、肿瘤定位等方面有显著优势。

Conclusion: 量子增强模型在医疗成像中表现出巨大潜力，可提高诊断的精确性和解释性，为临床应用提供支持。

Abstract: We propose HQCM-EBTC, a hybrid quantum-classical model for automated brain
tumor classification using MRI images. Trained on a dataset of 7,576 scans
covering normal, meningioma, glioma, and pituitary classes, HQCM-EBTC
integrates a 5-qubit, depth-2 quantum layer with 5 parallel circuits, optimized
via AdamW and a composite loss blending cross-entropy and attention
consistency.
  HQCM-EBTC achieves 96.48% accuracy, substantially outperforming the classical
baseline (86.72%). It delivers higher precision and F1-scores, especially for
glioma detection. t-SNE projections reveal enhanced feature separability in
quantum space, and confusion matrices show lower misclassification. Attention
map analysis (Jaccard Index) confirms more accurate and focused tumor
localization at high-confidence thresholds.
  These results highlight the promise of quantum-enhanced models in medical
imaging, advancing both diagnostic accuracy and interpretability for clinical
brain tumor assessment.

</details>


### [222] [GuiderNet: A Meta-Learning Framework for Optimizing Quantum Circuit Geometry and Mitigating Barren Plateaus](https://arxiv.org/abs/2506.21940)
*Marwan Ait Haddou,Mohamed Bennai*

Main category: cs.LG

TL;DR: 该论文介绍了GuiderNet，一个通过几何元条件化来改善量子机器学习可训练性和泛化能力的框架。


<details>
  <summary>Details</summary>
Motivation: 当前变分量子算法（VQA）面临训练时梯度消失（barren plateaus）和优化地形不佳的问题，该研究旨在通过几何元条件化来解决这些问题，从而提高量子电路的可训练性和泛化性能。

Method: 提出GuiderNet——一个依赖数据的几何元学习框架，旨在通过调整Fubini-Study度量张量的对数条件数，引导参数化量子电路（PQC）的参数至更优的几何区域。

Result: 在Kaggle Diabetes分类任务中，GuiderNet实现显著提升，训练损失降低超过5倍，测试集准确率从75.3%提升至98.6%，少数类别的F1得分从0.67提高到0.95，并有效抑制了梯度爆炸和参数更新不稳定。

Conclusion: 通过结合量子-经典混合管道，GuiderNet展示了几何元条件化在缓解量子机器学习中梯度消失和优化问题上的可行性及其可扩展性。

Abstract: Variational Quantum Algorithms (VQAs) offer potential for near-term quantum
advantage but face challenges from barren plateaus, where gradients vanish, and
poorly conditioned optimization landscapes. We introduce GuiderNet, a
meta-learning framework that conditions Parameterized Quantum Circuits (PQCs)
using data-dependent parameter shifts aimed at minimizing the log condition
number of the Fubini-Study metric tensor. Implemented as a classical neural
network, GuiderNet is meta-trained to guide PQC parameters into geometrically
favorable regions and is embedded within hybrid quantum-classical pipelines to
steer both initialization and adaptive modulation during training.
  Applied to the Kaggle Diabetes classification task, GuiderNet reduces
cumulative training loss by over 5x, improves test accuracy from 75.3% to
98.6%, and increases the minority-class F1 score from 0.67 to 0.95. It also
suppresses gradient explosion and stabilizes parameter updates, enabling
smoother and more robust optimization. These results demonstrate that geometric
meta-conditioning can mitigate barren plateaus and ill-conditioning, providing
a scalable approach to enhance trainability and generalization in quantum
machine learning.

</details>


### [223] [Physics-informed network paradigm with data generation and background noise removal for diverse distributed acoustic sensing applications](https://arxiv.org/abs/2506.21952)
*Yangyang Wan,Haotian Wang,Xuhui Yu,Jiageng Chen,Xinyu Fan,Zuyuan He*

Main category: cs.LG

TL;DR: 提出了一种物理信息驱动的分布式声学传感（DAS）神经网络范式，无需现实事件数据即可训练并应用于事件识别和消噪，展示了卓越的通用性和实用性。


<details>
  <summary>Details</summary>
Motivation: 传统AI模型对训练数据的需求与现实中事件数据的稀缺形成矛盾。该研究旨在通过物理建模解决这一问题，无需依赖现实事件数据。

Method: 通过物理建模目标事件及其现实世界和DAS系统的约束，推导物理函数用于生成DAS事件数据，训练生成网络。设计DAS消噪网络使用生成的数据去除背景噪声。

Result: 提出的方法在DAS时空数据的事件识别及皮带输送机故障监测的应用上达到了相当或更优于依赖现实数据训练的模型的效果，包括在无现场故障数据参与训练的情况下，达成91.8%的故障诊断准确性。

Conclusion: 该范式有效解决了DAS实践应用中的数据获取困难及噪声干扰问题，展示了其通用性与前瞻性，为DAS技术的进一步拓展提供了潜力。

Abstract: Distributed acoustic sensing (DAS) has attracted considerable attention
across various fields and artificial intelligence (AI) technology plays an
important role in DAS applications to realize event recognition and denoising.
Existing AI models require real-world data (RWD), whether labeled or not, for
training, which is contradictory to the fact of limited available event data in
real-world scenarios. Here, a physics-informed DAS neural network paradigm is
proposed, which does not need real-world events data for training. By
physically modeling target events and the constraints of real world and DAS
system, physical functions are derived to train a generative network for
generation of DAS events data. DAS debackground net is trained by using the
generated DAS events data to eliminate background noise in DAS data. The
effectiveness of the proposed paradigm is verified in event identification
application based on a public dataset of DAS spatiotemporal data and in belt
conveyor fault monitoring application based on DAS time-frequency data, and
achieved comparable or better performance than data-driven networks trained
with RWD. Owing to the introduction of physical information and capability of
background noise removal, the paradigm demonstrates generalization in same
application on different sites. A fault diagnosis accuracy of 91.8% is achieved
in belt conveyor field with networks which transferred from simulation test
site without any fault events data of test site and field for training. The
proposed paradigm is a prospective solution to address significant obstacles of
data acquisition and intense noise in practical DAS applications and explore
more potential fields for DAS.

</details>


### [224] [Optimal Return-to-Go Guided Decision Transformer for Auto-Bidding in Advertisement](https://arxiv.org/abs/2506.21956)
*Hao Jiang,Yongxiang Tang,Yanxiang Zeng,Pengjia Yuan,Yanhua Cheng,Teng Sha,Xialong Liu,Peng Jiang*

Main category: cs.LG

TL;DR: 提出了R* Decision Transformer，用于解决在线广告竞拍中自动化竞标的长期依赖性问题，并通过增强数据集提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 在线广告竞拍中，传统自动竞标系统存在短视和依赖高质量训练数据的问题，研究旨在提高竞标系统的自动化和适应能力。

Method: 先提出R DT作为基础模型，记录状态和RTG值以及预测RTG值；改进为R^ DT，通过预测最高RTG值生成次优策略；最终提出R* DT，通过模拟器生成高奖励轨迹增强训练集，从而优化政策。

Result: 在公开的竞标数据集上验证，R* DT显著提高了混合质量轨迹处理的效果。

Conclusion: R* DT通过补充高奖励轨迹逐步优化次优政策，展示了在复杂竞标场景中的实用价值和优势。

Abstract: In the realm of online advertising, advertisers partake in ad auctions to
obtain advertising slots, frequently taking advantage of auto-bidding tools
provided by demand-side platforms. To improve the automation of these bidding
systems, we adopt generative models, namely the Decision Transformer (DT), to
tackle the difficulties inherent in automated bidding. Applying the Decision
Transformer to the auto-bidding task enables a unified approach to sequential
modeling, which efficiently overcomes short-sightedness by capturing long-term
dependencies between past bidding actions and user behavior. Nevertheless,
conventional DT has certain drawbacks: (1) DT necessitates a preset
return-to-go (RTG) value before generating actions, which is not inherently
produced; (2) The policy learned by DT is restricted by its training data,
which is consists of mixed-quality trajectories. To address these challenges,
we introduce the R* Decision Transformer (R* DT), developed in a three-step
process: (1) R DT: Similar to traditional DT, R DT stores actions based on
state and RTG value, as well as memorizing the RTG for a given state using the
training set; (2) R^ DT: We forecast the highest value (within the training
set) of RTG for a given state, deriving a suboptimal policy based on the
current state and the forecasted supreme RTG value; (3) R* DT: Based on R^ DT,
we generate trajectories and select those with high rewards (using a simulator)
to augment our training dataset. This data enhancement has been shown to
improve the RTG of trajectories in the training data and gradually leads the
suboptimal policy towards optimality. Comprehensive tests on a publicly
available bidding dataset validate the R* DT's efficacy and highlight its
superiority when dealing with mixed-quality trajectories.

</details>


### [225] [SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model](https://arxiv.org/abs/2506.21976)
*Shuhan Tan,John Lambert,Hong Jeon,Sakshum Kulshrestha,Yijing Bai,Jing Luo,Dragomir Anguelov,Mingxing Tan,Chiyu Max Jiang*

Main category: cs.LG

TL;DR: 本文提出了一个名为SceneDiffuser++的模型，可基于单一损失函数实现城市规模的交通仿真，使得由起点到终点的模拟更加真实和无缝。


<details>
  <summary>Details</summary>
Motivation: 目前交通仿真主要用以弥补现实中手动行驶数据的不足，而某些技术如动态场景生成和环境仿真研究较少，本研究旨在整合所有相关需求实现市级交通仿真。

Method: 研发了SceneDiffuser++，一个端到端的生成世界模型，通过单一损失函数整合场景生成、代理行为建模、遮挡推理、动态场景生成及环境仿真功能。

Result: SceneDiffuser++可以实现从A点到B点的城市规模交通仿真，并证明在长时间仿真下具备更高的真实感。

Conclusion: 该研究验证了在扩展版WOMD数据集上的模型性能，表明其在模拟质量和规模上全面提升，有潜力推动交通仿真应用。

Abstract: The goal of traffic simulation is to augment a potentially limited amount of
manually-driven miles that is available for testing and validation, with a much
larger amount of simulated synthetic miles. The culmination of this vision
would be a generative simulated city, where given a map of the city and an
autonomous vehicle (AV) software stack, the simulator can seamlessly simulate
the trip from point A to point B by populating the city around the AV and
controlling all aspects of the scene, from animating the dynamic agents (e.g.,
vehicles, pedestrians) to controlling the traffic light states. We refer to
this vision as CitySim, which requires an agglomeration of simulation
technologies: scene generation to populate the initial scene, agent behavior
modeling to animate the scene, occlusion reasoning, dynamic scene generation to
seamlessly spawn and remove agents, and environment simulation for factors such
as traffic lights. While some key technologies have been separately studied in
various works, others such as dynamic scene generation and environment
simulation have received less attention in the research community. We propose
SceneDiffuser++, the first end-to-end generative world model trained on a
single loss function capable of point A-to-B simulation on a city scale
integrating all the requirements above. We demonstrate the city-scale traffic
simulation capability of SceneDiffuser++ and study its superior realism under
long simulation conditions. We evaluate the simulation quality on an augmented
version of the Waymo Open Motion Dataset (WOMD) with larger map regions to
support trip-level simulation.

</details>


### [226] [Binned semiparametric Bayesian networks](https://arxiv.org/abs/2506.21997)
*Rafael Sojo,Javier Díaz-Rozo,Concha Bielza,Pedro Larrañaga*

Main category: cs.LG

TL;DR: 该论文提出了一种新型的概率半参数模型，通过数据分箱降低核密度估计的计算成本，并发展了两种新型条件概率分布以解决维度灾难问题。


<details>
  <summary>Details</summary>
Motivation: 传统非参数分布的核密度估计计算成本较高且容易受维度灾难限制，亟需高效替代方案。

Method: 提出稀疏分箱核密度估计和傅里叶核密度估计，通过稀疏张量和减少条件概率计算中的参数节点数量，优化计算效率。

Result: 实验结果表明，提出的分箱半参数贝叶斯网络的结构学习和对数似然估计与传统方法无显著统计差异，但运行速度大幅提升。

Conclusion: 新的分箱半参数贝叶斯网络在计算效率上更优，是非分箱模型的可靠替代方案。

Abstract: This paper introduces a new type of probabilistic semiparametric model that
takes advantage of data binning to reduce the computational cost of kernel
density estimation in nonparametric distributions. Two new conditional
probability distributions are developed for the new binned semiparametric
Bayesian networks, the sparse binned kernel density estimation and the Fourier
kernel density estimation. These two probability distributions address the
curse of dimensionality, which typically impacts binned models, by using sparse
tensors and restricting the number of parent nodes in conditional probability
calculations. To evaluate the proposal, we perform a complexity analysis and
conduct several comparative experiments using synthetic data and datasets from
the UCI Machine Learning repository. The experiments include different binning
rules, parent restrictions, grid sizes, and number of instances to get a
holistic view of the model's behavior. As a result, our binned semiparametric
Bayesian networks achieve structural learning and log-likelihood estimations
with no statistically significant differences compared to the semiparametric
Bayesian networks, but at a much higher speed. Thus, the new binned
semiparametric Bayesian networks prove to be a reliable and more efficient
alternative to their non-binned counterparts.

</details>


### [227] [GKNet: Graph Kalman Filtering and Model Inference via Model-based Deep Learning](https://arxiv.org/abs/2506.22004)
*Mohammad Sabbaqi,Riccardo Taormina,Elvin Isufi*

Main category: cs.LG

TL;DR: 本文提出了一种面向图时间序列的图感知状态空间模型，用于处理基于图的时间序列推断任务，通过最大似然方法与深度学习框架联合优化。


<details>
  <summary>Details</summary>
Motivation: 图时间序列的推断任务在水网、经济学、网络神经科学等领域中应用重要，且需要模型能够同时捕捉图和时间模式且具备计算高效性。

Method: 提出一种图感知状态空间模型，结合图引导的参数化潜在状态与观察方程，利用图结构条件化噪声分布以及最大似然估计法，同时构建深度学习架构进行端到端训练，模拟Kalman神经网络。

Result: 通过理论可行性和端到端深度学习架构提高方法的可解释性和扩展性，能够对部分观测的数据进行有效的参数学习和状态跟踪。

Conclusion: 该方法在图时间序列建模上提供了一种新颖且高效的解决方案，为预测与数据插补等下游任务奠定了基础，同时平衡了理论性与实际模型性能。

Abstract: Inference tasks with time series over graphs are of importance in
applications such as urban water networks, economics, and networked
neuroscience. Addressing these tasks typically relies on identifying a
computationally affordable model that jointly captures the graph-temporal
patterns of the data. In this work, we propose a graph-aware state space model
for graph time series, where both the latent state and the observation equation
are parametric graph-induced models with a limited number of parameters that
need to be learned. More specifically, we consider the state equation to follow
a stochastic partial differential equation driven by noise over the graphs
edges accounting not only for potential edge uncertainties but also for
increasing the degrees of freedom in the latter in a tractable manner. The
graph structure conditioning of the noise dispersion allows the state variable
to deviate from the stochastic process in certain neighborhoods. The
observation model is a sampled and graph-filtered version of the state
capturing multi-hop neighboring influence. The goal is to learn the parameters
in both state and observation models from the partially observed data for
downstream tasks such as prediction and imputation. The model is inferred first
through a maximum likelihood approach that provides theoretical tractability
but is limited in expressivity and scalability. To improve on the latter, we
use the state-space formulation to build a principled deep learning
architecture that jointly learns the parameters and tracks the state in an
end-to-end manner in the spirit of Kalman neural networks.

</details>


### [228] [On the Necessity of Output Distribution Reweighting for Effective Class Unlearning](https://arxiv.org/abs/2506.20893)
*Yian Wang,Ali Ebrahimpour-Boroojeny,Hari Sundaram*

Main category: cs.LG

TL;DR: 提出了一种名为RWFT的轻量化分类器未学习方法，可以在无需完全重新训练的情况下有效移除训练模型中的某个类别。


<details>
  <summary>Details</summary>
Motivation: 为了解决用户训练数据删除权以及减轻有害或偏见预测问题，同时避免全量重新训练成本高昂的问题。

Method: 通过设计MIA-NN攻击证明现有未学习方法的问题，并提出基于概率质量再分配的新未学习方法RWFT，并引入基于总变化距离（TV）的新指标量化残留泄漏。

Result: 新方法RWFT在现有评估指标上与完全重新训练结果保持一致，并在所提出的TV指标上表现优异，相较最佳现有方法提高了111.45%。

Conclusion: RWFT方法能有效实现未学习需求，补足现有方法的不足，同时成本更低，是实现用户数据删除权的有效途径。

Abstract: In this work, we introduce an output-reweighting unlearning method, RWFT, a
lightweight technique that erases an entire class from a trained classifier
without full retraining. Forgetting specific classes from trained models is
essential for enforcing user deletion rights and mitigating harmful or biased
predictions. The full retraining is costly and existing unlearning methods fail
to replicate the behavior of the retrained models when predicting samples from
the unlearned class. We prove this failure by designing a variant of membership
inference attacks, MIA-NN that successfully reveals the unlearned class for any
of these methods. We propose a simple redistribution of the probability mass
for the prediction on the samples in the forgotten class which is robust to
MIA-NN. We also introduce a new metric based on the total variation (TV)
distance of the prediction probabilities to quantify residual leakage to
prevent future methods from susceptibility to the new attack. Through extensive
experiments with state of the art baselines in machine unlearning, we show that
our approach matches the results of full retraining in both metrics used for
evaluation by prior work and the new metric we propose in this work. Compare to
state-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%
in our new TV-based metric over the best existing method.

</details>


### [229] [TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning](https://arxiv.org/abs/2506.22008)
*Alessandro Sestini,Joakim Bergdahl,Konrad Tollmar,Andrew D. Bagdanov,Linus Gisslén*

Main category: cs.LG

TL;DR: 本文介绍了TROFI，一种不需要预定义奖励函数的离线逆强化学习方法，能有效训练策略并在D4RL基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决在没有奖励函数时的离线强化学习问题，特别是针对视频游戏等实际场景。

Method: 提出TROFI方法，首先基于人类偏好学习奖励函数，然后用于为数据集打标签，从而用于策略训练。

Result: TROFI在D4RL基准测试和3D游戏环境中表现优于基线，并在使用真实奖励时有可比的性能。

Conclusion: TROFI展示了无需预定义奖励函数训练策略的可能性，突出简单易学的奖励函数的重要性。

Abstract: In offline reinforcement learning, agents are trained using only a fixed set
of stored transitions derived from a source policy. However, this requires that
the dataset be labeled by a reward function. In applied settings such as video
game development, the availability of the reward function is not always
guaranteed. This paper proposes Trajectory-Ranked OFfline Inverse reinforcement
learning (TROFI), a novel approach to effectively learn a policy offline
without a pre-defined reward function. TROFI first learns a reward function
from human preferences, which it then uses to label the original dataset making
it usable for training the policy. In contrast to other approaches, our method
does not require optimal trajectories. Through experiments on the D4RL
benchmark we demonstrate that TROFI consistently outperforms baselines and
performs comparably to using the ground truth reward to learn policies.
Additionally, we validate the efficacy of our method in a 3D game environment.
Our studies of the reward model highlight the importance of the reward function
in this setting: we show that to ensure the alignment of a value function to
the actual future discounted reward, it is fundamental to have a
well-engineered and easy-to-learn reward function.

</details>


### [230] [Hyper-modal Imputation Diffusion Embedding with Dual-Distillation for Federated Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2506.22036)
*Ying Zhang,Yu Zhao,Xuhui Sui,Baohang Zhou,Xiangrui Cai,Li Shen,Xiaojie Yuan,Dacheng Tao*

Main category: cs.LG

TL;DR: 文章提出了一种名为FedMKGC的联邦多模态知识图补全任务及其解决方法MMFeD3-HidE，用于在不同客户端间不共享敏感知识的情况下预测缺失链接。


<details>
  <summary>Details</summary>
Motivation: 现有多模态知识图存在去中心化和协作能力不足的问题，并且需要保证传输的安全性和推理能力。本文旨在通过引入联邦学习框架解决多模态知识非共享环境下的知识图补全问题。

Method: 提出了MMFeD3-HidE框架，包括两部分：1) HidE模型用于恢复不完整的多模态分布；2) MMFeD3通过对客户端和服务器采用双蒸馏技术，实现知识共享与一致性。

Result: 在提出的FedMKGC基准（包括骨干架构、数据集以及基线）上进行实验，验证了MMFeD3-HidE框架的有效性、语义一致性和收敛稳健性。

Conclusion: MMFeD3-HidE框架在联邦多模态知识图补全任务中表现出显著的优势，具备应用于实际分布式多模态场景的潜力。

Abstract: With the increasing multimodal knowledge privatization requirements,
multimodal knowledge graphs in different institutes are usually decentralized,
lacking of effective collaboration system with both stronger reasoning ability
and transmission safety guarantees. In this paper, we propose the Federated
Multimodal Knowledge Graph Completion (FedMKGC) task, aiming at training over
federated MKGs for better predicting the missing links in clients without
sharing sensitive knowledge. We propose a framework named MMFeD3-HidE for
addressing multimodal uncertain unavailability and multimodal client
heterogeneity challenges of FedMKGC. (1) Inside the clients, our proposed
Hyper-modal Imputation Diffusion Embedding model (HidE) recovers the complete
multimodal distributions from incomplete entity embeddings constrained by
available modalities. (2) Among clients, our proposed Multimodal FeDerated Dual
Distillation (MMFeD3) transfers knowledge mutually between clients and the
server with logit and feature distillation to improve both global convergence
and semantic consistency. We propose a FedMKGC benchmark for a comprehensive
evaluation, consisting of a general FedMKGC backbone named MMFedE, datasets
with heterogeneous multimodal information, and three groups of constructed
baselines. Experiments conducted on our benchmark validate the effectiveness,
semantic consistency, and convergence robustness of MMFeD3-HidE.

</details>


### [231] [UniCA: Adapting Time Series Foundation Model to General Covariate-Aware Forecasting](https://arxiv.org/abs/2506.22039)
*Lu Han,Yu Liu,Qiwen Deng,Jian Jiang,Yinbo Sun,Zhe Yu,Binfeng Wang,Xingyu Lu,Lintao Ma,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.LG

TL;DR: 本文提出了一个统一的协变量适应（UniCA）框架，用于增强时间序列基础模型（TSFM）在处理协变量时的能力，尤其适用于涉及异构和不同模态数据的预测任务。


<details>
  <summary>Details</summary>
Motivation: 现有的TSFM主要设计用于实数值时间序列，难以有效处理复杂任务中的异构协变量（如类别变量和图像、文本等多模态数据）。因此，需要一种方法以统一方式整合多样化的协变量，提升其预测能力。

Method: 提出的UniCA框架包括两个阶段：1）协变量同质化，将异构协变量转化为统一的高级同质性时间序列表示；2）基于注意力机制的融合，整合这些同质化表示，实现对异构与同构协变量的自适应兼容。

Result: 测试表现表明，UniCA在多个单模态和多模态协变量感知预测基准上表现优越，证明此框架在实际预测场景中的潜力。

Conclusion: UniCA成功扩展了TSFM的应用范围，实现了对广泛协变量的统一适应，为异构与多模态数据的整合和预测提供了通用且高效的解决方案。

Abstract: Time Series Foundation Models (TSFMs) have achieved remarkable success
through large-scale pretraining. However, their design primarily targets
real-valued series, limiting their ability to handle general forecasting tasks
involving diverse and often heterogeneous covariates--such as categorical
variables and multimodal data (e.g., images, text)--which are typically
task-specific and difficult to leverage during pretraining. To address this
gap, we propose Unified Covariate Adaptation (UniCA), a framework to bridge
TSFMs with general covariate-aware forecasting. UniCA first performs covariate
homogenization to transform heterogeneous covariates into high-level
homogeneous series representations and then fuses them via a unified
attention-based fusion mechanism. UniCA is compatible and universal for
adaptation with both homogeneous and heterogeneous covariates, incorporating
extra covariate information while preserving the generalization ability of
TSFMs.Extensive experiments on multiple unimodal and multimodal covariate-aware
forecasting benchmarks demonstrate the superiority of UniCA, highlighting the
promise of covariate-aware TSFM adaptation in real-world forecasting scenarios.
Codes are released on https://github.com/hanlu-nju/UniCA.

</details>


### [232] [GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling](https://arxiv.org/abs/2506.22049)
*Tianhao Chen,Xin Xu,Zijing Liu,Pengxiang Li,Xinyuan Song,Ajay Kumar Jaiswal,Fan Zhang,Jishan Hu,Yang Wang,Hao Chen,Shizhe Diao,Shiwei Liu,Yu Li,Yin Lu,Can Yang*

Main category: cs.LG

TL;DR: 提出了一种方法GPAS，通过缩小中间激活值但保持梯度不变，以改进预训练的大型语言模型的学习能力。


<details>
  <summary>Details</summary>
Motivation: 解决Pre-LN Transformer因激活值在层间的指数增长导致深层学习能力受限的问题。

Method: 提议Gradient-Preserving Activation Scaling (GPAS)，通过下调中间激活值，同时保留梯度不变，从而优化模型训练过程和性能。

Result: GPAS在从71M到1B的不同模型规模上均表现出一致的性能提升，且还能增强一些替代性架构如Sandwich-LN和DeepNorm的表现。

Conclusion: GPAS是一种简单但有效的方法，能改善Pre-LN Transformers的训练动态，同时具有适用其他架构的潜力。

Abstract: Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,
predominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While
being stable during pretraining and scalable to large model sizes, Pre-LN
suffers from an exponential growth in activation variance across layers,
causing the residual path to dominate over sub-layer outputs and limiting the
learning capacity of deeper layers. To mitigate this issue, we propose
Gradient-Preserving Activation Scaling (GPAS), a simple technique that can be
used in combination with existing approaches. GPAS works by scaling down the
intermediate activations while keeping their gradients unchanged. This leaves
information in the activations intact, and avoids the gradient vanishing
problem associated with gradient downscaling. Extensive experiments across
various model sizes from 71M to 1B show that GPAS achieves consistent
performance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows
promise in improving alternative architectures such as Sandwich-LN and
DeepNorm, demonstrating its versatility and potential for improving training
dynamics in a wide range of settings.

</details>


### [233] [crypto price prediction using lstm+xgboost](https://arxiv.org/abs/2506.22055)
*Mehul Gautam*

Main category: cs.LG

TL;DR: 提出了一种结合LSTM和XGBoost的混合模型，用于加密货币价格预测，表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决加密货币市场价格预测的准确性问题，特别是处理市场波动性和复杂动态等挑战。

Method: 采用一种混合深度学习和机器学习的方法，结合LSTM捕捉时间依赖性和XGBoost处理非线性关系，利用全球与本地数据集进行模型训练和评估。

Result: 在比特币、以太坊、狗狗币和莱特币历史数据上，混合模型在MAPE和MinMax RMSE指标上优于独立模型和传统方法。

Conclusion: 混合模型在金融预测中的潜力得到了验证，同时展示了其在不同加密货币和市场环境中的适应性。

Abstract: The volatility and complex dynamics of cryptocurrency markets present unique
challenges for accurate price forecasting. This research proposes a hybrid deep
learning and machine learning model that integrates Long Short-Term Memory
(LSTM) networks and Extreme Gradient Boosting (XGBoost) for cryptocurrency
price prediction. The LSTM component captures temporal dependencies in
historical price data, while XGBoost enhances prediction by modeling nonlinear
relationships with auxiliary features such as sentiment scores and
macroeconomic indicators. The model is evaluated on historical datasets of
Bitcoin, Ethereum, Dogecoin, and Litecoin, incorporating both global and
localized exchange data. Comparative analysis using Mean Absolute Percentage
Error (MAPE) and Min-Max Normalized Root Mean Square Error (MinMax RMSE)
demonstrates that the LSTM+XGBoost hybrid consistently outperforms standalone
models and traditional forecasting methods. This study underscores the
potential of hybrid architectures in financial forecasting and provides
insights into model adaptability across different cryptocurrencies and market
contexts.

</details>


### [234] [Transformers are Graph Neural Networks](https://arxiv.org/abs/2506.22084)
*Chaitanya K. Joshi*

Main category: cs.LG

TL;DR: 研究连接Transformer与图神经网络（GNNs）的关系，提出Transformer可视为完全连接图上的消息传递GNN，同时证实Transformer在硬件表现上优于稀疏GNN。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer在自然语言处理之外的图表示学习应用，并探讨其与GNN的关系和硬件性能优势。

Method: 将Transformer视为在完全连接图上的GNNs，分析其基于自注意力机制捕捉关系及位置编码的表达能力，同时对比其硬件性能。

Result: 证明Transformer是表达能力强的图表示学习架构，其在现代硬件上的密集矩阵计算效率高于稀疏消息传递的GNN实现。

Conclusion: Transformer作为GNN被硬件性能推动，展现出其在图处理任务中的优势。

Abstract: We establish connections between the Transformer architecture, originally
introduced for natural language processing, and Graph Neural Networks (GNNs)
for representation learning on graphs. We show how Transformers can be viewed
as message passing GNNs operating on fully connected graphs of tokens, where
the self-attention mechanism capture the relative importance of all tokens
w.r.t. each-other, and positional encodings provide hints about sequential
ordering or structure. Thus, Transformers are expressive set processing
networks that learn relationships among input elements without being
constrained by apriori graphs. Despite this mathematical connection to GNNs,
Transformers are implemented via dense matrix operations that are significantly
more efficient on modern hardware than sparse message passing. This leads to
the perspective that Transformers are GNNs currently winning the hardware
lottery.

</details>


### [235] [Learning to Solve Multi-Objective Routing Problems on Multigraphs](https://arxiv.org/abs/2506.22095)
*Filip Rydin,Attila Lischka,Jiaming Wu,Morteza Haghir Chehreghani,Balázs Kulcsár*

Main category: cs.LG

TL;DR: 提出了两种神经网络方法解决多目标多图路由问题，并验证了其在TSP及CVRP等问题上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究对多目标路由的多图设置关注不足，尽管其具有重要的实际意义。

Method: 提出两种方法：直接在多图上自回归选择边以完成路径；或先将多图修剪为简单图，再构建路径。

Result: 实验验证了两种模型在多种问题上的强性能。

Conclusion: 两种方法在解决多目标多图路由问题上表现优异，具有实际价值。

Abstract: Learning-based methods for routing have gained significant attention in
recent years, both in single-objective and multi-objective contexts. However,
the multigraph setting, where multiple paths with distinct attributes can exist
between destinations, has largely been overlooked, despite its high practical
relevancy. In this paper, we introduce two neural approaches to address
multi-objective routing on multigraphs. Our first approach works directly on
the multigraph, by autoregressively selecting edges until a tour is completed.
On the other hand, our second model first prunes the multigraph into a simple
graph and then builds routes. We validate both models experimentally and find
that they demonstrate strong performance across a variety of problems,
including the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP).

</details>


### [236] [Transfer Learning for Assessing Heavy Metal Pollution in Seaports Sediments](https://arxiv.org/abs/2506.22096)
*Tin Lai,Farnaz Farid,Yueyang Kuan,Xintian Zhang*

Main category: cs.LG

TL;DR: 該研究提出了一種基於深度學習的新模型，用於簡化重金屬污染評估過程，並在新南威爾士州六大港口的數據測試中表現出色。


<details>
  <summary>Details</summary>
Motivation: 傳統的污染負荷指數 (PLI) 評估需要大量勞動力和分析，重金屬污染檢測面臨數據稀缺與標準不一致的問題。

Method: 通過深度學習和轉移學習，模型實現了跨域特徵轉移，用於準確預測PLI，並對重金屬污染進行量化評估。

Result: 在六大港口測試中，模型的平均絕對誤差（MAE）約為0.5，平均絕對百分比誤差（MAPE）約為0.03，表現優於其他基線模型。

Conclusion: 該模型具備創新性、可訪問性及成本效益，對於海洋生態保護、水產養殖與工業污染監測具有重要意義。

Abstract: Detecting heavy metal pollution in soils and seaports is vital for regional
environmental monitoring. The Pollution Load Index (PLI), an international
standard, is commonly used to assess heavy metal containment. However, the
conventional PLI assessment involves laborious procedures and data analysis of
sediment samples. To address this challenge, we propose a deep-learning-based
model that simplifies the heavy metal assessment process. Our model tackles the
issue of data scarcity in the water-sediment domain, which is traditionally
plagued by challenges in data collection and varying standards across nations.
By leveraging transfer learning, we develop an accurate quantitative assessment
method for predicting PLI. Our approach allows the transfer of learned features
across domains with different sets of features. We evaluate our model using
data from six major ports in New South Wales, Australia: Port Yamba, Port
Newcastle, Port Jackson, Port Botany, Port Kembla, and Port Eden. The results
demonstrate significantly lower Mean Absolute Error (MAE) and Mean Absolute
Percentage Error (MAPE) of approximately 0.5 and 0.03, respectively, compared
to other models. Our model performance is up to 2 orders of magnitude than
other baseline models. Our proposed model offers an innovative, accessible, and
cost-effective approach to predicting water quality, benefiting marine life
conservation, aquaculture, and industrial pollution monitoring.

</details>


### [237] [Earthquake Damage Grades Prediction using An Ensemble Approach Integrating Advanced Machine and Deep Learning Models](https://arxiv.org/abs/2506.22129)
*Anurag Panda,Gaurav Kumar Yadav*

Main category: cs.LG

TL;DR: 研究探讨多类分类模型预测地震后建筑损害程度的有效性，强调解决类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 及时评估地震后建筑损毁，优化救援和资源分配，提升救灾响应效率。

Method: 通过SMOTE技术处理类别不平衡，测试多种机器学习、深度学习与集成方法，进行特征操作实验并分析混淆矩阵。

Result: 识别地震脆弱性关键因素，提高了对建筑损害预测模型的性能理解。

Conclusion: 综合机器学习与数据处理技术，可以更准确预测地震后的损害程度，有助于优化救援与资源分配。

Abstract: In the aftermath of major earthquakes, evaluating structural and
infrastructural damage is vital for coordinating post-disaster response
efforts. This includes assessing damage's extent and spatial distribution to
prioritize rescue operations and resource allocation. Accurately estimating
damage grades to buildings post-earthquake is paramount for effective response
and recovery, given the significant impact on lives and properties,
underscoring the urgency of streamlining relief fund allocation processes.
Previous studies have shown the effectiveness of multi-class classification,
especially XGBoost, along with other machine learning models and ensembling
methods, incorporating regularization to address class imbalance. One
consequence of class imbalance is that it may give rise to skewed models that
undervalue minority classes and give preference to the majority class. This
research deals with the problem of class imbalance with the help of the
synthetic minority oversampling technique (SMOTE). We delve into multiple
multi-class classification machine learning, deep learning models, and
ensembling methods to forecast structural damage grades. The study elucidates
performance determinants through comprehensive feature manipulation experiments
and diverse training approaches. It identifies key factors contributing to
seismic vulnerability while evaluating model performance using techniques like
the confusion matrix further to enhance understanding of the effectiveness of
earthquake damage prediction.

</details>


### [238] [Thompson Sampling-Based Learning and Control for Unknown Dynamic Systems](https://arxiv.org/abs/2506.22186)
*Kaikai Zheng,Dawei Shi,Yang Shi,Long Wang*

Main category: cs.LG

TL;DR: 提出了一种基于TS的控制器设计方法，通过新的参数化方法解决TS在更广泛控制空间中的应用局限性。


<details>
  <summary>Details</summary>
Motivation: 解决TS方法在控制系统设计中的空间限制，使其适用于更广泛应用。

Method: 采用RKHS进行控制律的函数化表示，结合TS构建控制律的学习框架，并分析其收敛性和控制遗憾上限。

Result: 理论分析证明学习速度快且控制遗憾有界，数值实验验证了方法在控制未知非线性系统中的有效性。

Conclusion: 提出的方法能够高效学习控制律与性能指标，扩展了TS在更通用控制场景下的适用性。

Abstract: Thompson sampling (TS) is an effective method to explore parametric
uncertainties and can therefore be used for active learning-based controller
design. However, TS relies on finite parametric representations, which limits
its applicability to more general spaces, which are more commonly encountered
in control system design. To address this issue, this work pro poses a
parameterization method for control law learning using reproducing kernel
Hilbert spaces and designs a data-driven active learning control approach.
Specifically, the proposed method treats the control law as an element in a
function space, allowing the design of control laws without imposing
restrictions on the system structure or the form of the controller. A TS
framework is proposed in this work to explore potential optimal control laws,
and the convergence guarantees are further provided for the learning process.
Theoretical analysis shows that the proposed method learns the relationship
between control laws and closed-loop performance metrics at an exponential
rate, and the upper bound of control regret is also derived. Numerical
experiments on controlling unknown nonlinear systems validate the effectiveness
of the proposed method.

</details>


### [239] [Exploring Modularity of Agentic Systems for Drug Discovery](https://arxiv.org/abs/2506.22189)
*Laura van Weesep,Samuel Genheden,Ola Engkvist,Jens Sjölund*

Main category: cs.LG

TL;DR: 本文研究了大型语言模型(LLMs)在药物发现与设计中的模块化，并比较了不同LLMs及工具调用代理与代码生成代理的性能。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在药物发现中的模块化潜力，特别是各组成部分是否具备可互换性。

Method: 通过案例研究，用LLM-as-a-judge评分对多个LLM模型及工具调用代理和代码生成代理进行性能对比。

Result: Claude-3.5-Sonnet、Claude-3.7-Sonnet和GPT-4o表现优于其他模型；代码生成代理整体上优于工具调用代理，但表现取决于具体问题和模型。

Conclusion: 语言模型的替换需考虑提示重构；研究表明需进一步探索代理系统模块化以开发针对现实问题的稳定解决方案。

Abstract: Large-language models (LLMs) and agentic systems present exciting
opportunities to accelerate drug discovery and design. In this study, we
critically examine the modularity of LLM-based agentic systems for drug
discovery, i.e., whether parts of the agentic system such as the LLM are
interchangeable, a topic that has received limited attention in drug discovery
applications. We compare the performance of different large language models
(LLMs) and the effectiveness of tool-calling agents versus code-generating
agents in this domain. Our case study, comparing performance in orchestrating
tools for chemistry and drug discovery using an LLM-as-a-judge score, shows
that Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative
language models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and
Nova-Micro. Although we confirm that code-generating agents outperform the
tool-calling ones on average, we show that this is highly question and model
dependent. Furthermore, the impact of replacing system prompts is dependent on
the specific question asked and the model used, underscoring that -- even in
this particular domain -- one cannot just replace language models without
considering prompt re-engineering. Our study highlights the necessity of
further research into the modularity of agentic systems to enable the
development of stable and scalable solutions for real-world problems.

</details>


### [240] [dreaMLearning: Data Compression Assisted Machine Learning](https://arxiv.org/abs/2506.22190)
*Xiaobo Zhao,Aaron Hurst,Panagiotis Karras,Daniel E. Lucani*

Main category: cs.LG

TL;DR: 本文提出了dreaMLearning，一个无需解压即可从压缩数据中学习的框架，基于熵驱动的无损压缩方法EntroGeDe。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型依赖大量标注数据，计算和存储需求巨大，因此需要在更少资源下实现高效训练的架构。

Method: 使用熵驱动的无损压缩方法EntroGeDe，通过压缩出一组代表性样本数据，在不解压的情况下训练机器学习模型。

Result: 在回归与分类任务中，训练速度加快至8.8倍，内存使用减少至1/10，存储占用降低42%，且模型性能基本保持。

Conclusion: dreaMLearning为实现高效、可扩展的机器学习提供了可能，尤其在分布式学习、联邦学习和资源受限的TinyML设备中具有重要意义。

Abstract: Despite rapid advancements, machine learning, particularly deep learning, is
hindered by the need for large amounts of labeled data to learn meaningful
patterns without overfitting and immense demands for computation and storage,
which motivate research into architectures that can achieve good performance
with fewer resources. This paper introduces dreaMLearning, a novel framework
that enables learning from compressed data without decompression, built upon
Entropy-based Generalized Deduplication (EntroGeDe), an entropy-driven lossless
compression method that consolidates information into a compact set of
representative samples. DreaMLearning accommodates a wide range of data types,
tasks, and model architectures. Extensive experiments on regression and
classification tasks with tabular and image data demonstrate that dreaMLearning
accelerates training by up to 8.8x, reduces memory usage by 10x, and cuts
storage by 42%, with a minimal impact on model performance. These advancements
enhance diverse ML applications, including distributed and federated learning,
and tinyML on resource-constrained edge devices, unlocking new possibilities
for efficient and scalable learning.

</details>


### [241] [REDELEX: A Framework for Relational Deep Learning Exploration](https://arxiv.org/abs/2506.22199)
*Jakub Peleška,Gustav Šír*

Main category: cs.LG

TL;DR: 研究提出REDELEX框架评估关系深度学习（RDL）模型性能，并分析其与关系数据库（RDBs）特性间的关系。


<details>
  <summary>Details</summary>
Motivation: RDB被广泛应用于存储结构化信息，而将深度学习与关系数据库结合的RDL为预测任务提供了新范式，但其性能与数据库特性间的关系尚未充分研究。

Method: 引入REDELEX框架，在超过70个RDB上评估各种复杂度的RDL模型，比较其与传统方法表现，并分析影响性能的关键因子。

Result: 结果表明RDL模型整体性能优于传统方法，同时探明模型复杂性、数据库大小及其结构性质对性能的影响。

Conclusion: 本研究通过REDELEX展示了关系深度学习的优势及其性能影响因素，为未来研究提供了数据和工具支持。

Abstract: Relational databases (RDBs) are widely regarded as the gold standard for
storing structured information. Consequently, predictive tasks leveraging this
data format hold significant application promise. Recently, Relational Deep
Learning (RDL) has emerged as a novel paradigm wherein RDBs are conceptualized
as graph structures, enabling the application of various graph neural
architectures to effectively address these tasks. However, given its novelty,
there is a lack of analysis into the relationships between the performance of
various RDL models and the characteristics of the underlying RDBs.
  In this study, we present REDELEX$-$a comprehensive exploration framework for
evaluating RDL models of varying complexity on the most diverse collection of
over 70 RDBs, which we make available to the community. Benchmarked alongside
key representatives of classic methods, we confirm the generally superior
performance of RDL while providing insights into the main factors shaping
performance, including model complexity, database sizes and their structural
properties.

</details>


### [242] [EFRame: Deeper Reasoning via Exploration-Filtering-Replay Reinforcement Learning Framework](https://arxiv.org/abs/2506.22200)
*Chen Wang,Lai Wei,Yanzhi Zhang,Chenyang Shao,Zedong Dan,Weiran Huang,Yue Wang,Yuzhi Zhang*

Main category: cs.LG

TL;DR: 本文提出EFRame框架，通过额外探索、高质量样本筛选及经验回放，改进现有GRPO方法的效率与稳定性，显著提升在复杂推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 改进大语言模型使用强化学习解决复杂推理任务的效率和稳定性，克服现有GRPO方法存在的探索不足、样本效率低和训练不稳定的问题。

Method: EFRame框架包括三个核心步骤：对高质量轨迹进行额外探索，在线筛选去除低质量样本，以及利用经验回放多次利用稀有但有信息量的样本，形成完整稳定的学习周期。

Result: 实验表明，EFRame在多种推理基准测试中显著提升了训练的鲁棒性和效率，并实现了原始GRPO无法达到的深层推理能力。

Conclusion: EFRame不仅提高了复杂推理任务中强化学习的性能，还为训练样本的分类和分析提供了更细化的方法，对强化学习过程的研究具有重要意义。

Abstract: Recent advances in reinforcement learning (RL) have significantly enhanced
the reasoning capabilities of large language models (LLMs). Group Relative
Policy Optimization (GRPO), an efficient variant of PPO that lowers RL's
computational cost, still faces limited exploration, low sample efficiency and
instability, constraining its performance on complex reasoning tasks. To
address these limitations, we introduce EFRame, an Exploration-Filtering-Replay
framework that systematically augments GRPO along three critical dimensions.
EFRame performs additional rollouts to explore high-quality trajectories,
applies online filtering to eliminate low-quality samples that introduce noise
and variance, and leverages experience replay to repeatedly exploit rare but
informative samples. EFRame establishes a complete and stable learning cycle,
guiding the model through a structured transition from exploration to
convergence. Our experiments across a variety of reasoning benchmarks
demonstrate that EFRame not only improves the robustness and efficiency of
training, but also enables access to deeper reasoning capabilities that remain
unattainable under vanilla GRPO. Furthermore, EFRame enables a more
fine-grained categorization of training samples, allowing for a deeper analysis
of how different types of samples contribute to the learning process in RL. Our
code is available at https://github.com/597358816/EFRame.

</details>


### [243] [Risk-Averse Best Arm Set Identification with Fixed Budget and Fixed Confidence](https://arxiv.org/abs/2506.22253)
*Shunta Nonaga,Koji Tabata,Yuta Mizuno,Tamiki Komatsuzaki*

Main category: cs.LG

TL;DR: 提出了一个新的基于均值-方差(MV)准则的随机带宽优化问题,关注于最大化期望回报和最小化关联的不确定性,并提供算法框架和理论保证。


<details>
  <summary>Details</summary>
Motivation: 在不确定环境中,需同时优化期望回报与风险。传统方法无法很好地平衡效率和风险,需要更复杂的决策框架。

Method: 设计了一个统一的元算法框架,通过自适应的置信区间设计,支持固定置信和固定预算两种模式,实现了有效寻找期望回报与风险间的Pareto最优解。

Result: 理论上证明了算法结果的正确性,并在合成基准测试中展现了超越现有方法的表现,包括更高精度与样本效率。

Conclusion: 提出的方法在不确定环境中的风险感知决策任务上具备广泛适用性,同时在理论和实验上均表现优越。

Abstract: Decision making under uncertain environments in the maximization of expected
reward while minimizing its risk is one of the ubiquitous problems in many
subjects. Here, we introduce a novel problem setting in stochastic bandit
optimization that jointly addresses two critical aspects of decision-making:
maximizing expected reward and minimizing associated uncertainty, quantified
via the mean-variance(MV) criterion. Unlike traditional bandit formulations
that focus solely on expected returns, our objective is to efficiently and
accurately identify the Pareto-optimal set of arms that strikes the best
trade-off between expected performance and risk. We propose a unified
meta-algorithmic framework capable of operating under both fixed-confidence and
fixed-budget regimes, achieved through adaptive design of confidence intervals
tailored to each scenario using the same sample exploration strategy. We
provide theoretical guarantees on the correctness of the returned solutions in
both settings. To complement this theoretical analysis, we conduct extensive
empirical evaluations across synthetic benchmarks, demonstrating that our
approach outperforms existing methods in terms of both accuracy and sample
efficiency, highlighting its broad applicability to risk-aware decision-making
tasks in uncertain environments.

</details>


### [244] [Projected Compression: Trainable Projection for Efficient Transformer Compression](https://arxiv.org/abs/2506.22255)
*Maciej Stefaniak,Michał Krutul,Jan Małaśnicki,Maciej Pióro,Jakub Krajewski,Sebastian Jaszczur,Marek Cygan,Kamil Adamczewski,Jan Ludziejewski*

Main category: cs.LG

TL;DR: 提出了一种新的模型压缩技术——投影压缩，通过投影模块减少Transformer模型的权重大小，同时仍保持原始模型参数的访问权限。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模的增长，其推理时间和计算需求显著增加，提出新的方法降低模型大小成为研究重点。

Method: 通过结合可训练的投影权重和模型原参数，将其融合生成低维产物矩阵，从而实现模型压缩而不增加计算开销。

Result: 实验结果表明，相较于硬剪枝和重新训练的方法，投影压缩技术在高质量模型上表现更优，其性能优势随数据输入量增加而扩大。

Conclusion: 投影压缩在不增加推理计算量的前提下，能有效减少模型规模并提升性能，具有广泛的应用潜力。

Abstract: Large language models have steadily increased in size to achieve improved
performance; however, this growth has also led to greater inference time and
computational demands. Consequently, there is rising interest in model size
reduction methods. To address this issue, we propose Projected Compression, a
novel model compression technique, that reduces model weights by utilizing
projection modules. Specifically, we first train additional trainable
projections weights and preserve access to all the original model parameters.
Subsequently, these projections are merged into a lower-dimensional product
matrix, resulting in a reduced-size standard Transformer-based model. Unlike
alternative approaches that require additional computational overhead, our
method matches the base model's per-token computation step in FLOPs.
Experimental results show that Projected Compression outperforms the comparable
hard pruning and retraining approach on higher quality models. Moreover, the
performance margin scales well with the number of tokens.

</details>


### [245] [Score-Based Model for Low-Rank Tensor Recovery](https://arxiv.org/abs/2506.22295)
*Zhengyun Cheng,Changhao Wang,Guanwen Zhang,Yi Xu,Wei Zhou,Xiangyang Ji*

Main category: cs.LG

TL;DR: 本文研究了一种新型的基于分数的模型，用于低秩张量分解，摆脱了传统方法中对预设结构假设的依赖，从而提升了多种类型张量的数据处理表现。


<details>
  <summary>Details</summary>
Motivation: 传统低秩张量分解方法依赖预设的结构假设，但实际情况中这些假设往往不可得，这使得优化过程复杂并导致准确性损失。为了解决这一问题，本文提出了一种无需预定义结构假设的新方法。

Method: 设计了一个神经网络来学习能量函数，通过分数匹配优化以捕获张量条目和共享因子之间联合对数概率的梯度，并与块坐标下降算法结合实现张量补全和降噪。

Result: 实验表明，该方法在稀疏、连续时间张量和视觉数据等多种张量类型中均表现出显著的性能提升。

Conclusion: 提出了一种无需特定结构假设的低秩张量分解模型，通过实验验证，其在不同类型张量分析中的性能优越，证明了其通用性和有效性。

Abstract: Low-rank tensor decompositions (TDs) provide an effective framework for
multiway data analysis. Traditional TD methods rely on predefined structural
assumptions, such as CP or Tucker decompositions. From a probabilistic
perspective, these can be viewed as using Dirac delta distributions to model
the relationships between shared factors and the low-rank tensor. However, such
prior knowledge is rarely available in practical scenarios, particularly
regarding the optimal rank structure and contraction rules. The optimization
procedures based on fixed contraction rules are complex, and approximations
made during these processes often lead to accuracy loss. To address this issue,
we propose a score-based model that eliminates the need for predefined
structural or distributional assumptions, enabling the learning of
compatibility between tensors and shared factors. Specifically, a neural
network is designed to learn the energy function, which is optimized via score
matching to capture the gradient of the joint log-probability of tensor entries
and shared factors. Our method allows for modeling structures and distributions
beyond the Dirac delta assumption. Moreover, integrating the block coordinate
descent (BCD) algorithm with the proposed smooth regularization enables the
model to perform both tensor completion and denoising. Experimental results
demonstrate significant performance improvements across various tensor types,
including sparse and continuous-time tensors, as well as visual data.

</details>


### [246] [CoATA: Effective Co-Augmentation of Topology and Attribute for Graph Neural Networks](https://arxiv.org/abs/2506.22299)
*Tao Liu,Longlong Lin,Yunfeng Yu,Xi Ou,Youan Zhang,Zhiqiu Ye,Tao Jia*

Main category: cs.LG

TL;DR: 提出了一个名为CoATA的框架，旨在通过双通道增强图神经网络（GNN）在拓扑结构和属性上的性能。实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GNN方法在处理包含噪声和不完整信息的现实图数据时表现较差，且通常只关注单一维度增强，忽视了拓扑结构和节点属性之间的相互关系。

Method: 提出CoATA框架，首先通过传播结构信息丰富并去噪节点属性；接着通过节点-属性二分图在属性空间上进一步重建拓扑结构；最后通过对比学习，以原始图和增强图之间的原型对齐和一致性约束来实现相互修正。

Result: 在七个基准数据集上的实验表明，CoATA优于十一种最新的基线方法，验证了其对拓扑结构和属性协同关系的捕捉能力。

Conclusion: CoATA框架通过协同增强拓扑结构和节点属性，有效改善了GNN在噪声和不完整图数据上的表现，展现了强大的综合建模能力。

Abstract: Graph Neural Networks (GNNs) have garnered substantial attention due to their
remarkable capability in learning graph representations. However, real-world
graphs often exhibit substantial noise and incompleteness, which severely
degrades the performance of GNNs. Existing methods typically address this issue
through single-dimensional augmentation, focusing either on refining topology
structures or perturbing node attributes, thereby overlooking the deeper
interplays between the two. To bridge this gap, this paper presents CoATA, a
dual-channel GNN framework specifically designed for the Co-Augmentation of
Topology and Attribute. Specifically, CoATA first propagates structural signals
to enrich and denoise node attributes. Then, it projects the enhanced attribute
space into a node-attribute bipartite graph for further refinement or
reconstruction of the underlying structure. Subsequently, CoATA introduces
contrastive learning, leveraging prototype alignment and consistency
constraints, to facilitate mutual corrections between the augmented and
original graphs. Finally, extensive experiments on seven benchmark datasets
demonstrate that the proposed CoATA outperforms eleven state-of-the-art
baseline methods, showcasing its effectiveness in capturing the synergistic
relationship between topology and attributes.

</details>


### [247] [Weakly-Supervised Domain Adaptation with Proportion-Constrained Pseudo-Labeling](https://arxiv.org/abs/2506.22301)
*Takumi Okuo,Shinnosuke Matsuo,Shota Harada,Kiyohito Tanaka,Ryoma Bise*

Main category: cs.LG

TL;DR: 提出了一种弱监督领域适应方法，通过利用目标领域的类别比例信息，改善了模型在不同领域间的性能表现。


<details>
  <summary>Details</summary>
Motivation: 解决由于源领域和目标领域类别比例差异所带来的领域转移问题，特别是在医疗数据集中的应用挑战。

Method: 采用比例约束伪标签分配，通过目标领域的类别比例信息为未标注数据生成伪标签，提升目标领域预测性能。

Result: 在两个内窥镜数据集的实验中，即便目标领域仅有5%标记数据，该方法性能优于半监督领域适应技术；同时也证明了该方法对噪声比例的鲁棒性。

Conclusion: 该方法有效利用目标领域类别比例信息，在无需额外注释的情况下提高了领域适应性能，表现出强大的实际应用价值。

Abstract: Domain shift is a significant challenge in machine learning, particularly in
medical applications where data distributions differ across institutions due to
variations in data collection practices, equipment, and procedures. This can
degrade performance when models trained on source domain data are applied to
the target domain. Domain adaptation methods have been widely studied to
address this issue, but most struggle when class proportions between the source
and target domains differ. In this paper, we propose a weakly-supervised domain
adaptation method that leverages class proportion information from the target
domain, which is often accessible in medical datasets through prior knowledge
or statistical reports. Our method assigns pseudo-labels to the unlabeled
target data based on class proportion (called proportion-constrained
pseudo-labeling), improving performance without the need for additional
annotations. Experiments on two endoscopic datasets demonstrate that our method
outperforms semi-supervised domain adaptation techniques, even when 5% of the
target domain is labeled. Additionally, the experimental results with noisy
proportion labels highlight the robustness of our method, further demonstrating
its effectiveness in real-world application scenarios.

</details>


### [248] [Unfolding Generative Flows with Koopman Operators: Fast and Interpretable Sampling](https://arxiv.org/abs/2506.22304)
*Erkan Turan,Aristotelis Siozopoulos,Maks Ovsjanikov*

Main category: cs.LG

TL;DR: 本文提出将Koopman算子理论与条件流匹配（CFM）结合，以加速采样并提供可解释的生成模型动态分析。


<details>
  <summary>Details</summary>
Motivation: 当前CFM采样过程依赖于解非线性ODE，计算开销高且难以理解，而现有替代方案通常未揭示生成过程的结构性问题。

Method: 引入无解码器的Koopman-CFM架构，将生成动态映射为观测空间中的线性演化，实现通过矩阵指数的闭式一阶采样。

Result: 在2D数据集和实际基准（MNIST、F-MNIST、TFD）上实现了显著加速，且生成过程拥有明确结构，如谱特性和模式分解能力。

Conclusion: 该方法结合采样效率和解析结构，为快速且可解释的生成建模打开可能思路。

Abstract: Conditional Flow Matching (CFM) offers a simulation-free framework for
training continuous-time generative models, bridging diffusion and flow-based
approaches. However, sampling from CFM still relies on numerically solving
non-linear ODEs which can be computationally expensive and difficult to
interpret. Recent alternatives address sampling speed via trajectory
straightening, mini-batch coupling or distillation. However, these methods
typically do not shed light on the underlying \textit{structure} of the
generative process. In this work, we propose to accelerate CFM and introduce an
interpretable representation of its dynamics by integrating Koopman operator
theory, which models non-linear flows as linear evolution in a learned space of
observables. We introduce a decoder-free Koopman-CFM architecture that learns
an embedding where the generative dynamics become linear, enabling closed-form,
one-step sampling via matrix exponentiation. This results in significant
speedups over traditional CFM as demonstrated on controlled 2D datasets and
real-world benchmarks, MNIST, Fashion-MNIST (F-MNIST), and the Toronto Face
Dataset (TFD). Unlike previous methods, our approach leads to a well-structured
Koopman generator, whose spectral properties, eigenvalues, and eigenfunctions
offer principled tools for analyzing generative behavior such as temporal
scaling, mode stability, and decomposition in Koopman latent space. By
combining sampling efficiency with analytical structure, Koopman-enhanced flow
matching offers a potential step toward fast and interpretable generative
modeling.

</details>


### [249] [Less Greedy Equivalence Search](https://arxiv.org/abs/2506.22331)
*Adiba Ejaz,Elias Bareinboim*

Main category: cs.LG

TL;DR: 提出了一种新的因果发现算法LGES，比传统的GES算法更快速且准确。


<details>
  <summary>Details</summary>
Motivation: 传统的GES算法在计算成本和样本有限的情况下精度不高，需改进。

Method: 通过修改贪心搜索策略，避免在得分显示条件独立性的变量之间插入边，从而减少计算量，并加入利用先验假设的能力。

Result: LGES实现了最多10倍的性能提升，同时显著减少了结构错误，并在实验中显示其在速度、准确性和假设鲁棒性方面优于GES。

Conclusion: LGES在样本极限下能够从观察和干预数据中恢复真实等价类，并可纠正错误假设，表现优于其他基线算法。

Abstract: Greedy Equivalence Search (GES) is a classic score-based algorithm for causal
discovery from observational data. In the sample limit, it recovers the Markov
equivalence class of graphs that describe the data. Still, it faces two
challenges in practice: computational cost and finite-sample accuracy. In this
paper, we develop Less Greedy Equivalence Search (LGES), a variant of GES that
retains its theoretical guarantees while partially addressing these
limitations. LGES modifies the greedy step: rather than always applying the
highest-scoring insertion, it avoids edge insertions between variables for
which the score implies some conditional independence. This more targeted
search yields up to a \(10\)-fold speed-up and a substantial reduction in
structural error relative to GES. Moreover, LGES can guide the search using
prior assumptions, while correcting these assumptions when contradicted by the
data. Finally, LGES can exploit interventional data to refine the learned
observational equivalence class. We prove that LGES recovers the true
equivalence class in the sample limit from observational and interventional
data, even with misspecified prior assumptions. Experiments demonstrate that
LGES outperforms GES and other baselines in speed, accuracy, and robustness to
misspecified assumptions. Our code is available at
https://github.com/CausalAILab/lges.

</details>


### [250] [A Framework for Multi-source Privacy Preserving Epidemic Analysis](https://arxiv.org/abs/2506.22342)
*Zihan Guan,Zhiyuan Zhao,Fengwei Tian,Dung Nguyen,Payel Bhattacharjee,Ravi Tandon,B. Aditya Prakash,Anil Vullikanti*

Main category: cs.LG

TL;DR: 本文提出一个结合深度学习和流行病模型的框架，利用多种数据集（包括具备差分隐私保护的数据）进行流行病预测和传播机制学习，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决整合差分隐私保护数据集进行流行病预测和机制学习的问题，确保在敏感数据使用时提供隐私保护。

Method: 开发结合深度学习与流行病模型的框架，该框架整合支持差分隐私的数据集，并同时进行流行病预测和传播机制建模。

Result: 在一个包含差分隐私保护的合成金融数据集上验证了框架的有效性，结果表明即使在支持差分隐私的情况下，数据仍能显著提升预测和机制学习。

Conclusion: 该框架能够在包含敏感数据的场景下实现有效的流行病预测和机制学习，并确保隐私保护，扩展了差分隐私在公共健康分析中的应用前景。

Abstract: It is now well understood that diverse datasets provide a lot of value in key
epidemiology and public health analyses, such as forecasting and nowcasting,
development of epidemic models, evaluation and design of interventions and
resource allocation. Some of these datasets are often sensitive, and need
adequate privacy protections. There are many models of privacy, but
Differential Privacy (DP) has become a de facto standard because of its strong
guarantees, without making models about adversaries. In this paper, we develop
a framework the integrates deep learning and epidemic models to simultaneously
perform epidemic forecasting and learning a mechanistic model of epidemic
spread, while incorporating multiple datasets for these analyses, including
some with DP guarantees. We demonstrate our framework using a realistic but
synthetic financial dataset with DP; such a dataset has not been used in such
epidemic analyses. We show that this dataset provides significant value in
forecasting and learning an epidemic model, even when used with DP guarantees.

</details>


### [251] [Reinforcement Learning with Physics-Informed Symbolic Program Priors for Zero-Shot Wireless Indoor Navigation](https://arxiv.org/abs/2506.22365)
*Tao Li,Haozhe Lei,Mingsheng Yin,Yaqi Hu*

Main category: cs.LG

TL;DR: 提出了一种物理感知的程序引导强化学习（PiPRL）框架，将物理先验通过符号程序传递给强化学习代理，显著提升了训练效果和样本效率。


<details>
  <summary>Details</summary>
Motivation: 目前强化学习过程中融入物理先验的方式需要大量的人工操作与领域知识，这对普通用户不够友好，因此希望探索更自动化、更简易的方式。

Method: 开发了一个基于物理感知的程序引导强化学习（PiPRL）框架，通过层次化和模块化的神经-符号集成方式，将物理先验（以可解释的领域专用语言表示）引入强化学习；具体方法包括一个符号程序作为元控制程序，结合神经感知模块的语义信息和低层神经控制器的强化训练。

Result: 实验表明，PiPRL框架能够在保持性能优于纯符号或纯神经策略的同时，减少超过26%的训练时间。

Conclusion: PiPRL成功实现了物理先验的自动引入，显著提高了强化学习的样本效率，它在导航任务中的应用展示了潜力并降低了对领域知识的依赖。

Abstract: When using reinforcement learning (RL) to tackle physical control tasks,
inductive biases that encode physics priors can help improve sample efficiency
during training and enhance generalization in testing. However, the current
practice of incorporating these helpful physics-informed inductive biases
inevitably runs into significant manual labor and domain expertise, making them
prohibitive for general users. This work explores a symbolic approach to
distill physics-informed inductive biases into RL agents, where the physics
priors are expressed in a domain-specific language (DSL) that is human-readable
and naturally explainable. Yet, the DSL priors do not translate directly into
an implementable policy due to partial and noisy observations and additional
physical constraints in navigation tasks. To address this gap, we develop a
physics-informed program-guided RL (PiPRL) framework with applications to
indoor navigation. PiPRL adopts a hierarchical and modularized neuro-symbolic
integration, where a meta symbolic program receives semantically meaningful
features from a neural perception module, which form the bases for symbolic
programming that encodes physics priors and guides the RL process of a
low-level neural controller. Extensive experiments demonstrate that PiPRL
consistently outperforms purely symbolic or neural policies and reduces
training time by over 26% with the help of the program-based inductive biases.

</details>


### [252] [Sheaf-Based Decentralized Multimodal Learning for Next-Generation Wireless Communication Systems](https://arxiv.org/abs/2506.22374)
*Abdulmomen Ghalkha,Zhuojun Tian,Chaouki Ben Issaid,Mehdi Bennis*

Main category: cs.LG

TL;DR: 提出一种名为Sheaf-DMFL的新框架，用于在多模态通信系统中提高边缘设备之间的协作和学习能力，解决了传统联邦学习算法在多模态数据上的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习算法大多仅支持单模态数据和一致的模型结构，未能有效利用多模态数据的丰富信息，无法处理具有多样化模态和设备能力的实际场景。

Method: 通过酪理论构建Sheaf-DMFL框架，用于跨多模态设备的协作学习。每个客户端根据其模态特点实现局部特征编码器输出，并在任务层结合其他设备的编码结果建立模态相关性，同时提出结合注意力机制的Sheaf-DMFL-Att算法，以进一步捕捉模态间关联。

Result: 理论分析证明了Sheaf-DMFL-Att算法的收敛性，实验证明其在真实通信场景中的性能优越性，包括链路阻断预测和毫米波波束成形。

Conclusion: Sheaf-DMFL及其改进版本Sheaf-DMFL-Att成功解决了协作学习中的多模态数据问题，为多模态无线通信系统提供了有效的学习框架。

Abstract: In large-scale communication systems, increasingly complex scenarios require
more intelligent collaboration among edge devices collecting various multimodal
sensory data to achieve a more comprehensive understanding of the environment
and improve decision-making accuracy. However, conventional federated learning
(FL) algorithms typically consider unimodal datasets, require identical model
architectures, and fail to leverage the rich information embedded in multimodal
data, limiting their applicability to real-world scenarios with diverse
modalities and varying client capabilities. To address this issue, we propose
Sheaf-DMFL, a novel decentralized multimodal learning framework leveraging
sheaf theory to enhance collaboration among devices with diverse modalities.
Specifically, each client has a set of local feature encoders for its different
modalities, whose outputs are concatenated before passing through a
task-specific layer. While encoders for the same modality are trained
collaboratively across clients, we capture the intrinsic correlations among
clients' task-specific layers using a sheaf-based structure. To further enhance
learning capability, we propose an enhanced algorithm named Sheaf-DMFL-Att,
which tailors the attention mechanism within each client to capture
correlations among different modalities. A rigorous convergence analysis of
Sheaf-DMFL-Att is provided, establishing its theoretical guarantees. Extensive
simulations are conducted on real-world link blockage prediction and mmWave
beamforming scenarios, demonstrate the superiority of the proposed algorithms
in such heterogeneous wireless communication systems.

</details>


### [253] [Probabilistic Optimality for Inference-time Scaling](https://arxiv.org/abs/2506.22376)
*Youkang Wang,Jian Wang,Rubing Chen,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: 本文提出一种名为OptScale的新方法，通过概率框架在推理时间内优化大语言模型(LLMs)的推理性能，同时显著减少采样开销。


<details>
  <summary>Details</summary>
Motivation: 现有推理时间扩展方法多基于经验策略，缺乏理论依据，因而提出研究以弥补这一空白，提供计算高效的扩展指导。

Method: 通过理论分析推导采样数目与性能之间的关系，基于语言模型构建一个动态预测器OptScale，用于按需确定最小采样量以满足预设性能和置信度要求。

Result: 在数学推理数据集上验证了OptScale，显著降低了采样开销，同时推理性能优于或等同于最先进的方法。

Conclusion: 本文为推理时间扩展提供了理论基础与实际解决方案，提出的OptScale方法有效提升了LLMs复杂推理任务的效率。

Abstract: Inference-time scaling has emerged as a powerful technique for enhancing the
reasoning performance of Large Language Models (LLMs). However, existing
approaches often rely on heuristic strategies for parallel sampling, lacking a
principled foundation. To address this gap, we propose a probabilistic
framework that formalizes the optimality of inference-time scaling under the
assumption that parallel samples are independently and identically distributed
(i.i.d.), and where the Best-of-N selection strategy follows a probability
distribution that can be estimated. Within this framework, we derive a
theoretical lower bound on the required number of samples to achieve a target
performance level, providing the first principled guidance for
compute-efficient scaling. Leveraging this insight, we develop
\textsc{OptScale}, a practical algorithm that dynamically determines the
optimal number of sampled responses. \textsc{OptScale} employs a language
model-based predictor to estimate probabilistic prior parameters, enabling the
decision of the minimal number of samples needed that satisfy predefined
performance thresholds and confidence levels. Extensive experiments on
mathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)
demonstrate that \textsc{OptScale} significantly reduces sampling overhead
while remaining better or on par with state-of-the-art reasoning performance.
Our work offers both a theoretical foundation and a practical solution for
principled inference-time scaling, addressing a critical gap in the efficient
deployment of LLMs for complex reasoning.

</details>


### [254] [Towards Distributed Neural Architectures](https://arxiv.org/abs/2506.22389)
*Aditya Cowsik,Tianyu He,Andrey Gromov*

Main category: cs.LG

TL;DR: 研究提出了一种分布式神经结构（DNA）的框架并应用于视觉和语言领域，其核心思想是允许数据动态选择模块路线，从而实现高效计算和参数共享。


<details>
  <summary>Details</summary>
Motivation: 论文旨在推广稀疏方法（如专家混合等）的理念，通过引入分布式神经结构，动态优化数据流路径以提升模型性能和效率。

Method: 研究提出将多种模块（如Transformer、MLP、Attention等）整合为具有路由功能的“原型结构”，并通过端到端训练学习模块选择与计算模式。

Result: 实验结果表明，DNA在性能上可与传统密集模型竞争，同时能够从数据中学习计算效率和参数共享机制。此外，分析显示数据通过模型的路径符合幂律分布，且这些路径展示了显著的模块化和分工趋势。

Conclusion: DNAs提供了一种灵活、高效的模型训练框架，能够学习自动化分配计算和参数的方法，并且展现出模块间的自然分工和解释性特点。

Abstract: We introduce and train distributed neural architectures (DNA) in vision and
language domains. DNAs are initialized with a proto-architecture that consists
of (transformer, MLP, attention, etc.) modules and routers. Any token (or
patch) can traverse any series of modules in any order. DNAs are a natural
generalization of the sparse methods such as Mixture-of-Experts,
Mixture-of-Depths, parameter sharing, etc. Computation and communication
patterns of DNA modules are learnt end-to-end during training and depend on the
content and context of each token (or patch). These patterns can be shaped by
further requirements added to the optimization objective such as compute/memory
efficiency or load balancing. We empirically show that (i) trained DNAs are
competitive with the dense baselines in both domains and (ii) compute
efficiency/parameter sharing can be learnt from data. Next, we analyze the
emergent connectivity and computation patterns in the trained DNAs. We find
that the paths that tokens take through the models are themselves distributed
according to a power-law. We show that some paths (or, equivalently, groups of
modules) show emergent specialization. Finally, we demonstrate that models
learn to allocate compute and active parameters in an interpretable way.

</details>


### [255] [Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis](https://arxiv.org/abs/2506.22393)
*YongKyung Oh,Alex Bui*

Main category: cs.LG

TL;DR: 本文提出了利用多视角对比学习的框架，通过考虑时间模式、导数动态和频域特征，自适应处理不同医疗领域的时间序列数据，以提升模型在迁移学习任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决跨领域时间序列数据的迁移学习中因复杂时间依赖和动态分布变化所带来的挑战。

Method: 采用多视角对比学习框架，利用独立编码器和分层融合机制整合时间序列的多种特征形式，提取跨领域可迁移的特征不变表征。

Result: 在多种医疗数据集（EEG、ECG和EMG）上进行实验，结果显示该方法在迁移学习任务中显著优于现有最先进方法。

Conclusion: 该框架提升了医疗领域时间序列数据中机器学习模型的鲁棒性和泛化能力，为在多样化医疗环境中部署可靠的AI系统提供了可行之路。

Abstract: Adapting machine learning models to medical time series across different
domains remains a challenge due to complex temporal dependencies and dynamic
distribution shifts. Current approaches often focus on isolated feature
representations, limiting their ability to fully capture the intricate temporal
dynamics necessary for robust domain adaptation. In this work, we propose a
novel framework leveraging multi-view contrastive learning to integrate
temporal patterns, derivative-based dynamics, and frequency-domain features.
Our method employs independent encoders and a hierarchical fusion mechanism to
learn feature-invariant representations that are transferable across domains
while preserving temporal coherence. Extensive experiments on diverse medical
datasets, including electroencephalogram (EEG), electrocardiogram (ECG), and
electromyography (EMG) demonstrate that our approach significantly outperforms
state-of-the-art methods in transfer learning tasks. By advancing the
robustness and generalizability of machine learning models, our framework
offers a practical pathway for deploying reliable AI systems in diverse
healthcare settings.

</details>


### [256] [Exploration from a Primal-Dual Lens: Value-Incentivized Actor-Critic Methods for Sample-Efficient Online RL](https://arxiv.org/abs/2506.22401)
*Tong Yang,Bo Dai,Lin Xiao,Yuejie Chi*

Main category: cs.LG

TL;DR: 该论文提出了一种新的基于乐观正则化的价值激励型演员-评论家（VAC）方法来解决探索与利用之间的平衡问题，并在理论上证明了其在多种设置下的近似最优后悔界限。


<details>
  <summary>Details</summary>
Motivation: 目前强化学习在利用复杂函数近似（如变压器和深度神经网络）方面非常流行，但探索与利用的平衡仍是一个长期存在的挑战，缺乏具有理论保证的高效实用方案。

Method: 通过从原-对偶优化的角度审视乐观正则化，提出了一种新的价值激励型演员-评论家（VAC）方法，它优化了一个易于优化且整合探索与利用的单一目标函数，该目标函数促进了与收集的数据转换一致且具有更高值函数的状态-行动和策略估计。

Result: 理论上，提出的VAC方法在有限时间和无限时间的线性MDP下具有近似最优后悔界限，并可在适当假设下扩展到一般函数近似设置。

Conclusion: VAC方法以新颖的视角优化了探索与利用的平衡问题，具有良好的理论性能并展示了在强化学习领域中的潜力。

Abstract: Online reinforcement learning (RL) with complex function approximations such
as transformers and deep neural networks plays a significant role in the modern
practice of artificial intelligence. Despite its popularity and importance,
balancing the fundamental trade-off between exploration and exploitation
remains a long-standing challenge; in particular, we are still in lack of
efficient and practical schemes that are backed by theoretical performance
guarantees. Motivated by recent developments in exploration via optimistic
regularization, this paper provides an interpretation of the principle of
optimism through the lens of primal-dual optimization. From this fresh
perspective, we set forth a new value-incentivized actor-critic (VAC) method,
which optimizes a single easy-to-optimize objective integrating exploration and
exploitation -- it promotes state-action and policy estimates that are both
consistent with collected data transitions and result in higher value
functions. Theoretically, the proposed VAC method has near-optimal regret
guarantees under linear Markov decision processes (MDPs) in both finite-horizon
and infinite-horizon settings, which can be extended to the general function
approximation setting under appropriate assumptions.

</details>


### [257] [ARMOR: Robust Reinforcement Learning-based Control for UAVs under Physical Attacks](https://arxiv.org/abs/2506.22423)
*Pritam Dash,Ethan Chan,Nathan P. Lawrence,Karthik Pattabiraman*

Main category: cs.LG

TL;DR: 本文提出ARMOR，一种适用于无人机的抗攻击模型自由强化学习控制器，能在对抗性传感器操控下确保安全运行。


<details>
  <summary>Details</summary>
Motivation: 现有的无人机安全强化学习方法无法有效抵御物理攻击（如GPS欺骗）。

Method: 通过两阶段训练框架，分别用带有攻击信息的教师编码器生成隐状态，再由学生编码器通过历史数据学习该隐状态。

Result: ARMOR在对抗未见攻击的通用性和训练成本上比传统方法优越，可确保无人机安全。

Conclusion: ARMOR能利用健壮的隐状态表征，增强无人机在攻击情境下的操控能力且优化训练效率。

Abstract: Unmanned Aerial Vehicles (UAVs) depend on onboard sensors for perception,
navigation, and control. However, these sensors are susceptible to physical
attacks, such as GPS spoofing, that can corrupt state estimates and lead to
unsafe behavior. While reinforcement learning (RL) offers adaptive control
capabilities, existing safe RL methods are ineffective against such attacks. We
present ARMOR (Adaptive Robust Manipulation-Optimized State Representations),
an attack-resilient, model-free RL controller that enables robust UAV operation
under adversarial sensor manipulation. Instead of relying on raw sensor
observations, ARMOR learns a robust latent representation of the UAV's physical
state via a two-stage training framework. In the first stage, a teacher
encoder, trained with privileged attack information, generates attack-aware
latent states for RL policy training. In the second stage, a student encoder is
trained via supervised learning to approximate the teacher's latent states
using only historical sensor data, enabling real-world deployment without
privileged information. Our experiments show that ARMOR outperforms
conventional methods, ensuring UAV safety. Additionally, ARMOR improves
generalization to unseen attacks and reduces training cost by eliminating the
need for iterative adversarial training.

</details>


### [258] [CLoVE: Personalized Federated Learning through Clustering of Loss Vector Embeddings](https://arxiv.org/abs/2506.22427)
*Randeep Bhatia,Nikos Papadis,Murali Kodialam,TV Lakshman,Sayak Chakrabarty*

Main category: cs.LG

TL;DR: 本文提出了一种新的算法CLoVE，用于在联邦学习中实现客户的聚类，并优化特定群体的模型。


<details>
  <summary>Details</summary>
Motivation: 在聚类联邦学习中，由于客户数据分布的差异，自然会形成不同的聚类。当前难点在于无法直接识别这些聚类，因此需要新的方法有效分类客户。

Method: CLoVE基于客户数据的模型损失生成嵌入向量，通过分析损失模式相似性，实现客户的聚类，并通过联邦聚合优化每个聚类的模型。

Result: CLoVE能够高概率在单轮中准确识别聚类，并在不同数据集及非IID环境下实现高效训练，展现了最先进的模型准确性和集群恢复能力。

Conclusion: CLoVE在保持方法简单性及无需初始模型优化的同时，适用于有监督和无监督任务，使其更适合真实世界的应用场景。

Abstract: We propose CLoVE (Clustering of Loss Vector Embeddings), a novel algorithm
for Clustered Federated Learning (CFL). In CFL, clients are naturally grouped
into clusters based on their data distribution. However, identifying these
clusters is challenging, as client assignments are unknown. CLoVE utilizes
client embeddings derived from model losses on client data, and leverages the
insight that clients in the same cluster share similar loss values, while those
in different clusters exhibit distinct loss patterns. Based on these
embeddings, CLoVE is able to iteratively identify and separate clients from
different clusters and optimize cluster-specific models through federated
aggregation. Key advantages of CLoVE over existing CFL algorithms are (1) its
simplicity, (2) its applicability to both supervised and unsupervised settings,
and (3) the fact that it eliminates the need for near-optimal model
initialization, which makes it more robust and better suited for real-world
applications. We establish theoretical convergence bounds, showing that CLoVE
can recover clusters accurately with high probability in a single round and
converges exponentially fast to optimal models in a linear setting. Our
comprehensive experiments comparing with a variety of both CFL and generic
Personalized Federated Learning (PFL) algorithms on different types of datasets
and an extensive array of non-IID settings demonstrate that CLoVE achieves
highly accurate cluster recovery in just a few rounds of training, along with
state-of-the-art model accuracy, across a variety of both supervised and
unsupervised PFL tasks.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [259] [An Effective Two-Phase Genetic Algorithm for Solving the Resource Constrained Project Scheduling Problem (RCPSP)](https://arxiv.org/abs/2506.21915)
*D. Sun,S. Zhou*

Main category: cs.NE

TL;DR: 提出了一种名为2PGA的改进遗传算法，用于解决RCPSP问题，通过两阶段父代选择机制实现全局探索与局部优化，实验显示对标准问题有良好效果。


<details>
  <summary>Details</summary>
Motivation: 改进遗传算法以有效解决RCPSP问题，结合局部优化与全局探索能力。

Method: 提出2PGA算法，分为两阶段：第一阶段选用当前最优解进行局部优化，第二阶段排除当前最优解进行全局探索，两阶段交替进行迭代。

Result: 2PGA在PSPLIB的标准问题中表现优异，改进了一些已有的启发式最优解。

Conclusion: 2PGA算法具有有效性，成功平衡了解空间的探索与开发，提供了RCPSP问题求解的改进方案。

Abstract: This note presents a simple and effective variation of genetic algorithm (GA)
for solving RCPSP, denoted as 2-Phase Genetic Algorithm (2PGA). The 2PGA
implements GA parent selection in two phases: Phase-1 includes the best current
solutions in the parent pool, and Phase-2 excludes the best current solutions
from the parent pool. The 2PGA carries out the GA evolution by alternating the
two phases iteratively. In exploring a solution space, the Phase-1 emphasizes
intensification in current neighborhood, while the Phase-2 emphasizes
diversification to escape local traps. The 2PGA was tested on the standard
benchmark problems in PSPLIB, the results have shown that the algorithm is
effective and has improved some of the best heuristic solutions.

</details>


### [260] [In situ fine-tuning of in silico trained Optical Neural Networks](https://arxiv.org/abs/2506.22122)
*Gianluca Kosmella,Ripalta Stabile,Jaron Sanders*

Main category: cs.NE

TL;DR: 本文提出一种名为GIFT（梯度信息微调）的算法，有效提升光学神经网络（ONN）在噪声误差条件下的性能，相较于传统方法提升了高达28%的准确率。


<details>
  <summary>Details</summary>
Motivation: 光学神经网络具有超快计算、高带宽、低能耗等优势，但训练过程中需要将模拟的数字模型参数映射到实际硬件，常因噪声和制造误差导致性能下降。

Method: 提出一种名为GIFT的算法，使用ONN噪声结构中的梯度信息对预训练参数进行轻量化的直接适配，无需高昂的再训练或复杂的实验设置。

Result: 通过在5层ONN上的MNIST分类任务模拟实验，GIFT在噪声误差条件下提高了相对准确率高达28%。

Conclusion: GIFT为数字模型与实际ONN实现间的差距提供了一种实用的解决方案，有助于进一步提高光学神经网络的性能。

Abstract: Optical Neural Networks (ONNs) promise significant advantages over
traditional electronic neural networks, including ultrafast computation, high
bandwidth, and low energy consumption, by leveraging the intrinsic capabilities
of photonics. However, training ONNs poses unique challenges, notably the
reliance on simplified in silico models whose trained parameters must
subsequently be mapped to physical hardware. This process often introduces
inaccuracies due to discrepancies between the idealized digital model and the
physical ONN implementation, particularly stemming from noise and fabrication
imperfections.
  In this paper, we analyze how noise misspecification during in silico
training impacts ONN performance and we introduce Gradient-Informed Fine-Tuning
(GIFT), a lightweight algorithm designed to mitigate this performance
degradation. GIFT uses gradient information derived from the noise structure of
the ONN to adapt pretrained parameters directly in situ, without requiring
expensive retraining or complex experimental setups. GIFT comes with formal
conditions under which it improves ONN performance.
  We also demonstrate the effectiveness of GIFT via simulation on a five-layer
feed forward ONN trained on the MNIST digit classification task. GIFT achieves
up to $28\%$ relative accuracy improvement compared to the baseline performance
under noise misspecification, without resorting to costly retraining. Overall,
GIFT provides a practical solution for bridging the gap between simplified
digital models and real-world ONN implementations.

</details>
