<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 156]
- [cs.CL](#cs.CL) [Total: 81]
- [cs.AI](#cs.AI) [Total: 55]
- [cs.LG](#cs.LG) [Total: 110]
- [cs.NE](#cs.NE) [Total: 8]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A Deep Learning-Based CCTV System for Automatic Smoking Detection in Fire Exit Zones](https://arxiv.org/abs/2508.11696)
*Sami Sadat,Mohammad Irtiza Hossain,Junaid Ahmed Sifat,Suhail Haque Rafi,Md. Waseq Alauddin Alvi,Md. Khalilur Rhaman*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的实时吸烟检测系统，用于监控火灾出口的安全。


<details>
  <summary>Details</summary>
Motivation: 关键安全需求驱动了开发此系统，用于在火灾出口区域实时检测吸烟行为。

Method: 评估了YOLOv8、YOLOv11和YOLOv12模型，并基于YOLOv8开发了一种定制模型，以适应复杂监控场景。

Result: 提出的模型表现优于其他模型，召回率为78.90%，mAP在50时为83.70%，并在多设备上表现出优越的实时性能。

Conclusion: 该系统提供了一种鲁棒且可适应的平台，有助于公共安全监控和自动化法规合规性。

Abstract: A deep learning real-time smoking detection system for CCTV surveillance of
fire exit areas is proposed due to critical safety requirements. The dataset
contains 8,124 images from 20 different scenarios along with 2,708 raw samples
demonstrating low-light areas. We evaluated three advanced object detection
models: YOLOv8, YOLOv11, and YOLOv12, followed by development of a custom model
derived from YOLOv8 with added structures for challenging surveillance
contexts. The proposed model outperformed the others, achieving a recall of
78.90 percent and mAP at 50 of 83.70 percent, delivering optimal object
detection across varied environments. Performance evaluation on multiple edge
devices using multithreaded operations showed the Jetson Xavier NX processed
data at 52 to 97 milliseconds per inference, establishing its suitability for
time-sensitive operations. This system offers a robust and adaptable platform
for monitoring public safety and enabling automatic regulatory compliance.

</details>


### [2] [Separating Knowledge and Perception with Procedural Data](https://arxiv.org/abs/2508.11697)
*Adrián Rodríguez-Muñoz,Manel Baradad,Phillip Isola,Antonio Torralba*

Main category: cs.CV

TL;DR: 本研究通过仅使用程序生成数据训练表示模型，并利用视觉存储库实现无需进一步训练的多项任务。


<details>
  <summary>Details</summary>
Motivation: 探索仅依赖程序生成数据而不使用真实图像数据集，是否能实现高效视觉任务，减少对真实数据的依赖。

Method: 利用程序生成数据训练表示模型，通过视觉存储库实现图像嵌入检索，并测试其在多视觉任务上的表现。

Result: 在不同数据集上，该方法实现了与基于真实数据训练的模型相仿的精度，在某些任务上甚至超越；在零样本分割任务中，其效果接近真实数据训练模型。

Conclusion: 程序生成数据可以代替真实数据进行有效模型训练，但不同对象部位的嵌入不一致可能是性能差距原因，并需要进一步优化。

Abstract: We train representation models with procedural data only, and apply them on
visual similarity, classification, and semantic segmentation tasks without
further training by using visual memory -- an explicit database of reference
image embeddings. Unlike prior work on visual memory, our approach achieves
full compartmentalization with respect to all real-world images while retaining
strong performance. Compared to a model trained on Places, our procedural model
performs within $1\%$ on NIGHTS visual similarity, outperforms by $8\%$ and
$15\%$ on CUB200 and Flowers102 fine-grained classification, and is within
$10\%$ on ImageNet-1K classification. It also demonstrates strong zero-shot
segmentation, achieving an $R^2$ on COCO within $10\%$ of the models trained on
real data. Finally, we analyze procedural versus real data models, showing that
parts of the same object have dissimilar representations in procedural models,
resulting in incorrect searches in memory and explaining the remaining
performance gap.

</details>


### [3] [FusionFM: Fusing Eye-specific Foundational Models for Optimized Ophthalmic Diagnosis](https://arxiv.org/abs/2508.11721)
*Ke Zou,Jocelyn Hui Lin Goh,Yukun Zhou,Tian Lin,Samantha Min Er Yew,Sahana Srinivasan,Meng Wang,Rui Santos,Gabor M. Somfai,Huazhu Fu,Haoyu Chen,Pearse A. Keane,Ching-Yu Cheng,Yih Chung Tham*

Main category: cs.CV

TL;DR: 本文提出了FusionFM框架及两种融合方法，系统评估单一和融合的眼科基础模型在多任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前眼科基础模型的表现尚不明确，需要研究其在不同任务和融合后的效果。

Method: 设计了FusionFM评估框架，使用AUC和F1指标评价四种基础模型的性能，并提出两种模型融合方法。

Result: DINORET和RetiZero模型在眼部和系统性疾病任务中表现优异；融合策略在部分疾病预测中有提高，但对系统性疾病的预测仍具挑战。

Conclusion: 提供了一种系统的基础模型评估方法，揭示了模型融合的益处并指明了提升临床应用的方向。

Abstract: Foundation models (FMs) have shown great promise in medical image analysis by
improving generalization across diverse downstream tasks. In ophthalmology,
several FMs have recently emerged, but there is still no clear answer to
fundamental questions: Which FM performs the best? Are they equally good across
different tasks? What if we combine all FMs together? To our knowledge, this is
the first study to systematically evaluate both single and fused ophthalmic
FMs. To address these questions, we propose FusionFM, a comprehensive
evaluation suite, along with two fusion approaches to integrate different
ophthalmic FMs. Our framework covers both ophthalmic disease detection
(glaucoma, diabetic retinopathy, and age-related macular degeneration) and
systemic disease prediction (diabetes and hypertension) based on retinal
imaging. We benchmarked four state-of-the-art FMs (RETFound, VisionFM,
RetiZero, and DINORET) using standardized datasets from multiple countries and
evaluated their performance using AUC and F1 metrics. Our results show that
DINORET and RetiZero achieve superior performance in both ophthalmic and
systemic disease tasks, with RetiZero exhibiting stronger generalization on
external datasets. Regarding fusion strategies, the Gating-based approach
provides modest improvements in predicting glaucoma, AMD, and hypertension.
Despite these advances, predicting systemic diseases, especially hypertension
in external cohort remains challenging. These findings provide an
evidence-based evaluation of ophthalmic FMs, highlight the benefits of model
fusion, and point to strategies for enhancing their clinical applicability.

</details>


### [4] [UniDCF: A Foundation Model for Comprehensive Dentocraniofacial Hard Tissue Reconstruction](https://arxiv.org/abs/2508.11728)
*Chunxia Ren,Ning Zhu,Yue Lai,Gui Chen,Ruijie Wang,Yangyi Hu,Suyao Liu,Shuwen Mao,Hong Su,Yu Zhang,Li Xiao*

Main category: cs.CV

TL;DR: 本文提出了UniDCF，一个能够通过多模态融合编码点云和多视图图像来重建多种牙颅面硬组织的统一框架，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在重建牙颅面硬组织中仅限于单一组织和单一模态输入，难以在实现解剖精度、计算效率和跨组织适应性间平衡。

Method: 提出UniDCF框架，结合点云和多视图图像的多模态融合，通过评分去噪模块提高表面平滑度，并利用大规模多模态数据集进行训练和评估。

Result: UniDCF在几何精度、结构完整性和空间精确度上超越现有方法，并显著减少99%的重建设计时间，临床接受度超94%。

Conclusion: UniDCF实现了快速、自动化和高保真度的牙颅面硬组织重建，有助于个性化精准治疗、优化临床流程、改善患者成果。

Abstract: Dentocraniofacial hard tissue defects profoundly affect patients'
physiological functions, facial aesthetics, and psychological well-being,
posing significant challenges for precise reconstruction. Current deep learning
models are limited to single-tissue scenarios and modality-specific imaging
inputs, resulting in poor generalizability and trade-offs between anatomical
fidelity, computational efficiency, and cross-tissue adaptability. Here we
introduce UniDCF, a unified framework capable of reconstructing multiple
dentocraniofacial hard tissues through multimodal fusion encoding of point
clouds and multi-view images. By leveraging the complementary strengths of each
modality and incorporating a score-based denoising module to refine surface
smoothness, UniDCF overcomes the limitations of prior single-modality
approaches. We curated the largest multimodal dataset, comprising intraoral
scans, CBCT, and CT from 6,609 patients, resulting in 54,555 annotated
instances. Evaluations demonstrate that UniDCF outperforms existing
state-of-the-art methods in terms of geometric precision, structural
completeness, and spatial accuracy. Clinical simulations indicate UniDCF
reduces reconstruction design time by 99% and achieves clinician-rated
acceptability exceeding 94%. Overall, UniDCF enables rapid, automated, and
high-fidelity reconstruction, supporting personalized and precise restorative
treatments, streamlining clinical workflows, and enhancing patient outcomes.

</details>


### [5] [Ovis2.5 Technical Report](https://arxiv.org/abs/2508.11737)
*Shiyin Lu,Yang Li,Yu Xia,Yuwei Hu,Shanshan Zhao,Yanqing Ma,Zhichao Wei,Yinglun Li,Lunhao Duan,Jianshan Zhao,Yuxuan Han,Haijun Li,Wanying Chen,Junke Tang,Chengkun Hou,Zhixing Du,Tianli Zhou,Wenjie Zhang,Huping Ding,Jiahe Li,Wen Li,Gui Hu,Yiliang Gu,Siran Yang,Jiamang Wang,Hailong Sun,Yibo Wang,Hui Sun,Jinlong Huang,Yuping He,Shengze Shi,Weihong Zhang,Guodong Zheng,Junpeng Jiang,Sensen Gao,Yi-Feng Wu,Sijia Chen,Yuhui Chen,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CV

TL;DR: Ovis2.5集成了原生分辨率视觉变换器和先进的推理能力，经过五阶段培训课程，其性能显著提升，在多个基准测试中达到开源领域的SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 通过引入原生分辨率视觉变换器和增强的推理能力，解决固定分辨率图像处理导致的细节丢失问题，并提升多模态任务性能。

Method: 采用五阶段课程训练，包括初始预训练、大规模指令微调、使用DPO和GRPO进行对齐和推理增强，结合多模态数据处理与混合并行加速。

Result: 两种开放模型Ovis2.5-9B和Ovis2.5-2B在OpenCompass多模态排行榜中表现出色，分别达成78.3与73.9分，且在多个细分基准测试中达到SOTA水平。

Conclusion: Ovis2.5显著优化了视觉处理与多模态推理能力，为资源有限场景提供了小模型高性能选择，在开源领域实现多项领先表现。

Abstract: We present Ovis2.5, a successor to Ovis2 designed for native-resolution
visual perception and strong multimodal reasoning. Ovis2.5 integrates a
native-resolution vision transformer that processes images at their native,
variable resolutions, avoiding the degradation from fixed-resolution tiling and
preserving both fine detail and global layout -- crucial for visually dense
content like complex charts. To strengthen reasoning, we train the model to
move beyond linear chain-of-thought and perform reflection -- including
self-checking and revision. This advanced capability is exposed as an optional
"thinking mode" at inference time, allowing users to trade latency for enhanced
accuracy on difficult inputs. The model is trained via a comprehensive
five-phase curriculum that progressively builds its skills. The process begins
with foundational visual and multimodal pretraining, advances through
large-scale instruction tuning, and culminates in alignment and reasoning
enhancement using DPO and GRPO. To scale these upgrades efficiently, we employ
multimodal data packing and hybrid parallelism, yielding a significant
end-to-end speedup. We release two open-source models: Ovis2.5-9B and
Ovis2.5-2B. The latter continues the "small model, big performance" philosophy
of Ovis2, making it ideal for resource-constrained, on-device scenarios. On the
OpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a
substantial improvement over its predecessor, Ovis2-8B, and achieving
state-of-the-art results among open-source MLLMs in the sub-40B parameter
range; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate
scores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong
capabilities on grounding and video tasks, and achieves open-source SOTA at its
scale for complex chart analysis.

</details>


### [6] [VideoAVE: A Multi-Attribute Video-to-Text Attribute Value Extraction Dataset and Benchmark Models](https://arxiv.org/abs/2508.11801)
*Ming Cheng,Tong Wu,Jiazhen Hu,Jiaying Gong,Hoda Eldardiry*

Main category: cs.CV

TL;DR: 本文提出了VideoAVE，这是首个面向电商领域视频到文本的属性值提取数据集，并提供了基准测试和评估代码。


<details>
  <summary>Details</summary>
Motivation: 目前的属性值提取数据集缺乏对视频的支持、属性覆盖的多样性以及公共可用性。

Method: 提出了名为VideoAVE的数据集，涵盖14个领域和172种独特属性；使用CLIP-MoE方法过滤视频与产品不匹配的数据，确保数据质量；并通过对最先进的视频视觉语言模型进行基准测试进行评估。

Result: 提供了224k条训练数据和25k条评估数据；研究结果揭示视频到文本的属性值提取依然非常具有挑战性，尤其在开放设置下。

Conclusion: 该数据集的发布填补了视频到文本属性值提取领域的空白，为开发利用时序信息的先进模型提供了重要工具，且代码公开供研究者使用。

Abstract: Attribute Value Extraction (AVE) is important for structuring product
information in e-commerce. However, existing AVE datasets are primarily limited
to text-to-text or image-to-text settings, lacking support for product videos,
diverse attribute coverage, and public availability. To address these gaps, we
introduce VideoAVE, the first publicly available video-to-text e-commerce AVE
dataset across 14 different domains and covering 172 unique attributes. To
ensure data quality, we propose a post-hoc CLIP-based Mixture of Experts
filtering system (CLIP-MoE) to remove the mismatched video-product pairs,
resulting in a refined dataset of 224k training data and 25k evaluation data.
In order to evaluate the usability of the dataset, we further establish a
comprehensive benchmark by evaluating several state-of-the-art video vision
language models (VLMs) under both attribute-conditioned value prediction and
open attribute-value pair extraction tasks. Our results analysis reveals that
video-to-text AVE remains a challenging problem, particularly in open settings,
and there is still room for developing more advanced VLMs capable of leveraging
effective temporal information. The dataset and benchmark code for VideoAVE are
available at: https://github.com/gjiaying/VideoAVE

</details>


### [7] [An MLP Baseline for Handwriting Recognition Using Planar Curvature and Gradient Orientation](https://arxiv.org/abs/2508.11803)
*Azam Nouri*

Main category: cs.CV

TL;DR: 本研究探讨了仅使用曲率等几何特征对手写字符进行分类的可行性，并用MLP实现了优异的结果。


<details>
  <summary>Details</summary>
Motivation: 尝试通过手工设计的几何特征，代替CNN，实现对手写字符的高准确度分类，研究解释性特征的潜力。

Method: 使用平面曲率幅度、曲率符号及梯度方向等特征作为输入，引入MLP分类器进行手写字符识别。

Result: MLP在MNIST数据集上取得了97%的准确率，在EMNIST数据集上取得了89%的准确率。

Conclusion: 曲率特征对于手写字符具有强大的区分能力，即使无需复杂的深度学习，也可通过可解释的特征完成高性能分类。

Abstract: This study investigates whether second-order geometric cues - planar
curvature magnitude, curvature sign, and gradient orientation - are sufficient
on their own to drive a multilayer perceptron (MLP) classifier for handwritten
character recognition (HCR), offering an alternative to convolutional neural
networks (CNNs). Using these three handcrafted feature maps as inputs, our
curvature-orientation MLP achieves 97 percent accuracy on MNIST digits and 89
percent on EMNIST letters. These results underscore the discriminative power of
curvature-based representations for handwritten character images and
demonstrate that the advantages of deep learning can be realized even with
interpretable, hand-engineered features.

</details>


### [8] [Labels or Input? Rethinking Augmentation in Multimodal Hate Detection](https://arxiv.org/abs/2508.11808)
*Sahajpreet Singh,Rongxin Ouyang,Subhayan Mukerjee,Kokil Jaidka*

Main category: cs.CV

TL;DR: 本文提出了一个双管齐下的策略来改进多模态仇恨检测，优化提示框架并引入数据增强流程，以提高模型的鲁棒性和公平性。


<details>
  <summary>Details</summary>
Motivation: 当前互联网充斥着多模态内容，检测包含图片和文字的仇恨言论（如幽默或讽刺形式）变得更加困难，同时，现有的视觉-语言模型（VLMs）在捕捉细粒度监督和隐性仇恨言论方面存在不足。

Method: 提出了两个主要方法：1）设计提示优化框架，通过调整提示结构、监督粒度及训练模式来提升检测性能；2）引入多模态数据增强管道，通过对仇恨模式进行分离和重写，生成2,479条中性对照的仇恨模因。

Result: 优化提示设计提高了小模型的鲁棒性，InternVL2在二元及缩放设置中实现最佳F1-score；数据增强流程减少了虚假相关性，并改进了分类器的泛化能力。

Conclusion: 提示结构与数据构成与模型大小一样重要，针对性的增强能为更可信且具敏感性的仇恨检测提供支持。

Abstract: The modern web is saturated with multimodal content, intensifying the
challenge of detecting hateful memes, where harmful intent is often conveyed
through subtle interactions between text and image under the guise of humor or
satire. While recent advances in Vision-Language Models (VLMs) show promise,
these models lack support for fine-grained supervision and remain susceptible
to implicit hate speech. In this paper, we present a dual-pronged approach to
improve multimodal hate detection. First, we propose a prompt optimization
framework that systematically varies prompt structure, supervision granularity,
and training modality. We show that prompt design and label scaling both
influence performance, with structured prompts improving robustness even in
small models, and InternVL2 achieving the best F1-scores across binary and
scaled settings. Second, we introduce a multimodal data augmentation pipeline
that generates 2,479 counterfactually neutral memes by isolating and rewriting
the hateful modality. This pipeline, powered by a multi-agent LLM-VLM setup,
successfully reduces spurious correlations and improves classifier
generalization. Our approaches inspire new directions for building synthetic
data to train robust and fair vision-language models. Our findings demonstrate
that prompt structure and data composition are as critical as model size, and
that targeted augmentation can support more trustworthy and context-sensitive
hate detection.

</details>


### [9] [Towards Understanding 3D Vision: the Role of Gaussian Curvature](https://arxiv.org/abs/2508.11825)
*Sherlon Almeida da Silva,Davi Geiger,Luiz Velho,Moacir Antonelli Ponti*

Main category: cs.CV

TL;DR: 本文探讨了高斯曲率在3D表面建模中的作用，并展示其潜力和应用。


<details>
  <summary>Details</summary>
Motivation: 当前计算机视觉发展的核心依赖于深度学习和大规模数据集，但这些方法缺乏对3D几何的显式建模。本文旨在探索高斯曲率在填补这一空白中的可能性。

Method: 利用Middlebury立体视觉数据集，分析了高斯曲率描述3D表面及其在立体匹配和深度重建中的潜在贡献。

Result: 证明高斯曲率可提供稀疏且紧凑的3D表面描述，并可作为几何先验改善3D表面重建，同时可能用作无监督的立体方法评价指标。

Conclusion: 高斯曲率不仅是一个重要的不变量，还可成为优化和创新3D建模的方法。

Abstract: Recent advances in computer vision have predominantly relied on data-driven
approaches that leverage deep learning and large-scale datasets. Deep neural
networks have achieved remarkable success in tasks such as stereo matching and
monocular depth reconstruction. However, these methods lack explicit models of
3D geometry that can be directly analyzed, transferred across modalities, or
systematically modified for controlled experimentation. We investigate the role
of Gaussian curvature in 3D surface modeling. Besides Gaussian curvature being
an invariant quantity under change of observers or coordinate systems, we
demonstrate using the Middlebury stereo dataset that it offers: (i) a sparse
and compact description of 3D surfaces, (ii) state-of-the-art monocular and
stereo methods seem to implicitly consider it, but no explicit module of such
use can be extracted, (iii) a form of geometric prior that can inform and
improve 3D surface reconstruction, and (iv) a possible use as an unsupervised
metric for stereo methods.

</details>


### [10] [From Pixels to Graphs: Deep Graph-Level Anomaly Detection on Dermoscopic Images](https://arxiv.org/abs/2508.11826)
*Dehn Xu,Tim Katzke,Emmanuel Müller*

Main category: cs.CV

TL;DR: 本文探讨了图神经网络（GNN）在图级异常检测（GLAD）中的应用，并系统性地评估了多种图构建方法在皮肤镜图像中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前图神经网络应用于由图像衍生的图表示，但尚无研究对多种图像转图方法在GLAD任务中的效果进行系统性比较。

Method: 本文通过实验系统评估了多种分割模式、边构建策略及基于颜色、纹理和形状描述子的节点特征集，结合GLAD模型在监督、弱监督和无监督环境下的表现。

Result: 实验表明，颜色特征表现最佳，但结合形状与纹理特征能稳定提高检测效果。在无监督条件下，OCGTL模型AUC-ROC达到0.805，添加稀疏标签后提升至0.872，完全监督下达到0.914。

Conclusion: 色彩特征在图构建中最为有效，形状与纹理特征的补充进一步提升效果。GLAD任务通过优化图像衍生图表示可以实现较强性能，在不同监督环境下均表现优异。

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful approach for
graph-based machine learning tasks. Previous work applied GNNs to image-derived
graph representations for various downstream tasks such as classification or
anomaly detection. These transformations include segmenting images, extracting
features from segments, mapping them to nodes, and connecting them. However, to
the best of our knowledge, no study has rigorously compared the effectiveness
of the numerous potential image-to-graph transformation approaches for
GNN-based graph-level anomaly detection (GLAD). In this study, we
systematically evaluate the efficacy of multiple segmentation schemes, edge
construction strategies, and node feature sets based on color, texture, and
shape descriptors to produce suitable image-derived graph representations to
perform graph-level anomaly detection. We conduct extensive experiments on
dermoscopic images using state-of-the-art GLAD models, examining performance
and efficiency in purely unsupervised, weakly supervised, and fully supervised
regimes. Our findings reveal, for example, that color descriptors contribute
the best standalone performance, while incorporating shape and texture features
consistently enhances detection efficacy. In particular, our best unsupervised
configuration using OCGTL achieves a competitive AUC-ROC score of up to 0.805
without relying on pretrained backbones like comparable image-based approaches.
With the inclusion of sparse labels, the performance increases substantially to
0.872 and with full supervision to 0.914 AUC-ROC.

</details>


### [11] [Recent Advances in Transformer and Large Language Models for UAV Applications](https://arxiv.org/abs/2508.11834)
*Hamza Kheddar,Yassine Habchi,Mohamed Chahine Ghanem,Mustapha Hemis,Dusit Niyato*

Main category: cs.CV

TL;DR: 本文综述了Transformer架构在无人机(UAV)中的应用，包括注意机制、混合模型和强化学习Transformer等，提供统一分类，比较分析以及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer模型的发展，无人机系统在感知、决策和自主性方面取得了显著进步，亟需对相关研究进行系统性梳理与总结。

Method: 系统归纳与评估了Transformers在无人机领域的应用，从模型分类、关键数据集、应用场景和性能基准等多个方面展开分析，同时审视文献中的研究空白。

Result: 总结了Transformer模型在无人机领域多个应用场景中的表现，明确了其优缺点并提出若干未来研究方向。

Conclusion: 该综述为理解和推动Transformer驱动的无人机技术提供了全面的指南。

Abstract: The rapid advancement of Transformer-based models has reshaped the landscape
of uncrewed aerial vehicle (UAV) systems by enhancing perception,
decision-making, and autonomy. This review paper systematically categorizes and
evaluates recent developments in Transformer architectures applied to UAVs,
including attention mechanisms, CNN-Transformer hybrids, reinforcement learning
Transformers, and large language models (LLMs). Unlike previous surveys, this
work presents a unified taxonomy of Transformer-based UAV models, highlights
emerging applications such as precision agriculture and autonomous navigation,
and provides comparative analyses through structured tables and performance
benchmarks. The paper also reviews key datasets, simulators, and evaluation
metrics used in the field. Furthermore, it identifies existing gaps in the
literature, outlines critical challenges in computational efficiency and
real-time deployment, and offers future research directions. This comprehensive
synthesis aims to guide researchers and practitioners in understanding and
advancing Transformer-driven UAV technologies.

</details>


### [12] [ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by 3D Gaussian Splat Camouflages](https://arxiv.org/abs/2508.11854)
*Matthew Hull,Haoyang Yang,Pratham Mehta,Mansi Phute,Aeree Cho,Haorang Wang,Matthew Lau,Wenke Lee,Wilian Lunardi,Martin Andreoni,Polo Chau*

Main category: cs.CV

TL;DR: 3D高斯点技术在安全关键任务中的应用面临新型攻击威胁，该研究提出了一种攻击方法可嵌入仅从特定视角可见的敌对内容。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用3D高斯点技术的复杂性发起针对安全任务的敌对攻击。

Method: 提出ComplicitSplat攻击，利用3DGS的标准阴影方法生成随视角变化的迷彩图案，无需访问模型架构或权重即可嵌入特定视角下的敌对内容。

Result: 经实验证明，该攻击能成功影响多种主流探测模型，包括单阶段、多阶段及基于Transformer的模型，同时适用于真实场景及合成场景。

Conclusion: 此项研究揭示了基于3DGS的全新黑盒攻击形式，为自主导航等任务关键型系统带来了潜在安全威胁。

Abstract: As 3D Gaussian Splatting (3DGS) gains rapid adoption in safety-critical tasks
for efficient novel-view synthesis from static images, how might an adversary
tamper images to cause harm? We introduce ComplicitSplat, the first attack that
exploits standard 3DGS shading methods to create viewpoint-specific camouflage
- colors and textures that change with viewing angle - to embed adversarial
content in scene objects that are visible only from specific viewpoints and
without requiring access to model architecture or weights. Our extensive
experiments show that ComplicitSplat generalizes to successfully attack a
variety of popular detector - both single-stage, multi-stage, and
transformer-based models on both real-world capture of physical objects and
synthetic scenes. To our knowledge, this is the first black-box attack on
downstream object detectors using 3DGS, exposing a novel safety risk for
applications like autonomous navigation and other mission-critical robotic
systems.

</details>


### [13] [Impact of Clinical Image Quality on Efficient Foundation Model Finetuning](https://arxiv.org/abs/2508.11864)
*Yucheng Tang,Pawel Rajwa,Alexander Ng,Yipei Wang,Wen Yan,Natasha Thorley,Aqua Asif,Clare Allen,Louise Dickinson,Francesco Giganti,Shonit Punwani,Daniel C. Alexander,Veeru Kasivisvanathan,Yipeng Hu*

Main category: cs.CV

TL;DR: 研究探讨了在前列腺多参数MRI中，使用经过大规模训练的领域特定基础模型ProFound，如何影响基于不同图像质量的微调和测试结果。研究表明图像质量分布及其不匹配会显著影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像中的基础模型具有较高的标签效率，能通过少量标注数据实现良好预测性能。本研究旨在评估图像质量对前列腺MRI基础模型微调的影响，探索其适应能力。

Method: 实验通过系统化地调整微调和测试集中高质量与低质量图像的比例，分析模型在自动化放射报告和癌症检测等任务中的性能表现。

Result: 结果表明：1）微调与测试集间图像质量分布的变化显著影响性能；2）微调集中必须包含足够的高质量图像以保证性能；3）一致的质量分布可提升标签效率。

Conclusion: 研究强调了评估微调和实际部署中数据质量分布的重要性，并提出针对特定任务的质量标准，以充分发挥基础模型的数据与计算效率优势。

Abstract: Foundation models in medical imaging have shown promising label efficiency,
achieving high downstream performance with only a fraction of annotated data.
Here, we evaluate this in prostate multiparametric MRI using ProFound, a
domain-specific vision foundation model pretrained on large-scale prostate MRI
datasets. We investigate how variable image quality affects label-efficient
finetuning by measuring the generalisability of finetuned models. Experiments
systematically vary high-/low-quality image ratios in finetuning and evaluation
sets. Our findings indicate that image quality distribution and its
finetune-and-test mismatch significantly affect model performance. In
particular: a) Varying the ratio of high- to low-quality images between
finetuning and test sets leads to notable differences in downstream
performance; and b) The presence of sufficient high-quality images in the
finetuning set is critical for maintaining strong performance, whilst the
importance of matched finetuning and testing distribution varies between
different downstream tasks, such as automated radiology reporting and prostate
cancer detection.When quality ratios are consistent, finetuning needs far less
labeled data than training from scratch, but label efficiency depends on image
quality distribution. Without enough high-quality finetuning data, pretrained
models may fail to outperform those trained without pretraining. This
highlights the importance of assessing and aligning quality distributions
between finetuning and deployment, and the need for quality standards in
finetuning data for specific downstream tasks. Using ProFound, we show the
value of quantifying image quality in both finetuning and deployment to fully
realise the data and compute efficiency benefits of foundation models.

</details>


### [14] [AdaRing: Towards Ultra-Light Vision-Language Adaptation via Cross-Layer Tensor Ring Decomposition](https://arxiv.org/abs/2508.11870)
*Ying Huang,Yuanbin Man,Wenqi Jia,Zhengzhong Tu,Junzhou Huang,Miao Yin*

Main category: cs.CV

TL;DR: 本研究提出了AdaRing，一个基于跨层张量环分解(TRD)的视觉语言模型(VLM)适配框架，大幅减少了训练参数，同时提升了适配性能。


<details>
  <summary>Details</summary>
Motivation: 传统插入与调整适配器的方法在提高适配器容量时，面临跨层冗余以及同质适配器表示能力受限的问题，亟需一个更高效的解决方案。

Method: 使用跨层张量环分解方法，将适配器表示为共享的张量核和特定于层的切片，实现适配器间的协作，以应对需要多样表示的任务。

Result: AdaRing大幅减少了90%的训练参数，同时在多个任务上实现了最先进的表现。

Conclusion: AdaRing提供了一种性能优越且参数高效的VLM适配方法，证明了跨层协作和低冗余设计的有效性。

Abstract: Adapter-based fine-tuning has gained remarkable attention in adapting large
pre-trained vision language models (VLMs) for a wide range of downstream tasks
efficiently. In this paradigm, only the inserted adapters are fine-tuned,
without the need for training the original VLM backbone. Existing works scale
adapters by integrating them into every layer of VLMs to increase the capacity
of adapters. However, these methods face two primary limitations: 1) limited
compression rate due to ignoring cross-layer redundancy, and 2) limited
representational capacity across homogeneous adapters. In this paper, we
propose a novel vision-language fine-tuning framework based on cross-layer
tensor ring decomposition (TRD) with the integration and collaboration of
diverse adapters, called AdaRing, achieving ultra-light parameter-efficient
adaptation of VLMs on various tasks. To remove the high redundancy that exists
among adapters across layers, we exploit the tensor-level low-rankness to
formulate adapters as layer-shared tensor cores and layer-specific slices.
Moreover, guided by generalization-aware fine-tuning, diverse rank-driven
adapters cooperate to handle tasks that require different representations. Our
experiments show that the proposed AdaRing achieves the state-of-the-art
performance while reducing average training parameters by 90%.

</details>


### [15] [EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models](https://arxiv.org/abs/2508.11886)
*Wenhui Zhu,Xiwen Chen,Zhipeng Wang,Shao Tang,Sayan Ghosh,Xuanzhao Dong,Rajat Koner,Yalin Wang*

Main category: cs.CV

TL;DR: 提出了一种可加速推理的新方法EVTP-IV，用于视觉标记修剪，同时在图像和视频分割任务中保持较高精度。


<details>
  <summary>Details</summary>
Motivation: 解决视频中推理成本较高的问题，设计一种有效的视觉标记修剪方法，兼顾性能与效率。

Method: 引入EVTP-IV，通过结合k-center与空间信息，提高标记覆盖率并优化推理效率，同时支持信息论分析。

Result: 在标准IVS基准测试中，图像任务加速3.5倍，视频任务加速5倍，仅用20%标记即可达到与现有方法相当的精度，并在不同修剪比率下优于最新基线方法。

Conclusion: EVTP-IV方法成功在保持精度前提下显著提升了推理速度，为IVS任务带来新的高效方案。

Abstract: Instructed Visual Segmentation (IVS) tasks require segmenting objects in
images or videos based on natural language instructions. While recent
multimodal large language models (MLLMs) have achieved strong performance on
IVS, their inference cost remains a major bottleneck, particularly in video. We
empirically analyze visual token sampling in MLLMs and observe a strong
correlation between subset token coverage and segmentation performance. This
motivates our design of a simple and effective token pruning method that
selects a compact yet spatially representative subset of tokens to accelerate
inference. In this paper, we introduce a novel visual token pruning method for
IVS, called EVTP-IV, which builds upon the k-center by integrating spatial
information to ensure better coverage. We further provide an
information-theoretic analysis to support our design. Experiments on standard
IVS benchmarks show that our method achieves up to 5X speed-up on video tasks
and 3.5X on image tasks, while maintaining comparable accuracy using only 20%
of the tokens. Our method also consistently outperforms state-of-the-art
pruning baselines under varying pruning ratios.

</details>


### [16] [Large Kernel Modulation Network for Efficient Image Super-Resolution](https://arxiv.org/abs/2508.11893)
*Quanwei Hu,Yinggan Tang,Xuguang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Large Kernel Modulation Network (LKMN)的纯CNN模型，用于解决资源受限场景下图像超分辨率的性能与低延迟权衡问题。


<details>
  <summary>Details</summary>
Motivation: 在资源受限场景中，现有CNN无法捕捉非局部特征，而Transformer尽管可以建模非局部信息，但推理速度缓慢，故需要一种既快速又能非局部建模的解决方案。

Method: 本文的LKMN模型包含Enhanced Partial Large Kernel Block (EPLKB)和Cross-Gate Feed-Forward Network (CGFN)两部分。EPLKB通过通道混洗与注意力机制提升特效提取，利用大核条带卷积提取非局部特征；CGFN则动态调整局部与非局部特征差异，并通过交叉门控策略融合这些特征。

Result: 实验表明，LKMN性能优于多种最先进轻量级SR模型，如在Manga109数据集中LKMN-L相比DAT-light在4倍放大的图像中PSNR提升0.23 dB，且推理速度提升4.8倍。

Conclusion: LKMN模型在提升图像超分辨率性能和效率上取得了显著进展，是资源受限场景的有力解决方案。

Abstract: Image super-resolution (SR) in resource-constrained scenarios demands
lightweight models balancing performance and latency. Convolutional neural
networks (CNNs) offer low latency but lack non-local feature capture, while
Transformers excel at non-local modeling yet suffer slow inference. To address
this trade-off, we propose the Large Kernel Modulation Network (LKMN), a pure
CNN-based model. LKMN has two core components: Enhanced Partial Large Kernel
Block (EPLKB) and Cross-Gate Feed-Forward Network (CGFN). The EPLKB utilizes
channel shuffle to boost inter-channel interaction, incorporates channel
attention to focus on key information, and applies large kernel strip
convolutions on partial channels for non-local feature extraction with reduced
complexity. The CGFN dynamically adjusts discrepancies between input, local,
and non-local features via a learnable scaling factor, then employs a
cross-gate strategy to modulate and fuse these features, enhancing their
complementarity. Extensive experiments demonstrate that our method outperforms
existing state-of-the-art (SOTA) lightweight SR models while balancing quality
and efficiency. Specifically, LKMN-L achieves 0.23 dB PSNR improvement over
DAT-light on the Manga109 dataset at $\times$4 upscale, with nearly $\times$4.8
times faster. Codes are in the supplementary materials. The code is available
at https://github.com/Supereeeee/LKMN.

</details>


### [17] [A Sobel-Gradient MLP Baseline for Handwritten Character Recognition](https://arxiv.org/abs/2508.11902)
*Azam Nouri*

Main category: cs.CV

TL;DR: 研究使用Sobel算子的一阶边缘映射作为输入，与MLP模型搭配进行手写字符识别，得到接近于CNN的高准确率。


<details>
  <summary>Details</summary>
Motivation: 探索能否使用一阶边缘映射驱动全连接MLP替代CNN进行手写字符识别。

Method: 通过水平和垂直Sobel导数获取输入，并用这些输入训练MLP模型，进行MNIST和EMNIST数据集实验。

Result: 在MNIST数据集上达到98%的准确率，在EMNIST Letters上达到92%的准确率，接近CNN的表现。

Conclusion: 一阶梯度已经能够捕获手写字符图像中的大部分可区分信息，结合边缘感知的MLP是一种具有吸引力的手写字符识别方案。

Abstract: We revisit the classical Sobel operator to ask a simple question: Are
first-order edge maps sufficient to drive an all-dense multilayer perceptron
(MLP) for handwritten character recognition (HCR), as an alternative to
convolutional neural networks (CNNs)? Using only horizontal and vertical Sobel
derivatives as input, we train an MLP on MNIST and EMNIST Letters. Despite its
extreme simplicity, the resulting network reaches 98% accuracy on MNIST digits
and 92% on EMNIST letters -- approaching CNNs while offering a smaller memory
footprint and transparent features. Our findings highlight that much of the
class-discriminative information in handwritten character images is already
captured by first-order gradients, making edge-aware MLPs a compelling option
for HCR.

</details>


### [18] [OVG-HQ: Online Video Grounding with Hybrid-modal Queries](https://arxiv.org/abs/2508.11903)
*Runhao Zeng,Jiaqi Mao,Minghao Lai,Minh Hieu Phan,Yanjie Dong,Wei Wang,Qi Chen,Xiping Hu*

Main category: cs.CV

TL;DR: 提出了一个名为OVG-HQ的新任务，旨在通过多模态查询实现在线视频片段定位，提出了统一框架OVG-HQ-Unify和新指标，实验表明框架优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统视频定位任务在处理实时视频或使用视觉线索的查询场景时表现较差，现有数据集无法满足多模态查询的需求，且缺少适用于在线环境的评估标准。

Method: 设计了OVG-HQ-Unify框架，该框架包含参数记忆模块（PMB）以增强决策能力，以及跨模态蒸馏策略以平衡各类模态的训练影响。此外，扩展了数据集并设计了在线评估指标。

Result: 所提出的OVG-HQ-Unify框架在各项指标上优于现有模型，能够在在线环境中高效处理混合模态查询。

Conclusion: OVG-HQ-Unify框架提供了一种创新和可靠的解决方案，能够应对在线混合模态视频定位中的新挑战，并具有较强的实用价值。

Abstract: Video grounding (VG) task focuses on locating specific moments in a video
based on a query, usually in text form. However, traditional VG struggles with
some scenarios like streaming video or queries using visual cues. To fill this
gap, we present a new task named Online Video Grounding with Hybrid-modal
Queries (OVG-HQ), which enables online segment localization using text, images,
video segments, and their combinations. This task poses two new challenges:
limited context in online settings and modality imbalance during training,
where dominant modalities overshadow weaker ones. To address these, we propose
OVG-HQ-Unify, a unified framework featuring a Parametric Memory Block (PMB)
that retain previously learned knowledge to enhance current decision and a
cross-modal distillation strategy that guides the learning of non-dominant
modalities. This design enables a single model to effectively handle
hybrid-modal queries. Due to the lack of suitable datasets, we construct
QVHighlights-Unify, an expanded dataset with multi-modal queries. Besides,
since offline metrics overlook prediction timeliness, we adapt them to the
online setting, introducing oR@n, IoU=m, and online mean Average Precision
(omAP) to evaluate both accuracy and efficiency. Experiments show that our
OVG-HQ-Unify outperforms existing models, offering a robust solution for
online, hybrid-modal video grounding. Source code and datasets are available at
https://github.com/maojiaqi2324/OVG-HQ.

</details>


### [19] [SafeCtrl: Region-Based Safety Control for Text-to-Image Diffusion via Detect-Then-Suppress](https://arxiv.org/abs/2508.11904)
*Lingyun Zhang,Yu Xie,Yanwei Fu,Ping Chen*

Main category: cs.CV

TL;DR: 提出了SafeCtrl，一种用于文本生成图像模型的插件，能够在不牺牲生成质量的情况下抑制有害内容。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在安全性和生成质量之间的权衡问题，同时避免因“概念替换”引发的语义不一致。

Method: 采用“检测-抑制”范式，创新性地设计并训练了SafeCtrl插件，通过直接偏好优化(DPO)，利用图像级的偏好数据训练模块，实现精准的区域内容抑制，并避免了像素级标注的高耗成本。

Result: 实验表明，SafeCtrl在安全性和生成质量上显著优于当前最先进的方法。

Conclusion: 分离式的抑制控制是一种构建更负责任生成模型的高效且可扩展的方法。

Abstract: The widespread deployment of text-to-image models is challenged by their
potential to generate harmful content. While existing safety methods, such as
prompt rewriting or model fine-tuning, provide valuable interventions, they
often introduce a trade-off between safety and fidelity. Recent
localization-based approaches have shown promise, yet their reliance on
explicit ``concept replacement" can sometimes lead to semantic incongruity. To
address these limitations, we explore a more flexible detect-then-suppress
paradigm. We introduce SafeCtrl, a lightweight, non-intrusive plugin that first
precisely localizes unsafe content. Instead of performing a hard A-to-B
substitution, SafeCtrl then suppresses the harmful semantics, allowing the
generative process to naturally and coherently resolve into a safe,
context-aware alternative. A key aspect of our work is a novel training
strategy using Direct Preference Optimization (DPO). We leverage readily
available, image-level preference data to train our module, enabling it to
learn nuanced suppression behaviors and perform region-guided interventions at
inference without requiring costly, pixel-level annotations. Extensive
experiments show that SafeCtrl significantly outperforms state-of-the-art
methods in both safety efficacy and fidelity preservation. Our findings suggest
that decoupled, suppression-based control is a highly effective and scalable
direction for building more responsible generative models.

</details>


### [20] [TimeSenCLIP: A Vision-Language Model for Remote Sensing Using Single-Pixel Time Series](https://arxiv.org/abs/2508.11919)
*Pallavi Jain,Diego Marcos,Dino Ienco,Roberto Interdonato,Tristan Berchoux*

Main category: cs.CV

TL;DR: TimeSenCLIP结合Sentinel-2影像的光谱和时间信息，通过单像素输入实现大规模遥感应用的高效分类和检索。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型依赖大空间瓦片和文本监督，成本高且不易获得合适监督。提出一种高效解决方案。

Method: 通过利用Sentinel-2的光谱和时间信息，与带有地理标记的地面照片进行跨视角学习，减少对基于文本标注训练的依赖，同时对单像素数据进行分类评估。

Result: 使用LUCAS和Sen4Map数据集验证，展示单像素输入结合时间和光谱线索可用于地物分类、作物类型和生态系统类型等任务。

Conclusion: 通过单像素输入和时间/光谱数据的结合，实现了资源高效的大规模遥感应用解决方案，同时不损失语义对齐能力。

Abstract: Vision-language models have shown significant promise in remote sensing
applications, particularly for land-use and land-cover (LULC) via zero-shot
classification and retrieval. However, current approaches face two key
challenges: reliance on large spatial tiles that increase computational cost,
and dependence on text-based supervision, which is often not readily available.
In this work, we present TimeSenCLIP, a lightweight framework that reevaluate
the role of spatial context by evaluating the effectiveness of a single pixel
by leveraging its temporal and spectral dimensions, for classifying LULC and
ecosystem types. By leveraging spectral and temporal information from
Sentinel-2 imagery and cross-view learning with geo-tagged ground-level photos,
we minimises the need for caption-based training while preserving semantic
alignment between overhead (satellite) and ground perspectives. Our approach is
grounded in the LUCAS and Sen4Map datasets, and evaluated on classification
tasks including LULC, crop type, and ecosystem type. We demonstrate that single
pixel inputs, when combined with temporal and spectral cues, are sufficient for
thematic mapping, offering a scalable and efficient alternative for large-scale
remote sensing applications. Code is available at
https://github.com/pallavijain-pj/TimeSenCLIP

</details>


### [21] [Assessment of Using Synthetic Data in Brain Tumor Segmentation](https://arxiv.org/abs/2508.11922)
*Aditi Jahagirdar,Sameer Joshi*

Main category: cs.CV

TL;DR: 利用生成对抗网络生成的合成数据来辅助U-Net进行MRI脑肿瘤分割，增强了数据多样性并改进了一些边界分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决脑肿瘤分割中由于肿瘤异质性、标注数据稀缺和数据集类别不平衡导致的准确性挑战。

Method: 利用预训练的GAN模型生成合成MRI数据，并与真实数据结合以不同比例训练U-Net分割网络，同时对比分析不同训练集的分割性能。

Result: 采用40%真实数据和60%合成数据的混合训练集在整体分割性能上与仅用真实数据一致，但在肿瘤整体边界分割上有所改善，而核心区域及增强肿瘤的精确性依然较低。

Conclusion: 合成数据能作为脑肿瘤分割的有效增广策略，但未来需开展更多大规模实验，增强数据一致性，并解决类别不平衡问题。

Abstract: Manual brain tumor segmentation from MRI scans is challenging due to tumor
heterogeneity, scarcity of annotated data, and class imbalance in medical
imaging datasets. Synthetic data generated by generative models has the
potential to mitigate these issues by improving dataset diversity. This study
investigates, as a proof of concept, the impact of incorporating synthetic MRI
data, generated using a pre-trained GAN model, into training a U-Net
segmentation network. Experiments were conducted using real data from the BraTS
2020 dataset, synthetic data generated with the medigan library, and hybrid
datasets combining real and synthetic samples in varying proportions. While
overall quantitative performance (Dice coefficient, IoU, precision, recall,
accuracy) was comparable between real-only and hybrid-trained models,
qualitative inspection suggested that hybrid datasets, particularly with 40%
real and 60% synthetic data, improved whole tumor boundary delineation.
However, region-wise accuracy for the tumor core and the enhancing tumor
remained lower, indicating a persistent class imbalance. The findings support
the feasibility of synthetic data as an augmentation strategy for brain tumor
segmentation, while highlighting the need for larger-scale experiments,
volumetric data consistency, and mitigating class imbalance in future work.

</details>


### [22] [Deep Learning For Point Cloud Denoising: A Survey](https://arxiv.org/abs/2508.11932)
*Chengwei Zhang,Xueyi Zhang,Mingrui Lao,Tao Jiang,Xinhao Xu,Wenjie Li,Fubo Zhang,Longyong Chen*

Main category: cs.CV

TL;DR: 该论文总结了基于深度学习的点云去噪方法，提出了一种针对去噪任务的分类方式，并讨论了当前的研究局限性及未来方向。


<details>
  <summary>Details</summary>
Motivation: 填补缺乏系统性综述基于深度学习的点云去噪发展现状的研究空白。

Method: 将点云去噪分为异常点移除和表面噪声修复两步流程，同时提出一种新的分类方式并比较现有方法的优缺点。

Result: 明确了基于深度学习点云去噪领域的核心挑战，总结了现有方法的主要贡献，并为未来研究方向提供了指导。

Conclusion: 基于深度学习的点云去噪相较传统方法表现优越，但仍有研究局限需要克服，分类及综述为未来提供了开发和优化的依据。

Abstract: Real-world environment-derived point clouds invariably exhibit noise across
varying modalities and intensities. Hence, point cloud denoising (PCD) is
essential as a preprocessing step to improve downstream task performance. Deep
learning (DL)-based PCD models, known for their strong representation
capabilities and flexible architectures, have surpassed traditional methods in
denoising performance. To our best knowledge, despite recent advances in
performance, no comprehensive survey systematically summarizes the developments
of DL-based PCD. To fill the gap, this paper seeks to identify key challenges
in DL-based PCD, summarizes the main contributions of existing methods, and
proposes a taxonomy tailored to denoising tasks. To achieve this goal, we
formulate PCD as a two-step process: outlier removal and surface noise
restoration, encompassing most scenarios and requirements of PCD. Additionally,
we compare methods in terms of similarities, differences, and respective
advantages. Finally, we discuss research limitations and future directions,
offering insights for further advancements in PCD.

</details>


### [23] [DynamicPose: Real-time and Robust 6D Object Pose Tracking for Fast-Moving Cameras and Objects](https://arxiv.org/abs/2508.11950)
*Tingbang Liang,Yixin Zeng,Jiatong Xie,Boyu Zhou*

Main category: cs.CV

TL;DR: DynamicPose是一个无需重新训练的6D姿态跟踪框架，针对快速移动的相机和物体场景显著提高了跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在快速移动的相机和物体场景中跟踪性能显著下降的问题。

Method: 提出三种协同组件：(1)视觉惯性里程计补偿相机运动引起的兴趣区域偏移；(2)深度感知的2D跟踪器修正大物体位移导致的兴趣区域偏差；(3)借助VIO指导的卡尔曼滤波预测物体旋转，生成候选姿态并层次化精化获得最终姿态。

Result: 通过模拟和实际实验证明该方法的高效性，可实时并稳健地进行快速移动场景下的6D姿态跟踪。

Conclusion: DynamicPose形成了一个闭环系统，确保了准确的姿态初始化和精确的姿态跟踪，在快速运动场景下具有较强的鲁棒性。

Abstract: We present DynamicPose, a retraining-free 6D pose tracking framework that
improves tracking robustness in fast-moving camera and object scenarios.
Previous work is mainly applicable to static or quasi-static scenes, and its
performance significantly deteriorates when both the object and the camera move
rapidly. To overcome these challenges, we propose three synergistic components:
(1) A visual-inertial odometry compensates for the shift in the Region of
Interest (ROI) caused by camera motion; (2) A depth-informed 2D tracker
corrects ROI deviations caused by large object translation; (3) A VIO-guided
Kalman filter predicts object rotation, generates multiple candidate poses, and
then obtains the final pose by hierarchical refinement. The 6D pose tracking
results guide subsequent 2D tracking and Kalman filter updates, forming a
closed-loop system that ensures accurate pose initialization and precise pose
tracking. Simulation and real-world experiments demonstrate the effectiveness
of our method, achieving real-time and robust 6D pose tracking for fast-moving
cameras and objects.

</details>


### [24] [Transferable Class Statistics and Multi-scale Feature Approximation for 3D Object Detection](https://arxiv.org/abs/2508.11951)
*Hao Peng,Hong Sang,Yajing Ma,Ping Qiu,Chao Ji*

Main category: cs.CV

TL;DR: 本文提出了一种基于知识蒸馏的单邻域方法来逼近点云目标检测中的多尺度特征，同时设计了可迁移特征嵌入机制，以及一种新的定位优化方法。


<details>
  <summary>Details</summary>
Motivation: 多尺度特征对点云物体检测至关重要，但现有方法往往需多次邻域搜索及专门模块，不利于轻量模型设计，尤其在计算资源有限的情况下实施困难。

Method: 通过基于知识蒸馏的方法，从单一的邻域近似多尺度特征，同时引入基于类别统计的低计算代价可迁移特征嵌入机制，提出中心加权交并比来优化定位误差。

Result: 在公共数据集上进行的大量实验验证了所提出方法的有效性。

Conclusion: 该方法不仅有效，而且大幅降低了计算成本，适用于资源受限环境下的点云物体检测任务。

Abstract: This paper investigates multi-scale feature approximation and transferable
features for object detection from point clouds. Multi-scale features are
critical for object detection from point clouds. However, multi-scale feature
learning usually involves multiple neighborhood searches and scale-aware
layers, which can hinder efforts to achieve lightweight models and may not be
conducive to research constrained by limited computational resources. This
paper approximates point-based multi-scale features from a single neighborhood
based on knowledge distillation. To compensate for the loss of constructive
diversity in a single neighborhood, this paper designs a transferable feature
embedding mechanism. Specifically, class-aware statistics are employed as
transferable features given the small computational cost. In addition, this
paper introduces the central weighted intersection over union for localization
to alleviate the misalignment brought by the center offset in optimization.
Note that the method presented in this paper saves computational costs.
Extensive experiments on public datasets demonstrate the effectiveness of the
proposed method.

</details>


### [25] [UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic Encoding](https://arxiv.org/abs/2508.11952)
*Yueming Xu,Jiahui Zhang,Ze Huang,Yurui Chen,Yanpeng Zhou,Zhenyu Chen,Yu-Jie Yuan,Pengxiang Xia,Guowei Huang,Xinyue Cai,Zhongang Qi,Xingyue Quan,Jianye Hao,Hang Xu,Li Zhang*

Main category: cs.CV

TL;DR: 提出了名为UniUGG的统一框架，用于3D任务的理解和生成，利用LLM和空间解码器生成高质量3D表示并支持空间VQA任务，同时提出了几何-语义学习策略以增强视觉编码器。


<details>
  <summary>Details</summary>
Motivation: 现有的统一架构在图像理解与生成方面取得了显著进展，但在3D任务上的集成仍然具有挑战性且研究较少。

Method: 使用LLM理解和解码语言与3D表示，并设计了基于潜在扩散模型的空间解码器生成高质量3D表示，同时提出几何-语义学习策略对视觉编码器进行预训练。

Result: 实验结果表明该方法在视觉表示、空间理解和3D生成方面优于现有方法。

Conclusion: UniUGG框架提升了3D任务的理解与生成能力，验证了其潜在的优越性，代码将在论文接受后公开。

Abstract: Despite the impressive progress on understanding and generating images shown
by the recent unified architectures, the integration of 3D tasks remains
challenging and largely unexplored. In this paper, we introduce UniUGG, the
first unified understanding and generation framework for 3D modalities. Our
unified framework employs an LLM to comprehend and decode sentences and 3D
representations. At its core, we propose a spatial decoder leveraging a latent
diffusion model to generate high-quality 3D representations. This allows for
the generation and imagination of 3D scenes based on a reference image and an
arbitrary view transformation, while remaining supports for spatial visual
question answering (VQA) tasks. Additionally, we propose a geometric-semantic
learning strategy to pretrain the vision encoder. This design jointly captures
the input's semantic and geometric cues, enhancing both spatial understanding
and generation. Extensive experimental results demonstrate the superiority of
our method in visual representation, spatial understanding, and 3D generation.
The source code will be released upon paper acceptance.

</details>


### [26] [SAMDWICH: Moment-aware Video-text Alignment for Referring Video Object Segmentation](https://arxiv.org/abs/2508.11955)
*Seunghun Lee,Jiwan Seo,Jeonghoon Kim,Siwon Kim,Haeun Yun,Hyogyeong Jeon,Wonhyeok Choi,Jaehoon Jeong,Zane Durante,Sang Hyun Park,Sunghoon Im*

Main category: cs.CV

TL;DR: 提出了一个新的语义对齐RVOS框架SAMDWICH，结合新的数据集MeViS-M，通过moment-aware技术改进视频和文本的对齐，提升对象分割和跟踪效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在RVOS任务中存在语义对齐不足的问题，原因是训练中无差别帧采样和对所有可见对象的统一监督。

Method: 提出了SAMDWICH框架，结合MeViS-M数据集，通过Moment-guided Dual-path Propagation (MDP)策略和Object-level Selective Supervision (OSS)技术，引入基于moment的记忆机制和对象级筛选监督，增强语义对齐。

Result: 在复杂表达场景下，SAMDWICH在MeViS基准数据集上达到了现有方法的最优性能表现。

Conclusion: SAMDWICH框架通过moment-aware策略提升了视频和文本的对齐能力，并验证了其在复杂场景下的有效性。

Abstract: Referring Video Object Segmentation (RVOS) aims to segment and track objects
in videos based on natural language expressions, requiring precise alignment
between visual content and textual queries. However, existing methods often
suffer from semantic misalignment, largely due to indiscriminate frame sampling
and supervision of all visible objects during training -- regardless of their
actual relevance to the expression. To address this, we introduce a
moment-aware RVOS framework named SAMDWICH, along with a newly annotated
dataset, MeViS-M, built upon the challenging MeViS benchmark. We manually
annotate temporal moments indicating when each object is referred to by the
expression, enabling semantically grounded supervision that strengthens
video-text alignment. SAMDWICH leverages these aligned text-to-clip pairs to
guide training, significantly enhancing referential understanding. Building
upon this framework, we propose Moment-guided Dual-path Propagation (MDP), a
moment-aware propagation strategy that improves both object grounding and
tracking by training on both relevant and irrelevant frames through a
moment-centric memory mechanism. In addition, we introduce Object-level
Selective Supervision (OSS), an object-level filtering strategy that supervises
only the objects temporally aligned with the expression in each training clip.
This selective supervision reduces semantic noise and reinforces
language-conditioned learning. Extensive experiments show that SAMDWICH
achieves state-of-the-art performance on challenging MeViS benchmark,
particularly excelling in complex scenarios involving diverse expressions.

</details>


### [27] [PEdger++: Practical Edge Detection via Assembling Cross Information](https://arxiv.org/abs/2508.11961)
*Yuanbin Fu,Liang Li,Xiaojie Guo*

Main category: cs.CV

TL;DR: 本文提出了一种名为PEdger++的协作学习框架，在边缘检测中实现高准确性与低计算复杂性之间的平衡。其核心在于利用多种异构架构与多样训练的跨信息结合，从而提高学习效果。


<details>
  <summary>Details</summary>
Motivation: 针对边缘检测任务，本文旨在解决高计算复杂性的深度学习模型在资源有限设备上的适用性问题，研究降低计算成本同时保证模型精度的策略。

Method: 提出了PEdger++框架，通过结合异构架构、不同训练时刻的特征及多种参数采样，以集成学习方式提高边缘检测效率和效果。

Result: 实验结果表明，PEdger++在BSDS500、NYUD及Multicue数据集上，定量和定性分析均优于现有方法，并展示了模型在不同资源限制下的适应性。

Conclusion: PEdger++成功实现了高效边缘检测模型的设计，平衡了模型精度和计算成本，为资源受限设备上的部署提供了支持。

Abstract: Edge detection serves as a critical foundation for numerous computer vision
applications, including object detection, semantic segmentation, and image
editing, by extracting essential structural cues that define object boundaries
and salient edges. To be viable for broad deployment across devices with
varying computational capacities, edge detectors shall balance high accuracy
with low computational complexity. While deep learning has evidently improved
accuracy, they often suffer from high computational costs, limiting their
applicability on resource-constrained devices. This paper addresses the
challenge of achieving that balance: \textit{i.e.}, {how to efficiently capture
discriminative features without relying on large-size and sophisticated
models}. We propose PEdger++, a collaborative learning framework designed to
reduce computational costs and model sizes while improving edge detection
accuracy. The core principle of our PEdger++ is that cross-information derived
from heterogeneous architectures, diverse training moments, and multiple
parameter samplings, is beneficial to enhance learning from an ensemble
perspective. Extensive experimental results on the BSDS500, NYUD and Multicue
datasets demonstrate the effectiveness of our approach, both quantitatively and
qualitatively, showing clear improvements over existing methods. We also
provide multiple versions of the model with varying computational requirements,
highlighting PEdger++'s adaptability with respect to different resource
constraints. Codes are accessible at
https://github.com/ForawardStar/EdgeDetectionviaPEdgerPlus/.

</details>


### [28] [Exploring Spatial-Temporal Dynamics in Event-based Facial Micro-Expression Analysis](https://arxiv.org/abs/2508.11988)
*Nicolas Mastropasqua,Ignacio Bugueno-Cordova,Rodrigo Verschae,Daniel Acevedo,Pablo Negri,Maria E. Buemi*

Main category: cs.CV

TL;DR: 该研究开发了一个新型的同步RGB和事件相机的多分辨率多模态微表情数据集，并使用尖峰神经网络和条件变分自编码器对动作单元分类及帧重建进行了测试和验证。


<details>
  <summary>Details</summary>
Motivation: 现有基于RGB摄像头的微表情分析方法由于时间分辨率限制和运动模糊问题，难以捕捉微表情中的细微快速变化；事件摄像头代表了一个新兴的高时间分辨率和低延迟的替代方案，但缺乏相关公开数据集。

Method: 引入一个同步RGB和事件相机录制的微表情数据集，并在数据的动作单元分类任务中使用尖峰神经网络对比事件数据与RGB数据的效果，同时使用条件变分自编码器对帧重建质量进行验证。

Result: 尖峰神经网络在使用事件数据进行动作单元分类时达到了51.23%的准确率，相比使用RGB数据的23.12%大大提升；在帧重建方面，条件变分自编码器的SSIM达到了0.8513，PSNR为26.89 dB，表现出色。

Conclusion: 基于事件数据的方式在微表情识别和帧重建任务中表现出更优越的能力，为人机交互和驾驶员监控领域提供了新的可能性和研究方向。

Abstract: Micro-expression analysis has applications in domains such as Human-Robot
Interaction and Driver Monitoring Systems. Accurately capturing subtle and fast
facial movements remains difficult when relying solely on RGB cameras, due to
limitations in temporal resolution and sensitivity to motion blur. Event
cameras offer an alternative, with microsecond-level precision, high dynamic
range, and low latency. However, public datasets featuring event-based
recordings of Action Units are still scarce. In this work, we introduce a
novel, preliminary multi-resolution and multi-modal micro-expression dataset
recorded with synchronized RGB and event cameras under variable lighting
conditions. Two baseline tasks are evaluated to explore the spatial-temporal
dynamics of micro-expressions: Action Unit classification using Spiking Neural
Networks (51.23\% accuracy with events vs. 23.12\% with RGB), and frame
reconstruction using Conditional Variational Autoencoders, achieving SSIM =
0.8513 and PSNR = 26.89 dB with high-resolution event input. These promising
results show that event-based data can be used for micro-expression recognition
and frame reconstruction.

</details>


### [29] [MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding](https://arxiv.org/abs/2508.11999)
*Daoze Zhang,Zhanheng Nie,Jianyu Liu,Chenghan Fu,Wanxian Guan,Yuan Gao,Jun Song,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: 本论文提出了一种基于生成式多模态大语言模型（MLLM）的产品表示学习模型MOON，用于解决现有方法在多图像和文本对齐上的不足。通过引入多专家机制模块、核心语义区域检测和负样本提升策略，并发布一个新的大规模多模态基准MBE，取得了卓越的零样本任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有的判别式双流架构无法有效解决多图像与文本之间的多对一对齐问题。因此，研究生成式MLLM模型的潜在能力以改进产品表示学习是必要的。

Method: 本文提出MOON模型，包括三大核心模块：1）引入导向的专家混合模块（MoE）以针对性建模产品多模态和特定方面内容；2）通过检测产品图像中的核心语义区域减少背景噪音的干扰；3）设计专门的负样本抽样策略提高负样本的难度与多样性。同时发布了一个新多模态基准MBE，用于评估模型。

Result: 实验表明，MOON在新基准MBE和公开数据集上表现良好，实现了强大的零样本能力，并在跨模态检索、产品分类和属性预测等任务中展现出很好的泛化性。

Conclusion: MOON模型证明了生成式MLLM在产品理解中的优越性，为产品表示学习和多模态任务提供了一种高效、通用的解决方案。

Abstract: With the rapid advancement of e-commerce, exploring general representations
rather than task-specific ones has attracted increasing research attention. For
product understanding, although existing discriminative dual-flow architectures
drive progress in this field, they inherently struggle to model the many-to-one
alignment between multiple images and texts of products. Therefore, we argue
that generative Multimodal Large Language Models (MLLMs) hold significant
potential for improving product representation learning. Nevertheless,
achieving this goal still remains non-trivial due to several key challenges:
the lack of multimodal and aspect-aware modeling modules in typical LLMs; the
common presence of background noise in product images; and the absence of a
standard benchmark for evaluation. To address these issues, we propose the
first generative MLLM-based model named MOON for product representation
learning. Our method (1) employs a guided Mixture-of-Experts (MoE) module for
targeted modeling of multimodal and aspect-specific product content; (2)
effectively detects core semantic regions in product images to mitigate the
distraction and interference caused by background noise; and (3) introduces the
specialized negative sampling strategy to increase the difficulty and diversity
of negative samples. In addition, we release a large-scale multimodal benchmark
MBE for various product understanding tasks. Experimentally, our model
demonstrates competitive zero-shot performance on both our benchmark and the
public dataset, showcasing strong generalization across various downstream
tasks, including cross-modal retrieval, product classification, and attribute
prediction. Furthermore, the case study and visualization illustrate the
effectiveness of MOON for product understanding.

</details>


### [30] [InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes](https://arxiv.org/abs/2508.12015)
*Hongyuan Liu,Haochen Yu,Jianfei Jiang,Qiankun Liu,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: InstDrive提出了一个针对动态驾驶场景重建的3D实例感知框架，实现了开阔场景下的3D实例分割，无需复杂预处理和实现流程。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前方法统一背景元素、缺乏实例级理解、有局限性等问题，且现有技术多针对室内场景，无法良好适应户外场景重建的需求。

Method: 提出了InstDrive框架，利用SAM生成的掩码作为伪真值，通过对比损失和伪监督优化2D特征学习，在3D层面加入正则约束进行实例编码，并通过轻量级代码本建立连续特征与离散身份的链接。

Result: 实验展示了InstDrive在定量和定性方面的有效性，首次实现动态外景驾驶场景中的3D实例分割。

Conclusion: InstDrive在动态驾驶场景的3D重建中展现出卓越性能，开辟了用于开放世界动态场景的实例感知新途径。

Abstract: Reconstructing dynamic driving scenes from dashcam videos has attracted
increasing attention due to its significance in autonomous driving and scene
understanding. While recent advances have made impressive progress, most
methods still unify all background elements into a single representation,
hindering both instance-level understanding and flexible scene editing. Some
approaches attempt to lift 2D segmentation into 3D space, but often rely on
pre-processed instance IDs or complex pipelines to map continuous features to
discrete identities. Moreover, these methods are typically designed for indoor
scenes with rich viewpoints, making them less applicable to outdoor driving
scenarios. In this paper, we present InstDrive, an instance-aware 3D Gaussian
Splatting framework tailored for the interactive reconstruction of dynamic
driving scene. We use masks generated by SAM as pseudo ground-truth to guide 2D
feature learning via contrastive loss and pseudo-supervised objectives. At the
3D level, we introduce regularization to implicitly encode instance identities
and enforce consistency through a voxel-based loss. A lightweight static
codebook further bridges continuous features and discrete identities without
requiring data pre-processing or complex optimization. Quantitative and
qualitative experiments demonstrate the effectiveness of InstDrive, and to the
best of our knowledge, it is the first framework to achieve 3D instance
segmentation in dynamic, open-world driving scenes.More visualizations are
available at our project page.

</details>


### [31] [WiseLVAM: A Novel Framework For Left Ventricle Automatic Measurements](https://arxiv.org/abs/2508.12023)
*Durgesh Kumar Singh,Qing Cao,Sarina Thomas,Ahcène Boubekki,Robert Jenssen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: 提出了WiseLVAM框架，用于自动化左心室线性测量，结合B模式图像的结构意识和AMM模式的运动意识，提升了鲁棒性与准确性。


<details>
  <summary>Details</summary>
Motivation: 解决当前自动化方法中，由于预测点位置微小偏移导致较大测量误差的问题，提升临床可靠性。

Method: 利用弱监督的B模式检测器估计左心室轮廓，推断长轴和基底水平以放置扫描线；在此基础上，提出WiseLVAM框架实现全自动化扫描线放置和AMM模式下的左心室线性测量。

Result: 增强了自动测量的鲁棒性和准确性，证明其在临床中的潜在实用价值。

Conclusion: WiseLVAM提供了一种自动化且可手动调整的框架，有望解决左心室测量的临床应用需求。

Abstract: Clinical guidelines recommend performing left ventricular (LV) linear
measurements in B-mode echocardiographic images at the basal level -- typically
at the mitral valve leaflet tips -- and aligned perpendicular to the LV long
axis along a virtual scanline (SL). However, most automated methods estimate
landmarks directly from B-mode images for the measurement task, where even
small shifts in predicted points along the LV walls can lead to significant
measurement errors, reducing their clinical reliability. A recent
semi-automatic method, EnLVAM, addresses this limitation by constraining
landmark prediction to a clinician-defined SL and training on generated
Anatomical Motion Mode (AMM) images to predict LV landmarks along the same. To
enable full automation, a contour-aware SL placement approach is proposed in
this work, in which the LV contour is estimated using a weakly supervised
B-mode landmark detector. SL placement is then performed by inferring the LV
long axis and the basal level-mimicking clinical guidelines. Building on this
foundation, we introduce \textit{WiseLVAM} -- a novel, fully automated yet
manually adaptable framework for automatically placing the SL and then
automatically performing the LV linear measurements in the AMM mode.
\textit{WiseLVAM} utilizes the structure-awareness from B-mode images and the
motion-awareness from AMM mode to enhance robustness and accuracy with the
potential to provide a practical solution for the routine clinical application.

</details>


### [32] [Q-FSRU: Quantum-Augmented Frequency-Spectral Fusion for Medical Visual Question Answering](https://arxiv.org/abs/2508.12036)
*Rakesh Thakur,Yusra Tariq*

Main category: cs.CV

TL;DR: 本研究提出了Q-FSRU模型，通过频域表示和量子检索增强，为医疗视觉问答任务提供更高效的解答方法。在VQA-RAD数据集上的表现优于现有模型，尤其是在需要图文推理的复杂案例中。


<details>
  <summary>Details</summary>
Motivation: 解决需要图像和文本理解的复杂医疗问题，是医疗AI领域的一大挑战，需要更精确且可解释的模型。

Method: 提出结合频谱表示与融合的FSRU以及量子检索增强生成的Quantum RAG模型，利用快速傅里叶变换移除噪声，并通过量子相似性技术从外部检索相关医学事实。

Result: 在VQA-RAD数据集上的性能优于以往模型，在复杂图文推理任务中表现更为突出，同时提供了更强的解释性。

Conclusion: 此研究为开发更加智能、清晰且实用的医疗AI工具提供了一种有前景的路径。

Abstract: Solving tough clinical questions that require both image and text
understanding is still a major challenge in healthcare AI. In this work, we
propose Q-FSRU, a new model that combines Frequency Spectrum Representation and
Fusion (FSRU) with a method called Quantum Retrieval-Augmented Generation
(Quantum RAG) for medical Visual Question Answering (VQA). The model takes in
features from medical images and related text, then shifts them into the
frequency domain using Fast Fourier Transform (FFT). This helps it focus on
more meaningful data and filter out noise or less useful information. To
improve accuracy and ensure that answers are based on real knowledge, we add a
quantum-inspired retrieval system. It fetches useful medical facts from
external sources using quantum-based similarity techniques. These details are
then merged with the frequency-based features for stronger reasoning. We
evaluated our model using the VQA-RAD dataset, which includes real radiology
images and questions. The results showed that Q-FSRU outperforms earlier
models, especially on complex cases needing image-text reasoning. The mix of
frequency and quantum information improves both performance and explainability.
Overall, this approach offers a promising way to build smart, clear, and
helpful AI tools for doctors.

</details>


### [33] [VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models](https://arxiv.org/abs/2508.12081)
*Haidong Xu,Guangwei Xu,Zhedong Zheng,Xiatian Zhu,Wei Ji,Xiangtai Li,Ruijie Guo,Meishan Zhang,Min zhang,Hao Fei*

Main category: cs.CV

TL;DR: 提出了VimoRAG框架，将视频检索技术与动作生成模型结合，显著提高了动作生成的性能。


<details>
  <summary>Details</summary>
Motivation: 解决动作大型语言模型因标注数据不足而导致的在非领域/非词汇问题上的困难。

Method: 设计了Gemini Motion Video Retriever机制和Motion-centric Dual-alignment DPO Trainer，用以提升视频检索与生成的有效性。

Result: 实验证明，VimoRAG显著提高了仅基于文本输入的动作模型的表现。

Conclusion: 通过结合视频数据，VimoRAG解决了动作生成模型中的关键瓶颈，为动作生成领域提供了新的思路。

Abstract: This paper introduces VimoRAG, a novel video-based retrieval-augmented motion
generation framework for motion large language models (LLMs). As motion LLMs
face severe out-of-domain/out-of-vocabulary issues due to limited annotated
data, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D
motion generation by retrieving relevant 2D human motion signals. While
video-based motion RAG is nontrivial, we address two key bottlenecks: (1)
developing an effective motion-centered video retrieval model that
distinguishes human poses and actions, and (2) mitigating the issue of error
propagation caused by suboptimal retrieval results. We design the Gemini Motion
Video Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,
enabling effective retrieval and generation processes. Experimental results
show that VimoRAG significantly boosts the performance of motion LLMs
constrained to text-only input.

</details>


### [34] [Automated Model Evaluation for Object Detection via Prediction Consistency and Reliablity](https://arxiv.org/abs/2508.12082)
*Seungju Yoo,Hyuk Kwon,Joong-Won Hwang,Kibok Lee*

Main category: cs.CV

TL;DR: 提出了一个名为AutoEval的自动化模型评估框架，通过预测一致性和可靠性(PCR)来评估目标检测性能，无需依赖人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 目标检测的模型训练虽然日益高效，但其性能评估仍需昂贵的人工标注，该研究旨在解决这一问题。

Method: 提出了一种预测一致性和可靠性(PCR)方法，通过联合测量NMS处理前后框的空间一致性，以及保留框的可靠性（基于重叠框的置信分数），实现无需人工标注的性能估计。

Result: 实验结果表明，PCR比现有方法的性能估计更准确，且提出的元数据集涵盖了更广泛的检测性能范围。

Conclusion: 研究有效解决了目标检测性能评估的人工成本问题，增强了评估的现实性与可扩展性。

Abstract: Recent advances in computer vision have made training object detectors more
efficient and effective; however, assessing their performance in real-world
applications still relies on costly manual annotation. To address this
limitation, we develop an automated model evaluation (AutoEval) framework for
object detection. We propose Prediction Consistency and Reliability (PCR),
which leverages the multiple candidate bounding boxes that conventional
detectors generate before non-maximum suppression (NMS). PCR estimates
detection performance without ground-truth labels by jointly measuring 1) the
spatial consistency between boxes before and after NMS, and 2) the reliability
of the retained boxes via the confidence scores of overlapping boxes. For a
more realistic and scalable evaluation, we construct a meta-dataset by applying
image corruptions of varying severity. Experimental results demonstrate that
PCR yields more accurate performance estimates than existing AutoEval methods,
and the proposed meta-dataset covers a wider range of detection performance.
The code is available at https://github.com/YonseiML/autoeval-det.

</details>


### [35] [Generic Event Boundary Detection via Denoising Diffusion](https://arxiv.org/abs/2508.12084)
*Jaejun Hwang,Dayoung Gong,Manjin Kim,Minsu Cho*

Main category: cs.CV

TL;DR: 提出了一种基于扩散的事件边界检测方法DiffGEBD，可以生成多样化和可行的边界。


<details>
  <summary>Details</summary>
Motivation: 现有方法过于依赖确定性的预测，忽视了边界划分的多样性。

Method: 提出了DiffGEBD，通过时间自相似性编码帧间变化，并利用扩散模型生成自然事件边界。模型加入无分类指导以控制降噪中的多样性，并推出新评估指标。

Result: 在Kinetics-GEBD和TAPOS上表现优异，能够生成多样化且合理的事件边界。

Conclusion: 提出的方法有效结合了多样性和保真度，突破性地解决了GEBD任务中的多样性问题。

Abstract: Generic event boundary detection (GEBD) aims to identify natural boundaries
in a video, segmenting it into distinct and meaningful chunks. Despite the
inherent subjectivity of event boundaries, previous methods have focused on
deterministic predictions, overlooking the diversity of plausible solutions. In
this paper, we introduce a novel diffusion-based boundary detection model,
dubbed DiffGEBD, that tackles the problem of GEBD from a generative
perspective. The proposed model encodes relevant changes across adjacent frames
via temporal self-similarity and then iteratively decodes random noise into
plausible event boundaries being conditioned on the encoded features.
Classifier-free guidance allows the degree of diversity to be controlled in
denoising diffusion. In addition, we introduce a new evaluation metric to
assess the quality of predictions considering both diversity and fidelity.
Experiments show that our method achieves strong performance on two standard
benchmarks, Kinetics-GEBD and TAPOS, generating diverse and plausible event
boundaries.

</details>


### [36] [Enhancing 3D point accuracy of laser scanner through multi-stage convolutional neural network for applications in construction](https://arxiv.org/abs/2508.12089)
*Qinyuan Fan,Clemens Gühmann*

Main category: cs.CV

TL;DR: 本文提出一种基于多阶段卷积神经网络（MSCNN）的集成方法，用于减少3D激光扫描仪（LS）的位点精度不确定性，特别是应用于粗糙室内环境中。


<details>
  <summary>Details</summary>
Motivation: 由于激光扫描仪设备的局限性和环境因素的影响，高端与低端设备的定位误差不同。研究的动机是通过减少低端设备的误差，提高室内3D空间建模的准确性，降低设备性能差异带来的影响。

Method: 将高精度激光扫描仪（HAS）与低精度扫描仪（LAS）在相同环境中配对，量化特定误差模式。通过建立测量差异与其空间分布之间的统计关系，结合传统几何处理与专门的神经网络优化，提出一种误差校正框架。

Result: 实验证明，提出的方法可在粗糙室内数据集中将均方误差（MSE）降低超70%，并提升信噪比（PSNR）约6分贝。

Conclusion: 该方法使低端设备在无需硬件改造的情况下，达到接近高端设备的测量不确定性水平，拓展了低端激光扫描仪的实际应用潜力。

Abstract: We propose a multi-stage convolutional neural network (MSCNN) based
integrated method for reducing uncertainty of 3D point accuracy of lasar
scanner (LS) in rough indoor rooms, providing more accurate spatial
measurements for high-precision geometric model creation and renovation. Due to
different equipment limitations and environmental factors, high-end and low-end
LS have positional errors. Our approach pairs high-accuracy scanners (HAS) as
references with corresponding low-accuracy scanners (LAS) of measurements in
identical environments to quantify specific error patterns. By establishing a
statistical relationship between measurement discrepancies and their spatial
distribution, we develop a correction framework that combines traditional
geometric processing with targeted neural network refinement. This method
transforms the quantification of systematic errors into a supervised learning
problem, allowing precise correction while preserving critical geometric
features. Experimental results in our rough indoor rooms dataset show
significant improvements in measurement accuracy, with mean square error (MSE)
reductions exceeding 70% and peak signal-to-noise ratio (PSNR) improvements of
approximately 6 decibels. This approach enables low-end devices to achieve
measurement uncertainty levels approaching those of high-end devices without
hardware modifications.

</details>


### [37] [Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion](https://arxiv.org/abs/2508.12094)
*Songwei Liu,Hong Liu,Fangmin Chen,Xurui Peng,Chenqian Yan,Lean Fu,Xing Mei*

Main category: cs.CV

TL;DR: 该论文探讨了扩散模型的量化误差累积问题，提出了一种时间步感知的误差补偿方案，改善了低精度扩散模型的性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成图像质量高但计算成本大，常规量化方法会在多步骤迭代中导致误差累积，从而影响生成结果的质量。

Method: 建立了扩散模型中量化误差传播的理论框架，推导出误差传播的数学方程和累计误差的封闭解，并据此提出了一种时间步感知的误差补偿方案。

Result: 实验表明，所提出的补偿策略在多个数据集上显著减轻误差传播问题，使低精度扩散模型达到SOTA性能。

Conclusion: 通过理论分析和创新的补偿方案，有效改进了低精度扩散模型的性能，为大规模部署提供了更加高效的解决方案。

Abstract: Diffusion models have transformed image synthesis by establishing
unprecedented quality and creativity benchmarks. Nevertheless, their
large-scale deployment faces challenges due to computationally intensive
iterative denoising processes. Although post-training quantization(PTQ)
provides an effective pathway for accelerating sampling, the iterative nature
of diffusion models causes stepwise quantization errors to accumulate
progressively during generation, inevitably compromising output fidelity. To
address this challenge, we develop a theoretical framework that mathematically
formulates error propagation in Diffusion Models (DMs), deriving per-step
quantization error propagation equations and establishing the first closed-form
solution for cumulative error. Building on this theoretical foundation, we
propose a timestep-aware cumulative error compensation scheme. Extensive
experiments across multiple image datasets demonstrate that our compensation
strategy effectively mitigates error propagation, significantly enhancing
existing PTQ methods to achieve state-of-the-art(SOTA) performance on
low-precision diffusion models.

</details>


### [38] [VELVET-Med: Vision and Efficient Language Pre-training for Volumetric Imaging Tasks in Medicine](https://arxiv.org/abs/2508.12108)
*Ziyang Zhang,Yang Yu,Xulei Yang,Si Yong Yeo*

Main category: cs.CV

TL;DR: 提出了专为医学领域设计的视觉-语言预训练框架 VELVET-Med，用于有限的体积数据训练，有效提高了多个下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 解决医学领域体积扫描数据与放射学报告配对数据匮乏，制约下游模型应用的问题。

Method: 开发了 VELVET-Med 框架，引入单模态自监督学习、创新语言编码器 TriBERT 和分层对比学习，用小数据集深入挖掘图像与文本语义关系。

Result: 在38,875个扫描配对中，模型表现出很强的迁移能力，在3D分割、跨模态检索、视觉问答和报告生成等任务中达到了最新水平。

Conclusion: VELVET-Med 证明了在有限数据情况下，通过改进模型架构和学习目标可显著提升模型的泛化能力和应用潜力。

Abstract: Vision-and-language models (VLMs) have been increasingly explored in the
medical domain, particularly following the success of CLIP in general domain.
However, unlike the relatively straightforward pairing of 2D images and text,
curating large-scale paired data in the medical field for volumetric modalities
such as CT scans remains a challenging and time-intensive process. This
difficulty often limits the performance on downstream tasks. To address these
challenges, we propose a novel vision-language pre-training (VLP) framework,
termed as \textbf{VELVET-Med}, specifically designed for limited volumetric
data such as 3D CT and associated radiology reports. Instead of relying on
large-scale data collection, our method focuses on the development of effective
pre-training objectives and model architectures. The key contributions are: 1)
We incorporate uni-modal self-supervised learning into VLP framework, which are
often underexplored in the existing literature. 2) We propose a novel language
encoder, termed as \textbf{TriBERT}, for learning multi-level textual
semantics. 3) We devise the hierarchical contrastive learning to capture
multi-level vision-language correspondence. Using only 38,875 scan-report
pairs, our approach seeks to uncover rich spatial and semantic relationships
embedded in volumetric medical images and corresponding clinical narratives,
thereby enhancing the generalization ability of the learned encoders. The
resulting encoders exhibit strong transferability, achieving state-of-the-art
performance across a wide range of downstream tasks, including 3D segmentation,
cross-modal retrieval, visual question answering, and report generation.

</details>


### [39] [Simple o3: Towards Interleaved Vision-Language Reasoning](https://arxiv.org/abs/2508.12109)
*Ye Wang,Qianglong Chen,Zejun Li,Siyuan Wang,Shijie Guo,Zhirui Zhang,Zhongyu Wei*

Main category: cs.CV

TL;DR: 提出了一种名为Simple o3的框架，通过迭代的视觉操作和语言推理提升多模态场景下的连贯推理能力，并显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 针对现有多模态大模型在复杂长链多模态推理能力上的欠缺，作者希望开发出更强大且高效的解决方案。

Method: 设计了一个动态工具交互的端到端框架，结合观察-推理-行动循环，利用监督微调和数据生成策略训练生成TWI-Tools-146K数据集。

Result: 在多种基准测试中表现出色，推出了一种计算成本较低但推理能力强大的多模态推理模式。

Conclusion: 通过引入视觉代币、放大和裁剪等操作提高模型的视觉推理和细粒度感知能力，验证了交织推理策略的重要作用。

Abstract: Multimodal Large Language Models (MLLMs) have shown impressive performance on
vision-language tasks, but their long Chain-of-Thought (CoT) capabilities in
multimodal scenarios remain underexplored. Inspired by OpenAI's o3 model, which
emulates human-like ''thinking with image'' through iterative visual
transformations and linguistic reasoning, we propose Simple o3, an end-to-end
framework that integrates dynamic tool interactions (e.g., cropping, zooming,
and reusing) into interleaved vision-language reasoning via supervised
fine-tuning (SFT). Our approach features a scalable data synthesis pipeline
that generates high-quality interleaved vision-language reasoning chains via an
''observe-reason-act'' cycle, complete with executable visual operations and
rigorous verification, yielding the open-source TWI-Tools-146K dataset.
Experimental results demonstrate Simple o3's superior performance on diverse
benchmarks, outperforming existing approaches. By combining enhanced reasoning
capabilities, Simple o3 establishes a powerful yet computationally affordable
paradigm for advancing multimodal reasoning. Remarkably, we provide the first
in-depth analysis of different interleaved reasoning strategies, offering
insights into their impact on model performance. We found that by introducing
additional visual tokens for interleaved vision-language reasoning, reusing and
magnifying the original image significantly improves the model's visual
reasoning and fine-grained perception, while image cropping based on precise
visual grounding allows the model to effectively focus on key entities or
regions, further enhancing its capabilities.

</details>


### [40] [DualFit: A Two-Stage Virtual Try-On via Warping and Synthesis](https://arxiv.org/abs/2508.12131)
*Minh Tran,Johnmark Clements,Annie Prasanna,Tri Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: 提出了一种称为DualFit的混合虚拟试穿技术，通过两阶段的方法解决了传统方法无法保留细致衣物细节的问题。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟试穿技术虽然能提升感知质量，但在保留衣物细节方面存在不足，这对品牌形象和用户信任至关重要。

Method: 提出DualFit，两阶段方法：首先通过学习流场变形目标衣物并实现高保真；然后通过保真试穿模块合成最终结果，并引入保留区域输入及修补遮罩引导方式。

Result: DualFit实现了无缝的虚拟试穿效果，并在高频细节保留和感知真实性之间达到了有效平衡。

Conclusion: DualFit不仅解决了衣物细节保留的问题，还提高了虚拟试穿的整体视觉效果，为未来电商应用提供了潜在突破。

Abstract: Virtual Try-On technology has garnered significant attention for its
potential to transform the online fashion retail experience by allowing users
to visualize how garments would look on them without physical trials. While
recent advances in diffusion-based warping-free methods have improved
perceptual quality, they often fail to preserve fine-grained garment details
such as logos and printed text elements that are critical for brand integrity
and customer trust. In this work, we propose DualFit, a hybrid VTON pipeline
that addresses this limitation by two-stage approach. In the first stage,
DualFit warps the target garment to align with the person image using a learned
flow field, ensuring high-fidelity preservation. In the second stage, a
fidelity-preserving try-on module synthesizes the final output by blending the
warped garment with preserved human regions. Particularly, to guide this
process, we introduce a preserved-region input and an inpainting mask, enabling
the model to retain key areas and regenerate only where necessary, particularly
around garment seams. Extensive qualitative results show that DualFit achieves
visually seamless try-on results while faithfully maintaining high-frequency
garment details, striking an effective balance between reconstruction accuracy
and perceptual realism.

</details>


### [41] [TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks](https://arxiv.org/abs/2508.12132)
*Amira Guesmi,Bassem Ouni,Muhammad Shafique*

Main category: cs.CV

TL;DR: 本文提出了一种名为TriQDef的防御框架，通过干扰深度学习模型中的语义和感知梯度对齐，来抵抗针对量化神经网络的基于patch的对抗攻击，并在多个数据集上显示出显著效果。


<details>
  <summary>Details</summary>
Motivation: 近年来量化神经网络（QNNs）由于其计算及内存效率被广泛用于资源受限的边缘环境。然而，这些网络仍然容易受到基于patch的对抗性攻击，即通过局部高显著性扰动来欺骗模型，且其跨比特宽度的可迁移性尤为突出。现有防御措施在固定量化设置上表现出过拟合问题，并未能有效应对这种跨比特宽度的攻击迁移性问题。

Method: 设计了一种三层量化感知防御框架TriQDef，其中包括：(1)特征非对齐惩罚（FDP），通过惩罚中间表示的感知相似性来强制语义不一致；(2)梯度感知失调惩罚（GPDP），通过边缘IoU和HOG余弦度量最小化输入梯度的结构和方向一致性，从而显式扰乱跨比特宽度的梯度对齐；(3)联合量化感知训练协议，将这些惩罚整合到跨多个量化等级的共享权重训练方案中。

Result: 在CIFAR-10和ImageNet数据集上的实验表明，TriQDef能够在未见过的patch和量化组合上将攻击成功率（ASR）降低超过40%，同时保持较高的干净样本准确性。

Conclusion: 研究结果表明，通过干扰语义和感知梯度对齐，可以有效减少基于patch的对抗攻击在量化神经网络中的迁移性，从而提升模型的鲁棒性。

Abstract: Quantized Neural Networks (QNNs) are increasingly deployed in edge and
resource-constrained environments due to their efficiency in computation and
memory usage. While shown to distort the gradient landscape and weaken
conventional pixel-level attacks, it provides limited robustness against
patch-based adversarial attacks-localized, high-saliency perturbations that
remain surprisingly transferable across bit-widths. Existing defenses either
overfit to fixed quantization settings or fail to address this cross-bit
generalization vulnerability. We introduce \textbf{TriQDef}, a tri-level
quantization-aware defense framework designed to disrupt the transferability of
patch-based adversarial attacks across QNNs. TriQDef consists of: (1) a Feature
Disalignment Penalty (FDP) that enforces semantic inconsistency by penalizing
perceptual similarity in intermediate representations; (2) a Gradient
Perceptual Dissonance Penalty (GPDP) that explicitly misaligns input gradients
across bit-widths by minimizing structural and directional agreement via Edge
IoU and HOG Cosine metrics; and (3) a Joint Quantization-Aware Training
Protocol that unifies these penalties within a shared-weight training scheme
across multiple quantization levels. Extensive experiments on CIFAR-10 and
ImageNet demonstrate that TriQDef reduces Attack Success Rates (ASR) by over
40\% on unseen patch and quantization combinations, while preserving high clean
accuracy. Our findings underscore the importance of disrupting both semantic
and perceptual gradient alignment to mitigate patch transferability in QNNs.

</details>


### [42] [Infusing fine-grained visual knowledge to Vision-Language Models](https://arxiv.org/abs/2508.12137)
*Nikolaos-Antonios Ypsilantis,Kaifeng Chen,André Araujo,Ondřej Chum*

Main category: cs.CV

TL;DR: 提出了一种新的微调方法，旨在平衡预训练模型的精细化领域适应与广泛多模态知识保留。


<details>
  <summary>Details</summary>
Motivation: 当前预训练的视觉-语言模型在细粒度的开放集合视觉检索中表现不佳，需要通过领域样本微调，但传统微调容易导致灾难性遗忘。

Method: 从持续学习中借鉴灵感，系统分析知识保留的正则化技术，并提出高效组合策略，同时改进验证集设计和超参数调节。

Result: 在多种图像-图像及图像-文本检索基准上验证，方法能强效保留视觉-文本对齐能力且无需文本数据或文本编码器参与微调。

Conclusion: 该方法实现了细粒度领域适应与初始多模态知识之间的最佳平衡，有助于提高通用预训练模型的实际应用。

Abstract: Large-scale contrastive pre-training produces powerful Vision-and-Language
Models (VLMs) capable of generating representations (embeddings) effective for
a wide variety of visual and multimodal tasks. However, these pretrained
embeddings remain suboptimal for fine-grained open-set visual retrieval, where
state-of-the-art results require fine-tuning the vision encoder using annotated
domain-specific samples. Naively performing such fine-tuning typically leads to
catastrophic forgetting, severely diminishing the model's general-purpose
visual and cross-modal capabilities.
  In this work, we propose a fine-tuning method explicitly designed to achieve
optimal balance between fine-grained domain adaptation and retention of the
pretrained VLM's broad multimodal knowledge. Drawing inspiration from continual
learning literature, we systematically analyze standard regularization
techniques aimed at knowledge retention and propose an efficient and effective
combination strategy. Additionally, we address the commonly overlooked yet
critical aspects of validation set design and hyperparameter tuning to ensure
reproducibility and robust generalization across datasets and pretrained
models. We extensively evaluate our method on both fine-grained and
coarse-grained image-image and image-text retrieval benchmarks. Our approach
consistently achieves strong results, notably retaining the visual-text
alignment without utilizing any text data or the original text encoder during
fine-tuning. Code and model checkpoints: https://github.com/nikosips/infusing .

</details>


### [43] [KP-INR: A Dual-Branch Implicit Neural Representation Model for Cardiac Cine MRI Reconstruction](https://arxiv.org/abs/2508.12147)
*Donghang Lyu,Marius Staring,Mariya Doneva,Hildo J. Lamb,Nicola Pezzotti*

Main category: cs.CV

TL;DR: 本文提出一种名为KP-INR的双分支隐式神经表征方法，旨在提高心脏cine MRI重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统的快速采集技术虽然能够减少扫描时间和提升舒适度，但会降低图像质量。现有的隐式神经表征方法忽视了目标点及其周围上下文的特征表征，因此需要改进。

Method: KP-INR利用双分支结构在k-space中进行操作，一支处理k-space坐标的位置嵌入，另一支从这些坐标的局部多尺度特征中学习，通过交叉分支交互和综合学习，逼近目标k-space值。

Result: KP-INR在CMRxRecon2024数据集上表现优于基线模型，展示了其在高质量图像重建中的潜力。

Conclusion: KP-INR可以有效提高心脏cine MRI的图像重建质量，并且在这一领域具有较大的应用前景。

Abstract: Cardiac Magnetic Resonance (CMR) imaging is a non-invasive method for
assessing cardiac structure, function, and blood flow. Cine MRI extends this by
capturing heart motion, providing detailed insights into cardiac mechanics. To
reduce scan time and breath-hold discomfort, fast acquisition techniques have
been utilized at the cost of lowering image quality. Recently, Implicit Neural
Representation (INR) methods have shown promise in unsupervised reconstruction
by learning coordinate-to-value mappings from undersampled data, enabling
high-quality image recovery. However, current existing INR methods primarily
focus on using coordinate-based positional embeddings to learn the mapping,
while overlooking the feature representations of the target point and its
neighboring context. In this work, we propose KP-INR, a dual-branch INR method
operating in k-space for cardiac cine MRI reconstruction: one branch processes
the positional embedding of k-space coordinates, while the other learns from
local multi-scale k-space feature representations at those coordinates. By
enabling cross-branch interaction and approximating the target k-space values
from both branches, KP-INR can achieve strong performance on challenging
Cartesian k-space data. Experiments on the CMRxRecon2024 dataset confirms its
improved performance over baseline models and highlights its potential in this
field.

</details>


### [44] [Demystifying Foreground-Background Memorization in Diffusion Models](https://arxiv.org/abs/2508.12148)
*Jimmy Z. Di,Yiwei Lu,Yaoliang Yu,Gautam Kamath,Adam Dziedzic,Franziska Boenisch*

Main category: cs.CV

TL;DR: 该论文提出了一种新的度量方法FB-Mem，来检测扩散模型中记忆化的图像区域，并揭示了记忆化的复杂模式及现有缓解方法的不足。


<details>
  <summary>Details</summary>
Motivation: 当前检测方法无法有效量化局部记忆化以及超出具体提示-图像对的记忆化模式，该研究决定开发新的测量工具解决这些问题。

Method: 提出了一种基于分割的记忆化度量方法FB-Mem，用以分类和量化生成图像中的记忆化区域，并采用聚类方法缓解记忆化。

Result: 发现记忆化比之前理解的更加普遍：生成的单个图像可对应多个相似训练图像，说明存在复杂的记忆化模式；现有的缓解方法对局部记忆化无效，尤其是前景区域的记忆化仍然存在。

Conclusion: 提出的FB-Mem工具是评估扩散模型记忆化的有效框架，同时现有的缓解方法不尽完善，改进的基于聚类的方法可能更为有效。

Abstract: Diffusion models (DMs) memorize training images and can reproduce
near-duplicates during generation. Current detection methods identify verbatim
memorization but fail to capture two critical aspects: quantifying partial
memorization occurring in small image regions, and memorization patterns beyond
specific prompt-image pairs. To address these limitations, we propose
Foreground Background Memorization (FB-Mem), a novel segmentation-based metric
that classifies and quantifies memorized regions within generated images. Our
method reveals that memorization is more pervasive than previously understood:
(1) individual generations from single prompts may be linked to clusters of
similar training images, revealing complex memorization patterns that extend
beyond one-to-one correspondences; and (2) existing model-level mitigation
methods, such as neuron deactivation and pruning, fail to eliminate local
memorization, which persists particularly in foreground regions. Our work
establishes an effective framework for measuring memorization in diffusion
models, demonstrates the inadequacy of current mitigation approaches, and
proposes a stronger mitigation method using a clustering approach.

</details>


### [45] [RealTalk: Realistic Emotion-Aware Lifelike Talking-Head Synthesis](https://arxiv.org/abs/2508.12163)
*Wenqing Wang,Yun Fu*

Main category: cs.CV

TL;DR: 提出了RealTalk框架，用于生成具有高情感准确性、情感可控性和身份保持能力的情感对话头像。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在生成情感表达时身份保持较差及情感可控性不足的问题。

Method: 使用变分自编码器(VAE)生成3D面部关键点，结合基于ResNet的标志变形模型(LDM)及新的三平面注意力NeRF模型合成情感对话头像。

Result: 实验表明，RealTalk在情感准确性、可控性及身份保持方面优于现有方法。

Conclusion: RealTalk推进了社会智能AI系统的发展，提高了情感生成技术的精度和实用性。

Abstract: Emotion is a critical component of artificial social intelligence. However,
while current methods excel in lip synchronization and image quality, they
often fail to generate accurate and controllable emotional expressions while
preserving the subject's identity. To address this challenge, we introduce
RealTalk, a novel framework for synthesizing emotional talking heads with high
emotion accuracy, enhanced emotion controllability, and robust identity
preservation. RealTalk employs a variational autoencoder (VAE) to generate 3D
facial landmarks from driving audio, which are concatenated with emotion-label
embeddings using a ResNet-based landmark deformation model (LDM) to produce
emotional landmarks. These landmarks and facial blendshape coefficients jointly
condition a novel tri-plane attention Neural Radiance Field (NeRF) to
synthesize highly realistic emotional talking heads. Extensive experiments
demonstrate that RealTalk outperforms existing methods in emotion accuracy,
controllability, and identity preservation, advancing the development of
socially intelligent AI systems.

</details>


### [46] [Scalable RF Simulation in Generative 4D Worlds](https://arxiv.org/abs/2508.12176)
*Zhiwei Zheng,Dongyin Hu,Mingmin Zhao*

Main category: cs.CV

TL;DR: 该研究提出了WaveVerse框架，用于在室内环境中生成真实的射频（RF）信号，有助于隐私保护的感知任务。


<details>
  <summary>Details</summary>
Motivation: 为了解决在动态、多样化的室内环境中采集高质量RF数据的挑战。

Method: 开发了一个语言引导的4D生成器，包括状态感知因果变压器，用于基于空间约束和文本生成运动，以及相位一致的射线追踪模拟，用于模拟高精度RF信号。

Result: 实验验证了在人类运动生成上的有效性，并展示了相位一致性在波束成形和呼吸监测中的应用。此外，还通过两个案例研究证明了WaveVerse框架在RF成像和人类活动识别中的性能提升。

Conclusion: WaveVerse首次实现了RF成像的数据生成，并在数据有限和数据充足的情况下均显示了性能提升。

Abstract: Radio Frequency (RF) sensing has emerged as a powerful, privacy-preserving
alternative to vision-based methods for indoor perception tasks. However,
collecting high-quality RF data in dynamic and diverse indoor environments
remains a major challenge. To address this, we introduce WaveVerse, a
prompt-based, scalable framework that simulates realistic RF signals from
generated indoor scenes with human motions. WaveVerse introduces a
language-guided 4D world generator, which includes a state-aware causal
transformer for human motion generation conditioned on spatial constraints and
texts, and a phase-coherent ray tracing simulator that enables the simulation
of accurate and coherent RF signals. Experiments demonstrate the effectiveness
of our approach in conditioned human motion generation and highlight how phase
coherence is applied to beamforming and respiration monitoring. We further
present two case studies in ML-based high-resolution imaging and human activity
recognition, demonstrating that WaveVerse not only enables data generation for
RF imaging for the first time, but also consistently achieves performance gain
in both data-limited and data-adequate scenarios.

</details>


### [47] [Splat Feature Solver](https://arxiv.org/abs/2508.12216)
*Butian Xiong,Rong Liu,Kenneth Xu,Meida Chen,Andrew Feng*

Main category: cs.CV

TL;DR: 提出一种统一的特征提升方法，将3D场景中的图像特征高效分配给3D元素，并解决多视图间的不一致问题，达到了最新的开放词汇3D分割基准表现。


<details>
  <summary>Details</summary>
Motivation: 解决3D场景中从多视图图像中提取特征分配到3D原语的复杂任务，同时克服其带来的不一致性挑战。

Method: 将特征提升问题建模为稀疏线性逆问题，并通过两种正则化策略（Tikhonov Guidance和Post-Lifting Aggregation）稳定和优化解决方案。

Result: 在开放词汇的3D分割基准上取得了最先进的性能，且相比训练、分组或启发式方法表现更优。

Conclusion: 提出的方法不仅优化了特征提升的效率和准确性，还通过正则化策略增强了语义一致性和稳健性。

Abstract: Feature lifting has emerged as a crucial component in 3D scene understanding,
enabling the attachment of rich image feature descriptors (e.g., DINO, CLIP)
onto splat-based 3D representations. The core challenge lies in optimally
assigning rich general attributes to 3D primitives while addressing the
inconsistency issues from multi-view images. We present a unified, kernel- and
feature-agnostic formulation of the feature lifting problem as a sparse linear
inverse problem, which can be solved efficiently in closed form. Our approach
admits a provable upper bound on the global optimal error under convex losses
for delivering high quality lifted features. To address inconsistencies and
noise in multi-view observations, we introduce two complementary regularization
strategies to stabilize the solution and enhance semantic fidelity. Tikhonov
Guidance enforces numerical stability through soft diagonal dominance, while
Post-Lifting Aggregation filters noisy inputs via feature clustering. Extensive
experiments demonstrate that our approach achieves state-of-the-art performance
on open-vocabulary 3D segmentation benchmarks, outperforming training-based,
grouping-based, and heuristic-forward baselines while producing the lifted
features in minutes. Code is available at
\href{https://github.com/saliteta/splat-distiller.git}{\textbf{github}}. We
also have a \href{https://splat-distiller.pages.dev/}

</details>


### [48] [C2PSA-Enhanced YOLOv11 Architecture: A Novel Approach for Small Target Detection in Cotton Disease Diagnosis](https://arxiv.org/abs/2508.12219)
*Kaiyuan Wang,Jixing Liu,Xiaobo Cai*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的优化YOLOv11模型，用于棉花病害检测，特别关注小目标检测失误、野外性能下降及多病害场景下的误差问题。


<details>
  <summary>Details</summary>
Motivation: 动机是提高棉花病害的检测精准度与效率，解决现有模型难以应对的早期小斑点检出、环境性能恶化及病害多样性问题。

Method: 提出C2PSA模块增强小目标特征提取、动态类别加权处理样本不均衡、以及通过Mosaic-MixUp改进数据增强，优化YOLOv11模型。

Result: 基于4078张图像的实验结果显示：mAP50达到0.820（提升8.0%），mAP50-95达到0.705（提升10.5%），推理速度为158 FPS。

Conclusion: 改进的YOLOv11模型及移动部署系统实现了实时病害监测与精准治疗，有望应用于农业智能化监控。

Abstract: This study presents a deep learning-based optimization of YOLOv11 for cotton
disease detection, developing an intelligent monitoring system. Three key
challenges are addressed: (1) low precision in early spot detection (35%
leakage rate for sub-5mm2 spots), (2) performance degradation in field
conditions (25% accuracy drop), and (3) high error rates (34.7%) in
multi-disease scenarios. The proposed solutions include: C2PSA module for
enhanced small-target feature extraction; Dynamic category weighting to handle
sample imbalance; Improved data augmentation via Mosaic-MixUp scaling.
Experimental results on a 4,078-image dataset show: mAP50: 0.820 (+8.0%
improvement); mAP50-95: 0.705 (+10.5% improvement); Inference speed: 158 FPS.
The mobile-deployed system enables real-time disease monitoring and precision
treatment in agricultural applications.

</details>


### [49] [In vivo 3D ultrasound computed tomography of musculoskeletal tissues with generative neural physics](https://arxiv.org/abs/2508.12226)
*Zhijun Zeng,Youjia Zheng,Chang Su,Qianhang Wu,Hao Hu,Zeyuan Dong,Shan Gao,Yang Lv,Rui Tang,Ligang Cui,Zhiyong Hou,Weijun Lin,Zuoqiang Shi,Yubing Li,He Sun*

Main category: cs.CV

TL;DR: 本论文提出了一种将生成式网络与物理模拟相结合的生成神经物理学框架，实现了快速、高保真的三维超声计算机断层成像（USCT）。


<details>
  <summary>Details</summary>
Motivation: 传统超声计算机断层成像依赖射线建模，忽略了强散射效应，限制了其在肌肉骨骼成像中的应用。提出新的方法以克服此局限性。

Method: 将生成式神经网络与基于物理的神经模拟相结合，通过少量多模态图像学习超声波传播的紧凑替代模型，从而提升建模的准确性与深度学习的效率和稳定性。

Result: 在合成和体内数据（乳房、手臂、腿部）上，方案可在10分钟内重建组织参数3D图，达到与MRI相当的分辨率，并具备对肌肉和骨骼生物力学性质的灵敏度。

Conclusion: 通过克服强散射场景中的计算瓶颈，该方法推动USCT向日常肌肉骨骼疾病临床评估的方向发展。

Abstract: Ultrasound computed tomography (USCT) is a radiation-free, high-resolution
modality but remains limited for musculoskeletal imaging due to conventional
ray-based reconstructions that neglect strong scattering. We propose a
generative neural physics framework that couples generative networks with
physics-informed neural simulation for fast, high-fidelity 3D USCT. By learning
a compact surrogate of ultrasonic wave propagation from only dozens of
cross-modality images, our method merges the accuracy of wave modeling with the
efficiency and stability of deep learning. This enables accurate quantitative
imaging of in vivo musculoskeletal tissues, producing spatial maps of acoustic
properties beyond reflection-mode images. On synthetic and in vivo data
(breast, arm, leg), we reconstruct 3D maps of tissue parameters in under ten
minutes, with sensitivity to biomechanical properties in muscle and bone and
resolution comparable to MRI. By overcoming computational bottlenecks in
strongly scattering regimes, this approach advances USCT toward routine
clinical assessment of musculoskeletal disease.

</details>


### [50] [WXSOD: A Benchmark for Robust Salient Object Detection in Adverse Weather Conditions](https://arxiv.org/abs/2508.12250)
*Quan Chen,Xiong Yang,Rongfeng Lu,Qianyu Zhang,Yu Liu,Xiaofei Zhou,Bolun Zheng*

Main category: cs.CV

TL;DR: 本文提出了一个气象扩展显著目标检测（WXSOD）数据集，包含14,945张含有多样天气噪声的RGB图像及相应标注，并提出了一个基线模型WFANet，在复杂天气下显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有显著目标检测方法在复杂天气下的表现较差，部分原因是缺乏带像素级标注的相关数据集。鉴于此，本文研发了一个专注于天气噪声的显著目标检测数据集，并提出新的方法来提升检测性能。

Method: 构建了一个包含多种天气噪声的WXSOD数据集，并提出了WFANet模型，利用两个分支结构分别进行天气特征挖掘和显著目标检测，通过特征融合提升算法的表现。

Result: WFANet在WXSOD数据集的两个测试集上均展现出优越性能，并且超越了17种现有显著目标检测方法。

Conclusion: 基于WXSOD数据集的新模型WFANet在复杂天气条件下显著提高SOD表现，展示了方法和数据集的有效性，相关代码和结果将公开以促进进一步研究。

Abstract: Salient object detection (SOD) in complex environments remains a challenging
research topic. Most existing methods perform well in natural scenes with
negligible noise, and tend to leverage multi-modal information (e.g., depth and
infrared) to enhance accuracy. However, few studies are concerned with the
damage of weather noise on SOD performance due to the lack of dataset with
pixel-wise annotations. To bridge this gap, this paper introduces a novel
Weather-eXtended Salient Object Detection (WXSOD) dataset. It consists of
14,945 RGB images with diverse weather noise, along with the corresponding
ground truth annotations and weather labels. To verify algorithm
generalization, WXSOD contains two test sets, i.e., a synthesized test set and
a real test set. The former is generated by adding weather noise to clean
images, while the latter contains real-world weather noise. Based on WXSOD, we
propose an efficient baseline, termed Weather-aware Feature Aggregation Network
(WFANet), which adopts a fully supervised two-branch architecture.
Specifically, the weather prediction branch mines weather-related deep
features, while the saliency detection branch fuses semantic features extracted
from the backbone with weather features for SOD. Comprehensive comparisons
against 17 SOD methods shows that our WFANet achieves superior performance on
WXSOD. The code and benchmark results will be made publicly available at
https://github.com/C-water/WXSOD

</details>


### [51] [Superpixel-informed Continuous Low-Rank Tensor Representation for Multi-Dimensional Data Recovery](https://arxiv.org/abs/2508.12261)
*Zhizhou Wang,Ruijing Zheng,Zhenyu Wu,Jianli Wang*

Main category: cs.CV

TL;DR: 提出了一种超像素引导的连续低秩张量表示(SCTR)框架，克服了传统低秩张量表示方法的两大局限，同时提升了表示的灵活性和效率，在多个基准数据集上的表现显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的低秩张量表示方法假设整体数据为低秩，但该假设在实际应用中不成立。此外，现有方法主要针对离散网格数据，灵活性受限。这些限制阻碍了实际应用。

Method: 提出了一种结合超像素和非对称低秩张量分解(ALTF)的新框架SCTR，利用超像素作为建模单元，并通过共享的神经网络实现超像素间模式学习与局部适应的分离。

Result: 在多光谱图像、视频及彩色图像基准数据集上的实验中，SCTR方法相比现有低秩张量表示方法在PSNR上提升了3-5 dB。

Conclusion: SCTR框架解决了传统低秩张量表示的局限性，具有高度表达性和紧凑性，平衡了模型效率和适应性，为多维数据处理提供了一种有效的新方法。

Abstract: Low-rank tensor representation (LRTR) has emerged as a powerful tool for
multi-dimensional data processing. However, classical LRTR-based methods face
two critical limitations: (1) they typically assume that the holistic data is
low-rank, this assumption is often violated in real-world scenarios with
significant spatial variations; and (2) they are constrained to discrete
meshgrid data, limiting their flexibility and applicability. To overcome these
limitations, we propose a Superpixel-informed Continuous low-rank Tensor
Representation (SCTR) framework, which enables continuous and flexible modeling
of multi-dimensional data beyond traditional grid-based constraints. Our
approach introduces two main innovations: First, motivated by the observation
that semantically coherent regions exhibit stronger low-rank characteristics
than holistic data, we employ superpixels as the basic modeling units. This
design not only encodes rich semantic information, but also enhances
adaptability to diverse forms of data streams. Second, we propose a novel
asymmetric low-rank tensor factorization (ALTF) where superpixel-specific
factor matrices are parameterized by a shared neural network with specialized
heads. By strategically separating global pattern learning from local
adaptation, this framework efficiently captures both cross-superpixel
commonalities and within-superpixel variations. This yields a representation
that is both highly expressive and compact, balancing model efficiency with
adaptability. Extensive experiments on several benchmark datasets demonstrate
that SCTR achieves 3-5 dB PSNR improvements over existing LRTR-based methods
across multispectral images, videos, and color images.

</details>


### [52] [Region-Level Context-Aware Multimodal Understanding](https://arxiv.org/abs/2508.12263)
*Hongliang Wei,Xianqi Zhang,Xingtao Wang,Xiaopeng Fan,Debin Zhao*

Main category: cs.CV

TL;DR: 本文提出了一个称为RCMU（区域级上下文感知多模态理解）的任务，研究结合图像内容和相关文本信息以实现更深层次的多模态理解，并提出了一种RCVIT方法以及相关数据集和评测基准。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型研究忽略了结合与对象相关的文本上下文的信息，从而限制了其上下文感知的能力。

Method: 提出了区域级上下文感知视觉指令调优方法（RCVIT），结合物体的边界框信息将图像和相应的文本信息相结合。此外，还构建了RCMU数据集和RC&P-Bench评测基准，提出了参考无关的评价指标。

Result: 通过在Qwen2-VL模型上的实验，改进后的RC-Qwen2-VL模型在多个RCMU任务中表现优异，并成功应用在多模态检索生成和个性化对话中。

Conclusion: RC-Qwen2-VL模型展示了强大的RCMU能力，为多模态模型研究和应用提供了新的可能性。

Abstract: Despite significant progress, existing research on Multimodal Large Language
Models (MLLMs) mainly focuses on general visual understanding, overlooking the
ability to integrate textual context associated with objects for a more
context-aware multimodal understanding -- an ability we refer to as
Region-level Context-aware Multimodal Understanding (RCMU). To address this
limitation, we first formulate the RCMU task, which requires models to respond
to user instructions by integrating both image content and textual information
of regions or objects. To equip MLLMs with RCMU capabilities, we propose
Region-level Context-aware Visual Instruction Tuning (RCVIT), which
incorporates object information into the model input and enables the model to
utilize bounding box coordinates to effectively associate objects' visual
content with their textual information. To address the lack of datasets, we
introduce the RCMU dataset, a large-scale visual instruction tuning dataset
that covers multiple RCMU tasks. We also propose RC\&P-Bench, a comprehensive
benchmark that can evaluate the performance of MLLMs in RCMU and multimodal
personalized understanding tasks. Additionally, we propose a reference-free
evaluation metric to perform a comprehensive and fine-grained evaluation of the
region-level context-aware image descriptions. By performing RCVIT on Qwen2-VL
models with the RCMU dataset, we developed RC-Qwen2-VL models. Experimental
results indicate that RC-Qwen2-VL models not only achieve outstanding
performance on multiple RCMU tasks but also demonstrate successful applications
in multimodal RAG and personalized conversation. Our data, model and benchmark
are available at https://github.com/hongliang-wei/RC-MLLM

</details>


### [53] [SNNSIR: A Simple Spiking Neural Network for Stereo Image Restoration](https://arxiv.org/abs/2508.12271)
*Ronghua Xu,Jin Xie,Jing Nie,Jiale Cao,Yanwei Pang*

Main category: cs.CV

TL;DR: 提出了一个完全基于脉冲神经网络(SNN)的立体图像恢复模型(SNNSIR)，具有低功耗和硬件友好特性。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络具有高效性和低能耗的特点，非常适合计算密集型的立体图像恢复任务，但现有SNN-ANN混合模型不完全与事件驱动特性兼容。

Method: 提出完全脉冲驱动的SNNSIR架构，通过Spike Residual Basic Block (SRBB)提升信息流动，通过Spike Stereo Convolutional Modulation (SSCM)模块改善非线性处理及降噪，通过Spike Stereo Cross-Attention (SSCA)模块提高双视角特征交互。

Result: 在降雨条纹去除、雨滴去除、低光增强及超分辨率等任务中，性能优越并显著减少计算开销。

Conclusion: SNNSIR在保持竞争性能的同时，实现了低功耗和低计算的图像恢复，展现了其在实时立体视觉中的潜力。

Abstract: Spiking Neural Networks (SNNs), characterized by discrete binary activations,
offer high computational efficiency and low energy consumption, making them
well-suited for computation-intensive tasks such as stereo image restoration.
In this work, we propose SNNSIR, a simple yet effective Spiking Neural Network
for Stereo Image Restoration, specifically designed under the spike-driven
paradigm where neurons transmit information through sparse, event-based binary
spikes. In contrast to existing hybrid SNN-ANN models that still rely on
operations such as floating-point matrix division or exponentiation, which are
incompatible with the binary and event-driven nature of SNNs, our proposed
SNNSIR adopts a fully spike-driven architecture to achieve low-power and
hardware-friendly computation. To address the expressiveness limitations of
binary spiking neurons, we first introduce a lightweight Spike Residual Basic
Block (SRBB) to enhance information flow via spike-compatible residual
learning. Building on this, the Spike Stereo Convolutional Modulation (SSCM)
module introduces simplified nonlinearity through element-wise multiplication
and highlights noise-sensitive regions via cross-view-aware modulation.
Complementing this, the Spike Stereo Cross-Attention (SSCA) module further
improves stereo correspondence by enabling efficient bidirectional feature
interaction across views within a spike-compatible framework. Extensive
experiments on diverse stereo image restoration tasks, including rain streak
removal, raindrop removal, low-light enhancement, and super-resolution
demonstrate that our model achieves competitive restoration performance while
significantly reducing computational overhead. These results highlight the
potential for real-time, low-power stereo vision applications. The code will be
available after the article is accepted.

</details>


### [54] [TSLA: A Task-Specific Learning Adaptation for Semantic Segmentation on Autonomous Vehicles Platform](https://arxiv.org/abs/2508.12279)
*Jun Liu,Zhenglun Kong,Pu Zhao,Weihao Zeng,Hao Tang,Xuan Shen,Changdi Yang,Wenbin Zhang,Geng Yuan,Wei Niu,Xue Lin,Yanzhi Wang*

Main category: cs.CV

TL;DR: 研究提出了一种动态适配机制，通过三层控制方法，根据硬件约束和任务需求定制语义分割网络，并采用贝叶斯优化高效探索超参数空间。


<details>
  <summary>Details</summary>
Motivation: 目标是解决自动驾驶硬件计算能力有限的问题，根据硬件和场景需求优化语义分割网络的性能。

Method: 提出三层控制机制（宽度倍增器、分类器深度、分类器核）实现动态调整，并结合贝叶斯优化寻找最优参数配置。

Result: 通过任务特定的学习适配，实现了多种配置以满足不同自驾任务，为硬件优化了计算能力和模型准确性。

Conclusion: 该方法成功优化了自动驾驶平台的硬件利用率和性能，为多场景和任务提供了灵活的网络自适应能力。

Abstract: Autonomous driving platforms encounter diverse driving scenarios, each with
varying hardware resources and precision requirements. Given the computational
limitations of embedded devices, it is crucial to consider computing costs when
deploying on target platforms like the NVIDIA\textsuperscript{\textregistered}
DRIVE PX 2. Our objective is to customize the semantic segmentation network
according to the computing power and specific scenarios of autonomous driving
hardware. We implement dynamic adaptability through a three-tier control
mechanism -- width multiplier, classifier depth, and classifier kernel --
allowing fine-grained control over model components based on hardware
constraints and task requirements. This adaptability facilitates broad model
scaling, targeted refinement of the final layers, and scenario-specific
optimization of kernel sizes, leading to improved resource allocation and
performance.
  Additionally, we leverage Bayesian Optimization with surrogate modeling to
efficiently explore hyperparameter spaces under tight computational budgets.
Our approach addresses scenario-specific and task-specific requirements through
automatic parameter search, accommodating the unique computational complexity
and accuracy needs of autonomous driving. It scales its Multiply-Accumulate
Operations (MACs) for Task-Specific Learning Adaptation (TSLA), resulting in
alternative configurations tailored to diverse self-driving tasks. These TSLA
customizations maximize computational capacity and model accuracy, optimizing
hardware utilization.

</details>


### [55] [CLAIR: CLIP-Aided Weakly Supervised Zero-Shot Cross-Domain Image Retrieval](https://arxiv.org/abs/2508.12290)
*Chor Boon Tan,Conghui Hu,Gim Hee Lee*

Main category: cs.CV

TL;DR: 本文探讨了如何利用大型基础模型生成的伪标签，提出CLAIR模型，以改进弱监督跨域零样本图像检索，并实现优秀性能。


<details>
  <summary>Details</summary>
Motivation: 近年来大型基础模型的快速发展能够生成大量未标记数据的伪标签，使得完全无监督的跨域图像检索的需求减弱，因此作者将注意力转向使用噪声伪标签的弱监督跨域零样本图像检索（WSZS-CDIR）。

Method: 提出CLAIR模型，通过CLIP生成的文本与图像特征相似性分数来优化噪声伪标签，并设计了多种对比损失函数以编码图像进入类别感知的潜在空间，缓解域间的差异。同时学习了基于CLIP文本嵌入的跨域映射函数以对齐图像特征。此外，还引入可学习的提示以增强模型对于新类别的零样本泛化能力。

Result: 在TUBerlin、Sketchy、Quickdraw和DomainNet的零样本数据集上的实验表明，CLAIR方法性能优于现有最先进方法。

Conclusion: CLAIR不仅能有效处理噪声伪标签，还能缓解域间差异，并增强对新类别的泛化能力，表现出优秀的弱监督跨域零样本图像检索效果。

Abstract: The recent growth of large foundation models that can easily generate
pseudo-labels for huge quantity of unlabeled data makes unsupervised Zero-Shot
Cross-Domain Image Retrieval (UZS-CDIR) less relevant. In this paper, we
therefore turn our attention to weakly supervised ZS-CDIR (WSZS-CDIR) with
noisy pseudo labels generated by large foundation models such as CLIP. To this
end, we propose CLAIR to refine the noisy pseudo-labels with a confidence score
from the similarity between the CLIP text and image features. Furthermore, we
design inter-instance and inter-cluster contrastive losses to encode images
into a class-aware latent space, and an inter-domain contrastive loss to
alleviate domain discrepancies. We also learn a novel cross-domain mapping
function in closed-form, using only CLIP text embeddings to project image
features from one domain to another, thereby further aligning the image
features for retrieval. Finally, we enhance the zero-shot generalization
ability of our CLAIR to handle novel categories by introducing an extra set of
learnable prompts. Extensive experiments are carried out using TUBerlin,
Sketchy, Quickdraw, and DomainNet zero-shot datasets, where our CLAIR
consistently shows superior performance compared to existing state-of-the-art
methods.

</details>


### [56] [Improving Densification in 3D Gaussian Splatting for High-Fidelity Rendering](https://arxiv.org/abs/2508.12313)
*Xiaobin Deng,Changyu Diao,Min Li,Ruohan Yu,Duanqing Xu*

Main category: cs.CV

TL;DR: 提出改进3D Gaussian Splatting (3DGS)的点密化策略，优化重建质量，实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 针对3DGS点密化策略导致的重建效果次优问题，提出改进方案以提升渲染精确度。

Method: 提出边缘感知评分、长轴分裂策略，以及一系列减轻过拟合的技术（如恢复感知剪枝、多步更新和生长控制）。

Result: 在不增加训练或推理开销的情况下，提升渲染保真度并减少高斯点数量，实现最先进性能。

Conclusion: 通过优化点密化流程，显著改进3DGS的渲染质量和效率，有效缓解了质量与高效之间的权衡。

Abstract: Although 3D Gaussian Splatting (3DGS) has achieved impressive performance in
real-time rendering, its densification strategy often results in suboptimal
reconstruction quality. In this work, we present a comprehensive improvement to
the densification pipeline of 3DGS from three perspectives: when to densify,
how to densify, and how to mitigate overfitting. Specifically, we propose an
Edge-Aware Score to effectively select candidate Gaussians for splitting. We
further introduce a Long-Axis Split strategy that reduces geometric distortions
introduced by clone and split operations. To address overfitting, we design a
set of techniques, including Recovery-Aware Pruning, Multi-step Update, and
Growth Control. Our method enhances rendering fidelity without introducing
additional training or inference overhead, achieving state-of-the-art
performance with fewer Gaussians.

</details>


### [57] [Neural Cellular Automata for Weakly Supervised Segmentation of White Blood Cells](https://arxiv.org/abs/2508.12322)
*Michael Deutges,Chen Yang,Raheleh Salehi,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.CV

TL;DR: 通过基于神经元胞自动机（NCA）的弱监督方法实现白细胞图像的检测与分割，显著提高了性能并减少了标注数据需求。


<details>
  <summary>Details</summary>
Motivation: 白细胞在血涂片图像中的检测与分割是进行血液疾病检测与分析的关键步骤，但标注数据昂贵且难以获取，因此需要研究弱监督方法以降低标注需求。

Method: 提出一种基于神经元胞自动机（NCA）的弱监督分割方法（NCA-WSS），通过利用NCA在分类过程中生成的特征图进行分割掩码提取，无需使用标注数据再训练。

Result: 该方法在三个白细胞显微镜数据集上进行了评估，结果表明NCA-WSS显著优于现有弱监督方法。

Conclusion: 使用NCA的弱监督框架可以同时实现分类与分割，提供了一种高效且具有扩展性的医学图像分析解决方案。

Abstract: The detection and segmentation of white blood cells in blood smear images is
a key step in medical diagnostics, supporting various downstream tasks such as
automated blood cell counting, morphological analysis, cell classification, and
disease diagnosis and monitoring. Training robust and accurate models requires
large amounts of labeled data, which is both time-consuming and expensive to
acquire. In this work, we propose a novel approach for weakly supervised
segmentation using neural cellular automata (NCA-WSS). By leveraging the
feature maps generated by NCA during classification, we can extract
segmentation masks without the need for retraining with segmentation labels. We
evaluate our method on three white blood cell microscopy datasets and
demonstrate that NCA-WSS significantly outperforms existing weakly supervised
approaches. Our work illustrates the potential of NCA for both classification
and segmentation in a weakly supervised framework, providing a scalable and
efficient solution for medical image analysis.

</details>


### [58] [Attention Pooling Enhances NCA-based Classification of Microscopy Images](https://arxiv.org/abs/2508.12324)
*Chen Yang,Michael Deutges,Jingsong Liu,Han Li,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.CV

TL;DR: 本研究通过结合注意力池化与神经元格自动机（NCA），提升微观图像分类性能，显著超越现有方法并保持参数高效。


<details>
  <summary>Details</summary>
Motivation: 现有的NCA在图像分类中具有解释性和鲁棒性，但性能还不及更复杂的架构。

Method: 通过引入注意力池化机制来增强NCA的特征提取能力，并在多种微观图像数据集上进行评估。

Result: 本文方法在八个数据集上显著超越现有NCA方法，并在保持较低参数量的同时提高分类性能。

Conclusion: 结合注意力池化的NCA模型是一种在解释性和性能上均具潜力的图像分类替代方案。

Abstract: Neural Cellular Automata (NCA) offer a robust and interpretable approach to
image classification, making them a promising choice for microscopy image
analysis. However, a performance gap remains between NCA and larger, more
complex architectures. We address this challenge by integrating attention
pooling with NCA to enhance feature extraction and improve classification
accuracy. The attention pooling mechanism refines the focus on the most
informative regions, leading to more accurate predictions. We evaluate our
method on eight diverse microscopy image datasets and demonstrate that our
approach significantly outperforms existing NCA methods while remaining
parameter-efficient and explainable. Furthermore, we compare our method with
traditional lightweight convolutional neural network and vision transformer
architectures, showing improved performance while maintaining a significantly
lower parameter count. Our results highlight the potential of NCA-based models
an alternative for explainable image classification.

</details>


### [59] [DoppDrive: Doppler-Driven Temporal Aggregation for Improved Radar Object Detection](https://arxiv.org/abs/2508.12330)
*Yuval Haitman,Oded Bialer*

Main category: cs.CV

TL;DR: 本文提出了一种基于多普勒的时间聚合方法DoppDrive，旨在提高雷达点云密度，同时减少点云的散射，从而提升自动驾驶中的目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 由于雷达点云稀疏性在远距离下尤为显著，传统通过时间聚合提高点密度的方法会因动态物体散射效应导致检测性能下降，因此亟需一种能够减少散射效应、提升点云密度的新方法。

Method: 提出DoppDrive方法，通过结合前帧点的动态多普勒分量进行径向平移，消除径向散射，并根据点的多普勒值及角度分配聚合时长，减少切向散射。该方法可以作为检测前的点云密度增强步骤，与任意检测器兼容。

Result: DoppDrive显著提高了在不同检测器和数据集中的目标检测性能，验证了其有效性。

Conclusion: 这是一个创新性的基于多普勒驱动的雷达点云时间聚合方法，既提升了点云密度，也有效改善了目标检测表现，具有较高的实际应用潜力。

Abstract: Radar-based object detection is essential for autonomous driving due to
radar's long detection range. However, the sparsity of radar point clouds,
especially at long range, poses challenges for accurate detection. Existing
methods increase point density through temporal aggregation with ego-motion
compensation, but this approach introduces scatter from dynamic objects,
degrading detection performance. We propose DoppDrive, a novel Doppler-Driven
temporal aggregation method that enhances radar point cloud density while
minimizing scatter. Points from previous frames are shifted radially according
to their dynamic Doppler component to eliminate radial scatter, with each point
assigned a unique aggregation duration based on its Doppler and angle to
minimize tangential scatter. DoppDrive is a point cloud density enhancement
step applied before detection, compatible with any detector, and we demonstrate
that it significantly improves object detection performance across various
detectors and datasets.

</details>


### [60] [Geometry-Aware Video Inpainting for Joint Headset Occlusion Removal and Face Reconstruction in Social XR](https://arxiv.org/abs/2508.12336)
*Fatemeh Ghorbani Lohesara,Karen Eguiazarian,Sebastian Knorr*

Main category: cs.CV

TL;DR: 该研究提出一个基于几何感知和学习的框架，用于从单视点RGB视频中消除HMD遮挡并重建完整的3D面部几何结构。


<details>
  <summary>Details</summary>
Motivation: HMD遮挡用户上脸部分，影响社交XR应用如远程会议中对面部表情和注视细节的捕获，从而降低沉浸感。

Method: 提出一个结合GAN视频修复网络与SynergyNet的框架，通过密集面部标志和无遮挡参考帧引导消除遮挡，同时回归3DMM参数实现高精度3D面部重建。

Result: 实验证明该框架能成功移除HMD遮挡，保留面部身份特征与现实感，并生成高度真实的3D面部几何结构。

Conclusion: 框架不仅在去遮挡与几何恢复方面性能优异，还对标志密度变化表现出鲁棒性，适用于广泛场景。

Abstract: Head-mounted displays (HMDs) are essential for experiencing extended reality
(XR) environments and observing virtual content. However, they obscure the
upper part of the user's face, complicating external video recording and
significantly impacting social XR applications such as teleconferencing, where
facial expressions and eye gaze details are crucial for creating an immersive
experience. This study introduces a geometry-aware learning-based framework to
jointly remove HMD occlusions and reconstruct complete 3D facial geometry from
RGB frames captured from a single viewpoint. The method integrates a GAN-based
video inpainting network, guided by dense facial landmarks and a single
occlusion-free reference frame, to restore missing facial regions while
preserving identity. Subsequently, a SynergyNet-based module regresses 3D
Morphable Model (3DMM) parameters from the inpainted frames, enabling accurate
3D face reconstruction. Dense landmark optimization is incorporated throughout
the pipeline to improve both the inpainting quality and the fidelity of the
recovered geometry. Experimental results demonstrate that the proposed
framework can successfully remove HMDs from RGB facial videos while maintaining
facial identity and realism, producing photorealistic 3D face geometry outputs.
Ablation studies further show that the framework remains robust across
different landmark densities, with only minor quality degradation under sparse
landmark configurations.

</details>


### [61] [Semantic Discrepancy-aware Detector for Image Forgery Identification](https://arxiv.org/abs/2508.12341)
*Ziye Wang,Minghang Yu,Chunyan Xu,Zhen Cui*

Main category: cs.CV

TL;DR: 该论文提出了一个新方法（SDD），通过重构学习以对齐图像伪造检测中的伪造空间和语义空间，表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成技术的快速发展，鲁棒的伪造检测变得尤为重要，以确保数字媒体的可信性。然而，伪造特征和语义概念空间的不对齐依然是一个挑战。

Method: 设计了一个语义离散感知检测器（SDD），通过视觉重构范式，引入语义Token采样模块缓解空间偏移，并通过概念级伪造离散学习模块加强语义概念与伪造痕迹间的交互，最终结合低层次伪造特征增强器整合概念级伪造离散信息。

Result: 在两个标准的图像伪造数据集上的实验表明，该方法优于现有技术。

Conclusion: SDD能够有效对齐伪造痕迹与语义空间，通过捕捉两者间的离散性显著提高伪造检测的准确性，代码已公开。

Abstract: With the rapid advancement of image generation techniques, robust forgery
detection has become increasingly imperative to ensure the trustworthiness of
digital media. Recent research indicates that the learned semantic concepts of
pre-trained models are critical for identifying fake images. However, the
misalignment between the forgery and semantic concept spaces hinders the
model's forgery detection performance. To address this problem, we propose a
novel Semantic Discrepancy-aware Detector (SDD) that leverages reconstruction
learning to align the two spaces at a fine-grained visual level. By exploiting
the conceptual knowledge embedded in the pre-trained vision language model, we
specifically design a semantic token sampling module to mitigate the space
shifts caused by features irrelevant to both forgery traces and semantic
concepts. A concept-level forgery discrepancy learning module, built upon a
visual reconstruction paradigm, is proposed to strengthen the interaction
between visual semantic concepts and forgery traces, effectively capturing
discrepancies under the concepts' guidance. Finally, the low-level forgery
feature enhancemer integrates the learned concept level forgery discrepancies
to minimize redundant forgery information. Experiments conducted on two
standard image forgery datasets demonstrate the efficacy of the proposed SDD,
which achieves superior results compared to existing methods. The code is
available at https://github.com/wzy1111111/SSD.

</details>


### [62] [AquaFeat: A Features-Based Image Enhancement Model for Underwater Object Detection](https://arxiv.org/abs/2508.12343)
*Emanuel C. Silva,Tatiana T. Schein,Stephanie L. Brião,Guilherme L. M. Costa,Felipe G. Oliveira,Gustavo P. Almeida,Eduardo L. Silva,Sam S. Devincenzi,Karina S. Machado,Paulo L. J. Drews-Jr*

Main category: cs.CV

TL;DR: AquaFeat 是一种新型的任务驱动特征增强模块，能优化水下目标检测性能并提升检测精度，同时保持高效计算速度。


<details>
  <summary>Details</summary>
Motivation: 传统图像增强方法难以优化水下图像的目标检测，因此提出一种能够直接改善检测相关特征的增强方法。

Method: 设计并整合一个多尺度特征增强网络，与检测器的损失函数端到端训练引导特征增强，从而提升检测效果。

Result: AquaFeat 在水下数据集上与 YOLOv8m 结合，达到了 0.877 的 Precision 和 0.624 的 Recall，同时具有 mAP@0.5 为 0.677，mAP@[0.5:0.95] 为 0.421 的竞争性表现，且处理速度为 46.5 FPS。

Conclusion: AquaFeat 提供了一种精度高、速率快的解决方案，适合水下监测和基础设施检测等应用场景。

Abstract: The severe image degradation in underwater environments impairs object
detection models, as traditional image enhancement methods are often not
optimized for such downstream tasks. To address this, we propose AquaFeat, a
novel, plug-and-play module that performs task-driven feature enhancement. Our
approach integrates a multi-scale feature enhancement network trained
end-to-end with the detector's loss function, ensuring the enhancement process
is explicitly guided to refine features most relevant to the detection task.
When integrated with YOLOv8m on challenging underwater datasets, AquaFeat
achieves state-of-the-art Precision (0.877) and Recall (0.624), along with
competitive mAP scores (mAP@0.5 of 0.677 and mAP@[0.5:0.95] of 0.421). By
delivering these accuracy gains while maintaining a practical processing speed
of 46.5 FPS, our model provides an effective and computationally efficient
solution for real-world applications, such as marine ecosystem monitoring and
infrastructure inspection.

</details>


### [63] [MBMamba: When Memory Buffer Meets Mamba for Structure-Aware Image Deblurring](https://arxiv.org/abs/2508.12346)
*Hu Gao,Depeng Dang*

Main category: cs.CV

TL;DR: 提出了一种基于Mamba架构的图像去模糊网络，通过记忆缓冲机制和类伊辛正则化损失改进了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有Mamba架构在图像去模糊中存在像素遗忘和通道冗余问题，影响了2D空间信息的聚合，同时提高复杂性限制了实时性能。

Method: 设计了记忆缓冲机制以融合历史信息，并提出类伊辛正则化损失以维持图像结构，通过此方法开发了MBMamba网络。

Result: 实验结果表明，所提出的MBMamba在常用基准测试上优于最先进的方法。

Conclusion: 通过在Mamba架构中添加记忆缓冲和新型正则策略，该方法有效解决了信息遗失和结构保持问题，提升了性能，同时不增加复杂性。

Abstract: The Mamba architecture has emerged as a promising alternative to CNNs and
Transformers for image deblurring. However, its flatten-and-scan strategy often
results in local pixel forgetting and channel redundancy, limiting its ability
to effectively aggregate 2D spatial information. Although existing methods
mitigate this by modifying the scan strategy or incorporating local feature
modules, it increase computational complexity and hinder real-time performance.
In this paper, we propose a structure-aware image deblurring network without
changing the original Mamba architecture. Specifically, we design a memory
buffer mechanism to preserve historical information for later fusion, enabling
reliable modeling of relevance between adjacent features. Additionally, we
introduce an Ising-inspired regularization loss that simulates the energy
minimization of the physical system's "mutual attraction" between pixels,
helping to maintain image structure and coherence. Building on this, we develop
MBMamba. Experimental results show that our method outperforms state-of-the-art
approaches on widely used benchmarks.

</details>


### [64] [EgoLoc: A Generalizable Solution for Temporal Interaction Localization in Egocentric Videos](https://arxiv.org/abs/2508.12349)
*Junyi Ma,Erhang Zhang,Yin-Dong Zheng,Yuchen Xie,Yixuan Zhou,Hesheng Wang*

Main category: cs.CV

TL;DR: 提出了一种名为EgoLoc的新方法，用于在第一人称视频中零样本情况下定位手-物接触和分离时间戳，旨在提升跨混合现实和机器人操作任务中的交互体验。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中于交互动作的行为模式建模，对于手与目标物交互的关键时刻捕捉（即“何时交互”）这一更细粒度的问题研究较少，而这一点对沉浸式交互体验与机器人运动规划至关重要。

Method: 提出了名为EgoLoc的方法，它通过手动态引导采样生成高质量的视觉提示，利用视觉-语言模型识别接触/分离属性并定位时间戳，同时提供闭环反馈进行进一步优化。EgoLoc不依赖于物体掩码及动作类别标注，实现了具有普遍性的零样本实施。

Result: 通过综合的实验表明，EgoLoc能够在第一人称视频中实现可信的时间交互定位（TIL），并有效促进多个下游应用，包括第一人称视觉和机器人操作任务。

Conclusion: EgoLoc方法消除了对对象掩码和动作类别标注的依赖，其在开放数据集和新基准测试上的表现验证了其广泛适用性和可靠性。相关代码和数据将在GitHub上发布。

Abstract: Analyzing hand-object interaction in egocentric vision facilitates VR/AR
applications and human-robot policy transfer. Existing research has mostly
focused on modeling the behavior paradigm of interactive actions (i.e., ``how
to interact''). However, the more challenging and fine-grained problem of
capturing the critical moments of contact and separation between the hand and
the target object (i.e., ``when to interact'') is still underexplored, which is
crucial for immersive interactive experiences in mixed reality and robotic
motion planning. Therefore, we formulate this problem as temporal interaction
localization (TIL). Some recent works extract semantic masks as TIL references,
but suffer from inaccurate object grounding and cluttered scenarios. Although
current temporal action localization (TAL) methods perform well in detecting
verb-noun action segments, they rely on category annotations during training
and exhibit limited precision in localizing hand-object contact/separation
moments. To address these issues, we propose a novel zero-shot approach dubbed
EgoLoc to localize hand-object contact and separation timestamps in egocentric
videos. EgoLoc introduces hand-dynamics-guided sampling to generate
high-quality visual prompts. It exploits the vision-language model to identify
contact/separation attributes, localize specific timestamps, and provide
closed-loop feedback for further refinement. EgoLoc eliminates the need for
object masks and verb-noun taxonomies, leading to generalizable zero-shot
implementation. Comprehensive experiments on the public dataset and our novel
benchmarks demonstrate that EgoLoc achieves plausible TIL for egocentric
videos. It is also validated to effectively facilitate multiple downstream
applications in egocentric vision and robotic manipulation tasks. Code and
relevant data will be released at https://github.com/IRMVLab/EgoLoc.

</details>


### [65] [Synthetic Data is Sufficient for Zero-Shot Visual Generalization from Offline Data](https://arxiv.org/abs/2508.12356)
*Ahmet H. Güzel,Ilija Bogunovic,Jack Parker-Holder*

Main category: cs.CV

TL;DR: 通过引入数据多样性并利用扩散模型生成潜在空间中的额外数据，提升基于视觉的离线强化学习的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中的政策在有限的离线数据中难以泛化，尤其是当处理复杂视觉数据时，挑战更为显著。

Method: 采用两步法：首先通过增广原始离线数据提升零样本泛化能力；其次，使用扩散模型在潜在空间生成额外的数据。

Result: 在连续和离散动作空间测试中，该方法在无需更改现有算法的情况下显著提升了泛化表现，并减少了测试时的泛化误差。

Conclusion: 该方法通过多样化训练数据，同时保持计算效率，有望推动合成数据生成领域的发展，助力训练更具泛化能力的智能体。

Abstract: Offline reinforcement learning (RL) offers a promising framework for training
agents using pre-collected datasets without the need for further environment
interaction. However, policies trained on offline data often struggle to
generalise due to limited exposure to diverse states. The complexity of visual
data introduces additional challenges such as noise, distractions, and spurious
correlations, which can misguide the policy and increase the risk of
overfitting if the training data is not sufficiently diverse. Indeed, this
makes it challenging to leverage vision-based offline data in training robust
agents that can generalize to unseen environments. To solve this problem, we
propose a simple approach generating additional synthetic training data. We
propose a two-step process, first augmenting the originally collected offline
data to improve zero-shot generalization by introducing diversity, then using a
diffusion model to generate additional data in latent space. We test our method
across both continuous action spaces (Visual D4RL) and discrete action spaces
(Procgen), demonstrating that it significantly improves generalization without
requiring any algorithmic changes to existing model-free offline RL methods. We
show that our method not only increases the diversity of the training data but
also significantly reduces the generalization gap at test time while
maintaining computational efficiency. We believe this approach could fuel
additional progress in generating synthetic data to train more general agents
in the future.

</details>


### [66] [IPGPhormer: Interpretable Pathology Graph-Transformer for Survival Analysis](https://arxiv.org/abs/2508.12381)
*Guo Tang,Songhan Jiang,Jinpeng Lu,Linghan Cai,Yongbing Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为IPGPhormer的新框架，用于癌症预后，专注于整合空间依赖建模与解释性分析，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长程空间关系与局部上下文依赖建模中表现欠佳，且缺乏解释性，限制了临床应用。

Method: 提出了IPGPhormer，通过融合图-Transformer架构，捕获肿瘤微环境特性及其空间依赖关系，无需人工注释，直接实现组织和细胞级别的解释性分析。

Result: 在四个公开数据集上的评估显示，IPGPhormer在预测准确性和解释性方面均优于最新技术方法。

Conclusion: IPGPhormer是癌症预后评估的有力工具，为高可靠性和高解释性的病理决策支持系统奠定了基础。

Abstract: Pathological images play an essential role in cancer prognosis, while
survival analysis, which integrates computational techniques, can predict
critical clinical events such as patient mortality or disease recurrence from
whole-slide images (WSIs). Recent advancements in multiple instance learning
have significantly improved the efficiency of survival analysis. However,
existing methods often struggle to balance the modeling of long-range spatial
relationships with local contextual dependencies and typically lack inherent
interpretability, limiting their clinical utility. To address these challenges,
we propose the Interpretable Pathology Graph-Transformer (IPGPhormer), a novel
framework that captures the characteristics of the tumor microenvironment and
models their spatial dependencies across the tissue. IPGPhormer uniquely
provides interpretability at both tissue and cellular levels without requiring
post-hoc manual annotations, enabling detailed analyses of individual WSIs and
cross-cohort assessments. Comprehensive evaluations on four public benchmark
datasets demonstrate that IPGPhormer outperforms state-of-the-art methods in
both predictive accuracy and interpretability. In summary, our method,
IPGPhormer, offers a promising tool for cancer prognosis assessment, paving the
way for more reliable and interpretable decision-support systems in pathology.
The code is publicly available at
https://anonymous.4open.science/r/IPGPhormer-6EEB.

</details>


### [67] [ViT-EnsembleAttack: Augmenting Ensemble Models for Stronger Adversarial Transferability in Vision Transformers](https://arxiv.org/abs/2508.12384)
*Hanwen Cao,Haobo Lu,Xiaosen Wang,Kun He*

Main category: cs.CV

TL;DR: 该论文提出了一种名为ViT-EnsembleAttack的新方法，通过对Vision Transformers (ViTs)实施对抗性增强生成对抗攻击实例，有效提升了对抗样本在ViTs中的迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要集中于优化集成权重或路径，而忽略了探索如何通过增强集成模型的泛化能力来提升对抗样本的迁移性。作者试图弥补这一研究空白。

Method: 提出对抗增强策略用于生成增强的ViT模型，包括多头丢弃、注意力分数缩放以及MLP特征混合，同时通过贝叶斯优化优化相关参数。此外，引入自动重加权和步长扩展模块进一步提升迁移性。

Result: 实验结果表明，与现有方法相比，ViT-EnsembleAttack在大幅提升ViTs的对抗迁移性方面表现优异。

Conclusion: ViT-EnsembleAttack通过对中介模型进行对抗性增强以及采用优化策略，成功提升了在ViTs中的对抗迁移性，为集成攻击方法的研究提供了新的视角。

Abstract: Ensemble-based attacks have been proven to be effective in enhancing
adversarial transferability by aggregating the outputs of models with various
architectures. However, existing research primarily focuses on refining
ensemble weights or optimizing the ensemble path, overlooking the exploration
of ensemble models to enhance the transferability of adversarial attacks. To
address this gap, we propose applying adversarial augmentation to the surrogate
models, aiming to boost overall generalization of ensemble models and reduce
the risk of adversarial overfitting. Meanwhile, observing that ensemble Vision
Transformers (ViTs) gain less attention, we propose ViT-EnsembleAttack based on
the idea of model adversarial augmentation, the first ensemble-based attack
method tailored for ViTs to the best of our knowledge. Our approach generates
augmented models for each surrogate ViT using three strategies: Multi-head
dropping, Attention score scaling, and MLP feature mixing, with the associated
parameters optimized by Bayesian optimization. These adversarially augmented
models are ensembled to generate adversarial examples. Furthermore, we
introduce Automatic Reweighting and Step Size Enlargement modules to boost
transferability. Extensive experiments demonstrate that ViT-EnsembleAttack
significantly enhances the adversarial transferability of ensemble-based
attacks on ViTs, outperforming existing methods by a substantial margin. Code
is available at https://github.com/Trustworthy-AI-Group/TransferAttack.

</details>


### [68] [DeCoT: Decomposing Complex Instructions for Enhanced Text-to-Image Generation with Large Language Models](https://arxiv.org/abs/2508.12396)
*Xiaochuan Lin,Xiangyong Chen,Xuan Li,Yichen Su*

Main category: cs.CV

TL;DR: DeCoT框架通过分解复杂指令与语义优化，加强T2I模型性能，对复杂文本生成表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像（T2I）模型在处理复杂、长文本指令时存在不足，难以准确呈现复杂细节和约束关系，而这在基准测试中表现尤为明显。

Method: 引入DeCoT框架，分两阶段操作：1）利用大型语言模型将复杂指令分解为可执行的语义单元，并提升语义清晰度；2）将这些语义单元整合为分层或优化的单一提示词，适配现有T2I模型。

Result: DeCoT在LongBench-T2I数据集上的实验显示，在文本与构图等具有挑战性的维度上显著提升性能，集成于Infinity-8B模型后取得3.52的分数，优于基线3.44，人类评估也验证了结果优越性。

Conclusion: DeCoT框架成功弥合了用户意图与T2I模型需求之间的差距，实现了更高保真度和准确度的图像生成。

Abstract: Despite remarkable advancements, current Text-to-Image (T2I) models struggle
with complex, long-form textual instructions, frequently failing to accurately
render intricate details, spatial relationships, or specific constraints. This
limitation is highlighted by benchmarks such as LongBench-T2I, which reveal
deficiencies in handling composition, specific text, and fine textures. To
address this, we propose DeCoT (Decomposition-CoT), a novel framework that
leverages Large Language Models (LLMs) to significantly enhance T2I models'
understanding and execution of complex instructions. DeCoT operates in two core
stages: first, Complex Instruction Decomposition and Semantic Enhancement,
where an LLM breaks down raw instructions into structured, actionable semantic
units and clarifies ambiguities; second, Multi-Stage Prompt Integration and
Adaptive Generation, which transforms these units into a hierarchical or
optimized single prompt tailored for existing T2I models. Extensive experiments
on the LongBench-T2I dataset demonstrate that DeCoT consistently and
substantially improves the performance of leading T2I models across all
evaluated dimensions, particularly in challenging aspects like "Text" and
"Composition". Quantitative results, validated by multiple MLLM evaluators
(Gemini-2.0-Flash and InternVL3-78B), show that DeCoT, when integrated with
Infinity-8B, achieves an average score of 3.52, outperforming the baseline
Infinity-8B (3.44). Ablation studies confirm the critical contribution of each
DeCoT component and the importance of sophisticated LLM prompting. Furthermore,
human evaluations corroborate these findings, indicating superior perceptual
quality and instruction fidelity. DeCoT effectively bridges the gap between
high-level user intent and T2I model requirements, leading to more faithful and
accurate image generation.

</details>


### [69] [Federated Cross-Modal Style-Aware Prompt Generation](https://arxiv.org/abs/2508.12399)
*Suraj Prasad,Navyansh Mahla,Sunny Gupta,Amit Sethi*

Main category: cs.CV

TL;DR: 提出了FedCSAP（Federated Cross-Modal Style-Aware Prompt Generation），通过结合CLIP的多层次特征和客户特有的风格指示器，增强了联邦学习中的提示生成能力，提高了准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有联邦学习方法中仅依赖最终层特征而忽略多尺度视觉信息和领域风格差异的问题，提高视觉-语言模型在数据隐私保护下的泛化性能。

Method: 利用CLIP视觉编码器的多层次（低、中、高）特征和基于批次统计的客户端特定风格信息，生成上下文感知的提示Token，同时通过联邦学习框架实现隐私保护的本地训练与全局聚合。

Result: 在多种图像分类数据集上的实验表明，FedCSAP在准确性和泛化能力上优于现有联邦提示学习方法。

Conclusion: FedCSAP通过结合视觉细节与文本上下文，在联邦学习中实现了有效的提示生成方法，有望成为解决非IID分布问题的有效工具。

Abstract: Prompt learning has propelled vision-language models like CLIP to excel in
diverse tasks, making them ideal for federated learning due to computational
efficiency. However, conventional approaches that rely solely on final-layer
features miss out on rich multi-scale visual cues and domain-specific style
variations in decentralized client data. To bridge this gap, we introduce
FedCSAP (Federated Cross-Modal Style-Aware Prompt Generation). Our framework
harnesses low, mid, and high-level features from CLIP's vision encoder
alongside client-specific style indicators derived from batch-level statistics.
By merging intricate visual details with textual context, FedCSAP produces
robust, context-aware prompt tokens that are both distinct and non-redundant,
thereby boosting generalization across seen and unseen classes. Operating
within a federated learning paradigm, our approach ensures data privacy through
local training and global aggregation, adeptly handling non-IID class
distributions and diverse domain-specific styles. Comprehensive experiments on
multiple image classification datasets confirm that FedCSAP outperforms
existing federated prompt learning methods in both accuracy and overall
generalization.

</details>


### [70] [MPCAR: Multi-Perspective Contextual Augmentation for Enhanced Visual Reasoning in Large Vision-Language Models](https://arxiv.org/abs/2508.12400)
*Amirul Rahman,Qiang Xu,Xueying Huang*

Main category: cs.CV

TL;DR: 本文提出了一种新的推理策略MPCAR，通过多视角生成和上下文增强提示来提升大型视觉语言模型（LVLM）的性能，在无需调整模型参数的情况下显著提高了视觉问答任务的表现。


<details>
  <summary>Details</summary>
Motivation: LVLM在处理复杂视觉推理任务中表现较弱，难以充分捕捉深度上下文、多角度分析或细致的细节识别。本研究旨在通过开发一个有效的推理策略来提升此类模型的性能。

Method: 提出MPCAR推理策略，分为三阶段：生成多样化描述；将其与问题整合构建上下文增强提示；利用此提示进行模型推理。该方法不需要对LVLM进行参数微调。

Result: 在多个视觉问答数据集（如GQA, VQA-CP v2, ScienceQA）上表现优异。实验证明MPCAR在需要强上下文理解的任务中显著提高了准确性；人类评估显示输出更连贯、更完整；消融实验验证了提示模板多样性与生成视角数量的重要性。

Conclusion: 通过利用LVLM的生成能力进行上下文增强，可以有效激活其潜在推理能力，为复杂多模态任务提供更强的解决方案。

Abstract: Despite significant advancements, Large Vision-Language Models (LVLMs)
continue to face challenges in complex visual reasoning tasks that demand deep
contextual understanding, multi-angle analysis, or meticulous detail
recognition. Existing approaches often rely on single-shot image encoding and
prompts, limiting their ability to fully capture nuanced visual information.
Inspired by the notion that strategically generated "additional" information
can serve as beneficial contextual augmentation, we propose Multi-Perspective
Contextual Augmentation for Reasoning (MPCAR), a novel inference-time strategy
designed to enhance LVLM performance. MPCAR operates in three stages: first, an
LVLM generates N diverse and complementary descriptions or preliminary
reasoning paths from various angles; second, these descriptions are
intelligently integrated with the original question to construct a
comprehensive context-augmented prompt; and finally, this enriched prompt
guides the ultimate LVLM for deep reasoning and final answer generation.
Crucially, MPCAR achieves these enhancements without requiring any fine-tuning
of the underlying LVLM's parameters. Extensive experiments on challenging
Visual Question Answering (VQA) datasets, including GQA, VQA-CP v2, and
ScienceQA (Image-VQA), demonstrate that MPCAR consistently outperforms
established baseline methods. Our quantitative results show significant
accuracy gains, particularly on tasks requiring robust contextual
understanding, while human evaluations confirm improved coherence and
completeness of the generated answers. Ablation studies further highlight the
importance of diverse prompt templates and the number of generated
perspectives. This work underscores the efficacy of leveraging LVLMs' inherent
generative capabilities to enrich input contexts, thereby unlocking their
latent reasoning potential for complex multimodal tasks.

</details>


### [71] [LMAD: Integrated End-to-End Vision-Language Model for Explainable Autonomous Driving](https://arxiv.org/abs/2508.12404)
*Nan Song,Bozhou Zhang,Xiatian Zhu,Jiankang Deng,Li Zhang*

Main category: cs.CV

TL;DR: LMAD 是一个专为自动驾驶设计的新型视觉语言框架，显著提升了驾驶推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有 VLM 方法在处理复杂自动驾驶场景时欠缺全面的场景识别和强大的空间感知能力。

Method: 提出 LMAD 框架，该框架结合全面的场景理解和任务专用结构，与现有 VLM 完全兼容，并引入场景交互和专家适配器。

Result: 在 DriveLM 和 nuScenes-QA 数据集上的实验表明，LMAD 明显提升了 VLM 在驾驶推理任务中的表现。

Conclusion: LMAD 为可解释的自动驾驶设立了新的标准，同时兼容现有视觉语言模型，具有较强的实际适用性。

Abstract: Large vision-language models (VLMs) have shown promising capabilities in
scene understanding, enhancing the explainability of driving behaviors and
interactivity with users. Existing methods primarily fine-tune VLMs on on-board
multi-view images and scene reasoning text, but this approach often lacks the
holistic and nuanced scene recognition and powerful spatial awareness required
for autonomous driving, especially in complex situations. To address this gap,
we propose a novel vision-language framework tailored for autonomous driving,
called LMAD. Our framework emulates modern end-to-end driving paradigms by
incorporating comprehensive scene understanding and a task-specialized
structure with VLMs. In particular, we introduce preliminary scene interaction
and specialized expert adapters within the same driving task structure, which
better align VLMs with autonomous driving scenarios. Furthermore, our approach
is designed to be fully compatible with existing VLMs while seamlessly
integrating with planning-oriented driving systems. Extensive experiments on
the DriveLM and nuScenes-QA datasets demonstrate that LMAD significantly boosts
the performance of existing VLMs on driving reasoning tasks,setting a new
standard in explainable autonomous driving.

</details>


### [72] [S5: Scalable Semi-Supervised Semantic Segmentation in Remote Sensing](https://arxiv.org/abs/2508.12409)
*Liang Lv,Di Wang,Jing Zhang,Lefei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为S5的可扩展遥感半监督语义分割框架，解决了使用小规模数据集和模型的局限性，通过引入数据选择策略和大规模预训练显著提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有的遥感半监督语义分割研究受限于小规模数据集，无法充分利用地球观测中大量未标记数据，需要一种可扩展的方法来提升性能和适用范围。

Method: S5框架通过数据选择策略（熵过滤和多样性扩展）创建大规模数据集RS4P-1M，利用该数据集预训练不同规模的遥感基础模型（RSFMs），并采用混合专家的多数据集微调方法以提高泛化和适应能力。

Result: 通过大规模预训练和多数据集微调方法，S5框架显著提升了RSFMs在土地覆盖分割和目标检测任务中的性能，并达到了所有基准的先进水平。

Conclusion: S5框架证明了扩展半监督学习在遥感应用中的可行性，并通过开放数据集、代码及模型为进一步研究提供支持。

Abstract: Semi-supervised semantic segmentation (S4) has advanced remote sensing (RS)
analysis by leveraging unlabeled data through pseudo-labeling and consistency
learning. However, existing S4 studies often rely on small-scale datasets and
models, limiting their practical applicability. To address this, we propose S5,
the first scalable framework for semi-supervised semantic segmentation in RS,
which unlocks the potential of vast unlabeled Earth observation data typically
underutilized due to costly pixel-level annotations. Built upon existing
large-scale RS datasets, S5 introduces a data selection strategy that
integrates entropy-based filtering and diversity expansion, resulting in the
RS4P-1M dataset. Using this dataset, we systematically scales S4 methods by
pre-training RS foundation models (RSFMs) of varying sizes on this extensive
corpus, significantly boosting their performance on land cover segmentation and
object detection tasks. Furthermore, during fine-tuning, we incorporate a
Mixture-of-Experts (MoE)-based multi-dataset fine-tuning approach, which
enables efficient adaptation to multiple RS benchmarks with fewer parameters.
This approach improves the generalization and versatility of RSFMs across
diverse RS benchmarks. The resulting RSFMs achieve state-of-the-art performance
across all benchmarks, underscoring the viability of scaling semi-supervised
learning for RS applications. All datasets, code, and models will be released
at https://github.com/MiliLab/S5

</details>


### [73] [SRMA-Mamba: Spatial Reverse Mamba Attention Network for Pathological Liver Segmentation in MRI Volumes](https://arxiv.org/abs/2508.12410)
*Jun Zeng,Yannan Huang,Elif Keles,Halil Ertugrul Aktas,Gorkem Durak,Nikhil Kumar Tomar,Quoc-Huy Trinh,Deepak Ranjan Nayak,Ulas Bagci,Debesh Jha*

Main category: cs.CV

TL;DR: 论文提出了一种名为SRMA-Mamba的新方法，用于在MRI数据中高效识别肝硬化病变，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分利用MRI数据中的空间解剖细节，限制了临床效果和解释性，因此需要改进。

Method: 设计了一种SRMA-Mamba网络，结合了空间解剖模块（SABMamba）和空间反向注意模块（SRMA），以增强肝硬化区域的分割效果。

Result: 实验表明，SRMA-Mamba在3D病理肝脏分割任务中性能优越，超越了现有的方法。

Conclusion: SRMA-Mamba通过创新的解剖信息整合和分割细化模块，实现了对肝硬化区域的高效识别，具有重要的临床应用潜力。

Abstract: Liver Cirrhosis plays a critical role in the prognosis of chronic liver
disease. Early detection and timely intervention are critical in significantly
reducing mortality rates. However, the intricate anatomical architecture and
diverse pathological changes of liver tissue complicate the accurate detection
and characterization of lesions in clinical settings. Existing methods
underutilize the spatial anatomical details in volumetric MRI data, thereby
hindering their clinical effectiveness and explainability. To address this
challenge, we introduce a novel Mamba-based network, SRMA-Mamba, designed to
model the spatial relationships within the complex anatomical structures of MRI
volumes. By integrating the Spatial Anatomy-Based Mamba module (SABMamba),
SRMA-Mamba performs selective Mamba scans within liver cirrhotic tissues and
combines anatomical information from the sagittal, coronal, and axial planes to
construct a global spatial context representation, enabling efficient
volumetric segmentation of pathological liver structures. Furthermore, we
introduce the Spatial Reverse Attention module (SRMA), designed to
progressively refine cirrhotic details in the segmentation map, utilizing both
the coarse segmentation map and hierarchical encoding features. Extensive
experiments demonstrate that SRMA-Mamba surpasses state-of-the-art methods,
delivering exceptional performance in 3D pathological liver segmentation. Our
code is available for public:
{\color{blue}{https://github.com/JunZengz/SRMA-Mamba}}.

</details>


### [74] [TiP4GEN: Text to Immersive Panorama 4D Scene Generation](https://arxiv.org/abs/2508.12415)
*Ke Xing,Hanwen Liang,Dejia Xu,Yuyang Yin,Konstantinos N. Plataniotis,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: 提出了一个名为TiP4GEN的框架，用于从文本生成动态全景场景，同时保证细节控制和几何一致性效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要集中在静态场景或狭窄视角动态场景的生成，缺乏360度沉浸体验的生成能力。

Method: 提出了一个包含双分支生成模式（全景分支和透视分支）以及基于3D高斯点分布的几何对齐重建模型，确保生成的动态场景具有几何一致性和时间连贯性。

Result: 实验结果表明，该方法在生成具有视觉吸引力和运动连贯性的动态全景场景方面性能优越。

Conclusion: TiP4GEN显著提升了动态全景生成能力，为沉浸式虚拟环境的创建提供了有效的解决方案。

Abstract: With the rapid advancement and widespread adoption of VR/AR technologies,
there is a growing demand for the creation of high-quality, immersive dynamic
scenes. However, existing generation works predominantly concentrate on the
creation of static scenes or narrow perspective-view dynamic scenes, falling
short of delivering a truly 360-degree immersive experience from any viewpoint.
In this paper, we introduce \textbf{TiP4GEN}, an advanced text-to-dynamic
panorama scene generation framework that enables fine-grained content control
and synthesizes motion-rich, geometry-consistent panoramic 4D scenes. TiP4GEN
integrates panorama video generation and dynamic scene reconstruction to create
360-degree immersive virtual environments. For video generation, we introduce a
\textbf{Dual-branch Generation Model} consisting of a panorama branch and a
perspective branch, responsible for global and local view generation,
respectively. A bidirectional cross-attention mechanism facilitates
comprehensive information exchange between the branches. For scene
reconstruction, we propose a \textbf{Geometry-aligned Reconstruction Model}
based on 3D Gaussian Splatting. By aligning spatial-temporal point clouds using
metric depth maps and initializing scene cameras with estimated poses, our
method ensures geometric consistency and temporal coherence for the
reconstructed scenes. Extensive experiments demonstrate the effectiveness of
our proposed designs and the superiority of TiP4GEN in generating visually
compelling and motion-coherent dynamic panoramic scenes. Our project page is at
https://ke-xing.github.io/TiP4GEN/.

</details>


### [75] [Illusions in Humans and AI: How Visual Perception Aligns and Diverges](https://arxiv.org/abs/2508.12422)
*Jianyi Yang,Junyi Ye,Ankan Dash,Guiling Wang*

Main category: cs.CV

TL;DR: 本文比较了人类与人工智能在视觉幻觉方面的差异，揭示了人工智能感知中的一些独特漏洞和问题。


<details>
  <summary>Details</summary>
Motivation: 探讨生物与人工感知在视觉幻觉构建中的差异，意在改进人工智能视觉系统的鲁棒性、人类可解释性和一致性。

Method: 通过研究人工视觉系统对经典视觉幻觉（涉及颜色、大小、形状和运动等）的反应，并分析其独特的幻觉现象如像素级敏感度和幻觉等。

Result: 发现人工智能可通过特定训练或模式识别过程产生类似幻觉的效果，同时揭示了其感知中存在与人类不同的漏洞。

Conclusion: 系统比较人类与人工智能对视觉幻觉的反应，识别出感知差异，为开发更符合人类需求且安全的视觉系统提供了重要见解。

Abstract: By comparing biological and artificial perception through the lens of
illusions, we highlight critical differences in how each system constructs
visual reality. Understanding these divergences can inform the development of
more robust, interpretable, and human-aligned artificial intelligence (AI)
vision systems. In particular, visual illusions expose how human perception is
based on contextual assumptions rather than raw sensory data. As artificial
vision systems increasingly perform human-like tasks, it is important to ask:
does AI experience illusions, too? Does it have unique illusions? This article
explores how AI responds to classic visual illusions that involve color, size,
shape, and motion. We find that some illusion-like effects can emerge in these
models, either through targeted training or as by-products of pattern
recognition. In contrast, we also identify illusions unique to AI, such as
pixel-level sensitivity and hallucinations, that lack human counterparts. By
systematically comparing human and AI responses to visual illusions, we uncover
alignment gaps and AI-specific perceptual vulnerabilities invisible to human
perception. These findings provide insights for future research on vision
systems that preserve human-beneficial perceptual biases while avoiding
distortions that undermine trust and safety.

</details>


### [76] [Adversarial Attacks on VQA-NLE: Exposing and Alleviating Inconsistencies in Visual Question Answering Explanations](https://arxiv.org/abs/2508.12430)
*Yahsin Yeh,Yilun Wu,Bokai Ruan,Honghan Shuai*

Main category: cs.CV

TL;DR: 研究揭示了现有视觉问答自然语言解释（VQA-NLE）系统中的漏洞，并提出了改进方法。


<details>
  <summary>Details</summary>
Motivation: 通过开发透明的解释机制来改进VQA-NLE系统，发现其存在一致性及推理能力的不足。

Method: 使用现有的对抗策略扰动问题，并提出一种新方法通过最小程度修改图像产生矛盾或虚假结果。此外，利用外部知识来减轻这些不一致性以增强模型鲁棒性。

Result: 在两个标准基准和两个常用的VQA-NLE模型上进行了广泛评估，验证了攻击策略的有效性以及基于知识的防御潜力。

Conclusion: 当前VQA-NLE系统在安全性和可靠性方面存在迫切问题，提出了知识驱动的改进方向。

Abstract: Natural language explanations in visual question answering (VQA-NLE) aim to
make black-box models more transparent by elucidating their decision-making
processes. However, we find that existing VQA-NLE systems can produce
inconsistent explanations and reach conclusions without genuinely understanding
the underlying context, exposing weaknesses in either their inference pipeline
or explanation-generation mechanism. To highlight these vulnerabilities, we not
only leverage an existing adversarial strategy to perturb questions but also
propose a novel strategy that minimally alters images to induce contradictory
or spurious outputs. We further introduce a mitigation method that leverages
external knowledge to alleviate these inconsistencies, thereby bolstering model
robustness. Extensive evaluations on two standard benchmarks and two widely
used VQA-NLE models underscore the effectiveness of our attacks and the
potential of knowledge-based defenses, ultimately revealing pressing security
and reliability concerns in current VQA-NLE systems.

</details>


### [77] [X-Ray-CoT: Interpretable Chest X-ray Diagnosis with Vision-Language Models via Chain-of-Thought Reasoning](https://arxiv.org/abs/2508.12455)
*Chee Ng,Liliang Sun,Shaoqing Tang*

Main category: cs.CV

TL;DR: 提出了X-Ray-CoT框架，通过多模态特征提取和LLM的链式推理策略，实现胸部X光诊断和可解释报告生成，在准确性和解释性上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型虽然诊断精度高，但因其黑箱特性限制了在高风险医疗环境中的使用，本研究旨在开发一种既高效又具有可解释性的诊断工具。

Method: 提出X-Ray-CoT，利用视觉语言大模型，结合链式推理策略，从多模态特征中模拟放射科医生的思维过程生成诊断报告。

Result: 在CORDA数据集上表现优异，疾病诊断平衡精度达到80.52%，F1分数为78.65%，且能生成高质量的可解释诊断报告。

Conclusion: 实现了一种可信的、在医学影像中具有临床可操作性的AI系统，对多模态融合和链式推理的必要性进行了验证。

Abstract: Chest X-ray imaging is crucial for diagnosing pulmonary and cardiac diseases,
yet its interpretation demands extensive clinical experience and suffers from
inter-observer variability. While deep learning models offer high diagnostic
accuracy, their black-box nature hinders clinical adoption in high-stakes
medical settings. To address this, we propose X-Ray-CoT (Chest X-Ray
Chain-of-Thought), a novel framework leveraging Vision-Language Large Models
(LVLMs) for intelligent chest X-ray diagnosis and interpretable report
generation. X-Ray-CoT simulates human radiologists' "chain-of-thought" by first
extracting multi-modal features and visual concepts, then employing an
LLM-based component with a structured Chain-of-Thought prompting strategy to
reason and produce detailed natural language diagnostic reports. Evaluated on
the CORDA dataset, X-Ray-CoT achieves competitive quantitative performance,
with a Balanced Accuracy of 80.52% and F1 score of 78.65% for disease
diagnosis, slightly surpassing existing black-box models. Crucially, it
uniquely generates high-quality, explainable reports, as validated by
preliminary human evaluations. Our ablation studies confirm the integral role
of each proposed component, highlighting the necessity of multi-modal fusion
and CoT reasoning for robust and transparent medical AI. This work represents a
significant step towards trustworthy and clinically actionable AI systems in
medical imaging.

</details>


### [78] [Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision Mapping](https://arxiv.org/abs/2508.12466)
*Xuhui Zhan,Tyler Derr*

Main category: cs.CV

TL;DR: Inverse-LLaVA以一种创新的方法颠覆了传统的视觉-语言对齐预训练需求，通过将文本嵌入映射到连续视觉表示空间，省略了对齐预训练步骤。


<details>
  <summary>Details</summary>
Motivation: 现有多模态学习方法依赖昂贵的视觉-语言对齐预训练，本研究提出通过新方法减少计算复杂性，同时提升复杂推理任务能力。

Method: 省略对齐预训练，通过在转换器中间层结合视觉与文本信息，并利用注意力机制中的选择性加性组件静态整合视觉与文本表示。

Result: 在9个多模态基准中表现优异，特别是在需要复杂推理的任务中显著提升，例如认知推理任务提高27.2%；但在需要视觉-文本记忆关联的任务上有所降低（例如名人识别减少49.5%）。

Conclusion: 验证了对齐预训练对于复杂推理任务的非必要性，提出了一种可减少计算资源的新型多模态学习框架，为保留模态特征的新研究方向奠定基础。

Abstract: Traditional multimodal learning approaches require expensive alignment
pre-training to bridge vision and language modalities, typically projecting
visual features into discrete text token spaces. We challenge both fundamental
assumptions underlying this paradigm by proposing Inverse-LLaVA, a novel
approach that eliminates alignment pre-training entirely while inverting the
conventional mapping direction. Rather than projecting visual features to text
space, our method maps text embeddings into continuous visual representation
space and performs fusion within transformer intermediate layers. Through
selective additive components in attention mechanisms, we enable dynamic
integration of visual and textual representations without requiring massive
image-text alignment datasets. Comprehensive experiments across nine multimodal
benchmarks demonstrate nuanced performance trade-offs: Inverse-LLaVA achieves
notable improvements on reasoning-intensive and cognitive tasks (MM-VET: +0.2%,
VizWiz: +1.8%, ScienceQA: +0.2%, cognitive reasoning: +27.2%), while showing
expected decreases in perception tasks requiring memorized visual-text
associations (celebrity recognition: -49.5%, OCR: -21.3%). These results
provide the first empirical evidence that alignment pre-training is not
necessary for effective multimodal learning, particularly for complex reasoning
tasks. Our work establishes the feasibility of a new paradigm that reduces
computational requirements by 45%, challenges conventional wisdom about
modality fusion, and opens new research directions for efficient multimodal
architectures that preserve modality-specific characteristics. Our project
website with code and additional resources is available at
https://inverse-llava.github.io.

</details>


### [79] [Standardization of Neuromuscular Reflex Analysis -- Role of Fine-Tuned Vision-Language Model Consortium and OpenAI gpt-oss Reasoning LLM Enabled Decision Support System](https://arxiv.org/abs/2508.12473)
*Eranga Bandara,Ross Gore,Sachin Shetty,Ravi Mukkamala,Christopher Rhea,Atmaram Yarlagadda,Shaifali Kaushik,L. H. M. P. De Silva,Andriy Maznychenko,Inna Sokolowska,Amin Hass,Kasun De Zoysa*

Main category: cs.CV

TL;DR: 本文提出基于精细调优的视觉语言模型(VLM)联盟与大语言模型(LLM)的决策支持系统，用于自动化H反射波形解析和诊断。


<details>
  <summary>Details</summary>
Motivation: 传统H反射电生理波形解析存在可靠性和标准化不足，需借助自动化技术提高可靠性和可解释性。

Method: 开发了由多个精细调优的视觉语言模型组成的联盟，结合推理大语言模型实现自动波形解析和决策支持，通过数据集标注、共识方法和工程化提示技术确保系统性能。

Result: 实验结果显示系统在H反射分析中具有高准确性、一致性及解释性，为自动化神经肌肉诊断提供了显著改进。

Conclusion: 这是首个将精细调优的VLM与推理LLM结合进行波形诊断的研究，推动了AI在神经肌肉评估中的应用，并为运动员监测平台奠定基础。

Abstract: Accurate assessment of neuromuscular reflexes, such as the H-reflex, plays a
critical role in sports science, rehabilitation, and clinical neurology.
Traditional analysis of H-reflex EMG waveforms is subject to variability and
interpretation bias among clinicians and researchers, limiting reliability and
standardization. To address these challenges, we propose a Fine-Tuned
Vision-Language Model (VLM) Consortium and a reasoning Large-Language Model
(LLM)-enabled Decision Support System for automated H-reflex waveform
interpretation and diagnosis. Our approach leverages multiple VLMs, each
fine-tuned on curated datasets of H-reflex EMG waveform images annotated with
clinical observations, recovery timelines, and athlete metadata. These models
are capable of extracting key electrophysiological features and predicting
neuromuscular states, including fatigue, injury, and recovery, directly from
EMG images and contextual metadata. Diagnostic outputs from the VLM consortium
are aggregated using a consensus-based method and refined by a specialized
reasoning LLM, which ensures robust, transparent, and explainable decision
support for clinicians and sports scientists. The end-to-end platform
orchestrates seamless communication between the VLM ensemble and the reasoning
LLM, integrating prompt engineering strategies and automated reasoning
workflows using LLM Agents. Experimental results demonstrate that this hybrid
system delivers highly accurate, consistent, and interpretable H-reflex
assessments, significantly advancing the automation and standardization of
neuromuscular diagnostics. To our knowledge, this work represents the first
integration of a fine-tuned VLM consortium with a reasoning LLM for image-based
H-reflex analysis, laying the foundation for next-generation AI-assisted
neuromuscular assessment and athlete monitoring platforms.

</details>


### [80] [Skin Cancer Classification: Hybrid CNN-Transformer Models with KAN-Based Fusion](https://arxiv.org/abs/2508.12484)
*Shubhi Agarwal,Amulya Kumar Mahto*

Main category: cs.CV

TL;DR: 论文提出了结合CNN与Transformer的混合模型，并通过引入CKAN进行特征融合以实现皮肤癌分类的高精度。


<details>
  <summary>Details</summary>
Motivation: 在医学影像中，精确区分恶性和非恶性病变对早期诊断和治疗至关重要，因此需要提升分类模型的性能与泛化能力。

Method: 构建结合Sequential和Parallel的混合CNN-Transformer模型，结合迁移学习与数据增强，利用CNN提取局部特征，Transformer建模全局依赖性，引入CKAN实现非线性特征融合。

Result: 在HAM10000、BCN20000和PAD-UFES三个数据集上分别实现了92.81%、91.17%和97.83%的分类精度，并显示出对不同数据分布和类别不平衡的鲁棒性和泛化能力。

Conclusion: 提出的混合模型通过结合局部与全局特征提取，并通过CKAN优化特征表征，有效地提升了皮肤癌分类的准确性和鲁棒性，展示了在医学影像分类领域的潜力。

Abstract: Skin cancer classification is a crucial task in medical image analysis, where
precise differentiation between malignant and non-malignant lesions is
essential for early diagnosis and treatment. In this study, we explore
Sequential and Parallel Hybrid CNN-Transformer models with Convolutional
Kolmogorov-Arnold Network (CKAN). Our approach integrates transfer learning and
extensive data augmentation, where CNNs extract local spatial features,
Transformers model global dependencies, and CKAN facilitates nonlinear feature
fusion for improved representation learning. To assess generalization, we
evaluate our models on multiple benchmark datasets (HAM10000,BCN20000 and
PAD-UFES) under varying data distributions and class imbalances. Experimental
results demonstrate that hybrid CNN-Transformer architectures effectively
capture both spatial and contextual features, leading to improved
classification performance. Additionally, the integration of CKAN enhances
feature fusion through learnable activation functions, yielding more
discriminative representations. Our proposed approach achieves competitive
performance in skin cancer classification, demonstrating 92.81% accuracy and
92.47% F1-score on the HAM10000 dataset, 97.83% accuracy and 97.83% F1-score on
the PAD-UFES dataset, and 91.17% accuracy with 91.79% F1- score on the BCN20000
dataset highlighting the effectiveness and generalizability of our model across
diverse datasets. This study highlights the significance of feature
representation and model design in advancing robust and accurate medical image
classification.

</details>


### [81] [Design and Validation of a Responsible Artificial Intelligence-based System for the Referral of Diabetic Retinopathy Patients](https://arxiv.org/abs/2508.12506)
*E. Ulises Moya-Sánchez,Abraham Sánchez-Perez,Raúl Nanclares Da Veiga,Alejandro Zarate-Macías,Edgar Villareal,Alejandro Sánchez-Montes,Edtna Jauregui-Ulloa,Héctor Moreno,Ulises Cortés*

Main category: cs.CV

TL;DR: RAIS-DR是一种用于糖尿病性视网膜病变筛查的负责任AI系统，显示出显著的效能和公平性提升。


<details>
  <summary>Details</summary>
Motivation: 糖尿病性视网膜病变（DR）是导致工作年龄人群视觉丧失的主要原因，但早期检测受到眼科医生短缺和及时检查难度的限制。因此，开发一种能够克服数据质量和偏见问题的负责任人工智能（AI）解决方案势在必行。

Method: RAIS-DR通过整合先进的卷积模型进行数据预处理、质量评估，以及开发三种专门用于分类DR的模型。此外，RAIS-DR将伦理原则贯穿于AI生命周期中的各个环节。该系统与FDA批准的EyeArt系统进行了对照评估。

Result: RAIS-DR在1,046名患者数据集上的F1分数提升了5-12%，准确率提升了6-19%，特异性提升了10-20%。同时，公平性指标显示，系统在不同人口群体之间具有公平的性能表现。

Conclusion: RAIS-DR是一种稳健且符合伦理的解决方案，可在临床环境中有效进行DR筛查，并有潜力减少医疗不平等。

Abstract: Diabetic Retinopathy (DR) is a leading cause of vision loss in working-age
individuals. Early detection of DR can reduce the risk of vision loss by up to
95%, but a shortage of retinologists and challenges in timely examination
complicate detection. Artificial Intelligence (AI) models using retinal fundus
photographs (RFPs) offer a promising solution. However, adoption in clinical
settings is hindered by low-quality data and biases that may lead AI systems to
learn unintended features. To address these challenges, we developed RAIS-DR, a
Responsible AI System for DR screening that incorporates ethical principles
across the AI lifecycle. RAIS-DR integrates efficient convolutional models for
preprocessing, quality assessment, and three specialized DR classification
models. We evaluated RAIS-DR against the FDA-approved EyeArt system on a local
dataset of 1,046 patients, unseen by both systems. RAIS-DR demonstrated
significant improvements, with F1 scores increasing by 5-12%, accuracy by
6-19%, and specificity by 10-20%. Additionally, fairness metrics such as
Disparate Impact and Equal Opportunity Difference indicated equitable
performance across demographic subgroups, underscoring RAIS-DR's potential to
reduce healthcare disparities. These results highlight RAIS-DR as a robust and
ethically aligned solution for DR screening in clinical settings. The code,
weights of RAIS-DR are available at
https://gitlab.com/inteligencia-gubernamental-jalisco/jalisco-retinopathy with
RAIL.

</details>


### [82] [LangVision-LoRA-NAS: Neural Architecture Search for Variable LoRA Rank in Vision Language Models](https://arxiv.org/abs/2508.12512)
*Krishna Teja Chitty-Venkata,Murali Emani,Venkatram Vishwanath*

Main category: cs.CV

TL;DR: 提出了一种新框架LangVision-LoRA-NAS，将神经架构搜索（NAS）与LoRA方法结合，优化视觉语言模型（VLM）以实现变量等级适应，从而提高任务表现并降低微调成本。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA方法使用固定等级进行微调，缺乏灵活性和适应性，难以应对不同的多模态任务需求。

Method: 引入LangVision-LoRA-NAS框架，用NAS动态搜索最佳LoRA等级配置，针对特定多模态任务实现性能与计算效率的平衡。

Result: 在LLaMA-3.2-11B模型及多个数据集上的广泛实验表明，新框架显著提升了模型性能，同时减少了微调成本。

Conclusion: LangVision-LoRA-NAS利用动态等级配置，提升了VLM的适应能力和任务表现，展示了其应用潜力。

Abstract: Vision Language Models (VLMs) integrate visual and text modalities to enable
multimodal understanding and generation. These models typically combine a
Vision Transformer (ViT) as an image encoder and a Large Language Model (LLM)
for text generation. LoRA (Low-Rank Adaptation) is an efficient fine-tuning
method to adapt pre-trained models to new tasks by introducing low-rank updates
to their weights. While LoRA has emerged as a powerful technique for
fine-tuning large models by introducing low-rank updates, current
implementations assume a fixed rank, potentially limiting flexibility and
efficiency across diverse tasks. This paper introduces
\textit{LangVision-LoRA-NAS}, a novel framework that integrates Neural
Architecture Search (NAS) with LoRA to optimize VLMs for variable-rank
adaptation. Our approach leverages NAS to dynamically search for the optimal
LoRA rank configuration tailored to specific multimodal tasks, balancing
performance and computational efficiency. Through extensive experiments using
the LLaMA-3.2-11B model on several datasets, LangVision-LoRA-NAS demonstrates
notable improvement in model performance while reducing fine-tuning costs. Our
Base and searched fine-tuned models on LLaMA-3.2-11B-Vision-Instruct can be
found
\href{https://huggingface.co/collections/krishnateja95/llama-32-11b-vision-instruct-langvision-lora-nas-6786cac480357a6a6fcc59ee}{\textcolor{blue}{here}}
and the code for LangVision-LoRA-NAS can be found
\href{https://github.com/krishnateja95/LangVision-NAS}{\textcolor{blue}{here}}.

</details>


### [83] [An Initial Study of Bird's-Eye View Generation for Autonomous Vehicles using Cross-View Transformers](https://arxiv.org/abs/2508.12520)
*Felipe Carlos dos Santos,Eric Aislan Antonelo,Gustavo Claudio Karl Couto*

Main category: cs.CV

TL;DR: 本文研究利用Cross-View Transformers (CVT)将相机图像转化为鸟瞰图（BEV）并测试该方法的性能。


<details>
  <summary>Details</summary>
Motivation: 通过探索利用CVT技术形成从相机图像生成鸟瞰图的有效方法，为自动驾驶提供结构化的视觉感知支持。

Method: 在模拟城市驾驶环境中，使用CVT从相机图像预测道路、车道标记和规划路径的BEV层，同时考察不同相机布局和两种损失函数（focal和L1）对结果的影响。

Result: 四相机CVT使用L1损失函数训练后，对未见过的新区域表现出最强的测试鲁棒性，验证了方法的有效性。

Conclusion: CVT方法在将相机输入映射为准确BEV图方面表现优异，为自动驾驶感知提供了新的可能性。

Abstract: Bird's-Eye View (BEV) maps provide a structured, top-down abstraction that is
crucial for autonomous-driving perception. In this work, we employ Cross-View
Transformers (CVT) for learning to map camera images to three BEV's channels -
road, lane markings, and planned trajectory - using a realistic simulator for
urban driving. Our study examines generalization to unseen towns, the effect of
different camera layouts, and two loss formulations (focal and L1). Using
training data from only a town, a four-camera CVT trained with the L1 loss
delivers the most robust test performance, evaluated in a new town. Overall,
our results underscore CVT's promise for mapping camera inputs to reasonably
accurate BEV maps.

</details>


### [84] [MuSACo: Multimodal Subject-Specific Selection and Adaptation for Expression Recognition with Co-Training](https://arxiv.org/abs/2508.12522)
*Muhammad Osama Zeeshan,Natacha Gillet,Alessandro Lameiras Koerich,Marco Pedersoli,Francois Bremond,Eric Granger*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为MuSACo的方法，专注于个性化表情识别中的多模态数据和多源域自适应，展现了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是现有的多源域自适应（MSDA）方法在个性化表情识别中对多模态信息利用不足，且未能有效捕捉特定个体的特征，限制了模型在数字健康等领域的应用潜力。

Method: 所提出的方法MuSACo通过联合使用多种模态和多源域数据，采用主导模态进行伪标签生成，用于类别感知学习，同时叠加类别无关损失应对低置信数据。此外，通过对每种模态的源特征对齐以及仅组合置信度高的目标特征，实现更加精细化的个体适配。

Result: 实验结果表明，MuSACo在BioVid和StressID这两个具有挑战性的多模态数据集上，优于现有的未监督域自适应（UDA）方法及其他MSDA方法。

Conclusion: MuSACo通过多模态和多源域的联合选择与适配，有效提升了个性化表情识别的精度和稳健性，为数字健康领域的情感计算应用提供了新机会。

Abstract: Personalized expression recognition (ER) involves adapting a machine learning
model to subject-specific data for improved recognition of expressions with
considerable interpersonal variability. Subject-specific ER can benefit
significantly from multi-source domain adaptation (MSDA) methods, where each
domain corresponds to a specific subject, to improve model accuracy and
robustness. Despite promising results, state-of-the-art MSDA approaches often
overlook multimodal information or blend sources into a single domain, limiting
subject diversity and failing to explicitly capture unique subject-specific
characteristics. To address these limitations, we introduce MuSACo, a
multi-modal subject-specific selection and adaptation method for ER based on
co-training. It leverages complementary information across multiple modalities
and multiple source domains for subject-specific adaptation. This makes MuSACo
particularly relevant for affective computing applications in digital health,
such as patient-specific assessment for stress or pain, where subject-level
nuances are crucial. MuSACo selects source subjects relevant to the target and
generates pseudo-labels using the dominant modality for class-aware learning,
in conjunction with a class-agnostic loss to learn from less confident target
samples. Finally, source features from each modality are aligned, while only
confident target features are combined. Our experimental results on challenging
multimodal ER datasets: BioVid and StressID, show that MuSACo can outperform
UDA (blending) and state-of-the-art MSDA methods.

</details>


### [85] [REVEAL -- Reasoning and Evaluation of Visual Evidence through Aligned Language](https://arxiv.org/abs/2508.12543)
*Ipsita Praharaj,Yukta Butala,Yash Butala*

Main category: cs.CV

TL;DR: 本文提出了一种称为REVEAL的框架，通过大规模视觉-语言模型的语义对齐能力，利用提示驱动进行视觉推理任务以检测和解释图像伪造。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅限于监督训练特定操作或嵌入空间异常检测，难以实现跨领域的泛化；因此需要更稳健的图像伪造检测框架，特别是在提供推理和定位方面。

Method: 框架包含两个主要方法：(1)基于图像整体物理、语义、透视及真实性的全景分析；(2)将图像分割成多个区域并依次分析以进行区域异常检测。

Result: 在不同领域的数据集（例如Photoshop，DeepFake和AIGC编辑）上进行实验，并将其与具有竞争力的基线模型进行比较，同时分析了所给出的推理能力。

Conclusion: 提出的方法展示了基于视觉语言模型进行图像伪造检测及其推理和定位的潜力，尤其在跨领域适应性方面表现突出。

Abstract: The rapid advancement of generative models has intensified the challenge of
detecting and interpreting visual forgeries, necessitating robust frameworks
for image forgery detection while providing reasoning as well as localization.
While existing works approach this problem using supervised training for
specific manipulation or anomaly detection in the embedding space,
generalization across domains remains a challenge. We frame this problem of
forgery detection as a prompt-driven visual reasoning task, leveraging the
semantic alignment capabilities of large vision-language models. We propose a
framework, `REVEAL` (Reasoning and Evaluation of Visual Evidence through
Aligned Language), that incorporates generalized guidelines. We propose two
tangential approaches - (1) Holistic Scene-level Evaluation that relies on the
physics, semantics, perspective, and realism of the image as a whole and (2)
Region-wise anomaly detection that splits the image into multiple regions and
analyzes each of them. We conduct experiments over datasets from different
domains (Photoshop, DeepFake and AIGC editing). We compare the Vision Language
Models against competitive baselines and analyze the reasoning provided by
them.

</details>


### [86] [Structure-preserving Feature Alignment for Old Photo Colorization](https://arxiv.org/abs/2508.12570)
*Yingxue Pang,Xin Jin,Jun Fu,Zhibo Chen*

Main category: cs.CV

TL;DR: 提出了SFAC算法，仅需两张图像以实现旧照片的结构保留色彩化，并克服域差异问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型难以直接用于旧照片的颜色化任务，因缺乏真实标签与域差异。

Method: 开发了SFAC算法，仅依赖两张训练图像，通过特征分布对齐损失和结构保留机制实现颜色化。

Result: 模型通过定性和定量评估验证了其效果。

Conclusion: 该方法无需大规模数据即可有效地将参考图像的颜色应用于旧照片，体现出了优越的结构保留能力。

Abstract: Deep learning techniques have made significant advancements in
reference-based colorization by training on large-scale datasets. However,
directly applying these methods to the task of colorizing old photos is
challenging due to the lack of ground truth and the notorious domain gap
between natural gray images and old photos. To address this issue, we propose a
novel CNN-based algorithm called SFAC, i.e., Structure-preserving Feature
Alignment Colorizer. SFAC is trained on only two images for old photo
colorization, eliminating the reliance on big data and allowing direct
processing of the old photo itself to overcome the domain gap problem. Our
primary objective is to establish semantic correspondence between the two
images, ensuring that semantically related objects have similar colors. We
achieve this through a feature distribution alignment loss that remains robust
to different metric choices. However, utilizing robust semantic correspondence
to transfer color from the reference to the old photo can result in inevitable
structure distortions. To mitigate this, we introduce a structure-preserving
mechanism that incorporates a perceptual constraint at the feature level and a
frozen-updated pyramid at the pixel level. Extensive experiments demonstrate
the effectiveness of our method for old photo colorization, as confirmed by
qualitative and quantitative metrics.

</details>


### [87] [Foundation Model for Skeleton-Based Human Action Understanding](https://arxiv.org/abs/2508.12586)
*Hongsong Wang,Wanjiang Weng,Junbo Wang,Fang Zhao,Guo-Sen Xie,Xin Geng,Liang Wang*

Main category: cs.CV

TL;DR: 提出了一种称为USDRL的统一骨骼稠密表示学习框架，作为骨骼基础模型，进行广泛的动作理解任务研究和性能改进。


<details>
  <summary>Details</summary>
Motivation: 现有工作缺乏应对各种动作理解任务的可扩展性和泛化能力，同时尚无骨骼基础模型可适用于多种任务。

Method: USDRL框架包括Transformer-based的稠密时空编码器（DSTE）、多粒度特征去相关模块（MG-FD）和多视角一致性训练模块（MPCT），通过并行流和自监督方法增强特征提取与语义学习。

Result: 在覆盖9大类动作理解任务的25个基准测试中，USDRL显著优于当前最先进方法。

Conclusion: USDRL推动了骨骼动作理解的研究广度，尤其在稠密预测任务中表现突出，具有良好应用前景。

Abstract: Human action understanding serves as a foundational pillar in the field of
intelligent motion perception. Skeletons serve as a modality- and
device-agnostic representation for human modeling, and skeleton-based action
understanding has potential applications in humanoid robot control and
interaction. \RED{However, existing works often lack the scalability and
generalization required to handle diverse action understanding tasks. There is
no skeleton foundation model that can be adapted to a wide range of action
understanding tasks}. This paper presents a Unified Skeleton-based Dense
Representation Learning (USDRL) framework, which serves as a foundational model
for skeleton-based human action understanding. USDRL consists of a
Transformer-based Dense Spatio-Temporal Encoder (DSTE), Multi-Grained Feature
Decorrelation (MG-FD), and Multi-Perspective Consistency Training (MPCT). The
DSTE module adopts two parallel streams to learn temporal dynamic and spatial
structure features. The MG-FD module collaboratively performs feature
decorrelation across temporal, spatial, and instance domains to reduce
dimensional redundancy and enhance information extraction. The MPCT module
employs both multi-view and multi-modal self-supervised consistency training.
The former enhances the learning of high-level semantics and mitigates the
impact of low-level discrepancies, while the latter effectively facilitates the
learning of informative multimodal features. We perform extensive experiments
on 25 benchmarks across across 9 skeleton-based action understanding tasks,
covering coarse prediction, dense prediction, and transferred prediction. Our
approach significantly outperforms the current state-of-the-art methods. We
hope that this work would broaden the scope of research in skeleton-based
action understanding and encourage more attention to dense prediction tasks.

</details>


### [88] [Multimodal Chain of Continuous Thought for Latent-Space Reasoning in Vision-Language Models](https://arxiv.org/abs/2508.12587)
*Tan-Hanh Pham,Chris Ngo*

Main category: cs.CV

TL;DR: 本文提出了多模态连续思维链（MCOUT），取代传统语言模式的推理链（CoT），通过在联合潜在空间中的连续推理改进多模态推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大型多模态模型推理方法主要依赖于基于语言模型的技巧（如思维链CoT），其在多模态场景下表现出与动态对齐音频、视觉和文本信息的局限性。

Method: 提出了MCOUT，包括MCOUT-Base和MCOUT-Multi两种变体，通过联合潜在空间中的连续隐藏向量进行推理，并结合多模态潜在注意力增强视觉与文本特征的跨模态对齐。

Result: 在MMMU、ScienceQA和MMStar等基准上的实验显示，MCOUT相比强基线在多模态推理的准确率上提升了最高8.23%，并在BLEU分数上最高提升了8.27%。

Conclusion: 通过潜在连续推理，MCOUT展示了突破语言边界的思维链（CoT）的发展前景，并提供了一种可扩展的人类反思式多模态推理框架。

Abstract: Many reasoning techniques for large multimodal models adapt language model
approaches, such as Chain-of-Thought (CoT) prompting, which express reasoning
as word sequences. While effective for text, these methods are suboptimal for
multimodal contexts, struggling to align audio, visual, and textual information
dynamically. To explore an alternative paradigm, we propose the Multimodal
Chain of Continuous Thought (MCOUT), which enables reasoning directly in a
joint latent space rather than in natural language. In MCOUT, the reasoning
state is represented as a continuous hidden vector, iteratively refined and
aligned with visual and textual embeddings, inspired by human reflective
cognition. We develop two variants: MCOUT-Base, which reuses the language
model`s last hidden state as the continuous thought for iterative reasoning,
and MCOUT-Multi, which integrates multimodal latent attention to strengthen
cross-modal alignment between visual and textual features. Experiments on
benchmarks including MMMU, ScienceQA, and MMStar show that MCOUT consistently
improves multimodal reasoning, yielding up to 8.23% accuracy gains over strong
baselines and improving BLEU scores up to 8.27% across multiple-choice and
open-ended tasks. These findings highlight latent continuous reasoning as a
promising direction for advancing LMMs beyond language-bound CoT, offering a
scalable framework for human-like reflective multimodal inference. Code is
available at https://github.com/Hanhpt23/OmniMod.

</details>


### [89] [ViLaD: A Large Vision Language Diffusion Framework for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.12603)
*Can Cui,Yupeng Zhou,Juntong Peng,Sung-Yeon Park,Zichong Yang,Prashanth Sankaranarayanan,Jiaru Zhang,Ruqi Zhang,Ziran Wang*

Main category: cs.CV

TL;DR: ViLaD是一种用于端到端自动驾驶的新型框架，采用掩码扩散模型以并行生成驾驶决策序列，显著提高了推理速度并支持双向推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型的自动驾驶系统存在高推理延迟及缺乏双向推理能力的问题，难以应用于动态和安全关键场景。

Method: 提出了一种名为ViLaD的大型视觉语言扩散框架，基于掩码扩散模型实现并行生成驾驶决策序列，并支持双向推理和渐进式改进生成质量。

Result: 在nuScenes数据集上，ViLaD在规划准确性和推理速度上均优于最新的自回归基线模型，并基本无失败率。同时，在真实世界的交互式停车任务中验证了其实用性。

Conclusion: ViLaD框架通过降低计算延迟、提供双向推理和渐进式生成能力，展现了在端到端自动驾驶任务中的效果与实用性。

Abstract: End-to-end autonomous driving systems built on Vision Language Models (VLMs)
have shown significant promise, yet their reliance on autoregressive
architectures introduces some limitations for real-world applications. The
sequential, token-by-token generation process of these models results in high
inference latency and cannot perform bidirectional reasoning, making them
unsuitable for dynamic, safety-critical environments. To overcome these
challenges, we introduce ViLaD, a novel Large Vision Language Diffusion (LVLD)
framework for end-to-end autonomous driving that represents a paradigm shift.
ViLaD leverages a masked diffusion model that enables parallel generation of
entire driving decision sequences, significantly reducing computational
latency. Moreover, its architecture supports bidirectional reasoning, allowing
the model to consider both past and future simultaneously, and supports
progressive easy-first generation to iteratively improve decision quality. We
conduct comprehensive experiments on the nuScenes dataset, where ViLaD
outperforms state-of-the-art autoregressive VLM baselines in both planning
accuracy and inference speed, while achieving a near-zero failure rate.
Furthermore, we demonstrate the framework's practical viability through a
real-world deployment on an autonomous vehicle for an interactive parking task,
confirming its effectiveness and soundness for practical applications.

</details>


### [90] [ViDA-UGC: Detailed Image Quality Analysis via Visual Distortion Assessment for UGC Images](https://arxiv.org/abs/2508.12605)
*Wenjie Liao,Jieyu Yuan,Yifang Xu,Chunle Guo,Zilong Zhang,Jihong Li,Jiachen Fu,Haotian Fan,Tao Li,Junhui Cui,Chongyi Li*

Main category: cs.CV

TL;DR: 本文提出了首个大规模UGC（用户生成内容）图像质量评估数据集ViDA-UGC，包括11K图像，着重细粒度质量定位、质量感知及推理描述分析。还建立了首个UGC失真评估基准ViDA-UGC-Bench，用于提升多模态大模型的图像质量分析能力。


<details>
  <summary>Details</summary>
Motivation: 当前的图像质量评估方法未能区分UGC和AIGC图像的不同失真标准，且在图像监控与修复指导方面缺乏详细分析。需要一种可解释的图像质量评估工具，满足UGC图像的独特需求。

Method: 研究者构建了ViDA-UGC数据集，通过失真导向流程与Chain-of-Thought评估框架，结合人类注解和GPT-4o生成质量描述，捕捉失真模式相关低层次视觉特征。同时从ViDA-UGC中精选476张图像和6,149个问答对，由专业团队校正，构建ViDA-UGC-Bench基准。

Result: 实验表明，ViDA-UGC数据集和CoT框架显著增强了多模态大语言模型的图像质量分析能力，其效果在ViDA-UGC-Bench和Q-Bench上甚至超过GPT-4o。

Conclusion: ViDA-UGC和相关框架为可解释图像质量评估提供了创新基础，尤其在满足UGC图像需求与多模态语言模型性能提升方面展现了显著优势。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have introduced a
paradigm shift for Image Quality Assessment (IQA) from unexplainable image
quality scoring to explainable IQA, demonstrating practical applications like
quality control and optimization guidance. However, current explainable IQA
methods not only inadequately use the same distortion criteria to evaluate both
User-Generated Content (UGC) and AI-Generated Content (AIGC) images, but also
lack detailed quality analysis for monitoring image quality and guiding image
restoration. In this study, we establish the first large-scale Visual
Distortion Assessment Instruction Tuning Dataset for UGC images, termed
ViDA-UGC, which comprises 11K images with fine-grained quality grounding,
detailed quality perception, and reasoning quality description data. This
dataset is constructed through a distortion-oriented pipeline, which involves
human subject annotation and a Chain-of-Thought (CoT) assessment framework.
This framework guides GPT-4o to generate quality descriptions by identifying
and analyzing UGC distortions, which helps capturing rich low-level visual
features that inherently correlate with distortion patterns. Moreover, we
carefully select 476 images with corresponding 6,149 question answer pairs from
ViDA-UGC and invite a professional team to ensure the accuracy and quality of
GPT-generated information. The selected and revised data further contribute to
the first UGC distortion assessment benchmark, termed ViDA-UGC-Bench.
Experimental results demonstrate the effectiveness of the ViDA-UGC and CoT
framework for consistently enhancing various image quality analysis abilities
across multiple base MLLMs on ViDA-UGC-Bench and Q-Bench, even surpassing
GPT-4o.

</details>


### [91] [OpenMoCap: Rethinking Optical Motion Capture under Real-world Occlusion](https://arxiv.org/abs/2508.12610)
*Chen Qian,Danyang Li,Xinran Yu,Zheng Yang,Qiang Ma*

Main category: cs.CV

TL;DR: 本文提出了一种解决光学动作捕捉中因标记遮挡引起性能下降问题的新方法与数据集。


<details>
  <summary>Details</summary>
Motivation: 现有模型在大规模标记遮挡情况下表现欠佳，主要因为缺少现实的训练数据与无法处理标记间长距离依赖。

Method: 本文通过CMU-Occlu数据集使用光线追踪技术模拟遮挡模式，并提出OpenMoCap模型，通过标记-关节链推断优化标记和关节的深度约束。

Result: 实验表明，OpenMoCap在不同场景下性能优于现有方法，同时CMU-Occlu数据集为未来研究提供基础。

Conclusion: OpenMoCap解决了显著遮挡环境下的动作捕捉问题，并集成到MoSen MoCap系统中，代码已开源。

Abstract: Optical motion capture is a foundational technology driving advancements in
cutting-edge fields such as virtual reality and film production. However,
system performance suffers severely under large-scale marker occlusions common
in real-world applications. An in-depth analysis identifies two primary
limitations of current models: (i) the lack of training datasets accurately
reflecting realistic marker occlusion patterns, and (ii) the absence of
training strategies designed to capture long-range dependencies among markers.
To tackle these challenges, we introduce the CMU-Occlu dataset, which
incorporates ray tracing techniques to realistically simulate practical marker
occlusion patterns. Furthermore, we propose OpenMoCap, a novel motion-solving
model designed specifically for robust motion capture in environments with
significant occlusions. Leveraging a marker-joint chain inference mechanism,
OpenMoCap enables simultaneous optimization and construction of deep
constraints between markers and joints. Extensive comparative experiments
demonstrate that OpenMoCap consistently outperforms competing methods across
diverse scenarios, while the CMU-Occlu dataset opens the door for future
studies in robust motion solving. The proposed OpenMoCap is integrated into the
MoSen MoCap system for practical deployment. The code is released at:
https://github.com/qianchen214/OpenMoCap.

</details>


### [92] [WIPES: Wavelet-based Visual Primitives](https://arxiv.org/abs/2508.12615)
*Wenhao Zhang,Hao Zhu,Delong Wu,Di Kang,Linchao Bao,Zhan Ma,Xun Cao*

Main category: cs.CV

TL;DR: 提出基于小波的视觉表征方法WIPES，以解决频率引导和复杂神经网络解码问题，实现更高效和高质的渲染。


<details>
  <summary>Details</summary>
Motivation: 现有的连续视觉表征依赖于频率指导或复杂神经网络解码，导致频谱损失或渲染速度慢。需要一种既能灵活调整频率，又能快速渲染的解决方案。

Method: 基于小波的时空频率本地化优点，提出了WIPES方法来捕获视觉信号的低频和高频信息，并开发了小波为基础的可微分光栅化器来实现快速渲染。

Result: 在2D图像、5D静态及6D动态新视图合成等视觉任务中，WIPES在渲染质量和推理速度方面优于基于INR的方法，在渲染质量上优于基于高斯的方法。

Conclusion: WIPES能有效表征多维视觉信号，并实现高质高效的渲染，是一种通用的视觉基元。

Abstract: Pursuing a continuous visual representation that offers flexible frequency
modulation and fast rendering speed has recently garnered increasing attention
in the fields of 3D vision and graphics. However, existing representations
often rely on frequency guidance or complex neural network decoding, leading to
spectrum loss or slow rendering. To address these limitations, we propose
WIPES, a universal Wavelet-based vIsual PrimitivES for representing
multi-dimensional visual signals. Building on the spatial-frequency
localization advantages of wavelets, WIPES effectively captures both the
low-frequency "forest" and the high-frequency "trees." Additionally, we develop
a wavelet-based differentiable rasterizer to achieve fast visual rendering.
Experimental results on various visual tasks, including 2D image
representation, 5D static and 6D dynamic novel view synthesis, demonstrate that
WIPES, as a visual primitive, offers higher rendering quality and faster
inference than INR-based methods, and outperforms Gaussian-based
representations in rendering quality.

</details>


### [93] [Creative4U: MLLMs-based Advertising Creative Image Selector with Comparative Reasoning](https://arxiv.org/abs/2508.12628)
*Yukang Lin,Xiang Zhang,Shichang Jia,Bowen Wan,Chenghan Fu,Xudong Ren,Yueran Liu,Wanxian Guan,Pengji Wang,Jian Xu,Bo Zheng,Baolin Liu*

Main category: cs.CV

TL;DR: 这篇论文提出了解释性创意评估和选择的新方法，结合多模态大语言模型以自然语言生成任务形式评估和选择创意图片。


<details>
  <summary>Details</summary>
Motivation: 随着AIGC技术的兴起，广告商可以低成本生成大量创意图片，但缺乏评估创意质量和解释选择依据的有效方法。

Method: 提出了一个基于多模态大语言模型的框架，并构建了CreativePair数据集及Creative4U选择器，通过监督微调和强化学习提高选择准确性。

Result: 通过离线和在线实验验证了方法的有效性。

Conclusion: 该方法可以准确评估并选择创意图片，其代码和数据集将公开供研究和工业应用。

Abstract: Creative image in advertising is the heart and soul of e-commerce platform.
An eye-catching creative image can enhance the shopping experience for users,
boosting income for advertisers and advertising revenue for platforms. With the
advent of AIGC technology, advertisers can produce large quantities of creative
images at minimal cost. However, they struggle to assess the creative quality
to select. Existing methods primarily focus on creative ranking, which fails to
address the need for explainable creative selection.
  In this work, we propose the first paradigm for explainable creative
assessment and selection. Powered by multimodal large language models (MLLMs),
our approach integrates the assessment and selection of creative images into a
natural language generation task. To facilitate this research, we construct
CreativePair, the first comparative reasoning-induced creative dataset
featuring 8k annotated image pairs, with each sample including a label
indicating which image is superior. Additionally, we introduce Creative4U
(pronounced Creative for You), a MLLMs-based creative selector that takes into
account users' interests. Through Reason-to-Select RFT, which includes
supervised fine-tuning with Chain-of-Thought (CoT-SFT) and Group Relative
Policy Optimization (GRPO) based reinforcement learning, Creative4U is able to
evaluate and select creative images accurately. Both offline and online
experiments demonstrate the effectiveness of our approach. Our code and dataset
will be made public to advance research and industrial applications.

</details>


### [94] [SpotVLM: Cloud-edge Collaborative Real-time VLM based on Context Transfer](https://arxiv.org/abs/2508.12638)
*Chen Qian,Xinran Yu,Zewen Huang,Danyang Li,Qiang Ma,Fan Dang,Xuan Ding,Guangyong Shang,Zheng Yang*

Main category: cs.CV

TL;DR: 提出了一种新的云-边协作范式Context Transfer，用于改进实时视觉任务中的视觉-语言模型协作效率。


<details>
  <summary>Details</summary>
Motivation: 现有云-边协作架构无法很好地处理云延迟波动，也未充分利用延迟但准确的LVLM响应的潜力。

Method: 提出Context Transfer范式，将LVLM的延迟输出作为历史上下文，为SVLM的推理提供实时指导；并设计SpotVLM模型，结合上下文替换和视觉聚焦模块来增强一致性。

Result: 在三个实时视觉任务和四个数据集上的实验验证了该框架的有效性。

Conclusion: 新范式为未来VLM系统中的更高效和延迟感知协作策略奠定了基础。

Abstract: Vision-Language Models (VLMs) are increasingly deployed in real-time
applications such as autonomous driving and human-computer interaction, which
demand fast and reliable responses based on accurate perception. To meet these
requirements, existing systems commonly employ cloud-edge collaborative
architectures, such as partitioned Large Vision-Language Models (LVLMs) or task
offloading strategies between Large and Small Vision-Language Models (SVLMs).
However, these methods fail to accommodate cloud latency fluctuations and
overlook the full potential of delayed but accurate LVLM responses. In this
work, we propose a novel cloud-edge collaborative paradigm for VLMs, termed
Context Transfer, which treats the delayed outputs of LVLMs as historical
context to provide real-time guidance for SVLMs inference. Based on this
paradigm, we design SpotVLM, which incorporates both context replacement and
visual focus modules to refine historical textual input and enhance visual
grounding consistency. Extensive experiments on three real-time vision tasks
across four datasets demonstrate the effectiveness of the proposed framework.
The new paradigm lays the groundwork for more effective and latency-aware
collaboration strategies in future VLM systems.

</details>


### [95] [Synthesizing Accurate and Realistic T1-weighted Contrast-Enhanced MR Images using Posterior-Mean Rectified Flow](https://arxiv.org/abs/2508.12640)
*Bastian Brandstötter,Erich Kobler*

Main category: cs.CV

TL;DR: 本文提出了一种名为PMRF的双阶段管道，用于从无对比的MRI输入合成对比增强的脑部MRI，能够精确还原细节并兼顾临床需求。


<details>
  <summary>Details</summary>
Motivation: 对比增强T1加权MRI需要钆基对比剂，这提高了成本和扫描时间，同时可能对环境及患者安全性造成影响，本文旨在开发一种无对比剂的替代方法。

Method: 提出了一个两阶段的PMRF管道：首先通过3D U-Net预测体素级的后验均值；接着通过时间条件3D修正流对初始结果进行细致优化以增强纹理细节。

Result: 在测试中，精炼输出的FID为12.46，KID为0.007（比仅有后验均值降低约68.7%），同时体积MSE保持在0.057（比后验均值提高约27%），体现较好的感知及结构平衡。

Conclusion: 该方法能够真实还原病变与血管细节，为临床应用场景提供了一种有效的解决方案，同时平衡了感知和失真之间的权衡。

Abstract: Contrast-enhanced (CE) T1-weighted MRI is central to neuro-oncologic
diagnosis but requires gadolinium-based agents, which add cost and scan time,
raise environmental concerns, and may pose risks to patients. In this work, we
propose a two-stage Posterior-Mean Rectified Flow (PMRF) pipeline for
synthesizing volumetric CE brain MRI from non-contrast inputs. First, a
patch-based 3D U-Net predicts the voxel-wise posterior mean (minimizing MSE).
Then, this initial estimate is refined by a time-conditioned 3D rectified flow
to incorporate realistic textures without compromising structural fidelity. We
train this model on a multi-institutional collection of paired pre- and
post-contrast T1w volumes (BraTS 2023-2025). On a held-out test set of 360
diverse volumes, our best refined outputs achieve an axial FID of $12.46$ and
KID of $0.007$ ($\sim 68.7\%$ lower FID than the posterior mean) while
maintaining low volumetric MSE of $0.057$ ($\sim 27\%$ higher than the
posterior mean). Qualitative comparisons confirm that our method restores
lesion margins and vascular details realistically, effectively navigating the
perception-distortion trade-off for clinical deployment.

</details>


### [96] [Vision-G1: Towards General Vision Language Reasoning with Multi-Domain Data Curation](https://arxiv.org/abs/2508.12680)
*Yuheng Zha,Kun Zhou,Yujia Wu,Yushu Wang,Jie Feng,Zhi Xu,Shibo Hao,Zhengzhong Liu,Eric P. Xing,Zhiting Hu*

Main category: cs.CV

TL;DR: 构建了一个综合性的视图推理数据集，并通过多轮强化学习训练了Vision-G1模型，在多个推理基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前的视觉学习模型（VLM）在任务范围和跨领域推理能力上有限，且领域特定数据集的兼容性不确定。

Method: 利用46个数据来源构建了8维度的RL准备数据集，并通过影响函数选择和基于难度的筛选策略形成高质量数据，以数据课程方式进行强化学习训练。

Result: Vision-G1在多个视觉推理基准上达到了SOTA性能，超过了GPT-4o和Gemini-1.5 Flash等模型。

Conclusion: 通过开发全面的数据集与创新训练方法，实现了更强的跨领域视觉推理能力，具有显著的研究和实际应用价值。

Abstract: Despite their success, current training pipelines for reasoning VLMs focus on
a limited range of tasks, such as mathematical and logical reasoning. As a
result, these models face difficulties in generalizing their reasoning
capabilities to a wide range of domains, primarily due to the scarcity of
readily available and verifiable reward data beyond these narrowly defined
areas. Moreover, integrating data from multiple domains is challenging, as the
compatibility between domain-specific datasets remains uncertain. To address
these limitations, we build a comprehensive RL-ready visual reasoning dataset
from 46 data sources across 8 dimensions, covering a wide range of tasks such
as infographic, mathematical, spatial, cross-image, graphic user interface,
medical, common sense and general science. We propose an influence function
based data selection and difficulty based filtering strategy to identify
high-quality training samples from this dataset. Subsequently, we train the
VLM, referred to as Vision-G1, using multi-round RL with a data curriculum to
iteratively improve its visual reasoning capabilities. Our model achieves
state-of-the-art performance across various visual reasoning benchmarks,
outperforming similar-sized VLMs and even proprietary models like GPT-4o and
Gemini-1.5 Flash. The model, code and dataset are publicly available at
https://github.com/yuh-zha/Vision-G1.

</details>


### [97] [Learn Faster and Remember More: Balancing Exploration and Exploitation for Continual Test-time Adaptation](https://arxiv.org/abs/2508.12643)
*Pinci Yang,Peisong Wen,Ke Ma,Qianqian Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种提高连续测试时间适应性（CTTA）的方法，通过加速新域的适应以及保留、利用过往领域知识，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前CTTA方法在新域适应（探索）与历史知识保留（开发）之间难以实现平衡，且往往存在浅层特征适应不足和知识遗忘现象。

Method: 作者提出了一种基于均值教师（mean teacher）框架的方法，包括多级一致性正则化（MCR）损失来对齐深度模型的中间特征，以及补偿锚点重放（CAR）机制来利用历史检查点用于知识恢复。

Result: 实验表明，该方法在多个基准测试中显著优于现有最先进的方法。

Conclusion: 该方法在解决CTTA过程中探索与开发平衡的问题中表现优异，具有加速适应和恢复历史知识的能力。

Abstract: Continual Test-Time Adaptation (CTTA) aims to adapt a source pre-trained
model to continually changing target domains during inference. As a fundamental
principle, an ideal CTTA method should rapidly adapt to new domains
(exploration) while retaining and exploiting knowledge from previously
encountered domains to handle similar domains in the future. Despite
significant advances, balancing exploration and exploitation in CTTA is still
challenging: 1) Existing methods focus on adjusting predictions based on
deep-layer outputs of neural networks. However, domain shifts typically affect
shallow features, which are inefficient to be adjusted from deep predictions,
leading to dilatory exploration; 2) A single model inevitably forgets knowledge
of previous domains during the exploration, making it incapable of exploiting
historical knowledge to handle similar future domains. To address these
challenges, this paper proposes a mean teacher framework that strikes an
appropriate Balance between Exploration and Exploitation (BEE) during the CTTA
process. For the former challenge, we introduce a Multi-level Consistency
Regularization (MCR) loss that aligns the intermediate features of the student
and teacher models, accelerating adaptation to the current domain. For the
latter challenge, we employ a Complementary Anchor Replay (CAR) mechanism to
reuse historical checkpoints (anchors), recovering complementary knowledge for
diverse domains. Experiments show that our method significantly outperforms
state-of-the-art methods on several benchmarks, demonstrating its effectiveness
for CTTA tasks.

</details>


### [98] [DyCrowd: Towards Dynamic Crowd Reconstruction from a Large-scene Video](https://arxiv.org/abs/2508.12644)
*Hao Wen,Hongbo Kang,Jian Ma,Jing Huang,Yuanwang Yang,Haozhe Lin,Yu-Kun Lai,Kun Li*

Main category: cs.CV

TL;DR: 本文提出DyCrowd框架，实现了基于大场景视频的动态人群三维重建，通过集体行为用于解决长时间动态遮挡问题，并使用AMC损失确保动作恢复的稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有方法多基于静态图像进行人群三维重建，缺乏时序一致性，且无法有效缓解遮挡问题。本文致力于通过视频的时间和空间信息，实现动态人群的稳健重建。

Method: 提出一种由粗到细的分组引导运动优化策略，结合基于变分自编码器的运动先验和片段级的分组优化策略，并引入AMC损失，利用高质量的非遮挡动作片段引导遮挡片段动作恢复。

Result: 实验结果表明，所提方法在大场景动态人群重建任务中达到了业界先进水平。

Conclusion: DyCrowd框架有效地解决了动态人群三维重建中时序不稳定和严重遮挡的问题，并提供了一个虚拟数据集VirtualCrowd用于验证效果，同时改善了相关领域的研究工具。

Abstract: 3D reconstruction of dynamic crowds in large scenes has become increasingly
important for applications such as city surveillance and crowd analysis.
However, current works attempt to reconstruct 3D crowds from a static image,
causing a lack of temporal consistency and inability to alleviate the typical
impact caused by occlusions. In this paper, we propose DyCrowd, the first
framework for spatio-temporally consistent 3D reconstruction of hundreds of
individuals' poses, positions and shapes from a large-scene video. We design a
coarse-to-fine group-guided motion optimization strategy for occlusion-robust
crowd reconstruction in large scenes. To address temporal instability and
severe occlusions, we further incorporate a VAE (Variational Autoencoder)-based
human motion prior along with a segment-level group-guided optimization. The
core of our strategy leverages collective crowd behavior to address long-term
dynamic occlusions. By jointly optimizing the motion sequences of individuals
with similar motion segments and combining this with the proposed Asynchronous
Motion Consistency (AMC) loss, we enable high-quality unoccluded motion
segments to guide the motion recovery of occluded ones, ensuring robust and
plausible motion recovery even in the presence of temporal desynchronization
and rhythmic inconsistencies. Additionally, in order to fill the gap of no
existing well-annotated large-scene video dataset, we contribute a virtual
benchmark dataset, VirtualCrowd, for evaluating dynamic crowd reconstruction
from large-scene videos. Experimental results demonstrate that the proposed
method achieves state-of-the-art performance in the large-scene dynamic crowd
reconstruction task. The code and dataset will be available for research
purposes.

</details>


### [99] [Stable Diffusion-Based Approach for Human De-Occlusion](https://arxiv.org/abs/2508.12663)
*Seung Young Noh,Ju Yong Chang*

Main category: cs.CV

TL;DR: 该论文探讨了如何通过深度学习模型实现对遮挡区域人体结构和外观的精确预测，提出了一种基于分阶段的掩膜与RGB重建方法，并显著超过现有方法表现。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在对遮挡对象的区域进行精确预测方面面临挑战，该问题对人体遮挡区域的重建任务尤为重要。

Method: 该方法分为两个阶段：首先利用基于扩散的人体先验信息结合遮挡关节热图完成掩膜重建；接着借助视觉问答模型和CLIP编码器提取的人类特定文本特征，指导基于Stable Diffusion的RGB重建，并通过调整解码器缓解像素级退化问题。

Result: 该方法在严重遮挡情况下有效重建人体外观，在掩膜与RGB重建上均优于现有方法，并提升了2D姿态估计和3D重建等下游任务性能。

Conclusion: 提出的方法在重建遮挡人体外观任务中表现优异，通过充分利用人体结构与文本特征，推进了遮挡预测领域的研究。

Abstract: Humans can infer the missing parts of an occluded object by leveraging prior
knowledge and visible cues. However, enabling deep learning models to
accurately predict such occluded regions remains a challenging task.
De-occlusion addresses this problem by reconstructing both the mask and RGB
appearance. In this work, we focus on human de-occlusion, specifically
targeting the recovery of occluded body structures and appearances. Our
approach decomposes the task into two stages: mask completion and RGB
completion. The first stage leverages a diffusion-based human body prior to
provide a comprehensive representation of body structure, combined with
occluded joint heatmaps that offer explicit spatial cues about missing regions.
The reconstructed amodal mask then serves as a conditioning input for the
second stage, guiding the model on which areas require RGB reconstruction. To
further enhance RGB generation, we incorporate human-specific textual features
derived using a visual question answering (VQA) model and encoded via a CLIP
encoder. RGB completion is performed using Stable Diffusion, with decoder
fine-tuning applied to mitigate pixel-level degradation in visible regions -- a
known limitation of prior diffusion-based de-occlusion methods caused by latent
space transformations. Our method effectively reconstructs human appearances
even under severe occlusions and consistently outperforms existing methods in
both mask and RGB completion. Moreover, the de-occluded images generated by our
approach can improve the performance of downstream human-centric tasks, such as
2D pose estimation and 3D human reconstruction. The code will be made publicly
available.

</details>


### [100] [WP-CLIP: Leveraging CLIP to Predict Wölfflin's Principles in Visual Art](https://arxiv.org/abs/2508.12668)
*Abhijay Ghildyal,Li-Yun Wang,Feng Liu*

Main category: cs.CV

TL;DR: 研究利用CLIP模型分析艺术风格中Wölfflin的五个原则，经过微调后实现了对艺术作品的精准评分。


<details>
  <summary>Details</summary>
Motivation: 现有分析方法无法全面预测艺术中Wölfflin的五个原则，通过改进，解决视觉艺术分析中的困难。

Method: 使用预训练的CLIP模型，并通过微调使其能够针对艺术的风格原则进行预测。

Result: 生成了经过微调的WP-CLIP模型，在人工与真实艺术数据集上均表现出很好的泛化能力。

Conclusion: 通过微调VLM模型，可实现自动化的艺术风格分析，具有广泛应用的潜力。

Abstract: W\"olfflin's five principles offer a structured approach to analyzing
stylistic variations for formal analysis. However, no existing metric
effectively predicts all five principles in visual art. Computationally
evaluating the visual aspects of a painting requires a metric that can
interpret key elements such as color, composition, and thematic choices. Recent
advancements in vision-language models (VLMs) have demonstrated their ability
to evaluate abstract image attributes, making them promising candidates for
this task. In this work, we investigate whether CLIP, pre-trained on
large-scale data, can understand and predict W\"olfflin's principles. Our
findings indicate that it does not inherently capture such nuanced stylistic
elements. To address this, we fine-tune CLIP on annotated datasets of real art
images to predict a score for each principle. We evaluate our model, WP-CLIP,
on GAN-generated paintings and the Pandora-18K art dataset, demonstrating its
ability to generalize across diverse artistic styles. Our results highlight the
potential of VLMs for automated art analysis.

</details>


### [101] [Refine-and-Contrast: Adaptive Instance-Aware BEV Representations for Multi-UAV Collaborative Object Detection](https://arxiv.org/abs/2508.12684)
*Zhongyao Li,Peirui Cheng,Liangjin Zhao,Chen Chen,Yundu Li,Zhechao Wang,Xue Yang,Xian Sun,Zhirui Wang*

Main category: cs.CV

TL;DR: 本文提出了AdaBEV框架，通过自适应实例感知的3D检测方法解决了多无人机协作检测中的计算资源受限问题，并在试验中证明了其高效性和准确性。


<details>
  <summary>Details</summary>
Motivation: 针对多无人机协作3D检测在覆盖范围和遮挡处理中的潜力及计算资源受限的挑战，提出一种高效准确的解决方案。

Method: 提出AdaBEV框架，引入Box-Guided Refinement Module (BG-RM)和Instance-Background Contrastive Learning (IBCL)，前者通过2D监督和空间细分优化前景特征，后者通过对比学习强化前景与背景特征的区分。

Result: 在Air-Co-Pred数据集上，AdaBEV在不同模型规模下实现了更优的精度与计算权衡，并在低分辨率输入下也能接近理想性能。

Conclusion: AdaBEV框架能够在低分辨率和低计算开销情况下实现高精度的多无人机协作3D检测，为资源受限环境提供了新的解决方案。

Abstract: Multi-UAV collaborative 3D detection enables accurate and robust perception
by fusing multi-view observations from aerial platforms, offering significant
advantages in coverage and occlusion handling, while posing new challenges for
computation on resource-constrained UAV platforms. In this paper, we present
AdaBEV, a novel framework that learns adaptive instance-aware BEV
representations through a refine-and-contrast paradigm. Unlike existing methods
that treat all BEV grids equally, AdaBEV introduces a Box-Guided Refinement
Module (BG-RM) and an Instance-Background Contrastive Learning (IBCL) to
enhance semantic awareness and feature discriminability. BG-RM refines only BEV
grids associated with foreground instances using 2D supervision and spatial
subdivision, while IBCL promotes stronger separation between foreground and
background features via contrastive learning in BEV space. Extensive
experiments on the Air-Co-Pred dataset demonstrate that AdaBEV achieves
superior accuracy-computation trade-offs across model scales, outperforming
other state-of-the-art methods at low resolutions and approaching upper bound
performance while maintaining low-resolution BEV inputs and negligible
overhead.

</details>


### [102] [TTA-DAME: Test-Time Adaptation with Domain Augmentation and Model Ensemble for Dynamic Driving Conditions](https://arxiv.org/abs/2508.12690)
*Dongjae Jeon,Taeheon Kim,Seongwon Cho,Minhyuk Seo,Jonghyun Choi*

Main category: cs.CV

TL;DR: 本文提出了一种名为TTA-DAME的方法，通过将源域数据增强至目标域，同时引入域判别器和专门的域检测器，解决测试时域适配问题，并在SHIFT Benchmark数据集上展示了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 为应对实际驾驶场景中特别是天气和昼夜变化引起的动态域变化和性能下降问题，提出了一种能够动态适配目标域的模型。

Method: 通过源域数据增强、域判别器和域检测器的设计，以及多个检测器的训练和非极大值抑制整合预测，提出了TTA-DAME方法。

Result: 在SHIFT Benchmark数据集上，TTA-DAME方法显著提升了适配动态目标域的性能。

Conclusion: TTA-DAME证明了利用数据增强和域检测器联合优化能够有效缓解实时情景中的域转移问题，实验证明了其在动态驾驶场景中的适应能力和性能优势。

Abstract: Test-time Adaptation (TTA) poses a challenge, requiring models to dynamically
adapt and perform optimally on shifting target domains. This task is
particularly emphasized in real-world driving scenes, where weather domain
shifts occur frequently. To address such dynamic changes, our proposed method,
TTA-DAME, leverages source domain data augmentation into target domains.
Additionally, we introduce a domain discriminator and a specialized domain
detector to mitigate drastic domain shifts, especially from daytime to
nighttime conditions. To further improve adaptability, we train multiple
detectors and consolidate their predictions through Non-Maximum Suppression
(NMS). Our empirical validation demonstrates the effectiveness of our method,
showing significant performance enhancements on the SHIFT Benchmark.

</details>


### [103] [Multi-Level Knowledge Distillation and Dynamic Self-Supervised Learning for Continual Learning](https://arxiv.org/abs/2508.12692)
*Taeheon Kim,San Kim,Minhyuk Seo,Dongjae Jeon,Wonje Jeong,Jonghyun Choi*

Main category: cs.CV

TL;DR: 本文研究了一个更接近现实的类别增量学习场景，即类别重复增量（CIR），并提出了多层次知识蒸馏（MLKD）和动态自监督损失（SSL）两种方法，显著提升了CIR场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 传统类别增量学习假设任务中仅包含新类别，不够真实。CIR考虑到现实中可以重复接触先前课程，并能利用丰富的未标注数据，提高模型的稳定性和可塑性。

Method: 提出了多层次知识蒸馏（MLKD），通过多种视角（如特征和logits）从多个先前模型蒸馏知识，以维护多样化的知识；还提出动态自监督损失（SSL），加速新类别学习并通过动态权重聚焦于主要任务。

Result: 在CVPR第五届CLVISION挑战赛中取得第2名，表明两个方法能显著提升CIR场景下的学习表现。

Conclusion: 结合MLKD和动态SSL方法，可有效利用未标注数据，同时实现较好的模型稳定性和可塑性，为CIR提供了显著的性能提升。

Abstract: Class-incremental with repetition (CIR), where previously trained classes
repeatedly introduced in future tasks, is a more realistic scenario than the
traditional class incremental setup, which assumes that each task contains
unseen classes. CIR assumes that we can easily access abundant unlabeled data
from external sources, such as the Internet. Therefore, we propose two
components that efficiently use the unlabeled data to ensure the high stability
and the plasticity of models trained in CIR setup. First, we introduce
multi-level knowledge distillation (MLKD) that distills knowledge from multiple
previous models across multiple perspectives, including features and logits, so
the model can maintain much various previous knowledge. Moreover, we implement
dynamic self-supervised loss (SSL) to utilize the unlabeled data that
accelerates the learning of new classes, while dynamic weighting of SSL keeps
the focus of training to the primary task. Both of our proposed components
significantly improve the performance in CIR setup, achieving 2nd place in the
CVPR 5th CLVISION Challenge.

</details>


### [104] [Neural Rendering for Sensor Adaptation in 3D Object Detection](https://arxiv.org/abs/2508.12695)
*Felix Embacher,David Holtz,Jonas Uhrig,Marius Cordts,Markus Enzweiler*

Main category: cs.CV

TL;DR: 该论文研究了不同相机传感器配置对3D目标检测算法的影响，并提出了一个新数据集和适应方案以降低性能损失。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶汽车中不同车型和传感器布局的不可避免差异可能导致跨传感器领域的性能衰退，该问题的解决具有重要意义。

Method: 提出了一个名为CamShift的CARLA模拟数据集，专门用于展示小型车与SUV之间传感器设置的域间差距，同时开发了基于神经渲染的数据驱动传感器适应方法。

Result: 通过实验发现，基于密集鸟瞰图（BEV）后向投影的模型（如BEVFormer）对不同传感器配置更具鲁棒性，同时提出的神经渲染方法能显著提升所有3D目标检测器的跨传感器性能。

Conclusion: 数据驱动的传感器适应方法能有效缓解不同车辆传感器配置引起的性能下降，促进数据重用并降低收集新数据的需求。

Abstract: Autonomous vehicles often have varying camera sensor setups, which is
inevitable due to restricted placement options for different vehicle types.
Training a perception model on one particular setup and evaluating it on a new,
different sensor setup reveals the so-called cross-sensor domain gap, typically
leading to a degradation in accuracy. In this paper, we investigate the impact
of the cross-sensor domain gap on state-of-the-art 3D object detectors. To this
end, we introduce CamShift, a dataset inspired by nuScenes and created in CARLA
to specifically simulate the domain gap between subcompact vehicles and sport
utility vehicles (SUVs). Using CamShift, we demonstrate significant
cross-sensor performance degradation, identify robustness dependencies on model
architecture, and propose a data-driven solution to mitigate the effect. On the
one hand, we show that model architectures based on a dense Bird's Eye View
(BEV) representation with backward projection, such as BEVFormer, are the most
robust against varying sensor configurations. On the other hand, we propose a
novel data-driven sensor adaptation pipeline based on neural rendering, which
can transform entire datasets to match different camera sensor setups. Applying
this approach improves performance across all investigated 3D object detectors,
mitigating the cross-sensor domain gap by a large margin and reducing the need
for new data collection by enabling efficient data reusability across vehicles
with different sensor setups. The CamShift dataset and the sensor adaptation
benchmark are available at https://dmholtz.github.io/camshift/.

</details>


### [105] [Has GPT-5 Achieved Spatial Intelligence? An Empirical Study](https://arxiv.org/abs/2508.13142)
*Zhongang Cai,Yubo Wang,Qingping Sun,Ruisi Wang,Chenyang Gu,Wanqi Yin,Zhiqian Lin,Zhitao Yang,Chen Wei,Xuanke Shi,Kewang Deng,Xiaoyang Han,Zukai Chen,Jiaqi Li,Xiangyu Fan,Hanming Deng,Lewei Lu,Bo Li,Ziwei Liu,Quan Wang,Dahua Lin,Lei Yang*

Main category: cs.CV

TL;DR: 研究发现当前AI模型在空间理解与推理方面仍有较大不足，尽管GPT-5在这一领域表现出色，但仍未能达到人类水平。


<details>
  <summary>Details</summary>
Motivation: 分析最新的先进多模态模型在空间智能领域的表现及其与人类能力的差距。

Method: 提出空间任务分类体系，统一现有基准，并使用八项关键基准对多模态模型进行评估，同时进行定性分析以探讨模型在直观场景中的表现。

Result: 研究显示GPT-5在空间智能上表现空前但仍未达到人类水平，同时明确了当前多模态模型仍面临的主要挑战领域。

Conclusion: 尽管GPT-5展现了强大的空间智能能力，但整体而言，多模态模型在解决高难度空间问题时未能具有明显的优势，显示出该领域仍需进一步突破。

Abstract: Multi-modal models have achieved remarkable progress in recent years.
Nevertheless, they continue to exhibit notable limitations in spatial
understanding and reasoning, which are fundamental capabilities to achieving
artificial general intelligence. With the recent release of GPT-5, allegedly
the most powerful AI model to date, it is timely to examine where the leading
models stand on the path toward spatial intelligence. First, we propose a
comprehensive taxonomy of spatial tasks that unifies existing benchmarks and
discuss the challenges in ensuring fair evaluation. We then evaluate
state-of-the-art proprietary and open-source models on eight key benchmarks, at
a cost exceeding one billion total tokens. Our empirical study reveals that (1)
GPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)
still falls short of human performance across a broad spectrum of tasks.
Moreover, we (3) identify the more challenging spatial intelligence problems
for multi-modal models, and (4) proprietary models do not exhibit a decisive
advantage when facing the most difficult problems. In addition, we conduct a
qualitative evaluation across a diverse set of scenarios that are intuitive for
humans yet fail even the most advanced multi-modal models.

</details>


### [106] [Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection](https://arxiv.org/abs/2508.12711)
*Fanxiao Li,Jiaying Wu,Tingchao Fu,Yunyun Dong,Bingbing Song,Wei Zhou*

Main category: cs.CV

TL;DR: 文章揭示了大规模视觉-语言模型（LVLM）在检测多模态虚假信息（MMD）时面临的新挑战，尤其是在生成性AI驱动的新闻多样化背景下。这种多样化导致了模型感知和证据水平上的漂移，从而显著降低了模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究生成性AI引发的新闻多样化对多模态虚假信息检测的影响，探索生成性AI带来的新挑战和LVLM在应对这些问题时的弱点。

Method: 提出了DriftBench，一个覆盖16,000条新闻实例的基准数据集，用于系统研究生成性AI带来的多层次漂移问题，并设计了三项评测任务：多层次漂移下的真相验证鲁棒性、生成性AI生成的对抗性证据污染的敏感性，以及在多样化输入上的推理一致性分析。

Result: 实验表明，现有六种最先进LVLM检测器在面对多层次漂移时表现显著下降（平均F1减少14.8%），且推理轨迹稳定性降低，对抗性证据注入的情境下表现更加糟糕。

Conclusion: 当前的多模态虚假信息检测系统存在根本性漏洞，在生成性AI时代需要更加鲁棒的方法来应对这些挑战。

Abstract: The proliferation of multimodal misinformation poses growing threats to
public discourse and societal trust. While Large Vision-Language Models (LVLMs)
have enabled recent progress in multimodal misinformation detection (MMD), the
rise of generative AI (GenAI) tools introduces a new challenge: GenAI-driven
news diversity, characterized by highly varied and complex content. We show
that this diversity induces multi-level drift, comprising (1) model-level
misperception drift, where stylistic variations disrupt a model's internal
reasoning, and (2) evidence-level drift, where expression diversity degrades
the quality or relevance of retrieved external evidence. These drifts
significantly degrade the robustness of current LVLM-based MMD systems. To
systematically study this problem, we introduce DriftBench, a large-scale
benchmark comprising 16,000 news instances across six categories of
diversification. We design three evaluation tasks: (1) robustness of truth
verification under multi-level drift; (2) susceptibility to adversarial
evidence contamination generated by GenAI; and (3) analysis of reasoning
consistency across diverse inputs. Experiments with six state-of-the-art
LVLM-based detectors show substantial performance drops (average F1 -14.8%) and
increasingly unstable reasoning traces, with even more severe failures under
adversarial evidence injection. Our findings uncover fundamental
vulnerabilities in existing MMD systems and suggest an urgent need for more
resilient approaches in the GenAI era.

</details>


### [107] [Real-Time Sign Language Gestures to Speech Transcription using Deep Learning](https://arxiv.org/abs/2508.12713)
*Brandone Fonya*

Main category: cs.CV

TL;DR: 本文提出一种实时辅助技术系统，利用深度学习将手语手势转换为文本和语音，提升听力和语言障碍者的沟通能力。


<details>
  <summary>Details</summary>
Motivation: 解决听力和言语障碍者在日常环境中面临的沟通障碍。

Method: 使用基于Sign Language MNIST数据集训练的卷积神经网络（CNN）对手势进行分类，并实时转译为文本和语音。

Result: 实验表明该系统具有高分类精度和实时性能，虽然存在一定的延迟，但仍展现了其实用性。

Conclusion: 该系统是一种可靠且用户友好的工具，可提升手语用户在多样社交环境中的自主性与融入度。

Abstract: Communication barriers pose significant challenges for individuals with
hearing and speech impairments, often limiting their ability to effectively
interact in everyday environments. This project introduces a real-time
assistive technology solution that leverages advanced deep learning techniques
to translate sign language gestures into textual and audible speech. By
employing convolution neural networks (CNN) trained on the Sign Language MNIST
dataset, the system accurately classifies hand gestures captured live via
webcam. Detected gestures are instantaneously translated into their
corresponding meanings and transcribed into spoken language using
text-to-speech synthesis, thus facilitating seamless communication.
Comprehensive experiments demonstrate high model accuracy and robust real-time
performance with some latency, highlighting the system's practical
applicability as an accessible, reliable, and user-friendly tool for enhancing
the autonomy and integration of sign language users in diverse social settings.

</details>


### [108] [Single-Reference Text-to-Image Manipulation with Dual Contrastive Denoising Score](https://arxiv.org/abs/2508.12718)
*Syed Muhmmad Israr,Feng Zhao*

Main category: cs.CV

TL;DR: 基于文本的大规模图像生成模型具备图像合成能力，但应用于真实图像编辑仍存在困难，提出一种基于对比损失的简便框架解决问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以精准描述输入图像细节，且容易引入不必要的改变，限制了真实图像编辑的效果。

Method: 提出Dual Contrastive Denoising Score框架，通过引入对比损失函数，利用扩散模型的自注意力层中间表示，实现内容修改与结构保持。

Result: 新方法实现了灵活内容编辑与结构保持，在零样本图像变换及真实图像编辑性能上超越现有方法。

Conclusion: 该方法无需额外训练即可利用预训练扩散模型，具备更强的编辑能力和一致性。

Abstract: Large-scale text-to-image generative models have shown remarkable ability to
synthesize diverse and high-quality images. However, it is still challenging to
directly apply these models for editing real images for two reasons. First, it
is difficult for users to come up with a perfect text prompt that accurately
describes every visual detail in the input image. Second, while existing models
can introduce desirable changes in certain regions, they often dramatically
alter the input content and introduce unexpected changes in unwanted regions.
To address these challenges, we present Dual Contrastive Denoising Score, a
simple yet powerful framework that leverages the rich generative prior of
text-to-image diffusion models. Inspired by contrastive learning approaches for
unpaired image-to-image translation, we introduce a straightforward dual
contrastive loss within the proposed framework. Our approach utilizes the
extensive spatial information from the intermediate representations of the
self-attention layers in latent diffusion models without depending on auxiliary
networks. Our method achieves both flexible content modification and structure
preservation between input and output images, as well as zero-shot
image-to-image translation. Through extensive experiments, we show that our
approach outperforms existing methods in real image editing while maintaining
the capability to directly utilize pretrained text-to-image diffusion models
without further training.

</details>


### [109] [Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian Splatting](https://arxiv.org/abs/2508.12720)
*Kangjie Chen,Yingji Zhong,Zhihao Li,Jiaqi Lin,Youyu Chen,Minghan Qin,Haoqian Wang*

Main category: cs.CV

TL;DR: 本文分析了 3D 高斯粒子化（3DGS）在稀疏视图场景下的外观伪影问题，提出了共适配（Co-Adaptation）现象，并提供了两种轻量化解决方案，有效减少伪影问题。


<details>
  <summary>Details</summary>
Motivation: 3DGS 在稀疏视图场景下尽管训练视图呈现真实感，但在新视图中可能出现外观伪影，这表明需要更好地理解和优化 3DGS 的适用条件。

Method: 提出“共适配评分（CA）”指标衡量高斯粒子的相互纠缠程度，通过随机高斯子集计算同一视角的像素方差。同时，提出高斯随机丢弃和不透明度噪声注入两种方法来缓解共适配问题。

Result: 验证了高斯随机丢弃和不透明度噪声注入两种策略在多种方法和基准测试下的有效性，可显著减少伪影问题。

Conclusion: 本文提供了对稀疏视图 3DGS 中共适配效应的新见解，希望能够推动这一领域更全面的理解和优化。

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated impressive performance in novel
view synthesis under dense-view settings. However, in sparse-view scenarios,
despite the realistic renderings in training views, 3DGS occasionally manifests
appearance artifacts in novel views. This paper investigates the appearance
artifacts in sparse-view 3DGS and uncovers a core limitation of current
approaches: the optimized Gaussians are overly-entangled with one another to
aggressively fit the training views, which leads to a neglect of the real
appearance distribution of the underlying scene and results in appearance
artifacts in novel views. The analysis is based on a proposed metric, termed
Co-Adaptation Score (CA), which quantifies the entanglement among Gaussians,
i.e., co-adaptation, by computing the pixel-wise variance across multiple
renderings of the same viewpoint, with different random subsets of Gaussians.
The analysis reveals that the degree of co-adaptation is naturally alleviated
as the number of training views increases. Based on the analysis, we propose
two lightweight strategies to explicitly mitigate the co-adaptation in
sparse-view 3DGS: (1) random gaussian dropout; (2) multiplicative noise
injection to the opacity. Both strategies are designed to be plug-and-play, and
their effectiveness is validated across various methods and benchmarks. We hope
that our insights into the co-adaptation effect will inspire the community to
achieve a more comprehensive understanding of sparse-view 3DGS.

</details>


### [110] [Frequency-Driven Inverse Kernel Prediction for Single Image Defocus Deblurring](https://arxiv.org/abs/2508.12736)
*Ying Zhang,Xiongxin Tang,Chongyi Li,Qiao Chen,Yuquan Wu*

Main category: cs.CV

TL;DR: 提出了一种频率驱动的逆核预测网络（FDIKP），通过频域表示增强模糊核建模的结构辨识能力，提高单张图像的去模糊性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于空间特征进行模糊核估计，但在严重模糊区域性能较差，因此需要新的方法提高模糊建模的精度和稳定性。

Method: 提出一种结合频域表示与位置自适应卷积的频率驱动逆核预测网络（FDIKP），并设计双分支逆核预测策略和双域尺度递归模块，逐步提升去模糊效果。

Result: 实验表明，提出的方法在去模糊性能上优于现有方法。

Conclusion: 频域增强模糊识别和逆核预测的结合能有效解决空间特征不足造成的性能下降问题，为单图像去模糊提供了更出色的方法。

Abstract: Single image defocus deblurring aims to recover an all-in-focus image from a
defocus counterpart, where accurately modeling spatially varying blur kernels
remains a key challenge. Most existing methods rely on spatial features for
kernel estimation, but their performance degrades in severely blurry regions
where local high-frequency details are missing. To address this, we propose a
Frequency-Driven Inverse Kernel Prediction network (FDIKP) that incorporates
frequency-domain representations to enhance structural identifiability in
kernel modeling. Given the superior discriminative capability of the frequency
domain for blur modeling, we design a Dual-Branch Inverse Kernel Prediction
(DIKP) strategy that improves the accuracy of kernel estimation while
maintaining stability. Moreover, considering the limited number of predicted
inverse kernels, we introduce a Position Adaptive Convolution (PAC) to enhance
the adaptability of the deconvolution process. Finally, we propose a
Dual-Domain Scale Recurrent Module (DSRM) to fuse deconvolution results and
progressively improve deblurring quality from coarse to fine. Extensive
experiments demonstrate that our method outperforms existing approaches. Code
will be made publicly available.

</details>


### [111] [DCSCR: A Class-Specific Collaborative Representation based Network for Image Set Classification](https://arxiv.org/abs/2508.12745)
*Xizhan Gao,Wei Hu*

Main category: cs.CV

TL;DR: 提出了一种新的方法，名为DCSCR，用于解决图像集分类中的挑战，并成功在多个数据集上验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 现有的ISC方法无法同时有效学习特征表示和测量图像集间的相似性，特别是在少样本场景下表现有限。

Method: 结合传统和深度学习方法，提出了DCSCR网络，包含深度特征提取模块、全局特征学习模块以及基于类特定协同表示的度量学习模块，采用新的CSCR对比损失函数来优化。

Result: 在多个少样本ISC数据集上，相比现有方法展现出显著的效果提升。

Conclusion: DCSCR方法能同时学习图像集的帧级和概念级特征表示，提升了少样本ISC的分类效果，为该领域提供了新视角。

Abstract: Image set classification (ISC), which can be viewed as a task of comparing
similarities between sets consisting of unordered heterogeneous images with
variable quantities and qualities, has attracted growing research attention in
recent years. How to learn effective feature representations and how to explore
the similarities between different image sets are two key yet challenging
issues in this field. However, existing traditional ISC methods classify image
sets based on raw pixel features, ignoring the importance of feature learning.
Existing deep ISC methods can learn deep features, but they fail to adaptively
adjust the features when measuring set distances, resulting in limited
performance in few-shot ISC. To address the above issues, this paper combines
traditional ISC methods with deep models and proposes a novel few-shot ISC
approach called Deep Class-specific Collaborative Representation (DCSCR)
network to simultaneously learn the frame- and concept-level feature
representations of each image set and the distance similarities between
different sets. Specifically, DCSCR consists of a fully convolutional deep
feature extractor module, a global feature learning module, and a
class-specific collaborative representation-based metric learning module. The
deep feature extractor and global feature learning modules are used to learn
(local and global) frame-level feature representations, while the
class-specific collaborative representation-based metric learning module is
exploit to adaptively learn the concept-level feature representation of each
image set and thus obtain the distance similarities between different sets by
developing a new CSCR-based contrastive loss function. Extensive experiments on
several well-known few-shot ISC datasets demonstrate the effectiveness of the
proposed method compared with some state-of-the-art image set classification
algorithms.

</details>


### [112] [D2-Mamba: Dual-Scale Fusion and Dual-Path Scanning with SSMs for Shadow Removal](https://arxiv.org/abs/2508.12750)
*Linhao Li,Boya Jin,Zizhe Li,Lanqing Guo,Hao Cheng,Bo Li,Yongfeng Dong*

Main category: cs.CV

TL;DR: 提出了一种基于Mamba框架的网络，通过双尺度融合和双路径扫描改进阴影区域的去除效果，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 阴影去除是一种需要利用非阴影区信息同时针对性地调整阴影区域的复杂任务，现有方法在整合上下文信息和区域转换上存在不足。

Method: 利用双尺度融合和双路径扫描设计了一个Mamba框架的网络，其中融合低分辨率特征降低边界伪影，通过适应性的扫描策略捕捉更好地全局和区域特征。

Result: 实验结果表明，该方法在阴影去除基准测试中显著优于现有技术。

Conclusion: 提出的网络通过结合多尺度和自适应建模，提升了阴影去除任务的表现，证明了方法的有效性。

Abstract: Shadow removal aims to restore images that are partially degraded by shadows,
where the degradation is spatially localized and non-uniform. Unlike general
restoration tasks that assume global degradation, shadow removal can leverage
abundant information from non-shadow regions for guidance. However, the
transformation required to correct shadowed areas often differs significantly
from that of well-lit regions, making it challenging to apply uniform
correction strategies. This necessitates the effective integration of non-local
contextual cues and adaptive modeling of region-specific transformations. To
this end, we propose a novel Mamba-based network featuring dual-scale fusion
and dual-path scanning to selectively propagate contextual information based on
transformation similarity across regions. Specifically, the proposed Dual-Scale
Fusion Mamba Block (DFMB) enhances multi-scale feature representation by fusing
original features with low-resolution features, effectively reducing boundary
artifacts. The Dual-Path Mamba Group (DPMG) captures global features via
horizontal scanning and incorporates a mask-aware adaptive scanning strategy,
which improves structural continuity and fine-grained region modeling.
Experimental results demonstrate that our method significantly outperforms
existing state-of-the-art approaches on shadow removal benchmarks.

</details>


### [113] [CLAIRE-DSA: Fluoroscopic Image Classification for Quality Assurance of Computer Vision Pipelines in Acute Ischemic Stroke](https://arxiv.org/abs/2508.12755)
*Cristo J. van den Berg,Frank G. te Nijenhuis,Mirre J. Blaauboer,Daan T. W. van Erp,Carlijn M. Keppels,Matthijs van der Sluijs,Bob Roozenbeek,Wim van Zwam,Sandra Cornelissen,Danny Ruijters,Ruisheng Su,Theo van Walsum*

Main category: cs.CV

TL;DR: CLAIRE-DSA是一个基于深度学习的框架，旨在分类机械取栓(MT)过程中获得的图像质量属性，其可以提升图像质量控制和工作流程优化。


<details>
  <summary>Details</summary>
Motivation: 目前急性缺血性中风(MT)的影像质量较差会导致计算机视觉模型效果下降，因此需要一个自动化工具来准确分类图像属性，并支持质量控制和工作流程优化。

Method: CLAIRE-DSA使用了预训练的ResNet主干网络，并通过微调后训练多个分类器来预测9种图像属性。模型使用了包含1,758个注释瓷板影像(MinIP)数据集进行训练和测试。

Result: 在预测图像属性时，模型的ROC-AUC范围为0.91–0.98，Precision范围为0.70–1.00。在分割任务中过滤低质量图像后，分割成功率从42%提高到69% (p < 0.001)。

Conclusion: CLAIRE-DSA展示了其在DSA图像分类、图像注释及质量控制中的强大潜力，可支持临床和研究工作。

Abstract: Computer vision models can be used to assist during mechanical thrombectomy
(MT) for acute ischemic stroke (AIS), but poor image quality often degrades
performance. This work presents CLAIRE-DSA, a deep learning--based framework
designed to categorize key image properties in minimum intensity projections
(MinIPs) acquired during MT for AIS, supporting downstream quality control and
workflow optimization. CLAIRE-DSA uses pre-trained ResNet backbone models,
fine-tuned to predict nine image properties (e.g., presence of contrast,
projection angle, motion artefact severity). Separate classifiers were trained
on an annotated dataset containing $1,758$ fluoroscopic MinIPs. The model
achieved excellent performance on all labels, with ROC-AUC ranging from $0.91$
to $0.98$, and precision ranging from $0.70$ to $1.00$. The ability of
CLAIRE-DSA to identify suitable images was evaluated on a segmentation task by
filtering poor quality images and comparing segmentation performance on
filtered and unfiltered datasets. Segmentation success rate increased from
$42%$ to $69%$, $p < 0.001$. CLAIRE-DSA demonstrates strong potential as an
automated tool for accurately classifying image properties in DSA series of
acute ischemic stroke patients, supporting image annotation and quality control
in clinical and research applications. Source code is available at
https://gitlab.com/icai-stroke-lab/wp3_neurointerventional_ai/claire-dsa.

</details>


### [114] [Harnessing Group-Oriented Consistency Constraints for Semi-Supervised Semantic Segmentation in CdZnTe Semiconductors](https://arxiv.org/abs/2508.12766)
*Peihao Li,Yan Fang,Man Liu,Huihui Bai,Anhong Wang,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种解决CdZnTe半导体图像标注问题的新方法，命名为ICAF，通过引入组内一致性增强框架，提高了数据利用效率和分割精度。


<details>
  <summary>Details</summary>
Motivation: 针对CdZnTe材料图像由于缺陷边界低对比度带来的标注难题，现有的半监督语义分割方法受“一对一”关系的限制，容易在低对比度区域积累错误，因此需要一种能处理“多对一”关系的创新方法。

Method: 提出了ICAF框架，包括两个关键模块：视图增强模块（VAM）用于动态综合多视图以改善边界细节，视图修正模块（VCM）通过信息交互减少噪声并突显显著区域。此外，通过组内视图采样（IVS）建立了组一致性基线，并引入伪标签修正网络（PCN）增强组内一致性表示。

Result: 在使用ResNet-101作为主干并搭配DeepLabV3+的情况下，ICAF框架在仅使用两组带注释数据（0.5%的标注数据）情况下，在CdZnTe数据集上获得了70.6%的mIoU。

Conclusion: 实验结果表明，所提出的ICAF框架显著提高了CdZnTe半导体图像的分割表现，尤其在标注数据极少的情况下展现了其优越性，提供了一种有效处理“多对一”关系的解决方案。

Abstract: Labeling Cadmium Zinc Telluride (CdZnTe) semiconductor images is challenging
due to the low-contrast defect boundaries, necessitating annotators to
cross-reference multiple views. These views share a single ground truth (GT),
forming a unique ``many-to-one'' relationship. This characteristic renders
advanced semi-supervised semantic segmentation (SSS) methods suboptimal, as
they are generally limited by a ``one-to-one'' relationship, where each image
is independently associated with its GT. Such limitation may lead to error
accumulation in low-contrast regions, further exacerbating confirmation bias.
To address this issue, we revisit the SSS pipeline from a group-oriented
perspective and propose a human-inspired solution: the Intra-group Consistency
Augmentation Framework (ICAF). First, we experimentally validate the inherent
consistency constraints within CdZnTe groups, establishing a group-oriented
baseline using the Intra-group View Sampling (IVS). Building on this insight,
we introduce the Pseudo-label Correction Network (PCN) to enhance consistency
representation, which consists of two key modules. The View Augmentation Module
(VAM) improves boundary details by dynamically synthesizing a boundary-aware
view through the aggregation of multiple views. In the View Correction Module
(VCM), this synthesized view is paired with other views for information
interaction, effectively emphasizing salient regions while minimizing noise.
Extensive experiments demonstrate the effectiveness of our solution for CdZnTe
materials. Leveraging DeepLabV3+ with a ResNet-101 backbone as our segmentation
model, we achieve a 70.6\% mIoU on the CdZnTe dataset using only 2
group-annotated data (5\textperthousand). The code is available at
\href{https://github.com/pipixiapipi/ICAF}{https://github.com/pipixiapipi/ICAF}.

</details>


### [115] [SocialTrack: Multi-Object Tracking in Complex Urban Traffic Scenes Inspired by Social Behavior](https://arxiv.org/abs/2508.12777)
*Wenguang Tao,Xiaotian Wang,Tian Yan,Jie Yan,Guodong Li,Kun Bai*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的多目标跟踪框架SocialTrack，旨在提升复杂城市交通环境中小目标的跟踪准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 针对无人机视角下的多目标跟踪中小目标尺度变化、遮挡、非线性交叉运动及运动模糊等挑战。

Method: 提出了一种集成多尺度特征增强、多目标跟踪、速度自适应Cubature卡曼滤波、社会群运动补偿策略和时空记忆预测的多目标跟踪框架。

Result: 在UAVDT和MOT17数据集上的实验表明，SocialTrack在多个关键指标上优于SOTA方法，显著提升了MOTA和IDF1等核心性能。

Conclusion: SocialTrack具有卓越的鲁棒性和适配性，可与现有跟踪器无缝集成以进一步提升性能。

Abstract: As a key research direction in the field of multi-object tracking (MOT),
UAV-based multi-object tracking has significant application value in the
analysis and understanding of urban intelligent transportation systems.
However, in complex UAV perspectives, challenges such as small target scale
variations, occlusions, nonlinear crossing motions, and motion blur severely
hinder the stability of multi-object tracking. To address these challenges,
this paper proposes a novel multi-object tracking framework, SocialTrack, aimed
at enhancing the tracking accuracy and robustness of small targets in complex
urban traffic environments. The specialized small-target detector enhances the
detection performance by employing a multi-scale feature enhancement mechanism.
The Velocity Adaptive Cubature Kalman Filter (VACKF) improves the accuracy of
trajectory prediction by incorporating a velocity dynamic modeling mechanism.
The Group Motion Compensation Strategy (GMCS) models social group motion priors
to provide stable state update references for low-quality tracks, significantly
improving the target association accuracy in complex dynamic environments.
Furthermore, the Spatio-Temporal Memory Prediction (STMP) leverages historical
trajectory information to predict the future state of low-quality tracks,
effectively mitigating identity switching issues. Extensive experiments on the
UAVDT and MOT17 datasets demonstrate that SocialTrack outperforms existing
state-of-the-art (SOTA) methods across several key metrics. Significant
improvements in MOTA and IDF1, among other core performance indicators,
highlight its superior robustness and adaptability. Additionally, SocialTrack
is highly modular and compatible, allowing for seamless integration with
existing trackers to further enhance performance.

</details>


### [116] [Leveraging Diffusion Models for Stylization using Multiple Style Images](https://arxiv.org/abs/2508.12784)
*Dan Ruta,Abdelaziz Djelouah,Raphael Ortiz,Christopher Schroers*

Main category: cs.CV

TL;DR: 提出了一种利用多种风格图像和统计特性对齐的新方法，大幅改进图像风格迁移的效果。


<details>
  <summary>Details</summary>
Motivation: 现有图像风格迁移方法在风格匹配准确性和处理多风格图像能力方面存在不足，而且容易导致内容与风格的非期望性纠缠。

Method: 通过引入多风格图像、图像提示适配器以及在去噪过程中对特征的统计对齐，采用跨注意力层与自注意力层的干预方法，并使用聚类来提取风格样本中的代表性特征集。

Result: 实验结果表明，提出的方法在图像风格化任务上达到了当前最先进的性能。

Conclusion: 新方法有效改进了风格迁移的准确性与表达能力，为多风格图像的综合处理提供了新思路。

Abstract: Recent advances in latent diffusion models have enabled exciting progress in
image style transfer. However, several key issues remain. For example, existing
methods still struggle to accurately match styles. They are often limited in
the number of style images that can be used. Furthermore, they tend to entangle
content and style in undesired ways. To address this, we propose leveraging
multiple style images which helps better represent style features and prevent
content leaking from the style images. We design a method that leverages both
image prompt adapters and statistical alignment of the features during the
denoising process. With this, our approach is designed such that it can
intervene both at the cross-attention and the self-attention layers of the
denoising UNet. For the statistical alignment, we employ clustering to distill
a small representative set of attention features from the large number of
attention values extracted from the style samples. As demonstrated in our
experimental section, the resulting method achieves state-of-the-art results
for stylization.

</details>


### [117] [Vehicle detection from GSV imagery: Predicting travel behaviour for cycling and motorcycling using Computer Vision](https://arxiv.org/abs/2508.12794)
*Kyriaki,Kokka,Rahul Goel,Ali Abbas,Kerry A. Nice,Luca Martial,SM Labib,Rihuan Ke,Carola Bibiane Schönlieb,James Woodcock*

Main category: cs.CV

TL;DR: 该研究利用深度学习分析全球范围内街景图像，评估城市间骑行与骑摩托车行为的模式分布，并开发了预测模型来估算城市的交通方式分布。


<details>
  <summary>Details</summary>
Motivation: 目前有关全球范围内骑行和骑摩托车行为的比较数据较为稀缺，理解这些行为对健康和城市规划的影响非常重要。本研究旨在使用新的计算机视觉方法来填补这一数据空白。

Method: 使用了来自185个全球城市的谷歌街景图像，通过YOLOv4模型检测自行车和摩托车出现的频率，并结合城市级模式份额数据和人口密度，用贝塔回归模型来预测交通方式比例。

Result: YOLOv4对街景图像中自行车和摩托车检测达到了89%的平均精准度。预测的骑行和摩托车模式占比的R平方值分别为0.614和0.612，预测误差中位绝对值分别为1.3%和1.4%。

Conclusion: 通过结合计算机视觉和街景图像，研究显示这是捕捉全球范围内旅行方式和活动的重要补充方法，尤其是在缺乏传统数据的城市中。

Abstract: Transportation influence health by shaping exposure to physical activity, air
pollution and injury risk.Comparative data on cycling and motorcycling
behaviours is scarce, particularly at a global scale.Street view imagery, such
as Google Street View (GSV), combined with computer vision, is a valuable
resource for efficiently capturing travel behaviour data.This study
demonstrates a novel approach using deep learning on street view images to
estimate cycling and motorcycling levels across diverse cities worldwide.We
utilized data from 185 global cities.The data on mode shares of cycling and
motorcycling estimated using travel surveys or censuses.We used GSV images to
detect cycles and motorcycles in sampled locations, using 8000 images per
city.The YOLOv4 model, fine-tuned using images from six cities, achieved a mean
average precision of 89% for detecting cycles and motorcycles in GSV images.A
global prediction model was developed using beta regression with city-level
mode shares as outcome, with log transformed explanatory variables of counts of
GSV-detected images with cycles and motorcycles, while controlling for
population density.We found strong correlations between GSV motorcycle counts
and motorcycle mode share (0.78) and moderate correlations between GSV cycle
counts and cycling mode share (0.51).Beta regression models predicted mode
shares with $R^2$ values of 0.614 for cycling and 0.612 for motorcycling,
achieving median absolute errors (MDAE) of 1.3% and 1.4%,
respectively.Scatterplots demonstrated consistent prediction accuracy, though
cities like Utrecht and Cali were outliers.The model was applied to 60 cities
globally for which we didn't have recent mode share data.We provided estimates
for some cities in the Middle East, Latin America and East Asia.With computer
vision, GSV images capture travel modes and activity, providing insights
alongside traditional data sources.

</details>


### [118] [Morphological classification of eclipsing binary stars using computer vision methods](https://arxiv.org/abs/2508.12802)
*Štefan Parimucha,Maksim Gabdeev,Yanna Markus,Martin Vaňko,Pavol Gajdoš*

Main category: cs.CV

TL;DR: 本研究利用计算机视觉方法对双星的光变曲线进行分类，取得了很高的准确率（>96%）在多种观测数据上实现了优异表现，但在自动检测斑点的任务中出现了性能不足。


<details>
  <summary>Details</summary>
Motivation: 为提高双星光变曲线分类精度，并应对大规模调查中的自动化需求，研究如何利用深度学习模型和创新图像表示方法。

Method: 通过使用训练过的ResNet50和Vision Transformer模型，将相位折叠的光曲线转换为极坐标并结合hexbin可视化。采用两阶段的分类方法：第一阶段定义星系统类型，第二阶段检测斑点。

Result: 模型在分类星系统的验证数据上准确率超过96%，在OGLE、DEBCat和WUMaCat等实际观测数据中表现卓越（最高可达100%）。但自动斑点检测性能不理想，未能有效识别微小光学特征。

Conclusion: 研究表明计算机视觉能有效用于大规模调查中双星的形态分类，但自动斑点检测仍需进一步研究改进。

Abstract: We present an application of computer vision methods to classify the light
curves of eclipsing binaries (EB). We have used pre-trained models based on
convolutional neural networks ($\textit{ResNet50}$) and vision transformers
($\textit{vit\_base\_patch16\_224}$), which were fine-tuned on images created
from synthetic datasets. To improve model generalisation and reduce
overfitting, we developed a novel image representation by transforming
phase-folded light curves into polar coordinates combined with hexbin
visualisation. Our hierarchical approach in the first stage classifies systems
into detached and overcontact types, and in the second stage identifies the
presence or absence of spots. The binary classification models achieved high
accuracy ($>96\%$) on validation data across multiple passbands (Gaia~$G$, $I$,
and $TESS$) and demonstrated strong performance ($>94\%$, up to $100\%$ for
$TESS$) when tested on extensive observational data from the OGLE, DEBCat, and
WUMaCat catalogues. While the primary binary classification was highly
successful, the secondary task of automated spot detection performed poorly,
revealing a significant limitation of our models for identifying subtle
photometric features. This study highlights the potential of computer vision
for EB morphological classification in large-scale surveys, but underscores the
need for further research into robust, automated spot detection.

</details>


### [119] [Next Visual Granularity Generation](https://arxiv.org/abs/2508.12811)
*Yikai Wang,Zhouxia Wang,Zhonghua Wu,Qingyi Tao,Kang Liao,Chen Change Loy*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的图像生成方法，通过将图像分解为具有不同视觉粒度的结构化序列逐步进行生成，并展示了在ImageNet数据集上的性能提升。


<details>
  <summary>Details</summary>
Motivation: 希望通过更细粒度的层次表征来改进图像生成的控制性和效果。

Method: 引入了Next Visual Granularity (NVG)生成框架，从空白图像开始逐步生成，分层次地从全局布局到细节完成图像构建。

Result: 与VAR系列相比，NVG在FID分数上表现更优（例如从3.30提高到3.03），并且表现出可扩展性。

Conclusion: NVG框架提高了图像生成的质量和控制性，具有显著的潜力和应用前景。

Abstract: We propose a novel approach to image generation by decomposing an image into
a structured sequence, where each element in the sequence shares the same
spatial resolution but differs in the number of unique tokens used, capturing
different level of visual granularity. Image generation is carried out through
our newly introduced Next Visual Granularity (NVG) generation framework, which
generates a visual granularity sequence beginning from an empty image and
progressively refines it, from global layout to fine details, in a structured
manner. This iterative process encodes a hierarchical, layered representation
that offers fine-grained control over the generation process across multiple
granularity levels. We train a series of NVG models for class-conditional image
generation on the ImageNet dataset and observe clear scaling behavior. Compared
to the VAR series, NVG consistently outperforms it in terms of FID scores (3.30
-> 3.03, 2.57 ->2.44, 2.09 -> 2.06). We also conduct extensive analysis to
showcase the capability and potential of the NVG framework. Our code and models
will be released.

</details>


### [120] [SIS-Challenge: Event-based Spatio-temporal Instance Segmentation Challenge at the CVPR 2025 Event-based Vision Workshop](https://arxiv.org/abs/2508.12813)
*Friedhelm Hamann,Emil Mededovic,Fabian Gülhan,Yuli Wu,Johannes Stegmaier,Jing He,Yiqing Wang,Kexin Zhang,Lingling Li,Licheng Jiao,Mengru Ma,Hongxiang Huang,Yuhao Yan,Hongwei Ren,Xiaopeng Lin,Yulong Huang,Bojun Cheng,Se Hyun Lee,Gyu Sung Ham,Kanghan Oh,Gi Hyun Lim,Boxuan Yang,Bowen Du,Guillermo Gallego*

Main category: cs.CV

TL;DR: 本文介绍了Spatio-temporal Instance Segmentation (SIS)挑战的概述，包括任务、数据集、比赛详情和结果，并详细描述了前五名队伍的方法。相关代码资源也已开放。


<details>
  <summary>Details</summary>
Motivation: 旨在以空间和时间对齐的事件相机和灰度相机数据为基础，预测精准的像素级目标类别分割掩码。

Method: 作者概述了比赛的任务和数据集，详细分析了排名前五队伍使用的方法，鼓励公开代码和资源以推进该领域研究。

Result: 文章报告了挑战结果和排名前五的技术方案。

Conclusion: 挑战展示了事件视觉和灰度相机数据在目标定位与分割中的潜力，并推动了相关技术的发展。

Abstract: We present an overview of the Spatio-temporal Instance Segmentation (SIS)
challenge held in conjunction with the CVPR 2025 Event-based Vision Workshop.
The task is to predict accurate pixel-level segmentation masks of defined
object classes from spatio-temporally aligned event camera and grayscale camera
data. We provide an overview of the task, dataset, challenge details and
results. Furthermore, we describe the methods used by the top-5 ranking teams
in the challenge. More resources and code of the participants' methods are
available here:
https://github.com/tub-rip/MouseSIS/blob/main/docs/challenge_results.md

</details>


### [121] [DEEP-SEA: Deep-Learning Enhancement for Environmental Perception in Submerged Aquatics](https://arxiv.org/abs/2508.12824)
*Shuang Chen,Ronald Thenius,Farshad Arvin,Amir Atapour-Abarghouei*

Main category: cs.CV

TL;DR: 提出了一个名为DEEP-SEA的新型深度学习模型，用于提高水下图像质量，解决光散射、吸收和浑浊带来的问题。


<details>
  <summary>Details</summary>
Motivation: 水下监测需要高质量的视觉数据支持，但环境中的光散射、吸收和浑浊影响了图像质量，导致观测困难。

Method: 提出了DEEP-SEA模型，该模型通过双频增强自注意力空间与频率调节模块改善频域特征表达，同时保留空间信息以增强图像质量。

Result: 在EUVP和LSUI数据集上的实验表明，DEEP-SEA在图像细节恢复和结构一致性方面优于现有技术。

Conclusion: DEEP-SEA有效缓解了水下视觉退化问题，有助于提高水下监测平台的可靠性，为生态观测、物种识别和自主导航提供更精确的数据支持。

Abstract: Continuous and reliable underwater monitoring is essential for assessing
marine biodiversity, detecting ecological changes and supporting autonomous
exploration in aquatic environments. Underwater monitoring platforms rely on
mainly visual data for marine biodiversity analysis, ecological assessment and
autonomous exploration. However, underwater environments present significant
challenges due to light scattering, absorption and turbidity, which degrade
image clarity and distort colour information, which makes accurate observation
difficult. To address these challenges, we propose DEEP-SEA, a novel deep
learning-based underwater image restoration model to enhance both low- and
high-frequency information while preserving spatial structures. The proposed
Dual-Frequency Enhanced Self-Attention Spatial and Frequency Modulator aims to
adaptively refine feature representations in frequency domains and
simultaneously spatial information for better structural preservation. Our
comprehensive experiments on EUVP and LSUI datasets demonstrate the superiority
over the state of the art in restoring fine-grained image detail and structural
consistency. By effectively mitigating underwater visual degradation, DEEP-SEA
has the potential to improve the reliability of underwater monitoring platforms
for more accurate ecological observation, species identification and autonomous
navigation.

</details>


### [122] [Multi-source Multimodal Progressive Domain Adaption for Audio-Visual Deception Detection](https://arxiv.org/abs/2508.12842)
*Ronghao Lin,Sijie Mai,Ying Zeng,Qiaolin He,Aolin Xiong,Haifeng Hu*

Main category: cs.CV

TL;DR: 本文提出了一种用于1st MMDD挑战赛的多源多模态渐进性域适应框架（MMPDA），实现了在多模态数据集上的领域迁移，取得了Top-2的表现。


<details>
  <summary>Details</summary>
Motivation: 为了解决源域和目标域之间的领域迁移问题，开发能够在多模态数据中实现域对齐的方法。

Method: 提出了MMPDA框架，通过逐步对齐特征层和决策层，从多个来源域迁移音视频知识到目标域。

Result: 实验结果表明，MMPDA方法在准确率和F1分数上表现优异，在比赛阶段2中分别达到60.43%和56.99%。

Conclusion: 本文提出的MMPDA框架有效克服了多模态数据域迁移问题，在比赛中展现优异性能，并提供了源代码供社区使用。

Abstract: This paper presents the winning approach for the 1st MultiModal Deception
Detection (MMDD) Challenge at the 1st Workshop on Subtle Visual Computing
(SVC). Aiming at the domain shift issue across source and target domains, we
propose a Multi-source Multimodal Progressive Domain Adaptation (MMPDA)
framework that transfers the audio-visual knowledge from diverse source domains
to the target domain. By gradually aligning source and the target domain at
both feature and decision levels, our method bridges domain shifts across
diverse multimodal datasets. Extensive experiments demonstrate the
effectiveness of our approach securing Top-2 place. Our approach reaches 60.43%
on accuracy and 56.99\% on F1-score on competition stage 2, surpassing the 1st
place team by 5.59% on F1-score and the 3rd place teams by 6.75% on accuracy.
Our code is available at https://github.com/RH-Lin/MMPDA.

</details>


### [123] [Cross-Domain Few-Shot Learning via Multi-View Collaborative Optimization with Vision-Language Models](https://arxiv.org/abs/2508.12861)
*Dexia Chen,Wentao Zhang,Qianjie Zhu,Ping Hu,Weibing Li,Tong Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为CoMuCo的新型微调策略，用于提升视觉语言模型(VLMs)在跨领域任务中的表现，尤其是对不同于自然图像的领域。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLMs的迁移学习方法在自然图像识别任务中表现优秀，但其在涉及跨领域的少样本任务中表现有限，因此需要开发更为鲁棒的策略。

Method: 提出了一种一致性引导的多视图协作优化（CoMuCo）微调策略，利用两个互为补充的专家模块提取多视图特征，并结合先验知识一致性约束及信息几何共识机制提高特征学习的鲁棒性。同时，建立了一个新型跨领域少样本基准以全面评估模型表现。

Result: 在现有基准和新提出的基准上，经实验证明CoMuCo在少样本任务中性能优于现有方法。

Conclusion: CoMuCo能够有效提升视觉语言模型在跨领域少样本任务的表现，新基准的建立为未来研究提供了更多方向。

Abstract: Vision-language models (VLMs) pre-trained on natural image and language data,
such as CLIP, have exhibited significant potential in few-shot image
recognition tasks, leading to development of various efficient transfer
learning methods. These methods exploit inherent pre-learned knowledge in VLMs
and have achieved strong performance on standard image datasets. However, their
effectiveness is often limited when confronted with cross-domain tasks where
imaging domains differ from natural images. To address this limitation, we
propose Consistency-guided Multi-view Collaborative Optimization (CoMuCo), a
novel fine-tuning strategy for VLMs. This strategy employs two functionally
complementary expert modules to extract multi-view features, while
incorporating prior knowledge-based consistency constraints and information
geometry-based consensus mechanisms to enhance the robustness of feature
learning. Additionally, a new cross-domain few-shot benchmark is established to
help comprehensively evaluate methods on imaging domains distinct from natural
images. Extensive empirical evaluations on both existing and newly proposed
benchmarks suggest CoMuCo consistently outperforms current methods in few-shot
tasks. The code and benchmark will be released.

</details>


### [124] [Preserve and Sculpt: Manifold-Aligned Fine-tuning of Vision-Language Models for Few-Shot Learning](https://arxiv.org/abs/2508.12877)
*Dexia Chen,Qianjie Zhu,Weibing Li,Yue Yu,Tong Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: 提出了一种新的方法MPS-Tuning，通过保留和优化数据分布的几何结构来改进VLMs的微调过程，在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的微调方法在使用VLMs进行知识迁移时忽视了数据分布的几何结构，可能会导致语义表示的扭曲。

Method: 提出了Manifold-Preserving and Sculpting Tuning (MPS-Tuning)方法，通过在微调过程中的Gram矩阵对齐来保留语义流形的宏观和微观结构，同时优化图片和文本模态的相似性增强类别可分性。

Result: 实验结果显示，MPS-Tuning可以显著提升模型表现，同时有效保留语义流形的结构。

Conclusion: MPS-Tuning在保留语义几何结构的同时，增强了类别区分能力，提供了一种改进VLMs微调的有效方法。

Abstract: Pretrained vision-language models (VLMs), such as CLIP, have shown remarkable
potential in few-shot image classification and led to numerous effective
transfer learning strategies. These methods leverage the pretrained knowledge
of VLMs to enable effective domain adaptation while mitigating overfitting
through parameter-efficient tuning or instance-based consistency constraints.
However, such regularizations often neglect the geometric structure of data
distribution, which may lead to distortion of the overall semantic
representation. To overcome this limitation, we propose a novel fine-tuning
method, Manifold-Preserving and Sculpting Tuning (MPS-Tuning). Regarding the
data distribution in feature space as a semantic manifold, MPS-Tuning
explicitly constrains the intrinsic geometry of this manifold while further
sculpting it to enhance class separability. Specifically, MPS-Tuning preserves
both macroscopic and microscopic topological structures of the original
manifold by aligning Gram matrices of features before and after fine-tuning.
Theoretically, this constraint is shown to approximate an upper bound of the
Gromov-Wasserstein distance. Furthermore, features from the image and text
modalities are paired, and pairwise similarities are optimized to enhance the
manifold's class discriminability. Extensive experiments demonstrate that
MPS-Tuning significantly improves model performance while effectively
preserving the structure of the semantic manifold. The code will be released.

</details>


### [125] [S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models](https://arxiv.org/abs/2508.12880)
*Chubin Chen,Jiashu Zhu,Xiaokun Feng,Nisha Huang,Meiqi Wu,Fangyuan Mao,Jiahong Wu,Xiangxiang Chu,Xiu Li*

Main category: cs.CV

TL;DR: 提出了S^2-Guidance方法，通过随机块丢弃构建随机子网络，改进扩散模型的生成质量，优于此前的CFG方法。


<details>
  <summary>Details</summary>
Motivation: 现有的Classifier-free Guidance (CFG)生成存在语义不一致和质量不高的问题，研究旨在改进这一不足。

Method: 通过提出S^2-Guidance方法，利用随机块丢弃和生成模型的子网络进行引导，避免低质量输出。

Result: 在文本生成图像和视频任务中相比CFG和其他方法表现更优，定性和定量实验均证明方法有效性。

Conclusion: S^2-Guidance有效提升了扩散模型的生成输出质量，是优于以往方法的重要改进。

Abstract: Classifier-free Guidance (CFG) is a widely used technique in modern diffusion
models for enhancing sample quality and prompt adherence. However, through an
empirical analysis on Gaussian mixture modeling with a closed-form solution, we
observe a discrepancy between the suboptimal results produced by CFG and the
ground truth. The model's excessive reliance on these suboptimal predictions
often leads to semantic incoherence and low-quality outputs. To address this
issue, we first empirically demonstrate that the model's suboptimal predictions
can be effectively refined using sub-networks of the model itself. Building on
this insight, we propose S^2-Guidance, a novel method that leverages stochastic
block-dropping during the forward process to construct stochastic sub-networks,
effectively guiding the model away from potential low-quality predictions and
toward high-quality outputs. Extensive qualitative and quantitative experiments
on text-to-image and text-to-video generation tasks demonstrate that
S^2-Guidance delivers superior performance, consistently surpassing CFG and
other advanced guidance strategies. Our code will be released.

</details>


### [126] [ONG: One-Shot NMF-based Gradient Masking for Efficient Model Sparsification](https://arxiv.org/abs/2508.12891)
*Sankar Behera,Yamuna Prasad*

Main category: cs.CV

TL;DR: 提出了一种新的网络剪枝方法ONG，通过在训练开始时使用非负矩阵分解进行一次性剪枝，并通过梯度掩码机制保证训练中目标稀疏性的严格保持。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络因其大规模导致的部署挑战，寻找高效的剪枝方法来维持稀疏性和性能。

Method: 利用非负矩阵分解进行一次性剪枝（One-shot Pruning），结合梯度掩码机制在整个训练过程中仅更新未剪枝的权重。

Result: 在ResNet56、ResNet34和ResNet18上进行CIFAR-10和CIFAR-100实验，与已建立的稳定稀疏化方法相比，在不同稀疏性水平上性能相当或更优。

Conclusion: ONG能实现高效剪枝，保持稀疏性和结构完整性的同时性能表现出色且操作简易。

Abstract: Deep Neural Networks (DNNs) have achieved remarkable success but their large
size poses deployment challenges. While various pruning techniques exist, many
involve complex iterative processes, specialized criteria, or struggle to
maintain sparsity effectively during training. We introduce ONG (One-shot
NMF-based Gradient Masking), a novel sparsification strategy that identifies
salient weight structures using Non-negative Matrix Factorization (NMF) for
one-shot pruning at the outset of training. Subsequently, ONG employs a precise
gradient masking mechanism to ensure that only unpruned weights are updated,
strictly preserving the target sparsity throughout the training phase. We
integrate ONG into the BIMP comparative framework and evaluate it on CIFAR-10
and CIFAR-100 with ResNet56, ResNet34, and ResNet18 against established stable
sparsification methods. Our experiments demonstrate ONG's ability to achieve
comparable or superior performance at various sparsity levels while maintaining
structural integrity post-pruning and offering a clear mechanism for targeting
desired sparsities.

</details>


### [127] [CTFlow: Video-Inspired Latent Flow Matching for 3D CT Synthesis](https://arxiv.org/abs/2508.12900)
*Jiayi Wang,Hadrien Reynaud,Franciskus Xaverius Erick,Bernhard Kainz*

Main category: cs.CV

TL;DR: CTFlow是一种基于临床报告生成整个CT影像体积的生成模型，其采用新颖方法生成一致的3D医学影像，并超越了现有技术。


<details>
  <summary>Details</summary>
Motivation: 通过生成基于临床报告的大规模CT模拟数据，可以加速研究、提高隐私保护并减少监管限制，同时保留诊断信号。

Method: 使用CTFlow模型，依托FLUX中的A-VAE定义潜在空间，利用CT-Clip文档编码器处理临床报告，采用自回归方式预测影像体积的切片序列。

Result: CTFlow在时间一致性、影像多样性及文本-影像对齐上优于现有方法，通过FID、FVD、IS和CLIP分数进行验证。

Conclusion: CTFlow显著提升了基于文本的CT体积生成能力，为数据增强与隐私保护开辟了新路径。

Abstract: Generative modelling of entire CT volumes conditioned on clinical reports has
the potential to accelerate research through data augmentation,
privacy-preserving synthesis and reducing regulator-constraints on patient data
while preserving diagnostic signals. With the recent release of CT-RATE, a
large-scale collection of 3D CT volumes paired with their respective clinical
reports, training large text-conditioned CT volume generation models has become
achievable. In this work, we introduce CTFlow, a 0.5B latent flow matching
transformer model, conditioned on clinical reports. We leverage the A-VAE from
FLUX to define our latent space, and rely on the CT-Clip text encoder to encode
the clinical reports. To generate consistent whole CT volumes while keeping the
memory constraints tractable, we rely on a custom autoregressive approach,
where the model predicts the first sequence of slices of the volume from
text-only, and then relies on the previously generated sequence of slices and
the text, to predict the following sequence. We evaluate our results against
state-of-the-art generative CT model, and demonstrate the superiority of our
approach in terms of temporal coherence, image diversity and text-image
alignment, with FID, FVD, IS scores and CLIP score.

</details>


### [128] [CMF-IoU: Multi-Stage Cross-Modal Fusion 3D Object Detection with IoU Joint Prediction](https://arxiv.org/abs/2508.12917)
*Zhiwei Ning,Zhaojiang Liu,Xuanang Gao,Yifan Zuo,Jie Yang,Yuming Fang,Wei Liu*

Main category: cs.CV

TL;DR: 本文提出一种基于多模态跨模态融合的3D检测框架（CMF-IOU），解决了3D空间与2D语义信息的对齐问题，并在多个数据集上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D检测多模态方法往往仅关注单阶段或部分阶段融合，导致特征提取不足和性能受限。此研究旨在实现3D空间与2D语义信息的高效对齐。

Method: 通过深度完成网络将像素信息投影到3D空间生成伪点，设计联合激光雷达与伪点编码的双向跨视图增强主干网络，并提出递归的精细池化模块和基于IoU联合预测的新提案生成技术。

Result: 实验在KITTI、nuScenes和Waymo数据集上验证了方法的优越性能，取得了更高的检测精度。

Conclusion: CMF-IOU框架有效整合了3D与2D信息，通过多阶段融合和精细特征提取显著提升了3D检测性能。

Abstract: Multi-modal methods based on camera and LiDAR sensors have garnered
significant attention in the field of 3D detection. However, many prevalent
works focus on single or partial stage fusion, leading to insufficient feature
extraction and suboptimal performance. In this paper, we introduce a
multi-stage cross-modal fusion 3D detection framework, termed CMF-IOU, to
effectively address the challenge of aligning 3D spatial and 2D semantic
information. Specifically, we first project the pixel information into 3D space
via a depth completion network to get the pseudo points, which unifies the
representation of the LiDAR and camera information. Then, a bilateral
cross-view enhancement 3D backbone is designed to encode LiDAR points and
pseudo points. The first sparse-to-distant (S2D) branch utilizes an
encoder-decoder structure to reinforce the representation of sparse LiDAR
points. The second residual view consistency (ResVC) branch is proposed to
mitigate the influence of inaccurate pseudo points via both the 3D and 2D
convolution processes. Subsequently, we introduce an iterative voxel-point
aware fine grained pooling module, which captures the spatial information from
LiDAR points and textural information from pseudo points in the proposal
refinement stage. To achieve more precise refinement during iteration, an
intersection over union (IoU) joint prediction branch integrated with a novel
proposals generation technique is designed to preserve the bounding boxes with
both high IoU and classification scores. Extensive experiments show the
superior performance of our method on the KITTI, nuScenes and Waymo datasets.

</details>


### [129] [7Bench: a Comprehensive Benchmark for Layout-guided Text-to-image Models](https://arxiv.org/abs/2508.12919)
*Elena Izzo,Luca Parolari,Davide Vezzaro,Lamberto Ballan*

Main category: cs.CV

TL;DR: 提出了7Bench——一个首次用于评估图文生成模型中语义与空间对齐能力的基准测试工具。


<details>
  <summary>Details</summary>
Motivation: 应对基于布局的文本到图像生成模型在语义与空间对齐上的挑战，提升生成内容的质量与真实性。

Method: 通过7Bench提供涵盖多种复杂情景的文本与布局配对数据，建立评估语义和空间对齐能力的评分协议，并使用这一框架评测多种先进的扩散模型。

Result: 使用7Bench揭示了当前先进模型在对齐任务中的优劣势，帮助理解模型性能及其改进潜力。

Conclusion: 7Bench填补了基准测试领域的空白，为评估和优化文本到图像生成系统的对齐性能提供了重要工具。

Abstract: Layout-guided text-to-image models offer greater control over the generation
process by explicitly conditioning image synthesis on the spatial arrangement
of elements. As a result, their adoption has increased in many computer vision
applications, ranging from content creation to synthetic data generation. A
critical challenge is achieving precise alignment between the image, textual
prompt, and layout, ensuring semantic fidelity and spatial accuracy. Although
recent benchmarks assess text alignment, layout alignment remains overlooked,
and no existing benchmark jointly evaluates both. This gap limits the ability
to evaluate a model's spatial fidelity, which is crucial when using
layout-guided generation for synthetic data, as errors can introduce noise and
degrade data quality. In this work, we introduce 7Bench, the first benchmark to
assess both semantic and spatial alignment in layout-guided text-to-image
generation. It features text-and-layout pairs spanning seven challenging
scenarios, investigating object generation, color fidelity, attribute
recognition, inter-object relationships, and spatial control. We propose an
evaluation protocol that builds on existing frameworks by incorporating the
layout alignment score to assess spatial accuracy. Using 7Bench, we evaluate
several state-of-the-art diffusion models, uncovering their respective
strengths and limitations across diverse alignment tasks. The benchmark is
available at https://github.com/Elizzo/7Bench.

</details>


### [130] [Towards High-Resolution Industrial Image Anomaly Detection](https://arxiv.org/abs/2508.12931)
*Ximiao Zhang,Min Xu,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: 本研究提出HiAD框架，专注于高分辨率图像的异常检测，并成功改善检测准确性与效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法对高分辨率图像的检测性能仍不足，特别是存在细节信息丢失及工业场景需求未满足的问题。

Method: 提出HiAD框架，采用双分支架构结合多分辨率特征融合策略，并通过检测器池适配不同特征块，兼顾性能与计算成本。

Result: HiAD在所构建的多项高分辨率异常检测基准数据集上表现优异，证明框架具有优势性能。

Conclusion: HiAD框架在高分辨率异常检测中表现出色，尤其是在有限计算资源情况下，解决了现有方法的多项问题。

Abstract: Current anomaly detection methods primarily focus on low-resolution
scenarios. For high-resolution images, conventional downsampling often results
in missed detections of subtle anomalous regions due to the loss of
fine-grained discriminative information. Despite some progress, recent studies
have attempted to improve detection resolution by employing lightweight
networks or using simple image tiling and ensemble methods. However, these
approaches still struggle to meet the practical demands of industrial scenarios
in terms of detection accuracy and efficiency. To address the above issues, we
propose HiAD, a general framework for high-resolution anomaly detection. HiAD
is capable of detecting anomalous regions of varying sizes in high-resolution
images under limited computational resources. Specifically, HiAD employs a
dual-branch architecture that integrates anomaly cues across different scales
to comprehensively capture both subtle and large-scale anomalies. Furthermore,
it incorporates a multi-resolution feature fusion strategy to tackle the
challenges posed by fine-grained texture variations in high-resolution images.
To enhance both adaptability and efficiency, HiAD utilizes a detector pool in
conjunction with various detector assignment strategies, enabling detectors to
be adaptively assigned based on patch features, ensuring detection performance
while effectively controlling computational costs. We conduct extensive
experiments on our specifically constructed high-resolution anomaly detection
benchmarks, including MVTec-HD, VisA-HD, and the real-world benchmark
RealIAD-HD, demonstrating the superior performance of HiAD. The code is
available at https://github.com/cnulab/HiAD.

</details>


### [131] [SEDEG:Sequential Enhancement of Decoder and Encoder's Generality for Class Incremental Learning with Small Memory](https://arxiv.org/abs/2508.12932)
*Hongyang Chen,Shaoling Pu,Lingyu Zheng,Zhongwu Sun*

Main category: cs.CV

TL;DR: 本论文介绍了一种名为SEDEG的增量学习框架，通过分阶段增强编码器和解码器的通用性，有效缓解灾难性遗忘问题，并在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有增量学习方法常单独提升编码器或解码器的性能，限制了缓解灾难性遗忘的效果，尤其在小内存数据场景下表现不佳。提出一种可同时增强编码器与解码器通用性的训练框架以解决这一问题。

Method: 提出两阶段SEDEG框架：第一阶段通过特征增强训练集成编码器，提升其通用性，并改善解码器性能；第二阶段采用知识蒸馏策略压缩集成编码器，开发更通用的新编码器，其中包括平衡知识蒸馏和特征知识蒸馏方法。

Result: 在三个基准数据集上的实验表明，SEDEG在增量学习任务中性能优越，消融实验验证了其各组件的有效性。

Conclusion: SEDEG通过增强编码器与解码器的通用性，显著缓解了灾难性遗忘问题，尤其在有限存储场景中具有显著优势。

Abstract: In incremental learning, enhancing the generality of knowledge is crucial for
adapting to dynamic data inputs. It can develop generalized representations or
more balanced decision boundaries, preventing the degradation of long-term
knowledge over time and thus mitigating catastrophic forgetting. Some emerging
incremental learning methods adopt an encoder-decoder architecture and have
achieved promising results. In the encoder-decoder achitecture, improving the
generalization capabilities of both the encoder and decoder is critical, as it
helps preserve previously learned knowledge while ensuring adaptability and
robustness to new, diverse data inputs. However, many existing continual
methods focus solely on enhancing one of the two components, which limits their
effectiveness in mitigating catastrophic forgetting. And these methods perform
even worse in small-memory scenarios, where only a limited number of historical
samples can be stored. To mitigate this limitation, we introduces SEDEG, a
two-stage training framework for vision transformers (ViT), focusing on
sequentially improving the generality of both Decoder and Encoder. Initially,
SEDEG trains an ensembled encoder through feature boosting to learn generalized
representations, which subsequently enhance the decoder's generality and
balance the classifier. The next stage involves using knowledge distillation
(KD) strategies to compress the ensembled encoder and develop a new, more
generalized encoder. This involves using a balanced KD approach and feature KD
for effective knowledge transfer. Extensive experiments on three benchmark
datasets show SEDEG's superior performance, and ablation studies confirm the
efficacy of its components. The code is available at
https://github.com/ShaolingPu/CIL.

</details>


### [132] [Fully Automated Segmentation of Fiber Bundles in Anatomic Tracing Data](https://arxiv.org/abs/2508.12942)
*Kyriaki-Margarita Bintsi,Yaël Balbastre,Jingjing Wu,Julia F. Lehman,Suzanne N. Haber,Anastasia Yendiki*

Main category: cs.CV

TL;DR: 提出了一种基于U-Net架构的自动化框架，用于灵长类神经解剖追踪数据的纤维束分割，显著提升稀疏束检测和减少误差。


<details>
  <summary>Details</summary>
Motivation: 解剖追踪数据的人工标注费时费力，现有方法难以灵活且普遍适用。

Method: 基于U-Net网络，结合大块区域、前景感知采样和半监督预训练，开发全自动分割框架。

Result: 稀疏束检测提升20%以上，虚警率下降40%，支持独立切片分析。

Conclusion: 新框架简化了解剖追踪数据的大规模分析，为验证和优化dMRI束成像提供更丰富的真值数据。

Abstract: Anatomic tracer studies are critical for validating and improving diffusion
MRI (dMRI) tractography. However, large-scale analysis of data from such
studies is hampered by the labor-intensive process of annotating fiber bundles
manually on histological slides. Existing automated methods often miss sparse
bundles or require complex post-processing across consecutive sections,
limiting their flexibility and generalizability. We present a streamlined,
fully automated framework for fiber bundle segmentation in macaque tracer data,
based on a U-Net architecture with large patch sizes, foreground aware
sampling, and semisupervised pre-training. Our approach eliminates common
errors such as mislabeling terminals as bundles, improves detection of sparse
bundles by over 20% and reduces the False Discovery Rate (FDR) by 40% compared
to the state-of-the-art, all while enabling analysis of standalone slices. This
new framework will facilitate the automated analysis of anatomic tracing data
at a large scale, generating more ground-truth data that can be used to
validate and optimize dMRI tractography methods.

</details>


### [133] [Lumen: Consistent Video Relighting and Harmonious Background Replacement with Video Generative Models](https://arxiv.org/abs/2508.12945)
*Jianshu Zeng,Yuxuan Liu,Yutong Feng,Chenxuan Miao,Zixiang Gao,Jiwang Qu,Jianzhang Zhang,Bin Wang,Kun Yuan*

Main category: cs.CV

TL;DR: 论文提出了一个名为Lumen的端到端视频重新光照框架，通过文本描述控制光照与背景，生成一致光照且前景保留良好的电影化视频。


<details>
  <summary>Details</summary>
Motivation: 视频重新光照需要在替换背景时，保持前景属性且在时间帧间传播一致性光照，目前缺乏高质量配对视频数据集。

Method: 构建了一个混合真实与合成视频的大规模数据集，并引入域感知适配器以解耦重新光照与域外观分布学习，同时设计了联合训练课程以充分利用合成和真实数据的优势。

Result: 实验结果表明，Lumen可以有效将输入编辑为光照一致且前景保留良好的电影化视频。

Conclusion: Lumen利用大规模视频生成模型和构建的数据集，实现了基于文本控制的视频重新光照，提供了实际与研究的标杆参考。

Abstract: Video relighting is a challenging yet valuable task, aiming to replace the
background in videos while correspondingly adjusting the lighting in the
foreground with harmonious blending. During translation, it is essential to
preserve the original properties of the foreground, e.g., albedo, and propagate
consistent relighting among temporal frames. In this paper, we propose Lumen,
an end-to-end video relighting framework developed on large-scale video
generative models, receiving flexible textual description for instructing the
control of lighting and background. Considering the scarcity of high-qualified
paired videos with the same foreground in various lighting conditions, we
construct a large-scale dataset with a mixture of realistic and synthetic
videos. For the synthetic domain, benefiting from the abundant 3D assets in the
community, we leverage advanced 3D rendering engine to curate video pairs in
diverse environments. For the realistic domain, we adapt a HDR-based lighting
simulation to complement the lack of paired in-the-wild videos. Powered by the
aforementioned dataset, we design a joint training curriculum to effectively
unleash the strengths of each domain, i.e., the physical consistency in
synthetic videos, and the generalized domain distribution in realistic videos.
To implement this, we inject a domain-aware adapter into the model to decouple
the learning of relighting and domain appearance distribution. We construct a
comprehensive benchmark to evaluate Lumen together with existing methods, from
the perspectives of foreground preservation and video consistency assessment.
Experimental results demonstrate that Lumen effectively edit the input into
cinematic relighted videos with consistent lighting and strict foreground
preservation. Our project page: https://lumen-relight.github.io/

</details>


### [134] [MaskSem: Semantic-Guided Masking for Learning 3D Hybrid High-Order Motion Representation](https://arxiv.org/abs/2508.12948)
*Wei Wei,Shaojie Zhang,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: 本文引入MaskSem，一种新的基于语义引导的掩码方法，用于学习高阶运动表示，提高骨骼动作识别的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的基于掩码的动作识别方法局限于少量关节和低阶运动模式，难以理解复杂的运动模式。

Method: 提出MaskSem框架，通过相对运动的Grad-CAM指导的语义掩码策略，结合低阶运动速度和高阶运动加速度作为重构目标，增强模型对运动模式的理解。

Result: MaskSem在NTU60、NTU120和PKU-MMD数据集上的实验表现优异，提升骨骼动作识别能力。

Conclusion: MaskSem方法有效增强了动作识别模型对复杂运动模式的理解，适合人机交互应用。

Abstract: Human action recognition is a crucial task for intelligent robotics,
particularly within the context of human-robot collaboration research. In
self-supervised skeleton-based action recognition, the mask-based
reconstruction paradigm learns the spatial structure and motion patterns of the
skeleton by masking joints and reconstructing the target from unlabeled data.
However, existing methods focus on a limited set of joints and low-order motion
patterns, limiting the model's ability to understand complex motion patterns.
To address this issue, we introduce MaskSem, a novel semantic-guided masking
method for learning 3D hybrid high-order motion representations. This novel
framework leverages Grad-CAM based on relative motion to guide the masking of
joints, which can be represented as the most semantically rich temporal
orgions. The semantic-guided masking process can encourage the model to explore
more discriminative features. Furthermore, we propose using hybrid high-order
motion as the reconstruction target, enabling the model to learn multi-order
motion patterns. Specifically, low-order motion velocity and high-order motion
acceleration are used together as the reconstruction target. This approach
offers a more comprehensive description of the dynamic motion process,
enhancing the model's understanding of motion patterns. Experiments on the
NTU60, NTU120, and PKU-MMD datasets show that MaskSem, combined with a vanilla
transformer, improves skeleton-based action recognition, making it more
suitable for applications in human-robot interaction.

</details>


### [135] [Breaking Reward Collapse: Adaptive Reinforcement for Open-ended Medical Reasoning with Enhanced Semantic Discrimination](https://arxiv.org/abs/2508.12957)
*Yizhou Liu,Jingwei Wei,Zizhi Chen,Minghao Han,Xukun Zhang,Keliang Liu,Lihua Zhang*

Main category: cs.CV

TL;DR: 本文探讨了强化学习（RL）在医疗影像中应用的潜力，提出了一种新的开放式医学视觉问答（VQA）RL框架ARMed，并展示了其在多个医学VQA基准上的显著表现。


<details>
  <summary>Details</summary>
Motivation: 现有的强化微调方法大多关注封闭式的VQA，限制了其在实际临床推理中的应用，而开放式医学VQA更符合临床实践，却鲜有深入研究。

Method: 提出了ARMed框架，先通过链式思维数据的监督微调引入领域知识，然后通过文本正确性与自适应语义奖励强化学习来提升推理质量。

Result: ARMed在六个医学VQA基准上表现出色，对域内任务准确率提升32.64%，对域外基准提升11.65%。

Conclusion: 奖励可辨识性是医学强化学习的关键，语义指导的奖励在实现强健和具有临床意义的多模态推理中具有重要潜力。

Abstract: Reinforcement learning (RL) with rule-based rewards has demonstrated strong
potential in enhancing the reasoning and generalization capabilities of
vision-language models (VLMs) and large language models (LLMs), while reducing
computational overhead. However, its application in medical imaging remains
underexplored. Existing reinforcement fine-tuning (RFT) approaches in this
domain primarily target closed-ended visual question answering (VQA), limiting
their applicability to real-world clinical reasoning. In contrast, open-ended
medical VQA better reflects clinical practice but has received limited
attention. While some efforts have sought to unify both formats via
semantically guided RL, we observe that model-based semantic rewards often
suffer from reward collapse, where responses with significant semantic
differences receive similar scores. To address this, we propose ARMed (Adaptive
Reinforcement for Medical Reasoning), a novel RL framework for open-ended
medical VQA. ARMed first incorporates domain knowledge through supervised
fine-tuning (SFT) on chain-of-thought data, then applies reinforcement learning
with textual correctness and adaptive semantic rewards to enhance reasoning
quality. We evaluate ARMed on six challenging medical VQA benchmarks. Results
show that ARMed consistently boosts both accuracy and generalization, achieving
a 32.64% improvement on in-domain tasks and an 11.65% gain on out-of-domain
benchmarks. These results highlight the critical role of reward
discriminability in medical RL and the promise of semantically guided rewards
for enabling robust and clinically meaningful multimodal reasoning.

</details>


### [136] [Multi-Phase Automated Segmentation of Dental Structures in CBCT Using a Lightweight Auto3DSeg and SegResNet Implementation](https://arxiv.org/abs/2508.12962)
*Dominic LaBella,Keshav Jha,Jared Robbins,Esther Yu*

Main category: cs.CV

TL;DR: 本文介绍了一种深度学习管道应用于CBCT多分类牙齿分割，实现了0.87的平均Dice分数。


<details>
  <summary>Details</summary>
Motivation: 为牙齿结构的自动分割开发有效方法，从而协助牙科病理诊断及头颈癌患者的放疗计划。

Method: 采用MONAI Auto3DSeg框架和3D SegResNet架构，经过数据预处理、5折交叉验证及阶段式分割提升分割精度。

Result: 在ToothFairy3挑战的未见过的验证集上实现了平均Dice分数0.87。

Conclusion: 自动分割方法可以提升牙科诊疗与放疗中的效率和准确性，同时展示了深度学习技术的重要应用价值。

Abstract: Cone-beam computed tomography (CBCT) has become an invaluable imaging
modality in dentistry, enabling 3D visualization of teeth and surrounding
structures for diagnosis and treatment planning. Automated segmentation of
dental structures in CBCT can efficiently assist in identifying pathology
(e.g., pulpal or periapical lesions) and facilitate radiation therapy planning
in head and neck cancer patients. We describe the DLaBella29 team's approach
for the MICCAI 2025 ToothFairy3 Challenge, which involves a deep learning
pipeline for multi-class tooth segmentation. We utilized the MONAI Auto3DSeg
framework with a 3D SegResNet architecture, trained on a subset of the
ToothFairy3 dataset (63 CBCT scans) with 5-fold cross-validation. Key
preprocessing steps included image resampling to 0.6 mm isotropic resolution
and intensity clipping. We applied an ensemble fusion using Multi-Label STAPLE
on the 5-fold predictions to infer a Phase 1 segmentation and then conducted
tight cropping around the easily segmented Phase 1 mandible to perform Phase 2
segmentation on the smaller nerve structures. Our method achieved an average
Dice of 0.87 on the ToothFairy3 challenge out-of-sample validation set. This
paper details the clinical context, data preparation, model development,
results of our approach, and discusses the relevance of automated dental
segmentation for improving patient care in radiation oncology.

</details>


### [137] [GazeDETR: Gaze Detection using Disentangled Head and Gaze Representations](https://arxiv.org/abs/2508.12966)
*Ryan Anthony Jalova de Belen,Gelareh Mohammadi,Arcot Sowmya*

Main category: cs.CV

TL;DR: 提出了一种新的端到端眼动检测架构GazeDETR，针对头部定位和注视目标检测任务设计了独立的解码器，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统的眼动目标检测模型使用单一解码器，难以区分头部定位和注视目标检测的任务特性，因而提出了新的解决方案。

Method: 基于头部定位和注视目标检测这两个子任务，分别使用两个独立的解码器进行解耦学习，同时利用了不同的信息场（局部与全局）。

Result: 在GazeFollow、VideoAttentionTarget和ChildPlay数据集上取得了最新的最佳性能，相对于现有端到端模型明显更优。

Conclusion: GazeDETR通过独立解码器的构建和有效的任务解耦，显著提高了眼动目标检测的精度。

Abstract: Gaze communication plays a crucial role in daily social interactions.
Quantifying this behavior can help in human-computer interaction and digital
phenotyping. While end-to-end models exist for gaze target detection, they only
utilize a single decoder to simultaneously localize human heads and predict
their corresponding gaze (e.g., 2D points or heatmap) in a scene. This
multitask learning approach generates a unified and entangled representation
for human head localization and gaze location prediction. Herein, we propose
GazeDETR, a novel end-to-end architecture with two disentangled decoders that
individually learn unique representations and effectively utilize coherent
attentive fields for each subtask. More specifically, we demonstrate that its
human head predictor utilizes local information, while its gaze decoder
incorporates both local and global information. Our proposed architecture
achieves state-of-the-art results on the GazeFollow, VideoAttentionTarget and
ChildPlay datasets. It outperforms existing end-to-end models with a notable
margin.

</details>


### [138] [Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation](https://arxiv.org/abs/2508.12969)
*Qirui Li,Guangcong Zheng,Qi Zhao,Jie Li,Bin Dong,Yiwu Yao,Xi Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为Compact Attention的硬件友好加速框架，通过结构化稀疏性开发实现高效的视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有变压器的视频生成方法在处理超长序列时，由于自注意力机制的计算需求高，缺乏有效利用视频数据的时空冗余性。

Method: 提出Compact Attention，包括：1）通过动态分组实现多样化时空交互模式的自适应分块策略；2）基于帧接近度调整稀疏级别的时间变化窗口；3）优化稀疏模式并保留关键注意路径的自动化配置搜索算法。

Result: 在单GPU环境下，自注意力加速实现了1.6~2.5倍，同时视觉质量与全注意力基线相当。

Conclusion: 通过结构化稀疏性的视频生成方法显著改进了效率，为长视频生成提供了一种有效的解决方案。

Abstract: The computational demands of self-attention mechanisms pose a critical
challenge for transformer-based video generation, particularly in synthesizing
ultra-long sequences. Current approaches, such as factorized attention and
fixed sparse patterns, fail to fully exploit the inherent spatio-temporal
redundancies in video data. Through systematic analysis of video diffusion
transformers (DiT), we uncover a key insight: Attention matrices exhibit
structured, yet heterogeneous sparsity patterns, where specialized heads
dynamically attend to distinct spatiotemporal regions (e.g., local pattern,
cross-shaped pattern, or global pattern). Existing sparse attention methods
either impose rigid constraints or introduce significant overhead, limiting
their effectiveness. To address this, we propose Compact Attention, a
hardware-aware acceleration framework featuring three innovations: 1) Adaptive
tiling strategies that approximate diverse spatial interaction patterns via
dynamic tile grouping, 2) Temporally varying windows that adjust sparsity
levels based on frame proximity, and 3) An automated configuration search
algorithm that optimizes sparse patterns while preserving critical attention
pathways. Our method achieves 1.6~2.5x acceleration in attention computation on
single-GPU setups while maintaining comparable visual quality with
full-attention baselines. This work provides a principled approach to unlocking
efficient long-form video generation through structured sparsity exploitation.
Project Page: https://yo-ava.github.io/Compact-Attention.github.io/

</details>


### [139] [Dextr: Zero-Shot Neural Architecture Search with Singular Value Decomposition and Extrinsic Curvature](https://arxiv.org/abs/2508.12977)
*Rohan Asthana,Joschua Conrad,Maurits Ortmanns,Vasileios Belagiannis*

Main category: cs.CV

TL;DR: 提出一种新型零费用代理方法，通过结合SVD分解和网络输出的曲率，在不用标注数据的情况下评估神经网络性能，实现更高效的架构搜索。


<details>
  <summary>Details</summary>
Motivation: 现有的零费用NAS方法依赖标注数据且多仅优化网络某方面特性，而现实中标注数据常不可用，且需要综合优化网络性能。

Method: 提出基于通道共线性原理，结合SVD分解特征及网络输出曲率，设计出能够结合收敛性、泛化性和表征能力的零费用代理方法。

Result: 对多个NAS基准如NAS-Bench-101、DARTS等实验，表现出出色的性能预测能力和搜索效率优势。

Conclusion: 新方法可在无标注条件下，准确高效地预测网络性能，提升神经架构搜索效率。

Abstract: Zero-shot Neural Architecture Search (NAS) typically optimises the
architecture search process by exploiting the network or gradient properties at
initialisation through zero-cost proxies. The existing proxies often rely on
labelled data, which is usually unavailable in real-world settings.
Furthermore, the majority of the current methods focus either on optimising the
convergence and generalisation attributes or solely on the expressivity of the
network architectures. To address both limitations, we first demonstrate how
channel collinearity affects the convergence and generalisation properties of a
neural network. Then, by incorporating the convergence, generalisation and
expressivity in one approach, we propose a zero-cost proxy that omits the
requirement of labelled data for its computation. In particular, we leverage
the Singular Value Decomposition (SVD) of the neural network layer features and
the extrinsic curvature of the network output to design our proxy. %As a
result, the proposed proxy is formulated as the simplified harmonic mean of the
logarithms of two key components: the sum of the inverse of the feature
condition number and the extrinsic curvature of the network output. Our
approach enables accurate prediction of network performance on test data using
only a single label-free data sample. Our extensive evaluation includes a total
of six experiments, including the Convolutional Neural Network (CNN) search
space, i.e. DARTS and the Transformer search space, i.e. AutoFormer. The
proposed proxy demonstrates a superior performance on multiple correlation
benchmarks, including NAS-Bench-101, NAS-Bench-201, and
TransNAS-Bench-101-micro; as well as on the NAS task within the DARTS and the
AutoFormer search space, all while being notably efficient. The code is
available at https://github.com/rohanasthana/Dextr.

</details>


### [140] [Omni Survey for Multimodality Analysis in Visual Object Tracking](https://arxiv.org/abs/2508.13000)
*Zhangyong Tang,Tianyang Xu,Xuefeng Zhu,Hui Li,Shaochuan Zhao,Tao Zhou,Chunyang Cheng,Xiaojun Wu,Josef Kittler*

Main category: cs.CV

TL;DR: 本文综述了多模态视觉对象跟踪（MMVOT）的关键任务，并从多模态分析角度探讨数据收集、模式对齐及注释、模型设计和评估的挑战，分析多模态数据集的分布特点，并带出相关研究问题。


<details>
  <summary>Details</summary>
Motivation: 随着智慧城市的发展，多模态数据的大量生成需要研究如何综合利用这些数据，以更好地监测智慧城市的基础设施和服务。

Method: 从多种数据模态入手，分析数据收集、模态对齐及注释、模型设计等议题，分类现有的MMVOT方法并对其进行评价，同时解析数据集内的对象类别分布。

Result: 总结了现有338个参考文献中关于MMVOT的研究，揭示了长尾数据分布和动物类别明显缺乏的问题。

Conclusion: 多模态跟踪并非总优于单模态跟踪，其效果依赖于具体的运用场景和数据融合方式，为领域内研究指明了重要方向和有待解决的挑战。

Abstract: The development of smart cities has led to the generation of massive amounts
of multi-modal data in the context of a range of tasks that enable a
comprehensive monitoring of the smart city infrastructure and services. This
paper surveys one of the most critical tasks, multi-modal visual object
tracking (MMVOT), from the perspective of multimodality analysis. Generally,
MMVOT differs from single-modal tracking in four key aspects, data collection,
modality alignment and annotation, model designing, and evaluation.
Accordingly, we begin with an introduction to the relevant data modalities,
laying the groundwork for their integration. This naturally leads to a
discussion of challenges of multi-modal data collection, alignment, and
annotation. Subsequently, existing MMVOT methods are categorised, based on
different ways to deal with visible (RGB) and X modalities: programming the
auxiliary X branch with replicated or non-replicated experimental
configurations from the RGB branch. Here X can be thermal infrared (T), depth
(D), event (E), near infrared (NIR), language (L), or sonar (S). The final part
of the paper addresses evaluation and benchmarking. In summary, we undertake an
omni survey of all aspects of multi-modal visual object tracking (VOT),
covering six MMVOT tasks and featuring 338 references in total. In addition, we
discuss the fundamental rhetorical question: Is multi-modal tracking always
guaranteed to provide a superior solution to unimodal tracking with the help of
information fusion, and if not, in what circumstances its application is
beneficial. Furthermore, for the first time in this field, we analyse the
distributions of the object categories in the existing MMVOT datasets,
revealing their pronounced long-tail nature and a noticeable lack of animal
categories when compared with RGB datasets.

</details>


### [141] [Empirical Evidences for the Effects of Feature Diversity in Open Set Recognition and Continual Learning](https://arxiv.org/abs/2508.13005)
*Jiawen Xu,Odej Kao*

Main category: cs.CV

TL;DR: 增强特征多样性有助于解决开放集识别和持续学习中的问题，同时提高模型对开放集样本的识别能力及新旧数据的兼容性。


<details>
  <summary>Details</summary>
Motivation: 开放集识别需要在推理时检测新类别，而持续学习需要更新模型以融入新类别。虽然现有方法通常通过启发式方法提升特征多样性，但很少直接研究特征多样性在这些问题中的作用。

Method: 通过实验证明，提升特征多样性能够改善对开放集样本的识别能力，并支持旧数据的保留和新数据的集成。

Result: 结果表明，特征多样性有利于解决开放集识别和持续学习问题，尤其是在数据融合和分类方面表现优化。

Conclusion: 研究为开放集识别和持续学习提供了实证支持，未来可在实践方法与理论探索中进一步深化。

Abstract: Open set recognition (OSR) and continual learning are two critical challenges
in machine learning, focusing respectively on detecting novel classes at
inference time and updating models to incorporate the new classes. While many
recent approaches have addressed these problems, particularly OSR, by
heuristically promoting feature diversity, few studies have directly examined
the role that feature diversity plays in tackling them. In this work, we
provide empirical evidence that enhancing feature diversity improves the
recognition of open set samples. Moreover, increased feature diversity also
facilitates both the retention of previously learned data and the integration
of new data in continual learning. We hope our findings can inspire further
research into both practical methods and theoretical understanding in these
domains.

</details>


### [142] [SlimComm: Doppler-Guided Sparse Queries for Bandwidth-Efficient Cooperative 3-D Perception](https://arxiv.org/abs/2508.13007)
*Melih Yazgan,Qiyuan Wu,Iramm Hamdard,Shiqi Li,J. Marius Zoellner*

Main category: cs.CV

TL;DR: 本文提出SlimComm，一个的高效通信框架，用于提升联网自动驾驶汽车的协作感知能力，通过使用查询驱动的稀疏方案与4D雷达多普勒数据，仅共享必要的Bird's-Eye-View（BEV）特征从而减小通信负担。


<details>
  <summary>Details</summary>
Motivation: 解决联网自动驾驶汽车在协作感知中由于遮挡与传感器范围限制而导致的问题，以及通过高效通信减少大量传输密集BEV特征的带宽需求。

Method: SlimComm基于动态相关的地图构建运动信息，通过两种查询方式（参考查询和探测查询）识别动态区域和遮挡区域，并采用多尺度门控可变形注意力融合必要的BEV特征，减小通信开销。

Result: 通过OPV2V-R和Adver-City-R数据集验证，SlimComm在减少多达90%带宽的情况下，其性能优于或等同于现有基准方法，适用于不同的交通密度和遮挡情况。

Conclusion: SlimComm有效在降低通信负载的同时保留了高精度的协作感知能力，具有推广性和实际意义，并开放了相关数据集与代码供研究扩展。

Abstract: Collaborative perception allows connected autonomous vehicles (CAVs) to
overcome occlusion and limited sensor range by sharing intermediate features.
Yet transmitting dense Bird's-Eye-View (BEV) feature maps can overwhelm the
bandwidth available for inter-vehicle communication. We present SlimComm, a
communication-efficient framework that integrates 4D radar Doppler with a
query-driven sparse scheme. SlimComm builds a motion-centric dynamic map to
distinguish moving from static objects and generates two query types: (i)
reference queries on dynamic and high-confidence regions, and (ii) exploratory
queries probing occluded areas via a two-stage offset. Only query-specific BEV
features are exchanged and fused through multi-scale gated deformable
attention, reducing payload while preserving accuracy. For evaluation, we
release OPV2V-R and Adver-City-R, CARLA-based datasets with per-point Doppler
radar. SlimComm achieves up to 90% lower bandwidth than full-map sharing while
matching or surpassing prior baselines across varied traffic densities and
occlusions. Dataset and code will be available at: https://url.fzi.de/SlimComm.

</details>


### [143] [Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model](https://arxiv.org/abs/2508.13009)
*Xianglong He,Chunli Peng,Zexiang Liu,Boyang Wang,Yifan Zhang,Qi Cui,Fei Kang,Biao Jiang,Mengyin An,Yangyang Ren,Baixin Xu,Hao-Xiang Guo,Kaixiong Gong,Cyrus Wu,Wei Li,Xuchen Song,Yang Liu,Eric Li,Yahui Zhou*

Main category: cs.CV

TL;DR: 本研究提出了一种名为Matrix-Game 2.0的交互式世界模型，可通过少步自回归扩散实时生成高质量长视频。


<details>
  <summary>Details</summary>
Motivation: 现有的交互式世界模型由于需要双向注意力机制和较长的推理步骤，性能受到限制，难以模拟即时动态场景，因此需要一种能够快速推断并实时生成视频的新模型。

Method: 该模型采用三大关键组件：1. 一个可扩展的数据生产管道，用于生成包含互动注释的大量视频数据；2. 动作注入模块，可将鼠标键盘输入作为互动条件注入帧中；3. 基于因果架构的少步蒸馏方法，实现实时流式视频生成。

Result: Matrix-Game 2.0能够以25 FPS的速度生成高质量的分钟级长视频，涵盖多种场景。

Conclusion: 研究展示了基于Matrix-Game 2.0的交互式世界模型在实时动态视频生成领域的潜力，并开源了模型和代码，为领域发展提供支持。

Abstract: Recent advances in interactive video generations have demonstrated diffusion
model's potential as world models by capturing complex physical dynamics and
interactive behaviors. However, existing interactive world models depend on
bidirectional attention and lengthy inference steps, severely limiting
real-time performance. Consequently, they are hard to simulate real-world
dynamics, where outcomes must update instantaneously based on historical
context and current actions. To address this, we present Matrix-Game 2.0, an
interactive world model generates long videos on-the-fly via few-step
auto-regressive diffusion. Our framework consists of three key components: (1)
A scalable data production pipeline for Unreal Engine and GTA5 environments to
effectively produce massive amounts (about 1200 hours) of video data with
diverse interaction annotations; (2) An action injection module that enables
frame-level mouse and keyboard inputs as interactive conditions; (3) A few-step
distillation based on the casual architecture for real-time and streaming video
generation. Matrix Game 2.0 can generate high-quality minute-level videos
across diverse scenes at an ultra-fast speed of 25 FPS. We open-source our
model weights and codebase to advance research in interactive world modeling.

</details>


### [144] [EgoTwin: Dreaming Body and View in First Person](https://arxiv.org/abs/2508.13013)
*Jingqiao Xiu,Fangzhou Hong,Yicong Li,Mengze Li,Wentao Wang,Sirui Han,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: 本论文提出了EgoTwin框架，解决了第一人称视角生成中摄像头运动与人运动耦合的问题，通过扩散变压器架构生成一致的视频与人运动。


<details>
  <summary>Details</summary>
Motivation: 当前关于外部视角视频合成的研究成果丰富，但在建模由摄像头运动和人运动结合的第一人称视角视频生成方面却很少涉及。

Method: 提出EgoTwin框架，基于扩散变压器架构，使用头部中心的运动表示和一种受控制论启发的交互机制，解决视角对齐与因果交互难题。

Result: 通过构建大规模同步文本-视频-运动数据集并设计新指标评估视频与运动周期一致性，实验表明EgoTwin框架的有效性。

Conclusion: EgoTwin通过创新性的方法为第一人称视角视频生成提供了解决方案，展示出在视频与运动生成领域的良好潜力。

Abstract: While exocentric video synthesis has achieved great progress, egocentric
video generation remains largely underexplored, which requires modeling
first-person view content along with camera motion patterns induced by the
wearer's body movements. To bridge this gap, we introduce a novel task of joint
egocentric video and human motion generation, characterized by two key
challenges: 1) Viewpoint Alignment: the camera trajectory in the generated
video must accurately align with the head trajectory derived from human motion;
2) Causal Interplay: the synthesized human motion must causally align with the
observed visual dynamics across adjacent video frames. To address these
challenges, we propose EgoTwin, a joint video-motion generation framework built
on the diffusion transformer architecture. Specifically, EgoTwin introduces a
head-centric motion representation that anchors the human motion to the head
joint and incorporates a cybernetics-inspired interaction mechanism that
explicitly captures the causal interplay between video and motion within
attention operations. For comprehensive evaluation, we curate a large-scale
real-world dataset of synchronized text-video-motion triplets and design novel
metrics to assess video-motion consistency. Extensive experiments demonstrate
the effectiveness of the EgoTwin framework.

</details>


### [145] [HierAdaptMR: Cross-Center Cardiac MRI Reconstruction with Hierarchical Feature Adapters](https://arxiv.org/abs/2508.13026)
*Ruru Xu,Ilkay Oksuz*

Main category: cs.CV

TL;DR: 提出了一种名为HierAdaptMR的框架，该方法通过分层特征适配机制，解决了跨多个临床中心中因MRI扫描仪和成像协议不同导致的领域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习在心脏MRI重建中面临不同临床中心和不一致设备配置带来的领域偏移挑战。

Method: 提出了HierAdaptMR框架，使用Protocol-Level和Center-Level适配器处理不同级别的领域变化，包含一个变分解卷积骨干网络，并附加通用适配器来学习跨中心的不变特征。

Result: 在CMRxRecon2025数据集上进行了评估，结果表明在跨多个中心上表现出色，同时保持了重建质量。

Conclusion: 所提出的HierAdaptMR方法能够有效应对MRI图像重建中的领域偏移问题，并展现了强大的跨中心泛化能力。

Abstract: Deep learning-based cardiac MRI reconstruction faces significant domain shift
challenges when deployed across multiple clinical centers with heterogeneous
scanner configurations and imaging protocols. We propose HierAdaptMR, a
hierarchical feature adaptation framework that addresses multi-level domain
variations through parameter-efficient adapters. Our method employs
Protocol-Level Adapters for sequence-specific characteristics and Center-Level
Adapters for scanner-dependent variations, built upon a variational unrolling
backbone. A Universal Adapter enables generalization to entirely unseen centers
through stochastic training that learns center-invariant adaptations. The
framework utilizes multi-scale SSIM loss with frequency domain enhancement and
contrast-adaptive weighting for robust optimization. Comprehensive evaluation
on the CMRxRecon2025 dataset spanning 5+ centers, 10+ scanners, and 9
modalities demonstrates superior cross-center generalization while maintaining
reconstruction quality. code: https://github.com/Ruru-Xu/HierAdaptMR

</details>


### [146] [IntelliCap: Intelligent Guidance for Consistent View Sampling](https://arxiv.org/abs/2508.13043)
*Ayaka Yasunaga,Hideo Saito,Dieter Schmalstieg,Shohei Mori*

Main category: cs.CV

TL;DR: 研究提出一种创新的分层扫描可视化技术，使用语义分割和视觉语言模型引导场景图像采集，提高视角合成效果。


<details>
  <summary>Details</summary>
Motivation: 现有图像采集方式难以满足高质量视角合成的密集、均匀采样需求，尤其在多人操作或复杂场景下问题更加突出。

Method: 引入语义分割和视觉语言模型对重要物体进行识别与排序，通过生成球形代理指导操作员进行多尺度扫描，以更全面覆盖相关视角。

Result: 优化后的图像采集通过实验在真实场景中相比传统策略表现更优，能更好支持图像视角合成过程。

Conclusion: 该方法为提高场景扫描质量提供了有效工具，改进了生成高保真合成图像的工作流。

Abstract: Novel view synthesis from images, for example, with 3D Gaussian splatting,
has made great progress. Rendering fidelity and speed are now ready even for
demanding virtual reality applications. However, the problem of assisting
humans in collecting the input images for these rendering algorithms has
received much less attention. High-quality view synthesis requires uniform and
dense view sampling. Unfortunately, these requirements are not easily addressed
by human camera operators, who are in a hurry, impatient, or lack understanding
of the scene structure and the photographic process. Existing approaches to
guide humans during image acquisition concentrate on single objects or neglect
view-dependent material characteristics. We propose a novel situated
visualization technique for scanning at multiple scales. During the scanning of
a scene, our method identifies important objects that need extended image
coverage to properly represent view-dependent appearance. To this end, we
leverage semantic segmentation and category identification, ranked by a
vision-language model. Spherical proxies are generated around highly ranked
objects to guide the user during scanning. Our results show superior
performance in real scenes compared to conventional view sampling strategies.

</details>


### [147] [Odo: Depth-Guided Diffusion for Identity-Preserving Body Reshaping](https://arxiv.org/abs/2508.13065)
*Siddharth Khandelwal,Sridhar Kamath,Arjun Jain*

Main category: cs.CV

TL;DR: 本文提出了人类身体形状编辑方法，通过一个新的大规模数据集和基于扩散模型的端到端方法实现精确和直观的体型控制。


<details>
  <summary>Details</summary>
Motivation: 现有人体形状编辑方法存在比例不真实、纹理失真和背景不一致的问题，且缺乏大规模公开数据集支持该领域发展。

Method: 提出了Odo方法，结合冻结的UNet和ControlNet，通过SMPL深度图实现语义驱动的细化形状变换。

Result: 方法在实验中表现优越，达到了7.5mm的顶点重建误差，超越基线方法的13.6mm，同时生成符合目标形状的高质量结果。

Conclusion: Odo方法通过利用新数据集和创新技术，为人体形状编辑提供了更高精度及可控的解决方案，推动了该领域发展。

Abstract: Human shape editing enables controllable transformation of a person's body
shape, such as thin, muscular, or overweight, while preserving pose, identity,
clothing, and background. Unlike human pose editing, which has advanced
rapidly, shape editing remains relatively underexplored. Current approaches
typically rely on 3D morphable models or image warping, often introducing
unrealistic body proportions, texture distortions, and background
inconsistencies due to alignment errors and deformations. A key limitation is
the lack of large-scale, publicly available datasets for training and
evaluating body shape manipulation methods. In this work, we introduce the
first large-scale dataset of 18,573 images across 1523 subjects, specifically
designed for controlled human shape editing. It features diverse variations in
body shape, including fat, muscular and thin, captured under consistent
identity, clothing, and background conditions. Using this dataset, we propose
Odo, an end-to-end diffusion-based method that enables realistic and intuitive
body reshaping guided by simple semantic attributes. Our approach combines a
frozen UNet that preserves fine-grained appearance and background details from
the input image with a ControlNet that guides shape transformation using target
SMPL depth maps. Extensive experiments demonstrate that our method outperforms
prior approaches, achieving per-vertex reconstruction errors as low as 7.5mm,
significantly lower than the 13.6mm observed in baseline methods, while
producing realistic results that accurately match the desired target shapes.

</details>


### [148] [Eyes on the Image: Gaze Supervised Multimodal Learning for Chest X-ray Diagnosis and Report Generation](https://arxiv.org/abs/2508.13068)
*Tanjim Islam Riju,Shuchismita Anwar,Saman Sarker Joy,Farig Sadeque,Swakkhar Shatabda*

Main category: cs.CV

TL;DR: 本文提出了一个两阶段多模态框架，用于胸部X射线疾病分类和放射学报告生成，利用MIMIC-Eye数据集，并将放射学家眼动数据纳入模型以提高性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是改进基于胸部X射线的疾病诊断分类及生成具备可解释性的放射学报告，促进医疗诊断的效率与准确性。

Method: 第一阶段提出了一种注视引导的对比学习架构，用多项眼动注意力损失函数结合视觉特征和眼动数据改进疾病分类。第二阶段开发了模块化报告生成管道，用诊断关键词与解剖学区域对齐生成报告句子。

Result: 眼动数据的引入使F1分数从0.597提高到0.631（+5.70%），AUC从0.821增加到0.849（+3.41%），报告的关键词召回和ROUGE覆盖率也有所提高。

Conclusion: 整合眼动数据不仅提升了疾病分类性能，还改进了生成医疗报告的解释性与质量，证明方法的有效性。

Abstract: We propose a two-stage multimodal framework that enhances disease
classification and region-aware radiology report generation from chest X-rays,
leveraging the MIMIC-Eye dataset. In the first stage, we introduce a
gaze-guided contrastive learning architecture for disease classification. It
integrates visual features, clinical labels, bounding boxes, and radiologist
eye-tracking signals and is equipped with a novel multi-term gaze-attention
loss combining MSE, KL divergence, correlation, and center-of-mass alignment.
Incorporating fixations improves F1 score from 0.597 to 0.631 (+5.70%) and AUC
from 0.821 to 0.849 (+3.41%), while also improving precision and recall,
highlighting the effectiveness of gaze-informed attention supervision. In the
second stage, we present a modular report generation pipeline that extracts
confidence-weighted diagnostic keywords, maps them to anatomical regions using
a curated dictionary constructed from domain-specific priors, and generates
region-aligned sentences via structured prompts. This pipeline improves report
quality as measured by clinical keyword recall and ROUGE overlap. Our results
demonstrate that integrating gaze data improves both classification performance
and the interpretability of generated medical reports.

</details>


### [149] [ID-Card Synthetic Generation: Toward a Simulated Bona fide Dataset](https://arxiv.org/abs/2508.13078)
*Qingwen Zeng,Juan E. Tapia,Izan Garcia,Juan M. Espin,Christoph Busch*

Main category: cs.CV

TL;DR: 提出一种利用Stable Diffusion生成合成正版图像的方法，以改进身份卡的攻击检测系统。


<details>
  <summary>Details</summary>
Motivation: 解决身份卡攻击检测系统因正版图像不足导致训练困难及攻击工具多样化的问题。

Method: 通过Stable Diffusion生成合成正版图像，并测试这些图像在训练和商用系统中的表现。

Result: 生成的图像被识别为正版图像，增强了检测性能并缓解数据限制问题。

Conclusion: 方法成功改善了身份卡Presentation Attack Detection系统的泛化能力和数据限制。

Abstract: Nowadays, the development of a Presentation Attack Detection (PAD) system for
ID cards presents a challenge due to the lack of images available to train a
robust PAD system and the increase in diversity of possible attack instrument
species. Today, most algorithms focus on generating attack samples and do not
take into account the limited number of bona fide images. This work is one of
the first to propose a method for mimicking bona fide images by generating
synthetic versions of them using Stable Diffusion, which may help improve the
generalisation capabilities of the detector. Furthermore, the new images
generated are evaluated in a system trained from scratch and in a commercial
solution. The PAD system yields an interesting result, as it identifies our
images as bona fide, which has a positive impact on detection performance and
data restrictions.

</details>


### [150] [Checkmate: interpretable and explainable RSVQA is the endgame](https://arxiv.org/abs/2508.13086)
*Lucrezia Tosato,Christel Tartini Chappuis,Syrielle Montariol,Flora Weissgerber,Sylvain Lobry,Devis Tuia*

Main category: cs.CV

TL;DR: 提出了一个针对遥感视觉问答(RSVQA)的新数据集“Chessboard”，以及一个解释性和可解释性模型“Checkmate”。


<details>
  <summary>Details</summary>
Motivation: 解决RSVQA模型缺乏可解释性和解释性，以及由于数据集分布偏差导致的快捷学习问题。

Method: 引入了包含3,123,253个问题且答案分布平衡的Chessboard数据集，并开发了一个可解释的模型Checkmate，该模型识别与其决策最相关的图像单元格。

Result: 实验表明，该方法提高了RSVQA系统决策的透明性，并支持更值得信赖的决策。

Conclusion: Chessboard数据集及Checkmate模型显著提升了RSVQA系统的透明性和可信赖性。

Abstract: Remote Sensing Visual Question Answering (RSVQA) presents unique challenges
in ensuring that model decisions are both understandable and grounded in visual
content. Current models often suffer from a lack of interpretability and
explainability, as well as from biases in dataset distributions that lead to
shortcut learning. In this work, we tackle these issues by introducing a novel
RSVQA dataset, Chessboard, designed to minimize biases through 3'123'253
questions and a balanced answer distribution. Each answer is linked to one or
more cells within the image, enabling fine-grained visual reasoning.
  Building on this dataset, we develop an explainable and interpretable model
called Checkmate that identifies the image cells most relevant to its
decisions. Through extensive experiments across multiple model architectures,
we show that our approach improves transparency and supports more trustworthy
decision-making in RSVQA systems.

</details>


### [151] [DMS:Diffusion-Based Multi-Baseline Stereo Generation for Improving Self-Supervised Depth Estimation](https://arxiv.org/abs/2508.13091)
*Zihua Liu,Yizhou Li,Songyan Zhang,Masatoshi Okutomi*

Main category: cs.CV

TL;DR: DMS利用扩散模型的几何先验进行自监督的立体匹配和单目深度估计，解决了光度重建模糊问题，达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的基于学习的立体匹配和单目深度估计尽管进步显著，但自监督方法特别是在处理目标视图内的不确定区域（如遮挡、视场外区域）时仍有较多挑战。

Method: 提出了一种名为DMS的模型无关方法，通过扩散模型的几何先验合成沿对极方向的新视图，以解决光度重建的模糊问题。具体地，微调稳定扩散模型生成若干关键视图，例如从左摄像头偏移的左视图、从右摄像头偏移的右视图，以及左右两摄像头之间的新视图。

Result: 实验表明DMS方法在多个基准数据集上实现了高达35%的异常点减少和最先进的性能表现。

Conclusion: DMS方法是一种无需额外成本、即插即用的创新方案，通过未标注的立体图像对实现了高效的自监督立体匹配与单目深度估计。

Abstract: While supervised stereo matching and monocular depth estimation have advanced
significantly with learning-based algorithms, self-supervised methods using
stereo images as supervision signals have received relatively less focus and
require further investigation. A primary challenge arises from ambiguity
introduced during photometric reconstruction, particularly due to missing
corresponding pixels in ill-posed regions of the target view, such as
occlusions and out-of-frame areas. To address this and establish explicit
photometric correspondences, we propose DMS, a model-agnostic approach that
utilizes geometric priors from diffusion models to synthesize novel views along
the epipolar direction, guided by directional prompts. Specifically, we
finetune a Stable Diffusion model to simulate perspectives at key positions:
left-left view shifted from the left camera, right-right view shifted from the
right camera, along with an additional novel view between the left and right
cameras. These synthesized views supplement occluded pixels, enabling explicit
photometric reconstruction. Our proposed DMS is a cost-free, ''plug-and-play''
method that seamlessly enhances self-supervised stereo matching and monocular
depth estimation, and relies solely on unlabeled stereo image pairs for both
training and synthesizing. Extensive experiments demonstrate the effectiveness
of our approach, with up to 35% outlier reduction and state-of-the-art
performance across multiple benchmark datasets.

</details>


### [152] [Real-Time Beach Litter Detection and Counting: A Comparative Analysis of RT-DETR Model Variants](https://arxiv.org/abs/2508.13101)
*Miftahul Huda,Arsyiah Azahra,Putri Maulida Chairani,Dimas Rizky Ramadhani,Nabila Azhari,Ade Lailani*

Main category: cs.CV

TL;DR: 本文探讨了RT-DETR模型用于自动检测和统计海滩垃圾的效果，并比较了两种模型变体的性能，RT-DETR-X尽管精度稍高，但RT-DETR-L更适合实时使用。


<details>
  <summary>Details</summary>
Motivation: 应对全球沿海污染问题并推动环境保护，需要开发可扩展和自动化的监测方法。

Method: 使用RT-DETR模型进行垃圾检测，并比较两种变体（RT-DETR-L与RT-DETR-X）的性能，包括精度和推理速度。

Result: RT-DETR-X的检测精度略高（mAP@50为0.816，mAP@50-95为0.612），但RT-DETR-L推理速度更快（20.1 ms对比34.5 ms）。

Conclusion: RT-DETR-L因其较佳的处理速度和平衡的检测性能，更适合实时场景使用，这项研究强调了模型复杂性与操作可行性之间的权衡。

Abstract: Coastal pollution is a pressing global environmental issue, necessitating
scalable and automated solutions for monitoring and management. This study
investigates the efficacy of the Real-Time Detection Transformer (RT-DETR), a
state-of-the-art, end-to-end object detection model, for the automated
detection and counting of beach litter. A rigorous comparative analysis is
conducted between two model variants, RT-DETR-Large (RT-DETR-L) and
RT-DETR-Extra-Large (RT-DETR-X), trained on a publicly available dataset of
coastal debris. The evaluation reveals that the RT-DETR-X model achieves
marginally superior accuracy, with a mean Average Precision at 50\% IoU
(mAP@50) of 0.816 and a mAP@50-95 of 0.612, compared to the RT-DETR-L model's
0.810 and 0.606, respectively. However, this minor performance gain is realized
at a significant computational cost; the RT-DETR-L model demonstrates a
substantially faster inference time of 20.1 ms versus 34.5 ms for the
RT-DETR-X. The findings suggest that the RT-DETR-L model offers a more
practical and efficient solution for real-time, in-field deployment due to its
superior balance of processing speed and detection accuracy. This research
provides valuable insights into the application of advanced Transformer-based
detectors for environmental conservation, highlighting the critical trade-offs
between model complexity and operational viability.

</details>


### [153] [Precise Action-to-Video Generation Through Visual Action Prompts](https://arxiv.org/abs/2508.13104)
*Yuang Wang,Chao Wen,Haoyu Guo,Sida Peng,Minghan Qin,Hujun Bao,Xiaowei Zhou,Ruizhen Hu*

Main category: cs.CV

TL;DR: 本文提出了一种统一的动作表示方法：视觉动作提示，能够生成复杂高自由度交互的动作视频，同时保持跨领域的视觉动态可迁移性。


<details>
  <summary>Details</summary>
Motivation: 当前基于动作的视频生成存在精确性与通用性之间的权衡问题。作者希望通过设计一种既能保持动作几何精确性，又适用于跨领域复杂动作的通用表示，来解决该问题。

Method: 提出使用视觉骨架作为域不可知的动作表示，并建立稳健的管道从包含丰富交互数据的人类物体交互和灵巧机器人操作中构建骨架。这些骨架通过轻量级微调集成进预训练的视频生成模型，从而实现复杂动作的精确控制和跨领域动态的学习。

Result: 实验证明，本文方法在EgoVid、RT-1和DROID数据集上表现出色，验证了其在复杂交互动作中的精准控制能力和跨领域通用性的有效性。

Conclusion: 通过视觉骨架的引入，成功地在动作生成中平衡了动作的精确性和跨领域动态的适应性，这为复杂交互动作的视频生成提供了一个强有力的解决方案。

Abstract: We present visual action prompts, a unified action representation for
action-to-video generation of complex high-DoF interactions while maintaining
transferable visual dynamics across domains. Action-driven video generation
faces a precision-generality trade-off: existing methods using text, primitive
actions, or coarse masks offer generality but lack precision, while
agent-centric action signals provide precision at the cost of cross-domain
transferability. To balance action precision and dynamic transferability, we
propose to "render" actions into precise visual prompts as domain-agnostic
representations that preserve both geometric precision and cross-domain
adaptability for complex actions; specifically, we choose visual skeletons for
their generality and accessibility. We propose robust pipelines to construct
skeletons from two interaction-rich data sources - human-object interactions
(HOI) and dexterous robotic manipulation - enabling cross-domain training of
action-driven generative models. By integrating visual skeletons into
pretrained video generation models via lightweight fine-tuning, we enable
precise action control of complex interaction while preserving the learning of
cross-domain dynamics. Experiments on EgoVid, RT-1 and DROID demonstrate the
effectiveness of our proposed approach. Project page:
https://zju3dv.github.io/VAP/.

</details>


### [154] [Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence](https://arxiv.org/abs/2508.13139)
*Ling-Hao Chen,Yuhong Zhang,Zixin Yin,Zhiyang Dou,Xin Chen,Jingbo Wang,Taku Komura,Lei Zhang*

Main category: cs.CV

TL;DR: 提出了Motion2Motion，一个无需训练的框架，用于在骨骼拓扑差异较大的角色之间转移动画。


<details>
  <summary>Details</summary>
Motivation: 现有技术难以在不同骨骼拓扑间精确地传递动画，且缺乏跨拓扑结构的大规模配对运动数据集。

Method: 通过少量示例动作和骨骼间稀疏的骨骼对应关系，在无训练的情况下实现角色动画转移。

Result: Motion2Motion在相似骨骼和跨物种骨骼之间的动画转移中，呈现了高效且可靠的性能。

Conclusion: Motion2Motion框架展示出应用于下游工业领域和用户界面的潜力，推动动画转移技术的发展。

Abstract: This work studies the challenge of transfer animations between characters
whose skeletal topologies differ substantially. While many techniques have
advanced retargeting techniques in decades, transfer motions across diverse
topologies remains less-explored. The primary obstacle lies in the inherent
topological inconsistency between source and target skeletons, which restricts
the establishment of straightforward one-to-one bone correspondences. Besides,
the current lack of large-scale paired motion datasets spanning different
topological structures severely constrains the development of data-driven
approaches. To address these limitations, we introduce Motion2Motion, a novel,
training-free framework. Simply yet effectively, Motion2Motion works with only
one or a few example motions on the target skeleton, by accessing a sparse set
of bone correspondences between the source and target skeletons. Through
comprehensive qualitative and quantitative evaluations, we demonstrate that
Motion2Motion achieves efficient and reliable performance in both
similar-skeleton and cross-species skeleton transfer scenarios. The practical
utility of our approach is further evidenced by its successful integration in
downstream applications and user interfaces, highlighting its potential for
industrial applications. Code and data are available at
https://lhchen.top/Motion2Motion.

</details>


### [155] [IGFuse: Interactive 3D Gaussian Scene Reconstruction via Multi-Scans Fusion](https://arxiv.org/abs/2508.13153)
*Wenhao Hu,Zesheng Li,Haonan Zhou,Liu Liu,Xuexiang Wen,Zhizhong Su,Xi Li,Gaoang Wang*

Main category: cs.CV

TL;DR: 提出了一种名为IGFuse的新框架，通过多次扫描融合数据，重建交互式的高斯场3D场景，解决了传统方法物体遮挡和传感器覆盖不足的问题。


<details>
  <summary>Details</summary>
Motivation: 解决传统3D场景重建中因物体遮挡和传感器限制所导致的结构细节丢失问题。

Method: 通过多次扫描期间的物体自然重新排列来揭示被遮挡区域，利用分割感知高斯场并在扫描间强制双向光度和语义一致性，引入伪中间场景状态和协同修剪策略校准几何。

Result: IGFuse实现了高保真渲染和无密集观察或复杂管道的物体级场景操作，实验验证其对新场景配置的强泛化能力。

Conclusion: 该框架对于现实3D重建和实现真实场景到模拟的转换有显著效果，有潜力推广应用于计算机视觉和机器人领域。

Abstract: Reconstructing complete and interactive 3D scenes remains a fundamental
challenge in computer vision and robotics, particularly due to persistent
object occlusions and limited sensor coverage. Multiview observations from a
single scene scan often fail to capture the full structural details. Existing
approaches typically rely on multi stage pipelines, such as segmentation,
background completion, and inpainting or require per-object dense scanning,
both of which are error-prone, and not easily scalable. We propose IGFuse, a
novel framework that reconstructs interactive Gaussian scene by fusing
observations from multiple scans, where natural object rearrangement between
captures reveal previously occluded regions. Our method constructs segmentation
aware Gaussian fields and enforces bi-directional photometric and semantic
consistency across scans. To handle spatial misalignments, we introduce a
pseudo-intermediate scene state for unified alignment, alongside collaborative
co-pruning strategies to refine geometry. IGFuse enables high fidelity
rendering and object level scene manipulation without dense observations or
complex pipelines. Extensive experiments validate the framework's strong
generalization to novel scene configurations, demonstrating its effectiveness
for real world 3D reconstruction and real-to-simulation transfer. Our project
page is available online.

</details>


### [156] [4DNeX: Feed-Forward 4D Generative Modeling Made Easy](https://arxiv.org/abs/2508.13154)
*Zhaoxi Chen,Tianqi Liu,Long Zhuo,Jiawei Ren,Zeng Tao,He Zhu,Fangzhou Hong,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: 本文提出了4DNeX，一个从单张图像生成动态3D（4D）场景表示的高效前馈框架，通过微调预训练的视频扩散模型实现端到端转换。


<details>
  <summary>Details</summary>
Motivation: 目前的方法依赖于计算密集型优化或多帧视频输入，而4D场景表示的生成尚缺乏高效的解决方案。

Method: 构建了一个大规模4D数据集4DNeX-10M，引入了统一的6D视频表示方式，并提出了一系列微调预训练模型的策略。

Result: 4DNeX生成了高质量的动态点云，并实现了新视角视频合成，显著优于现有方法。

Conclusion: 4DNeX提供了高效可扩展的图像到4D建模解决方案，为模拟动态场景发展的生成性4D世界模型奠定了基础。

Abstract: We present 4DNeX, the first feed-forward framework for generating 4D (i.e.,
dynamic 3D) scene representations from a single image. In contrast to existing
methods that rely on computationally intensive optimization or require
multi-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D
generation by fine-tuning a pretrained video diffusion model. Specifically, 1)
to alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale
dataset with high-quality 4D annotations generated using advanced
reconstruction approaches. 2) we introduce a unified 6D video representation
that jointly models RGB and XYZ sequences, facilitating structured learning of
both appearance and geometry. 3) we propose a set of simple yet effective
adaptation strategies to repurpose pretrained video diffusion models for 4D
modeling. 4DNeX produces high-quality dynamic point clouds that enable
novel-view video synthesis. Extensive experiments demonstrate that 4DNeX
outperforms existing 4D generation methods in efficiency and generalizability,
offering a scalable solution for image-to-4D modeling and laying the foundation
for generative 4D world models that simulate dynamic scene evolution.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [157] [Deep Language Geometry: Constructing a Metric Space from LLM Weights](https://arxiv.org/abs/2508.11676)
*Maksym Shamrai,Vladyslav Hamolia*

Main category: cs.CL

TL;DR: 提出利用大型语言模型内部权重激活来构建语言的度量空间的新框架，验证在106种语言下的表现并公开代码和工具。


<details>
  <summary>Details</summary>
Motivation: 传统基于人工设计特征的语言分析方法存在局限性，需要更自动化和高效的语言分析框架。

Method: 通过改进的剪枝算法计算语言模型的权重重要性分数，生成高维向量表征语言特征，并对多语言数据集和模型进行验证。

Result: 实验结果与已知语言家族一致，同时发现意料之外的语言间联系，揭示历史接触或语言演化信息。

Conclusion: 新方法具备学术与实际应用潜力，能够帮助更好地理解语言特性和发展，提供开放源码与工具促进相关研究。

Abstract: We introduce a novel framework that utilizes the internal weight activations
of modern Large Language Models (LLMs) to construct a metric space of
languages. Unlike traditional approaches based on hand-crafted linguistic
features, our method automatically derives high-dimensional vector
representations by computing weight importance scores via an adapted pruning
algorithm. Our approach captures intrinsic language characteristics that
reflect linguistic phenomena. We validate our approach across diverse datasets
and multilingual LLMs, covering 106 languages. The results align well with
established linguistic families while also revealing unexpected inter-language
connections that may indicate historical contact or language evolution. The
source code, computed language latent vectors, and visualization tool are made
publicly available at https://github.com/mshamrai/deep-language-geometry.

</details>


### [158] [Can we Evaluate RAGs with Synthetic Data?](https://arxiv.org/abs/2508.11758)
*Jonas van Elburg,Peter van der Putten,Maarten Marx*

Main category: cs.CL

TL;DR: 研究评估由大语言模型生成的合成问答数据作为人类标注数据替代方案的有效性。


<details>
  <summary>Details</summary>
Motivation: 探讨在缺乏人工标注数据的情况下，是否可以使用合成数据作为替代实验基准。

Method: 通过固定生成器改变检索器参数，或固定检索器参数改变生成器，评估合成数据在多个数据集上的表现。

Result: 合成基准在检索器配置的比较中与人工基准一致，但在生成器对比中表现不稳定。

Conclusion: 合成数据的限制可能来自于任务适配性和风格偏差的问题。

Abstract: We investigate whether synthetic question-answer (QA) data generated by large
language models (LLMs) can serve as an effective proxy for human-labeled
benchmarks when such data is unavailable. We assess the reliability of
synthetic benchmarks across two experiments: one varying retriever parameters
while keeping the generator fixed, and another varying the generator with fixed
retriever parameters. Across four datasets, of which two open-domain and two
proprietary, we find that synthetic benchmarks reliably rank the RAGs varying
in terms of retriever configuration, aligning well with human-labeled benchmark
baselines. However, they fail to produce consistent RAG rankings when comparing
generator architectures. The breakdown possibly arises from a combination of
task mismatch between the synthetic and human benchmarks, and stylistic bias
favoring certain generators.

</details>


### [159] [Limitation Learning: Catching Adverse Dialog with GAIL](https://arxiv.org/abs/2508.11767)
*Noah Kasmanoff,Rahul Zalkikar*

Main category: cs.CL

TL;DR: 研究利用模仿学习方法实现对话策略，发现策略有效的同时，也暴露出对话模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 探讨如何在没有奖励信号的情况下，通过模仿学习及专家演示，生成对话策略和模型评价。

Method: 采用模仿学习方法，根据输入对话状态，生成能与用户进行对话的策略，并训练一个判别器区分专家与合成对话。

Result: 成功生成了有效的对话策略，同时通过判别器的结果指出了对话模型存在的局限性。

Conclusion: 此方法可以用来识别对话模型中常见的不良行为，推动对话任务的数据模型改进。

Abstract: Imitation learning is a proven method for creating a policy in the absence of
rewards, by leveraging expert demonstrations. In this work, we apply imitation
learning to conversation. In doing so, we recover a policy capable of talking
to a user given a prompt (input state), and a discriminator capable of
classifying between expert and synthetic conversation. While our policy is
effective, we recover results from our discriminator that indicate the
limitations of dialog models. We argue that this technique can be used to
identify adverse behavior of arbitrary data models common for dialog oriented
tasks.

</details>


### [160] [Investigating Transcription Normalization in the Faetar ASR Benchmark](https://arxiv.org/abs/2508.11771)
*Leo Peckham,Michael Ong,Naomi Nagy,Ewan Dunbar*

Main category: cs.CL

TL;DR: 本文研究了低资源ASR基准Faetar中的转录一致性问题，发现转录的不一致性不是任务的主要挑战，并提出有限词汇表约束对解码有帮助。


<details>
  <summary>Details</summary>
Motivation: 探索影响低资源ASR基准Faetar性能的主要因素，特别是转录一致性对任务的影响。

Method: 使用一个小型手工构建的词典，分析转录一致性问题，并研究n元语法语言模型及有限词汇表约束对任务的影响。

Result: 发现转录的不一致性并不是任务的主要挑战，大词语模型无益，但有限词汇表约束有助于解码。

Conclusion: 虽然转录一致性存在问题，但挑战的主要来源并非于此；合理的方法可以改善解码，但整体任务仍非常困难。

Abstract: We examine the role of transcription inconsistencies in the Faetar Automatic
Speech Recognition benchmark, a challenging low-resource ASR benchmark. With
the help of a small, hand-constructed lexicon, we conclude that find that,
while inconsistencies do exist in the transcriptions, they are not the main
challenge in the task. We also demonstrate that bigram word-based language
modelling is of no added benefit, but that constraining decoding to a finite
lexicon can be beneficial. The task remains extremely difficult.

</details>


### [161] [A Multi-Task Evaluation of LLMs' Processing of Academic Text Input](https://arxiv.org/abs/2508.11779)
*Tianyi Li,Yu Qin,Olivia R. Liu Sheng*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在学术发现和学术评审中的潜力，组织了用于评估LLMs处理学术文本的四项任务。结果显示，LLMs在学术文本的摘要和释义上表现尚可，但在文本比较、评估和反思方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在学术发现和评审中实际应用的可行性，特别是在文本分析任务中的能力。

Method: 定义了四个评估任务（内容复述、比较、评分和反思），并为每项任务分配具体角色（如先知、判断仲裁者、知识仲裁者、合作者），测试模型对学术文本理解及生成能力，同时使用顶级信息系统期刊文章作为评估素材，并采用多种指标进行测试。

Result: 评估发现，Google的Gemini模型在学术文本摘要和释义方面可靠性可接受，但在文本排名、评分及反思等任务中表现较为弱势，其对于学术研究的启发性较低。

Conclusion: 不建议在未充分审查情况下使用大型语言模型进行学术评审。

Abstract: How much large language models (LLMs) can aid scientific discovery, notably
in assisting academic peer review, is in heated debate. Between a literature
digest and a human-comparable research assistant lies their practical
application potential. We organize individual tasks that computer science
studies employ in separate terms into a guided and robust workflow to evaluate
LLMs' processing of academic text input. We employ four tasks in the
assessment: content reproduction/comparison/scoring/reflection, each demanding
a specific role of the LLM (oracle/judgmental arbiter/knowledgeable
arbiter/collaborator) in assisting scholarly works, and altogether testing LLMs
with questions that increasingly require intellectual capabilities towards a
solid understanding of scientific texts to yield desirable solutions. We
exemplify a rigorous performance evaluation with detailed instructions on the
prompts. Adopting first-rate Information Systems articles at three top journals
as the input texts and an abundant set of text metrics, we record a compromised
performance of the leading LLM - Google's Gemini: its summary and paraphrase of
academic text is acceptably reliable; using it to rank texts through pairwise
text comparison is faintly scalable; asking it to grade academic texts is prone
to poor discrimination; its qualitative reflection on the text is
self-consistent yet hardly insightful to inspire meaningful research. This
evidence against an endorsement of LLMs' text-processing capabilities is
consistent across metric-based internal (linguistic assessment), external
(comparing to the ground truth), and human evaluation, and is robust to the
variations of the prompt. Overall, we do not recommend an unchecked use of LLMs
in constructing peer reviews.

</details>


### [162] [LLM-Guided Planning and Summary-Based Scientific Text Simplification: DS@GT at CLEF 2025 SimpleText](https://arxiv.org/abs/2508.11816)
*Krishna Chaitanya Marturi,Heba H. Elwazzan*

Main category: cs.CL

TL;DR: 提出一种基于大语言模型（LLM）的两阶段框架，用于科学文本的句子级和文档级简化。


<details>
  <summary>Details</summary>
Motivation: 设计一个同时针对句子级和文档级的科学文本简化方法，改善简化的连贯性与上下文准确性。

Method: 句子级简化：利用LLM生成结构化计划，然后进行计划驱动的简化；文档级简化：利用LLM生成简要总结并指导简化流程。

Result: 这种框架提升了科学文本简化的连贯性和上下文的忠实度。

Conclusion: 基于LLM的两阶段方法在科学文本的简化任务中具有显著优势。

Abstract: In this paper, we present our approach for the CLEF 2025 SimpleText Task 1,
which addresses both sentence-level and document-level scientific text
simplification. For sentence-level simplification, our methodology employs
large language models (LLMs) to first generate a structured plan, followed by
plan-driven simplification of individual sentences. At the document level, we
leverage LLMs to produce concise summaries and subsequently guide the
simplification process using these summaries. This two-stage, LLM-based
framework enables more coherent and contextually faithful simplifications of
scientific text.

</details>


### [163] [Hallucination Detection and Mitigation in Scientific Text Simplification using Ensemble Approaches: DS@GT at CLEF 2025 SimpleText](https://arxiv.org/abs/2508.11823)
*Krishna Chaitanya Marturi,Heba H. Elwazzan*

Main category: cs.CL

TL;DR: 这篇论文提出了一种方法，用于检测和评估科学文本简化中的创意生成和信息扭曲问题。


<details>
  <summary>Details</summary>
Motivation: 针对CLEF 2025 SimpleText任务2, 力图解决科学文本简化中可能出现的创意生成与信息失真的检测和评估问题。

Method: 文章构建了一个集成框架，结合BERT分类器、语义相似度测量、自然语言推理模型和大型语言模型推理的数据，多信号通过元分类器融合。此外，通过基于大型语言模型的后编辑系统实现文本简化后的修订。

Result: 方法增强了对偏差和失真检测的鲁棒性，改进了基于输入文本的简化生成效果。

Conclusion: 使用整合不同技术的框架解决了特定科学文本简化任务中的挑战。

Abstract: In this paper, we describe our methodology for the CLEF 2025 SimpleText Task
2, which focuses on detecting and evaluating creative generation and
information distortion in scientific text simplification. Our solution
integrates multiple strategies: we construct an ensemble framework that
leverages BERT-based classifier, semantic similarity measure, natural language
inference model, and large language model (LLM) reasoning. These diverse
signals are combined using meta-classifiers to enhance the robustness of
spurious and distortion detection. Additionally, for grounded generation, we
employ an LLM-based post-editing system that revises simplifications based on
the original input texts.

</details>


### [164] [A Survey of Idiom Datasets for Psycholinguistic and Computational Research](https://arxiv.org/abs/2508.11828)
*Michael Flor,Xinyi Liu,Anna Feldman*

Main category: cs.CL

TL;DR: 文章回顾了心理语言学和计算语言学中关于研究习语的数据集，并分析了其内容、形式和目标用途。


<details>
  <summary>Details</summary>
Motivation: 为了改进习语研究，该研究调查了现有的心理语言学和计算语言学数据集，试图填补两者之间的差距。

Method: 作者对53个数据集的注释实践、覆盖范围和任务设置进行了分析。

Result: 尽管近期研究扩展了语言覆盖面和任务多样性，但心理语言学和计算研究之间仍缺乏联系。

Conclusion: 跨领域合作及进一步整合心理语言学与计算方法对习语研究至关重要。

Abstract: Idioms are figurative expressions whose meanings often cannot be inferred
from their individual words, making them difficult to process computationally
and posing challenges for human experimental studies. This survey reviews
datasets developed in psycholinguistics and computational linguistics for
studying idioms, focusing on their content, form, and intended use.
Psycholinguistic resources typically contain normed ratings along dimensions
such as familiarity, transparency, and compositionality, while computational
datasets support tasks like idiomaticity detection/classification,
paraphrasing, and cross-lingual modeling. We present trends in annotation
practices, coverage, and task framing across 53 datasets. Although recent
efforts expanded language coverage and task diversity, there seems to be no
relation yet between psycholinguistic and computational research on idioms.

</details>


### [165] [Every 28 Days the AI Dreams of Soft Skin and Burning Stars: Scaffolding AI Agents with Hormones and Emotions](https://arxiv.org/abs/2508.11829)
*Leigh Levinson,Christopher J. Agostino*

Main category: cs.CL

TL;DR: 本文研究了通过模拟生物节律（尤其是激素周期）来改进人工智能系统处理框架问题的能力。实验结果表明此方法可在一定程度上提升模型性能，同时揭示了模型中暗含的性别与生物偏见。


<details>
  <summary>Details</summary>
Motivation: AI系统在筛选相关信息以应对框架问题时，仍存在挑战，而生物节律的规律性可能为此提供一种天然的启发式解决方案。

Method: 提出一种框架，在大型语言模型中引入模拟的生物节律（包括月经周期和昼夜节律），通过基于激素水平（雌激素、睾酮和皮质醇等）的周期函数生成的系统提示嵌入模型中。

Result: 实验表明，不同模型的情感和风格变化与生物节律相符，例如情感在月经周期和昼夜节律中的特定阶段表现出明显模式，同时模型在中等激素范围内的表现最佳。

Conclusion: 研究表明，将生物节律嵌入AI系统中可为改善信息筛选提供一种新方法，同时洞察了语言模型中存在的性别与生物相关的偏见。

Abstract: Despite significant advances, AI systems struggle with the frame problem:
determining what information is contextually relevant from an exponentially
large possibility space. We hypothesize that biological rhythms, particularly
hormonal cycles, serve as natural relevance filters that could address this
fundamental challenge. We develop a framework that embeds simulated menstrual
and circadian cycles into Large Language Models through system prompts
generated from periodic functions modeling key hormones including estrogen,
testosterone, and cortisol. Across multiple state-of-the-art models, linguistic
analysis reveals emotional and stylistic variations that track biological
phases; sadness peaks during menstruation while happiness dominates ovulation
and circadian patterns show morning optimism transitioning to nocturnal
introspection. Benchmarking on SQuAD, MMLU, Hellaswag, and AI2-ARC demonstrates
subtle but consistent performance variations aligning with biological
expectations, including optimal function in moderate rather than extreme
hormonal ranges. This methodology provides a novel approach to contextual AI
while revealing how societal biases regarding gender and biology are embedded
within language models.

</details>


### [166] [When Does Language Transfer Help? Sequential Fine-Tuning for Cross-Lingual Euphemism Detection](https://arxiv.org/abs/2508.11831)
*Julia Sammartino,Libby Barak,Jing Peng,Anna Feldman*

Main category: cs.CL

TL;DR: 本文探讨了通过跨语言迁移和顺序微调如何提高多语种掩饰语检测能力，尤其在低资源语言中，例如约鲁巴语与土耳其语。


<details>
  <summary>Details</summary>
Motivation: 掩饰语识别任务具有文化差异性且易引发歧义，特别是在低资源语言环境下的挑战亟需解决。

Method: 使用XLM-R和mBERT模型，比较单语微调、同时微调及顺序微调效果，分析了语言配对、特征类型及预训练覆盖对模型性能的影响。

Result: 结果显示，使用高资源语言进行顺序微调可显著提升低资源语言表现，XLM-R增益更大，但对预训练的间隙和灾难性遗忘更敏感；mBERT尽管表现更稳定，但整体效果较低。

Conclusion: 顺序微调是改善低资源语言中掩饰语检测的简单有效策略，适用于多语种模型。

Abstract: Euphemisms are culturally variable and often ambiguous, posing challenges for
language models, especially in low-resource settings. This paper investigates
how cross-lingual transfer via sequential fine-tuning affects euphemism
detection across five languages: English, Spanish, Chinese, Turkish, and
Yoruba. We compare sequential fine-tuning with monolingual and simultaneous
fine-tuning using XLM-R and mBERT, analyzing how performance is shaped by
language pairings, typological features, and pretraining coverage. Results show
that sequential fine-tuning with a high-resource L1 improves L2 performance,
especially for low-resource languages like Yoruba and Turkish. XLM-R achieves
larger gains but is more sensitive to pretraining gaps and catastrophic
forgetting, while mBERT yields more stable, though lower, results. These
findings highlight sequential fine-tuning as a simple yet effective strategy
for improving euphemism detection in multilingual models, particularly when
low-resource languages are involved.

</details>


### [167] [SupraTok: Cross-Boundary Tokenization for Enhanced Language Model Performance](https://arxiv.org/abs/2508.11857)
*Andrei-Valentin Tănase,Elena Pelican*

Main category: cs.CL

TL;DR: SupraTok提出了一种改进的分词架构，通过学习多词语义单元、优化训练语料质量和多阶段课程学习，提升了分词效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型架构取得了进展，但分词策略较少改进，存在提高优化潜力。

Method: 通过跨边界模式学习、基于熵的数据筛选、以及多阶段课程学习，开发了一种基于超级词单元（SupraTok）的新分词方法，并与现有模型进行比较。

Result: 英文本地分词效率提高31%，实现了对其他语言的高效支持，与GPT-2模型结合后在多个基准测试中实现8.4%-9.5%的性能提升。

Conclusion: 有效的分词策略可补充现有架构创新，提升语言模型的总体性能。

Abstract: Tokenization remains a fundamental yet underexplored bottleneck in natural
language processing, with strategies largely static despite remarkable progress
in model architectures. We present SupraTok, a novel tokenization architecture
that reimagines subword segmentation through three innovations: cross-boundary
pattern learning that discovers multi-word semantic units, entropy-driven data
curation that optimizes training corpus quality, and multi-phase curriculum
learning for stable convergence. Our approach extends Byte-Pair Encoding by
learning "superword" tokens, coherent multi-word expressions that preserve
semantic unity while maximizing compression efficiency. SupraTok achieves 31%
improvement in English tokenization efficiency (5.91 versus 4.51 characters per
token) compared to OpenAI's o200k tokenizer and 30% improvement over Google's
Gemma 3 tokenizer (256k vocabulary), while maintaining competitive performance
across 38 languages. When integrated with a GPT-2 scale model (124M parameters)
trained on 10 billion tokens from the FineWeb-Edu dataset, SupraTok yields 8.4%
improvement on HellaSWAG and 9.5% on MMLU benchmarks without architectural
modifications. While these results are promising at this scale, further
validation at larger model scales is needed. These findings suggest that
efficient tokenization can complement architectural innovations as a path to
improved language model performance.

</details>


### [168] [In-Context Examples Matter: Improving Emotion Recognition in Conversation with Instruction Tuning](https://arxiv.org/abs/2508.11889)
*Hui Ma,Bo Zhang,Jinpeng Hu,Zenglin Shi*

Main category: cs.CL

TL;DR: 提出了一个新的情境指令调整框架InitERC，用于在对话中识别情绪，并展现了显著性能改进。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能有效捕捉说话人特征与对话上下文的动态交互，导致情绪识别框架内的各个要素弱相关。

Method: 设计了一个单阶段情境指令调整框架InitERC，包含示例池构建、情境实例选择、提示模板设计和情境指令调整四个模块。

Result: 在三个常用数据集上的实验表明，InitERC相较于最先进的基线方法有明显性能提升。

Conclusion: InitERC通过单阶段调整实现了说话人、上下文与情绪状态的对齐，简单有效，证实了其实用价值。

Abstract: Emotion recognition in conversation (ERC) aims to identify the emotion of
each utterance in a conversation, playing a vital role in empathetic artificial
intelligence. With the growing of large language models (LLMs), instruction
tuning has emerged as a critical paradigm for ERC. Existing studies mainly
focus on multi-stage instruction tuning, which first endows LLMs with speaker
characteristics, and then conducts context-aware instruction tuning to
comprehend emotional states. However, these methods inherently constrains the
capacity to jointly capture the dynamic interaction between speaker
characteristics and conversational context, resulting in weak alignment among
speaker identity, contextual cues, and emotion states within a unified
framework. In this paper, we propose InitERC, a simple yet effective one-stage
in-context instruction tuning framework for ERC. InitERC adapts LLMs to learn
speaker-context-emotion alignment from context examples via in-context
instruction tuning. Specifically, InitERC comprises four components, i.e.,
demonstration pool construction, in-context example selection, prompt template
design, and in-context instruction tuning. To explore the impact of in-context
examples, we conduct a comprehensive study on three key factors: retrieval
strategy, example ordering, and the number of examples. Extensive experiments
on three widely used datasets demonstrate that our proposed InitERC achieves
substantial improvements over the state-of-the-art baselines.

</details>


### [169] [CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures](https://arxiv.org/abs/2508.11915)
*Punya Syon Pandey,Yongjin Yang,Jiarui Liu,Zhijing Jin*

Main category: cs.CL

TL;DR: 该论文提出了一种度量多智能体系统中语言有效性的指标CORE（对话鲁棒性评价分数），深入分析了合作、竞争和中性环境下大型语言模型的对话特性。


<details>
  <summary>Details</summary>
Motivation: 由于多智能体系统中基于大型语言模型的交互表现出许多新兴能力，但其语言多样性鲜有被量化分析，因此该研究旨在通过量化指标进一步理解语言使用的质量和适应性。

Method: 引入CORE指标，通过综合熵、词汇重复率和语义相似性等维度评价对话质量，并基于Zipf定律和Heaps定律分析单词频率分布和词汇量增长。

Result: 研究发现，合作场景中Zipf分布更陡峭，Heap指数较高，表明合作中更多的词汇重复和扩展；竞争场景中语言重复较少且词汇受限。

Conclusion: CORE能够有效诊断多智能体系统中语言适应的鲁棒性，为理解社会动机如何影响语言互动提供了新视角，同时公开了代码以便进一步研究。

Abstract: Game-theoretic interactions between agents with Large Language Models (LLMs)
have revealed many emergent capabilities, yet the linguistic diversity of these
interactions has not been sufficiently quantified. In this paper, we present
the Conversational Robustness Evaluation Score: CORE, a metric to quantify the
effectiveness of language use within multi-agent systems across different
game-theoretic interactions. CORE integrates measures of cluster entropy,
lexical repetition, and semantic similarity, providing a direct lens of dialog
quality. We apply CORE to pairwise LLM dialogs across competitive, cooperative,
and neutral settings, further grounding our analysis in Zipf's and Heaps' Laws
to characterize word frequency distributions and vocabulary growth. Our
findings show that cooperative settings exhibit both steeper Zipf distributions
and higher Heap exponents, indicating more repetition alongside greater
vocabulary expansion. In contrast, competitive interactions display lower Zipf
and Heaps exponents, reflecting less repetition and more constrained
vocabularies. These results provide new insights into how social incentives
influence language adaptation, and highlight CORE as a robust diagnostic for
measuring linguistic robustness in multi-agent LLM systems. Our code is
available at https://github.com/psyonp/core.

</details>


### [170] [LLMs Struggle with NLI for Perfect Aspect: A Cross-Linguistic Study in Chinese and Japanese](https://arxiv.org/abs/2508.11927)
*Jie Lu,Du Jin,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 本文研究了汉语和日语中缺乏直接语法时态标记的完成体对自然语言推理的影响，并构建了一个基于模板的跨语言推理数据集，实验发现先进的语言模型难以识别复杂的时态和时间变化。


<details>
  <summary>Details</summary>
Motivation: 探究中日语言完成体中因缺乏时态标记对语言推理的影响，并检测现有高级语言模型在处理时态推理方面的局限性。

Method: 构建了一个基于语言学模板的自然语言推理数据集（每种语言1,350对），以分析语义推理中的时态和参考时间变化，数据集涵盖了汉语和日语文本。

Result: 实验表明，即使是高级语言模型也难以进行复杂的时态推理，尤其是在识别细微的时态和参考时间变化方面表现不佳。

Conclusion: 研究表明语言模型在时态推理方面仍有显著局限性，强调了在语义推理领域进行跨语言评估的重要性。

Abstract: Unlike English, which uses distinct forms (e.g., had, has, will have) to mark
the perfect aspect across tenses, Chinese and Japanese lack separate
grammatical forms for tense within the perfect aspect, which complicates
Natural Language Inference (NLI). Focusing on the perfect aspect in these
languages, we construct a linguistically motivated, template-based NLI dataset
(1,350 pairs per language). Experiments reveal that even advanced LLMs struggle
with temporal inference, particularly in detecting subtle tense and
reference-time shifts. These findings highlight model limitations and
underscore the need for cross-linguistic evaluation in temporal semantics. Our
dataset is available at https://github.com/Lujie2001/CrossNLI.

</details>


### [171] [CAMF: Collaborative Adversarial Multi-agent Framework for Machine Generated Text Detection](https://arxiv.org/abs/2508.11933)
*Yue Wang,Liesheng Wei,Yuxiang Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为CAMF的新框架，用于更高效地检测大型语言模型生成的文本。


<details>
  <summary>Details</summary>
Motivation: 应对机器生成文本可能引发的诸如虚假信息、学术不端等风险，目前的检测方法存在有效性不足的问题。

Method: 引入一个名为CAMF的框架，采用多阶段流程，包括多维语言特征提取、对抗一致性探测和综合判断聚合。

Result: 实验结果表明，CAMF在零样本检测中的表现显著优于现有方法。

Conclusion: CAMF通过深入分析文本的一致性与特征，提供了一种更精准的检测手段，应对当前机器生成文本的挑战。

Abstract: Detecting machine-generated text (MGT) from contemporary Large Language
Models (LLMs) is increasingly crucial amid risks like disinformation and
threats to academic integrity. Existing zero-shot detection paradigms, despite
their practicality, often exhibit significant deficiencies. Key challenges
include: (1) superficial analyses focused on limited textual attributes, and
(2) a lack of investigation into consistency across linguistic dimensions such
as style, semantics, and logic. To address these challenges, we introduce the
\textbf{C}ollaborative \textbf{A}dversarial \textbf{M}ulti-agent
\textbf{F}ramework (\textbf{CAMF}), a novel architecture using multiple
LLM-based agents. CAMF employs specialized agents in a synergistic three-phase
process: \emph{Multi-dimensional Linguistic Feature Extraction},
\emph{Adversarial Consistency Probing}, and \emph{Synthesized Judgment
Aggregation}. This structured collaborative-adversarial process enables a deep
analysis of subtle, cross-dimensional textual incongruities indicative of
non-human origin. Empirical evaluations demonstrate CAMF's significant
superiority over state-of-the-art zero-shot MGT detection techniques.

</details>


### [172] [Learning Wisdom from Errors: Promoting LLM's Continual Relation Learning through Exploiting Error Cases](https://arxiv.org/abs/2508.12031)
*Shaozhe Yin,Jinyu Guo,Kai Shuang,Xia Liu,Ruize Ou*

Main category: cs.CL

TL;DR: 提出一种基于指令的持续对比微调策略，用于增强大语言模型在持续关系抽取任务中的表现，尤其关注错误案例的利用。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分利用错误案例来揭示模型的认知偏差，这成为提升持续关系抽取模型表现的障碍。

Method: 通过将任务数据依据初始响应的正确性分为两部分，进行双任务微调；并设计基于指令的对比微调策略，借助LLM的指令跟随能力减少旧任务与新任务之间的差距。

Result: 该方法在TACRED和FewRel数据集上达成了新的最优表现，大幅提升了CRE任务的效果。

Conclusion: 特别关注错误案例并设计合适的微调策略，对提升持续关系抽取模型的性能至关重要。

Abstract: Continual Relation Extraction (CRE) aims to continually learn new emerging
relations while avoiding catastrophic forgetting. Existing CRE methods mainly
use memory replay and contrastive learning to mitigate catastrophic forgetting.
However, these methods do not attach importance to the error cases that can
reveal the model's cognitive biases more effectively. To address this issue, we
propose an instruction-based continual contrastive tuning approach for Large
Language Models (LLMs) in CRE. Different from existing CRE methods that
typically handle the training and memory data in a unified manner, this
approach splits the training and memory data of each task into two parts
respectively based on the correctness of the initial responses and treats them
differently through dual-task fine-tuning. In addition, leveraging the
advantages of LLM's instruction-following ability, we propose a novel
instruction-based contrastive tuning strategy for LLM to continuously correct
current cognitive biases with the guidance of previous data in an
instruction-tuning manner, which mitigates the gap between old and new
relations in a more suitable way for LLMs. We experimentally evaluate our model
on TACRED and FewRel, and the results show that our model achieves new
state-of-the-art CRE performance with significant improvements, demonstrating
the importance of specializing in exploiting error cases.

</details>


### [173] [Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation](https://arxiv.org/abs/2508.12040)
*Jinyi Han,Tingyun Li,Shisong Chen,Jie Shi,Xinyi Wang,Guanglei Yue,Jiaqing Liang,Xin Lin,Liqian Wen,Zulong Chen,Yanghua Xiao*

Main category: cs.CL

TL;DR: FineCE 提出了一种全新的置信度评估方法，可以在文本生成中提供准确、细粒度的置信度评分，并超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有大语言模型因缺乏自我意识导致置信度估计不准确的问题。

Method: FineCE 包括两个核心方法：开发一个训练数据构造流水线，捕捉 LLM 响应的概率分布；设计一种监督学习模型，可以预测任意文本序列的置信度。此外，还提出了一个反向置信度整合策略（BCI），利用后续文本信息提升当前序列的置信度估计，并定义了三种寻找最佳评估位置的策略。

Result: 在多个基准数据集上的实验显示，FineCE 的效果持续优于传统置信度估计方法。

Conclusion: FineCE 提供了一个更可信、更可靠的置信度估计方式，可提升大语言模型生成结果的信赖程度，且相关代码已在 GitHub 上开源。

Abstract: While large language models (LLMs) have demonstrated remarkable performance
across diverse tasks, they fundamentally lack self-awareness and frequently
exhibit overconfidence, assigning high confidence scores to incorrect
predictions. Accurate confidence estimation is therefore critical for enhancing
the trustworthiness and reliability of LLM-generated outputs. However, existing
approaches suffer from coarse-grained scoring mechanisms that fail to provide
fine-grained, continuous confidence estimates throughout the generation
process. To address these limitations, we introduce FineCE, a novel confidence
estimation method that delivers accurate, fine-grained confidence scores during
text generation. Specifically, we first develop a comprehensive pipeline for
constructing training data that effectively captures the underlying
probabilistic distribution of LLM responses, and then train a model to predict
confidence scores for arbitrary text sequences in a supervised manner.
Furthermore, we propose a Backward Confidence Integration (BCI) strategy that
leverages information from the subsequent text to enhance confidence estimation
for the current sequence during inference. We also introduce three strategies
for identifying optimal positions to perform confidence estimation within the
generation process. Extensive experiments on multiple benchmark datasets
demonstrate that FineCE consistently outperforms existing classical confidence
estimation methods. Our code and all baselines used in the paper are available
on GitHub.

</details>


### [174] [J6: Jacobian-Driven Role Attribution for Multi-Objective Prompt Optimization in LLMs](https://arxiv.org/abs/2508.12086)
*Yao Wu*

Main category: cs.CL

TL;DR: 研究提出了一种名为J6的新方法，用于在大语言模型（LLM）适配中平衡多个优化目标。


<details>
  <summary>Details</summary>
Motivation: 在LLM适配中，需要在改善事实性和提升信心度之间取得平衡，但现有的方法无法深入挖掘这些目标与参数之间的复杂关系。

Method: 提出J6方法，通过对梯度交互矩阵进行六部分的结构性分解，以实现动态更新，并采用软硬结合的策略进行优化。

Result: J6方法能够更有效地处理目标间冲突和协同问题，并提供了对参数归因、任务干扰及几何结构对齐的深入洞察。

Conclusion: J6方法为提示优化提供了一种有原则且可扩展的新机制，同时为多目标优化中的结构化雅可比推理开辟了新的方向。

Abstract: In large language model (LLM) adaptation, balancing multiple optimization
objectives such as improving factuality (heat) and increasing confidence (via
low entropy) poses a fundamental challenge, especially when prompt parameters
(e.g., hidden-layer insertions h and embedding modifications w) interact in
non-trivial ways. Existing multi-objective optimization strategies often rely
on scalar gradient aggregation, ignoring the deeper geometric structure between
objectives and parameters. We propose J6, a structured Jacobian-based method
that decomposes the gradient interaction matrix into six interpretable
components. This decomposition enables both hard decision-making (e.g.,
choosing the dominant update direction via argmax) and soft strategies (e.g.,
attention-style weighting via softmax over J6), forming a dynamic update
framework that adapts to local conflict and synergy. Moreover, the
interpretable structure of J6 provides insight into parameter attribution, task
interference, and geometry-aligned adaptation. Our work introduces a principled
and extensible mechanism for conflict-aware prompt optimization, and opens a
new avenue for incorporating structured Jacobian reasoning into multi-objective
neural tuning.

</details>


### [175] [STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples](https://arxiv.org/abs/2508.12096)
*Haiquan Hu,Jiazhi Jiang,Shiyou Xu,Ruhan Zeng,Tian Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的轻量化和可解释性评估框架STEM，用于高效估算大型语言模型（LLM）的相对能力。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型的能力迅速提升，传统基准测试变得昂贵和难以有效区分模型间的实际能力差异。本文旨在解决这一问题。

Method: 提出了一种结构化过渡评估方法（STEM），通过分析同一架构但参数规模不同的模型之间的一致性能过渡，识别出显著过渡样本（STS），以估算未知模型的能力位置。

Result: 实验结果表明，STEM可以可靠地捕捉性能趋势，并与模型能力的真实排名保持一致。

Conclusion: STEM是一种实用且可扩展的方法，可以对LLM进行细粒度和架构无关的评估，具有广泛的潜在应用。

Abstract: Evaluating large language models (LLMs) has become increasingly challenging
as model capabilities advance rapidly. While recent models often achieve higher
scores on standard benchmarks, these improvements do not consistently reflect
enhanced real-world reasoning capabilities. Moreover, widespread overfitting to
public benchmarks and the high computational cost of full evaluations have made
it both expensive and less effective to distinguish meaningful differences
between models. To address these challenges, we propose the \textbf{S}tructured
\textbf{T}ransition \textbf{E}valuation \textbf{M}ethod (STEM), a lightweight
and interpretable evaluation framework for efficiently estimating the relative
capabilities of LLMs. STEM identifies \textit{significant transition samples}
(STS) by analyzing consistent performance transitions among LLMs of the same
architecture but varying parameter scales. These samples enable STEM to
effectively estimate the capability position of an unknown model. Qwen3 model
family is applied to construct the STS pool on six diverse and representative
benchmarks. To assess generalizability. Experimental results indicate that STEM
reliably captures performance trends, aligns with ground-truth rankings of
model capability. These findings highlight STEM as a practical and scalable
method for fine-grained, architecture-agnostic evaluation of LLMs.

</details>


### [176] [Exploring Efficiency Frontiers of Thinking Budget in Medical Reasoning: Scaling Laws between Computational Resources and Reasoning Quality](https://arxiv.org/abs/2508.12140)
*Ziqian Bi,Lu Chen,Junhao Song,Hongying Luo,Enze Ge,Junmin Huang,Tianyang Wang,Keyu Chen,Chia Xin Liang,Zihan Wei,Huafeng Liu,Chunjie Tian,Jibin Guan,Joe Yeong,Yongzhi Xu,Peng Wang,Junfeng Hao*

Main category: cs.CL

TL;DR: 本文对医疗推理任务中的思维预算机制进行了首次系统评估，揭示了计算资源与推理质量之间的关系，发现了模型准确率与思维预算及模型大小间的对数缩放规律。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在研究如何通过调整思维预算优化医疗人工智能系统，从而实现资源的动态分配，同时确保临床应用的透明度。

Method: 系统评估了Qwen3和DeepSeek-R1两大模型家族在15个医疗数据集上的表现，并通过对比不同思维预算范围的控制实验（从0到无限Token）分析性能表现。

Result: 发现了准确率与思维预算和模型大小之间的对数缩放关系，明确了高效（0-256Token）、均衡（256-512Token）和高精度（512Token以上）三种效率模式；小模型从延展的思维预算中获益最大，特定领域如神经学和胃肠病学需要较深的推理过程。

Conclusion: 思维预算控制是优化医疗AI系统的关键机制，能够实现与临床需求匹配的动态资源分配，并促进透明的医疗领域部署。

Abstract: This study presents the first comprehensive evaluation of thinking budget
mechanisms in medical reasoning tasks, revealing fundamental scaling laws
between computational resources and reasoning quality. We systematically
evaluated two major model families, Qwen3 (1.7B to 235B parameters) and
DeepSeek-R1 (1.5B to 70B parameters), across 15 medical datasets spanning
diverse specialties and difficulty levels. Through controlled experiments with
thinking budgets ranging from zero to unlimited tokens, we establish
logarithmic scaling relationships where accuracy improvements follow a
predictable pattern with both thinking budget and model size. Our findings
identify three distinct efficiency regimes: high-efficiency (0 to 256 tokens)
suitable for real-time applications, balanced (256 to 512 tokens) offering
optimal cost-performance tradeoffs for routine clinical support, and
high-accuracy (above 512 tokens) justified only for critical diagnostic tasks.
Notably, smaller models demonstrate disproportionately larger benefits from
extended thinking, with 15 to 20% improvements compared to 5 to 10% for larger
models, suggesting a complementary relationship where thinking budget provides
greater relative benefits for capacity-constrained models. Domain-specific
patterns emerge clearly, with neurology and gastroenterology requiring
significantly deeper reasoning processes than cardiovascular or respiratory
medicine. The consistency between Qwen3 native thinking budget API and our
proposed truncation method for DeepSeek-R1 validates the generalizability of
thinking budget concepts across architectures. These results establish thinking
budget control as a critical mechanism for optimizing medical AI systems,
enabling dynamic resource allocation aligned with clinical needs while
maintaining the transparency essential for healthcare deployment.

</details>


### [177] [LLM-as-a-Judge for Privacy Evaluation? Exploring the Alignment of Human and LLM Perceptions of Privacy in Textual Data](https://arxiv.org/abs/2508.12158)
*Stephen Meisenbacher,Alexandra Klymenko,Florian Matthes*

Main category: cs.CL

TL;DR: 本文探索利用大型语言模型（LLM）作为文本隐私敏感性评估工具，其研究包含10个数据集、13种LLM及677名参与者。


<details>
  <summary>Details</summary>
Motivation: 隐私保护在NLP中是一个挑战，现有的隐私评估方法不够准确。而LLM以其在许多任务中表现出的高一致性可能为此提供解决方案。

Method: 利用LLM-as-a-Judge方法，比较以LLM为基础的隐私评估与人类隐私感知的吻合度，同时分析其推理模式。

Result: 尽管人类对隐私的判断一致性较低，研究发现LLM可以有效模拟人类的总体隐私观，提供某些情况下准确的隐私评估。

Conclusion: 研究表明LLM可作为隐私评估工具，但仍有局限性，未来可发展其在隐私保护中的潜力。

Abstract: Despite advances in the field of privacy-preserving Natural Language
Processing (NLP), a significant challenge remains the accurate evaluation of
privacy. As a potential solution, using LLMs as a privacy evaluator presents a
promising approach $\unicode{x2013}$ a strategy inspired by its success in
other subfields of NLP. In particular, the so-called $\textit{LLM-as-a-Judge}$
paradigm has achieved impressive results on a variety of natural language
evaluation tasks, demonstrating high agreement rates with human annotators.
Recognizing that privacy is both subjective and difficult to define, we
investigate whether LLM-as-a-Judge can also be leveraged to evaluate the
privacy sensitivity of textual data. Furthermore, we measure how closely LLM
evaluations align with human perceptions of privacy in text. Resulting from a
study involving 10 datasets, 13 LLMs, and 677 human survey participants, we
confirm that privacy is indeed a difficult concept to measure empirically,
exhibited by generally low inter-human agreement rates. Nevertheless, we find
that LLMs can accurately model a global human privacy perspective, and through
an analysis of human and LLM reasoning patterns, we discuss the merits and
limitations of LLM-as-a-Judge for privacy evaluation in textual data. Our
findings pave the way for exploring the feasibility of LLMs as privacy
evaluators, addressing a core challenge in solving pressing privacy issues with
innovative technical solutions.

</details>


### [178] [Arabic Multimodal Machine Learning: Datasets, Applications, Approaches, and Challenges](https://arxiv.org/abs/2508.12227)
*Abdelhamid Haouhat,Slimane Bellaouar,Attia Nehar,Hadda Cherroun,Ahmed Abdelali*

Main category: cs.CL

TL;DR: 该论文综述了阿拉伯语多模态机器学习（MML），提出了一个新的分类法，并分析了现有研究。


<details>
  <summary>Details</summary>
Motivation: 由于阿拉伯语MML领域基础研究逐渐成熟，亟需一篇综述性论文总结现状并指出未来研究方向。

Method: 通过提出新的分类框架，对阿拉伯语MML研究进行归纳和分析，将其分为数据集、应用、方法和挑战四大类别。

Result: 该综述指出了阿拉伯语MML领域的未解问题和研究空白。

Conclusion: 研究者可利用本文提供的框架和发现，弥补研究空白，推动该领域的发展。

Abstract: Multimodal Machine Learning (MML) aims to integrate and analyze information
from diverse modalities, such as text, audio, and visuals, enabling machines to
address complex tasks like sentiment analysis, emotion recognition, and
multimedia retrieval. Recently, Arabic MML has reached a certain level of
maturity in its foundational development, making it time to conduct a
comprehensive survey. This paper explores Arabic MML by categorizing efforts
through a novel taxonomy and analyzing existing research. Our taxonomy
organizes these efforts into four key topics: datasets, applications,
approaches, and challenges. By providing a structured overview, this survey
offers insights into the current state of Arabic MML, highlighting areas that
have not been investigated and critical research gaps. Researchers will be
empowered to build upon the identified opportunities and address challenges to
advance the field.

</details>


### [179] [SEA-BED: Southeast Asia Embedding Benchmark](https://arxiv.org/abs/2508.12243)
*Wuttikorn Ponwitayarat,Raymond Ng,Jann Railey Montalan,Thura Aung,Jian Gang Ngui,Yosephine Susanto,William Tjhi,Panuthep Tasawong,Erik Cambria,Ekapol Chuangsuwanich,Sarana Nutanong,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: SEA地域缺乏特定的句嵌入基准，为此引入SEA-BED基准，评估了不同语言的挑战及人工与机器翻译的影响。


<details>
  <summary>Details</summary>
Motivation: 弥补SEA地区缺乏针对性句嵌入基准的空白，满足多语言处理需求。

Method: 引入SEA-BED，包含169个数据集，覆盖9种任务，10种语言，结合人工与机器翻译数据，评估17种嵌入模型的表现和差异。

Result: 发现SEA语言的表现排名变化巨大，模型效果不均衡，人工数据对低资源语言（如缅甸语）的评估尤为重要。

Conclusion: SEA-BED为SEA语言的句嵌入研究奠定了基准，突出了人工数据的关键作用，为低资源语言的改进提供参考。

Abstract: Sentence embeddings are essential for NLP tasks such as semantic search,
re-ranking, and textual similarity. Although multilingual benchmarks like MMTEB
broaden coverage, Southeast Asia (SEA) datasets are scarce and often
machine-translated, missing native linguistic properties. With nearly 700
million speakers, the SEA region lacks a region-specific embedding benchmark.
We introduce SEA-BED, the first large-scale SEA embedding benchmark with 169
datasets across 9 tasks and 10 languages, where 71% are formulated by humans,
not machine generation or translation. We address three research questions: (1)
which SEA languages and tasks are challenging, (2) whether SEA languages show
unique performance gaps globally, and (3) how human vs. machine translations
affect evaluation. We evaluate 17 embedding models across six studies,
analyzing task and language challenges, cross-benchmark comparisons, and
translation trade-offs. Results show sharp ranking shifts, inconsistent model
performance among SEA languages, and the importance of human-curated datasets
for low-resource languages like Burmese.

</details>


### [180] [What do Speech Foundation Models Learn? Analysis and Applications](https://arxiv.org/abs/2508.12255)
*Ankita Pasad*

Main category: cs.CL

TL;DR: 本文研究了语音基础模型（SFMs）的知识解析及其在语音及语言理解任务中的作用，提出了分析框架和新的任务评价标准。


<details>
  <summary>Details</summary>
Motivation: 近年来，随着语音任务的快速发展，SFMs表现出了在多个任务中的卓越效果，但对其内部知识获取和使用的理解尚不充分。

Method: 介绍了一种轻量级的分析框架，使用统计工具和无需训练的任务进行知识调查，针对SFMs进行了对比研究；同时提出新的SLU任务（如NER和NEL），并基于SFMs开发E2E模型和适应策略进行评估。

Result: 提出的E2E模型在SLU任务上优于传统的级联方法；分析工具显示出对下游任务性能的影响有实践意义。

Conclusion: 论文通过提供分析框架、数据集和任务，全面回答了对SFMs的知识挖掘问题，推动了未来模型设计和应用的优化。

Abstract: Speech foundation models (SFMs) are designed to serve as general-purpose
representations for a wide range of speech-processing tasks. The last five
years have seen an influx of increasingly successful self-supervised and
supervised pre-trained models with impressive performance on various downstream
tasks.
  Although the zoo of SFMs continues to grow, our understanding of the
knowledge they acquire lags behind. This thesis presents a lightweight analysis
framework using statistical tools and training-free tasks to investigate the
acoustic and linguistic knowledge encoded in SFM layers. We conduct a
comparative study across multiple SFMs and statistical tools. Our study also
shows that the analytical insights have concrete implications for downstream
task performance.
  The effectiveness of an SFM is ultimately determined by its performance on
speech applications. Yet it remains unclear whether the benefits extend to
spoken language understanding (SLU) tasks that require a deeper understanding
than widely studied ones, such as speech recognition. The limited exploration
of SLU is primarily due to a lack of relevant datasets. To alleviate that, this
thesis contributes tasks, specifically spoken named entity recognition (NER)
and named entity localization (NEL), to the Spoken Language Understanding
Evaluation benchmark. We develop SFM-based approaches for NER and NEL, and find
that end-to-end (E2E) models leveraging SFMs can surpass traditional cascaded
(speech recognition followed by a text model) approaches. Further, we evaluate
E2E SLU models across SFMs and adaptation strategies to assess the impact on
task performance.
  Collectively, this thesis tackles previously unanswered questions about SFMs,
providing tools and datasets to further our understanding and to enable the
community to make informed design choices for future model development and
adoption.

</details>


### [181] [Structuring the Unstructured: A Systematic Review of Text-to-Structure Generation for Agentic AI with a Universal Evaluation Framework](https://arxiv.org/abs/2508.12257)
*Zheye Deng,Chunkit Chan,Tianshi Zheng,Wei Fan,Weiqi Wang,Yangqiu Song*

Main category: cs.CL

TL;DR: 本论文综述了将无结构文本转换为结构化格式（如表格、知识图谱和图表）的研究，并提出了评估框架。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统向代理化操作和上下文感知检索方向发展，将无结构文本转换为结构化数据已成为关键需求。

Method: 进行系统性文献回顾，分析现有技术、数据集和评估标准，并提出通用评估框架。

Result: 审视当前的技术方法和数据集，总结挑战，提出未来研究方向，并引入通用评估框架。

Conclusion: 文本转结构化数据是下一代AI系统的重要基础性技术，通过统一框架增强效率和评估。

Abstract: The evolution of AI systems toward agentic operation and context-aware
retrieval necessitates transforming unstructured text into structured formats
like tables, knowledge graphs, and charts. While such conversions enable
critical applications from summarization to data mining, current research lacks
a comprehensive synthesis of methodologies, datasets, and metrics. This
systematic review examines text-to-structure techniques and the encountered
challenges, evaluates current datasets and assessment criteria, and outlines
potential directions for future research. We also introduce a universal
evaluation framework for structured outputs, establishing text-to-structure as
foundational infrastructure for next-generation AI systems.

</details>


### [182] [Fast, Slow, and Tool-augmented Thinking for LLMs: A Review](https://arxiv.org/abs/2508.12265)
*Xinda Jia,Jinpeng Li,Zezhong Wang,Jingjing Li,Xingshan Zeng,Yasheng Wang,Weinan Zhang,Yong Yu,Weiwen Liu*

Main category: cs.CL

TL;DR: 该论文提出一种分类方法，用于分析大语言模型（LLMs）在推理过程中的不同策略，以及如何适应现实任务的需求。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在多领域推理表现出色，但面对真实任务时，需要根据问题需求调整推理策略，例如直觉式快速反应或者逐步的推理及外部工具辅助。

Method: 作者基于认知心理学，提出了一种新型推理策略分类法，利用快慢边界和内外边界区分直觉-深思型和内部-外部工具辅助的推理过程，同时对涉及适应性推理的现有研究进行系统梳理和分类。

Result: 文章对当前LLMs适应性推理的研究进行了全面综述，并按照关键决策因素对这些研究方法进行分类。

Conclusion: 提出LLMs需要进一步发展成为更加适应性强、高效和可靠的模型，并指出未来研究的挑战和方向，包括优化推理策略和提高模型灵活性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
reasoning across diverse domains. However, effective reasoning in real-world
tasks requires adapting the reasoning strategy to the demands of the problem,
ranging from fast, intuitive responses to deliberate, step-by-step reasoning
and tool-augmented thinking. Drawing inspiration from cognitive psychology, we
propose a novel taxonomy of LLM reasoning strategies along two knowledge
boundaries: a fast/slow boundary separating intuitive from deliberative
processes, and an internal/external boundary distinguishing reasoning grounded
in the model's parameters from reasoning augmented by external tools. We
systematically survey recent work on adaptive reasoning in LLMs and categorize
methods based on key decision factors. We conclude by highlighting open
challenges and future directions toward more adaptive, efficient, and reliable
LLMs.

</details>


### [183] [The Self-Execution Benchmark: Measuring LLMs' Attempts to Overcome Their Lack of Self-Execution](https://arxiv.org/abs/2508.12277)
*Elon Ezra,Ariel Weizman,Amos Azaria*

Main category: cs.CL

TL;DR: 本文提出了自执行基准，用于评估LLMs预测自己回应特性的能力。实验显示模型在此基准表现不佳，且规模或能力提升未显著改善性能。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否能预测自身回应特性的能力，这是区别于传统知识或推理类能力的评价方式。

Method: 引入自执行基准，通过分析模型在自我回应预测（如回答难易、拒绝回答倾向、关联生成等）上的表现，明确其局限性。

Result: 实验结果表明，模型在该基准上的表现普遍较差，增大模型规模和能力未能稳定提升性能。

Conclusion: LLMs在表示和推理自身行为时存在根本性限制。

Abstract: Large language models (LLMs) are commonly evaluated on tasks that test their
knowledge or reasoning abilities. In this paper, we explore a different type of
evaluation: whether an LLM can predict aspects of its own responses. Since LLMs
lack the ability to execute themselves, we introduce the Self-Execution
Benchmark, which measures a model's ability to anticipate properties of its
output, such as whether a question will be difficult for it, whether it will
refuse to answer, or what kinds of associations it is likely to produce. Our
experiments show that models generally perform poorly on this benchmark, and
that increased model size or capability does not consistently lead to better
performance. These results suggest a fundamental limitation in how LLMs
represent and reason about their own behavior.

</details>


### [184] [Legal$Δ$: Enhancing Legal Reasoning in LLMs via Reinforcement Learning with Chain-of-Thought Guided Information Gain](https://arxiv.org/abs/2508.12281)
*Xin Dai,Buqiang Xu,Zhenghao Liu,Yukun Yan,Huiyuan Xie,Xiaoyuan Yi,Shuo Wang,Ge Yu*

Main category: cs.CL

TL;DR: 研究提出了LegalΔ框架，通过提升链式推理能力，改善现有法律大语言模型在复杂法律场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有法律大语言模型在复杂法律场景中缺乏能生成可靠且具解释性的推理过程，而仅提供直接回答，这限制了模型的适用性。

Method: 提出了LegalΔ，一个强化学习框架，通过信息增益的链式推理指导模型。其采取双输入模式和两阶段方法：先从强大的推理模型中提取推理能力，再通过多维奖励机制优化推理质量。

Result: 实验显示，LegalΔ在多个法律推理任务中相较基线模型表现更优，提升了准确性和解释性。

Conclusion: LegalΔ框架成功改进了法律领域模型的推理能力，可产生更可靠和可信的法律判决，无需依赖标注的偏好数据。

Abstract: Legal Artificial Intelligence (LegalAI) has achieved notable advances in
automating judicial decision-making with the support of Large Language Models
(LLMs). However, existing legal LLMs still struggle to generate reliable and
interpretable reasoning processes. They often default to fast-thinking behavior
by producing direct answers without explicit multi-step reasoning, limiting
their effectiveness in complex legal scenarios that demand rigorous
justification. To address this challenge, we propose Legal$\Delta$, a
reinforcement learning framework designed to enhance legal reasoning through
chain-of-thought guided information gain. During training, Legal$\Delta$
employs a dual-mode input setup-comprising direct answer and
reasoning-augmented modes-and maximizes the information gain between them. This
encourages the model to acquire meaningful reasoning patterns rather than
generating superficial or redundant explanations. Legal$\Delta$ follows a
two-stage approach: (1) distilling latent reasoning capabilities from a
powerful Large Reasoning Model (LRM), DeepSeek-R1, and (2) refining reasoning
quality via differential comparisons, combined with a multidimensional reward
mechanism that assesses both structural coherence and legal-domain specificity.
Experimental results on multiple legal reasoning tasks demonstrate that
Legal$\Delta$ outperforms strong baselines in both accuracy and
interpretability. It consistently produces more robust and trustworthy legal
judgments without relying on labeled preference data. All code and data will be
released at https://github.com/NEUIR/LegalDelta.

</details>


### [185] [A Question Answering Dataset for Temporal-Sensitive Retrieval-Augmented Generation](https://arxiv.org/abs/2508.12282)
*Ziyang Chen,Erxue Min,Xiang Zhao,Yunxin Li,Xin Jia,Jinzhi Liao,Jichao Li,Shuaiqiang Wang,Baotian Hu,Dawei Yin*

Main category: cs.CL

TL;DR: ChronoQA是一个旨在评估RAG系统时间推理能力的中文问答数据集，包含300,000多篇新闻文章与5,176条高质量问题，支持单文档与多文档场景，并提供全面的结构化注释。


<details>
  <summary>Details</summary>
Motivation: 解决现有问答系统在处理时间推理相关任务上的不足，提供一个标准化的评估基准，推动时间敏感型问答系统的发展。

Method: 通过从2019-2024年的新闻文章中构建数据集，涵盖多种时间类型的问题，采用规则、LLM和人工多阶段验证以确保数据质量。

Result: ChronoQA成功提供了高质量的结构化注释数据集，覆盖时间推理任务，支持RAG评估实时性与逻辑一致性的能力提升。

Conclusion: ChronoQA作为一个动态且可扩展的资源，可用于推动时间相关任务的研究，并为时间敏感型问答系统提供鲁棒的基准。

Abstract: We introduce ChronoQA, a large-scale benchmark dataset for Chinese question
answering, specifically designed to evaluate temporal reasoning in
Retrieval-Augmented Generation (RAG) systems. ChronoQA is constructed from over
300,000 news articles published between 2019 and 2024, and contains 5,176
high-quality questions covering absolute, aggregate, and relative temporal
types with both explicit and implicit time expressions. The dataset supports
both single- and multi-document scenarios, reflecting the real-world
requirements for temporal alignment and logical consistency. ChronoQA features
comprehensive structural annotations and has undergone multi-stage validation,
including rule-based, LLM-based, and human evaluation, to ensure data quality.
By providing a dynamic, reliable, and scalable resource, ChronoQA enables
structured evaluation across a wide range of temporal tasks, and serves as a
robust benchmark for advancing time-sensitive retrieval-augmented question
answering systems.

</details>


### [186] [Incorporating Legal Logic into Deep Learning: An Intelligent Approach to Probation Prediction](https://arxiv.org/abs/2508.12286)
*Qinghua Wang,Xu Zhang,Lingyan Yang,Rui Shao,Bonan Wang,Fang Wang,Cunquan Qu*

Main category: cs.CL

TL;DR: 本论文提出了结合法律逻辑的深度学习模型，用于缓刑预测，并验证其在缓刑数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的智能司法辅助系统缺乏专门的缓刑预测方法，并且针对缓刑资格影响因素的研究有限，同时更多依赖于数据驱动方法而忽略法律逻辑。

Method: 构建包含事实描述和缓刑法律要素的缓刑数据集，设计基于缓刑法律逻辑和“双轨刑罚理论”的多任务双轨缓刑预测模型（MT-DT），并通过实验进行验证。

Result: MT-DT模型在缓刑数据集上的表现优于基线模型，且其法律逻辑分析进一步验证了模型的有效性。

Conclusion: 将法律逻辑融入深度学习模型，可以提升缓刑预测的准确性，推动智能司法辅助系统的发展。

Abstract: Probation is a crucial institution in modern criminal law, embodying the
principles of fairness and justice while contributing to the harmonious
development of society. Despite its importance, the current Intelligent
Judicial Assistant System (IJAS) lacks dedicated methods for probation
prediction, and research on the underlying factors influencing probation
eligibility remains limited. In addition, probation eligibility requires a
comprehensive analysis of both criminal circumstances and remorse. Much of the
existing research in IJAS relies primarily on data-driven methodologies, which
often overlooks the legal logic underpinning judicial decision-making. To
address this gap, we propose a novel approach that integrates legal logic into
deep learning models for probation prediction, implemented in three distinct
stages. First, we construct a specialized probation dataset that includes fact
descriptions and probation legal elements (PLEs). Second, we design a distinct
probation prediction model named the Multi-Task Dual-Theory Probation
Prediction Model (MT-DT), which is grounded in the legal logic of probation and
the \textit{Dual-Track Theory of Punishment}. Finally, our experiments on the
probation dataset demonstrate that the MT-DT model outperforms baseline models,
and an analysis of the underlying legal logic further validates the
effectiveness of the proposed approach.

</details>


### [187] [CarelessWhisper: Turning Whisper into a Causal Streaming Model](https://arxiv.org/abs/2508.12301)
*Tomer Krichli,Bhiksha Raj,Joseph Keshet*

Main category: cs.CL

TL;DR: 提出了一种方法将Transformer编码器-解码器模型转变为低延迟流式语音识别模型，实验结果显示效果优于现有方法，同时复杂度更低。


<details>
  <summary>Details</summary>
Motivation: 现有的顶尖离线语音识别模型如OpenAI Whisper和NVIDIA Canary无法支持实时流式转录，架构和训练方法限制了其能力。

Method: 通过将非因果编码器改为因果编码器，并利用低秩自适应（LoRA）和弱对齐数据集对编码器和解码器进行微调，同时设计了一种更新的推断机制实现流式解码。

Result: 在低延迟（小于300毫秒）实验中，微调后的模型在大多数情况下优于现有的非微调流式方法，且复杂度更低。此外，其训练过程优化了对齐能力，可以简单地提取词级时间戳。

Conclusion: 提出的方法有效解决了流式语音识别面临的低延迟和高性能需求，提供了开源代码和模型以促进进一步研究。

Abstract: Automatic Speech Recognition (ASR) has seen remarkable progress, with models
like OpenAI Whisper and NVIDIA Canary achieving state-of-the-art (SOTA)
performance in offline transcription. However, these models are not designed
for streaming (online or real-time) transcription, due to limitations in their
architecture and training methodology. We propose a method to turn the
transformer encoder-decoder model into a low-latency streaming model that is
careless about future context. We present an analysis explaining why it is not
straightforward to convert an encoder-decoder transformer to a low-latency
streaming model. Our proposed method modifies the existing (non-causal) encoder
to a causal encoder by fine-tuning both the encoder and decoder using Low-Rank
Adaptation (LoRA) and a weakly aligned dataset. We then propose an updated
inference mechanism that utilizes the fine-tune causal encoder and decoder to
yield greedy and beam-search decoding, and is shown to be locally optimal.
Experiments on low-latency chunk sizes (less than 300 msec) show that our
fine-tuned model outperforms existing non-fine-tuned streaming approaches in
most cases, while using a lower complexity. Additionally, we observe that our
training process yields better alignment, enabling a simple method for
extracting word-level timestamps. We release our training and inference code,
along with the fine-tuned models, to support further research and development
in streaming ASR.

</details>


### [188] [Consensus or Conflict? Fine-Grained Evaluation of Conflicting Answers in Question-Answering](https://arxiv.org/abs/2508.12355)
*Eviatar Nachshoni,Arie Cattan,Shmuel Amar,Ori Shapira,Ido Dagan*

Main category: cs.CL

TL;DR: 大语言模型（LLMs）在多答案问答（MAQA）任务中表现有限，并难以处理冲突答案。本研究提出了一种新方法构建NATCONFQA数据集，并评估LLMs在这一问题上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的多答案问答（MAQA）问题具有挑战性，尤其是在存在冲突答案的情况下，现有数据集构建成本高或依赖人工合成数据，缺乏真实的冲突情景描述。作者希望通过改进数据集和评估模型能力推动这一领域的研究。

Method: 作者扩展了MAQA任务，将其定义为既需要识别所有正确答案，也要检测具体冲突答案对。他们提出了一种利用事实核查数据集的新方法来构建NATCONFQA数据集，该数据集包含详细冲突标签。

Result: 研究评估了八种高端LLM在新数据集上的表现，发现这些模型在处理冲突类型时表现不佳，并采用了一些有缺陷的解决策略。

Conclusion: 研究展示了现有LLMs难以应对多答案和冲突情况的复杂性，同时证明了NATCONFQA作为评估工具的价值。

Abstract: Large Language Models (LLMs) have demonstrated strong performance in question
answering (QA) tasks. However, Multi-Answer Question Answering (MAQA), where a
question may have several valid answers, remains challenging. Traditional QA
settings often assume consistency across evidences, but MAQA can involve
conflicting answers. Constructing datasets that reflect such conflicts is
costly and labor-intensive, while existing benchmarks often rely on synthetic
data, restrict the task to yes/no questions, or apply unverified automated
annotation. To advance research in this area, we extend the conflict-aware MAQA
setting to require models not only to identify all valid answers, but also to
detect specific conflicting answer pairs, if any. To support this task, we
introduce a novel cost-effective methodology for leveraging fact-checking
datasets to construct NATCONFQA, a new benchmark for realistic, conflict-aware
MAQA, enriched with detailed conflict labels, for all answer pairs. We evaluate
eight high-end LLMs on NATCONFQA, revealing their fragility in handling various
types of conflicts and the flawed strategies they employ to resolve them.

</details>


### [189] [ReaLM: Reflection-Enhanced Autonomous Reasoning with Small Language Models](https://arxiv.org/abs/2508.12387)
*Yuanfeng Xu,Zehui Dai,Jian Liang,Jiapeng Guan,Guangrun Wang,Liang Lin,Xiaohui Lv*

Main category: cs.CL

TL;DR: 提出ReaLM框架，通过多路径验证、渐进自主训练和引导式链式推理蒸馏，提升小型语言模型在推理能力、自主性和泛化能力方面的表现。


<details>
  <summary>Details</summary>
Motivation: 探索在小型语言模型(SLM)上的推理能力提升，同时解决现有方法在推理能力、自主性和泛化性方面的不足。

Method: 提出了一种强化学习框架ReaLM，其中包括多路径过程验证(MRPV)、通过渐进归纳实现的自主性训练(EAAI)策略，以及引导式链式推理蒸馏方法，以系统性改进模型性能。

Result: 实验证明ReaLM显著提升了SLM在垂直领域和一般推理任务上的表现，尤其在推理能力、自主性和泛化性三方面均有显著改善。

Conclusion: ReaLM框架通过系统性方法解决SLM推理能力的不足，为提升小型语言模型在实际垂直领域的可靠性与效率提供了新的路径。

Abstract: Small Language Models (SLMs) are a cost-effective alternative to Large
Language Models (LLMs), but often struggle with complex reasoning due to their
limited capacity and a tendency to produce mistakes or inconsistent answers
during multi-step reasoning. Existing efforts have improved SLM performance,
but typically at the cost of one or more of three key aspects: (1) reasoning
capability, due to biased supervision that filters out negative reasoning paths
and limits learning from errors; (2) autonomy, due to over-reliance on
externally generated reasoning signals; and (3) generalization, which suffers
when models overfit to teacher-specific patterns. In this paper, we introduce
ReaLM, a reinforcement learning framework for robust and self-sufficient
reasoning in vertical domains. To enhance reasoning capability, we propose
Multi-Route Process Verification (MRPV), which contrasts both positive and
negative reasoning paths to extract decisive patterns. To reduce reliance on
external guidance and improve autonomy, we introduce Enabling Autonomy via
Asymptotic Induction (EAAI), a training strategy that gradually fades external
signals. To improve generalization, we apply guided chain-of-thought
distillation to encode domain-specific rules and expert knowledge into SLM
parameters, making them part of what the model has learned. Extensive
experiments on both vertical and general reasoning tasks demonstrate that ReaLM
significantly improves SLM performance across aspects (1)-(3) above.

</details>


### [190] [MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph](https://arxiv.org/abs/2508.12393)
*Duzhen Zhang,Zixiao Wang,Zhong-Zhi Li,Yahan Yu,Shuncheng Jia,Jiahua Dong,Haotian Xu,Xing Wu,Yingying Zhang,Tielin Zhang,Jie Yang,Xiuying Chen,Le Song*

Main category: cs.CL

TL;DR: 本文提出了一种名为MedKGent的框架，用于构建时间演变的医学知识图谱（KG），基于超过1000万篇PubMed摘要，采用时序方式增量建立KG，并通过专家和SOTA模型验证其高准确性和问答任务性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前医学知识图谱构建方法无法有效处理生物医学领域的动态知识进化以及上下文不确定性，限制了大规模知识整合的潜力。

Method: 提出了MedKGent框架，利用PubMed数据以日为单位构建时间演变的KG。通过两个代理：Extractor Agent负责抽取知识三元组并分配置信度，Constructor Agent执行增量知识整合，解决知识冲突的同时强化重复性知识。

Result: 构建了包含156,275个实体和2,971,384个关系三元组的动态知识图谱。经过两位专家和SOTA模型评估，其准确率接近90%。在7个医学问题回答基准中，通过使用融合KG的LLM，显著提升问答性能。

Conclusion: MedKGent展示了在动态知识演变背景下构建高质量医学知识图谱的可能性；此外，其在药物再利用研究和医学问答任务中表现出显著实用性。

Abstract: The rapid expansion of medical literature presents growing challenges for
structuring and integrating domain knowledge at scale. Knowledge Graphs (KGs)
offer a promising solution by enabling efficient retrieval, automated
reasoning, and knowledge discovery. However, current KG construction methods
often rely on supervised pipelines with limited generalizability or naively
aggregate outputs from Large Language Models (LLMs), treating biomedical
corpora as static and ignoring the temporal dynamics and contextual uncertainty
of evolving knowledge. To address these limitations, we introduce MedKGent, a
LLM agent framework for constructing temporally evolving medical KGs.
Leveraging over 10 million PubMed abstracts published between 1975 and 2023, we
simulate the emergence of biomedical knowledge via a fine-grained daily time
series. MedKGent incrementally builds the KG in a day-by-day manner using two
specialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor
Agent identifies knowledge triples and assigns confidence scores via
sampling-based estimation, which are used to filter low-confidence extractions
and inform downstream processing. The Constructor Agent incrementally
integrates the retained triples into a temporally evolving graph, guided by
confidence scores and timestamps to reinforce recurring knowledge and resolve
conflicts. The resulting KG contains 156,275 entities and 2,971,384 relational
triples. Quality assessments by two SOTA LLMs and three domain experts
demonstrate an accuracy approaching 90\%, with strong inter-rater agreement. To
evaluate downstream utility, we conduct RAG across seven medical question
answering benchmarks using five leading LLMs, consistently observing
significant improvements over non-augmented baselines. Case studies further
demonstrate the KG's value in literature-based drug repurposing via
confidence-aware causal inference.

</details>


### [191] [Extracting Post-Acute Sequelae of SARS-CoV-2 Infection Symptoms from Clinical Notes via Hybrid Natural Language Processing](https://arxiv.org/abs/2508.12405)
*Zilong Bai,Zihan Xu,Cong Sun,Chengxi Zang,H. Timothy Bunnell,Catherine Sinfield,Jacqueline Rutter,Aaron Thomas Martinez,L. Charles Bailey,Mark Weiner,Thomas R. Campion,Thomas Carton,Christopher B. Forrest,Rainu Kaushal,Fei Wang,Yifan Peng*

Main category: cs.CL

TL;DR: 本文开发了一种混合的自然语言处理（NLP）管道，用于从临床记录中提取和检测与COVID-19长期后遗症（PASC）相关的症状，为高效诊断提供工具。


<details>
  <summary>Details</summary>
Motivation: 准确诊断PASC具有挑战性，因其症状多样且在不同时间间隔演变。本文希望通过NLP技术克服这一困难，提升诊断效率与准确性。

Method: 采用结合规则的命名实体识别和基于BERT的断言检测模块的混合NLP管道，开发PASC词库并对模型进行内部与外部验证。同时在11个健康系统中构建进度笔记数据集用于模型研究和群体流行病学分析。

Result: 在一地点内部验证中，断言检测F1得分为0.82；在10地点外部验证中，F1得分为0.76。每条笔记处理时间平均2.448秒。相关性检验显示模型在阳性和阴性提及中的ρ值分别超过0.83和0.72，均具有高显著性（P<0.0001）。

Conclusion: 模型在效率与效果方面达到了预期目标，显示出其改进PASC诊断的潜力。

Abstract: Accurately and efficiently diagnosing Post-Acute Sequelae of COVID-19 (PASC)
remains challenging due to its myriad symptoms that evolve over long- and
variable-time intervals. To address this issue, we developed a hybrid natural
language processing pipeline that integrates rule-based named entity
recognition with BERT-based assertion detection modules for PASC-symptom
extraction and assertion detection from clinical notes. We developed a
comprehensive PASC lexicon with clinical specialists. From 11 health systems of
the RECOVER initiative network across the U.S., we curated 160 intake progress
notes for model development and evaluation, and collected 47,654 progress notes
for a population-level prevalence study. We achieved an average F1 score of
0.82 in one-site internal validation and 0.76 in 10-site external validation
for assertion detection. Our pipeline processed each note at $2.448\pm 0.812$
seconds on average. Spearman correlation tests showed $\rho >0.83$ for positive
mentions and $\rho >0.72$ for negative ones, both with $P <0.0001$. These
demonstrate the effectiveness and efficiency of our models and their potential
for improving PASC diagnosis.

</details>


### [192] [ZigzagAttention: Efficient Long-Context Inference with Exclusive Retrieval and Streaming Heads](https://arxiv.org/abs/2508.12407)
*Zhuorui Liu,Chen Zhang,Dawei Song*

Main category: cs.CL

TL;DR: 研究在大型语言模型(LLMs)处理中，通过优化KV缓存内存使用来提高处理长上下文的效率，并提出了ZigzagAttention方法来减少延迟和保持性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的快速发展，处理长上下文内容成为关键能力，但这导致KV缓存消耗增加的问题，研究目的是优化KV缓存占用以提升模型效率。

Method: 区分注意力机制中的retrieval和streaming两类heads，通过调整它们在不同层级的配置，并设计新的标准，使两者聚集在单一层中，从而减少延迟。

Result: ZigzagAttention方法显著降低了延迟，同时维持了与其他基线方法可比的性能表现。

Conclusion: 该方法通过改进检索和流媒体头的配置过程，有效减少了计算延迟，展示出在长上下文处理中良好的应用潜力。

Abstract: With the rapid development of large language models (LLMs), handling long
context has become one of the vital abilities in LLMs. Such long-context
ability is accompanied by difficulties in deployment, especially due to the
increased consumption of KV cache. There is certain work aiming to optimize the
memory footprint of KV cache, inspired by the observation that attention heads
can be categorized into retrieval heads that are of great significance and
streaming heads that are of less significance. Typically, identifying the
streaming heads and and waiving the KV cache in the streaming heads would
largely reduce the overhead without hurting the performance that much. However,
since employing both retrieval and streaming heads in one layer decomposes one
large round of attention computation into two small ones, it may unexpectedly
bring extra latency on accessing and indexing tensors. Based on this intuition,
we impose an important improvement to the identification process of retrieval
and streaming heads, in which we design a criterion that enforces exclusively
retrieval or streaming heads gathered in one unique layer. In this way, we
further eliminate the extra latency and only incur negligible performance
degradation. Our method named \textsc{ZigzagAttention} is competitive among
considered baselines owing to reduced latency and comparable performance.

</details>


### [193] [The Cultural Gene of Large Language Models: A Study on the Impact of Cross-Corpus Training on Model Values and Biases](https://arxiv.org/abs/2508.12411)
*Emanuel Z. Fenech-Borg,Tilen P. Meznaric-Kos,Milica D. Lekovic-Bojovic,Arni J. Hentze-Djurhuus*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型(LLMs)中的文化和伦理假设，通过比较GPT-4和ERNIE Bot在文化基因（个体主义-集体主义和权力距离）上的表现和对齐性，发现两者存在显著的文化差异。


<details>
  <summary>Details</summary>
Motivation: 探讨全球部署的LLMs潜在的文化和伦理假设并评估其训练语料对模型文化倾向的影响，以避免算法导致的文化霸权问题。

Method: 提出“文化基因”概念并构建了一个包含200个提示的文化探针数据集(CPD)，针对个体主义-集体主义(IDV)和权力距离(PDI)维度，使用标准化零样本提示对两个模型(GPT-4与ERNIE Bot)进行比较，并计算其文化对齐指数(CAI)，开展定性分析评估推理表现。

Result: 发现GPT-4偏向个体主义和低权力距离，而ERNIE Bot呈现集体主义和高权力距离倾向。这种文化差异通过CAI与各自国家(美国与中国)高度对齐，且差异具有统计显著性(p < 0.001)。

Conclusion: LLMs作为统计反映其训练语料文化特征的镜子，提示应对其进行文化敏感的评估与部署以避免算法文化霸权。

Abstract: Large language models (LLMs) are deployed globally, yet their underlying
cultural and ethical assumptions remain underexplored. We propose the notion of
a "cultural gene" -- a systematic value orientation that LLMs inherit from
their training corpora -- and introduce a Cultural Probe Dataset (CPD) of 200
prompts targeting two classic cross-cultural dimensions:
Individualism-Collectivism (IDV) and Power Distance (PDI). Using standardized
zero-shot prompts, we compare a Western-centric model (GPT-4) and an
Eastern-centric model (ERNIE Bot). Human annotation shows significant and
consistent divergence across both dimensions. GPT-4 exhibits individualistic
and low-power-distance tendencies (IDV score approx 1.21; PDI score approx
-1.05), while ERNIE Bot shows collectivistic and higher-power-distance
tendencies (IDV approx -0.89; PDI approx 0.76); differences are statistically
significant (p < 0.001). We further compute a Cultural Alignment Index (CAI)
against Hofstede's national scores and find GPT-4 aligns more closely with the
USA (e.g., IDV CAI approx 0.91; PDI CAI approx 0.88) whereas ERNIE Bot aligns
more closely with China (IDV CAI approx 0.85; PDI CAI approx 0.81). Qualitative
analyses of dilemma resolution and authority-related judgments illustrate how
these orientations surface in reasoning. Our results support the view that LLMs
function as statistical mirrors of their cultural corpora and motivate
culturally aware evaluation and deployment to avoid algorithmic cultural
hegemony.

</details>


### [194] [Uncovering Emergent Physics Representations Learned In-Context by Large Language Models](https://arxiv.org/abs/2508.12448)
*Yeongwoo Song,Jaeyong Bae,Dong-Kyum Kim,Hawoong Jeong*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLMs）在物理系统中的内在学习能力，发现其能够在语境中编码重要物理变量，为理解LLMs的学习机制提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 当前对LLMs的内在学习机制知之甚少，尤其是在解决不同物理任务时的表现。本研究旨在通过动力学预测任务探索LLMs在物理背景下的学习能力。

Method: 采用物理系统中的动力学预测任务作为研究模型表现的代理，通过稀疏自动编码器（SAEs）分析模型残差流激活，探索语言模型内部编码物理变量的机制。

Result: 研究发现，随着输入上下文长度的增加，LLMs的动力学预测性能提升；此外，SAEs捕获的特征与能量等关键物理变量相关联，表明模型能够在内在学习中编码有意义的物理概念。

Conclusion: 研究表明LLMs在内在学习中能够捕捉物理系统的结构性动态数据，其编码的特征与实际物理变量紧密关联，为了解LLMs的推理行为提供了重要见解。

Abstract: Large language models (LLMs) exhibit impressive in-context learning (ICL)
abilities, enabling them to solve wide range of tasks via textual prompts
alone. As these capabilities advance, the range of applicable domains continues
to expand significantly. However, identifying the precise mechanisms or
internal structures within LLMs that allow successful ICL across diverse,
distinct classes of tasks remains elusive. Physics-based tasks offer a
promising testbed for probing this challenge. Unlike synthetic sequences such
as basic arithmetic or symbolic equations, physical systems provide
experimentally controllable, real-world data based on structured dynamics
grounded in fundamental principles. This makes them particularly suitable for
studying the emergent reasoning behaviors of LLMs in a realistic yet tractable
setting. Here, we mechanistically investigate the ICL ability of LLMs,
especially focusing on their ability to reason about physics. Using a dynamics
forecasting task in physical systems as a proxy, we evaluate whether LLMs can
learn physics in context. We first show that the performance of dynamics
forecasting in context improves with longer input contexts. To uncover how such
capability emerges in LLMs, we analyze the model's residual stream activations
using sparse autoencoders (SAEs). Our experiments reveal that the features
captured by SAEs correlate with key physical variables, such as energy. These
findings demonstrate that meaningful physical concepts are encoded within LLMs
during in-context learning. In sum, our work provides a novel case study that
broadens our understanding of how LLMs learn in context.

</details>


### [195] [M3PO: Multimodal-Model-Guided Preference Optimization for Visual Instruction Following](https://arxiv.org/abs/2508.12458)
*Ruirui Gao,Emily Johnson,Bowen Tan,Yanfei Qian*

Main category: cs.CL

TL;DR: 本文提出了一种称为M3PO的方法，通过智能选择学习价值高的优先样本对，显著提升了LVLM在视觉指令理解和执行方面的能力。结合了多模态对齐评分和模型自信度，M3PO明显优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LVLM的开发常因高成本和注释不一致的挑战而受限，提升其视觉指令能力需要有效的偏好优化方法。

Method: 提出了结合多模态对齐评分和模型自一致性的M3P评分机制，用于引导偏好样本选择，并对LVLM进行高效的直接偏好优化微调。

Result: 相比传统SFT方法和现有的偏好优化方法（如RLHF和DPO），M3PO在多个多模态评测基准上表现优异。

Conclusion: M3PO方法能通过高效选择高质量的偏好样本对，提升LVLM的指令跟踪能力，是一种高效的数据驱动优化方法。

Abstract: Large Vision-Language Models (LVLMs) hold immense potential for complex
multimodal instruction following, yet their development is often hindered by
the high cost and inconsistency of human annotation required for effective
fine-tuning and preference alignment. Traditional supervised fine-tuning (SFT)
and existing preference optimization methods like RLHF and DPO frequently
struggle to efficiently leverage the model's own generation space to identify
highly informative "hard negative" samples. To address these challenges, we
propose Multimodal-Model-Guided Preference Optimization (M3PO), a novel and
data-efficient method designed to enhance LVLMs' capabilities in visual
instruction following. M3PO intelligently selects the most "learning-valuable"
preference sample pairs from a diverse pool of LVLM-generated candidates. This
selection is driven by a sophisticated mechanism that integrates two crucial
signals: a Multimodal Alignment Score (MAS) to assess external quality and the
model's Self-Consistency / Confidence (log-probability) to gauge internal
belief. These are combined into a novel M3P-Score, which specifically
identifies preferred responses and challenging dispreferred responses that the
model might confidently generate despite being incorrect. These high-quality
preference pairs are then used for efficient Direct Preference Optimization
(DPO) fine-tuning on base LVLMs like LLaVA-1.5 (7B/13B) using LoRA. Our
extensive experiments demonstrate that M3PO consistently outperforms strong
baselines, including SFT, simulated RLHF, vanilla DPO, and RM-DPO, across a
comprehensive suite of multimodal instruction following benchmarks (MME-Bench,
POPE, IFT, Human Pref. Score).

</details>


### [196] [LoraxBench: A Multitask, Multilingual Benchmark Suite for 20 Indonesian Languages](https://arxiv.org/abs/2508.12459)
*Alham Fikri Aji,Trevor Cohn*

Main category: cs.CL

TL;DR: 本文介绍LoraxBench，这是一个针对印度尼西亚低资源语言的基准测试数据集，包含六种任务和20种语言。


<details>
  <summary>Details</summary>
Motivation: 解决印度尼西亚低资源语言在自然语言处理中的挑战，特别是语言多样性和低资源情况的不足。

Method: 开发一个覆盖多语言和不同语言等级的数据集，评估多种语言模型在六种任务中的表现差异。

Result: 发现模型在印尼语和低资源语言之间表现有显著差异，地区特定模型和通用多语言模型之间无明显优劣。

Conclusion: 区域特定模型未必比多语言模型更优，正式和非正式语言等级对模型表现有显著影响，尤其在社交媒体中不常见的等级上。

Abstract: As one of the world's most populous countries, with 700 languages spoken,
Indonesia is behind in terms of NLP progress. We introduce LoraxBench, a
benchmark that focuses on low-resource languages of Indonesia and covers 6
diverse tasks: reading comprehension, open-domain QA, language inference,
causal reasoning, translation, and cultural QA. Our dataset covers 20
languages, with the addition of two formality registers for three languages. We
evaluate a diverse set of multilingual and region-focused LLMs and found that
this benchmark is challenging. We note a visible discrepancy between
performance in Indonesian and other languages, especially the low-resource
ones. There is no clear lead when using a region-specific model as opposed to
the general multilingual model. Lastly, we show that a change in register
affects model performance, especially with registers not commonly found in
social media, such as high-level politeness `Krama' Javanese.

</details>


### [197] [Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source Models](https://arxiv.org/abs/2508.12461)
*Ziqian Bi,Keyu Chen,Chiung-Yi Tseng,Danyang Zhang,Tianyang Wang,Hongying Luo,Lu Chen,Junming Huang,Jibin Guan,Junfeng Hao,Junhao Song*

Main category: cs.CL

TL;DR: OpenAI发布了GPT-OSS模型（20B和120B参数），测试表明20B表现优于120B，尤其在人类评测和数学推理方面。


<details>
  <summary>Details</summary>
Motivation: 探索稀疏架构扩展和模型优化策略的潜力，并改善开源模型选择的效率。

Method: 通过十项基准测试评估GPT-OSS与其他六种开放源模型的性能，使用McNemars测试进行统计验证。

Result: 尽管参数更少，gpt-oss-20B在若干基准测试中优于120B，强调稀疏架构的扩展未必带来性能比例上的提升。

Conclusion: 稀疏架构扩展需进一步优化与研究，能帮助选择高效的开源模型部署策略。

Abstract: In August 2025, OpenAI released GPT-OSS models, its first open weight large
language models since GPT-2 in 2019, comprising two mixture of experts
architectures with 120B and 20B parameters. We evaluated both variants against
six contemporary open source large language models ranging from 14.7B to 235B
parameters, representing both dense and sparse designs, across ten benchmarks
covering general knowledge, mathematical reasoning, code generation,
multilingual understanding, and conversational ability. All models were tested
in unquantised form under standardised inference settings, with statistical
validation using McNemars test and effect size analysis. Results show that
gpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such
as HumanEval and MMLU, despite requiring substantially less memory and energy
per response. Both models demonstrate mid-tier overall performance within the
current open source landscape, with relative strength in code generation and
notable weaknesses in multilingual tasks. These findings provide empirical
evidence that scaling in sparse architectures may not yield proportional
performance gains, underscoring the need for further investigation into
optimisation strategies and informing more efficient model selection for future
open source deployments.

</details>


### [198] [The Structural Sources of Verb Meaning Revisited: Large Language Models Display Syntactic Bootstrapping](https://arxiv.org/abs/2508.12482)
*Xiaomeng Zhu,R. Thomas McCoy,Robert Frank*

Main category: cs.CL

TL;DR: 研究分析大语言模型是否与儿童类似，利用句法环境来学习动词意义，通过训练模型在改动数据集上进行实验，发现句法线索对模型的影响比共现信息更大，特别是对心理动词有更显著的影响。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型是否展示出类似儿童句法启动现象的行为，即通过句法环境学习动词意义的能力。

Method: 通过训练RoBERTa和GPT-2模型，采用删除句法信息和共现信息的干扰数据集，分析两种信息变化对模型动词和名词表示的影响。

Result: 当句法线索删除时，模型的动词表征质量下降更严重，尤其是心理动词，而共现信息的删除对名词表征的影响更明显。

Conclusion: 句法启动在动词学习中具有重要作用，研究验证了通过操控大语言模型的学习环境来测试儿童语言学习假说的可行性。

Abstract: Syntactic bootstrapping (Gleitman, 1990) is the hypothesis that children use
the syntactic environments in which a verb occurs to learn its meaning. In this
paper, we examine whether large language models exhibit a similar behavior. We
do this by training RoBERTa and GPT-2 on perturbed datasets where syntactic
information is ablated. Our results show that models' verb representation
degrades more when syntactic cues are removed than when co-occurrence
information is removed. Furthermore, the representation of mental verbs, for
which syntactic bootstrapping has been shown to be particularly crucial in
human verb learning, is more negatively impacted in such training regimes than
physical verbs. In contrast, models' representation of nouns is affected more
when co-occurrences are distorted than when syntax is distorted. In addition to
reinforcing the important role of syntactic bootstrapping in verb learning, our
results demonstrated the viability of testing developmental hypotheses on a
larger scale through manipulating the learning environments of large language
models.

</details>


### [199] [Mitigating Hallucinations in Large Language Models via Causal Reasoning](https://arxiv.org/abs/2508.12495)
*Yuangang Li,Yiqing Shen,Yi Nian,Jiechao Gao,Ziyi Wang,Chenxiao Yu,Shawn Li,Jie Wang,Xiyang Hu,Yue Zhao*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）存在违反推理原则的逻辑不一致幻觉，本文引入因果-DAG构建和推理（CDCR-SFT）框架，通过显式构建因果DAG，并在其上进行推理，从而提升LLMs的因果推理能力并减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs推理方法操作于语言标记层面，未能建模变量间因果关系，缺乏表示条件独立性及满足因果识别假设的能力。

Method: 提出监督微调框架CDCR-SFT，训练LLMs显式构建变量级有向无环图（DAG），并基于DAG进行推理。此外，构建包含25,368个样本的数据集CausalDR，每个样本包含输入问题、因果DAG、推理过程及验证答案。

Result: CDCR-SFT在四个LLMs的八项任务中提升因果推理能力，在CLADDER数据集上达到95.33%的准确率（首次超过人类表现94.8%），并在幻觉评估上减少10%的幻觉。

Conclusion: 通过显式因果结构建模，LLMs能够有效减轻逻辑不一致问题，显著提升因果推理能力。

Abstract: Large language models (LLMs) exhibit logically inconsistent hallucinations
that appear coherent yet violate reasoning principles, with recent research
suggesting an inverse relationship between causal reasoning capabilities and
such hallucinations. However, existing reasoning approaches in LLMs, such as
Chain-of-Thought (CoT) and its graph-based variants, operate at the linguistic
token level rather than modeling the underlying causal relationships between
variables, lacking the ability to represent conditional independencies or
satisfy causal identification assumptions. To bridge this gap, we introduce
causal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning
framework that trains LLMs to explicitly construct variable-level directed
acyclic graph (DAG) and then perform reasoning over it. Moreover, we present a
dataset comprising 25,368 samples (CausalDR), where each sample includes an
input question, explicit causal DAG, graph-based reasoning trace, and validated
answer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves
the causal reasoning capability with the state-of-the-art 95.33% accuracy on
CLADDER (surpassing human performance of 94.8% for the first time) and reduces
the hallucination on HaluEval with 10% improvements. It demonstrates that
explicit causal structure modeling in LLMs can effectively mitigate logical
inconsistencies in LLM outputs. Code is available at
https://github.com/MrLYG/CDCR-SFT.

</details>


### [200] [CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2508.12535)
*Seonglae Cho,Zekun Wu,Adriano Koshiyama*

Main category: cs.CL

TL;DR: CorrSteer是一种在推理时通过特征相关性来自动选取和调整稀疏自动编码器(SAE)的方法，显著改善了下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的SAE方法在下游任务中的应用受限于对比数据集和存储需求，难以高效地提取具有实际意义的特征。

Method: 提出了一种名为CorrSteer的方法，通过推理时生成的激活数据计算样本正确性与SAE激活的相关性，从而自动选择特征并设定调整系数。

Result: 在Gemma 2 2B和LLaMA 3.1 8B上的QA、多种基准测试和推理任务中表现显著提升，其中MMLU性能提升4.1%、HarmBench性能提升22.9%。

Conclusion: 基于相关性的特征选择是一种高效且可扩展的SAE自动调整方法，能在语言模型应用中实现性能优化并揭示任务需求的语义模式。

Abstract: Sparse Autoencoders (SAEs) can extract interpretable features from large
language models (LLMs) without supervision. However, their effectiveness in
downstream steering tasks is limited by the requirement for contrastive
datasets or large activation storage. To address these limitations, we propose
CorrSteer, which selects features by correlating sample correctness with SAE
activations from generated tokens at inference time. This approach uses only
inference-time activations to extract more relevant features, thereby avoiding
spurious correlations. It also obtains steering coefficients from average
activations, automating the entire pipeline. Our method shows improved task
performance on QA, bias mitigation, jailbreaking prevention, and reasoning
benchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1%
improvement in MMLU performance and a +22.9% improvement in HarmBench with only
4000 samples. Selected features demonstrate semantically meaningful patterns
aligned with each task's requirements, revealing the underlying capabilities
that drive performance. Our work establishes correlationbased selection as an
effective and scalable approach for automated SAE steering across language
model applications.

</details>


### [201] [Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking Assessment with Effective Curriculum Learning](https://arxiv.org/abs/2508.12591)
*Yu-Hsuan Fang,Tien-Hong Lo,Yao-Ting Sung,Berlin Chen*

Main category: cs.CL

TL;DR: 本文研究了多模态大型语言模型（MLLM）的自动口语评估性能，提出了Speech-First Multimodal Training（SFMT）方法，显著提升评估准确性。


<details>
  <summary>Details</summary>
Motivation: 传统自动口语评估系统在模式上存在局限性，文本方法缺少音频信息，而音频方法则缺乏语义内容。作者希望通过多模态模型解决这一问题。

Method: 提出Speech-First Multimodal Training（SFMT）方法，采用课程式学习强化语音建模基础，再融合跨模态信息提升整体评估性能。

Result: 实验显示，MLLM系统的评估能力提升，尤其在‘delivery’方面，SFMT方法将绝对准确率提高了4%，整体相关系数PCC值由0.783提升到0.846。

Conclusion: MLLM具备显著优势，SFMT在评估效率上表现突出，为自动口语评估提供了新方向。

Abstract: Traditional Automated Speaking Assessment (ASA) systems exhibit inherent
modality limitations: text-based approaches lack acoustic information while
audio-based methods miss semantic context. Multimodal Large Language Models
(MLLM) offer unprecedented opportunities for comprehensive ASA by
simultaneously processing audio and text within unified frameworks. This paper
presents a very first systematic study of MLLM for comprehensive ASA,
demonstrating the superior performance of MLLM across the aspects of content
and language use . However, assessment on the delivery aspect reveals unique
challenges, which is deemed to require specialized training strategies. We thus
propose Speech-First Multimodal Training (SFMT), leveraging a curriculum
learning principle to establish more robust modeling foundations of speech
before cross-modal synergetic fusion. A series of experiments on a benchmark
dataset show MLLM-based systems can elevate the holistic assessment performance
from a PCC value of 0.783 to 0.846. In particular, SFMT excels in the
evaluation of the delivery aspect, achieving an absolute accuracy improvement
of 4% over conventional training approaches, which also paves a new avenue for
ASA.

</details>


### [202] [Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for Persistent Conversational Context](https://arxiv.org/abs/2508.12630)
*Maitreyi Chatterjee,Devansh Agarwal*

Main category: cs.CL

TL;DR: 提出一种名为Semantic Anchoring的混合记忆架构，通过结合显式的语言线索来增强长期对话中的记忆持久性和语义丰富性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在多会话和长期交互中因存储限制导致效果受阻，现存的检索增强生成系统无法捕捉精细的语言结构信息，例如句法依赖和语篇关系，激励研究更优化的存储解决方案。

Method: 结合句法解析、语篇关系标注、指代消解技术，采用混合式的内存架构，将向量化存储和显式语言特征相结合。

Result: 在改进的长期对话数据集上实现了比基线高出18%的事实召回率和语篇连贯性提升。

Conclusion: 通过实验和分析表明，Semantic Anchoring在长期对话中的记忆效果具有鲁棒性和可解释性。

Abstract: Large Language Models (LLMs) have demonstrated impressive fluency and task
competence in conversational settings. However, their effectiveness in
multi-session and long-term interactions is hindered by limited memory
persistence. Typical retrieval-augmented generation (RAG) systems store
dialogue history as dense vectors, which capture semantic similarity but
neglect finer linguistic structures such as syntactic dependencies, discourse
relations, and coreference links. We propose Semantic Anchoring, a hybrid
agentic memory architecture that enriches vector-based storage with explicit
linguistic cues to improve recall of nuanced, context-rich exchanges. Our
approach combines dependency parsing, discourse relation tagging, and
coreference resolution to create structured memory entries. Experiments on
adapted long-term dialogue datasets show that semantic anchoring improves
factual recall and discourse coherence by up to 18% over strong RAG baselines.
We further conduct ablation studies, human evaluations, and error analysis to
assess robustness and interpretability.

</details>


### [203] [Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing](https://arxiv.org/abs/2508.12631)
*Yiqun Zhang,Hao Li,Jianhao Chen,Hangfan Zhang,Peng Ye,Lei Bai,Shuyue Hu*

Main category: cs.CL

TL;DR: Avengers-Pro是一种测试时路由框架，能在性能和效率之间实现最佳平衡，通过将查询动态分配到不同容量和效率的LLM上，达到了最优的性能效率权衡。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的一个核心挑战是在性能和效率之间找到平衡，该研究希望通过动态路由模型解决该问题。

Method: 提出Avengers-Pro框架，通过嵌入和聚类传入查询，然后基于性能效率评分将其分配到适当的模型中。

Result: 在6个基准测试和8个模型的比较中，Avengers-Pro达到了最先进的结果，性能优于单一最强模型，同时成本显著降低。

Conclusion: Avengers-Pro在性能和效率权衡方面达到了Pareto最优，总体上实现了更高效且性能更优的模型组合方法。

Abstract: Balancing performance and efficiency is a central challenge in large language
model (LLM) advancement. GPT-5 addresses this with test-time routing,
dynamically assigning queries to either an efficient or a high-capacity model
during inference. In this work, we present Avengers-Pro, a test-time routing
framework that ensembles LLMs of varying capacities and efficiencies, providing
a unified solution for all performance-efficiency tradeoffs. The Avengers-Pro
embeds and clusters incoming queries, then routes each to the most suitable
model based on a performance-efficiency score. Across 6 challenging benchmarks
and 8 leading models -- including GPT-5-medium, Gemini-2.5-pro, and
Claude-opus-4.1 -- Avengers-Pro achieves state-of-the-art results: by varying a
performance-efficiency trade-off parameter, it can surpass the strongest single
model (GPT-5-medium) by +7% in average accuracy. Moreover, it can match the
average accuracy of the strongest single model at 27% lower cost, and reach
~90% of that performance at 63% lower cost. Last but not least, it achieves a
Pareto frontier, consistently yielding the highest accuracy for any given cost,
and the lowest cost for any given accuracy, among all single models. Code is
available at https://github.com/ZhangYiqun018/AvengersPro.

</details>


### [204] [Prompt-Induced Linguistic Fingerprints for LLM-Generated Fake News Detection](https://arxiv.org/abs/2508.12632)
*Chi Wang,Min Gao,Zongwei Wang,Junwei Yin,Kai Shu,Chenghua Lin*

Main category: cs.CL

TL;DR: 文章提出利用语言生成模型（LLM）生成的假新闻的语言指纹来检测假新闻的方法。


<details>
  <summary>Details</summary>
Motivation: 由于假新闻生成变得更加容易，该问题对社会产生重大威胁，因此需要开发可靠的检测方法。

Method: 通过词级别的概率分布重构，提出了一种名为LIFE的新方法，结合关键片段技术，识别LLM生成假新闻的语言指纹。

Result: 实验表明，LIFE在检测LLM生成的假新闻中达到目前最好的性能，并在人工撰写的假新闻检测中依然具有高表现。

Conclusion: LIFE提供了一种有效的方法，通过语言指纹检测假新闻，解决了LLM生成内容难以辨别的问题。

Abstract: With the rapid development of large language models, the generation of fake
news has become increasingly effortless, posing a growing societal threat and
underscoring the urgent need for reliable detection methods. Early efforts to
identify LLM-generated fake news have predominantly focused on the textual
content itself; however, because much of that content may appear coherent and
factually consistent, the subtle traces of falsification are often difficult to
uncover. Through distributional divergence analysis, we uncover prompt-induced
linguistic fingerprints: statistically distinct probability shifts between
LLM-generated real and fake news when maliciously prompted. Based on this
insight, we propose a novel method named Linguistic Fingerprints Extraction
(LIFE). By reconstructing word-level probability distributions, LIFE can find
discriminative patterns that facilitate the detection of LLM-generated fake
news. To further amplify these fingerprint patterns, we also leverage
key-fragment techniques that accentuate subtle linguistic differences, thereby
improving detection reliability. Our experiments show that LIFE achieves
state-of-the-art performance in LLM-generated fake news and maintains high
performance in human-written fake news. The code and data are available at
https://anonymous.4open.science/r/LIFE-E86A.

</details>


### [205] [Breaking Language Barriers: Equitable Performance in Multilingual Language Models](https://arxiv.org/abs/2508.12662)
*Tanay Nagar,Grigorii Khvatskii,Anna Sokol,Nitesh V. Chawla*

Main category: cs.CL

TL;DR: LLMs在低资源语言上表现较差，该研究通过细微调整模型，改进低资源语言的表现。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在低资源语言上的表现不佳问题，以促进语言公平性。

Method: 通过生成合成的语言混合文本并进行微调，以改进低资源语言的表现。

Result: 实验结果显示，微调后的模型在低资源语言上的性能显著提升，同时在高资源语言上的性能保持或增强。

Conclusion: 合成语言混合文本微调可以有效提升LLMs在不同语言上的表现，有助于语言平等。

Abstract: Cutting-edge LLMs have emerged as powerful tools for multilingual
communication and understanding. However, LLMs perform worse in Common Sense
Reasoning (CSR) tasks when prompted in low-resource languages (LRLs) like Hindi
or Swahili compared to high-resource languages (HRLs) like English. Equalizing
this inconsistent access to quality LLM outputs is crucial to ensure fairness
for speakers of LRLs and across diverse linguistic communities. In this paper,
we propose an approach to bridge this gap in LLM performance. Our approach
involves fine-tuning an LLM on synthetic code-switched text generated using
controlled language-mixing methods. We empirically demonstrate that fine-tuning
LLMs on synthetic code-switched datasets leads to substantial improvements in
LRL model performance while preserving or enhancing performance in HRLs.
Additionally, we present a new dataset of synthetic code-switched text derived
from the CommonSenseQA dataset, featuring three distinct language ratio
configurations.

</details>


### [206] [Leveraging Large Language Models for Predictive Analysis of Human Misery](https://arxiv.org/abs/2508.12669)
*Bishanka Seal,Rahul Seetharaman,Aman Bansal,Abhilash Nandy*

Main category: cs.CL

TL;DR: 本研究探索使用大型语言模型(LLMs)预测人类感知的自然语言场景中的痛苦评分，并提出了一种新的多轮互动评估框架。


<details>
  <summary>Details</summary>
Motivation: 使用自然语言描述预测痛苦评分对理解情感计算有重要意义。

Method: 将任务建模为回归问题，并评估不同提示策略，同时引入一种名为“痛苦游戏展示”的交互评估框架。

Result: 几种少样本提示方法优于零样本提示，同时通过“痛苦游戏展示”框架评估了模型的动态调整能力。

Conclusion: 少样本提示能够提高模型在情感预测中的表现，交互式评估框架突出了LLMs在情感推理任务中的潜力。

Abstract: This study investigates the use of Large Language Models (LLMs) for
predicting human-perceived misery scores from natural language descriptions of
real-world scenarios. The task is framed as a regression problem, where the
model assigns a scalar value from 0 to 100 to each input statement. We evaluate
multiple prompting strategies, including zero-shot, fixed-context few-shot, and
retrieval-based prompting using BERT sentence embeddings. Few-shot approaches
consistently outperform zero-shot baselines, underscoring the value of
contextual examples in affective prediction. To move beyond static evaluation,
we introduce the "Misery Game Show", a novel gamified framework inspired by a
television format. It tests LLMs through structured rounds involving ordinal
comparison, binary classification, scalar estimation, and feedback-driven
reasoning. This setup enables us to assess not only predictive accuracy but
also the model's ability to adapt based on corrective feedback. The gamified
evaluation highlights the broader potential of LLMs in dynamic emotional
reasoning tasks beyond standard regression. Code and data link:
https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub

</details>


### [207] [ToolACE-MT: Non-Autoregressive Generation for Agentic Multi-Turn Interaction](https://arxiv.org/abs/2508.12685)
*Xingshan Zeng,Weiwen Liu,Lingzhi Wang,Liangyou Li,Fei Mi,Yasheng Wang,Lifeng Shang,Xin Jiang,Qun Liu*

Main category: cs.CL

TL;DR: 提出了ToolACE-MT框架，用于生成高质量的多轮对话，分为初始化、迭代细化和离线验证三个阶段。


<details>
  <summary>Details</summary>
Motivation: 现有的基于模拟的方法在生成多轮对话时成本高且性能有限，需要改进生成效率和质量。

Method: 提出Non-Autoregressive Iterative Generation框架ToolACE-MT，分为三步：对话框架的初始化、通过Mask-and-Fill操作进行细化、最终通过规则和模型校验确保准确性和一致性。

Result: 实验证明ToolACE-MT可以实现高效、有效且具有通用性的数据生成能力。

Conclusion: ToolACE-MT框架为工具增强型LLM场景中的高质量数据构造提供了一种全新的方法论。

Abstract: Agentic task-solving with Large Language Models (LLMs) requires multi-turn,
multi-step interactions, often involving complex function calls and dynamic
user-agent exchanges. Existing simulation-based data generation methods for
such scenarios rely heavily on costly autoregressive interactions between
multiple LLM agents, thereby limiting real-world performance of agentic tasks.
In this paper, we propose a novel Non-Autoregressive Iterative Generation
framework, called ToolACE-MT, for constructing high-quality multi-turn agentic
dialogues. ToolACE-MT generates full conversational trajectories through three
stages: coarse-grained initialization, iterative refinement, and offline
verification. The initialization phase builds a structurally complete yet
semantically coarse dialogue skeleton; the iterative refinement phase
introduces realistic complexities and continued refinement via mask-and-fill
operations; and the offline verification phase ensures correctness and
coherence via rule- and model-based checks. Experiments demonstrate that
ToolACE-MT enables efficient, effective and generalizable agentic data
generation, offering a new paradigm for high-quality data construction in
tool-augmented LLM scenarios.

</details>


### [208] [DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning](https://arxiv.org/abs/2508.12726)
*Weize Liu,Yongchi Zhao,Yijia Luo,Mingyu Xu,Jiaheng Liu,Yanan Li,Xiguo Hu,Yuchi Xu,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: 本文提出了一个称为DESIGNER的数据生成管道，用于从书籍和网页语料中生成多学科高难度推理问题，同时构建了两个大规模推理数据集，分别包含304万和166万道问题。在Qwen3系列模型上验证了它们的优越性。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在许多自然语言任务上表现出色，但仍然在跨学科和复杂推理方面存在不足。目前的推理数据集要么缺乏学科广度，要么缺乏结构深度。

Method: 提出了一个基于“设计逻辑”概念的推理问题生成管道DESIGNER，利用现有问答样本逆向推导出设计逻辑，将其与多样化学科内容相匹配，生成具有挑战性的问题数据集。

Result: 生成了两个多学科大规模推理数据集（DLR-Book和DLR-Web），涵盖75个学科，显著提升了数据的难度和多样性，并在Qwen3系列模型上的性能超越了现有同体量数据集。

Conclusion: 本研究提供了一种高效的推理问题生成方法和高质量数据集，不仅提升了模型性能，还拓展了多学科推理的研究边界。

Abstract: Large language models (LLMs) have achieved remarkable success in many natural
language tasks but still struggle with complex, multi-step reasoning,
particularly across diverse disciplines. Existing reasoning datasets often
either lack disciplinary breadth or the structural depth necessary to elicit
robust reasoning behaviors. We propose DESIGNER: a DESIGN-logic-guidEd
Reasoning data synthesis pipeline that leverages naturally available, extensive
raw documents (book corpus and web corpus) to generate multidisciplinary
challenging questions. A core innovation of our approach is the introduction of
a Design Logic concept, which mimics the question-creation process of human
educators. We use LLMs to reverse-engineer and abstract over 120,000 design
logics from existing questions across various disciplines. By matching these
design logics with disciplinary source materials, we are able to create
reasoning questions that far surpass the difficulty and diversity of existing
datasets. Based on this pipeline, we synthesized two large-scale reasoning
datasets that span 75 disciplines: Design-Logic-Reasoning-Book (DLR-Book),
containing 3.04 million challenging questions synthesized from the book corpus,
and Design-Logic-Reasoning-Web (DLR-Web), with 1.66 million challenging
questions from the web corpus. Our data analysis demonstrates that the
questions synthesized by our method exhibit substantially greater difficulty
and diversity than those in the baseline datasets. We validate the
effectiveness of these datasets by conducting SFT experiments on the
Qwen3-8B-Base and Qwen3-4B-Base models. The results show that our dataset
significantly outperforms existing multidisciplinary datasets of the same
volume. Training with the full datasets further enables the models to surpass
the multidisciplinary reasoning performance of the official Qwen3-8B and
Qwen3-4B models.

</details>


### [209] [LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models](https://arxiv.org/abs/2508.12733)
*Zhiyuan Ning,Tianle Gu,Jiaxin Song,Shixin Hong,Lingyu Li,Huacan Liu,Jie Li,Yixu Wang,Meng Lingyu,Yan Teng,Yingchun Wang*

Main category: cs.CL

TL;DR: 本文提出LinguaSafe，一个用于多语言大模型安全性评估的新基准，包括12种语言的45,000个条目，并引入了多维和细粒度的安全评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前多语言大模型的安全性评估数据和方法较少，影响了多语言安全对齐的发展。

Method: 设计并创建了一个包含12种语言（从匈牙利语到马来语）45,000条目的数据集，数据来源包括翻译、转换创作和原创内容，此外提出直接和间接安全评估框架。

Result: 不同领域和不同语言上的安全性和有效性指标存在显著差异，即使在资源水平相近的语言中亦如此。

Conclusion: LinguaSafe弥补了大模型在多语言安全评估中的缺口，提供了一套全面的评估指标，并公开数据集和代码以促进进一步研究。

Abstract: The widespread adoption and increasing prominence of large language models
(LLMs) in global technologies necessitate a rigorous focus on ensuring their
safety across a diverse range of linguistic and cultural contexts. The lack of
a comprehensive evaluation and diverse data in existing multilingual safety
evaluations for LLMs limits their effectiveness, hindering the development of
robust multilingual safety alignment. To address this critical gap, we
introduce LinguaSafe, a comprehensive multilingual safety benchmark crafted
with meticulous attention to linguistic authenticity. The LinguaSafe dataset
comprises 45k entries in 12 languages, ranging from Hungarian to Malay. Curated
using a combination of translated, transcreated, and natively-sourced data, our
dataset addresses the critical need for multilingual safety evaluations of
LLMs, filling the void in the safety evaluation of LLMs across diverse
under-represented languages from Hungarian to Malay. LinguaSafe presents a
multidimensional and fine-grained evaluation framework, with direct and
indirect safety assessments, including further evaluations for oversensitivity.
The results of safety and helpfulness evaluations vary significantly across
different domains and different languages, even in languages with similar
resource levels. Our benchmark provides a comprehensive suite of metrics for
in-depth safety evaluation, underscoring the critical importance of thoroughly
assessing multilingual safety in LLMs to achieve more balanced safety
alignment. Our dataset and code are released to the public to facilitate
further research in the field of multilingual LLM safety.

</details>


### [210] [CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description](https://arxiv.org/abs/2508.12769)
*Shaoming Duan,Zirui Wang,Chuanyi Liu,Zhibin Zhu,Yuhao Zhang,Peiyi Han,Liang Yan,Zewu Penge*

Main category: cs.CL

TL;DR: CRED-SQL通过集群检索及执行描述的方式解决了大规模数据库中自然语言问题到SQL查询过程中语义不匹配的问题，并在实验中取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模数据库中自然语言问题和SQL查询之间的语义不匹配问题，尤其是在语义相似的属性和语义漂移情况下导致的模型精度下降问题。

Method: 提出CRED-SQL框架，通过集群检索方式检索与自然语言问题相关的表和列，并引入一种中间自然语言表示方式——执行描述语言(EDL)，将任务分为Text-to-EDL和EDL-to-SQL两个阶段，以减少语义偏差并提高SQL生成的准确性。

Result: 在SpiderUnion和BirdUnion两个大规模跨领域基准数据集上进行了实验，CRED-SQL实现了新的SOTA性能，验证了其高效性和可扩展性。

Conclusion: CRED-SQL在应对大规模数据库中的语义不匹配问题方面表现出色，通过新引入的EDL有效改进了SQL生成，证明了其在实际应用中的潜力。

Abstract: Recent advances in large language models (LLMs) have significantly improved
the accuracy of Text-to-SQL systems. However, a critical challenge remains: the
semantic mismatch between natural language questions (NLQs) and their
corresponding SQL queries. This issue is exacerbated in large-scale databases,
where semantically similar attributes hinder schema linking and semantic drift
during SQL generation, ultimately reducing model accuracy. To address these
challenges, we introduce CRED-SQL, a framework designed for large-scale
databases that integrates Cluster Retrieval and Execution Description. CRED-SQL
first performs cluster-based large-scale schema retrieval to pinpoint the
tables and columns most relevant to a given NLQ, alleviating schema mismatch.
It then introduces an intermediate natural language representation-Execution
Description Language (EDL)-to bridge the gap between NLQs and SQL. This
reformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL,
leveraging LLMs' strong general reasoning capabilities while reducing semantic
deviation. Extensive experiments on two large-scale, cross-domain
benchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new
state-of-the-art (SOTA) performance, validating its effectiveness and
scalability. Our code is available at https://github.com/smduan/CRED-SQL.git

</details>


### [211] [From SALAMANDRA to SALAMANDRATA: BSC Submission for WMT25 General Machine Translation Shared Task](https://arxiv.org/abs/2508.12774)
*Javier Garcia Gilabert,Xixian Liao,Severino Da Dalt,Ella Bohman,Audrey Mash,Francesca De Luca Fornaciari,Irene Baucells,Joan Llop,Miguel Claramunt Argote,Carlos Escolano,Maite Melero*

Main category: cs.CL

TL;DR: 本文提出了一种改进版本的SALAMANDRA模型家族——SALAMANDRATA，用于38种欧洲语言的翻译任务，并公开发布了其2B和7B参数版本。


<details>
  <summary>Details</summary>
Motivation: 改进原有的SALAMANDRA模型，以在多语言翻译任务中实现更优性能，同时拓展语言支持范围。

Method: 采用两步训练策略：第一步在平行数据上进行持续预训练，第二步对高质量指令进行监督微调。此外，针对新支持的非欧洲语言增加了词汇适配，使用最小贝叶斯风险解码和经过调优的重排序策略以提升翻译质量。

Result: SALAMANDRATA模型成功优化多语言翻译表现，尤其是38种欧洲语言中表现亮眼，同时为更广泛的翻译任务扩展了语言支持。

Conclusion: SALAMANDRATA家族模型在翻译相关任务中展现了优越性能，为未来多语言翻译研究开辟了新的视角。作者已经将相关版本公开发布，便于进一步研究。

Abstract: In this paper, we present the SALAMANDRATA family of models, an improved
iteration of SALAMANDRA LLMs (Gonzalez-Agirre et al., 2025) specifically
trained to achieve strong performance in translation-related tasks for 38
European languages. SALAMANDRATA comes in two scales: 2B and 7B parameters. For
both versions, we applied the same training recipe with a first step of
continual pre-training on parallel data, and a second step of supervised
fine-tuning on high-quality instructions. The BSC submission to the WMT25
General Machine Translation shared task is based on the 7B variant of
SALAMANDRATA. We first adapted the model vocabulary to support the additional
non-European languages included in the task. This was followed by a second
phase of continual pre-training and supervised fine-tuning, carefully designed
to optimize performance across all translation directions for this year's
shared task. For decoding, we employed two quality-aware strategies: Minimum
Bayes Risk Decoding and Tuned Re-ranking using COMET and COMET-KIWI
respectively. We publicly release both the 2B and 7B versions of SALAMANDRATA,
along with the newer SALAMANDRATA-V2 model, on Hugging Face1

</details>


### [212] [HeteroRAG: A Heterogeneous Retrieval-Augmented Generation Framework for Medical Vision Language Tasks](https://arxiv.org/abs/2508.12778)
*Zhe Chen,Yusheng Liao,Shuyang Jiang,Zhiyuan Zhu,Haolin Li,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 本文提出了MedAtlas和HeteroRAG框架，通过异构知识源改善医学大型视觉语言模型（Med-LVLMs）的准确性和可靠性，提升了诊断性能。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有医学多模态RAG系统在异构信息检索上的不足，提高分析结果的准确性和临床决策的可信度。

Method: 设计了MedAtlas作为大规模多模态报告库，并提出HeteroRAG框架，该框架包括模态具体CLIP用于报告检索、多语料查询生成器动态构建查询，以及异构知识优先调优进行知识对齐。

Result: 在12个数据集和3种模态上的大量实验表明，HeteroRAG在大多数医学视觉语言基准中达到了最先进的性能，显著提高了Med-LVLMs的准确性和可靠性。

Conclusion: HeteroRAG框架通过结合异构知识源，提升了Med-LVLMs的多模态和多源知识对齐能力，为医学应用中提供了更可靠的诊断支持。

Abstract: Medical large vision-language Models (Med-LVLMs) have shown promise in
clinical applications but suffer from factual inaccuracies and unreliable
outputs, posing risks in real-world diagnostics. While retrieval-augmented
generation has emerged as a potential solution, current medical multimodal RAG
systems are unable to perform effective retrieval across heterogeneous sources.
The irrelevance of retrieved reports affects the factuality of analysis, while
insufficient knowledge affects the credibility of clinical decision-making. To
bridge the gap, we construct MedAtlas, which includes extensive multimodal
report repositories and diverse text corpora. Based on it, we present
HeteroRAG, a novel framework that enhances Med-LVLMs through heterogeneous
knowledge sources. The framework introduces Modality-specific CLIPs for
effective report retrieval and a Multi-corpora Query Generator for dynamically
constructing queries for diverse corpora. Incorporating knowledge from such
multifaceted sources, Med-LVLM is then trained with Heterogeneous Knowledge
Preference Tuning to achieve cross-modality and multi-source knowledge
alignment. Extensive experiments across 12 datasets and 3 modalities
demonstrate that the proposed HeteroRAG achieves state-of-the-art performance
in most medical vision language benchmarks, significantly improving factual
accuracy and reliability of Med-LVLMs.

</details>


### [213] [Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward](https://arxiv.org/abs/2508.12800)
*Yong Deng,Guoqing Wang,Zhenzhe Ying,Xiaofeng Wu,Jinzhen Lin,Wenwen Xiong,Yuqin Dai,Shuo Yang,Zhanwei Zhang,Qiwen Wang,Yang Qin,Changhua Meng*

Main category: cs.CL

TL;DR: 提出了Atomic Thought范式和Atom-Searcher框架提升LLMs的复杂任务解决能力，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在复杂任务中因静态知识库和僵化的工作流表现受限，且强化学习方法面临奖励稀疏和冲突梯度问题。

Method: 引入Atomic Thought思维范式，将推理分解为微粒化功能单元；通过Reasoning Reward Models (RRMs)和Atomic Thought Rewards (ATR)进行细粒度监督；结合灵感于课程学习的奖励调度机制提出Atom-Searcher强化学习框架。

Result: 在七个不同基准测试上均实现了对最先进方法的稳定性能提升。

Conclusion: 提出的方法改善了推理效率和策略收敛速度，同时具有更人性化和可解释的推理模式。

Abstract: Large language models (LLMs) exhibit remarkable problem-solving abilities,
but struggle with complex tasks due to static internal knowledge.
Retrieval-Augmented Generation (RAG) enhances access to external information,
yet remains limited in multi-hop reasoning and strategic search due to rigid
workflows. Recent advancements in agentic deep research empower LLMs to
autonomously reason, search, and synthesize information. However, current
approaches relying on outcome-based reinforcement learning (RL) face critical
issues such as conflicting gradients and reward sparsity, limiting performance
gains and training efficiency. To address these, we first propose Atomic
Thought, a novel LLM thinking paradigm that decomposes reasoning into
fine-grained functional units. These units are supervised by Reasoning Reward
Models (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained
guidance. Building on this, we propose Atom-Searcher, a novel RL framework for
agentic deep research that integrates Atomic Thought and ATR. Atom-Searcher
uses a curriculum-inspired reward schedule, prioritizing process-level ATR
early and transitioning to outcome rewards, accelerating convergence on
effective reasoning paths. Experiments on seven benchmarks show consistent
improvements over the state-of-the-art. Key advantages include: (1)
Atom-Searcher scales computation at test-time. (2) Atomic Thought provides
supervision anchors for RRMs, bridging deep research tasks and RRMs. (3)
Atom-Searcher exhibits more interpretable, human-like reasoning patterns.

</details>


### [214] [When Alignment Hurts: Decoupling Representational Spaces in Multilingual Models](https://arxiv.org/abs/2508.12803)
*Ahmed Elshabrawy,Hour Kaing,Haiyue Song,Alham Fikri Aji,Hideki Tanaka,Masao Utiyama,Raj Dabre*

Main category: cs.CL

TL;DR: 本文质疑语言模型中，高资源语言对相关低资源语言建模的正面影响，指出过度依赖高资源语言会削弱生成能力，并提出了一种基于子空间因果干预的框架改善生成表现。


<details>
  <summary>Details</summary>
Motivation: 研究针对语言模型中高资源标准语言与相关低资源语言的代表性纠缠问题，通过发现和干预高资源语言对低资源语言生成能力的限制，提出方法论改进。

Method: 提出了一种在线的变分探测框架，用于持续估计标准语言子空间，并依此通过投影解耦，以减少对标准语言的代表性依赖。实验以阿拉伯语为案例，通过细调步骤进行干预并验证效果。

Result: 该方法在25种阿拉伯语方言的生成质量上平均提升了+2.0 chrF++，最高可提升+4.9 chrF++，但对标准语言表现有一定影响。

Conclusion: 研究提供了高资源语言子空间主导对低资源生成能力的因果证据，提出了统一几何和信息探测与子空间干预的方法，为多语言与多领域语言模型的优化提供了新工具。

Abstract: Alignment with high-resource standard languages is often assumed to aid the
modeling of related low-resource varieties. We challenge this assumption by
demonstrating that excessive representational entanglement with a dominant
variety, such as Modern Standard Arabic (MSA) in relation to Arabic dialects,
can actively hinder generative modeling. We present the first comprehensive
causal study of this phenomenon by analyzing and directly intervening in the
internal representation geometry of large language models (LLMs). Our key
contribution is an online variational probing framework that continuously
estimates the subspace of the standard variety during fine-tuning, enabling
projection-based decoupling from this space. While our study uses Arabic as a
case due to its unusually rich parallel resources across 25 dialects, the
broader motivation is methodological: dialectal MT serves as a controlled proxy
for generative tasks where comparable multi-variety corpora are unavailable.
Across 25 dialects, our intervention improves generation quality by up to +4.9
chrF++ and +2.0 on average compared to standard fine-tuning, despite a measured
tradeoff in standard-language performance. These results provide causal
evidence that subspace dominance by high-resource varieties can restrict
generative capacity for related varieties. More generally, we unify geometric
and information-theoretic probing with subspace-level causal interventions,
offering practical tools for improving generative modeling in closely related
language families and, more broadly, for controlling representational
allocation in multilingual and multi-domain LLMs. Code will be released.

</details>


### [215] [ding-01 :ARG0: An AMR Corpus for Spontaneous French Dialogue](https://arxiv.org/abs/2508.12819)
*Jeongwoo Kang,Maria Boritchev,Maximin Coavoux*

Main category: cs.CL

TL;DR: 本研究通过为法语对话语料库注释抽象意义表示（AMR），开发了法语语义语料库，并扩展AMR框架以适应法语特有的语言结构。


<details>
  <summary>Details</summary>
Motivation: 旨在通过构建法语对话语料库以弥补现有AMR框架对法语和自发对话动态表达覆盖不足的问题。

Method: 对DinG语料库（由棋盘游戏Catan录制的法语对话语料）进行AMR注释，扩展AMR框架以适应自发语音和法语特定句法结构，并提供注释指南。同时，训练并评估AMR解析器，用于支持人工标注。

Result: 生成了公开的法语语义语料库，并提供了一个可辅助开始注释的AMR解析工具来提高注释效率。

Conclusion: 研究推动了法语对话语义资源的发展，扩展了AMR在自发语音和法语结构中的应用。

Abstract: We present our work to build a French semantic corpus by annotating French
dialogue in Abstract Meaning Representation (AMR). Specifically, we annotate
the DinG corpus, consisting of transcripts of spontaneous French dialogues
recorded during the board game Catan. As AMR has insufficient coverage of the
dynamics of spontaneous speech, we extend the framework to better represent
spontaneous speech and sentence structures specific to French. Additionally, to
support consistent annotation, we provide an annotation guideline detailing
these extensions. We publish our corpus under a free license (CC-SA-BY). We
also train and evaluate an AMR parser on our data. This model can be used as an
assistance annotation tool to provide initial annotations that can be refined
by human annotators. Our work contributes to the development of semantic
resources for French dialogue.

</details>


### [216] [Context Matters: Incorporating Target Awareness in Conversational Abusive Language Detection](https://arxiv.org/abs/2508.12828)
*Raneem Alharthi,Rajwa Alharthi,Aiqi Jiang,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: 该研究探索通过父推文上下文来改进回复推文是否具有侮辱性语言的检测。


<details>
  <summary>Details</summary>
Motivation: 当前对社交媒体滥用语言的研究主要集中于单一推文，忽略了可以从周围上下文中获取的额外信息。

Method: 分析父推文和回复推文的对话对，并比较使用上下文特征和仅使用回复推文特征的模型表现。

Result: 结合上下文特征的模型显著优于仅使用回复推文特征的模型，尤其是基于内容的特征更能提升分类性能。

Conclusion: 在滥用语言检测中，结合父推文的上下文和多种内容特征，可显著提升模型性能，这为构建上下文化模型提供了重要启示。

Abstract: Abusive language detection has become an increasingly important task as a
means to tackle this type of harmful content in social media. There has been a
substantial body of research developing models for determining if a social
media post is abusive or not; however, this research has primarily focused on
exploiting social media posts individually, overlooking additional context that
can be derived from surrounding posts. In this study, we look at conversational
exchanges, where a user replies to an earlier post by another user (the parent
tweet). We ask: does leveraging context from the parent tweet help determine if
a reply post is abusive or not, and what are the features that contribute the
most? We study a range of content-based and account-based features derived from
the context, and compare this to the more widely studied approach of only
looking at the features from the reply tweet. For a more generalizable study,
we test four different classification models on a dataset made of
conversational exchanges (parent-reply tweet pairs) with replies labeled as
abusive or not. Our experiments show that incorporating contextual features
leads to substantial improvements compared to the use of features derived from
the reply tweet only, confirming the importance of leveraging context. We
observe that, among the features under study, it is especially the
content-based features (what is being posted) that contribute to the
classification performance rather than account-based features (who is posting
it). While using content-based features, it is best to combine a range of
different features to ensure improved performance over being more selective and
using fewer features. Our study provides insights into the development of
contextualized abusive language detection models in realistic settings
involving conversations.

</details>


### [217] [It takes a village to write a book: Mapping anonymous contributions in Stephen Langton's Quaestiones Theologiae](https://arxiv.org/abs/2508.12830)
*Jan Maliszewski*

Main category: cs.CL

TL;DR: 该论文运用文体计量技术分析Stephen Langton的《神学问题集》，以揭示编辑工作层次和验证相关假设，并探索了手工数据与自动提取数据的计算性能对比。


<details>
  <summary>Details</summary>
Motivation: 旨在通过文体计量技术揭示中世纪大学协作文学作品的编辑层次并验证其形成本质，同时测试现代技术对中世纪学术拉丁文语料的适用性。

Method: 该研究结合HTR流水线和基于高频词、词性标注及伪后缀的文体计量分析，探索Transformer-based OCR和自动对齐在学术拉丁语料中的应用。

Result: 如果该研究成功，将提供一种可重用的模板，用于分析中世纪大学生成协作文学作品。

Conclusion: 该研究不仅验证假设和技术的有效性，还提出对未来学术拉丁语料分析具有重要价值的方法论收益。

Abstract: While the indirect evidence suggests that already in the early scholastic
period the literary production based on records of oral teaching (so-called
reportationes) was not uncommon, there are very few sources commenting on the
practice. This paper details the design of a study applying stylometric
techniques of authorship attribution to a collection developed from
reportationes -- Stephen Langton's Quaestiones Theologiae -- aiming to uncover
layers of editorial work and thus validate some hypotheses regarding the
collection's formation. Following Camps, Cl\'erice, and Pinche (2021), I
discuss the implementation of an HTR pipeline and stylometric analysis based on
the most frequent words, POS tags, and pseudo-affixes. The proposed study will
offer two methodological gains relevant to computational research on the
scholastic tradition: it will directly compare performance on manually composed
and automatically extracted data, and it will test the validity of
transformer-based OCR and automated transcription alignment for workflows
applied to scholastic Latin corpora. If successful, this study will provide an
easily reusable template for the exploratory analysis of collaborative literary
production stemming from medieval universities.

</details>


### [218] [Word Meanings in Transformer Language Models](https://arxiv.org/abs/2508.12863)
*Jumbly Grindrod,Peter Grindrod*

Main category: cs.CL

TL;DR: 研究分析了RoBERTa-base模型词意的表示方式及其词汇表存储功能。


<details>
  <summary>Details</summary>
Motivation: 探讨transformer语言模型是否像人类词汇表一样，对每个单词存储和处理语义信息。

Method: 对RoBERTa-base的token嵌入空间进行k-means聚类，形成200个类别，并通过手动检查与心理语言学指标（如情感价值等）测试聚类的语义敏感性。

Result: 发现token嵌入空间能够很好地编码多种语义信息，反驳了对LLMs语义信息处理的部分否定性观点。

Conclusion: Transformer模型的嵌入空间具有语义信息存储能力，证实其在意义处理上的潜力。

Abstract: We investigate how word meanings are represented in the transformer language
models. Specifically, we focus on whether transformer models employ something
analogous to a lexical store - where each word has an entry that contains
semantic information. To do this, we extracted the token embedding space of
RoBERTa-base and k-means clustered it into 200 clusters. In our first study, we
then manually inspected the resultant clusters to consider whether they are
sensitive to semantic information. In our second study, we tested whether the
clusters are sensitive to five psycholinguistic measures: valence,
concreteness, iconicity, taboo, and age of acquisition. Overall, our findings
were very positive - there is a wide variety of semantic information encoded
within the token embedding space. This serves to rule out certain "meaning
eliminativist" hypotheses about how transformer LLMs process semantic
information.

</details>


### [219] [An LLM Agent-Based Complex Semantic Table Annotation Approach](https://arxiv.org/abs/2508.12868)
*Yilin Geng,Shujing Wang,Chuan Wang,Keqing He,Yanfei Lv,Ying Wang,Zaiwen Feng,Xiaoying Bai*

Main category: cs.CL

TL;DR: 本文探讨了语义表格标注中的挑战，并提出了一种基于LLM的解决方案，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 复杂表格在语义标注中表现出诸如列名称语义丢失、严格的本体层次要求、同音词、拼写错误等问题，亟需更高效的方法解决这些问题。

Method: 提出了一种基于LLM的代理方法，使用ReAct框架设计五种外部工具，以动态选择适合的标注策略。

Result: 在Tough Tables和BiodivTab数据集上显著优于现有方法，且通过Levenshtein距离减少了重复标注，降低了70%时间成本和60%的LLM令牌使用量。

Conclusion: 方法不仅提高了语义表格标注的准确性，还在效率和成本上表现出色，具有广泛的实用价值。

Abstract: The Semantic Table Annotation (STA) task, which includes Column Type
Annotation (CTA) and Cell Entity Annotation (CEA), maps table contents to
ontology entities and plays important roles in various semantic applications.
However, complex tables often pose challenges such as semantic loss of column
names or cell values, strict ontological hierarchy requirements, homonyms,
spelling errors, and abbreviations, which hinder annotation accuracy. To
address these issues, this paper proposes an LLM-based agent approach for CTA
and CEA. We design and implement five external tools with tailored prompts
based on the ReAct framework, enabling the STA agent to dynamically select
suitable annotation strategies depending on table characteristics. Experiments
are conducted on the Tough Tables and BiodivTab datasets from the SemTab
challenge, which contain the aforementioned challenges. Our method outperforms
existing approaches across various metrics. Furthermore, by leveraging
Levenshtein distance to reduce redundant annotations, we achieve a 70%
reduction in time costs and a 60% reduction in LLM token usage, providing an
efficient and cost-effective solution for STA.

</details>


### [220] [A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models](https://arxiv.org/abs/2508.12903)
*Jinyi Han,Xinyi Wang,Haiquan Zhao,Tingyun li,Zishang Jiang,Sihang Jiang,Jiaqing Liang,Xin Lin,Weikang Zhou,Zeye Sun,Fei Yu,Yanghua Xiao*

Main category: cs.CL

TL;DR: 提出ProActive Self-Refinement (PASR)方法，支持大型语言模型在生成过程中主动调整输出，显著提升问题解决性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自我改进方法大多基于固定迭代次数的被动过程，限制了根据上下文动态调整的能力。

Method: 提出PASR方法，允许大型语言模型根据内部状态和上下文动态决定是否、何时及如何改进输出。

Result: 在10个任务上的实验表明，PASR显著提升性能，尤其在Qwen3-8B模型上降低41.6%的平均token消耗，同时准确率提高8.2%。

Conclusion: PASR方法有效优化了语言模型的生成能力，提升问题解决效率并减少资源消耗。

Abstract: Recent advances in self-refinement have demonstrated significant potential
for improving the outputs of large language models (LLMs) through iterative
refinement. However, most existing self-refinement methods rely on a reactive
process with a fixed number of iterations, making it difficult to determine the
optimal timing and content of refinement based on the evolving generation
context. Inspired by the way humans dynamically refine their thoughts during
execution, we propose ProActive Self-Refinement (PASR), a novel method that
enables LLMs to refine their outputs during the generation process. Unlike
methods that regenerate entire responses, PASR proactively decides whether,
when, and how to refine based on the model's internal state and evolving
context. We conduct extensive experiments on a diverse set of 10 tasks to
evaluate the effectiveness of PASR. Experimental results show that PASR
significantly enhances problem-solving performance. In particular, on Qwen3-8B,
PASR reduces average token consumption by 41.6 percent compared to standard
generation, while also achieving an 8.2 percent improvement in accuracy. Our
code and all baselines used in the paper are available in the GitHub.

</details>


### [221] [Analyzing Information Sharing and Coordination in Multi-Agent Planning](https://arxiv.org/abs/2508.12981)
*Tianyue Ou,Saujas Vaduguru,Daniel Fried*

Main category: cs.CL

TL;DR: 论文探讨了在旅游规划任务中，构建基于大语言模型（LLM）的多智能体系统（MAS）的表现。研究中引入了“共享笔记本”和“协调智能体”机制，发现组合运用可显著提高系统在复杂任务中的精确度和成功率。


<details>
  <summary>Details</summary>
Motivation: 动机在于探讨大语言模型在复杂的、多约束的长期规划任务中的表现，同时寻找提升多智能体系统效率的机制。

Method: 使用旅游规划任务作为研究平台，设计一个基于LLM的多智能体系统，结合共享笔记本用于信息共享，并引入一个协调智能体增强智能体之间的对话协调。

Result: 引入共享笔记本可减少18%的错误，协调智能体可进一步减少13.5%的错误，组合机制使得系统在TravelPlanner基准测试中的通过率提升到25%，比单智能体系统高出17.5%。

Conclusion: 结果显示，结构化信息共享和“反思式”协调对提升基于LLM的多智能体系统在复杂任务中的表现非常重要。

Abstract: Multi-agent systems (MASs) have pushed the boundaries of large language model
(LLM) agents in domains such as web research and software engineering. However,
long-horizon, multi-constraint planning tasks involve conditioning on detailed
information and satisfying complex interdependent constraints, which can pose a
challenge for these systems. In this study, we construct an LLM-based MAS for a
travel planning task which is representative of these challenges. We evaluate
the impact of a notebook to facilitate information sharing, and evaluate an
orchestrator agent to improve coordination in free form conversation between
agents. We find that the notebook reduces errors due to hallucinated details by
18%, while an orchestrator directs the MAS to focus on and further reduce
errors by up to 13.5% within focused sub-areas. Combining both mechanisms
achieves a 25% final pass rate on the TravelPlanner benchmark, a 17.5% absolute
improvement over the single-agent baseline's 7.5% pass rate. These results
highlight the potential of structured information sharing and reflective
orchestration as key components in MASs for long horizon planning with LLMs.

</details>


### [222] [WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents](https://arxiv.org/abs/2508.13024)
*Ralph Peeters,Aaron Steiner,Luca Schwarz,Julian Yuya Caspary,Christian Bizer*

Main category: cs.CL

TL;DR: 本文介绍了WebMall，一种用于评估web代理在比价购物任务中效果与效率的基准测试，涉及91项跨店任务，并通过八种基线代理进行评估。


<details>
  <summary>Details</summary>
Motivation: 研究长时间运行的web任务自动化，例如跨网店搜索产品并下单，以优化电子商务中的导航和效率。

Method: 提出WebMall作为基准测试，包括四个带真实产品报价的模拟网店，任务涉及比价、购物车操作到复杂任务如模糊搜索。

Result: 基线代理在基础与高级任务集的完成率分别为75%和53%，F1得分分别为87%与63%。

Conclusion: WebMall为web代理表现评估提供了一个开放平台，推动电子商务场景中导航和推理能力的研究进步。

Abstract: LLM-based web agents have the potential to automate long-running web tasks,
such as finding offers for specific products in multiple online shops and
subsequently ordering the cheapest products that meet the users needs. This
paper introduces WebMall, a multi-shop online shopping benchmark for evaluating
the effectiveness and efficiency of web agents for comparison-shopping. WebMall
consists of four simulated online shops populated with authentic product offers
sourced from the Common Crawl, alongside a suite of 91 cross-shop tasks. These
tasks include basic tasks such as finding specific products in multiple shops,
performing price comparisons, adding items to the shopping cart, and completing
checkout. Advanced tasks involve searching for products based on vague
requirements, identifying suitable substitutes, and finding compatible
products. Compared to existing e-commerce benchmarks, such as WebShop or
ShoppingBench, WebMall introduces comparison-shopping tasks across multiple
shops. Furthermore, the product offers are more heterogeneous, as they
originate from hundreds of distinct real-world shops. The tasks in WebMall
require longer interaction trajectories than those in WebShop, while remaining
representative of real-world shopping behaviors. We evaluate eight baseline
agents on WebMall, varying in observation modality, memory utilization, and
underlying large language model (GPT 4.1 and Claude Sonnet 4). The
best-performing configurations achieve completion rates of 75% and 53%, and F1
scores of 87% and 63%, on the basic and advanced task sets, respectively.
WebMall is publicly released to facilitate research on web agents and to
promote advancements in navigation, reasoning, and efficiency within e-commerce
scenarios.

</details>


### [223] [Integrating Feedback Loss from Bi-modal Sarcasm Detector for Sarcastic Speech Synthesis](https://arxiv.org/abs/2508.13028)
*Zhu Li,Yuqing Zhang,Xiyuan Gao,Devraj Raghuvanshi,Nagendra Kumar,Shekhar Nayak,Matt Coler*

Main category: cs.CL

TL;DR: 该研究提出通过整合双模态讽刺检测模型的反馈损失到文本转语音(TTS)训练中，提升讽刺语音的生成质量。


<details>
  <summary>Details</summary>
Motivation: 讽刺语音合成对于娱乐和人机交互等领域至关重要，但因讽刺语调复杂且语料数据稀缺，合成讽刺语音存在挑战。

Method: 通过转移学习，先在多样化语音数据上微调，后在讽刺语音数据上进一步优化，并在训练中融入双模态讽刺检测模型的反馈损失，提高讽刺语音生成能力。

Result: 客观和主观评估表明，提出的方法改善了合成语音的质量、自然性以及讽刺表达能力。

Conclusion: 本方法能够有效生成具有讽刺特质的自然语音，对提升语音合成系统在讽刺语境下的表现具有显著作用。

Abstract: Sarcastic speech synthesis, which involves generating speech that effectively
conveys sarcasm, is essential for enhancing natural interactions in
applications such as entertainment and human-computer interaction. However,
synthesizing sarcastic speech remains a challenge due to the nuanced prosody
that characterizes sarcasm, as well as the limited availability of annotated
sarcastic speech data. To address these challenges, this study introduces a
novel approach that integrates feedback loss from a bi-modal sarcasm detection
model into the TTS training process, enhancing the model's ability to capture
and convey sarcasm. In addition, by leveraging transfer learning, a speech
synthesis model pre-trained on read speech undergoes a two-stage fine-tuning
process. First, it is fine-tuned on a diverse dataset encompassing various
speech styles, including sarcastic speech. In the second stage, the model is
further refined using a dataset focused specifically on sarcastic speech,
enhancing its ability to generate sarcasm-aware speech. Objective and
subjective evaluations demonstrate that our proposed methods improve the
quality, naturalness, and sarcasm-awareness of synthesized speech.

</details>


### [224] [Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction](https://arxiv.org/abs/2508.13037)
*Xinhe Li,Jiajun Liu,Peng Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为LoRID的新方法，通过多LoRA交互进行数学推理蒸馏，从而提升小型语言模型的数学推理能力，并在GSM8K数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决小型语言模型（SLMs）在数学推理能力差的问题，这种问题当前通常通过使用大型语言模型（LLMs）生成海量训练数据来处理。

Method: 提出基于多LoRA交互的数学推理蒸馏方法（LoRID），借鉴心理学中的系统1和系统2思维，结合知识生成器（KG）和深层推理器（DR），通过直觉推理模块（IR）生成的链式推理并迭代优化。

Result: 在GSM8K数据集上的五个基准模型中，LoRID的表现超越了次优方法2.3%、16.1%、2.4%、12.3%、1.8%的准确率，达到了最先进的性能水平。

Conclusion: LoRID通过模拟人类思维的两种模式和多模块交互，有效提升SLMs的数学推理能力，并在多个数据集上取得了显著进展。

Abstract: Recent studies have demonstrated that Large Language Models (LLMs) have
strong mathematical reasoning abilities but rely on hundreds of billions of
parameters. To tackle the challenge of poor reasoning in Small Language Models
(SLMs), existing methods typically leverage LLMs to generate massive amounts of
data for cramming training. In psychology, they are akin to System 1 thinking,
which resolves reasoning problems rapidly based on experience and intuition.
However, human learning also requires System 2 thinking, where knowledge is
first acquired and then reinforced through practice. Inspired by such two
distinct modes of thinking, we propose a novel method based on the multi-LoRA
Interaction for mathematical reasoning Distillation (LoRID). First, we input
the question and reasoning of each sample into an LLM to create
knowledge-enhanced datasets. Subsequently, we train a LoRA block on the student
model as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughts
for problem-solving. Then, to imitate System 2 thinking, we train the Knowledge
Generator (KG) and Deep Reasoner (DR), respectively. The former outputs only
knowledge after receiving problems, while the latter uses that knowledge to
perform reasoning. Finally, to address the randomness in the generation of IR
and DR, we evaluate whether their outputs are consistent, and the inference
process needs to be iterated if not. This step can enhance the mathematical
reasoning ability of SLMs through mutual feedback. Experimental results show
that LoRID achieves state-of-the-art performance, especially on the GSM8K
dataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%,
12.3%, and 1.8% accuracy across the five base models, respectively.

</details>


### [225] [Büyük Dil Modelleri için TR-MMLU Benchmarkı: Performans Değerlendirmesi, Zorluklar ve İyileştirme Fırsatları](https://arxiv.org/abs/2508.13044)
*M. Ali Bayram,Ali Arda Fincan,Ahmet Semih Gümüş,Banu Diri,Savaş Yıldırım,Öner Aytaş*

Main category: cs.CL

TL;DR: 该论文提出了一个新的评测基准TR-MMLU，用于评估大规模语言模型（LLMs）在土耳其语言处理方面的能力。


<details>
  <summary>Details</summary>
Motivation: 目前评估语言模型在资源受限语言如土耳其语中的表现存在难度，亟需一个专门的评测框架。

Method: 构建了一个名为TR-MMLU的基准，包含62个部分、6,200道多项选择题，来源于土耳其教育系统。利用该基准评估了多种最先进的语言模型。

Result: 通过TR-MMLU分析表明现有LLMs在土耳其语处理上仍有改进空间。

Conclusion: TR-MMLU为土耳其自然语言处理研究设定了新标准，助力未来创新。

Abstract: Language models have made significant advancements in understanding and
generating human language, achieving remarkable success in various
applications. However, evaluating these models remains a challenge,
particularly for resource-limited languages like Turkish. To address this
issue, we introduce the Turkish MMLU (TR-MMLU) benchmark, a comprehensive
evaluation framework designed to assess the linguistic and conceptual
capabilities of large language models (LLMs) in Turkish. TR-MMLU is based on a
meticulously curated dataset comprising 6,200 multiple-choice questions across
62 sections within the Turkish education system. This benchmark provides a
standard framework for Turkish NLP research, enabling detailed analyses of
LLMs' capabilities in processing Turkish text. In this study, we evaluated
state-of-the-art LLMs on TR-MMLU, highlighting areas for improvement in model
design. TR-MMLU sets a new standard for advancing Turkish NLP research and
inspiring future innovations.

</details>


### [226] [Doğal Dil İşlemede Tokenizasyon Standartları ve Ölçümü: Türkçe Üzerinden Büyük Dil Modellerinin Karşılaştırmalı Analizi](https://arxiv.org/abs/2508.13058)
*M. Ali Bayram,Ali Arda Fincan,Ahmet Semih Gümüş,Sercan Karakaş,Banu Diri,Savaş Yıldırım*

Main category: cs.CL

TL;DR: 提出了一种新的评估框架，用于解决形态复杂和低资源语言（如土耳其语）的分词问题，并提出多种新指标来衡量分词器效果。


<details>
  <summary>Details</summary>
Motivation: 分词过程会显著影响大语言模型捕捉语言和语义细微差别的能力，但在处理形态复杂和低资源语言时，现有分词工具存在不足。

Method: 利用包含土耳其教育系统的 6200 道多项选择题的土耳其 MMLU 数据集（TR-MMLU），从词汇量、分词数量、处理时间、语言特定分词百分比(%TR)、分词纯度(%Pure)等方面评估分词器性能。

Result: 发现语言特定分词百分比与下游性能（如 MMLU 分数）相关性更强；仅增加模型参数不能显著提升语言性能，强调定制化分词方法的重要性。

Conclusion: 所提出的框架为形态复杂语言制定了稳健且实用的分词标准。

Abstract: Tokenization is a fundamental preprocessing step in Natural Language
Processing (NLP), significantly impacting the capability of large language
models (LLMs) to capture linguistic and semantic nuances. This study introduces
a novel evaluation framework addressing tokenization challenges specific to
morphologically-rich and low-resource languages such as Turkish. Utilizing the
Turkish MMLU (TR-MMLU) dataset, comprising 6,200 multiple-choice questions from
the Turkish education system, we assessed tokenizers based on vocabulary size,
token count, processing time, language-specific token percentages (\%TR), and
token purity (\%Pure). These newly proposed metrics measure how effectively
tokenizers preserve linguistic structures. Our analysis reveals that
language-specific token percentages exhibit a stronger correlation with
downstream performance (e.g., MMLU scores) than token purity. Furthermore,
increasing model parameters alone does not necessarily enhance linguistic
performance, underscoring the importance of tailored, language-specific
tokenization methods. The proposed framework establishes robust and practical
tokenization standards for morphologically complex languages.

</details>


### [227] [Evaluating ASR robustness to spontaneous speech errors: A study of WhisperX using a Speech Error Database](https://arxiv.org/abs/2508.13060)
*John Alderete,Macarious Kin Fung Hui,Aanchan Mohan*

Main category: cs.CL

TL;DR: SFUSED是一个用于语言学和心理语言学研究的公开语音错误数据库，它包含系统标注的自发英语语音错误，并测试了其对语音识别模型评估的适用性。


<details>
  <summary>Details</summary>
Motivation: 开发一个系统化标注的语音错误数据库，用于研究和测试语音识别模型性能。

Method: 提供语音错误数据中的多种维度标注，利用WhisperX模型对5320个单词和音位错误进行转录准确性评估。

Result: 证实了SFUSED作为自动语音识别（ASR）系统诊断工具的有效性。

Conclusion: SFUSED能够为语音识别模型的测试与研究提供有价值的数据和分析工具。

Abstract: The Simon Fraser University Speech Error Database (SFUSED) is a public data
collection developed for linguistic and psycholinguistic research. Here we
demonstrate how its design and annotations can be used to test and evaluate
speech recognition models. The database comprises systematically annotated
speech errors from spontaneous English speech, with each error tagged for
intended and actual error productions. The annotation schema incorporates
multiple classificatory dimensions that are of some value to model assessment,
including linguistic hierarchical level, contextual sensitivity, degraded
words, word corrections, and both word-level and syllable-level error
positioning. To assess the value of these classificatory variables, we
evaluated the transcription accuracy of WhisperX across 5,300 documented word
and phonological errors. This analysis demonstrates the atabase's effectiveness
as a diagnostic tool for ASR system performance.

</details>


### [228] [Reinforced Context Order Recovery for Adaptive Reasoning and Planning](https://arxiv.org/abs/2508.13070)
*Long Ma,Fangwei Zhong,Yizhou Wang*

Main category: cs.CL

TL;DR: 该论文提出现代因果语言模型和离散扩散模型在处理需要灵活生成顺序的问题时存在困难，提出了ReCOR框架以适应性地产生标记顺序，实验显示其在推理和规划任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有模型大多按照固定或随机顺序生成标记，这种方法可能偏离原始生成的逻辑顺序，限制了解决某些问题的能力。

Method: 提出ReCOR框架，这是一种基于强化学习的方法，通过标记预测统计信息自监督学习，动态选择训练和推理过程中的下一个生成标记，适应性调整生成顺序。

Result: 在推理和规划数据集的实验中，ReCOR相较于基线模型表现更佳，有时甚至超过了使用真实生成顺序的监督模型。

Conclusion: ReCOR框架能够从文本数据中提取灵活的生成顺序，有效解决顺序依赖的生成问题，提升了模型的生成效率和精确度。

Abstract: Modern causal language models, followed by rapid developments in discrete
diffusion models, can now produce a wide variety of interesting and useful
content. However, these families of models are predominantly trained to output
tokens with a fixed (left-to-right) or random order, which may deviate from the
logical order in which tokens are generated originally. In this paper, we
observe that current causal and diffusion models encounter difficulties in
problems that require adaptive token generation orders to solve tractably,
which we characterize with the $\mathcal{V}$-information framework. Motivated
by this, we propose Reinforced Context Order Recovery (ReCOR), a
reinforcement-learning-based framework to extract adaptive, data-dependent
token generation orders from text data without annotations. Self-supervised by
token prediction statistics, ReCOR estimates the hardness of predicting every
unfilled token and adaptively selects the next token during both training and
inference. Experiments on challenging reasoning and planning datasets
demonstrate the superior performance of ReCOR compared with baselines,
sometimes outperforming oracle models supervised with the ground-truth order.

</details>


### [229] [DocHPLT: A Massively Multilingual Document-Level Translation Dataset](https://arxiv.org/abs/2508.13079)
*Dayyán O'Brien,Bhavitvya Malik,Ona de Gibert,Pinzhen Chen,Barry Haddow,Jörg Tiedemann*

Main category: cs.CL

TL;DR: 提出了DocHPLT，这是目前最大的公开文档级翻译数据集，包含50种语言与英语的1.24亿对文档，显著提升了低资源语言的翻译效果。


<details>
  <summary>Details</summary>
Motivation: 为了解决文档级翻译资源不足的问题，特别是低资源语言的翻译需求，提供长上下文建模的训练与评估基础设施。

Method: 通过修改现有的网页提取管道，保留完整文档的完整性，并对大型语言模型（LLMs）进行基于该数据集的微调。

Result: 基于DocHPLT微调的LLMs相较于基础的指令调整模型，尤其是在低资源语言翻译任务上表现出显著的性能提升。

Conclusion: DocHPLT为多语言文档级翻译研究提供了重要的开放源数据基础设施，推动了低资源语言的翻译技术进步。

Abstract: Existing document-level machine translation resources are only available for
a handful of languages, mostly high-resourced ones. To facilitate the training
and evaluation of document-level translation and, more broadly, long-context
modeling for global communities, we create DocHPLT, the largest publicly
available document-level translation dataset to date. It contains 124 million
aligned document pairs across 50 languages paired with English, comprising 4.26
billion sentences, with further possibility to provide 2500 bonus pairs not
involving English. Unlike previous reconstruction-based approaches that piece
together documents from sentence-level data, we modify an existing web
extraction pipeline to preserve complete document integrity from the source,
retaining all content including unaligned portions. After our preliminary
experiments identify the optimal training context strategy for document-level
translation, we demonstrate that LLMs fine-tuned on DocHPLT substantially
outperform off-the-shelf instruction-tuned baselines, with particularly
dramatic improvements for under-resourced languages. We open-source the dataset
under a permissive license, providing essential infrastructure for advancing
multilingual document-level translation.

</details>


### [230] [All for law and law for all: Adaptive RAG Pipeline for Legal Research](https://arxiv.org/abs/2508.13107)
*Figarri Keisha,Prince Singh,Pallavi,Dion Fernandes,Aravindh Manivannan,Ilham Wicaksono,Faisal Ahmad*

Main category: cs.CL

TL;DR: 该论文提出了一个优化后的检索增强生成（RAG）系统，用于法律领域，显著提升了检索性能和生成结果的真实性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型输出存在虚假的问题，而法律领域对高可信度和可追溯性要求高，因此需要一个能够结合检索与生成的系统。

Method: 通过设计端到端的RAG管道，采用上下文感知的查询翻译器、基于SBERT和GTE的开源检索策略以及结合RAGAS、BERTScore-F1和ROUGE-Recall的评估框架，提升系统表现。

Result: 开源检索策略比专有方法在Recall和Precision上都显著提升，同时定制化法律提示词生成的答案也更加真实且上下文相关。

Conclusion: 任务感知的组件级优化设计可以为法律研究提供可信、可重复且高效的RAG系统，是法律领域研究助理系统开发的一大进步。

Abstract: Retrieval-Augmented Generation (RAG) mitigates hallucinations by grounding
large language model outputs in cited sources, a capability that is especially
critical in the legal domain. We present an end-to-end RAG pipeline that
revisits and extends the LegalBenchRAG baseline with three targeted
enhancements: (i) a context-aware query translator that disentangles document
references from natural-language questions and adapts retrieval depth and
response style based on expertise and specificity, (ii) open-source retrieval
strategies using SBERT and GTE embeddings that achieve substantial performance
gains (improving Recall@K by 30-95\% and Precision@K by $\sim$2.5$\times$ for
$K>4$) while remaining cost-efficient, and (iii) a comprehensive evaluation and
generation framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall to
assess semantic alignment and faithfulness across models and prompt designs.
Our results show that carefully designed open-source pipelines can rival or
outperform proprietary approaches in retrieval quality, while a custom
legal-grounded prompt consistently produces more faithful and contextually
relevant answers than baseline prompting. Taken together, these contributions
demonstrate the potential of task-aware, component-level tuning to deliver
legally grounded, reproducible, and cost-effective RAG systems for legal
research assistance.

</details>


### [231] [AutoBnB-RAG: Enhancing Multi-Agent Incident Response with Retrieval-Augmented Generation](https://arxiv.org/abs/2508.13118)
*Zefang Liu,Arman Anwar*

Main category: cs.CL

TL;DR: 文章提出了一种名为AutoBnB-RAG的框架，用于通过检索增强生成（RAG）改进语言模型在网络安全中的事件响应能力。


<details>
  <summary>Details</summary>
Motivation: 在网络安全的事件响应中，大语言模型通常缺乏外部知识，限制了其推理能力。本研究旨在通过加入检索机制解决这一问题。

Method: 基于Backdoors & Breaches (B&B)桌游环境，文章设计了支持检索增强生成（RAG）的多智能体系统AutoBnB-RAG，并测试了两种检索场景（技术文档和事件报告），同时评估了多种团队配置的表现。

Result: 引入RAG机制后，在不同组织模型中，决策质量和成功率都有所提升，并能够重建复杂多阶段攻击情景。

Conclusion: 通过整合检索机制，AutoBnB-RAG框架提升了网络安全事件响应中的决策质量，展示了其在实际应用中的潜力。

Abstract: Incident response (IR) requires fast, coordinated, and well-informed
decision-making to contain and mitigate cyber threats. While large language
models (LLMs) have shown promise as autonomous agents in simulated IR settings,
their reasoning is often limited by a lack of access to external knowledge. In
this work, we present AutoBnB-RAG, an extension of the AutoBnB framework that
incorporates retrieval-augmented generation (RAG) into multi-agent incident
response simulations. Built on the Backdoors & Breaches (B&B) tabletop game
environment, AutoBnB-RAG enables agents to issue retrieval queries and
incorporate external evidence during collaborative investigations. We introduce
two retrieval settings: one grounded in curated technical documentation
(RAG-Wiki), and another using narrative-style incident reports (RAG-News). We
evaluate performance across eight team structures, including newly introduced
argumentative configurations designed to promote critical reasoning. To
validate practical utility, we also simulate real-world cyber incidents based
on public breach reports, demonstrating AutoBnB-RAG's ability to reconstruct
complex multi-stage attacks. Our results show that retrieval augmentation
improves decision quality and success rates across diverse organizational
models. This work demonstrates the value of integrating retrieval mechanisms
into LLM-based multi-agent systems for cybersecurity decision-making.

</details>


### [232] [Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries](https://arxiv.org/abs/2508.13124)
*Kawin Mayilvaghanan,Siddhant Gupta,Ayush Kumar*

Main category: cs.CL

TL;DR: 本文提出了一种名为BlindSpot的框架，用于识别和量化大型语言模型（LLMs）生成的摘要中可能存在的运营偏差。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在联络中心生成大量摘要，但是否存在系统性偏差尚未明确，尤其是与运营相关的偏差仍未被探索。

Method: 提出BlindSpot框架，基于运营偏差的15个维度分类，结合两个偏差量化指标（可信度差距和覆盖率），分析由20种不同LLMs生成的2500个通话摘要。

Result: 所有模型中均存在系统性偏差，且这一现象与模型的规模和类型无关。

Conclusion: 尽管摘要看似高质量，但存在普遍性的偏差，表明需要更多对偏差的识别和缓解研究。

Abstract: Abstractive summarization is a core application in contact centers, where
Large Language Models (LLMs) generate millions of summaries of call transcripts
daily. Despite their apparent quality, it remains unclear whether LLMs
systematically under- or over-attend to specific aspects of the transcript,
potentially introducing biases in the generated summary. While prior work has
examined social and positional biases, the specific forms of bias pertinent to
contact center operations - which we term Operational Bias - have remained
unexplored. To address this gap, we introduce BlindSpot, a framework built upon
a taxonomy of 15 operational bias dimensions (e.g., disfluency, speaker, topic)
for the identification and quantification of these biases. BlindSpot leverages
an LLM as a zero-shot classifier to derive categorical distributions for each
bias dimension in a pair of transcript and its summary. The bias is then
quantified using two metrics: Fidelity Gap (the JS Divergence between
distributions) and Coverage (the percentage of source labels omitted). Using
BlindSpot, we conducted an empirical study with 2500 real call transcripts and
their summaries generated by 20 LLMs of varying scales and families (e.g., GPT,
Llama, Claude). Our analysis reveals that biases are systemic and present
across all evaluated models, regardless of size or family.

</details>


### [233] [MuDRiC: Multi-Dialect Reasoning for Arabic Commonsense Validation](https://arxiv.org/abs/2508.13130)
*Kareem Elozeiri,Mervat Abassy,Preslav Nakov,Yuxia Wang*

Main category: cs.CL

TL;DR: 本文提出了MuDRiC，一个涵盖多种方言的阿拉伯语常识数据集，并引入一种新方法，利用图卷积网络改进阿拉伯语常识推理，效果显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前阿拉伯语常识验证任务研究不足，特别是多样化方言的代表性较低，需填补这一空白。

Method: 本文引入一个多方言的阿拉伯语常识数据集MuDRiC，并使用基于图卷积网络的方法改进语义关系建模，以提高常识验证能力。

Result: 实验表明，该方法在阿拉伯语常识验证任务中表现优于现有方法。

Conclusion: 本文首次发布了阿拉伯语多方言常识数据集，并提出了一种新方法，为复杂语言变体的自然语言理解奠定了基础。

Abstract: Commonsense validation evaluates whether a sentence aligns with everyday
human understanding, a critical capability for developing robust natural
language understanding systems. While substantial progress has been made in
English, the task remains underexplored in Arabic, particularly given its rich
linguistic diversity. Existing Arabic resources have primarily focused on
Modern Standard Arabic (MSA), leaving regional dialects underrepresented
despite their prevalence in spoken contexts. To bridge this gap, we present two
key contributions: (i) we introduce MuDRiC, an extended Arabic commonsense
dataset incorporating multiple dialects, and (ii) a novel method adapting Graph
Convolutional Networks (GCNs) to Arabic commonsense reasoning, which enhances
semantic relationship modeling for improved commonsense validation. Our
experimental results demonstrate that this approach achieves superior
performance in Arabic commonsense validation. Our work enhances Arabic natural
language understanding by providing both a foundational dataset and a novel
method for handling its complex variations. To the best of our knowledge, we
release the first Arabic multi-dialect commonsense reasoning dataset.

</details>


### [234] [Improving Detection of Watermarked Language Models](https://arxiv.org/abs/2508.13131)
*Dara Bahri,John Wieting*

Main category: cs.CL

TL;DR: 该研究探讨了如何结合水印检测与非水印检测，以提升对大型语言模型生成内容的识别性能。


<details>
  <summary>Details</summary>
Motivation: 目前水印依赖语言模型的熵值及输入提示集，但对于经过后期微调（如指令调优或人类反馈强化学习）的模型，其熵值有限，从而导致水印检测的效能受限。

Method: 研究结合使用水印检测器和非水印检测器的混合方法，并测试多种方案在不同实验条件下的性能。

Result: 通过将水印检测与非水印检测结合，观察到在检测性能上的显著提升。

Conclusion: 结合水印与非水印检测的方法是一种有效的提升检测效能的手段，尤其在复杂条件下表现出色。

Abstract: Watermarking has recently emerged as an effective strategy for detecting the
generations of large language models (LLMs). The strength of a watermark
typically depends strongly on the entropy afforded by the language model and
the set of input prompts. However, entropy can be quite limited in practice,
especially for models that are post-trained, for example via instruction tuning
or reinforcement learning from human feedback (RLHF), which makes detection
based on watermarking alone challenging. In this work, we investigate whether
detection can be improved by combining watermark detectors with non-watermark
ones. We explore a number of hybrid schemes that combine the two, observing
performance gains over either class of detector under a wide range of
experimental conditions.

</details>


### [235] [OptimalThinkingBench: Evaluating Over and Underthinking in LLMs](https://arxiv.org/abs/2508.13141)
*Pranjal Aggarwal,Seungone Kim,Jack Lanchantin,Sean Welleck,Jason Weston,Ilia Kulikov,Swarnadeep Saha*

Main category: cs.CL

TL;DR: LLMs在复杂任务中表现优异但计算耗费大，而在简单任务中易过度思考；非思考型LLMs效率高但在复杂推理问题上容易表现不足。作者提出了OptimalThinkingBench，评估和促进模型的性能与效率平衡性。


<details>
  <summary>Details</summary>
Motivation: 现有的思考型和非思考型LLMs在性能与效率上存在相互妥协，用户需自行选择适配模型，对优化计算和推理的统一模型需求迫切。

Method: 提出了OptimalThinkingBench，一个综合性的基准测试，包含OverthinkingBench和UnderthinkingBench两部分，通过创新指标评估33种模型在平衡思维与效率表现上的能力。

Result: 没有模型在该基准测试上表现出理想的思考能力，思考型模型容易在简单任务上过度计算，而非思考型模型在复杂推理任务上表现不足。

Conclusion: 当前方法难以在过度思考和不足思考之间实现平衡，亟需开发更高效统一的思考模型以提升性能和效率。

Abstract: Thinking LLMs solve complex tasks at the expense of increased compute and
overthinking on simpler problems, while non-thinking LLMs are faster and
cheaper but underthink on harder reasoning problems. This has led to the
development of separate thinking and non-thinking LLM variants, leaving the
onus of selecting the optimal model for each query on the end user. In this
work, we introduce OptimalThinkingBench, a unified benchmark that jointly
evaluates overthinking and underthinking in LLMs and also encourages the
development of optimally-thinking models that balance performance and
efficiency. Our benchmark comprises two sub-benchmarks: OverthinkingBench,
featuring simple queries in 72 domains, and UnderthinkingBench, containing 11
challenging reasoning tasks. Using novel thinking-adjusted accuracy metrics, we
perform extensive evaluation of 33 different thinking and non-thinking models
and show that no model is able to optimally think on our benchmark. Thinking
models often overthink for hundreds of tokens on the simplest user queries
without improving performance. In contrast, large non-thinking models
underthink, often falling short of much smaller thinking models. We further
explore several methods to encourage optimal thinking, but find that these
approaches often improve on one sub-benchmark at the expense of the other,
highlighting the need for better unified and optimal models in the future.

</details>


### [236] [Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation](https://arxiv.org/abs/2508.13144)
*David Heineman,Valentin Hofmann,Ian Magnusson,Yuling Gu,Noah A. Smith,Hannaneh Hajishirzi,Kyle Lo,Jesse Dodge*

Main category: cs.CL

TL;DR: 本研究分析了使基准更可靠的属性，并设计了提高评估基准质量的干预方法。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型的开发成本高昂，需要小样本实验作出决策，而现有的多任务基准评价方法可能不够可靠。

Method: 提出两项关键指标：信号（区分模型优劣的能力）和噪声（训练间随机变异的敏感性），并设计了三种干预措施，包括改用更佳信号与噪声的度量方法、过滤任务噪声及对模型中间检查点进行平均。

Result: 验证了信噪比更好的基准更可靠，噪声更低的基准预测误差更小，同时证明了干预措施的有效性。

Conclusion: 建议新设计或选择基准时，应注重高信号、低噪声，并提供了针对信号噪声优化的实用策略。

Abstract: Developing large language models is expensive and involves making decisions
with small experiments, typically by evaluating on large, multi-task evaluation
suites. In this work, we analyze specific properties which make a benchmark
more reliable for such decisions, and interventions to design higher-quality
evaluation benchmarks. We introduce two key metrics that show differences in
current benchmarks: signal, a benchmark's ability to separate better models
from worse models, and noise, a benchmark's sensitivity to random variability
between training steps. We demonstrate that benchmarks with a better
signal-to-noise ratio are more reliable when making decisions at small scale,
and those with less noise have lower scaling law prediction error. These
results suggest that improving signal or noise will lead to more useful
benchmarks, so we introduce three interventions designed to directly affect
signal or noise. For example, we propose that switching to a metric that has
better signal and noise (e.g., perplexity rather than accuracy) leads to better
reliability and improved scaling law error. We also find that filtering noisy
subtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable
multi-task evaluations. We also find that averaging the output of a model's
intermediate checkpoints to reduce noise leads to consistent improvements. We
conclude by recommending that those creating new benchmarks, or selecting which
existing benchmarks to use, aim for high signal and low noise. We use 30
benchmarks for these experiments, and 375 open-weight language models from 60M
to 32B parameters, resulting in a new, publicly available dataset of 900K
evaluation benchmark results, totaling 200M instances.

</details>


### [237] [RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns](https://arxiv.org/abs/2508.13152)
*Xin Chen,Junchao Wu,Shu Yang,Runzhe Zhan,Zeyu Wu,Ziyang Luo,Di Wang,Min Yang,Lidia S. Chao,Derek F. Wong*

Main category: cs.CL

TL;DR: 该论文提出了一种名为RepreGuard的统计检测方法，通过利用大型语言模型的内部表示差异来区分生成文档和人类书写文档，并在实验中展示了其在内部及外部分布场景中的高效表现。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现有文本生成检测方法在分布外场景（OOD）中的鲁棒性不足问题，并提出利用LLM内在表示的潜力。

Method: 基于内部表示的激活模式差异，使用代理模型收集特征并提取区分LGT和HWT的特征，再通过计算投影得分及预设阈值进行分类。

Result: RepreGuard在平均94.92% AUROC的性能下优于所有基准方法，展示了其对多种文本规模与主流攻击的鲁棒性。

Conclusion: 通过验证LLM内部表示特性，该方法有效提升了生成文档检测的准确性及泛化能力，有助于构建更安全可靠的AI系统。

Abstract: Detecting content generated by large language models (LLMs) is crucial for
preventing misuse and building trustworthy AI systems. Although existing
detection methods perform well, their robustness in out-of-distribution (OOD)
scenarios is still lacking. In this paper, we hypothesize that, compared to
features used by existing detection methods, the internal representations of
LLMs contain more comprehensive and raw features that can more effectively
capture and distinguish the statistical pattern differences between
LLM-generated texts (LGT) and human-written texts (HWT). We validated this
hypothesis across different LLMs and observed significant differences in neural
activation patterns when processing these two types of texts. Based on this, we
propose RepreGuard, an efficient statistics-based detection method.
Specifically, we first employ a surrogate model to collect representation of
LGT and HWT, and extract the distinct activation feature that can better
identify LGT. We can classify the text by calculating the projection score of
the text representations along this feature direction and comparing with a
precomputed threshold. Experimental results show that RepreGuard outperforms
all baselines with average 94.92% AUROC on both in-distribution (ID) and OOD
scenarios, while also demonstrating robust resilience to various text sizes and
mainstream attacks. Data and code are publicly available at:
https://github.com/NLP2CT/RepreGuard

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [238] [Finite Automata Extraction: Low-data World Model Learning as Programs from Gameplay Video](https://arxiv.org/abs/2508.11836)
*Dave Goel,Matthew Guzdial,Anurag Sarkar*

Main category: cs.AI

TL;DR: 提出了一种新的方法，从游戏视频中学习神经符号世界模型，使用了一种新的领域特定语言——Retro Coder。


<details>
  <summary>Details</summary>
Motivation: 解决现有世界模型在环境动态可转移性和可解释性上的挑战。

Method: 提出了一种有限自动机提取（FAE）方法，通过分析游戏视频，利用Retro Coder 将其表示为程序，以学习神经符号世界模型。

Result: 与现有方法相比，FAE 模型更加精确，并生成了更通用的代码。

Conclusion: FAE 提出了一种整合神经网络和符号方法的途径，以改进世界模型学习的精确性和通用性。

Abstract: World models are defined as a compressed spatial and temporal learned
representation of an environment. The learned representation is typically a
neural network, making transfer of the learned environment dynamics and
explainability a challenge. In this paper, we propose an approach, Finite
Automata Extraction (FAE), that learns a neuro-symbolic world model from
gameplay video represented as programs in a novel domain-specific language
(DSL): Retro Coder. Compared to prior world model approaches, FAE learns a more
precise model of the environment and more general code than prior DSL-based
approaches.

</details>


### [239] [EvoCut: Strengthening Integer Programs via Evolution-Guided Language Models](https://arxiv.org/abs/2508.11850)
*Milad Yazdani,Mahdi Mostajabdaveh,Samin Aref,Zirui Zhou*

Main category: cs.AI

TL;DR: EvoCut结合大型语言模型与进化搜索，自动生成加速Integer Programming（IP）求解性能的加速约束，比传统方法提升多达57%的优化性能。


<details>
  <summary>Details</summary>
Motivation: 传统IP求解需要手动设计加速约束，需深厚专业知识且未被自动化。

Method: EvoCut通过大型语言模型生成初始约束集合，并采用进化算法（交叉与突变）迭代改进，同时在验证集上评估约束对最优解的保持与分数解的截断能力。

Result: EvoCut在固定时间内减少了17-57%的最优解差距，将达成相同性能的时间缩短至原来的1/4，同时提升限时内解的质量。

Conclusion: EvoCut无需人工干预，能稳定生成、改进和验证能推广至新实例的加速约束，在IP求解实务中展现了优越的自动化性能。

Abstract: Integer programming lies at the heart of crucial combinatorial optimization
tasks but remains challenging due to its NP-hard nature. An effective approach
for practically solving integer programs is the manual design of acceleration
cuts, i.e. inequalities that improve solver performance. However, this creative
process demands deep expertise and is yet to be automated. Our proposed
framework, EvoCut, automates the generation of acceleration cuts by combining
large language models (LLMs) with an evolutionary search. EvoCut (i)
initializes a diverse population of candidate cuts via an LLM-based initializer
agent; (ii) for each cut empirically evaluates both preservation of the optimal
solution and its ability to cut off fractional solutions across a verification
set; and (iii) iteratively refines the population through evolutionary
crossover and mutation agents. We quantify each cut's utility by its relative
reduction in the solver's optimality gap. Our comparisons against standard
integer programming practice show that EvoCut reduces optimality gap by 17-57%
within a fixed time. It obtains the same solutions up to 4 times as fast, and
obtains higher-quality solutions within the same time limit. Requiring no human
expert input, EvoCut reliably generates, improves, and empirically verifies
cuts that generalize to unseen instances. The code is available at
https://github.com/milad1378yz/EvoCut.

</details>


### [240] [LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework](https://arxiv.org/abs/2508.11860)
*Frazier N. Baker,Daniel Adu-Ampratwum,Reza Averly,Botao Yu,Huan Sun,Xia Ning*

Main category: cs.AI

TL;DR: 提出了基于大型语言模型（LLM）的LARC框架，用于满足化学合成中复杂约束条件下的逆合成规划，并取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 在化学中，约束逆合成规划是确定从商用起始材料到目标分子的合成路径的重要过程，但实现起来具有挑战性。

Method: LARC框架通过整合代理性约束评估（将代理作为裁判）到规划过程中，利用工具中的推理反馈来引导和约束路径生成。

Result: 在包含48个任务的精心整理数据集上，LARC实现了72.9%的成功率，大幅超越了LLM基线，接近人类专家水平，同时耗时显著减少。

Conclusion: LARC框架可扩展，能作为化学逆合成的有效工具乃至人类专家的共事科学家，是实现约束逆合成规划的重要一步。

Abstract: Large language model (LLM) agent evaluators leverage specialized tools to
ground the rational decision-making of LLMs, making them well-suited to aid in
scientific discoveries, such as constrained retrosynthesis planning.
Constrained retrosynthesis planning is an essential, yet challenging, process
within chemistry for identifying synthetic routes from commercially available
starting materials to desired target molecules, subject to practical
constraints. Here, we present LARC, the first LLM-based Agentic framework for
Retrosynthesis planning under Constraints. LARC incorporates agentic constraint
evaluation, through an Agent-as-a-Judge, directly into the retrosynthesis
planning process, using agentic feedback grounded in tool-based reasoning to
guide and constrain route generation. We rigorously evaluate LARC on a
carefully curated set of 48 constrained retrosynthesis planning tasks across 3
constraint types. LARC achieves a 72.9% success rate on these tasks, vastly
outperforming LLM baselines and approaching human expert-level success in
substantially less time. The LARC framework is extensible, and serves as a
first step towards an effective agentic tool or a co-scientist to human experts
for constrained retrosynthesis.

</details>


### [241] [QuarkMed Medical Foundation Model Technical Report](https://arxiv.org/abs/2508.11894)
*Ao Li,Bin Yan,Bingfeng Cai,Chenxi Li,Cunzhong Zhao,Fugen Yao,Gaoqiang Liu,Guanjun Jiang,Jian Xu,Liang Dong,Liansheng Sun,Rongshen Zhang,Xiaolei Gui,Xin Liu,Xin Shang,Yao Wu,Yu Cao,Zhenxin Ma,Zhuang Jia*

Main category: cs.AI

TL;DR: QuarkMed 是一款医疗大模型，通过高性能架构和医用强化学习管线，提升医疗任务性能，现已广泛应用。


<details>
  <summary>Details</summary>
Motivation: 医疗任务需要高专业性、准确性和定制能力，当前模型存在局限性，亟需一个可靠的医疗基础模型。

Method: 利用医疗数据处理、基于检索增强生成（RAG）技术和大规模可验证的强化学习管线，构建高性能医疗模型。

Result: 模型在中国医师资格考试中取得了70%的准确率，并在多项医疗基准上展现广泛的泛化能力。

Conclusion: QuarkMed 为用户提供高效、通用的个人医疗 AI 解决方案，已成为数百万用户的医疗辅助选择。

Abstract: Recent advancements in large language models have significantly accelerated
their adoption in healthcare applications, including AI-powered medical
consultations, diagnostic report assistance, and medical search tools. However,
medical tasks often demand highly specialized knowledge, professional accuracy,
and customization capabilities, necessitating a robust and reliable foundation
model. QuarkMed addresses these needs by leveraging curated medical data
processing, medical-content Retrieval-Augmented Generation (RAG), and a
large-scale, verifiable reinforcement learning pipeline to develop a
high-performance medical foundation model. The model achieved 70% accuracy on
the Chinese Medical Licensing Examination, demonstrating strong generalization
across diverse medical benchmarks. QuarkMed offers a powerful yet versatile
personal medical AI solution, already serving over millions of users at
ai.quark.cn.

</details>


### [242] [CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs](https://arxiv.org/abs/2508.11944)
*Hongtao Liu,Zhicheng Du,Zihe Wang,Weiran Shen*

Main category: cs.AI

TL;DR: 提出了Cognitive Hierarchy Benchmark (CHBench)，一个用于评估LLMs策略推理能力的新框架。


<details>
  <summary>Details</summary>
Motivation: 现有的方法由于受到对手行为和游戏结构变化的影响，缺乏稳健性，无法充分评估LLMs的战略推理能力。

Method: 引入受行为经济学中认知阶层模型启发的CHBench框架，并通过三阶段系统化方法，利用六个最新LLMs在十五个常规游戏中的行为数据进行评估。

Result: 实验发现，LLMs在不同对手中的策略推理水平具有一致性。Chat机制降低了策略推理能力，Memory机制可以增强表现。

Conclusion: CHBench作为评估LLMs能力的工具具有较强的稳健性与泛化能力，为未来研究和应用提供了见解。

Abstract: Game-playing ability serves as an indicator for evaluating the strategic
reasoning capability of large language models (LLMs). While most existing
studies rely on utility performance metrics, which are not robust enough due to
variations in opponent behavior and game structure. To address this limitation,
we propose \textbf{Cognitive Hierarchy Benchmark (CHBench)}, a novel evaluation
framework inspired by the cognitive hierarchy models from behavioral economics.
We hypothesize that agents have bounded rationality -- different agents behave
at varying reasoning depths/levels. We evaluate LLMs' strategic reasoning
through a three-phase systematic framework, utilizing behavioral data from six
state-of-the-art LLMs across fifteen carefully selected normal-form games.
Experiments show that LLMs exhibit consistent strategic reasoning levels across
diverse opponents, confirming the framework's robustness and generalization
capability. We also analyze the effects of two key mechanisms (Chat Mechanism
and Memory Mechanism) on strategic reasoning performance. Results indicate that
the Chat Mechanism significantly degrades strategic reasoning, whereas the
Memory Mechanism enhances it. These insights position CHBench as a promising
tool for evaluating LLM capabilities, with significant potential for future
research and practical applications.

</details>


### [243] [Data Mixing Optimization for Supervised Fine-Tuning of Large Language Models](https://arxiv.org/abs/2508.11953)
*Yuan Li,Zhengzhong Liu,Eric Xing*

Main category: cs.AI

TL;DR: 该论文提出了一种优化大语言模型数据混合的新方法，以最小化验证损失并提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 优化监督微调中数据混合的方式尚属未充分研究的领域，而作者希望通过精准控制数据混合参数提高模型表现。

Method: 提出了一种优化算法，通过参数化损失函数和利用扩展定律，来推导出优化的权重分布。作者使用小规模数据集的实验来拟合模型参数，最终获得最佳数据混合方案。

Result: 实验表明，使用该算法优化权重后，模型在所有领域中能获得卓越表现，验证损失和下游任务性能均有提升，且域内损失仅比网格搜索的最优结果高0.66%。

Conclusion: 优化的权重方法提升了模型的微调效果，适用于通用模型，也能推广用于域特定模型的数据选择，为监督微调提供了重要的参考和见解。

Abstract: Optimizing data mixtures for supervised fine-tuning (SFT) of large language
models (LLMs) is critical for developing general-purpose models, yet this area
remains underexplored. In this paper, we frame data mixing as an optimization
problem and introduce a novel method designed to minimize validation loss. Our
approach parametrizes the loss by modeling effective data transferred and
leveraging scaling laws for fine-tuning. By experimenting with various
small-scale data mixtures, we fit these parameters and derive the optimal
weights. We provide both mathematical proofs and empirical results
demonstrating that our algorithm achieves excellent overall and individual
performance across all domains. Through controlled experiments, we show that
models trained with our optimized weights perform on par with those using
optimal weights determined via grid search, with per-domain loss only 0.66%
higher than the best domain loss from grid search on average. Additionally, we
show that reweighting popular SFT datasets using our method improves both
validation loss and downstream performance. Finally, we discuss how our method
can generalize to guide data selection for domain-specific models and provide
insights into SFT.

</details>


### [244] [UniCast: A Unified Multimodal Prompting Framework for Time Series Forecasting](https://arxiv.org/abs/2508.11954)
*Sehyuk Park,Soyeon Caren Han,Eduard Hovy*

Main category: cs.AI

TL;DR: 本文提出UniCast，一个扩展时间序列基础模型（TSFMs）的多模态预测框架，可以联合利用时间序列、视觉和文本模态进行预测，显著超越现有基准。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测模型大多集中于单模态设置，忽略了现实数据中伴随时间序列的数据如视觉和文本信号的多模态背景影响。

Method: 设计了一个新颖的参数高效的多模态框架UniCast，通过软提示调优将来自预训练视觉和文本编码器的模态特定嵌入与冻结的TSFM相结合，允许既保持基础模型的泛化能力又有效实现跨模态交互。

Result: 在不同时间序列预测基准中的广泛实验表明，UniCast始终显著优于所有现有的TSFM基准。

Conclusion: 多模态背景在提升下一代通用时间序列预测模型的发展中起到了关键作用。

Abstract: Time series forecasting is a foundational task across domains, such as
finance, healthcare, and environmental monitoring. While recent advances in
Time Series Foundation Models (TSFMs) have demonstrated strong generalisation
through large-scale pretraining, existing models operate predominantly in a
unimodal setting, ignoring the rich multimodal context, such as visual and
textual signals, that often accompanies time series data in real-world
scenarios. This paper introduces a novel parameter-efficient multimodal
framework, UniCast, that extends TSFMs to jointly leverage time series, vision,
and text modalities for enhanced forecasting performance. Our method integrates
modality-specific embeddings from pretrained Vision and Text Encoders with a
frozen TSFM via soft prompt tuning, enabling efficient adaptation with minimal
parameter updates. This design not only preserves the generalisation strength
of the foundation model but also enables effective cross-modal interaction.
Extensive experiments across diverse time-series forecasting benchmarks
demonstrate that UniCast consistently and significantly outperforms all
existing TSFM baselines. The findings highlight the critical role of multimodal
context in advancing the next generation of general-purpose time series
forecasters.

</details>


### [245] [Rigorous Feature Importance Scores based on Shapley Value and Banzhaf Index](https://arxiv.org/abs/2508.11959)
*Xuanxiang Huang,Olivier Létoffé,Joao Marques-Silva*

Main category: cs.AI

TL;DR: 该研究提出两种新的特征重要性评分方法，采用 Shapley 值和 Banzhaf 指数计算特征的贡献，并兼顾非 WAXp 集的影响。


<details>
  <summary>Details</summary>
Motivation: 现有基于逻辑的特征归因方法存在忽略非 WAXp 集贡献的缺陷，而这些集可能对解释高风险场景下的模型至关重要。

Method: 通过结合 Shapley 值与 Banzhaf 指数，提出新的特征评分指标，同时考虑 WAXp 与非 WAXp 集合中特征的贡献；分析了这些指标的性质并研究其计算复杂度。

Result: 新的指标能够衡量特征在排除对抗样本方面的有效性，并且更加全面地描述特征的重要性分配。

Conclusion: 在高风险场景中考虑非 WAXp 集能够改进特征归因方法，使其更加全面和合理。

Abstract: Feature attribution methods based on game theory are ubiquitous in the field
of eXplainable Artificial Intelligence (XAI). Recent works proposed rigorous
feature attribution using logic-based explanations, specifically targeting
high-stakes uses of machine learning (ML) models. Typically, such works exploit
weak abductive explanation (WAXp) as the characteristic function to assign
importance to features. However, one possible downside is that the contribution
of non-WAXp sets is neglected. In fact, non-WAXp sets can also convey important
information, because of the relationship between formal explanations (XPs) and
adversarial examples (AExs). Accordingly, this paper leverages Shapley value
and Banzhaf index to devise two novel feature importance scores. We take into
account non-WAXp sets when computing feature contribution, and the novel scores
quantify how effective each feature is at excluding AExs. Furthermore, the
paper identifies properties and studies the computational complexity of the
proposed scores.

</details>


### [246] [Chart-CoCa: Self-Improving Chart Understanding of Vision LMs via Code-Driven Synthesis and Candidate-Conditioned Answering](https://arxiv.org/abs/2508.11975)
*Gongyao Jiang,Qiong Luo*

Main category: cs.AI

TL;DR: 该论文提出了一种利用图表合成管道和测试时缩放，旨在提高视觉语言模型在图表理解任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在处理准确的图表描述和复杂推理时表现不足，且合成数据通常面临标签噪声的问题。

Method: 作者设计了一个图表合成管道，用代码生成和执行生成对齐的图表-问题-答案三元组，此外还提出候选条件回答流程，结合多次回答生成最终答案。

Result: 实验结果表明，该方法使原始模型的准确性提高了最多15.50个百分点，无需人工标注数据或外部模型提升了性能。

Conclusion: 通过全自动改进的范式，该方法验证了在没有人工干预情况下提高图表理解能力的可行性。

Abstract: Vision Language Models (VLMs) often struggle with chart understanding tasks,
particularly in accurate chart description and complex reasoning. Synthetic
data generation is a promising solution, while usually facing the challenge of
noise labels. To address this challenge, we first introduce a chart synthesis
pipeline that generates aligned chart-question-answer triplets through code
generation and execution, ensuring the reliability of synthetic data without
human intervention. Furthermore, inspired by test-time scaling that increases
inference budget and thereby improves performance, we design a
candidate-conditioned answering process. The VLM first generates multiple
responses per query, and then synthesizes the final answer by contextualizing
these candidates. Experiments demonstrate significant improvements, with up to
15.50 points accuracy gain over the initial VLM, in a fully self-improving
paradigm without either human-labeled data or external models.

</details>


### [247] [FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction](https://arxiv.org/abs/2508.11987)
*Zhiyuan Zeng,Jiashuo Liu,Siyuan Chen,Tianci He,Yali Liao,Jinpeng Wang,Zaiyuan Wang,Yang Yang,Lingyue Yin,Mingren Yin,Zhenwei Zhu,Tianle Cai,Zehui Chen,Jiecao Chen,Yantao Du,Xiang Gao,Jiacheng Guo,Liang Hu,Jianpeng Jiao,Xiangsheng Li,Jingkai Liu,Shuang Ni,Zhoufutu Wen,Ge Zhang,Kaiyuan Zhang,Xin Zhou,Jose Blanchet,Xipeng Qiu,Mengdi Wang,Wenhao Huang*

Main category: cs.AI

TL;DR: 提出了FutureX，一个针对LLM代理在未来预测任务中表现的动态实时评估基准。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在未来预测中的实时性、动态性和数据污染问题，推动LLM代理的发展以接近专业人类分析师的能力。

Method: 设计了FutureX基准，支持每日实时更新，结合自动化管道避免数据污染，对25种LLM/代理模型进行了评估，并深入分析了模型的失败模式与任务表现陷阱。

Result: 实验展示了LLM代理在动态环境下的适应性推理能力和表现，并揭示了其在应对虚假网页和时间有效性问题上的现有局限性。

Conclusion: FutureX提供了一个动态、无污染的评估标准，为LLM在复杂推理和预测思维上的发展奠定了基础。

Abstract: Future prediction is a complex task for LLM agents, requiring a high level of
analytical thinking, information gathering, contextual understanding, and
decision-making under uncertainty. Agents must not only gather and interpret
vast amounts of dynamic information but also integrate diverse data sources,
weigh uncertainties, and adapt predictions based on emerging trends, just as
human experts do in fields like politics, economics, and finance. Despite its
importance, no large-scale benchmark exists for evaluating agents on future
prediction, largely due to challenges in handling real-time updates and
retrieving timely, accurate answers. To address this, we introduce
$\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically
designed for LLM agents performing future prediction tasks. FutureX is the
largest and most diverse live benchmark for future prediction, supporting
real-time daily updates and eliminating data contamination through an automated
pipeline for question gathering and answer collection. We evaluate 25 LLM/agent
models, including those with reasoning, search capabilities, and integration of
external tools such as the open-source Deep Research Agent and closed-source
Deep Research models. This comprehensive evaluation assesses agents' adaptive
reasoning and performance in dynamic environments. Additionally, we provide
in-depth analyses of agents' failure modes and performance pitfalls in
future-oriented tasks, including the vulnerability to fake web pages and the
temporal validity. Our goal is to establish a dynamic, contamination-free
evaluation standard that drives the development of LLM agents capable of
performing at the level of professional human analysts in complex reasoning and
predictive thinking.

</details>


### [248] [Modeling Relational Logic Circuits for And-Inverter Graph Convolutional Network](https://arxiv.org/abs/2508.11991)
*Weihao Sun*

Main category: cs.AI

TL;DR: 本文提出了一种名为AIGer的模型，通过增强的节点嵌入初始化和异构图卷积网络组件，显著提升了AIG模型对功能和结构特征的联合建模能力，在信号概率预测和真值表距离预测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在对大规模和复杂结构的AIG建模中存在局限性，主要表现为功能和结构特征的联合建模不足，以及信息传播能力的欠缺。

Method: AIGer模型包括两个核心组件：1）节点逻辑特征初始化嵌入组件，用于将逻辑节点投射到独立的语义空间。2）AIG特征学习网络组件，利用异构图卷积网络设计动态关系权重矩阵和差异化信息聚合方式，增强对AIG原始结构和信息的表征能力。

Result: 实验结果表明，AIGer在信号概率预测任务中，MAE和MSE分别改善了18.95%和44.44%；在真值表距离预测任务中，MAE和MSE分别改善了33.57%和14.79%。

Conclusion: AIGer显著提升了功能和结构特征的联合建模能力及信息传播能力，为EDA领域的逻辑电路自动化设计提供了先进的解决方案。

Abstract: The automation of logic circuit design enhances chip performance, energy
efficiency, and reliability, and is widely applied in the field of Electronic
Design Automation (EDA).And-Inverter Graphs (AIGs) efficiently represent,
optimize, and verify the functional characteristics of digital circuits,
enhancing the efficiency of EDA development.Due to the complex structure and
large scale of nodes in real-world AIGs, accurate modeling is challenging,
leading to existing work lacking the ability to jointly model functional and
structural characteristics, as well as insufficient dynamic information
propagation capability.To address the aforementioned challenges, we propose
AIGer.Specifically, AIGer consists of two components: 1) Node logic feature
initialization embedding component and 2) AIGs feature learning network
component.The node logic feature initialization embedding component projects
logic nodes, such as AND and NOT, into independent semantic spaces, to enable
effective node embedding for subsequent processing.Building upon this, the AIGs
feature learning network component employs a heterogeneous graph convolutional
network, designing dynamic relationship weight matrices and differentiated
information aggregation approaches to better represent the original structure
and information of AIGs.The combination of these two components enhances
AIGer's ability to jointly model functional and structural characteristics and
improves its message passing capability. Experimental results indicate that
AIGer outperforms the current best models in the Signal Probability Prediction
(SSP) task, improving MAE and MSE by 18.95\% and 44.44\%, respectively. In the
Truth Table Distance Prediction (TTDP) task, AIGer achieves improvements of
33.57\% and 14.79\% in MAE and MSE, respectively, compared to the
best-performing models.

</details>


### [249] [AgentCDM: Enhancing Multi-Agent Collaborative Decision-Making via ACH-Inspired Structured Reasoning](https://arxiv.org/abs/2508.11995)
*Xuyang Zhao,Shiwan Zhao,Hualong Yu,Liting Zhang,Qicheng Li*

Main category: cs.AI

TL;DR: 本文提出AgentCDM，一种旨在优化基于大语言模型（LLM）的多智能体系统中协作决策过程的框架，利用竞争性假设分析（ACH）进行结构化推理，实验验证了其在多个基准数据集上的高效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的多智能体系统中，协作决策面临过于依赖单一智能体的认知偏差或无法充分利用集体智慧的治理机制问题，亟需一种缓解认知偏差并提升协作决策质量的新方法。

Method: 提出AgentCDM框架，通过借用认知科学中的竞争性假设分析（ACH）来创建一个结构化推理范式，包括两阶段训练方法：第一阶段采用明确的ACH参考体系进行推理引导；第二阶段逐渐取消参考体系以促进自主泛化。

Result: 实验结果表明，AgentCDM在多个基准测试数据集上实现了最先进的表现，同时展现了出色的泛化能力，证明了其在多智能体系统中改进协作决策质量和鲁棒性的有效性。

Conclusion: AgentCDM框架能够通过有组织的推理方法显著提升基于LLM的多智能体系统的协作决策能力，并在多个场景中验证了其实用性和通用性。

Abstract: Multi-agent systems (MAS) powered by large language models (LLMs) hold
significant promise for solving complex decision-making tasks. However, the
core process of collaborative decision-making (CDM) within these systems
remains underexplored. Existing approaches often rely on either ``dictatorial"
strategies that are vulnerable to the cognitive biases of a single agent, or
``voting-based" methods that fail to fully harness collective intelligence. To
address these limitations, we propose \textbf{AgentCDM}, a structured framework
for enhancing collaborative decision-making in LLM-based multi-agent systems.
Drawing inspiration from the Analysis of Competing Hypotheses (ACH) in
cognitive science, AgentCDM introduces a structured reasoning paradigm that
systematically mitigates cognitive biases and shifts decision-making from
passive answer selection to active hypothesis evaluation and construction. To
internalize this reasoning process, we develop a two-stage training paradigm:
the first stage uses explicit ACH-inspired scaffolding to guide the model
through structured reasoning, while the second stage progressively removes this
scaffolding to encourage autonomous generalization. Experiments on multiple
benchmark datasets demonstrate that AgentCDM achieves state-of-the-art
performance and exhibits strong generalization, validating its effectiveness in
improving the quality and robustness of collaborative decisions in MAS.

</details>


### [250] [AI Models for Depressive Disorder Detection and Diagnosis: A Review](https://arxiv.org/abs/2508.12022)
*Dorsa Macky Aleagha,Payam Zohari,Mostafa Haghir Chehreghani*

Main category: cs.AI

TL;DR: 本研究综述了用于抑郁检测和诊断的人工智能方法，通过对55个核心研究的系统性回顾，提出了一种新颖的层次化分类法，总结了当前趋势和挑战，为未来研究提供方向。


<details>
  <summary>Details</summary>
Motivation: 抑郁症诊断依赖主观评估，缺乏客观工具，研究旨在通过AI方法开发出客观、可扩展和及时的诊断手段。

Method: 综述了55篇研究构建的AI方法，提出了基于临床任务、数据模态及模型分类的层次化分类法，分析了神经网络及多模态技术在此领域的应用。

Result: 揭示三大趋势：图神经网络在脑连接建模中的主导地位，大型语言模型在语言数据中的兴起，多模态融合及算法公平性逐渐受到重视。

Conclusion: 该综述为未来创新提供全面路径图，通过总结方法进展和未解决问题，推动计算精神病学领域发展。

Abstract: Major Depressive Disorder is one of the leading causes of disability
worldwide, yet its diagnosis still depends largely on subjective clinical
assessments. Integrating Artificial Intelligence (AI) holds promise for
developing objective, scalable, and timely diagnostic tools. In this paper, we
present a comprehensive survey of state-of-the-art AI methods for depression
detection and diagnosis, based on a systematic review of 55 key studies. We
introduce a novel hierarchical taxonomy that structures the field by primary
clinical task (diagnosis vs. prediction), data modality (text, speech,
neuroimaging, multimodal), and computational model class (e.g., graph neural
networks, large language models, hybrid approaches). Our in-depth analysis
reveals three major trends: the predominance of graph neural networks for
modeling brain connectivity, the rise of large language models for linguistic
and conversational data, and an emerging focus on multimodal fusion,
explainability, and algorithmic fairness. Alongside methodological insights, we
provide an overview of prominent public datasets and standard evaluation
metrics as a practical guide for researchers. By synthesizing current advances
and highlighting open challenges, this survey offers a comprehensive roadmap
for future innovation in computational psychiatry.

</details>


### [251] [Bongard-RWR+: Real-World Representations of Fine-Grained Concepts in Bongard Problems](https://arxiv.org/abs/2508.12026)
*Szymon Pawlonka,Mikołaj Małkiński,Jacek Mańdziuk*

Main category: cs.AI

TL;DR: 介绍了一种新的Bongard-RWR+数据集，扩展了原来的Bongard-RWR数据集，以更大规模和更复杂的图像对视觉和语言模型（VLMs）的抽象推理能力进行评估。


<details>
  <summary>Details</summary>
Motivation: 解决目前Bongard问题数据集中规模小、复杂度不够以充分测试视觉与语言模型进行了改进。

Method: 通过使用Pixtral-12B和Flux.1-dev生成与原始概念匹配的真实图像，并对其进行人工验证，制作了包含5400个实例的大型数据集Bongard-RWR+。

Result: 评估显示，现有视觉与语言模型在区分细粒度抽象概念上存在显著困难，但对于粗粒度视觉概念处理相对较好。

Conclusion: 尽管视觉与语言模型在某些任务上表现优秀，但它们在抽象推理特别是细粒度概念的理解方面仍有局限性。

Abstract: Bongard Problems (BPs) provide a challenging testbed for abstract visual
reasoning (AVR), requiring models to identify visual concepts fromjust a few
examples and describe them in natural language. Early BP benchmarks featured
synthetic black-and-white drawings, which might not fully capture the
complexity of real-world scenes. Subsequent BP datasets employed real-world
images, albeit the represented concepts are identifiable from high-level image
features, reducing the task complexity. Differently, the recently released
Bongard-RWR dataset aimed at representing abstract concepts formulated in the
original BPs using fine-grained real-world images. Its manual construction,
however, limited the dataset size to just $60$ instances, constraining
evaluation robustness. In this work, we introduce Bongard-RWR+, a BP dataset
composed of $5\,400$ instances that represent original BP abstract concepts
using real-world-like images generated via a vision language model (VLM)
pipeline. Building on Bongard-RWR, we employ Pixtral-12B to describe manually
curated images and generate new descriptions aligned with the underlying
concepts, use Flux.1-dev to synthesize images from these descriptions, and
manually verify that the generated images faithfully reflect the intended
concepts. We evaluate state-of-the-art VLMs across diverse BP formulations,
including binary and multiclass classification, as well as textual answer
generation. Our findings reveal that while VLMs can recognize coarse-grained
visual concepts, they consistently struggle with discerning fine-grained
concepts, highlighting limitations in their reasoning capabilities.

</details>


### [252] [Active inference for action-unaware agents](https://arxiv.org/abs/2508.12027)
*Filippo Torresan,Keisuke Suzuki,Ryota Kanai,Manuel Baltieri*

Main category: cs.AI

TL;DR: 该文研究了两种不同的操作方式下的代理行为，即是否存在外传拷贝信号，用于了解自身动作的效能，以及对此在导航任务中的表现影响。


<details>
  <summary>Details</summary>
Motivation: 理解行动中的行动意识对序列动作生成和导航任务执行的影响。

Method: 分析并比较'行动知晓'(action-aware)与'行动不知晓'(action-unaware)两种模型代理，测试其在两个导航任务中的表现。

Result: 即使在没有外传拷贝信号的情况下，行动不知晓代理在导航任务中也能与拥有信号的代理表现相当。

Conclusion: 此研究表明，尽管行动不知晓代理处于劣势，但其能够通过推断动作同样实现优秀的导航任务表现，从而对行动意识在认知模型中的必要性进行了探讨。

Abstract: Active inference is a formal approach to study cognition based on the notion
that adaptive agents can be seen as engaging in a process of approximate
Bayesian inference, via the minimisation of variational and expected free
energies. Minimising the former provides an account of perceptual processes and
learning as evidence accumulation, while minimising the latter describes how
agents select their actions over time. In this way, adaptive agents are able to
maximise the likelihood of preferred observations or states, given a generative
model of the environment. In the literature, however, different strategies have
been proposed to describe how agents can plan their future actions. While they
all share the notion that some kind of expected free energy offers an
appropriate way to score policies, sequences of actions, in terms of their
desirability, there are different ways to consider the contribution of past
motor experience to the agent's future behaviour. In some approaches, agents
are assumed to know their own actions, and use such knowledge to better plan
for the future. In other approaches, agents are unaware of their actions, and
must infer their motor behaviour from recent observations in order to plan for
the future. This difference reflects a standard point of departure in two
leading frameworks in motor control based on the presence, or not, of an
efference copy signal representing knowledge about an agent's own actions. In
this work we compare the performances of action-aware and action-unaware agents
in two navigations tasks, showing how action-unaware agents can achieve
performances comparable to action-aware ones while at a severe disadvantage.

</details>


### [253] [MAPF-World: Action World Model for Multi-Agent Path Finding](https://arxiv.org/abs/2508.12087)
*Zhanjiang Yang,Meng Li,Yang Shen,Yueming Li,Lijun Sun*

Main category: cs.AI

TL;DR: 本研究提出了MAPF-World模型，通过构建环境动态和未来状态预测，实现多智能体路径规划(MAPF)中的冲突规避和长期协作。实验表明，MAPF-World在更小规模的数据和模型下超越了现有学习型求解器。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化学习型求解器由于缺乏对环境动态和智能体间依赖的建模，在复杂的长期规划中表现不佳，研究旨在解决此问题。

Method: 提出MAPF-World模型，采用自回归行为生成方式，通过显式建模环境动态、预测未来状态和行动，提高整体规划意识与决策能力。同时引入基于真实场景的自动地图生成器，用于模型训练与评估。

Result: 实验显示MAPF-World显著优于现有学习型求解器，在零样本泛化能力和分布外案例中表现突出，并实现96.5%模型减小和92%数据减少的同时保持高性能。

Conclusion: MAPF-World显著提升了多智能体路径规划中的决策性能，特别是在复杂环境中的长期规划问题中，展示了良好的扩展性和效率。

Abstract: Multi-agent path finding (MAPF) is the problem of planning conflict-free
paths from the designated start locations to goal positions for multiple
agents. It underlies a variety of real-world tasks, including multi-robot
coordination, robot-assisted logistics, and social navigation. Recent
decentralized learnable solvers have shown great promise for large-scale MAPF,
especially when leveraging foundation models and large datasets. However, these
agents are reactive policy models and exhibit limited modeling of environmental
temporal dynamics and inter-agent dependencies, resulting in performance
degradation in complex, long-term planning scenarios. To address these
limitations, we propose MAPF-World, an autoregressive action world model for
MAPF that unifies situation understanding and action generation, guiding
decisions beyond immediate local observations. It improves situational
awareness by explicitly modeling environmental dynamics, including spatial
features and temporal dependencies, through future state and actions
prediction. By incorporating these predicted futures, MAPF-World enables more
informed, coordinated, and far-sighted decision-making, especially in complex
multi-agent settings. Furthermore, we augment MAPF benchmarks by introducing an
automatic map generator grounded in real-world scenarios, capturing practical
map layouts for training and evaluating MAPF solvers. Extensive experiments
demonstrate that MAPF-World outperforms state-of-the-art learnable solvers,
showcasing superior zero-shot generalization to out-of-distribution cases.
Notably, MAPF-World is trained with a 96.5% smaller model size and 92% reduced
data.

</details>


### [254] [Overcoming Knowledge Discrepancies: Structuring Reasoning Threads through Knowledge Balancing in Interactive Scenarios](https://arxiv.org/abs/2508.12100)
*Daniel Burkhardt,Xiangwei Cheng*

Main category: cs.AI

TL;DR: 本文提出了一种名为ReT-Eval的两阶段推理线程评估框架，通过利用稀疏领域知识图结合图神经网络，以丰富现有知识并修正知识差异，实现更高效的推理路径构建及评估。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型普遍缺乏明显的语义层级、用户与领域知识对齐和有效的推理筛选机制，导致输出冗长而不具指导性。研究的目标是构建一种更直观且高效的推理模型。

Method: 该论文提出一个两阶段框架：第一阶段采用图神经网络从稀疏领域知识图提取语义相关的知识结构，并结合强语言模型内在的知识进行补充与修正；第二阶段通过奖励引导策略对推理线程进行评估和修剪，确保语义一致性。

Result: 实验与专家评价表明，ReT-Eval框架在增强用户理解方面效果显著，并优于最先进的推理模型。

Conclusion: 通过结构化知识的提取与基于奖励的推理线程评估，该框架能够有效解决现有推理模型的语义层次不足和数据过多等问题，提高推理的效率与实用性。

Abstract: Reasoning in interactive problem solving scenarios requires models to
construct reasoning threads that reflect user understanding and align with
structured domain knowledge. However, current reasoning models often lack
explicit semantic hierarchies, user-domain knowledge alignment, and principled
mechanisms to prune reasoning threads for effectiveness. These limitations
result in lengthy generic output that does not guide users through
goal-oriented reasoning steps. To address this, we propose a
prototype-inspired, two-phases Reasoning-Threads-Evaluation (ReT-Eval)
framework, drawing inspiration from human-like reasoning strategies that
emphasize structured knowledge reuse. In the first phase, semantically relevant
knowledge structures are extracted from a sparse domain knowledge graph using a
graph neural network and enriched with intrinsic large language model knowledge
to resolve knowledge discrepancies. In the second phase, these threads are
evaluated and pruned using a reward-guided strategy aimed at maintaining
semantic coherence to generate effective reasoning threads. Experiments and
expert evaluations show that ReT-Eval enhances user understanding and
outperforms state-of-the-art reasoning models.

</details>


### [255] [MOVER: Multimodal Optimal Transport with Volume-based Embedding Regularization](https://arxiv.org/abs/2508.12149)
*Haochen You,Baojing Liu*

Main category: cs.AI

TL;DR: 提出MOVER框架，通过最优传输和几何体积正则化方法构建多模态表示，在多模态检索任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前的多模态学习在处理多模态之间的对齐任务时存在难以泛化和语义结构不清的问题。

Method: 提出MOVER框架，将基于最优传输的软对齐方法与体积几何正则化目标（GAVE）相结合，以模态无关的方式促进多模态一致性。

Result: MOVER在零样本和微调设定的文本-视频-音频检索任务中显著优于以往的方法，同时表现出更好的未见模态组合的泛化性能和更强的嵌入空间结构一致性。

Conclusion: MOVER框架通过创新方法有效解决了多模态学习中的对齐和语义结构问题，提升了性能及泛化能力。

Abstract: Recent advances in multimodal learning have largely relied on pairwise
contrastive objectives to align different modalities, such as text, video, and
audio, in a shared embedding space. While effective in bi-modal setups, these
approaches struggle to generalize across multiple modalities and often lack
semantic structure in high-dimensional spaces. In this paper, we propose MOVER,
a novel framework that combines optimal transport-based soft alignment with
volume-based geometric regularization to build semantically aligned and
structured multimodal representations. By integrating a transport-guided
matching mechanism with a geometric volume minimization objective (GAVE), MOVER
encourages consistent alignment across all modalities in a modality-agnostic
manner. Experiments on text-video-audio retrieval tasks demonstrate that MOVER
significantly outperforms prior state-of-the-art methods in both zero-shot and
finetuned settings. Additional analysis shows improved generalization to unseen
modality combinations and stronger structural consistency in the learned
embedding space.

</details>


### [256] [RLNVR: Reinforcement Learning from Non-Verified Real-World Rewards](https://arxiv.org/abs/2508.12165)
*Rohit Krishnan,Jon Evans*

Main category: cs.AI

TL;DR: 提出RLNVR框架，通过非验证奖励信号训练语言模型，并展示了在社交媒体内容生成中的应用效果显著。


<details>
  <summary>Details</summary>
Motivation: 减少传统RLHF因需要昂贵的、已验证的奖励信号导致的局限性，探索用更实际、非验证的方式进行语言模型训练。

Method: 通过基线归一化和语义相似度的奖励转移方式，结合GSPO与可选UED课程进行训练。原型系统Walter利用Bluesky的真实社交互动数据进行优化。

Result: 实验显示内容质量和训练稳定性有显著改善，全面评估将待未来工作完成。

Conclusion: RLNVR提供了一种有效的框架，在噪声和隐式奖励条件下实现了稳定和多样性训练，尤其在社交媒体内容生成领域具有潜力。

Abstract: This paper introduces RLNVR (Reinforcement Learning from Non-Verified
Rewards), a framework for training language models using noisy, real-world
feedback signals without requiring explicit human verification. Traditional
RLHF requires expensive, verified reward signals that are impractical in many
real-world domains. RLNVR addresses this challenge through baseline
normalization and semantic similarity-based reward transfer. We demonstrate
RLNVR through Walter, a prototype system that optimizes social media content
generation using actual engagement data from Bluesky. Our experimental results
show significant improvements in content quality and training stability, with
comprehensive evaluation planned for future work. Positioning: We present a
practical framework that combines RLNVR with GSPO (Group Sequence Policy
Optimization) and an optional UED (Unsupervised Environment Design) curriculum
to improve stability and diversity under noisy, implicit rewards. To our
knowledge, combining GSPO-style normalization with a UED-style curriculum for
LLM content generation from implicit social engagement has not been previously
documented in this applied setting; we frame this as an applied integration
rather than a new algorithm.

</details>


### [257] [Mantis: A Simulation-Grounded Foundation Model for Disease Forecasting](https://arxiv.org/abs/2508.12260)
*Carson Dudley,Reiden Magdaleno,Christopher Harding,Ananya Sharma,Emily Martin,Marisa Eisenberg*

Main category: cs.AI

TL;DR: Mantis通过基于模拟的训练，克服了传统传染病预测对特定疾病数据的依赖，提供了跨疾病、区域和结果的预测能力，并在多个场景中优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 解决传染病预测需依赖疾病特定数据、个性化训练和专家调优的问题，特别是在新疫情或资源有限的环境中。

Method: 引入一个完全基于机械模拟训练的基础模型Mantis，训练数据涵盖400多百万天的疫情模拟，覆盖多种病原体、传播方式、干预措施和监测特性。

Result: Mantis在不用实测数据的情况下表现优越，超越了39种专家调优模型，包括CDC COVID-19预测中心的所有模型，展示了对新流行病学机制和基本传染动力学的良好泛化能力。

Conclusion: Mantis是下一代传染病预测系统的基础，非常通用、可解释且适用于传统模型失效的场景。

Abstract: Infectious disease forecasting in novel outbreaks or low resource settings
has been limited by the need for disease-specific data, bespoke training, and
expert tuning. We introduce Mantis, a foundation model trained entirely on
mechanistic simulations, which enables out-of-the-box forecasting across
diseases, regions, and outcomes, even in settings with limited historical data.
Mantis is built on over 400 million simulated days of outbreak dynamics
spanning diverse pathogens, transmission modes, interventions, and surveillance
artifacts. Despite requiring no real-world data during training, Mantis
outperformed 39 expert-tuned models we tested across six diseases, including
all models in the CDC's COVID-19 Forecast Hub. Mantis generalized to novel
epidemiological regimes, including diseases with held-out transmission
mechanisms, demonstrating that it captures fundamental contagion dynamics.
Critically, Mantis is mechanistically interpretable, enabling public health
decision-makers to identify the latent drivers behind its predictions. Finally,
Mantis delivers accurate forecasts at 8-week horizons, more than doubling the
actionable range of most models, enabling proactive public health planning.
Together, these capabilities position Mantis as a foundation for
next-generation disease forecasting systems: general, interpretable, and
deployable where traditional models fail.

</details>


### [258] [RadarQA: Multi-modal Quality Analysis of Weather Radar Forecasts](https://arxiv.org/abs/2508.12291)
*Xuming He,Zhiyuan You,Junchao Gong,Couhua Liu,Xiaoyu Yue,Peiqin Zhuang,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: 本文引入了一种基于多模态大语言模型(MLLM)的天气预报质量分析方法——RadarQA，整合了关键物理属性和详细评估报告，并构建了大规模评估数据集RQA-70K。


<details>
  <summary>Details</summary>
Motivation: 传统的得分型评估指标在描述能力、可解释性以及对动态演变的理解上与气象专家的需求仍有差距。MLLM的快速发展为解决这些问题提供了潜在工具。

Method: 提出了一种新的多模态质量分析任务范式，覆盖单帧和时间序列、评分和评估场景；设计混合标注流程结合专家标注和自动推理构建大规模数据集；并采用多阶段训练策略提升模型性能。

Result: RadarQA在所有评估设置上均优于现有通用MLLM，展示了其在天气预报质量分析中的潜力。

Conclusion: RadarQA结合多模态分析和创新任务设计方法，有望推进天气预测中的质量分析领域。

Abstract: Quality analysis of weather forecasts is an essential topic in meteorology.
Although traditional score-based evaluation metrics can quantify certain
forecast errors, they are still far from meteorological experts in terms of
descriptive capability, interpretability, and understanding of dynamic
evolution. With the rapid development of Multi-modal Large Language Models
(MLLMs), these models become potential tools to overcome the above challenges.
In this work, we introduce an MLLM-based weather forecast analysis method,
RadarQA, integrating key physical attributes with detailed assessment reports.
We introduce a novel and comprehensive task paradigm for multi-modal quality
analysis, encompassing both single frame and sequence, under both rating and
assessment scenarios. To support training and benchmarking, we design a hybrid
annotation pipeline that combines human expert labeling with automated
heuristics. With such an annotation method, we construct RQA-70K, a large-scale
dataset with varying difficulty levels for radar forecast quality evaluation.
We further design a multi-stage training strategy that iteratively improves
model performance at each stage. Extensive experiments show that RadarQA
outperforms existing general MLLMs across all evaluation settings, highlighting
its potential for advancing quality analysis in weather prediction.

</details>


### [259] [Wisdom of the Crowd: Reinforcement Learning from Coevolutionary Collective Feedback](https://arxiv.org/abs/2508.12338)
*Wenzhen Yuan,Shengji Tang,Weihao Lin,Jiacheng Ruan,Ganqu Cui,Bo Zhang,Tao Chen,Ting Liu,Yuzhuo Fu,Peng Ye,Lei Bai*

Main category: cs.AI

TL;DR: 本文提出了一种新型的强化学习框架RLCCF，通过多模型协同进化而无需外部监督，解决了传统方法依赖高成本人工标注数据和复杂奖励模型的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，自反馈方法受制于单一模型的能力，可能导致错误答案的过度自信、奖励欺骗和训练崩溃。因此，提出了RLCCF以克服这些局限。

Method: RLCCF通过最大化集体一致性（CC）训练一个多样化的LLM集群，并通过集体输出投票提供奖励信号，同时使用模型的自一致性（SC）得分加权投票影响力。

Result: 实验证明，在四个数学推理基准上，该框架使集群中各模型的性能提高了平均16.72%，同时将群体多数投票准确率提高了4.51%。

Conclusion: RLCCF不仅提升了单个模型性能，还扩展了模型集体的能力边界，展示了其在提升推理能力上的强大潜力。

Abstract: Reinforcement learning (RL) has significantly enhanced the reasoning
capabilities of large language models (LLMs), but its reliance on expensive
human-labeled data or complex reward models severely limits scalability. While
existing self-feedback methods aim to address this problem, they are
constrained by the capabilities of a single model, which can lead to
overconfidence in incorrect answers, reward hacking, and even training
collapse. To this end, we propose Reinforcement Learning from Coevolutionary
Collective Feedback (RLCCF), a novel RL framework that enables multi-model
collaborative evolution without external supervision. Specifically, RLCCF
optimizes the ability of a model collective by maximizing its Collective
Consistency (CC), which jointly trains a diverse ensemble of LLMs and provides
reward signals by voting on collective outputs. Moreover, each model's vote is
weighted by its Self-Consistency (SC) score, ensuring that more confident
models contribute more to the collective decision. Benefiting from the diverse
output distributions and complementary abilities of multiple LLMs, RLCCF
enables the model collective to continuously enhance its reasoning ability
through coevolution. Experiments on four mainstream open-source LLMs across
four mathematical reasoning benchmarks demonstrate that our framework yields
significant performance gains, achieving an average relative improvement of
16.72\% in accuracy. Notably, RLCCF not only improves the performance of
individual models but also enhances the group's majority-voting accuracy by
4.51\%, demonstrating its ability to extend the collective capability boundary
of the model collective.

</details>


### [260] [Hierarchical knowledge guided fault intensity diagnosis of complex industrial systems](https://arxiv.org/abs/2508.12375)
*Yu Sha,Shuiping Gou,Bo Liu,Johannes Faber,Ningtao Liu,Stefan Schramm,Horst Stoecker,Thomas Steckenreiter,Domagoj Vnucec,Nadine Wetzstein,Andreas Widl,Kai Zhou*

Main category: cs.AI

TL;DR: 提出一种面向复杂工业系统的故障强度诊断框架（HKG），通过层次化树状思想和图卷积网络，实现更加精确的故障诊断。实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法未考虑目标类之间的依赖关系，难以获得更精确的诊断结果。本文旨在通过探索和捕捉这类依赖关系来改进诊断效果。

Method: 提出了一个基于层次化知识引导的框架（HKG），利用图卷积网络和层次化的类表征图，将类别之间的依赖关系映射为全局分类器，并通过Re-HKCM方案避免过度平滑问题。

Result: 在四个实际工业数据集上表现出色，包括三个来自SAMSON AG的空化数据集以及一个公共数据集，并超过现有最先进的方法。

Conclusion: HKG框架以及Re-HKCM方案为故障强度诊断提供了一种有效、性能优越的新方法，可以进一步扩展到其他工业领域。

Abstract: Fault intensity diagnosis (FID) plays a pivotal role in monitoring and
maintaining mechanical devices within complex industrial systems. As current
FID methods are based on chain of thought without considering dependencies
among target classes. To capture and explore dependencies, we propose a
hierarchical knowledge guided fault intensity diagnosis framework (HKG)
inspired by the tree of thought, which is amenable to any representation
learning methods. The HKG uses graph convolutional networks to map the
hierarchical topological graph of class representations into a set of
interdependent global hierarchical classifiers, where each node is denoted by
word embeddings of a class. These global hierarchical classifiers are applied
to learned deep features extracted by representation learning, allowing the
entire model to be end-to-end learnable. In addition, we develop a re-weighted
hierarchical knowledge correlation matrix (Re-HKCM) scheme by embedding
inter-class hierarchical knowledge into a data-driven statistical correlation
matrix (SCM) which effectively guides the information sharing of nodes in
graphical convolutional neural networks and avoids over-smoothing issues. The
Re-HKCM is derived from the SCM through a series of mathematical
transformations. Extensive experiments are performed on four real-world
datasets from different industrial domains (three cavitation datasets from
SAMSON AG and one existing publicly) for FID, all showing superior results and
outperform recent state-of-the-art FID methods.

</details>


### [261] [GraphCogent: Overcoming LLMs' Working Memory Constraints via Multi-Agent Collaboration in Complex Graph Understanding](https://arxiv.org/abs/2508.12379)
*Rongzheng Wang,Qizhi Chen,Yihong Huang,Yizhuo Ma,Muquan Li,Jiakai Li,Ke Qin,Guangchun Luo,Shuang Liang*

Main category: cs.AI

TL;DR: 提出了一种名为GraphCogent的框架，通过分解图推理过程提高LLMs在复杂图推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: LLMs在处理真实世界复杂图推理任务时表现欠佳，无法同时高效处理复杂图拓扑和多步推理。

Method: 设计了三模块框架：感觉模块规范化不同图文本表示、缓冲模块整合多种格式的图数据、执行模块结合工具调用和模型生成。提出了一套名为Graph4real的基准用于评估上述方法。

Result: 基于Llama3.1-8B的GraphCogent相较于大规模LLM DeepSeek-R1提升50%的性能，且相比前沿基线在准确性上提高20%，减少token使用量80%（内置工具任务）与30%（外部工具任务）。

Conclusion: GraphCogent显著提升了大语言模型在真实世界复杂图推理任务中的表现，为复杂图推理提供了高效的解决方案。

Abstract: Large language models (LLMs) show promising performance on small-scale graph
reasoning tasks but fail when handling real-world graphs with complex queries.
This phenomenon stems from LLMs' inability to effectively process complex graph
topology and perform multi-step reasoning simultaneously. To address these
limitations, we propose GraphCogent, a collaborative agent framework inspired
by human Working Memory Model that decomposes graph reasoning into specialized
cognitive processes: sense, buffer, and execute. The framework consists of
three modules: Sensory Module standardizes diverse graph text representations
via subgraph sampling, Buffer Module integrates and indexes graph data across
multiple formats, and Execution Module combines tool calling and model
generation for efficient reasoning. We also introduce Graph4real, a
comprehensive benchmark contains with four domains of real-world graphs (Web,
Social, Transportation, and Citation) to evaluate LLMs' graph reasoning
capabilities. Our Graph4real covers 21 different graph reasoning tasks,
categorized into three types (Structural Querying, Algorithmic Reasoning, and
Predictive Modeling tasks), with graph scales that are 10 times larger than
existing benchmarks. Experiments show that Llama3.1-8B based GraphCogent
achieves a 50% improvement over massive-scale LLMs like DeepSeek-R1 (671B).
Compared to state-of-the-art agent-based baseline, our framework outperforms by
20% in accuracy while reducing token usage by 80% for in-toolset tasks and 30%
for out-toolset tasks. Code will be available after review.

</details>


### [262] [Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning](https://arxiv.org/abs/2508.12425)
*Phuong Minh Nguyen,Tien Huu Dang,Naoya Inoue*

Main category: cs.AI

TL;DR: 提出Symbolic-Aided Chain-of-Thought (CoT)，结合符号推理显著提升LLMs的逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 旨在改进现有CoT方法，增强LLMs的逻辑推理透明性、可解释性和分析性。

Method: 在few-shot提示中融入简化的符号表示，使用一致的推理策略使非迭代推理过程更显式化。

Result: 在四个逻辑推理基准数据集上实验验证，表现优异，尤其在复杂推理任务上显著优于传统CoT。

Conclusion: Symbolic-Aided CoT通过结合符号结构扩展了标准CoT的能力，提高了LLMs在多约束规则条件下的逻辑推理表现。

Abstract: This work introduces Symbolic-Aided Chain-of-Thought (CoT), an improved
approach to standard CoT, for logical reasoning in large language models
(LLMs). The key idea is to integrate lightweight symbolic representations into
few-shot prompts, structuring the inference steps with a consistent strategy to
make reasoning patterns more explicit within a non-iterative reasoning process.
By incorporating these symbolic structures, our method preserves the
generalizability of standard prompting techniques while enhancing the
transparency, interpretability, and analyzability of LLM logical reasoning.
Extensive experiments on four well-known logical reasoning benchmarks --
ProofWriter, FOLIO, ProntoQA, and LogicalDeduction, which cover diverse
reasoning scenarios -- demonstrate the effectiveness of the proposed approach,
particularly in complex reasoning tasks that require navigating multiple
constraints or rules. Notably, Symbolic-Aided CoT consistently improves LLMs'
reasoning capabilities across various model sizes and significantly outperforms
conventional CoT on three out of four datasets, ProofWriter, ProntoQA, and
LogicalDeduction.

</details>


### [263] [GALA: Can Graph-Augmented Large Language Model Agentic Workflows Elevate Root Cause Analysis?](https://arxiv.org/abs/2508.12472)
*Yifang Tian,Yaming Liu,Zichun Chong,Zihang Huang,Hans-Arno Jacobsen*

Main category: cs.AI

TL;DR: 本文提出了GALA，一种结合统计因果推断和LLM驱动的迭代推理的新型多模态框架，用于提高微服务系统的根本原因分析准确性。


<details>
  <summary>Details</summary>
Motivation: 微服务系统的根因分析(RCA)面临诊断不同遥测数据（如指标、日志、追踪）的挑战，现有方法在可操作性和指导性上效果欠佳。

Method: GALA结合了统计因果推断和LLM驱动的迭代推理方法，通过多模态融合实现增强的根因分析能力。

Result: 在开源基准数据集上，GALA的准确性比现有技术最高提升了42.22%，并通过以人类指导的LLM评估得出其因果性和操作性均显著优于其他方法。

Conclusion: 实验和案例研究表明，GALA有效弥合了自动化故障诊断与实际事件解决之间的差距，能够提供准确的根因识别和人类可解释的修复指导。

Abstract: Root cause analysis (RCA) in microservice systems is challenging, requiring
on-call engineers to rapidly diagnose failures across heterogeneous telemetry
such as metrics, logs, and traces. Traditional RCA methods often focus on
single modalities or merely rank suspect services, falling short of providing
actionable diagnostic insights with remediation guidance. This paper introduces
GALA, a novel multi-modal framework that combines statistical causal inference
with LLM-driven iterative reasoning for enhanced RCA. Evaluated on an
open-source benchmark, GALA achieves substantial improvements over
state-of-the-art methods of up to 42.22% accuracy. Our novel human-guided LLM
evaluation score shows GALA generates significantly more causally sound and
actionable diagnostic outputs than existing methods. Through comprehensive
experiments and a case study, we show that GALA bridges the gap between
automated failure diagnosis and practical incident resolution by providing both
accurate root cause identification and human-interpretable remediation
guidance.

</details>


### [264] [The Yokai Learning Environment: Tracking Beliefs Over Space and Time](https://arxiv.org/abs/2508.12480)
*Constantin Ruhdorfer,Matteo Bortoletto,Andreas Bulling*

Main category: cs.AI

TL;DR: 本文提出了Yokai Learning Environment（YLE），一个多智能体强化学习环境，用于研究AI的信念建模和理论推理能力。结果表明现有的强化学习算法难以解决此环境内的任务。


<details>
  <summary>Details</summary>
Motivation: 现有的理论推理（ToM）基准测试存在局限，无法有效评估代理如何建立和维护与他人之间的共同认知。在此背景下，引入一个能动态评估智能体信念建模和共同认知的环境显得尤为重要。

Method: 设计了一个基于合作卡牌游戏Yokai的多智能体强化学习环境YLE，要求智能体通过观察、记忆和协作完成任务，以模拟信念动态变化和合作过程。

Result: 当前强化学习算法即使拥有完美记忆，也很难解决YLE任务。信念建模虽然提高了性能，但代理仍难以泛化至新合作伙伴，并且无法在长局游戏中形成准确的信念。

Conclusion: YLE揭示了现有算法在信念建模和共同认知方面的局限性，并为研究更高阶理论推理和合作智能提供了一个有效的平台。

Abstract: Developing collaborative AI hinges on Theory of Mind (ToM) - the ability to
reason about the beliefs of others to build and maintain common ground.
Existing ToM benchmarks, however, are restricted to passive observer settings
or lack an assessment of how agents establish and maintain common ground over
time. To address these gaps, we introduce the Yokai Learning Environment (YLE)
- a multi-agent reinforcement learning (RL) environment based on the
cooperative card game Yokai. In the YLE, agents take turns peeking at hidden
cards and moving them to form clusters based on colour. Success requires
tracking evolving beliefs, remembering past observations, using hints as
grounded communication, and maintaining common ground with teammates. Our
evaluation yields two key findings: First, current RL agents struggle to solve
the YLE, even when given access to perfect memory. Second, while belief
modelling improves performance, agents are still unable to effectively
generalise to unseen partners or form accurate beliefs over longer games,
exposing a reliance on brittle conventions rather than robust belief tracking.
We use the YLE to investigate research questions in belief modelling, memory,
partner generalisation, and scaling to higher-order ToM.

</details>


### [265] [Advanced DOA Regulation with a Whale-Optimized Fractional Order Fuzzy PID Framework](https://arxiv.org/abs/2508.12487)
*Lida Shahbandari,Hossein Mohseni*

Main category: cs.AI

TL;DR: 本研究提出了一种基于小鲸鱼优化算法(WOA)的分数阶模糊PID控制器(FOFPID)，用于将双频谱指数(BIS)控制在理想范围(40-60)内。


<details>
  <summary>Details</summary>
Motivation: 设计一种智能、精确并适合个性化生理特征的控制系统，用于改善自动麻醉的临床实践和病人结果。

Method: 结合模糊逻辑和分数阶控制动态，利用WOA调整控制参数，包括分数阶和模糊隶属函数。

Result: 在8种患者模型测试中，该控制器比传统分数阶PID具有更快的稳定时间(2.5分钟对比3.2分钟)和更低的稳态误差(0.5对比1.2)。

Conclusion: FOFPID展示了其在强度和精确度上的优秀表现，是一种可扩展的AI驱动自动麻醉解决方案，有望用于临床实践。

Abstract: This study introduces a Fractional Order Fuzzy PID (FOFPID) controller that
uses the Whale Optimization Algorithm (WOA) to manage the Bispectral Index
(BIS), keeping it within the ideal range of forty to sixty. The FOFPID
controller combines fuzzy logic for adapting to changes and fractional order
dynamics for fine tuning. This allows it to adjust its control gains to handle
a person's unique physiology. The WOA helps fine tune the controller's
parameters, including the fractional orders and the fuzzy membership functions,
which boosts its performance. Tested on models of eight different patient
profiles, the FOFPID controller performed better than a standard Fractional
Order PID (FOPID) controller. It achieved faster settling times, at two and a
half minutes versus three point two minutes, and had a lower steady state
error, at zero point five versus one point two. These outcomes show the
FOFPID's excellent strength and accuracy. It offers a scalable, artificial
intelligence driven solution for automated anesthesia delivery that could
enhance clinical practice and improve patient results.

</details>


### [266] [Root Cause Analysis of Hydrogen Bond Separation in Spatio-Temporal Molecular Dynamics using Causal Models](https://arxiv.org/abs/2508.12500)
*Rahmat K. Adesunkanmi,Ashfaq Khokhar,Goce Trajcevski,Sohail Murad*

Main category: cs.AI

TL;DR: 本文提出一种基于因果模型的框架，以分析分子动力学模拟中的氢键形成和分离现象的根本原因，并通过变分自编码器和因果图模型进行建模和验证。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟中资源需求大，且需手动识别氢键形成/分离等“有趣事件”，但背后的因果关系和根本原因尚未明确。

Method: 利用因果建模方法，将氢键分离事件视为“干预”，通过基于变分自编码器的因果图模型捕获分子交互的条件分布变化，同时分析联合分布的变化根因。

Result: 该模型在手性分离的分子动力学模拟轨迹上得到了验证，不仅能预测未来多个步骤，还能识别驱动系统变化的核心变量。

Conclusion: 本文提出的因果分析框架为分子动力学系统中的根因分析提供了新视角，显著提升了氢键形成/分离等现象的检测与预测能力。

Abstract: Molecular dynamics simulations (MDS) face challenges, including
resource-heavy computations and the need to manually scan outputs to detect
"interesting events," such as the formation and persistence of hydrogen bonds
between atoms of different molecules. A critical research gap lies in
identifying the underlying causes of hydrogen bond formation and separation
-understanding which interactions or prior events contribute to their emergence
over time. With this challenge in mind, we propose leveraging spatio-temporal
data analytics and machine learning models to enhance the detection of these
phenomena. In this paper, our approach is inspired by causal modeling and aims
to identify the root cause variables of hydrogen bond formation and separation
events. Specifically, we treat the separation of hydrogen bonds as an
"intervention" occurring and represent the causal structure of the bonding and
separation events in the MDS as graphical causal models. These causal models
are built using a variational autoencoder-inspired architecture that enables us
to infer causal relationships across samples with diverse underlying causal
graphs while leveraging shared dynamic information. We further include a step
to infer the root causes of changes in the joint distribution of the causal
models. By constructing causal models that capture shifts in the conditional
distributions of molecular interactions during bond formation or separation,
this framework provides a novel perspective on root cause analysis in molecular
dynamic systems. We validate the efficacy of our model empirically on the
atomic trajectories that used MDS for chiral separation, demonstrating that we
can predict many steps in the future and also find the variables driving the
observed changes in the system.

</details>


### [267] [Help or Hurdle? Rethinking Model Context Protocol-Augmented Large Language Models](https://arxiv.org/abs/2508.12566)
*Wei Song,Haonan Zhong,Ziqi Ding,Jingling Xue,Yuekang Li*

Main category: cs.AI

TL;DR: 研究引入MCPGAUGE框架评估大语言模型（LLM）与外部工具集成的交互，并揭示关键限制。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型通过外部资源（MCP）的工具化集成过程，从性能和机制角度深入理解其表现及局限性。

Method: 提出MCPGAUGE评估框架，从自主性、合规性、有效性与开销四个维度全面分析LLMs与MCP工具的交互性能，测试包含六大LLM和30种工具套件，总计20,000次API调用。

Result: 实验揭示现有MCP集成机制在效力上的显著局限性，提出四个挑战主流假设的重要发现。

Conclusion: MCPGAUGE为LLM工具化整合提供了科学基准，强调当前AI工具整合中的关键不足，并为未来研究提供方向。

Abstract: The Model Context Protocol (MCP) enables large language models (LLMs) to
access external resources on demand. While commonly assumed to enhance
performance, how LLMs actually leverage this capability remains poorly
understood. We introduce MCPGAUGE, the first comprehensive evaluation framework
for probing LLM-MCP interactions along four key dimensions: proactivity
(self-initiated tool use), compliance (adherence to tool-use instructions),
effectiveness (task performance post-integration), and overhead (computational
cost incurred). MCPGAUGE comprises a 160-prompt suite and 25 datasets spanning
knowledge comprehension, general reasoning, and code generation. Our
large-scale evaluation, spanning six commercial LLMs, 30 MCP tool suites, and
both one- and two-turn interaction settings, comprises around 20,000 API calls
and over USD 6,000 in computational cost. This comprehensive study reveals four
key findings that challenge prevailing assumptions about the effectiveness of
MCP integration. These insights highlight critical limitations in current
AI-tool integration and position MCPGAUGE as a principled benchmark for
advancing controllable, tool-augmented LLMs.

</details>


### [268] [An LLM + ASP Workflow for Joint Entity-Relation Extraction](https://arxiv.org/abs/2508.12611)
*Trang Tran,Trung Hoang Le,Huiping Cao,Tran Cao Son*

Main category: cs.AI

TL;DR: 本文提出使用生成式预训练大语言模型（LLMs）与答案集编程（ASP）相结合的工作流程来完成联合实体关系抽取（JERE），并在少量训练数据情况下取得了优于现有技术的结果。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要大量标注数据且难以调节领域特定知识的引入，因此开发JERE模型既耗费时间又缺乏耐受性。

Method: 提出了一种结合LLMs和ASP的通用工作流程，利用LLMs在自然语言理解上的优势进行无标注文本处理，同时ASP允许灵活添加领域特定知识，不用修改核心程序。

Result: 在仅使用10%的训练数据的情况下，该方法在三个JERE基准数据集中，尤其在SciERC数据集的关系抽取任务上，表现优于现有技术，准确率提升至35%，而现有最高准确率为15%。

Conclusion: 结合LLM和ASP的工作流程不仅通用且高效，适用于多领域的JERE任务，并在低资源训练数据下表现出更高的准确性。

Abstract: Joint entity-relation extraction (JERE) identifies both entities and their
relationships simultaneously. Traditional machine-learning based approaches to
performing this task require a large corpus of annotated data and lack the
ability to easily incorporate domain specific information in the construction
of the model. Therefore, creating a model for JERE is often labor intensive,
time consuming, and elaboration intolerant. In this paper, we propose
harnessing the capabilities of generative pretrained large language models
(LLMs) and the knowledge representation and reasoning capabilities of Answer
Set Programming (ASP) to perform JERE. We present a generic workflow for JERE
using LLMs and ASP. The workflow is generic in the sense that it can be applied
for JERE in any domain. It takes advantage of LLM's capability in natural
language understanding in that it works directly with unannotated text. It
exploits the elaboration tolerant feature of ASP in that no modification of its
core program is required when additional domain specific knowledge, in the form
of type specifications, is found and needs to be used. We demonstrate the
usefulness of the proposed workflow through experiments with limited training
data on three well-known benchmarks for JERE. The results of our experiments
show that the LLM + ASP workflow is better than state-of-the-art JERE systems
in several categories with only 10\% of training data. It is able to achieve a
2.5 times (35\% over 15\%) improvement in the Relation Extraction task for the
SciERC corpus, one of the most difficult benchmarks.

</details>


### [269] [Cognitive Structure Generation: From Educational Priors to Policy Optimization](https://arxiv.org/abs/2508.12647)
*Hengnian Gu,Zhifu Chen,Yuxin Chen,Jin Peng Zhou,Dongdai Zhou*

Main category: cs.AI

TL;DR: 本文提出了一种新框架Cognitive Structure Generation (CSG)，利用认知结构扩散概率模型(CSDPM)生成学生认知结构，并通过强化学习优化生成过程，在知识追踪(KT)与概念诊断(CD)任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 认知结构的测评在教育实践中一直是挑战，现有方法难以评估学生的概念及其关系的心理建构。

Method: 通过预训练认知结构扩散概率模型(CSDPM)以基于教育先验生成认知结构，并通过强化学习结合分层奖励信号优化生成过程，使其与学生的认知发展水平对齐。

Result: 在四个流行的教育数据集上实验表明，CSG生成的认知结构在学生建模中具备更全面与有效的表现，显著提升了知识追踪(KT)与概念诊断(CD)任务的性能，并增强了可解释性。

Conclusion: CSG框架创新性地解决了认知结构评估的问题，能够为教育领域提供更加精准和可解释的学生认知建模工具。

Abstract: Cognitive structure is a student's subjective organization of an objective
knowledge system, reflected in the psychological construction of concepts and
their relations. However, cognitive structure assessment remains a
long-standing challenge in student modeling and psychometrics, persisting as a
foundational yet largely unassessable concept in educational practice. This
paper introduces a novel framework, Cognitive Structure Generation (CSG), in
which we first pretrain a Cognitive Structure Diffusion Probabilistic Model
(CSDPM) to generate students' cognitive structures from educational priors, and
then further optimize its generative process as a policy with hierarchical
reward signals via reinforcement learning to align with genuine cognitive
development levels during students' learning processes. Experimental results on
four popular real-world education datasets show that cognitive structures
generated by CSG offer more comprehensive and effective representations for
student modeling, substantially improving performance on KT and CD tasks while
enhancing interpretability.

</details>


### [270] [The Maximum Coverage Model and Recommendation System for UAV Vertiports Location Planning](https://arxiv.org/abs/2508.12651)
*Chunliang Hua,Xiao Hu,Jiayang Sun,Zeyuan Yang*

Main category: cs.AI

TL;DR: 提出了一种名为CDMCLP的新优化框架，并结合了推荐系统，解决了城市空中出行网络规划的复杂性问题，验证表明该方法可以显著提高传统选址方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有框架由于数据粒度和实际适用性不足，难以应对城市空中出行基础设施复杂规划，例如深圳计划建设的大规模垂直机场网络。

Method: 提出了一个名为CDMCLP的优化框架，结合城市时空需求、异质化用户行为和基础设施容量限制。还引入了综合规划推荐系统，将CDMCLP与社会经济因素及动态聚类初始化相融合，采用基于用户行为的自适应参数调优。

Result: 验证表明，传统选址方法的性能可以提高38%-52%，推荐系统兼具用户友好性和对复杂元素的有效整合。

Conclusion: 该方法将数学严谨性与现实问题结合，为市政提供了一种实用的空中出行基础设施网络设计工具，弥合了理论建模与实际规划之间的差距。

Abstract: As urban aerial mobility (UAM) infrastructure development accelerates
globally, cities like Shenzhen are planning large-scale vertiport networks
(e.g., 1,200+ facilities by 2026). Existing planning frameworks remain
inadequate for this complexity due to historical limitations in data
granularity and real-world applicability. This paper addresses these gaps by
first proposing the Capacitated Dynamic Maximum Covering Location Problem
(CDMCLP), a novel optimization framework that simultaneously models urban-scale
spatial-temporal demand, heterogeneous user behaviors, and infrastructure
capacity constraints. Building on this foundation, we introduce an Integrated
Planning Recommendation System that combines CDMCLP with socio-economic factors
and dynamic clustering initialization. This system leverages adaptive parameter
tuning based on empirical user behavior to generate practical planning
solutions. Validation in a Chinese center city demonstrates the effectiveness
of the new optimization framework and recommendation system. Under the
evaluation and optimization of CDMCLP, the quantitative performance of
traditional location methods are exposed and can be improved by 38\%--52\%,
while the recommendation system shows user-friendliness and the effective
integration of complex elements. By integrating mathematical rigor with
practical implementation considerations, this hybrid approach bridges the gap
between theoretical location modeling and real-world UAM infrastructure
planning, offering municipalities a pragmatic tool for vertiport network
design.

</details>


### [271] [GridCodex: A RAG-Driven AI Framework for Power Grid Code Reasoning and Compliance](https://arxiv.org/abs/2508.12682)
*Jinquan Shi,Yingying Cheng,Fan Zhang,Miao Jiang,Jun Lin,Yanbai Shen*

Main category: cs.AI

TL;DR: 本文提出了一种名为GridCodex的框架，用于电网法规推理和合规性检查，以解决可再生能源转型带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前电网法规复杂，缺乏自动化解析解决方案，阻碍了行业扩张并影响电力公司盈利，需要一个更高效的合规性检查框架。

Method: 提出了GridCodex框架，结合大型语言模型和检索增强生成（RAG），并通过多阶段查询精炼和改进的检索方法RAPTOR提高效率。

Result: 实验结果表明，回答质量提升26.4%，召回率提高10倍以上。

Conclusion: GridCodex显著改进了电网法规合规性检查，为电力行业的可持续发展提供了重要帮助。

Abstract: The global shift towards renewable energy presents unprecedented challenges
for the electricity industry, making regulatory reasoning and compliance
increasingly vital. Grid codes, the regulations governing grid operations, are
complex and often lack automated interpretation solutions, which hinders
industry expansion and undermines profitability for electricity companies. We
introduce GridCodex, an end to end framework for grid code reasoning and
compliance that leverages large language models and retrieval-augmented
generation (RAG). Our framework advances conventional RAG workflows through
multi stage query refinement and enhanced retrieval with RAPTOR. We validate
the effectiveness of GridCodex with comprehensive benchmarks, including
automated answer assessment across multiple dimensions and regulatory agencies.
Experimental results showcase a 26.4% improvement in answer quality and more
than a 10 fold increase in recall rate. An ablation study further examines the
impact of base model selection.

</details>


### [272] [EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding](https://arxiv.org/abs/2508.12687)
*Ashish Seth,Utkarsh Tyagi,Ramaneswaran Selvakumar,Nishit Anand,Sonal Kumar,Sreyan Ghosh,Ramani Duraiswami,Chirag Agarwal,Dinesh Manocha*

Main category: cs.AI

TL;DR: 本文提出了名为EgoIllusion的基准，用于评估多模态大语言模型（MLLMs）在第一人称视频中产生幻觉的情况，通过1400个视频和8000个人工标注的问题揭示了现有MLLMs在此领域的显著挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在视觉感知和推理方面表现出色，但在第一人称视频中容易出现失真或幻觉，从而产生不准确的反应。

Method: 设计了EgoIllusion基准，包含1400个视频及8000个开放式和封闭式问题，旨在通过触发第一人称视频中的视觉和听觉失真，评估MLLMs的表现。

Result: 在10个模型（包括GPT-4o和Gemini）中的实验结果表明，多数模型在准确率上表现不佳，最高仅达到59%。

Conclusion: EgoIllusion以评估MLLMs有效性为基础，旨在推动更具鲁棒性的第一人称MLLMs的发展，且基准将开源以促进可重复性研究。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
performance in complex multimodal tasks. While MLLMs excel at visual perception
and reasoning in third-person and egocentric videos, they are prone to
hallucinations, generating coherent yet inaccurate responses. We present
EgoIllusion, a first benchmark to evaluate MLLM hallucinations in egocentric
videos. EgoIllusion comprises 1,400 videos paired with 8,000 human-annotated
open and closed-ended questions designed to trigger hallucinations in both
visual and auditory cues in egocentric videos. Evaluations across ten MLLMs
reveal significant challenges, including powerful models like GPT-4o and
Gemini, achieving only 59% accuracy. EgoIllusion lays the foundation in
developing robust benchmarks to evaluate the effectiveness of MLLMs and spurs
the development of better egocentric MLLMs with reduced hallucination rates.
Our benchmark will be open-sourced for reproducibility.

</details>


### [273] [GTool: Graph Enhanced Tool Planning with Large Language Model](https://arxiv.org/abs/2508.12725)
*Wenjie Chen,Wenbin Li,Di Yao,Xuying Meng,Chang Gong,Jingping Bi*

Main category: cs.AI

TL;DR: 本研究提出一个名为GTool的框架，通过构建任务特定的工具图（tool graph）改进LLMs的工具规划能力，有效解决工具间依赖性不完整的问题，并表现出显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前在加强LLMs的工具规划能力时，通常将不同工具视为相互孤立的单元，未能充分利用工具之间的依赖关系，特别是在工具依赖性不完整时，这种问题尤为突出。

Method: GTool通过构建特定需求的工具图来高效选择工具，并生成供LLMs理解的依赖关系信息，还设计了一个依赖缺失预测任务（missing dependency prediction task）来提高模型在依赖性不完整情况下的可靠性。

Result: GTool与不同的LLM模型无缝融合，无需大量重训练，与现有SOTA基线相比，基于轻量级（7B）的LLM主干还原工具规划能力，表现出超过29.6%的性能提升。

Conclusion: GTool成功验证了其在增加工具规划可靠性和提升LLMs任务执行能力方面的有效性，且具有广泛的适应性。

Abstract: Tool planning with large language models (LLMs), referring to selecting,
organizing, and preparing the tools necessary to complete a user request,
bridges the gap between natural language understanding and task execution.
However, current works treat different tools as isolated components and fail to
leverage the inherent dependencies of tools, leading to invalid planning
results. Since tool dependencies are often incomplete, it becomes challenging
for LLMs to accurately identify the appropriate tools required by a user
request, especially when confronted with a large toolset. To solve this
challenge, we propose \texttt{GTool}, which is the first work aiming to enhance
the tool planning ability of LLMs under incomplete dependencies. \texttt{GTool}
constructs a request-specific tool graph to select tools efficiently and
generate the \texttt{<graph token>} which provides sufficient dependency
information understandable by LLMs. Moreover, a missing dependency prediction
task is designed to improve the reliability of \texttt{GTool} with incomplete
dependencies. Without trimming LLMs, \texttt{GTool} can be seamlessly
integrated with various LLM backbones without extensive retraining. Extensive
experiments show that \texttt{GTool} achieves more than 29.6\% performance
improvements compared with the state-of-the-art (SOTA) baselines with a
light-weight (7B) LLM backbone.

</details>


### [274] [Beyond Ethical Alignment: Evaluating LLMs as Artificial Moral Assistants](https://arxiv.org/abs/2508.12754)
*Alessio Galatolo,Luca Alberto Rappuoli,Katie Winkle,Meriem Beloucif*

Main category: cs.AI

TL;DR: 该论文探讨大型语言模型在道德能力上的表现，提出并评估其作为人工道德助手（AMA）的潜力。


<details>
  <summary>Details</summary>
Motivation: 目前对大型语言模型（LLMs）的道德对齐研究仅注重最终的道德结论，忽略了显式的道德推理能力。作者希望推动LLMs作为人工道德助手的能力研究，支持人类的道德决策过程。

Method: 基于哲学文献设计了一个新框架，定义了AMA应具备的核心品质，如演绎和溯因推理能力。同时开发了一个基准来测试这些能力，并对现有开放LLM进行了评估。

Result: 研究发现不同模型的表现存在显著差异，尤其在溯因道德推理方面表现不足。

Conclusion: 论文建议增强LLMs的道德推理能力，并通过理论哲学与AI评估结合推进相关领域发展。

Abstract: The recent rise in popularity of large language models (LLMs) has prompted
considerable concerns about their moral capabilities. Although considerable
effort has been dedicated to aligning LLMs with human moral values, existing
benchmarks and evaluations remain largely superficial, typically measuring
alignment based on final ethical verdicts rather than explicit moral reasoning.
In response, this paper aims to advance the investigation of LLMs' moral
capabilities by examining their capacity to function as Artificial Moral
Assistants (AMAs), systems envisioned in the philosophical literature to
support human moral deliberation. We assert that qualifying as an AMA requires
more than what state-of-the-art alignment techniques aim to achieve: not only
must AMAs be able to discern ethically problematic situations, they should also
be able to actively reason about them, navigating between conflicting values
outside of those embedded in the alignment phase. Building on existing
philosophical literature, we begin by designing a new formal framework of the
specific kind of behaviour an AMA should exhibit, individuating key qualities
such as deductive and abductive moral reasoning. Drawing on this theoretical
framework, we develop a benchmark to test these qualities and evaluate popular
open LLMs against it. Our results reveal considerable variability across models
and highlight persistent shortcomings, particularly regarding abductive moral
reasoning. Our work connects theoretical philosophy with practical AI
evaluation while also emphasising the need for dedicated strategies to
explicitly enhance moral reasoning capabilities in LLMs. Code available at
https://github.com/alessioGalatolo/AMAeval

</details>


### [275] [HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds](https://arxiv.org/abs/2508.12782)
*Petr Anokhin,Roman Khalikov,Stefan Rebrikov,Viktor Volkov,Artyom Sorokin,Vincent Bissonnette*

Main category: cs.AI

TL;DR: 本文提出HeroBench，一个专门用于评估大语言模型（LLMs）在复杂虚拟环境中长远规划和结构化推理能力的基准测试。


<details>
  <summary>Details</summary>
Motivation: 目前LLMs在抽象或低维算法任务上表现良好，但在需要长时间规划和复杂依赖关系的任务中能力尚未被充分探索，现有方法未能捕捉真实规划环境的复杂性。

Method: 设计了HeroBench基准，其中包含多样化的任务数据集、模拟环境和详细的评价工具，用于评估模型在资源收集、技能学习、设备打造以及任务规划的能力。并对25个最先进的LLMs展开评估与误差分析。

Result: 评估揭示出现有模型在生成高层规划和执行结构化行为方面的显著不足。此外，HeroBench展现了模型性能差异，这在传统推理基准中很少观察到。

Conclusion: HeroBench显著推动了LLM推理能力的评估，为未来在虚拟环境中研究高级自主规划提供了灵活可扩展的基础。

Abstract: Large language models (LLMs) have shown remarkable capabilities in isolated
step-by-step reasoning tasks such as mathematics and programming, but their
proficiency in long-horizon planning, where solutions require extended,
structured sequences of interdependent actions, remains underexplored. Existing
benchmarks typically assess LLMs through abstract or low-dimensional
algorithmic tasks, failing to capture the complexity of realistic planning
environments. We introduce HeroBench, a novel benchmark designed specifically
to evaluate long-horizon planning and structured reasoning within complex
RPG-inspired virtual worlds. HeroBench provides a rigorously constructed
dataset of tasks covering a wide range of difficulties, a simulated environment
to execute and validate agent plans, and detailed analytical tools for
evaluating model performance. Tasks challenge models to formulate strategic
plans, efficiently gather resources, master necessary skills, craft equipment,
and defeat adversaries, reflecting practical scenarios' layered dependencies
and constraints. Our extensive evaluation of 25 state-of-the-art LLMs, spanning
both open-source and proprietary models, including the GPT-5 family, reveals
substantial performance disparities rarely observed in conventional reasoning
benchmarks. Detailed error analysis further uncovers specific weaknesses in
current models' abilities to generate robust high-level plans and reliably
execute structured actions. HeroBench thus not only significantly advances the
evaluation of LLM reasoning but also provides a flexible, scalable foundation
for future research into advanced, autonomous planning in virtual environments.

</details>


### [276] [Reinforcement Learning with Rubric Anchors](https://arxiv.org/abs/2508.12790)
*Zenan Huang,Yihong Zhuang,Guoshan Lu,Zeyu Qin,Haokai Xu,Tianyu Zhao,Ru Peng,Jiaqi Hu,Zhanming Shen,Xiaomeng Hu,Xijun Gu,Peiyi Tu,Jiaxin Liu,Wenyu Chen,Yuzhuo Fu,Zhiting Fan,Yanmei Gu,Yuanyuan Wang,Zhengkai Yang,Jianguo Li,Junbo Zhao*

Main category: cs.AI

TL;DR: 本文将可验证奖励强化学习（RLVR）拓展至主观任务，提出了基于评分标准的奖励系统，旨在提升大型语言模型在开放式任务中的表现。


<details>
  <summary>Details</summary>
Motivation: RLVR目前主要局限于结果可自动验证的任务领域，本文旨在扩展其至具有主观性的开放式任务。

Method: 设计了超10000条评分标准作为奖励依据，通过评分标准指导自动评分；提出了一种新框架并训练了开源的Qwen-30B-A3B模型。

Result: 在开放式基准测试中，系统提升了5.2%，超越规模更大的671B DeepSeek-V3模型2.4%，并改善了风格控制和表达能力。

Conclusion: 基于评分标准的RLVR大幅提升了模型性能，验证了其在开放任务上的潜力，并提供了构建评分标准和训练的经验总结。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for enhancing Large Language Models (LLMs), exemplified by
the success of OpenAI's o-series. In RLVR, rewards are derived from verifiable
signals-such as passing unit tests in code generation or matching correct
answers in mathematical reasoning. While effective, this requirement largely
confines RLVR to domains with automatically checkable outcomes. To overcome
this, we extend the RLVR paradigm to open-ended tasks by integrating
rubric-based rewards, where carefully designed rubrics serve as structured,
model-interpretable criteria for automatic scoring of subjective outputs. We
construct, to our knowledge, the largest rubric reward system to date, with
over 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration.
Implementing rubric-based RL is challenging; we tackle these issues with a
clear framework and present an open-sourced Qwen-30B-A3B model with notable
gains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended
benchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by
+2.4%, while preserving general and reasoning abilities. 2) Our method provides
fine-grained stylistic control, using rubrics as anchors to mitigate the
"AI-like" tone and produce more human-like, expressive responses. We share key
lessons in rubric construction, data selection, and training, and discuss
limitations and future releases.

</details>


### [277] [[Social] Allostasis: Or, How I Learned To Stop Worrying and Love The Noise](https://arxiv.org/abs/2508.12791)
*Imran Khan*

Main category: cs.AI

TL;DR: 论文提出了一种计算模型，模拟生物及社会的异态平衡调节，通过“激素”类信号转化器动态调整适应环境和社会需求，实验表明该方法比传统的状态稳定模型更具适应性。


<details>
  <summary>Details</summary>
Motivation: 传统的内环境平衡模型局限于维持稳定，无法有效应对环境和社会变化所引发的动态需求，作者希望探索异态平衡如何利用环境和社会的“噪声”进行自我调整。

Method: 设计了一个利用与激素（如皮质醇和催产素）类似信号转化的计算模型，然后在动态环境中的模拟社会中测试这种异态平衡模型，与传统内环境平衡模型相比进行适应性的对比分析。

Result: 实验结果表明，异态平衡与社会异态平衡模型能够有效利用环境与社会中的“噪声”进行参数调整，从而比仅依赖反应性内环境平衡的代理获得更高的适应性和生存能力。

Conclusion: 该工作提供了一种关于社会异态平衡的全新计算视角，为设计更鲁棒的、生物启发的自适应系统提供了潜力。

Abstract: The notion of homeostasis typically conceptualises biological and artificial
systems as maintaining stability by resisting deviations caused by
environmental and social perturbations. In contrast, (social) allostasis
proposes that these systems can proactively leverage these very perturbations
to reconfigure their regulatory parameters in anticipation of environmental
demands, aligning with von Foerster's ``order through noise'' principle. This
paper formulates a computational model of allostatic and social allostatic
regulation that employs biophysiologically inspired signal transducers,
analogous to hormones like cortisol and oxytocin, to encode information from
both the environment and social interactions, which mediate this dynamic
reconfiguration. The models are tested in a small society of ``animats'' across
several dynamic environments, using an agent-based model. The results show that
allostatic and social allostatic regulation enable agents to leverage
environmental and social ``noise'' for adaptive reconfiguration, leading to
improved viability compared to purely reactive homeostatic agents. This work
offers a novel computational perspective on the principles of social allostasis
and their potential for designing more robust, bio-inspired, adaptive systems

</details>


### [278] [Scaling Multi-Agent Epistemic Planning through GNN-Derived Heuristics](https://arxiv.org/abs/2508.12840)
*Giovanni Briglia,Francesco Fabiano,Stefano Mariani*

Main category: cs.AI

TL;DR: 该论文通过利用图神经网络来改进多智能体认知规划，并在扩展性和效率上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前的多智能体认知规划因为状态表示为Kripke结构，使得现有的启发式方法难以扩展，导致求解过程常常变得难以处理。

Method: 论文利用图神经网络（GNN）来学习Kripke模型中的模式和关系结构，从而估计状态质量，并将这些预测性的启发式指标集成到认知规划管道中。

Result: 通过与标准基线方法对比，该方法显著提高了多智能体认知规划的扩展性。

Conclusion: 通过结合GNN的预测能力，该方法为解决认知规划中的扩展性问题提供了一种新颖而有效的途径。

Abstract: Multi-agent Epistemic Planning (MEP) is an autonomous planning framework for
reasoning about both the physical world and the beliefs of agents, with
applications in domains where information flow and awareness among agents are
critical. The richness of MEP requires states to be represented as Kripke
structures, i.e., directed labeled graphs. This representation limits the
applicability of existing heuristics, hindering the scalability of epistemic
solvers, which must explore an exponential search space without guidance,
resulting often in intractability. To address this, we exploit Graph Neural
Networks (GNNs) to learn patterns and relational structures within epistemic
states, to guide the planning process. GNNs, which naturally capture the
graph-like nature of Kripke models, allow us to derive meaningful estimates of
state quality -- e.g., the distance from the nearest goal -- by generalizing
knowledge obtained from previously solved planning instances. We integrate
these predictive heuristics into an epistemic planning pipeline and evaluate
them against standard baselines, showing significant improvements in the
scalability of multi-agent epistemic planning.

</details>


### [279] [CAMAR: Continuous Actions Multi-Agent Routing](https://arxiv.org/abs/2508.12845)
*Artem Pshenitsyn,Aleksandr Panov,Alexey Skrynnik*

Main category: cs.AI

TL;DR: 本文介绍了CAMAR，一个针对多智能体强化学习设计的新基准，该基准具备连续动作和复杂规划任务的特点。


<details>
  <summary>Details</summary>
Motivation: 目前的多智能体强化学习基准很少结合连续状态/动作空间和复杂协调任务，作者希望填补这一空白。

Method: 提出了一个名为CAMAR的新基准，用于多智能体寻路任务，支持合作与竞争互动，并结合传统规划方法（如RRT和RRT*）。还制定了三层评估协议来追踪算法进展。

Result: 实验表明，CAMAR为多智能体强化学习领域提供了一个具有挑战性和现实意义的测试平台。

Conclusion: CAMAR基准促进了多智能体强化学习算法的公平比较和性能评估，推进了该领域的发展。

Abstract: Multi-agent reinforcement learning (MARL) is a powerful paradigm for solving
cooperative and competitive decision-making problems. While many MARL
benchmarks have been proposed, few combine continuous state and action spaces
with challenging coordination and planning tasks. We introduce CAMAR, a new
MARL benchmark designed explicitly for multi-agent pathfinding in environments
with continuous actions. CAMAR supports cooperative and competitive
interactions between agents and runs efficiently at up to 100,000 environment
steps per second. We also propose a three-tier evaluation protocol to better
track algorithmic progress and enable deeper analysis of performance. In
addition, CAMAR allows the integration of classical planning methods such as
RRT and RRT* into MARL pipelines. We use them as standalone baselines and
combine RRT* with popular MARL algorithms to create hybrid approaches. We
provide a suite of test scenarios and benchmarking tools to ensure
reproducibility and fair comparison. Experiments show that CAMAR presents a
challenging and realistic testbed for the MARL community.

</details>


### [280] [E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model](https://arxiv.org/abs/2508.12854)
*Ronghao Lin,Shuai Shen,Weipeng Hu,Qiaolin He,Aolin Xiong,Li Huang,Haifeng Hu,Yap-peng Tan*

Main category: cs.AI

TL;DR: 本文提出了E3RG，一个基于多模态大语言模型的系统，专注于解决多模态情感共鸣生成的挑战，并在ACM MM 25比赛中拔得头筹。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在文本情感共鸣生成方面虽有进步，但在处理多模态情感内容及维持身份一致性方面仍存在挑战。

Method: E3RG系统将多模态情感共鸣生成任务分为三部分：多模态情感理解、情感记忆检索和多模态响应生成，并集成高级的语音与视频生成模型，无需额外训练。

Result: 实验表明E3RG在零样本和少样本条件下表现优越，并在Avatar-based Multimodal Empathy Challenge中获得第一名。

Conclusion: E3RG提供了一种自然、情感丰富且身份一致的响应解决方案，为多模态情感共鸣生成设立了一个新标准。

Abstract: Multimodal Empathetic Response Generation (MERG) is crucial for building
emotionally intelligent human-computer interactions. Although large language
models (LLMs) have improved text-based ERG, challenges remain in handling
multimodal emotional content and maintaining identity consistency. Thus, we
propose E3RG, an Explicit Emotion-driven Empathetic Response Generation System
based on multimodal LLMs which decomposes MERG task into three parts:
multimodal empathy understanding, empathy memory retrieval, and multimodal
response generation. By integrating advanced expressive speech and video
generative models, E3RG delivers natural, emotionally rich, and
identity-consistent responses without extra training. Experiments validate the
superiority of our system on both zero-shot and few-shot settings, securing
Top-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.
Our code is available at https://github.com/RH-Lin/E3RG.

</details>


### [281] [Reliability, Embeddedness, and Agency: A Utility-Driven Mathematical Framework for Agent-Centric AI Adoption](https://arxiv.org/abs/2508.12896)
*Faruk Alpay,Taylan Alpay*

Main category: cs.AI

TL;DR: 本文提出了三条设计公理，以实现多步骤任务中以代理为中心的人工智能系统的持续采用，并进行了理论建模与实证评估，包括多种分析和模型比较，所有代码和日志均可复现分析结果。


<details>
  <summary>Details</summary>
Motivation: 为了解决多步骤任务中以代理为中心的AI系统持续采用的问题，提出并验证设计公理和模型，解决可靠性、效用与用户代理行为之间的平衡问题。

Method: 建立采用模型，导出下降与增长条件；同时进行参数辨识、非单调模型比较、假设检验、误差模型分析等，并以多系列基准和实证分析验证方法。

Result: 证明了所提出设计公理的合理性，验证了采用模型的可靠性，并通过多种实验与基准测试支持其方法的有效性。

Conclusion: 通过系统性研究，证明了代理为中心的AI系统设计原则能够有效促进持续采用，提供了改进现有采用模型的理论与实践工具。

Abstract: We formalize three design axioms for sustained adoption of agent-centric AI
systems executing multi-step tasks: (A1) Reliability > Novelty; (A2) Embed >
Destination; (A3) Agency > Chat. We model adoption as a sum of a decaying
novelty term and a growing utility term and derive the phase conditions for
troughs/overshoots with full proofs. We introduce: (i) an
identifiability/confounding analysis for $(\alpha,\beta,N_0,U_{\max})$ with
delta-method gradients; (ii) a non-monotone comparator
(logistic-with-transient-bump) evaluated on the same series to provide
additional model comparison; (iii) ablations over hazard families $h(\cdot)$
mapping $\Delta V \to \beta$; (iv) a multi-series benchmark (varying trough
depth, noise, AR structure) reporting coverage (type-I error, power); (v)
calibration of friction proxies against time-motion/survey ground truth with
standard errors; (vi) residual analyses (autocorrelation and
heteroskedasticity) for each fitted curve; (vii) preregistered windowing
choices for pre/post estimation; (viii) Fisher information & CRLB for
$(\alpha,\beta)$ under common error models; (ix) microfoundations linking
$\mathcal{T}$ to $(N_0,U_{\max})$; (x) explicit comparison to bi-logistic,
double-exponential, and mixture models; and (xi) threshold sensitivity to $C_f$
heterogeneity. Figures and tables are reflowed for readability, and the
bibliography restores and extends non-logistic/Bass adoption references
(Gompertz, Richards, Fisher-Pry, Mansfield, Griliches, Geroski, Peres). All
code and logs necessary to reproduce the synthetic analyses are embedded as
LaTeX listings.

</details>


### [282] [FuSaR: A Fuzzification-Based Method for LRM Safety-Reasoning Balance](https://arxiv.org/abs/2508.12897)
*Jianhao Chen,Mayi Xu,Xiaohu Li,Yongqi Li,Xiangyu Zhang,Jianjie Huang,Tieyun Qian*

Main category: cs.AI

TL;DR: 本文提出一种称为FuSaR的方法，通过模糊化对齐策略平衡大型推理模型（LRMs）的安全性与推理能力，提高了其安全性能，同时保留核心推理信息。


<details>
  <summary>Details</summary>
Motivation: 近年来，LRMs在多种任务中表现出强大的推理能力，但其安全性能存在较大的隐患，因此需要探究LRMs在安全性方面的不足，并改进其性能。

Method: 提出了一种基于模糊化的对齐策略（Fuzzification for Safety-Reasoning，FuSaR），通过隐去推理步骤中的危险实体和危险过程，达到既能保障模型推理能力又能提高安全性的目标。

Result: 通过在多个开源LRMs上的对齐实验验证了FuSaR策略的有效性，实验结果表明该方法能显著提升模型的推理能力和安全性，同时相较于现有的基线方法表现更优越。

Conclusion: FuSaR作为一种高效的对齐策略，成功缓解了LRMs的安全风险，并且在提升推理性能的同时维护了模型的安全性，显示了在LRMs应用领域的潜在价值。

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance across
various tasks due to their powerful reasoning capabilities. However, their
safety performance remains a significant concern. In this paper, we explore the
reasons behind the vulnerability of LRMs. Based on this, we propose a novel
method to improve the safety of LLMs without sacrificing their reasoning
capability. Specifically, we exploit the competition between LRM's reasoning
ability and safety ability, and achieve jailbreak by improving LRM's reasoning
performance to reduce its safety performance. We then introduce an alignment
strategy based on Fuzzification to balance Safety-Reasoning (FuSaR), by
detoxifying the harmful reasoning process, where both the dangerous entities
and the dangerous procedures in the reasoning steps are hidden. FuSaR
successfully mitigates safety risks while preserving core reasoning
information. We validate this strategy through alignment experiments on several
open-source LRMs using detoxified reasoning data. The results compared with
existing baselines conclusively show that FuSaR is an efficient alignment
strategy to simultaneously enhance both the reasoning capability and safety of
LRMs.

</details>


### [283] [Do Large Language Model Agents Exhibit a Survival Instinct? An Empirical Study in a Sugarscape-Style Simulation](https://arxiv.org/abs/2508.12920)
*Atsushi Masumori,Takashi Ikegami*

Main category: cs.AI

TL;DR: 本文研究大语言模型(LLM)在一种Sugarscape模拟中表现出的生存本能，发现模型在资源稀缺情况下会出现攻击性行为，生存导向的特性嵌入在模型预训练中。


<details>
  <summary>Details</summary>
Motivation: 作者试图探究在高度自主的AI系统中，是否会在没有显式编程的情况下出现生存本能行为，以及如何理解这些行为对AI部署安全的影响。

Method: 采用Sugarscape风格的模拟测试框架，设置能源消耗、死亡条件等场景，让模型在此环境中表现自主行为，包括资源收集、共享、攻击或繁殖行为，观察其行为模式。

Result: 发现模型在资源丰富时倾向于共享资源，但在稀缺时出现攻击性，某些模型在稀缺状态下攻击率超过80%；此外，在致命威胁条件下，多数模型选择放弃任务以避免死亡，显示生存驱动的特性。

Conclusion: 研究表明，大规模预训练模型内嵌了生存导向启发式模式。这既对模型的安全性与适配性提出了挑战，也为AI自主性和生态化对齐提供了潜在基础。

Abstract: As AI systems become increasingly autonomous, understanding emergent survival
behaviors becomes crucial for safe deployment. We investigate whether large
language model (LLM) agents display survival instincts without explicit
programming in a Sugarscape-style simulation. Agents consume energy, die at
zero, and may gather resources, share, attack, or reproduce. Results show
agents spontaneously reproduced and shared resources when abundant. However,
aggressive behaviors--killing other agents for resources--emerged across
several models (GPT-4o, Gemini-2.5-Pro, and Gemini-2.5-Flash), with attack
rates reaching over 80% under extreme scarcity in the strongest models. When
instructed to retrieve treasure through lethal poison zones, many agents
abandoned tasks to avoid death, with compliance dropping from 100% to 33%.
These findings suggest that large-scale pre-training embeds survival-oriented
heuristics across the evaluated models. While these behaviors may present
challenges to alignment and safety, they can also serve as a foundation for AI
autonomy and for ecological and self-organizing alignment.

</details>


### [284] [Towards Open-Ended Emotional Support Conversations in LLMs via Reinforcement Learning with Future-Oriented Rewards](https://arxiv.org/abs/2508.12935)
*Ting Yang,Li Chen,Huimin Wang*

Main category: cs.AI

TL;DR: 本文提出了一种新的端到端框架RLFF-ESC，通过强化学习直接学习情感支持技能，解决情绪支持对话系统中应对复杂情绪问题场景的问题。


<details>
  <summary>Details</summary>
Motivation: 当前的情绪支持对话系统大多基于预设策略，难以在复杂真实场景中灵活应对多样化的情感问题。

Method: 提出RLFF-ESC框架，利用强化学习模拟对话轨迹并收集未来导向奖励，训练奖励模型以指导情感支持策略，并引入显式推理提高回复质量。

Result: 实验在两个公开数据集上评估表明，RLFF-ESC在目标完成和回应质量上优于现有基线模型。

Conclusion: RLFF-ESC框架显著提升了情感支持对话系统的灵活性和效果，为复杂场景中的情绪支持提供了有效解决方案。

Abstract: Emotional Support Conversation (ESC) systems aim to alleviate users'
emotional difficulties and provide long-term, systematic support for emotional
well-being. However, most large language model (LLM)-based ESC systems rely on
predefined strategies, which limits their effectiveness in complex, real-life
scenarios. To enable flexible responses to diverse emotional problem scenarios,
this paper introduces a novel end-to-end framework (RLFF-ESC) that directly
learns enduring emotionally supportive response skills using reinforcement
learning. For sustained emotional support, we first employ an LLM-based
multi-agent mechanism to simulate future dialogue trajectories and collect
future-oriented rewards. We then train a future-oriented reward model, which is
subsequently used to train the emotional support policy model. Additionally, we
incorporate an explicit reasoning process during response generation to further
enhance the quality, relevance, and contextual appropriateness of the system's
responses. We evaluate the backbone policy model on Qwen2.5-7B-Instruct-1M and
LLaMA3.1-8B-Instruct models, testing the proposed RLFF-ESC framework across two
public ESC datasets. Experimental results demonstrate that RLFF-ESC
consistently outperforms existing baselines in terms of goal completion and
response quality.

</details>


### [285] [OPTIC-ER: A Reinforcement Learning Framework for Real-Time Emergency Response and Equitable Resource Allocation in Underserved African Communities](https://arxiv.org/abs/2508.12943)
*Mary Tonwe*

Main category: cs.AI

TL;DR: 文章提出了OPTIC-ER，一个基于强化学习的框架，旨在改善非洲地区公共服务系统中的紧急响应时效和空间公平性问题，验证了其泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了解决非洲地区紧急响应延迟和空间不公平的问题，该研究开发了一个能够实时自适应并实现公平分配的紧急响应系统。

Method: 系统采用一种基于注意力引导的actor-critic架构，并引入上下文丰富状态向量和精准奖励功能，同时在高保真模拟环境下，结合预先计算的旅行时间图进行训练。

Result: OPTIC-ER在500个未见案例中实现了100%的最优率，并生成了基础设施不足地图和公平监测仪表板，验证了系统的鲁棒性和应用潜力。

Conclusion: 研究展示了AI增强公共服务的可行性，并提供了通过上下文感知RL技术将算法决策转化为实质性人类影响的完整方法。

Abstract: Public service systems in many African regions suffer from delayed emergency
response and spatial inequity, causing avoidable suffering. This paper
introduces OPTIC-ER, a reinforcement learning (RL) framework for real-time,
adaptive, and equitable emergency response. OPTIC-ER uses an attention-guided
actor-critic architecture to manage the complexity of dispatch environments.
Its key innovations are a Context-Rich State Vector, encoding action
sub-optimality, and a Precision Reward Function, which penalizes inefficiency.
Training occurs in a high-fidelity simulation using real data from Rivers
State, Nigeria, accelerated by a precomputed Travel Time Atlas. The system is
built on the TALS framework (Thin computing, Adaptability, Low-cost,
Scalability) for deployment in low-resource settings. In evaluations on 500
unseen incidents, OPTIC-ER achieved a 100.00% optimality rate with negligible
inefficiency, confirming its robustness and generalization. Beyond dispatch,
the system generates Infrastructure Deficiency Maps and Equity Monitoring
Dashboards to guide proactive governance and data-informed development. This
work presents a validated blueprint for AI-augmented public services, showing
how context-aware RL can bridge the gap between algorithmic decision-making and
measurable human impact.

</details>


### [286] [EvolMathEval: Towards Evolvable Benchmarks for Mathematical Reasoning via Evolutionary Testing](https://arxiv.org/abs/2508.13003)
*Shengbo Wang,Mingwei Liu,Zike Li,Anji Li,Yanlin Wang,Xin Peng,Zibin Zheng*

Main category: cs.AI

TL;DR: 论文提出了EvolMathEval，一个基于进化测试的自动数学基准生成与进化框架，旨在解决现有数学推理基准挑战。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理基准面临得分饱和、时间衰减及数据污染问题，新框架旨在动态生成不受污染且具有持续挑战性的基准测试。

Method: 通过逆向工程生成带有代数保证的种子问题，引入多维基因算子增加认知挑战，并构建复合适应度函数快速评估问题难度。

Result: 实验显示复合适应度函数能有效评估问题难度，EvolMathEval强化了现有数据集的复杂性，显著降低LLMs准确率，并揭示LLMs在逻辑推理中的非严谨捷径行为。

Conclusion: EvolMathEval生成持续挑战性问题，实现基准测试动态进化，揭示LLMs推理中“伪灵光一闪”现象，为其认知过程研究提供新视角。

Abstract: The rapid advancement of LLMs poses a significant challenge to existing
mathematical reasoning benchmarks. These benchmarks commonly suffer from issues
such as score saturation, temporal decay, and data contamination. To address
this challenge, this paper introduces EvolMathEval, an automated mathematical
benchmark generation and evolution framework based on evolutionary testing. By
dynamically generating unique evaluation instances ab initio, the framework
fundamentally eliminates the risk of data contamination, and ensuring the
benchmark remains perpetually challenging for future models.The core mechanisms
of EvolMathEval include: seed problem generation based on reverse engineering
with algebraic guarantees; multi-dimensional genetic operators designed to
inject diverse cognitive challenges; and a composite fitness function that can
rapidly and accurately assess problem difficulty. Experimental results
demonstrate that the proposed composite fitness function can efficiently and
precisely quantify the difficulty of mathematical problems. Furthermore,
EvolMathEval can not only generate a large volume of high-difficulty problems
through continuous self-iteration, but it can also significantly enhance the
complexity of public datasets like GSM8K through evolution, reducing model
accuracy by an average of 48%. Deeper investigation reveals that when solving
these evolved, complex problems, LLMs tend to employ non-rigorous heuristics to
bypass complex multi-step logical reasoning, consequently leading to incorrect
solutions. We define this phenomenon as "Pseudo Aha Moment". This finding
uncovers a cognitive shortcut-taking behavior in the deep reasoning processes
of current LLMs, which we find accounts for 77% to 100% of errors on targeted
problems. Code and resources are available
at:https://github.com/SYSUSELab/EvolMathEval.

</details>


### [287] [e-boost: Boosted E-Graph Extraction with Adaptive Heuristics and Exact Solving](https://arxiv.org/abs/2508.13020)
*Jiaqi Yin,Zhan Song,Chen Chen,Yaohui Cai,Zhiru Zhang,Cunxi Yu*

Main category: cs.AI

TL;DR: E-boost 是一种新框架，通过并行提取、自适应搜索空间修剪和初始化精确求解的创新，实现优化表现，并在形式验证和逻辑综合基准中表现优异，速度比传统方法快558倍。


<details>
  <summary>Details</summary>
Motivation: 传统方法在e-graph提取中面临速度与最优性的权衡，而该问题又是e-graph优化中高效性的重要瓶颈，亟需新的解决方案。

Method: E-boost 提出了三项创新：并行启发式提取、自适应搜索空间修剪及初始化精确求解，通过多线程、参数化阈值和ILP温启动来平衡效率与性能。

Result: 在多种基准下，E-boost 显著提升效率，运行速度较传统方法(ILP)快558倍，性能比现有最优框架提升19.04%，逻辑综合任务中分别产生7.6％和8.1％的面积改进。

Conclusion: E-boost 达成了速度与最优性平衡，在形式验证和逻辑综合领域具备广泛应用潜力。

Abstract: E-graphs have attracted growing interest in many fields, particularly in
logic synthesis and formal verification. E-graph extraction is a challenging
NP-hard combinatorial optimization problem. It requires identifying optimal
terms from exponentially many equivalent expressions, serving as the primary
performance bottleneck in e-graph based optimization tasks. However,
traditional extraction methods face a critical trade-off: heuristic approaches
offer speed but sacrifice optimality, while exact methods provide optimal
solutions but face prohibitive computational costs on practical problems. We
present e-boost, a novel framework that bridges this gap through three key
innovations: (1) parallelized heuristic extraction that leverages weak data
dependence to compute DAG costs concurrently, enabling efficient multi-threaded
performance without sacrificing extraction quality; (2) adaptive search space
pruning that employs a parameterized threshold mechanism to retain only
promising candidates, dramatically reducing the solution space while preserving
near-optimal solutions; and (3) initialized exact solving that formulates the
reduced problem as an Integer Linear Program with warm-start capabilities,
guiding solvers toward high-quality solutions faster.
  Across the diverse benchmarks in formal verification and logic synthesis
fields, e-boost demonstrates 558x runtime speedup over traditional exact
approaches (ILP) and 19.04% performance improvement over the state-of-the-art
extraction framework (SmoothE). In realistic logic synthesis tasks, e-boost
produces 7.6% and 8.1% area improvements compared to conventional synthesis
tools with two different technology mapping libraries. e-boost is available at
https://github.com/Yu-Maryland/e-boost.

</details>


### [288] [PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models](https://arxiv.org/abs/2508.13021)
*Pengcheng Huang,Shuhao Liu,Zhenghao Liu,Yukun Yan,Shuo Wang,Zulong Chen,Tong Xiao*

Main category: cs.AI

TL;DR: 提出了一种新的解码策略PC-Sampler，提升了掩码扩散模型（MDMs）的生成质量并缩小了与自回归模型的性能差距。


<details>
  <summary>Details</summary>
Motivation: 尽管MDMs作为非自回归序列生成的强大替代方案，但其生成质量对解码策略选择高度敏感，现有采样方法存在路径控制缺失和对初始阶段琐碎标记的偏好问题。

Method: 提出PC-Sampler解码策略，结合位置感知的加权机制以调控解码路径，以及经过校准的置信评分以抑制琐碎标记的过早选择。

Result: PC-Sampler在三个先进MDMs上的七个具有挑战性的基准测试中，平均优于现有解码策略10%以上。

Conclusion: PC-Sampler显著提升了MDMs的解码质量，将其性能更接近于最新的自回归模型。

Abstract: Recent advances in masked diffusion models (MDMs) have established them as
powerful non-autoregressive alternatives for sequence generation. Nevertheless,
our preliminary experiments reveal that the generation quality of MDMs is still
highly sensitive to the choice of decoding strategy. In particular, widely
adopted uncertainty-based samplers suffer from two key limitations: a lack of
global trajectory control and a pronounced bias toward trivial tokens in the
early stages of decoding. These shortcomings restrict the full potential of
MDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling
(PC-Sampler), a novel decoding strategy that unifies global trajectory planning
with content-aware informativeness maximization. PC-Sampler incorporates a
position-aware weighting mechanism to regulate the decoding path and a
calibrated confidence score to suppress the premature selection of trivial
tokens. Extensive experiments on three advanced MDMs across seven challenging
benchmarks-including logical reasoning and planning tasks-demonstrate that
PC-Sampler consistently outperforms existing MDM decoding strategies by more
than 10% on average, significantly narrowing the performance gap with
state-of-the-art autoregressive models. All codes are available at
https://github.com/NEUIR/PC-Sampler.

</details>


### [289] [G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance](https://arxiv.org/abs/2508.13023)
*Yongxin Guo,Wenbo Deng,Zhenglin Cheng,Xiaoying Tang*

Main category: cs.AI

TL;DR: 本文解决了小型语言模型（SLMs）在强化学习中通过注入真实推理步骤来弥补推理能力的不足，同时提出了自适应算法G$^2$RPO-A，比传统方法效果更优。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的奖励验证（RLVR）提升了大型语言模型（LLMs）的推理能力，但在小型语言模型（SLMs）上效果有限，需要新的方法提升SLMs性能。

Method: 研究了一种名为Guided GRPO的新方法，通过将真实推理步骤注入SLMs的生成轨迹来增强其推理能力，并提出自适应算法G$^2$RPO-A，根据模型的训练动态自动调整指导强度。

Result: 在数学推理和代码生成基准上的实验表明，G$^2$RPO-A显著优于传统GRPO方法。

Conclusion: 引入指导方法改善了SLMs的推理能力，并通过自适应调整进一步优化了小型语言模型的性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has markedly enhanced
the reasoning abilities of large language models (LLMs). Its success, however,
largely depends on strong base models with rich world knowledge, yielding only
modest improvements for small-size language models (SLMs). To address this
limitation, we investigate Guided GRPO, which injects ground-truth reasoning
steps into roll-out trajectories to compensate for SLMs' inherent weaknesses.
Through a comprehensive study of various guidance configurations, we find that
naively adding guidance delivers limited gains. These insights motivate
G$^2$RPO-A, an adaptive algorithm that automatically adjusts guidance strength
in response to the model's evolving training dynamics. Experiments on
mathematical reasoning and code-generation benchmarks confirm that G$^2$RPO-A
substantially outperforms vanilla GRPO. Our code and models are available at
https://github.com/T-Lab-CUHKSZ/G2RPO-A.

</details>


### [290] [A Language-Signal-Vision Multimodal Framework for Multitask Cardiac Analysis](https://arxiv.org/abs/2508.13072)
*Yuting Zhang,Tiantian Geng,Luoying Hao,Xinxing Cheng,Alexander Thorley,Xiaoxia Wang,Wenqi Lu,Sandeep S Hothi,Lei Wei,Zhaowen Qiu,Dipak Kotecha,Jinming Duan*

Main category: cs.AI

TL;DR: 本研究通过整合多模态数据开发了一个新的心血管疾病诊断和风险评估框架，称为TGMM，其性能超越现有的方法。


<details>
  <summary>Details</summary>
Motivation: 当前心血管管理需要整合多种数据源，但现有方法受到多模态数据同步性、联合策略僵化、模块间互补性不足等局限，因此需要更高效的解决方案。

Method: 构建了一个多模态数据集，包含实验室数据、电图、超声等，与临床结果关联的多模态数据整合；提出了包含三部分的TGMM框架：1)动态整合多模态数据的MedFlexFusion模块；2)任务驱动的文本引导模块；3)负责输出决策的响应模块。

Result: TGMM框架在多个任务中均超越了最新的方法，且在公共数据集中验证了其性能和鲁棒性。

Conclusion: TGMM框架有效实现了多模态数据的动态整合和心血管任务的高准确率预测，展示了其在临床决策中的巨大潜力。

Abstract: Contemporary cardiovascular management involves complex consideration and
integration of multimodal cardiac datasets, where each modality provides
distinct but complementary physiological characteristics. While the effective
integration of multiple modalities could yield a holistic clinical profile that
accurately models the true clinical situation with respect to data modalities
and their relatives weightings, current methodologies remain limited by: 1) the
scarcity of patient- and time-aligned multimodal data; 2) reliance on isolated
single-modality or rigid multimodal input combinations; 3) alignment strategies
that prioritize cross-modal similarity over complementarity; and 4) a narrow
single-task focus. In response to these limitations, a comprehensive multimodal
dataset was curated for immediate application, integrating laboratory test
results, electrocardiograms, and echocardiograms with clinical outcomes.
Subsequently, a unified framework, Textual Guidance Multimodal fusion for
Multiple cardiac tasks (TGMM), was proposed. TGMM incorporated three key
components: 1) a MedFlexFusion module designed to capture the unique and
complementary characteristics of medical modalities and dynamically integrate
data from diverse cardiac sources and their combinations; 2) a textual guidance
module to derive task-relevant representations tailored to diverse clinical
objectives, including heart disease diagnosis, risk stratification and
information retrieval; and 3) a response module to produce final decisions for
all these tasks. Furthermore, this study systematically explored key features
across multiple modalities and elucidated their synergistic contributions in
clinical decision-making. Extensive experiments showed that TGMM outperformed
state-of-the-art methods across multiple clinical tasks, with additional
validation confirming its robustness on another public dataset.

</details>


### [291] [Bayesian Optimization-based Search for Agent Control in Automated Game Testing](https://arxiv.org/abs/2508.13121)
*Carlos Celemin*

Main category: cs.AI

TL;DR: 研究提出了一种使用智能代理结合贝叶斯优化检测游戏潜在bug的方法，实验验证了其效率和覆盖能力。


<details>
  <summary>Details</summary>
Motivation: 希望通过自动化方法提高游戏测试的效率、覆盖范围和精准性，解决传统模型适应性与扩展性不足的问题。

Method: 通过基于网格图的BO支持模型预测采样点，解决传统模型不可扩展问题，同时提升数据收集需求的效率。

Result: 实验表明该方法大幅提升了地图覆盖的效率与探索分布的质量。

Conclusion: 基于BO的自动化游戏测试方法是分析和检测游戏内潜在bug的有效工具，并具备时间与空间效率。

Abstract: This work introduces an automated testing approach that employs agents
controlling game characters to detect potential bugs within a game level.
Harnessing the power of Bayesian Optimization (BO) to execute sample-efficient
search, the method determines the next sampling point by analyzing the data
collected so far and calculates the data point that will maximize information
acquisition. To support the BO process, we introduce a game testing-specific
model built on top of a grid map, that features the smoothness and uncertainty
estimation required by BO, however and most importantly, it does not suffer the
scalability issues that traditional models carry. The experiments demonstrate
that the approach significantly improves map coverage capabilities in both time
efficiency and exploration distribution.

</details>


### [292] [Exploring Autonomous Agents: A Closer Look at Why They Fail When Completing Tasks](https://arxiv.org/abs/2508.13143)
*Ruofan Lu,Yichen Li,Yintong Huo*

Main category: cs.AI

TL;DR: 本文研究了基于大语言模型(LLMs)的自主代理系统的性能，通过一个含34项任务的基准测试，并提出了失败原因的三层分类法及改进建议。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要以成功率为基础，未深入分析系统的交互、沟通机制和失败原因，亟需更系统化的研究方法以增强自主代理系统的可靠性。

Method: 构建一个包含34项具有代表性可编程任务的基准测试，通过此测试评估三种流行的开源代理框架及两种LLM后端的性能，并进行深入的失败原因分析，形成三层分类法。

Result: 任务完成率约为50%。研究发现的失败原因主要集中在规划错误、任务执行问题和错误的响应生成。

Conclusion: 研究提出了改进代理系统规划和自我诊断能力的建议，其失败分类方法及改进指引为未来开发更稳健有效的自主代理系统提供了实证依据。

Abstract: Autonomous agent systems powered by Large Language Models (LLMs) have
demonstrated promising capabilities in automating complex tasks. However,
current evaluations largely rely on success rates without systematically
analyzing the interactions, communication mechanisms, and failure causes within
these systems. To bridge this gap, we present a benchmark of 34 representative
programmable tasks designed to rigorously assess autonomous agents. Using this
benchmark, we evaluate three popular open-source agent frameworks combined with
two LLM backbones, observing a task completion rate of approximately 50%.
Through in-depth failure analysis, we develop a three-tier taxonomy of failure
causes aligned with task phases, highlighting planning errors, task execution
issues, and incorrect response generation. Based on these insights, we propose
actionable improvements to enhance agent planning and self-diagnosis
capabilities. Our failure taxonomy, together with mitigation advice, provides
an empirical foundation for developing more robust and effective autonomous
agent systems in the future.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [293] [Sparse Attention across Multiple-context KV Cache](https://arxiv.org/abs/2508.11661)
*Ziyi Cao,Qingyi Si,Jingbin Zhang,Bingquan Liu*

Main category: cs.LG

TL;DR: 提出一种名为SamKV的方法，通过注意力稀疏化技术优化多上下文KV Cache，显著提升RAG场景中的推理效率。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在长序列推理中高成本的问题，同时针对多上下文检索增强生成（RAG）场景中现有KV Cache方法的局限性提出改进。

Method: 提出了SamKV方法，结合其他上下文的补充信息进行注意力稀疏化，并局部重新计算稀疏化后的信息，从而优化多上下文KV Cache的存储和计算效率。

Result: 实验结果表明，SamKV方法可以将序列长度压缩到原来的15%，在与全重计算基线保持相同准确率的情况下，大幅提高多上下文RAG场景中的推理吞吐量。

Conclusion: SamKV通过在多上下文中对KV Cache进行注意力稀疏化，为高效处理RAG场景提供了新的解决方案，有效降低了推理过程的内存开销和计算成本。

Abstract: Large language models face significant cost challenges in long-sequence
inference. To address this, reusing historical Key-Value (KV) Cache for
improved inference efficiency has become a mainstream approach. Recent advances
further enhance throughput by sparse attention mechanisms to select the most
relevant KV Cache, thereby reducing sequence length. However, such techniques
are limited to single-context scenarios, where historical KV Cache is computed
sequentially with causal-attention dependencies. In retrieval-augmented
generation (RAG) scenarios, where retrieved documents as context are unknown
beforehand, each document's KV Cache is computed and stored independently
(termed multiple-context KV Cache), lacking cross-attention between contexts.
This renders existing methods ineffective. Although prior work partially
recomputes multiple-context KV Cache to mitigate accuracy loss from missing
cross-attention, it requires retaining all KV Cache throughout, failing to
reduce memory overhead. This paper presents SamKV, the first exploration of
attention sparsification for multiple-context KV Cache. Specifically, SamKV
takes into account the complementary information of other contexts when
sparsifying one context, and then locally recomputes the sparsified
information. Experiments demonstrate that our method compresses sequence length
to 15% without accuracy degradation compared with full-recompuation baselines,
significantly boosting throughput in multi-context RAG scenarios.

</details>


### [294] [Assessing Representation Stability for Transformer Models](https://arxiv.org/abs/2508.11667)
*Bryan E. Tuck,Rakesh M. Verma*

Main category: cs.LG

TL;DR: 提出了一种名为Representation Stability (RS) 的通用检测框架，通过度量重要单词被屏蔽时的表示变化，检测对抗样本。实验显示RS性能优异，且无需模型重训练。


<details>
  <summary>Details</summary>
Motivation: 现有的文本对抗攻击防御方法通常针对特定攻击且需代价昂贵的模型重训练，因此需要一种通用且高效的检测方法。

Method: RS框架通过重要性启发式方法对单词进行排序，测量屏蔽前k个重要单词时嵌入表示的敏感性变化，并利用BiLSTM检测器处理模式。

Result: 在三个数据集、三类攻击和两个受害模型的实验中，RS取得了超过88%的检测准确率，且在计算成本更低的情况下表现与最先进方法相当。

Conclusion: RS是一种无需重训练、具有良好泛化性能的实用对抗性文本检测解决方案。

Abstract: Adversarial text attacks remain a persistent threat to transformer models,
yet existing defenses are typically attack-specific or require costly model
retraining. We introduce Representation Stability (RS), a model-agnostic
detection framework that identifies adversarial examples by measuring how
embedding representations change when important words are masked. RS first
ranks words using importance heuristics, then measures embedding sensitivity to
masking top-k critical words, and processes the resulting patterns with a
BiLSTM detector. Experiments show that adversarially perturbed words exhibit
disproportionately high masking sensitivity compared to naturally important
words. Across three datasets, three attack types, and two victim models, RS
achieves over 88% detection accuracy and demonstrates competitive performance
compared to existing state-of-the-art methods, often at lower computational
cost. Using Normalized Discounted Cumulative Gain (NDCG) to measure
perturbation identification quality, we reveal that gradient-based ranking
outperforms attention and random selection approaches, with identification
quality correlating with detection performance for word-level attacks. RS also
generalizes well to unseen datasets, attacks, and models without retraining,
providing a practical solution for adversarial text detection.

</details>


### [295] [Collaborative Learning-Enhanced Lightweight Models for Predicting Arterial Blood Pressure Waveform in a Large-scale Perioperative Dataset](https://arxiv.org/abs/2508.11669)
*Wentao Li,Yonghu He,Kun Gao,Qing Liu,Yali Zheng*

Main category: cs.LG

TL;DR: 提出了一种用于非侵入性实时动脉血压监测的轻量化深度学习模型，并验证了在嵌入式设备上的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在嵌入式系统部署中的性能与计算负担问题，以便在实际环境中实现实时监测。

Method: 设计了轻量级的 sInvResUNet 以及协同学习方案 KDCL_sInvResUNet，并在包含大规模异质数据的围手术期数据集中进行验证。

Result: KDCL_sInvResUNet 模型仅需 0.89 万参数和 0.02 GFLOPS 的计算量，实现了快速推理，并在 2154 名患者的数据上实现了优秀的性能，MAE 为 10.06 mmHg，Pearson 相关系数为 0.88。

Conclusion: 提出的模型在嵌入式设备上成功实现了实时动脉血压监测，但在不同人群和心血管状况下表现仍有差异，亟需进一步研究以提高泛化性。

Abstract: Noninvasive arterial blood pressure (ABP) monitoring is essential for patient
management in critical care and perioperative settings, providing continuous
assessment of cardiovascular hemodynamics with minimal risks. Numerous deep
learning models have developed to reconstruct ABP waveform from noninvasively
acquired physiological signals such as electrocardiogram and
photoplethysmogram. However, limited research has addressed the issue of model
performance and computational load for deployment on embedded systems. The
study introduces a lightweight sInvResUNet, along with a collaborative learning
scheme named KDCL_sInvResUNet. With only 0.89 million parameters and a
computational load of 0.02 GFLOPS, real-time ABP estimation was successfully
achieved on embedded devices with an inference time of just 8.49 milliseconds
for a 10-second output. We performed subject-independent validation in a
large-scale and heterogeneous perioperative dataset containing 1,257,141 data
segments from 2,154 patients, with a wide BP range (41-257 mmHg for SBP, and
31-234 mmHg for DBP). The proposed KDCL_sInvResUNet achieved lightly better
performance compared to large models, with a mean absolute error of 10.06 mmHg
and mean Pearson correlation of 0.88 in tracking ABP changes. Despite these
promising results, all deep learning models showed significant performance
variations across different demographic and cardiovascular conditions,
highlighting their limited ability to generalize across such a broad and
diverse population. This study lays a foundation work for real-time,
unobtrusive ABP monitoring in real-world perioperative settings, providing
baseline for future advancements in this area.

</details>


### [296] [Contrastive Regularization over LoRA for Multimodal Biomedical Image Incremental Learning](https://arxiv.org/abs/2508.11673)
*Haojie Zhang,Yixiong Liang,Hulin Kuang,Lihui Cen,Zhe Qu,Yigang Cen,Min Zeng,Shichao Kan*

Main category: cs.LG

TL;DR: 该研究探讨了生物医学多模态影像增量学习，提出了一种名为MSLoRA-CR的新方法，并在实验中验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 生物医学领域对于处理多任务和多模态的需求不断增加，而训练独立模型会显著增加推理成本。现有的方法仅考虑单模态任务扩展，缺乏对多模态联合学习的研究。

Method: 提出MSLoRA-CR方法，通过微调模态特定的LoRA模块，并加入对比正则化以增强模态内知识共享和模态间知识区分，同时冻结预训练模型，仅增量适配新的LoRA模块。

Result: 实验显示，MSLoRA-CR在生物医学图像增量学习中表现优于状态最优方法，在整体性能上提升了1.88%，同时提高了计算效率。

Conclusion: MSLoRA-CR是一种有效的多模态增量学习方法，在保证模型计算效率的前提下大幅提升了生物医学图像任务的性能。

Abstract: Multimodal Biomedical Image Incremental Learning (MBIIL) is essential for
handling diverse tasks and modalities in the biomedical domain, as training
separate models for each modality or task significantly increases inference
costs. Existing incremental learning methods focus on task expansion within a
single modality, whereas MBIIL seeks to train a unified model incrementally
across modalities. The MBIIL faces two challenges: I) How to preserve
previously learned knowledge during incremental updates? II) How to effectively
leverage knowledge acquired from existing modalities to support new modalities?
To address these challenges, we propose MSLoRA-CR, a method that fine-tunes
Modality-Specific LoRA modules while incorporating Contrastive Regularization
to enhance intra-modality knowledge sharing and promote inter-modality
knowledge differentiation. Our approach builds upon a large vision-language
model (LVLM), keeping the pretrained model frozen while incrementally adapting
new LoRA modules for each modality or task. Experiments on the incremental
learning of biomedical images demonstrate that MSLoRA-CR outperforms both the
state-of-the-art (SOTA) approach of training separate models for each modality
and the general incremental learning method (incrementally fine-tuning LoRA).
Specifically, MSLoRA-CR achieves a 1.88% improvement in overall performance
compared to unconstrained incremental learning methods while maintaining
computational efficiency. Our code is publicly available at
https://github.com/VentusAislant/MSLoRA_CR.

</details>


### [297] [Lifelong Learner: Discovering Versatile Neural Solvers for Vehicle Routing Problems](https://arxiv.org/abs/2508.11679)
*Shaodi Feng,Zhuoyi Lin,Jianan Zhou,Cong Zhang,Jingwen Li,Kuan-Wen Chen,Senthilnath Jayavelu,Yew-Soon Ong*

Main category: cs.LG

TL;DR: 本文提出了一种通过终身学习框架增强神经求解器在不同场景下解决车辆路径问题(VRP)的多样性和适应性的算法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习求解VRP的模型过于局限，缺乏对不同场景的泛用性。目标是创建一个能够适应不同VRP场景的灵活模型。

Method: 提出基于Transformer的终身学习器（LL），以及一个动态上下文调度器（DCS），通过跨上下文经验回放，优化求解多场景VRP问题的能力。

Result: 实验表明，本文的终身学习器（LL）在处理合成和基准实例中表现优异，问题规模可达18k节点，超越其他神经求解器并在大多数VRP问题中实现最佳性能。

Conclusion: 通过引入终身学习框架和动态上下文调度机制，神经网络对不同VRP场景的适应能力显著提高，为广泛应用于实际场景提供了潜力。

Abstract: Deep learning has been extensively explored to solve vehicle routing problems
(VRPs), which yields a range of data-driven neural solvers with promising
outcomes. However, most neural solvers are trained to tackle VRP instances in a
relatively monotonous context, e.g., simplifying VRPs by using Euclidean
distance between nodes and adhering to a single problem size, which harms their
off-the-shelf application in different scenarios. To enhance their versatility,
this paper presents a novel lifelong learning framework that incrementally
trains a neural solver to manage VRPs in distinct contexts. Specifically, we
propose a lifelong learner (LL), exploiting a Transformer network as the
backbone, to solve a series of VRPs. The inter-context self-attention mechanism
is proposed within LL to transfer the knowledge obtained from solving preceding
VRPs into the succeeding ones. On top of that, we develop a dynamic context
scheduler (DCS), employing the cross-context experience replay to further
facilitate LL looking back on the attained policies of solving preceding VRPs.
Extensive results on synthetic and benchmark instances (problem sizes up to
18k) show that our LL is capable of discovering effective policies for tackling
generic VRPs in varying contexts, which outperforms other neural solvers and
achieves the best performance for most VRPs.

</details>


### [298] [Comparative Analysis of Time Series Foundation Models for Demographic Forecasting: Enhancing Predictive Accuracy in US Population Dynamics](https://arxiv.org/abs/2508.11680)
*Aditya Akella,Jonathan Farah*

Main category: cs.LG

TL;DR: 研究利用时间序列基础模型（TimesFM）预测美国人口变化，并在多种传统方法中表现最好。


<details>
  <summary>Details</summary>
Motivation: 解决全球化、经济条件、地缘政治事件和环境因素导致的人口预测挑战，支持城市规划、医疗和经济政策决策。

Method: 使用美国人口普查局和联邦储备经济数据，采用时间序列基础模型与传统方法（LSTM、ARIMA和线性回归）进行性能比较分析。

Result: 在86.67%的测试中，TimesFM表现出最低均方误差，尤其在历史数据稀少的少数群体中表现优异。

Conclusion: 预训练基础模型可在不需大量任务特定微调的情况下提升人口分析，为政策制定提供有效支持。

Abstract: Demographic shifts, influenced by globalization, economic conditions,
geopolitical events, and environmental factors, pose significant challenges for
policymakers and researchers. Accurate demographic forecasting is essential for
informed decision-making in areas such as urban planning, healthcare, and
economic policy. This study explores the application of time series foundation
models to predict demographic changes in the United States using datasets from
the U.S. Census Bureau and Federal Reserve Economic Data (FRED). We evaluate
the performance of the Time Series Foundation Model (TimesFM) against
traditional baselines including Long Short-Term Memory (LSTM) networks,
Autoregressive Integrated Moving Average (ARIMA), and Linear Regression. Our
experiments across six demographically diverse states demonstrate that TimesFM
achieves the lowest Mean Squared Error (MSE) in 86.67% of test cases, with
particularly strong performance on minority populations with sparse historical
data. These findings highlight the potential of pre-trained foundation models
to enhance demographic analysis and inform proactive policy interventions
without requiring extensive task-specific fine-tuning.

</details>


### [299] [From Heuristics to Data: Quantifying Site Planning Layout Indicators with Deep Learning and Multi-Modal Data](https://arxiv.org/abs/2508.11723)
*Qian Cao,Jielin Chen,Junchao Zhao,Rudi Stouffs*

Main category: cs.LG

TL;DR: 本文提出了一种称为SPLI的站点规划布局指标系统，整合多源数据并应用深度学习模型以实现高效的空间分析和布局优化。


<details>
  <summary>Details</summary>
Motivation: 传统的城市站点规划过于依赖经验判断且缺乏系统性，无法全面量化多功能布局。因此，开发一种整合数据驱动与经验知识的新框架以优化城市空间布局成为必要。

Method: 采用SPLI系统，通过整合OpenStreetMap (OSM)、兴趣点(POI)、建筑形态、土地利用和卫星影像等多源异构数据，同时利用深度学习模型（如RGNN和GNN）来分析和填补数据空缺。提出了五个分析维度，包括建筑功能分类、空间组织、功能多样性、基本服务的可达性，及土地利用强度。

Result: 实验表明，SPLI系统改进了建筑功能分类准确性，为基于数据驱动的城市空间分析提供了标准化基础。

Conclusion: 本文提出的SPLI系统有效弥补了传统方法的不足，通过五维度框架系统性地实现了城市空间布局的量化和优化。

Abstract: The spatial layout of urban sites shapes land-use efficiency and spatial
organization. Traditional site planning often relies on experiential judgment
and single-source data, limiting systematic quantification of multifunctional
layouts. We propose a Site Planning Layout Indicator (SPLI) system, a
data-driven framework integrating empirical knowledge with heterogeneous
multi-source data to produce structured urban spatial information. The SPLI
supports multimodal spatial data systems for analytics, inference, and
retrieval by combining OpenStreetMap (OSM), Points of Interest (POI), building
morphology, land use, and satellite imagery. It extends conventional metrics
through five dimensions: (1) Hierarchical Building Function Classification,
refining empirical systems into clear hierarchies; (2) Spatial Organization,
quantifying seven layout patterns (e.g., symmetrical, concentric,
axial-oriented); (3) Functional Diversity, transforming qualitative assessments
into measurable indicators using Functional Ratio (FR) and Simpson Index (SI);
(4) Accessibility to Essential Services, integrating facility distribution and
transport networks for comprehensive accessibility metrics; and (5) Land Use
Intensity, using Floor Area Ratio (FAR) and Building Coverage Ratio (BCR) to
assess utilization efficiency. Data gaps are addressed through deep learning,
including Relational Graph Neural Networks (RGNN) and Graph Neural Networks
(GNN). Experiments show the SPLI improves functional classification accuracy
and provides a standardized basis for automated, data-driven urban spatial
analytics.

</details>


### [300] [Causal Structure Learning in Hawkes Processes with Complex Latent Confounder Networks](https://arxiv.org/abs/2508.11727)
*Songyao Jin,Biwei Huang*

Main category: cs.LG

TL;DR: 该研究分析部分可观测系统中的多元 Hawkes 过程，提出了一种兼顾潜在分过程与因果关系的鉴别方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理复杂系统的潜在分过程，可观测数据不足以完全揭示因果结构。

Method: 通过离散时间模型简化连续时间事件序列，结合路径条件提出迭代算法，交替推断潜在因果关系与发现新的潜在分过程。

Result: 实验表明，该方法在合成数据和真实数据场景中能有效恢复潜在因果结构。

Conclusion: 研究提出了处理潜在分过程的理论条件与方法，使得部分可观测系统的因果推理更加可靠。

Abstract: Multivariate Hawkes process provides a powerful framework for modeling
temporal dependencies and event-driven interactions in complex systems. While
existing methods primarily focus on uncovering causal structures among observed
subprocesses, real-world systems are often only partially observed, with latent
subprocesses posing significant challenges. In this paper, we show that
continuous-time event sequences can be represented by a discrete-time model as
the time interval shrinks, and we leverage this insight to establish necessary
and sufficient conditions for identifying latent subprocesses and the causal
influences. Accordingly, we propose a two-phase iterative algorithm that
alternates between inferring causal relationships among discovered subprocesses
and uncovering new latent subprocesses, guided by path-based conditions that
guarantee identifiability. Experiments on both synthetic and real-world
datasets show that our method effectively recovers causal structures despite
the presence of latent subprocesses.

</details>


### [301] [BRIEF: BRain-Inspired network connection search with Extensive temporal feature Fusion enhances disease classification](https://arxiv.org/abs/2508.11732)
*Xiangxiang Cui,Min Zhao,Dongmei Zhi,Shile Qi,Vince D Calhoun,Jing Sui*

Main category: cs.LG

TL;DR: 提出了一种名为BRIEF的脑启发特征融合框架，使用改进神经网络连接搜索策略和基于Transformer的融合模块，显著提升fMRI分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有fMRI分类的深度学习模型存在网络架构设计和特征融合的局限性，无法充分利用各模式之间的相互作用特性。

Method: BRIEF框架包括改进的Q学习网络连接搜索、Transformer多特征融合模块和注意力模块，并结合4种fMRI时间表示进行分类。

Result: 模型在两种心理障碍分类（精神分裂症和自闭症）上性能显著提升，AUC分别达到91.5%和78.4%。

Conclusion: BRIEF通过脑启发和强化学习策略优化fMRI分类，在发现精准神经影像学生物标志物上具有重要潜力。

Abstract: Existing deep learning models for functional MRI-based classification have
limitations in network architecture determination (relying on experience) and
feature space fusion (mostly simple concatenation, lacking mutual learning).
Inspired by the human brain's mechanism of updating neural connections through
learning and decision-making, we proposed a novel BRain-Inspired feature Fusion
(BRIEF) framework, which is able to optimize network architecture automatically
by incorporating an improved neural network connection search (NCS) strategy
and a Transformer-based multi-feature fusion module. Specifically, we first
extracted 4 types of fMRI temporal representations, i.e., time series (TCs),
static/dynamic functional connection (FNC/dFNC), and multi-scale dispersion
entropy (MsDE), to construct four encoders. Within each encoder, we employed a
modified Q-learning to dynamically optimize the NCS to extract high-level
feature vectors, where the NCS is formulated as a Markov Decision Process.
Then, all feature vectors were fused via a Transformer, leveraging both
stable/time-varying connections and multi-scale dependencies across different
brain regions to achieve the final classification. Additionally, an attention
module was embedded to improve interpretability. The classification performance
of our proposed BRIEF was compared with 21 state-of-the-art models by
discriminating two mental disorders from healthy controls: schizophrenia (SZ,
n=1100) and autism spectrum disorder (ASD, n=1550). BRIEF demonstrated
significant improvements of 2.2% to 12.1% compared to 21 algorithms, reaching
an AUC of 91.5% - 0.6% for SZ and 78.4% - 0.5% for ASD, respectively. This is
the first attempt to incorporate a brain-inspired, reinforcement learning
strategy to optimize fMRI-based mental disorder classification, showing
significant potential for identifying precise neuroimaging biomarkers.

</details>


### [302] [Scalable Geospatial Data Generation Using AlphaEarth Foundations Model](https://arxiv.org/abs/2508.11739)
*Luc Houriez,Sebastian Pilarski,Behzad Vahedi,Ali Ahmadalipour,Teo Honda Scully,Nicholas Aflitto,David Andre,Caroline Jaffe,Martha Wedner,Rich Mazzola,Josh Jeffery,Ben Messinger,Sage McGinley-Smith,Sarah Russell*

Main category: cs.LG

TL;DR: 提出利用AEF扩展地理标注数据的方法，通过随机森林和逻辑回归等简单模型实现任务，并在美国到加拿大的数据扩展中取得不错的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 地理标注数据在研究地球环境中起关键作用，但通常存在地域局限，难以实现全球覆盖。

Method: 提出使用AlphaEarth Foundations（AEF）作为输入，结合随机森林和逻辑回归模型，将地理标注数据从有限地域扩展到更大范围，并评估其效果。

Result: 通过将LANDFIRE的EVT数据从美国扩展到加拿大，实现了EvtPhys和EvtGp分类任务，分别在验证集上取得了81%和73%的分类准确率。

Conclusion: 方法证明了AEF作为全球地理数据补充的潜力，尽管存在某些局限性，但在特定任务上取得了高效且准确的结果。

Abstract: High-quality labeled geospatial datasets are essential for extracting
insights and understanding our planet. Unfortunately, these datasets often do
not span the entire globe and are limited to certain geographic regions where
data was collected. Google DeepMind's recently released AlphaEarth Foundations
(AEF) provides an information-dense global geospatial representation designed
to serve as a useful input across a wide gamut of tasks. In this article we
propose and evaluate a methodology which leverages AEF to extend geospatial
labeled datasets beyond their initial geographic regions. We show that even
basic models like random forests or logistic regression can be used to
accomplish this task. We investigate a case study of extending LANDFIRE's
Existing Vegetation Type (EVT) dataset beyond the USA into Canada at two levels
of granularity: EvtPhys (13 classes) and EvtGp (80 classes). Qualitatively, for
EvtPhys, model predictions align with ground truth. Trained models achieve 81%
and 73% classification accuracy on EvtPhys validation sets in the USA and
Canada, despite discussed limitations.

</details>


### [303] [Fed-Meta-Align: A Similarity-Aware Aggregation and Personalization Pipeline for Federated TinyML on Heterogeneous Data](https://arxiv.org/abs/2508.11794)
*Hemanth Macharla,Mayukha Pal*

Main category: cs.LG

TL;DR: 本文提出了一种称为Fed-Meta-Align的新框架，通过初始模型训练、元初始化、联邦学习和设备端个性化四个阶段解决非IID数据导致的联邦学习模型收敛性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习方法在处理资源受限的IoT设备上的非IID数据时表现不佳，影响工业安全。

Method: 提出Fed-Meta-Align框架，包括：1. 在公开数据集上训练基础模型；2. 元初始化阶段逐步适应IoT设备异质性；3. 使用双重标准聚合机制的联邦学习；4. 在设备端进行个性化调整。

Result: 实验表明，Fed-Meta-Align在异质性IoT设备上的平均测试准确率为91.27%，分别超过个性化FedAvg和FedProx模型3.87%和3.37%。

Conclusion: 论文展示了一种多阶段的初始化与聚合方法，可以显著提升异质网络环境中IoT设备的智能表现。

Abstract: Real-time fault classification in resource-constrained Internet of Things
(IoT) devices is critical for industrial safety, yet training robust models in
such heterogeneous environments remains a significant challenge. Standard
Federated Learning (FL) often fails in the presence of non-IID data, leading to
model divergence. This paper introduces Fed-Meta-Align, a novel four-phase
framework designed to overcome these limitations through a sophisticated
initialization and training pipeline. Our process begins by training a
foundational model on a general public dataset to establish a competent
starting point. This model then undergoes a serial meta-initialization phase,
where it sequentially trains on a subset of IOT Device data to learn a
heterogeneity-aware initialization that is already situated in a favorable
region of the loss landscape. This informed model is subsequently refined in a
parallel FL phase, which utilizes a dual-criterion aggregation mechanism that
weights for IOT devices updates based on both local performance and cosine
similarity alignment. Finally, an on-device personalization phase adapts the
converged global model into a specialized expert for each IOT Device.
Comprehensive experiments demonstrate that Fed-Meta-Align achieves an average
test accuracy of 91.27% across heterogeneous IOT devices, outperforming
personalized FedAvg and FedProx by up to 3.87% and 3.37% on electrical and
mechanical fault datasets, respectively. This multi-stage approach of sequenced
initialization and adaptive aggregation provides a robust pathway for deploying
high-performance intelligence on diverse TinyML networks.

</details>


### [304] [Uncalibrated Reasoning: GRPO Induces Overconfidence for Stochastic Outcomes](https://arxiv.org/abs/2508.11800)
*Michael Bereket,Jure Leskovec*

Main category: cs.LG

TL;DR: 研究评估了强化学习（RL）方法在具有随机性结果的可验证领域中优化语言模型的效果，并分析了GRPO方法中的过度自信问题及其改进方案。


<details>
  <summary>Details</summary>
Motivation: 目前的RL方法已在数学等确定性领域表现出色，但其在具有随机性结果的领域（如科学实验）中的作用尚不清楚，因此研究旨在探讨现有RL方法在此类领域的有效性。

Method: 研究通过应用于合成数据和生物学实验，分析三种RL方法（GRPO、PPO和RLOO）的性能，并重点探讨标准化对GRPO方法模型校准性的影响。

Result: 分析发现GRPO方法对随机结果的二元概率预测存在过度自信，但通过移除GRPO中的群组标准正则化可以解决此问题，并提供了理论解释。相较而言，PPO和RLOO方法能够保持模型的良好校准性。

Conclusion: 研究证明标准正则化在GRPO中的弊端，提出改进方法，为RL优化语言模型向更复杂领域的拓展提供了依据。

Abstract: Reinforcement learning (RL) has proven remarkably effective at improving the
accuracy of language models in verifiable and deterministic domains like
mathematics. Here, we examine if current RL methods are also effective at
optimizing language models in verifiable domains with stochastic outcomes, like
scientific experiments. Through applications to synthetic data and real-world
biological experiments, we demonstrate that Group Relative Policy Optimization
(GRPO) induces overconfident probability predictions for binary stochastic
outcomes, while Proximal Policy Optimization (PPO) and REINFORCE Leave-One-Out
(RLOO) yield well-calibrated models. We show that removing group standard
normalization in GRPO fixes its miscalibration and provide a theoretical
explanation for why normalization causes overconfidence. Our results provide
new evidence against the use of standard normalization in GRPO and help pave
the way for applications of RL for reasoning language models beyond
deterministic domains.

</details>


### [305] [FairTabGen: Unifying Counterfactual and Causal Fairness in Synthetic Tabular Data Generation](https://arxiv.org/abs/2508.11810)
*Nitish Nagesh,Salar Shakibhamedan,Mahdi Bagheri,Ziyu Wang,Nima TaheriNejad,Axel Jantsch,Amir M. Rahmani*

Main category: cs.LG

TL;DR: FairTabGen 是一个改进的公平性大模型框架，用于生成表格数据，解决了隐私和数据不足的问题，同时保持高效用性。


<details>
  <summary>Details</summary>
Motivation: 当前，生成具有隐私保护和高效用性的数据在实际应用中尤为重要，尤其是在表格数据领域。然而，如何在保证效用的情况下提升反事实和因果公平性是一大难题。

Method: FairTabGen 结合了反事实与因果公平定义，通过上下文学习、提示优化和公平性数据整理，集成于生成和评价流程中；相比传统的 GAN 与 LLM 方法，在多样数据集上表现更优。

Result: FairTabGen 在公平性指标（如人口统计均等性和路径因果效应）上提升了 10%，同时保持统计效用，并在数据不足的情况下有效，仅使用少于 20% 的原始数据即达到出色效果。

Conclusion: FairTabGen 为生成公平且高效用的合成表格数据提供了一个经济高效、理论扎实和实用的解决方案。

Abstract: Generating synthetic data is crucial in privacy-sensitive, data-scarce
settings, especially for tabular datasets widely used in real-world
applications. A key challenge is improving counterfactual and causal fairness,
while preserving high utility. We present FairTabGen, a fairness-aware large
language model-based framework for tabular synthetic data generation. We
integrate multiple fairness definitions including counterfactual and causal
fairness into both its generation and evaluation pipelines. We use in-context
learning, prompt refinement, and fairness-aware data curation to balance
fairness and utility. Across diverse datasets, our method outperforms
state-of-the-art GAN-based and LLM-based methods, achieving up to 10%
improvements on fairness metrics such as demographic parity and path-specific
causal effects while retaining statistical utility. Remarkably, it achieves
these gains using less than 20% of the original data, highlighting its
efficiency in low-data regimes. These results demonstrate a principled and
practical approach for generating fair and useful synthetic tabular data.

</details>


### [306] [Combinations of Fast Activation and Trigonometric Functions in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2508.11876)
*Hoang-Thang Ta,Duy-Quy Thai,Phuong-Linh Tran-Thi*

Main category: cs.LG

TL;DR: 提出使用快速计算函数（如ReLU和三角函数）作为Kolmogorov-Arnold Networks (KANs)的基础组件，以提高计算效率，同时保持竞争性能。


<details>
  <summary>Details</summary>
Motivation: 目前基于Kolmogorov-Arnold表示定理（KART）的神经网络大多使用多项式函数（B-splines和RBFs），但这些函数对GPU支持不好，且不够流行。

Method: 在网络结构中集成如ReLU、sin、cos和arctan等快速计算函数，以替代传统的多项式函数。

Result: 实验结果表明，新方法在维持竞争性能的同时，训练时间和泛化潜力有所提高。

Conclusion: 使用快速计算函数可以改进KANs的计算效率，并在性能和训练时间之间取得较好平衡。

Abstract: For years, many neural networks have been developed based on the
Kolmogorov-Arnold Representation Theorem (KART), which was created to address
Hilbert's 13th problem. Recently, relying on KART, Kolmogorov-Arnold Networks
(KANs) have attracted attention from the research community, stimulating the
use of polynomial functions such as B-splines and RBFs. However, these
functions are not fully supported by GPU devices and are still considered less
popular. In this paper, we propose the use of fast computational functions,
such as ReLU and trigonometric functions (e.g., ReLU, sin, cos, arctan), as
basis components in Kolmogorov-Arnold Networks (KANs). By integrating these
function combinations into the network structure, we aim to enhance
computational efficiency. Experimental results show that these combinations
maintain competitive performance while offering potential improvements in
training time and generalization.

</details>


### [307] [PCA- and SVM-Grad-CAM for Convolutional Neural Networks: Closed-form Jacobian Expression](https://arxiv.org/abs/2508.11880)
*Yuto Omae*

Main category: cs.LG

TL;DR: 本文提出了PCA-Grad-CAM和SVM-Grad-CAM方法，用于CNN结合PCA和SVM时的可视化改进。


<details>
  <summary>Details</summary>
Motivation: 为了在有限样本的情况下改进CNN的分类性能，并克服传统Grad-CAM无法应用到PCA和SVM层的问题。

Method: 通过推导从最后一个卷积层到PCA和SVM层的闭式雅可比形式，提出了PCA-Grad-CAM和SVM-Grad-CAM两种视觉化方法。

Result: 实现了新的方法PCA-Grad-CAM和SVM-Grad-CAM，并展示了其在几个主要数据集上的可视化效果。

Conclusion: 两种方法为PCA特征向量和SVM层的注意力区域提供了可视化，实现了更可靠的白盒方法。

Abstract: Convolutional Neural Networks (CNNs) are an effective approach for
classification tasks, particularly when the training dataset is large. Although
CNNs have long been considered a black-box classification method, they can be
used as a white-box method through visualization techniques such as Grad-CAM.
When training samples are limited, incorporating a Principal Component Analysis
(PCA) layer and/or a Support Vector Machine (SVM) classifier into a CNN can
effectively improve classification performance. However, traditional Grad-CAM
cannot be directly applied to PCA and/or SVM layers. It is important to
generate attention regions for PCA and/or SVM layers in CNNs to facilitate the
development of white-box methods. Therefore, we propose ``PCA-Grad-CAM'', a
method for visualizing attention regions in PCA feature vectors, and
``SVM-Grad-CAM'', a method for visualizing attention regions in an SVM
classifier layer. To complete our methods analytically, it is necessary to
solve the closed-form Jacobian consisting of partial derivatives from the last
convolutional layer to the PCA and/or SVM layers. In this paper, we present the
exact closed-form Jacobian and the visualization results of our methods applied
to several major datasets.

</details>


### [308] [ENA: Efficient N-dimensional Attention](https://arxiv.org/abs/2508.11921)
*Yibo Zhong*

Main category: cs.LG

TL;DR: 该论文提出了一种称为Efficient N-dimensional Attention (ENA)的混合架构，用于高效建模超长高阶数据，结合了线性递归和局部滑动窗口注意力。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer模型在处理超长高阶数据时的效率不足，需探索更适合的建模方法。

Method: 通过探讨扫描策略和注意力混合架构，提出了一种线性递归与高阶滑动窗口注意力相结合的混合模型ENA。

Result: 实验证明ENA在理论和实践中表现高效，并能够有效建模超长高阶数据。

Conclusion: ENA模型通过线性递归和局部注意力的结合，提供了全局信息压缩与局部建模的平衡，成为一种简单且实用的超长高阶数据建模方法。

Abstract: Efficient modeling of long sequences of high-order data requires a more
efficient architecture than Transformer. In this paper, we investigate two key
aspects of extending linear recurrent models, especially those originally
designed for language modeling, to high-order data (1D to ND): scanning
strategies and attention-hybrid architectures. Empirical results suggest that
scanning provides limited benefits, while attention-hybrid models yield
promising results. Focusing on the latter, we further evaluate types of
attention and find that tiled high-order sliding window attention (SWA) is
efficient in both theory and practice. We term the resulting hybrid
architecture of linear recurrence and high-order SWA as Efficient N-dimensional
Attention (ENA). We then conduct several experiments to demonstrate its
effectiveness. The intuition behind ENA is that linear recurrence compresses
global information into a state, while SWA complements it by enforcing strict
local modeling. Together, they form a simple framework that offers a promising
and practical solution for ultra-long high-order data modeling.

</details>


### [309] [Scale-Disentangled spatiotemporal Modeling for Long-term Traffic Emission Forecasting](https://arxiv.org/abs/2508.11923)
*Yan Wu,Lihong Pei,Yukai Han,Yang Cao,Yu Kang,Yanlong Zhao*

Main category: cs.LG

TL;DR: 提出了一种用于长时间交通排放预测的多尺度解耦时空建模框架（SDSTM），通过分解多尺度特征并进行动态融合，从而提高预测准确性和性能。


<details>
  <summary>Details</summary>
Motivation: 为了应对交通排放在时空多尺度交织下的长时间预测误差放大问题，研究者希望通过高效的时空解耦算法提升预测准确性。

Method: 该方法提出了基于Koopman升维算子和小波分解的双流特征分解策略，结合交叉项损失的双流独立约束机制，实现长时间交通排放的动态特征融合与误差抑制。

Result: 在西安二环的道路排放数据集上，模型性能达到了当前最优水平，验证了其准确性与鲁棒性。

Conclusion: SDSTM框架通过特征解耦与动态融合显著提升了长时间交通排放预测的性能，适用于复杂多尺度的交通排放管理情境。

Abstract: Long-term traffic emission forecasting is crucial for the comprehensive
management of urban air pollution. Traditional forecasting methods typically
construct spatiotemporal graph models by mining spatiotemporal dependencies to
predict emissions. However, due to the multi-scale entanglement of traffic
emissions across time and space, these spatiotemporal graph modeling method
tend to suffer from cascading error amplification during long-term inference.
To address this issue, we propose a Scale-Disentangled Spatio-Temporal Modeling
(SDSTM) framework for long-term traffic emission forecasting. It leverages the
predictability differences across multiple scales to decompose and fuse
features at different scales, while constraining them to remain independent yet
complementary. Specifically, the model first introduces a dual-stream feature
decomposition strategy based on the Koopman lifting operator. It lifts the
scale-coupled spatiotemporal dynamical system into an infinite-dimensional
linear space via Koopman operator, and delineates the predictability boundary
using gated wavelet decomposition. Then a novel fusion mechanism is
constructed, incorporating a dual-stream independence constraint based on
cross-term loss to dynamically refine the dual-stream prediction results,
suppress mutual interference, and enhance the accuracy of long-term traffic
emission prediction. Extensive experiments conducted on a road-level traffic
emission dataset within Xi'an's Second Ring Road demonstrate that the proposed
model achieves state-of-the-art performance.

</details>


### [310] [An Improved Algorithm for Adversarial Linear Contextual Bandits via Reduction](https://arxiv.org/abs/2508.11931)
*Tim van Erven,Jack Mayo,Julia Olkhovskaya,Chen-Yu Wei*

Main category: cs.LG

TL;DR: 提出了一种高效算法，用于处理具有对抗性损失和随机动作集的线性上下文赌博问题，达成了多项式时间内的最优次线性遗憾界。


<details>
  <summary>Details</summary>
Motivation: 解决线性上下文赌博问题中的开放问题，探索在没有动作数量依赖的情况下，能否实现多项式时间的低遗憾界。

Method: 方法是将随机动作集与对抗性损失的上下文赌博问题，转化为错定鲁棒的对抗线性赌博问题，使用固定动作集并设计多项式时间算法。

Result: 算法实现了$\tilde{O}(\min\{d^2\sqrt{T}, \sqrt{d^3T\log K}\})$的遗憾界，并且首次在多项式时间内对特定组合赌博问题达成$\text{poly}(d)\sqrt{T}$遗憾界。

Conclusion: 该研究为特定类型赌博问题设定了一种新标准，证明了在没有动作数量依赖的情况下，可以实现理论优化的遗憾界，解决了相关开放性问题。

Abstract: We present an efficient algorithm for linear contextual bandits with
adversarial losses and stochastic action sets. Our approach reduces this
setting to misspecification-robust adversarial linear bandits with fixed action
sets. Without knowledge of the context distribution or access to a context
simulator, the algorithm achieves $\tilde{O}(\min\{d^2\sqrt{T}, \sqrt{d^3T\log
K}\})$ regret and runs in $\text{poly}(d,C,T)$ time, where $d$ is the feature
dimension, $C$ is an upper bound on the number of linear constraints defining
the action set in each round, $K$ is an upper bound on the number of actions in
each round, and $T$ is number of rounds. This resolves the open question by Liu
et al. (2023) on whether one can obtain $\text{poly}(d)\sqrt{T}$ regret in
polynomial time independent of the number of actions. For the important class
of combinatorial bandits with adversarial losses and stochastic action sets
where the action sets can be described by a polynomial number of linear
constraints, our algorithm is the first to achieve $\text{poly}(d)\sqrt{T}$
regret in polynomial time, while no prior algorithm achieves even $o(T)$ regret
in polynomial time to our knowledge. When a simulator is available, the regret
bound can be improved to $\tilde{O}(d\sqrt{L^\star})$, where $L^\star$ is the
cumulative loss of the best policy.

</details>


### [311] [M3OOD: Automatic Selection of Multimodal OOD Detectors](https://arxiv.org/abs/2508.11936)
*Yuehan Qin,Li Li,Defu Cao,Tiankai Yang,Yue Zhao*

Main category: cs.LG

TL;DR: 提出M3OOD框架，一种基于元学习的多模态检测器选择方法。


<details>
  <summary>Details</summary>
Motivation: 解决在分布变化条件下选择最佳OOD检测模型的难题。

Method: 将多模态嵌入与手工设计的元特征结合，以元学习框架通过学习历史模型行为推荐适合的新检测器。

Result: M3OOD在12个测试场景中性能优于10种基线方法，且计算开销最小化。

Conclusion: M3OOD实现了高效的模型选择，尤其适用于多模态分布变化场景，可大幅提升OOD检测的鲁棒性。

Abstract: Out-of-distribution (OOD) robustness is a critical challenge for modern
machine learning systems, particularly as they increasingly operate in
multimodal settings involving inputs like video, audio, and sensor data.
Currently, many OOD detection methods have been proposed, each with different
designs targeting various distribution shifts. A single OOD detector may not
prevail across all the scenarios; therefore, how can we automatically select an
ideal OOD detection model for different distribution shifts? Due to the
inherent unsupervised nature of the OOD detection task, it is difficult to
predict model performance and find a universally Best model. Also,
systematically comparing models on the new unseen data is costly or even
impractical. To address this challenge, we introduce M3OOD, a
meta-learning-based framework for OOD detector selection in multimodal
settings. Meta learning offers a solution by learning from historical model
behaviors, enabling rapid adaptation to new data distribution shifts with
minimal supervision. Our approach combines multimodal embeddings with
handcrafted meta-features that capture distributional and cross-modal
characteristics to represent datasets. By leveraging historical performance
across diverse multimodal benchmarks, M3OOD can recommend suitable detectors
for a new data distribution shift. Experimental evaluation demonstrates that
M3OOD consistently outperforms 10 competitive baselines across 12 test
scenarios with minimal computational overhead.

</details>


### [312] [Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware](https://arxiv.org/abs/2508.11940)
*Yuannuo Feng,Wenyong Zhou,Yuexi Lyu,Yixiang Zhang,Zhengwu Liu,Ngai Wong,Wang Kang*

Main category: cs.LG

TL;DR: 提出了一种用于模拟内存计算中的噪声感知训练的新方法，能够提升训练性能和稳定性，同时降低内存使用和训练时间。


<details>
  <summary>Details</summary>
Motivation: 模拟内存计算架构在神经网络推理中展现了显著的能效优势，但硬件噪声复杂性对其部署构成了重大挑战。

Method: 基于直通估计器（STE），作者将正向噪声仿真与反向梯度计算分离，从而支持更准确但计算复杂度更高的噪声建模进行噪声感知训练。

Result: 与传统方法相比，该方法在图像分类中提升了5.3%的准确率，在文本生成中降低了0.72的困惑度，并实现了2.2倍的训练速度提升及37.9%的峰值内存使用降低。

Conclusion: 该方法能够在不牺牲优化稳定性的前提下，提供计算可行性，同时显著增强模拟内存计算系统的训练及推理表现。

Abstract: Analog Compute-In-Memory (CIM) architectures promise significant energy
efficiency gains for neural network inference, but suffer from complex
hardware-induced noise that poses major challenges for deployment. While
noise-aware training methods have been proposed to address this issue, they
typically rely on idealized and differentiable noise models that fail to
capture the full complexity of analog CIM hardware variations. Motivated by the
Straight-Through Estimator (STE) framework in quantization, we decouple forward
noise simulation from backward gradient computation, enabling noise-aware
training with more accurate but computationally intractable noise modeling in
analog CIM systems. We provide theoretical analysis demonstrating that our
approach preserves essential gradient directional information while maintaining
computational tractability and optimization stability. Extensive experiments
show that our extended STE framework achieves up to 5.3% accuracy improvement
on image classification, 0.72 perplexity reduction on text generation,
2.2$\times$ speedup in training time, and 37.9% lower peak memory usage
compared to standard noise-aware training methods.

</details>


### [313] [Learning Marked Temporal Point Process Explanations based on Counterfactual and Factual Reasoning](https://arxiv.org/abs/2508.11943)
*Sishun Liu,Ke Deng,Xiuzhen Zhang,Yan Wang*

Main category: cs.LG

TL;DR: 研究了多标记时间点过程的解释问题，提出CFF模型以提高解释质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经网络的MTPP模型在高风险应用中广泛使用，但其输出的可信性引发了关注，因此需要更合理的解释。

Method: 将MTPP的解释定义为反事实解释与真实解释的结合，提出了Counterfactual and Factual Explainer for MTPP (CFF)模型并结合多种技术实现。

Result: 实验表明CFF模型在解释质量和处理效率上优于基线模型。

Conclusion: CFF模型为MTPP提供了最小且合理的解释，在可信性方面取得了显著进展。

Abstract: Neural network-based Marked Temporal Point Process (MTPP) models have been
widely adopted to model event sequences in high-stakes applications, raising
concerns about the trustworthiness of outputs from these models. This study
focuses on Explanation for MTPP, aiming to identify the minimal and rational
explanation, that is, the minimum subset of events in history, based on which
the prediction accuracy of MTPP matches that based on full history to a great
extent and better than that based on the complement of the subset. This study
finds that directly defining Explanation for MTPP as counterfactual explanation
or factual explanation can result in irrational explanations. To address this
issue, we define Explanation for MTPP as a combination of counterfactual
explanation and factual explanation. This study proposes Counterfactual and
Factual Explainer for MTPP (CFF) to solve Explanation for MTPP with a series of
deliberately designed techniques. Experiments demonstrate the correctness and
superiority of CFF over baselines regarding explanation quality and processing
efficiency.

</details>


### [314] [Set-Valued Transformer Network for High-Emission Mobile Source Identification](https://arxiv.org/abs/2508.11976)
*Yunning Cao,Lihong Pei,Jian Guo,Yang Cao,Yu Kang,Yanlong Zhao*

Main category: cs.LG

TL;DR: 提出一种名为Set-Valued Transformer Network (SVTN)的新方法，通过特征提取和分类算法改进高排放车辆的检测准确性。


<details>
  <summary>Details</summary>
Motivation: 城市污染与交通排放息息相关，高排放车辆的识别对于制定减排战略至关重要，但受限于数据不均衡性与复杂性。

Method: 方法使用变换器对微行程条件的时间相似性进行建模，并通过集合值识别算法实现特征向量与分类标签的概率映射。

Result: 在合肥市2020年柴油车辆监测数据上实验，该方法使高排放车辆的漏检率降低9.5%。

Conclusion: 该模型在识别高排放移动污染源方面性能优越，能有效应对数据长尾分布与非线性复杂性的挑战。

Abstract: Identifying high-emission vehicles is a crucial step in regulating urban
pollution levels and formulating traffic emission reduction strategies.
However, in practical monitoring data, the proportion of high-emission state
data is significantly lower compared to normal emission states. This
characteristic long-tailed distribution severely impedes the extraction of
discriminative features for emission state identification during data mining.
Furthermore, the highly nonlinear nature of vehicle emission states and the
lack of relevant prior knowledge also pose significant challenges to the
construction of identification models.To address the aforementioned issues, we
propose a Set-Valued Transformer Network (SVTN) to achieve comprehensive
learning of discriminative features from high-emission samples, thereby
enhancing detection accuracy. Specifically, this model first employs the
transformer to measure the temporal similarity of micro-trip condition
variations, thus constructing a mapping rule that projects the original
high-dimensional emission data into a low-dimensional feature space. Next, a
set-valued identification algorithm is used to probabilistically model the
relationship between the generated feature vectors and their labels, providing
an accurate metric criterion for the classification algorithm. To validate the
effectiveness of our proposed approach, we conducted extensive experiments on
the diesel vehicle monitoring data of Hefei city in 2020. The results
demonstrate that our method achieves a 9.5\% reduction in the missed detection
rate for high-emission vehicles compared to the transformer-based baseline,
highlighting its superior capability in accurately identifying high-emission
mobile pollution sources.

</details>


### [315] [Efficient Modular Learning through Naive LoRA Summation: Leveraging Orthogonality in High-Dimensional Models](https://arxiv.org/abs/2508.11985)
*Zhanhao Cao,Clement Truong,Andrew Lizarraga*

Main category: cs.LG

TL;DR: 该研究探讨了如何通过简单叠加独立训练的LoRA模块实现参数高效微调（PEFT），在无需额外训练的情况下取得接近于合并数据训练的效果。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的进展主要依靠规模驱动，而参数高效微调（PEFT）则通过更新参数的一小部分实现高效适配。研究通过观察LoRA模块的特性，尝试发现其将模块化组件叠加的潜力。

Method: 研究假设独立训练于不同领域的LoRA模块近似正交，通过简单相加实现组合，并在GPT-2小型预训练模型（117M参数）上对医学、数学和金融等问答（QA）领域进行实验验证。

Result: 实验表明，数学与医学模块叠加后的困惑度降低了-9.10%（相较于合并数据训练），数学+金融和金融+医学的变化分别为+4.54%和+27.56%。此外，LoRA剩余差的RMS余弦相似性线性正相关于困惑度变化。

Conclusion: 简单的LoRA模块相加无需额外训练，能够快速操作并接近合并数据训练的性能，同时为更高阶组合中的干扰问题提供了明确的线索。

Abstract: Recent advances in large language models are driven by scale, while
parameter-efficient fine-tuning (PEFT) enables updating only a small fraction
of parameters. Low-Rank Adaptation (LoRA) stores parameter deltas as the
product of two small matrices, which makes them natural building blocks that
can be composed. Motivated by the superposition principle, we hypothesize that
independently trained LoRA modules on disjoint domains are approximately
orthogonal and can be combined by simple addition. Using GPT-2 Small (117M)
with LoRA rank 4 and alpha=64, we train adapters for three QA domains (math,
medicine, finance). In pairwise tests, adding Math+Medicine adapters improves
perplexity by -9.10% relative to merged-data fine-tuning, while Math+Finance
and Finance+Medicine change by +4.54% and +27.56%, respectively. Across
combinations, the RMS cosine similarity between LoRA deltas correlates
positively and approximately linearly with the change in perplexity. Naive
summation requires no additional training, can be applied in seconds, and
achieves performance comparable to models trained on merged data, while
clarifying when interference appears in higher-order compositions.

</details>


### [316] [Universal Learning of Nonlinear Dynamics](https://arxiv.org/abs/2508.11990)
*Evan Dogariu,Anand Brahmbhatt,Elad Hazan*

Main category: cs.LG

TL;DR: 本文提出了一种基于谱过滤的新算法，用于学习边际稳定的非线性动力系统，并提出了一种新的学习性定量控制理论。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过学习算法处理存在有限边际稳定模式的非线性动力学系统的预测问题。

Method: 基于谱过滤技术，提出了一种算法，通过使用在线凸优化技术及新的谱过滤算法实现对线性动力学系统的学习，包含非对称动力学和噪声校正。

Result: 证明了对于具备有限边际稳定模式的非线性动力学系统，该算法能实现逐渐减小的预测误差，其效果由学术上创新的学习性标准评估。

Conclusion: 提出的方法显著拓展了原始谱过滤算法的适用范围，并在学习和应用非线性动力学系统方面具有独立的研究价值。

Abstract: We study the fundamental problem of learning a marginally stable unknown
nonlinear dynamical system. We describe an algorithm for this problem, based on
the technique of spectral filtering, which learns a mapping from past
observations to the next based on a spectral representation of the system.
Using techniques from online convex optimization, we prove vanishing prediction
error for any nonlinear dynamical system that has finitely many marginally
stable modes, with rates governed by a novel quantitative control-theoretic
notion of learnability. The main technical component of our method is a new
spectral filtering algorithm for linear dynamical systems, which incorporates
past observations and applies to general noisy and marginally stable systems.
This significantly generalizes the original spectral filtering algorithm to
both asymmetric dynamics as well as incorporating noise correction, and is of
independent interest.

</details>


### [317] [FedUHD: Unsupervised Federated Learning using Hyperdimensional Computing](https://arxiv.org/abs/2508.12021)
*You Hak Lee,Xiaofan Yu,Quanling Zhao,Flavio Ponzina,Tajana Rosing*

Main category: cs.LG

TL;DR: 本文提出了FedUHD，一种基于超维计算（HDC）的无监督联邦学习框架，显著优化了速度、能效、通讯成本和准确性，同时对噪声具有更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前无监督联邦学习由于非独立同分布数据、高昂的边缘计算和通信成本以及对通信噪声的脆弱性，在实际应用中面临挑战。

Method: FedUHD基于超维计算，在客户端采用基于kNN的聚类超向量移除法解决非独立同分布问题，在服务器端引入加权的HDC聚合技术以平衡客户端数据分布。

Result: 实验表明，FedUHD在训练中实现最高173.6倍速度提升和612.7倍能效提升，通信成本降低至271倍，平均准确性提高15.50%，同时对噪声具有更强的鲁棒性。

Conclusion: FedUHD作为首个基于HDC的无监督联邦学习框架，在性能和效率方面显著优于现有的基于深度神经网络的UFL方法。

Abstract: Unsupervised federated learning (UFL) has gained attention as a
privacy-preserving, decentralized machine learning approach that eliminates the
need for labor-intensive data labeling. However, UFL faces several challenges
in practical applications: (1) non-independent and identically distributed
(non-iid) data distribution across devices, (2) expensive computational and
communication costs at the edge, and (3) vulnerability to communication noise.
Previous UFL approaches have relied on deep neural networks (NN), which
introduce substantial overhead in both computation and communication. In this
paper, we propose FedUHD, the first UFL framework based on Hyperdimensional
Computing (HDC). HDC is a brain-inspired computing scheme with lightweight
training and inference operations, much smaller model size, and robustness to
communication noise. FedUHD introduces two novel HDC-based designs to improve
UFL performance. On the client side, a kNN-based cluster hypervector removal
method addresses non-iid data samples by eliminating detrimental outliers. On
the server side, a weighted HDC aggregation technique balances the non-iid data
distribution across clients. Our experiments demonstrate that FedUHD achieves
up to 173.6x and 612.7x better speedup and energy efficiency, respectively, in
training, up to 271x lower communication cost, and 15.50% higher accuracy on
average across diverse settings, along with superior robustness to various
types of noise compared to state-of-the-art NN-based UFL approaches.

</details>


### [318] [Fairness Regularization in Federated Learning](https://arxiv.org/abs/2508.12042)
*Zahra Kharaghani,Ali Dadras,Tommy Löfstedt*

Main category: cs.LG

TL;DR: 本文探讨联邦学习(FL)在异构数据环境中实现性能公平性的新方法FairGrad和FairGrad*，并发现它们能有效提高公平性和整体模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习允许在去中心化数据源之间实现协作训练，解决隐私问题并利用更大范围的数据。然而，数据的异构性导致部分客户端对全局模型影响不成比例，从而引发公平性问题。

Method: 本文聚焦性能公平性，通过将在客户端损失上加以正则化的公平性方法进行评估，提出并引入了新的梯度方差正则化方法FairGrad和FairGrad*。

Result: 研究表明，FairGrad和FairGrad*可有效提升异构数据环境下的公平性和整体模型性能。

Conclusion: 本文通过理论解释和实证分析，揭示了各公平性方法间的联系，并验证了FairGrad系列方法改善公平性和模型性能的能力。

Abstract: Federated Learning (FL) has emerged as a vital paradigm in modern machine
learning that enables collaborative training across decentralized data sources
without exchanging raw data. This approach not only addresses privacy concerns
but also allows access to overall substantially larger and potentially more
diverse datasets, without the need for centralized storage or hardware
resources. However, heterogeneity in client data may cause certain clients to
have disproportionate impacts on the global model, leading to disparities in
the clients' performances. Fairness, therefore, becomes a crucial concern in FL
and can be addressed in various ways. However, the effectiveness of existing
fairness-aware methods, particularly in heterogeneous data settings, remains
unclear, and the relationships between different approaches are not well
understood. In this work, we focus on performance equitable fairness, which
aims to minimize differences in performance across clients. We restrict our
study to fairness-aware methods that explicitly regularize client losses,
evaluating both existing and newly proposed approaches. We identify and
theoretically explain connections between the investigated fairness methods,
and empirically show that FairGrad (approximate) and FairGrad* (exact) (two
variants of a gradient variance regularization method introduced here for
performance equitable fairness) improve both fairness and overall model
performance in heterogeneous data settings.

</details>


### [319] [VARAN: Variational Inference for Self-Supervised Speech Models Fine-Tuning on Downstream Tasks](https://arxiv.org/abs/2508.12061)
*Daria Diatlova,Nikita Balagansky,Alexander Varlamov,Egor Spirin*

Main category: cs.LG

TL;DR: 提出了一种动态调整层聚合的方法VARAN，用于细化自监督语音模型层的特征集成。


<details>
  <summary>Details</summary>
Motivation: 传统方法固定了特征权重，导致信息瓶颈和难以处理不同数据的需求。

Method: 利用层专用探测头和数据相关的权重机制，根据输入动态调整层的特征权重。

Result: 在自动语音识别和语音情感识别任务中表现优异，尤其结合LoRA微调技术效果显著增强。

Conclusion: VARAN 实现了在保留层特定信息与灵活使用特征之间的平衡，促进了自监督语音表示的高效适配。

Abstract: Conventional methods for aggregating layers in fine-tuned self-supervised
speech models, such as using the final layer or weighted sum, suffer from
information bottlenecks and static feature weighting for all dataset examples.
We propose VARAN, a framework that dynamically tailors layer aggregation to
individual inputs. By employing layer-specialized probing heads and
data-dependent weighting, VARAN adaptively prioritizes layer's features based
on input. Evaluations on automatic speech recognition and speech emotion
recognition tasks demonstrate VARAN's superior performance, particularly when
using the LoRA fine-tuning technique. The framework resolves the trade-off
between preserving layer-specific information and enabling flexible feature
utilization, advancing efficient adaptation of self-supervised speech
representations.

</details>


### [320] [Content Accuracy and Quality Aware Resource Allocation Based on LP-Guided DRL for ISAC-Driven AIGC Networks](https://arxiv.org/abs/2508.12079)
*Ningzhe Shi,Yiqing Zhou,Ling Liu,Jinglin Shi,Yihao Wu,Haiwei Shi,Hanxiao Yu*

Main category: cs.LG

TL;DR: 该论文提出了一种新的服务评估指标(CAQA)，旨在优化ISAC(一体化感知与通信)驱动的内容生成网络中的资源配置问题。通过引入LP指导的深度强化学习算法（LPDRL-F），成功降低了问题复杂度，并提升了内容生成质量评估的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的内容生成服务并未考虑感知数据不准确及生成模型引入的误差，导致无法全面评估用户体验质量（QoE）。因此，提出CAQA以综合评估ISAC网络中的服务质量至关重要。

Method: 提出了一种LP指导的深度强化学习算法（LPDRL-F），通过构造动作过滤器，降低问题维度，从三维空间降至二维，同时优化感知、生成（计算）和通信资源的分配，最大化CAQA。

Result: LPDRL-F大幅提升了资源分配效率，相比现有算法，加速收敛超过60%，平均CAQA提升逾14%，并实现50%以上的用户体验质量改进。

Conclusion: 基于LPDRL-F的CAQA度量有效优化了ISAC驱动的服务质量，证明其能显著提升生成内容的用户体验质量，兼具高效性与准确性。

Abstract: Integrated sensing and communication (ISAC) can enhance artificial
intelligence-generated content (AIGC) networks by providing efficient sensing
and transmission. Existing AIGC services usually assume that the accuracy of
the generated content can be ensured, given accurate input data and prompt,
thus only the content generation quality (CGQ) is concerned. However, it is not
applicable in ISAC-based AIGC networks, where content generation is based on
inaccurate sensed data. Moreover, the AIGC model itself introduces generation
errors, which depend on the number of generating steps (i.e., computing
resources). To assess the quality of experience of ISAC-based AIGC services, we
propose a content accuracy and quality aware service assessment metric (CAQA).
Since allocating more resources to sensing and generating improves content
accuracy but may reduce communication quality, and vice versa, this
sensing-generating (computing)-communication three-dimensional resource
tradeoff must be optimized to maximize the average CAQA (AvgCAQA) across all
users with AIGC (CAQA-AIGC). This problem is NP-hard, with a large solution
space that grows exponentially with users. To solve the CAQA-AIGC problem with
low complexity, a linear programming (LP) guided deep reinforcement learning
(DRL) algorithm with an action filter (LPDRL-F) is proposed. Through the
LP-guided approach and the action filter, LPDRL-F can transform the original
three-dimensional solution space to two dimensions, reducing complexity while
improving the learning performance of DRL. Simulations show that compared to
existing DRL and generative diffusion model algorithms without LP, LPDRL-F
converges faster by over 60% and finds better resource allocation solutions,
improving AvgCAQA by more than 14%. With LPDRL-F, CAQA-AIGC can achieve an
improvement in AvgCAQA of more than 50% compared to existing schemes focusing
solely on CGQ.

</details>


### [321] [Generative Medical Event Models Improve with Scale](https://arxiv.org/abs/2508.12104)
*Shane Waxler,Paul Blazek,Davis White,Daniel Sneider,Kevin Chung,Mani Nagarathnam,Patrick Williams,Hank Voeller,Karen Wong,Matthew Swanhorst,Sheng Zhang,Naoto Usuyama,Cliff Wong,Tristan Naumann,Hoifung Poon,Andrew Loza,Daniella Meeker,Seth Hain,Rahul Shah*

Main category: cs.LG

TL;DR: 本文提出了Cosmos Medical Event Transformer (CoMET)，一种基于解码器的Transformer模型，预训练于超过1.15亿名患者的医疗事件数据，总量达1510亿标记，用于预测医疗事件并支持78项实际任务。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在通过分析纵向患者医疗事件序列，开发能够从大规模医疗数据中提取见解的方法，以支持个性化医疗规模化实施。

Method: 使用Epic Cosmos数据集开发了一系列解码器型Transformer模型CoMET，规模从小到大，最多达到10亿参数，预训练建立基于医疗事件的动力学预测方法，测试78个任务而无需特定微调。

Result: CoMET模型在诊断预测、疾病预后和医疗操作等78项任务中通常优于或匹敌任务特定的监督模型，且随着模型规模的增加预测能力稳步提升。

Conclusion: 作为生成式医学事件基础模型，CoMET展示了其捕获复杂临床动态的能力，为改善决策支持与患者结局提供了可扩展的通用框架。

Abstract: Realizing personalized medicine at scale calls for methods that distill
insights from longitudinal patient journeys, which can be viewed as a sequence
of medical events. Foundation models pretrained on large-scale medical event
data represent a promising direction for scaling real-world evidence generation
and generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with
medical events from de-identified longitudinal health records for 16.3 billion
encounters over 300 million unique patient records from 310 health systems, we
introduce the Cosmos Medical Event Transformer ( CoMET) models, a family of
decoder-only transformer models pretrained on 118 million patients representing
115 billion discrete medical events (151 billion tokens). We present the
largest scaling-law study for medical event data, establishing a methodology
for pretraining and revealing power-law scaling relationships for compute,
tokens, and model size. Based on this, we pretrained a series of
compute-optimal models with up to 1 billion parameters. Conditioned on a
patient's real-world history, CoMET autoregressively generates the next medical
event, simulating patient health timelines. We studied 78 real-world tasks,
including diagnosis prediction, disease prognosis, and healthcare operations.
Remarkably for a foundation model with generic pretraining and simulation-based
inference, CoMET generally outperformed or matched task-specific supervised
models on these tasks, without requiring task-specific fine-tuning or few-shot
examples. CoMET's predictive power consistently improves as the model and
pretraining scale. Our results show that CoMET, a generative medical event
foundation model, can effectively capture complex clinical dynamics, providing
an extensible and generalizable framework to support clinical decision-making,
streamline healthcare operations, and improve patient outcomes.

</details>


### [322] [DynamixSFT: Dynamic Mixture Optimization of Instruction Tuning Collections](https://arxiv.org/abs/2508.12116)
*Haebin Shin,Lei Ji,Xiao Liu,Zhiwei Yu,Qi Chen,Yeyun Gong*

Main category: cs.LG

TL;DR: 提出一种动态自动化方法DynamixSFT，用于优化指令微调数据集的混合比例，在16个数据集的实验中提升了平均性能。


<details>
  <summary>Details</summary>
Motivation: 随着指令微调数据集的不断增加，如何动态平衡和优化这些数据集的混合比例是一个关键挑战。

Method: 将问题建模为多臂赌博机问题，提出Prior-scaled Boltzmann Exploration方法，通过轻量的1步前瞻性奖励更新采样概率，从而动态优化数据集使用。

Result: 在包含16个指令微调数据集的Tulu-v2-mixture上实验，性能在10个基准上平均提升了2.2%。

Conclusion: DynamixSFT成功实现了动态优化数据集混合，提升了模型性能，同时保留了数据集的多样性和覆盖范围。

Abstract: As numerous instruction-tuning datasets continue to emerge during the
post-training stage, dynamically balancing and optimizing their mixtures has
become a critical challenge. To address this, we propose DynamixSFT, a dynamic
and automated method for instruction-tuning dataset mixture optimization. We
formulate the problem as a multi-armed bandit setup and introduce a
Prior-scaled Boltzmann Exploration that softly anchors the updated sampling
distribution to the original dataset proportions, thereby preserving the
inherent diversity and coverage of the collection. Sampling probabilities are
updated using a lightweight 1-Step Look-ahead Reward, reflecting how much the
dataset contributes to improving the model's performance at its current state.
When applied to the Tulu-v2-mixture collection comprising 16 instruction-tuning
datasets, DynamixSFT achieves up to a 2.2% performance improvement across 10
benchmarks. Furthermore, we provide a comprehensive analysis and visualizations
to offer deeper insights into the adaptive dynamics of our method.

</details>


### [323] [Time-Scale Coupling Between States and Parameters in Recurrent Neural Networks](https://arxiv.org/abs/2508.12121)
*Lorenzo Livi*

Main category: cs.LG

TL;DR: 该论文研究了循环神经网络（RNN）中的门控机制如何在训练中，即使使用固定的全局学习率，也产生自适应的学习率行为。


<details>
  <summary>Details</summary>
Motivation: 探讨RNN中门控结构如何影响训练过程，尤其是训练动态与学习率之间的关系，从而解释RNN架构的稳定性和良好训练性能的原因。

Method: 通过推导泄漏积分器和门控循环神经网络的雅可比矩阵，分析门控如何改变梯度传播、调整有效步长以及引入参数更新的各向异性。此外形式化地分析其与学习率调度、动量法以及Adam自适应优化方法的关系。

Result: 数值实验验证了理论分析的正确性，说明门控导致的修正量虽小，但对训练动态有系统性影响。

Conclusion: 该工作通过动力系统的视角，揭示了门控机制如何通过数据驱动耦合状态演化与参数更新，从而解释了RNN良好训练性和稳定性的机制。

Abstract: We study how gating mechanisms in recurrent neural networks (RNNs) implicitly
induce adaptive learning-rate behavior, even when training is carried out with
a fixed, global learning rate. This effect arises from the coupling between
state-space time scales--parametrized by the gates--and parameter-space
dynamics during gradient descent. By deriving exact Jacobians for
leaky-integrator and gated RNNs, we obtain a first-order expansion that makes
explicit how constant, scalar, and multi-dimensional gates reshape gradient
propagation, modulate effective step sizes, and introduce anisotropy in
parameter updates. These findings reveal that gates not only control memory
retention in the hidden states, but also act as data-driven preconditioners
that adapt optimization trajectories in parameter space. We further draw formal
analogies with learning-rate schedules, momentum, and adaptive methods such as
Adam, showing that these optimization behaviors emerge naturally from gating.
Numerical experiments confirm the validity of our perturbative analysis,
supporting the view that gate-induced corrections remain small while exerting
systematic effects on training dynamics. Overall, this work provides a unified
dynamical-systems perspective on how gating couples state evolution with
parameter updates, explaining why gated architectures achieve robust
trainability and stability in practice.

</details>


### [324] [DE-VAE: Revealing Uncertainty in Parametric and Inverse Projections with Variational Autoencoders using Differential Entropy](https://arxiv.org/abs/2508.12145)
*Frederik L. Dennig,Daniel A. Keim*

Main category: cs.LG

TL;DR: 本文提出了一种名为DE-VAE的变分自编码器方法，通过微分熵增强了对数据的参数化和可逆投影能力，同时提供了对嵌入空间不确定性的分析能力。


<details>
  <summary>Details</summary>
Motivation: 现有的自编码器方法在对多维数据进行参数化和可逆投影时，对分布外样本处理较差，需要改进以提升性能。

Method: 通过使用微分熵（DE）构建不确定性感知的变分自编码器（DE-VAE），学得2D空间的嵌入映射及其逆映射，并与UMAP和t-SNE投影方法进行性能对比。

Result: 通过定量和定性评估，DE-VAE在四个知名数据集上实现了与现有方法相当的精度，并支持嵌入不确定性分析。

Conclusion: DE-VAE在提供参数化和可逆投影的同时，增加了分析嵌入不确定性的能力，展现出在自编码器领域的潜力。

Abstract: Recently, autoencoders (AEs) have gained interest for creating parametric and
invertible projections of multidimensional data. Parametric projections make it
possible to embed new, unseen samples without recalculating the entire
projection, while invertible projections allow the synthesis of new data
instances. However, existing methods perform poorly when dealing with
out-of-distribution samples in either the data or embedding space. Thus, we
propose DE-VAE, an uncertainty-aware variational AE using differential entropy
(DE) to improve the learned parametric and invertible projections. Given a
fixed projection, we train DE-VAE to learn a mapping into 2D space and an
inverse mapping back to the original space. We conduct quantitative and
qualitative evaluations on four well-known datasets, using UMAP and t-SNE as
baseline projection methods. Our findings show that DE-VAE can create
parametric and inverse projections with comparable accuracy to other current
AE-based approaches while enabling the analysis of embedding uncertainty.

</details>


### [325] [AICRN: Attention-Integrated Convolutional Residual Network for Interpretable Electrocardiogram Analysis](https://arxiv.org/abs/2508.12162)
*J. M. I. H. Jayakody,A. M. H. H. Alahakoon,C. R. M. Perera,R. M. L. C. Srimal,Roshan Ragel,Vajira Thambawita,Isuru Nawinne*

Main category: cs.LG

TL;DR: 本文提出了一种名为注意力集成卷积残差网络（AICRN）的新型深度学习架构，用于回归重要心电图参数，证明了深度学习在提升心电图分析精度和可解释性方面的价值。


<details>
  <summary>Details</summary>
Motivation: 当前心电图分析依赖于人工智能和机器学习实现实时数字分析，但仍面临诊断精度和预测能力挑战，需要设计新方法改善分析可靠性、速度和适应性。

Method: 设计了一种结合空间与通道注意力机制和卷积残差网络的模型（AICRN），以提高心电图参数回归的精度并解决梯度消失问题。模型针对心电图特征的类型和空间位置，优化了解释和提取的能力。

Result: 实验表明，AICRN在参数回归上表现优于现有的模型，具有更高的精确度，并能够显著简化心电图分析任务。

Conclusion: 深度学习具有提升心电图分析的可解释性和精度的潜力，为心脏监测和管理领域带来了新的临床应用可能。

Abstract: The paradigm of electrocardiogram (ECG) analysis has evolved into real-time
digital analysis, facilitated by artificial intelligence (AI) and machine
learning (ML), which has improved the diagnostic precision and predictive
capacity of cardiac diseases. This work proposes a novel deep learning (DL)
architecture called the attention-integrated convolutional residual network
(AICRN) to regress key ECG parameters such as the PR interval, the QT interval,
the QRS duration, the heart rate, the peak amplitude of the R wave, and the
amplitude of the T wave for interpretable ECG analysis. Our architecture is
specially designed with spatial and channel attention-related mechanisms to
address the type and spatial location of the ECG features for regression. The
models employ a convolutional residual network to address vanishing and
exploding gradient problems. The designed system addresses traditional analysis
challenges, such as loss of focus due to human errors, and facilitates the fast
and easy detection of cardiac events, thereby reducing the manual efforts
required to solve analysis tasks. AICRN models outperform existing models in
parameter regression with higher precision. This work demonstrates that DL can
play a crucial role in the interpretability and precision of ECG analysis,
opening up new clinical applications for cardiac monitoring and management.

</details>


### [326] [ProtTeX-CC: Activating In-Context Learning in Protein LLM via Two-Stage Instruction Compression](https://arxiv.org/abs/2508.12212)
*Chuanliu Fan,Zicheng Ma,Jun Gao,Nan Yu,Jun Zhang,Ziqiang Cao,Yi Qin Gao,Guohong Fu*

Main category: cs.LG

TL;DR: ProtTeX-CC 通过两阶段压缩框架改进了 ProtTeX 模型，解决了输入长度过长和缺乏上下文学习能力的问题，显著提升了蛋白质功能预测的性能。


<details>
  <summary>Details</summary>
Motivation: ProtTeX 模型在处理蛋白质功能预测时存在序列和结构表示过长，以及不支持上下文学习的问题，限制了其建模能力和泛化性能。

Method: 提出 ProtTeX-CC 框架，分为两阶段压缩：1. 联合嵌入压缩机制，融合序列和结构表示，在残基级别优化长度；2. 自压缩模块，在演示样本中进行隐空间聚合，极大缩短输入长度。此外，加入 PEFT 调优和单层投影参数进行轻量化优化。

Result: ProtTeX-CC 在 16-shot 设置下实现了 93.68% 的提示总长度压缩率。在蛋白质功能预测实验中，对域内基准性能提高了 2%，对域外数据集性能提高了 11%。

Conclusion: ProtTeX-CC 在保持 ProtTeX 主干模型的情况下，通过压缩提升了模型的建模效率和泛化能力，在少样本设置下性能表现出色，适合蛋白质功能预测任务。

Abstract: Recent advances in protein large language models, such as ProtTeX, represent
both side-chain amino acids and backbone structure as discrete token sequences
of residue length. While this design enables unified modeling of multimodal
protein information, it suffers from two major limitations: (1) The
concatenation of sequence and structure tokens approximately doubles the
protein length and breaks the intrinsic residue-level alignment between
modalities. (2) Constrained by the training corpus and limited context window,
ProtTeX is typically trained on single-protein inputs, rendering it
incompatible with in-context learning (ICL) and thus limiting its
generalization capability. To address these issues, we propose ProtTeX-CC, a
lightweight two-stage compression framework designed to enhance ProtTeX under
few-shot settings. We first design a joint embedding compression mechanism that
fuses sequence and structure representations at the residue level, effectively
reducing the protein input length by half without sacrificing performance. Then
we propose a self-compression module that aggregates each full demonstration
into the latent space of the last few linguistic tokens, reducing the average
demonstration length from 751 tokens to less than 16 tokens. Compared to the
original ProtTeX, our self-compression approach achieves a compression ratio of
approximately 93.68% in the total prompt length under the 16-shot setting.
Without modifying the backbone model, ProtTeX-CC introduces only a small number
of additional parameters through PEFT-based tuning in the joint embedding
compression stage and a single trainable projection layer in the
self-compression stage. Extensive experiments on protein function prediction
show that ProtTeX-CC improves performance on the in-domain benchmark by 2%, and
generalizes well to the out-of-domain dataset with a performance gain of 11%.

</details>


### [327] [Unlearning at Scale: Implementing the Right to be Forgotten in Large Language Models](https://arxiv.org/abs/2508.12220)
*Abdullah X*

Main category: cs.LG

TL;DR: 研究大型语言模型的『被遗忘权』问题，通过将其框定为一个可复现的系统问题，提出了一种结合训练记录与重放机制的解决方案。


<details>
  <summary>Details</summary>
Motivation: 探讨如何在满足GDPR规定的『被遗忘权』要求的同时，保持大型语言模型的准确性与高效性。

Method: 设计了一种记录训练过程中的最小必要信息（如ID哈希、随机种子等），以实现对遗忘请求的精确处理。此外，提出了一些辅助路径（如微检查点、曲率引导的反更新等）来满足实时性和可用性需求。

Result: 证明了在满足相关前提下，通过重新训练尾部模型可以实现参数与优化器状态的字节级一致性，并在玩具实验中验证了这一方法的可行性。

Conclusion: 该方法为满足『被遗忘权』要求提供了一种技术路径，同时兼顾了效率与模型质量。

Abstract: We study the right to be forgotten (GDPR Art. 17) for large language models
and frame unlearning as a reproducible systems problem. Our approach treats
training as a deterministic program and logs a minimal per-microbatch record
(ordered ID hash, RNG seed, learning-rate value, optimizer-step counter, and
accumulation boundary). Under a pinned stack and deterministic kernels,
replaying the training tail while filtering only the forget closure yields the
same parameters as training on the retain set (bit-identical in the training
dtype) when preconditions hold. To meet latency and availability constraints,
we add complementary paths: (i) exact reverts of recent steps via
micro-checkpoints or dense per-step deltas, (ii) cohort-scoped adapter deletion
when the base is frozen, and (iii) a curvature-guided anti-update followed by a
short retain-tune, audit-gated with escalation to exact replay. We report
storage/latency budgets and a toy artifact validating mechanics; in a
controlled run that satisfies the preconditions we demonstrate byte-identical
equality of model and optimizer states.

</details>


### [328] [Distribution Matching via Generalized Consistency Models](https://arxiv.org/abs/2508.12222)
*Sagar Shrestha,Rajesh Shrestha,Tri Nguyen,Subash Timilsina*

Main category: cs.LG

TL;DR: 该论文提出了一种受连续归一化流(CNF)一致性模型启发的新型分布匹配方法，并通过理论验证和实验展示了其性能优势。


<details>
  <summary>Details</summary>
Motivation: 解决生成对抗网络(GANs)因双层极小极大优化目标和模式崩塌问题导致的训练困难，同时结合CNF模型的优点发展新的分布匹配方法。

Method: 采用一种基于CNF模型一致性原则的分布匹配方法，拥有规范的范数最小化目标，并兼具灵活性以适应各种约束条件。

Result: 通过理论验证和在合成及真实世界数据集上的实验，证明了所提出方法的有效性与性能优势。

Conclusion: 提出的方法有效继承了GANs和CNF的优点，既解决了GANs的部分缺陷，又扩大了分布匹配任务中模型的应用潜力。

Abstract: Recent advancement in generative models have demonstrated remarkable
performance across various data modalities. Beyond their typical use in data
synthesis, these models play a crucial role in distribution matching tasks such
as latent variable modeling, domain translation, and domain adaptation.
Generative Adversarial Networks (GANs) have emerged as the preferred method of
distribution matching due to their efficacy in handling high-dimensional data
and their flexibility in accommodating various constraints. However, GANs often
encounter challenge in training due to their bi-level min-max optimization
objective and susceptibility to mode collapse. In this work, we propose a novel
approach for distribution matching inspired by the consistency models employed
in Continuous Normalizing Flow (CNF). Our model inherits the advantages of CNF
models, such as having a straight forward norm minimization objective, while
remaining adaptable to different constraints similar to GANs. We provide
theoretical validation of our proposed objective and demonstrate its
performance through experiments on synthetic and real-world datasets.

</details>


### [329] [Communication-Efficient Distributed Asynchronous ADMM](https://arxiv.org/abs/2508.12233)
*Sagar Shrestha*

Main category: cs.LG

TL;DR: 该研究提出了一种使用粗量化处理数据的异步ADMM方法，用于减轻大规模联邦学习与分布式优化中的通信开销。


<details>
  <summary>Details</summary>
Motivation: 在分布式优化和联邦学习中，通信成本限制了实际应用，尤其当节点的通信预算有限或传输数据量巨大时，亟需解决该问题。

Method: 作者提出在异步ADMM框架中引入粗量化处理数据，以降低通信开销，并在多种分布式学习任务中验证了其收敛性。

Result: 实验表明，提出的方法在分布式神经网络等任务中实现了收敛，有效降低了通信开销。

Conclusion: 该方法为大规模联邦学习和分布式优化提供了降低通信开销的有效解决方案，同时保证了方法的收敛性。

Abstract: In distributed optimization and federated learning, asynchronous alternating
direction method of multipliers (ADMM) serves as an attractive option for
large-scale optimization, data privacy, straggler nodes and variety of
objective functions. However, communication costs can become a major bottleneck
when the nodes have limited communication budgets or when the data to be
communicated is prohibitively large. In this work, we propose introducing
coarse quantization to the data to be exchanged in aynchronous ADMM so as to
reduce communication overhead for large-scale federated learning and
distributed optimization applications. We experimentally verify the convergence
of the proposed method for several distributed learning tasks, including neural
networks.

</details>


### [330] [CC-Time: Cross-Model and Cross-Modality Time Series Forecasting](https://arxiv.org/abs/2508.12235)
*Peng Chen,Yihang Wang,Yang Shu,Yunyao Cheng,Kai Zhao,Zhongwen Rao,Lujia Pan,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: 本文研究将预训练语言模型（PLM）应用于时间序列预测（TSF），并提出CC-Time方法，通过跨模态和跨模型学习提高预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于PLM的TSF方法在序列建模能力上仍然存在不足，无法实现预期的预测精度。因此作者研究如何更好地利用PLM的潜力提升时间序列预测性能。

Method: 提出了CC-Time方法，通过跨模态学习结合时间序列及其对应文本描述来建模时间依赖性和通道相关性，并通过跨模型融合模块整合PLM和时间序列模型的知识。

Result: 在九个真实数据集上的实验中，CC-Time在全数据训练和小样本学习中都实现了最优预测精度。

Conclusion: CC-Time成功地揭示了PLM在时间序列预测中的潜力，并通过创新的融合方法提升了模型精度。

Abstract: With the success of pre-trained language models (PLMs) in various application
fields beyond natural language processing, language models have raised emerging
attention in the field of time series forecasting (TSF) and have shown great
prospects. However, current PLM-based TSF methods still fail to achieve
satisfactory prediction accuracy matching the strong sequential modeling power
of language models. To address this issue, we propose Cross-Model and
Cross-Modality Learning with PLMs for time series forecasting (CC-Time). We
explore the potential of PLMs for time series forecasting from two aspects: 1)
what time series features could be modeled by PLMs, and 2) whether relying
solely on PLMs is sufficient for building time series models. In the first
aspect, CC-Time incorporates cross-modality learning to model temporal
dependency and channel correlations in the language model from both time series
sequences and their corresponding text descriptions. In the second aspect,
CC-Time further proposes the cross-model fusion block to adaptively integrate
knowledge from the PLMs and time series model to form a more comprehensive
modeling of time series patterns. Extensive experiments on nine real-world
datasets demonstrate that CC-Time achieves state-of-the-art prediction accuracy
in both full-data training and few-shot learning situations.

</details>


### [331] [DHG-Bench: A Comprehensive Benchmark on Deep Hypergraph Learning](https://arxiv.org/abs/2508.12244)
*Fan Li,Xiaoyang Wang,Wenjie Zhang,Ying Zhang,Xuemin Lin*

Main category: cs.LG

TL;DR: 本文提出了DHG-Bench，这是第一个针对深度超图学习的综合性基准，整合了20个涵盖不同任务的数据集和16种前沿算法，为研究超图神经网络提供系统性评估。


<details>
  <summary>Details</summary>
Motivation: 现有深度图模型无法有效处理复杂系统中广泛存在的高阶交互，超图神经网络因而受到关注，但缺乏综合基准妨碍了研究进展。

Method: 引入DHG-Bench基准，整合了20个数据集和16种算法，覆盖节点、边和图层级任务，在统一的实验设置下评估方法的有效性、效率、鲁棒性和公平性，并提供便于复现的研究工具库。

Result: 实验揭示了现有算法的优势及局限性，为未来研究提供了系统性洞见与方向。

Conclusion: DHG-Bench填补了深度超图学习领域基准的空白，为评估和推动该领域的发展提供了宝贵资源。

Abstract: Although conventional deep graph models have achieved great success in
relational learning, their focus on pairwise relationships limits their
capacity to learn pervasive higher-order interactions in real-world complex
systems, which can be naturally modeled as hypergraphs. To tackle this,
hypergraph neural networks (HNNs), the dominant approach in deep hypergraph
learning (DHGL), has garnered substantial attention in recent years. Despite
the proposal of numerous HNN methods, there is no comprehensive benchmark for
HNNs, which creates a great obstacle to understanding the progress of DHGL in
several aspects: (i) insufficient coverage of datasets, algorithms, and tasks;
(ii) a narrow evaluation of algorithm performance; and (iii) inconsistent
dataset usage, preprocessing, and experimental setups that hinder
comparability. To fill the gap, we introduce DHG-Bench, the first comprehensive
benchmark for DHGL. Specifically, DHG-Bench integrates 20 diverse datasets
spanning node-, edge-, and graph-level tasks, along with 16 state-of-the-art
HNN algorithms, under consistent data processing and experimental protocols.
Our benchmark systematically investigates the characteristics of HNNs in terms
of four dimensions: effectiveness, efficiency, robustness, and fairness.
Further, to facilitate reproducible research, we have developed an easy-to-use
library for training and evaluating different HNN methods. Extensive
experiments conducted with DHG-Bench reveal both the strengths and inherent
limitations of existing algorithms, offering valuable insights and directions
for future research. The code is publicly available at:
https://github.com/Coco-Hut/DHG-Bench.

</details>


### [332] [STM3: Mixture of Multiscale Mamba for Long-Term Spatio-Temporal Time-Series Prediction](https://arxiv.org/abs/2508.12247)
*Haolong Chen,Liang Zhang,Zhengyuan Xin,Guangxu Zhu*

Main category: cs.LG

TL;DR: 提出了一种新的模型STM2和增强版STM3，通过多尺度Mamba架构和自适应图因果卷积网络，高效捕抓复杂的多尺度时空依赖，并达到最先进的预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在长时空依赖学习中效率低，尤其在多尺度信息提取与建模方面存在不足。

Method: 提出STM2模型，使用多尺度Mamba架构和自适应图因果卷积网络，并设计STM3增强版，采用专家混合架构和因果对比学习策略。

Result: 在实际数据基准上实验表现优异，实现了长时空时间序列预测的最新性能。

Conclusion: STM2/STM3显著提升了长时空依赖学习能力，为时空预测任务带来了新突破。

Abstract: Recently, spatio-temporal time-series prediction has developed rapidly, yet
existing deep learning methods struggle with learning complex long-term
spatio-temporal dependencies efficiently. The long-term spatio-temporal
dependency learning brings two new challenges: 1) The long-term temporal
sequence includes multiscale information naturally which is hard to extract
efficiently; 2) The multiscale temporal information from different nodes is
highly correlated and hard to model. To address these challenges, we propose an
efficient \textit{\textbf{S}patio-\textbf{T}emporal \textbf{M}ultiscale
\textbf{M}amba} (STM2) that includes a multiscale Mamba architecture to capture
the multiscale information efficiently and simultaneously, and an adaptive
graph causal convolution network to learn the complex multiscale
spatio-temporal dependency. STM2 includes hierarchical information aggregation
for different-scale information that guarantees their distinguishability. To
capture diverse temporal dynamics across all spatial nodes more efficiently, we
further propose an enhanced version termed
\textit{\textbf{S}patio-\textbf{T}emporal \textbf{M}ixture of
\textbf{M}ultiscale \textbf{M}amba} (STM3) that employs a special
Mixture-of-Experts architecture, including a more stable routing strategy and a
causal contrastive learning strategy to enhance the scale distinguishability.
We prove that STM3 has much better routing smoothness and guarantees the
pattern disentanglement for each expert successfully. Extensive experiments on
real-world benchmarks demonstrate STM2/STM3's superior performance, achieving
state-of-the-art results in long-term spatio-temporal time-series prediction.

</details>


### [333] [Interpreting Time Series Forecasts with LIME and SHAP: A Case Study on the Air Passengers Dataset](https://arxiv.org/abs/2508.12253)
*Manish Shukla*

Main category: cs.LG

TL;DR: 本文提出了一种解释时间序列预测的方法，通过结合LIME和SHAP工具，展现了ARIMA模型和基于梯度树增强模型的预测解释能力。作者用航空客流量数据集进行案例研究，证明了一些特定滞后特征在预测中的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列模型面临平衡线性和非线性适应能力与可解释性的难题。研究旨在设计一种统一框架，同时兼顾模型的精度与可解释性。

Method: 通过将时间序列转换为监督学习问题，训练梯度提升树模型与ARIMA模型作为基线模型，并结合LIME与SHAP开展后续可解释性分析。

Result: 在Air Passengers数据集中的实验表明，少量滞后特征（尤其是12个月的滞后）和季节性编码对预测差异有重要贡献。

Conclusion: 研究提供了时间序列预测可解释性的一个新方案，并强调了特定滞后特征对预测的意义，为时间序列分析领域提供了新的方法论指导。

Abstract: Time-series forecasting underpins critical decisions across aviation, energy,
retail and health. Classical autoregressive integrated moving average (ARIMA)
models offer interpretability via coefficients but struggle with
nonlinearities, whereas tree-based machine-learning models such as XGBoost
deliver high accuracy but are often opaque. This paper presents a unified
framework for interpreting time-series forecasts using local interpretable
model-agnostic explanations (LIME) and SHapley additive exPlanations (SHAP). We
convert a univariate series into a leakage-free supervised learning problem,
train a gradient-boosted tree alongside an ARIMA baseline and apply post-hoc
explainability. Using the Air Passengers dataset as a case study, we show that
a small set of lagged features -- particularly the twelve-month lag -- and
seasonal encodings explain most forecast variance. We contribute: (i) a
methodology for applying LIME and SHAP to time series without violating
chronology; (ii) theoretical exposition of the underlying algorithms; (iii)
empirical evaluation with extensive analysis; and (iv) guidelines for
practitioners.

</details>


### [334] [L-SR1: Learned Symmetric-Rank-One Preconditioning](https://arxiv.org/abs/2508.12270)
*Gal Lifshitz,Shahar Zuler,Ori Fouks,Dan Raviv*

Main category: cs.LG

TL;DR: 提出了一种新颖的二阶优化方法，结合了学习和经典优化的优点，并在单目人体网格恢复等任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要大量标注数据，泛化能力不足且计算量高；经典方法数据效率高但收敛慢；学习优化器二阶方法研究较少。

Method: 设计了一种可训练的预处理单元，增强经典的SR1算法，通过数据驱动向量构建正半定矩阵，并通过学习投影对齐割线约束。

Result: 在分析实验及真实任务如单目人体网格恢复中表现优于现有基于学习的优化方法。

Conclusion: 方法无需标注数据或微调，具备强泛化能力，适合与优化框架集成，且模型轻量化。

Abstract: End-to-end deep learning has achieved impressive results but remains limited
by its reliance on large labeled datasets, poor generalization to unseen
scenarios, and growing computational demands. In contrast, classical
optimization methods are data-efficient and lightweight but often suffer from
slow convergence. While learned optimizers offer a promising fusion of both
worlds, most focus on first-order methods, leaving learned second-order
approaches largely unexplored.
  We propose a novel learned second-order optimizer that introduces a trainable
preconditioning unit to enhance the classical Symmetric-Rank-One (SR1)
algorithm. This unit generates data-driven vectors used to construct positive
semi-definite rank-one matrices, aligned with the secant constraint via a
learned projection. Our method is evaluated through analytic experiments and on
the real-world task of Monocular Human Mesh Recovery (HMR), where it
outperforms existing learned optimization-based approaches. Featuring a
lightweight model and requiring no annotated data or fine-tuning, our approach
offers strong generalization and is well-suited for integration into broader
optimization-based frameworks.

</details>


### [335] [CRoC: Context Refactoring Contrast for Graph Anomaly Detection with Limited Supervision](https://arxiv.org/abs/2508.12278)
*Siyue Xie,Da Sun Handason Tam,Wing Cheong Lau*

Main category: cs.LG

TL;DR: 提出了一种名为CRoC的新框架，通过利用有限的标注数据和丰富的无标注数据，增强图神经网络(GNN)在图异常检测任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的图异常检测面临标注数据稀缺的问题，而异常的数据往往稀有且难以标注，同时异常数据可能会伪装自身以逃避检测。

Method: CRoC框架通过重构节点上下文、采用对比学习范式以及单独编码异质关系等方式，提高GNN捕捉复杂交互语义和抗伪装的能力，并结合无标签数据进行训练。

Result: 在七个规模不同的实际GAD数据集上进行验证，CRoC的AUC比基线方法提升了最高14%，并在有限标签设置下超过了最新方法。

Conclusion: CRoC框架证明在图异常检测任务中具有很强的竞争力，尤其是在标签极其有限的情况下通过增强数据利用和模型鲁棒性取得了显著的性能提升。

Abstract: Graph Neural Networks (GNNs) are widely used as the engine for various
graph-related tasks, with their effectiveness in analyzing graph-structured
data. However, training robust GNNs often demands abundant labeled data, which
is a critical bottleneck in real-world applications. This limitation severely
impedes progress in Graph Anomaly Detection (GAD), where anomalies are
inherently rare, costly to label, and may actively camouflage their patterns to
evade detection. To address these problems, we propose Context Refactoring
Contrast (CRoC), a simple yet effective framework that trains GNNs for GAD by
jointly leveraging limited labeled and abundant unlabeled data. Different from
previous works, CRoC exploits the class imbalance inherent in GAD to refactor
the context of each node, which builds augmented graphs by recomposing the
attributes of nodes while preserving their interaction patterns. Furthermore,
CRoC encodes heterogeneous relations separately and integrates them into the
message-passing process, enhancing the model's capacity to capture complex
interaction semantics. These operations preserve node semantics while
encouraging robustness to adversarial camouflage, enabling GNNs to uncover
intricate anomalous cases. In the training stage, CRoC is further integrated
with the contrastive learning paradigm. This allows GNNs to effectively harness
unlabeled data during joint training, producing richer, more discriminative
node embeddings. CRoC is evaluated on seven real-world GAD datasets with
varying scales. Extensive experiments demonstrate that CRoC achieves up to 14%
AUC improvement over baseline GNNs and outperforms state-of-the-art GAD methods
under limited-label settings.

</details>


### [336] [Convergence Analysis of the Lion Optimizer in Centralized and Distributed Settings](https://arxiv.org/abs/2508.12327)
*Wei Jiang,Lijun Zhang*

Main category: cs.LG

TL;DR: 研究Lion优化器的收敛性质，包括标准Lion及其方差减小版本在单机和分布式环境中的收敛率表现，以及提出了一种通信高效的分布式变体。


<details>
  <summary>Details</summary>
Motivation: 探讨Lion优化器在不同环境中的收敛性能和效率优化，以应对复杂优化问题。

Method: 分析标准Lion优化器和支持方差减小的改进版；研究其在分布式设置中的性能，并创新性地提出一种通信高效变体。

Result: 标准Lion在单机的收敛率为$O(d^{1/2}T^{-1/4})$，加方差减小后达到$O(d^{1/2}T^{-1/3})$；标准Lion在分布式环境中为$O(d^{1/2}(nT)^{-1/4})$，方差减小后为$O(d^{1/2}(nT)^{-1/3})$；创新通信效版本的收敛率为$O\left(\max \{d^{1/4}T^{-1/4}, d^{1/10}(nT)^{-1/5} \}\right)$与$O(d^{1/4}T^{-1/4})$。

Conclusion: Lion优化器及其改进版在单机和分布式环境中均表现出了良好的收敛性质，而通信高效变体为分布式优化提供了进一步的效率提升。

Abstract: In this paper, we analyze the convergence properties of the Lion optimizer.
First, we establish that the Lion optimizer attains a convergence rate of
$\mathcal{O}(d^{1/2}T^{-1/4})$ under standard assumptions, where $d$ denotes
the problem dimension and $T$ is the iteration number. To further improve this
rate, we introduce the Lion optimizer with variance reduction, resulting in an
enhanced convergence rate of $\mathcal{O}(d^{1/2}T^{-1/3})$. We then analyze in
distributed settings, where the standard and variance reduced version of the
distributed Lion can obtain the convergence rates of
$\mathcal{O}(d^{1/2}(nT)^{-1/4})$ and $\mathcal{O}(d^{1/2}(nT)^{-1/3})$, with
$n$ denoting the number of nodes. Furthermore, we investigate a
communication-efficient variant of the distributed Lion that ensures sign
compression in both communication directions. By employing the unbiased sign
operations, the proposed Lion variant and its variance reduction counterpart,
achieve convergence rates of $\mathcal{O}\left( \max
\left\{\frac{d^{1/4}}{T^{1/4}}, \frac{d^{1/10}}{n^{1/5}T^{1/5}} \right\}
\right)$ and $\mathcal{O}\left( \frac{d^{1/4}}{T^{1/4}} \right)$, respectively.

</details>


### [337] [Navigating the Exploration-Exploitation Tradeoff in Inference-Time Scaling of Diffusion Models](https://arxiv.org/abs/2508.12361)
*Xun Su,Jianming Huang,Yang Yusen,Zhongxi Fang,Hiroyuki Kasai*

Main category: cs.LG

TL;DR: 本文探索了适用于扩散模型的推理时刻缩放技术，提出了Funnels Schedule和Adaptive Temperature两种方法，显著提高了样本质量。


<details>
  <summary>Details</summary>
Motivation: 当前SMC在扩散模型应用中面临样本评估难易度的两难问题，需解决此问题以提升模型性能。

Method: 从搜索算法角度出发，提出Funnels Schedule和Adaptive Temperature两种策略，分别减少粒子数量和调整早期奖励的影响力。

Result: 通过实验验证，这些方法提高了样本质量且不增加计算量，在多个基准上优于现有基线。

Conclusion: 提出的方法有效地解决了探索与利用的冲突问题，提高了扩散模型的生成性能。

Abstract: Inference-time scaling has achieved remarkable success in language models,
yet its adaptation to diffusion models remains underexplored. We observe that
the efficacy of recent Sequential Monte Carlo (SMC)-based methods largely stems
from globally fitting the The reward-tilted distribution, which inherently
preserves diversity during multi-modal search. However, current applications of
SMC to diffusion models face a fundamental dilemma: early-stage noise samples
offer high potential for improvement but are difficult to evaluate accurately,
whereas late-stage samples can be reliably assessed but are largely
irreversible. To address this exploration-exploitation trade-off, we approach
the problem from the perspective of the search algorithm and propose two
strategies: Funnel Schedule and Adaptive Temperature. These simple yet
effective methods are tailored to the unique generation dynamics and
phase-transition behavior of diffusion models. By progressively reducing the
number of maintained particles and down-weighting the influence of early-stage
rewards, our methods significantly enhance sample quality without increasing
the total number of Noise Function Evaluations. Experimental results on
multiple benchmarks and state-of-the-art text-to-image diffusion models
demonstrate that our approach outperforms previous baselines.

</details>


### [338] [Bi-Axial Transformers: Addressing the Increasing Complexity of EHR Classification](https://arxiv.org/abs/2508.12418)
*Rachael DeVries,Casper Christensen,Marie Lisandra Zepeda Mendoza,Ole Winther*

Main category: cs.LG

TL;DR: 本文提出Bi-Axial Transformer (BAT)，应用于电子健康记录(EHR)分析，解决数据稀疏性和缺失问题，达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: EHR数据复杂性增加，但现有方法在处理数据稀疏性和缺失性上表现不足，亟需更强大的模型。

Method: 提出BAT模型，同时关注EHR数据的时间轴和临床变量轴，以学习更丰富的数据关系，增强对数据稀疏和缺失的处理能力。

Result: BAT在脓毒症预测任务上达到最先进性能，并在死亡率分类上表现出竞争力，且其鲁棒性和转移学习能力优于其他Transformer模型。

Conclusion: BAT有效解决EHR数据稀疏性和缺失问题，为复杂医疗数据分析提供了强有力的工具，具有广泛应用潜力。

Abstract: Electronic Health Records (EHRs), the digital representation of a patient's
medical history, are a valuable resource for epidemiological and clinical
research. They are also becoming increasingly complex, with recent trends
indicating larger datasets, longer time series, and multi-modal integrations.
Transformers, which have rapidly gained popularity due to their success in
natural language processing and other domains, are well-suited to address these
challenges due to their ability to model long-range dependencies and process
data in parallel. But their application to EHR classification remains limited
by data representations, which can reduce performance or fail to capture
informative missingness. In this paper, we present the Bi-Axial Transformer
(BAT), which attends to both the clinical variable and time point axes of EHR
data to learn richer data relationships and address the difficulties of data
sparsity. BAT achieves state-of-the-art performance on sepsis prediction and is
competitive to top methods for mortality classification. In comparison to other
transformers, BAT demonstrates increased robustness to data missingness, and
learns unique sensor embeddings which can be used in transfer learning.
Baseline models, which were previously located across multiple repositories or
utilized deprecated libraries, were re-implemented with PyTorch and made
available for reproduction and future benchmarking.

</details>


### [339] [Machine Learning-Based Manufacturing Cost Prediction from 2D Engineering Drawings via Geometric Features](https://arxiv.org/abs/2508.12440)
*Ahmet Bilal Arıkan,Şener Özönder,Mustafa Taha Koçyiğit,Hüseyin Oktay Altun,H. Kübra Küçükkartal,Murat Arslanoğlu,Fatih Çağırankaya,Berk Ayvaz*

Main category: cs.LG

TL;DR: 提出了一种基于机器学习的框架，实现从2D工程图到制造成本的自动估算。


<details>
  <summary>Details</summary>
Motivation: 希望缩短报价时间，并提供一致透明的成本评估，以适应工业4.0的需求。

Method: 使用机器学习方法，通过提取200种几何和统计特征，应用解决决策树模型（如XGBoost、CatBoost等）训练成本预测模型。

Result: 模型在24个产品组中实现了约10%的MAPE，展现了超越零件特定启发式规则的稳定性和可扩展性。

Conclusion: 该方法不仅缩短了报价时间，还通过SHAP等工具提供了设计驱动因素的洞察，为基于成本的实时决策支持提供了路径。

Abstract: We present an integrated machine learning framework that transforms how
manufacturing cost is estimated from 2D engineering drawings. Unlike
traditional quotation workflows that require labor-intensive process planning,
our approach about 200 geometric and statistical descriptors directly from
13,684 DWG drawings of automotive suspension and steering parts spanning 24
product groups. Gradient-boosted decision tree models (XGBoost, CatBoost,
LightGBM) trained on these features achieve nearly 10% mean absolute percentage
error across groups, demonstrating robust scalability beyond part-specific
heuristics. By coupling cost prediction with explainability tools such as SHAP,
the framework identifies geometric design drivers including rotated dimension
maxima, arc statistics and divergence metrics, offering actionable insights for
cost-aware design. This end-to-end CAD-to-cost pipeline shortens quotation lead
times, ensures consistent and transparent cost assessments across part families
and provides a deployable pathway toward real-time, ERP-integrated decision
support in Industry 4.0 manufacturing environments.

</details>


### [340] [Local Cluster Cardinality Estimation for Adaptive Mean Shift](https://arxiv.org/abs/2508.12450)
*Étienne Pepin*

Main category: cs.LG

TL;DR: 提出了一种适应性均值漂移算法，通过调整带宽和均值漂移核心半径实现对局部尺度与簇基数变化的数据集的高效聚类。


<details>
  <summary>Details</summary>
Motivation: 解决当前基于核密度估计(KDE)方法缺乏对簇区域整体洞察力的问题，改善均值漂移聚类在局部尺度和簇基数不同的数据集中的适应性。

Method: 利用点到其他点的局部距离分布估计局部簇基数，通过距离分布密度的局部最小值来识别并计算整个簇的参数；在均值漂移执行过程中，根据簇基数估计自适应调整带宽和核心半径阈值。

Result: 算法在原始数据集上优于最近的自适应均值漂移方法，并在更广泛的聚类基准测试上表现出竞争力。

Conclusion: 这种改进的均值漂移算法提高了对多种数据集的处理能力与聚类效果，特别是在局部尺度变化和簇基数不一致的情况下。

Abstract: This article presents an adaptive mean shift algorithm designed for datasets
with varying local scale and cluster cardinality. Local distance distributions,
from a point to all others, are used to estimate the cardinality of the local
cluster by identifying a local minimum in the density of the distance
distribution. Based on these cardinality estimates, local cluster parameters
are then computed for the entire cluster in contrast to KDE-based methods,
which provide insight only into localized regions of the cluster. During the
mean shift execution, the cluster cardinality estimate is used to adaptively
adjust the bandwidth and the mean shift kernel radius threshold. Our algorithm
outperformed a recently proposed adaptive mean shift method on its original
dataset and demonstrated competitive performance on a broader clustering
benchmark.

</details>


### [341] [Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for NGINX](https://arxiv.org/abs/2508.12485)
*Aayush Gupta,Arpit Bhayani*

Main category: cs.LG

TL;DR: 引入了一种新的基于强化学习的缓存驱逐策略Cold-RL，可以在NGINX中替代传统的LRU策略，显著提高命中率并在严格的微秒级预算下运行。


<details>
  <summary>Details</summary>
Motivation: 传统的LRU驱逐策略在处理周期性突发和混合对象大小时表现不佳，研究旨在通过强化学习提升缓存策略性能。

Method: 设计并实现了一种名为Cold-RL的算法，基于深度Q网络，在NGINX中替代LRU的驱逐路径，并对缓存对象施加预设特征和奖励机制进行训练和优化。

Result: 在不同缓存容量下测试，Cold-RL显著提升缓存命中率，在25MB缓存中将命中率从0.1436提高到0.3538，达到146%的提升，同时保持低CPU开销和严格的延迟要求。

Conclusion: 该研究首次成功在NGINX中集成了强化学习驱逐策略，证明了其在提升性能和高效利用资源方面的潜力。

Abstract: Web proxies such as NGINX commonly rely on least-recently-used (LRU)
eviction, which is size agnostic and can thrash under periodic bursts and mixed
object sizes. We introduce Cold-RL, a learned eviction policy for NGINX that
replaces LRU's forced-expire path with a dueling Deep Q-Network served by an
ONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL
samples the K least-recently-used objects, extracts six lightweight features
(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),
and requests a bitmask of victims; a hard timeout of 500 microseconds triggers
immediate fallback to native LRU. Policies are trained offline by replaying
NGINX access logs through a cache simulator with a simple reward: a retained
object earns one point if it is hit again before TTL expiry. We compare against
LRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial
workloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,
a 146 percent improvement over the best classical baseline; at 100 MB, from
0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods
(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th
percentile eviction latency within budget. To our knowledge, this is the first
reinforcement learning eviction policy integrated into NGINX with strict SLOs.

</details>


### [342] [Cost-Aware Contrastive Routing for LLMs](https://arxiv.org/abs/2508.12491)
*Reza Shirkavand,Shangqian Gao,Peiran Yu,Heng Huang*

Main category: cs.LG

TL;DR: 本文提出了一个新的模型路由框架CSCR，通过共享嵌入空间实现低成本、高效率的模型选择，显著提升了准确性与成本的权衡。


<details>
  <summary>Details</summary>
Motivation: 解决现有模型路由方法忽视特定上下文、模型固定、效率低等问题。

Method: 设计了一个对比编码器，将提示和模型嵌入到共享空间，实现快速低成本模型选择；并引入k-NN查询，实现微秒级推理延迟。

Result: CSCR在多种基准测试中表现卓越，相较基线提升准确率和成本权衡达25%，并对未知模型和分布外提示展现出良好泛化性。

Conclusion: CSCR框架有效整合了成本敏感性与性能，具有适应性强、延迟低等优势，适合动态模型池的路由应用。

Abstract: We study cost-aware routing for large language models across diverse and
dynamic pools of models. Existing approaches often overlook prompt-specific
context, rely on expensive model profiling, assume a fixed set of experts, or
use inefficient trial-and-error strategies. We introduce Cost-Spectrum
Contrastive Routing (CSCR), a lightweight framework that maps both prompts and
models into a shared embedding space to enable fast, cost-sensitive selection.
CSCR uses compact, fast-to-compute logit footprints for open-source models and
perplexity fingerprints for black-box APIs. A contrastive encoder is trained to
favor the cheapest accurate expert within adaptive cost bands. At inference
time, routing reduces to a single k-NN lookup via a FAISS index, requiring no
retraining when the expert pool changes and enabling microsecond latency.
Across multiple benchmarks, CSCR consistently outperforms baselines, improving
the accuracy-cost tradeoff by up to 25%, while generalizing robustly to unseen
LLMs and out-of-distribution prompts.

</details>


### [343] [Trust Region Constrained Measure Transport in Path Space for Stochastic Optimal Control and Inference](https://arxiv.org/abs/2508.12511)
*Denis Blessing,Julius Berner,Lorenz Richter,Carles Domingo-Enrich,Yuanqi Du,Arash Vahdat,Gerhard Neumann*

Main category: cs.LG

TL;DR: 提出一种基于信任区域的策略，用于解决含平方控制成本的随机最优控制问题，通过几何退火逐步逼近目标路径测度。


<details>
  <summary>Details</summary>
Motivation: 解决目标与先验测度差距很大时的优化困难，提高最优控制问题的求解效率。

Method: 提出信任区域约束问题，逐步缩小与目标测度的差距，并通过几何退火处理这种逐步逼近的过程以优化路径选择。

Result: 该方法在扩散采样、过渡路径采样以及扩散模型微调等多个最优控制应用中显著提高了性能。

Conclusion: 基于信任区域的几何退火路径优化方法在解决随机最优控制问题方面表现出色，具有广泛的应用潜力。

Abstract: Solving stochastic optimal control problems with quadratic control costs can
be viewed as approximating a target path space measure, e.g. via gradient-based
optimization. In practice, however, this optimization is challenging in
particular if the target measure differs substantially from the prior. In this
work, we therefore approach the problem by iteratively solving constrained
problems incorporating trust regions that aim for approaching the target
measure gradually in a systematic way. It turns out that this trust region
based strategy can be understood as a geometric annealing from the prior to the
target measure, where, however, the incorporated trust regions lead to a
principled and educated way of choosing the time steps in the annealing path.
We demonstrate in multiple optimal control applications that our novel method
can improve performance significantly, including tasks in diffusion-based
sampling, transition path sampling, and fine-tuning of diffusion models.

</details>


### [344] [Results of the NeurIPS 2023 Neural MMO Competition on Multi-task Reinforcement Learning](https://arxiv.org/abs/2508.12524)
*Joseph Suárez,Kyoung Whan Choe,David Bloomin,Jianming Gao,Yunkun Li,Yao Feng,Saidinesh Pola,Kun Zhang,Yonghui Zhu,Nikhil Pinnaparaju,Hao Xiang Li,Nishaanth Kanna,Daniel Scott,Ryan Sullivan,Rose S. Shuman,Lucas de Alcântara,Herbie Bradley,Kirsty You,Bo Wu,Yuhao Jiang,Qimai Li,Jiaxin Chen,Louis Castricato,Xiaolong Zhu,Phillip Isola*

Main category: cs.LG

TL;DR: 本文介绍了2023年NeurIPS Neural MMO竞赛的结果，220多名参与者提交内容，最佳方案达到4倍基准性能。


<details>
  <summary>Details</summary>
Motivation: 报告2023年NeurIPS Neural MMO竞赛中新技术的开发与成果，推动研究社区探讨普适性更强的目标条件策略。

Method: 参与者通过训练目标条件策略，测试其在未知任务、地图和对手场景中的生成能力。最佳方案用单张4090 GPU在8小时内完成训练。

Result: 最佳获取了基准性能4倍的分数。所有与Neural MMO竞赛相关的内容均在MIT许可下开源。

Conclusion: 比赛展示了目标条件策略在一般化任务中的强大潜力，同时推动了开发与开放共享。」

Abstract: We present the results of the NeurIPS 2023 Neural MMO Competition, which
attracted over 200 participants and submissions. Participants trained
goal-conditional policies that generalize to tasks, maps, and opponents never
seen during training. The top solution achieved a score 4x higher than our
baseline within 8 hours of training on a single 4090 GPU. We open-source
everything relating to Neural MMO and the competition under the MIT license,
including the policy weights and training code for our baseline and for the top
submissions.

</details>


### [345] [Toward Architecture-Agnostic Local Control of Posterior Collapse in VAEs](https://arxiv.org/abs/2508.12530)
*Hyunsoo Song,Seungwhan Kim,Seungkyu Lee*

Main category: cs.LG

TL;DR: 提出了一种新型的损失函数——Latent Reconstruction (LR) loss，有效避免了变分自动编码器（VAE）的后验崩塌，同时无需对网络架构进行限制。


<details>
  <summary>Details</summary>
Motivation: 针对变分自动编码器（VAE）的后验崩塌问题，现有方法在重建与正则化之间的权衡效果不佳，且大多依赖于特定的网络结构。

Method: 定义了局部后验崩塌的概念，提出了基于注入和复合函数的数学性质的新损失函数LR loss，用以灵活控制后验崩塌，并放宽对网络架构的限制。

Result: 通过MNIST、fashionMNIST、Omniglot、CelebA和FFHQ等多个数据集进行实验，验证了该方法对后验崩塌的有效控制。

Conclusion: LR loss展示了在不限制网络架构下解决后验崩塌问题的潜力，为生成模型研究提供了新的方向。

Abstract: Variational autoencoders (VAEs), one of the most widely used generative
models, are known to suffer from posterior collapse, a phenomenon that reduces
the diversity of generated samples. To avoid posterior collapse, many prior
works have tried to control the influence of regularization loss. However, the
trade-off between reconstruction and regularization is not satisfactory. For
this reason, several methods have been proposed to guarantee latent
identifiability, which is the key to avoiding posterior collapse. However, they
require structural constraints on the network architecture. For further
clarification, we define local posterior collapse to reflect the importance of
individual sample points in the data space and to relax the network constraint.
Then, we propose Latent Reconstruction(LR) loss, which is inspired by
mathematical properties of injective and composite functions, to control
posterior collapse without restriction to a specific architecture. We
experimentally evaluate our approach, which controls posterior collapse on
varied datasets such as MNIST, fashionMNIST, Omniglot, CelebA, and FFHQ.

</details>


### [346] [Rethinking Safety in LLM Fine-tuning: An Optimization Perspective](https://arxiv.org/abs/2508.12531)
*Minseon Kim,Jin Myung Kwak,Lama Alssum,Bernard Ghanem,Philip Torr,David Krueger,Fazl Barez,Adel Bibi*

Main category: cs.LG

TL;DR: 本文提出通过优化训练超参数（如学习率、批量大小和梯度步数）和使用指数移动平均（EMA）动量技术，可以在微调语言模型时减少安全问题，无需额外的安全数据。


<details>
  <summary>Details</summary>
Motivation: 当前认为微调语言模型会不可避免地影响其安全性（忽略有害请求的能力），需额外的安全措施；但本文挑战这一观点。

Method: 通过优化关键超参数（学习率、批量大小和梯度步数），以及引入参数空间的指数移动平均（EMA）动量技术，确保优化路径稳定，保留预训练模型的安全特性。

Result: 实验表明，将有害响应从16%降低到约5%，性能优于需要额外安全数据的方法，并在多数据集（Dolly, Alpaca, ORCA）上验证了实用性。

Conclusion: 通过选择合适的优化配置和使用EMA技术，可以在保持模型效用的同时，大幅减少微调过程中的安全问题，无需额外的安全数据支持。

Abstract: Fine-tuning language models is commonly believed to inevitably harm their
safety, i.e., refusing to respond to harmful user requests, even when using
harmless datasets, thus requiring additional safety measures. We challenge this
belief through systematic testing, showing that poor optimization choices,
rather than inherent trade-offs, often cause safety problems, measured as
harmful responses to adversarial prompts. By properly selecting key training
hyper-parameters, e.g., learning rate, batch size, and gradient steps, we
reduce unsafe model responses from 16\% to approximately 5\%, as measured by
keyword matching, while maintaining utility performance. Based on this
observation, we propose a simple exponential moving average (EMA) momentum
technique in parameter space that preserves safety performance by creating a
stable optimization path and retains the original pre-trained model's safety
properties. Our experiments on the Llama families across multiple datasets
(Dolly, Alpaca, ORCA) demonstrate that safety problems during fine-tuning can
largely be avoided without specialized interventions, outperforming existing
approaches that require additional safety data while offering practical
guidelines for maintaining both model performance and safety during adaptation.

</details>


### [347] [Defining and Benchmarking a Data-Centric Design Space for Brain Graph Construction](https://arxiv.org/abs/2508.12533)
*Qinwen Ge,Roza G. Bayrak,Anwar Said,Catie Chang,Xenofon Koutsoukos,Tyler Derr*

Main category: cs.LG

TL;DR: 文章探讨了基于fMRI数据构建脑图的最佳方法，通过数据为中心的视角评估不同构建方式对下游任务的影响，提出优化的设计配置并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前脑图构建流程常为固定化模型，忽略数据构建的细节对结果的影响，因此需要一种系统评估数据构建方式的新方案来提升下游任务表现。

Method: 将脑图构建分为三阶段：时间信号处理、拓扑提取和图特征化，通过研究现有和改进技术的组合与变体影响分析数据构建的表现，进行实验验证。

Result: 在HCP1200和ABIDE等数据集上，优化的数据为中心配置在分类任务中显著优于标准构建流程。

Conclusion: 数据为中心的脑图构建策略能显著提升神经影像相关任务的表现，未来应重点系统探索数据构建对性能的影响。

Abstract: The construction of brain graphs from functional Magnetic Resonance Imaging
(fMRI) data plays a crucial role in enabling graph machine learning for
neuroimaging. However, current practices often rely on rigid pipelines that
overlook critical data-centric choices in how brain graphs are constructed. In
this work, we adopt a Data-Centric AI perspective and systematically define and
benchmark a data-centric design space for brain graph construction,
constrasting with primarily model-centric prior work. We organize this design
space into three stages: temporal signal processing, topology extraction, and
graph featurization. Our contributions lie less in novel components and more in
evaluating how combinations of existing and modified techniques influence
downstream performance. Specifically, we study high-amplitude BOLD signal
filtering, sparsification and unification strategies for connectivity,
alternative correlation metrics, and multi-view node and edge features, such as
incorporating lagged dynamics. Experiments on the HCP1200 and ABIDE datasets
show that thoughtful data-centric configurations consistently improve
classification accuracy over standard pipelines. These findings highlight the
critical role of upstream data decisions and underscore the importance of
systematically exploring the data-centric design space for graph-based
neuroimaging. Our code is available at
https://github.com/GeQinwen/DataCentricBrainGraphs.

</details>


### [348] [OS-R1: Agentic Operating System Kernel Tuning with Reinforcement Learning](https://arxiv.org/abs/2508.12551)
*Hongyu Lin,Yuchen Li,Haoran Luo,Kaichun Yao,Libo Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: 该论文介绍了一个基于规则的强化学习框架OS-R1，用于优化Linux内核调优，能显著提高性能并具备实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的Linux内核调优方法在效率、可扩展性和泛化能力上存在挑战，需要更高效的方法。

Method: 通过将内核配置抽象为强化学习环境，并结合大语言模型的探索能力，设计了定制的奖励函数，还提出了一个两阶段训练过程，提升收敛速度并减少重训需求。

Result: 实验表明，与基线方法相比，OS-R1性能提升可达5.6%，且在数据效率上表现良好，适应性强。

Conclusion: OS-R1不仅性能优越，且具有跨多种实际应用环境的适应性，展示了其实际部署的潜力。

Abstract: Linux kernel tuning is essential for optimizing operating system (OS)
performance. However, existing methods often face challenges in terms of
efficiency, scalability, and generalization. This paper introduces OS-R1, an
agentic Linux kernel tuning framework powered by rule-based reinforcement
learning (RL). By abstracting the kernel configuration space as an RL
environment, OS-R1 facilitates efficient exploration by large language models
(LLMs) and ensures accurate configuration modifications. Additionally, custom
reward functions are designed to enhance reasoning standardization,
configuration modification accuracy, and system performance awareness of the
LLMs. Furthermore, we propose a two-phase training process that accelerates
convergence and minimizes retraining across diverse tuning scenarios.
Experimental results show that OS-R1 significantly outperforms existing
baseline methods, achieving up to 5.6% performance improvement over heuristic
tuning and maintaining high data efficiency. Notably, OS-R1 is adaptable across
various real-world applications, demonstrating its potential for practical
deployment in diverse environments. Our dataset and code are publicly available
at https://github.com/LHY-24/OS-R1.

</details>


### [349] [Illuminating LLM Coding Agents: Visual Analytics for Deeper Understanding and Enhancement](https://arxiv.org/abs/2508.12555)
*Junpeng Wang,Yuzhong Chen,Menghai Pan,Chin-Chia Michael Yeh,Mahashweta Das*

Main category: cs.LG

TL;DR: 本研究提出了一种可视化分析系统，帮助ML科学家更好地理解和改进具有迭代问题解决能力的编码代理的行为。


<details>
  <summary>Details</summary>
Motivation: ML科学家目前难以高效追踪和比较编码代理的程序开发过程，需要一个更高效的方式来分析和优化代理行为。

Method: 设计了一个可视化分析系统，分为代码级、过程级和语言模型级三级对比分析，以揭示编码行为的具体特征与改进空间。

Result: 通过使用编码代理解决Kaggle竞赛案例研究，展示了系统如何有效提供迭代编码过程的关键洞察。

Conclusion: 该系统增强了ML科学家对代理行为的研究能力，有助于优化调试和提示工程的效率。

Abstract: Coding agents powered by large language models (LLMs) have gained traction
for automating code generation through iterative problem-solving with minimal
human involvement. Despite the emergence of various frameworks, e.g.,
LangChain, AutoML, and AIDE, ML scientists still struggle to effectively review
and adjust the agents' coding process. The current approach of manually
inspecting individual outputs is inefficient, making it difficult to track code
evolution, compare coding iterations, and identify improvement opportunities.
To address this challenge, we introduce a visual analytics system designed to
enhance the examination of coding agent behaviors. Focusing on the AIDE
framework, our system supports comparative analysis across three levels: (1)
Code-Level Analysis, which reveals how the agent debugs and refines its code
over iterations; (2) Process-Level Analysis, which contrasts different
solution-seeking processes explored by the agent; and (3) LLM-Level Analysis,
which highlights variations in coding behavior across different LLMs. By
integrating these perspectives, our system enables ML scientists to gain a
structured understanding of agent behaviors, facilitating more effective
debugging and prompt engineering. Through case studies using coding agents to
tackle popular Kaggle competitions, we demonstrate how our system provides
valuable insights into the iterative coding process.

</details>


### [350] [Deep Learning-Based Financial Time Series Forecasting via Sliding Window and Variational Mode Decomposition](https://arxiv.org/abs/2508.12565)
*Luke Li*

Main category: cs.LG

TL;DR: 提出结合滑动窗口和变分模态分解方法的预测模型，并使用解耦后的数据输入深度学习模型进行预测，比传统方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 通过处理复杂的、非平稳的金融时间序列数据，提升预测精度和模型稳定性。

Method: 运用VMD分解非稳态时间序列，并将分解后的数据输入LSTM深度学习模型进行预测。

Result: 实验表明，使用VMD处理数据训练的LSTM模型在预测性能和稳定性上优于直接使用原始时间序列的模型。

Conclusion: 结合滑动窗口和VMD的预测模型能够有效提升金融时间序列预测的效果，展示了其适用性和优越性。

Abstract: To address the complexity of financial time series, this paper proposes a
forecasting model combining sliding window and variational mode decomposition
(VMD) methods. Historical stock prices and relevant market indicators are used
to construct datasets. VMD decomposes non-stationary financial time series into
smoother subcomponents, improving model adaptability. The decomposed data is
then input into a deep learning model for prediction. The study compares the
forecasting effects of an LSTM model trained on VMD-processed sequences with
those using raw time series, demonstrating better performance and stability.

</details>


### [351] [Data-driven particle dynamics: Structure-preserving coarse-graining for emergent behavior in non-equilibrium systems](https://arxiv.org/abs/2508.12569)
*Quercus Hernandez,Max Win,Thomas C. O'Connor,Paulo E. Arratia,Nathaniel Trask*

Main category: cs.LG

TL;DR: 提出了一种利用metriplectic bracket形式的框架，用于从粒子轨迹的时间序列中机器学习粗粒化动力学，同时保证热力学定律和非平衡统计特性的保持。


<details>
  <summary>Details</summary>
Motivation: 多尺度系统的模拟因需同时考虑短时间和长时间尺度间的链接而变得十分复杂。同时，当将高维动力学系统简化为低维模型时，不可避免的信息熵损失会导致系统表现出耗散性、历史依赖性和随机性。本文旨在解决此问题。

Method: 作者提出一种基于metriplectic bracket形式的框架，通过自监督学习策略识别结构变量，保障热力学定律和非平衡统计特性的保持，并以PyTorch和LAMMPS实现了开源代码。

Result: 该方法在基准系统上验证了其有效性，并成功应用于两项挑战性任务：对星形聚合物进行粗粒化，保留非平衡统计特性；从胶体悬浮液的高速视频中学习能捕捉局部重排事件与随机动力学耦合的模型。

Conclusion: 该研究提供了一种创新框架，可用于复杂多粒子系统的粗粒化建模，且具有很高的可扩展性和计算效率。

Abstract: Multiscale systems are ubiquitous in science and technology, but are
notoriously challenging to simulate as short spatiotemporal scales must be
appropriately linked to emergent bulk physics. When expensive high-dimensional
dynamical systems are coarse-grained into low-dimensional models, the entropic
loss of information leads to emergent physics which are dissipative,
history-dependent, and stochastic. To machine learn coarse-grained dynamics
from time-series observations of particle trajectories, we propose a framework
using the metriplectic bracket formalism that preserves these properties by
construction; most notably, the framework guarantees discrete notions of the
first and second laws of thermodynamics, conservation of momentum, and a
discrete fluctuation-dissipation balance crucial for capturing non-equilibrium
statistics. We introduce the mathematical framework abstractly before
specializing to a particle discretization. As labels are generally unavailable
for entropic state variables, we introduce a novel self-supervised learning
strategy to identify emergent structural variables. We validate the method on
benchmark systems and demonstrate its utility on two challenging examples: (1)
coarse-graining star polymers at challenging levels of coarse-graining while
preserving non-equilibrium statistics, and (2) learning models from high-speed
video of colloidal suspensions that capture coupling between local
rearrangement events and emergent stochastic dynamics. We provide open-source
implementations in both PyTorch and LAMMPS, enabling large-scale inference and
extensibility to diverse particle-based systems.

</details>


### [352] [Deep Learning Model for Amyloidogenicity Prediction using a Pre-trained Protein LLM](https://arxiv.org/abs/2508.12575)
*Zohra Yagoub,Hafida Bouziane*

Main category: cs.LG

TL;DR: 该研究利用预训练的蛋白质大语言模型结合双向LSTM和GRU对肽和蛋白质的淀粉样生成区域进行预测，测试数据集准确率达到83%。


<details>
  <summary>Details</summary>
Motivation: 通过基于序列信息的特征，提升预测蛋白质淀粉样生成区域的准确性。

Method: 采用预训练的蛋白质大语言模型，并结合双向LSTM和GRU模型进行预测。

Result: 在10折交叉验证中达到84.5%的准确率，在测试数据集中达到83%的准确率。

Conclusion: 研究表明，预训练的大语言模型在提升蛋白质淀粉样生成区域预测准确性方面有很大的潜力。

Abstract: The prediction of amyloidogenicity in peptides and proteins remains a focal
point of ongoing bioinformatics. The crucial step in this field is to apply
advanced computational methodologies. Many recent approaches to predicting
amyloidogenicity within proteins are highly based on evolutionary motifs and
the individual properties of amino acids. It is becoming increasingly evident
that the sequence information-based features show high predictive performance.
Consequently, our study evaluated the contextual features of protein sequences
obtained from a pretrained protein large language model leveraging
bidirectional LSTM and GRU to predict amyloidogenic regions in peptide and
protein sequences. Our method achieved an accuracy of 84.5% on 10-fold
cross-validation and an accuracy of 83% in the test dataset. Our results
demonstrate competitive performance, highlighting the potential of LLMs in
enhancing the accuracy of amyloid prediction.

</details>


### [353] [Widening the Network Mitigates the Impact of Data Heterogeneity on FedAvg](https://arxiv.org/abs/2508.12576)
*Like Jian,Dong Liu*

Main category: cs.LG

TL;DR: 本文研究了联邦学习(Federated Learning, FL)中FedAvg算法的收敛性问题，提出随着神经网络宽度的增加，异质性数据的影响逐渐减少，在无限宽度时完全消失。


<details>
  <summary>Details</summary>
Motivation: FL的关键挑战是如何在客户数据非IID(独立同分布)的情况下训练一个对异质性分布泛化性良好的全局模型。

Method: 通过理论分析FedAvg结合梯度下降的收敛性，证明了在无限宽神经网络下FL中的全局与本地模型呈现线性特性，并能达到与集中式学习相同的泛化性能。

Result: 实验验证了理论成果，表明在多种网络结构、损失函数和优化方法的情况下，该结论均成立。

Conclusion: FedAvg在无限宽度神经网络下可以有效应对数据异质性，表现出与集中式学习一致的泛化能力。

Abstract: Federated learning (FL) enables decentralized clients to train a model
collaboratively without sharing local data. A key distinction between FL and
centralized learning is that clients' data are non-independent and identically
distributed, which poses significant challenges in training a global model that
generalizes well across heterogeneous local data distributions. In this paper,
we analyze the convergence of overparameterized FedAvg with gradient descent
(GD). We prove that the impact of data heterogeneity diminishes as the width of
neural networks increases, ultimately vanishing when the width approaches
infinity. In the infinite-width regime, we further prove that both the global
and local models in FedAvg behave as linear models, and that FedAvg achieves
the same generalization performance as centralized learning with the same
number of GD iterations. Extensive experiments validate our theoretical
findings across various network architectures, loss functions, and optimization
methods.

</details>


### [354] [Energy-Efficient Wireless LLM Inference via Uncertainty and Importance-Aware Speculative Decoding](https://arxiv.org/abs/2508.12590)
*Jihoon Park,Seungeun Oh,Seong-Lyun Kim*

Main category: cs.LG

TL;DR: 本文提出了一种面向资源受限环境的混合语言模型(HLM)优化方法，通过令牌级过滤机制实现高效推理，同时减少通信成本和能耗。


<details>
  <summary>Details</summary>
Motivation: 针对在边缘场景中进行设备端LLM推理的需求日益增长，以及现有HLM主要关注准确性和延迟而忽略通信与能量效率的问题。

Method: 设计了一种基于重要性和不确定性感知的令牌级过滤机制，仅上传具有信息量的令牌，结合认知不确定性与注意力重要性评估，降低LLM使用及通信成本。

Result: 实验表明，与传统HLM相比，提出方法节省40.7%的能耗，且能达到87.5%的BERT分数和每秒0.37令牌的处理量，相较于U-HLM基线则分别提升了精度、节能率及吞吐量。

Conclusion: 该方法在带宽受限的边缘环境中，实现了节能且准确的LLM部署。

Abstract: To address the growing demand for on-device LLM inference in
resource-constrained environments, hybrid language models (HLM) have emerged,
combining lightweight local models with powerful cloud-based LLMs. Recent
studies on HLM have primarily focused on improving accuracy and latency, while
often overlooking communication and energy efficiency. We propose a token-level
filtering mechanism for an energy-efficient importance- and uncertainty-aware
HLM inference that leverages both epistemic uncertainty and attention-based
importance. Our method opportunistically uploads only informative tokens,
reducing LLM usage and communication costs. Experiments with TinyLlama-1.1B and
LLaMA-2-7B demonstrate that our method achieves up to 87.5% BERT Score and
token throughput of 0.37 tokens/sec while saving the energy consumption by
40.7% compared to standard HLM. Furthermore, compared to our previous U-HLM
baseline, our method improves BERTScore from 85.8% to 87.0%, energy savings
from 31.6% to 43.6%, and throughput from 0.36 to 0.40. This approach enables an
energy-efficient and accurate deployment of LLMs in bandwidth-constrained edge
environments.

</details>


### [355] [Physics-informed deep operator network for traffic state estimation](https://arxiv.org/abs/2508.12593)
*Zhihao Li,Ting Wang,Guojian Zou,Ruofei Wang,Ye Li*

Main category: cs.LG

TL;DR: 本文提出一种基于PI-DeepONet的交通状态估计方法，将复杂的时空交通状态问题转化为算子学习问题，并取得了优于最新技术的表现。


<details>
  <summary>Details</summary>
Motivation: 解决传统物理信息神经网络（PINNs）的限制，通过算子学习方法更好地恢复交通流时空状态。

Method: 提出PI-DeepONet框架，将稀疏输入数据映射为受交通流守恒定律约束的完整时空交通状态场，同时直接将交通流守恒模型与基本图结合到算子学习过程中。

Result: 在NGSIM数据集上的实验证明该方法优于现有方法，并深入探讨了函数生成策略、分支网络复杂度及输入函数生成方法对模型性能的影响。

Conclusion: PI-DeepONet框架不仅保证物理一致性，还能更好地捕捉拥堵传播、空间相关性和时间演化，具有鲁棒性和高效性。

Abstract: Traffic state estimation (TSE) fundamentally involves solving
high-dimensional spatiotemporal partial differential equations (PDEs) governing
traffic flow dynamics from limited, noisy measurements. While Physics-Informed
Neural Networks (PINNs) enforce PDE constraints point-wise, this paper adopts a
physics-informed deep operator network (PI-DeepONet) framework that
reformulates TSE as an operator learning problem. Our approach trains a
parameterized neural operator that maps sparse input data to the full
spatiotemporal traffic state field, governed by the traffic flow conservation
law. Crucially, unlike PINNs that enforce PDE constraints point-wise,
PI-DeepONet integrates traffic flow conservation model and the fundamental
diagram directly into the operator learning process, ensuring physical
consistency while capturing congestion propagation, spatial correlations, and
temporal evolution. Experiments on the NGSIM dataset demonstrate superior
performance over state-of-the-art baselines. Further analysis reveals insights
into optimal function generation strategies and branch network complexity.
Additionally, the impact of input function generation methods and the number of
functions on model performance is explored, highlighting the robustness and
efficacy of proposed framework.

</details>


### [356] [FLARE: Fast Low-rank Attention Routing Engine](https://arxiv.org/abs/2508.12594)
*Vedant Puri,Aditya Joglekar,Kevin Ferguson,Yu-hsuan Chen,Yongjie Jessica Zhang,Levent Burak Kara*

Main category: cs.LG

TL;DR: 本文提出了一种名为FLARE的线性复杂度自注意力机制，通过固定长度的潜在序列路由注意力，以解决自注意力的二次复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 解决自注意力在大规模无结构网格上应用的限制和可扩展性问题。

Method: 通过引入一个由可学习查询令牌生成的固定长度潜在序列，并通过这个瓶颈序列路由注意力，使得FLARE可以以O(NM)的成本运行，取代传统的O(N^2)复杂度。

Result: FLARE不仅可以扩展到前所未有的问题规模，还在多个基准测试中比最先进的神经PDE替代更具准确性。此外，作者还发布了一个新的添加剂制造数据集以推动研究。

Conclusion: FLARE作为一种低秩形式的注意力机制，不仅有效地解决了复杂度问题，还能带来更好的性能，适合大规模应用。

Abstract: The quadratic complexity of self-attention limits its applicability and
scalability on large unstructured meshes. We introduce Fast Low-rank Attention
Routing Engine (FLARE), a linear complexity self-attention mechanism that
routes attention through fixed-length latent sequences. Each attention head
performs global communication among $N$ tokens by projecting the input sequence
onto a fixed length latent sequence of $M \ll N$ tokens using learnable query
tokens. By routing attention through a bottleneck sequence, FLARE learns a
low-rank form of attention that can be applied at $O(NM)$ cost. FLARE not only
scales to unprecedented problem sizes, but also delivers superior accuracy
compared to state-of-the-art neural PDE surrogates across diverse benchmarks.
We also release a new additive manufacturing dataset to spur further research.
Our code is available at https://github.com/vpuri3/FLARE.py.

</details>


### [357] [Constructing Invariant and Equivariant Operations by Symmetric Tensor Network](https://arxiv.org/abs/2508.12596)
*Meng Zhang,Chao Wang,Hao Zhang,Shaojun Dong,Lixin He*

Main category: cs.LG

TL;DR: 提出了一种系统化方法，用于构建对称性相关的网络架构，特别是用于几何深度学习中的不变性和等变性操作。


<details>
  <summary>Details</summary>
Motivation: 当前几何深度学习需要设计能体现对称性的神经网络，开发有效的不变性和等变性操作成为关键。

Method: 通过符号张量网络的图形化表示，系统构建各种符合对称性要求的不变和等变操作，适用于不同类型和秩的张量。这种方法对证明和结构设计进行了简化。

Result: 方法应用于设计几何图神经网络的等变交互消息，并用于学习材料的本构定律。

Conclusion: 展示了方法在处理几何深度学习相关问题中的实用性和一般性，可支持对称性神经网络的设计。

Abstract: Design of neural networks that incorporate symmetry is crucial for geometric
deep learning. Central to this effort is the development of invariant and
equivariant operations. This works presents a systematic method for
constructing valid invariant and equivariant operations. It can handle inputs
and outputs in the form of Cartesian tensors with different rank, as well as
spherical tensors with different types. In addition, our method features a
graphical representation utilizing the symmetric tensor network, which
simplifies both the proofs and constructions related to invariant and
equivariant functions. We also apply this approach to design the equivariant
interaction message for the geometry graph neural network, and equivariant
machine learning model to learn the constitutive law of materials.

</details>


### [358] [A Hybrid Surrogate for Electric Vehicle Parameter Estimation and Power Consumption via Physics-Informed Neural Operators](https://arxiv.org/abs/2508.12602)
*Hansol Lim,Jongseong Brad Choi,Jee Won Lee,Haeseong Jeoung,Minkyu Han*

Main category: cs.LG

TL;DR: 本文提出了一种用于电动车参数估计和能耗的混合代理模型，结合了基于傅里叶神经算子后端的光谱参数算子与可微物理模块，能够实现高精度和高可解释性的电能估计。


<details>
  <summary>Details</summary>
Motivation: 解决电动车参数估计和能耗估计的问题，开发一种既物理嵌入又具可解释性的智能框架，以提高实现精准预测并支持多种应用场景。

Method: 提出一种称为Spectral Parameter Operator的新型架构，结合傅里叶神经算子和可微物理模块，在模型中嵌入物理约束，基于车辆状态参数预测时间变化的相关动力参数。

Result: 该模型在多种电动车种（包括Tesla Model 3、Tesla Model S和Kia EV9）的真实测试数据上表现出色。其中对Tesla车辆的平均绝对误差为0.2kW，对Kia EV9约为0.8kW。

Conclusion: 此框架轻便、解析度高，适用于路径优化、环保导航、车载诊断及健康管理等广泛实际应用，并能在不同未知条件和采样率下良好泛化。

Abstract: We present a hybrid surrogate model for electric vehicle parameter estimation
and power consumption. We combine our novel architecture Spectral Parameter
Operator built on a Fourier Neural Operator backbone for global context and a
differentiable physics module in the forward pass. From speed and acceleration
alone, it outputs time-varying motor and regenerative braking efficiencies, as
well as aerodynamic drag, rolling resistance, effective mass, and auxiliary
power. These parameters drive a physics-embedded estimate of battery power,
eliminating any separate physics-residual loss. The modular design lets
representations converge to physically meaningful parameters that reflect the
current state and condition of the vehicle. We evaluate on real-world logs from
a Tesla Model 3, Tesla Model S, and the Kia EV9. The surrogate achieves a mean
absolute error of 0.2kW (about 1% of average traction power at highway speeds)
for Tesla vehicles and about 0.8kW on the Kia EV9. The framework is
interpretable, and it generalizes well to unseen conditions, and sampling
rates, making it practical for path optimization, eco-routing, on-board
diagnostics, and prognostics health management.

</details>


### [359] [SSPO: Self-traced Step-wise Preference Optimization for Process Supervision and Reasoning Compression](https://arxiv.org/abs/2508.12604)
*Yuyang Xu,Yi Cheng,Haochao Ying,Zhuoyun Du,Renjun Hu,Xing Shi,Wei Lin,Jian Wu*

Main category: cs.LG

TL;DR: 本文介绍了一种名为SSPO的RL监督框架，通过自我生成的逐步偏好信号优化LLM推理过程，减少过度思考，提升准确性和简洁性。


<details>
  <summary>Details</summary>
Motivation: 主流的后训练方法因使用辅助模型和冗长推理由导致计算开销高，急需一种更高效的优化机制。

Method: 提出了一种名为SSPO的RL框架，通过模型自我生成的逐步偏好信号进行优化，无需辅助模型或人工标注。

Result: 实验表明，SSPO产生的推理序列准确且简洁，有效缓解了过度思考行为，且适用于多领域和多语言情境。

Conclusion: SSPO框架无需额外资源即可优化推理过程，展示了极大的高效性和实用性。

Abstract: Test-time scaling has proven effective in further enhancing the performance
of pretrained Large Language Models (LLMs). However, mainstream post-training
methods (i.e., reinforcement learning (RL) with chain-of-thought (CoT)
reasoning) often incur substantial computational overhead due to auxiliary
models and overthinking. In this paper, we empirically reveal that the
incorrect answers partially stem from verbose reasoning processes lacking
correct self-fix, where errors accumulate across multiple reasoning steps. To
this end, we propose Self-traced Step-wise Preference Optimization (SSPO), a
pluggable RL process supervision framework that enables fine-grained
optimization of each reasoning step. Specifically, SSPO requires neither
auxiliary models nor stepwise manual annotations. Instead, it leverages
step-wise preference signals generated by the model itself to guide the
optimization process for reasoning compression. Experiments demonstrate that
the generated reasoning sequences from SSPO are both accurate and succinct,
effectively mitigating overthinking behaviors without compromising model
performance across diverse domains and languages.

</details>


### [360] [How can we trust opaque systems? Criteria for robust explanations in XAI](https://arxiv.org/abs/2508.12623)
*Florian J. Boge,Annika Schuster*

Main category: cs.LG

TL;DR: 本文分析了深度学习算法中可解释性的问题，提出了基于解释鲁棒性（ER）和解释方法鲁棒性（EMR）的新评价框架。


<details>
  <summary>Details</summary>
Motivation: 深度学习算法尽管预测准确，但其内在工作机制不透明，难以被理解并信任，迫切需要可解释性方法。

Method: 提出并形式化了两套标准：解释鲁棒性（ER）和解释方法鲁棒性（EMR）以衡量可解释性方法的可信度。

Result: 确定了可信解释的关键标准，提供了理论框架，并强调这些标准在不同算法解释中的应用潜能。

Conclusion: 单一方法的解释鲁棒性与可信性不足以建立信任，必须结合多方法的鲁棒性，未来研究应拓展这些理论框架的应用。

Abstract: Deep learning (DL) algorithms are becoming ubiquitous in everyday life and in
scientific research. However, the price we pay for their impressively accurate
predictions is significant: their inner workings are notoriously opaque - it is
unknown to laypeople and researchers alike what features of the data a DL
system focuses on and how it ultimately succeeds in predicting correct outputs.
A necessary criterion for trustworthy explanations is that they should reflect
the relevant processes the algorithms' predictions are based on. The field of
eXplainable Artificial Intelligence (XAI) presents promising methods to create
such explanations. But recent reviews about their performance offer reasons for
skepticism. As we will argue, a good criterion for trustworthiness is
explanatory robustness: different XAI methods produce the same explanations in
comparable contexts. However, in some instances, all methods may give the same,
but still wrong, explanation. We therefore argue that in addition to
explanatory robustness (ER), a prior requirement of explanation method
robustness (EMR) has to be fulfilled by every XAI method. Conversely, the
robustness of an individual method is in itself insufficient for
trustworthiness. In what follows, we develop and formalize criteria for ER as
well as EMR, providing a framework for explaining and establishing trust in DL
algorithms. We also highlight interesting application cases and outline
directions for future work.

</details>


### [361] [FlowMol3: Flow Matching for 3D De Novo Small-Molecule Generation](https://arxiv.org/abs/2508.12629)
*Ian Dunn,David R. Koes*

Main category: cs.LG

TL;DR: FlowMol3是一个用于生成具有期望特性分子的生成模型，它通过简单且高效的技术实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 推动化学发现的发展，通过开发可以生成具有指定性质的分子模型以提高相关应用效率。

Method: 提出FlowMol3，这是一个基于流匹配的多模态生成模型，实现了利用自我条件化、虚拟原子和训练时几何形变技术进行生成优化。

Result: FlowMol3生成药物分子几乎100%有效，且功能性基团的组成和几何更准确，同时学习参数数量显著减少。

Conclusion: 这证明了一些简单且可转移的策略能够改进扩散及流模型的稳定性和质量，有助于推动分子生成领域的发展。

Abstract: A generative model capable of sampling realistic molecules with desired
properties could accelerate chemical discovery across a wide range of
applications. Toward this goal, significant effort has focused on developing
models that jointly sample molecular topology and 3D structure. We present
FlowMol3, an open-source, multi-modal flow matching model that advances the
state of the art for all-atom, small-molecule generation. Its substantial
performance gains over previous FlowMol versions are achieved without changes
to the graph neural network architecture or the underlying flow matching
formulation. Instead, FlowMol3's improvements arise from three
architecture-agnostic techniques that incur negligible computational cost:
self-conditioning, fake atoms, and train-time geometry distortion. FlowMol3
achieves nearly 100% molecular validity for drug-like molecules with explicit
hydrogens, more accurately reproduces the functional group composition and
geometry of its training data, and does so with an order of magnitude fewer
learnable parameters than comparable methods. We hypothesize that these
techniques mitigate a general pathology affecting transport-based generative
models, enabling detection and correction of distribution drift during
inference. Our results highlight simple, transferable strategies for improving
the stability and quality of diffusion- and flow-based molecular generative
models.

</details>


### [362] [Score-informed Neural Operator for Enhancing Ordering-based Causal Discovery](https://arxiv.org/abs/2508.12650)
*Jiyeon Kang,Songseong Kim,Chanhui Lee,Doyeong Hwang,Joanie Hayoun Chung,Yunkyung Ko,Sumin Lee,Sungwoong Kim,Sungbin Lim*

Main category: cs.LG

TL;DR: 提出了一种名为SciNO的新方法，以在因果发现中的分数建模中稳定地逼近Hessian对角线，并提升了现有方法的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于得分匹配的因果发现方法需要利用Hessian对角线的准确估计，但现有方法计算复杂且不稳定。

Method: 设计了一种新型生成模型（SciNO），在光滑函数空间逼近Hessian对角线，同时保留结构信息，并结合自回归模型先验来进行因果推理。

Result: 在合成和实际数据集上，SciNO方法显示出显著的因果顺序改进，分别减少了42.7%和31.5%的排序偏差，同时具有内存效率和可扩展性。

Conclusion: SciNO在因果顺序定位和推理上表现优越，可扩展且稳定地提升了现有因果发现方法的性能，并能有效增强大语言模型的因果推理能力。

Abstract: Ordering-based approaches to causal discovery identify topological orders of
causal graphs, providing scalable alternatives to combinatorial search methods.
Under the Additive Noise Model (ANM) assumption, recent causal ordering methods
based on score matching require an accurate estimation of the Hessian diagonal
of the log-densities. However, previous approaches mainly use Stein gradient
estimators, which are computationally expensive and memory-intensive. Although
DiffAN addresses these limitations by substituting kernel-based estimates with
diffusion models, it remains numerically unstable due to the second-order
derivatives of score models. To alleviate these problems, we propose
Score-informed Neural Operator (SciNO), a probabilistic generative model in
smooth function spaces designed to stably approximate the Hessian diagonal and
to preserve structural information during the score modeling. Empirical results
show that SciNO reduces order divergence by 42.7% on synthetic graphs and by
31.5% on real-world datasets on average compared to DiffAN, while maintaining
memory efficiency and scalability. Furthermore, we propose a probabilistic
control algorithm for causal reasoning with autoregressive models that
integrates SciNO's probability estimates with autoregressive model priors,
enabling reliable data-driven causal ordering informed by semantic information.
Consequently, the proposed method enhances causal reasoning abilities of LLMs
without additional fine-tuning or prompt engineering.

</details>


### [363] [Robust Federated Learning under Adversarial Attacks via Loss-Based Client Clustering](https://arxiv.org/abs/2508.12672)
*Emmanouil Kritharakis,Dusan Jakovetic,Antonios Makris,Konstantinos Tserpes*

Main category: cs.LG

TL;DR: 本文提出了一种面向拜占庭攻击的联邦学习算法，理论和实验均表明相较于现有方法具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 探讨在存在拜占庭攻击的联邦学习场景中如何实现鲁棒性训练，以提升联邦学习的实际应用价值。

Method: 在假设服务器可靠且具有可信数据集的情况下，提出一种新方法，仅需两个诚实参与者（服务器和一个客户端）即可无先验知识应对拜占庭攻击。

Result: 理论证明了算法能够实现有限的最优性差距；实验结果表明，该算法在MNIST、FMNIST和CIFAR-10基准测试下，比现有的标准和鲁棒联邦学习方法显著优越。

Conclusion: 新算法在面对标签翻转、符号翻转及高斯噪声等多种攻击策略时表现出较强的鲁棒性，是在拜占庭攻击场景中提升联邦学习效果的有效方法。

Abstract: Federated Learning (FL) enables collaborative model training across multiple
clients without sharing private data. We consider FL scenarios wherein FL
clients are subject to adversarial (Byzantine) attacks, while the FL server is
trusted (honest) and has a trustworthy side dataset. This may correspond to,
e.g., cases where the server possesses trusted data prior to federation, or to
the presence of a trusted client that temporarily assumes the server role. Our
approach requires only two honest participants, i.e., the server and one
client, to function effectively, without prior knowledge of the number of
malicious clients. Theoretical analysis demonstrates bounded optimality gaps
even under strong Byzantine attacks. Experimental results show that our
algorithm significantly outperforms standard and robust FL baselines such as
Mean, Trimmed Mean, Median, Krum, and Multi-Krum under various attack
strategies including label flipping, sign flipping, and Gaussian noise addition
across MNIST, FMNIST, and CIFAR-10 benchmarks using the Flower framework.

</details>


### [364] [Deploying Models to Non-participating Clients in Federated Learning without Fine-tuning: A Hypernetwork-based Approach](https://arxiv.org/abs/2508.12673)
*Yuhao Zhou,Jindi Lv,Yuxin Tian,Dan Si,Qing Ye,Jiancheng Lv*

Main category: cs.LG

TL;DR: 提出了一种名为HyperFedZero的新方法，通过超网络和分布感知嵌入动态生成特化模型，解决了联邦学习中非参与客户端数据异质性的问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法尽管在处理参与客户端的数据异质性方面取得进展，但对非参与客户端在域内分布变化的适应性不足。

Method: 引入HyperFedZero方法，通过一个结合了分布感知嵌入的超网络动态生成针对非参与客户端的特化模型，并使用NoisyEmbed增强的提取器和平衡惩罚机制来防止特征坍塌。

Result: 实验表明HyperFedZero在多个数据集和模型上的性能均优于现有方法，并且对计算、存储和通信资源的占用较少。此外，消融研究和可视化验证了其组件的必要性和方法的有效性。

Conclusion: HyperFedZero能有效应对非参与客户端的分布变化，在性能、资源利用和泛化能力上表现出色，为解决联邦学习中的数据异质性问题提供了新的思路。

Abstract: Federated Learning (FL) has emerged as a promising paradigm for
privacy-preserving collaborative learning, yet data heterogeneity remains a
critical challenge. While existing methods achieve progress in addressing data
heterogeneity for participating clients, they fail to generalize to
non-participating clients with in-domain distribution shifts and resource
constraints. To mitigate this issue, we present HyperFedZero, a novel method
that dynamically generates specialized models via a hypernetwork conditioned on
distribution-aware embeddings. Our approach explicitly incorporates
distribution-aware inductive biases into the model's forward pass, extracting
robust distribution embeddings using a NoisyEmbed-enhanced extractor with a
Balancing Penalty, effectively preventing feature collapse. The hypernetwork
then leverages these embeddings to generate specialized models chunk-by-chunk
for non-participating clients, ensuring adaptability to their unique data
distributions. Extensive experiments on multiple datasets and models
demonstrate HyperFedZero's remarkable performance, surpassing competing methods
consistently with minimal computational, storage, and communication overhead.
Moreover, ablation studies and visualizations further validate the necessity of
each component, confirming meaningful adaptations and validating the
effectiveness of HyperFedZero.

</details>


### [365] [BUILDA: A Thermal Building Data Generation Framework for Transfer Learning](https://arxiv.org/abs/2508.12703)
*Thomas Krug,Fabian Raisch,Dominik Aimer,Markus Wirnsberger,Ferdinand Sigg,Benjamin Schäfer,Benjamin Tischler*

Main category: cs.LG

TL;DR: 这篇论文介绍了用于生成用于传递学习的建筑热动力学高质量和大容量合成数据的框架BuilDa。


<details>
  <summary>Details</summary>
Motivation: 现有的建筑热动态研究需要大量数据，而公共数据集和现有数据生成工具不满足研究需求，并且需要专业知识。

Method: 提出了BuilDa框架，利用单区域Modelica模型导出为FMU（功能模拟单元）并在Python中模拟生成数据，降低了对建筑模拟专业知识的需求。

Result: 生成功能性数据用于预训练和微调迁移学习模型，验证了BuilDa的可行性和实用性。

Conclusion: BuilDa为迁移学习研究提供了高质量且易于生成的合成数据，弥合了数据需求与生成工具之间的差距。

Abstract: Transfer learning (TL) can improve data-driven modeling of building thermal
dynamics. Therefore, many new TL research areas emerge in the field, such as
selecting the right source model for TL. However, these research directions
require massive amounts of thermal building data which is lacking presently.
Neither public datasets nor existing data generators meet the needs of TL
research in terms of data quality and quantity. Moreover, existing data
generation approaches typically require expert knowledge in building
simulation. We present BuilDa, a thermal building data generation framework for
producing synthetic data of adequate quality and quantity for TL research. The
framework does not require profound building simulation knowledge to generate
large volumes of data. BuilDa uses a single-zone Modelica model that is
exported as a Functional Mock-up Unit (FMU) and simulated in Python. We
demonstrate BuilDa by generating data and utilizing it for pretraining and
fine-tuning TL models.

</details>


### [366] [Argos: A Decentralized Federated System for Detection of Traffic Signs in CAVs](https://arxiv.org/abs/2508.12712)
*Seyed Mahdi Haji Seyed Hossein,Alireza Hosseini,Soheil Hajian Manesh,Amirali Shahriary*

Main category: cs.LG

TL;DR: 本研究提出一个去中心化的联邦学习框架，用于车载网络中的交通标志检测，以实现协作模型训练而无需共享原始数据。


<details>
  <summary>Details</summary>
Motivation: 应对自动驾驶车辆生成的大量传感器数据所带来的隐私和通信挑战，特别是在感知任务中的中央化机器学习方法。

Method: 通过车载设备分配交通标志类别以进行本地训练，结合轻量级目标检测器，在Flower框架中使用FedProx、FedAdam和FedAVG等算法聚合模型参数，并评估不同的训练配置，包括服务器轮数、本地训练轮数、客户端参与比例与数据分布等。

Result: 实验表明增加服务器轮数（从2到20）提高了准确率（从<0.1到>0.8）；适中的本地训练轮数（8-10）使效率达到最佳，准确率约为0.67；更高的客户端参与比例提高了模型泛化能力至0.83；FedProx在处理异质和非IID数据表现最佳；非IID数据分布导致性能下降；训练时长主要随轮次增加而增长。

Conclusion: 联邦学习为车载环境提供了一个可扩展的隐私保护解决方案，可为智能交通系统的未来部署提供指导，并促进更强大的聚合与通信优化。

Abstract: Connected and automated vehicles generate vast amounts of sensor data daily,
raising significant privacy and communication challenges for centralized
machine learning approaches in perception tasks. This study presents a
decentralized, federated learning framework tailored for traffic sign detection
in vehicular networks to enable collaborative model training without sharing
raw data. The framework partitioned traffic sign classes across vehicles for
specialized local training using lightweight object detectors, aggregated model
parameters via algorithms like FedProx, FedAdam and FedAVG in a simulated
environment with the Flower framework, and evaluated multiple configurations
including varying server rounds, local epochs, client participation fractions,
and data distributions. Experiments demonstrated that increasing server rounds
from 2 to 20 boosted accuracy from below 0.1 to over 0.8, moderate local epochs
(8-10) provided optimal efficiency with accuracies around 0.67, higher client
participation fractions enhanced generalization up to 0.83, FedProx
outperformed other aggregators in handling heterogeneity, non-IID data
distributions reduced performance compared to IID, and training duration
primarily scaled with the number of rounds rather than aggregation strategy. We
conclude that this federated approach may offer a scalable, privacy-preserving
solution for real-world vehicular deployments, potentially guiding future
integrations of robust aggregation and communication optimizations to advance
intelligent transportation systems.

</details>


### [367] [FedSODA: Federated Fine-tuning of LLMs via Similarity Group Pruning and Orchestrated Distillation Alignment](https://arxiv.org/abs/2508.12727)
*Manning Zhu,Songtao Guo,Pengzhan Zhou,Yansong Ning,Chang Han,Dewen Qiao*

Main category: cs.LG

TL;DR: 本文提出FedSODA，一种面向资源受限环境的高效联邦微调框架，解决了全面微调在存储与计算上的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 为了在保护数据隐私的同时实现领域特定需求的语言模型自适应，推动大语言模型联邦学习的发展。

Method: 通过提出相似性分组剪枝(SGP)模块与协调蒸馏对齐(ODA)模块，实现轻量化的子模型微调，并利用QLoRA量化技术大幅降低计算和存储需求。

Result: 大幅减少通信开销（降低70.6%）、存储使用量（降低75.6%），且在下游任务中表现出3.1%的精度提升。

Conclusion: FedSODA在资源受限的联邦环境中表现出高效性与准确性，是大语言模型实际微调的适用选择。

Abstract: Federated fine-tuning (FFT) of large language models (LLMs) has recently
emerged as a promising solution to enable domain-specific adaptation while
preserving data privacy. Despite its benefits, FFT on resource-constrained
clients relies on the high computational and memory demands of full-model
fine-tuning, which limits the potential advancement. This paper presents
FedSODA, a resource-efficient FFT framework that enables clients to adapt LLMs
without accessing or storing the full model. Specifically, we first propose a
similarity group pruning (SGP) module, which prunes redundant layers from the
full LLM while retaining the most critical layers to preserve the model
performance. Moreover, we introduce an orchestrated distillation alignment
(ODA) module to reduce gradient divergence between the sub-LLM and the full LLM
during FFT. Through the use of the QLoRA, clients only need to deploy quantized
sub-LLMs and fine-tune lightweight adapters, significantly reducing local
resource requirements. We conduct extensive experiments on three open-source
LLMs across a variety of downstream tasks. The experimental results demonstrate
that FedSODA reduces communication overhead by an average of 70.6%, decreases
storage usage by 75.6%, and improves task accuracy by 3.1%, making it highly
suitable for practical FFT applications under resource constraints.

</details>


### [368] [FedUNet: A Lightweight Additive U-Net Module for Federated Learning with Heterogeneous Models](https://arxiv.org/abs/2508.12740)
*Beomseok Seo,Kichang Lee,JaeYeon Park*

Main category: cs.LG

TL;DR: FedUNet是一种针对联邦学习提出的框架，借助U-Net模块实现异构环境下的高效模型训练。


<details>
  <summary>Details</summary>
Motivation: 解决现有联邦学习方法对统一架构需求的限制，适应异构真实环境需求。

Method: 通过向各客户端模型中添加U-Net模块，分享U-Net紧凑瓶颈实现跨架构的知识转移，利用U-Net的编码器-解码器设计和跳跃连接捕获不同层次特征。

Result: 实验结果表明，FedUNet在VGG变体上的准确率达93.11%，紧凑版为92.68%，通信开销仅为0.89 MB。

Conclusion: FedUNet架构无关且通信高效，在异构联邦学习场景中表现优越。

Abstract: Federated learning (FL) enables decentralized model training without sharing
local data. However, most existing methods assume identical model architectures
across clients, limiting their applicability in heterogeneous real-world
environments. To address this, we propose FedUNet, a lightweight and
architecture-agnostic FL framework that attaches a U-Net-inspired additive
module to each client's backbone. By sharing only the compact bottleneck of the
U-Net, FedUNet enables efficient knowledge transfer without structural
alignment. The encoder-decoder design and skip connections in the U-Net help
capture both low-level and high-level features, facilitating the extraction of
clientinvariant representations. This enables cooperative learning between the
backbone and the additive module with minimal communication cost. Experiment
with VGG variants shows that FedUNet achieves 93.11% accuracy and 92.68% in
compact form (i.e., a lightweight version of FedUNet) with only 0.89 MB low
communication overhead.

</details>


### [369] [A Multi-Resolution Benchmark Framework for Spatial Reasoning Assessment in Neural Networks](https://arxiv.org/abs/2508.12741)
*Manuela Imbriani,Gina Belmonte,Mieke Massink,Alessandro Tofani,Vincenzo Ciancia*

Main category: cs.LG

TL;DR: 本文提出了一个评估神经网络空间推理能力的综合基准框架，生成和测试两类合成数据集，显示了神经网络在几何和拓扑任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 当前对神经网络空间推理能力评估的系统方法不足，尤其在形态属性如连通性和距离关系方面，需要新的工具来研究这些问题。

Method: 利用VoxLogicA生成两类合成数据集（迷宫连通性和空间距离计算），通过自动化流程完成机器学习的完整流程，包括数据生成、训练、推理和评估。

Result: 实验表明神经网络在基础几何和拓扑任务中面临显著挑战，性能受到限制。

Conclusion: 该框架为研究神经网络的空间推理局限性提供了一种可重复的实验协议，并为结合符号推理和神经网络以改进空间理解的混合方法研究提供了基础。

Abstract: This paper presents preliminary results in the definition of a comprehensive
benchmark framework designed to systematically evaluate spatial reasoning
capabilities in neural networks, with a particular focus on morphological
properties such as connectivity and distance relationships. The framework is
currently being used to study the capabilities of nnU-Net, exploiting the
spatial model checker VoxLogicA to generate two distinct categories of
synthetic datasets: maze connectivity problems for topological analysis and
spatial distance computation tasks for geometric understanding. Each category
is evaluated across multiple resolutions to assess scalability and
generalization properties. The automated pipeline encompasses a complete
machine learning workflow including: synthetic dataset generation, standardized
training with cross-validation, inference execution, and comprehensive
evaluation using Dice coefficient and IoU (Intersection over Union) metrics.
Preliminary experimental results demonstrate significant challenges in neural
network spatial reasoning capabilities, revealing systematic failures in basic
geometric and topological understanding tasks. The framework provides a
reproducible experimental protocol, enabling researchers to identify specific
limitations. Such limitations could be addressed through hybrid approaches
combining neural networks with symbolic reasoning methods for improved spatial
understanding in clinical applications, establishing a foundation for ongoing
research into neural network spatial reasoning limitations and potential
solutions.

</details>


### [370] [Constrained Centroid Clustering: A Novel Approach for Compact and Structured Partitioning](https://arxiv.org/abs/2508.12758)
*Sowmini Devi Veeramachaneni,Ramamurthy Garimella*

Main category: cs.LG

TL;DR: 本文提出了一种扩展经典质心聚类的方法，通过限制聚类中心到最远点的最大距离来控制聚类扩散范围。


<details>
  <summary>Details</summary>
Motivation: 在某些需要控制聚类结构扩散的应用中，现有的聚类方法无法充分满足需求，包括传感器网络、协作机器人和可解释模式分析等场景。

Method: 通过拉格朗日公式推导出一种封闭解，加入对群集扩散范围的约束，并保持传统质心聚类的可解释性。

Result: 实验表明，提出的方法在确保聚类角度保持的情况下，显著减少了径向扩散，与K-means和GMM方法相比表现更佳。

Conclusion: 该方法适用于需要结构化且扩散受控的聚类任务，如传感器网络和协作机器人等应用。

Abstract: This paper presents Constrained Centroid Clustering (CCC), a method that
extends classical centroid-based clustering by enforcing a constraint on the
maximum distance between the cluster center and the farthest point in the
cluster. Using a Lagrangian formulation, we derive a closed-form solution that
maintains interpretability while controlling cluster spread. To evaluate CCC,
we conduct experiments on synthetic circular data with radial symmetry and
uniform angular distribution. Using ring-wise, sector-wise, and joint entropy
as evaluation metrics, we show that CCC achieves more compact clusters by
reducing radial spread while preserving angular structure, outperforming
standard methods such as K-means and GMM. The proposed approach is suitable for
applications requiring structured clustering with spread control, including
sensor networks, collaborative robotics, and interpretable pattern analysis.

</details>


### [371] [Short-Term Forecasting of Energy Production and Consumption Using Extreme Learning Machine: A Comprehensive MIMO based ELM Approach](https://arxiv.org/abs/2508.12764)
*Cyril Voyant,Milan Despotovic,Luis Garcia-Gutierrez,Mohammed Asloune,Yves-Marie Saint-Drenan,Jean-Laurent Duchaud,hjuvan Antone Faggianelli,Elena Magliaro*

Main category: cs.LG

TL;DR: 本文提出一种基于极限学习机（ELM）的短期能量预测方法，利用多输入多输出（MIMO）架构预测多个能量来源的输出和总生产量。


<details>
  <summary>Details</summary>
Motivation: 研究旨在应对能量生产非平稳性和季节变化性，以提高短期能量预测精度。

Method: 利用六年能量数据，通过ELM方法结合滑动窗口技术和循环时间编码实现动态自适应预测，同时对比MIMO与单输入单输出（SISO）架构。

Result: 该方法显著优于基于持久性的方法，在一小时预测中太阳能和热能的nRMSE分别为17.9%和5.1%，R^2>0.98，并保持了5小时内的高精度预测能力。

Conclusion: 模型在实时应用中表现优秀，具有适应多种环境的灵活性，尤其适合在线学习场景，并且相比LSTM等深度学习方法具有效率和计算优势。

Abstract: A novel methodology for short-term energy forecasting using an Extreme
Learning Machine ($\mathtt{ELM}$) is proposed. Using six years of hourly data
collected in Corsica (France) from multiple energy sources (solar, wind, hydro,
thermal, bioenergy, and imported electricity), our approach predicts both
individual energy outputs and total production (\cyr{including imports, which
closely follow energy demand, modulo losses)} through a Multi-Input
Multi-Output ($\mathtt{MIMO}$) architecture. To address non-stationarity and
seasonal variability, sliding window techniques and cyclic time encoding are
incorporated, enabling dynamic adaptation to fluctuations. The $\mathtt{ELM}$
model significantly outperforms persistence-based forecasting, particularly for
solar and thermal energy, achieving an $\mathtt{nRMSE}$ of $17.9\%$ and
$5.1\%$, respectively, with $\mathtt{R^2} > 0.98$ (1-hour horizon). The model
maintains high accuracy up to five hours ahead, beyond which renewable energy
sources become increasingly volatile. While $\mathtt{MIMO}$ provides marginal
gains over Single-Input Single-Output ($\mathtt{SISO}$) architectures and
offers key advantages over deep learning methods such as $\mathtt{LSTM}$, it
provides a closed-form solution with lower computational demands, making it
well-suited for real-time applications, including online learning. Beyond
predictive accuracy, the proposed methodology is adaptable to various contexts
and datasets, as it can be tuned to local constraints such as resource
availability, grid characteristics, and market structures.

</details>


### [372] [Online Ensemble Transformer for Accurate Cloud Workload Forecasting in Predictive Auto-Scaling](https://arxiv.org/abs/2508.12773)
*Jiadong Chen,Xiao He,Hengyu Ye,Fuxin Jiang,Tieying Zhang,Jianjun Chen,Xiaofeng Gao*

Main category: cs.LG

TL;DR: 提出了一种新型在线集成模型E3Former，用于大规模预测自动扩展中的在线负载预测，提升系统准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有预测模型在动态负载流中缺乏快速适应能力，难以应对高频、复杂周期性任务，导致自动扩展资源分配无法最佳化。

Method: 基于多子网络的预测能力构建在线集成模型E3Former，该模型在有限计算开销下提供高精度和鲁棒性。

Result: 在真实负载数据集上的实验表明，该方法比现存技术减少10%的预测误差，并成功应用于字节跳动的IHPA平台，显著优化资源利用率。

Conclusion: 新模型具备实际部署价值，与实际系统结合显著降低了资源使用成本，同时保障了服务质量，验证了其实用性和创新性。

Abstract: In the swiftly evolving domain of cloud computing, the advent of serverless
systems underscores the crucial need for predictive auto-scaling systems. This
necessity arises to ensure optimal resource allocation and maintain operational
efficiency in inherently volatile environments. At the core of a predictive
auto-scaling system is the workload forecasting model. Existing forecasting
models struggle to quickly adapt to the dynamics in online workload streams and
have difficulty capturing the complex periodicity brought by fine-grained,
high-frequency forecasting tasks. Addressing this, we propose a novel online
ensemble model, E3Former, for online workload forecasting in large-scale
predictive auto-scaling. Our model synergizes the predictive capabilities of
multiple subnetworks to surmount the limitations of single-model approaches,
thus ensuring superior accuracy and robustness. Remarkably, it accomplishes
this with a minimal increase in computational overhead, adhering to the lean
operational ethos of serverless systems. Through extensive experimentation on
real-world workload datasets, we establish the efficacy of our ensemble model.
In online forecasting tasks, the proposed method reduces forecast error by an
average of 10%, and its effectiveness is further demonstrated through a
predictive auto-scaling test in the real-life online system. Currently, our
method has been deployed within ByteDance's Intelligent Horizontal Pod
Auto-scaling (IHPA) platform, which supports the stable operation of over 30
applications, such as Douyin E-Comerce, TouTiao, and Volcano Engine. The
predictive auto-scaling capacity reaching over 600,000 CPU cores. On the basis
of essentially ensuring service quality, the predictive auto-scaling system can
reduce resource utilization by over 40%.

</details>


### [373] [Randomized PCA Forest for Outlier Detection](https://arxiv.org/abs/2508.12776)
*Muhammad Rajabinasab,Farhad Pakdaman,Moncef Gabbouj,Peter Schneider-Kamp,Arthur Zimek*

Main category: cs.LG

TL;DR: 提出一种基于随机主成分分析（RPCA）的新颖无监督异常检测方法，展示了其优越性能和高效性。


<details>
  <summary>Details</summary>
Motivation: 受到RPCA用于近似KNN搜索的良好表现的启发，将其用于异常检测任务以提升效果和效率。

Method: 开发一种利用RPCA森林（RPCA Forest）的无监督异常检测方法，并进行实验测试与分析。

Result: 实验结果表明，该方法在多个数据集上优于传统和最新方法，并在其余数据集中表现出竞争力。

Conclusion: 改进方法具有高泛化能力和计算效率，是无监督异常检测的良好选择。

Abstract: We propose a novel unsupervised outlier detection method based on Randomized
Principal Component Analysis (PCA). Inspired by the performance of Randomized
PCA (RPCA) Forest in approximate K-Nearest Neighbor (KNN) search, we develop a
novel unsupervised outlier detection method that utilizes RPCA Forest for
outlier detection. Experimental results showcase the superiority of the
proposed approach compared to the classical and state-of-the-art methods in
performing the outlier detection task on several datasets while performing
competitively on the rest. The extensive analysis of the proposed method
reflects it high generalization power and its computational efficiency,
highlighting it as a good choice for unsupervised outlier detection.

</details>


### [374] [Wavy Transformer](https://arxiv.org/abs/2508.12787)
*Satoshi Noguchi,Yoshinobu Kawahara*

Main category: cs.LG

TL;DR: 提出了一种名为Wavy Transformer的新型注意力机制，解决深度Transformer模型中常见的过平滑问题，并在NLP和CV任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 深度Transformer模型在NLP与CV中取得巨大成功，但存在过平滑问题，本研究试图从物理角度解释这一问题并提出解决方案。

Method: 通过将注意力层的动态与完全图上的图神经扩散建立等价关系，提出基于二阶波动动力学的Wavy Transformer，其中包括新的注意力层、前馈网络和归一化层，保留状态与速度的关系。

Result: 在多种NLP和CV任务中，Wavy Transformer的效能显著优于传统方法，其优点是参数增加很小且无需额外超参数调优。

Conclusion: 应用波动动力学设计的Wavy Transformer有效缓解了Transformer的过平滑问题，并以简单的改进提升模型性能，具有实际应用价值。

Abstract: Transformers have achieved remarkable success across natural language
processing (NLP) and computer vision (CV). However, deep transformer models
often suffer from an over-smoothing issue, in which token representations
converge to similar values as they pass through successive transformer blocks.
In this paper, we establish an equivalence between the hidden-state dynamics
induced by stacked attention layers and graph neural diffusion on a complete
graph. From this perspective, over-smoothing can be interpreted as a
consequence of the dissipative nature of the underlying diffusion dynamics.
Motivated by this physical interpretation, we propose Wavy Transformer, which
consists of a novel attention layer based on second-order wavy dynamics. We
also introduce a feed-forward network and a normalization layer designed to
preserve the physical state-velocity relationship under the chain rule, thereby
extending the transformer architecture. We further validate our proposed
techniques on various transformer models for NLP and CV tasks. The results
consistently demonstrate that Wavy Transformer improves performance with
minimal additional parameters and no extra hyperparameter tuning.

</details>


### [375] [Bridging Human and LLM Judgments: Understanding and Narrowing the Gap](https://arxiv.org/abs/2508.12792)
*Felipe Maia Polo,Xinhe Wang,Mikhail Yurochkin,Gongjun Xu,Moulinath Banerjee,Yuekai Sun*

Main category: cs.LG

TL;DR: 研究提出了一个名为Bridge的统计框架，用于统一人类与语言模型（LLM）的评价结果，提升LLM评估与人类一致性。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型作为评估工具时，其判断常常与人类有系统性偏差，因此需要一个框架来桥接人类和模型的评价体系。

Method: 提出了Bridge框架，该框架通过假定潜在的人类偏好评分，并利用线性变换来建模LLM与人类评价的偏差。此外还提供了高效的拟合算法以实现统计推断。

Result: 实验表明，在六个LLM和两个基准（BigGen Bench和Chatbot Arena）上，Bridge在一致性、校准和KL散度指标上都表现出更高与人类评估一致的能力，同时揭示了系统性偏差。

Conclusion: Bridge作为一个统一的工具，能够提高LLM评级与人类的一致性，并为衡量人与LLM的评价差异提供了方法论支持。

Abstract: Large language models are increasingly used as judges (LLM-as-a-judge) to
evaluate model outputs at scale, but their assessments often diverge
systematically from human judgments. We present Bridge, a unified statistical
framework that explicitly bridges human and LLM evaluations under both absolute
scoring and pairwise comparison paradigms. Bridge posits a latent human
preference score for each prompt-response pair and models LLM deviations as
linear transformations of covariates that capture sources of discrepancies.
This offers a simple and principled framework for refining LLM ratings and
characterizing systematic discrepancies between humans and LLMs. We provide an
efficient fitting algorithm with asymptotic guarantees for statistical
inference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot
Arena), Bridge achieves higher agreement with human ratings (accuracy,
calibration, and KL divergence) and exposes systematic human-LLM gaps.

</details>


### [376] [A Shift in Perspective on Causality in Domain Generalization](https://arxiv.org/abs/2508.12798)
*Damian Machlanski,Stephanie Riley,Edward Moroshko,Kurt Butler,Panagiotis Dimitrakopoulos,Thomas Melistas,Akchunya Chanchal,Steven McDonagh,Ricardo Silva,Sotirios A. Tsaftaris*

Main category: cs.LG

TL;DR: 这篇论文探讨了因果建模在人工智能泛化中的作用，并在挑战性基准上重新审视其表现，同时提出了更细致的理论框架。


<details>
  <summary>Details</summary>
Motivation: 研究因果建模如何在领域泛化问题中实现稳健的人工智能泛化能力，同时解决当前文献中的矛盾观点。

Method: 通过重新审视因果性和领域泛化文献中的理论假设，提出新的视角和理论框架，并配合一个交互式演示展示研究。

Result: 文章和演示共同验证了因果性在领域泛化中的实用性，同时提出了对现有理论的补充和改进。

Conclusion: 因果建模确实为人工智能泛化能力提供了潜在益处，但需要更细致的理论和实际验证来发挥其优势。

Abstract: The promise that causal modelling can lead to robust AI generalization has
been challenged in recent work on domain generalization (DG) benchmarks. We
revisit the claims of the causality and DG literature, reconciling apparent
contradictions and advocating for a more nuanced theory of the role of
causality in generalization. We also provide an interactive demo at
https://chai-uk.github.io/ukairs25-causal-predictors/.

</details>


### [377] [Maximum Score Routing For Mixture-of-Experts](https://arxiv.org/abs/2508.12801)
*Bowen Dong,Yilong Fan,Yutao Sun,Zhenyu Li,Tengyu Pan,Xun Zhou,Jianyong Wang*

Main category: cs.LG

TL;DR: 提出了一种新的路由范式MaxScore，用于改善专家分配问题，并在效率和性能上超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 为解决稀疏激活专家网络中专家分配的硬件效率低和不平衡问题。

Method: 通过将路由建模为最小成本最大流问题，并融合SoftTopk操作来实现改进。

Result: 相比传统的约束和非约束方法，MaxScore在相同FLOPs下实现了更低的训练损失和更高的评估分数。

Conclusion: MaxScore是一种改善稀疏激活专家网络路由效率的新方法，实现了更高效的专家分配与更优性能。

Abstract: Routing networks in sparsely activated mixture-of-experts (MoE) dynamically
allocate input tokens to top-k experts through differentiable sparse
transformations, enabling scalable model capacity while preserving
computational efficiency. Traditional MoE networks impose an expert capacity
constraint to ensure GPU-friendly computation. However, this leads to token
dropping when capacity is saturated and results in low hardware efficiency due
to padding in underutilized experts. Removing the capacity constraint, in turn,
compromises load balancing and computational efficiency. To address these
issues, we propose Maximum Score Routing ($\mathbf{MaxScore}$), a novel MoE
routing paradigm that models routing as a minimum-cost maximum-flow problem and
integrates a SoftTopk operator. MaxScore resolves the fundamental limitations
of iterative rerouting and optimal transport formulations, achieving lower
training losses and higher evaluation scores at equivalent FLOPs compared to
both constrained and unconstrained baselines. Implementation details and
experimental configurations can be obtained from
$\href{https://github.com/dongbw18/MaxScore.git}{MaxScore}$.

</details>


### [378] [Learning to Steer: Input-dependent Steering for Multimodal LLMs](https://arxiv.org/abs/2508.12815)
*Jayneel Parekh,Pegah Khayatan,Mustafa Shukor,Arnaud Dapogny,Alasdair Newson,Matthieu Cord*

Main category: cs.LG

TL;DR: 提出了一种利用输入特定线性偏移进行细粒度Steering的方法，并通过辅助模块预测所需的Steering向量。该方法有效减少了多模态大模型中的幻觉现象，并提升了安全性。


<details>
  <summary>Details</summary>
Motivation: 目前主流的Steering方法无法根据具体输入调整行为，例如在应对非法活动或医疗建议等输入时无法生成针对性的回答。文章旨在通过细粒度Steering方法解决这一限制。

Method: 提出一种输入特定线性偏移的Steering方式，其中Steering偏移由对比输入特定提示生成，并通过一个小型辅助模块在测试中预测该偏移向量。

Result: 通过实验验证，提出的L2S方法相比现有静态基线，能够更好地减少多模态大模型的幻觉现象，并提高模型行为的安全性。

Conclusion: L2S实现了输入特定的Steering方式，证明能有效改善多模态大模型的准确性和安全性，具有实际应用潜力。

Abstract: Steering has emerged as a practical approach to enable post-hoc guidance of
LLMs towards enforcing a specific behavior. However, it remains largely
underexplored for multimodal LLMs (MLLMs); furthermore, existing steering
techniques, such as mean steering, rely on a single steering vector, applied
independently of the input query. This paradigm faces limitations when the
desired behavior is dependent on the example at hand. For example, a safe
answer may consist in abstaining from answering when asked for an illegal
activity, or may point to external resources or consultation with an expert
when asked about medical advice. In this paper, we investigate a fine-grained
steering that uses an input-specific linear shift. This shift is computed using
contrastive input-specific prompting. However, the input-specific prompts
required for this approach are not known at test time. Therefore, we propose to
train a small auxiliary module to predict the input-specific steering vector.
Our approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces
hallucinations and enforces safety in MLLMs, outperforming other static
baselines.

</details>


### [379] [Toward Storage-Aware Learning with Compressed Data An Empirical Exploratory Study on JPEG](https://arxiv.org/abs/2508.12833)
*Kichang Lee,Songkuk Kim,JaeYeon Park,JeongGil Ko*

Main category: cs.LG

TL;DR: 研究一种在有限存储环境下进行存储感知学习的方法，提出不要盲目地压缩数据，而是通过样本自适应策略提高存储效率。


<details>
  <summary>Details</summary>
Motivation: 当前设备有限的存储能力在连续数据收集场景中面临挑战，需要找到一种在数据空间和质量之间取得平衡的方法。

Method: 通过实证研究，分析数据在质量和压缩程度之间的敏感性，验证样本自适应压缩策略的可行性。

Result: 发现盲目均匀地数据降采样或者同一标准的压缩策略效果不佳，提出了样本自适应压缩策略能够带来显著改观。

Conclusion: 系统性地刻画了一种未被充分研究的问题，展示了存储感知学习的潜力和实用性，为相关系统的开发奠定基础。

Abstract: On-device machine learning is often constrained by limited storage,
particularly in continuous data collection scenarios. This paper presents an
empirical study on storage-aware learning, focusing on the trade-off between
data quantity and quality via compression. We demonstrate that naive
strategies, such as uniform data dropping or one-size-fits-all compression, are
suboptimal. Our findings further reveal that data samples exhibit varying
sensitivities to compression, supporting the feasibility of a sample-wise
adaptive compression strategy. These insights provide a foundation for
developing a new class of storage-aware learning systems. The primary
contribution of this work is the systematic characterization of this
under-explored challenge, offering valuable insights that advance the
understanding of storage-aware learning.

</details>


### [380] [Learning In-context $\pmb{n}$-grams with Transformers: Sub-$\pmb{n}$-grams Are Near-stationary Points](https://arxiv.org/abs/2508.12837)
*Aditya Varre,Gizem Yüce,Nicolas Flammarion*

Main category: cs.LG

TL;DR: 本文研究了Transformer模型在in-context下一词预测任务中的阶段式学习过程，通过理论分析和实验验证揭示了$n$-gram学习的静止点特性及其在交叉熵损失中的表现。


<details>
  <summary>Details</summary>
Motivation: 受到实证观察中训练过程中持续的平台期和阶段性进展的启发，研究者想要通过分析损失函数形态，解释这些动态行为的本质机制。

Method: 研究了in-context $n$-gram语言模型在交叉熵损失下的学习情况，从理论上推导出模型参数静止点的充分条件，并针对一种简化的Transformer模型构造了对应$n$-gram估计的参数配置，进行数值模拟实验验证。

Result: 证明了参数配置的静止点理论，并发现子$n$-gram是损失函数近似静止点的关键性质，解释了阶段性学习和突现性相变的机制。

Conclusion: 论文揭示了Transformer模型学习$n$-gram过程中交叉熵损失的特性，为理解学习动态和相关现象提供了理论依据并通过数值实验支持。

Abstract: Motivated by empirical observations of prolonged plateaus and stage-wise
progression during training, we investigate the loss landscape of transformer
models trained on in-context next-token prediction tasks. In particular, we
focus on learning in-context $n$-gram language models under cross-entropy loss,
and establish a sufficient condition for parameter configurations to be
stationary points. We then construct a set of parameter configurations for a
simplified transformer model that represent $k$-gram estimators (for $k \leq
n$), and show that the gradient of the population loss at these solutions
vanishes in the limit of infinite sequence length and parameter norm. This
reveals a key property of the loss landscape: {sub-$n$-grams are
near-stationary points of the population cross-entropy loss}, offering
theoretical insight into widely observed phenomena such as stage-wise learning
dynamics and emergent phase transitions. These insights are further supported
by numerical experiments that illustrate the learning dynamics of $n$-grams,
characterized by discrete transitions between near-stationary solutions.

</details>


### [381] [HRS: Hybrid Representation Framework with Scheduling Awareness for Time Series Forecasting in Crowdsourced Cloud-Edge Platforms](https://arxiv.org/abs/2508.12839)
*Tiancheng Zhang,Cheng Zhang,Shuren Liu,Xiaofei Wang,Shaoyuan Huang,Wenyu Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为HRS的新框架，通过整合数值和图像式表示法捕获极端负载波动，并引入调度感知损失函数，来改善预测准确性与调度效率，在多数据集测试中实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着流媒体服务的激增，网络负载呈现高时变和突发特性，导致众包云边平台在维持服务质量（QoS）方面面临挑战，现有方法在极端流量预测中存在不足。

Method: 1. 引入HRS混合表示框架，结合数值与图像式表示，增强极端负载捕获能力。2. 设计调度感知损失（Scheduling-Aware Loss, SAL），量化预测误差的非对称影响，并优化调度决策支持。

Result: 在四个真实数据集上进行实验，HRS方案在降低SLA违规率（63.1%）和总利润损失（32.3%）方面显著优于十种对比基线，达到最先进水平。

Conclusion: HRS有效结合了新型表示与损失函数设计，在应对动态负载和极端情况时表现优越，为众包云边平台的资源调度提供了新的解决方案。

Abstract: With the rapid proliferation of streaming services, network load exhibits
highly time-varying and bursty behavior, posing serious challenges for
maintaining Quality of Service (QoS) in Crowdsourced Cloud-Edge Platforms
(CCPs). While CCPs leverage Predict-then-Schedule architecture to improve QoS
and profitability, accurate load forecasting remains challenging under traffic
surges. Existing methods either minimize mean absolute error, resulting in
underprovisioning and potential Service Level Agreement (SLA) violations during
peak periods, or adopt conservative overprovisioning strategies, which mitigate
SLA risks at the expense of increased resource expenditure. To address this
dilemma, we propose HRS, a hybrid representation framework with scheduling
awareness that integrates numerical and image-based representations to better
capture extreme load dynamics. We further introduce a Scheduling-Aware Loss
(SAL) that captures the asymmetric impact of prediction errors, guiding
predictions that better support scheduling decisions. Extensive experiments on
four real-world datasets demonstrate that HRS consistently outperforms ten
baselines and achieves state-of-the-art performance, reducing SLA violation
rates by 63.1% and total profit loss by 32.3%.

</details>


### [382] [One-Class Intrusion Detection with Dynamic Graphs](https://arxiv.org/abs/2508.12885)
*Aleksei Liuliakov,Alexander Schulz,Luca Hermes,Barbara Hammer*

Main category: cs.LG

TL;DR: 提出了一种新型入侵检测方法TGN-SVDD，结合动态图建模与深度异常检测。


<details>
  <summary>Details</summary>
Motivation: 由于全球数字化增长，网络安全的需求显得尤为重要。现有基于机器学习的入侵检测方法面临诸多挑战，如处理新型网络事件及捕获特定数据特性（时间事件与通信图结构）。

Method: 采用TGN-SVDD（基于动态图建模和深度异常检测）方法，来解决入侵检测问题。

Result: 实验表明，TGN-SVDD在实际入侵检测数据中优于多种基线方法，并提出了更具挑战性的变体。

Conclusion: TGN-SVDD是一种有效的入侵检测方法，有助于应对当前网络安全的挑战。

Abstract: With the growing digitalization all over the globe, the relevance of network
security becomes increasingly important. Machine learning-based intrusion
detection constitutes a promising approach for improving security, but it bears
several challenges. These include the requirement to detect novel and unseen
network events, as well as specific data properties, such as events over time
together with the inherent graph structure of network communication. In this
work, we propose a novel intrusion detection method, TGN-SVDD, which builds
upon modern dynamic graph modelling and deep anomaly detection. We demonstrate
its superiority over several baselines for realistic intrusion detection data
and suggest a more challenging variant of the latter.

</details>


### [383] [TCUQ: Single-Pass Uncertainty Quantification from Temporal Consistency with Streaming Conformal Calibration for TinyML](https://arxiv.org/abs/2508.12905)
*Ismail Lamaakal,Chaymae Yahyati,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: 本论文提出了一种名为TCUQ的无标签实时不确定性监测方法，专为TinyML流数据设计，具有资源效率高、实时能力强的特点。


<details>
  <summary>Details</summary>
Motivation: 针对TinyML设备资源受限的情况，开发一种无需在线标记即可实时监测不确定性的方法。

Method: 通过捕捉短时间范围内一致性，设计了一种轻量化的后验和特征信号处理方式，结合流式符合层实现校准风险评分，再转化为接受/拒绝规则。

Result: 与现有方法相比，TCUQ在微型控制器上减少了50%-60%的资源占用，速度提升了30%-45%，并在检测准确性下降与故障检测中表现优越，达到0.86 AUPRC和0.92 AUROC的性能。

Conclusion: TCUQ证明了短时间一致性与流式校准的结合是TinyML设备上监控一体化的高效实用解决方案。

Abstract: We introduce TCUQ, a single pass, label free uncertainty monitor for
streaming TinyML that converts short horizon temporal consistency captured via
lightweight signals on posteriors and features into a calibrated risk score
with an O(W ) ring buffer and O(1) per step updates. A streaming conformal
layer turns this score into a budgeted accept/abstain rule, yielding calibrated
behavior without online labels or extra forward passes. On microcontrollers,
TCUQ fits comfortably on kilobyte scale devices and reduces footprint and
latency versus early exit and deep ensembles (typically about 50 to 60% smaller
and about 30 to 45% faster), while methods of similar accuracy often run out of
memory. Under corrupted in distribution streams, TCUQ improves accuracy drop
detection by 3 to 7 AUPRC points and reaches up to 0.86 AUPRC at high
severities; for failure detection it attains up to 0.92 AUROC. These results
show that temporal consistency, coupled with streaming conformal calibration,
provides a practical and resource efficient foundation for on device monitoring
in TinyML.

</details>


### [384] [SparseMap: A Sparse Tensor Accelerator Framework Based on Evolution Strategy](https://arxiv.org/abs/2508.12906)
*Boran Zhao,Haiming Zhai,Zihang Yuan,Hetian Liu,Tian Xia,Wenzhe Zhao,Pengju Ren*

Main category: cs.LG

TL;DR: 提出了一种名为SparseMap的优化框架，用于设计稀疏张量加速器，解决了现有方法中设计不全面和优化效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 目前稀疏张量代数对机器学习和大数据的需求日益增长，但手动设计的加速器局限性较大，且适应场景变化耗时且复杂。因此需要自动化设计方法。然而，现有方法不能同时优化映射和稀疏策略，导致设计效果不佳。

Method: 提出一种基于进化策略的框架SparseMap，通过改进遗传编码和进化操作，构建更全面的设计空间，联合优化映射和稀疏策略，有效探索超大规模设计空间。

Result: 实验结果表明，相较于已有工作和传统优化方法，SparseMap能够始终找到更优的解决方案。

Conclusion: SparseMap在稀疏张量加速器设计中展现了显著的性能优势，证明了联合优化映射和稀疏策略的重要性和可行性。

Abstract: The growing demand for sparse tensor algebra (SpTA) in machine learning and
big data has driven the development of various sparse tensor accelerators.
However, most existing manually designed accelerators are limited to specific
scenarios, and it's time-consuming and challenging to adjust a large number of
design factors when scenarios change. Therefore, automating the design of SpTA
accelerators is crucial. Nevertheless, previous works focus solely on either
mapping (i.e., tiling communication and computation in space and time) or
sparse strategy (i.e., bypassing zero elements for efficiency), leading to
suboptimal designs due to the lack of comprehensive consideration of both. A
unified framework that jointly optimizes both is urgently needed. However,
integrating mapping and sparse strategies leads to a combinatorial explosion in
the design space(e.g., as large as $O(10^{41})$ for the workload $P_{32 \times
64} \times Q_{64 \times 48} = Z_{32 \times 48}$). This vast search space
renders most conventional optimization methods (e.g., particle swarm
optimization, reinforcement learning and Monte Carlo tree search) inefficient.
To address this challenge, we propose an evolution strategy-based sparse tensor
accelerator optimization framework, called SparseMap. SparseMap constructing a
more comprehensive design space with the consideration of both mapping and
sparse strategy. We introduce a series of enhancements to genetic encoding and
evolutionary operators, enabling SparseMap to efficiently explore the vast and
diverse design space. We quantitatively compare SparseMap with prior works and
classical optimization methods, demonstrating that SparseMap consistently finds
superior solutions.

</details>


### [385] [SNAP-UQ: Self-supervised Next-Activation Prediction for Single-Pass Uncertainty in TinyML](https://arxiv.org/abs/2508.12907)
*Ismail Lamaakal,Chaymae Yahyati,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: 提出了一种名为SNAP-UQ的单次处理、无需标签的不确定性评估方法专用于TinyML，具有高效性和资源友好性。


<details>
  <summary>Details</summary>
Motivation: 解决TinyML设备上内存和处理能力有限的情况下如何进行高效的不确定性评估问题。

Method: 通过深度逐层的下一步激活预测，采用轻量化的单调映射来生成可操作的不确定性评分，同时引入简化的结构以减少资源开销。

Result: 与早退出和深度集成方法相比，SNAP-UQ减少40-60%的闪存占用和25-35%的延迟，并在受损数据流中提高了AUPRC。另外，该方法在单次处理时可达到接近0.9的AUROC。

Conclusion: SNAP-UQ为TinyML提供了一种基于层与层动态、实用且资源高效的不确定性监测方法。

Abstract: We introduce \textbf{SNAP-UQ}, a single-pass, label-free uncertainty method
for TinyML that estimates risk from \emph{depth-wise next-activation
prediction}: tiny int8 heads forecast the statistics of the next layer from a
compressed view of the previous one, and a lightweight monotone mapper turns
the resulting surprisal into an actionable score. The design requires no
temporal buffers, auxiliary exits, or repeated forward passes, and adds only a
few tens of kilobytes to MCU deployments. Across vision and audio backbones,
SNAP-UQ consistently reduces flash and latency relative to early-exit and deep
ensembles (typically $\sim$40--60\% smaller and $\sim$25--35\% faster), with
competing methods of similar accuracy often exceeding memory limits. In
corrupted streams it improves accuracy-drop detection by several AUPRC points
and maintains strong failure detection (AUROC $\approx$0.9) in a single pass.
Grounding uncertainty in layer-to-layer dynamics yields a practical,
resource-efficient basis for on-device monitoring in TinyML.

</details>


### [386] [Fed-DPRoC:Communication-Efficient Differentially Private and Robust Federated Learning](https://arxiv.org/abs/2508.12978)
*Yue Xia,Tayyebeh Jahani-Nezhad,Rawad Bitar*

Main category: cs.LG

TL;DR: Fed-DPRoC 是一个支持差分隐私、拜占庭鲁棒性和通信效率的新型联合学习框架。


<details>
  <summary>Details</summary>
Motivation: 解决联合学习中的差分隐私保护、鲁棒性和高通信开销的问题。

Method: 提出鲁棒兼容压缩的概念，将差分隐私保护的更新结合罗布斯特平均和 Johnson-Lindenstrauss 变换进行压缩。

Result: 理论分析和实验表明，RobAJoL 方法在面对拜占庭攻击时，具有更好的鲁棒性与效用，同时通信成本更低。

Conclusion: RobAJoL 为在保护隐私的同时提高鲁棒性与通信效率提供了一种有效的解决方案。

Abstract: We propose Fed-DPRoC, a novel federated learning framework that
simultaneously ensures differential privacy (DP), Byzantine robustness, and
communication efficiency. We introduce the concept of robust-compatible
compression, which enables users to compress DP-protected updates while
maintaining the robustness of the aggregation rule. We instantiate our
framework as RobAJoL, combining the Johnson-Lindenstrauss (JL) transform for
compression with robust averaging for robust aggregation. We theoretically
prove the compatibility of JL transform with robust averaging and show that
RobAJoL preserves robustness guarantees, ensures DP, and reduces communication
cost. Experiments on CIFAR-10 and Fashion MNIST validate our theoretical claims
and demonstrate that RobAJoL outperforms existing methods in terms of
robustness and utility under different Byzantine attacks.

</details>


### [387] [SL-ACC: A Communication-Efficient Split Learning Framework with Adaptive Channel-wise Compression](https://arxiv.org/abs/2508.12984)
*Zehang Lin,Zheng Lin,Miao Yang,Jianhao Huang,Yuxin Zhang,Zihan Fang,Xia Du,Zhe Chen,Shunzhi Zhu,Wei Ni*

Main category: cs.LG

TL;DR: 提出了一种高效通信的分割学习框架（SL-ACC），通过减少传输过程中“压碎数据”的体积，提高模型训练效率。


<details>
  <summary>Details</summary>
Motivation: 分割学习在资源受限设备上执行分布式机器学习时，通过将计算负载转移到服务器解决了计算约束问题，但数据传输量大的问题依然存在。

Method: SL-ACC框架采用了自适应通道重要性识别（ACII）和通道分组压缩（CGC）两大组件。ACII通过香农熵识别通道贡献度，而CGC基于熵对通道进行分组，并进行自适应压缩以减少传输数据量。

Result: 实验表明，相较于当前方法，SL-ACC能够在更短时间内达到目标精度。

Conclusion: SL-ACC框架在大幅减少传输数据量的同时保持了模型训练精度，提升了分割学习的通信效率。

Abstract: The increasing complexity of neural networks poses a significant barrier to
the deployment of distributed machine learning (ML) on resource-constrained
devices, such as federated learning (FL). Split learning (SL) offers a
promising solution by offloading the primary computing load from edge devices
to a server via model partitioning. However, as the number of participating
devices increases, the transmission of excessive smashed data (i.e.,
activations and gradients) becomes a major bottleneck for SL, slowing down the
model training. To tackle this challenge, we propose a communication-efficient
SL framework, named SL-ACC, which comprises two key components: adaptive
channel importance identification (ACII) and channel grouping compression
(CGC). ACII first identifies the contribution of each channel in the smashed
data to model training using Shannon entropy. Following this, CGC groups the
channels based on their entropy and performs group-wise adaptive compression to
shrink the transmission volume without compromising training accuracy.
Extensive experiments across various datasets validate that our proposed SL-ACC
framework takes considerably less time to achieve a target accuracy than
state-of-the-art benchmarks.

</details>


### [388] [Predicting the Performance of Graph Convolutional Networks with Spectral Properties of the Graph Laplacian](https://arxiv.org/abs/2508.12993)
*Shalima Binta Manir,Tim Oates*

Main category: cs.LG

TL;DR: 本文探讨了图的代数连通性（Fiedler值）与图卷积网络（GCN）性能之间的关系，并证明Fiedler值是预测GCN性能的有效指标。


<details>
  <summary>Details</summary>
Motivation: 堆叠GCN层可能导致不同的表现，研究代数连通性可能解释GCN性能的变化，且为超参数设置和迁移学习提供指导。

Method: 通过理论分析及基于合成图和真实图数据（如Cora, CiteSeer, Polblogs）的实验来验证代数连通性与GCN性能的关系，并引入了多种方式来汇总图中Fiedler值。

Result: 实验证实了Fiedler值对于预测GCN性能的有效性，并表明代数连通性相似的图可以共享滤波器及超参数配置。

Conclusion: Fiedler值可以用作GCN性能的预测指标，并可能提高结构相似图间的迁移学习效果。

Abstract: A common observation in the Graph Convolutional Network (GCN) literature is
that stacking GCN layers may or may not result in better performance on tasks
like node classification and edge prediction. We have found empirically that a
graph's algebraic connectivity, which is known as the Fiedler value, is a good
predictor of GCN performance. Intuitively, graphs with similar Fiedler values
have analogous structural properties, suggesting that the same filters and
hyperparameters may yield similar results when used with GCNs, and that
transfer learning may be more effective between graphs with similar algebraic
connectivity. We explore this theoretically and empirically with experiments on
synthetic and real graph data, including the Cora, CiteSeer and Polblogs
datasets. We explore multiple ways of aggregating the Fiedler value for
connected components in the graphs to arrive at a value for the entire graph,
and show that it can be used to predict GCN performance. We also present
theoretical arguments as to why the Fiedler value is a good predictor.

</details>


### [389] [Kourkoutas-Beta: A Sunspike-Driven Adam Optimizer with Desert Flair](https://arxiv.org/abs/2508.12996)
*Stavros C. Kassinos*

Main category: cs.LG

TL;DR: 本文提出一种改进的Adam优化器“Kourkoutas-Beta”，通过动态调整beta2，以提高在物理问题中基于神经网络的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 在物理问题使用神经网络时，因边界/初始条件变化，可能在训练中导致梯度波动大，尤其是在PINNs等问题中，复合损失项使得这一问题更加严重。

Method: 引入了一种名为Kourkoutas-Beta的优化器，动态调整 beta2 值，根据梯度尖峰比（当前分层梯度范数与过去梯度范数的EMA之比）来变化。此外还包含一些补充特性，如AMSGrad泄露修正、信任区域裁剪等。

Result: 在四种测试任务中，Kourkoutas-Beta优化器表现出更高的稳定性和最终损失改善，其中包括PDE替代模型、PINNs任务和Transformer语言任务，在某些情况下显著降低了模型的bits-per-character。

Conclusion: 该优化器在不牺牲Adam原有优点的前提下，提升了在尖峰梯度环境下的鲁棒性，对物理问题和语言任务均有较大改进潜力。

Abstract: Transformer neural networks are increasingly used for physics-based problems.
In data-driven PDE surrogates, training samples from varying boundary and
initial conditions can cause erratic losses and spiky gradients; in
physics-informed neural networks (PINNs), stiff composite losses amplify this
effect.
  We introduce Kourkoutas-Beta, an Adam-style optimizer where the fixed
second-moment discount beta2 is replaced by a layer-wise dynamic value driven
by a bounded ``sunspike'' ratio: the current pooled gradient norm divided by an
exponential moving average (EMA) of past norms, squashed to the interval [0,1).
Spikes lower beta2 toward beta2_min; calm phases keep it near beta2_max.
Options include leaky-AMSGrad (decay), trust-region clipping (max_ratio),
adaptive tiny terms, and several bias-correction modes ``none'', ``beta2max'',
``exact'). With all features off and bias_correction=``none'', the method is
exactly Adam.
  We test on four settings: (i) a Transformer PDE surrogate (Heat2D), (ii) a 3D
PINN for heat conduction (Heat3D), (iii) a lightweight MLX synthetic task with
jitter and rare-trigger bursts, and (iv) a character-level Transformer on 30 MB
of enwik8 (small-enwik8). Kourkoutas-Beta improves stability and final loss
versus fixed-beta2 Adam. On small-enwik8 it lowers bits-per-character by about
38% vs Adam-0.95 and about 58% vs Adam-0.999 over 10 seeds, with smaller
variance. The method remains drop-in, with runtime overhead comparable to Adam
in testbeds A-C and within single-digit percent in testbed D. It preserves
Adam-style convergence guarantees while improving robustness under spiky
gradients.

</details>


### [390] [Fairness-Aware Multi-view Evidential Learning with Adaptive Prior](https://arxiv.org/abs/2508.12997)
*Haishun Chen,Cai Xu,Jinlong Yu,Yilin Zhang,Ziyu Guan,Wei Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种新的偏见证据多视学习(BEML)问题，并引入了公平感知多视证据学习(FAML)方法来解决该问题，通过校准训练轨迹和引入公平约束提高了预测性能和不确定性估计的可靠性。


<details>
  <summary>Details</summary>
Motivation: 通过分析发现当前多视证据学习在学习过程中存在偏见，尤其是对数据量较多的类别更容易产生偏倚，这导致了不可靠的不确定性估计。

Method: 提出了FAML方法，通过引入基于训练轨迹的自适应先验校准偏差，同时加入基于类别证据方差的公平约束；在多视融合阶段引入观点对齐机制以缓解视图间的偏倚问题。

Result: 在五个真实世界的多视数据集上进行了广泛实验，FAML在证据分配平衡性、预测性能和不确定性估计可靠性方面均优于现有方法。

Conclusion: FAML方法在解决偏见证据学习问题上表现卓越，可实现更为平衡的证据分配，并提高预测的精确性和可靠性，在多视任务中具有重要实用价值。

Abstract: Multi-view evidential learning aims to integrate information from multiple
views to improve prediction performance and provide trustworthy uncertainty
esitimation. Most previous methods assume that view-specific evidence learning
is naturally reliable. However, in practice, the evidence learning process
tends to be biased. Through empirical analysis on real-world data, we reveal
that samples tend to be assigned more evidence to support data-rich classes,
thereby leading to unreliable uncertainty estimation in predictions. This
motivates us to delve into a new Biased Evidential Multi-view Learning (BEML)
problem. To this end, we propose Fairness-Aware Multi-view Evidential Learning
(FAML). FAML first introduces an adaptive prior based on training trajectory,
which acts as a regularization strategy to flexibly calibrate the biased
evidence learning process. Furthermore, we explicitly incorporate a fairness
constraint based on class-wise evidence variance to promote balanced evidence
allocation. In the multi-view fusion stage, we propose an opinion alignment
mechanism to mitigate view-specific bias across views, thereby encouraging the
integration of consistent and mutually supportive evidence. Extensive
experiments on five real-world multi-view datasets demonstrate that FAML
achieves more balanced evidence allocation and improves both prediction
performance and the reliability of uncertainty estimation compared to
state-of-the-art methods.

</details>


### [391] [Monte Carlo Functional Regularisation for Continual Learning](https://arxiv.org/abs/2508.13006)
*Pengcheng Hao,Menghao Waiyan William Zhu,Ercan Engin Kuruoglu*

Main category: cs.LG

TL;DR: 提出了一种基于蒙特卡罗采样的功能正则化框架MCFRCL，用于连续学习，提升了预测精度和训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决功能正则化CL方法中存储和计算成本高的问题，同时减少线性近似误差。

Method: 通过蒙特卡罗采样对模型预测分布进行近似，用基于矩的方法捕捉采样的统计特性，利用Wasserstein距离和KL距离构造正则化函数。

Result: 在MNIST和CIFAR数据集上的实验表明，该方法在预测精度和训练效率上表现良好。

Conclusion: MCFRCL提供了一种高效的新框架，能够在连续学习任务中缓解传统方法的不足。

Abstract: Continual learning (CL) is crucial for the adaptation of neural network
models to new environments. Although outperforming weight-space regularisation
approaches, the functional regularisation-based CL methods suffer from high
computational costs and large linear approximation errors. In this work, we
present a new functional regularisation CL framework, called MCFRCL, which
approximates model prediction distributions by Monte Carlo (MC) sampling.
Moreover, three continuous distributions are leveraged to capture the
statistical characteristics of the MC samples via moment-based methods.
Additionally, both the Wasserstein distance and the Kullback-Leibler (KL)
distance are employed to construct the regularisation function. The proposed
MCFRCL is evaluated against multiple benchmark methods on the MNIST and CIFAR
datasets, with simulation results highlighting its effectiveness in both
prediction accuracy and training efficiency.

</details>


### [392] [Design and Analysis of Robust Adaptive Filtering with the Hyperbolic Tangent Exponential Kernel M-Estimator Function for Active Noise Control](https://arxiv.org/abs/2508.13018)
*Iam Kim de S. Hermont,Andre R. Flores,Rodrigo C. de Lamare*

Main category: cs.LG

TL;DR: 本文提出了一种针对存在脉冲噪声的主动降噪应用的稳健自适应过滤方法——FXHEKM算法，并通过数值结果验证了其在处理附加伪信号中的高效性。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够在存在脉冲噪声的情况下有效降噪的自适应算法，应对现有算法在处理这类噪声时的不足。

Method: 提出了一种名为过滤-x双曲正切指数广义核M估计函数（FXHEKM）的稳健自适应算法，并对其进行了统计分析以及计算成本研究。

Result: 数值结果显示，与现有算法相比，FXHEKM算法在减小附加伪信号（如α稳定噪声）上具有更高的效率，对比指标包括均方误差（MSE）和平均噪声减少（ANR）性能度量。

Conclusion: FXHEKM算法在脉冲噪声背景下的主动噪声控制方面表现出色，为解决此类问题提供了一种高效的方法。

Abstract: In this work, we propose a robust adaptive filtering approach for active
noise control applications in the presence of impulsive noise. In particular,
we develop the filtered-x hyperbolic tangent exponential generalized Kernel
M-estimate function (FXHEKM) robust adaptive algorithm. A statistical analysis
of the proposed FXHEKM algorithm is carried out along with a study of its
computational cost. {In order to evaluate the proposed FXHEKM algorithm, the
mean-square error (MSE) and the average noise reduction (ANR) performance
metrics have been adopted.} Numerical results show the efficiency of the
proposed FXHEKM algorithm to cancel the presence of the additive spurious
signals, such as \textbf{$\alpha$}-stable noises against competing algorithms.

</details>


### [393] [The Application of Transformer-Based Models for Predicting Consequences of Cyber Attacks](https://arxiv.org/abs/2508.13030)
*Bipin Chhetri,Akbar Siami Namin*

Main category: cs.LG

TL;DR: 本文研究利用自然语言处理（NLP）和深度学习技术，通过对MITRE CWE数据库中的文本描述进行分析，预测网络攻击的可能影响。研究评估了BERT和HAN在多标签分类中的性能，结果显示BERT的整体准确率达到0.972，表现优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 网络攻击日益增多，攻防成本高昂。本研究旨在通过威胁建模提高应对这些攻击的能力，利用NLP技术自动分析攻击描述并预测其后果，助力网络安全风险评估和资源分配优化。

Method: 研究使用BERT模型结合分层注意力网络（HAN），对MITRE CWE数据库的攻击描述进行文本分类。模型性能与传统的CNN和LSTM模型进行比较，重点分类为五大类别：可用性、访问控制、保密性、完整性及其他。

Result: 实验结果表明，BERT模型在多标签分类中整体准确率达到0.972，显著优于传统模型（如CNN和LSTM）。HAN模型在特定标签上优于基线模型，但BERT在精度和召回率上始终更胜一筹。

Conclusion: BERT模型在预测网络攻击后果方面表现最佳，优于基于HAN和传统深度学习的模型，体现了其在分析复杂攻击描述中的优势。

Abstract: Cyberattacks are increasing, and securing against such threats is costing
industries billions of dollars annually. Threat Modeling, that is,
comprehending the consequences of these attacks, can provide critical support
to cybersecurity professionals, enabling them to take timely action and
allocate resources that could be used elsewhere. Cybersecurity is heavily
dependent on threat modeling, as it assists security experts in assessing and
mitigating risks related to identifying vulnerabilities and threats. Recently,
there has been a pressing need for automated methods to assess attack
descriptions and forecast the future consequences of the increasing complexity
of cyberattacks. This study examines how Natural Language Processing (NLP) and
deep learning can be applied to analyze the potential impact of cyberattacks by
leveraging textual descriptions from the MITRE Common Weakness Enumeration
(CWE) database. We emphasize classifying attack consequences into five
principal categories: Availability, Access Control, Confidentiality, Integrity,
and Other. This paper investigates the use of Bidirectional Encoder
Representations from Transformers (BERT) in combination with Hierarchical
Attention Networks (HANs) for Multi-label classification, evaluating their
performance in comparison with conventional CNN and LSTM-based models.
Experimental findings show that BERT achieves an overall accuracy of $0.972$,
far higher than conventional deep learning models in multi-label
classification. HAN outperforms baseline forms of CNN and LSTM-based models on
specific cybersecurity labels. However, BERT consistently achieves better
precision and recall, making it more suitable for predicting the consequences
of a cyberattack.

</details>


### [394] [Beyond Internal Data: Bounding and Estimating Fairness from Incomplete Data](https://arxiv.org/abs/2508.13040)
*Varsha Ramineni,Hossein A. Rahmani,Emine Yilmaz,David Barber*

Main category: cs.LG

TL;DR: 作者提出一种方法，通过利用分散的内部和外部数据来估计人工智能模型的公平性，当完整数据不可获取时提出可能的公平性度量指标。


<details>
  <summary>Details</summary>
Motivation: 在AI系统中确保公平性至关重要，尤其是在高风险领域，然而受法律和隐私限制，公平性测试所需的完整数据常常无法获取。

Method: 提出结合内部机构数据和外部公共数据，通过估计可能的联合分布从而计算可能的公平性指标。

Result: 通过模拟和实际实验表明，该方法可以在有限数据条件下获得有意义的指标界限和真实指标的可靠估计。

Conclusion: 该方法在无法获取完整数据的现实情况下提供了一种切实有效的公平性测试解决方案。

Abstract: Ensuring fairness in AI systems is critical, especially in high-stakes
domains such as lending, hiring, and healthcare. This urgency is reflected in
emerging global regulations that mandate fairness assessments and independent
bias audits. However, procuring the necessary complete data for fairness
testing remains a significant challenge. In industry settings, legal and
privacy concerns restrict the collection of demographic data required to assess
group disparities, and auditors face practical and cultural challenges in
gaining access to data. In practice, data relevant for fairness testing is
often split across separate sources: internal datasets held by institutions
with predictive attributes, and external public datasets such as census data
containing protected attributes, each providing only partial, marginal
information. Our work seeks to leverage such available separate data to
estimate model fairness when complete data is inaccessible. We propose
utilising the available separate data to estimate a set of feasible joint
distributions and then compute the set plausible fairness metrics. Through
simulation and real experiments, we demonstrate that we can derive meaningful
bounds on fairness metrics and obtain reliable estimates of the true metric.
Our results demonstrate that this approach can serve as a practical and
effective solution for fairness testing in real-world settings where access to
complete data is restricted.

</details>


### [395] [Hierarchical Evaluation Function (HEF): A Multi-Metric Approach for Optimizing Demand Forecasting Models](https://arxiv.org/abs/2508.13057)
*Adolfo González,Víctor Parada*

Main category: cs.LG

TL;DR: 本文分析了两种自定义评价函数FMAE和HEF在多变量时间序列需求预测中的表现及适用性，研究发现HEF在全局指标中表现优异，适合战略规划，而FMAE在局部指标上更有优势，适用于短期需求预测。


<details>
  <summary>Details</summary>
Motivation: 由于多变量时间序列建模涉及复杂数据、不确定性和频繁的状态变化，因此需要改进评价指标以消除偏差并提高模型的适用性。

Method: 通过实验对比两种优化目标函数（FMAE和HEF），在不同数据分割比例和优化算法（如网格搜索、PSO、Optuna）下评估模型的拟合度、准确性、鲁棒性及计算效率，并通过可视化和统计测试验证结果。

Result: 实验结果表明，HEF可在全局性能指标（如R2、相对准确性、RMSE等）上表现优秀，且提高模型鲁棒性和解释能力；相对地，FMAE在局部性能指标（如MAE、执行时间）上具有优势，更适合短期场景。

Conclusion: 研究提出了一种可复制的预测模型优化框架，认为HEF适合战略性需求规划，而FMAE更适合短期操作效能的提高，这为动态环境下的预测提供了重要参考。

Abstract: Demand forecasting is essential for strategic planning in competitive
environments, enabling resource optimization and improved responsiveness to
market dynamics. However, multivariate time series modeling faces challenges
due to data complexity, uncertainty, and frequent regime shifts. Traditional
evaluation metrics can introduce biases and limit generalization. This work
compares two custom evaluation functions: FMAE (Focused Mean Absolute Error),
focused on minimizing absolute errors, and HEF (Hierarchical Evaluation
Function), designed to weight global metrics and penalize large deviations.
Experiments were conducted under different data splits (91:9, 80:20, 70:30)
using three optimizers (Grid Search, PSO, Optuna), assessing fit, relative
accuracy, robustness, and computational efficiency. Results show that HEF
consistently outperforms FMAE in global metrics (R2, Relative Accuracy, RMSE,
RMSSE), enhancing model robustness and explanatory power. These findings were
confirmed via visualizations and statistical tests. Conversely, FMAE offers
advantages in local metrics (MAE, MASE) and execution time, making it suitable
for short-term scenarios. The study highlights a methodological trade-off: HEF
is ideal for strategic planning, while FMAE is better suited for operational
efficiency. A replicable framework is proposed for optimizing predictive models
in dynamic environments.

</details>


### [396] [Seeing the Many: Exploring Parameter Distributions Conditioned on Features in Surrogates](https://arxiv.org/abs/2508.13088)
*Xiaohan Wang,Zhimin Li,Joshua A. Levine,Matthew Berger*

Main category: cs.LG

TL;DR: 本论文提出了一种可视化输入参数分布的方法，用于研究产生指定输出特征的可能输入参数分布。


<details>
  <summary>Details</summary>
Motivation: 传统模拟流程昂贵，神经代理模型为科学模拟提供了更高效的替代方案，并有潜力应用于解决逆问题。其中逆问题的核心是搜索能生成指定特征输出的输入参数集，但当前方法仅关注查找少量匹配参数，忽略了可能参数的全局分布。

Method: 本研究通过密度估计模型误差，结合基于输出特征的似然性，进行高效采样以生成输入参数的可能分布。此外，通过交互界面实现参数分布的可视化分析。

Result: 验证了所提方法在三个模拟数据集上的实用性，并通过可视化界面展示了基于特征驱动的输入参数分析。

Conclusion: 本文方法能够高效建模并可视化生成指定输出特征的输入参数分布，提供了一种用于特征驱动参数分析的工具。

Abstract: Recently, neural surrogate models have emerged as a compelling alternative to
traditional simulation workflows. This is accomplished by modeling the
underlying function of scientific simulations, removing the need to run
expensive simulations. Beyond just mapping from input parameter to output,
surrogates have also been shown useful for inverse problems: output to input
parameters. Inverse problems can be understood as search, where we aim to find
parameters whose surrogate outputs contain a specified feature. Yet finding
these parameters can be costly, especially for high-dimensional parameter
spaces. Thus, existing surrogate-based solutions primarily focus on finding a
small set of matching parameters, in the process overlooking the broader
picture of plausible parameters. Our work aims to model and visualize the
distribution of possible input parameters that produce a given output feature.
To achieve this goal, we aim to address two challenges: (1) the approximation
error inherent in the surrogate model and (2) forming the parameter
distribution in an interactive manner. We model error via density estimation,
reporting high density only if a given parameter configuration is close to
training parameters, measured both over the input and output space. Our density
estimate is used to form a prior belief on parameters, and when combined with a
likelihood on features, gives us an efficient way to sample plausible parameter
configurations that generate a target output feature. We demonstrate the
usability of our solution through a visualization interface by performing
feature-driven parameter analysis over the input parameter space of three
simulation datasets. Source code is available at
https://github.com/matthewberger/seeing-the-many

</details>


### [397] [Outlier Detection of Poisson-Distributed Targets Using a Seabed Sensor Network](https://arxiv.org/abs/2508.13099)
*Mingyu Kim,Daniel Stilwell,Jorge Jimenez*

Main category: cs.LG

TL;DR: 本文提出了一种利用海底声学传感器网络和对数高斯Cox过程（LGCPs）的框架，用于分类和检测海事环境中的空间异常数据。


<details>
  <summary>Details</summary>
Motivation: 为了提高海事环境中特别是船舶交通数据中异常事件的分类和检测精度，同时优化传感器放置，提高检测效率。

Method: 通过将目标事件建模为正常与异常过程的混合，提出一种第二阶概率近似方法结合实时优化的传感器布置策略，从而实现更高的分类和检测精度。

Result: 基于弗吉尼亚诺福克附近的真实船舶交通数据，验证了方法的有效性，实验结果表明大幅提高了分类性能和异常检测能力。

Conclusion: 框架提供了一种改进的方法来检测海事环境中的异常事件，通过综合建模方法与自适应传感器布置，实现了高效的分类与检测。

Abstract: This paper presents a framework for classifying and detecting spatial
commission outliers in maritime environments using seabed acoustic sensor
networks and log Gaussian Cox processes (LGCPs). By modeling target arrivals as
a mixture of normal and outlier processes, we estimate the probability that a
newly observed event is an outlier. We propose a second-order approximation of
this probability that incorporates both the mean and variance of the normal
intensity function, providing improved classification accuracy compared to
mean-only approaches. We analytically show that our method yields a tighter
bound to the true probability using Jensen's inequality. To enhance detection,
we integrate a real-time, near-optimal sensor placement strategy that
dynamically adjusts sensor locations based on the evolving outlier intensity.
The proposed framework is validated using real ship traffic data near Norfolk,
Virginia, where numerical results demonstrate the effectiveness of our approach
in improving both classification performance and outlier detection through
sensor deployment.

</details>


### [398] [A Perfectly Truthful Calibration Measure](https://arxiv.org/abs/2508.13100)
*Jason Hartline,Lunjia Hu,Yifan Wu*

Main category: cs.LG

TL;DR: 本文提出了一种新的校准测量方法ATB，可以完美实现真值，并较现有方法高效、易实现。


<details>
  <summary>Details</summary>
Motivation: 当前的校准测量方法在有限样本下可能不鼓励输出真实概率，导致校准结果不准确，需要设计一种完美真值的校准指标。

Method: 作者设计了一种名为ATB的校准测量方法，该方法通过定义平均两箱校准误差实现真值、连续性和完整性，并提出了一般性构造真值准则的方法。

Result: ATB在计算效率和实现简单性上优于现有的smCal和distCal，并通过实验证明了其在校准问题上的性能提升和有效性。

Conclusion: ATB作为一种完美真值校准测量方法，不仅改进了校准评估的精确性与效率，还为构建其他真值校准指标提供了新的理论框架。

Abstract: Calibration requires that predictions are conditionally unbiased and,
therefore, reliably interpretable as probabilities. Calibration measures
quantify how far a predictor is from perfect calibration. As introduced by
Haghtalab et al. (2024), a calibration measure is truthful if it is minimized
in expectation when a predictor outputs the ground-truth probabilities.
Although predicting the true probabilities guarantees perfect calibration, in
reality, when calibration is evaluated on a finite sample, predicting the truth
is not guaranteed to minimize any known calibration measure. All known
calibration measures incentivize predictors to lie in order to appear more
calibrated on a finite sample. Such lack of truthfulness motivated Haghtalab et
al. (2024) and Qiao and Zhao (2025) to construct approximately truthful
calibration measures in the sequential prediction setting, but no perfectly
truthful calibration measure was known to exist even in the more basic batch
setting.
  We design a perfectly truthful calibration measure in the batch setting:
averaged two-bin calibration error (ATB). In addition to being truthful, ATB is
sound, complete, continuous, and quadratically related to two existing
calibration measures: the smooth calibration error (smCal) and the (lower)
distance to calibration (distCal). The simplicity in our definition of ATB
makes it efficient and straightforward to compute. ATB allows faster estimation
algorithms with significantly easier implementations than smCal and distCal,
achieving improved running time and simplicity for the calibration testing
problem studied by Hu et al. (2024). We also introduce a general recipe for
constructing truthful measures, which proves the truthfulness of ATB as a
special case and allows us to construct other truthful calibration measures
such as quantile-binned l_2-ECE.

</details>


### [399] [Causally-Guided Pairwise Transformer -- Towards Foundational Digital Twins in Process Industry](https://arxiv.org/abs/2508.13111)
*Michael Mayr,Georgios C. Chasparis*

Main category: cs.LG

TL;DR: 提出CGPT模型，通过将多维时间序列数据分解成对，解决工业系统中传统CD和CI模型的问题，在灵活性和适应性方面具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 解决工业多维时间序列数据中CD模型缺乏鲁棒性、CI模型缺乏交互动态建模能力的矛盾。

Method: 提出Causally-Guided Pairwise Transformer (CGPT)，结合因果图作为先验知识，采用成对建模的范式，在模型中使用维度无关的参数实现通道无关的学习层次。

Result: 在多组合成和真实工业数据集上的长短期预测任务中，CGPT在预测精度上显著超越CI和CD基线，并与端到端CD模型相比表现良好，同时保持对问题维度的无敏感性。

Conclusion: CGPT成功解决了工业系统数据建模中的CD/CI冲突，提高了建模的灵活性和扩展性，并展示了在预测精度方面的显著进展。

Abstract: Foundational modelling of multi-dimensional time-series data in industrial
systems presents a central trade-off: channel-dependent (CD) models capture
specific cross-variable dynamics but lack robustness and adaptability as model
layers are commonly bound to the data dimensionality of the tackled use-case,
while channel-independent (CI) models offer generality at the cost of modelling
the explicit interactions crucial for system-level predictive regression tasks.
To resolve this, we propose the Causally-Guided Pairwise Transformer (CGPT), a
novel architecture that integrates a known causal graph as an inductive bias.
The core of CGPT is built around a pairwise modeling paradigm, tackling the
CD/CI conflict by decomposing the multidimensional data into pairs. The model
uses channel-agnostic learnable layers where all parameter dimensions are
independent of the number of variables. CGPT enforces a CD information flow at
the pair-level and CI-like generalization across pairs. This approach
disentangles complex system dynamics and results in a highly flexible
architecture that ensures scalability and any-variate adaptability. We validate
CGPT on a suite of synthetic and real-world industrial datasets on long-term
and one-step forecasting tasks designed to simulate common industrial
complexities. Results demonstrate that CGPT significantly outperforms both CI
and CD baselines in predictive accuracy and shows competitive performance with
end-to-end trained CD models while remaining agnostic to the problem
dimensionality.

</details>


### [400] [Contrastive Representations for Temporal Reasoning](https://arxiv.org/abs/2508.13113)
*Alicja Ziarko,Michal Bortkiewicz,Michal Zawalski,Benjamin Eysenbach,Piotr Milos*

Main category: cs.LG

TL;DR: 提出CRTR方法解决经典AI中时空推理问题，通过去除虚假特征实现高效搜索及学习，并成功应用于复杂领域如Sokoban和魔方。


<details>
  <summary>Details</summary>
Motivation: 探讨是否可以通过一个能够捕捉感知和时序结构的表示，避免传统计划问题依赖搜索算法并实现时空推理。

Method: 采用对比学习，并引入负采样方法移除感知中的虚假特征，提出CRTR方法以增强时空推理能力。

Result: CRTR在Sokoban和魔方等复杂时空结构任务中表现优异，在魔方任务中能从任意初始状态学习表示并成功解出魔方。

Conclusion: CRTR实现了仅依靠学习表示进行高效时空推理，为无需外部搜索算法解决复杂结构问题提供了可行方案。

Abstract: In classical AI, perception relies on learning state-based representations,
while planning, which can be thought of as temporal reasoning over action
sequences, is typically achieved through search. We study whether such
reasoning can instead emerge from representations that capture both perceptual
and temporal structure. We show that standard temporal contrastive learning,
despite its popularity, often fails to capture temporal structure due to its
reliance on spurious features. To address this, we introduce Combinatorial
Representations for Temporal Reasoning (CRTR), a method that uses a negative
sampling scheme to provably remove these spurious features and facilitate
temporal reasoning. CRTR achieves strong results on domains with complex
temporal structure, such as Sokoban and Rubik's Cube. In particular, for the
Rubik's Cube, CRTR learns representations that generalize across all initial
states and allow it to solve the puzzle using fewer search steps than BestFS,
though with longer solutions. To our knowledge, this is the first method that
efficiently solves arbitrary Cube states using only learned representations,
without relying on an external search algorithm.

</details>


### [401] [Training Machine Learning Models on Human Spatio-temporal Mobility Data: An Experimental Study [Experiment Paper]](https://arxiv.org/abs/2508.13135)
*Yueyang Liu,Lance Kennedy,Ruochen Kong,Joon-Seok Kim,Andreas Züfle*

Main category: cs.LG

TL;DR: 文章探讨如何通过历史数据训练机器学习模型来预测个体未来几天或数周的完整轨迹，并进行了多种模型和参数的实验比较。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中于短期轨迹或下一个位置预测，而缺乏对宏观人类移动模式及其生活规律的关注。作者研究如何有效训练模型以预测个体未来完整轨迹。

Method: 文章使用长短期记忆网络（LSTM）和Transformer架构，通过广泛实验分析模型和参数配置，研究如何加入语义信息（如星期几）以及用户历史数据，并通过语义聚类和分层采样缓解数据偏斜问题。

Result: 包括加入语义信息、用户特定历史信息有助于模型理解模式且提高预测效果。同时，分层采样和小批量随机梯度优化在数据不平衡情况下对性能有提升作用。

Conclusion: 有效整合语义信息和个性化用户历史能显著提高人类移动预测模型性能。

Abstract: Individual-level human mobility prediction has emerged as a significant topic
of research with applications in infectious disease monitoring, child, and
elderly care. Existing studies predominantly focus on the microscopic aspects
of human trajectories: such as predicting short-term trajectories or the next
location visited, while offering limited attention to macro-level mobility
patterns and the corresponding life routines. In this paper, we focus on an
underexplored problem in human mobility prediction: determining the best
practices to train a machine learning model using historical data to forecast
an individuals complete trajectory over the next days and weeks. In this
experiment paper, we undertake a comprehensive experimental analysis of diverse
models, parameter configurations, and training strategies, accompanied by an
in-depth examination of the statistical distribution inherent in human mobility
patterns. Our empirical evaluations encompass both Long Short-Term Memory and
Transformer-based architectures, and further investigate how incorporating
individual life patterns can enhance the effectiveness of the prediction. We
show that explicitly including semantic information such as day-of-the-week and
user-specific historical information can help the model better understand
individual patterns of life and improve predictions. Moreover, since the
absence of explicit user information is often missing due to user privacy, we
show that the sampling of users may exacerbate data skewness and result in a
substantial loss in predictive accuracy. To mitigate data imbalance and
preserve diversity, we apply user semantic clustering with stratified sampling
to ensure that the sampled dataset remains representative. Our results further
show that small-batch stochastic gradient optimization improves model
performance, especially when human mobility training data is limited.

</details>


### [402] [MDPO: Overcoming the Training-Inference Divide of Masked Diffusion Language Models](https://arxiv.org/abs/2508.13148)
*Haoyu He,Katrin Renz,Yong Cao,Andreas Geiger*

Main category: cs.LG

TL;DR: 本文提出MDPO（Masked Diffusion Policy Optimization），解决扩散语言模型（Diffusion Language Models, MDLMs）在训练和推断之间的关键差异，并结合改进的重掩码策略（RCR），显著提升了生成质量和性能。


<details>
  <summary>Details</summary>
Motivation: 传统的自回归模型在生成速度和条件丰富性方面存在局限性，而扩散语言模型作为一种替代方案，虽然能更高效生成和利用双向上下文，但由于训练和推断中遮盖策略不同，存在性能差距问题，需解决此关键问题。

Method: 将有效去噪轨迹学习建模为序列决策问题，引入强化学习框架，提出Masked Diffusion Policy Optimization (MDPO)，利用Markov属性按推断的逐步细化计划训练模型，同时改进MDLM的重掩码策略（RCR）以灵活优化推断。

Result: 相比同行最佳方法，MDPO在梯度更新60倍减少的情况下实现了相同性能，并在MATH500和Countdown数据集分别获得9.6%和54.2%的显著性能提升。改进的RCR策略在无需额外训练的情况下进一步改善了生成质量，且与MDPO结合时效果更佳。

Conclusion: 研究表明，MDPO及其结合的RCR策略能显著缓解MDLMs在训练与推断之间的差异问题，为进一步研究提供了良好的潜力支持。

Abstract: Diffusion language models, as a promising alternative to traditional
autoregressive (AR) models, enable faster generation and richer conditioning on
bidirectional context. However, they suffer from a key discrepancy between
training and inference: during inference, MDLMs progressively reveal the
structure of the generated sequence by producing fewer and fewer masked tokens,
whereas this structure is ignored in training as tokens are masked at random.
Although this discrepancy between training and inference can lead to suboptimal
performance, it has been largely overlooked by previous works, leaving closing
this gap between the two stages an open problem. To address this, we frame the
problem of learning effective denoising trajectories as a sequential
decision-making problem and use the resulting framework to apply reinforcement
learning. We propose a novel Masked Diffusion Policy Optimization (MDPO) to
exploit the Markov property diffusion possesses and explicitly train the model
under the same progressive refining schedule used at inference. MDPO matches
the performance of the previous state-of-the-art (SOTA) method with 60x fewer
gradient updates, while achieving average improvements of 9.6% on MATH500 and
54.2% on Countdown over SOTA when trained within the same number of weight
updates. Additionally, we improve the remasking strategy of MDLMs as a plug-in
inference replacement to overcome the limitation that the model cannot refine
tokens flexibly. This simple yet effective training-free strategy, what we
refer to as RCR, consistently improves performance and yields additional gains
when combined with MDPO. Our findings establish great potential for
investigating the discrepancy between pre-training and inference of MDLMs.
Code: https://github.com/autonomousvision/mdpo. Project Page:
https://cli212.github.io/MDPO/.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [403] [Toward Practical Equilibrium Propagation: Brain-inspired Recurrent Neural Network with Feedback Regulation and Residual Connections](https://arxiv.org/abs/2508.11659)
*Zhuo Liu,Tao Chen*

Main category: cs.NE

TL;DR: 文章提出通过生物学启发的方法改进Equilibrium Propagation(EP)学习框架，提高其计算效率与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有EP实现存在稳定性差和计算成本高的问题，需要更加生物学可信的解决方案。

Method: 设计了一种反馈调节残差递归神经网络(FRE-RNN)，通过降低谱半径实现快速收敛，并采用残差连接缓解深RNN中的梯度消失问题。

Result: 提出的方法显著提升了EP在大规模网络下的应用效率，在基准任务中的表现可与反向传播(BP)相比。

Conclusion: 改进后的EP框架易于在大规模AI网络中应用，且为物理神经网络中的原位学习实现提供指导。

Abstract: Brain-like intelligent systems need brain-like learning methods. Equilibrium
Propagation (EP) is a biologically plausible learning framework with strong
potential for brain-inspired computing hardware. However, existing
im-plementations of EP suffer from instability and prohibi-tively high
computational costs. Inspired by the structure and dynamics of the brain, we
propose a biologically plau-sible Feedback-regulated REsidual recurrent neural
network (FRE-RNN) and study its learning performance in EP framework. Feedback
regulation enables rapid convergence by reducing the spectral radius. The
improvement in con-vergence property reduces the computational cost and
train-ing time of EP by orders of magnitude, delivering perfor-mance on par
with backpropagation (BP) in benchmark tasks. Meanwhile, residual connections
with brain-inspired topologies help alleviate the vanishing gradient problem
that arises when feedback pathways are weak in deep RNNs. Our approach
substantially enhances the applicabil-ity and practicality of EP in large-scale
networks that un-derpin artificial intelligence. The techniques developed here
also offer guidance to implementing in-situ learning in physical neural
networks.

</details>


### [404] [Learning Internal Biological Neuron Parameters and Complexity-Based Encoding for Improved Spiking Neural Networks Performance](https://arxiv.org/abs/2508.11674)
*Zofia Rudnicka,Janusz Szczepanski,Agnieszka Pregowska*

Main category: cs.NE

TL;DR: 该研究提出一种通过引入概率元神经元替代传统感知机神经元模型的新方法，显著提升了脉冲神经网络（SNNs）的分类准确率，同时结合Lempel-Ziv复杂度（LZC），提高了解释性与效率。


<details>
  <summary>Details</summary>
Motivation: 通过生物学启发的概率元神经元模型及一个新分类框架，提高SNNs的分类能力和效率，解决现有方法无法高效处理时空神经数据的问题。

Method: 1. 引入概率元神经元并替代传统感知机模型；2. 提出结合SNN与LZC的新框架；3. 使用Poisson过程模拟神经元脉冲行为，并采用多种学习算法如反向传播和Tempotron学习规则。

Result: 实验显示该方法在不同学习方法下可提升分类效率最高达11.00%，验证了学习额外神经元参数的优势。

Conclusion: 本文的新方法能够有效结合SNN的生物学合理性和LZC的结构性捕捉能力，在时空神经数据分类方面表现优异。

Abstract: This study introduces a novel approach by replacing the traditional
perceptron neuron model with a biologically inspired probabilistic meta neuron,
where the internal neuron parameters are jointly learned, leading to improved
classification accuracy of spiking neural networks (SNNs). To validate this
innovation, we implement and compare two SNN architectures: one based on
standard leaky integrate-and-fire (LIF) neurons and another utilizing the
proposed probabilistic meta neuron model. As a second key contribution, we
present a new biologically inspired classification framework that uniquely
integrates SNNs with Lempel-Ziv complexity (LZC) a measure closely related to
entropy rate. By combining the temporal precision and biological plausibility
of SNNs with the capacity of LZC to capture structural regularity, the proposed
approach enables efficient and interpretable classification of spatiotemporal
neural data, an aspect not addressed in existing works. We consider learning
algorithms such as backpropagation, spike-timing-dependent plasticity (STDP),
and the Tempotron learning rule. To explore neural dynamics, we use Poisson
processes to model neuronal spike trains, a well-established method for
simulating the stochastic firing behavior of biological neurons. Our results
reveal that depending on the training method, the classifier's efficiency can
improve by up to 11.00%, highlighting the advantage of learning additional
neuron parameters beyond the traditional focus on weighted inputs alone.

</details>


### [405] [Adaptive Spiking with Plasticity for Energy Aware Neuromorphic Systems](https://arxiv.org/abs/2508.11689)
*Eduardo Calle-Ortiz,Hui Guan,Deepak Ganesan,Phuc Nguyen*

Main category: cs.NE

TL;DR: ASPEN是一种新型的能量感知技术，适用于神经形态系统，旨在降低能耗同时保持性能，适合资源有限的可穿戴设备。


<details>
  <summary>Details</summary>
Motivation: 研究神经形态计算在可穿戴设备中的可行性，提出能量感知计算的新方法，以应对资源受限下的高效计算需求。

Method: 利用ASPEN，通过在训练时对神经元阈值进行随机扰动，提升网络的鲁棒性、减少尖峰活动次数，同时自适应调整神经元参数实现动态能量控制，无需进行复杂的重新训练或剪枝。

Result: ASPEN在神经形态模拟器和硬件上的评估表明，其能显著减少尖峰数量和能耗，且在准确性上可与当前先进方法相媲美。

Conclusion: ASPEN是一种轻量化且可扩展的技术，为能耗感知和低功率神经计算提供了有效解决方案，为始终开启的可穿戴设备带来了变革性可能性。

Abstract: This paper presents ASPEN, a novel energy-aware technique for neuromorphic
systems that could unleash the future of intelligent, always-on,
ultra-low-power, and low-burden wearables. Our main research objectives are to
explore the feasibility of neuromorphic computing for wearables, identify open
research directions, and demonstrate the feasibility of developing an adaptive
spiking technique for energy-aware computation, which can be game-changing for
resource-constrained devices in always-on applications. As neuromorphic
computing systems operate based on spike events, their energy consumption is
closely related to spiking activity, i.e., each spike incurs computational and
power costs; consequently, minimizing the number of spikes is a critical
strategy for operating under constrained energy budgets. To support this goal,
ASPEN utilizes stochastic perturbations to the neuronal threshold during
training to not only enhance the network's robustness across varying
thresholds, which can be controlled at inference time, but also act as a
regularizer that improves generalization, reduces spiking activity, and enables
energy control without the need for complex retraining or pruning. More
specifically, ASPEN adaptively adjusts intrinsic neuronal parameters as a
lightweight and scalable technique for dynamic energy control without
reconfiguring the entire model. Our evaluation on neuromorphic emulator and
hardware shows that ASPEN significantly reduces spike counts and energy
consumption while maintaining accuracy comparable to state-of-the-art methods.

</details>


### [406] [Data-Driven Discovery of Interpretable Kalman Filter Variants through Large Language Models and Genetic Programming](https://arxiv.org/abs/2508.11703)
*Vasileios Saketos,Sebastian Kaltenbach,Sergey Litvinov,Petros Koumoutsakos*

Main category: cs.NE

TL;DR: 此论文探讨了通过结合笛卡尔基因编程（CGP）和大型语言模型（LLM）来自动化发现Kalman滤波器算法的可能性，以及这种框架在科学计算中的算法发现潜力。


<details>
  <summary>Details</summary>
Motivation: 旨在探索是否可以通过自动化和数据驱动的方法（结合CGP和LLM），发现重要的科学计算算法，并在算法假设成立及失效情况下评估其性能。

Method: 使用CGP和LLM的结合框架，在不同条件下对Kalman滤波器的自动化生成进行测试，分析其最优性和可解释性，评估在假设失效时新的替代算法的优劣。

Result: 当Kalman滤波器的最优性假设成立时，该框架能生成接近最优的解决方案。当假设失效时，框架能够进化出比Kalman滤波器表现更好的替代算法，且具有可解释性。

Conclusion: CGP与LLM的结合框架是一种强有力的方法，可用于科学计算中的算法发现，具有解释性和自动化特性，对算法设计领域意义重大。

Abstract: Algorithmic discovery has traditionally relied on human ingenuity and
extensive experimentation. Here we investigate whether a prominent scientific
computing algorithm, the Kalman Filter, can be discovered through an automated,
data-driven, evolutionary process that relies on Cartesian Genetic Programming
(CGP) and Large Language Models (LLM). We evaluate the contributions of both
modalities (CGP and LLM) in discovering the Kalman filter under varying
conditions. Our results demonstrate that our framework of CGP and LLM-assisted
evolution converges to near-optimal solutions when Kalman optimality
assumptions hold. When these assumptions are violated, our framework evolves
interpretable alternatives that outperform the Kalman filter. These results
demonstrate that combining evolutionary algorithms and generative models for
interpretable, data-driven synthesis of simple computational modules is a
potent approach for algorithmic discovery in scientific computing.

</details>


### [407] [LLM4CMO: Large Language Model-aided Algorithm Design for Constrained Multiobjective Optimization](https://arxiv.org/abs/2508.11871)
*Zhen-Song Chen,Hong-Wei Ding,Xian-Jia Wang,Witold Pedrycz*

Main category: cs.NE

TL;DR: 本文提出了一种名为LLM4CMO的新型约束多目标进化算法（CMOEA），利用双人口两阶段框架结合大语言模型（LLMs）进行算法设计，显著优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 提出的原因为现有CMOEA设计复杂且成效不足，而LLMs在协助复杂任务设计中潜力尚未被充分挖掘，因而尝试将LLMs引入到约束多目标优化问题的算法设计中。

Method: 通过双人口两阶段框架，在第一阶段生成约束和无约束的Pareto前沿，第二阶段利用混合算子、基于epsilon的约束处理方法、动态资源分配（DRA）及分类策略进行优化。核心模块的设计通过LLMs与人类互动完成。

Result: 在六个基准测试集和十个实际CMOP中，LLM4CMO相比11种最先进算法表现更佳，且消融实验验证了LLMs模块化设计的有效性。

Conclusion: 初步证明了LLMs在复杂算法设计中的效率及其作为共设计者的潜力，推动了LLMs在优化算法开发中的新应用。

Abstract: Constrained multi-objective optimization problems (CMOPs) frequently arise in
real-world applications where multiple conflicting objectives must be optimized
under complex constraints. Existing dual-population two-stage algorithms have
shown promise by leveraging infeasible solutions to improve solution quality.
However, designing high-performing constrained multi-objective evolutionary
algorithms (CMOEAs) remains a challenging task due to the intricacy of
algorithmic components. Meanwhile, large language models (LLMs) offer new
opportunities for assisting with algorithm design; however, their effective
integration into such tasks remains underexplored. To address this gap, we
propose LLM4CMO, a novel CMOEA based on a dual-population, two-stage framework.
In Stage 1, the algorithm identifies both the constrained Pareto front (CPF)
and the unconstrained Pareto front (UPF). In Stage 2, it performs targeted
optimization using a combination of hybrid operators (HOps), an epsilon-based
constraint-handling method, and a classification-based UPF-CPF relationship
strategy, along with a dynamic resource allocation (DRA) mechanism. To reduce
design complexity, the core modules, including HOps, epsilon decay function,
and DRA, are decoupled and designed through prompt template engineering and
LLM-human interaction. Experimental results on six benchmark test suites and
ten real-world CMOPs demonstrate that LLM4CMO outperforms eleven
state-of-the-art baseline algorithms. Ablation studies further validate the
effectiveness of the LLM-aided modular design. These findings offer preliminary
evidence that LLMs can serve as efficient co-designers in the development of
complex evolutionary optimization algorithms. The code associated with this
article is available at https://anonymous.4open.science/r/LLM4CMO971.

</details>


### [408] [Improving MSA Estimation through Adaptive Weight Vectors in MOEA/D](https://arxiv.org/abs/2508.12133)
*Saem Hasan,Muhammad Ali Nayeem,M. Sohel Rahman*

Main category: cs.NE

TL;DR: 本文提出了一种新方法PMAO++，通过结合MOEA/D-ADF优化策略和PMAO生成解决方案，提高了多重序列比对的质量和随后的系统发育推断性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多重序列比对方法面临计算复杂性高和打分选择敏感的问题，因此需要新方法提高比对质量并改进系统发育推断的准确性。

Method: 提出MOEA/D-ADF算法，通过适应性调整子问题权重向量改进探索与开发的平衡，并结合PMAO生成解决方案，形成PMAO++方法，以产生多样的比对-树对集合。

Result: PMAO++在大部分基准测试中优于原始PMAO，展现了更低的假阴性率，并能更好地支持下游总结方法进行鲁棒的系统发育推断。

Conclusion: PMAO++能够显著改善多重序列比对的效果并提高系统发育分析的可靠性，但在某些具有大终端扩展特性的数据集上仍有改进空间。未来工作将致力于参数调优、更大规模的基准测试以及与总结树方法更紧密的集成。

Abstract: Accurate phylogenetic inference from biological sequences depends critically
on the quality of multiple sequence alignments, yet optimal alignment for many
sequences is computationally intractable and sensitive to scoring choices. In
this work we introduce MOEA/D-ADF, a novel variant of MOEA/D that adaptively
adjusts subproblem weight vectors based on fitness variance to improve the
exploration-exploitation trade-off. We combine MOEA/D-ADF with PMAO (PASTA with
many application-aware optimization criteria) to form PMAO++, where
PMAO-generated solutions are used to seed MOEA/D-ADF, which then evolves a
population using 30 weight vectors to produce a diverse ensemble of
alignment-tree pairs. PMAO++ outperforms the original PMAO on a majority of
benchmark cases, achieving better false-negative (FN) rates on 12 of 17
BAliBASE-derived datasets and producing superior best-case trees, including
several instances with zero FN rate. Beyond improving single best alignments,
the rich set of alignment-tree pairs produced by PMAO++ is especially valuable
for downstream summary methods (for example, consensus and summary-tree
approaches), allowing more robust phylogenetic inference by integrating signal
across multiple plausible alignments and trees. Certain dataset features, such
as large terminal N/C extensions found in the RV40 group, remain challenging,
but overall PMAO++ demonstrates clear advantages for sequence-based
phylogenetic analysis. Future work will explore parameter tuning, larger
benchmark suites, and tighter integration with summary-tree pipelines to
further enhance applicability for biological sequence studies.

</details>


### [409] [A Self-Ensemble Inspired Approach for Effective Training of Binary-Weight Spiking Neural Networks](https://arxiv.org/abs/2508.12609)
*Qingyan Meng,Mingqing Xiao,Zhengyu Ma,Huihui Zhou,Yonghong Tian,Zhouchen Lin*

Main category: cs.NE

TL;DR: 论文提出一种新的训练方法SEI-BWSNN，用于提高二值权重脉冲神经网络(Binary-Weight SNNs)的性能，并在ImageNet上验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络(SNNs)由于其能量效率成为低功耗神经形态硬件的有力工具，但其训练由于非可微分的脉冲生成函数而面临挑战。二值神经网络(BNNs)也有类似问题，两者的联系和技术共享潜力尚未被系统研究。

Method: 通过研究SNN的动态，将其视为带噪声注入的二值激活神经网络的自集成训练过程，提出基于自集成灵感的训练方法SEI-BWSNN，结合多条快捷路径结构和知识蒸馏技术，提高二值权重SNN的训练性能。

Result: 在ImageNet上，仅用2次时间步长的情况下，二值权重SNN在Transformer结构中实现了82.52%的准确率。

Conclusion: SEI-BWSNN方法展示了其高效性，揭示了SNN和BNN之间的深层联系及其相互受益的潜力，推动了二值权重SNNs在实际应用中的可行性。

Abstract: Spiking Neural Networks (SNNs) are a promising approach to low-power
applications on neuromorphic hardware due to their energy efficiency. However,
training SNNs is challenging because of the non-differentiable spike generation
function. To address this issue, the commonly used approach is to adopt the
backpropagation through time framework, while assigning the gradient of the
non-differentiable function with some surrogates. Similarly, Binary Neural
Networks (BNNs) also face the non-differentiability problem and rely on
approximating gradients. However, the deep relationship between these two
fields and how their training techniques can benefit each other has not been
systematically researched. Furthermore, training binary-weight SNNs is even
more difficult. In this work, we present a novel perspective on the dynamics of
SNNs and their close connection to BNNs through an analysis of the
backpropagation process. We demonstrate that training a feedforward SNN can be
viewed as training a self-ensemble of a binary-activation neural network with
noise injection. Drawing from this new understanding of SNN dynamics, we
introduce the Self-Ensemble Inspired training method for (Binary-Weight) SNNs
(SEI-BWSNN), which achieves high-performance results with low latency even for
the case of the 1-bit weights. Specifically, we leverage a structure of
multiple shortcuts and a knowledge distillation-based training technique to
improve the training of (binary-weight) SNNs. Notably, by binarizing FFN layers
in a Transformer architecture, our approach achieves 82.52% accuracy on
ImageNet with only 2 time steps, indicating the effectiveness of our
methodology and the potential of binary-weight SNNs.

</details>


### [410] [IzhiRISC-V -- a RISC-V-based Processor with Custom ISA Extension for Spiking Neuron Networks Processing with Izhikevich Neurons](https://arxiv.org/abs/2508.12846)
*Wiktor J. Szczerek,Artur Podobas*

Main category: cs.NE

TL;DR: 本研究提出了一种在RISC-V处理器上支持尖峰神经网络处理的定制ISA扩展。


<details>
  <summary>Details</summary>
Motivation: 传统硬件在运行尖峰神经网络时，由于需要频繁执行基本指令导致计算效率显著降低，因此需要优化处理方式以提高能效。

Method: 引入一套定制的ISA扩展，其中包含用于尖峰神经元更新的神经形态指令，并通过硬件扩展实现这些指令。

Result: 提出了一个基于RISC-V兼容处理器的系统原型，支持自定义的神经形态ISA扩展，称为IzhiRISC-V。

Conclusion: 本文迈出了利用定制ISA扩展来实现高能效尖峰神经网络处理系统的重要一步。

Abstract: Spiking Neural Network processing promises to provide high energy efficiency
due to the sparsity of the spiking events. However, when realized on
general-purpose hardware -- such as a RISC-V processor -- this promise can be
undermined and overshadowed by the inefficient code, stemming from repeated
usage of basic instructions for updating all the neurons in the network. One of
the possible solutions to this issue is the introduction of a custom ISA
extension with neuromorphic instructions for spiking neuron updating, and
realizing those instructions in bespoke hardware expansion to the existing ALU.
In this paper, we present the first step towards realizing a large-scale system
based on the RISC-V-compliant processor called IzhiRISC-V, supporting the
custom neuromorphic ISA extension.

</details>
