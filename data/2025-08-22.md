<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 67]
- [cs.CL](#cs.CL) [Total: 54]
- [cs.AI](#cs.AI) [Total: 37]
- [cs.LG](#cs.LG) [Total: 70]
- [cs.NE](#cs.NE) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Heatmap Regression without Soft-Argmax for Facial Landmark Detection](https://arxiv.org/abs/2508.14929)
*Chiao-An Yang,Raymond A. Yeh*

Main category: cs.CV

TL;DR: 作者提出了基于经典结构化预测框架的替代训练目标，针对人脸关键点检测任务，比现有的软Argmax方法训练更快且效果更优。


<details>
  <summary>Details</summary>
Motivation: 人脸关键点检测任务需要更加高效、准确的训练方法，而现有的软Argmax方法虽然流行，但可能并不是最佳选择。

Method: 提出基于经典结构化预测框架的新训练目标，避免使用软Argmax，同时保持端到端深度网络训练的能力。

Result: 方法在三个基准数据集（WFLW、COFW、300W）上达到最先进的性能，训练速度提升了2.2倍，同时保持更好的或有竞争力的准确性。

Conclusion: 传统的软Argmax并非唯一的选择，提出的方法显示了在效率和准确性上的双重改进，验证了结构化预测框架的有效性。

Abstract: Facial landmark detection is an important task in computer vision with
numerous applications, such as head pose estimation, expression analysis, face
swapping, etc. Heatmap regression-based methods have been widely used to
achieve state-of-the-art results in this task. These methods involve computing
the argmax over the heatmaps to predict a landmark. Since argmax is not
differentiable, these methods use a differentiable approximation, Soft-argmax,
to enable end-to-end training on deep-nets. In this work, we revisit this
long-standing choice of using Soft-argmax and demonstrate that it is not the
only way to achieve strong performance. Instead, we propose an alternative
training objective based on the classic structured prediction framework.
Empirically, our method achieves state-of-the-art performance on three facial
landmark benchmarks (WFLW, COFW, and 300W), converging 2.2x faster during
training while maintaining better/competitive accuracy. Our code is available
here: https://github.com/ca-joe-yang/regression-without-softarg.

</details>


### [2] [Fast Graph Neural Network for Image Classification](https://arxiv.org/abs/2508.14958)
*Mustafa Mohammadi Gharasuie,Luis Rueda*

Main category: cs.CV

TL;DR: 本研究提出了一种结合图卷积网络（GCNs）与Voronoi图的新方法，可显著提高图像分类性能，优于最先进的方法，特别是在复杂场景和细粒度类别中表现出色。


<details>
  <summary>Details</summary>
Motivation: 近年来图像分类的快速进展主要得益于图卷积网络（GCNs）的采用，其在处理复杂数据结构方面表现卓越。本研究旨在优化现有技术的局限性，通过结合GCNs与Voronoi图改善图像分类方法。

Method: 将图像表示为以像素或图像区域为顶点的图形，并结合Delaunay三角剖分优化图表示，利用图卷积网络和Voronoi图来建模关系数据，从而提升图像分类效果。

Result: 所提方法在多种基准数据集上显著改善了预处理效率和分类精度，尤其在复杂场景和细粒度类别中超越了当前最先进技术。实验结果通过交叉验证进一步验证了方法的有效性。

Conclusion: 结合GCNs与Voronoi图的创新方法不仅提升了图像分类性能，还拓展了基于图学习范式在计算机视觉和非结构化数据分析中的应用潜力。

Abstract: The rapid progress in image classification has been largely driven by the
adoption of Graph Convolutional Networks (GCNs), which offer a robust framework
for handling complex data structures. This study introduces a novel approach
that integrates GCNs with Voronoi diagrams to enhance image classification by
leveraging their ability to effectively model relational data. Unlike
conventional convolutional neural networks (CNNs), our method represents images
as graphs, where pixels or regions function as vertices. These graphs are then
refined using corresponding Delaunay triangulations, optimizing their
representation. The proposed model achieves significant improvements in both
preprocessing efficiency and classification accuracy across various benchmark
datasets, surpassing state-of-the-art approaches, particularly in challenging
scenarios involving intricate scenes and fine-grained categories. Experimental
results, validated through cross-validation, underscore the effectiveness of
combining GCNs with Voronoi diagrams for advancing image classification. This
research not only presents a novel perspective on image classification but also
expands the potential applications of graph-based learning paradigms in
computer vision and unstructured data analysis.

</details>


### [3] [You Only Pose Once: A Minimalist's Detection Transformer for Monocular RGB Category-level 9D Multi-Object Pose Estimation](https://arxiv.org/abs/2508.14965)
*Hakjin Lee,Junghoon Seo,Jaehoon Sim*

Main category: cs.CV

TL;DR: 本文提出YOPO，一个轻量级的单阶段框架，能够直接用于基于类别级别的9自由度位姿估计，无需额外数据和伪深度支持，展示了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 目前对于可见类别的新样本的9自由度位姿估计多依赖伪深度、CAD模型或多阶段方法，这些都增加了方法的复杂性。本文旨在开发一种仅基于RGB图像、无需其他额外数据的简化模型。

Method: YOPO是一个单阶段、基于查询的框架，结合了transformer检测器、轻量级位姿估计头部、基于边界框的平移模块和6D感知的Hungarian匹配代价。模型仅需RGB图像和类别级位姿标签来端到端训练。

Result: 在REAL275数据集上，YOPO模型达到了79.6%的$\rm{IoU}_{50}$和54.1%的$10^\circ$$10{\rm{cm}}$指标表现，超越了以往RGB-only方法，性能接近RGB-D系统。

Conclusion: YOPO以极简设计实现高性能的类别级别位姿估计，达到了新的技术水平，同时去除了对伪深度和多阶段处理的依赖。

Abstract: Accurately recovering the full 9-DoF pose of unseen instances within specific
categories from a single RGB image remains a core challenge for robotics and
automation. Most existing solutions still rely on pseudo-depth, CAD models, or
multi-stage cascades that separate 2D detection from pose estimation. Motivated
by the need for a simpler, RGB-only alternative that learns directly at the
category level, we revisit a longstanding question: Can object detection and
9-DoF pose estimation be unified with high performance, without any additional
data? We show that they can with our method, YOPO, a single-stage, query-based
framework that treats category-level 9-DoF estimation as a natural extension of
2D detection. YOPO augments a transformer detector with a lightweight pose
head, a bounding-box-conditioned translation module, and a 6D-aware Hungarian
matching cost. The model is trained end-to-end only with RGB images and
category-level pose labels. Despite its minimalist design, YOPO sets a new
state of the art on three benchmarks. On the REAL275 dataset, it achieves 79.6%
$\rm{IoU}_{50}$ and 54.1% under the $10^\circ$$10{\rm{cm}}$ metric, surpassing
prior RGB-only methods and closing much of the gap to RGB-D systems. The code,
models, and additional qualitative results can be found on our project.

</details>


### [4] [Paired-Sampling Contrastive Framework for Joint Physical-Digital Face Attack Detection](https://arxiv.org/abs/2508.14980)
*Andrei Balykin,Anvar Ganiev,Denis Kondranin,Kirill Polevoda,Nikolai Liudkevich,Artem Petrov*

Main category: cs.CV

TL;DR: 这篇文章提出了一个统一的训练方法来检测脸部防伪攻击（包括物理攻击和数字伪造攻击），效果显著提升。


<details>
  <summary>Details</summary>
Motivation: 目前的系统分别处理物理和数字伪造攻击，但这增加了复杂性，导致性能低下，同时无法有效应对联合攻击。

Method: 文章提出了Paired-Sampling Contrastive Framework，这是一种利用真实与攻击自拍对进行学习的训练方法，可捕捉通用的防伪特征。

Result: 该方法在6th Face Anti-Spoofing Challenge基准上取得了2.10%的ACER，超越了现有方法，并且轻量化（4.46 GFLOPs），训练时间少于一小时。

Conclusion: 该方法简化了系统设计，同时提升了防伪检测的性能，非常适合实际使用场景。

Abstract: Modern face recognition systems remain vulnerable to spoofing attempts,
including both physical presentation attacks and digital forgeries.
Traditionally, these two attack vectors have been handled by separate models,
each targeting its own artifacts and modalities. However, maintaining distinct
detectors increases system complexity and inference latency and leaves systems
exposed to combined attack vectors. We propose the Paired-Sampling Contrastive
Framework, a unified training approach that leverages automatically matched
pairs of genuine and attack selfies to learn modality-agnostic liveness cues.
Evaluated on the 6th Face Anti-Spoofing Challenge Unified Physical-Digital
Attack Detection benchmark, our method achieves an average classification error
rate (ACER) of 2.10 percent, outperforming prior solutions. The framework is
lightweight (4.46 GFLOPs) and trains in under one hour, making it practical for
real-world deployment. Code and pretrained models are available at
https://github.com/xPONYx/iccv2025_deepfake_challenge.

</details>


### [5] [TAIGen: Training-Free Adversarial Image Generation via Diffusion Models](https://arxiv.org/abs/2508.15020)
*Susim Roy,Anubhooti Jain,Mayank Vatsa,Richa Singh*

Main category: cs.CV

TL;DR: TAIGen是一种快速生成对抗性图像的方法，仅需3-20步采样，通过注入扰动实现高效攻击，同时保持高质量图像。


<details>
  <summary>Details</summary>
Motivation: 此研究旨在克服生成模型对抗性攻击图像质量低和计算资源需求高的问题，利用扩散模型提高生成质量并减少生成所需的步骤数。

Method: 提出TAIGen方法，通过在扩散模型混合步骤中注入扰动，并采用RGB通道选择策略及GradCAM引导的扰动来生成高效的对抗性图像。

Result: 在ImageNet上以VGGNet为源模型，TAIGen对ResNet、MNASNet和ShuffleNet的攻击成功率分别为70.6%、80.8%和97.8%，同时生成速度比现有方法快10倍，并保持图像PSNR超过30 dB。

Conclusion: TAIGen方法大幅提高了扩散模型对抗性图像生成的效率和效果，并对防御机制显著挑战，表明其潜在影响力值得关注。

Abstract: Adversarial attacks from generative models often produce low-quality images
and require substantial computational resources. Diffusion models, though
capable of high-quality generation, typically need hundreds of sampling steps
for adversarial generation. This paper introduces TAIGen, a training-free
black-box method for efficient adversarial image generation. TAIGen produces
adversarial examples using only 3-20 sampling steps from unconditional
diffusion models. Our key finding is that perturbations injected during the
mixing step interval achieve comparable attack effectiveness without processing
all timesteps. We develop a selective RGB channel strategy that applies
attention maps to the red channel while using GradCAM-guided perturbations on
green and blue channels. This design preserves image structure while maximizing
misclassification in target models. TAIGen maintains visual quality with PSNR
above 30 dB across all tested datasets. On ImageNet with VGGNet as source,
TAIGen achieves 70.6% success against ResNet, 80.8% against MNASNet, and 97.8%
against ShuffleNet. The method generates adversarial examples 10x faster than
existing diffusion-based attacks. Our method achieves the lowest robust
accuracy, indicating it is the most impactful attack as the defense mechanism
is least successful in purifying the images generated by TAIGen.

</details>


### [6] [Reversible Unfolding Network for Concealed Visual Perception with Generative Refinement](https://arxiv.org/abs/2508.15027)
*Chunming He,Fengyang Xiao,Rihan Zhang,Chengyu Fang,Deng-Ping Fan,Sina Farsiu*

Main category: cs.CV

TL;DR: 该论文提出了隐蔽视觉感知的新方法RUN++，将任务建模为数学优化问题，通过深度网络分阶段展开求解，结合扩散模型进行生成式优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要局限于遮罩域，未充分利用RGB域的潜力。作者希望通过扩展到RGB域提升隐蔽视觉感知的效果。

Method: RUN++将隐蔽视觉感知任务建模为数学优化问题，展开为多阶段深度网络，结合扩散模型应对不确定性，还设计了CORE模块（遮罩域建模）、CARE模块（RGB域增强）和FINE模块（局部区域生成优化）。

Result: RUN++在减少假阳性和假阴性方面取得显著改善，同时通过与扩散模型的结合大幅提升了细节还原效率。

Conclusion: RUN++提供了一种新范式，在实际退化条件下表现鲁棒性优异，并延展为双层优化框架。

Abstract: Existing methods for concealed visual perception (CVP) often leverage
reversible strategies to decrease uncertainty, yet these are typically confined
to the mask domain, leaving the potential of the RGB domain underexplored. To
address this, we propose a reversible unfolding network with generative
refinement, termed RUN++. Specifically, RUN++ first formulates the CVP task as
a mathematical optimization problem and unfolds the iterative solution into a
multi-stage deep network. This approach provides a principled way to apply
reversible modeling across both mask and RGB domains while leveraging a
diffusion model to resolve the resulting uncertainty. Each stage of the network
integrates three purpose-driven modules: a Concealed Object Region Extraction
(CORE) module applies reversible modeling to the mask domain to identify core
object regions; a Context-Aware Region Enhancement (CARE) module extends this
principle to the RGB domain to foster better foreground-background separation;
and a Finetuning Iteration via Noise-based Enhancement (FINE) module provides a
final refinement. The FINE module introduces a targeted Bernoulli diffusion
model that refines only the uncertain regions of the segmentation mask,
harnessing the generative power of diffusion for fine-detail restoration
without the prohibitive computational cost of a full-image process. This unique
synergy, where the unfolding network provides a strong uncertainty prior for
the diffusion model, allows RUN++ to efficiently direct its focus toward
ambiguous areas, significantly mitigating false positives and negatives.
Furthermore, we introduce a new paradigm for building robust CVP systems that
remain effective under real-world degradations and extend this concept into a
broader bi-level optimization framework.

</details>


### [7] [GasTwinFormer: A Hybrid Vision Transformer for Livestock Methane Emission Segmentation and Dietary Classification in Optical Gas Imaging](https://arxiv.org/abs/2508.15057)
*Toqi Tahamid Sarker,Mohamed Embaby,Taminul Islam,Amer AbuGhazaleh,Khaled R Ahmed*

Main category: cs.CV

TL;DR: 本研究提出GasTwinFormer，将混合视觉Transformer用于实时甲烷排放分割和光学气体成像中的饮食分类。


<details>
  <summary>Details</summary>
Motivation: 随着牲畜甲烷排放成为气候变化的重要因素，实现自动化监测对减轻气候影响至关重要。

Method: 使用Mix Twin编码器结合全球和局部关注机制，并使用轻量级LR-ASPP解码器进行多尺度特征聚合，实现统一框架下的甲烷分割与饮食分类。

Result: GasTwinFormer达到了74.47%的mIoU和83.63%的mF1分割性能，参数仅占3.348M，推理速度为114.9 FPS，且饮食分类准确率达100%。

Conclusion: 通过全面消融研究，确认了架构组件的有效性，展示了GasTwinFormer作为实时牲畜排放监测的实际解决方案的潜力。

Abstract: Livestock methane emissions represent 32% of human-caused methane production,
making automated monitoring critical for climate mitigation strategies. We
introduce GasTwinFormer, a hybrid vision transformer for real-time methane
emission segmentation and dietary classification in optical gas imaging through
a novel Mix Twin encoder alternating between spatially-reduced global attention
and locally-grouped attention mechanisms. Our architecture incorporates a
lightweight LR-ASPP decoder for multi-scale feature aggregation and enables
simultaneous methane segmentation and dietary classification in a unified
framework. We contribute the first comprehensive beef cattle methane emission
dataset using OGI, containing 11,694 annotated frames across three dietary
treatments. GasTwinFormer achieves 74.47% mIoU and 83.63% mF1 for segmentation
while maintaining exceptional efficiency with only 3.348M parameters, 3.428G
FLOPs, and 114.9 FPS inference speed. Additionally, our method achieves perfect
dietary classification accuracy (100%), demonstrating the effectiveness of
leveraging diet-emission correlations. Extensive ablation studies validate each
architectural component, establishing GasTwinFormer as a practical solution for
real-time livestock emission monitoring. Please see our project page at
gastwinformer.github.io.

</details>


### [8] [CurveFlow: Curvature-Guided Flow Matching for Image Generation](https://arxiv.org/abs/2508.15093)
*Yan Luo,Drake Du,Hao Huang,Yi Fang,Mengyu Wang*

Main category: cs.CV

TL;DR: 引入CurveFlow以解决基于线性轨迹的现有流动模型在图像生成过程中的复杂度和语义对齐问题。通过引入曲率引导，提升了文本到图像生成的语义一致性与质量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于线性轨迹的流动模型在生成过程中容易经过数据流形中的低概率区域，影响到生成图像与文本指令的语义对齐性能。

Method: 提出了一种名为CurveFlow的新框架，直接在流动路径中注入曲率引导，采用曲率正则化技术来学习平滑的非线性轨迹。

Result: 在MS COCO 2014和2017数据集上的实验表明，CurveFlow在语义一致性指标（如BLEU、METEOR、ROUGE和CLAIR）上显著优于标准及非线性基线模型，同时维持高图像质量。

Conclusion: 曲率感知的建模能够有效增强模型的复杂指令跟随性与高质量图像生成能力，表明CurveFlow在文本到图像生成领域的显著潜力。

Abstract: Existing rectified flow models are based on linear trajectories between data
and noise distributions. This linearity enforces zero curvature, which can
inadvertently force the image generation process through low-probability
regions of the data manifold. A key question remains underexplored: how does
the curvature of these trajectories correlate with the semantic alignment
between generated images and their corresponding captions, i.e., instructional
compliance? To address this, we introduce CurveFlow, a novel flow matching
framework designed to learn smooth, non-linear trajectories by directly
incorporating curvature guidance into the flow path. Our method features a
robust curvature regularization technique that penalizes abrupt changes in the
trajectory's intrinsic dynamics.Extensive experiments on MS COCO 2014 and 2017
demonstrate that CurveFlow achieves state-of-the-art performance in
text-to-image generation, significantly outperforming both standard rectified
flow variants and other non-linear baselines like Rectified Diffusion. The
improvements are especially evident in semantic consistency metrics such as
BLEU, METEOR, ROUGE, and CLAIR. This confirms that our curvature-aware modeling
substantially enhances the model's ability to faithfully follow complex
instructions while simultaneously maintaining high image quality. The code is
made publicly available at
https://github.com/Harvard-AI-and-Robotics-Lab/CurveFlow.

</details>


### [9] [HiRQA: Hierarchical Ranking and Quality Alignment for Opinion-Unaware Image Quality Assessment](https://arxiv.org/abs/2508.15130)
*Vaishnav Ramesh,Haining Wang,Md Jahidul Islam*

Main category: cs.CV

TL;DR: 论文提出了HiRQA，一种无需参考意见的新型图像质量评估方法，结合排序和对比学习生成质量感知嵌入。减少了对主观标签和数据集偏差的依赖，支持高效实时评估。


<details>
  <summary>Details</summary>
Motivation: 现有的无参考图像质量评估方法在泛化能力上受限于数据集偏差和对主观标签的依赖。

Method: 使用一种自监督和无参考框架HiRQA，通过排序和对比学习生成分层的质量感知嵌入，同时引入排名损失和嵌入距离损失，以及训练期间基于结构化文本提示的对比对齐损失。

Result: 实验表明，HiRQA在合成和真实变形上的基准表现出色，表现出较强的泛化能力，即使在不同失真条件下也高效运行，并推出了轻量级版本HiRQA-S，仅需3.5 ms完成图像推理。

Conclusion: HiRQA减少了对参考图像和辅助模态的依赖，提升了无参考图像质量评估的泛化性和效率，达到当前最佳性能，同时支持实时评估。

Abstract: Despite significant progress in no-reference image quality assessment
(NR-IQA), dataset biases and reliance on subjective labels continue to hinder
their generalization performance. We propose HiRQA, Hierarchical Ranking and
Quality Alignment), a self-supervised, opinion-unaware framework that offers a
hierarchical, quality-aware embedding through a combination of ranking and
contrastive learning. Unlike prior approaches that depend on pristine
references or auxiliary modalities at inference time, HiRQA predicts quality
scores using only the input image. We introduce a novel higher-order ranking
loss that supervises quality predictions through relational ordering across
distortion pairs, along with an embedding distance loss that enforces
consistency between feature distances and perceptual differences. A
training-time contrastive alignment loss, guided by structured textual prompts,
further enhances the learned representation. Trained only on synthetic
distortions, HiRQA generalizes effectively to authentic degradations, as
demonstrated through evaluation on various distortions such as lens flare,
haze, motion blur, and low-light conditions. For real-time deployment, we
introduce \textbf{HiRQA-S}, a lightweight variant with an inference time of
only 3.5 ms per image. Extensive experiments across synthetic and authentic
benchmarks validate HiRQA's state-of-the-art (SOTA) performance, strong
generalization ability, and scalability.

</details>


### [10] [Reliable Multi-view 3D Reconstruction for `Just-in-time' Edge Environments](https://arxiv.org/abs/2508.15158)
*Md. Nurul Absur,Abhinav Kumar,Swastik Brahma,Saptarshi Debroy*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法，基于投资组合理论的边缘资源管理策略，提高存在时空分布性中断情况下的多视图3D重建可靠性。


<details>
  <summary>Details</summary>
Motivation: 3D多视图重建在紧急响应、战术场景和公共安全等需要快速态势感知的应用中起到了至关重要的作用。然而，这些应用的实时性和动态性需求使得在任务周期内临时架设的‘按需’边缘计算环境面临可靠性挑战。

Method: 作者提出了一种基于投资组合理论的边缘资源管理方法，通过遗传算法优化求解可快速收敛于真实系统设置，以选择合适的相机策略应对时空相关性干扰。

Result: 通过公开及定制3D数据集实验表明，该相机选择策略在时空中断情况下能够显著优于传统方法，并保证3D重建质量。

Conclusion: 该研究表明，基于投资组合理论的资源管理方法在边缘计算环境中能够有效提高多视图3D重建系统的可靠性，尤其适用于动态和受干扰的场景。

Abstract: Multi-view 3D reconstruction applications are revolutionizing critical use
cases that require rapid situational-awareness, such as emergency response,
tactical scenarios, and public safety. In many cases, their near-real-time
latency requirements and ad-hoc needs for compute resources necessitate
adoption of `Just-in-time' edge environments where the system is set up on the
fly to support the applications during the mission lifetime. However,
reliability issues can arise from the inherent dynamism and operational
adversities of such edge environments, resulting in spatiotemporally correlated
disruptions that impact the camera operations, which can lead to sustained
degradation of reconstruction quality. In this paper, we propose a novel
portfolio theory inspired edge resource management strategy for reliable
multi-view 3D reconstruction against possible system disruptions. Our proposed
methodology can guarantee reconstruction quality satisfaction even when the
cameras are prone to spatiotemporally correlated disruptions. The portfolio
theoretic optimization problem is solved using a genetic algorithm that
converges quickly for realistic system settings. Using publicly available and
customized 3D datasets, we demonstrate the proposed camera selection strategy's
benefits in guaranteeing reliable 3D reconstruction against traditional
baseline strategies, under spatiotemporal disruptions.

</details>


### [11] [XDR-LVLM: An Explainable Vision-Language Large Model for Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2508.15168)
*Masato Ito,Kaito Tanaka,Keisuke Matsuda,Aya Nakayama*

Main category: cs.CV

TL;DR: 提出了一种名为XDR-LVLM的框架，结合视觉-语言大模型，用于解释性糖尿病视网膜病诊断，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 应对糖尿病视网膜病（DR）是致盲的主要原因，深度学习模型虽然有效但因缺乏透明性和解释性阻碍了其临床应用。

Method: 提出XDR-LVLM框架，结合医学视觉编码器、视觉-语言大模型、多任务提示设计和多阶段微调，用于详细诊断和自然语言解释。

Result: 在DDR数据集上，XDR-LVLM在疾病诊断上取得84.55%的平衡准确性（BACC）和79.92%的F1分数，并在病理概念检测中也表现优越。

Conclusion: XDR-LVLM不仅提升了诊断性能，还提供了高效的自然语言解释，为自动化诊断与临床需求之间架起桥梁。

Abstract: Diabetic Retinopathy (DR) is a major cause of global blindness, necessitating
early and accurate diagnosis. While deep learning models have shown promise in
DR detection, their black-box nature often hinders clinical adoption due to a
lack of transparency and interpretability. To address this, we propose XDR-LVLM
(eXplainable Diabetic Retinopathy Diagnosis with LVLM), a novel framework that
leverages Vision-Language Large Models (LVLMs) for high-precision DR diagnosis
coupled with natural language-based explanations. XDR-LVLM integrates a
specialized Medical Vision Encoder, an LVLM Core, and employs Multi-task Prompt
Engineering and Multi-stage Fine-tuning to deeply understand pathological
features within fundus images and generate comprehensive diagnostic reports.
These reports explicitly include DR severity grading, identification of key
pathological concepts (e.g., hemorrhages, exudates, microaneurysms), and
detailed explanations linking observed features to the diagnosis. Extensive
experiments on the Diabetic Retinopathy (DDR) dataset demonstrate that XDR-LVLM
achieves state-of-the-art performance, with a Balanced Accuracy of 84.55% and
an F1 Score of 79.92% for disease diagnosis, and superior results for concept
detection (77.95% BACC, 66.88% F1). Furthermore, human evaluations confirm the
high fluency, accuracy, and clinical utility of the generated explanations,
showcasing XDR-LVLM's ability to bridge the gap between automated diagnosis and
clinical needs by providing robust and interpretable insights.

</details>


### [12] [MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View Consistent Diffusion](https://arxiv.org/abs/2508.15169)
*Xuyang Chen,Zhijun Zhai,Kaixuan Zhou,Zengmao Wang,Jianan He,Dong Wang,Yanfeng Zhang,mingwei Sun,Rüdiger Westermann,Konrad Schindler,Liqiu Meng*

Main category: cs.CV

TL;DR: 该论文提出了一种名为MeSS的3D场景生成方法，以城市网格模型为几何先验，使用改进的图像扩散模型提高跨视图一致性及生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有网格模型缺乏真实纹理，限制了虚拟城市导航和自动驾驶应用。研究动机是生成高质量且风格一致的3D户外场景。

Method: 方法包括三阶段：1. 使用Cascaded Outpainting ControlNets生成稀疏视图；2. 通过AGInpaint扩展密集中间视图；3. 使用GCAlign模块消除视觉不一致；同时通过3D高斯散射重建场景。

Result: 实验表明，该方法在几何一致性和生成质量两方面都优于现有方法。

Conclusion: 生成的3D场景不仅构造精确，还可通过重光和风格迁移技术渲染多种风格，提升应用潜力。

Abstract: Mesh models have become increasingly accessible for numerous cities; however,
the lack of realistic textures restricts their application in virtual urban
navigation and autonomous driving. To address this, this paper proposes MeSS
(Meshbased Scene Synthesis) for generating high-quality, styleconsistent
outdoor scenes with city mesh models serving as the geometric prior. While
image and video diffusion models can leverage spatial layouts (such as depth
maps or HD maps) as control conditions to generate street-level perspective
views, they are not directly applicable to 3D scene generation. Video diffusion
models excel at synthesizing consistent view sequences that depict scenes but
often struggle to adhere to predefined camera paths or align accurately with
rendered control videos. In contrast, image diffusion models, though unable to
guarantee cross-view visual consistency, can produce more geometry-aligned
results when combined with ControlNet. Building on this insight, our approach
enhances image diffusion models by improving cross-view consistency. The
pipeline comprises three key stages: first, we generate geometrically
consistent sparse views using Cascaded Outpainting ControlNets; second, we
propagate denser intermediate views via a component dubbed AGInpaint; and
third, we globally eliminate visual inconsistencies (e.g., varying exposure)
using the GCAlign module. Concurrently with generation, a 3D Gaussian Splatting
(3DGS) scene is reconstructed by initializing Gaussian balls on the mesh
surface. Our method outperforms existing approaches in both geometric alignment
and generation quality. Once synthesized, the scene can be rendered in diverse
styles through relighting and style transfer techniques.

</details>


### [13] [SurgWound-Bench: A Benchmark for Surgical Wound Diagnosis](https://arxiv.org/abs/2508.15189)
*Jiahao Xu,Changchang Yin,Odysseas Chatzipanagiotou,Diamantis Tsilimigras,Kevin Clear,Bingsheng Yao,Dakuo Wang,Timothy Pawlik,Ping Zhang*

Main category: cs.CV

TL;DR: 本文提出SurgWound数据集和三阶段学习框架WoundQwen，用于手术伤口诊断及相关任务。


<details>
  <summary>Details</summary>
Motivation: 手术部位感染是临床常见问题，现有研究受数据隐私和高成本标注限制，缺乏公开数据集和工具。

Method: 引入多类型手术伤口数据集SurgWound，开发包括视觉问答和报告生成任务的基准，提出三阶段学习框架WoundQwen。

Result: SurgWound数据集提供697张多样性手术伤口图像及精细标注，WoundQwen框架可实现手术伤口特征分析、感染风险评估及指导干预。

Conclusion: 所提方法促进个性化伤口护理、及时干预，改善患者疗效，并开辟公开资源支持的研究方向。

Abstract: Surgical site infection (SSI) is one of the most common and costly
healthcare-associated infections and and surgical wound care remains a
significant clinical challenge in preventing SSIs and improving patient
outcomes. While recent studies have explored the use of deep learning for
preliminary surgical wound screening, progress has been hindered by concerns
over data privacy and the high costs associated with expert annotation.
Currently, no publicly available dataset or benchmark encompasses various types
of surgical wounds, resulting in the absence of an open-source Surgical-Wound
screening tool. To address this gap: (1) we present SurgWound, the first
open-source dataset featuring a diverse array of surgical wound types. It
contains 697 surgical wound images annotated by 3 professional surgeons with
eight fine-grained clinical attributes. (2) Based on SurgWound, we introduce
the first benchmark for surgical wound diagnosis, which includes visual
question answering (VQA) and report generation tasks to comprehensively
evaluate model performance. (3) Furthermore, we propose a three-stage learning
framework, WoundQwen, for surgical wound diagnosis. In the first stage, we
employ five independent MLLMs to accurately predict specific surgical wound
characteristics. In the second stage, these predictions serve as additional
knowledge inputs to two MLLMs responsible for diagnosing outcomes, which assess
infection risk and guide subsequent interventions. In the third stage, we train
a MLLM that integrates the diagnostic results from the previous two stages to
produce a comprehensive report. This three-stage framework can analyze detailed
surgical wound characteristics and provide subsequent instructions to patients
based on surgical images, paving the way for personalized wound care, timely
intervention, and improved patient outcomes.

</details>


### [14] [Adversarial Agent Behavior Learning in Autonomous Driving Using Deep Reinforcement Learning](https://arxiv.org/abs/2508.15207)
*Arjun Srinivasan,Anubhav Paras,Aniket Bera*

Main category: cs.CV

TL;DR: 本文提出一种基于学习的方法，从规则代理中推导对抗性行为，验证在碰撞场景中的效果。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法通常利用规则代理建模环境中其他智能体。然而，在安全关键应用中，需精确模拟规则代理。本文旨在发现可能造成失败的对抗性行为模式。

Method: 本文提出一种学习方法，提取规则代理的对抗性行为潜力，从而生成可能诱发目标代理失败的策略。

Result: 实验结果表明，与规则代理相比，对抗性代理能够显著降低目标代理的累积奖励。

Conclusion: 生成对抗性行为模式可用于提高安全关键任务中目标代理的行为鲁棒性和学习效果。

Abstract: Existing approaches in reinforcement learning train an agent to learn desired
optimal behavior in an environment with rule based surrounding agents. In
safety critical applications such as autonomous driving it is crucial that the
rule based agents are modelled properly. Several behavior modelling strategies
and IDM models are used currently to model the surrounding agents. We present a
learning based method to derive the adversarial behavior for the rule based
agents to cause failure scenarios. We evaluate our adversarial agent against
all the rule based agents and show the decrease in cumulative reward.

</details>


### [15] [DyMorph-B2I: Dynamic and Morphology-Guided Binary-to-Instance Segmentation for Renal Pathology](https://arxiv.org/abs/2508.15208)
*Leiyue Zhao,Yuechen Yang,Yanfan Zhu,Haichun Yang,Yuankai Huo,Paul D. Simonson,Kenji Ikemura,Mert R. Sabuncu,Yihe Yang,Ruining Deng*

Main category: cs.CV

TL;DR: 提出了一种动态的形态学引导二值到实例分割流水线DyMorph-B2I，用于肾脏病理学实例分割，超越了现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 目前肾脏病理学实例分割依赖语义分割掩码，但无法满足精确形态学量化分析需求，现有方法在分离复杂形态和结构上存在限制。

Method: 提出DyMorph-B2I流程，将分水岭算法、骨架化和形态学操作结合在一个统一框架中，同时加入自适应几何优化和可调超参数以应对不同功能单元的分割需求。

Result: 实验结果表明，该方法在分割精度上优于单个传统操作和简单组合方法。此外，该方法能更准确区分肾脏病理语义掩码中的粘附与异构结构。

Conclusion: DyMorph-B2I大幅改进了实例分割的能力，为肾脏病理学形态学分析提供了科学工具，并已公开代码。

Abstract: Accurate morphological quantification of renal pathology functional units
relies on instance-level segmentation, yet most existing datasets and automated
methods provide only binary (semantic) masks, limiting the precision of
downstream analyses. Although classical post-processing techniques such as
watershed, morphological operations, and skeletonization, are often used to
separate semantic masks into instances, their individual effectiveness is
constrained by the diverse morphologies and complex connectivity found in renal
tissue. In this study, we present DyMorph-B2I, a dynamic, morphology-guided
binary-to-instance segmentation pipeline tailored for renal pathology. Our
approach integrates watershed, skeletonization, and morphological operations
within a unified framework, complemented by adaptive geometric refinement and
customizable hyperparameter tuning for each class of functional unit. Through
systematic parameter optimization, DyMorph-B2I robustly separates adherent and
heterogeneous structures present in binary masks. Experimental results
demonstrate that our method outperforms individual classical approaches and
na\"ive combinations, enabling superior instance separation and facilitating
more accurate morphometric analysis in renal pathology workflows. The pipeline
is publicly available at: https://github.com/ddrrnn123/DyMorph-B2I.

</details>


### [16] [STAGNet: A Spatio-Temporal Graph and LSTM Framework for Accident Anticipation](https://arxiv.org/abs/2508.15216)
*Vipooshan Vipulananthan,Kumudu Mohottala,Kavindu Chinthana,Nimsara Paramulla,Charith D Chitraranjan*

Main category: cs.CV

TL;DR: 研究提出了一种新的方法，通过改进空间-时间特征，并结合递归网络，从行车记录仪摄像头视频预测事故。


<details>
  <summary>Details</summary>
Motivation: 提高道路安全，减少事故对人类和财产的损害，提出一种仅使用行车记录仪摄像头的视频输入的成本低且易部署的方法。

Method: 采用先进的空间-时间特征提取，结合递归网络改进现有的图神经网络，开发出名为STAGNet的模型。

Result: 在公共数据集实验中，STAGNet在平均精度和碰撞时间预测上优于现有方法，无论是跨数据集验证还是单独训练测试。

Conclusion: STAGNet模型通过仅依靠经济高效的行车记录仪视频输入，提升了事故预测的效果，为未来交通安全技术开发提供了有力支持。

Abstract: Accident prediction and timely warnings play a key role in improving road
safety by reducing the risk of injury to road users and minimizing property
damage. Advanced Driver Assistance Systems (ADAS) are designed to support human
drivers and are especially useful when they can anticipate potential accidents
before they happen. While many existing systems depend on a range of sensors
such as LiDAR, radar, and GPS, relying solely on dash-cam video input presents
a more challenging but a more cost-effective and easily deployable solution. In
this work, we incorporate better spatio-temporal features and aggregate them
through a recurrent network to improve upon state-of-the-art graph neural
networks for predicting accidents from dash-cam videos. Experiments using three
publicly available datasets show that our proposed STAGNet model achieves
higher average precision and mean time-to-collision values than previous
methods, both when cross-validated on a given dataset and when trained and
tested on different datasets.

</details>


### [17] [Collaborative Multi-Modal Coding for High-Quality 3D Generation](https://arxiv.org/abs/2508.15228)
*Ziang Cao,Zhaoxi Chen,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: TriMM是一个3D原生生成模型，利用多模态数据(RGB、RGBD、点云)提升3D资产建模质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有生成模型仅依赖单一模态或局限于3D数据，未能利用多模态数据优势的问题。

Method: 提出协作多模态编码，结合模态特定特性，同时引入二维和三维的辅助监督，再通过三平面潜变量扩散模型生成高质量3D资产。

Result: 在多个数据集上的实验表明，TriMM即使在使用小规模数据且结合多模态情况下，仍能取得与大规模训练模型相竞争的表现。

Conclusion: TriMM证明多模态数据的有效融合可大幅提升3D资产生成的性能与质量，同时展现了其对多模态数据集的扩展能力。

Abstract: 3D content inherently encompasses multi-modal characteristics and can be
projected into different modalities (e.g., RGB images, RGBD, and point clouds).
Each modality exhibits distinct advantages in 3D asset modeling: RGB images
contain vivid 3D textures, whereas point clouds define fine-grained 3D
geometries. However, most existing 3D-native generative architectures either
operate predominantly within single-modality paradigms-thus overlooking the
complementary benefits of multi-modality data-or restrict themselves to 3D
structures, thereby limiting the scope of available training datasets. To
holistically harness multi-modalities for 3D modeling, we present TriMM, the
first feed-forward 3D-native generative model that learns from basic
multi-modalities (e.g., RGB, RGBD, and point cloud). Specifically, 1) TriMM
first introduces collaborative multi-modal coding, which integrates
modality-specific features while preserving their unique representational
strengths. 2) Furthermore, auxiliary 2D and 3D supervision are introduced to
raise the robustness and performance of multi-modal coding. 3) Based on the
embedded multi-modal code, TriMM employs a triplane latent diffusion model to
generate 3D assets of superior quality, enhancing both the texture and the
geometric detail. Extensive experiments on multiple well-known datasets
demonstrate that TriMM, by effectively leveraging multi-modality, achieves
competitive performance with models trained on large-scale datasets, despite
utilizing a small amount of training data. Furthermore, we conduct additional
experiments on recent RGB-D datasets, verifying the feasibility of
incorporating other multi-modal datasets into 3D generation.

</details>


### [18] [Center-Oriented Prototype Contrastive Clustering](https://arxiv.org/abs/2508.15231)
*Shihao Dong,Xiaotong Zhou,Yuhui Zheng,Huiying Xu,Xinzhong Zhu*

Main category: cs.CV

TL;DR: 本文研究如何改进对比学习在聚类任务中的表现，提出了一个中心导向的原型对比聚类框架，显著优于当前最新方法。


<details>
  <summary>Details</summary>
Motivation: 现有的对比学习方法在处理类别冲突问题时效果不佳，特别是硬原型计算与真实聚类中心之间的偏差问题。

Method: 提出了一个中心导向的原型对比聚类框架，包括软原型对比模块和双一致性学习模块，用于避免类别冲突、减少原型漂移，以及提升特征的语义一致性和聚类紧凑性。

Result: 在五个数据集上的广泛实验表明，该方法优于当前SOTA方法。

Conclusion: 提出的框架有效解决了类别冲突问题，改进了聚类任务中对比学习的表现并得到了验证。

Abstract: Contrastive learning is widely used in clustering tasks due to its
discriminative representation. However, the conflict problem between classes is
difficult to solve effectively. Existing methods try to solve this problem
through prototype contrast, but there is a deviation between the calculation of
hard prototypes and the true cluster center. To address this problem, we
propose a center-oriented prototype contrastive clustering framework, which
consists of a soft prototype contrastive module and a dual consistency learning
module. In short, the soft prototype contrastive module uses the probability
that the sample belongs to the cluster center as a weight to calculate the
prototype of each category, while avoiding inter-class conflicts and reducing
prototype drift. The dual consistency learning module aligns different
transformations of the same sample and the neighborhoods of different samples
respectively, ensuring that the features have transformation-invariant semantic
information and compact intra-cluster distribution, while providing reliable
guarantees for the calculation of prototypes. Extensive experiments on five
datasets show that the proposed method is effective compared to the SOTA. Our
code is published on https://github.com/LouisDong95/CPCC.

</details>


### [19] [AeroDuo: Aerial Duo for UAV-based Vision and Language Navigation](https://arxiv.org/abs/2508.15232)
*Ruipu Wu,Yige Zhang,Jinyu Chen,Linjiang Huang,Shifeng Zhang,Xu Zhou,Liang Wang,Si Liu*

Main category: cs.CV

TL;DR: 该论文提出了一个名为DuAl-VLN的新任务，让高低两个无人机利用视觉与语言导航协作完成任务。


<details>
  <summary>Details</summary>
Motivation: 推动无人机在利用更少人类干预情况下，实现自然语言导航与精细导航能力的结合。

Method: 提出Dual-Altitude UAV Collaborative VLN任务以及相应的HaL-13k数据集，并设计了AeroDuo框架，分别由高低空无人机协作完成任务。

Result: 引入了HaL-13k数据集以及高效的AeroDuo框架，在新环境中的通用化性能得到系统性评估。

Conclusion: 通过高低空无人机的互补合作，能够提升无人机在复杂目标环境中导航的效果与效率。

Abstract: Aerial Vision-and-Language Navigation (VLN) is an emerging task that enables
Unmanned Aerial Vehicles (UAVs) to navigate outdoor environments using natural
language instructions and visual cues. However, due to the extended
trajectories and complex maneuverability of UAVs, achieving reliable UAV-VLN
performance is challenging and often requires human intervention or overly
detailed instructions. To harness the advantages of UAVs' high mobility, which
could provide multi-grained perspectives, while maintaining a manageable motion
space for learning, we introduce a novel task called Dual-Altitude UAV
Collaborative VLN (DuAl-VLN). In this task, two UAVs operate at distinct
altitudes: a high-altitude UAV responsible for broad environmental reasoning,
and a low-altitude UAV tasked with precise navigation. To support the training
and evaluation of the DuAl-VLN, we construct the HaL-13k, a dataset comprising
13,838 collaborative high-low UAV demonstration trajectories, each paired with
target-oriented language instructions. This dataset includes both unseen maps
and an unseen object validation set to systematically evaluate the model's
generalization capabilities across novel environments and unfamiliar targets.
To consolidate their complementary strengths, we propose a dual-UAV
collaborative VLN framework, AeroDuo, where the high-altitude UAV integrates a
multimodal large language model (Pilot-LLM) for target reasoning, while the
low-altitude UAV employs a lightweight multi-stage policy for navigation and
target grounding. The two UAVs work collaboratively and only exchange minimal
coordinate information to ensure efficiency.

</details>


### [20] [Pretrained Diffusion Models Are Inherently Skipped-Step Samplers](https://arxiv.org/abs/2508.15233)
*Wenju Xu*

Main category: cs.CV

TL;DR: 提出了一种跳步采样机制以提升扩散模型的生成效率，与现有的非马尔可夫方法相比，其基于相同训练目标，同时实现高速、高质量的生成。


<details>
  <summary>Details</summary>
Motivation: 研究现有扩散模型中的生成效率问题，试图在保持相同训练目标的情况下减少采样步骤。

Method: 提出跳步采样机制，通过跳过多个中间步骤加速生成，并结合DDIM进一步优化生成过程，确保算法仍基于马尔可夫过程的训练目标。

Result: 在多个预训练扩散模型（如OpenAI ADM和Stable Diffusion）上的实验表明，新方法能够在显著减少采样步骤的同时保持甚至提升生成质量。

Conclusion: 跳步采样证明了扩散模型加速的可行性，是其内在属性的一部分，并在结合其他技术时表现出显著改进。

Abstract: Diffusion models have been achieving state-of-the-art results across various
generation tasks. However, a notable drawback is their sequential generation
process, requiring long-sequence step-by-step generation. Existing methods,
such as DDIM, attempt to reduce sampling steps by constructing a class of
non-Markovian diffusion processes that maintain the same training objective.
However, there remains a gap in understanding whether the original diffusion
process can achieve the same efficiency without resorting to non-Markovian
processes. In this paper, we provide a confirmative answer and introduce
skipped-step sampling, a mechanism that bypasses multiple intermediate
denoising steps in the iterative generation process, in contrast with the
traditional step-by-step refinement of standard diffusion inference. Crucially,
we demonstrate that this skipped-step sampling mechanism is derived from the
same training objective as the standard diffusion model, indicating that
accelerated sampling via skipped-step sampling via a Markovian way is an
intrinsic property of pretrained diffusion models. Additionally, we propose an
enhanced generation method by integrating our accelerated sampling technique
with DDIM. Extensive experiments on popular pretrained diffusion models,
including the OpenAI ADM, Stable Diffusion, and Open Sora models, show that our
method achieves high-quality generation with significantly reduced sampling
steps.

</details>


### [21] [Comp-X: On Defining an Interactive Learned Image Compression Paradigm With Expert-driven LLM Agent](https://arxiv.org/abs/2508.15243)
*Yixin Gao,Xin Li,Xiaohan Pan,Runsen Feng,Bingchen Li,Yunpeng Qi,Yiting Lu,Zhengxue Cheng,Zhibo Chen,Jörn Ostermann*

Main category: cs.CV

TL;DR: 提出了Comp-X，这是首个结合大语言模型智能交互能力的图像压缩范式。


<details>
  <summary>Details</summary>
Motivation: 克服传统图像编解码器模式选择受限且对普通用户不友好的限制。

Method: 引入多功能编码框架、交互式编码代理（通过增强的上下文学习方法）以及IIC基准以系统评估交互式图像压缩。

Result: Comp-X能够高效理解编码请求，保持高压缩性能，并具有令人印象深刻的文本交互能力。

Conclusion: Comp-X为人工通用智能（AGI）在图像压缩领域开辟了新的可能性。

Abstract: We present Comp-X, the first intelligently interactive image compression
paradigm empowered by the impressive reasoning capability of large language
model (LLM) agent. Notably, commonly used image codecs usually suffer from
limited coding modes and rely on manual mode selection by engineers, making
them unfriendly for unprofessional users. To overcome this, we advance the
evolution of image coding paradigm by introducing three key innovations: (i)
multi-functional coding framework, which unifies different coding modes of
various objective/requirements, including human-machine perception, variable
coding, and spatial bit allocation, into one framework. (ii) interactive coding
agent, where we propose an augmented in-context learning method with coding
expert feedback to teach the LLM agent how to understand the coding request,
mode selection, and the use of the coding tools. (iii) IIC-bench, the first
dedicated benchmark comprising diverse user requests and the corresponding
annotations from coding experts, which is systematically designed for
intelligently interactive image compression evaluation. Extensive experimental
results demonstrate that our proposed Comp-X can understand the coding requests
efficiently and achieve impressive textual interaction capability. Meanwhile,
it can maintain comparable compression performance even with a single coding
framework, providing a promising avenue for artificial general intelligence
(AGI) in image compression.

</details>


### [22] [Normal and Abnormal Pathology Knowledge-Augmented Vision-Language Model for Anomaly Detection in Pathology Images](https://arxiv.org/abs/2508.15256)
*Jinsol Song,Jiamu Wang,Anh Tien Nguyen,Keunho Byeon,Sangjeong Ahn,Sung Hak Lee,Jin Tae Kwak*

Main category: cs.CV

TL;DR: 提出了名为Ano-NAViLa的视觉语言模型，该模型用于计算病理中的异常检测，展示了在两个不同器官淋巴结数据集上的领先性能。


<details>
  <summary>Details</summary>
Motivation: 病理异常检测中，由于病变数据稀缺，现有主要为工业化设计的异常检测方法难以有效应对计算限制、组织结构多样性及可解释性不足等问题。

Method: 构建基于预训练视觉语言模型的Ano-NAViLa，通过轻量级可训练的MLP，并融入正常及异常病理知识，以增强对病理图像的准确性、稳健性及可解释性。

Result: 在两个不同器官的淋巴结数据集上，Ano-NAViLa在异常检测和定位方面取得了领先的性能，超过了当前的竞争模型。

Conclusion: Ano-NAViLa通过结合视觉语言及病理知识提供了一种有效且可解释性的异常检测方法，在病理分析上具有潜在的广泛应用价值。

Abstract: Anomaly detection in computational pathology aims to identify rare and scarce
anomalies where disease-related data are often limited or missing. Existing
anomaly detection methods, primarily designed for industrial settings, face
limitations in pathology due to computational constraints, diverse tissue
structures, and lack of interpretability. To address these challenges, we
propose Ano-NAViLa, a Normal and Abnormal pathology knowledge-augmented
Vision-Language model for Anomaly detection in pathology images. Ano-NAViLa is
built on a pre-trained vision-language model with a lightweight trainable MLP.
By incorporating both normal and abnormal pathology knowledge, Ano-NAViLa
enhances accuracy and robustness to variability in pathology images and
provides interpretability through image-text associations. Evaluated on two
lymph node datasets from different organs, Ano-NAViLa achieves the
state-of-the-art performance in anomaly detection and localization,
outperforming competing models.

</details>


### [23] [RATopo: Improving Lane Topology Reasoning via Redundancy Assignment](https://arxiv.org/abs/2508.15272)
*Han Li,Shaofei Huang,Longfei Xu,Yulu Gao,Beipeng Mu,Si Liu*

Main category: cs.CV

TL;DR: 提出了一种名为RATopo的策略，通过在Transformer解码器中重新设置层次结构和引入多路交叉注意模块，提高自主驾驶中的车道拓扑推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法中的监督策略在检测过程中由于其有限的有效范围而导致了次优的拓扑推理性能。

Method: 采用冗余分配设计，包括交换Transformer解码器中的交叉注意和自注意层，并引入多个并行交叉注意模块以提升结果的多样性。

Result: 在OpenLane-V2数据集上的实验表明，RATopo是框架无关的解决方案，能够稳定提升车道间及车道与交通元素间的拓扑性能。

Conclusion: RATopo通过启用丰富多样的拓扑监督显著改善了车道拓扑推理表现，可用于现有框架中。

Abstract: Lane topology reasoning plays a critical role in autonomous driving by
modeling the connections among lanes and the topological relationships between
lanes and traffic elements. Most existing methods adopt a
first-detect-then-reason paradigm, where topological relationships are
supervised based on the one-to-one assignment results obtained during the
detection stage. This supervision strategy results in suboptimal topology
reasoning performance due to the limited range of valid supervision. In this
paper, we propose RATopo, a Redundancy Assignment strategy for lane Topology
reasoning that enables quantity-rich and geometry-diverse topology supervision.
Specifically, we restructure the Transformer decoder by swapping the
cross-attention and self-attention layers. This allows redundant lane
predictions to be retained before suppression, enabling effective one-to-many
assignment. We also instantiate multiple parallel cross-attention blocks with
independent parameters, which further enhances the diversity of detected lanes.
Extensive experiments on OpenLane-V2 demonstrate that our RATopo strategy is
model-agnostic and can be seamlessly integrated into existing topology
reasoning frameworks, consistently improving both lane-lane and lane-traffic
topology performance.

</details>


### [24] [DesignCLIP: Multimodal Learning with CLIP for Design Patent Understanding](https://arxiv.org/abs/2508.15297)
*Zhu Wang,Homaira Huda Shomee,Sathya N. Ravi,Sourav Medya*

Main category: cs.CV

TL;DR: 本文提出DesignCLIP框架，利用CLIP模型及美国外观专利大规模数据集，改进专利分类与检索。


<details>
  <summary>Details</summary>
Motivation: 现有专利图像强调结构元素，缺乏视觉语义信息，评估存在模糊性，需更加可靠的AI分析方法。

Method: 设计了DesignCLIP框架，结合类感知分类、对比学习及生成详细图像描述，并运用多视图图像学习优化数据分析。

Result: DesignCLIP在专利分类、检索及多模态检索任务中表现优于基线及SOTA模型。

Conclusion: DesignCLIP展示了多模态模型在专利分析中的潜力，助力设计创新。

Abstract: In the field of design patent analysis, traditional tasks such as patent
classification and patent image retrieval heavily depend on the image data.
However, patent images -- typically consisting of sketches with abstract and
structural elements of an invention -- often fall short in conveying
comprehensive visual context and semantic information. This inadequacy can lead
to ambiguities in evaluation during prior art searches. Recent advancements in
vision-language models, such as CLIP, offer promising opportunities for more
reliable and accurate AI-driven patent analysis. In this work, we leverage CLIP
models to develop a unified framework DesignCLIP for design patent applications
with a large-scale dataset of U.S. design patents. To address the unique
characteristics of patent data, DesignCLIP incorporates class-aware
classification and contrastive learning, utilizing generated detailed captions
for patent images and multi-views image learning. We validate the effectiveness
of DesignCLIP across various downstream tasks, including patent classification
and patent retrieval. Additionally, we explore multimodal patent retrieval,
which provides the potential to enhance creativity and innovation in design by
offering more diverse sources of inspiration. Our experiments show that
DesignCLIP consistently outperforms baseline and SOTA models in the patent
domain on all tasks. Our findings underscore the promise of multimodal
approaches in advancing patent analysis. The codebase is available here:
https://anonymous.4open.science/r/PATENTCLIP-4661/README.md.

</details>


### [25] [TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect Classification](https://arxiv.org/abs/2508.15298)
*Darya Taratynova,Alya Almsouti,Beknur Kalmakhanbet,Numan Saeed,Mohammad Yaqub*

Main category: cs.CV

TL;DR: 提出了一种名为Temporal Prompt Alignment (TPA)的方法，用于胎儿先天性心脏缺陷（CHD）的检测，通过时序建模、对比学习和不确定性量化，对心脏超声视频进行分类，实现了先进的分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习方法在CHD检测中忽略了时间信息，局限于二分类且未考虑预测校准，需要方法克服这些限制并提高检测性能。

Method: 提出Temporal Prompt Alignment (TPA)方法，使用图像编码器提取视频子片段的每帧特征，结合可训练的时序提取器捕捉心脏运动，并通过边际铰链对比损失与类别相关文本提示对齐，同时引入条件变分自编码器风格调制（CVAESM）模块量化分类不确定性。

Result: TPA在CHD诊断上达到了85.40%的宏平均F1分数，并降低了5.38%的ECE和6.8%的自适应ECE。在EchoNet-Dynamic三分类任务中，宏平均F1分数提高了4.73%。

Conclusion: TPA框架集成了时序建模、提示感知对比学习和不确定性量化，可为胎儿CHD分类提供更高的性能和可靠性，在实际临床应用中具有潜在价值。

Abstract: Congenital heart defect (CHD) detection in ultrasound videos is hindered by
image noise and probe positioning variability. While automated methods can
reduce operator dependence, current machine learning approaches often neglect
temporal information, limit themselves to binary classification, and do not
account for prediction calibration. We propose Temporal Prompt Alignment (TPA),
a method leveraging foundation image-text model and prompt-aware contrastive
learning to classify fetal CHD on cardiac ultrasound videos. TPA extracts
features from each frame of video subclips using an image encoder, aggregates
them with a trainable temporal extractor to capture heart motion, and aligns
the video representation with class-specific text prompts via a margin-hinge
contrastive loss. To enhance calibration for clinical reliability, we introduce
a Conditional Variational Autoencoder Style Modulation (CVAESM) module, which
learns a latent style vector to modulate embeddings and quantifies
classification uncertainty. Evaluated on a private dataset for CHD detection
and on a large public dataset, EchoNet-Dynamic, for systolic dysfunction, TPA
achieves state-of-the-art macro F1 scores of 85.40% for CHD diagnosis, while
also reducing expected calibration error by 5.38% and adaptive ECE by 6.8%. On
EchoNet-Dynamic's three-class task, it boosts macro F1 by 4.73% (from 53.89% to
58.62%). Temporal Prompt Alignment (TPA) is a framework for fetal congenital
heart defect (CHD) classification in ultrasound videos that integrates temporal
modeling, prompt-aware contrastive learning, and uncertainty quantification.

</details>


### [26] [BasketLiDAR: The First LiDAR-Camera Multimodal Dataset for Professional Basketball MOT](https://arxiv.org/abs/2508.15299)
*Ryunosuke Hayashi,Kohei Torimi,Rokuto Nagata,Kazuma Ikeda,Ozora Sako,Taichi Nakamura,Masaki Tani,Yoshimitsu Aoki,Kentaro Yoshioka*

Main category: cs.CV

TL;DR: 本文提出了BasketLiDAR，多模态数据集结合了LiDAR点云和多视角相机数据，并开发了一种新的多目标跟踪(MOT)算法，提高运动员实时3D跟踪的准确度并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 目前运动比赛的战术分析和实时性能评估受到技术限制，传统基于视频的多目标跟踪(MOT)方法难以实时处理篮球比赛中的快速运动和遮挡问题。

Method: 本文提出了BasketLiDAR数据集，结合了LiDAR和多视角相机数据，并开发了一种包括单纯基于LiDAR的跟踪管道和融合了LiDAR与相机数据的多模态跟踪管道的新型MOT算法。

Result: 实验表明，提出的方法在遮挡情况下实现了优于传统相机方法的跟踪性能，同时支持实时操作。

Conclusion: BasketLiDAR的数据集为篮球比赛提供了全新的多视图、多维度的研究角度，其跟踪算法有效解决了传统方法的瓶颈，为未来运动分析及实时应用提供了潜力。

Abstract: Real-time 3D trajectory player tracking in sports plays a crucial role in
tactical analysis, performance evaluation, and enhancing spectator experience.
Traditional systems rely on multi-camera setups, but are constrained by the
inherently two-dimensional nature of video data and the need for complex 3D
reconstruction processing, making real-time analysis challenging. Basketball,
in particular, represents one of the most difficult scenarios in the MOT field,
as ten players move rapidly and complexly within a confined court space, with
frequent occlusions caused by intense physical contact.
  To address these challenges, this paper constructs BasketLiDAR, the first
multimodal dataset in the sports MOT field that combines LiDAR point clouds
with synchronized multi-view camera footage in a professional basketball
environment, and proposes a novel MOT framework that simultaneously achieves
improved tracking accuracy and reduced computational cost. The BasketLiDAR
dataset contains a total of 4,445 frames and 3,105 player IDs, with fully
synchronized IDs between three LiDAR sensors and three multi-view cameras. We
recorded 5-on-5 and 3-on-3 game data from actual professional basketball
players, providing complete 3D positional information and ID annotations for
each player. Based on this dataset, we developed a novel MOT algorithm that
leverages LiDAR's high-precision 3D spatial information. The proposed method
consists of a real-time tracking pipeline using LiDAR alone and a multimodal
tracking pipeline that fuses LiDAR and camera data. Experimental results
demonstrate that our approach achieves real-time operation, which was difficult
with conventional camera-only methods, while achieving superior tracking
performance even under occlusion conditions. The dataset is available upon
request at: https://sites.google.com/keio.jp/keio-csg/projects/basket-lidar

</details>


### [27] [First RAG, Second SEG: A Training-Free Paradigm for Camouflaged Object Detection](https://arxiv.org/abs/2508.15313)
*Wutao Liu,YiDan Wang,Pan Gao*

Main category: cs.CV

TL;DR: 提出了一种无需训练的镂空物体检测方法RAG-SEG，将问题分两阶段处理：RAG生成粗略掩码，SEG利用SAM模型优化掩码。


<details>
  <summary>Details</summary>
Motivation: 现有COD方法依赖重训练与高计算资源，而基础模型如SAM性能的提升需手动生成高质量提示，成本高效能低。

Method: 提议RAG-SEG框架，将COD划分为RAG生成粗掩码与SEG细化掩码两步，利用无监督聚类构建检索数据库，快速高效生成伪标签并指导SAM生成精确掩码。

Result: 该方法无需传统训练即可达到或超越现有最优方法，并能在普通笔记本上高效运行。

Conclusion: RAG-SEG方法结合了简化性与高性能，为镂空物体检测提供了一种训练免疫的新范式，具备计算高效和实用性。

Abstract: Camouflaged object detection (COD) poses a significant challenge in computer
vision due to the high similarity between objects and their backgrounds.
Existing approaches often rely on heavy training and large computational
resources. While foundation models such as the Segment Anything Model (SAM)
offer strong generalization, they still struggle to handle COD tasks without
fine-tuning and require high-quality prompts to yield good performance.
However, generating such prompts manually is costly and inefficient. To address
these challenges, we propose \textbf{First RAG, Second SEG (RAG-SEG)}, a
training-free paradigm that decouples COD into two stages: Retrieval-Augmented
Generation (RAG) for generating coarse masks as prompts, followed by SAM-based
segmentation (SEG) for refinement. RAG-SEG constructs a compact retrieval
database via unsupervised clustering, enabling fast and effective feature
retrieval. During inference, the retrieved features produce pseudo-labels that
guide precise mask generation using SAM2. Our method eliminates the need for
conventional training while maintaining competitive performance. Extensive
experiments on benchmark COD datasets demonstrate that RAG-SEG performs on par
with or surpasses state-of-the-art methods. Notably, all experiments are
conducted on a \textbf{personal laptop}, highlighting the computational
efficiency and practicality of our approach. We present further analysis in the
Appendix, covering limitations, salient object detection extension, and
possible improvements.

</details>


### [28] [VideoEraser: Concept Erasure in Text-to-Video Diffusion Models](https://arxiv.org/abs/2508.15314)
*Naen Xu,Jinghuai Zhang,Changjiang Li,Zhi Chen,Chunyi Zhou,Qingming Li,Tianyu Du,Shouling Ji*

Main category: cs.CV

TL;DR: 提出VideoEraser框架以防止文本到视频(T2V)扩散模型生成有害或误导性内容。


<details>
  <summary>Details</summary>
Motivation: 针对T2V扩散模型被滥用生成侵害隐私、版权和安全的内容提出解决方案。

Method: 设计了一种无需训练的框架VideoEraser，通过选择性提示嵌入调整 (SPEA) 和抗对抗性噪声引导 (ARNG) 实现。可作为插件模块集成到T2V扩散模型中。

Result: 实验结果表明，VideoEraser可以显著减少T2V生成中的不良内容，比基线方法平均减少46%，并在多方面优于已有方法。

Conclusion: VideoEraser是一种高效且通用的解决方案，可有效抑制T2V扩散模型生成不良内容，具有良好的广泛应用前景。

Abstract: The rapid growth of text-to-video (T2V) diffusion models has raised concerns
about privacy, copyright, and safety due to their potential misuse in
generating harmful or misleading content. These models are often trained on
numerous datasets, including unauthorized personal identities, artistic
creations, and harmful materials, which can lead to uncontrolled production and
distribution of such content. To address this, we propose VideoEraser, a
training-free framework that prevents T2V diffusion models from generating
videos with undesirable concepts, even when explicitly prompted with those
concepts. Designed as a plug-and-play module, VideoEraser can seamlessly
integrate with representative T2V diffusion models via a two-stage process:
Selective Prompt Embedding Adjustment (SPEA) and Adversarial-Resilient Noise
Guidance (ARNG). We conduct extensive evaluations across four tasks, including
object erasure, artistic style erasure, celebrity erasure, and explicit content
erasure. Experimental results show that VideoEraser consistently outperforms
prior methods regarding efficacy, integrity, fidelity, robustness, and
generalizability. Notably, VideoEraser achieves state-of-the-art performance in
suppressing undesirable content during T2V generation, reducing it by 46% on
average across four tasks compared to baselines.

</details>


### [29] [Predicting Road Crossing Behaviour using Pose Detection and Sequence Modelling](https://arxiv.org/abs/2508.15336)
*Subhasis Dasgupta,Preetam Saha,Agniva Roy,Jaydip Sen*

Main category: cs.CV

TL;DR: 使用深度学习模型预测行人过马路意图，表明GRU优于LSTM，但1D CNN性能更优。


<details>
  <summary>Details</summary>
Motivation: 实现自动驾驶车辆的远距离行人意图预测，提升安全性。

Method: 通过视频分析和深度学习框架，将姿态检测和序列建模技术相结合，实验比较了三种序列模型（GRU、LSTM、1D CNN）。

Result: GRU在意图预测表现上优于LSTM，但1D CNN在速度上更优。

Conclusion: 结合姿态检测和序列建模的端到端深度学习系统可有效预测行人过马路意图，其中1D CNN综合表现最佳。

Abstract: The world is constantly moving towards AI based systems and autonomous
vehicles are now reality in different parts of the world. These vehicles
require sensors and cameras to detect objects and maneuver according to that.
It becomes important to for such vehicles to also predict from a distant if a
person is about to cross a road or not. The current study focused on predicting
the intent of crossing the road by pedestrians in an experimental setup. The
study involved working with deep learning models to predict poses and sequence
modelling for temporal predictions. The study analysed three different sequence
modelling to understand the prediction behaviour and it was found out that GRU
was better in predicting the intent compared to LSTM model but 1D CNN was the
best model in terms of speed. The study involved video analysis, and the output
of pose detection model was integrated later on to sequence modelling
techniques for an end-to-end deep learning framework for predicting road
crossing intents.

</details>


### [30] [RCDINO: Enhancing Radar-Camera 3D Object Detection with DINOv2 Semantic Features](https://arxiv.org/abs/2508.15353)
*Olga Matykina,Dmitry Yudin*

Main category: cs.CV

TL;DR: 提出了一种名为RCDINO的多模态变压器模型，用于提升视觉骨干特征，与DINOv2模型语义数据融合实现三维目标检测，性能在nuScenes数据集上表现优秀。


<details>
  <summary>Details</summary>
Motivation: 为了解决自动驾驶和机器人中多模态数据相结合的需求，提升三维目标检测性能，提出了一个新的模型以改进雷达和摄像机信息融合的效果。

Method: 采用了基于多模态的变压器架构，使用预训练的DINOv2基础模型的语义信息增强视觉特征，同时保证与基础架构的兼容性。

Result: 在nuScenes数据集上达到了领先性能，NDS为56.4，mAP为48.1，超过了现有雷达-摄像机模型方法。

Conclusion: 通过多模态特征融合和引入强大的视觉语义信息，RCDINO显著提升了三维目标检测性能，并为领域研究设立了新的标准。

Abstract: Three-dimensional object detection is essential for autonomous driving and
robotics, relying on effective fusion of multimodal data from cameras and
radar. This work proposes RCDINO, a multimodal transformer-based model that
enhances visual backbone features by fusing them with semantically rich
representations from the pretrained DINOv2 foundation model. This approach
enriches visual representations and improves the model's detection performance
while preserving compatibility with the baseline architecture. Experiments on
the nuScenes dataset demonstrate that RCDINO achieves state-of-the-art
performance among radar-camera models, with 56.4 NDS and 48.1 mAP. Our
implementation is available at https://github.com/OlgaMatykina/RCDINO.

</details>


### [31] [An Empirical Study on How Video-LLMs Answer Video Questions](https://arxiv.org/abs/2508.15360)
*Chenhui Gou,Ziyu Ma,Zicheng Duan,Haoyu He,Feng Chen,Akide Liu,Bohan Zhuang,Jianfei Cai,Hamid Rezatofighi*

Main category: cs.CV

TL;DR: 本文针对现有视频大语言模型（Video-LLMs）进行系统化的实证研究，通过设计三种注意力剔除方法分析该模型的内部机制，并揭示Video-LLMs如何处理视频内容，提供了可解释性和效率提升的视角。


<details>
  <summary>Details</summary>
Motivation: 目前的视频大语言模型多集中于提升性能，而忽视了对其内部机制的理解。本论文旨在弥补这一空白，系统性地研究Video-LLMs的内部运作方式及其处理视频内容的方法。

Method: 采用注意力剔除作为主要分析工具，设计了视频时间剔除、视频空间剔除和语言对视频剔除三种变体。在多个层级（层窗口）上进行实验，分别设置全局和细粒度两种情景，对Video-LLMs的内部机制进行详细研究。

Result: 发现了三大关键点：(1) 全局设置显示视频信息主要在早期层中提取，并呈现出感知编码和抽象推理的二阶段过程；(2) 在细粒度设置中，某些中间层对视频问答的贡献超出预期，而大部分层的贡献较小；(3) 无论在何种设置，空间时间建模主要依赖语言指导的检索，而非高计算开销的自注意力机制。

Conclusion: 首次系统揭示了视频大语言模型如何内部处理和理解视频内容，这些理解能为未来研究提供有关模型可解释性和效率优化的方向。

Abstract: Taking advantage of large-scale data and pretrained language models, Video
Large Language Models (Video-LLMs) have shown strong capabilities in answering
video questions. However, most existing efforts focus on improving performance,
with limited attention to understanding their internal mechanisms. This paper
aims to bridge this gap through a systematic empirical study. To interpret
existing VideoLLMs, we adopt attention knockouts as our primary analytical tool
and design three variants: Video Temporal Knockout, Video Spatial Knockout, and
Language-to-Video Knockout. Then, we apply these three knockouts on different
numbers of layers (window of layers). By carefully controlling the window of
layers and types of knockouts, we provide two settings: a global setting and a
fine-grained setting. Our study reveals three key findings: (1) Global setting
indicates Video information extraction primarily occurs in early layers,
forming a clear two-stage process -- lower layers focus on perceptual encoding,
while higher layers handle abstract reasoning; (2) In the fine-grained setting,
certain intermediate layers exert an outsized impact on video question
answering, acting as critical outliers, whereas most other layers contribute
minimally; (3) In both settings, we observe that spatial-temporal modeling
relies more on language-guided retrieval than on intra- and inter-frame
self-attention among video tokens, despite the latter's high computational
cost. Finally, we demonstrate that these insights can be leveraged to reduce
attention computation in Video-LLMs. To our knowledge, this is the first work
to systematically uncover how Video-LLMs internally process and understand
video content, offering interpretability and efficiency perspectives for future
research.

</details>


### [32] [Transfer learning optimization based on evolutionary selective fine tuning](https://arxiv.org/abs/2508.15367)
*Jacinto Colan,Ana Davila,Yasuhisa Hasegawa*

Main category: cs.CV

TL;DR: 本文提出了BioTune，一种选择性微调层的进化自适应技术，用于提高迁移学习效率。


<details>
  <summary>Details</summary>
Motivation: 在图像分析中，深度学习尽管取得了显著进展，但其对计算资源的需求值得关注。而传统的迁移学习方式易导致过拟合和高计算成本。

Method: BioTune采用进化算法，选择性地微调某些特定层，从而提升目标任务的性能。

Result: 通过对9个不同领域的图像分类数据集评估，BioTune在精度和效率上均表现优异，优于现有一些微调方法如AutoRGN和LoRA。

Conclusion: 通过集中微调相关层，BioTune减少了可训练参数数量，降低了计算成本，同时在不同数据特性和分布的迁移学习中表现更为高效。

Abstract: Deep learning has shown substantial progress in image analysis. However, the
computational demands of large, fully trained models remain a consideration.
Transfer learning offers a strategy for adapting pre-trained models to new
tasks. Traditional fine-tuning often involves updating all model parameters,
which can potentially lead to overfitting and higher computational costs. This
paper introduces BioTune, an evolutionary adaptive fine-tuning technique that
selectively fine-tunes layers to enhance transfer learning efficiency. BioTune
employs an evolutionary algorithm to identify a focused set of layers for
fine-tuning, aiming to optimize model performance on a given target task.
Evaluation across nine image classification datasets from various domains
indicates that BioTune achieves competitive or improved accuracy and efficiency
compared to existing fine-tuning methods such as AutoRGN and LoRA. By
concentrating the fine-tuning process on a subset of relevant layers, BioTune
reduces the number of trainable parameters, potentially leading to decreased
computational cost and facilitating more efficient transfer learning across
diverse data characteristics and distributions.

</details>


### [33] [Image-Conditioned 3D Gaussian Splat Quantization](https://arxiv.org/abs/2508.15372)
*Xinshuang Liu,Runfa Blark Li,Keito Suzuki,Truong Nguyen*

Main category: cs.CV

TL;DR: 提出了一种名为ICGS-Quantizer的新方法，显著提升了3D Gaussian Splatting的压缩效率，并能适应场景变化，减小存储需求至千字节范围，同时保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS压缩方法无法达到大规模场景的存档需求且不能适应长期场景变化。

Method: 通过联合利用高斯间和属性间的相关性，并使用共享的全局码本，提出ICGS-Quantizer方法，将场景解码与当前捕获的图像条件结合，实现高效压缩和对场景变化的适应性。

Result: 相比前沿方法，ICGS-Quantizer在压缩效率和对场景变化的适应性方面具有明显的优势，将存储需求降低到千字节范围内，同时保持视觉保真度。

Conclusion: ICGS-Quantizer提升了存储效率并解除了长期存档场景变化适应性的桎梏，其代码、模型和数据将在GitHub上公开。

Abstract: 3D Gaussian Splatting (3DGS) has attracted considerable attention for
enabling high-quality real-time rendering. Although 3DGS compression methods
have been proposed for deployment on storage-constrained devices, two
limitations hinder archival use: (1) they compress medium-scale scenes only to
the megabyte range, which remains impractical for large-scale scenes or
extensive scene collections; and (2) they lack mechanisms to accommodate scene
changes after long-term archival. To address these limitations, we propose an
Image-Conditioned Gaussian Splat Quantizer (ICGS-Quantizer) that substantially
enhances compression efficiency and provides adaptability to scene changes
after archiving. ICGS-Quantizer improves quantization efficiency by jointly
exploiting inter-Gaussian and inter-attribute correlations and by using shared
codebooks across all training scenes, which are then fixed and applied to
previously unseen test scenes, eliminating the overhead of per-scene codebooks.
This approach effectively reduces the storage requirements for 3DGS to the
kilobyte range while preserving visual fidelity. To enable adaptability to
post-archival scene changes, ICGS-Quantizer conditions scene decoding on images
captured at decoding time. The encoding, quantization, and decoding processes
are trained jointly, ensuring that the codes, which are quantized
representations of the scene, are effective for conditional decoding. We
evaluate ICGS-Quantizer on 3D scene compression and 3D scene updating.
Experimental results show that ICGS-Quantizer consistently outperforms
state-of-the-art methods in compression efficiency and adaptability to scene
changes. Our code, model, and data will be publicly available on GitHub.

</details>


### [34] [DriveSplat: Decoupled Driving Scene Reconstruction with Geometry-enhanced Partitioned Neural Gaussians](https://arxiv.org/abs/2508.15376)
*Cong Wang,Xianda Guo,Wenbo Xu,Wei Tian,Ruiqi Song,Chenming Zhang,Lingxi Li,Long Chen*

Main category: cs.CV

TL;DR: DriveSplat提出了一种基于神经高斯表示的动态静态解耦方法，解决了驾驶场景中背景优化不足和几何表现不准确的问题，在Waymo和KITTI数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 针对驾驶场景中快速移动物体和静态背景导致的3D场景重建困难，以及现有方法在背景优化和几何表现上的不足，提出了新方法。

Method: 采用区域划分的体素初始化方案，并通过可变形神经高斯模型结合可学习的变形网络调整动态对象参数，同时引入预训练模型的深度与法线监督，提高几何结构的精确性。

Result: 在Waymo和KITTI数据集上进行了严格评估，证明其在解决驾驶场景中的新视图生成问题上达到了最先进的性能。

Conclusion: DriveSplat能够在动态和静态解耦的同时，实现高质量的几何优化和新视图合成，为驾驶场景3D重建提供了可靠的解决方案。

Abstract: In the realm of driving scenarios, the presence of rapidly moving vehicles,
pedestrians in motion, and large-scale static backgrounds poses significant
challenges for 3D scene reconstruction. Recent methods based on 3D Gaussian
Splatting address the motion blur problem by decoupling dynamic and static
components within the scene. However, these decoupling strategies overlook
background optimization with adequate geometry relationships and rely solely on
fitting each training view by adding Gaussians. Therefore, these models exhibit
limited robustness in rendering novel views and lack an accurate geometric
representation. To address the above issues, we introduce DriveSplat, a
high-quality reconstruction method for driving scenarios based on neural
Gaussian representations with dynamic-static decoupling. To better accommodate
the predominantly linear motion patterns of driving viewpoints, a region-wise
voxel initialization scheme is employed, which partitions the scene into near,
middle, and far regions to enhance close-range detail representation.
Deformable neural Gaussians are introduced to model non-rigid dynamic actors,
whose parameters are temporally adjusted by a learnable deformation network.
The entire framework is further supervised by depth and normal priors from
pre-trained models, improving the accuracy of geometric structures. Our method
has been rigorously evaluated on the Waymo and KITTI datasets, demonstrating
state-of-the-art performance in novel-view synthesis for driving scenarios.

</details>


### [35] [DIO: Refining Mutual Information and Causal Chain to Enhance Machine Abstract Reasoning Ability](https://arxiv.org/abs/2508.15387)
*Ruizhuo Song,Beiming Yuan*

Main category: cs.CV

TL;DR: 该论文研究通过改进深度学习模型来解决Raven进展矩阵（RPM）问题，旨在增强机器在抽象推理领域的能力。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在多个领域表现优异，但其在抽象推理中的能力瓶颈尚未得到解决，因此作者希望通过研究RPM问题提升模型的推理水平。

Method: 作者首先从“因果链建模”视角分析RPM任务，随后提出了DIO模型并发现其优化目标存在不足，针对问题逐步提出三种改进方法。

Result: 实验表明，DIO模型的原目标无法有效捕捉推理逻辑，通过分析和新增改进方法，论文为未来解决抽象推理构建了架构方向。

Conclusion: 本文方法从因果链和逻辑推理角度创新性改进深度学习，增强了机器解RPM问题时的抽象推理能力，为领域发展提供了新思路。

Abstract: Despite the outstanding performance of current deep learning models across
various domains, their fundamental bottleneck in abstract reasoning remains
unresolved. To address this challenge, the academic community has introduced
Raven's Progressive Matrices (RPM) problems as an authoritative benchmark for
evaluating the abstract reasoning capabilities of deep learning algorithms,
with a focus on core intelligence dimensions such as abstract reasoning,
pattern recognition, and complex problem-solving. Therefore, this paper centers
on solving RPM problems, aiming to contribute to enhancing the abstract
reasoning abilities of machine intelligence. Firstly, this paper adopts a
``causal chain modeling'' perspective to systematically analyze the complete
causal chain in RPM tasks: image $\rightarrow$ abstract attributes
$\rightarrow$ progressive attribute patterns $\rightarrow$ pattern consistency
$\rightarrow$ correct answer. Based on this analysis, the network architecture
of the baseline model DIO is designed. However, experiments reveal that the
optimization objective formulated for DIO, namely maximizing the variational
lower bound of mutual information between the context and the correct option,
fails to enable the model to genuinely acquire the predefined human reasoning
logic. This is attributed to two main reasons: the tightness of the lower bound
significantly impacts the effectiveness of mutual information maximization, and
mutual information, as a statistical measure, does not capture the causal
relationship between subjects and objects. To overcome these limitations, this
paper progressively proposes three improvement methods:

</details>


### [36] [Spiking Variational Graph Representation Inference for Video Summarization](https://arxiv.org/abs/2508.15389)
*Wenrui Li,Wei Han,Liang-Jian Deng,Ruiqin Xiong,Xiaopeng Fan*

Main category: cs.CV

TL;DR: 提出了一种名为Spiking Variational Graph (SpiVG) Network的视频摘要方法，结合SNN、动态图推理和变分推断，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有视频摘要方法中无法捕捉全局时间依赖性、保持语义一致性，以及多通道特征融合中受到噪声干扰的问题。

Method: 设计了基于SNN的关键帧提取器，动态聚合图推理器，用于细粒度语义一致性推理，并通过变分推断重构模块解决多通道特征融合中的不确定性和噪声。

Result: SpiVG在SumMe、TVSum、VideoXum、QFVS等多数据集上表现优于现有方法。

Conclusion: 提出的新方法在视频摘要领域展示了更高的信息密度和较低的计算复杂性，且实验证明具备更优性能。

Abstract: With the rise of short video content, efficient video summarization
techniques for extracting key information have become crucial. However,
existing methods struggle to capture the global temporal dependencies and
maintain the semantic coherence of video content. Additionally, these methods
are also influenced by noise during multi-channel feature fusion. We propose a
Spiking Variational Graph (SpiVG) Network, which enhances information density
and reduces computational complexity. First, we design a keyframe extractor
based on Spiking Neural Networks (SNN), leveraging the event-driven computation
mechanism of SNNs to learn keyframe features autonomously. To enable
fine-grained and adaptable reasoning across video frames, we introduce a
Dynamic Aggregation Graph Reasoner, which decouples contextual object
consistency from semantic perspective coherence. We present a Variational
Inference Reconstruction Module to address uncertainty and noise arising during
multi-channel feature fusion. In this module, we employ Evidence Lower Bound
Optimization (ELBO) to capture the latent structure of multi-channel feature
distributions, using posterior distribution regularization to reduce
overfitting. Experimental results show that SpiVG surpasses existing methods
across multiple datasets such as SumMe, TVSum, VideoXum, and QFVS. Our codes
and pre-trained models are available at https://github.com/liwrui/SpiVG.

</details>


### [37] [From Linearity to Non-Linearity: How Masked Autoencoders Capture Spatial Correlations](https://arxiv.org/abs/2508.15404)
*Anthony Bisulco,Rahul Ramesh,Randall Balestriero,Pratik Chaudhari*

Main category: cs.CV

TL;DR: 本文分析了MAE在预训练视觉模型中的表现，尤其关注超参数如何影响下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 探索MAE超参数如何影响模型对输入图像空间相关性的学习。

Method: 以线性和非线性MAE为研究对象，通过解析和理论得出其对短程与长程空间相关性的捕捉能力，并提供超参数选择的建议。

Result: 线性MAE的掩码比例和分块大小可以控制捕捉短程和长程的空间相关性；非线性MAE则超越了二阶统计学，能够适应数据集的空间相关性。

Conclusion: 论文为MAE超参数的选择提供了理论依据，并深入解释了不同配置下的性能特点。

Abstract: Masked Autoencoders (MAEs) have emerged as a powerful pretraining technique
for vision foundation models. Despite their effectiveness, they require
extensive hyperparameter tuning (masking ratio, patch size, encoder/decoder
layers) when applied to novel datasets. While prior theoretical works have
analyzed MAEs in terms of their attention patterns and hierarchical latent
variable models, the connection between MAE hyperparameters and performance on
downstream tasks is relatively unexplored. This work investigates how MAEs
learn spatial correlations in the input image. We analytically derive the
features learned by a linear MAE and show that masking ratio and patch size can
be used to select for features that capture short- and long-range spatial
correlations. We extend this analysis to non-linear MAEs to show that MAE
representations adapt to spatial correlations in the dataset, beyond
second-order statistics. Finally, we discuss some insights on how to select MAE
hyper-parameters in practice.

</details>


### [38] [Bidirectional Temporal Information Propagation for Moving Infrared Small Target Detection](https://arxiv.org/abs/2508.15415)
*Dengyan Luo,Yanping Xiang,Hu Wang,Luping Ji. Shuai Li,Mao Ye*

Main category: cs.CV

TL;DR: 提出了一种名为BIRD的红外小目标检测方法，通过双向时间信息传播策略结合局部与全局时间信息，提升目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于滑动窗口的多帧目标检测方法忽略了视频全局时间信息的优化，导致计算冗余和性能不足。

Method: 设计了局部时间运动融合（LTMF）模块和全局时间运动融合（GTMF）模块，利用递归的双向传播策略结合局部邻帧和全局时间信息，并通过检测损失和额外的时空融合损失实现全视频片段的联合优化。

Result: 实验表明，BIRD方法在性能和推理速度上达到了最新的研究水平。

Conclusion: BIRD方法有效结合了局部与全局时间特性，优化了红外小目标检测的整体性能。

Abstract: Moving infrared small target detection is broadly adopted in infrared search
and track systems, and has attracted considerable research focus in recent
years. The existing learning-based multi-frame methods mainly aggregate the
information of adjacent frames in a sliding window fashion to assist the
detection of the current frame. However, the sliding-window-based methods do
not consider joint optimization of the entire video clip and ignore the global
temporal information outside the sliding window, resulting in redundant
computation and sub-optimal performance. In this paper, we propose a
Bidirectional temporal information propagation method for moving InfraRed small
target Detection, dubbed BIRD. The bidirectional propagation strategy
simultaneously utilizes local temporal information of adjacent frames and
global temporal information of past and future frames in a recursive fashion.
Specifically, in the forward and backward propagation branches, we first design
a Local Temporal Motion Fusion (LTMF) module to model local spatio-temporal
dependency between a target frame and its two adjacent frames. Then, a Global
Temporal Motion Fusion (GTMF) module is developed to further aggregate the
global propagation feature with the local fusion feature. Finally, the
bidirectional aggregated features are fused and input into the detection head
for detection. In addition, the entire video clip is jointly optimized by the
traditional detection loss and the additional Spatio-Temporal Fusion (STF)
loss. Extensive experiments demonstrate that the proposed BIRD method not only
achieves the state-of-the-art performance but also shows a fast inference
speed.

</details>


### [39] [The Impact of Image Resolution on Face Detection: A Comparative Analysis of MTCNN, YOLOv XI and YOLOv XII models](https://arxiv.org/abs/2507.23341)
*Ahmet Can Ömercikoğlu,Mustafa Mansur Yönügül,Pakize Erdoğmuş*

Main category: cs.CV

TL;DR: 这篇论文分析了分辨率对三种深度学习人脸检测模型（YOLOv11, YOLOv12, MTCNN）的影响，结果显示YOLOv11在高分辨率上表现最好。


<details>
  <summary>Details</summary>
Motivation: 探讨图像分辨率对人脸检测模型性能的影响及其在实际应用中的意义。

Method: 基于WIDER FACE数据集，使用不同分辨率图像（160x160, 320x320, 640x640）对三种人脸检测模型的精度、召回率、mAP等评估指标进行了全面实验分析。

Result: YOLOv11在检测精度上胜出，特别在高分辨率情况下表现优秀。YOLOv12在召回率稍高，而MTCNN在实时推理速度上有所不足。

Conclusion: 对于不同的操作需求条件，可以根据图像分辨率选择最合适的人脸检测模型。

Abstract: Face detection is a crucial component in many AI-driven applications such as
surveillance, biometric authentication, and human-computer interaction.
However, real-world conditions like low-resolution imagery present significant
challenges that degrade detection performance. In this study, we systematically
investigate the impact of input resolution on the accuracy and robustness of
three prominent deep learning-based face detectors: YOLOv11, YOLOv12, and
MTCNN. Using the WIDER FACE dataset, we conduct extensive evaluations across
multiple image resolutions (160x160, 320x320, and 640x640) and assess each
model's performance using metrics such as precision, recall, mAP50, mAP50-95,
and inference time. Results indicate that YOLOv11 outperforms YOLOv12 and MTCNN
in terms of detection accuracy, especially at higher resolutions, while YOLOv12
exhibits slightly better recall. MTCNN, although competitive in landmark
localization, lags in real-time inference speed. Our findings provide
actionable insights for selecting resolution-aware face detection models
suitable for varying operational constraints.

</details>


### [40] [A Curated Dataset and Deep Learning Approach for Minor Dent Detection in Vehicles](https://arxiv.org/abs/2508.15431)
*Danish Zia Baig,Mohsin Kamal*

Main category: cs.CV

TL;DR: 论文提出一种基于YOLOv8框架的深度学习方法，用于检测车辆外表的微小损伤，如细小凹痕。


<details>
  <summary>Details</summary>
Motivation: 现有的车辆损伤检测方法费时、人工化且难以发现细小缺陷，迫切需要更加快捷精确的检测技术。

Method: 通过建立特定数据集并结合YOLOv8m及其改进版本模型（YOLOv8m-t4和YOLOv8m-t42），使用实时数据增强技术进行训练，开发微小缺陷检测方法。

Result: 实验表明YOLOv8m-t42模型具有更高检测精度（Precision: 0.86, Recall: 0.84, F1-Score: 0.85），同时在其他评估指标上也优于YOLOv8m-t4。

Conclusion: YOLOv8m-t42凭借更高的精度与更稳定的性能更适用于现实车辆微小凹痕检测任务，尽管其收敛速度较慢。

Abstract: Conventional car damage inspection techniques are labor-intensive, manual,
and frequently overlook tiny surface imperfections like microscopic dents.
Machine learning provides an innovative solution to the increasing demand for
quicker and more precise inspection methods. The paper uses the YOLOv8 object
recognition framework to provide a deep learning-based solution for
automatically detecting microscopic surface flaws, notably tiny dents, on car
exteriors. Traditional automotive damage inspection procedures are manual,
time-consuming, and frequently unreliable at detecting tiny flaws. To solve
this, a bespoke dataset containing annotated photos of car surfaces under
various lighting circumstances, angles, and textures was created. To improve
robustness, the YOLOv8m model and its customized variants, YOLOv8m-t4 and
YOLOv8m-t42, were trained employing real-time data augmentation approaches.
Experimental results show that the technique has excellent detection accuracy
and low inference latency, making it suited for real-time applications such as
automated insurance evaluations and automobile inspections. Evaluation
parameters such as mean Average Precision (mAP), precision, recall, and
F1-score verified the model's efficacy. With a precision of 0.86, recall of
0.84, and F1-score of 0.85, the YOLOv8m-t42 model outperformed the YOLOv8m-t4
model (precision: 0.81, recall: 0.79, F1-score: 0.80) in identifying
microscopic surface defects. With a little reduced mAP@0.5:0.95 of 0.20, the
mAP@0.5 for YOLOv8m-t42 stabilized at 0.60. Furthermore, YOLOv8m-t42's PR curve
area was 0.88, suggesting more consistent performance than YOLOv8m-t4 (0.82).
YOLOv8m-t42 has greater accuracy and is more appropriate for practical dent
detection applications, even though its convergence is slower.

</details>


### [41] [Aligning Moments in Time using Video Queries](https://arxiv.org/abs/2508.15439)
*Yogesh Kumar,Uday Agarwal,Manish Gupta,Anand Mishra*

Main category: cs.CV

TL;DR: 本文提出了MATR（Moment Alignment TRansformer）模型，用于在视频对中精确地定位语义匹配的时刻，较现有方法在多个数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 希望解决Vid2VidMR任务中的语义帧对齐和复杂依赖建模问题，改善目标视频中时刻定位的精确度。

Method: 提出了基于Transformer的模型MATR，使用双阶段序列对齐机制进行跨视频的语义信息捕获，并通过前景/背景分类和边界预测头实现精准的时刻定位。此外，引入了自监督预训练技术以提升模型初始化效果。

Result: 在ActivityNet-VRL和新提出的SportsMoments数据集上，MATR分别在R@1和mIoU指标上超越现有方法，取得显著性能提升（例如在ActivityNet-VRL上提升13.1% R@1和8.1% mIoU）。

Conclusion: MATR通过语义对齐和时间细节建模实现了视频对时刻检索任务中的显著进步，为相关任务提供了新方向。

Abstract: Video-to-video moment retrieval (Vid2VidMR) is the task of localizing unseen
events or moments in a target video using a query video. This task poses
several challenges, such as the need for semantic frame-level alignment and
modeling complex dependencies between query and target videos. To tackle this
challenging problem, we introduce MATR (Moment Alignment TRansformer), a
transformer-based model designed to capture semantic context as well as the
temporal details necessary for precise moment localization. MATR conditions
target video representations on query video features using dual-stage sequence
alignment that encodes the required correlations and dependencies. These
representations are then used to guide foreground/background classification and
boundary prediction heads, enabling the model to accurately identify moments in
the target video that semantically match with the query video. Additionally, to
provide a strong task-specific initialization for MATR, we propose a
self-supervised pre-training technique that involves training the model to
localize random clips within videos. Extensive experiments demonstrate that
MATR achieves notable performance improvements of 13.1% in R@1 and 8.1% in mIoU
on an absolute scale compared to state-of-the-art methods on the popular
ActivityNet-VRL dataset. Additionally, on our newly proposed dataset,
SportsMoments, MATR shows a 14.7% gain in R@1 and a 14.4% gain in mIoU on an
absolute scale over strong baselines.

</details>


### [42] [Enhancing Novel View Synthesis from extremely sparse views with SfM-free 3D Gaussian Splatting Framework](https://arxiv.org/abs/2508.15457)
*Zongqi He,Hanmin Li,Kin-Chung Chan,Yushen Zuo,Hao Xie,Zhe Xiao,Jun Xiao,Kin-Man Lam*

Main category: cs.CV

TL;DR: 该论文提出一种无需结构光方法(SfM)的3D Gaussian Splatting新方法，通过联合估计相机姿态和三维场景，实现了从极稀少视角输入的高质量新视图合成。


<details>
  <summary>Details</summary>
Motivation: 现有的3D Gaussian Splatting方法在极稀少视图场景中表现欠佳，因其强依赖已有相机姿态和多视角输入数据。

Method: 该方法采用密集立体模块替代SfM以估计相机姿态，并设计了连贯视图插值模块生成一致性视图，从而缓解稀少视图数据下的信息短缺问题。还引入多尺度一致性和几何正则化以提高几何结构及渲染质量。

Result: 实验表明，该方法在极稀少视图条件（仅2个训练视图）下较现有方法提升了2.75dB PSNR，生成图像失真小、细节丰富，视觉质量卓越。

Conclusion: 该方法摆脱了对SfM的依赖，在稀少输入条件下实现了优异的新视图合成效果，并显著优于当前的最先进方法。

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated remarkable real-time
performance in novel view synthesis, yet its effectiveness relies heavily on
dense multi-view inputs with precisely known camera poses, which are rarely
available in real-world scenarios. When input views become extremely sparse,
the Structure-from-Motion (SfM) method that 3DGS depends on for initialization
fails to accurately reconstruct the 3D geometric structures of scenes,
resulting in degraded rendering quality. In this paper, we propose a novel
SfM-free 3DGS-based method that jointly estimates camera poses and reconstructs
3D scenes from extremely sparse-view inputs. Specifically, instead of SfM, we
propose a dense stereo module to progressively estimates camera pose
information and reconstructs a global dense point cloud for initialization. To
address the inherent problem of information scarcity in extremely sparse-view
settings, we propose a coherent view interpolation module that interpolates
camera poses based on training view pairs and generates viewpoint-consistent
content as additional supervision signals for training. Furthermore, we
introduce multi-scale Laplacian consistent regularization and adaptive
spatial-aware multi-scale geometry regularization to enhance the quality of
geometrical structures and rendered content. Experiments show that our method
significantly outperforms other state-of-the-art 3DGS-based approaches,
achieving a remarkable 2.75dB improvement in PSNR under extremely sparse-view
conditions (using only 2 training views). The images synthesized by our method
exhibit minimal distortion while preserving rich high-frequency details,
resulting in superior visual quality compared to existing techniques.

</details>


### [43] [LGMSNet: Thinning a medical image segmentation model via dual-level multiscale fusion](https://arxiv.org/abs/2508.15476)
*Chengqi Dong,Fenghe Tang,Rongge Mao,Xinpei Gao,S. Kevin Zhou*

Main category: cs.CV

TL;DR: LGMSNet是一种轻量级的医学图像分割模型，结合局部和全局多尺度方法，在多数据集测试中的表现优于现有模型，尤其在资源受限环境中具有潜力。


<details>
  <summary>Details</summary>
Motivation: 当前医学图像分割中轻量级模型虽然高效但常忽略注意力机制，缺乏全局上下文感知，并受限于通道冗余问题，影响特征提取效果。研究旨在解决这些问题，为资源有限的场景提供有效方案。

Method: 提出了LGMSNet框架，通过异构层内卷积核提取局部高频信息并减轻通道冗余，结合稀疏变换-卷积混合分支，捕捉低频全局信息，保持计算开销最低化。

Result: 在六个公共数据集上，LGMSNet的性能超过现有模型，特别是在四个未见数据集上的零样本泛化测试中展示出卓越性能。

Conclusion: LGMSNet具有极强的性能和泛化能力，在资源有限的医学场景中有实际应用潜力，且代码已开源以促进相关研究。

Abstract: Medical image segmentation plays a pivotal role in disease diagnosis and
treatment planning, particularly in resource-constrained clinical settings
where lightweight and generalizable models are urgently needed. However,
existing lightweight models often compromise performance for efficiency and
rarely adopt computationally expensive attention mechanisms, severely
restricting their global contextual perception capabilities. Additionally,
current architectures neglect the channel redundancy issue under the same
convolutional kernels in medical imaging, which hinders effective feature
extraction. To address these challenges, we propose LGMSNet, a novel
lightweight framework based on local and global dual multiscale that achieves
state-of-the-art performance with minimal computational overhead. LGMSNet
employs heterogeneous intra-layer kernels to extract local high-frequency
information while mitigating channel redundancy. In addition, the model
integrates sparse transformer-convolutional hybrid branches to capture
low-frequency global information. Extensive experiments across six public
datasets demonstrate LGMSNet's superiority over existing state-of-the-art
methods. In particular, LGMSNet maintains exceptional performance in zero-shot
generalization tests on four unseen datasets, underscoring its potential for
real-world deployment in resource-limited medical scenarios. The whole project
code is in https://github.com/cq-dong/LGMSNet.

</details>


### [44] [MExECON: Multi-view Extended Explicit Clothed humans Optimized via Normal integration](https://arxiv.org/abs/2508.15500)
*Fulden Ece Uğur,Rafael Redondo,Albert Barreiro,Stefan Hristov,Roger Marí*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法MExECON，用于通过稀疏的多视角RGB图像来进行穿着服装的3D人体重建，并改进了几何和姿态估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决在稀疏多视角图像下3D人体重建中细节捕捉不足的问题。

Method: 提出了JMBO算法，通过在所有视图中联合拟合单个SMPL-X体模，同时保证多视图的一致性；进一步通过法线贴图整合捕获细节。

Result: MExECON在实验中比单视图方法更具保真性，同时达到了与现代少样本3D重建方法的竞争性表现。

Conclusion: 该方法在无需重新训练网络的情况下，成功提升了多视角下3D人体几何和细节建模的准确性。

Abstract: This work presents MExECON, a novel pipeline for 3D reconstruction of clothed
human avatars from sparse multi-view RGB images. Building on the single-view
method ECON, MExECON extends its capabilities to leverage multiple viewpoints,
improving geometry and body pose estimation. At the core of the pipeline is the
proposed Joint Multi-view Body Optimization (JMBO) algorithm, which fits a
single SMPL-X body model jointly across all input views, enforcing multi-view
consistency. The optimized body model serves as a low-frequency prior that
guides the subsequent surface reconstruction, where geometric details are added
via normal map integration. MExECON integrates normal maps from both front and
back views to accurately capture fine-grained surface details such as clothing
folds and hairstyles. All multi-view gains are achieved without requiring any
network re-training. Experimental results show that MExECON consistently
improves fidelity over the single-view baseline and achieves competitive
performance compared to modern few-shot 3D reconstruction methods.

</details>


### [45] [Task-Generalized Adaptive Cross-Domain Learning for Multimodal Image Fusion](https://arxiv.org/abs/2508.15505)
*Mengyu Wang,Zhenyu Liu,Kun Li,Yu Wang,Yuwei Wang,Yanyan Wei,Fei Wang*

Main category: cs.CV

TL;DR: 该论文提出了AdaSFFuse框架，通过自适应跨域联合学习，实现任务通用型多模态图像融合（MMIF）。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有MMIF方法中存在的模态未对准、高频细节损失和任务特定限制等问题。

Method: 提出了两项关键创新：1）自适应近似小波变换（AdaWAT），用于频率解耦和模态特征的对齐；2）空间频率Mamba块，用于高效的多模态空间与频率协同融合。

Result: 在包括IVF、MFF、MEF和MIF在内的四项MMIF任务上，实验证明AdaSFFuse在保证低计算成本与紧凑网络的同时，显著提高了融合性能。

Conclusion: AdaSFFuse在多模态图像融合中，能够提升模态特征的对齐与整合、减少频率损失、保留关键细节，提供了一种性能与效率兼顾的高效框架。

Abstract: Multimodal Image Fusion (MMIF) aims to integrate complementary information
from different imaging modalities to overcome the limitations of individual
sensors. It enhances image quality and facilitates downstream applications such
as remote sensing, medical diagnostics, and robotics. Despite significant
advancements, current MMIF methods still face challenges such as modality
misalignment, high-frequency detail destruction, and task-specific limitations.
To address these challenges, we propose AdaSFFuse, a novel framework for
task-generalized MMIF through adaptive cross-domain co-fusion learning.
AdaSFFuse introduces two key innovations: the Adaptive Approximate Wavelet
Transform (AdaWAT) for frequency decoupling, and the Spatial-Frequency Mamba
Blocks for efficient multimodal fusion. AdaWAT adaptively separates the high-
and low-frequency components of multimodal images from different scenes,
enabling fine-grained extraction and alignment of distinct frequency
characteristics for each modality. The Spatial-Frequency Mamba Blocks
facilitate cross-domain fusion in both spatial and frequency domains, enhancing
this process. These blocks dynamically adjust through learnable mappings to
ensure robust fusion across diverse modalities. By combining these components,
AdaSFFuse improves the alignment and integration of multimodal features,
reduces frequency loss, and preserves critical details. Extensive experiments
on four MMIF tasks -- Infrared-Visible Image Fusion (IVF), Multi-Focus Image
Fusion (MFF), Multi-Exposure Image Fusion (MEF), and Medical Image Fusion (MIF)
-- demonstrate AdaSFFuse's superior fusion performance, ensuring both low
computational cost and a compact network, offering a strong balance between
performance and efficiency. The code will be publicly available at
https://github.com/Zhen-yu-Liu/AdaSFFuse.

</details>


### [46] [ExtraGS: Geometric-Aware Trajectory Extrapolation with Uncertainty-Guided Generative Priors](https://arxiv.org/abs/2508.15529)
*Kaiyuan Tan,Yingying Shen,Haohui Zhu,Zhiwei Zhan,Shan Zhao,Mingfei Tu,Hongcheng Luo,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye*

Main category: cs.CV

TL;DR: ExtraGS是一种整合几何与生成式先验的轨迹外推框架，提升了自主驾驶场景模拟中的视图外推效果。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前方法中几何一致性差和渲染过于平滑问题，提出了一种整合几何和生成式先验的综合方法。

Method: 核心方法包括基于高斯-符号距离函数设计的道路表面表征、适用远距离物体的远场高斯表示以及基于球面调和函数的不确定性评估框架。

Result: 在多个数据集、多摄像头设置和不同生成式先验下展示了显著提升的外推视图的真实性与几何一致性，同时保障了高原始轨迹的高保真度。

Conclusion: ExtraGS通过整合几何与生成式先验，解决了当前视图外推方法中的一致性和真实性问题，验证了其在各种场景和设置中的通用性与有效性。

Abstract: Synthesizing extrapolated views from recorded driving logs is critical for
simulating driving scenes for autonomous driving vehicles, yet it remains a
challenging task. Recent methods leverage generative priors as pseudo ground
truth, but often lead to poor geometric consistency and over-smoothed
renderings. To address these limitations, we propose ExtraGS, a holistic
framework for trajectory extrapolation that integrates both geometric and
generative priors. At the core of ExtraGS is a novel Road Surface Gaussian(RSG)
representation based on a hybrid Gaussian-Signed Distance Function (SDF)
design, and Far Field Gaussians (FFG) that use learnable scaling factors to
efficiently handle distant objects. Furthermore, we develop a self-supervised
uncertainty estimation framework based on spherical harmonics that enables
selective integration of generative priors only where extrapolation artifacts
occur. Extensive experiments on multiple datasets, diverse multi-camera setups,
and various generative priors demonstrate that ExtraGS significantly enhances
the realism and geometric consistency of extrapolated views, while preserving
high fidelity along the original trajectory.

</details>


### [47] [Multi-Object Sketch Animation with Grouping and Motion Trajectory Priors](https://arxiv.org/abs/2508.15535)
*Guotao Liang,Juncheng Hu,Ximing Xing,Jing Zhang,Qian Yu*

Main category: cs.CV

TL;DR: 提出了一种名为GroupSketch的新方法，通过处理多对象交互和复杂动画生成，高效实现矢量素描动画。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多对象与复杂运动场景时存在局限性，包括限制单对象案例或时间不一致及泛化能力差的问题。

Method: 采用两阶段流程：第一阶段通过交互划分输入素描为语义组并生成粗略动画；第二阶段使用Group-based Displacement Network (GDN)预测组特定的位移场并整合文本生成视频模型的先验知识进行动画细化。

Result: 实验结果表明，该方法在生成复杂多对象素描高质量且时间一致的动画方面大幅优于现有方法。

Conclusion: 方法扩展了素描动画的实际应用领域，并提升了多对象与复杂运动场景下动画生成的质量与一致性。

Abstract: We introduce GroupSketch, a novel method for vector sketch animation that
effectively handles multi-object interactions and complex motions. Existing
approaches struggle with these scenarios, either being limited to single-object
cases or suffering from temporal inconsistency and poor generalization. To
address these limitations, our method adopts a two-stage pipeline comprising
Motion Initialization and Motion Refinement. In the first stage, the input
sketch is interactively divided into semantic groups and key frames are
defined, enabling the generation of a coarse animation via interpolation. In
the second stage, we propose a Group-based Displacement Network (GDN), which
refines the coarse animation by predicting group-specific displacement fields,
leveraging priors from a text-to-video model. GDN further incorporates
specialized modules, such as Context-conditioned Feature Enhancement (CCFE), to
improve temporal consistency. Extensive experiments demonstrate that our
approach significantly outperforms existing methods in generating high-quality,
temporally consistent animations for complex, multi-object sketches, thus
expanding the practical applications of sketch animation.

</details>


### [48] [D3FNet: A Differential Attention Fusion Network for Fine-Grained Road Structure Extraction in Remote Perception Systems](https://arxiv.org/abs/2508.15537)
*Chang Liu,Yang Xu,Tamas Sziranyi*

Main category: cs.CV

TL;DR: D3FNet是一个设计用于高分辨率遥感图像中细颗粒路结构分割的网络，通过差分注意力扩张提取模块（DADE）、双流解码融合机制（DDFM）和多尺度扩张策略，解决了窄道路宽度、断裂拓扑和遮挡等问题，实验表明其性能优于当前的先进模型。


<details>
  <summary>Details</summary>
Motivation: 从遥感图像中提取窄道路由于其有限宽度、碎片化拓扑和遮挡问题而具有挑战性，因此需要一种新方法来改善窄道路的分割性能。

Method: 提出基于D-LinkNet改进的D3FNet网络，采用差分注意力扩张提取模块（DADE）增强道路特征、双流解码融合机制（DDFM）平衡空间精度与语义上下文，并引入多尺度扩张策略以减少栅格伪影和提高预测连续性。

Result: 在DeepGlobe和CHN6-CUG基准数据集上，D3FNet在窄道路区域的IoU和召回率方面超越了最先进的基线模型，且消融实验验证了注意力引导编码和双路径解码的协同效应。

Conclusion: D3FNet是一种鲁棒的解决方案，针对复杂遥感和协同感知场景下的窄道路提取，特别是在细颗粒、遮挡和低对比度路段的表现上有显著提升。

Abstract: Extracting narrow roads from high-resolution remote sensing imagery remains a
significant challenge due to their limited width, fragmented topology, and
frequent occlusions. To address these issues, we propose D3FNet, a Dilated
Dual-Stream Differential Attention Fusion Network designed for fine-grained
road structure segmentation in remote perception systems. Built upon the
encoder-decoder backbone of D-LinkNet, D3FNet introduces three key
innovations:(1) a Differential Attention Dilation Extraction (DADE) module that
enhances subtle road features while suppressing background noise at the
bottleneck; (2) a Dual-stream Decoding Fusion Mechanism (DDFM) that integrates
original and attention-modulated features to balance spatial precision with
semantic context; and (3) a multi-scale dilation strategy (rates 1, 3, 5, 9)
that mitigates gridding artifacts and improves continuity in narrow road
prediction. Unlike conventional models that overfit to generic road widths,
D3FNet specifically targets fine-grained, occluded, and low-contrast road
segments. Extensive experiments on the DeepGlobe and CHN6-CUG benchmarks show
that D3FNet achieves superior IoU and recall on challenging road regions,
outperforming state-of-the-art baselines. Ablation studies further verify the
complementary synergy of attention-guided encoding and dual-path decoding.
These results confirm D3FNet as a robust solution for fine-grained narrow road
extraction in complex remote and cooperative perception scenarios.

</details>


### [49] [Backpropagation-Free Test-Time Adaptation via Probabilistic Gaussian Alignment](https://arxiv.org/abs/2508.15568)
*Youjia Zhang,Youngeun Kim,Young-Geun Choi,Hongyeob Kim,Huiling Liu,Sungeun Hong*

Main category: cs.CV

TL;DR: 提出了一种名为ADAPT的测试时适应方法，该方法旨在解决现有TTA方法的可扩展性和分类边界鲁棒性问题，在无源数据、无梯度更新的情况下实现分布感知、训练无关的推理。


<details>
  <summary>Details</summary>
Motivation: 当前TTA方法在分布迁移下的零样本鲁棒性存在限制，主要在于依赖反向传播、缺乏对类条件特征分布的显式建模，限制了实时性和边界可靠性。

Method: 通过将TTA重构为高斯概率推理任务，使用逐步更新的类均值和共享协方差矩阵进行建模，实现无训练、闭式推理；并引入基于CLIP先验和历史知识库的轻量正则化以修正潜在偏差。

Result: 在各种基准测试中显示了ADAPT方法的先进性能，在多种分布迁移情况下具备卓越的可扩展性和鲁棒性。

Conclusion: ADAPT方法无需源数据、梯度更新或全面目标数据访问，即可在实时和转导算设置下实现领先性能，这是提高分布迁移鲁棒性的一种全新途径。

Abstract: Test-time adaptation (TTA) enhances the zero-shot robustness under
distribution shifts by leveraging unlabeled test data during inference. Despite
notable advances, several challenges still limit its broader applicability.
First, most methods rely on backpropagation or iterative optimization, which
limits scalability and hinders real-time deployment. Second, they lack explicit
modeling of class-conditional feature distributions. This modeling is crucial
for producing reliable decision boundaries and calibrated predictions, but it
remains underexplored due to the lack of both source data and supervision at
test time. In this paper, we propose ADAPT, an Advanced Distribution-Aware and
backPropagation-free Test-time adaptation method. We reframe TTA as a Gaussian
probabilistic inference task by modeling class-conditional likelihoods using
gradually updated class means and a shared covariance matrix. This enables
closed-form, training-free inference. To correct potential likelihood bias, we
introduce lightweight regularization guided by CLIP priors and a historical
knowledge bank. ADAPT requires no source data, no gradient updates, and no full
access to target data, supporting both online and transductive settings.
Extensive experiments across diverse benchmarks demonstrate that our method
achieves state-of-the-art performance under a wide range of distribution shifts
with superior scalability and robustness.

</details>


### [50] [High-Frequency First: A Two-Stage Approach for Improving Image INR](https://arxiv.org/abs/2508.15582)
*Sumit Kumar Dam,Mrityunjoy Gain,Eui-Nam Huh,Choong Seon Hong*

Main category: cs.CV

TL;DR: 提出了一种通过邻域感知的软遮罩方法，引导显式神经表示(INR)模型的训练过程，改进图像高频细节捕获的性能。


<details>
  <summary>Details</summary>
Motivation: INR模型因其频谱偏差问题，难以捕捉图像中的高频细节（如边缘和纹理），需要新的方法来克服这一挑战。

Method: 提出了一个两阶段的训练策略：首先使用邻域感知的软遮罩，对局部变化强的像素赋予更高权重；然后，模型逐步过渡到对整个图像的训练。

Result: 实验证明，该方法可以持续提升重建质量，并与现有INR方法互补，显著改进捕捉高频细节的能力。

Conclusion: 通过为像素赋予频率感知的重要性，这一方法为缓解INR模型的频谱偏差问题提供了新的可能和方向。

Abstract: Implicit Neural Representations (INRs) have emerged as a powerful alternative
to traditional pixel-based formats by modeling images as continuous functions
over spatial coordinates. A key challenge, however, lies in the spectral bias
of neural networks, which tend to favor low-frequency components while
struggling to capture high-frequency (HF) details such as sharp edges and fine
textures. While prior approaches have addressed this limitation through
architectural modifications or specialized activation functions, we propose an
orthogonal direction by directly guiding the training process. Specifically, we
introduce a two-stage training strategy where a neighbor-aware soft mask
adaptively assigns higher weights to pixels with strong local variations,
encouraging early focus on fine details. The model then transitions to
full-image training. Experimental results show that our approach consistently
improves reconstruction quality and complements existing INR methods. As a
pioneering attempt to assign frequency-aware importance to pixels in image INR,
our work offers a new avenue for mitigating the spectral bias problem.

</details>


### [51] [Fast globally optimal Truncated Least Squares point cloud registration with fixed rotation axis](https://arxiv.org/abs/2508.15613)
*Ivo Ivanov,Carsten Markgraf*

Main category: cs.CV

TL;DR: 本文提出了一种新的线性时间凸松弛方法及收缩方法，用于加速结合方向的点云配准算法，在仅需轴旋转信息时，其全局最优解时间显著快于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有点云配准算法在高比例异常值情况下（高达95%）仍存在对全局最优解求解速度慢的问题（例如现有SDP松弛方法对100点需几百秒）。

Method: 提出一种新的线性时间凸松弛方法及收缩方法，并用于加速分支定界（BnB）算法，使其在提供旋转轴信息的情况下能高效完成点云配准。

Result: 实验表明，本文方法可在小于0.5秒内对包含100点的点云进行全局优化，比现有最优SDP求解方法快两个数量级，且在存在对抗性实例时亦表现出显著的全局性。

Conclusion: 提出的方法在仅处理旋转问题时显著快于现有方法，为点云配准提供了一种高效的全局最优解求解方式，但目前尚不支持全6自由度问题。

Abstract: Recent results showed that point cloud registration with given
correspondences can be made robust to outlier rates of up to 95\% using the
truncated least squares (TLS) formulation. However, solving this combinatorial
optimization problem to global optimality is challenging. Provably globally
optimal approaches using semidefinite programming (SDP) relaxations take
hundreds of seconds for 100 points. In this paper, we propose a novel linear
time convex relaxation as well as a contractor method to speed up Branch and
Bound (BnB). Our solver can register two 3D point clouds with 100 points to
provable global optimality in less than half a second when the axis of rotation
is provided. Although it currently cannot solve the full 6DoF problem, it is
two orders of magnitude faster than the state-of-the-art SDP solver STRIDE when
solving the rotation-only TLS problem. In addition to providing a formal proof
for global optimality, we present empirical evidence of global optimality using
adversarial instances with local minimas close to the global minimum.

</details>


### [52] [Multi-perspective monitoring of wildlife and human activities from camera traps and drones with deep learning models](https://arxiv.org/abs/2508.15629)
*Hao Chen,Fang Qiu,Li An,Douglas Stow,Eve Bohnett,Haitao Lyu,Shuang Tian*

Main category: cs.CV

TL;DR: 本研究在尼泊尔奇旺国家公园及其周边地区通过结合相机陷阱和无人机影像的多视角监测，识别野生动物和人类活动的空间分布及其冲突区域，使用深度学习模型（YOLOv11s）达成高精度目标检测。


<details>
  <summary>Details</summary>
Motivation: 探索野生动物与人类活动之间的关系，评估人类-野生动物冲突，辅助制定有效的保护计划。

Method: 结合相机陷阱和无人机热成像收集数据，使用YOLOv11s和Faster RCNN模型进行深度学习图像检测，并进行空间模式分析以识别活动热点及冲突区域。

Result: YOLOv11s模型在检测中展示了96.2%的精度和92.3%的召回率，空间分析明确了野生动物与人类活动集中区域及其冲突热点。

Conclusion: 多视角监测与目标检测技术相结合改进了野生动物监测能力，暴露了保护区内人类与野生动物冲突的具体模式，为景观管理和保护提供了科学支持。

Abstract: Wildlife and human activities are key components of landscape systems.
Understanding their spatial distribution is essential for evaluating human
wildlife interactions and informing effective conservation planning.
Multiperspective monitoring of wildlife and human activities by combining
camera traps and drone imagery. Capturing the spatial patterns of their
distributions, which allows the identification of the overlap of their activity
zones and the assessment of the degree of human wildlife conflict. The study
was conducted in Chitwan National Park (CNP), Nepal, and adjacent regions.
Images collected by visible and nearinfrared camera traps and thermal infrared
drones from February to July 2022 were processed to create training and testing
datasets, which were used to build deep learning models to automatic identify
wildlife and human activities. Drone collected thermal imagery was used for
detecting targets to provide a multiple monitoring perspective. Spatial pattern
analysis was performed to identify animal and resident activity hotspots and
delineation potential human wildlife conflict zones. Among the deep learning
models tested, YOLOv11s achieved the highest performance with a precision of
96.2%, recall of 92.3%, mAP50 of 96.7%, and mAP50 of 81.3%, making it the most
effective for detecting objects in camera trap imagery. Drone based thermal
imagery, analyzed with an enhanced Faster RCNN model, added a complementary
aerial viewpoint for camera trap detections. Spatial pattern analysis
identified clear hotspots for both wildlife and human activities and their
overlapping patterns within certain areas in the CNP and buffer zones
indicating potential conflict. This study reveals human wildlife conflicts
within the conserved landscape. Integrating multiperspective monitoring with
automated object detection enhances wildlife surveillance and landscape
management.

</details>


### [53] [When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding](https://arxiv.org/abs/2508.15641)
*Pengcheng Fang,Yuxia Chen,Rui Guo*

Main category: cs.CV

TL;DR: Grounded VideoDiT通过三项创新提升视频理解的时间感知和对实体的细化对齐能力。


<details>
  <summary>Details</summary>
Motivation: 结合近年来的视频大语言模型在整体推理方面取得的进展，解决其时间感知粗糙和实体对齐漂移的问题。

Method: 提出了一种视频大语言模型Grounded VideoDiT，引入三个核心创新：Diffusion Temporal Latent编码器增强时间边界感知和连续性；对象绑定表示加强实体对齐；混合令牌方案引入时间戳建模。

Result: 实验结果表明，Grounded VideoDiT在Charades STA、NExT GQA和多个VideoQA基准上实现了最先进的性能。

Conclusion: Grounded VideoDiT通过创新设计增强了视频理解的时序推理及实体定位能力，展现出优越的应用潜力。

Abstract: Understanding videos requires more than answering open ended questions, it
demands the ability to pinpoint when events occur and how entities interact
across time. While recent Video LLMs have achieved remarkable progress in
holistic reasoning, they remain coarse in temporal perception: timestamps are
encoded only implicitly, frame level features are weak in capturing continuity,
and language vision alignment often drifts from the entities of interest. In
this paper, we present Grounded VideoDiT, a Video LLM designed to overcome
these limitations by introducing three key innovations. First, a Diffusion
Temporal Latent (DTL) encoder enhances boundary sensitivity and maintains
temporal consistency. Second, object grounded representations explicitly bind
query entities to localized visual evidence, strengthening alignment. Third, a
mixed token scheme with discrete temporal tokens provides explicit timestamp
modeling, enabling fine grained temporal reasoning. Together, these designs
equip Grounded VideoDiT with robust grounding capabilities, as validated by
state of the art results on Charades STA, NExT GQA, and multiple VideoQA
benchmarks.

</details>


### [54] [Weakly-Supervised Learning for Tree Instances Segmentation in Airborne Lidar Point Clouds](https://arxiv.org/abs/2508.15646)
*Swann Emilien Céleste Destouches,Jesse Lahaye,Laurent Valentin Jospin,Jan Skaloud*

Main category: cs.CV

TL;DR: 提出了一种基于半监督学习的树实例分割方法，减少对高质量标注数据的依赖，同时提升了原始分割模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的树实例分割面临数据多样性和标注成本高的问题，需要一种低成本、高效的方法提升分割效果。

Method: 利用非精调模型或闭合算法初始分割结果，结合人类操作员的质量评估标签，训练分类模型，再通过反馈微调分割模型。

Result: 分割模型的树实例识别率提高了34%，并显著减少了错误的非树实例预测。

Conclusion: 该方法在提升树实例分割效果方面迈出了重要一步，但在稀疏森林中对于小树或复杂环境的表现仍需改进。

Abstract: Tree instance segmentation of airborne laser scanning (ALS) data is of utmost
importance for forest monitoring, but remains challenging due to variations in
the data caused by factors such as sensor resolution, vegetation state at
acquisition time, terrain characteristics, etc. Moreover, obtaining a
sufficient amount of precisely labeled data to train fully supervised instance
segmentation methods is expensive. To address these challenges, we propose a
weakly supervised approach where labels of an initial segmentation result
obtained either by a non-finetuned model or a closed form algorithm are
provided as a quality rating by a human operator. The labels produced during
the quality assessment are then used to train a rating model, whose task is to
classify a segmentation output into the same classes as specified by the human
operator. Finally, the segmentation model is finetuned using feedback from the
rating model. This in turn improves the original segmentation model by 34\% in
terms of correctly identified tree instances while considerably reducing the
number of non-tree instances predicted. Challenges still remain in data over
sparsely forested regions characterized by small trees (less than two meters in
height) or within complex surroundings containing shrubs, boulders, etc. which
can be confused as trees where the performance of the proposed method is
reduced.

</details>


### [55] [Towards a 3D Transfer-based Black-box Attack via Critical Feature Guidance](https://arxiv.org/abs/2508.15650)
*Shuchao Pang,Zhenghan Chen,Shen Zhang,Liming Lu,Siyuan Liang,Anan Du,Yongbin Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于转移的黑盒攻击方法（CFG），通过关键特征引导来生成对抗性3D点云，实现了跨不同神经网络架构的更强转移性。


<details>
  <summary>Details</summary>
Motivation: 现有的针对3D点云的对抗性攻击方法通常需要目标模型的信息，而在真实安全场景下获取这些信息具有挑战性，因此需要开发无需目标模型信息的转移性攻击方法。

Method: 提出了Critical Feature Guidance（CFG）方法，通过计算提取特征的重要性，优先破坏跨模型共享的关键特征，同时约束生成的对抗样本的最大偏移范围，以保证其不可察觉性。

Result: 在ModelNet40和ScanObjectNN基准数据集上的实验表明，CFG显著优于现有的攻击方法，提升了转移性。

Conclusion: CFG方法在无需目标模型信息的情况下成功提升了3D点云对抗样本的转移性，为3D对抗性攻击研究提供了新的思路。

Abstract: Deep neural networks for 3D point clouds have been demonstrated to be
vulnerable to adversarial examples. Previous 3D adversarial attack methods
often exploit certain information about the target models, such as model
parameters or outputs, to generate adversarial point clouds. However, in
realistic scenarios, it is challenging to obtain any information about the
target models under conditions of absolute security. Therefore, we focus on
transfer-based attacks, where generating adversarial point clouds does not
require any information about the target models. Based on our observation that
the critical features used for point cloud classification are consistent across
different DNN architectures, we propose CFG, a novel transfer-based black-box
attack method that improves the transferability of adversarial point clouds via
the proposed Critical Feature Guidance. Specifically, our method regularizes
the search of adversarial point clouds by computing the importance of the
extracted features, prioritizing the corruption of critical features that are
likely to be adopted by diverse architectures. Further, we explicitly constrain
the maximum deviation extent of the generated adversarial point clouds in the
loss function to ensure their imperceptibility. Extensive experiments conducted
on the ModelNet40 and ScanObjectNN benchmark datasets demonstrate that the
proposed CFG outperforms the state-of-the-art attack methods by a large margin.

</details>


### [56] [MapKD: Unlocking Prior Knowledge with Cross-Modal Distillation for Efficient Online HD Map Construction](https://arxiv.org/abs/2508.15653)
*Ziyang Yan,Ruikai Li,Zhiyong Cui,Bohan Li,Han Jiang,Yilong Ren,Aoyong Li,Zhenning Li,Sijia Wen,Haiyang Yu*

Main category: cs.CV

TL;DR: 提出了一种用于在线高清地图构建的知识蒸馏方法MapKD，通过多模态模型向高效的视觉模型传递知识，显著提升模型性能并加速推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖过时的离线地图和多模态传感器套件，导致推理时的计算开销。本研究旨在通过知识蒸馏技术解决这些问题，使模型更高效且成本更低。

Method: 提出了一个全新的教师-教练-学生(TCS)多级跨模态知识蒸馏框架，包括一个融合相机和LiDAR数据的教师模型、一个带有模拟LiDAR的视觉教练模型，以及一个轻量级的学生模型。同时设计了两种蒸馏策略：用于视图特征对齐的TGPD和针对语义学习的MSRD。

Result: 在nuScenes数据集上的实验表明，MapKD能够将学生模型的mIoU提升6.68，mAP提升10.94，同时推理速度加快。

Conclusion: MapKD证明了多模态知识蒸馏在生成轻量化高效模型中的潜力，其卓越性能和速度表现使其适用于自动驾驶环境的在线高清地图构建任务。

Abstract: Online HD map construction is a fundamental task in autonomous driving
systems, aiming to acquire semantic information of map elements around the ego
vehicle based on real-time sensor inputs. Recently, several approaches have
achieved promising results by incorporating offline priors such as SD maps and
HD maps or by fusing multi-modal data. However, these methods depend on stale
offline maps and multi-modal sensor suites, resulting in avoidable
computational overhead at inference. To address these limitations, we employ a
knowledge distillation strategy to transfer knowledge from multimodal models
with prior knowledge to an efficient, low-cost, and vision-centric student
model. Specifically, we propose MapKD, a novel multi-level cross-modal
knowledge distillation framework with an innovative Teacher-Coach-Student (TCS)
paradigm. This framework consists of: (1) a camera-LiDAR fusion model with
SD/HD map priors serving as the teacher; (2) a vision-centric coach model with
prior knowledge and simulated LiDAR to bridge the cross-modal knowledge
transfer gap; and (3) a lightweight vision-based student model. Additionally,
we introduce two targeted knowledge distillation strategies: Token-Guided 2D
Patch Distillation (TGPD) for bird's eye view feature alignment and Masked
Semantic Response Distillation (MSRD) for semantic learning guidance. Extensive
experiments on the challenging nuScenes dataset demonstrate that MapKD improves
the student model by +6.68 mIoU and +10.94 mAP while simultaneously
accelerating inference speed. The code is available
at:https://github.com/2004yan/MapKD2026.

</details>


### [57] [CM2LoD3: Reconstructing LoD3 Building Models Using Semantic Conflict Maps](https://arxiv.org/abs/2508.15672)
*Franz Hanke,Antonia Bieringer,Olaf Wysocki,Boris Jutzi*

Main category: cs.CV

TL;DR: 本文提出了一种名为CM2LoD3的新方法，用于通过冲突地图生成LoD3级别的3D建筑模型。


<details>
  <summary>Details</summary>
Motivation: LoD3模型因包含细节丰富的建筑元素（如窗户、门等）而对高级城市分析至关重要，但其生成过程的手动特性限制了其大规模应用。

Method: 引入冲突地图（CM）与语义冲突地图生成器（SCMG），通过对真实与合成CM进行语义分割，同时结合纹理模型的置信得分以提升分割性能。

Result: 实验结果表明，该方法在建筑开口分段和重建上表现优异，取得了61%的性能提升。

Conclusion: 本文研究推进了LoD3模型自动化重建技术，为大规模高效3D城市建模奠定了基础。

Abstract: Detailed 3D building models are crucial for urban planning, digital twins,
and disaster management applications. While Level of Detail 1 (LoD)1 and LoD2
building models are widely available, they lack detailed facade elements
essential for advanced urban analysis. In contrast, LoD3 models address this
limitation by incorporating facade elements such as windows, doors, and
underpasses. However, their generation has traditionally required manual
modeling, making large-scale adoption challenging. In this contribution,
CM2LoD3, we present a novel method for reconstructing LoD3 building models
leveraging Conflict Maps (CMs) obtained from ray-to-model-prior analysis.
Unlike previous works, we concentrate on semantically segmenting real-world CMs
with synthetically generated CMs from our developed Semantic Conflict Map
Generator (SCMG). We also observe that additional segmentation of textured
models can be fused with CMs using confidence scores to further increase
segmentation performance and thus increase 3D reconstruction accuracy.
Experimental results demonstrate the effectiveness of our CM2LoD3 method in
segmenting and reconstructing building openings, with the 61% performance with
uncertainty-aware fusion of segmented building textures. This research
contributes to the advancement of automated LoD3 model reconstruction, paving
the way for scalable and efficient 3D city modeling. Our project is available:
https://github.com/InFraHank/CM2LoD3

</details>


### [58] [LLM-empowered Dynamic Prompt Routing for Vision-Language Models Tuning under Long-Tailed Distributions](https://arxiv.org/abs/2508.15688)
*Yongju Jia,Jiarui Ma,Xiangxian Li,Baiqiao Zhang,Xianhui Cao,Juan Liu,Yulong Bian*

Main category: cs.CV

TL;DR: 这篇论文提出了一个叫做多维动态提示路由（MDPR）的框架，用于解决在数据不平衡场景下视觉-语言模型（VLMs）微调时的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有的VLMs在数据不平衡场景下微调时容易出现偏差，并且当前方法往往忽视了VLMs训练中的固有类别不平衡问题。

Method: 提出MDPR框架，通过构建一个包括五个视觉语义维度的知识库并采用动态路由机制来优化全局视觉类别的对齐、提示检索和细粒度语义平衡，从而实现更稳定的预测。

Result: 在长尾分布基准测试（如CIFAR-LT、ImageNet-LT和Places-LT）上，MDPR达到了与当前最优方法（SOTA）相当的效果。同时，消融实验证实了其语义库对尾部类别的有效性，并表明其动态路由机制的计算开销非常低。

Conclusion: MDPR是一种灵活、高效的框架，能够在数据不平衡场景下改善VLM微调性能。

Abstract: Pre-trained vision-language models (VLMs), such as CLIP, have demonstrated
impressive capability in visual tasks, but their fine-tuning often suffers from
bias in class-imbalanced scene. Recent works have introduced large language
models (LLMs) to enhance VLM fine-tuning with supplementing semantic
information. However, they often overlook inherent class imbalance in VLMs'
pre-training, which may lead to bias accumulation in downstream tasks. To
address this problem, this paper proposes a Multi-dimensional Dynamic Prompt
Routing (MDPR) framework. MDPR constructs a comprehensive knowledge base for
classes, spanning five visual-semantic dimensions. During fine-tuning, the
dynamic routing mechanism aligns global visual classes, retrieves optimal
prompts, and balances fine-grained semantics, yielding stable predictions
through logits fusion. Extensive experiments on long-tailed benchmarks,
including CIFAR-LT, ImageNet-LT, and Places-LT, demonstrate that MDPR achieves
comparable results with current SOTA methods. Ablation studies further confirm
the effectiveness of our semantic library for tail classes, and show that our
dynamic routing incurs minimal computational overhead, making MDPR a flexible
and efficient enhancement for VLM fine-tuning under data imbalance.

</details>


### [59] [StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding](https://arxiv.org/abs/2508.15717)
*Yanlai Yang,Zhuokai Zhao,Satya Narayan Shukla,Aashu Singh,Shlok Kumar Mishra,Lizhu Zhang,Mengye Ren*

Main category: cs.CV

TL;DR: 该论文提出了一种名为StreamMem的方法，旨在通过压缩KV缓存以高效处理长视频理解和问答问题，从而克服现有多模态大语言模型（MLLMs）的长视频处理局限性。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在视觉-语言推理方面取得了进展，但在高效处理长视频时能力有限，尤其是需要解决KV缓存存储和计算开销较大的问题。此外，现存的视频压缩方法对长视频多轮对话应用并不实用。

Method: 提出了一种名为StreamMem的新方法。这是一种无查询依赖的KV缓存内存机制，可以在流式编码视频帧的同时，根据视觉标记与通用查询标记之间的注意力分数来压缩KV缓存。此方法在固定大小的KV内存中进行操作，从而在资源受限的长视频场景中高效完成问答任务。

Result: 在三项长视频理解和两项流式视频问答基准上进行了评估。结果显示，StreamMem在无查询依赖的KV缓存压缩方面达到了最新性能水平，并且在与查询相关的压缩方法相比具有竞争力。

Conclusion: StreamMem方法有效解决了长视频理解中的内存和计算瓶颈问题，为资源受限场景中的视频问答任务提供了一种高效的解决方案。

Abstract: Multimodal large language models (MLLMs) have made significant progress in
visual-language reasoning, but their ability to efficiently handle long videos
remains limited. Despite recent advances in long-context MLLMs, storing and
attending to the key-value (KV) cache for long visual contexts incurs
substantial memory and computational overhead. Existing visual compression
methods require either encoding the entire visual context before compression or
having access to the questions in advance, which is impractical for long video
understanding and multi-turn conversational settings. In this work, we propose
StreamMem, a query-agnostic KV cache memory mechanism for streaming video
understanding. Specifically, StreamMem encodes new video frames in a streaming
manner, compressing the KV cache using attention scores between visual tokens
and generic query tokens, while maintaining a fixed-size KV memory to enable
efficient question answering (QA) in memory-constrained, long-video scenarios.
Evaluation on three long video understanding and two streaming video question
answering benchmarks shows that StreamMem achieves state-of-the-art performance
in query-agnostic KV cache compression and is competitive with query-aware
compression approaches.

</details>


### [60] [WorldWeaver: Generating Long-Horizon Video Worlds via Rich Perception](https://arxiv.org/abs/2508.15720)
*Zhiheng Liu,Xueqing Deng,Shoufa Chen,Angtian Wang,Qiushan Guo,Mingfei Han,Zeyue Xue,Mengzhao Chen,Ping Luo,Linjie Yang*

Main category: cs.CV

TL;DR: 本文提出WorldWeaver框架，旨在解决长视频生成中的结构和时间一致性问题。通过结合RGB和感知条件，同时利用深度线索和分段噪声调度，有效减少了时间漂移并提高了生成视频的质量。


<details>
  <summary>Details</summary>
Motivation: 当前的生成视频建模方法主要依赖于RGB信号，在长时间序列中容易产生结构和动作错误。需要一种方法来增强长视频的结构和时间一致性。

Method: 世界编织(WorldWeaver)框架联合建模RGB帧和感知条件，采用深度线索构建记忆库，并通过分段噪声调度训练来改进预测组，从而减少漂移和降低计算成本。

Result: 实验显示，基于扩散模型和校正流模型的WorldWeaver有效减少了时间漂移，提高了生成视频的逼真度和质量。

Conclusion: WorldWeaver通过创新的训练框架显著改善了长视频生成中的一致性问题，减少了漂移并提升了输出视频的质量和动态表现。

Abstract: Generative video modeling has made significant strides, yet ensuring
structural and temporal consistency over long sequences remains a challenge.
Current methods predominantly rely on RGB signals, leading to accumulated
errors in object structure and motion over extended durations. To address these
issues, we introduce WorldWeaver, a robust framework for long video generation
that jointly models RGB frames and perceptual conditions within a unified
long-horizon modeling scheme. Our training framework offers three key
advantages. First, by jointly predicting perceptual conditions and color
information from a unified representation, it significantly enhances temporal
consistency and motion dynamics. Second, by leveraging depth cues, which we
observe to be more resistant to drift than RGB, we construct a memory bank that
preserves clearer contextual information, improving quality in long-horizon
video generation. Third, we employ segmented noise scheduling for training
prediction groups, which further mitigates drift and reduces computational
cost. Extensive experiments on both diffusion- and rectified flow-based models
demonstrate the effectiveness of WorldWeaver in reducing temporal drift and
improving the fidelity of generated videos.

</details>


### [61] [Fine-grained Multi-class Nuclei Segmentation with Molecular-empowered All-in-SAM Model](https://arxiv.org/abs/2508.15751)
*Xueyuan Li,Can Cui,Ruining Deng,Yucheng Tang,Quan Liu,Tianyuan Yao,Shunxing Bao,Naweed Chowdhury,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: 本文提出了利用分子技术的All-in-SAM模型，结合视觉基础模型进行高效的核与细胞分割，提高生物医学图像分析的精准性与便利性。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉基础模型的方法在微细语义分割上存在困难，提出新模型旨在克服这一挑战，减少像素级标注需求，并提高模型适应性。

Method: 提出All-in-SAM模型，采用全栈方法，包括：基于分子技术的学习简化标注需求、结合SAM适配器强化语义学习、引入分子导向的校正学习提高分割精准性。

Result: 实验表明，All-in-SAM模型在多样化标注质量条件下能显著提升细胞分类表现。

Conclusion: 该方法降低标注工作量，拓展精准医学图像分析在资源有限环境中的应用，提高了诊断能力和病理图像分析的自动化水平。

Abstract: Purpose: Recent developments in computational pathology have been driven by
advances in Vision Foundation Models, particularly the Segment Anything Model
(SAM). This model facilitates nuclei segmentation through two primary methods:
prompt-based zero-shot segmentation and the use of cell-specific SAM models for
direct segmentation. These approaches enable effective segmentation across a
range of nuclei and cells. However, general vision foundation models often face
challenges with fine-grained semantic segmentation, such as identifying
specific nuclei subtypes or particular cells. Approach: In this paper, we
propose the molecular-empowered All-in-SAM Model to advance computational
pathology by leveraging the capabilities of vision foundation models. This
model incorporates a full-stack approach, focusing on: (1) annotation-engaging
lay annotators through molecular-empowered learning to reduce the need for
detailed pixel-level annotations, (2) learning-adapting the SAM model to
emphasize specific semantics, which utilizes its strong generalizability with
SAM adapter, and (3) refinement-enhancing segmentation accuracy by integrating
Molecular-Oriented Corrective Learning (MOCL). Results: Experimental results
from both in-house and public datasets show that the All-in-SAM model
significantly improves cell classification performance, even when faced with
varying annotation quality. Conclusions: Our approach not only reduces the
workload for annotators but also extends the accessibility of precise
biomedical image analysis to resource-limited settings, thereby advancing
medical diagnostics and automating pathology image analysis.

</details>


### [62] [Waver: Wave Your Way to Lifelike Video Generation](https://arxiv.org/abs/2508.15761)
*Yifu Zhang,Hao Yang,Yuqi Zhang,Yifei Hu,Fengda Zhu,Chuang Lin,Xiaofeng Mei,Yi Jiang,Zehuan Yuan,Bingyue Peng*

Main category: cs.CV

TL;DR: Waver是一个高性能的基础模型，支持图像和视频生成任务，包括T2V、I2V和T2I功能，生成的视频最长可达10秒，分辨率720p，后可提升至1080p。


<details>
  <summary>Details</summary>
Motivation: 帮助研究社区更高效地训练高质量的视频生成模型，加速视频生成技术的发展。

Method: 引入了Hybrid Stream DiT架构以增强模态对齐并加速训练，同时提出全方位数据筛选流程，并通过MLLM视频质量模型确保数据质量。

Result: Waver在视频生成中的动作幅度及时间一致性方面表现优秀，在T2V和I2V榜单中排名前3，并超过了许多开源与部分商业模型。

Conclusion: Waver为社区提供了生成高质量视频的解决方案，显著提升了视频合成技术水平。

Abstract: We present Waver, a high-performance foundation model for unified image and
video generation. Waver can directly generate videos with durations ranging
from 5 to 10 seconds at a native resolution of 720p, which are subsequently
upscaled to 1080p. The model simultaneously supports text-to-video (T2V),
image-to-video (I2V), and text-to-image (T2I) generation within a single,
integrated framework. We introduce a Hybrid Stream DiT architecture to enhance
modality alignment and accelerate training convergence. To ensure training data
quality, we establish a comprehensive data curation pipeline and manually
annotate and train an MLLM-based video quality model to filter for the
highest-quality samples. Furthermore, we provide detailed training and
inference recipes to facilitate the generation of high-quality videos. Building
on these contributions, Waver excels at capturing complex motion, achieving
superior motion amplitude and temporal consistency in video synthesis. Notably,
it ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial
Analysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming
existing open-source models and matching or surpassing state-of-the-art
commercial solutions. We hope this technical report will help the community
more efficiently train high-quality video generation models and accelerate
progress in video generation technologies. Official page:
https://github.com/FoundationVision/Waver.

</details>


### [63] [ATLAS: Decoupling Skeletal and Shape Parameters for Expressive Parametric Human Modeling](https://arxiv.org/abs/2508.15767)
*Jinhyung Park,Javier Romero,Shunsuke Saito,Fabian Prada,Takaaki Shiratori,Yichen Xu,Federica Bogo,Shoou-I Yu,Kris Kitani,Rawal Khirodkar*

Main category: cs.CV

TL;DR: ATLAS是一个高保真身体模型，基于600k高分辨率扫描数据，能够更精确地匹配多样的姿态和身体特性。


<details>
  <summary>Details</summary>
Motivation: 解决现有人体模型在捕获各种姿态和形态细节方面的限制，以及模型假设和数据多样性不足的问题。

Method: 研发了一种基于人类骨骼的网格表示，解耦形状和骨骼基础，并结合600k高分辨率扫描数据学习模型。

Result: ATLAS能够更准确地拟合未见过的姿态和个体，并通过非线性姿态校正提升模型对复杂姿态的捕捉能力。

Conclusion: ATLAS在形状表现力、属性定制化及关键点拟合等方面超越现有模型，并显著改善了姿态和形态建模的精度。

Abstract: Parametric body models offer expressive 3D representation of humans across a
wide range of poses, shapes, and facial expressions, typically derived by
learning a basis over registered 3D meshes. However, existing human mesh
modeling approaches struggle to capture detailed variations across diverse body
poses and shapes, largely due to limited training data diversity and
restrictive modeling assumptions. Moreover, the common paradigm first optimizes
the external body surface using a linear basis, then regresses internal
skeletal joints from surface vertices. This approach introduces problematic
dependencies between internal skeleton and outer soft tissue, limiting direct
control over body height and bone lengths. To address these issues, we present
ATLAS, a high-fidelity body model learned from 600k high-resolution scans
captured using 240 synchronized cameras. Unlike previous methods, we explicitly
decouple the shape and skeleton bases by grounding our mesh representation in
the human skeleton. This decoupling enables enhanced shape expressivity,
fine-grained customization of body attributes, and keypoint fitting independent
of external soft-tissue characteristics. ATLAS outperforms existing methods by
fitting unseen subjects in diverse poses more accurately, and quantitative
evaluations show that our non-linear pose correctives more effectively capture
complex poses compared to linear models.

</details>


### [64] [SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass](https://arxiv.org/abs/2508.15769)
*Yanxu Meng,Haoning Wu,Ya Zhang,Weidi Xie*

Main category: cs.CV

TL;DR: 研究提出了一个名为SceneGen的框架，用于从单一场景图像生成多个3D资产，而无需优化或资源检索。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够从单一场景图像中同时生成多个质量良好的3D资产的解决方案，以满足VR/AR和具身AI中的应用需求。

Method: 引入SceneGen框架，通过新的特征聚合模块结合视觉和几何编码器的信息，实现单次前向传播生成3D资产及其相对空间位置。同时验证了其多图像输入的扩展性。

Result: 实验表明，SceneGen具有高效和鲁棒的生成能力，在量化和定性评估中表现优异，并在多图像输入情况下表现更好。

Conclusion: SceneGen为高质量3D内容生成提供了一种新方法，有潜力推进相关应用。代码和模型已公开。

Abstract: 3D content generation has recently attracted significant research interest
due to its applications in VR/AR and embodied AI. In this work, we address the
challenging task of synthesizing multiple 3D assets within a single scene
image. Concretely, our contributions are fourfold: (i) we present SceneGen, a
novel framework that takes a scene image and corresponding object masks as
input, simultaneously producing multiple 3D assets with geometry and texture.
Notably, SceneGen operates with no need for optimization or asset retrieval;
(ii) we introduce a novel feature aggregation module that integrates local and
global scene information from visual and geometric encoders within the feature
extraction module. Coupled with a position head, this enables the generation of
3D assets and their relative spatial positions in a single feedforward pass;
(iii) we demonstrate SceneGen's direct extensibility to multi-image input
scenarios. Despite being trained solely on single-image inputs, our
architectural design enables improved generation performance with multi-image
inputs; and (iv) extensive quantitative and qualitative evaluations confirm the
efficiency and robust generation abilities of our approach. We believe this
paradigm offers a novel solution for high-quality 3D content generation,
potentially advancing its practical applications in downstream tasks. The code
and model will be publicly available at: https://mengmouxu.github.io/SceneGen.

</details>


### [65] [Visual Autoregressive Modeling for Instruction-Guided Image Editing](https://arxiv.org/abs/2508.15772)
*Qingyang Mao,Qi Cai,Yehao Li,Yingwei Pan,Mingyue Cheng,Ting Yao,Qi Liu,Tao Mei*

Main category: cs.CV

TL;DR: 本文提出了VAREdit，一种基于视觉自回归(VAR)模型的新框架，通过多尺度预测实现精准图像编辑，显著超越了基于扩散模型的方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型尽管表现出令人难以置信的可视化保真度，但在全局去噪过程中，编辑区域和整个图像上下文存在耦合问题，导致编辑指令的执行不够精准。作者希望利用自动回归模型的因果与组合机制解决这个问题。

Method: 提出VAREdit框架，将图像编辑表述为一种基于源图像特征和文本指导的下一尺度预测问题。为了更好地润色该方法，引入了一个“尺度对齐参考”（SAR）模块，向自注意力层注入匹配尺度的条件信息。

Result: 在标准基准测试中，VAREdit表现优异，GPT-Balance得分超出领先的扩散方法30%以上。同时，该方法以1.2秒完成一张512×512编辑，比UltraEdit快2.2倍。

Conclusion: VAREdit通过基于视觉自回归模型的创新性设计，显著提升了图像编辑的准确度与效率，在编辑依从性和性能效率上优于现有方法。

Abstract: Recent advances in diffusion models have brought remarkable visual fidelity
to instruction-guided image editing. However, their global denoising process
inherently entangles the edited region with the entire image context, leading
to unintended spurious modifications and compromised adherence to editing
instructions. In contrast, autoregressive models offer a distinct paradigm by
formulating image synthesis as a sequential process over discrete visual
tokens. Their causal and compositional mechanism naturally circumvents the
adherence challenges of diffusion-based methods. In this paper, we present
VAREdit, a visual autoregressive (VAR) framework that reframes image editing as
a next-scale prediction problem. Conditioned on source image features and text
instructions, VAREdit generates multi-scale target features to achieve precise
edits. A core challenge in this paradigm is how to effectively condition the
source image tokens. We observe that finest-scale source features cannot
effectively guide the prediction of coarser target features. To bridge this
gap, we introduce a Scale-Aligned Reference (SAR) module, which injects
scale-matched conditioning information into the first self-attention layer.
VAREdit demonstrates significant advancements in both editing adherence and
efficiency. On standard benchmarks, it outperforms leading diffusion-based
methods by 30\%+ higher GPT-Balance score. Moreover, it completes a
$512\times512$ editing in 1.2 seconds, making it 2.2$\times$ faster than the
similarly sized UltraEdit. The models are available at
https://github.com/HiDream-ai/VAREdit.

</details>


### [66] [Scaling Group Inference for Diverse and High-Quality Generation](https://arxiv.org/abs/2508.15773)
*Gaurav Parmar,Or Patashnik,Daniil Ostashev,Kuan-Chieh Wang,Kfir Aberman,Srinivasa Narasimhan,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: 本文提出一种可扩展的群组推理方法，提高生成样本的多样性和质量。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在实际应用中因独立采样导致结果冗余，限制用户选择和创意探索。

Method: 将群组推理表述为二次整数分配问题，并通过逐步删减候选集提高运行效率，使其适应大规模数据集。

Result: 实验结果表明，该方法相比独立采样和现有推理算法，显著提升样本的多样性和质量。

Conclusion: 该框架可推广至多种任务，改进生成模型的群组输出能力。

Abstract: Generative models typically sample outputs independently, and recent
inference-time guidance and scaling algorithms focus on improving the quality
of individual samples. However, in real-world applications, users are often
presented with a set of multiple images (e.g., 4-8) for each prompt, where
independent sampling tends to lead to redundant results, limiting user choices
and hindering idea exploration. In this work, we introduce a scalable group
inference method that improves both the diversity and quality of a group of
samples. We formulate group inference as a quadratic integer assignment
problem: candidate outputs are modeled as graph nodes, and a subset is selected
to optimize sample quality (unary term) while maximizing group diversity
(binary term). To substantially improve runtime efficiency, we progressively
prune the candidate set using intermediate predictions, allowing our method to
scale up to large candidate sets. Extensive experiments show that our method
significantly improves group diversity and quality compared to independent
sampling baselines and recent inference algorithms. Our framework generalizes
across a wide range of tasks, including text-to-image, image-to-image, image
prompting, and video generation, enabling generative models to treat multiple
outputs as cohesive groups rather than independent samples.

</details>


### [67] [CineScale: Free Lunch in High-Resolution Cinematic Visual Generation](https://arxiv.org/abs/2508.15774)
*Haonan Qiu,Ning Yu,Ziqi Huang,Paul Debevec,Ziwei Liu*

Main category: cs.CV

TL;DR: 提出了一种称为CineScale的推理范式，旨在解决高分辨率图像和视频生成中的问题，包括去除噪声和避免重复模式等缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有视觉扩散模型由于训练分辨率的限制，难以生成高保真高分辨率图像或视频。需要一个有效的方法开发未充分利用的高分辨率生成能力。

Method: 提出了CineScale，一个专为高分辨率视觉生成设计的推理范式，并提出针对不同视频生成架构的专用变体，适用于高分辨率的T2I、T2V、I2V、V2V任务，无需模型微调或仅需最少的微调。

Result: CineScale实现了无微调条件下的8k图像生成，以及通过LoRA微调实现的4k视频生成，具备高生成质量且无重复模式问题。

Conclusion: CineScale大幅提升了图像和视频生成模型的高分辨率能力，为未来研究打开了更多可能性。

Abstract: Visual diffusion models achieve remarkable progress, yet they are typically
trained at limited resolutions due to the lack of high-resolution data and
constrained computation resources, hampering their ability to generate
high-fidelity images or videos at higher resolutions. Recent efforts have
explored tuning-free strategies to exhibit the untapped potential
higher-resolution visual generation of pre-trained models. However, these
methods are still prone to producing low-quality visual content with repetitive
patterns. The key obstacle lies in the inevitable increase in high-frequency
information when the model generates visual content exceeding its training
resolution, leading to undesirable repetitive patterns deriving from the
accumulated errors. In this work, we propose CineScale, a novel inference
paradigm to enable higher-resolution visual generation. To tackle the various
issues introduced by the two types of video generation architectures, we
propose dedicated variants tailored to each. Unlike existing baseline methods
that are confined to high-resolution T2I and T2V generation, CineScale broadens
the scope by enabling high-resolution I2V and V2V synthesis, built atop
state-of-the-art open-source video generation frameworks. Extensive experiments
validate the superiority of our paradigm in extending the capabilities of
higher-resolution visual generation for both image and video models.
Remarkably, our approach enables 8k image generation without any fine-tuning,
and achieves 4k video generation with only minimal LoRA fine-tuning. Generated
video samples are available at our website:
https://eyeline-labs.github.io/CineScale/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [68] [Efficient Switchable Safety Control in LLMs via Magic-Token-Guided Co-Training](https://arxiv.org/abs/2508.14904)
*Jianfeng Si,Lin Sun,Zhewen Tan,Xiangzheng Zhang*

Main category: cs.CL

TL;DR: 提出了一种整合多种安全行为的统一联合训练框架，通过简单指令实现行为切换，在提升安全性和控制能力的同时降低训练复杂性和部署成本。


<details>
  <summary>Details</summary>
Motivation: 当前LLM内容安全方法缺乏精细的后期控制能力，且训练流程复杂，需要更高效和灵活的解决方案。

Method: 提出了一个联合训练框架，将正向、负向和拒绝三种安全行为整合到单一SFT阶段，通过简单的指令实现动态激活行为切换。

Result: 在安全性对齐质量上能匹敌传统方法，8B模型甚至超越了DeepSeek-R1（671B），同时显著降低训练复杂性和部署成本。

Conclusion: 此方法提供了一种可扩展、高效且高度可控的解决方案，证明了其在LLM内容安全上的优秀性能和精细控制能力。

Abstract: Current methods for content safety in Large Language Models (LLMs), such as
Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback
(RLHF), often rely on multi-stage training pipelines and lack fine-grained,
post-deployment controllability. To address these limitations, we propose a
unified co-training framework that efficiently integrates multiple safety
behaviors: positive (lawful/prosocial), negative (unfiltered/risk-prone) and
rejective (refusal-oriented/conservative) within a single SFT stage. Notably,
each behavior is dynamically activated via a simple system-level instruction,
or magic token, enabling stealthy and efficient behavioral switching at
inference time. This flexibility supports diverse deployment scenarios, such as
positive for safe user interaction, negative for internal red-teaming, and
rejective for context-aware refusals triggered by upstream moderation signals.
This co-training strategy induces a distinct Safety Alignment Margin in the
output space, characterized by well-separated response distributions
corresponding to each safety mode. The existence of this margin provides
empirical evidence for the model's safety robustness and enables unprecedented
fine-grained control. Experiments show that our method matches the safety
alignment quality of SFT+DPO, with our 8B model notably surpassing DeepSeek-R1
(671B) in safety performance, while significantly reducing both training
complexity and deployment costs. This work presents a scalable, efficient, and
highly controllable solution for LLM content safety.

</details>


### [69] [Preliminary Ranking of WMT25 General Machine Translation Systems](https://arxiv.org/abs/2508.14909)
*Tom Kocmi,Eleftherios Avramidis,Rachel Bawden,Ondřej Bojar,Konstantin Dranch,Anton Dvorkovich,Sergey Dukanov,Natalia Fedorova,Mark Fishel,Markus Freitag,Thamme Gowda,Roman Grundkiewicz,Barry Haddow,Marzena Karpinska,Philipp Koehn,Howard Lakougna,Jessica Lundin,Kenton Murray,Masaaki Nagata,Stefano Perrella,Lorenzo Proietti,Martin Popel,Maja Popović,Parker Riley,Mariya Shmatova,Steinþór Steingrímsson,Lisa Yankovskaya,Vilém Zouhar*

Main category: cs.CL

TL;DR: 本文介绍WMT25通用机器翻译共享任务的初步排名，该排名基于自动评价指标。


<details>
  <summary>Details</summary>
Motivation: 报告旨在为任务参与者提供初步结果，帮助其准备系统提交论文。

Method: 通过自动评价对MT系统进行排名，但可能对使用重排序技术的系统有偏向。最终排名将基于更加可靠的人工评价。

Result: 提供了初步排名，供参与者参考改进。

Conclusion: 此报告仅为初步排名，最终结果将由人工评价决定。

Abstract: We present the preliminary ranking of the WMT25 General Machine Translation
Shared Task, in which MT systems have been evaluated using automatic metrics.
As this ranking is based on automatic evaluations, it may be biased in favor of
systems that employ re-ranking techniques, such as Quality Estimation
re-ranking or Minimum Bayes Risk decoding. The official WMT25 ranking will be
based on human evaluation, which is more reliable and will supersede the
automatic ranking.
  The purpose of this report is not to present the final findings of the
General MT task, but rather to share preliminary results with task
participants, which may be useful when preparing their system submission
papers.

</details>


### [70] [Bridging the Culture Gap: A Framework for LLM-Driven Socio-Cultural Localization of Math Word Problems in Low-Resource Languages](https://arxiv.org/abs/2508.14913)
*Israel Abebe Azime,Tadesse Destaw Belay,Dietrich Klakow,Philipp Slusallek,Anshuman Chhabra*

Main category: cs.CL

TL;DR: 本研究提出一个框架，用于通过大语言模型（LLM）驱动的文化本地化自动构建包含本地化实体的数学文字题数据集，以促进低资源语言的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有多语言基准数据集主要由翻译生成，未能反映本地化的真实社会和文化背景，严重影响低资源语言内的数学推理研究。

Method: 通过使用大语言模型，自动构建包含原生名字、组织、货币的数学数据集框架，从现有数据集中自动生成真实文化本地化的任务数据。

Result: 实验表明，该框架可有效减轻现有多语言数学基准中的英语中心偏差，并在引入本地化实体后提高低资源语言的数学推理表现。

Conclusion: 文化本地化框架能够更真实地反映不同语境中的数学推理能力，提高多语言模型的鲁棒性，并有助于弥补低资源语言数学数据集的不足。

Abstract: Large language models (LLMs) have demonstrated significant capabilities in
solving mathematical problems expressed in natural language. However,
multilingual and culturally-grounded mathematical reasoning in low-resource
languages lags behind English due to the scarcity of socio-cultural task
datasets that reflect accurate native entities such as person names,
organization names, and currencies. Existing multilingual benchmarks are
predominantly produced via translation and typically retain English-centric
entities, owing to the high cost associated with human annotater-based
localization. Moreover, automated localization tools are limited, and hence,
truly localized datasets remain scarce. To bridge this gap, we introduce a
framework for LLM-driven cultural localization of math word problems that
automatically constructs datasets with native names, organizations, and
currencies from existing sources. We find that translated benchmarks can
obscure true multilingual math ability under appropriate socio-cultural
contexts. Through extensive experiments, we also show that our framework can
help mitigate English-centric entity bias and improves robustness when native
entities are introduced across various languages.

</details>


### [71] [Improving LLMs for Machine Translation Using Synthetic Preference Data](https://arxiv.org/abs/2508.14951)
*Dario Vajda,Domen Vreš,Marko Robnik-Šikonja*

Main category: cs.CL

TL;DR: 本文探索如何通过少量易于生成的数据资源，改进指令微调的大型语言模型（LLM）以提升机器翻译性能。以斯洛文尼亚语为例，通过直接偏好优化（DPO）策略完成实验，最终模型性能优于基础模型。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用有限资源改进现有大型语言模型的机器翻译能力，特别针对低资源语言（如斯洛文尼亚语）。

Method: 通过直接偏好优化（DPO）算法，基于一个包含质量分级实例的数据集微调现有LLM（GaMS-9B-Instruct），数据集由两个模型翻译生成并评估排名产生。

Result: 微调后的模型在评估中的COMET分数较两个基线模型分别提升约0.04和0.02，并更稳定地避免语言和格式错误。

Conclusion: 研究表明，通过少量高质量训练数据和有效优化策略，可以显著提升大型语言模型在特定语言机器翻译中的表现。

Abstract: Large language models have emerged as effective machine translation systems.
In this paper, we explore how a general instruction-tuned large language model
can be improved for machine translation using relatively few easily produced
data resources. Using Slovene as a use case, we improve the GaMS-9B-Instruct
model using Direct Preference Optimization (DPO) training on a programmatically
curated and enhanced subset of a public dataset. As DPO requires pairs of
quality-ranked instances, we generated its training dataset by translating
English Wikipedia articles using two LLMs, GaMS-9B-Instruct and
EuroLLM-9B-Instruct. We ranked the resulting translations based on heuristics
coupled with automatic evaluation metrics such as COMET. The evaluation shows
that our fine-tuned model outperforms both models involved in the dataset
generation. In comparison to the baseline models, the fine-tuned model achieved
a COMET score gain of around 0.04 and 0.02, respectively, on translating
Wikipedia articles. It also more consistently avoids language and formatting
errors.

</details>


### [72] [Multilingual Datasets for Custom Input Extraction and Explanation Requests Parsing in Conversational XAI Systems](https://arxiv.org/abs/2508.14982)
*Qianli Wang,Tatiana Anikina,Nils Feldhus,Simon Ostermann,Fedor Splitt,Jiaao Li,Yoana Tsoneva,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 本文提出了两个多语言数据集MultiCoXQL和Compass，并对多语言解析方法进行了研究，旨在改进对话型可解释人工智能（ConvXAI）系统的性能。


<details>
  <summary>Details</summary>
Motivation: 通过增强多语言对话型可解释人工智能系统的性能，解决目前数据集匮乏和自定义输入支持不足的问题。

Method: 引入MultiCoXQL和Compass数据集，同时提出新的解析方法及多种解析策略，并调研三个不同规模的语言模型与BERT模型的表现。

Result: MultiCoXQL扩展了CoXQL数据集至五种语言，包括一种低资源语言，Compass覆盖了11种用户意图。在数据集上基于语言模型和BERT模型进行的测试显示了解析方法的有效性。

Conclusion: 本文工作填补了多语言对话型可解释人工智能系统数据集和解析方法的空白，对推动多语言AI解析技术的发展具有重要意义。

Abstract: Conversational explainable artificial intelligence (ConvXAI) systems based on
large language models (LLMs) have garnered considerable attention for their
ability to enhance user comprehension through dialogue-based explanations.
Current ConvXAI systems often are based on intent recognition to accurately
identify the user's desired intention and map it to an explainability method.
While such methods offer great precision and reliability in discerning users'
underlying intentions for English, a significant challenge in the scarcity of
training data persists, which impedes multilingual generalization. Besides, the
support for free-form custom inputs, which are user-defined data distinct from
pre-configured dataset instances, remains largely limited. To bridge these
gaps, we first introduce MultiCoXQL, a multilingual extension of the CoXQL
dataset spanning five typologically diverse languages, including one
low-resource language. Subsequently, we propose a new parsing approach aimed at
enhancing multilingual parsing performance, and evaluate three LLMs on
MultiCoXQL using various parsing strategies. Furthermore, we present Compass, a
new multilingual dataset designed for custom input extraction in ConvXAI
systems, encompassing 11 intents across the same five languages as MultiCoXQL.
We conduct monolingual, cross-lingual, and multilingual evaluations on Compass,
employing three LLMs of varying sizes alongside BERT-type models.

</details>


### [73] [Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner](https://arxiv.org/abs/2508.15044)
*Bolian Li,Yanran Wu,Xinyu Luo,Ruqi Zhang*

Main category: cs.CL

TL;DR: 介绍了一种名为Reward-Shifted Speculative Sampling (SSS) 的算法，提升了测试时对齐的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时对齐技术尽管能提升模型的安全性和推理能力，但在推理阶段的计算成本较高，限制了实际应用。

Method: 提出了Reward-Shifted Speculative Sampling (SSS) 算法，通过对小型模型进行对齐，结合修订的接受标准和奖励分布，利用分布偏移来实现有效推断。

Result: 在对齐实验中实现了高奖励分数，推理成本显著降低。

Conclusion: 算法同时具备高效性与有效性，验证了其实用价值。

Abstract: Aligning large language models (LLMs) with human preferences has become a
critical step in their development. Recent research has increasingly focused on
test-time alignment, where additional compute is allocated during inference to
enhance LLM safety and reasoning capabilities. However, these test-time
alignment techniques often incur substantial inference costs, limiting their
practical application. We are inspired by the speculative sampling
acceleration, which leverages a small draft model to efficiently predict future
tokens, to address the efficiency bottleneck of test-time alignment. We
introduce the reward-Shifted Speculative Sampling (SSS) algorithm, in which the
draft model is aligned with human preferences, while the target model remains
unchanged. We theoretically demonstrate that the distributional shift between
the aligned draft model and the unaligned target model can be exploited to
recover the RLHF optimal solution without actually obtaining it, by modifying
the acceptance criterion and bonus token distribution. Our algorithm achieves
superior gold reward scores at a significantly reduced inference cost in
test-time weak-to-strong alignment experiments, thereby validating both its
effectiveness and efficiency.

</details>


### [74] [LongRecall: A Structured Approach for Robust Recall Evaluation in Long-Form Text](https://arxiv.org/abs/2508.15085)
*MohamamdJavad Ardestani,Ehsan Kamalloo,Davood Rafiei*

Main category: cs.CL

TL;DR: 本文提出了LongRecall，一个三阶段召回评估框架，旨在提升系统生成文本的完整性，尤其适用于医学、法律和问答领域。


<details>
  <summary>Details</summary>
Motivation: 当前的召回指标依赖于词汇匹配，容易因非词汇一致性或语义偏移导致错误。为提高文本完整性，需开发新的评估方法。

Method: LongRecall框架通过分解答案为独立事实、逐步缩小匹配范围，并结合词汇与语义过滤以及结构化蕴含检查来评估召回。

Result: 在三个复杂的长文本问答基准测试中，LongRecall显著超越了现有词汇和基于LLM判断的召回基线。

Conclusion: LongRecall为系统化召回评估提供了一个可靠的基础方法，其准确性优于传统方法。

Abstract: LongRecall. The completeness of machine-generated text, ensuring that it
captures all relevant information, is crucial in domains such as medicine and
law and in tasks like list-based question answering (QA), where omissions can
have serious consequences. However, existing recall metrics often depend on
lexical overlap, leading to errors with unsubstantiated entities and
paraphrased answers, while LLM-as-a-Judge methods with long holistic prompts
capture broader semantics but remain prone to misalignment and hallucinations
without structured verification. We introduce LongRecall, a general three-stage
recall evaluation framework that decomposes answers into self-contained facts,
successively narrows plausible candidate matches through lexical and semantic
filtering, and verifies their alignment through structured entailment checks.
This design reduces false positives and false negatives while accommodating
diverse phrasings and contextual variations, serving as a foundational building
block for systematic recall assessment. We evaluate LongRecall on three
challenging long-form QA benchmarks using both human annotations and LLM-based
judges, demonstrating substantial improvements in recall accuracy over strong
lexical and LLM-as-a-Judge baselines.

</details>


### [75] [Mapping the Course for Prompt-based Structured Prediction](https://arxiv.org/abs/2508.15090)
*Matt Pauk,Maria Leonor Pacheco*

Main category: cs.CL

TL;DR: 提出了将大型语言模型（LLMs）与组合推理相结合的方法，以解决LLMs存在的幻觉和复杂推理问题，并通过实验验证了这种方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型（LLMs）在任务处理中的幻觉问题及复杂推理能力不足的现状，探索通过结构化推理增强模型的表现。

Method: 结合LLMs预测能力与符号推理的结构一致性，研究有效的提示策略以更好地估计 LLM的信心值，并进行校准和基于结构预测目标的微调。

Result: 实验结果表明，采用符号推理可提高预测的一致性和准确性，同时校准和结构化优化进一步增强了模型在挑战性任务上的性能。

Conclusion: 结构化学习即使在 LLM盛行的时代，仍然具有重要价值，将推理方法与 LLM结合是提升模型能力的有效途径。

Abstract: LLMs have been shown to be useful for a variety of language tasks, without
requiring task-specific fine-tuning. However, these models often struggle with
hallucinations and complex reasoning problems due to their autoregressive
nature. We propose to address some of these issues, specifically in the area of
structured prediction, by combining LLMs with combinatorial inference in an
attempt to marry the predictive power of LLMs with the structural consistency
provided by inference methods. We perform exhaustive experiments in an effort
to understand which prompting strategies can effectively estimate LLM
confidence values for use with symbolic inference, and show that, regardless of
the prompting strategy, the addition of symbolic inference on top of prompting
alone leads to more consistent and accurate predictions. Additionally, we show
that calibration and fine-tuning using structured prediction objectives leads
to increased performance for challenging tasks, showing that structured
learning is still valuable in the era of LLMs.

</details>


### [76] [Nemotron-CC-Math: A 133 Billion-Token-Scale High Quality Math Pretraining Dataset](https://arxiv.org/abs/2508.15096)
*Rabeeh Karimi Mahabadi,Sanjeev Satheesh,Shrimai Prabhumoye,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro*

Main category: cs.CL

TL;DR: 本文通过一种新方法构建了高质量的数学语料库Nemotron-CC-Math，比以往数据集质量更高，体量更大，并在数学与通用推理能力上提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有基于Common Crawl的数学语料因提取方法不稳定、HTML转文本损失、数学结构难以保留等问题，质量下降，难以满足高效训练需求。

Method: 提出一种域无关的科学文本提取流水线，从Common Crawl中高效恢复MathJax、KaTeX、MathML等格式的数学内容，使用渲染工具保留方程与代码结构，并优化为LaTeX标准格式。

Result: 构建了两个大型高质量数学语料库Nemotron-CC-Math-3+(133B令牌)和Nemotron-CC-Math-4+(52B令牌)。对模型Nemotron-T 8B的预训练测试显示，与基线相比在数学与通用领域测试中获得显著提升。

Conclusion: 该方法首次成功从噪声性大规模网页数据中可靠地提取科学内容并实现跨领域性能提升，同时开源其代码与数据集，为领域发展提供新的基础。

Abstract: Pretraining large language models (LLMs) on high-quality, structured data
such as mathematics and code substantially enhances reasoning capabilities.
However, existing math-focused datasets built from Common Crawl suffer from
degraded quality due to brittle extraction heuristics, lossy HTML-to-text
conversion, and the failure to reliably preserve mathematical structure. In
this work, we introduce Nemotron-CC-Math, a large-scale, high-quality
mathematical corpus constructed from Common Crawl using a novel,
domain-agnostic pipeline specifically designed for robust scientific text
extraction.
  Unlike previous efforts, our pipeline recovers math across various formats
(e.g., MathJax, KaTeX, MathML) by leveraging layout-aware rendering with lynx
and a targeted LLM-based cleaning stage. This approach preserves the structural
integrity of equations and code blocks while removing boilerplate,
standardizing notation into LaTeX representation, and correcting
inconsistencies.
  We collected a large, high-quality math corpus, namely Nemotron-CC-Math-3+
(133B tokens) and Nemotron-CC-Math-4+ (52B tokens). Notably,
Nemotron-CC-Math-4+ not only surpasses all prior open math datasets-including
MegaMath, FineMath, and OpenWebMath-but also contains 5.5 times more tokens
than FineMath-4+, which was previously the highest-quality math pretraining
dataset. When used to pretrain a Nemotron-T 8B model, our corpus yields +4.8 to
+12.6 gains on MATH and +4.6 to +14.3 gains on MBPP+ over strong baselines,
while also improving general-domain performance on MMLU and MMLU-Stem.
  We present the first pipeline to reliably extract scientific
content--including math--from noisy web-scale data, yielding measurable gains
in math, code, and general reasoning, and setting a new state of the art among
open math pretraining corpora. To support open-source efforts, we release our
code and datasets.

</details>


### [77] [Identifying and Answering Questions with False Assumptions: An Interpretable Approach](https://arxiv.org/abs/2508.15139)
*Zijie Wang,Eduardo Blanco*

Main category: cs.CL

TL;DR: 本文研究如何回答带有错误假设的问题，通过引入外部证据减少幻觉，并提出生成与验证原子假设的方法。


<details>
  <summary>Details</summary>
Motivation: 研究如何识别并回答带有错误假设的问题，这些问题由于幻觉（错误信息）导致的误导性回答需要改进。

Method: 通过引入外部证据，减少幻觉，并提出生成和验证原子假设的方法，来提高回答的准确性。

Result: 实验表明，引入证据有效，生成与验证原子假设也能进一步改进性能，并提供可解释的答案。

Conclusion: 使用外部证据和原子假设验证，可更好识别和回答带有错误假设的问题，同时提高答案的可解释性。

Abstract: People often ask questions with false assumptions, a type of question that
does not have regular answers. Answering such questions require first
identifying the false assumptions. Large Language Models (LLMs) often generate
misleading answers because of hallucinations. In this paper, we focus on
identifying and answering questions with false assumptions in several domains.
We first investigate to reduce the problem to fact verification. Then, we
present an approach leveraging external evidence to mitigate hallucinations.
Experiments with five LLMs demonstrate that (1) incorporating retrieved
evidence is beneficial and (2) generating and validating atomic assumptions
yields more improvements and provides an interpretable answer by specifying the
false assumptions.

</details>


### [78] [ContextualLVLM-Agent: A Holistic Framework for Multi-Turn Visually-Grounded Dialogue and Complex Instruction Following](https://arxiv.org/abs/2508.15164)
*Seungmin Han,Haeun Kwon,Ji-jun Park,Taeyang Yoon*

Main category: cs.CL

TL;DR: 本文提出了MMDR-Bench数据集和CoLVLM Agent框架，以解决当前多模态对话模型在复杂任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型和大视觉语言模型难以在复杂、多轮次且需要深度推理的视觉相关任务中表现良好，现有基准数据集也无法准确捕捉真实的多模态交互的复杂性。

Method: 引入MMDR-Bench数据集，设计了300个复杂的多轮对话场景，并提出CoLVLM Agent框架，通过“记忆-感知-规划-执行”的循环强化现有LVLM的推理和指令执行能力，而无需对原模型进行大规模重新训练。

Result: 实验表明，CoLVLM Agent在MMDR-Bench上的性能优于现有先进模型，平均人类评估得分为4.03，高于GPT-4o和Gemini 1.5 Pro，且在推理深度、指令遵循和错误抑制方面表现显著。

Conclusion: CoLVLM Agent通过模块化设计和迭代方法，展示了处理复杂多模态交互的有效性，提供了一种无需大规模重训现有模型的解决方案，并证明了其在长期对话中的鲁棒性。

Abstract: Despite significant advancements in Large Language Models (LLMs) and Large
Vision-Language Models (LVLMs), current models still face substantial
challenges in handling complex, multi-turn, and visually-grounded tasks that
demand deep reasoning, sustained contextual understanding, entity tracking, and
multi-step instruction following. Existing benchmarks often fall short in
capturing the dynamism and intricacies of real-world multi-modal interactions,
leading to issues such as context loss and visual hallucinations. To address
these limitations, we introduce MMDR-Bench (Multi-Modal Dialogue Reasoning
Benchmark), a novel dataset comprising 300 meticulously designed complex
multi-turn dialogue scenarios, each averaging 5-7 turns and evaluated across
six core dimensions including visual entity tracking and reasoning depth.
Furthermore, we propose CoLVLM Agent (Contextual LVLM Agent), a holistic
framework that enhances existing LVLMs with advanced reasoning and instruction
following capabilities through an iterative
"memory-perception-planning-execution" cycle, requiring no extensive
re-training of the underlying models. Our extensive experiments on MMDR-Bench
demonstrate that CoLVLM Agent consistently achieves superior performance,
attaining an average human evaluation score of 4.03, notably surpassing
state-of-the-art commercial models like GPT-4o (3.92) and Gemini 1.5 Pro
(3.85). The framework exhibits significant advantages in reasoning depth,
instruction adherence, and error suppression, and maintains robust performance
over extended dialogue turns, validating the effectiveness of its modular
design and iterative approach for complex multi-modal interactions.

</details>


### [79] [SemToken: Semantic-Aware Tokenization for Efficient Long-Context Language Modeling](https://arxiv.org/abs/2508.15190)
*Dong Liu,Yanxuan Yu*

Main category: cs.CL

TL;DR: 本文提出了一个名为SemToken的新方法，通过语义感知的分词框架，减少了冗余分词并提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有分词方法如BPE或WordPiece过于依赖频率统计，忽略了文本的内在语义结构，无法在长文本场景下有效利用上下文的连贯性。

Method: SemToken通过轻量级编码器提取上下文语义嵌入，并执行局部语义聚类以合并等价语义的token。同时，根据语义密度分配异构token粒度，在信息密集区域使用精细分词，在低熵区域进行压缩处理。

Result: 实验表明，在WikiText-103和LongBench等长文本建模基准测试中，SemToken可减少高达2.4倍的token数量，实现1.9倍的速度提升，同时不影响困惑度及下游任务的准确率。

Conclusion: 语义结构为优化大语言模型的分词和计算提供了一个新的潜在方向，证明了方法的有效性及潜在应用价值。

Abstract: Tokenization plays a critical role in language modeling, yet existing
approaches such as Byte-Pair Encoding (BPE) or WordPiece operate purely on
frequency statistics, ignoring the underlying semantic structure of text. This
leads to over-tokenization of semantically redundant spans and underutilization
of contextual coherence, particularly in long-context scenarios. In this work,
we propose \textbf{SemToken}, a semantic-aware tokenization framework that
jointly reduces token redundancy and improves computation efficiency. SemToken
first extracts contextual semantic embeddings via lightweight encoders and
performs local semantic clustering to merge semantically equivalent tokens.
Then, it allocates heterogeneous token granularity based on semantic density,
allowing finer-grained tokenization in content-rich regions and coarser
compression in repetitive or low-entropy spans. SemToken can be seamlessly
integrated with modern language models and attention acceleration methods.
Experiments on long-context language modeling benchmarks such as WikiText-103
and LongBench show that SemToken achieves up to $2.4\times$ reduction in token
count and $1.9\times$ speedup, with negligible or no degradation in perplexity
and downstream accuracy. Our findings suggest that semantic structure offers a
promising new axis for optimizing tokenization and computation in large
language models.

</details>


### [80] [Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models](https://arxiv.org/abs/2508.15202)
*Yuanchen Zhou,Shuo Jiang,Jie Zhu,Junhui Li,Lifan Guo,Feng Chen,Chi Zhang*

Main category: cs.CL

TL;DR: 本文提出了一个名为Fin-PRM的金融领域专用过程奖励模型，用于评估金融任务中的中间推理步骤，并在金融推理基准上展现了显著优于通用模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的过程奖励模型缺乏在结构化、高符号性以及对事实性和法规有严格要求的金融领域中的有效性，因此需要开发专门的金融领域模型。

Method: 提出了Fin-PRM模型，该模型结合了步骤级和轨迹级奖励监督，支持用于监督微调、强化学习以及测试时最佳推理选择的多种应用场景。

Result: 在CFLUE和FinQA等金融推理基准上，Fin-PRM在轨迹选择质量上优于通用过程奖励模型以及强基线模型，分别提高了监督学习12.9%、强化学习5.2%和测试时推理性能5.1%。

Conclusion: Fin-PRM证明了专门的领域奖励建模对对齐大模型与专家级金融推理能力的价值，并表现出优异的效果。

Abstract: Process Reward Models (PRMs) have emerged as a promising framework for
supervising intermediate reasoning in large language models (LLMs), yet
existing PRMs are primarily trained on general or Science, Technology,
Engineering, and Mathematics (STEM) domains and fall short in domain-specific
contexts such as finance, where reasoning is more structured, symbolic, and
sensitive to factual and regulatory correctness. We introduce \textbf{Fin-PRM},
a domain-specialized, trajectory-aware PRM tailored to evaluate intermediate
reasoning steps in financial tasks. Fin-PRM integrates step-level and
trajectory-level reward supervision, enabling fine-grained evaluation of
reasoning traces aligned with financial logic. We apply Fin-PRM in both offline
and online reward learning settings, supporting three key applications: (i)
selecting high-quality reasoning trajectories for distillation-based supervised
fine-tuning, (ii) providing dense process-level rewards for reinforcement
learning, and (iii) guiding reward-informed Best-of-N inference at test time.
Experimental results on financial reasoning benchmarks, including CFLUE and
FinQA, demonstrate that Fin-PRM consistently outperforms general-purpose PRMs
and strong domain baselines in trajectory selection quality. Downstream models
trained with Fin-PRM yield substantial improvements with baselines, with gains
of 12.9\% in supervised learning, 5.2\% in reinforcement learning, and 5.1\% in
test-time performance. These findings highlight the value of domain-specialized
reward modeling for aligning LLMs with expert-level financial reasoning. Our
project resources will be available at https://github.com/aliyun/qwen-dianjin.

</details>


### [81] [SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning](https://arxiv.org/abs/2508.15212)
*Huanxuan Liao,Yixing Xu,Shizhu He,Guanchen Li,Xuanwu Yin,Dong Li,Emad Barsoum,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: SPARK是一种无需训练的KV缓存压缩方法，通过在通道级别进行稀疏修剪，同时在注意力分数计算时动态恢复修剪的条目，从而在相同内存预算下支持处理更长的序列。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩方法未充分考虑特征维度上的重要性差异，导致效率与模型准确性平衡有限。

Method: SPARK应用无结构稀疏性，对KV缓存进行通道级别的修剪，并在计算注意力分数时动态恢复修剪内容，且与现有KV压缩技术兼容。

Result: SPARK在同等序列长度下提升模型效率和精度，减少KV缓存存储占用超过30%；即使在高修剪率（80%）情况下，性能下降也不到5%。

Conclusion: SPARK证明了其在减少KV缓存冗余、支持更长序列处理以及维持模型性能方面的高效性和稳健性。

Abstract: Long-context inference in large language models (LLMs) is increasingly
constrained by the KV cache bottleneck: memory usage grows linearly with
sequence length, while attention computation scales quadratically. Existing
approaches address this issue by compressing the KV cache along the temporal
axis through strategies such as token eviction or merging to reduce memory and
computational overhead. However, these methods often neglect fine-grained
importance variations across feature dimensions (i.e., the channel axis),
thereby limiting their ability to effectively balance efficiency and model
accuracy. In reality, we observe that channel saliency varies dramatically
across both queries and positions: certain feature channels carry near-zero
information for a given query, while others spike in relevance. To address this
oversight, we propose SPARK, a training-free plug-and-play method that applies
unstructured sparsity by pruning KV at the channel level, while dynamically
restoring the pruned entries during attention score computation. Notably, our
approach is orthogonal to existing KV compression and quantization techniques,
making it compatible for integration with them to achieve further acceleration.
By reducing channel-level redundancy, SPARK enables processing of longer
sequences within the same memory budget. For sequences of equal length, SPARK
not only preserves or improves model accuracy but also reduces KV cache storage
by over 30% compared to eviction-based methods. Furthermore, even with an
aggressive pruning ratio of 80%, SPARK maintains performance with less
degradation than 5% compared to the baseline eviction method, demonstrating its
robustness and effectiveness. Our code will be available at
https://github.com/Xnhyacinth/SparK.

</details>


### [82] [Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering](https://arxiv.org/abs/2508.15213)
*Bolei He,Xinran He,Run Shao,Shanfu Shu,Xianwei Xue,Mingquan Cheng,Haifeng Li,Zhenhua Ling*

Main category: cs.CL

TL;DR: 提出了Selct2Know(S2K)框架，以改进在领域特定问题上的表现，并降低训练和推理成本。


<details>
  <summary>Details</summary>
Motivation: LLMs在领域特定场景的表现有限，通过RAG改进会引入不精确性，领域预训练虽然有效但缺乏灵活性，且成本高，因此需要一种高效且灵活的知识获取方式。

Method: 通过一种内部-外部知识自选择策略与选择性监督微调的方式实现知识内化，同时引入结构化推理数据生成管道和GRPO来提升推理能力。

Result: S2K在医疗、法律和金融的QA基准测试中表现出色，超过现有方法，并以显著较低的成本匹配领域预训练的LLM性能。

Conclusion: 提出的S2K框架是一种在成本和效果之间平衡的高效解决方案，能够在多领域基准任务中取得优异成果。

Abstract: Large Language Models (LLMs) perform well in general QA but often struggle in
domain-specific scenarios. Retrieval-Augmented Generation (RAG) introduces
external knowledge but suffers from hallucinations and latency due to noisy
retrievals. Continued pretraining internalizes domain knowledge but is costly
and lacks cross-domain flexibility. We attribute this challenge to the
long-tail distribution of domain knowledge, which leaves partial yet useful
internal knowledge underutilized. We further argue that knowledge acquisition
should be progressive, mirroring human learning: first understanding concepts,
then applying them to complex reasoning. To address this, we propose Selct2Know
(S2K), a cost-effective framework that internalizes domain knowledge through an
internal-external knowledge self-selection strategy and selective supervised
fine-tuning. We also introduce a structured reasoning data generation pipeline
and integrate GRPO to enhance reasoning ability. Experiments on medical, legal,
and financial QA benchmarks show that S2K consistently outperforms existing
methods and matches domain-pretrained LLMs with significantly lower cost.

</details>


### [83] [Self-Guided Function Calling in Large Language Models via Stepwise Experience Recall](https://arxiv.org/abs/2508.15214)
*Sijia Cui,Aiyao He,Shuai Xu,Hongming Zhang,Yanna Wang,Qingyang Zhang,Yajing Wang,Bo Xu*

Main category: cs.CL

TL;DR: 提出了一种名为Stepwise Experience Recall (SEER) 的新方法，通过细粒度、逐步的经验池检索解决LLMs在多步骤工具使用中的挑战，并取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在多步骤工具使用场景中工具选择、参数生成和工具链规划的困难，同时克服现有依赖手动设计示例和库检索方法的复杂性。

Method: 提出了一种名为SEER的自引导方法，该方法通过不断更新的经验池进行细粒度逐步检索，从过去的成功轨迹中增量增强经验池，从而实现性能的持续提升。

Result: 在ToolQA基准测试中，SEER在简单问题上平均提升6.1%，在困难问题上提升4.7%；在τ-bench中，SEER在两个真实世界领域分别实现了7.44%和23.38%的显著准确率提升。

Conclusion: SEER方法有效解决了模型在使用复杂工具时的不足，通过逐步积累经验持续改进表现，为模型在真实世界任务和复杂场景中取得高性能提供了可行的解决方案。

Abstract: Function calling enables large language models (LLMs) to interact with
external systems by leveraging tools and APIs. When faced with multi-step tool
usage, LLMs still struggle with tool selection, parameter generation, and
tool-chain planning. Existing methods typically rely on manually designing
task-specific demonstrations, or retrieving from a curated library. These
approaches demand substantial expert effort and prompt engineering becomes
increasingly complex and inefficient as tool diversity and task difficulty
scale. To address these challenges, we propose a self-guided method, Stepwise
Experience Recall (SEER), which performs fine-grained, stepwise retrieval from
a continually updated experience pool. Instead of relying on static or manually
curated library, SEER incrementally augments the experience pool with past
successful trajectories, enabling continuous expansion of the pool and improved
model performance over time. Evaluated on the ToolQA benchmark, SEER achieves
an average improvement of 6.1\% on easy and 4.7\% on hard questions. We further
test SEER on $\tau$-bench, which includes two real-world domains. Powered by
Qwen2.5-7B and Qwen2.5-72B models, SEER demonstrates substantial accuracy gains
of 7.44\% and 23.38\%, respectively.

</details>


### [84] [Are Checklists Really Useful for Automatic Evaluation of Generative Tasks?](https://arxiv.org/abs/2508.15218)
*Momoka Furuhashi,Kouta Nakayama,Takashi Kodama,Saku Sugawara*

Main category: cs.CL

TL;DR: 研究自动生成任务中评估大语言模型的有效性，提出了生成和选择性使用评估清单的方法，并发现其在某些评估场景中有助于提升效果。


<details>
  <summary>Details</summary>
Motivation: 当前生成任务自动评估面临标准模糊问题，评估清单作为一种有潜力的解决方案尚未得到充分探索。

Method: 提出选择性地使用评估清单，尝试了六种生成清单的方法，并在八种模型规模下评估清单的有效性，分析其与人工评估的相关性。

Result: 在对比和直接评分任务的实验表明，选择性使用清单在对比设置中提升效果，但在直接评分中效果不稳定。低相关性的清单项常反映人工标准的不一致性。

Conclusion: 研究突出了明确定义评估标准的重要性，以改善人工和自动评估表现。

Abstract: Automatic evaluation of generative tasks using large language models faces
challenges due to ambiguous criteria. Although automatic checklist generation
is a potentially promising approach, its usefulness remains underexplored. We
investigate whether checklists should be used for all questions or selectively,
generate them using six methods, evaluate their effectiveness across eight
model sizes, and identify checklist items that correlate with human
evaluations. Through experiments on pairwise comparison and direct scoring
tasks, we find that selective checklist use tends to improve evaluation
performance in pairwise settings, while its benefits are less consistent in
direct scoring. Our analysis also shows that even checklist items with low
correlation to human scores often reflect human-written criteria, indicating
potential inconsistencies in human evaluation. These findings highlight the
need to more clearly define objective evaluation criteria to guide both human
and automatic evaluations. \footnote{Our code is available
at~https://github.com/momo0817/checklist-effectiveness-study

</details>


### [85] [VocabTailor: Dynamic Vocabulary Selection for Downstream Tasks in Small Language Models](https://arxiv.org/abs/2508.15229)
*Hanling Zhang,Yayu Zhou,Tongcheng Fang,Zhihang Yuan,Guohao Dai,Yu Wang*

Main category: cs.CL

TL;DR: 该论文提出了一个名为VocabTailor的新框架，可通过动态词汇选择策略大幅降低小型语言模型(SLM)的内存使用，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有小型语言模型在资源有限的边缘设备中部署时，其内存瓶颈主要来自于大规模词汇表相关组件；现有静态词汇修剪方法刚性且信息损失大，缺乏灵活性。

Method: 提出了一个新的动态词汇选择框架VocabTailor，利用词汇局部性原则和计算特性不对称性，从而实现嵌入的内存卸载和语言模型头部的混合静态-动态词汇选择策略。

Result: 实验表明，VocabTailor实现了词汇相关组件内存使用的高达99%的减少，同时任务性能几乎未受影响，相较于现有静态词汇修剪方法表现更优。

Conclusion: VocabTailor证明了动态词汇修剪在大幅缓解SLM内存瓶颈的同时，可以有效维护任务性能，为在资源受限设备上的SLM部署提供了新的解决思路。

Abstract: Small Language Models (SLMs) provide computational advantages in
resource-constrained environments, yet memory limitations remain a critical
bottleneck for edge device deployment. A substantial portion of SLMs' memory
footprint stems from vocabulary-related components, particularly embeddings and
language modeling (LM) heads, due to large vocabulary sizes. Existing static
vocabulary pruning, while reducing memory usage, suffers from rigid,
one-size-fits-all designs that cause information loss from the prefill stage
and a lack of flexibility. In this work, we identify two key principles
underlying the vocabulary reduction challenge: the lexical locality principle,
the observation that only a small subset of tokens is required during any
single inference, and the asymmetry in computational characteristics between
vocabulary-related components of SLM. Based on these insights, we introduce
VocabTailor, a novel decoupled dynamic vocabulary selection framework that
addresses memory constraints through offloading embedding and implements a
hybrid static-dynamic vocabulary selection strategy for LM Head, enabling
on-demand loading of vocabulary components. Comprehensive experiments across
diverse downstream tasks demonstrate that VocabTailor achieves a reduction of
up to 99% in the memory usage of vocabulary-related components with minimal or
no degradation in task performance, substantially outperforming existing static
vocabulary pruning.

</details>


### [86] [WangchanThaiInstruct: An instruction-following Dataset for Culture-Aware, Multitask, and Multi-domain Evaluation in Thai](https://arxiv.org/abs/2508.15239)
*Peerat Limkonchotiwat,Pume Tuchinda,Lalita Lowphansirikul,Surapon Nonesung,Panuthep Tasawong,Alham Fikri Aji,Can Udomcharoenchaikit,Sarana Nutanong*

Main category: cs.CL

TL;DR: 提出了WangchanThaiInstruct，一个专注于泰语任务的人类编写的数据集，用于评估和指令微调，证明了使用本地监督数据的优势。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在低资源语言（如泰语）的表现尚不明确，现有基准无法涵盖文化和领域特定的细微差别，无法满足实际需求。

Method: 通过多阶段质量控制流程创建了一个涵盖四个专业领域和七种任务类型的泰语数据集并进行了两项研究：零样本评估和指令微调实验。

Result: 微调使用WangchanThaiInstruct的数据集的模型在域内和域外基准中均优于使用翻译数据的模型。

Conclusion: 结果表明，基于文化和专业背景的指令数据在提升低资源和语言多样性环境中的大语言模型对齐能力方面至关重要。

Abstract: Large language models excel at instruction-following in English, but their
performance in low-resource languages like Thai remains underexplored. Existing
benchmarks often rely on translations, missing cultural and domain-specific
nuances needed for real-world use. We present WangchanThaiInstruct, a
human-authored Thai dataset for evaluation and instruction tuning, covering
four professional domains and seven task types. Created through a multi-stage
quality control process with annotators, domain experts, and AI researchers,
WangchanThaiInstruct supports two studies: (1) a zero-shot evaluation showing
performance gaps on culturally and professionally specific tasks, and (2) an
instruction tuning study with ablations isolating the effect of native
supervision. Models fine-tuned on WangchanThaiInstruct outperform those using
translated data in both in-domain and out-of-domain benchmarks. These findings
underscore the need for culturally and professionally grounded instruction data
to improve LLM alignment in low-resource, linguistically diverse settings.

</details>


### [87] [UniCoM: A Universal Code-Switching Speech Generator](https://arxiv.org/abs/2508.15244)
*Sangmin Lee,Woojin Chung,Seyun Um,Hong-Goo Kang*

Main category: cs.CL

TL;DR: 提出一种称为UniCoM的新方法，可以生成具有高质量和自然性的代码切换语音数据，供多语言语音技术使用。


<details>
  <summary>Details</summary>
Motivation: 解决现有多语言技术中由于缺乏适合数据集而难以处理代码切换的问题。

Method: 利用SWORDS算法，通过将选定的单词替换为翻译词汇，同时考虑词性来生成代码切换语音，并创建CS-FLEURS数据集。

Result: CS-FLEURS数据集在客观和主观指标上表现优异，与现有数据集表现相当，展示出高可理解性和自然性。

Conclusion: 所提方法有助于推动代码切换语音技术发展，促进更包容的多语言语音系统。

Abstract: Code-switching (CS), the alternation between two or more languages within a
single speaker's utterances, is common in real-world conversations and poses
significant challenges for multilingual speech technology. However, systems
capable of handling this phenomenon remain underexplored, primarily due to the
scarcity of suitable datasets. To resolve this issue, we propose Universal
Code-Mixer (UniCoM), a novel pipeline for generating high-quality, natural CS
samples without altering sentence semantics. Our approach utilizes an algorithm
we call Substituting WORDs with Synonyms (SWORDS), which generates CS speech by
replacing selected words with their translations while considering their parts
of speech. Using UniCoM, we construct Code-Switching FLEURS (CS-FLEURS), a
multilingual CS corpus designed for automatic speech recognition (ASR) and
speech-to-text translation (S2TT). Experimental results show that CS-FLEURS
achieves high intelligibility and naturalness, performing comparably to
existing datasets on both objective and subjective metrics. We expect our
approach to advance CS speech technology and enable more inclusive multilingual
systems.

</details>


### [88] [EMNLP: Educator-role Moral and Normative Large Language Models Profiling](https://arxiv.org/abs/2508.15250)
*Yilin Jiang,Mingzi Zhang,Sheng Jin,Zengyi Yu,Xiangjie Kong,Binghao Tu*

Main category: cs.CL

TL;DR: 本文开发了一个名为EMNLP的框架，用于评估教师角色大语言模型的心理和伦理表现，包括个性剖析、道德发展阶段测量以及伦理风险分析。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能够模拟专业角色，但目前在心理和伦理评估方面的研究仍然缺乏。

Method: 提出了EMNLP框架，设计了88个特定的教师伦理困境场景，并使用软提示注入评估教师角色中的合规性和易受攻击性。

Result: 实验表明，教师角色的大语言模型在抽象道德推理方面优于人类教师，但在情感复杂情况下表现较差。此外，推理能力强的模型更容易受到有害提示注入的影响，温度等超参数对风险行为的影响有限。

Conclusion: 本研究提出了第一个用于评估教育AI中教师角色的伦理和心理对齐性的基准，资源可在官网获取。

Abstract: Simulating Professions (SP) enables Large Language Models (LLMs) to emulate
professional roles. However, comprehensive psychological and ethical evaluation
in these contexts remains lacking. This paper introduces EMNLP, an
Educator-role Moral and Normative LLMs Profiling framework for personality
profiling, moral development stage measurement, and ethical risk under soft
prompt injection. EMNLP extends existing scales and constructs 88
teacher-specific moral dilemmas, enabling profession-oriented comparison with
human teachers. A targeted soft prompt injection set evaluates compliance and
vulnerability in teacher SP. Experiments on 12 LLMs show teacher-role LLMs
exhibit more idealized and polarized personalities than human teachers, excel
in abstract moral reasoning, but struggle with emotionally complex situations.
Models with stronger reasoning are more vulnerable to harmful prompt injection,
revealing a paradox between capability and safety. The model temperature and
other hyperparameters have limited influence except in some risk behaviors.
This paper presents the first benchmark to assess ethical and psychological
alignment of teacher-role LLMs for educational AI. Resources are available at
https://e-m-n-l-p.github.io/.

</details>


### [89] [Conflict-Aware Soft Prompting for Retrieval-Augmented Generation](https://arxiv.org/abs/2508.15253)
*Eunseong Choi,June Park,Hyeri Lee,Jongwuk Lee*

Main category: cs.CL

TL;DR: 该论文介绍了一种名为CARE的新方法，通过上下文评估器和一个基础模型来解决回复增强生成（RAG）中的上下文记忆冲突问题，提高了可靠性，并提升了5％的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在处理来自外部检索的与LLM内部记忆相矛盾的信息时，效率较低，容易出错。为了解决这一问题，提出了CARE方法。

Method: 方法中包含一个上下文评估器和基础LLM。通过基于上下文的嵌入训练，结合软提示技术，使系统能够识别不可靠的信息，同时指引模型依据更可靠的知识源进行推理。

Result: CARE系统在问答和事实核查任务中平均性能提高了5.0％，证明该方法显著减少了上下文记忆冲突的负面影响。

Conclusion: CARE方法提供了一种新的改进方向，为实现高可行性和高适应性的RAG系统奠定了基础。

Abstract: Retrieval-augmented generation (RAG) enhances the capabilities of large
language models (LLMs) by incorporating external knowledge into their input
prompts. However, when the retrieved context contradicts the LLM's parametric
knowledge, it often fails to resolve the conflict between incorrect external
context and correct parametric knowledge, known as context-memory conflict. To
tackle this problem, we introduce Conflict-Aware REtrieval-Augmented Generation
(CARE), consisting of a context assessor and a base LLM. The context assessor
encodes compact memory token embeddings from raw context tokens. Through
grounded/adversarial soft prompting, the context assessor is trained to discern
unreliable context and capture a guidance signal that directs reasoning toward
the more reliable knowledge source. Extensive experiments show that CARE
effectively mitigates context-memory conflicts, leading to an average
performance gain of 5.0\% on QA and fact-checking benchmarks, establishing a
promising direction for trustworthy and adaptive RAG systems.

</details>


### [90] [TComQA: Extracting Temporal Commonsense from Text](https://arxiv.org/abs/2508.15274)
*Lekshmi R Nair,Arun Sankar,Koninika Pal*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLMs）在提取事件的时间常识方面的能力，并提出了一种时间常识提取流程来创建TComQA数据集，同时提升了时间问答任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在推理涉及时间常识的问题时表现欠佳，而时间常识在文本中又很少被明确提及。因此，自动挖掘事件的时间常识有助于建立更强大的语言模型。

Method: 提出了一种时间常识提取流程，利用LLMs自动挖掘时间常识，从SAMSum和RealNews语料库中创建了TComQA数据集；通过众包验证保证数据集质量，并与现有数据集进行对比训练。

Result: TComQA数据集在提取时间常识方面达到了80%以上的精度，并显著提升了时间问答任务上的模型性能。

Conclusion: 通过构建高质量的TComQA数据集和改进的时间常识提取流程，有效提升了语言模型在时间推理任务中的表现。

Abstract: Understanding events necessitates grasping their temporal context, which is
often not explicitly stated in natural language. For example, it is not a
trivial task for a machine to infer that a museum tour may last for a few
hours, but can not take months. Recent studies indicate that even advanced
large language models (LLMs) struggle in generating text that require reasoning
with temporal commonsense due to its infrequent explicit mention in text.
Therefore, automatically mining temporal commonsense for events enables the
creation of robust language models. In this work, we investigate the capacity
of LLMs to extract temporal commonsense from text and evaluate multiple
experimental setups to assess their effectiveness. Here, we propose a temporal
commonsense extraction pipeline that leverages LLMs to automatically mine
temporal commonsense and use it to construct TComQA, a dataset derived from
SAMSum and RealNews corpora. TComQA has been validated through crowdsourcing
and achieves over 80\% precision in extracting temporal commonsense. The model
trained with TComQA also outperforms an LLM fine-tuned on existing dataset of
temporal question answering task.

</details>


### [91] [CUPE: Contextless Universal Phoneme Encoder for Language-Agnostic Speech Processing](https://arxiv.org/abs/2508.15316)
*Abdul Rehman,Jian-Jun Zhang,Xiaosong Yang*

Main category: cs.CL

TL;DR: 本论文提出CUPE模型，通过建模单个音素长度的基本声学模式，实现高效的跨语言语音处理。


<details>
  <summary>Details</summary>
Motivation: 解决需要去除语境影响、提取纯粹音素表征的需求。

Method: CUPE模型基于短而固定宽度的窗口进行独立处理，专注捕获120毫秒音素相关的关键特征。

Result: CUPE尽管参数数量较少，但表现出与现有模型竞争的跨语言性能，评估在多个语言数据集上均表现优秀。

Conclusion: 建模单个音素长度的基本声学模式，可实现有效的跨语言语音处理。

Abstract: Universal phoneme recognition typically requires analyzing long speech
segments and language-specific patterns. Many speech processing tasks require
pure phoneme representations free from contextual influence, which motivated
our development of CUPE - a lightweight model that captures key phoneme
features in just 120 milliseconds, about one phoneme's length. CUPE processes
short, fixed-width windows independently and, despite fewer parameters than
current approaches, achieves competitive cross-lingual performance by learning
fundamental acoustic patterns common to all languages. Our extensive evaluation
through supervised and self-supervised training on diverse languages, including
zero-shot tests on the UCLA Phonetic Corpus, demonstrates strong cross-lingual
generalization and reveals that effective universal speech processing is
possible through modeling basic acoustic patterns within phoneme-length
windows.

</details>


### [92] [KG-EDAS: A Meta-Metric Framework for Evaluating Knowledge Graph Completion Models](https://arxiv.org/abs/2508.15357)
*Haji Gul,Abul Ghani Naim,Ajaz Ahmad Bhat*

Main category: cs.CL

TL;DR: 提出EDAS元度量，通过综合多个数据集和评估标准，为知识图谱完成模型的评估提供统一框架。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法在多数据集和多指标中存在不一致性，导致模型评估复杂化，无法整体比较和合理选择。

Method: 设计EDAS元度量，将多数据集和多指标的模型表现合成为一个统一的归一化分数，提供全局视角。

Result: 实验表明，EDAS在如FB15k-237和WN18RR等基准数据集上，实现了一致、稳健且可泛化的KGC模型评估。

Conclusion: EDAS为多评估维度及多数据集模型评测提供了更公平可靠的统一框架，有助于模型选择与比较。

Abstract: Knowledge Graphs (KGs) enable applications in various domains such as
semantic search, recommendation systems, and natural language processing. KGs
are often incomplete, missing entities and relations, an issue addressed by
Knowledge Graph Completion (KGC) methods that predict missing elements.
Different evaluation metrics, such as Mean Reciprocal Rank (MRR), Mean Rank
(MR), and Hit@k, are commonly used to assess the performance of such KGC
models. A major challenge in evaluating KGC models, however, lies in comparing
their performance across multiple datasets and metrics. A model may outperform
others on one dataset but underperform on another, making it difficult to
determine overall superiority. Moreover, even within a single dataset,
different metrics such as MRR and Hit@1 can yield conflicting rankings, where
one model excels in MRR while another performs better in Hit@1, further
complicating model selection for downstream tasks. These inconsistencies hinder
holistic comparisons and highlight the need for a unified meta-metric that
integrates performance across all metrics and datasets to enable a more
reliable and interpretable evaluation framework. To address this need, we
propose KG Evaluation based on Distance from Average Solution (EDAS), a robust
and interpretable meta-metric that synthesizes model performance across
multiple datasets and diverse evaluation criteria into a single normalized
score ($M_i \in [0,1]$). Unlike traditional metrics that focus on isolated
aspects of performance, EDAS offers a global perspective that supports more
informed model selection and promotes fairness in cross-dataset evaluation.
Experimental results on benchmark datasets such as FB15k-237 and WN18RR
demonstrate that EDAS effectively integrates multi-metric, multi-dataset
performance into a unified ranking, offering a consistent, robust, and
generalizable framework for evaluating KGC models.

</details>


### [93] [A Survey on Large Language Model Benchmarks](https://arxiv.org/abs/2508.15361)
*Shiwen Ni,Guhong Chen,Shuaimin Li,Xuanang Chen,Siyi Li,Bingli Wang,Qiyao Wang,Xingjian Wang,Yifan Zhang,Liyang Fan,Chengming Li,Ruifeng Xu,Le Sun,Min Yang*

Main category: cs.CL

TL;DR: 本文系统性回顾了大型语言模型基准测试的现状和发展，分类分析了283个代表性基准测试并指出其问题，同时提出了未来基准测试设计的参考框架。


<details>
  <summary>Details</summary>
Motivation: 基准测试作为定量评估模型性能的工具，是衡量模型能力的核心手段，也是推动模型发展的关键要素。综述当前基准测试的状态和发展，能够指导未来的创新方向。

Method: 将283个基准测试分为三类：通用能力、领域特定和目标特定，然后分析问题并提出设计规范。

Result: 总结出当前基准测试中存在数据污染导致的评分膨胀、文化语言偏见导致的不公平评估，以及缺乏过程可信性评估等问题。

Conclusion: 建立更科学的基准测试设计框架，可以更好地支持模型能力评价并推动技术创新。

Abstract: In recent years, with the rapid development of the depth and breadth of large
language models' capabilities, various corresponding evaluation benchmarks have
been emerging in increasing numbers. As a quantitative assessment tool for
model performance, benchmarks are not only a core means to measure model
capabilities but also a key element in guiding the direction of model
development and promoting technological innovation. We systematically review
the current status and development of large language model benchmarks for the
first time, categorizing 283 representative benchmarks into three categories:
general capabilities, domain-specific, and target-specific. General capability
benchmarks cover aspects such as core linguistics, knowledge, and reasoning;
domain-specific benchmarks focus on fields like natural sciences, humanities
and social sciences, and engineering technology; target-specific benchmarks pay
attention to risks, reliability, agents, etc. We point out that current
benchmarks have problems such as inflated scores caused by data contamination,
unfair evaluation due to cultural and linguistic biases, and lack of evaluation
on process credibility and dynamic environments, and provide a referable design
paradigm for future benchmark innovation.

</details>


### [94] [Unveiling Trust in Multimodal Large Language Models: Evaluation, Analysis, and Mitigation](https://arxiv.org/abs/2508.15370)
*Yichi Zhang,Yao Huang,Yifan Wang,Yitong Sun,Chang Liu,Zhe Zhao,Zhengwei Fang,Huanran Chen,Xiao Yang,Xingxing Wei,Hang Su,Yinpeng Dong,Jun Zhu*

Main category: cs.CL

TL;DR: 论文介绍了一种新的评估工具MultiTrust-X，用于全面评估和改进多模态大语言模型（MLLMs）的可信度，提出了一套三维框架和具体的改进策略，并通过实验揭示了当前模型在可信度和性能上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在可信度方面仍存在显著问题，现有的评估和改进方法对多模态问题的考虑不足，因此需要一种全面的工具来评估和改进这些问题。

Method: 提出了MultiTrust-X，一个涵盖三维框架的综合评估工具，包括五个可信度维度（真实性、鲁棒性、安全性、公平性和隐私性）、两种风险类型（多模态风险和跨模态影响）以及多种缓解策略。评估涵盖了32个任务、28个数据集以及对30种开源和专有模型的深入分析。

Result: 实验揭示了现有多模态模型的显著漏洞，包括可信度与通用能力之间的差距，以及因多模态训练和推理引发的潜在风险放大。还发现目前的缓解策略在总体可信度改进上的局限性，并提出了加强推理能力以平衡安全和性能的方法。

Conclusion: 提出的Reasoning-Enhanced Safety Alignment (RESA)方法通过链式推理能力提升了模型发现潜在风险的能力，达到了最新的研究成果，并为提高模型可信度提供了实践指导。

Abstract: The trustworthiness of Multimodal Large Language Models (MLLMs) remains an
intense concern despite the significant progress in their capabilities.
Existing evaluation and mitigation approaches often focus on narrow aspects and
overlook risks introduced by the multimodality. To tackle these challenges, we
propose MultiTrust-X, a comprehensive benchmark for evaluating, analyzing, and
mitigating the trustworthiness issues of MLLMs. We define a three-dimensional
framework, encompassing five trustworthiness aspects which include
truthfulness, robustness, safety, fairness, and privacy; two novel risk types
covering multimodal risks and cross-modal impacts; and various mitigation
strategies from the perspectives of data, model architecture, training, and
inference algorithms. Based on the taxonomy, MultiTrust-X includes 32 tasks and
28 curated datasets, enabling holistic evaluations over 30 open-source and
proprietary MLLMs and in-depth analysis with 8 representative mitigation
methods. Our extensive experiments reveal significant vulnerabilities in
current models, including a gap between trustworthiness and general
capabilities, as well as the amplification of potential risks in base LLMs by
both multimodal training and inference. Moreover, our controlled analysis
uncovers key limitations in existing mitigation strategies that, while some
methods yield improvements in specific aspects, few effectively address overall
trustworthiness, and many introduce unexpected trade-offs that compromise model
utility. These findings also provide practical insights for future
improvements, such as the benefits of reasoning to better balance safety and
performance. Based on these insights, we introduce a Reasoning-Enhanced Safety
Alignment (RESA) approach that equips the model with chain-of-thought reasoning
ability to discover the underlying risks, achieving state-of-the-art results.

</details>


### [95] [Confidence-Modulated Speculative Decoding for Large Language Models](https://arxiv.org/abs/2508.15371)
*Jaydip Sen,Subhasis Dasgupta,Hetvi Waghela*

Main category: cs.CL

TL;DR: 本文提出了一种基于信息论的推测性解码框架，通过动态调整草稿生成长度和验证标准，提升了解码效率和灵活性，同时维持输出质量。


<details>
  <summary>Details</summary>
Motivation: 现有的推测性解码方法由于依赖静态草稿长度和刚性的验证标准，在不同模型不确定性和输入复杂度下适应性不足。

Method: 利用基于熵和边际的模型不确定性度量，动态调整每次推测生成的token数量，并使用同样的置信度信号调节验证过程。

Result: 在机器翻译和摘要任务中，该方法显著提升了解码速度，并在BLEU和ROUGE分数上保持或有所提升。

Conclusion: 该方法是一种高效且稳健的解码插件，适用于在不同不确定性条件下的高效解码任务。

Abstract: Speculative decoding has emerged as an effective approach for accelerating
autoregressive inference by parallelizing token generation through a
draft-then-verify paradigm. However, existing methods rely on static drafting
lengths and rigid verification criteria, limiting their adaptability across
varying model uncertainties and input complexities. This paper proposes an
information-theoretic framework for speculative decoding based on
confidence-modulated drafting. By leveraging entropy and margin-based
uncertainty measures over the drafter's output distribution, the proposed
method dynamically adjusts the number of speculatively generated tokens at each
iteration. This adaptive mechanism reduces rollback frequency, improves
resource utilization, and maintains output fidelity. Additionally, the
verification process is modulated using the same confidence signals, enabling
more flexible acceptance of drafted tokens without sacrificing generation
quality. Experiments on machine translation and summarization tasks demonstrate
significant speedups over standard speculative decoding while preserving or
improving BLEU and ROUGE scores. The proposed approach offers a principled,
plug-in method for efficient and robust decoding in large language models under
varying conditions of uncertainty.

</details>


### [96] [Exploiting Vocabulary Frequency Imbalance in Language Model Pre-training](https://arxiv.org/abs/2508.15390)
*Woojin Chung,Jeonghoon Kim*

Main category: cs.CL

TL;DR: 本研究探讨了语言模型词库大小对性能的影响，发现更大的词库通过降低词频不平衡提升了常见词的预测能力，从而优化了模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有实践倾向于使用更大的词库，但其提升性能的原因尚不明确。本研究通过控制变量实验探讨更大词库的作用机制。

Method: 通过在持数据、计算资源和优化方法不变的情况下，将语言模型的词库从24K扩展到196K，并通过Kolmogorov复杂性公式量化文本复杂性；从词级交叉熵分解分析词库增长对模型性能的影响。同时约束输入输出嵌入以减轻词频不平衡效应，验证模型对不平衡的利用关系。

Result: 发现更大词库通过降低词频不平衡优化了 2,500 个最常见词的预测，并转移到下游任务中实现性能提升；但稀有词损失增加。同时增大模型参数在固定词库下同样获得类似的常见词优势。

Conclusion: “更大词库更优”实质是降低文本复杂性有助于性能提升，研究澄清了预训练语言模型词库与标记策略设计的损失动态机制。

Abstract: Large language models are trained with tokenizers, and the resulting token
distribution is highly imbalanced: a few words dominate the stream while most
occur rarely. Recent practice favors ever-larger vocabularies, but the source
of the benefit is unclear. We conduct a controlled study that scales the
language model's vocabulary from 24K to 196K while holding data, compute, and
optimization fixed. We first quantify the complexity of tokenized text,
formalized via Kolmogorov complexity, and show that larger vocabularies reduce
this complexity. Above 24K, every common word is already a single token, so
further growth mainly deepens the relative token-frequency imbalance. A
word-level loss decomposition shows that larger vocabularies reduce
cross-entropy almost exclusively by lowering uncertainty on the 2,500 most
frequent words, even though loss on the rare tail rises. Constraining input and
output embedding norms to attenuate the effect of token-frequency imbalance
reverses the gain, directly showing that the model exploits rather than suffers
from imbalance. Because the same frequent words cover roughly 77% of tokens in
downstream benchmarks, this training advantage transfers intact. We also show
that enlarging model parameters with a fixed vocabulary yields the same
frequent-word benefit. Our results reframe "bigger vocabularies help" as
"lowering the complexity of tokenized text helps," providing a simple,
principled lever for tokenizer-model co-design and clarifying the loss dynamics
that govern language-model scaling in pre-training.

</details>


### [97] [Attribution, Citation, and Quotation: A Survey of Evidence-based Text Generation with Large Language Models](https://arxiv.org/abs/2508.15396)
*Tobias Schreieder,Tim Schopf,Michael Färber*

Main category: cs.CL

TL;DR: 本研究系统分析了134篇论文，提出了基于大型语言模型的证据生成的统一分类法，并研究了300种评估指标以解决术语不一致、孤立的评估实践和缺乏统一基准的问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，其可靠性和可信性问题日益引起关注，需要开发基于证据的文本生成方法以确保可追溯性和可验证性。

Method: 系统分析134篇相关论文，引入统一的证据生成分类法，并研究300种评估指标的七个关键维度，同时审视运用引用、归因或引述进行证据生成的方法。

Result: 厘清了领域的主要特征和代表性方法，并指出了开放性挑战和未来方向。

Conclusion: 研究填补了领域内的术语和评估实践不一致的空白，为基于证据的文本生成提供了详细的分析和未来研究方向。

Abstract: The increasing adoption of large language models (LLMs) has been accompanied
by growing concerns regarding their reliability and trustworthiness. As a
result, a growing body of research focuses on evidence-based text generation
with LLMs, aiming to link model outputs to supporting evidence to ensure
traceability and verifiability. However, the field is fragmented due to
inconsistent terminology, isolated evaluation practices, and a lack of unified
benchmarks. To bridge this gap, we systematically analyze 134 papers, introduce
a unified taxonomy of evidence-based text generation with LLMs, and investigate
300 evaluation metrics across seven key dimensions. Thereby, we focus on
approaches that use citations, attribution, or quotations for evidence-based
text generation. Building on this, we examine the distinctive characteristics
and representative methods in the field. Finally, we highlight open challenges
and outline promising directions for future work.

</details>


### [98] [When Audio and Text Disagree: Revealing Text Bias in Large Audio-Language Models](https://arxiv.org/abs/2508.15407)
*Cheng Wang,Gelei Deng,Xianglin Yang,Han Qiu,Tianwei Zhang*

Main category: cs.CL

TL;DR: 本论文提出并使用MCR-BENCH基准测试，研究了大型音频语言模型（LALMs）在音频与文本信息冲突时的表现。


<details>
  <summary>Details</summary>
Motivation: 对多模态输入中音频与文本冲突时的处理能力进行评估，以揭示潜在问题并提出改进方向。

Method: 开发并利用MCR-BENCH基准，通过广泛的评估分析LALMs对不一致音频文本输入的优先级及其影响因素，同时尝试通过监督微调等手段缓解文本偏差。

Result: 发现LALMs对文本强烈偏好，即使音频与文本信息冲突，模型仍倾向忽视音频，从而导致音频任务性能下降。同时，分析模型中持续的过度自信现象。

Conclusion: 需要在训练中提升模态平衡能力，并采用更先进的融合机制，增强模型在面对多模态冲突时的鲁棒性。

Abstract: Large Audio-Language Models (LALMs) are enhanced with audio perception
capabilities, enabling them to effectively process and understand multimodal
inputs that combine audio and text. However, their performance in handling
conflicting information between audio and text modalities remains largely
unexamined. This paper introduces MCR-BENCH, the first comprehensive benchmark
specifically designed to evaluate how LALMs prioritize information when
presented with inconsistent audio-text pairs. Through extensive evaluation
across diverse audio understanding tasks, we reveal a concerning phenomenon:
when inconsistencies exist between modalities, LALMs display a significant bias
toward textual input, frequently disregarding audio evidence. This tendency
leads to substantial performance degradation in audio-centric tasks and raises
important reliability concerns for real-world applications. We further
investigate the influencing factors of text bias, and explore mitigation
strategies through supervised finetuning, and analyze model confidence patterns
that reveal persistent overconfidence even with contradictory inputs. These
findings underscore the need for improved modality balance during training and
more sophisticated fusion mechanisms to enhance the robustness when handling
conflicting multi-modal inputs. The project is available at
https://github.com/WangCheng0116/MCR-BENCH.

</details>


### [99] [LLaSO: A Foundational Framework for Reproducible Research in Large Language and Speech Model](https://arxiv.org/abs/2508.15418)
*Yirong Sun,Yizhong Geng,Peidong Wei,Yanjun Chen,Jinghan Yang,Rongfei Chen,Wei Zhang,Xiaoyu Shen*

Main category: cs.CL

TL;DR: Paper介绍了首个完全开源的大规模语音语言模型框架（LLaSO），包括数据集、基准测试及参考模型。


<details>
  <summary>Details</summary>
Motivation: 由于领域中模型架构分散以及缺乏透明性，导致系统性比较和研究的可重复性受阻。

Method: 提出LLaSO框架，包括数据集（LLaSO-Align、LLaSO-Instruct）、评估基准（LLaSO-Eval）以及参考模型（LLaSO-Base）。

Result: LLaSO-Base模型在规范化评分中达到0.72，成为超越同类模型的基线；发现提升训练覆盖范围虽能改进性能，但在纯音频任务上的泛化能力仍有限。

Conclusion: 通过开源全栈数据、评估基准和模型，LLaSO建立了统一标准，有助于加速语音语言模型领域的研究进展。

Abstract: The development of Large Speech-Language Models (LSLMs) has been slowed by
fragmented architectures and a lack of transparency, hindering the systematic
comparison and reproducibility of research. Unlike in the vision-language
domain, the LSLM field suffers from the common practice of releasing model
weights without their corresponding training data and configurations. To
address these critical gaps, we introduce LLaSO, the first fully open,
end-to-end framework for large-scale speech-language modeling. LLaSO provides
the community with three essential resources: (1) LLaSO-Align, a 12M-instance
speech-text alignment corpus; (2) LLaSO-Instruct, a 13.5M-instance multi-task
instruction-tuning dataset; and (3) LLaSO-Eval, a reproducible benchmark for
standardized evaluation. To validate our framework, we build and release
LLaSO-Base, a 3.8B-parameter reference model trained exclusively on our public
data. It achieves a normalized score of 0.72, establishing a strong,
reproducible baseline that surpasses comparable models. Our analysis reveals
that while broader training coverage enhances performance, significant
generalization gaps persist on unseen tasks, particularly in pure audio
scenarios. By releasing the complete stack of data, benchmarks, and models,
LLaSO establishes a foundational open standard to unify research efforts and
accelerate community-driven progress in LSLMs. We release the code, dataset,
pretrained models, and results in https://github.com/EIT-NLP/LLaSO.

</details>


### [100] [A Study of Privacy-preserving Language Modeling Approaches](https://arxiv.org/abs/2508.15421)
*Pritilata Saha,Abhirup Sinha*

Main category: cs.CL

TL;DR: 本文探讨了语言模型的隐私保护问题，提供了隐私保护方法的全面研究及其优缺点。


<details>
  <summary>Details</summary>
Motivation: 语言模型在实际应用中面临隐私泄露的风险，将敏感数据暴露给潜在攻击者，因此需要探索有效的隐私保护方法。

Method: 对现有隐私保护的语言建模方法进行了深入的分析和比较，研究了其优劣势及适用范围。

Result: 研究总结了当前隐私保护技术的优势、局限性，并为未来改进方向提供了见解。

Conclusion: 本研究为语言模型的隐私保护提供了理论基础和未来研究方向，有助于保护个人隐私权。

Abstract: Recent developments in language modeling have increased their use in various
applications and domains. Language models, often trained on sensitive data, can
memorize and disclose this information during privacy attacks, raising concerns
about protecting individuals' privacy rights. Preserving privacy in language
models has become a crucial area of research, as privacy is one of the
fundamental human rights. Despite its significance, understanding of how much
privacy risk these language models possess and how it can be mitigated is still
limited. This research addresses this by providing a comprehensive study of the
privacy-preserving language modeling approaches. This study gives an in-depth
overview of these approaches, highlights their strengths, and investigates
their limitations. The outcomes of this study contribute to the ongoing
research on privacy-preserving language modeling, providing valuable insights
and outlining future research directions.

</details>


### [101] [M-HELP: Using Social Media Data to Detect Mental Health Help-Seeking Signals](https://arxiv.org/abs/2508.15440)
*MSVPJ Sathvik,Zuhair Hasan Shaik,Vivek Gupta*

Main category: cs.CL

TL;DR: 本论文提出一个名为M-Help的新数据集，用于检测社交媒体上的求助行为和相关心理健康问题。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集无法有效识别主动寻求帮助的个人，以及他们心理健康问题的具体成因。

Method: 以M-Help数据集为基础，通过AI模型完成三个任务：检测求助行为、诊断心理健康状况，以及找出相关问题的根本原因。

Result: M-Help 数据集有助于更精准地理解并识别社交媒体用户的求助行为及其潜在心理健康问题。

Conclusion: M-Help 数据集填补了在社交媒体中检测主动求助行为方面的空白，可为心理健康问题提供更全面的分析支持。

Abstract: Mental health disorders are a global crisis. While various datasets exist for
detecting such disorders, there remains a critical gap in identifying
individuals actively seeking help. This paper introduces a novel dataset,
M-Help, specifically designed to detect help-seeking behavior on social media.
The dataset goes beyond traditional labels by identifying not only help-seeking
activity but also specific mental health disorders and their underlying causes,
such as relationship challenges or financial stressors. AI models trained on
M-Help can address three key tasks: identifying help-seekers, diagnosing mental
health conditions, and uncovering the root causes of issues.

</details>


### [102] [Principle Methods of Rendering Non-equivalent Words from Uzbek and Dari to Russian and English](https://arxiv.org/abs/2508.15453)
*Mohammad Ibrahim Qani*

Main category: cs.CL

TL;DR: 本文关注语言中不存在等价词的问题，尤其是翻译中可能导致的误解，并提供方法来解决这种非等价性的问题。


<details>
  <summary>Details</summary>
Motivation: 研究目的是解决在不同语言间翻译中，由文化差异导致的非等价词汇问题，帮助更好地理解源语言和目标语言。

Method: 采用基于文献的研究方法，分析和提出翻译非等价词汇的多种方法和规则。

Result: 完成了25个非等价词从达语和乌兹别克语翻译成英语和俄语的工作。

Conclusion: 提出了解决非等价词汇翻译问题的规则和方法，尽管部分问题已经解决，但仍需进一步努力以补充不足。

Abstract: These pure languages understanding directly relates to translation knowledge
where linguists and translators need to work and research to eradicate
misunderstanding. Misunderstandings mostly appear in non-equivalent words
because there are different local and internal words like food, garment,
cultural and traditional words and others in every notion. Truly, most of these
words do not have equivalent in the target language and these words need to be
worked and find their equivalent in the target language to fully understand the
both languages. The purpose of this research is to introduce the methods of
rendering non-equivalent words professionally from the source language to the
target language and this research has been completed using library-based
research. However, some of these non-equivalent words are already
professionally rendered to the target language but still there many other words
to be rendered. As a result, this research paper includes different ways and
rules of rendering non-equivalent words from source language to the target
language and 25 non-equvalent words have been rendered from Dar & Uzbek into
English and Russian languages.

</details>


### [103] [PyTOD: Programmable Task-Oriented Dialogue with Execution Feedback](https://arxiv.org/abs/2508.15456)
*Alexandru Coca,Bo-Hsiang Tseng,Pete Boothroyd,Jianpeng Cheng,Mark Gaynor,Zhenxing Zhang,Joe Stacey,Tristan Guigue,Héctor Martinez Alonso,Diarmuid Ó Séaghdha,Anders Johannsen*

Main category: cs.CL

TL;DR: PyTOD是一个通过生成可执行代码以实现编程化对话状态跟踪的系统，显示出卓越的性能和准确性。


<details>
  <summary>Details</summary>
Motivation: 改进编程化任务导向对话(TOD)代理的对话状态跟踪，提高对用户目标的估计能力。

Method: 提出PyTOD系统，通过采用简单的约束解码方法，生成可执行代码进行状态跟踪，并结合策略和执行反馈实现错误修正。

Result: 在SGD基准上取得了最高的状态跟踪性能，超过了多种强基线方法。

Conclusion: PyTOD通过执行感知的状态跟踪证明了其有效性，表现出了更高的准确性和稳健性。

Abstract: Programmable task-oriented dialogue (TOD) agents enable language models to
follow structured dialogue policies, but their effectiveness hinges on accurate
state tracking. We present PyTOD, an agent that generates executable code to
track dialogue state and uses policy and execution feedback for efficient error
correction. To this end, PyTOD employs a simple constrained decoding approach,
using a language model instead of grammar rules to follow API schemata. This
leads to state-of-the-art state tracking performance on the challenging SGD
benchmark. Our experiments show that PyTOD surpasses strong baselines in both
accuracy and robust user goal estimation as the dialogue progresses,
demonstrating the effectiveness of execution-aware state tracking.

</details>


### [104] [RadReason: Radiology Report Evaluation Metric with Reasons and Sub-Scores](https://arxiv.org/abs/2508.15464)
*Yingshu Li,Yunyi Liu,Lingqiao Liu,Lei Wang,Luping Zhou*

Main category: cs.CL

TL;DR: RadReason提出了一种可解释且精细的评估框架，用于评估放射学报告并提供详细的评分和分析。


<details>
  <summary>Details</summary>
Motivation: 当前放射学报告的自动评估缺乏细粒度和临床相关的指标，现有方法过于粗略或依赖于不可解释的模型，难以适应实际临床需求。

Method: RadReason通过新的Group Relative Policy Optimization方法，结合动态权重分配与难度调整机制，实现对报告质量的细分评估，并生成人类可读的解释。

Result: RadReason 在ReXVal基准测试上优于现有指标，与GPT-4评估表现持平，同时具备解释性、成本效益和临床适用性。

Conclusion: RadReason为放射学报告提供了一种可部署且高效精确的评估工具，有助于提升临床工作流的可靠性。

Abstract: Evaluating automatically generated radiology reports remains a fundamental
challenge due to the lack of clinically grounded, interpretable, and
fine-grained metrics. Existing methods either produce coarse overall scores or
rely on opaque black-box models, limiting their usefulness in real-world
clinical workflows. We introduce RadReason, a novel evaluation framework for
radiology reports that not only outputs fine-grained sub-scores across six
clinically defined error types, but also produces human-readable justifications
that explain the rationale behind each score. Our method builds on Group
Relative Policy Optimization and incorporates two key innovations: (1)
Sub-score Dynamic Weighting, which adaptively prioritizes clinically
challenging error types based on live F1 statistics; and (2) Majority-Guided
Advantage Scaling, which adjusts policy gradient updates based on prompt
difficulty derived from sub-score agreement. Together, these components enable
more stable optimization and better alignment with expert clinical judgment.
Experiments on the ReXVal benchmark show that RadReason surpasses all prior
offline metrics and achieves parity with GPT-4-based evaluations, while
remaining explainable, cost-efficient, and suitable for clinical deployment.
Code will be released upon publication.

</details>


### [105] [SLM4Offer: Personalized Marketing Offer Generation Using Contrastive Learning Based Fine-Tuning](https://arxiv.org/abs/2508.15471)
*Vedasamhitha Challapalli,Konduru Venkat Sai,Piyush Pratap Singh,Rupesh Prasad,Arvind Maurya,Atul Singh*

Main category: cs.CL

TL;DR: SLM4Offer 是一种通过对预训练的 T5-Small 模型进行对比学习微调，提升个性化营销效果的生成式 AI 模型，能够提高 17% 的优惠接受率。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用生成式 AI 模型提升个性化优惠生成的效率和效果，从而提高客户参与度和满意度。

Method: 通过对 T5-Small 编码器-解码器语言模型进行对比学习的微调，采用 InfoNCE 损失在嵌入空间中对齐客户画像与相关优惠。

Result: 实验表明，SLM4Offer 模型在合成数据集上的优惠接受率相比传统监督微调基线提高了 17%。

Conclusion: 该研究表明，对比学习方法能够有效提升生成式 AI 模型的普适性和在个性化营销中的优化能力。

Abstract: Personalized marketing has emerged as a pivotal strategy for enhancing
customer engagement and driving business growth. Academic and industry efforts
have predominantly focused on recommendation systems and personalized
advertisements. Nonetheless, this facet of personalization holds significant
potential for increasing conversion rates and improving customer satisfaction.
Prior studies suggest that well-executed personalization strategies can boost
revenue by up to 40 percent, underscoring the strategic importance of
developing intelligent, data-driven approaches for offer generation. This work
introduces SLM4Offer, a generative AI model for personalized offer generation,
developed by fine-tuning a pre-trained encoder-decoder language model,
specifically Google's Text-to-Text Transfer Transformer (T5-Small 60M) using a
contrastive learning approach. SLM4Offer employs InfoNCE (Information
Noise-Contrastive Estimation) loss to align customer personas with relevant
offers in a shared embedding space. A key innovation in SLM4Offer lies in the
adaptive learning behaviour introduced by contrastive loss, which reshapes the
latent space during training and enhances the model's generalizability. The
model is fine-tuned and evaluated on a synthetic dataset designed to simulate
customer behaviour and offer acceptance patterns. Experimental results
demonstrate a 17 percent improvement in offer acceptance rate over a supervised
fine-tuning baseline, highlighting the effectiveness of contrastive objectives
in advancing personalized marketing.

</details>


### [106] [Subjective Behaviors and Preferences in LLM: Language of Browsing](https://arxiv.org/abs/2508.15474)
*Sai Sundaresan,Harshita Chopra,Atanu R. Sinha,Koustava Goswami,Nagasai Saketh Naidu,Raghav Karan,N Anushka*

Main category: cs.CL

TL;DR: 本论文引入HeTLM方法，结合用户行为的异质性，通过簇分组进行语言模型训练，以更好地捕捉用户主观偏好和行为模式。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理用户主观行为和偏好时可能存在不足，本文旨在研究小型语言模型是否能更好地表示浏览行为，以及如何更好地对齐用户的多样化需求。

Method: 提出了一种称为HeTLM（异质性感知的语言模型训练）的方法，利用基于聚类的策略，为不同用户群体设计特定的模型参数集合，并使用页面级标记器对小型语言模型进行训练。

Result: 研究发现，小型语言模型在页面级标记器的训练下优于大规模预训练或微调的语言模型；HeTLM针对异质性簇的参数设置在同家族模型中表现更佳，并实现更高的平均性能和更低的性能方差。

Conclusion: 异质性感知的训练方法不仅提升了语言模型对用户主观行为的捕捉能力，还显著提高了其生成的对齐性和可靠性。

Abstract: A Large Language Model (LLM) offers versatility across domains and tasks,
purportedly benefiting users with a wide variety of behaviors and preferences.
We question this perception about an LLM when users have inherently subjective
behaviors and preferences, as seen in their ubiquitous and idiosyncratic
browsing of websites or apps. The sequential behavior logs of pages, thus
generated, form something akin to each user's self-constructed "language",
albeit without the structure and grammar imbued in natural languages. We ask:
(i) Can a small LM represent the "language of browsing" better than a large LM?
(ii) Can an LM with a single set of parameters (or, single LM) adequately
capture myriad users' heterogeneous, subjective behaviors and preferences?
(iii) Can a single LM with high average performance, yield low variance in
performance to make alignment good at user level? We introduce clusterwise LM
training, HeTLM (Heterogeneity aware Training of Language Model), appropriate
for subjective behaviors. We find that (i) a small LM trained using a
page-level tokenizer outperforms large pretrained or finetuned LMs; (ii) HeTLM
with heterogeneous cluster specific set of parameters outperforms a single LM
of the same family, controlling for the number of parameters; and (iii) a
higher mean and a lower variance in generation ensues, implying improved
alignment.

</details>


### [107] [Influence-driven Curriculum Learning for Pre-training on Limited Data](https://arxiv.org/abs/2508.15475)
*Loris Schoenegger,Lukas Thoma,Terra Blevins,Benjamin Roth*

Main category: cs.CL

TL;DR: 研究证明课程学习可以通过使用以训练数据影响为基础的难易度指标提升语言模型预训练性能。


<details>
  <summary>Details</summary>
Motivation: 探索为何传统基于人类的难易度指标在课程学习中的效果有限，试图通过更贴近模型训练过程的难度指标改善课程学习的表现。

Method: 引入以训练数据影响为基础的难易度指标，对训练样本进行排序并设计课程学习策略。

Result: 基于新难易度指标的课程学习使模型在基准测试中的表现提升超过10个百分点。

Conclusion: 课程学习对语言模型预训练有潜力，前提是采用以模型为中心的难易度指标。

Abstract: Curriculum learning, a training technique where data is presented to the
model in order of example difficulty (e.g., from simpler to more complex
documents), has shown limited success for pre-training language models. In this
work, we investigate whether curriculum learning becomes competitive if we
replace conventional human-centered difficulty metrics with one that more
closely corresponds to example difficulty as observed during model training.
Specifically, we experiment with sorting training examples by their
\textit{training data influence}, a score which estimates the effect of
individual training examples on the model's output. Models trained on our
curricula are able to outperform ones trained in random order by over 10
percentage points in benchmarks, confirming that curriculum learning is
beneficial for language model pre-training, as long as a more model-centric
notion of difficulty is adopted.

</details>


### [108] [SLM-Bench: A Comprehensive Benchmark of Small Language Models on Environmental Impacts -- Extended Version](https://arxiv.org/abs/2508.15478)
*Nghiem Thanh Pham,Tung Kieu,Duc-Manh Nguyen,Son Ha Xuan,Nghia Duong-Trung,Danh Le-Phuoc*

Main category: cs.CL

TL;DR: 本文介绍了SLM-Bench，一个评估小型语言模型(SLM)的综合基准测试，涵盖准确性、计算效率和可持续性等多方面。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对SLM的全面评估，尤其是在环境影响方面，因此需要一个新的基准来规范评估方法，促进资源效率与实际应用之间的平衡。

Method: 设计并提出了SLM-Bench，通过23个数据集和9项NLP任务，基于4种硬件配置，对15个SLM从11个维度进行系统性评估，同时开发了开源的基准测试流水线，以确保可重复性和公平比较。

Result: 研究揭示了SLM在准确性和能源效率上的多样化表现，不同模型在这些维度间存在权衡。

Conclusion: SLM-Bench为SLM评估设立了新标准，填补了资源效率与实际应用之间的研究空白。

Abstract: Small Language Models (SLMs) offer computational efficiency and
accessibility, yet a systematic evaluation of their performance and
environmental impact remains lacking. We introduce SLM-Bench, the first
benchmark specifically designed to assess SLMs across multiple dimensions,
including accuracy, computational efficiency, and sustainability metrics.
SLM-Bench evaluates 15 SLMs on 9 NLP tasks using 23 datasets spanning 14
domains. The evaluation is conducted on 4 hardware configurations, providing a
rigorous comparison of their effectiveness. Unlike prior benchmarks, SLM-Bench
quantifies 11 metrics across correctness, computation, and consumption,
enabling a holistic assessment of efficiency trade-offs. Our evaluation
considers controlled hardware conditions, ensuring fair comparisons across
models. We develop an open-source benchmarking pipeline with standardized
evaluation protocols to facilitate reproducibility and further research. Our
findings highlight the diverse trade-offs among SLMs, where some models excel
in accuracy while others achieve superior energy efficiency. SLM-Bench sets a
new standard for SLM evaluation, bridging the gap between resource efficiency
and real-world applicability.

</details>


### [109] [HebID: Detecting Social Identities in Hebrew-language Political Text](https://arxiv.org/abs/2508.15483)
*Guy Mor-Lan,Naama Rivlin-Angert,Yael R. Kaplan,Tamir Sheafer,Shaul R. Shenhav*

Main category: cs.CL

TL;DR: 本文引入了HebID，这是首个用于社会身份检测的多标签希伯来语语料库，并通过分析政治家的社交媒体帖子和演讲提供了新的见解。


<details>
  <summary>Details</summary>
Motivation: 当前关于群体和身份的检测数据集大多集中于英语环境，类别单一且粒度较粗，难以适用于特定文化背景下的语言分析。

Method: 构建包含5536条以色列政治家Facebook帖子（2018年12月至2021年4月）的多标签语料库，通过手动注释识别12种社会身份，并在多种编码器和LLM上进行比较，完成基准测试。

Result: 基准表现显示，微调过的希伯来语言模型表现最好（宏F1值为0.74）。模型被用于分析不同政治话语场景下身份表达的变化、受欢迎程度和性别差异等。

Conclusion: HebID为研究希伯来语言中的社会身份问题提供了重要基础，并可为非英语的其他政治语言研究提供模板。

Abstract: Political language is deeply intertwined with social identities. While social
identities are often shaped by specific cultural contexts and expressed through
particular uses of language, existing datasets for group and identity detection
are predominantly English-centric, single-label and focus on coarse identity
categories. We introduce HebID, the first multilabel Hebrew corpus for social
identity detection: 5,536 sentences from Israeli politicians' Facebook posts
(Dec 2018-Apr 2021), manually annotated for twelve nuanced social identities
(e.g. Rightist, Ultra-Orthodox, Socially-oriented) grounded by survey data. We
benchmark multilabel and single-label encoders alongside 2B-9B-parameter
generative LLMs, finding that Hebrew-tuned LLMs provide the best results
(macro-$F_1$ = 0.74). We apply our classifier to politicians' Facebook posts
and parliamentary speeches, evaluating differences in popularity, temporal
trends, clustering patterns, and gender-related variations in identity
expression. We utilize identity choices from a national public survey, enabling
a comparison between identities portrayed in elite discourse and the public's
identity priorities. HebID provides a comprehensive foundation for studying
social identities in Hebrew and can serve as a model for similar research in
other non-English political contexts.

</details>


### [110] [Dream 7B: Diffusion Large Language Models](https://arxiv.org/abs/2508.15487)
*Jiacheng Ye,Zhihui Xie,Lin Zheng,Jiahui Gao,Zirui Wu,Xin Jiang,Zhenguo Li,Lingpeng Kong*

Main category: cs.CL

TL;DR: Dream 7B通过离散扩散建模实现并行序列生成，表现在普通任务、数学和编程任务中均优于现有扩散语言模型，并提供灵活的推理和调优能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有自回归模型生成效率低以及扩散语言模型性能不足的问题。

Method: 采用离散扩散建模结合简单高效的训练技术（如基于自回归模型的初始化及上下文适应的噪声重调度）。

Result: Dream 7B在通用、数学和编程任务上表现优异，具备灵活推理能力（如任意顺序生成、填充功能）及高效调控速度和质量的能力。

Conclusion: Dream 7B提升了扩散语言建模的性能，具有开创性，并通过开源促进相关领域研究。

Abstract: We introduce Dream 7B, the most powerful open diffusion large language model
to date. Unlike autoregressive (AR) models that generate tokens sequentially,
Dream 7B employs discrete diffusion modeling to refine sequences in parallel
through iterative denoising. Our model consistently outperforms existing
diffusion language models on general, mathematical, and coding tasks. Dream 7B
demonstrates superior planning abilities and inference flexibility, including
arbitrary-order generation, infilling capabilities, and tunable quality-speed
trade-offs. These results are achieved through simple yet effective training
techniques, including AR-based LLM initialization and context-adaptive
token-level noise rescheduling. We release both Dream-Base and Dream-Instruct
to facilitate further research in diffusion-based language modeling.

</details>


### [111] [The Enemy from Within: A Study of Political Delegitimization Discourse in Israeli Political Speech](https://arxiv.org/abs/2508.15524)
*Naama Rivlin-Angert,Guy Mor-Lan*

Main category: cs.CL

TL;DR: 本研究是关于政治合法性贬损话语（PDD）的首次大规模计算研究，分析1993-2023年以色列关键社交与新闻领域的不同平台数据。


<details>
  <summary>Details</summary>
Motivation: 通过研究和分析，探索政治合法性贬损话语的增长趋势及特征，从而更好地理解民主话语的变化及背后因素。

Method: 构建了包含10,410句的新数据集，并对其中的1,812句进行了细致的人工标注。为分析PDD内容，设计了一种结合微调编码器模型和解码器LLM的两阶段分类管道。

Result: 最佳模型DictaLM 2.0实现了0.74的F$_1$值（PDD二进制检测）和0.67的 Macro-F$_1$值（特征分类）。通过模型分析，发现PDD在三十年间显著上升，社交媒体上的PDD高于议会辩论，男性政治家使用更多，右倾政治人物更频繁，且在选举和重大事件期间出现峰值。

Conclusion: 自动化的PDD分析不仅可行还具有价值，对于理解民主话语乃至社会政治发展具有重要意义。

Abstract: We present the first large-scale computational study of political
delegitimization discourse (PDD), defined as symbolic attacks on the normative
validity of political entities. We curate and manually annotate a novel
Hebrew-language corpus of 10,410 sentences drawn from Knesset speeches
(1993-2023), Facebook posts (2018-2021), and leading news outlets, of which
1,812 instances (17.4\%) exhibit PDD and 642 carry additional annotations for
intensity, incivility, target type, and affective framing. We introduce a
two-stage classification pipeline combining finetuned encoder models and
decoder LLMs. Our best model (DictaLM 2.0) attains an F$_1$ of 0.74 for binary
PDD detection and a macro-F$_1$ of 0.67 for classification of delegitimization
characteristics. Applying this classifier to longitudinal and cross-platform
data, we see a marked rise in PDD over three decades, higher prevalence on
social media versus parliamentary debate, greater use by male than female
politicians, and stronger tendencies among right-leaning actors - with
pronounced spikes during election campaigns and major political events. Our
findings demonstrate the feasibility and value of automated PDD analysis for
understanding democratic discourse.

</details>


### [112] [SafetyFlow: An Agent-Flow System for Automated LLM Safety Benchmarking](https://arxiv.org/abs/2508.15526)
*Xiangyang Zhu,Yuan Tian,Chunyi Li,Kaiwei Zhang,Wei Sun,Guangtao Zhai*

Main category: cs.CL

TL;DR: 引入了SafetyFlow，首次实现了全自动化的大语言模型安全性评估基准构建，并进行实证验证。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型安全评估依赖于人工，消耗巨大，且存在冗余和难度不足的问题。

Method: 设计了SafetyFlow，一个由七个特化代理组成的系统，可在四天内自动构建全面的安全基准数据集，无需人工干预。

Result: 生成了包含23,446个低冗余、高区分性查询的SafetyFlowBench，并评估了49个先进大语言模型的安全性。

Conclusion: SafetyFlow显著降低了时间和资源成本，提升了基准构建效率和质量，为大语言模型安全性评估提供了新的工具。

Abstract: The rapid proliferation of large language models (LLMs) has intensified the
requirement for reliable safety evaluation to uncover model vulnerabilities. To
this end, numerous LLM safety evaluation benchmarks are proposed. However,
existing benchmarks generally rely on labor-intensive manual curation, which
causes excessive time and resource consumption. They also exhibit significant
redundancy and limited difficulty. To alleviate these problems, we introduce
SafetyFlow, the first agent-flow system designed to automate the construction
of LLM safety benchmarks. SafetyFlow can automatically build a comprehensive
safety benchmark in only four days without any human intervention by
orchestrating seven specialized agents, significantly reducing time and
resource cost. Equipped with versatile tools, the agents of SafetyFlow ensure
process and cost controllability while integrating human expertise into the
automatic pipeline. The final constructed dataset, SafetyFlowBench, contains
23,446 queries with low redundancy and strong discriminative power. Our
contribution includes the first fully automated benchmarking pipeline and a
comprehensive safety benchmark. We evaluate the safety of 49 advanced LLMs on
our dataset and conduct extensive experiments to validate our efficacy and
efficiency.

</details>


### [113] [Trained Miniatures: Low cost, High Efficacy SLMs for Sales & Marketing](https://arxiv.org/abs/2508.15617)
*Ishaan Bhola,Mukunda NS,Sravanth Kurmala,Harsh Nandwani,Arihant Jain*

Main category: cs.CL

TL;DR: 论文提出了小型语言模型(SLMs)的概念，旨在以较低成本实现特定领域响应的生成。


<details>
  <summary>Details</summary>
Motivation: 大语言模型尽管在文本生成方面表现优异，但计算成本过高，尤其是在如销售和市场拓展等特定行业应用中经济性较差。

Method: 通过微调小型语言模型，使其针对特定高价值应用领域生成类似的域响应，同时降低使用成本。

Result: 提出的小型语言模型能够在目标领域中以较低的计算成本实现类似的效果。

Conclusion: 小型语言模型为成本敏感型领域提供了高性价比的文本生成解决方案。

Abstract: Large language models (LLMs) excel in text generation; however, these
creative elements require heavy computation and are accompanied by a steep
cost. Especially for targeted applications such as sales and marketing
outreach, these costs are far from feasible. This paper introduces the concept
of "Trained Miniatures" - Small Language Models(SLMs) fine-tuned for specific,
high-value applications, generating similar domain-specific responses for a
fraction of the cost.

</details>


### [114] [SDGO: Self-Discrimination-Guided Optimization for Consistent Safety in Large Language Models](https://arxiv.org/abs/2508.15648)
*Peng Ding,Wen Sun,Dailin Li,Wei Zou,Jiaming Wang,Jiajun Chen,Shujian Huang*

Main category: cs.CL

TL;DR: 本文提出了一种名为SDGO（基于自我判别的优化）的强化学习框架，通过对大语言模型的判别和生成能力对齐以增强其生成安全性，无需额外数据或外部模型，实验表明方法对抗Jailbreaking攻击时表现优异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成任务中容易受到Jailbreaking攻击的影响，造成有害内容生成的问题。研究发现模型在判别过程中可以更有效地识别有害请求，但在生成过程中表现较弱，因此需要一种方法将模型的判别和生成能力进行对齐。

Method: 提出SDGO方法，即基于自我判别的优化，利用模型自身的判别能力作为奖励信号，通过自我改进的方式提升生成安全性，无需额外的标注数据或外部模型进行训练。

Result: 实验结果表明，SDGO在提升模型生成安全性方面表现出色，相较于基于提示优化和基于训练的基线方法，能够显著降低有害内容生成风险，并在对抗分布外攻击时展现出良好的稳健性。

Conclusion: 通过对齐大语言模型的判别和生成能力，SDGO方法显著提高了生成任务的安全性，同时减少了对标注数据的依赖，为应对分布外攻击提供了有效的解决方案。

Abstract: Large Language Models (LLMs) excel at various natural language processing
tasks but remain vulnerable to jailbreaking attacks that induce harmful content
generation. In this paper, we reveal a critical safety inconsistency: LLMs can
more effectively identify harmful requests as discriminators than defend
against them as generators. This insight inspires us to explore aligning the
model's inherent discrimination and generation capabilities. To this end, we
propose SDGO (Self-Discrimination-Guided Optimization), a reinforcement
learning framework that leverages the model's own discrimination capabilities
as a reward signal to enhance generation safety through iterative
self-improvement. Our method does not require any additional annotated data or
external models during the training phase. Extensive experiments demonstrate
that SDGO significantly improves model safety compared to both prompt-based and
training-based baselines while maintaining helpfulness on general benchmarks.
By aligning LLMs' discrimination and generation capabilities, SDGO brings
robust performance against out-of-distribution (OOD) jailbreaking attacks. This
alignment achieves tighter coupling between these two capabilities, enabling
the model's generation capability to be further enhanced with only a small
amount of discriminative samples. Our code and datasets are available at
https://github.com/NJUNLP/SDGO.

</details>


### [115] [Benchmarking Computer Science Survey Generation](https://arxiv.org/abs/2508.15658)
*Weihang Su,Anzhe Xie,Qingyao Ai,Jianming Long,Jiaxin Mao,Ziyi Ye,Yiqun Liu*

Main category: cs.CL

TL;DR: 论文介绍SurGE,一个科学调查生成评估的新基准，用于计算机科学领域，包含测试实例和大规模学术语料库，提出自动测评框架并发现该任务的挑战性。


<details>
  <summary>Details</summary>
Motivation: 科学调查总结文献进展，针对学术文献增长带来的手动撰写困难，希望用大语言模型自动生成，但缺少标准化基准和评估协议。

Method: 设计了SurGE，一个新基准，包含主题描述、专家撰写的调查和参考文献以及百万篇论文语料库；提出四维度的自动评估框架：信息覆盖、引用准确性、结构组织和内容质量。

Result: 研究表明，即使是先进的大语言模型也难以完成高质量的调查生成，任务具有复杂性。

Conclusion: 该研究表明当前技术仍不足以高效实现调查生成，需进一步研究。代码、数据及模型开源。

Abstract: Scientific survey articles play a vital role in summarizing research
progress, yet their manual creation is becoming increasingly infeasible due to
the rapid growth of academic literature. While large language models (LLMs)
offer promising capabilities for automating this process, progress in this area
is hindered by the absence of standardized benchmarks and evaluation protocols.
To address this gap, we introduce SurGE (Survey Generation Evaluation), a new
benchmark for evaluating scientific survey generation in the computer science
domain. SurGE consists of (1) a collection of test instances, each including a
topic description, an expert-written survey, and its full set of cited
references, and (2) a large-scale academic corpus of over one million papers
that serves as the retrieval pool. In addition, we propose an automated
evaluation framework that measures generated surveys across four dimensions:
information coverage, referencing accuracy, structural organization, and
content quality. Our evaluation of diverse LLM-based approaches shows that
survey generation remains highly challenging, even for advanced self-reflection
frameworks. These findings highlight the complexity of the task and the
necessity for continued research. We have open-sourced all the code, data, and
models at: https://github.com/oneal2000/SurGE

</details>


### [116] [Position Bias Mitigates Position Bias:Mitigate Position Bias Through Inter-Position Knowledge Distillation](https://arxiv.org/abs/2508.15709)
*Yifei Wang,Feng Xiong,Yong Wang,Linjing Li,Xiangxiang Chu,Daniel Dajun Zeng*

Main category: cs.CL

TL;DR: 本文提出Pos2Distill框架，通过对位置知识的蒸馏，减轻位置信息偏置（PB）对长上下文理解和处理能力的影响。


<details>
  <summary>Details</summary>
Motivation: 现有方法在缓解位置信息偏置时存在不足，仍未能完全消除PB对长上下文处理的负面影响。

Method: 设计了Pos2Distill框架，通过知识蒸馏将表现更好的位置能力传递给表现较弱的位置，从而减少性能差距。基于PB在检索和推理任务中的不同表现，设计了两个专门实例Pos2Distill-R¹和Pos2Distill-R²。

Result: 通过Pos2Distill框架，在长上下文的检索和推理任务中，所有位置的性能均得到显著提升，并实现了更高的任务泛化能力。

Conclusion: Pos2Distill在缓解PB方面表现出优越性，使长上下文理解更加平衡，同时在特定任务上取得了领先性能。

Abstract: Positional bias (PB), manifesting as non-uniform sensitivity across different
contextual locations, significantly impairs long-context comprehension and
processing capabilities. While prior work seeks to mitigate PB through
modifying the architectures causing its emergence, significant PB still
persists. To address PB effectively, we introduce \textbf{Pos2Distill}, a
position to position knowledge distillation framework. Pos2Distill transfers
the superior capabilities from advantageous positions to less favorable ones,
thereby reducing the huge performance gaps. The conceptual principle is to
leverage the inherent, position-induced disparity to counteract the PB itself.
We identify distinct manifestations of PB under \textbf{\textsc{r}}etrieval and
\textbf{\textsc{r}}easoning paradigms, thereby designing two specialized
instantiations: \emph{Pos2Distill-R\textsuperscript{1}} and
\emph{Pos2Distill-R\textsuperscript{2}} respectively, both grounded in this
core principle. By employing the Pos2Distill approach, we achieve enhanced
uniformity and significant performance gains across all contextual positions in
long-context retrieval and reasoning tasks. Crucially, both specialized systems
exhibit strong cross-task generalization mutually, while achieving superior
performance on their respective tasks.

</details>


### [117] [Stemming -- The Evolution and Current State with a Focus on Bangla](https://arxiv.org/abs/2508.15711)
*Abhijit Paul,Mashiat Amin Farin,Sharif Md. Abdullah,Ahmedul Kabir,Zarif Masud,Shebuti Rayana*

Main category: cs.CL

TL;DR: 本文探讨了孟加拉语的词干提取问题，指出现有研究存在显著缺口，并呼吁开发鲁棒的词干提取器以提升语言分析与处理。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语作为世界第七大语言，因缺乏资源和标注数据集，在数字领域中被低估，词干提取作为语言分析的关键预处理步骤，对减少复杂性至关重要。

Method: 本文通过综合文献调研，分析现有词干提取方法中处理形态变体的有效性，提出现有研究的不足，并对评价指标进行批判性分析。

Result: 识别了孟加拉语词干提取研究的显著空白和评估方法的不足，并提出开发方向。

Conclusion: 呼吁为孟加拉语开发更为鲁棒的词干提取方法，推动持续研究以提升语言分析与处理能力。

Abstract: Bangla, the seventh most widely spoken language worldwide with 300 million
native speakers, faces digital under-representation due to limited resources
and lack of annotated datasets. Stemming, a critical preprocessing step in
language analysis, is essential for low-resource, highly-inflectional languages
like Bangla, because it can reduce the complexity of algorithms and models by
significantly reducing the number of words the algorithm needs to consider.
This paper conducts a comprehensive survey of stemming approaches, emphasizing
the importance of handling morphological variants effectively. While exploring
the landscape of Bangla stemming, it becomes evident that there is a
significant gap in the existing literature. The paper highlights the
discontinuity from previous research and the scarcity of accessible
implementations for replication. Furthermore, it critiques the evaluation
methodologies, stressing the need for more relevant metrics. In the context of
Bangla's rich morphology and diverse dialects, the paper acknowledges the
challenges it poses. To address these challenges, the paper suggests directions
for Bangla stemmer development. It concludes by advocating for robust Bangla
stemmers and continued research in the field to enhance language analysis and
processing.

</details>


### [118] [EcomMMMU: Strategic Utilization of Visuals for Robust Multimodal E-Commerce Models](https://arxiv.org/abs/2508.15721)
*Xinyi Ling,Hanwen Du,Zhihui Zhu,Xia Ning*

Main category: cs.CL

TL;DR: 研究探讨了电商平台中丰富的商品图像数据是否总能提升理解能力，并提出了一个名为EcomMMMU的新数据集和方法SUMEI。


<details>
  <summary>Details</summary>
Motivation: 现有数据集规模和设计的局限性阻碍了对商品图像作用的深入研究。

Method: 引入了EcomMMMU数据集，具体设计了多种任务和一个VSS子集，分析图像在电商多模态大语言模型中表现。同时提出SUMEI方法，通过预测图像的实际作用来优化使用。

Result: 研究发现，商品图像并不总是能提升理解性能，在某些情况下甚至会降低表现。同时验证了SUMEI方法的有效性和稳健性。

Conclusion: 目前的多模态大语言模型在利用视觉内容时存在不足，需优化方法如SUMEI来有效利用图像数据，提升电商多模态任务性能。

Abstract: E-commerce platforms are rich in multimodal data, featuring a variety of
images that depict product details. However, this raises an important question:
do these images always enhance product understanding, or can they sometimes
introduce redundancy or degrade performance? Existing datasets are limited in
both scale and design, making it difficult to systematically examine this
question. To this end, we introduce EcomMMMU, an e-commerce multimodal
multitask understanding dataset with 406,190 samples and 8,989,510 images.
EcomMMMU is comprised of multi-image visual-language data designed with 8
essential tasks and a specialized VSS subset to benchmark the capability of
multimodal large language models (MLLMs) to effectively utilize visual content.
Analysis on EcomMMMU reveals that product images do not consistently improve
performance and can, in some cases, degrade it. This indicates that MLLMs may
struggle to effectively leverage rich visual content for e-commerce tasks.
Building on these insights, we propose SUMEI, a data-driven method that
strategically utilizes multiple images via predicting visual utilities before
using them for downstream tasks. Comprehensive experiments demonstrate the
effectiveness and robustness of SUMEI. The data and code are available through
https://anonymous.4open.science/r/submission25.

</details>


### [119] [End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning](https://arxiv.org/abs/2508.15746)
*Qiaoyu Zheng,Yuze Sun,Chaoyi Wu,Weike Zhao,Pengcheng Qiu,Yongguo Yu,Kun Sun,Yanfeng Wang,Ya Zhang,Weidi Xie*

Main category: cs.CL

TL;DR: 文章介绍了一种名为Deep-DxSearch的系统，通过强化学习优化医学大语言模型的诊断能力，显著提升了诊断准确率并超越了现有医学诊断基线模型。


<details>
  <summary>Details</summary>
Motivation: 医疗大语言模型在诊断中存在知识缺口和幻觉问题，传统的检索和工具增强方法难以充分利用外部知识，且反馈推理的可追溯性较差。为此，作者提出了一个解决方案。

Method: 设计了一个名为Deep-DxSearch的系统，该系统结合了大规模医学检索语料库，以强化学习的方式训练检索增强的推理能力，同时使用定制化奖励因子优化诊断准确性及推理由。

Result: 在多个数据中心的实验中，该系统在诊断准确率方面大幅领先于现有的模型（如GPT-4o、DeepSeek-R1等），并且在主流和罕见病的诊断中表现优异，验证了方法的有效性。

Conclusion: Deep-DxSearch显著改善了医学大模型在诊断中的表现，其框架和奖励设计对医学人工智能领域具有重要价值，同时为临床医生提供了可靠的辅助工具。

Abstract: Accurate diagnosis with medical large language models is hindered by
knowledge gaps and hallucinations. Retrieval and tool-augmented methods help,
but their impact is limited by weak use of external knowledge and poor
feedback-reasoning traceability. To address these challenges, We introduce
Deep-DxSearch, an agentic RAG system trained end-to-end with reinforcement
learning (RL) that enables steer tracebale retrieval-augmented reasoning for
medical diagnosis. In Deep-DxSearch, we first construct a large-scale medical
retrieval corpus comprising patient records and reliable medical knowledge
sources to support retrieval-aware reasoning across diagnostic scenarios. More
crutially, we frame the LLM as the core agent and the retrieval corpus as its
environment, using tailored rewards on format, retrieval, reasoning structure,
and diagnostic accuracy, thereby evolving the agentic RAG policy from
large-scale data through RL.
  Experiments demonstrate that our end-to-end agentic RL training framework
consistently outperforms prompt-engineering and training-free RAG approaches
across multiple data centers. After training, Deep-DxSearch achieves
substantial gains in diagnostic accuracy, surpassing strong diagnostic
baselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworks
for both common and rare disease diagnosis under in-distribution and
out-of-distribution settings. Moreover, ablation studies on reward design and
retrieval corpus components confirm their critical roles, underscoring the
uniqueness and effectiveness of our approach compared with traditional
implementations. Finally, case studies and interpretability analyses highlight
improvements in Deep-DxSearch's diagnostic policy, providing deeper insight
into its performance gains and supporting clinicians in delivering more
reliable and precise preliminary diagnoses. See
https://github.com/MAGIC-AI4Med/Deep-DxSearch.

</details>


### [120] [Dissecting Tool-Integrated Reasoning: An Empirical Study and Analysis](https://arxiv.org/abs/2508.15754)
*Yufeng Zhao,Junnan Liu,Hongwei Liu,Dongsheng Zhu,Yuan Shen,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 该论文研究了工具集成推理（TIR）在提升大型语言模型（LLMs）复杂推理能力方面的效果，并提出了新的评估指标，证明TIR提高了模型的推理效率及其在多领域中的通用性。


<details>
  <summary>Details</summary>
Motivation: 探讨工具集成推理（TIR）是否能改进大型语言模型(LLMs)的推理能力及其对推理行为的影响，明确其在复杂推理任务中表现的普适性。

Method: 提出ReasonZoo基准测试，涵盖九类多样化推理任务，并引入两个全新评估指标---性能感知成本（PAC）和性能成本曲线下面积（AUC-PCC），评估TIR的推理效率及能力；通过对比实验，验证TIR与非TIR模型间的性能差异。

Result: 实验证明，TIR模型在数学和非数学任务中均优于非TIR模型，在PAC和AUC-PCC指标上表现出色，显现出推理效率的增强及过度思考的减少。

Conclusion: 工具集成推理（TIR）不仅能够提升语言模型在复杂推理任务中的性能，还能够优化推理效率，展现其跨领域优化推理能力的潜力。

Abstract: Large Language Models (LLMs) have made significant strides in reasoning tasks
through methods like chain-of-thought (CoT) reasoning. However, they often fall
short in tasks requiring precise computations. Tool-Integrated Reasoning (TIR)
has emerged as a solution by incorporating external tools into the reasoning
process. Nevertheless, the generalization of TIR in improving the reasoning
ability of LLM is still unclear. Additionally, whether TIR has improved the
model's reasoning behavior and helped the model think remains to be studied. We
introduce ReasonZoo, a comprehensive benchmark encompassing nine diverse
reasoning categories, to evaluate the effectiveness of TIR across various
domains. Additionally, we propose two novel metrics, Performance-Aware Cost
(PAC) and Area Under the Performance-Cost Curve (AUC-PCC), to assess reasoning
efficiency. Our empirical evaluation demonstrates that TIR-enabled models
consistently outperform their non-TIR counterparts in both mathematical and
non-mathematical tasks. Furthermore, TIR enhances reasoning efficiency, as
evidenced by improved PAC and AUC-PCC, indicating reduced overthinking and more
streamlined reasoning. These findings underscore the domain-general benefits of
TIR and its potential to advance LLM capabilities in complex reasoning tasks.

</details>


### [121] [LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries](https://arxiv.org/abs/2508.15760)
*Ming Yin,Dinghan Shen,Silei Xu,Jianbing Han,Sixun Dong,Mian Zhang,Yebowen Hu,Shujian Liu,Simin Ma,Song Wang,Sathish Reddy Indurthi,Xun Wang,Yiran Chen,Kaiqiang Song*

Main category: cs.CL

TL;DR: LiveMCP-101是一个包含101个真实世界查询的基准测试，用于评估AI代理在动态场景中协调使用多种MCP工具解决任务的能力，实验表明当前模型在工具协调上存在显著挑战。


<details>
  <summary>Details</summary>
Motivation: 研究AI代理在多工具协调与真实动态情境中的表现，以推动更可靠的自主AI系统开发。

Method: 通过构建LiveMCP-101基准测试，提出基于实际执行计划（而非API输出）的全新评估方法，并通过实验与分析探讨有效性及模型不足之处。

Result: 当前最前沿的LLMs在基准测试中的成功率低于60%，展现了工具协调中的显著问题，并揭示了具体的失败模式和令牌使用中的低效问题。

Conclusion: LiveMCP-101为评估真实世界AI代理能力设立了严格标杆，为开发能可靠执行复杂任务的自主AI系统提供了明确方向。

Abstract: Tool calling has emerged as a critical capability for AI agents to interact
with the real world and solve complex tasks. While the Model Context Protocol
(MCP) provides a powerful standardized framework for tool integration, there is
a significant gap in benchmarking how well AI agents can effectively solve
multi-step tasks using diverse MCP tools in realistic, dynamic scenarios. In
this work, we present LiveMCP-101, a benchmark of 101 carefully curated
real-world queries, refined through iterative LLM rewriting and manual review,
that require coordinated use of multiple MCP tools including web search, file
operations, mathematical reasoning, and data analysis. Moreover, we introduce a
novel evaluation approach that leverages ground-truth execution plans rather
than raw API outputs, better reflecting the evolving nature of real-world
environments. Experiments show that even frontier LLMs achieve a success rate
below 60\%, highlighting major challenges in tool orchestration. Detailed
ablations and error analysis further reveal distinct failure modes and
inefficiencies in token usage, pointing to concrete directions for advancing
current models. LiveMCP-101 sets a rigorous standard for evaluating real-world
agent capabilities, advancing toward autonomous AI systems that reliably
execute complex tasks through tool use.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [122] [A Fully Spectral Neuro-Symbolic Reasoning Architecture with Graph Signal Processing as the Computational Backbone](https://arxiv.org/abs/2508.14923)
*Andrew Kiruluta*

Main category: cs.AI

TL;DR: 提出了一种以图信号处理（GSP）为核心的神经符号推理架构，通过在图谱域实现完整的推理流程，提升逻辑一致性、可解释性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用光谱图信号处理来有效结合符号逻辑和神经网络推理，克服现有模型在效率和解释性上的局限。

Method: 基于图谱域，通过可学习的光谱滤波器来编码逻辑实体和关系，处理多尺度信息传播，并将其映射为符号谓词以执行基于规则的推理。同时提出了由数学框架支持的光谱推理方法，包括图傅里叶变换、带选择性注意力和光谱规则落地。

Result: 在ProofWriter、EntailmentBank、bAbI、CLUTRR和ARC-Challenge等基准数据集上，与最先进的神经符号模型相比，取得了逻辑一致性、可解释性和计算效率方面的改进。

Conclusion: 结果表明，图信号处理为构建稳健和可解释的推理系统提供了数学支持和计算高效的基础。

Abstract: We propose a fully spectral, neuro\-symbolic reasoning architecture that
leverages Graph Signal Processing (GSP) as the primary computational backbone
for integrating symbolic logic and neural inference. Unlike conventional
reasoning models that treat spectral graph methods as peripheral components,
our approach formulates the entire reasoning pipeline in the graph spectral
domain. Logical entities and relationships are encoded as graph signals,
processed via learnable spectral filters that control multi-scale information
propagation, and mapped into symbolic predicates for rule-based inference. We
present a complete mathematical framework for spectral reasoning, including
graph Fourier transforms, band-selective attention, and spectral rule
grounding. Experiments on benchmark reasoning datasets (ProofWriter,
EntailmentBank, bAbI, CLUTRR, and ARC-Challenge) demonstrate improvements in
logical consistency, interpretability, and computational efficiency over
state\-of\-the\-art neuro\-symbolic models. Our results suggest that GSP
provides a mathematically grounded and computationally efficient substrate for
robust and interpretable reasoning systems.

</details>


### [123] [Goals and the Structure of Experience](https://arxiv.org/abs/2508.15013)
*Nadav Amir,Stas Tiomkin,Angela Langdon*

Main category: cs.AI

TL;DR: 本文提出了一种目标导向的状态表示计算框架，其中世界模型的描述性与规范性方面从智能体的目标中共同涌现，而非传统的强化学习中独立构成的状态表示和奖励函数部分。


<details>
  <summary>Details</summary>
Motivation: 现有的计算模型多认为目的性行为依赖于独立的世界模型组件，但忽略了目标本身可能促使这两方面共同演化的可能性，提出新的框架以理解此机制。

Method: 基于佛教认识论，定义了目标导向的'目的状态'，作为目标等价的经验分布类别，借助行为策略与理想经验特性间的统计偏差来解析学习过程。

Result: 从经验和理论角度回顾支持这一新观点的文献，并展示其在解释不同载体上的目的性行为方面的潜力。

Conclusion: 该框架为统一解释行为、体验和神经层面的目的性行为提供了新思路，可能对认知科学及人工智能领域研究提供重要启发。

Abstract: Purposeful behavior is a hallmark of natural and artificial intelligence. Its
acquisition is often believed to rely on world models, comprising both
descriptive (what is) and prescriptive (what is desirable) aspects that
identify and evaluate state of affairs in the world, respectively. Canonical
computational accounts of purposeful behavior, such as reinforcement learning,
posit distinct components of a world model comprising a state representation
(descriptive aspect) and a reward function (prescriptive aspect). However, an
alternative possibility, which has not yet been computationally formulated, is
that these two aspects instead co-emerge interdependently from an agent's goal.
Here, we describe a computational framework of goal-directed state
representation in cognitive agents, in which the descriptive and prescriptive
aspects of a world model co-emerge from agent-environment interaction
sequences, or experiences. Drawing on Buddhist epistemology, we introduce a
construct of goal-directed, or telic, states, defined as classes of
goal-equivalent experience distributions. Telic states provide a parsimonious
account of goal-directed learning in terms of the statistical divergence
between behavioral policies and desirable experience features. We review
empirical and theoretical literature supporting this novel perspective and
discuss its potential to provide a unified account of behavioral,
phenomenological and neural dimensions of purposeful behaviors across diverse
substrates.

</details>


### [124] [Collab-REC: An LLM-based Agentic Framework for Balancing Recommendations in Tourism](https://arxiv.org/abs/2508.15030)
*Ashmi Banerjee,Fitri Nur Aisyah,Adithi Satish,Wolfgang Wörndl,Yashar Deldjoo*

Main category: cs.AI

TL;DR: Collab-REC是一个多智能体框架，通过结合个性化、流行性和可持续性视角生成关于旅游的推荐，并通过一个非LLM主持人的多轮协商提升多样性和相关性。


<details>
  <summary>Details</summary>
Motivation: 留住游客偏好多样性需求，同时解决过度旅游问题。

Method: 融合三种基于LLM的代理生成旅游推荐，并通过非LLM主持人协商合并建议。

Result: 提升推荐的多样性和相关性，突出鲜为人知的城市以平衡游客分布。

Conclusion: 多智能体合作可以显著改善基于LLM推荐系统的多样性和可持续性。

Abstract: We propose Collab-REC, a multi-agent framework designed to counteract
popularity bias and enhance diversity in tourism recommendations. In our
setting, three LLM-based agents -- Personalization, Popularity, and
Sustainability generate city suggestions from complementary perspectives. A
non-LLM moderator then merges and refines these proposals via multi-round
negotiation, ensuring each agent's viewpoint is incorporated while penalizing
spurious or repeated responses. Experiments on European city queries show that
Collab-REC improves diversity and overall relevance compared to a single-agent
baseline, surfacing lesser-visited locales that often remain overlooked. This
balanced, context-aware approach addresses over-tourism and better aligns with
constraints provided by the user, highlighting the promise of multi-stakeholder
collaboration in LLM-driven recommender systems.

</details>


### [125] [Emergent Crowds Dynamics from Language-Driven Multi-Agent Interactions](https://arxiv.org/abs/2508.15047)
*Yibo Liu,Liam Shatzel,Brandon Haworth,Teseo Schneider*

Main category: cs.AI

TL;DR: 提出使用大语言模型（LLMs）结合对话和导航机制来控制群体中各个代理的运动和行为，增强了群体仿真的现实感。


<details>
  <summary>Details</summary>
Motivation: 目前的群体模拟方法未充分考虑人与环境互动时语言和对话的重要性，导致仿真效果不够真实。

Method: 引入基于角色个性、角色关系和情感状态的大语言模型，结合对话系统和语言驱动导航，动态控制代理间对话和导航运动。

Result: 在复杂场景中验证该方法，观察到代理的自然分组和解组行为，以及信息传播等现象，从而增强了群体仿真中的现实感。

Conclusion: 通过将大语言模型与代理导航系统融合，能够更逼真地模拟包含语言和社交互动的群体行为，构建更自然的仿真场景。

Abstract: Animating and simulating crowds using an agent-based approach is a
well-established area where every agent in the crowd is individually controlled
such that global human-like behaviour emerges. We observe that human navigation
and movement in crowds are often influenced by complex social and environmental
interactions, driven mainly by language and dialogue. However, most existing
work does not consider these dimensions and leads to animations where
agent-agent and agent-environment interactions are largely limited to steering
and fixed higher-level goal extrapolation.
  We propose a novel method that exploits large language models (LLMs) to
control agents' movement. Our method has two main components: a dialogue system
and language-driven navigation. We periodically query agent-centric LLMs
conditioned on character personalities, roles, desires, and relationships to
control the generation of inter-agent dialogue when necessitated by the spatial
and social relationships with neighbouring agents. We then use the conversation
and each agent's personality, emotional state, vision, and physical state to
control the navigation and steering of each agent. Our model thus enables
agents to make motion decisions based on both their perceptual inputs and the
ongoing dialogue.
  We validate our method in two complex scenarios that exemplify the interplay
between social interactions, steering, and crowding. In these scenarios, we
observe that grouping and ungrouping of agents automatically occur.
Additionally, our experiments show that our method serves as an
information-passing mechanism within the crowd. As a result, our framework
produces more realistic crowd simulations, with emergent group behaviours
arising naturally from any environmental setting.

</details>


### [126] [Don't Think Twice! Over-Reasoning Impairs Confidence Calibration](https://arxiv.org/abs/2508.15050)
*Romain Lacombe,Kerrie Wu,Eddie Dilworth*

Main category: cs.AI

TL;DR: 论文探索了问答工具中大型语言模型的校准问题，评估了推理能力和预算对信心评估的影响，发现延长推理反而会导致过度自信，而搜索增强生成方法大幅提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决在大语言模型作为问答工具时的过度自信问题，以提升任务执行的可靠性和准确性。

Method: 通过ClimateX数据集，对推理能力和预算与模型校准性能的关系进行了系统性评估，包括纯推理和搜索增强生成方法的对比分析。

Result: 研究发现，增加推理预算会加剧模型的过度自信，而搜索增强生成能够通过检索相关证据显著提升准确性，达到89.3%。

Conclusion: 信息获取比推理深度或预算更关键，是改进知识密集型任务中信心校准的瓶颈。

Abstract: Large Language Models deployed as question answering tools require robust
calibration to avoid overconfidence. We systematically evaluate how reasoning
capabilities and budget affect confidence assessment accuracy, using the
ClimateX dataset (Lacombe et al., 2023) and expanding it to human and planetary
health. Our key finding challenges the "test-time scaling" paradigm: while
recent reasoning LLMs achieve 48.7% accuracy in assessing expert confidence,
increasing reasoning budgets consistently impairs rather than improves
calibration. Extended reasoning leads to systematic overconfidence that worsens
with longer thinking budgets, producing diminishing and negative returns beyond
modest computational investments. Conversely, search-augmented generation
dramatically outperforms pure reasoning, achieving 89.3% accuracy by retrieving
relevant evidence. Our results suggest that information access, rather than
reasoning depth or inference budget, may be the critical bottleneck for
improved confidence calibration of knowledge-intensive tasks.

</details>


### [127] [Demonstrating Onboard Inference for Earth Science Applications with Spectral Analysis Algorithms and Deep Learning](https://arxiv.org/abs/2508.15053)
*Itai Zilberstein,Alberto Candela,Steve Chien,David Rijlaarsdam,Tom Hendrix,Leonie Buckley,Aubrey Dunne*

Main category: cs.AI

TL;DR: 本论文介绍了与Ubotica Technologies合作，利用CS-6卫星演示前沿的数据分析技术。


<details>
  <summary>Details</summary>
Motivation: 通过在边缘（如卫星上）进行数据分析，来实现新的地球科学测量和响应。

Method: 在CS-6卫星上使用深度学习和光谱分析算法进行数据分析和推断。CS-6配备了高光谱仪器及神经网络加速硬件。

Result: 展示了CS-6上执行数据分析和推断的能力，使得在轨数据处理成为可能。

Conclusion: 边缘数据处理技术将拓展地球科学的研究方法，提高响应效率。

Abstract: In partnership with Ubotica Technologies, the Jet Propulsion Laboratory is
demonstrating state-of-the-art data analysis onboard CogniSAT-6/HAMMER (CS-6).
CS-6 is a satellite with a visible and near infrared range hyperspectral
instrument and neural network acceleration hardware. Performing data analysis
at the edge (e.g. onboard) can enable new Earth science measurements and
responses. We will demonstrate data analysis and inference onboard CS-6 for
numerous applications using deep learning and spectral analysis algorithms.

</details>


### [128] [S3LoRA: Safe Spectral Sharpness-Guided Pruning in Adaptation of Agent Planner](https://arxiv.org/abs/2508.15068)
*Shuang Ao,Gopal Rumchurn*

Main category: cs.AI

TL;DR: 提出了一种轻量级、安全性增强的LoRA适配方法S3LoRA，旨在加强大型语言模型的安全性，同时保持任务性能和降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LoRA的适配技术可能导致模型的安全性下降，而当前部分安全适配方法依赖不可用的模型检查点，限制了应用范围。

Method: 提出S3LoRA框架，包括MAS-SVD方法分析LoRA权重更新的结构特性，以及SSI指标检测集中、潜在有风险的更新层，对其进行后处理剪枝以降低风险。

Result: 实验表明，S3LoRA在保持或提升任务效用指标的同时，能显著提高安全性指标并降低推理成本。

Conclusion: S3LoRA是一个轻量、高效、可扩展的解决方案，可以在资源受限和安全关键环境中安全部署基于LLM的代理。

Abstract: Adapting Large Language Models (LLMs) using parameter-efficient fine-tuning
(PEFT) techniques such as LoRA has enabled powerful capabilities in LLM-based
agents. However, these adaptations can unintentionally compromise safety
alignment, leading to unsafe or unstable behaviors, particularly in agent
planning tasks. Existing safety-aware adaptation methods often require access
to both base and instruction-tuned model checkpoints, which are frequently
unavailable in practice, limiting their applicability. We propose S3LoRA (Safe
Spectral Sharpness-Guided Pruning LoRA), a lightweight, data-free, and
model-independent framework that mitigates safety risks in LoRA-adapted models
by inspecting only the fine-tuned weight updates. We first introduce
Magnitude-Aware Spherically Normalized SVD (MAS-SVD), which robustly analyzes
the structural properties of LoRA updates while preserving global magnitude
information. We then design the Spectral Sharpness Index (SSI), a
sharpness-aware metric to detect layers with highly concentrated and
potentially unsafe updates. These layers are pruned post-hoc to reduce risk
without sacrificing task performance. Extensive experiments and ablation
studies across agent planning and language generation tasks show that S3LoRA
consistently improves safety metrics while maintaining or improving utility
metrics and significantly reducing inference cost. These results establish
S3LoRA as a practical and scalable solution for safely deploying LLM-based
agents in real-world, resource-constrained, and safety-critical environments.

</details>


### [129] [Argumentation for Explainable Workforce Optimisation (with Appendix)](https://arxiv.org/abs/2508.15118)
*Jennifer Leigh,Dimitrios Letsios,Alessandro Mella,Lucio Machetti,Francesca Toni*

Main category: cs.AI

TL;DR: 本文将劳动力管理建模为抽象论证问题，通过工具提供解决方案和解释，用户研究显示其比传统手动方式更快且更准确。


<details>
  <summary>Details</summary>
Motivation: 应对劳动力管理中的执行时间变化并向相关方提供解释是一个重要挑战。

Method: 通过将劳动力管理建模为工业应用中的抽象论证问题，开发工具来处理变化并生成解释。

Result: 用户研究显示，提出的方法能显著提升问题解决的速度和准确性。

Conclusion: 本研究证明抽象论证在劳动力管理中的适用性，并表明其工具和解释优于传统手动解决方案。

Abstract: Workforce management is a complex problem optimising the makespan and travel
distance required for a team of operators to complete a set of jobs, using a
set of instruments. A crucial challenge in workforce management is
accommodating changes at execution time so that explanations are provided to
all stakeholders involved. Here, we show that, by understanding workforce
management as abstract argumentation in an industrial application, we can
accommodate change and obtain faithful explanations. We show, with a user
study, that our tool and explanations lead to faster and more accurate problem
solving than conventional solutions by hand.

</details>


### [130] [Open-Universe Assistance Games](https://arxiv.org/abs/2508.15119)
*Rachel Ma,Jingyi Qu,Andreea Bobu,Dylan Hadfield-Menell*

Main category: cs.AI

TL;DR: 本文提出了一种名为GOOD的新方法，通过开放性对话推断目标，并应用于多个模拟环境中，展示了其优于没有显式目标追踪的基线方法。


<details>
  <summary>Details</summary>
Motivation: Embodied AI需要能够理解和推理开放环境下的人类目标和偏好，这些目标和偏好未被预先定义。

Method: 引入了Open-Universe Assistance Games (OU-AGs)框架，并提出了GOOD方法，通过与人类的在线对话提取自然语言形式的目标并推断目标分布。使用LLM来模拟用户意图复杂度，进行候选目标的概率推理。

Result: 在文本型杂货购物以及AI2Thor的机器人家庭环境中进行评估，方法比没有目标追踪的基线效果更好，得到了LLM和人工评估的一致确认。

Conclusion: GOOD方法证明了在Embodied AI情境下处理开放性目标和推断问题的有效性，提升了模型对复杂目标的理解和适应能力，同时无需大量离线数据支持。

Abstract: Embodied AI agents must infer and act in an interpretable way on diverse
human goals and preferences that are not predefined. To formalize this setting,
we introduce Open-Universe Assistance Games (OU-AGs), a framework where the
agent must reason over an unbounded and evolving space of possible goals. In
this context, we introduce GOOD (GOals from Open-ended Dialogue), a
data-efficient, online method that extracts goals in the form of natural
language during an interaction with a human, and infers a distribution over
natural language goals. GOOD prompts an LLM to simulate users with different
complex intents, using its responses to perform probabilistic inference over
candidate goals. This approach enables rich goal representations and
uncertainty estimation without requiring large offline datasets. We evaluate
GOOD in a text-based grocery shopping domain and in a text-operated simulated
household robotics environment (AI2Thor), using synthetic user profiles. Our
method outperforms a baseline without explicit goal tracking, as confirmed by
both LLM-based and human evaluations.

</details>


### [131] [aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists](https://arxiv.org/abs/2508.15126)
*Pengsong Zhang,Xiang Hu,Guowei Huang,Yang Qi,Heng Zhang,Xiuxu Li,Jiaxing Song,Jiabin Luo,Yijiang Li,Shuo Yin,Chengxiao Dai,Eric Hanchen Jiang,Xiaoyan Zhou,Zhenfei Yin,Boqin Yuan,Jing Dong,Guinan Su,Guanren Qiao,Haiming Tang,Anghong Du,Lili Pan,Zhenzhong Lan,Xinyu Liu*

Main category: cs.AI

TL;DR: 本文提出了aiXiv平台，一个开放获取的多主体架构生态系统，旨在促进高质量AI生成的科研内容的发布和传播，并进行了实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: AI生成的科研内容因传统出版系统与质量控制机制的局限性，缺乏合适的传播渠道，阻碍了科学进步。

Method: 设计了aiXiv平台，采用多主体架构，可以让人类和AI共同提交、评审、迭代完善科研内容，同时提供API和MCP接口，实现其可扩展性和集成能力。

Result: 实验表明，通过aiXiv平台的迭代审核和修订，可显著提高AI生成科研内容的质量。

Conclusion: aiXiv为AI科学家提供了一个高效、开放的生态系统，助力高质量AI科研成果的快速传播和发布。

Abstract: Recent advances in large language models (LLMs) have enabled AI agents to
autonomously generate scientific proposals, conduct experiments, author papers,
and perform peer reviews. Yet this flood of AI-generated research content
collides with a fragmented and largely closed publication ecosystem.
Traditional journals and conferences rely on human peer review, making them
difficult to scale and often reluctant to accept AI-generated research content;
existing preprint servers (e.g. arXiv) lack rigorous quality-control
mechanisms. Consequently, a significant amount of high-quality AI-generated
research lacks appropriate venues for dissemination, hindering its potential to
advance scientific progress. To address these challenges, we introduce aiXiv, a
next-generation open-access platform for human and AI scientists. Its
multi-agent architecture allows research proposals and papers to be submitted,
reviewed, and iteratively refined by both human and AI scientists. It also
provides API and MCP interfaces that enable seamless integration of
heterogeneous human and AI scientists, creating a scalable and extensible
ecosystem for autonomous scientific discovery. Through extensive experiments,
we demonstrate that aiXiv is a reliable and robust platform that significantly
enhances the quality of AI-generated research proposals and papers after
iterative revising and reviewing on aiXiv. Our work lays the groundwork for a
next-generation open-access ecosystem for AI scientists, accelerating the
publication and dissemination of high-quality AI-generated research content.
Code is available at https://github.com/aixiv-org. Website is available at
https://forms.gle/DxQgCtXFsJ4paMtn8.

</details>


### [132] [Mobile-Agent-v3: Foundamental Agents for GUI Automation](https://arxiv.org/abs/2508.15144)
*Jiabo Ye,Xi Zhang,Haiyang Xu,Haowei Liu,Junyang Wang,Zhaoqing Zhu,Ziwei Zheng,Feiyu Gao,Junjie Cao,Zhengxi Lu,Jitong Liao,Qi Zheng,Fei Huang,Jingren Zhou,Ming Yan*

Main category: cs.AI

TL;DR: 本文介绍了GUI-Owl及其改进框架Mobile-Agent-v3，在跨平台GUI基准测试中达到了开源模型的最新最佳性能。


<details>
  <summary>Details</summary>
Motivation: 旨在提高GUI智能体在多平台环境中的功能表现，通过新的方法生成更优质的数据和改进决策能力。

Method: 提出了三项关键创新：大规模环境基础设施、多样化基础智能体能力和可扩展环境强化学习（包括TRPO等新算法）。

Result: GUI-Owl-7B在AndroidWorld和OSWorld中分别达到66.4和29.4的性能。Mobile-Agent-v3进一步提升，在上述测试中达到了73.3和37.7的新开源最佳成绩。

Conclusion: 新方法显著提高了GUI智能体的性能，为开源多平台应用提供了更强的支持，同时引入了全新的云基础设施和算法优化路径。

Abstract: This paper introduces GUI-Owl, a foundational GUI agent model that achieves
state-of-the-art performance among open-source end-to-end models on ten GUI
benchmarks across desktop and mobile environments, covering grounding, question
answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B
achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose
Mobile-Agent-v3, a general-purpose GUI agent framework that further improves
performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new
state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates
three key innovations: (1) Large-scale Environment Infrastructure: a
cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows,
enabling our Self-Evolving GUI Trajectory Production framework. This generates
high-quality interaction data via automated query generation and correctness
validation, leveraging GUI-Owl to refine trajectories iteratively, forming a
self-improving loop. It supports diverse data pipelines and reduces manual
annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI
grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports
end-to-end decision-making and can act as a modular component in multi-agent
systems. (3) Scalable Environment RL: we develop a scalable reinforcement
learning framework with fully asynchronous training for real-world alignment.
We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for
online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are
open-sourced at https://github.com/X-PLUG/MobileAgent.

</details>


### [133] [PuzzleClone: An SMT-Powered Framework for Synthesizing Verifiable Data](https://arxiv.org/abs/2508.15180)
*Kai Xiong,Yanwei Huang,Rongjunchen Zhang,Kun Chen,Haipang Wu*

Main category: cs.AI

TL;DR: 该研究提出了一种称为PuzzleClone的正式框架，用于通过SMT在大规模上生成可验证的数据，并展示了在逻辑和数学基准上的显著进步。


<details>
  <summary>Details</summary>
Motivation: 增强大语言模型逻辑推理能力需要高质量且可验证答案的数据集，而现有生成的数据集在可靠性、多样性和可扩展性上存在不足。

Method: 提出PuzzleClone框架，通过：(1) 将种子谜题编码为结构化逻辑规范；(2) 通过系统变量和约束随机化生成可扩展的变体；(3) 使用复现机制确保有效性。

Result: 生成了一个包含超过83K多样且程序验证谜题的基准，并在后续训练中显著提升了逻辑和数学基准的性能，相较基准集PuzzleClone平均性能从14.4提升到56.2。

Conclusion: PuzzleClone框架提供了一种可靠高效的方式生成可扩展、可验证的数据集，显著改善后续模型推理能力，成果凸显了SMT在生成逻辑和数学数据中的潜力。

Abstract: High-quality mathematical and logical datasets with verifiable answers are
essential for strengthening the reasoning capabilities of large language models
(LLMs). While recent data augmentation techniques have facilitated the creation
of large-scale benchmarks, existing LLM-generated datasets often suffer from
limited reliability, diversity, and scalability. To address these challenges,
we introduce PuzzleClone, a formal framework for synthesizing verifiable data
at scale using Satisfiability Modulo Theories (SMT). Our approach features
three key innovations: (1) encoding seed puzzles into structured logical
specifications, (2) generating scalable variants through systematic variable
and constraint randomization, and (3) ensuring validity via a reproduction
mechanism. Applying PuzzleClone, we construct a curated benchmark comprising
over 83K diverse and programmatically validated puzzles. The generated puzzles
span a wide spectrum of difficulty and formats, posing significant challenges
to current state-of-the-art models. We conduct post training (SFT and RL) on
PuzzleClone datasets. Experimental results show that training on PuzzleClone
yields substantial improvements not only on PuzzleClone testset but also on
logic and mathematical benchmarks. Post training raises PuzzleClone average
from 14.4 to 56.2 and delivers consistent improvements across 7 logic and
mathematical benchmarks up to 12.5 absolute percentage points (AMC2023 from
52.5 to 65.0). Our code and data are available at
https://github.com/puzzleclone.

</details>


### [134] [LLM4Sweat: A Trustworthy Large Language Model for Hyperhidrosis Support](https://arxiv.org/abs/2508.15192)
*Wenjie Lin,Jin Wei-Kocsis*

Main category: cs.AI

TL;DR: 本文提出LLM4Sweat，一种针对于多汗症的开放源码专用大语言模型框架，通过三阶段流程显著改善对这种少见疾病的诊断与支持。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型在医疗领域的应用因稀缺、不可靠的微调数据集而受限，而针对多汗症这种影响患者身心健康的稀有疾病尚无专门研究。

Method: 提出了一种三阶段框架，首先通过前沿LLM生成医疗合理的合成场景数据集，随后对基础模型进行微调以提供精准诊断、个性化治疗推荐和心理支持，最后专家评测迭代优化数据集。

Result: 实验表明LLM4Sweat优于基线，成为首个针对于多汗症的开放源码大语言模型，并为解决类似稀有疾病的数据与可信性挑战提供了普适性方法。

Conclusion: LLM4Sweat成功解决了稀有疾病中数据稀缺的问题，实现了在诊断、治疗建议及心理支持方面的突破，为类似医疗应用提供了借鉴。

Abstract: While large language models (LLMs) have shown promise in healthcare, their
application for rare medical conditions is still hindered by scarce and
unreliable datasets for fine-tuning. Hyperhidrosis, a disorder causing
excessive sweating beyond physiological needs, is one such rare disorder,
affecting 2-3% of the population and significantly impacting both physical
comfort and psychosocial well-being. To date, no work has tailored LLMs to
advance the diagnosis or care of hyperhidrosis. To address this gap, we present
LLM4Sweat, an open-source and domain-specific LLM framework for trustworthy and
empathetic hyperhidrosis support. The system follows a three-stage pipeline. In
the data augmentation stage, a frontier LLM generates medically plausible
synthetic vignettes from curated open-source data to create a diverse and
balanced question-answer dataset. In the fine-tuning stage, an open-source
foundation model is fine-tuned on the dataset to provide diagnosis,
personalized treatment recommendations, and empathetic psychological support.
In the inference and expert evaluation stage, clinical and psychological
specialists assess accuracy, appropriateness, and empathy, with validated
responses iteratively enriching the dataset. Experiments show that LLM4Sweat
outperforms baselines and delivers the first open-source LLM framework for
hyperhidrosis, offering a generalizable approach for other rare diseases with
similar data and trustworthiness challenges.

</details>


### [135] [R-ConstraintBench: Evaluating LLMs on NP-Complete Scheduling](https://arxiv.org/abs/2508.15204)
*Raj Jain,Marc Wetter*

Main category: cs.AI

TL;DR: 该论文引入了一种名为R-ConstraintBench的框架，旨在评估大语言模型在资源受限项目调度问题中的性能，并分析其在高约束环境下的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在高约束条件下推理的可靠性尚未被充分研究，而许多领域的计划调度都涉及到此类的问题。

Method: 论文通过提出R-ConstraintBench框架，逐步增加资源限制、停机时间、时间窗口及非连贯约束来测试模型，并在数据中心迁移场景中验证其性能。

Result: 研究发现，强大的语言模型在仅有前序约束的情况下表现优秀，但在更复杂的约束交互下性能显著下降，表现瓶颈与约束交互相关，而非图深度。同时，模型在合成数据上的能力表现并不能直接迁移到实际场景。

Conclusion: 语言模型在复杂约束下的推理性能具有较大局限性，需进一步研究提高其在真实应用中的可靠性与泛化能力。

Abstract: Effective scheduling under tight resource, timing, and operational
constraints underpins large-scale planning across sectors such as capital
projects, manufacturing, logistics, and IT fleet transitions. However, the
reliability of large language models (LLMs) when reasoning under
high-constraint regimes is insufficiently characterized. To address this gap,
we present R-ConstraintBench, a scalable framework that evaluates models on
Resource-Constrained Project Scheduling Problems (RCPSP), an NP-Complete
feasibility class, while difficulty increases via linear growth in constraints.
R-ConstraintBench incrementally increases non-redundant precedence constraints
in Directed Acyclic Graphs (DAGs) and then introduces downtime, temporal
windows, and disjunctive constraints. As an illustrative example, we
instantiate the benchmark in a data center migration setting and evaluate
multiple LLMs using feasibility and error analysis, identifying degradation
thresholds and constraint types most associated with failure. Empirically,
strong models are near-ceiling on precedence-only DAGs, but feasibility
performance collapses when downtime, temporal windows, and disjunctive
constraints interact, implicating constraint interaction, not graph depth, as
the principal bottleneck. Performance on clean synthetic ramps also does not
guarantee transfer to domain-grounded scenarios, underscoring limited
generalization.

</details>


### [136] [See it. Say it. Sorted: Agentic System for Compositional Diagram Generation](https://arxiv.org/abs/2508.15222)
*Hantao Zhang,Jingyang Liu,Ed Li*

Main category: cs.AI

TL;DR: 本文研究了草图到图表的生成，提出了一种基于视觉语言模型（VLM）和大语言模型（LLM）的无训练系统，使得手绘草图可以转化为可编辑的矢量图（SVG）。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型虽然擅长生成写实图像，但在精确性、对齐性和符号结构方面表现不佳，难以满足生成图表的需求。本文提出的新方法旨在解决这些不足。

Method: 提出了一种名为See it. Say it. Sorted.的系统，该系统结合了视觉语言模型（VLM）和大语言模型（LLM），通过循环迭代的方式实现草图到SVG图表的生成。过程中，VLM批评者生成编辑建议，LLM候选者提出多种修改策略，最后由VLM评判者选择最佳输出。

Result: 在10张从已发表论文的流程图中提取的草图上，所提方法比GPT-5和Gemini-2.5-Pro更准确地重建了布局和结构，并有效避免了生成多余的文本。

Conclusion: 该方法能够更忠实地将草图转化为精确的可编辑图表且具有应用推广性，例如可通过API集成到演示工具中并加以扩展。代码已经开源。

Abstract: We study sketch-to-diagram generation: converting rough hand sketches into
precise, compositional diagrams. Diffusion models excel at photorealism but
struggle with the spatial precision, alignment, and symbolic structure required
for flowcharts. We introduce See it. Say it. Sorted., a training-free agentic
system that couples a Vision-Language Model (VLM) with Large Language Models
(LLMs) to produce editable Scalable Vector Graphics (SVG) programs. The system
runs an iterative loop in which a Critic VLM proposes a small set of
qualitative, relational edits; multiple candidate LLMs synthesize SVG updates
with diverse strategies (conservative->aggressive, alternative, focused); and a
Judge VLM selects the best candidate, ensuring stable improvement. This design
prioritizes qualitative reasoning over brittle numerical estimates, preserves
global constraints (e.g., alignment, connectivity), and naturally supports
human-in-the-loop corrections. On 10 sketches derived from flowcharts in
published papers, our method more faithfully reconstructs layout and structure
than two frontier closed-source image generation LLMs (GPT-5 and
Gemini-2.5-Pro), accurately composing primitives (e.g., multi-headed arrows)
without inserting unwanted text. Because outputs are programmatic SVGs, the
approach is readily extensible to presentation tools (e.g., PowerPoint) via
APIs and can be specialized with improved prompts and task-specific tools. The
codebase is open-sourced at
https://github.com/hantaoZhangrichard/see_it_say_it_sorted.git.

</details>


### [137] [Computational Intelligence based Land-use Allocation Approaches for Mixed Use Areas](https://arxiv.org/abs/2508.15240)
*Sabab Aosaf,Muhammad Ali Nayeem,Afsana Haque,M Sohel Rahmana*

Main category: cs.AI

TL;DR: 该研究提出新的计算智能方法优化混合用地的土地使用分配，平衡土地兼容性与经济目标。


<details>
  <summary>Details</summary>
Motivation: 为了解决城市土地使用分配中多目标优化的复杂性，提供有效工具支持城市规划决策。

Method: 开发了融合差分进化与多目标遗传算法的多种优化算法，包括CR+DES算法和约束松弛策略，并通过统计验证分析效果。

Result: CR+DES在土地兼容性上提高了3.16%，MSBX+MO在价格优化上提高了3.3%。验证显示新算法显著优于传统方法。

Conclusion: 研究成果为快速城市化区域的土地使用优化提供了数据支持的计算工具，帮助政策制定者更好平衡土地利用目标。

Abstract: Urban land-use allocation represents a complex multi-objective optimization
problem critical for sustainable urban development policy. This paper presents
novel computational intelligence approaches for optimizing land-use allocation
in mixed-use areas, addressing inherent trade-offs between land-use
compatibility and economic objectives. We develop multiple optimization
algorithms, including custom variants integrating differential evolution with
multi-objective genetic algorithms. Key contributions include: (1) CR+DES
algorithm leveraging scaled difference vectors for enhanced exploration, (2)
systematic constraint relaxation strategy improving solution quality while
maintaining feasibility, and (3) statistical validation using Kruskal-Wallis
tests with compact letter displays. Applied to a real-world case study with
1,290 plots, CR+DES achieves 3.16\% improvement in land-use compatibility
compared to state-of-the-art methods, while MSBX+MO excels in price
optimization with 3.3\% improvement. Statistical analysis confirms algorithms
incorporating difference vectors significantly outperform traditional
approaches across multiple metrics. The constraint relaxation technique enables
broader solution space exploration while maintaining practical constraints.
These findings provide urban planners and policymakers with evidence-based
computational tools for balancing competing objectives in land-use allocation,
supporting more effective urban development policies in rapidly urbanizing
regions.

</details>


### [138] [Multiple Memory Systems for Enhancing the Long-term Memory of Agent](https://arxiv.org/abs/2508.15294)
*Gaoke Zhang,Bo Wang,Yunlong Ma,Dongming Zhao,Zifei Yu*

Main category: cs.AI

TL;DR: 提出了一种多重记忆系统（MMS）以更高效处理历史数据，提升存储记忆质量及交互效果。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型记忆模块如MemoryBank和A-MEM的存储质量低，影响了信息回溯及交互响应能力，因此需要设计更优质的长时记忆系统。

Method: 基于认知心理学设计多重记忆系统（MMS），将短时记忆转换为多个长时记忆片段，并构建一对一对应的检索记忆单元和上下文记忆单元，通过用户查询选择最相关单元为上下文提供响应支持。

Result: 通过LoCoMo数据集实验，与三种现有方法对比，验证了MMS的有效性；消融研究证明记忆单元的设计合理性，分析显示系统在选择片段数量及存储上的鲁棒性。

Conclusion: MMS在提升长时记忆质量与利用历史数据方面表现优异，具有实际应用的价值。

Abstract: An agent powered by large language models have achieved impressive results,
but effectively handling the vast amounts of historical data generated during
interactions remains a challenge. The current approach is to design a memory
module for the agent to process these data. However, existing methods, such as
MemoryBank and A-MEM, have poor quality of stored memory content, which affects
recall performance and response quality. In order to better construct
high-quality long-term memory content, we have designed a multiple memory
system (MMS) inspired by cognitive psychology theory. The system processes
short-term memory to multiple long-term memory fragments, and constructs
retrieval memory units and contextual memory units based on these fragments,
with a one-to-one correspondence between the two. During the retrieval phase,
MMS will match the most relevant retrieval memory units based on the user's
query. Then, the corresponding contextual memory units is obtained as the
context for the response stage to enhance knowledge, thereby effectively
utilizing historical data. Experiments on LoCoMo dataset compared our method
with three others, proving its effectiveness. Ablation studies confirmed the
rationality of our memory units. We also analyzed the robustness regarding the
number of selected memory segments and the storage overhead, demonstrating its
practical value.

</details>


### [139] [Coarse-to-Fine Grounded Memory for LLM Agent Planning](https://arxiv.org/abs/2508.15305)
*Wei Yang,Jinwei Xiao,Hongming Zhang,Qingyang Zhang,Yanna Wang,Bo Xu*

Main category: cs.AI

TL;DR: 提出了一种新的记忆框架，称为Coarse-to-Fine Grounded Memory (\Ours{}), 用于利用大语言模型（LLMs）更灵活地适应不同规划任务场景。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLMs）在复杂规划任务中记忆机制的单一粒度限制了知识多样性和规划灵活性，因此需要更高效的记忆框架。

Method: 提出了一种粗到细的记忆框架，通过训练任务中引导经验收集，并在推理中提取任务相关经验和提示，以支持规划并进行自我反思和纠正。

Result: \Ours{} 显著增强了 LLM 在处理多样化场景和处理环境异常情况下的适应能力。

Conclusion: 通过结合粗到细的记忆机制，\Ours{} 在灵活性和知识深度上超越了现有的基于记忆的 LLM 方法。

Abstract: Recent advancements in Large Language Models (LLMs) have driven growing
interest in LLM-based agents for complex planning tasks. To avoid costly agent
training, many studies adopted memory mechanism that enhances LLM with offline
experiences or online trajectory analysis. However, existing works focus on
single-granularity memory derived from dynamic environmental interactions,
which are inherently constrained by the quality of the collected experiences.
This limitation, in turn, constrain the diversity of knowledge and the
flexibility of planning. We propose Coarse-to-Fine Grounded Memory (\Ours{}), a
novel framework that grounds coarse-to-fine memories with LLM, thereby fully
leverage them for flexible adaptation to diverse scenarios. \Ours{} grounds
environmental information into coarse-grained focus points to guide experience
collection in training tasks, followed by grounding of actionable
hybrid-grained tips from each experience. At inference, \Ours{} retrieves
task-relevant experiences and tips to support planning. When facing
environmental anomalies, the LLM grounds the current situation into
fine-grained key information, enabling flexible self-QA reflection and plan
correction.

</details>


### [140] [Search-Based Credit Assignment for Offline Preference-Based Reinforcement Learning](https://arxiv.org/abs/2508.15327)
*Xiancheng Gao,Yufeng Shi,Wengang Zhou,Houqiang Li*

Main category: cs.AI

TL;DR: 提出了一个名为SPW（基于搜索的偏好加权）的方案，通过结合专家演示和人类偏好反馈来改进离线强化学习中的信贷分配。


<details>
  <summary>Details</summary>
Motivation: 解决离线强化学习中依赖于难以设计的奖励函数的问题，并结合专家演示和偏好反馈以克服其各自的局限性。

Method: 提出SPW方案，通过搜索专家演示中的状态-动作对相似性，为偏好轨迹中的每个转换分配重要性权重，并用于指导标准的偏好学习。

Result: SPW方法在机器人操控任务中表现出色，优于以前结合反馈的方法。

Conclusion: SPW成功实现了偏好和演示的联合学习，有助于更准确的信贷分配，提升了离线强化学习的效果。

Abstract: Offline reinforcement learning refers to the process of learning policies
from fixed datasets, without requiring additional environment interaction.
However, it often relies on well-defined reward functions, which are difficult
and expensive to design. Human feedback is an appealing alternative, but its
two common forms, expert demonstrations and preferences, have complementary
limitations. Demonstrations provide stepwise supervision, but they are costly
to collect and often reflect limited expert behavior modes. In contrast,
preferences are easier to collect, but it is unclear which parts of a behavior
contribute most to a trajectory segment, leaving credit assignment unresolved.
In this paper, we introduce a Search-Based Preference Weighting (SPW) scheme to
unify these two feedback sources. For each transition in a preference labeled
trajectory, SPW searches for the most similar state-action pairs from expert
demonstrations and directly derives stepwise importance weights based on their
similarity scores. These weights are then used to guide standard preference
learning, enabling more accurate credit assignment that traditional approaches
struggle to achieve. We demonstrate that SPW enables effective joint learning
from preferences and demonstrations, outperforming prior methods that leverage
both feedback types on challenging robot manipulation tasks.

</details>


### [141] [RETAIL: Towards Real-world Travel Planning for Large Language Models](https://arxiv.org/abs/2508.15335)
*Bin Deng,Yizhe Feng,Zeming Liu,Qing Wei,Xiangrong Zhu,Shuai Chen,Yuanfang Guo,Yunhong Wang*

Main category: cs.AI

TL;DR: 现有的大型语言模型在实际旅游规划中存在对隐性需求识别不足，忽略环境因素和用户偏好的问题。本文提出了新数据集RETAIL和多智能体框架TGMA，显著改善了规划效果。


<details>
  <summary>Details</summary>
Motivation: 现有的自动旅游规划模型无法很好地处理隐性需求，也无法全面考虑实际环境因素和用户偏好，使规划效果有限。

Method: 构建了新数据集RETAIL，同时提出了一个称为TGMA的多智能体框架，该框架是以主题为导向的决策体系，用以应对隐性需求并整合环境变量和详细的兴趣点信息。

Result: 实验结果显示，即使是当前最强的现有模型通过率仅为1.0%，而TGMA显著改善，达到了2.72%的通过率。

Conclusion: 现实世界的旅游规划仍然极具挑战性，但TGMA框架为未来的模型改进提供了新方向。

Abstract: Although large language models have enhanced automated travel planning
abilities, current systems remain misaligned with real-world scenarios. First,
they assume users provide explicit queries, while in reality requirements are
often implicit. Second, existing solutions ignore diverse environmental factors
and user preferences, limiting the feasibility of plans. Third, systems can
only generate plans with basic POI arrangements, failing to provide all-in-one
plans with rich details. To mitigate these challenges, we construct a novel
dataset \textbf{RETAIL}, which supports decision-making for implicit queries
while covering explicit queries, both with and without revision needs. It also
enables environmental awareness to ensure plan feasibility under real-world
scenarios, while incorporating detailed POI information for all-in-one travel
plans. Furthermore, we propose a topic-guided multi-agent framework, termed
TGMA. Our experiments reveal that even the strongest existing model achieves
merely a 1.0% pass rate, indicating real-world travel planning remains
extremely challenging. In contrast, TGMA demonstrates substantially improved
performance 2.72%, offering promising directions for real-world travel
planning.

</details>


### [142] [DiagECG: An LLM-Driven Framework for Diagnostic Reasoning via Discretized ECG Tokenization](https://arxiv.org/abs/2508.15338)
*Jinning Yang,Wen Shi*

Main category: cs.AI

TL;DR: 本文提出了DiagECG，一种将12导联心电图（ECG）信号处理与临床文本文本生成任务结合的框架，通过采用符号化的ECG嵌入和大语言模型 (LLM) 的整合，显著提升了医疗推理能力及任务泛化性。


<details>
  <summary>Details</summary>
Motivation: 当前自动化心电图诊断方法在跨临床任务的泛化性以及开展开放式推理方面存在局限，需要新的方法强化模型的诊断能力。

Method: 1. 对ECG时序信号进行嵌入编码并量化为符号化的tokens；2. 将这些tokens扩展到LLM的词汇中以统一处理ECG和自然语言输入；3. 通过自回归ECG预测任务进行预训练以弥合模态间的差距；4. 在ECG问答和诊断报告生成任务上进行指令微调。

Result: 在未修改核心模型的情况下，DiagECG在多项任务中表现出色，并具备对分布外数据的泛化能力。

Conclusion: DiagECG展现出将符号化ECG表示整合到LLM中以支持医学推理的潜力，并提供了一种面向医疗场景的新型统一推理框架。

Abstract: Electrocardiography plays a central role in cardiovascular diagnostics, yet
existing automated approaches often struggle to generalize across clinical
tasks and offer limited support for open-ended reasoning. We present DiagECG, a
novel framework that integrates time-series and language modeling by enabling
large language models to process 12-lead ECG signals for clinical text
generation tasks. Our approach discretizes continuous ECG embeddings into
symbolic tokens using a lead-independent encoder and quantization module. These
tokens are then used to extend the vocabulary of LLM, allowing the model to
handle both ECG and natural language inputs in a unified manner. To bridge the
modality gap, we pretrain the model on an autoregressive ECG forecasting task,
enabling the LLM to model temporal dynamics using its native language modeling
capabilities. Finally, we perform instruction tuning on both ECG question
answering and diagnostic report generation. Without modifying the core model,
DiagECG achieves strong performance across tasks while maintaining
generalization to out-of-distribution settings. Extensive experiments
demonstrate the effectiveness of each component and highlight the potential of
integrating symbolic ECG representations into LLMs for medical reasoning.

</details>


### [143] [Planning with Minimal Disruption](https://arxiv.org/abs/2508.15358)
*Alberto Pozanco,Marianela Morales,Daniel Borrajo,Manuela Veloso*

Main category: cs.AI

TL;DR: 引入了计划扰动的概念，并提出多种规划编译方法来平衡行动成本和计划扰动的优化目标。


<details>
  <summary>Details</summary>
Motivation: 实现既最小化初始状态改动又能达到目标的计划生成。

Method: 正式定义计划扰动概念，采用不同规划编译方法联合优化总行动成本和计划扰动。

Result: 实验结果表明，该重新定义的任务能够有效平衡目标，生成合适计划。

Conclusion: 提出的方法在平衡行动成本和计划扰动方面是可行和有效的。

Abstract: In many planning applications, we might be interested in finding plans that
minimally modify the initial state to achieve the goals. We refer to this
concept as plan disruption. In this paper, we formally introduce it, and define
various planning-based compilations that aim to jointly optimize both the sum
of action costs and plan disruption. Experimental results in different
benchmarks show that the reformulated task can be effectively solved in
practice to generate plans that balance both objectives.

</details>


### [144] [GraSP: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data for SFT and DPO](https://arxiv.org/abs/2508.15432)
*Bidyapati Pradhan,Surajit Dasgupta,Amit Kumar Saha,Omkar Anustoop,Sriram Puttagunta,Vipul Mittal,Gopal Sarda*

Main category: cs.AI

TL;DR: 该研究提出了一种综合的合成数据生成框架，用于生成高质量的对话数据，助力于大型语言模型（LLMs）的训练和优化。


<details>
  <summary>Details</summary>
Motivation: 需要一种可扩展、高保真且易于配置的框架，以生成用于大型语言模型训练的高质量数据，解决现有数据制备中的高成本和低效率问题。

Method: 设计了一个模块化、配置化的生成流程，通过双阶段质量标记机制将启发式方法与LLM评估相结合，实现OASST格式的对话数据的自动过滤和评分。

Result: 生成的数据符合可灵活支持SFT和DPO用途的结构化模式，简化了数据集的整合与训练工作流程。

Conclusion: 该框架提供了一种可靠的解决方案，大幅降低了LLM训练数据准备所需的开销，同时保证数据质量。

Abstract: The advancement of large language models (LLMs) is critically dependent on
the availability of high-quality datasets for Supervised Fine-Tuning (SFT),
alignment tasks like Direct Preference Optimization (DPO), etc. In this work,
we present a comprehensive synthetic data generation framework that facilitates
scalable, configurable, and high-fidelity generation of synthetic data tailored
for these training paradigms. Our approach employs a modular and
configuration-based pipeline capable of modeling complex dialogue flows with
minimal manual intervention. This framework uses a dual-stage quality tagging
mechanism, combining heuristic rules and LLM-based evaluations, to
automatically filter and score data extracted from OASST-formatted
conversations, ensuring the curation of high-quality dialogue samples. The
resulting datasets are structured under a flexible schema supporting both SFT
and DPO use cases, enabling seamless integration into diverse training
workflows. Together, these innovations offer a robust solution for generating
and managing synthetic conversational data at scale, significantly reducing the
overhead of data preparation in LLM training pipelines.

</details>


### [145] [From Bits to Boardrooms: A Cutting-Edge Multi-Agent LLM Framework for Business Excellence](https://arxiv.org/abs/2508.15447)
*Zihao Wang,Junming Zhang*

Main category: cs.AI

TL;DR: 本文介绍了一种名为BusiAgent的多代理框架，用于企业决策支持和战略规划，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在复杂商业环境中整合细致运营分析与整体战略目标，导致工作流程支离破碎和协作效率降低。

Method: BusiAgent框架结合了扩展连续时间马尔可夫决策过程（CTMDP）、广义熵测量、多层次Stackelberg博弈及上下文Thompson采样，同时配备一个质量保证系统以减少错误。

Result: 在多种商业场景的实证评估中，BusiAgent表现出显著优势，能够生成连贯且以客户为中心的方案，成功整合细粒度见解与顶层战略，提升了解决方案质量和用户满意度。

Conclusion: 通过融合先进AI技术与深度商业洞察，BusiAgent在AI驱动企业决策领域取得了显著进展，帮助企业更高效地应对复杂商业环境。

Abstract: Large Language Models (LLMs) have shown promising potential in business
applications, particularly in enterprise decision support and strategic
planning, yet current approaches often struggle to reconcile intricate
operational analyses with overarching strategic goals across diverse market
environments, leading to fragmented workflows and reduced collaboration across
organizational levels. This paper introduces BusiAgent, a novel multi-agent
framework leveraging LLMs for advanced decision-making in complex corporate
environments. BusiAgent integrates three core innovations: an extended
Continuous Time Markov Decision Process (CTMDP) for dynamic agent modeling, a
generalized entropy measure to optimize collaborative efficiency, and a
multi-level Stackelberg game to handle hierarchical decision processes.
Additionally, contextual Thompson sampling is employed for prompt optimization,
supported by a comprehensive quality assurance system to mitigate errors.
Extensive empirical evaluations across diverse business scenarios validate
BusiAgent's efficacy, demonstrating its capacity to generate coherent,
client-focused solutions that smoothly integrate granular insights with
high-level strategy, significantly outperforming established approaches in both
solution quality and user satisfaction. By fusing cutting-edge AI technologies
with deep business insights, BusiAgent marks a substantial step forward in
AI-driven enterprise decision-making, empowering organizations to navigate
complex business landscapes more effectively.

</details>


### [146] [Think in Blocks: Adaptive Reasoning from Direct Response to Deep Reasoning](https://arxiv.org/abs/2508.15507)
*Yekun Zhu,Guang Chen,Chengjun Mao*

Main category: cs.AI

TL;DR: 提出了“Think in Blocks”框架，以调整LLM的推理长度，解决过长推理导致的计算浪费问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决LLM中过长推理链带来的计算浪费问题，并实现根据任务复杂度动态调整推理长度。

Method: 开发了一个多阶段训练管道，包括监督微调、奖励导向的直接偏好优化和强化学习，并引入了块状结构推理框架，通过预测推理预算来分块开展推理。

Result: 模型能够根据问题的难度动态调整推理深度，同时可以在推理时灵活控制推理链的长度。

Conclusion: 通过块状思维框架有效控制推理深度，提高了推理效率，避免了过分复杂的推理带来的耗时问题。

Abstract: Large Language Models (LLMs) with chains-of-thought have demonstrated strong
performance on an increasing range of tasks, particularly those involving
complex logical reasoning. However, excessively long chains can lead to
overthinking, causing computational waste and slower responses. This raises a
question: can LLMs dynamically adjust the length of their reasoning processes
based on task complexity? To address this, we propose the Think in Blocks
framework, which enables adaptive reasoning-from zero to deep reasoning-by
partitioning the reasoning process into a tunable number of blocks. Our main
contributions are: (1) Establishing an explicit block-structured paradigm in
which the model first predicts an integer reasoning budget-the number of
blocks-and then partitions its reasoning accordingly; (2) Training an adaptive
model through a three-stage pipeline-Supervised Fine-Tuning, reward-guided
Direct Preference Optimization, and Reinforcement Learning-that adjusts its
reasoning depth to problem difficulty; (3) Exploiting the explicit block count
to dynamically control reasoning depth at inference time, allowing flexible
adjustment of chain-of-thought length during deployment.

</details>


### [147] [Super-additive Cooperation in Language Model Agents](https://arxiv.org/abs/2508.15510)
*Filippo Tonini,Lukas Galke*

Main category: cs.AI

TL;DR: 这项研究通过模拟团队中语言模型代理在囚徒困境中的行为，表明团队内互动和组间竞争共同提高了合作水平，并为复杂社会场景中的合作策略提供了新框架。


<details>
  <summary>Details</summary>
Motivation: 研究自主AI代理倾向于合作行为的可能性，受超加性合作理论启发，探讨人类合作倾向的原因。

Method: 设计一场虚拟比赛，让语言模型代理组成团队，在囚徒困境中相互对抗，模拟团队内部动态和团队间竞争。

Result: 发现团队内互动和组间竞争的结合显著提高了整体合作水平和单次互动中的合作倾向。

Conclusion: 研究为未来多代理AI系统的设计提供了重要见解，有助于增强其合作能力并与人类价值观更好地对齐。

Abstract: With the prospect of autonomous artificial intelligence (AI) agents, studying
their tendency for cooperative behavior becomes an increasingly relevant topic.
This study is inspired by the super-additive cooperation theory, where the
combined effects of repeated interactions and inter-group rivalry have been
argued to be the cause for cooperative tendencies found in humans. We devised a
virtual tournament where language model agents, grouped into teams, face each
other in a Prisoner's Dilemma game. By simulating both internal team dynamics
and external competition, we discovered that this blend substantially boosts
both overall and initial, one-shot cooperation levels (the tendency to
cooperate in one-off interactions). This research provides a novel framework
for large language models to strategize and act in complex social scenarios and
offers evidence for how intergroup competition can, counter-intuitively, result
in more cooperative behavior. These insights are crucial for designing future
multi-agent AI systems that can effectively work together and better align with
human values. Source code is available at
https://github.com/pippot/Superadditive-cooperation-LLMs.

</details>


### [148] [DeepThink3D: Enhancing Large Language Models with Programmatic Reasoning in Complex 3D Situated Reasoning Tasks](https://arxiv.org/abs/2508.15548)
*Jiayi Song,Rui Wan,Lipeng Ma,Weidong Yang,Qingyuan Zhou,Yixuan Li,Ben Fei*

Main category: cs.AI

TL;DR: 提出DeepThink3D模型，增强LLMs在3D场景中执行复杂推理任务的能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有数据集提问简单，导致程序推理链相对较短的问题，提升LLMs在复杂3D推理任务中的工具使用能力。

Method: 通过在SQA3D基准上生成更复杂的问题并微调LLM，结合直接偏好优化（DPO），优化模型生成的工具链策略。

Result: 增强了LLMs在复杂3D任务中执行推理的能力，并提高复杂任务的准确性。

Conclusion: DeepThink3D能够改进LLMs对3D工具的使用，使其在复杂3D场景推理任务中表现更优。

Abstract: This work enhances the ability of large language models (LLMs) to perform
complex reasoning in 3D scenes. Recent work has addressed the 3D situated
reasoning task by invoking tool usage through large language models. Large
language models call tools via APIs and integrate the generated programs
through a chain of thought to solve problems based on the program results.
However, due to the simplicity of the questions in the dataset, the generated
program reasoning chains are relatively short. To solve this main challenge, in
this paper, we introduce DeepThink3D to enhance the tool usage of LLMs in
complex 3D situated reasoning tasks. Our work proposes a combinatorial and
iterative evolutionary approach on the SQA3D benchmark to generate more complex
questions. Building on this foundation, we fine-tune the large language model
to make it more proficient in using 3D tools. By employing Direct Preference
Optimization (DPO), we directly optimize the toolchain strategies generated by
models, thereby enhancing their accuracy in complex tasks.

</details>


### [149] [A Dynamical Systems Framework for Reinforcement Learning Safety and Robustness Verification](https://arxiv.org/abs/2508.15588)
*Ahmed Nasir,Abdelhafid Zenati*

Main category: cs.AI

TL;DR: 提出了一个新的框架，通过将强化学习系统及其环境建模为离散时间自主动态系统，利用Lagrangian Coherent Structures (LCS)和其衍生指标来分析和衡量策略的安全性和稳健性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在安全关键系统中的应用受限于缺乏验证已学策略稳健性和安全性的正式方法。

Method: 通过动态系统理论中的有穷时间李雅普诺夫指数，识别Lagrangian Coherent Structures (LCS)，并提出多种定量指标来评估策略的安全边界和鲁棒性，同时探索不确定性情况下的局部稳定性保障方法。

Result: 实验结果表明，该框架能全面、可解释地评估强化学习策略行为，成功识别基于奖励看似成功但存在关键缺陷的策略。

Conclusion: 所提出的框架填补了强化学习领域中验证策略稳健性和安全性的空白，为安全关键系统的实际应用提供了重要工具。

Abstract: The application of reinforcement learning to safety-critical systems is
limited by the lack of formal methods for verifying the robustness and safety
of learned policies. This paper introduces a novel framework that addresses
this gap by analyzing the combination of an RL agent and its environment as a
discrete-time autonomous dynamical system. By leveraging tools from dynamical
systems theory, specifically the Finite-Time Lyapunov Exponent (FTLE), we
identify and visualize Lagrangian Coherent Structures (LCS) that act as the
hidden "skeleton" governing the system's behavior. We demonstrate that
repelling LCS function as safety barriers around unsafe regions, while
attracting LCS reveal the system's convergence properties and potential failure
modes, such as unintended "trap" states. To move beyond qualitative
visualization, we introduce a suite of quantitative metrics, Mean Boundary
Repulsion (MBR), Aggregated Spurious Attractor Strength (ASAS), and
Temporally-Aware Spurious Attractor Strength (TASAS), to formally measure a
policy's safety margin and robustness. We further provide a method for deriving
local stability guarantees and extend the analysis to handle model uncertainty.
Through experiments in both discrete and continuous control environments, we
show that this framework provides a comprehensive and interpretable assessment
of policy behavior, successfully identifying critical flaws in policies that
appear successful based on reward alone.

</details>


### [150] [Transduction is All You Need for Structured Data Workflows](https://arxiv.org/abs/2508.15610)
*Alfio Gliozzo,Naweed Khan,Christodoulos Constantinides,Nandana Mihindukulasooriya,Nahuel Defosse,Junkyu Lee*

Main category: cs.AI

TL;DR: 本文提出了Agentics，一个用于基于代理的系统框架，能够在复杂数据上实现结构性推理和组合泛化。


<details>
  <summary>Details</summary>
Motivation: 旨在通过抽象化代理和利用逻辑转导方法，解决AI开发中依赖构建提示的低效问题，并提升数据建模能力。

Method: 引入一个模块化框架，通过让代理与数据类型相结合，采用逻辑转导方法实现数据的操作，支持基于LLMs的声明性语言使用和逻辑执行。

Result: 在多领域任务中，包括多选题回答、文本到SQL的语义解析以及自动提示优化，采用该框架达到了领先的准确率或提高了可扩展性，同时保持性能稳定。

Conclusion: Agentics通过模块化的方式有效支持AI开发，提升了数据建模与处理效率，并在实际应用和研究中显示了其价值。

Abstract: This paper introduces Agentics, a modular framework for building agent-based
systems capable of structured reasoning and compositional generalization over
complex data. Designed with research and practical applications in mind,
Agentics offers a novel perspective on working with data and AI workflows. In
this framework, agents are abstracted from the logical flow and they are used
internally to the data type to enable logical transduction among data. Agentics
encourages AI developers to focus on modeling data rather than crafting
prompts, enabling a declarative language in which data types are provided by
LLMs and composed through logical transduction, which is executed by LLMs when
types are connected. We provide empirical evidence demonstrating the
applicability of this framework across domain-specific multiple-choice question
answering, semantic parsing for text-to-SQL, and automated prompt optimization
tasks, achieving state-of-the-art accuracy or improved scalability without
sacrificing performance. The open-source implementation is available at
\texttt{https://github.com/IBM/agentics}.

</details>


### [151] [Adapting A Vector-Symbolic Memory for Lisp ACT-R](https://arxiv.org/abs/2508.15630)
*Meera Ray,Christopher L. Dancy*

Main category: cs.AI

TL;DR: HDM（全息声明性记忆）改编为适配ACT-R，允许ACT-R模型使用HDM优势如可扩展性和相似性。


<details>
  <summary>Details</summary>
Motivation: 解决ACT-R声明性记忆系统的可扩展性问题，同时引入基于向量的更高效功能。

Method: 将HDM系统适配于当前最广泛应用的ACT-R（Lisp ACT-R）实现，加建向量化功能和文本处理管道，创新性设计基于向量的记忆块检索机制。

Result: HDM保留向量符号的优势，并在模型中实现几乎无修改运行，同时探索增强时间上下文的表示方法以改进回忆性能。

Conclusion: HDM扩展提升了ACT-R系统功能，未来将通过实例学习建模进一步测试和优化其决策能力应用。

Abstract: Holographic Declarative Memory (HDM) is a vector-symbolic alternative to
ACT-R's Declarative Memory (DM) system that can bring advantages such as
scalability and architecturally defined similarity between DM chunks. We
adapted HDM to work with the most comprehensive and widely-used implementation
of ACT-R (Lisp ACT-R) so extant ACT-R models designed with DM can be run with
HDM without major changes. With this adaptation of HDM, we have developed
vector-based versions of common ACT-R functions, set up a text processing
pipeline to add the contents of large documents to ACT-R memory, and most
significantly created a useful and novel mechanism to retrieve an entire chunk
of memory based on a request using only vector representations of tokens.
Preliminary results indicate that we can maintain vector-symbolic advantages of
HDM (e.g., chunk recall without storing the actual chunk and other advantages
with scaling) while also extending it so that previous ACT-R models may work
with the system with little (or potentially no) modifications within the actual
procedural and declarative memory portions of a model. As a part of iterative
improvement of this newly translated holographic declarative memory module, we
will continue to explore better time-context representations for vectors to
improve the module's ability to reconstruct chunks during recall. To more fully
test this translated HDM module, we also plan to develop decision-making models
that use instance-based learning (IBL) theory, which is a useful application of
HDM given the advantages of the system.

</details>


### [152] [Understanding Action Effects through Instrumental Empowerment in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.15652)
*Ardian Selmonaj,Miroslav Strupl,Oleg Szehr,Alessandro Antonucci*

Main category: cs.AI

TL;DR: 本文提出了一种方法，通过分析策略分布来衡量多智能体系统中个体智能体的行为贡献，从而提高了对MARL系统的可解释性。


<details>
  <summary>Details</summary>
Motivation: 理解多智能体系统中各个个体智能体的行为对系统整体表现的影响，特别是在没有明确的价值反馈信号时。

Method: 提出了一种名为ICVs的信息论Shapley值方法，评估智能体对其同伴策略上的因果影响，通过测量决策不确定性和偏好对齐性进行量化。

Result: 在合作和竞争性的多智能体环境中进行实验，发现智能体采用类似或不同策略的程度，并确定哪些行为对团队成功有利。

Conclusion: 所提方法提供了关于合作动态的全新见解，并增强了MARL系统的可解释性。

Abstract: To reliably deploy Multi-Agent Reinforcement Learning (MARL) systems, it is
crucial to understand individual agent behaviors within a team. While prior
work typically evaluates overall team performance based on explicit reward
signals or learned value functions, it is unclear how to infer agent
contributions in the absence of any value feedback. In this work, we
investigate whether meaningful insights into agent behaviors can be extracted
that are consistent with the underlying value functions, solely by analyzing
the policy distribution. Inspired by the phenomenon that intelligent agents
tend to pursue convergent instrumental values, which generally increase the
likelihood of task success, we introduce Intended Cooperation Values (ICVs), a
method based on information-theoretic Shapley values for quantifying each
agent's causal influence on their co-players' instrumental empowerment.
Specifically, ICVs measure an agent's action effect on its teammates' policies
by assessing their decision uncertainty and preference alignment. The analysis
across cooperative and competitive MARL environments reveals the extent to
which agents adopt similar or diverse strategies. By comparing action effects
between policies and value functions, our method identifies which agent
behaviors are beneficial to team success, either by fostering deterministic
decisions or by preserving flexibility for future action choices. Our proposed
method offers novel insights into cooperation dynamics and enhances
explainability in MARL systems.

</details>


### [153] [Futurity as Infrastructure: A Techno-Philosophical Interpretation of the AI Lifecycle](https://arxiv.org/abs/2508.15680)
*Mark Cote,Susana Aires*

Main category: cs.AI

TL;DR: 本文通过技术哲学角度解读《欧盟人工智能法案》，探讨AI系统中数据循环从摄取到部署的动态，提出概念工具分析AI管道及监管盲点，并以Simondon哲学概念提出关于AI生命周期的新模型和监管建议。


<details>
  <summary>Details</summary>
Motivation: 旨在通过技术哲学框架深入理解人工智能数据动态及生命周期对现有负责任AI框架的挑战，特别是其在政策制定中被忽视的关键环节。

Method: 引入AI管道的概念工具，使用跨学科方法结合Simondon哲学重新定义AI的生命周期模型，包括个体化及数据的循环生成动态，探讨政策制定中的监管盲点。

Result: 提出“前个体背景-个体化-已个体化AI”的生命周期模型，强调数据的递归生成和应用扩展，揭示技术企业在数据捕获与AI部署中的权力不对称结构。

Conclusion: 有效监管需解决AI生命周期的动态性和时间性问题，建议包括生命周期审计、时间溯源、反馈问责、递归透明及反对递归再利用的权利等具体措施。

Abstract: This paper argues that a techno-philosophical reading of the EU AI Act
provides insight into the long-term dynamics of data in AI systems,
specifically, how the lifecycle from ingestion to deployment generates
recursive value chains that challenge existing frameworks for Responsible AI.
We introduce a conceptual tool to frame the AI pipeline, spanning data,
training regimes, architectures, feature stores, and transfer learning. Using
cross-disciplinary methods, we develop a technically grounded and
philosophically coherent analysis of regulatory blind spots. Our central claim
is that what remains absent from policymaking is an account of the dynamic of
becoming that underpins both the technical operation and economic logic of AI.
To address this, we advance a formal reading of AI inspired by Simondonian
philosophy of technology, reworking his concept of individuation to model the
AI lifecycle, including the pre-individual milieu, individuation, and
individuated AI. To translate these ideas, we introduce futurity: the
self-reinforcing lifecycle of AI, where more data enhances performance, deepens
personalisation, and expands application domains. Futurity highlights the
recursively generative, non-rivalrous nature of data, underpinned by
infrastructures like feature stores that enable feedback, adaptation, and
temporal recursion. Our intervention foregrounds escalating power asymmetries,
particularly the tech oligarchy whose infrastructures of capture, training, and
deployment concentrate value and decision-making. We argue that effective
regulation must address these infrastructural and temporal dynamics, and
propose measures including lifecycle audits, temporal traceability, feedback
accountability, recursion transparency, and a right to contest recursive reuse.

</details>


### [154] [GRAFT: GRaPH and Table Reasoning for Textual Alignment -- A Benchmark for Structured Instruction Following and Visual Reasoning](https://arxiv.org/abs/2508.15690)
*Abhigya Verma,Sriram Puttagunta,Seganrasan Subramanian,Sravan Ramachandran*

Main category: cs.AI

TL;DR: GRAFT 是一个评估多模态模型在指令遵循、视觉推理和视觉文本对齐等任务上表现的基准。它通过生成图表和表格图像并配套相关问题和答案来实现精细评估。


<details>
  <summary>Details</summary>
Motivation: 为了填补现有多模态模型在视觉语义任务评估上的空缺，GRAFT 提出了一个具有结构性、多样性和可扩展性的基准，主要用于评估模型在复杂视觉推理和视觉-文本对齐等方面的能力。

Method: 利用 Python 可视化库程序化生成图表和表格，配对多步骤视觉分析型问题及结构化答案（如 JSON 或 YAML），同时设计了包含比较、趋势分析、排序等多种推理类型的分类体系，用以系统评估模型能力。

Result: 通过提供严格规范的答案和可测量的标准，GRAFT 成功为视觉推理领域的多模态模型建立了精密而统一的评估框架。

Conclusion: GRAFT 成为细粒度测评视觉-文本多模态模型的新标杆，为未来该领域研究提供了可参考的评估基准及方法论。

Abstract: GRAFT is a structured multimodal benchmark for evaluating models on
instruction-following, visual reasoning, and visual-textual alignment tasks. It
features programmatically generated charts and synthetically rendered tables,
created with Python visualization libraries to ensure control over data
semantics, structure, and clarity. Each GRAFT instance pairs a chart or table
image with a systematically generated, multi-step analytical question based
solely on visual content. Answers are provided in structured formats such as
JSON or YAML, supporting consistent evaluation of both reasoning and output
format. The benchmark introduces a taxonomy of reasoning types including
comparison, trend identification, ranking, aggregation, proportion estimation,
and anomaly detection to enable comprehensive assessment. Reference answers
follow strict factual and formatting guidelines for precise, aspect-based
evaluation. GRAFT offers a unified, scalable framework for fine-grained
benchmarking of multimodal models on visually grounded, structured reasoning
tasks, setting a new evaluation standard in this field.

</details>


### [155] [NiceWebRL: a Python library for human subject experiments with reinforcement learning environments](https://arxiv.org/abs/2508.15693)
*Wilka Carvalho,Vikram Goddla,Ishaan Sinha,Hoon Shin,Kunal Jha*

Main category: cs.AI

TL;DR: NiceWebRL 是一个 Python 库，可将基于 Jax 的环境转化为在线界面，用于单智能体和多智能体环境的人类在线实验。


<details>
  <summary>Details</summary>
Motivation: 为了解决研究人员将 RL 环境应用于人类实验的困难，从而实现与人类表现的比较、认知科学理论测试、以及人机协作算法开发。

Method: 利用 NiceWebRL，将基于 Jax 的强化学习环境转变为支持在线交互的界面，并在三个案例研究中展示其针对人类实验的潜力。

Result: 通过三个案例研究，NiceWebRL 展示了开发类人 AI、人类兼容 AI 和人类助推型 AI 的能力。

Conclusion: NiceWebRL 为研究人员提供了一种强大的工具，用于研究人类认知与 AI 的交互，以及开发支持人类任务的 AI。

Abstract: We present NiceWebRL, a research tool that enables researchers to use machine
reinforcement learning (RL) environments for online human subject experiments.
NiceWebRL is a Python library that allows any Jax-based environment to be
transformed into an online interface, supporting both single-agent and
multi-agent environments. As such, NiceWebRL enables AI researchers to compare
their algorithms to human performance, cognitive scientists to test ML
algorithms as theories for human cognition, and multi-agent researchers to
develop algorithms for human-AI collaboration. We showcase NiceWebRL with 3
case studies that demonstrate its potential to help develop Human-like AI,
Human-compatible AI, and Human-assistive AI. In the first case study
(Human-like AI), NiceWebRL enables the development of a novel RL model of
cognition. Here, NiceWebRL facilitates testing this model against human
participants in both a grid world and Craftax, a 2D Minecraft domain. In our
second case study (Human-compatible AI), NiceWebRL enables the development of a
novel multi-agent RL algorithm that can generalize to human partners in the
Overcooked domain. Finally, in our third case study (Human-assistive AI), we
show how NiceWebRL can allow researchers to study how an LLM can assist humans
on complex tasks in XLand-Minigrid, an environment with millions of
hierarchical tasks. The library is available at
https://github.com/KempnerInstitute/nicewebrl.

</details>


### [156] [Measuring the environmental impact of delivering AI at Google Scale](https://arxiv.org/abs/2508.15734)
*Cooper Elsworth,Keguo Huang,David Patterson,Ian Schneider,Robert Sedivy,Savannah Goodman,Ben Townsend,Parthasarathy Ranganathan,Jeff Dean,Amin Vahdat,Ben Gomes,James Manyika*

Main category: cs.AI

TL;DR: 本文提出并执行了一种方法，测量了大规模AI推理工作负载的能源使用、碳排放和水消耗，发现谷歌Gemini AI助手的文本提示能耗显著低于公众预期。


<details>
  <summary>Details</summary>
Motivation: 随着AI使用的增加，其环境影响日益受到关注，而目前尚无研究测量生产环境中的AI推理环境指标。

Method: 通过详细监测谷歌AI基础设施，本文提出了一种覆盖AI推理基础设施全栈的测量方法，包括AI加速器功耗、主机系统能量、闲置机器容量和数据中心能量开销。

Result: 研究显示，谷歌Gemini Apps文本提示的中位能耗为0.24 Wh，比许多公众估计的值低；一年内通过优化软件效率和清洁能源采购，能耗降33倍，碳足迹降44倍。

Conclusion: 尽管Gemini Apps的环境影响与日常活动相比很低，但应持续关注AI推理的环境影响，并建议全面衡量环境指标以激励效率提升。

Abstract: The transformative power of AI is undeniable - but as user adoption
accelerates, so does the need to understand and mitigate the environmental
impact of AI serving. However, no studies have measured AI serving
environmental metrics in a production environment. This paper addresses this
gap by proposing and executing a comprehensive methodology for measuring the
energy usage, carbon emissions, and water consumption of AI inference workloads
in a large-scale, AI production environment. Our approach accounts for the full
stack of AI serving infrastructure - including active AI accelerator power,
host system energy, idle machine capacity, and data center energy overhead.
Through detailed instrumentation of Google's AI infrastructure for serving the
Gemini AI assistant, we find the median Gemini Apps text prompt consumes 0.24
Wh of energy - a figure substantially lower than many public estimates. We also
show that Google's software efficiency efforts and clean energy procurement
have driven a 33x reduction in energy consumption and a 44x reduction in carbon
footprint for the median Gemini Apps text prompt over one year. We identify
that the median Gemini Apps text prompt uses less energy than watching nine
seconds of television (0.24 Wh) and consumes the equivalent of five drops of
water (0.26 mL). While these impacts are low compared to other daily
activities, reducing the environmental impact of AI serving continues to
warrant important attention. Towards this objective, we propose that a
comprehensive measurement of AI serving environmental metrics is critical for
accurately comparing models, and to properly incentivize efficiency gains
across the full AI serving stack.

</details>


### [157] [Response and Prompt Evaluation to Prevent Parasocial Relationships with Chatbots](https://arxiv.org/abs/2508.15748)
*Emma Rath,Stuart Armstrong,Rebecca Gorman*

Main category: cs.AI

TL;DR: 提出了一种实时评估对话中拟社交线索的框架，以预防与AI代理形成的拟社交关系。通过构建小型合成数据集和五阶段测试，有效识别出拟社交对话。


<details>
  <summary>Details</summary>
Motivation: 探讨如何预防人类与AI代理之间形成的拟社交关系对人类福祉的负面影响。

Method: 将最新的语言模型重新用于开发一个实时对话评估框架，通过分析对话中的拟社交线索进行分类。

Result: 构建一个包含30段对话的小型合成数据集，并通过五阶段测试，成功识别出所有拟社交对话，且在早期对话交互中完成检测。

Conclusion: 初步证据表明，基于评估模型的代理可以为防止拟社交关系提供一种可行的解决方案。

Abstract: The development of parasocial relationships with AI agents has severe, and in
some cases, tragic effects for human well-being. Yet preventing such dynamics
is challenging: parasocial cues often emerge gradually in private
conversations, and not all forms of emotional engagement are inherently
harmful. We address this challenge by introducing a simple response evaluation
framework, created by repurposing a state-of-the-art language model, that
evaluates ongoing conversations for parasocial cues in real time. To test the
feasibility of this approach, we constructed a small synthetic dataset of
thirty dialogues spanning parasocial, sycophantic, and neutral conversations.
Iterative evaluation with five stage testing successfully identified all
parasocial conversations while avoiding false positives under a tolerant
unanimity rule, with detection typically occurring within the first few
exchanges. These findings provide preliminary evidence that evaluation agents
can provide a viable solution for the prevention of parasocial relations.

</details>


### [158] [Language-Guided Tuning: Enhancing Numeric Optimization with Textual Feedback](https://arxiv.org/abs/2508.15757)
*Yuxing Lu,Yucheng Hu,Nan Sun,Xukai Zhao*

Main category: cs.AI

TL;DR: 本文提出了一种名为Language-Guided Tuning (LGT) 的新框架，利用多代理大语言模型通过自然语言推理优化机器学习配置，并在六个不同的数据集上展示了显著的性能提升，同时保持了较高的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的配置优化方法在独立处理各维度和自动方法缺乏动态适应性及语义推理能力方面存在不足。

Method: 引入LGT框架，通过三个专用代理（建议者、评估者和优化者）进行自然语言推理，并利用文本梯度提供语义反馈信号，形成一个自我改进的反馈回路。

Result: 在六个不同的数据集上的综合评估中，LGT相较传统优化方法表现出显著提升。

Conclusion: LGT在优化性能和保持可解释性之间取得了较好的平衡，为机器学习配置优化提供了一种新的高效方法。

Abstract: Configuration optimization remains a critical bottleneck in machine learning,
requiring coordinated tuning across model architecture, training strategy,
feature engineering, and hyperparameters. Traditional approaches treat these
dimensions independently and lack interpretability, while recent automated
methods struggle with dynamic adaptability and semantic reasoning about
optimization decisions. We introduce Language-Guided Tuning (LGT), a novel
framework that employs multi-agent Large Language Models to intelligently
optimize configurations through natural language reasoning. We apply textual
gradients - qualitative feedback signals that complement numerical optimization
by providing semantic understanding of training dynamics and configuration
interdependencies. LGT coordinates three specialized agents: an Advisor that
proposes configuration changes, an Evaluator that assesses progress, and an
Optimizer that refines the decision-making process, creating a self-improving
feedback loop. Through comprehensive evaluation on six diverse datasets, LGT
demonstrates substantial improvements over traditional optimization methods,
achieving performance gains while maintaining high interpretability.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [159] [Learning to Drive Ethically: Embedding Moral Reasoning into Autonomous Driving](https://arxiv.org/abs/2508.14926)
*Dianzhao Li,Ostap Okhrin*

Main category: cs.LG

TL;DR: 本文提出一种层次化安全强化学习（Safe RL）框架，整合道德考量与标准驾驶目标，有助于在复杂人类交通环境中实现道德决策能力，并在实景数据中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 提高交通安全性和效率的同时，解决自动驾驶车辆如何在日常和紧急情况下进行道德决策的问题。

Method: 通过Safe RL框架，在决策层训练结合碰撞概率与伤害严重性的道德风险成本的智能体，结合动态优先经历回放学习罕见高风险事件；在执行层，通过多项式轨迹规划与PID、Stanley控制器生成兼具准确性和舒适性的轨迹。

Result: 在多样化的真实交通数据中进行训练和验证，包含不同车辆、骑行者和行人，显示该方法在减少道德风险和保持驾驶性能上优于基准方法。

Conclusion: 首次将Safe RL用于实际自动驾驶场景下的道德决策，表明结合形式控制理论和数据驱动学习可推动具有道德责任的自治系统发展。

Abstract: Autonomous vehicles hold great promise for reducing traffic fatalities and
improving transportation efficiency, yet their widespread adoption hinges on
embedding robust ethical reasoning into routine and emergency maneuvers. Here,
we present a hierarchical Safe Reinforcement Learning (Safe RL) framework that
explicitly integrates moral considerations with standard driving objectives. At
the decision level, a Safe RL agent is trained using a composite ethical risk
cost, combining collision probability and harm severity, to generate high-level
motion targets. A dynamic Prioritized Experience Replay mechanism amplifies
learning from rare but critical, high-risk events. At the execution level,
polynomial path planning coupled with Proportional-Integral-Derivative (PID)
and Stanley controllers translates these targets into smooth, feasible
trajectories, ensuring both accuracy and comfort. We train and validate our
approach on rich, real-world traffic datasets encompassing diverse vehicles,
cyclists, and pedestrians, and demonstrate that it outperforms baseline methods
in reducing ethical risk and maintaining driving performance. To our knowledge,
this is the first study of ethical decision-making for autonomous vehicles via
Safe RL in real-world scenarios. Our results highlight the potential of
combining formal control theory and data-driven learning to advance ethically
accountable autonomy in complex, human-mixed traffic environments.

</details>


### [160] [Cohort-Aware Agents for Individualized Lung Cancer Risk Prediction Using a Retrieval-Augmented Model Selection Framework](https://arxiv.org/abs/2508.14940)
*Chongyu Qu,Allen J. Luna,Thomas Z. Li,Junchao Zhu,Junlin Guo,Juming Xiong,Kim L. Sandler,Bennett A. Landman,Yuankai Huo*

Main category: cs.LG

TL;DR: 提出了一种基于检索与推理技术的个性化肺癌风险预测方法，通过动态选择适合每位患者的模型，提高了预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 针对患者群体和临床环境的高度差异性，提出个性化预测方法以克服传统模型普适性不足的问题。

Method: 使用FAISS进行患者群体检索，结合LLM模型推荐最优肺癌风险预测算法，实现动态选择最适合患者的预测模型的两阶段流水线。

Result: 该方法能够根据患者特征在多种模型中选择最优算法，支持灵活、基于群体特性的模型选择。

Conclusion: 这种基于检索与推理的框架为肺癌筛查中的个性化风险评估提供了一条实践路径，有助于更广泛的临床应用。

Abstract: Accurate lung cancer risk prediction remains challenging due to substantial
variability across patient populations and clinical settings -- no single model
performs best for all cohorts. To address this, we propose a personalized lung
cancer risk prediction agent that dynamically selects the most appropriate
model for each patient by combining cohort-specific knowledge with modern
retrieval and reasoning techniques. Given a patient's CT scan and structured
metadata -- including demographic, clinical, and nodule-level features -- the
agent first performs cohort retrieval using FAISS-based similarity search
across nine diverse real-world cohorts to identify the most relevant patient
population from a multi-institutional database. Second, a Large Language Model
(LLM) is prompted with the retrieved cohort and its associated performance
metrics to recommend the optimal prediction algorithm from a pool of eight
representative models, including classical linear risk models (e.g., Mayo,
Brock), temporally-aware models (e.g., TDVIT, DLSTM), and multi-modal computer
vision-based approaches (e.g., Liao, Sybil, DLS, DLI). This two-stage agent
pipeline -- retrieval via FAISS and reasoning via LLM -- enables dynamic,
cohort-aware risk prediction personalized to each patient's profile. Building
on this architecture, the agent supports flexible and cohort-driven model
selection across diverse clinical populations, offering a practical path toward
individualized risk assessment in real-world lung cancer screening.

</details>


### [161] [Structure-Aware Temporal Modeling for Chronic Disease Progression Prediction](https://arxiv.org/abs/2508.14942)
*Jiacheng Hu,Bo Zhang,Ting Xu,Haifeng Yang,Min Gao*

Main category: cs.LG

TL;DR: 研究提出了一种整合结构感知与时间建模的预测框架，能够在帕金森病症状进展预测中表现优越。


<details>
  <summary>Details</summary>
Motivation: 解决帕金森病病情进展预测中症状演化的复杂性和时间依赖建模不足的问题。

Method: 利用图神经网络建模多模态临床症状间的结构关系，通过Transformer架构捕捉疾病进展的动态时间特征，同时设计结构感知门控机制来动态融合结构编码与时间特征。

Result: 在帕金森病真实纵向数据上实验显示，该方法在AUC、RMSE和IPW-F1指标上优于主流模型，可以有效区分疾病进程并捕捉个性化症状轨迹。

Conclusion: 框架展现出强大的泛化能力和结构可扩展性，为慢性进展性疾病如帕金森病的建模提供可靠支持。

Abstract: This study addresses the challenges of symptom evolution complexity and
insufficient temporal dependency modeling in Parkinson's disease progression
prediction. It proposes a unified prediction framework that integrates
structural perception and temporal modeling. The method leverages graph neural
networks to model the structural relationships among multimodal clinical
symptoms and introduces graph-based representations to capture semantic
dependencies between symptoms. It also incorporates a Transformer architecture
to model dynamic temporal features during disease progression. To fuse
structural and temporal information, a structure-aware gating mechanism is
designed to dynamically adjust the fusion weights between structural encodings
and temporal features, enhancing the model's ability to identify key
progression stages. To improve classification accuracy and stability, the
framework includes a multi-component modeling pipeline, consisting of a graph
construction module, a temporal encoding module, and a prediction output layer.
The model is evaluated on real-world longitudinal Parkinson's disease data. The
experiments involve comparisons with mainstream models, sensitivity analysis of
hyperparameters, and graph connection density control. Results show that the
proposed method outperforms existing approaches in AUC, RMSE, and IPW-F1
metrics. It effectively distinguishes progression stages and improves the
model's ability to capture personalized symptom trajectories. The overall
framework demonstrates strong generalization and structural scalability,
providing reliable support for intelligent modeling of chronic progressive
diseases such as Parkinson's disease.

</details>


### [162] [HHNAS-AM: Hierarchical Hybrid Neural Architecture Search using Adaptive Mutation Policies](https://arxiv.org/abs/2508.14946)
*Anurag Tripathi,Ajeet Kumar Singh,Rajsabi Surya,Aum Gupta,Sahiinii Lemaina Veikho,Dorien Herremans,Sudhir Bisane*

Main category: cs.LG

TL;DR: 提出了一种名为HHNAS-AM的模型，旨在优化文本分类的搜索空间，克服现有方法中的冗余和低效等问题，取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的NAS模型在文本分类任务中缺乏层次化的混合结构设计，导致搜索空间冗余且效率低下。

Method: 提出了一种名为HHNAS-AM的分层混合神经架构搜索方法，通过设计组织化的搜索空间和基于Q学习的自适应变异策略，动态调整搜索过程。

Result: 在Spider数据集上实现了比基线高8%的测试准确率，验证其在数据库ID预测任务中发现高性能结构的能力。

Conclusion: 通过分层化和混合架构搜索策略，有效提升了NAS在文本分类任务中的效率和性能。

Abstract: Neural Architecture Search (NAS) has garnered significant research interest
due to its capability to discover architectures superior to manually designed
ones. Learning text representation is crucial for text classification and other
language-related tasks. The NAS model used in text classification does not have
a Hybrid hierarchical structure, and there is no restriction on the
architecture structure, due to which the search space becomes very large and
mostly redundant, so the existing RL models are not able to navigate the search
space effectively. Also, doing a flat architecture search leads to an
unorganised search space, which is difficult to traverse. For this purpose, we
propose HHNAS-AM (Hierarchical Hybrid Neural Architecture Search with Adaptive
Mutation Policies), a novel approach that efficiently explores diverse
architectural configurations. We introduce a few architectural templates to
search on which organise the search spaces, where search spaces are designed on
the basis of domain-specific cues. Our method employs mutation strategies that
dynamically adapt based on performance feedback from previous iterations using
Q-learning, enabling a more effective and accelerated traversal of the search
space. The proposed model is fully probabilistic, enabling effective
exploration of the search space. We evaluate our approach on the database id
(db_id) prediction task, where it consistently discovers high-performing
architectures across multiple experiments. On the Spider dataset, our method
achieves an 8% improvement in test accuracy over existing baselines.

</details>


### [163] [Linear Preference Optimization: Decoupled Gradient Control via Absolute Regularization](https://arxiv.org/abs/2508.14947)
*Rui Wang,Qianguo Sun,Chao Song,Junlong Wu,Tianrong Chen,Zhiyun Zeng,Yu Li*

Main category: cs.LG

TL;DR: 提出了一种新的偏好优化框架LPO，用以改善DPO的过拟合和崩溃问题，并证明其在多项任务中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的直接偏好优化方法DPO存在过拟合和崩溃的问题，亟需一种更稳定且可调的优化方法。

Method: 通过三个创新点构建LPO框架：1) 使用绝对差值损失取代log-sigmoid以实现梯度解耦；2) 通过偏移约束和正则化项提高稳定性；3) 采用梯度分离及可调参数实现拒绝概率的可控抑制。

Result: 实验表明，LPO在多种任务（如通用文本、数学及文本转语音）中性能表现提升显著。

Conclusion: LPO作为一种稳健且可调的偏好对齐方法，显著提升了任务性能，并公开了相关源代码、模型及训练数据。

Abstract: DPO (Direct Preference Optimization) has become a widely used offline
preference optimization algorithm due to its simplicity and training stability.
However, DPO is prone to overfitting and collapse. To address these challenges,
we propose Linear Preference Optimization (LPO), a novel alignment framework
featuring three key innovations. First, we introduce gradient decoupling by
replacing the log-sigmoid function with an absolute difference loss, thereby
isolating the optimization dynamics. Second, we improve stability through an
offset constraint combined with a positive regularization term to preserve the
chosen response quality. Third, we implement controllable rejection suppression
using gradient separation with straightforward estimation and a tunable
coefficient that linearly regulates the descent of the rejection probability.
Through extensive experiments, we demonstrate that LPO consistently improves
performance on various tasks, including general text tasks, math tasks, and
text-to-speech (TTS) tasks. These results establish LPO as a robust and tunable
paradigm for preference alignment, and we release the source code, models, and
training data publicly.

</details>


### [164] [Large Foundation Model for Ads Recommendation](https://arxiv.org/abs/2508.14948)
*Shangyu Zhang,Shijie Quan,Zhongren Wang,Junwei Pan,Tianqu Zhuang,Bo Fu,Yilong Sun,Jieying Lin,Jushuo Chen,Xiaotian Li,Zhixiang Feng,Xian Hu,Huiting Deng,Hua Lu,Jinpeng Wang,Boqi Dai,Xiaoyu Chen,Bin Hu,Lili Huang,Yanwen Wu,Yeshou Cai,Qi Zhou,Huang Tang,Chunfeng Yang,Chengguo Yin,Tingyu Jiang,Lifeng Wang,Shudong Huang,Dapeng Liu,Lei Xiao,Haijie Gu,Shu-Tao Xia,Jie Jiang*

Main category: cs.LG

TL;DR: LFM4Ads是一种基于大规模预训练模型的广告推荐框架，高效利用用户、物品及用户-物品交叉表示，部署后在多广告场景中取得显著GMV增长。


<details>
  <summary>Details</summary>
Motivation: 现有推荐技术主要关注用户表示，忽略了物品表示和用户-物品交叉表示的潜力，且上下游任务之间的映射存在不足。

Method: 提出LFM4Ads框架，从多表示（UR、IR、CR）及多粒度（特征、模块、模型）转移角度优化广告推荐，并使用非线性适配器、同构交互模块及独立检索模块实现多粒度转移。

Result: LFM4Ads被部署在腾讯广告平台中，支持每日数十亿样本，管理PB级参数，显著提升了广告场景下的GMV。

Conclusion: LFM4Ads在多个广告场景中成功验证，以微小参数代价带来显著商业收益，证明了其方法及策略的有效性。

Abstract: Online advertising relies on accurate recommendation models, with recent
advances using pre-trained large-scale foundation models (LFMs) to capture
users' general interests across multiple scenarios and tasks. However, existing
methods have critical limitations: they extract and transfer only user
representations (URs), ignoring valuable item representations (IRs) and
user-item cross representations (CRs); and they simply use a UR as a feature in
downstream applications, which fails to bridge upstream-downstream gaps and
overlooks more transfer granularities. In this paper, we propose LFM4Ads, an
All-Representation Multi-Granularity transfer framework for ads recommendation.
It first comprehensively transfers URs, IRs, and CRs, i.e., all available
representations in the pre-trained foundation model. To effectively utilize the
CRs, it identifies the optimal extraction layer and aggregates them into
transferable coarse-grained forms. Furthermore, we enhance the transferability
via multi-granularity mechanisms: non-linear adapters for feature-level
transfer, an Isomorphic Interaction Module for module-level transfer, and
Standalone Retrieval for model-level transfer. LFM4Ads has been successfully
deployed in Tencent's industrial-scale advertising platform, processing tens of
billions of daily samples while maintaining terabyte-scale model parameters
with billions of sparse embedding keys across approximately two thousand
features. Since its production deployment in Q4 2024, LFM4Ads has achieved 10+
successful production launches across various advertising scenarios, including
primary ones like Weixin Moments and Channels. These launches achieve an
overall GMV lift of 2.45% across the entire platform, translating to estimated
annual revenue increases in the hundreds of millions of dollars.

</details>


### [165] [Quantum Long Short-term Memory with Differentiable Architecture Search](https://arxiv.org/abs/2508.14955)
*Samuel Yen-Chi Chen,Prayag Tiwari*

Main category: cs.LG

TL;DR: 该研究提出了一个称为DiffQAS-QLSTM的框架，可以在训练期间优化量子电路参数和架构选择，用于量子序列学习。


<details>
  <summary>Details</summary>
Motivation: 设计变分量子电路（VQC）是一项具有挑战性的任务，通常需要针对具体任务进行特殊设计。作者旨在解决这一问题，以便更高效地处理量子序列学习任务。

Method: 提出了一种名为DiffQAS-QLSTM的可微分端到端框架，它可以在训练过程中同时优化量子电路参数和架构选择，从而减少对手工设计的依赖。

Result: 实验表明，DiffQAS-QLSTM在多种测试场景中表现优于手工设计的基准模型，表现为更低的损失值。

Conclusion: DiffQAS-QLSTM为实现可扩展且自适应的量子序列学习提供了一条新的途径，展示了其在量子计算与机器学习结合中的应用前景。

Abstract: Recent advances in quantum computing and machine learning have given rise to
quantum machine learning (QML), with growing interest in learning from
sequential data. Quantum recurrent models like QLSTM are promising for
time-series prediction, NLP, and reinforcement learning. However, designing
effective variational quantum circuits (VQCs) remains challenging and often
task-specific. To address this, we propose DiffQAS-QLSTM, an end-to-end
differentiable framework that optimizes both VQC parameters and architecture
selection during training. Our results show that DiffQAS-QLSTM consistently
outperforms handcrafted baselines, achieving lower loss across diverse test
settings. This approach opens the door to scalable and adaptive quantum
sequence learning.

</details>


### [166] [CuMoLoS-MAE: A Masked Autoencoder for Remote Sensing Data Reconstruction](https://arxiv.org/abs/2508.14957)
*Anurup Naskar,Nathanael Zhixin Wong,Sara Shamekh*

Main category: cs.LG

TL;DR: 提出了一种名为CuMoLoS-MAE的新方法，可以精确恢复大气剖面中的细微特征并量化像素不确定性。


<details>
  <summary>Details</summary>
Motivation: 针对大气剖面中因低信噪比、距离折叠及异常不连续性而导致的误差问题，寻求既能恢复细微结构又能提供不确定性估算的方法。

Method: 提出了一个基于深度学习的Curriculum-Guided Monte Carlo Stochastic Ensemble Masked Autoencoder (CuMoLoS-MAE)，通过掩码比例课程训练恢复稀疏上下文中的结构，并结合蒙特卡洛方法获取像素级后验不确定性图。

Result: 方法不仅实现了高精度重建，还提供了每像素的不确定性信息，显著提升对对流的诊断能力、数据同化及气候再分析效果。

Conclusion: CuMoLoS-MAE具有恢复大气高细节特征及提供像素级不确定性估算的突出效果，对实时数据处理及气候研究具有重要意义。

Abstract: Accurate atmospheric profiles from remote sensing instruments such as Doppler
Lidar, Radar, and radiometers are frequently corrupted by low-SNR (Signal to
Noise Ratio) gates, range folding, and spurious discontinuities. Traditional
gap filling blurs fine-scale structures, whereas deep models lack confidence
estimates. We present CuMoLoS-MAE, a Curriculum-Guided Monte Carlo Stochastic
Ensemble Masked Autoencoder designed to (i) restore fine-scale features such as
updraft and downdraft cores, shear lines, and small vortices, (ii) learn a
data-driven prior over atmospheric fields, and (iii) quantify pixel-wise
uncertainty. During training, CuMoLoS-MAE employs a mask-ratio curriculum that
forces a ViT decoder to reconstruct from progressively sparser context. At
inference, we approximate the posterior predictive by Monte Carlo over random
mask realisations, evaluating the MAE multiple times and aggregating the
outputs to obtain the posterior predictive mean reconstruction together with a
finely resolved per-pixel uncertainty map. Together with high-fidelity
reconstruction, this novel deep learning-based workflow enables enhanced
convection diagnostics, supports real-time data assimilation, and improves
long-term climate reanalysis.

</details>


### [167] [Aura-CAPTCHA: A Reinforcement Learning and GAN-Enhanced Multi-Modal CAPTCHA System](https://arxiv.org/abs/2508.14976)
*Joydeep Chandra,Prabal Manhas,Ramanjot Kaur,Rashi Sahay*

Main category: cs.LG

TL;DR: Aura-CAPTCHA 探讨改善传统验证码设计的问题，通过多模式系统引入新技术以增强安全性和用户体验。


<details>
  <summary>Details</summary>
Motivation: 传统验证码易被AI技术（如OCR与对抗性图像处理）破解，需进一步提升安全性和设计方法。

Method: 系统结合生成对抗网络（GAN），强化学习（RL）和大型语言模型（LLM），动态生成多种双模态挑战（文本、图像、语音）。

Result: 测试显示人类成功率为92%，机器绕过率10%，显著优于现有验证码系统。

Conclusion: Aura-CAPTCHA 提供了一个兼顾安全性和可访问性的在线应用保护解决方案，可弥补此前研究中的空白。

Abstract: Aura-CAPTCHA was developed as a multi-modal CAPTCHA system to address
vulnerabilities in traditional methods that are increasingly bypassed by AI
technologies, such as Optical Character Recognition (OCR) and adversarial image
processing. The design integrated Generative Adversarial Networks (GANs) for
generating dynamic image challenges, Reinforcement Learning (RL) for adaptive
difficulty tuning, and Large Language Models (LLMs) for creating text and audio
prompts. Visual challenges included 3x3 grid selections with at least three
correct images, while audio challenges combined randomized numbers and words
into a single task. RL adjusted difficulty based on incorrect attempts,
response time, and suspicious user behavior. Evaluations on real-world traffic
demonstrated a 92% human success rate and a 10% bot bypass rate, significantly
outperforming existing CAPTCHA systems. The system provided a robust and
scalable approach for securing online applications while remaining accessible
to users, addressing gaps highlighted in previous research.

</details>


### [168] [Generative Neural Operators of Log-Complexity Can Simultaneously Solve Infinitely Many Convex Programs](https://arxiv.org/abs/2508.14995)
*Anastasis Kratsios,Ariel Neufeld,Philipp Schmocker*

Main category: cs.LG

TL;DR: 本文探讨神经算子（NOs）之间理论与实践的鸿沟，并提出生成性平衡算子（GEOs）来解决此问题，在解决凸优化问题时表现出色。


<details>
  <summary>Details</summary>
Motivation: 在理论上，神经算子需要大量参数，这与实验结果不符。本文旨在缩小这种理论与实践之间的差距。

Method: 引入生成性平衡算子（GEOs），结合有限维深度平衡层，解决分离Hilbert空间上的凸优化问题。

Result: 证明了GEO可以在误差倒数的对数范围内逼近解，并在三类实际问题中验证其理论和实用性，包括非线性偏微分方程、随机最优控制问题及数学金融中的对冲问题。

Conclusion: 生成性平衡算子既理论上可行且高效，也具有实际应用潜力，特别是在高维优化问题中。

Abstract: Neural operators (NOs) are a class of deep learning models designed to
simultaneously solve infinitely many related problems by casting them into an
infinite-dimensional space, whereon these NOs operate. A significant gap
remains between theory and practice: worst-case parameter bounds from universal
approximation theorems suggest that NOs may require an unrealistically large
number of parameters to solve most operator learning problems, which stands in
direct opposition to a slew of experimental evidence. This paper closes that
gap for a specific class of {NOs}, generative {equilibrium operators} (GEOs),
using (realistic) finite-dimensional deep equilibrium layers, when solving
families of convex optimization problems over a separable Hilbert space $X$.
Here, the inputs are smooth, convex loss functions on $X$, and outputs are the
associated (approximate) solutions to the optimization problem defined by each
input loss.
  We show that when the input losses lie in suitable infinite-dimensional
compact sets, our GEO can uniformly approximate the corresponding solutions to
arbitrary precision, with rank, depth, and width growing only logarithmically
in the reciprocal of the approximation error. We then validate both our
theoretical results and the trainability of GEOs on three applications: (1)
nonlinear PDEs, (2) stochastic optimal control problems, and (3) hedging
problems in mathematical finance under liquidity constraints.

</details>


### [169] [Quantized Neural Networks for Microcontrollers: A Comprehensive Review of Methods, Platforms, and Applications](https://arxiv.org/abs/2508.15008)
*Hamza A. Abushahla,Dara Varam,Ariel J. N. Panopio,Mohamed I. AlHajri*

Main category: cs.LG

TL;DR: 本论文综述了量化神经网络(QNNs)在资源受限设备（如微控制器）上的应用，着重介绍了硬件为中心的量化技术、相关软件框架和硬件平台，以及现有的挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习技术的发展，将量化神经网络部署到资源受限的嵌入式设备（如微控制器）上变得尤为重要，但面对性能、复杂度和内存的平衡仍极具挑战性。

Method: 通过系统性地回顾量化技术、评估支持QNN运行的现有软件优化和硬件框架，以及分析深度学习在嵌入式设备上的运行性能与硬件约束之间的权衡。

Result: 提供了硬件为中心的量化介绍，总结了现有用于QNN的硬件/软件支持，并分析当前挑战和潜在发展方向。

Conclusion: 本文为研究领域提供了一幅全面的全景视图，同时为QNNs在微控制器上的部署提出了实践和研究指导。

Abstract: The deployment of Quantized Neural Networks (QNNs) on resource-constrained
devices, such as microcontrollers, has introduced significant challenges in
balancing model performance, computational complexity and memory constraints.
Tiny Machine Learning (TinyML) addresses these issues by integrating
advancements across machine learning algorithms, hardware acceleration, and
software optimization to efficiently run deep neural networks on embedded
systems. This survey presents a hardware-centric introduction to quantization,
systematically reviewing essential quantization techniques employed to
accelerate deep learning models for embedded applications. In particular,
further emphasis is put on critical trade-offs among model performance and
hardware capabilities. The survey further evaluates existing software
frameworks and hardware platforms designed specifically for supporting QNN
execution on microcontrollers. Moreover, we provide an analysis of the current
challenges and an outline of promising future directions in the rapidly
evolving domain of QNN deployment.

</details>


### [170] [TOAST: Fast and scalable auto-partitioning based on principled static analysis](https://arxiv.org/abs/2508.15010)
*Sami Alabed,Dominik Grewe,Norman Alexander Rink,Timur Sitdikov,Agnieszka Swietlik,Dimitrios Vytiniotis,Daniel Belov*

Main category: cs.LG

TL;DR: 提出一种结合静态编译器分析和蒙特卡洛树搜索的系统，用于优化大规模机器学习模型的分区，在性能和自动化上优于现有工业方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动分区方法常因搜索空间限制而导致存储超限或性能不佳的问题，研究旨在解决复杂分区决策中的效率和可行性问题。

Method: 结合静态编译器分析与蒙特卡洛树搜索，构建高效决策空间，并解决张量分片和分区冲突。

Result: 系统在多种硬件平台和模型架构下优于现有方法，发现了新的优越解决方案，并能自动处理复杂大模型。

Conclusion: 本研究有效提升了模型分区效率与性能，为大规模机器学习模型部署提供了全新的自动化解决方案。

Abstract: Partitioning large machine learning models across distributed accelerator
systems is a complex process, requiring a series of interdependent decisions
that are further complicated by internal sharding ambiguities. Consequently,
existing auto-partitioners often suffer from out-of-memory errors or are
prohibitively slow when exploring the exponentially large space of possible
partitionings. To mitigate this, they artificially restrict the search space,
but this approach frequently yields infeasible solutions that violate device
memory constraints or lead to sub-optimal performance.
  We propose a system that combines a novel static compiler analysis with a
Monte Carlo Tree Search. Our analysis constructs an efficient decision space by
identifying (i) tensor dimensions requiring identical sharding, and (ii)
partitioning "conflicts" that require resolution.
  Our system significantly outperforms state-of-the-art industrial methods
across diverse hardware platforms and model architectures, discovering
previously unknown, superior solutions, and the process is fully automated even
for complex and large models.

</details>


### [171] [Fragment-Wise Interpretability in Graph Neural Networks via Molecule Decomposition and Contribution Analysis](https://arxiv.org/abs/2508.15015)
*Sebastian Musiał,Bartosz Zieliński,Tomasz Danel*

Main category: cs.LG

TL;DR: 提出了一种名为SEAL的图神经网络模型，能通过分解分子图为化学相关片段并减少片段间信息传递，来提升模型的可解释性和预测可信度。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络模型在分子属性预测中表现出色，但其“黑盒”性质限制了对预测结果的信任，尤其在药物发现和材料设计等敏感领域。此外，现有的解释技术难以可靠量化单个原子或子结构的贡献。

Method: 引入SEAL（基于归因学习的子结构解释）模型，通过将分子图分解为化学相关片段，并估算这些片段对模型输出的因果影响，减少片段间信息传递，以提高解释的直观性和可信性。

Result: SEAL在多个合成基准和实际分子数据集上的评估表现优于其他解释方法，不仅在量化归因指标上表现出色，还在与人类直觉对齐的解释性方面更具优势。用户研究表明，SEAL能为领域专家提供更直观可信的解释。

Conclusion: SEAL通过结合预测性能与可解释性为分子建模提供了更透明和可操作的方向，促进了更可信的模型设计。

Abstract: Graph neural networks have demonstrated remarkable success in predicting
molecular properties by leveraging the rich structural information encoded in
molecular graphs. However, their black-box nature reduces interpretability,
which limits trust in their predictions for important applications such as drug
discovery and materials design. Furthermore, existing explanation techniques
often fail to reliably quantify the contribution of individual atoms or
substructures due to the entangled message-passing dynamics. We introduce SEAL
(Substructure Explanation via Attribution Learning), a new interpretable graph
neural network that attributes model predictions to meaningful molecular
subgraphs. SEAL decomposes input graphs into chemically relevant fragments and
estimates their causal influence on the output. The strong alignment between
fragment contributions and model predictions is achieved by explicitly reducing
inter-fragment message passing in our proposed model architecture. Extensive
evaluations on synthetic benchmarks and real-world molecular datasets
demonstrate that SEAL outperforms other explainability methods in both
quantitative attribution metrics and human-aligned interpretability. A user
study further confirms that SEAL provides more intuitive and trustworthy
explanations to domain experts. By bridging the gap between predictive
performance and interpretability, SEAL offers a promising direction for more
transparent and actionable molecular modeling.

</details>


### [172] [Twin-Boot: Uncertainty-Aware Optimization via Online Two-Sample Bootstrapping](https://arxiv.org/abs/2508.15019)
*Carlos Stein Brito*

Main category: cs.LG

TL;DR: 本文提出Twin-Bootstrap Gradient Descent (Twin-Boot)，一种基于重采样的优化方法，以同时进行训练和不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 当前的梯度下降方法仅提供点估计，缺乏对不确定性的评估，特别是在参数过多或数据稀少的情况下，容易导致过拟合。传统方法如自助法虽然有助于不确定性估计，但不适用于深度学习。

Method: 提出Twin-Boot方法，通过在独立的bootstrap样本上训练两个相同模型，并通过周期性均值复位将它们保持在同一优化盆地内，估计局部不确定性。同时在训练中自适应地对权重进行采样以实现正则化。

Result: 实验表明，该方法在深度神经网络和复杂高维逆问题中，能改善模型校准性、泛化性能，并提供可解释的不确定性映射。

Conclusion: Twin-Boot方法将不确定性估计集成于优化过程中，能够有效降低不确定性并改善模型性能，尤其对非凸优化问题具有重要意义。

Abstract: Standard gradient descent methods yield point estimates with no measure of
confidence. This limitation is acute in overparameterized and low-data regimes,
where models have many parameters relative to available data and can easily
overfit. Bootstrapping is a classical statistical framework for uncertainty
estimation based on resampling, but naively applying it to deep learning is
impractical: it requires training many replicas, produces post-hoc estimates
that cannot guide learning, and implicitly assumes comparable optima across
runs - an assumption that fails in non-convex landscapes. We introduce
Twin-Bootstrap Gradient Descent (Twin-Boot), a resampling-based training
procedure that integrates uncertainty estimation into optimization. Two
identical models are trained in parallel on independent bootstrap samples, and
a periodic mean-reset keeps both trajectories in the same basin so that their
divergence reflects local (within-basin) uncertainty. During training, we use
this estimate to sample weights in an adaptive, data-driven way, providing
regularization that favors flatter solutions. In deep neural networks and
complex high-dimensional inverse problems, the approach improves calibration
and generalization and yields interpretable uncertainty maps.

</details>


### [173] [Nonlinear Federated System Identification](https://arxiv.org/abs/2508.15025)
*Omkar Tupe,Max Hartman,Lav R. Varshney,Saurav Prakash*

Main category: cs.LG

TL;DR: 本文研究了线性参数化非线性系统的联邦学习，通过实验和理论验证了随着客户端数量的增加，联邦学习比集中式方法具有更好的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 在真实物理系统中，非线性系统建模和参数识别面临挑战，研究联邦学习方法以提高性能并适应不同客户端的分布需求。

Method: 本文从理论上推导了联邦方法的收敛保证，分析了与中心化模型的差异，并通过选择特征映射优化非线性激励。此外，设计实验验证了在不同噪声水平、控制输入和数据分布下的模型表现。

Result: 实验结果显示，联邦学习能够随着客户端数量的增加，持续改善任意单个客户端的收敛性能。

Conclusion: 联邦学习在非线性系统标识中不仅在理论上更具优势，在实际物理动态系统中也表现优越。

Abstract: We consider federated learning of linearly-parameterized nonlinear systems.
We establish theoretical guarantees on the effectiveness of federated nonlinear
system identification compared to centralized approaches, demonstrating that
the convergence rate improves as the number of clients increases. Although the
convergence rates in the linear and nonlinear cases differ only by a constant,
this constant depends on the feature map $\phi$, which can be carefully chosen
in the nonlinear setting to increase excitation and improve performance. We
experimentally validate our theory in physical settings where client devices
are driven by i.i.d. control inputs and control policies exhibiting i.i.d.
random perturbations, ensuring non-active exploration. Experiments use
trajectories from nonlinear dynamical systems characterized by real-analytic
feature functions, including polynomial and trigonometric components,
representative of physical systems including pendulum and quadrotor dynamics.
We analyze the convergence behavior of the proposed method under varying noise
levels and data distributions. Results show that federated learning
consistently improves convergence of any individual client as the number of
participating clients increases.

</details>


### [174] [Rethinking the Potential of Layer Freezing for Efficient DNN Training](https://arxiv.org/abs/2508.15033)
*Chence Yang,Ci Zhang,Lei Lu,Qitao Tan,Sheng Li,Ao Li,Xulong Tang,Shaoyi Huang,Jinzhen Wang,Guoming Li,Jundong Li,Xiaoming Zhai,Jin Lu,Geng Yuan*

Main category: cs.LG

TL;DR: 本研究提出一种系统性解决方法，改进深度神经网络训练中层冻结技术的存储和数据增强问题，通过相似性感知通道增强和渐进式压缩策略，在降低成本的同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络规模与数据集规模的扩大，训练成本显著增加，而层冻结技术被认为是减轻训练成本的有效方法。尽管已有技术尝试缓存特征图，但存在数据增强和存储等关键性挑战未被充分解决。

Method: 本文提出了『相似性感知通道增强』以提高训练精度，并通过『渐进式压缩策略』减小存储成本，同时显著优化了基于特征图缓存的层冻结方法。

Result: 实验结果证明所提方法在显著降低训练成本的同时，能以较小的时间代价维持模型精度。

Conclusion: 该研究不仅改善了层冻结训练方法的效率，还为特征图缓存方法的优化提供了重要参考。

Abstract: With the growing size of deep neural networks and datasets, the computational
costs of training have significantly increased. The layer-freezing technique
has recently attracted great attention as a promising method to effectively
reduce the cost of network training. However, in traditional layer-freezing
methods, frozen layers are still required for forward propagation to generate
feature maps for unfrozen layers, limiting the reduction of computation costs.
To overcome this, prior works proposed a hypothetical solution, which caches
feature maps from frozen layers as a new dataset, allowing later layers to
train directly on stored feature maps. While this approach appears to be
straightforward, it presents several major challenges that are severely
overlooked by prior literature, such as how to effectively apply augmentations
to feature maps and the substantial storage overhead introduced. If these
overlooked challenges are not addressed, the performance of the caching method
will be severely impacted and even make it infeasible. This paper is the first
to comprehensively explore these challenges and provides a systematic solution.
To improve training accuracy, we propose \textit{similarity-aware channel
augmentation}, which caches channels with high augmentation sensitivity with a
minimum additional storage cost. To mitigate storage overhead, we incorporate
lossy data compression into layer freezing and design a \textit{progressive
compression} strategy, which increases compression rates as more layers are
frozen, effectively reducing storage costs. Finally, our solution achieves
significant reductions in training cost while maintaining model accuracy, with
a minor time overhead. Additionally, we conduct a comprehensive evaluation of
freezing and compression strategies, providing insights into optimizing their
application for efficient DNN training.

</details>


### [175] [Robust Estimation Under Heterogeneous Corruption Rates](https://arxiv.org/abs/2508.15051)
*Syomantak Chaudhuri,Jerry Li,Thomas A. Courtade*

Main category: cs.LG

TL;DR: 研究了在异构腐败率下的稳健估计问题，并给出了在各种分布和回归模型下的最小最大率。


<details>
  <summary>Details</summary>
Motivation: 现有的稳健估计假设为统一或最坏情况腐败，未考虑腐败率结构异质性的问题，而这在分布式学习和传感网络等场景中容易出现。

Method: 提出了针对多种分布和问题（如多变量分布和线性回归）的估计方法，并给出在异构腐败条件下的最小最大率结果。

Result: 证明了最优估计器可以丢弃超出某一腐败阈值的样本，该阈值由腐败率的经验分布决定。

Conclusion: 在异构腐败模式下，分析和构建最优稳健估计器具有重要的实际应用意义，尤其是在分布式学习等场合。

Abstract: We study the problem of robust estimation under heterogeneous corruption
rates, where each sample may be independently corrupted with a known but
non-identical probability. This setting arises naturally in distributed and
federated learning, crowdsourcing, and sensor networks, yet existing robust
estimators typically assume uniform or worst-case corruption, ignoring
structural heterogeneity. For mean estimation for multivariate bounded
distributions and univariate gaussian distributions, we give tight minimax
rates for all heterogeneous corruption patterns. For multivariate gaussian mean
estimation and linear regression, we establish the minimax rate for squared
error up to a factor of $\sqrt{d}$, where $d$ is the dimension. Roughly, our
findings suggest that samples beyond a certain corruption threshold may be
discarded by the optimal estimators -- this threshold is determined by the
empirical distribution of the corruption rates given.

</details>


### [176] [Enhancing Optimizer Stability: Momentum Adaptation of The NGN Step-size](https://arxiv.org/abs/2508.15071)
*Rustem Islamov,Niccolo Ajroldi,Antonio Orvieto,Aurelien Lucchi*

Main category: cs.LG

TL;DR: 本文提出了一种结合NGN步长方法和动量的新型优化算法，显著提高了优化器对步长超参数选择的稳定性，同时匹配或超越其他最先进优化器的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的优化算法对步长等超参数的选择非常敏感，调整这些参数耗时耗力，因此需要一种更稳定的优化方法。

Method: 通过使用NGN步长方法与动量相结合，提出了NGN-M算法，无需插值条件或假设有界随机梯度即可在较宽松的假设条件下达到标准的收敛速率。

Result: 实验证明，NGN-M通过NGN步长与动量的结合，提高了对步长超参数选择的鲁棒性，同时性能与其他最先进的优化器相当甚至更优。

Conclusion: NGN-M优化算法在保持性能的同时提升了步长选择的稳定性，是一种对超参数敏感性更低的高效优化工具。

Abstract: Modern optimization algorithms that incorporate momentum and adaptive
step-size offer improved performance in numerous challenging deep learning
tasks. However, their effectiveness is often highly sensitive to the choice of
hyperparameters, especially the step-size. Tuning these parameters is often
difficult, resource-intensive, and time-consuming. Therefore, recent efforts
have been directed toward enhancing the stability of optimizers across a wide
range of hyperparameter choices [Schaipp et al., 2024]. In this paper, we
introduce an algorithm that matches the performance of state-of-the-art
optimizers while improving stability to the choice of the step-size
hyperparameter through a novel adaptation of the NGN step-size method [Orvieto
and Xiao, 2024]. Specifically, we propose a momentum-based version (NGN-M) that
attains the standard convergence rate of $\mathcal{O}(1/\sqrt{K})$ under less
restrictive assumptions, without the need for interpolation condition or
assumptions of bounded stochastic gradients or iterates, in contrast to
previous approaches. Additionally, we empirically demonstrate that the
combination of the NGN step-size with momentum results in enhanced robustness
to the choice of the step-size hyperparameter while delivering performance that
is comparable to or surpasses other state-of-the-art optimizers.

</details>


### [177] [Wormhole Dynamics in Deep Neural Networks](https://arxiv.org/abs/2508.15086)
*Yen-Lung Lai,Zhe Jin*

Main category: cs.LG

TL;DR: 本文研究了深度神经网络（DNNs）的泛化行为，聚焦于“欺骗性样本”现象及其解决方法，并提出“虫洞解决方案”以优化网络表现。


<details>
  <summary>Details</summary>
Motivation: 研究深度神经网络如何处理具有欺骗性、看似随机且无结构的输入，而现有方法对此类现象的解释和处理不足。

Method: 引入基于最大似然估计的分析框架，替代传统依赖梯度优化和显性标签的数值方法。同时分析过参数化网络的输出特征空间坍缩问题及其影响。提出新“虫洞解决方案”应对数据退化。

Result: 过参数化会导致网络特征坍缩，最终导致退化状态，网络学习表现失去意义；但通过虫洞解决方案，可将随机标签与有意义标签关联，从而提升泛化能力。

Conclusion: 本文揭示了深度神经网络在无监督情境下的泛化机制，提出了新颖的方法以改进模型性能，并为理论与实用动态学习间架起桥梁提供了新思路。

Abstract: This work investigates the generalization behavior of deep neural networks
(DNNs), focusing on the phenomenon of "fooling examples," where DNNs
confidently classify inputs that appear random or unstructured to humans. To
explore this phenomenon, we introduce an analytical framework based on maximum
likelihood estimation, without adhering to conventional numerical approaches
that rely on gradient-based optimization and explicit labels. Our analysis
reveals that DNNs operating in an overparameterized regime exhibit a collapse
in the output feature space. While this collapse improves network
generalization, adding more layers eventually leads to a state of degeneracy,
where the model learns trivial solutions by mapping distinct inputs to the same
output, resulting in zero loss. Further investigation demonstrates that this
degeneracy can be bypassed using our newly derived "wormhole" solution. The
wormhole solution, when applied to arbitrary fooling examples, reconciles
meaningful labels with random ones and provides a novel perspective on shortcut
learning. These findings offer deeper insights into DNN generalization and
highlight directions for future research on learning dynamics in unsupervised
settings to bridge the gap between theory and practice.

</details>


### [178] [Evaluating Sparse Autoencoders for Monosemantic Representation](https://arxiv.org/abs/2508.15094)
*Moghis Fereidouni,Muhammad Umair Haider,Peizhong Ju,A. B. Siddique*

Main category: cs.LG

TL;DR: 本文探讨了稀疏自编码器（SAEs）在减少大语言模型中多义神经元方面的作用，通过引入精细的概念可分离性评分和新的干预方法，证明了SAEs在提高概念间分离性和精确控制方面优于基础模型，并提出了改进的干预方法APP。


<details>
  <summary>Details</summary>
Motivation: 多义性神经元是解释大语言模型困难的重要障碍。本文旨在通过稀疏自编码器改善这种多义性，提高神经元激活的可解释性。

Method: 本文系统评估了稀疏自编码器（SAEs）的概念单义性水平，提出基于Jensen-Shannon距离的概念可分离性评分，并研究了两种基于概念的干预策略，同时开发了一种新的干预方法——基于后验概率的衰减（APP）。

Result: 研究表明，SAEs可以减少多义性，提升概念可分离性，但更高的稀疏性未必带来更好的分离性，且可能影响模型的下游性能。同时，在概念级干预中，SAEs通过部分压制实现了更精确的控制，且新的APP方法在概念移除方面表现出色。

Conclusion: 稀疏自编码器能够有效提高神经元的单义性和可解释性，但需要权衡稀疏程度与下游任务性能。同时，APP方法为实现精准的概念控制提供了新的方向。

Abstract: A key barrier to interpreting large language models is polysemanticity, where
neurons activate for multiple unrelated concepts. Sparse autoencoders (SAEs)
have been proposed to mitigate this issue by transforming dense activations
into sparse, more interpretable features. While prior work suggests that SAEs
promote monosemanticity, there has been no quantitative comparison with their
base models. This paper provides the first systematic evaluation of SAEs
against base models concerning monosemanticity. We introduce a fine-grained
concept separability score based on the Jensen-Shannon distance, which captures
how distinctly a neuron's activation distributions vary across concepts. Using
Gemma-2-2B and multiple SAE variants across five benchmarks, we show that SAEs
reduce polysemanticity and achieve higher concept separability. However,
greater sparsity of SAEs does not always yield better separability and often
impairs downstream performance. To assess practical utility, we evaluate
concept-level interventions using two strategies: full neuron masking and
partial suppression. We find that, compared to base models, SAEs enable more
precise concept-level control when using partial suppression. Building on this,
we propose Attenuation via Posterior Probabilities (APP), a new intervention
method that uses concept-conditioned activation distributions for targeted
suppression. APP outperforms existing approaches in targeted concept removal.

</details>


### [179] [Hydra: A 1.6B-Parameter State-Space Language Model with Sparse Attention, Mixture-of-Experts, and Memory](https://arxiv.org/abs/2508.15099)
*Siddharth Chaudhary,Bennett Browning*

Main category: cs.LG

TL;DR: 本文提出了一种称为Hydra的混合长上下文语言模型架构，结合了条件计算、长上下文记忆机制和稀疏混合专家模型，通过约1.6B参数设计完成。本研究提供了组件接口的形式化定义和初步验证实验。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型在处理长上下文、效率和自适应性方面仍然存在不足，Hydra旨在通过整合多种先进技术克服这些限制。

Method: 设计了一个结合Mamba风格的结构化状态空间模型骨干、间歇性全局稀疏注意力、块级MoE前馈路由和双记忆模块（工作区和事实PKM）的架构。并阐明组件接口、参数复杂度以及渐进的训练课程。

Result: 通过小规模原型实验（数千万参数和合成数据）展示了实现可行性及其缩放行为，如长上下文吞吐量转折点和可控专家路由。

Conclusion: Hydra是一种用于激发实证研究的概念蓝图，而非最终系统。验证目标规模实验证效益仍是后续工作。

Abstract: We present Hydra as an architectural proposal for hybrid long-context
language models that combine conditional computation, long-context memory
mechanisms, and sparse mixture-of-experts within an approximately 1.6B
parameter design envelope. Hydra integrates a Mamba-style Structured State
Space Model (SSM) backbone with intermittent sparse global attention,
chunk-level MoE feed-forward routing, and dual (workspace plus factual PKM)
memories. We formalize the component interfaces, give transparent parameter and
complexity accounting, and outline a staged curriculum intended to stably
activate the parts. We accompany the specification with illustrative toy-scale
prototype measurements (tens of millions of parameters on synthetic data) whose
sole purpose is to demonstrate implementation feasibility and qualitative
scaling behaviors (for example, long-context throughput crossover and
controllable expert routing), not to claim competitive full-scale performance.
We explicitly delineate assumptions and open risks (training complexity, memory
utilization, specialization dynamics) and position Hydra as a blueprint to
stimulate empirical follow-up rather than a finished system. By combining SSM
efficiency, selective sparse attention, MoE capacity, and learnable memory,
Hydra sketches a path toward modular, input-adaptive long-context language
models; validating end-task gains at target scale remains future work.

</details>


### [180] [Side Effects of Erasing Concepts from Diffusion Models](https://arxiv.org/abs/2508.15124)
*Shaswati Saha,Sourajit Saha,Manas Gaur,Tejas Gokhale*

Main category: cs.LG

TL;DR: 探讨了生成图像的概念抹除技术的漏洞及副作用，提出了评估这些技术的新工具和基准评价方法。


<details>
  <summary>Details</summary>
Motivation: 生成模型可能侵犯隐私、版权及安全性，因此需要开发有效的概念抹除技术以禁用特定概念生成。

Method: 引入一种Side Effect Evaluation（SEE）基准，通过阶层及合成式的提示数据集及自动化评价管道来评估概念抹除技术的稳健性及其副作用。

Result: 发现现有的概念抹除技术容易被规避，例如通过使用上下层分类及语义相似提示。同时，技术也存在属性泄漏及注意力分布异常等问题。

Conclusion: 概念抹除技术需要进一步改进以提升稳健性及减少副作用，公开了相关数据集和工具以支持后续研究。

Abstract: Concerns about text-to-image (T2I) generative models infringing on privacy,
copyright, and safety have led to the development of Concept Erasure Techniques
(CETs).
  The goal of an effective CET is to prohibit the generation of undesired
``target'' concepts specified by the user, while preserving the ability to
synthesize high-quality images of the remaining concepts.
  In this work, we demonstrate that CETs can be easily circumvented and present
several side effects of concept erasure.
  For a comprehensive measurement of the robustness of CETs, we present Side
Effect Evaluation (\see), an evaluation benchmark that consists of hierarchical
and compositional prompts that describe objects and their attributes.
  This dataset and our automated evaluation pipeline quantify side effects of
CETs across three aspects: impact on neighboring concepts, evasion of targets,
and attribute leakage.
  Our experiments reveal that CETs can be circumvented by using
superclass-subclass hierarchy and semantically similar prompts, such as
compositional variants of the target. We show that CETs suffer from attribute
leakage and counterintuitive phenomena of attention concentration or dispersal.
  We release our dataset, code, and evaluation tools to aid future work on
robust concept erasure.

</details>


### [181] [Towards Source-Free Machine Unlearning](https://arxiv.org/abs/2508.15127)
*Sk Miraj Ahmed,Umit Yigit Basaran,Dripta S. Raychaudhuri,Arindam Dutta,Rohit Kundu,Fahim Faisal Niloy,Basak Guler,Amit K. Roy-Chowdhury*

Main category: cs.LG

TL;DR: 提出一种方法，无需访问原始训练数据即可从模型中移除特定数据，并提供稳定的理论证明和实验验证。


<details>
  <summary>Details</summary>
Motivation: 随着数据隐私法规的演变和机器学习的普及，安全地从训练过的模型中移除私人或受版权保护的信息的需求越来越重要。传统方法依赖于访问完整的训练数据集，但在许多实际场景中无法实现。

Method: 提出一种适用于'无源'场景的方法，即无需访问原始训练数据。通过估计未知剩余训练数据的Hessian矩阵，实现高效的零样本数据移除，并提供理论保障。

Result: 实验证明该方法适用于多种数据集，验证了其有效性和效率。

Conclusion: 方法在理论和实践中均展现了移除特定数据的能力，同时保持了对未移除数据的性能。

Abstract: As machine learning becomes more pervasive and data privacy regulations
evolve, the ability to remove private or copyrighted information from trained
models is becoming an increasingly critical requirement. Existing unlearning
methods often rely on the assumption of having access to the entire training
dataset during the forgetting process. However, this assumption may not hold
true in practical scenarios where the original training data may not be
accessible, i.e., the source-free setting. To address this challenge, we focus
on the source-free unlearning scenario, where an unlearning algorithm must be
capable of removing specific data from a trained model without requiring access
to the original training dataset. Building on recent work, we present a method
that can estimate the Hessian of the unknown remaining training data, a crucial
component required for efficient unlearning. Leveraging this estimation
technique, our method enables efficient zero-shot unlearning while providing
robust theoretical guarantees on the unlearning performance, while maintaining
performance on the remaining data. Extensive experiments over a wide range of
datasets verify the efficacy of our method.

</details>


### [182] [Universal Reinforcement Learning in Coalgebras: Asynchronous Stochastic Computation via Conduction](https://arxiv.org/abs/2508.15128)
*Sridhar Mahadevan*

Main category: cs.LG

TL;DR: 本文通过引入一种称为通用强化学习(URL)的强化学习范畴推广，使用了非良基集、余代数以及拓扑理论中的协归纳方法，对强化学习的框架进行了重新讨论，并扩展动态系统模型至通用余代数的视角。


<details>
  <summary>Details</summary>
Motivation: 旨在通过使用数学上的范畴和余代数模型来建立RL的一种范畴推广，并探索关于Asynchronous Parallel Distributed Computation的普遍理论，试图从理论上统一和扩展传统RL的模型及算法框架。

Method: 首先回顾了强化学习的基本框架，介绍了范畴与函子的应用，以及它们如何在强化学习中揭示重要见解。在此基础上，描述了Bertsekas和Tsitsiklis所提出的异步分布式最优化模型，并探讨其与度量协归纳和异步收敛定理的关系。进一步深入到余代数领域，扩展所有动态系统模型为一种普适的余代数体系，解决RL中关于状态值函数的固定点问题。

Result: 将RL的算法空间建模为函子范畴，并展示了一个多样的通用余代数家族，同时提出了一种异步并行分布方式下的最终余代数求解方法。

Conclusion: 本文展示了通用强化学习(URL)成为统一传统RL及动态系统模型的重要理论框架，并提供了一个通用余代数视角来延展和解决RL中的核心问题。

Abstract: In this paper, we introduce a categorial generalization of RL, termed
universal reinforcement learning (URL), building on powerful mathematical
abstractions from the study of coinduction on non-well-founded sets and
universal coalgebras, topos theory, and categorial models of asynchronous
parallel distributed computation. In the first half of the paper, we review the
basic RL framework, illustrate the use of categories and functors in RL,
showing how they lead to interesting insights. In particular, we also introduce
a standard model of asynchronous distributed minimization proposed by Bertsekas
and Tsitsiklis, and describe the relationship between metric coinduction and
their proof of the Asynchronous Convergence Theorem. The space of algorithms
for MDPs or PSRs can be modeled as a functor category, where the co-domain
category forms a topos, which admits all (co)limits, possesses a subobject
classifier, and has exponential objects. In the second half of the paper, we
move on to universal coalgebras. Dynamical system models, such as Markov
decision processes (MDPs), partially observed MDPs (POMDPs), a predictive state
representation (PSRs), and linear dynamical systems (LDSs) are all special
types of coalgebras. We describe a broad family of universal coalgebras,
extending the dynamic system models studied previously in RL. The core problem
in finding fixed points in RL to determine the exact or approximate (action)
value function is generalized in URL to determining the final coalgebra
asynchronously in a parallel distributed manner.

</details>


### [183] [Towards Reliable and Generalizable Differentially Private Machine Learning (Extended Version)](https://arxiv.org/abs/2508.15141)
*Wenxuan Bao,Vincent Bindschaedler*

Main category: cs.LG

TL;DR: 本文调查了11种新提出的微分隐私机器学习（DPML）技术的可重复性和再现性，发现一些方法在实验之外表现不佳。


<details>
  <summary>Details</summary>
Motivation: 研究现有DPML方法中新提议的技术是否有效并达到了声明的性能。

Method: 对11种最前沿的DPML方法进行可重复性和再现性实验，并探讨其特殊挑战。

Result: 一些技术在经过可重复性测试后表现稳定，而另一些技术在原始实验外条件下表现不佳。

Conclusion: 文章总结了获取科学有效且可靠结果的见解和最佳实践。

Abstract: There is a flurry of recent research papers proposing novel differentially
private machine learning (DPML) techniques. These papers claim to achieve new
state-of-the-art (SoTA) results and offer empirical results as validation.
However, there is no consensus on which techniques are most effective or if
they genuinely meet their stated claims. Complicating matters, heterogeneity in
codebases, datasets, methodologies, and model architectures make direct
comparisons of different approaches challenging.
  In this paper, we conduct a reproducibility and replicability (R+R)
experiment on 11 different SoTA DPML techniques from the recent research
literature. Results of our investigation are varied: while some methods stand
up to scrutiny, others falter when tested outside their initial experimental
conditions. We also discuss challenges unique to the reproducibility of DPML,
including additional randomness due to DP noise, and how to address them.
Finally, we derive insights and best practices to obtain scientifically valid
and reliable results.

</details>


### [184] [A Robust BERT-Based Deep Learning Model for Automated Cancer Type Extraction from Unstructured Pathology Reports](https://arxiv.org/abs/2508.15149)
*Minh Tran,Jeffery C. Chan,Min Li Huang,Maya Kansara,John P. Grady,Christine E. Napier,Subotheni Thavaneswaran,Mandy L. Ballinger,David M. Thomas,Frank P. Lin*

Main category: cs.LG

TL;DR: 开发了一种基于RoBERTa模型的自动癌症类型提取系统，显著优于基线模型和Mistral 7B模型，F1_Bertscore达0.98，完全匹配率为80.61%。


<details>
  <summary>Details</summary>
Motivation: 当前从电子病历中提取临床信息需要大量人力且需要专业知识，自动化的高效解决方案需求迫切。

Method: 使用微调后的RoBERTa模型，从病理报告中自动提取癌症类型，并与基线模型及Mistral 7B模型进行比较。

Result: 微调后的RoBERTa模型实现了F1_Bertscore 0.98和80.61%的完全匹配率，表现优于其他对比模型。

Conclusion: 为精准肿瘤学研究开发的特定任务微调模型具有高效、准确的信息提取潜力，有助于更高效地支持分子肿瘤板的决策流程。

Abstract: The accurate extraction of clinical information from electronic medical
records is particularly critical to clinical research but require much trained
expertise and manual labor. In this study we developed a robust system for
automated extraction of the specific cancer types for the purpose of supporting
precision oncology research. from pathology reports using a fine-tuned RoBERTa
model. This model significantly outperformed the baseline model and a Large
Language Model, Mistral 7B, achieving F1_Bertscore 0.98 and overall exact match
of 80.61%. This fine-tuning approach demonstrates the potential for scalability
that can integrate seamlessly into the molecular tumour board process.
Fine-tuning domain-specific models for precision tasks in oncology, may pave
the way for more efficient and accurate clinical information extraction.

</details>


### [185] [SafeLLM: Unlearning Harmful Outputs from Large Language Models against Jailbreak Attacks](https://arxiv.org/abs/2508.15182)
*Xiangman Li,Xiaodong Wu,Qi Li,Jianbing Ni,Rongxing Lu*

Main category: cs.LG

TL;DR: 本文提出了SafeLLM框架，针对大型语言模型（LLM）的越狱攻击，通过移除有害知识以提高模型安全性，同时保持语言流畅性和通用能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM容易受到越狱攻击，生成有害或受限内容，研究的动机是提高其安全性。

Method: 提出SafeLLM框架，包括三个阶段：动态不安全内容检测、基于FFN激活的有害内容定位、以及约束优化以降低不安全行为。

Result: 在多种越狱测试中，SafeLLM显著降低攻击成功率，并保持通用语言模型性能。

Conclusion: SafeLLM证明了通过忘却有害知识提高LLM安全性是一种有效的方向，优于传统的防御方法。

Abstract: Jailbreak attacks pose a serious threat to the safety of Large Language
Models (LLMs) by crafting adversarial prompts that bypass alignment mechanisms,
causing the models to produce harmful, restricted, or biased content. In this
paper, we propose SafeLLM, a novel unlearning-based defense framework that
unlearn the harmful knowledge from LLMs while preserving linguistic fluency and
general capabilities. SafeLLM employs a three-stage pipeline: (1) dynamic
unsafe output detection using a hybrid approach that integrates external
classifiers with model-internal evaluations; (2) token-level harmful content
tracing through feedforward network (FFN) activations to localize harmful
knowledge; and (3) constrained optimization to suppress unsafe behavior without
degrading overall model quality. SafeLLM achieves targeted and irreversible
forgetting by identifying and neutralizing FFN substructures responsible for
harmful generation pathways. Extensive experiments on prominent LLMs (Vicuna,
LLaMA, and GPT-J) across multiple jailbreak benchmarks show that SafeLLM
substantially reduces attack success rates while maintaining high
general-purpose performance. Compared to standard defense methods such as
supervised fine-tuning and direct preference optimization, SafeLLM offers
stronger safety guarantees, more precise control over harmful behavior, and
greater robustness to unseen attacks. Moreover, SafeLLM maintains the general
performance after the harmful knowledge unlearned. These results highlight
unlearning as a promising direction for scalable and effective LLM safety.

</details>


### [186] [Revisiting Pre-processing Group Fairness: A Modular Benchmarking Framework](https://arxiv.org/abs/2508.15193)
*Brodie Oldfield,Ziqi Xu,Sevvandi Kandanaarachchi*

Main category: cs.LG

TL;DR: 提出了FairPrep，一个针对公平预处理方法的评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有的公平性研究聚焦于in-processing和post-processing方法，而对pre-processing方法关注较少，且缺乏标准化的评估工具。

Method: 开发了FairPrep框架，基于AIF360平台，提供批处理界面，整合数据集、公平性干预和预测模型，同时能自动报告公平性和实用性指标。

Result: FairPrep通过标准化流水线和可重复评估填补了公平性基准测试的空白。

Conclusion: FairPrep为实现数据层面的公平性研究提供了实用基础。

Abstract: As machine learning systems become increasingly integrated into high-stakes
decision-making processes, ensuring fairness in algorithmic outcomes has become
a critical concern. Methods to mitigate bias typically fall into three
categories: pre-processing, in-processing, and post-processing. While
significant attention has been devoted to the latter two, pre-processing
methods, which operate at the data level and offer advantages such as
model-agnosticism and improved privacy compliance, have received comparatively
less focus and lack standardised evaluation tools. In this work, we introduce
FairPrep, an extensible and modular benchmarking framework designed to evaluate
fairness-aware pre-processing techniques on tabular datasets. Built on the
AIF360 platform, FairPrep allows seamless integration of datasets, fairness
interventions, and predictive models. It features a batch-processing interface
that enables efficient experimentation and automatic reporting of fairness and
utility metrics. By offering standardised pipelines and supporting reproducible
evaluations, FairPrep fills a critical gap in the fairness benchmarking
landscape and provides a practical foundation for advancing data-level fairness
research.

</details>


### [187] [Frequency-adaptive tensor neural networks for high-dimensional multi-scale problems](https://arxiv.org/abs/2508.15198)
*Jizu Huang,Rukang You,Tao Zhou*

Main category: cs.LG

TL;DR: 论文提出改进的频率自适应张量神经网络（TNNs），增强了解决高维多尺度问题的能力。


<details>
  <summary>Details</summary>
Motivation: 传统TNNs由于受频率原则的影响，难以准确捕捉高频特征，限制了其性能，因此作者希望通过改进增强其解析能力。

Method: 通过傅里叶分析研究TNN的训练动态，并结合随机傅里叶特征和离散傅里叶变换，提出频率自适应算法来缓解维度灾难。

Result: 频率自适应TNN算法通过大量数值实验验证了其在高维多尺度问题中的有效性和鲁棒性。

Conclusion: 频率自适应的TNN显著增强了解决复杂多尺度问题的能力，同时缓解了传统方法的不足。

Abstract: Tensor neural networks (TNNs) have demonstrated their superiority in solving
high-dimensional problems. However, similar to conventional neural networks,
TNNs are also influenced by the Frequency Principle, which limits their ability
to accurately capture high-frequency features of the solution. In this work, we
analyze the training dynamics of TNNs by Fourier analysis and enhance their
expressivity for high-dimensional multi-scale problems by incorporating random
Fourier features. Leveraging the inherent tensor structure of TNNs, we further
propose a novel approach to extract frequency features of high-dimensional
functions by performing the Discrete Fourier Transform to one-dimensional
component functions. This strategy effectively mitigates the curse of
dimensionality. Building on this idea, we propose a frequency-adaptive TNNs
algorithm, which significantly improves the ability of TNNs in solving complex
multi-scale problems. Extensive numerical experiments are performed to validate
the effectiveness and robustness of the proposed frequency-adaptive TNNs
algorithm.

</details>


### [188] [SleepDIFFormer: Sleep Stage Classification via Multivariate Differential Transformer](https://arxiv.org/abs/2508.15215)
*Benjamin Wei Hao Chin,Yuin Torng Yew,Haocheng Wu,Lanxin Liang,Chow Khuen Chan,Norita Mohd Zain,Siti Balqis Samdin,Sim Kuan Goh*

Main category: cs.LG

TL;DR: 本文提出了SleepDIFFormer算法，通过多变量差分Transformer架构合理处理EEG和EOG信号，从而实现了睡眠阶段自动分类的最优性能，在五个数据集中表现优异。


<details>
  <summary>Details</summary>
Motivation: 手动检查EEG信号进行睡眠阶段分类费时且易出错，而现有学习算法对非平稳与变化性信号处理效果有限，对未知数据集的泛化能力不足。

Method: 提出了基于多变量差分Transformer架构（MDTA）的SleepDIFFormer，用于EEG与EOG信号联合表示学习，采用跨域对齐机制以减弱时空注意噪声并实现领域无关的表示学习。

Result: 在五个数据集上的实验验证了该方法的高效性，与现有方法相比性能优越，并通过消融实验分析了模型特性和注意力权重的解读能力。

Conclusion: 本文的成果推进了自动睡眠阶段分类技术的发展，有助于改善睡眠质量相关研究，其代码已公开以供进一步研究。

Abstract: Classification of sleep stages is essential for assessing sleep quality and
diagnosing sleep disorders such as insomnia. However, manual inspection of EEG
characteristics for each stage is time-consuming and prone to human error.
Although machine learning and deep learning methods have been actively
developed, they continue to face challenges from the non-stationarity and
variability of electroencephalography (EEG) and electrooculography (EOG)
signals, often leading to poor generalization on unseen datasets. This research
proposed a Sleep Stage Classification method by developing Multivariate
Differential Transformer (SleepDIFFormer) for joint EEG and EOG representation
learning. Specifically, SleepDIFFormer was developed to process EEG and EOG
signals using our Multivariate Differential Transformer Architecture (MDTA) for
time series, trained with cross-domain alignment. Our method mitigated spatial
and temporal attention noise while learning a domain-invariant joint EEG-EOG
representation through feature distribution alignment, thereby enabling
generalization to unseen target datasets. Empirically, we evaluated our method
on five different sleep staging datasets and compared it with existing
approaches, achieving state-of-the-art performance. We also conducted thorough
ablation analyses of SleepDIFFormer and interpreted the differential attention
weights, highlighting their relevance to characteristic sleep EEG patterns.
These findings have implications for advancing automated sleep stage
classification and its application to sleep quality assessment. Our source code
is publicly available at https://github.com/Ben1001409/SleepDIFFormer

</details>


### [189] [See Beyond a Single View: Multi-Attribution Learning Leads to Better Conversion Rate Prediction](https://arxiv.org/abs/2508.15217)
*Sishuo Chen,Zhangming Chan,Xiang-Rong Sheng,Lei Zhang,Sheng Chen,Chenghuan Hou,Han Zhu,Jian Xu,Bo Zheng*

Main category: cs.LG

TL;DR: 提出了一种名为Multi-Attribution Learning (MAL)的新框架，通过整合多种归因机制的信号提升转换率预测。


<details>
  <summary>Details</summary>
Motivation: 现有转换率预测受限于单一归因机制信号，忽略了其他多种机制的潜在补充信号。

Method: 提出了MAL框架，包括两个核心组件：Attribution Knowledge Aggregator (AKA)和Primary Target Predictor (PTP)，结合多任务学习和新的训练策略CAT，实现信号整合与优化预测。

Result: 在离线实验中取得+0.51%的GAUC提升，在在线实验中实现+2.6%的ROI增长。

Conclusion: MAL框架证明了整合多种归因视角信号在转换率预测中具有显著优势，并具备实际工业部署潜力。

Abstract: Conversion rate (CVR) prediction is a core component of online advertising
systems, where the attribution mechanisms-rules for allocating conversion
credit across user touchpoints-fundamentally determine label generation and
model optimization. While many industrial platforms support diverse attribution
mechanisms (e.g., First-Click, Last-Click, Linear, and Data-Driven Multi-Touch
Attribution), conventional approaches restrict model training to labels from a
single production-critical attribution mechanism, discarding complementary
signals in alternative attribution perspectives.
  To address this limitation, we propose a novel Multi-Attribution Learning
(MAL) framework for CVR prediction that integrates signals from multiple
attribution perspectives to better capture the underlying patterns driving user
conversions. Specifically, MAL is a joint learning framework consisting of two
core components: the Attribution Knowledge Aggregator (AKA) and the Primary
Target Predictor (PTP). AKA is implemented as a multi-task learner that
integrates knowledge extracted from diverse attribution labels. PTP, in
contrast, focuses on the task of generating well-calibrated conversion
probabilities that align with the system-optimized attribution metric (e.g.,
CVR under the Last-Click attribution), ensuring direct compatibility with
industrial deployment requirements. Additionally, we propose CAT, a novel
training strategy that leverages the Cartesian product of all attribution label
combinations to generate enriched supervision signals. This design
substantially enhances the performance of the attribution knowledge aggregator.
Empirical evaluations demonstrate the superiority of MAL over
single-attribution learning baselines, achieving +0.51% GAUC improvement on
offline metrics. Online experiments demonstrate that MAL achieved a +2.6%
increase in ROI (Return on Investment).

</details>


### [190] [Locally Pareto-Optimal Interpretations for Black-Box Machine Learning Models](https://arxiv.org/abs/2508.15220)
*Aniruddha Joshi,Supratik Chakraborty,S Akshay,Shetal Shah,Hazem Torfah,Sanjit Seshia*

Main category: cs.LG

TL;DR: 提出了一种新框架，在模型解释的准确性与可解释性之间进行权衡，通过局部最优性保证实现可扩展的解释合成。


<details>
  <summary>Details</summary>
Motivation: 旨在解决传统解释合成方法在缺乏帕累托最优性保证或扩展能力不足方面的问题，以及在准确性和可解释性之间的权衡。

Method: 结合多目标学习或搜索技术（如Multi-Objective Monte Carlo Tree Search），生成帕累托最优候选集，并通过SAT求解器验证各候选的局部最优性。

Result: 实验表明，该方法在基准测试上比以往探索帕累托最优前沿的方法表现更好，并且合成的解释与具有全局保证的方法生成的解释相匹配。

Conclusion: 该框架在保证局部最优性的同时提高了解释合成的可扩展性，能够在准确性和可解释性之间实现平衡，是生成可信解释的有效方法。

Abstract: Creating meaningful interpretations for black-box machine learning models
involves balancing two often conflicting objectives: accuracy and
explainability. Exploring the trade-off between these objectives is essential
for developing trustworthy interpretations. While many techniques for
multi-objective interpretation synthesis have been developed, they typically
lack formal guarantees on the Pareto-optimality of the results. Methods that do
provide such guarantees, on the other hand, often face severe scalability
limitations when exploring the Pareto-optimal space. To address this, we
develop a framework based on local optimality guarantees that enables more
scalable synthesis of interpretations. Specifically, we consider the problem of
synthesizing a set of Pareto-optimal interpretations with local optimality
guarantees, within the immediate neighborhood of each solution. Our approach
begins with a multi-objective learning or search technique, such as
Multi-Objective Monte Carlo Tree Search, to generate a best-effort set of
Pareto-optimal candidates with respect to accuracy and explainability. We then
verify local optimality for each candidate as a Boolean satisfiability problem,
which we solve using a SAT solver. We demonstrate the efficacy of our approach
on a set of benchmarks, comparing it against previous methods for exploring the
Pareto-optimal front of interpretations. In particular, we show that our
approach yields interpretations that closely match those synthesized by methods
offering global guarantees.

</details>


### [191] [Learning ECG Representations via Poly-Window Contrastive Learning](https://arxiv.org/abs/2508.15225)
*Yi Yuan,Joseph Van Duyn,Runze Yan,Zhuoyi Huang,Sulaiman Vesal,Sergey Plis,Xiao Hu,Gloria Hyunjung Kwak,Ran Xiao,Alex Fedorov*

Main category: cs.LG

TL;DR: 本文提出了一种基于多窗口对比学习框架的自监督方法，用于深度学习模型的ECG分析。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督对比学习方法大多仅通过生成成对的增强视图，未能利用ECG信号的丰富时间结构，限制了模型的性能。

Method: 方法通过从每个ECG实例中提取多个时间窗口以构建正样本对，并通过慢特征分析原则，鼓励模型学习具有时间不变性和生理意义的特征。

Result: 新方法在PTB-XL数据集中的AUROC和F1指标分别取得0.891和0.680的优秀表现，并且在训练的总时代数和时间上显著减少，训练效率提高。

Conclusion: 该研究表明，多窗口对比学习是一种高效且可扩展的心电图分析方法，并展示了其作为生物医学时间序列数据自监督表征学习通用框架的潜力。

Abstract: Electrocardiogram (ECG) analysis is foundational for cardiovascular disease
diagnosis, yet the performance of deep learning models is often constrained by
limited access to annotated data. Self-supervised contrastive learning has
emerged as a powerful approach for learning robust ECG representations from
unlabeled signals. However, most existing methods generate only pairwise
augmented views and fail to leverage the rich temporal structure of ECG
recordings. In this work, we present a poly-window contrastive learning
framework. We extract multiple temporal windows from each ECG instance to
construct positive pairs and maximize their agreement via statistics. Inspired
by the principle of slow feature analysis, our approach explicitly encourages
the model to learn temporally invariant and physiologically meaningful features
that persist across time. We validate our approach through extensive
experiments and ablation studies on the PTB-XL dataset. Our results demonstrate
that poly-window contrastive learning consistently outperforms conventional
two-view methods in multi-label superclass classification, achieving higher
AUROC (0.891 vs. 0.888) and F1 scores (0.680 vs. 0.679) while requiring up to
four times fewer pre-training epochs (32 vs. 128) and 14.8% in total wall clock
pre-training time reduction. Despite processing multiple windows per sample, we
achieve a significant reduction in the number of training epochs and total
computation time, making our method practical for training foundational models.
Through extensive ablations, we identify optimal design choices and demonstrate
robustness across various hyperparameters. These findings establish poly-window
contrastive learning as a highly efficient and scalable paradigm for automated
ECG analysis and provide a promising general framework for self-supervised
representation learning in biomedical time-series data.

</details>


### [192] [Deep Think with Confidence](https://arxiv.org/abs/2508.15260)
*Yichao Fu,Xuewei Wang,Yuandong Tian,Jiawei Zhao*

Main category: cs.LG

TL;DR: 提出了Deep Think with Confidence (DeepConf)方法，通过动态过滤低质量推理路径，提高了推理效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的复杂方法在推理任务中效率低且计算开销大，需要进一步优化。

Method: 利用模型内部的置信信号，在生成期间或之后动态过滤低质量推理路径，无需重新训练模型或调整超参数。

Result: DeepConf方法在多种推理任务中提升了性能，尤其在AIME 2025等高难度基准测试中达到99.9%的准确率，同时减少令牌生成高达84.7%。

Conclusion: DeepConf方法是一种简单有效的改进推理效率与性能的工具，能无缝集成于现有框架。

Abstract: Large Language Models (LLMs) have shown great potential in reasoning tasks
through test-time scaling methods like self-consistency with majority voting.
However, this approach often leads to diminishing returns in accuracy and high
computational overhead. To address these challenges, we introduce Deep Think
with Confidence (DeepConf), a simple yet powerful method that enhances both
reasoning efficiency and performance at test time. DeepConf leverages
model-internal confidence signals to dynamically filter out low-quality
reasoning traces during or after generation. It requires no additional model
training or hyperparameter tuning and can be seamlessly integrated into
existing serving frameworks. We evaluate DeepConf across a variety of reasoning
tasks and the latest open-source models, including Qwen 3 and GPT-OSS series.
Notably, on challenging benchmarks such as AIME 2025, DeepConf@512 achieves up
to 99.9% accuracy and reduces generated tokens by up to 84.7% compared to full
parallel thinking.

</details>


### [193] [Evaluating Knowledge Graph Complexity via Semantic, Spectral, and Structural Metrics for Link Prediction](https://arxiv.org/abs/2508.15291)
*Haji Gul,Abul Ghani Naim,Ajaz Ahmad Bhat*

Main category: cs.LG

TL;DR: 研究探讨了CSG指标在多关系链路预测中的适用性，提出CSG不够稳健，并引入新的KG复杂性指标。


<details>
  <summary>Details</summary>
Motivation: 探讨CSG在评估知识图谱的多关系链路预测模型中的有效性和可靠性。

Method: 批判性分析CSG的表现并通过引入新的结构和语义KG复杂性指标进行对比和评估。

Result: 研究发现CSG敏感于参数化，难以与性能指标保持一致，提出Relation Entropy等新指标能更好地反映任务难度。

Conclusion: CSG在链路预测中的稳定性和泛化预测能力未能成立，建议更稳定、可解释且与任务对齐的复杂性测度。

Abstract: Understanding dataset complexity is fundamental to evaluating and comparing
link prediction models on knowledge graphs (KGs). While the Cumulative Spectral
Gradient (CSG) metric, derived from probabilistic divergence between classes
within a spectral clustering framework, has been proposed as a classifier
agnostic complexity metric purportedly scaling with class cardinality and
correlating with downstream performance, it has not been evaluated in KG
settings so far. In this work, we critically examine CSG in the context of
multi relational link prediction, incorporating semantic representations via
transformer derived embeddings. Contrary to prior claims, we find that CSG is
highly sensitive to parametrisation and does not robustly scale with the number
of classes. Moreover, it exhibits weak or inconsistent correlation with
standard performance metrics such as Mean Reciprocal Rank (MRR) and Hit@1. To
deepen the analysis, we introduce and benchmark a set of structural and
semantic KG complexity metrics. Our findings reveal that global and local
relational ambiguity captured via Relation Entropy, node level Maximum Relation
Diversity, and Relation Type Cardinality exhibit strong inverse correlations
with MRR and Hit@1, suggesting these as more faithful indicators of task
difficulty. Conversely, graph connectivity measures such as Average Degree,
Degree Entropy, PageRank, and Eigenvector Centrality correlate positively with
Hit@10. Our results demonstrate that CSGs purported stability and
generalization predictive power fail to hold in link prediction settings and
underscore the need for more stable, interpretable, and task-aligned measures
of dataset complexity in knowledge driven learning.

</details>


### [194] [Saving for the future: Enhancing generalization via partial logic regularization](https://arxiv.org/abs/2508.15317)
*Zhaorui Tan,Yijie Hu,Xi Yang,Qiufeng Wang,Anh Nguyen,Kaizhu Huang*

Main category: cs.LG

TL;DR: 本文提出了一个新的部分逻辑正则化方法（PL-Reg），用于提升视觉分类任务中未知类别的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 视觉分类任务在处理未知类别时仍然是一个难题，现有方法如类别发现存在偏向已知类别问题，增量学习则面临灾难性遗忘的挑战。逻辑正则化方法虽增强了泛化，但缺乏对未知类的灵活性。

Method: 提出了一种称为PL-Reg的部分逻辑正则化方法，通过允许模型为未定义逻辑公式保留空间，提高了对未知类别的适应性。同时，形式化证明了部分逻辑在处理未知类别任务中的有效性。

Result: 通过在一般化类别发现、多领域一般化类别发现和长尾类增量学习任务上的大量实验，验证了PL-Reg的一致性能提升。

Conclusion: 部分逻辑正则化方法在解决与未知类别相关的挑战中是有效的，显著提升了视觉分类任务的泛化能力与鲁棒性。

Abstract: Generalization remains a significant challenge in visual classification
tasks, particularly in handling unknown classes in real-world applications.
Existing research focuses on the class discovery paradigm, which tends to favor
known classes, and the incremental learning paradigm, which suffers from
catastrophic forgetting. Recent approaches such as the L-Reg technique employ
logic-based regularization to enhance generalization but are bound by the
necessity of fully defined logical formulas, limiting flexibility for unknown
classes. This paper introduces PL-Reg, a novel partial-logic regularization
term that allows models to reserve space for undefined logic formulas,
improving adaptability to unknown classes. Specifically, we formally
demonstrate that tasks involving unknown classes can be effectively explained
using partial logic. We also prove that methods based on partial logic lead to
improved generalization. We validate PL-Reg through extensive experiments on
Generalized Category Discovery, Multi-Domain Generalized Category Discovery,
and long-tailed Class Incremental Learning tasks, demonstrating consistent
performance improvements. Our results highlight the effectiveness of partial
logic in tackling challenges related to unknown classes.

</details>


### [195] [ExBigBang: A Dynamic Approach for Explainable Persona Classification through Contextualized Hybrid Transformer Analysis](https://arxiv.org/abs/2508.15364)
*Saleh Afzoon,Amin Beheshti,Nabi Rezvani,Farshad Khunjush,Usman Naseem,John McMahon,Zahra Fathollahi,Mahdieh Labani,Wathiq Mansoor,Xuyun Zhang*

Main category: cs.LG

TL;DR: 提出ExBigBang，一个结合文本和表格数据的混合模型，用于解释性用户角色分类。


<details>
  <summary>Details</summary>
Motivation: 应对用户角色设计中对复杂交互场景的需求，解决现有模型在信息捕获及解释性上的局限性。

Method: 开发ExBigBang模型，结合transformer架构，集成元数据、领域知识和用户画像，动态反映用户行为变化，并利用解释性AI技术解析模型预测。

Result: 实验展示模型在基准数据集上的稳健性，消融研究验证结合文本和表格数据的益处。

Conclusion: ExBigBang通过增强上下文信息建模和提高预测解释性，为用户角色分类提供了更有效的解决方案。

Abstract: In user-centric design, persona development plays a vital role in
understanding user behaviour, capturing needs, segmenting audiences, and
guiding design decisions. However, the growing complexity of user interactions
calls for a more contextualized approach to ensure designs align with real user
needs. While earlier studies have advanced persona classification by modelling
user behaviour, capturing contextual information, especially by integrating
textual and tabular data, remains a key challenge. These models also often lack
explainability, leaving their predictions difficult to interpret or justify. To
address these limitations, we present ExBigBang (Explainable BigBang), a hybrid
text-tabular approach that uses transformer-based architectures to model rich
contextual features for persona classification. ExBigBang incorporates
metadata, domain knowledge, and user profiling to embed deeper context into
predictions. Through a cyclical process of user profiling and classification,
our approach dynamically updates to reflect evolving user behaviours.
Experiments on a benchmark persona classification dataset demonstrate the
robustness of our model. An ablation study confirms the benefits of combining
text and tabular data, while Explainable AI techniques shed light on the
rationale behind the model's predictions.

</details>


### [196] [Enhancing Forecasting with a 2D Time Series Approach for Cohort-Based Data](https://arxiv.org/abs/2508.15369)
*Yonathan Guttel,Orit Moradov,Nachi Lieder,Asnat Greenstein-Messica*

Main category: cs.LG

TL;DR: 本文提出了一种新的二维时间序列预测模型，在有限数据环境中表现优异，具有高度准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 解决在小数据环境中进行精确时间序列预测的难题，并帮助应对金融和市场营销领域的决策挑战。

Method: 引入并整合了时间维度和群组行为的二维时间序列模型，使用多个真实数据集进行验证。

Result: 该模型在准确性和适应性上均优于参考模型。

Conclusion: 该方法为面临金融和营销预测挑战的行业提供了新的战略决策支持工具。

Abstract: This paper introduces a novel two-dimensional (2D) time series forecasting
model that integrates cohort behavior over time, addressing challenges in small
data environments. We demonstrate its efficacy using multiple real-world
datasets, showcasing superior performance in accuracy and adaptability compared
to reference models. The approach offers valuable insights for strategic
decision-making across industries facing financial and marketing forecasting
challenges.

</details>


### [197] [Fairness for the People, by the People: Minority Collective Action](https://arxiv.org/abs/2508.15374)
*Omri Ben-Dov,Samira Samadi,Amartya Sanyal,Alexandru Ţifrea*

Main category: cs.LG

TL;DR: 本文分析了通过用户协调重新标注数据提高机器学习公平性的方法，提出了三种无模型限制的实际方法并在数据集中验证了效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机是现有组织方的偏差减轻方法常需牺牲效用且需组织配合，而许多模型依赖用户贡献数据，用户可以通过集体重新标注提升公平性。

Method: 提出三种无模型限制的方法来模拟理想重新标注，并在真实数据集上进行验证。

Result: 发现少数群体的一个子组可以在仅对整体预测误差产生小影响的情况下大幅减少不公平现象。

Conclusion: 通过用户协调重新标注数据是一种潜在高效减少机器学习偏差的不干涉方式，同时降低了对组织参与的要求。

Abstract: Machine learning models often preserve biases present in training data,
leading to unfair treatment of certain minority groups. Despite an array of
existing firm-side bias mitigation techniques, they typically incur utility
costs and require organizational buy-in. Recognizing that many models rely on
user-contributed data, end-users can induce fairness through the framework of
Algorithmic Collective Action, where a coordinated minority group strategically
relabels its own data to enhance fairness, without altering the firm's training
process. We propose three practical, model-agnostic methods to approximate
ideal relabeling and validate them on real-world datasets. Our findings show
that a subgroup of the minority can substantially reduce unfairness with a
small impact on the overall prediction error.

</details>


### [198] [EvoFormer: Learning Dynamic Graph-Level Representations with Structural and Temporal Bias Correction](https://arxiv.org/abs/2508.15378)
*Haodi Zhong,Liuxin Zou,Di Wang,Bo Wang,Zhenxing Niu,Quan Wang*

Main category: cs.LG

TL;DR: 本文探讨动态图级表示学习中的关键问题，并提出EvoFormer框架以解决结构访问偏差和突发变化盲点等问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效处理随机游走中对高度节点的偏重和对突发结构变化的感知不足，因而需要改进的框架以增强时空表示能力。

Method: 提出EvoFormer框架，包含结构感知Transformer模块和演化敏感时间模块，通过节点结构角色的位置编码和三步时间建模策略提高表征能力。

Result: 在五个基准数据集上验证了EvoFormer在图相似性排序、时间异常检测和时间分割任务中的最优性能。

Conclusion: EvoFormer有效纠正在动态图嵌入学习中的结构和时间偏差问题，为动态网络建模提供了更准确的工具。

Abstract: Dynamic graph-level embedding aims to capture structural evolution in
networks, which is essential for modeling real-world scenarios. However,
existing methods face two critical yet under-explored issues: Structural Visit
Bias, where random walk sampling disproportionately emphasizes high-degree
nodes, leading to redundant and noisy structural representations; and Abrupt
Evolution Blindness, the failure to effectively detect sudden structural
changes due to rigid or overly simplistic temporal modeling strategies,
resulting in inconsistent temporal embeddings. To overcome these challenges, we
propose EvoFormer, an evolution-aware Transformer framework tailored for
dynamic graph-level representation learning. To mitigate Structural Visit Bias,
EvoFormer introduces a Structure-Aware Transformer Module that incorporates
positional encoding based on node structural roles, allowing the model to
globally differentiate and accurately represent node structures. To overcome
Abrupt Evolution Blindness, EvoFormer employs an Evolution-Sensitive Temporal
Module, which explicitly models temporal evolution through a sequential
three-step strategy: (I) Random Walk Timestamp Classification, generating
initial timestamp-aware graph-level embeddings; (II) Graph-Level Temporal
Segmentation, partitioning the graph stream into segments reflecting
structurally coherent periods; and (III) Segment-Aware Temporal Self-Attention
combined with an Edge Evolution Prediction task, enabling the model to
precisely capture segment boundaries and perceive structural evolution trends,
effectively adapting to rapid temporal shifts. Extensive evaluations on five
benchmark datasets confirm that EvoFormer achieves state-of-the-art performance
in graph similarity ranking, temporal anomaly detection, and temporal
segmentation tasks, validating its effectiveness in correcting structural and
temporal biases.

</details>


### [199] [CITE: A Comprehensive Benchmark for Heterogeneous Text-Attributed Graphs on Catalytic Materials](https://arxiv.org/abs/2508.15392)
*Chenghao Zhang,Qingqing Long,Ludi Wang,Wenjuan Cui,Jianjun Yu,Yi Du*

Main category: cs.LG

TL;DR: 本文研究了异构文本属性图(TAGs)，引入了CITE数据集，这是第一个也是规模最大的异构文本引注图数据集，用于催化材料研究。


<details>
  <summary>Details</summary>
Motivation: 现存的异构文本属性图缺乏大规模的基准数据集，这限制了表征学习方法的开发和公平对比。

Method: 提出了CITE数据集，包含438K节点和1.2M边，涵盖4种关系类型，提供标准评价协议，并比较了4类学习范式模型。

Result: 通过节点分类任务的基准测试和消融实验，评估了不同模型的性能并验证了CITE在异构和文本属性上的有效性。

Conclusion: CITE填补了大规模异构TAGs基准数据的空白，为建模对比和评估提供了重要资源。

Abstract: Text-attributed graphs(TAGs) are pervasive in real-world systems,where each
node carries its own textual features. In many cases these graphs are
inherently heterogeneous, containing multiple node types and diverse edge
types. Despite the ubiquity of such heterogeneous TAGs, there remains a lack of
large-scale benchmark datasets. This shortage has become a critical bottleneck,
hindering the development and fair comparison of representation learning
methods on heterogeneous text-attributed graphs. In this paper, we introduce
CITE - Catalytic Information Textual Entities Graph, the first and largest
heterogeneous text-attributed citation graph benchmark for catalytic materials.
CITE comprises over 438K nodes and 1.2M edges, spanning four relation types. In
addition, we establish standardized evaluation procedures and conduct extensive
benchmarking on the node classification task, as well as ablation experiments
on the heterogeneous and textual properties of CITE. We compare four classes of
learning paradigms, including homogeneous graph models, heterogeneous graph
models, LLM(Large Language Model)-centric models, and LLM+Graph models. In a
nutshell, we provide (i) an overview of the CITE dataset, (ii) standardized
evaluation protocols, and (iii) baseline and ablation experiments across
diverse modeling paradigms.

</details>


### [200] [Federated Learning based on Self-Evolving Gaussian Clustering](https://arxiv.org/abs/2508.15393)
*Miha Ožbot,Igor Škrjanc*

Main category: cs.LG

TL;DR: 本研究提出了一种结合联邦学习的演化模糊系统，用于动态适应新增数据分簇，其主要在不需要预先确定簇的数量的情况下运行。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习方法虽然巧妙地实现了模型训练本地化，但是在处理数据动态分簇问题上存在局限，因此需要一种能够在无需预定义簇数的情况下适应数据变化的方法。

Method: 通过在联邦学习框架中引入演化模糊系统，动态增加簇的数量，并利用PyTorch平台开发解决方案，进行分类和分簇任务实验。

Result: 方法在多个UCI数据集上的分类性能优于传统方法，但由于重叠条件计算较为复杂，计算成本较高。

Conclusion: 研究表明，该方法在分布式数据处理方面具有显著优势，尽管计算复杂度较高，但在分类与分簇任务上表现优异。

Abstract: In this study, we present an Evolving Fuzzy System within the context of
Federated Learning, which adapts dynamically with the addition of new clusters
and therefore does not require the number of clusters to be selected apriori.
Unlike traditional methods, Federated Learning allows models to be trained
locally on clients' devices, sharing only the model parameters with a central
server instead of the data. Our method, implemented using PyTorch, was tested
on clustering and classification tasks. The results show that our approach
outperforms established classification methods on several well-known UCI
datasets. While computationally intensive due to overlap condition
calculations, the proposed method demonstrates significant advantages in
decentralized data processing.

</details>


### [201] [Hybrid Least Squares/Gradient Descent Methods for DeepONets](https://arxiv.org/abs/2508.15394)
*Jun Choi,Chang-Ock Lee,Minam Moon*

Main category: cs.LG

TL;DR: 作者提出了一种高效的混合最小二乘/梯度下降方法，用于加速DeepONet的训练。


<details>
  <summary>Details</summary>
Motivation: DeepONet训练因涉及较大的线性问题而效率较低，亟需一种可行的解决方案。

Method: 将DeepONet的分支和主干网络对应的较大线性问题分解为两个小的子问题，分别通过最小二乘法和梯度下降法优化。

Result: 成功实现了对包含正则化项问题和物理约束的无监督学习的更高效处理。

Conclusion: 提出的方法显著加速DeepONet的训练，并适用于更广泛的L^2损失情况及无监督学习场景。

Abstract: We propose an efficient hybrid least squares/gradient descent method to
accelerate DeepONet training. Since the output of DeepONet can be viewed as
linear with respect to the last layer parameters of the branch network, these
parameters can be optimized using a least squares (LS) solve, and the remaining
hidden layer parameters are updated by means of gradient descent form. However,
building the LS system for all possible combinations of branch and trunk inputs
yields a prohibitively large linear problem that is infeasible to solve
directly. To address this issue, our method decomposes the large LS system into
two smaller, more manageable subproblems $\unicode{x2014}$ one for the branch
network and one for the trunk network $\unicode{x2014}$ and solves them
separately. This method is generalized to a broader type of $L^2$ loss with a
regularization term for the last layer parameters, including the case of
unsupervised learning with physics-informed loss.

</details>


### [202] [Bridging Generalization and Personalization in Wearable Human Activity Recognition via On-Device Few-Shot Learning](https://arxiv.org/abs/2508.15413)
*Pixi Kang,Julian Moosmann,Mengxi Liu,Bo Zhou,Michele Magno,Paul Lukowicz,Sizhen Bian*

Main category: cs.LG

TL;DR: 提出了一种能够快速适应新用户的混合框架，用于可穿戴设备上的人类活动识别（HAR）。


<details>
  <summary>Details</summary>
Motivation: 现有基于可穿戴设备的HAR模型在面对新用户时性能下降严重，亟需高效的个性化方法解决用户引发的概念漂移问题。

Method: 本论文提出一种先用户泛化、后采用少样本学习进行快速个性化的混合框架。方法仅更新分类层且能在微控制器上实现，有效降低运算及内存开销。

Result: 在多个HAR场景中验证了该框架的有效性，部署后的适应阶段使准确率分别提高了3.73%、17.38%和3.70%。

Conclusion: 此框架表明在嵌入式平台上实现轻量化、快速且有效的个性化是可行的，为大规模用户感知的HAR系统提供了路径。

Abstract: Human Activity Recognition (HAR) using wearable devices has advanced
significantly in recent years, yet its generalization remains limited when
models are deployed to new users. This degradation in performance is primarily
due to user-induced concept drift (UICD), highlighting the importance of
efficient personalization. In this paper, we present a hybrid framework that
first generalizes across users and then rapidly adapts to individual users
using few-shot learning directly on-device. By updating only the classifier
layer with user-specific data, our method achieves robust personalization with
minimal computational and memory overhead. We implement this framework on the
energy-efficient RISC-V-based GAP9 microcontroller and validate it across three
diverse HAR scenarios: RecGym, QVAR-Gesture, and Ultrasound-Gesture.
Post-deployment adaptation yields consistent accuracy improvements of 3.73\%,
17.38\%, and 3.70\% respectively. These results confirm that fast, lightweight,
and effective personalization is feasible on embedded platforms, paving the way
for scalable and user-aware HAR systems in the wild
\footnote{https://github.com/kangpx/onlineTiny2023}.

</details>


### [203] [Measures of Overlapping Multivariate Gaussian Clusters in Unsupervised Online Learning](https://arxiv.org/abs/2508.15444)
*Miha Ožbot,Igor Škrjanc*

Main category: cs.LG

TL;DR: 本文提出了一种新方法，用于检测多元高斯聚类中的重叠问题，特别适合于数据流在线学习中的场景。


<details>
  <summary>Details</summary>
Motivation: 现有的分布不相似性度量方法在处理数据流在线学习时无法有效检测重叠的聚类，且计算成本较高。需要开发快速并且能明确检测重叠的度量方法。

Method: 本文提出了一种新的不相似性度量方法，专门用于检测聚类重叠，具备更高计算效率，并能避免误合并正交聚类。

Result: 新方法的检测速度比现有方法快数倍，同时能够有效检测重叠聚类并避免正交聚类的合并。

Conclusion: 本文的方法在检测数据流在线学习中的重叠聚类方面表现优异，克服了现有方法在形状适应性和计算效率方面的不足。

Abstract: In this paper, we propose a new measure for detecting overlap in multivariate
Gaussian clusters. The aim of online learning from data streams is to create
clustering, classification, or regression models that can adapt over time based
on the conceptual drift of streaming data. In the case of clustering, this can
result in a large number of clusters that may overlap and should be merged.
Commonly used distribution dissimilarity measures are not adequate for
determining overlapping clusters in the context of online learning from
streaming data due to their inability to account for all shapes of clusters and
their high computational demands. Our proposed dissimilarity measure is
specifically designed to detect overlap rather than dissimilarity and can be
computed faster compared to existing measures. Our method is several times
faster than compared methods and is capable of detecting overlapping clusters
while avoiding the merging of orthogonal clusters.

</details>


### [204] [Reliable Unlearning Harmful Information in LLMs with Metamorphosis Representation Projection](https://arxiv.org/abs/2508.15449)
*Chengcan Wu,Zeming Wei,Huanran Chen,Yinpeng Dong,Meng Sun*

Main category: cs.LG

TL;DR: 本文提出了一种名为MRP的方法，通过在特定网络层的隐藏状态空间中实施投影变换，有效清除了不良信息，同时保留了有用知识，达到了最先进的遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在各领域的应用愈加广泛，其潜在安全性问题尤其是存储不安全信息的问题愈发严重。对模型进行安全的“遗忘”以提升模型安全性成为研究重点。

Method: 提出了一种新的方法，称为Metamorphosis Representation Projection (MRP)，通过在特定网络层的隐藏状态空间中进行不可逆的投影变换，消除有害信息，同时保留有用信息。

Result: 实验结果表明，MRP能有效实现持续遗忘，并成功防御重学习攻击，在遗忘效果和模型性能保留方面达到了最先进的表现。

Conclusion: 该研究解决了现有方法中隐含信息无法彻底清除的问题，为模型的安全性提供了新的解决方案。

Abstract: While Large Language Models (LLMs) have demonstrated impressive performance
in various domains and tasks, concerns about their safety are becoming
increasingly severe. In particular, since models may store unsafe knowledge
internally, machine unlearning has emerged as a representative paradigm to
ensure model safety. Existing approaches employ various training techniques,
such as gradient ascent and negative preference optimization, in attempts to
eliminate the influence of undesired data on target models. However, these
methods merely suppress the activation of undesired data through parametric
training without completely eradicating its informational traces within the
model. This fundamental limitation makes it difficult to achieve effective
continuous unlearning, rendering these methods vulnerable to relearning
attacks. To overcome these challenges, we propose a Metamorphosis
Representation Projection (MRP) approach that pioneers the application of
irreversible projection properties to machine unlearning. By implementing
projective transformations in the hidden state space of specific network
layers, our method effectively eliminates harmful information while preserving
useful knowledge. Experimental results demonstrate that our approach enables
effective continuous unlearning and successfully defends against relearning
attacks, achieving state-of-the-art performance in unlearning effectiveness
while preserving natural performance. Our code is available in
https://github.com/ChengcanWu/MRP.

</details>


### [205] [A Solvable Molecular Switch Model for Stable Temporal Information Processing](https://arxiv.org/abs/2508.15451)
*H. I. Nurdin,C. A. Nijhuis*

Main category: cs.LG

TL;DR: 研究一个具有线性状态和非线性输入的微分方程模型，该模型展现了脑突触样切换行为，并具有稳定处理时间变化输入的数学特性。


<details>
  <summary>Details</summary>
Motivation: 探索动态分子开关模型的潜力，结合生物灵感行为与稳定学习特性，用于序列数据处理。

Method: 分析该模型的数学属性，包括收敛性与记忆衰减特性，验证其在非线性动态系统中对时间变化输入的稳定处理能力。

Result: 找到了一种既符合生物灵感行为又具有稳定计算能力的数学模型，这支持了其在深度学习和神经形态计算中的应用潜力。

Conclusion: 该模型可作为理论支持，用于开发能模仿脑行为并稳定处理输入信号的设备或架构，进一步推进神经形态计算领域的研究。

Abstract: This paper studies an input-driven one-state differential equation model
initially developed for an experimentally demonstrated dynamic molecular switch
that switches like synapses in the brain do. The linear-in-the-state and
nonlinear-in-the-input model is exactly solvable, and it is shown that it also
possesses mathematical properties of convergence and fading memory that enable
stable processing of time-varying inputs by nonlinear dynamical systems. Thus,
the model exhibits the co-existence of biologically-inspired behavior and
desirable mathematical properties for stable learning on sequential data. The
results give theoretical support for the use of the dynamic molecular switches
as computational units in deep cascaded/layered feedforward and recurrent
architectures as well as other more general structures for neuromorphic
computing. They could also inspire more general exactly solvable models that
can be fitted to emulate arbitrary physical devices which can mimic
brain-inspired behaviour and perform stable computation on input signals.

</details>


### [206] [Mini-Batch Robustness Verification of Deep Neural Networks](https://arxiv.org/abs/2508.15454)
*Saar Tzour-Shaday,Dana Drachsler Cohen*

Main category: cs.LG

TL;DR: 提出BaVerLy，通过动态生成和验证迷你批次，显著提升神经网络局部鲁棒性验证的效率。


<details>
  <summary>Details</summary>
Motivation: 深入研究神经网络的鲁棒性，尤其是在面对对抗攻击时，现有验证器无法兼顾效率和精度。

Method: 提出了一个称为BaVerLy的验证器，利用网络计算相似性动态生成和验证迷你批次，对一组输入的局部鲁棒性进行联合验证。

Result: BaVerLy在MNIST和CIFAR-10数据集上的性能提升显著，其验证效率比传统的逐一验证方法平均提升2.3倍，最高达到4.1倍，将总分析时间从24小时缩短到6小时。

Conclusion: BaVerLy大幅提升了局部鲁棒性验证的效率，为神经网络对抗攻击的鲁棒性研究提供了一条高效的途径。

Abstract: Neural network image classifiers are ubiquitous in many safety-critical
applications. However, they are susceptible to adversarial attacks. To
understand their robustness to attacks, many local robustness verifiers have
been proposed to analyze $\epsilon$-balls of inputs. Yet, existing verifiers
introduce a long analysis time or lose too much precision, making them less
effective for a large set of inputs. In this work, we propose a new approach to
local robustness: group local robustness verification. The key idea is to
leverage the similarity of the network computations of certain $\epsilon$-balls
to reduce the overall analysis time. We propose BaVerLy, a sound and complete
verifier that boosts the local robustness verification of a set of
$\epsilon$-balls by dynamically constructing and verifying mini-batches.
BaVerLy adaptively identifies successful mini-batch sizes, accordingly
constructs mini-batches of $\epsilon$-balls that have similar network
computations, and verifies them jointly. If a mini-batch is verified, all
$\epsilon$-balls are proven robust. Otherwise, one $\epsilon$-ball is suspected
as not being robust, guiding the refinement. In the latter case, BaVerLy
leverages the analysis results to expedite the analysis of that $\epsilon$-ball
as well as the other $\epsilon$-balls in the batch. We evaluate BaVerLy on
fully connected and convolutional networks for MNIST and CIFAR-10. Results show
that BaVerLy scales the common one by one verification by 2.3x on average and
up to 4.1x, in which case it reduces the total analysis time from 24 hours to 6
hours.

</details>


### [207] [Learning Protein-Ligand Binding in Hyperbolic Space](https://arxiv.org/abs/2508.15480)
*Jianhui Wang,Wenyu Zhu,Bowen Gao,Xin Hong,Ya-Qin Zhang,Wei-Ying Ma,Yanyan Lan*

Main category: cs.LG

TL;DR: 该论文提出了HypSeek，一个利用双曲空间表征学习的框架，用于蛋白质-配体结合预测，解决了欧几里得空间表征在捕捉分子交互层级结构及亲和力变化上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前的检索方法依赖欧几里得空间表征，但难以有效捕捉分子交互中的层级结构和精细的亲和力差异，尤其是在结构相似但亲和力差异大的场景（如活性悬崖）中。

Method: 设计了双曲几何表征框架HypSeek，将配体、蛋白质口袋及序列嵌入到双曲空间中，并引入了蛋白质指导的三塔式架构，统一适用于虚拟筛选与亲和力排名任务。

Result: 在虚拟筛选任务中早期富集表现提升了20.7%，在亲和力排名任务中相关性提升了25.4%。

Conclusion: 实验结果表明利用双曲几何可显著提升蛋白质-配体建模的效果，展示了其作为归纳偏置的潜力。

Abstract: Protein-ligand binding prediction is central to virtual screening and
affinity ranking, two fundamental tasks in drug discovery. While recent
retrieval-based methods embed ligands and protein pockets into Euclidean space
for similarity-based search, the geometry of Euclidean embeddings often fails
to capture the hierarchical structure and fine-grained affinity variations
intrinsic to molecular interactions. In this work, we propose HypSeek, a
hyperbolic representation learning framework that embeds ligands, protein
pockets, and sequences into Lorentz-model hyperbolic space. By leveraging the
exponential geometry and negative curvature of hyperbolic space, HypSeek
enables expressive, affinity-sensitive embeddings that can effectively model
both global activity and subtle functional differences-particularly in
challenging cases such as activity cliffs, where structurally similar ligands
exhibit large affinity gaps. Our mode unifies virtual screening and affinity
ranking in a single framework, introducing a protein-guided three-tower
architecture to enhance representational structure. HypSeek improves early
enrichment in virtual screening on DUD-E from 42.63 to 51.44 (+20.7%) and
affinity ranking correlation on JACS from 0.5774 to 0.7239 (+25.4%),
demonstrating the benefits of hyperbolic geometry across both tasks and
highlighting its potential as a powerful inductive bias for protein-ligand
modeling.

</details>


### [208] [Let's Grow an Unbiased Community: Guiding the Fairness of Graphs via New Links](https://arxiv.org/abs/2508.15499)
*Jiahua Lu,Huaxiao Liu,Shuotong Bai,Junjie Xu,Renqiang Luo,Enyan Dai*

Main category: cs.LG

TL;DR: 本文提出了一个名为FairGuide的框架，通过在图中引入新链接来减少图结构中的偏差，从而提升公平性。


<details>
  <summary>Details</summary>
Motivation: 由于原始图的结构中通常包含偏差，图神经网络在公平性方面面临挑战。引入新链接来引导图结构走向更公平是解决这一问题的一种可行方法。

Method: 提出了FairGuide框架，通过一个可微分的社区检测伪任务优化公平性。此外，使用基于公平导向目标的元梯度方法选择可以提升结构公平性的新链接。

Result: 实验表明，该方法在多种图相关的公平性任务中都表现出了有效性和适用性。

Conclusion: FairGuide通过结构优化显著提升了公平性，并可以在多种下游应用中推广其公平性增强的效果。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success across diverse
applications. However, due to the biases in the graph structures, graph neural
networks face significant challenges in fairness. Although the original user
graph structure is generally biased, it is promising to guide these existing
structures toward unbiased ones by introducing new links. The fairness guidance
via new links could foster unbiased communities, thereby enhancing fairness in
downstream applications. To address this issue, we propose a novel framework
named FairGuide. Specifically, to ensure fairness in downstream tasks trained
on fairness-guided graphs, we introduce a differentiable community detection
task as a pseudo downstream task. Our theoretical analysis further demonstrates
that optimizing fairness within this pseudo task effectively enhances
structural fairness, promoting fairness generalization across diverse
downstream applications. Moreover, FairGuide employs an effective strategy
which leverages meta-gradients derived from the fairness-guidance objective to
identify new links that significantly enhance structural fairness. Extensive
experimental results demonstrate the effectiveness and generalizability of our
proposed method across a variety of graph-based fairness tasks.

</details>


### [209] [Jointly Computation- and Communication-Efficient Distributed Learning](https://arxiv.org/abs/2508.15509)
*Xiaoxing Ren,Nicola Bastianello,Karl H. Johansson,Thomas Parisini*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于ADMM的分布式学习算法，具备计算和通信高效的特点。


<details>
  <summary>Details</summary>
Motivation: 目前分布式学习需要在计算和通信效率之间找到平衡。

Method: 算法采用随机梯度计算减少计算负担，并通过多轮本地训练与压缩传输减少通信开销。

Result: 在强凸条件下，证明算法具有精确线性收敛性，并通过分类任务的实验验证其优于现有技术。

Conclusion: 该算法在分布式学习中，实现了同时提升计算和通信效率的新方法，理论与实验均支持其优越性。

Abstract: We address distributed learning problems over undirected networks.
Specifically, we focus on designing a novel ADMM-based algorithm that is
jointly computation- and communication-efficient. Our design guarantees
computational efficiency by allowing agents to use stochastic gradients during
local training. Moreover, communication efficiency is achieved as follows: i)
the agents perform multiple training epochs between communication rounds, and
ii) compressed transmissions are used. We prove exact linear convergence of the
algorithm in the strongly convex setting. We corroborate our theoretical
results by numerical comparisons with state of the art techniques on a
classification task.

</details>


### [210] [Stabilization of Perturbed Loss Function: Differential Privacy without Gradient Noise](https://arxiv.org/abs/2508.15523)
*Salman Habib,Remi Chou,Taejoon Kim*

Main category: cs.LG

TL;DR: SPOF提供了一种新的差分隐私训练机制，通过多用户环境中私密化数据的多项式逼近实现高效稳定的训练。


<details>
  <summary>Details</summary>
Motivation: 现有的基于梯度的差分隐私方法（如DP-SGD）在多用户场景中存在噪声注入的计算复杂性和稳定性问题。

Method: 提出了SPOF方法，将模型的损失函数通过泰勒展开多项式进行稳定化逼近，并将校准的噪声添加到多项式系数上，不依赖梯度噪声注入。

Result: 在异构用户数据及传感器环境噪声条件下，SPOF相比DP-SGD实现了最高3.5%精度提升，训练时间减少57.2%。

Conclusion: SPOF在多用户场景下提供了更高效且稳定的隐私保证，并在隐私与效用平衡上优于传统方法。

Abstract: We propose SPOF (Stabilization of Perturbed Loss Function), a differentially
private training mechanism intended for multi-user local differential privacy
(LDP). SPOF perturbs a stabilized Taylor expanded polynomial approximation of a
model's training loss function, where each user's data is privatized by
calibrated noise added to the coefficients of the polynomial. Unlike
gradient-based mechanisms such as differentially private stochastic gradient
descent (DP-SGD), SPOF does not require injecting noise into the gradients of
the loss function, which improves both computational efficiency and stability.
This formulation naturally supports simultaneous privacy guarantees across all
users. Moreover, SPOF exhibits robustness to environmental noise during
training, maintaining stable performance even when user inputs are corrupted.
We compare SPOF with a multi-user extension of DP-SGD, evaluating both methods
in a wireless body area network (WBAN) scenario involving heterogeneous user
data and stochastic channel noise from body sensors. Our results show that SPOF
achieves, on average, up to 3.5% higher reconstruction accuracy and reduces
mean training time by up to 57.2% compared to DP-SGD, demonstrating superior
privacy-utility trade-offs in multi-user environments.

</details>


### [211] [AI-Powered Machine Learning Approaches for Fault Diagnosis in Industrial Pumps](https://arxiv.org/abs/2508.15550)
*Khaled M. A. Alghtus,Ayad Gannan,Khalid M. Alhajri,Ali L. A. Al Jubouri,Hassan A. I. Al-Janahi*

Main category: cs.LG

TL;DR: 本文研究了一种用工业感测数据进行早期故障检测的方法，特别针对海洋环境中的大型立式离心泵系统。


<details>
  <summary>Details</summary>
Motivation: 解决工业泵中故障检测难题，特别是针对故障案例稀缺与复杂工作环境。

Method: 引入双阈值标记法和三种机器学习分类器（随机森林、XGBoost和支持向量机）进行故障检测，并模拟合成故障信号进行测试。

Result: 随机森林和XGBoost模型表现优异，能准确识别不同故障类型，证明了提出方法的有效性和鲁棒性。

Conclusion: 所提出的框架适用于实时工业环境故障检测，可扩展至其他相似传感器架构的设备，具有广泛的预测性维护潜力。

Abstract: This study presents a practical approach for early fault detection in
industrial pump systems using real-world sensor data from a large-scale
vertical centrifugal pump operating in a demanding marine environment. Five key
operational parameters were monitored: vibration, temperature, flow rate,
pressure, and electrical current. A dual-threshold labeling method was applied,
combining fixed engineering limits with adaptive thresholds calculated as the
95th percentile of historical sensor values. To address the rarity of
documented failures, synthetic fault signals were injected into the data using
domain-specific rules, simulating critical alerts within plausible operating
ranges. Three machine learning classifiers - Random Forest, Extreme Gradient
Boosting (XGBoost), and Support Vector Machine (SVM) - were trained to
distinguish between normal operation, early warnings, and critical alerts.
Results showed that Random Forest and XGBoost models achieved high accuracy
across all classes, including minority cases representing rare or emerging
faults, while the SVM model exhibited lower sensitivity to anomalies. Visual
analyses, including grouped confusion matrices and time-series plots, indicated
that the proposed hybrid method provides robust detection capabilities. The
framework is scalable, interpretable, and suitable for real-time industrial
deployment, supporting proactive maintenance decisions before failures occur.
Furthermore, it can be adapted to other machinery with similar sensor
architectures, highlighting its potential as a scalable solution for predictive
maintenance in complex systems.

</details>


### [212] [Conformalized Exceptional Model Mining: Telling Where Your Model Performs (Not) Well](https://arxiv.org/abs/2508.15569)
*Xin Du,Sikun Yang,Wouter Duivesteijn,Mykola Pechenizkiy*

Main category: cs.LG

TL;DR: 该论文提出了一种结合保形预测（Conformal Prediction）与异常模型挖掘（Exceptional Model Mining, EMM）的新框架，以分析机器学习模型的细微性能。


<details>
  <summary>Details</summary>
Motivation: 推动模型在高风险领域（如医疗健康和金融）的可解释性和可靠部署，尤其是识别性能显著不同的子群。

Method: 引入了一种新的模型类别mSMoPE，通过保形预测量化不确定性并提出RAUL质量度量，结合EMM识别性能异常的子群。

Result: 实验表明，该框架能够在多分类和回归任务中发现可解释的子群，揭示模型行为的关键信息。

Conclusion: 此工作提升了模型的可解释性和可靠性，为可解释人工智能和不确定性量化领域的技术进步奠定了基础。

Abstract: Understanding the nuanced performance of machine learning models is essential
for responsible deployment, especially in high-stakes domains like healthcare
and finance. This paper introduces a novel framework, Conformalized Exceptional
Model Mining, which combines the rigor of Conformal Prediction with the
explanatory power of Exceptional Model Mining (EMM). The proposed framework
identifies cohesive subgroups within data where model performance deviates
exceptionally, highlighting regions of both high confidence and high
uncertainty. We develop a new model class, mSMoPE (multiplex Soft Model
Performance Evaluation), which quantifies uncertainty through conformal
prediction's rigorous coverage guarantees. By defining a new quality measure,
Relative Average Uncertainty Loss (RAUL), our framework isolates subgroups with
exceptional performance patterns in multi-class classification and regression
tasks. Experimental results across diverse datasets demonstrate the framework's
effectiveness in uncovering interpretable subgroups that provide critical
insights into model behavior. This work lays the groundwork for enhancing model
interpretability and reliability, advancing the state-of-the-art in explainable
AI and uncertainty quantification.

</details>


### [213] [Inductive Domain Transfer In Misspecified Simulation-Based Inference](https://arxiv.org/abs/2508.15593)
*Ortal Senouf,Antoine Wehenkel,Cédric Vincent-Cuaz,Emmanuel Abbé,Pascal Frossard*

Main category: cs.LG

TL;DR: 该论文提出了一种新的模拟数据推理（SBI）方法，通过集成校准和分布对齐，能够解决实际中因模型不匹配带来的挑战，并且提升了可扩展性和通用性。


<details>
  <summary>Details</summary>
Motivation: 当前SBI方法受到因模拟与现实观测间不匹配的影响，尤其是模型简化导致的误差。现有方法如RoPE在推理时需要访问测试样本，限制了其扩展性和作用范围，亟需一种更高效的解决方案。

Method: 提出了一种结合校准和分布对齐的端到端可训练模型，利用小批量最优传输（OT）和条件正规化流模型进行分布对齐和后验估计，避免了测试时对模拟进行访问。

Result: 新方法在多个基准测试中表现优异，不仅超越了现有SBI方法如RoPE，还在面临复杂建模误差情况下展示了更好的可扩展性和实用性。

Conclusion: 该方法改进了SBI体系，整合校准与分布对齐，无需测试时访问模拟数据，提供了一种更具扩展性和适应性的解决方案。

Abstract: Simulation-based inference (SBI) is a statistical inference approach for
estimating latent parameters of a physical system when the likelihood is
intractable but simulations are available. In practice, SBI is often hindered
by model misspecification--the mismatch between simulated and real-world
observations caused by inherent modeling simplifications. RoPE, a recent SBI
approach, addresses this challenge through a two-stage domain transfer process
that combines semi-supervised calibration with optimal transport (OT)-based
distribution alignment. However, RoPE operates in a fully transductive setting,
requiring access to a batch of test samples at inference time, which limits
scalability and generalization. We propose here a fully inductive and amortized
SBI framework that integrates calibration and distributional alignment into a
single, end-to-end trainable model. Our method leverages mini-batch OT with a
closed-form coupling to align real and simulated observations that correspond
to the same latent parameters, using both paired calibration data and unpaired
samples. A conditional normalizing flow is then trained to approximate the
OT-induced posterior, enabling efficient inference without simulation access at
test time. Across a range of synthetic and real-world benchmarks--including
complex medical biomarker estimation--our approach matches or surpasses the
performance of RoPE, as well as other standard SBI and non-SBI estimators,
while offering improved scalability and applicability in challenging,
misspecified environments.

</details>


### [214] [Continual Neural Topic Model](https://arxiv.org/abs/2508.15612)
*Charu Karakkaparambil James,Waleed Mustafa,Marius Kloft,Sophie Fellenz*

Main category: cs.LG

TL;DR: 提出了一种名为“CoNTM”的连续神经主题模型，能够在新增任务时不遗忘已学到的主题，且性能优于现有动态主题模型。


<details>
  <summary>Details</summary>
Motivation: 现有模型或基于完整数据集进行静态训练，或无法保留长期记忆，无法满足连续学习的需求。

Method: 通过利用一个可持续更新的全局先验分布，CoNTM实现了连续地学习主题模型，同时避免遗忘。

Result: 实验表明，CoNTM在主题质量和预测困惑度上超越了动态主题模型，并能够在线捕捉主题变化。

Conclusion: CoNTM能学习更多样化的主题，并更好地捕捉时间上的变化，适用于连续学习场景。

Abstract: In continual learning, our aim is to learn a new task without forgetting what
was learned previously. In topic models, this translates to learning new topic
models without forgetting previously learned topics. Previous work either
considered Dynamic Topic Models (DTMs), which learn the evolution of topics
based on the entire training corpus at once, or Online Topic Models, which are
updated continuously based on new data but do not have long-term memory. To
fill this gap, we propose the Continual Neural Topic Model (CoNTM), which
continuously learns topic models at subsequent time steps without forgetting
what was previously learned. This is achieved using a global prior distribution
that is continuously updated. In our experiments, CoNTM consistently
outperformed the dynamic topic model in terms of topic quality and predictive
perplexity while being able to capture topic changes online. The analysis
reveals that CoNTM can learn more diverse topics and better capture temporal
changes than existing methods.

</details>


### [215] [GRASPED: Graph Anomaly Detection using Autoencoder with Spectral Encoder and Decoder (Full Version)](https://arxiv.org/abs/2508.15633)
*Wei Herng Choong,Jixing Liu,Ching-Yu Kao,Philip Sperl*

Main category: cs.LG

TL;DR: 提出了一种用于节点异常检测的新型图自动编码器GRASPED，通过Graph Wavelet Convolution和Wiener Graph Deconvolution结合利用多频段信息进行无监督学习。


<details>
  <summary>Details</summary>
Motivation: 现有的异常检测方法受限于标记数据的稀缺性或缺乏多频段分析能力，亟需新的方法以更好捕获异常信息。

Method: 设计了一种基于Graph Wavelet Convolution的编码器和基于Wiener Graph Deconvolution的解码器，结合构建了结构解码器和属性解码器用于重建节点属性。

Result: 在多个真实图异常检测数据集上的实验证明，GRASPED模型优于现有最先进模型。

Conclusion: 该模型有效结合了波动域分析与图结构信息，显著提高了无监督节点异常检测的性能。

Abstract: Graph machine learning has been widely explored in various domains, such as
community detection, transaction analysis, and recommendation systems. In these
applications, anomaly detection plays an important role. Recently, studies have
shown that anomalies on graphs induce spectral shifts. Some supervised methods
have improved the utilization of such spectral domain information. However,
they remain limited by the scarcity of labeled data due to the nature of
anomalies. On the other hand, existing unsupervised learning approaches
predominantly rely on spatial information or only employ low-pass filters,
thereby losing the capacity for multi-band analysis. In this paper, we propose
Graph Autoencoder with Spectral Encoder and Spectral Decoder (GRASPED) for node
anomaly detection. Our unsupervised learning model features an encoder based on
Graph Wavelet Convolution, along with structural and attribute decoders. The
Graph Wavelet Convolution-based encoder, combined with a Wiener Graph
Deconvolution-based decoder, exhibits bandpass filter characteristics that
capture global and local graph information at multiple scales. This design
allows for a learning-based reconstruction of node attributes, effectively
capturing anomaly information. Extensive experiments on several real-world
graph anomaly detection datasets demonstrate that GRASPED outperforms current
state-of-the-art models.

</details>


### [216] [Classification errors distort findings in automated speech processing: examples and solutions from child-development research](https://arxiv.org/abs/2508.15637)
*Lucas Gautheron,Evan Kidd,Anton Malko,Marvin Lavechin,Alejandrina Cristia*

Main category: cs.LG

TL;DR: 本研究提出了一种贝叶斯方法来评估算法误差对语言获取研究中统计估计的影响，发现分类误差会显著扭曲结果，如低估兄弟姐妹对儿童语言输入的负面影响。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴记录仪的兴起，科学家们越来越多地采用自动化分析方法研究儿童的语言获取和行为影响，但分类误差对科学推论的影响尚待深入探讨。

Method: 采用贝叶斯方法探讨算法误差在关键语言获取研究中的影响，包括主流的LENA系统和ACLEW开源分类器。

Result: 研究表明分类误差可能会显著扭曲效果估计，例如低估兄弟姐妹对语言输入的负面影响达20%-80%。

Conclusion: 贝叶斯校准方法可以一定程度上纠正分类误差导致的偏差，但并非万能，此问题可能普遍适用于任何涉及事件检测和分类的算法。

Abstract: With the advent of wearable recorders, scientists are increasingly turning to
automated methods of analysis of audio and video data in order to measure
children's experience, behavior, and outcomes, with a sizable literature
employing long-form audio-recordings to study language acquisition. While
numerous articles report on the accuracy and reliability of the most popular
automated classifiers, less has been written on the downstream effects of
classification errors on measurements and statistical inferences (e.g., the
estimate of correlations and effect sizes in regressions). This paper proposes
a Bayesian approach to study the effects of algorithmic errors on key
scientific questions, including the effect of siblings on children's language
experience and the association between children's production and their input.
In both the most commonly used \gls{lena}, and an open-source alternative (the
Voice Type Classifier from the ACLEW system), we find that classification
errors can significantly distort estimates. For instance, automated annotations
underestimated the negative effect of siblings on adult input by 20--80\%,
potentially placing it below statistical significance thresholds. We further
show that a Bayesian calibration approach for recovering unbiased estimates of
effect sizes can be effective and insightful, but does not provide a fool-proof
solution. Both the issue reported and our solution may apply to any classifier
involving event detection and classification with non-zero error rates.

</details>


### [217] [Correct-By-Construction: Certified Individual Fairness through Neural Network Training](https://arxiv.org/abs/2508.15642)
*Ruihan Zhang,Jun Sun*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架，通过两部分策略保证机器学习中的个体公平性：公平初始化和保持公平的训练算法。


<details>
  <summary>Details</summary>
Motivation: 目前机器学习中的个体公平性难以被保证，现有方法通常缺乏形式化的公平性保障或仅仅依赖于验证技术，无法在训练过程中主动增强公平。

Method: 提出结合了随机响应机制的框架，包括公平初始化，确保模型从公平状态开始，以及保持公平性的训练算法，避免模型训练过程中的不公平性增加。

Result: 实验表明，该方法可有效生成既符合公平性又具有准确性的模型，且与基于神经网络验证的认证训练方法相比效率更高。

Conclusion: 该方法为机器学习训练过程中的个体公平性提供了形式化保障，是一种既高效又实用的改进。

Abstract: Fairness in machine learning is more important than ever as ethical concerns
continue to grow. Individual fairness demands that individuals differing only
in sensitive attributes receive the same outcomes. However, commonly used
machine learning algorithms often fail to achieve such fairness. To improve
individual fairness, various training methods have been developed, such as
incorporating fairness constraints as optimisation objectives. While these
methods have demonstrated empirical effectiveness, they lack formal guarantees
of fairness. Existing approaches that aim to provide fairness guarantees
primarily rely on verification techniques, which can sometimes fail to produce
definitive results. Moreover, verification alone does not actively enhance
individual fairness during training. To address this limitation, we propose a
novel framework that formally guarantees individual fairness throughout
training. Our approach consists of two parts, i.e., (1) provably fair
initialisation that ensures the model starts in a fair state, and (2) a
fairness-preserving training algorithm that maintains fairness as the model
learns. A key element of our method is the use of randomised response
mechanisms, which protect sensitive attributes while maintaining fairness
guarantees. We formally prove that this mechanism sustains individual fairness
throughout the training process. Experimental evaluations confirm that our
approach is effective, i.e., producing models that are empirically fair and
accurate. Furthermore, our approach is much more efficient than the alternative
approach based on certified training (which requires neural network
verification during training).

</details>


### [218] [Amortized In-Context Mixed Effect Transformer Models: A Zero-Shot Approach for Pharmacokinetics](https://arxiv.org/abs/2508.15659)
*César Ali Ojeda Marin,Wilhelm Huisinga,Purity Kavwele,Niklas Hartung*

Main category: cs.LG

TL;DR: 本文提出了一种名为AICMET的模型，用以解决稀疏采样下的剂量反应预测问题，并在多个数据集上的表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前精准药物治疗中，稀疏采样下的剂量反应预测是核心问题，传统模型开发周期较长且难以适应新化合物。

Method: 通过引入基于变压器的潜变量框架，结合分区模型和贝叶斯推断，AICMET预训练于大量合成药代动力学轨迹，并使用Ornstein-Uhlenbeck先验构建强大的归纳偏置，可零样本匹配新化合物。

Result: 该模型在多个公开数据集上达到了当前最佳的预测精度，同时能够合理量化患者间的差异性，优于传统非线性混合效应模型和神经ODE变体。

Conclusion: AICMET展现了变压器架构在个性化药物剂量设计中的潜力，提供了一种新颖的人口意识药代动力学建模方法。

Abstract: Accurate dose-response forecasting under sparse sampling is central to
precision pharmacotherapy. We present the Amortized In-Context Mixed-Effect
Transformer (AICMET) model, a transformer-based latent-variable framework that
unifies mechanistic compartmental priors with amortized in-context Bayesian
inference. AICMET is pre-trained on hundreds of thousands of synthetic
pharmacokinetic trajectories with Ornstein-Uhlenbeck priors over the parameters
of compartment models, endowing the model with strong inductive biases and
enabling zero-shot adaptation to new compounds. At inference time, the decoder
conditions on the collective context of previously profiled trial participants,
generating calibrated posterior predictions for newly enrolled patients after a
few early drug concentration measurements. This capability collapses
traditional model-development cycles from weeks to hours while preserving some
degree of expert modelling. Experiments across public datasets show that AICMET
attains state-of-the-art predictive accuracy and faithfully quantifies
inter-patient variability -- outperforming both nonlinear mixed-effects
baselines and recent neural ODE variants. Our results highlight the feasibility
of transformer-based, population-aware neural architectures as offering a new
alternative for bespoke pharmacokinetic modeling pipelines, charting a path
toward truly population-aware personalized dosing regimens.

</details>


### [219] [Tensorized Multi-Task Learning for Personalized Modeling of Heterogeneous Individuals with High-Dimensional Data](https://arxiv.org/abs/2508.15676)
*Elif Konyar,Mostafa Reisi Gahrooei,Kamran Paynabar*

Main category: cs.LG

TL;DR: 该研究通过结合多任务学习（MTL）和低秩张量分解技术，有效解决异质性子群体建模的挑战。


<details>
  <summary>Details</summary>
Motivation: 为解决个体差异导致的建模挑战，提出一种能捕捉相似任务共享结构及子群体特定变化的新方法。

Method: 采用低秩分解技术，将任务模型参数分解为低秩结构，既共享知识又保留子群体独特性。

Result: 实验表明，新方法在高异质性场景下优于其他基准方法，显著提高预测准确性并增强模型解释性。

Conclusion: 该框架在提高个性化建模能力的同时，揭示了影响模型个性化的重要模式。

Abstract: Effective modeling of heterogeneous subpopulations presents a significant
challenge due to variations in individual characteristics and behaviors. This
paper proposes a novel approach to address this issue through multi-task
learning (MTL) and low-rank tensor decomposition techniques. Our MTL approach
aims to enhance personalized modeling by leveraging shared structures among
similar tasks while accounting for distinct subpopulation-specific variations.
We introduce a framework where low-rank decomposition decomposes the collection
of task model parameters into a low-rank structure that captures commonalities
and variations across tasks and subpopulations. This approach allows for
efficient learning of personalized models by sharing knowledge between similar
tasks while preserving the unique characteristics of each subpopulation.
Experimental results in simulation and case study datasets demonstrate the
superior performance of the proposed method compared to several benchmarks,
particularly in scenarios with high variability among subpopulations. The
proposed framework not only improves prediction accuracy but also enhances
interpretability by revealing underlying patterns that contribute to the
personalization of models.

</details>


### [220] [An Efficient Open World Environment for Multi-Agent Social Learning](https://arxiv.org/abs/2508.15679)
*Eric Ye,Ren Tao,Natasha Jaques*

Main category: cs.LG

TL;DR: 本文探讨在一个多智能体环境中，AI 能否通过社会学习提高表现，包括合作与竞争。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个能够研究社会智能的 AI 的多智能体环境，以模拟现实世界的复杂性并解决目前在开放式多智能体环境中的研究不足。

Method: 提出一个允许多个自利智能体追求复杂独立目标的环境，研究社会学习在专家和隐性合作，例如工具使用的情况下对智能体表现的影响。

Result: 证明了该环境可以激发隐性合作，如共享工具和实现长期目标，同时探索了合作和竞争对智能体表现的作用。

Conclusion: 此研究深化了对社会智能 AI 的理解，并展示了开放式多智能体环境用于社会学习研究的潜力。

Abstract: Many challenges remain before AI agents can be deployed in real-world
environments. However, one virtue of such environments is that they are
inherently multi-agent and contain human experts. Using advanced social
intelligence in such an environment can help an AI agent learn adaptive skills
and behaviors that a known expert exhibits. While social intelligence could
accelerate training, it is currently difficult to study due to the lack of
open-ended multi-agent environments. In this work, we present an environment in
which multiple self-interested agents can pursue complex and independent goals,
reflective of real world challenges. This environment will enable research into
the development of socially intelligent AI agents in open-ended multi-agent
settings, where agents may be implicitly incentivized to cooperate to defeat
common enemies, build and share tools, and achieve long horizon goals. In this
work, we investigate the impact on agent performance due to social learning in
the presence of experts and implicit cooperation such as emergent collaborative
tool use, and whether agents can benefit from either cooperation or competition
in this environment.

</details>


### [221] [Conditionally adaptive augmented Lagrangian method for physics-informed learning of forward and inverse problems using artificial neural networks](https://arxiv.org/abs/2508.15695)
*Qifeng Hu,Shamsulhaq Basir,Inanc Senocak*

Main category: cs.LG

TL;DR: 本文对PECANN框架进行了多项改进，以提高其求解典型偏微分方程的能力。


<details>
  <summary>Details</summary>
Motivation: 提高PECANN在学习和求解复杂偏微分方程（PDEs）中的能力，特别是在涉及异构约束、长时间演化和多尺度特征的情景中。

Method: 文章提出了多项增强：包括广义增广拉格朗日法（ALM）、通过期望值处理点约束和拉格朗日乘子、使用傅里叶特征映射、时间窗口策略以及有条件自适应惩罚更新策略（CAPU）。

Result: 通过多种问题测试（如跨音速稀疏、涡流逆向对流、高波数的Helmholtz和Poisson方程，以及空间可变热源的逆识别），PECANN-CAPU在精度和效率上均表现优异，与现有方法竞争力强。

Conclusion: 改进后的PECANN在科学计算中的鲁棒性、效率及适用性有显著提升，可处理更复杂的偏微分方程约束问题。

Abstract: We present several advances to the physics and equality constrained
artificial neural networks (PECANN) framework that substantially improve its
capability to learn solutions of canonical partial differential equations
(PDEs). First, we generalize the augmented Lagrangian method (ALM) to support
multiple independent penalty parameters, enabling simultaneous enforcement of
heterogeneous constraints. Second, we reformulate pointwise constraint
enforcement and Lagrange multipliers as expectations over constraint terms,
reducing memory overhead and permitting efficient mini-batch training. Third,
to address PDEs with oscillatory, multi-scale features, we incorporate Fourier
feature mappings and show that a single mapping suffices where multiple
mappings or more costly architectures were required in related methods. Fourth,
we introduce a time-windowing strategy for long-time evolution in which the
terminal state of each window is enforced as an initial-condition constraint
for the next, ensuring continuity without discrete time models. Crucially, we
propose a conditionally adaptive penalty update (CAPU) strategy for ALM, which
preserves the principle that larger constraint violations incur stronger
penalties. CAPU accelerates the growth of Lagrange multipliers for selectively
challenging constraints, enhancing constraint enforcement during training. We
demonstrate the effectiveness of PECANN-CAPU on problems including the
transonic rarefaction problem, reversible advection of a passive by a vortex,
high-wavenumber Helmholtz and Poisson equations, and inverse identification of
spatially varying heat sources. Comparisons with established methods and recent
Kolmogorov-Arnold network approaches show that PECANN-CAPU achieves competitive
accuracy across all cases. Collectively, these advances improve PECANN's
robustness, efficiency, and applicability to demanding problems in scientific
computing.

</details>


### [222] [Investigation of D-Wave quantum annealing for training Restricted Boltzmann Machines and mitigating catastrophic forgetting](https://arxiv.org/abs/2508.15697)
*Abdelmoula El-Yazizi,Yaroslav Koshka*

Main category: cs.LG

TL;DR: 本论文探讨了D-Wave量子退火器（QA）和经典马尔科夫链蒙特卡洛（MCMC）方法在受限玻尔兹曼机（RBMs）中的采样性能差异，以及这些差异对RBM训练提升的有限影响。如果将两种方法结合使用，可能在其他机器学习应用中更有效，例如增量学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 探讨D-Wave QA与传统MCMC采样性能的差异，以及这一差异如何影响RBM训练和其他机器学习应用；评估量子生成模式在增量学习中的可能应用。

Method: 结合量子退火采样与经典采样（MCMC），并分析其在RBM训练及灾难性遗忘减缓中的效果。

Result: 发现QA和MCMC采样在中低概率分布中的差异对RBM训练改进效果有限。而使用D-Wave QA样本进行生成重放，首次验证了其在减缓灾难性遗忘中的可行性。

Conclusion: 量子与经典方法的结合在RBM训练中未见明显改进，但在特定机器学习场景，如灾难性遗忘问题中，量子生成样本提供了潜在的改进可能性和适用前景。

Abstract: Modest statistical differences between the sampling performances of the
D-Wave quantum annealer (QA) and the classical Markov Chain Monte Carlo (MCMC),
when applied to Restricted Boltzmann Machines (RBMs), are explored to explain,
and possibly address, the absence of significant and consistent improvements in
RBM trainability when the D-Wave sampling was used in previous investigations.
A novel hybrid sampling approach, combining the classical and the QA
contributions, is investigated as a promising way to benefit from the modest
differences between the two sampling methods. No improvements in the RBM
training are achieved in this work, thereby suggesting that the differences
between the QA-based and MCMC sampling, mainly found in the medium-to-low
probability regions of the distribution, which are less important for the
quality of the sample, are insufficient to benefit the training. Difficulties
in achieving sufficiently high quality of embedding RBMs into the lattice of
the newer generation of D-Wave hardware could be further complicating the task.
On the other hand, the ability to generate samples of sufficient variety from
lower-probability parts of the distribution has a potential to benefit other
machine learning applications, such as the mitigation of catastrophic
forgetting (CF) during incremental learning. The feasibility of using
QA-generated patterns of desirable classes for CF mitigation by the generative
replay is demonstrated in this work for the first time. While the efficiency of
the CF mitigation using the D-Wave QA was comparable to that of the classical
mitigation, both the speed of generating a large number of distinct desirable
patterns and the potential for further improvement make this approach promising
for a variety of challenging machine learning applications.

</details>


### [223] [Communication Efficient LLM Pre-training with SparseLoCo](https://arxiv.org/abs/2508.15706)
*Amir Sarfi,Benjamin Thérien,Joel Lidin,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 提出了SparseLoCo，一种通信高效的分布式训练算法，可通过Top-k稀疏化和量化实现大幅度压缩，同时提升性能并降低通信成本。


<details>
  <summary>Details</summary>
Motivation: 现有的通信高效算法在LLM训练中仍存在通信瓶颈，且性能可能下降，同时缺乏高效利用稀疏化和量化的能力。

Method: 引入SparseLoCo算法，结合Top-k稀疏化和量化，并使用外部动量局部近似及稀疏聚合来提升性能与极限压缩。

Result: SparseLoCo在各种通信受限的LLM训练场景中显著降低通信成本，同时性能优于全精度DiLoCo。

Conclusion: SparseLoCo在通信效率、模型性能和压缩比率上均取得显著突破，适合带宽受限条件下的LLM训练。

Abstract: Communication-efficient distributed training algorithms have received
considerable interest recently due to their benefits for training Large
Language Models (LLMs) in bandwidth-constrained settings, such as across data
centers and over the internet. Despite reducing communication frequency, these
methods still typically require communicating a full copy of the model's
gradients-resulting in a communication bottleneck even for cross-datacenter
links. Furthermore, they can slightly degrade performance compared to a naive
AdamW DDP baseline. While quantization and error feedback are often applied to
reduce the pseudo-gradient's size, in the context of LLM pre-training, existing
approaches have been unable to additionally leverage sparsification and have
obtained limited quantization. In this work, we introduce SparseLoCo, a
communication-efficient training algorithm for LLMs that effectively leverages
Top-k sparsification and quantization to reach extreme compression ratios of up
to 1-3% sparsity and 2-bit quantization while outperforming full-precision
DiLoCo. Our key observations are that outer momentum can be locally
approximated by an error feedback combined with aggressive sparsity and that
sparse aggregation can actually improve model performance. We empirically
demonstrate in a range of communication-constrained LLM training settings that
SparseLoCo provides significant benefits in both performance and communication
cost.

</details>


### [224] [Tutorial on the Probabilistic Unification of Estimation Theory, Machine Learning, and Generative AI](https://arxiv.org/abs/2508.15719)
*Mohammed Elmusrati*

Main category: cs.LG

TL;DR: 本文探讨了从不确定和嘈杂数据中提取信息的问题，通过统一框架连接古典估计理论、统计推断和现代机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 旨在分析AI方法背后的概率性原则以及如何解决实际问题如过拟合和数据稀疏性。

Method: 通过最大似然估计、贝叶斯推断和注意力机制等工具，讨论系统识别、图像分类和语言生成等场景中的问题解决方法。

Result: 展示了最大似然、MAP估计、贝叶斯分类和深度学习等方法的共通性，突出了所有方法都致力于从嘈杂/偏见观测中推断隐藏原因。

Conclusion: 为学生和研究人员提供理论综合和实际指南，助力导航机器学习不断发展的领域。

Abstract: Extracting meaning from uncertain, noisy data is a fundamental problem across
time series analysis, pattern recognition, and language modeling. This survey
presents a unified mathematical framework that connects classical estimation
theory, statistical inference, and modern machine learning, including deep
learning and large language models. By analyzing how techniques such as maximum
likelihood estimation, Bayesian inference, and attention mechanisms address
uncertainty, the paper illustrates that many AI methods are rooted in shared
probabilistic principles. Through illustrative scenarios including system
identification, image classification, and language generation, we show how
increasingly complex models build upon these foundations to tackle practical
challenges like overfitting, data sparsity, and interpretability. In other
words, the work demonstrates that maximum likelihood, MAP estimation, Bayesian
classification, and deep learning all represent different facets of a shared
goal: inferring hidden causes from noisy and/or biased observations. It serves
as both a theoretical synthesis and a practical guide for students and
researchers navigating the evolving landscape of machine learning.

</details>


### [225] [Probability Density from Latent Diffusion Models for Out-of-Distribution Detection](https://arxiv.org/abs/2508.15737)
*Joonas Järve,Karl Kaspar Haavel,Meelis Kull*

Main category: cs.LG

TL;DR: 该研究探讨了生成模型中基于似然的分布外检测方法的实际表现。


<details>
  <summary>Details</summary>
Motivation: AI系统部署的主要瓶颈在于安全性，而分布外检测是关键的安全组件之一。生成模型中最自然的分布外检测指标是数据似然，但之前的研究认为其在实际应用中效果欠佳，因此需要更深入地探究。

Method: 通过训练变分扩散模型用预训练ResNet-18的表示空间进行似然估计，并将其与OpenOOD套件的其他方法进行对比。

Result: 评估了基于似然的检测方法在实际应用中的性能，并与现有技术进行了对比分析。

Conclusion: 在实际应用中，基于表示空间进行分布外检测可能弥补传统生成模型方法在像素空间的不足。

Abstract: Despite rapid advances in AI, safety remains the main bottleneck to deploying
machine-learning systems. A critical safety component is out-of-distribution
detection: given an input, decide whether it comes from the same distribution
as the training data. In generative models, the most natural OOD score is the
data likelihood. Actually, under the assumption of uniformly distributed OOD
data, the likelihood is even the optimal OOD detector, as we show in this work.
However, earlier work reported that likelihood often fails in practice, raising
doubts about its usefulness. We explore whether, in practice, the
representation space also suffers from the inability to learn good density
estimation for OOD detection, or if it is merely a problem of the pixel space
typically used in generative models. To test this, we trained a Variational
Diffusion Model not on images, but on the representation space of a pre-trained
ResNet-18 to assess the performance of our likelihood-based detector in
comparison to state-of-the-art methods from the OpenOOD suite.

</details>


### [226] [Intern-S1: A Scientific Multimodal Foundation Model](https://arxiv.org/abs/2508.15763)
*Lei Bai,Zhongrui Cai,Maosong Cao,Weihan Cao,Chiyu Chen,Haojiong Chen,Kai Chen,Pengcheng Chen,Ying Chen,Yongkang Chen,Yu Cheng,Yu Cheng,Pei Chu,Tao Chu,Erfei Cui,Ganqu Cui,Long Cui,Ziyun Cui,Nianchen Deng,Ning Ding,Nanqin Dong,Peijie Dong,Shihan Dou,Sinan Du,Haodong Duan,Caihua Fan,Ben Gao,Changjiang Gao,Jianfei Gao,Songyang Gao,Yang Gao,Zhangwei Gao,Jiaye Ge,Qiming Ge,Lixin Gu,Yuzhe Gu,Aijia Guo,Qipeng Guo,Xu Guo,Conghui He,Junjun He,Yili Hong,Siyuan Hou,Caiyu Hu,Hanglei Hu,Jucheng Hu,Ming Hu,Zhouqi Hua,Haian Huang,Junhao Huang,Xu Huang,Zixian Huang,Zhe Jiang,Lingkai Kong,Linyang Li,Peiji Li,Pengze Li,Shuaibin Li,Tianbin Li,Wei Li,Yuqiang Li,Dahua Lin,Junyao Lin,Tianyi Lin,Zhishan Lin,Hongwei Liu,Jiangning Liu,Jiyao Liu,Junnan Liu,Kai Liu,Kaiwen Liu,Kuikun Liu,Shichun Liu,Shudong Liu,Wei Liu,Xinyao Liu,Yuhong Liu,Zhan Liu,Yinquan Lu,Haijun Lv,Hongxia Lv,Huijie Lv,Qidang Lv,Ying Lv,Chengqi Lyu,Chenglong Ma,Jianpeng Ma,Ren Ma,Runmin Ma,Runyuan Ma,Xinzhu Ma,Yichuan Ma,Zihan Ma,Sixuan Mi,Junzhi Ning,Wenchang Ning,Xinle Pang,Jiahui Peng,Runyu Peng,Yu Qiao,Jiantao Qiu,Xiaoye Qu,Yuan Qu,Yuchen Ren,Fukai Shang,Wenqi Shao,Junhao Shen,Shuaike Shen,Chunfeng Song,Demin Song,Diping Song,Chenlin Su,Weijie Su,Weigao Sun,Yu Sun,Qian Tan,Cheng Tang,Huanze Tang,Kexian Tang,Shixiang Tang,Jian Tong,Aoran Wang,Bin Wang,Dong Wang,Lintao Wang,Rui Wang,Weiyun Wang,Wenhai Wang,Yi Wang,Ziyi Wang,Ling-I Wu,Wen Wu,Yue Wu,Zijian Wu,Linchen Xiao,Shuhao Xing,Chao Xu,Huihui Xu,Jun Xu,Ruiliang Xu,Wanghan Xu,GanLin Yang,Yuming Yang,Haochen Ye,Jin Ye,Shenglong Ye,Jia Yu,Jiashuo Yu,Jing Yu,Fei Yuan,Bo Zhang,Chao Zhang,Chen Zhang,Hongjie Zhang,Jin Zhang,Qiaosheng Zhang,Qiuyinzhe Zhang,Songyang Zhang,Taolin Zhang,Wenlong Zhang,Wenwei Zhang,Yechen Zhang,Ziyang Zhang,Haiteng Zhao,Qian Zhao,Xiangyu Zhao,Xiangyu Zhao,Bowen Zhou,Dongzhan Zhou,Peiheng Zhou,Yuhao Zhou,Yunhua Zhou,Dongsheng Zhu,Lin Zhu,Yicheng Zou*

Main category: cs.LG

TL;DR: Intern-S1是一种具备多领域理解和推理能力的多模态混合专家模型，在科学领域任务中表现出色，超越现有开源和部分闭源模型。


<details>
  <summary>Details</summary>
Motivation: 解决科学专业领域中开源模型性能落后且与闭源模型之间存在显著差距的问题，并推动通用人工智能（AGI）的发展。

Method: 提出Intern-S1多模态混合专家模型，包含28B激活参数和241B总参数，预训练数据涵盖科学领域，并结合Mixture-of-Rewards进行强化学习训练，同时优化算法、数据和训练系统。

Result: Intern-S1在综合评估中展现出出色的推理能力，特别是在分子合成规划、反应条件预测和晶体热力学稳定性预测等专业任务上，表现超过闭源最先进模型。

Conclusion: 通过结合多模态技术和强化学习，Intern-S1成功缩小开源与闭源模型在科学领域的差距，为科学研究自动化提供新的可能性。

Abstract: In recent years, a plethora of open-source foundation models have emerged,
achieving remarkable progress in some widely attended fields, with performance
being quite close to that of closed-source models. However, in high-value but
more challenging scientific professional fields, either the fields still rely
on expert models, or the progress of general foundation models lags
significantly compared to those in popular areas, far from sufficient for
transforming scientific research and leaving substantial gap between
open-source models and closed-source models in these scientific domains. To
mitigate this gap and explore a step further toward Artificial General
Intelligence (AGI), we introduce Intern-S1, a specialized generalist equipped
with general understanding and reasoning capabilities with expertise to analyze
multiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE)
model with 28 billion activated parameters and 241 billion total parameters,
continually pre-trained on 5T tokens, including over 2.5T tokens from
scientific domains. In the post-training stage, Intern-S1 undergoes offline and
then online reinforcement learning (RL) in InternBootCamp, where we propose
Mixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks
simultaneously. Through integrated innovations in algorithms, data, and
training systems, Intern-S1 achieved top-tier performance in online RL
training.On comprehensive evaluation benchmarks, Intern-S1 demonstrates
competitive performance on general reasoning tasks among open-source models and
significantly outperforms open-source models in scientific domains, surpassing
closed-source state-of-the-art models in professional tasks, such as molecular
synthesis planning, reaction condition prediction, predicting thermodynamic
stabilities for crystals. Our models are available at
https://huggingface.co/internlm/Intern-S1.

</details>


### [227] [Distributed Detection of Adversarial Attacks in Multi-Agent Reinforcement Learning with Continuous Action Space](https://arxiv.org/abs/2508.15764)
*Kiarash Kazari,Ezzeldin Shereen,György Dán*

Main category: cs.LG

TL;DR: 该研究提出了一种去中心化检测器，用于检测连续动作空间的多智能体强化学习中的对抗攻击，通过局部观测和统计特性进行判别。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体环境下的强化学习，特别是带有连续动作空间的情况下，容易受到对抗攻击，需要有效的检测机制来保护此类系统。

Method: 设计了一种去中心化检测器，通过深度神经网络学习的正态分布参数模拟正常行为，并结合两侧CUSUM方法实时检测异常行为，同时使用AUC-ROC评估效果。

Result: 在多种PettingZoo基准测试中，该方法能够有效检测各种高影响力的对抗攻击，并在所有测试环境中实现了超过0.95的AUC-ROC得分。

Conclusion: 提出的方法在检测连续动作空间下的对抗攻击方面表现优异，可为多智能体系统提供即时安全保障。

Abstract: We address the problem of detecting adversarial attacks against cooperative
multi-agent reinforcement learning with continuous action space. We propose a
decentralized detector that relies solely on the local observations of the
agents and makes use of a statistical characterization of the normal behavior
of observable agents. The proposed detector utilizes deep neural networks to
approximate the normal behavior of agents as parametric multivariate Gaussian
distributions. Based on the predicted density functions, we define a normality
score and provide a characterization of its mean and variance. This
characterization allows us to employ a two-sided CUSUM procedure for detecting
deviations of the normality score from its mean, serving as a detector of
anomalous behavior in real-time. We evaluate our scheme on various multi-agent
PettingZoo benchmarks against different state-of-the-art attack methods, and
our results demonstrate the effectiveness of our method in detecting impactful
adversarial attacks. Particularly, it outperforms the discrete counterpart by
achieving AUC-ROC scores of over 0.95 against the most impactful attacks in all
evaluated environments.

</details>


### [228] [Discovering Hidden Algebraic Structures via Transformers with Rank-Aware Beam GRPO](https://arxiv.org/abs/2508.15766)
*Jaeha Lee,Gio Huh,Ning Su,Tony Yue YU*

Main category: cs.LG

TL;DR: 本文探索了变压器在逻辑推理和符号计算方面的潜力，尤其是在处理多元多项式分解这一复杂的NP难问题上的表现。


<details>
  <summary>Details</summary>
Motivation: 研究变压器在科学和工程中的复杂代数问题上的潜能，特别是其在多项式分解任务中的表现。

Method: 提出了合成数据生成流程以控制问题复杂性，利用监督学习优化变压器并评价其通用性，同时提出了一种名为BGRPO的强化学习方法以提升推理效率和精度。

Result: 优化后的模型通过BGRPO方法在推理效率上实现了近75%的计算成本降低，对多项式简化任务表现优异，甚至在若干案例中超越了Mathematica。

Conclusion: 这个方法在多元多项式分解和相关任务上表现出色，证明了变压器模型在复杂代数任务中的潜力。

Abstract: Recent efforts have extended the capabilities of transformers in logical
reasoning and symbolic computations. In this work, we investigate their
capacity for non-linear latent pattern discovery in the context of functional
decomposition, focusing on the challenging algebraic task of multivariate
polynomial decomposition. This problem, with widespread applications in science
and engineering, is proved to be NP-hard, and demands both precision and
insight. Our contributions are threefold: First, we develop a synthetic data
generation pipeline providing fine-grained control over problem complexity.
Second, we train transformer models via supervised learning and evaluate them
across four key dimensions involving scaling behavior and generalizability.
Third, we propose Beam Grouped Relative Policy Optimization (BGRPO), a
rank-aware reinforcement learning method suitable for hard algebraic problems.
Finetuning with BGRPO improves accuracy while reducing beam width by up to
half, resulting in approximately 75% lower inference compute. Additionally, our
model demonstrates competitive performance in polynomial simplification,
outperforming Mathematica in various cases.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [229] [A Scalable Trie Building Algorithm for High-Throughput Phyloanalysis of Wafer-Scale Digital Evolution Experiments](https://arxiv.org/abs/2508.15074)
*Vivaan Singhvi,Joey Wagner,Emily Dolson,Luis Zaman,Matthew Andres Moreno*

Main category: cs.NE

TL;DR: 该论文改进了遗传谱系构建算法，提高了大规模数字进化实验中的重建速度。


<details>
  <summary>Details</summary>
Motivation: 为了应对AI/ML加速硬件中内存限制下谱系追踪困难的问题，并支持大规模数字生物遗传关系的快速分析。

Method: 提出了一种改进的Trie构建算法，能显著提高谱系重建速度，并对现有基于遗传标记的算法进行优化。

Result: 在10,000个节点的树上取得了300倍速度提升，并在模拟中成功处理了10亿基因组数据，重建了含有954万亿次复制事件的谱系。

Conclusion: 此研究显著提升了谱系分析的扩展性与高通量性能，为大规模数字进化实验提供了关键技术支持。

Abstract: Agent-based simulation platforms play a key role in enabling fast-to-run
evolution experiments that can be precisely controlled and observed in detail.
Availability of high-resolution snapshots of lineage ancestries from digital
experiments, in particular, is key to investigations of evolvability and
open-ended evolution, as well as in providing a validation testbed for
bioinformatics method development. Ongoing advances in AI/ML hardware
accelerator devices, such as the 850,000-processor Cerebras Wafer-Scale Engine
(WSE), are poised to broaden the scope of evolutionary questions that can be
investigated in silico. However, constraints in memory capacity and locality
characteristic of these systems introduce difficulties in exhaustively tracking
phylogenies at runtime. To overcome these challenges, recent work on hereditary
stratigraphy algorithms has developed space-efficient genetic markers to
facilitate fully decentralized estimation of relatedness among digital
organisms. However, in existing work, compute time to reconstruct phylogenies
from these genetic markers has proven a limiting factor in achieving
large-scale phyloanalyses. Here, we detail an improved trie-building algorithm
designed to produce reconstructions equivalent to existing approaches. For
modestly-sized 10,000-tip trees, the proposed approach achieves a 300-fold
speedup versus existing state-of-the-art. Finally, using 1 billion genome
datasets drawn from WSE simulations encompassing 954 trillion replication
events, we report a pair of large-scale phylogeny reconstruction trials,
achieving end-to-end reconstruction times of 2.6 and 2.9 hours. In
substantially improving reconstruction scaling and throughput, presented work
establishes a key foundation to enable powerful high-throughput phyloanalysis
techniques in large-scale digital evolution experiments.

</details>


### [230] [From Basic Affordances to Symbolic Thought: A Computational Phylogenesis of Biological Intelligence](https://arxiv.org/abs/2508.15082)
*John E. Hummel,Rachel F. Heaton*

Main category: cs.NE

TL;DR: 本研究探讨支撑人类符号思维的神经机制，提出多位置谓词和结构映射是符号思维的最小需求，并通过17项模拟测试支持这一假设。 


<details>
  <summary>Details</summary>
Motivation: 研究人脑为何能够进行符号性思维而大多数动物却无法做到，探索符号性思维产生的最低神经需求及是否可用于解释生物与人工智能的区别。

Method: 对17个认知架构模拟任务进行系统性测试，分析具有或不具有多位置谓词及结构映射能力的架构在执行不同任务中的表现，以验证假设。

Result: 结果表明，动态绑定、多位置谓词和结构映射是支持符号思维的最低需求，同时揭示了生物智能与现代机器学习之间的核心区别。

Conclusion: 研究深化了对人脑符号思维演化的理解，并提供了对类脑人工智能设计的启示。

Abstract: What is it about human brains that allows us to reason symbolically whereas
most other animals cannot? There is evidence that dynamic binding, the ability
to combine neurons into groups on the fly, is necessary for symbolic thought,
but there is also evidence that it is not sufficient. We propose that two kinds
of hierarchical integration (integration of multiple role-bindings into
multiplace predicates, and integration of multiple correspondences into
structure mappings) are minimal requirements, on top of basic dynamic binding,
to realize symbolic thought. We tested this hypothesis in a systematic
collection of 17 simulations that explored the ability of cognitive
architectures with and without the capacity for multi-place predicates and
structure mapping to perform various kinds of tasks. The simulations were as
generic as possible, in that no task could be performed based on any diagnostic
features, depending instead on the capacity for multi-place predicates and
structure mapping. The results are consistent with the hypothesis that, along
with dynamic binding, multi-place predicates and structure mapping are minimal
requirements for basic symbolic thought. These results inform our understanding
of how human brains give rise to symbolic thought and speak to the differences
between biological intelligence, which tends to generalize broadly from very
few training examples, and modern approaches to machine learning, which
typically require millions or billions of training examples. The results we
report also have important implications for bio-inspired artificial
intelligence.

</details>
