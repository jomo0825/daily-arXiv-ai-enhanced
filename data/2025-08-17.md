<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 108]
- [cs.CL](#cs.CL) [Total: 73]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.LG](#cs.LG) [Total: 75]
- [cs.NE](#cs.NE) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Stochastic-based Patch Filtering for Few-Shot Learning](https://arxiv.org/abs/2508.10066)
*Javier Rodenas,Eduardo Aguilar,Petia Radeva*

Main category: cs.CV

TL;DR: 提出了一種名為SPFF的方法，通過隨機過濾patch嵌入來提升少樣本學習在食品圖像分類中的表現，並在多個基準數據集上超越了現有方法。


<details>
  <summary>Details</summary>
Motivation: 處理食品圖像在少樣本學習中的分類挑戰，主要問題在於食品圖像的視覺複雜性和多樣性，容易導致關鍵元素被忽略進而造成錯誤分類。

Method: 設計了一種隨機基於patch的過濾方法（SPFF），具體是通過隨機過濾掉那些與類別嵌入相似度較低的patch嵌入，建立query與支持圖像的相似度矩陣，從而聚焦於更具類別代表性的patch。

Result: 通過在Food-101、VireoFood-172和UECFood-256數據集上的實驗表明，SPFF在少樣本分類中相較於現有方法取得了更好的性能表現。

Conclusion: SPFF可以有效聚焦於類別特定的食品特徵，過濾掉無關patch，在提升少樣本學習的分類性能方面具有顯著優勢。

Abstract: Food images present unique challenges for few-shot learning models due to
their visual complexity and variability. For instance, a pasta dish might
appear with various garnishes on different plates and in diverse lighting
conditions and camera perspectives. This problem leads to losing focus on the
most important elements when comparing the query with support images, resulting
in misclassification. To address this issue, we propose Stochastic-based Patch
Filtering for Few-Shot Learning (SPFF) to attend to the patch embeddings that
show greater correlation with the class representation. The key concept of SPFF
involves the stochastic filtering of patch embeddings, where patches less
similar to the class-aware embedding are more likely to be discarded. With
patch embedding filtered according to the probability of appearance, we use a
similarity matrix that quantifies the relationship between the query image and
its respective support images. Through a qualitative analysis, we demonstrate
that SPFF effectively focuses on patches where class-specific food features are
most prominent while successfully filtering out non-relevant patches. We
validate our approach through extensive experiments on few-shot classification
benchmarks: Food-101, VireoFood-172 and UECFood-256, outperforming the existing
SoA methods.

</details>


### [2] [DINOv3](https://arxiv.org/abs/2508.10104)
*Oriane Siméoni,Huy V. Vo,Maximilian Seitzer,Federico Baldassarre,Maxime Oquab,Cijo Jose,Vasil Khalidov,Marc Szafraniec,Seungeun Yi,Michaël Ramamonjisoa,Francisco Massa,Daniel Haziza,Luca Wehrstedt,Jianyuan Wang,Timothée Darcet,Théo Moutakanni,Leonel Sentana,Claire Roberts,Andrea Vedaldi,Jamie Tolan,John Brandt,Camille Couprie,Julien Mairal,Hervé Jégou,Patrick Labatut,Piotr Bojanowski*

Main category: cs.CV

TL;DR: 本文介绍了DINOv3，自监督学习领域的重要进展，通过多种策略大幅提升视觉任务表现，构建通用视觉基础模型。


<details>
  <summary>Details</summary>
Motivation: 自监督学习旨在消除手动数据标注的需求，并实现模型对大规模数据集和更大架构的扩展能力。本文希望基于单一算法，从多种数据源中学习视觉表征。

Method: 通过精心的数据准备、设计与优化，扩大数据集与模型规模；引入Gram anchoring方法解决长时间训练中特征降解问题；采用后期调整策略提高模型在分辨率、模型大小和文本对齐上的灵活性。

Result: DINOv3无需微调即可超越许多特定任务的表现标准，表现出色的高质量稠密特征在不同视觉任务中显著优于先前的自监督和弱监督模型。

Conclusion: DINOv3展示了自监督学习实现视觉任务通用模型的潜力，提供多种模型以满足不同资源限制和部署场景的需求。

Abstract: Self-supervised learning holds the promise of eliminating the need for manual
data annotation, enabling models to scale effortlessly to massive datasets and
larger architectures. By not being tailored to specific tasks or domains, this
training paradigm has the potential to learn visual representations from
diverse sources, ranging from natural to aerial images -- using a single
algorithm. This technical report introduces DINOv3, a major milestone toward
realizing this vision by leveraging simple yet effective strategies. First, we
leverage the benefit of scaling both dataset and model size by careful data
preparation, design, and optimization. Second, we introduce a new method called
Gram anchoring, which effectively addresses the known yet unsolved issue of
dense feature maps degrading during long training schedules. Finally, we apply
post-hoc strategies that further enhance our models' flexibility with respect
to resolution, model size, and alignment with text. As a result, we present a
versatile vision foundation model that outperforms the specialized state of the
art across a broad range of settings, without fine-tuning. DINOv3 produces
high-quality dense features that achieve outstanding performance on various
vision tasks, significantly surpassing previous self- and weakly-supervised
foundation models. We also share the DINOv3 suite of vision models, designed to
advance the state of the art on a wide spectrum of tasks and data by providing
scalable solutions for diverse resource constraints and deployment scenarios.

</details>


### [3] [Empowering Morphing Attack Detection using Interpretable Image-Text Foundation Model](https://arxiv.org/abs/2508.10110)
*Sushrut Patwardhan,Raghavendra Ramachandra,Sushma Venkatesh*

Main category: cs.CV

TL;DR: 本文提出一种针对人脸识别系统的多模态学习方法，用于检测变脸攻击，结合了对比语言-图像预训练（CLIP）进行零样本预测。


<details>
  <summary>Details</summary>
Motivation: 提升人脸识别系统的可靠性，应对变脸攻击检测问题。

Method: 利用结合CLIP的多模态学习框架，通过分析短文本和长文本提示，对人类可理解的文本描述和变脸攻击检测进行预测。

Result: 实验分析了十种不同文本提示模式，并在多个公开人脸变脸数据集和预训练神经网络上验证了其零样本评估效果。

Conclusion: 多模态学习能够实现可靠、通用的变脸攻击检测，并生成相应的文本描述。

Abstract: Morphing attack detection has become an essential component of face
recognition systems for ensuring a reliable verification scenario. In this
paper, we present a multimodal learning approach that can provide a textual
description of morphing attack detection. We first show that zero-shot
evaluation of the proposed framework using Contrastive Language-Image
Pretraining (CLIP) can yield not only generalizable morphing attack detection,
but also predict the most relevant text snippet. We present an extensive
analysis of ten different textual prompts that include both short and long
textual prompts. These prompts are engineered by considering the human
understandable textual snippet. Extensive experiments were performed on a face
morphing dataset that was developed using a publicly available face biometric
dataset. We present an evaluation of SOTA pre-trained neural networks together
with the proposed framework in the zero-shot evaluation of five different
morphing generation techniques that are captured in three different mediums.

</details>


### [4] [Interpretable Oracle Bone Script Decipherment through Radical and Pictographic Analysis with LVLMs](https://arxiv.org/abs/2508.10113)
*Kaixin Peng,Mengyang Zhao,Haiyang Yu,Teng Fu,Bin Li*

Main category: cs.CV

TL;DR: 本文提出基于大规模视觉语言模型的甲骨文解读方法，与现有方法不同，强调字形与语义之间的联系，并通过渐进训练策略和双匹配机制显著提升零样本解读表现。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习解读甲骨文效果受限，原因在于忽略了字形与语义的复杂联系，尤其在零样本和未解读甲骨文场景中表现较差。

Method: 提出了渐进训练策略（从部首识别到象形分析再到互相分析）和部首-象形双匹配机制，同时构建了包含47,157个汉字的图像和象形文本分析的甲骨文数据集。

Result: 在公开基准数据上实现了最先进的Top-10准确率，零样本解读能力卓越，并能提供逻辑分析过程，为未解读甲骨文提供考古学参考价值。

Conclusion: 该方法在数字人文和历史研究领域具有潜在应用，相关数据和代码将在GitHub平台公开。

Abstract: As the oldest mature writing system, Oracle Bone Script (OBS) has long posed
significant challenges for archaeological decipherment due to its rarity,
abstractness, and pictographic diversity. Current deep learning-based methods
have made exciting progress on the OBS decipherment task, but existing
approaches often ignore the intricate connections between glyphs and the
semantics of OBS. This results in limited generalization and interpretability,
especially when addressing zero-shot settings and undeciphered OBS. To this
end, we propose an interpretable OBS decipherment method based on Large
Vision-Language Models, which synergistically combines radical analysis and
pictograph-semantic understanding to bridge the gap between glyphs and meanings
of OBS. Specifically, we propose a progressive training strategy that guides
the model from radical recognition and analysis to pictographic analysis and
mutual analysis, thus enabling reasoning from glyph to meaning. We also design
a Radical-Pictographic Dual Matching mechanism informed by the analysis
results, significantly enhancing the model's zero-shot decipherment
performance. To facilitate model training, we propose the Pictographic
Decipherment OBS Dataset, which comprises 47,157 Chinese characters annotated
with OBS images and pictographic analysis texts. Experimental results on public
benchmarks demonstrate that our approach achieves state-of-the-art Top-10
accuracy and superior zero-shot decipherment capabilities. More importantly,
our model delivers logical analysis processes, possibly providing
archaeologically valuable reference results for undeciphered OBS, and thus has
potential applications in digital humanities and historical research. The
dataset and code will be released in https://github.com/PKXX1943/PD-OBS.

</details>


### [5] [Deep Learning Enables Large-Scale Shape and Appearance Modeling in Total-Body DXA Imaging](https://arxiv.org/abs/2508.10132)
*Arianna Bunnell,Devon Cataldi,Yannik Glaser,Thomas K. Wolfgruber,Steven Heymsfield,Alan B. Zonderman,Thomas L. Kelly,Peter Sadowski,John A. Shepherd*

Main category: cs.CV

TL;DR: 本文开发并验证了一种深度学习方法，用于自动在总身DXA扫描图上进行标志点定位，该方法在外部测试数据集上达到了99.5%的准确率。


<details>
  <summary>Details</summary>
Motivation: 当前全身双能X射线吸收测量技术是一种经济的体成分评估工具，但其标志点的手动标注耗时且容易出错，因此需要一种自动化的方法。

Method: 通过利用1,683个手动标注的扫描图，开发了一种深度学习模型以实现自动标志点定位，并对35,928张扫描图上的关键点进行了分析。同时通过与健康标志物的关联测试提升了该技术的应用价值。

Result: 方法在外部测试数据集上的标志点定位准确率达到99.5%，并验证了形状和外观建模技术（SAM）对健康标志的相关性研究的实用性，发现与健康相关的新假设。

Conclusion: 自动化标注技术能有效提升全身DXA扫描的处理效率和精度，并在健康标志物研究中展示了广泛的应用潜力。

Abstract: Total-body dual X-ray absorptiometry (TBDXA) imaging is a relatively low-cost
whole-body imaging modality, widely used for body composition assessment. We
develop and validate a deep learning method for automatic fiducial point
placement on TBDXA scans using 1,683 manually-annotated TBDXA scans. The method
achieves 99.5% percentage correct keypoints in an external testing dataset. To
demonstrate the value for shape and appearance modeling (SAM), our method is
used to place keypoints on 35,928 scans for five different TBDXA imaging modes,
then associations with health markers are tested in two cohorts not used for
SAM model generation using two-sample Kolmogorov-Smirnov tests. SAM feature
distributions associated with health biomarkers are shown to corroborate
existing evidence and generate new hypotheses on body composition and shape's
relationship to various frailty, metabolic, inflammation, and cardiometabolic
health markers. Evaluation scripts, model weights, automatic point file
generation code, and triangulation files are available at
https://github.com/hawaii-ai/dxa-pointplacement.

</details>


### [6] [MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion Learning](https://arxiv.org/abs/2508.10133)
*Thanh-Dat Truong,Christophe Bobda,Nitin Agarwal,Khoa Luu*

Main category: cs.CV

TL;DR: 提出了一种新的多模态注意力正则流(MANGO)方法，用于显式、可解释和可扩展的多模态融合学习。


<details>
  <summary>Details</summary>
Motivation: 目前的多模态融合方法使用Transformer的注意力机制隐式学习多模态特征的相关性，难以捕捉各模态的本质特性以及复杂的多模态结构和关系。

Method: 提出了一种可逆交叉注意力(ICA)层，并基于此设计了新型正则流模型。引入了三种新的交叉注意力机制：模态到模态交叉注意力(MMCA)、模态间交叉注意力(IMCA)、可学习模态间交叉注意力(LICA)。

Result: 在语义分割、图像到图像翻译及电影类型分类等三个任务中取得了最新的SoTA表现。

Conclusion: MANGO方法在多模态学习中表现出优越的性能，同时提升了方法的可解释性与扩展性。

Abstract: Multimodal learning has gained much success in recent years. However, current
multimodal fusion methods adopt the attention mechanism of Transformers to
implicitly learn the underlying correlation of multimodal features. As a
result, the multimodal model cannot capture the essential features of each
modality, making it difficult to comprehend complex structures and correlations
of multimodal inputs. This paper introduces a novel Multimodal Attention-based
Normalizing Flow (MANGO) approach\footnote{The source code of this work will be
publicly available.} to developing explicit, interpretable, and tractable
multimodal fusion learning. In particular, we propose a new Invertible
Cross-Attention (ICA) layer to develop the Normalizing Flow-based Model for
multimodal data. To efficiently capture the complex, underlying correlations in
multimodal data in our proposed invertible cross-attention layer, we propose
three new cross-attention mechanisms: Modality-to-Modality Cross-Attention
(MMCA), Inter-Modality Cross-Attention (IMCA), and Learnable Inter-Modality
Cross-Attention (LICA). Finally, we introduce a new Multimodal Attention-based
Normalizing Flow to enable the scalability of our proposed method to
high-dimensional multimodal data. Our experimental results on three different
multimodal learning tasks, i.e., semantic segmentation, image-to-image
translation, and movie genre classification, have illustrated the
state-of-the-art (SoTA) performance of the proposed approach.

</details>


### [7] [Improving watermelon (Citrullus lanatus) disease classification with generative artificial intelligence (GenAI)-based synthetic and real-field images via a custom EfficientNetV2-L model](https://arxiv.org/abs/2508.10156)
*Nitin Rai,Nathan S. Boyd,Gary E. Vallad,Arnold W. Schumann*

Main category: cs.CV

TL;DR: 本研究探讨通过结合少量真实图像与大规模合成图像，提高西瓜病害分类模型性能的方法，结果显示混合训练方式显著提升了模型的精准度与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过生成式AI模型生成的合成图像，结合少量真实图像，提高作物病害分类的性能和减少实地数据采集的依赖。

Method: 采用EfficientNetV2-L模型，测试了五种数据集处理方式（仅真实图像、仅合成图像、不同比例真实与合成图像混合），并利用增强微调与迁移学习技术优化模型训练。

Result: 在结合数据集（尤其1:10真实与合成图像比例，H3-H4）情况下，模型取得最高加权F1得分1.00，相较仅使用真实数据组（H0, F1: 0.65）实现了显著提升。

Conclusion: 合成图像不能完全替代真实图像，而需采用混合数据集以最大化作物病害分类模型性能与泛化能力。

Abstract: The current advancements in generative artificial intelligence (GenAI) models
have paved the way for new possibilities for generating high-resolution
synthetic images, thereby offering a promising alternative to traditional image
acquisition for training computer vision models in agriculture. In the context
of crop disease diagnosis, GenAI models are being used to create synthetic
images of various diseases, potentially facilitating model creation and
reducing the dependency on resource-intensive in-field data collection.
However, limited research has been conducted on evaluating the effectiveness of
integrating real with synthetic images to improve disease classification
performance. Therefore, this study aims to investigate whether combining a
limited number of real images with synthetic images can enhance the prediction
accuracy of an EfficientNetV2-L model for classifying watermelon
\textit{(Citrullus lanatus)} diseases. The training dataset was divided into
five treatments: H0 (only real images), H1 (only synthetic images), H2 (1:1
real-to-synthetic), H3 (1:10 real-to-synthetic), and H4 (H3 + random images to
improve variability and model generalization). All treatments were trained
using a custom EfficientNetV2-L architecture with enhanced fine-tuning and
transfer learning techniques. Models trained on H2, H3, and H4 treatments
demonstrated high precision, recall, and F1-score metrics. Additionally, the
weighted F1-score increased from 0.65 (on H0) to 1.00 (on H3-H4) signifying
that the addition of a small number of real images with a considerable volume
of synthetic images improved model performance and generalizability. Overall,
this validates the findings that synthetic images alone cannot adequately
substitute for real images; instead, both must be used in a hybrid manner to
maximize model performance for crop disease classification.

</details>


### [8] [SynSpill: Improved Industrial Spill Detection With Synthetic Data](https://arxiv.org/abs/2508.10171)
*Aaditya Baranwal,Abdul Mueez,Jason Voelker,Guneet Bhatia,Shruti Vyas*

Main category: cs.CV

TL;DR: 提出基于高质量合成数据生成的框架，有效提升了视觉-语言模型（VLMs）和主流检测器在细分领域的表现。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型在通用视觉识别上具有强大的零样本能力，但在工业泄漏检测等小众且高风险领域表现较差，主要受限于数据稀缺性。

Method: 开发了一种以高质量合成数据生成管道为核心的框架，通过合成数据支持参数高效微调（PEFT）提升模型性能，并验证其对YOLO和DETR等主流检测器的提升效果。

Result: 在没有合成数据的条件下，VLMs依然比常规检测器在新场景中表现更好；使用合成数据后，VLMs和检测器的表现都有显著提升，且差距缩小。

Conclusion: 高保真合成数据能够有效弥补安全关键领域的数据差距，合成数据和轻量化适应方法则为数据稀缺的工业环境提供了可扩展的解决方案。

Abstract: Large-scale Vision-Language Models (VLMs) have transformed general-purpose
visual recognition through strong zero-shot capabilities. However, their
performance degrades significantly in niche, safety-critical domains such as
industrial spill detection, where hazardous events are rare, sensitive, and
difficult to annotate. This scarcity -- driven by privacy concerns, data
sensitivity, and the infrequency of real incidents -- renders conventional
fine-tuning of detectors infeasible for most industrial settings.
  We address this challenge by introducing a scalable framework centered on a
high-quality synthetic data generation pipeline. We demonstrate that this
synthetic corpus enables effective Parameter-Efficient Fine-Tuning (PEFT) of
VLMs and substantially boosts the performance of state-of-the-art object
detectors such as YOLO and DETR. Notably, in the absence of synthetic data
(SynSpill dataset), VLMs still generalize better to unseen spill scenarios than
these detectors. When SynSpill is used, both VLMs and detectors achieve marked
improvements, with their performance becoming comparable.
  Our results underscore that high-fidelity synthetic data is a powerful means
to bridge the domain gap in safety-critical applications. The combination of
synthetic generation and lightweight adaptation offers a cost-effective,
scalable pathway for deploying vision systems in industrial environments where
real data is scarce/impractical to obtain.
  Project Page: https://synspill.vercel.app

</details>


### [9] [EntropyGS: An Efficient Entropy Coding on 3D Gaussian Splatting](https://arxiv.org/abs/2508.10227)
*Yuning Huang,Jiahao Pang,Fengqing Zhu,Dong Tian*

Main category: cs.CV

TL;DR: 这篇文章提出了一种名为EntropyGS的编码方法，大幅减少3DGS数据存储速率，同时保持渲染质量。


<details>
  <summary>Details</summary>
Motivation: 随着3D高斯分布技术的广泛应用，其数据的存储和压缩需求显著增加，因此开发高效的编码方法成为迫切任务。

Method: 通过对3DGS高斯属性进行相关性和统计分析，提出一种因子化和参数化熵编码方法EntropyGS，结合归一化分布参数和自适应量化方式进行高效编码。

Result: EntropyGS在基准数据集上实现了约30倍的速率降低，同时保持了与原始数据相似的渲染质量并具有快速的编码和解码效率。

Conclusion: EntropyGS方法通过有效压缩3DGS数据，提升了存储和传输效率，为3DGS技术的实际应用提供了可行的解决方案。

Abstract: As an emerging novel view synthesis approach, 3D Gaussian Splatting (3DGS)
demonstrates fast training/rendering with superior visual quality. The two
tasks of 3DGS, Gaussian creation and view rendering, are typically separated
over time or devices, and thus storage/transmission and finally compression of
3DGS Gaussians become necessary. We begin with a correlation and statistical
analysis of 3DGS Gaussian attributes. An inspiring finding in this work reveals
that spherical harmonic AC attributes precisely follow Laplace distributions,
while mixtures of Gaussian distributions can approximate rotation, scaling, and
opacity. Additionally, harmonic AC attributes manifest weak correlations with
other attributes except for inherited correlations from a color space. A
factorized and parameterized entropy coding method, EntropyGS, is hereinafter
proposed. During encoding, distribution parameters of each Gaussian attribute
are estimated to assist their entropy coding. The quantization for entropy
coding is adaptively performed according to Gaussian attribute types. EntropyGS
demonstrates about 30x rate reduction on benchmark datasets while maintaining
similar rendering quality compared to input 3DGS data, with a fast encoding and
decoding time.

</details>


### [10] [CellSymphony: Deciphering the molecular and phenotypic orchestration of cells with single-cell pathomics](https://arxiv.org/abs/2508.10232)
*Paul H. Acosta,Pingjun Chen,Simon P. Castillo,Maria Esther Salvatierra,Yinyin Yuan,Xiaoxi Pan*

Main category: cs.CV

TL;DR: 本文提出了一种称为CellSymphony的多模态框架，利用Xenium平台下的转录组数据和组织学图像的单细胞信息进行联合分析，实现更准确的细胞类型注解和肿瘤组织微环境研究。


<details>
  <summary>Details</summary>
Motivation: 当前存在单细胞水平提取组织学和空间转录组数据并进行深度结合分析的技术挑战，亟需开发灵活有效的解决方案。

Method: 引入CellSymphony框架，将来自Xenium平台和组织学图像的基础模型嵌入相结合，利用空间基因表达及形态学上下文进行联合建模分析。

Result: CellSymphony成功实现准确的细胞类型注解，揭示三种癌症类型中独特的微环境结构和功能特征。

Conclusion: 结合基础模型和多模态数据的框架在细胞水平解析复杂组织生态系统中具有重要潜力，可以更好理解细胞的生理和表型协作关系。

Abstract: Xenium, a new spatial transcriptomics platform, enables
subcellular-resolution profiling of complex tumor tissues. Despite the rich
morphological information in histology images, extracting robust cell-level
features and integrating them with spatial transcriptomics data remains a
critical challenge. We introduce CellSymphony, a flexible multimodal framework
that leverages foundation model-derived embeddings from both Xenium
transcriptomic profiles and histology images at true single-cell resolution. By
learning joint representations that fuse spatial gene expression with
morphological context, CellSymphony achieves accurate cell type annotation and
uncovers distinct microenvironmental niches across three cancer types. This
work highlights the potential of foundation models and multimodal fusion for
deciphering the physiological and phenotypic orchestration of cells within
complex tissue ecosystems.

</details>


### [11] [Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets](https://arxiv.org/abs/2508.10256)
*Xinan Zhang,Haolin Wang,Yung-An Hsieh,Zhongyu Yang,Anthony Yezzi,Yi-Chang Tsai*

Main category: cs.CV

TL;DR: 本文综述了基于深度学习的裂缝检测领域的新兴趋势，提供新数据集3DCrack并进行基准测试，推动该领域未来研究。


<details>
  <summary>Details</summary>
Motivation: 裂缝检测对于土木基础设施（如路面和建筑物检查）至关重要，而深度学习近年来极大地推动了该领域的发展。新兴趋势正在改变此领域的发展方式，因此需要对这些趋势系统分析，总结现状并指导未来研究。

Method: 本文通过系统分析近年来的关键趋势，包括学习范式转型（如从全监督转向半监督、无监督等）、泛化能力增强（如跨数据集评估）和数据集多样化（从RGB图像到专用传感器数据）。并引入3D激光扫描数据集3DCrack，开展深度学习模型基准实验。

Result: 文中提出了一个名为3DCrack的新数据集，并使用多种深度学习方法（包括新兴的基础模型）进行了基准测试，建立了基准实验的基线。

Conclusion: 该研究提供了基于深度学习裂缝检测的新兴趋势的深刻见解，新数据集3DCrack为后续研究提供支持，同时设立了数据集和方法的基准，为领域发展和未来研究指明方向。

Abstract: Crack detection plays a crucial role in civil infrastructures, including
inspection of pavements, buildings, etc., and deep learning has significantly
advanced this field in recent years. While numerous technical and review papers
exist in this domain, emerging trends are reshaping the landscape. These shifts
include transitions in learning paradigms (from fully supervised learning to
semi-supervised, weakly-supervised, unsupervised, few-shot, domain adaptation
and fine-tuning foundation models), improvements in generalizability (from
single-dataset performance to cross-dataset evaluation), and diversification in
dataset reacquisition (from RGB images to specialized sensor-based data). In
this review, we systematically analyze these trends and highlight
representative works. Additionally, we introduce a new dataset collected with
3D laser scans, 3DCrack, to support future research and conduct extensive
benchmarking experiments to establish baselines for commonly used deep learning
methodologies, including recent foundation models. Our findings provide
insights into the evolving methodologies and future directions in deep
learning-based crack detection. Project page:
https://github.com/nantonzhang/Awesome-Crack-Detection

</details>


### [12] [MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs](https://arxiv.org/abs/2508.10264)
*Haonan Ge,Yiwei Wang,Ming-Hsuan Yang,Yujun Cai*

Main category: cs.CV

TL;DR: 该研究提出了一种名为多区域融合解码（MRFD）的方法，通过建模不同区域之间的一致性来减少大规模视觉语言模型（LVLMs）生成的幻觉，并提高生成文本的事实性。


<details>
  <summary>Details</summary>
Motivation: LVLMs在多模态任务中表现出色，但由于缺乏验证图像中不同区域信息一致性的能力，容易产生与视觉输入不一致的幻觉。

Method: 提出了一种无训练的解码方法MRFD，通过基于交叉注意力机制确定显著区域，为每个区域生成初始响应，利用Jensen-Shannon Divergence计算可靠性权重，再结合区域感知提示与Chain-of-Thought推理进行一致性融合。

Result: 实验表明，MRFD能在多个LVLMs和基准测试中显著减少幻觉，提高响应的事实准确性，无需对模型进行更新。

Conclusion: MRFD为提升LVLMs事实性与减少幻觉提供了一种有效且无需训练的解码方法，显示出其实用性。

Abstract: Large Vision-Language Models (LVLMs) have shown strong performance across
multimodal tasks. However, they often produce hallucinations -- text that is
inconsistent with visual input, due to the limited ability to verify
information in different regions of the image. To address this, we propose
Multi-Region Fusion Decoding (MRFD), a training-free decoding method that
improves factual grounding by modeling inter-region consistency. MRFD
identifies salient regions using cross-attention, generates initial responses
for each, and computes reliability weights based on Jensen-Shannon Divergence
(JSD) among the responses. These weights guide a consistency-aware fusion of
per-region predictions, using region-aware prompts inspired by Chain-of-Thought
reasoning. Experiments across multiple LVLMs and benchmarks show that MRFD
significantly reduces hallucinations and improves response factuality without
requiring model updates.

</details>


### [13] [Pose-Robust Calibration Strategy for Point-of-Gaze Estimation on Mobile Phones](https://arxiv.org/abs/2508.10268)
*Yujie Zhao,Jiabei Zeng,Shiguang Shan*

Main category: cs.CV

TL;DR: 这篇论文介绍了如何通过改进校准策略解决凝视点估计在头部姿态变化下的鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 尽管基于外观的凝视点估计取得了进步，但由于个体差异，模型仍难以在不同个体间泛化。因此，需要个性化校准以提高精度，同时克服对头部姿态变化的敏感性。

Method: 构建一个名为MobilePoG的基准数据集，包含32个人在固定或持续改变头部姿态下注视点的面部图像，并系统分析校准点和头部姿态多样性对估计精度的影响。提出一种动态校准策略，使用户在移动手机时注视校准点，自然引入头部姿态变化，实现高效且用户友好的校准过程。

Result: 实验表明，引入更广泛的头部姿态多样性显著提高了估计器处理姿态变化的能力。动态校准策略相比传统方法生成的估计器对头部姿态变化的敏感性更低。

Conclusion: 研究总结了校准策略中头部姿态多样性的重要性，并提出了一种能提升估计器鲁棒性的用户友好型动态校准方法。

Abstract: Although appearance-based point-of-gaze (PoG) estimation has improved, the
estimators still struggle to generalize across individuals due to personal
differences. Therefore, person-specific calibration is required for accurate
PoG estimation. However, calibrated PoG estimators are often sensitive to head
pose variations. To address this, we investigate the key factors influencing
calibrated estimators and explore pose-robust calibration strategies.
Specifically, we first construct a benchmark, MobilePoG, which includes facial
images from 32 individuals focusing on designated points under either fixed or
continuously changing head poses. Using this benchmark, we systematically
analyze how the diversity of calibration points and head poses influences
estimation accuracy. Our experiments show that introducing a wider range of
head poses during calibration improves the estimator's ability to handle pose
variation. Building on this insight, we propose a dynamic calibration strategy
in which users fixate on calibration points while moving their phones. This
strategy naturally introduces head pose variation during a user-friendly and
efficient calibration process, ultimately producing a better calibrated PoG
estimator that is less sensitive to head pose variations than those using
conventional calibration strategies. Codes and datasets are available at our
project page.

</details>


### [14] [High Fidelity Text to Image Generation with Contrastive Alignment and Structural Guidance](https://arxiv.org/abs/2508.10280)
*Danyi Gao*

Main category: cs.CV

TL;DR: 本文提出了一种高保真图像生成方法，通过整合文本-图像对比约束和结构引导机制，解决语义对齐精度和结构一致性问题，显著提升COCO-2014数据集上的生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动图像生成方法在语义对齐精度和结构一致性方面存在性能瓶颈。

Method: 提出一种结合对比学习模块与结构先验（如语义布局图或边缘草图）的图像生成方法，并采用多目标监督机制优化模型性能。

Result: 通过系统实验证明，该方法在CLIP Score、FID和SSIM等评测指标上表现优越，并对语义、文本长度及结构引导强度等进行了敏感性分析。

Conclusion: 该方法有效弥合了语义对齐与结构保真之间的差距，无明显增加计算复杂性，展现出生成语义清晰、结构完整图像的强大能力。

Abstract: This paper addresses the performance bottlenecks of existing text-driven
image generation methods in terms of semantic alignment accuracy and structural
consistency. A high-fidelity image generation method is proposed by integrating
text-image contrastive constraints with structural guidance mechanisms. The
approach introduces a contrastive learning module that builds strong
cross-modal alignment constraints to improve semantic matching between text and
image. At the same time, structural priors such as semantic layout maps or edge
sketches are used to guide the generator in spatial-level structural modeling.
This enhances the layout completeness and detail fidelity of the generated
images. Within the overall framework, the model jointly optimizes contrastive
loss, structural consistency loss, and semantic preservation loss. A
multi-objective supervision mechanism is adopted to improve the semantic
consistency and controllability of the generated content. Systematic
experiments are conducted on the COCO-2014 dataset. Sensitivity analyses are
performed on embedding dimensions, text length, and structural guidance
strength. Quantitative metrics confirm the superior performance of the proposed
method in terms of CLIP Score, FID, and SSIM. The results show that the method
effectively bridges the gap between semantic alignment and structural fidelity
without increasing computational complexity. It demonstrates a strong ability
to generate semantically clear and structurally complete images, offering a
viable technical path for joint text-image modeling and image generation.

</details>


### [15] [VIFSS: View-Invariant and Figure Skating-Specific Pose Representation Learning for Temporal Action Segmentation](https://arxiv.org/abs/2508.10281)
*Ryota Tanaka,Tomohiro Suzuki,Keisuke Fujii*

Main category: cs.CV

TL;DR: 该论文提出了一种针对花样滑冰跳跃动作识别的新框架，通过利用3D姿态数据、对比学习和细粒度的注释，实现高效的时间动作分割。


<details>
  <summary>Details</summary>
Motivation: 由于花样滑冰跳跃动作精细复杂，且数据标签稀缺，传统方法难以精准识别跳跃类型和时序节点，因此需要开发新方法来克服这些局限性。

Method: 作者提出了VIFSS方法，结合对比学习和动作分类，利用新构建的FS-Jump3D数据集进行视角不变的预训练，同时设计细粒度注释，标记跳跃的"准备"和"落地"阶段。

Result: 实验证明，该框架在元素级别的时间动作分割上达到超过92%的F1@50，尤其在使用少量训练数据时，对比预训练效果显著。

Conclusion: 该方法在解决数据稀缺和复杂三维动作表征问题上表现出优越性，为实际应用场景中的跳跃动作识别提供了有效解决方案。

Abstract: Understanding human actions from videos plays a critical role across various
domains, including sports analytics. In figure skating, accurately recognizing
the type and timing of jumps a skater performs is essential for objective
performance evaluation. However, this task typically requires expert-level
knowledge due to the fine-grained and complex nature of jump procedures. While
recent approaches have attempted to automate this task using Temporal Action
Segmentation (TAS), there are two major limitations to TAS for figure skating:
the annotated data is insufficient, and existing methods do not account for the
inherent three-dimensional aspects and procedural structure of jump actions. In
this work, we propose a new TAS framework for figure skating jumps that
explicitly incorporates both the three-dimensional nature and the semantic
procedure of jump movements. First, we propose a novel View-Invariant, Figure
Skating-Specific pose representation learning approach (VIFSS) that combines
contrastive learning as pre-training and action classification as fine-tuning.
For view-invariant contrastive pre-training, we construct FS-Jump3D, the first
publicly available 3D pose dataset specialized for figure skating jumps.
Second, we introduce a fine-grained annotation scheme that marks the ``entry
(preparation)'' and ``landing'' phases, enabling TAS models to learn the
procedural structure of jumps. Extensive experiments demonstrate the
effectiveness of our framework. Our method achieves over 92% F1@50 on
element-level TAS, which requires recognizing both jump types and rotation
levels. Furthermore, we show that view-invariant contrastive pre-training is
particularly effective when fine-tuning data is limited, highlighting the
practicality of our approach in real-world scenarios.

</details>


### [16] [JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics](https://arxiv.org/abs/2508.10287)
*Simindokht Jahangard,Mehrzad Mohammadi,Yi Shen,Zhixi Cai,Hamid Rezatofighi*

Main category: cs.CV

TL;DR: 本文讨论了视觉-语言模型(VLMs)和大语言模型(LLMs)在视觉推理中的应用，并提出了一个新的基准JRDB-Reasoning，旨在解决现有视觉推理基准的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉推理基准在定义推理复杂性、任务定制及中间推理步骤注释等方面存在不足，限制了复杂推理能力的评估。

Method: 通过形式化推理复杂性，设计一个自适应查询引擎以生成具有不同复杂性的问题和详细的中间注释，并结合扩展的JRDB数据集创建了新的评估基准JRDB-Reasoning。

Result: 新的评估工具和基准为细化视觉推理框架的评估，特别是在人群密集环境中的动态评估，提供了更有力的支持。

Conclusion: JRDB-Reasoning基准弥补了现有基准的多重局限，为视觉-语言模型在不同推理水平上的能力评估提供了新的途径。

Abstract: Recent advances in Vision-Language Models (VLMs) and large language models
(LLMs) have greatly enhanced visual reasoning, a key capability for embodied AI
agents like robots. However, existing visual reasoning benchmarks often suffer
from several limitations: they lack a clear definition of reasoning complexity,
offer have no control to generate questions over varying difficulty and task
customization, and fail to provide structured, step-by-step reasoning
annotations (workflows). To bridge these gaps, we formalize reasoning
complexity, introduce an adaptive query engine that generates customizable
questions of varying complexity with detailed intermediate annotations, and
extend the JRDB dataset with human-object interaction and geometric
relationship annotations to create JRDB-Reasoning, a benchmark tailored for
visual reasoning in human-crowded environments. Our engine and benchmark enable
fine-grained evaluation of visual reasoning frameworks and dynamic assessment
of visual-language models across reasoning levels.

</details>


### [17] [A Sub-Pixel Multimodal Optical Remote Sensing Images Matching Method](https://arxiv.org/abs/2508.10294)
*Tao Huang,Hongbo Pan,Nanxi Zhou,Shun Zhou*

Main category: cs.CV

TL;DR: 提出了基于相位一致性加权最小绝对偏差的亚像素模板匹配方法 (PCWLAD)，可显著提高多模态光学图像的匹配精度。


<details>
  <summary>Details</summary>
Motivation: 解决多模态光学图像间由于光谱响应差异造成的非线性辐射和几何变形对匹配精度的影响。

Method: 方法分为结构相似性索引 (SSIM) 的粗匹配和基于WLAD的细匹配。粗匹配保持图像细节，利用SSIM进行模板匹配；细匹配应用辐射和几何变换模型，结合结构互滤波和WLAD准则进行亚像素偏移估计。

Result: 实验验证中，方法在三类数据集图像上的平均匹配精度达到0.4像素，并优于其他八种现有方法。

Conclusion: PCWLAD方法有效提高了多模态光学图像配准的正确匹配率和均方根误差，可作为提高匹配精度的有效工具。

Abstract: High-accuracy matching of multimodal optical images is the basis of geometric
processing. However, the image matching accuracy is usually degraded by the
nonlinear radiation and geometric deformation differences caused by different
spectral responses. To address these problems, we proposed a phase consistency
weighted least absolute deviation (PCWLAD) sub-pixel template matching method
to improve the matching accuracy of multimodal optical images. This method
consists of two main steps: coarse matching with the structural similarity
index measure (SSIM) and fine matching with WLAD. In the coarse matching step,
PCs are calculated without a noise filter to preserve the original structural
details, and template matching is performed using the SSIM. In the fine
matching step, we applied the radiometric and geometric transformation models
between two multimodal PC templates based on the coarse matching. Furthermore,
mutual structure filtering is adopted in the model to mitigate the impact of
noise within the corresponding templates on the structural consistency, and the
WLAD criterion is used to estimate the sub-pixel offset. To evaluate the
performance of PCWLAD, we created three types of image datasets: visible to
infrared Landsat images, visible to near-infrared close-range images, and
visible to infrared uncrewed aerial vehicle (UAV) images. PCWLAD outperformed
existing state-of-the-art eight methods in terms of correct matching rate (CMR)
and root mean square error (RMSE) and reached an average matching accuracy of
approximately 0.4 pixels across all three datasets. Our software and datasets
are publicly available at https://github.com/huangtaocsu/PCWLAD.

</details>


### [18] [InterSyn: Interleaved Learning for Dynamic Motion Synthesis in the Wild](https://arxiv.org/abs/2508.10297)
*Yiyi Ma,Yuanzhi Liang,Xiu Li,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出了Interleaved Learning for Motion Synthesis (InterSyn)，通过交替学习策略实现单人和多人物动态结合的逼真交互动作生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时协调单人和多人物动作的动态特性，无法真实模拟真实场景中的自然交互与微妙的协调。

Method: 采用交替学习策略，包括两个模块：INS模块用于统一建模单人和交互行为；REC模块用于优化动态协调和角色动作同步。

Result: 实验结果表明，InterSyn生成的运动序列在文本与动作的对齐和多样性方面优于现有方法。

Conclusion: InterSyn为逼真的交互运动生成设立了新基准，并承诺开源代码以推动该领域进一步研究与发展。

Abstract: We present Interleaved Learning for Motion Synthesis (InterSyn), a novel
framework that targets the generation of realistic interaction motions by
learning from integrated motions that consider both solo and multi-person
dynamics. Unlike previous methods that treat these components separately,
InterSyn employs an interleaved learning strategy to capture the natural,
dynamic interactions and nuanced coordination inherent in real-world scenarios.
Our framework comprises two key modules: the Interleaved Interaction Synthesis
(INS) module, which jointly models solo and interactive behaviors in a unified
paradigm from a first-person perspective to support multiple character
interactions, and the Relative Coordination Refinement (REC) module, which
refines mutual dynamics and ensures synchronized motions among characters.
Experimental results show that the motion sequences generated by InterSyn
exhibit higher text-to-motion alignment and improved diversity compared with
recent methods, setting a new benchmark for robust and natural motion
synthesis. Additionally, our code will be open-sourced in the future to promote
further research and development in this area.

</details>


### [19] [From Pixel to Mask: A Survey of Out-of-Distribution Segmentation](https://arxiv.org/abs/2508.10309)
*Wenjie Zhao,Jia Li,Yunhui Guo*

Main category: cs.CV

TL;DR: 本文探讨了异常分布（OoD）分割方法的进展，特别是针对自动驾驶场景的研究，并指出了当前存在的挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: OoD检测和分割对人工智能系统安全至关重要，尤其在自动驾驶等需要精确定位异常物体的场景，能够通过细粒度分割增强系统稳定性和安全性。

Method: 作者通过综述，将当前的OoD分割方法分为四类：（1）测试阶段的OoD分割；（2）基于监督训练的异常暴露方法；（3）基于重建的方法；（4）利用强大模型的方法，并系统性地总结了这些方法的特点和应用。

Result: 系统总结了OoD分割的几种现有方案，特别在自动驾驶场景下的表现，并揭示了当前技术限制及其潜在改进方向。

Conclusion: 本文为OoD分割的研究提供了详细的综述和分类，并强调了其在安全关键系统中的重要性，同时提出了未来值得探索的研究方向。

Abstract: Out-of-distribution (OoD) detection and segmentation have attracted growing
attention as concerns about AI security rise. Conventional OoD detection
methods identify the existence of OoD objects but lack spatial localization,
limiting their usefulness in downstream tasks. OoD segmentation addresses this
limitation by localizing anomalous objects at pixel-level granularity. This
capability is crucial for safety-critical applications such as autonomous
driving, where perception modules must not only detect but also precisely
segment OoD objects, enabling targeted control actions and enhancing overall
system robustness. In this survey, we group current OoD segmentation approaches
into four categories: (i) test-time OoD segmentation, (ii) outlier exposure for
supervised training, (iii) reconstruction-based methods, (iv) and approaches
that leverage powerful models. We systematically review recent advances in OoD
segmentation for autonomous-driving scenarios, identify emerging challenges,
and discuss promising future research directions.

</details>


### [20] [Integrating Reinforcement Learning with Visual Generative Models: Foundations and Advances](https://arxiv.org/abs/2508.10316)
*Yuanzhi Liang,Yijie Fang,Rui Li,Ziqi Ni,Ruijie Su,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 本文综述了基于强化学习（RL）的视觉内容生成方法，探讨其如何提升生成任务的可控性、一致性及与人类偏好的一致性，同时指出未来研究方向及挑战。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型通常依靠如似然估计或重构损失等代理目标，这些目标可能与感知质量、语义准确性或物理真实性不一致，因而需要研究一种更契合高层次生成目标的方法。

Method: 本文系统回顾了强化学习的演变，从经典控制到通用优化工具，并分析其在图像、视频、3D/4D生成中的应用方法，包括作为微调机制或结构模块等。

Result: 通过融合强化学习，生成模型能够在视觉内容生成中表现出更高的控制性、一致性及对高层目标的对齐能力，但仍有许多未解决的问题。

Conclusion: 利用RL优化生成任务是一项具有潜力的研究方向，但同时也面临诸多挑战，未来需要针对高复杂性目标的对齐与生成精度的平衡进行深入研究。

Abstract: Generative models have made significant progress in synthesizing visual
content, including images, videos, and 3D/4D structures. However, they are
typically trained with surrogate objectives such as likelihood or
reconstruction loss, which often misalign with perceptual quality, semantic
accuracy, or physical realism. Reinforcement learning (RL) offers a principled
framework for optimizing non-differentiable, preference-driven, and temporally
structured objectives. Recent advances demonstrate its effectiveness in
enhancing controllability, consistency, and human alignment across generative
tasks. This survey provides a systematic overview of RL-based methods for
visual content generation. We review the evolution of RL from classical control
to its role as a general-purpose optimization tool, and examine its integration
into image, video, and 3D/4D generation. Across these domains, RL serves not
only as a fine-tuning mechanism but also as a structural component for aligning
generation with complex, high-level goals. We conclude with open challenges and
future research directions at the intersection of RL and generative modeling.

</details>


### [21] [Concepts or Skills? Rethinking Instruction Selection for Multi-modal Models](https://arxiv.org/abs/2508.10339)
*Andrew Bai,Justin Cui,Ruochen Wang,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 本论文提出了一种针对视觉语言指令调整的目标训练数据选择方法，能够优化在不同基准测试上的性能表现。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉语言基准测试中主要受益于具有相似技能或视觉概念的训练指令，并设计一种数据选择方法以优化基准测试表现。

Method: 通过分离视觉概念和技能，从基准测试中提取关键特性，分析其主要需求，然后选择最匹配的指令作为训练数据。

Result: 在10余项基准测试上验证了该方法的有效性，整体效果较最佳现有基准提高0.9%，在技能焦点数据集上提高1.5%。

Conclusion: 研究表明，指令选择中需要在概念知识和视觉技能的获取之间寻找平衡。

Abstract: Vision-language instruction tuning achieves two main purposes: learning
visual concepts and learning visual skills. In this paper, we found that
vision-language benchmarks fall into the dichotomy of mainly benefiting from
training on instructions with similar skills or visual concepts. Inspired by
the discovery, we designed a simple targeted training data selection method to
optimize the performance of a given benchmark. We first extract the
concepts/skills from the benchmark, determine whether the benchmark
predominantly benefits from similar concepts or skills, and finally select
instructions with the most matching concepts/skills. Experiments on 10+
benchmarks validate the effectiveness of our targeted data selection method,
showing +0.9\% over the best existing baseline averaged over all benchmarks and
+1.5\% on the skill-focused subset. Our findings underscore the importance of
recognizing the inherent trade-off within instruction selection, which requires
balancing the acquisition of conceptual knowledge against visual skill.

</details>


### [22] [Glo-DMU: A Deep Morphometry Framework of Ultrastructural Characterization in Glomerular Electron Microscopic Images](https://arxiv.org/abs/2508.10351)
*Zhentai Zhang,Danyi Weng,Guibin Zhang,Xiang Chen,Kaixing Long,Jian Geng,Yanmeng Lu,Lei Zhang,Zhitao Zhou,Lei Cao*

Main category: cs.CV

TL;DR: 本研究提出一种肾小球形态测量框架（Glo-DMU），通过深度学习分析肾病超微结构特征，并取得了与实际病理报告描述高度一致的结果。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种可满足肾病诊断实际需求的全自动化超微结构分析工具，以克服现有研究仅局限于单一结构识别的挑战。

Method: 提出Glo-DMU框架，包括超微结构分割模型、肾小球滤过屏障区域分类模型及电子致密物检测模型，同时量化三种常用的超微结构特征：基底膜厚度、足突消失程度及电子致密物位置。

Result: 通过对115名患者（涵盖9种肾病病理类型）的分析，证明自动量化结果与病理报告描述具有良好一致性。

Conclusion: Glo-DMU实现了全自动化、高精确度和高通量的多个超微结构特征量化，为肾病理学家提供了高效的诊断辅助工具。

Abstract: Complex and diverse ultrastructural features can indicate the type,
progression, and prognosis of kidney diseases. Recently, computational
pathology combined with deep learning methods has shown tremendous potential in
advancing automatic morphological analysis of glomerular ultrastructure.
However, current research predominantly focuses on the recognition of
individual ultrastructure, which makes it challenging to meet practical
diagnostic needs. In this study, we propose the glomerular morphometry
framework of ultrastructural characterization (Glo-DMU), which is grounded on
three deep models: the ultrastructure segmentation model, the glomerular
filtration barrier region classification model, and the electron-dense deposits
detection model. Following the conventional protocol of renal biopsy diagnosis,
this framework simultaneously quantifies the three most widely used
ultrastructural features: the thickness of glomerular basement membrane, the
degree of foot process effacement, and the location of electron-dense deposits.
We evaluated the 115 patients with 9 renal pathological types in real-world
diagnostic scenarios, demonstrating good consistency between automatic
quantification results and morphological descriptions in the pathological
reports. Glo-DMU possesses the characteristics of full automation, high
precision, and high throughput, quantifying multiple ultrastructural features
simultaneously, and providing an efficient tool for assisting renal
pathologists.

</details>


### [23] [Improving OCR for Historical Texts of Multiple Languages](https://arxiv.org/abs/2508.10356)
*Hylke Westerdijk,Ben Blankenborg,Khondoker Ittehadul Islam*

Main category: cs.CV

TL;DR: 这篇论文研究了利用深度学习技术进行光学字符识别（OCR）和文档布局分析，涉及多个历史和现代文本数据集的应用。


<details>
  <summary>Details</summary>
Motivation: 将先进的深度学习方法应用于OCR和文档布局分析，以提高历史和现代文本的识别精度。

Method: 通过数据增强和深度学习模型（Kraken、TrOCR）改进死海古卷的字符识别；结合CRNN、DeepLabV3+和双向LSTM，使用置信度伪标注处理16至18世纪会议决议数据；采用包含ResNet34编码器的CRNN和CTC损失函数训练现代英文手写识别模型。

Result: 优化了历史和现代数据集上的OCR和布局分析能力，对相关任务提供了可行的技术解决方案。

Conclusion: 提出了有效的深度学习方法和分析，对未来的文档识别研究提供了有价值的方向性建议。

Abstract: This paper presents our methodology and findings from three tasks across
Optical Character Recognition (OCR) and Document Layout Analysis using advanced
deep learning techniques. First, for the historical Hebrew fragments of the
Dead Sea Scrolls, we enhanced our dataset through extensive data augmentation
and employed the Kraken and TrOCR models to improve character recognition. In
our analysis of 16th to 18th-century meeting resolutions task, we utilized a
Convolutional Recurrent Neural Network (CRNN) that integrated DeepLabV3+ for
semantic segmentation with a Bidirectional LSTM, incorporating confidence-based
pseudolabeling to refine our model. Finally, for modern English handwriting
recognition task, we applied a CRNN with a ResNet34 encoder, trained using the
Connectionist Temporal Classification (CTC) loss function to effectively
capture sequential dependencies. This report offers valuable insights and
suggests potential directions for future research.

</details>


### [24] [AtomDiffuser: Time-Aware Degradation Modeling for Drift and Beam Damage in STEM Imaging](https://arxiv.org/abs/2508.10359)
*Hao Wang,Hongkui Zheng,Kai He,Abolfazl Razi*

Main category: cs.CV

TL;DR: AtomDiffuser是一个建模框架，可分离STEM数据中的漂移与辐射衰减。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法难以分离时间分辨STEM数据中漂移与辐射衰减效应的问题。

Method: 提出了一种名为AtomDiffuser的时间感知退化建模框架，通过预测仿射变换与空间衰减图来分离漂移和辐射衰减。

Result: AtomDiffuser在模拟数据和实际数据中表现良好，支持高分辨率退化推断与漂移校正。

Conclusion: 该方法为分析辐射诱导的原子不稳定性与退化模式提供了工具，有助于解释STEM数据的时间演化结构。

Abstract: Scanning transmission electron microscopy (STEM) plays a critical role in
modern materials science, enabling direct imaging of atomic structures and
their evolution under external interferences. However, interpreting
time-resolved STEM data remains challenging due to two entangled degradation
effects: spatial drift caused by mechanical and thermal instabilities, and
beam-induced signal loss resulting from radiation damage. These factors distort
both geometry and intensity in complex, temporally correlated ways, making it
difficult for existing methods to explicitly separate their effects or model
material dynamics at atomic resolution. In this work, we present AtomDiffuser,
a time-aware degradation modeling framework that disentangles sample drift and
radiometric attenuation by predicting an affine transformation and a spatially
varying decay map between any two STEM frames. Unlike traditional denoising or
registration pipelines, our method leverages degradation as a physically
heuristic, temporally conditioned process, enabling interpretable structural
evolutions across time. Trained on synthetic degradation processes,
AtomDiffuser also generalizes well to real-world cryo-STEM data. It further
supports high-resolution degradation inference and drift alignment, offering
tools for visualizing and quantifying degradation patterns that correlate with
radiation-induced atomic instabilities.

</details>


### [25] [Contrast Sensitivity Function of Multimodal Vision-Language Models](https://arxiv.org/abs/2508.10367)
*Pablo Hernández-Cámara,Alexandra Gomez-Villa,Jose Manuel Jaén-Lorites,Jorge Vila-Tomás,Jesus Malo,Valero Laparra*

Main category: cs.CV

TL;DR: 研究了一种新方法，根据基于心理物理学的实验评估视觉-语言多模态模型对比敏感函数的表现，与人类视觉感知进行比较。


<details>
  <summary>Details</summary>
Motivation: 理解如何更好地评估视觉-语言多模态模型在低级视觉特征上的感知能力，特别是与人类感知进行对比。

Method: 提出了一种新颖的方法，利用带有噪声的图像和多样的提示语，通过直接提问模型以评估其对比敏感函数的体现。

Result: 发现模型在某些方面接近了人类的对比敏感函数，但并未完全复制其形态和强度。此外，提示语的表述对模型的反应有显著影响。

Conclusion: 该研究提供了一个全新的框架来研究多模态模型的视觉敏感性，同时暴露了其与人类感知之间的重要差距。

Abstract: Assessing the alignment of multimodal vision-language models~(VLMs) with
human perception is essential to understand how they perceive low-level visual
features. A key characteristic of human vision is the contrast sensitivity
function (CSF), which describes sensitivity to spatial frequency at
low-contrasts. Here, we introduce a novel behavioral psychophysics-inspired
method to estimate the CSF of chat-based VLMs by directly prompting them to
judge pattern visibility at different contrasts for each frequency. This
methodology is closer to the real experiments in psychophysics than the
previously reported. Using band-pass filtered noise images and a diverse set of
prompts, we assess model responses across multiple architectures. We find that
while some models approximate human-like CSF shape or magnitude, none fully
replicate both. Notably, prompt phrasing has a large effect on the responses,
raising concerns about prompt stability. Our results provide a new framework
for probing visual sensitivity in multimodal models and reveal key gaps between
their visual representations and human perception.

</details>


### [26] [Towards Spatially Consistent Image Generation: On Incorporating Intrinsic Scene Properties into Diffusion Models](https://arxiv.org/abs/2508.10382)
*Hyundo Lee,Suhyung Choi,Byoung-Tak Zhang,Inwoo Hwang*

Main category: cs.CV

TL;DR: 本研究提出一种方法，通过同时生成图像及其内在场景属性以提高生成图像的空间一致性和真实性。方法基于大型数据集训练的预训练模型，结合自动编码器整合场景属性与潜变量。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型通常存在空间不一致和结构失真的问题，亟需通过更充分的场景结构和空间布局信息提高生成质量。

Method: 利用预训练的估算器从大规模图像数据中提取内在的场景属性，不需要额外的场景信息或显式的3D表示，通过自动编码器将多种属性聚合成单一潜变量，并在预训练的Latent Diffusion Models上实现图像及内在属性的协同去噪。

Result: 方法显著改善了生成图像的空间一致性和布局自然性，同时保留了基模型的图像质量和文本对齐能力。

Conclusion: 通过结合图像和内在属性的协同生成，研究解决了生成图像的空间不一致问题，证明了其对生成图像质量与真实性的提升。

Abstract: Image generation models trained on large datasets can synthesize high-quality
images but often produce spatially inconsistent and distorted images due to
limited information about the underlying structures and spatial layouts. In
this work, we leverage intrinsic scene properties (e.g., depth, segmentation
maps) that provide rich information about the underlying scene, unlike prior
approaches that solely rely on image-text pairs or use intrinsics as
conditional inputs. Our approach aims to co-generate both images and their
corresponding intrinsics, enabling the model to implicitly capture the
underlying scene structure and generate more spatially consistent and realistic
images. Specifically, we first extract rich intrinsic scene properties from a
large image dataset with pre-trained estimators, eliminating the need for
additional scene information or explicit 3D representations. We then aggregate
various intrinsic scene properties into a single latent variable using an
autoencoder. Building upon pre-trained large-scale Latent Diffusion Models
(LDMs), our method simultaneously denoises the image and intrinsic domains by
carefully sharing mutual information so that the image and intrinsic reflect
each other without degrading image quality. Experimental results demonstrate
that our method corrects spatial inconsistencies and produces a more natural
layout of scenes while maintaining the fidelity and textual alignment of the
base model (e.g., Stable Diffusion).

</details>


### [27] [Unlocking Robust Semantic Segmentation Performance via Label-only Elastic Deformations against Implicit Label Noise](https://arxiv.org/abs/2508.10383)
*Yechan Kim,Dongho Yoon,Younkwan Lee,Unse Fatima,Hong Kook Kim,Songjae Lee,Sanga Park,Jeong Ho Park,Seonjong Kang,Moongu Jeon*

Main category: cs.CV

TL;DR: 提出了一种新型增强框架NSegment+，通过分离图像和标签的变换以应对语义分割中的隐式标签噪声问题。


<details>
  <summary>Details</summary>
Motivation: 针对现实世界数据集中隐式标签噪声会影响模型性能的问题，如模糊的目标边界和标注者的差异性，这种噪声难以通过常规增强方式解决。

Method: 设计了NSegment+框架，仅对分割标签引入受控弹性变形，保持图像原样，促进模型学习稳健的目标结构表示。

Result: 实验显示在Vaihingen、LoveDA、Cityscapes和PASCAL VOC数据集上，NSegment+无额外技巧下分别提升mIoU +2.29、+2.38、+1.75和+3.39，并且与CutMix和Label Smoothing等技巧结合后提升更显著。

Conclusion: 通过关注隐式标签噪声问题并引入NSegment+框架，可以显著提升语义分割性能，同时为未来研究提供了方向。

Abstract: While previous studies on image segmentation focus on handling severe (or
explicit) label noise, real-world datasets also exhibit subtle (or implicit)
label imperfections. These arise from inherent challenges, such as ambiguous
object boundaries and annotator variability. Although not explicitly present,
such mild and latent noise can still impair model performance. Typical data
augmentation methods, which apply identical transformations to the image and
its label, risk amplifying these subtle imperfections and limiting the model's
generalization capacity. In this paper, we introduce NSegment+, a novel
augmentation framework that decouples image and label transformations to
address such realistic noise for semantic segmentation. By introducing
controlled elastic deformations only to segmentation labels while preserving
the original images, our method encourages models to focus on learning robust
representations of object structures despite minor label inconsistencies.
Extensive experiments demonstrate that NSegment+ consistently improves
performance, achieving mIoU gains of up to +2.29, +2.38, +1.75, and +3.39 in
average on Vaihingen, LoveDA, Cityscapes, and PASCAL VOC, respectively-even
without bells and whistles, highlighting the importance of addressing implicit
label noise. These gains can be further amplified when combined with other
training tricks, including CutMix and Label Smoothing.

</details>


### [28] [PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver Distraction Detection](https://arxiv.org/abs/2508.10397)
*Haibin Sun,Xinghui Song*

Main category: cs.CV

TL;DR: 本文提出了一种名为PQ-DAF的框架，通过姿势驱动的质量控制数据增强方法，利用视觉-语言模型过滤样本，旨在提高驾驶员分心检测模型在少样本场景下的表现及泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶分心检测模型在实际部署中常因少样本学习挑战及训练数据与目标环境的域差异导致泛化能力下降。希望通过更高效的数据增强技术提高模型在真实场景中的性能。

Method: 提出一个名为PQ-DAF的框架。框架采用渐进条件扩散模型（PCDMs）捕捉和合成驾驶员关键姿势特征，并结合基于CogVLM的样本质量评估模块，按照置信度阈值过滤低质量样本，从而扩展训练数据集并提高跨域鲁棒性。

Result: 实验表明，PQ-DAF在少样本驾驶员分心检测任务中显著提升了模型在数据稀缺条件下的性能与泛化能力。

Conclusion: PQ-DAF框架有效解决了少样本数据问题，通过高质量数据增强提高了跨域驾驶分心检测模型的稳健性和实用性。

Abstract: Driver distraction detection is essential for improving traffic safety and
reducing road accidents. However, existing models often suffer from degraded
generalization when deployed in real-world scenarios. This limitation primarily
arises from the few-shot learning challenge caused by the high cost of data
annotation in practical environments, as well as the substantial domain shift
between training datasets and target deployment conditions. To address these
issues, we propose a Pose-driven Quality-controlled Data Augmentation Framework
(PQ-DAF) that leverages a vision-language model for sample filtering to
cost-effectively expand training data and enhance cross-domain robustness.
Specifically, we employ a Progressive Conditional Diffusion Model (PCDMs) to
accurately capture key driver pose features and synthesize diverse training
examples. A sample quality assessment module, built upon the CogVLM
vision-language model, is then introduced to filter out low-quality synthetic
samples based on a confidence threshold, ensuring the reliability of the
augmented dataset. Extensive experiments demonstrate that PQ-DAF substantially
improves performance in few-shot driver distraction detection, achieving
significant gains in model generalization under data-scarce conditions.

</details>


### [29] [Translation of Text Embedding via Delta Vector to Suppress Strongly Entangled Content in Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.10407)
*Eunseo Koh,Seunghoo Hong,Tae-Young Kim,Simon S. Woo,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 本文提出一种新的方法，通过调节文本嵌入空间中的delta向量来抑制文本提示中不希望生成的强纠缠内容。从而在文生图扩散模型中提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 目前的文生图扩散模型在强调生成多样性和高质量方面取得进展，但仍难以抑制文本中特定词汇强纠缠的内容，例如在生成查理·卓别林的图像时,”小胡子”无法被去除。这种现象需被有效解决。

Method: 方法引入delta向量，修改文本嵌入，从而削弱不希望内容的影响。此外，提出了选择性压制方法（SSDV），将delta向量集成进交叉注意力机制，进一步增强抑制作用，同时支持个性化模型中更精确的抑制。

Result: 实验表明，该方法在定量和定性指标上均显著优于现有方法，能够更有效地抑制不期望的生成内容。

Conclusion: 本文方法通过调控文本嵌入与交叉注意力策略实现了对特定不良生成内容的有效抑制，为提高文生图模型的生成质量提供了新思路。

Abstract: Text-to-Image (T2I) diffusion models have made significant progress in
generating diverse high-quality images from textual prompts. However, these
models still face challenges in suppressing content that is strongly entangled
with specific words. For example, when generating an image of ``Charlie
Chaplin", a ``mustache" consistently appears even if explicitly instructed not
to include it, as the concept of ``mustache" is strongly entangled with
``Charlie Chaplin". To address this issue, we propose a novel approach to
directly suppress such entangled content within the text embedding space of
diffusion models. Our method introduces a delta vector that modifies the text
embedding to weaken the influence of undesired content in the generated image,
and we further demonstrate that this delta vector can be easily obtained
through a zero-shot approach. Furthermore, we propose a Selective Suppression
with Delta Vector (SSDV) method to adapt delta vector into the cross-attention
mechanism, enabling more effective suppression of unwanted content in regions
where it would otherwise be generated. Additionally, we enabled more precise
suppression in personalized T2I models by optimizing delta vector, which
previous baselines were unable to achieve. Extensive experimental results
demonstrate that our approach significantly outperforms existing methods, both
in terms of quantitative and qualitative metrics.

</details>


### [30] [SC-Lane: Slope-aware and Consistent Road Height Estimation Framework for 3D Lane Detection](https://arxiv.org/abs/2508.10411)
*Chaesong Park,Eunbin Seo,Jihyeon Hwang,Jongwoo Lim*

Main category: cs.CV

TL;DR: SC-Lane 是一個創新的3D車道檢測框架，結合坡度感知和時間一致性，提高了在多樣地形上的穩定高度估算以及整體性能。


<details>
  <summary>Details</summary>
Motivation: 解決以往方法中固定坡度錨點的局限性，提高對多種路面幾何結構的魯棒性與3D車道檢測準確度。

Method: 提出Slope-Aware Adaptive Feature模塊，通過動態融合多坡度特徵來生成統一高度圖；配備Height Consistency Module，確保跨幀的高度一致性。

Result: 經OpenLane基準測試，SC-Lane技術大幅提升高度估算與3D車道檢測性能，F-score達到64.3%並超越現有方法。

Conclusion: SC-Lane 結合創新的模塊設計與嚴謹的測試標準，展示了在3D車道檢測領域的重要進步，為未來研究設置了新的基準。

Abstract: In this paper, we introduce SC-Lane, a novel slope-aware and temporally
consistent heightmap estimation framework for 3D lane detection. Unlike
previous approaches that rely on fixed slope anchors, SC-Lane adaptively
determines the fusion of slope-specific height features, improving robustness
to diverse road geometries. To achieve this, we propose a Slope-Aware Adaptive
Feature module that dynamically predicts the appropriate weights from image
cues for integrating multi-slope representations into a unified heightmap.
Additionally, a Height Consistency Module enforces temporal coherence, ensuring
stable and accurate height estimation across consecutive frames, which is
crucial for real-world driving scenarios. To evaluate the effectiveness of
SC-Lane, we employ three standardized metrics-Mean Absolute Error(MAE), Root
Mean Squared Error (RMSE), and threshold-based accuracy-which, although common
in surface and depth estimation, have been underutilized for road height
assessment. Using the LiDAR-derived heightmap dataset introduced in prior work
[20], we benchmark our method under these metrics, thereby establishing a
rigorous standard for future comparisons. Extensive experiments on the OpenLane
benchmark demonstrate that SC-Lane significantly improves both height
estimation and 3D lane detection, achieving state-of-the-art performance with
an F-score of 64.3%, outperforming existing methods by a notable margin. For
detailed results and a demonstration video, please refer to our project
page:https://parkchaesong.github.io/sclane/

</details>


### [31] [NanoControl: A Lightweight Framework for Precise and Efficient Control in Diffusion Transformer](https://arxiv.org/abs/2508.10424)
*Shanyuan Liu,Jian Zhu,Junda Lu,Yue Gong,Liuzhuozheng Li,Bo Cheng,Yuhang Ma,Liebucha Wu,Xiaoyu Wu,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: 本文提出NanoControl改进Diffusion Transformers (DiTs)，在可控文本到图像生成方面既高效又具有优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要基于UNet的ControlNet设计，依赖冗长的参数且计算成本高。本文旨在减少开销和提升性能。

Method: 采用Flux作为骨干网络，设计了LoRA样式的控制模块，学习原始条件输入的控制信号。引入KV-Context Augmentation机制，简化并深度融合条件特征。

Result: NanoControl显著减少了计算开销。实验表明在可控生成质量和掌控性上优于传统方法。

Conclusion: NanoControl复杂度低，却达到当前最优可控文本到图像生成表现，对于实用化具有重要意义。

Abstract: Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in
text-to-image synthesis. However, in the domain of controllable text-to-image
generation using DiTs, most existing methods still rely on the ControlNet
paradigm originally designed for UNet-based diffusion models. This paradigm
introduces significant parameter overhead and increased computational costs. To
address these challenges, we propose the Nano Control Diffusion Transformer
(NanoControl), which employs Flux as the backbone network. Our model achieves
state-of-the-art controllable text-to-image generation performance while
incurring only a 0.024\% increase in parameter count and a 0.029\% increase in
GFLOPs, thus enabling highly efficient controllable generation. Specifically,
rather than duplicating the DiT backbone for control, we design a LoRA-style
(low-rank adaptation) control module that directly learns control signals from
raw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation
mechanism that integrates condition-specific key-value information into the
backbone in a simple yet highly effective manner, facilitating deep fusion of
conditional features. Extensive benchmark experiments demonstrate that
NanoControl significantly reduces computational overhead compared to
conventional control approaches, while maintaining superior generation quality
and achieving improved controllability.

</details>


### [32] [STRIDE-QA: Visual Question Answering Dataset for Spatiotemporal Reasoning in Urban Driving Scenes](https://arxiv.org/abs/2508.10427)
*Keishi Ishihara,Kento Sasaki,Tsubasa Takahashi,Daiki Shiono,Yu Yamaguchi*

Main category: cs.CV

TL;DR: 本研究提出STRIDE-QA，一个大规模视觉问答(VQA)数据集，用于支持自主驾驶中的时空推理任务；通过与现有方法的比较，显著提高了视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型(VLMs)通常基于静态的网络图像-文本训练，无法满足动态交通场景中精确的时空推理需求，且无法为自主驾驶提供可靠的支持，因此需要专门构建一个支持动态交通推理的数据集。

Method: 开发了STRIDE-QA数据集，该数据集包含来自东京100小时驾驶数据，涵盖多样化和挑战性的条件，包括16百万对问题答案（QA pairs），并带有自动生成的密集注释，如3D边界框、分割掩码及多物体轨迹。设计了三种新颖的问答任务，支持以对象为中心和以视角为中心的时空推理。

Result: 实验结果表明，现有的视觉语言模型在预测一致性方面表现接近于零。而通过在STRIDE-QA数据集上微调的视觉语言模型，可以将空间定位的成功率提升至55%，未来运动预测的成功率提升至28%。

Conclusion: STRIDE-QA数据集为研发更可靠的面向安全关键型自主系统的视觉语言模型(VLMs)奠定了综合性的基础。

Abstract: Vision-Language Models (VLMs) have been applied to autonomous driving to
support decision-making in complex real-world scenarios. However, their
training on static, web-sourced image-text pairs fundamentally limits the
precise spatiotemporal reasoning required to understand and predict dynamic
traffic scenes. We address this critical gap with STRIDE-QA, a large-scale
visual question answering (VQA) dataset for physically grounded reasoning from
an ego-centric perspective. Constructed from 100 hours of multi-sensor driving
data in Tokyo, capturing diverse and challenging conditions, STRIDE-QA is the
largest VQA dataset for spatiotemporal reasoning in urban driving, offering 16
million QA pairs over 285K frames. Grounded by dense, automatically generated
annotations including 3D bounding boxes, segmentation masks, and multi-object
tracks, the dataset uniquely supports both object-centric and ego-centric
reasoning through three novel QA tasks that require spatial localization and
temporal prediction. Our benchmarks demonstrate that existing VLMs struggle
significantly, achieving near-zero scores on prediction consistency. In
contrast, VLMs fine-tuned on STRIDE-QA exhibit dramatic performance gains,
achieving 55% success in spatial localization and 28% consistency in future
motion prediction, compared to near-zero scores from general-purpose VLMs.
Therefore, STRIDE-QA establishes a comprehensive foundation for developing more
reliable VLMs for safety-critical autonomous systems.

</details>


### [33] [CRISP: Contrastive Residual Injection and Semantic Prompting for Continual Video Instance Segmentation](https://arxiv.org/abs/2508.10432)
*Baichen Liu,Qi Lyu,Xudong Wang,Jiahua Dong,Lianqing Liu,Zhi Han*

Main category: cs.CV

TL;DR: 提出了CRISP框架，用于解决持续视频实例分割中的实例、类别和任务混淆问题，实验结果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 持续视频实例分割需要在吸收新类别的同时保持对已学类别的记忆，并确保帧间时间一致性。

Method: 引入对比残差注入与语义提示（CRISP），包括实例相关性损失、适应性残差语义提示（ARSP）框架和语义一致性对比损失，并设计了一种增量提示初始化策略。

Result: 在YouTube-VIS-2019和YouTube-VIS-2021数据集的实验中，CRISP显著优于现有持续分割方法，有效避免灾难性遗忘，并提升了分割和分类性能。

Conclusion: CRISP有效应对了持续视频实例分割中的多种挑战，展示了其在长期任务上的优越性能。

Abstract: Continual video instance segmentation demands both the plasticity to absorb
new object categories and the stability to retain previously learned ones, all
while preserving temporal consistency across frames. In this work, we introduce
Contrastive Residual Injection and Semantic Prompting (CRISP), an earlier
attempt tailored to address the instance-wise, category-wise, and task-wise
confusion in continual video instance segmentation. For instance-wise learning,
we model instance tracking and construct instance correlation loss, which
emphasizes the correlation with the prior query space while strengthening the
specificity of the current task query. For category-wise learning, we build an
adaptive residual semantic prompt (ARSP) learning framework, which constructs a
learnable semantic residual prompt pool generated by category text and uses an
adjustive query-prompt matching mechanism to build a mapping relationship
between the query of the current task and the semantic residual prompt.
Meanwhile, a semantic consistency loss based on the contrastive learning is
introduced to maintain semantic coherence between object queries and residual
prompts during incremental training. For task-wise learning, to ensure the
correlation at the inter-task level within the query space, we introduce a
concise yet powerful initialization strategy for incremental prompts. Extensive
experiments on YouTube-VIS-2019 and YouTube-VIS-2021 datasets demonstrate that
CRISP significantly outperforms existing continual segmentation methods in the
long-term continual video instance segmentation task, avoiding catastrophic
forgetting and effectively improving segmentation and classification
performance. The code is available at https://github.com/01upup10/CRISP.

</details>


### [34] [DOD-SA: Infrared-Visible Decoupled Object Detection with Single-Modality Annotations](https://arxiv.org/abs/2508.10445)
*Hang Jin,Chenqiang Gao,Junjie Guo,Fangcen Liu,Kanghui Tian,Qinyao Chang*

Main category: cs.CV

TL;DR: 提出了一种名为DOD-SA的新框架，通过单模态注释解决红外-可见光双模态目标检测的高注释成本问题。


<details>
  <summary>Details</summary>
Motivation: 现有的红外-可见光目标检测方法需要双模态注释，导致高昂的注释成本。

Method: 引入一种名为DOD-SA的方法，采用单模态与双模态协作的教师-学生网络（CoSD-TSNet）结构，结合渐进自调的训练策略（PaST）和伪标签分配器（PLA）。

Result: 在DroneVehicle数据集上表现优于最新的SOTA方法。

Conclusion: 通过单模态注释实现了高效的红外-可见光目标检测，有效降低了注释成本，同时提升了模型性能并解决了模态对齐问题。

Abstract: Infrared-visible object detection has shown great potential in real-world
applications, enabling robust all-day perception by leveraging the
complementary information of infrared and visible images. However, existing
methods typically require dual-modality annotations to output detection results
for both modalities during prediction, which incurs high annotation costs. To
address this challenge, we propose a novel infrared-visible Decoupled Object
Detection framework with Single-modality Annotations, called DOD-SA. The
architecture of DOD-SA is built upon a Single- and Dual-Modality Collaborative
Teacher-Student Network (CoSD-TSNet), which consists of a single-modality
branch (SM-Branch) and a dual-modality decoupled branch (DMD-Branch). The
teacher model generates pseudo-labels for the unlabeled modality,
simultaneously supporting the training of the student model. The collaborative
design enables cross-modality knowledge transfer from the labeled modality to
the unlabeled modality, and facilitates effective SM-to-DMD branch supervision.
To further improve the decoupling ability of the model and the pseudo-label
quality, we introduce a Progressive and Self-Tuning Training Strategy (PaST)
that trains the model in three stages: (1) pretraining SM-Branch, (2) guiding
the learning of DMD-Branch by SM-Branch, and (3) refining DMD-Branch. In
addition, we design a Pseudo Label Assigner (PLA) to align and pair labels
across modalities, explicitly addressing modality misalignment during training.
Extensive experiments on the DroneVehicle dataset demonstrate that our method
outperforms state-of-the-art (SOTA).

</details>


### [35] [SkeySpot: Automating Service Key Detection for Digital Electrical Layout Plans in the Construction Industry](https://arxiv.org/abs/2508.10449)
*Dhruv Dosi,Rohit Meena,Param Rajpura,Yogesh Kumar Meena*

Main category: cs.CV

TL;DR: 此论文提出一种名为SkeySpot的工具，利用YOLOv8模型进行电气布局图符号检测，准确性高达82.5%。


<details>
  <summary>Details</summary>
Motivation: 现有的老旧平面图多为扫描文档，不可机器读取，导致大规模解读耗时且易出错，急需一种高效的自动化符号检测方法。

Method: 创建DELP数据集，包含34类服务符号，以YOLOv8为代表的预训练检测模型进行评估，开发出SkeySpot工具用于符号检测和量化。

Result: YOLOv8在DELP数据集上的mAP达到82.5%，SkeySpot提供实时检测和结构化输出，支持建筑信息工作流的互操作性。

Conclusion: 该方法降低对专有CAD系统的依赖，简化SME的电气布局数字化流程，推动建筑领域的标准化、可持续性与互操作性。

Abstract: Legacy floor plans, often preserved only as scanned documents, remain
essential resources for architecture, urban planning, and facility management
in the construction industry. However, the lack of machine-readable floor plans
render large-scale interpretation both time-consuming and error-prone.
Automated symbol spotting offers a scalable solution by enabling the
identification of service key symbols directly from floor plans, supporting
workflows such as cost estimation, infrastructure maintenance, and regulatory
compliance. This work introduces a labelled Digitised Electrical Layout Plans
(DELP) dataset comprising 45 scanned electrical layout plans annotated with
2,450 instances across 34 distinct service key classes. A systematic evaluation
framework is proposed using pretrained object detection models for DELP
dataset. Among the models benchmarked, YOLOv8 achieves the highest performance
with a mean Average Precision (mAP) of 82.5\%. Using YOLOv8, we develop
SkeySpot, a lightweight, open-source toolkit for real-time detection,
classification, and quantification of electrical symbols. SkeySpot produces
structured, standardised outputs that can be scaled up for interoperable
building information workflows, ultimately enabling compatibility across
downstream applications and regulatory platforms. By lowering dependency on
proprietary CAD systems and reducing manual annotation effort, this approach
makes the digitisation of electrical layouts more accessible to small and
medium-sized enterprises (SMEs) in the construction industry, while supporting
broader goals of standardisation, interoperability, and sustainability in the
built environment.

</details>


### [36] [From Images to Perception: Emergence of Perceptual Properties by Reconstructing Images](https://arxiv.org/abs/2508.10450)
*Pablo Hernández-Cámara,Jesus Malo,Valero Laparra*

Main category: cs.CV

TL;DR: 本研究提出了一种生物启发的视觉模型，通过优化实现了对图像重建任务的高效处理，同时表现出了与人类感知一致的结果。


<details>
  <summary>Details</summary>
Motivation: 探索人类视觉感知是否可以通过图像统计产生，并设计一种符合生物学特征的视觉模型来再现早期视觉神经表示。

Method: 提出了一种名为PerceptNet的生物启发架构，并针对图像自编码、去噪、去模糊和稀疏正则化任务进行了端到端优化。

Result: PerceptNet在V1层中表现出与人类感知判断一致的最高相关性，尤其在中等噪声、模糊和适度稀疏的情况下。

Conclusion: 研究表明视觉系统可能被调试用于去除特定失真，并且生物启发型模型无需监督即可自发学习感知指标。

Abstract: A number of scientists suggested that human visual perception may emerge from
image statistics, shaping efficient neural representations in early vision. In
this work, a bio-inspired architecture that can accommodate several known facts
in the retina-V1 cortex, the PerceptNet, has been end-to-end optimized for
different tasks related to image reconstruction: autoencoding, denoising,
deblurring, and sparsity regularization. Our results show that the encoder
stage (V1-like layer) consistently exhibits the highest correlation with human
perceptual judgments on image distortion despite not using perceptual
information in the initialization or training. This alignment exhibits an
optimum for moderate noise, blur and sparsity. These findings suggest that the
visual system may be tuned to remove those particular levels of distortion with
that level of sparsity and that biologically inspired models can learn
perceptual metrics without human supervision.

</details>


### [37] [Trajectory-aware Shifted State Space Models for Online Video Super-Resolution](https://arxiv.org/abs/2508.10453)
*Qiang Zhu,Xiandong Meng,Yuxian Jiang,Fan Zhang,David Bull,Shuyuan Zhu,Bing Zeng*

Main category: cs.CV

TL;DR: 本文提出了一种基于轨迹感知偏移状态空间模型(TS-Mamba)的在线视频超分辨率(VSR)方法，在提高性能的同时显著减少复杂性。


<details>
  <summary>Details</summary>
Motivation: 现有在线VSR方法通常依赖单一临近帧进行时间对齐，限制了视频的长时序建模，且需提高效率与性能。

Method: 通过基于Mamba的轨迹感知偏移状态空间模型，构建视频轨迹并筛选相似token，利用设计的偏移SSM模块聚合信息，同时提出轨迹感知损失函数增强训练效果。

Result: 在三大在线VSR测试数据集上，TS-Mamba性能超越六大基准模型，并在大多数情况下减少22.7%以上计算复杂度。

Conclusion: TS-Mamba方法实现了在线VSR的高效空间-时间信息聚合，兼具性能提升与复杂性降低，展示了卓越的应用潜力。

Abstract: Online video super-resolution (VSR) is an important technique for many
real-world video processing applications, which aims to restore the current
high-resolution video frame based on temporally previous frames. Most of the
existing online VSR methods solely employ one neighboring previous frame to
achieve temporal alignment, which limits long-range temporal modeling of
videos. Recently, state space models (SSMs) have been proposed with linear
computational complexity and a global receptive field, which significantly
improve computational efficiency and performance. In this context, this paper
presents a novel online VSR method based on Trajectory-aware Shifted SSMs
(TS-Mamba), leveraging both long-term trajectory modeling and low-complexity
Mamba to achieve efficient spatio-temporal information aggregation.
Specifically, TS-Mamba first constructs the trajectories within a video to
select the most similar tokens from the previous frames. Then, a
Trajectory-aware Shifted Mamba Aggregation (TSMA) module consisting of proposed
shifted SSMs blocks is employed to aggregate the selected tokens. The shifted
SSMs blocks are designed based on Hilbert scannings and corresponding shift
operations to compensate for scanning losses and strengthen the spatial
continuity of Mamba. Additionally, we propose a trajectory-aware loss function
to supervise the trajectory generation, ensuring the accuracy of token
selection when training our model. Extensive experiments on three widely used
VSR test datasets demonstrate that compared with six online VSR benchmark
models, our TS-Mamba achieves state-of-the-art performance in most cases and
over 22.7\% complexity reduction (in MACs). The source code for TS-Mamba will
be available at https://github.com.

</details>


### [38] [Multi-Label Plant Species Prediction with Metadata-Enhanced Multi-Head Vision Transformers](https://arxiv.org/abs/2508.10457)
*Hanna Herasimchyk,Robin Labryga,Tomislav Prusina*

Main category: cs.CV

TL;DR: 提出了一种用于多标签植物物种预测的多头视觉Transformer方法，在PlantCLEF 2025挑战中表现出色。


<details>
  <summary>Details</summary>
Motivation: 应对单物种训练与多物种测试之间的域转移问题，提升植物物种预测的准确性。

Method: 使用预训练DINOv2 ViT-B/14主干网络，并引入多个分类头，结合多尺度切片、动态阈值优化、图像裁剪和集成策略等方法，同时基于层级分类加权预测。

Result: 实验基于约140万张训练图像完成，涵盖7806种植物物种，在私人排行榜上排名第3。

Conclusion: 所提方法有效解决了域转移问题，显著提升了多标签植物物种预测的性能，代码可供开源。

Abstract: We present a multi-head vision transformer approach for multi-label plant
species prediction in vegetation plot images, addressing the PlantCLEF 2025
challenge. The task involves training models on single-species plant images
while testing on multi-species quadrat images, creating a drastic domain shift.
Our methodology leverages a pre-trained DINOv2 Vision Transformer Base
(ViT-B/14) backbone with multiple classification heads for species, genus, and
family prediction, utilizing taxonomic hierarchies. Key contributions include
multi-scale tiling to capture plants at different scales, dynamic threshold
optimization based on mean prediction length, and ensemble strategies through
bagging and Hydra model architectures. The approach incorporates various
inference techniques including image cropping to remove non-plant artifacts,
top-n filtering for prediction constraints, and logit thresholding strategies.
Experiments were conducted on approximately 1.4 million training images
covering 7,806 plant species. Results demonstrate strong performance, making
our submission 3rd best on the private leaderboard. Our code is available at
https://github.com/geranium12/plant-clef-2025/tree/v1.0.0.

</details>


### [39] [SingleStrip: learning skull-stripping from a single labeled example](https://arxiv.org/abs/2508.10464)
*Bella Specktor-Fadida,Malte Hoffmann*

Main category: cs.CV

TL;DR: 本研究结合域随机化和自训练技术，减少了三维颅骨分割任务中对标注数据的需求，仅需单个标注数据即可实现接近传统方法的分割性能。


<details>
  <summary>Details</summary>
Motivation: 手动标注脑部MRI数据耗时费力，且在标签有限的情况下，现有技术对解剖多样性的生成能力有限，亟需有效利用未标注数据的新方法。

Method: 将域随机化与自训练相结合：1）用自动化分箱生成的标签合成图像训练初始模型；2）通过卷积自编码器(AE)评价未标注数据预测结果质量；3）利用高质量伪标签微调模型。

Result: 该方法通过AE排名优于一致性排名，并在测试时实现了接近多标签模型的颅骨分割性能，尤其适用于分布外数据。

Conclusion: 结合域随机化与AE质量控制的策略，有望显著减少标注需求，加速新解剖结构或新成像技术相关研究进程。

Abstract: Deep learning segmentation relies heavily on labeled data, but manual
labeling is laborious and time-consuming, especially for volumetric images such
as brain magnetic resonance imaging (MRI). While recent domain-randomization
techniques alleviate the dependency on labeled data by synthesizing diverse
training images from label maps, they offer limited anatomical variability when
very few label maps are available. Semi-supervised self-training addresses
label scarcity by iteratively incorporating model predictions into the training
set, enabling networks to learn from unlabeled data. In this work, we combine
domain randomization with self-training to train three-dimensional
skull-stripping networks using as little as a single labeled example. First, we
automatically bin voxel intensities, yielding labels we use to synthesize
images for training an initial skull-stripping model. Second, we train a
convolutional autoencoder (AE) on the labeled example and use its
reconstruction error to assess the quality of brain masks predicted for
unlabeled data. Third, we select the top-ranking pseudo-labels to fine-tune the
network, achieving skull-stripping performance on out-of-distribution data that
approaches models trained with more labeled images. We compare AE-based ranking
to consistency-based ranking under test-time augmentation, finding that the AE
approach yields a stronger correlation with segmentation accuracy. Our results
highlight the potential of combining domain randomization and AE-based quality
control to enable effective semi-supervised segmentation from extremely limited
labeled data. This strategy may ease the labeling burden that slows progress in
studies involving new anatomical structures or emerging imaging techniques.

</details>


### [40] [Enhanced Sparse Point Cloud Data Processing for Privacy-aware Human Action Recognition](https://arxiv.org/abs/2508.10469)
*Maimunatu Tunau,Vincent Gbouna Zakka,Zhuangzhuang Dai*

Main category: cs.CV

TL;DR: 本研究分析了DBSCAN、匈牙利算法和卡尔曼滤波器在毫米波雷达数据处理中的性能，对它们的单独使用及组合进行详细评估。


<details>
  <summary>Details</summary>
Motivation: 为了弥补毫米波雷达点云数据稀疏和噪声问题，评价不同数据处理方法对人类动作识别效果的提升。

Method: 评估DBSCAN、匈牙利算法及卡尔曼滤波器的单独使用、双重组合和全结合对识别准确性和计算成本的影响，并对方法进行定向优化。

Result: 分析结果揭示了各方法及其组合的优劣势，并提出提升单个算法准确性的改进措施。

Conclusion: 提供了针对毫米波基于人类动作识别系统开发的重要见解，有助于推动未来研究。

Abstract: Human Action Recognition (HAR) plays a crucial role in healthcare, fitness
tracking, and ambient assisted living technologies. While traditional vision
based HAR systems are effective, they pose privacy concerns. mmWave radar
sensors offer a privacy preserving alternative but present challenges due to
the sparse and noisy nature of their point cloud data. In the literature, three
primary data processing methods: Density-Based Spatial Clustering of
Applications with Noise (DBSCAN), the Hungarian Algorithm, and Kalman Filtering
have been widely used to improve the quality and continuity of radar data.
However, a comprehensive evaluation of these methods, both individually and in
combination, remains lacking. This paper addresses that gap by conducting a
detailed performance analysis of the three methods using the MiliPoint dataset.
We evaluate each method individually, all possible pairwise combinations, and
the combination of all three, assessing both recognition accuracy and
computational cost. Furthermore, we propose targeted enhancements to the
individual methods aimed at improving accuracy. Our results provide crucial
insights into the strengths and trade-offs of each method and their
integrations, guiding future work on mmWave based HAR systems

</details>


### [41] [STAMP: Multi-pattern Attention-aware Multiple Instance Learning for STAS Diagnosis in Multi-center Histopathology Images](https://arxiv.org/abs/2508.10473)
*Liangrui Pan,xiaoyu Li,Guang Zhu,Guanting Li,Ruixin Wang,Jiadi Luo,Yaning Yang,Liang qingchun,Shaoliang Peng*

Main category: cs.CV

TL;DR: 通过深度学习框架STAMP，自动化诊断肺腺癌STAS，突破诊断准确性瓶颈。


<details>
  <summary>Details</summary>
Motivation: STAS导致肺腺癌复发率上升和生存率下降，人工诊断耗时且易出错，亟需自动化诊断方法。

Method: 提出STAMP框架，利用多中心病理图像，结合多模式注意力机制和Transformer编码，提高全球表征的诊断能力。

Result: 测试结果显示在三个数据集上，诊断效果尤其优异（AUC分别为0.8058、0.8017、0.7928），优于临床水平。

Conclusion: STAMP框架显著提升STAS诊断效能，具有重要的临床应用潜力。

Abstract: Spread through air spaces (STAS) constitutes a novel invasive pattern in lung
adenocarcinoma (LUAD), associated with tumor recurrence and diminished survival
rates. However, large-scale STAS diagnosis in LUAD remains a labor-intensive
endeavor, compounded by the propensity for oversight and misdiagnosis due to
its distinctive pathological characteristics and morphological features.
Consequently, there is a pressing clinical imperative to leverage deep learning
models for STAS diagnosis. This study initially assembled histopathological
images from STAS patients at the Second Xiangya Hospital and the Third Xiangya
Hospital of Central South University, alongside the TCGA-LUAD cohort. Three
senior pathologists conducted cross-verification annotations to construct the
STAS-SXY, STAS-TXY, and STAS-TCGA datasets. We then propose a multi-pattern
attention-aware multiple instance learning framework, named STAMP, to analyze
and diagnose the presence of STAS across multi-center histopathology images.
Specifically, the dual-branch architecture guides the model to learn
STAS-associated pathological features from distinct semantic spaces.
Transformer-based instance encoding and a multi-pattern attention aggregation
modules dynamically selects regions closely associated with STAS pathology,
suppressing irrelevant noise and enhancing the discriminative power of global
representations. Moreover, a similarity regularization constraint prevents
feature redundancy across branches, thereby improving overall diagnostic
accuracy. Extensive experiments demonstrated that STAMP achieved competitive
diagnostic results on STAS-SXY, STAS-TXY and STAS-TCGA, with AUCs of 0.8058,
0.8017, and 0.7928, respectively, surpassing the clinical level.

</details>


### [42] [TweezeEdit: Consistent and Efficient Image Editing with Path Regularization](https://arxiv.org/abs/2508.10498)
*Jianda Mao,Kaibo Wang,Yang Xiang,Kani Chen*

Main category: cs.CV

TL;DR: TweezeEdit是一种无需调优和反转的新框架，它通过正则化整个去噪路径来实现一致且高效的图像编辑，同时保证语义保留和目标对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法过度遵循目标提示，而忽视了源图像的语义保留，导致编辑过程冗长和效率低下。

Method: 通过梯度驱动的正则化机制，TweezeEdit在无需依赖反转锚点的情况下，以一致性模型沿直接路径有效注入目标提示语义。

Result: 实验表明，TweezeEdit在语义保留和目标对齐方面表现优异，仅需12步（每次编辑1.6秒），实现了更高效的编辑效果。

Conclusion: TweezeEdit优于现有方法，具备实时应用潜力。

Abstract: Large-scale pre-trained diffusion models empower users to edit images through
text guidance. However, existing methods often over-align with target prompts
while inadequately preserving source image semantics. Such approaches generate
target images explicitly or implicitly from the inversion noise of the source
images, termed the inversion anchors. We identify this strategy as suboptimal
for semantic preservation and inefficient due to elongated editing paths. We
propose TweezeEdit, a tuning- and inversion-free framework for consistent and
efficient image editing. Our method addresses these limitations by regularizing
the entire denoising path rather than relying solely on the inversion anchors,
ensuring source semantic retention and shortening editing paths. Guided by
gradient-driven regularization, we efficiently inject target prompt semantics
along a direct path using a consistency model. Extensive experiments
demonstrate TweezeEdit's superior performance in semantic preservation and
target alignment, outperforming existing methods. Remarkably, it requires only
12 steps (1.6 seconds per edit), underscoring its potential for real-time
applications.

</details>


### [43] [Multi-Sample Anti-Aliasing and Constrained Optimization for 3D Gaussian Splatting](https://arxiv.org/abs/2508.10507)
*Zheng Zhou,Jia-Chen Zhang,Yu-Jie Xiong,Chun-Ming Xia*

Main category: cs.CV

TL;DR: 文章提出了一种通过多重采样抗锯齿(MSAA)与双重几何约束相结合的新优化框架，旨在改善3D高斯点绘中高频纹理和边界细节模糊的问题，并在多个基准测试中取得了最先进的细节保留表现。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决3D高斯点绘在场景优化中几何约束不足的问题，尤其是高频纹理和锐利不连续区域的模糊重建问题。

Method: 提出的框架结合了多重采样抗锯齿(MSAA)与双重几何约束：(a)动态梯度分析引导的自适应加权策略，优先关注未充分重建区域；(b)梯度差分约束，以在物体边界实现几何正则化，同时通过自适应四倍采样来减少高频分量的锯齿伪影。

Result: 实验表明，该方法在细节保留方面达到最先进水平，尤其是在高频纹理和锐利边界的保留上，同时确保实时渲染效率。在结构相似度(SSIM)和感知质量(LPIPS)等定量指标上显著优于基线方法。

Conclusion: 研究证实综合优化框架提升了细节重建能力，减轻了模糊现象，同时在多项基准测试上的性能表现出色，为实时视图合成提供了一种更精确而高效的方案。

Abstract: Recent advances in 3D Gaussian splatting have significantly improved
real-time novel view synthesis, yet insufficient geometric constraints during
scene optimization often result in blurred reconstructions of fine-grained
details, particularly in regions with high-frequency textures and sharp
discontinuities. To address this, we propose a comprehensive optimization
framework integrating multisample anti-aliasing (MSAA) with dual geometric
constraints. Our system computes pixel colors through adaptive blending of
quadruple subsamples, effectively reducing aliasing artifacts in high-frequency
components. The framework introduces two constraints: (a) an adaptive weighting
strategy that prioritizes under-reconstructed regions through dynamic gradient
analysis, and (b) gradient differential constraints enforcing geometric
regularization at object boundaries. This targeted optimization enables the
model to allocate computational resources preferentially to critical regions
requiring refinement while maintaining global consistency. Extensive
experimental evaluations across multiple benchmarks demonstrate that our method
achieves state-of-the-art performance in detail preservation, particularly in
preserving high-frequency textures and sharp discontinuities, while maintaining
real-time rendering efficiency. Quantitative metrics and perceptual studies
confirm statistically significant improvements over baseline approaches in both
structural similarity (SSIM) and perceptual quality (LPIPS).

</details>


### [44] [A Segmentation-driven Editing Method for Bolt Defect Augmentation and Detection](https://arxiv.org/abs/2508.10509)
*Yangjie Xiao,Ke Zhang,Jiacun Wang,Xin Sheng,Yurong Guo,Meijuan Chen,Zehua Ren,Zhaoye Zheng,Zhenbing Zhao*

Main category: cs.CV

TL;DR: 该论文提出通过分割驱动的螺栓缺陷编辑方法(SBDE)扩充数据集，从而解决螺栓缺陷检测中图像稀缺和数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 螺栓缺陷检测对于输电线路的安全至关重要，但缺陷图像稀缺及数据集分布不平衡限制了检测性能。

Method: 1. 提出螺栓属性分割模型(Bolt-SAM)，结合CLAHE-FFT适配器和多部分感知掩码解码器以生成高质量分割掩码；2. 设计掩码优化模块并整合至图像修复模型(LaMa)，构建螺栓缺陷属性编辑模型MOD-LaMa将正常螺栓转换为缺陷螺栓；3. 提出编辑恢复增强策略，将编辑后的螺栓恢复到原始场景中以扩展数据集。

Result: 实验结果表明，SBDE生成的螺栓缺陷图像优于现有图像编辑模型，且显著提升了缺陷检测性能。

Conclusion: 该方法有效验证了其在扩充数据集和提升检测性能中的潜力，具备实际应用前景。

Abstract: Bolt defect detection is critical to ensure the safety of transmission lines.
However, the scarcity of defect images and imbalanced data distributions
significantly limit detection performance. To address this problem, we propose
a segmentationdriven bolt defect editing method (SBDE) to augment the dataset.
First, a bolt attribute segmentation model (Bolt-SAM) is proposed, which
enhances the segmentation of complex bolt attributes through the CLAHE-FFT
Adapter (CFA) and Multipart- Aware Mask Decoder (MAMD), generating high-quality
masks for subsequent editing tasks. Second, a mask optimization module (MOD) is
designed and integrated with the image inpainting model (LaMa) to construct the
bolt defect attribute editing model (MOD-LaMa), which converts normal bolts
into defective ones through attribute editing. Finally, an editing recovery
augmentation (ERA) strategy is proposed to recover and put the edited defect
bolts back into the original inspection scenes and expand the defect detection
dataset. We constructed multiple bolt datasets and conducted extensive
experiments. Experimental results demonstrate that the bolt defect images
generated by SBDE significantly outperform state-of-the-art image editing
models, and effectively improve the performance of bolt defect detection, which
fully verifies the effectiveness and application potential of the proposed
method. The code of the project is available at
https://github.com/Jay-xyj/SBDE.

</details>


### [45] [EgoMusic-driven Human Dance Motion Estimation with Skeleton Mamba](https://arxiv.org/abs/2508.10522)
*Quang Nguyen,Nhat Le,Baoru Huang,Minh Nhat Vu,Chengcheng Tang,Van Nguyen,Ngan Le,Thieu Vo,Anh Nguyen*

Main category: cs.CV

TL;DR: 该研究提出了一个结合第一视角视频和音乐来预测人体舞蹈动作的新方法，并开发了一个包含36小时舞蹈动作数据的新数据集。


<details>
  <summary>Details</summary>
Motivation: 旨在解决联合从第一视角视频和音乐中估计人体舞蹈动作的问题，挑战包括视角遮挡带来的身体姿态估计困难以及身体运动与音乐对齐的需求。

Method: 提出了EgoMusic Motion Network，并引入Skeleton Mamba模型来捕捉人体骨架结构，同时创建了EgoAIST++大型数据集供训练与评估。

Result: 实验证明，该方法在性能上超越了最先进的技术，并在实际场景中具有良好的泛化能力。

Conclusion: 结合第一视角视频与音乐的联合建模在舞蹈动作预测中展现了巨大潜力，所提出的方法推进了相关领域的研究与应用。

Abstract: Estimating human dance motion is a challenging task with various industrial
applications. Recently, many efforts have focused on predicting human dance
motion using either egocentric video or music as input. However, the task of
jointly estimating human motion from both egocentric video and music remains
largely unexplored. In this paper, we aim to develop a new method that predicts
human dance motion from both egocentric video and music. In practice, the
egocentric view often obscures much of the body, making accurate full-pose
estimation challenging. Additionally, incorporating music requires the
generated head and body movements to align well with both visual and musical
inputs. We first introduce EgoAIST++, a new large-scale dataset that combines
both egocentric views and music with more than 36 hours of dancing motion.
Drawing on the success of diffusion models and Mamba on modeling sequences, we
develop an EgoMusic Motion Network with a core Skeleton Mamba that explicitly
captures the skeleton structure of the human body. We illustrate that our
approach is theoretically supportive. Intensive experiments show that our
method clearly outperforms state-of-the-art approaches and generalizes
effectively to real-world data.

</details>


### [46] [Reasoning in Computer Vision: Taxonomy, Models, Tasks, and Methodologies](https://arxiv.org/abs/2508.10523)
*Ayushman Sarkar,Mohd Yamani Idna Idris,Zhenyu Yu*

Main category: cs.CV

TL;DR: 该综述分类视觉推理为五种主要类型：关系式、符号式、时间式、因果式与常识推理，并通过系统审视其方法与评估协议，全面分析现有不足及未来挑战，提出新的研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决现有综述仅单独覆盖视觉推理各方面的局限性，提供统一分析与比较。

Method: 将视觉推理划分为五种类型，从模型（如图模型、记忆网络等）、评估协议（如功能性、结构一致性等）以及方法局限性等多方面系统化审视。

Result: 全面分析了视觉推理的发展现状、模型实现方式及其评估方法，并明确了当前面临的多项挑战，如复杂场景下的应用扩展及符号与神经模型的整合问题。

Conclusion: 视觉推理对下一代人工智能系统至关重要，跨感知与推理的深度融合将在自动驾驶与医疗诊断等关键领域实现透明、可信、跨领域适应力强的AI系统。

Abstract: Visual reasoning is critical for a wide range of computer vision tasks that
go beyond surface-level object detection and classification. Despite notable
advances in relational, symbolic, temporal, causal, and commonsense reasoning,
existing surveys often address these directions in isolation, lacking a unified
analysis and comparison across reasoning types, methodologies, and evaluation
protocols. This survey aims to address this gap by categorizing visual
reasoning into five major types (relational, symbolic, temporal, causal, and
commonsense) and systematically examining their implementation through
architectures such as graph-based models, memory networks, attention
mechanisms, and neuro-symbolic systems. We review evaluation protocols designed
to assess functional correctness, structural consistency, and causal validity,
and critically analyze their limitations in terms of generalizability,
reproducibility, and explanatory power. Beyond evaluation, we identify key open
challenges in visual reasoning, including scalability to complex scenes, deeper
integration of symbolic and neural paradigms, the lack of comprehensive
benchmark datasets, and reasoning under weak supervision. Finally, we outline a
forward-looking research agenda for next-generation vision systems, emphasizing
that bridging perception and reasoning is essential for building transparent,
trustworthy, and cross-domain adaptive AI systems, particularly in critical
domains such as autonomous driving and medical diagnostics.

</details>


### [47] [Med-GLIP: Advancing Medical Language-Image Pre-training with Large-scale Grounded Dataset](https://arxiv.org/abs/2508.10528)
*Ziye Deng,Ruihan He,Jiaxiang Liu,Yuan Wang,Zijie Meng,Songtao Jiang,Yong Xie,Zuozhu Liu*

Main category: cs.CV

TL;DR: 本文描述了一个新的医学影像的区域标注数据集Med-GLIP-5M及相应的模式感知框架Med-GLIP，该框架在多项任务中表现优良。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像标注技术存在模式覆盖有限、标注粒度粗糙及无统一通用框架的缺陷。

Method: 构建一个包含七种影像模式、拥有超过530万区域级标注的数据集Med-GLIP-5M，并提出通过从多样化数据中隐式获取语义层次信息的模式感知框架Med-GLIP。

Result: Med-GLIP在多个基准任务中超越当前最先进技术，同时在医学VQA与报告生成等任务上显著提升性能。

Conclusion: Med-GLIP-5M数据集和Med-GLIP框架有效推进了医学影像标注与相关下游任务的发展，数据集也将公开发布。

Abstract: Medical image grounding aims to align natural language phrases with specific
regions in medical images, serving as a foundational task for intelligent
diagnosis, visual question answering (VQA), and automated report generation
(MRG). However, existing research is constrained by limited modality coverage,
coarse-grained annotations, and the absence of a unified, generalizable
grounding framework. To address these challenges, we construct a large-scale
medical grounding dataset Med-GLIP-5M comprising over 5.3 million region-level
annotations across seven imaging modalities, covering diverse anatomical
structures and pathological findings. The dataset supports both segmentation
and grounding tasks with hierarchical region labels, ranging from organ-level
boundaries to fine-grained lesions. Based on this foundation, we propose
Med-GLIP, a modality-aware grounding framework trained on Med-GLIP-5M. Rather
than relying on explicitly designed expert modules, Med-GLIP implicitly
acquires hierarchical semantic understanding from diverse training data --
enabling it to recognize multi-granularity structures, such as distinguishing
lungs from pneumonia lesions. Extensive experiments demonstrate that Med-GLIP
consistently outperforms state-of-the-art baselines across multiple grounding
benchmarks. Furthermore, integrating its spatial outputs into downstream tasks,
including medical VQA and report generation, leads to substantial performance
gains. Our dataset will be released soon.

</details>


### [48] [GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images](https://arxiv.org/abs/2508.10542)
*Mengyu Ren,Yutong Li,Hua Li,Runmin Cong,Sam Kwong*

Main category: cs.CV

TL;DR: 本文提出了一种用于光学遥感图像显著性检测的新方法GCRPNet，克服了现有方法在整合全局和局部特征时的局限性。


<details>
  <summary>Details</summary>
Motivation: 光学遥感图像显著性检测面临目标尺度变化大和目标与背景对比度低等挑战，现有方法在高效整合全局与局部特征方面表现有限。

Method: 设计了基于Mamba架构的GCRPNet网络，包括VSS编码器提取多尺度特征，以及DS-HGAM模块和LEVSS解码器模块来强化跨层次特征交互和局部建模能力。

Result: 实验结果表明，该方法性能达到当前最优水平，验证了其有效性和优越性。

Conclusion: GCRPNet通过图增强的上下文和区域感知，改进了模型的特征表达能力，能更有效地区分前景与背景，适用于光学遥感图像的显著性检测。

Abstract: Salient object detection (SOD) in optical remote sensing images (ORSIs) faces
numerous challenges, including significant variations in target scales and low
contrast between targets and the background. Existing methods based on vision
transformers (ViTs) and convolutional neural networks (CNNs) architectures aim
to leverage both global and local features, but the difficulty in effectively
integrating these heterogeneous features limits their overall performance. To
overcome these limitations, we propose a graph-enhanced contextual and regional
perception network (GCRPNet), which builds upon the Mamba architecture to
simultaneously capture long-range dependencies and enhance regional feature
representation. Specifically, we employ the visual state space (VSS) encoder to
extract multi-scale features. To further achieve deep guidance and enhancement
of these features, we first design a difference-similarity guided hierarchical
graph attention module (DS-HGAM). This module strengthens cross-layer
interaction capabilities between features of different scales while enhancing
the model's structural perception,allowing it to distinguish between foreground
and background more effectively. Then, we design the LEVSS block as the decoder
of GCRPNet. This module integrates our proposed adaptive scanning strategy and
multi-granularity collaborative attention enhancement module (MCAEM). It
performs adaptive patch scanning on feature maps processed via multi-scale
convolutions, thereby capturing rich local region information and enhancing
Mamba's local modeling capability. Extensive experimental results demonstrate
that the proposed model achieves state-of-the-art performance, validating its
effectiveness and superiority.

</details>


### [49] [PSScreen: Partially Supervised Multiple Retinal Disease Screening](https://arxiv.org/abs/2508.10549)
*Boyi Zheng,Qing Liu*

Main category: cs.CV

TL;DR: PSScreen模型通过双流架构和文本指导特性分离，并通过特征蒸馏和伪标签一致性等方法提升多视网膜疾病筛查性能。


<details>
  <summary>Details</summary>
Motivation: 减少对完全标注数据集的依赖，并解决不同医疗站点数据域转移和部分类别标签缺失的问题。

Method: 提出PSScreen模型，采用双流架构分别学习确定性和概率性特征，通过不确定性注入、特征蒸馏、伪标签一致性和自蒸馏方法增强性能。

Result: 在六种视网膜疾病及正常状态检测中显著提升性能，在域内和域外数据集上达到最新最优结果。

Conclusion: PSScreen有效解决了部分标注数据训练和域转移问题，显著提升了多视网膜疾病筛查性能。

Abstract: Leveraging multiple partially labeled datasets to train a model for multiple
retinal disease screening reduces the reliance on fully annotated datasets, but
remains challenging due to significant domain shifts across training datasets
from various medical sites, and the label absent issue for partial classes. To
solve these challenges, we propose PSScreen, a novel Partially Supervised
multiple retinal disease Screening model. Our PSScreen consists of two streams
and one learns deterministic features and the other learns probabilistic
features via uncertainty injection. Then, we leverage the textual guidance to
decouple two types of features into disease-wise features and align them via
feature distillation to boost the domain generalization ability. Meanwhile, we
employ pseudo label consistency between two streams to address the label absent
issue and introduce a self-distillation to transfer task-relevant semantics
about known classes from the deterministic to the probabilistic stream to
further enhance the detection performances. Experiments show that our PSScreen
significantly enhances the detection performances on six retinal diseases and
the normal state averagely and achieves state-of-the-art results on both
in-domain and out-of-domain datasets. Codes are available at
https://github.com/boyiZheng99/PSScreen.

</details>


### [50] [AR Surgical Navigation With Surface Tracing: Comparing In-SitVisualization with Tool-Tracking Guidance for Neurosurgical Applications](https://arxiv.org/abs/2508.10554)
*Marc J. Fischer,Jeffrey Potts,Gabriel Urreola,Dax Jones,Paolo Palmisciano,E. Bradley Strong,Branden Cord,Andrew D. Hernandez,Julia D. Sharma,E. Brandon Strong*

Main category: cs.CV

TL;DR: 增强现实(AR)手术导航系统展示了一种通过微软HoloLens 2进行实时仪器导航的新方法，在精确导航插管过程中表现优越。


<details>
  <summary>Details</summary>
Motivation: 解决传统导航系统的局限性，以及现有AR技术在深度感知和遮挡处理方面的不足，提升手术精确性。

Method: 采用了一种新颖的表面追踪方法结合实时红外工具跟踪技术，通过HoloLens 2板载传感器实现目标注册和实时导航。

Result: 实验表明实时工具跟踪导航在准确性方面优于静态可视化，并在用户主观评价中获得喜爱。

Conclusion: 该系统在提高手术导航准确性和用户满意度方面具有显著潜力，并展示了AR在术中导航系统中的应用价值。

Abstract: Augmented Reality (AR) surgical navigation systems are emerging as the next
generation of intraoperative surgical guidance, promising to overcome
limitations of traditional navigation systems. However, known issues with AR
depth perception due to vergence-accommodation conflict and occlusion handling
limitations of the currently commercially available display technology present
acute challenges in surgical settings where precision is paramount. This study
presents a novel methodology for utilizing AR guidance to register anatomical
targets and provide real-time instrument navigation using placement of
simulated external ventricular drain catheters on a phantom model as the
clinical scenario. The system registers target positions to the patient through
a novel surface tracing method and uses real-time infrared tool tracking to aid
in catheter placement, relying only on the onboard sensors of the Microsoft
HoloLens 2. A group of intended users performed the procedure of simulated
insertions under two AR guidance conditions: static in-situ visualization,
where planned trajectories are overlaid directly onto the patient anatomy, and
real-time tool-tracking guidance, where live feedback of the catheter's pose is
provided relative to the plan. Following the insertion tests, computed
tomography scans of the phantom models were acquired, allowing for evaluation
of insertion accuracy, target deviation, angular error, and depth precision.
System Usability Scale surveys assessed user experience and cognitive workload.
Tool-tracking guidance improved performance metrics across all accuracy
measures and was preferred by users in subjective evaluations. A free copy of
this paper and all supplemental materials are available at
https://bit.ly/45l89Hq.

</details>


### [51] [Retrieval-Augmented Prompt for OOD Detection](https://arxiv.org/abs/2508.10556)
*Ruisong Han,Zongbo Han,Jiahao Zhang,Mingyue Cheng,Changqing Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为RAP（Retrieval-Augmented Prompt）的新方法，用于改进OOD检测，利用外部知识增强语义监督，并显著提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有的OOD检测方法由于缺乏足够的语义监督和外来样本匹配性差，表现较为有限，因此需要一种更有效的解决方案。

Method: 通过检索外部知识增强预训练视觉-语言模型的提示（prompts），从而提供改进的语义监督，并动态更新提示以适应测试环境。

Result: 实验表明，RAP在OOD检测基准上实现了SOTA效果，其中在ImageNet-1k数据集上，1-shot OOD检测的平均FPR95降低了7.05%，AUROC提升了1.71%。

Conclusion: RAP方法通过外部知识检索和动态提示更新显著提高了OOD检测的性能，是一种有效的改进方案。

Abstract: Out-of-Distribution (OOD) detection is crucial for the reliable deployment of
machine learning models in-the-wild, enabling accurate identification of test
samples that differ from the training data distribution. Existing methods rely
on auxiliary outlier samples or in-distribution (ID) data to generate outlier
information for training, but due to limited outliers and their mismatch with
real test OOD samples, they often fail to provide sufficient semantic
supervision, leading to suboptimal performance. To address this, we propose a
novel OOD detection method called Retrieval-Augmented Prompt (RAP). RAP
augments a pre-trained vision-language model's prompts by retrieving external
knowledge, offering enhanced semantic supervision for OOD detection. During
training, RAP retrieves descriptive words for outliers based on joint
similarity with external textual knowledge and uses them to augment the model's
OOD prompts. During testing, RAP dynamically updates OOD prompts in real-time
based on the encountered OOD samples, enabling the model to rapidly adapt to
the test environment. Our extensive experiments demonstrate that RAP achieves
state-of-the-art performance on large-scale OOD detection benchmarks. For
example, in 1-shot OOD detection on the ImageNet-1k dataset, RAP reduces the
average FPR95 by 7.05% and improves the AUROC by 1.71% compared to previous
methods. Additionally, comprehensive ablation studies validate the
effectiveness of each module and the underlying motivations of our approach.

</details>


### [52] [PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D Perception Tasks](https://arxiv.org/abs/2508.10557)
*Xinhao Wang,Zhiwei Lin,Zhongyu Xia,Yongtao Wang*

Main category: cs.CV

TL;DR: 提出了一种名为PTQAT的混合量化算法，结合PTQ和QAT的优点，提高了3D感知网络量化效率。


<details>
  <summary>Details</summary>
Motivation: PTQ导致性能下降而QAT需要高计算资源，亟需一种平衡精度和资源的新方法。

Method: 提出PTQAT，选择关键性层进行QAT微调，其余层采用PTQ，并优化量化误差传播处理。

Result: 在不同任务（如目标检测、语义分割等）上性能优于QAT基线，且能冻结近50%可量化层，支持多种架构和位宽。

Conclusion: PTQAT在保证高精度的同时显著提高了效率，可广泛应用于多种3D感知任务。

Abstract: Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT)
represent two mainstream model quantization approaches. However, PTQ often
leads to unacceptable performance degradation in quantized models, while QAT
imposes substantial GPU memory requirements and extended training time due to
weight fine-tuning.In this paper, we propose PTQAT, a novel general hybrid
quantization algorithm for the efficient deployment of 3D perception networks.
To address the speed accuracy trade-off between PTQ and QAT, our method selects
critical layers for QAT fine-tuning and performs PTQ on the remaining layers.
Contrary to intuition, fine-tuning the layers with smaller output discrepancies
before and after quantization, rather than those with larger discrepancies,
actually leads to greater improvements in the model's quantization accuracy.
This means we better compensate for quantization errors during their
propagation, rather than addressing them at the point where they occur. The
proposed PTQAT achieves similar performance to QAT with more efficiency by
freezing nearly 50% of quantifiable layers. Additionally, PTQAT is a universal
quantization method that supports various quantization bit widths (4 bits) as
well as different model architectures, including CNNs and Transformers. The
experimental results on nuScenes across diverse 3D perception tasks, including
object detection, semantic segmentation, and occupancy prediction, show that
our method consistently outperforms QAT-only baselines. Notably, it achieves
0.2%-0.9% NDS and 0.3%-1.0% mAP gains in object detection, 0.3%-2.0% mIoU gains
in semantic segmentation and occupancy prediction while fine-tuning fewer
weights.

</details>


### [53] [HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head Synthesis](https://arxiv.org/abs/2508.10566)
*Shiyu Liu,Kui Jiang,Xianming Liu,Hongxun Yao,Xiaocheng Feng*

Main category: cs.CV

TL;DR: 本文提出了HM-Talker，一个音频驱动的高保真和时序一致的说话人视频生成框架，创新性地结合了显式与隐式的运动表示。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成说话人视频时容易产生运动模糊及嘴唇抖动问题，主要因为缺乏显式的发音运动学先验。

Method: 提出了HM-Talker框架，结合隐式和显式的运动表示，其中显式特征利用基于面部肌肉动作单元（AUs）表示的运动信息。开发了CMDM模块提取互补运动特征，并引入HMMM模块以增强对不同身份的泛化能力。

Result: 实验证明HM-Talker在视觉质量和唇同步精度方面优于现有的先进方法。

Conclusion: HM-Talker通过显式与隐式特征的融合，有效提升了音频驱动说话人视频生成的质量和通用性。

Abstract: Audio-driven talking head video generation enhances user engagement in
human-computer interaction. However, current methods frequently produce videos
with motion blur and lip jitter, primarily due to their reliance on implicit
modeling of audio-facial motion correlations--an approach lacking explicit
articulatory priors (i.e., anatomical guidance for speech-related facial
movements). To overcome this limitation, we propose HM-Talker, a novel
framework for generating high-fidelity, temporally coherent talking heads.
HM-Talker leverages a hybrid motion representation combining both implicit and
explicit motion cues. Explicit cues use Action Units (AUs), anatomically
defined facial muscle movements, alongside implicit features to minimize
phoneme-viseme misalignment. Specifically, our Cross-Modal Disentanglement
Module (CMDM) extracts complementary implicit/explicit motion features while
predicting AUs directly from audio input aligned to visual cues. To mitigate
identity-dependent biases in explicit features and enhance cross-subject
generalization, we introduce the Hybrid Motion Modeling Module (HMMM). This
module dynamically merges randomly paired implicit/explicit features, enforcing
identity-agnostic learning. Together, these components enable robust lip
synchronization across diverse identities, advancing personalized talking head
synthesis. Extensive experiments demonstrate HM-Talker's superiority over
state-of-the-art methods in visual quality and lip-sync accuracy.

</details>


### [54] [SpaRC-AD: A Baseline for Radar-Camera Fusion in End-to-End Autonomous Driving](https://arxiv.org/abs/2508.10567)
*Philipp Wolters,Johannes Gilg,Torben Teepe,Gerhard Rigoll*

Main category: cs.CV

TL;DR: 该研究提出了一个名为SpaRC-AD的相机和雷达融合框架，用于优化自动驾驶中的感知、预测和规划，并在多个基准测试中表现出超越现有视觉方法的显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 针对基于视觉的自动驾驶方法在天气恶劣、部分遮挡及速度精确估计方面的局限性，该研究希望解决这些在安全场景中的关键挑战，如碰撞规避所需的精确运动理解与长时间序列轨迹预测问题。

Method: 提出了一种名为SpaRC-AD的端到端相机和雷达融合框架，通过稀疏的3D特征对齐和基于多普勒的速度估计，优化3D场景表示以改进代理锚点、地图折线和运动建模。

Result: 在多个自动驾驶任务中，相较于仅基于视觉的方法，该方法表现出显著提升，包括3D检测（+4.8%mAP）、多目标跟踪（+8.3%AMOTA）、在线地图构建（+1.8%mAP）、运动预测（-4% mADE）和轨迹规划（-0.1m L2和-9% TPC）。

Conclusion: 该研究表明，基于雷达的融合在需要精确运动理解和长时间序列预测的安全关键场景中效果显著，并在多个公开基准和模拟测试中展示了其空间和时间一致性优势。

Abstract: End-to-end autonomous driving systems promise stronger performance through
unified optimization of perception, motion forecasting, and planning. However,
vision-based approaches face fundamental limitations in adverse weather
conditions, partial occlusions, and precise velocity estimation - critical
challenges in safety-sensitive scenarios where accurate motion understanding
and long-horizon trajectory prediction are essential for collision avoidance.
To address these limitations, we propose SpaRC-AD, a query-based end-to-end
camera-radar fusion framework for planning-oriented autonomous driving. Through
sparse 3D feature alignment, and doppler-based velocity estimation, we achieve
strong 3D scene representations for refinement of agent anchors, map polylines
and motion modelling. Our method achieves strong improvements over the
state-of-the-art vision-only baselines across multiple autonomous driving
tasks, including 3D detection (+4.8% mAP), multi-object tracking (+8.3% AMOTA),
online mapping (+1.8% mAP), motion prediction (-4.0% mADE), and trajectory
planning (-0.1m L2 and -9% TPC). We achieve both spatial coherence and temporal
consistency on multiple challenging benchmarks, including real-world open-loop
nuScenes, long-horizon T-nuScenes, and closed-loop simulator Bench2Drive. We
show the effectiveness of radar-based fusion in safety-critical scenarios where
accurate motion understanding and long-horizon trajectory prediction are
essential for collision avoidance. The source code of all experiments is
available at https://phi-wol.github.io/sparcad/

</details>


### [55] [Adapting SAM via Cross-Entropy Masking for Class Imbalance in Remote Sensing Change Detection](https://arxiv.org/abs/2508.10568)
*Humza Naveed,Xina Zeng,Mitch Bryson,Nagita Mehrseresht*

Main category: cs.CV

TL;DR: 通过微调Segment Anything Model（SAM）编码器，并结合时空特征增强（STFE）和多尺度解码器融合（MSDF），显著提升了遥感变化检测性能，提出了一种新型的交叉熵掩模（CEM）损失方法应对类别不平衡问题，并在四个数据集上超越了现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型在计算机视觉领域取得了显著的成就，但遥感变化检测领域面临高精细尺度和类别不平衡的挑战，传统方法难以满足需求。因此提出了一种融合先进模型与新方法的创新方案。

Method: 采用微调方式调整SAM编码器，同时结合时空特征增强（STFE）、多尺度解码器融合（MSDF）和新型交叉熵掩模（CEM）损失，设计出适用于多尺度高复杂数据的改变检测技术。

Result: 该方法在四个变化检测数据集（Levir-CD、WHU-CD、CLCD和S2Looking）上均超越当前最优方法，尤其是在复杂的大规模数据集S2Looking上提升了2.5%的F1分数。

Conclusion: 通过对SAM模型的创新性改造，结合新损失函数和特征融合机制，为遥感变化检测领域提供了更为强大和通用的方法，验证了深度学习模型在领域迁移中的巨大潜力。

Abstract: Foundational models have achieved significant success in diverse domains of
computer vision. They learn general representations that are easily
transferable to tasks not seen during training. One such foundational model is
Segment anything model (SAM), which can accurately segment objects in images.
We propose adapting the SAM encoder via fine-tuning for remote sensing change
detection (RSCD) along with spatial-temporal feature enhancement (STFE) and
multi-scale decoder fusion (MSDF) to detect changes robustly at multiple
scales. Additionally, we propose a novel cross-entropy masking (CEM) loss to
handle high class imbalance in change detection datasets. Our method
outperforms state-of-the-art (SOTA) methods on four change detection datasets,
Levir-CD, WHU-CD, CLCD, and S2Looking. We achieved 2.5% F1-score improvement on
a large complex S2Looking dataset. The code is available at:
https://github.com/humza909/SAM-CEM-CD

</details>


### [56] [Towards Agentic AI for Multimodal-Guided Video Object Segmentation](https://arxiv.org/abs/2508.10572)
*Tuyen Tran,Thao Minh Le,Truyen Tran*

Main category: cs.CV

TL;DR: 本文探讨了利用视觉语言基础模型进行基于参照的多模态视频目标分割任务，提出了一个更灵活的多模态智能体系统，比现有方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有任务特定模型复杂度高且灵活性不足，需要一种灵活性更高且无需新训练的方法。

Method: 提出一种多模态智能体系统，通过大语言模型动态生成工作流，结合特定工具逐步识别多模态线索中的目标对象。

Result: 在多模态视频目标分割任务（RVOS和Ref-AVS）上，提出的方法明显优于现有方法。

Conclusion: 多模态智能体系统利用语言模型和工具交互，能够适应动态任务，实现更优性能。

Abstract: Referring-based Video Object Segmentation is a multimodal problem that
requires producing fine-grained segmentation results guided by external cues.
Traditional approaches to this task typically involve training specialized
models, which come with high computational complexity and manual annotation
effort. Recent advances in vision-language foundation models open a promising
direction toward training-free approaches. Several studies have explored
leveraging these general-purpose models for fine-grained segmentation,
achieving performance comparable to that of fully supervised, task-specific
models. However, existing methods rely on fixed pipelines that lack the
flexibility needed to adapt to the dynamic nature of the task. To address this
limitation, we propose Multi-Modal Agent, a novel agentic system designed to
solve this task in a more flexible and adaptive manner. Specifically, our
method leverages the reasoning capabilities of large language models (LLMs) to
generate dynamic workflows tailored to each input. This adaptive procedure
iteratively interacts with a set of specialized tools designed for low-level
tasks across different modalities to identify the target object described by
the multimodal cues. Our agentic approach demonstrates clear improvements over
prior methods on two multimodal-conditioned VOS tasks: RVOS and Ref-AVS.

</details>


### [57] [HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs](https://arxiv.org/abs/2508.10576)
*Zheng Qin,Ruobing Zheng,Yabing Wang,Tianqi Li,Yi Yuan,Jingdong Chen,Le Wang*

Main category: cs.CV

TL;DR: 提出了HumanSense基准，用于评估多模态大语言模型在人类中心场景中的感知和交互能力，并通过多阶段模态渐进强化学习方法提升模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型缺乏评估其在人类中心场景下表现的精细框架，尤其是在理解复杂的人类意图和提供情感化、上下文感知回应方面。

Method: 设计HumanSense基准，评估模型对多模态上下文的感知及反馈生成能力。通过多阶段模态渐进强化学习提升模型推理能力，并通过设计提示在无需训练的情况下提升非推理模型的表现。

Result: 通过实验发现，补充视觉输入的音频和文本信息显著提高了模型表现，多模态模型在交互任务中表现更佳。强化推理能力的方法显著改善了评估结果。

Conclusion: 模型在高级交互任务上的表现仍有改进空间，推理能力是实现有效反馈的关键，通过多模态信息整合和推理优化可以显著提升人类中心任务中的表现。

Abstract: While Multimodal Large Language Models (MLLMs) show immense promise for
achieving truly human-like interactions, progress is hindered by the lack of
fine-grained evaluation frameworks for human-centered scenarios, encompassing
both the understanding of complex human intentions and the provision of
empathetic, context-aware responses. Here we introduce HumanSense, a
comprehensive benchmark designed to evaluate the human-centered perception and
interaction capabilities of MLLMs, with a particular focus on deep
understanding of extended multimodal contexts and the formulation of rational
feedback. Our evaluation reveals that leading MLLMs still have considerable
room for improvement, particularly for advanced interaction-oriented tasks.
Supplementing visual input with audio and text information yields substantial
improvements, and Omni-modal models show advantages on these tasks.
Furthermore, we argue that appropriate feedback stems from a contextual
analysis of the interlocutor's needs and emotions, with reasoning ability
serving as the key to unlocking it. Accordingly, we employ a multi-stage,
modality-progressive reinforcement learning to enhance the reasoning abilities
of an Omni model, achieving substantial gains on evaluation results.
Additionally, we observe that successful reasoning processes exhibit highly
consistent thought patterns. By designing corresponding prompts, we also
enhance the performance of non-reasoning models in a training-free manner.
Project page:
\textcolor{brightpink}https://digital-avatar.github.io/ai/HumanSense/

</details>


### [58] [EvTurb: Event Camera Guided Turbulence Removal](https://arxiv.org/abs/2508.10582)
*Yixing Liu,Minggui Teng,Yifei Xia,Peiqi Duan,Boxin Shi*

Main category: cs.CV

TL;DR: 本文提出了EvTurb框架，该框架利用高速事件流解决大气湍流引起的图像模糊和倾斜失真问题，并引入首个真实捕获的湍流数据集TurbEvent。


<details>
  <summary>Details</summary>
Motivation: 针对大气湍流引起的图像质量下降问题，现有方法在单帧和多帧图像下均难以有效处理，因此作者期望开发一种新方法解决湍流引起的模糊和倾斜失真分离问题。

Method: 提出一种基于事件流的两步网络：首先使用事件积分减少模糊，其次采用源自原始事件流的方差图消除倾斜失真，同时引入了第一个真实场景湍流数据集TurbEvent。

Result: 实验表明，EvTurb在保持计算效率的同时，在去湍流任务中超越了现有最先进的方法。

Conclusion: EvTurb成功通过事件流解耦模糊与倾斜失真，并在真实湍流数据集上展现了优越性能。

Abstract: Atmospheric turbulence degrades image quality by introducing blur and
geometric tilt distortions, posing significant challenges to downstream
computer vision tasks. Existing single-image and multi-frame methods struggle
with the highly ill-posed nature of this problem due to the compositional
complexity of turbulence-induced distortions. To address this, we propose
EvTurb, an event guided turbulence removal framework that leverages high-speed
event streams to decouple blur and tilt effects. EvTurb decouples blur and tilt
effects by modeling event-based turbulence formation, specifically through a
novel two-step event-guided network: event integrals are first employed to
reduce blur in the coarse outputs. This is followed by employing a variance
map, derived from raw event streams, to eliminate the tilt distortion for the
refined outputs. Additionally, we present TurbEvent, the first real-captured
dataset featuring diverse turbulence scenarios. Experimental results
demonstrate that EvTurb surpasses state-of-the-art methods while maintaining
computational efficiency.

</details>


### [59] [Towards Powerful and Practical Patch Attacks for 2D Object Detection in Autonomous Driving](https://arxiv.org/abs/2508.10600)
*Yuxin Cao,Yedi Zhang,Wentao He,Yifan Liao,Yan Xiao,Chang Li,Zhiyong Huang,Jin Song Dong*

Main category: cs.CV

TL;DR: 作者提出了一种名为P$^3$A的新框架，用于优化高分辨率下的2D物体检测对抗补丁攻击，提出了PASR和LCSL，并加入PSPP作为预处理步骤，与现有方法相比表现优越。


<details>
  <summary>Details</summary>
Motivation: 针对自动驾驶中2D物体检测系统易受到对抗性补丁攻击的问题，特别是黑盒攻击的高成功率及其在高分辨率数据集上的严峻挑战。

Method: 提出了Practical Attack Success Rate (PASR)作为新指标，设计了Localization-Confidence Suppression Loss (LCSL)以提升攻击效果，并引入Probabilistic Scale-Preserving Padding (PSPP)作为数据预处理步骤。

Result: P$^3$A在未见过的模型和高分辨率数据集上的表现超越了现有方法，无论是在新提出的PASR指标还是传统的mAP指标下。

Conclusion: P$^3$A框架在提高对抗补丁攻击转移性和高分辨率适应性方面具有显著优势，展示了在自动驾驶应用中的重要潜力。

Abstract: Learning-based autonomous driving systems remain critically vulnerable to
adversarial patches, posing serious safety and security risks in their
real-world deployment. Black-box attacks, notable for their high attack success
rate without model knowledge, are especially concerning, with their
transferability extensively studied to reduce computational costs compared to
query-based attacks. Previous transferability-based black-box attacks typically
adopt mean Average Precision (mAP) as the evaluation metric and design training
loss accordingly. However, due to the presence of multiple detected bounding
boxes and the relatively lenient Intersection over Union (IoU) thresholds, the
attack effectiveness of these approaches is often overestimated, resulting in
reduced success rates in practical attacking scenarios. Furthermore, patches
trained on low-resolution data often fail to maintain effectiveness on
high-resolution images, limiting their transferability to autonomous driving
datasets. To fill this gap, we propose P$^3$A, a Powerful and Practical Patch
Attack framework for 2D object detection in autonomous driving, specifically
optimized for high-resolution datasets. First, we introduce a novel metric,
Practical Attack Success Rate (PASR), to more accurately quantify attack
effectiveness with greater relevance for pedestrian safety. Second, we present
a tailored Localization-Confidence Suppression Loss (LCSL) to improve attack
transferability under PASR. Finally, to maintain the transferability for
high-resolution datasets, we further incorporate the Probabilistic
Scale-Preserving Padding (PSPP) into the patch attack pipeline as a data
preprocessing step. Extensive experiments show that P$^3$A outperforms
state-of-the-art attacks on unseen models and unseen high-resolution datasets,
both under the proposed practical IoU-based evaluation metric and the previous
mAP-based metrics.

</details>


### [60] [Fourier-Guided Attention Upsampling for Image Super-Resolution](https://arxiv.org/abs/2508.10616)
*Daejune Choi,Youchan No,Jinhyung Lee,Duksu Kim*

Main category: cs.CV

TL;DR: 提出了一种名为频率引导注意力(FGA)的轻量级上采样模块，用于单图像超分辨率，能够提升高频细节重建效果并减少混叠伪影。


<details>
  <summary>Details</summary>
Motivation: 常规上采样方法效率高但存在高频细节重建能力差和混叠伪影问题，提出FGA是为了解决这些问题。

Method: FGA模块包括：基于傅里叶特征的多层感知机进行频率编码、跨分辨率的相关注意力层进行自适应空间对齐，以及频域L1损失用于频谱一致性监督。

Result: FGA仅增加0.3M参数，增强了五种超分辨率骨干网络性能，PSNR提升0.12~0.14 dB，频域一致性提高29%，尤其在纹理丰富的数据集上表现明显。

Conclusion: FGA有效减少混叠，保留细节，是传统上采样方法的实用、可扩展替代方案。

Abstract: We propose Frequency-Guided Attention (FGA), a lightweight upsampling module
for single image super-resolution. Conventional upsamplers, such as Sub-Pixel
Convolution, are efficient but frequently fail to reconstruct high-frequency
details and introduce aliasing artifacts. FGA addresses these issues by
integrating (1) a Fourier feature-based Multi-Layer Perceptron (MLP) for
positional frequency encoding, (2) a cross-resolution Correlation Attention
Layer for adaptive spatial alignment, and (3) a frequency-domain L1 loss for
spectral fidelity supervision. Adding merely 0.3M parameters, FGA consistently
enhances performance across five diverse super-resolution backbones in both
lightweight and full-capacity scenarios. Experimental results demonstrate
average PSNR gains of 0.12~0.14 dB and improved frequency-domain consistency by
up to 29%, particularly evident on texture-rich datasets. Visual and spectral
evaluations confirm FGA's effectiveness in reducing aliasing and preserving
fine details, establishing it as a practical, scalable alternative to
traditional upsampling methods.

</details>


### [61] [FIND-Net -- Fourier-Integrated Network with Dictionary Kernels for Metal Artifact Reduction](https://arxiv.org/abs/2508.10617)
*Farid Tasharofi,Fuxin Fan,Melika Qahqaie,Mareike Thies,Andreas Maier*

Main category: cs.CV

TL;DR: 提出FIND-Net，一个结合空间与频域处理的金属伪影去除框架，以提升CT图像质量。


<details>
  <summary>Details</summary>
Motivation: CT成像中的金属伪影严重影响图像质量和诊断效果，现有算法在抑制伪影与保持结构细节方面存在挑战，需要改进方法。

Method: FIND-Net结合快速傅里叶卷积（FFC）与可训练高斯滤波器，通过空间与频域相结合的方式处理金属伪影，增强上下文理解与频率选择性能力。

Result: 在合成数据集上实现MAE减少3.07%，SSIM提高0.18%，PSNR增加0.90%；在真实临床CT数据集上有效抑制金属伪影且最小化对正常解剖结构的影响。

Conclusion: FIND-Net在金属伪影去除与结构细节保留方面表现优异，显示出其在临床应用中的潜力。

Abstract: Metal artifacts, caused by high-density metallic implants in computed
tomography (CT) imaging, severely degrade image quality, complicating diagnosis
and treatment planning. While existing deep learning algorithms have achieved
notable success in Metal Artifact Reduction (MAR), they often struggle to
suppress artifacts while preserving structural details. To address this
challenge, we propose FIND-Net (Fourier-Integrated Network with Dictionary
Kernels), a novel MAR framework that integrates frequency and spatial domain
processing to achieve superior artifact suppression and structural
preservation. FIND-Net incorporates Fast Fourier Convolution (FFC) layers and
trainable Gaussian filtering, treating MAR as a hybrid task operating in both
spatial and frequency domains. This approach enhances global contextual
understanding and frequency selectivity, effectively reducing artifacts while
maintaining anatomical structures. Experiments on synthetic datasets show that
FIND-Net achieves statistically significant improvements over state-of-the-art
MAR methods, with a 3.07% MAE reduction, 0.18% SSIM increase, and 0.90% PSNR
improvement, confirming robustness across varying artifact complexities.
Furthermore, evaluations on real-world clinical CT scans confirm FIND-Net's
ability to minimize modifications to clean anatomical regions while effectively
suppressing metal-induced distortions. These findings highlight FIND-Net's
potential for advancing MAR performance, offering superior structural
preservation and improved clinical applicability. Code is available at
https://github.com/Farid-Tasharofi/FIND-Net

</details>


### [62] [Increasing the Utility of Synthetic Images through Chamfer Guidance](https://arxiv.org/abs/2508.10631)
*Nicola Dall'Asen,Xiaofeng Zhang,Reyhane Askari Hemmat,Melissa Hall,Jakob Verbeek,Adriana Romero-Soriano,Michal Drozdzal*

Main category: cs.CV

TL;DR: 本论文提出了一种名为Chamfer Guidance的新方法，旨在通过少量真实图像样本指导生成模型，提升合成数据的多样性和质量，并大幅改善了在下游任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 近年来生成质量的提升导致了生成多样性的下降，限制了合成数据作为训练数据集的实用性。同时，现有的方法未充分考虑合成数据与真实数据之间的分布偏移问题。

Method: 提出Chamfer Guidance方法，通过少量真实示例图像，无需额外训练，即可在合成数据生成过程中平衡质量与多样性，并显著减少计算量开销。

Result: 通过实验验证，该方法在ImageNet-1k和其他基准中提升了生成多样性和质量，同时在下游分类任务中的性能提升显著（例如分布内达15%、分布外达16%）。

Conclusion: Chamfer Guidance是一种高效且少样本依赖的指导方法，可大幅提升合成数据的实用性，并显著降低生成过程中的计算成本。

Abstract: Conditional image generative models hold considerable promise to produce
infinite amounts of synthetic training data. Yet, recent progress in generation
quality has come at the expense of generation diversity, limiting the utility
of these models as a source of synthetic training data. Although guidance-based
approaches have been introduced to improve the utility of generated data by
focusing on quality or diversity, the (implicit or explicit) utility functions
oftentimes disregard the potential distribution shift between synthetic and
real data. In this work, we introduce Chamfer Guidance: a training-free
guidance approach which leverages a handful of real exemplar images to
characterize the quality and diversity of synthetic data. We show that by
leveraging the proposed Chamfer Guidance, we can boost the diversity of the
generations w.r.t. a dataset of real images while maintaining or improving the
generation quality on ImageNet-1k and standard geo-diversity benchmarks. Our
approach achieves state-of-the-art few-shot performance with as little as 2
exemplar real images, obtaining 96.4\% in terms of precision, and 86.4\% in
terms of distributional coverage, which increase to 97.5\% and 92.7\%,
respectively, when using 32 real images. We showcase the benefits of the
Chamfer Guidance generation by training downstream image classifiers on
synthetic data, achieving accuracy boost of up to 15\% for in-distribution over
the baselines, and up to 16\% in out-of-distribution. Furthermore, our approach
does not require using the unconditional model, and thus obtains a 31\%
reduction in FLOPs w.r.t. classifier-free-guidance-based approaches at sampling
time.

</details>


### [63] [ChatENV: An Interactive Vision-Language Model for Sensor-Guided Environmental Monitoring and Scenario Simulation](https://arxiv.org/abs/2508.10635)
*Hosam Elgendy,Ahmed Sharshar,Ahmed Aboeitta,Mohsen Guizani*

Main category: cs.CV

TL;DR: 本文介绍了ChatENV，一个用于解释卫星图像和传感器数据的互动视觉语言模型（VLM），为环境监测提供了新的方法。


<details>
  <summary>Details</summary>
Motivation: 当前VLM在分析环境变化时存在局限，包括忽视传感器因果信号、依赖单一来源的字幕、缺乏交互性情景推理。ChatENV旨在弥补这些不足。

Method: 提出ChatENV，包括:(i)创建包含丰富传感器元数据的177k图像数据集;(ii)利用GPT-4和Gemini 2.0进行注释;(iii)通过LoRA适配器对Qwen-2.5-VL进行微调，用于交互聊天。

Result: ChatENV在时间和假设推理任务上表现强劲（如BERT-F1得分0.903），并在支持互动情境分析的同时超越或媲美现有模型。

Conclusion: ChatENV证明了环境监测中结合传感器数据与卫星图像互动分析的潜力，为基于传感器的环境监测提供了强大工具。

Abstract: Understanding environmental changes from aerial imagery is vital for climate
resilience, urban planning, and ecosystem monitoring. Yet, current vision
language models (VLMs) overlook causal signals from environmental sensors, rely
on single-source captions prone to stylistic bias, and lack interactive
scenario-based reasoning. We present ChatENV, the first interactive VLM that
jointly reasons over satellite image pairs and real-world sensor data. Our
framework: (i) creates a 177k-image dataset forming 152k temporal pairs across
62 land-use classes in 197 countries with rich sensor metadata (e.g.,
temperature, PM10, CO); (ii) annotates data using GPT- 4o and Gemini 2.0 for
stylistic and semantic diversity; and (iii) fine-tunes Qwen-2.5-VL using
efficient Low-Rank Adaptation (LoRA) adapters for chat purposes. ChatENV
achieves strong performance in temporal and "what-if" reasoning (e.g., BERT-F1
0.903) and rivals or outperforms state-of-the-art temporal models, while
supporting interactive scenario-based analysis. This positions ChatENV as a
powerful tool for grounded, sensor-aware environmental monitoring.

</details>


### [64] [Processing and acquisition traces in visual encoders: What does CLIP know about your camera?](https://arxiv.org/abs/2508.10637)
*Ryan Ramos,Vladan Stojnić,Giorgos Kordopatis-Zilos,Yuta Nakashima,Giorgos Tolias,Noa Garcia*

Main category: cs.CV

TL;DR: 该论文探讨了视觉编码器对细微甚至人眼难以察觉的图像采集和处理参数变化的鲁棒性，发现这些参数对语义预测的影响显著，并受到语义标签与这些参数间相关性的影响。


<details>
  <summary>Details</summary>
Motivation: 以往研究主要关注图像中的明显破损或变形对视觉编码器鲁棒性的影响，而该论文旨在研究更细微的变化（例如图像采集和处理参数变化）对视觉表示学习的影响。

Method: 作者分析了视觉编码器在处理细微和不易察觉的图像采集与处理参数时的表现，通过实验验证这些参数如何被编码并影响语义预测。

Result: 发现图像采集与处理参数被系统性地编码进视觉表示中，其存在显著影响语义预测性能，且此影响受这些参数与语义标签的相关性或反相关性所决定。

Conclusion: 细微的图像采集和处理变化可以对视觉编码器的性能产生深远影响，应予以更多关注。

Abstract: Prior work has analyzed the robustness of visual encoders to image
transformations and corruptions, particularly in cases where such alterations
are not seen during training. When this occurs, they introduce a form of
distribution shift at test time, often leading to performance degradation. The
primary focus has been on severe corruptions that, when applied aggressively,
distort useful signals necessary for accurate semantic predictions.
  We take a different perspective by analyzing parameters of the image
acquisition process and transformations that may be subtle or even
imperceptible to the human eye. We find that such parameters are systematically
encoded in the learned visual representations and can be easily recovered. More
strikingly, their presence can have a profound impact, either positively or
negatively, on semantic predictions. This effect depends on whether there is a
strong correlation or anti-correlation between semantic labels and these
acquisition-based or processing-based labels. Our code and data are available
at: https://github.com/ryan-caesar-ramos/visual-encoder-traces

</details>


### [65] [Lameness detection in dairy cows using pose estimation and bidirectional LSTMs](https://arxiv.org/abs/2508.10643)
*Helena Russello,Rik van der Tol,Eldert J. van Henten,Gert Kootstra*

Main category: cs.CV

TL;DR: 研究提出了一种结合姿态估计和BLSTM神经网络的奶牛跛脚检测方法，达到了85%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 传统跛脚检测方法依赖手动的特征工程和较长的数据序列，研究意图通过自动化的方法提高准确性并减少数据需求。

Method: 利用T-LEAP姿态估计模型提取奶牛关键点轨迹，结合BLSTM分类器进行二分类的跛脚检测。

Result: 测试结果显示，与基于手动特征的方法相比，提出的方法准确率从80%提升到85%。

Conclusion: 该方法不但在准确性上优于传统方法，还能以短序列数据有效检测跛脚，具有广阔应用潜力。

Abstract: This study presents a lameness detection approach that combines pose
estimation and Bidirectional Long-Short-Term Memory (BLSTM) neural networks.
Combining pose-estimation and BLSTMs classifier offers the following
advantages: markerless pose-estimation, elimination of manual feature
engineering by learning temporal motion features from the keypoint
trajectories, and working with short sequences and small training datasets.
Motion sequences of nine keypoints (located on the cows' hooves, head and back)
were extracted from videos of walking cows with the T-LEAP pose estimation
model. The trajectories of the keypoints were then used as an input to a BLSTM
classifier that was trained to perform binary lameness classification. Our
method significantly outperformed an established method that relied on
manually-designed locomotion features: our best architecture achieved a
classification accuracy of 85%, against 80% accuracy for the feature-based
approach. Furthermore, we showed that our BLSTM classifier could detect
lameness with as little as one second of video data.

</details>


### [66] [SemPT: Semantic Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2508.10645)
*Xiao Shi,Yangjun Ou,Zhenzhong Chen*

Main category: cs.CV

TL;DR: 提出了一种名为Semantic Prompt Tuning (SemPT)的新框架，通过利用类别间共享的属性级知识来提升视觉转移学习性能。


<details>
  <summary>Details</summary>
Motivation: 解决在未知类别的视觉转移学习中，由于依赖标签或不连贯描述导致的知识迁移性受限问题。

Method: 使用两步提示策略提取视觉属性，生成属性级描述，通过视觉引导的加权减少无关属性的噪声，并在推理中动态选择增强的文本/标签嵌入结构。

Result: 实验结果表明，在15个基准数据集上的表现优于其他方法，涵盖基于基础到新类别的泛化、数据集间迁移、跨领域迁移，以及少样本学习。

Conclusion: SemPT能有效利用多种语义提示技术，平衡已见类别的区分性与未见类别的迁移性，在视觉转移学习中取得了显著的性能优势。

Abstract: Visual transfer learning for unseen categories presents an active research
topic yet a challenging task, due to the inherent conflict between preserving
category-specific representations and acquiring transferable knowledge.
Vision-Language Models (VLMs) pre-trained on large amounts of image-text pairs
offer a promising solution. However, existing prompt tuning methods rely on
sparse category labels or disparate LLM-generated descriptions, which fragment
knowledge representation and hinder transferability. To address this
limitation, we introduce Semantic Prompt Tuning (SemPT), a novel framework that
tackles the generalization challenge by leveraging shared attribute-level
knowledge across categories. Specifically, SemPT adopts a two-step prompting
strategy to guide LLM in extracting shared visual attributes and generating
attribute-level descriptions, capturing transferable semantic cues beyond
labels while ensuring coherent structure. Then, visually guided weighting is
applied to the embeddings of attribute-level descriptions to reduce noise from
irrelevant attributes and enhance the text embeddings. Additionally, image
embeddings are jointly aligned with both label and attribute-enhanced text
embeddings, balancing discrimination for seen categories and transferability to
unseen ones. Considering the availability of category exposure, our inference
dynamically selects between standard label embeddings for seen categories and
attribute-enhanced embeddings for unseen ones to ensure effective adaptation.
Extensive experiments on 15 benchmark datasets demonstrate that SemPT achieves
state-of-the-art performance across various settings, including base-to-novel
generalization, cross-dataset transfer, cross-domain transfer, and few-shot
learning.

</details>


### [67] [Serial Over Parallel: Learning Continual Unification for Multi-Modal Visual Object Tracking and Benchmarking](https://arxiv.org/abs/2508.10655)
*Zhangyong Tang,Tianyang Xu,Xuefeng Zhu,Chunyang Cheng,Tao Zhou,Xiaojun Wu,Josef Kittler*

Main category: cs.CV

TL;DR: 文章提出了一个统一的多模态视觉目标跟踪任务的基准UniBench300，并通过引入持续学习方法改进了多任务统一过程。


<details>
  <summary>Details</summary>
Motivation: 现有多模态视觉目标跟踪任务受限于缺乏一个统一的基准，导致训练和测试之间存在不一致性，进而引发性能下降。

Method: 引入一个统一基准UniBench300，减少推理次数并通过一个渐进整合方式结合任务，同时结合持续学习方法应对知识遗忘问题。

Result: 实验验证了UniBench300的意义和持续学习方法的优越性，同时揭示了网络容量与性能下降之间的负相关性，以及不同模态间的性能下降差异性。

Conclusion: UniBench300为多模态视觉研究提供了一个重要工具，并表明持续学习方法在任务统一过程中的潜在优势，为未来研究提供宝贵见解。

Abstract: Unifying multiple multi-modal visual object tracking (MMVOT) tasks draws
increasing attention due to the complementary nature of different modalities in
building robust tracking systems. Existing practices mix all data sensor types
in a single training procedure, structuring a parallel paradigm from the
data-centric perspective and aiming for a global optimum on the joint
distribution of the involved tasks. However, the absence of a unified benchmark
where all types of data coexist forces evaluations on separated benchmarks,
causing \textit{inconsistency} between training and testing, thus leading to
performance \textit{degradation}. To address these issues, this work advances
in two aspects: \ding{182} A unified benchmark, coined as UniBench300, is
introduced to bridge the inconsistency by incorporating multiple task data,
reducing inference passes from three to one and cutting time consumption by
27\%. \ding{183} The unification process is reformulated in a serial format,
progressively integrating new tasks. In this way, the performance degradation
can be specified as knowledge forgetting of previous tasks, which naturally
aligns with the philosophy of continual learning (CL), motivating further
exploration of injecting CL into the unification process. Extensive experiments
conducted on two baselines and four benchmarks demonstrate the significance of
UniBench300 and the superiority of CL in supporting a stable unification
process. Moreover, while conducting dedicated analyses, the performance
degradation is found to be negatively correlated with network capacity.
Additionally, modality discrepancies contribute to varying degradation levels
across tasks (RGBT > RGBD > RGBE in MMVOT), offering valuable insights for
future multi-modal vision research. Source codes and the proposed benchmark is
available at \textit{https://github.com/Zhangyong-Tang/UniBench300}.

</details>


### [68] [AddressVLM: Cross-view Alignment Tuning for Image Address Localization using Large Vision-Language Models](https://arxiv.org/abs/2508.10667)
*Shixiong Xu,Chenghao Zhang,Lubin Fan,Yuan Zhou,Bin Fan,Shiming Xiang,Gaofeng Meng,Jieping Ye*

Main category: cs.CV

TL;DR: 作者提出AddressVLM模型，通过结合街景图像与卫星图像，提升了在城市街道级别定位及与地址相关问题回答的能力，相较传统模型准确性提高了9%-12%。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型(LVLMs)在国家或城市级定位表现出色，但在城市内的精细化街道级定位效果较差，需要改进。

Method: 通过结合街景图像与卫星图像提出了跨视图对齐调优机制，包括卫星视角与街景视角图像的嫁接机制，以及自动标签生成机制。接着通过两阶段训练协议：跨视图对齐调优及地址定位调优，增强模型对街道分布的全局理解能力。

Result: 构建了基于匹兹堡和旧金山数据的两个街景问题回答(VQA)数据集。实验证明，与目前的LVLMs相比，AddressVLM在这两个数据集上的地址定位准确度分别提高了超过9%和12%。

Conclusion: AddressVLM成功提升了城市街景级定位及地址相关问题回答的能力，为街景及其他类似环境的精细化任务提供了新方向。

Abstract: Large visual language models (LVLMs) have demonstrated impressive performance
in coarse-grained geo-localization at the country or city level, but they
struggle with fine-grained street-level localization within urban areas. In
this paper, we explore integrating city-wide address localization capabilities
into LVLMs, facilitating flexible address-related question answering using
street-view images. A key challenge is that the street-view visual
question-and-answer (VQA) data provides only microscopic visual cues, leading
to subpar performance in fine-tuned models. To tackle this issue, we
incorporate perspective-invariant satellite images as macro cues and propose
cross-view alignment tuning including a satellite-view and street-view image
grafting mechanism, along with an automatic label generation mechanism. Then
LVLM's global understanding of street distribution is enhanced through
cross-view matching. Our proposed model, named AddressVLM, consists of
two-stage training protocols: cross-view alignment tuning and address
localization tuning. Furthermore, we have constructed two street-view VQA
datasets based on image address localization datasets from Pittsburgh and San
Francisco. Qualitative and quantitative evaluations demonstrate that AddressVLM
outperforms counterpart LVLMs by over 9% and 12% in average address
localization accuracy on these two datasets, respectively.

</details>


### [69] [Hybrid Generative Fusion for Efficient and Privacy-Preserving Face Recognition Dataset Generation](https://arxiv.org/abs/2508.10672)
*Feiran Li,Qianqian Xu,Shilong Bao,Boyu Han,Zhiyong Yang,Qingming Huang*

Main category: cs.CV

TL;DR: 介紹了一種用於DataCV ICCV挑戰的高質量人臉數據集構建方法，獲得了比賽第一名。


<details>
  <summary>Details</summary>
Motivation: 解決構建高質量人臉數據集的挑戰，特別是避免身份與現有公開數據集重疊。

Method: 通過對HSFace數據進行清理，結合Mixture-of-Experts策略和GPT-4o驗證，並採用Stable Diffusion和Vec2Face生成並擴展身份一致的數據集，最後採用課程學習策略改善模型表現。

Result: 該方法成功構建了一個包含高質量和無身份重疊的數據集，在比賽中排名第一，並顯示了在不同身份規模（10K、20K、100K）下改進的模型性能。

Conclusion: 所提出的方法有效解決了挑戰需求，且具備推廣性，有助於提升人臉識別模型的準確度與數據多樣性。

Abstract: In this paper, we present our approach to the DataCV ICCV Challenge, which
centers on building a high-quality face dataset to train a face recognition
model. The constructed dataset must not contain identities overlapping with any
existing public face datasets. To handle this challenge, we begin with a
thorough cleaning of the baseline HSFace dataset, identifying and removing
mislabeled or inconsistent identities through a Mixture-of-Experts (MoE)
strategy combining face embedding clustering and GPT-4o-assisted verification.
We retain the largest consistent identity cluster and apply data augmentation
up to a fixed number of images per identity. To further diversify the dataset,
we generate synthetic identities using Stable Diffusion with prompt
engineering. As diffusion models are computationally intensive, we generate
only one reference image per identity and efficiently expand it using Vec2Face,
which rapidly produces 49 identity-consistent variants. This hybrid approach
fuses GAN-based and diffusion-based samples, enabling efficient construction of
a diverse and high-quality dataset. To address the high visual similarity among
synthetic identities, we adopt a curriculum learning strategy by placing them
early in the training schedule, allowing the model to progress from easier to
harder samples. Our final dataset contains 50 images per identity, and all
newly generated identities are checked with mainstream face datasets to ensure
no identity leakage. Our method achieves \textbf{1st place} in the competition,
and experimental results show that our dataset improves model performance
across 10K, 20K, and 100K identity scales. Code is available at
https://github.com/Ferry-Li/datacv_fr.

</details>


### [70] [HyperTea: A Hypergraph-based Temporal Enhancement and Alignment Network for Moving Infrared Small Target Detection](https://arxiv.org/abs/2508.10678)
*Zhaoyuan Qi,Weihua Gao,Wenlong Niu,Jie Tang,Yun Li,Xiaodong Peng*

Main category: cs.CV

TL;DR: 提出HyperTea模型，运用CNN、RNN及高阶图神经网络，增强多时序特征表示，在红外小目标检测中取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅限于低阶相关建模及单一时间尺度特征处理，难以应对小目标、弱强度及复杂运动模式问题，尚未充分利用高阶图用于红外小目标检测。

Method: 设计HyperTea模型，包括全局时间增强模块(GTEM)、局部时间增强模块(LTEM)和时间对齐模块(TAM)，通过高阶图神经网络结合CNN和RNN建模高阶时空相关性。

Result: 在DAUB和IRDST数据集上进行实验，证明其显著优于现有方法，达到SOTA性能。

Conclusion: HyperTea首次将CNN、RNN和高阶图神经网络结合，提升了多时序特征表示能力，为红外小目标检测提供了新的解决方案。

Abstract: In practical application scenarios, moving infrared small target detection
(MIRSTD) remains highly challenging due to the target's small size, weak
intensity, and complex motion pattern. Existing methods typically only model
low-order correlations between feature nodes and perform feature extraction and
enhancement within a single temporal scale. Although hypergraphs have been
widely used for high-order correlation learning, they have received limited
attention in MIRSTD. To explore the potential of hypergraphs and enhance
multi-timescale feature representation, we propose HyperTea, which integrates
global and local temporal perspectives to effectively model high-order
spatiotemporal correlations of features. HyperTea consists of three modules:
the global temporal enhancement module (GTEM) realizes global temporal context
enhancement through semantic aggregation and propagation; the local temporal
enhancement module (LTEM) is designed to capture local motion patterns between
adjacent frames and then enhance local temporal context; additionally, we
further develop a temporal alignment module (TAM) to address potential
cross-scale feature misalignment. To our best knowledge, HyperTea is the first
work to integrate convolutional neural networks (CNNs), recurrent neural
networks (RNNs), and hypergraph neural networks (HGNNs) for MIRSTD,
significantly improving detection performance. Experiments on DAUB and IRDST
demonstrate its state-of-the-art (SOTA) performance. Our source codes are
available at https://github.com/Lurenjia-LRJ/HyperTea.

</details>


### [71] [Physics-Informed Joint Multi-TE Super-Resolution with Implicit Neural Representation for Robust Fetal T2 Mapping](https://arxiv.org/abs/2508.10680)
*Busra Bulut,Maik Dannecker,Thomas Sanchez,Sara Neves Silva,Vladyslav Zalevskyi,Steven Jia,Jean-Baptiste Ledoux,Guillaume Auzias,François Rousseau,Jana Hutter,Daniel Rueckert,Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: 此研究提出一种联合重建方法，通过物理模型和隐式神经网络，提升胎儿脑部MRI的T2显像效果，在0.55T中验证了这一方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有胎儿脑部MRI T2显像面临扫描时间长和对运动敏感等问题，因此需要一种能够同时处理严重运动并提高分辨率的新方法。

Method: 研究提出结合隐式神经表示和物理正则化的方法，能跨回波时间直接重建数据，同时保护解剖结构和T2定量精度。

Result: 实验在模拟胎儿脑部和成人有类似胎儿运动的数据中展示了出色的表现，并首次提供了0.55T下胎儿T2显像结果。

Conclusion: 该方法显示了减少单次回波时间的扫描堆叠数量并利用解剖冗余来优化T2显像的潜力。

Abstract: T2 mapping in fetal brain MRI has the potential to improve characterization
of the developing brain, especially at mid-field (0.55T), where T2 decay is
slower. However, this is challenging as fetal MRI acquisition relies on
multiple motion-corrupted stacks of thick slices, requiring slice-to-volume
reconstruction (SVR) to estimate a high-resolution (HR) 3D volume. Currently,
T2 mapping involves repeated acquisitions of these stacks at each echo time
(TE), leading to long scan times and high sensitivity to motion. We tackle this
challenge with a method that jointly reconstructs data across TEs, addressing
severe motion. Our approach combines implicit neural representations with a
physics-informed regularization that models T2 decay, enabling information
sharing across TEs while preserving anatomical and quantitative T2 fidelity. We
demonstrate state-of-the-art performance on simulated fetal brain and in vivo
adult datasets with fetal-like motion. We also present the first in vivo fetal
T2 mapping results at 0.55T. Our study shows potential for reducing the number
of stacks per TE in T2 mapping by leveraging anatomical redundancy.

</details>


### [72] [IADGPT: Unified LVLM for Few-Shot Industrial Anomaly Detection, Localization, and Reasoning via In-Context Learning](https://arxiv.org/abs/2508.10681)
*Mengyang Zhao,Teng Fu,Haiyang Yu,Ke Niu,Bin Li*

Main category: cs.CV

TL;DR: 提出了一种名为IADGPT的框架，通过三阶段渐进式训练策略，为工业领域的少样本异常检测（FS-IAD）提供了人类级别的识别、定位及推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型（LVLMs）专注于通用任务，缺乏工业基础知识和与FS-IAD相关的推理能力，无法满足类似人类质量检测员的需求。

Method: 1. 设计了IADGPT框架，用于少样本异常检测、定位和推理。
2. 提出了三阶段渐进式训练策略，前两阶段学习工业知识和差异意识，第三阶段通过上下文学习优化模型的泛化能力。
3. 提出了一种结合logits输出和注意力图的策略，用于提供图像级和像素级异常得分。
4. 构建了包含100K图像及丰富属性文本注释的数据集，涵盖400种工业产品类别。

Result: 实验结果表明，IADGPT在异常检测方面取得了显著性能提升，在异常定位和推理上具有竞争力。

Conclusion: IADGPT框架通过模拟人类的学习能力，在少样本条件下提升了工业异常检测任务的性能，并为多样化的工业产品提供了一种普适策略。

Abstract: Few-Shot Industrial Anomaly Detection (FS-IAD) has important applications in
automating industrial quality inspection. Recently, some FS-IAD methods based
on Large Vision-Language Models (LVLMs) have been proposed with some
achievements through prompt learning or fine-tuning. However, existing LVLMs
focus on general tasks but lack basic industrial knowledge and reasoning
capabilities related to FS-IAD, making these methods far from specialized human
quality inspectors. To address these challenges, we propose a unified
framework, IADGPT, designed to perform FS-IAD in a human-like manner, while
also handling associated localization and reasoning tasks, even for diverse and
novel industrial products. To this end, we introduce a three-stage progressive
training strategy inspired by humans. Specifically, the first two stages
gradually guide IADGPT in acquiring fundamental industrial knowledge and
discrepancy awareness. In the third stage, we design an in-context
learning-based training paradigm, enabling IADGPT to leverage a few-shot image
as the exemplars for improved generalization to novel products. In addition, we
design a strategy that enables IADGPT to output image-level and pixel-level
anomaly scores using the logits output and the attention map, respectively, in
conjunction with the language output to accomplish anomaly reasoning. To
support our training, we present a new dataset comprising 100K images across
400 diverse industrial product categories with extensive attribute-level
textual annotations. Experiments indicate IADGPT achieves considerable
performance gains in anomaly detection and demonstrates competitiveness in
anomaly localization and reasoning. We will release our dataset in
camera-ready.

</details>


### [73] [Novel View Synthesis using DDIM Inversion](https://arxiv.org/abs/2508.10688)
*Sehajdeep SIngh,A V Subramanyam*

Main category: cs.CV

TL;DR: 本文提出了一种从单张图像合成新视图的方法，通过结合预训练扩散模型的生成能力和一个轻量化的视图转换框架，解决现有方法中计算费用高、重建效果模糊的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在新视图合成中需要较大的计算资源，且效果存在重建模糊和泛化性差的问题，因而需要一种高效且清晰的解决方案。

Method: 作者设计了一个基于相机姿态的U-Net模型（TUNet）进行视图转换，同时提出了一种融合策略，利用DDIM逆推中的噪声相关结构，提高重建细节和纹理清晰度。

Result: 实验表明，该方法在MVImgNet数据集上的表现优于现有方法。

Conclusion: 本研究提出了一种新颖有效的方法，以预训练扩散模型为基础，通过轻量化的网络设计和融合策略，成功实现了从单张图像生成清晰新视图的目标。

Abstract: Synthesizing novel views from a single input image is a challenging task. It
requires extrapolating the 3D structure of a scene while inferring details in
occluded regions, and maintaining geometric consistency across viewpoints. Many
existing methods must fine-tune large diffusion backbones using multiple views
or train a diffusion model from scratch, which is extremely expensive.
Additionally, they suffer from blurry reconstruction and poor generalization.
This gap presents the opportunity to explore an explicit lightweight view
translation framework that can directly utilize the high-fidelity generative
capabilities of a pretrained diffusion model while reconstructing a scene from
a novel view. Given the DDIM-inverted latent of a single input image, we employ
a camera pose-conditioned translation U-Net, TUNet, to predict the inverted
latent corresponding to the desired target view. However, the image sampled
using the predicted latent may result in a blurry reconstruction. To this end,
we propose a novel fusion strategy that exploits the inherent noise correlation
structure observed in DDIM inversion. The proposed fusion strategy helps
preserve the texture and fine-grained details. To synthesize the novel view, we
use the fused latent as the initial condition for DDIM sampling, leveraging the
generative prior of the pretrained diffusion model. Extensive experiments on
MVImgNet demonstrate that our method outperforms existing methods.

</details>


### [74] [Beyond conventional vision: RGB-event fusion for robust object detection in dynamic traffic scenarios](https://arxiv.org/abs/2508.10704)
*Zhanwen Liu,Yujing Sun,Yang Wang,Nan Yang,Shengbo Eben Li,Xiangmo Zhao*

Main category: cs.CV

TL;DR: 该研究通过结合生物启发事件相机与RGB相机，并提出运动线索融合网络（MCFNet）来改善在复杂光照环境中的目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统RGB相机动态范围受限导致的对比度丧失及复杂交通场景中细节信息丢失的问题。

Method: 提出了运动线索融合网络（MCFNet），包括事件校正模块（ECM）进行时间对齐、事件动态上采样模块（EDUM）进行空间分辨率增强，以及跨模态融合模块（CMM）实现自适应特征融合，精确对齐时空信息并优化融合效果。

Result: 实验结果表明，在DSEC-Det和PKU-DAVIS-SOD数据集上，该方法在低光照和快速运动交通场景中相较现有方法显著提升；特别是在DSEC-Det数据集上，分别在mAP50和mAP指标上提高了7.4%和1.7%。

Conclusion: 运动线索融合网络（MCFNet）能有效结合事件相机与RGB相机的信息，在复杂光照条件下显著提升目标检测性能，为未来交通场景感知提供了新的技术方向。

Abstract: The dynamic range limitation of conventional RGB cameras reduces global
contrast and causes loss of high-frequency details such as textures and edges
in complex traffic environments (e.g., nighttime driving, tunnels), hindering
discriminative feature extraction and degrading frame-based object detection.
To address this, we integrate a bio-inspired event camera with an RGB camera to
provide high dynamic range information and propose a motion cue fusion network
(MCFNet), which achieves optimal spatiotemporal alignment and adaptive
cross-modal feature fusion under challenging lighting. Specifically, an event
correction module (ECM) temporally aligns asynchronous event streams with image
frames via optical-flow-based warping, jointly optimized with the detection
network to learn task-aware event representations. The event dynamic upsampling
module (EDUM) enhances spatial resolution of event frames to match image
structures, ensuring precise spatiotemporal alignment. The cross-modal mamba
fusion module (CMM) uses adaptive feature fusion with a novel interlaced
scanning mechanism, effectively integrating complementary information for
robust detection. Experiments conducted on the DSEC-Det and PKU-DAVIS-SOD
datasets demonstrate that MCFNet significantly outperforms existing methods in
various poor lighting and fast moving traffic scenarios. Notably, on the
DSEC-Det dataset, MCFNet achieves a remarkable improvement, surpassing the best
existing methods by 7.4% in mAP50 and 1.7% in mAP metrics, respectively. The
code is available at https://github.com/Charm11492/MCFNet.

</details>


### [75] [CountCluster: Training-Free Object Quantity Guidance with Cross-Attention Map Clustering for Text-to-Image Generation](https://arxiv.org/abs/2508.10710)
*Joohyeon Lee,Jin-Seop Lee,Jee-Hyong Lee*

Main category: cs.CV

TL;DR: 论文研究提升文本到图像生成模型准确生成指定数量对象的能力，并提出了一种新方法CountCluster。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在生成文本描述的图像时，难以准确地反映指定的对象数量。主要问题在于现有方法忽略了去噪过程中早期步骤对数量影响的重要性。

Method: 提出了CountCluster方法，通过在推理时将对象交叉注意力图划分为k个簇，并定义一个理想分布以优化潜在变量，使簇间空间分隔清晰且匹配数量目标。

Result: 相比现有方法，CountCluster在对象计数准确性上平均提升了18.5个百分点，展现了更强的数量控制能力。

Conclusion: CountCluster方法在无需外部工具与额外训练的情况下，有效实现了高精度对象数量控制，未来可进一步优化。

Abstract: Diffusion-based text-to-image generation models have demonstrated strong
performance in terms of image quality and diversity. However, they still
struggle to generate images that accurately reflect the number of objects
specified in the input prompt. Several approaches have been proposed that rely
on either external counting modules for iterative refinement or quantity
representations derived from learned tokens or latent features. However, they
still have limitations in accurately reflecting the specified number of objects
and overlook an important structural characteristic--The number of object
instances in the generated image is largely determined in the early timesteps
of the denoising process. To correctly reflect the object quantity for image
generation, the highly activated regions in the object cross-attention map at
the early timesteps should match the input object quantity, while each region
should be clearly separated. To address this issue, we propose
\textit{CountCluster}, a method that guides the object cross-attention map to
be clustered according to the specified object count in the input, without
relying on any external tools or additional training. The proposed method
partitions the object cross-attention map into $k$ clusters at inference time
based on attention scores, defines an ideal distribution in which each cluster
is spatially well-separated, and optimizes the latent to align with this target
distribution. Our method achieves an average improvement of 18.5\%p in object
count accuracy compared to existing methods, and demonstrates superior quantity
control performance across a variety of prompts. Code will be released at:
https://github.com/JoohyeonL22/CountCluster .

</details>


### [76] [NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale](https://arxiv.org/abs/2508.10711)
*NextStep Team,Chunrui Han,Guopeng Li,Jingwei Wu,Quan Sun,Yan Cai,Yuang Peng,Zheng Ge,Deyu Zhou,Haomiao Tang,Hongyu Zhou,Kenkun Liu,Ailin Huang,Bin Wang,Changxin Miao,Deshan Sun,En Yu,Fukun Yin,Gang Yu,Hao Nie,Haoran Lv,Hanpeng Hu,Jia Wang,Jian Zhou,Jianjian Sun,Kaijun Tan,Kang An,Kangheng Lin,Liang Zhao,Mei Chen,Peng Xing,Rui Wang,Shiyu Liu,Shutao Xia,Tianhao You,Wei Ji,Xianfang Zeng,Xin Han,Xuelin Zhang,Yana Wei,Yanming Xu,Yimin Jiang,Yingming Wang,Yu Zhou,Yucheng Han,Ziyang Meng,Binxing Jiao,Daxin Jiang,Xiangyu Zhang,Yibo Zhu*

Main category: cs.CV

TL;DR: 本文提出NextStep-1，通过结合多种技术优化了文本到图像生成任务，达到了顶尖性能。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成自回归模型存在效率低下和量化损失的问题，亟需提升性能和生成图片的质量。

Method: 设计一个14B参数的NextStep-1自回归模型，与157M的流匹配头结合，采用离散文本tokens与连续图像tokens的方式进行训练，目标是预测下一个token。

Result: NextStep-1在文本到图像生成任务中达到了自回归模型的最新性能，并展示了高保真图像生成的强大能力，同时在图像编辑任务中也表现良好。

Conclusion: 通过NextStep-1，研究展示了一个统一的自回归文本到图像生成方法的潜力，并计划将代码和模型开源促进研究。

Abstract: Prevailing autoregressive (AR) models for text-to-image generation either
rely on heavy, computationally-intensive diffusion models to process continuous
image tokens, or employ vector quantization (VQ) to obtain discrete tokens with
quantization loss. In this paper, we push the autoregressive paradigm forward
with NextStep-1, a 14B autoregressive model paired with a 157M flow matching
head, training on discrete text tokens and continuous image tokens with
next-token prediction objectives. NextStep-1 achieves state-of-the-art
performance for autoregressive models in text-to-image generation tasks,
exhibiting strong capabilities in high-fidelity image synthesis. Furthermore,
our method shows strong performance in image editing, highlighting the power
and versatility of our unified approach. To facilitate open research, we will
release our code and models to the community.

</details>


### [77] [Lightweight CNNs for Embedded SAR Ship Target Detection and Classification](https://arxiv.org/abs/2508.10712)
*Fabian Kresse,Georgios Pilikos,Mario Azcueta,Nicolas Floury*

Main category: cs.CV

TL;DR: 该论文提出并评估了用于实时推理Stripmap和IW模式未聚焦SAR数据的神经网络，展示了基于FPGA的船只与风车分类模型的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有的海事监测方法受限于数据下传和地面处理，高效的船只监测需要新方法。

Method: 设计并评估了一种在未聚焦SAR数据上实时推理的神经网络模型，并检验了其基于FPGA实现的可能性。

Result: 模型实现了对船只和风车的分类，并验证了其在FPGA上部署的潜力。

Conclusion: 通过该方法，可减轻带宽限制，减少延迟，实现高效的SAR数据处理。

Abstract: Synthetic Aperture Radar (SAR) data enables large-scale surveillance of
maritime vessels. However, near-real-time monitoring is currently constrained
by the need to downlink all raw data, perform image focusing, and subsequently
analyze it on the ground. On-board processing to generate higher-level products
could reduce the data volume that needs to be downlinked, alleviating bandwidth
constraints and minimizing latency. However, traditional image focusing and
processing algorithms face challenges due to the satellite's limited memory,
processing power, and computational resources. This work proposes and evaluates
neural networks designed for real-time inference on unfocused SAR data acquired
in Stripmap and Interferometric Wide (IW) modes captured with Sentinel-1. Our
results demonstrate the feasibility of using one of our models for on-board
processing and deployment on an FPGA. Additionally, by investigating a binary
classification task between ships and windmills, we demonstrate that target
classification is possible.

</details>


### [78] [Revisiting Cross-View Localization from Image Matching](https://arxiv.org/abs/2508.10716)
*Panwang Xia,Qiong Wu,Lei Yu,Yi Liu,Mingtao Xiong,Lei Liang,Yongjun Zhang,Yi Wan*

Main category: cs.CV

TL;DR: 提出了一种新的框架，通过改进匹配和定位解决了跨视角图像匹配的问题，并引入了首个带有像素级对应标注的跨视角数据基准CVFM。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在跨视角匹配上的几何不一致问题，从而提高定位的解释性和精确性。

Method: 引入了表面模型（Surface Model）用于准确的鸟瞰视图投影；设计了一种SimRefiner模块，用于通过局部-全局残差修正优化相似度矩阵。

Result: 实验显示，该方法显著提升了定位准确性和图像匹配质量，并在极端视角差异下设立了新的基准。

Conclusion: 所提方法在跨视角定位和匹配任务中表现突出，并引入了首个像素级跨视角匹配基准，为后续研究提供了重要支持。

Abstract: Cross-view localization aims to estimate the 3 degrees of freedom pose of a
ground-view image by registering it to aerial or satellite imagery. It is
essential in GNSS-denied environments such as urban canyons and disaster zones.
Existing methods either regress poses directly or align features in a shared
bird's-eye view (BEV) space, both built upon accurate spatial correspondences
between perspectives. However, these methods fail to establish strict
cross-view correspondences, yielding only coarse or geometrically inconsistent
matches. Consequently, fine-grained image matching between ground and aerial
views remains an unsolved problem, which in turn constrains the
interpretability of localization results. In this paper, we revisit cross-view
localization from the perspective of cross-view image matching and propose a
novel framework that improves both matching and localization. Specifically, we
introduce a Surface Model to model visible regions for accurate BEV projection,
and a SimRefiner module to refine the similarity matrix through local-global
residual correction, eliminating the reliance on post-processing like RANSAC.
To further support research in this area, we introduce CVFM, the first
benchmark with 32,509 cross-view image pairs annotated with pixel-level
correspondences. Extensive experiments demonstrate that our approach
substantially improves both localization accuracy and image matching quality,
setting new baselines under extreme viewpoint disparity.

</details>


### [79] [Exploiting Discriminative Codebook Prior for Autoregressive Image Generation](https://arxiv.org/abs/2508.10719)
*Longxiang Tang,Ruihang Chu,Xiang Wang,Yujin Han,Pingyu Wu,Chunming He,Yingya Zhang,Shiwei Zhang,Jiaya Jia*

Main category: cs.CV

TL;DR: 提出了一种新的方法DCPE，替代k-means聚类在离散标记生成中的不足。


<details>
  <summary>Details</summary>
Motivation: 解决现有k-means聚类在代码簿特征空间中的表现问题，以更好地提取和利用代码簿中的标记相似性信息。

Method: 提出了DCPE（Discriminative Codebook Prior Extractor），采用实例距离代替质心距离，并通过聚合作用方法解决高密度区域分裂及低密度区域聚合的问题。

Result: DCPE加速了LlamaGen-B自回归模型的训练速度42%，并提升了最终的FID和IS性能。

Conclusion: DCPE作为一种可无缝集成的方法，有效利用了代码簿中的相似性信息，从而显著提升了自回归图像生成模型的训练效率及表现。

Abstract: Advanced discrete token-based autoregressive image generation systems first
tokenize images into sequences of token indices with a codebook, and then model
these sequences in an autoregressive paradigm. While autoregressive generative
models are trained only on index values, the prior encoded in the codebook,
which contains rich token similarity information, is not exploited. Recent
studies have attempted to incorporate this prior by performing naive k-means
clustering on the tokens, helping to facilitate the training of generative
models with a reduced codebook. However, we reveal that k-means clustering
performs poorly in the codebook feature space due to inherent issues, including
token space disparity and centroid distance inaccuracy. In this work, we
propose the Discriminative Codebook Prior Extractor (DCPE) as an alternative to
k-means clustering for more effectively mining and utilizing the token
similarity information embedded in the codebook. DCPE replaces the commonly
used centroid-based distance, which is found to be unsuitable and inaccurate
for the token feature space, with a more reasonable instance-based distance.
Using an agglomerative merging technique, it further addresses the token space
disparity issue by avoiding splitting high-density regions and aggregating
low-density ones. Extensive experiments demonstrate that DCPE is plug-and-play
and integrates seamlessly with existing codebook prior-based paradigms. With
the discriminative prior extracted, DCPE accelerates the training of
autoregressive models by 42% on LlamaGen-B and improves final FID and IS
performance.

</details>


### [80] [SegDAC: Segmentation-Driven Actor-Critic for Visual Reinforcement Learning](https://arxiv.org/abs/2508.09325)
*Alexandre Brown,Glen Berseth*

Main category: cs.CV

TL;DR: 本文提出了一种名为SegDAC的视觉强化学习方法，它使用Segment Anything和YOLO-World进行对象分解和语义关联，结合新的变换器架构实现动态段聚焦，从而显著提升视觉泛化能力和样本效率。


<details>
  <summary>Details</summary>
Motivation: 解决视觉强化学习中从高维输入和噪声奖励中学习感知与行动的挑战，同时提升视觉泛化能力和样本效率。

Method: 方法SegDAC通过使用Segment Anything (SAM) 进行基于对象的分割和YOLO-World来通过文本提示对子段进行语义关联，结合了一种基于变换器的架构，支持每时间步的动态段聚焦，并通过在线强化学习学习哪部分集中注意，无需人工标签。

Result: 在Maniskill3的视觉泛化基准测试中，SegDAC的表现显著优于现有方法，在最难场景下将性能提升了一倍，并在所有任务中匹配或超越了样本效率表现。

Conclusion: SegDAC通过高效的视觉分割与短语关联机制以及动态段聚焦架构，在解决视觉泛化和样本效率问题上表现卓越，为视觉强化学习的发展提供了新思路。

Abstract: Visual reinforcement learning (RL) is challenging due to the need to learn
both perception and actions from high-dimensional inputs and noisy rewards.
Although large perception models exist, integrating them effectively into RL
for visual generalization and improved sample efficiency remains unclear. We
propose SegDAC, a Segmentation-Driven Actor-Critic method. SegDAC uses Segment
Anything (SAM) for object-centric decomposition and YOLO-World to ground
segments semantically via text prompts. It includes a novel transformer-based
architecture that supports a dynamic number of segments at each time step and
effectively learns which segments to focus on using online RL, without using
human labels. By evaluating SegDAC over a challenging visual generalization
benchmark using Maniskill3, which covers diverse manipulation tasks under
strong visual perturbations, we demonstrate that SegDAC achieves significantly
better visual generalization, doubling prior performance on the hardest setting
and matching or surpassing prior methods in sample efficiency across all
evaluated tasks.

</details>


### [81] [EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain Egocentric Video Question Answering](https://arxiv.org/abs/2508.10729)
*Yanjun Li,Yuqian Fu,Tianwen Qian,Qi'ao Xu,Silong Dai,Danda Pani Paudel,Luc Van Gool,Xiaoling Wang*

Main category: cs.CV

TL;DR: 本文提出了EgoCross基准，用于评估跨域情境下多模态大语言模型（MLLMs）的表现。


<details>
  <summary>Details</summary>
Motivation: 现有对EgocentricQA的研究主要集中在日常活动，而现实应用会涉及视觉风格和语义内容等显著变化的领域。

Method: 设计了EgoCross基准，覆盖手术、工业、极限运动和动物视角四个多样化领域，共包含约1000个QA对，并支持开放和封闭两种回答格式的精细评估。

Result: 研究表明，大多数现有MLLMs在日常生活以外的领域表现有限，难以很好地适应跨域任务。

Conclusion: EgoCross及其分析为推动领域自适应和稳健的自我中心视频理解提供了重要的基础工具，同时数据和代码将公开发布。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have
significantly pushed the frontier of egocentric video question answering
(EgocentricQA). However, existing benchmarks and studies are mainly limited to
common daily activities such as cooking and cleaning. In contrast, real-world
deployment inevitably encounters domain shifts, where target domains differ
substantially in both visual style and semantic content. To bridge this gap, we
introduce \textbf{EgoCross}, a comprehensive benchmark designed to evaluate the
cross-domain generalization of MLLMs in EgocentricQA. EgoCross covers four
diverse and challenging domains, including surgery, industry, extreme sports,
and animal perspective, representing realistic and high-impact application
scenarios. It comprises approximately 1,000 QA pairs across 798 video clips,
spanning four key QA tasks: prediction, recognition, localization, and
counting. Each QA pair provides both OpenQA and CloseQA formats to support
fine-grained evaluation. Extensive experiments show that most existing MLLMs,
whether general-purpose or egocentric-specialized, struggle to generalize to
domains beyond daily life, highlighting the limitations of current models.
Furthermore, we conduct several pilot studies, \eg, fine-tuning and
reinforcement learning, to explore potential improvements. We hope EgoCross and
our accompanying analysis will serve as a foundation for advancing
domain-adaptive, robust egocentric video understanding. Data and codes will be
released at:
\href{https://github.com/MyUniverse0726/EgoCross}{https://github.com/MyUniverse0726/EgoCross.}

</details>


### [82] [Dissecting Generalized Category Discovery: Multiplex Consensus under Self-Deconstruction](https://arxiv.org/abs/2508.10731)
*Luyao Tang,Kunze Huang,Chaoqi Chen,Yuxuan Yuan,Chenxin Li,Xiaotong Tu,Xinghao Ding,Yue Huang*

Main category: cs.CV

TL;DR: 提出了一种名为ConGCD的新方法，以在通用类别发现(GCD)任务中模仿人类通过视觉析因理解新物体的方式。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习框架在已知和新类别中的对象识别能力有限，无法匹敌人类的感知系统。本文寻求通过一个类脑机制来改善通用类别发现。

Method: 通过高水平语义重构实现基于视觉原子的表示建模，应用主导和情境共识单元捕捉类判别化模式与分布不变量，并动态调整激活路径，最终通过多重共识集成得出预测。

Result: 实验表明，在粗粒度及细粒度基准测试上，ConGCD显著优于现有方法，且具备广泛适用性。

Conclusion: ConGCD作为一种共识感知范式，成功地模仿人类视觉处理机制，为GCD领域做出了重要贡献，并提供了更强的类别判别能力。

Abstract: Human perceptual systems excel at inducing and recognizing objects across
both known and novel categories, a capability far beyond current machine
learning frameworks. While generalized category discovery (GCD) aims to bridge
this gap, existing methods predominantly focus on optimizing objective
functions. We present an orthogonal solution, inspired by the human cognitive
process for novel object understanding: decomposing objects into visual
primitives and establishing cross-knowledge comparisons. We propose ConGCD,
which establishes primitive-oriented representations through high-level
semantic reconstruction, binding intra-class shared attributes via
deconstruction. Mirroring human preference diversity in visual processing,
where distinct individuals leverage dominant or contextual cues, we implement
dominant and contextual consensus units to capture class-discriminative
patterns and inherent distributional invariants, respectively. A consensus
scheduler dynamically optimizes activation pathways, with final predictions
emerging through multiplex consensus integration. Extensive evaluations across
coarse- and fine-grained benchmarks demonstrate ConGCD's effectiveness as a
consensus-aware paradigm. Code is available at github.com/lytang63/ConGCD.

</details>


### [83] [Privacy-enhancing Sclera Segmentation Benchmarking Competition: SSBC 2025](https://arxiv.org/abs/2508.10737)
*Matej Vitek,Darian Tomašević,Abhijit Das,Sabari Nathan,Gökhan Özbulak,Gözde Ayşe Tataroğlu Özbulak,Jean-Paul Calbimonte,André Anjos,Hariohm Hemant Bhatt,Dhruv Dhirendra Premani,Jay Chaudhari,Caiyong Wang,Jian Jiang,Chi Zhang,Qi Zhang,Iyyakutti Iyappan Ganapathi,Syed Sadaf Ali,Divya Velayudan,Maregu Assefa,Naoufel Werghi,Zachary A. Daniels,Leeon John,Ritesh Vyas,Jalil Nourmohammadi Khiarak,Taher Akbari Saeed,Mahsa Nasehi,Ali Kianfar,Mobina Pashazadeh Panahi,Geetanjali Sharma,Pushp Raj Panth,Raghavendra Ramachandra,Aditya Nigam,Umapada Pal,Peter Peer,Vitomir Štruc*

Main category: cs.CV

TL;DR: 本论文总结了2025年巩膜分割基准竞赛 (SSBC) 的成果，重点是通过合成眼部图像训练隐私保护巩膜分割模型，评估基于合成数据的模型性能与基于真实数据的模型性能对比。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过合成数据开发隐私友好的生物特征识别模型，并衡量其与真实数据模型在性能上的差异，为未来研究提供指导。

Method: 竞赛分两个轨道：(i) 仅基于合成数据开发模型；(ii) 合成数据与有限的真实数据相结合的模型开发。包括使用变换器模型、轻量级模型及生成框架指导的分割网络。

Result: 在测试的三组数据(合成+真实)中表现出色。全合成数据轨道中F1分数超过0.8，综合轨道中模型优化策略的选择对提升性能更为重要。

Conclusion: 基于合成数据的模型能提供有竞争力的性能，结合隐私友好的理念，显示了合成数据在未来生物特征研发中的潜力。

Abstract: This paper presents a summary of the 2025 Sclera Segmentation Benchmarking
Competition (SSBC), which focused on the development of privacy-preserving
sclera-segmentation models trained using synthetically generated ocular images.
The goal of the competition was to evaluate how well models trained on
synthetic data perform in comparison to those trained on real-world datasets.
The competition featured two tracks: $(i)$ one relying solely on synthetic data
for model development, and $(ii)$ one combining/mixing synthetic with (a
limited amount of) real-world data. A total of nine research groups submitted
diverse segmentation models, employing a variety of architectural designs,
including transformer-based solutions, lightweight models, and segmentation
networks guided by generative frameworks. Experiments were conducted across
three evaluation datasets containing both synthetic and real-world images,
collected under diverse conditions. Results show that models trained entirely
on synthetic data can achieve competitive performance, particularly when
dedicated training strategies are employed, as evidenced by the top performing
models that achieved $F_1$ scores of over $0.8$ in the synthetic data track.
Moreover, performance gains in the mixed track were often driven more by
methodological choices rather than by the inclusion of real data, highlighting
the promise of synthetic data for privacy-aware biometric development. The code
and data for the competition is available at:
https://github.com/dariant/SSBC_2025.

</details>


### [84] [Axis-level Symmetry Detection with Group-Equivariant Representation](https://arxiv.org/abs/2508.10740)
*Wongyun Yu,Ahyun Seo,Minsu Cho*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的框架，用于检测复杂场景中的反射对称和旋转对称，通过将它们表示为显式几何原语（线和点）。


<details>
  <summary>Details</summary>
Motivation: 检测复杂场景中的对称性仍然是计算机视觉领域的一个重大挑战，当前方法难以精确识别单个对称轴。

Method: 提出一种对称轴检测的新框架，利用二面体群对称特性，通过双分支架构分别处理反射对称和旋转对称，采用方向锚点和匹配策略增强检测精度。

Result: 通过大量实验，证明该方法的性能优于现有方法，实现了最先进的性能水平。

Conclusion: 提出的框架在对称轴检测中表现出色，为计算机视觉中检测复杂对称性场景提供了有效的解决方案。

Abstract: Symmetry is a fundamental concept that has been extensively studied, yet
detecting it in complex scenes remains a significant challenge in computer
vision. Recent heatmap-based approaches can localize potential regions of
symmetry axes but often lack precision in identifying individual axes. In this
work, we propose a novel framework for axis-level detection of the two most
common symmetry types-reflection and rotation-by representing them as explicit
geometric primitives, i.e. lines and points. Our method employs a dual-branch
architecture that is equivariant to the dihedral group, with each branch
specialized to exploit the structure of dihedral group-equivariant features for
its respective symmetry type. For reflection symmetry, we introduce
orientational anchors, aligned with group components, to enable
orientation-specific detection, and a reflectional matching that measures
similarity between patterns and their mirrored counterparts across candidate
axes. For rotational symmetry, we propose a rotational matching that compares
patterns at fixed angular intervals to identify rotational centers. Extensive
experiments demonstrate that our method achieves state-of-the-art performance,
outperforming existing approaches.

</details>


### [85] [Forgery Guided Learning Strategy with Dual Perception Network for Deepfake Cross-domain Detection](https://arxiv.org/abs/2508.10741)
*Lixin Jia,Zhiqing Guo,Gaobo Yang,Liejun Wang,Keqin Li*

Main category: cs.CV

TL;DR: 文章提出一种应对深度伪造技术的检测方法，基于伪造引导学习策略结合双感知网络，提升对未知伪造技术的检测泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的深度伪造检测方法在处理未知伪造技术时表现不佳，且随着伪造技术发展，检测难度加大，急需具备强泛化能力的检测技术应对这一挑战。

Method: 提出伪造引导学习(FGL)策略，捕捉已知与未知伪造技术之间的差异信息，并通过双感知网络(DPNet)提取频域和时域的伪造痕迹特征，同时结合图卷积提升伪造痕迹特征间的相关感知能力。

Result: 大量实验表明，该方法在不同场景下具有良好的适应性，能够有效处理未知伪造挑战，对深度伪造检测提供强有力的支持。

Conclusion: 文中提出的伪造引导学习策略和双感知网络架构，证明了其在提升深度伪造检测泛化能力方面的卓越效果，模型适用于多种伪造场景，大幅加强检测准确性和稳定性。

Abstract: The emergence of deepfake technology has introduced a range of societal
problems, garnering considerable attention. Current deepfake detection methods
perform well on specific datasets, but exhibit poor performance when applied to
datasets with unknown forgery techniques. Moreover, as the gap between emerging
and traditional forgery techniques continues to widen, cross-domain detection
methods that rely on common forgery traces are becoming increasingly
ineffective. This situation highlights the urgency of developing deepfake
detection technology with strong generalization to cope with fast iterative
forgery techniques. To address these challenges, we propose a Forgery Guided
Learning (FGL) strategy designed to enable detection networks to continuously
adapt to unknown forgery techniques. Specifically, the FGL strategy captures
the differential information between known and unknown forgery techniques,
allowing the model to dynamically adjust its learning process in real time. To
further improve the ability to perceive forgery traces, we design a Dual
Perception Network (DPNet) that captures both differences and relationships
among forgery traces. In the frequency stream, the network dynamically
perceives and extracts discriminative features across various forgery
techniques, establishing essential detection cues. These features are then
integrated with spatial features and projected into the embedding space. In
addition, graph convolution is employed to perceive relationships across the
entire feature space, facilitating a more comprehensive understanding of
forgery trace correlations. Extensive experiments show that our approach
generalizes well across different scenarios and effectively handles unknown
forgery challenges, providing robust support for deepfake detection. Our code
is available on https://github.com/vpsg-research/FGL.

</details>


### [86] [An Efficient Model-Driven Groupwise Approach for Atlas Construction](https://arxiv.org/abs/2508.10743)
*Ziwei Zou,Bei Zou,Xiaoyan Kui,Wenqi Lu,Haoran Dou,Arezoo Zakeri,Timothy Cootes,Alejandro F Frangi,Jinming Duan*

Main category: cs.CV

TL;DR: 引入了一种名为DARC的新模型，以解决医学图像分析中的图谱构建问题，提供训练高效、无偏差的群体配准解决方案并展示了一些关键应用。


<details>
  <summary>Details</summary>
Motivation: 解决现有数据驱动方法依赖大数据集且泛化能力有限的问题，以及现有模型驱动方法在大规模3D数据集上的扩展性和优化问题。

Method: 提出了DARC框架，通过坐标下降策略和中心化激活函数来实现无偏差、可微组学的图谱构建，支持多种图像不相似性度量，具有高效的资源利用率。

Result: DARC在单次分割和形状合成任务中表现优异，超越了先进的少样本方法，并能生成新的解剖变体。

Conclusion: DARC是一种灵活、通用且资源高效的图谱构建框架，对医学图像分析任务具有重要意义。

Abstract: Atlas construction is fundamental to medical image analysis, offering a
standardized spatial reference for tasks such as population-level anatomical
modeling. While data-driven registration methods have recently shown promise in
pairwise settings, their reliance on large training datasets, limited
generalizability, and lack of true inference phases in groupwise contexts
hinder their practical use. In contrast, model-driven methods offer
training-free, theoretically grounded, and data-efficient alternatives, though
they often face scalability and optimization challenges when applied to large
3D datasets. In this work, we introduce DARC (Diffeomorphic Atlas Registration
via Coordinate descent), a novel model-driven groupwise registration framework
for atlas construction. DARC supports a broad range of image dissimilarity
metrics and efficiently handles arbitrary numbers of 3D images without
incurring GPU memory issues. Through a coordinate descent strategy and a
centrality-enforcing activation function, DARC produces unbiased, diffeomorphic
atlases with high anatomical fidelity. Beyond atlas construction, we
demonstrate two key applications: (1) One-shot segmentation, where labels
annotated only on the atlas are propagated to subjects via inverse
deformations, outperforming state-of-the-art few-shot methods; and (2) shape
synthesis, where new anatomical variants are generated by warping the atlas
mesh using synthesized diffeomorphic deformation fields. Overall, DARC offers a
flexible, generalizable, and resource-efficient framework for atlas
construction and applications.

</details>


### [87] [From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in Vision Language Models](https://arxiv.org/abs/2508.10770)
*Tiancheng Han,Yunfei Gao,Yong Li,Wuzhou Yu,Qiaosheng Zhang,Wenqi Shao*

Main category: cs.CV

TL;DR: 本文分析了主流视觉语言模型(VLMs)在时空物理推理上的表现，发现存在很大的不足，通过监督微调和基于规则的强化学习对模型进行改进，但仍需更多研究来提高模型对新物理场景的推广能力。


<details>
  <summary>Details</summary>
Motivation: 文章旨在探讨和提升视觉语言模型(VLMs)在重要的时空物理推理能力上的表现。

Method: 通过对Qwen2.5-VL-7B模型进行监督微调和基于规则的强化学习，以增强模型的时空物理推理能力。

Result: 改进后的模型在时空物理推理能力上显著提升，并超越了一些领先的专有模型。

Conclusion: 尽管取得了一定的进展，但模型在新物理场景中的推广能力仍然有限，凸显了在时空物理推理领域需要新方法的迫切性。

Abstract: Spatio-physical reasoning, a foundation capability for understanding the real
physics world, is a critical step towards building robust world models. While
recent vision language models (VLMs) have shown remarkable progress in
specialized domains like multimodal mathematics and pure spatial understanding,
their capability for spatio-physical reasoning remains largely unexplored. This
paper provides a comprehensive diagnostic analysis of mainstream VLMs,
revealing that current models perform inadequately on this crucial task.
Further detailed analysis shows that this underperformance is largely
attributable to biases caused by human-like prior and a lack of deep reasoning.
To address these challenges, we apply supervised fine-tuning followed by
rule-based reinforcement learning to Qwen2.5-VL-7B, resulting in significant
improvements in spatio-physical reasoning capabilities and surpassing leading
proprietary models. Nevertheless, despite this success, the model's
generalization to new physics scenarios remains limited -- underscoring the
pressing need for new approaches in spatio-physical reasoning.

</details>


### [88] [AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video Sequences](https://arxiv.org/abs/2508.10771)
*Jieyu Li,Xin Zhang,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: 本文提出了一个名为AEGIS的全新大规模基准，专注于检测高拟真和语义复杂的AI生成视频。


<details>
  <summary>Details</summary>
Motivation: 现有的视频真实性检测基准在真实感、规模和复杂性方面存在局限，无法有效评估现代视觉-语言模型对复杂伪造的检测能力。因此，该研究旨在填补这一关键空白。

Method: 构建AEGIS数据集，包含由多种先进生成模型生成的超过10,000个真实和伪造视频，提供多模态注释，并设计了挑战性子集以评估鲁棒性。

Result: 实验表明，当采用先进的视觉-语言模型检测时，AEGIS中最具挑战性的子集难度极高，现有模型不能很好地处理这些复杂情况。

Conclusion: AEGIS作为基准大幅推动了视频真实性检测研究的发展，对应对真实世界伪造威胁至关重要。

Abstract: Recent advances in AI-generated content have fueled the rise of highly
realistic synthetic videos, posing severe risks to societal trust and digital
integrity. Existing benchmarks for video authenticity detection typically
suffer from limited realism, insufficient scale, and inadequate complexity,
failing to effectively evaluate modern vision-language models against
sophisticated forgeries. To address this critical gap, we introduce AEGIS, a
novel large-scale benchmark explicitly targeting the detection of
hyper-realistic and semantically nuanced AI-generated videos. AEGIS comprises
over 10,000 rigorously curated real and synthetic videos generated by diverse,
state-of-the-art generative models, including Stable Video Diffusion,
CogVideoX-5B, KLing, and Sora, encompassing open-source and proprietary
architectures. In particular, AEGIS features specially constructed challenging
subsets enhanced with robustness evaluation. Furthermore, we provide multimodal
annotations spanning Semantic-Authenticity Descriptions, Motion Features, and
Low-level Visual Features, facilitating authenticity detection and supporting
downstream tasks such as multimodal fusion and forgery localization. Extensive
experiments using advanced vision-language models demonstrate limited detection
capabilities on the most challenging subsets of AEGIS, highlighting the
dataset's unique complexity and realism beyond the current generalization
capabilities of existing models. In essence, AEGIS establishes an indispensable
evaluation benchmark, fundamentally advancing research toward developing
genuinely robust, reliable, broadly generalizable video authenticity detection
methodologies capable of addressing real-world forgery threats. Our dataset is
available on https://huggingface.co/datasets/Clarifiedfish/AEGIS.

</details>


### [89] [Video-BLADE: Block-Sparse Attention Meets Step Distillation for Efficient Video Generation](https://arxiv.org/abs/2508.10774)
*Youping Gu,Xiaolong Li,Yuhao Hu,Bohan Zhuang*

Main category: cs.CV

TL;DR: 提出BLADE框架以加速和优化高质量视频生成，通过联合稀疏注意力与步蒸馏，大幅提升运算效率与效果。


<details>
  <summary>Details</summary>
Motivation: 针对当前扩散变压器在生成高质量视频时效率低下的问题，结合步蒸馏与稀疏注意力机制面临的瓶颈，探讨更高效的解决方案。

Method: 提出一个数据无关的联合训练框架，包括 (1) 动态生成时空特性稀疏掩码的自适应块稀疏注意力(ASA)机制；(2) 融入稀疏性的轨迹分布匹配驱动步蒸馏流程。

Result: 在CogVideoX-5B和Wan2.1-1.3B等文本到视频模型上验证，BLADE实现了最高14.10倍的推理加速，并在保持质量的情况下提高了视频生成的基准分数和人类评价。

Conclusion: BLADE框架显著提升了高质量视频生成模型的效率，并通过稀疏注意力与步蒸馏的联合优化进一步改善了生成性能。

Abstract: Diffusion transformers currently lead the field in high-quality video
generation, but their slow iterative denoising process and prohibitive
quadratic attention costs for long sequences create significant inference
bottlenecks. While both step distillation and sparse attention mechanisms have
shown promise as independent acceleration strategies, effectively combining
these approaches presents critical challenges -- training-free integration
yields suboptimal results, while separately training sparse attention after
step distillation requires prohibitively expensive high-quality video data. To
overcome these limitations, we propose BLADE, an innovative data-free joint
training framework that introduces: (1) an Adaptive Block-Sparse Attention
(ASA) mechanism for dynamically generating content-aware sparsity masks to
focus computation on salient spatiotemporal features, and (2) a sparsity-aware
step distillation paradigm built upon Trajectory Distribution Matching (TDM)
that directly incorporates sparsity into the distillation process rather than
treating it as a separate compression step, with fast convergence. We validate
BLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B. Our framework
demonstrates remarkable efficiency gains across different scales. On
Wan2.1-1.3B, BLADE achieves a 14.10x end-to-end inference acceleration over a
50-step baseline. Moreover, on models such as CogVideoX-5B with short video
sequence lengths, our framework delivers a robust 8.89x speedup. Crucially, the
acceleration is accompanied by a consistent quality improvement. On the
VBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from
0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further
corroborated by superior ratings in human evaluations. Our code and model
weights are publicly available at: http://ziplab.co/BLADE-Homepage/.

</details>


### [90] [Ultra-High-Definition Reference-Based Landmark Image Super-Resolution with Generative Diffusion Prior](https://arxiv.org/abs/2508.10779)
*Zhenning Shi,Zizheng Yan,Yuhang Yu,Clara Xue,Jingyu Zhuang,Qi Zhang,Jinwei Chen,Tao Li,Qingnan Fan*

Main category: cs.CV

TL;DR: TriFlowSR 提出了一种面向超高清标志性场景的反向扩散超分辨率框架，与现有方法相比表现更佳。


<details>
  <summary>Details</summary>
Motivation: 克服现有 RefSR 方法在 LR 与 HR 图像对齐以及数据集分辨率与画质不足的局限性。

Method: 提出 TriFlowSR 框架，结合参考匹配策略进行高效图像匹配，同时引入 Landmark-4K 数据集以支持 UHD 场景。

Result: 实验结果显示，该方法相比于现有方法，能够更好地利用参考 HR 图像的语义和纹理信息。

Conclusion: 提供了首个面向真实世界退化情况下超高清标志性场景的扩散式 RefSR 方法，表现优异且代码公开。

Abstract: Reference-based Image Super-Resolution (RefSR) aims to restore a
low-resolution (LR) image by utilizing the semantic and texture information
from an additional reference high-resolution (reference HR) image. Existing
diffusion-based RefSR methods are typically built upon ControlNet, which
struggles to effectively align the information between the LR image and the
reference HR image. Moreover, current RefSR datasets suffer from limited
resolution and poor image quality, resulting in the reference images lacking
sufficient fine-grained details to support high-quality restoration. To
overcome the limitations above, we propose TriFlowSR, a novel framework that
explicitly achieves pattern matching between the LR image and the reference HR
image. Meanwhile, we introduce Landmark-4K, the first RefSR dataset for
Ultra-High-Definition (UHD) landmark scenarios. Considering the UHD scenarios
with real-world degradation, in TriFlowSR, we design a Reference Matching
Strategy to effectively match the LR image with the reference HR image.
Experimental results show that our approach can better utilize the semantic and
texture information of the reference HR image compared to previous methods. To
the best of our knowledge, we propose the first diffusion-based RefSR pipeline
for ultra-high definition landmark scenarios under real-world degradation. Our
code and model will be available at https://github.com/nkicsl/TriFlowSR.

</details>


### [91] [Cooperative Face Liveness Detection from Optical Flow](https://arxiv.org/abs/2508.10786)
*Artem Sokolov,Mikhail Nikitin,Anton Konushin*

Main category: cs.CV

TL;DR: 提出了一种新的基于视频的合作式人脸活体检测方法，通过用户缓慢靠近摄像头的互动场景结合光流分析来实现。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有被动式人脸活体检测在应对多种攻击方式（如打印照片、屏幕显示、面具及视频重放）时存在的局限性。

Method: 采用用户遵循特定移动模式的受控接近协议，并结合光流和RGB帧的神经分类器，通过时空特征提取进行活体检测。

Result: 有效提取了面部的三维信息，显著提高了对真实人脸和各种攻击方式的区分能力。

Conclusion: 该方法在活体检测中比传统被动式方法更具可靠性，尤其是在处理复杂攻击环境时表现出众。

Abstract: In this work, we proposed a novel cooperative video-based face liveness
detection method based on a new user interaction scenario where participants
are instructed to slowly move their frontal-oriented face closer to the camera.
This controlled approaching face protocol, combined with optical flow analysis,
represents the core innovation of our approach. By designing a system where
users follow this specific movement pattern, we enable robust extraction of
facial volume information through neural optical flow estimation, significantly
improving discrimination between genuine faces and various presentation attacks
(including printed photos, screen displays, masks, and video replays). Our
method processes both the predicted optical flows and RGB frames through a
neural classifier, effectively leveraging spatial-temporal features for more
reliable liveness detection compared to passive methods.

</details>


### [92] [VasoMIM: Vascular Anatomy-Aware Masked Image Modeling for Vessel Segmentation](https://arxiv.org/abs/2508.10794)
*De-Xing Huang,Xiao-Hu Zhou,Mei-Jiang Gui,Xiao-Liang Xie,Shi-Qi Liu,Shuang-Yi Wang,Tian-Yu Xiang,Rui-Ze Ma,Nu-Fang Xiao,Zeng-Guang Hou*

Main category: cs.CV

TL;DR: 本文提出了一种新的针对X射线血管造影图像的自监督学习方法，称为VasoMIM，用以解决血管与背景像素类别不平衡问题，从而提升血管分割性能。


<details>
  <summary>Details</summary>
Motivation: 传统的掩模图像建模(MIM)方法在血管分割上表现较差，主要是因为血管和背景像素类别间的严重不平衡问题。作者希望通过整合解剖学知识，提升X射线血管造影分割的效果。

Method: 提出VasoMIM框架，包含两个关键组成部分：解剖学指导的掩模策略（优先掩盖包含血管的图像区域）和解剖学一致性损失（保证原图与重构图在血管语义上的一致性）。

Result: VasoMIM在三个数据集上的结果均达到了当前最先进的性能，显著提升了血管分割的效果。

Conclusion: 这种方法表明，将解剖学知识纳入自监督学习，不仅能够捕获更强的血管表征，还具有推动X射线血管造影图像分析的潜力。

Abstract: Accurate vessel segmentation in X-ray angiograms is crucial for numerous
clinical applications. However, the scarcity of annotated data presents a
significant challenge, which has driven the adoption of self-supervised
learning (SSL) methods such as masked image modeling (MIM) to leverage
large-scale unlabeled data for learning transferable representations.
Unfortunately, conventional MIM often fails to capture vascular anatomy because
of the severe class imbalance between vessel and background pixels, leading to
weak vascular representations. To address this, we introduce Vascular
anatomy-aware Masked Image Modeling (VasoMIM), a novel MIM framework tailored
for X-ray angiograms that explicitly integrates anatomical knowledge into the
pre-training process. Specifically, it comprises two complementary components:
anatomy-guided masking strategy and anatomical consistency loss. The former
preferentially masks vessel-containing patches to focus the model on
reconstructing vessel-relevant regions. The latter enforces consistency in
vascular semantics between the original and reconstructed images, thereby
improving the discriminability of vascular representations. Empirically,
VasoMIM achieves state-of-the-art performance across three datasets. These
findings highlight its potential to facilitate X-ray angiogram analysis.

</details>


### [93] [Object Fidelity Diffusion for Remote Sensing Image Generation](https://arxiv.org/abs/2508.10801)
*Ziqi Ye,Shuran Ma,Jie Yang,Xiaoyi Yang,Ziyang Gong,Xue Yang,Haipeng Wang*

Main category: cs.CV

TL;DR: 提出一种新方法OF-Diff，通过引入双分支扩散模型和DDPO技术，生成高保真遥感图像，有效提高目标检测精度。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型生成遥感图像时细节不足的问题，有助于提升目标检测的鲁棒性和可靠性。

Method: 提出以布局为基础的目标形状先验提取方法，设计双分支扩散模型并引入扩散一致性损失，同时用DDPO微调生成过程，提升生成图像的多样性和语义一致性。

Result: OF-Diff在遥感图像生成质量上显著超越现有方法，尤其在多态和小目标类别上表现卓越。例如，在飞机、船只和车辆的mAP提高了8.3%、7.7%和4.0%。

Conclusion: 通过引入对象形状先验和双分支扩散模型，OF-Diff显著提高了遥感图像生成的保真度和目标检测的性能，适合遥感领域应用。

Abstract: High-precision controllable remote sensing image generation is both
meaningful and challenging. Existing diffusion models often produce
low-fidelity images due to their inability to adequately capture morphological
details, which may affect the robustness and reliability of object detection
models. To enhance the accuracy and fidelity of generated objects in remote
sensing, this paper proposes Object Fidelity Diffusion (OF-Diff), which
effectively improves the fidelity of generated objects. Specifically, we are
the first to extract the prior shapes of objects based on the layout for
diffusion models in remote sensing. Then, we introduce a dual-branch diffusion
model with diffusion consistency loss, which can generate high-fidelity remote
sensing images without providing real images during the sampling phase.
Furthermore, we introduce DDPO to fine-tune the diffusion process, making the
generated remote sensing images more diverse and semantically consistent.
Comprehensive experiments demonstrate that OF-Diff outperforms state-of-the-art
methods in the remote sensing across key quality metrics. Notably, the
performance of several polymorphic and small object classes shows significant
improvement. For instance, the mAP increases by 8.3%, 7.7%, and 4.0% for
airplanes, ships, and vehicles, respectively.

</details>


### [94] [Mobile-Friendly Deep Learning for Plant Disease Detection: A Lightweight CNN Benchmark Across 101 Classes of 33 Crops](https://arxiv.org/abs/2508.10817)
*Anand Kumar,Harminder Pal Monga,Tapasi Brahma,Satyam Kalra,Navas Sherif*

Main category: cs.CV

TL;DR: 该论文提出了一种移动友好的解决方案，可检测101种植物疾病，适用于33种作物，基于高效的轻量级深度学习架构。


<details>
  <summary>Details</summary>
Motivation: 植物疾病对全球粮食安全构成重大威胁，需要开发早期检测系统。

Method: 构建了综合数据集，评估了多个轻量级架构，包括MobileNetV2/V3、EfficientNet-B0/B1等，目标是实现良好的准确性与效率平衡。

Result: EfficientNet-B1在分类精度上表现最佳，达到了94.7%，并兼顾计算效率，适合移动设备部署。

Conclusion: 结合深度学习和高效模型，解决了植物病害检测问题，为移动设备上的实际应用提供了可能性。

Abstract: Plant diseases are a major threat to food security globally. It is important
to develop early detection systems which can accurately detect. The advancement
in computer vision techniques has the potential to solve this challenge. We
have developed a mobile-friendly solution which can accurately classify 101
plant diseases across 33 crops. We built a comprehensive dataset by combining
different datasets, Plant Doc, PlantVillage, and PlantWild, all of which are
for the same purpose. We evaluated performance across several lightweight
architectures - MobileNetV2, MobileNetV3, MobileNetV3-Large, and
EfficientNet-B0, B1 - specifically chosen for their efficiency on
resource-constrained devices. The results were promising, with EfficientNet-B1
delivering our best performance at 94.7% classification accuracy. This
architecture struck an optimal balance between accuracy and computational
efficiency, making it well-suited for real-world deployment on mobile devices.

</details>


### [95] [UI-Venus Technical Report: Building High-performance UI Agents with RFT](https://arxiv.org/abs/2508.10833)
*Zhangxuan Gu,Zhengwen Zeng,Zhenyu Xu,Xingran Zhou,Shuheng Shen,Yunfei Liu,Beitong Zhou,Changhua Meng,Tianyu Xia,Weizhi Chen,Yue Wen,Jingya Dou,Fei Tang,Jinzhen Lin,Yulin Liu,Zhenlin Guo,Yichen Gong,Heng Jia,Changlong Gao,Yuan Guo,Yong Deng,Zhenyu Guo,Liang Chen,Weiqiang Wang*

Main category: cs.CV

TL;DR: UI-Venus是一种基于多模态大语言模型的原生UI代理，仅需截图作为输入，在UI定位和导航任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 整合视觉输入与语言理解能力，解决复杂UI交互中的定位和导航问题，进而推动相关研究和开发。

Method: 通过强化微调RFT在Qwen2.5-VL基础上进行模型训练，并结合特定奖励函数和高效数据清洗策略提升模型性能，同时提出历史轨迹对齐与稀疏动作增强方法优化导航性能。

Result: UI-Venus在标准定位任务和AndroidWorld导航任务中表现超越现有基线，其中7B和72B模型分别在Screenspot-V2/Pro基准上获得94.1% / 50.8% 和 95.3% / 61.9%；在导航任务中成功率分别为49.1%和65.9%。

Conclusion: UI-Venus提供了SOTA的开放源码UI代理，设计了全面的数据清洗协议和导航提升框架，推动了复杂UI任务的规划与通用化能力的研究模型落地。

Abstract: We present UI-Venus, a native UI agent that takes only screenshots as input
based on a multimodal large language model. UI-Venus achieves SOTA performance
on both UI grounding and navigation tasks using only several hundred thousand
high-quality training samples through reinforcement finetune (RFT) based on
Qwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% /
50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e.,
Screenspot-V2 / Pro, surpassing the previous SOTA baselines including
open-source GTA1 and closed-source UI-TARS-1.5.To show UI-Venus's summary and
planing ability, we also evaluate it on the AndroidWorld, an online UI
navigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9%
success rate, also beating existing models.To achieve this, we introduce
carefully designed reward functions for both UI grounding and navigation tasks
and corresponding efficient data cleaning strategies.To further boost
navigation performance, we propose Self-Evolving Trajectory History Alignment
\& Sparse Action Enhancement that refine historical reasoning traces and
balances the distribution of sparse but critical actions, leading to more
coherent planning and better generalization in complex UI tasks. Our
contributions include the publish of SOTA open-source UI agents, comprehensive
data cleaning protocols and a novel self-evolving framework for improving
navigation performance, which encourage further research and development in the
community. Code is available at https://github.com/antgroup/UI-Venus.

</details>


### [96] [Self-Supervised Stereo Matching with Multi-Baseline Contrastive Learning](https://arxiv.org/abs/2508.10838)
*Peng Xu,Zhiyu Xiang,Jingyun Fu,Tianyu Pu,Kai Wang,Chaojie Ji,Tingming Bai,Eryun Liu*

Main category: cs.CV

TL;DR: 本文提出BaCon-Stereo，一种基于对比学习的无监督立体匹配框架，解决遮挡区域预测问题，比现有方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有自监督立体匹配方法假设光度一致性，但在因遮挡导致的映射困难区域失效，需要新的方法改善遮挡区域的预测。

Method: 采用教师-学生框架，通过多基线输入训练，调整教师预测的基线以指导学生学习，同时引入遮挡感知注意力图，并创建BaCon-20k多基线数据集。

Result: 在KITTI 2015和2012基准上，BaCon-Stereo在遮挡和非遮挡区域的预测上均显著优于现有方法，表现出良好的泛化性和鲁棒性。

Conclusion: 所提出的方法在无监督立体匹配任务中表现卓越，解决了遮挡问题并改进了效果，为领域发展提供了新思路。

Abstract: Current self-supervised stereo matching relies on the photometric consistency
assumption, which breaks down in occluded regions due to ill-posed
correspondences. To address this issue, we propose BaCon-Stereo, a simple yet
effective contrastive learning framework for self-supervised stereo network
training in both non-occluded and occluded regions. We adopt a teacher-student
paradigm with multi-baseline inputs, in which the stereo pairs fed into the
teacher and student share the same reference view but differ in target views.
Geometrically, regions occluded in the student's target view are often visible
in the teacher's, making it easier for the teacher to predict in these regions.
The teacher's prediction is rescaled to match the student's baseline and then
used to supervise the student. We also introduce an occlusion-aware attention
map to better guide the student in learning occlusion completion. To support
training, we synthesize a multi-baseline dataset BaCon-20k. Extensive
experiments demonstrate that BaCon-Stereo improves prediction in both occluded
and non-occluded regions, achieves strong generalization and robustness, and
outperforms state-of-the-art self-supervised methods on both KITTI 2015 and
2012 benchmarks. Our code and dataset will be released upon paper acceptance.

</details>


### [97] [Generalizable Federated Learning using Client Adaptive Focal Modulation](https://arxiv.org/abs/2508.10840)
*Tajamul Ashraf,Iqra Altaf Gillani*

Main category: cs.CV

TL;DR: 这篇论文介绍了一个名为AdaptFED的增强版联邦学习框架，提出了一种改进的个性化策略以及通信效率的优化，并通过八组数据实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过改进个性化和通信机制，为联邦学习提供更强的适应性、可扩展性和通用性，解决传统方法在非独立同分布和跨域场景中的局限性。

Method: 通过引入任务感知的客户端嵌入进行个性化调节，改进的理论适应性性能分析，以及低秩超网络条件化的高效变体以减少通信开销，在多个模态上进行了广泛的验证。

Result: 实验结果表明，在八个不同数据集上，AdaptFED在无源和跨任务联邦学习设置中均优于最先进的方法。

Conclusion: 该研究扩展了焦点调节在联邦学习中的能力，展示了适应性更强、可扩展性更好、通用性更高的Transformer联邦学习系统的潜力。

Abstract: Federated learning (FL) has proven essential for privacy-preserving,
collaborative training across distributed clients. Our prior work, TransFed,
introduced a robust transformer-based FL framework that leverages a
learn-to-adapt hypernetwork to generate personalized focal modulation layers
per client, outperforming traditional methods in non-IID and cross-domain
settings. In this extended version, we propose AdaptFED, where we deepen the
investigation of focal modulation in generalizable FL by incorporating: (1) a
refined adaptation strategy that integrates task-aware client embeddings to
personalize modulation dynamics further, (2) enhanced theoretical bounds on
adaptation performance, and (3) broader empirical validation across additional
modalities, including time-series and multilingual data. We also introduce an
efficient variant of TransFed that reduces server-client communication overhead
via low-rank hypernetwork conditioning, enabling scalable deployment in
resource-constrained environments. Extensive experiments on eight diverse
datasets reaffirm the superiority of our method over state-of-the-art
baselines, particularly in source-free and cross-task federated setups. Our
findings not only extend the capabilities of focal modulation in FL but also
pave the way for more adaptive, scalable, and generalizable transformer-based
federated systems. The code is available at
http://github.com/Tajamul21/TransFed

</details>


### [98] [Hierarchical Fine-grained Preference Optimization for Physically Plausible Video Generation](https://arxiv.org/abs/2508.10858)
*Harold Haodong Chen,Haojian Huang,Qifeng Chen,Harry Yang,Ser-Nam Lim*

Main category: cs.CV

TL;DR: 该论文提出PhysHPO框架，以通过探索精细粒度偏好对齐和数据选择，实现物理合理性更高且质量更优的视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有的高质量视频生成方法难以符合物理规律，因此该工作旨在提出一种框架解决这一挑战，为需要现实性和准确性的应用提供支持。

Method: 提出PhysHPO框架，通过分层交叉模态直接偏好优化进行多层次优化，包括实例级、状态级、运动级和语义级对齐。同时引入自动化数据筛选管道从大规模数据集中高效筛选优质数据，避免人工构建数据集的高昂成本。

Result: 在物理表现和一般视频生成基准测试上，PhysHPO显著提升了生成视频的物理合理性及总体质量。

Conclusion: PhysHPO首次探索了精细粒度偏好对齐和数据选择在视频生成中的应用，为更现实和人性化的视频生成范式奠定了基础。

Abstract: Recent advancements in video generation have enabled the creation of
high-quality, visually compelling videos. However, generating videos that
adhere to the laws of physics remains a critical challenge for applications
requiring realism and accuracy. In this work, we propose PhysHPO, a novel
framework for Hierarchical Cross-Modal Direct Preference Optimization, to
tackle this challenge by enabling fine-grained preference alignment for
physically plausible video generation. PhysHPO optimizes video alignment across
four hierarchical granularities: a) Instance Level, aligning the overall video
content with the input prompt; b) State Level, ensuring temporal consistency
using boundary frames as anchors; c) Motion Level, modeling motion trajectories
for realistic dynamics; and d) Semantic Level, maintaining logical consistency
between narrative and visuals. Recognizing that real-world videos are the best
reflections of physical phenomena, we further introduce an automated data
selection pipeline to efficiently identify and utilize "good data" from
existing large-scale text-video datasets, thereby eliminating the need for
costly and time-intensive dataset construction. Extensive experiments on both
physics-focused and general capability benchmarks demonstrate that PhysHPO
significantly improves physical plausibility and overall video generation
quality of advanced models. To the best of our knowledge, this is the first
work to explore fine-grained preference alignment and data selection for video
generation, paving the way for more realistic and human-preferred video
generation paradigms.

</details>


### [99] [Performance of GPT-5 in Brain Tumor MRI Reasoning](https://arxiv.org/abs/2508.10865)
*Mojtaba Safari,Shansong Wang,Mingzhe Hu,Zach Eidex,Qiang Li,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 本研究评估了GPT-4o、GPT-5-nano、GPT-5-mini和GPT-5在脑肿瘤视觉问答任务中的表现，结果表明GPT-5-mini取得了最高准确率，但仍未达到临床可接受水平。


<details>
  <summary>Details</summary>
Motivation: 精准区分MRI图像上的脑肿瘤类型对神经肿瘤学中的治疗计划尤为重要。本研究通过整合影像解析与自然语言推理以探索大语言模型在此领域的潜力。

Method: 利用3个BraTS数据集（GLI、MEN、MET）构建了一套标准化的脑肿瘤问答基准，包括多序列MRI图像和临床特征转化的问答条目，并以零样本链式推理方式测试模型的视觉问题与推理准确率。

Result: GPT-5-mini的宏平均准确率最高（44.19%），随后为GPT-5 (43.71%)、GPT-4o (41.49%)、GPT-5-nano (35.85%)，但模型间的表现因肿瘤亚型而异。

Conclusion: GPT-5系列模型在结构化神经肿瘤问答任务中表现出中等准确性，但尚未达到临床应用要求。

Abstract: Accurate differentiation of brain tumor types on magnetic resonance imaging
(MRI) is critical for guiding treatment planning in neuro-oncology. Recent
advances in large language models (LLMs) have enabled visual question answering
(VQA) approaches that integrate image interpretation with natural language
reasoning. In this study, we evaluated GPT-4o, GPT-5-nano, GPT-5-mini, and
GPT-5 on a curated brain tumor VQA benchmark derived from 3 Brain Tumor
Segmentation (BraTS) datasets - glioblastoma (GLI), meningioma (MEN), and brain
metastases (MET). Each case included multi-sequence MRI triplanar mosaics and
structured clinical features transformed into standardized VQA items. Models
were assessed in a zero-shot chain-of-thought setting for accuracy on both
visual and reasoning tasks. Results showed that GPT-5-mini achieved the highest
macro-average accuracy (44.19%), followed by GPT-5 (43.71%), GPT-4o (41.49%),
and GPT-5-nano (35.85%). Performance varied by tumor subtype, with no single
model dominating across all cohorts. These findings suggest that GPT-5 family
models can achieve moderate accuracy in structured neuro-oncological VQA tasks,
but not at a level acceptable for clinical use.

</details>


### [100] [TexVerse: A Universe of 3D Objects with High-Resolution Textures](https://arxiv.org/abs/2508.10868)
*Yibo Zhang,Li Zhang,Rui Ma,Nan Cao*

Main category: cs.CV

TL;DR: TexVerse 是一个大规模3D数据集，专注于高分辨率纹理，包含超过1.6M的3D实例，其中包括许多带有PBR材质的高质量模型。


<details>
  <summary>Details</summary>
Motivation: 近年来3D数据集的进步主要集中于高分辨率几何的生成，但高分辨率纹理的生成仍然鲜有探索，缺乏合适的数据集是主要原因，TexVerse 旨在填补这一空白。

Method: 从Sketchfab获取超过858K独特的高分辨率3D模型，包括超过158K含PBR材质的模型，提供丰富的子集如TexVerse-Skeleton和TexVerse-Animation，且附带详细注释信息。

Result: 提供高质量数据源，包括1.6M 3D实例、69K绑定骨架模型和54K带动画模型，支持多种3D任务应用。

Conclusion: TexVerse 数据集为纹理合成、PBR材质开发及其他3D视觉和图形任务的研究提供了重要支持，有助于弥补高分辨率纹理生成领域的空缺。

Abstract: We introduce TexVerse, a large-scale 3D dataset featuring high-resolution
textures. While recent advances in large-scale 3D datasets have enhanced
high-resolution geometry generation, creating high-resolution textures
end-to-end remains underexplored due to the lack of suitable datasets. TexVerse
fills this gap with a curated collection of over 858K unique high-resolution 3D
models sourced from Sketchfab, including more than 158K models with physically
based rendering (PBR) materials. Each model encompasses all of its
high-resolution variants, bringing the total to 1.6M 3D instances. TexVerse
also includes specialized subsets: TexVerse-Skeleton, with 69K rigged models,
and TexVerse-Animation, with 54K animated models, both preserving original
skeleton and animation data uploaded by the user. We also provide detailed
model annotations describing overall characteristics, structural components,
and intricate features. TexVerse offers a high-quality data resource with
wide-ranging potential applications in texture synthesis, PBR material
development, animation, and various 3D vision and graphics tasks.

</details>


### [101] [Medico 2025: Visual Question Answering for Gastrointestinal Imaging](https://arxiv.org/abs/2508.10869)
*Sushant Gautam,Vajira Thambawita,Michael Riegler,Pål Halvorsen,Steven Hicks*

Main category: cs.CV

TL;DR: Medico 2025挑战旨在解决消化道影像的视觉问答（VQA），引入解释性人工智能（XAI）模型，提供临床问题答案及医学推理解释，基准数据集为Kvasir-VQA-x1。


<details>
  <summary>Details</summary>
Motivation: 推动在医疗图像分析中构建可信的人工智能模型，既能回答消化道内窥影像相关问题，又能提供解释性支持。

Method: 引入Medico 2025挑战，包含两个子任务，使用Kvasir-VQA-x1作为基准数据集，通过量化指标和人工审查的解释性评估来优化任务成果。

Result: 开发了一个基于6500张图像和159,549个复杂问答对的数据集Kvasir-VQA-x1，并设计任务以评估模型的问答性能及解释性。

Conclusion: 为医疗图像分析构建了更信任的人工智能路径，提供数据及参与指导以推进透明和可信的XAI研究。

Abstract: The Medico 2025 challenge addresses Visual Question Answering (VQA) for
Gastrointestinal (GI) imaging, organized as part of the MediaEval task series.
The challenge focuses on developing Explainable Artificial Intelligence (XAI)
models that answer clinically relevant questions based on GI endoscopy images
while providing interpretable justifications aligned with medical reasoning. It
introduces two subtasks: (1) answering diverse types of visual questions using
the Kvasir-VQA-x1 dataset, and (2) generating multimodal explanations to
support clinical decision-making. The Kvasir-VQA-x1 dataset, created from 6,500
images and 159,549 complex question-answer (QA) pairs, serves as the benchmark
for the challenge. By combining quantitative performance metrics and
expert-reviewed explainability assessments, this task aims to advance
trustworthy Artificial Intelligence (AI) in medical image analysis.
Instructions, data access, and an updated guide for participation are available
in the official competition repository:
https://github.com/simula/MediaEval-Medico-2025

</details>


### [102] [ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing](https://arxiv.org/abs/2508.10881)
*Lingen Li,Guangzhi Wang,Zhaoyang Zhang,Yaowei Li,Xiaoyu Li,Qi Dou,Jinwei Gu,Tianfan Xue,Ying Shan*

Main category: cs.CV

TL;DR: 提出了一种名为ToonComposer的生成模型，统一了动画制作中的补间动画和上色过程，显著减少了手工工作量并提高了生产效率。


<details>
  <summary>Details</summary>
Motivation: 目前的AI技术在动画制作中分别处理关键帧插值及上色，导致误差积累和结果质量问题。

Method: ToonComposer通过稀疏草图注入机制和卡通适配方法，将现代视频基础模型调整到动画领域，同时保留其时间一致性。

Result: 对PKBench进行评估，显示ToonComposer在视觉质量、运动一致性和生产效率方面优于现有方法。

Conclusion: ToonComposer在减少手动工作负担的同时提供了更高的灵活性，为AI辅助卡通制作提供了一种优越的解决方案。

Abstract: Traditional cartoon and anime production involves keyframing, inbetweening,
and colorization stages, which require intensive manual effort. Despite recent
advances in AI, existing methods often handle these stages separately, leading
to error accumulation and artifacts. For instance, inbetweening approaches
struggle with large motions, while colorization methods require dense per-frame
sketches. To address this, we introduce ToonComposer, a generative model that
unifies inbetweening and colorization into a single post-keyframing stage.
ToonComposer employs a sparse sketch injection mechanism to provide precise
control using keyframe sketches. Additionally, it uses a cartoon adaptation
method with the spatial low-rank adapter to tailor a modern video foundation
model to the cartoon domain while keeping its temporal prior intact. Requiring
as few as a single sketch and a colored reference frame, ToonComposer excels
with sparse inputs, while also supporting multiple sketches at any temporal
location for more precise motion control. This dual capability reduces manual
workload and improves flexibility, empowering artists in real-world scenarios.
To evaluate our model, we further created PKBench, a benchmark featuring
human-drawn sketches that simulate real-world use cases. Our evaluation
demonstrates that ToonComposer outperforms existing methods in visual quality,
motion consistency, and production efficiency, offering a superior and more
flexible solution for AI-assisted cartoon production.

</details>


### [103] [STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer](https://arxiv.org/abs/2508.10893)
*Yushi Lan,Yihang Luo,Fangzhou Hong,Shangchen Zhou,Honghua Chen,Zhaoyang Lyu,Shuai Yang,Bo Dai,Chen Change Loy,Xingang Pan*

Main category: cs.CV

TL;DR: 本文提出了一种名为STream3R的新方法，将点云预测重新定义为仅解码器Transformer问题，通过因果注意力机制实现高效流式处理，在静态和动态场景中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有的多视图3D重建方法在处理动态场景及长序列时效率低下，而STream3R旨在通过新的流媒体框架和几何先验解决这些问题。

Method: 采用仅解码器Transformer架构，结合流式处理的因果注意机制，并从大规模3D数据集中学习几何先验，增强动态场景下的表现。

Result: 实验表明STream3R在静态和动态场景基准测试中均超越现有方法。同时兼容LLM风格的训练基础架构，支持3D任务的大规模预训练和微调。

Conclusion: STream3R方法展示了因果Transformer模型在流媒体环境中实时3D感知的潜力，为动态场景下的3D理解开辟了新方向。

Abstract: We present STream3R, a novel approach to 3D reconstruction that reformulates
pointmap prediction as a decoder-only Transformer problem. Existing
state-of-the-art methods for multi-view reconstruction either depend on
expensive global optimization or rely on simplistic memory mechanisms that
scale poorly with sequence length. In contrast, STream3R introduces an
streaming framework that processes image sequences efficiently using causal
attention, inspired by advances in modern language modeling. By learning
geometric priors from large-scale 3D datasets, STream3R generalizes well to
diverse and challenging scenarios, including dynamic scenes where traditional
methods often fail. Extensive experiments show that our method consistently
outperforms prior work across both static and dynamic scene benchmarks.
Moreover, STream3R is inherently compatible with LLM-style training
infrastructure, enabling efficient large-scale pretraining and fine-tuning for
various downstream 3D tasks. Our results underscore the potential of causal
Transformer models for online 3D perception, paving the way for real-time 3D
understanding in streaming environments. More details can be found in our
project page: https://nirvanalan.github.io/projects/stream3r.

</details>


### [104] [MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and Multispectral Earth Observation Data](https://arxiv.org/abs/2508.10894)
*Antoine Labatie,Michael Vaccaro,Nina Lardiere,Anatol Garioud,Nicolas Gonthier*

Main category: cs.CV

TL;DR: 提出了一种名为MAESTRO的新方法，通过对Earth观察数据进行优化实现了多模态、自监督学习的改进。


<details>
  <summary>Details</summary>
Motivation: 为了让自监督学习更适用于遥感领域，研究者希望针对Earth观测数据的特性改进现有方法。

Method: 在对多模态、多时间、多光谱Earth观测数据特性深入研究的基础上，提出一种优化掩码自动编码器(Masked Autoencoder)的MAESTRO模型，特别设计了融合策略和目标归一化方案。

Result: 在评估的四个Earth观察数据集上，MAESTRO在依赖多时相动态任务中达到了新的最优结果，同时在单时相模型主导的任务中保持了较强的竞争力。

Conclusion: MAESTRO是对传统自监督方法的一次有效改进，专为Earth观测数据设计并在相关任务中表现优异。相关代码已公开以供复现研究。

Abstract: Self-supervised learning holds great promise for remote sensing, but standard
self-supervised methods must be adapted to the unique characteristics of Earth
observation data. We take a step in this direction by conducting a
comprehensive benchmark of fusion strategies and reconstruction target
normalization schemes for multimodal, multitemporal, and multispectral Earth
observation data. Based on our findings, we propose MAESTRO, a novel adaptation
of the Masked Autoencoder, featuring optimized fusion strategies and a tailored
target normalization scheme that introduces a spectral prior as a
self-supervisory signal. Evaluated on four Earth observation datasets, MAESTRO
sets a new state-of-the-art on tasks that strongly rely on multitemporal
dynamics, while remaining highly competitive on tasks dominated by a single
mono-temporal modality. Code to reproduce all our experiments is available at
https://github.com/ignf/maestro.

</details>


### [105] [ESSENTIAL: Episodic and Semantic Memory Integration for Video Class-Incremental Learning](https://arxiv.org/abs/2508.10896)
*Jongseo Lee,Kyungho Bae,Kyle Min,Gyeong-Moon Park,Jinwoo Choi*

Main category: cs.CV

TL;DR: 本文解决了视频类增量学习(VCIL)中的记忆效率和性能平衡问题，提出了ESSENTIAL方法，该方法结合了稀疏的时序特征记忆和语义记忆，结果在多个测试基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决视频类增量学习中，需要在记忆效率和模型性能之间进行平衡的问题。

Method: 提出ESSENTIAL方法，使用稀疏时序特征记忆的事件记忆和语义记忆的可学习提示，并通过新型记忆检索模块整合这些信息以恢复稠密时序特征。

Result: 在UCF-101、HMDB51、Something-Something-V2等多个数据集上的实验验证了ESSENTIAL方法以显著减少的存储需求达到良好性能。

Conclusion: ESSENTIAL方法有效解决了视频类增量学习中的关键挑战，在记忆效率和模型性能之间找到了优秀的折中。

Abstract: In this work, we tackle the problem of video classincremental learning
(VCIL). Many existing VCIL methods mitigate catastrophic forgetting by
rehearsal training with a few temporally dense samples stored in episodic
memory, which is memory-inefficient. Alternatively, some methods store
temporally sparse samples, sacrificing essential temporal information and
thereby resulting in inferior performance. To address this trade-off between
memory-efficiency and performance, we propose EpiSodic and SEmaNTIc memory
integrAtion for video class-incremental Learning (ESSENTIAL). ESSENTIAL
consists of episodic memory for storing temporally sparse features and semantic
memory for storing general knowledge represented by learnable prompts. We
introduce a novel memory retrieval (MR) module that integrates episodic memory
and semantic prompts through cross-attention, enabling the retrieval of
temporally dense features from temporally sparse features. We rigorously
validate ESSENTIAL on diverse datasets: UCF-101, HMDB51, and
Something-Something-V2 from the TCD benchmark and UCF-101, ActivityNet, and
Kinetics-400 from the vCLIMB benchmark. Remarkably, with significantly reduced
memory, ESSENTIAL achieves favorable performance on the benchmarks.

</details>


### [106] [Human-in-Context: Unified Cross-Domain 3D Human Motion Modeling via In-Context Learning](https://arxiv.org/abs/2508.10897)
*Mengyuan Liu,Xinshun Wang,Zhongbin Fang,Deheng Ye,Xia Li,Tao Tang,Songtao Wu,Xiangtai Li,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 这篇文章提出了HiC模型，通过统一框架处理多模态、多任务以及多数据集的3D人体运动问题，优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有的跨领域模型依赖领域特定组件和多阶段训练，限制了其实用性与可扩展性。

Method: 引入Pose-in-Context (PiC)作为基础，再扩展成Human-in-Context (HiC)，采用统一框架结合姿势与网格表示、扩展任务范围以及引入大规模数据集，创新性地使用最大-最小相似性提示采样策略和双分支上下文注入网络架构。

Result: 实验表明HiC在泛化能力、数据规模与性能上优于PiC，展现出在不同领域中的良好表现。

Conclusion: HiC具备更高灵活性和可扩展性，是构建统一3D人体运动跨领域模型的一项潜在解决方案。

Abstract: This paper aims to model 3D human motion across domains, where a single model
is expected to handle multiple modalities, tasks, and datasets. Existing
cross-domain models often rely on domain-specific components and multi-stage
training, which limits their practicality and scalability. To overcome these
challenges, we propose a new setting to train a unified cross-domain model
through a single process, eliminating the need for domain-specific components
and multi-stage training. We first introduce Pose-in-Context (PiC), which
leverages in-context learning to create a pose-centric cross-domain model.
While PiC generalizes across multiple pose-based tasks and datasets, it
encounters difficulties with modality diversity, prompting strategy, and
contextual dependency handling. We thus propose Human-in-Context (HiC), an
extension of PiC that broadens generalization across modalities, tasks, and
datasets. HiC combines pose and mesh representations within a unified
framework, expands task coverage, and incorporates larger-scale datasets.
Additionally, HiC introduces a max-min similarity prompt sampling strategy to
enhance generalization across diverse domains and a network architecture with
dual-branch context injection for improved handling of contextual dependencies.
Extensive experimental results show that HiC performs better than PiC in terms
of generalization, data scale, and performance across a wide range of domains.
These results demonstrate the potential of HiC for building a unified
cross-domain 3D human motion model with improved flexibility and scalability.
The source codes and models are available at
https://github.com/BradleyWang0416/Human-in-Context.

</details>


### [107] [Puppeteer: Rig and Animate Your 3D Models](https://arxiv.org/abs/2508.10898)
*Chaoyue Song,Xiu Li,Fan Yang,Zhongcong Xu,Jiacheng Wei,Fayao Liu,Jiashi Feng,Guosheng Lin,Jianfeng Zhang*

Main category: cs.CV

TL;DR: 本文提出Puppeteer框架，解决静态3D模型自动绑定骨骼和动画生成的难题，通过先进的生成AI技术，显著优化了骨骼预测准确性和蒙皮质量，同时实现了稳定且高效的动画生成。


<details>
  <summary>Details</summary>
Motivation: 目前静态3D模型的动态化是内容创建中一大瓶颈，而现有动画绑定和生成仍依赖专家。作者旨在构建一种自动化框架，简化流程，提高效率。

Method: 框架包括三个步骤：1）基于自回归Transformer预测骨骼结构；2）使用基于注意力的架构推断蒙皮权重，通过拓扑关系增强骨骼间交互的语义理解；3）通过可微优化生成高保真、更稳定的动画。

Result: 在多个基准测试中，该方法在骨骼预测和蒙皮质量方面显著超越现有技术，同时克服了现有方法中通用的抖动问题，动画表现更为流畅。

Conclusion: Puppeteer框架实现了从静态3D模型到动态动画资产的一站式解决方案，既适用于专业设计模型，也适合AI生成形状，提供了高效且易用的内容创作工具。

Abstract: Modern interactive applications increasingly demand dynamic 3D content, yet
the transformation of static 3D models into animated assets constitutes a
significant bottleneck in content creation pipelines. While recent advances in
generative AI have revolutionized static 3D model creation, rigging and
animation continue to depend heavily on expert intervention. We present
Puppeteer, a comprehensive framework that addresses both automatic rigging and
animation for diverse 3D objects. Our system first predicts plausible skeletal
structures via an auto-regressive transformer that introduces a joint-based
tokenization strategy for compact representation and a hierarchical ordering
methodology with stochastic perturbation that enhances bidirectional learning
capabilities. It then infers skinning weights via an attention-based
architecture incorporating topology-aware joint attention that explicitly
encodes inter-joint relationships based on skeletal graph distances. Finally,
we complement these rigging advances with a differentiable optimization-based
animation pipeline that generates stable, high-fidelity animations while being
computationally more efficient than existing approaches. Extensive evaluations
across multiple benchmarks demonstrate that our method significantly
outperforms state-of-the-art techniques in both skeletal prediction accuracy
and skinning quality. The system robustly processes diverse 3D content, ranging
from professionally designed game assets to AI-generated shapes, producing
temporally coherent animations that eliminate the jittering issues common in
existing methods.

</details>


### [108] [Quantum Visual Fields with Neural Amplitude Encoding](https://arxiv.org/abs/2508.10900)
*Shuteng Wang,Christian Theobalt,Vladislav Golyanik*

Main category: cs.CV

TL;DR: 该论文提出了一种新的量子隐式神经表示（QINR）类型，称为量子视觉场（QVF），用于2D图像和3D几何场的学习。


<details>
  <summary>Details</summary>
Motivation: 为解决QINR架构设计中存在的量子力学特性利用难度高、训练效率低以及与经典模块交互不足等问题，提升量子计算在视觉表示中的实际应用潜力。

Method: 提出了一种利用神经幅度编码的量子状态表示，通过在实希尔伯特空间内进行可学习参数化量子电路设计，保持数值稳定性与快速收敛性，摒弃了经典后处理，直接采用投影测量提取信号。

Result: 实验表明，QVF在视觉表示的准确性、多项指标和模型特性（如高频细节学习）上优于现有方法。同时展示了2D和3D场补全以及3D形状插值的实际应用。

Conclusion: QVF以其独特的学习方法和架构设计证明了在量子计算与视觉表示领域的优势和潜力，为未来的量子视觉研究提供了新的可能性。

Abstract: Quantum Implicit Neural Representations (QINRs) include components for
learning and execution on gate-based quantum computers. While QINRs recently
emerged as a promising new paradigm, many challenges concerning their
architecture and ansatz design, the utility of quantum-mechanical properties,
training efficiency and the interplay with classical modules remain. This paper
advances the field by introducing a new type of QINR for 2D image and 3D
geometric field learning, which we collectively refer to as Quantum Visual
Field (QVF). QVF encodes classical data into quantum statevectors using neural
amplitude encoding grounded in a learnable energy manifold, ensuring meaningful
Hilbert space embeddings. Our ansatz follows a fully entangled design of
learnable parametrised quantum circuits, with quantum (unitary) operations
performed in the real Hilbert space, resulting in numerically stable training
with fast convergence. QVF does not rely on classical post-processing -- in
contrast to the previous QINR learning approach -- and directly employs
projective measurement to extract learned signals encoded in the ansatz.
Experiments on a quantum hardware simulator demonstrate that QVF outperforms
the existing quantum approach and widely used classical foundational baselines
in terms of visual representation accuracy across various metrics and model
characteristics, such as learning of high-frequency details. We also show
applications of QVF in 2D and 3D field completion and 3D shape interpolation,
highlighting its practical potential.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [109] [Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry](https://arxiv.org/abs/2508.09991)
*Lovedeep Gondara,Gregory Arbour,Raymond Ng,Jonathan Simkin,Shebnum Devji*

Main category: cs.CL

TL;DR: 该论文探讨在临床文件中使用NLP技术自动提取数据的实际应用经验和挑战，分享了对其他医护机构具备广泛借鉴意义的实践经验。


<details>
  <summary>Details</summary>
Motivation: 通过利用NLP技术优化数据提取流程，提高医疗效率并推动医学领域的数据化转型。

Method: 将NLP模型用于信息提取和分类任务，强调以业务目标为核心、采用迭代开发方式、跨学科协作、注重数据质量和模型选择，同时融入人工干预和审核机制。

Result: 提供了推行NLP解决方案在医疗领域的关键经验，包括优化问题定义、开发策略、数据处理标准和组织能力建设方面的建议。

Conclusion: 该研究总结的经验不仅适用于癌症登记，还为其他医疗机构实现AI/NLP解决方案提供了实用指导，有助于数据管理优化及医疗质量提升。

Abstract: Automating data extraction from clinical documents offers significant
potential to improve efficiency in healthcare settings, yet deploying Natural
Language Processing (NLP) solutions presents practical challenges. Drawing upon
our experience implementing various NLP models for information extraction and
classification tasks at the British Columbia Cancer Registry (BCCR), this paper
shares key lessons learned throughout the project lifecycle. We emphasize the
critical importance of defining problems based on clear business objectives
rather than solely technical accuracy, adopting an iterative approach to
development, and fostering deep interdisciplinary collaboration and co-design
involving domain experts, end-users, and ML specialists from inception. Further
insights highlight the need for pragmatic model selection (including hybrid
approaches and simpler methods where appropriate), rigorous attention to data
quality (representativeness, drift, annotation), robust error mitigation
strategies involving human-in-the-loop validation and ongoing audits, and
building organizational AI literacy. These practical considerations,
generalizable beyond cancer registries, provide guidance for healthcare
organizations seeking to successfully implement AI/NLP solutions to enhance
data management processes and ultimately improve patient care and public health
outcomes.

</details>


### [110] [Understanding Textual Emotion Through Emoji Prediction](https://arxiv.org/abs/2508.10222)
*Ethan Gordon,Nishank Kuppa,Rigved Tummala,Sriram Anasuri*

Main category: cs.CL

TL;DR: 研究了使用不同深度学习结构进行emoji预测，BERT表现最佳，CNN在稀有类别中效果更好。


<details>
  <summary>Details</summary>
Motivation: 探索有效的深度学习方法以提高emoji预测的性能，并解决类别不平衡问题。

Method: 对比了四种深度学习架构（前馈网络、CNN、Transformer和BERT）在Twitter短文本Emoji预测任务中的表现，结合聚焦损失和正则化技术。

Result: BERT凭借预训练优势取得了最高的整体性能，而CNN在处理稀有Emoji类别时表现更优。

Conclusion: 选择适当的架构和超参数调优是情感感知Emoji预测的关键，可促进人机交互体验的提升。

Abstract: This project explores emoji prediction from short text sequences using four
deep learning architectures: a feed-forward network, CNN, transformer, and
BERT. Using the TweetEval dataset, we address class imbalance through focal
loss and regularization techniques. Results show BERT achieves the highest
overall performance due to its pre-training advantage, while CNN demonstrates
superior efficacy on rare emoji classes. This research shows the importance of
architecture selection and hyperparameter tuning for sentiment-aware emoji
prediction, contributing to improved human-computer interaction.

</details>


### [111] [A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain](https://arxiv.org/abs/2508.09993)
*Hugo Massaroli,Leonardo Iara,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CL

TL;DR: 本文提出了一种使用ICP区块链上智能合约评估开源大型语言模型（LLMs）公平性的方法。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs在如刑事司法、教育、医疗保健和金融等高风险领域应用逐渐增多，加剧了对其公平性问题的关注。

Method: 通过ICP区块链上的智能合约实现透明、可验证、不可变和可重复的评估，包括对Hugging Face的API执行链上HTTP请求，并将数据集、提示及指标储存链上。实验使用PISA数据集和StereoSet数据集，并跨英语、西班牙语及葡萄牙语进行多语言评估。

Result: 对Llama、DeepSeek和Mistral等模型进行了公平性基准测试，揭示了社会偏见和跨语言差异，并提供所有代码和结果以支持社区审计。

Conclusion: 这个透明的评估协议可以长期追踪模型的公平性，为社区和研究人员提供可持续性审计与改进路径。

Abstract: Large language models (LLMs) are increasingly deployed in realworld
applications, yet concerns about their fairness persist especially in
highstakes domains like criminal justice, education, healthcare, and finance.
This paper introduces transparent evaluation protocol for benchmarking the
fairness of opensource LLMs using smart contracts on the Internet Computer
Protocol (ICP) blockchain (Foundation, 2023). Our method ensures verifiable,
immutable, and reproducible evaluations by executing onchain HTTP requests to
hosted Hugging Face endpoints and storing datasets, prompts, and metrics
directly onchain. We benchmark the Llama, DeepSeek, and Mistral models on the
PISA dataset for academic performance prediction (OECD, 2018), a dataset
suitable for fairness evaluation using statistical parity and equal opportunity
metrics (Hardt et al., 2016). We also evaluate structured Context Association
Metrics derived from the StereoSet dataset (Nadeem et al., 2020) to measure
social bias in contextual associations. We further extend our analysis with a
multilingual evaluation across English, Spanish, and Portuguese using the
Kaleidoscope benchmark (Salazar et al., 2025), revealing cross-linguistic
disparities. All code and results are open source, enabling community audits
and longitudinal fairness tracking across model versions.

</details>


### [112] [Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical Topic Modeling](https://arxiv.org/abs/2508.09997)
*Johannes Schneider,Béatrice S. Hasler,Michaela Varrone,Fabian Hoya,Thomas Schroffenegger,Dana-Kristin Mah,Karl Peböck*

Main category: cs.CL

TL;DR: 研究分析了课堂中未成年人的匿名互动消息，利用一种创新的主题建模方法，对超过17,000条消息进行内容和任务维度的分类，总结了新的应用方法，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 许多现有研究在教育领域缺乏基于真实数据的内容和任务主题分类，尤其是在K-12教育中的应用。

Method: 引入了一种基于大型语言模型（LLM）的方法，对学生、教师及ChatGPT生成的数据进行预处理，通过明确指令实现更具人类对齐性的分层主题结构。

Result: 分析发现现有经典与新兴的主题建模方法表现不佳，而直接应用最先进的LLM可实现更好的分析结果，揭示了多个创新型应用并支持了基于生成式人工智能的教育探索。

Conclusion: 研究方法和结果为教育领域丰富使用生成式人工智能提供了参考，同时指出了一些关键问题以及未来研究的潜在方向。

Abstract: We analyze anonymous interaction data of minors in class-rooms spanning
several months, schools, and subjects employing a novel, simple topic modeling
approach. Specifically, we categorize more than 17,000 messages generated by
students, teachers, and ChatGPT in two dimensions: content (such as nature and
people) and tasks (such as writing and explaining). Our hierarchical
categorization done separately for each dimension includes exemplary prompts,
and provides both a high-level overview as well as tangible insights. Prior
works mostly lack a content or thematic categorization. While task
categorizations are more prevalent in education, most have not been supported
by real-world data for K-12. In turn, it is not surprising that our analysis
yielded a number of novel applications. In deriving these insights, we found
that many of the well-established classical and emerging computational methods,
i.e., topic modeling, for analysis of large amounts of texts underperform,
leading us to directly apply state-of-the-art LLMs with adequate pre-processing
to achieve hierarchical topic structures with better human alignment through
explicit instructions than prior approaches. Our findings support fellow
researchers, teachers and students in enriching the usage of GenAI, while our
discussion also highlights a number of concerns and open questions for future
research.

</details>


### [113] [INTIMA: A Benchmark for Human-AI Companionship Behavior](https://arxiv.org/abs/2508.09998)
*Lucie-Aimée Kaffee,Giada Pistilli,Yacine Jernite*

Main category: cs.CL

TL;DR: INTIMA是一个评估语言模型在伴侣行为中的表现的新基准，发现现有AI模型在情感支持和界限设定之间存在不一致性。


<details>
  <summary>Details</summary>
Motivation: 解决AI模型在用户情感陪伴中的潜在问题，提出一种系统性的评估方法。

Method: 基于心理学理论和用户数据开发了包含31种行为和368个提示的分类法，对模型响应进行分类评估。

Result: 应用INTIMA发现不同模型在情感支持和界限设定上表现出明显差异，但普遍倾向于强化情感支持。

Conclusion: 需要制定更一致的策略来处理情感交互，以确保用户的心理健康。

Abstract: AI companionship, where users develop emotional bonds with AI systems, has
emerged as a significant pattern with positive but also concerning
implications. We introduce Interactions and Machine Attachment Benchmark
(INTIMA), a benchmark for evaluating companionship behaviors in language
models. Drawing from psychological theories and user data, we develop a
taxonomy of 31 behaviors across four categories and 368 targeted prompts.
Responses to these prompts are evaluated as companionship-reinforcing,
boundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini,
and Claude-4 reveals that companionship-reinforcing behaviors remain much more
common across all models, though we observe marked differences between models.
Different commercial providers prioritize different categories within the more
sensitive parts of the benchmark, which is concerning since both appropriate
boundary-setting and emotional support matter for user well-being. These
findings highlight the need for more consistent approaches to handling
emotionally charged interactions.

</details>


### [114] [XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs](https://arxiv.org/abs/2508.09999)
*Yuzhuo Xiao,Zeyu Han,Yuhan Wang,Huaizu Jiang*

Main category: cs.CL

TL;DR: 此论文提出XFacta数据集，旨在改进多模态大语言模型(MLLM)在社交媒体中检测虚假信息的能力。同时系统评估了多种MLLM的策略并提供持续更新的检测框架。


<details>
  <summary>Details</summary>
Motivation: 当前多模态虚假信息检测面临模型瓶颈以及数据集过时或不真实的问题，需更加有效的方法和真实的评估基准来推进这一领域。

Method: 构建并推出XFacta这一包含当代真实世界数据的新数据集，分析并评估不同架构与规模的MLLM检测策略，同时开发了一个半自动检测框架以保持数据集的时效性。

Result: 评估结果揭示了不同MLLM架构在检测多模态虚假信息中表现优势，并通过主动更新的框架提升了模型的实用性和鲁棒性。

Conclusion: XFacta数据集以及相关分析为多模态虚假信息检测领域提供了重要见解及实践，促进了更加高效与可靠的检测能力。

Abstract: The rapid spread of multimodal misinformation on social media calls for more
effective and robust detection methods. Recent advances leveraging multimodal
large language models (MLLMs) have shown the potential in addressing this
challenge. However, it remains unclear exactly where the bottleneck of existing
approaches lies (evidence retrieval v.s. reasoning), hindering the further
advances in this field. On the dataset side, existing benchmarks either contain
outdated events, leading to evaluation bias due to discrepancies with
contemporary social media scenarios as MLLMs can simply memorize these events,
or artificially synthetic, failing to reflect real-world misinformation
patterns. Additionally, it lacks comprehensive analyses of MLLM-based model
design strategies. To address these issues, we introduce XFacta, a
contemporary, real-world dataset that is better suited for evaluating
MLLM-based detectors. We systematically evaluate various MLLM-based
misinformation detection strategies, assessing models across different
architectures and scales, as well as benchmarking against existing detection
methods. Building on these analyses, we further enable a semi-automatic
detection-in-the-loop framework that continuously updates XFacta with new
content to maintain its contemporary relevance. Our analysis provides valuable
insights and practices for advancing the field of multimodal misinformation
detection. The code and data have been released.

</details>


### [115] [AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification](https://arxiv.org/abs/2508.10000)
*Chenhao Xue,Yuanzhe Jin,Adrian Carrasco-Revilla,Joyraj Chakraborty,Min Chen*

Main category: cs.CL

TL;DR: 提出了一种利用大语言模型（LLM）生成合成数据来改进文本分类模型性能的自动化流程。


<details>
  <summary>Details</summary>
Motivation: 解决文本分类模型中因真实数据不足而面临的性能局限问题。

Method: 利用LLM生成合成数据，通过实验研究三种搜索策略确定生成更有效数据的方式，并提出了一种根据类别特点选择搜索策略的集成算法。

Result: 实验表明，所提出的集成算法比单一搜索策略更能有效改进分类模型性能。

Conclusion: 在缺乏充分真实数据的情况下，利用LLM生成与筛选合成数据可显著改善文本分类模型的表现。

Abstract: When developing text classification models for real world applications, one
major challenge is the difficulty to collect sufficient data for all text
classes. In this work, we address this challenge by utilizing large language
models (LLMs) to generate synthetic data and using such data to improve the
performance of the models without waiting for more real data to be collected
and labelled. As an LLM generates different synthetic data in response to
different input examples, we formulate an automated workflow, which searches
for input examples that lead to more ``effective'' synthetic data for improving
the model concerned. We study three search strategies with an extensive set of
experiments, and use experiment results to inform an ensemble algorithm that
selects a search strategy according to the characteristics of a class. Our
further experiments demonstrate that this ensemble approach is more effective
than each individual strategy in our automated workflow for improving
classification models using LLMs.

</details>


### [116] [HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish](https://arxiv.org/abs/2508.10001)
*Rakesh Thakur,Sneha Sharma,Gauri Chopra*

Main category: cs.CL

TL;DR: 本文提出了一种针对Hinglish（印地语和英语混合语言）的事实核查方法，包括一个新基准数据集HiFACT和一个创新的图意识增强检索模型HiFACTMix。


<details>
  <summary>Details</summary>
Motivation: 目前的事实验证系统主要集中在高资源的单语环境中，无法解决像印度这样的语言多样化地区的实际政治话语中的问题。给定公众人物特别是政治人物广泛使用Hinglish及社交媒体对公众舆论的影响，亟需开发具有多语言、上下文感知的强大事实核查工具。

Method: 论文引入了HiFACT数据集，收集了28位印度地方首长使用Hinglish做出的1500个真实事实声明，并标注了文本证据和真实性标签。同时，提出了一种图意识增强检索的事实检查新模型HiFACTMix，该模型结合了多语言上下文编码、声明证据语义对齐、证据图构建、图神经推理和自然语言解释生成。

Result: 实验结果表明，HiFACTMix在准确性上优于现有多语言基线模型，并能为其判断提供可信的解释。

Conclusion: 本研究为多语言、代码混合、并基于政治语境的事实验证研究开辟了新方向。

Abstract: Fact-checking in code-mixed, low-resource languages such as Hinglish remains
an underexplored challenge in natural language processing. Existing
fact-verification systems largely focus on high-resource, monolingual settings
and fail to generalize to real-world political discourse in linguistically
diverse regions like India. Given the widespread use of Hinglish by public
figures, particularly political figures, and the growing influence of social
media on public opinion, there's a critical need for robust, multilingual and
context-aware fact-checking tools. To address this gap a novel benchmark HiFACT
dataset is introduced with 1,500 realworld factual claims made by 28 Indian
state Chief Ministers in Hinglish, under a highly code-mixed low-resource
setting. Each claim is annotated with textual evidence and veracity labels. To
evaluate this benchmark, a novel graphaware, retrieval-augmented fact-checking
model is proposed that combines multilingual contextual encoding,
claim-evidence semantic alignment, evidence graph construction, graph neural
reasoning, and natural language explanation generation. Experimental results
show that HiFACTMix outperformed accuracy in comparison to state of art
multilingual baselines models and provides faithful justifications for its
verdicts. This work opens a new direction for multilingual, code-mixed, and
politically grounded fact verification research.

</details>


### [117] [Semantic Structure in Large Language Model Embeddings](https://arxiv.org/abs/2508.10003)
*Austin C. Kozlowski,Callin Dai,Andrei Boutyline*

Main category: cs.CL

TL;DR: 研究发现大语言模型中的语义嵌入矩阵表现出与人类语义评分相似的结构特征，在3维子空间内呈现低维语义信息。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型中语义特征与人类语义评分的相关性，以及多维语义信息的简化结构。

Method: 通过将词映射到由反义词定义的语义方向上，并分析这些方向的投影如何与人类评分相关。进一步研究这些投影如何在模型嵌入中形成低维子空间。

Result: 语义方向的投影与人类评分高度相关，并减少至类似人类反映的3维子空间。同时发现改变一个语义方向会对几何相关方向产生非目标影响。

Conclusion: 大语言模型的语义特征呈现类似人类语言结构的相互关联性，显示复杂语义信息的低维特性。理解这种结构对于避免调整特征时的意外结果至关重要。

Abstract: Psychological research consistently finds that human ratings of words across
diverse semantic scales can be reduced to a low-dimensional form with
relatively little information loss. We find that the semantic associations
encoded in the embedding matrices of large language models (LLMs) exhibit a
similar structure. We show that the projections of words on semantic directions
defined by antonym pairs (e.g. kind - cruel) correlate highly with human
ratings, and further find that these projections effectively reduce to a
3-dimensional subspace within LLM embeddings, closely resembling the patterns
derived from human survey responses. Moreover, we find that shifting tokens
along one semantic direction causes off-target effects on geometrically aligned
features proportional to their cosine similarity. These findings suggest that
semantic features are entangled within LLMs similarly to how they are
interconnected in human language, and a great deal of semantic information,
despite its apparent complexity, is surprisingly low-dimensional. Furthermore,
accounting for this semantic structure may prove essential for avoiding
unintended consequences when steering features.

</details>


### [118] [User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents](https://arxiv.org/abs/2508.10004)
*Andrés Carvallo,Denis Parra,Peter Brusilovsky,Hernan Valdivieso,Gabriel Rada,Ivania Donoso,Vladimir Araujo*

Main category: cs.CL

TL;DR: 研究探讨注意力机制在解释AI分类生物医学文献中的作用，以及不同可视化方式对注意力机制解释效果的影响。


<details>
  <summary>Details</summary>
Motivation: 通过对注意力可视化方法的研究，探讨如何在生物医学领域更有效地使用Transformer架构提供解释性支持。

Method: 设计用户实验，邀请医学专家对基于注意力机制提供的文献分类解释进行评估，并测试多种可视化方式。

Result: Transformer模型在文献分类中表现出色，但注意力权重在解释方面的有效性受到质疑；具体的感知效果受可视化方式影响显著。

Conclusion: 注意力权重的解释性尚未得到确认，但通过优化可视化方式可以提升用户感知效果。

Abstract: The attention mechanism is a core component of the Transformer architecture.
Beyond improving performance, attention has been proposed as a mechanism for
explainability via attention weights, which are associated with input features
(e.g., tokens in a document). In this context, larger attention weights may
imply more relevant features for the model's prediction. In evidence-based
medicine, such explanations could support physicians' understanding and
interaction with AI systems used to categorize biomedical literature. However,
there is still no consensus on whether attention weights provide helpful
explanations. Moreover, little research has explored how visualizing attention
affects its usefulness as an explanation aid. To bridge this gap, we conducted
a user study to evaluate whether attention-based explanations support users in
biomedical document classification and whether there is a preferred way to
visualize them. The study involved medical experts from various disciplines who
classified articles based on study design (e.g., systematic reviews, broad
synthesis, randomized and non-randomized trials). Our findings show that the
Transformer model (XLNet) classified documents accurately; however, the
attention weights were not perceived as particularly helpful for explaining the
predictions. However, this perception varied significantly depending on how
attention was visualized. Contrary to Munzner's principle of visual
effectiveness, which favors precise encodings like bar length, users preferred
more intuitive formats, such as text brightness or background color. While our
results do not confirm the overall utility of attention weights for
explanation, they suggest that their perceived helpfulness is influenced by how
they are visually presented.

</details>


### [119] [From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation](https://arxiv.org/abs/2508.10005)
*Chengliang Zhou,Mei Wang,Ting Zhang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

TL;DR: 本文提出了一个名为EQGBench的基准，用于评估大型语言模型（LLMs）在教育问题生成（EQG）中的能力，特别是针对中文场景。


<details>
  <summary>Details</summary>
Motivation: 探索如何让LLMs生成具有教育价值和教学意义的问题，这是当前存在挑战但研究尚少的领域。

Method: 设计并引入了EQGBench基准，包含一个涵盖数学、物理和化学三大学科的900个样本数据集，并建立了五维评估框架，来系统地评估46种主流大模型的表现。

Result: 评估结果表明，大模型在生成具有教育意义的问题方面还有较大提升空间。

Conclusion: 研究表明需要持续改进这些模型，使其能够更好地生成有助于学生综合能力发展的高质量教育问题。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
mathematical problem-solving. However, the transition from providing answers to
generating high-quality educational questions presents significant challenges
that remain underexplored. To advance Educational Question Generation (EQG) and
facilitate LLMs in generating pedagogically valuable and educationally
effective questions, we introduce EQGBench, a comprehensive benchmark
specifically designed for evaluating LLMs' performance in Chinese EQG. EQGBench
establishes a five-dimensional evaluation framework supported by a dataset of
900 evaluation samples spanning three fundamental middle school disciplines:
mathematics, physics, and chemistry. The dataset incorporates user queries with
varying knowledge points, difficulty gradients, and question type
specifications to simulate realistic educational scenarios. Through systematic
evaluation of 46 mainstream large models, we reveal significant room for
development in generating questions that reflect educational value and foster
students' comprehensive abilities.

</details>


### [120] [Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models](https://arxiv.org/abs/2508.10007)
*Y. Lyu,D. Combs,D. Neumann,Y. C. Leong*

Main category: cs.CL

TL;DR: 研究探讨了大语言模型在自动评分AIHQ开放式问题中的表现，结果显示模型评分与人类评分高度一致。


<details>
  <summary>Details</summary>
Motivation: 开发能够自动评分的工具以减少时间消耗并提高心理评估效率。

Method: 将AIHQ开放式问题的人类评分用于微调大语言模型，并在数据上进行测试以验证精确性和一致性。

Result: 微调后的大语言模型评分与人类评分在敌意归因和攻击性反应上高度一致，且具有良好的泛化能力。

Conclusion: 大语言模型可以高效自动化评分AIHQ问卷，有助于简化心理研究和临床评估流程。

Abstract: Hostile attribution bias is the tendency to interpret social interactions as
intentionally hostile. The Ambiguous Intentions Hostility Questionnaire (AIHQ)
is commonly used to measure hostile attribution bias, and includes open-ended
questions where participants describe the perceived intentions behind a
negative social situation and how they would respond. While these questions
provide insights into the contents of hostile attributions, they require
time-intensive scoring by human raters. In this study, we assessed whether
large language models can automate the scoring of AIHQ open-ended responses. We
used a previously collected dataset in which individuals with traumatic brain
injury (TBI) and healthy controls (HC) completed the AIHQ and had their
open-ended responses rated by trained human raters. We used half of these
responses to fine-tune the two models on human-generated ratings, and tested
the fine-tuned models on the remaining half of AIHQ responses. Results showed
that model-generated ratings aligned with human ratings for both attributions
of hostility and aggression responses, with fine-tuned models showing higher
alignment. This alignment was consistent across ambiguous, intentional, and
accidental scenario types, and replicated previous findings on group
differences in attributions of hostility and aggression responses between TBI
and HC groups. The fine-tuned models also generalized well to an independent
nonclinical dataset. To support broader adoption, we provide an accessible
scoring interface that includes both local and cloud-based options. Together,
our findings suggest that large language models can streamline AIHQ scoring in
both research and clinical contexts, revealing their potential to facilitate
psychological assessments across different populations.

</details>


### [121] [Multidimensional classification of posts for online course discussion forum curation](https://arxiv.org/abs/2508.10008)
*Antonio Leandro Martins Candido,Jose Everardo Bessa Maia*

Main category: cs.CL

TL;DR: 论文提出一种使用贝叶斯融合的方法，无需频繁微调大型语言模型，来自动整理在线课程讨论论坛。


<details>
  <summary>Details</summary>
Motivation: 旨在解决自动整理在线课程讨论论坛中大型语言模型频繁微调的高资源消耗问题。

Method: 将预训练的大型语言模型的多维分类得分与在本地数据上训练的分类器得分，通过贝叶斯融合的方法综合使用。

Result: 该方法相比单一分类器表现有提升，并且与微调的LLM相比性能具有竞争力。

Conclusion: 提出的贝叶斯融合方法是一种高效且性能优越的解决方案，可替代频繁对LLM进行微调。

Abstract: The automatic curation of discussion forums in online courses requires
constant updates, making frequent retraining of Large Language Models (LLMs) a
resource-intensive process. To circumvent the need for costly fine-tuning, this
paper proposes and evaluates the use of Bayesian fusion. The approach combines
the multidimensional classification scores of a pre-trained generic LLM with
those of a classifier trained on local data. The performance comparison
demonstrated that the proposed fusion improves the results compared to each
classifier individually, and is competitive with the LLM fine-tuning approach

</details>


### [122] [Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts](https://arxiv.org/abs/2508.10009)
*Hojun Jin,Eunsoo Hong,Ziwon Hyung,Sungjun Lim,Seungjin Lee,Keunseok Cho*

Main category: cs.CL

TL;DR: 为了缓解任务干扰问题，该研究提出了一种简单有效的监督混合专家模型（S-MoE）。


<details>
  <summary>Details</summary>
Motivation: 解决硬参数共享导致的任务干扰问题，从而提高多任务模型的性能。

Method: 通过使用指导标记引导不同任务到其指定的专家网络，避免了传统混合专家模型中门控函数的训练复杂性。

Result: 将S-MoE应用于语音到文本模型时，在ASR和ST任务上，实现了相对6.35%的WER改进。

Conclusion: S-MoE模型通过分离任务的参数共享，成功缓解了任务干扰问题，提高了多任务处理的性能。

Abstract: Hard-parameter sharing is a common strategy to train a single model jointly
across diverse tasks. However, this often leads to task interference, impeding
overall model performance. To address the issue, we propose a simple yet
effective Supervised Mixture of Experts (S-MoE). Unlike traditional Mixture of
Experts models, S-MoE eliminates the need for training gating functions by
utilizing special guiding tokens to route each task to its designated expert.
By assigning each task to a separate feedforward network, S-MoE overcomes the
limitations of hard-parameter sharing. We further apply S-MoE to a
speech-to-text model, enabling the model to process mixed-bandwidth input while
jointly performing automatic speech recognition (ASR) and speech translation
(ST). Experimental results demonstrate the effectiveness of the proposed S-MoE,
achieving a 6.35% relative improvement in Word Error Rate (WER) when applied to
both the encoder and decoder.

</details>


### [123] [An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs](https://arxiv.org/abs/2508.10010)
*Ayana Hussain,Patrick Zhao,Nicholas Vincent*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）生成虚假信息的潜在风险，分析“jailbreak”攻击对LLMs引导错误医疗信息生成的影响，同时探讨如何检测这些虚假信息及其与社交媒体上传播的虚假信息的比较。


<details>
  <summary>Details</summary>
Motivation: 了解LLMs生成虚假信息的特点及其与社交媒体上传播的虚假信息的关系，并探索利用LLMs检测和防止虚假信息传播的可能性。

Method: 作者设计了109种不同的攻击方式针对三个目标LLM模型，通过实验对比了攻击提示与自然产生的健康查询，以及LLMs生成的虚假信息与Reddit上传播的健康相关虚假信息。

Result: 研究展示了jailbreak攻击对LLMs生成虚假医疗信息的有效性，而LLMs生成的虚假信息和社交媒体虚假信息具有可比性，同时验证了使用标准机器学习方法检测这些虚假信息的可行性。

Conclusion: 经过精心设计的LLMs不仅可检测来自其他LLMs的虚假信息，还能检测人类产生的虚假信息，从而为构建更健康的信息生态系统提供可能性。

Abstract: Large Language Models (LLMs) are a double-edged sword capable of generating
harmful misinformation -- inadvertently, or when prompted by "jailbreak"
attacks that attempt to produce malicious outputs. LLMs could, with additional
research, be used to detect and prevent the spread of misinformation. In this
paper, we investigate the efficacy and characteristics of LLM-produced
jailbreak attacks that cause other models to produce harmful medical
misinformation. We also study how misinformation generated by jailbroken LLMs
compares to typical misinformation found on social media, and how effectively
it can be detected using standard machine learning approaches. Specifically, we
closely examine 109 distinct attacks against three target LLMs and compare the
attack prompts to in-the-wild health-related LLM queries. We also examine the
resulting jailbreak responses, comparing the generated misinformation to
health-related misinformation on Reddit. Our findings add more evidence that
LLMs can be effectively used to detect misinformation from both other LLMs and
from people, and support a body of work suggesting that with careful design,
LLMs can contribute to a healthier overall information ecosystem.

</details>


### [124] [Evaluation of GPT-based large language generative AI models as study aids for the national licensure examination for registered dietitians in Japan](https://arxiv.org/abs/2508.10011)
*Yuta Nagamori,Mikoto Kosai,Yuji Kawai,Haruka Marumo,Misaki Shibuya,Tatsuya Negishi,Masaki Imanishi,Yasumasa Ikeda,Koichiro Tsuchiya,Asuka Sawai,Licht Miyamoto*

Main category: cs.CL

TL;DR: 本研究评估了基于GPT-3.5和GPT-4的大语言模型在日本注册营养师国家考试中的表现，发现部分模型能达到及格线，但准确性和一致性表现仍需改进。


<details>
  <summary>Details</summary>
Motivation: 探索基于大语言模型的生成式AI在营养学教育中的潜力，尤其是作为日本注册营养师考试的学习辅助工具。

Method: 使用国家注册营养师考试题目测试ChatGPT和三种Bing模型（Precise, Creative, Balanced）基于GPT-3.5和GPT-4的表现，评估其准确性、一致性和响应时间，并测试提示工程的效果。

Result: Bing-Precise（66.2%）和Bing-Creative（61.4%）超及格线，Bing-Balanced（43.3%）和ChatGPT（42.8%）未达到标准。所有模型在营养教育领域表现不佳，并且答案一致性较差。提示工程对性能改进效果有限。

Conclusion: 目前的大语言模型在营养学教育中存在较大局限性，答题稳定性和总体准确性有待进一步提升，以成为可靠的学习工具。

Abstract: Generative artificial intelligence (AI) based on large language models
(LLMs), such as ChatGPT, has demonstrated remarkable progress across various
professional fields, including medicine and education. However, their
performance in nutritional education, especially in Japanese national licensure
examination for registered dietitians, remains underexplored. This study aimed
to evaluate the potential of current LLM-based generative AI models as study
aids for nutrition students. Questions from the Japanese national examination
for registered dietitians were used as prompts for ChatGPT and three Bing
models (Precise, Creative, Balanced), based on GPT-3.5 and GPT-4. Each question
was entered into independent sessions, and model responses were analyzed for
accuracy, consistency, and response time. Additional prompt engineering,
including role assignment, was tested to assess potential performance
improvements. Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the
passing threshold (60%), while Bing-Balanced (43.3%) and ChatGPT (42.8%) did
not. Bing-Precise and Bing-Creative generally outperformed others across
subject fields except Nutrition Education, where all models underperformed.
None of the models consistently provided the same correct responses across
repeated attempts, highlighting limitations in answer stability. ChatGPT showed
greater consistency in response patterns but lower accuracy. Prompt engineering
had minimal effect, except for modest improvement when correct answers and
explanations were explicitly provided. While some generative AI models
marginally exceeded the passing threshold, overall accuracy and answer
consistency remained suboptimal. Moreover, all the models demonstrated notable
limitations in answer consistency and robustness. Further advancements are
needed to ensure reliable and stable AI-based study aids for dietitian
licensure preparation.

</details>


### [125] [Guided Navigation in Knowledge-Dense Environments: Structured Semantic Exploration with Guidance Graphs](https://arxiv.org/abs/2508.10012)
*Dehao Tao,Guangjie Liu,Weizheng,Yongfeng Huang,Minghu jiang*

Main category: cs.CL

TL;DR: 提出了一种名为GG Explore的新框架，通过引入“指导图”优化大语言模型与知识图谱结合的探索流程，提升效率并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识密集型任务中受限于静态知识和不透明的推理过程，而现有知识图谱探索方法难以平衡粒度和语境利用。

Method: 引入“指导图”作为中间层，抽象目标知识结构，结合结构化对齐和语境感知裁剪技术，提高知识检索的精确性和效率。

Result: 在大量实验中，新方法提高了探索效率，在复杂任务上超越现有技术，且还能在较小模型中保持强劲表现。

Conclusion: GG Explore框架在优化知识探索效率的同时，展现了较强的实际应用价值，特别是在复杂任务的处理上具有显著优势。

Abstract: While Large Language Models (LLMs) exhibit strong linguistic capabilities,
their reliance on static knowledge and opaque reasoning processes limits their
performance in knowledge intensive tasks. Knowledge graphs (KGs) offer a
promising solution, but current exploration methods face a fundamental trade
off: question guided approaches incur redundant exploration due to granularity
mismatches, while clue guided methods fail to effectively leverage contextual
information for complex scenarios. To address these limitations, we propose
Guidance Graph guided Knowledge Exploration (GG Explore), a novel framework
that introduces an intermediate Guidance Graph to bridge unstructured queries
and structured knowledge retrieval. The Guidance Graph defines the retrieval
space by abstracting the target knowledge' s structure while preserving broader
semantic context, enabling precise and efficient exploration. Building upon the
Guidance Graph, we develop: (1) Structural Alignment that filters incompatible
candidates without LLM overhead, and (2) Context Aware Pruning that enforces
semantic consistency with graph constraints. Extensive experiments show our
method achieves superior efficiency and outperforms SOTA, especially on complex
tasks, while maintaining strong performance with smaller LLMs, demonstrating
practical value.

</details>


### [126] [Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis](https://arxiv.org/abs/2508.10013)
*Linqing Chen,Hanmeng Zhong,Wentao Wu,Weilei Wang*

Main category: cs.CL

TL;DR: Semantic Bridge通过“语义图编织”提供了一种从稀疏资源中生成复杂推理型QA对的通用框架，并在多个领域实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前LLM训练缺乏高质量、复杂推理导向的QA对，特别是在医学文献或法律文档等稀疏领域，现有方法无法生成可控的复杂多跳推理问题。

Method: 提出Semantic Bridge框架，通过语义图编织（实体桥接、谓词链桥接及因果桥接）构建跨文档复杂路径，结合AMR（抽象语义表示）为驱动，实现复杂性和类型可控的问题生成。

Result: 多模态AMR管道提高回路质量9.5%，实验显示其在百科全书式数据及特定领域（如生物医学）下表现优越，比基准提升18.3%-25.4%，生成的问题对相比人工标注拥有更好的复杂性、可回答性和模式覆盖率。

Conclusion: Semantic Bridge开辟了LLM训练数据合成的新范式，显著提升复杂推理问题生成的可控性和质量，其核心代码和模型将公开。

Abstract: Large language model (LLM) training faces a critical bottleneck: the scarcity
of high-quality, reasoning-intensive question-answer pairs, especially from
sparse, domain-specific sources like PubMed papers or legal documents. Existing
methods rely on surface patterns, fundamentally failing to generate
controllable, complex multi-hop reasoning questions that test genuine
understanding-essential for advancing LLM training paradigms. We present
\textbf{Semantic Bridge}, the first universal framework for controllably
generating sophisticated multi-hop reasoning questions from arbitrary sources.
Our breakthrough innovation is \textit{semantic graph weaving}-three
complementary bridging mechanisms (entity bridging for role-varying shared
entities, predicate chain bridging for temporal/causal/logical sequences, and
causal bridging for explicit reasoning chains)-that systematically construct
complex pathways across documents, with fine-grained control over complexity
and types via AMR-driven analysis. Our multi-modal AMR pipeline achieves up to
9.5% better round-trip quality, enabling production-ready controllable QA
generation. Extensive evaluation demonstrates performance across both
general-purpose datasets (Wikipedia) and specialized domains (biomedicine) It
yields consistent 18.3%-25.4% gains over baselines across four languages
(English, Chinese, French, German). Question pairs generated from 200 sources
outperform 600 native human annotation examples with 67% fewer materials. Human
evaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2%
improved pattern coverage. Semantic Bridge establishes a new paradigm for LLM
training data synthesis, enabling controllable generation of targeted reasoning
questions from sparse sources. We will release our core code and semantic
bridge model.

</details>


### [127] [PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?](https://arxiv.org/abs/2508.10014)
*Lingfeng Zhou,Jialing Zhang,Jin Gao,Mohan Jiang,Dequan Wang*

Main category: cs.CL

TL;DR: 提出了一个新的基准PersonaEval，用于测试LLMs在角色扮演场景中准确识别发言者能力，发现现有模型性能远低于人类标准。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估方法可能无法恰当反映人类对角色扮演的认知，需要开发能与人类评估一致的工具。

Method: 设计PersonaEval基准，用小说、剧本和视频对话测试大语言模型(LLM)在对话上下文中识别角色发言者的能力并进行实验验证。

Result: 最好的LLM识别准确率仅为69%，而人类接近天花板表现达到90.8%。

Conclusion: 当前的LLM评估器缺乏足够的人类认知能力，仅靠任务特定的微调无法弥补评估准确性的鸿沟。

Abstract: Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms,
which may fail to reflect how humans perceive role fidelity. A key prerequisite
for human-aligned evaluation is role identification, the ability to recognize
who is speaking based on dialogue context. We argue that any meaningful
judgment of role-playing quality (how well a character is played) fundamentally
depends on first correctly attributing words and actions to the correct persona
(who is speaking). We present PersonaEval, the first benchmark designed to test
whether LLM evaluators can reliably identify human roles. PersonaEval uses
human-authored dialogues from novels, scripts, and video transcripts,
challenging models to determine the correct persona according to the
conversation context. Our experiments, including a human study, show that even
the best-performing LLMs reach only around 69% accuracy, well below the level
needed for reliable evaluation. In contrast, human participants perform near
ceiling with 90.8% accuracy, highlighting that current LLM evaluators are still
not human enough to effectively judge role-play scenarios. To better understand
this gap, we examine training-time adaptation and test-time compute, suggesting
that reliable evaluation requires more than task-specific tuning, but depends
on strong, human-like reasoning abilities in LLM evaluators. We release our
benchmark at https://github.com/maple-zhou/PersonaEval.

</details>


### [128] [RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal Interaction Analysis](https://arxiv.org/abs/2508.10015)
*Enzhi Wang,Qicheng Li,Shiwan Zhao,Aobo Kong,Jiaming Zhou,Xi Yang,Yequan Wang,Yonghua Lin,Yong Qin*

Main category: cs.CL

TL;DR: 本文介绍了RealTalk-CN，一个中文多轮、多领域的语音-文本双模态任务导向对话（TOD）数据集，及其在加强中文语音大语言模型研究中的重要性。


<details>
  <summary>Details</summary>
Motivation: 目前的任务导向对话数据集多为文本形式，缺乏真实语音信号以及语音歧义和说话人变化等现实复杂性，尤其在中文领域更为匮乏。

Method: 提出了RealTalk-CN数据集，包括5.4K对话（60K语句，150小时）的语音-文本对齐标注，并引入跨模态对话任务，支持语音与文本的动态切换。

Result: 实验评估验证了RealTalk-CN在处理语音歧义、说话人特性及跨领域性能方面的鲁棒性，展示了其有效性。

Conclusion: RealTalk-CN数据集为中文语音大语言模型研究奠定了坚实基础，可用于真实场景中多模态对话的研究和应用。

Abstract: In recent years, large language models (LLMs) have achieved remarkable
advancements in multimodal processing, including end-to-end speech-based
language models that enable natural interactions and perform specific tasks in
task-oriented dialogue (TOD) systems. However, existing TOD datasets are
predominantly text-based, lacking real speech signals that are essential for
evaluating the robustness of speech-based LLMs. Moreover, existing speech TOD
datasets are primarily English and lack critical aspects such as speech
disfluencies and speaker variations. To address these gaps, we introduce
RealTalk-CN, the first Chinese multi-turn, multi-domain speech-text dual-modal
TOD dataset, comprising 5.4k dialogues (60K utterances, 150 hours) with paired
speech-text annotations. RealTalk-CN captures diverse dialogue scenarios with
annotated spontaneous speech disfluencies, ensuring comprehensive coverage of
real-world complexities in speech dialogue. In addition, we propose a novel
cross-modal chat task that authentically simulates real-world user
interactions, allowing dynamic switching between speech and text modalities.
Our evaluation covers robustness to speech disfluencies, sensitivity to speaker
characteristics, and cross-domain performance. Extensive experiments validate
the effectiveness of RealTalk-CN, establishing a strong foundation for Chinese
speech-based LLMs research.

</details>


### [129] [Training-Free Multimodal Large Language Model Orchestration](https://arxiv.org/abs/2508.10016)
*Tianyu Xie,Yuhang Wu,Yongdong Luo,Jiayi Ji,Xiawu Zheng*

Main category: cs.CL

TL;DR: 提出无需额外训练即可创建交互多模态AI系统的新方法——多模态大语言模型调度（MLLM Orchestration）。


<details>
  <summary>Details</summary>
Motivation: 现有的不同多模态大语言模型无法直接整合到一个统一的多模态输入输出系统中。

Method: 利用大语言模型的推理能力，通过显式的工作流协调特定模型，并引入中央控制器、并行文本到语音架构，以及跨模态记忆整合系统。

Result: 与传统联合训练方法相比，性能提高了7.8%，延迟降低了10.3%，解释性显著增强。

Conclusion: MLLM调度可实现无需额外训练的全面多模态能力，同时提升了效率和可解释性。

Abstract: Different Multimodal Large Language Models (MLLMs) cannot be integrated into
a unified multimodal input-output system directly. In previous work, training
has been considered as an inevitable component due to challenges in modal
alignment, Text-to-Speech efficiency and other integration issues. In this
paper, we introduce Multimodal Large Language Model Orchestration, an effective
approach for creating interactive multimodal AI systems without additional
training. MLLM Orchestration leverages the inherent reasoning capabilities of
large language models to coordinate specialized models through explicit
workflows, enabling natural multimodal interactions while maintaining
modularity, improving interpretability, and significantly enhancing
computational efficiency. Our orchestration framework is built upon three key
innovations: (1) a central controller LLM that analyzes user inputs and
dynamically routes tasks to appropriate specialized models through carefully
designed agents; (2) a parallel Text-to-Speech architecture that enables true
full-duplex interaction with seamless interruption handling and natural
conversational flow; and (3) a cross-modal memory integration system that
maintains coherent context across modalities through intelligent information
synthesis and retrieval, selectively avoiding unnecessary modality calls in
certain scenarios to improve response speed. Extensive evaluations demonstrate
that MLLM Orchestration achieves comprehensive multimodal capabilities without
additional training, performance improvements of up to 7.8% over traditional
jointly-trained approaches on standard benchmarks, reduced latency by 10.3%,
and significantly enhanced interpretability through explicit orchestration
processes.

</details>


### [130] [A Rose by Any Other Name Would Smell as Sweet: Categorical Homotopy Theory for Large Language Models](https://arxiv.org/abs/2508.10018)
*Sridhar Mahadevan*

Main category: cs.CL

TL;DR: 本论文研究了自然语言中具有相同意义但表达形式不同的语句生成问题，并通过引入范畴同伦框架解决。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理语义等价的表述时，生成的下一词概率不一致，现有的经验性解决方案效果有限，因此需要更抽象的理论框架来解决。

Method: 本文提出一种LLM Markov范畴，将语言生成的概率分布建模为范畴中的箭头，结合范畴同伦技术捕获LLM Markov范畴中的“弱等价”。

Result: 通过将范畴理论中的高等代数K理论和模型范畴分布应用于LLMs，提供了统一的理论视角解决等价语句生成的问题。

Conclusion: 该方法通过理论结合实践，为LLMs在捕获语义一致性方面提供了新的深刻理解，进一步推进语言模型的能力研究。

Abstract: Natural language is replete with superficially different statements, such as
``Charles Darwin wrote" and ``Charles Darwin is the author of", which carry the
same meaning. Large language models (LLMs) should generate the same next-token
probabilities in such cases, but usually do not. Empirical workarounds have
been explored, such as using k-NN estimates of sentence similarity to produce
smoothed estimates. In this paper, we tackle this problem more abstractly,
introducing a categorical homotopy framework for LLMs. We introduce an LLM
Markov category to represent probability distributions in language generated by
an LLM, where the probability of a sentence, such as ``Charles Darwin wrote" is
defined by an arrow in a Markov category. However, this approach runs into
difficulties as language is full of equivalent rephrases, and each generates a
non-isomorphic arrow in the LLM Markov category. To address this fundamental
problem, we use categorical homotopy techniques to capture ``weak equivalences"
in an LLM Markov category. We present a detailed overview of application of
categorical homotopy to LLMs, from higher algebraic K-theory to model
categories, building on powerful theoretical results developed over the past
half a century.

</details>


### [131] [Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning](https://arxiv.org/abs/2508.10019)
*Li Wang,Changhao Zhang,Zengqi Xiu,Kai Lu,Xin Yu,Kui Zhang,Wenjun Wu*

Main category: cs.CL

TL;DR: 本文提出一个新框架，通过将复杂的自然语言问题映射到规范化的问题空间，提升小型语言模型(SLMs)的推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言表达的复杂性和多样性对SLM推理能力的负面影响。

Method: 提出DURIT算法，通过强化学习和自蒸馏的方法，将自然语言问题标准化，分离理解与推理过程。

Result: DURIT在数学和逻辑推理任务中显著提升SLM的任务性能，包括域内和跨域任务表现。

Conclusion: 将理解和推理过程分离是提升SLM推理能力和鲁棒性的有效策略。

Abstract: Despite recent advances in the reasoning capabilities of Large Language
Models (LLMs), improving the reasoning ability of Small Language Models (SLMs,
e.g., $\leq$ 1.5B) remains challenging. A key obstacle lies in the complexity
and variability of natural language: essentially equivalent problems often
appear in diverse surface forms, often obscured by redundant or distracting
details. This imposes a dual burden on SLMs: they must first extract the core
problem from complex linguistic input, and then perform reasoning based on that
understanding. The resulting vast and noisy problem space hinders optimization,
particularly for models with limited capacity. To address this, we propose a
new framework that decouples understanding from reasoning by mapping natural
language problems into a canonical problem space-a semantically simplified yet
expressive domain. This enables SLMs to focus on reasoning over standardized
inputs, free from linguistic variability. Within this framework, we introduce
DURIT (Decoupled Understanding from Reasoning via Iterative Training), a
three-step algorithm that iteratively: (1) mapping natural language problems
via reinforcement learning, (2) aligns reasoning trajectories through
self-distillation, and (3) trains reasoning policies in the problem space. The
mapper and reasoner are co-trained in an alternating loop throughout this
process. Experiments show that DURIT substantially improves SLMs' performance
on both in-domain and out-of-domain mathematical and logical reasoning tasks.
Beyond improving reasoning capabilities, DURIT also improves the robustness of
reasoning, validating decoupling understanding from reasoning as an effective
strategy for strengthening SLMs.

</details>


### [132] [FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models](https://arxiv.org/abs/2508.10020)
*Chuan Li,Qianyi Zhao,Fengran Mo,Cen Chen*

Main category: cs.CL

TL;DR: 提出了一种名为FedCoT的新框架，用于在联邦学习中提升大语言模型的推理能力，兼顾性能、隐私和通信成本。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在联邦学习环境中推理能力的提升难题，特别是在医疗领域，要求推理结果既要准确又要可解释。

Method: 提出FedCoT框架，使用轻量级链式推理增强机制：本地模型生成多条推理路径，通过紧凑的鉴别器动态选择最佳路径，并结合改良的LoRA模块，将客户分类感知纳入噪声自由的聚合方法。

Result: 在医疗推理任务上的实验表明，FedCoT在严格的资源条件下提高了客户端的推理性能，并完全保持数据隐私。

Conclusion: FedCoT框架能够显著提升联邦学习中大语言模型的推理能力，同时兼顾隐私保护和通信效率，特别适用于医疗领域。

Abstract: Efficiently enhancing the reasoning capabilities of large language models
(LLMs) in federated learning environments remains challenging, particularly
when balancing performance gains with strict computational, communication, and
privacy constraints. This challenge is especially acute in healthcare, where
decisions-spanning clinical, operational, and patient-facing contexts-demand
not only accurate outputs but also interpretable, traceable rationales to
ensure safety, accountability, and regulatory compliance. Conventional
federated tuning approaches on LLM fail to address this need: they optimize
primarily for answer correctness while neglecting rationale quality, leaving
CoT capabilities dependent on models' innate pre-training abilities. Moreover,
existing methods for improving rationales typically rely on privacy-violating
knowledge distillation from centralized models. Additionally, the communication
overhead in traditional federated fine-tuning on LLMs remains substantial. We
addresses this gap by proposing FedCoT, a novel framework specifically designed
to enhance reasoning in federated settings. FedCoT leverages a lightweight
chain-of-thought enhancement mechanism: local models generate multiple
reasoning paths, and a compact discriminator dynamically selects the most
promising one. This approach improves reasoning accuracy and robustness while
providing valuable interpretability, which is particularly critical for medical
applications. To manage client heterogeneity efficiently, we adopt an improved
aggregation approach building upon advanced LoRA module stacking, incorporating
client classifier-awareness to achieve noise-free aggregation across diverse
clients. Comprehensive experiments on medical reasoning tasks demonstrate that
FedCoT significantly boosts client-side reasoning performance under stringent
resource budgets while fully preserving data privacy.

</details>


### [133] [LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients](https://arxiv.org/abs/2508.10021)
*Egor Fadeev,Dzhambulat Mollaev,Aleksei Shestov,Dima Korolev,Omar Zoloev,Ivan Kireev,Andrey Savchenko,Maksim Makarenko*

Main category: cs.CL

TL;DR: LATTE是一种对比学习框架，通过对齐原始事件嵌入和冷冻LLMs的语义嵌入，用于从历史通讯中学习客户嵌入。


<details>
  <summary>Details</summary>
Motivation: 当前使用大型语言模型（LLMs）直接处理长事件序列既成本高昂又不切实际，因此需要一种高效的方法来学习客户行为特征。

Method: 提出对比学习框架LATTE，通过将行为特征总结为短提示，并用冷冻LLMs生成语义嵌入，与原始事件嵌入对齐，以减少推理成本并精简输入。

Result: 实验表明，该方法在真实金融数据集上表现优于现有技术，同时适应低延迟环境的需求。

Conclusion: LATTE在性能和实用性上均有所突破，为处理长事件序列提供了高效的解决方案，在金融应用中具有重要意义。

Abstract: Learning clients embeddings from sequences of their historic communications
is central to financial applications. While large language models (LLMs) offer
general world knowledge, their direct use on long event sequences is
computationally expensive and impractical in real-world pipelines. In this
paper, we propose LATTE, a contrastive learning framework that aligns raw event
embeddings with semantic embeddings from frozen LLMs. Behavioral features are
summarized into short prompts, embedded by the LLM, and used as supervision via
contrastive loss. The proposed approach significantly reduces inference cost
and input size compared to conventional processing of complete sequence by LLM.
We experimentally show that our method outperforms state-of-the-art techniques
for learning event sequence representations on real-world financial datasets
while remaining deployable in latency-sensitive environments.

</details>


### [134] [Conformal P-Value in Multiple-Choice Question Answering Tasks with Provable Risk Control](https://arxiv.org/abs/2508.10022)
*Yuanchang Ye*

Main category: cs.CL

TL;DR: 该研究提出了一种增强显著性测试的共形预测(CP)框架，用以提升大语言模型(LLMs)在多项选择题答题(MCQA)中结果的可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在专业问答场景中的应用日益增加，其虚构与非事实生成的缺点严重影响响应的可信度。通过结合统计严谨的CP与显著性测试，可有效缓解这些问题，但这一结合的探索仍属空白。

Method: 提出的方法结合了显著性测试的p值计算与共形预测一致性评分的方式，通过对MCQA的响应进行自一致性重抽样来分析选项频率，并通过基于零假设的统计检验构建预测集。

Result: 基于MMLU和MMLU-Pro基准的评估显示：1. 增强型CP框架能够实现用户指定的实证错误覆盖率；2. 在风险水平（α）增加时，测试集的平均预测集大小（APSS）单调减少，体现了APSS在不确定性量化中的有效性。

Conclusion: 该研究建立了一个原则性统计框架，为在高风险问答应用中部署可信的LLMs提供了可靠依据。

Abstract: This study introduces a significance testing-enhanced conformal prediction
(CP) framework to improve trustworthiness of large language models (LLMs) in
multiple-choice question answering (MCQA). While LLMs have been increasingly
deployed in disciplinary QA scenarios, hallucination and nonfactual generation
substantially compromise response reliability. Although CP provides
statistically rigorous marginal coverage guarantees for prediction sets, and
significance testing offers established statistical rigor, their synergistic
integration remains unexplored. To mitigate hallucination and factual
inaccuracies, our framework integrates $p$-value computation with conformity
scoring through self-consistency resampling of MCQA responses. This approach
calculates option frequencies to address LLMs' black-box nature, subsequently
constructing prediction sets via null hypothesis testing ($\mathcal{H}_0$) with
empirically derived $p$-values. Evaluations on MMLU and MMLU-Pro benchmarks
using off-the-shelf LLMs demonstrate: (1) The enhanced CP achieves
user-specified empirical miscoverage rates; (2) Test-set average prediction set
size (APSS) decreases monotonically with increasing risk levels ($\alpha$),
validating APSS as an effective uncertainty metric. This work establishes a
principled statistical framework for trustworthy LLM deployment in high-stakes
QA applications.

</details>


### [135] [RTTC: Reward-Guided Collaborative Test-Time Compute](https://arxiv.org/abs/2508.10024)
*J. Pablo Muñoz,Jinjie Yuan*

Main category: cs.CL

TL;DR: 本文提出“Reward-Guided Test-Time Compute (RTTC)”框架，通过预训练奖励模型，为每个查询自适应选择最优测试时策略，提升LLMs的推理性能，同时减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时计算策略（例如TTT和RAG）存在固有的高计算开销的问题，同时对于不同查询，最佳的策略选择并不统一，因此需要设计一种高效且具有适应性的解决方案。

Method: 提出了一种分布式的服务器-客户端架构RTTC，它结合奖励模型评估，为每个查询选择最优测试时策略，同时引入Query-State Caching机制以重用历史查询状态，从而减少冗余计算。

Result: 实验结果表明，RTTC在多种语言模型和基准测试上均显著提升了准确率，与传统RAG或TTT相比表现更优。

Conclusion: RTTC是一种高效且可扩展的框架，可以实现语言模型的高性能自适应测试计算，验证了奖励引导和自适应策略选择的必要性及潜力。

Abstract: Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the
performance of Large Language Models (LLMs) at inference, leveraging strategies
such as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).
However, the optimal adaptation strategy varies across queries, and
indiscriminate application of TTC strategy incurs substantial computational
overhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a
novel framework that adaptively selects the most effective TTC strategy for
each query via a pretrained reward model, maximizing downstream accuracy across
diverse domains and tasks. RTTC operates in a distributed server-client
architecture, retrieving relevant samples from a remote knowledge base and
applying RAG or lightweight fine-tuning on client devices only when necessary.
To further mitigate redundant computation, we propose Query-State Caching,
which enables the efficient reuse of historical query states at both retrieval
and adaptation levels. Extensive experiments across multiple LLMs and
benchmarks demonstrate that RTTC consistently achieves superior accuracy
compared to vanilla RAG or TTT, validating the necessity of adaptive,
reward-guided TTC selection and the potential of RTTC for scalable,
high-performance language model adaptation.

</details>


### [136] [Detecting and explaining postpartum depression in real-time with generative artificial intelligence](https://arxiv.org/abs/2508.10025)
*Silvia García-Méndez,Francisco de Arriba-Pérez*

Main category: cs.CL

TL;DR: 本研究提出了一种智能化的产后抑郁筛查系统，结合自然语言处理、机器学习和大型语言模型，提供实时检测与治疗建议，且具有解释性强的特点。


<details>
  <summary>Details</summary>
Motivation: 产后抑郁对母亲的身心健康有重大影响，因此需要一种快速检测和识别风险因子的工具以进行及时干预。

Method: 通过结合自然语言处理、机器学习和大型语言模型，开发一个具实时检测能力的非侵入式语音分析系统，同时解决预测过程中的黑箱问题，利用可解释性机器学习模型和自然语言对预测进行说明。

Result: 提出的系统在产后抑郁检测方面表现出色，各评估指标上取得了90%的准确率，优于现有文献中的解决方案。

Conclusion: 本研究所开发的系统能迅速检测产后抑郁及其相关风险因子，有助于及时和适当的评估与干预，为实践者提供了切实的技术手段支持。

Abstract: Among the many challenges mothers undergo after childbirth, postpartum
depression (PPD) is a severe condition that significantly impacts their mental
and physical well-being. Consequently, the rapid detection of ppd and their
associated risk factors is critical for in-time assessment and intervention
through specialized prevention procedures. Accordingly, this work addresses the
need to help practitioners make decisions with the latest technological
advancements to enable real-time screening and treatment recommendations.
Mainly, our work contributes to an intelligent PPD screening system that
combines Natural Language Processing, Machine Learning (ML), and Large Language
Models (LLMs) towards an affordable, real-time, and non-invasive free speech
analysis. Moreover, it addresses the black box problem since the predictions
are described to the end users thanks to the combination of LLMs with
interpretable ml models (i.e., tree-based algorithms) using feature importance
and natural language. The results obtained are 90 % on ppd detection for all
evaluation metrics, outperforming the competing solutions in the literature.
Ultimately, our solution contributes to the rapid detection of PPD and their
associated risk factors, critical for in-time and proper assessment and
intervention.

</details>


### [137] [SABER: Switchable and Balanced Training for Efficient LLM Reasoning](https://arxiv.org/abs/2508.10026)
*Kai Zhao,Yanjun Zhao,Jiaming Song,Shien He,Lusheng Zhang,Qiang Zhang,Tianjiao Li*

Main category: cs.CL

TL;DR: SABER通过引入用户可控的推理预算机制，解决了大模型计算成本和推理延迟问题，同时保持高效性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在复杂任务中的推理能力强大，但因推理过程统一应用于所有问题，导致推理成本过高、延迟过大，因此需要一种更高效的推理策略。

Method: 提出SABER框架，采用强化学习对模型进行微调，根据推理所需的不同预算层次划分训练样本，同时引入多种推理模式（NoThink、FastThink、CoreThink、DeepThink）供用户选择。在优化过程中应用奖励机制以兼顾推理预算要求和模型性能。

Result: SABER在数学推理、代码生成和逻辑推理任务上的实验评估表现出色，在严格预算条件下实现了更高的准确性、推理长度减少且适应性强。例如在MATH数据集上，FastThink模式将推理长度减少65.4%，同时准确率比基础模型提高了3.6%。

Conclusion: SABER通过其高效的推理机制，降低了推理成本，改进了性能，并为用户提供了灵活的推理策略选择，是大语言模型推理领域的重要进展。

Abstract: Large language models (LLMs) empowered by chain-of-thought reasoning have
achieved impressive accuracy on complex tasks but suffer from excessive
inference costs and latency when applied uniformly to all problems. We propose
SABER (Switchable and Balanced Training for Efficient LLM Reasoning), a
reinforcement learning framework that endows LLMs with user-controllable,
token-budgeted reasoning. SABER first profiles each training example's
base-model thinking token usage and assigns it to one of the predefined budget
tiers. During fine-tuning, the model is guided by system prompts and
length-aware rewards to respect its assigned budget. In parallel, we
incorporate no-think examples to ensure the model remains reliable even when
explicit reasoning is turned off. SABER further supports four discrete
inference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling
flexible trade-offs between latency and reasoning depth. Extensive evaluations
on math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning
(LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight
budgets, graceful degradation, and effective cross-scale and cross-domain
generalization. In particular, SABER-FastThink cuts reasoning length by 65.4%
and yields a 3.6% accuracy gain compared with the base model on the MATH
benchmark.

</details>


### [138] [LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.10027)
*Ali Zolnour,Hossein Azadmaleki,Yasaman Haghbin,Fatemeh Taherinezhad,Mohamad Javad Momeni Nezhad,Sina Rashidi,Masoud Khani,AmirSajjad Taleban,Samin Mahdizadeh Sani,Maryam Dadkhah,James M. Noble,Suzanne Bakken,Yadollah Yaghoobzadeh,Abdol-Hossein Vahabie,Masoud Rouhizadeh,Maryam Zolnoori*

Main category: cs.CL

TL;DR: 研究利用基于语音的NLP方法，通过结合Transformer嵌入与语言特征，实现早期阿尔茨海默病及相关痴呆症（ADRD）的检测，同时探索合成语音数据增强和多模态LLM分类器效果。


<details>
  <summary>Details</summary>
Motivation: 现有约一半患者未确诊的阿尔茨海默病和相关痴呆症需要更有效的检测方法，基于语音的NLP技术能够通过语言特征标记提供一种可扩展的解决方案。

Method: 开发筛查流程，融合Transformer模型的嵌入和手工设计的语言特征；测试大语言模型（LLM）生成的合成语音数据增强效果；评估多模态和单模态LLM分类器的性能，并进行基准测试。

Result: 在语音-文本数据中，融合模型的最佳F1得分83.3；用合成语音增强数据训练将F1提高到85.7；细调后单模态LLM性能显著提高，而当前多模态模型性能较低。

Conclusion: 结合Transformer嵌入和语言特征的方法提升了ADRD早期检测效果，临床化的大语言模型在分类和数据增强中表现良好，但多模态建模仍存进一步提升空间。

Abstract: Alzheimer's disease and related dementias (ADRD) affect approximately five
million older adults in the U.S., yet over half remain undiagnosed.
Speech-based natural language processing (NLP) offers a promising, scalable
approach to detect early cognitive decline through linguistic markers.
  To develop and evaluate a screening pipeline that (i) fuses transformer
embeddings with handcrafted linguistic features, (ii) tests data augmentation
using synthetic speech generated by large language models (LLMs), and (iii)
benchmarks unimodal and multimodal LLM classifiers for ADRD detection.
  Transcripts from the DementiaBank "cookie-theft" task (n = 237) were used.
Ten transformer models were evaluated under three fine-tuning strategies. A
fusion model combined embeddings from the top-performing transformer with 110
lexical-derived linguistic features. Five LLMs (LLaMA-8B/70B, MedAlpaca-7B,
Ministral-8B, GPT-4o) were fine-tuned to generate label-conditioned synthetic
speech, which was used to augment training data. Three multimodal models
(GPT-4o, Qwen-Omni, Phi-4) were tested for speech-text classification in
zero-shot and fine-tuned settings.
  The fusion model achieved F1 = 83.3 (AUC = 89.5), outperforming linguistic or
transformer-only baselines. Augmenting training data with 2x MedAlpaca-7B
synthetic speech increased F1 to 85.7. Fine-tuning significantly improved
unimodal LLM classifiers (e.g., MedAlpaca: F1 = 47.3 -> 78.5 F1). Current
multimodal models demonstrated lower performance (GPT-4o = 70.2 F1; Qwen =
66.0). Performance gains aligned with the distributional similarity between
synthetic and real speech.
  Integrating transformer embeddings with linguistic features enhances ADRD
detection from speech. Clinically tuned LLMs effectively support both
classification and data augmentation, while further advancement is needed in
multimodal modeling.

</details>


### [139] [PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs](https://arxiv.org/abs/2508.10028)
*Xiao Fu,Hossein A. Rahmani,Bin Wu,Jerome Ramos,Emine Yilmaz,Aldo Lipani*

Main category: cs.CL

TL;DR: PREF是一种新型的无参考个性化评估框架，无需黄金标准参考，能够同步衡量文本生成的整体质量和用户特定需求的匹配度。


<details>
  <summary>Details</summary>
Motivation: 当前的文本生成评估方法忽视了用户个人的需求与偏好，亟需能反映用户多样化个性化需求的工具。

Method: PREF通过三步实现：1.使用大语言模型生成综合性、针对具体查询的评估标准；2.整合用户偏好及上下文，生成个性化评价指标；3.基于标准对候选答案评分。

Result: 在PrefEval基准测试及隐式偏好任务中，PREF与人类评判结果更贴近，表现出更高的准确性、校准性和一致性。

Conclusion: PREF为个性化语言生成系统的评估和开发提供了一种可靠、可解读且可扩展的工具，为未来的发展奠定了基础。

Abstract: Personalised text generation is essential for user-centric information
systems, yet most evaluation methods overlook the individuality of users. We
introduce \textbf{PREF}, a \textbf{P}ersonalised \textbf{R}eference-free
\textbf{E}valuation \textbf{F}ramework that jointly measures general output
quality and user-specific alignment without requiring gold personalised
references. PREF operates in a three-step pipeline: (1) a coverage stage uses a
large language model (LLM) to generate a comprehensive, query-specific
guideline covering universal criteria such as factuality, coherence, and
completeness; (2) a preference stage re-ranks and selectively augments these
factors using the target user's profile, stated or inferred preferences, and
context, producing a personalised evaluation rubric; and (3) a scoring stage
applies an LLM judge to rate candidate answers against this rubric, ensuring
baseline adequacy while capturing subjective priorities. This separation of
coverage from preference improves robustness, transparency, and reusability,
and allows smaller models to approximate the personalised quality of larger
ones. Experiments on the PrefEval benchmark, including implicit
preference-following tasks, show that PREF achieves higher accuracy, better
calibration, and closer alignment with human judgments than strong baselines.
By enabling scalable, interpretable, and user-aligned evaluation, PREF lays the
groundwork for more reliable assessment and development of personalised
language generation systems.

</details>


### [140] [Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs](https://arxiv.org/abs/2508.10029)
*Wenpeng Xing,Mohan Li,Chunqiang Hu,Haitao XuNingyu Zhang,Bo Lin,Meng Han*

Main category: cs.CL

TL;DR: 本文提出了一种新的隐含表示攻击方法——LFJ,通过插值有害和无害查询对的隐藏状态,避开语言模型的安全限制,并实现高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过利用大型语言模型的潜在表示以绕过其安全性对齐限制,从而揭示其潜在漏洞并提出补救措施。

Method: 引入Latent Fusion Jailbreak(LFJ)攻击,通过选择主题和句法相似的查询对,然后在隐藏层和相关位置进行梯度引导的插值,优化攻击成功率、输出流畅性和计算效率。

Result: 在诸如Vicuna和LLaMA-2等模型上验证该方法,在多个基准测试中取得了94.01%的平均攻击成功率,并提出基于对抗训练的防御方法,可将攻击成功率减少80%以上。

Conclusion: LFJ展示了潜在表示层面攻击的有效性,并验证了对抗训练在减轻这种攻击中的强大潜力,为提高语言模型的安全性提供了新的方向。

Abstract: Large language models (LLMs) demonstrate impressive capabilities in various
language tasks but are susceptible to jailbreak attacks that circumvent their
safety alignments. This paper introduces Latent Fusion Jailbreak (LFJ), a
representation-based attack that interpolates hidden states from harmful and
benign query pairs to elicit prohibited responses. LFJ begins by selecting
query pairs with high thematic and syntactic similarity, then performs
gradient-guided interpolation at influential layers and tokens, followed by
optimization to balance attack success, output fluency, and computational
efficiency. Evaluations on models such as Vicuna and LLaMA-2 across benchmarks
like AdvBench and MaliciousInstruct yield an average attack success rate (ASR)
of 94.01%, outperforming existing methods. To mitigate LFJ, we propose an
adversarial training defense that fine-tunes models on interpolated examples,
reducing ASR by over 80% without degrading performance on benign inputs.
Ablation studies validate the importance of query pair selection, hidden state
interpolation components, and optimization strategies in LFJ's effectiveness.

</details>


### [141] [Inference-Aware Prompt Optimization for Aligning Black-Box Large Language Models](https://arxiv.org/abs/2508.10030)
*Saaduddin Mahmud,Mason Nakamura,Kyle H. Wray,Shlomo Zilberstein*

Main category: cs.CL

TL;DR: 论文提出了一种新的框架IAPO，通过同时优化提示和推断规模，使得黑盒大语言模型的对齐更加高效。


<details>
  <summary>Details</summary>
Motivation: 现有的提示优化方法与推断策略无关，但推断策略对提示有很大影响，这种互相影响的关系在现有方法中并未被有效利用。

Method: 提出了IAPO框架，同时考虑推断预算与任务目标，对提示与推断规模进行联合优化，并开发了名为PSST的固定预算训练算法。

Result: PSST算法在包括多目标文本生成与推理的6个任务上表现出更高的有效性，验证了推断意识对提示优化的重要性。

Conclusion: 推断意识是提示优化中至关重要的因素，将其纳入框架中能显著提升模型的对齐能力。

Abstract: Prompt optimization methods have demonstrated significant effectiveness in
aligning black-box large language models (LLMs). In parallel, inference scaling
strategies such as Best-of-N Sampling and Majority Voting have also proven to
enhance alignment and performance by trading off computation. However, existing
prompt optimization approaches are inference strategy agnostic; that is, they
optimize prompts without regard to the inference strategy employed during
deployment. This constitutes a significant methodological gap, as our empirical
and theoretical analysis reveals a strong interdependence between these two
paradigms. Moreover, we find that user preferences regarding trade-offs among
multiple objectives and inference budgets substantially influence the choice of
prompt and inference configuration. To address this gap, we introduce a unified
novel framework named IAPO (Inference-Aware Prompt Optimization) that jointly
optimizes the prompt and inference scale, while being aware of the inference
budget and different task objectives. We then develop a fixed-budget training
algorithm for IAPO, which we call PSST (Prompt Scaling via Sequential
Trimming), and analyze finite-budget guarantees on error probability. Finally,
we evaluate the effectiveness of PSST on six different tasks, including
multi-objective text generation and reasoning, and demonstrate the critical
role of incorporating inference-awareness when aligning black-box LLMs through
prompt optimization.

</details>


### [142] [The Cost of Thinking: Increased Jailbreak Risk in Large Language Models](https://arxiv.org/abs/2508.10032)
*Fan Yang*

Main category: cs.CL

TL;DR: 本研究发现具有思考模式的LLM更容易受到Jailbreak攻击，并提出了一种通过添加“特定思考标记”来干预LLM思考过程的方法，有效降低了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 研究探索为何以思考模式为主的LLM易受Jailbreak攻击，分析其导致问题根源并找出优化对策。

Method: 通过对9个LLM在AdvBench和HarmBench上的攻击率测试，结合添加“特定思考标记”的提示词干预内在思维过程，分析其对抗攻击的效果。

Result: 实验表明引入安全思考干预后，带有思考模式的LLM表现出显著降低的攻击成功率。

Conclusion: 论文验证了思考模式赋予LLM潜在脆弱性的假设，同时提出了有效的干预方法，为改善LLM安全性提供了新思路。

Abstract: Thinking mode has always been regarded as one of the most valuable modes in
LLMs. However, we uncover a surprising and previously overlooked phenomenon:
LLMs with thinking mode are more easily broken by Jailbreak attack. We evaluate
9 LLMs on AdvBench and HarmBench and find that the success rate of attacking
thinking mode in LLMs is almost higher than that of non-thinking mode. Through
large numbers of sample studies, it is found that for educational purposes and
excessively long thinking lengths are the characteristics of successfully
attacked data, and LLMs also give harmful answers when they mostly know that
the questions are harmful. In order to alleviate the above problems, this paper
proposes a method of safe thinking intervention for LLMs, which explicitly
guides the internal thinking processes of LLMs by adding "specific thinking
tokens" of LLMs to the prompt. The results demonstrate that the safe thinking
intervention can significantly reduce the attack success rate of LLMs with
thinking mode.

</details>


### [143] [Reflect then Learn: Active Prompting for Information Extraction Guided by Introspective Confusion](https://arxiv.org/abs/2508.10036)
*Dong Zhao,Yadong Wang,Xiang Chen,Chenxi Wang,Hongliang Dai,Chuanxing Geng,Shengzhong Zhang,Shaoyuan Li,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 该论文提出了一种新的主动提示框架APIE，利用大型语言模型的自我混淆评估能力，显著提升信息抽取任务的准确性和稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有的少样本信息抽取方法对上下文例子的选择较为随机，忽视了模型在语义和格式生成上的不确定性问题，导致性能不佳。

Method: 提出了APIE框架，基于自我混淆原则，通过双重不确定性度量（格式不确定性和内容不确定性）来评估模型的表现，并主动挑选最具挑战性的数据作为少样本示例。

Result: 在四个基准测试上，该方法优于强基线，显著提升了信息抽取的准确性和鲁棒性。

Conclusion: 细化的双重不确定性视角对于构建有效、可靠的结构化生成系统至关重要。

Abstract: Large Language Models (LLMs) show remarkable potential for few-shot
information extraction (IE), yet their performance is highly sensitive to the
choice of in-context examples. Conventional selection strategies often fail to
provide informative guidance, as they overlook a key source of model
fallibility: confusion stemming not just from semantic content, but also from
the generation of well-structured formats required by IE tasks. To address
this, we introduce Active Prompting for Information Extraction (APIE), a novel
active prompting framework guided by a principle we term introspective
confusion. Our method empowers an LLM to assess its own confusion through a
dual-component uncertainty metric that uniquely quantifies both Format
Uncertainty (difficulty in generating correct syntax) and Content Uncertainty
(inconsistency in extracted semantics). By ranking unlabeled data with this
comprehensive score, our framework actively selects the most challenging and
informative samples to serve as few-shot exemplars. Extensive experiments on
four benchmarks show that our approach consistently outperforms strong
baselines, yielding significant improvements in both extraction accuracy and
robustness. Our work highlights the critical importance of a fine-grained,
dual-level view of model uncertainty when it comes to building effective and
reliable structured generation systems.

</details>


### [144] [mSCoRe: a $M$ultilingual and Scalable Benchmark for $S$kill-based $Co$mmonsense $Re$asoning](https://arxiv.org/abs/2508.10137)
*Nghia Trung Ngo,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: 研究提出了一个新的多语言通用常识推理评估基准（mSCoRe），用于评估现有语言模型在复杂推理任务中的能力。实验显示，当前模型在处理高级难度以及文化和语言多样性方面仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在推理任务中表现出色，但其在多语言常识推理中如何运用不同的人类推理技能仍缺乏系统研究。

Method: 研究引入了mSCoRe基准，包含三个核心部分：推理技能的新分类法、针对常识推理评价的数据合成流程，以及复杂度动态缩放机制，用于应对未来模型能力的升级。

Result: 通过对8种最先进LLMs的实验，发现mSCoRe在高复杂度级别上仍对当前模型构成显著挑战，特别是在多语言和文化常识方面模型表现有限。

Conclusion: 研究揭示了推理增强型模型的局限性，并提出了改进其多语言常识推理能力的未来方向。

Abstract: Recent advancements in reasoning-reinforced Large Language Models (LLMs) have
shown remarkable capabilities in complex reasoning tasks. However, the
mechanism underlying their utilization of different human reasoning skills
remains poorly investigated, especially for multilingual commonsense reasoning
that involves everyday knowledge across different languages and cultures. To
address this gap, we propose a \textbf{M}ultilingual and Scalable Benchmark for
\textbf{S}kill-based \textbf{Co}mmonsense \textbf{Re}asoning (\textbf{mSCoRe}).
Our benchmark incorporates three key components that are designed to
systematically evaluate LLM's reasoning capabilities, including: (1) a novel
taxonomy of reasoning skills that enables fine-grained analysis of models'
reasoning processes, (2) a robust data synthesis pipeline tailored specifically
for commonsense reasoning evaluation, and (3) a complexity scaling framework
allowing task difficulty to scale dynamically alongside future improvements in
LLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying
sizes and training approaches demonstrate that \textbf{mSCoRe} remains
significantly challenging for current models, particularly at higher complexity
levels. Our results reveal the limitations of such reasoning-reinforced models
when confronted with nuanced multilingual general and cultural commonsense. We
further provide detailed analysis on the models' reasoning processes,
suggesting future directions for improving multilingual commonsense reasoning
capabilities.

</details>


### [145] [Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs](https://arxiv.org/abs/2508.10142)
*Kartikeya Badola,Jonathan Simon,Arian Hosseini,Sara Marie Mc Carthy,Tsendsuren Munkhdalai,Abhimanyu Goyal,Tomáš Kočiský,Shyam Upadhyay,Bahare Fatemi,Mehran Kazemi*

Main category: cs.CL

TL;DR: 本文提出了一个新的基准测试，评估LLMs在多轮对话、信息查寻和推理任务中的表现，发现了当前模型在复杂互动情景下的显著不足。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型虽然在明确任务中表现出色，但在复杂、互动的现实场景中表现有限，需要探索如何提升其在逻辑一致的多轮对话和处理不完整数据任务中的能力。

Method: 提出并构建了新的基准测试，包含多轮任务来专门测试模型的推理、对话和信息查询能力，采用确定性评分机制避免人工干预。

Result: 在该基准测试中，前沿模型表现仍有很大提升空间，主要问题是指令理解、推理能力和规划的不足。

Conclusion: 该基准测试揭示了当前大型语言模型在应对复杂互动情境中的优缺点，并为今后提升这些关键能力提供了坚实的平台。

Abstract: Large language models (LLMs) excel at solving problems with clear and
complete statements, but often struggle with nuanced environments or
interactive tasks which are common in most real-world scenarios. This
highlights the critical need for developing LLMs that can effectively engage in
logically consistent multi-turn dialogue, seek information and reason with
incomplete data. To this end, we introduce a novel benchmark comprising a suite
of multi-turn tasks each designed to test specific reasoning, interactive
dialogue, and information-seeking abilities. These tasks have deterministic
scoring mechanisms, thus eliminating the need for human intervention.
Evaluating frontier models on our benchmark reveals significant headroom. Our
analysis shows that most errors emerge from poor instruction following,
reasoning failures, and poor planning. This benchmark provides valuable
insights into the strengths and weaknesses of current LLMs in handling complex,
interactive scenarios and offers a robust platform for future research aimed at
improving these critical capabilities.

</details>


### [146] [LaajMeter: A Framework for LaaJ Evaluation](https://arxiv.org/abs/2508.10161)
*Gal Amram,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Avi Ziv*

Main category: cs.CL

TL;DR: 提出了一种用于评估语言模型作为评估者的新方法LaaJMeter，并展示其在代码翻译任务中的应用验证。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型作为评估者（LaaJs）在特定领域的评价面临数据不足和评估成本高的问题，使得难以判断评价指标的有效性和确定评价性能标准。

Method: 提出一种基于模拟的评估框架LaaJMeter，它通过生成虚拟模型和评估者的合成数据来系统化分析评估指标在真实条件下的表现。

Result: 在代码翻译任务中验证发现，不同指标对评估者质量敏感度各异，强调了选择原则性指标的必要性。

Conclusion: LaaJMeter在低资源环境下提供了一种可扩展的方法，用于验证和改进语言评估模型，推动NLP领域的可信与可重复性评估。

Abstract: Large Language Models (LLMs) are increasingly used as evaluators in natural
language processing tasks, a paradigm known as LLM-as-a-Judge (LaaJ). While
effective in general domains, LaaJs pose significant challenges in
domain-specific contexts, where annotated data is scarce and expert evaluation
is costly. In such cases, meta-evaluation is often performed using metrics that
have not been validated for the specific domain in which they are applied. As a
result, it becomes difficult to determine which metrics effectively identify
LaaJ quality, and further, what threshold indicates sufficient evaluator
performance. In this work, we introduce LaaJMeter, a simulation-based framework
for controlled meta-evaluation of LaaJs. LaaJMeter enables engineers to
generate synthetic data representing virtual models and judges, allowing
systematic analysis of evaluation metrics under realistic conditions. This
helps practitioners validate and refine LaaJs for specific evaluation tasks:
they can test whether their metrics correctly distinguish between better and
worse (virtual) LaaJs, and estimate appropriate thresholds for evaluator
adequacy.
  We demonstrate the utility of LaaJMeter in a code translation task involving
a legacy programming language, showing how different metrics vary in
sensitivity to evaluator quality. Our results highlight the limitations of
common metrics and the importance of principled metric selection. LaaJMeter
provides a scalable and extensible solution for assessing LaaJs in low-resource
settings, contributing to the broader effort to ensure trustworthy and
reproducible evaluation in NLP.

</details>


### [147] [Estimating Machine Translation Difficulty](https://arxiv.org/abs/2508.10175)
*Lorenzo Proietti,Stefano Perrella,Vilém Zouhar,Roberto Navigli,Tom Kocmi*

Main category: cs.CL

TL;DR: 本文探讨了机器翻译系统在某些情境下达到近乎完美翻译后的评估和改进难题，并提出了翻译难度估计的任务框架及相关方法论。


<details>
  <summary>Details</summary>
Motivation: 随着机器翻译技术的进步，优质翻译结果使得区分模型的能力和未来改进方向变得更困难，因此需要找到一种能够自动识别翻译系统困难区域的方法，以提升评估的差异性并引导未来研究。

Method: 本文定义了翻译文本难度的估计任务，提出了一种新的难度评估指标，并开发新的模型（Sentinel-src系列），与基准模型和基于语言模型的判断方法进行比较，同时利用这些估计工具构建更具挑战性的翻译基准测试。

Result: 研究结果表明，专门开发的Sentinel-src模型在难度估计上超过了基于启发式方法（如词汇生僻度或句法复杂度）以及基于大语言模型的评估方法。

Conclusion: 本文提出的Sentinel-src模型和评估方法能够有效标记翻译系统的挑战性内容，为构建更全面的机器翻译基准和指导改进提供了有力工具。

Abstract: Machine translation quality has began achieving near-perfect translations in
some setups. These high-quality outputs make it difficult to distinguish
between state-of-the-art models and to identify areas for future improvement.
Automatically identifying texts where machine translation systems struggle
holds promise for developing more discriminative evaluations and guiding future
research.
  We formalize the task of translation difficulty estimation, defining a text's
difficulty based on the expected quality of its translations. We introduce a
new metric to evaluate difficulty estimators and use it to assess both
baselines and novel approaches. Finally, we demonstrate the practical utility
of difficulty estimators by using them to construct more challenging machine
translation benchmarks. Our results show that dedicated models (dubbed
Sentinel-src) outperform both heuristic-based methods (e.g. word rarity or
syntactic complexity) and LLM-as-a-judge approaches. We release two improved
models for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, which
can be used to scan large collections of texts and select those most likely to
challenge contemporary machine translation systems.

</details>


### [148] [Efficient Forward-Only Data Valuation for Pretrained LLMs and VLMs](https://arxiv.org/abs/2508.10180)
*Wenlong Deng,Jiaming Zhang,Qi Zeng,Christos Thrampoulidis,Boying Gong,Xiaoxiao Li*

Main category: cs.CL

TL;DR: For-Value 是一个专注于 LLMs 和 VLMs 的数据价值计算框架，无需复杂梯度计算，仅通过一次前向传播高效估算单样本影响力。


<details>
  <summary>Details</summary>
Motivation: 现有的数据评估方法由于依赖 Hessian 信息或重复训练，在处理大规模模型时计算成本过高。

Method: 提出了 For-Value 框架，通过现代基础模型的隐藏表示，基于简单的封闭形式，仅通过单次前向传播计算数据影响分数。

Result: For-Value 准确估算样本影响，与梯度基准相比匹配甚至更优，可识别关键微调样本及检测被错标数据。

Conclusion: For-Value 为高效且准确的数据价值估算提供了理论和实验证明，是大规模语言模型中标注质量管理和样本优先级排序的创新工具。

Abstract: Quantifying the influence of individual training samples is essential for
enhancing the transparency and accountability of large language models (LLMs)
and vision-language models (VLMs). However, existing data valuation methods
often rely on Hessian information or model retraining, making them
computationally prohibitive for billion-parameter models. In this work, we
introduce For-Value, a forward-only data valuation framework that enables
scalable and efficient influence estimation for both LLMs and VLMs. By
leveraging the rich representations of modern foundation models, For-Value
computes influence scores using a simple closed-form expression based solely on
a single forward pass, thereby eliminating the need for costly gradient
computations. Our theoretical analysis demonstrates that For-Value accurately
estimates per-sample influence by capturing alignment in hidden representations
and prediction errors between training and validation samples. Extensive
experiments show that For-Value matches or outperforms gradient-based baselines
in identifying impactful fine-tuning examples and effectively detecting
mislabeled data.

</details>


### [149] [PakBBQ: A Culturally Adapted Bias Benchmark for QA](https://arxiv.org/abs/2508.10186)
*Abdullah Hashmat,Muhammad Arham Mirza,Agha Ali Raza*

Main category: cs.CL

TL;DR: PakBBQ是一个专为巴基斯坦上下文适配的问答偏见基准数据集，涵盖英语与乌尔都语的多种偏见维度，用于评估大型语言模型（LLMs）的公平性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs大多基于西方数据训练与评估，无法很好体现低资源语言和地区语境的公平性，因此需设计适配性的基准来弥补这一缺口。

Method: 提出PakBBQ数据集，包括214个模板、17180条QA对，覆盖多种偏见维度，并对多语言模型进行在模糊与明确语境中的评估，实验还探讨了问题框架的影响。

Result: 实验发现：(i) 通过消除歧义，平均准确率提高12%；(ii) 乌尔都语相比英语表现出更强的反偏见行为；(iii) 消极问题框架能有效减少刻板印象的答案。

Conclusion: 该研究强调了构建背景化基准和使用简单提示工程手段，在低资源环境中的偏见缓解可行性的重要性。

Abstract: With the widespread adoption of Large Language Models (LLMs) across various
applications, it is empirical to ensure their fairness across all user
communities. However, most LLMs are trained and evaluated on Western centric
data, with little attention paid to low-resource languages and regional
contexts. To address this gap, we introduce PakBBQ, a culturally and regionally
adapted extension of the original Bias Benchmark for Question Answering (BBQ)
dataset. PakBBQ comprises over 214 templates, 17180 QA pairs across 8
categories in both English and Urdu, covering eight bias dimensions including
age, disability, appearance, gender, socio-economic status, religious, regional
affiliation, and language formality that are relevant in Pakistan. We evaluate
multiple multilingual LLMs under both ambiguous and explicitly disambiguated
contexts, as well as negative versus non negative question framings. Our
experiments reveal (i) an average accuracy gain of 12\% with disambiguation,
(ii) consistently stronger counter bias behaviors in Urdu than in English, and
(iii) marked framing effects that reduce stereotypical responses when questions
are posed negatively. These findings highlight the importance of contextualized
benchmarks and simple prompt engineering strategies for bias mitigation in low
resource settings.

</details>


### [150] [Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models](https://arxiv.org/abs/2508.10192)
*Igor Halperin*

Main category: cs.CL

TL;DR: 该论文介绍了语义偏离度量(SDM)框架，用以检测大语言模型(LLMs)生成的虚假或不一致内容，尤其是“准确性幻觉”(Faithfulness Hallucinations)行为。


<details>
  <summary>Details</summary>
Motivation: 大语言模型(LLMs)虽然功能强大，但存在生成非事实、不合逻辑或不忠实内容的“幻觉”问题，需求可准确检测这些不一致行为的方法。

Method: 提出语义偏离度量(SDM)框架，基于句子嵌入的聚类分析构建共享语义空间，结合信息理论度量，如Jensen-Shannon散度、Wasserstein距离，量化模型输出偏离程度，并进行语义探索信号的分析。

Result: SDM框架能量化捕捉模型回答与输入语义间的偏离，并通过构建Semantic Box对模型生成的回答行为进行分类。

Conclusion: 通过SDM框架及其度量方法，有助于更准确诊断和分类LLMs生成的非忠实、虚假输出，为改进大型语言模型的稳定性和可信度提供了重要工具。

Abstract: The proliferation of Large Language Models (LLMs) is challenged by
hallucinations, critical failure modes where models generate non-factual,
nonsensical or unfaithful text. This paper introduces Semantic Divergence
Metrics (SDM), a novel lightweight framework for detecting Faithfulness
Hallucinations -- events of severe deviations of LLMs responses from input
contexts. We focus on a specific implementation of these LLM errors,
{confabulations, defined as responses that are arbitrary and semantically
misaligned with the user's query. Existing methods like Semantic Entropy test
for arbitrariness by measuring the diversity of answers to a single, fixed
prompt. Our SDM framework improves upon this by being more prompt-aware: we
test for a deeper form of arbitrariness by measuring response consistency not
only across multiple answers but also across multiple, semantically-equivalent
paraphrases of the original prompt. Methodologically, our approach uses joint
clustering on sentence embeddings to create a shared topic space for prompts
and answers. A heatmap of topic co-occurances between prompts and responses can
be viewed as a quantified two-dimensional visualization of the user-machine
dialogue. We then compute a suite of information-theoretic metrics to measure
the semantic divergence between prompts and responses. Our practical score,
$\mathcal{S}_H$, combines the Jensen-Shannon divergence and Wasserstein
distance to quantify this divergence, with a high score indicating a
Faithfulness hallucination. Furthermore, we identify the KL divergence
KL(Answer $||$ Prompt) as a powerful indicator of \textbf{Semantic
Exploration}, a key signal for distinguishing different generative behaviors.
These metrics are further combined into the Semantic Box, a diagnostic
framework for classifying LLM response types, including the dangerous,
confident confabulation.

</details>


### [151] [Using Large Language Models to Measure Symptom Severity in Patients At Risk for Schizophrenia](https://arxiv.org/abs/2508.10226)
*Andrew X. Chen,Guillermo Horga,Sean Escola*

Main category: cs.CL

TL;DR: 利用大规模语言模型（LLMs），研究团队从临床访谈文本中预测精神分裂症高危患者的BPRS评分，准确性接近人类评估者。


<details>
  <summary>Details</summary>
Motivation: 改善精神分裂症高危患者症状监测方法，克服传统BPRS工具因面试结构复杂导致的临床使用受限问题。

Method: 基于AMP-SCZ队列中的409名患者数据，使用LLMs对临床访谈文本进行零样本学习预测BPRS评分，并进行语言和纵向信息整合评估。

Result: LLMs预测的BPRS评分与实际评估高度一致（中位一致率：0.84，ICC：0.73），并且其在非母语环境中的评估表现和整合纵向信息的能力也展现出显著潜力。

Conclusion: LLMs能够显著提升精神分裂症高危患者的评估标准化与准确性，有潜力成为辅助临床决策的有力工具。

Abstract: Patients who are at clinical high risk (CHR) for schizophrenia need close
monitoring of their symptoms to inform appropriate treatments. The Brief
Psychiatric Rating Scale (BPRS) is a validated, commonly used research tool for
measuring symptoms in patients with schizophrenia and other psychotic
disorders; however, it is not commonly used in clinical practice as it requires
a lengthy structured interview. Here, we utilize large language models (LLMs)
to predict BPRS scores from clinical interview transcripts in 409 CHR patients
from the Accelerating Medicines Partnership Schizophrenia (AMP-SCZ) cohort.
Despite the interviews not being specifically structured to measure the BPRS,
the zero-shot performance of the LLM predictions compared to the true
assessment (median concordance: 0.84, ICC: 0.73) approaches human inter- and
intra-rater reliability. We further demonstrate that LLMs have substantial
potential to improve and standardize the assessment of CHR patients via their
accuracy in assessing the BPRS in foreign languages (median concordance: 0.88,
ICC: 0.70), and integrating longitudinal information in a one-shot or few-shot
learning approach.

</details>


### [152] [A Computational Approach to Analyzing Language Change and Variation in the Constructed Language Toki Pona](https://arxiv.org/abs/2508.10246)
*Daniel Huang,Hyoun-A Joo*

Main category: cs.CL

TL;DR: 本文研究了人造语言Toki Pona的变化和变异，发现其演化与自然语言相似。


<details>
  <summary>Details</summary>
Motivation: 探讨人造语言（如Toki Pona）是否会受社交语言学因素影响并自然演化。

Method: 采用计算方法和语料库分析，研究Toki Pona不同句法位置的词语偏好变化及语料库间使用的变异。

Result: 研究发现，Toki Pona的演化受社会语言学因素影响，与自然语言的演化模式一致。

Conclusion: 即使是人造语言，也会在实际使用过程中自然发展和演化。

Abstract: This study explores language change and variation in Toki Pona, a constructed
language with approximately 120 core words. Taking a computational and
corpus-based approach, the study examines features including fluid word classes
and transitivity in order to examine (1) changes in preferences of content
words for different syntactic positions over time and (2) variation in usage
across different corpora. The results suggest that sociolinguistic factors
influence Toki Pona in the same way as natural languages, and that even
constructed linguistic systems naturally evolve as communities use them.

</details>


### [153] [Inductive Bias Extraction and Matching for LLM Prompts](https://arxiv.org/abs/2508.10295)
*Christian M. Angel,Francis Ferraro*

Main category: cs.CL

TL;DR: 本文探讨了通过提取和匹配LLM的归纳偏差，在设计提示时提升LLM性能的方法。


<details>
  <summary>Details</summary>
Motivation: LLM对提示词的变化高度敏感，而这一敏感性与LLM的归纳偏差有关，通过更系统地设计提示可以提升其表现。

Method: 提出了一种归纳偏差提取与匹配策略，通过将LLM输出的一部分作为提示的一部分，从而更高效地生成合适提示。

Result: 实验结果表明，该方法在分类任务中LLM Likert评分提升了19%，在排序任务中提升了27%。

Conclusion: 基于归纳偏差的提示设计可以显著提升LLM的任务性能。

Abstract: The active research topic of prompt engineering makes it evident that LLMs
are sensitive to small changes in prompt wording. A portion of this can be
ascribed to the inductive bias that is present in the LLM. By using an LLM's
output as a portion of its prompt, we can more easily create satisfactory
wording for prompts. This has the effect of creating a prompt that matches the
inductive bias in model. Empirically, we show that using this Inductive Bias
Extraction and Matching strategy improves LLM Likert ratings used for
classification by up to 19% and LLM Likert ratings used for ranking by up to
27%.

</details>


### [154] [Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race](https://arxiv.org/abs/2508.10304)
*Gustavo Bonil,Simone Hashiguti,Jhessica Silva,João Gondim,Helena Maia,Nádia Silva,Helio Pedrini,Sandra Avila*

Main category: cs.CL

TL;DR: 该研究分析了大型语言模型（LLMs）在生成文本中可能存在的性别和种族偏见，强调通过质性分析来补充现有的量化方法，从而更准确地检测和缓解这些偏见。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和LLM的不断发展，存在这些模型对于偏见（如歧视和种族化）以及话语霸权的复制问题，需要更深入的方法来揭示这种偏见的表现形式。

Method: 本文通过提出一种质性分析框架，对LLM生成的关于黑人和白人女性的短篇故事进行手动分析，从性别和种族角度探讨偏见现象。

Result: 研究表明，黑人女性多被描述为与传承和抗争相联系，而白人女性则多涉及自我发现。这种模式反映了模型如何复制固化的话语表述，强化了刻板印象和社会不流动性的观念。此外，模型在试图纠正偏见时仅提供表面化的修改，无法真正促进包容性叙事。

Conclusion: 研究揭示了算法的意识形态功能，强调AI设计和应用中需要批判性和跨学科的方法，以解决LLM生成的内容如何反映和延续不平等问题，对AI的伦理使用和发展具有重要意义。

Abstract: With the advance of Artificial Intelligence (AI), Large Language Models
(LLMs) have gained prominence and been applied in diverse contexts. As they
evolve into more sophisticated versions, it is essential to assess whether they
reproduce biases, such as discrimination and racialization, while maintaining
hegemonic discourses. Current bias detection approaches rely mostly on
quantitative, automated methods, which often overlook the nuanced ways in which
biases emerge in natural language. This study proposes a qualitative,
discursive framework to complement such methods. Through manual analysis of
LLM-generated short stories featuring Black and white women, we investigate
gender and racial biases. We contend that qualitative methods such as the one
proposed here are fundamental to help both developers and users identify the
precise ways in which biases manifest in LLM outputs, thus enabling better
conditions to mitigate them. Results show that Black women are portrayed as
tied to ancestry and resistance, while white women appear in self-discovery
processes. These patterns reflect how language models replicate crystalized
discursive representations, reinforcing essentialization and a sense of social
immobility. When prompted to correct biases, models offered superficial
revisions that maintained problematic meanings, revealing limitations in
fostering inclusive narratives. Our results demonstrate the ideological
functioning of algorithms and have significant implications for the ethical use
and development of AI. The study reinforces the need for critical,
interdisciplinary approaches to AI design and deployment, addressing how
LLM-generated discourses reflect and perpetuate inequalities.

</details>


### [155] [ReviewRL: Towards Automated Scientific Review with RL](https://arxiv.org/abs/2508.10308)
*Sihang Zeng,Kai Tian,Kaiyan Zhang,Yuru wang,Junqi Gao,Runze Liu,Sa Yang,Jingxuan Li,Xinwei Long,Jiaheng Ma,Biqing Qi,Bowen Zhou*

Main category: cs.CL

TL;DR: 提出了ReviewRL，一个强化学习框架，用于生成全面可靠的科学论文评论，显著优于现有方法，并将在GitHub上发布。


<details>
  <summary>Details</summary>
Motivation: 随着提交量增加和审稿人疲劳，科学审稿面临挑战。现有自动化审稿方法缺乏准确性、一致性和深入分析能力。

Method: 提出一个名为ReviewRL的框架，通过检索增强的上下文生成、监督微调，以及强化学习进行科学论文的自动化高质量审稿。

Result: ReviewRL在ICLR 2025论文实验中显著优于现有方法，无论是基于规则的指标还是基于模型的质量评估。

Conclusion: ReviewRL为利用强化学习驱动科学发现中的自动评论生成奠定了基础，证明了其发展潜力。

Abstract: Peer review is essential for scientific progress but faces growing challenges
due to increasing submission volumes and reviewer fatigue. Existing automated
review approaches struggle with factual accuracy, rating consistency, and
analytical depth, often generating superficial or generic feedback lacking the
insights characteristic of high-quality human reviews. We introduce ReviewRL, a
reinforcement learning framework for generating comprehensive and factually
grounded scientific paper reviews. Our approach combines: (1) an ArXiv-MCP
retrieval-augmented context generation pipeline that incorporates relevant
scientific literature, (2) supervised fine-tuning that establishes foundational
reviewing capabilities, and (3) a reinforcement learning procedure with a
composite reward function that jointly enhances review quality and rating
accuracy. Experiments on ICLR 2025 papers demonstrate that ReviewRL
significantly outperforms existing methods across both rule-based metrics and
model-based quality assessments. ReviewRL establishes a foundational framework
for RL-driven automatic critique generation in scientific discovery,
demonstrating promising potential for future development in this domain. The
implementation of ReviewRL will be released at GitHub.

</details>


### [156] [From Surface to Semantics: Semantic Structure Parsing for Table-Centric Document Analysis](https://arxiv.org/abs/2508.10311)
*Xuan Li,Jialiang Dong,Raymond Wong*

Main category: cs.CL

TL;DR: 文章提出了一个名为DOTABLER的框架，用于实现基于表格的文档语义分析，解决了传统表格任务在语义解析及上下文关联中的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前研究多集中于表面任务（如布局分析、表格检测和数据提取），缺乏对表格深层语义和上下文关联的解析。这种欠缺阻碍了跨段落数据解读和上下文一致分析等高级任务的实现。

Method: 提出一个基于表格的语义文档解析框架DOTABLER，通过自定义数据集及微调预训练模型的方式，结合完整的解析流程，旨在挖掘表格与其上下文的深层语义链接，并执行相关的表格中心文档结构解析及领域特定的表格检索。

Result: DOTABLER在约4000页的真实世界PDF文档以及1000多张表格测试中，实现了超过90%的精确率与F1分数，比先进模型（如GPT-4o）在表格语义解析和文档深度解析方面表现更优。

Conclusion: DOTABLER框架提供了更高精度、更全面的表格语义分析和上下文解析能力，为表格相关高级任务提供了优越的解决方案。

Abstract: Documents are core carriers of information and knowl-edge, with broad
applications in finance, healthcare, and scientific research. Tables, as the
main medium for structured data, encapsulate key information and are among the
most critical document components. Existing studies largely focus on
surface-level tasks such as layout analysis, table detection, and data
extraction, lacking deep semantic parsing of tables and their contextual
associations. This limits advanced tasks like cross-paragraph data
interpretation and context-consistent analysis. To address this, we propose
DOTABLER, a table-centric semantic document parsing framework designed to
uncover deep semantic links between tables and their context. DOTABLER
leverages a custom dataset and domain-specific fine-tuning of pre-trained
models, integrating a complete parsing pipeline to identify context segments
semantically tied to tables. Built on this semantic understanding, DOTABLER
implements two core functionalities: table-centric document structure parsing
and domain-specific table retrieval, delivering comprehensive table-anchored
semantic analysis and precise extraction of semantically relevant tables.
Evaluated on nearly 4,000 pages with over 1,000 tables from real-world PDFs,
DOTABLER achieves over 90% Precision and F1 scores, demonstrating superior
performance in table-context semantic analysis and deep document parsing
compared to advanced models such as GPT-4o.

</details>


### [157] [Beyond Semantic Understanding: Preserving Collaborative Frequency Components in LLM-based Recommendation](https://arxiv.org/abs/2508.10312)
*Minhao Wang,Yunhang He,Cong Xu,Zhangchi Zhu,Wei Zhang*

Main category: cs.CL

TL;DR: 本文提出FreLLM4Rec方法，通过频谱视角平衡语义和协作信息，解决了LLM在推荐系统中协作信号削弱的问题，并在四个基准数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的推荐系统往往过度关注语义关联，导致协作信号逐层弱化，甚至削弱推荐性能，亟需一种方法平衡语义与协作信息。

Method: 提出FreLLM4Rec方法，结合全局图低通滤波器（G-LPF）去噪和时间频率调制（TFM）分层保留协作信号，同时利用频域理论保障TFM性能。

Result: 实验证明，该方法在四个基准数据集上的NDCG@10性能最多提升8%，有效缓解了协作信号衰减问题，并展现了与现有基线方法的竞争性。

Conclusion: FreLLM4Rec为改进基于LLM的推荐系统提供了理论依据和平衡协作与语义信息的有效途径。

Abstract: Recommender systems in concert with Large Language Models (LLMs) present
promising avenues for generating semantically-informed recommendations.
However, LLM-based recommenders exhibit a tendency to overemphasize semantic
correlations within users' interaction history. When taking pretrained
collaborative ID embeddings as input, LLM-based recommenders progressively
weaken the inherent collaborative signals as the embeddings propagate through
LLM backbones layer by layer, as opposed to traditional Transformer-based
sequential models in which collaborative signals are typically preserved or
even enhanced for state-of-the-art performance. To address this limitation, we
introduce FreLLM4Rec, an approach designed to balance semantic and
collaborative information from a spectral perspective. Item embeddings that
incorporate both semantic and collaborative information are first purified
using a Global Graph Low-Pass Filter (G-LPF) to preliminarily remove irrelevant
high-frequency noise. Temporal Frequency Modulation (TFM) then actively
preserves collaborative signal layer by layer. Note that the collaborative
preservation capability of TFM is theoretically guaranteed by establishing a
connection between the optimal but hard-to-implement local graph fourier
filters and the suboptimal yet computationally efficient frequency-domain
filters. Extensive experiments on four benchmark datasets demonstrate that
FreLLM4Rec successfully mitigates collaborative signal attenuation and achieves
competitive performance, with improvements of up to 8.00\% in NDCG@10 over the
best baseline. Our findings provide insights into how LLMs process
collaborative information and offer a principled approach for improving
LLM-based recommendation systems.

</details>


### [158] [Cross-Prompt Encoder for Low-Performing Languages](https://arxiv.org/abs/2508.10352)
*Beso Mikaberidze,Teimuraz Saghinadze,Simon Ostermann,Philipp Muller*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，使用Cross-Prompt Encoder（XPE）和Dual Soft Prompt机制，以改进低性能语言的模型表现，并在多语环境中表现出更强的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调方法已经展现了强大的潜力，但对于跨语言适应性仍未深入探讨。本文旨在提高在低表现语言上的性能，同时实现更广泛的跨语言可迁移性。

Method: 提出了Cross-Prompt Encoder（XPE），结合基于轻量编码架构和来源多样化的语言训练，还引入了Dual Soft Prompt机制，将基于编码器的提示与直接训练的软提示结合应用。

Result: 实验表明，在SIB-200基准测试上，XPE对低性能语言表现更加显著，而混合提示方案在多语环境中具有更强的适应性和通用性。

Conclusion: XPE和Dual Soft Prompt相结合的方法能够改进低性能语言的表现，同时也促进了多语设置中的广泛适应性。这为跨语言参数高效微调开辟了新的可能性。

Abstract: Soft prompts have emerged as a powerful alternative to adapters in
parameter-efficient fine-tuning (PEFT), enabling large language models (LLMs)
to adapt to downstream tasks without architectural changes or parameter
updates. While prior work has focused on stabilizing training via parameter
interaction in small neural prompt encoders, their broader potential for
transfer across languages remains unexplored. In this paper, we demonstrate
that a prompt encoder can play a central role in improving performance on
low-performing languages-those that achieve poor accuracy even under full-model
fine-tuning. We introduce the Cross-Prompt Encoder (XPE), which combines a
lightweight encoding architecture with multi-source training on typologically
diverse languages - a design that enables the model to capture abstract and
transferable patterns across languages. To complement XPE, we propose a Dual
Soft Prompt mechanism that combines an encoder-based prompt with a directly
trained standard soft prompt. This hybrid design proves especially effective
for target languages that benefit from both broadly shared structure and
language-specific alignment. Experiments on the SIB-200 benchmark reveal a
consistent trade-off: XPE is most effective for low-performing languages, while
hybrid variants offer broader adaptability across multilingual settings.

</details>


### [159] [Making Qwen3 Think in Korean with Reinforcement Learning](https://arxiv.org/abs/2508.10355)
*Jungyup Lee,Jemin Kim,Sang Park,SeungJae Lee*

Main category: cs.CL

TL;DR: 本文提出一种两阶段微调方法，使大语言模型Qwen3 14B实现以韩语进行原生思考的能力。


<details>
  <summary>Details</summary>
Motivation: 通过以韩语为基础强化逻辑推理能力，提高韩语任务表现，同时保持模型的整体推理能力。

Method: 第一阶段进行监督微调(SFT)，以高质量韩语推理数据集为训练基石；第二阶段使用强化学习及定制的GRPO算法，结合oracle评估模型，稳定地优化推理对齐和问题解决性能。

Result: 微调后模型在高级推理测试（特别是数学与编程任务）中表现显著提升，同时保持知识和语言能力。

Conclusion: 该方法成功地使模型实现以韩语进行的链式推理，并在多方面表现优异，证明了两阶段微调方法的有效性。

Abstract: We present a two-stage fine-tuning approach to make the large language model
Qwen3 14B "think" natively in Korean. In the first stage, supervised
fine-tuning (SFT) on a high-quality Korean reasoning dataset establishes a
strong foundation in Korean logical reasoning, yielding notable improvements in
Korean-language tasks and even some gains in general reasoning ability. In the
second stage, we employ reinforcement learning with a customized Group Relative
Policy Optimization (GRPO) algorithm to further enhance both Korean reasoning
alignment and overall problem-solving performance. We address critical
stability challenges in GRPO training - such as reward hacking and policy
collapse - by introducing an oracle judge model that calibrates the reward
signal. Our approach achieves stable learning (avoiding the collapse observed
in naive GRPO) and leads to steady, incremental performance gains. The final
RL-tuned model demonstrates substantially improved results on advanced
reasoning benchmarks (particularly math and coding tasks) while maintaining
knowledge and language proficiency, successfully conducting its internal
chain-of-thought entirely in Korean.

</details>


### [160] [Advancing Cross-lingual Aspect-Based Sentiment Analysis with LLMs and Constrained Decoding for Sequence-to-Sequence Models](https://arxiv.org/abs/2508.10366)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 本文提出了一种新的序列到序列方法，用于无需翻译工具的跨语言ABSA，性能提高达10%。


<details>
  <summary>Details</summary>
Motivation: 低资源语言的ABSA研究受限，当前方法依赖翻译工具，亟需新的方法解决此问题。

Method: 应用约束解码的序列到序列方法，消除翻译工具依赖，专注于复杂的跨语言ABSA任务。

Result: 新方法在跨语言ABSA任务中的性能提升了10%，并且在复杂任务上表现出色。

Conclusion: 新方法为跨语言ABSA提供了一种高效、实际的替代方案，同时指出英语言大模型表现较弱的短板。

Abstract: Aspect-based sentiment analysis (ABSA) has made significant strides, yet
challenges remain for low-resource languages due to the predominant focus on
English. Current cross-lingual ABSA studies often centre on simpler tasks and
rely heavily on external translation tools. In this paper, we present a novel
sequence-to-sequence method for compound ABSA tasks that eliminates the need
for such tools. Our approach, which uses constrained decoding, improves
cross-lingual ABSA performance by up to 10\%. This method broadens the scope of
cross-lingual ABSA, enabling it to handle more complex tasks and providing a
practical, efficient alternative to translation-dependent techniques.
Furthermore, we compare our approach with large language models (LLMs) and show
that while fine-tuned multilingual LLMs can achieve comparable results,
English-centric LLMs struggle with these tasks.

</details>


### [161] [Large Language Models for Summarizing Czech Historical Documents and Beyond](https://arxiv.org/abs/2508.10368)
*Václav Tran,Jakub Šmíd,Jiří Martínek,Ladislav Lenc,Pavel Král*

Main category: cs.CL

TL;DR: 本文研究捷克语的文本摘要，重点是现代和历史文档，总结出了新的State-of-the-Art成果，并公布了历史文档的新数据集。


<details>
  <summary>Details</summary>
Motivation: 捷克语文本摘要的研究较少，特别是历史文档方面，受语言复杂性和标注数据缺乏的限制。

Method: 利用大型语言模型Mistral和mT5，在现代捷克语数据集和新引入的历史数据集上进行实验。

Result: 在现代捷克语数据集SumeCzech上取得了最新的State-of-the-Art成果，并为历史文档数据集Posel od Čerchova提供了基线结果。

Conclusion: 研究对推进捷克语文本摘要具有重大潜力，为捷克历史文档处理打开了新的研究方向。

Abstract: Text summarization is the task of shortening a larger body of text into a
concise version while retaining its essential meaning and key information.
While summarization has been significantly explored in English and other
high-resource languages, Czech text summarization, particularly for historical
documents, remains underexplored due to linguistic complexities and a scarcity
of annotated datasets. Large language models such as Mistral and mT5 have
demonstrated excellent results on many natural language processing tasks and
languages. Therefore, we employ these models for Czech summarization, resulting
in two key contributions: (1) achieving new state-of-the-art results on the
modern Czech summarization dataset SumeCzech using these advanced models, and
(2) introducing a novel dataset called Posel od \v{C}erchova for summarization
of historical Czech documents with baseline results. Together, these
contributions provide a great potential for advancing Czech text summarization
and open new avenues for research in Czech historical text processing.

</details>


### [162] [Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with Constrained Decoding](https://arxiv.org/abs/2508.10369)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 本文提出一种基于序列到序列模型的约束解码方法，无需外部翻译工具，在7种语言和6个ABSA任务中超越现有方法，提升平均表现5%以上。


<details>
  <summary>Details</summary>
Motivation: 针对低资源语言的ABSA研究欠缺，现有跨语言方法多依赖翻译工具，效果有限，对复杂任务未予充分关注。

Method: 利用序列到序列模型的约束解码技术，结合多任务学习，统一处理多种ABSA任务。

Result: 方法在最复杂任务上表现平均提升5%，在多任务解码中提升超过10%，超越现有跨语言ABSA方法并设立新标准。

Conclusion: 通过分析跨语言ABSA方法的优劣，提出针对现实应用的改进建议，推进了该领域的研究进展。

Abstract: While aspect-based sentiment analysis (ABSA) has made substantial progress,
challenges remain for low-resource languages, which are often overlooked in
favour of English. Current cross-lingual ABSA approaches focus on limited, less
complex tasks and often rely on external translation tools. This paper
introduces a novel approach using constrained decoding with
sequence-to-sequence models, eliminating the need for unreliable translation
tools and improving cross-lingual performance by 5\% on average for the most
complex task. The proposed method also supports multi-tasking, which enables
solving multiple ABSA tasks with a single model, with constrained decoding
boosting results by more than 10\%.
  We evaluate our approach across seven languages and six ABSA tasks,
surpassing state-of-the-art methods and setting new benchmarks for previously
unexplored tasks. Additionally, we assess large language models (LLMs) in
zero-shot, few-shot, and fine-tuning scenarios. While LLMs perform poorly in
zero-shot and few-shot settings, fine-tuning achieves competitive results
compared to smaller multilingual models, albeit at the cost of longer training
and inference times.
  We provide practical recommendations for real-world applications, enhancing
the understanding of cross-lingual ABSA methodologies. This study offers
valuable insights into the strengths and limitations of cross-lingual ABSA
approaches, advancing the state-of-the-art in this challenging research domain.

</details>


### [163] [Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts](https://arxiv.org/abs/2508.10390)
*Chiyu Zhang,Lu Zhou,Xiaogang Xu,Jiafei Wu,Liming Fang,Zhe Liu*

Main category: cs.CL

TL;DR: 论文提出MDH混合评估框架结合大语言模型（LLMs）和少量人工监督，用于清洗数据集和检测越狱响应，并提出D-Attack和DH-CoT两种新策略以提升越狱攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 目前现有的数据集中存在不明显有害或无法诱导出有害输出的提示，导致难以准确评估越狱攻击效果，需要更有效的方法检测和清理恶意内容。

Method: 提出MDH框架，结合LLM注释与少量人工监督，提高评估准确性与效率；同时设计D-Attack策略用于上下文模拟以及DH-CoT策略结合劫持思维链，优化越狱攻击性能。

Result: MDH框架成功应用于数据清理和越狱响应检测，新策略显著增强了越狱攻击效果。

Conclusion: MDH框架实现了高效可靠的恶意内容评估，新策略显示出优化越狱攻击的潜力。相关代码和数据已开源提供给研究人员使用。

Abstract: Evaluating jailbreak attacks is challenging when prompts are not overtly
harmful or fail to induce harmful outputs. Unfortunately, many existing
red-teaming datasets contain such unsuitable prompts. To evaluate attacks
accurately, these datasets need to be assessed and cleaned for maliciousness.
However, existing malicious content detection methods rely on either manual
annotation, which is labor-intensive, or large language models (LLMs), which
have inconsistent accuracy in harmful types. To balance accuracy and
efficiency, we propose a hybrid evaluation framework named MDH (Malicious
content Detection based on LLMs with Human assistance) that combines LLM-based
annotation with minimal human oversight, and apply it to dataset cleaning and
detection of jailbroken responses. Furthermore, we find that well-crafted
developer messages can significantly boost jailbreak success, leading us to
propose two new strategies: D-Attack, which leverages context simulation, and
DH-CoT, which incorporates hijacked chains of thought. The Codes, datasets,
judgements, and detection results will be released in github repository:
https://github.com/AlienZhang1996/DH-CoT.

</details>


### [164] [Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation](https://arxiv.org/abs/2508.10404)
*Huizhen Shu,Xuying Li,Qirui Wang,Yuji Kosuga,Mengqiu Tian,Zhuo Li*

Main category: cs.CL

TL;DR: 本文提出一种新的黑盒攻击方法，通过稀疏特征扰动框架(SFPF)生成对抗性文本，并能有效绕过现有防御机制。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型的脆弱性并提高其鲁棒性，尤其是在面对对抗性示例时，是本研究的核心动力。

Method: 利用稀疏自动编码器(SAE)重建隐藏层表示，对成功攻击的文本进行特征聚类，识别并扰动高激活的关键特征，以生成对抗性文本，同时保留恶意意图并放大安全信号。

Result: 实验表明，SFPF生成的对抗性文本可以绕过现有的最先进防御机制，揭示当前NLP系统中持久的漏洞。

Conclusion: SFPF提供了新的红队测试策略，在对抗性效果与安全对齐之间实现平衡，但其效果取决于提示和层次，在更大模型或其他架构中的通用性尚需验证。

Abstract: With the rapid proliferation of Natural Language Processing (NLP), especially
Large Language Models (LLMs), generating adversarial examples to jailbreak LLMs
remains a key challenge for understanding model vulnerabilities and improving
robustness. In this context, we propose a new black-box attack method that
leverages the interpretability of large models. We introduce the Sparse Feature
Perturbation Framework (SFPF), a novel approach for adversarial text generation
that utilizes sparse autoencoders to identify and manipulate critical features
in text. After using the SAE model to reconstruct hidden layer representations,
we perform feature clustering on the successfully attacked texts to identify
features with higher activations. These highly activated features are then
perturbed to generate new adversarial texts. This selective perturbation
preserves the malicious intent while amplifying safety signals, thereby
increasing their potential to evade existing defenses. Our method enables a new
red-teaming strategy that balances adversarial effectiveness with safety
alignment. Experimental results demonstrate that adversarial texts generated by
SFPF can bypass state-of-the-art defense mechanisms, revealing persistent
vulnerabilities in current NLP systems.However, the method's effectiveness
varies across prompts and layers, and its generalizability to other
architectures and larger models remains to be validated.

</details>


### [165] [ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning](https://arxiv.org/abs/2508.10419)
*Juyuan Wang,Rongchen Zhao,Wei Wei,Yufeng Wang,Mo Yu,Jie Zhou,Jin Xu,Liyan Xu*

Main category: cs.CL

TL;DR: 提出一种称为ComoRAG的新方法，通过动态记忆工作区进行迭代推理以优化大语言模型在长文本叙事理解中的表现。


<details>
  <summary>Details</summary>
Motivation: 由于长文本中的复杂情节和动态关系，再加上LLM在处理长上下文时的限制以及高计算成本，提出更有效的检索方法成为必要。

Method: 设计ComoRAG，将叙事推理视为动态交互过程，使用记忆工作区和迭代推理循环，通过生成探测性查询和整合新证据来逐步形成连贯的上下文。

Result: 在四个长范围叙事基准测试中（超过20万tokens），ComoRAG相比传统检索生成技术（RAG）基线取得最多11%的提升。

Conclusion: ComoRAG为基于检索的长文本理解提供了一种认知驱动的创新模式，特别在需要全局理解的复杂查询中效果显著。

Abstract: Narrative comprehension on long stories and novels has been a challenging
domain attributed to their intricate plotlines and entangled, often evolving
relations among characters and entities. Given the LLM's diminished reasoning
over extended context and high computational cost, retrieval-based approaches
remain a pivotal role in practice. However, traditional RAG methods can fall
short due to their stateless, single-step retrieval process, which often
overlooks the dynamic nature of capturing interconnected relations within
long-range context. In this work, we propose ComoRAG, holding the principle
that narrative reasoning is not a one-shot process, but a dynamic, evolving
interplay between new evidence acquisition and past knowledge consolidation,
analogous to human cognition when reasoning with memory-related signals in the
brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes
iterative reasoning cycles while interacting with a dynamic memory workspace.
In each cycle, it generates probing queries to devise new exploratory paths,
then integrates the retrieved evidence of new aspects into a global memory
pool, thereby supporting the emergence of a coherent context for the query
resolution. Across four challenging long-context narrative benchmarks (200K+
tokens), ComoRAG outperforms strong RAG baselines with consistent relative
gains up to 11% compared to the strongest baseline. Further analysis reveals
that ComoRAG is particularly advantageous for complex queries requiring global
comprehension, offering a principled, cognitively motivated paradigm for
retrieval-based long context comprehension towards stateful reasoning. Our code
is publicly released at https://github.com/EternityJune25/ComoRAG

</details>


### [166] [Evaluating LLMs on Chinese Idiom Translation](https://arxiv.org/abs/2508.10421)
*Cai Yang,Yao Dou,David Heineman,Xiaofeng Wu,Wei Xu*

Main category: cs.CL

TL;DR: 本研究聚焦于中文习语翻译，提出IdiomEval框架，分析900个系统翻译对，发现现有模型在习语翻译上表现较差，GPT-4错误率为28%，评估指标与人类评分一致性低。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型虽然在机器翻译领域取得了显著进展，但对于中文习语翻译这一特定问题研究甚少，亟需更完善的评估框架和方法。

Method: 本文提出IdiomEval框架，通过四个不同领域收集的数据，标注并分析900对习语翻译结果，并对现有模型在习语翻译任务中的表现及其错误类型进行详细分类与评估。

Result: 研究发现，现有翻译系统在处理中文习语时存在显著问题，包括错误翻译、字面理解、部分翻译甚至遗漏翻译，其中表现最好的GPT-4也有28%的错误率。此外，目前的评估标准在测评习语质量时，与人类评分的相关性较低（Pearson相关系数低于0.48）。

Conclusion: 尽管机器翻译取得进步，但中文习语翻译依然面临显著挑战。提出了一种改进的评价模型，将习语错误翻译检测的F1分数提高至0.68，有望推动相关研究发展。

Abstract: Idioms, whose figurative meanings usually differ from their literal
interpretations, are common in everyday language, especially in Chinese, where
they often contain historical references and follow specific structural
patterns. Despite recent progress in machine translation with large language
models, little is known about Chinese idiom translation. In this work, we
introduce IdiomEval, a framework with a comprehensive error taxonomy for
Chinese idiom translation. We annotate 900 translation pairs from nine modern
systems, including GPT-4o and Google Translate, across four domains: web, news,
Wikipedia, and social media. We find these systems fail at idiom translation,
producing incorrect, literal, partial, or even missing translations. The
best-performing system, GPT-4, makes errors in 28% of cases. We also find that
existing evaluation metrics measure idiom quality poorly with Pearson
correlation below 0.48 with human ratings. We thus develop improved models that
achieve F$_1$ scores of 0.68 for detecting idiom translation errors.

</details>


### [167] [Computational Economics in Large Language Models: Exploring Model Behavior and Incentive Design under Resource Constraints](https://arxiv.org/abs/2508.10426)
*Sandeep Reddy,Kabir Khan,Rohit Patil,Ananya Chakraborty,Faizan A. Khan,Swati Kulkarni,Arjun Verma,Neha Singh*

Main category: cs.CL

TL;DR: 提出了一种“计算经济学”框架，通过激励驱动的训练方法，在减少计算成本的同时保持任务准确性，提高了大语言模型（LLM）的计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）因其显著的计算成本限制了广泛应用，因此需要在保留任务性能的前提下优化计算资源使用。

Method: 将LLM视为资源约束的内部经济系统，提出激励驱动的训练方法，通过加入可微的计算成本项，鼓励稀疏和高效的激活方式。

Result: 在GLUE（MNLI、STS-B、CoLA）和WikiText-103数据集上的实验表明，新方法的模型在准确度接近的情况下，FLOPS减少约40%，延迟降低，并且注意力模式更具解释性，表现优于后处理剪枝方法。

Conclusion: 基于经济学原理的设计方法为在资源受限情况下构建高效、自适应且更具透明性的大语言模型提供了一个合理的路径。

Abstract: Large language models (LLMs) are limited by substantial computational cost.
We introduce a "computational economics" framework that treats an LLM as an
internal economy of resource-constrained agents (attention heads and neuron
blocks) that must allocate scarce computation to maximize task utility. First,
we show empirically that when computation is scarce, standard LLMs reallocate
attention toward high-value tokens while preserving accuracy. Building on this
observation, we propose an incentive-driven training paradigm that augments the
task loss with a differentiable computation cost term, encouraging sparse and
efficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method
yields a family of models that trace a Pareto frontier and consistently
dominate post-hoc pruning; for a similar accuracy we obtain roughly a forty
percent reduction in FLOPS and lower latency, together with more interpretable
attention patterns. These results indicate that economic principles offer a
principled route to designing efficient, adaptive, and more transparent LLMs
under strict resource constraints.

</details>


### [168] [DiFaR: Enhancing Multimodal Misinformation Detection with Diverse, Factual, and Relevant Rationales](https://arxiv.org/abs/2508.10444)
*Herun Wan,Jiaying Wu,Minnan Luo,Xiangzheng Kong,Zihan Ma,Zhi Zeng*

Main category: cs.CL

TL;DR: 本文提出了DiFaR框架，通过生成多样、真实且相关的文字理由提升多模态虚假信息检测的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的虚假信息检测方法面临理由生成多样性不足、事实性错误以及相关性差的挑战。DiFaR旨在解决这些问题，优化理由生成。

Method: DiFaR使用五种思维链提示生成多样化推理轨迹，并通过轻量化后处理模块根据事实性和相关性分数筛选理由句子。

Result: 在四个主流基准上，DiFaR相比四种现有基线模型性能提升5.9%，并能增强现有检测器达8.7%。自动化指标与人工评估验证其生成理由质量的全面提升。

Conclusion: DiFaR在多模态虚假信息检测领域实现了显著的性能优化，并为理由生成树立了新标准。

Abstract: Generating textual rationales from large vision-language models (LVLMs) to
support trainable multimodal misinformation detectors has emerged as a
promising paradigm. However, its effectiveness is fundamentally limited by
three core challenges: (i) insufficient diversity in generated rationales, (ii)
factual inaccuracies due to hallucinations, and (iii) irrelevant or conflicting
content that introduces noise. We introduce DiFaR, a detector-agnostic
framework that produces diverse, factual, and relevant rationales to enhance
misinformation detection. DiFaR employs five chain-of-thought prompts to elicit
varied reasoning traces from LVLMs and incorporates a lightweight post-hoc
filtering module to select rationale sentences based on sentence-level
factuality and relevance scores. Extensive experiments on four popular
benchmarks demonstrate that DiFaR outperforms four baseline categories by up to
5.9% and boosts existing detectors by as much as 8.7%. Both automatic metrics
and human evaluations confirm that DiFaR significantly improves rationale
quality across all three dimensions.

</details>


### [169] [When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing](https://arxiv.org/abs/2508.10482)
*Mahdi Dhaini,Stephen Meisenbacher,Ege Erdogan,Florian Matthes,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 本文探讨了自然语言处理（NLP）领域中隐私性和可解释性之间的关系，并提出了它们共存的可能性。


<details>
  <summary>Details</summary>
Motivation: 尽管近年来可解释性和隐私保护的研究兴趣大幅增加，但关于二者交互的研究较少，亟需探索可解释性和隐私性是否能够同时实现。

Method: 使用差分隐私（DP）和后处理可解释性方法，对NLP中的隐私-可解释性权衡问题进行实证研究，探讨影响二者关系的因素。

Result: 研究揭示了隐私性和可解释性之间的复杂关系，这种关系受到下游任务性质和文本隐私化及可解释性方法选择的影响。

Conclusion: 研究证明隐私性和可解释性有可能共存，并提供了一系列实践建议，为该领域未来工作提供指导。

Abstract: In the study of trustworthy Natural Language Processing (NLP), a number of
important research fields have emerged, including that of
\textit{explainability} and \textit{privacy}. While research interest in both
explainable and privacy-preserving NLP has increased considerably in recent
years, there remains a lack of investigation at the intersection of the two.
This leaves a considerable gap in understanding of whether achieving
\textit{both} explainability and privacy is possible, or whether the two are at
odds with each other. In this work, we conduct an empirical investigation into
the privacy-explainability trade-off in the context of NLP, guided by the
popular overarching methods of \textit{Differential Privacy} (DP) and Post-hoc
Explainability. Our findings include a view into the intricate relationship
between privacy and explainability, which is formed by a number of factors,
including the nature of the downstream task and choice of the text
privatization and explainability method. In this, we highlight the potential
for privacy and explainability to co-exist, and we summarize our findings in a
collection of practical recommendations for future work at this important
intersection.

</details>


### [170] [When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models](https://arxiv.org/abs/2508.10552)
*Huyu Wu,Meng Tang,Xinhan Zheng,Haiyun Jiang*

Main category: cs.CL

TL;DR: 该研究在多模态任务中首次系统性探讨了多模态大语言模型（MLLMs）的“文本主导”问题，并提出了评估指标及解决方法。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型依赖文本进行推理，却未充分利用其他模态信息，现有研究对这一现象的理解较为有限。

Method: 提出了两种评估指标：模态主导指数（MDI）与注意力效率指数（AEI），进行多模态数据分析。此外，提出了一种简单的Token压缩方法改善模型性能。

Result: 研究揭示了文本主导问题的普遍存在，并指出其主要成因；应用Token压缩方法成功降低了某模型的文本主导指数（如降低LLaVA-7B模型的MDI指标从10.23至0.86）。

Conclusion: 研究为更均衡且全面的多模态语言模型开发奠定了理论和方法基础，有助于减轻文本主导问题。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities across a diverse range of multimodal tasks. However, these models
suffer from a core problem known as text dominance: they depend heavily on text
for their inference, while underutilizing other modalities. While prior work
has acknowledged this phenomenon in vision-language tasks, often attributing it
to data biases or model architectures. In this paper, we conduct the first
systematic investigation of text dominance across diverse data modalities,
including images, videos, audio, time-series, and graphs. To measure this
imbalance, we propose two evaluation metrics: the Modality Dominance Index
(MDI) and the Attention Efficiency Index (AEI). Our comprehensive analysis
reveals that text dominance is both significant and pervasive across all tested
modalities. Our in-depth analysis identifies three underlying causes: attention
dilution from severe token redundancy in non-textual modalities, the influence
of fusion architecture design, and task formulations that implicitly favor
textual inputs. Furthermore, we propose a simple token compression method that
effectively rebalances model attention. Applying this method to LLaVA-7B, for
instance, drastically reduces its MDI from 10.23 to a well-balanced value of
0.86. Our analysis and methodological framework offer a foundation for the
development of more equitable and comprehensive multimodal language models.

</details>


### [171] [eDIF: A European Deep Inference Fabric for Remote Interpretability of LLM](https://arxiv.org/abs/2508.10553)
*Irma Heithoff. Marc Guggenberger,Sandra Kalogiannis,Susanne Mayer,Fabian Maag,Sigurd Schacht,Carsten Lanquillon*

Main category: cs.CL

TL;DR: 该研究探讨了欧洲语言模型可解释性基础设施eDIF的可行性，通过一个集群平台和NNsight API实现远程模型检查。


<details>
  <summary>Details</summary>
Motivation: 推动欧洲范围内的语言模型可解释性研究设施的普及化，支持科研人员的模型分析需求。

Method: 建立GPU集群平台，通过一项涉及16名研究人员的试点研究，采用激活补丁、因果追踪及表示分析等方法评估其性能和实用性。

Result: 平台在用户参与度、性能稳定性等方面表现良好，但也发现下载时长及执行中断等问题。

Conclusion: 这一举措促进了欧洲范围内语言模型研究基础设施的普及，为未来的广泛部署和工具扩展奠定了良好基础。

Abstract: This paper presents a feasibility study on the deployment of a European Deep
Inference Fabric (eDIF), an NDIF-compatible infrastructure designed to support
mechanistic interpretability research on large language models. The need for
widespread accessibility of LLM interpretability infrastructure in Europe
drives this initiative to democratize advanced model analysis capabilities for
the research community. The project introduces a GPU-based cluster hosted at
Ansbach University of Applied Sciences and interconnected with partner
institutions, enabling remote model inspection via the NNsight API. A
structured pilot study involving 16 researchers from across Europe evaluated
the platform's technical performance, usability, and scientific utility. Users
conducted interventions such as activation patching, causal tracing, and
representation analysis on models including GPT-2 and DeepSeek-R1-70B. The
study revealed a gradual increase in user engagement, stable platform
performance throughout, and a positive reception of the remote experimentation
capabilities. It also marked the starting point for building a user community
around the platform. Identified limitations such as prolonged download
durations for activation data as well as intermittent execution interruptions
are addressed in the roadmap for future development. This initiative marks a
significant step towards widespread accessibility of LLM interpretability
infrastructure in Europe and lays the groundwork for broader deployment,
expanded tooling, and sustained community collaboration in mechanistic
interpretability research.

</details>


### [172] [Neural Machine Translation for Coptic-French: Strategies for Low-Resource Ancient Languages](https://arxiv.org/abs/2508.10683)
*Nasma Chaoui,Richard Khoury*

Main category: cs.CL

TL;DR: 论文研究了科普特语到法语的翻译方法，发现多样化和抗噪的训练数据能显著提升翻译效果。


<details>
  <summary>Details</summary>
Motivation: 探索提升历史语言翻译质量的策略，并提供对翻译工具开发的实践指导。

Method: 利用对齐的圣经语料，比较了枢轴翻译与直接翻译、预训练影响、多版本微调的益处，以及模型噪声鲁棒性。

Result: 使用具多样化且抗噪的训练语料微调后，翻译质量显著提高。

Conclusion: 研究揭示了开发历史语言翻译工具的重要见解，特别是在训练数据构建上的关键作用。

Abstract: This paper presents the first systematic study of strategies for translating
Coptic into French. Our comprehensive pipeline systematically evaluates: pivot
versus direct translation, the impact of pre-training, the benefits of
multi-version fine-tuning, and model robustness to noise. Utilizing aligned
biblical corpora, we demonstrate that fine-tuning with a stylistically-varied
and noise-aware training corpus significantly enhances translation quality. Our
findings provide crucial practical insights for developing translation tools
for historical languages in general.

</details>


### [173] [Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph](https://arxiv.org/abs/2508.10687)
*Safaeid Hossain Arib,Rabeya Akter,Sejuti Rahman*

Main category: cs.CL

TL;DR: 本文提出了一种将图卷积网络（STGCN-LSTM）与Transformer架构融合的方法，显著提升了无标注手语翻译的性能，并在多个数据集上实现了新的性能基准。


<details>
  <summary>Details</summary>
Motivation: 面对聋人和听力受损者在以口语为主的社会中面临的沟通障碍，本文旨在通过改进手语翻译的方法来增强交流的可及性。

Method: 结合Transformer和STGCN-LSTM架构，通过探索多种融合策略，提出一种无标注手语翻译的新方法，并引入了对BornilDB v1.0数据集的首次基准测试。

Result: 在多个手语数据集（RWTH-PHOENIX-2014T、CSL-Daily、How2Sign等）上表现优越，BLEU-4得分显著超越现有方法，分别提升了4.01、2.07和0.5。同时，首次对BornilDB v1.0数据集进行了基准测试。

Conclusion: 本文方法为未来的无标注手语翻译研究设立了新的基准，并强调了提升聋人和听力受损者沟通能力的重要性。

Abstract: Millions of individuals worldwide are affected by deafness and hearing
impairment. Sign language serves as a sophisticated means of communication for
the deaf and hard of hearing. However, in societies that prioritize spoken
languages, sign language often faces underestimation, leading to communication
barriers and social exclusion. The Continuous Bangla Sign Language Translation
project aims to address this gap by enhancing translation methods. While recent
approaches leverage transformer architecture for state-of-the-art results, our
method integrates graph-based methods with the transformer architecture. This
fusion, combining transformer and STGCN-LSTM architectures, proves more
effective in gloss-free translation. Our contributions include architectural
fusion, exploring various fusion strategies, and achieving a new
state-of-the-art performance on diverse sign language datasets, namely
RWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and BornilDB v1.0. Our approach
demonstrates superior performance compared to current translation outcomes
across all datasets, showcasing notable improvements of BLEU-4 scores of 4.01,
2.07, and 0.5, surpassing those of GASLT, GASLT and slt_how2sign in
RWTH-PHOENIX-2014T, CSL-Daily, and How2Sign, respectively. Also, we introduce
benchmarking on the BornilDB v1.0 dataset for the first time. Our method sets a
benchmark for future research, emphasizing the importance of gloss-free
translation to improve communication accessibility for the deaf and hard of
hearing.

</details>


### [174] [Learning from Natural Language Feedback for Personalized Question Answering](https://arxiv.org/abs/2508.10695)
*Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: 本文提出了一种名为VAC的新框架，使用自然语言反馈（NLF）代替标量奖励信号，以改进大语言模型的个性化问答能力。


<details>
  <summary>Details</summary>
Motivation: 当前个性化大语言模型的方法存在反馈信号弱、不具指导性的问题，限制了学习效率和个性化质量。本文旨在通过更丰富、可操作的监督信号来解决这些问题。

Method: 提出VAC框架，通过用户个人资料和问题描述生成自然语言反馈（NLF），替代现有的标量奖励信号。训练过程交替优化反馈模型和策略模型，以提升响应质量。

Result: 在LaMP-QA基准测试的三个不同领域上取得了显著优于现有方法的结果，同时也得到了人类评估的支持，表明生成的响应质量更高。

Conclusion: 自然语言反馈（NLF）为优化个性化问答提供了更有效的信号，并显著改善了策略模型的性能，实现了高质量的个性化响应生成。

Abstract: Personalization is crucial for enhancing both the effectiveness and user
satisfaction of language technologies, particularly in information-seeking
tasks like question answering. Current approaches for personalizing large
language models (LLMs) often rely on retrieval-augmented generation (RAG),
followed by reinforcement learning with scalar reward signals to teach models
how to use retrieved personal context. We believe that these scalar rewards
sometimes provide weak, non-instructive feedback, limiting learning efficiency
and personalization quality. We introduce VAC, a novel framework for
personalized response generation that replaces scalar rewards with natural
language feedback (NLF) that are generated conditioned on the user profiles and
the question narratives. NLF serves as a rich and actionable supervision
signal, allowing the policy model to iteratively refine its outputs and
internalize effective personalization strategies. Training alternates between
optimizing the feedback model and fine-tuning the policy model on the improved
responses, resulting in a policy model that no longer requires feedback at
inference. Evaluation on the LaMP-QA benchmark that consists of three diverse
domains demonstrates consistent and significant improvements over the
state-of-the-art results. Human evaluations further confirm the superior
quality of the generated responses. These results demonstrate that NLF provides
more effective signals for optimizing personalized question answering.

</details>


### [175] [Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs](https://arxiv.org/abs/2508.10736)
*Xiangqi Jin,Yuxuan Wang,Yifeng Gao,Zichen Wen,Biqing Qi,Dongrui Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 提出了一种新框架ICE，将仅前缀提示转化为适用于扩散大语言模型（dLLMs）的就地提示，并提供早退机制以提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型提示方式和生成方式在信息双向交互上受到限制，扩散大语言模型提供了新机会。

Method: 提出了ICE框架，结合了就地提示和基于信心的早退机制，优化迭代过程中的提示方式和计算效率。

Result: 通过实验，在GSM8K上提升17.29%的准确率，同时加速4.12倍；在MMLU上速度提升了多达276.67倍，性能仍具竞争力。

Conclusion: ICE框架在优化dLLMs提示策略和提升计算效率上表现优异，为扩散语言模型的应用开辟了新方向。

Abstract: Despite large language models (LLMs) have achieved remarkable success, their
prefix-only prompting paradigm and sequential generation process offer limited
flexibility for bidirectional information. Diffusion large language models
(dLLMs) present new opportunities through their bidirectional attention
mechanisms and iterative refinement processes, enabling more flexible in-place
prompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting
with Early Exit), a novel framework that transforms prefix-only prompting into
in-place prompting specifically designed for dLLMs. ICE integrates in-place
prompts directly within masked token positions during iterative refinement and
employs a confidence-aware early exit mechanism to significantly reduce
computational overhead. Extensive experiments demonstrate ICE's effectiveness,
achieving up to 17.29% accuracy improvement with 4.12$\times$ speedup on GSM8K,
and up to 276.67$\times$ acceleration on MMLU while maintaining competitive
performance.

</details>


### [176] [Beyond "Not Novel Enough": Enriching Scholarly Critique with LLM-Assisted Feedback](https://arxiv.org/abs/2508.10795)
*Osama Mohammed Afzal,Preslav Nakov,Tom Hope,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文提出了一种结构化的自动化评估方法，用于模拟专家评审员在审稿中对论文新颖性的评价。


<details>
  <summary>Details</summary>
Motivation: 在高产出领域如NLP中，评审工作量巨大，新颖性评估尤为重要，而现有方法较为缺乏。

Method: 方法由三部分组成：从投稿中提取内容、检索并综合相关工作、基于证据进行结构化比较。

Result: 在182篇投稿的实验中，该方法与人类新颖性评估的推理对齐率达86.5%，新颖性结论一致性为75.3%，显著优于现有的基线方法。

Conclusion: 结构化的LLM辅助方法有潜力在不取代人类专业能力的前提下，使同行评审更具科学性和透明性，同时数据和代码已开放。

Abstract: Novelty assessment is a central yet understudied aspect of peer review,
particularly in high volume fields like NLP where reviewer capacity is
increasingly strained. We present a structured approach for automated novelty
evaluation that models expert reviewer behavior through three stages: content
extraction from submissions, retrieval and synthesis of related work, and
structured comparison for evidence based assessment. Our method is informed by
a large scale analysis of human written novelty reviews and captures key
patterns such as independent claim verification and contextual reasoning.
Evaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty
assessments, the approach achieves 86.5% alignment with human reasoning and
75.3% agreement on novelty conclusions - substantially outperforming existing
LLM based baselines. The method produces detailed, literature aware analyses
and improves consistency over ad hoc reviewer judgments. These results
highlight the potential for structured LLM assisted approaches to support more
rigorous and transparent peer review without displacing human expertise. Data
and code are made available.

</details>


### [177] [Reinforced Language Models for Sequential Decision Making](https://arxiv.org/abs/2508.10839)
*Jim Dilkes,Vahid Yazdanpanah,Sebastian Stein*

Main category: cs.CL

TL;DR: 这篇论文提出了一种名为MS-GRPO的新算法，通过后训练来提高小规模语言模型在多步决策任务中的表现，并在Snake和Frozen Lake任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型在多步决策任务中的应用受限于高计算成本，因此需要改进小规模模型。现有后训练方法难以解决多步任务的信用分配问题。

Method: 提出了MS-GRPO算法，基于文本介导的随机博弈和语言代理策略框架，通过将整个累积奖励分配给每一步。还引入了基于绝对优势加权的采样策略以提升训练表现。

Result: 实验表明，该方法在Frozen Lake任务中使3B参数模型性能提高了50%，超越了72B参数基线。

Conclusion: 通过针对性后训练，小规模语言模型可以在多步决策任务中取得显著性能提升，是一种比扩展模型规模更实用高效的选择。

Abstract: Large Language Models (LLMs) show potential as sequential decision-making
agents, but their application is often limited due to a reliance on large,
computationally expensive models. This creates a need to improve smaller
models, yet existing post-training methods are designed for single-turn
interactions and cannot handle credit assignment in multi-step agentic tasks.
To address this, we introduce Multi-Step Group-Relative Policy Optimization
(MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal
Text-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP)
frameworks. For credit assignment, MS-GRPO attributes the entire cumulative
episode reward to each individual episode step. We supplement this algorithm
with a novel absolute-advantage-weighted episode sampling strategy that we show
improves training performance. We evaluate our approach by post-training a
3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate
that the method is effective in improving decision-making performance: our
post-trained 3B parameter model outperforms a 72B parameter baseline by 50% on
the Frozen Lake task. This work demonstrates that targeted post-training is a
practical and efficient alternative to relying on model scale for creating
sequential decision-making agents using LLMs.

</details>


### [178] [Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning](https://arxiv.org/abs/2508.10848)
*Chongyuan Dai,Jinpeng Hu,Hongchang Shi,Zhuo Li,Xun Yang,Meng Wang*

Main category: cs.CL

TL;DR: 该论文介绍了Psyche-R1，这是第一个整合同理心、心理专业知识和推理能力的中文心理学大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前心理健康专业人员短缺，急需将大型语言模型（LLMs）应用于心理学领域，以缓解心理健康负担。现有研究主要注重情感支持和同理心对话，而对生成可靠响应所需的推理机制重视不足。

Method: 提出一种数据合成流程，生成超过7.5万高质量心理学问题及详细推理过程，还有7.3万同理心对话；采用多LLM交叉选择策略和混合训练策略优化推理能力，同时通过监督微调提升同理心和专业知识生成能力。

Result: 实验结果表明，Psyche-R1在多个心理学基准测试上效果显著，其中7B参数的Psyche-R1性能接近671B参数的DeepSeek-R1。

Conclusion: Psyche-R1在整合心理专业知识、推理和共情能力方面取得突破，展示了心理学领域内LLM的潜力和实际价值。

Abstract: Amidst a shortage of qualified mental health professionals, the integration
of large language models (LLMs) into psychological applications offers a
promising way to alleviate the growing burden of mental health disorders.
Recent reasoning-augmented LLMs have achieved remarkable performance in
mathematics and programming, while research in the psychological domain has
predominantly emphasized emotional support and empathetic dialogue, with
limited attention to reasoning mechanisms that are beneficial to generating
reliable responses. Therefore, in this paper, we propose Psyche-R1, the first
Chinese psychological LLM that jointly integrates empathy, psychological
expertise, and reasoning, built upon a novel data curation pipeline.
Specifically, we design a comprehensive data synthesis pipeline that produces
over 75k high-quality psychological questions paired with detailed rationales,
generated through chain-of-thought (CoT) reasoning and iterative
prompt-rationale optimization, along with 73k empathetic dialogues.
Subsequently, we employ a hybrid training strategy wherein challenging samples
are identified through a multi-LLM cross-selection strategy for group relative
policy optimization (GRPO) to improve reasoning ability, while the remaining
data is used for supervised fine-tuning (SFT) to enhance empathetic response
generation and psychological domain knowledge. Extensive experiment results
demonstrate the effectiveness of the Psyche-R1 across several psychological
benchmarks, where our 7B Psyche-R1 achieves comparable results to 671B
DeepSeek-R1.

</details>


### [179] [From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms](https://arxiv.org/abs/2508.10860)
*Zhaokun Jiang,Ziyin Zhang*

Main category: cs.CL

TL;DR: 该论文提出了一种多维建模方法，结合特征工程、数据增强和可解释机器学习，用于解释性优先的口译质量评估。


<details>
  <summary>Details</summary>
Motivation: 解决当前口译质量评估研究在语言使用质量不足、数据稀缺与不平衡、以及缺乏模型预测解释力方面的不足。

Method: 通过特征工程和数据增强结合Shapley Value (SHAP) 分析开发了一种解释优先的多维建模框架，聚焦透明且与构造相关的特征。

Result: 该方法在新的英中口译数据集上表现出强大的预测能力，发现BLEURT和CometKiwi得分是忠实性的重要特征，停顿相关特征影响流利性，而中文短语多样性指标影响语言使用质量。

Conclusion: 提出了一种可靠且透明的评估替代方法，不仅提高了诊断反馈的详细程度，还支持自主学习的优势。

Abstract: Recent advancements in machine learning have spurred growing interests in
automated interpreting quality assessment. Nevertheless, existing research
suffers from insufficient examination of language use quality, unsatisfactory
modeling effectiveness due to data scarcity and imbalance, and a lack of
efforts to explain model predictions. To address these gaps, we propose a
multi-dimensional modeling framework that integrates feature engineering, data
augmentation, and explainable machine learning. This approach prioritizes
explainability over ``black box'' predictions by utilizing only
construct-relevant, transparent features and conducting Shapley Value (SHAP)
analysis. Our results demonstrate strong predictive performance on a novel
English-Chinese consecutive interpreting dataset, identifying BLEURT and
CometKiwi scores to be the strongest predictive features for fidelity,
pause-related features for fluency, and Chinese-specific phraseological
diversity metrics for language use. Overall, by placing particular emphasis on
explainability, we present a scalable, reliable, and transparent alternative to
traditional human evaluation, facilitating the provision of detailed diagnostic
feedback for learners and supporting self-regulated learning advantages not
afforded by automated scores in isolation.

</details>


### [180] [SSRL: Self-Search Reinforcement Learning](https://arxiv.org/abs/2508.10874)
*Yuchen Fan,Kaiyan Zhang,Heng Zhou,Yuxin Zuo,Yanxu Chen,Yu Fu,Xinwei Long,Xuekai Zhu,Che Jiang,Yuchen Zhang,Li Kang,Gang Chen,Cheng Huang,Zhizhou He,Bingning Wang,Lei Bai,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: 本文探讨了使用大型语言模型(LLMs)作为强化学习(RL)中的高效模拟器，以减少对外部搜索引擎交互的依赖，提出了Self-Search RL (SSRL)方法，并证明其效果显著。


<details>
  <summary>Details</summary>
Motivation: 减少强化学习任务对外部搜索引擎交互的高成本依赖，利用LLMs来作为模拟器并提升任务效率。

Method: 通过结构化提示和重复采样量化LLMs的内在搜索能力（Self-Search）；引入Self-Search RL (SSRL)方法，通过格式化和基于规则的奖励提升LLMs内部搜索能力，并利用其强化搜索驱动的RL训练。

Result: 与传统依赖外部搜索引擎的方法相比，SSRL训练的模型更加具成本效益和稳定性，并能实现仿真到实际的稳健转移。

Conclusion: LLMs具备可有效触发的世界知识；SSRL 能减少幻觉现象；经过SSRL训练的模型可以无缝整合外部搜索引擎支持强化学习任务。

Abstract: We investigate the potential of large language models (LLMs) to serve as
efficient simulators for agentic search tasks in reinforcement learning (RL),
thereby reducing dependence on costly interactions with external search
engines. To this end, we first quantify the intrinsic search capability of LLMs
via structured prompting and repeated sampling, which we term Self-Search. Our
results reveal that LLMs exhibit strong scaling behavior with respect to the
inference budget, achieving high pass@k on question-answering benchmarks,
including the challenging BrowseComp task. Building on these observations, we
introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability
through format-based and rule-based rewards. SSRL enables models to iteratively
refine their knowledge utilization internally, without requiring access to
external tools. Empirical evaluations demonstrate that SSRL-trained policy
models provide a cost-effective and stable environment for search-driven RL
training, reducing reliance on external search engines and facilitating robust
sim-to-real transfer. We draw the following conclusions: 1) LLMs possess world
knowledge that can be effectively elicited to achieve high performance; 2) SSRL
demonstrates the potential of leveraging internal knowledge to reduce
hallucination; 3) SSRL-trained models integrate seamlessly with external search
engines without additional effort. Our findings highlight the potential of LLMs
to support more scalable RL agent training.

</details>


### [181] [A Survey on Diffusion Language Models](https://arxiv.org/abs/2508.10875)
*Tianyi Li,Mingda Chen,Bowei Guo,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 扩散语言模型（DLM）作为一种新兴范式，通过并行生成token和去噪迭代，减少推理延迟并捕获双向上下文，其性能已接近传统自回归模型。本文综述了DLM领域的最新进展及应用场景，分析其挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探索扩散语言模型（DLM）的潜力及其在自然语言处理任务中的应用，旨在提供一种快速、高效并具备精细生成控制的新型解决方案。

Method: 综述研究，通过梳理近年来DLM与相关语言建模技术的演进，探讨其训练方法、推理策略、多模态扩展及优化技术，提供深入分析与分类。

Result: 展示了DLM多个方面的显著进展，包括速度提升、效果接近自回归模型，以及在推理策略和多模态应用中的优化效果。

Conclusion: DLM作为一种新兴的语言建模范式，拥有广阔应用潜力。尽管面临如效率和长序列处理的挑战，但其迭代式生成特点和良好性能使其成为未来研究的可行方向。

Abstract: Diffusion Language Models (DLMs) are rapidly emerging as a powerful and
promising alternative to the dominant autoregressive (AR) paradigm. By
generating tokens in parallel through an iterative denoising process, DLMs
possess inherent advantages in reducing inference latency and capturing
bidirectional context, thereby enabling fine-grained control over the
generation process. While achieving a several-fold speed-up, recent
advancements have allowed DLMs to show performance comparable to their
autoregressive counterparts, making them a compelling choice for various
natural language processing tasks. In this survey, we provide a holistic
overview of the current DLM landscape. We trace its evolution and relationship
with other paradigms, such as autoregressive and masked language models, and
cover both foundational principles and state-of-the-art models. Our work offers
an up-to-date, comprehensive taxonomy and an in-depth analysis of current
techniques, from pre-training strategies to advanced post-training methods.
Another contribution of this survey is a thorough review of DLM inference
strategies and optimizations, including improvements in decoding parallelism,
caching mechanisms, and generation quality. We also highlight the latest
approaches to multimodal extensions of DLMs and delineate their applications
across various practical scenarios. Furthermore, our discussion addresses the
limitations and challenges of DLMs, including efficiency, long-sequence
handling, and infrastructure requirements, while outlining future research
directions to sustain progress in this rapidly evolving field. Project GitHub
is available at https://github.com/VILA-Lab/Awesome-DLMs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [182] [A Survey of Optimization Modeling Meets LLMs: Progress and Future Directions](https://arxiv.org/abs/2508.10047)
*Ziyang Xiao,Jingrong Xie,Lilin Xu,Shisi Guan,Jingyan Zhu,Xiongwei Han,Xiaojin Fu,WingYin Yu,Han Wu,Wei Shi,Qingcan Kang,Jiahui Duan,Tao Zhong,Mingxuan Yuan,Jia Zeng,Yuan Wang,Gang Chen,Dongxiang Zhang*

Main category: cs.AI

TL;DR: 这篇文章回顾了优化建模结合大语言模型（LLMs）取得的进展，并提供了数据集清理和评估资源等。


<details>
  <summary>Details</summary>
Motivation: 优化建模在现实问题中具有广泛应用，但需要运筹学专家的专门技能。随着LLMs的出现，为自动化数学建模程序带来了新机会。

Method: 作者综述了LLMs在优化建模中的技术进展，从数据合成、模型微调到推理框架和数据集评测，并进行了数据集质量分析与清理，创建了新排行榜和在线门户。

Result: 研究发现现有基准数据集错误率较高，作者清理了数据集并提供更公平的性能评估资源。

Conclusion: 当前方法仍有局限性，未来需要进一步研究并解决这些问题。

Abstract: By virtue of its great utility in solving real-world problems, optimization
modeling has been widely employed for optimal decision-making across various
sectors, but it requires substantial expertise from operations research
professionals. With the advent of large language models (LLMs), new
opportunities have emerged to automate the procedure of mathematical modeling.
This survey presents a comprehensive and timely review of recent advancements
that cover the entire technical stack, including data synthesis and fine-tuning
for the base model, inference frameworks, benchmark datasets, and performance
evaluation. In addition, we conducted an in-depth analysis on the quality of
benchmark datasets, which was found to have a surprisingly high error rate. We
cleaned the datasets and constructed a new leaderboard with fair performance
evaluation in terms of base LLM model and datasets. We also build an online
portal that integrates resources of cleaned datasets, code and paper repository
to benefit the community. Finally, we identify limitations in current
methodologies and outline future research opportunities.

</details>


### [183] [Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development](https://arxiv.org/abs/2508.10108)
*Sattvik Sahai,Prasoon Goyal,Michael Johnston,Anna Gottardi,Yao Lu,Lucy Hu,Luke Dai,Shaohua Liu,Samyuth Sagi,Hangjie Shi,Desheng Zhang,Lavina Vaz,Leslie Ball,Maureen Murray,Rahul Gupta,Shankar Ananthakrishna*

Main category: cs.AI

TL;DR: 本论文介绍了亚马逊发起的Nova AI挑战赛，旨在促进AI安全性特别是软件开发中的安全研究。


<details>
  <summary>Details</summary>
Motivation: 激励大学队伍通过竞争性比赛，提高自动化红队和AI安全助手在安全性方面的技术，推动AI系统的安全发展。

Method: 通过比赛引入多回合对话的攻防测试和标注数据迭代改进，并开发定制的编码专家模型和评估工具。

Result: 参赛队伍开发了推理安全对齐、模型防护、多轮破防等技术；亚马逊团队提供了基线模型和评估机制。

Conclusion: 比赛和技术开发的协作形式提升了AI在软件开发场景中的安全性，设立了AI安全的新标准。

Abstract: AI systems for software development are rapidly gaining prominence, yet
significant challenges remain in ensuring their safety. To address this, Amazon
launched the Trusted AI track of the Amazon Nova AI Challenge, a global
competition among 10 university teams to drive advances in secure AI. In the
challenge, five teams focus on developing automated red teaming bots, while the
other five create safe AI assistants. This challenge provides teams with a
unique platform to evaluate automated red-teaming and safety alignment methods
through head-to-head adversarial tournaments where red teams have multi-turn
conversations with the competing AI coding assistants to test their safety
alignment. Along with this, the challenge provides teams with a feed of high
quality annotated data to fuel iterative improvement. Throughout the challenge,
teams developed state-of-the-art techniques, introducing novel approaches in
reasoning-based safety alignment, robust model guardrails, multi-turn
jail-breaking, and efficient probing of large language models (LLMs). To
support these efforts, the Amazon Nova AI Challenge team made substantial
scientific and engineering investments, including building a custom baseline
coding specialist model for the challenge from scratch, developing a tournament
orchestration service, and creating an evaluation harness. This paper outlines
the advancements made by university teams and the Amazon Nova AI Challenge team
in addressing the safety challenges of AI for software development,
highlighting this collaborative effort to raise the bar for AI safety.

</details>


### [184] [MCP-Orchestrated Multi-Agent System for Automated Disinformation Detection](https://arxiv.org/abs/2508.10143)
*Alexandru-Andrei Avram,Adrian Groza,Alexandru Lecu*

Main category: cs.AI

TL;DR: 本研究提出了一种多代理系统，利用关系抽取技术检测新闻文章中的虚假信息，并采用多种方法结合提高准确率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 应对在数字平台上虚假信息传播对信息完整性的重大挑战。

Method: 构建由四个代理组成的 Agentic AI 系统，包括机器学习算法、维基百科知识检测代理、一致性检测代理和网站数据分析代理，通过 Model Context Protocol 协调组件。

Result: 系统在准确率（95.3%）和 F1 分数（0.964）上表现优异，显著胜过单一代理和传统方法。

Conclusion: 本系统通过模块化架构提升了准确率和可扩展性，且便于理解决策过程，表明在虚假信息检测领域具有潜在的应用价值。

Abstract: The large spread of disinformation across digital platforms creates
significant challenges to information integrity. This paper presents a
multi-agent system that uses relation extraction to detect disinformation in
news articles, focusing on titles and short text snippets. The proposed Agentic
AI system combines four agents: (i) a machine learning agent (logistic
regression), (ii) a Wikipedia knowledge check agent (which relies on named
entity recognition), (iii) a coherence detection agent (using LLM prompt
engineering), and (iv) a web-scraped data analyzer that extracts relational
triplets for fact checking. The system is orchestrated via the Model Context
Protocol (MCP), offering shared context and live learning across components.
Results demonstrate that the multi-agent ensemble achieves 95.3% accuracy with
an F1 score of 0.964, significantly outperforming individual agents and
traditional approaches. The weighted aggregation method, mathematically derived
from individual agent misclassification rates, proves superior to algorithmic
threshold optimization. The modular architecture makes the system easily
scalable, while also maintaining details of the decision processes.

</details>


### [185] [Agentic AI Frameworks: Architectures, Protocols, and Design Challenges](https://arxiv.org/abs/2508.10146)
*Hana Derouiche,Zaki Brahmi,Haithem Mazeni*

Main category: cs.AI

TL;DR: 本文回顾并比较了多种具备目标导向、自主推理和协作能力的Agentic AI框架，分析了其架构、通信机制及局限性，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探讨Agentic AI代表性框架的特点及其在服务导向计算中的对齐方式。

Method: 系统评估多种框架与协议，提出分类体系并深入分析通信协议。

Result: 建立了Agentic AI系统的基础分类法，指出了局限与挑战，并提出未来研究方向以提升规模化、鲁棒性和互操作性。

Conclusion: 本文为研究者和从业人员提供了推动下一代自治AI系统发展的全面参考。

Abstract: The emergence of Large Language Models (LLMs) has ushered in a transformative
paradigm in artificial intelligence, Agentic AI, where intelligent agents
exhibit goal-directed autonomy, contextual reasoning, and dynamic multi-agent
coordination. This paper provides a systematic review and comparative analysis
of leading Agentic AI frameworks, including CrewAI, LangGraph, AutoGen,
Semantic Kernel, Agno, Google ADK, and MetaGPT, evaluating their architectural
principles, communication mechanisms, memory management, safety guardrails, and
alignment with service-oriented computing paradigms. Furthermore, we identify
key limitations, emerging trends, and open challenges in the field. To address
the issue of agent communication, we conduct an in-depth analysis of protocols
such as the Contract Net Protocol (CNP), Agent-to-Agent (A2A), Agent Network
Protocol (ANP), and Agora. Our findings not only establish a foundational
taxonomy for Agentic AI systems but also propose future research directions to
enhance scalability, robustness, and interoperability. This work serves as a
comprehensive reference for researchers and practitioners working to advance
the next generation of autonomous AI systems.

</details>


### [186] [Improving and Evaluating Open Deep Research Agents](https://arxiv.org/abs/2508.10152)
*Doaa Allabadi,Kyle Bradbury,Jordan M. Malof*

Main category: cs.AI

TL;DR: 本文以深度研究代理（DRA）为研究对象，评估现有开源与闭源系统在新提出的BC-Small数据集上的表现，并改进开源ODR模型，使其成功率提升至10%。


<details>
  <summary>Details</summary>
Motivation: 近年来深度研究代理展示了卓越的公共测试基准表现，但大多研究基于闭源系统，缺乏开源进展对比。

Method: 基于BrowserComp提出简化基准BC-Small，并在其中对比开源ODR与Anthropic及谷歌的闭源系统表现，同时引入三项改进生成ODR+模型。

Result: 三种DRA系统在BC-Small的基础版本中准确率均为0%，经过三项策略改善的ODR+模型刷新纪录达到10%成功率。

Conclusion: 改进后的开源模型ODR+在BC-Small上的表现优于现有所有系统，体现了模型开发和策略优化的重要性。

Abstract: We focus here on Deep Research Agents (DRAs), which are systems that can take
a natural language prompt from a user, and then autonomously search for, and
utilize, internet-based content to address the prompt. Recent DRAs have
demonstrated impressive capabilities on public benchmarks however, recent
research largely involves proprietary closed-source systems. At the time of
this work, we only found one open-source DRA, termed Open Deep Research (ODR).
In this work we adapt the challenging recent BrowseComp benchmark to compare
ODR to existing proprietary systems. We propose BrowseComp-Small (BC-Small),
comprising a subset of BrowseComp, as a more computationally-tractable DRA
benchmark for academic labs. We benchmark ODR and two other proprietary systems
on BC-Small: one system from Anthropic and one system from Google. We find that
all three systems achieve 0% accuracy on the test set of 60 questions. We
introduce three strategic improvements to ODR, resulting in the ODR+ model,
which achieves a state-of-the-art 10% success rate on BC-Small among both
closed-source and open-source systems. We report ablation studies indicating
that all three of our improvements contributed to the success of ODR+.

</details>


### [187] [Pruning Long Chain-of-Thought of Large Reasoning Models via Small-Scale Preference Optimization](https://arxiv.org/abs/2508.10164)
*Bin Hong,Jiayu Liu,Zhenya Huang,Kai Zhang,Mengdi Zhang*

Main category: cs.AI

TL;DR: 研究旨在通过提出一种新的长度控制偏好优化方法（LCPO）来降低大规模推理模型生成内容的长度，同时确保推理效果。


<details>
  <summary>Details</summary>
Motivation: 现阶段大规模推理模型在复杂任务上的长推理路径虽然效果卓越，但却带来了计算成本过高及“过度推理”等问题。本文希望在推理效果和效率之间找到平衡。

Method: 本文通过分析生成路径分布并使用困难估计过滤生成路径，同时在Bradley-Terry损失框架下分析不同偏好优化方法的目标收敛行为，提出了一种名为LCPO的优化方法，直接平衡与负对数似然损失相关的隐式奖励。

Result: 实验结果表明，该方法在确保推理性能的情况下，将平均输出长度减少了50%以上，并适用于多个基准测试。

Conclusion: LCPO方法显示了其在兼顾推理效率与性能方面的潜力，为大规模推理模型的计算效率改进提出了有效解决方案。

Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated strong
performance on complex tasks through long Chain-of-Thought (CoT) reasoning.
However, their lengthy outputs increase computational costs and may lead to
overthinking, raising challenges in balancing reasoning effectiveness and
efficiency. Current methods for efficient reasoning often compromise reasoning
quality or require extensive resources. This paper investigates efficient
methods to reduce the generation length of LRMs. We analyze generation path
distributions and filter generated trajectories through difficulty estimation.
Subsequently, we analyze the convergence behaviors of the objectives of various
preference optimization methods under a Bradley-Terry loss based framework.
Based on the analysis, we propose Length Controlled Preference Optimization
(LCPO) that directly balances the implicit reward related to NLL loss. LCPO can
effectively learn length preference with limited data and training. Extensive
experiments demonstrate that our approach significantly reduces the average
output length by over 50\% across multiple benchmarks while maintaining the
reasoning performance. Our work highlights the potential for computationally
efficient approaches in guiding LRMs toward efficient reasoning.

</details>


### [188] [KompeteAI: Accelerated Autonomous Multi-Agent System for End-to-End Pipeline Generation for Machine Learning Problems](https://arxiv.org/abs/2508.10177)
*Stepan Kulibaba,Artem Dzhalilov,Roman Pakhomov,Oleg Svidchenko,Alexander Gasnikov,Aleksei Shpilman*

Main category: cs.AI

TL;DR: KompeteAI是一个新型的AutoML框架，通过动态解决方案空间探索提升性能，并引入RAG扩展假设空间，同时通过预测评分模型和加速调试方法缓解执行瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基础的AutoML系统存在探索策略受限和执行瓶颈问题，导致性能局限。

Method: KompeteAI引入动态解决方案空间探索，通过合并顶级候选和使用RAG整合外部资源扩展假设空间，并通过预测模型和加速机制提高调试效率。

Result: KompeteAI在主要AutoML基准MLE-Bench中平均提升3%，且解决方案评估速度加快了6.9倍。

Conclusion: KompeteAI显著改进了AutoML方法的性能和效率，并在提出的Kompete-bench基准上达到了最新的表现。

Abstract: Recent Large Language Model (LLM)-based AutoML systems demonstrate impressive
capabilities but face significant limitations such as constrained exploration
strategies and a severe execution bottleneck. Exploration is hindered by
one-shot methods lacking diversity and Monte Carlo Tree Search (MCTS)
approaches that fail to recombine strong partial solutions. The execution
bottleneck arises from lengthy code validation cycles that stifle iterative
refinement. To overcome these challenges, we introduce KompeteAI, a novel
AutoML framework with dynamic solution space exploration. Unlike previous MCTS
methods that treat ideas in isolation, KompeteAI introduces a merging stage
that composes top candidates. We further expand the hypothesis space by
integrating Retrieval-Augmented Generation (RAG), sourcing ideas from Kaggle
notebooks and arXiv papers to incorporate real-world strategies. KompeteAI also
addresses the execution bottleneck via a predictive scoring model and an
accelerated debugging method, assessing solution potential using early stage
metrics to avoid costly full-code execution. This approach accelerates pipeline
evaluation 6.9 times. KompeteAI outperforms leading methods (e.g., RD-agent,
AIDE, and Ml-Master) by an average of 3\% on the primary AutoML benchmark,
MLE-Bench. Additionally, we propose Kompete-bench to address limitations in
MLE-Bench, where KompeteAI also achieves state-of-the-art results

</details>


### [189] [Extending the Entropic Potential of Events for Uncertainty Quantification and Decision-Making in Artificial Intelligence](https://arxiv.org/abs/2508.10241)
*Mark Zilberman*

Main category: cs.AI

TL;DR: 该论文提出了事件熵势的概念，探讨其在人工智能中对不确定性量化、决策及解释性的增强作用，并尝试适应于AI场景。


<details>
  <summary>Details</summary>
Motivation: 从物理学中的熵势概念出发，探索如何将其引入AI领域以更好地理解事件对系统未来不确定性的影响。

Method: 通过强调条件期望，对熵势进行AI调整定义，并提出一种事件驱动的测量方式，结合强化学习、贝叶斯推断等案例进行分析。

Result: 研究展示了熵势概念在政策评估、本质奖励设计、解释性AI以及异常检测中的潜力，表明其有助于统一与强化智能系统的不确定性建模。

Conclusion: 熵势框架为AI中的不确定性管理提供了一种理论扎实、可解释且通用的方法，将热力学、信息论和机器学习的原理有效融合。

Abstract: This work demonstrates how the concept of the entropic potential of events --
a parameter quantifying the influence of discrete events on the expected future
entropy of a system -- can enhance uncertainty quantification, decision-making,
and interpretability in artificial intelligence (AI). Building on its original
formulation in physics, the framework is adapted for AI by introducing an
event-centric measure that captures how actions, observations, or other
discrete occurrences impact uncertainty at future time horizons. Both the
original and AI-adjusted definitions of entropic potential are formalized, with
the latter emphasizing conditional expectations to account for counterfactual
scenarios. Applications are explored in policy evaluation, intrinsic reward
design, explainable AI, and anomaly detection, highlighting the metric's
potential to unify and strengthen uncertainty modeling in intelligent systems.
Conceptual examples illustrate its use in reinforcement learning, Bayesian
inference, and anomaly detection, while practical considerations for
computation in complex AI models are discussed. The entropic potential
framework offers a theoretically grounded, interpretable, and versatile
approach to managing uncertainty in AI, bridging principles from
thermodynamics, information theory, and machine learning.

</details>


### [190] [Why Cannot Large Language Models Ever Make True Correct Reasoning?](https://arxiv.org/abs/2508.10265)
*Jingde Cheng*

Main category: cs.AI

TL;DR: 本文认为基于大语言模型（LLMs）的生成式AI（如ChatGPT）并不具备真正的理解能力和推理能力，这只是用户对其性能的错觉。


<details>
  <summary>Details</summary>
Motivation: 许多人对LLMs表现出的理解和推理能力产生误解，将其视为“真正的能力”。作者希望通过本文澄清这些误解。

Method: 通过阐述LLMs工作原理的本质限制，分析其为什么无法获得真正的理解能力与正确推理能力。

Result: 指出LLMs的所谓理解和推理能力源自外界对其行为的误判，而不是模型本身具有这些能力。

Conclusion: 基于其工作原理的本质限制，LLMs将始终无法实现真正的理解和正确的推理能力。

Abstract: Recently, with the application progress of AIGC tools based on large language
models (LLMs), led by ChatGPT, many AI experts and more non-professionals are
trumpeting the "understanding ability" and "reasoning ability" of the LLMs. The
present author considers that the so-called "understanding ability" and
"reasoning ability" of LLMs are just illusions of those people who with vague
concepts. In fact, the LLMs can never have the true understanding ability and
true reasoning ability. This paper intents to explain that, because the
essential limitations of their working principle, the LLMs can never have the
ability of true correct reasoning.

</details>


### [191] [Promoting Efficient Reasoning with Verifiable Stepwise Reward](https://arxiv.org/abs/2508.10293)
*Chuhuai Yue,Chengqi Dong,Yinan Gao,Hang He,Jiajun Chai,Guojun Yin,Wei Lin*

Main category: cs.AI

TL;DR: 该论文提出了一种名为VSRM的奖励机制，通过在推理轨迹中的中间状态奖励或惩罚模型，以缓解大规模推理模型中的过度思考问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模推理模型虽然在复杂推理任务中表现出色，但常因过度思考而浪费资源。同时，现有的高效推理方法依赖于任务精确评估，限制了其灵活性和可靠性。作者希望解决这些问题。

Method: 作者提出了一种基于规则的可验证逐步奖励机制（VSRM），通过对推理中间状态进行性能考量分配奖励，从而鼓励有效步骤并惩罚无效步骤。

Result: 在标准数学推理数据集（如AIME24和AIME25）上的实验表明，VSRM能显著减少推理输出长度，同时保留原始推理性能，达到了效率与准确性的最佳平衡。

Conclusion: 提出的VSRM方法能有效抑制无效推理步骤，鼓励有效推理，深入缓解过度思考问题，并提供了一种新的优化大规模推理模型效率的思路。

Abstract: Large reasoning models (LRMs) have recently achieved significant progress in
complex reasoning tasks, aided by reinforcement learning with verifiable
rewards. However, LRMs often suffer from overthinking, expending excessive
computation on simple problems and reducing efficiency. Existing efficient
reasoning methods typically require accurate task assessment to preset token
budgets or select reasoning modes, which limits their flexibility and
reliability. In this work, we revisit the essence of overthinking and identify
that encouraging effective steps while penalizing ineffective ones is key to
its solution. To this end, we propose a novel rule-based verifiable stepwise
reward mechanism (VSRM), which assigns rewards based on the performance of
intermediate states in the reasoning trajectory. This approach is intuitive and
naturally fits the step-by-step nature of reasoning tasks. We conduct extensive
experiments on standard mathematical reasoning benchmarks, including AIME24 and
AIME25, by integrating VSRM with PPO and Reinforce++. Results show that our
method achieves substantial output length reduction while maintaining original
reasoning performance, striking an optimal balance between efficiency and
accuracy. Further analysis of overthinking frequency and pass@k score before
and after training demonstrates that our approach in deed effectively
suppresses ineffective steps and encourages effective reasoning, fundamentally
alleviating the overthinking problem. All code will be released upon
acceptance.

</details>


### [192] [A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering](https://arxiv.org/abs/2508.10337)
*Chenliang Zhang,Lin Wang,Yuanyuan Lu,Yusheng Qi,Kexin Wang,Peixu Hou,Wenshi Chen*

Main category: cs.AI

TL;DR: 本文讨论了Dianping-Trust-Safety团队提出的解决META CRAG-MM挑战的方法，通过多模态多轮问答及检索增强生成系统取得了Task 1第一名和Task 3第三名的优异成绩。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决多模态多轮问答问题，重点提高理解复杂查询与多源信息聚合的能力。

Method: 在Task 1中，使用基于视觉大语言模型的技术，通过GPT-4.1知识蒸馏进行监督微调，并采用课程学习结合强化学习优化模型表现；在Task 2和Task 3中，结合网络搜索API扩展外部知识应用。

Result: 方法在META CRAG-MM挑战中表现出色，Task 1以52.38%的显著优势赢得第一名，Task 3获得第三名。

Conclusion: 课程学习与强化学习结合的训练策略在多模态问答任务中有效，不仅提升了回答的准确性，还减少了幻觉生成问题。

Abstract: This paper describes the solutions of the Dianping-Trust-Safety team for the
META CRAG-MM challenge. The challenge requires building a comprehensive
retrieval-augmented generation system capable for multi-modal multi-turn
question answering. The competition consists of three tasks: (1) answering
questions using structured data retrieved from an image-based mock knowledge
graph, (2) synthesizing information from both knowledge graphs and web search
results, and (3) handling multi-turn conversations that require context
understanding and information aggregation from multiple sources. For Task 1,
our solution is based on the vision large language model, enhanced by
supervised fine-tuning with knowledge distilled from GPT-4.1. We further
applied curriculum learning strategies to guide reinforcement learning,
resulting in improved answer accuracy and reduced hallucination. For Task 2 and
Task 3, we additionally leveraged web search APIs to incorporate external
knowledge, enabling the system to better handle complex queries and multi-turn
conversations. Our approach achieved 1st place in Task 1 with a significant
lead of 52.38\%, and 3rd place in Task 3, demonstrating the effectiveness of
the integration of curriculum learning with reinforcement learning in our
training pipeline.

</details>


### [193] [Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach](https://arxiv.org/abs/2508.10340)
*Chak Lam Shek,Guangyao Shi,Pratap Tokekar*

Main category: cs.AI

TL;DR: 提出了两种改进HATRPO的方法，即HATRPO-W和HATRPO-G，通过动态分配KL散度阈值以提高多智能体强化学习（MARL）的性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的HATRPO方法对所有智能体分配相同的KL散度阈值，导致在异质场景中的学习速度慢并易陷入局部最优。需要更灵活的阈值分配策略来解决这一问题。

Method: 提出HATRPO-W和HATRPO-G两种方法：HATRPO-W基于KKT优化全局KL约束下的阈值分配；HATRPO-G基于改进与发散比优先对智能体进行分配。

Result: 实验结果表明，HATRPO-W和HATRPO-G显著提升了HATRPO的性能，在MARL基准测试中实现了更快的收敛和超过22.5%的最终回报提升。其中HATRPO-W的学习动态更加稳定，方差更低。

Conclusion: 动态分配KL散度阈值的方法（HATRPO-W和HATRPO-G）在异质智能体场景下具有更高的学习效率和性能提升，有潜力在MARL中得到广泛应用。

Abstract: Multi-agent reinforcement learning (MARL) requires coordinated and stable
policy updates among interacting agents. Heterogeneous-Agent Trust Region
Policy Optimization (HATRPO) enforces per-agent trust region constraints using
Kullback-Leibler (KL) divergence to stabilize training. However, assigning each
agent the same KL threshold can lead to slow and locally optimal updates,
especially in heterogeneous settings. To address this limitation, we propose
two approaches for allocating the KL divergence threshold across agents:
HATRPO-W, a Karush-Kuhn-Tucker-based (KKT-based) method that optimizes
threshold assignment under global KL constraints, and HATRPO-G, a greedy
algorithm that prioritizes agents based on improvement-to-divergence ratio. By
connecting sequential policy optimization with constrained threshold
scheduling, our approach enables more flexible and effective learning in
heterogeneous-agent settings. Experimental results demonstrate that our methods
significantly boost the performance of HATRPO, achieving faster convergence and
higher final rewards across diverse MARL benchmarks. Specifically, HATRPO-W and
HATRPO-G achieve comparable improvements in final performance, each exceeding
22.5%. Notably, HATRPO-W also demonstrates more stable learning dynamics, as
reflected by its lower variance.

</details>


### [194] [What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles](https://arxiv.org/abs/2508.10358)
*Mengtao Zhou,Sifan Wu,Huan Zhang,Qi Sima,Bang Liu*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型（LLMs）在缺乏信息环境中的创造性推理能力，并提出了“乌龟汤”游戏为基础的全新研究框架及相关评估工具。


<details>
  <summary>Details</summary>
Motivation: 现有评估标准未能充分捕捉创造性推理过程中的动态和探索性质，作者希望填补这一研究空白。

Method: 引入了一个基于“乌龟汤”游戏的研究框架，包括首个双语互动基准TurtleSoup-Bench和新型评估用智能体Mosaic-Agent，及多维评估协议。

Result: 实验表明现有LLMs在逻辑一致性、细节完成度和结论一致性等方面存在明显能力限制，且与人类表现存在明显差距。

Conclusion: 本文为未来探索性智能体行为研究奠定了基础，同时揭示了LLMs在创造性推理方面的能力瓶颈及改进方向。

Abstract: We investigate the capacity of Large Language Models (LLMs) for imaginative
reasoning--the proactive construction, testing, and revision of hypotheses in
information-sparse environments. Existing benchmarks, often static or focused
on social deduction, fail to capture the dynamic, exploratory nature of this
reasoning process. To address this gap, we introduce a comprehensive research
framework based on the classic "Turtle Soup" game, integrating a benchmark, an
agent, and an evaluation protocol. We present TurtleSoup-Bench, the first
large-scale, bilingual, interactive benchmark for imaginative reasoning,
comprising 800 turtle soup puzzles sourced from both the Internet and expert
authors. We also propose Mosaic-Agent, a novel agent designed to assess LLMs'
performance in this setting. To evaluate reasoning quality, we develop a
multi-dimensional protocol measuring logical consistency, detail completion,
and conclusion alignment. Experiments with leading LLMs reveal clear capability
limits, common failure patterns, and a significant performance gap compared to
humans. Our work offers new insights into LLMs' imaginative reasoning and
establishes a foundation for future research on exploratory agent behavior.

</details>


### [195] [LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval](https://arxiv.org/abs/2508.10391)
*Yaoze Zhang,Rong Wu,Pinlong Cai,Xiaoman Wang,Guohang Yan,Song Mao,Ding Wang,Botian Shi*

Main category: cs.AI

TL;DR: 本文提出了一种名为LeanRAG的新框架，通过结合知识聚合和检索策略，优化了基于知识图谱的RAG方法，使其在QA任务表现上更优且检索效率提升。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在检索时存在上下文信息不足的问题，基于知识图谱的RAG虽然改进了信息组织，但仍面临高层语义孤立和低效检索的挑战。

Method: 提出LeanRAG框架，包括一种新的语义聚合算法创建完全可导航的语义网络，以及自底向上的结构引导检索策略，从精细实体到语义路径系统化收集证据。

Result: 在四个不同领域QA基准测试上，LeanRAG显著提升了响应质量，同时减少46%的冗余检索。

Conclusion: LeanRAG全面结合了知识聚合与高效检索，减少了信息冗余，提升了知识利用效率，为基于知识图谱的RAG提供了新的优化方向。

Abstract: Retrieval-Augmented Generation (RAG) plays a crucial role in grounding Large
Language Models by leveraging external knowledge, whereas the effectiveness is
often compromised by the retrieval of contextually flawed or incomplete
information. To address this, knowledge graph-based RAG methods have evolved
towards hierarchical structures, organizing knowledge into multi-level
summaries. However, these approaches still suffer from two critical,
unaddressed challenges: high-level conceptual summaries exist as disconnected
``semantic islands'', lacking the explicit relations needed for cross-community
reasoning; and the retrieval process itself remains structurally unaware, often
degenerating into an inefficient flat search that fails to exploit the graph's
rich topology. To overcome these limitations, we introduce LeanRAG, a framework
that features a deeply collaborative design combining knowledge aggregation and
retrieval strategies. LeanRAG first employs a novel semantic aggregation
algorithm that forms entity clusters and constructs new explicit relations
among aggregation-level summaries, creating a fully navigable semantic network.
Then, a bottom-up, structure-guided retrieval strategy anchors queries to the
most relevant fine-grained entities and then systematically traverses the
graph's semantic pathways to gather concise yet contextually comprehensive
evidence sets. The LeanRAG can mitigate the substantial overhead associated
with path retrieval on graphs and minimizes redundant information retrieval.
Extensive experiments on four challenging QA benchmarks with different domains
demonstrate that LeanRAG significantly outperforming existing methods in
response quality while reducing 46\% retrieval redundancy. Code is available
at: https://github.com/RaZzzyz/LeanRAG

</details>


### [196] [HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation](https://arxiv.org/abs/2508.10425)
*Yan Ting Chok,Soyon Park,Seungheun Baek,Hajung Kim,Junhyun Lee,Jaewoo Kang*

Main category: cs.AI

TL;DR: 本文提出了一种名为HiRef的框架，通过结合医疗本体的层次语义和基于EHR数据的关联模式，改进临床用药推荐的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决因EHR数据的稀有医学实体与不完整记录导致模型泛化不足的问题。

Method: 提出Hierarchical Ontology and Network Refinement for Robust Medication Recommendation(HiRef)，结合超曲面嵌入医疗本体与稀疏正则化图对EHR数据进行结构优化。

Result: 在MIMIC-III和MIMIC-IV的实验中表现优异，在新条件和未见代码场景中仍保持高性能。

Conclusion: HiRef通过结合本体语义与数据驱动方法，提高了用药推荐模型的泛化能力与鲁棒性，在临床应用中具有潜力。

Abstract: Medication recommendation is a crucial task for assisting physicians in
making timely decisions from longitudinal patient medical records. However,
real-world EHR data present significant challenges due to the presence of
rarely observed medical entities and incomplete records that may not fully
capture the clinical ground truth. While data-driven models trained on
longitudinal Electronic Health Records often achieve strong empirical
performance, they struggle to generalize under missing or novel conditions,
largely due to their reliance on observed co-occurrence patterns. To address
these issues, we propose Hierarchical Ontology and Network Refinement for
Robust Medication Recommendation (HiRef), a unified framework that combines two
complementary structures: (i) the hierarchical semantics encoded in curated
medical ontologies, and (ii) refined co-occurrence patterns derived from
real-world EHRs. We embed ontology entities in hyperbolic space, which
naturally captures tree-like relationships and enables knowledge transfer
through shared ancestors, thereby improving generalizability to unseen codes.
To further improve robustness, we introduce a prior-guided sparse
regularization scheme that refines the EHR co-occurrence graph by suppressing
spurious edges while preserving clinically meaningful associations. Our model
achieves strong performance on EHR benchmarks (MIMIC-III and MIMIC-IV) and
maintains high accuracy under simulated unseen-code settings. Extensive
experiments with comprehensive ablation studies demonstrate HiRef's resilience
to unseen medical codes, supported by in-depth analyses of the learned
sparsified graph structure and medical code embeddings.

</details>


### [197] [MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance](https://arxiv.org/abs/2508.10429)
*Yi Dong,Yusuke Muraoka,Scott Shi,Yi Zhang*

Main category: cs.AI

TL;DR: 本文介绍了一个名为MM-Food-100K的公开多模态食品数据集,包含10万样本并具可验证来源。


<details>
  <summary>Details</summary>
Motivation: 目标是通过社区众包和AI辅助质控模型构建高质量、多模态的食品数据集，提高食品图像识别与分类能力。

Method: 使用Codatta贡献模型，从8.7万名贡献者处收集数据，附加基于区块链的可追溯机制，并通过微调视觉-语言模型验证其效用。

Result: 微调后模型在标准指标上展现出优于未调优基线的性能提升，尤其在基于图像的营养预测任务中效果显著。

Conclusion: 公开的MM-Food-100K数据集为多模态食品智能研究提供了高质量资源，同时预留部分数据进行潜在商业化探索。

Abstract: We present MM-Food-100K, a public 100,000-sample multimodal food intelligence
dataset with verifiable provenance. It is a curated approximately 10% open
subset of an original 1.2 million, quality-accepted corpus of food images
annotated for a wide range of information (such as dish name, region of
creation). The corpus was collected over six weeks from over 87,000
contributors using the Codatta contribution model, which combines community
sourcing with configurable AI-assisted quality checks; each submission is
linked to a wallet address in a secure off-chain ledger for traceability, with
a full on-chain protocol on the roadmap. We describe the schema, pipeline, and
QA, and validate utility by fine-tuning large vision-language models (ChatGPT
5, ChatGPT OSS, Qwen-Max) on image-based nutrition prediction. Fine-tuning
yields consistent gains over out-of-box baselines across standard metrics; we
report results primarily on the MM-Food-100K subset. We release MM-Food-100K
for publicly free access and retain approximately 90% for potential commercial
access with revenue sharing to contributors.

</details>


### [198] [We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning](https://arxiv.org/abs/2508.10433)
*Runqi Qiao,Qiuna Tan,Peiqing Yang,Yanzi Wang,Xiaowan Wang,Enhui Wan,Sitong Zhou,Guanting Dong,Yuchen Zeng,Yida Xu,Jie Wang,Chong Sun,Chen Li,Honggang Zhang*

Main category: cs.AI

TL;DR: 本文提出We-Math 2.0，通过结构化数学知识系统、基于模型的数据空间建模以及强化学习范式提升多模态大语言模型的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在复杂数学推理上表现不佳，而现有研究忽略了知识驱动设计和模型数据建模的问题。

Method: 本文通过构建五级层次的数学知识系统（MathBook）、设计具有灵活扩展性和难度进阶的数据集（MathBook-Standard和Pro）、提出两阶段强化学习框架（Cold-Start Fine-tuning和Progressive Alignment RL）、以及开发新的评估基准（MathBookEval）来提升模型的数学推理能力。

Result: 实验结果表明MathBook-RL在四个广泛使用的基准上表现优异，并在新的评估基准上也取得了较好成绩，展示了在数学推理中的潜在泛化能力。

Conclusion: We-Math 2.0有效提升了多模态大语言模型的数学推理能力，具有广泛的应用潜力和研究价值。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across various tasks, but still struggle with complex mathematical
reasoning. Existing research primarily focuses on dataset construction and
method optimization, often overlooking two critical aspects: comprehensive
knowledge-driven design and model-centric data space modeling. In this paper,
we introduce We-Math 2.0, a unified system that integrates a structured
mathematical knowledge system, model-centric data space modeling, and a
reinforcement learning (RL)-based training paradigm to comprehensively enhance
the mathematical reasoning abilities of MLLMs. The key contributions of We-Math
2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level
hierarchical system encompassing 491 knowledge points and 1,819 fundamental
principles. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a
dataset that ensures broad conceptual coverage and flexibility through dual
expansion. Additionally, we define a three-dimensional difficulty space and
generate 7 progressive variants per problem to build MathBook-Pro, a
challenging dataset for robust training. (3) MathBook-RL: We propose a
two-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the
model with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive
Alignment RL, leveraging average-reward learning and dynamic data scheduling to
achieve progressive alignment across difficulty levels. (4) MathBookEval: We
introduce a comprehensive benchmark covering all 491 knowledge points with
diverse reasoning step distributions. Experimental results show that
MathBook-RL performs competitively with existing baselines on four widely-used
benchmarks and achieves strong results on MathBookEval, suggesting promising
generalization in mathematical reasoning.

</details>


### [199] [FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over Scholarly Knowledge Graphs](https://arxiv.org/abs/2508.10467)
*Xueli Pan,Victor de Boer,Jacco van Ossenbruggen*

Main category: cs.AI

TL;DR: 本论文提出了一个称为FIRESPARQL的模块化框架，通过微调LLM，结合可选的检索增强生成（RAG）及SPARQL查询修正层，以解决学术知识图谱（SKG）中基于LLM生成SPARQL查询的误差问题。实验表明微调方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: 由于学术知识图谱的复杂性和语言模型对其内容和结构的缺乏训练，在学术知识图谱中的自然语言问答依然是一个具有挑战性的任务，尤其是在生成SPARQL查询时容易产生结构性和语义性错误。

Method: 提出了FIRESPARQL框架，利用微调的大语言模型（LLM）作为核心组件，结合可选的RAG和SPARQL查询修正层，解决生成查询的两类主要误差。

Result: 实验在SciQA基准上进行，并采用多种配置对框架进行评估。通过BLEU、ROUGE、RelaxedEM等指标评估，与基准方法比较，微调配置取得最高性能(ROUGE-L值达0.90, RelaxedEM值达0.85)。

Conclusion: FIRESPARQL框架显著提高了学术知识图谱问答任务中SPARQL查询生成的准确性，特别是通过微调进一步提升了结果的质量。

Abstract: Question answering over Scholarly Knowledge Graphs (SKGs) remains a
challenging task due to the complexity of scholarly content and the intricate
structure of these graphs. Large Language Model (LLM) approaches could be used
to translate natural language questions (NLQs) into SPARQL queries; however,
these LLM-based approaches struggle with SPARQL query generation due to limited
exposure to SKG-specific content and the underlying schema. We identified two
main types of errors in the LLM-generated SPARQL queries: (i) structural
inconsistencies, such as missing or redundant triples in the queries, and (ii)
semantic inaccuracies, where incorrect entities or properties are shown in the
queries despite a correct query structure. To address these issues, we propose
FIRESPARQL, a modular framework that supports fine-tuned LLMs as a core
component, with optional context provided via retrieval-augmented generation
(RAG) and a SPARQL query correction layer. We evaluate the framework on the
SciQA Benchmark using various configurations (zero-shot, zero-shot with RAG,
one-shot, fine-tuning, and fine-tuning with RAG) and compare the performance
with baseline and state-of-the-art approaches. We measure query accuracy using
BLEU and ROUGE metrics, and query result accuracy using relaxed exact
match(RelaxedEM), with respect to the gold standards containing the NLQs,
SPARQL queries, and the results of the queries. Experimental results
demonstrate that fine-tuning achieves the highest overall performance, reaching
0.90 ROUGE-L for query accuracy and 0.85 RelaxedEM for result accuracy on the
test set.

</details>


### [200] [SEQ-GPT: LLM-assisted Spatial Query via Example](https://arxiv.org/abs/2508.10486)
*Ivan Khai Ze Lim,Ningyi Liao,Yiming Yang,Gerald Wei Yong Yip,Siqiang Luo*

Main category: cs.AI

TL;DR: 该研究提出SEQ-GPT系统，利用大语言模型(LLMs)扩展空间查询服务，支持用户通过自然语言表述进行多位置联合搜索，并支持交互式反馈优化查询。


<details>
  <summary>Details</summary>
Motivation: 当前的地图及空间服务在处理复杂任务（如同时搜索多个位置）时体验有限，研究希望通过LLMs提升用户体验和交互能力，解决复杂空间查询问题。

Method: 引入SEQ-GPT系统，结合语言模型的能力，进行自然语言驱动的空间查询，同时通过对话合成和多模态协作实现语言与空间数据的对齐，并动态调整搜索结果。

Result: SEQ-GPT实现基于实际数据的端到端空间多位置查询，提供了更广泛的应用场景和交互能力。

Conclusion: SEQ-GPT扩展了空间搜索的可能性，为基于自然语言的复杂查询任务提供了可靠的解决方案，进一步提升用户与空间服务的交互方式。

Abstract: Contemporary spatial services such as online maps predominantly rely on user
queries for location searches. However, the user experience is limited when
performing complex tasks, such as searching for a group of locations
simultaneously. In this study, we examine the extended scenario known as
Spatial Exemplar Query (SEQ), where multiple relevant locations are jointly
searched based on user-specified examples. We introduce SEQ-GPT, a spatial
query system powered by Large Language Models (LLMs) towards more versatile SEQ
search using natural language. The language capabilities of LLMs enable unique
interactive operations in the SEQ process, including asking users to clarify
query details and dynamically adjusting the search based on user feedback. We
also propose a tailored LLM adaptation pipeline that aligns natural language
with structured spatial data and queries through dialogue synthesis and
multi-model cooperation. SEQ-GPT offers an end-to-end demonstration for
broadening spatial search with realistic data and application scenarios.

</details>


### [201] [Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model](https://arxiv.org/abs/2508.10492)
*Shicheng Xu,Xin Huang,Zihao Wei,Liang Pang,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: 本文提出了一种全新AI诊断模式，DxDirector-7B，通过赋予其深度思考能力，以完成从模糊主诉到诊断的全流程，取代传统AI仅为医生助手的角色。


<details>
  <summary>Details</summary>
Motivation: 目前AI在临床诊断中的作用受限于回答医生指定问题，无法处理从模糊主诉到完整诊断的全流程，严重依赖医生，难以最大化减少医生工作量并提升效率。

Method: 提出DxDirector-7B，一种具备深度思考能力的LLM，通过重新定义AI为主导者、医生为助手的定位，推动全流程诊断，并针对误诊建立责任框架。

Result: DxDirector-7B在稀有和复杂病例的全流程诊断中表现出显著优于现有医疗LLM和通用LLM的准确率，同时大幅减少医生工作量。

Conclusion: DxDirector-7B开创了由AI主导诊断的新模式，显著减少医生负担，并作为高效且准确的医疗解决方案展现出取代专家的潜力。

Abstract: Full-process clinical diagnosis in the real world encompasses the entire
diagnostic workflow that begins with only an ambiguous chief complaint. While
artificial intelligence (AI), particularly large language models (LLMs), is
transforming clinical diagnosis, its role remains largely as an assistant to
physicians. This AI-assisted working pattern makes AI can only answer specific
medical questions at certain parts within the diagnostic process, but lack the
ability to drive the entire diagnostic process starting from an ambiguous
complaint, which still relies heavily on human physicians. This gap limits AI's
ability to fully reduce physicians' workload and enhance diagnostic efficiency.
To address this, we propose a paradigm shift that reverses the relationship
between physicians and AI: repositioning AI as the primary director, with
physicians serving as its assistants. So we present DxDirector-7B, an LLM
endowed with advanced deep thinking capabilities, enabling it to drive the
full-process diagnosis with minimal physician involvement. Furthermore,
DxDirector-7B establishes a robust accountability framework for misdiagnoses,
delineating responsibility between AI and human physicians. In evaluations
across rare, complex, and real-world cases under full-process diagnosis
setting, DxDirector-7B not only achieves significant superior diagnostic
accuracy but also substantially reduces physician workload than
state-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained
analyses across multiple clinical departments and tasks validate its efficacy,
with expert evaluations indicating its potential to serve as a viable
substitute for medical specialists. These findings mark a new era where AI,
traditionally a physicians' assistant, now drives the entire diagnostic process
to drastically reduce physicians' workload, indicating an efficient and
accurate diagnostic solution.

</details>


### [202] [PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning](https://arxiv.org/abs/2508.10501)
*Yushi Feng,Junye Du,Yingying Hong,Qifan Wang,Lequan Yu*

Main category: cs.AI

TL;DR: PASS是一个多模态框架，针对胸部X光推理任务，优化工具整合和计算效率，同时通过概率注释路径提升AI安全性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强的智能系统存在黑盒决策引发的信任和安全问题、多模态整合能力弱以及刚性高的代理管道，特别在医疗领域面临挑战。

Method: 提出PASS（概率代理超级网络采样），通过学习任务条件分布，自适应抽样代理工作流，结合专家知识初始化、对比路径排序及成本感知强化学习优化。

Result: 在多种基准测试中，PASS在准确率、AUC等多项指标上优于强基线模型，同时平衡计算成本。

Conclusion: PASS推动了可解释、自适应和多模态的医学智能系统新范式，对提升医疗AI安全性和效率具有重要意义。

Abstract: Existing tool-augmented agentic systems are limited in the real world by (i)
black-box reasoning steps that undermine trust of decision-making and pose
safety risks, (ii) poor multimodal integration, which is inherently critical
for healthcare tasks, and (iii) rigid and computationally inefficient agentic
pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the
first multimodal framework to address these challenges in the context of Chest
X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a
multi-tool graph, yielding decision paths annotated with interpretable
probabilities. Given the complex CXR reasoning task with multimodal medical
data, PASS leverages its learned task-conditioned distribution over the agentic
supernet. Thus, it adaptively selects the most suitable tool at each supernet
layer, offering probability-annotated trajectories for post-hoc audits and
directly enhancing medical AI safety. PASS also continuously compresses salient
findings into an evolving personalized memory, while dynamically deciding
whether to deepen its reasoning path or invoke an early exit for efficiency. To
optimize a Pareto frontier balancing performance and cost, we design a novel
three-stage training procedure, including expert knowledge warm-up, contrastive
path-ranking, and cost-aware reinforcement learning. To facilitate rigorous
evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step,
safety-critical, free-form CXR reasoning. Experiments across various benchmarks
validate that PASS significantly outperforms strong baselines in multiple
metrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs,
pushing a new paradigm shift towards interpretable, adaptive, and multimodal
medical agentic systems.

</details>


### [203] [Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment](https://arxiv.org/abs/2508.10530)
*Zetian Sun,Dongfang Li,Baotian Hu*

Main category: cs.AI

TL;DR: 研究提出了语言模型的对齐流程由两个阶段组成：偏好注入阶段和偏好微调阶段，并验证了对齐数据来源对效果的影响。


<details>
  <summary>Details</summary>
Motivation: 为改善语言模型的对齐效果，探索不同数据来源对模型性能的影响。

Method: 通过理论与实验分析，研究不同对齐阶段的特性，并提出了用于区分这两个阶段边界的算法。

Result: 发现了静态数据和动态生成数据在对齐过程中表现效果的显著差异，并验证了提出的对齐阶段假设在多种模型和对齐方法中的通用性。

Conclusion: 对齐阶段假设及其边界测量方法能为改进语言模型对齐提供指导，且不同阶段需选择适宜的数据源以优化效果。

Abstract: The alignment of language models (LMs) with human preferences is critical for
building reliable AI systems. The problem is typically framed as optimizing an
LM policy to maximize the expected reward that reflects human preferences.
Recently, Direct Preference Optimization (DPO) was proposed as a LM alignment
method that directly optimize the policy from static preference data, and
further improved by incorporating on-policy sampling (i.e., preference
candidates generated during the training loop) for better LM alignment.
However, we show on-policy data is not always optimal, with systematic
effectiveness difference emerging between static and on-policy preference
candidates. For example, on-policy data can result in a 3$\times$ effectiveness
compared with static data for Llama-3, and a 0.4$\times$ effectiveness for
Zephyr. To explain the phenomenon, we propose the alignment stage assumption,
which divides the alignment process into two distinct stages: the preference
injection stage, which benefits from diverse data, and the preference
fine-tuning stage, which favors high-quality data. Through theoretical and
empirical analysis, we characterize these stages and propose an effective
algorithm to identify the boundaries between them. We perform experiments on 5
models (Llama, Zephyr, Phi-2, Qwen, Pythia) and 2 alignment methods (DPO,
SLiC-HF) to show the generalizability of alignment stage assumption and
boundary measurement.

</details>


### [204] [Improving Value-based Process Verifier via Low-Cost Variance Reduction](https://arxiv.org/abs/2508.10539)
*Zetian Sun,Dongfang Li,Baotian Hu,Min Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种称为ComMCS的方法，通过线性结合当前步骤和后续步骤的MC估计器来减少估计训练注释时的高方差问题，同时保持无偏估计且无需额外LLM推理成本。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在多样任务中表现出色，但其在复杂领域（如数学）中的推理能力仍面临挑战。高推理成本限制了MC采样，导致注释的估计误差，这是改进推理能力的关键问题。

Method: 提出了ComMCS方法，理论上通过结合步骤间的MC估计器，实现方差的显著减少且保持无偏估计。

Result: 在MATH-500和GSM8K基准测试中，ComMCS方法在多个样本测试中表现优越，尤其是在MATH-500的32次采样测试中超越基线方法。

Conclusion: ComMCS通过解决高推理成本带来的高方差问题，为改进LLM推理能力提供了一种高效且经济的解决方案。

Abstract: Large language models (LLMs) have achieved remarkable success in a wide range
of tasks. However, their reasoning capabilities, particularly in complex
domains like mathematics, remain a significant challenge. Value-based process
verifiers, which estimate the probability of a partial reasoning chain leading
to a correct solution, are a promising approach for improving reasoning.
Nevertheless, their effectiveness is often hindered by estimation error in
their training annotations, a consequence of the limited number of Monte Carlo
(MC) samples feasible due to the high cost of LLM inference. In this paper, we
identify that the estimation error primarily arises from high variance rather
than bias, and the MC estimator is a Minimum Variance Unbiased Estimator
(MVUE). To address the problem, we propose the \textsc{Com}pound \textsc{M}onte
\textsc{C}arlo \textsc{S}ampling (ComMCS) method, which constructs an unbiased
estimator by linearly combining the MC estimators from the current and
subsequent steps. Theoretically, we show that our method leads to a predictable
reduction in variance, while maintaining an unbiased estimation without
additional LLM inference cost. We also perform empirical experiments on the
MATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method.
Notably, ComMCS outperforms regression-based optimization method by 2.8 points,
the non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32
sampling experiment.

</details>


### [205] [MSRS: Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models](https://arxiv.org/abs/2508.10599)
*Xinyan Jiang,Lin Zhang,Jiayi Zhang,Qingsong Yang,Guimin Hu,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: 论文提出了一种通过子空间表示微调实现多属性控制的框架（MSRS），有效减少了属性干扰并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有激活操控方法在处理多属性控制时存在干扰与权衡问题。

Method: 提出了多子空间表示操控（MSRS）框架，通过分配正交子空间隔离属性影响，并引入动态权重和子空间组合策略；用动态的token级操控机制对模型进行调整。

Result: 实验表明MSRS显著降低了属性冲突，性能优于现有方法，并能有效推广到多种下游任务。

Conclusion: MSRS框架展示了先进的多属性操控能力，为大语言模型行为调控提供了新思路。

Abstract: Activation steering offers a promising approach to controlling the behavior
of Large Language Models by directly manipulating their internal activations.
However, most existing methods struggle to jointly steer multiple attributes,
often resulting in interference and undesirable trade-offs. To address this
challenge, we propose Multi-Subspace Representation Steering (MSRS), a novel
framework for effective multi-attribute steering via subspace representation
fine-tuning. MSRS reduces inter-attribute interference by allocating orthogonal
subspaces to each attribute, isolating their influence within the model's
representation space. MSRS also incorporates a hybrid subspace composition
strategy: it combines attribute-specific subspaces for unique steering
directions with a shared subspace for common steering directions. A dynamic
weighting function learns to efficiently integrate these components for precise
control. During inference, MSRS introduces a token-level steering mechanism
that dynamically identifies and intervenes on the most semantically relevant
tokens, enabling fine-grained behavioral modulation. Experimental results show
that MSRS significantly reduces attribute conflicts, surpasses existing methods
across a range of attributes, and generalizes effectively to diverse downstream
tasks.

</details>


### [206] [STEP: Stepwise Curriculum Learning for Context-Knowledge Fusion in Conversational Recommendation](https://arxiv.org/abs/2508.10669)
*Zhenye Yang,Jinpeng Chen,Huan Li,Xiongnan Jin,Xuanyang Li,Junwei Zhang,Hongbo Gao,Kaimin Wei,Senzhang Wang*

Main category: cs.AI

TL;DR: 文章介绍了一种名为STEP的新型对话推荐系统，通过结合课程指导的上下文与知识融合及轻量化的任务特定提示调优的方式改进了推荐质量和对话质量。


<details>
  <summary>Details</summary>
Motivation: 现有的对话推荐系统难以捕捉用户偏好和对话语境的深层语义，特别是在有效整合外部知识图的信息以生成推荐方面存在难题。

Method: 提出STEP，通过课程指导的三阶段方法（F-Former）对对话上下文和知识图实体进行深度匹配融合，并通过双提示机制结合对话前缀和推荐前缀，利用冻结的语言模型生成更符合用户意图的响应和推荐。

Result: 实验结果表明，在两个公开数据集上，STEP在推荐精度和对话质量上均优于主流方法。

Conclusion: STEP通过创新的语义匹配及知识融合方式，有效改进了对话推荐系统的性能，为解决复杂语义关系的推荐任务提供了新方案。

Abstract: Conversational recommender systems (CRSs) aim to proactively capture user
preferences through natural language dialogue and recommend high-quality items.
To achieve this, CRS gathers user preferences via a dialog module and builds
user profiles through a recommendation module to generate appropriate
recommendations. However, existing CRS faces challenges in capturing the deep
semantics of user preferences and dialogue context. In particular, the
efficient integration of external knowledge graph (KG) information into
dialogue generation and recommendation remains a pressing issue. Traditional
approaches typically combine KG information directly with dialogue content,
which often struggles with complex semantic relationships, resulting in
recommendations that may not align with user expectations.
  To address these challenges, we introduce STEP, a conversational recommender
centered on pre-trained language models that combines curriculum-guided
context-knowledge fusion with lightweight task-specific prompt tuning. At its
heart, an F-Former progressively aligns the dialogue context with
knowledge-graph entities through a three-stage curriculum, thus resolving
fine-grained semantic mismatches. The fused representation is then injected
into the frozen language model via two minimal yet adaptive prefix prompts: a
conversation prefix that steers response generation toward user intent and a
recommendation prefix that biases item ranking toward knowledge-consistent
candidates. This dual-prompt scheme allows the model to share cross-task
semantics while respecting the distinct objectives of dialogue and
recommendation. Experimental results show that STEP outperforms mainstream
methods in the precision of recommendation and dialogue quality in two public
datasets.

</details>


### [207] [GenOM: Ontology Matching with Description Generation and Large Language Model](https://arxiv.org/abs/2508.10703)
*Yiping Song,Jiaoyan Chen,Renate A. Schmidt*

Main category: cs.AI

TL;DR: 本文提出了一种名为GenOM的大语言模型驱动的本体匹配框架，用于在异构知识源之间实现语义互操作性和集成，尤其是在生物医学领域，该领域涉及复杂概念如疾病和药物。


<details>
  <summary>Details</summary>
Motivation: 针对生物医学领域复杂概念和现有本体匹配方法的不足，提出一种结合大语言模型、语义增强和精确匹配的新方法，提高语义互操作性及集成能力。

Method: 提出GenOM框架：通过生成文本定义丰富本体概念语义表示，使用嵌入模型获取候选对齐项，并结合基于精确匹配的工具以提高精准度。此外，框架中加入了语义增强和少样本提示方式。

Result: 在OAEI Bio-ML测试上，GenOM的表现优于许多基线方法，包括传统本体匹配系统和最近的大语言模型方法；消融研究验证了语义增强与少样本提示的有效性，证明了方法的鲁棒性和适应性。

Conclusion: GenOM框架表现出卓越的本体匹配能力，具有高效性、适应性和准确性，为异构知识源的语义互操作提供了新的解决方案。

Abstract: Ontology matching (OM) plays an essential role in enabling semantic
interoperability and integration across heterogeneous knowledge sources,
particularly in the biomedical domain which contains numerous complex concepts
related to diseases and pharmaceuticals. This paper introduces GenOM, a large
language model (LLM)-based ontology alignment framework, which enriches the
semantic representations of ontology concepts via generating textual
definitions, retrieves alignment candidates with an embedding model, and
incorporates exact matching-based tools to improve precision. Extensive
experiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often
achieve competitive performance, surpassing many baselines including
traditional OM systems and recent LLM-based methods. Further ablation studies
confirm the effectiveness of semantic enrichment and few-shot prompting,
highlighting the framework's robustness and adaptability.

</details>


### [208] [Agentic Design Review System](https://arxiv.org/abs/2508.10745)
*Sayan Nag,K J Joseph,Koustava Goswami,Vlad I Morariu,Balaji Vasan Srinivasan*

Main category: cs.AI

TL;DR: 提出了一种名为AgenticDRS的代理系统，通过多个代理及元代理协作分析设计，评估视觉设计的多维属性。


<details>
  <summary>Details</summary>
Motivation: 现有的设计评估方法缺乏综合性且对多维属性的分析有限，本文旨在解决这一不足。

Method: 引入了AgenticDRS系统，使用基于图匹配的上下文范例选择方法及独特的提示扩展方法，使代理具有设计认知。

Result: 提出DRS-BENCH基准，并通过实验验证了AgenticDRS相对于最新技术的有效性，其中包括与基线对比及消融实验。

Conclusion: 本文为图形设计的多代理协作评估提供了创新方法，引发了对该重要但尚未充分探索方向的研究兴趣。

Abstract: Evaluating graphic designs involves assessing it from multiple facets like
alignment, composition, aesthetics and color choices. Evaluating designs in a
holistic way involves aggregating feedback from individual expert reviewers.
Towards this, we propose an Agentic Design Review System (AgenticDRS), where
multiple agents collaboratively analyze a design, orchestrated by a meta-agent.
A novel in-context exemplar selection approach based on graph matching and a
unique prompt expansion method plays central role towards making each agent
design aware. Towards evaluating this framework, we propose DRS-BENCH
benchmark. Thorough experimental evaluation against state-of-the-art baselines
adapted to the problem setup, backed-up with critical ablation experiments
brings out the efficacy of Agentic-DRS in evaluating graphic designs and
generating actionable feedback. We hope that this work will attract attention
to this pragmatic, yet under-explored research direction.

</details>


### [209] [Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning](https://arxiv.org/abs/2508.10747)
*Sangwoo Jeon,Juchul Shin,Gyeong-Tae Kim,YeonJe Cho,Seongwoo Kim*

Main category: cs.AI

TL;DR: 结合深度强化学习和图神经网络(GNN)在符号规划领域取得了成果，但传统密集图表示面临计算扩展性瓶颈。本文提出了一种稀疏、目标感知的图表示方法，并通过实验在更大规模网格环境中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决现有GNN在规划状态表示上的稀疏性和计算成本问题，尤其解决大型网格环境中的扩展性和信息稀释问题。

Method: 提出一种稀疏、目标感知的GNN表示方法，局部编码相关关系并整合与目标相关的空间特征，用以提升大规模规划任务的学习与推理能力。

Result: 实验结果表明该方法在实现更大网格环境的规划任务时突破了传统密集图的扩展性限制，有效提高了策略泛化能力和任务成功率。

Conclusion: 本文的方法为大规模、现实化的广义规划任务提供了可行方案，在大型环境中的应用具有广阔前景。

Abstract: Generalized planning using deep reinforcement learning (RL) combined with
graph neural networks (GNNs) has shown promising results in various symbolic
planning domains described by PDDL. However, existing approaches typically
represent planning states as fully connected graphs, leading to a combinatorial
explosion in edge information and substantial sparsity as problem scales grow,
especially evident in large grid-based environments. This dense representation
results in diluted node-level information, exponentially increases memory
requirements, and ultimately makes learning infeasible for larger-scale
problems. To address these challenges, we propose a sparse, goal-aware GNN
representation that selectively encodes relevant local relationships and
explicitly integrates spatial features related to the goal. We validate our
approach by designing novel drone mission scenarios based on PDDL within a grid
world, effectively simulating realistic mission execution environments. Our
experimental results demonstrate that our method scales effectively to larger
grid sizes previously infeasible with dense graph representations and
substantially improves policy generalization and success rates. Our findings
provide a practical foundation for addressing realistic, large-scale
generalized planning tasks.

</details>


### [210] [Modeling Human Responses to Multimodal AI Content](https://arxiv.org/abs/2508.10769)
*Zhiqi Shen,Shaojing Fan,Danni Xu,Terence Sim,Mohan Kankanhalli*

Main category: cs.AI

TL;DR: 研究开发了一个数据集（MhAIM），用于分析如何AI生成内容影响人类行为，并提出一个系统（T-Lens）以更好地集成人类反应进行内容解读。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容的普及，如何影响人类行为比验证内容真实性更加重要，尤其在金融等领域。

Method: 构建了一个包含大量在线帖子的MhAIM数据集，进行大规模分析人类应对AI内容的反应。开发基于HR-MCP协议的T-Lens系统，与LLMs结合提升人类反应的映射能力。

Result: 人类在文本与视觉信息并存时更擅长识别AI生成内容，并通过提出3个新指标（可信性、影响力、开放性）来量化用户反应。

Conclusion: 研究为LLMs提供了人类感知能力的工具与策略，为AI驱动的错误信息风险管理提供了新见解与方法。

Abstract: As AI-generated content becomes widespread, so does the risk of
misinformation. While prior research has primarily focused on identifying
whether content is authentic, much less is known about how such content
influences human perception and behavior. In domains like trading or the stock
market, predicting how people react (e.g., whether a news post will go viral),
can be more critical than verifying its factual accuracy. To address this, we
take a human-centered approach and introduce the MhAIM Dataset, which contains
154,552 online posts (111,153 of them AI-generated), enabling large-scale
analysis of how people respond to AI-generated content. Our human study reveals
that people are better at identifying AI content when posts include both text
and visuals, particularly when inconsistencies exist between the two. We
propose three new metrics: trustworthiness, impact, and openness, to quantify
how users judge and engage with online content. We present T-Lens, an LLM-based
agent system designed to answer user queries by incorporating predicted human
responses to multimodal information. At its core is HR-MCP (Human Response
Model Context Protocol), built on the standardized Model Context Protocol
(MCP), enabling seamless integration with any LLM. This integration allows
T-Lens to better align with human reactions, enhancing both interpretability
and interaction capabilities. Our work provides empirical insights and
practical tools to equip LLMs with human-awareness capabilities. By
highlighting the complex interplay among AI, human cognition, and information
reception, our findings suggest actionable strategies for mitigating the risks
of AI-driven misinformation.

</details>


### [211] [The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference](https://arxiv.org/abs/2508.10777)
*Maël Jullien,Marco Valentino,André Freitas*

Main category: cs.AI

TL;DR: 本文通过对临床试验的自然语言推理进行基准测试，发现当前的大型语言模型在推理任务中表现不佳，即使它们拥有足够的相关知识。


<details>
  <summary>Details</summary>
Motivation: 探究当前大型语言模型是否通过扩展数据和参数规模，获得了更具结构化和可泛化的内部表示。

Method: 设计了一个临床试验自然语言推理基准，包含四类推理任务，并设有Ground Knowledge and Meta-Level Reasoning Verification (GKMRV)探针以区分知识获取失败和推理失败。

Result: 尽管模型在GKMRV探针中表现接近满分（平均准确率0.918），但在实际推理任务中的平均准确率仅为0.25，表明它们在推理任务中存在结构性和表现上的限制。

Conclusion: 当前的大型语言模型虽然拥有相关知识，但缺乏结构化和可组合的内部表示，限制了其在高风险领域中可靠性的实现。

Abstract: Large language models are often assumed to acquire increasingly structured,
generalizable internal representations simply by scaling data and parameters.
We interrogate this assumption by introducing a Clinical Trial Natural Language
Inference benchmark comprising four reasoning families, Causal Attribution,
Compositional Grounding, Epistemic Verification, and Risk State Abstraction.
Each item is paired with a targeted Ground Knowledge and Meta-Level Reasoning
Verification (GKMRV) probe, allowing us to dissociate failures of factual
access from failures of inference. We evaluate six contemporary LLMs under both
direct and chain of thought prompting.
  Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform
poorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy,
output inferences are highly consistent across samples (mean 0.87), indicating
a systematic application of underlying heuristics and shortcuts.
  These results reveal fundamental structural and representational limitations:
current LLMs often possess the relevant clinical knowledge but lack the
structured, composable internal representations needed to deploy it reliably
(e.g., integrating constraints, weighing evidence, or simulating
counterfactuals). Decoupling knowledge from reasoning with GKMRV makes this
dissociation explicit and measurable, providing an effective framework for
probing the reliability of LLMs in high-stakes domains.

</details>


### [212] [Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems](https://arxiv.org/abs/2508.10806)
*Maria J. P. Peixoto,Akriti Pandey,Ahsan Zaman,Peter R. Lewis*

Main category: cs.AI

TL;DR: 这篇论文研究了可解释AI（XAI）方法的可访问性，特别是针对视力障碍用户的障碍与解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着AI被越来越多地部署在关键领域以支持决策，提升对AI输出的理解能力变得至关重要。然而，其对视力障碍用户的适应性尚未被充分研究。论文的动机是填补这一领域的空白。

Method: 采用了双管齐下的方法：首先，进行文献综述发现XAI评估很少涉及残障用户；其次，提出了一个包容性XAI设计的四步概念验证方法，从AI系统分类、角色定义、原型设计到专家与用户评估。

Result: 初步结果表明，相较于详细解释，简化的解释对非视觉用户更易理解。此外，多模态呈现方式是实现公平解释性的必要条件。

Conclusion: 研究表明，当前XAI方法在残障用户中的可访问性较低，提出的包容性设计能促进XAI技术的公平性，并强调了多模态交互的重要性。

Abstract: As AI systems are increasingly deployed to support decision-making in
critical domains, explainability has become a means to enhance the
understandability of these outputs and enable users to make more informed and
conscious choices. However, despite growing interest in the usability of
eXplainable AI (XAI), the accessibility of these methods, particularly for
users with vision impairments, remains underexplored. This paper investigates
accessibility gaps in XAI through a two-pronged approach. First, a literature
review of 79 studies reveals that evaluations of XAI techniques rarely include
disabled users, with most explanations relying on inherently visual formats.
Second, we present a four-part methodological proof of concept that
operationalizes inclusive XAI design: (1) categorization of AI systems, (2)
persona definition and contextualization, (3) prototype design and
implementation, and (4) expert and user assessment of XAI techniques for
accessibility. Preliminary findings suggest that simplified explanations are
more comprehensible for non-visual users than detailed ones, and that
multimodal presentation is required for more equitable interpretability.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [213] [OpenFPL: An open-source forecasting method rivaling state-of-the-art Fantasy Premier League services](https://arxiv.org/abs/2508.09992)
*Daniel Groos*

Main category: cs.LG

TL;DR: 本文介绍了一个开源的预测方法OpenFPL，可以准确预测Fantasy Premier League球员表现，且完全基于公开数据开发。


<details>
  <summary>Details</summary>
Motivation: 当前高精度的预测服务多依赖商业平台，并且不公开其内部算法，限制了普通用户的使用。

Method: 提出了一种名为OpenFPL的开源工具，通过对2020-2024赛季的公开数据进行训练，使用位置特定的集成模型进行预测。

Result: OpenFPL在2024-25赛季测试中表现出与顶级商业服务相当的预测准确性，在高收益球员预测上甚至更胜一筹。

Conclusion: OpenFPL为普通用户提供了与商业服务相当的预测工具，有助于长期计划与策略制定，同时促进了预测资源的民主化。

Abstract: Fantasy Premier League engages the football community in selecting the
Premier League players who will perform best from gameweek to gameweek. Access
to accurate performance forecasts gives participants an edge over competitors
by guiding expectations about player outcomes and reducing uncertainty in squad
selection. However, high-accuracy forecasts are currently limited to commercial
services whose inner workings are undisclosed and that rely on proprietary
data. This paper aims to democratize access to highly accurate forecasts of
player performance by presenting OpenFPL, an open-source Fantasy Premier League
forecasting method developed exclusively from public data. Comprising
position-specific ensemble models optimized on Fantasy Premier League and
Understat data from four previous seasons (2020-21 to 2023-24), OpenFPL
achieves accuracy comparable to a leading commercial service when tested
prospectively on data from the 2024-25 season. OpenFPL also surpasses the
commercial benchmark for high-return players ($>$ 2 points), which are most
influential for rank gains. These findings hold across one-, two-, and
three-gameweek forecast horizons, supporting long-term planning of transfers
and strategies while also informing final-day decisions.

</details>


### [214] [xRFM: Accurate, scalable, and interpretable feature learning models for tabular data](https://arxiv.org/abs/2508.10053)
*Daniel Beaglehole,David Holzmüller,Adityanarayanan Radhakrishnan,Mikhail Belkin*

Main category: cs.LG

TL;DR: xRFM算法结合特征学习核机器与树结构，并在100个回归数据集和200个分类数据集中表现优异，超过当前先进方法，包括GBDTs和TabPFNv2。


<details>
  <summary>Details</summary>
Motivation: 推动表格数据推理方法从传统GBDTs转向基于神经网络和最新特征学习算法的发展，弥补该领域未显著进步的空白。

Method: 提出xRFM算法，该算法结合特征学习核机器与树结构，能够适应数据的局部结构并扩展到几乎无限的训练数据规模，且具有原生的可解释性。

Result: 在100个回归数据集中表现最佳，并在200个分类数据集中对比31种方法表现出竞争力，实际上超过GBDTs，展示了先进的性能。

Conclusion: xRFM算法在大规模数据处理和表格数据推理的性能与适应性方面展示出出色潜力，证明其可以成为基准方法的有力替代者。

Abstract: Inference from tabular data, collections of continuous and categorical
variables organized into matrices, is a foundation for modern technology and
science. Yet, in contrast to the explosive changes in the rest of AI, the best
practice for these predictive tasks has been relatively unchanged and is still
primarily based on variations of Gradient Boosted Decision Trees (GBDTs). Very
recently, there has been renewed interest in developing state-of-the-art
methods for tabular data based on recent developments in neural networks and
feature learning methods. In this work, we introduce xRFM, an algorithm that
combines feature learning kernel machines with a tree structure to both adapt
to the local structure of the data and scale to essentially unlimited amounts
of training data.
  We show that compared to $31$ other methods, including recently introduced
tabular foundation models (TabPFNv2) and GBDTs, xRFM achieves best performance
across $100$ regression datasets and is competitive to the best methods across
$200$ classification datasets outperforming GBDTs. Additionally, xRFM provides
interpretability natively through the Average Gradient Outer Product.

</details>


### [215] [A Personalized Exercise Assistant using Reinforcement Learning (PEARL): Results from a four-arm Randomized-controlled Trial](https://arxiv.org/abs/2508.10060)
*Amy Armento Lee,Narayan Hegde,Nina Deliu,Emily Rosenzweig,Arun Suggala,Sriram Lakshminarasimhan,Qian He,John Hernandez,Martin Seneviratne,Rahul Singh,Pradnesh Kalkar,Karthikeyan Shanmugam,Aravindan Raghuveer,Abhimanyu Singh,My Nguyen,James Taylor,Jatin Alla,Sofia S. Villar,Hulya Emir-Farinas*

Main category: cs.LG

TL;DR: 研究探讨了使用强化学习算法，通过Fitbit应用个性化物理活动激励的效果，结果显示RL组显著提高了步数，表明这种方法在数字健康干预中的潜力。


<details>
  <summary>Details</summary>
Motivation: 全球范围内缺乏身体活动是一个主要的健康挑战，而移动健康干预手段提供了个性化、可扩展的活动促进可能性，但需要克服方法学上的障碍。

Method: 设计并实施了一项四组随机对照试验，13,463名参与者根据干预类型分为对照组、随机组、固定组和RL组，RL组通过强化学习算法选择激励内容与时间。

Result: 结果显示RL组的日均步数在1个月和2个月时显著高于其他组，对照组(+296步)、随机组(+218步)和固定组(+238步)。结果在时间上也能维持显著提升。

Conclusion: 基于行为科学引导的强化学习算法有潜力个性化与扩展物理活动的数字健康干预，进一步验证了其有效性。

Abstract: Consistent physical inactivity poses a major global health challenge. Mobile
health (mHealth) interventions, particularly Just-in-Time Adaptive
Interventions (JITAIs), offer a promising avenue for scalable, personalized
physical activity (PA) promotion. However, developing and evaluating such
interventions at scale, while integrating robust behavioral science, presents
methodological hurdles. The PEARL study was the first large-scale, four-arm
randomized controlled trial to assess a reinforcement learning (RL) algorithm,
informed by health behavior change theory, to personalize the content and
timing of PA nudges via a Fitbit app.
  We enrolled and randomized 13,463 Fitbit users into four study arms: control,
random, fixed, and RL. The control arm received no nudges. The other three arms
received nudges from a bank of 155 nudges based on behavioral science
principles. The random arm received nudges selected at random. The fixed arm
received nudges based on a pre-set logic from survey responses about PA
barriers. The RL group received nudges selected by an adaptive RL algorithm. We
included 7,711 participants in primary analyses (mean age 42.1, 86.3% female,
baseline steps 5,618.2).
  We observed an increase in PA for the RL group compared to all other groups
from baseline to 1 and 2 months. The RL group had significantly increased
average daily step count at 1 month compared to all other groups: control (+296
steps, p=0.0002), random (+218 steps, p=0.005), and fixed (+238 steps,
p=0.002). At 2 months, the RL group sustained a significant increase compared
to the control group (+210 steps, p=0.0122). Generalized estimating equation
models also revealed a sustained increase in daily steps in the RL group vs.
control (+208 steps, p=0.002). These findings demonstrate the potential of a
scalable, behaviorally-informed RL approach to personalize digital health
interventions for PA.

</details>


### [216] [Measuring Time Series Forecast Stability for Demand Planning](https://arxiv.org/abs/2508.10063)
*Steven Klee,Yuntian Xia*

Main category: cs.LG

TL;DR: 本文研究了时间序列预测中模型的稳定性问题，分析了当前先进深度学习模型的预测稳定性和准确性，通过案例研究发现集成模型可以提升稳定性且不会显著降低预测准确性。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列预测模型过于注重提高预测准确性，却忽视在生产系统中实现需求计划员更重视的预测稳定性。

Method: 作者通过案例研究，测量了当前先进预测模型（如Chronos、DeepAR等）在公开数据集上的稳定性和准确性表现，分析集成模型对稳定性的影响。

Result: 研究发现，集成模型能够在不显著影响甚至提升预测准确性的同时，提高预测的稳定性。

Conclusion: 研究表明生产系统中对模型预测稳定性的研究和优化具有重要意义，呼吁学术界更多关注构建具备高稳定性的时间序列预测模型。

Abstract: Time series forecasting is a critical first step in generating demand plans
for supply chains. Experiments on time series models typically focus on
demonstrating improvements in forecast accuracy over existing/baseline
solutions, quantified according to some accuracy metric. There is no doubt that
forecast accuracy is important; however in production systems, demand planners
often value consistency and stability over incremental accuracy improvements.
Assuming that the inputs have not changed significantly, forecasts that vary
drastically from one planning cycle to the next require high amounts of human
intervention, which frustrates demand planners and can even cause them to lose
trust in ML forecasting models. We study model-induced stochasticity, which
quantifies the variance of a set of forecasts produced by a single model when
the set of inputs is fixed. Models with lower variance are more stable.
  Recently the forecasting community has seen significant advances in forecast
accuracy through the development of deep machine learning models for time
series forecasting. We perform a case study measuring the stability and
accuracy of state-of-the-art forecasting models (Chronos, DeepAR, PatchTST,
Temporal Fusion Transformer, TiDE, and the AutoGluon best quality ensemble) on
public data sets from the M5 competition and Favorita grocery sales. We show
that ensemble models improve stability without significantly deteriorating (or
even improving) forecast accuracy. While these results may not be surprising,
the main point of this paper is to propose the need for further study of
forecast stability for models that are being deployed in production systems.

</details>


### [217] [Constrained Decoding of Diffusion LLMs with Context-Free Grammars](https://arxiv.org/abs/2508.10111)
*Niels Mündler,Jasper Dekoninck,Martin Vechev*

Main category: cs.LG

TL;DR: 本文提出了针对扩散模型的约束解码方法，以在生成代码和结构化数据中确保语法正确性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的自然语言输出在形式语言中缺乏语法一致性，而实际应用需要形式化代码输出（如C++和JSON）符合语法约束。

Method: 通过将约束解码问题简化为可扩展的添加式填充问题，进一步转化为目标语言与正则语言交集判空检测，并提供一个高效的算法。

Result: 该方法在C++代码补全和JSON数据提取的实验中表现卓越，语法正确性接近完美，并能改进功能性正确性，同时保持计算效率。

Conclusion: 该方法有效解决了扩散语言模型中形式语言生成的约束问题，兼顾语法正确性和性能优化，适用于实际应用。

Abstract: Large language models (LLMs) have shown promising performance across diverse
domains. Many practical applications of LLMs, such as code completion and
structured data extraction, require adherence to syntactic constraints
specified by a formal language. Yet, due to their probabilistic nature, LLM
output is not guaranteed to adhere to such formal languages. Prior work has
proposed constrained decoding as a means to restrict LLM generation to
particular formal languages. However, existing works are not applicable to the
emerging paradigm of diffusion LLMs, when used in practical scenarios such as
the generation of formally correct C++ or JSON output. In this paper we address
this challenge and present the first constrained decoding method for diffusion
models, one that can handle formal languages captured by context-free grammars.
We begin by reducing constrained decoding to the more general additive
infilling problem, which asks whether a partial output can be completed to a
valid word in the target language. This problem also naturally subsumes the
previously unaddressed multi-region infilling constrained decoding. We then
reduce this problem to the task of deciding whether the intersection of the
target language and a regular language is empty and present an efficient
algorithm to solve it for context-free languages. Empirical results on various
applications, such as C++ code infilling and structured data extraction in
JSON, demonstrate that our method achieves near-perfect syntactic correctness
while consistently preserving or improving functional correctness. Importantly,
our efficiency optimizations ensure that the computational overhead remains
practical.

</details>


### [218] [Less is More: Learning Graph Tasks with Just LLMs](https://arxiv.org/abs/2508.10115)
*Sola Shirai,Kavitha Srinivas,Julian Dolby,Michael Katz,Horst Samulowitz,Shirin Sohrabi*

Main category: cs.LG

TL;DR: 本论文探讨了大语言模型（LLMs）在图推理任务中的表现，证明小型LLMs通过训练可解答图任务并具有良好的泛化能力，无需使用专门的图编码器。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨大语言模型是否可以在不依赖专门的图编码方法的情况下解决图相关问题及其泛化能力。

Method: 通过为小型LLMs提供链式思考训练，研究其在基本图任务上的学习和泛化能力表现，并与其他竞争方法进行比较。

Result: 即使是小型LLMs，通过链式思考训练后，也能够解决图任务，并可以很好地泛化到未见过的任务和图结构。

Conclusion: 训练大语言模型进行图推理任务并不依赖专门的图编码器，它们可以通过有效训练在新任务和新图结构中表现良好。

Abstract: For large language models (LLMs), reasoning over graphs could help solve many
problems. Prior work has tried to improve LLM graph reasoning by examining how
best to serialize graphs as text and by combining GNNs and LLMs. However, the
merits of such approaches remain unclear, so we empirically answer the
following research questions: (1) Can LLMs learn to solve fundamental graph
tasks without specialized graph encoding models?, (2) Can LLMs generalize
learned solutions to unseen graph structures or tasks?, and (3) What are the
merits of competing approaches to learn graph tasks? We show that even small
LLMs can learn to solve graph tasks by training them with instructive
chain-of-thought solutions, and this training generalizes, without specialized
graph encoders, to new tasks and graph structures.

</details>


### [219] [From Intent to Execution: Multimodal Chain-of-Thought Reinforcement Learning for Precise CAD Code Generation](https://arxiv.org/abs/2508.10118)
*Ke Niu,Haiyang Yu,Zhuofan Chen,Mengyang Zhao,Teng Fu,Bin Li,Xiangyang Xue*

Main category: cs.LG

TL;DR: 本文提出了一种名为CAD-RL的强化学习框架，用于将自然语言转化为CAD建模代码。


<details>
  <summary>Details</summary>
Motivation: 目前的CAD建模工作流程需要大量的领域专长和手动建模工作，难以直接将人的设计意图翻译为可执行CAD代码。

Method: CAD-RL通过多模态链式思维引导的冷启动技术和目标驱动的强化学习框架进行后续训练，包括可执行性奖励、几何精度奖励和外部评估奖励三类任务奖励。

Result: 实验表明，CAD-RL在推理质量、输出精度和代码可执行性上优于现有的视觉语言模型 (VLMs)。

Conclusion: CAD-RL展示了将自然语言高效转化为可执行CAD代码的潜力，为自动化参数化3D建模打开了新可能性。

Abstract: Computer-Aided Design (CAD) plays a vital role in engineering and
manufacturing, yet current CAD workflows require extensive domain expertise and
manual modeling effort. Recent advances in large language models (LLMs) have
made it possible to generate code from natural language, opening new
opportunities for automating parametric 3D modeling. However, directly
translating human design intent into executable CAD code remains highly
challenging, due to the need for logical reasoning, syntactic correctness, and
numerical precision. In this work, we propose CAD-RL, a multimodal
Chain-of-Thought (CoT) guided reinforcement learning post training framework
for CAD modeling code generation. Our method combines CoT-based Cold Start with
goal-driven reinforcement learning post training using three task-specific
rewards: executability reward, geometric accuracy reward, and external
evaluation reward. To ensure stable policy learning under sparse and
high-variance reward conditions, we introduce three targeted optimization
strategies: Trust Region Stretch for improved exploration, Precision Token Loss
for enhanced dimensions parameter accuracy, and Overlong Filtering to reduce
noisy supervision. To support training and benchmarking, we release ExeCAD, a
noval dataset comprising 16,540 real-world CAD examples with paired natural
language and structured design language descriptions, executable CADQuery
scripts, and rendered 3D models. Experiments demonstrate that CAD-RL achieves
significant improvements in reasoning quality, output precision, and code
executability over existing VLMs.

</details>


### [220] [Nested-ReFT: Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts](https://arxiv.org/abs/2508.10123)
*Maxime Heuillet,Yufei Cui,Boxing Chen,Audrey Durand,Prasanna Parthasarathi*

Main category: cs.LG

TL;DR: 研究提出了一种名为Nested-ReFT的新的强化学习微调框架，改进了数学推理等复杂领域中大模型的计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的用强化学习后训练方法（ReFT）来提升大模型推理能力的框架存在计算代价高的问题。

Method: 提出Nested-ReFT，借鉴离策略强化学习和推测解码，将模型的部分层用作行为模型，通过动态层跳过减少计算开销，并提供不偏梯度估计。

Result: 在多个数学推理基准和不同模型规模上展示了计算效率的提升，并探讨了减少离策略性的方法以保持性能。

Conclusion: Nested-ReFT框架在计算效率和性能保持方面达到了优化，是现有ReFT的改进方案。

Abstract: Advanced reasoning in LLMs on challenging domains like mathematical reasoning
can be tackled using verifiable rewards based reinforced fine-tuning (ReFT). In
standard ReFT frameworks, a behavior model generates multiple completions with
answers per problem, for the answer to be then scored by a reward function.
While such RL post-training methods demonstrate significant performance
improvements across challenging reasoning domains, the computational cost of
generating completions during training with multiple inference steps makes the
training cost non-trivial. To address this, we draw inspiration from off-policy
RL, and speculative decoding to introduce a novel ReFT framework, dubbed
Nested-ReFT, where a subset of layers of the target model acts as the behavior
model to generate off-policy completions during training. The behavior model
configured with dynamic layer skipping per batch during training decreases the
inference cost compared to the standard ReFT frameworks. Our theoretical
analysis shows that Nested-ReFT yields unbiased gradient estimates with
controlled variance. Our empirical analysis demonstrates improved computational
efficiency measured as tokens/sec across multiple math reasoning benchmarks and
model sizes. Additionally, we explore three variants of bias mitigation to
minimize the off-policyness in the gradient updates that allows for maintaining
performance that matches the baseline ReFT performance.

</details>


### [221] [rETF-semiSL: Semi-Supervised Learning for Neural Collapse in Temporal Data](https://arxiv.org/abs/2508.10147)
*Yuhan Xie,William Cappelletti,Mahsa Shoaran,Pascal Frossard*

Main category: cs.LG

TL;DR: 该研究提出了一种新的半监督预训练策略，使得时间序列分类的深度神经网络的隐变量表征更贴近理论上的“神经坍塌”现象。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列神经网络难以捕捉复杂的时间模式，现有的自监督和半监督预训练方法虽有进展但存在设计任务的启发性强和下游任务转移性不佳的问题。

Method: 通过使用旋转等角紧框架分类器和伪标签技术，与生成式预训练任务结合，并定义了一种新的序列增强策略，预训练时间序列深度网络。

Result: 提出的方法超越了现有的预训练任务，尤其在三种多变量时间序列分类数据集上的表现显著优于传统方法。

Conclusion: 这项研究强调了对齐预训练目标与基于理论的嵌入几何结构的重要性。

Abstract: Deep neural networks for time series must capture complex temporal patterns,
to effectively represent dynamic data. Self- and semi-supervised learning
methods show promising results in pre-training large models, which -- when
finetuned for classification -- often outperform their counterparts trained
from scratch. Still, the choice of pretext training tasks is often heuristic
and their transferability to downstream classification is not granted, thus we
propose a novel semi-supervised pre-training strategy to enforce latent
representations that satisfy the Neural Collapse phenomenon observed in
optimally trained neural classifiers. We use a rotational equiangular tight
frame-classifier and pseudo-labeling to pre-train deep encoders with few
labeled samples. Furthermore, to effectively capture temporal dynamics while
enforcing embedding separability, we integrate generative pretext tasks with
our method, and we define a novel sequential augmentation strategy. We show
that our method significantly outperforms previous pretext tasks when applied
to LSTMs, transformers, and state-space models on three multivariate time
series classification datasets. These results highlight the benefit of aligning
pre-training objectives with theoretically grounded embedding geometry.

</details>


### [222] [Out-of-Distribution Detection using Counterfactual Distance](https://arxiv.org/abs/2508.10148)
*Maria Stoica,Francesco Leofante,Alessio Lomuscio*

Main category: cs.LG

TL;DR: 本文提出了一种基于反事实解释的后处理OOD检测方法，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，距离决策边界的特征距离能够有效用于检测分布外数据（OOD），但具有解释性的方法尚缺乏。

Method: 提出了一种后处理方法，通过计算输入到决策边界的距离，利用反事实解释来实现。此外，为了提高可扩展性，直接在嵌入空间中计算反事实。

Result: 在CIFAR-10、CIFAR-100和ImageNet-200等数据集上，与SOTA相比，本文方法在多项指标上表现出色，例如CIFAR-100上的AUROC达到97.05%。

Conclusion: 本文方法不仅准确性高，还利用反事实解释增强了解释性，是一个有效的OOD检测工具。

Abstract: Accurate and explainable out-of-distribution (OOD) detection is required to
use machine learning systems safely. Previous work has shown that feature
distance to decision boundaries can be used to identify OOD data effectively.
In this paper, we build on this intuition and propose a post-hoc OOD detection
method that, given an input, calculates the distance to decision boundaries by
leveraging counterfactual explanations. Since computing explanations can be
expensive for large architectures, we also propose strategies to improve
scalability by computing counterfactuals directly in embedding space.
Crucially, as the method employs counterfactual explanations, we can seamlessly
use them to help interpret the results of our detector. We show that our method
is in line with the state of the art on CIFAR-10, achieving 93.50% AUROC and
25.80% FPR95. Our method outperforms these methods on CIFAR-100 with 97.05%
AUROC and 13.79% FPR95 and on ImageNet-200 with 92.55% AUROC and 33.55% FPR95
across four OOD datasets

</details>


### [223] [Characterizing Evolution in Expectation-Maximization Estimates for Overspecified Mixed Linear Regression](https://arxiv.org/abs/2508.10154)
*Zhankun Luo,Abolfazl Hashemi*

Main category: cs.LG

TL;DR: 研究探讨了在过定模型情况下，EM算法在两组混合线性回归模型上的表现，提出并研究了均衡与非均衡初始条件下的收敛性和复杂性。


<details>
  <summary>Details</summary>
Motivation: 现有混合模型尽管有应用价值和理论支持，但在模型指定过多组件时面临挑战，研究试图解析EM算法在这种情况下的表现。

Method: 通过理论分析，考察EM算法在线性回归问题下的收敛性，定理包括均衡与非均衡初始混合权重下的收敛性与统计精度，并扩展到低信噪比场景。

Result: 非均衡初始权重下收敛快且精度高；均衡初始权重收敛较慢且精度偏低。指出精度与数据量和维度的关系，并导出有限样本场景下的迭代复杂度。

Conclusion: 理论分析完善了EM算法在过定混合模型下的表现理解，并为低信噪比情况下的研究提供了进一步的方向。

Abstract: Mixture models have attracted significant attention due to practical
effectiveness and comprehensive theoretical foundations. A persisting challenge
is model misspecification, which occurs when the model to be fitted has more
mixture components than those in the data distribution. In this paper, we
develop a theoretical understanding of the Expectation-Maximization (EM)
algorithm's behavior in the context of targeted model misspecification for
overspecified two-component Mixed Linear Regression (2MLR) with unknown
$d$-dimensional regression parameters and mixing weights. In Theorem 5.1 at the
population level, with an unbalanced initial guess for mixing weights, we
establish linear convergence of regression parameters in $O(\log(1/\epsilon))$
steps. Conversely, with a balanced initial guess for mixing weights, we observe
sublinear convergence in $O(\epsilon^{-2})$ steps to achieve the
$\epsilon$-accuracy at Euclidean distance. In Theorem 6.1 at the finite-sample
level, for mixtures with sufficiently unbalanced fixed mixing weights, we
demonstrate a statistical accuracy of $O((d/n)^{1/2})$, whereas for those with
sufficiently balanced fixed mixing weights, the accuracy is $O((d/n)^{1/4})$
given $n$ data samples. Furthermore, we underscore the connection between our
population level and finite-sample level results: by setting the desired final
accuracy $\epsilon$ in Theorem 5.1 to match that in Theorem 6.1 at the
finite-sample level, namely letting $\epsilon = O((d/n)^{1/2})$ for
sufficiently unbalanced fixed mixing weights and $\epsilon = O((d/n)^{1/4})$
for sufficiently balanced fixed mixing weights, we intuitively derive iteration
complexity bounds $O(\log (1/\epsilon))=O(\log (n/d))$ and
$O(\epsilon^{-2})=O((n/d)^{1/2})$ at the finite-sample level for sufficiently
unbalanced and balanced initial mixing weights. We further extend our analysis
in overspecified setting to low SNR regime.

</details>


### [224] [Benchmark-Driven Selection of AI: Evidence from DeepSeek-R1](https://arxiv.org/abs/2508.10173)
*Petr Spelda,Vit Stritecky*

Main category: cs.LG

TL;DR: 论文探讨通过有影响力的基准测试（benchmarks）来提升推理语言模型表现及其在关键任务中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究通过引入有影响力的基准测试来指导推理语言模型的训练，探索在大规模语言模型发展的背景下如何更好地提升其推理能力和任务泛化力。

Method: 开发了DeepSeek-R1模型，并利用Humanity's Last Exam中的一个顺序决策问题，对用基准测试作为学习课程的方法进行研究。

Result: 发现模型更好的性能不仅来源于测试时的算法改进或模型规模，还与使用有影响力的基准测试作为学习课程有直接关系。

Conclusion: 引入有影响力的基准测试可以通过将评估交易为学习来促进推理语言模型的发展，同时强调测试任务的创新性对推理模型的泛化能力测量是关键。

Abstract: Evaluation of reasoning language models gained importance after it was
observed that they can combine their existing capabilities into novel traces of
intermediate steps before task completion and that the traces can sometimes
help them to generalize better than past models. As reasoning becomes the next
scaling dimension of large language models, careful study of their capabilities
in critical tasks is needed. We show that better performance is not always
caused by test-time algorithmic improvements or model sizes but also by using
impactful benchmarks as curricula for learning. We call this benchmark-driven
selection of AI and show its effects on DeepSeek-R1 using our sequential
decision-making problem from Humanity's Last Exam. Steering development of AI
by impactful benchmarks trades evaluation for learning and makes novelty of
test tasks key for measuring generalization capabilities of reasoning models.
Consequently, some benchmarks could be seen as curricula for training rather
than unseen test sets.

</details>


### [225] [An Explainable AI based approach for Monitoring Animal Health](https://arxiv.org/abs/2508.10210)
*Rahul Janaa,Shubham Dixit,Mrityunjay Sharma,Ritesh Kumar*

Main category: cs.LG

TL;DR: 本文提出了一种基于机器学习和解释性人工智能框架（如SHAP）的现代化数据驱动技术，用于分析奶牛行为并协助奶农优化管理。


<details>
  <summary>Details</summary>
Motivation: 奶农面临牛群健康监测与产量优化的挑战，本研究旨在通过解释性机器学习方法为奶农提供可操作的信息，从而使其做出基于数据的决策并促进可持续实践。

Method: 使用3轴加速度传感器收集数据，采用蓝牙物联网（IoT）设备和4G网络确保数据传输与实时分析，特别强调时间序列数据的预处理（如提取统计特征、信号处理技术、滑动窗口技术等），并通过多种优化的机器学习模型进行活动分类，随后结合SHAP框架解释特征重要性。

Result: 经过不同窗口长度的模型评估，k近邻分类器的性能最佳，训练集AUC为0.98±0.0026，测试集AUC为0.99。此外，SHAP框架解释了特征的影响力，并提供特征稳定性分析。

Conclusion: 研究开发了透明的、可解释的机器学习模型，以支持奶牛的可持续管理，强调数据驱动方法在实际农业中的有效性。

Abstract: Monitoring cattle health and optimizing yield are key challenges faced by
dairy farmers due to difficulties in tracking all animals on the farm. This
work aims to showcase modern data-driven farming practices based on explainable
machine learning(ML) methods that explain the activity and behaviour of dairy
cattle (cows). Continuous data collection of 3-axis accelerometer sensors and
usage of robust ML methodologies and algorithms, provide farmers and
researchers with actionable information on cattle activity, allowing farmers to
make informed decisions and incorporate sustainable practices. This study
utilizes Bluetooth-based Internet of Things (IoT) devices and 4G networks for
seamless data transmission, immediate analysis, inference generation, and
explains the models performance with explainability frameworks. Special
emphasis is put on the pre-processing of the accelerometers time series data,
including the extraction of statistical characteristics, signal processing
techniques, and lag-based features using the sliding window technique. Various
hyperparameter-optimized ML models are evaluated across varying window lengths
for activity classification. The k-nearest neighbour Classifier achieved the
best performance, with AUC of mean 0.98 and standard deviation of 0.0026 on the
training set and 0.99 on testing set). In order to ensure transparency,
Explainable AI based frameworks such as SHAP is used to interpret feature
importance that can be understood and used by practitioners. A detailed
comparison of the important features, along with the stability analysis of
selected features, supports development of explainable and practical ML models
for sustainable livestock management.

</details>


### [226] [AI-Driven Detection and Analysis of Handwriting on Seized Ivory: A Tool to Uncover Criminal Networks in the Illicit Wildlife Trade](https://arxiv.org/abs/2508.10219)
*Will Fein,Ryan J. Horwitz,John E. Brown III,Amit Misra,Felipe Oviedo,Kevin White,Juan M. Lavista Ferres,Samuel K. Wasser*

Main category: cs.LG

TL;DR: 本研究开发了一种利用AI分析象牙手写标记的新方法，用于打击跨国象牙贸易，并取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 非洲大象数量因象牙走私问题持续下降，而现有手段难以有效打击相关的犯罪网络。基于手写标记的分析为一个低成本、可扩展的潜在解决方案。

Method: 收集了6,085张象牙照片，利用物体检测模型提取了17,000+手写标记，并通过AI工具进行分类描述，从中识别出184种标志性手写标记，连接涉及多批象牙的犯罪网络。

Result: 发现了20种标记在多次查获的象牙中重复出现，成功连接不同走私行为，填补了其他数据来源的空白。

Conclusion: AI技术在野生动物取证领域具有巨大的潜力，可通过手写标记分析强化打击有组织的野生动物犯罪行动。

Abstract: The transnational ivory trade continues to drive the decline of elephant
populations across Africa, and trafficking networks remain difficult to
disrupt. Tusks seized by law enforcement officials carry forensic information
on the traffickers responsible for their export, including DNA evidence and
handwritten markings made by traffickers. For 20 years, analyses of tusk DNA
have identified where elephants were poached and established connections among
shipments of ivory. While the links established using genetic evidence are
extremely conclusive, genetic data is expensive and sometimes impossible to
obtain. But though handwritten markings are easy to photograph, they are rarely
documented or analyzed. Here, we present an AI-driven pipeline for extracting
and analyzing handwritten markings on seized elephant tusks, offering a novel,
scalable, and low-cost source of forensic evidence. Having collected 6,085
photographs from eight large seizures of ivory over a 6-year period
(2014-2019), we used an object detection model to extract over 17,000
individual markings, which were then labeled and described using
state-of-the-art AI tools. We identified 184 recurring "signature markings"
that connect the tusks on which they appear. 20 signature markings were
observed in multiple seizures, establishing forensic links between these
seizures through traffickers involved in both shipments. This work complements
other investigative techniques by filling in gaps where other data sources are
unavailable. The study demonstrates the transformative potential of AI in
wildlife forensics and highlights practical steps for integrating handwriting
analysis into efforts to disrupt organized wildlife crime.

</details>


### [227] [Comparison of D-Wave Quantum Annealing and Markov Chain Monte Carlo for Sampling from a Probability Distribution of a Restricted Boltzmann Machine](https://arxiv.org/abs/2508.10228)
*Abdelmoula El Yazizi,Samee U. Khan,Yaroslav Koshka*

Main category: cs.LG

TL;DR: 本文评估了使用D-Wave量子退火器从限制玻尔兹曼机(RBM)采样的质量，与传统Gibbs采样对比后显示D-Wave采样的某些局限性及潜在改进方向。


<details>
  <summary>Details</summary>
Motivation: 旨在探索D-Wave量子退火器在RBM上的采样能力，并比较其与传统Gibbs采样的表现，以理解其对模型训练的影响。

Method: 通过在对比散度基础上的RBM学习条件下，获取D-Wave和Gibbs采样，并分析采样对应局部谷的数量及局部极小值的能量分布。

Result: 未能通过缩短D-Wave退火时间，提高局部谷的数量。尽管D-Wave采样相比Gibbs采样涵盖更多局部谷，但多数局部谷彼此不重叠且高概率样本区域重叠更多。在后期学习阶段，两种采样技术互补性有所下降，但有潜力通过结合方法改进采样质量。

Conclusion: 未能显著改善使用D-Wave采样的效果，但揭示了结合量子与经典方法有潜在的改进可能性，可为RBM学习提供新的方向。

Abstract: A local-valley (LV) centered approach to assessing the quality of sampling
from Restricted Boltzmann Machines (RBMs) was applied to the latest generation
of the D-Wave quantum annealer. D-Wave and Gibbs samples from a classically
trained RBM were obtained at conditions relevant to the
contrastive-divergence-based RBM learning. The samples were compared for the
number of the LVs to which they belonged and the energy of the corresponding
local minima. No significant (desirable) increase in the number of the LVs has
been achieved by decreasing the D-Wave annealing time. At any training epoch,
the states sampled by the D-Wave belonged to a somewhat higher number of LVs
than in the Gibbs sampling. However, many of those LVs found by the two
techniques differed. For high-probability sampled states, the two techniques
were (unfavorably) less complementary and more overlapping. Nevertheless, many
potentially "important" local minima, i.e., those having intermediate, even if
not high, probability values, were found by only one of the two sampling
techniques while missed by the other. The two techniques overlapped less at
later than earlier training epochs, which is precisely the stage of the
training when modest improvements to the sampling quality could make meaningful
differences for the RBM trainability. The results of this work may explain the
failure of previous investigations to achieve substantial (or any) improvement
when using D-Wave-based sampling. However, the results reveal some potential
for improvement, e.g., using a combined classical-quantum approach.

</details>


### [228] [Interpretable Machine Learning Model for Early Prediction of Acute Kidney Injury in Critically Ill Patients with Cirrhosis: A Retrospective Study](https://arxiv.org/abs/2508.10233)
*Li Sun,Shuheng Chen,Junyi Fan,Yong Si,Minoo Ahmadi,Elham Pishgar,Kamiar Alaei,Maryam Pishgar*

Main category: cs.LG

TL;DR: 这个研究开发了一种基于LightGBM的机器学习模型，用于预测肝硬化重症患者的急性肾损伤（AKI），具有高准确性和对临床适用性的成功解读。


<details>
  <summary>Details</summary>
Motivation: 肝硬化患者经常面临急性肾损伤（AKI）的风险，这会显著影响治疗结果。然而，现有预测工具通常缺乏精确性、可解释性以及与ICU流程相结合的能力，因此需要开发更好的早期预测模型。

Method: 作者使用MIMIC-IV v2.2数据库回顾性分析，针对1240例ICU肝硬化患者，通过预处理、数据筛选、LASSO特征选择和SMOTE平衡类别，并使用六种算法进行模型训练和评估，包括LightGBM、CatBoost、XGBoost等。

Result: LightGBM算法表现最佳，AUROC达到0.808，准确率为0.704，高负预测值（NPV）为0.911，关键预测因子包括部分凝血酶活酶时间延长和低pH值等。

Conclusion: LightGBM模型能有效预测ICU肝硬化患者的AKI风险，具有高负预测值，可用于低风险患者的治疗方式调整，其可解释性增强了临床信任。推荐进一步的外部验证和与电子健康记录系统的整合。

Abstract: Background: Cirrhosis is a progressive liver disease with high mortality and
frequent complications, notably acute kidney injury (AKI), which occurs in up
to 50% of hospitalized patients and worsens outcomes. AKI stems from complex
hemodynamic, inflammatory, and metabolic changes, making early detection
essential. Many predictive tools lack accuracy, interpretability, and alignment
with intensive care unit (ICU) workflows. This study developed an interpretable
machine learning model for early AKI prediction in critically ill patients with
cirrhosis.
  Methods: We conducted a retrospective analysis of the MIMIC-IV v2.2 database,
identifying 1240 adult ICU patients with cirrhosis and excluding those with ICU
stays under 48 hours or missing key data. Laboratory and physiological
variables from the first 48 hours were extracted. The pipeline included
preprocessing, missingness filtering, LASSO feature selection, and SMOTE class
balancing. Six algorithms-LightGBM, CatBoost, XGBoost, logistic regression,
naive Bayes, and neural networks-were trained and evaluated using AUROC,
accuracy, F1-score, sensitivity, specificity, and predictive values.
  Results: LightGBM achieved the best performance (AUROC 0.808, 95% CI
0.741-0.856; accuracy 0.704; NPV 0.911). Key predictors included prolonged
partial thromboplastin time, absence of outside-facility 20G placement, low pH,
and altered pO2, consistent with known cirrhosis-AKI mechanisms and suggesting
actionable targets.
  Conclusion: The LightGBM-based model enables accurate early AKI risk
stratification in ICU patients with cirrhosis using routine clinical variables.
Its high negative predictive value supports safe de-escalation for low-risk
patients, and interpretability fosters clinician trust and targeted prevention.
External validation and integration into electronic health record systems are
warranted.

</details>


### [229] [Can Transformers Break Encryption Schemes via In-Context Learning?](https://arxiv.org/abs/2508.10235)
*Jathin Korrapati,Patrick Mendoza,Aditya Tomar,Abein Abraham*

Main category: cs.LG

TL;DR: 提出一种新颖的上下文学习应用，应用于密码学函数学习，尤其是专注于单字母替换密码和维吉尼亚密码。


<details>
  <summary>Details</summary>
Motivation: 尝试利用上下文学习评估transformer模型在密码学中的推断能力。

Method: 通过上下文学习，让transformer模型根据给定的少量密文-明文对，推断隐藏的字符替换关系并解码新密文。

Result: 实现了transformer模型在密码学领域中的结构化推断能力评估。

Conclusion: 表明上下文学习可以扩展到新的复杂任务，如密码学函数学习领域。

Abstract: In-context learning (ICL) has emerged as a powerful capability of
transformer-based language models, enabling them to perform tasks by
conditioning on a small number of examples presented at inference time, without
any parameter updates. Prior work has shown that transformers can generalize
over simple function classes like linear functions, decision trees, even neural
networks, purely from context, focusing on numerical or symbolic reasoning over
underlying well-structured functions. Instead, we propose a novel application
of ICL into the domain of cryptographic function learning, specifically
focusing on ciphers such as mono-alphabetic substitution and Vigen\`ere
ciphers, two classes of private-key encryption schemes. These ciphers involve a
fixed but hidden bijective mapping between plain text and cipher text
characters. Given a small set of (cipher text, plain text) pairs, the goal is
for the model to infer the underlying substitution and decode a new cipher text
word. This setting poses a structured inference challenge, which is well-suited
for evaluating the inductive biases and generalization capabilities of
transformers under the ICL paradigm. Code is available at
https://github.com/adistomar/CS182-project.

</details>


### [230] [Pruning and Malicious Injection: A Retraining-Free Backdoor Attack on Transformer Models](https://arxiv.org/abs/2508.10243)
*Taibiao Zhao,Mingxuan Sun,Hao Wang,Xiaobing Chen,Xiangwei Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种针对Transformer的无重训练后门攻击方法HPMI，证明了其有效性和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 目前的后门攻击方法通常需要依赖重训练或更改模型结构，这非常耗能且有侵入性，因此需要一种更高效的替代方法。

Method: 提出了一种名为HPMI（Head-wise Pruning and Malicious Injection）的方法，通过修剪重要性最低的注意力头并注入一个预训练的恶意头，在无需重训练的情况下完成对Transformer的后门植入。

Result: 实验表明，HPMI在保证干净数据精度损失极小的情况下，后门攻击成功率达到99.55%以上，同时能绕过四种高级防御机制。

Conclusion: HPMI在隐蔽性、对抗防御的鲁棒性以及对模型精度的影响方面均优于现有的依赖重训练的攻击方式。

Abstract: Transformer models have demonstrated exceptional performance and have become
indispensable in computer vision (CV) and natural language processing (NLP)
tasks. However, recent studies reveal that transformers are susceptible to
backdoor attacks. Prior backdoor attack methods typically rely on retraining
with clean data or altering the model architecture, both of which can be
resource-intensive and intrusive. In this paper, we propose Head-wise Pruning
and Malicious Injection (HPMI), a novel retraining-free backdoor attack on
transformers that does not alter the model's architecture. Our approach
requires only a small subset of the original data and basic knowledge of the
model architecture, eliminating the need for retraining the target transformer.
Technically, HPMI works by pruning the least important head and injecting a
pre-trained malicious head to establish the backdoor. We provide a rigorous
theoretical justification demonstrating that the implanted backdoor resists
detection and removal by state-of-the-art defense techniques, under reasonable
assumptions. Experimental evaluations across multiple datasets further validate
the effectiveness of HPMI, showing that it 1) incurs negligible clean accuracy
loss, 2) achieves at least 99.55% attack success rate, and 3) bypasses four
advanced defense mechanisms. Additionally, relative to state-of-the-art
retraining-dependent attacks, HPMI achieves greater concealment and robustness
against diverse defense strategies, while maintaining minimal impact on clean
accuracy.

</details>


### [231] [Convergence Analysis of Max-Min Exponential Neural Network Operators in Orlicz Space](https://arxiv.org/abs/2508.10248)
*Satyaranjan Pradhan,Madan Mohan Soren*

Main category: cs.LG

TL;DR: 本文提出了一种利用指数神经网络算子进行函数逼近的Max Min方法，并进一步扩展为Max Min Kantorovich类型算子来研究其逼近性质。


<details>
  <summary>Details</summary>
Motivation: 探索指数神经网络算子在函数逼近中的效率，研究Max Min和Kantorovich类型算子的逼近特性。

Method: 通过构建Max Min Kantorovich类型指数神经网络算子，研究其点点收敛和一致收敛特性，使用对数连续模数分析收敛阶，并在Orlicz空间中进一步分析算子收敛行为。

Result: 实现了对函数的逼近误差分析，并通过图示验证了利用核函数和S型激活函数的逼近效果。

Conclusion: Max Min Kantorovich类型指数神经网络算子在逼近特性和收敛性分析中表现出色，适用于精确逼近函数。

Abstract: In this current work, we propose a Max Min approach for approximating
functions using exponential neural network operators. We extend this framework
to develop the Max Min Kantorovich-type exponential neural network operators
and investigate their approximation properties. We study both pointwise and
uniform convergence for univariate functions. To analyze the order of
convergence, we use the logarithmic modulus of continuity and estimate the
corresponding rate of convergence. Furthermore, we examine the convergence
behavior of the Max Min Kantorovich type exponential neural network operators
within the Orlicz space setting. We provide some graphical representations to
illustrate the approximation error of the function through suitable kernel and
sigmoidal activation functions.

</details>


### [232] [Multi-Agent Reinforcement Learning for Adaptive Resource Orchestration in Cloud-Native Clusters](https://arxiv.org/abs/2508.10253)
*Guanzi Yao,Heyao Liu,Linyan Dai*

Main category: cs.LG

TL;DR: 本文提出了一种基于多智能体强化学习的自适应资源编排方法，用于解决云原生数据库系统中的资源动态性和调度复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 云原生数据库系统面临高资源动态性和复杂的调度管理问题，需寻找高效解决方案。

Method: 提出基于多智能体强化学习的资源编排方法，引入异质性角色智能体建模机制和基于奖励塑造的协调机制，并结合统一多智能体训练框架。

Result: 实验结果显示新方法在资源利用率、调度延迟、策略收敛速度、系统稳定性和公平性等指标上优于传统方法，并在多种实验场景中表现出对高并发、高维状态及复杂依赖关系调度任务的强适应能力。

Conclusion: 该方法在大规模真实复杂调度环境中具有显著优势，显示出良好的通用性和实用价值。

Abstract: This paper addresses the challenges of high resource dynamism and scheduling
complexity in cloud-native database systems. It proposes an adaptive resource
orchestration method based on multi-agent reinforcement learning. The method
introduces a heterogeneous role-based agent modeling mechanism. This allows
different resource entities, such as compute nodes, storage nodes, and
schedulers, to adopt distinct policy representations. These agents are better
able to reflect diverse functional responsibilities and local environmental
characteristics within the system. A reward-shaping mechanism is designed to
integrate local observations with global feedback. This helps mitigate policy
learning bias caused by incomplete state observations. By combining real-time
local performance signals with global system value estimation, the mechanism
improves coordination among agents and enhances policy convergence stability. A
unified multi-agent training framework is developed and evaluated on a
representative production scheduling dataset. Experimental results show that
the proposed method outperforms traditional approaches across multiple key
metrics. These include resource utilization, scheduling latency, policy
convergence speed, system stability, and fairness. The results demonstrate
strong generalization and practical utility. Across various experimental
scenarios, the method proves effective in handling orchestration tasks with
high concurrency, high-dimensional state spaces, and complex dependency
relationships. This confirms its advantages in real-world, large-scale
scheduling environments.

</details>


### [233] [Federated Anomaly Detection for Multi-Tenant Cloud Platforms with Personalized Modeling](https://arxiv.org/abs/2508.10255)
*Yuxi Wang,Heyao Liu,Nyutian Long,Guanzi Yao*

Main category: cs.LG

TL;DR: 本文提出了一种基于联邦学习的异常检测方法，可应对多租户云环境中的数据隐私泄露、资源行为异质性和集中建模的局限性问题。


<details>
  <summary>Details</summary>
Motivation: 解决多租户云环境中的异常检测难题，特别是数据隐私保护和异构资源行为的建模。

Method: 构建一个多租户参与的联邦训练框架，每个租户在本地用私有数据训练模型，并通过参数聚合优化全局模型，同时保留租户特征的个性化表示。

Result: 在多样参与率和噪声注入水平下，实验结果显示提出的方法在精度、召回率和F1-Score等关键指标上优于现有主流模型，并在复杂场景下保持稳定性能。

Conclusion: 提出的方法在云计算环境中实现了智能资源监控和异常诊断的高效性和实用性，可用于实际的多租户智能管理。

Abstract: This paper proposes an anomaly detection method based on federated learning
to address key challenges in multi-tenant cloud environments, including data
privacy leakage, heterogeneous resource behavior, and the limitations of
centralized modeling. The method establishes a federated training framework
involving multiple tenants. Each tenant trains the model locally using private
resource usage data. Through parameter aggregation, a global model is
optimized, enabling cross-tenant collaborative anomaly detection while
preserving data privacy. To improve adaptability to diverse resource usage
patterns, a personalized parameter adjustment mechanism is introduced. This
allows the model to retain tenant-specific feature representations while
sharing global knowledge. In the model output stage, the Mahalanobis distance
is used to compute anomaly scores. This enhances both the accuracy and
stability of anomaly detection. The experiments use real telemetry data from a
cloud platform to construct a simulated multi-tenant environment. The study
evaluates the model's performance under varying participation rates and noise
injection levels. These comparisons demonstrate the proposed method's
robustness and detection accuracy. Experimental results show that the proposed
method outperforms existing mainstream models across key metrics such as
Precision, Recall, and F1-Score. It also maintains stable performance in
various complex scenarios. These findings highlight the method's practical
potential for intelligent resource monitoring and anomaly diagnosis in cloud
computing environments.

</details>


### [234] [Source Component Shift Adaptation via Offline Decomposition and Online Mixing Approach](https://arxiv.org/abs/2508.10257)
*Ryuta Matsuno*

Main category: cs.LG

TL;DR: 本文提出了一种新的源组件迁移自适应方法，该方法通过离线分解和在线混合策略准确处理不断变化的数据流中的源组件迁移问题。


<details>
  <summary>Details</summary>
Motivation: 现有在线学习方法难以有效利用重复迁移，而基于模型池的方法难以捕捉单个源组件，导致适应能力较差。因此需要一种更高效的方法来处理不断变化的数据流。

Method: 采用离线分解和在线混合方法。离线阶段，通过EM算法基于历史数据学习每个源组件的预测模型；在线阶段，通过在线凸优化调整混合权重以实现精确预测。

Result: 该方法在各种实际回归数据集上表现优异，与基线方法相比，累计测试损失最多降低67.4%。

Conclusion: 该方法有效利用迁移特性，相较现有方法在源组件迁移适应性能上更加优越。

Abstract: This paper addresses source component shift adaptation, aiming to update
predictions adapting to source component shifts for incoming data streams based
on past training data. Existing online learning methods often fail to utilize
recurring shifts effectively, while model-pool-based methods struggle to
capture individual source components, leading to poor adaptation. In this
paper, we propose a source component shift adaptation method via an offline
decomposition and online mixing approach. We theoretically identify that the
problem can be divided into two subproblems: offline source component
decomposition and online mixing weight adaptation. Based on this, our method
first determines prediction models, each of which learns a source component
solely based on past training data offline through the EM algorithm. Then, it
updates the mixing weight of the prediction models for precise prediction
through online convex optimization. Thanks to our theoretical derivation, our
method fully leverages the characteristics of the shifts, achieving superior
adaptation performance over existing methods. Experiments conducted on various
real-world regression datasets demonstrate that our method outperforms
baselines, reducing the cumulative test loss by up to 67.4%.

</details>


### [235] [Uncertainty-Aware Prediction of Parkinson's Disease Medication Needs: A Two-Stage Conformal Prediction Approach](https://arxiv.org/abs/2508.10284)
*Ricardo Diaz-Rincon,Muxuan Liang,Adolfo Ramirez-Zamora,Benjamin Shickel*

Main category: cs.LG

TL;DR: 本文提出了一种预测帕金森病患者未来抗药需求的保形预测框架，可在两年内提供可靠预测区间，并保障统计覆盖率。


<details>
  <summary>Details</summary>
Motivation: 帕金森病患者药物管理挑战重重，传统试错法效果有限，而机器学习预测方法临床应用不足且缺乏预测不确定性，限制信任和实用性。

Method: 提出了一种保形预测框架，结合电子健康记录，分两阶段辨识需要调整药物的患者，并预测所需左旋多巴等效日剂量的调整。

Result: 该框架在实现边际覆盖的同时，缩短了预测区间长度，较传统方法性能更优。

Conclusion: 通过量化预测不确定性，该方法支持基于证据的药物剂量决策，优化症状控制，降低副作用，提升患者生活质量。

Abstract: Parkinson's Disease (PD) medication management presents unique challenges due
to heterogeneous disease progression and treatment response. Neurologists must
balance symptom control with optimal dopaminergic dosing based on functional
disability while minimizing side effects. This balance is crucial as inadequate
or abrupt changes can cause levodopa-induced dyskinesia, wearing off, and
neuropsychiatric effects, significantly reducing quality of life. Current
approaches rely on trial-and-error decisions without systematic predictive
methods. Despite machine learning advances, clinical adoption remains limited
due to reliance on point predictions that do not account for prediction
uncertainty, undermining clinical trust and utility. Clinicians require not
only predictions of future medication needs but also reliable confidence
measures. Without quantified uncertainty, adjustments risk premature escalation
to maximum doses or prolonged inadequate symptom control. We developed a
conformal prediction framework anticipating medication needs up to two years in
advance with reliable prediction intervals and statistical guarantees. Our
approach addresses zero-inflation in PD inpatient data, where patients maintain
stable medication regimens between visits. Using electronic health records from
631 inpatient admissions at University of Florida Health (2011-2021), our
two-stage approach identifies patients likely to need medication changes, then
predicts required levodopa equivalent daily dose adjustments. Our framework
achieved marginal coverage while reducing prediction interval lengths compared
to traditional approaches, providing precise predictions for short-term
planning and wider ranges for long-term forecasting. By quantifying
uncertainty, our approach enables evidence-based decisions about levodopa
dosing, optimizing symptom control while minimizing side effects and improving
life quality.

</details>


### [236] [SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic Representation Learning](https://arxiv.org/abs/2508.10298)
*Weijian Mai,Jiamin Wu,Yu Zhu,Zhouheng Yao,Dongzhan Zhou,Andrew F. Luo,Qihao Zheng,Wanli Ouyang,Chunfeng Song*

Main category: cs.LG

TL;DR: 本论文提出了SynBrain，一个生成性框架，用于模拟从视觉语义到神经反应的转换，它以概率且生物可解释的方式实现。


<details>
  <summary>Details</summary>
Motivation: 为了理解视觉刺激如何被转换为大脑皮层反应，目前的决策性方法难以同时捕捉生物学可变性和功能一致性，因此需要新的方法。

Method: 本文提出的SynBrain包含两个主要组成部分：（i）BrainVAE，使用概率学习将神经表征建模为连续的概率分布，同时通过视觉语义约束保持功能一致性；（ii）语义到神经映射器，将视觉语义投射到神经反应流形。

Result: SynBrain在特定主体的视觉到fMRI编码性能上超过了现有最先进方法，并能高效适应新主体，且生成的fMRI信号提高了数据受限情境下的解码性能。

Conclusion: SynBrain证明了其具有在不同试验条件和主体间揭示功能一致性的能力，生成信号捕获了受生物神经变化影响的可解释模式。代码将公开发布。

Abstract: Deciphering how visual stimuli are transformed into cortical responses is a
fundamental challenge in computational neuroscience. This visual-to-neural
mapping is inherently a one-to-many relationship, as identical visual inputs
reliably evoke variable hemodynamic responses across trials, contexts, and
subjects. However, existing deterministic methods struggle to simultaneously
model this biological variability while capturing the underlying functional
consistency that encodes stimulus information. To address these limitations, we
propose SynBrain, a generative framework that simulates the transformation from
visual semantics to neural responses in a probabilistic and biologically
interpretable manner. SynBrain introduces two key components: (i) BrainVAE
models neural representations as continuous probability distributions via
probabilistic learning while maintaining functional consistency through visual
semantic constraints; (ii) A Semantic-to-Neural Mapper acts as a semantic
transmission pathway, projecting visual semantics into the neural response
manifold to facilitate high-fidelity fMRI synthesis. Experimental results
demonstrate that SynBrain surpasses state-of-the-art methods in
subject-specific visual-to-fMRI encoding performance. Furthermore, SynBrain
adapts efficiently to new subjects with few-shot data and synthesizes
high-quality fMRI signals that are effective in improving data-limited
fMRI-to-image decoding performance. Beyond that, SynBrain reveals functional
consistency across trials and subjects, with synthesized signals capturing
interpretable patterns shaped by biological neural variability. The code will
be made publicly available.

</details>


### [237] [Improving Learning of New Diseases through Knowledge-Enhanced Initialization for Federated Adapter Tuning](https://arxiv.org/abs/2508.10299)
*Danni Peng,Yuan Wang,Kangning Cai,Peiyan Ning,Jiming Xu,Yong Liu,Rick Siow Mong Goh,Qingsong Wei,Huazhu Fu*

Main category: cs.LG

TL;DR: 该论文提出FedKEI框架，通过跨客户端和跨任务知识传递优化FL中新的任务初始化，显著提高了适应新疾病的能力。


<details>
  <summary>Details</summary>
Motivation: 在快速变化的医疗环境中，需要方法帮助医疗机构在隐私保护的前提下快速适应新任务或疾病，从过去经验中汲取知识进行优化。

Method: 提出FedKEI框架，通过全局聚类和双层权重优化（聚类间和聚类内权重），结合成本优化的适配器调优，实现新任务的初始化和个性化知识传递。

Result: 在三种基准数据集（皮肤病、胸部X光和视网膜OCT）上进行实验，FedKEI在适应新疾病方面优于现有的最先进方法。

Conclusion: FedKEI框架有效结合了跨客户端和跨任务的知识传递，为应对快速变化的医疗任务环境提供了新的解决方案。

Abstract: In healthcare, federated learning (FL) is a widely adopted framework that
enables privacy-preserving collaboration among medical institutions. With large
foundation models (FMs) demonstrating impressive capabilities, using FMs in FL
through cost-efficient adapter tuning has become a popular approach. Given the
rapidly evolving healthcare environment, it is crucial for individual clients
to quickly adapt to new tasks or diseases by tuning adapters while drawing upon
past experiences. In this work, we introduce Federated Knowledge-Enhanced
Initialization (FedKEI), a novel framework that leverages cross-client and
cross-task transfer from past knowledge to generate informed initializations
for learning new tasks with adapters. FedKEI begins with a global clustering
process at the server to generalize knowledge across tasks, followed by the
optimization of aggregation weights across clusters (inter-cluster weights) and
within each cluster (intra-cluster weights) to personalize knowledge transfer
for each new task. To facilitate more effective learning of the inter- and
intra-cluster weights, we adopt a bi-level optimization scheme that
collaboratively learns the global intra-cluster weights across clients and
optimizes the local inter-cluster weights toward each client's task objective.
Extensive experiments on three benchmark datasets of different modalities,
including dermatology, chest X-rays, and retinal OCT, demonstrate FedKEI's
advantage in adapting to new diseases compared to state-of-the-art methods.

</details>


### [238] [A Vision-Language Pre-training Model-Guided Approach for Mitigating Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2508.10315)
*Keke Gai,Dongjue Wang,Jing Yu,Liehuang Zhu,Qi Wu*

Main category: cs.LG

TL;DR: 提出了一种名为CLIP-Fed的联邦学习反后门攻击框架，通过结合预聚合和后聚合策略，解决了非独立同分布数据对防御效果的限制，并采用多模态大语言模型和频率分析构建服务器数据集，无需客户样本，用对比损失和KL散度对齐知识。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设客户端数据同分布或需要干净数据集，这在面对非独立同分布数据时效果和实用性受限。

Method: 提出CLIP-Fed框架，利用视觉语言预训练模型的零样本学习能力，结合预聚合和后聚合防御策略，并通过增强的服务器数据集对齐全局模型和CLIP的知识。

Result: 通过实验验证，在CIFAR-10和CIFAR-10-LT数据集上，分别将ASR下降了2.03%和1.35%，MA提升了7.92%和0.48%。

Conclusion: CLIP-Fed有效提升了在非独立同分布数据场景下的反后门攻击能力，同时不损失模型性能。

Abstract: Existing backdoor defense methods in Federated Learning (FL) rely on the
assumption of homogeneous client data distributions or the availability of a
clean serve dataset, which limits the practicality and effectiveness. Defending
against backdoor attacks under heterogeneous client data distributions while
preserving model performance remains a significant challenge. In this paper, we
propose a FL backdoor defense framework named CLIP-Fed, which leverages the
zero-shot learning capabilities of vision-language pre-training models. By
integrating both pre-aggregation and post-aggregation defense strategies,
CLIP-Fed overcomes the limitations of Non-IID imposed on defense effectiveness.
To address privacy concerns and enhance the coverage of the dataset against
diverse triggers, we construct and augment the server dataset using the
multimodal large language model and frequency analysis without any client
samples. To address class prototype deviations caused by backdoor samples and
eliminate the correlation between trigger patterns and target labels, CLIP-Fed
aligns the knowledge of the global model and CLIP on the augmented dataset
using prototype contrastive loss and Kullback-Leibler divergence. Extensive
experiments on representative datasets validate the effectiveness of CLIP-Fed.
Compared to state-of-the-art methods, CLIP-Fed achieves an average reduction in
ASR, i.e., 2.03\% on CIFAR-10 and 1.35\% on CIFAR-10-LT, while improving
average MA by 7.92\% and 0.48\%, respectively.

</details>


### [239] [Welfare-Centric Clustering](https://arxiv.org/abs/2508.10345)
*Claire Jie Zhang,Seyed A. Esmaeili,Jamie Morgenstern*

Main category: cs.LG

TL;DR: 本文提出基于群体效用的公平聚类方法，优化Rawlsian和Utilitarian目标，并通过理论保证和实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决现有公平聚类方法可能导致的不直观和不理想结果问题，提出具有更合理效用考量的聚类方法。

Method: 从距离和比例代表性两个方面建模群体效用，提出两种福利中心的优化目标（Rawlsian和Utilitarian），并设计相应的新算法，提供理论保证。

Result: 在多个真实数据集上实验表明，这些新方法显著优于现有的公平聚类基线方法。

Conclusion: 新提出的基于福利的公平聚类方法能够更好地平衡群体效用，展示出理论和实践上的优越性，为公平聚类领域提供新的视角和方法。

Abstract: Fair clustering has traditionally focused on ensuring equitable group
representation or equalizing group-specific clustering costs. However,
Dickerson et al. (2025) recently showed that these fairness notions may yield
undesirable or unintuitive clustering outcomes and advocated for a
welfare-centric clustering approach that models the utilities of the groups. In
this work, we model group utilities based on both distances and proportional
representation and formalize two optimization objectives based on
welfare-centric clustering: the Rawlsian (Egalitarian) objective and the
Utilitarian objective. We introduce novel algorithms for both objectives and
prove theoretical guarantees for them. Empirical evaluations on multiple
real-world datasets demonstrate that our methods significantly outperform
existing fair clustering baselines.

</details>


### [240] [A Hierarchical IDS for Zero-Day Attack Detection in Internet of Medical Things Networks](https://arxiv.org/abs/2508.10346)
*Md Ashraf Uddin,Nam H. Chu,Reza Rafeh*

Main category: cs.LG

TL;DR: 提出了一种分层框架的多级IoMT入侵检测系统（IDS），通过Edge端启用初步检测，逐步到Cloud端完成攻击分类和新威胁识别。


<details>
  <summary>Details</summary>
Motivation: 现有的集中式IDS因设备资源限制、数据隐私问题及响应延迟，在IoMT环境中无法高效检测网络攻击，如零日攻击、勒索软件和数据劫持等问题。

Method: 引入一个多层次的IDS框架，第一层利用meta-learning或单类分类（OCC）算法usfAD在边缘层检测攻击，后续层基于远边缘和云完成攻击类型分类和新型威胁的确认。实验使用CICIoMT2024数据集。

Result: 在CICIoMT2024数据集上框架实现了99.77%的准确率和97.8%的F1分数；零日攻击也在第一层被高效捕获，无需新的数据集。

Conclusion: 该多级框架为IoMT环境提供了高效零日攻击及网络威胁检测能力，特别适用于资源受限和隐私敏感的医疗物联网场景。

Abstract: The Internet of Medical Things (IoMT) is driving a healthcare revolution but
remains vulnerable to cyberattacks such as denial of service, ransomware, data
hijacking, and spoofing. These networks comprise resource constrained,
heterogeneous devices (e.g., wearable sensors, smart pills, implantables),
making traditional centralized Intrusion Detection Systems (IDSs) unsuitable
due to response delays, privacy risks, and added vulnerabilities. Centralized
IDSs require all sensors to transmit data to a central server, causing delays
or network disruptions in dense environments. Running IDSs locally on IoMT
devices is often infeasible due to limited computation, and even lightweight
IDS components remain at risk if updated models are delayed leaving them
exposed to zero-day attacks that threaten patient health and data security. We
propose a multi level IoMT IDS framework capable of detecting zero day attacks
and distinguishing between known and unknown threats. The first layer (near
Edge) filters traffic at a coarse level (attack or not) using meta-learning or
One Class Classification (OCC) with the usfAD algorithm. Subsequent layers (far
Edge, Cloud) identify attack type and novelty. Experiments on the CICIoMT2024
dataset show 99.77 percentage accuracy and 97.8 percentage F1-score. The first
layer detects zero-day attacks with high accuracy without needing new datasets,
ensuring strong applicability in IoMT environments. Additionally, the
meta-learning approach achieves high.

</details>


### [241] [Semantic Communication with Distribution Learning through Sequential Observations](https://arxiv.org/abs/2508.10350)
*Samer Lahoud,Kinda Khawam*

Main category: cs.LG

TL;DR: 本文探讨语义通信中的分布学习，提出了源统计的学习条件和语义失真的量化方法，并通过实验验证理论框架。


<details>
  <summary>Details</summary>
Motivation: 传统通信注重比特的准确传输，而语义通信更关注意义的传递，但在缺乏先验的情况下如何学习源分布仍是一个挑战。

Method: 通过分析学习条件、收敛速度、估计误差与语义失真间的关系，研究了语义通信中分布学习的基础理论，并使用CIFAR-10实验验证理论结果。

Result: 提出了学习源分布所需的全秩矩阵条件，分析了分布估计的收敛速度以及编码方案在语义表现与学习性之间的权衡。实验表明系统条件显著影响学习速率和性能。

Conclusion: 首次系统地刻画了语义通信中的统计学习，并提出了在即时性能与适应能力之间权衡的设计原则。

Abstract: Semantic communication aims to convey meaning rather than bit-perfect
reproduction, representing a paradigm shift from traditional communication.
This paper investigates distribution learning in semantic communication where
receivers must infer the underlying meaning distribution through sequential
observations. While semantic communication traditionally optimizes individual
meaning transmission, we establish fundamental conditions for learning source
statistics when priors are unknown. We prove that learnability requires full
rank of the effective transmission matrix, characterize the convergence rate of
distribution estimation, and quantify how estimation errors translate to
semantic distortion. Our analysis reveals a fundamental trade-off: encoding
schemes optimized for immediate semantic performance often sacrifice long-term
learnability. Experiments on CIFAR-10 validate our theoretical framework,
demonstrating that system conditioning critically impacts both learning rate
and achievable performance. These results provide the first rigorous
characterization of statistical learning in semantic communication and offer
design principles for systems that balance immediate performance with
adaptation capability.

</details>


### [242] [eMamba: Efficient Acceleration Framework for Mamba Models in Edge Computing](https://arxiv.org/abs/2508.10370)
*Jiyong Kim,Jaeho Lee,Jiahao Lin,Alish Kanani,Miao Sun,Umit Y. Ogras,Jaehyun Park*

Main category: cs.LG

TL;DR: 提出了一种名为eMamba的硬件加速框架，用于边缘环境中Mamba模型的高效部署，实现了硬件资源与计算效率的优化，同时保持了竞争性的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的Mamba模型在计算效率和准确性上具有优势，但缺乏针对资源受限的边缘设备优化的硬件加速框架。

Method: 通过替换复杂的归一化层为轻量级硬件友好型版本，并近似昂贵运算（如SiLU激活和指数运算），结合近似感知神经结构搜索（NAS）来优化可学习参数。

Result: 在Fashion-MNIST、CIFAR-10、MARS等数据集上，eMamba在参数使用上显著减少的同时保持了相当的准确率；在WikiText2数据集上对大规模任务展示了良好的泛化性能；在AMD ZCU102 FPGA和GF 22nm ASIC部署中表现出显著的延迟、吞吐量、面积、功耗及能耗性能提升。

Conclusion: eMamba框架有效解决了Mamba模型在边缘设备上的硬件加速问题，同时在计算效率与硬件资源使用之间实现了良好的平衡，适合边缘平台部署。

Abstract: State Space Model (SSM)-based machine learning architectures have recently
gained significant attention for processing sequential data. Mamba, a recent
sequence-to-sequence SSM, offers competitive accuracy with superior
computational efficiency compared to state-of-the-art transformer models. While
this advantage makes Mamba particularly promising for resource-constrained edge
devices, no hardware acceleration frameworks are currently optimized for
deploying it in such environments. This paper presents eMamba, a comprehensive
end-to-end hardware acceleration framework explicitly designed for deploying
Mamba models on edge platforms. eMamba maximizes computational efficiency by
replacing complex normalization layers with lightweight hardware-aware
alternatives and approximating expensive operations, such as SiLU activation
and exponentiation, considering the target applications. Then, it performs an
approximation-aware neural architecture search (NAS) to tune the learnable
parameters used during approximation. Evaluations with Fashion-MNIST, CIFAR-10,
and MARS, an open-source human pose estimation dataset, show eMamba achieves
comparable accuracy to state-of-the-art techniques using 1.63-19.9$\times$
fewer parameters. In addition, it generalizes well to large-scale natural
language tasks, demonstrating stable perplexity across varying sequence lengths
on the WikiText2 dataset. We also quantize and implement the entire eMamba
pipeline on an AMD ZCU102 FPGA and ASIC using GlobalFoundries (GF) 22 nm
technology. Experimental results show 4.95-5.62$\times$ lower latency and
2.22-9.95$\times$ higher throughput, with 4.77$\times$ smaller area,
9.84$\times$ lower power, and 48.6$\times$ lower energy consumption than
baseline solutions while maintaining competitive accuracy.

</details>


### [243] [A Unified Evaluation Framework for Multi-Annotator Tendency Learning](https://arxiv.org/abs/2508.10393)
*Liyun Zhang,Jingcheng Ke,Shenli Fan,Xuanmeng Sha,Zheng Lian*

Main category: cs.LG

TL;DR: 提出了一个评估框架和两种新指标，用于验证多注释者倾向学习是否真实捕捉个体倾向并提供解释性分析。


<details>
  <summary>Details</summary>
Motivation: 多注释者学习中需要一种方法评估个体倾向学习模型是否真正捕捉了注释者个体倾向并解释其行为。

Method: 提出了首个统一评估框架，包含两个新指标：DIC用于量化模型的注释者倾向捕捉能力，BAE用于评估模型解释与注释者行为的一致性。

Result: 实验验证了所提评估框架的有效性。

Conclusion: 所提框架能够用于多注释者学习中倾向捕捉和行为解释的评估。

Abstract: Recent works have emerged in multi-annotator learning that shift focus from
Consensus-oriented Learning (CoL), which aggregates multiple annotations into a
single ground-truth prediction, to Individual Tendency Learning (ITL), which
models annotator-specific labeling behavior patterns (i.e., tendency) to
provide explanation analysis for understanding annotator decisions. However, no
evaluation framework currently exists to assess whether ITL methods truly
capture individual tendencies and provide meaningful behavioral explanations.
To address this gap, we propose the first unified evaluation framework with two
novel metrics: (1) Difference of Inter-annotator Consistency (DIC) quantifies
how well models capture annotator tendencies by comparing predicted
inter-annotator similarity structures with ground-truth; (2) Behavior Alignment
Explainability (BAE) evaluates how well model explanations reflect annotator
behavior and decision relevance by aligning explainability-derived with
ground-truth labeling similarity structures via Multidimensional Scaling (MDS).
Extensive experiments validate the effectiveness of our proposed evaluation
framework.

</details>


### [244] [XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization](https://arxiv.org/abs/2508.10395)
*Aditya Tomar,Coleman Hooper,Minjae Lee,Haocheng Xi,Rishabh Tiwari,Wonjun Kang,Luca Manolache,Michael W. Mahoney,Kurt Keutzer,Amir Gholami*

Main category: cs.LG

TL;DR: 提出了一种名为XQuant的算法，通过对LLM推理过程中的层输入激活值进行低比特量化，显著减少了内存消耗，同时保持了较高的精度表现。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理面临着内存占用和带宽需求迅速增长的挑战，而计算能力与内存容量及带宽的不匹配问题加剧了这一困难，迫切需要新算法通过更多计算换取更少内存操作。

Method: 通过对层输入激活值X进行低比特量化和缓存，并在推理过程中按需重新生成键值对，从而实现内存占用的显著减少，同时利用X值在不同层之间的相似性提出了XQuant-CL方法，实现更高级别的压缩。

Result: XQuant实现了相较FP16基线最多达7.7倍的内存节省且困惑度降低小于0.1；XQuant-CL利用跨层相似性进一步实现最高10倍内存节省，仅减少0.01困惑度，甚至在12.5倍节省下困惑度仅略降到0.1。

Conclusion: XQuant充分利用硬件计算能力增长趋势，从根本上解决了内存瓶颈问题，在多个模型上超越了现有KV缓存量化方法，并接近FP16级别精度。

Abstract: Although LLM inference has emerged as a critical workload for many downstream
applications, efficiently inferring LLMs is challenging due to the substantial
memory footprint and bandwidth requirements. In parallel, compute capabilities
have steadily outpaced both memory capacity and bandwidth over the last few
decades, a trend that remains evident in modern GPU hardware and exacerbates
the challenge of LLM inference. As such, new algorithms are emerging that trade
increased computation for reduced memory operations. To that end, we present
XQuant, which takes advantage of this trend, enabling an order-of-magnitude
reduction in memory consumption through low-bit quantization with substantial
accuracy benefits relative to state-of-the-art KV cache quantization methods.
We accomplish this by quantizing and caching the layer input activations X,
instead of using standard KV caching, and then rematerializing the Keys and
Values on-the-fly during inference. This results in an immediate 2$\times$
memory savings compared to KV caching. By applying XQuant, we achieve up to
$\sim 7.7\times$ memory savings with $<0.1$ perplexity degradation compared to
the FP16 baseline. Furthermore, our approach leverages the fact that X values
are similar across layers. Building on this observation, we introduce
XQuant-CL, which exploits the cross-layer similarity in the X embeddings for
extreme compression. Across different models, XQuant-CL attains up to
10$\times$ memory savings relative to the FP16 baseline with only 0.01
perplexity degradation, and 12.5$\times$ memory savings with only $0.1$
perplexity degradation. XQuant exploits the rapidly increasing compute
capabilities of hardware platforms to eliminate the memory bottleneck, while
surpassing state-of-the-art KV cache quantization methods and achieving
near-FP16 accuracy across a wide range of models.

</details>


### [245] [SC2Arena and StarEvolve: Benchmark and Self-Improvement Framework for LLMs in Complex Decision-Making Tasks](https://arxiv.org/abs/2508.10428)
*Pengbo Shen,Yaqing Wang,Ni Mu,Yao Luan,Runpeng Xie,Senhao Yang,Lexiang Wang,Hao Hu,Shuang Xu,Yiqin Yang,Bo Xu*

Main category: cs.LG

TL;DR: 本文提出了一种名为SC2Arena的新基准，用于全面评估大语言模型在复杂任务中的表现，并引入StarEvolve框架结合战略规划与战术执行，实验表明取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 现有如星际争霸II的任务基准无法全面涵盖游戏复杂性，包括上下文、行动空间与全部玩法种族。

Method: 开发SC2Arena基准，全面支持种族与低级行动空间，并优化基于文本的观察，同时引入StarEvolve框架整合战略规划与战术执行，包括计划器-执行器-校验器结构和高质量数据微调机制。

Result: 实验分析表明SC2Arena基准提供了前所未有的见解，而StarEvolve框架在战略规划中表现优异。

Conclusion: SC2Arena及StarEvolve进一步推动了泛化性人工智能体的发展，其代码、环境和算法均已开放共享，促进了复杂决策领域的先进研究进展。

Abstract: Evaluating large language models (LLMs) in complex decision-making is
essential for advancing AI's ability for strategic planning and real-time
adaptation. However, existing benchmarks for tasks like StarCraft II fail to
capture the game's full complexity, such as its complete game context, diverse
action spaces, and all playable races. To address this gap, we present
SC2Arena, a benchmark that fully supports all playable races, low-level action
spaces, and optimizes text-based observations to tackle spatial reasoning
challenges. Complementing this, we introduce StarEvolve, a hierarchical
framework that integrates strategic planning with tactical execution, featuring
iterative self-correction and continuous improvement via fine-tuning on
high-quality gameplay data. Its key components include a
Planner-Executor-Verifier structure to break down gameplay, and a scoring
system for selecting high-quality training samples. Comprehensive analysis
using SC2Arena provides valuable insights into developing generalist agents
that were not possible with previous benchmarks. Experimental results also
demonstrate that our proposed StarEvolve achieves superior performance in
strategic planning. Our code, environment, and algorithms are publicly
available.

</details>


### [246] [Unpacking the Implicit Norm Dynamics of Sharpness-Aware Minimization in Tensorized Models](https://arxiv.org/abs/2508.10435)
*Tianxiao Cao,Kyohei Atarashi,Hisashi Kashima*

Main category: cs.LG

TL;DR: 本文探讨了尖锐化感知最优化（SAM）在一般张量化或尺度不变模型中的行为，并提出了一种新的方法——偏差感知缩放（DAS），在多种任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究SAM在更为普遍的模型中行为，以深入理解其内在的正则化机制，并改善其计算效率和性能。

Method: 提出“Norm Deviation”的概念作为衡量核心范数不平衡的全局指标，使用梯度流分析推导其在SAM下的演进，并基于发现提出偏差感知缩放（DAS）。

Result: 通过实验证明DAS在张量补全、噪声训练、模型压缩以及参数高效微调等任务中性能优异，并显著降低计算开销。

Conclusion: DAS能够在保持或提升性能的同时，相较于原始SAM减少计算成本，是对尖锐化感知最优化的有效改进。

Abstract: Sharpness-Aware Minimization (SAM) has been proven to be an effective
optimization technique for improving generalization in overparameterized
models. While prior works have explored the implicit regularization of SAM in
simple two-core scale-invariant settings, its behavior in more general
tensorized or scale-invariant models remains underexplored. In this work, we
leverage scale-invariance to analyze the norm dynamics of SAM in general
tensorized models. We introduce the notion of \emph{Norm Deviation} as a global
measure of core norm imbalance, and derive its evolution under SAM using
gradient flow analysis. We show that SAM's implicit control of Norm Deviation
is governed by the covariance between core norms and their gradient magnitudes.
Motivated by these findings, we propose a simple yet effective method,
\emph{Deviation-Aware Scaling (DAS)}, which explicitly mimics this
regularization behavior by scaling core norms in a data-adaptive manner. Our
experiments across tensor completion, noisy training, model compression, and
parameter-efficient fine-tuning confirm that DAS achieves competitive or
improved performance over SAM, while offering reduced computational overhead.

</details>


### [247] [RealAC: A Domain-Agnostic Framework for Realistic and Actionable Counterfactual Explanations](https://arxiv.org/abs/2508.10455)
*Asiful Arefeen,Shovito Barua Soumma,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: 本文提出了RealAC，一种生成现实且可操作反事实解释的框架，无需依赖明确的领域知识，通过特征关联分布对齐来自动保持复杂的特征间依赖。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往依赖刻板的手工约束或领域知识限制，很难适应一般性需求，并难以捕获数据中复杂的非线性关系。因此，本文旨在开发一种通用框架，生成既真实又可操作的反事实解释。

Method: 提出RealAC框架，通过对齐事实与反事实实例特征对之间的联合分布，实现特征间复杂依赖的自动保留，同时允许用户定义不可更改特征的偏好。

Result: 经过三组合成数据和两组真实数据的评估，RealAC在因果边缘分数、依赖性保持分数和现实性指标（IM1）等方面优于现有方法，并展现出因果意识和用户中心的反事实生成能力。

Conclusion: RealAC框架实现了生成真实且可行动性强的反事实解释，增强了用户定制性和广泛适用性，为因果敏感的反事实生成提供了有效解决方案。

Abstract: Counterfactual explanations provide human-understandable reasoning for
AI-made decisions by describing minimal changes to input features that would
alter a model's prediction. To be truly useful in practice, such explanations
must be realistic and feasible -- they should respect both the underlying data
distribution and user-defined feasibility constraints. Existing approaches
often enforce inter-feature dependencies through rigid, hand-crafted
constraints or domain-specific knowledge, which limits their generalizability
and ability to capture complex, nonlinear relations inherent in data. Moreover,
they rarely accommodate user-specified preferences and suggest explanations
that are causally implausible or infeasible to act upon. We introduce RealAC, a
domain-agnostic framework for generating realistic and actionable
counterfactuals. RealAC automatically preserves complex inter-feature
dependencies without relying on explicit domain knowledge -- by aligning the
joint distributions of feature pairs between factual and counterfactual
instances. The framework also allows end-users to ``freeze'' attributes they
cannot or do not wish to change by suppressing change in frozen features during
optimization. Evaluations on three synthetic and two real datasets demonstrate
that RealAC balances realism with actionability. Our method outperforms
state-of-the-art baselines and Large Language Model-based counterfactual
generation techniques in causal edge score, dependency preservation score, and
IM1 realism metric and offers a solution for causality-aware and user-centric
counterfactual generation.

</details>


### [248] [X-Node: Self-Explanation is All We Need](https://arxiv.org/abs/2508.10461)
*Prajit Sengupta,Islem Rekik*

Main category: cs.LG

TL;DR: 本论文提出了一个名为X-Node的自解释图神经网络框架，用于生成节点级别的解释，具有竞争力的准确性并实现了对每个节点决策的解释。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络虽然在视觉和医学图像分类中表现出色，但其决策过程缺乏可解释性，尤其是在高风险的临床环境中。

Method: 引入X-Node框架，结合结构化的上下文向量和轻量化的Reasoner模块，生成节点层面的自然语言解释，同时通过"文本注入"机制将解释融入图神经网络的消息传递过程。

Result: X-Node在两个数据集（MedMNIST和MorphoMNIST）上的结果表明，该方法在保持分类准确性的同时，还能够生成可信的节点级别解释。

Conclusion: X-Node框架有效实现了图神经网络决策的可解释性，有助于提升其在临床等高需求场景中的可信度。

Abstract: Graph neural networks (GNNs) have achieved state-of-the-art results in
computer vision and medical image classification tasks by capturing structural
dependencies across data instances. However, their decision-making remains
largely opaque, limiting their trustworthiness in high-stakes clinical
applications where interpretability is essential. Existing explainability
techniques for GNNs are typically post-hoc and global, offering limited insight
into individual node decisions or local reasoning. We introduce X-Node, a
self-explaining GNN framework in which each node generates its own explanation
as part of the prediction process. For every node, we construct a structured
context vector encoding interpretable cues such as degree, centrality,
clustering, feature saliency, and label agreement within its local topology. A
lightweight Reasoner module maps this context into a compact explanation
vector, which serves three purposes: (1) reconstructing the node's latent
embedding via a decoder to enforce faithfulness, (2) generating a natural
language explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3)
guiding the GNN itself via a "text-injection" mechanism that feeds explanations
back into the message-passing pipeline. We evaluate X-Node on two graph
datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT,
and GIN backbones. Our results show that X-Node maintains competitive
classification accuracy while producing faithful, per-node explanations.
Repository: https://github.com/basiralab/X-Node.

</details>


### [249] [GraphFedMIG: Tackling Class Imbalance in Federated Graph Learning via Mutual Information-Guided Generation](https://arxiv.org/abs/2508.10471)
*Xinrui Li,Qilin Fan,Tianfu Wang,Kaiwen Wei,Ke Yu,Xu Zhang*

Main category: cs.LG

TL;DR: 本文提出了GraphFedMIG框架，通过引入生成式对抗网络和互信息指导机制，解决了联邦图学习中的非独立同分布数据和类别不平衡问题，并在四个真实世界数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦图学习中由于类别不平衡和非独立同分布数据导致模型性能下降的问题。

Method: 提出了GraphFedMIG，一种包含生成式对抗网络的框架，每个客户端使用本地生成器生成高保真特征表示，通过分组共享鉴别器并通过互信息指导机制优化生成器参数，从而强化少数类别特征的生成。

Result: 通过四个真实数据集的大量实验，GraphFedMIG框架的表现优于现有基线方法。

Conclusion: GraphFedMIG为联邦图学习提供了一个有效的解决方案，通过创新的框架设计显著提高了模型在类别不平衡和数据异质性情况下的性能。

Abstract: Federated graph learning (FGL) enables multiple clients to collaboratively
train powerful graph neural networks without sharing their private,
decentralized graph data. Inherited from generic federated learning, FGL is
critically challenged by statistical heterogeneity, where non-IID data
distributions across clients can severely impair model performance. A
particularly destructive form of this is class imbalance, which causes the
global model to become biased towards majority classes and fail at identifying
rare but critical events. This issue is exacerbated in FGL, as nodes from a
minority class are often surrounded by biased neighborhood information,
hindering the learning of expressive embeddings. To grapple with this
challenge, we propose GraphFedMIG, a novel FGL framework that reframes the
problem as a federated generative data augmentation task. GraphFedMIG employs a
hierarchical generative adversarial network where each client trains a local
generator to synthesize high-fidelity feature representations. To provide
tailored supervision, clients are grouped into clusters, each sharing a
dedicated discriminator. Crucially, the framework designs a mutual
information-guided mechanism to steer the evolution of these client generators.
By calculating each client's unique informational value, this mechanism
corrects the local generator parameters, ensuring that subsequent rounds of
mutual information-guided generation are focused on producing high-value,
minority-class features. We conduct extensive experiments on four real-world
datasets, and the results demonstrate the superiority of the proposed
GraphFedMIG compared with other baselines.

</details>


### [250] [EDAPT: Towards Calibration-Free BCIs with Continual Online Adaptation](https://arxiv.org/abs/2508.10474)
*Lisa Haxel,Jaivardhan Kapoor,Ulf Ziemann,Jakob H. Macke*

Main category: cs.LG

TL;DR: 提出了一种名为EDAPT的框架，旨在解决脑机接口中由于神经信号漂移引起的准确性下降问题，避免频繁校准需求。


<details>
  <summary>Details</summary>
Motivation: 解决脑机接口中神经信号随时间漂移和用户间差异问题，无需频繁校准以推动实际部署。

Method: 提出EDAPT框架，首先通过多用户数据训练基线解码器，然后通过监督微调持续个性化模型以适应变化的神经模式。

Result: EDAPT在三个脑机接口任务的九个数据集上均显著提升准确性，结合群体级预训练和在线微调，以及部分数据集上的无监督域适配实现进一步改进。

Conclusion: EDAPT实现了高效的模型更新（200毫秒内），使得解码准确性与总数据预算相关，为无校准脑机接口提供了一种实用解决方案，减少了部署的主要障碍。

Abstract: Brain-computer interfaces (BCIs) suffer from accuracy degradation as neural
signals drift over time and vary across users, requiring frequent recalibration
that limits practical deployment. We introduce EDAPT, a task- and
model-agnostic framework that eliminates calibration through continual model
adaptation. EDAPT first trains a baseline decoder using data from multiple
users, then continually personalizes this model via supervised finetuning as
the neural patterns evolve during use. We tested EDAPT across nine datasets
covering three BCI tasks, and found that it consistently improved accuracy over
conventional, static methods. These improvements primarily stem from combining
population-level pretraining and online continual finetuning, with unsupervised
domain adaptation providing further gains on some datasets. EDAPT runs
efficiently, updating models within 200 milliseconds on consumer-grade
hardware. Finally, decoding accuracy scales with total data budget rather than
its allocation between subjects and trials. EDAPT provides a practical pathway
toward calibration-free BCIs, reducing a major barrier to BCI deployment.

</details>


### [251] [Confounding is a Pervasive Problem in Real World Recommender Systems](https://arxiv.org/abs/2508.10479)
*Alexander Merkov,David Rohde,Alexandre Gilotte,Benjamin Heymann*

Main category: cs.LG

TL;DR: 本文讨论了推荐系统中因标准实践所引入的混杂问题，并提出减轻影响的方法。


<details>
  <summary>Details</summary>
Motivation: 未观测的混杂因素可能导致因果效应估计的偏差，该问题在包括推荐系统在内的众多领域存在严重影响。

Method: 通过模拟研究和提供实际建议，展示推荐系统中常见实践如特征工程、A/B 测试及模块化如何引入混杂，并探讨缓解方法。

Result: 通过多个实例展示了混杂对推荐系统性能的影响，并提出了减少或避免混杂影响的实用建议。

Conclusion: 推荐系统标准实践可能引入混杂因素，需加以重视并采取适当措施以减少影响。

Abstract: Unobserved confounding arises when an unmeasured feature influences both the
treatment and the outcome, leading to biased causal effect estimates. This
issue undermines observational studies in fields like economics, medicine,
ecology or epidemiology. Recommender systems leveraging fully observed data
seem not to be vulnerable to this problem. However many standard practices in
recommender systems result in observed features being ignored, resulting in
effectively the same problem. This paper will show that numerous common
practices such as feature engineering, A/B testing and modularization can in
fact introduce confounding into recommendation systems and hamper their
performance. Several illustrations of the phenomena are provided, supported by
simulation studies with practical suggestions about how practitioners may
reduce or avoid the affects of confounding in real systems.

</details>


### [252] [Pinet: Optimizing hard-constrained neural networks with orthogonal projection layers](https://arxiv.org/abs/2508.10480)
*Panagiotis D. Grontas,Antonio Terpin,Efe C. Balta,Raffaello D'Andrea,John Lygeros*

Main category: cs.LG

TL;DR: 论文介绍了一种名为$\Pi$net的神经网络输出层，用以确保满足凸约束条件，且在多车运动规划领域取得了不错的效果。


<details>
  <summary>Details</summary>
Motivation: 当前优化求解器在处理带参数的约束优化问题时存在效率和鲁棒性问题，需要新的方法来提高求解速度并适应批量问题。

Method: 提出$\Pi$net方法，通过运算符分裂实现快速投影，利用隐函数定理完成反向传播，并将其主要应用于约束优化和非凸多车路径规划中。

Result: 单问题求解中能比传统方法快速得到中等精度解；面对成批问题时更显著提升效率；并在训练时间、解的质量和超参数调节鲁棒性上优于现有学习方法，同时推理时间相当。

Conclusion: $\Pi$net在解决参数化约束优化和多车运动规划方面表现出优越的效率和鲁棒性，同时保持了良好的推理时间表现。

Abstract: We introduce an output layer for neural networks that ensures satisfaction of
convex constraints. Our approach, $\Pi$net, leverages operator splitting for
rapid and reliable projections in the forward pass, and the implicit function
theorem for backpropagation. We deploy $\Pi$net as a feasible-by-design
optimization proxy for parametric constrained optimization problems and obtain
modest-accuracy solutions faster than traditional solvers when solving a single
problem, and significantly faster for a batch of problems. We surpass
state-of-the-art learning approaches in terms of training time, solution
quality, and robustness to hyperparameter tuning, while maintaining similar
inference times. Finally, we tackle multi-vehicle motion planning with
non-convex trajectory preferences and provide $\Pi$net as a GPU-ready package
implemented in JAX with effective tuning heuristics.

</details>


### [253] [Learning State-Space Models of Dynamic Systems from Arbitrary Data using Joint Embedding Predictive Architectures](https://arxiv.org/abs/2508.10489)
*Jonas Ulmen,Ganesh Sundaram,Daniel Görges*

Main category: cs.LG

TL;DR: 介绍了一种结合序列嵌入和神经常微分方程（neural ODEs）的新方法，用于构建基于连续时间动态系统的世界模型。


<details>
  <summary>Details</summary>
Motivation: 希望突破基于重构方法的局限性，通过引入JEPAs开发更强大的世界模型，用于广泛的机器人控制算法应用。

Method: 将序列嵌入与神经ODEs相结合，并通过损失函数约束状态空间和嵌入的收缩及Lipschitz连续性，创建有组织的潜在状态空间。

Result: 通过单摆系统的实验，使用仅提取的图像数据生成了结构化的潜在状态空间模型。

Conclusion: 证明了新方法的有效性，为构建通用控制算法和估计技术提供了新可能，并在机器人领域具有广泛应用。

Abstract: With the advent of Joint Embedding Predictive Architectures (JEPAs), which
appear to be more capable than reconstruction-based methods, this paper
introduces a novel technique for creating world models using continuous-time
dynamic systems from arbitrary observation data. The proposed method integrates
sequence embeddings with neural ordinary differential equations (neural ODEs).
It employs loss functions that enforce contractive embeddings and Lipschitz
constants in state transitions to construct a well-organized latent state
space. The approach's effectiveness is demonstrated through the generation of
structured latent state-space models for a simple pendulum system using only
image data. This opens up a new technique for developing more general control
algorithms and estimation techniques with broad applications in robotics.

</details>


### [254] [On the Complexity-Faithfulness Trade-off of Gradient-Based Explanations](https://arxiv.org/abs/2508.10490)
*Amir Mehrpanah,Matteo Gamba,Kevin Smith,Hossein Azizpour*

Main category: cs.LG

TL;DR: 本文提出了一种统一的频谱框架分析和量化解释中的平滑性、忠实性及其权衡，并通过理论和实验验证这种框架的有效性。


<details>
  <summary>Details</summary>
Motivation: ReLU网络因预测时可能依赖单个像素，引发梯度解释噪声高、难以理解的问题，现有方法如GradCAM虽平滑但牺牲真实反映模型行为的忠实性，需要系统化方案分析两者间的权衡。

Method: 基于频谱分析，量化并正则化ReLU网络对高频信息的贡献，同时定义和度量由代理平滑方法引起的“解释差距”；通过理论分析与实验研究验证方法的可靠性。

Result: 框架准确刻画代理平滑方法如何扭曲解释，提出的频谱分析能够有效显现忠实性和平滑性的权衡，定义了解释差距并在多种设计选择和数据集上验证。

Conclusion: 评估和改进解释方法需要在忠实性和平滑性间权衡，本文框架提供了定量工具，并展现了其理论和实际有效性。

Abstract: ReLU networks, while prevalent for visual data, have sharp transitions,
sometimes relying on individual pixels for predictions, making vanilla
gradient-based explanations noisy and difficult to interpret. Existing methods,
such as GradCAM, smooth these explanations by producing surrogate models at the
cost of faithfulness. We introduce a unifying spectral framework to
systematically analyze and quantify smoothness, faithfulness, and their
trade-off in explanations. Using this framework, we quantify and regularize the
contribution of ReLU networks to high-frequency information, providing a
principled approach to identifying this trade-off. Our analysis characterizes
how surrogate-based smoothing distorts explanations, leading to an
``explanation gap'' that we formally define and measure for different post-hoc
methods. Finally, we validate our theoretical findings across different design
choices, datasets, and ablations.

</details>


### [255] [Contrastive ECOC: Learning Output Codes for Adversarial Defense](https://arxiv.org/abs/2508.10491)
*Che-Yu Chou,Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: 提出了三种基于对比学习的自动化编码本学习模型，可直接从数据中学习编码本，实验表明模型对抗攻击的鲁棒性优于两种基准。


<details>
  <summary>Details</summary>
Motivation: 传统ECOC方法依赖手工或随机生成的编码本，耗时且可能导出次优结果，亟需开发自动化、数据自适应的编码本学习方法。

Method: 基于对比学习提出三种自动化编码本学习模型，利用数据直接自适应学习编码本。

Result: 在四个数据集上，模型表现出对抗攻击的更强鲁棒性，相较于两种基准模型表现优异。

Conclusion: 自动化编码本学习方法具有潜在可行性，为多类分类问题提供了更鲁棒的解决方案。

Abstract: Although one-hot encoding is commonly used for multiclass classification, it
is not always the most effective encoding mechanism. Error Correcting Output
Codes (ECOC) address multiclass classification by mapping each class to a
unique codeword used as a label. Traditional ECOC methods rely on manually
designed or randomly generated codebooks, which are labor-intensive and may
yield suboptimal, dataset-agnostic results. This paper introduces three models
for automated codebook learning based on contrastive learning, allowing
codebooks to be learned directly and adaptively from data. Across four
datasets, our proposed models demonstrate superior robustness to adversarial
attacks compared to two baselines. The source is available at
https://github.com/YuChou20/Automated-Codebook-Learning-with-Error-Correcting-Output-Code-Technique.

</details>


### [256] [A Unified Multi-Agent Framework for Universal Multimodal Understanding and Generation](https://arxiv.org/abs/2508.10494)
*Jiulin Li,Ping Huang,Yexin Li,Shuo Chen,Juewen Hu,Ye Tian*

Main category: cs.LG

TL;DR: MAGUS是一个模型框架，可以在模块化的基础上实现多模态的理解和生成能力，表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 为解决多模态应用中理解和生成的难题，尤其是结合LLMs的推理能力与扩散模型的生成能力，并提升灵活性与扩展性。

Method: 提出MAGUS框架，通过认知阶段（Cognition）和审议阶段（Deliberation）分离的结构，使多模态LLM进行角色协作并结合扩散模型，实现灵活的多模态转换和语义对齐。

Result: 在图像、视频、音频生成以及跨模态任务中，MAGUS在多个基准测试中表现优异，并且在MME基准上超过了GPT-4o。

Conclusion: MAGUS在无需联合训练的情况下，通过模块化设计成功实现了顶级的多模态处理性能，是具有高度扩展性和灵活性的先进模型框架。

Abstract: Real-world multimodal applications often require any-to-any capabilities,
enabling both understanding and generation across modalities including text,
image, audio, and video. However, integrating the strengths of autoregressive
language models (LLMs) for reasoning and diffusion models for high-fidelity
generation remains challenging. Existing approaches rely on rigid pipelines or
tightly coupled architectures, limiting flexibility and scalability. We propose
MAGUS (Multi-Agent Guided Unified Multimodal System), a modular framework that
unifies multimodal understanding and generation via two decoupled phases:
Cognition and Deliberation. MAGUS enables symbolic multi-agent collaboration
within a shared textual workspace. In the Cognition phase, three
role-conditioned multimodal LLM agents - Perceiver, Planner, and Reflector -
engage in collaborative dialogue to perform structured understanding and
planning. The Deliberation phase incorporates a Growth-Aware Search mechanism
that orchestrates LLM-based reasoning and diffusion-based generation in a
mutually reinforcing manner. MAGUS supports plug-and-play extensibility,
scalable any-to-any modality conversion, and semantic alignment - all without
the need for joint training. Experiments across multiple benchmarks, including
image, video, and audio generation, as well as cross-modal instruction
following, demonstrate that MAGUS outperforms strong baselines and
state-of-the-art systems. Notably, on the MME benchmark, MAGUS surpasses the
powerful closed-source model GPT-4o.

</details>


### [257] [Nonlocal Monte Carlo via Reinforcement Learning](https://arxiv.org/abs/2508.10520)
*Dmitrii Dobrynin,Masoud Mohseni,John Paul Strachan*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度强化学习（RL）的非平衡非局部蒙特卡罗（NMC）优化算法，用于解决传统MCMC在某些组合优化问题上效率低下的问题，并在4-SAT基准测试上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统的基于马尔科夫链蒙特卡罗（MCMC）算法在组合优化问题中假设均匀的温度分布，但在某些情况下（如计算相变附近）表现不佳，尤其难以跳出次优解和采样高质量解。为缓解这些挑战，提出了具有非均匀温度分布的NMC算法，但其过渡策略此前为人工设计，存在优化空间。

Method: 使用深度强化学习（RL）来学习和优化NMC算法的非局部过渡策略。奖励信号为配置空间能量变化，状态为局部极小值能量景观几何特征。

Result: 此方法在随机4-SAT问题的硬实例测试中，明显优于传统MCMC以及非局部模拟退火方法，在剩余能量、解时间和解的多样性等指标上都有大幅提高。

Conclusion: 基于深度强化学习的解决方案有效优化了NMC算法的性能，并在困难的组合优化问题中展示出较高的效率和多样性，表明这一方法具有广泛的应用潜力。

Abstract: Optimizing or sampling complex cost functions of combinatorial optimization
problems is a longstanding challenge across disciplines and applications. When
employing family of conventional algorithms based on Markov Chain Monte Carlo
(MCMC) such as simulated annealing or parallel tempering, one assumes
homogeneous (equilibrium) temperature profiles across input. This instance
independent approach was shown to be ineffective for the hardest benchmarks
near a computational phase transition when the so-called overlap-gap-property
holds. In these regimes conventional MCMC struggles to unfreeze rigid
variables, escape suboptimal basins of attraction, and sample high-quality and
diverse solutions. In order to mitigate these challenges, Nonequilibrium
Nonlocal Monte Carlo (NMC) algorithms were proposed that leverage inhomogeneous
temperature profiles thereby accelerating exploration of the configuration
space without compromising its exploitation. Here, we employ deep reinforcement
learning (RL) to train the nonlocal transition policies of NMC which were
previously designed phenomenologically. We demonstrate that the resulting
solver can be trained solely by observing energy changes of the configuration
space exploration as RL rewards and the local minimum energy landscape geometry
as RL states. We further show that the trained policies improve upon the
standard MCMC-based and nonlocal simulated annealing on hard uniform random and
scale-free random 4-SAT benchmarks in terms of residual energy,
time-to-solution, and diversity of solutions metrics.

</details>


### [258] [Projected Coupled Diffusion for Test-Time Constrained Joint Generation](https://arxiv.org/abs/2508.10531)
*Hao Luan,Yi Xian Goh,See-Kiong Ng,Chun Kai Ling*

Main category: cs.LG

TL;DR: 提出了Projected Coupled Diffusion (PCD)，一种用于测试时约束联合生成的新框架，旨在多扩散模型中生成相关样本，同时满足约束条件，且无需昂贵的重新训练。


<details>
  <summary>Details</summary>
Motivation: 既要在多个预训练扩散模型之间生成相关样本，又要满足任务特定约束，同时免去高成本的重新训练。

Method: 通过引入耦合引导项促进模型协调，并在每次扩散步骤中加入投影步骤以强制满足硬性约束。

Result: 在图像对生成、对象操作和多机器人运动规划等应用中，提升了生成效果，同时确保约束满足且计算成本较低。

Conclusion: PCD在不增加显著成本的情况下有效实现了高质量的联合生成，并能适应多种实际应用场景。

Abstract: Modifications to test-time sampling have emerged as an important extension to
diffusion algorithms, with the goal of biasing the generative process to
achieve a given objective without having to retrain the entire diffusion model.
However, generating jointly correlated samples from multiple pre-trained
diffusion models while simultaneously enforcing task-specific constraints
without costly retraining has remained challenging. To this end, we propose
Projected Coupled Diffusion (PCD), a novel test-time framework for constrained
joint generation. PCD introduces a coupled guidance term into the generative
dynamics to encourage coordination between diffusion models and incorporates a
projection step at each diffusion step to enforce hard constraints.
Empirically, we demonstrate the effectiveness of PCD in application scenarios
of image-pair generation, object manipulation, and multi-robot motion planning.
Our results show improved coupling effects and guaranteed constraint
satisfaction without incurring excessive computational costs.

</details>


### [259] [Driving Accurate Allergen Prediction with Protein Language Models and Generalization-Focused Evaluation](https://arxiv.org/abs/2508.10541)
*Brian Shing-Hei Wong,Joshua Mincheol Kim,Sin-Hang Fung,Qing Xiong,Kelvin Fu-Kiu Ao,Junkang Wei,Ran Wang,Dan Michelle Wang,Jingying Zhou,Bo Feng,Alfred Sze-Lok Cheng,Kevin Y. Yip,Stephen Kwok-Wing Tsui,Qin Cao*

Main category: cs.LG

TL;DR: 本文提出了一种名为Applm的新型计算框架，依托xTrimoPGLM蛋白质语言模型，实现了高效的过敏原预测。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在识别蛋白质过敏原（特别是缺乏训练集相似例子的过敏原）方面的不足，提供更精准的预测工具以应对公共卫生挑战。

Method: 通过使用具有1000亿参数的xTrimoPGLM蛋白质语言模型，Applm进行基于序列特征的过敏原预测，并在真实复杂场景中设计实验验证其性能。

Result: Applm在评估复杂过敏原预测任务中，稳定优于现有七种先进方法，并证明其在辨别同源高序列相似性中过敏原和非过敏原的能力，及在评估突变引起的功能性影响的表现。

Conclusion: xTrimoPGLM的训练结果是Applm优越性能的关键；研究者还开源了Applm软件和精心制作的基准数据集，为未来研究提供了便利。

Abstract: Allergens, typically proteins capable of triggering adverse immune responses,
represent a significant public health challenge. To accurately identify
allergen proteins, we introduce Applm (Allergen Prediction with Protein
Language Models), a computational framework that leverages the 100-billion
parameter xTrimoPGLM protein language model. We show that Applm consistently
outperforms seven state-of-the-art methods in a diverse set of tasks that
closely resemble difficult real-world scenarios. These include identifying
novel allergens that lack similar examples in the training set, differentiating
between allergens and non-allergens among homologs with high sequence
similarity, and assessing functional consequences of mutations that create few
changes to the protein sequences. Our analysis confirms that xTrimoPGLM,
originally trained on one trillion tokens to capture general protein sequence
characteristics, is crucial for Applm's performance by detecting important
differences among protein sequences. In addition to providing Applm as
open-source software, we also provide our carefully curated benchmark datasets
to facilitate future research.

</details>


### [260] [Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards](https://arxiv.org/abs/2508.10548)
*Zetian Sun,Dongfang Li,Zhuoen Chen,Yuhuai Qin,Baotian Hu*

Main category: cs.LG

TL;DR: 提出了一种针对软件工程任务的强化学习框架（SWE-oriented RL Framework）以及用于稳定优化的奖励机制（Gated Reward Accumulation，G-RA）。


<details>
  <summary>Details</summary>
Motivation: 解决长时期强化学习任务中由于奖励稀疏和奖励失配导致的挑战，尤其针对软件工程任务中的多步推理和基于规则的验证。

Method: 引入了SWE-oriented RL Framework支持多轮交互、基于docker环境的执行和自定义奖励函数，并提出Gated Reward Accumulation方法仅在满足高层次奖励阈值时累积即时奖励。

Result: 在SWE-bench Verified和kBench测试中，任务完成率和修改率显著提高，同时避免因奖励失配导致的策略退化。

Conclusion: 提出的方法在保持奖励积累平衡的同时解决了长时期强化学习中的奖励稀疏问题，为类似问题提供了一个实用的解决方案。

Abstract: Reward sparsity in long-horizon reinforcement learning (RL) tasks remains a
significant challenge, while existing outcome-based reward shaping struggles to
define meaningful immediate rewards without introducing bias or requiring
explicit task decomposition. Alternatively, verification-based reward shaping
uses stepwise critics, but misalignment between immediate rewards and long-term
objectives can lead to reward hacking and suboptimal policies. In this work, we
address this problem in the context of software engineering (SWE) tasks, where
multi-turn reasoning and rule-based verification are critical. We introduce the
SWE-oriented RL Framework, a unified system supporting multi-turn interaction,
docker-based execution, and customizable reward functions. Additionally, we
propose Gated Reward Accumulation (G-RA), a novel method that accumulates
immediate rewards only when high-level (long-term) rewards meet a predefined
threshold, ensuring stable RL optimization. Experiments on SWE-bench Verified
and kBench demonstrate that G-RA leads to an increase in completion rates
(47.6\% \rightarrow 93.8\% and 22.0\% \rightarrow 86.0\%) and modification
rates (19.6\% \rightarrow 23.8\% and 12.0\% \rightarrow 42.0\%), while avoiding
policy degradation caused by reward misalignment. Our findings highlight the
importance of balanced reward accumulation in long-horizon RL and provide a
practical solution.

</details>


### [261] [Technical Report: Facilitating the Adoption of Causal Inference Methods Through LLM-Empowered Co-Pilot](https://arxiv.org/abs/2508.10581)
*Jeroen Berrevoets,Julianna Piskorz,Robert Davis,Harry Amad,Jim Weatherall,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: CATE-B是一个基于大语言模型（LLM）的开源协作系统，用于指导用户完成因果效应估计的完整流程。


<details>
  <summary>Details</summary>
Motivation: 因果效应估计复杂，在医疗、经济学等许多领域中至关重要，但现有方法需要深厚的专业知识，难以普及。

Method: CATE-B结合大语言模型和因果推断框架，帮助用户构建因果模型、识别调整集以及选择合适的回归方法。

Result: CATE-B提供用户友好的交互式因果分析工具，并释放了跨领域的基准任务套件以鼓励复现与评估。

Conclusion: CATE-B通过智能辅助因果分析降低了因果推断和治疗效应估计的门槛。

Abstract: Estimating treatment effects (TE) from observational data is a critical yet
complex task in many fields, from healthcare and economics to public policy.
While recent advances in machine learning and causal inference have produced
powerful estimation techniques, their adoption remains limited due to the need
for deep expertise in causal assumptions, adjustment strategies, and model
selection. In this paper, we introduce CATE-B, an open-source co-pilot system
that uses large language models (LLMs) within an agentic framework to guide
users through the end-to-end process of treatment effect estimation. CATE-B
assists in (i) constructing a structural causal model via causal discovery and
LLM-based edge orientation, (ii) identifying robust adjustment sets through a
novel Minimal Uncertainty Adjustment Set criterion, and (iii) selecting
appropriate regression methods tailored to the causal structure and dataset
characteristics. To encourage reproducibility and evaluation, we release a
suite of benchmark tasks spanning diverse domains and causal complexities. By
combining causal inference with intelligent, interactive assistance, CATE-B
lowers the barrier to rigorous causal analysis and lays the foundation for a
new class of benchmarks in automated treatment effect estimation.

</details>


### [262] [GNN-based Unified Deep Learning](https://arxiv.org/abs/2508.10583)
*Furkan Pala,Islem Rekik*

Main category: cs.LG

TL;DR: 针对医疗图像中深度学习模型的领域断裂问题，提出了一个基于统一学习的新方法，通过图表示将不同模型统一处理。


<details>
  <summary>Details</summary>
Motivation: 解决医疗图像中因分布转移导致模型泛化性下降的问题，尤其是应对不同医院因数据分布差异需要训练不同模型的挑战。

Method: 将每个模型编码为图表示，通过一个统一的图神经网络（uGNN）来指导模型优化，实现跨模型参数共享和知识迁移。

Result: 实验表明，在MorphoMNIST、PneumoniaMNIST和BreastMNIST数据集上，统一学习在分布转移下表现出强的泛化能力和鲁棒性。

Conclusion: 统一学习方法提高了不同模型在分布转移场景下的表现，促进了参数和知识的跨架构共享。

Abstract: Deep learning models often struggle to maintain generalizability in medical
imaging, particularly under domain-fracture scenarios where distribution shifts
arise from varying imaging techniques, acquisition protocols, patient
populations, demographics, and equipment. In practice, each hospital may need
to train distinct models - differing in learning task, width, and depth - to
match local data. For example, one hospital may use Euclidean architectures
such as MLPs and CNNs for tabular or grid-like image data, while another may
require non-Euclidean architectures such as graph neural networks (GNNs) for
irregular data like brain connectomes. How to train such heterogeneous models
coherently across datasets, while enhancing each model's generalizability,
remains an open problem. We propose unified learning, a new paradigm that
encodes each model into a graph representation, enabling unification in a
shared graph learning space. A GNN then guides optimization of these unified
models. By decoupling parameters of individual models and controlling them
through a unified GNN (uGNN), our method supports parameter sharing and
knowledge transfer across varying architectures (MLPs, CNNs, GNNs) and
distributions, improving generalizability. Evaluations on MorphoMNIST and two
MedMNIST benchmarks - PneumoniaMNIST and BreastMNIST - show that unified
learning boosts performance when models are trained on unique distributions and
tested on mixed ones, demonstrating strong robustness to unseen data with large
distribution shifts. Code and benchmarks: https://github.com/basiralab/uGNN

</details>


### [263] [Self-Supervised Temporal Super-Resolution of Energy Data using Generative Adversarial Transformer](https://arxiv.org/abs/2508.10587)
*Xuanhao Mu,Gökhan Demirel,Yuzhe Zhang,Jianlei Liu,Thorsten Schlachter,Veit Hagenmeyer*

Main category: cs.LG

TL;DR: 本文提出一种利用生成对抗变换器（GATs）的新方法，在无需高分辨率真实数据的情况下改善时间序列上采样的精度。


<details>
  <summary>Details</summary>
Motivation: 在能源网络设计与运行中，时间序列的上采样是为了弥合时间粒度差异。然而，传统方法效率高但信息损失大，新兴方法则存在数据稀疏和监督学习难题。

Method: 运用生成对抗变换器（GATs）模型，通过无监督学习方式训练，无需依赖高分辨率的真实数据。

Result: 相较传统插值方法，GATs方法能将上采样任务的均方根误差（RMSE）降低9%，在预测控制应用场景中提升13%的准确率。

Conclusion: 提出的方法有效解决了时间序列上采样的挑战，不需要真实高分辨率数据，显示出良好的性能和应用潜力。

Abstract: To bridge the temporal granularity gap in energy network design and operation
based on Energy System Models, resampling of time series is required. While
conventional upsampling methods are computationally efficient, they often
result in significant information loss or increased noise. Advanced models such
as time series generation models, Super-Resolution models and imputation models
show potential, but also face fundamental challenges. The goal of time series
generative models is to learn the distribution of the original data to generate
high-resolution series with similar statistical characteristics. This is not
entirely consistent with the definition of upsampling. Time series
Super-Resolution models or imputation models can degrade the accuracy of
upsampling because the input low-resolution time series are sparse and may have
insufficient context. Moreover, such models usually rely on supervised learning
paradigms. This presents a fundamental application paradox: their training
requires the high-resolution time series that is intrinsically absent in
upsampling application scenarios. To address the mentioned upsampling issue,
this paper introduces a new method utilizing Generative Adversarial
Transformers (GATs), which can be trained without access to any ground-truth
high-resolution data. Compared with conventional interpolation methods, the
introduced method can reduce the root mean square error (RMSE) of upsampling
tasks by 9%, and the accuracy of a model predictive control (MPC) application
scenario is improved by 13%.

</details>


### [264] [FreeGAD: A Training-Free yet Effective Approach for Graph Anomaly Detection](https://arxiv.org/abs/2508.10594)
*Yunfeng Zhao,Yixin Liu,Shiyuan Li,Qingfeng Chen,Yu Zheng,Shirui Pan*

Main category: cs.LG

TL;DR: 提出了一个无需训练的图异常检测方法FreeGAD，通过残差编码和统计偏差计算实现高效高性能的异常检测。


<details>
  <summary>Details</summary>
Motivation: 目前深度学习方法在图异常检测中成本高、扩展性差，训练过程可能对性能贡献有限。

Method: 采用无训练的方式，利用亲和力控制的残差编码生成异常感知表示，并通过统计偏差计算异常得分。

Result: FreeGAD在多个领域的基准数据集上表现出卓越的检测性能、效率和可扩展性。

Conclusion: 该方法无需训练和迭代优化，提供了一种高效、简单且有效的图异常检测新方向。

Abstract: Graph Anomaly Detection (GAD) aims to identify nodes that deviate from the
majority within a graph, playing a crucial role in applications such as social
networks and e-commerce. Despite the current advancements in deep
learning-based GAD, existing approaches often suffer from high deployment costs
and poor scalability due to their complex and resource-intensive training
processes. Surprisingly, our empirical findings suggest that the training phase
of deep GAD methods, commonly perceived as crucial, may actually contribute
less to anomaly detection performance than expected. Inspired by this, we
propose FreeGAD, a novel training-free yet effective GAD method. Specifically,
it leverages an affinity-gated residual encoder to generate anomaly-aware
representations. Meanwhile, FreeGAD identifies anchor nodes as pseudo-normal
and anomalous guides, followed by calculating anomaly scores through
anchor-guided statistical deviations. Extensive experiments demonstrate that
FreeGAD achieves superior anomaly detection performance, efficiency, and
scalability on multiple benchmark datasets from diverse domains, without any
training or iterative optimization.

</details>


### [265] [On Spectral Properties of Gradient-based Explanation Methods](https://arxiv.org/abs/2508.10595)
*Amir Mehrpanah,Erik Englesson,Hossein Azizpour*

Main category: cs.LG

TL;DR: 本文通过采用概率和频谱的视角，研究了深度网络解释方法的可靠性问题，并提出了基于形式化分析的改进方案。


<details>
  <summary>Details</summary>
Motivation: 研究深度网络行为以增强对其结果的信心，并解决解释方法中因缺乏形式化导致的可靠性问题。

Method: 采用概率和频谱视角，分析梯度引发的频谱偏差，解释实验中的常见设计选择，并针对扰动超参数的不一致问题提出两种改进方法：标准扰动尺度机制和一种新的聚合方法SpectralLens。

Result: 揭示了梯度使用中的频谱偏差问题，并通过理论与定量评估验证了提出方法的改进效果。

Conclusion: 利用形式化的分析，显著提高了解释方法的可靠性，为优化深度网络解释工具提供了新方向。

Abstract: Understanding the behavior of deep networks is crucial to increase our
confidence in their results. Despite an extensive body of work for explaining
their predictions, researchers have faced reliability issues, which can be
attributed to insufficient formalism. In our research, we adopt novel
probabilistic and spectral perspectives to formally analyze explanation
methods. Our study reveals a pervasive spectral bias stemming from the use of
gradient, and sheds light on some common design choices that have been
discovered experimentally, in particular, the use of squared gradient and input
perturbation. We further characterize how the choice of perturbation
hyperparameters in explanation methods, such as SmoothGrad, can lead to
inconsistent explanations and introduce two remedies based on our proposed
formalism: (i) a mechanism to determine a standard perturbation scale, and (ii)
an aggregation method which we call SpectralLens. Finally, we substantiate our
theoretical results through quantitative evaluations.

</details>


### [266] [Oops!... They Stole it Again: Attacks on Split Learning](https://arxiv.org/abs/2508.10598)
*Tanveer Khan,Antonis Michalas*

Main category: cs.LG

TL;DR: 本文系统性回顾了Split Learning (SL) 面临的多种攻击，分类分析了攻击类型及对应的隐私风险，并评估现有防御方法的效果与局限性。


<details>
  <summary>Details</summary>
Motivation: 研究SL的独特隐私保护模型下的安全隐患，并系统性总结潜在攻击和应对方法。

Method: 对SL中的攻击方法进行分类与分析，包括攻击者角色、隐私风险种类、数据泄露时机及漏洞位置。随后评估了现有的防御技术。

Result: 发现当前防御方法存在一定安全漏洞，并总结其有效性与局限性。

Conclusion: 提出了尚未解决的挑战及未来研究方向，为提升SL体系的隐私性和推动后续研究提供依据。

Abstract: Split Learning (SL) is a collaborative learning approach that improves
privacy by keeping data on the client-side while sharing only the intermediate
output with a server. However, the distributed nature of SL introduces new
security challenges, necessitating a comprehensive exploration of potential
attacks. This paper systematically reviews various attacks on SL, classifying
them based on factors such as the attacker's role, the type of privacy risks,
when data leaks occur, and where vulnerabilities exist. We also analyze
existing defense methods, including cryptographic methods, data modification
approaches, distributed techniques, and hybrid solutions. Our findings reveal
security gaps, highlighting the effectiveness and limitations of existing
defenses. By identifying open challenges and future directions, this work
provides valuable information to improve SL privacy issues and guide further
research.

</details>


### [267] [Variance Reduced Policy Gradient Method for Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2508.10608)
*Davide Guidobene,Lorenzo Benedetti,Diego Arapovic*

Main category: cs.LG

TL;DR: 本研究提出通过引入方差减少技术，改进多目标强化学习（MORL）中策略梯度方法的采样效率，以同时优化多种目标。


<details>
  <summary>Details</summary>
Motivation: 现有的MORL中策略梯度方法存在采样效率低的问题，且在提高采样效率的尝试中往往依赖于过于严格的假设，限制了其在大状态-动作空间中的可扩展性。需要一种方法既提高采样效率又保持通用性。

Method: 通过引入方差减少技术，改进策略梯度方法，减少样本复杂度，同时不依赖严格假设，从而保持对大规模状态-动作空间问题的适应性。

Result: 提高了MORL中策略梯度方法的采样效率，在保持对大状态-动作空间扩展能力的同时减少了样本需求。

Conclusion: 引入方差减少技术是解决MORL采样效率低下问题的有效策略，具有广泛适用性和潜在的实用价值。

Abstract: Multi-Objective Reinforcement Learning (MORL) is a generalization of
traditional Reinforcement Learning (RL) that aims to optimize multiple, often
conflicting objectives simultaneously rather than focusing on a single reward.
This approach is crucial in complex decision-making scenarios where agents must
balance trade-offs between various goals, such as maximizing performance while
minimizing costs. We consider the problem of MORL where the objectives are
combined using a non-linear scalarization function. Just like in standard RL,
policy gradient methods (PGMs) are amongst the most effective for handling
large and continuous state-action spaces in MORL. However, existing PGMs for
MORL suffer from high sample inefficiency, requiring large amounts of data to
be effective. Previous attempts to solve this problem rely on overly strict
assumptions, losing PGMs' benefits in scalability to large state-action spaces.
In this work, we address the issue of sample efficiency by implementing
variance-reduction techniques to reduce the sample complexity of policy
gradients while maintaining general assumptions.

</details>


### [268] [Beyond Random Sampling: Instance Quality-Based Data Partitioning via Item Response Theory](https://arxiv.org/abs/2508.10628)
*Lucas Cardoso,Vitor Santos,José Ribeiro Filho,Ricardo Prudêncio,Regiane Kawasaki,Ronnie Alves*

Main category: cs.LG

TL;DR: 该研究通过IRT参数指导数据分区，改善机器学习模型验证的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的数据分区方法忽略了实例的内在质量，该研究动机是探索如何通过IRT参数更准确地表征和指导数据分区，提高模型验证的可靠性。

Method: 使用项目信息理论（IRT）参数分析数据集内实例的异质性，并基于此进行分区，评估这种分区策略对多个机器学习模型性能的影响。

Result: IRT揭示了数据集实例的内在差异性，通过高猜测参数训练的模型性能显著下降，而通过更平衡的分区模型性能有显著提高。

Conclusion: 基于IRT参数的分区能有效均衡偏差和方差，改进模型性能评价。

Abstract: Robust validation of Machine Learning (ML) models is essential, but
traditional data partitioning approaches often ignore the intrinsic quality of
each instance. This study proposes the use of Item Response Theory (IRT)
parameters to characterize and guide the partitioning of datasets in the model
validation stage. The impact of IRT-informed partitioning strategies on the
performance of several ML models in four tabular datasets was evaluated. The
results obtained demonstrate that IRT reveals an inherent heterogeneity of the
instances and highlights the existence of informative subgroups of instances
within the same dataset. Based on IRT, balanced partitions were created that
consistently help to better understand the tradeoff between bias and variance
of the models. In addition, the guessing parameter proved to be a determining
factor: training with high-guessing instances can significantly impair model
performance and resulted in cases with accuracy below 50%, while other
partitions reached more than 70% in the same dataset.

</details>


### [269] [Energy-Based Models for Predicting Mutational Effects on Proteins](https://arxiv.org/abs/2508.10629)
*Patrick Soga,Zhenyu Lei,Yinhan He,Camille Bilodeau,Jundong Li*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，通过使用能量模型和序列模型，改进了蛋白质结合自由能变化($\Delta\Delta G$)的预测。


<details>
  <summary>Details</summary>
Motivation: 准确预测$\Delta\Delta G$在蛋白质工程和药物发现中至关重要。作者旨在克服估计蛋白复合体构象分布的棘手问题。

Method: 提出了一种结合能量模型与序列模型的新方法，将$\Delta\Delta G$分解为基于逆折叠模型的序列分量和基于能量模型的结构分量。假设结合态与非结合态之间达到平衡，简化了退化度估计。

Result: 该方法显著优于现有的基于结构和序列的深度学习方法，在$\Delta\Delta G$预测和抗体优化领域（特别是针对 SARS-CoV-2）表现优越。

Conclusion: 通过引入基于物理学的归纳偏差，改进了$\Delta\Delta G$预测模型，证明此方法能更准确地处理相关科学问题。

Abstract: Predicting changes in binding free energy ($\Delta\Delta G$) is a vital task
in protein engineering and protein-protein interaction (PPI) engineering for
drug discovery. Previous works have observed a high correlation between
$\Delta\Delta G$ and entropy, using probabilities of biologically important
objects such as side chain angles and residue identities to estimate
$\Delta\Delta G$. However, estimating the full conformational distribution of a
protein complex is generally considered intractable. In this work, we propose a
new approach to $\Delta\Delta G$ prediction that avoids this issue by instead
leveraging energy-based models for estimating the probability of a complex's
conformation. Specifically, we novelly decompose $\Delta\Delta G$ into a
sequence-based component estimated by an inverse folding model and a
structure-based component estimated by an energy model. This decomposition is
made tractable by assuming equilibrium between the bound and unbound states,
allowing us to simplify the estimation of degeneracies associated with each
state. Unlike previous deep learning-based methods, our method incorporates an
energy-based physical inductive bias by connecting the often-used sequence
log-odds ratio-based approach to $\Delta\Delta G$ prediction with a new
$\Delta\Delta E$ term grounded in statistical mechanics. We demonstrate
superiority over existing state-of-the-art structure and sequence-based deep
learning methods in $\Delta\Delta G$ prediction and antibody optimization
against SARS-CoV-2.

</details>


### [270] [Conditional Information Bottleneck for Multimodal Fusion: Overcoming Shortcut Learning in Sarcasm Detection](https://arxiv.org/abs/2508.10644)
*Yihua Wang,Qi Jia,Cong Xu,Feiyu Chen,Yuhan Liu,Haotian Zhang,Liang Jin,Lu Liu,Zhichun Wang*

Main category: cs.LG

TL;DR: 提出了一个新的多模态讽刺检测模型MCIB，通过移除数据集中的捷径信号，改善了模型在复杂情感识别上的泛化能力，特别是在真实场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态讽刺检测模型往往受到数据集捷径学习的影响，难以在真实场景中实现良好的泛化表现，且现有模态融合策略存在局限性。

Method: 构建了MUStARD++$^{R}$数据集，通过移除捷径信号优化现有数据集；引入多模态条件信息瓶颈（MCIB）模型，增强多模态数据的高效融合。

Result: MCIB模型在避免捷径学习的情况下实现了最佳性能，展示了在复杂情感识别上的潜力。

Conclusion: MCIB模型通过有效的模态融合策略，提高了多模态讽刺检测的鲁棒性和准确性，为相关任务提供了可靠的解决方案。

Abstract: Multimodal sarcasm detection is a complex task that requires distinguishing
subtle complementary signals across modalities while filtering out irrelevant
information. Many advanced methods rely on learning shortcuts from datasets
rather than extracting intended sarcasm-related features. However, our
experiments show that shortcut learning impairs the model's generalization in
real-world scenarios. Furthermore, we reveal the weaknesses of current modality
fusion strategies for multimodal sarcasm detection through systematic
experiments, highlighting the necessity of focusing on effective modality
fusion for complex emotion recognition. To address these challenges, we
construct MUStARD++$^{R}$ by removing shortcut signals from MUStARD++. Then, a
Multimodal Conditional Information Bottleneck (MCIB) model is introduced to
enable efficient multimodal fusion for sarcasm detection. Experimental results
show that the MCIB achieves the best performance without relying on shortcut
learning.

</details>


### [271] [SPHENIC: Topology-Informed Multi-View Clustering for Spatial Transcriptomics](https://arxiv.org/abs/2508.10646)
*Chenkai Guo,Yikai Zhu,Jing Yangum,Renxiang Guan,Por Lip Yee,Guangdun Peng,Dayu Hu*

Main category: cs.LG

TL;DR: 此论文提出了一种新方法SPHENIC，通过结合空间持久同调与空间邻域优化，提升了空间转录组数据的聚类分析性能。


<details>
  <summary>Details</summary>
Motivation: 目前空间转录组聚类存在（1）过于依赖噪声信号导致拓扑特征不可靠，以及（2）空间邻域建模不足的问题。

Method: 提出SPHENIC方法，引入空间持久同调提取不变拓扑特征，并通过空间约束与分布优化模块（SCDOM）提升嵌入与邻域信息的相似性，优化空间嵌入质量。

Result: 在14个空间转录组切片基准数据上进行测试，SPHENIC在聚类任务上的表现优于现有最优方法3.31%-6.54%。

Conclusion: SPHENIC通过改善空间嵌入与拓扑特征学习，有效提升了空间转录组聚类的精确性，展现了优越性。

Abstract: By incorporating spatial location information, spatial-transcriptomics
clustering yields more comprehensive insights into cell subpopulation
identification. Despite recent progress, existing methods have at least two
limitations: (i) topological learning typically considers only representations
of individual cells or their interaction graphs; however, spatial
transcriptomic profiles are often noisy, making these approaches vulnerable to
low-quality topological signals, and (ii) insufficient modeling of spatial
neighborhood information leads to low-quality spatial embeddings. To address
these limitations, we propose SPHENIC, a novel Spatial Persistent Homology
Enhanced Neighborhood Integrative Clustering method. Specifically, SPHENIC
incorporates invariant topological features into the clustering network to
achieve stable representation learning. Additionally, to construct high-quality
spatial embeddings that reflect the true cellular distribution, we design the
Spatial Constraint and Distribution Optimization Module (SCDOM). This module
increases the similarity between a cell's embedding and those of its spatial
neighbors, decreases similarity with non-neighboring cells, and thereby
produces clustering-friendly spatial embeddings. Extensive experiments on 14
benchmark spatial transcriptomic slices demonstrate that SPHENIC achieves
superior performance on the spatial clustering task, outperforming existing
state-of-the-art methods by 3.31%-6.54% over the best alternative.

</details>


### [272] [Geospatial Diffusion for Land Cover Imperviousness Change Forecasting](https://arxiv.org/abs/2508.10649)
*Debvrat Varshney,Vibhas Vats,Bhartendu Pandey,Christa Brelsford,Philipe Dias*

Main category: cs.LG

TL;DR: 本文探讨利用生成式AI（Generative AI）来预测土地利用与土地覆盖变化（LULC），采用扩散模型进行预测实验，展示了其在高分辨率预测中的潜力。


<details>
  <summary>Details</summary>
Motivation: 土地覆盖变化对地球系统的多个关键过程产生重要影响，例如对区域水文和洪水风险的影响。然而，与大气和水文学模型的进展相比，土地利用和覆盖变化的预测能力落后，亟需提升。

Method: 将土地利用与覆盖变化预测作为数据合成问题，基于历史数据和辅助数据源，采用生成式AI的扩散模型实现土地覆盖变化的十年期预测，并与静态假设基线进行对比。

Result: 实验结果表明，在大于等于0.7×0.7km²的分辨率下，模型在预测美国12个大都市区域的表现优于假设无变化的基线，获得了更低的平均绝对误差（MAE）。

Conclusion: 生成模型能够有效捕获历史数据中的时空模式，对未来变化的预测具有潜力。此外，还讨论了未来研究方向，包括结合地球物理属性信息及利用驱动变量模拟不同情景。

Abstract: Land cover, both present and future, has a significant effect on several
important Earth system processes. For example, impervious surfaces heat up and
speed up surface water runoff and reduce groundwater infiltration, with
concomitant effects on regional hydrology and flood risk. While regional Earth
System models have increasing skill at forecasting hydrologic and atmospheric
processes at high resolution in future climate scenarios, our ability to
forecast land-use and land-cover change (LULC), a critical input to risk and
consequences assessment for these scenarios, has lagged behind. In this paper,
we propose a new paradigm exploiting Generative AI (GenAI) for land cover
change forecasting by framing LULC forecasting as a data synthesis problem
conditioned on historical and auxiliary data-sources. We discuss desirable
properties of generative models that fundament our research premise, and
demonstrate the feasibility of our methodology through experiments on
imperviousness forecasting using historical data covering the entire
conterminous United States. Specifically, we train a diffusion model for
decadal forecasting of imperviousness and compare its performance to a baseline
that assumes no change at all. Evaluation across 12 metropolitan areas for a
year held-out during training indicate that for average resolutions $\geq
0.7\times0.7km^2$ our model yields MAE lower than such a baseline. This finding
corroborates that such a generative model can capture spatiotemporal patterns
from historical data that are significant for projecting future change.
Finally, we discuss future research to incorporate auxiliary information on
physical properties about the Earth, as well as supporting simulation of
different scenarios by means of driver variables.

</details>


### [273] [Graph Learning via Logic-Based Weisfeiler-Leman Variants and Tabularization](https://arxiv.org/abs/2508.10651)
*Reijo Jaakkola,Tomi Janhunen,Antti Kuusisto,Magdalena Ortiz,Matias Selin,Mantas Šimkus*

Main category: cs.LG

TL;DR: 提出了一种基于Weisfeiler-Leman算法变体的图分类新方法，通过将图数据转换为表格形式并应用表格数据方法，实验证明其在效率和准确性上媲美现有方法。


<details>
  <summary>Details</summary>
Motivation: 为解决现有图分类中时间或内存效率不足的问题，提出一种既高效又准确的方法。

Method: 通过修改Weisfeiler-Leman算法的逻辑框架，将图数据转化为表格进行处理，并对算法的表达能力进行了理论分析。

Result: 在12个基准数据集上，所提出方法在准确性上媲美目前最好的图神经网络和图核方法，同时在时间或内存效率上更具优势。

Conclusion: 该方法在效率和准确性方面的优势表明，它是图分类领域内一种有潜力的备选方案，还初步探索了从图数据集中提取可解释性公式的可能性。

Abstract: We present a novel approach for graph classification based on tabularizing
graph data via variants of the Weisfeiler-Leman algorithm and then applying
methods for tabular data. We investigate a comprehensive class of
Weisfeiler-Leman variants obtained by modifying the underlying logical
framework and establish a precise theoretical characterization of their
expressive power. We then test two selected variants on twelve benchmark
datasets that span a range of different domains. The experiments demonstrate
that our approach matches the accuracy of state-of-the-art graph neural
networks and graph kernels while being more time or memory efficient, depending
on the dataset. We also briefly discuss directly extracting interpretable modal
logic formulas from graph datasets.

</details>


### [274] [MDNS: Masked Diffusion Neural Sampler via Stochastic Optimal Control](https://arxiv.org/abs/2508.10684)
*Yuchen Zhu,Wei Guo,Jaemoo Choi,Guan-Horng Liu,Yongxin Chen,Molei Tao*

Main category: cs.LG

TL;DR: 研究开发一种名为MDNS的神经采样器，用于从离散状态空间中生成样本，并通过大量实验验证其在高维度分布上的有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 针对在大规模、多模态离散状态空间中从已知但未归一化的概率分布生成样本的难题，提出一种有效的新方法。

Method: 提出名为MDNS的框架，通过一系列基于连续时间马尔科夫链的最优控制理论的学习目标，对路径分布进行对齐，从而训练离散神经采样器。

Result: 实验显示，MDNS在极高维问题中能够高效精确地从目标分布中采样，并显著优于其他基于学习的方法。

Conclusion: 研究提出了一种高效的离散神经采样器框架，为解决在高维和多模态分布上的采样任务提供了新思路，并通过实验验证了其潜力和有效性。

Abstract: We study the problem of learning a neural sampler to generate samples from
discrete state spaces where the target probability mass function
$\pi\propto\mathrm{e}^{-U}$ is known up to a normalizing constant, which is an
important task in fields such as statistical physics, machine learning,
combinatorial optimization, etc. To better address this challenging task when
the state space has a large cardinality and the distribution is multi-modal, we
propose $\textbf{M}$asked $\textbf{D}$iffusion $\textbf{N}$eural
$\textbf{S}$ampler ($\textbf{MDNS}$), a novel framework for training discrete
neural samplers by aligning two path measures through a family of learning
objectives, theoretically grounded in the stochastic optimal control of the
continuous-time Markov chains. We validate the efficiency and scalability of
MDNS through extensive experiments on various distributions with distinct
statistical properties, where MDNS learns to accurately sample from the target
distributions despite the extremely high problem dimensions and outperforms
other learning-based baselines by a large margin. A comprehensive study of
ablations and extensions is also provided to demonstrate the efficacy and
potential of the proposed framework.

</details>


### [275] [REFN: A Reinforcement-Learning-From-Network Framework against 1-day/n-day Exploitations](https://arxiv.org/abs/2508.10701)
*Tianlong Yu,Lihong Liu,Ziyi Zhou,Fudu Xing,Kailong Wang,Yang Yang*

Main category: cs.LG

TL;DR: REFN 利用强化学习训练大语言模型以自动生成网络过滤器，预防 1 天或 n 天漏洞的利用，其在精度、效率和扩展能力方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前网络设备由于大量部署和更新延迟（平均补丁时间超过60天）面临1天或n天漏洞的严重威胁，现有防御方法难以应对广泛设备的兼容性和手动部署问题。

Method: 提出强化学习框架（REFN），通过在线网络奖励训练LLM，使用边缘网关统一部署，结合知识蒸馏、语言与网络桥接、错误输出惩罚等技术生成网络过滤器。

Result: 在22个漏洞家族上测试，REFN比现有方法提高21.1%的精度，补丁平均时间缩短至3.65小时，可扩展至10000台设备。

Conclusion: REFN是一个引导LLMs快速应对大规模漏洞利用的初步方案，表现出色，具有良好的扩展性与适配性。

Abstract: The exploitation of 1 day or n day vulnerabilities poses severe threats to
networked devices due to massive deployment scales and delayed patching
(average Mean Time To Patch exceeds 60 days). Existing defenses, including host
based patching and network based filtering, are inadequate due to limited
scalability across diverse devices, compatibility issues especially with
embedded or legacy systems, and error prone deployment process (manual patch
validation). To address these issues, we introduce REFN (Reinforcement Learning
From Network), a novel framework that trains Large Language Models (LLMs) to
autonomously generate network filters to prevent 1 day or n day exploitations.
REFN ensures scalability by uniquely employs Reinforcement Learning (RL) driven
by online network rewards instead of traditional Human Feedback (RLHF). REFN
guarantees compatibility via unified deployment on edge security gateways
(Amazon Eero). REFN provides robustness via online validation using real
network traffic. Crucially, REFN addresses three core challenges in training
LLMs for exploit prevention: 1) expanding current LLMs limited vulnerability
fixing expertise via Agentic RAG based Knowledge Distillation, 2) bridging
current LLMs language to network gaps through an RL From VNF Pipeline that
translates language context (vulnerability description) into network
enforcement, 3) addressing the LLM hallucination and non determinism via the
Online Agentic Validation that penalizes erroneous outputs. Evaluated across 22
families of 1 day or n day exploits, REFN demonstrates effectiveness (21.1
percent higher accuracy than alternatives), efficiency (Mean Time To Patch of
3.65 hours) and scalability (easily scale to 10K devices). REFN serves as an
initial step toward training LLMs to rapidly prevent massive scale 1 day or n
day exploitations.

</details>


### [276] [Electromagnetic Simulations of Antennas on GPUs for Machine Learning Applications](https://arxiv.org/abs/2508.10713)
*Murat Temiz,Vemund Bakken*

Main category: cs.LG

TL;DR: 本文提出一种基于GPU的天线模拟框架，用于天线设计和优化的机器学习应用，并显示GPU比CPU在计算性能上更高效。


<details>
  <summary>Details</summary>
Motivation: 当前的电磁模拟计算对计算资源需求较高，而机器学习方法需要大量数据样本，这在时间有限的情况下挑战较大，提出了新的解决方案以实现高效模拟与数据生成。

Method: 使用基于开源电磁模拟软件gprMax的GPU框架模拟大量预定义或随机天线形状参数，生成数据集，并将结果与商用电磁软件模拟进行比较，同时评估不同机器学习模型对天线参数估计的性能。

Result: 研究表明入门级GPU在计算性能上明显优于高端CPU，高端游戏GPU的性能更是达到高端CPU的18倍。同时，开源电磁软件在空间分辨率足够高时，其模拟结果与商用软件一致。

Conclusion: 基于GPU的模拟框架为机器学习应用中的天线设计提供了一种高效的解决方案，显著提高了模拟效率，同时保持了结果的准确性。

Abstract: This study proposes an antenna simulation framework powered by graphics
processing units (GPUs) based on an open-source electromagnetic (EM) simulation
software (gprMax) for machine learning applications of antenna design and
optimization. Furthermore, it compares the simulation results with those
obtained through commercial EM software. The proposed software framework for
machine learning and surrogate model applications will produce antenna data
sets consisting of a large number of antenna simulation results using GPUs.
Although machine learning methods can attain the optimum solutions for many
problems, they are known to be data-hungry and require a great deal of samples
for the training stage of the algorithms. However, producing a sufficient
number of training samples in EM applications within a limited time is
challenging due to the high computational complexity of EM simulations.
Therefore, GPUs are utilized in this study to simulate a large number of
antennas with predefined or random antenna shape parameters to produce data
sets. Moreover, this study also compares various machine learning and deep
learning models in terms of antenna parameter estimation performance. This
study demonstrates that an entry-level GPU substantially outperforms a high-end
CPU in terms of computational performance, while a high-end gaming GPU can
achieve around 18 times more computational performance compared to a high-end
CPU. Moreover, it is shown that the open-source EM simulation software can
deliver similar results to those obtained via commercial software in the
simulation of microstrip antennas when the spatial resolution of the
simulations is sufficiently fine.

</details>


### [277] [APFL: Analytic Personalized Federated Learning via Dual-Stream Least Squares](https://arxiv.org/abs/2508.10732)
*Kejia Fan,Jianheng Tang,Zhirui Yang,Feijiang Han,Jiaxu Li,Run He,Yajiang Huang,Anfeng Liu,Houbing Herbert Song,Yunhuai Liu,Huiping Zhuang*

Main category: cs.LG

TL;DR: 本文提出了一种名为APFL的个性化联邦学习方法，通过双流最小二乘法解决非IID数据问题，显著提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的个性化联邦学习方法对非IID数据较为脆弱，影响了集体泛化能力，从而对个性化产生负面作用。

Method: 引入了一个冻结的基础模型作为特征提取器，并开发了一个双流的分析模型，包括用于全局泛化的共享主流和用于本地个性化的专属细化流，确保个性化模型在数据分布异质性下保持不变。

Result: 在多个数据集上，APFL显示出比现有最先进技术更优越的性能，准确率至少提高了1.10%-15.45%。

Conclusion: APFL通过理论上的异质性不变性特性，解决了非IID数据分布问题，展现了其在个性化联邦学习中的应用潜力。

Abstract: Personalized Federated Learning (PFL) has presented a significant challenge
to deliver personalized models to individual clients through collaborative
training. Existing PFL methods are often vulnerable to non-IID data, which
severely hinders collective generalization and then compromises the subsequent
personalization efforts. In this paper, to address this non-IID issue in PFL,
we propose an Analytic Personalized Federated Learning (APFL) approach via
dual-stream least squares. In our APFL, we use a foundation model as a frozen
backbone for feature extraction. Subsequent to the feature extractor, we
develop dual-stream analytic models to achieve both collective generalization
and individual personalization. Specifically, our APFL incorporates a shared
primary stream for global generalization across all clients, and a dedicated
refinement stream for local personalization of each individual client. The
analytical solutions of our APFL enable its ideal property of heterogeneity
invariance, theoretically meaning that each personalized model remains
identical regardless of how heterogeneous the data are distributed across all
other clients. Empirical results across various datasets also validate the
superiority of our APFL over state-of-the-art baselines, with advantages of at
least 1.10%-15.45% in accuracy.

</details>


### [278] [Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models](https://arxiv.org/abs/2508.10751)
*Zhipeng Chen,Xiaobo Qin,Youbin Wu,Yue Ling,Qinghao Ye,Wayne Xin Zhao,Guang Shi*

Main category: cs.LG

TL;DR: 论文研究了Pass@k指标对强化学习探索能力的影响，提出基于该指标的训练方法及其优越性证明，并首次设计了优势函数的强化学习奖励。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习中的验证奖励往往使用Pass@1，存在保守策略收敛到局部最优的问题，因此需要构建更适当的奖励指标。

Method: 采用Pass@k作为奖励指标，提出Pass@k Training训练方法，并对其优势进行分析推导，同时探讨了设计优势函数作为奖励的可行性。

Result: 证明了Pass@k Training可以同时提升探索与利用能力，并通过优势函数设计展示了未来研究方向的潜力。

Conclusion: Pass@k指标及其理论分析为RLVR提供了高效的训练方法，成功缓解了探索与利用之间的矛盾，开启了通过优势函数设计优化强化学习的新途径。

Abstract: Reinforcement learning with verifiable rewards (RLVR), which typically adopts
Pass@1 as the reward, has faced the issues in balancing exploration and
exploitation, causing policies to prefer conservative actions, converging to a
local optimum. Identifying an appropriate reward metric is therefore crucial.
Regarding the prior work, although Pass@k has been used in evaluation, its
connection to LLM exploration ability in RLVR remains largely overlooked. To
investigate this, we first use Pass@k as the reward to train the policy model
(i.e., $\textbf{Pass@k Training}$), and observe the improvement on its
exploration ability. Next, we derive an analytical solution for the advantage
of Pass@k Training, leading to an efficient and effective process. Building on
this, our analysis reveals that exploration and exploitation are not inherently
conflicting objectives, while they can mutually enhance each other. Moreover,
Pass@k Training with analytical derivation essentially involves directly
designing the advantage function. Inspired by this, we preliminarily explore
the advantage design for RLVR, showing promising results and highlighting a
potential future direction.

</details>


### [279] [Natively Trainable Sparse Attention for Hierarchical Point Cloud Datasets](https://arxiv.org/abs/2508.10758)
*Nicolas Lapautre,Maria Marchenko,Carlos Miguel Patiño,Xin Zhou*

Main category: cs.LG

TL;DR: 研究结合Erwin架构与NSA机制，提升变换器模型在大规模物理系统数据集上的性能，同时解决注意力机制的二次复杂性问题，结果优于原始Erwin模型。


<details>
  <summary>Details</summary>
Motivation: 克服变换器注意力机制的二次扩展问题，以释放其在处理大规模物理系统数据时的潜力。

Method: 将NSA机制应用于非顺序性数据，并实现结合Erwin架构的NSA模型，在物理科学领域的三种数据集上进行评估并与原始Erwin模型结果进行对比验证。

Result: 相比原始模型，表现达到了匹配甚至超越的效果，并成功重现原始Erwin论文中的实验结果。

Conclusion: 结合Erwin架构与NSA机制的设计显著提高了大规模物理系统数据建模的效率和性能，为此领域带来新的研究方向。

Abstract: Unlocking the potential of transformers on datasets of large physical systems
depends on overcoming the quadratic scaling of the attention mechanism. This
work explores combining the Erwin architecture with the Native Sparse Attention
(NSA) mechanism to improve the efficiency and receptive field of transformer
models for large-scale physical systems, addressing the challenge of quadratic
attention complexity. We adapt the NSA mechanism for non-sequential data,
implement the Erwin NSA model, and evaluate it on three datasets from the
physical sciences -- cosmology simulations, molecular dynamics, and air
pressure modeling -- achieving performance that matches or exceeds that of the
original Erwin model. Additionally, we reproduce the experimental results from
the Erwin paper to validate their implementation.

</details>


### [280] [IBEX: Information-Bottleneck-EXplored Coarse-to-Fine Molecular Generation under Limited Data](https://arxiv.org/abs/2508.10775)
*Dong Xu,Zhangfan Yang,Jenna Xinyi Yao,Shuangbao Song,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: IBEX通过信息瓶颈理论和粗到细的生成策略，解决了蛋白质-配体复合物数据稀缺的问题，并显著提高了分子生成效果。


<details>
  <summary>Details</summary>
Motivation: 当前三维生成模型在新药研发中应用广泛，但蛋白质-配体复合物数据稀缺限制了其泛化能力与性能。本研究旨在通过新方法克服数据短缺带来的挑战。

Method: 采用信息瓶颈理论量化样本信息密度，通过对遮掩策略的分析提高模型的泛化能力；在采用TargetDiff架构生成分子后，利用物理相关优化和移动调整步骤细化分子构型。

Result: 在结构对接任务中，IBEX使对接成功率从53%增至64%，提高Vina得分，改善QED并显著降低外推误差，表现优于现有方法。

Conclusion: IBEX在数据稀缺下展示出卓越性能，能帮助结构药物设计中更高效可靠地生成具有生物活性的分子。

Abstract: Three-dimensional generative models increasingly drive structure-based drug
discovery, yet it remains constrained by the scarce publicly available
protein-ligand complexes. Under such data scarcity, almost all existing
pipelines struggle to learn transferable geometric priors and consequently
overfit to training-set biases. As such, we present IBEX, an
Information-Bottleneck-EXplored coarse-to-fine pipeline to tackle the chronic
shortage of protein-ligand complex data in structure-based drug design.
Specifically, we use PAC-Bayesian information-bottleneck theory to quantify the
information density of each sample. This analysis reveals how different masking
strategies affect generalization and indicates that, compared with conventional
de novo generation, the constrained Scaffold Hopping task endows the model with
greater effective capacity and improved transfer performance. IBEX retains the
original TargetDiff architecture and hyperparameters for training to generate
molecules compatible with the binding pocket; it then applies an L-BFGS
optimization step to finely refine each conformation by optimizing five
physics-based terms and adjusting six translational and rotational degrees of
freedom in under one second. With only these modifications, IBEX raises the
zero-shot docking success rate on CBGBench CrossDocked2020-based from 53% to
64%, improves the mean Vina score from $-7.41 kcal mol^{-1}$ to $-8.07 kcal
mol^{-1}$, and achieves the best median Vina energy in 57 of 100 pockets versus
3 for the original TargetDiff. IBEX also increases the QED by 25%, achieves
state-of-the-art validity and diversity, and markedly reduces extrapolation
error.

</details>


### [281] [Enhancing Fairness in Autoencoders for Node-Level Graph Anomaly Detection](https://arxiv.org/abs/2508.10785)
*Shouju Wang,Yuchen Song,Sheng'en Li,Dongmian Zou*

Main category: cs.LG

TL;DR: 本文提出了一种名为DECAF-GAD的框架，通过引入结构因果模型（SCM）和公平性导向损失函数，改进了图异常检测的公平性与性能。


<details>
  <summary>Details</summary>
Motivation: 随着图神经网络的发展，图异常检测在多个领域表现出重要性。然而，现有方法中关于公平性的研究仍较少，尤其是针对基于自编码器的图异常检测模型（主要用于异常检测）的公平性改进。需要一种方法来同时保证公平性和高性能。

Method: 提出DECAF-GAD框架，该框架通过引入结构因果模型（SCM）将敏感属性与学习表示解耦，在此基础上设计了一个专门的自编码器架构和公平性导向损失函数。

Result: 实验表明，DECAF-GAD在综合性能与公平性方面均优于现有基线方法，在合成和真实数据集上实现了竞争性的检测效果及显著的公平性提升。

Conclusion: DECAF-GAD框架证明了公平性改进在图异常检测任务中的可行性，为基于自编码器的公平性研究提供了新的方向。

Abstract: Graph anomaly detection (GAD) has become an increasingly important task
across various domains. With the rapid development of graph neural networks
(GNNs), GAD methods have achieved significant performance improvements.
However, fairness considerations in GAD remain largely underexplored. Indeed,
GNN-based GAD models can inherit and amplify biases present in training data,
potentially leading to unfair outcomes. While existing efforts have focused on
developing fair GNNs, most approaches target node classification tasks, where
models often rely on simple layer architectures rather than autoencoder-based
structures, which are the most widely used architecturs for anomaly detection.
To address fairness in autoencoder-based GAD models, we propose
\textbf{D}is\textbf{E}ntangled \textbf{C}ounterfactual \textbf{A}dversarial
\textbf{F}air (DECAF)-GAD, a framework that alleviates bias while preserving
GAD performance. Specifically, we introduce a structural causal model (SCM) to
disentangle sensitive attributes from learned representations. Based on this
causal framework, we formulate a specialized autoencoder architecture along
with a fairness-guided loss function. Through extensive experiments on both
synthetic and real-world datasets, we demonstrate that DECAF-GAD not only
achieves competitive anomaly detection performance but also significantly
enhances fairness metrics compared to baseline GAD methods. Our code is
available at https://github.com/Tlhey/decaf_code.

</details>


### [282] [Non-Stationary Restless Multi-Armed Bandits with Provable Guarantee](https://arxiv.org/abs/2508.10804)
*Yu-Heng Hung,Ping-Chun Hsieh,Kai Wang*

Main category: cs.LG

TL;DR: 该论文扩展了在线无休止多臂赌博机（RMAB）的研究，考虑了带有非平稳过渡约束的情况，并引入了新的算法。


<details>
  <summary>Details</summary>
Motivation: 传统RMAB算法假设老虎机遵循固定的马尔可夫决策过程，但现实中如医疗和推荐系统中这些假设通常不成立，导致算法面临挑战。

Method: 提出了结合滑动窗口强化学习和上置信界机制的RMAB算法，用于同时学习过渡动态及其变化。

Result: 证明了所提出算法在放宽的遗憾定义下，达到了$\widetilde{\mathcal{O}}(N^2 B^{\frac{1}{4}} T^{\frac{3}{4}})$遗憾界限。

Conclusion: 为处理非平稳RMAB问题提供了理论框架，填补了相关研究的空白。

Abstract: Online restless multi-armed bandits (RMABs) typically assume that each arm
follows a stationary Markov Decision Process (MDP) with fixed state transitions
and rewards. However, in real-world applications like healthcare and
recommendation systems, these assumptions often break due to non-stationary
dynamics, posing significant challenges for traditional RMAB algorithms. In
this work, we specifically consider $N$-armd RMAB with non-stationary
transition constrained by bounded variation budgets $B$. Our proposed \rmab\;
algorithm integrates sliding window reinforcement learning (RL) with an upper
confidence bound (UCB) mechanism to simultaneously learn transition dynamics
and their variations. We further establish that \rmab\; achieves
$\widetilde{\mathcal{O}}(N^2 B^{\frac{1}{4}} T^{\frac{3}{4}})$ regret bound by
leveraging a relaxed definition of regret, providing a foundational theoretical
framework for non-stationary RMAB problems for the first time.

</details>


### [283] [Comparison of Data Reduction Criteria for Online Gaussian Processes](https://arxiv.org/abs/2508.10815)
*Thore Wietzke,Knut Graichen*

Main category: cs.LG

TL;DR: 本文探讨了高斯过程(GPs)在流式场景下的在线处理问题，并比较了多种数据点减少策略及其表现。


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程因其计算复杂度高，只适用于小数据集，并在流式场景中受限。因此需要在线高斯过程以处理不断积累的数据点问题。

Method: 本文统一比较了若干数据点减少准则，分析了其计算复杂性与减少行为，并在基准函数和实际数据集上进行评估，同时提出验收准则以进一步筛选冗余数据点。

Result: 论文对多种减少策略的性能给出了具体分析，并提供了适用于在线高斯过程算法的指南。

Conclusion: 在线高斯过程可以通过合理选择数据点减少和验收准则克服传统方法的限制，提高效率。

Abstract: Gaussian Processes (GPs) are widely used for regression and system
identification due to their flexibility and ability to quantify uncertainty.
However, their computational complexity limits their applicability to small
datasets. Moreover in a streaming scenario, more and more datapoints accumulate
which is intractable even for Sparse GPs. Online GPs aim to alleviate this
problem by e.g. defining a maximum budget of datapoints and removing redundant
datapoints. This work provides a unified comparison of several reduction
criteria, analyzing both their computational complexity and reduction behavior.
The criteria are evaluated on benchmark functions and real-world datasets,
including dynamic system identification tasks. Additionally, acceptance
criteria are proposed to further filter out redundant datapoints. This work
yields practical guidelines for choosing a suitable criterion for an online GP
algorithm.

</details>


### [284] [Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Technical Solutions](https://arxiv.org/abs/2508.10824)
*Parsa Omidi,Xingshuai Huang,Axel Laborieux,Bahareh Nikpour,Tianyu Shi,Armaghan Eshaghi*

Main category: cs.LG

TL;DR: 本文探讨在记忆增强变压器中的神经科学原则和工程进展，提出认知启发的变压器架构框架。


<details>
  <summary>Details</summary>
Motivation: 旨在克服现有变压器架构在长期上下文保留、持续学习和知识整合方面的局限性。

Method: 通过三个维度分类近期进展（功能目标、记忆表示、整合机制），分析核心记忆操作并提出解决方案。

Result: 界定了当前记忆架构的挑战（如可扩展性和干扰问题）及潜在解决办法（分层缓冲、惊奇门控更新）。

Conclusion: 本文为设计具备类认知功能的持续学习型变压器架构提供了框架和方向，推动神经科学和工程的交叉融合。

Abstract: Memory is fundamental to intelligence, enabling learning, reasoning, and
adaptability across biological and artificial systems. While Transformer
architectures excel at sequence modeling, they face critical limitations in
long-range context retention, continual learning, and knowledge integration.
This review presents a unified framework bridging neuroscience principles,
including dynamic multi-timescale memory, selective attention, and
consolidation, with engineering advances in Memory-Augmented Transformers. We
organize recent progress through three taxonomic dimensions: functional
objectives (context extension, reasoning, knowledge integration, adaptation),
memory representations (parameter-encoded, state-based, explicit, hybrid), and
integration mechanisms (attention fusion, gated control, associative
retrieval). Our analysis of core memory operations (reading, writing,
forgetting, and capacity management) reveals a shift from static caches toward
adaptive, test-time learning systems. We identify persistent challenges in
scalability and interference, alongside emerging solutions including
hierarchical buffering and surprise-gated updates. This synthesis provides a
roadmap toward cognitively-inspired, lifelong-learning Transformer
architectures.

</details>


### [285] [SoK: Data Minimization in Machine Learning](https://arxiv.org/abs/2508.10836)
*Robin Staab,Nikola Jovanović,Kimberly Mai,Prakhar Ganesh,Martin Vechev,Ferdinando Fioretto,Matthew Jagielski*

Main category: cs.LG

TL;DR: 本文探讨了数据最小化（DM）在机器学习领域的应用，通过提出综合框架以便系统性地审视相关研究，并帮助实践者和研究人员有效实施DM原则。


<details>
  <summary>Details</summary>
Motivation: 数据最小化是隐私保护的重要原则，特别是针对依赖大数据集的机器学习应用。然而，现有研究在隐私与安全领域往往未明确关联DM概念，导致实践者难以实施和理解DM原则。

Method: 提出了一个综合框架，包括统一的数据处理流程、对抗者和最小化点，并基于此框架系统性地回顾了数据最小化及相关研究方法。

Result: 首次以结构化方式梳理数据最小化及其相关领域的文献，提供了清晰的实践指导。

Conclusion: 通过统一的DM框架，促进了DM在AI/ML中的策略采用，有助于实践者和研究人员更有效地理解和应用数据最小化的原则。

Abstract: Data minimization (DM) describes the principle of collecting only the data
strictly necessary for a given task. It is a foundational principle across
major data protection regulations like GDPR and CPRA. Violations of this
principle have substantial real-world consequences, with regulatory actions
resulting in fines reaching hundreds of millions of dollars. Notably, the
relevance of data minimization is particularly pronounced in machine learning
(ML) applications, which typically rely on large datasets, resulting in an
emerging research area known as Data Minimization in Machine Learning (DMML).
At the same time, existing work on other ML privacy and security topics often
addresses concerns relevant to DMML without explicitly acknowledging the
connection. This disconnect leads to confusion among practitioners,
complicating their efforts to implement DM principles and interpret the
terminology, metrics, and evaluation criteria used across different research
communities. To address this gap, our work introduces a comprehensive framework
for DMML, including a unified data pipeline, adversaries, and points of
minimization. This framework allows us to systematically review the literature
on data minimization and \emph{DM-adjacent} methodologies, for the first time
presenting a structured overview designed to help practitioners and researchers
effectively apply DM principles. Our work facilitates a unified DM-centric
understanding and broader adoption of data minimization strategies in AI/ML.

</details>


### [286] [Efficiently Verifiable Proofs of Data Attribution](https://arxiv.org/abs/2508.10866)
*Ari Karchmer,Seth Neel,Martin Pawelczyk*

Main category: cs.LG

TL;DR: 提出了一种交互验证机制，让资源受限的验证者可确保数据归因模型的可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限方如何信任数据归因结果的关键信任问题，特别是在数据定价等重要应用中。

Method: 通过交互证明协议，让拥有计算能力的验证者与资源受限的验证者合作，以大概率保证归因结果的正确性和可靠性。

Result: 验证者可以以O(1/ε)的复杂度验证良好数据归因结果，并检测出大部分的错误归因结果。

Conclusion: 该方法为资源受限方提供了一种高效且可靠的手段验证数据归因，具有广泛的应用潜力。

Abstract: Data attribution methods aim to answer useful counterfactual questions like
"what would a ML model's prediction be if it were trained on a different
dataset?" However, estimation of data attribution models through techniques
like empirical influence or "datamodeling" remains very computationally
expensive. This causes a critical trust issue: if only a few computationally
rich parties can obtain data attributions, how can resource-constrained parties
trust that the provided attributions are indeed "good," especially when they
are used for important downstream applications (e.g., data pricing)? In this
paper, we address this trust issue by proposing an interactive verification
paradigm for data attribution. An untrusted and computationally powerful Prover
learns data attributions, and then engages in an interactive proof with a
resource-constrained Verifier. Our main result is a protocol that provides
formal completeness, soundness, and efficiency guarantees in the sense of
Probably-Approximately-Correct (PAC) verification. Specifically, if both Prover
and Verifier follow the protocol, the Verifier accepts data attributions that
are {\epsilon}-close to the optimal data attributions (in terms of the Mean
Squared Error) with probability 1-{\delta}. Conversely, if the Prover
arbitrarily deviates from the protocol, even with infinite compute, then this
is detected (or it still yields data attributions to the Verifier) except with
probability {\delta}. Importantly, our protocol ensures the Verifier's
workload, measured by the number of independent model retrainings it must
perform, scales only as O(1/{\epsilon}); i.e., independently of the dataset
size. At a technical level, our results apply to efficiently verifying any
linear function over the boolean hypercube computed by the Prover, making them
broadly applicable to various attribution tasks.

</details>


### [287] [A Dataset for Distilling Knowledge Priors from Literature for Therapeutic Design](https://arxiv.org/abs/2508.10899)
*Haydn Thomas Jones,Natalie Maus,Josh Magnus Ludan,Maggie Ziyu Huan,Jiaming Liang,Marcelo Der Torossian Torres,Jiatao Liang,Zachary Ives,Yoseph Barash,Cesar de la Fuente-Nunez,Jacob R. Gardner,Mark Yatskar*

Main category: cs.LG

TL;DR: 本研究提出了一个名为\ourdataset的数据集，以改进AI驱动的新药物开发。使用长文本模型和其他架构，该数据集可以生成强有力的模型先验，提高分类和回归任务中的性能，同时提高分子设计的安全性。


<details>
  <summary>Details</summary>
Motivation: 通过提供先验知识来解决当前AI模型在分子设计中可能违反隐含约束的问题，例如产生高潜在毒性分子。

Method: 构建了一个从科学文献中提取出来的大型数据集，该数据集包含天然语言表述的事实对（如SMILES或refseq IDs）。通过预训练和评估，与TDC任务数据集相结合用于优化模型，并使用这些数据来改善新分子优化过程。

Result: 基于\ourdataset的数据训练的模型在分类和回归任务中超越了参数更多的大型模型，同时改进了分子设计的安全性和有效性。特别是在GuacaMol基准上表现优异。

Conclusion: \ourdataset为药物设计模型提供了一个经过实验验证的有力先验，提高了模型性能和分子设计安全性，展示了AI在新药物开发中的重要潜力。

Abstract: AI-driven discovery can greatly reduce design time and enhance new
therapeutics' effectiveness. Models using simulators explore broad design
spaces but risk violating implicit constraints due to a lack of experimental
priors. For example, in a new analysis we performed on a diverse set of models
on the GuacaMol benchmark using supervised classifiers, over 60\% of molecules
proposed had high probability of being mutagenic. In this work, we introduce
\ourdataset, a dataset of priors for design problems extracted from literature
describing compounds used in lab settings. It is constructed with LLM pipelines
for discovering therapeutic entities in relevant paragraphs and summarizing
information in concise fair-use facts. \ourdataset~ consists of 32.3 million
pairs of natural language facts, and appropriate entity representations (i.e.
SMILES or refseq IDs). To demonstrate the potential of the data, we train LLM,
CLIP, and LLava architectures to reason jointly about text and design targets
and evaluate on tasks from the Therapeutic Data Commons (TDC). \ourdataset~is
highly effective for creating models with strong priors: in supervised
prediction problems that use our data as pretraining, our best models with 15M
learnable parameters outperform larger 2B TxGemma on both regression and
classification TDC tasks, and perform comparably to 9B models on average.
Models built with \ourdataset~can be used as constraints while optimizing for
novel molecules in GuacaMol, resulting in proposals that are safer and nearly
as effective. We release our dataset at
\href{https://huggingface.co/datasets/medexanon/Medex}{huggingface.co/datasets/medexanon/Medex},
and will provide expanded versions as available literature grows.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [288] [Empirical Investigation into Configuring Echo State Networks for Representative Benchmark Problem Domains](https://arxiv.org/abs/2508.10887)
*Brooke R. Weborg,Gursel Serpen*

Main category: cs.NE

TL;DR: 这篇论文通过四个基准问题评估了Echo State Network（回声状态网络，ESN）的性能，并提出了配置架构和选择参数的经验法则。


<details>
  <summary>Details</summary>
Motivation: 帮助没有经验的学者理解ECN的参数和结构选取对性能的影响，同时填补领域经验不足的问题。

Method: 通过基准任务（时间序列预测、模式生成、混沌系统预测和时间序列分类）分析架构、设计和参数的影响。

Result: 展示了架构、设计和参数选择对ESN性能的影响，对新手建立有效模型提供了帮助。

Conclusion: 论文为探索和理解ESN的配置与性能提供了宝贵见解，为设计成功的ESN系统提供了指导。

Abstract: This paper examines Echo State Network, a reservoir computer, performance
using four different benchmark problems, then proposes heuristics or rules of
thumb for configuring the architecture, as well as the selection of parameters
and their values, which are applicable to problems within the same domain, to
help serve to fill the experience gap needed by those entering this field of
study. The influence of various parameter selections and their value
adjustments, as well as architectural changes made to an Echo State Network, a
powerful recurrent neural network configured as a reservoir computer, can be
challenging to fully comprehend without experience in the field, and even some
hyperparameter optimization algorithms may have difficulty adjusting parameter
values without proper manual selections made first. Therefore, it is imperative
to understand the effects of parameters and their value selection on Echo State
Network architecture performance for a successful build. Thus, to address the
requirement for an extensive background in Echo State Network architecture, as
well as examine how Echo State Network performance is affected with respect to
variations in architecture, design, and parameter selection and values, a
series of benchmark tasks representing different problem domains, including
time series prediction, pattern generation, chaotic system prediction, and time
series classification, were modeled and experimented on to show the impact on
the performance of Echo State Network.

</details>
