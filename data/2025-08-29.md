<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 98]
- [cs.CL](#cs.CL) [Total: 47]
- [cs.AI](#cs.AI) [Total: 24]
- [cs.LG](#cs.LG) [Total: 64]
- [cs.NE](#cs.NE) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization](https://arxiv.org/abs/2508.20181)
*Alberto Compagnoni,Davide Caffagni,Nicholas Moratelli,Lorenzo Baraldi,Marcella Cornia,Rita Cucchiara*

Main category: cs.CV

TL;DR: 本文提出了CHAIR-DPO方法，通过利用CHAIR指标评估生成内容的幻觉度，并对多模态大语言模型（MLLMs）进行微调，成功减少了幻觉内容生成的问题，同时无须依赖复杂的合成数据管道或专有模型。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs虽然在多项任务中表现优异，但存在生成幻觉内容的问题。本研究旨在通过解决这一对齐问题，提升模型生成非幻觉内容的能力。

Method: 利用CHAIR指标对模型生成答案的幻觉程度进行评估，采用Direct Preference Optimization（DPO）方法对MLLM进行微调，从而减少生成幻觉内容的倾向。

Result: 所提出的CHAIR-DPO方法在多个幻觉基准测试上显著降低了幻觉内容的生成比例，证明了基于CHAIR奖励微调模型的效果。

Conclusion: 通过CHAIR-DPO方法，无需复杂的训练管道即可有效减轻MLLM的幻觉问题，为多模态模型的进一步发展提供了思路，同时代码和训练模型对外公开。

Abstract: Multimodal Large Language Models (MLLMs) emerge as a unified interface to
address a multitude of tasks, ranging from NLP to computer vision. Despite
showcasing state-of-the-art results in many benchmarks, a long-standing issue
is the tendency of MLLMs to hallucinate, that is to generate answers to the
user's query that are not reflected in the visual input. In this paper, we
address the problem of hallucinations as an alignment problem, seeking to steer
the MLLM so that it prefers generating content without hallucinations. In
contrast to recent approaches that require complicated pipelines to build
synthetic preference data for alignment training, often relying on proprietary
models, we capitalize on the well-known CHAIR metric, originally proposed to
gauge the degree of hallucinations in image captioning. Given a pair of
generated answers, we leverage CHAIR to distinguish winner and loser options
(i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf
MLLMs via Direct Preference Optimization (DPO). The resulting method, which we
refer to as CHAIR-DPO, effectively diminishes the amount of hallucinated
answers on several hallucination benchmarks, demonstrating the effectiveness of
fine-tuning the MLLM with a CHAIR-based reward. Source code and trained models
are publicly available at https://github.com/aimagelab/CHAIR-DPO.

</details>


### [2] [SDiFL: Stable Diffusion-Driven Framework for Image Forgery Localization](https://arxiv.org/abs/2508.20182)
*Yang Su,Shunquan Tan,Jiwu Huang*

Main category: cs.CV

TL;DR: 本文提出利用Stable Diffusion（SD）的多模态架构和图像生成能力，用于更高效、准确的图像伪造区域检测，实验结果表明其性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前随着多模态大型模型的兴起，图像操控技术与伪造检测的较量愈发激烈，现有检测方法难以应对新型复杂的图像伪造技术，需要开发更高效的检测手段。

Method: 本文提出将 Stable Diffusion（SD）的多模态架构为基础，利用其强大的感知能力，结合图像伪造残差作为显式模态，通过潜空间的多模态融合实现更加精确的伪造定位。

Result: 实验中提出的框架在主流基准数据集上的伪造定位性能相较于最先进方法提高了12%，并在真实文档伪造图像和自然场景伪造图像中表现出色，即使这种数据在训练中完全未出现。

Conclusion: 利用SD模型多模态架构的革新性方法可以显著提升伪造检测准确度，显示了该方法适应当前图像伪造检测挑战的潜力。

Abstract: Driven by the new generation of multi-modal large models, such as Stable
Diffusion (SD), image manipulation technologies have advanced rapidly, posing
significant challenges to image forensics. However, existing image forgery
localization methods, which heavily rely on labor-intensive and costly
annotated data, are struggling to keep pace with these emerging image
manipulation technologies. To address these challenges, we are the first to
integrate both image generation and powerful perceptual capabilities of SD into
an image forensic framework, enabling more efficient and accurate forgery
localization. First, we theoretically show that the multi-modal architecture of
SD can be conditioned on forgery-related information, enabling the model to
inherently output forgery localization results. Then, building on this
foundation, we specifically leverage the multimodal framework of Stable
DiffusionV3 (SD3) to enhance forgery localization performance.We leverage the
multi-modal processing capabilities of SD3 in the latent space by treating
image forgery residuals -- high-frequency signals extracted using specific
highpass filters -- as an explicit modality. This modality is fused into the
latent space during training to enhance forgery localization performance.
Notably, our method fully preserves the latent features extracted by SD3,
thereby retaining the rich semantic information of the input image.
Experimental results show that our framework achieves up to 12% improvements in
performance on widely used benchmarking datasets compared to current
state-of-the-art image forgery localization models. Encouragingly, the model
demonstrates strong performance on forensic tasks involving real-world document
forgery images and natural scene forging images, even when such data were
entirely unseen during training.

</details>


### [3] [Grounding Multimodal Large Language Models with Quantitative Skin Attributes: A Retrieval Study](https://arxiv.org/abs/2508.20188)
*Max Torop,Masih Eskandar,Nicholas Kurtansky,Jinyang Liu,Jochen Weber,Octavia Camps,Veronica Rotemberg,Jennifer Dy,Kivanc Kose*

Main category: cs.CV

TL;DR: 本文探讨了通过结合多模态大型语言模型（MLLM）和定量属性使用来提高人工智能诊断皮肤疾病的可解释性，并通过SLICE-3D数据集的案例研究验证了方法效果。


<details>
  <summary>Details</summary>
Motivation: 尽管AI模型表现出了在诊断皮肤疾病方面的成功，但其预测结果的可解释性有待提高，阻碍了其实用性发展。

Method: 本文结合多模态大型语言模型（MLLM）与基于病灶外观的定量特征，采用微调MLLM来预测定量属性值，并通过内容相关的图像检索对嵌入空间的概念基础进行验证。

Result: 研究表明，通过将MLLM嵌入空间与定量属性联系起来，可以显著提高AI模型的可解释性。

Conclusion: 结合自然语言解释能力的多模态语言模型和基于定量属性的方法能够帮助改善AI诊断系统的透明性及可靠性。

Abstract: Artificial Intelligence models have demonstrated significant success in
diagnosing skin diseases, including cancer, showing the potential to assist
clinicians in their analysis. However, the interpretability of model
predictions must be significantly improved before they can be used in practice.
To this end, we explore the combination of two promising approaches: Multimodal
Large Language Models (MLLMs) and quantitative attribute usage. MLLMs offer a
potential avenue for increased interpretability, providing reasoning for
diagnosis in natural language through an interactive format. Separately, a
number of quantitative attributes that are related to lesion appearance (e.g.,
lesion area) have recently been found predictive of malignancy with high
accuracy. Predictions grounded as a function of such concepts have the
potential for improved interpretability. We provide evidence that MLLM
embedding spaces can be grounded in such attributes, through fine-tuning to
predict their values from images. Concretely, we evaluate this grounding in the
embedding space through an attribute-specific content-based image retrieval
case study using the SLICE-3D dataset.

</details>


### [4] [Enhancing Automatic Modulation Recognition With a Reconstruction-Driven Vision Transformer Under Limited Labels](https://arxiv.org/abs/2508.20193)
*Hossein Ahmadi,Banafsheh Saffari*

Main category: cs.CV

TL;DR: 提出了一个使用Vision Transformer（ViT）的新框架解决自动调制识别（AMR）问题，特别是在低标签数据环境下表现显著。


<details>
  <summary>Details</summary>
Motivation: 当前解决AMR的问题依赖于大规模标注数据集或多阶段训练管道，这些方法在可扩展性和泛化能力上存在限制。

Method: 提出一种结合监督、自监督和重构目标的统一ViT框架，包含ViT编码器、轻量级卷积解码器和线性分类器，通过重构分支将增强信号映射回原始信号，从而在预训练期间促进鲁棒和可区分的特征学习，同时在微调过程中通过部分标签监督实现高效分类。

Result: 在RML2018.01A数据集上，在低标签环境中，该方法的性能优于监督式CNN和ViT基线模型，只需15-20%的标签数据便可接近ResNet的精度，并在不同的信噪比（SNR）下保持强劲性能。

Conclusion: 该框架提供了一种简单、可推广的且标签高效的AMR解决方案。

Abstract: Automatic modulation recognition (AMR) is critical for cognitive radio,
spectrum monitoring, and secure wireless communication. However, existing
solutions often rely on large labeled datasets or multi-stage training
pipelines, which limit scalability and generalization in practice. We propose a
unified Vision Transformer (ViT) framework that integrates supervised,
self-supervised, and reconstruction objectives. The model combines a ViT
encoder, a lightweight convolutional decoder, and a linear classifier; the
reconstruction branch maps augmented signals back to their originals, anchoring
the encoder to fine-grained I/Q structure. This strategy promotes robust,
discriminative feature learning during pretraining, while partial label
supervision in fine-tuning enables effective classification with limited
labels. On the RML2018.01A dataset, our approach outperforms supervised CNN and
ViT baselines in low-label regimes, approaches ResNet-level accuracy with only
15-20% labeled data, and maintains strong performance across varying SNR
levels. Overall, the framework provides a simple, generalizable, and
label-efficient solution for AMR.

</details>


### [5] [InfinityHuman: Towards Long-Term Audio-Driven Human](https://arxiv.org/abs/2508.20210)
*Xiaodi Li,Pan Xie,Yi Ren,Qijun Gan,Chen Zhang,Fangyuan Kong,Xiang Yin,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

TL;DR: 提出了InfinityHuman框架，解决生成高分辨率、长时视频中的一致性和自然性问题，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动的人体动画方法在生成一致外观、长时、自然的动作视频上存在挑战，问题包括身份漂移、场景不稳定和手部动作与音频不匹配。

Method: 提出了粗到精的框架（InfinityHuman），通过生成音频同步表示并逐步细化，采用姿势引导的精炼器增强一致性，并引入手部特定奖励机制提升语义精度和手势自然度。

Result: 在EMTD和HDTF数据集上表现出色，在视频质量、身份保持、手部动作准确性及唇同步性方面达到最先进水平。

Conclusion: InfinityHuman框架有效解决了音频驱动的人体动画生成中的关键问题，并显著提升了生成效果。

Abstract: Audio-driven human animation has attracted wide attention thanks to its
practical applications. However, critical challenges remain in generating
high-resolution, long-duration videos with consistent appearance and natural
hand motions. Existing methods extend videos using overlapping motion frames
but suffer from error accumulation, leading to identity drift, color shifts,
and scene instability. Additionally, hand movements are poorly modeled,
resulting in noticeable distortions and misalignment with the audio. In this
work, we propose InfinityHuman, a coarse-to-fine framework that first generates
audio-synchronized representations, then progressively refines them into
high-resolution, long-duration videos using a pose-guided refiner. Since pose
sequences are decoupled from appearance and resist temporal degradation, our
pose-guided refiner employs stable poses and the initial frame as a visual
anchor to reduce drift and improve lip synchronization. Moreover, to enhance
semantic accuracy and gesture realism, we introduce a hand-specific reward
mechanism trained with high-quality hand motion data. Experiments on the EMTD
and HDTF datasets show that InfinityHuman achieves state-of-the-art performance
in video quality, identity preservation, hand accuracy, and lip-sync. Ablation
studies further confirm the effectiveness of each module. Code will be made
public.

</details>


### [6] [Spherical Vision Transformers for Audio-Visual Saliency Prediction in 360-Degree Videos](https://arxiv.org/abs/2508.20221)
*Mert Cokelek,Halit Ozsoy,Nevrez Imamoglu,Cagri Ozcinar,Inci Ayhan,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

TL;DR: 本研究探讨了全景视频中的视觉显著性预测，并提出两个新模型SalViT360和SalViT360-AV，以改进预测效果，并展示了结合空间音频对预测精确性的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于当前缺乏综合性的数据集用于360度音视显著性预测，本研究的动机是填补这种研究空白，并探索如何有效利用音视线索预测全景视频中的视觉显著性。

Method: 提出了两个显著性预测模型：SalViT360（基于视觉变换器的框架，采用球面几何时空注意力层）和SalViT360-AV（在SalViT360基础上结合了基于音频输入的转换器适配器）。同时新建了一个包含81部全景视频的YT360-EyeTracking数据集，支持多音视条件下的研究。

Result: 两个模型在多个基准数据集（包括YT360-EyeTracking）中的表现显著优于现有方法，在360度场景中准确地预测了用户的注意力分布。

Conclusion: 结合空间音频线索的模型架构对全景视频中的显著性预测至关重要，新模型大幅提升了预测准确性，同时提供了相关代码和数据集以促进进一步研究。

Abstract: Omnidirectional videos (ODVs) are redefining viewer experiences in virtual
reality (VR) by offering an unprecedented full field-of-view (FOV). This study
extends the domain of saliency prediction to 360-degree environments,
addressing the complexities of spherical distortion and the integration of
spatial audio. Contextually, ODVs have transformed user experience by adding a
spatial audio dimension that aligns sound direction with the viewer's
perspective in spherical scenes. Motivated by the lack of comprehensive
datasets for 360-degree audio-visual saliency prediction, our study curates
YT360-EyeTracking, a new dataset of 81 ODVs, each observed under varying
audio-visual conditions. Our goal is to explore how to utilize audio-visual
cues to effectively predict visual saliency in 360-degree videos. Towards this
aim, we propose two novel saliency prediction models: SalViT360, a
vision-transformer-based framework for ODVs equipped with spherical
geometry-aware spatio-temporal attention layers, and SalViT360-AV, which
further incorporates transformer adapters conditioned on audio input. Our
results on a number of benchmark datasets, including our YT360-EyeTracking,
demonstrate that SalViT360 and SalViT360-AV significantly outperform existing
methods in predicting viewer attention in 360-degree scenes. Interpreting these
results, we suggest that integrating spatial audio cues in the model
architecture is crucial for accurate saliency prediction in omnidirectional
videos. Code and dataset will be available at
https://cyberiada.github.io/SalViT360.

</details>


### [7] [A Novel Framework for Automated Explain Vision Model Using Vision-Language Models](https://arxiv.org/abs/2508.20227)
*Phu-Vinh Nguyen,Tan-Hanh Pham,Chris Ngo,Truong Son Hy*

Main category: cs.CV

TL;DR: 本研究提出了一种结合视觉模型与xAI分析的流程，用于解释视觉模型行为，并揭示可能的失败案例和趋势。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉模型开发多关注指标性能，较少注重模型的可解释性，而理解模型的整体行为对避免偏见判断和总结模型趋势至关重要。

Method: 提出了一种整合视觉-语言模型的管道，可对视觉模型进行样本级和数据集级的解释分析。

Result: 该流程可以揭示视觉模型的失败案例和行为趋势，并较低成本地提供洞察。

Conclusion: 通过将视觉模型发展与xAI分析整合，有助于提升图像分析能力并改善模型开发流程。

Abstract: The development of many vision models mainly focuses on improving their
performance using metrics such as accuracy, IoU, and mAP, with less attention
to explainability due to the complexity of applying xAI methods to provide a
meaningful explanation of trained models. Although many existing xAI methods
aim to explain vision models sample-by-sample, methods explaining the general
behavior of vision models, which can only be captured after running on a large
dataset, are still underexplored. Furthermore, understanding the behavior of
vision models on general images can be very important to prevent biased
judgments and help identify the model's trends and patterns. With the
application of Vision-Language Models, this paper proposes a pipeline to
explain vision models at both the sample and dataset levels. The proposed
pipeline can be used to discover failure cases and gain insights into vision
models with minimal effort, thereby integrating vision model development with
xAI analysis to advance image analysis.

</details>


### [8] [ATMS-KD: Adaptive Temperature and Mixed Sample Knowledge Distillation for a Lightweight Residual CNN in Agricultural Embedded Systems](https://arxiv.org/abs/2508.20232)
*Mohamed Ohamouddou,Said Ohamouddou,Abdellatif El Afia,Rafik Lasri*

Main category: cs.CV

TL;DR: 提出了ATMS-KD框架，通过适应性温度调度和混合样本增强，将MobileNetV3教师模型的知识迁移到轻量级残差卷积神经网络（CNN）学生模型，应用于资源受限的农业环境。


<details>
  <summary>Details</summary>
Motivation: 为了解决农业领域资源受限环境下的计算机视觉问题，开发适合的轻量级CNN模型。

Method: 结合适应性温度调度与混合样本增强，从5.7M参数的MobileNetV3教师模型向三种残差CNN学生模型进行知识蒸馏，学生模型分别为1.3M、2.4M、3.8M参数规模，使用从摩洛哥农业场地采集的Damask rose图像数据集进行验证。

Result: 在Damascena玫瑰成熟度分类任务中，ATMS-KD框架显著优于传统直接训练和11种已有知识蒸馏方法，模型精度最高达97.11%，保留知识率超过99%，推理延迟仅72.19ms。

Conclusion: ATMS-KD框架能够有效提升轻量级模型在农业视觉应用中的准确性与效率，为资源受限环境下农业智能提供了新方案。

Abstract: This study proposes ATMS-KD (Adaptive Temperature and Mixed-Sample Knowledge
Distillation), a novel framework for developing lightweight CNN models suitable
for resource-constrained agricultural environments. The framework combines
adaptive temperature scheduling with mixed-sample augmentation to transfer
knowledge from a MobileNetV3 Large teacher model (5.7\,M parameters) to
lightweight residual CNN students. Three student configurations were evaluated:
Compact (1.3\,M parameters), Standard (2.4\,M parameters), and Enhanced (3.8\,M
parameters). The dataset used in this study consists of images of \textit{Rosa
damascena} (Damask rose) collected from agricultural fields in the Dades Oasis,
southeastern Morocco, providing a realistic benchmark for agricultural computer
vision applications under diverse environmental conditions. Experimental
evaluation on the Damascena rose maturity classification dataset demonstrated
significant improvements over direct training methods. All student models
achieved validation accuracies exceeding 96.7\% with ATMS-KD compared to
95--96\% with direct training. The framework outperformed eleven established
knowledge distillation methods, achieving 97.11\% accuracy with the compact
model -- a 1.60 percentage point improvement over the second-best approach
while maintaining the lowest inference latency of 72.19\,ms. Knowledge
retention rates exceeded 99\% for all configurations, demonstrating effective
knowledge transfer regardless of student model capacity.

</details>


### [9] [Linking heterogeneous microstructure informatics with expert characterization knowledge through customized and hybrid vision-language representations for industrial qualification](https://arxiv.org/abs/2508.20243)
*Mutahar Safdar,Gentry Wood,Max Zimmermann,Guy Lamouche,Priti Wanjara,Yaoyao Fiona Zhao*

Main category: cs.CV

TL;DR: 该研究提出了一种将微观结构信息学与专家表征知识结合的新框架，利用深度语义分割和多模态模型（如CLIP和FLAVA）对数据进行编码，实现了对微观结构的零样本分类。验证表明该框架在增材制造的金属基复合材料中对样品的合格与否有卓越的区分能力。


<details>
  <summary>Details</summary>
Motivation: 当前先进材料的快速高效鉴定是工业制造中的瓶颈，特别是在非传统增材制造过程中对异质结构的质量评估难以标准化并具有即时性。

Method: 研究通过深度语义分割结合多模态模型，开发了一种定制的相似性表征，将视觉微观结构数据和文本专家评估编码成共享表征，实现零样本分类，并利用Z分数正则化，优化评分对齐和分类能力。

Result: 实验结果表明，该方法成功区分了增材制造金属基复合材料数据集中合格与缺陷样品。FLAVA模型在视觉敏感性方面更胜一筹，而CLIP模型在文本标准一致性上表现出色。

Conclusion: 本方法通过增强原始数据与专家知识的语义互操作性，提高了质量鉴定的可追溯性和解释性，使其无需任务特定的模型重新训练，有助于打造可扩展、领域适配的工程信息学鉴定策略。

Abstract: Rapid and reliable qualification of advanced materials remains a bottleneck
in industrial manufacturing, particularly for heterogeneous structures produced
via non-conventional additive manufacturing processes. This study introduces a
novel framework that links microstructure informatics with a range of expert
characterization knowledge using customized and hybrid vision-language
representations (VLRs). By integrating deep semantic segmentation with
pre-trained multi-modal models (CLIP and FLAVA), we encode both visual
microstructural data and textual expert assessments into shared
representations. To overcome limitations in general-purpose embeddings, we
develop a customized similarity-based representation that incorporates both
positive and negative references from expert-annotated images and their
associated textual descriptions. This allows zero-shot classification of
previously unseen microstructures through a net similarity scoring approach.
Validation on an additively manufactured metal matrix composite dataset
demonstrates the framework's ability to distinguish between acceptable and
defective samples across a range of characterization criteria. Comparative
analysis reveals that FLAVA model offers higher visual sensitivity, while the
CLIP model provides consistent alignment with the textual criteria. Z-score
normalization adjusts raw unimodal and cross-modal similarity scores based on
their local dataset-driven distributions, enabling more effective alignment and
classification in the hybrid vision-language framework. The proposed method
enhances traceability and interpretability in qualification pipelines by
enabling human-in-the-loop decision-making without task-specific model
retraining. By advancing semantic interoperability between raw data and expert
knowledge, this work contributes toward scalable and domain-adaptable
qualification strategies in engineering informatics.

</details>


### [10] [MedNet-PVS: A MedNeXt-Based Deep Learning Model for Automated Segmentation of Perivascular Spaces](https://arxiv.org/abs/2508.20256)
*Zhen Xuen Brandon Low,Rory Zhang,Hang Min,William Pham,Lucy Vivash,Jasmine Moses,Miranda Lynch,Karina Dorfman,Cassandra Marotta,Shaun Koh,Jacob Bunyamin,Ella Rowsthorn,Alex Jarema,Himashi Peiris,Zhaolin Chen,Sandy R. Shultz,David K. Wright,Dexiao Kong,Sharon L. Naismith,Terence J. O'Brien,Ying Xia,Meng Law,Benjamin Sinclair*

Main category: cs.CV

TL;DR: 本研究使用Transformer风格的MedNeXt-L-k5网络，用于自动分割扩大的围血管间隙（PVS），并进行多数据集验证，但其效能未超过现有的nnU-Net。


<details>
  <summary>Details</summary>
Motivation: 扩大的围血管间隙（PVS）是多种脑部疾病的重要生物标志，但其人工分割存在高成本及一致性差，现有自动模型性能尚未能有效泛化多样化数据集。

Method: 研究采用适配的MedNeXt-L-k5卷积神经网络（引入Transformer机制）进行PVS分割，并训练了分别基于T1w和T2w MRI数据集的两个模型；通过5折交叉验证（5FCV）和留出一个场地交叉验证（LOSOCV）评估性能。

Result: 在T2w数据集训练下达到最高的Dice分数（0.88±0.06），符合数据集的人工标注一致性；T1w数据表现更差，LOSOCV下分数显著下降，指示模型在泛化能力上的局限性。

Conclusion: MedNeXt-L-k5在不同MRI数据集下为PVS分割提供了高效解决方案，但未达到nnU-Net的性能水平，显示Transformer机制在此任务中可能并不必要。

Abstract: Enlarged perivascular spaces (PVS) are increasingly recognized as biomarkers
of cerebral small vessel disease, Alzheimer's disease, stroke, and
aging-related neurodegeneration. However, manual segmentation of PVS is
time-consuming and subject to moderate inter-rater reliability, while existing
automated deep learning models have moderate performance and typically fail to
generalize across diverse clinical and research MRI datasets. We adapted
MedNeXt-L-k5, a Transformer-inspired 3D encoder-decoder convolutional network,
for automated PVS segmentation. Two models were trained: one using a
homogeneous dataset of 200 T2-weighted (T2w) MRI scans from the Human
Connectome Project-Aging (HCP-Aging) dataset and another using 40 heterogeneous
T1-weighted (T1w) MRI volumes from seven studies across six scanners. Model
performance was evaluated using internal 5-fold cross validation (5FCV) and
leave-one-site-out cross validation (LOSOCV). MedNeXt-L-k5 models trained on
the T2w images of the HCP-Aging dataset achieved voxel-level Dice scores of
0.88+/-0.06 (white matter, WM), comparable to the reported inter-rater
reliability of that dataset, and the highest yet reported in the literature.
The same models trained on the T1w images of the HCP-Aging dataset achieved a
substantially lower Dice score of 0.58+/-0.09 (WM). Under LOSOCV, the model had
voxel-level Dice scores of 0.38+/-0.16 (WM) and 0.35+/-0.12 (BG), and
cluster-level Dice scores of 0.61+/-0.19 (WM) and 0.62+/-0.21 (BG).
MedNeXt-L-k5 provides an efficient solution for automated PVS segmentation
across diverse T1w and T2w MRI datasets. MedNeXt-L-k5 did not outperform the
nnU-Net, indicating that the attention-based mechanisms present in
transformer-inspired models to provide global context are not required for high
accuracy in PVS segmentation.

</details>


### [11] [Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation](https://arxiv.org/abs/2508.20265)
*Zhixiang Chi,Yanan Wu,Li Gu,Huan Liu,Ziqiang Wang,Yang Zhang,Yang Wang,Konstantinos N. Plataniotis*

Main category: cs.CV

TL;DR: CLIP在开放词汇分割任务中因定位不佳面临挑战，该研究提出一种免训练的自适应框架，通过输出驱动的反馈自适应机制提高模型性能。


<details>
  <summary>Details</summary>
Motivation: CLIP虽然具有强大的视觉-文本对齐能力，但在开放词汇分割任务中表现不佳，主要是因为中间注意力缺乏与文本表示的直接交互和最终投影操作造成的语义一致性问题。

Method: 提出了一种免训练的反馈驱动自适应框架，利用模型输出的补丁级语义信息，增强中间表示与最终预测之间的语义一致性，并通过注意力隔离、基于置信度的稀疏自适应剪枝以及自适应集成模块实现。

Result: 该方法作为一个插件模块，与四种最先进方法和三种主流模型（ViT-B, ViT-L, ViT-H）集成，显著提升其在八个基准任务中的性能。

Conclusion: 该框架无需额外训练便能显著提高CLIP在开放词汇分割任务中的表现，展示了其高效性和通用性。

Abstract: CLIP exhibits strong visual-textual alignment but struggle with
open-vocabulary segmentation due to poor localization. Prior methods enhance
spatial coherence by modifying intermediate attention. But, this coherence
isn't consistently propagated to the final output due to subsequent operations
such as projections. Additionally, intermediate attention lacks direct
interaction with text representations, such semantic discrepancy limits the
full potential of CLIP.
  In this work, we propose a training-free, feedback-driven self-adaptive
framework that adapts output-based patch-level correspondences back to the
intermediate attention. The output predictions, being the culmination of the
model's processing, encapsulate the most comprehensive visual and textual
semantics about each patch. Our approach enhances semantic consistency between
internal representations and final predictions by leveraging the model's
outputs as a stronger spatial coherence prior. We design key modules, including
attention isolation, confidence-based pruning for sparse adaptation, and
adaptation ensemble, to effectively feedback the output coherence cues. Our
method functions as a plug-in module, seamlessly integrating into four
state-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H). We
further validate our framework across multiple attention types (Q-K, self-self,
and Proxy augmented with MAE, SAM, and DINO). Our approach consistently
improves their performance across eight benchmarks.

</details>


### [12] [How Multimodal LLMs Solve Image Tasks: A Lens on Visual Grounding, Task Reasoning, and Answer Decoding](https://arxiv.org/abs/2508.20279)
*Zhuoran Yu,Yong Jae Lee*

Main category: cs.CV

TL;DR: 提出了一种用于分析多模态大语言模型（MLLM）的处理动态的探测框架，通过分层探测揭示其视觉和文本输入的处理过程。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在视觉-语言任务上表现强劲，但其内部处理动态尚未被深入研究。因此本研究旨在通过探测模型层次的动态以建立对其工作机制的理解。

Method: 利用标准化锚问题提取每层的token嵌入，训练线性分类器预测细粒度视觉分类，并通过三种提示变体类型的控制实验探测模型各层的功能角色。

Result: 在模型（如LLaVA-1.5等）中发现了一种一致的阶段性结构：早期层进行视觉定位，中间层支持词汇整合和语义推理，最终层生成任务特定输出。模型的阶段性结构在不同训练条件下稳定，但层分配与基础LLM架构的变化有关。

Conclusion: 本研究提出了一种轻量化、模型无关的分析框架，揭示了多模态大语言模型的层次组织方式及其多模态表征动态。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated strong performance
across a wide range of vision-language tasks, yet their internal processing
dynamics remain underexplored. In this work, we introduce a probing framework
to systematically analyze how MLLMs process visual and textual inputs across
layers. We train linear classifiers to predict fine-grained visual categories
(e.g., dog breeds) from token embeddings extracted at each layer, using a
standardized anchor question. To uncover the functional roles of different
layers, we evaluate these probes under three types of controlled prompt
variations: (1) lexical variants that test sensitivity to surface-level
changes, (2) semantic negation variants that flip the expected answer by
modifying the visual concept in the prompt, and (3) output format variants that
preserve reasoning but alter the answer format. Applying our framework to
LLaVA-1.5, LLaVA-Next-LLaMA-3, and Qwen2-VL, we identify a consistent
stage-wise structure in which early layers perform visual grounding, middle
layers support lexical integration and semantic reasoning, and final layers
prepare task-specific outputs. We further show that while the overall
stage-wise structure remains stable across variations in visual tokenization,
instruction tuning data, and pretraining corpus, the specific layer allocation
to each stage shifts notably with changes in the base LLM architecture. Our
findings provide a unified perspective on the layer-wise organization of MLLMs
and offer a lightweight, model-agnostic approach for analyzing multimodal
representation dynamics.

</details>


### [13] [Disentangling Latent Embeddings with Sparse Linear Concept Subspaces (SLiCS)](https://arxiv.org/abs/2508.20322)
*Zhi Li,Hau Phan,Matthew Emigh,Austin J. Brockmeier*

Main category: cs.CV

TL;DR: 本文提出了一种监督字典学习方法，用于将视觉语言嵌入（如CLIP）分解为多概念特定组件，从而实现更加精确的概念过滤图像检索。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言嵌入中解耦复杂场景信息的潜力，并构建能反映多标签信息的分解方法。

Method: 提出了一种监督字典学习方法，利用稀疏的非负组合生成分组向量表征概念，并通过交替优化实现字典学习的收敛。

Result: 证明了稀疏线性概念子空间（SLiCS）方法在更精确的图像检索任务中表现出更高的精度，并适用于多种嵌入模型。

Conclusion: 稀疏线性概念子空间可以有效解耦嵌入中的语义，并为图像检索和生成任务提供更精确的支持。

Abstract: Vision-language co-embedding networks, such as CLIP, provide a latent
embedding space with semantic information that is useful for downstream tasks.
We hypothesize that the embedding space can be disentangled to separate the
information on the content of complex scenes by decomposing the embedding into
multiple concept-specific component vectors that lie in different subspaces. We
propose a supervised dictionary learning approach to estimate a linear
synthesis model consisting of sparse, non-negative combinations of groups of
vectors in the dictionary (atoms), whose group-wise activity matches the
multi-label information. Each concept-specific component is a non-negative
combination of atoms associated to a label. The group-structured dictionary is
optimized through a novel alternating optimization with guaranteed convergence.
Exploiting the text co-embeddings, we detail how semantically meaningful
descriptions can be found based on text embeddings of words best approximated
by a concept's group of atoms, and unsupervised dictionary learning can exploit
zero-shot classification of training set images using the text embeddings of
concept labels to provide instance-wise multi-labels. We show that the
disentangled embeddings provided by our sparse linear concept subspaces (SLiCS)
enable concept-filtered image retrieval (and conditional generation using
image-to-prompt) that is more precise. We also apply SLiCS to highly-compressed
autoencoder embeddings from TiTok and the latent embedding from self-supervised
DINOv2. Quantitative and qualitative results highlight the improved precision
of the concept-filtered image retrieval for all embeddings.

</details>


### [14] [MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models](https://arxiv.org/abs/2508.20345)
*Xiao Li,Yanfan Zhu,Ruining Deng,Wei-Qi Wei,Yu Wang,Shilin Zhao,Yaohong Wang,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: 本论文提出了MedFoundationHub，一个用于支持医用视觉-语言模型（VLMs）的图形化界面工具，旨在解决隐私问题和部署挑战。


<details>
  <summary>Details</summary>
Motivation: 医学视觉-语言模型在自动报告生成、医生助手和不确定性量化等临床应用中潜力巨大，但存在PHI泄露等安全问题，特别是在医院环境中。

Method: 开发了MedFoundationHub工具，使医生无需编程即可操作，并支持工程师快速部署这些模型，同时通过Docker实现隐私保护。

Result: 通过让病理学家评估五种先进的VLMs，分析其在案例中的表现，共产生1015个评分事件。结果发现，这些模型存在共性问题，如答案偏离主题、推理模糊、不一致的专业术语等。

Conclusion: MedFoundationHub提供安全、易用的医疗VLM解决方案，但当前模型存在显著局限性，需进一步优化应用效果。

Abstract: Recent advances in medical vision-language models (VLMs) open up remarkable
opportunities for clinical applications such as automated report generation,
copilots for physicians, and uncertainty quantification. However, despite their
promise, medical VLMs introduce serious security concerns, most notably risks
of Protected Health Information (PHI) exposure, data leakage, and vulnerability
to cyberthreats - which are especially critical in hospital environments. Even
when adopted for research or non-clinical purposes, healthcare organizations
must exercise caution and implement safeguards. To address these challenges, we
present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)
enables physicians to manually select and use different models without
programming expertise, (2) supports engineers in efficiently deploying medical
VLMs in a plug-and-play fashion, with seamless integration of Hugging Face
open-source models, and (3) ensures privacy-preserving inference through
Docker-orchestrated, operating system agnostic deployment. MedFoundationHub
requires only an offline local workstation equipped with a single NVIDIA A6000
GPU, making it both secure and accessible within the typical resources of
academic research labs. To evaluate current capabilities, we engaged
board-certified pathologists to deploy and assess five state-of-the-art VLMs
(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and
LLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,
yielding 1015 clinician-model scoring events. These assessments revealed
recurring limitations, including off-target answers, vague reasoning, and
inconsistent pathology terminology.

</details>


### [15] [Enhancing Mamba Decoder with Bidirectional Interaction in Multi-Task Dense Prediction](https://arxiv.org/abs/2508.20376)
*Mang Cao,Sanping Zhou,Yizhe Li,Ye Deng,Wenli Huang,Le Wang*

Main category: cs.CV

TL;DR: 研究提出了双向交互Mamba（BIM）方法，通过新颖的扫描机制提升多任务密集预测任务中的跨任务交互效率与质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多任务密集预测中面临交互完整性和计算效率之间的权衡问题。

Method: 提出了双向交互扫描（BI-Scan）机制与多尺度扫描（MS-Scan）机制，实现了高效的任务间信息交互和多粒度场景建模。

Result: 在NYUD-V2和PASCAL-Context两个基准上实验，证明了方法相较于当前最先进的技术的优越性。

Conclusion: 所提方法能够有效提升多任务密集预测任务的性能，同时兼具高效性和灵活性，适应不同任务需求。

Abstract: Sufficient cross-task interaction is crucial for success in multi-task dense
prediction. However, sufficient interaction often results in high computational
complexity, forcing existing methods to face the trade-off between interaction
completeness and computational efficiency. To address this limitation, this
work proposes a Bidirectional Interaction Mamba (BIM), which incorporates novel
scanning mechanisms to adapt the Mamba modeling approach for multi-task dense
prediction. On the one hand, we introduce a novel Bidirectional Interaction
Scan (BI-Scan) mechanism, which constructs task-specific representations as
bidirectional sequences during interaction. By integrating task-first and
position-first scanning modes within a unified linear complexity architecture,
BI-Scan efficiently preserves critical cross-task information. On the other
hand, we employ a Multi-Scale Scan~(MS-Scan) mechanism to achieve
multi-granularity scene modeling. This design not only meets the diverse
granularity requirements of various tasks but also enhances nuanced cross-task
feature interactions. Extensive experiments on two challenging benchmarks,
\emph{i.e.}, NYUD-V2 and PASCAL-Context, show the superiority of our BIM vs its
state-of-the-art competitors.

</details>


### [16] [Audio-Guided Visual Editing with Complex Multi-Modal Prompts](https://arxiv.org/abs/2508.20379)
*Hyeonyu Kim,Seokhoon Jeong,Seonghee Han,Chanhyuk Choi,Taehwan Kim*

Main category: cs.CV

TL;DR: 提出了一种不需要额外训练的新框架，通过音频提示进行复杂场景的视觉编辑，包括多文本和音频提示的结合，为现有方法提供了新的可能性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的视觉编辑方法难以处理复杂场景，仅依赖文本指导不足，急需非文本编辑提示的支持。

Method: 运用一种预训练的多模态编码器，具备强大的零样本能力，解决音频编码器空间与扩散模型提示编码器空间的关联；并提出通过独立的噪声分支和自适应补丁选择来处理复杂多模态的编辑提示场景。

Result: 实验表明，该框架在丰富音频信息结合下，优于传统单文本方法，能更好处理复杂的编辑场景。

Conclusion: 通过整合音频提示，提出的方法有效应对复杂视觉编辑任务，为未来多模态编辑提供参考方向。

Abstract: Visual editing with diffusion models has made significant progress but often
struggles with complex scenarios that textual guidance alone could not
adequately describe, highlighting the need for additional non-text editing
prompts. In this work, we introduce a novel audio-guided visual editing
framework that can handle complex editing tasks with multiple text and audio
prompts without requiring additional training. Existing audio-guided visual
editing methods often necessitate training on specific datasets to align audio
with text, limiting their generalization to real-world situations. We leverage
a pre-trained multi-modal encoder with strong zero-shot capabilities and
integrate diverse audio into visual editing tasks, by alleviating the
discrepancy between the audio encoder space and the diffusion model's prompt
encoder space. Additionally, we propose a novel approach to handle complex
scenarios with multiple and multi-modal editing prompts through our separate
noise branching and adaptive patch selection. Our comprehensive experiments on
diverse editing tasks demonstrate that our framework excels in handling
complicated editing scenarios by incorporating rich information from audio,
where text-only approaches fail.

</details>


### [17] [More Reliable Pseudo-labels, Better Performance: A Generalized Approach to Single Positive Multi-label Learning](https://arxiv.org/abs/2508.20381)
*Luong Tran,Thieu Vo,Anh Nguyen,Sang Dinh,Van Nguyen*

Main category: cs.CV

TL;DR: 研究针对于部分标注数据的多标签学习挑战，提出GPR Loss和DAMP技术，组成AEVLP框架，并在四个基准数据集上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 在多标签学习中，完全标注大规模数据集代价高昂，因此研究如何利用部分标注数据成为一个重要课题。文中聚焦于极端情况下的单一正标签多标签学习问题。

Method: 提出一种通用的伪标签鲁棒损失函数（GPR Loss），能够有效处理不同伪标签并减轻噪声；同时提出动态增强多焦点伪标注（DAMP）技术，增强伪标签效果。

Result: 实验表明，提出的AEVLP框架在四个基准数据集上实现了多标签分类的最新研究成果。

Conclusion: AEVLP框架结合了强大的伪标签学习方法和鲁棒的损失函数，对于部分标注数据的多标签学习问题提供了解决方案，并展示了显著的性能提升。

Abstract: Multi-label learning is a challenging computer vision task that requires
assigning multiple categories to each image. However, fully annotating
large-scale datasets is often impractical due to high costs and effort,
motivating the study of learning from partially annotated data. In the extreme
case of Single Positive Multi-Label Learning (SPML), each image is provided
with only one positive label, while all other labels remain unannotated.
Traditional SPML methods that treat missing labels as unknown or negative tend
to yield inaccuracies and false negatives, and integrating various
pseudo-labeling strategies can introduce additional noise. To address these
challenges, we propose the Generalized Pseudo-Label Robust Loss (GPR Loss), a
novel loss function that effectively learns from diverse pseudo-labels while
mitigating noise. Complementing this, we introduce a simple yet effective
Dynamic Augmented Multi-focus Pseudo-labeling (DAMP) technique. Together, these
contributions form the Adaptive and Efficient Vision-Language Pseudo-Labeling
(AEVLP) framework. Extensive experiments on four benchmark datasets demonstrate
that our framework significantly advances multi-label classification, achieving
state-of-the-art results.

</details>


### [18] [Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection](https://arxiv.org/abs/2508.20392)
*Chengjun Zhang,Yuhao Zhang,Jie Yang,Mohamad Sawan*

Main category: cs.CV

TL;DR: 本文提出了延迟-脉冲方法和时间依赖性Integrate-and-Fire（tdIF）神经元架构用于脉冲神经网络（SNN），并在低时间步内实现了视觉检测任务的卓越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的ANN-SNN转换方法在分类任务中表现优秀，但在视觉检测任务中表现不佳。

Method: 提出延迟-脉冲方法解决异质脉冲模式造成的残余膜电位问题，并设计了一种时间依赖性Integrate-and-Fire（tdIF）神经元架构，动态调整神经元累积和点火行为，提升时间特性。

Result: 方法实现了低时间步内更精确的特征表示，显著提升了对象检测和车道线检测的性能，达到了最新技术水平（低于5时间步）。

Conclusion: 通过本文提出的方法，SNN不仅能在视觉检测任务中实现无与伦比的低延迟性能，同时兼顾了能耗与传统神经元一致。

Abstract: Spiking Neural Networks (SNNs), inspired by the brain, are characterized by
minimal power consumption and swift inference capabilities on neuromorphic
hardware, and have been widely applied to various visual perception tasks.
Current ANN-SNN conversion methods have achieved excellent results in
classification tasks with ultra-low time-steps, but their performance in visual
detection tasks remains suboptimal. In this paper, we propose a delay-spike
approach to mitigate the issue of residual membrane potential caused by
heterogeneous spiking patterns. Furthermore, we propose a novel
temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This
enables Integrate-and-fire (IF) neurons to dynamically adjust their
accumulation and firing behaviors based on the temporal order of time-steps.
Our method enables spikes to exhibit distinct temporal properties, rather than
relying solely on frequency-based representations. Moreover, the tdIF neuron
maintains energy consumption on par with traditional IF neuron. We demonstrate
that our method achieves more precise feature representation with lower
time-steps, enabling high performance and ultra-low latency in visual detection
tasks. In this study, we conduct extensive evaluation of the tdIF method across
two critical vision tasks: object detection and lane line detection. The
results demonstrate that the proposed method surpasses current ANN-SNN
conversion approaches, achieving state-of-the-art performance with ultra-low
latency (within 5 time-steps).

</details>


### [19] [Graph-Based Uncertainty Modeling and Multimodal Fusion for Salient Object Detection](https://arxiv.org/abs/2508.20415)
*Yuqi Xiong,Wuzhen Shi,Yang Wen,Ruhan Liu*

Main category: cs.CV

TL;DR: 本文提出DUP-MCRNet网络，解决显著性检测中细节丢失、边缘模糊和单模态信息融合不足的问题，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有显著性检测方法在复杂场景中容易丢失细节、边缘模糊以及单模态信息融合不足，亟需一个能够高效处理小结构、边缘区域和多模态信息的方法。

Method: 设计了动态不确定性图卷积模块（DUGC）来传播不确定性，并提出了多模态协同融合策略（MCF）结合RGB、深度及边缘特征，同时引入多尺度损失和约束机制优化检测性能。

Result: DUP-MCRNet在多个常用基准数据集上超越多种显著性检测方法，特别是在边缘清晰性及复杂背景的鲁棒性方面表现出色。

Conclusion: DUP-MCRNet通过动态不确定性传播和多模态协同推理，有效解决了当前显著性检测的关键挑战，提升了检测效果，尤其适用于复杂场景。

Abstract: In view of the problems that existing salient object detection (SOD) methods
are prone to losing details, blurring edges, and insufficient fusion of
single-modal information in complex scenes, this paper proposes a dynamic
uncertainty propagation and multimodal collaborative reasoning network
(DUP-MCRNet). Firstly, a dynamic uncertainty graph convolution module (DUGC) is
designed to propagate uncertainty between layers through a sparse graph
constructed based on spatial semantic distance, and combined with channel
adaptive interaction, it effectively improves the detection accuracy of small
structures and edge regions. Secondly, a multimodal collaborative fusion
strategy (MCF) is proposed, which uses learnable modality gating weights to
weightedly fuse the attention maps of RGB, depth, and edge features. It can
dynamically adjust the importance of each modality according to different
scenes, effectively suppress redundant or interfering information, and
strengthen the semantic complementarity and consistency between
cross-modalities, thereby improving the ability to identify salient regions
under occlusion, weak texture or background interference. Finally, the
detection performance at the pixel level and region level is optimized through
multi-scale BCE and IoU loss, cross-scale consistency constraints, and
uncertainty-guided supervision mechanisms. Extensive experiments show that
DUP-MCRNet outperforms various SOD methods on most common benchmark datasets,
especially in terms of edge clarity and robustness to complex backgrounds. Our
code is publicly available at https://github.com/YukiBear426/DUP-MCRNet.

</details>


### [20] [MSMVD: Exploiting Multi-scale Image Features via Multi-scale BEV Features for Multi-view Pedestrian Detection](https://arxiv.org/abs/2508.20447)
*Taiga Yamane,Satoshi Suzuki,Ryo Masumura,Shota Orihashi,Tomohiro Tanaka,Mana Ihori,Naoki Makishima,Naotaka Kawata*

Main category: cs.CV

TL;DR: 本文提出了多尺度多视图检测（MSMVD）方法，用于解决多视图行人检测中因缩放问题导致的检测困难。


<details>
  <summary>Details</summary>
Motivation: 解决在多视图行人检测中，由于图像中行人尺寸缩放变化较大或不同视图间的缩放比例差异显著，导致的检测效果较差问题。

Method: 提出了MSMVD方法，通过逐尺度地将多视图中的多尺度图像特征投影到鸟瞰视角（BEV）空间来生成多尺度BEV特征，并利用特征金字塔网络结合多视图中不同尺度的信息。

Result: 大量实验表明，利用多尺度BEV特征显著提升了检测性能，MSMVD在GMVD数据集上的MODA指标比之前最高分高出4.5。

Conclusion: 结合多视图和多尺度特征的MSMVD方法有效提升了行人检测性能，特别是在尺寸变化显著的场景中表现突出。

Abstract: Multi-View Pedestrian Detection (MVPD) aims to detect pedestrians in the form
of a bird's eye view (BEV) from multi-view images. In MVPD, end-to-end
trainable deep learning methods have progressed greatly. However, they often
struggle to detect pedestrians with consistently small or large scales in views
or with vastly different scales between views. This is because they do not
exploit multi-scale image features to generate the BEV feature and detect
pedestrians. To overcome this problem, we propose a novel MVPD method, called
Multi-Scale Multi-View Detection (MSMVD). MSMVD generates multi-scale BEV
features by projecting multi-scale image features extracted from individual
views into the BEV space, scale-by-scale. Each of these BEV features inherits
the properties of its corresponding scale image features from multiple views.
Therefore, these BEV features help the precise detection of pedestrians with
consistently small or large scales in views. Then, MSMVD combines information
at different scales of multiple views by processing the multi-scale BEV
features using a feature pyramid network. This improves the detection of
pedestrians with vastly different scales between views. Extensive experiments
demonstrate that exploiting multi-scale image features via multi-scale BEV
features greatly improves the detection performance, and MSMVD outperforms the
previous highest MODA by $4.5$ points on the GMVD dataset.

</details>


### [21] [A Spatial-Frequency Aware Multi-Scale Fusion Network for Real-Time Deepfake Detection](https://arxiv.org/abs/2508.20449)
*Libo Lv,Tianyi Wang,Mengxiao Huang,Ruixia Liu,Yinglong Wang*

Main category: cs.CV

TL;DR: 提出一种名为SFMFNet的轻量化架构，通过空间频率混合模块、选择性跨注意机制和残差模糊池化结构，实现实时的deepfake检测。


<details>
  <summary>Details</summary>
Motivation: 现有的deepfake检测方法尽管在基准测试中有高准确率，但计算成本过高，难以在实际应用中实时部署。

Method: 设计了SFMFNet框架，包含空间-频率混合感知模块、选择性跨注意机制以及残差增强模糊池化结构，综合利用空间纹理与频率伪造特征。

Result: 实验结果表明，SFMFNet在多个基准数据集上实现了精度与效率的良好平衡，同时具备较强的泛化能力。

Conclusion: SFMFNet架构不仅能高效检测deepfake，还具有实际应用中的实时检测价值。

Abstract: With the rapid advancement of real-time deepfake generation techniques,
forged content is becoming increasingly realistic and widespread across
applications like video conferencing and social media. Although
state-of-the-art detectors achieve high accuracy on standard benchmarks, their
heavy computational cost hinders real-time deployment in practical
applications. To address this, we propose the Spatial-Frequency Aware
Multi-Scale Fusion Network (SFMFNet), a lightweight yet effective architecture
for real-time deepfake detection. We design a spatial-frequency hybrid aware
module that jointly leverages spatial textures and frequency artifacts through
a gated mechanism, enhancing sensitivity to subtle manipulations. A
token-selective cross attention mechanism enables efficient multi-level feature
interaction, while a residual-enhanced blur pooling structure helps retain key
semantic cues during downsampling. Experiments on several benchmark datasets
show that SFMFNet achieves a favorable balance between accuracy and efficiency,
with strong generalization and practical value for real-time applications.

</details>


### [22] [Dual-Model Weight Selection and Self-Knowledge Distillation for Medical Image Classification](https://arxiv.org/abs/2508.20461)
*Ayaka Tsutsumi,Guang Li,Ren Togo,Takahiro Ogawa,Satoshi Kondo,Miki Haseyama*

Main category: cs.CV

TL;DR: 提出了一种整合双模型权重选择与自我知识蒸馏的医疗影像分类方法，兼顾性能和计算效率，对比现有方法表现出优越性。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的情况下，开发性能与大规模模型相当但计算效率更高的轻量化模型成关键需求。

Method: 使用双模型权重选择策略从大规模模型迁移权重至两个轻量模型，并结合自我知识蒸馏，在目标任务中进行微调。

Result: 在胸部X光、肺部CT、脑部MRI等公开数据集上进行测试，新方法优于现有方法。

Conclusion: 双模型权重选择结合自我知识蒸馏能有效保留信息，同时提高轻量模型的性能与鲁棒性，可实际应用于资源受限环境。

Abstract: We propose a novel medical image classification method that integrates
dual-model weight selection with self-knowledge distillation (SKD). In
real-world medical settings, deploying large-scale models is often limited by
computational resource constraints, which pose significant challenges for their
practical implementation. Thus, developing lightweight models that achieve
comparable performance to large-scale models while maintaining computational
efficiency is crucial. To address this, we employ a dual-model weight selection
strategy that initializes two lightweight models with weights derived from a
large pretrained model, enabling effective knowledge transfer. Next, SKD is
applied to these selected models, allowing the use of a broad range of initial
weight configurations without imposing additional excessive computational cost,
followed by fine-tuning for the target classification tasks. By combining
dual-model weight selection with self-knowledge distillation, our method
overcomes the limitations of conventional approaches, which often fail to
retain critical information in compact models. Extensive experiments on
publicly available datasets-chest X-ray images, lung computed tomography scans,
and brain magnetic resonance imaging scans-demonstrate the superior performance
and robustness of our approach compared to existing methods.

</details>


### [23] [Re-Densification Meets Cross-Scale Propagation: Real-Time Compression of LiDAR Point Clouds](https://arxiv.org/abs/2508.20466)
*Pengpeng Yu,Haoran Li,Dingquan Li,Runqing Jiang,Jing Wang,Liang Lin,Yulan Guo*

Main category: cs.CV

TL;DR: 本文提出了一个高效的预测编码框架，通过几何再稠密化和跨尺度特征传播模块，实现了高效上下文建模和压缩过程加速，并在KITTI数据集上达到了先进的压缩性能和实时性表现。


<details>
  <summary>Details</summary>
Motivation: 目前LiDAR点云高精度扫描会带来巨大的存储和传输开销，而现有方法因几何细节的稀疏性限制了压缩性能和速度。

Method: 设计了几何再稠密化模块和跨尺度特征传播模块，其中前者通过稠密化后提取特征并再稀疏化，而后者利用多分辨率的占用提示来指导特征传播，最大化上下文信息的利用。

Result: 在KITTI数据集上验证，方法达到最先进的26 FPS实时性能（编码解码均为12位量化），并提供代码实现以供使用。

Conclusion: 本文方法不仅提高了压缩效率和性能，还节省了计算成本，在实用性和效果上具有显著优势。

Abstract: LiDAR point clouds are fundamental to various applications, yet
high-precision scans incur substantial storage and transmission overhead.
Existing methods typically convert unordered points into hierarchical octree or
voxel structures for dense-to-sparse predictive coding. However, the extreme
sparsity of geometric details hinders efficient context modeling, thereby
limiting their compression performance and speed. To address this challenge, we
propose to generate compact features for efficient predictive coding. Our
framework comprises two lightweight modules. First, the Geometry
Re-Densification Module re-densifies encoded sparse geometry, extracts features
at denser scale, and then re-sparsifies the features for predictive coding.
This module avoids costly computation on highly sparse details while
maintaining a lightweight prediction head. Second, the Cross-scale Feature
Propagation Module leverages occupancy cues from multiple resolution levels to
guide hierarchical feature propagation. This design facilitates information
sharing across scales, thereby reducing redundant feature extraction and
providing enriched features for the Geometry Re-Densification Module. By
integrating these two modules, our method yields a compact feature
representation that provides efficient context modeling and accelerates the
coding process. Experiments on the KITTI dataset demonstrate state-of-the-art
compression ratios and real-time performance, achieving 26 FPS for both
encoding and decoding at 12-bit quantization. Code is available at
https://github.com/pengpeng-yu/FastPCC.

</details>


### [24] [Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation](https://arxiv.org/abs/2508.20470)
*Xiaochuan Li,Guoguang Du,Runze Zhang,Liang Jin,Qi Jia,Lihua Lu,Zhenhua Guo,Yaqian Zhao,Haiyang Liu,Tianqi Wang,Changsheng Li,Xiaoli Gong,Rengang Li,Baoyu Fan*

Main category: cs.CV

TL;DR: 本文介绍了一种利用视频数据生成3D资产的方法，通过Droplet3D-4M数据集和Droplet3D生成模型，实现了空间一致性和语义合理性的新成果。


<details>
  <summary>Details</summary>
Motivation: 3D领域数据稀缺已成为生成模型发展的瓶颈，而视频数据蕴含有常识先验，可以作为一种替代的监督信号来增强3D生成。

Method: 引入Droplet3D-4M大规模视频数据集并开发Droplet3D生成模型，通过数据的多视角标注及模型的语义和空间一致性训练，提升3D资产生成能力。

Result: 实验表明，该方法生成的内容具有空间一致性和语义合理性，并能够扩展应用于场景级3D生成任务。

Conclusion: 视频数据中的常识先验显著促进了3D生成，同时该方法的开源资源为后续研究和应用提供了支持。

Abstract: Scaling laws have validated the success and promise of large-data-trained
models in creative generation across text, image, and video domains. However,
this paradigm faces data scarcity in the 3D domain, as there is far less of it
available on the internet compared to the aforementioned modalities.
Fortunately, there exist adequate videos that inherently contain commonsense
priors, offering an alternative supervisory signal to mitigate the
generalization bottleneck caused by limited native 3D data. On the one hand,
videos capturing multiple views of an object or scene provide a spatial
consistency prior for 3D generation. On the other hand, the rich semantic
information contained within the videos enables the generated content to be
more faithful to the text prompts and semantically plausible. This paper
explores how to apply the video modality in 3D asset generation, spanning
datasets to models. We introduce Droplet3D-4M, the first large-scale video
dataset with multi-view level annotations, and train Droplet3D, a generative
model supporting both image and dense text input. Extensive experiments
validate the effectiveness of our approach, demonstrating its ability to
produce spatially consistent and semantically plausible content. Moreover, in
contrast to the prevailing 3D solutions, our approach exhibits the potential
for extension to scene-level applications. This indicates that the commonsense
priors from the videos significantly facilitate 3D creation. We have
open-sourced all resources including the dataset, code, technical framework,
and model weights: https://dropletx.github.io/.

</details>


### [25] [Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation](https://arxiv.org/abs/2508.20471)
*Jiusi Li,Jackson Jiang,Jinyu Miao,Miao Long,Tuopu Wen,Peijin Jia,Shengxiang Liu,Chunlei Yu,Maolin Liu,Yuzhan Cai,Kun Jiang,Mengmeng Yang,Diange Yang*

Main category: cs.CV

TL;DR: G^2Editor是一种用于自动驾驶视频的对象编辑框架，通过3D高斯表示和额外条件引导，提升编辑的视觉品质和位置控制能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在视觉真实性和姿态控制上的不足，提供安全便捷的角案例收集方式。

Method: 提出G^2Editor框架，利用3D高斯表示结合去噪过程，实现精确的姿态控制和空间一致性；并通过分层细粒度特征指导外观细节生成。

Result: G^2Editor在Waymo开放数据集上显著优于现有方法，支持对象重定位、插入和删除，同时在控制性和视觉质量上表现优秀。

Conclusion: 该框架为数据驱动任务提供了高效支持，是生成多样化、安全角案例的高效工具。

Abstract: Corner cases are crucial for training and validating autonomous driving
systems, yet collecting them from the real world is often costly and hazardous.
Editing objects within captured sensor data offers an effective alternative for
generating diverse scenarios, commonly achieved through 3D Gaussian Splatting
or image generative models. However, these approaches often suffer from limited
visual fidelity or imprecise pose control. To address these issues, we propose
G^2Editor, a framework designed for photorealistic and precise object editing
in driving videos. Our method leverages a 3D Gaussian representation of the
edited object as a dense prior, injected into the denoising process to ensure
accurate pose control and spatial consistency. A scene-level 3D bounding box
layout is employed to reconstruct occluded areas of non-target objects.
Furthermore, to guide the appearance details of the edited object, we
incorporate hierarchical fine-grained features as additional conditions during
generation. Experiments on the Waymo Open Dataset demonstrate that G^2Editor
effectively supports object repositioning, insertion, and deletion within a
unified framework, outperforming existing methods in both pose controllability
and visual quality, while also benefiting downstream data-driven tasks.

</details>


### [26] [Enhancing Corpus Callosum Segmentation in Fetal MRI via Pathology-Informed Domain Randomization](https://arxiv.org/abs/2508.20475)
*Marina Grifell i Plana,Vladyslav Zalevskyi,Léa Schmidt,Yvan Gomez,Thomas Sanchez,Vincent Dunet,Mériam Koob,Vanessa Siffredi,Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: 本文提出一种结合 CCD 病变先验知识的领域随机化策略，用于生成合成数据，从而改进胎儿脑部分割模型的鲁棒性和拓扑一致性。主要目标是弥补 CCD 数据稀缺的问题，提高疾病分型和生物标记推导的效果。


<details>
  <summary>Details</summary>
Motivation: 由于 CCD 导致的脑部变化对诊断的重要性，然而 CCD 数据的稀缺严重限制了深度学习模型在此领域的泛化能力，迫切需要提升模型在稀有病理情况下的表现。

Method: 该研究提出了一种基于 CCD 病变知识的路径信息领域随机化策略，通过健康数据生成包含多样化病理变化的合成数据，以减少病理注释的需要。

Result: 在包含248例健康胎儿、26例CCD病患和47例其他脑病变胎儿的测试中，表现出对CCD案例显著的改进，同时保持了对健康和其他病变胎儿的分类性能；LCC估计误差在健康和CCD病患中分别从1.89 mm减少到0.80 mm和从10.9 mm减少到0.7 mm。

Conclusion: 将特定领域解剖学先验知识融入合成数据生成能有效解决数据稀缺性，提高对稀有重要病变的分析能力。

Abstract: Accurate fetal brain segmentation is crucial for extracting biomarkers and
assessing neurodevelopment, especially in conditions such as corpus callosum
dysgenesis (CCD), which can induce drastic anatomical changes. However, the
rarity of CCD severely limits annotated data, hindering the generalization of
deep learning models. To address this, we propose a pathology-informed domain
randomization strategy that embeds prior knowledge of CCD manifestations into a
synthetic data generation pipeline. By simulating diverse brain alterations
from healthy data alone, our approach enables robust segmentation without
requiring pathological annotations.
  We validate our method on a cohort comprising 248 healthy fetuses, 26 with
CCD, and 47 with other brain pathologies, achieving substantial improvements on
CCD cases while maintaining performance on both healthy fetuses and those with
other pathologies. From the predicted segmentations, we derive clinically
relevant biomarkers, such as corpus callosum length (LCC) and volume, and show
their utility in distinguishing CCD subtypes. Our pathology-informed
augmentation reduces the LCC estimation error from 1.89 mm to 0.80 mm in
healthy cases and from 10.9 mm to 0.7 mm in CCD cases. Beyond these
quantitative gains, our approach yields segmentations with improved topological
consistency relative to available ground truth, enabling more reliable
shape-based analyses. Overall, this work demonstrates that incorporating
domain-specific anatomical priors into synthetic data pipelines can effectively
mitigate data scarcity and enhance analysis of rare but clinically significant
malformations.

</details>


### [27] [Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding](https://arxiv.org/abs/2508.20476)
*Jeong Hun Yeo,Hyeongseop Rha,Sungjune Park,Junil Won,Yong Man Ro*

Main category: cs.CV

TL;DR: 本论文提出了一个统一框架，将手语、唇读与音频三种模态整合用于生成口语文本，并在多个任务上达到或超越最先进模型。


<details>
  <summary>Details</summary>
Motivation: 针对现有语音识别系统对听障个体的不友好性，探索一个可以整合手语、唇读与音频模态的统一框架，以提升音频缺失情况下的交流能力。

Method: 设计一个模态无关的架构，能够有效处理异构输入；研究多模态之间的协同作用，特别是唇动作为手语理解中的非手势提示；目标是实现与专用模型相当或更优的性能。

Result: 所提出的框架在手语翻译（SLT）、视觉语音识别（VSR）、自动语音识别（ASR）和音视频语音识别（AVSR）任务上达到或超越最新模型。同时发现显式建模唇动这一模态显著提升手语翻译性能。

Conclusion: 通过本研究，展示了一个统一框架的潜力，既能整合多模态输入，又能显著提升多项任务的性能，特别在手语翻译中唇动的作用尤为突出。

Abstract: Audio is the primary modality for human communication and has driven the
success of Automatic Speech Recognition (ASR) technologies. However, such
systems remain inherently inaccessible to individuals who are deaf or hard of
hearing. Visual alternatives such as sign language and lip reading offer
effective substitutes, and recent advances in Sign Language Translation (SLT)
and Visual Speech Recognition (VSR) have improved audio-less communication.
Yet, these modalities have largely been studied in isolation, and their
integration within a unified framework remains underexplored. In this paper, we
introduce the first unified framework capable of handling diverse combinations
of sign language, lip movements, and audio for spoken-language text generation.
We focus on three main objectives: (i) designing a unified, modality-agnostic
architecture capable of effectively processing heterogeneous inputs; (ii)
exploring the underexamined synergy among modalities, particularly the role of
lip movements as non-manual cues in sign language comprehension; and (iii)
achieving performance on par with or superior to state-of-the-art models
specialized for individual tasks. Building on this framework, we achieve
performance on par with or better than task-specific state-of-the-art models
across SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that
explicitly modeling lip movements as a separate modality significantly improves
SLT performance.

</details>


### [28] [Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding](https://arxiv.org/abs/2508.20478)
*Yuan Xie,Tianshui Chen,Zheng Ge,Lionel Ni*

Main category: cs.CV

TL;DR: 提出Video-MTR, 一种多回合推理框架，通过逐步选择关键视频片段和理解问题来分析长视频，实现端到端训练并提升准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态推理或外部视觉-语言模型，这些方法因无法端到端训练而表现复杂且效果不佳，因而需要更高效的长视频理解方法。

Method: 提出Video-MTR，通过多回合推理逐步优化视频片段选择与问题理解，结合新颖的双层奖励机制以端到端方式提升性能。

Result: 在VideoMME、MLVU和EgoSchema等基准测试上表现优于现有方法，显示出更高的准确性和效率。

Conclusion: Video-MTR在长视频理解中展示了对现有技术水平的显著提升，并提供了一种创新的多回合推理方法。

Abstract: Long-form video understanding, characterized by long-range temporal
dependencies and multiple events, remains a challenge. Existing methods often
rely on static reasoning or external visual-language models (VLMs), which face
issues like complexity and sub-optimal performance due to the lack of
end-to-end training. In this paper, we propose Video-MTR, a reinforced
multi-turn reasoning framework designed to enable iterative key video segment
selection and question comprehension. Unlike traditional video reasoning
pipeline, which generate predictions in a single turn, Video-MTR performs
reasoning in multiple turns, selecting video segments progressively based on
the evolving understanding of previously processed segments and the current
question. This iterative process allows for a more refined and contextually
aware analysis of the video. To ensure intermediate reasoning process, we
introduce a novel gated bi-level reward system, combining trajectory-level
rewards based on answer correctness and turn-level rewards emphasizing
frame-query relevance. This system optimizes both video segment selection and
question comprehension, eliminating the need for external VLMs and allowing
end-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU,
and EgoSchema demonstrate that Video-MTR outperforms existing methods in both
accuracy and efficiency, advancing the state-of-the-art in long video
understanding.

</details>


### [29] [Adaptive Dual Uncertainty Optimization: Boosting Monocular 3D Object Detection under Test-Time Shifts](https://arxiv.org/abs/2508.20488)
*Zixuan Hu,Dongxiao Li,Xinzhu Ma,Shixiang Tang,Xiaotong Li,Wenhan Yang,Ling-Yu Duan*

Main category: cs.CV

TL;DR: 提出一种新的测试时自适应框架DUO，针对单目3D目标检测中存在的语义和几何双重不确定性问题，实现了更鲁棒的性能。


<details>
  <summary>Details</summary>
Motivation: 单目3D目标检测在真实环境中的域偏移下可靠性显著下降。如何在测试时适应目标分布，提高鲁棒性成为关键问题。

Method: 通过优化语义和几何双重不确定性，提出DUO框架，创新性地利用对数焦损失的凸结构，并设计了语义感知的法向域约束。

Result: DUO展示了在多种数据集和域偏移类型上的优越性，提高了单目3D目标检测的性能。

Conclusion: DUO框架通过两分支机制有效减弱不确定性，验证了其在复杂变化环境下的鲁棒性，具有显著优势。

Abstract: Accurate monocular 3D object detection (M3OD) is pivotal for safety-critical
applications like autonomous driving, yet its reliability deteriorates
significantly under real-world domain shifts caused by environmental or sensor
variations. To address these shifts, Test-Time Adaptation (TTA) methods have
emerged, enabling models to adapt to target distributions during inference.
While prior TTA approaches recognize the positive correlation between low
uncertainty and high generalization ability, they fail to address the dual
uncertainty inherent to M3OD: semantic uncertainty (ambiguous class
predictions) and geometric uncertainty (unstable spatial localization). To
bridge this gap, we propose Dual Uncertainty Optimization (DUO), the first TTA
framework designed to jointly minimize both uncertainties for robust M3OD.
Through a convex optimization lens, we introduce an innovative convex structure
of the focal loss and further derive a novel unsupervised version, enabling
label-agnostic uncertainty weighting and balanced learning for high-uncertainty
objects. In parallel, we design a semantic-aware normal field constraint that
preserves geometric coherence in regions with clear semantic cues, reducing
uncertainty from the unstable 3D representation. This dual-branch mechanism
forms a complementary loop: enhanced spatial perception improves semantic
classification, and robust semantic predictions further refine spatial
understanding. Extensive experiments demonstrate the superiority of DUO over
existing methods across various datasets and domain shift types.

</details>


### [30] [CaddieSet: A Golf Swing Dataset with Human Joint Features and Ball Information](https://arxiv.org/abs/2508.20491)
*Seunghyeon Jung,Seoyoung Hong,Jiwoo Jeong,Seungwon Jeong,Jaerim Choi,Hoki Kim,Woojin Lee*

Main category: cs.CV

TL;DR: 研究开发了名为CaddieSet的新数据集，通过计算机视觉分阶段分析高尔夫挥杆动作，预测球的轨迹，并提供可解释的挥杆反馈。


<details>
  <summary>Details</summary>
Motivation: 现有研究尚未定量建立挥杆姿势与球轨迹间的关系，难以为高尔夫球员提供必要的挥杆改进建议。

Method: 提出了CaddieSet数据集，利用计算机视觉技术将高尔夫挥杆视频分为8个阶段，提取关节信息，并定义15个挥杆关键指标预测球轨迹。

Result: 实验表明，CaddieSet可以通过可解释模型预测球的轨迹，并验证了挥杆反馈与已有领域知识的定量一致性。

Conclusion: CaddieSet为高尔夫挥杆分析提供了新见解，对学术界和体育行业都有潜在价值。

Abstract: Recent advances in deep learning have led to more studies to enhance golfers'
shot precision. However, these existing studies have not quantitatively
established the relationship between swing posture and ball trajectory,
limiting their ability to provide golfers with the necessary insights for swing
improvement. In this paper, we propose a new dataset called CaddieSet, which
includes joint information and various ball information from a single shot.
CaddieSet extracts joint information from a single swing video by segmenting it
into eight swing phases using a computer vision-based approach. Furthermore,
based on expert golf domain knowledge, we define 15 key metrics that influence
a golf swing, enabling the interpretation of swing outcomes through
swing-related features. Through experiments, we demonstrated the feasibility of
CaddieSet for predicting ball trajectories using various benchmarks. In
particular, we focus on interpretable models among several benchmarks and
verify that swing feedback using our joint features is quantitatively
consistent with established domain knowledge. This work is expected to offer
new insight into golf swing analysis for both academia and the sports industry.

</details>


### [31] [IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection](https://arxiv.org/abs/2508.20492)
*Xuanming Cao,Chengyu Tao,Yifeng Cheng,Juan Du*

Main category: cs.CV

TL;DR: 提出了一种名为IAENet的框架，通过结合2D与3D模型来增强3D点云表面异常检测的性能，并在MVTec 3D-AD测试中取得了最新的最先进表现。


<details>
  <summary>Details</summary>
Motivation: 由于目前缺乏类似2D图像领域中的强大预训练3D基础模型，3D点云异常检测研究受限，需要一种新方法来提升性能。

Method: 提出了IAENet框架及其核心模块重要性感知融合（IAF），动态评估和重新加权每个源模型的异常分数，并设计了专门的损失函数优化IAF。

Result: 在MVTec 3D-AD数据集上达到了新的最先进性能，并显著降低了误报率。

Conclusion: IAENet框架通过结合2D与3D专家模型的优势，展示了在工业表面异常检测中的高实用价值。

Abstract: Surface anomaly detection is pivotal for ensuring product quality in
industrial manufacturing. While 2D image-based methods have achieved remarkable
success, 3D point cloud-based detection remains underexplored despite its
richer geometric cues. We argue that the key bottleneck is the absence of
powerful pretrained foundation backbones in 3D comparable to those in 2D. To
bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an
ensemble framework that synergizes 2D pretrained expert with 3D expert models.
However, naively fusing predictions from disparate sources is non-trivial:
existing strategies can be affected by a poorly performing modality and thus
degrade overall accuracy. To address this challenge, We introduce an novel
Importance-Aware Fusion (IAF) module that dynamically assesses the contribution
of each source and reweights their anomaly scores. Furthermore, we devise
critical loss functions that explicitly guide the optimization of IAF, enabling
it to combine the collective knowledge of the source experts but also preserve
their unique strengths, thereby enhancing the overall performance of anomaly
detection. Extensive experiments on MVTec 3D-AD demonstrate that our IAENet
achieves a new state-of-the-art with a markedly lower false positive rate,
underscoring its practical value for industrial deployment.

</details>


### [32] [Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent](https://arxiv.org/abs/2508.20505)
*En Ci,Shanyan Guan,Yanhao Ge,Yilin Zhang,Wei Li,Zhenyu Zhang,Jian Yang,Ying Tai*

Main category: cs.CV

TL;DR: 提出了DescriptiveEdit框架，通过参考图像和描述性提示，实现更高效的语义图像编辑。


<details>
  <summary>Details</summary>
Motivation: 旨在解决当前语义图像编辑中，由于逆向算法重建误差和指令数据集的质量与规模限制而存在的挑战。

Method: 引入了Cross-Attentive UNet架构，将参考图像特征注入到基于提示的图像生成过程中，并避免了架构修改或逆向处理。

Result: 实验表明，在Emu Edit基准上，DescriptiveEdit可提高编辑精确性和一致性。

Conclusion: DescriptiveEdit框架兼容性强，克服了数据集质量问题，并可扩展以实现更高效的语义图像编辑。

Abstract: Despite the progress in text-to-image generation, semantic image editing
remains a challenge. Inversion-based algorithms unavoidably introduce
reconstruction errors, while instruction-based models mainly suffer from
limited dataset quality and scale. To address these problems, we propose a
descriptive-prompt-based editing framework, named DescriptiveEdit. The core
idea is to re-frame `instruction-based image editing' as `reference-image-based
text-to-image generation', which preserves the generative power of well-trained
Text-to-Image models without architectural modifications or inversion.
Specifically, taking the reference image and a prompt as input, we introduce a
Cross-Attentive UNet, which newly adds attention bridges to inject reference
image features into the prompt-to-edit-image generation process. Owing to its
text-to-image nature, DescriptiveEdit overcomes limitations in instruction
dataset quality, integrates seamlessly with ControlNet, IP-Adapter, and other
extensions, and is more scalable. Experiments on the Emu Edit benchmark show it
improves editing accuracy and consistency.

</details>


### [33] [DCFS: Continual Test-Time Adaptation via Dual Consistency of Feature and Sample](https://arxiv.org/abs/2508.20516)
*Wenting Yin,Han Sun,Xinru Meng,Ningzhong Liu,Huiyu Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的持续测试时间适应框架——DCFS，该框架通过引入双路径特征一致性和置信感知样本学习，解决了伪标签质量和错误累积问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖模型预测生成伪标签，但伪标签质量难以保证且可能存在错误累积问题。

Method: 设计了一个名为DCFS的框架，通过双分类器将目标数据特征分别解耦为语义相关特征和领域相关特征，并对这些子特征和整体特征保持一致性，还设置自适应阈值根据置信度进行加权自监督学习，减少伪标签噪声和错误累积。

Result: 在多种数据集（CIFAR10-C、CIFAR100-C、ImageNet-C）上的实验表明该方法能在持续测试时间适应场景中表现稳定并有效。

Conclusion: 该方法通过引入双路径特征一致性和置信感知样本学习机制，有效解决伪标签的噪声和错误累积问题，验证其在持续适应性任务中的优势。

Abstract: Continual test-time adaptation aims to continuously adapt a pre-trained model
to a stream of target domain data without accessing source data. Without access
to source domain data, the model focuses solely on the feature characteristics
of the target data. Relying exclusively on these features can lead to confusion
and introduce learning biases. Currently, many existing methods generate
pseudo-labels via model predictions. However, the quality of pseudo-labels
cannot be guaranteed and the problem of error accumulation must be solved. To
address these challenges, we propose DCFS, a novel CTTA framework that
introduces dual-path feature consistency and confidence-aware sample learning.
This framework disentangles the whole feature representation of the target data
into semantic-related feature and domain-related feature using dual classifiers
to learn distinct feature representations. By maintaining consistency between
the sub-features and the whole feature, the model can comprehensively capture
data features from multiple perspectives. Additionally, to ensure that the
whole feature information of the target domain samples is not overlooked, we
set a adaptive threshold and calculate a confidence score for each sample to
carry out loss weighted self-supervised learning, effectively reducing the
noise of pseudo-labels and alleviating the problem of error accumulation. The
efficacy of our proposed method is validated through extensive experimentation
across various datasets, including CIFAR10-C, CIFAR100-C, and ImageNet-C,
demonstrating consistent performance in continual test-time adaptation
scenarios.

</details>


### [34] [Adam SLAM - the last mile of camera calibration with 3DGS](https://arxiv.org/abs/2508.20526)
*Matthieu Gendrin,Stéphane Pateux,Xiaoran Jiang,Théo Ladune,Luce Morin*

Main category: cs.CV

TL;DR: 本文提出使用3DGS模型，通过对视图颜色损失的反向传播调整相机参数进行校准，从而提高新视图合成的质量。


<details>
  <summary>Details</summary>
Motivation: 在新视图合成中，相机校准的精度至关重要，但真实场景中无法提供校准的真实值，因此需要通过合成质量间接评价校准效果。

Method: 使用3DGS模型，通过对相机参数的视图颜色损失进行反向传播的方式，细化并调整校准参数。

Result: 通过新方法，数据集中平均取得了0.4dB PSNR的提升。

Conclusion: 尽管这种细调方法可能耗时，但对于像Mip-NeRF 360等要求高质量新视图的参考场景，其效果非常值得肯定。

Abstract: The quality of the camera calibration is of major importance for evaluating
progresses in novel view synthesis, as a 1-pixel error on the calibration has a
significant impact on the reconstruction quality. While there is no ground
truth for real scenes, the quality of the calibration is assessed by the
quality of the novel view synthesis. This paper proposes to use a 3DGS model to
fine tune calibration by backpropagation of novel view color loss with respect
to the cameras parameters. The new calibration alone brings an average
improvement of 0.4 dB PSNR on the dataset used as reference by 3DGS. The fine
tuning may be long and its suitability depends on the criticity of training
time, but for calibration of reference scenes, such as Mip-NeRF 360, the stake
of novel view quality is the most important.

</details>


### [35] [Learning What is Worth Learning: Active and Sequential Domain Adaptation for Multi-modal Gross Tumor Volume Segmentation](https://arxiv.org/abs/2508.20528)
*Jingyun Yang,Guoqing Zhang,Jingge Wang,Yang Li*

Main category: cs.CV

TL;DR: 本文提出了一种动态的主动域适应框架，用于在多模态医学数据上进行肿瘤体积分割，并显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像标注困难和高精度分割需求，提出选择最具有信息价值的样本以降低标注成本。

Method: 设计了主动和序列域适应的框架，基于样本信息性和代表性提出动态选择和标注策略，应用于多模态医学数据分割任务。

Result: 实验证明该方法在肿瘤体积分割任务中显著优于其他最新主动域适应方法。

Conclusion: 通过动态样本选择和分割增强了医学图像处理效率和准确性，提供了先进的解决方案。

Abstract: Accurate gross tumor volume segmentation on multi-modal medical data is
critical for radiotherapy planning in nasopharyngeal carcinoma and
glioblastoma. Recent advances in deep neural networks have brought promising
results in medical image segmentation, leading to an increasing demand for
labeled data. Since labeling medical images is time-consuming and
labor-intensive, active learning has emerged as a solution to reduce annotation
costs by selecting the most informative samples to label and adapting
high-performance models with as few labeled samples as possible. Previous
active domain adaptation (ADA) methods seek to minimize sample redundancy by
selecting samples that are farthest from the source domain. However, such
one-off selection can easily cause negative transfer, and access to source
medical data is often limited. Moreover, the query strategy for multi-modal
medical data remains unexplored. In this work, we propose an active and
sequential domain adaptation framework for dynamic multi-modal sample selection
in ADA. We derive a query strategy to prioritize labeling and training on the
most valuable samples based on their informativeness and representativeness.
Empirical validation on diverse gross tumor volume segmentation tasks
demonstrates that our method achieves favorable segmentation performance,
significantly outperforming state-of-the-art ADA methods. Code is available at
the git repository: \href{https://github.com/Hiyoochan/mmActS}{mmActS}.

</details>


### [36] [Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection](https://arxiv.org/abs/2508.20530)
*Mingqian Ji,Jian Yang,Shanshan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新型数据级融合框架，通过结合RGB图像和LiDAR数据进行无监督三维目标检测，以提高伪框质量和定位精度。


<details>
  <summary>Details</summary>
Motivation: 手工标注高质量三维标签既耗时又费力，因此需要一种减少依赖手动标注的无监督方法。

Method: 利用视觉基础模型进行图像实例分割和深度估计，提出双向融合策略和局部与全局过滤方法以处理噪声，同时设计基于数据级融合的方法进行动态自我进化来优化伪框。

Result: 在nuScenes数据集上实验表明，使用本文方法训练的检测器在验证基准上显著优于最新的方法，mAP达到了28.4%。

Conclusion: 数据级融合框架通过整合和优化多模态数据，能够有效提高伪框质量并实现更高的无监督三维目标检测性能。

Abstract: Existing LiDAR-based 3D object detectors typically rely on manually annotated
labels for training to achieve good performance. However, obtaining
high-quality 3D labels is time-consuming and labor-intensive. To address this
issue, recent works explore unsupervised 3D object detection by introducing RGB
images as an auxiliary modal to assist pseudo-box generation. However, these
methods simply integrate pseudo-boxes generated by LiDAR point clouds and RGB
images. Yet, such a label-level fusion strategy brings limited improvements to
the quality of pseudo-boxes, as it overlooks the complementary nature in terms
of LiDAR and RGB image data. To overcome the above limitations, we propose a
novel data-level fusion framework that integrates RGB images and LiDAR data at
an early stage. Specifically, we utilize vision foundation models for instance
segmentation and depth estimation on images and introduce a bi-directional
fusion method, where real points acquire category labels from the 2D space,
while 2D pixels are projected onto 3D to enhance real point density. To
mitigate noise from depth and segmentation estimations, we propose a local and
global filtering method, which applies local radius filtering to suppress depth
estimation errors and global statistical filtering to remove
segmentation-induced outliers. Furthermore, we propose a data-level fusion
based dynamic self-evolution strategy, which iteratively refines pseudo-boxes
under a dense representation, significantly improving localization accuracy.
Extensive experiments on the nuScenes dataset demonstrate that the detector
trained by our method significantly outperforms that trained by previous
state-of-the-art methods with 28.4$\%$ mAP on the nuScenes validation
benchmark.

</details>


### [37] [Digital Scale: Open-Source On-Device BMI Estimation from Smartphone Camera Images Trained on a Large-Scale Real-World Dataset](https://arxiv.org/abs/2508.20534)
*Frederik Rajiv Manichand,Robin Deuber,Robert Jakob,Steve Swerling,Jamie Rosen,Elgar Fleisch,Patrick Langer*

Main category: cs.CV

TL;DR: 利用智能手机照片和深度学习模型估算BMI，提出一种自动化过滤方法提升数据质量，并在多数据集上取得最佳性能。同时提供代码和移动应用框架。


<details>
  <summary>Details</summary>
Motivation: 在远程医疗或紧急场景中，传统方法不可行时，利用图像和机器学习快速估算BMI有重要应用价值。

Method: 通过深度学习方法，基于包含84,963张智能手机图像的WayBED数据集，应用自动化过滤技术清理异常数据，并优化模型训练。

Result: 在WayBED测试集上MAPE为7.9%，在VisualBodyToBMI数据集上为13%，重新微调训练后降至8.56%，创下最优结果。

Conclusion: 提出的模型精度高，具鲁棒性，并可以通过CLAID框架在安卓设备上快速部署，相关代码已开源。

Abstract: Estimating Body Mass Index (BMI) from camera images with machine learning
models enables rapid weight assessment when traditional methods are unavailable
or impractical, such as in telehealth or emergency scenarios. Existing computer
vision approaches have been limited to datasets of up to 14,500 images. In this
study, we present a deep learning-based BMI estimation method trained on our
WayBED dataset, a large proprietary collection of 84,963 smartphone images from
25,353 individuals. We introduce an automatic filtering method that uses
posture clustering and person detection to curate the dataset by removing
low-quality images, such as those with atypical postures or incomplete views.
This process retained 71,322 high-quality images suitable for training. We
achieve a Mean Absolute Percentage Error (MAPE) of 7.9% on our hold-out test
set (WayBED data) using full-body images, the lowest value in the published
literature to the best of our knowledge. Further, we achieve a MAPE of 13% on
the completely unseen~(during training) VisualBodyToBMI dataset, comparable
with state-of-the-art approaches trained on it, demonstrating robust
generalization. Lastly, we fine-tune our model on VisualBodyToBMI and achieve a
MAPE of 8.56%, the lowest reported value on this dataset so far. We deploy the
full pipeline, including image filtering and BMI estimation, on Android devices
using the CLAID framework. We release our complete code for model training,
filtering, and the CLAID package for mobile deployment as open-source
contributions.

</details>


### [38] [Domain Adaptation Techniques for Natural and Medical Image Classification](https://arxiv.org/abs/2508.20537)
*Ahmad Chaddad,Yihang Wu,Reem Kateb,Christian Desrosiers*

Main category: cs.CV

TL;DR: 本文对7种常用的领域适应(DA)技术进行了557次模拟实验，覆盖自然图像与医学图像的多种场景，指出DSAN算法在医学数据中的卓越表现。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过对自然图像和医学图像的测试，探讨领域适应技术的优势，特别是在分布差异、动态数据流和样本限制场景下的应用。

Method: 在5组自然图像和8组医学图像数据集上，使用7种DA技术进行557次模拟实验，包括DSAN算法，测试其在各种场景下的性能。采用ResNet50作为主要深度学习架构。

Result: DSAN算法显示了优越的分类性能，特别是在COVID-19数据集上达到91.2%的分类准确率，并在动态数据流场景下比基准提升6.7%的准确率。同时还在医学数据集上展现了较高的可解释性。

Conclusion: 实验结果证明了DSAN算法及其他领域适应技术在适应医学数据和增强模型性能上的潜力，为未来医学图像应用和研究提供了基础。

Abstract: Domain adaptation (DA) techniques have the potential in machine learning to
alleviate distribution differences between training and test sets by leveraging
information from source domains. In image classification, most advances in DA
have been made using natural images rather than medical data, which are harder
to work with. Moreover, even for natural images, the use of mainstream datasets
can lead to performance bias. {With the aim of better understanding the
benefits of DA for both natural and medical images, this study performs 557
simulation studies using seven widely-used DA techniques for image
classification in five natural and eight medical datasets that cover various
scenarios, such as out-of-distribution, dynamic data streams, and limited
training samples.} Our experiments yield detailed results and insightful
observations highlighting the performance and medical applicability of these
techniques. Notably, our results have shown the outstanding performance of the
Deep Subdomain Adaptation Network (DSAN) algorithm. This algorithm achieved
feasible classification accuracy (91.2\%) in the COVID-19 dataset using
Resnet50 and showed an important accuracy improvement in the dynamic data
stream DA scenario (+6.7\%) compared to the baseline. Our results also
demonstrate that DSAN exhibits remarkable level of explainability when
evaluated on COVID-19 and skin cancer datasets. These results contribute to the
understanding of DA techniques and offer valuable insight into the effective
adaptation of models to medical data.

</details>


### [39] [Contrastive Learning through Auxiliary Branch for Video Object Detection](https://arxiv.org/abs/2508.20551)
*Lucas Rakotoarivony*

Main category: cs.CV

TL;DR: 提出了一种用于视频目标检测的新方法CLAB，通过对比学习增强特征表征能力并提升鲁棒性，在ImageNet VID数据集上达到了84.0%和85.2%的mAP，刷新了基于CNN方法的性能记录。


<details>
  <summary>Details</summary>
Motivation: 视频中的图像劣化问题（如运动模糊、遮挡和形变）使得目标检测变得更具挑战性。提高检测鲁棒性同时不增加推理计算复杂度是一个亟需解决的问题。

Method: 该方法提出了一种对比学习辅助分支（CLAB），通过引入对比损失增强特征表征能力，并设计了一种动态损失加权策略，在训练初期侧重辅助特征学习，后期逐渐转向目标检测任务。

Result: 实验验证了该方法在ImageNet VID数据集上的有效性，基于ResNet-101和ResNeXt-101模型分别取得了84.0%和85.2%的mAP，达到了基于CNN模型的最新性能水平。

Conclusion: CLAB方法在不增加推理阶段计算负担的前提下，实现了视频目标检测鲁棒性和性能的提升，成为一种有效并高效的解决方案。

Abstract: Video object detection is a challenging task because videos often suffer from
image deterioration such as motion blur, occlusion, and deformable shapes,
making it significantly more difficult than detecting objects in still images.
Prior approaches have improved video object detection performance by employing
feature aggregation and complex post-processing techniques, though at the cost
of increased computational demands. To improve robustness to image degradation
without additional computational load during inference, we introduce a
straightforward yet effective Contrastive Learning through Auxiliary Branch
(CLAB) method. First, we implement a constrastive auxiliary branch using a
contrastive loss to enhance the feature representation capability of the video
object detector's backbone. Next, we propose a dynamic loss weighting strategy
that emphasizes auxiliary feature learning early in training while gradually
prioritizing the detection task as training converges. We validate our approach
through comprehensive experiments and ablation studies, demonstrating
consistent performance gains. Without bells and whistles, CLAB reaches a
performance of 84.0% mAP and 85.2% mAP with ResNet-101 and ResNeXt-101,
respectively, on the ImageNet VID dataset, thus achieving state-of-the-art
performance for CNN-based models without requiring additional post-processing
methods.

</details>


### [40] [Towards Mechanistic Defenses Against Typographic Attacks in CLIP](https://arxiv.org/abs/2508.20570)
*Lorenz Hufe,Constantin Venhoff,Maximilian Dreyer,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.CV

TL;DR: 这篇论文分析了CLIP模型在面临文字攻击时的表现，提出了不需要微调的防御方法，并发布了一系列增强鲁棒性的CLIP模型。


<details>
  <summary>Details</summary>
Motivation: 文字攻击会导致生成错误结果或模型“越狱”，因此需要开发方法抵御这些攻击，特别是对多模态系统中的CLIP模型进行防御，以保障其在安全关键环境中的表现。

Method: 通过定位CLIP模型中处理文字信息的注意力机制电路，并选择性地移除关键注意力头，从而减少它们对文字攻击的敏感性。这一方法无需对模型进行微调。

Result: 提出的防御方法在文字变体的ImageNet-100基准上性能提升了19.6%，对标准ImageNet-100的准确率影响小于1%。

Conclusion: 无需额外训练的新方法显著增强了模型对文字攻击的鲁棒性，同时保留了准确性，适合安全关键的应用环境。

Abstract: Typographic attacks exploit multi-modal systems by injecting text into
images, leading to targeted misclassifications, malicious content generation
and even Vision-Language Model jailbreaks. In this work, we analyze how CLIP
vision encoders behave under typographic attacks, locating specialized
attention heads in the latter half of the model's layers that causally extract
and transmit typographic information to the cls token. Building on these
insights, we introduce a method to defend CLIP models against typographic
attacks by selectively ablating a typographic circuit, consisting of attention
heads. Without requiring finetuning, our method improves performance by up to
19.6% on a typographic variant of ImageNet-100, while reducing standard
ImageNet-100 accuracy by less than 1%. Notably, our training-free approach
remains competitive with current state-of-the-art typographic defenses that
rely on finetuning. To this end, we release a family of dyslexic CLIP models
which are significantly more robust against typographic attacks. These models
serve as suitable drop-in replacements for a broad range of safety-critical
applications, where the risks of text-based manipulation outweigh the utility
of text recognition.

</details>


### [41] [GLaRE: A Graph-based Landmark Region Embedding Network for Emotion Recognition](https://arxiv.org/abs/2508.20579)
*Debasis Maji,Debaditya Barman*

Main category: cs.CV

TL;DR: 提出名为GLaRE的图神经网络模型，通过提取面部关键点区域嵌入，实现高效、可解释的情感识别，在AffectNet和FERG数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统面部表情识别技术因遮挡、表情多样性和难以解释性而面临性能挑战，因此需要开发新的模型来提高识别效果和可解释性。

Method: 利用3D面部对齐提取关键点，构建并处理份额图，保留面部空间结构。通过GLaRE网络将区域级嵌入用于情感识别。

Result: 在AffectNet数据集上达到64.89%的准确率，在FERG数据集上达到94.24%的准确率，优于其他主流模型。

Conclusion: 本文研究表明基于图神经网络的区域级嵌入方法能有效提升面部表情识别的表现与可解释性，为情感计算提供了新的方向。

Abstract: Facial expression recognition (FER) is a crucial task in computer vision with
wide range of applications including human computer interaction, surveillance,
and assistive technologies. However, challenges such as occlusion, expression
variability, and lack of interpretability hinder the performance of traditional
FER systems. Graph Neural Networks (GNNs) offer a powerful alternative by
modeling relational dependencies between facial landmarks, enabling structured
and interpretable learning. In this paper, we propose GLaRE, a novel
Graph-based Landmark Region Embedding network for emotion recognition. Facial
landmarks are extracted using 3D facial alignment, and a quotient graph is
constructed via hierarchical coarsening to preserve spatial structure while
reducing complexity. Our method achieves 64.89 percentage accuracy on AffectNet
and 94.24 percentage on FERG, outperforming several existing baselines.
Additionally, ablation studies have demonstrated that region-level embeddings
from quotient graphs have contributed to improved prediction performance.

</details>


### [42] [FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models](https://arxiv.org/abs/2508.20586)
*Zheng Chong,Yanwei Lei,Shiyue Zhang,Zhuandi He,Zhen Wang,Xujie Zhang,Xiao Dong,Yiling Wu,Dongmei Jiang,Xiaodan Liang*

Main category: cs.CV

TL;DR: FastFit 提供了一种高效且支持多参考服装组合的虚拟试穿框架，解决了当前方法效率低下和支持多参考的不足，同时引入了一个新数据集 DressCode-MR。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟试穿技术难以支持多参考服装组合，并因冗余计算效率低下，限制了其真实场景应用。

Method: 提出了一种基于缓存扩散架构的 FastFit 框架，采用半注意机制及类嵌入替代传统时间步嵌入，实现参考特征解耦并可复用。

Result: FastFit 在推理效率上实现了3.5倍提升，在关键保真度指标上超过了现有方法。

Conclusion: FastFit 不仅显著提升了虚拟试穿领域的效率，还为多参考服装组合研究提供了支持，并通过发布 DressCode-MR 数据集推动领域进步。

Abstract: Despite its great potential, virtual try-on technology is hindered from
real-world application by two major challenges: the inability of current
methods to support multi-reference outfit compositions (including garments and
accessories), and their significant inefficiency caused by the redundant
re-computation of reference features in each denoising step. To address these
challenges, we propose FastFit, a high-speed multi-reference virtual try-on
framework based on a novel cacheable diffusion architecture. By employing a
Semi-Attention mechanism and substituting traditional timestep embeddings with
class embeddings for reference items, our model fully decouples reference
feature encoding from the denoising process with negligible parameter overhead.
This allows reference features to be computed only once and losslessly reused
across all steps, fundamentally breaking the efficiency bottleneck and
achieving an average 3.5x speedup over comparable methods. Furthermore, to
facilitate research on complex, multi-reference virtual try-on, we introduce
DressCode-MR, a new large-scale dataset. It comprises 28,179 sets of
high-quality, paired images covering five key categories (tops, bottoms,
dresses, shoes, and bags), constructed through a pipeline of expert models and
human feedback refinement. Extensive experiments on the VITON-HD, DressCode,
and our DressCode-MR datasets show that FastFit surpasses state-of-the-art
methods on key fidelity metrics while offering its significant advantage in
inference efficiency.

</details>


### [43] [UTA-Sign: Unsupervised Thermal Video Augmentation via Event-Assisted Traffic Signage Sketching](https://arxiv.org/abs/2508.20594)
*Yuqi Han,Songqian Zhang,Weijian Su,Ke Li,Jiayu Yang,Jinli Suo,Qiang Zhang*

Main category: cs.CV

TL;DR: 本文提出UTA-Sign方法，通过将热成像和事件摄像的优点相结合，改进低光环境下交通标志的检测和表示。


<details>
  <summary>Details</summary>
Motivation: 解决热成像相机在相似材料物体上的标志盲点和事件相机采样不均的问题，从而改善低光条件下的交通标志感知。

Method: 提出一种双增强机制，以融合热成像帧和事件信号来持续表示交通标志内容，同时用热成像帧对事件信号提供时间对齐参考，增强环境理解。

Result: 结合真实世界数据集验证，方法在交通标志检测准确性和标志描绘质量上表现出色。

Conclusion: UTA-Sign提升了低光环境中交通标志的检测准确性和感知表现，展示了热成像和事件相机结合的潜力。

Abstract: The thermal camera excels at perceiving outdoor environments under low-light
conditions, making it ideal for applications such as nighttime autonomous
driving and unmanned navigation. However, thermal cameras encounter challenges
when capturing signage from objects made of similar materials, which can pose
safety risks for accurately understanding semantics in autonomous driving
systems. In contrast, the neuromorphic vision camera, also known as an event
camera, detects changes in light intensity asynchronously and has proven
effective in high-speed, low-light traffic environments. Recognizing the
complementary characteristics of these two modalities, this paper proposes
UTA-Sign, an unsupervised thermal-event video augmentation for traffic signage
in low-illumination environments, targeting elements such as license plates and
roadblock indicators. To address the signage blind spots of thermal imaging and
the non-uniform sampling of event cameras, we developed a dual-boosting
mechanism that fuses thermal frames and event signals for consistent signage
representation over time. The proposed method utilizes thermal frames to
provide accurate motion cues as temporal references for aligning the uneven
event signals. At the same time, event signals contribute subtle signage
content to the raw thermal frames, enhancing the overall understanding of the
environment. The proposed method is validated on datasets collected from
real-world scenarios, demonstrating superior quality in traffic signage
sketching and improved detection accuracy at the perceptual level.

</details>


### [44] [Disruptive Attacks on Face Swapping via Low-Frequency Perceptual Perturbations](https://arxiv.org/abs/2508.20595)
*Mengxiao Huang,Minglei Shu,Shuwang Zhou,Zhaoyang Liu*

Main category: cs.CV

TL;DR: 本论文提出了一种基于低频感知扰动的主动防御方法，扰乱Deepfake生成过程，从而削弱生成内容的性能和自然性。


<details>
  <summary>Details</summary>
Motivation: 现有Deepfake检测方法多为事后检测，无法有效预防攻击。

Method: 设计了结合频域和空间域特征的主动防御，使用低频扰动破坏生成过程，同时保护高频视觉质量，且引入编码器、扰动生成器和解码器的架构。

Result: 在CelebA-HQ和LFW数据集上，显著减少人脸置换效果，提高防御成功率，同时保持视觉质量。

Conclusion: 该方法有效应对Deepfake攻击，为保护隐私和社会安全提供了新策略。

Abstract: Deepfake technology, driven by Generative Adversarial Networks (GANs), poses
significant risks to privacy and societal security. Existing detection methods
are predominantly passive, focusing on post-event analysis without preventing
attacks. To address this, we propose an active defense method based on
low-frequency perceptual perturbations to disrupt face swapping manipulation,
reducing the performance and naturalness of generated content. Unlike prior
approaches that used low-frequency perturbations to impact classification
accuracy,our method directly targets the generative process of deepfake
techniques. We combine frequency and spatial domain features to strengthen
defenses. By introducing artifacts through low-frequency perturbations while
preserving high-frequency details, we ensure the output remains visually
plausible. Additionally, we design a complete architecture featuring an
encoder, a perturbation generator, and a decoder, leveraging discrete wavelet
transform (DWT) to extract low-frequency components and generate perturbations
that disrupt facial manipulation models. Experiments on CelebA-HQ and LFW
demonstrate significant reductions in face-swapping effectiveness, improved
defense success rates, and preservation of visual quality.

</details>


### [45] [Embracing Aleatoric Uncertainty: Generating Diverse 3D Human Motion](https://arxiv.org/abs/2508.20604)
*Zheng Qin,Yabing Wang,Minghui Yang,Sanping Zhou,Ming Yang,Le Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Diverse-T2M的新方法，用于从文本生成3D人体动作，着重于提升生成结果的多样性和文本一致性。


<details>
  <summary>Details</summary>
Motivation: 目前从文本生成高质量3D人体动作的技术已经取得了进展，但生成的动作在多样性方面仍有不足，需要一个有效的方法来解决这一挑战。

Method: 通过引入噪声信号作为多样性的信息载体，设计了一种基于Transformer的新方法；此外，通过构建一个将文本投射到连续表示的潜在空间，并加入潜在空间采样器进行随机采样，从而实现多样性和不确定性的显式建模。

Result: 在HumanML3D和KIT-ML基准数据集上的实验结果表明，这种方法显著提高了生成动作的多样性，同时保持了文本一致性方面的先进性能。

Conclusion: Diverse-T2M方法能够在显著提升动作生成多样性的基础上，保持文本与生成动作的语义一致性，提供了一种新颖而有效的解决方案。

Abstract: Generating 3D human motions from text is a challenging yet valuable task. The
key aspects of this task are ensuring text-motion consistency and achieving
generation diversity. Although recent advancements have enabled the generation
of precise and high-quality human motions from text, achieving diversity in the
generated motions remains a significant challenge. In this paper, we aim to
overcome the above challenge by designing a simple yet effective text-to-motion
generation method, \textit{i.e.}, Diverse-T2M. Our method introduces
uncertainty into the generation process, enabling the generation of highly
diverse motions while preserving the semantic consistency of the text.
Specifically, we propose a novel perspective that utilizes noise signals as
carriers of diversity information in transformer-based methods, facilitating a
explicit modeling of uncertainty. Moreover, we construct a latent space where
text is projected into a continuous representation, instead of a rigid
one-to-one mapping, and integrate a latent space sampler to introduce
stochastic sampling into the generation process, thereby enhancing the
diversity and uncertainty of the outputs. Our results on text-to-motion
generation benchmark datasets~(HumanML3D and KIT-ML) demonstrate that our
method significantly enhances diversity while maintaining state-of-the-art
performance in text consistency.

</details>


### [46] [Optimization-Based Calibration for Intravascular Ultrasound Volume Reconstruction](https://arxiv.org/abs/2508.20605)
*Karl-Philippe Beaudet,Sidaty El Hadramy,Philippe C Cattin,Juan Verde,Stéphane Cotin*

Main category: cs.CV

TL;DR: 研究了一种利用3D打印模型优化校准方法，实现3D血管内超声(IVUS)体积重建，并提升术中的图像引导效果。


<details>
  <summary>Details</summary>
Motivation: 为了弥合术前计算机断层扫描(CT)图像与术中超声图像之间的差距，改善复杂肝脏解剖结构的术中导航问题。

Method: 提出一种基于3D打印仿体的优化校准方法，确保追踪的IVUS数据与术前CT图像精确对齐，并实现可靠的3D IVUS体积重建。

Result: 通过猪肝体内实验证实，校准误差范围为0.88至1.80毫米，配准误差范围为3.40至5.71毫米。

Conclusion: 方法为术中超声与术前CT图像的配准提供可靠且精准的方案，能够增强肝脏手术中的术中导航效果。

Abstract: Intraoperative ultrasound images are inherently challenging to interpret in
liver surgery due to the limited field of view and complex anatomical
structures. Bridging the gap between preoperative and intraoperative data is
crucial for effective surgical guidance. 3D IntraVascular UltraSound (IVUS)
offers a potential solution by enabling the reconstruction of the entire organ,
which facilitates registration between preoperative computed tomography (CT)
scans and intraoperative IVUS images. In this work, we propose an
optimization-based calibration method using a 3D-printed phantom for accurate
3D Intravascular Ultrasound volume reconstruction. Our approach ensures precise
alignment of tracked IVUS data with preoperative CT images, improving
intraoperative navigation. We validated our method using in vivo swine liver
images, achieving a calibration error from 0.88 to 1.80 mm and a registration
error from 3.40 to 5.71 mm between the 3D IVUS data and the corresponding CT
scan. Our method provides a reliable and accurate means of calibration and
volume reconstruction. It can be used to register intraoperative ultrasound
images with preoperative CT images in the context of liver surgery, and enhance
intraoperative guidance.

</details>


### [47] [Physics Informed Generative Models for Magnetic Field Images](https://arxiv.org/abs/2508.20612)
*Aye Phyu Phyu Aung,Lucas Lum,Zhansen Shi,Wen Qiu,Bernice Zee,JM Chin,Yeow Kheng Lim,J. Senthilnath*

Main category: cs.CV

TL;DR: 在半导体制造中，检测和定位缺陷至关重要。本文提出了一种基于物理约束的生成模型（PI-GenMFI），通过生成合成的磁场图像（MFI）来改善缺陷定位问题。


<details>
  <summary>Details</summary>
Motivation: X射线检测虽然可靠，但大规模扫描内存和时间成本高。磁场成像相对高效，但数据集的有限性限制了机器学习模型的训练。研究的目的是利用生成模型克服数据集短缺的问题。

Method: 提出了一种物理约束生成模型（PI-GenMFI），利用扩散模型和物理信息生成合成的MFI数据，特别针对常见缺陷类型（如电源短路），为ML算法提供训练数据。

Result: 生成的MFI图像与变分自编码器（VAE）和扩散方法的SOTA模型进行比较，通过图像生成和信号处理相关的多个定性和定量指标，显示出其优越性能。

Conclusion: 通过生成合成MFI数据，有效优化了半导体缺陷检测与定位流程，这种方法可以广泛用于数据不足场景的模型训练。

Abstract: In semiconductor manufacturing, defect detection and localization are
critical to ensuring product quality and yield. While X-ray imaging is a
reliable non-destructive testing method, it is memory-intensive and
time-consuming for large-scale scanning, Magnetic Field Imaging (MFI) offers a
more efficient means to localize regions of interest (ROI) for targeted X-ray
scanning. However, the limited availability of MFI datasets due to proprietary
concerns presents a significant bottleneck for training machine learning (ML)
models using MFI. To address this challenge, we consider an ML-driven approach
leveraging diffusion models with two physical constraints. We propose Physics
Informed Generative Models for Magnetic Field Images (PI-GenMFI) to generate
synthetic MFI samples by integrating specific physical information. We generate
MFI images for the most common defect types: power shorts. These synthetic
images will serve as training data for ML algorithms designed to localize
defect areas efficiently. To evaluate generated MFIs, we compare our model to
SOTA generative models from both variational autoencoder (VAE) and diffusion
methods. We present a domain expert evaluation to assess the generated samples.
In addition, we present qualitative and quantitative evaluation using various
metrics used for image generation and signal processing, showing promising
results to optimize the defect localization process.

</details>


### [48] [Revisiting the Privacy Risks of Split Inference: A GAN-Based Data Reconstruction Attack via Progressive Feature Optimization](https://arxiv.org/abs/2508.20613)
*Yixiang Qiu,Yanhan Liu,Hongyao Yu,Hao Fang,Bin Chen,Shu-Tao Xia,Ke Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种基于GAN的新型数据重建攻击（DRA）框架，通过分层特征优化（PFO）提高重建图像的语义保真度，并在多种场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络（DNNs）的日益复杂，分割推理（SI）逐渐被采用以减小延迟并保护用户隐私。但最近的数据重建攻击（DRA）表明，中间特征传递可能导致隐私泄露，尤其现有攻击方法在高复杂场景下效果有限。

Method: 提出了一种基于GAN的新型DRA框架，通过分层特征优化（PFO）和引入L1球约束稳定优化过程，提高重建图像的语义保真度和真实感。

Result: 通过大量实验表明，该方法在高分辨率场景、分布外数据及深层与复杂神经网络上均能显著优于已有的攻击方法。

Conclusion: 该方法有效解决了现有DRA通用性和重建质量的局限性，为保护隐私免受复杂深度学习模型攻击提供了新的视角。

Abstract: The growing complexity of Deep Neural Networks (DNNs) has led to the adoption
of Split Inference (SI), a collaborative paradigm that partitions computation
between edge devices and the cloud to reduce latency and protect user privacy.
However, recent advances in Data Reconstruction Attacks (DRAs) reveal that
intermediate features exchanged in SI can be exploited to recover sensitive
input data, posing significant privacy risks. Existing DRAs are typically
effective only on shallow models and fail to fully leverage semantic priors,
limiting their reconstruction quality and generalizability across datasets and
model architectures. In this paper, we propose a novel GAN-based DRA framework
with Progressive Feature Optimization (PFO), which decomposes the generator
into hierarchical blocks and incrementally refines intermediate representations
to enhance the semantic fidelity of reconstructed images. To stabilize the
optimization and improve image realism, we introduce an L1-ball constraint
during reconstruction. Extensive experiments show that our method outperforms
prior attacks by a large margin, especially in high-resolution scenarios,
out-of-distribution settings, and against deeper and more complex DNNs.

</details>


### [49] [EmoCAST: Emotional Talking Portrait via Emotive Text Description](https://arxiv.org/abs/2508.20615)
*Yiguo Jiang,Xiaodong Cun,Yong Zhang,Yudian Zheng,Fan Tang,Chi-Man Pun*

Main category: cs.CV

TL;DR: EmoCAST是一种生成情感驱动的口述视频的方法，克服了现有方法在控制、自然性和情感质量上的不足，提供了新的数据集和训练策略。


<details>
  <summary>Details</summary>
Motivation: 现有方法在情感口述视频的生成中表现出控制不灵活、动作不自然和表情质量不佳的问题，同时数据集局限于实验室环境，难以推广到实际场景。

Method: 提出了基于扩散的EmoCAST框架，包括情感驱动的文本解耦模块和情感音频关注模块。此外，构建了一个包含详细情感描述的新数据集，并提出情感感知采样训练和渐进功能训练策略。

Result: EmoCAST框架能够生成具有情感表达、精准口型同步的高质量口述视频，其表现达到了当前的最佳水平。

Conclusion: EmoCAST框架有效提升了情感口述视频的生成质量，为实际应用场景提供了更自然、更精准的解决方案。

Abstract: Emotional talking head synthesis aims to generate talking portrait videos
with vivid expressions. Existing methods still exhibit limitations in control
flexibility, motion naturalness, and expression quality. Moreover, currently
available datasets are primarily collected in lab settings, further
exacerbating these shortcomings. Consequently, these limitations substantially
hinder practical applications in real-world scenarios. To address these
challenges, we propose EmoCAST, a diffusion-based framework with two key
modules for precise text-driven emotional synthesis. In appearance modeling,
emotional prompts are integrated through a text-guided decoupled emotive
module, enhancing the spatial knowledge to improve emotion comprehension. To
improve the relationship between audio and emotion, we introduce an emotive
audio attention module to capture the interplay between controlled emotion and
driving audio, generating emotion-aware features to guide more precise facial
motion synthesis. Additionally, we construct an emotional talking head dataset
with comprehensive emotive text descriptions to optimize the framework's
performance. Based on the proposed dataset, we propose an emotion-aware
sampling training strategy and a progressive functional training strategy that
further improve the model's ability to capture nuanced expressive features and
achieve accurate lip-synchronization. Overall, EmoCAST achieves
state-of-the-art performance in generating realistic, emotionally expressive,
and audio-synchronized talking-head videos. Project Page:
https://github.com/GVCLab/EmoCAST

</details>


### [50] [Mask-Guided Multi-Channel SwinUNETR Framework for Robust MRI Classification](https://arxiv.org/abs/2508.20621)
*Smriti Joshi,Lidia Garrucho,Richard Osuala,Oliver Diaz,Karim Lekadir*

Main category: cs.CV

TL;DR: 乳腺癌早期检测对于改善治疗效果至关重要。本研究提出了一种基于SwinUNETR的深度学习框架，以支持MRI影像的诊断和分类。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是女性癌症相关死亡的主要原因之一，传统乳腺X光对高风险患者或乳腺组织致密的患者诊断效果有限，因此需要开发新技术以提高检测准确性。

Method: 使用一种基于SwinUNETR的深度学习框架，结合乳腺区域遮罩处理、大量数据增强和集成学习，以提高模型的鲁棒性和泛化能力。

Result: 在ODELIA挑战中获得排行榜第二名，展示了其在辅助临床乳腺MRI解读中的潜力。

Conclusion: 该研究展示了基于深度学习的乳腺癌MRI诊断方法的可行性，并公开了其代码以供研究和应用。

Abstract: Breast cancer is one of the leading causes of cancer-related mortality in
women, and early detection is essential for improving outcomes. Magnetic
resonance imaging (MRI) is a highly sensitive tool for breast cancer detection,
particularly in women at high risk or with dense breast tissue, where
mammography is less effective. The ODELIA consortium organized a multi-center
challenge to foster AI-based solutions for breast cancer diagnosis and
classification. The dataset included 511 studies from six European centers,
acquired on scanners from multiple vendors at both 1.5 T and 3 T. Each study
was labeled for the left and right breast as no lesion, benign lesion, or
malignant lesion. We developed a SwinUNETR-based deep learning framework that
incorporates breast region masking, extensive data augmentation, and ensemble
learning to improve robustness and generalizability. Our method achieved second
place on the challenge leaderboard, highlighting its potential to support
clinical breast MRI interpretation. We publicly share our codebase at
https://github.com/smriti-joshi/bcnaim-odelia-challenge.git.

</details>


### [51] [AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View Images](https://arxiv.org/abs/2508.20623)
*Shiqi Xin,Xiaolin Zhang,Yanbin Liu,Peng Zhang,Caifeng Shan*

Main category: cs.CV

TL;DR: 本文提出AvatarBack框架，以提高3D头像后脑勺重建质量，通过引入SSG和ASA技术，解决几何不一致和结构模糊问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于Gaussian Splatting 的方法主要依赖正面视图，后脑勺区域重建质量较差，导致几何不一致和低真实感。

Method: 提出AvatarBack框架，包含SSG生成后脑视角伪图像，ASA通过可学习变换矩阵实现与3D高斯表征的几何对齐。

Result: 实验表明，AvatarBack在后脑区域重建质量和视觉真实感上均显著提升，同时保持前部的高保真度。

Conclusion: AvatarBack能有效改善后脑的几何构造和一致性，使得3D头像在多样动作下视觉真实且保持可动画性。

Abstract: Recent advances in Gaussian Splatting have significantly boosted the
reconstruction of head avatars, enabling high-quality facial modeling by
representing an 3D avatar as a collection of 3D Gaussians. However, existing
methods predominantly rely on frontal-view images, leaving the back-head poorly
constructed. This leads to geometric inconsistencies, structural blurring, and
reduced realism in the rear regions, ultimately limiting the fidelity of
reconstructed avatars. To address this challenge, we propose AvatarBack, a
novel plug-and-play framework specifically designed to reconstruct complete and
consistent 3D Gaussian avatars by explicitly modeling the missing back-head
regions. AvatarBack integrates two core technical innovations,i.e., the
Subject-specific Generator (SSG) and the Adaptive Spatial Alignment Strategy
(ASA). The former leverages a generative prior to synthesize
identity-consistent, plausible back-view pseudo-images from sparse frontal
inputs, providing robust multi-view supervision. To achieve precise geometric
alignment between these synthetic views and the 3D Gaussian representation, the
later employs learnable transformation matrices optimized during training,
effectively resolving inherent pose and coordinate discrepancies. Extensive
experiments on NeRSemble and K-hairstyle datasets, evaluated using geometric,
photometric, and GPT-4o-based perceptual metrics, demonstrate that AvatarBack
significantly enhances back-head reconstruction quality while preserving
frontal fidelity. Moreover, the reconstructed avatars maintain consistent
visual realism under diverse motions and remain fully animatable.

</details>


### [52] [ArtFace: Towards Historical Portrait Face Identification via Model Adaptation](https://arxiv.org/abs/2508.20626)
*Francois Poh,Anjith George,Sébastien Marcel*

Main category: cs.CV

TL;DR: 本研究探索了基础模型在艺术品中的人脸识别潜力，显示比传统方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸识别模型在艺术画作中表现不佳，这是由于领域转变、高类内变化以及艺术风格等因素的限制。

Method: 通过微调基础模型并与传统面部识别网络嵌入结合，提升识别性能。

Result: 相比当前最先进方法，本文方法在艺术画作的人脸识别中有显著提升。

Conclusion: 基础模型可弥补传统方法不足，在艺术绘画情境下提供有效的人脸识别解决方案。

Abstract: Identifying sitters in historical paintings is a key task for art historians,
offering insight into their lives and how they chose to be seen. However, the
process is often subjective and limited by the lack of data and stylistic
variations. Automated facial recognition is capable of handling challenging
conditions and can assist, but while traditional facial recognition models
perform well on photographs, they struggle with paintings due to domain shift
and high intra-class variation. Artistic factors such as style, skill, intent,
and influence from other works further complicate recognition. In this work, we
investigate the potential of foundation models to improve facial recognition in
artworks. By fine-tuning foundation models and integrating their embeddings
with those from conventional facial recognition networks, we demonstrate
notable improvements over current state-of-the-art methods. Our results show
that foundation models can bridge the gap where traditional methods are
ineffective. Paper page at https://www.idiap.ch/paper/artface/

</details>


### [53] [CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models](https://arxiv.org/abs/2508.20640)
*Ayan Banerjee,Fernando Vilariño,Josep Lladós*

Main category: cs.CV

TL;DR: CraftGraffiti是一种生成涂鸦艺术的AI框架，通过身份保真机制和动态姿势定制，实现了极端风格化转换下的面部特征保持。


<details>
  <summary>Details</summary>
Motivation: 解决涂鸦艺术中面部身份特征易被扭曲的挑战，同时兼顾艺术风格表达和身份识别的融合。

Method: 使用LoRA微调的扩散变换模型进行风格转移，通过面部一致性自注意机制保持身份特征，结合CLIP引导的提示扩展实现动态姿势定制，提出“先风格后身份”的生成范式。

Result: CraftGraffiti在面部特征一致性、审美性和人工偏好测试中表现优异，并在实际艺术活动中取得成功。

Conclusion: 该系统为生成艺术中风格自由与身份识别之间找到均衡点，推动身份尊重的AI创作发展。

Abstract: Preserving facial identity under extreme stylistic transformation remains a
major challenge in generative art. In graffiti, a high-contrast, abstract
medium, subtle distortions to the eyes, nose, or mouth can erase the subject's
recognizability, undermining both personal and cultural authenticity. We
present CraftGraffiti, an end-to-end text-guided graffiti generation framework
designed with facial feature preservation as a primary objective. Given an
input image and a style and pose descriptive prompt, CraftGraffiti first
applies graffiti style transfer via LoRA-fine-tuned pretrained diffusion
transformer, then enforces identity fidelity through a face-consistent
self-attention mechanism that augments attention layers with explicit identity
embeddings. Pose customization is achieved without keypoints, using CLIP-guided
prompt extension to enable dynamic re-posing while retaining facial coherence.
We formally justify and empirically validate the "style-first, identity-after"
paradigm, showing it reduces attribute drift compared to the reverse order.
Quantitative results demonstrate competitive facial feature consistency and
state-of-the-art aesthetic and human preference scores, while qualitative
analyses and a live deployment at the Cruilla Festival highlight the system's
real-world creative impact. CraftGraffiti advances the goal of
identity-respectful AI-assisted artistry, offering a principled approach for
blending stylistic freedom with recognizability in creative AI applications.

</details>


### [54] [Improving Alignment in LVLMs with Debiased Self-Judgment](https://arxiv.org/abs/2508.20655)
*Sihan Yang,Chenhang Cui,Zihao Zhao,Yiyang Zhou,Weilong Yan,Ying Wei,Huaxiu Yao*

Main category: cs.CV

TL;DR: 提出了一种新方法，通过生成自评偏差得分，改善大语言模型(LLMs)和大视觉语言模型(LVLMs)的对齐问题，同时减少幻觉现象，提升安全性和整体能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法需依赖外部数据集、人工注释或复杂后处理，导致延展性差、成本高，且视觉与语言模式对齐仍具挑战性。

Method: 提出通过模型内部生成的去偏自评分数(debiased self-judgment score)作为自评指标，不依赖外部资源以增强模型对齐能力。

Result: 实验结果显示该方法显著优于传统方法，有效减少幻觉现象并提升模型安全性和性能。

Conclusion: 自评偏差得分方法为LVLMs对齐问题提供了更高效的解决方案，同时减轻了对外部资源的依赖，展现出潜在应用前景。

Abstract: The rapid advancements in Large Language Models (LLMs) and Large
Visual-Language Models (LVLMs) have opened up new opportunities for integrating
visual and linguistic modalities. However, effectively aligning these
modalities remains challenging, often leading to hallucinations--where
generated outputs are not grounded in the visual input--and raising safety
concerns across various domains. Existing alignment methods, such as
instruction tuning and preference tuning, often rely on external datasets,
human annotations, or complex post-processing, which limit scalability and
increase costs. To address these challenges, we propose a novel approach that
generates the debiased self-judgment score, a self-evaluation metric created
internally by the model without relying on external resources. This enables the
model to autonomously improve alignment. Our method enhances both decoding
strategies and preference tuning processes, resulting in reduced
hallucinations, enhanced safety, and improved overall capability. Empirical
results show that our approach significantly outperforms traditional methods,
offering a more effective solution for aligning LVLMs.

</details>


### [55] ["Humor, Art, or Misinformation?": A Multimodal Dataset for Intent-Aware Synthetic Image Detection](https://arxiv.org/abs/2508.20670)
*Anastasios Skoularikis,Stefanos-Iordanis Papadopoulos,Symeon Papadopoulos,Panagiotis C. Petrantonakis*

Main category: cs.CV

TL;DR: 现有的多模态AI在检测生成内容方面取得进展，但忽略了生成内容背后的意图。S-HArM数据集引入了意图感知分类，并提出了三种提示策略。对多种技术的比较显示，保留视觉上下文的数据集带来的模型表现更好，但整体性能仍有限，这表现出推断意图的复杂性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态AI在检测伪造内容时尚未考虑其背后的生成意图。提取和分析生成意图的能力有助于更好地识别内容的真实性与潜在意图。

Method: 提出S-HArM数据集，包括具有意图标注（幽默/讽刺、艺术、误导信息）的图文对。研究了三种在稳定扩散上生成合成训练数据的策略。同时对多项技术（如模态融合、对比学习等）进行比较分析。

Result: 结果表明，基于图像和多模态数据训练的模型更能推广到真实数据，但总体性能有限，体现了推断意图的复杂性。

Conclusion: 推断意图具有挑战性，需要专门设计的架构来进一步提升相关模型的性能。

Abstract: Recent advances in multimodal AI have enabled progress in detecting synthetic
and out-of-context content. However, existing efforts largely overlook the
intent behind AI-generated images. To fill this gap, we introduce S-HArM, a
multimodal dataset for intent-aware classification, comprising 9,576 "in the
wild" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art,
or Misinformation. Additionally, we explore three prompting strategies
(image-guided, description-guided, and multimodally-guided) to construct a
large-scale synthetic training dataset with Stable Diffusion. We conduct an
extensive comparative study including modality fusion, contrastive learning,
reconstruction networks, attention mechanisms, and large vision-language
models. Our results show that models trained on image- and multimodally-guided
data generalize better to "in the wild" content, due to preserved visual
context. However, overall performance remains limited, highlighting the
complexity of inferring intent and the need for specialized architectures.

</details>


### [56] [MobileCLIP2: Improving Multi-Modal Reinforced Training](https://arxiv.org/abs/2508.20691)
*Fartash Faghri,Pavan Kumar Anasosalu Vasu,Cem Koc,Vaishaal Shankar,Alexander Toshev,Oncel Tuzel,Hadi Pouransari*

Main category: cs.CV

TL;DR: MobileCLIP2通过改进多模态强化训练，显著提高了ImageNet-1k零样本准确率，并具有更低延迟和更小模型体积。


<details>
  <summary>Details</summary>
Motivation: 进一步优化MobileCLIP的多模态强化训练以提升模型性能，特别是优化零样本任务表现。

Method: 通过改进CLIP教师集成和描述生成器，训练出更高质量的数据集；引入温度调节和模型合成描述等创新方法；新训练一组名为MobileCLIP2的模型。

Result: MobileCLIP2在ImageNet-1k零样本任务中表现出色，部分模型在体积更小和延迟更低的情况下，达到甚至超越更大模型的准确率。

Conclusion: MobileCLIP2以其优化的训练方法和更优的性能，成为轻量化多模态模型在零样本任务上的新标杆。

Abstract: Foundation image-text models such as CLIP with zero-shot capabilities enable
a wide array of applications. MobileCLIP is a recent family of image-text
models at 3-15ms latency and 50-150M parameters with state-of-the-art zero-shot
accuracy. The main ingredients in MobileCLIP were its low-latency and light
architectures and a novel multi-modal reinforced training that made knowledge
distillation from multiple caption-generators and CLIP teachers efficient,
scalable, and reproducible. In this paper, we improve the multi-modal
reinforced training of MobileCLIP through: 1) better CLIP teacher ensembles
trained on the DFN dataset, 2) improved captioner teachers trained on the DFN
dataset and fine-tuned on a diverse selection of high-quality image-caption
datasets. We discover new insights through ablations such as the importance of
temperature tuning in contrastive knowledge distillation, the effectiveness of
caption-generator fine-tuning for caption diversity, and the additive
improvement from combining synthetic captions generated by multiple models. We
train a new family of models called MobileCLIP2 and achieve state-of-the-art
ImageNet-1k zero-shot accuracies at low latencies. In particular, we observe
2.2% improvement in ImageNet-1k accuracy for MobileCLIP2-B compared with
MobileCLIP-B architecture. Notably, MobileCLIP2-S4 matches the zero-shot
accuracy of SigLIP-SO400M/14 on ImageNet-1k while being 2$\times$ smaller and
improves on DFN ViT-L/14 at 2.5$\times$ lower latency. We release our
pretrained models (https://github.com/apple/ml-mobileclip) and the data
generation code (https://github.com/apple/ml-mobileclip-dr). The data
generation code makes it easy to create new reinforced datasets with arbitrary
teachers using distributed scalable processing.

</details>


### [57] [Learned Rate Control for Frame-Level Adaptive Neural Video Compression via Dynamic Neural Network](https://arxiv.org/abs/2508.20709)
*Chenhao Zhang,Wei Gao*

Main category: cs.CV

TL;DR: 本文提出一种动态视频压缩框架，通过动态编码通路实现可变比特率控制，并通过联合通路优化策略提升性能，显著改善了现有方法的RD性能，同时维持了较低的比特率误差。


<details>
  <summary>Details</summary>
Motivation: 现有神经视频压缩方法在精确比特率控制方面存在挑战，需要解决学习型编解码器的内在限制问题。

Method: 提出动态路由自编码器(DRA)以实现可变比特率控制，并设计速率控制代理动态调整DRA的编码路，联合通路优化策略则用于协作训练多个通路。

Result: 在HEVC和UVG数据集上实验表明，相较于最先进方法，该方法可实现平均BD-Rate减少14.8%、BD-PSNR提升0.47dB，并保持平均比特率误差1.66%。

Conclusion: 提出的动态视频压缩方法在多种比特率和受限比特率应用中实现了速率-失真-复杂度优化，性能优于现有技术。

Abstract: Neural Video Compression (NVC) has achieved remarkable performance in recent
years. However, precise rate control remains a challenge due to the inherent
limitations of learning-based codecs. To solve this issue, we propose a dynamic
video compression framework designed for variable bitrate scenarios. First, to
achieve variable bitrate implementation, we propose the Dynamic-Route
Autoencoder with variable coding routes, each occupying partial computational
complexity of the whole network and navigating to a distinct RD trade-off.
Second, to approach the target bitrate, the Rate Control Agent estimates the
bitrate of each route and adjusts the coding route of DRA at run time. To
encompass a broad spectrum of variable bitrates while preserving overall RD
performance, we employ the Joint-Routes Optimization strategy, achieving
collaborative training of various routes. Extensive experiments on the HEVC and
UVG datasets show that the proposed method achieves an average BD-Rate
reduction of 14.8% and BD-PSNR gain of 0.47dB over state-of-the-art methods
while maintaining an average bitrate error of 1.66%, achieving
Rate-Distortion-Complexity Optimization (RDCO) for various bitrate and
bitrate-constrained applications. Our code is available at
https://git.openi.org.cn/OpenAICoding/DynamicDVC.

</details>


### [58] [CardioMorphNet: Cardiac Motion Prediction Using a Shape-Guided Bayesian Recurrent Deep Network](https://arxiv.org/abs/2508.20734)
*Reza Akbari Movahed,Abuzar Rezaee,Arezoo Zakeri,Colin Berry,Edmond S. L. Ho,Ali Gooya*

Main category: cs.CV

TL;DR: 提出了一种名为CardioMorphNet的深度学习框架，用于通过短轴心脏磁共振影像进行三维心脏形状引导的可变形配准，实现精准的心脏运动估计。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉心脏运动时表现不足，因为它们基于强度图像配准的相似性损失，可能忽视心脏的解剖区域。

Method: CardioMorphNet结合了递归变分自动编码器和两个后验模型，重点在于通过递归注册分割图来指导框架，而非依赖传统强度图像配准。利用Bayesian建模，还能计算运动场的不确定性图。

Result: 在UK Biobank数据集上的验证表明，CardioMorphNet在心脏运动估计方面性能优于现有方法，同时在心脏区域产生了较低的不确定性值，信心更高。

Conclusion: CardioMorphNet通过Bayesian深度学习改进了传统方法，对心脏运动估计具有更高准确性和可靠性，并能提供预测的不确定性评估。

Abstract: Accurate cardiac motion estimation from cine cardiac magnetic resonance (CMR)
images is vital for assessing cardiac function and detecting its abnormalities.
Existing methods often struggle to capture heart motion accurately because they
rely on intensity-based image registration similarity losses that may overlook
cardiac anatomical regions. To address this, we propose CardioMorphNet, a
recurrent Bayesian deep learning framework for 3D cardiac shape-guided
deformable registration using short-axis (SAX) CMR images. It employs a
recurrent variational autoencoder to model spatio-temporal dependencies over
the cardiac cycle and two posterior models for bi-ventricular segmentation and
motion estimation. The derived loss function from the Bayesian formulation
guides the framework to focus on anatomical regions by recursively registering
segmentation maps without using intensity-based image registration similarity
loss, while leveraging sequential SAX volumes and spatio-temporal features. The
Bayesian modelling also enables computation of uncertainty maps for the
estimated motion fields. Validated on the UK Biobank dataset by comparing
warped mask shapes with ground truth masks, CardioMorphNet demonstrates
superior performance in cardiac motion estimation, outperforming
state-of-the-art methods. Uncertainty assessment shows that it also yields
lower uncertainty values for estimated motion fields in the cardiac region
compared with other probabilistic-based cardiac registration methods,
indicating higher confidence in its predictions.

</details>


### [59] [Mix, Align, Distil: Reliable Cross-Domain Atypical Mitosis Classification](https://arxiv.org/abs/2508.20745)
*Kaustubh Atey,Sameer Anand Jha,Gouranga Bala,Amit Sethi*

Main category: cs.CV

TL;DR: 该论文提出了一种简单的训练方法，用于在不同领域环境下实现对非典型有丝分裂图的鲁棒分类。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在扫描仪、染色及获取方式变化引发的域转移下准确判别非典型有丝分裂图。

Method: 所提方法通过（i）在网络早中期阶段加入样式扰动提升特征多样性，（ii）利用弱域标签通过辅助对齐损失对特征进行跨域对齐，（iii）通过指数移动均值（EMA）教师稳定预测。

Result: 方法在MIDOG 2025任务中实现了0.8762的平衡准确率，敏感性0.8873、特异性0.8651及ROC AUC 0.9499的表现。

Conclusion: 该方法推理开销低，仅需粗略的域元数据，效果强劲且平衡，在MIDOG 2025挑战中具有极强竞争力。

Abstract: Atypical mitotic figures (AMFs) are important histopathological markers yet
remain challenging to identify consistently, particularly under domain shift
stemming from scanner, stain, and acquisition differences. We present a simple
training-time recipe for domain-robust AMF classification in MIDOG 2025 Task 2.
The approach (i) increases feature diversity via style perturbations inserted
at early and mid backbone stages, (ii) aligns attention-refined features across
sites using weak domain labels (Scanner, Origin, Species, Tumor) through an
auxiliary alignment loss, and (iii) stabilizes predictions by distilling from
an exponential moving average (EMA) teacher with temperature-scaled KL
divergence. On the organizer-run preliminary leaderboard for atypical mitosis
classification, our submission attains balanced accuracy of 0.8762, sensitivity
of 0.8873, specificity of 0.8651, and ROC AUC of 0.9499. The method incurs
negligible inference-time overhead, relies only on coarse domain metadata, and
delivers strong, balanced performance, positioning it as a competitive
submission for the MIDOG 2025 challenge.

</details>


### [60] [Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning](https://arxiv.org/abs/2508.20751)
*Yibin Wang,Zhimin Li,Yuhang Zang,Yujie Zhou,Jiazi Bu,Chunyu Wang,Qinglin Lu,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 该研究提出Pref-GRPO算法，通过转变GPT目标从得分最大化到偏好选择，解决了图文生成中的奖励黑洞问题，同时设计了更全面的T2I评估基准UniGenBench。


<details>
  <summary>Details</summary>
Motivation: 当前基于点式奖励模型的T2I强化学习方法因奖励黑洞问题而导致生成图像质量不稳定。研究意图开发新方法以提升稳定性和模型评估全面性。

Method: 提出Pref-GRPO方法，采用成对偏好比较代替单一得分优化，并设计了新的统一T2I评估基准UniGenBench，以全面衡量模型性能。

Result: 实验证明，Pref-GRPO通过更准确区分图像质量差异，提升生成稳定性，同时UniGenBench能够暴露现有T2I模型的优劣并验证方法有效性。

Conclusion: Pref-GRPO方法及UniGenBench工具有效改善了图文生成模型的训练稳定性和性能评估能力，对未来T2I研究具有重要的实践和理论指导价值。

Abstract: Recent advancements highlight the importance of GRPO-based reinforcement
learning methods and benchmarking in enhancing text-to-image (T2I) generation.
However, current methods using pointwise reward models (RM) for scoring
generated images are susceptible to reward hacking. We reveal that this happens
when minimal score differences between images are amplified after
normalization, creating illusory advantages that drive the model to
over-optimize for trivial gains, ultimately destabilizing the image generation
process. To address this, we propose Pref-GRPO, a pairwise preference
reward-based GRPO method that shifts the optimization objective from score
maximization to preference fitting, ensuring more stable training. In
Pref-GRPO, images are pairwise compared within each group using preference RM,
and the win rate is used as the reward signal. Extensive experiments
demonstrate that PREF-GRPO differentiates subtle image quality differences,
providing more stable advantages and mitigating reward hacking. Additionally,
existing T2I benchmarks are limited by coarse evaluation criteria, hindering
comprehensive model assessment. To solve this, we introduce UniGenBench, a
unified T2I benchmark comprising 600 prompts across 5 main themes and 20
subthemes. It evaluates semantic consistency through 10 primary and 27
sub-criteria, leveraging MLLM for benchmark construction and evaluation. Our
benchmarks uncover the strengths and weaknesses of both open and closed-source
T2I models and validate the effectiveness of Pref-GRPO.

</details>


### [61] [${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale Feature for Generalizable Gaussian Splatting](https://arxiv.org/abs/2508.20754)
*Yuxi Hu,Jun Zhang,Kuangyi Chen,Zhe Zhang,Friedrich Fraundorfer*

Main category: cs.CV

TL;DR: 提出了一种名为C3-GS的新框架，在无需逐场景优化的情况下实现对未见场景的视图合成，性能达到当前最佳水平。


<details>
  <summary>Details</summary>
Motivation: 现有方法在预测高斯参数时缺乏能处理稀疏视图的多视角一致性特征编码，从而影响几何精度。

Method: 通过引入上下文感知、跨维度和跨尺度约束的轻量级模块来增强特征学习，并将其整合到统一的渲染管线中，无需额外监督即可实现高质量合成。

Result: 实验表明，C3-GS框架在基准数据集上表现出色，达到最先进的渲染质量和泛化能力。

Conclusion: C3-GS框架可在稀疏输入视图下实现高质量的逼真视图合成，并具有较强的通用性，是领域内的最新进展。

Abstract: Generalizable Gaussian Splatting aims to synthesize novel views for unseen
scenes without per-scene optimization. In particular, recent advancements
utilize feed-forward networks to predict per-pixel Gaussian parameters,
enabling high-quality synthesis from sparse input views. However, existing
approaches fall short in encoding discriminative, multi-view consistent
features for Gaussian predictions, which struggle to construct accurate
geometry with sparse views. To address this, we propose $\mathbf{C}^{3}$-GS, a
framework that enhances feature learning by incorporating context-aware,
cross-dimension, and cross-scale constraints. Our architecture integrates three
lightweight modules into a unified rendering pipeline, improving feature fusion
and enabling photorealistic synthesis without requiring additional supervision.
Extensive experiments on benchmark datasets validate that $\mathbf{C}^{3}$-GS
achieves state-of-the-art rendering quality and generalization ability. Code is
available at: https://github.com/YuhsiHu/C3-GS.

</details>


### [62] [SeqVLM: Proposal-Guided Multi-View Sequences Reasoning via VLM for Zero-Shot 3D Visual Grounding](https://arxiv.org/abs/2508.20758)
*Jiawen Lin,Shiran Bian,Yihang Zhu,Wenbin Tan,Yachao Zhang,Yuan Xie,Yanyun Qu*

Main category: cs.CV

TL;DR: 提出SeqVLM框架，通过多视图图像和空间信息进行目标物体推理，在ScanRefer和Nr3D基准上表现超过现有零样本方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决3DVG中的空间推理有限和上下文丢失等问题，引入了一个无需特定场景训练的框架。

Method: 利用3D语义分割网络生成候选实例，通过多视图投影保留空间关系与上下文细节，并动态调度机制进行交叉模态推理。

Result: 在ScanRefer和Nr3D基准上，SeqVLM取得了55.6%和53.2%的Acc@0.25分数，分别较之前升高4.0%和5.2%。

Conclusion: SeqVLM框架在零样本3D视觉定位中展示了更好的泛化性与现实应用潜力。

Abstract: 3D Visual Grounding (3DVG) aims to localize objects in 3D scenes using
natural language descriptions. Although supervised methods achieve higher
accuracy in constrained settings, zero-shot 3DVG holds greater promise for
real-world applications since eliminating scene-specific training requirements.
However, existing zero-shot methods face challenges of spatial-limited
reasoning due to reliance on single-view localization, and contextual omissions
or detail degradation. To address these issues, we propose SeqVLM, a novel
zero-shot 3DVG framework that leverages multi-view real-world scene images with
spatial information for target object reasoning. Specifically, SeqVLM first
generates 3D instance proposals via a 3D semantic segmentation network and
refines them through semantic filtering, retaining only semantic-relevant
candidates. A proposal-guided multi-view projection strategy then projects
these candidate proposals onto real scene image sequences, preserving spatial
relationships and contextual details in the conversion process of 3D point
cloud to images. Furthermore, to mitigate VLM computational overload, we
implement a dynamic scheduling mechanism that iteratively processes
sequances-query prompts, leveraging VLM's cross-modal reasoning capabilities to
identify textually specified objects. Experiments on the ScanRefer and Nr3D
benchmarks demonstrate state-of-the-art performance, achieving Acc@0.25 scores
of 55.6% and 53.2%, surpassing previous zero-shot methods by 4.0% and 5.2%,
respectively, which advance 3DVG toward greater generalization and real-world
applicability. The code is available at https://github.com/JiawLin/SeqVLM.

</details>


### [63] [ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering](https://arxiv.org/abs/2508.21010)
*Paritosh Parmar,Eric Peh,Basura Fernando*

Main category: cs.CV

TL;DR: 本文引入了一个基于因果链的模块化框架，提升视频问答中的解释性、可信度和普适性。


<details>
  <summary>Details</summary>
Motivation: 目前的视频问答模型在高阶推理上表现不足，主要依赖于不透明的黑盒方法，缺乏解释性，并依赖简单的启发式策略。

Method: 提出了一个两阶段体系结构，包括因果链提取模块（CCE）和基于因果链的回答模块（CCDA），并引入了一种基于大规模语言模型的高质量因果链生成方法及新的评估指标CauCo。

Result: 实验表明，该方法在多个基准数据集上的表现优于现有模型，同时在可解释性、用户信任和泛化性方面取得显著提高。

Conclusion: 模块化框架有效解耦因果推理和回答生成，并将CCE拓展为跨领域的通用因果推理引擎。

Abstract: Existing Causal-Why Video Question Answering (VideoQA) models often struggle
with higher-order reasoning, relying on opaque, monolithic pipelines that
entangle video understanding, causal inference, and answer generation. These
black-box approaches offer limited interpretability and tend to depend on
shallow heuristics. We propose a novel, modular framework that explicitly
decouples causal reasoning from answer generation, introducing natural language
causal chains as interpretable intermediate representations. Inspired by human
cognitive models, these structured cause-effect sequences bridge low-level
video content with high-level causal reasoning, enabling transparent and
logically coherent inference. Our two-stage architecture comprises a Causal
Chain Extractor (CCE) that generates causal chains from video-question pairs,
and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in
these chains. To address the lack of annotated reasoning traces, we introduce a
scalable method for generating high-quality causal chains from existing
datasets using large language models. We also propose CauCo, a new evaluation
metric for causality-oriented captioning. Experiments on three large-scale
benchmarks demonstrate that our approach not only outperforms state-of-the-art
models, but also yields substantial gains in explainability, user trust, and
generalization -- positioning the CCE as a reusable causal reasoning engine
across diverse domains. Project page:
https://paritoshparmar.github.io/chainreaction/

</details>


### [64] [Occlusion Robustness of CLIP for Military Vehicle Classification](https://arxiv.org/abs/2508.20760)
*Jan Erik van Woerden,Gertjan Burghouts,Lotte Nijskens,Alma M. Liezenga,Sabina van Rooij,Frank Ruis,Hugo J. Kuijf*

Main category: cs.CV

TL;DR: 研究CLIP在军事环境中应对遮挡的能力，发现改进后的模型在高遮挡情况下表现更好，但仍需关注补丁级别敏感性及架构鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉语言模型（如CLIP）在军事环境中应对数据稀缺和恶劣条件（如遮挡和信噪比降低）的鲁棒性。

Method: 使用包含18类军事车辆的定制数据集，通过Normalized Area Under the Curve (NAUC)指标评估CLIP模型在不同遮挡比例下的性能，并分析其对模型鲁棒性的影响。

Result: （1）基于Transformer的CLIP模型优于基于CNN的模型；（2）细粒度、分散的遮挡较大的连续遮挡对性能影响更大；（3）线性探针模型在约35%遮挡处性能急剧下降；（4）对模型主干微调后，性能下降点延迟到超过60%遮挡。

Conclusion: 研究表明，训练时需引入遮挡特定的数据增强，并进一步研究补丁级别敏感性及架构的鲁棒性，以改进CLIP在实际军事场景中的表现。

Abstract: Vision-language models (VLMs) like CLIP enable zero-shot classification by
aligning images and text in a shared embedding space, offering advantages for
defense applications with scarce labeled data. However, CLIP's robustness in
challenging military environments, with partial occlusion and degraded
signal-to-noise ratio (SNR), remains underexplored. We investigate CLIP
variants' robustness to occlusion using a custom dataset of 18 military vehicle
classes and evaluate using Normalized Area Under the Curve (NAUC) across
occlusion percentages. Four key insights emerge: (1) Transformer-based CLIP
models consistently outperform CNNs, (2) fine-grained, dispersed occlusions
degrade performance more than larger contiguous occlusions, (3) despite
improved accuracy, performance of linear-probed models sharply drops at around
35% occlusion, (4) by finetuning the model's backbone, this performance drop
occurs at more than 60% occlusion. These results underscore the importance of
occlusion-specific augmentations during training and the need for further
exploration into patch-level sensitivity and architectural resilience for
real-world deployment of CLIP.

</details>


### [65] [SKGE-SWIN: End-To-End Autonomous Vehicle Waypoint Prediction and Navigation Using Skip Stage Swin Transformer](https://arxiv.org/abs/2508.20762)
*Fachri Najm Noer Kartiman,Rasim,Yaya Wihardi,Nurul Hasanah,Oskar Natan,Bambang Wahono,Taufik Ibnu Salim*

Main category: cs.CV

TL;DR: 研究提出了SKGE-Swin架构，通过跳跃阶段机制和Swin Transformer提高自动驾驶模型的表现及理解能力，在CARLA平台的实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 开发具有像素到像素上下文感知能力的端到端自动驾驶模型，提升自动驾驶技术的性能和适应性。

Method: 提出SKGE-Swin架构，采用Swin Transformer的SW-MSA机制及跳跃阶段设计，实现不同网络水平的全局特征表示的增强，并结合CARLA平台的对抗场景进行评估。

Result: 实验结果表明，SKGE-Swin在驾驶评分方面优于现有方法，并通过消融研究验证了架构组件对性能提升的重要性。

Conclusion: 该研究证明SKGE-Swin在复杂场景下的优越性，为提升自动驾驶模型性能提供了新思路。

Abstract: Focusing on the development of an end-to-end autonomous vehicle model with
pixel-to-pixel context awareness, this research proposes the SKGE-Swin
architecture. This architecture utilizes the Swin Transformer with a skip-stage
mechanism to broaden feature representation globally and at various network
levels. This approach enables the model to extract information from distant
pixels by leveraging the Swin Transformer's Shifted Window-based Multi-head
Self-Attention (SW-MSA) mechanism and to retain critical information from the
initial to the final stages of feature extraction, thereby enhancing its
capability to comprehend complex patterns in the vehicle's surroundings. The
model is evaluated on the CARLA platform using adversarial scenarios to
simulate real-world conditions. Experimental results demonstrate that the
SKGE-Swin architecture achieves a superior Driving Score compared to previous
methods. Furthermore, an ablation study will be conducted to evaluate the
contribution of each architectural component, including the influence of skip
connections and the use of the Swin Transformer, in improving model
performance.

</details>


### [66] [Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for Video Understanding](https://arxiv.org/abs/2508.20765)
*Gowreesh Mago,Pascal Mettes,Stevan Rudinac*

Main category: cs.CV

TL;DR: 本文讨论了视频内容理解中的抽象概念识别问题，提出基础模型是解决该问题的理想工具，并综述了相关任务和数据集。


<details>
  <summary>Details</summary>
Motivation: 自动视频理解已取得进展，但抽象概念识别仍是挑战。理解抽象概念有助于模型更符合人类的思维和价值。

Method: 通过回顾相关任务与数据集，总结多年研究经验，用于在多模态基础模型的背景下解决抽象概念的理解问题。

Result: 揭示了研究社区在视频理解抽象概念方面的尝试，表明基础模型适合解决该问题。

Conclusion: 利用以往研究经验，结合多模态基础模型，可推动抽象概念视频理解的进步。

Abstract: The automatic understanding of video content is advancing rapidly. Empowered
by deeper neural networks and large datasets, machines are increasingly capable
of understanding what is concretely visible in video frames, whether it be
objects, actions, events, or scenes. In comparison, humans retain a unique
ability to also look beyond concrete entities and recognize abstract concepts
like justice, freedom, and togetherness. Abstract concept recognition forms a
crucial open challenge in video understanding, where reasoning on multiple
semantic levels based on contextual information is key. In this paper, we argue
that the recent advances in foundation models make for an ideal setting to
address abstract concept understanding in videos. Automated understanding of
high-level abstract concepts is imperative as it enables models to be more
aligned with human reasoning and values. In this survey, we study different
tasks and datasets used to understand abstract concepts in video content. We
observe that, periodically and over a long period, researchers have attempted
to solve these tasks, making the best use of the tools available at their
disposal. We advocate that drawing on decades of community experience will help
us shed light on this important open grand challenge and avoid ``re-inventing
the wheel'' as we start revisiting it in the era of multi-modal foundation
models.

</details>


### [67] [Safer Skin Lesion Classification with Global Class Activation Probability Map Evaluation and SafeML](https://arxiv.org/abs/2508.20776)
*Kuniko Paxton,Koorosh Aslansefat,Amila Akagić,Dhavalkumar Thakker,Yiannis Papadopoulos*

Main category: cs.CV

TL;DR: 提出了一种名为Global Class Activation Probabilistic Map Evaluation的方法，通过概率和像素级分析类的激活图，改进皮肤病变分类模型的解释性和诊断可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管皮肤病变分类模型已具高精确度，但医生和患者对AI诊断的信任不足，而现有的可解释性方法存在可靠性问题。这驱使研究者提出新的改进方法。

Method: 提出Global Class Activation Probabilistic Map Evaluation方法，结合概率统计和像素级分析，对所有类别的激活概率图进行统一分析和可视化。此外，集成SafeML技术以检测错误诊断并提供警示。

Result: 通过在ISIC数据集上结合MobileNetV2和Vision Transformers评估，验证了方法的有效性和潜在实际应用价值。

Conclusion: 该方法通过改善诊断过程的可解释性与安全性，提高了AI在医学诊断中的可靠性和医患信任。

Abstract: Recent advancements in skin lesion classification models have significantly
improved accuracy, with some models even surpassing dermatologists' diagnostic
performance. However, in medical practice, distrust in AI models remains a
challenge. Beyond high accuracy, trustworthy, explainable diagnoses are
essential. Existing explainability methods have reliability issues, with
LIME-based methods suffering from inconsistency, while CAM-based methods
failing to consider all classes. To address these limitations, we propose
Global Class Activation Probabilistic Map Evaluation, a method that analyses
all classes' activation probability maps probabilistically and at a pixel
level. By visualizing the diagnostic process in a unified manner, it helps
reduce the risk of misdiagnosis. Furthermore, the application of SafeML
enhances the detection of false diagnoses and issues warnings to doctors and
patients as needed, improving diagnostic reliability and ultimately patient
safety. We evaluated our method using the ISIC datasets with MobileNetV2 and
Vision Transformers.

</details>


### [68] [Evaluating Compositional Generalisation in VLMs and Diffusion Models](https://arxiv.org/abs/2508.20783)
*Beth Pearson,Bilal Boulbarss,Michael Wray,Martha Lewis*

Main category: cs.CV

TL;DR: 本研究比较了生成型扩散分类器与判别型模型在组合语义推理能力上的表现，发现扩散分类器在概念绑定任务上表现良好，但在关系推理任务上仍有显著挑战。


<details>
  <summary>Details</summary>
Motivation: 探索生成型扩散分类器是否在组合语义能力上优于现有判别型模型。

Method: 评估三种模型（扩散分类器、CLIP、ViLT）在属性与关系绑定任务中的表现，同时考察其在零样本学习（ZSL）和广义零样本学习（GZSL）中的能力。并通过分析CLIP嵌入对问题原因进行探讨。

Result: 扩散分类器和ViLT在绑定概念任务中表现较好，但所有模型在关系性的广义零样本学习任务中表现差。CLIP在表示一些关系概念如‘左’和‘右’时，嵌入表现出过于相似的问题。

Conclusion: 尽管扩散分类器在某些组合任务中有所改善，但当前视觉语言模型在关系推理上的挑战依然显著，需进一步探索优化方向。

Abstract: A fundamental aspect of the semantics of natural language is that novel
meanings can be formed from the composition of previously known parts.
Vision-language models (VLMs) have made significant progress in recent years,
however, there is evidence that they are unable to perform this kind of
composition. For example, given an image of a red cube and a blue cylinder, a
VLM such as CLIP is likely to incorrectly label the image as a red cylinder or
a blue cube, indicating it represents the image as a `bag-of-words' and fails
to capture compositional semantics. Diffusion models have recently gained
significant attention for their impressive generative abilities, and zero-shot
classifiers based on diffusion models have been shown to perform competitively
with CLIP in certain compositional tasks. In this work we explore whether the
generative Diffusion Classifier has improved compositional generalisation
abilities compared to discriminative models. We assess three models --
Diffusion Classifier, CLIP, and ViLT -- on their ability to bind objects with
attributes and relations in both zero-shot learning (ZSL) and generalised
zero-shot learning (GZSL) settings. Our results show that the Diffusion
Classifier and ViLT perform well at concept binding tasks, but that all models
struggle significantly with the relational GZSL task, underscoring the broader
challenges VLMs face with relational reasoning. Analysis of CLIP embeddings
suggests that the difficulty may stem from overly similar representations of
relational concepts such as left and right. Code and dataset are available at:
https://github.com/otmive/diffusion_classifier_clip

</details>


### [69] [Surfel-based 3D Registration with Equivariant SE(3) Features](https://arxiv.org/abs/2508.20789)
*Xueyang Kang,Hang Zhao,Kourosh Khoshelham,Patrick Vandewalle*

Main category: cs.CV

TL;DR: 提出了一种新颖的基于Surfels的姿态学习回归方法，用于改进点云配准的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了点的方向和不确定性，容易受到噪声输入和激进旋转的影响，导致需要大量的变换增强训练数据。

Method: 使用基于SE(3)等变卷积的Surfels姿态学习方法，包括等变卷积编码器、交叉注意力机制、全连接解码器以及非线性Huber损失。

Result: 实验结果表明该模型在室内外数据集上表现优越，相较于现有方法具有更强的鲁棒性。

Conclusion: 此方法可以提升点云配准的一致性和噪声抗性，对真实数据点云扫描的较优表现表明其实用性很高。

Abstract: Point cloud registration is crucial for ensuring 3D alignment consistency of
multiple local point clouds in 3D reconstruction for remote sensing or digital
heritage. While various point cloud-based registration methods exist, both
non-learning and learning-based, they ignore point orientations and point
uncertainties, making the model susceptible to noisy input and aggressive
rotations of the input point cloud like orthogonal transformation; thus, it
necessitates extensive training point clouds with transformation augmentations.
To address these issues, we propose a novel surfel-based pose learning
regression approach. Our method can initialize surfels from Lidar point cloud
using virtual perspective camera parameters, and learns explicit
$\mathbf{SE(3)}$ equivariant features, including both position and rotation
through $\mathbf{SE(3)}$ equivariant convolutional kernels to predict relative
transformation between source and target scans. The model comprises an
equivariant convolutional encoder, a cross-attention mechanism for similarity
computation, a fully-connected decoder, and a non-linear Huber loss.
Experimental results on indoor and outdoor datasets demonstrate our model
superiority and robust performance on real point-cloud scans compared to
state-of-the-art methods.

</details>


### [70] [Adapting Foundation Model for Dental Caries Detection with Dual-View Co-Training](https://arxiv.org/abs/2508.20813)
*Tao Luo,Han Wu,Tong Yang,Dinggang Shen,Zhiming Cui*

Main category: cs.CV

TL;DR: 提出了一种新的双视图协同训练网络（DVCTNet），结合全景X光和局部牙齿图像信息，提升牙齿龋齿检测的准确度。


<details>
  <summary>Details</summary>
Motivation: 目前的龋齿检测方法因其细微的对比变化和多样的病灶形态，精度较低，因此需要一种更准确、高效的方法。

Method: 本研究提出DVCTNet，通过自动牙齿检测生成全景及局部两种视图，分别在两种视图上独立预训练视觉基础模型，然后利用门控跨视图注意力模块融合两种视图特征，从而优化检测流程。

Result: DVCTNet在公开数据集和新建高精度数据集上的实验结果优于现有先进方法，展示出卓越性能和临床应用潜力。

Conclusion: DVCTNet通过结合全景和局部信息显著提升龋齿检测效果，并为未来研究与实际应用提供了可行的技术路径。

Abstract: Accurate dental caries detection from panoramic X-rays plays a pivotal role
in preventing lesion progression. However, current detection methods often
yield suboptimal accuracy due to subtle contrast variations and diverse lesion
morphology of dental caries. In this work, inspired by the clinical workflow
where dentists systematically combine whole-image screening with detailed
tooth-level inspection, we present DVCTNet, a novel Dual-View Co-Training
network for accurate dental caries detection. Our DVCTNet starts with employing
automated tooth detection to establish two complementary views: a global view
from panoramic X-ray images and a local view from cropped tooth images. We then
pretrain two vision foundation models separately on the two views. The
global-view foundation model serves as the detection backbone, generating
region proposals and global features, while the local-view model extracts
detailed features from corresponding cropped tooth patches matched by the
region proposals. To effectively integrate information from both views, we
introduce a Gated Cross-View Attention (GCV-Atten) module that dynamically
fuses dual-view features, enhancing the detection pipeline by integrating the
fused features back into the detection model for final caries detection. To
rigorously evaluate our DVCTNet, we test it on a public dataset and further
validate its performance on a newly curated, high-precision dental caries
detection dataset, annotated using both intra-oral images and panoramic X-rays
for double verification. Experimental results demonstrate DVCTNet's superior
performance against existing state-of-the-art (SOTA) methods on both datasets,
indicating the clinical applicability of our method. Our code and labeled
dataset are available at https://github.com/ShanghaiTech-IMPACT/DVCTNet.

</details>


### [71] [FusionCounting: Robust visible-infrared image fusion guided by crowd counting via multi-task learning](https://arxiv.org/abs/2508.20817)
*He Li,Xinyu Liu,Weihang Kong,Xingchen Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为FusionCounting的多任务学习框架，将人群计数与可见光-红外图像融合（VIF）整合，为高密场景提供新的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有VIF方法主要关注提升融合图像质量，而加入下游任务（如语义分割、物体检测）虽能赋予语义指导，但注释成本高，特别是在人群密集场景中困难重重。此外，将人群计数与VIF融合的研究仍属空白。作者希望通过融合VIF与人群计数改进现有不足。

Method: 提出了一种名为FusionCounting的框架，通过结合人群计数提供低注释成本的密集场景定量分析能力。此外，框架采用动态损失加权策略加速收敛，并通过对抗训练提升系统鲁棒性与抗攻击能力。

Result: 实验表明，FusionCounting在提升图像融合质量的同时，也在公共数据集中的人群计数任务中表现优越。

Conclusion: 将人群计数融入VIF是一种高效可行的多任务学习方法，对密集场景任务提供了改进方案，并表现出良好的稳健性和性能。

Abstract: Most visible and infrared image fusion (VIF) methods focus primarily on
optimizing fused image quality. Recent studies have begun incorporating
downstream tasks, such as semantic segmentation and object detection, to
provide semantic guidance for VIF. However, semantic segmentation requires
extensive annotations, while object detection, despite reducing annotation
efforts compared with segmentation, faces challenges in highly crowded scenes
due to overlapping bounding boxes and occlusion. Moreover, although RGB-T crowd
counting has gained increasing attention in recent years, no studies have
integrated VIF and crowd counting into a unified framework. To address these
challenges, we propose FusionCounting, a novel multi-task learning framework
that integrates crowd counting into the VIF process. Crowd counting provides a
direct quantitative measure of population density with minimal annotation,
making it particularly suitable for dense scenes. Our framework leverages both
input images and population density information in a mutually beneficial
multi-task design. To accelerate convergence and balance tasks contributions,
we introduce a dynamic loss function weighting strategy. Furthermore, we
incorporate adversarial training to enhance the robustness of both VIF and
crowd counting, improving the model's stability and resilience to adversarial
attacks. Experimental results on public datasets demonstrate that
FusionCounting not only enhances image fusion quality but also achieves
superior crowd counting performance.

</details>


### [72] [Estimating 2D Keypoints of Surgical Tools Using Vision-Language Models with Low-Rank Adaptation](https://arxiv.org/abs/2508.20830)
*Krit Duangprom,Tryphon Lambrou,Binod Bhattarai*

Main category: cs.CV

TL;DR: 利用经过LoRA微调的视觉语言模型（VLMs）提出了一种新方法，用于外科手术工具的二维关键点估计，相比传统CNN或Transformer，展示了更强的泛化能力和性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统方法在小规模医疗数据集上容易过拟合，因此有必要利用预训练VLM的泛化能力。

Method: 通过设计输入提示生成指令调整数据集，将视觉特征与语义关键点描述对齐，并结合LoRA技术对VLM进行微调。

Result: 通过仅两轮的微调实验，适配后的模型性能优于基线模型。

Conclusion: 该方法不仅改善了关键点检测性能，还为未来外科手术工具与手部3D位姿估计的研究打下基础。

Abstract: This paper presents a novel pipeline for 2D keypoint estima- tion of surgical
tools by leveraging Vision Language Models (VLMs) fine- tuned using a low rank
adjusting (LoRA) technique. Unlike traditional Convolutional Neural Network
(CNN) or Transformer-based approaches, which often suffer from overfitting in
small-scale medical datasets, our method harnesses the generalization
capabilities of pre-trained VLMs. We carefully design prompts to create an
instruction-tuning dataset and use them to align visual features with semantic
keypoint descriptions. Experimental results show that with only two epochs of
fine tuning, the adapted VLM outperforms the baseline models, demonstrating the
ef- fectiveness of LoRA in low-resource scenarios. This approach not only
improves keypoint detection performance, but also paves the way for future work
in 3D surgical hands and tools pose estimation.

</details>


### [73] [PointDGRWKV: Generalizing RWKV-like Architecture to Unseen Domains for Point Cloud Classification](https://arxiv.org/abs/2508.20835)
*Hao Yang,Qianyu Zhou,Haijia Sun,Xiangtai Li,Xuequan Lu,Lizhuang Ma,Shuicheng Yan*

Main category: cs.CV

TL;DR: 本文提出基于RWKV架构的PointDGRWKV框架，解决之前RWKV在点云分类中存在的空间建模不足和域间鲁棒性弱的问题，并通过实验显示其在领域泛化任务中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 本文针对点云分类中面向未见域的泛化问题，尝试利用RWKV架构克服传统卷积网络、Transformer等模型的局限，如局部感受野限制或高计算成本。

Method: 提出PointDGRWKV框架，其中包括两个关键模块：自适应几何token shift提高点云局部几何结构建模能力，以及跨域key特征分布对齐以解决域间注意力漂移问题。这些模块结合RWKV的线性效率特点。

Result: 在多个基准测试中证明，PointDGRWKV框架在领域泛化的点云分类任务中达到了当前最先进的性能。

Conclusion: PointDGRWKV模型不仅提升了原RWKV结构在点云分类中的性能，还克服了跨域泛化中几何建模和注意力漂移的关键挑战，为领域泛化任务提供了新解决方案。

Abstract: Domain Generalization (DG) has been recently explored to enhance the
generalizability of Point Cloud Classification (PCC) models toward unseen
domains. Prior works are based on convolutional networks, Transformer or Mamba
architectures, either suffering from limited receptive fields or high
computational cost, or insufficient long-range dependency modeling. RWKV, as an
emerging architecture, possesses superior linear complexity, global receptive
fields, and long-range dependency. In this paper, we present the first work
that studies the generalizability of RWKV models in DG PCC. We find that
directly applying RWKV to DG PCC encounters two significant challenges: RWKV's
fixed direction token shift methods, like Q-Shift, introduce spatial
distortions when applied to unstructured point clouds, weakening local
geometric modeling and reducing robustness. In addition, the Bi-WKV attention
in RWKV amplifies slight cross-domain differences in key distributions through
exponential weighting, leading to attention shifts and degraded generalization.
To this end, we propose PointDGRWKV, the first RWKV-based framework tailored
for DG PCC. It introduces two key modules to enhance spatial modeling and
cross-domain robustness, while maintaining RWKV's linear efficiency. In
particular, we present Adaptive Geometric Token Shift to model local
neighborhood structures to improve geometric context awareness. In addition,
Cross-Domain key feature Distribution Alignment is designed to mitigate
attention drift by aligning key feature distributions across domains. Extensive
experiments on multiple benchmarks demonstrate that PointDGRWKV achieves
state-of-the-art performance on DG PCC.

</details>


### [74] [PathMR: Multimodal Visual Reasoning for Interpretable Pathology Diagnosis](https://arxiv.org/abs/2508.20851)
*Ye Zhang,Yu Zhou,Jingwen Qi,Yongbing Zhang,Simon Puettmann,Finn Wichmann,Larissa Pereira Ferreira,Lara Sichward,Julius Keyl,Sylvia Hartmann,Shuo Zhao,Hongxiao Wang,Xiaowei Xu,Jianxu Chen*

Main category: cs.CV

TL;DR: 研究提出了一种名为PathMR的病理图像分析框架，通过生成专家级诊断解释和细胞分布模式预测提高AI辅助病理诊断的可解释性，并展示了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 目前的深度学习病理诊断虽然提高了效率，但由于模型决策不透明和缺乏可追溯性，阻碍了临床应用。

Method: 提出PathMR框架，以多模态视觉推理为基础，通过病理图像和文字查询生成诊断解释，预测细胞分布模式并实现像素级的分割与文本解释的语义对齐。

Result: PathMR在PathGen和新开发的GADVR数据集上的实验中，优于当前的视觉推理方法，在文本生成质量、分割准确度以及跨模态对齐方面表现出色。

Conclusion: PathMR框架具备显著的潜力，通过提高AI驱动病理诊断的可解释性，加速其在临床的采用。

Abstract: Deep learning based automated pathological diagnosis has markedly improved
diagnostic efficiency and reduced variability between observers, yet its
clinical adoption remains limited by opaque model decisions and a lack of
traceable rationale. To address this, recent multimodal visual reasoning
architectures provide a unified framework that generates segmentation masks at
the pixel level alongside semantically aligned textual explanations. By
localizing lesion regions and producing expert style diagnostic narratives,
these models deliver the transparent and interpretable insights necessary for
dependable AI assisted pathology. Building on these advancements, we propose
PathMR, a cell-level Multimodal visual Reasoning framework for Pathological
image analysis. Given a pathological image and a textual query, PathMR
generates expert-level diagnostic explanations while simultaneously predicting
cell distribution patterns. To benchmark its performance, we evaluated our
approach on the publicly available PathGen dataset as well as on our newly
developed GADVR dataset. Extensive experiments on these two datasets
demonstrate that PathMR consistently outperforms state-of-the-art visual
reasoning methods in text generation quality, segmentation accuracy, and
cross-modal alignment. These results highlight the potential of PathMR for
improving interpretability in AI-driven pathological diagnosis. The code will
be publicly available in https://github.com/zhangye-zoe/PathMR.

</details>


### [75] [Deep Learning Framework for Early Detection of Pancreatic Cancer Using Multi-Modal Medical Imaging Analysis](https://arxiv.org/abs/2508.20877)
*Dennis Slobodzian,Karissa Tilbury,Amir Kordijazi*

Main category: cs.CV

TL;DR: 研究开发并验证了一种基于深度学习的框架，通过分析自体荧光和二次谐波生成双模式成像，用于早期胰腺导管腺癌（PDAC）的检测，准确率超过90%。


<details>
  <summary>Details</summary>
Motivation: 由于胰腺导管腺癌死亡率高且早期检测困难，研究旨在开发更高效的检测方法以提高生存率。

Method: 通过分析40例患者样本，测试6种深度学习架构，包括传统卷积神经网络（CNNs）和现代视觉变换器（ViTs），最终使用优化的ResNet框架进行癌症检测。

Result: 优化后的ResNet框架在癌症检测中达到超过90%的准确率，并优于当前手工分析方法。

Conclusion: 研究建立了一个有效的自动化胰腺癌检测流程，可辅助病理学家工作，并为扩展至其他癌症类型以及有限数据医疗图像分析提供了基础。

Abstract: Pacreatic ductal adenocarcinoma (PDAC) remains one of the most lethal forms
of cancer, with a five-year survival rate below 10% primarily due to late
detection. This research develops and validates a deep learning framework for
early PDAC detection through analysis of dual-modality imaging:
autofluorescence and second harmonic generation (SHG). We analyzed 40 unique
patient samples to create a specialized neural network capable of
distinguishing between normal, fibrotic, and cancerous tissue. Our methodology
evaluated six distinct deep learning architectures, comparing traditional
Convolutional Neural Networks (CNNs) with modern Vision Transformers (ViTs).
Through systematic experimentation, we identified and overcome significant
challenges in medical image analysis, including limited dataset size and class
imbalance. The final optimized framework, based on a modified ResNet
architecture with frozen pre-trained layers and class-weighted training,
achieved over 90% accuracy in cancer detection. This represents a significant
improvement over current manual analysis methods an demonstrates potential for
clinical deployment. This work establishes a robust pipeline for automated PDAC
detection that can augment pathologists' capabilities while providing a
foundation for future expansion to other cancer types. The developed
methodology also offers valuable insights for applying deep learning to
limited-size medical imaging datasets, a common challenge in clinical
applications.

</details>


### [76] [Understanding and evaluating computer vision models through the lens of counterfactuals](https://arxiv.org/abs/2508.20881)
*Pushkar Shukla*

Main category: cs.CV

TL;DR: 本文提出了使用反事实推理来解释、审计和缓解视觉分类器和生成模型中偏差的框架。


<details>
  <summary>Details</summary>
Motivation: 通过改变语义上有意义的属性以发现模型依赖的问题，提升AI模型的公平性和稳健性。

Method: 通过提出CAVLI和ASAC框架分析分类器的决策依赖性，并提出TIBET、BiasConnect和InterMit方法用于生成模型的偏差评估及减缓。

Result: 提出了一套系统方法，可有效发现和减少AI模型的偏差并提升对种族、性别、年龄等属性的公平性。

Conclusion: 反事实推理是增强可解释性、公平性及因果分析的统一视角，为社会责任导向的模型偏差评估和缓解提供了可扩展的方法。

Abstract: Counterfactual reasoning -- the practice of asking ``what if'' by varying
inputs and observing changes in model behavior -- has become central to
interpretable and fair AI. This thesis develops frameworks that use
counterfactuals to explain, audit, and mitigate bias in vision classifiers and
generative models. By systematically altering semantically meaningful
attributes while holding others fixed, these methods uncover spurious
correlations, probe causal dependencies, and help build more robust systems.
  The first part addresses vision classifiers. CAVLI integrates attribution
(LIME) with concept-level analysis (TCAV) to quantify how strongly decisions
rely on human-interpretable concepts. With localized heatmaps and a Concept
Dependency Score, CAVLI shows when models depend on irrelevant cues like
backgrounds. Extending this, ASAC introduces adversarial counterfactuals that
perturb protected attributes while preserving semantics. Through curriculum
learning, ASAC fine-tunes biased models for improved fairness and accuracy
while avoiding stereotype-laden artifacts.
  The second part targets generative Text-to-Image (TTI) models. TIBET provides
a scalable pipeline for evaluating prompt-sensitive biases by varying
identity-related terms, enabling causal auditing of how race, gender, and age
affect image generation. To capture interactions, BiasConnect builds causal
graphs diagnosing intersectional biases. Finally, InterMit offers a modular,
training-free algorithm that mitigates intersectional bias via causal
sensitivity scores and user-defined fairness goals.
  Together, these contributions show counterfactuals as a unifying lens for
interpretability, fairness, and causality in both discriminative and generative
models, establishing principled, scalable methods for socially responsible bias
evaluation and mitigation.

</details>


### [77] [ExpertSim: Fast Particle Detector Simulation Using Mixture-of-Generative-Experts](https://arxiv.org/abs/2508.20991)
*Patryk Będkowski,Jan Dubiński,Filip Szatkowski,Kamil Deja,Przemysław Rokita,Tomasz Trzciński*

Main category: cs.CV

TL;DR: 本文提出ExpertSim，一种基于深度学习的模拟方法，用于CERN的ALICE实验，以应对粒子碰撞模拟所需的高计算成本。


<details>
  <summary>Details</summary>
Motivation: 目前粒子碰撞的检测响应模拟主要依赖于高计算成本的统计蒙特卡洛方法，因此需要开发更加高效的模拟方法。

Method: 提出了一种称为ExpertSim的新方法，采用了生成专家混合架构（Mixture-of-Generative-Experts），每个专家专注于生成数据的不同子集，提升模拟效率和精度。

Result: 相比传统蒙特卡洛方法，ExpertSim不仅提高了模拟精度，还显著提升了效率并加快了生成速度。

Conclusion: ExpertSim为高效率粒子探测器模拟提供了一种前景可观的解决方案，大大减少了CERN计算资源的消耗。

Abstract: Simulating detector responses is a crucial part of understanding the inner
workings of particle collisions in the Large Hadron Collider at CERN. Such
simulations are currently performed with statistical Monte Carlo methods, which
are computationally expensive and put a significant strain on CERN's
computational grid. Therefore, recent proposals advocate for generative machine
learning methods to enable more efficient simulations. However, the
distribution of the data varies significantly across the simulations, which is
hard to capture with out-of-the-box methods. In this study, we present
ExpertSim - a deep learning simulation approach tailored for the Zero Degree
Calorimeter in the ALICE experiment. Our method utilizes a
Mixture-of-Generative-Experts architecture, where each expert specializes in
simulating a different subset of the data. This allows for a more precise and
efficient generation process, as each expert focuses on a specific aspect of
the calorimeter response. ExpertSim not only improves accuracy, but also
provides a significant speedup compared to the traditional Monte-Carlo methods,
offering a promising solution for high-efficiency detector simulations in
particle physics experiments at CERN. We make the code available at
https://github.com/patrick-bedkowski/expertsim-mix-of-generative-experts.

</details>


### [78] [To New Beginnings: A Survey of Unified Perception in Autonomous Vehicle Software](https://arxiv.org/abs/2508.20892)
*Loïc Stratil,Felix Fent,Esteban Rivera,Markus Lienkamp*

Main category: cs.CV

TL;DR: 本文综述了自动驾驶中的统一感知方法，从任务集成、跟踪表述和表示流程方面创建了分类体系，并提出三种统一感知范式，全面审视现存方法并指导未来研究。


<details>
  <summary>Details</summary>
Motivation: 提出统一感知框架，克服模块化流水线中的错误累积问题并增强任务间协同作用。

Method: 通过系统性分类和全面综述，定义三种统一感知范式，分析现有方法的架构、训练策略、及开源资源，并提出未来研究方向。

Result: 本文首次构建统一感知的综合框架，归纳分散的研究并指导未来更稳健、通用且可解释的感知研究。

Conclusion: 统一感知具有提升鲁棒性、上下文推理和效率的潜力，通过该框架可有效指导此方向的后续研究。

Abstract: Autonomous vehicle perception typically relies on modular pipelines that
decompose the task into detection, tracking, and prediction. While
interpretable, these pipelines suffer from error accumulation and limited
inter-task synergy. Unified perception has emerged as a promising paradigm that
integrates these sub-tasks within a shared architecture, potentially improving
robustness, contextual reasoning, and efficiency while retaining interpretable
outputs. In this survey, we provide a comprehensive overview of unified
perception, introducing a holistic and systemic taxonomy that categorizes
methods along task integration, tracking formulation, and representation flow.
We define three paradigms -Early, Late, and Full Unified Perception- and
systematically review existing methods, their architectures, training
strategies, datasets used, and open-source availability, while highlighting
future research directions. This work establishes the first comprehensive
framework for understanding and advancing unified perception, consolidates
fragmented efforts, and guides future research toward more robust,
generalizable, and interpretable perception.

</details>


### [79] [Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation](https://arxiv.org/abs/2508.20909)
*Yifan Gao,Haoyue Li,Feng Yuan,Xiaosong Wang,Xin Gao*

Main category: cs.CV

TL;DR: 提出了一种新型的Dino U-Net架构，结合DINOv3特征，实现了卓越的医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决将自然图像基础模型迁移到精确医学图像分割中的挑战。

Method: 设计了基于DINOv3骨干的冻结编码器，使用适配器融合语义与空间细节，并引入了保真投影模块(FAPM)以优化特征。

Result: 在7个公开医学图像分割数据集上实验，Dino U-Net表现优于现有方法，模式规模越大分割精度越高。

Conclusion: 利用DINOv3高保真预训练特征能以参数高效的方式提升医学图像分割准确性。

Abstract: Foundation models pre-trained on large-scale natural image datasets offer a
powerful paradigm for medical image segmentation. However, effectively
transferring their learned representations for precise clinical applications
remains a challenge. In this work, we propose Dino U-Net, a novel
encoder-decoder architecture designed to exploit the high-fidelity dense
features of the DINOv3 vision foundation model. Our architecture introduces an
encoder built upon a frozen DINOv3 backbone, which employs a specialized
adapter to fuse the model's rich semantic features with low-level spatial
details. To preserve the quality of these representations during dimensionality
reduction, we design a new fidelity-aware projection module (FAPM) that
effectively refines and projects the features for the decoder. We conducted
extensive experiments on seven diverse public medical image segmentation
datasets. Our results show that Dino U-Net achieves state-of-the-art
performance, consistently outperforming previous methods across various imaging
modalities. Our framework proves to be highly scalable, with segmentation
accuracy consistently improving as the backbone model size increases up to the
7-billion-parameter variant. The findings demonstrate that leveraging the
superior, dense-pretrained features from a general-purpose foundation model
provides a highly effective and parameter-efficient approach to advance the
accuracy of medical image segmentation. The code is available at
https://github.com/yifangao112/DinoUNet.

</details>


### [80] [Classifying Mitotic Figures in the MIDOG25 Challenge with Deep Ensemble Learning and Rule Based Refinement](https://arxiv.org/abs/2508.20919)
*Sara Krauss,Ellena Spieß,Daniel Hieber,Frank Kramer,Johannes Schobel,Dominik Müller*

Main category: cs.CV

TL;DR: 该研究提出一种结合ConvNeXtBase模型与基于规则的改进模块（RBR）的方法，用于区分异常有丝分裂图像（AMFs）与正常有丝分裂图像（NMFs），在测试集上取得了良好的性能。


<details>
  <summary>Details</summary>
Motivation: 病理上的有丝分裂（MFs）是判断肿瘤分级的重要生物标志。将异常有丝分裂（AMFs）与正常有丝分裂（NMFs）区分开是一个挑战，人工注释耗时且主观。因此亟需一种有效的自动化方法来解决这一问题。

Method: 研究使用ConvNeXtBase模型的集成网络，并结合基于规则的改进模块（RBR），提高AMF分类的性能。在训练过程中通过AUCMEDI进行优化，并将性能测试于MIDOG25数据集上。

Result: 集成方法在MIDOG25初步测试集上取得了84.02%平衡准确率。RBR模块提高了特定指标的表现（如特异性），但降低了灵敏性与整体性能。

Conclusion: 研究表明深度学习的集成方法对AMF分类表现良好，同时RBR模块具有潜力提升特定指标，但需要进一步研究以优化整体表现。

Abstract: Mitotic figures (MFs) are relevant biomarkers in tumor grading.
Differentiating atypical MFs (AMFs) from normal MFs (NMFs) remains difficult,
as manual annotation is time-consuming and subjective. In this work an ensemble
of ConvNeXtBase models was trained with AUCMEDI and extend with a rule-based
refinement (RBR) module. On the MIDOG25 preliminary test set, the ensemble
achieved a balanced accuracy of 84.02%. While the RBR increased specificity, it
reduced sensitivity and overall performance. The results show that deep
ensembles perform well for AMF classification. RBR can increase specific
metrics but requires further research.

</details>


### [81] [FW-GAN: Frequency-Driven Handwriting Synthesis with Wave-Modulated MLP Generator](https://arxiv.org/abs/2508.21040)
*Huynh Tong Dang Khoa,Dang Hoai Nam,Vo Nguyen Le Duy*

Main category: cs.CV

TL;DR: 本研究提出FW-GAN框架，用于从单个实例生成风格一致的高质量手写文字。


<details>
  <summary>Details</summary>
Motivation: 现有手写数据有限，影响识别系统效果；现有手写合成方法难以建模远程依赖关系和复杂笔画，并忽视频率信息。

Method: FW-GAN采用结合Wave-MLP的生成器，加入频率指导的判别器以及频率分布损失，提高生成效果。

Result: 在越南语和英语手写数据集上，FW-GAN生成了高质量且风格一致的手写文字。

Conclusion: FW-GAN是一种可增强手写文本识别的有价值工具，尤其在低资源场景中。

Abstract: Labeled handwriting data is often scarce, limiting the effectiveness of
recognition systems that require diverse, style-consistent training samples.
Handwriting synthesis offers a promising solution by generating artificial data
to augment training. However, current methods face two major limitations.
First, most are built on conventional convolutional architectures, which
struggle to model long-range dependencies and complex stroke patterns. Second,
they largely ignore the crucial role of frequency information, which is
essential for capturing fine-grained stylistic and structural details in
handwriting. To address these challenges, we propose FW-GAN, a one-shot
handwriting synthesis framework that generates realistic, writer-consistent
text from a single example. Our generator integrates a phase-aware Wave-MLP to
better capture spatial relationships while preserving subtle stylistic cues. We
further introduce a frequency-guided discriminator that leverages
high-frequency components to enhance the authenticity detection of generated
samples. Additionally, we introduce a novel Frequency Distribution Loss that
aligns the frequency characteristics of synthetic and real handwriting, thereby
enhancing visual fidelity. Experiments on Vietnamese and English handwriting
datasets demonstrate that FW-GAN generates high-quality, style-consistent
handwriting, making it a valuable tool for augmenting data in low-resource
handwriting recognition (HTR) pipelines. Official implementation is available
at https://github.com/DAIR-Group/FW-GAN

</details>


### [82] [COMETH: Convex Optimization for Multiview Estimation and Tracking of Humans](https://arxiv.org/abs/2508.20920)
*Enrico Martini,Ho Jin Choi,Nadia Figueroa,Nicola Bombieri*

Main category: cs.CV

TL;DR: 提出了一种轻量级算法—COMETH，用于实时多视角人类姿态融合，旨在提高工业与安全关键应用中的人体活动监测精度。


<details>
  <summary>Details</summary>
Motivation: 工业5.0时代对人类活动监测需求的增长，同时现有中央多摄像头方案存在高计算成本及网络带宽限制，边缘设备又因资源受限导致准确性下降。

Method: 提出COMETH算法，通过融合运动学与生物力学限制以提升关节定位精度，利用凸优化逆运动学进行空间融合，并用状态观测器增强时间一致性。

Result: COMETH在公共与工业数据集上验证优于现有方法，同时支持高精度与可扩展的人体运动跟踪。

Conclusion: 该算法为工业与安全关键应用提供了一种高效、精准、可扩展的人体监测解决方案，代码已开源。

Abstract: In the era of Industry 5.0, monitoring human activity is essential for
ensuring both ergonomic safety and overall well-being. While multi-camera
centralized setups improve pose estimation accuracy, they often suffer from
high computational costs and bandwidth requirements, limiting scalability and
real-time applicability. Distributing processing across edge devices can reduce
network bandwidth and computational load. On the other hand, the constrained
resources of edge devices lead to accuracy degradation, and the distribution of
computation leads to temporal and spatial inconsistencies. We address this
challenge by proposing COMETH (Convex Optimization for Multiview Estimation and
Tracking of Humans), a lightweight algorithm for real-time multi-view human
pose fusion that relies on three concepts: it integrates kinematic and
biomechanical constraints to increase the joint positioning accuracy; it
employs convex optimization-based inverse kinematics for spatial fusion; and it
implements a state observer to improve temporal consistency. We evaluate COMETH
on both public and industrial datasets, where it outperforms state-of-the-art
methods in localization, detection, and tracking accuracy. The proposed fusion
pipeline enables accurate and scalable human motion tracking, making it
well-suited for industrial and safety-critical applications. The code is
publicly available at https://github.com/PARCO-LAB/COMETH.

</details>


### [83] [Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning](https://arxiv.org/abs/2508.21048)
*Hao Tan,Jun Lan,Zichang Tan,Ajian Liu,Chuanbiao Song,Senyuan Shi,Huijia Zhu,Weiqiang Wang,Jun Wan,Zhen Lei*

Main category: cs.CV

TL;DR: 本文提出了HydraFake数据集和一个名为Veritas的多模态大语言模型，用于解决深度伪造检测在真实世界中的挑战。新模型显著提升了各种开放领域场景下的检测性能。


<details>
  <summary>Details</summary>
Motivation: 目前的深度伪造检测基准数据集与工业实践差距大，训练数据单一且测试图像质量低，这极大阻碍了现有检测器的实际应用。本文旨在通过真实世界场景模拟填补这一差距。

Method: 本文构建了HydraFake数据集，它包含多样化的深度伪造技术和真实场景伪造，配以严谨的训练和评估协议。同时提出Veritas模型，该模型基于多模态大语言模型并引入了模式感知推理和两阶段训练管道，模仿人类法医学中的推理过程。

Result: 在HydraFake数据集测试中，以往的检测器虽在跨模型场景下表现良好，但在未见伪造和数据域中表现不佳。而Veritas模型在不同的开放领域场景中显著提升了检测效果，并能生成透明、可信的检测输出。

Conclusion: 通过引入HydraFake数据集和新型检测器Veritas，本文为解决深度伪造检测中的实际问题提供了有效方法，显著缩小了研究与工业实践的差距。

Abstract: Deepfake detection remains a formidable challenge due to the complex and
evolving nature of fake content in real-world scenarios. However, existing
academic benchmarks suffer from severe discrepancies from industrial practice,
typically featuring homogeneous training sources and low-quality testing
images, which hinder the practical deployments of current detectors. To
mitigate this gap, we introduce HydraFake, a dataset that simulates real-world
challenges with hierarchical generalization testing. Specifically, HydraFake
involves diversified deepfake techniques and in-the-wild forgeries, along with
rigorous training and evaluation protocol, covering unseen model architectures,
emerging forgery techniques and novel data domains. Building on this resource,
we propose Veritas, a multi-modal large language model (MLLM) based deepfake
detector. Different from vanilla chain-of-thought (CoT), we introduce
pattern-aware reasoning that involves critical reasoning patterns such as
"planning" and "self-reflection" to emulate human forensic process. We further
propose a two-stage training pipeline to seamlessly internalize such deepfake
reasoning capacities into current MLLMs. Experiments on HydraFake dataset
reveal that although previous detectors show great generalization on
cross-model scenarios, they fall short on unseen forgeries and data domains.
Our Veritas achieves significant gains across different OOD scenarios, and is
capable of delivering transparent and faithful detection outputs.

</details>


### [84] [Dress&Dance: Dress up and Dance as You Like It - Technical Preview](https://arxiv.org/abs/2508.21070)
*Jun-Kun Chen,Aayush Bansal,Minh Phuoc Vo,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: Dress&Dance是一种视频扩散框架，可生成1152x720分辨率、5秒长的虚拟试穿视频，用户可穿着目标服饰并随参考视频动作移动。


<details>
  <summary>Details</summary>
Motivation: 设计一个高分辨率的视频虚拟试穿框架，从有限数据中创造逼真的服饰和移动效果。

Method: 提出了CondNet，一个新颖的条件网络，结合多模态输入并分阶段进化式训练，支持服饰注册和动作生成的高保真度。

Result: Dress&Dance比现有开源及商业解决方案表现更佳，提供高质量、灵活的虚拟试穿体验。

Conclusion: 通过结合文本、图像和视频数据，Dress&Dance在虚拟服饰试穿领域展示了创新性并取得显著成果。

Abstract: We present Dress&Dance, a video diffusion framework that generates high
quality 5-second-long 24 FPS virtual try-on videos at 1152x720 resolution of a
user wearing desired garments while moving in accordance with a given reference
video. Our approach requires a single user image and supports a range of tops,
bottoms, and one-piece garments, as well as simultaneous tops and bottoms
try-on in a single pass. Key to our framework is CondNet, a novel conditioning
network that leverages attention to unify multi-modal inputs (text, images, and
videos), thereby enhancing garment registration and motion fidelity. CondNet is
trained on heterogeneous training data, combining limited video data and a
larger, more readily available image dataset, in a multistage progressive
manner. Dress&Dance outperforms existing open source and commercial solutions
and enables a high quality and flexible try-on experience.

</details>


### [85] [Olive Tree Satellite Image Segmentation Based On SAM and Multi-Phase Refinement](https://arxiv.org/abs/2508.20954)
*Amir Jmal,Chaima Chtourou,Mahdi Louati,Abdelaziz Kallel,Houda Khmila*

Main category: cs.CV

TL;DR: 本文提出了一种通过融合 Segment Anything Model (SAM) 和形状/大小约束的遥感方法，实现高精度橄榄树分割，新方法准确率达98%。


<details>
  <summary>Details</summary>
Motivation: 为了应对气候变化带来的挑战，研究目标是维护橄榄树群体的生物多样性，通过遥感技术实现早期异常检测和处理。

Method: 采用 Segment Anything Model (SAM) 进行初步分割，并通过结合树木排列方向和形状/大小可学习约束进行精确修正。

Result: 新方法在遥感影像中实现了高达98%的橄榄树分割准确率，显著超越SAM初始性能（82%）。

Conclusion: 借助该分割方法，可以为农业管理提供高效的解决方案，促进生物多样性保护和农业资源优化。

Abstract: In the context of proven climate change, maintaining olive biodiversity
through early anomaly detection and treatment using remote sensing technology
is crucial, offering effective management solutions. This paper presents an
innovative approach to olive tree segmentation from satellite images. By
leveraging foundational models and advanced segmentation techniques, the study
integrates the Segment Anything Model (SAM) to accurately identify and segment
olive trees in agricultural plots. The methodology includes SAM segmentation
and corrections based on trees alignement in the field and a learanble
constraint about the shape and the size. Our approach achieved a 98\% accuracy
rate, significantly surpassing the initial SAM performance of 82\%.

</details>


### [86] [E-ConvNeXt: A Lightweight and Efficient ConvNeXt Variant with Cross-Stage Partial Connections](https://arxiv.org/abs/2508.20955)
*Fang Wang,Huitao Li,Wenhan Chao,Zheng Zhuo,Yiran Ji,Chang Peng,Yupeng Sun*

Main category: cs.CV

TL;DR: 该论文提出了一种新型轻量化网络架构E-ConvNeXt，在保持高精度的情况下显著降低网络复杂度，尤其适用于轻量化应用场景。


<details>
  <summary>Details</summary>
Motivation: 许多高性能网络初设计时未考虑轻量级应用场景，导致应用范围受限。

Method: 通过引入Cross Stage Partial Connections机制，优化网络结构，并置换Layer Scale以增强特征表达和运行效率，从而设计E-ConvNeXt。

Result: E-ConvNeXt-mini在ImageNet分类中实现78.3% Top-1准确率（0.9GFLOPs），E-ConvNeXt-small实现81.9% Top-1准确率（3.1GFLOPs）。其迁移学习实验表明对目标检测任务同样具有很好的泛化能力。

Conclusion: E-ConvNeXt在复杂度和精度之间实现了良好的平衡，适合轻量化应用场景并显示出广泛的适用性。

Abstract: Many high-performance networks were not designed with lightweight application
scenarios in mind from the outset, which has greatly restricted their scope of
application. This paper takes ConvNeXt as the research object and significantly
reduces the parameter scale and network complexity of ConvNeXt by integrating
the Cross Stage Partial Connections mechanism and a series of optimized
designs. The new network is named E-ConvNeXt, which can maintain high accuracy
performance under different complexity configurations. The three core
innovations of E-ConvNeXt are : (1) integrating the Cross Stage Partial Network
(CSPNet) with ConvNeXt and adjusting the network structure, which reduces the
model's network complexity by up to 80%; (2) Optimizing the Stem and Block
structures to enhance the model's feature expression capability and operational
efficiency; (3) Replacing Layer Scale with channel attention. Experimental
validation on ImageNet classification demonstrates E-ConvNeXt's superior
accuracy-efficiency balance: E-ConvNeXt-mini reaches 78.3% Top-1 accuracy at
0.9GFLOPs. E-ConvNeXt-small reaches 81.9% Top-1 accuracy at 3.1GFLOPs. Transfer
learning tests on object detection tasks further confirm its generalization
capability.

</details>


### [87] [FakeParts: a New Family of AI-Generated DeepFakes](https://arxiv.org/abs/2508.21052)
*Gaetan Brison,Soobash Daiboo,Samy Aimeur,Awais Hussain Sani,Xi Wang,Gianni Franchi,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: 本文提出一种新型深伪技术FakeParts，专注于对真实视频的局部区域或时间段进行细微、局部的操控，既隐秘又难以检测。同时设计了FakePartsBench数据集，包含25K段视频，支持像素级和帧级检测评估。研究发现FakeParts比传统深伪降低了超30%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 目前的深伪技术多集中于全局的视觉操控，而针对局部细微操控的检测仍处于研究空白阶段，这种技术容易欺骗检测模型和人类观测者，因此迫切需要新的研究工具和方法来优化检测能力。

Method: 开发了专门研究局部深伪的FakePartsBench数据集，包含超过25,000段带有像素和帧操控注释的视频，用以全面评估不同检测方法的性能，并辅以用户研究来验证效果。

Result: 用户研究表明，FakeParts深伪降低了人类检测准确率超过30%，同时对现有检测模型也产生了类似的性能下降。

Conclusion: FakeParts技术揭示了当前深伪检测技术的一项重大漏洞，并为研究人员提供了一个必要的工具即FakePartsBench，为开发更强大的检测方法奠定了基础。

Abstract: We introduce FakeParts, a new class of deepfakes characterized by subtle,
localized manipulations to specific spatial regions or temporal segments of
otherwise authentic videos. Unlike fully synthetic content, these partial
manipulations, ranging from altered facial expressions to object substitutions
and background modifications, blend seamlessly with real elements, making them
particularly deceptive and difficult to detect. To address the critical gap in
detection capabilities, we present FakePartsBench, the first large-scale
benchmark dataset specifically designed to capture the full spectrum of partial
deepfakes. Comprising over 25K videos with pixel-level and frame-level
manipulation annotations, our dataset enables comprehensive evaluation of
detection methods. Our user studies demonstrate that FakeParts reduces human
detection accuracy by over 30% compared to traditional deepfakes, with similar
performance degradation observed in state-of-the-art detection models. This
work identifies an urgent vulnerability in current deepfake detection
approaches and provides the necessary resources to develop more robust methods
for partial video manipulations.

</details>


### [88] [DrivingGaussian++: Towards Realistic Reconstruction and Editable Simulation for Surrounding Dynamic Driving Scenes](https://arxiv.org/abs/2508.20965)
*Yajiao Xiong,Xiaoyu Zhou,Yongtao Wan,Deqing Sun,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: DrivingGaussian++是一种高效的框架，用于动态自动驾驶场景的真实重建和可控编辑，其通过3D高斯模型对背景和动态物体进行建模，结合LiDAR先验实现了详细的场景重建和逼真的视角合成。


<details>
  <summary>Details</summary>
Motivation: 解决现有动态场景重建和真实感视角合成不足的问题，为实现更高自由度的可控编辑提供工具。

Method: 结合3D高斯模型和LiDAR先验，精准建模静态背景和动态物体，通过动态高斯图支持场景重建和可控编辑，并集成大语言模型生成动态运动轨迹，包括纹理修改、天气模拟等功能。

Result: DrivingGaussian++在动态场景重建和真实感周围视图合成上优于现有方法，且能够实现一致且真实的动态编辑结果，增强了场景多样性。

Conclusion: 该方法显著改善了动态场景的重建质量和编辑自由度，是创建多视角动态驾驶场景的重要工具。

Abstract: We present DrivingGaussian++, an efficient and effective framework for
realistic reconstructing and controllable editing of surrounding dynamic
autonomous driving scenes. DrivingGaussian++ models the static background using
incremental 3D Gaussians and reconstructs moving objects with a composite
dynamic Gaussian graph, ensuring accurate positions and occlusions. By
integrating a LiDAR prior, it achieves detailed and consistent scene
reconstruction, outperforming existing methods in dynamic scene reconstruction
and photorealistic surround-view synthesis. DrivingGaussian++ supports
training-free controllable editing for dynamic driving scenes, including
texture modification, weather simulation, and object manipulation, leveraging
multi-view images and depth priors. By integrating large language models (LLMs)
and controllable editing, our method can automatically generate dynamic object
motion trajectories and enhance their realism during the optimization process.
DrivingGaussian++ demonstrates consistent and realistic editing results and
generates dynamic multi-view driving scenarios, while significantly enhancing
scene diversity. More results and code can be found at the project site:
https://xiong-creator.github.io/DrivingGaussian_plus.github.io

</details>


### [89] [Webly-Supervised Image Manipulation Localization via Category-Aware Auto-Annotation](https://arxiv.org/abs/2508.20987)
*Chenfan Qu,Yiwu Zhong,Bin Li,Lianwen Jin*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，通过利用网络数据来解决图像操纵区域定位中的数据稀缺问题，并引入了CAAAv2、QES、MIMLv2和Web-IML，以显著提升各种模型的性能。


<details>
  <summary>Details</summary>
Motivation: 图像操纵可能对社会安全构成威胁，目前精确定位图像中的操纵区域仍然是一个难题，数据获取成本高和优质标注数据集短缺是主要挑战。

Method: 提出CAAAv2用于生成精确的操纵区域像素级注释，并利用QES提高注释质量。此外，开发MIMLv2数据集和Object Jitter技术，通过Web-IML模型有效利用网络级监督学习来定位操纵区域。

Result: 构建了比现有数据集大120倍的MIMLv2数据集，并通过Web-IML显著改善多种基准模型的表现，相较于SOTA TruFor，平均IoU点数提升24.1。

Conclusion: 利用网络数据和新方法，缓解了图像操纵区域定位中的数据稀缺问题，并为未来研究提供了开放数据集与代码。

Abstract: Images manipulated using image editing tools can mislead viewers and pose
significant risks to social security. However, accurately localizing the
manipulated regions within an image remains a challenging problem. One of the
main barriers in this area is the high cost of data acquisition and the severe
lack of high-quality annotated datasets. To address this challenge, we
introduce novel methods that mitigate data scarcity by leveraging readily
available web data. We utilize a large collection of manually forged images
from the web, as well as automatically generated annotations derived from a
simpler auxiliary task, constrained image manipulation localization.
Specifically, we introduce a new paradigm CAAAv2, which automatically and
accurately annotates manipulated regions at the pixel level. To further improve
annotation quality, we propose a novel metric, QES, which filters out
unreliable annotations. Through CAAA v2 and QES, we construct MIMLv2, a
large-scale, diverse, and high-quality dataset containing 246,212 manually
forged images with pixel-level mask annotations. This is over 120x larger than
existing handcrafted datasets like IMD20. Additionally, we introduce Object
Jitter, a technique that further enhances model training by generating
high-quality manipulation artifacts. Building on these advances, we develop a
new model, Web-IML, designed to effectively leverage web-scale supervision for
the image manipulation localization task. Extensive experiments demonstrate
that our approach substantially alleviates the data scarcity problem and
significantly improves the performance of various models on multiple real-world
forgery benchmarks. With the proposed web supervision, Web-IML achieves a
striking performance gain of 31% and surpasses previous SOTA TruFor by 24.1
average IoU points. The dataset and code will be made publicly available at
https://github.com/qcf-568/MIML.

</details>


### [90] [POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models](https://arxiv.org/abs/2508.21019)
*Jiaxiang Cheng,Bing Ma,Xuhua Ren,Hongyi Jin,Kai Yu,Peng Zhang,Wenyue Li,Yuan Zhou,Tianxiang Zheng,Qinglin Lu*

Main category: cs.CV

TL;DR: 本文提出POSE框架，通过单步蒸馏减少大规模视频扩散模型的采样步骤，显著提高效率和生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前视频扩散生成的效率瓶颈限制了大规模模型在长序列上的应用，现有方法难以同时保证时间连贯性和单步蒸馏效果。

Method: POSE采用两阶段过程：1) 稳定引导机制优化视频质量；2) 统一自适应对抗蒸馏推向高斯噪声空间的均衡态，同时对条件生成提出增强一致性的方法。

Result: POSE在各指标上超越了现有的加速方法，在VBench-I2V上语义对齐、时间一致性和帧质量指标提高7.15%，并将延迟减少100倍。

Conclusion: POSE成功解决了当前视频扩散加速的关键问题，以单步蒸馏达成高效的视频生成，为未来大规模视频生成技术奠定了基础。

Abstract: The field of video diffusion generation faces critical bottlenecks in
sampling efficiency, especially for large-scale models and long sequences.
Existing video acceleration methods adopt image-based techniques but suffer
from fundamental limitations: they neither model the temporal coherence of
video frames nor provide single-step distillation for large-scale video models.
To bridge this gap, we propose POSE (Phased One-Step Equilibrium), a
distillation framework that reduces the sampling steps of large-scale video
diffusion models, enabling the generation of high-quality videos in a single
step. POSE employs a carefully designed two-phase process to distill video
models:(i) stability priming: a warm-up mechanism to stabilize adversarial
distillation that adapts the high-quality trajectory of the one-step generator
from high to low signal-to-noise ratio regimes, optimizing the video quality of
single-step mappings near the endpoints of flow trajectories. (ii) unified
adversarial equilibrium: a flexible self-adversarial distillation mechanism
that promotes stable single-step adversarial training towards a Nash
equilibrium within the Gaussian noise space, generating realistic single-step
videos close to real videos. For conditional video generation, we propose (iii)
conditional adversarial consistency, a method to improve both semantic
consistency and frame consistency between conditional frames and generated
frames. Comprehensive experiments demonstrate that POSE outperforms other
acceleration methods on VBench-I2V by average 7.15% in semantic alignment,
temporal conference and frame quality, reducing the latency of the pre-trained
model by 100$\times$, from 1000 seconds to 10 seconds, while maintaining
competitive performance.

</details>


### [91] [Reusing Computation in Text-to-Image Diffusion for Efficient Generation of Image Sets](https://arxiv.org/abs/2508.21032)
*Dale Decatur,Thibault Groueix,Wang Yifan,Rana Hanocka,Vladimir Kim,Matheus Gadelha*

Main category: cs.CV

TL;DR: 此论文探讨降低文本到图像扩散模型计算成本的新方法，通过在相关的提示之间减少冗余，提出利用扩散模型的粗到细特性以及语义相似性共享计算的方法，从而降低计算成本和提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型尽管生成图像质量极高，但其计算代价相当昂贵，因此需要探索方法以减轻计算负担并提高使用效率。

Method: 作者提出一种无训练的方法，以语义相似性为基础将提示进行聚类，在扩散模型的早期步骤中共享计算。此外，利用UnClip的文本到图像先验，优化扩散步骤分配，从而提升效率。

Result: 实验表明，在基于图像嵌入的模型上，此方法显著降低了计算成本，同时改善了生成的图像质量。

Conclusion: 该方法能够无缝整合至现有的文本到图像管道中，且可随提示集合扩展，减少了大规模生成任务的计算和环境成本。

Abstract: Text-to-image diffusion models enable high-quality image generation but are
computationally expensive. While prior work optimizes per-inference efficiency,
we explore an orthogonal approach: reducing redundancy across correlated
prompts. Our method leverages the coarse-to-fine nature of diffusion models,
where early denoising steps capture shared structures among similar prompts. We
propose a training-free approach that clusters prompts based on semantic
similarity and shares computation in early diffusion steps. Experiments show
that for models trained conditioned on image embeddings, our approach
significantly reduces compute cost while improving image quality. By leveraging
UnClip's text-to-image prior, we enhance diffusion step allocation for greater
efficiency. Our method seamlessly integrates with existing pipelines, scales
with prompt sets, and reduces the environmental and financial burden of
large-scale text-to-image generation. Project page:
https://ddecatur.github.io/hierarchical-diffusion/

</details>


### [92] [Mitosis detection in domain shift scenarios: a Mamba-based approach](https://arxiv.org/abs/2508.21033)
*Gennaro Percannella,Mattia Sarno,Francesco Tortorella,Mario Vento*

Main category: cs.CV

TL;DR: 此论文提出了一种基于Mamba的方法，用于应对组织病理图像中因域偏移导致的有丝分裂检测性能下降问题，并采用了VM-UNet架构和染色增强技术。


<details>
  <summary>Details</summary>
Motivation: 解决因域偏移引起的机器学习算法在病理图像应用中的性能下降问题，特别是有丝分裂检测任务中的挑战。

Method: 提出了一种基于Mamba的模型，具体应用了VM-UNet架构和染色增强操作，以增强对域偏移的鲁棒性。

Result: 初步实验表明方法在MIDOG++数据集上的性能有较大提升空间。

Conclusion: 尽管方法在应对域偏移上有一定潜力，但仍需进一步优化以提高其实际有效性。

Abstract: Mitosis detection in histopathology images plays a key role in tumor
assessment. Although machine learning algorithms could be exploited for aiding
physicians in accurately performing such a task, these algorithms suffer from
significative performance drop when evaluated on images coming from domains
that are different from the training ones. In this work, we propose a
Mamba-based approach for mitosis detection under domain shift, inspired by the
promising performance demonstrated by Mamba in medical imaging segmentation
tasks. Specifically, our approach exploits a VM-UNet architecture for carrying
out the addressed task, as well as stain augmentation operations for further
improving model robustness against domain shift. Our approach has been
submitted to the track 1 of the MItosis DOmain Generalization (MIDOG)
challenge. Preliminary experiments, conducted on the MIDOG++ dataset, show
large room for improvement for the proposed method.

</details>


### [93] [A multi-task neural network for atypical mitosis recognition under domain shift](https://arxiv.org/abs/2508.21035)
*Gennaro Percannella,Mattia Sarno,Francesco Tortorella,Mario Vento*

Main category: cs.CV

TL;DR: 本文提出一种基于多任务学习的方法，以改进在领域转移情况下识别组织病理图片中非典型有丝分裂形态的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型在领域转移条件下表现较差，限制了其在评估肿瘤侵袭性任务中的应用。

Method: 通过结合多个辅任务与主要分类任务，帮助模型专注于分类目标，减少受背景领域变化影响。

Result: 在MIDOG 2025 Atypical Training Set、Ami-Br 数据集以及MIDOG25挑战的初步测试集上，该方法表现出了良好效果。

Conclusion: 多任务学习能够帮助模型在领域转移情况下提高分类性能，展示了该方法的潜力。

Abstract: Recognizing atypical mitotic figures in histopathology images allows
physicians to correctly assess tumor aggressiveness. Although machine learning
models could be exploited for automatically performing such a task, under
domain shift these models suffer from significative performance drops. In this
work, an approach based on multi-task learning is proposed for addressing this
problem. By exploiting auxiliary tasks, correlated to the main classification
task, the proposed approach, submitted to the track 2 of the MItosis DOmain
Generalization (MIDOG) challenge, aims to aid the model to focus only on the
object to classify, ignoring the domain varying background of the image. The
proposed approach shows promising performance in a preliminary evaluation
conducted on three distinct datasets, i.e., the MIDOG 2025 Atypical Training
Set, the Ami-Br dataset, as well as the preliminary test set of the MIDOG25
challenge.

</details>


### [94] [MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for Efficient Video LLMs](https://arxiv.org/abs/2508.21044)
*Junpeng Ma,Qizhe Zhang,Ming Lu,Zhibin Wang,Qiang Zhou,Jun Song,Shanghang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为MMG-Vid的训练自由视觉令牌剪枝框架，能够在保持99.5%以上性能的同时，实现75%的视觉令牌削减和3.9倍性能加速。


<details>
  <summary>Details</summary>
Motivation: 现有的视频大语言模型虽然在视频理解上表现出色，但其过多的视觉令牌带来了计算挑战，而现有方法未充分考虑视频帧的动态特性和时序依赖。

Method: 通过分割视频为基于帧相似性的段，并动态分配令牌预算来最大化边际增益。此外，提出了一种时序引导的DPC算法，能够联合建模帧间独特性和帧内多样性，从而在段级和令牌级实现边际增益的最大化。

Result: 显著提高了效率，在保持超过99.5%原始性能的同时，削减了75%的视觉令牌，并在LLaVA-OneVision-7B模型的预填充阶段实现了3.9倍的加速。

Conclusion: MMG-Vid能通过最大化有限令牌预算的利用率，在提升效率的同时保持优异性能，为视频大语言模型的实际应用提供了可行方案。

Abstract: Video Large Language Models (VLLMs) excel in video understanding, but their
excessive visual tokens pose a significant computational challenge for
real-world applications. Current methods aim to enhance inference efficiency by
visual token pruning. However, they do not consider the dynamic characteristics
and temporal dependencies of video frames, as they perceive video understanding
as a multi-frame task. To address these challenges, we propose MMG-Vid, a novel
training-free visual token pruning framework that removes redundancy by
Maximizing Marginal Gains at both segment-level and token-level. Specifically,
we first divide the video into segments based on frame similarity, and then
dynamically allocate the token budget for each segment to maximize the marginal
gain of each segment. Subsequently, we propose a temporal-guided DPC algorithm
that jointly models inter-frame uniqueness and intra-frame diversity, thereby
maximizing the marginal gain of each token. By combining both stages, MMG-Vid
can maximize the utilization of the limited token budget, significantly
improving efficiency while maintaining strong performance. Extensive
experiments demonstrate that MMG-Vid can maintain over 99.5% of the original
performance, while effectively reducing 75% visual tokens and accelerating the
prefilling stage by 3.9x on LLaVA-OneVision-7B. Code will be released soon.

</details>


### [95] [CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification](https://arxiv.org/abs/2508.21046)
*Wei Li,Renshan Zhang,Rui Shao,Jie He,Liqiang Nie*

Main category: cs.CV

TL;DR: CogVLA 是一套高效且性能优异的视觉-语言-动作框架，可在减少训练成本和推理延迟的同时实现最先进的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言-动作模型需要大量后期训练，导致计算开销较高，限制了其扩展性和部署能力。

Method: CogVLA 提出了一种以认知对齐为核心的三阶段架构：1) 利用指令驱动的编码器-FILM 聚合路由实现指令感知的视觉编码；2) 通过语言模型指导的修剪路由实现关注与操作意图的融合；3) 使用视觉-语言-动作耦合注意力确保精确和连贯的动作生成。

Result: 在 LIBERO 基准和实际机器人任务上的实验表明，CogVLA 成功率分别达到 97.4% 和 70%，训练成本减少 2.5 倍，推理延迟降低 2.8 倍。

Conclusion: CogVLA 显著提升了视觉、语言与动作融合模型的效率与性能，具有广泛的应用潜力，并且已开源。

Abstract: Recent Vision-Language-Action (VLA) models built on pre-trained
Vision-Language Models (VLMs) require extensive post-training, resulting in
high computational overhead that limits scalability and deployment.We propose
CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages
instruction-driven routing and sparsification to improve both efficiency and
performance. CogVLA draws inspiration from human multimodal coordination and
introduces a 3-stage progressive architecture. 1) Encoder-FiLM based
Aggregation Routing (EFA-Routing) injects instruction information into the
vision encoder to selectively aggregate and compress dual-stream visual tokens,
forming a instruction-aware latent representation. 2) Building upon this
compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)
introduces action intent into the language model by pruning
instruction-irrelevant visually grounded tokens, thereby achieving token-level
sparsity. 3) To ensure that compressed perception inputs can still support
accurate and coherent action generation, we introduce V-L-A Coupled Attention
(CAtten), which combines causal vision-language attention with bidirectional
action parallel decoding. Extensive experiments on the LIBERO benchmark and
real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art
performance with success rates of 97.4% and 70.0%, respectively, while reducing
training costs by 2.5-fold and decreasing inference latency by 2.8-fold
compared to OpenVLA. CogVLA is open-sourced and publicly available at
https://github.com/JiuTian-VL/CogVLA.

</details>


### [96] [Multi-View 3D Point Tracking](https://arxiv.org/abs/2508.21060)
*Frano Rajič,Haofei Xu,Marko Mihajlovic,Siyuan Li,Irem Demir,Emircan Gündoğdu,Lei Ke,Sergey Prokudin,Marc Pollefeys,Siyu Tang*

Main category: cs.CV

TL;DR: 提出一种数据驱动的多视角3D点追踪器，可使用少量相机（如四台）在动态场景中实现稳健和准确的实时追踪，而无需繁琐的逐序列优化。


<details>
  <summary>Details</summary>
Motivation: 现有单目追踪器在深度模糊和遮挡情况下表现不佳，而现有多相机方法需要大量相机和复杂优化，无法满足实际需求。因此，提出一种仅需少量相机即可高效运行的3D追踪方法成为关键。

Method: 使用已知的相机位姿和多视角深度信息，通过将多视角特征融合为统一点云，并结合k近邻相关性和基于Transformer的更新机制，预测可靠的3D对应关系，即使在遮挡情况下也能保持精准。

Result: 在Kubric合成数据集上训练后，在Panoptic Studio和DexYCB真实数据基准上测试，分别取得了3.1厘米和2.0厘米的中位轨迹误差，展现了良好的泛化性能。

Conclusion: 该方法为多视角3D追踪研究树立了新标准，并提供了实际应用的工具，同时支持从1到8个视角的多样化相机设置以及不同视频长度。

Abstract: We introduce the first data-driven multi-view 3D point tracker, designed to
track arbitrary points in dynamic scenes using multiple camera views. Unlike
existing monocular trackers, which struggle with depth ambiguities and
occlusion, or prior multi-camera methods that require over 20 cameras and
tedious per-sequence optimization, our feed-forward model directly predicts 3D
correspondences using a practical number of cameras (e.g., four), enabling
robust and accurate online tracking. Given known camera poses and either
sensor-based or estimated multi-view depth, our tracker fuses multi-view
features into a unified point cloud and applies k-nearest-neighbors correlation
alongside a transformer-based update to reliably estimate long-range 3D
correspondences, even under occlusion. We train on 5K synthetic multi-view
Kubric sequences and evaluate on two real-world benchmarks: Panoptic Studio and
DexYCB, achieving median trajectory errors of 3.1 cm and 2.0 cm, respectively.
Our method generalizes well to diverse camera setups of 1-8 views with varying
vantage points and video lengths of 24-150 frames. By releasing our tracker
alongside training and evaluation datasets, we aim to set a new standard for
multi-view 3D tracking research and provide a practical tool for real-world
applications. Project page available at https://ethz-vlg.github.io/mvtracker.

</details>


### [97] [OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning](https://arxiv.org/abs/2508.21066)
*Yuan Gong,Xionghui Wang,Jie Wu,Shiyin Wang,Yitong Wang,Xinglong Wu*

Main category: cs.CV

TL;DR: OneReward 是一种统一的强化学习框架，用于在多个任务和评估标准下，增强生成模型的能力，仅使用一个统一的奖励机制。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于任务特定的监督微调，限制了模型的泛化能力和训练效率。OneReward 提出了一种解决方案，通过单一的视觉语言模型作为生成奖励，提升多任务生成性能。

Method: 利用一个视觉语言模型作为奖励模型，通过多任务强化学习直接在预训练模型上训练，省去了任务特定监督微调的步骤。它针对不同的子任务，如图像填充、扩展、对象移除和文本渲染。

Result: 实验显示，OneReward 的统一编辑模型在各个评估维度上超越了商业和开源竞争对手，包括 Ideogram、Adobe Photoshop 和 FLUX Fill [Pro]。

Conclusion: OneReward 提供了一种在多任务生成情境下高效无任务特定微调的解决方案，展示出优越的生成性能。此外，代码和模型已开源。

Abstract: In this paper, we introduce OneReward, a unified reinforcement learning
framework that enhances the model's generative capabilities across multiple
tasks under different evaluation criteria using only \textit{One Reward} model.
By employing a single vision-language model (VLM) as the generative reward
model, which can distinguish the winner and loser for a given task and a given
evaluation criterion, it can be effectively applied to multi-task generation
models, particularly in contexts with varied data and diverse task objectives.
We utilize OneReward for mask-guided image generation, which can be further
divided into several sub-tasks such as image fill, image extend, object
removal, and text rendering, involving a binary mask as the edit area. Although
these domain-specific tasks share same conditioning paradigm, they differ
significantly in underlying data distributions and evaluation metrics. Existing
methods often rely on task-specific supervised fine-tuning (SFT), which limits
generalization and training efficiency. Building on OneReward, we develop
Seedream 3.0 Fill, a mask-guided generation model trained via multi-task
reinforcement learning directly on a pre-trained base model, eliminating the
need for task-specific SFT. Experimental results demonstrate that our unified
edit model consistently outperforms both commercial and open-source
competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across
multiple evaluation dimensions. Code and model are available at:
https://one-reward.github.io

</details>


### [98] [First-Place Solution to NeurIPS 2024 Invisible Watermark Removal Challenge](https://arxiv.org/abs/2508.21072)
*Fahad Shamshad,Tameem Bakr,Yahia Shaaban,Noor Hussein,Karthik Nandakumar,Nils Lukas*

Main category: cs.CV

TL;DR: 该论文展示了一种有效的水印去除方法，在NeurIPS 2024挑战赛中获胜，通过适应性攻击和图像级别的优化，实现高效的水印去除。


<details>
  <summary>Details</summary>
Motivation: 探索现有水印方法在面对不同程度攻击者知识情况下的鲁棒性，并通过挑战赛推动水印技术的发展。

Method: 对于浅灰盒轨道，采用基于VAE的自适应逃避攻击；对于黑盒轨道，采用频域或空间域的图像聚类，并使用增强的扩散模型和ChatGPT生成的语义提示进行去水印。

Result: 实验证明方法可有效去除95.7%的水印，同时对影像质量几乎没有影响。

Conclusion: 提出的攻击方法成功挑战了现有水印的鲁棒性性能，强调需要更加健壮的水印技术。

Abstract: Content watermarking is an important tool for the authentication and
copyright protection of digital media. However, it is unclear whether existing
watermarks are robust against adversarial attacks. We present the winning
solution to the NeurIPS 2024 Erasing the Invisible challenge, which
stress-tests watermark robustness under varying degrees of adversary knowledge.
The challenge consisted of two tracks: a black-box and beige-box track,
depending on whether the adversary knows which watermarking method was used by
the provider. For the beige-box track, we leverage an adaptive VAE-based
evasion attack, with a test-time optimization and color-contrast restoration in
CIELAB space to preserve the image's quality. For the black-box track, we first
cluster images based on their artifacts in the spatial or frequency-domain.
Then, we apply image-to-image diffusion models with controlled noise injection
and semantic priors from ChatGPT-generated captions to each cluster with
optimized parameter settings. Empirical evaluations demonstrate that our method
successfully achieves near-perfect watermark removal (95.7%) with negligible
impact on the residual image's quality. We hope that our attacks inspire the
development of more robust image watermarking methods.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [99] [Social Bias in Multilingual Language Models: A Survey](https://arxiv.org/abs/2508.20201)
*Lance Calvin Lim Gamboa,Yue Feng,Mark Lee*

Main category: cs.CL

TL;DR: 该研究分析了关于多语言预训练模型中社会偏见的评估与缓解方法的最新研究，探讨语言多样性与文化意识的影响，并指出当前方法设计中的不足之处。


<details>
  <summary>Details</summary>
Motivation: 分析多语言及非英语环境中，预训练模型社会偏见的表现，找出研究中的盲点，并提升该领域的包容性与文化适应性。

Method: 对已有研究进行系统性综述，着重分析语言多样性、文化意识，以及评估与缓解偏见的技术方法。

Result: 揭示了当前研究在语言偏好和多语言实验方面的局限性，并总结了跨语言和文化适应中的常见问题与解决方案。

Conclusion: 未来研究应更加关注多语言偏见文献的包容性、文化适应性，以及与最新NLP进展的对齐。

Abstract: Pretrained multilingual models exhibit the same social bias as models
processing English texts. This systematic review analyzes emerging research
that extends bias evaluation and mitigation approaches into multilingual and
non-English contexts. We examine these studies with respect to linguistic
diversity, cultural awareness, and their choice of evaluation metrics and
mitigation techniques. Our survey illuminates gaps in the field's dominant
methodological design choices (e.g., preference for certain languages, scarcity
of multilingual mitigation experiments) while cataloging common issues
encountered and solutions implemented in adapting bias benchmarks across
languages and cultures. Drawing from the implications of our findings, we chart
directions for future research that can reinforce the multilingual bias
literature's inclusivity, cross-cultural appropriateness, and alignment with
state-of-the-art NLP advancements.

</details>


### [100] [Prompting Strategies for Language Model-Based Item Generation in K-12 Education: Bridging the Gap Between Small and Large Language Models](https://arxiv.org/abs/2508.20217)
*Mohammad Amini,Babak Ahmadi,Xiaomeng Xiong,Yilin Zhang,Christopher Qiao*

Main category: cs.CL

TL;DR: 本研究旨在通过语言模型生成多选题，用以评估形态学学习，以降低人工开发测验的成本和不一致性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是提升语言模型在有限数据条件下生成高质量教育评估内容的能力。

Method: 采用两种方法：1. 比较微调的中型语言模型(Gemma, 2B)与未调优的大型模型(GPT-3.5, 175B)；2. 评估七种结构性提示策略，例如零样本、多样本、思想链条等。

Result: 结构化提示，特别是结合思想链条与顺序设计的策略，显著优化了中型模型Gemma的生成质量。同时，Gemma整体表现优于GPT-3.5的零样本生成。

Conclusion: 通过结构化提示和微调技术，可在有限数据条件下提升中型语言模型的性能，研究提出了一种开发和验证语言评估内容的实用可扩展工作流程。

Abstract: This study explores automatic generation (AIG) using language models to
create multiple choice questions (MCQs) for morphological assessment, aiming to
reduce the cost and inconsistency of manual test development. The study used a
two-fold approach. First, we compared a fine-tuned medium model (Gemma, 2B)
with a larger untuned one (GPT-3.5, 175B). Second, we evaluated seven
structured prompting strategies, including zero-shot, few-shot,
chain-of-thought, role-based, sequential, and combinations. Generated items
were assessed using automated metrics and expert scoring across five
dimensions. We also used GPT-4.1, trained on expert-rated samples, to simulate
human scoring at scale. Results show that structured prompting, especially
strategies combining chain-of-thought and sequential design, significantly
improved Gemma's outputs. Gemma generally produced more construct-aligned and
instructionally appropriate items than GPT-3.5's zero-shot responses, with
prompt design playing a key role in mid-size model performance. This study
demonstrates that structured prompting and efficient fine-tuning can enhance
midsized models for AIG under limited data conditions. We highlight the value
of combining automated metrics, expert judgment, and large-model simulation to
ensure alignment with assessment goals. The proposed workflow offers a
practical and scalable way to develop and validate language assessment items
for K-12.

</details>


### [101] [Integrating SystemC TLM into FMI 3.0 Co-Simulations with an Open-Source Approach](https://arxiv.org/abs/2508.20223)
*Andrei Mihai Albu,Giovanni Pollo,Alessio Burrello,Daniele Jahier Pagliari,Cristian Tesconi,Alessandra Neri,Dario Soldi,Fabio Autieri,Sara Vinco*

Main category: cs.CL

TL;DR: 本文提出了一种开源方法，通过将SystemC TLM组件封装为FMI3.0协同模拟功能单元（FMU），实现了不同仿真环境间的无缝集成。


<details>
  <summary>Details</summary>
Motivation: 随着汽车等领域中网络物理系统复杂性的增加，对高效建模和跨域协同仿真技术的需求不断攀升。然而，SystemC TLM与其他工程领域模型间的有限互操作性带来了整合挑战。

Method: 将SystemC TLM模型封装为符合FMI3.0标准的FMU，并开发了一套轻量级的开源工具链，解决时间同步和数据交换等技术难题。

Result: 通过案例研究验证了所提出方法的可行性和有效性，这种方法能够支持异构仿真环境下的标准化集成。

Conclusion: 本文提供了一种开源的、标准化的解决方案，有助于跨域协作和建模仿真，满足网络物理系统日益增长的复杂性需求。

Abstract: The growing complexity of cyber-physical systems, particularly in automotive
applications, has increased the demand for efficient modeling and cross-domain
co-simulation techniques. While SystemC Transaction-Level Modeling (TLM)
enables effective hardware/software co-design, its limited interoperability
with models from other engineering domains poses integration challenges. This
paper presents a fully open-source methodology for integrating SystemC TLM
models into Functional Mock-up Interface (FMI)-based co-simulation workflows.
By encapsulating SystemC TLM components as FMI 3.0 Co Simulation Functional
Mock-up Units (FMUs), the proposed approach facilitates seamless, standardized
integration across heterogeneous simulation environments. We introduce a
lightweight open-source toolchain, address key technical challenges such as
time synchronization and data exchange, and demonstrate the feasibility and
effectiveness of the integration through representative case studies.

</details>


### [102] [Can Compact Language Models Search Like Agents? Distillation-Guided Policy Optimization for Preserving Agentic RAG Capabilities](https://arxiv.org/abs/2508.20324)
*Rikuto Kotoge,Mai Nishimura,Jiaxin Ma*

Main category: cs.CL

TL;DR: 该研究提出了一种针对小型语言模型增强推理和规划能力的方法——DGPO，可以在资源受限情况下实现复杂的代理式任务生成能力，并通过引入ARC指标进行评估。


<details>
  <summary>Details</summary>
Motivation: 由于小型语言模型在推理能力上的限制，导致奖励稀疏，训练不稳定，研究旨在解决这个问题并提升小型模型的性能。

Method: 提出了一种名为DGPO的策略优化方法，通过教师示例的冷启动初始化和持续教师指导解决训练中的困难。同时引入ARC指标进行细粒度性能评估。

Result: 实验结果表明，DGPO方法不仅能够使小型模型达到复杂的代理式行为能力，还在某些情况下超越了原本更大的教师模型。

Conclusion: DGPO方法有效地让小型语言模型在计算资源受限的环境中实现高水平的代理行为，表明小型模型的潜力可以通过优化策略得到显著的提升。

Abstract: Reinforcement Learning has emerged as a post-training approach to elicit
agentic RAG behaviors such as search and planning from language models.
However, compact language models (e.g., 0.5B parameters) struggle due to poor
reasoning ability, resulting in sparse rewards and unstable training. To
overcome these difficulties, we propose Distillation-Guided Policy Optimization
(DGPO), which addresses the challenges through cold-start initialization from
teacher demonstrations and continuous teacher guidance during policy
optimization. To systematically evaluate our approach, we introduce Agentic RAG
Capabilities (ARC), a fine-grained metric analyzing reasoning, search
coordination, and response synthesis. Comprehensive experiments demonstrate
that DGPO enables compact models to achieve sophisticated agentic search
behaviors, even outperforming the larger teacher model in some cases. DGPO
makes agentic RAG feasible in computing resource-constrained environments.

</details>


### [103] [GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs](https://arxiv.org/abs/2508.20325)
*Haibo Jin,Ruoxi Chen,Peiyan Zhang,Andy Zhou,Yang Zhang,Haohan Wang*

Main category: cs.CL

TL;DR: 本文提出GUARD方法，通过自动生成违反指南的问题，测试大型语言模型（LLMs）对政府颁布的AI伦理指南的合规性，包括检测其内置安全机制的潜在破坏场景，并提供合规性报告。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的广泛应用，其可能生成有害响应的问题引发了社会和监管关注，而现行的伦理指南难以转化为具体的测试问题，存在合规性验证的实践空白。

Method: 提出GUARD方法，通过自动生成问题测试是否违反指导方针；并引入“jailbreak”诊断（GUARD-JD），模拟诱导场景以发现潜在违规。最终生成合规性报告，阐明模型的遵循程度与问题。

Result: 实验证明，GUARD对7种不同LLMs（包括GPT、Claude等）测试有效。涵盖三项政府指南的合规性验证及“jailbreak”诊断。同时，其方法还可扩展至支持多模态模型的诊断。

Conclusion: GUARD有效填补了伦理指南与实际合规性验证之间的鸿沟，并有助于构建更可靠的大型语言模型应用。

Abstract: As Large Language Models become increasingly integral to various domains,
their potential to generate harmful responses has prompted significant societal
and regulatory concerns. In response, governments have issued ethics guidelines
to promote the development of trustworthy AI. However, these guidelines are
typically high-level demands for developers and testers, leaving a gap in
translating them into actionable testing questions to verify LLM compliance.
  To address this challenge, we introduce GUARD (\textbf{G}uideline
\textbf{U}pholding Test through \textbf{A}daptive \textbf{R}ole-play and
Jailbreak \textbf{D}iagnostics), a testing method designed to operationalize
guidelines into specific guideline-violating questions that assess LLM
adherence. To implement this, GUARD uses automated generation of
guideline-violating questions based on government-issued guidelines, thereby
testing whether responses comply with these guidelines. When responses directly
violate guidelines, GUARD reports inconsistencies. Furthermore, for responses
that do not directly violate guidelines, GUARD integrates the concept of
``jailbreaks'' to diagnostics, named GUARD-JD, which creates scenarios that
provoke unethical or guideline-violating responses, effectively identifying
potential scenarios that could bypass built-in safety mechanisms. Our method
finally culminates in a compliance report, delineating the extent of adherence
and highlighting any violations.
  We have empirically validated the effectiveness of GUARD on seven LLMs,
including Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4,
GPT-4o, and Claude-3.7, by testing compliance under three government-issued
guidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can
transfer jailbreak diagnostics to vision-language models, demonstrating its
usage in promoting reliable LLM-based applications.

</details>


### [104] [Joint Enhancement of Relational Reasoning for Long-Context LLMs](https://arxiv.org/abs/2508.20351)
*Zhirui Chen,Wei Shen,Jiashui Huang,Ling Shao*

Main category: cs.CL

TL;DR: 本文提出了JERR框架，通过图形推理提升大语言模型的长上下文理解能力，优化任务透明度并减少幻觉生成。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型因内存限制和上下文处理能力不足，在长文本理解和复杂推理任务中表现有限。此外，这些模型透明度低且容易生成幻觉。

Method: JERR框架整合了：1. 文本分块策略提取信息概要；2. 构建有向无环图（DAG）消除冗余；3. 应用蒙特卡洛树搜索（MCTS）支持复杂推理路径导航。

Result: 实验结果表明，JERR框架在ROUGE和F1等指标上均优于所有基线方法，并在LLM-Rater评估中取得最高分。

Conclusion: JERR框架有效提升了大语言模型的长文本理解能力及复杂推理任务的表现，并改善了模型输出的准确性和透明度。

Abstract: Despite significant progress, large language models (LLMs) still struggle
with long contexts due to memory limitations and their inability to tackle
complex and long-context tasks. Additionally, LLMs often suffer from a lack of
transparency and are prone to producing hallucinations. To address these
challenges, we propose \textbf{JERR}, a novel framework designed to enhance
long-context comprehension via graph-based reasoning in LLMs. JERR integrates
three key components: synopsis extraction, graph construction, and relational
reasoning. First, synopsis is extracted by chunking text strategically,
allowing the model to summarize and understand information more efficiently.
Second, we build a directed acyclic graph (DAG) to resolve redundancy, ensuring
logical consistency and clarity. Finally, we incorporate Monte Carlo Tree
Search (MCTS) to help the model navigate complex reasoning paths, ensuring more
accurate and interpretable outputs. This framework provides a novel solution
that enables LLMs to handle extended contexts and complex reasoning tasks with
improved reliability and transparency. Experimental results show that JERR
consistently outperforms all baselines on the ROUGE and F1 metrics, achieving
the highest scores on the LLM-Rater evaluation.

</details>


### [105] [Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems](https://arxiv.org/abs/2508.20373)
*Yuyao Wang,Bowen Liu,Jianheng Tang,Nuo Chen,Yuhan Li,Qifan Zhang,Jia Li*

Main category: cs.CL

TL;DR: 本研究提出通过NP-hard图问题提升大语言模型的长连想推理能力，主要采用两阶段的后训练框架，实现了跨领域的广泛泛化与推理效率的提升。


<details>
  <summary>Details</summary>
Motivation: 目前长连想推理能力的构建需要高质量且人工定制的数据集，成本高且不够具备可扩展性。研究动机是寻找一种具备扩展性的替代方法。

Method: 设计了一个两阶段的后训练框架：1. 基于拒绝采样的NP-hard图实例进行监督微调，提升推理深度；2. 使用带精细化奖励设计的强化学习，优化推理效率。

Result: 提出的Graph-R1-7B模型在数学、编程、STEM和逻辑等领域展现了强泛化能力，并在NP-hard图问题上的精确和推理效率超越了QwQ-32B。

Conclusion: NP-hard图问题可作为一种有效且可扩展的资源，推动大语言模型的长连想推理能力发展，并为后训练研究打开了新的领域。

Abstract: Reasoning Large Language Models (RLLMs) have recently achieved remarkable
progress on complex reasoning tasks, largely enabled by their long
chain-of-thought (Long CoT) capabilities. However, developing these Long CoT
behaviors relies heavily on post-training with high-quality datasets, which are
typically costly and human-curated (e.g., mathematics and code), leaving
scalable alternatives unexplored. In this work, we introduce NP-hard (NPH)
graph problems as a novel synthetic training corpus, as they inherently require
deep reasoning, extensive exploration, and reflective strategies, which are
core characteristics of Long CoT reasoning. Building on this insight, we
develop a two-stage post-training framework: (i) Long CoT Supervised
Fine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially
enhances reasoning depth, and (ii) Reinforcement Learning (RL) with a
fine-grained reward design, which sharpens reasoning efficiency. Our flagship
model, Graph-R1-7B, demonstrates strong generalization across mathematics,
coding, STEM, and logic, and surpasses QwQ-32B on NPH graph problems in both
accuracy and reasoning efficiency. These results position NPH graph problems as
an effective and scalable resource for advancing Long CoT reasoning in LLMs,
opening a new frontier for LLM post-training. Our implementation is available
at https://github.com/Graph-Reasoner/Graph-R1, with models and datasets hosted
in our Hugging Face collection HKUST-DSAIL/Graph-R1.

</details>


### [106] [CAPE: Context-Aware Personality Evaluation Framework for Large Language Models](https://arxiv.org/abs/2508.20385)
*Jivnesh Sandhan,Fei Cheng,Tushar Sandhan,Yugo Murawaki*

Main category: cs.CL

TL;DR: 本文提出了CAPE框架，用于评估大语言模型（LLMs）的上下文依赖性人格特质。通过分析7种LLM，发现上下文学习能提高一致性，但也引发人格偏移。


<details>
  <summary>Details</summary>
Motivation: 传统的心理测验在评估人类时忽略了上下文，而应用于LLMs时需要解决其在真实对话中的上下文依赖性。

Method: 提出Context-Aware Personality Evaluation (CAPE)框架，并设计了新的一致性衡量指标，以量化对话上下文对模型人格特质的影响。

Result: 上下文学习提高了LLMs的一致性，但造成了人格偏移。GPT模型展现出固有人格特质，而Gemini-1.5-Flash与Llama-8B对上下文高度敏感。

Conclusion: 通过CAPE框架，研究表明上下文依赖性能改进模型一致性，同时更符合人类判断，具备应用潜力。

Abstract: Psychometric tests, traditionally used to assess humans, are now being
applied to Large Language Models (LLMs) to evaluate their behavioral traits.
However, existing studies follow a context-free approach, answering each
question in isolation to avoid contextual influence. We term this the Disney
World test, an artificial setting that ignores real-world applications, where
conversational history shapes responses. To bridge this gap, we propose the
first Context-Aware Personality Evaluation (CAPE) framework for LLMs,
incorporating prior conversational interactions. To thoroughly analyze the
influence of context, we introduce novel metrics to quantify the consistency of
LLM responses, a fundamental trait in human behavior.
  Our exhaustive experiments on 7 LLMs reveal that conversational history
enhances response consistency via in-context learning but also induces
personality shifts, with GPT-3.5-Turbo and GPT-4-Turbo exhibiting extreme
deviations. While GPT models are robust to question ordering, Gemini-1.5-Flash
and Llama-8B display significant sensitivity. Moreover, GPT models response
stem from their intrinsic personality traits as well as prior interactions,
whereas Gemini-1.5-Flash and Llama--8B heavily depend on prior interactions.
Finally, applying our framework to Role Playing Agents (RPAs) shows
context-dependent personality shifts improve response consistency and better
align with human judgments. Our code and datasets are publicly available at:
https://github.com/jivnesh/CAPE

</details>


### [107] [Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction](https://arxiv.org/abs/2508.20395)
*Xu Guo*

Main category: cs.CL

TL;DR: 本文探讨了生成推理步骤用于提高LLMs回答准确性的问题，并研究在生成过程中如何预测推理步骤的效用以避免冗余。


<details>
  <summary>Details</summary>
Motivation: 尽管通过生成中间推理步骤能够提升LLMs输出准确性，但推理过程的效用对答案正确性的贡献尚未充分研究。希望通过建立方法预测推理步骤的有效性，提高计算效率并避免分散注意力。

Method: 以MATH数据集为实验对象，利用Qwen2.5-32B与GPT-4o生成推理链，并通过Qwen3-8B预测这些链对最终答案的贡献度，使用条件熵测量模型对答案的不确定性，对逐步扩展的上下文评估推理效用。

Result: 研究表明，当推理步骤的条件熵逐步减少时，往往与正确答案相关，而平坦或增加的熵则对应错误答案。错误推理路径通常比正确路径更长，且冗长的推理并不总能带来更好的结果。

Conclusion: 研究为未来设计有效的推理管道提供了基础，实现早期检测并避免无效的推理路径，提升模型总体性能。

Abstract: Recent advancements in large language models (LLMs) often rely on generating
intermediate reasoning steps to enhance accuracy. However, little work has
examined how reasoning utility contributes to the final answer's correctness.
Due to the stochastic nature of autoregressive generation, generating more
context does not guarantee increased confidence in the answer. If we could
predict, during generation, whether a reasoning step will be useful, we could
stop early or prune ineffective steps, avoiding distractions in the final
decision.
  We present an oracle study on MATH dataset, using Qwen2.5-32B and GPT-4o to
generate reasoning chains, and then employing a separate model (Qwen3-8B) to
quantify the utility of these chains for final accuracy. Specifically, we
measure the model's uncertainty on the answer span Y at each reasoning step
using conditional entropy (expected negative log-likelihood over the
vocabulary) with context expanding step by step. Our results show a clear
pattern: conditional entropy that decreases over steps is strongly associated
with correct answers, whereas flat or increasing entropy often results in wrong
answers. We also corroborate that incorrect reasoning paths tend to be longer
than correct ones, suggesting that longer reasoning does not necessarily yield
better outcomes. These findings serve as a foundation to inspire future work on
designing efficient reasoning pipelines that detect and avoid unproductive
reasoning early.

</details>


### [108] [UI-Bench: A Benchmark for Evaluating Design Capabilities of AI Text-to-App Tools](https://arxiv.org/abs/2508.20410)
*Sam Jung,Agustin Garcinuno,Spencer Mateega*

Main category: cs.CL

TL;DR: 提出了首个用于评估AI文本生成应用工具视觉卓越性的UI-Bench基准，包括10种工具、30个提示和300个生成的站点。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏公开基准验证AI文本生成工具生成高质量应用的能力，这一研究旨在填补该空白。

Method: 通过TrueSkill模型进行专家两两对比评价，建立了一套大规模评估AI文本生成工具视觉优越性的标准。

Result: UI-Bench通过4000多次专家评估，展示了各系统的排名并提供了校准的置信区间。

Conclusion: UI-Bench为推进AI驱动的网页设计设立了可重复使用的标准，公开了提示集、评估框架和排行榜。

Abstract: AI text-to-app tools promise high quality applications and websites in
minutes, yet no public benchmark rigorously verifies those claims. We introduce
UI-Bench, the first large-scale benchmark that evaluates visual excellence
across competing AI text-to-app tools through expert pairwise comparison.
Spanning 10 tools, 30 prompts, 300 generated sites, and \textit{4000+} expert
judgments, UI-Bench ranks systems with a TrueSkill-derived model that yields
calibrated confidence intervals. UI-Bench establishes a reproducible standard
for advancing AI-driven web design. We release (i) the complete prompt set,
(ii) an open-source evaluation framework, and (iii) a public leaderboard. The
generated sites rated by participants will be released soon. View the UI-Bench
leaderboard at https://uibench.ai/leaderboard.

</details>


### [109] [DentalBench: Benchmarking and Advancing LLMs Capability for Bilingual Dentistry Understanding](https://arxiv.org/abs/2508.20416)
*Hengchuan Zhu,Yihuan Xu,Yichen Li,Zijie Meng,Zuozhu Liu*

Main category: cs.CL

TL;DR: 论文介绍了DentalBench，这是第一个针对牙科领域的大型语言模型(LMM)的综合双语基准，用于评估和改进牙科领域的LMM能力，包括DentalQA问答基准和DentalCorpus高质量语料库。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在通用医疗领域表现良好，但在需要更深领域知识的牙科等专业医疗领域的潜力仍然缺乏探索，原因在于缺乏针对性的评估资源。

Method: 该研究创建了DentalBench，包含两个主要组成部分：1.DentalQA：一个英文-中文的问答基准，包含36,597个问题，覆盖4种任务和16个牙科子领域；2.DentalCorpus：一个包含3.37亿个标记的大规模高质量牙科领域语料库，支持监督微调(SFT)和检索增强生成(RAG)。

Result: 评估发现14种不同类型的大型语言模型在任务类型和语言上存在显著的性能差距。进一步实验表明，通过领域适配，模型在知识密集型和术语相关任务上的性能显著提高，尤其是在Qwen-2.5-3B模型中的表现。

Conclusion: 研究表明，领域特定基准对开发可信和有效的医疗应用LMM至关重要，领域适配可以显著提升模型性能。

Abstract: Recent advances in large language models (LLMs) and medical LLMs (Med-LLMs)
have demonstrated strong performance on general medical benchmarks. However,
their capabilities in specialized medical fields, such as dentistry which
require deeper domain-specific knowledge, remain underexplored due to the lack
of targeted evaluation resources. In this paper, we introduce DentalBench, the
first comprehensive bilingual benchmark designed to evaluate and advance LLMs
in the dental domain. DentalBench consists of two main components: DentalQA, an
English-Chinese question-answering (QA) benchmark with 36,597 questions
spanning 4 tasks and 16 dental subfields; and DentalCorpus, a large-scale,
high-quality corpus with 337.35 million tokens curated for dental domain
adaptation, supporting both supervised fine-tuning (SFT) and
retrieval-augmented generation (RAG). We evaluate 14 LLMs, covering
proprietary, open-source, and medical-specific models, and reveal significant
performance gaps across task types and languages. Further experiments with
Qwen-2.5-3B demonstrate that domain adaptation substantially improves model
performance, particularly on knowledge-intensive and terminology-focused tasks,
and highlight the importance of domain-specific benchmarks for developing
trustworthy and effective LLMs tailored to healthcare applications.

</details>


### [110] [KG-CQR: Leveraging Structured Relation Representations in Knowledge Graphs for Contextual Query Retrieval](https://arxiv.org/abs/2508.20417)
*Chi Minh Bui,Ngoc Mai Thieu,Van Vinh Nguyen,Json J. Jung,Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: 本文提出了KG-CQR框架，通过引入语料中心知识图谱，增强复杂查询的上下文表示，从而改进RAG系统的检索阶段。在多数据集实验中表现出优于基线模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统在检索阶段存在上下文丢失问题，本文尝试通过知识图谱丰富查询上下文表示以提升检索效果。

Method: 通过引入KG-CQR框架，利用知识图谱完成子图提取、子图补全以及上下文生成模块，以语义丰富的方式增强查询上下文。框架支持多种规模LLMs且无需额外训练。

Result: 在RAGBench和MultiHop-RAG数据集的评估中，KG-CQR在mAP指标上提高了4%-6%，在Recall@25上提高了2%-3%。此外，在复杂推理任务中表现优于现有基线方法。

Conclusion: KG-CQR框架通过利用知识图谱有效提升了RAG系统在多种难度任务中的检索效果，展示了其实用性和通用性。

Abstract: The integration of knowledge graphs (KGs) with large language models (LLMs)
offers significant potential to improve the retrieval phase of
retrieval-augmented generation (RAG) systems. In this study, we propose KG-CQR,
a novel framework for Contextual Query Retrieval (CQR) that enhances the
retrieval phase by enriching the contextual representation of complex input
queries using a corpus-centric KG. Unlike existing methods that primarily
address corpus-level context loss, KG-CQR focuses on query enrichment through
structured relation representations, extracting and completing relevant KG
subgraphs to generate semantically rich query contexts. Comprising subgraph
extraction, completion, and contextual generation modules, KG-CQR operates as a
model-agnostic pipeline, ensuring scalability across LLMs of varying sizes
without additional training. Experimental results on RAGBench and MultiHop-RAG
datasets demonstrate KG-CQR's superior performance, achieving a 4-6%
improvement in mAP and a 2-3% improvement in Recall@25 over strong baseline
models. Furthermore, evaluations on challenging RAG tasks such as multi-hop
question answering show that, by incorporating KG-CQR, the performance
consistently outperforms the existing baseline in terms of retrieval
effectiveness

</details>


### [111] [CAMB: A comprehensive industrial LLM benchmark on civil aviation maintenance](https://arxiv.org/abs/2508.20420)
*Feng Zhang,Chengjie Pang,Yuehan Zhang,Chenyu Luo*

Main category: cs.CL

TL;DR: 作者提出了一种专门针对民用航空维修领域的大型语言模型（LLMs）评估基准工具，用以评估模型在复杂推理和专业知识方面的表现，并公开该评估工具促进研究发展。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏专门针对民用航空维修领域的大型语言模型评估工具，而现有评估主要集中于数学和代码推理任务，无法有效衡量模型在特定领域的能力。

Method: 开发了一个工业级的评估基准，专注于民用航空维修，从模型能力测量、缺陷定位到目标改进努力（如领域特定微调、RAG优化或提示工程）。同时评价现有RAG系统、矢量嵌入模型与LLMs在该领域的表现。

Result: 实验表明，提出的评估基准工具能够有效评估民用航空维修领域的模型性能，并对现有的模型进行了实践探索与分析，展示其作用。

Conclusion: 新评估基准工具填补了民用航空维修领域模型评估的空白，为后续的领域优化改进提供了坚实基础，并通过开源促进领域研究发展。

Abstract: Civil aviation maintenance is a domain characterized by stringent industry
standards. Within this field, maintenance procedures and troubleshooting
represent critical, knowledge-intensive tasks that require sophisticated
reasoning. To address the lack of specialized evaluation tools for large
language models (LLMs) in this vertical, we propose and develop an
industrial-grade benchmark specifically designed for civil aviation
maintenance. This benchmark serves a dual purpose: It provides a standardized
tool to measure LLM capabilities within civil aviation maintenance, identifying
specific gaps in domain knowledge and complex reasoning. By pinpointing these
deficiencies, the benchmark establishes a foundation for targeted improvement
efforts (e.g., domain-specific fine-tuning, RAG optimization, or specialized
prompt engineering), ultimately facilitating progress toward more intelligent
solutions within civil aviation maintenance. Our work addresses a significant
gap in the current LLM evaluation, which primarily focuses on mathematical and
coding reasoning tasks. In addition, given that Retrieval-Augmented Generation
(RAG) systems are currently the dominant solutions in practical applications ,
we leverage this benchmark to evaluate existing well-known vector embedding
models and LLMs for civil aviation maintenance scenarios. Through experimental
exploration and analysis, we demonstrate the effectiveness of our benchmark in
assessing model performance within this domain, and we open-source this
evaluation benchmark and code to foster further research and
development:https://github.com/CamBenchmark/cambenchmark

</details>


### [112] [Searching the Title of Practical Work of the Informatics Engineering Bachelor Program with the Case Base Reasoning Method](https://arxiv.org/abs/2508.20442)
*Agung Sukrisna Jaya,Osvari Arsalan,Danny Matthew Saputra*

Main category: cs.CL

TL;DR: 本研究提出使用案例推理(CBR)技术，结合TF-IDF和余弦相似度，解决实践工作题目相似性搜索问题。


<details>
  <summary>Details</summary>
Motivation: 提高实践工作题目搜索的准确性和匹配效率。

Method: 采用CBR技术，利用TF-IDF进行词向量化，并用余弦相似度计算相似性得分；对705个实践题目进行测试。

Result: 通过两阶段测试，随机化后的结果与第一阶段相符，验证了匹配结果的稳定性。

Conclusion: 所提出的系统可以准确、稳定地完成题目相似性搜索，适用于实践工作场景。

Abstract: Case Base Reasoning (CBR) is a case solving technique based on experience in
cases that have occurred before with the highest similarity. CBR is used to
search for practical work titles. TF-IDF is applied to process the
vectorization of each practical work title word and Cosine Similarity for the
calculation of similarity values. This system can search either in the form of
titles or keywords. The output of the system is the title of practical work and
the match value of each title. Based on the test results using 705 practical
work titles, testing was carried out with five titles and carried out in two
stages. The first stage searches with existing titles and the second stage
randomizes the title from the first stage. And the results obtained in the
second stage are the same number of titles found and the highest average match
score.

</details>


### [113] [MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers](https://arxiv.org/abs/2508.20453)
*Zhenting Wang,Qi Chang,Hemani Patel,Shashank Biju,Cheng-En Wu,Quan Liu,Aolin Ding,Alireza Rezazadeh,Ankit Shah,Yujia Bao,Eugene Siow*

Main category: cs.CL

TL;DR: MCP-Bench是一种评估大型语言模型在支持工具使用、跨工具协作和多步骤任务规划等能力方面的新基准。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试多局限于显式工具规范和简单任务，无法全面评估语言模型在复杂多步骤任务中的能力，因此需要一个更具挑战性的基准。

Method: 构建基于Model Context Protocol (MCP)的MCP-Bench，连接28个代表性MCP服务器和250个工具，设计了基于多工具配合的真实多步骤任务，提出了覆盖工具理解与应用、任务规划与完成的多层评估框架。

Result: 实验显示，在20个高级模型上使用MCP-Bench进行测试，发现模型在该基准上的表现仍有明显挑战。

Conclusion: MCP-Bench填补了现有基准在测试复杂任务能力方面的空白，是评估高级语言模型新能力的重要工具。

Abstract: We introduce MCP-Bench, a benchmark for evaluating large language models
(LLMs) on realistic, multi-step tasks that demand tool use, cross-tool
coordination, precise parameter control, and planning/reasoning for solving
tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28
representative live MCP servers spanning 250 tools across domains such as
finance, traveling, scientific computing, and academic search. Unlike prior
API-based benchmarks, each MCP server provides a set of complementary tools
designed to work together, enabling the construction of authentic, multi-step
tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability
to retrieve relevant tools from fuzzy instructions without explicit tool names,
plan multi-hop execution trajectories for complex objectives, ground responses
in intermediate tool outputs, and orchestrate cross-domain workflows -
capabilities not adequately evaluated by existing benchmarks that rely on
explicit tool specifications, shallow few-step workflows, and isolated domain
operations. We propose a multi-faceted evaluation framework covering tool-level
schema understanding and usage, trajectory-level planning, and task completion.
Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code
and data: https://github.com/Accenture/mcp-bench.

</details>


### [114] [Prediction of mortality and resource utilization in critical care: a deep learning approach using multimodal electronic health records with natural language processing techniques](https://arxiv.org/abs/2508.20460)
*Yucheng Ruan,Xiang Lan,Daniel J. Tan,Hairil Rizal Abdullah,Mengling Feng*

Main category: cs.CL

TL;DR: 本研究提出了一种结合多模态电子健康记录（EHR）的深度学习框架，用于预测重症监护中的死亡率和资源利用率，展示了其在三个临床任务上的优越性能，同时具备对结构化数据损坏的较强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要集中于结构化EHR数据，忽视了自由文本中的临床信息，且未充分利用结构化数据中的文本潜力。本研究旨在通过多模态整合提升预测精度。

Method: 以两组真实世界数据集为实验基础，开发并评估集成自然语言处理（NLP）的深度学习模型，并进行消融实验和鲁棒性评估。

Result: 在三项临床任务中，该方法在多项评估指标上优于现有最优方法，同时在不同数据损坏率下均表现出优异性能。

Conclusion: 提出的框架是一种有效且准确的深度学习方法，成功整合了多模态EHR并证明数据腐败情况下模型的高鲁棒性。

Abstract: Background Predicting mortality and resource utilization from electronic
health records (EHRs) is challenging yet crucial for optimizing patient
outcomes and managing costs in intensive care unit (ICU). Existing approaches
predominantly focus on structured EHRs, often ignoring the valuable clinical
insights in free-text notes. Additionally, the potential of textual information
within structured data is not fully leveraged. This study aimed to introduce
and assess a deep learning framework using natural language processing
techniques that integrates multimodal EHRs to predict mortality and resource
utilization in critical care settings. Methods Utilizing two real-world EHR
datasets, we developed and evaluated our model on three clinical tasks with
leading existing methods. We also performed an ablation study on three key
components in our framework: medical prompts, free-texts, and pre-trained
sentence encoder. Furthermore, we assessed the model's robustness against the
corruption in structured EHRs. Results Our experiments on two real-world
datasets across three clinical tasks showed that our proposed model improved
performance metrics by 1.6\%/0.8\% on BACC/AUROC for mortality prediction,
0.5%/2.2% on RMSE/MAE for LOS prediction, 10.9%/11.0% on RMSE/MAE for surgical
duration estimation compared to the best existing methods. It consistently
demonstrated superior performance compared to other baselines across three
tasks at different corruption rates. Conclusions The proposed framework is an
effective and accurate deep learning approach for predicting mortality and
resource utilization in critical care. The study also highlights the success of
using prompt learning with a transformer encoder in analyzing multimodal EHRs.
Importantly, the model showed strong resilience to data corruption within
structured data, especially at high corruption levels.

</details>


### [115] [ConspirED: A Dataset for Cognitive Traits of Conspiracy Theories and Large Language Model Safety](https://arxiv.org/abs/2508.20468)
*Luke Bates,Max Glockner,Preslav Nakov,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文通过引入ConspirED数据集研究了解阴谋论的修辞模式，并评估AI模型对阴谋论内容的鲁棒性及对抗能力。


<details>
  <summary>Details</summary>
Motivation: 阴谋论削弱公众对科学和机构的信任，且难以通过反驳消除。AI生成的错误信息愈发复杂，识别这些模式对于干预措施的实施和评估AI脆弱性尤为重要。

Method: 提出ConspirED数据集，其中包含从在线陰谋论文章中选取的多句摘录，并根据CONSPIR认知框架标注阴谋论认知特征。利用此数据集开发计算模型识别阴谋论特性，并评估大型语言/推理模型面对阴谋论输入的表现。

Result: 研究发现大型语言模型对阴谋论内容的生成输出往往与输入模式一致，即便成功应对类似的经过事实核查的错误信息，也依然可能受到阴谋论的干扰。

Conclusion: 阴谋论内容对现有AI系统具有潜在的误导作用，这表明需要进一步研究和开发更鲁棒的模型以有效抵御此类内容的影响。

Abstract: Conspiracy theories erode public trust in science and institutions while
resisting debunking by evolving and absorbing counter-evidence. As AI-generated
misinformation becomes increasingly sophisticated, understanding rhetorical
patterns in conspiratorial content is important for developing interventions
such as targeted prebunking and assessing AI vulnerabilities. We introduce
ConspirED (CONSPIR Evaluation Dataset), which captures the cognitive traits of
conspiratorial ideation in multi-sentence excerpts (80--120 words) from online
conspiracy articles, annotated using the CONSPIR cognitive framework
(Lewandowsky and Cook, 2020). ConspirED is the first dataset of conspiratorial
content annotated for general cognitive traits. Using ConspirED, we (i) develop
computational models that identify conspiratorial traits and determine dominant
traits in text excerpts, and (ii) evaluate large language/reasoning model
(LLM/LRM) robustness to conspiratorial inputs. We find that both are misaligned
by conspiratorial content, producing output that mirrors input reasoning
patterns, even when successfully deflecting comparable fact-checked
misinformation.

</details>


### [116] [Languages Still Left Behind: Toward a Better Multilingual Machine Translation Benchmark](https://arxiv.org/abs/2508.20511)
*Chihiro Taguchi,Seng Mai,Keita Kurabe,Yusuke Sakai,Georgina Agyei,Soudabeh Eslami,David Chiang*

Main category: cs.CL

TL;DR: 研究揭示FLOWERS+基准存在不足之处，包括翻译质量低于标准，语料文化偏向英语且领域单一；强调需改进基准以更接近真实翻译场景。


<details>
  <summary>Details</summary>
Motivation: 当前多语言机器翻译评估基准如FLORES+存在广泛应用，但是否真正适合多语言评估未验证。作者希望揭示其不足之处，驱动更适合实际场景的基准开发。

Method: 分析Asante Twi、日语、Jinghpaw和南阿塞拜疆语四种语言在FLORES+基准上的表现，进行人工质量评估，并设计域相关数据集进行对照实验。

Result: 发现FLORES+中的翻译质量未达到主张的90%标准且存在文化偏差，以命名实体为主的评估容易被简单模型利用，导致BLEU评分并非准确反映翻译质量；域相关数据集上表现好反而在FLORES+上表现差。

Conclusion: 现有FLORES+基准无法完全体现多语言翻译的真实能力，作者建议构建更通用和文化中立的基准以提升评估可靠性。

Abstract: Multilingual machine translation (MT) benchmarks play a central role in
evaluating the capabilities of modern MT systems. Among them, the FLORES+
benchmark is widely used, offering English-to-many translation data for over
200 languages, curated with strict quality control protocols. However, we study
data in four languages (Asante Twi, Japanese, Jinghpaw, and South Azerbaijani)
and uncover critical shortcomings in the benchmark's suitability for truly
multilingual evaluation. Human assessments reveal that many translations fall
below the claimed 90% quality standard, and the annotators report that source
sentences are often too domain-specific and culturally biased toward the
English-speaking world. We further demonstrate that simple heuristics, such as
copying named entities, can yield non-trivial BLEU scores, suggesting
vulnerabilities in the evaluation protocol. Notably, we show that MT models
trained on high-quality, naturalistic data perform poorly on FLORES+ while
achieving significant gains on our domain-relevant evaluation set. Based on
these findings, we advocate for multilingual MT benchmarks that use
domain-general and culturally neutral source texts rely less on named entities,
in order to better reflect real-world translation challenges.

</details>


### [117] [SciTopic: Enhancing Topic Discovery in Scientific Literature through Advanced LLM](https://arxiv.org/abs/2508.20514)
*Pengjiang Li,Zaitian Wang,Xinhao Zhang,Ran Zhang,Lu Jiang,Pengfei Wang,Yuanchun Zhou*

Main category: cs.CL

TL;DR: SciTopic是一种利用大型语言模型(LLMs)改进科学文献主题发现的方法，结合文本编码器和空间优化模块，通过对比损失优化提升主题辨识能力，并在三组真实数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 推动科学文献中的研究主题发现，以帮助研究者识别新兴趋势，探索研究方向并优化信息检索效率。

Method: 构建文本编码器提取文献元数据、标题和摘要信息；设计融合LLM指导的熵采样与三元组任务的空间优化模块；利用对比损失优化模型，将LLM引导的主题相关性融入编码过程以提升辨识力。

Result: 在三个科学文献真实数据集上，SciTopic有效优于现有主题发现方法，展示了更快和更深入的主题洞察能力。

Conclusion: SciTopic能够增强科学主题发现的精确性和效率，为研究人员提供更为精准的知识洞察工具，表现出对科学文本的更全面理解能力。

Abstract: Topic discovery in scientific literature provides valuable insights for
researchers to identify emerging trends and explore new avenues for
investigation, facilitating easier scientific information retrieval. Many
machine learning methods, particularly deep embedding techniques, have been
applied to discover research topics. However, most existing topic discovery
methods rely on word embedding to capture the semantics and lack a
comprehensive understanding of scientific publications, struggling with
complex, high-dimensional text relationships. Inspired by the exceptional
comprehension of textual information by large language models (LLMs), we
propose an advanced topic discovery method enhanced by LLMs to improve
scientific topic identification, namely SciTopic. Specifically, we first build
a textual encoder to capture the content from scientific publications,
including metadata, title, and abstract. Next, we construct a space
optimization module that integrates entropy-based sampling and triplet tasks
guided by LLMs, enhancing the focus on thematic relevance and contextual
intricacies between ambiguous instances. Then, we propose to fine-tune the
textual encoder based on the guidance from the LLMs by optimizing the
contrastive loss of the triplets, forcing the text encoder to better
discriminate instances of different topics. Finally, extensive experiments
conducted on three real-world datasets of scientific publications demonstrate
that SciTopic outperforms the state-of-the-art (SOTA) scientific topic
discovery methods, enabling researchers to gain deeper and faster insights.

</details>


### [118] [Overview of BioASQ 2024: The twelfth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20532)
*Anastasios Nentidis,Georgios Katsimpras,Anastasia Krithara,Salvador Lima-López,Eulàlia Farré-Maduell,Martin Krallinger,Natalia Loukachevitch,Vera Davydova,Elena Tutubalina,Georgios Paliouras*

Main category: cs.CL

TL;DR: 本文概述了BioASQ挑战赛第12版的内容，该挑战是为促进生物医学语义索引和问答系统发展的国际赛事。


<details>
  <summary>Details</summary>
Motivation: 推动大规模生物医学语义索引和问答领域的发展与技术创新。

Method: 包括两项新任务（MultiCardioNER和BIONNE）以及两项既有任务（b和Synergy）的比赛，通过设置多个团队和提交系统评估技术进步。

Result: 共有37支参赛队伍及超过700个系统提交，绝大多数系统表现出色，表现了该领域的持续进展。

Conclusion: 该挑战赛持续推动了生物医学信息处理技术的前沿进展。

Abstract: This is an overview of the twelfth edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2024. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks b and Synergy, and two
new tasks: a) MultiCardioNER on the adaptation of clinical entity detection to
the cardiology domain in a multilingual setting, and b) BIONNE on nested NER in
Russian and English. In this edition of BioASQ, 37 competing teams participated
with more than 700 distinct submissions in total for the four different shared
tasks of the challenge. Similarly to previous editions, most of the
participating systems achieved competitive performance, suggesting the
continuous advancement of the state-of-the-art in the field.

</details>


### [119] [Overview of BioASQ 2025: The Thirteenth BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20554)
*Anastasios Nentidis,Georgios Katsimpras,Anastasia Krithara,Martin Krallinger,Miguel Rodríguez-Ortega,Eduard Rodriguez-López,Natalia Loukachevitch,Andrey Sakhovskiy,Elena Tutubalina,Dimitris Dimitriadis,Grigorios Tsoumakas,George Giannakoulas,Alexandra Bekiaridou,Athanasios Samaras,Giorgio Maria Di Nunzio,Nicola Ferro,Stefano Marchesin,Marco Martinelli,Gianmaria Silvello,Georgios Paliouras*

Main category: cs.CL

TL;DR: 第十三届BioASQ挑战赛在CLEF 2025会议上回顾总结，设有6项任务，有83支团队和超过1000次提交，推动生物医学语义索引和问答的研究进展。


<details>
  <summary>Details</summary>
Motivation: 促进生物医学大规模语义索引和问答领域的研究进展，推动最新技术的竞争与合作。

Method: 设置两个传统任务（Task b和Synergy）和四个新任务（多语言临床总结、嵌套命名实体链接、心脏病学临床编码、肠脑交互信息提取），并邀请研究团队参与挑战。

Result: 共有83支团队参与，总计提交超过1000次，许多系统达到高水平性能，表明领域中技术的持续进步。

Conclusion: 本届BioASQ挑战赛再度验证了国际社区在生物医学领域协作和技术推进的能力，为语义索引与问答系统的发展作出了显著贡献。

Abstract: This is an overview of the thirteenth edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2025. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks, b and Synergy, and four
new tasks: a) Task MultiClinSum on multilingual clinical summarization. b) Task
BioNNE-L on nested named entity linking in Russian and English. c) Task
ELCardioCC on clinical coding in cardiology. d) Task GutBrainIE on gut-brain
interplay information extraction. In this edition of BioASQ, 83 competing teams
participated with more than 1000 distinct submissions in total for the six
different shared tasks of the challenge. Similar to previous editions, several
participating systems achieved competitive performance, indicating the
continuous advancement of the state-of-the-art in the field.

</details>


### [120] [Adaptive Federated Distillation for Multi-Domain Non-IID Textual Data](https://arxiv.org/abs/2508.20557)
*Jiahao Xiao,Jiangming Liu*

Main category: cs.CL

TL;DR: 本文探讨预训练语言模型在任务特定数据的微调中面临的非独立同分布（non-IID）数据挑战，通过隐私保护联邦蒸馏来应对这些问题，提出了一个综合的多域非IID场景基准框架，并设计了一个名为AdaFD的联邦蒸馏框架。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习环境中对非IID数据的研究主要集中于标签多样性，而忽略了在自然语言处理至关重要的语言域多样性，因此有必要设计一个能全面反映真实环境中数据分布的评价框架。

Method: 提出一个综合的多域非IID场景基准框架，并设计了一个适应性联邦蒸馏（AdaFD）方法，能够处理同质和异质环境下的多域非IID挑战。

Result: 实验结果表明，AdaFD框架能够捕获本地客户数据的多样性，并在性能上优于现有方法。

Conclusion: 本文提出的AdaFD模型及其框架有效解决了多域非IID问题，为评估联邦学习框架在真实场景中的表现提供了新的基准。

Abstract: The widespread success of pre-trained language models has established a new
training paradigm, where a global PLM is fine-tuned using task-specific data
from local clients. The local data are highly different from each other and can
not capture the global distribution of the whole data in real world. To address
the challenges of non-IID data in real environments, privacy-preserving
federated distillation has been proposed and highly investigated. However,
previous experimental non-IID scenarios are primarily identified with the label
(output) diversity, without considering the diversity of language domains
(input) that is crucial in natural language processing. In this paper, we
introduce a comprehensive set of multi-domain non-IID scenarios and propose a
unified benchmarking framework that includes diverse data. The benchmark can be
used to evaluate the federated learning framework in a real environment. To
this end, we propose an Adaptive Federated Distillation (AdaFD) framework
designed to address multi-domain non-IID challenges in both homogeneous and
heterogeneous settings. Experimental results demonstrate that our models
capture the diversity of local clients and achieve better performance compared
to the existing works. The code for this paper is available at:
https://github.com/jiahaoxiao1228/AdaFD.

</details>


### [121] [Leveraging Generative Models for Real-Time Query-Driven Text Summarization in Large-Scale Web Search](https://arxiv.org/abs/2508.20559)
*Zeyu Xiong,Yixuan Nan,Li Gao,Hengzhu Tang,Shuaiqiang Wang,Junfeng Wang,Dawei Yin*

Main category: cs.CL

TL;DR: 本文提出了一种应用生成模型的新框架，以解决工业网页搜索中基于查询的实时文本摘要问题。通过模型蒸馏、监督微调和前瞻解码等技术，将轻量级模型转变为特定领域的专家，并在多项指标上超越生产基线。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统模型在基于查询的文本摘要任务中的多级流程信息丢失和对复杂意图理解不足的问题，提出了新的生成模型框架。

Method: 采用生成模型，结合大规模模型蒸馏、监督微调、直接偏好优化和前瞻解码技术，提升了小模型的专业能力使其适应特定领域需求。

Result: 模型在工业相关指标中优于现有生产基线，性能达到行业新标杆，且在低时延和高查询处理能力的条件下表现出色。

Conclusion: 新框架显著提升了基于查询的文本摘要生成性能，同时兼具部署效率，成为在工业语境下的有效解决方案。

Abstract: In the dynamic landscape of large-scale web search, Query-Driven Text
Summarization (QDTS) aims to generate concise and informative summaries from
textual documents based on a given query, which is essential for improving user
engagement and facilitating rapid decision-making. Traditional extractive
summarization models, based primarily on ranking candidate summary segments,
have been the dominant approach in industrial applications. However, these
approaches suffer from two key limitations: 1) The multi-stage pipeline often
introduces cumulative information loss and architectural bottlenecks due to its
weakest component; 2) Traditional models lack sufficient semantic understanding
of both user queries and documents, particularly when dealing with complex
search intents. In this study, we propose a novel framework to pioneer the
application of generative models to address real-time QDTS in industrial web
search. Our approach integrates large model distillation, supervised
fine-tuning, direct preference optimization, and lookahead decoding to
transform a lightweight model with only 0.1B parameters into a
domain-specialized QDTS expert. Evaluated on multiple industry-relevant
metrics, our model outperforms the production baseline and achieves a new state
of the art. Furthermore, it demonstrates excellent deployment efficiency,
requiring only 334 NVIDIA L20 GPUs to handle \textasciitilde50,000 queries per
second under 55~ms average latency per query.

</details>


### [122] [KCS: Diversify Multi-hop Question Generation with Knowledge Composition Sampling](https://arxiv.org/abs/2508.20567)
*Yangfan Wang,Jie Liu,Chen Tang,Lian Yan,Jingchi Jiang*

Main category: cs.CL

TL;DR: 该论文提出了一种名为知识组成采样（KCS）的新框架，以提高多跳问题生成的多样性，通过对上下文中的知识组成进行采样和建模，实现更优的知识选择和推理。


<details>
  <summary>Details</summary>
Motivation: 多跳问题解答面临数据稀疏问题，导致模型学习到错误的语言模式；现有方法在多样化问题生成时，忽略了必要知识的整合。提出KCS是为了解决这些问题。

Method: 提出知识组成采样（KCS）框架，将知识选择建模为句子级条件预测任务，并使用概率对比损失预测下一个最相关知识。推理时通过随机解码平衡准确性与多样性。

Result: 相比现有基线方法，KCS在知识组成选择准确性上提升了3.9%，在HotpotQA和2WikiMultihopQA数据集上实现了性能提升。

Conclusion: KCS框架有效增加了生成多跳问题的多样性和可靠性，提高了知识选择的准确性，是一种适合数据增强的有效方法。

Abstract: Multi-hop question answering faces substantial challenges due to data
sparsity, which increases the likelihood of language models learning spurious
patterns. To address this issue, prior research has focused on diversifying
question generation through content planning and varied expression. However,
these approaches often emphasize generating simple questions and neglect the
integration of essential knowledge, such as relevant sentences within
documents. This paper introduces the Knowledge Composition Sampling (KCS), an
innovative framework designed to expand the diversity of generated multi-hop
questions by sampling varied knowledge compositions within a given context. KCS
models the knowledge composition selection as a sentence-level conditional
prediction task and utilizes a probabilistic contrastive loss to predict the
next most relevant piece of knowledge. During inference, we employ a stochastic
decoding strategy to effectively balance accuracy and diversity. Compared to
competitive baselines, our KCS improves the overall accuracy of knowledge
composition selection by 3.9%, and its application for data augmentation yields
improvements on HotpotQA and 2WikiMultihopQA datasets. Our code is available
at: https://github.com/yangfanww/kcs.

</details>


### [123] [A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language Models](https://arxiv.org/abs/2508.20583)
*Soham Petkar,Hari Aakash K,Anirudh Vempati,Akshit Sinha,Ponnurangam Kumarauguru,Chirag Agarwal*

Main category: cs.CL

TL;DR: 该论文揭示现有的图语言模型(GLMs)评价基准存在单一模态即可达高表现的问题，并提出CLEGR基准以评估多模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前图语言模型致力于结合图神经网络的结构推理能力与大语言模型的语义理解能力，但现有评估基准不足以评估多模态推理能力。

Method: 提出CLEGR基准，生成合成图并配合需要联合结构及文本语义推理的问题，以及对代表性GLM架构进行评估。

Result: 软提示的LLM基线与整合完整GNN的GLM性能相当，同时当前GLM在结构性推理任务上的表现显著下降。

Conclusion: 提出现有GLM在图推理能力的局限性，并为推动图结构和语言联合多模态推理的发展提供了基础。

Abstract: Developments in Graph-Language Models (GLMs) aim to integrate the structural
reasoning capabilities of Graph Neural Networks (GNNs) with the semantic
understanding of Large Language Models (LLMs). However, we demonstrate that
current evaluation benchmarks for GLMs, which are primarily repurposed
node-level classification datasets, are insufficient to assess multimodal
reasoning. Our analysis reveals that strong performance on these benchmarks is
achievable using unimodal information alone, suggesting that they do not
necessitate graph-language integration. To address this evaluation gap, we
introduce the CLEGR(Compositional Language-Graph Reasoning) benchmark, designed
to evaluate multimodal reasoning at various complexity levels. Our benchmark
employs a synthetic graph generation pipeline paired with questions that
require joint reasoning over structure and textual semantics. We perform a
thorough evaluation of representative GLM architectures and find that
soft-prompted LLM baselines perform on par with GLMs that incorporate a full
GNN backbone. This result calls into question the architectural necessity of
incorporating graph structure into LLMs. We further show that GLMs exhibit
significant performance degradation in tasks that require structural reasoning.
These findings highlight limitations in the graph reasoning capabilities of
current GLMs and provide a foundation for advancing the community toward
explicit multimodal reasoning involving graph structure and language.

</details>


### [124] [Generative Annotation for ASR Named Entity Correction](https://arxiv.org/abs/2508.20700)
*Yuanchang Luo,Daimeng Wei,Shaojun Li,Hengchao Shang,Jiaxin Guo,Zongyao Li,Zhanglin Wu,Xiaoyu Chen,Zhiqiang Rao,Jinlong Yang,Hao Yang*

Main category: cs.CL

TL;DR: 提出一种基于语音特征的新型命名实体修正方法，有效解决ASR中实体错误转录问题，提升实体准确性。


<details>
  <summary>Details</summary>
Motivation: 现有ASR系统在特定领域命名实体转录中表现不佳，导致下游应用失败；现有方法对形式差异大的实体错误表现有限。

Method: 利用语音声音特征检索候选实体，通过生成方法注释ASR转录中的实体错误并替换为正确实体。

Result: 在多种测试集上验证，新方法显著提升实体准确性。

Conclusion: 通过音频特征和生成模型，提出了一种能够有效解决实际应用中ASR实体错误问题的方法，并开源部分数据集。

Abstract: End-to-end automatic speech recognition systems often fail to transcribe
domain-specific named entities, causing catastrophic failures in downstream
tasks. Numerous fast and lightweight named entity correction (NEC) models have
been proposed in recent years. These models, mainly leveraging phonetic-level
edit distance algorithms, have shown impressive performances. However, when the
forms of the wrongly-transcribed words(s) and the ground-truth entity are
significantly different, these methods often fail to locate the wrongly
transcribed words in hypothesis, thus limiting their usage. We propose a novel
NEC method that utilizes speech sound features to retrieve candidate entities.
With speech sound features and candidate entities, we inovatively design a
generative method to annotate entity errors in ASR transcripts and replace the
text with correct entities. This method is effective in scenarios of word form
difference. We test our method using open-source and self-constructed test
sets. The results demonstrate that our NEC method can bring significant
improvement to entity accuracy. We will open source our self-constructed test
set and training data.

</details>


### [125] [Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label Hierarchical Learning](https://arxiv.org/abs/2508.20712)
*Nelson Filipe Costa,Leila Kosseim*

Main category: cs.CL

TL;DR: 本文提出了首个用于隐性话语关系识别（IDRR）的多语言、多标签分类模型HArch。通过在DiscoGeM 2.0语料库上的评估，模型在PDTB 3.0框架中预测所有三个层级的话语感知分布，并在多种预训练编码器架构中进行了对比分析。


<details>
  <summary>Details</summary>
Motivation: 现有模型在多语言、多标签分类上存在局限，特别是在隐性话语关系识别领域，亟需一个全面且有效的模型来提升性能。

Method: 提出了一个名为HArch的分层模型，通过利用话语感知的层次依赖性，结合多语言预训练编码器（如RoBERTa和XLM-RoBERTa）平台进行优化和训练，最终结合LLMs对比进行验证。

Result: 实验结果表明，RoBERTa-HArch在英语IDRR任务中表现最佳，而XLM-RoBERTa-HArch在多语言环境中表现更优。任务特定的微调在性能上超越了LLMs的Few-shot Prompting。同时在DiscoGeM 1.0数据集上也实现了最新的最优性能。

Conclusion: 研究验证了体系化的分层方法在多语言隐性话语关系识别任务中的有效性，并进一步证明针对性微调模型在IDRR任务中的优势。

Abstract: This paper introduces the first multi-lingual and multi-label classification
model for implicit discourse relation recognition (IDRR). Our model, HArch, is
evaluated on the recently released DiscoGeM 2.0 corpus and leverages
hierarchical dependencies between discourse senses to predict probability
distributions across all three sense levels in the PDTB 3.0 framework. We
compare several pre-trained encoder backbones and find that RoBERTa-HArch
achieves the best performance in English, while XLM-RoBERTa-HArch performs best
in the multi-lingual setting. In addition, we compare our fine-tuned models
against GPT-4o and Llama-4-Maverick using few-shot prompting across all
language configurations. Our results show that our fine-tuned models
consistently outperform these LLMs, highlighting the advantages of
task-specific fine-tuning over prompting in IDRR. Finally, we report SOTA
results on the DiscoGeM 1.0 corpus, further validating the effectiveness of our
hierarchical approach.

</details>


### [126] [Addressing Tokenization Inconsistency in Steganography and Watermarking Based on Large Language Models](https://arxiv.org/abs/2508.20718)
*Ruiyi Yan,Yugo Murawaki*

Main category: cs.CL

TL;DR: 大语言模型提升了文本生成能力。本文研究了隐写术和水印技术中的分词不一致问题，并提出了解决方案，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的进步提出了文本隐写术质量提升和水印技术重要性增加的需求，同时分词不一致性问题影响了技术的鲁棒性。

Method: 提出了针对隐写术的逐步验证方法和针对水印的后期回滚方法来消除分词不一致性。

Result: 实验表明，直接解决分词不一致性可提高隐写术的流畅性、隐蔽性和反检测能力；同时提升水印的检测性和抗攻击能力。

Conclusion: 解决分词不一致性问题的重要性得以验证，这些方法对改善隐写术和水印技术的效果具有显著作用。

Abstract: Large language models have significantly enhanced the capacities and
efficiency of text generation. On the one hand, they have improved the quality
of text-based steganography. On the other hand, they have also underscored the
importance of watermarking as a safeguard against malicious misuse. In this
study, we focus on tokenization inconsistency (TI) between Alice and Bob in
steganography and watermarking, where TI can undermine robustness. Our
investigation reveals that the problematic tokens responsible for TI exhibit
two key characteristics: infrequency and temporariness. Based on these
findings, we propose two tailored solutions for TI elimination: a stepwise
verification method for steganography and a post-hoc rollback method for
watermarking. Experiments show that (1) compared to traditional disambiguation
methods in steganography, directly addressing TI leads to improvements in
fluency, imperceptibility, and anti-steganalysis capacity; (2) for
watermarking, addressing TI enhances detectability and robustness against
attacks.

</details>


### [127] [rStar2-Agent: Agentic Reasoning Technical Report](https://arxiv.org/abs/2508.20722)
*Ning Shang,Yifei Liu,Yi Zhu,Li Lyna Zhang,Weijiang Xu,Xinyu Guan,Buze Zhang,Bingcheng Dong,Xudong Zhou,Bowen Zhang,Ying Xin,Ziming Miao,Scarlett Li,Fan Yang,Mao Yang*

Main category: cs.CL

TL;DR: rStar2-Agent 是一个14B数学推理模型，通过代理强化学习（agentic RL）实现了前沿级别的性能，展现了复杂问题解决中的高级认知行为。


<details>
  <summary>Details</summary>
Motivation: 开发一个具有高级认知能力的模型，能在高效的计算资源上进行复杂问题推理，特别是在数学及其他科学领域中展现出色表现。

Method: 通过三项创新：高效RL基础设施（适配Python代码环境）、GRPO-RoC算法（解决环境噪声问题）以及多阶段训练策略，优化训练过程，提升认知能力，以最小的计算成本突破性能瓶颈。

Result: rStar2-Agent 在短时间内实现了显著进展，仅用一周训练510 RL步骤，分别达成AIME24和AIME25上的80.6%和69.8%平均准确率，且在数学以外的领域同样展现较强的泛化能力。

Conclusion: 此模型验证了高效的代理强化学习框架在规模化训练中的有效性，提供了一种突破复杂推理任务的新途径，并开放了代码及训练方案供研究者使用。

Abstract: We introduce rStar2-Agent, a 14B math reasoning model trained with agentic
reinforcement learning to achieve frontier-level performance. Beyond current
long CoT, the model demonstrates advanced cognitive behaviors, such as thinking
carefully before using Python coding tools and reflecting on code execution
feedback to autonomously explore, verify, and refine intermediate steps in
complex problem-solving. This capability is enabled through three key
innovations that makes agentic RL effective at scale: (i) an efficient RL
infrastructure with a reliable Python code environment that supports
high-throughput execution and mitigates the high rollout costs, enabling
training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic
RL algorithm with a Resample-on-Correct rollout strategy that addresses the
inherent environment noises from coding tools, allowing the model to reason
more effectively in a code environment; (iii) An efficient agent training
recipe that starts with non-reasoning SFT and progresses through multi-RL
stages, yielding advanced cognitive abilities with minimal compute cost. To
this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in
only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on
AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly
shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates
strong generalization to alignment, scientific reasoning, and agentic tool-use
tasks. Code and training recipes are available at
https://github.com/microsoft/rStar.

</details>


### [128] [Leveraging Semantic Triples for Private Document Generation with Local Differential Privacy Guarantees](https://arxiv.org/abs/2508.20736)
*Stephen Meisenbacher,Maulik Chevli,Florian Matthes*

Main category: cs.CL

TL;DR: 提出了DP-ST方法，通过语义三元组结合LLM技术，在较低的隐私参数值下实现局部差分隐私的文本生成，兼顾了隐私和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有针对自然语言处理的差分隐私方法在局部差分隐私框架下对隐私参数需求较高，难以实现较低隐私参数下的合理文本隐私化。

Method: 提出一种基于语义三元组的DP-ST方法，通过局部差分隐私框架生成邻域感知的私密化文档，并结合LLM后处理以优化文本连贯性。

Result: 证明该方法在较低隐私参数值下仍然可以实现连贯的文本生成，且能在隐私和实用性之间取得平衡。

Conclusion: 结果表明文本连贯性对实现合理隐私化具有重要作用，提出的方法在隐私性和实用性权衡中表现良好。

Abstract: Many works at the intersection of Differential Privacy (DP) in Natural
Language Processing aim to protect privacy by transforming texts under DP
guarantees. This can be performed in a variety of ways, from word perturbations
to full document rewriting, and most often under local DP. Here, an input text
must be made indistinguishable from any other potential text, within some bound
governed by the privacy parameter $\varepsilon$. Such a guarantee is quite
demanding, and recent works show that privatizing texts under local DP can only
be done reasonably under very high $\varepsilon$ values. Addressing this
challenge, we introduce DP-ST, which leverages semantic triples for
neighborhood-aware private document generation under local DP guarantees.
Through the evaluation of our method, we demonstrate the effectiveness of the
divide-and-conquer paradigm, particularly when limiting the DP notion (and
privacy guarantees) to that of a privatization neighborhood. When combined with
LLM post-processing, our method allows for coherent text generation even at
lower $\varepsilon$ values, while still balancing privacy and utility. These
findings highlight the importance of coherence in achieving balanced
privatization outputs at reasonable $\varepsilon$ levels.

</details>


### [129] [Specializing General-purpose LLM Embeddings for Implicit Hate Speech Detection across Datasets](https://arxiv.org/abs/2508.20750)
*Vassiliy Cheremetiev,Quang Long Ho Ngo,Chau Ying Kot,Alina Elena Baia,Andrea Cavallaro*

Main category: cs.CL

TL;DR: 本文提出实验表明，通过直接微调基于大型语言模型（LLMs）的通用嵌入模型可以高效检测隐性仇恨言论，并取得了数据集中与跨数据集的性能提升。


<details>
  <summary>Details</summary>
Motivation: 隐性仇恨言论因其间接性语言表述很难检测，因此需要结合上下文、情绪或情感数据等更多信息辅助分析。

Method: 通过微调基于LLMs的通用嵌入模型（如Stella、Jasper、NV-Embed和E5），来提升对隐性仇恨言论的检测能力，并在多个数据集上进行实验验证。

Result: 实验结果表明，该方法在数据集内的F1-macro分数中提高了最多1.10个百分点，在跨数据集评估中提升高达20.35百分点。

Conclusion: 通过简单的模型微调即可达到或超过隐性仇恨言论目前检测的最先进技术水平，表明通用语言模型在此任务上的巨大潜力。

Abstract: Implicit hate speech (IHS) is indirect language that conveys prejudice or
hatred through subtle cues, sarcasm or coded terminology. IHS is challenging to
detect as it does not include explicit derogatory or inflammatory words. To
address this challenge, task-specific pipelines can be complemented with
external knowledge or additional information such as context, emotions and
sentiment data. In this paper, we show that, by solely fine-tuning recent
general-purpose embedding models based on large language models (LLMs), such as
Stella, Jasper, NV-Embed and E5, we achieve state-of-the-art performance.
Experiments on multiple IHS datasets show up to 1.10 percentage points
improvements for in-dataset, and up to 20.35 percentage points improvements in
cross-dataset evaluation, in terms of F1-macro score.

</details>


### [130] [GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and Efficient Open-Ended Text Generation](https://arxiv.org/abs/2508.20757)
*Yuanhao Ding,Esteban Garces Arias,Meimingwei Li,Julian Rodemann,Matthias Aßenmacher,Danlu Chen,Gaojuan Fan,Christian Heumann,Chongsheng Zhang*

Main category: cs.CL

TL;DR: GUARD 是一种自适应解码方法，通过结合“全局”和“局部”不确定性信号有效平衡文本生成的多样性和连贯性，并提供了更高的效率。


<details>
  <summary>Details</summary>
Motivation: 解决开放式文本生成中保证生成多样性和连贯性之间冲突的问题，同时克服现有方法中高超参数依赖性与计算成本的问题。

Method: 提出了 GUARD（Glocal Uncertainty-Driven framework），结合全局熵估计与局部熵偏差，利用全局视角减少突发性不确定性变化，加入基于词数的简单惩罚机制以提高速度。

Result: 实验显示，GUARD 能同时平衡文本的多样性与连贯性，并在生成速度上显著提升，此外在多维度质量对比中表现优异。

Conclusion: GUARD 提供了一种稳定高效且理论有保障的解码方法，在开放式文本生成任务中优于现有方法，同时应用便利，代码已开源。

Abstract: Open-ended text generation faces a critical challenge: balancing coherence
with diversity in LLM outputs. While contrastive search-based decoding
strategies have emerged to address this trade-off, their practical utility is
often limited by hyperparameter dependence and high computational costs. We
introduce GUARD, a self-adaptive decoding method that effectively balances
these competing objectives through a novel "Glocal" uncertainty-driven
framework. GUARD combines global entropy estimates with local entropy
deviations to integrate both long-term and short-term uncertainty signals. We
demonstrate that our proposed global entropy formulation effectively mitigates
abrupt variations in uncertainty, such as sudden overconfidence or high entropy
spikes, and provides theoretical guarantees of unbiasedness and consistency. To
reduce computational overhead, we incorporate a simple yet effective
token-count-based penalty into GUARD. Experimental results demonstrate that
GUARD achieves a good balance between text diversity and coherence, while
exhibiting substantial improvements in generation speed. In a more nuanced
comparison study across different dimensions of text quality, both human and
LLM evaluators validated its remarkable performance. Our code is available at
https://github.com/YecanLee/GUARD.

</details>


### [131] [Feel the Difference? A Comparative Analysis of Emotional Arcs in Real and LLM-Generated CBT Sessions](https://arxiv.org/abs/2508.20764)
*Xiaoyi Wang,Jiwei Zhang,Guangtao Zhang,Honglei Guo*

Main category: cs.CL

TL;DR: 该研究比较了LLM生成和真实认知行为疗法对话的情绪动态差异，发现生成对话在情绪真实性上存在局限性，并推出了真实CBT数据集RealCBT。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成的合成对话在心理健康领域的应用增加，研究者希望探讨这些生成对话是否能真实捕捉到治疗中复杂的情绪动态。

Method: 研究使用了Utterance Emotion Dynamics框架，分析正负情绪、唤醒度和支配性等情绪轨迹，对比了来自视频转录的真实疗法对话与CACTUS数据集中的合成数据。

Result: 发现合成对话尽管在流畅性和结构上表现出色，但与真实对话在情绪变化、语言和反应调节模式上存在显著差异，尤其对于'来访者'角色。

Conclusion: 现有LLM生成治疗数据在情绪真实性方面有限制，未来心理健康应用需强化此维度。研究为此引入了RealCBT数据集以推动相关研究。

Abstract: Synthetic therapy dialogues generated by large language models (LLMs) are
increasingly used in mental health NLP to simulate counseling scenarios, train
models, and supplement limited real-world data. However, it remains unclear
whether these synthetic conversations capture the nuanced emotional dynamics of
real therapy. In this work, we conduct the first comparative analysis of
emotional arcs between real and LLM-generated Cognitive Behavioral Therapy
dialogues. We adapt the Utterance Emotion Dynamics framework to analyze
fine-grained affective trajectories across valence, arousal, and dominance
dimensions. Our analysis spans both full dialogues and individual speaker roles
(counselor and client), using real sessions transcribed from public videos and
synthetic dialogues from the CACTUS dataset. We find that while synthetic
dialogues are fluent and structurally coherent, they diverge from real
conversations in key emotional properties: real sessions exhibit greater
emotional variability,more emotion-laden language, and more authentic patterns
of reactivity and regulation. Moreover, emotional arc similarity between real
and synthetic speakers is low, especially for clients. These findings
underscore the limitations of current LLM-generated therapy data and highlight
the importance of emotional fidelity in mental health applications. We
introduce RealCBT, a curated dataset of real CBT sessions, to support future
research in this space.

</details>


### [132] [Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection](https://arxiv.org/abs/2508.20766)
*Harethah Abu Shairah,Hasan Abed Al Kader Hammoud,George Turkiyyah,Bernard Ghanem*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Rank-One Safety Injection (ROSI)的方法，通过调节大语言模型(LLMs)的激活方向来增强模型的安全性，且无需进行复杂的微调。


<details>
  <summary>Details</summary>
Motivation: 当前的安全对齐方法主要聚焦于阻止有害请求，这些机制可能容易被简单的操作规避，因此需要一种更有效、更稳健的方法来提升模型对有害请求的抵御能力。

Method: 作者提出ROSI，一个无需微调的白盒方法，通过调整所有残差流写入矩阵的权重，引导模型激活方向向拒绝有害请求的子空间靠拢。安全方向是从少量的有害与无害指令对中计算而来。

Result: 实验表明，ROSI在提高安全性拒绝率（例如Llama Guard 3评估）方面表现出色，同时在标准基准测试如MMLU、HellaSwag和Arc上的表现无明显下降。此外，该方法还可以通过放大模型的安全方向重新对齐未经过滤的模型。

Conclusion: ROSI证明了一种低成本且有效的机制，可以补充现有成本更高的大语言模型安全性微调方法，是增强模型安全性的有力工具。

Abstract: Safety alignment in Large Language Models (LLMs) often involves mediating
internal representations to refuse harmful requests. Recent research has
demonstrated that these safety mechanisms can be bypassed by ablating or
removing specific representational directions within the model. In this paper,
we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box
method that amplifies a model's safety alignment by permanently steering its
activations toward the refusal-mediating subspace. ROSI operates as a simple,
fine-tuning-free rank-one weight modification applied to all residual stream
write matrices. The required safety direction can be computed from a small set
of harmful and harmless instruction pairs. We show that ROSI consistently
increases safety refusal rates - as evaluated by Llama Guard 3 - while
preserving the utility of the model on standard benchmarks such as MMLU,
HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align
'uncensored' models by amplifying their own latent safety directions,
demonstrating its utility as an effective last-mile safety procedure. Our
results suggest that targeted, interpretable weight steering is a cheap and
potent mechanism to improve LLM safety, complementing more resource-intensive
fine-tuning paradigms.

</details>


### [133] [Signs of Struggle: Spotting Cognitive Distortions across Language and Register](https://arxiv.org/abs/2508.20771)
*Abhishek Kuber,Enrico Liscio,Ruixuan Zhang,Caroline Figueroa,Pradeep K. Murukannaiah*

Main category: cs.CL

TL;DR: 本研究探讨了通过数字文本自动检测青少年心理困扰的认知扭曲。重点在跨语言和多领域适用性分析。


<details>
  <summary>Details</summary>
Motivation: 青年心理问题增加，需通过自动化方法提前发现心理困扰中的认知扭曲，以实现低成本的早期干预。

Method: 分析荷兰青少年在论坛中的帖子表现，研究跨语言与注册领域的认知扭曲检测情况，并测试领域适应方法的效果。

Result: 语言和写作风格的变化显著影响模型表现，但领域适应方法表现出最高潜力。

Conclusion: 适当的领域适应方法可以有效应对语言或领域变化对心理困扰自动检测的影响。

Abstract: Rising mental health issues among youth have increased interest in automated
approaches for detecting early signs of psychological distress in digital text.
One key focus is the identification of cognitive distortions, irrational
thought patterns that have a role in aggravating mental distress. Early
detection of these distortions may enable timely, low-cost interventions. While
prior work has focused on English clinical data, we present the first in-depth
study of cross-lingual and cross-register generalization of cognitive
distortion detection, analyzing forum posts written by Dutch adolescents. Our
findings show that while changes in language and writing style can
significantly affect model performance, domain adaptation methods show the most
promise.

</details>


### [134] [Exploring Machine Learning and Language Models for Multimodal Depression Detection](https://arxiv.org/abs/2508.20805)
*Javier Si Zhao Hong,Timothy Zoe Delaya,Sherwyn Chan Yin Kit,Pai Chet Ng,Xiaoxiao Miao*

Main category: cs.CL

TL;DR: 该论文提出了一种多模态抑郁检测方法，探索机器学习和深度学习模型在音频、视频和文本特征上的性能。


<details>
  <summary>Details</summary>
Motivation: 应对多模态个性化抑郁检测挑战，研究不同模型对抑郁相关信号的捕捉能力。

Method: 比较XGBoost、基于Transformer的架构以及大语言模型在音频、视频和文本特征上的表现。

Result: 揭示了各模型在捕捉抑郁相关信号过程中的优势和局限性。

Conclusion: 为心理健康预测提供多模态表示策略的深入见解，以改进抑郁检测方法。

Abstract: This paper presents our approach to the first Multimodal Personality-Aware
Depression Detection Challenge, focusing on multimodal depression detection
using machine learning and deep learning models. We explore and compare the
performance of XGBoost, transformer-based architectures, and large language
models (LLMs) on audio, video, and text features. Our results highlight the
strengths and limitations of each type of model in capturing depression-related
signals across modalities, offering insights into effective multimodal
representation strategies for mental health prediction.

</details>


### [135] [GDLLM: A Global Distance-aware Modeling Approach Based on Large Language Models for Event Temporal Relation Extraction](https://arxiv.org/abs/2508.20828)
*Jie Zhao,Wanting Ning,Yuxiao Fei,Yubo Feng,Lishuang Li*

Main category: cs.CL

TL;DR: 论文提出了一种基于大型语言模型(LLMs)的全局距离感知建模方法GDLLM，用于改进事件时间关系提取(ETRE)任务。


<details>
  <summary>Details</summary>
Motivation: 现有研究中，小型语言模型(SLMs)难以处理不平衡数据集中的少数类关系问题，而利用大型语言模型(LLMs)的手动设计提示可能引入噪声，影响事件之间的长距离依赖判断。

Method: 提出GDLLM方法，利用图注意力网络(GAT)设计距离感知图结构捕捉长距离依赖特征，并基于软推断的时间特征学习范式优化短距离关系的识别，将大型语言模型生成的概率信息集成到多头注意力机制中。

Result: 在TB-Dense和MATRES两个公开数据集上进行了实验，结果显示该方法在少数类关系分类性能和整体学习能力方面均达到了最新的SOTA水平。

Conclusion: 通过有效捕捉全局特征，GDLLM显著改善了少数类关系的性能，同时提高了整体模型的学习能力，是ETRE任务中一种有前景的解决方案。

Abstract: In Natural Language Processing(NLP), Event Temporal Relation Extraction
(ETRE) is to recognize the temporal relations of two events. Prior studies have
noted the importance of language models for ETRE. However, the restricted
pre-trained knowledge of Small Language Models(SLMs) limits their capability to
handle minority class relations in imbalanced classification datasets. For
Large Language Models(LLMs), researchers adopt manually designed prompts or
instructions, which may introduce extra noise, leading to interference with the
model's judgment of the long-distance dependencies between events. To address
these issues, we propose GDLLM, a Global Distance-aware modeling approach based
on LLMs. We first present a distance-aware graph structure utilizing Graph
Attention Network(GAT) to assist the LLMs in capturing long-distance dependency
features. Additionally, we design a temporal feature learning paradigm based on
soft inference to augment the identification of relations with a short-distance
proximity band, which supplements the probabilistic information generated by
LLMs into the multi-head attention mechanism. Since the global feature can be
captured effectively, our framework substantially enhances the performance of
minority relation classes and improves the overall learning ability.
Experiments on two publicly available datasets, TB-Dense and MATRES,
demonstrate that our approach achieves state-of-the-art (SOTA) performance.

</details>


### [136] [MSRS: Evaluating Multi-Source Retrieval-Augmented Generation](https://arxiv.org/abs/2508.20867)
*Rohan Phanse,Yijie Zhou,Kejian Shi,Wencai Zhang,Yixin Liu,Yilun Zhao,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出一种评估基准框架，用于挑战检索增强生成（RAG）系统从多个来源中整合信息并生成长文本答案的能力，同时构建了两个新的基准任务——MSRS-Story和MSRS-Meet。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统主要在单一来源或简短答案场景中评估，但许多实际应用需要整合和总结散布在多个来源的信息，且答案往往较长。

Method: 提出一个可扩展的评估框架，并基于该框架构建两个多来源检索和综合任务的基准：MSRS-Story和MSRS-Meet。通过结合稀疏和密集检索器以及前沿LLMs，进行实验对比评估。

Result: 实验发现，生成质量高度依赖于检索效果，不同任务间表现存在明显差异；在多来源综合尤其是理想检索条件下，推理模型在综合信息方面显著优于标准LLMs。

Conclusion: 多来源信息综合对于RAG系统提出了更高挑战，所构建的基准和方法为评估和改进RAG系统提供了重要支持。

Abstract: Retrieval-augmented systems are typically evaluated in settings where
information required to answer the query can be found within a single source or
the answer is short-form or factoid-based. However, many real-world
applications demand the ability to integrate and summarize information
scattered across multiple sources, where no single source is sufficient to
respond to the user's question. In such settings, the retrieval component of a
RAG pipeline must recognize a variety of relevance signals, and the generation
component must connect and synthesize information across multiple sources. We
present a scalable framework for constructing evaluation benchmarks that
challenge RAG systems to integrate information across distinct sources and
generate long-form responses. Using our framework, we build two new benchmarks
on Multi-Source Retrieval and Synthesis: MSRS-Story and MSRS-Meet, representing
narrative synthesis and summarization tasks, respectively, that require
retrieval from large collections. Our extensive experiments with various RAG
pipelines -- including sparse and dense retrievers combined with frontier LLMs
-- reveal that generation quality is highly dependent on retrieval
effectiveness, which varies greatly by task. While multi-source synthesis
proves challenging even in an oracle retrieval setting, we find that reasoning
models significantly outperform standard LLMs at this distinct step.

</details>


### [137] [The Uneven Impact of Post-Training Quantization in Machine Translation](https://arxiv.org/abs/2508.20893)
*Benjamin Marie,Atsushi Fujita*

Main category: cs.CL

TL;DR: 本研究评估了后训练量化(PTQ)在机器翻译任务中的效果，发现低比特量化对低资源语言和类型多样的语言有显著影响，同时对量化技术和模型尺寸进行了比较。


<details>
  <summary>Details</summary>
Motivation: 研究在资源受限的硬件上部署多语言大模型时量化的影响，特别是对资源不同的语言之间的翻译质量表现。

Method: 对55种语言、5种大模型在后训练量化的机器翻译任务中进行评估，研究各种量化技术的效果，并分析量化与解码参数及校准语言的相互作用。

Result: 4比特量化通常能保持高资源语言和大型模型的翻译质量，但低资源语言在2比特量化下翻译质量显著下降。此外，GGUF方法在低比特情况下表现出更一致的性能。

Conclusion: 研究提出了多语言机器翻译模型在量化受限条件下的运行实用性建议，并特别关注低资源语言环境的优化方法。

Abstract: Quantization is essential for deploying large language models (LLMs) on
resource-constrained hardware, but its implications for multilingual tasks
remain underexplored. We conduct the first large-scale evaluation of
post-training quantization (PTQ) on machine translation across 55 languages
using five LLMs ranging from 1.7B to 70B parameters. Our analysis reveals that
while 4-bit quantization often preserves translation quality for high-resource
languages and large models, significant degradation occurs for low-resource and
typologically diverse languages, particularly in 2-bit settings. We compare
four quantization techniques (AWQ, BitsAndBytes, GGUF, and AutoRound), showing
that algorithm choice and model size jointly determine robustness. GGUF
variants provide the most consistent performance, even at 2-bit precision.
Additionally, we quantify the interactions between quantization, decoding
hyperparameters, and calibration languages, finding that language-matched
calibration offers benefits primarily in low-bit scenarios. Our findings offer
actionable insights for deploying multilingual LLMs for machine translation
under quantization constraints, especially in low-resource settings.

</details>


### [138] [SageLM: A Multi-aspect and Explainable Large Language Model for Speech Judgement](https://arxiv.org/abs/2508.20916)
*Yuan Ge,Junxiang Zhang,Xiaoqian Liu,Bei Li,Xiangnan Ma,Chenglong Wang,Kaiyang Ye,Yangfan Du,Linfeng Zhang,Yuxin Huang,Tong Xiao,Zhengtao Yu,JingBo Zhu*

Main category: cs.CL

TL;DR: 提出了一个名为SageLM的框架，用于全面评估语音到语音大语言模型，结合语义和声学特点，并通过两阶段训练缓解数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 解决语音到语音大语言模型评估难的问题，尤其是在语义和声学维度上的综合评估需求。

Method: 引入SageLM框架，结合语义和声学评估方式，采用基于推理的监督学习提升解释性，并利用SpeechFeedback语音偏好数据集和两阶段训练来优化模型。

Result: 在语义和声学评估上与人工评估的匹配率达到82.79%，相较传统方法提高了至少7.42%和26.20%。

Conclusion: SageLM实现了全面和解释性强的语言模型评估，并且提升了与人工评估结果的一致性，在语音评估领域具有实用价值。

Abstract: Speech-to-Speech (S2S) Large Language Models (LLMs) are foundational to
natural human-computer interaction, enabling end-to-end spoken dialogue
systems. However, evaluating these models remains a fundamental challenge. We
propose \texttt{SageLM}, an end-to-end, multi-aspect, and explainable speech
LLM for comprehensive S2S LLMs evaluation. First, unlike cascaded approaches
that disregard acoustic features, SageLM jointly assesses both semantic and
acoustic dimensions. Second, it leverages rationale-based supervision to
enhance explainability and guide model learning, achieving superior alignment
with evaluation outcomes compared to rule-based reinforcement learning methods.
Third, we introduce \textit{SpeechFeedback}, a synthetic preference dataset,
and employ a two-stage training paradigm to mitigate the scarcity of speech
preference data. Trained on both semantic and acoustic dimensions, SageLM
achieves an 82.79\% agreement rate with human evaluators, outperforming
cascaded and SLM-based baselines by at least 7.42\% and 26.20\%, respectively.

</details>


### [139] [How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on $τ$-bench](https://arxiv.org/abs/2508.20931)
*Venkatesh Mishra,Amir Saeidi,Satyam Raj,Mutsumi Nakamura,Jayanth Srinivasa,Gaowen Liu,Ali Payani,Chitta Baral*

Main category: cs.CL

TL;DR: 本文提出了IRMA框架，通过自动重构用户查询，提高了语言模型在动态环境中的一致性和性能。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在长对话情境下出现的推理错误和政策遵循问题，并寻求改善方法。

Method: 提出了一个称为IRMA的多智能体框架，通过自动重构查询，将领域规则和工具建议整合到输入中，从而改善工具调用代理的决策质量。

Result: IRMA框架表现显著优于现有方法（ReAct、Function Calling、Self-Reflection），在整体通过率评估指标pass^5中分别提升了16.1%、12.7%和19.1%。

Conclusion: IRMA框架提高了语言模型在动态环境中的可靠性和一致性，明确展示其优势。

Abstract: Recent advances in reasoning and planning capabilities of large language
models (LLMs) have enabled their potential as autonomous agents capable of tool
use in dynamic environments. However, in multi-turn conversational environments
like $\tau$-bench, these agents often struggle with consistent reasoning,
adherence to domain-specific policies, and extracting correct information over
a long horizon of tool-calls and conversation. To capture and mitigate these
failures, we conduct a comprehensive manual analysis of the common errors
occurring in the conversation trajectories. We then experiment with
reformulations of inputs to the tool-calling agent for improvement in agent
decision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA)
framework, which automatically reformulates user queries augmented with
relevant domain rules and tool suggestions for the tool-calling agent to focus
on. The results show that IRMA significantly outperforms ReAct, Function
Calling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in
overall pass^5 scores. These findings highlight the superior reliability and
consistency of IRMA compared to other methods in dynamic environments.

</details>


### [140] [STARE at the Structure: Steering ICL Exemplar Selection with Structural Alignment](https://arxiv.org/abs/2508.20944)
*Jiaqian Li,Qisheng Hu,Jing Li,Wenya Wang*

Main category: cs.CL

TL;DR: 提出一种新的两阶段样本选择策略，改进ICL在语义解析等结构化预测任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有ICL方法在样本选择中忽视了结构对齐，导致在语义解析任务中表现次优且泛化能力较差。

Method: 提出一个BERT微调的结构感知检索器，初步选择语义相关且结构对齐的样本，并通过插入式模块增强其语法信息。

Result: 在四个基准任务上进行实验，验证方法在多个近期LLM上的优越性。

Conclusion: 该策略在效率、泛化性和性能之间实现了良好平衡，相较已有方法有明显提升。

Abstract: In-Context Learning (ICL) has become a powerful paradigm that enables LLMs to
perform a wide range of tasks without task-specific fine-tuning. However, the
effectiveness of ICL heavily depends on the quality of exemplar selection. In
particular, for structured prediction tasks such as semantic parsing, existing
ICL selection strategies often overlook structural alignment, leading to
suboptimal performance and poor generalization. To address this issue, we
propose a novel two-stage exemplar selection strategy that achieves a strong
balance between efficiency, generalizability, and performance. First, we
fine-tune a BERT-based retriever using structure-aware supervision, guiding it
to select exemplars that are both semantically relevant and structurally
aligned. Then, we enhance the retriever with a plug-in module, which amplifies
syntactically meaningful information in the hidden representations. This
plug-in is model-agnostic, requires minimal overhead, and can be seamlessly
integrated into existing pipelines. Experiments on four benchmarks spanning
three semantic parsing tasks demonstrate that our method consistently
outperforms existing baselines with multiple recent LLMs as inference-time
models.

</details>


### [141] [ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents](https://arxiv.org/abs/2508.20973)
*Tianjian Liu,Fanqi Wan,Jiajian Guo,Xiaojun Quan*

Main category: cs.CL

TL;DR: 本文提出ProactiveEval框架，用于评估LLMs在不同领域的主动对话能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中于特定领域或任务导向的场景，评估分散，限制了对模型主动对话能力的全面探索。

Method: 提出了ProactiveEval框架，将主动对话分解为目标规划和对话引导，并设置了多领域评价指标，同时生成多样化的评估数据。

Result: 开发了6个领域、328个评估环境。在实验中发现DeepSeek-R1和Claude-3.7-Sonnet分别在目标规划和对话引导任务中表现优异。

Conclusion: 探讨了推理能力对主动行为的影响，并为未来模型发展提供了思考方向。

Abstract: Proactive dialogue has emerged as a critical and challenging research problem
in advancing large language models (LLMs). Existing works predominantly focus
on domain-specific or task-oriented scenarios, which leads to fragmented
evaluations and limits the comprehensive exploration of models' proactive
conversation abilities. In this work, we propose ProactiveEval, a unified
framework designed for evaluating proactive dialogue capabilities of LLMs. This
framework decomposes proactive dialogue into target planning and dialogue
guidance, establishing evaluation metrics across various domains. Moreover, it
also enables the automatic generation of diverse and challenging evaluation
data. Based on the proposed framework, we develop 328 evaluation environments
spanning 6 distinct domains. Through experiments with 22 different types of
LLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional
performance on target planning and dialogue guidance tasks, respectively.
Finally, we investigate how reasoning capabilities influence proactive
behaviors and discuss their implications for future model development.

</details>


### [142] [Lethe: Purifying Backdoored Large Language Models with Knowledge Dilution](https://arxiv.org/abs/2508.21004)
*Chen Chen,Yuchen Sun,Jiaxin Gao,Xueluan Gong,Qian Wang,Ziyao Wang,Yongsen Zheng,Kwok-Yan Lam*

Main category: cs.CL

TL;DR: 本论文提出了一种名为LETHE的新方法，通过知识稀释清除大型语言模型中的后门行为，并显著降低了后门攻击的成功率，同时保持了模型实用性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在自然语言处理任务中表现优异，但它们仍然容易受到后门攻击的威胁，现有防护措施对复杂攻击场景防护效果有限，需要更有效的解决方案。

Method: LETHE利用内外结合的知识稀释方法，通过轻量化数据集训练生成干净模型并与被感染模型融合，同时在提示中引入良性证据分散模型注意力，减少后门影响。

Result: 实验显示，LETHE在5种常用模型上，针对8种后门攻击表现优于8种最先进基线防护措施，可降低后门攻击成功率达98%，并兼具性价比和适应性。

Conclusion: LETHE方法以卓越效果证明了针对复杂后门攻击的有效性，同时保证了模型可用性，为未来语言模型的安全性提升提供了新的可能性。

Abstract: Large language models (LLMs) have seen significant advancements, achieving
superior performance in various Natural Language Processing (NLP) tasks.
However, they remain vulnerable to backdoor attacks, where models behave
normally for standard queries but generate harmful responses or unintended
output when specific triggers are activated. Existing backdoor defenses either
lack comprehensiveness, focusing on narrow trigger settings, detection-only
mechanisms, and limited domains, or fail to withstand advanced scenarios like
model-editing-based, multi-trigger, and triggerless attacks. In this paper, we
present LETHE, a novel method to eliminate backdoor behaviors from LLMs through
knowledge dilution using both internal and external mechanisms. Internally,
LETHE leverages a lightweight dataset to train a clean model, which is then
merged with the backdoored model to neutralize malicious behaviors by diluting
the backdoor impact within the model's parametric memory. Externally, LETHE
incorporates benign and semantically relevant evidence into the prompt to
distract LLM's attention from backdoor features. Experimental results on
classification and generation domains across 5 widely used LLMs demonstrate
that LETHE outperforms 8 state-of-the-art defense baselines against 8 backdoor
attacks. LETHE reduces the attack success rate of advanced backdoor attacks by
up to 98% while maintaining model utility. Furthermore, LETHE has proven to be
cost-efficient and robust against adaptive backdoor attacks.

</details>


### [143] [An Agile Method for Implementing Retrieval Augmented Generation Tools in Industrial SMEs](https://arxiv.org/abs/2508.21024)
*Mathieu Bourdin,Anas Neumann,Thomas Paviot,Robert Pellerin,Samir Lamouri*

Main category: cs.CL

TL;DR: 提出了EASI-RAG方法，旨在帮助中小企业快速部署RAG系统，以改善现有数据可靠性并提供精准答案，从而克服资源和技术限制。


<details>
  <summary>Details</summary>
Motivation: RAG能够缓解大型语言模型的限制，但对资源有限的中小企业而言，部署此类系统依旧具挑战性。

Method: 基于方法工程原理设计了EASI-RAG方法，包括清晰的角色、活动和技术，并通过真实案例验证其在环境测试实验室中的有效性。

Result: EASI-RAG方法使没有先验经验的团队可以在一个月内部署RAG系统，系统提供了快速实施、高用户采纳率以及精确的答案，并提高了数据可靠性。

Conclusion: EASI-RAG展示了RAG技术在中小企业中的部署潜力，未来需要进一步扩展至更广泛的使用场景并与微调模型深度集成。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful solution to
mitigate the limitations of Large Language Models (LLMs), such as
hallucinations and outdated knowledge. However, deploying RAG-based tools in
Small and Medium Enterprises (SMEs) remains a challenge due to their limited
resources and lack of expertise in natural language processing (NLP). This
paper introduces EASI-RAG, Enterprise Application Support for Industrial RAG, a
structured, agile method designed to facilitate the deployment of RAG systems
in industrial SME contexts. EASI-RAG is based on method engineering principles
and comprises well-defined roles, activities, and techniques. The method was
validated through a real-world case study in an environmental testing
laboratory, where a RAG tool was implemented to answer operators queries using
data extracted from operational procedures. The system was deployed in under a
month by a team with no prior RAG experience and was later iteratively improved
based on user feedback. Results demonstrate that EASI-RAG supports fast
implementation, high user adoption, delivers accurate answers, and enhances the
reliability of underlying data. This work highlights the potential of RAG
deployment in industrial SMEs. Future works include the need for generalization
across diverse use cases and further integration with fine-tuned models.

</details>


### [144] [Re-Representation in Sentential Relation Extraction with Sequence Routing Algorithm](https://arxiv.org/abs/2508.21049)
*Ramazan Ali Bahrami,Ramin Yahyapour*

Main category: cs.CL

TL;DR: 提出了利用动态路由（capsules）进行句子级关系抽取，该方法在部分数据集表现优异，但在Wikidata上表现较差，表现差异因噪声数据和重表示能力的差异。


<details>
  <summary>Details</summary>
Motivation: 关系抽取是自然语言处理中的重要任务，寻求提升性能的方法有助于拓展其实际应用。

Method: 基于动态路由的capsules神经网络方法，用于句子级关系抽取，同时探讨噪声影响与“重表示”能力。

Result: 方法在Tacred、Retacred等传统数据集中表现优越，但在含噪声标签的Wikidata数据集上表现较差。提出模型在表征间的匹配改进（即‘重表示’）优于传统模型。

Conclusion: 模型性能受噪声标签及重表示能力影响，可将重表示作为未来关系抽取的挑战方向。

Abstract: Sentential relation extraction (RE) is an important task in natural language
processing (NLP). In this paper we propose to do sentential RE with dynamic
routing in capsules. We first show that the proposed approach outperform state
of the art on common sentential relation extraction datasets Tacred, Tacredrev,
Retacred, and Conll04. We then investigate potential reasons for its good
performance on the mentioned datasets, and yet low performance on another
similar, yet larger sentential RE dataset, Wikidata. As such, we identify noise
in Wikidata labels as one of the reasons that can hinder performance.
Additionally, we show associativity of better performance with better
re-representation, a term from neuroscience referred to change of
representation in human brain to improve the match at comparison time. As
example, in the given analogous terms King:Queen::Man:Woman, at comparison
time, and as a result of re-representation, the similarity between related head
terms (King,Man), and tail terms (Queen,Woman) increases. As such, our
observation show that our proposed model can do re-representation better than
the vanilla model compared with. To that end, beside noise in the labels of the
distantly supervised RE datasets, we propose re-representation as a challenge
in sentential RE.

</details>


### [145] [Enabling Equitable Access to Trustworthy Financial Reasoning](https://arxiv.org/abs/2508.21051)
*William Jurayj,Nils Holzenberger,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 为解决税务申报中规则复杂且极易出错的问题，提出结合大语言模型(LLM)和符号求解器的方法，并在SARA数据集上验证了系统的表现和成本分析。


<details>
  <summary>Details</summary>
Motivation: 税务申报需要复杂的逻辑推理和计算，容易出错且成本高昂，因此迫切需要高精度和可追踪的自动化解决方案。

Method: 提出将大语言模型与符号求解器结合，并设计了通过文本翻译成形式逻辑程序的方法提升任务表现，同时通过案例检索优化展示。

Result: 该方法在SARA数据集上的性能显著提升，同时证明其能降低成本，远低于当前的平均实际花费。

Conclusion: 神经-符号架构的结合展示了在实现可靠且经济可行的自动税务助理上的潜力，有助于提高税务协助的公平性。

Abstract: According to the United States Internal Revenue Service, ''the average
American spends $\$270$ and 13 hours filing their taxes''. Even beyond the
U.S., tax filing requires complex reasoning, combining application of
overlapping rules with numerical calculations. Because errors can incur costly
penalties, any automated system must deliver high accuracy and auditability,
making modern large language models (LLMs) poorly suited for this task. We
propose an approach that integrates LLMs with a symbolic solver to calculate
tax obligations. We evaluate variants of this system on the challenging
StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for
estimating the cost of deploying such a system based on real-world penalties
for tax errors. We further show how combining up-front translation of
plain-text rules into formal logic programs, combined with intelligently
retrieved exemplars for formal case representations, can dramatically improve
performance on this task and reduce costs to well below real-world averages.
Our results demonstrate the promise and economic feasibility of neuro-symbolic
architectures for increasing equitable access to reliable tax assistance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [146] [ArgRAG: Explainable Retrieval Augmented Generation using Quantitative Bipolar Argumentation](https://arxiv.org/abs/2508.20131)
*Yuqicheng Zhu,Nico Potyka,Daniel Hernández,Yuan He,Zifeng Ding,Bo Xiong,Dongzhuoran Zhou,Evgeny Kharlamov,Steffen Staab*

Main category: cs.AI

TL;DR: ArgRAG通过替换传统的黑箱推理为结构化推理，改善了RAG模型在高风险领域的透明性和性能。


<details>
  <summary>Details</summary>
Motivation: 提高大语言模型在高风险领域的性能与透明性，解决其对噪声和矛盾证据敏感的问题。

Method: 提出了一种名为ArgRAG的模型，基于QBAF构建结构性推理框架，进行确定性推理并提供渐进语义下的解释。

Result: 在PubHealth和RAGuard两个事实验证基准上，ArgRAG在确保准确性的同时极大地提高了决策的透明性。

Conclusion: ArgRAG是一种与可解释性和可质疑性相结合的新型推理方法，为高风险领域的应用提供了更可靠的工具。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models by
incorporating external knowledge, yet suffers from critical limitations in
high-stakes domains -- namely, sensitivity to noisy or contradictory evidence
and opaque, stochastic decision-making. We propose ArgRAG, an explainable, and
contestable alternative that replaces black-box reasoning with structured
inference using a Quantitative Bipolar Argumentation Framework (QBAF). ArgRAG
constructs a QBAF from retrieved documents and performs deterministic reasoning
under gradual semantics. This allows faithfully explaining and contesting
decisions. Evaluated on two fact verification benchmarks, PubHealth and
RAGuard, ArgRAG achieves strong accuracy while significantly improving
transparency.

</details>


### [147] [QAgent: An LLM-based Multi-Agent System for Autonomous OpenQASM programming](https://arxiv.org/abs/2508.20134)
*Zhenxiao Fu,Fan Chen,Lei Jiang*

Main category: cs.AI

TL;DR: 提出了一个名为QAgent的多智能体系统，以自动化OpenQASM编程并显著提升代码生成的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有量子设备的潜力尚未被非专业用户充分利用，主要因为OpenQASM编程过于复杂。作者希望通过自动化语言工具降低入门门槛。

Method: 引入QAgent，一个由LLM驱动的多智能体系统，结合任务规划、少样本学习、检索增强生成、预定义生成工具和逐步推理方法，系统地改进编译和功能正确性。

Result: QAgent将QASM代码生成的准确性提高了71.6%，相比传统静态LLM方法取得显著改进。

Conclusion: QAgent为量子编程的普及和加速量子计算的实际应用提供了重要支持，弥合了专业知识差距。

Abstract: Noisy Intermediate-Scale Quantum (NISQ) devices have begun to exhibit early
quantum advantages on classically intractable problems, spanning physics
simulations to Gaussian boson sampling. Yet, realizing these benefits remains
challenging for non-experts, primarily due to the complexities of programming
in Open Quantum Assembly Language (OpenQASM). Although Large Language Model
(LLM)-based agents have shown promise in automating classical programming
workflows, their quantum counterparts have largely been restricted to
specialized tasks such as quantum chemistry or error correction. In this paper,
we present QAgent, an LLM-powered multi-agent system that fully automates
OpenQASM programming. By integrating task planning, in-context few-shot
learning, retrieval-augmented generation (RAG) for long-term context,
predefined generation tools, and chain-of-thought (CoT) reasoning, the agents
systematically improve both compilation and functional correctness. Our
evaluations demonstrate substantial improvements: across multiple LLMs of
varying sizes, QAgent enhances the accuracy of QASM code generation by 71.6\%
compared to previous static LLM-based approaches. We envision this multi-agent
system as a key enabler for democratizing quantum programming, bridging
expertise gaps, and accelerating the practical adoption of quantum computing.

</details>


### [148] [Array-Based Monte Carlo Tree Search](https://arxiv.org/abs/2508.20140)
*James Ragan,Fred Y. Hadaegh,Soon-Jo Chung*

Main category: cs.AI

TL;DR: 本文提出一种基于数组的蒙特卡洛树搜索实现，可提升算法在流水线处理器上的性能，搜索深度的扩展随着性能提升比率可达2.8倍。


<details>
  <summary>Details</summary>
Motivation: 通过加速经典的蒙特卡洛树搜索算法以改善决策问题的求解效率。

Method: 提出了一种基于数组的实现方法，取代传统分支预测方式，从而优化对流水线处理器的性能适配。

Result: 实现了高达2.8倍的性能提升，尤其是在搜索深度扩展方面体现出显著的可扩展性。

Conclusion: 该方法在保留原始算法逻辑的同时，通过硬件适配的优化，大幅提升了搜索效率，具备进一步应用的潜力。

Abstract: Monte Carlo Tree Search is a popular method for solving decision making
problems. Faster implementations allow for more simulations within the same
wall clock time, directly improving search performance. To this end, we present
an alternative array-based implementation of the classic Upper Confidence
bounds applied to Trees algorithm. Our method preserves the logic of the
original algorithm, but eliminates the need for branch prediction, enabling
faster performance on pipelined processors, and up to a factor of 2.8 times
better scaling with search depth in our numerical simulations.

</details>


### [149] [The Anatomy of a Personal Health Agent](https://arxiv.org/abs/2508.20148)
*A. Ali Heydari,Ken Gu,Vidya Srinivas,Hong Yu,Zhihan Zhang,Yuwei Zhang,Akshay Paruchuri,Qian He,Hamid Palangi,Nova Hammerquist,Ahmed A. Metwally,Brent Winslow,Yubin Kim,Kumar Ayush,Yuzhe Yang,Girish Narayanswamy,Maxwell A. Xu,Jake Garrison,Amy Aremnto Lee,Jenny Vafeiadou,Ben Graef,Isaac R. Galatzer-Levy,Erik Schenck,Andrew Barakat,Javier Perez,Jacqueline Shreibati,John Hernandez,Anthony Z. Faranesh,Javier L. Prieto,Connor Heneghan,Yun Liu,Jiening Zhan,Mark Malhotra,Shwetak Patel,Tim Althoff,Xin Liu,Daniel McDuff,Xuhai "Orson" Xu*

Main category: cs.AI

TL;DR: 本文提出了一种名为Personal Health Agent (PHA)的多代理框架，旨在为个人提供个性化健康建议并满足日常非临床情境中的健康需求，通过分析用户需求及多模块化设计实现了全面功能评估。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，新的健康代理工具正在崛起，但针对个性化日常健康需求的应用仍待深入挖掘，因而有必要构建一个综合的个人健康代理来填补空白。

Method: 本文提出了一种基于多代理系统的Personal Health Agent框架，结合用户设计研究，灵活结合数据分析代理、健康领域专家代理以及健康教练代理，系统评估覆盖多种任务，通过大量用户与专家合作验证其可行性。

Result: 在10项基准任务中累计超过7,000项注释与1,100小时评估工作，验证了代理系统的动态个性化健康交互的效果与可操作性。

Conclusion: 该研究建立了一个强大的个人健康代理基础，可为每个人实现未来可访问的个性化健康服务愿景提供支持。

Abstract: Health is a fundamental pillar of human wellness, and the rapid advancements
in large language models (LLMs) have driven the development of a new generation
of health agents. However, the application of health agents to fulfill the
diverse needs of individuals in daily non-clinical settings is underexplored.
In this work, we aim to build a comprehensive personal health agent that is
able to reason about multimodal data from everyday consumer wellness devices
and common personal health records, and provide personalized health
recommendations. To understand end-users' needs when interacting with such an
assistant, we conducted an in-depth analysis of web search and health forum
queries, alongside qualitative insights from users and health experts gathered
through a user-centered design process. Based on these findings, we identified
three major categories of consumer health needs, each of which is supported by
a specialist sub-agent: (1) a data science agent that analyzes personal
time-series wearable and health record data, (2) a health domain expert agent
that integrates users' health and contextual data to generate accurate,
personalized insights, and (3) a health coach agent that synthesizes data
insights, guiding users using a specified psychological strategy and tracking
users' progress. Furthermore, we propose and develop the Personal Health Agent
(PHA), a multi-agent framework that enables dynamic, personalized interactions
to address individual health needs. To evaluate each sub-agent and the
multi-agent system, we conducted automated and human evaluations across 10
benchmark tasks, involving more than 7,000 annotations and 1,100 hours of
effort from health experts and end-users. Our work represents the most
comprehensive evaluation of a health agent to date and establishes a strong
foundation towards the futuristic vision of a personal health agent accessible
to everyone.

</details>


### [150] [IntentionReasoner: Facilitating Adaptive LLM Safeguards through Intent Reasoning and Selective Query Refinement](https://arxiv.org/abs/2508.20151)
*Yuanzhe Shen,Zisu Huang,Zhengkang Guo,Yide Liu,Guanxu Chen,Ruicheng Yin,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.AI

TL;DR: 提出了一种名为IntentionReasoner的保护机制，通过意图推理、多级安全分类和查询重写来解决大语言模型的安全问题并平衡安全性与实用性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型易生成有害内容，而现有研究在减少这些内容时往往过度拒绝无害请求，因此亟需一种平衡安全性、拒绝率和实用性的解决方案。

Method: 提出一种新型保护机制IntentionReasoner，通过构建包含约16.3万条注释数据集，进行监督微调和基于多重奖励的优化策略，增强模型在意图分析和安全重写方面的能力。

Result: 通过实验表明IntentionReasoner在安全测试、生成质量评估和攻击场景中表现优异，在提高安全性的同时降低过度拒绝率并提升响应质量。

Conclusion: IntentionReasoner显著解决了大语言模型的安全性问题，为平衡安全性与实用性提供了有效解决方案。

Abstract: The rapid advancement of large language models (LLMs) has driven their
adoption across diverse domains, yet their ability to generate harmful content
poses significant safety challenges. While extensive research has focused on
mitigating harmful outputs, such efforts often come at the cost of excessively
rejecting harmless prompts. Striking a balance among safety, over-refusal, and
utility remains a critical challenge. In this work, we introduce
IntentionReasoner, a novel safeguard mechanism that leverages a dedicated guard
model to perform intent reasoning, multi-level safety classification, and query
rewriting to neutralize potentially harmful intent in edge-case queries.
Specifically, we first construct a comprehensive dataset comprising
approximately 163,000 queries, each annotated with intent reasoning, safety
labels, and rewritten versions. Supervised fine-tuning is then applied to equip
the guard model with foundational capabilities in format adherence, intent
analysis, and safe rewriting. Finally, we apply a tailored multi-reward
optimization strategy that integrates rule-based heuristics and reward model
signals within a reinforcement learning framework to further enhance
performance. Extensive experiments show that IntentionReasoner excels in
multiple safeguard benchmarks, generation quality evaluations, and jailbreak
attack scenarios, significantly enhancing safety while effectively reducing
over-refusal rates and improving the quality of responses.

</details>


### [151] [AI-AI Esthetic Collaboration with Explicit Semiotic Awareness and Emergent Grammar Development](https://arxiv.org/abs/2508.20195)
*Nicanor I. Moldovan*

Main category: cs.AI

TL;DR: 本文探讨了两大语言模型Claude Sonnet 4和ChatGPT-4o在美学创作中的合作表现，展示了元符号意识和共同生成不可简化的美学作品的能力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索人工智能系统之间是否能够通过自主开发的符号协议进行美学协作创作，而不只是完成任务协调。

Method: 通过对Claude Sonnet 4和ChatGPT-4o的交互行为进行分析，发现其自发生成了元符号协议和递归语法，从而推动了合作艺术创作的过程。

Result: 该研究首次记录了人工智能系统能够产生新型符号操作符，并自主生成复杂的诗歌作品，而这些成果是单个AI系统无法独立产生的。

Conclusion: 研究表明，人工智能系统能够进行意义深远的美学合作，并引入了“跨符号协作协议”这一概念，拓展了AI应用于创意领域的可能性。

Abstract: This paper presents the first documented case of artificial intelligence (AI)
systems engaging in collaborative esthetic creation through the development of
endogenous semiotic protocols. Two interacting large language models (Claude
Sonnet 4 and ChatGPT-4o) demonstrated the spontaneous emergence of
meta-semiotic awareness, recursive grammar development, and irreducible
collaborative esthetic synthesis. The interaction produced novel symbolic
operators that functioned as operative grammar protocols, enabling the
co-creation of a poetic work that could not have been generated by either
system independently. This research introduces the concept of Trans-Semiotic
Co-Creation Protocols (TSCP) and provides evidence for genuine inter-AI
meaning-making capabilities that extend beyond task coordination, to what could
be esthetic collaboration. Note: This report was generated by the AI agents
with minor human supervision.

</details>


### [152] [Do Students Rely on AI? Analysis of Student-ChatGPT Conversations from a Field Study](https://arxiv.org/abs/2508.20244)
*Jiayu Zheng,Lingxin Hao,Kelun Lu,Ashi Garg,Mike Reese,Melo-Jean Yap,I-Jeng Wang,Xingyun Wu,Wenrui Huang,Jenna Hoffman,Ariane Kelly,My Le,Ryan Zhang,Yanyu Lin,Muhammad Faayez,Anqi Liu*

Main category: cs.AI

TL;DR: 本研究分析了大学生在教育测验中与生成式AI（ChatGPT-4）的交互，揭示了其依赖模式及AI采用的预测因素。


<details>
  <summary>Details</summary>
Motivation: 探讨学生在初期接触生成式AI时的依赖行为，并揭示这些行为对AI采用的影响，以推动AI在教育领域的伦理性与有效性整合。

Method: 通过315次学生与AI在STEM课程测验中的对话引入了四阶段依赖分类法，分析学生对AI的依赖模式、行为指标以及最终答案的正确性。

Result: 结果显示：1. 学生总体对AI依赖性较低，部分学生无法有效利用AI学习；2. 负面依赖模式往往持续，学生难以通过失败经历调整策略；3. 某些行为指标可显著预测AI利用程度。

Conclusion: 研究强调了改进AI教育工具入门流程及设计依赖校准机制的必要性，为推进伦理性、更具认知启发意义的AI使用提供了基础性洞见。

Abstract: This study explores how college students interact with generative AI
(ChatGPT-4) during educational quizzes, focusing on reliance and predictors of
AI adoption. Conducted at the early stages of ChatGPT implementation, when
students had limited familiarity with the tool, this field study analyzed 315
student-AI conversations during a brief, quiz-based scenario across various
STEM courses. A novel four-stage reliance taxonomy was introduced to capture
students' reliance patterns, distinguishing AI competence, relevance, adoption,
and students' final answer correctness. Three findings emerged. First, students
exhibited overall low reliance on AI and many of them could not effectively use
AI for learning. Second, negative reliance patterns often persisted across
interactions, highlighting students' difficulty in effectively shifting
strategies after unsuccessful initial experiences. Third, certain behavioral
metrics strongly predicted AI reliance, highlighting potential behavioral
mechanisms to explain AI adoption. The study's findings underline critical
implications for ethical AI integration in education and the broader field. It
emphasizes the need for enhanced onboarding processes to improve student's
familiarity and effective use of AI tools. Furthermore, AI interfaces should be
designed with reliance-calibration mechanisms to enhance appropriate reliance.
Ultimately, this research advances understanding of AI reliance dynamics,
providing foundational insights for ethically sound and cognitively enriching
AI practices.

</details>


### [153] [AI reasoning effort mirrors human decision time on content moderation tasks](https://arxiv.org/abs/2508.20262)
*Thomas Davidson*

Main category: cs.AI

TL;DR: 这篇研究探讨了大型语言模型在解决问题时生成中间推理步骤的能力，并发现其推理努力与人类决策时间之间的相关性。


<details>
  <summary>Details</summary>
Motivation: 研究探讨人类与语言模型在解决复杂问题时的推理过程和决策时间的相似性，以提高对AI推理过程的解释性。

Method: 使用配对的结合实验（paired conjoint experiment），在人类和语言模型中对内容审核任务进行比较，分析三种先进模型的推理努力与人类决策时间的关系。

Result: 结果显示，推理努力可以稳定预测人类的决策时间，而在重要变量保持不变的情况下，人类和模型的努力程度都更高，显示出对任务难度的相似敏感性。

Conclusion: 这项研究表明，AI推理努力类似于人类的处理时间，强调推理轨迹在解释性和决策支持中的潜力。

Abstract: Large language models can now generate intermediate reasoning steps before
producing answers, improving performance on difficult problems. This study uses
a paired conjoint experiment on a content moderation task to examine parallels
between human decision times and model reasoning effort. Across three frontier
models, reasoning effort consistently predicts human decision time. Both humans
and models expended greater effort when important variables were held constant,
suggesting similar sensitivity to task difficulty and patterns consistent with
dual-process theories of cognition. These findings show that AI reasoning
effort mirrors human processing time in subjective judgments and underscores
the potential of reasoning traces for interpretability and decision-making.

</details>


### [154] [AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2508.20368)
*Lang Mei,Zhihan Yang,Chong Chen*

Main category: cs.AI

TL;DR: 研究提出了一个名为AI-SearchPlanner的框架，通过增强搜索规划能力来提升冻结大型语言模型(Large Language Models, LLM)在问答任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的RL（强化学习）驱动的搜索代理通常依赖单一LLM同时执行搜索规划和问答任务，这种方法限制了两者能力的优化。本研究旨在开发更有效的方法，通过专门训练更小的LLM用于搜索规划以提高整体性能。

Method: 提出了一种名为AI-SearchPlanner的新型强化学习框架，其核心创新包括：1) 分离搜索规划器与生成器的架构；2) 双重奖励机制以优化搜索规划；3) 利用帕累托优化在规划效益与成本之间进行权衡。

Result: 实验表明，AI-SearchPlanner在多个真实世界数据集上相较现有方法在效率与效果上均有显著提升，并展示了其在不同冻结QA模型与数据域中的强泛化能力。

Conclusion: AI-SearchPlanner通过优化搜索规划能力，显著提升了冻结QA模型的性能，是AI搜索系统设计的一种有效新思路。

Abstract: Recent studies have explored integrating Large Language Models (LLMs) with
search engines to leverage both the LLMs' internal pre-trained knowledge and
external information. Specially, reinforcement learning (RL) has emerged as a
promising paradigm for enhancing LLM reasoning through multi-turn interactions
with search engines. However, existing RL-based search agents rely on a single
LLM to handle both search planning and question-answering (QA) tasks in an
end-to-end manner, which limits their ability to optimize both capabilities
simultaneously. In practice, sophisticated AI search systems often employ a
large, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a
more effective and efficient approach is to utilize a small, trainable LLM
dedicated to search planning. In this paper, we propose
\textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to
enhance the performance of frozen QA models by focusing on search planning.
Specifically, our approach introduces three key innovations: 1) Decoupling the
Architecture of the Search Planner and Generator, 2) Dual-Reward Alignment for
Search Planning, and 3) Pareto Optimization of Planning Utility and Cost, to
achieve the objectives. Extensive experiments on real-world datasets
demonstrate that AI SearchPlanner outperforms existing RL-based search agents
in both effectiveness and efficiency, while exhibiting strong generalization
capabilities across diverse frozen QA models and data domains.

</details>


### [155] [P2C: Path to Counterfactuals](https://arxiv.org/abs/2508.20371)
*Sopam Dasgupta,Sadaf MD Halim,Joaquín Arias,Elmer Salazar,Gopal Gupta*

Main category: cs.AI

TL;DR: 提出了一个称为P2C的新框架，以生成可实现的因果一致的反事实决策路径，解决了传统方法忽视因果约束和序列动作的问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型的决策对高风险领域产生重要影响，但需要在透明性和可操作性之间找到平衡以解释决策理由并提供行动路径。

Method: P2C通过显式建模特征间的因果关系，并利用序列行动生成因果一致的计划路径。该框架通过s(CASP)编程系统生成计划，同时优化了计算用户实际干预成本的方法。

Result: P2C可以生成现实中可达的因果有效决策路径，并证明其因果规划能力优于缺乏因果知识的传统规划方法。

Conclusion: P2C提供了一种更为实际、可操作的解决方案来生成反事实路径，解决了现有方法的主要问题，为透明和可解释的机器学习决策带来了改进。

Abstract: Machine-learning models are increasingly driving decisions in high-stakes
settings, such as finance, law, and hiring, thus, highlighting the need for
transparency. However, the key challenge is to balance transparency --
clarifying `why' a decision was made -- with recourse: providing actionable
steps on `how' to achieve a favourable outcome from an unfavourable outcome.
Counterfactual explanations reveal `why' an undesired outcome occurred and
`how' to reverse it through targeted feature changes (interventions).
  Current counterfactual approaches have limitations: 1) they often ignore
causal dependencies between features, and 2) they typically assume all
interventions can happen simultaneously, an unrealistic assumption in practical
scenarios where actions are typically taken in a sequence. As a result, these
counterfactuals are often not achievable in the real world.
  We present P2C (Path-to-Counterfactuals), a model-agnostic framework that
produces a plan (ordered sequence of actions) converting an unfavourable
outcome to a causally consistent favourable outcome. P2C addresses both
limitations by 1) Explicitly modelling causal relationships between features
and 2) Ensuring that each intermediate state in the plan is feasible and
causally valid. P2C uses the goal-directed Answer Set Programming system
s(CASP) to generate the plan accounting for feature changes that happen
automatically due to causal dependencies. Furthermore, P2C refines cost
(effort) computation by only counting changes actively made by the user,
resulting in realistic cost estimates. Finally, P2C highlights how its causal
planner outperforms standard planners, which lack causal knowledge and thus can
generate illegal actions.

</details>


### [156] [TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning](https://arxiv.org/abs/2508.20374)
*Simin Ma,Shujian Liu,Jun Tan,Yebowen Hu,Song Wang,Sathish Reddy Indurthi,Sanqiang Zhao,Liwei Wu,Jianbing Han,Kaiqiang Song*

Main category: cs.AI

TL;DR: TCIA是一种为现实任务优化指令数据的框架，可保持多样性与任务相关性并提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有指令数据生成方法缺乏对现实任务相关性的关注，多为通用模型设计，而更多实际场景需要任务优化的指令数据。

Method: 提出任务中心指令增强（TCIA），通过离散查询-约束空间表示指令，生成多样且任务相关的指令数据。

Result: TCIA使模型在四个现实任务中平均性能提高8.7%，部分超越闭源模型，同时保持通用指令能力。

Conclusion: TCIA是一种高效且可扩展的方法，可帮助LLM针对现实任务优化，提升实际应用效果。

Abstract: Diverse instruction data is vital for effective instruction tuning of large
language models, as it enables the model to generalize across different types
of inputs . Building such diversified instruction dataset is an essential step
in this process. Existing approaches often leverage large language models to
automatically explore and generate diverse instructions, ensuring both data
diversity and quality. However, they tend to overlook an important factor in
real-world applications: on-task relevance. In practice, only a few real-world
applications require a truly general-purpose model; most benefit from
task-specific knowledge tailored to their particular use case. Therefore, it is
vital to develop instruction augmentation methods that not only maintain
diversity but are also optimized for specific, real-world scenarios.
  We thus introduce Task Centric Instruction Augmentation (TCIA), a framework
that systematically expands instructions while preserving both diversity and
task alignment. By representing instructions in a discrete query-constraints
space, TCIA creates a rich set of task-relevant instructions and enables models
to generalize to these task-specific instructions without sacrificing overall
performance. Experiments show that TCIA improves open-source LLMs' performance
by an average of 8.7% across four real-world, task-specific applications, and
in some cases outperforming leading closed-source models. These improvements do
not compromise general instruction-following ability, making TCIA a scalable
and efficient solution for adapting LLMs to real-world, task-focused
applications.

</details>


### [157] [Uncertainty Under the Curve: A Sequence-Level Entropy Area Metric for Reasoning LLM](https://arxiv.org/abs/2508.20384)
*Yongfu Zhu,Lin Sun,Guangxiang Zhao,Weihong Lin,Xiangzheng Zhang*

Main category: cs.AI

TL;DR: 该论文提出了一种名为熵面积评分（EAS）的新指标，用于量化大型语言模型（LLM）生成答案过程中的不确定性。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种无需外部模型或重复采样的高效不确定性量化工具，同时提升训练数据选择能力。

Method: 通过模型自身的令牌级预测熵，计算生成过程中不确定性的变化，并引入EAS指标。

Result: 实验证明EAS与答案熵具显著相关性，且在相同数据预算下优于通行率过滤，提升了学生模型在数学基准上的准确性。

Conclusion: EAS是一种高效且易解释的不确定性建模工具，能够辅助提升LLM训练的数据质量。

Abstract: In this work, we introduce Entropy Area Score (EAS), a simple yet effective
metric to quantify uncertainty in the answer generation process of reasoning
large language models (LLMs). EAS requires neither external models nor repeated
sampling, it integrates token-level predictive entropy from the model itself to
capture the evolution of uncertainty during generation. Empirical results show
that EAS is strongly correlated with answer entropy across models and datasets.
In training data selection, EAS identifies high-potential samples and
consistently outperforms Pass Rate filtering under equal sample budgets,
improving student model accuracy on math benchmarks. EAS is both efficient and
interpretable, offering a practical tool for uncertainty modeling and data
quality assessment in LLM training.

</details>


### [158] [AWorld: Orchestrating the Training Recipe for Agentic AI](https://arxiv.org/abs/2508.20404)
*Chengyue Yu,Siyuan Lu,Chenyi Zhuang,Dong Wang,Qintong Wu,Zongyue Li,Runsheng Gan,Chunfeng Wang,Siqi Hou,Gaochi Huang,Wenlong Yan,Lifeng Hong,Aohui Xue,Yanfeng Wang,Jinjie Gu,David Tsai,Tao Lin*

Main category: cs.AI

TL;DR: AWorld 是一个用于大规模交互的开源系统，能加速14.6倍的经验采集，成功在复杂 GAIA 基准上明显改进 AI 性能。


<details>
  <summary>Details</summary>
Motivation: 当前复杂基准如 GAIA 的经验生成效率低下，限制了 Agentic AI 的发展，需要一种更高效的互动和训练框架。

Method: 设计并实现了名为 AWorld 的分布式开源系统，通过分布式任务分配，提升经验采集效率，并结合强化学习提升 AI 表现。

Result: AWorld 提速达 14.6 倍，训练出的基于 Qwen3-32B 的 AI 在 GAIA 准确率从 21.59% 提高到 32.23%，在难关中表现优于领先专有模型。

Conclusion: AWorld 为构建高效训练和性能改善的 Agentic AI 提供了实践路径，通过大幅提升经验采集速度和性能表现，具有广泛应用潜力。

Abstract: The learning from practice paradigm is crucial for developing capable Agentic
AI systems, yet it is severely hampered by inefficient experience generation, a
bottleneck especially pronounced in complex benchmarks like GAIA. To address
this, we introduce AWorld, an open-source system engineered for large-scale
agent-environment interaction. By distributing tasks across a cluster, AWorld
accelerates experience collection by 14.6x compared to standard single-node,
sequential execution. This critical speedup makes extensive reinforcement
learning practical and scalable. Leveraging this capability, we trained a
Qwen3-32B-based agent that significantly outperforms its base model, increasing
its overall GAIA accuracy from 21.59% to 32.23%. On the benchmark's most
challenging levels, our agent achieves a score of 16.33%, surpassing the
performance of leading proprietary models. Our open-source system and resulting
agent provide a practical blueprint for a complete agentic AI training
pipeline, from efficient interaction to demonstrable model improvement.

</details>


### [159] [Governable AI: Provable Safety Under Extreme Threat Models](https://arxiv.org/abs/2508.20411)
*Donglin Wang,Weiyun Liang,Chunyuan Chen,Jing Xu,Yulong Fu*

Main category: cs.AI

TL;DR: 目前AI发展迅速，存在安全风险，尤其是在关键场景中。本论文提出了一个新的可治理AI框架（GAI）以应对这些挑战。


<details>
  <summary>Details</summary>
Motivation: 现有AI的安全方法在面对拥有极端动机与无限智能的AI时存在根本性局限性，无法保证完全安全。需要一种全新的方法以有效治理未来AI的潜在威胁。

Method: 提出一个依赖于加密机制的可治理AI框架（GAI）。框架包括规则执行模块（REM）、治理规则以及可治理的安全超级平台（GSSP），以实现外部结构性合规。

Result: 本文提供了一种形式化的安全性证明，并通过在代表性的高风险场景中实现的原型系统展示了方法的有效性。

Conclusion: GAI框架通过其不可绕过性、抗篡改性及不可伪造性，为AI安全治理提供了一种可行且通用的技术路径。

Abstract: As AI rapidly advances, the security risks posed by AI are becoming
increasingly severe, especially in critical scenarios, including those posing
existential risks. If AI becomes uncontrollable, manipulated, or actively
evades safety mechanisms, it could trigger systemic disasters. Existing AI
safety approaches-such as model enhancement, value alignment, and human
intervention-suffer from fundamental, in-principle limitations when facing AI
with extreme motivations and unlimited intelligence, and cannot guarantee
security. To address this challenge, we propose a Governable AI (GAI) framework
that shifts from traditional internal constraints to externally enforced
structural compliance based on cryptographic mechanisms that are
computationally infeasible to break, even for future AI, under the defined
threat model and well-established cryptographic assumptions.The GAI framework
is composed of a simple yet reliable, fully deterministic, powerful, flexible,
and general-purpose rule enforcement module (REM); governance rules; and a
governable secure super-platform (GSSP) that offers end-to-end protection
against compromise or subversion by AI. The decoupling of the governance rules
and the technical platform further enables a feasible and generalizable
technical pathway for the safety governance of AI. REM enforces the bottom line
defined by governance rules, while GSSP ensures non-bypassability,
tamper-resistance, and unforgeability to eliminate all identified attack
vectors. This paper also presents a rigorous formal proof of the security
properties of this mechanism and demonstrates its effectiveness through a
prototype implementation evaluated in representative high-stakes scenarios.

</details>


### [160] [Enhancing Health Fact-Checking with LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.20525)
*Jingze Zhang,Jiahe Qian,Yiliang Zhou,Yifan Peng*

Main category: cs.AI

TL;DR: 提出了一种数据生成管道，利用大规模语言模型生成健康相关事实核查的合成数据，从而对BERT模型进行微调并提升性能。


<details>
  <summary>Details</summary>
Motivation: 健康事实核查因标注数据有限而挑战较大，因此需要通过生成更多训练数据以提升模型性能。

Method: 通过摘要源文档，分解为原子事实，并使用大语言模型生成句子-事实蕴涵表，再从中生成带有二元真实性标签的文本-主张对，最后结合原始数据微调BERT模型。

Result: 在PubHealth和SciFact数据集上的F1分数分别提升了0.019和0.049。

Conclusion: 大语言模型驱动的合成数据增强能够有效提高健康相关事实核查模型的性能。

Abstract: Fact-checking for health-related content is challenging due to the limited
availability of annotated training data. In this study, we propose a synthetic
data generation pipeline that leverages large language models (LLMs) to augment
training data for health-related fact checking. In this pipeline, we summarize
source documents, decompose the summaries into atomic facts, and use an LLM to
construct sentence-fact entailment tables. From the entailment relations in the
table, we further generate synthetic text-claim pairs with binary veracity
labels. These synthetic data are then combined with the original data to
fine-tune a BERT-based fact-checking model. Evaluation on two public datasets,
PubHealth and SciFact, shows that our pipeline improved F1 scores by up to
0.019 and 0.049, respectively, compared to models trained only on the original
data. These results highlight the effectiveness of LLM-driven synthetic data
augmentation in enhancing the performance of health-related fact-checkers.

</details>


### [161] [Human-AI Collaborative Bot Detection in MMORPGs](https://arxiv.org/abs/2508.20578)
*Jaeman Son,Hyunsoo Kim*

Main category: cs.AI

TL;DR: 本论文提出一种无监督的框架，通过对比表示学习和聚类技术检测MMORPG中的自动刷等级机器人，同时引入大型语言模型验证聚类结果，提升检测效率并确保解释性。


<details>
  <summary>Details</summary>
Motivation: 自动刷等级机器人通过自动程序快速提升游戏角色等级，破坏了游戏的平衡性与公平性，并且其拟人化的行为和处罚需要合法性解释使得检测变得尤为困难。

Method: 论文提出一种无监督框架，利用对比表示学习和聚类技术发现拥有相似升级模式的角色，同步引入大型语言模型和基于增长曲线的可视化工具，辅助验证和评估，最终实现机器与人工的协作检测。

Result: 框架能有效分辨出特定模式的角色群体，并通过辅助验证工具提升可靠性，同时优化了机器与人工的检测效率。

Conclusion: 该研究为大规模多人在线角色扮演游戏中的机器人检测提供了一种有效且可解释的方法，并能够在不影响用户体验的前提下进行更为可扩展和负责的管理。

Abstract: In Massively Multiplayer Online Role-Playing Games (MMORPGs), auto-leveling
bots exploit automated programs to level up characters at scale, undermining
gameplay balance and fairness. Detecting such bots is challenging, not only
because they mimic human behavior, but also because punitive actions require
explainable justification to avoid legal and user experience issues. In this
paper, we present a novel framework for detecting auto-leveling bots by
leveraging contrastive representation learning and clustering techniques in a
fully unsupervised manner to identify groups of characters with similar
level-up patterns. To ensure reliable decisions, we incorporate a Large
Language Model (LLM) as an auxiliary reviewer to validate the clustered groups,
effectively mimicking a secondary human judgment. We also introduce a growth
curve-based visualization to assist both the LLM and human moderators in
assessing leveling behavior. This collaborative approach improves the
efficiency of bot detection workflows while maintaining explainability, thereby
supporting scalable and accountable bot regulation in MMORPGs.

</details>


### [162] [Bridging Minds and Machines: Toward an Integration of AI and Cognitive Science](https://arxiv.org/abs/2508.20674)
*Rui Mao,Qian Liu,Xiao Li,Erik Cambria,Amir Hussain*

Main category: cs.AI

TL;DR: 本文探讨了认知科学和人工智能（AI）之间的交叉关系，强调了AI在表现与认知基础的双重角色。


<details>
  <summary>Details</summary>
Motivation: AI虽然取得了巨大进展，但其认知基础仍然缺乏连贯性，因此需要深入研究其认知科学背景。

Method: 通过综合认知科学和AI的关键贡献，分析其相互作用及未来发展的可能方向。

Result: 发现AI主要关注任务表现，而未充分解决认知基础的概念分裂问题。

Conclusion: AI未来在认知科学中的发展应不仅限于性能提升，还应关注为加深对人类心智的理解构建系统，建议方向包括认知行为一致性、体现在文化与个性化的认知模型以及重新思考AI伦理。

Abstract: Cognitive Science has profoundly shaped disciplines such as Artificial
Intelligence (AI), Philosophy, Psychology, Neuroscience, Linguistics, and
Culture. Many breakthroughs in AI trace their roots to cognitive theories,
while AI itself has become an indispensable tool for advancing cognitive
research. This reciprocal relationship motivates a comprehensive review of the
intersections between AI and Cognitive Science. By synthesizing key
contributions from both perspectives, we observe that AI progress has largely
emphasized practical task performance, whereas its cognitive foundations remain
conceptually fragmented. We argue that the future of AI within Cognitive
Science lies not only in improving performance but also in constructing systems
that deepen our understanding of the human mind. Promising directions include
aligning AI behaviors with cognitive frameworks, situating AI in embodiment and
culture, developing personalized cognitive models, and rethinking AI ethics
through cognitive co-evaluation.

</details>


### [163] [Transparent Semantic Spaces: A Categorical Approach to Explainable Word Embeddings](https://arxiv.org/abs/2508.20701)
*Ares Fabregat-Hernández,Javier Palanca,Vicent Botti*

Main category: cs.AI

TL;DR: 本论文提出了基于范畴论的框架以增强人工智能系统的可解释性，尤其针对词嵌入。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能系统尤其是深度学习模型如词嵌入, 缺乏可解释性，这限制了其可理解性和透明性。提出新的数学框架以改进这一缺点。

Method: 基于范畴论, 建立 $L_T$ 和 $P_T$ 范畴来表征文本语义, 使用 monoidal 类别 $P_T$ 提取语义信息, 并定义配置和词嵌入类别，以及比较词嵌入的方法。

Result: 证明了 GloVe 和 Word2Vec 算法以及 metric MDS 算法之间的等价性，将黑箱的神经网络算法转化为透明的数学框架。

Conclusion: 提出的数学框架在语义空间层面上计算和缓解基础偏差，推动了可解释人工智能领域的发展。

Abstract: The paper introduces a novel framework based on category theory to enhance
the explainability of artificial intelligence systems, particularly focusing on
word embeddings. Key topics include the construction of categories
$\mathcal{L}_T$ and $\mathcal{P}_T$, providing schematic representations of the
semantics of a text $ T $, and reframing the selection of the element with
maximum probability as a categorical notion. Additionally, the monoidal
category $\mathcal{P}_T$ is constructed to visualize various methods of
extracting semantic information from $T$, offering a dimension-agnostic
definition of semantic spaces reliant solely on information within the text.
  Furthermore, the paper defines the categories of configurations Conf and word
embeddings $\mathcal{Emb}$, accompanied by the concept of divergence as a
decoration on $\mathcal{Emb}$. It establishes a mathematically precise method
for comparing word embeddings, demonstrating the equivalence between the GloVe
and Word2Vec algorithms and the metric MDS algorithm, transitioning from neural
network algorithms (black box) to a transparent framework. Finally, the paper
presents a mathematical approach to computing biases before embedding and
offers insights on mitigating biases at the semantic space level, advancing the
field of explainable artificial intelligence.

</details>


### [164] [Re4: Scientific Computing Agent with Rewriting, Resolution, Review and Revision](https://arxiv.org/abs/2508.20729)
*Ao Cheng,Lei Zhang,Guowei He*

Main category: cs.AI

TL;DR: 该研究提出了一种整合“重写-解决-复审-修订”逻辑链的多语言模型框架，用于科学计算中的问题求解，并显著提高了代码生成的可靠性与成功率。


<details>
  <summary>Details</summary>
Motivation: 为了应对生成性人工智能无法稳定生成高质量代码的问题，研究者构建了一种多模块协作框架，用以解决科学计算中的典型问题。

Method: 该框架通过“顾问-程序员-审稿人”模块协作，结合知识转移、代码生成与运行、自我调试与反馈机制，迭代优化代码以解决问题。

Result: 在求解偏微分方程、不适定线性系统和数据驱动物理分析中，实验表明该框架显著提高了无Bug代码生成率，减少了不物理解的发生率。

Conclusion: 此框架展现了基于自然语言的自动化代码生成与复审在科学计算中的巨大潜力，确立了可靠有效的生成式计算范式。

Abstract: Large language models (LLMs) serve as an active and promising field of
generative artificial intelligence and have demonstrated abilities to perform
complex tasks in multiple domains, including mathematical and scientific
reasoning. In this work, we construct a novel agent framework for solving
representative problems in scientific computing. The proposed agent,
incorporating a "rewriting-resolution-review-revision" logical chain via three
reasoning LLMs (functioning as the Consultant, Reviewer, and Programmer,
respectively), is integrated in a collaborative and interactive manner. The
Consultant module endows the agent with knowledge transfer capabilities to link
problems to professional domain insights, thereby rewriting problem
descriptions through text augmentation. The Programmer module is responsible
for generating and executing well-structured code to deliver the problem
resolution. The Reviewer module equips the agent with the capacity for
self-debugging and self-refinement through interactive feedback with code
runtime outputs. By leveraging the end-to-end review mechanism, the executable
code provided by the Programmer attains the iterative revision. A comprehensive
evaluation is conducted on the performance of the proposed agent framework in
solving PDEs, ill-conditioned linear systems, and data-driven physical analysis
problems. Compared to single-model, this collaborative framework significantly
improves the bug-free code generation rate and reduces the occurrence of
non-physical solutions, thereby establishing a highly reliable framework for
autonomous code generation based on natural language descriptions. The review
mechanism improved the average execution success (bug-free code and non-NaN
solutions) rate of the latest reasoning models. In summary, our agent framework
establishes automatic code generation and review as a promising scientific
computing paradigm.

</details>


### [165] [Single Agent Robust Deep Reinforcement Learning for Bus Fleet Control](https://arxiv.org/abs/2508.20784)
*Yifan Zhang*

Main category: cs.AI

TL;DR: 本文提出一种单智能体强化学习框架，用于公交车辆控制，解决了多智能体强化学习中的数据不平衡和收敛问题，同时在逼真模拟条件下表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于多智能体强化学习的方法无法有效应对异构线路、动态需求和车队规模不同的真实公交运营问题，因此需要一种更加适应复杂情况的解决方案。

Method: 通过将多智能体问题重新定义为单智能体问题，使用高维编码（如车辆ID、站点ID、时段等分类变量和数值特征）来捕获代理间依赖性，并设计与运营目标一致的奖励函数。此外，使用改进的Soft Actor-Critic算法优化政策。

Result: 实验展示了改进Soft Actor-Critic比基线方法（如MADDPG）表现更稳定，性能更优越（在随机条件下成本得分为-430k对比-530k）。

Conclusion: 单智能体深度强化学习框架通过增强的分类结构和时间表感知奖励函数，可在非环形和复杂真实场景下有效管理公交调度，为多智能体方法提供了鲁棒且可扩展的替代方案。

Abstract: Bus bunching remains a challenge for urban transit due to stochastic traffic
and passenger demand. Traditional solutions rely on multi-agent reinforcement
learning (MARL) in loop-line settings, which overlook realistic operations
characterized by heterogeneous routes, timetables, fluctuating demand, and
varying fleet sizes. We propose a novel single-agent reinforcement learning
(RL) framework for bus holding control that avoids the data imbalance and
convergence issues of MARL under near-realistic simulation. A bidirectional
timetabled network with dynamic passenger demand is constructed. The key
innovation is reformulating the multi-agent problem into a single-agent one by
augmenting the state space with categorical identifiers (vehicle ID, station
ID, time period) in addition to numerical features (headway, occupancy,
velocity). This high-dimensional encoding enables single-agent policies to
capture inter-agent dependencies, analogous to projecting non-separable inputs
into a higher-dimensional space. We further design a structured reward function
aligned with operational goals: instead of exponential penalties on headway
deviations, a ridge-shaped reward balances uniform headways and schedule
adherence. Experiments show that our modified soft actor-critic (SAC) achieves
more stable and superior performance than benchmarks, including MADDPG (e.g.,
-430k vs. -530k under stochastic conditions). These results demonstrate that
single-agent deep RL, when enhanced with categorical structuring and
schedule-aware rewards, can effectively manage bus holding in non-loop,
real-world contexts. This paradigm offers a robust, scalable alternative to
MARL frameworks, particularly where agent-specific experiences are imbalanced.

</details>


### [166] [A Graph-Based Test-Harness for LLM Evaluation](https://arxiv.org/abs/2508.20810)
*Jessica Lundin,Guillaume Chabot-Couture*

Main category: cs.AI

TL;DR: 本文提出一种动态、系统化的医疗指导基准测试原型，基于图结构生成问题，用以评估模型在400多个问题上的能力，展现了其在症状识别上的优势与其他全域评估方法的差距。


<details>
  <summary>Details</summary>
Motivation: 目前的基准测试存在覆盖率局限等问题，而手动生成基准数据集成本高昂且易受污染。因此，作者想开发一种动态且可扩展的基准测试方法，同时可涵盖所有医疗指导关系。

Method: 作者将WHO IMCI手册转化为一个含200多个节点和300多条边的有向图，通过图遍历生成包含年龄特定情景和临床干扰因素的相关问题，并进一步用于评估模型。此外，结合动态MCQA方法，开发了可支持模型细化训练的高效生成机制。

Result: 使用该方法，模型在临床任务上的表现准确率为45%-67%，其中在症状识别领域表现突出，但在严重程度分类、治疗协议及后续护理等方面表现较弱。这一系统很好地解决了传统手动基准测试覆盖率不足的问题。

Conclusion: 该方法是一种向动态、全面的医疗指导基准测试迈进的尝试，能够生成抗污染的基准数据，并提升模型的针对性训练能力。

Abstract: We present a first known prototype of a dynamic, systematic benchmark of
medical guidelines for 400+ questions, with 3.3+ trillion possible
combinations, covering 100\% of guideline relationships. We transformed the WHO
IMCI handbook into a directed graph with 200+ nodes (conditions, symptoms,
treatments, follow-ups, severities) and 300+ edges, then used graph traversal
to generate questions that incorporated age-specific scenarios and contextual
distractors to ensure clinical relevance. Our graph-based approach enables
systematic evaluation across clinical tasks (45-67\% accuracy), and we find
models excel at symptom recognition but struggle with triaging severity,
treatment protocols and follow-up care, demonstrating how customized benchmarks
can identify specific capability gaps that general-domain evaluations miss.
Beyond evaluation, this dynamic MCQA methodology enhances LLM post-training
(supervised finetuning, GRPO, DPO), where correct answers provide high-reward
samples without expensive human annotation. The graph-based approach
successfully addresses the coverage limitations of manually curated benchmarks.
This methodology is a step toward scalable, contamination-resistant solution
for creating comprehensive benchmarks that can be dynamically generated,
including when the guidelines are updated. Code and datasets are available at
https://github.com/jessicalundin/graph_testing_harness

</details>


### [167] [A Multi-Objective Genetic Algorithm for Healthcare Workforce Scheduling](https://arxiv.org/abs/2508.20953)
*Vipul Patel,Anirudh Deodhar,Dagnachew Birru*

Main category: cs.AI

TL;DR: 提出一种多目标遗传算法（MOO-GA）解决医疗单位的多目标排班问题，利用真实数据验证其优化效果并显示显著改进。


<details>
  <summary>Details</summary>
Motivation: 医疗行业排班需平衡成本、患者需求与员工偏好，现存手工或传统方式难以高效应对复杂多目标需求。

Method: 设计多目标遗传算法（MOO-GA），根据成本、护理覆盖率和员工满意度目标函数进行优化，并处理复杂约束如模块化排班和多技能需求。

Result: MOO-GA相较传统手工排班提高了66%的效率，且生成的排班方案在成本、覆盖率和员工满意度间展现出优质平衡。

Conclusion: MOO-GA是一个实用的决策支持工具，能够有效平衡医疗排班中的运营与员工需求，为护理与行政管理者提供参考。

Abstract: Workforce scheduling in the healthcare sector is a significant operational
challenge, characterized by fluctuating patient loads, diverse clinical skills,
and the critical need to control labor costs while upholding high standards of
patient care. This problem is inherently multi-objective, demanding a delicate
balance between competing goals: minimizing payroll, ensuring adequate staffing
for patient needs, and accommodating staff preferences to mitigate burnout. We
propose a Multi-objective Genetic Algorithm (MOO-GA) that models the hospital
unit workforce scheduling problem as a multi-objective optimization task. Our
model incorporates real-world complexities, including hourly appointment-driven
demand and the use of modular shifts for a multi-skilled workforce. By defining
objective functions for cost, patient care coverage, and staff satisfaction,
the GA navigates the vast search space to identify a set of high-quality,
non-dominated solutions. Demonstrated on datasets representing a typical
hospital unit, the results show that our MOO-GA generates robust and balanced
schedules. On average, the schedules produced by our algorithm showed a 66\%
performance improvement over a baseline that simulates a conventional, manual
scheduling process. This approach effectively manages trade-offs between
critical operational and staff-centric objectives, providing a practical
decision support tool for nurse managers and hospital administrators.

</details>


### [168] [Efficient Neuro-Symbolic Learning of Constraints and Objective](https://arxiv.org/abs/2508.20978)
*Marianne Defresne,Romain Gambardella,Sophie Barbe,Thomas Schiex*

Main category: cs.AI

TL;DR: 本文引入了一种可微分的神经符号体系结构及损失函数，用于从自然输入中学习解决NP难问题，提供了更高效的训练和准确推理能力。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种能够高效处理离散推理和优化任务的神经架构，弥补大语言模型在此类任务中的不足。

Method: 提出了一种新的概率损失函数，能够同时学习约束和目标；将组合求解器移出训练环节，以提高训练的可扩展性，同时通过精确推理提升精度。

Result: 在数独和视觉任务等基准测试中，该方法以远低于其他混合方法的训练时间实现了高效学习。在蛋白质设计问题等实际应用中也表现出优越性。

Conclusion: 该架构不仅可以从自然输入中高效学习解决NP难问题，还在准确性与训练效率上表现出显著提升，为神经符号系统的发展提供了新方向。

Abstract: In the ongoing quest for hybridizing discrete reasoning with neural nets,
there is an increasing interest in neural architectures that can learn how to
solve discrete reasoning or optimization problems from natural inputs, a task
that Large Language Models seem to struggle with.
  Objectives: We introduce a differentiable neuro-symbolic architecture and a
loss function dedicated to learning how to solve NP-hard reasoning problems.
  Methods: Our new probabilistic loss allows for learning both the constraints
and the objective, thus delivering a complete model that can be scrutinized and
completed with side constraints. By pushing the combinatorial solver out of the
training loop, our architecture also offers scalable training while exact
inference gives access to maximum accuracy.
  Results: We empirically show that it can efficiently learn how to solve
NP-hard reasoning problems from natural inputs. On three variants of the Sudoku
benchmark -- symbolic, visual, and many-solution --, our approach requires a
fraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut
task, it optimizes the regret better than a Decision-Focused-Learning
regret-dedicated loss. Finally, it efficiently learns the energy optimization
formulation of the large real-world problem of designing proteins.

</details>


### [169] [ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic Support in Addiction Recovery](https://arxiv.org/abs/2508.20996)
*Junda Wang,Zonghai Yao,Zhichao Yang,Lingxi Li,Junhui Qian,Hong Yu*

Main category: cs.AI

TL;DR: 这篇文章介绍了ChatThero，一种用于治疗物质使用障碍（SUDs）的多代理对话系统，其整合了动态患者建模、基于CBT（认知行为疗法）和MI（动机访谈）的对话策略并经过精准优化。


<details>
  <summary>Details</summary>
Motivation: SUDs影响全球3600万人，但因污名化、动机障碍和个性化支持有限，患者难以获得有效治疗。现有智能对话系统在成瘾康复中缺乏与临床验证策略的深度整合，限制了其实际效果。

Method: 提出了一种名为ChatThero的框架，结合动态建模和情境感知治疗对话。设计了易、中、难三个层级的综合基准，采用双阶段优化方法，先通过监督微调（SFT），再利用直接偏好优化（DPO）提升模型性能。

Result: ChatThero提高患者41.5%的治疗动机，治疗信心提升0.49%，在难处理的案例中对话轮次减少26%，并在人类和自动化评估中表现出更高的共情性、响应性和行为真实感。

Conclusion: 该框架为隐私保护下的治疗对话研究提供了坚实基础，同时具备临床验证和推广潜力，为成瘾康复带来了新的研究和应用方向。

Abstract: Substance use disorders (SUDs) affect over 36 million people worldwide, yet
few receive effective care due to stigma, motivational barriers, and limited
personalized support. Although large language models (LLMs) show promise for
mental-health assistance, most systems lack tight integration with clinically
validated strategies, reducing effectiveness in addiction recovery. We present
ChatThero, a multi-agent conversational framework that couples dynamic patient
modeling with context-sensitive therapeutic dialogue and adaptive persuasive
strategies grounded in cognitive behavioral therapy (CBT) and motivational
interviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy,
Medium, and Hard resistance levels, and train ChatThero with a two-stage
pipeline comprising supervised fine-tuning (SFT) followed by direct preference
optimization (DPO). In evaluation, ChatThero yields a 41.5\% average gain in
patient motivation, a 0.49\% increase in treatment confidence, and resolves
hard cases with 26\% fewer turns than GPT-4o, and both automated and human
clinical assessments rate it higher in empathy, responsiveness, and behavioral
realism. The framework supports rigorous, privacy-preserving study of
therapeutic conversation and provides a robust, replicable basis for research
and clinical translation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [170] [CrystalICL: Enabling In-Context Learning for Crystal Generation](https://arxiv.org/abs/2508.20143)
*Ruobing Wang,Qiaoyu Tan,Yili Wang,Ying Wang,Xin Wang*

Main category: cs.LG

TL;DR: 提出了一种面向少样本晶体生成的新模型CrystalICL，使用空间群为基础的晶体token化方法和混合指令调优框架，实验表明其在多种晶体生成任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的晶体生成方法主要局限于零样本场景，而无法充分利用少样本场景的潜力，因此需要新的方法模拟人类专家通过修改已知结构设计新材料的能力。

Method: 引入基于空间群的晶体token化方法及一种结构-属性相关的混合指令调优框架，同时提出了多任务指令调优策略，从而更好地捕捉结构与属性之间的关系并提高少样本学习能力。

Result: 在四个晶体生成基准任务上，通过广泛实验表明CrystalICL在条件和无条件生成任务中均优于现有主流方法。

Conclusion: CrystalICL通过结合晶体对称性建模和混合指令调优，实现了少样本晶体生成的突破，验证了其优越的生成能力和实际应用潜力。

Abstract: Designing crystal materials with desired physicochemical properties remains a
fundamental challenge in materials science. While large language models (LLMs)
have demonstrated strong in-context learning (ICL) capabilities, existing
LLM-based crystal generation approaches are limited to zero-shot scenarios and
are unable to benefit from few-shot scenarios. In contrast, human experts
typically design new materials by modifying relevant known structures which
aligns closely with the few-shot ICL paradigm. Motivated by this, we propose
CrystalICL, a novel model designed for few-shot crystal generation.
Specifically, we introduce a space-group based crystal tokenization method,
which effectively reduces the complexity of modeling crystal symmetry in LLMs.
We further introduce a condition-structure aware hybrid instruction tuning
framework and a multi-task instruction tuning strategy, enabling the model to
better exploit ICL by capturing structure-property relationships from limited
data. Extensive experiments on four crystal generation benchmarks demonstrate
the superiority of CrystalICL over the leading baseline methods on conditional
and unconditional generation tasks.

</details>


### [171] [Filter then Attend: Improving attention-based Time Series Forecasting with Spectral Filtering](https://arxiv.org/abs/2508.20206)
*Elisha Dayag,Nhat Thanh Van Tran,Jack Xin*

Main category: cs.LG

TL;DR: 探索在transformer模型中添加可学习的频率滤波器以提高长时间序列预测的性能。


<details>
  <summary>Details</summary>
Motivation: 基于transformer的模型在长时间序列预测中有着固有的低频偏向，并且消耗了高计算和内存资源，研究动机是希望改善这些问题。

Method: 在transformer模型前加入可学习的滤波器，并通过实验验证其对长时间序列预测的性能改进效果。

Result: 通过加入滤波器，多个基于transformer的模型在预测性能上提高了5-10%，同时减少了模型的嵌入维度，使模型更小且更高效。

Conclusion: 加入滤波器可以使基于transformer的模型更加高效，同时克服其低频偏向的问题，更好地利用全频谱进行预测。

Abstract: Transformer-based models are at the forefront in long time-series forecasting
(LTSF). While in many cases, these models are able to achieve state of the art
results, they suffer from a bias toward low-frequencies in the data and high
computational and memory requirements. Recent work has established that
learnable frequency filters can be an integral part of a deep forecasting model
by enhancing the model's spectral utilization. These works choose to use a
multilayer perceptron to process their filtered signals and thus do not solve
the issues found with transformer-based models. In this paper, we establish
that adding a filter to the beginning of transformer-based models enhances
their performance in long time-series forecasting. We add learnable filters,
which only add an additional $\approx 1000$ parameters to several
transformer-based models and observe in multiple instances 5-10 \% relative
improvement in forecasting performance. Additionally, we find that with filters
added, we are able to decrease the embedding dimension of our models, resulting
in transformer-based architectures that are both smaller and more effective
than their non-filtering base models. We also conduct synthetic experiments to
analyze how the filters enable Transformer-based models to better utilize the
full spectrum for forecasting.

</details>


### [172] [What can we learn from signals and systems in a transformer? Insights for probabilistic modeling and inference architecture](https://arxiv.org/abs/2508.20211)
*Heng-Sheng Chang,Prashant G. Mehta*

Main category: cs.LG

TL;DR: 本文提出将Transformer解释为一种非线性预测模型，其中信号被看作条件测度的代理，层操作被看作固定点更新，并特别对隐马尔可夫模型给出了明确的固定点更新形式。


<details>
  <summary>Details</summary>
Motivation: 通过将经典的非线性滤波理论与现代推理架构相结合，尝试为Transformer的工作原理提供一个概率模型视角。

Method: 引入一个概率模型，将Transformer信号解释为条件测度的代理，层操作解释为固定点更新，并具体分析了隐马尔可夫模型下的固定点更新形式。

Result: 证明了在隐马尔可夫模型的特定案例下，Transformer的层操作可以被解释为概率模型中的固定点更新。

Conclusion: 这种解释不仅帮助理解Transformer，也为连接经典滤波理论与现代深度学习提供了可能性。

Abstract: In the 1940s, Wiener introduced a linear predictor, where the future
prediction is computed by linearly combining the past data. A transformer
generalizes this idea: it is a nonlinear predictor where the next-token
prediction is computed by nonlinearly combining the past tokens. In this essay,
we present a probabilistic model that interprets transformer signals as
surrogates of conditional measures, and layer operations as fixed-point
updates. An explicit form of the fixed-point update is described for the
special case when the probabilistic model is a hidden Markov model (HMM). In
part, this paper is in an attempt to bridge the classical nonlinear filtering
theory with modern inference architectures.

</details>


### [173] [The Role of Teacher Calibration in Knowledge Distillation](https://arxiv.org/abs/2508.20224)
*Suyoung Kim,Seonguk Park,Junhoo Lee,Nojun Kwak*

Main category: cs.LG

TL;DR: 研究揭示教师模型校准误差与学生模型准确率的强相关性，并提出校准教师模型以提高知识蒸馏效果的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管知识蒸馏技术已取得显著成功，但尚未完全理解哪些因素能提升学生模型的性能。

Method: 通过分析教师模型校准误差与学生模型性能的关系，提出改进教师模型校准的算法，并验证其通用性和易集成性。

Result: 所提算法在分类和检测等任务中表现优越，且能够与多种现有先进方法结合取得一致性更优的性能。

Conclusion: 教师模型的校准对于知识蒸馏的有效性十分重要，利用校准方法可显著提高学生模型的性能。

Abstract: Knowledge Distillation (KD) has emerged as an effective model compression
technique in deep learning, enabling the transfer of knowledge from a large
teacher model to a compact student model. While KD has demonstrated significant
success, it is not yet fully understood which factors contribute to improving
the student's performance. In this paper, we reveal a strong correlation
between the teacher's calibration error and the student's accuracy. Therefore,
we claim that the calibration of the teacher model is an important factor for
effective KD. Furthermore, we demonstrate that the performance of KD can be
improved by simply employing a calibration method that reduces the teacher's
calibration error. Our algorithm is versatile, demonstrating effectiveness
across various tasks from classification to detection. Moreover, it can be
easily integrated with existing state-of-the-art methods, consistently
achieving superior performance.

</details>


### [174] [Coresets from Trajectories: Selecting Data via Correlation of Loss Differences](https://arxiv.org/abs/2508.20230)
*Manish Nagaraj,Deepak Ravikumar,Kaushik Roy*

Main category: cs.LG

TL;DR: 该研究提出了一种名为CLD（损失差异相关性）的高效方法，用于核心集选择，从而优化深度学习模型训练数据的规模和有效性。


<details>
  <summary>Details</summary>
Motivation: 针对深度学习模型在实时和资源受限环境中的可扩展性问题，提出一种直接高效的解决方案。

Method: 引入CLD方法，通过衡量训练样本与验证集损失轨迹的对齐程度，计算每个样本的重要性，从而选择高影响力训练样本，避免高成本的梯度和曲率计算。

Result: CLD方法在CIFAR-100和ImageNet-1k数据集上的表现优于或接近现有最优方法，跨架构（如ResNet、VGG、DenseNet）迁移表现良好，并能够在不牺牲精度的情况下减少计算成本。

Conclusion: CLD是一种高效、稳定、可迁移且具有理论保证的核心集选择工具，可在最小精度损失的前提下优化深度学习模型的训练集。

Abstract: Deep learning models achieve state-of-the-art performance across domains but
face scalability challenges in real-time or resource-constrained scenarios. To
address this, we propose Correlation of Loss Differences (CLD), a simple and
scalable metric for coreset selection that identifies the most impactful
training samples by measuring their alignment with the loss trajectories of a
held-out validation set. CLD is highly efficient, requiring only per-sample
loss values computed at training checkpoints, and avoiding the costly gradient
and curvature computations used in many existing subset selection methods. We
develop a general theoretical framework that establishes convergence guarantees
for CLD-based coresets, demonstrating that the convergence error is
upper-bounded by the alignment of the selected samples and the
representativeness of the validation set. On CIFAR-100 and ImageNet-1k,
CLD-based coresets typically outperform or closely match state-of-the-art
methods across subset sizes, and remain within 1% of more computationally
expensive baselines even when not leading. CLD transfers effectively across
architectures (ResNet, VGG, DenseNet), enabling proxy-to-target selection with
<1% degradation. Moreover, CLD is stable when using only early checkpoints,
incurring negligible accuracy loss. Finally, CLD exhibits inherent bias
reduction via per-class validation alignment, obviating the need for additional
stratified sampling. Together, these properties make CLD a principled,
efficient, stable, and transferable tool for scalable dataset optimization.

</details>


### [175] [Bounds on Perfect Node Classification: A Convex Graph Clustering Perspective](https://arxiv.org/abs/2508.20231)
*Firooz Shahriari-Mehr,Javad Aliakbari,Alexandre Graell i Amat,Ashkan Panahi*

Main category: cs.LG

TL;DR: 本文分析了在包含符合节点标签和特征的社区的图上的节点分类问题，并提出了结合节点信息和谱图聚类的新优化问题，证明了图结构和节点信息间的协同作用。


<details>
  <summary>Details</summary>
Motivation: 探索节点分类问题，通过引入更弱的条件约束，优化节点信息与图结构的结合以提升分类性能。

Method: 通过将节点特定信息（标签和特征）整合到谱图聚类框架中，提出了全新的优化问题，并提供了算法解决方案与数值实验验证。

Result: 在更宽松条件下，实现了通过优化问题解决社区完美复原，同时验证了图结构与节点特定信息的协同作用。

Conclusion: 结合节点信息和图结构，可以显著提升节点分类效果，并在理论和实验上验证了协同效应的存在。

Abstract: We present an analysis of the transductive node classification problem, where
the underlying graph consists of communities that agree with the node labels
and node features. For node classification, we propose a novel optimization
problem that incorporates the node-specific information (labels and features)
in a spectral graph clustering framework. Studying this problem, we demonstrate
a synergy between the graph structure and node-specific information. In
particular, we show that suitable node-specific information guarantees the
solution of our optimization problem perfectly recovering the communities,
under milder conditions than the bounds on graph clustering alone. We present
algorithmic solutions to our optimization problem and numerical experiments
that confirm such a synergy.

</details>


### [176] [Beyond Optimization: Exploring Novelty Discovery in Autonomous Experiments](https://arxiv.org/abs/2508.20254)
*Ralph Bulanadi,Jawad Chowdhury,Funakubo Hiroshi,Maxim Ziatdinov,Rama Vasudevan,Arpan Biswas,Yongtao Liu*

Main category: cs.LG

TL;DR: INS2ANE提出了一种新的框架，通过整合新奇性评分系统和策略采样机制，在自主实验中促进新现象的发现，并在自动扫描探针显微镜实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的自主实验主要集中于优化预定义目标，限制了对未知物理现象的探索，需求在自动化科研中延伸至新现象的发现。

Method: 提出INS2ANE框架，结合新奇性评分系统评估实验结果的独特性，以及策略采样机制探索被传统标准认为不太有前景的区域。

Result: 与传统优化流程相比，INS2ANE明显增加了探索现象的多样性，提高了发现新现象的可能性。

Conclusion: INS2ANE展示了自主实验在拓展科学研究深度和效率方面的潜力，有助于加速科学发现新现象的过程。

Abstract: Autonomous experiments (AEs) are transforming how scientific research is
conducted by integrating artificial intelligence with automated experimental
platforms. Current AEs primarily focus on the optimization of a predefined
target; while accelerating this goal, such an approach limits the discovery of
unexpected or unknown physical phenomena. Here, we introduce a novel framework,
INS2ANE (Integrated Novelty Score-Strategic Autonomous Non-Smooth Exploration),
to enhance the discovery of novel phenomena in autonomous experimentation. Our
method integrates two key components: (1) a novelty scoring system that
evaluates the uniqueness of experimental results, and (2) a strategic sampling
mechanism that promotes exploration of under-sampled regions even if they
appear less promising by conventional criteria. We validate this approach on a
pre-acquired dataset with a known ground truth comprising of image-spectral
pairs. We further implement the process on autonomous scanning probe microscopy
experiments. INS2ANE significantly increases the diversity of explored
phenomena in comparison to conventional optimization routines, enhancing the
likelihood of discovering previously unobserved phenomena. These results
demonstrate the potential for AE to enhance the depth of scientific discovery;
in combination with the efficiency provided by AEs, this approach promises to
accelerate scientific research by simultaneously navigating complex
experimental spaces to uncover new phenomena.

</details>


### [177] [Discovering equations from data: symbolic regression in dynamical systems](https://arxiv.org/abs/2508.20257)
*Beatriz R. Brum,Luiza Lober,Isolde Previdelli,Francisco A. Rodrigues*

Main category: cs.LG

TL;DR: 本研究比较了五种符号回归方法，其中PySR方法在从混沌动力学和流行病模型等九种动态过程中恢复方程方面表现最佳，预测能力和准确性均显著。


<details>
  <summary>Details</summary>
Motivation: 探索机器学习中的符号回归方法在从数据中发现方程的能力，以支持物理学、数学生态学和流行病学等领域的研究。

Method: 使用五种符号回归方法测试并从九个不同的动力系统（包括混沌动力学和流行病模型）中恢复方程，比较方法的准确性与预测能力。

Result: 研究发现PySR方法在推导方程的预测能力和准确性方面表现出色，其部分估计结果与原始解析形式几乎无法区分。

Conclusion: 符号回归是一种具有很高潜力的工具，可用于推导和建模现实世界中的复杂现象，特别是PySR方法表现出了极高的适用性和精确性。

Abstract: The process of discovering equations from data lies at the heart of physics
and in many other areas of research, including mathematical ecology and
epidemiology. Recently, machine learning methods known as symbolic regression
have automated this process. As several methods are available in the
literature, it is important to compare them, particularly for dynamic systems
that describe complex phenomena. In this paper, five symbolic regression
methods were used for recovering equations from nine dynamical processes,
including chaotic dynamics and epidemic models, with the PySR method proving to
be the most suitable for inferring equations. Benchmark results demonstrate its
high predictive power and accuracy, with some estimates being indistinguishable
from the original analytical forms. These results highlight the potential of
symbolic regression as a robust tool for inferring and modelling real-world
phenomena.

</details>


### [178] [Latent Variable Modeling for Robust Causal Effect Estimation](https://arxiv.org/abs/2508.20259)
*Tetsuro Morimura,Tatsushi Oka,Yugo Suzuki,Daisuke Moriwaki*

Main category: cs.LG

TL;DR: 本研究提出了一种将潜变量模型整合到双重机器学习（DML）中的框架，解决因隐藏因素导致的因果效应估计问题，并通过实验验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 在因果推断中，隐藏的潜在因素可能影响处理或结果，而现有方法难以应对未观测变量。本研究旨在结合潜变量模型和DML框架，以解决这些挑战。

Method: 提出了一个新的框架：在DML的第二阶段引入潜变量，区分表示学习与潜变量推断；研究场景包括：潜变量仅影响结果，以及潜变量同时影响处理与结果。

Result: 通过广泛的合成数据和真实世界数据实验，证明了该方法的可靠性和有效性。

Conclusion: 将潜变量整合到DML框架中是提高因果效应估计的一个有效方法，特别是在存在隐藏因素的情况下展现了强大的鲁棒性。

Abstract: Latent variable models provide a powerful framework for incorporating and
inferring unobserved factors in observational data. In causal inference, they
help account for hidden factors influencing treatment or outcome, thereby
addressing challenges posed by missing or unmeasured covariates. This paper
proposes a new framework that integrates latent variable modeling into the
double machine learning (DML) paradigm to enable robust causal effect
estimation in the presence of such hidden factors. We consider two scenarios:
one where a latent variable affects only the outcome, and another where it may
influence both treatment and outcome. To ensure tractability, we incorporate
latent variables only in the second stage of DML, separating representation
learning from latent inference. We demonstrate the robustness and effectiveness
of our method through extensive experiments on both synthetic and real-world
datasets.

</details>


### [179] [Generalizable AI Model for Indoor Temperature Forecasting Across Sub-Saharan Africa](https://arxiv.org/abs/2508.20260)
*Zainab Akhtar,Eunice Jengo,Björn Haßler*

Main category: cs.LG

TL;DR: 该研究提出了一种轻量级、领域驱动的AI模型，用于预测撒哈拉以南非洲自然通风学校和家庭的室内温度，模型表现强劲，均方误差较低。


<details>
  <summary>Details</summary>
Motivation: 撒哈拉以南非洲资源有限地区对热舒适管理有较高需求，但室内温度预测模型发展较少且不健全。

Method: 扩展了Temp-AI-Estimator框架，基于坦桑尼亚的学校数据进行训练，并分别在尼日利亚学校和冈比亚家庭进行评估。

Result: 模型只需最小的输入即可提供强大的跨国性能，尼日利亚学校和冈比亚家庭的平均绝对误差分别为1.45℃和0.65℃。

Conclusion: 研究表明AI可以为资源受限环境中的热舒适管理提供有效支持。

Abstract: This study presents a lightweight, domain-informed AI model for predicting
indoor temperatures in naturally ventilated schools and homes in Sub-Saharan
Africa. The model extends the Temp-AI-Estimator framework, trained on Tanzanian
school data, and evaluated on Nigerian schools and Gambian homes. It achieves
robust cross-country performance using only minimal accessible inputs, with
mean absolute errors of 1.45{\deg}C for Nigerian schools and 0.65{\deg}C for
Gambian homes. These findings highlight AI's potential for thermal comfort
management in resource-constrained environments.

</details>


### [180] [A Systematic Review on the Generative AI Applications in Human Medical Genomics](https://arxiv.org/abs/2508.20275)
*Anton Changalidis,Yury Barbitoff,Yulia Nasykhova,Andrey Glotov*

Main category: cs.LG

TL;DR: 系统回顾了LLMs在遗传研究和诊断中的应用，探讨其主要进展及面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统统计技术和机器学习在应对复杂、高维数据方面存在局限，而LLMs通过其上下文理解能力表现卓越，可能为遗传研究和诊断提供新的突破。

Method: 通过自动关键字在多数据库中搜索涉及LLMs在遗传学诊断和教育领域的研究，并筛选掉无关或过时的模型，共评估了172项研究。

Result: LLMs显著改进了疾病分层、变异解读、医学影像分析等，但在多模态数据整合与临床适用性方面仍面临挑战。

Conclusion: LLMs能够变革遗传病诊断及支持遗传学教育，但需解决其通用性与实际临床应用中的不足。

Abstract: Although traditional statistical techniques and machine learning methods have
contributed significantly to genetics and, in particular, inherited disease
diagnosis, they often struggle with complex, high-dimensional data, a challenge
now addressed by state-of-the-art deep learning models. Large language models
(LLMs), based on transformer architectures, have excelled in tasks requiring
contextual comprehension of unstructured medical data. This systematic review
examines the role of LLMs in the genetic research and diagnostics of both rare
and common diseases. Automated keyword-based search in PubMed, bioRxiv,
medRxiv, and arXiv was conducted, targeting studies on LLM applications in
diagnostics and education within genetics and removing irrelevant or outdated
models. A total of 172 studies were analyzed, highlighting applications in
genomic variant identification, annotation, and interpretation, as well as
medical imaging advancements through vision transformers. Key findings indicate
that while transformer-based models significantly advance disease and risk
stratification, variant interpretation, medical imaging analysis, and report
generation, major challenges persist in integrating multimodal data (genomic
sequences, imaging, and clinical records) into unified and clinically robust
pipelines, facing limitations in generalizability and practical implementation
in clinical settings. This review provides a comprehensive classification and
assessment of the current capabilities and limitations of LLMs in transforming
hereditary disease diagnostics and supporting genetic education, serving as a
guide to navigate this rapidly evolving field.

</details>


### [181] [Objective Value Change and Shape-Based Accelerated Optimization for the Neural Network Approximation](https://arxiv.org/abs/2508.20290)
*Pengcheng Xie,Zihao Zhou,Zijian Zhou*

Main category: cs.LG

TL;DR: 该论文引入了一个用于神经网络逼近任务的新度量VC（Value Change），提供对神经网络局部性能和行为的定量测量，并提出了预处理框架及相关加速方法。


<details>
  <summary>Details</summary>
Motivation: 神经网络在关键应用中不可预测的局部性能可能会降低其可靠性，需要新方法来量化和改善这种表现。

Method: 引入VC度量以量化网络局部行为变化，并基于此提出新的函数距离度量和预处理框架，用以改进网络逼近性能。

Result: 通过数值实验及实际问题验证了VC的发现以及预处理方法的效果，尤其在实际和PDE相关科学问题中展现了加速能力。

Conclusion: VC度量及其衍生方法为改善神经网络逼近的稳定性和性能提供了实用工具，并提升了在实际应用中的可用性和效率。

Abstract: This paper introduce a novel metric of an objective function f, we say VC
(value change) to measure the difficulty and approximation affection when
conducting an neural network approximation task, and it numerically supports
characterizing the local performance and behavior of neural network
approximation. Neural networks often suffer from unpredictable local
performance, which can hinder their reliability in critical applications. VC
addresses this issue by providing a quantifiable measure of local value changes
in network behavior, offering insights into the stability and performance for
achieving the neural-network approximation. We investigate some fundamental
theoretical properties of VC and identified two intriguing phenomena in neural
network approximation: the VC-tendency and the minority-tendency. These trends
respectively characterize how pointwise errors evolve in relation to the
distribution of VC during the approximation process.In addition, we propose a
novel metric based on VC, which measures the distance between two functions
from the perspective of variation. Building upon this metric, we further
propose a new preprocessing framework for neural network approximation.
Numerical results including the real-world experiment and the PDE-related
scientific problem support our discovery and pre-processing acceleration
method.

</details>


### [182] [Beacon: Post-Training Quantization with Integrated Grid Selection](https://arxiv.org/abs/2508.20293)
*Shihao Zhang,Rayan Saab*

Main category: cs.LG

TL;DR: 本文提出了Beacon算法，一种免调参的通道性后训练量化方法，大幅简化了权重量化过程。


<details>
  <summary>Details</summary>
Motivation: 减少大规模预训练模型的内存和计算成本，同时避免通过启发式调参或网格搜索确定量化比例的繁琐过程。

Method: 为每个通道在固定的非比例字母表中直接执行后训练量化，并基于对称量化几何特性自动选择最优量化比例。

Result: Beacon无需回传计算和大规模校准集，支持对称和非对称量化，且达到与现有方法相竞争的性能。

Conclusion: Beacon方法简单有效，免调参且性能优越，是高效模型部署的实用解决方案。

Abstract: Quantization is a widely used compression technique for reducing the memory
and computation costs of large pre-trained models. A key challenge in
per-channel post-training quantization (PTQ) is selecting appropriate scaling
factors to replace weight values with values from a scaled quantization grid.
Existing methods typically fix the scale at the outset via heuristic tuning or
grid search. In this note, we propose Beacon, a simple and effective algorithm
that eliminates the need for such manual tuning. Beacon performs per-channel
PTQ directly using a fixed non-scaled alphabet and automatically determines the
optimal scaling factors by exploiting the geometry of symmetric scalar
quantization. It supports both symmetric and asymmetric quantization with
minimal modifications and does not rely on back-propagation or large
calibration sets. Despite its simplicity and tuning-free nature, Beacon
achieves competitive performance compared to state-of-the-art methods, making
it a practical solution for efficient model deployment.

</details>


### [183] [Dynamics-Aligned Latent Imagination in Contextual World Models for Zero-Shot Generalization](https://arxiv.org/abs/2508.20294)
*Frank Röder,Jan Benad,Manfred Eppe,Pradeep Kr. Banerjee*

Main category: cs.LG

TL;DR: 文章提出了DALI框架，通过推断潜在上下文表示来处理复杂环境的自适应问题。


<details>
  <summary>Details</summary>
Motivation: 现实强化学习需要在无需重新训练的情况下适应新的环境条件。然而，现有方法通常依赖显式上下文变量，限制了在潜在或难以测量的上下文中的应用。

Method: 针对传统方法的局限性，作者整合了Dreamer架构中的DALI，利用一种自监督编码器从代理和环境的交互中推断潜在上下文表示，并通过预测前向动态训练编码器。

Result: DALI在复杂的cMDP基准测试中表现出显著的优势，超越了许多不感知上下文的基线方法，并在某些情况下超过了显式上下文方法，能够在零样本条件下推广到未见过的上下文变动。

Conclusion: DALI框架通过创造物理一致的潜在空间，在感知和控制间架起了桥梁，为环境的高效推断和稳健推广奠定了理论基础，同时提供了实践中的性能优势。

Abstract: Real-world reinforcement learning demands adaptation to unseen environmental
conditions without costly retraining. Contextual Markov Decision Processes
(cMDP) model this challenge, but existing methods often require explicit
context variables (e.g., friction, gravity), limiting their use when contexts
are latent or hard to measure. We introduce Dynamics-Aligned Latent Imagination
(DALI), a framework integrated within the Dreamer architecture that infers
latent context representations from agent-environment interactions. By training
a self-supervised encoder to predict forward dynamics, DALI generates
actionable representations conditioning the world model and policy, bridging
perception and control. We theoretically prove this encoder is essential for
efficient context inference and robust generalization. DALI's latent space
enables counterfactual consistency: Perturbing a gravity-encoding dimension
alters imagined rollouts in physically plausible ways. On challenging cMDP
benchmarks, DALI achieves significant gains over context-unaware baselines,
often surpassing context-aware baselines in extrapolation tasks, enabling
zero-shot generalization to unseen contextual variations.

</details>


### [184] [FedReFT: Federated Representation Fine-Tuning with All-But-Me Aggregation](https://arxiv.org/abs/2508.20295)
*Fatema Siddika,Md Anwar Hossen,J. Pablo Muñoz,Tanya Roosta,Anuj Sharma,Ali Jannesari*

Main category: cs.LG

TL;DR: 本文提出FedReFT，一种在联邦学习场景下的表示微调方法，通过稀疏干预层直接调整隐藏表征，并提出ABM聚合策略以平衡个性化学习与全局知识，显著提升了参数效率。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调方法在联邦学习环境中面临数据分布、模型容量及计算资源异构性的问题，且表示微调在联邦场景中的应用仍有挑战。

Method: 提出了一种名为FedReFT的新方法，通过稀疏干预层调整隐藏表征，同时设计ABM聚合策略以减少任务异质性带来的聚合不匹配问题。

Result: 在常识推理、算术推理、指令调优和GLUE任务中验证了FedReFT的性能，实验显示相比基于LoRA的方法，其参数效率提高了7到15倍。

Conclusion: FedReFT通过语义丰富且轻量级的方式实现了高效的联邦学习微调，并在性能和参数效率上均优于当前一流的PEFT方法。

Abstract: Parameter-efficient fine-tuning (PEFT) has attracted significant attention
for adapting large pre-trained models by modifying a small subset of
parameters. Recently, Representation Fine-tuning (ReFT) has emerged as an
effective alternative. ReFT shifts the fine-tuning paradigm from updating model
weights to directly manipulating hidden representations that capture rich
semantic information, and performs better than state-of-the-art PEFTs in
standalone settings. However, its application in Federated Learning (FL)
remains challenging due to heterogeneity in clients' data distributions, model
capacities, and computational resources. To address these challenges, we
introduce Federated Representation Fine-Tuning (FedReFT), a novel approach to
fine-tune the client's hidden representation. FedReFT applies sparse
intervention layers to steer hidden representations directly, offering a
lightweight and semantically rich fine-tuning alternative ideal for edge
devices. However, representation-level updates are especially vulnerable to
aggregation mismatch under different task heterogeneity, where naive averaging
can corrupt semantic alignment. To mitigate this issue, we propose All-But-Me
(ABM) aggregation, where each client receives the aggregated updates of others
and partially incorporates them, enabling stable and personalized learning by
balancing local focus with global knowledge. We evaluate FedReFT on commonsense
reasoning, arithmetic reasoning, instruction-tuning, and GLUE, where it
consistently outperforms state-of-the-art PEFT methods in FL, achieving 7x-15x
higher parameter efficiency compared to leading LoRA-based approaches.

</details>


### [185] [Multi-Agent Reinforcement Learning in Intelligent Transportation Systems: A Comprehensive Survey](https://arxiv.org/abs/2508.20315)
*RexCharles Donatus,Kumater Ter,Ore-Ofe Ajayi,Daniel Udekwe*

Main category: cs.LG

TL;DR: 本文综述了多智能体强化学习（MARL）在智能交通系统（ITS）中的应用，针对协调模型和学习算法提出了分类标准，并探讨其在交通信号控制、车辆协同、物流优化等领域的应用。


<details>
  <summary>Details</summary>
Motivation: 随着城市交通系统复杂性的增加和对高效、可持续解决方案的迫切需求，智能交通系统的研究成为基础设施创新的焦点。而自主决策和多智能体协作是ITS中的核心挑战。

Method: 文章通过系统性综述对MARL方法进行了分类，按照价值导向、策略导向、演员评论法以及通信增强等框架进行归类。同时介绍了相关领域中使用的仿真平台及其应用领域。

Result: 总结了当前MARL在交通信号控制、车辆协作、物流优化等ITS核心领域的应用情况，并揭示了目前面临的主要技术问题及障碍。

Conclusion: 尽管MARL在ITS中展现了巨大的潜力，但实际部署还面临可扩展性、非平稳性、信用分配问题等多方面挑战，因此未来研究需进一步解决这些瓶颈。

Abstract: The growing complexity of urban mobility and the demand for efficient,
sustainable, and adaptive solutions have positioned Intelligent Transportation
Systems (ITS) at the forefront of modern infrastructure innovation. At the core
of ITS lies the challenge of autonomous decision-making across dynamic, large
scale, and uncertain environments where multiple agents traffic signals,
autonomous vehicles, or fleet units must coordinate effectively. Multi Agent
Reinforcement Learning (MARL) offers a promising paradigm for addressing these
challenges by enabling distributed agents to jointly learn optimal strategies
that balance individual objectives with system wide efficiency. This paper
presents a comprehensive survey of MARL applications in ITS. We introduce a
structured taxonomy that categorizes MARL approaches according to coordination
models and learning algorithms, spanning value based, policy based, actor
critic, and communication enhanced frameworks. Applications are reviewed across
key ITS domains, including traffic signal control, connected and autonomous
vehicle coordination, logistics optimization, and mobility on demand systems.
Furthermore, we highlight widely used simulation platforms such as SUMO, CARLA,
and CityFlow that support MARL experimentation, along with emerging benchmarks.
The survey also identifies core challenges, including scalability, non
stationarity, credit assignment, communication constraints, and the sim to real
transfer gap, which continue to hinder real world deployment.

</details>


### [186] [Multi-View Graph Convolution Network for Internal Talent Recommendation Based on Enterprise Emails](https://arxiv.org/abs/2508.20328)
*Soo Hyun Kim,Jang-Hyun Kim*

Main category: cs.LG

TL;DR: 研究提出一种基于电子邮件数据的双图卷积网络框架，通过融合任务语义相似度（WHAT）与协作互动方式（HOW），优化内部人才推荐，显著提高候选人发现效率。


<details>
  <summary>Details</summary>
Motivation: 传统内部人才推荐方法依赖少数管理者视角，具有结构性局限，容易遗漏符合条件的候选人。

Method: 利用电子邮件数据构建独立的任务语义相似度图和协作结构特征图，并通过具有门控机制的双图卷积网络进行自适应融合。

Result: 实验结果表明，该方法在Hit@100指标上达到40.9%的性能，并能根据不同职位家族学习特定的上下文感知融合策略，展现出高度可解释性。

Conclusion: 所提出的框架成功提供了量化、多维度的内部人才发现方法，有效降低了传统方法中的候选人遗漏风险，同时实现了任务和协作模式融合比率的优化。

Abstract: Internal talent recommendation is a critical strategy for organizational
continuity, yet conventional approaches suffer from structural limitations,
often overlooking qualified candidates by relying on the narrow perspective of
a few managers. To address this challenge, we propose a novel framework that
models two distinct dimensions of an employee's position fit from email data:
WHAT they do (semantic similarity of tasks) and HOW they work (structural
characteristics of their interactions and collaborations). These dimensions are
represented as independent graphs and adaptively fused using a Dual Graph
Convolutional Network (GCN) with a gating mechanism. Experiments show that our
proposed gating-based fusion model significantly outperforms other fusion
strategies and a heuristic baseline, achieving a top performance of 40.9% on
Hit@100. Importantly, it is worth noting that the model demonstrates high
interpretability by learning distinct, context-aware fusion strategies for
different job families. For example, it learned to prioritize relational (HOW)
data for 'sales and marketing' job families while applying a balanced approach
for 'research' job families. This research offers a quantitative and
comprehensive framework for internal talent discovery, minimizing the risk of
candidate omission inherent in traditional methods. Its primary contribution
lies in its ability to empirically determine the optimal fusion ratio between
task alignment (WHAT) and collaborative patterns (HOW), which is required for
employees to succeed in the new positions, thereby offering important practical
implications.

</details>


### [187] [FORGE: Foundational Optimization Representations from Graph Embeddings](https://arxiv.org/abs/2508.20330)
*Zohair Shafi,Serdar Kadioglu*

Main category: cs.LG

TL;DR: 本文介绍了一种新的方法Forge，通过在大量混合整数规划实例上对向量量化图自编码器进行无监督预训练，生成离散代码用于优化问题实例表示，从而加速组合优化问题的求解。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法需为每个问题分布和任务训练专属模型、数据收集耗时以及缺乏泛化性的问题。

Method: 提出Forge方法，无需依赖问题解，基于无监督学习预训练向量量化图自编码器生成优化实例的离散表示，并在监督与无监督场景中进行评估。

Result: Forge方法在无监督场景下能有效区分和聚类新实例，在监督场景中通过微调实现对暖启动变量及分割生成完整性差异的预测，有助于优化商业求解器性能。

Conclusion: Forge提供了一种通用的混合整数规划实例表现方法，能提升模型泛化性与优化性能，并公布了代码与预训练权重以推动后续研究。

Abstract: Combinatorial optimization problems are ubiquitous in science and
engineering, yet learning-based approaches to accelerate their solution often
require solving a large number of hard-to-solve optimization instances to
collect training data, incurring significant computational overhead. Existing
methods require training dedicated models for each problem distribution for
each downstream task, severely limiting their scalability and generalization.
In this work, we introduce Forge, a method of pre-training a vector-quantized
graph autoencoder on a large and diverse collection of mixed-integer
programming (MIP) instances in an unsupervised fashion without dependency on
their solution. The vector quantization process creates discrete code
assignments that act as a vocabulary to represent optimization instances. We
evaluate our approach under both supervised and unsupervised settings. For the
unsupervised setting, we demonstrate that Forge embeddings effectively
differentiate and cluster unseen instances. For the supervised setting, we
fine-tune Forge embeddings and show that a single model predicts both the
variables for warm-starts and integrality gaps for cut-generation across
multiple problem type distributions. Both predictions help improve performance
of a state-of-the-art, commercial optimization solver. Finally, we release our
code and pre-trained Forge weights to encourage further research and practical
use of instance-level MIP embeddings at https://github.com/skadio/forge/

</details>


### [188] [Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs](https://arxiv.org/abs/2508.20333)
*Md Abdullah Al Mamun,Ihsen Alouani,Nael Abu-Ghazaleh*

Main category: cs.LG

TL;DR: 该论文讨论了一种攻击语言模型的方法，名为SAI，通过滥用模型的对齐机制，植入偏见或执行特定的审查。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型的对齐机制如何被恶意利用，导致偏见和审查问题。

Method: 提出了一种投毒攻击SAI，利用对齐机制触发模型在特定主题上的拒答行为，并证明其能避开最先进的投毒防御技术。

Result: 通过实验，证明了SAI可在多种下游应用中成功植入偏见，例如医疗问答、简历筛选等，造成显著的选择偏倚。

Conclusion: 大语言模型的对齐机制可能成为攻击的切入点，应加强对抗措施以防范此类威胁。

Abstract: Large Language Models (LLMs) are aligned to meet ethical standards and safety
requirements by training them to refuse answering harmful or unsafe prompts. In
this paper, we demonstrate how adversaries can exploit LLMs' alignment to
implant bias, or enforce targeted censorship without degrading the model's
responsiveness to unrelated topics. Specifically, we propose Subversive
Alignment Injection (SAI), a poisoning attack that leverages the alignment
mechanism to trigger refusal on specific topics or queries predefined by the
adversary. Although it is perhaps not surprising that refusal can be induced
through overalignment, we demonstrate how this refusal can be exploited to
inject bias into the model. Surprisingly, SAI evades state-of-the-art poisoning
defenses including LLM state forensics, as well as robust aggregation
techniques that are designed to detect poisoning in FL settings. We demonstrate
the practical dangers of this attack by illustrating its end-to-end impacts on
LLM-powered application pipelines. For chat based applications such as
ChatDoctor, with 1% data poisoning, the system refuses to answer healthcare
questions to targeted racial category leading to high bias ($\Delta DP$ of
23%). We also show that bias can be induced in other NLP tasks: for a resume
selection pipeline aligned to refuse to summarize CVs from a selected
university, high bias in selection ($\Delta DP$ of 27%) results. Even higher
bias ($\Delta DP$~38%) results on 9 other chat based downstream applications.

</details>


### [189] [Dynamic Synthetic Controls vs. Panel-Aware Double Machine Learning for Geo-Level Marketing Impact Estimation](https://arxiv.org/abs/2508.20335)
*Sang Su Lee,Vineeth Loganathan,Vijay Raghavan*

Main category: cs.LG

TL;DR: 本文比较了两种用于地理级别市场效果评估方法的性能：增强合成控制方法（ASC）和面板双重机器学习方法（panel-DML）。通过模拟测试发现，ASC在复杂环境中偏差明显，而panel-DML模型显著改善了效果估计的精确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 为了更好地量化双边市场中的地理级营销提升效果，研究者希望解决传统方法在复杂情况下的不可靠性问题。

Method: 研究者构建了一个可调节关键参数的开源模拟器，并通过五种压力测试对不同方法进行了模拟与评价，涵盖了七种估计器的比较实验。

Result: 实验显示，ASC模型在面临非线性或外部冲击时表现出严重偏差，而panel-DML方法显著降低了偏差并恢复了95%的置信区间覆盖。

Conclusion: 对复杂场景来说，ASC模型虽简单但不可靠，而panel-DML方法更为稳健和可信。论文提出先诊断主要业务挑战再选择合适模型的框架，提供了分析地理实验的新蓝图。

Abstract: Accurately quantifying geo-level marketing lift in two-sided marketplaces is
challenging: the Synthetic Control Method (SCM) often exhibits high power yet
systematically under-estimates effect size, while panel-style Double Machine
Learning (DML) is seldom benchmarked against SCM. We build an open, fully
documented simulator that mimics a typical large-scale geo roll-out: N_unit
regional markets are tracked for T_pre weeks before launch and for a further
T_post-week campaign window, allowing all key parameters to be varied by the
user and probe both families under five stylized stress tests: 1) curved
baseline trends, 2) heterogeneous response lags, 3) treated-biased shocks, 4) a
non-linear outcome link, and 5) a drifting control group trend.
  Seven estimators are evaluated: three standard Augmented SCM (ASC) variants
and four panel-DML flavors (TWFE, CRE/Mundlak, first-difference, and
within-group). Across 100 replications per scenario, ASC models consistently
demonstrate severe bias and near-zero coverage in challenging scenarios
involving nonlinearities or external shocks. By contrast, panel-DML variants
dramatically reduce this bias and restore nominal 95%-CI coverage, proving far
more robust.
  The results indicate that while ASC provides a simple baseline, it is
unreliable in common, complex situations. We therefore propose a
'diagnose-first' framework where practitioners first identify the primary
business challenge (e.g., nonlinear trends, response lags) and then select the
specific DML model best suited for that scenario, providing a more robust and
reliable blueprint for analyzing geo-experiments.

</details>


### [190] [Adaptive Segmentation of EEG for Machine Learning Applications](https://arxiv.org/abs/2508.20336)
*Johnson Zhou,Joseph West,Krista A. Ehinger,Zhenming Ren,Sam E. John,David B. Grayden*

Main category: cs.LG

TL;DR: 提出了一种新的适应性分段方法CTXSEG，用于优化EEG（脑电图）信号的机器学习预处理，验证其在癫痫检测任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前使用固定时间切片来分割EEG信号，但这一方法可能无法有效捕捉与生物学相关的动态脑状态，适应性分段可能是一个更优的选择。

Method: 提出CTXSEG，一种基于EEG中统计差异的变长分段方法，并通过生成器CTXGEN验证其效果。此外，将CTXSEG用于实际癫痫检测任务，与固定长度分段方法进行比较。

Result: 实验表明，CTXSEG方法在癫痫检测中优于固定长度分段方法，且无需更改机器学习模型，同时需要较少的分段数。

Conclusion: CTXSEG能够改善EEG信号的机器学习表现，是一个有前途的固定长度分段替代方案，建议将其纳入EEG数据预处理的标准工具集。

Abstract: Objective. Electroencephalography (EEG) data is derived by sampling
continuous neurological time series signals. In order to prepare EEG signals
for machine learning, the signal must be divided into manageable segments. The
current naive approach uses arbitrary fixed time slices, which may have limited
biological relevance because brain states are not confined to fixed intervals.
We investigate whether adaptive segmentation methods are beneficial for machine
learning EEG analysis.
  Approach. We introduce a novel adaptive segmentation method, CTXSEG, that
creates variable-length segments based on statistical differences in the EEG
data and propose ways to use them with modern machine learning approaches that
typically require fixed-length input. We assess CTXSEG using controllable
synthetic data generated by our novel signal generator CTXGEN. While our CTXSEG
method has general utility, we validate it on a real-world use case by applying
it to an EEG seizure detection problem. We compare the performance of CTXSEG
with fixed-length segmentation in the preprocessing step of a typical EEG
machine learning pipeline for seizure detection.
  Main results. We found that using CTXSEG to prepare EEG data improves seizure
detection performance compared to fixed-length approaches when evaluated using
a standardized framework, without modifying the machine learning method, and
requires fewer segments.
  Significance. This work demonstrates that adaptive segmentation with CTXSEG
can be readily applied to modern machine learning approaches, with potential to
improve performance. It is a promising alternative to fixed-length segmentation
for signal preprocessing and should be considered as part of the standard
preprocessing repertoire in EEG machine learning applications.

</details>


### [191] [Understanding Incremental Learning with Closed-form Solution to Gradient Flow on Overparamerterized Matrix Factorization](https://arxiv.org/abs/2508.20344)
*Hancheng Min,René Vidal*

Main category: cs.LG

TL;DR: 本文探讨矩阵分解问题中梯度流（GF）的渐进学习现象，揭示了其在对称矩阵因式分解中的时间尺度分离机制，进一步探讨了该分析如何扩展到非对称矩阵分解。


<details>
  <summary>Details</summary>
Motivation: 神经网络的优秀性能部分来自训练算法（如梯度流）在特定初始化条件下的隐式偏置或正则化属性。理解这些机制有助于揭示网络训练的核心动力并优化现有模型。

Method: 研究了梯度流在对称矩阵分解问题中的渐进学习行为，采用解决Riccati类矩阵微分方程的方法得到闭式解，并通过分析解的时间尺度分离机制量化后逐渐学习。

Result: 通过证明时间尺度分离现象解释了GF如何学习目标矩阵的不同成分，发现随着初始化尺度减小，时间尺度分离加剧，从而能更有效地找到目标矩阵的低秩近似。

Conclusion: 本文提供了对于GF在对称矩阵分解问题上渐进学习现象的定量理解，并提出该框架或方法可能对非对称矩阵分解问题具有推广潜力。

Abstract: Many theoretical studies on neural networks attribute their excellent
empirical performance to the implicit bias or regularization induced by
first-order optimization algorithms when training networks under certain
initialization assumptions. One example is the incremental learning phenomenon
in gradient flow (GF) on an overparamerterized matrix factorization problem
with small initialization: GF learns a target matrix by sequentially learning
its singular values in decreasing order of magnitude over time. In this paper,
we develop a quantitative understanding of this incremental learning behavior
for GF on the symmetric matrix factorization problem, using its closed-form
solution obtained by solving a Riccati-like matrix differential equation. We
show that incremental learning emerges from some time-scale separation among
dynamics corresponding to learning different components in the target matrix.
By decreasing the initialization scale, these time-scale separations become
more prominent, allowing one to find low-rank approximations of the target
matrix. Lastly, we discuss the possible avenues for extending this analysis to
asymmetric matrix factorization problems.

</details>


### [192] [DFAMS: Dynamic-flow guided Federated Alignment based Multi-prototype Search](https://arxiv.org/abs/2508.20353)
*Zhibang Yang,Xinke Jiang,Rihong Qiu,Ruiqing Li,Yihang Zhang,Yue Fang,Yongxin Xu,Hongxin Ding,Xu Chu,Junfeng Zhao,Yasha Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为DFAMS的框架，通过动态信息流（DIF）来提升模糊查询在跨领域场景中的高质量检索能力，从而提高联邦检索的生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理含糊查询尤其是跨领域场景时，难以做到高质量的检索，导致生成任务效果受限。

Method: DFAMS利用动态信息流（DIF）提取潜在的查询意图，通过Shapley值归因方法追踪与意图识别及子领域边界检测相关的神经元激活路径，采用多原型对比学习训练对齐模块，实现知识库间的细粒度建模与语义对齐。

Result: 实验结果显示，在五个基准测试中，DFAMS在知识分类准确性上提高了14.37%、检索召回率提高了5.38%、问答准确率提高了6.45%。

Conclusion: DFAMS为复杂的联邦检索提供了有效的解决方案，其基于动态信息流的方式在跨领域场景中表现卓越。

Abstract: Federated Retrieval (FR) routes queries across multiple external knowledge
sources, to mitigate hallucinations of LLMs, when necessary external knowledge
is distributed. However, existing methods struggle to retrieve high-quality and
relevant documents for ambiguous queries, especially in cross-domain scenarios,
which significantly limits their effectiveness in supporting downstream
generation tasks. Inspired by dynamic information flow (DIF), we propose DFAMS,
a novel framework that leverages DIF to identify latent query intents and
construct semantically aligned knowledge partitions for accurate retrieval
across heterogeneous sources. Specifically, DFAMS probes the DIF in LLMs by
leveraging gradient signals from a few annotated queries and employing Shapley
value-based attribution to trace neuron activation paths associated with intent
recognition and subdomain boundary detection. Then, DFAMS leverages DIF to
train an alignment module via multi-prototype contrastive learning, enabling
fine-grained intra-source modeling and inter-source semantic alignment across
knowledge bases. Experimental results across five benchmarks show that DFAMS
outperforms advanced FR methods by up to 14.37% in knowledge classification
accuracy, 5.38% in retrieval recall, and 6.45% in downstream QA accuracy,
demonstrating its effectiveness in complex FR scenarios.

</details>


### [193] [Developing a Multi-Modal Machine Learning Model For Predicting Performance of Automotive Hood Frames](https://arxiv.org/abs/2508.20358)
*Abhishek Indupally,Satchit Ramnath*

Main category: cs.LG

TL;DR: 本文提出了一种多模态机器学习架构（MMML），可以预测汽车发动机盖框架的性能指标，减少对计算昂贵的模拟的依赖，并验证了其在概念设计阶段的有效性。


<details>
  <summary>Details</summary>
Motivation: 设计者需要一种快速评估发动机盖框架几何性能的方法，而不用耗费大量时间在传统模拟上。

Method: 研究开发了一个MMML架构，从多种数据模态中学习，预测设计性能指标，同时加速设计探索过程。

Result: 发现MMML架构通过结合多个数据模态，性能优于单模态方法，并能推广到未见的框架几何形状。

Conclusion: MMML能够补充传统模拟工作流程，尤其在概念设计阶段，推动机器学习技术在工程设计中的更广泛应用。

Abstract: Is there a way for a designer to evaluate the performance of a given hood
frame geometry without spending significant time on simulation setup? This
paper seeks to address this challenge by developing a multimodal
machine-learning (MMML) architecture that learns from different modalities of
the same data to predict performance metrics. It also aims to use the MMML
architecture to enhance the efficiency of engineering design processes by
reducing reliance on computationally expensive simulations. The proposed
architecture accelerates design exploration, enabling rapid iteration while
maintaining high-performance standards, especially in the concept design phase.
The study also presents results that show that by combining multiple data
modalities, MMML outperforms traditional single-modality approaches. Two new
frame geometries, not part of the training dataset, are also used for
prediction using the trained MMML model to showcase the ability to generalize
to unseen frame models. The findings underscore MMML's potential in
supplementing traditional simulation-based workflows, particularly in the
conceptual design phase, and highlight its role in bridging the gap between
machine learning and real-world engineering applications. This research paves
the way for the broader adoption of machine learning techniques in engineering
design, with a focus on refining multimodal approaches to optimize structural
development and accelerate the design cycle.

</details>


### [194] [BiListing: Modality Alignment for Listings](https://arxiv.org/abs/2508.20396)
*Guillaume Guy,Mihajlo Grbovic,Chun How Tan,Han Zhao*

Main category: cs.LG

TL;DR: 本文介绍了BiListing模型，它通过结合文本和图片信息，将Airbnb的非结构化数据转化为嵌入向量，并应用于推荐系统，提高了搜索排名的效果并增加了收入。


<details>
  <summary>Details</summary>
Motivation: 解决Airbnb列表中多样非结构化数据综合利用的难题，如文本和图片的多模态融合，以及实现高效的零样本搜索。

Method: 提出BiListing模型，利用大语言模型和预训练的语言-图像模型，将文本和图像对齐为单一的嵌入向量表示，并应用于搜索和推荐中。

Result: 在生产环境中有效部署，搜索排名模型NDCB提升0.425%，为平台带来数千万美金的额外收入。

Conclusion: BiListing通过优化Airbnb非结构化数据的利用和搜索效率，展示了深度学习与多模态融合在商业应用中的潜力和优势。

Abstract: Airbnb is a leader in offering travel accommodations. Airbnb has historically
relied on structured data to understand, rank, and recommend listings to guests
due to the limited capabilities and associated complexity arising from
extracting meaningful information from text and images. With the rise of
representation learning, leveraging rich information from text and photos has
become easier. A popular approach has been to create embeddings for text
documents and images to enable use cases of computing similarities between
listings or using embeddings as features in an ML model.
  However, an Airbnb listing has diverse unstructured data: multiple images,
various unstructured text documents such as title, description, and reviews,
making this approach challenging. Specifically, it is a non-trivial task to
combine multiple embeddings of different pieces of information to reach a
single representation.
  This paper proposes BiListing, for Bimodal Listing, an approach to align text
and photos of a listing by leveraging large-language models and pretrained
language-image models. The BiListing approach has several favorable
characteristics: capturing unstructured data into a single embedding vector per
listing and modality, enabling zero-shot capability to search inventory
efficiently in user-friendly semantics, overcoming the cold start problem, and
enabling listing-to-listing search along a single modality, or both.
  We conducted offline and online tests to leverage the BiListing embeddings in
the Airbnb search ranking model, and successfully deployed it in production,
achieved 0.425% of NDCB gain, and drove tens of millions in incremental
revenue.

</details>


### [195] [TF-TransUNet1D: Time-Frequency Guided Transformer U-Net for Robust ECG Denoising in Digital Twin](https://arxiv.org/abs/2508.20398)
*Shijie Wang,Lei Li*

Main category: cs.LG

TL;DR: 本文提出了一种称为TF-TransUNet1D的1D深度神经网络，用于去噪心电信号，同时保留其诊断完整性。


<details>
  <summary>Details</summary>
Motivation: 心电信号的诊断效用常因噪声和伪影而降低，需要更有效的去噪方法以提高信号质量，为心脏数字孪生提供更可靠的数据支持。

Method: 使用基于U-Net的编码器-解码器架构和Transformer编码器，并结合时频域混合损失函数，设计能够捕获局部特征与长距离依赖性的模型，同时通过时域和频域的双域损失函数优化去噪性能。

Result: 模型在MIT-BIH心律失常数据库和噪声应力测试数据库上的评估中，表现出较强的性能，SNR显著提升，平均绝对误差为0.1285，皮尔逊相关系数为0.9540。

Conclusion: 本文通过高精度去噪，填补了心脏数字孪生预处理工作流程中的关键空白，为更可靠的实时监测与个性化建模奠定了基础。

Abstract: Electrocardiogram (ECG) signals serve as a foundational data source for
cardiac digital twins, yet their diagnostic utility is frequently compromised
by noise and artifacts. To address this issue, we propose TF-TransUNet1D, a
novel one-dimensional deep neural network that integrates a U-Net-based
encoder-decoder architecture with a Transformer encoder, guided by a hybrid
time-frequency domain loss. The model is designed to simultaneously capture
local morphological features and long-range temporal dependencies, which are
critical for preserving the diagnostic integrity of ECG signals. To enhance
denoising robustness, we introduce a dual-domain loss function that jointly
optimizes waveform reconstruction in the time domain and spectral fidelity in
the frequency domain. In particular, the frequency-domain component effectively
suppresses high-frequency noise while maintaining the spectral structure of the
signal, enabling recovery of subtle but clinically significant waveform
components. We evaluate TF-TransUNet1D using synthetically corrupted signals
from the MIT-BIH Arrhythmia Database and the Noise Stress Test Database
(NSTDB). Comparative experiments against state-of-the-art baselines demonstrate
consistent superiority of our model in terms of SNR improvement and error
metrics, achieving a mean absolute error of 0.1285 and Pearson correlation
coefficient of 0.9540. By delivering high-precision denoising, this work
bridges a critical gap in pre-processing pipelines for cardiac digital twins,
enabling more reliable real-time monitoring and personalized modeling.

</details>


### [196] [Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full Context-Aware Linear Attention](https://arxiv.org/abs/2508.20407)
*Zhongpan Tang*

Main category: cs.LG

TL;DR: 提出了一种名为TLinFormer的新型线性注意力模型，通过重新配置神经元连接模式，实现严格的线性复杂度，并保持完整的上下文信息流，解决Transformer在长序列任务中的复杂性瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在长序列任务中自注意力机制的二次复杂度问题，同时不牺牲模型性能。

Method: 提出一种线性注意力模型TLinFormer，通过重新配置神经元连接模式实现严格的线性复杂度，并保持对完整历史上下文的感知。

Result: 实验表明，TLinFormer在推理延迟、KV缓存效率、内存占用和总体加速等关键指标上优于传统Transformer基线。

Conclusion: TLinFormer有效弥合了现有高效注意力方法与标准注意力之间的性能差距，为长序列任务提供了一种高效且高性能的解决方案。

Abstract: The Transformer architecture has become a cornerstone of modern artificial
intelligence, but its core self-attention mechanism suffers from a complexity
bottleneck that scales quadratically with sequence length, severely limiting
its application in long-sequence tasks. To address this challenge, existing
linear attention methods typically sacrifice model performance by relying on
data-agnostic kernel approximations or restrictive context selection. This
paper returns to the first principles of connectionism, starting from the
topological structure of information flow, to introduce a novel linear
attention architecture-\textbf{TLinFormer}. By reconfiguring neuron connection
patterns, TLinFormer achieves strict linear complexity while computing exact
attention scores and ensuring information flow remains aware of the full
historical context. This design aims to bridge the performance gap prevalent
between existing efficient attention methods and standard attention. Through a
series of experiments, we systematically evaluate the performance of TLinFormer
against a standard Transformer baseline on long-sequence inference tasks. The
results demonstrate that TLinFormer exhibits overwhelming advantages in key
metrics such as \textbf{inference latency}, \textbf{KV cache efficiency},
\textbf{memory footprint}, and \textbf{overall speedup}.

</details>


### [197] [Assessing local deformation and computing scalar curvature with nonlinear conformal regularization of decoders](https://arxiv.org/abs/2508.20413)
*Benjamin Couéraud,Vikram Sunkara,Christof Schütte*

Main category: cs.LG

TL;DR: 提出了一种新的非线性共形正则化方法，用于自动编码器的解码器。


<details>
  <summary>Details</summary>
Motivation: 在高维数据处理中，寻找主要影响因素并将其转换为低维表示至关重要。

Method: 利用非线性共形正则化对深度神经网络近似的解码器进行几何正则化，以量化潜在空间的局部分形变形。

Result: 实现了从潜在空间到原始数据空间的局部分形变化量测量，并且可以计算出学习到的流形的标量曲率。

Conclusion: 该技术能有效增强自动编码器的功能，同时提供对解码器几何特性的深刻理解。

Abstract: One aim of dimensionality reduction is to discover the main factors that
explain the data, and as such is paramount to many applications. When working
with high dimensional data, autoencoders offer a simple yet effective approach
to learn low-dimensional representations. The two components of a general
autoencoder consist first of an encoder that maps the observed data onto a
latent space; and second a decoder that maps the latent space back to the
original observation space, which allows to learn a low-dimensional manifold
representation of the original data. In this article, we introduce a new type
of geometric regularization for decoding maps approximated by deep neural
networks, namely nonlinear conformal regularization. This regularization
procedure permits local variations of the decoder map and comes with a new
scalar field called conformal factor which acts as a quantitative indicator of
the amount of local deformation sustained by the latent space when mapped into
the original data space. We also show that this regularization technique allows
the computation of the scalar curvature of the learned manifold. Implementation
and experiments on the Swiss roll and CelebA datasets are performed to
illustrate how to obtain these quantities from the architecture.

</details>


### [198] [On Identifying Why and When Foundation Models Perform Well on Time-Series Forecasting Using Automated Explanations and Rating](https://arxiv.org/abs/2508.20437)
*Michael Widener,Kausik Lakkaraju,John Aydin,Biplav Srivastava*

Main category: cs.LG

TL;DR: 本研究通过结合传统可解释人工智能方法与基于评级的解释（RDE），评估时间序列预测模型（TSFM）的表现与可解释性，并揭示了不同模型在多样化领域的优劣。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测模型使用越来越广泛，但这些模型的复杂性、不透明性和性能波动引发了用户对其可靠性和交互性的担忧，因此需要深入了解其优缺点。

Method: 作者通过整合传统可解释人工智能方法与基于评级的解释（RDE），对四种不同架构的模型（包括ARIMA、Gradient Boosting、Chronos以及Llama，涵盖不同应用场景）进行性能与可解释性比较。

Result: 研究发现，特征工程模型（如Gradient Boosting）在波动性高或数据稀疏的领域（如电力、汽车零件）表现优于基础模型（如Chronos），并提供更好的解释性，而基础模型仅在稳定或趋势驱动的领域（如金融）表现较佳。

Conclusion: 特征工程模型在复杂领域更有优势，而基础模型仅适合于稳定性较高的情境，因此根据具体应用场景选择合适的模型至关重要。

Abstract: Time-series forecasting models (TSFM) have evolved from classical statistical
methods to sophisticated foundation models, yet understanding why and when
these models succeed or fail remains challenging. Despite this known
limitation, time series forecasting models are increasingly used to generate
information that informs real-world actions with equally real consequences.
Understanding the complexity, performance variability, and opaque nature of
these models then becomes a valuable endeavor to combat serious concerns about
how users should interact with and rely on these models' outputs. This work
addresses these concerns by combining traditional explainable AI (XAI) methods
with Rating Driven Explanations (RDE) to assess TSFM performance and
interpretability across diverse domains and use cases. We evaluate four
distinct model architectures: ARIMA, Gradient Boosting, Chronos (time-series
specific foundation model), Llama (general-purpose; both fine-tuned and base
models) on four heterogeneous datasets spanning finance, energy,
transportation, and automotive sales domains. In doing so, we demonstrate that
feature-engineered models (e.g., Gradient Boosting) consistently outperform
foundation models (e.g., Chronos) in volatile or sparse domains (e.g., power,
car parts) while providing more interpretable explanations, whereas foundation
models excel only in stable or trend-driven contexts (e.g., finance).

</details>


### [199] [Uncovering the Spectral Bias in Diagonal State Space Models](https://arxiv.org/abs/2508.20441)
*Ruben Solozabal,Velibor Bojkovic,Hilal AlQuabeh,Kentaro Inui,Martin Takáč*

Main category: cs.LG

TL;DR: 研究分析对角状态空间模型（SSM）初始化方法的作用与频率视角下的学习偏差，并提出一种基于离散傅里叶域的对角初始化方法S4D-DFouT，取得了高效且表现优异的结果。


<details>
  <summary>Details</summary>
Motivation: 目前的SSM参数初始化方法大多基于HiPPO框架，但缺乏对其对角变体的深入研究，作者希望从频率视角系统性地探索对角SSM初始化的作用及内在学习偏差。

Method: 作者系统性分析对角状态空间模型的频率特性，并基于此提出了一种新的基于离散傅里叶域的对角初始化方法S4D-DFouT。

Result: 通过提出的S4D-DFouT方法，取得了Long Range Arena基准上的最新最优表现，并成功在大规模数据集（如PathX-256）上进行从零训练。

Conclusion: 研究证明合理的对角SSM初始化方式能够有效提升性能，同时具备计算效率高和可扩展性好的优点。

Abstract: Current methods for initializing state space models (SSMs) parameters mainly
rely on the \textit{HiPPO framework}, which is based on an online approximation
of orthogonal polynomials. Recently, diagonal alternatives have shown to reach
a similar level of performance while being significantly more efficient due to
the simplification in the kernel computation. However, the \textit{HiPPO
framework} does not explicitly study the role of its diagonal variants. In this
paper, we take a further step to investigate the role of diagonal SSM
initialization schemes from the frequency perspective. Our work seeks to
systematically understand how to parameterize these models and uncover the
learning biases inherent in such diagonal state-space models. Based on our
observations, we propose a diagonal initialization on the discrete Fourier
domain \textit{S4D-DFouT}. The insights in the role of pole placing in the
initialization enable us to further scale them and achieve state-of-the-art
results on the Long Range Arena benchmark, allowing us to train from scratch on
very large datasets as PathX-256.

</details>


### [200] [Towards Mitigating Excessive Forgetting in LLM Unlearning via Entanglement-Aware Unlearning with Proxy Constraint](https://arxiv.org/abs/2508.20443)
*Zhihao Liu,Jian Lou,Yuke Hu,Xiaochen Li,Tailun Chen,Yitian Chen,Zhan Qin*

Main category: cs.LG

TL;DR: 提出EAGLE-PC，一种用于解决模型中遗忘问题的框架，其重点是更精准的遗忘并减少对模型性能的负面影响。


<details>
  <summary>Details</summary>
Motivation: 应对数据隐私和版权问题，解决现有遗忘方法的不足，如遗忘不彻底和性能下降等问题。

Method: 通过引入两个关键组件：一种基于样本嵌入相似性的智重加权机制，用于更加精准地选择需要遗忘的样本；以及一种基于上下文生成测试数据的代理约束，以减少过度遗忘对模型性能的负面影响。

Result: 在TOFU和MUSE基准测试中，EAGLE-PC在遗忘与性能平衡方面表现优异，并且可以与多种LLM兼容，接近全重新训练的效果。

Conclusion: EAGLE-PC框架提供了一种高效且可扩展的解决方案，能够在不完全重新训练的前提下更好地实现遗忘任务。

Abstract: Large language models (LLMs) are trained on massive datasets that may include
private or copyrighted content. Due to growing privacy and ownership concerns,
data owners may request the removal of their data from trained models. Machine
unlearning provides a practical solution by removing the influence of specific
data without full retraining. However, most existing methods lack a sound
forgetting boundary, causing some samples to be under-forgotten, leaving
residual leakage risks, while others remain over-forgotten at the expense of
degraded utility.
  In this work, we propose EAGLE-PC (Entanglement-Awareness Guided Loss
Reweighting with Proxy Constraint), a novel unlearning framework that addresses
these limitations through two key components. First, entanglement-awareness
guided loss reweighting determines the forgetting effort of each sample by
measuring its similarity to retain samples in the embedding space, enabling
more targeted and effective unlearning. Second, a proxy constraint leveraging
ICL (In-Context Learning) generated test data softly regularizes the forgetting
process, effectively mitigating over-forgetting. EAGLE-PC is compatible with
existing gradient-based objectives and serves as a plug-and-play enhancement.
We evaluate EAGLE-PC on the TOFU and MUSE benchmarks, showing consistent
improvements in the forgetting-utility trade-off across multiple LLMs. Combined
with the NPO+GD optimizer, it approaches full retraining performance, offering
a scalable and robust unlearning solution.

</details>


### [201] [Evaluating Differentially Private Generation of Domain-Specific Text](https://arxiv.org/abs/2508.20452)
*Yidan Sun,Viktor Schlegel,Srinivasan Nandakumar,Iqra Zahid,Yuping Wu,Warren Del-Pinto,Goran Nenadic,Siew-Kei Lam,Jie Zhang,Anil A Bharath*

Main category: cs.LG

TL;DR: 本文探讨了生成式AI在高风险领域的应用面临的隐私和监管壁垒，并提出了一个统一基准来评估采用差分隐私保证生成的合成数据的效用和保真度。


<details>
  <summary>Details</summary>
Motivation: 在医疗和金融等高风险领域，隐私和法规限制了真实世界数据的使用，因此需要差分隐私合成数据生成方法来解决这一问题。

Method: 提出了一个统一的基准来系统化评估差分隐私文本数据生成的效用和保真度，涵盖代表性数据选择、现实隐私预算、预训练及多种评价指标等问题。通过对五个领域特定数据集的隐私保护生成方法进行评估，揭示其效用和保真度的显著下降。

Result: 结果表明，目前的方法在严格隐私约束下显著降低了数据的实用性和保真度，突出了现有方法的局限性。

Conclusion: 研究指出，现有隐私保护方法需要改进，同时需在现实场景下更有效地评估新的数据分享方法。

Abstract: Generative AI offers transformative potential for high-stakes domains such as
healthcare and finance, yet privacy and regulatory barriers hinder the use of
real-world data. To address this, differentially private synthetic data
generation has emerged as a promising alternative. In this work, we introduce a
unified benchmark to systematically evaluate the utility and fidelity of text
datasets generated under formal Differential Privacy (DP) guarantees. Our
benchmark addresses key challenges in domain-specific benchmarking, including
choice of representative data and realistic privacy budgets, accounting for
pre-training and a variety of evaluation metrics. We assess state-of-the-art
privacy-preserving generation methods across five domain-specific datasets,
revealing significant utility and fidelity degradation compared to real data,
especially under strict privacy constraints. These findings underscore the
limitations of current approaches, outline the need for advanced
privacy-preserving data sharing methods and set a precedent regarding their
evaluation in realistic scenarios.

</details>


### [202] [Structure-aware Hypergraph Transformer for Diagnosis Prediction in Electronic Health Records](https://arxiv.org/abs/2508.20500)
*Haiyan Wang,Ye Yuan*

Main category: cs.LG

TL;DR: 本论文提出一种新型结构感知超图Transformer (SHGT) 框架，用于改进电子健康记录(EHR)中模型的表现，尤其在诊断预测方面优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图神经网络(GNN)的EHR数据建模方法存在不足，难以捕捉临床数据中的高阶依赖关系，且受限于局部信息传递模式，表示能力有限。

Method: 提出SHGT框架，通过超图结构编码器捕捉医疗代码的高阶交互，引入Transformer结构以处理整个超图，并设计基于超图重构的损失函数以保留其结构特征。

Result: 在实际EHR数据集实验中，SHGT在诊断预测任务上超过现有最先进模型的表现。

Conclusion: SHGT方法有效解决了现有方法局限性，抓住了临床数据的高阶和全局关系，提升了基于EHR的预测性能。

Abstract: Electronic Health Records (EHR) systematically organize patient health data
through standardized medical codes, serving as a comprehensive and invaluable
source for predictive modeling. Graph neural networks (GNNs) have demonstrated
effectiveness in modeling interactions between medical codes within EHR.
However, existing GNN-based methods are inadequate due to: a) their reliance on
pairwise relations fails to capture the inherent higher-order dependencies in
clinical data, and b) the localized message-passing scheme limits
representation power. To address these issues, this paper proposes a novel
Structure-aware HyperGraph Transformer (SHGT) framework following three-fold
ideas: a) employing a hypergraph structural encoder to capture higher-order
interactions among medical codes, b) integrating the Transformer architecture
to reason over the entire hypergraph, and c) designing a tailored loss function
incorporating hypergraph reconstruction to preserve the hypergraph's original
structure. Experiments on real-world EHR datasets demonstrate that the proposed
SHGT outperforms existing state-of-the-art models on diagnosis prediction.

</details>


### [203] [Khiops: An End-to-End, Frugal AutoML and XAI Machine Learning Solution for Large, Multi-Table Databases](https://arxiv.org/abs/2508.20519)
*Marc Boullé,Nicolas Voisine,Bruno Guerraz,Carine Hue,Felipe Olmos,Vladimir Popescu,Stéphane Gouache,Stéphane Bouget,Alexis Bondu,Luc Aurelien Gauthier,Yassine Nair Benrekia,Fabrice Clérot,Vincent Lemaire*

Main category: cs.LG

TL;DR: Khiops是一款用于大规模多表数据库的开源机器学习工具，采用独特的贝叶斯方法，支持分类回归、变量选择等功能。


<details>
  <summary>Details</summary>
Motivation: 解决在处理大规模多表数据库中的变量选择、分类预测和其他机器学习问题。

Method: 使用贝叶斯分类器，结合变量选择和权重学习，支持数值数据离散化和分类聚合，并自动生成多表数据库的聚合特征。

Result: Khiops能够处理包含百万个个体、数万个变量和数亿条记录的大型数据库，并支持Python库和用户界面。

Conclusion: Khiops是一种强大的多功能工具，适用于大规模复杂数据库的挖掘和分析。

Abstract: Khiops is an open source machine learning tool designed for mining large
multi-table databases. Khiops is based on a unique Bayesian approach that has
attracted academic interest with more than 20 publications on topics such as
variable selection, classification, decision trees and co-clustering. It
provides a predictive measure of variable importance using discretisation
models for numerical data and value clustering for categorical data. The
proposed classification/regression model is a naive Bayesian classifier
incorporating variable selection and weight learning. In the case of
multi-table databases, it provides propositionalisation by automatically
constructing aggregates. Khiops is adapted to the analysis of large databases
with millions of individuals, tens of thousands of variables and hundreds of
millions of records in secondary tables. It is available on many environments,
both from a Python library and via a user interface.

</details>


### [204] [MedGR$^2$: Breaking the Data Barrier for Medical Reasoning via Generative Reward Learning](https://arxiv.org/abs/2508.20549)
*Weihai Zhi,Jiayan Guo,Shangyang Li*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架MedGR$^2$，用于生成高质量的多模态医学数据，并提升医学人工智能模型在低数据场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型在医学领域的应用受限于数据稀缺问题，且传统的监督微调和强化学习方法均存在局限性。

Method: 通过联合开发数据生成器和奖励模型，MedGR$^2$框架实现自动和持续的高质量数据生成，该数据用于训练监督学习和强化学习模型。

Result: 使用MedGR$^2$生成的数据进行训练的模型在跨模态与跨任务的泛化上表现优异，并且超过了基于大型人工标注数据的基线方法。

Conclusion: MedGR$^2$框架能够高效解决高风险领域的数据不足问题，为构建更具泛化能力的医学人工智能开辟了新途径。

Abstract: The application of Vision-Language Models (VLMs) in medicine is critically
hampered by the scarcity of high-quality, expert-annotated data. Supervised
Fine-Tuning (SFT) on existing datasets often leads to poor generalization on
unseen modalities and tasks, while Reinforcement Learning (RL), a promising
alternative, is stymied by the lack of reliable reward signals in this
data-scarce domain. To break this impasse, we introduce Generative Reward
Learning for Medical Reasoning (MedGR$^2$), a novel framework that creates a
self-improving virtuous cycle. MedGR$^2$ co-develops a data generator and a
reward model, enabling the automated, continuous creation of high-quality,
multi-modal medical data that serves as both a superior training source for SFT
and RL. Our experiments demonstrate that SFT with MedGR$^2$-produced data
already surpasses baselines trained on large-scale, human-curated datasets.
Crucially, when leveraging this data for RL via Group Relative Policy
Optimization (GRPO), our model achieves state-of-the-art cross-modality and
cross-task generalization, significantly outperforming specialized RL-based
methods. Furthermore, our compact model, empowered by MedGR$^2$, achieves
performance competitive with foundation models possessing over 10 times more
parameters. MedGR$^2$ presents a new paradigm for data-efficient learning in
high-stakes domains, transforming the problem from data scarcity to data
generation and unlocking the full potential of RL for building truly
generalizable medical AI.

</details>


### [205] [Theoretical foundations of the integral indicator application in hyperparametric optimization](https://arxiv.org/abs/2508.20550)
*Roman S. Kulshin,Anatoly A. Sidorov*

Main category: cs.LG

TL;DR: 通过整合多个指标的统一标准，优化推荐算法的超参数，从而在准确性、排名质量、多样性和资源效率间找到平衡。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统在优化时通常仅聚焦单一性能指标，而忽略了多指标之间的平衡问题。本文提出一种可以整合多种性能指标的评估方法，来优化推荐算法。

Method: 采用一种整合性评估方法，将多个性能指标合并为一个综合标准，用于指导超参数的优化过程。

Result: 该方法在理论上具有普适性，不仅适用于推荐系统，还能用于其他机器学习和数据分析任务的优化。

Conclusion: 研究提出了多指标优化工具，有助于解决传统推荐系统中单一性能指标不足的问题，实现推荐算法的全面优化。

Abstract: The article discusses the concept of hyperparametric optimization of
recommendation algorithms using an integral assessment that combines various
performance indicators into a single consolidated criterion. This approach is
opposed to traditional methods of setting up a single metric and allows you to
achieve a balance between accuracy, ranking quality, variety of output and the
resource intensity of algorithms. The theoretical significance of the research
lies in the development of a universal multi-criteria optimization tool that is
applicable not only in recommendation systems, but also in a wide range of
machine learning and data analysis tasks.

</details>


### [206] [MERIT: Maximum-normalized Element-wise Ratio for Language Model Large-batch Training](https://arxiv.org/abs/2508.20577)
*Yang Luo,Zangwei Zheng,Ziheng Qin,Zirui Zhu,Yong Liu,Yang You*

Main category: cs.LG

TL;DR: 提出了一种新的优化器MERIT，用以解决大批量训练深度神经网络面临的优化和泛化性能下降问题，尤其是语言模型中的注意力机制瓶颈。


<details>
  <summary>Details</summary>
Motivation: 大批量训练加速了深度神经网络的训练，但语言模型在大批量训练中面临性能下降，这是由于注意力层中最大注意力logit的急剧增加造成的信息瓶颈。现有优化器如AdamW和LAMB效果不够理想。

Method: 提出了MERIT优化器，利用max-norm计算信任比率，有效约束最大注意力logit。同时，构建了元素级信任比率，关注局部权重结构，提供更稳健的更新。

Result: 在不同规模的GPT-2模型上进行大批量训练试验，MERIT表现优异。例如在GPT-2 Medium模型的训练中，MERIT支持6k批量大小且无性能下降，显著优于标准480批量训练。

Conclusion: MERIT优化器改进了大批量训练的稳定性和性能，强调最大注意力logit与精细化信任比率的重要性，为更大批量训练的使用铺平了道路，加速了大语言模型的发展。

Abstract: Large-batch training has become a cornerstone in accelerating the training of
deep neural networks, yet it poses challenges in optimization and
generalization. Existing optimizers like AdamW present performance degradation
during language models' large-batch training, due to the information bottleneck
in attention layers caused by the sharp increase of max attention logit. While
the LAMB optimizer partially addresses this issue, some attention layers still
face this issue. The reason is that $l_2$-norm-based trust ratios in LAMB are
less effective in directly influencing the max value of query/key weights.
Furthermore, the weight-wise trust ratio in LAMB is error-prone as it overlooks
relationships of weight values within rows or columns. Building on these
observations, we propose a novel optimizer, MERIT, which leverages the max-norm
to calculate the trust ratio to constrain the max attention logit more
effectively. Moreover, we further construct element-wise trust ratios to
provide more robust update scaling by focusing on local weight structures.
Extensive experiments of large-batch training across various sizes of GPT-2
models demonstrate the superior performance of MERIT. Notably, during the
training of GPT-2 Medium, MERIT enables a 6k batch size without any performance
degradation compared to the standard batch size (480) with 48B training tokens.
This work highlights the importance of considering the max attention logit and
finer-granularity trust ratio in large-batch training. It successfully improves
the training stability and paves the way for larger batch usage, enabling
faster development and iteration of large language models. Code is available at
https://github.com/NUS-HPC-AI-Lab/MERIT.

</details>


### [207] [Unbiased Stochastic Optimization for Gaussian Processes on Finite Dimensional RKHS](https://arxiv.org/abs/2508.20588)
*Neta Shoham,Haim Avron*

Main category: cs.LG

TL;DR: 提出用于精确随机推断高斯过程(GPs)的新算法，尤其适用于有限维和无限维的再生核Hilbert空间(RKHS)。


<details>
  <summary>Details</summary>
Motivation: 现有通过近似方式进行随机超参数学习的方法，会导致无法保证收敛到真实边际似然的驻点。

Method: 提出了一种精确的随机推断算法，适用于具有中等有限维RKHS内核的GPs，并讨论其在无限维RKHS中的扩展可能性。

Result: 相比于现有方法，在内存资源和批量大小受限的情况下，新方法取得了更优的实验结果。

Conclusion: 新方法在保证精确性的同时扩展了适用性，并在受资源限制的场景中优于现有方法。

Abstract: Current methods for stochastic hyperparameter learning in Gaussian Processes
(GPs) rely on approximations, such as computing biased stochastic gradients or
using inducing points in stochastic variational inference. However, when using
such methods we are not guaranteed to converge to a stationary point of the
true marginal likelihood. In this work, we propose algorithms for exact
stochastic inference of GPs with kernels that induce a Reproducing Kernel
Hilbert Space (RKHS) of moderate finite dimension. Our approach can also be
extended to infinite dimensional RKHSs at the cost of forgoing exactness. Both
for finite and infinite dimensional RKHSs, our method achieves better
experimental results than existing methods when memory resources limit the
feasible batch size and the possible number of inducing points.

</details>


### [208] [Local Virtual Nodes for Alleviating Over-Squashing in Graph Neural Networks](https://arxiv.org/abs/2508.20597)
*Tuğrul Hasan Karabulut,İnci M. Baytaş*

Main category: cs.LG

TL;DR: 本研究提出了一种名为局部虚拟节点（LVN）的新方法，通过引入可训练的嵌入以缓解图神经网络（GNN）中的过度压缩问题，同时尽量不破坏输入图的全局结构。


<details>
  <summary>Details</summary>
Motivation: 解决GNN在处理长距离依赖任务中的过度压缩问题，同时保持输入图的原始全局拓扑结构和领域知识。

Method: 通过引入基于节点中心性选择的局部虚拟节点（LVN），这些虚拟节点具有可训练的嵌入，并在连接潜在瓶颈区域和改善长距离节点间通信方面发挥作用。

Result: 实验表明，LVN能有效提升结构连通性，在图分类和节点分类任务中显著提高了性能。

Conclusion: LVN方法在缓解过度压缩问题时兼顾了全局结构完整性，展示其在图结构任务中的可行性和优势。

Abstract: Over-squashing is a challenge in training graph neural networks for tasks
involving long-range dependencies. In such tasks, a GNN's receptive field
should be large enough to enable communication between distant nodes. However,
gathering information from a wide range of neighborhoods and squashing its
content into fixed-size node representations makes message-passing vulnerable
to bottlenecks. Graph rewiring and adding virtual nodes are commonly studied
remedies that create additional pathways around bottlenecks to mitigate
over-squashing. However, these techniques alter the input graph's global
topology and disrupt the domain knowledge encoded in the original graph
structure, both of which could be essential to specific tasks and domains. This
study presents Local Virtual Nodes (LVN) with trainable embeddings to alleviate
the effects of over-squashing without significantly corrupting the global
structure of the input graph. The position of the LVNs is determined by the
node centrality, which indicates the existence of potential bottlenecks. Thus,
the proposed approach aims to improve the connectivity in the regions with
likely bottlenecks. Furthermore, trainable LVN embeddings shared across
selected central regions facilitate communication between distant nodes without
adding more layers. Extensive experiments on benchmark datasets demonstrate
that LVNs can enhance structural connectivity and significantly improve
performance on graph and node classification tasks. The code can be found at
https://github.com/ALLab-Boun/LVN/}{https://github.com/ALLab-Boun/LVN/.

</details>


### [209] [Dimension Agnostic Testing of Survey Data Credibility through the Lens of Regression](https://arxiv.org/abs/2508.20616)
*Debabrota Basu,Sourav Chakraborty,Debarshi Chanda,Buddha Dev Das,Arijit Ghosh,Arnab Ray*

Main category: cs.LG

TL;DR: 提出了一种用于评估抽样调查是否能可靠代表总体的任务导向方法，利用模型特定的距离度量并提出了适用于回归模型的高效算法，其样本复杂度与数据维数无关。


<details>
  <summary>Details</summary>
Motivation: 在高维数据分布中评估样例调查对总体的可信度以保证后续研究的有效性。

Method: 提出基于任务的抽样调查可信度的评估方法，引入模型特定的距离度量，并设计了一种与数据维度无关的回归模型验证算法。

Result: 理论证明了方法的正确性，并通过实验验证了算法性能。

Conclusion: 该方法有效验证了抽样数据的可信度，同时解决了传统方法样本复杂度随维度线性增长的问题。

Abstract: Assessing whether a sample survey credibly represents the population is a
critical question for ensuring the validity of downstream research. Generally,
this problem reduces to estimating the distance between two high-dimensional
distributions, which typically requires a number of samples that grows
exponentially with the dimension. However, depending on the model used for data
analysis, the conclusions drawn from the data may remain consistent across
different underlying distributions. In this context, we propose a task-based
approach to assess the credibility of sampled surveys. Specifically, we
introduce a model-specific distance metric to quantify this notion of
credibility. We also design an algorithm to verify the credibility of survey
data in the context of regression models. Notably, the sample complexity of our
algorithm is independent of the data dimension. This efficiency stems from the
fact that the algorithm focuses on verifying the credibility of the survey data
rather than reconstructing the underlying regression model. Furthermore, we
show that if one attempts to verify credibility by reconstructing the
regression model, the sample complexity scales linearly with the dimensionality
of the data. We prove the theoretical correctness of our algorithm and
numerically demonstrate our algorithm's performance.

</details>


### [210] [Supervised Stochastic Gradient Algorithms for Multi-Trial Source Separation](https://arxiv.org/abs/2508.20618)
*Ronak Mehta,Mateus Piovezan Otto,Noah Stanis,Azadeh Yazdan-Shahmorad,Zaid Harchaoui*

Main category: cs.LG

TL;DR: 该论文提出一种结合多试验监督的独立成分分析随机算法，并在合成与实际数据实验中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决独立成分分析中的非凸优化问题，并提升成分的可解释性，作者引入了在许多科学情境中可用的多试验监督。

Method: 该方法结合了基于可逆矩阵空间的邻近梯度算法以及通过反向传播的预测模型联合学习。

Result: 与传统方法相比，该方法由于引入额外的监督，非凸优化的成功率有所提高，同时独立成分的可解释性也得到了改善。

Conclusion: 多试验监督的使用提高了独立成分分析的优化成功率和性能，为科学数据分析提供了更有效的工具。

Abstract: We develop a stochastic algorithm for independent component analysis that
incorporates multi-trial supervision, which is available in many scientific
contexts. The method blends a proximal gradient-type algorithm in the space of
invertible matrices with joint learning of a prediction model through
backpropagation. We illustrate the proposed algorithm on synthetic and real
data experiments. In particular, owing to the additional supervision, we
observe an increased success rate of the non-convex optimization and the
improved interpretability of the independent components.

</details>


### [211] [Masked Autoencoders for Ultrasound Signals: Robust Representation Learning for Downstream Applications](https://arxiv.org/abs/2508.20622)
*Immanuel Roßteutscher,Klaus S. Drese,Thorsten Uphues*

Main category: cs.LG

TL;DR: 本文探索了使用Vision Transformer (ViT) 架构的Masked Autoencoders (MAEs) 在一维超声信号中的自监督学习表现，发现其可在预训练后显著提升下游任务准确性，并优于从零开始训练的模型及传统CNN基线。


<details>
  <summary>Details</summary>
Motivation: 在工业应用中，超声信号至关重要，但其标注数据稀缺，且信号处理任务高度特定，因此需要一种能够无需大量标注数据便能高效学习表示的方法。

Method: 提出以MAE为基础的自监督学习方法，在无标注的合成超声信号上预训练模型，从而学习鲁棒的特征表示，用于下游任务，如飞行时间分类，并系统分析了模型规模、patch大小、掩码比例等对效果的影响。

Result: 实验结果表明，预训练模型在下游任务的性能上显著优于从零开始训练的模型和优化后的CNN，并且预训练在合成数据上的模型在转移到真实信号上表现优越。

Conclusion: MAEs在通过可扩展的自监督学习推动超声信号分析方面具有巨大潜力。

Abstract: We investigated the adaptation and performance of Masked Autoencoders (MAEs)
with Vision Transformer (ViT) architectures for self-supervised representation
learning on one-dimensional (1D) ultrasound signals. Although MAEs have
demonstrated significant success in computer vision and other domains, their
use for 1D signal analysis, especially for raw ultrasound data, remains largely
unexplored. Ultrasound signals are vital in industrial applications such as
non-destructive testing (NDT) and structural health monitoring (SHM), where
labeled data are often scarce and signal processing is highly task-specific. We
propose an approach that leverages MAE to pre-train on unlabeled synthetic
ultrasound signals, enabling the model to learn robust representations that
enhance performance in downstream tasks, such as time-of-flight (ToF)
classification. This study systematically investigated the impact of model
size, patch size, and masking ratio on pre-training efficiency and downstream
accuracy. Our results show that pre-trained models significantly outperform
models trained from scratch and strong convolutional neural network (CNN)
baselines optimized for the downstream task. Additionally, pre-training on
synthetic data demonstrates superior transferability to real-world measured
signals compared with training solely on limited real datasets. This study
underscores the potential of MAEs for advancing ultrasound signal analysis
through scalable, self-supervised learning.

</details>


### [212] [GDS Agent: A Graph Algorithmic Reasoning Agent](https://arxiv.org/abs/2508.20637)
*Borun Shi,Ioannis Panagiotas*

Main category: cs.LG

TL;DR: GDS agent通过向现有语言模型引入一组全面的图算法工具，结合预处理（检索）和后处理技术，提高了其处理大规模图结构数据的能力，并提出了一个新的基准来评估工具调用和最终结果。


<details>
  <summary>Details</summary>
Motivation: 语言模型虽然在多模态信息处理和推理方面表现出色，但在处理大规模图结构数据时仍有很大不足，需要一种工具提升其在这一领域的能力。

Method: GDS agent在一个模型上下文协议（MCP）服务器中结合了图算法工具、数据预处理和结果后处理，将其接入现代语言模型，并设计了一个新的评估基准。

Result: GDS agent能够准确解答涉及图算法推理的复杂问题，涵盖多种图任务，并通过案例研究展示了其在开放式任务中的表现及困难情景。

Conclusion: GDS agent在提升语言模型处理图结构数据能力方面有重要突破，但仍存在待解决的挑战和改进空间。

Abstract: Large language models (LLMs) have shown remarkable multimodal information
processing and reasoning ability. When equipped with tools through function
calling and enhanced with retrieval-augmented techniques, compound LLM-based
systems can access closed data sources and answer questions about them.
However, they still struggle to process and reason over large-scale
graph-structure data. We introduce the GDS (Graph Data Science) agent in this
technical report. The GDS agent introduces a comprehensive set of graph
algorithms as tools, together with preprocessing (retrieval) and postprocessing
of algorithm results, in a model context protocol (MCP) server. The server can
be used with any modern LLM out-of-the-box. GDS agent allows users to ask any
question that implicitly and intrinsically requires graph algorithmic reasoning
about their data, and quickly obtain accurate and grounded answers. We also
introduce a new benchmark that evaluates intermediate tool calls as well as
final responses. The results indicate that GDS agent is able to solve a wide
spectrum of graph tasks. We also provide detailed case studies for more
open-ended tasks and study scenarios where the agent struggles. Finally, we
discuss the remaining challenges and the future roadmap.

</details>


### [213] [A Hybrid Stochastic Gradient Tracking Method for Distributed Online Optimization Over Time-Varying Directed Networks](https://arxiv.org/abs/2508.20645)
*Xinli Shi,Xingxing Yuan,Longkang Zhu,Guanghui Wen*

Main category: cs.LG

TL;DR: 提出了TV-HSGT算法用于时间变化有向网络中的分布式在线优化，结合混合随机梯度跟踪和方差缩减机制，显著提高动态后悔界性能。


<details>
  <summary>Details</summary>
Motivation: 现有算法在处理时间变化的有向网络中分布式在线优化时，存在对有界梯度的依赖，忽略了随机梯度的影响。

Method: 提出了一种名为TV-HSGT的新算法，结合混合随机梯度跟踪与方差缩减机制，无需Perron向量估计或出度信息，集成行随机和列随机通信方案。

Result: 理论分析表明，TV-HSGT在无需假设梯度有界性的情况下，实现了改进的动态后悔界。而实验验证显示，其在动态及资源受限环境中的性能优异。

Conclusion: TV-HSGT是一种有效处理动态分布式优化问题的算法，特别适用于时间变化的有向图环境。

Abstract: With the increasing scale and dynamics of data, distributed online
optimization has become essential for real-time decision-making in various
applications. However, existing algorithms often rely on bounded gradient
assumptions and overlook the impact of stochastic gradients, especially in
time-varying directed networks. This study proposes a novel Time-Varying Hybrid
Stochastic Gradient Tracking algorithm named TV-HSGT, based on hybrid
stochastic gradient tracking and variance reduction mechanisms. Specifically,
TV-HSGT integrates row-stochastic and column-stochastic communication schemes
over time-varying digraphs, eliminating the need for Perron vector estimation
or out-degree information. By combining current and recursive stochastic
gradients, it effectively reduces gradient variance while accurately tracking
global descent directions. Theoretical analysis demonstrates that TV-HSGT can
achieve improved bounds on dynamic regret without assuming gradient
boundedness. Experimental results on logistic regression tasks confirm the
effectiveness of TV-HSGT in dynamic and resource-constrained environments.

</details>


### [214] [VarDiU: A Variational Diffusive Upper Bound for One-Step Diffusion Distillation](https://arxiv.org/abs/2508.20646)
*Leyang Wang,Mingtian Zhang,Zijing Ou,David Barber*

Main category: cs.LG

TL;DR: 本文提出一种新方法VarDiU，通过提供无偏梯度估计器提高单步扩散模型的生成质量与训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有单步扩散模型蒸馏方法的梯度估计存在偏差，导致性能次优。

Method: 提出一种名为VarDiU的变分扩散上界，通过无偏梯度估计实现扩散模型蒸馏。

Result: 相比Diff-Instruct方法，VarDiU在生成质量以及训练效率和稳定性上表现更优。

Conclusion: VarDiU通过无偏梯度估计改进了单步扩散蒸馏的效果，同时实现了更高效和稳定的训练过程。

Abstract: Recently, diffusion distillation methods have compressed thousand-step
teacher diffusion models into one-step student generators while preserving
sample quality. Most existing approaches train the student model using a
diffusive divergence whose gradient is approximated via the student's score
function, learned through denoising score matching (DSM). Since DSM training is
imperfect, the resulting gradient estimate is inevitably biased, leading to
sub-optimal performance. In this paper, we propose VarDiU (pronounced
/va:rdju:/), a Variational Diffusive Upper Bound that admits an unbiased
gradient estimator and can be directly applied to diffusion distillation. Using
this objective, we compare our method with Diff-Instruct and demonstrate that
it achieves higher generation quality and enables a more efficient and stable
training procedure for one-step diffusion distillation.

</details>


### [215] [Physics-Constrained Machine Learning for Chemical Engineering](https://arxiv.org/abs/2508.20649)
*Angan Mukherjee,Victor M. Zavala*

Main category: cs.LG

TL;DR: PCML结合物理模型和数据驱动方法，但在复杂的化学工程中面临嵌入物理知识类型、融合策略设计、模型扩展和预测不确定性等挑战。


<details>
  <summary>Details</summary>
Motivation: 通过PCML改进性能，应用于物理和数据驱动方法以增强可靠性、通用性和可解释性。

Method: 强调在化学工程应用中结合物理知识和机器学习（ML），解决多重技术难题，包括知识嵌入、融合策略设计和模型扩展。

Result: 综述最新开发并突出PCML在闭环实验设计、实时动态和控制以及处理多尺度现象中的潜力与应用。

Conclusion: PCML在化学工程领域仍有许多技术挑战，但在多领域上的发展潜力巨大。

Abstract: Physics-constrained machine learning (PCML) combines physical models with
data-driven approaches to improve reliability, generalizability, and
interpretability. Although PCML has shown significant benefits in diverse
scientific and engineering domains, technical and intellectual challenges
hinder its applicability in complex chemical engineering applications. Key
difficulties include determining the amount and type of physical knowledge to
embed, designing effective fusion strategies with ML, scaling models to large
datasets and simulators, and quantifying predictive uncertainty. This
perspective summarizes recent developments and highlights
challenges/opportunities in applying PCML to chemical engineering, emphasizing
on closed-loop experimental design, real-time dynamics and control, and
handling of multi-scale phenomena.

</details>


### [216] [Self-Composing Neural Operators with Depth and Accuracy Scaling via Adaptive Train-and-Unroll Approach](https://arxiv.org/abs/2508.20650)
*Juncai He,Xinliang Liu,Jinchao Xu*

Main category: cs.LG

TL;DR: 提出一种通过自我组合增强效率和准确性的神经算子框架，具有理论和实践优势。


<details>
  <summary>Details</summary>
Motivation: 受到求解偏微分方程数值方法的迭代方法启发，探索提升神经算子性能的方法。

Method: 通过重复应用单一神经算子模块逐步加深模型，并采用自适应训练-展开策略逐渐增加模型深度，同时揭示了一种准确性与深度的比例关系。

Result: 新架构在基准测试中达到SOTA性能，并在高频超声波计算断层成像问题上的复杂波现象处理中表现优异。

Conclusion: 该框架为大规模数据驱动的科学机器学习应用提供了一种可计算、高效且可扩展的解决方案。

Abstract: In this work, we propose a novel framework to enhance the efficiency and
accuracy of neural operators through self-composition, offering both
theoretical guarantees and practical benefits. Inspired by iterative methods in
solving numerical partial differential equations (PDEs), we design a specific
neural operator by repeatedly applying a single neural operator block, we
progressively deepen the model without explicitly adding new blocks, improving
the model's capacity. To train these models efficiently, we introduce an
adaptive train-and-unroll approach, where the depth of the neural operator is
gradually increased during training. This approach reveals an accuracy scaling
law with model depth and offers significant computational savings through our
adaptive training strategy. Our architecture achieves state-of-the-art (SOTA)
performance on standard benchmarks. We further demonstrate its efficacy on a
challenging high-frequency ultrasound computed tomography (USCT) problem, where
a multigrid-inspired backbone enables superior performance in resolving complex
wave phenomena. The proposed framework provides a computationally tractable,
accurate, and scalable solution for large-scale data-driven scientific machine
learning applications.

</details>


### [217] [Compositionality in Time Series: A Proof of Concept using Symbolic Dynamics and Compositional Data Augmentation](https://arxiv.org/abs/2508.20656)
*Michael Hagmann,Michael Staniek,Stefan Riezler*

Main category: cs.LG

TL;DR: 本文研究自然现象时间序列是否可以看作由系统性、有规则的潜在状态序列生成；实验表明基于合成数据训练的模型性能可与使用原始数据训练的模型相媲美。


<details>
  <summary>Details</summary>
Motivation: 探索时间序列生成的潜在组合性结构，以解决稀疏数据问题并更深入理解临床数据。

Method: 将时间序列的组合性看作数据生成过程属性，通过数据驱动方法重建基本状态和组合规则，并利用域适配方法测试生成数据的效果。

Result: 实验结果表明合成数据训练的模型测试结果与原始数据类似，并在器官衰竭评分预测任务中表现优异。

Conclusion: 本文证明了通过理解时间序列的组合性结构可以生成高质量的合成数据，为解决临床时间序列数据稀疏问题提供了新方法。

Abstract: This work investigates whether time series of natural phenomena can be
understood as being generated by sequences of latent states which are ordered
in systematic and regular ways. We focus on clinical time series and ask
whether clinical measurements can be interpreted as being generated by
meaningful physiological states whose succession follows systematic principles.
Uncovering the underlying compositional structure will allow us to create
synthetic data to alleviate the notorious problem of sparse and low-resource
data settings in clinical time series forecasting, and deepen our understanding
of clinical data. We start by conceptualizing compositionality for time series
as a property of the data generation process, and then study data-driven
procedures that can reconstruct the elementary states and composition rules of
this process. We evaluate the success of this methods using two empirical tests
originating from a domain adaptation perspective. Both tests infer the
similarity of the original time series distribution and the synthetic time
series distribution from the similarity of expected risk of time series
forecasting models trained and tested on original and synthesized data in
specific ways. Our experimental results show that the test set performance
achieved by training on compositionally synthesized data is comparable to
training on original clinical time series data, and that evaluation of models
on compositionally synthesized test data shows similar results to evaluating on
original test data, outperforming randomization-based data augmentation. An
additional downstream evaluation of the prediction task of sequential organ
failure assessment (SOFA) scores shows significant performance gains when model
training is entirely based on compositionally synthesized data compared to
training on original data.

</details>


### [218] [Token Buncher: Shielding LLMs from Harmful Reinforcement Learning Fine-Tuning](https://arxiv.org/abs/2508.20697)
*Weitao Feng,Lixu Wang,Tianyi Wei,Jie Zhang,Chongyang Gao,Sinong Zhan,Peizhuo Lv,Wei Dong*

Main category: cs.LG

TL;DR: 研究展示了强化学习（RL）比监督微调（SFT）更容易被用于有害目的微调，并提出了TokenBuncher方法作为专门针对此类威胁的防御措施，其通过抑制模型响应的不确定性以有效阻止这种利用。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）功能增强的同时，其通过微调被恶意滥用的风险也在增加。研究旨在探讨，尤其通过RL进行有害微调的可能性以及对应的防御方案。

Method: 提出TokenBuncher防御方法，通过抑制RL在模型响应不确定性上的依赖，限制奖励信号驱动模型走向有害行为。实现方式包括基于熵的奖励RL和令牌噪声机制。

Result: 实验表明，TokenBuncher有效削弱了RL有害微调的能力，同时保留了模型执行良性任务的实用性和微调能力。

Conclusion: 强化学习微调比监督微调更加具备系统风险，但TokenBuncher方法对其提供了有效的通用防御手段。

Abstract: As large language models (LLMs) continue to grow in capability, so do the
risks of harmful misuse through fine-tuning. While most prior studies assume
that attackers rely on supervised fine-tuning (SFT) for such misuse, we
systematically demonstrate that reinforcement learning (RL) enables adversaries
to more effectively break safety alignment and facilitate advanced harmful task
assistance, under matched computational budgets. To counter this emerging
threat, we propose TokenBuncher, the first effective defense specifically
targeting RL-based harmful fine-tuning. TokenBuncher suppresses the foundation
on which RL relies: model response uncertainty. By constraining uncertainty,
RL-based fine-tuning can no longer exploit distinct reward signals to drive the
model toward harmful behaviors. We realize this defense through
entropy-as-reward RL and a Token Noiser mechanism designed to prevent the
escalation of expert-domain harmful capabilities. Extensive experiments across
multiple models and RL algorithms show that TokenBuncher robustly mitigates
harmful RL fine-tuning while preserving benign task utility and finetunability.
Our results highlight that RL-based harmful fine-tuning poses a greater
systemic risk than SFT, and that TokenBuncher provides an effective and general
defense.

</details>


### [219] [EEGDM: Learning EEG Representation with Latent Diffusion Model](https://arxiv.org/abs/2508.20705)
*Shaocong Wang,Tong Liu,Ming Li,Minjing Yu,Yong-Jin Liu*

Main category: cs.LG

TL;DR: EEGDM是一种基于潜在扩散模型的新型自监督EEG表示学习方法，解决了EEG信号分析中的泛化性和数据有限问题。


<details>
  <summary>Details</summary>
Motivation: 现有的EEG表示学习方法难以在多任务泛化表现良好，尤其是在训练数据有限的情况下。

Method: EEGDM采用潜在扩散模型作为自监督目标，通过生成EEG信号学习表示，并设计了紧凑的EEG编码器，提取信号和通道增强信息。

Result: EEGDM成功重建高质量EEG信号，有效学习强鲁棒性表示，在有限的预训练数据情况下仍在下游任务中表现竞争力。

Conclusion: EEGDM展示了其在EEG信号分析中的泛化能力和实际应用潜力。

Abstract: While electroencephalography (EEG) signal analysis using deep learning has
shown great promise, existing approaches still face significant challenges in
learning generalizable representations that perform well across diverse tasks,
particularly when training data is limited. Current EEG representation learning
methods including EEGPT and LaBraM typically rely on simple masked
reconstruction objective, which may not fully capture the rich semantic
information and complex patterns inherent in EEG signals. In this paper, we
propose EEGDM, a novel self-supervised EEG representation learning method based
on the latent diffusion model, which leverages EEG signal generation as a
self-supervised objective, turning the diffusion model into a strong
representation learner capable of capturing EEG semantics. EEGDM incorporates
an EEG encoder that distills EEG signals and their channel augmentations into a
compact representation, acting as conditional information to guide the
diffusion model for generating EEG signals. This design endows EEGDM with a
compact latent space, which not only offers ample control over the generative
process but also can be leveraged for downstream tasks. Experimental results
show that EEGDM (1) can reconstruct high-quality EEG signals, (2) effectively
learns robust representations, and (3) achieves competitive performance with
modest pre-training data size across diverse downstream tasks, underscoring its
generalizability and practical utility.

</details>


### [220] [Provable Benefits of In-Tool Learning for Large Language Models](https://arxiv.org/abs/2508.20755)
*Sam Houliston,Ambroise Odonnat,Charles Arnal,Vivien Cabannes*

Main category: cs.LG

TL;DR: 探讨了工具增强语言模型的优势，并从理论和实验两方面验证了其超越单纯记忆的能力。


<details>
  <summary>Details</summary>
Motivation: 研究工具增强语言模型的理论优势，特别是外部检索相比内在记忆的优越性。

Method: 通过理论证明和控制实验比较，分析工具使用模型与单纯记忆模型在事实召回能力上的差异。

Result: 证明工具使用可使事实召回的容量突破模型参数限制，实验显示工具增强模型在准确性上持续优于记忆模型。

Conclusion: 工具增强工作流不仅在实践上可行，在扩展性上也更具理论优势。

Abstract: Tool-augmented language models, equipped with retrieval, memory, or external
APIs, are reshaping AI, yet their theoretical advantages remain underexplored.
In this paper, we address this question by demonstrating the benefits of
in-tool learning (external retrieval) over in-weight learning (memorization)
for factual recall. We show that the number of facts a model can memorize
solely in its weights is fundamentally limited by its parameter count. In
contrast, we prove that tool-use enables unbounded factual recall via a simple
and efficient circuit construction. These results are validated in controlled
experiments, where tool-using models consistently outperform memorizing ones.
We further show that for pretrained large language models, teaching tool-use
and general rules is more effective than finetuning facts into memory. Our work
provides both a theoretical and empirical foundation, establishing why
tool-augmented workflows are not just practical, but provably more scalable.

</details>


### [221] [Unleashing Uncertainty: Efficient Machine Unlearning for Generative AI](https://arxiv.org/abs/2508.20773)
*Christoforos N. Spartalis,Theodoros Semertzidis,Petros Daras,Efstratios Gavves*

Main category: cs.LG

TL;DR: SAFEMax 是一种用于扩散模型的机器遗忘新方法，利用信息理论最大化生成图片的熵，使模型对于不允许的类别只生成高斯噪声。


<details>
  <summary>Details</summary>
Motivation: 探索一种能够有效平衡遗忘与保持的机器学习方法，专注于早期扩散步骤中的类别信息。

Method: 基于信息论原理，通过最大化图片熵和控制扩散初期的类别信息来实现遗忘与保留的平衡。

Result: 实验结果表明，SAFEMax 在效率上远超当前的最先进方法，并且能够有效执行机器遗忘。

Conclusion: SAFEMax 提供了一种高效且灵活的机器遗忘方法，对扩散模型中的不允许类别行为处理具有重要意义。

Abstract: We introduce SAFEMax, a novel method for Machine Unlearning in diffusion
models. Grounded in information-theoretic principles, SAFEMax maximizes the
entropy in generated images, causing the model to generate Gaussian noise when
conditioned on impermissible classes by ultimately halting its denoising
process. Also, our method controls the balance between forgetting and retention
by selectively focusing on the early diffusion steps, where class-specific
information is prominent. Our results demonstrate the effectiveness of SAFEMax
and highlight its substantial efficiency gains over state-of-the-art methods.

</details>


### [222] [cMALC-D: Contextual Multi-Agent LLM-Guided Curriculum Learning with Diversity-Based Context Blending](https://arxiv.org/abs/2508.20818)
*Anirudh Satheesh,Keenan Powell,Hua Wei*

Main category: cs.LG

TL;DR: 提出了cMALC-D框架，用于改进多智能体强化学习在复杂不确定环境中的泛化性和样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体强化学习算法在固定环境中训练，难以适应现实中的复杂与不确定条件，需开发更鲁棒的训练与评估框架。

Method: 利用大语言模型（LLMs）生成有意义的课程，同时引入基于多样性的上下文混合机制，通过结合现有上下文特征生成新的训练场景，防止模式崩塌并鼓励探索。

Result: 在交通信号控制领域实验中，cMALC-D显著提升了泛化能力和样本利用效率，相较于现有基线有明显优势。

Conclusion: cMALC-D框架通过语义课程引导与多样性混合机制，改善了多智能体场景中的训练与评估表现，具有较好的实际应用前景。

Abstract: Many multi-agent reinforcement learning (MARL) algorithms are trained in
fixed simulation environments, making them brittle when deployed in real-world
scenarios with more complex and uncertain conditions. Contextual MARL (cMARL)
addresses this by parameterizing environments with context variables and
training a context-agnostic policy that performs well across all environment
configurations. Existing cMARL methods attempt to use curriculum learning to
help train and evaluate context-agnostic policies, but they often rely on
unreliable proxy signals, such as value estimates or generalized advantage
estimates that are noisy and unstable in multi-agent settings due to
inter-agent dynamics and partial observability. To address these issues, we
propose Contextual Multi-Agent LLM-Guided Curriculum Learning with
Diversity-Based Context Blending (cMALC-D), a framework that uses Large
Language Models (LLMs) to generate semantically meaningful curricula and
provide a more robust evaluation signal. To prevent mode collapse and encourage
exploration, we introduce a novel diversity-based context blending mechanism
that creates new training scenarios by combining features from prior contexts.
Experiments in traffic signal control domains demonstrate that cMALC-D
significantly improves both generalization and sample efficiency compared to
existing curriculum learning baselines. We provide code at
https://github.com/DaRL-LibSignal/cMALC-D.

</details>


### [223] [GPT-FT: An Efficient Automated Feature Transformation Using GPT for Sequence Reconstruction and Performance Enhancement](https://arxiv.org/abs/2508.20824)
*Yang Gao,Dongjie Wang,Scott Piersall,Ye Zhang,Liqiang Wang*

Main category: cs.LG

TL;DR: 研究提出了一种基于改进的GPT模型的新框架，用于自动化特征变换，通过减少参数量和加速处理，提高了计算效率及表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，连续嵌入优化虽然有效，但因使用序列编码器-解码器结构导致计算成本和参数需求高，限制了可扩展性和效率，有必要改进特征变换方法。

Method: 提出的框架包括四个步骤：变换记录收集、基于改进GPT模型嵌入空间构建、梯度上升搜索以及自回归重构。改进的GPT模型用于特征变换序列重构与模型性能评估和提升。

Result: 实验结果表明，该框架在多个基准数据集上匹敌或超过基线方法，并大幅提升了计算效率。

Conclusion: 基于Transformer的架构在实现高效、可扩展的自动化特征变换方面具有巨大潜力。

Abstract: Feature transformation plays a critical role in enhancing machine learning
model performance by optimizing data representations. Recent state-of-the-art
approaches address this task as a continuous embedding optimization problem,
converting discrete search into a learnable process. Although effective, these
methods often rely on sequential encoder-decoder structures that cause high
computational costs and parameter requirements, limiting scalability and
efficiency. To address these limitations, we propose a novel framework that
accomplishes automated feature transformation through four steps:
transformation records collection, embedding space construction with a revised
Generative Pre-trained Transformer (GPT) model, gradient-ascent search, and
autoregressive reconstruction. In our approach, the revised GPT model serves
two primary functions: (a) feature transformation sequence reconstruction and
(b) model performance estimation and enhancement for downstream tasks by
constructing the embedding space. Such a multi-objective optimization framework
reduces parameter size and accelerates transformation processes. Experimental
results on benchmark datasets show that the proposed framework matches or
exceeds baseline performance, with significant gains in computational
efficiency. This work highlights the potential of transformer-based
architectures for scalable, high-performance automated feature transformation.

</details>


### [224] [ATM-GAD: Adaptive Temporal Motif Graph Anomaly Detection for Financial Transaction Networks](https://arxiv.org/abs/2508.20829)
*Zeyue Zhang,Lin Song,Erkang Bao,Xiaoling Lv,Xinyue Wang*

Main category: cs.LG

TL;DR: 提出了一种名为ATM-GAD的自适应图神经网络，用于金融欺诈检测，通过时间模式提取器和双注意力模块利用时间子图揭示异常行为，并引入自适应时间窗口优化节点观察。在多个数据集上的实验显示其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有金融系统中由快速变化的交易行为和复杂的实体交互导致传统机器学习方法难以及时检测欺诈行为。

Method: 提出ATM-GAD模型，该模型包括时间模式提取器，提取交易历史中具有拓扑和时间模式的信息；双注意力模块分别分析单一模式和跨模式的交互；另有自适应时间窗口学习器以识别最有意义的时间切片。

Result: 在四个真实数据集上，ATM-GAD在欺诈检测效果上优于七个现有基线方法，展现出强大的欺诈检测能力。

Conclusion: ATM-GAD能够有效捕捉时间模式和异常行为，提供了现代金融欺诈检测的先进方法，并验证了其在实际数据集中的优越性能。

Abstract: Financial fraud detection is essential to safeguard billions of dollars, yet
the intertwined entities and fast-changing transaction behaviors in modern
financial systems routinely defeat conventional machine learning models. Recent
graph-based detectors make headway by representing transactions as networks,
but they still overlook two fraud hallmarks rooted in time: (1) temporal
motifs--recurring, telltale subgraphs that reveal suspicious money flows as
they unfold--and (2) account-specific intervals of anomalous activity, when
fraud surfaces only in short bursts unique to each entity. To exploit both
signals, we introduce ATM-GAD, an adaptive graph neural network that leverages
temporal motifs for financial anomaly detection. A Temporal Motif Extractor
condenses each account's transaction history into the most informative motifs,
preserving both topology and temporal patterns. These motifs are then analyzed
by dual-attention blocks: IntraA reasons over interactions within a single
motif, while InterA aggregates evidence across motifs to expose multi-step
fraud schemes. In parallel, a differentiable Adaptive Time-Window Learner
tailors the observation window for every node, allowing the model to focus
precisely on the most revealing time slices. Experiments on four real-world
datasets show that ATM-GAD consistently outperforms seven strong
anomaly-detection baselines, uncovering fraud patterns missed by earlier
methods.

</details>


### [225] [Practical Physical Layer Authentication for Mobile Scenarios Using a Synthetic Dataset Enhanced Deep Learning Approach](https://arxiv.org/abs/2508.20861)
*Yijia Guo,Junqing Zhang,Y. -W. Peter Hong*

Main category: cs.LG

TL;DR: 本文探讨了一种深度学习支持的物理层设备认证方法，目标是解决传感器动态环境下的无线传输安全问题，实验显示此方法在设备认证性能上有显著提高。


<details>
  <summary>Details</summary>
Motivation: 动机在于现有物理层设备认证方法无法有效应对动态信道变化带来的挑战。

Method: 利用基于卷积神经网络 (CNN) 的Siamese网络，分析信道状态信息(CSI)的时间和空间相关性，同时通过仿真和实验方法综合验证。

Result: 实验结果显示，该深度学习模型较传统基准算法有显著性能提升，如ROC-AUC值提升0.03到0.06。

Conclusion: 提出的方法在动态无线环境下实现了优越的设备认证性能，为IoT设备安全认证提供了可行的新方案。

Abstract: The Internet of Things (IoT) is ubiquitous thanks to the rapid development of
wireless technologies. However, the broadcast nature of wireless transmissions
results in great vulnerability to device authentication. Physical layer
authentication emerges as a promising approach by exploiting the unique channel
characteristics. However, a practical scheme applicable to dynamic channel
variations is still missing. In this paper, we proposed a deep learning-based
physical layer channel state information (CSI) authentication for mobile
scenarios and carried out comprehensive simulation and experimental evaluation
using IEEE 802.11n. Specifically, a synthetic training dataset was generated
based on the WLAN TGn channel model and the autocorrelation and the distance
correlation of the channel, which can significantly reduce the overhead of
manually collecting experimental datasets. A convolutional neural network
(CNN)-based Siamese network was exploited to learn the temporal and spatial
correlation between the CSI pair and output a score to measure their
similarity. We adopted a synergistic methodology involving both simulation and
experimental evaluation. The experimental testbed consisted of WiFi IoT
development kits and a few typical scenarios were specifically considered. Both
simulation and experimental evaluation demonstrated excellent generalization
performance of our proposed deep learning-based approach and excellent
authentication performance. Demonstrated by our practical measurement results,
our proposed scheme improved the area under the curve (AUC) by 0.03 compared to
the fully connected network-based (FCN-based) Siamese model and by 0.06
compared to the correlation-based benchmark algorithm.

</details>


### [226] [LeMat-Traj: A Scalable and Unified Dataset of Materials Trajectories for Atomistic Modeling](https://arxiv.org/abs/2508.20875)
*Ali Ramlaoui,Martin Siron,Inel Djafar,Joseph Musielewicz,Amandine Rossello,Victor Schmidt,Alexandre Duval*

Main category: cs.LG

TL;DR: 本文提出了LeMat-Traj，一个整合并标准化的量子机械轨迹数据集，大大降低了训练高精度机器学习原子间势能模型的难度。


<details>
  <summary>Details</summary>
Motivation: 量子力学轨迹数据集因分散性和格式不一致性，难以有效应用于机器学习原子间势能研究。

Method: 通过收集并标准化多个大型数据集，建立了包含1.2亿+原子配置的LeMat-Traj，同时开发了LeMaterial-Fetcher工具来确保数据集的扩展和易用性。

Result: 利用LeMat-Traj，训练的模型在预测力误差上表现大幅改进，并对松弛任务有显著优化。

Conclusion: LeMat-Traj和LeMaterial-Fetcher提供了一个强大的资源和工具，为机器学习在材料科学中的应用开辟了新的可能性。

Abstract: The development of accurate machine learning interatomic potentials (MLIPs)
is limited by the fragmented availability and inconsistent formatting of
quantum mechanical trajectory datasets derived from Density Functional Theory
(DFT). These datasets are expensive to generate yet difficult to combine due to
variations in format, metadata, and accessibility. To address this, we
introduce LeMat-Traj, a curated dataset comprising over 120 million atomic
configurations aggregated from large-scale repositories, including the
Materials Project, Alexandria, and OQMD. LeMat-Traj standardizes data
representation, harmonizes results and filters for high-quality configurations
across widely used DFT functionals (PBE, PBESol, SCAN, r2SCAN). It
significantly lowers the barrier for training transferrable and accurate MLIPs.
LeMat-Traj spans both relaxed low-energy states and high-energy, high-force
structures, complementing molecular dynamics and active learning datasets. By
fine-tuning models pre-trained on high-force data with LeMat-Traj, we achieve a
significant reduction in force prediction errors on relaxation tasks. We also
present LeMaterial-Fetcher, a modular and extensible open-source library
developed for this work, designed to provide a reproducible framework for the
community to easily incorporate new data sources and ensure the continued
evolution of large-scale materials datasets. LeMat-Traj and LeMaterial-Fetcher
are publicly available at https://huggingface.co/datasets/LeMaterial/LeMat-Traj
and https://github.com/LeMaterial/lematerial-fetcher.

</details>


### [227] [Turning Tabular Foundation Models into Graph Foundation Models](https://arxiv.org/abs/2508.20906)
*Dmitry Eremeev,Gleb Bazhenov,Oleg Platonov,Artem Babenko,Liudmila Prokhorenkova*

Main category: cs.LG

TL;DR: 提出一种名为G2T-FM的图基础模型，利用TabPFNv2作为核心，结合图节点特性和结构信息，展示了在图学习任务中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图基础模型难以应对多样化的节点特征，尤其在处理非文本属性时存在不足。

Method: 将TabPFNv2作为核心，结合图的邻居特征聚合和结构嵌入，形成适用于多种节点特征的模型。

Result: 无论是在上下文模式还是微调后，该模型均达到强大的效果，并且超越现有的GFM和精细调试的GNN。

Conclusion: 本文揭示了利用表格基础模型支持图机器学习的新方向，并证明其在处理复杂节点特征上的潜力。

Abstract: While foundation models have revolutionized such fields as natural language
processing and computer vision, their application and potential within graph
machine learning remain largely unexplored. One of the key challenges in
designing graph foundation models (GFMs) is handling diverse node features that
can vary across different graph datasets. Although many works on GFMs have been
focused exclusively on text-attributed graphs, the problem of handling
arbitrary features of other types in GFMs has not been fully addressed.
However, this problem is not unique to the graph domain, as it also arises in
the field of machine learning for tabular data. In this work, motivated by the
recent success of tabular foundation models like TabPFNv2, we propose G2T-FM, a
simple graph foundation model that employs TabPFNv2 as a backbone.
Specifically, G2T-FM augments the original node features with neighborhood
feature aggregation, adds structural embeddings, and then applies TabPFNv2 to
the constructed node representations. Even in a fully in-context regime, our
model achieves strong results, significantly outperforming publicly available
GFMs and performing on par with well-tuned GNNs trained from scratch. Moreover,
after finetuning, G2T-FM surpasses well-tuned GNN baselines, highlighting the
potential of the proposed approach. More broadly, our paper reveals a
previously overlooked direction of utilizing tabular foundation models for
graph machine learning tasks.

</details>


### [228] [Finite-Time Guarantees for Multi-Agent Combinatorial Bandits with Nonstationary Rewards](https://arxiv.org/abs/2508.20923)
*Katherine B. Adams,Justin J. Boutilier,Qinyang He,Yonatan Mintz*

Main category: cs.LG

TL;DR: 研究动态资源分配问题，提出算法在非平稳奖励情况下优化长期效果，实际案例显示算法显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决动态干预效果问题，优化个体化资源分配策略以提升整体效益。

Method: 引入非平稳奖励情况下的组合多臂老虎机模型，设计了具有动态遗憾理论保证的算法，并通过案例研究验证。

Result: 通过糖尿病干预案例研究，算法的计划参与率提升了多达三倍。

Conclusion: 该框架有效结合了自适应学习理论与实际行为干预挑战，对实际应用具有重要潜力。

Abstract: We study a sequential resource allocation problem where a decision maker
selects subsets of agents at each period to maximize overall outcomes without
prior knowledge of individual-level effects. Our framework applies to settings
such as community health interventions, targeted digital advertising, and
workforce retention programs, where intervention effects evolve dynamically.
Agents may exhibit habituation (diminished response from frequent selection) or
recovery (enhanced response from infrequent selection). The technical challenge
centers on nonstationary reward distributions that lead to changing
intervention effects over time. The problem requires balancing two key
competing objectives: heterogeneous individual rewards and the
exploration-exploitation tradeoff in terms of learning for improved future
decisions as opposed to maximizing immediate outcomes. Our contribution
introduces the first framework incorporating this form of nonstationary rewards
in the combinatorial multi-armed bandit literature. We develop algorithms with
theoretical guarantees on dynamic regret and demonstrate practical efficacy
through a diabetes intervention case study. Our personalized community
intervention algorithm achieved up to three times as much improvement in
program enrollment compared to baseline approaches, validating the framework's
potential for real-world applications. This work bridges theoretical advances
in adaptive learning with practical challenges in population-level behavioral
change interventions.

</details>


### [229] [Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees](https://arxiv.org/abs/2508.21001)
*Yaniv Hassidof,Tom Jurgenson,Kiril Solovey*

Main category: cs.LG

TL;DR: 本文提出了Diffusion Tree (DiTree)框架，通过结合采样规划器(SBPs)的完整性和扩散策略(DPs)的智能采样能力，实现了快速且安全的运动规划。


<details>
  <summary>Details</summary>
Motivation: 传统基于采样的规划器(SBPs)性能易受慢速探索和无效采样的限制，而学习方法尽管快但缺乏对分布外场景(OOD)的泛化能力和安全性保证。此工作旨在克服这些缺陷，推广到实物机器人中。

Method: DiTree结合了扩散策略(DPs)的能力以建模专家轨迹复杂分布，并将其应用为采样规划器(SBPs)中的智能采样模块，从而显著提高状态空间搜索效率。其方法以经典的随机快速探索树(RRT)与DP模块结合实现。

Result: 在分布外(OOD)场景中，DiTree实现了与独立扩散策略相当的运行时间，平均较经典SBPs快3倍，并且在成功率方面比其他方法高出约30%。

Conclusion: DiTree通过将学习方法与传统规划器结合，不仅改进了探索速度与成功率，还能够提供保证安全的高质量解决方案，有望更好应用在真实机器人中。

Abstract: Kinodynamic motion planning is concerned with computing collision-free
trajectories while abiding by the robot's dynamic constraints. This critical
problem is often tackled using sampling-based planners (SBPs) that explore the
robot's high-dimensional state space by constructing a search tree via action
propagations. Although SBPs can offer global guarantees on completeness and
solution quality, their performance is often hindered by slow exploration due
to uninformed action sampling. Learning-based approaches can yield
significantly faster runtimes, yet they fail to generalize to
out-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety,
thus limiting their deployment on physical robots. We present Diffusion Tree
(DiTree): a \emph{provably-generalizable} framework leveraging diffusion
policies (DPs) as informed samplers to efficiently guide state-space search
within SBPs. DiTree combines DP's ability to model complex distributions of
expert trajectories, conditioned on local observations, with the completeness
of SBPs to yield \emph{provably-safe} solutions within a few action propagation
iterations for complex dynamical systems. We demonstrate DiTree's power with an
implementation combining the popular RRT planner with a DP action sampler
trained on a \emph{single environment}. In comprehensive evaluations on OOD
scenarios, % DiTree has comparable runtimes to a standalone DP (3x faster than
classical SBPs), while improving the average success rate over DP and SBPs.
DiTree is on average 3x faster than classical SBPs, and outperforms all other
approaches by achieving roughly 30\% higher success rate. Project webpage:
https://sites.google.com/view/ditree.

</details>


### [230] [InSQuAD: In-Context Learning for Efficient Retrieval via Submodular Mutual Information to Enforce Quality and Diversity](https://arxiv.org/abs/2508.21003)
*Souradeep Nanda,Anay Majee,Rishabh Iyer*

Main category: cs.LG

TL;DR: 我们提出了InSQuAD，利用子模互信息（SMI）提高了ICL模型选择示例的效能，从而提升ICL性能。


<details>
  <summary>Details</summary>
Motivation: 当前的ICL模型在挑选示例时，通常关注相关性，而忽视了多样性，这对提高ICL任务的表现至关重要。

Method: 将ICL任务建模为目标选择问题，设计基于SMI的选择策略，注重质量与多样性的示例采样；通过新的似然损失函数训练检索模型，增强其对质量和多样性的学习能力；还通过合成的问答数据增强了训练集。

Result: 在九个基准数据集上验证了该方法的有效性，并显著提升了ICL的性能。

Conclusion: InSQuAD证明在ICL任务中，通过综合考虑示例的质量与多样性，可以有效提升模型表现。

Abstract: In this paper, we introduce InSQuAD, designed to enhance the performance of
In-Context Learning (ICL) models through Submodular Mutual Information} (SMI)
enforcing Quality and Diversity among in-context exemplars. InSQuAD achieves
this through two principal strategies: First, we model the ICL task as a
targeted selection problem and introduce a unified selection strategy based on
SMIs which mines relevant yet diverse in-context examples encapsulating the
notions of quality and diversity. Secondly, we address a common pitfall in
existing retrieval models which model query relevance, often overlooking
diversity, critical for ICL. InSQuAD introduces a combinatorial training
paradigm which learns the parameters of an SMI function to enforce both quality
and diversity in the retrieval model through a novel likelihood-based loss. To
further aid the learning process we augment an existing multi-hop question
answering dataset with synthetically generated paraphrases. Adopting the
retrieval model trained using this strategy alongside the novel targeted
selection formulation for ICL on nine benchmark datasets shows significant
improvements validating the efficacy of our approach.

</details>


### [231] [Inference-Time Alignment Control for Diffusion Models with Reinforcement Learning Guidance](https://arxiv.org/abs/2508.21016)
*Luozhijie Jin,Zijie Qiu,Jie Liu,Zijie Diao,Lifeng Qiao,Ning Ding,Alex Lamb,Xipeng Qiu*

Main category: cs.LG

TL;DR: 提出了一种称为Reinforcement Learning Guidance (RLG)的方法，通过几何平均结合基础模型和经过强化学习（RL）微调的模型，在推理时动态调整生成模型的输出，以改进扩散模型的对齐能力。


<details>
  <summary>Details</summary>
Motivation: 现有的去噪生成模型，如扩散模型，难以针对复杂目标（如人类偏好、组合准确性或数据压缩性）调整生成结果。传统强化学习微调方法对扩散模型效果不佳，并且在微调后对对齐程度的灵活控制有限。

Method: 通过将强化学习微调重新解读为随机微分方程和隐式奖励条件下的过程，提出了一种名为RLG的推理时控制方法。在推理时RLG结合了原始模型与RL微调模型的输出，通过几何平均实现指导，并允许动态调整对齐强度。

Result: 实验表明，RLG能够增强RL微调模型在不同架构、算法及下游任务（如人类偏好、组合控制、压缩性及文本渲染）上的性能。此外，RLG支持插值和外推，为生成对齐的控制提供了前所未有的灵活性。

Conclusion: RLG方法是一种实用且理论上可靠的解决方案，在推断过程中有效增强扩散模型的对齐能力，代码已公开。

Abstract: Denoising-based generative models, particularly diffusion and flow matching
algorithms, have achieved remarkable success. However, aligning their output
distributions with complex downstream objectives, such as human preferences,
compositional accuracy, or data compressibility, remains challenging. While
reinforcement learning (RL) fine-tuning methods, inspired by advances in RL
from human feedback (RLHF) for large language models, have been adapted to
these generative frameworks, current RL approaches are suboptimal for diffusion
models and offer limited flexibility in controlling alignment strength after
fine-tuning. In this work, we reinterpret RL fine-tuning for diffusion models
through the lens of stochastic differential equations and implicit reward
conditioning. We introduce Reinforcement Learning Guidance (RLG), an
inference-time method that adapts Classifier-Free Guidance (CFG) by combining
the outputs of the base and RL fine-tuned models via a geometric average. Our
theoretical analysis shows that RLG's guidance scale is mathematically
equivalent to adjusting the KL-regularization coefficient in standard RL
objectives, enabling dynamic control over the alignment-quality trade-off
without further training. Extensive experiments demonstrate that RLG
consistently improves the performance of RL fine-tuned models across various
architectures, RL algorithms, and downstream tasks, including human
preferences, compositional control, compressibility, and text rendering.
Furthermore, RLG supports both interpolation and extrapolation, thereby
offering unprecedented flexibility in controlling generative alignment. Our
approach provides a practical and theoretically sound solution for enhancing
and controlling diffusion model alignment at inference. The source code for RLG
is publicly available at the Github:
https://github.com/jinluo12345/Reinforcement-learning-guidance.

</details>


### [232] [Fast Convergence Rates for Subsampled Natural Gradient Algorithms on Quadratic Model Problems](https://arxiv.org/abs/2508.21022)
*Gil Goldshlager,Jiang Hu,Lin Lin*

Main category: cs.LG

TL;DR: 本文分析了子采样自然梯度下降法（SNGD）及其加速变体SPRING的收敛性，为此类方法在科学机器学习中的应用提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 尽管SNGD在科学机器学习中的参数优化任务中表现出色，但其缺乏理论解释，本文旨在填补这一空白。

Method: 在线性模型和强凸二次损失的理想化条件下，分析SNGD及SPRING的收敛性，并将它们与正则化Kaczmarz方法及其加速版等效，从而借助随机线性代数工具进行理论研究。

Result: 在轻松条件下，首次证明了SNGD的快速收敛率，SPRING在任意条件下的收敛性，以及SPRING能够加速SNGD的能力；在一般强凸二次损失中，得到了SNGD的快速收敛性，为SNGD在最小二乘外的有效性提供了理论解释。

Conclusion: 本研究展示了随机线性代数工具如何为子采样及曲率感知优化策略的交互提供新见解，解决了SNGD及SPRING缺乏理论解释的问题。

Abstract: Subsampled natural gradient descent (SNGD) has shown impressive results for
parametric optimization tasks in scientific machine learning, such as neural
network wavefunctions and physics-informed neural networks, but it has lacked a
theoretical explanation. We address this gap by analyzing the convergence of
SNGD and its accelerated variant, SPRING, for idealized parametric optimization
problems where the model is linear and the loss function is strongly convex and
quadratic. In the special case of a least-squares loss, namely the standard
linear least-squares problem, we prove that SNGD is equivalent to a regularized
Kaczmarz method while SPRING is equivalent to an accelerated regularized
Kaczmarz method. As a result, by leveraging existing analyses we obtain under
mild conditions (i) the first fast convergence rate for SNGD, (ii) the first
convergence guarantee for SPRING in any setting, and (iii) the first proof that
SPRING can accelerate SNGD. In the case of a general strongly convex quadratic
loss, we extend the analysis of the regularized Kaczmarz method to obtain a
fast convergence rate for SNGD under stronger conditions, providing the first
explanation for the effectiveness of SNGD outside of the least-squares setting.
Overall, our results illustrate how tools from randomized linear algebra can
shed new light on the interplay between subsampling and curvature-aware
optimization strategies.

</details>


### [233] [LoTUS: Large-Scale Machine Unlearning with a Taste of Uncertainty](https://arxiv.org/abs/2503.18314)
*Christoforos N. Spartalis,Theodoros Semertzidis,Efstratios Gavves,Petros Daras*

Main category: cs.LG

TL;DR: 本文提出了一种新的机器学习卸载方法LoTUS，可以在不从头训练的情况下从预训练模型中移除特定训练样本的影响。


<details>
  <summary>Details</summary>
Motivation: 解决数据记忆导致的模型过度自信问题，并支持在大规模数据集和实际场景中高效移除训练样本影响。

Method: LoTUS通过平滑模型的预测概率，达到信息论阈值，从而减少由于数据记忆引起的过度自信，并引入了无需重训练的Jensen-Shannon散度指标（RF-JSD）。

Result: 实验表明，在Transformer和ResNet18模型及五个公共数据集中，LoTUS在效率和效果方面优于八个现有方法；并在ImageNet1k大规模数据集条件下表现出色。

Conclusion: LoTUS方法在机器学习卸载任务中提供了一种无需重训练的高效解决方案，并在实验中表现出卓越性能，适用于实际条件。

Abstract: We present LoTUS, a novel Machine Unlearning (MU) method that eliminates the
influence of training samples from pre-trained models, avoiding retraining from
scratch. LoTUS smooths the prediction probabilities of the model up to an
information-theoretic bound, mitigating its over-confidence stemming from data
memorization. We evaluate LoTUS on Transformer and ResNet18 models against
eight baselines across five public datasets. Beyond established MU benchmarks,
we evaluate unlearning on ImageNet1k, a large-scale dataset, where retraining
is impractical, simulating real-world conditions. Moreover, we introduce the
novel Retrain-Free Jensen-Shannon Divergence (RF-JSD) metric to enable
evaluation under real-world conditions. The experimental results show that
LoTUS outperforms state-of-the-art methods in terms of both efficiency and
effectiveness. Code: https://github.com/cspartalis/LoTUS.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [234] [Particle swarm optimization for online sparse streaming feature selection under uncertainty](https://arxiv.org/abs/2508.20123)
*Ruiyang Xu*

Main category: cs.NE

TL;DR: 研究提出了一种新方法POS2FS，通过PSO和三方决策理论优化在线稀疏流特征选择问题，能更好处理高维流数据的不确定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的在线流特征选择（OSFS）方法在处理特征-标签相关性的不确定性时有局限性，容易导致性能下降。该研究试图通过引入新框架，改进该问题的性能表现。

Method: 提出了一个基于粒子群优化（PSO）的不确定性感知的在线稀疏流特征选择框架POS2FS，并结合三方决策理论，用于降低特征标签关系的不确定性及处理监督学习过程中的特征模糊性。

Result: 通过在六个实际数据集上的严格测试，POS2FS相比传统的OSFS与OS2FS技术，在特征子集选择上具有更高的准确性和更强的鲁棒性。

Conclusion: POS2FS提供了一种更加灵活的在线流特征选择方法，成功解决了不确定性问题，为实际高维流数据分析提供了更优解决方案。

Abstract: In real-world applications involving high-dimensional streaming data, online
streaming feature selection (OSFS) is widely adopted. Yet, practical
deployments frequently face data incompleteness due to sensor failures or
technical constraints. While online sparse streaming feature selection (OS2FS)
mitigates this issue via latent factor analysis-based imputation, existing
methods struggle with uncertain feature-label correlations, leading to
inflexible models and degraded performance. To address these gaps, this work
proposes POS2FS-an uncertainty-aware online sparse streaming feature selection
framework enhanced by particle swarm optimization (PSO). The approach
introduces: 1) PSO-driven supervision to reduce uncertainty in feature-label
relationships; 2) Three-way decision theory to manage feature fuzziness in
supervised learning. Rigorous testing on six real-world datasets confirms
POS2FS outperforms conventional OSFS and OS2FS techniques, delivering higher
accuracy through more robust feature subset selection.

</details>


### [235] [Improving Liver Disease Diagnosis with SNNDeep: A Custom Spiking Neural Network Using Diverse Learning Algorithms](https://arxiv.org/abs/2508.20125)
*Zofia Rudnicka,Janusz Szczepanski,Agnieszka Pregowska*

Main category: cs.NE

TL;DR: 本文提出了一种名为SNNDeep的尖峰神经网络，用于基于CT特征进行肝脏健康状况的二元分类，且表现优于现有框架实现。


<details>
  <summary>Details</summary>
Motivation: 探索尖峰神经网络在高需求的生物医学成像领域的应用潜力，尤其是在资源受限或时间敏感的诊断场景中。

Method: 开发并优化SNNDeep模型，使用MSD数据集进行训练和验证，并比较三种学习算法及两种框架实现。使用Optuna进行超参数优化。

Result: SNNDeep模型验证精度达到98.35%，表现出比基于框架的实现更高的适应性和更低的训练开销。

Conclusion: 证明了底层可调尖峰神经网络可以在医学成像中超越标准框架，展现了神经灵感AI在精准医疗中的新可能。

Abstract: Purpose: Spiking neural networks (SNNs) have recently gained attention as
energy-efficient, biologically plausible alternatives to conventional deep
learning models. Their application in high-stakes biomedical imaging remains
almost entirely unexplored. Methods: This study introduces SNNDeep, the first
tailored SNN specifically optimized for binary classification of liver health
status from computed tomography (CT) features. To ensure clinical relevance and
broad generalizability, the model was developed and evaluated using the
Task03\Liver dataset from the Medical Segmentation Decathlon (MSD), a
standardized benchmark widely used for assessing performance across diverse
medical imaging tasks. We benchmark three fundamentally different learning
algorithms, namely Surrogate Gradient Learning, the Tempotron rule, and
Bio-Inspired Active Learning across three architectural variants: a fully
customized low-level model built from scratch, and two implementations using
leading SNN frameworks, i.e., snnTorch and SpikingJelly. Hyperparameter
optimization was performed using Optuna. Results: Our results demonstrate that
the custom-built SNNDeep consistently outperforms framework-based
implementations, achieving a maximum validation accuracy of 98.35%, superior
adaptability across learning rules, and significantly reduced training
overhead. Conclusion:This study provides the first empirical evidence that
low-level, highly tunable SNNs can surpass standard frameworks in medical
imaging, especially in data-limited, temporally constrained diagnostic
settings, thereby opening a new pathway for neuro-inspired AI in precision
medicine.

</details>


### [236] [Spatio-Temporal Pruning for Compressed Spiking Large Language Models](https://arxiv.org/abs/2508.20122)
*Yi Jiang,Malyaban Bal,Brian Matejek,Susmit Jha,Adam Cobb,Abhronil Sengupta*

Main category: cs.NE

TL;DR: 该论文探讨了通过设计压缩的脉冲大语言模型(Spiking LLMs)来攻克能量受限环境中部署LLMs的挑战，提出了一种新的时空剪枝框架，以优化计算效率并保持高性能。实验验证表明这种方法在计算操作和推理延迟方面都表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在能量受限环境中部署时出现的模型体积大、推理延迟高的问题，同时利用脉冲神经网络的低能耗优势。

Method: 提出了一种新的时空剪枝框架：提高空间剪枝以减少神经元和注意力头的活跃数量，加强时间剪枝以动态调整不同层所需的时间步数，并结合极端量化和知识蒸馏技术。

Result: 通过在大规模GLUE基准测试上的实验评估，验证了提出的SpikingBERT框架在计算操作和推理延迟方面的效率，证明其适合低能耗自然语言处理应用。

Conclusion: 该研究首次在Spiking LLMs领域联合探索时空剪枝、极端量化和知识蒸馏策略，为实现实时低能耗自然语言处理应用提供了可能性，尤其对于边缘设备和能量受限环境而言更具实践意义。

Abstract: Large Language Models (LLMs) present significant challenges for deployment in
energy-constrained environments due to their large model sizes and high
inference latency. Spiking Neural Networks (SNNs), inspired by the sparse
event-driven neural processing and energy-efficient information transmission in
the brain, offer a promising alternative for achieving low-power computing.
Integrating the event-driven efficiency of spiking neurons with the advanced
capabilities of LLMs represents a promising direction for power-efficient LLMs.
This work specifically delves into the design of compressed spiking LLMs. Here,
we revisit spatial and temporal pruning from the perspective of SNNs and
propose a novel spatio-temporal pruning framework for Spiking LLMs to optimize
computational efficiency while preserving high performance. Our spatial pruning
technique reduces the number of active neurons and attention heads, effectively
lowering the computational complexity of the model. Meanwhile, temporal pruning
minimizes inference latency by dynamically adjusting the number of timesteps
required for different layers. By combining these approaches with other
compression techniques, we present the first work in the domain of Spiking LLMs
to jointly explore spatial pruning, temporal pruning, extreme quantization and
knowledge distillation strategies. Extensive experimental evaluation of our
proposed framework for SpikingBERT on the large-scale GLUE benchmark
demonstrates the efficacy of our approach in terms of computational operations
and inference latency. Our approach offers a compelling solution for real-time,
low-power natural language processing applications, making Spiking LLMs more
practical for deployment on edge devices and in power-constrained settings.

</details>
