<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 162]
- [cs.CL](#cs.CL) [Total: 63]
- [cs.AI](#cs.AI) [Total: 44]
- [cs.LG](#cs.LG) [Total: 162]
- [cs.NE](#cs.NE) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [View Invariant Learning for Vision-Language Navigation in Continuous Environments](https://arxiv.org/abs/2507.08831)
*Josh Qixuan Sun,Xiaoying Xing,Huaiyuan Weng,Chul Min Yeum,Mark Crowley*

Main category: cs.CV

TL;DR: 该论文提出了一种名为VIL（View Invariant Learning）的方法，提升了视觉语言导航（VLNCE）任务中代理对视角变化的鲁棒性，同时在新的V2-VLNCE场景下取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言导航方法对摄像机视角变化敏感，导致在实际环境中的应用受限。为解决这一问题，该研究提出一种能够增强导航策略对视角变化鲁棒性的泛化场景。

Method: 提出了VIL，一种基于对比学习的后训练策略，用于学习稀疏且与视角无关的特征。同时引入了教师-学生框架，让视角相关的教师模型向视角无关的学生模型传递知识。此外，采用端到端训练优化各模块，避免了单独训练模块带来的额外成本。

Result: 在R2R-CE和RxR-CE数据集上的成功率相比最先进方法提升了8-15%。在RxR-CE等更具挑战的数据集上，各项指标也达到了目前最优效果，验证了VIL的有效性。

Conclusion: VIL方法既提升了导航策略在变视角下的鲁棒性，也不影响其在标准视角设置下的性能，具有通用性和可移植性，可作为插件式后训练方法应用于现有导航模型。

Abstract: Vision-Language Navigation in Continuous Environments (VLNCE), where an agent
follows instructions and moves freely to reach a destination, is a key research
problem in embodied AI. However, most navigation policies are sensitive to
viewpoint changes, i.e., variations in camera height and viewing angle that
alter the agent's observation. In this paper, we introduce a generalized
scenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View
Invariant Learning), a view-invariant post-training strategy that enhances the
robustness of existing navigation policies to changes in camera viewpoint. VIL
employs a contrastive learning framework to learn sparse and view-invariant
features. Additionally, we introduce a teacher-student framework for the
Waypoint Predictor Module, a core component of most VLNCE baselines, where a
view-dependent teacher model distills knowledge into a view-invariant student
model. We employ an end-to-end training paradigm to jointly optimize these
components, thus eliminating the cost for individual module training. Empirical
results show that our method outperforms state-of-the-art approaches on
V2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets
R2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE
setting and find that, despite being trained for varied viewpoints, it often
still improves performance. On the more challenging RxR-CE dataset, our method
also achieved state-of-the-art performance across all metrics when compared to
other map-free methods. This suggests that adding VIL does not diminish the
standard viewpoint performance and can serve as a plug-and-play post-training
method.

</details>


### [2] [Detecting Deepfake Talking Heads from Facial Biometric Anomalies](https://arxiv.org/abs/2507.08917)
*Justin D. Norman,Hany Farid*

Main category: cs.CV

TL;DR: 本文提出了一种新的机器学习技术，利用面部生物特征的异常模式检测深伪视频。


<details>
  <summary>Details</summary>
Motivation: 当前深伪技术易被用于欺诈和政治虚假信息，需要更可靠的检测技术。

Method: 通过利用面部生物特征的非自然模式，设计了一种新的机器学习方法来检测深伪视频。

Result: 该技术在大规模深伪数据集及不同生成器情况下均表现出了良好的检测可靠性和泛化能力。

Conclusion: 所提出的方法有效检测深伪视频，可为防范深伪相关欺诈与虚假信息提供技术支持。

Abstract: The combination of highly realistic voice cloning, along with visually
compelling avatar, face-swap, or lip-sync deepfake video generation, makes it
relatively easy to create a video of anyone saying anything. Today, such
deepfake impersonations are often used to power frauds, scams, and political
disinformation. We propose a novel forensic machine learning technique for the
detection of deepfake video impersonations that leverages unnatural patterns in
facial biometrics. We evaluate this technique across a large dataset of
deepfake techniques and impersonations, as well as assess its reliability to
video laundering and its generalization to previously unseen video deepfake
generators.

</details>


### [3] [PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection](https://arxiv.org/abs/2507.08979)
*Mahdiyar Molahasani,Azadeh Motamedi,Michael Greenspan,Il-Min Kim,Ali Etemad*

Main category: cs.CV

TL;DR: 提出了一种名为PRISM的新方法，用于解决视觉语言模型中的隐性偏差问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型容易继承和放大训练数据中的偏差，导致预测结果偏向性强。需要一种无需额外数据且不依赖具体偏差类别的解决方案。

Method: PRISM采用两阶段方法。第一步，通过提示生成包含虚假相关性的场景描述；第二步，引入对比风格的去偏差损失，将嵌入投影到减少偏差的潜在空间，同时保持图像和文本嵌入的对齐。

Result: PRISM在Waterbirds和CelebA数据集上的性能优于现有的去偏差方法。

Conclusion: PRISM提供了一种无需额外数据或预定义偏差类别的有效去偏差方法，并公开了其代码供研究者使用。

Abstract: We introduce Projection-based Reduction of Implicit Spurious bias in
vision-language Models (PRISM), a new data-free and task-agnostic solution for
bias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in
their training data, leading to skewed predictions. PRISM is designed to debias
VLMs without relying on predefined bias categories or additional external data.
It operates in two stages: first, an LLM is prompted with simple class prompts
to generate scene descriptions that contain spurious correlations. Next, PRISM
uses our novel contrastive-style debiasing loss to learn a projection that maps
the embeddings onto a latent space that minimizes spurious correlations while
preserving the alignment between image and text embeddings.Extensive
experiments demonstrate that PRISM outperforms current debiasing methods on the
commonly used Waterbirds and CelebA datasets We make our code public at:
https://github.com/MahdiyarMM/PRISM.

</details>


### [4] [Video Inference for Human Mesh Recovery with Vision Transformer](https://arxiv.org/abs/2507.08981)
*Hanbyel Cho,Jaesung Ahn,Yooshin Cho,Junmo Kim*

Main category: cs.CV

TL;DR: 本文提出了一种结合时序与运动学信息的新方法HMR-ViT，用于从视频中恢复人体网格模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅利用时序或运动学信息来提高人体网格恢复的准确性，而没有结合两者的解决方案。

Method: 提出HMR-ViT方法，构建时序运动学特征图像并利用视觉Transformer进行编码，最后通过回归网络推断SMPL姿态与形状参数。

Result: 在3DPW和Human3.6M数据集上表现出竞争力的性能。

Conclusion: 引入时序与运动学信息的新方法有效提升了人体网格恢复的精度。

Abstract: Human Mesh Recovery (HMR) from an image is a challenging problem because of
the inherent ambiguity of the task. Existing HMR methods utilized either
temporal information or kinematic relationships to achieve higher accuracy, but
there is no method using both. Hence, we propose "Video Inference for Human
Mesh Recovery with Vision Transformer (HMR-ViT)" that can take into account
both temporal and kinematic information. In HMR-ViT, a Temporal-kinematic
Feature Image is constructed using feature vectors obtained from video frames
by an image encoder. When generating the feature image, we use a Channel
Rearranging Matrix (CRM) so that similar kinematic features could be located
spatially close together. The feature image is then further encoded using
Vision Transformer, and the SMPL pose and shape parameters are finally inferred
using a regression network. Extensive evaluation on the 3DPW and Human3.6M
datasets indicates that our method achieves a competitive performance in HMR.

</details>


### [5] [From images to properties: a NeRF-driven framework for granular material parameter inversion](https://arxiv.org/abs/2507.09005)
*Cheng-Hsi Hsiao,Krishna Kumar*

Main category: cs.CV

TL;DR: 本研究结合NeRF和MPM模拟，通过视觉观测来推测颗粒材料的摩擦角。


<details>
  <summary>Details</summary>
Motivation: 旨在解决直接测量颗粒材料属性困难的问题，寻求通过视觉观察来间接表征颗粒材料性能的方法。

Method: 以模拟犁与沙子的交互生成实验数据，利用NeRF进行初始3D几何的重建，并通过与MPM模拟的图像比较结合贝叶斯优化估算摩擦角。

Result: 实验结果表明摩擦角的估算误差低于2度。

Conclusion: 该方法基于视觉观察实现精确颗粒材料特性估算，为实际无法测量的应用场景提供了有效解决方案。

Abstract: We introduce a novel framework that integrates Neural Radiance Fields (NeRF)
with Material Point Method (MPM) simulation to infer granular material
properties from visual observations. Our approach begins by generating
synthetic experimental data, simulating an plow interacting with sand. The
experiment is rendered into realistic images as the photographic observations.
These observations include multi-view images of the experiment's initial state
and time-sequenced images from two fixed cameras. Using NeRF, we reconstruct
the 3D geometry from the initial multi-view images, leveraging its capability
to synthesize novel viewpoints and capture intricate surface details. The
reconstructed geometry is then used to initialize material point positions for
the MPM simulation, where the friction angle remains unknown. We render images
of the simulation under the same camera setup and compare them to the observed
images. By employing Bayesian optimization, we minimize the image loss to
estimate the best-fitting friction angle. Our results demonstrate that friction
angle can be estimated with an error within 2 degrees, highlighting the
effectiveness of inverse analysis through purely visual observations. This
approach offers a promising solution for characterizing granular materials in
real-world scenarios where direct measurement is impractical or impossible.

</details>


### [6] [VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels](https://arxiv.org/abs/2507.09008)
*Xiwei Xuan,Xiaoqi Wang,Wenbin He,Jorge Piazentin Ono,Liang Gou,Kwan-Liu Ma,Liu Ren*

Main category: cs.CV

TL;DR: 本文提出了一个名为VISTA的视觉分析框架，用于提升多模态模型生成标签的数据质量，特别是在开放词汇图像分割中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有的基础模型生成数据标签方法对数据质量关注较少，而验证大规模数据的质量在实践中具有较大挑战。

Method: VISTA框架整合多阶段数据验证策略和人类专家的能力，帮助发现和纠正基础模型生成标签中的隐藏问题。

Result: 通过两个基准数据集的详细用例及专家评审，验证了VISTA从定量和定性两个视角的有效性。

Conclusion: VISTA有效提升了多模态模型生成的标签质量，从而有助于改进开放词汇图像分割任务的模型性能。

Abstract: The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA)
have facilitated the auto-labeling of large-scale datasets, enhancing model
performance in challenging downstream tasks such as open-vocabulary object
detection and segmentation. However, the quality of FM-generated labels is less
studied as existing approaches focus more on data quantity over quality. This
is because validating large volumes of data without ground truth presents a
considerable challenge in practice. Existing methods typically rely on limited
metrics to identify problematic data, lacking a comprehensive perspective, or
apply human validation to only a small data fraction, failing to address the
full spectrum of potential issues. To overcome these challenges, we introduce
VISTA, a visual analytics framework that improves data quality to enhance the
performance of multi-modal models. Targeting the complex and demanding domain
of open-vocabulary image segmentation, VISTA integrates multi-phased data
validation strategies with human expertise, enabling humans to identify,
understand, and correct hidden issues within FM-generated labels. Through
detailed use cases on two benchmark datasets and expert reviews, we demonstrate
VISTA's effectiveness from both quantitative and qualitative perspectives.

</details>


### [7] [BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis](https://arxiv.org/abs/2507.09036)
*Florian Kofler,Marcel Rosier,Mehdi Astaraki,Hendrik Möller,Ilhem Isra Mekki,Josef A. Buchner,Anton Schmick,Arianna Pfiffer,Eva Oswald,Lucas Zimmer,Ezequiel de la Rosa,Sarthak Pati,Julian Canisius,Arianna Piffer,Ujjwal Baid,Mahyar Valizadeh,Akis Linardos,Jan C. Peeken,Surprosanna Shit,Felix Steinbauer,Daniel Rueckert,Rolf Heckemann,Spyridon Bakas,Jan Kirschke,Constantin von See,Ivan Ezhov,Marie Piraud,Benedikt Wiestler,Bjoern Menze*

Main category: cs.CV

TL;DR: BrainLesion Suite是一款基于Python的工具包，用于构建脑部病变图像分析流程，支持多种功能如预处理、病变修复及分割等，可用于多个医学图像分析领域。


<details>
  <summary>Details</summary>
Motivation: 开发一个模块化且易于使用的工具包，以减少复杂脑病变图像分析的开发难度，支持科学和临床应用。

Method: 设计了一种灵活的预处理模块，可进行多模态图像的配准、去头及隐私处理，并结合BraTS算法实现模态合成和病变修复，同时支持分割模型性能评估功能。

Result: BrainLesion Suite成功实现了可扩展的模块化框架，支持脑部疾病如胶质瘤及多发性硬化的病变图像分析，并可拓展至其它医学领域。

Conclusion: 该工具包在多领域医学图像分析中具有广泛适用性，并已通过GitHub提供相关模块及教程。

Abstract: BrainLesion Suite is a versatile toolkit for building modular brain lesion
image analysis pipelines in Python. Following Pythonic principles, BrainLesion
Suite is designed to provide a 'brainless' development experience, minimizing
cognitive effort and streamlining the creation of complex workflows for
clinical and scientific practice. At its core is an adaptable preprocessing
module that performs co-registration, atlas registration, and optional
skull-stripping and defacing on arbitrary multi-modal input images. BrainLesion
Suite leverages algorithms from the BraTS challenge to synthesize missing
modalities, inpaint lesions, and generate pathology-specific tumor
segmentations. BrainLesion Suite also enables quantifying segmentation model
performance, with tools such as panoptica to compute lesion-wise metrics.
Although BrainLesion Suite was originally developed for image analysis
pipelines of brain lesions such as glioma, metastasis, and multiple sclerosis,
it can be adapted for other biomedical image analysis applications. The
individual BrainLesion Suite packages and tutorials are accessible on GitHub.

</details>


### [8] [Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?](https://arxiv.org/abs/2507.09052)
*Fang Chen,Alex Villa,Gongbo Liang,Xiaoyi Lu,Meng Tang*

Main category: cs.CV

TL;DR: 该论文解决了分类条件图像合成中的类别数据不平衡问题，通过对扩散模型引入对比损失函数改善尾类别图像的多样性。


<details>
  <summary>Details</summary>
Motivation: 现有分类条件扩散模型在长尾分布数据上表现较差，特别是尾类图像合成多样性不足。

Method: 提出两种对比损失函数：①利用无监督的InfoNCE损失通过负样本增加尾类图像之间的不相似性；②使用在大时间步长上的无条件生成与类别条件生成的MSE损失，对初步去噪过程变得对类别条件不敏感，从而实现知识共享。

Result: 提出的方法在CIFAR10/100-LT、PlacesLT、TinyImageNetLT和ImageNetLT等数据集上的表现优于标准DDPM及其他方法。

Conclusion: 通过对比学习框架改进分类条件扩散模型，不仅简单易实现，更显著提高了不平衡数据上的图像生成质量。

Abstract: Training data for class-conditional image synthesis often exhibit a
long-tailed distribution with limited images for tail classes. Such an
imbalance causes mode collapse and reduces the diversity of synthesized images
for tail classes. For class-conditional diffusion models trained on imbalanced
data, we aim to improve the diversity of tail class images without compromising
the fidelity and diversity of head class images. We achieve this by introducing
two deceptively simple but highly effective contrastive loss functions.
Firstly, we employ an unsupervised InfoNCE loss utilizing negative samples to
increase the distance/dissimilarity among synthetic images, particularly for
tail classes. To further enhance the diversity of tail classes, our second loss
is an MSE loss that contrasts class-conditional generation with unconditional
generation at large timesteps. This second loss makes the denoising process
insensitive to class conditions for the initial steps, which enriches tail
classes through knowledge sharing from head classes. Conditional-unconditional
alignment has been shown to enhance the performance of long-tailed GAN. We are
the first to adapt such alignment to diffusion models. We successfully
leveraged contrastive learning for class-imbalanced diffusion models. Our
contrastive learning framework is easy to implement and outperforms standard
DDPM and alternative methods for class-imbalanced diffusion models across
various datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and
ImageNetLT.

</details>


### [9] [Infinite Video Understanding](https://arxiv.org/abs/2507.09068)
*Dell Zhang,Xiangyu Chen,Jixiang Luo,Mengxi Jia,Changzhi Sun,Ruilong Ren,Jingren Liu,Hao Sun,Xuelong Li*

Main category: cs.CV

TL;DR: 该论文提出无限视频理解作为一个蓝海研究目标，旨在使模型能够持续处理、理解和推理任意长度的视频数据，并探讨了实现这一目标的关键挑战和研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和多模态扩展的快速发展，视频理解取得了显著进展。但处理超长时长视频和保持时间一致性等仍是重大挑战，因此作者提出“无限视频理解”以推动该领域的进一步创新。

Method: 论文并未提出具体方法，而是讨论了实现无限视频理解的核心挑战和关键研究方向，如流式架构、持久性记忆机制、分层和自适应表示、事件驱动推理以及新的评估范式等。

Result: 论文未提供具体实验结果，但通过总结当前的研究进展和瓶颈，为未来相关研究提供了方向性指导。

Conclusion: 提出“无限视频理解”这一研究愿景，为多媒体及更广泛的人工智能研究指明了方向，并激励在相关技术上的创新探索。

Abstract: The rapid advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have ushered in remarkable progress in video understanding.
However, a fundamental challenge persists: effectively processing and
comprehending video content that extends beyond minutes or hours. While recent
efforts like Video-XL-2 have demonstrated novel architectural solutions for
extreme efficiency, and advancements in positional encoding such as HoPE and
VideoRoPE++ aim to improve spatio-temporal understanding over extensive
contexts, current state-of-the-art models still encounter significant
computational and memory constraints when faced with the sheer volume of visual
tokens from lengthy sequences. Furthermore, maintaining temporal coherence,
tracking complex events, and preserving fine-grained details over extended
periods remain formidable hurdles, despite progress in agentic reasoning
systems like Deep Video Discovery. This position paper posits that a logical,
albeit ambitious, next frontier for multimedia research is Infinite Video
Understanding -- the capability for models to continuously process, understand,
and reason about video data of arbitrary, potentially never-ending duration. We
argue that framing Infinite Video Understanding as a blue-sky research
objective provides a vital north star for the multimedia, and the wider AI,
research communities, driving innovation in areas such as streaming
architectures, persistent memory mechanisms, hierarchical and adaptive
representations, event-centric reasoning, and novel evaluation paradigms.
Drawing inspiration from recent work on long/ultra-long video understanding and
several closely related fields, we outline the core challenges and key research
directions towards achieving this transformative capability.

</details>


### [10] [BlindSight: Harnessing Sparsity for Efficient VLMs](https://arxiv.org/abs/2507.09071)
*Tharun Adithya Srikrishnan,Deval Shah,Steven K. Reinhardt*

Main category: cs.CV

TL;DR: 该论文提出BlindSight方法，通过输入模板感知的稀疏注意力掩码优化视觉语言模型（VLM）的推理效率，在精度几乎不变的情况下减少了32%-41%的计算量。


<details>
  <summary>Details</summary>
Motivation: 想解决视觉数据扩展提示长度和注意力计算复杂度导致的推理效率瓶颈问题。

Method: 通过分析VLM的注意力模式，发现多数层的跨图像注意力较少，基于这一观察提出无需训练的稀疏注意力掩码优化方法BlindSight。利用数据集样本推导对每个注意力头的通用稀疏性分类。

Result: BlindSight方法在多个多图像理解基准测试中，实现了计算量平均减少32%-41%，精度变化为-2%到+2%。

Conclusion: BlindSight不需要重新训练，可以显著降低推理过程中的计算开销，同时保持模型的准确性，是一种有效的推理优化方法。

Abstract: Large vision-language models (VLMs) enable the joint processing of text and
images. However, the inclusion of vision data significantly expands the prompt
length. Along with the quadratic complexity of the attention computation, this
results in a longer prefill duration. An approach to mitigate this bottleneck
is to leverage the inherent sparsity in the attention computation. In our
analysis of attention patterns in VLMs, we observe that a substantial portion
of layers exhibit minimal cross-image attention, except through attention-sink
tokens per image. These sparse attention patterns fall into distinct
categories: sink-only, document mask and a hybrid document-sink mask. Based on
this, we propose BlindSight: a training-free approach to optimize VLM inference
using a input template-aware attention sparsity mask. We utilize samples from a
dataset to derive a prompt-agnostic sparsity categorization for every attention
head. We evaluate the proposed technique using VLMs such as Qwen2-VL,
Qwen2.5-VL and Gemma-3. BlindSight results in a 32%-41% reduction in FLOPs on
average with -2%-+2% accuracy compared to the original model in most evaluated
multi-image understanding benchmarks.

</details>


### [11] [From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion](https://arxiv.org/abs/2507.09081)
*Zhenyu Yu,Mohd Yamani Idna Idris,Hua Wang,Pei Wang,Junyi Chen,Kun Wang*

Main category: cs.CV

TL;DR: 本文回顾并比较了遥感反演技术从物理模型到数据驱动及基础模型的演变，讨论其假设、场景及局限，并展望下一代基础模型的发展方向。


<details>
  <summary>Details</summary>
Motivation: 旨在概述遥感反演技术的演化过程，分析当前技术局限并提出未来基础模型的发展方向。

Method: 系统性综述与比较传统物理模型（如PROSPECT, SCOPE, DART）、机器学习方法（如深度学习、多模态融合）及基础模型（如SatMAE, GFM, mmEarth）在建模假设、应用场景及限制作出的不同贡献。

Result: 解析了遥感反演领域的技术进展，强调了自监督预训练、多模态整合及跨任务适配在基础模型中的最新发展，同时指出物理解释性、领域泛化、有限监督及不确定性量化等持续难题。

Conclusion: 下一代遥感反演基础模型应具备统一建模能力、跨领域泛化性及物理解释性，以进一步推动该领域发展。

Abstract: Quantitative remote sensing inversion aims to estimate continuous surface
variables-such as biomass, vegetation indices, and evapotranspiration-from
satellite observations, supporting applications in ecosystem monitoring, carbon
accounting, and land management. With the evolution of remote sensing systems
and artificial intelligence, traditional physics-based paradigms are giving way
to data-driven and foundation model (FM)-based approaches. This paper
systematically reviews the methodological evolution of inversion techniques,
from physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods
(e.g., deep learning, multimodal fusion), and further to foundation models
(e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application
scenarios, and limitations of each paradigm, with emphasis on recent FM
advances in self-supervised pretraining, multi-modal integration, and
cross-task adaptation. We also highlight persistent challenges in physical
interpretability, domain generalization, limited supervision, and uncertainty
quantification. Finally, we envision the development of next-generation
foundation models for remote sensing inversion, emphasizing unified modeling
capacity, cross-domain generalization, and physical interpretability.

</details>


### [12] [Taming generative video models for zero-shot optical flow extraction](https://arxiv.org/abs/2507.09082)
*Seungwoo Kim,Khai Loong Aw,Klemen Kotar,Cristobal Eyzaguirre,Wanhee Lee,Yunong Liu,Jared Watrous,Stefan Stojanov,Juan Carlos Niebles,Jiajun Wu,Daniel L. K. Yamins*

Main category: cs.CV

TL;DR: 本文探讨了冻结的自监督视频模型能否在不进行微调的情况下，通过提示而输出光流，这是通过扩展目前的生成视频模型实现的。


<details>
  <summary>Details</summary>
Motivation: 受到大规模通用模型成功的启发，研究如何利用已训练的生成视频模型来输出光流，避免对流罕见的标签依赖和合成数据集的现实差距。

Method: 基于Counterfactual World Model（CWM）范式，提出一种新的KL-tracing方法，在测试时通过局部扰动注入和KL散度计算来获得光流特征。

Result: 在无需微调的情况下，本文的方法在真实和合成数据集上均超越了当前的性能，分别在TAP-Vid DAVIS和TAP-Vid Kubric数据集上取得了16.6%和4.7%的提升。

Conclusion: 反事实提示生成视频模型是一种可扩展且高效的高质量光流提取方法，可替代监督学习或基于光度损失的方法。

Abstract: Extracting optical flow from videos remains a core computer vision problem.
Motivated by the success of large general-purpose models, we ask whether frozen
self-supervised video models trained only for future frame prediction can be
prompted, without fine-tuning, to output flow. Prior work reading out depth or
illumination from video generators required fine-tuning, which is impractical
for flow where labels are scarce and synthetic datasets suffer from a
sim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm,
which can obtain point-wise correspondences by injecting a small tracer
perturbation into a next-frame predictor and tracking its propagation, we
extend this idea to generative video models. We explore several popular
architectures and find that successful zero-shot flow extraction in this manner
is aided by three model properties: (1) distributional prediction of future
frames (avoiding blurry or noisy outputs); (2) factorized latents that treat
each spatio-temporal patch independently; and (3) random-access decoding that
can condition on any subset of future pixels. These properties are uniquely
present in the recent Local Random Access Sequence (LRAS) architecture.
Building on LRAS, we propose KL-tracing: a novel test-time procedure that
injects a localized perturbation into the first frame, rolls out the model one
step, and computes the Kullback-Leibler divergence between perturbed and
unperturbed predictive distributions. Without any flow-specific fine-tuning,
our method outperforms state-of-the-art models on real-world TAP-Vid DAVIS
dataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid
Kubric (4.7% relative improvement). Our results indicate that counterfactual
prompting of controllable generative video models is a scalable and effective
alternative to supervised or photometric-loss approaches for high-quality flow.

</details>


### [13] [MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks](https://arxiv.org/abs/2507.09092)
*Ram S Iyer,Narayan S Iyer,Rugmini Ammal P*

Main category: cs.CV

TL;DR: 本文提出了一种基于激活映射的新型后处理视觉解释方法 (MI CAM)，通过互信息权重生成因果解释，并与现有算法相媲美甚至超越。


<details>
  <summary>Details</summary>
Motivation: 随着机器视觉在重要领域（如医疗和自动化电厂）的广泛应用，人们希望解释卷积神经网络的内在机制及其输出推理原因。

Method: 提出MI CAM方法，通过互信息权重和激活图的线性组合生成显著性可视化并进行因果解释，并用反事实分析来验证。

Result: 该方法在定性和定量指标上表现优异，与现有最新技术方法持平，有时表现更佳。

Conclusion: MI CAM能够提供视觉性能和模型推断过程的公平解释，其性能在多项指标上优于现有方法。

Abstract: With the intervention of machine vision in our crucial day to day necessities
including healthcare and automated power plants, attention has been drawn to
the internal mechanisms of convolutional neural networks, and the reason why
the network provides specific inferences. This paper proposes a novel post-hoc
visual explanation method called MI CAM based on activation mapping. Differing
from previous class activation mapping based approaches, MI CAM produces
saliency visualizations by weighing each feature map through its mutual
information with the input image and the final result is generated by a linear
combination of weights and activation maps. It also adheres to producing causal
interpretations as validated with the help of counterfactual analysis. We aim
to exhibit the visual performance and unbiased justifications for the model
inferencing procedure achieved by MI CAM. Our approach works at par with all
state-of-the-art methods but particularly outperforms some in terms of
qualitative and quantitative measures. The implementation of proposed method
can be found on https://anonymous.4open.science/r/MI-CAM-4D27

</details>


### [14] [RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze](https://arxiv.org/abs/2507.09097)
*Yunsoo Kim,Jinge Wu,Honghan Wu*

Main category: cs.CV

TL;DR: 本文提出RadEyeVideo方法，通过将放射科医生的眼动数据集成为视频序列，以捕捉其凝视的时空动态。在胸部X光报告生成和疾病诊断任务中，此方法使模型性能提高了24.6%和平均15.2%。


<details>
  <summary>Details</summary>
Motivation: 现有方法在涉及放射科医生的视线数据时，常忽略了眼动的顺序性。本研究旨在通过将放射科医生的眼动信息以视频形式融入模型，充分利用其时空特性。

Method: 提出RadEyeVideo方法，将放射科医生的眼动数据作为视频序列输入至具有视频处理能力的大型视觉-语言模型，分析其时空视线动态，在报告生成和疾病诊断任务中验证其效果。

Result: 在胸部X光报告生成任务中模型性能提升24.6%，在报告生成和疾病诊断任务中平均提升15.2%。同时，提升后的一般领域模型超越了任务专用的医学模型。

Conclusion: 有效整合领域专家知识（如眼动信息）到通用领域的大型视觉-语言模型中，可以显著增强其在临床任务中的表现。RadEyeVideo为以人为中心的大规模医学影像分析迈出了重要一步。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated promising performance
in chest X-ray (CXR) analysis. To enhance human-computer interaction, several
studies have incorporated radiologists' eye gaze, typically through heatmaps or
textual prompts. However, these methods often overlook the sequential order of
eye movements, which could provide valuable insights by highlighting both the
areas of interest and the order in which they are examined. In this work, we
propose a novel approach called RadEyeVideo that integrates radiologists'
eye-fixation data as a video sequence, capturing both the temporal and spatial
dynamics of their gaze. We evaluate this method in CXR report generation and
disease diagnosis using three general-domain, open-source LVLMs with video
input capabilities. When prompted with eye-gaze videos, model performance
improves by up to 24.6% in the report generation task and on average 15.2% for
both tasks using scaled evaluation metrics. Notably, RadEyeVideo enhanced an
open-domain LVLM model, LLaVA-OneVision, to surpass task-specific medical LVLMs
such as MAIRA-2 and CheXagent, trained on large Chest X-ray data. This work
highlights that domain expert's knowledge (eye-gaze information in this case),
when effectively integrated with LVLMs, can significantly enhance
general-domain models' capabilities in clinical tasks. RadEyeVideo is a step
toward a scalable human-centered approach of utilizing LVLMs in medical image
analytics.

</details>


### [15] [Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning](https://arxiv.org/abs/2507.09102)
*Yiyang Chen,Shanshan Zhao,Lunhao Duan,Changxing Ding,Dacheng Tao*

Main category: cs.CV

TL;DR: 本文提出了PointSD框架，通过利用Stable Diffusion（SD）模型将其应用于3D自监督学习，以提升点云表征能力。


<details>
  <summary>Details</summary>
Motivation: 传统3D扩散模型在小规模数据集上的限制使得其性能受阻，本文旨在利用Stable Diffusion在大规模数据集训练中的强大能力来克服这些限制。

Method: 通过替换SD模型的文本编码器为3D编码器训练点到图像的扩散模型，使点云指导噪声图像的去噪，并通过特征对齐训练3D模型主干，进行语义学习。

Result: 实验表明，PointSD显著增强了点云自监督学习，代码已开源。

Conclusion: 通过引入Stable Diffusion模型，本文有效提升了3D自监督学习的性能，展示了其在点云任务中的潜力。

Abstract: Diffusion-based models, widely used in text-to-image generation, have proven
effective in 2D representation learning. Recently, this framework has been
extended to 3D self-supervised learning by constructing a conditional point
generator for enhancing 3D representations. However, its performance remains
constrained by the 3D diffusion model, which is trained on the available 3D
datasets with limited size. We hypothesize that the robust capabilities of
text-to-image diffusion models, particularly Stable Diffusion (SD), which is
trained on large-scale datasets, can help overcome these limitations. To
investigate this hypothesis, we propose PointSD, a framework that leverages the
SD model for 3D self-supervised learning. By replacing the SD model's text
encoder with a 3D encoder, we train a point-to-image diffusion model that
allows point clouds to guide the denoising of rendered noisy images. With the
trained point-to-image diffusion model, we use noise-free images as the input
and point clouds as the condition to extract SD features. Next, we train a 3D
backbone by aligning its features with these SD features, thereby facilitating
direct semantic learning. Comprehensive experiments on downstream point cloud
tasks and ablation studies demonstrate that the SD model can enhance point
cloud self-supervised learning. Code is publicly available at
https://github.com/wdttt/PointSD.

</details>


### [16] [Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production](https://arxiv.org/abs/2507.09105)
*Maoxiao Ye,Xinfeng Ye,Mano Manoharan*

Main category: cs.CV

TL;DR: 本文首次将混合自回归和扩散模型应用于手语生成任务，提出创新模块增强生成效率和质量。


<details>
  <summary>Details</summary>
Motivation: 传统自回归SLP模型训练时易于模型崩塌且推理阶段误差累积问题未能解决，扩散模型虽能生成高质量序列但不适用于实时任务，亟需结合二者优点的创新方法。

Method: 提出混合自回归和扩散模型的SLP方法，设计包含多尺度姿态表示和融合模块，并加入基于关节置信度的因果注意机制以改善生成过程的准确性与鲁棒性。

Result: 在PHOENIX14T和How2Sign数据集上的实验表明，新方法在生成质量和实时性方面均有所提升。

Conclusion: 混合模型成功结合了自回归和扩散模型的优点，为SLP领域提供了更高效和高质量的实时生成方法。

Abstract: Earlier Sign Language Production (SLP) models typically relied on
autoregressive methods that generate output tokens one by one, which inherently
provide temporal alignment. Although techniques like Teacher Forcing can
prevent model collapse during training, they still cannot solve the problem of
error accumulation during inference, since ground truth is unavailable at that
stage. In contrast, more recent approaches based on diffusion models leverage
step-by-step denoising to enable high-quality generation. However, the
iterative nature of these models and the requirement to denoise entire
sequences limit their applicability in real-time tasks like SLP. To address it,
we apply a hybrid approach combining autoregressive and diffusion models to SLP
for the first time, leveraging the strengths of both models in sequential
dependency modeling and output refinement. To capture fine-grained body
movements, we design a Multi-Scale Pose Representation module that separately
extracts detailed features from distinct articulators and integrates them via a
Multi-Scale Fusion module. Furthermore, we introduce a Confidence-Aware Causal
Attention mechanism that utilizes joint-level confidence scores to dynamically
guide the pose generation process, improving accuracy and robustness. Extensive
experiments on the PHOENIX14T and How2Sign datasets demonstrate the
effectiveness of our method in both generation quality and real-time streaming
efficiency.

</details>


### [17] [RoHOI: Robustness Benchmark for Human-Object Interaction Detection](https://arxiv.org/abs/2507.09111)
*Di Wen,Kunyu Peng,Kailun Yang,Yufan Chen,Ruiping Liu,Junwei Zheng,Alina Roitberg,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 本文旨在解决人机交互检测模型在真实场景中的鲁棒性问题，引入首个基准测试RoHOI，并提出提高模型鲁棒性的SAMPL策略。


<details>
  <summary>Details</summary>
Motivation: 当前人机交互检测模型在现实环境下表现不佳，面临环境变化、遮挡和噪声干扰等挑战，亟需改进模型鲁棒性。

Method: 提出基于语义感知的掩码渐进学习（SAMPL）策略，让模型动态调整优化方向，结合整体和局部线索学习鲁棒特征。

Result: 实验表明，该方法在鲁棒性上优于现有方法，为人机交互检测设立了新的标准。

Conclusion: 通过RoHOI基准和SAMPL策略，显著提升了人机交互检测的鲁棒性，并将公开基准、数据集和代码以推动领域发展。

Abstract: Human-Object Interaction (HOI) detection is crucial for robot-human
assistance, enabling context-aware support. However, models trained on clean
datasets degrade in real-world conditions due to unforeseen corruptions,
leading to inaccurate prediction. To address this, we introduce the first
robustness benchmark for HOI detection, evaluating model resilience under
diverse challenges. Despite advances, current models struggle with
environmental variability, occlusion, and noise. Our benchmark, RoHOI, includes
20 corruption types based on HICO-DET and V-COCO datasets and a new
robustness-focused metric. We systematically analyze existing models in the
related field, revealing significant performance drops under corruptions. To
improve robustness, we propose a Semantic-Aware Masking-based Progressive
Learning (SAMPL) strategy to guide the model to be optimized based on holistic
and partial cues, dynamically adjusting the model's optimization to enhance
robust feature learning. Extensive experiments show our approach outperforms
state-of-the-art methods, setting a new standard for robust HOI detection.
Benchmarks, datasets, and code will be made publicly available at
https://github.com/Kratos-Wen/RoHOI.

</details>


### [18] [Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning](https://arxiv.org/abs/2507.09118)
*Linlan Huang,Xusheng Cao,Haori Lu,Yifan Meng,Fei Yang,Xialei Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新方法MG-CLIP，以解决CLIP模型在增量学习中的遗忘问题和适应新数据的能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多忽视了CLIP中的模态间差异问题，而这种差异是CLIP通用性和适应性的关键。

Method: 通过分析视觉-语言预训练模型在微调过程中的模态间差异变化，提出了模态差距保留和补偿的方法，以改进CLIP在增量学习中的表现。

Result: MG-CLIP方法在多个基准测试中表现优于现有方法，无需额外的数据重放。

Conclusion: MG-CLIP有效地利用模态间差异，可减少遗忘并提高对新数据的适应性，为持续学习提供了新的视角。

Abstract: Continual learning aims to enable models to learn sequentially from
continuously incoming data while retaining performance on previously learned
tasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting
strong capabilities across various downstream tasks, there has been growing
interest in leveraging CLIP for continual learning in such scenarios. Most
existing works overlook the inherent modality gap in CLIP, a key factor in its
generalization and adaptability. In this paper, we analyze the variations in
the modality gap during the fine-tuning of vision-language pre-trained models.
Our observations reveal that the modality gap effectively reflects the extent
to which pre-trained knowledge is preserved. Based on these insights, we
propose a simple yet effective method, MG-CLIP, that improves CLIP's
performance in class-incremental learning. Our approach leverages modality gap
preservation to mitigate forgetting and modality gap compensation to enhance
the capacity for new data, introducing a novel modality-gap-based perspective
for continual learning. Extensive experiments on multiple benchmarks
demonstrate that our method outperforms existing approaches without requiring
additional replay data. Our code is available at
https://github.com/linlany/MindtheGap.

</details>


### [19] [SnapMoGen: Human Motion Generation from Expressive Texts](https://arxiv.org/abs/2507.09122)
*Chuan Guo,Inwoo Hwang,Jian Wang,Bing Zhou*

Main category: cs.CV

TL;DR: 该论文提出了一个名为SnapMoGen的新型文本到动作生成系统，并且引入了高质量数据集和改进的生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到动作生成方法存在对短文本或通用文本生成效果有限的问题；缺乏细粒度控制和未见提示的泛化能力。

Method: 提出SnapMoGen数据集，包括20,000个动作片段和122,000个详细文本描述。改进生成模型为MoMask++，通过多尺度序列生成和单一生成器的训练来提升生成效果。结合LLM处理用户提示。

Result: SnapMoGen在HumanML3D和SnapMoGen基准上达到了最新的性能，并能处理复杂的用户提示。

Conclusion: 拟议的数据集和生成模型显著提高了文本到动作生成任务的效果，同时扩展了对细粒度控制和长时序生成的支持。

Abstract: Text-to-motion generation has experienced remarkable progress in recent
years. However, current approaches remain limited to synthesizing motion from
short or general text prompts, primarily due to dataset constraints. This
limitation undermines fine-grained controllability and generalization to unseen
prompts. In this paper, we introduce SnapMoGen, a new text-motion dataset
featuring high-quality motion capture data paired with accurate, expressive
textual annotations. The dataset comprises 20K motion clips totaling 44 hours,
accompanied by 122K detailed textual descriptions averaging 48 words per
description (vs. 12 words of HumanML3D). Importantly, these motion clips
preserve original temporal continuity as they were in long sequences,
facilitating research in long-term motion generation and blending. We also
improve upon previous generative masked modeling approaches. Our model,
MoMask++, transforms motion into multi-scale token sequences that better
exploit the token capacity, and learns to generate all tokens using a single
generative masked transformer. MoMask++ achieves state-of-the-art performance
on both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the
ability to process casual user prompts by employing an LLM to reformat inputs
to align with the expressivity and narration style of SnapMoGen. Project
webpage: https://snap-research.github.io/SnapMoGen/

</details>


### [20] [PoseLLM: Enhancing Language-Guided Human Pose Estimation with MLP Alignment](https://arxiv.org/abs/2507.09139)
*Dewen Zhang,Tahir Hussain,Wangpeng An,Hayaru Shouno*

Main category: cs.CV

TL;DR: 提出了PoseLLM，一种基于大语言模型的姿态估计框架，通过非线性MLP连接器提升视觉与文本特征融合效果，在COCO验证集中取得了77.8 AP的表现，比现有方法LocLLM高出0.4 AP。


<details>
  <summary>Details</summary>
Motivation: 传统基于关键点先验的姿态估计方法在面对新颖姿态或未见过的关键点时表现有限，而LocLLM利用语言描述实现了零样本泛化，但其线性投影器无法有效捕捉复杂的空间文本交互特性。

Method: 提出PoseLLM，通过非线性MLP连接器替代线性投影器，该连接器采用两层MLP与GELU激活，能实现分层的跨模态特征转换，从而增强视觉补丁与文本关键点描述的融合效果。

Result: PoseLLM在COCO验证集中获得77.8 AP，比LocLLM提升0.4 AP，同时在Human-Art和MPII上保持了强劲的零样本泛化能力。

Conclusion: 使用简单但功能强大的非线性连接器能够在不牺牲泛化性的前提下显著提升定位精度，推动了语言引导姿态估计的技术进步。

Abstract: Human pose estimation traditionally relies on architectures that encode
keypoint priors, limiting their generalization to novel poses or unseen
keypoints. Recent language-guided approaches like LocLLM reformulate keypoint
localization as a vision-language task, enabling zero-shot generalization
through textual descriptions. However, LocLLM's linear projector fails to
capture complex spatial-textual interactions critical for high-precision
localization. To address this, we propose PoseLLM, the first Large Language
Model (LLM)-based pose estimation framework that replaces the linear projector
with a nonlinear MLP vision-language connector. This lightweight two-layer MLP
with GELU activation enables hierarchical cross-modal feature transformation,
enhancing the fusion of visual patches and textual keypoint descriptions.
Trained exclusively on COCO data, PoseLLM achieves 77.8 AP on the COCO
validation set, outperforming LocLLM by +0.4 AP, while maintaining strong
zero-shot generalization on Human-Art and MPII. Our work demonstrates that a
simple yet powerful nonlinear connector significantly boosts localization
accuracy without sacrificing generalization, advancing the state-of-the-art in
language-guided pose estimation. Code is available at
https://github.com/Ody-trek/PoseLLM.

</details>


### [21] [$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting](https://arxiv.org/abs/2507.09144)
*Zhimin Liao,Ping Wei,Ruijie Zhang,Shuaijia Chen,Haoxuan Wang,Ziyang Ren*

Main category: cs.CV

TL;DR: 本文提出了$I^{2}$-World，一种高效的4D占用预测框架，通过场景内和场景间的双重编码器设计，实现了对复杂3D场景的高效压缩与准确预测。


<details>
  <summary>Details</summary>
Motivation: 现有的3D场景建模面临高效编码与动态变化表达的挑战，研究旨在改善自动驾驶系统中3D场景的预测与生成能力。

Method: 采用场景内多尺度残差量化与场景间时间依赖聚合的双重编码器；通过编码器和解码器结合，生成高效且一致的4D占用预测。

Result: 与现有方法相比，mIoU性能提升25.1%，IoU提升36.9%，同时具备显著的计算效率，仅需2.9GB内存，推断速度达37.0 FPS。

Conclusion: $I^{2}$-World框架显著提升了4D占用预测的性能与效率，解决了3D场景建模中的关键难题，为自动驾驶系统发展提供了新的思路。

Abstract: Forecasting the evolution of 3D scenes and generating unseen scenarios via
occupancy-based world models offers substantial potential for addressing corner
cases in autonomous driving systems. While tokenization has revolutionized
image and video generation, efficiently tokenizing complex 3D scenes remains a
critical challenge for 3D world models. To address this, we propose
$I^{2}$-World, an efficient framework for 4D occupancy forecasting. Our method
decouples scene tokenization into intra-scene and inter-scene tokenizers. The
intra-scene tokenizer employs a multi-scale residual quantization strategy to
hierarchically compress 3D scenes while preserving spatial details. The
inter-scene tokenizer residually aggregates temporal dependencies across
timesteps. This dual design preserves the compactness of 3D tokenizers while
retaining the dynamic expressiveness of 4D tokenizers. Unlike decoder-only
GPT-style autoregressive models, $I^{2}$-World adopts an encoder-decoder
architecture. The encoder aggregates spatial context from the current scene and
predicts a transformation matrix to enable high-level control over scene
generation. The decoder, conditioned on this matrix and historical tokens,
ensures temporal consistency during generation. Experiments demonstrate that
$I^{2}$-World achieves state-of-the-art performance, outperforming existing
methods by 25.1\% in mIoU and 36.9\% in IoU for 4D occupancy forecasting while
exhibiting exceptional computational efficiency: it requires merely 2.9 GB of
training memory and achieves real-time inference at 37.0 FPS. Our code is
available on https://github.com/lzzzzzm/II-World.

</details>


### [22] [Stable Score Distillation](https://arxiv.org/abs/2507.09168)
*Haiming Zhu,Yangyang Xu,Chenshu Xu,Tingrui Shen,Wenxi Liu,Yong Du,Jun Yu,Shengfeng He*

Main category: cs.CV

TL;DR: 本文提出了一种名为Stable Score Distillation (SSD)的简化框架，用于提高文本引导的2D和3D图像编辑任务的稳定性和控制力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的图像和3D编辑方法（如Delta Denoising Score）在稳定性、空间控制和编辑强度方面面临挑战，主要原因是依赖复杂的辅助结构，导致优化信号冲突且难以实现精确和局部的编辑。

Method: SSD通过引入一种简化的方法：将单一分类器锚定到源提示，结合无分类器引导（CFG）方程实现跨提示对齐，并引入常数项的空文本分支以稳定优化过程。此外，SSD还通过引入一个提示增强分支来提升编辑强度，特别是在样式变换任务中。

Result: 该方法在2D和3D编辑任务（包括NeRF和文本驱动的样式编辑）上达到了最新的技术水平，并展现出更快的收敛速度和更低的复杂性。

Conclusion: SSD方法提供了一种稳健、高效的文本引导编辑解决方案，同时确保了编辑过程中的内容结构完整性和周围区域的连贯性。

Abstract: Text-guided image and 3D editing have advanced with diffusion-based models,
yet methods like Delta Denoising Score often struggle with stability, spatial
control, and editing strength. These limitations stem from reliance on complex
auxiliary structures, which introduce conflicting optimization signals and
restrict precise, localized edits. We introduce Stable Score Distillation
(SSD), a streamlined framework that enhances stability and alignment in the
editing process by anchoring a single classifier to the source prompt.
Specifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves
cross-prompt alignment, and introduces a constant term null-text branch to
stabilize the optimization process. This approach preserves the original
content's structure and ensures that editing trajectories are closely aligned
with the source prompt, enabling smooth, prompt-specific modifications while
maintaining coherence in surrounding regions. Additionally, SSD incorporates a
prompt enhancement branch to boost editing strength, particularly for style
transformations. Our method achieves state-of-the-art results in 2D and 3D
editing tasks, including NeRF and text-driven style edits, with faster
convergence and reduced complexity, providing a robust and efficient solution
for text-guided editing.

</details>


### [23] [Learning and Transferring Better with Depth Information in Visual Reinforcement Learning](https://arxiv.org/abs/2507.09180)
*Zichun Xu,Yuntao Li,Zhaomin Wang,Lei Zhuang,Guocai Yang,Jingdong Zhao*

Main category: cs.CV

TL;DR: 提出了一种基于视觉Transformer的视觉骨干网络，用于融合RGB与深度信息，并引入对比无监督学习和灵活的课程学习方法来提高样本效率和领域迁移能力。


<details>
  <summary>Details</summary>
Motivation: 如何有效融合RGB和深度信息以增强模型在多样场景中的泛化能力，是该论文的核心研究动机。

Method: 模型采用分别对RGB和深度信息进行CNN处理后，利用视觉Transformer融合并提取特征，并通过对比无监督学习（结合掩码和非掩码Token）提升强化学习的样本效率，同时运用灵活课程学习进行领域随机化以实现Sim2Real传递。

Result: 该网络有效提升了多模态信息处理能力和强化学习过程中的样本效率，并具备较好的领域迁移性能。

Conclusion: 通过结合多模态信息处理与增强学习技术，所提出的方法能更有效地在不同场景实现鲁棒性能，并显著改善从模拟环境到现实场景的迁移能力。

Abstract: Depth information is robust to scene appearance variations and inherently
carries 3D spatial details. In this paper, a visual backbone based on the
vision transformer is proposed to fuse RGB and depth modalities for enhancing
generalization. Different modalities are first processed by separate CNN stems,
and the combined convolutional features are delivered to the scalable vision
transformer to obtain visual representations. Moreover, a contrastive
unsupervised learning scheme is designed with masked and unmasked tokens to
accelerate the sample efficiency during the reinforcement learning progress.
For sim2real transfer, a flexible curriculum learning schedule is developed to
deploy domain randomization over training processes.

</details>


### [24] [Revisiting Pool-based Prompt Learning for Few-shot Class-incremental Learning](https://arxiv.org/abs/2507.09183)
*Yongwei Jiang,Yixiong Zou,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: 该论文首次研究了当前提示池方法在小样本类增量学习（FSCIL）任务中的表现，并提出了一种新的方法LGSP-Prompt，在空间维度上进行提示学习，显著提升了多个FSCIL基准的表现。


<details>
  <summary>Details</summary>
Motivation: 传统提示池方法在增量学习中的成功尚未在FSCIL场景中得到验证，初步结果显示增量会导致模型性能下降，需要新的解决方案。

Method: 提出LGSP-Prompt，通过在空间维度生成提示，将局部空间特征与全局频域表示相结合，同时构建两个空间提示池以进行动态提示选择，有效保留基准知识并学习新任务。

Result: LGSP-Prompt在多个FSCIL基准上达到了最新的性能标准，相较于现有方法在基础知识保留和增量学习中表现出显著优势。

Conclusion: 通过将提示学习维度从token转向空间维度，LGSP-Prompt解决了数据稀少和增量任务中提示竞争导致过拟合的问题，为FSCIL提供了更高效的方法。

Abstract: Few-Shot Class-Incremental Learning (FSCIL) faces dual challenges of data
scarcity and incremental learning in real-world scenarios. While pool-based
prompting methods have demonstrated success in traditional incremental
learning, their effectiveness in FSCIL settings remains unexplored. This paper
presents the first study of current prompt pool methods in FSCIL tasks,
revealing an unanticipated performance degradation in incremental sessions.
Through comprehensive analysis, we identify that this phenomenon stems from
token-dimension saturation: with limited data, excessive prompts compete for
task-relevant information, leading to model overfitting. Based on this finding,
we propose LGSP-Prompt (Local-Global Spatial Prompting), which innovatively
shifts pool-based prompt learning from the token dimension to the spatial
dimension. LGSP-Prompt generates spatial prompts by synergistically combining
local spatial features and global frequency-domain representations to highlight
key patterns in input images. We construct two spatial prompt pools enabling
dynamic prompt selection to maintain acquired knowledge while effectively
learning novel sessions. Extensive experiments demonstrate that our approach
achieves state-of-the-art performance across multiple FSCIL benchmarks, showing
significant advantages in both base knowledge preservation and incremental
learning. Our implementation is available at
https://github.com/Jywsuperman/LGSP.

</details>


### [25] [MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2507.09184)
*Qiyan Zhao,Xiaofeng Zhang,Yiheng Li,Yun Xing,Xiaosong Yuan,Feilong Tang,Sinan Fan,Xuhang Chen,Xuyao Zhang,Dahan Wang*

Main category: cs.CV

TL;DR: 本文提出一种名为MCA-LLaVA的改进方法，通过基于曼哈顿距离的二维、多方向空间衰减机制，加强多模态对齐，缓解LVLM模型中的图像对齐偏差与幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型视觉语言模型（LVLMs）中由于Rotary Position Encoding引发的图像对齐偏差问题，以减轻幻觉现象并提升多模态对齐效果。

Method: 提出MCA-LLaVA方法，基于曼哈顿距离的二维、多方向空间衰减扩展，优化位置建模，结合一维和二维图像token的序列和空间位置信息。

Result: 在多项幻觉和通用基准测试中，MCA-LLaVA展现了卓越的效果与广泛的适用性。

Conclusion: MCA-LLaVA通过改善多模态对齐有效缓解了LVLMs中出现的幻觉现象，提供了一种通用且高效的解决方案。

Abstract: Hallucinations pose a significant challenge in Large Vision Language Models
(LVLMs), with misalignment between multimodal features identified as a key
contributing factor. This paper reveals the negative impact of the long-term
decay in Rotary Position Encoding (RoPE), used for positional modeling in
LVLMs, on multimodal alignment. Concretely, under long-term decay, instruction
tokens exhibit uneven perception of image tokens located at different positions
within the two-dimensional space: prioritizing image tokens from the
bottom-right region since in the one-dimensional sequence, these tokens are
positionally closer to the instruction tokens. This biased perception leads to
insufficient image-instruction interaction and suboptimal multimodal alignment.
We refer to this phenomenon as image alignment bias. To enhance instruction's
perception of image tokens at different spatial locations, we propose
MCA-LLaVA, based on Manhattan distance, which extends the long-term decay to a
two-dimensional, multi-directional spatial decay. MCA-LLaVA integrates the
one-dimensional sequence order and two-dimensional spatial position of image
tokens for positional modeling, mitigating hallucinations by alleviating image
alignment bias. Experimental results of MCA-LLaVA across various hallucination
and general benchmarks demonstrate its effectiveness and generality. The code
can be accessed in https://github.com/ErikZ719/MCA-LLaVA.

</details>


### [26] [THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage](https://arxiv.org/abs/2507.09200)
*Trong-Thuan Nguyen,Pha Nguyen,Jackson Cothren,Alper Yilmaz,Minh-Triet Tran,Khoa Luu*

Main category: cs.CV

TL;DR: 本文提出了THYME方法和AeroEye-v1.0数据集，用于改进视频场景图生成的效果，特别是在细粒度空间细节和时间一致性建模上表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前的视频场景图生成方法在捕获复杂的时空依赖和细微的空间细节时存在不足，因此需要开发新的方法解决这些问题以提升动态场景分析能力。

Method: 提出了一种名为THYME的多尺度时空建模方法，通过层级特征聚合和循环时间优化，提高场景图生成的空间细节捕获和跨帧时间一致性，同时引入新数据集AeroEye-v1.0增强测评能力。

Result: 在ASPIRe和AeroEye-v1.0数据集上的实验表明，THYME在地面和空中场景的场景图生成中均优于现有方法。

Conclusion: THYME方法通过创新性结合多尺度空间建模和时间一致性优化，为动态场景图生成提供了更精确与连贯的解决方案，同时新的数据集AeroEye-v1.0也拓展了该领域的研究潜力。

Abstract: The rapid proliferation of video in applications such as autonomous driving,
surveillance, and sports analytics necessitates robust methods for dynamic
scene understanding. Despite advances in static scene graph generation and
early attempts at video scene graph generation, previous methods often suffer
from fragmented representations, failing to capture fine-grained spatial
details and long-range temporal dependencies simultaneously. To address these
limitations, we introduce the Temporal Hierarchical Cyclic Scene Graph (THYME)
approach, which synergistically integrates hierarchical feature aggregation
with cyclic temporal refinement to address these limitations. In particular,
THYME effectively models multi-scale spatial context and enforces temporal
consistency across frames, yielding more accurate and coherent scene graphs. In
addition, we present AeroEye-v1.0, a novel aerial video dataset enriched with
five types of interactivity that overcome the constraints of existing datasets
and provide a comprehensive benchmark for dynamic scene graph generation.
Empirically, extensive experiments on ASPIRe and AeroEye-v1.0 demonstrate that
the proposed THYME approach outperforms state-of-the-art methods, offering
improved scene understanding in ground-view and aerial scenarios.

</details>


### [27] [Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves](https://arxiv.org/abs/2507.09207)
*Alexander C. Ogren,Berthy T. Feng,Jihoon Ahn,Katherine L. Bouman,Chiara Daraio*

Main category: cs.CV

TL;DR: 本文提出了一种通过视频中表面波来推断结构厚度和刚度的方法，并使用物理优化问题进行参数拟合。


<details>
  <summary>Details</summary>
Motivation: 探究表面波传播与材料内部物理属性之间的关系，以实现通过视频捕捉推断材料特性的新方法。

Method: 从视频中提取表面波的色散关系，然后利用基于物理的优化算法估算最佳厚度和刚度参数。

Result: 方法在模拟数据和真实数据中的验证显示，与真实测量值高度一致。

Conclusion: 提出一种可行的方法用于居家医疗健康监测和其他领域，证明了技术的概念与实用性。

Abstract: Wave propagation on the surface of a material contains information about
physical properties beneath its surface. We propose a method for inferring the
thickness and stiffness of a structure from just a video of waves on its
surface. Our method works by extracting a dispersion relation from the video
and then solving a physics-based optimization problem to find the best-fitting
thickness and stiffness parameters. We validate our method on both simulated
and real data, in both cases showing strong agreement with ground-truth
measurements. Our technique provides a proof-of-concept for at-home health
monitoring of medically-informative tissue properties, and it is further
applicable to fields such as human-computer interaction.

</details>


### [28] [Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models](https://arxiv.org/abs/2507.09209)
*Xiao Liang,Di Wang,Zhicheng Jiao,Ronghan Li,Pengfei Yang,Quan Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出一种名为Expert-Controlled Classifier-Free Guidance（Expert-CFG）的框架，通过引入不确定性估计策略和专家参与来提高医疗视觉语言模型（MedVLM）的可靠性和临床对齐性，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有医疗视觉语言模型（MedVLM）存在生成错误或无法验证的响应，尤其在医疗应用中可能产生严重影响，因此需要一个无需额外训练、能与临床知识更好对齐的解决方案。

Method: 设计Expert-CFG框架，结合不确定性估计策略以识别不可靠的输出，随后检索相关参考资料协助专家标注关键术语，并使用无分类器自由指导技术（Classifier-Free Guidance）来调整模型的词嵌入输出，从而实现响应的优化和临床对齐。

Result: 通过在三个医疗视觉问答基准上的评估，Expert-CFG在仅使用4.2B参数和有限专家标注的条件下超越了13B参数的最新模型，表现出色。

Conclusion: 在资源有限的环境中，Expert-CFG框架验证了其用于临床实践的可行性，提供了一种新的方向以改进MedVLM的可靠性和应用性。

Abstract: The rapid advancements in Vision Language Models (VLMs) have prompted the
development of multi-modal medical assistant systems. Despite this progress,
current models still have inherent probabilistic uncertainties, often producing
erroneous or unverified responses-an issue with serious implications in medical
applications. Existing methods aim to enhance the performance of Medical Vision
Language Model (MedVLM) by adjusting model structure, fine-tuning with
high-quality data, or through preference fine-tuning. However, these
training-dependent strategies are costly and still lack sufficient alignment
with clinical expertise. To address these issues, we propose an
expert-in-the-loop framework named Expert-Controlled Classifier-Free Guidance
(Expert-CFG) to align MedVLM with clinical expertise without additional
training. This framework introduces an uncertainty estimation strategy to
identify unreliable outputs. It then retrieves relevant references to assist
experts in highlighting key terms and applies classifier-free guidance to
refine the token embeddings of MedVLM, ensuring that the adjusted outputs are
correct and align with expert highlights. Evaluations across three medical
visual question answering benchmarks demonstrate that the proposed Expert-CFG,
with 4.2B parameters and limited expert annotations, outperforms
state-of-the-art models with 13B parameters. The results demonstrate the
feasibility of deploying such a system in resource-limited settings for
clinical use.

</details>


### [29] [Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline](https://arxiv.org/abs/2507.09214)
*Shiyi Mu,Zichong Gu,Hanqi Lyu,Yilin Gao,Shugong Xu*

Main category: cs.CV

TL;DR: 本文提出了一个名为S3AD的算法，用于解决3D目标检测在开放道路上面对异常目标时的泛化能力不足问题，并通过创建新的增强数据集KITTI-AR验证了算法的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于3D检测模型在开放道路上的泛化能力受到限制，尤其在面对异常目标时，现有的模型训练集无法有效覆盖各种可能情况，因此本文提出方法增强3D检测的泛化及对异常目标的过滤能力。

Method: 提出了S3AD算法，解耦了2D与3D的训练策略，并设计了一种基于前景置信度预测的异常目标评分方法。此外，创建了KITTI-AR增强数据集，包含正常类别及从未见过的模拟异常类别用于实现零样本检测验证。

Result: 通过实验验证该算法在开放环境下对于异常目标的检测性能优异，同时KITTI-AR数据集增强了模型对不同场景异常的泛化能力。

Conclusion: S3AD不仅提升了3D目标检测模型在异常目标下的性能，同时KITTI-AR数据集展现其在模型训练及评价中的重要性，有助于推动自动驾驶3D检测技术的进步。

Abstract: 3D detection technology is widely used in the field of autonomous driving,
with its application scenarios gradually expanding from enclosed highways to
open conventional roads. For rare anomaly categories that appear on the road,
3D detection models trained on closed sets often misdetect or fail to detect
anomaly objects. To address this risk, it is necessary to enhance the
generalization ability of 3D detection models for targets of arbitrary shapes
and to possess the capability to filter out anomalies. The generalization of 3D
detection is limited by two factors: the coupled training of 2D and 3D, and the
insufficient diversity in the scale distribution of training samples. This
paper proposes a Stereo-based 3D Anomaly object Detection (S3AD) algorithm,
which decouples the training strategy of 3D and 2D to release the
generalization ability for arbitrary 3D foreground detection, and proposes an
anomaly scoring algorithm based on foreground confidence prediction, achieving
target-level anomaly scoring. In order to further verify and enhance the
generalization of anomaly detection, we use a 3D rendering method to synthesize
two augmented reality binocular stereo 3D detection datasets which named
KITTI-AR. KITTI-AR extends upon KITTI by adding 97 new categories, totaling 6k
pairs of stereo images. The KITTI-AR-ExD subset includes 39 common categories
as extra training data to address the sparse sample distribution issue.
Additionally, 58 rare categories form the KITTI-AR-OoD subset, which are not
used in training to simulate zero-shot scenarios in real-world settings, solely
for evaluating 3D anomaly detection. Finally, the performance of the algorithm
and the dataset is verified in the experiments. (Code and dataset can be
obtained at https://github.com/xxxx/xxx).

</details>


### [30] [360-Degree Full-view Image Segmentation by Spherical Convolution compatible with Large-scale Planar Pre-trained Models](https://arxiv.org/abs/2507.09216)
*Jingguo Liu,Han Yu,Shigang Li,Jianfeng Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的球面采样方法，用于在全景图像中应用二维图像的预训练模型，有效减少了全景图像的失真问题并提高了任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有任务依赖二维图像的预训练模型，但这些模型难以识别全景图像中的失真和不连续性，影响性能。

Method: 提出一种基于球面离散采样的新方法，该方法根据预训练模型的权重进行采样，直接适配全景图像。同时，将该方法用于全景图像分割，并通过球面模型特征生成特定通道注意力的掩码。

Result: 在常用的室内数据集Stanford2D3D上表现良好，表明方法有效缓解了全景失真问题并取得了优异的分割效果。

Conclusion: 该方法成功实现了全景图像对二维预训练模型的高效利用，在分割任务上展示了显著效果。

Abstract: Due to the current lack of large-scale datasets at the million-scale level,
tasks involving panoramic images predominantly rely on existing two-dimensional
pre-trained image benchmark models as backbone networks. However, these
networks are not equipped to recognize the distortions and discontinuities
inherent in panoramic images, which adversely affects their performance in such
tasks. In this paper, we introduce a novel spherical sampling method for
panoramic images that enables the direct utilization of existing pre-trained
models developed for two-dimensional images. Our method employs spherical
discrete sampling based on the weights of the pre-trained models, effectively
mitigating distortions while achieving favorable initial training values.
Additionally, we apply the proposed sampling method to panoramic image
segmentation, utilizing features obtained from the spherical model as masks for
specific channel attentions, which yields commendable results on commonly used
indoor datasets, Stanford2D3D.

</details>


### [31] [Online Long-term Point Tracking in the Foundation Model Era](https://arxiv.org/abs/2507.09217)
*Görkay Aydemir*

Main category: cs.CV

TL;DR: 本文提出了一种名为Track-On的在线点追踪方法，该方法使用transformer模型在不依赖未来信息的情况下进行长时间追踪，达到了新的基准性能。


<details>
  <summary>Details</summary>
Motivation: 在许多真实世界场景中，如流媒体视频和自主AI中，需要在线点追踪以便立即决策，但目前多数方法无法满足这一需求。

Method: 提出了Track-On，一个基于transformer的模型，将追踪点作为查询逐帧处理视频，并通过引入记忆机制传播外观和上下文信息，适应在线因果推理的要求。

Result: Track-On在七个公共基准测试中实现了新的性能标准，证明了在无未来信息情况下进行长时间追踪的可行性。

Conclusion: Track-On成功表明视觉基础模型可以用于在线长时间追踪。同时，通过专门设计的追踪方法，可以在在线因果条件下实现鲁棒的点追踪。

Abstract: Point tracking aims to identify the same physical point across video frames
and serves as a geometry-aware representation of motion. This representation
supports a wide range of applications, from robotics to augmented reality, by
enabling accurate modeling of dynamic environments. Most existing long-term
tracking approaches operate in an offline setting, where future frames are
available to refine predictions and recover from occlusions. However,
real-world scenarios often demand online predictions: the model must operate
causally, using only current and past frames. This constraint is critical in
streaming video and embodied AI, where decisions must be made immediately based
on past observations. Under such constraints, viewpoint invariance becomes
essential. Visual foundation models, trained on diverse large-scale datasets,
offer the potential for robust geometric representations. While they lack
temporal reasoning on their own, they can be integrated into tracking pipelines
to enrich spatial features. In this thesis, we address the problem of long-term
point tracking in an online setting, where frames are processed sequentially
without access to future information or sliding windows. We begin by evaluating
the suitability of visual foundation models for this task and find that they
can serve as useful initializations and be integrated into tracking pipelines.
However, to enable long-term tracking in an online setting, a dedicated design
is still required. In particular, maintaining coherence over time in this
causal regime requires memory to propagate appearance and context across
frames. To address this, we introduce Track-On, a transformer-based model that
treats each tracked point as a query and processes video frames one at a time.
Track-On sets a new state of the art across seven public benchmarks,
demonstrating the feasibility of long-term tracking without future access.

</details>


### [32] [Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift](https://arxiv.org/abs/2507.09222)
*Behraj Khan,Tahir Syed*

Main category: cs.CV

TL;DR: StaRFM框架通过Fisher信息惩罚(FIP)和置信度不匹配惩罚(CMP)，解决基础模型在视觉和医学影像任务中的分布偏移和置信度失配问题，提升性能。


<details>
  <summary>Details</summary>
Motivation: 基础模型在迁移学习中表现显著，但分布偏移和置信度失配限制了其实际部署性能。该研究旨在提出一个通用框架克服这些障碍。

Method: StaRFM引入FIP以减少分布偏移，并通过CMP校准分割任务的不确定性。此外，利用PAC-Bayes界限理论支持，展示FIP控制了Fisher-Rao范数的泛化，而CMP通过Brier评分优化减少校准误差。

Result: StaRFM在19个视觉数据集上提高+3.5%的准确率，降低28%的ECE；在医学分割任务中实现84.7%的DSC和4.8mm的HD95；跨域性能差距减少40%。

Conclusion: StaRFM是一种通用、无缝集成的框架，通过提升基础模型的泛化和校准能力改善视觉与医学影像任务表现。

Abstract: Foundation models like CLIP and SAM have transformed computer vision and
medical imaging via low-shot transfer learning. However, deployment of these
models hindered by two key challenges: \textit{distribution shift} between
training and test data, and \textit{confidence misalignment} that leads to
overconfident incorrect predictions. These issues manifest differently in
vision-language classification and medical segmentation tasks, yet existing
solutions remain domain-specific. We propose \textit{StaRFM}, a unified
framework addressing both challenges. It introduces a Fisher information
penalty (FIP), extended to 3D medical data via patch-wise regularization, to
reduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence
misalignment penalty (CMP), reformulated for voxel-level predictions,
calibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes
bounds showing FIP controls generalization via the Fisher-Rao norm, while CMP
minimizes calibration error through Brier score optimization. StaRFM shows
consistent performance like \texttt{+}3.5\% accuracy and 28\% lower ECE on 19
vision datasets (e.g., ImageNet, Office-Home), 84.7\% DSC and 4.8mm HD95 in
medical segmentation (e.g., BraTS, ATLAS), and 40\% lower cross-domain
performance gap compared to prior benchmarking methods. The framework is
plug-and-play, requiring minimal architectural changes for seamless integration
with foundation models. Code and models will be released at
https://anonymous.4open.science/r/StaRFM-C0CD/README.md

</details>


### [33] [EgoAnimate: Generating Human Animations from Egocentric top-down Views](https://arxiv.org/abs/2507.09230)
*G. Kutay Türkoglu,Julian Tanke,Iheb Belgacem,Lev Markhasin*

Main category: cs.CV

TL;DR: 该研究开发了一种基于稳定扩散（Stable Diffusion）的生成性方法，从单一顶视图的自我中心图像生成可动画的化身，为远程呈现系统提供了高效率和高普适性的解决方案。


<details>
  <summary>Details</summary>
Motivation: 为了实现理想的数字远程呈现体验，需要准确捕捉和重现人的身体、衣物及动作。传统方法中自我中心视角设备具成本及便携性优势，但同时也引入了视野遮挡及身体比例失真的问题。

Method: 该研究首次从生成式骨干网络出发，结合控制控制网络（ControlNet）及稳定扩散方法，将单一顶视图自我中心图象生成现实感强的前视图，并通过图像转动作模型生成化身的动作。

Result: 提出一种无需多视角训练数据的算法，能够从单一顶视图生成高质量的前视图，并生成对应的动作，从而减轻了训练负担并提高了泛化能力。

Conclusion: 这项研究推进了基于自我中心视角的可动画化身生成方法的发展，对更具普适性和可访问性的远程呈现系统具有重要意义。

Abstract: An ideal digital telepresence experience requires accurate replication of a
person's body, clothing, and movements. To capture and transfer these movements
into virtual reality, the egocentric (first-person) perspective can be adopted,
which enables the use of a portable and cost-effective device without
front-view cameras. However, this viewpoint introduces challenges such as
occlusions and distorted body proportions.
  There are few works reconstructing human appearance from egocentric views,
and none use a generative prior-based approach. Some methods create avatars
from a single egocentric image during inference, but still rely on multi-view
datasets during training. To our knowledge, this is the first study using a
generative backbone to reconstruct animatable avatars from egocentric inputs.
Based on Stable Diffusion, our method reduces training burden and improves
generalizability.
  Inspired by methods such as SiTH and MagicMan, which perform 360-degree
reconstruction from a frontal image, we introduce a pipeline that generates
realistic frontal views from occluded top-down images using ControlNet and a
Stable Diffusion backbone.
  Our goal is to convert a single top-down egocentric image into a realistic
frontal representation and feed it into an image-to-motion model. This enables
generation of avatar motions from minimal input, paving the way for more
accessible and generalizable telepresence systems.

</details>


### [34] [PPJudge: Towards Human-Aligned Assessment of Artistic Painting Process](https://arxiv.org/abs/2507.09242)
*Shiqi Jiang,Xinpeng Li,Xi Mao,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: 本文提出了一个新的框架，旨在评估绘画过程的美学质量，包括引入PPAD数据集和基于Transformer的PPJudge模型。


<details>
  <summary>Details</summary>
Motivation: 目前的艺术评估方法大多只关注静态的最终图像，忽视了绘画过程的动态性和多阶段特性。这个研究希望填补这一研究空白。

Method: 该研究引入了一个新数据集PPAD，包含实际和合成的绘画过程图像，并为其提供八个属性的专家注释。同时，提出了一个基于Transformer的PPJudge模型，配备了时间位置编码和异质专家混合架构。

Result: 实验结果表明，所提出的方法在准确性、鲁棒性和与人类评判的一致性方面优于现有基线。

Conclusion: 该方法可为计算创作和艺术教育提供新的见解，为评估绘画过程的美学质量提供了强有力的工具。

Abstract: Artistic image assessment has become a prominent research area in computer
vision. In recent years, the field has witnessed a proliferation of datasets
and methods designed to evaluate the aesthetic quality of paintings. However,
most existing approaches focus solely on static final images, overlooking the
dynamic and multi-stage nature of the artistic painting process. To address
this gap, we propose a novel framework for human-aligned assessment of painting
processes. Specifically, we introduce the Painting Process Assessment Dataset
(PPAD), the first large-scale dataset comprising real and synthetic painting
process images, annotated by domain experts across eight detailed attributes.
Furthermore, we present PPJudge (Painting Process Judge), a Transformer-based
model enhanced with temporally-aware positional encoding and a heterogeneous
mixture-of-experts architecture, enabling effective assessment of the painting
process. Experimental results demonstrate that our method outperforms existing
baselines in accuracy, robustness, and alignment with human judgment, offering
new insights into computational creativity and art education.

</details>


### [35] [AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition](https://arxiv.org/abs/2507.09248)
*Varsha Devi,Amine Bohi,Pardeep Kumar*

Main category: cs.CV

TL;DR: 提出了一种新模型AGCD-Net，用于上下文感知情感识别，设计了混合ConvNeXt编码器并引入因果干预模块，从而减少上下文偏差并取得了突破性效果。


<details>
  <summary>Details</summary>
Motivation: 解决传统上下文感知情感识别方法中背景上下文与情感标签的虚假相关性问题，例如将“花园”错误关联为“快乐”。

Method: 设计了AGCD-Net，包含混合ConvNeXt编码器（融合了空间转换网络和压缩激励层）以及“注意力引导-因果干预模块（AG-CIM）”，通过干预背景上下文特征并利用人脸特征引导校正，减少虚假相关性。

Result: 在CAER-S数据集上进行了实验，AGCD-Net取得了领先的情感识别性能、证明了去偏因果干预的有效性。

Conclusion: 引入因果理论可以有效减轻情感识别中的上下文偏差，模型AGCD-Net展现了处理复杂情感识别环境中显著的鲁棒性。

Abstract: Context-aware emotion recognition (CAER) enhances affective computing in
real-world scenarios, but traditional methods often suffer from context
bias-spurious correlation between background context and emotion labels (e.g.
associating ``garden'' with ``happy''). In this paper, we propose
\textbf{AGCD-Net}, an Attention Guided Context Debiasing model that introduces
\textit{Hybrid ConvNeXt}, a novel convolutional encoder that extends the
ConvNeXt backbone by integrating Spatial Transformer Network and
Squeeze-and-Excitation layers for enhanced feature recalibration. At the core
of AGCD-Net is the Attention Guided - Causal Intervention Module (AG-CIM),
which applies causal theory, perturbs context features, isolates spurious
correlations, and performs an attention-driven correction guided by face
features to mitigate context bias. Experimental results on the CAER-S dataset
demonstrate the effectiveness of AGCD-Net, achieving state-of-the-art
performance and highlighting the importance of causal debiasing for robust
emotion recognition in complex settings.

</details>


### [36] [Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching](https://arxiv.org/abs/2507.09256)
*Junyu Chen,Yihua Gao,Mingyuan Ge,Mingyong Li*

Main category: cs.CV

TL;DR: 本文提出了AAHR框架，通过动态聚类原型对比学习和GNN等技术有效提升图文匹配的精度，对比现有方法在多个数据集上表现更优异。


<details>
  <summary>Details</summary>
Motivation: 现有图文匹配方法难以处理高阶关联和语义模糊性，以及未充分利用语义相似样本间的邻域关系。

Method: 提出了AAHR框架，包含动态聚类原型对比学习、全局和局部特征提取、自适应聚合网络和基于GNN的邻域关系建模，同时加入动量对比学习扩大负样本集合。

Result: AAHR在Flickr30K、MSCOCO和ECCV Caption等数据集上超越当前最优，显著提升图文匹配的准确性和效率。

Conclusion: AAHR框架通过解决高阶关联与语义模糊问题，并充分利用邻域信息，有效改进了图文匹配性能。

Abstract: Image-text matching is crucial for bridging the semantic gap between computer
vision and natural language processing. However, existing methods still face
challenges in handling high-order associations and semantic ambiguities among
similar instances. These ambiguities arise from subtle differences between soft
positive samples (semantically similar but incorrectly labeled) and soft
negative samples (locally matched but globally inconsistent), creating matching
uncertainties. Furthermore, current methods fail to fully utilize the
neighborhood relationships among semantically similar instances within training
batches, limiting the model's ability to learn high-order shared knowledge.
This paper proposes the Ambiguity-Aware and High-order Relation learning
framework (AAHR) to address these issues. AAHR constructs a unified
representation space through dynamic clustering prototype contrastive learning,
effectively mitigating the soft positive sample problem. The framework
introduces global and local feature extraction mechanisms and an adaptive
aggregation network, significantly enhancing full-grained semantic
understanding capabilities. Additionally, AAHR employs intra-modal and
inter-modal correlation matrices to investigate neighborhood relationships
among sample instances thoroughly. It incorporates GNN to enhance semantic
interactions between instances. Furthermore, AAHR integrates momentum
contrastive learning to expand the negative sample set. These combined
strategies significantly improve the model's ability to discriminate between
features. Experimental results demonstrate that AAHR outperforms existing
state-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets,
considerably improving the accuracy and efficiency of image-text matching. The
code and model checkpoints for this research are available at
https://github.com/Image-Text-Matching/AAHR .

</details>


### [37] [SAGE: Segment-Aware Gloss-Free Encoding for Token-Efficient Sign Language Translation](https://arxiv.org/abs/2507.09266)
*JianHe Low,Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: 该研究提出了一种段感知视觉标记化框架，利用手语段分割将连续视频转化为离散视觉标记，在无需词汇级监督的情况下显著降低输入序列长度和内存使用，同时提高在PHOENIX14T基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有无词汇级注释的手语翻译模型复杂度高，计算要求高，难以在大规模手语数据集上扩展。

Method: 引入段感知视觉标记化框架，将视频转化为离散视觉标记；通过标记对比对齐目标和双级监督联结视觉和语言模态，无需依赖词汇级监督。

Result: 相比现有方法，输入序列长度减少50%、内存使用降低2.67倍；在PHOENIX14T基准上性能超过现有技术，同时在相同序列长度情况下表现更优。

Conclusion: 段感知视觉标记化框架有效提升手语翻译性能，降低计算复杂度，验证了其标记化和对齐策略在手语翻译中的潜力。

Abstract: Gloss-free Sign Language Translation (SLT) has advanced rapidly, achieving
strong performances without relying on gloss annotations. However, these gains
have often come with increased model complexity and high computational demands,
raising concerns about scalability, especially as large-scale sign language
datasets become more common. We propose a segment-aware visual tokenization
framework that leverages sign segmentation to convert continuous video into
discrete, sign-informed visual tokens. This reduces input sequence length by up
to 50% compared to prior methods, resulting in up to 2.67x lower memory usage
and better scalability on larger datasets. To bridge the visual and linguistic
modalities, we introduce a token-to-token contrastive alignment objective,
along with a dual-level supervision that aligns both language embeddings and
intermediate hidden states. This improves fine-grained cross-modal alignment
without relying on gloss-level supervision. Our approach notably exceeds the
performance of state-of-the-art methods on the PHOENIX14T benchmark, while
significantly reducing sequence length. Further experiments also demonstrate
our improved performance over prior work under comparable sequence-lengths,
validating the potential of our tokenization and alignment strategies.

</details>


### [38] [Cross Knowledge Distillation between Artificial and Spiking Neural Networks](https://arxiv.org/abs/2507.09269)
*Shuhan Ye,Yuanbin Qian,Chong Wang,Sunqi Lin,Jiazhen Xu,Jiangbo Qian,Yuqi Li*

Main category: cs.CV

TL;DR: 该研究提出了一个跨模态知识蒸馏（CKD）方法，用于提升用于DVS数据的脉冲神经网络（SNNs）的性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前脉冲神经网络因数据有限和架构不成熟导致性能低于传统人工神经网络的瓶颈问题。

Method: 设计了一种跨模态知识蒸馏方法。采用语义相似性和滑动替换解决跨模态问题，同时使用间接分阶段知识蒸馏解决跨架构问题。

Result: 在N-Caltech101和CEP-DVS等常用数据集上验证了方法优越性，结果优于现有最优方法。

Conclusion: 跨模态知识蒸馏方法有效提升了SNNs在DVS数据上的表现，且具有广泛的实用潜力。

Abstract: Recently, Spiking Neural Networks (SNNs) have demonstrated rich potential in
computer vision domain due to their high biological plausibility, event-driven
characteristic and energy-saving efficiency. Still, limited annotated
event-based datasets and immature SNN architectures result in their performance
inferior to that of Artificial Neural Networks (ANNs). To enhance the
performance of SNNs on their optimal data format, DVS data, we explore using
RGB data and well-performing ANNs to implement knowledge distillation. In this
case, solving cross-modality and cross-architecture challenges is necessary. In
this paper, we propose cross knowledge distillation (CKD), which not only
leverages semantic similarity and sliding replacement to mitigate the
cross-modality challenge, but also uses an indirect phased knowledge
distillation to mitigate the cross-architecture challenge. We validated our
method on main-stream neuromorphic datasets, including N-Caltech101 and
CEP-DVS. The experimental results show that our method outperforms current
State-of-the-Art methods. The code will be available at
https://github.com/ShawnYE618/CKD

</details>


### [39] [Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models](https://arxiv.org/abs/2507.09279)
*Anita Kriz,Elizabeth Laura Janes,Xing Shen,Tal Arbel*

Main category: cs.CV

TL;DR: 本文提出了Prompt4Trust，一个用于增强多模态大语言模型（MLLMs）信心校准的RL框架，旨在提升医疗领域模型的安全性和可信赖性。


<details>
  <summary>Details</summary>
Motivation: 在医疗领域，MLLMs 应用潜力巨大，但其对提示设计敏感和生成高置信度的错误响应限制了其部署。此外，依赖模型置信度可能影响临床决策的安全性。

Method: 提出一个增强的RL框架Prompt4Trust，通过训练轻量级LLM生成上下文感知的辅助提示，引导下游任务模型生成置信度与预测准确性更匹配的响应。

Result: 该方法不仅提升了医疗视觉问题解答（VQA）任务的准确性，在PMC-VQA基准上达到了最先进的性能，同时小型模型还展示了对大型模型的零样本泛化潜力。

Conclusion: 通过以人类对齐的自动化提示工程，提高了MLLMs在安全关键设置中的可信度，且提供了在计算成本较低的前提下扩展校准的可能性。

Abstract: Multimodal large language models (MLLMs) hold considerable promise for
applications in healthcare. However, their deployment in safety-critical
settings is hindered by two key limitations: (i) sensitivity to prompt design,
and (ii) a tendency to generate incorrect responses with high confidence. As
clinicians may rely on a model's stated confidence to gauge the reliability of
its predictions, it is especially important that when a model expresses high
confidence, it is also highly accurate. We introduce Prompt4Trust, the first
reinforcement learning (RL) framework for prompt augmentation targeting
confidence calibration in MLLMs. A lightweight LLM is trained to produce
context-aware auxiliary prompts that guide a downstream task MLLM to generate
responses in which the expressed confidence more accurately reflects predictive
accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically
prioritizes aspects of calibration most critical for safe and trustworthy
clinical decision-making. Beyond improvements driven by this clinically
motivated calibration objective, our proposed method also improves task
accuracy, achieving state-of-the-art medical visual question answering (VQA)
performance on the PMC-VQA benchmark, which is composed of multiple-choice
questions spanning diverse medical imaging modalities. Moreover, our framework
trained with a small downstream task MLLM showed promising zero-shot
generalization to larger MLLMs in our experiments, suggesting the potential for
scalable calibration without the associated computational costs. This work
demonstrates the potential of automated yet human-aligned prompt engineering
for improving the the trustworthiness of MLLMs in safety critical settings. Our
codebase can be found at https://github.com/xingbpshen/vccrl-llm.

</details>


### [40] [Generative Latent Kernel Modeling for Blind Motion Deblurring](https://arxiv.org/abs/2507.09285)
*Chenhao Ding,Jiangtao Zhang,Zongsheng Yue,Hui Wang,Qian Zhao,Deyu Meng*

Main category: cs.CV

TL;DR: 该论文提出了一种基于深度生成模型的盲运动去模糊（BMD）方法，通过生成式对抗网络（GAN）预训练模糊核生成器和初始化器，提升去模糊性能，并支持非均匀运动去模糊，取得了基准测试数据集上的最新性能。


<details>
  <summary>Details</summary>
Motivation: 现有盲运动去模糊方法因优化过程的高非凸性，对模糊核初始化极其敏感，限制了性能。

Method: 通过GAN预训练模糊核生成器描述模糊核的先验分布，并设计模糊核初始化器提供高质量的初始值，将BMD解决方案约束在紧致的核空间内。该框架以插件形式集成现有方法，扩展支持非均匀运动去模糊。

Result: 在不需要额外先验的条件下，方法在多个挑战性基准测试数据集上取得了最新性能。

Conclusion: 论文开发了一种新的盲运动去模糊框架，通过深度生成模型提高稳健性和性能，并标志性地扩展了非均匀分布去模糊的能力，提升了整体效果。

Abstract: Deep prior-based approaches have demonstrated remarkable success in blind
motion deblurring (BMD) recently. These methods, however, are often limited by
the high non-convexity of the underlying optimization process in BMD, which
leads to extreme sensitivity to the initial blur kernel. To address this issue,
we propose a novel framework for BMD that leverages a deep generative model to
encode the kernel prior and induce a better initialization for the blur kernel.
Specifically, we pre-train a kernel generator based on a generative adversarial
network (GAN) to aptly characterize the kernel's prior distribution, as well as
a kernel initializer to provide a well-informed and high-quality starting point
for kernel estimation. By combining these two components, we constrain the BMD
solution within a compact latent kernel manifold, thus alleviating the
aforementioned sensitivity for kernel initialization. Notably, the kernel
generator and initializer are designed to be easily integrated with existing
BMD methods in a plug-and-play manner, enhancing their overall performance.
Furthermore, we extend our approach to tackle blind non-uniform motion
deblurring without the need for additional priors, achieving state-of-the-art
performance on challenging benchmark datasets. The source code is available at
https://github.com/dch0319/GLKM-Deblur.

</details>


### [41] [Supercharging Floorplan Localization with Semantic Rays](https://arxiv.org/abs/2507.09291)
*Yuval Grader,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 本文提出了一种语义感知的楼层平面定位框架，通过联合估计深度与语义射线来预测结构-语义概率体积，并以粗到细的方式进行优化，从而改善楼层平面定位的性能。


<details>
  <summary>Details</summary>
Motivation: 目前的楼层平面定位技术主要依赖于深度的结构线索，缺乏对平面图中丰富的语义信息的利用，因此需要一个能够同时利用深度与语义信息的方法。

Method: 提出了一种语义感知的框架，联合估计深度和语义射线。通过先采样低分辨率的概率体积，再在高概率区域密集采样细化概率值，从而预测平面位置和方向角。此外，框架还能整合额外元数据如房间标签。

Result: 在两个标准楼层平面定位基准上评估，显著优于现有方法，在召回率等指标上获得了大幅提升。

Conclusion: 该框架验证了语义与深度联合建模在楼层平面定位中的有效性，同时其扩展性支持整合元数据，进一步提高了准确性与效率。

Abstract: Floorplans provide a compact representation of the building's structure,
revealing not only layout information but also detailed semantics such as the
locations of windows and doors. However, contemporary floorplan localization
techniques mostly focus on matching depth-based structural cues, ignoring the
rich semantics communicated within floorplans. In this work, we introduce a
semantic-aware localization framework that jointly estimates depth and semantic
rays, consolidating over both for predicting a structural-semantic probability
volume. Our probability volume is constructed in a coarse-to-fine manner: We
first sample a small set of rays to obtain an initial low-resolution
probability volume. We then refine these probabilities by performing a denser
sampling only in high-probability regions and process the refined values for
predicting a 2D location and orientation angle. We conduct an evaluation on two
standard floorplan localization benchmarks. Our experiments demonstrate that
our approach substantially outperforms state-of-the-art methods, achieving
significant improvements in recall metrics compared to prior works. Moreover,
we show that our framework can easily incorporate additional metadata such as
room labels, enabling additional gains in both accuracy and efficiency.

</details>


### [42] [Geo-RepNet: Geometry-Aware Representation Learning for Surgical Phase Recognition in Endoscopic Submucosal Dissection](https://arxiv.org/abs/2507.09294)
*Rui Tang,Haochen Yin,Guankun Wang,Long Bai,An Wang,Huxin Gao,Jiazheng Wang,Hongliang Ren*

Main category: cs.CV

TL;DR: 提出一种名为Geo-RepNet的新方法，利用深度信息强化外科手术阶段识别的精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的RGB图像在外科手术阶段识别中因视觉相似性和缺乏结构线索而受限，引入深度信息以提升空间关系和解剖结构理解。

Method: 提出基于RGB和深度信息的Geo-RepNet框架,包括重参数化的RepVGG骨干网络、深度引导几何先验生成模块（DGPG）和几何增强多尺度注意模块（GEMA）。

Result: 利用实际ESD视频构建的九阶段数据集上进行了密集标注和实验，Geo-RepNet在性能、鲁棒性以及计算效率方面达到了先进水平。

Conclusion: 通过整合深度信息和几何先验,Geo-RepNet在复杂低纹理手术环境中表现出优越性能，推动了外科手术智能辅助系统的发展。

Abstract: Surgical phase recognition plays a critical role in developing intelligent
assistance systems for minimally invasive procedures such as Endoscopic
Submucosal Dissection (ESD). However, the high visual similarity across
different phases and the lack of structural cues in RGB images pose significant
challenges. Depth information offers valuable geometric cues that can
complement appearance features by providing insights into spatial relationships
and anatomical structures. In this paper, we pioneer the use of depth
information for surgical phase recognition and propose Geo-RepNet, a
geometry-aware convolutional framework that integrates RGB image and depth
information to enhance recognition performance in complex surgical scenes.
Built upon a re-parameterizable RepVGG backbone, Geo-RepNet incorporates the
Depth-Guided Geometric Prior Generation (DGPG) module that extracts geometry
priors from raw depth maps, and the Geometry-Enhanced Multi-scale Attention
(GEMA) to inject spatial guidance through geometry-aware cross-attention and
efficient multi-scale aggregation. To evaluate the effectiveness of our
approach, we construct a nine-phase ESD dataset with dense frame-level
annotations from real-world ESD videos. Extensive experiments on the proposed
dataset demonstrate that Geo-RepNet achieves state-of-the-art performance while
maintaining robustness and high computational efficiency under complex and
low-texture surgical environments.

</details>


### [43] [ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation](https://arxiv.org/abs/2507.09299)
*Abdulvahap Mutlu,Şengül Doğan,Türker Tuncer*

Main category: cs.CV

TL;DR: 本文提出了一种名为ViT-ProtoNet的新方法，将Vision Transformers (ViTs) 集成到Prototypical Network框架中用于少样本图像分类，并在多个基准数据集上显著超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 探讨如何充分利用ViTs的表征能力以提升少样本图像分类性能。

Method: 将ViT-Small作为主干网络与Prototypical Network框架结合，通过对支持样本的类别条件token嵌入进行平均，构建更鲁棒的类别原型。并针对不同参数进行了消融实验。

Result: 在Mini-ImageNet、FC100、CUB-200和CIFAR-FS等数据集上，ViT-ProtoNet在5-shot分类准确率提升了最多3.2%，且特征空间表现出更好的分离性。

Conclusion: 提出的ViT-ProtoNet为基于transformer的少样本分类设立了新的基准，展现出强大的灵活性与表现力，同时开源代码和预训练权重以促进复现。

Abstract: The remarkable representational power of Vision Transformers (ViTs) remains
underutilized in few-shot image classification. In this work, we introduce
ViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical
Network framework. By averaging class conditional token embeddings from a
handful of support examples, ViT-ProtoNet constructs robust prototypes that
generalize to novel categories under 5-shot settings. We conduct an extensive
empirical evaluation on four standard benchmarks: Mini-ImageNet, FC100,
CUB-200, and CIFAR-FS, including overlapped support variants to assess
robustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based
prototypical counterparts, achieving up to a 3.2\% improvement in 5-shot
accuracy and demonstrating superior feature separability in latent space.
Furthermore, it outperforms or is competitive with transformer-based
competitors using a more lightweight backbone. Comprehensive ablations examine
the impact of transformer depth, patch size, and fine-tuning strategy. To
foster reproducibility, we release code and pretrained weights. Our results
establish ViT-ProtoNet as a powerful, flexible approach for few-shot
classification and set a new baseline for transformer-based meta-learners.

</details>


### [44] [DAA*: Deep Angular A Star for Image-based Path Planning](https://arxiv.org/abs/2507.09305)
*Zhiwei Xu*

Main category: cs.CV

TL;DR: 本文提出了深度角度A*算法（DAA*），通过添加路径角度自由度（PAF）改善路径平滑性，使得模仿学习生成的路径与专家示范更加相似。


<details>
  <summary>Details</summary>
Motivation: 当前模仿学习中路径平滑性被忽视，为此提出结合路径角度自由度的新方法来提升路径相似性。

Method: 通过将路径角度自由度（PAF）融入A*算法，联合优化路径缩短与路径平滑性，提高路径的适应性和相似性。

Result: 在7个数据集上测试，包括迷宫、视频游戏和真实场景，DAA*相比神经A*和TransPath在路径相似性和长度上均有显著提升，例如SPR提高9%。

Conclusion: DAA*算法通过探索路径角度限制与搜索效率的权衡显著提升了模仿学习路径的质量，提供了一种创新的方法处理路径相似性问题。

Abstract: Path smoothness is often overlooked in path imitation learning from expert
demonstrations. In this paper, we introduce a novel learning method, termed
deep angular A* (DAA*), by incorporating the proposed path angular freedom
(PAF) into A* to improve path similarity through adaptive path smoothness. The
PAF aims to explore the effect of move angles on path node expansion by finding
the trade-off between their minimum and maximum values, allowing for high
adaptiveness for imitation learning. DAA* improves path optimality by closely
aligning with the reference path through joint optimization of path shortening
and smoothing, which correspond to heuristic distance and PAF, respectively.
Throughout comprehensive evaluations on 7 datasets, including 4 maze datasets,
2 video-game datasets, and a real-world drone-view dataset containing 2
scenarios, we demonstrate remarkable improvements of our DAA* over neural A* in
path similarity between the predicted and reference paths with a shorter path
length when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM,
and 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path
loss and path probability map loss, DAA* significantly outperforms the
state-of-the-art TransPath by 6.7% SPR, 6.5% PSIM, and 3.7% ASIM. We also
discuss the minor trade-off between path optimality and search efficiency where
applicable.

</details>


### [45] [AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning](https://arxiv.org/abs/2507.09308)
*Zile Wang,Hao Yu,Jiabo Zhan,Chun Yuan*

Main category: cs.CV

TL;DR: 提出ALPHA基准测试和ALPHAVAE模型，显著提升RGBA图像生成和重建性能，并免费提供代码及数据。


<details>
  <summary>Details</summary>
Motivation: 现有的潜在扩散模型在RGB图像生成方面表现出色，但对透明或分层内容(RGBA图像)的生成缺乏探索，主要因为缺乏大规模基准测试。

Method: 设计ALPHA基准测试，将标准RGB评估指标适配为四通道图像；提出ALPHAVAE模型，通过训练一个结合多种损失目标的端到端RGBA VAE，提高透明图像生成与重建能力。

Result: 尽管只使用8K图像训练，ALPHAVAE在PSNR和SSIM上分别比LayerDiffuse提高+4.9 dB和+3.2%，并在透明图像生成中表现出色。

Conclusion: ALPHAVAE显著提高了RGBA图像的重建和生成质量，为相关模型和数据的未来发展提供了新的基准和方向。

Abstract: Recent advances in latent diffusion models have achieved remarkable results
in high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress
and reconstruct pixel data at low computational cost. However, the generation
of transparent or layered content (RGBA image) remains largely unexplored, due
to the lack of large-scale benchmarks. In this work, we propose ALPHA, the
first comprehensive RGBA benchmark that adapts standard RGB metrics to
four-channel images via alpha blending over canonical backgrounds. We further
introduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB
VAE by incorporating a dedicated alpha channel. The model is trained with a
composite objective that combines alpha-blended pixel reconstruction,
patch-level fidelity, perceptual consistency, and dual KL divergence
constraints to ensure latent fidelity across both RGB and alpha
representations. Our RGBA VAE, trained on only 8K images in contrast to 1M used
by prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase
in SSIM over LayerDiffuse in reconstruction. It also enables superior
transparent image generation when fine-tuned within a latent diffusion
framework. Our code, data, and models are released on
https://github.com/o0o0o00o0/AlphaVAE for reproducibility.

</details>


### [46] [An Enhanced Classification Method Based on Adaptive Multi-Scale Fusion for Long-tailed Multispectral Point Clouds](https://arxiv.org/abs/2412.11407)
*TianZhu Liu,BangYan Hu,YanFeng Gu,Xian Li,Aleksandra Pižurica*

Main category: cs.CV

TL;DR: 提出一种针对长尾分布的自适应多尺度融合方法，用于多光谱点云（MPC）分类，并通过实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 处理现有室外场景中稀疏标注目标、地表尺度差异以及长尾分布带来的分类问题。

Method: 设计了网格平衡采样策略、提出一个多尺度特征融合模块以及自适应混合损失模块。

Result: 实验表明该方法在三个MPC数据集上优于现有的先进方法。

Conclusion: 通过提出新的采样、融合和损失策略，有效提升了室外MPC数据分类的性能，尤其在对小类样本分类时表现出色。

Abstract: Multispectral point cloud (MPC) captures 3D spatial-spectral information from
the observed scene, which can be used for scene understanding and has a wide
range of applications. However, most of the existing classification methods
were extensively tested on indoor datasets, and when applied to outdoor
datasets they still face problems including sparse labeled targets, differences
in land-covers scales, and long-tailed distributions. To address the above
issues, an enhanced classification method based on adaptive multi-scale fusion
for MPCs with long-tailed distributions is proposed. In the training set
generation stage, a grid-balanced sampling strategy is designed to reliably
generate training samples from sparse labeled datasets. In the feature learning
stage, a multi-scale feature fusion module is proposed to fuse shallow features
of land-covers at different scales, addressing the issue of losing fine
features due to scale variations in land-covers. In the classification stage,
an adaptive hybrid loss module is devised to utilize multi-classification heads
with adaptive weights to balance the learning ability of different classes,
improving the classification performance of small classes due to various-scales
and long-tailed distributions in land-covers. Experimental results on three MPC
datasets demonstrate the effectiveness of the proposed method compared with the
state-of-the-art methods.

</details>


### [47] [ProactiveBench: A Comprehensive Benchmark Evaluating Proactive Interactions in Video Large Language Models](https://arxiv.org/abs/2507.09313)
*Yueqian Wang,Xiaojun Meng,Yifan Wang,Huishuai Zhang,Dongyan Zhao*

Main category: cs.CV

TL;DR: 提出了一个评估系统主动交互能力的新基准ProactiveBench，以及一种新的评价指标PAUC，较传统方法更准确地衡量用户体验。


<details>
  <summary>Details</summary>
Motivation: 研究致力于构建能够主动交互的多模态对话系统，以满足用户日益增长的期望，如在视频播放期间实时提供多轮响应的能力。

Method: 开发了ProactiveBench基准，用以评估系统主动交互能力；并提出了PAUC指标，能够考量时间动态，以提升评价准确性。

Result: 基于ProactiveBench对多种基线系统进行了广泛测试，并通过用户研究表明，PAUC比传统评价指标更符合人类偏好，能够更加真实地评估用户体验。

Conclusion: PAUC是评估主动交互场景中用户体验的更加可靠的方法，有助于推动多模态对话系统的研究进展。

Abstract: With the growing research focus on multimodal dialogue systems, the
capability for proactive interaction is gradually gaining recognition. As an
alternative to conventional turn-by-turn dialogue, users increasingly expect
multimodal systems to be more initiative, for example, by autonomously
determining the timing of multi-turn responses in real time during video
playback. To facilitate progress in this emerging area, we introduce
ProactiveBench, the first comprehensive benchmark to evaluate a system's
ability to engage in proactive interaction. Since model responses are generated
at varying timestamps, we further propose PAUC, the first metric that accounts
for the temporal dynamics of model responses. This enables a more accurate
evaluation of systems operating in proactive settings. Through extensive
benchmarking of various baseline systems on ProactiveBench and a user study of
human preferences, we show that PAUC is in better agreement with human
preferences than traditional evaluation metrics, which typically only consider
the textual content of responses. These findings demonstrate that PAUC provides
a more faithful assessment of user experience in proactive interaction
scenarios. Project homepage:
https://github.com/yellow-binary-tree/ProactiveBench

</details>


### [48] [Dynamic Inter-Class Confusion-Aware Encoder for Audio-Visual Fusion in Human Activity Recognition](https://arxiv.org/abs/2507.09323)
*Kaixuan Cong,Yifan Wang,Rongkun Xue,Yuyang Jiang,Yiming Feng,Jing Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新型编码器DICCAE，通过动态调整类别混淆损失，解决了音频和视频类别混淆问题，并采用自监督预训练策略，在人类活动识别任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有音视频预训练方法忽略了训练过程中可混淆类别的认知归纳与对比强化，无法有效区分类别间相似的活动。

Method: 提出DICCAE，通过动态调整类别混淆损失来增强类别区分能力，结合音频和视频模态及其融合，并引入集群指导的自监督预训练策略。

Result: DICCAE在VGGSound数据集上取得了65.5%的Top-1精度，接近最先进水平，并通过消融实验验证了模块的重要性。

Conclusion: DICCAE在细粒度音视频表征及类别区分方面有显著提升，为人类活动识别任务提供了有效解决方案。

Abstract: Humans do not understand individual events in isolation; rather, they
generalize concepts within classes and compare them to others. Existing
audio-video pre-training paradigms only focus on the alignment of the overall
audio-video modalities, without considering the reinforcement of distinguishing
easily confused classes through cognitive induction and contrast during
training. This paper proposes the Dynamic Inter-Class Confusion-Aware Encoder
(DICCAE), an encoder that aligns audio-video representations at a fine-grained,
category-level. DICCAE addresses category confusion by dynamically adjusting
the confusion loss based on inter-class confusion degrees, thereby enhancing
the model's ability to distinguish between similar activities. To further
extend the application of DICCAE, we also introduce a novel training framework
that incorporates both audio and video modalities, as well as their fusion. To
mitigate the scarcity of audio-video data in the human activity recognition
task, we propose a cluster-guided audio-video self-supervised pre-training
strategy for DICCAE. DICCAE achieves near state-of-the-art performance on the
VGGSound dataset, with a top-1 accuracy of 65.5%. We further evaluate its
feature representation quality through extensive ablation studies, validating
the necessity of each module.

</details>


### [49] [Lightweight Cloud Masking Models for On-Board Inference in Hyperspectral Imaging](https://arxiv.org/abs/2507.08052)
*Mazen Ali,António Pereira,Fabio Gentile,Aser Cortines,Sam Mugel,Román Orús,Stelios P. Neophytides,Michalis Mavrovouniotis*

Main category: cs.CV

TL;DR: 提出了一种用机器学习（尤其是CNNs）进行云和云影遮罩的高效方法，实现了超过93%的准确率。


<details>
  <summary>Details</summary>
Motivation: 实现高效、实时、高质量的高光谱卫星图像分析，通过云和云影遮罩的预处理，提升数据质量。

Method: 对多种机器学习方法（如XGBoost、LightGBM和CNN）进行评估，重点研究特征减少的轻量级CNN模型。

Result: 所有模型的准确率均超过93%，特征减少的CNN模型表现最佳，具有高准确度、较低存储需求和快速推理能力。

Conclusion: 轻量级人工智能模型具有用作实时高光谱图像处理的潜力，尤其适用于卫星上的AI处理系统。

Abstract: Cloud and cloud shadow masking is a crucial preprocessing step in
hyperspectral satellite imaging, enabling the extraction of high-quality,
analysis-ready data. This study evaluates various machine learning approaches,
including gradient boosting methods such as XGBoost and LightGBM as well as
convolutional neural networks (CNNs). All boosting and CNN models achieved
accuracies exceeding 93%. Among the investigated models, the CNN with feature
reduction emerged as the most efficient, offering a balance of high accuracy,
low storage requirements, and rapid inference times on both CPUs and GPUs.
Variations of this version, with only up to 597 trainable parameters,
demonstrated the best trade-off in terms of deployment feasibility, accuracy,
and computational efficiency. These results demonstrate the potential of
lightweight artificial intelligence (AI) models for real-time hyperspectral
image processing, supporting the development of on-board satellite AI systems
for space-based applications.

</details>


### [50] [Fast3D: Accelerating 3D Multi-modal Large Language Models for Efficient 3D Scene Understanding](https://arxiv.org/abs/2507.09334)
*Wencan Huang,Daizong Liu,Wei Hu*

Main category: cs.CV

TL;DR: 提出了一个名为Fast3D的用于3D多模态大语言模型(MLLMs)的视觉token剪枝框架，可以减少计算开销，同时保持场景理解性能。


<details>
  <summary>Details</summary>
Motivation: 目前的3D MLLMs计算效率低，主要因为需要处理大量的以对象为中心的视觉token以全面表示3D场景。虽然在2D MLLMs中视觉token剪枝已初见成效，但其在3D领域的应用尚未得到深入研究。

Method: 提供了两项技术创新：(1) 全局注意力预测（GAP），通过轻量化神经网络预测目标模型的全局注意力分布，从而高效评估token的重要性；(2) 样本自适应的视觉token剪枝（SAP），基于注意力复杂性评估，引入动态token预算，自动调整每层的剪枝比例。两种技术均无需修改目标模型参数。

Result: 通过五个基准任务的广泛评估验证了Fast3D的效果，尤其是在高视觉token剪枝比率下表现出色。

Conclusion: Fast3D框架能够高效剪枝3D视觉token，显著降低计算开销，同时保持模型的性能，为3D多模态大语言模型的实际部署提供了新的可能性。

Abstract: While 3D Multi-modal Large Language Models (MLLMs) demonstrate remarkable
scene understanding capabilities, their practical deployment faces critical
challenges due to computational inefficiency. The key bottleneck stems from
processing excessive object-centric visual tokens required for comprehensive 3D
scene representation. Although visual token pruning has shown promise in
accelerating 2D MLLMs, its applicability to 3D domains remains largely
unexplored due to fundamental disparities in token structures. In this paper,
we reveal two critical insights: (1) Significant redundancy exists in
object-level 3D token representations, analogous to patch-level redundancy in
2D systems; (2) Global attention patterns exhibit strong predictive power for
identifying non-essential tokens in 3D contexts. Building on these
observations, we propose Fast3D, a plug-and-play visual token pruning framework
for 3D MLLMs featuring two technical innovations: (1) Global Attention
Prediction (GAP), where a lightweight neural network learns to predict the
global attention distributions of the target model, enabling efficient token
importance estimation for precise pruning guidance; (2) Sample-Adaptive visual
token Pruning (SAP), which introduces dynamic token budgets through
attention-based complexity assessment, automatically adjusting layer-wise
pruning ratios based on input characteristics. Both of these two techniques
operate without modifying the parameters of the target model. Extensive
evaluations across five benchmarks validate the effectiveness of Fast3D,
particularly under high visual token pruning ratios. Code is available at
https://github.com/wencan25/Fast3D

</details>


### [51] [Simplifying Traffic Anomaly Detection with Video Foundation Models](https://arxiv.org/abs/2507.09338)
*Svetlana Orlova,Tommie Kerssies,Brunó B. Englert,Gijs Dubbelman*

Main category: cs.CV

TL;DR: 本研究提出了一种简单的编码器方法，利用Video ViT模型实现了高效、高性能的交通异常检测。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖复杂的多阶段或多表示融合结构，但其必要性尚未明确。本研究受到视觉感知领域基础模型研究的启发，探索简单模型能否实现更优性能。

Method: 提出了一种仅基于编码器的架构，使用预训练的视频视觉Transformer（Video ViT），并探讨了不同的预训练方式（包括自监督的遮掩视频建模和领域自适应预训练）。

Result: 发现简单编码器模型在预训练支持下可匹配甚至超越复杂的最新交通异常检测方法；自监督的遮掩视频建模优于弱监督和完全监督方式；领域适应预训练在无标记驾驶视频上进一步提升了性能。

Conclusion: 有效的预训练能够让简单高效的架构在交通异常检测中表现优异，凸显预训练的重要性并表明简单架构具有可扩展性。

Abstract: Recent methods for ego-centric Traffic Anomaly Detection (TAD) often rely on
complex multi-stage or multi-representation fusion architectures, yet it
remains unclear whether such complexity is necessary. Recent findings in visual
perception suggest that foundation models, enabled by advanced pre-training,
allow simple yet flexible architectures to outperform specialized designs.
Therefore, in this work, we investigate an architecturally simple encoder-only
approach using plain Video Vision Transformers (Video ViTs) and study how
pre-training enables strong TAD performance. We find that: (i) strong
pre-training enables simple encoder-only models to match or even surpass the
performance of specialized state-of-the-art TAD methods, while also being
significantly more efficient; (ii) although weakly- and fully-supervised
pre-training are advantageous on standard benchmarks, we find them less
effective for TAD. Instead, self-supervised Masked Video Modeling (MVM)
provides the strongest signal; and (iii) Domain-Adaptive Pre-Training (DAPT) on
unlabeled driving videos further improves downstream performance, without
requiring anomalous examples. Our findings highlight the importance of
pre-training and show that effective, efficient, and scalable TAD models can be
built with minimal architectural complexity. We release our code,
domain-adapted encoders, and fine-tuned models to support future work:
https://github.com/tue-mps/simple-tad.

</details>


### [52] [Automated Multi-Class Crop Pathology Classification via Convolutional Neural Networks: A Deep Learning Approach for Real-Time Precision Agriculture](https://arxiv.org/abs/2507.09375)
*Sourish Suri,Yifei Shao*

Main category: cs.CV

TL;DR: 该研究开发了一种基于卷积神经网络（CNN）的图像分类系统，用于自动检测和分类八种常见农作物疾病，达到较高的训练准确性并与诊断结合的建议模块。


<details>
  <summary>Details</summary>
Motivation: 解决农作物病害早期识别不及时、不准确的问题，提升农业生产力和全球粮食安全。

Method: 利用深度学习管道，包括图像采集、预处理（大小调整、归一化、扩增）、基于TensorFlow与Keras的CNN建模，三层卷积层架构，附加治疗建议模块，并部署于开源手机平台。

Result: 模型的训练准确率约90%，验证准确率约60%（存在轻微过拟合），实现了实时诊断功能。

Conclusion: 通过将深度学习技术与实践农艺支持相结合，该系统为精确农业提供了可扩展和可访问的工具，有助于改善作物健康监测并增强全球粮食生产的韧性。

Abstract: Crop diseases present a significant barrier to agricultural productivity and
global food security, especially in large-scale farming where early
identification is often delayed or inaccurate. This research introduces a
Convolutional Neural Network (CNN)-based image classification system designed
to automate the detection and classification of eight common crop diseases
using leaf imagery. The methodology involves a complete deep learning pipeline:
image acquisition from a large, labeled dataset, preprocessing via resizing,
normalization, and augmentation, and model training using TensorFlow with
Keras' Sequential API. The CNN architecture comprises three convolutional
layers with increasing filter sizes and ReLU activations, followed by max
pooling, flattening, and fully connected layers, concluding with a softmax
output for multi-class classification. The system achieves high training
accuracy (~90%) and demonstrates reliable performance on unseen data, although
a validation accuracy of ~60% suggests minor overfitting. Notably, the model
integrates a treatment recommendation module, providing actionable guidance by
mapping each detected disease to suitable pesticide or fungicide interventions.
Furthermore, the solution is deployed on an open-source, mobile-compatible
platform, enabling real-time image-based diagnostics for farmers in remote
areas. This research contributes a scalable and accessible tool to the field of
precision agriculture, reducing reliance on manual inspection and promoting
sustainable disease management practices. By merging deep learning with
practical agronomic support, this work underscores the potential of CNNs to
transform crop health monitoring and enhance food production resilience on a
global scale.

</details>


### [53] [GreenCrossingAI: A Camera Trap/Computer Vision Pipeline for Environmental Science Research Groups](https://arxiv.org/abs/2507.09410)
*Bernie Boscoe,Shawn Johnson,Andrea Osborn,Chandler Campbell,Karen Mager*

Main category: cs.CV

TL;DR: 本文提出了一种低资源的工作流程，结合ML/AI能力，专为小型研究团队处理相机陷阱数据设计。


<details>
  <summary>Details</summary>
Motivation: 解决相机陷阱数据量庞大、标注困难、环境影响数据质量及ML/AI工具整合流程难等问题。

Method: 开发并提供了一种实用的低资源相机陷阱数据处理工作流程，结合了本地ML/AI能力，适合有限资源和计算能力的小型研究团队。

Result: 工作流程实现了数据传输、推断及评估的实际解决方案，帮助研究者从不断增长的数据集中提取有意义的洞察。

Conclusion: 该方法通过简化工具整合，降低了处理数据的资源门槛，为小型团队处理相机陷阱数据提供了切实可行的解决方案。

Abstract: Camera traps have long been used by wildlife researchers to monitor and study
animal behavior, population dynamics, habitat use, and species diversity in a
non-invasive and efficient manner. While data collection from the field has
increased with new tools and capabilities, methods to develop, process, and
manage the data, especially the adoption of ML/AI tools, remain challenging.
These challenges include the sheer volume of data generated, the need for
accurate labeling and annotation, variability in environmental conditions
affecting data quality, and the integration of ML/AI tools into existing
workflows that often require domain-specific customization and computational
resources. This paper provides a guide to a low-resource pipeline to process
camera trap data on-premise, incorporating ML/AI capabilities tailored for
small research groups with limited resources and computational expertise. By
focusing on practical solutions, the pipeline offers accessible approaches for
data transmission, inference, and evaluation, enabling researchers to discover
meaningful insights from their ever-increasing camera trap datasets.

</details>


### [54] [Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data](https://arxiv.org/abs/2507.09420)
*Timothy Chase Jr,Karthik Dantu*

Main category: cs.CV

TL;DR: 本文提出了一种用于行星表面地形特征检测与跟踪的新系统，结合轻量神经网络实现实时处理，在硬件有限的飞行处理器上表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统地形相对导航方法依赖大量的先验成像与离线处理，耗时且成本高，因此需要一种更高效、更通用的解决方案。

Method: 提出改进的领域自适应方法用于特点检测，并设计了一种注意力对齐机制实现地标描述，同时优化神经网络架构以适配飞行硬件设备。

Result: 该系统在地标检测与描述任务上优于现有的技术，同时可以实现实时性能。

Conclusion: 所提方法有效解决了当前空间飞行地形导航所面临的计算及训练数据稀缺问题，为航天任务提供了高效的技术支持。

Abstract: The detection and tracking of celestial surface terrain features are crucial
for autonomous spaceflight applications, including Terrain Relative Navigation
(TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data
collection. Traditional photoclinometry-based pipelines often rely on extensive
a priori imaging and offline processing, constrained by the computational
limitations of radiation-hardened systems. While historically effective, these
approaches typically increase mission costs and duration, operate at low
processing rates, and have limited generalization. Recently, learning-based
computer vision has gained popularity to enhance spacecraft autonomy and
overcome these limitations. While promising, emerging techniques frequently
impose computational demands exceeding the capabilities of typical spacecraft
hardware for real-time operation and are further challenged by the scarcity of
labeled training data for diverse extraterrestrial environments. In this work,
we present novel formulations for in-situ landmark tracking via detection and
description. We utilize lightweight, computationally efficient neural network
architectures designed for real-time execution on current-generation spacecraft
flight processors. For landmark detection, we propose improved domain
adaptation methods that enable the identification of celestial terrain features
with distinct, cheaply acquired training data. Concurrently, for landmark
description, we introduce a novel attention alignment formulation that learns
robust feature representations that maintain correspondence despite significant
landmark viewpoint variations. Together, these contributions form a unified
system for landmark tracking that demonstrates superior performance compared to
existing state-of-the-art techniques.

</details>


### [55] [Efficient Multi-Person Motion Prediction by Lightweight Spatial and Temporal Interactions](https://arxiv.org/abs/2507.09446)
*Yuanhong Zheng,Ruixuan Yu,Jian Sun*

Main category: cs.CV

TL;DR: 该研究提出了一种高效的多人物动作预测模型，专注于简化时空交互。


<details>
  <summary>Details</summary>
Motivation: 解决多人物动作预测中复杂的依赖关系和高计算成本问题。

Method: 设计轻量级双分支结构学习局部和全局表示，并引入跨层交互模块和空间距离嵌入以提升时空建模效率。

Result: 在CMU-Mocap、MuPoTS-3D和3DPW等标准数据集上达到了多项指标的最新性能，同时显著降低了计算成本。

Conclusion: 提出的模型在保证预测精度的同时，大幅度提升了计算效率，为多人物动作预测领域提供了一种高效的解决方案。

Abstract: 3D multi-person motion prediction is a highly complex task, primarily due to
the dependencies on both individual past movements and the interactions between
agents. Moreover, effectively modeling these interactions often incurs
substantial computational costs. In this work, we propose a computationally
efficient model for multi-person motion prediction by simplifying spatial and
temporal interactions. Our approach begins with the design of lightweight dual
branches that learn local and global representations for individual and
multiple persons separately. Additionally, we introduce a novel cross-level
interaction block to integrate the spatial and temporal representations from
both branches. To further enhance interaction modeling, we explicitly
incorporate the spatial inter-person distance embedding. With above efficient
temporal and spatial design, we achieve state-of-the-art performance for
multiple metrics on standard datasets of CMU-Mocap, MuPoTS-3D, and 3DPW, while
significantly reducing the computational cost. Code is available at
https://github.com/Yuanhong-Zheng/EMPMP.

</details>


### [56] [SegVec3D: A Method for Vector Embedding of 3D Objects Oriented Towards Robot manipulation](https://arxiv.org/abs/2507.09459)
*Zhihan Kang,Boyu Wang*

Main category: cs.CV

TL;DR: SegVec3D是一个创新的3D点云实例分割框架，结合注意力机制、嵌入学习和跨模态对齐，支持零样本检索和无监督实例分割。


<details>
  <summary>Details</summary>
Motivation: 解决当前3D点云任务多样性和语义理解局限性的问题，提供统一且高效的实例分割和多模态理解方法。

Method: 通过分层特征提取器实现几何结构建模，采用对比聚类进行无监督实例分割，同时在共享语义空间中对齐3D数据与自然语言查询。

Result: 相比于Mask3D和ULIP等方法，SegVec3D在统一实例分割和多模态任务上表现更为优越，并具备更少的监督需求和现实中的可部署性。

Conclusion: SegVec3D展示了在3D点云任务中的创新能力，结合语义对齐与无监督方法，为多样化的应用提供更强的适应性和性能。

Abstract: We propose SegVec3D, a novel framework for 3D point cloud instance
segmentation that integrates attention mechanisms, embedding learning, and
cross-modal alignment. The approach builds a hierarchical feature extractor to
enhance geometric structure modeling and enables unsupervised instance
segmentation via contrastive clustering. It further aligns 3D data with natural
language queries in a shared semantic space, supporting zero-shot retrieval.
Compared to recent methods like Mask3D and ULIP, our method uniquely unifies
instance segmentation and multimodal understanding with minimal supervision and
practical deployability.

</details>


### [57] [CKAA: Cross-subspace Knowledge Alignment and Aggregation for Robust Continual Learning](https://arxiv.org/abs/2507.09471)
*Lingfeng He,De Cheng,Zhiheng Ma,Huaijie Wang,Dingwen Zhang,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: 提出一种新的框架CKAA，通过知识对齐和适配器混合机制提高连续学习模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前PEFT方法在处理错误任务标识时易产生模糊决策。

Method: 通过提出双层知识对齐和任务置信度引导的适配器混合两种创新机制，增强模型的鲁棒性。

Result: 实验表明CKAA在性能上优于现有的PEFT类连续学习方法。

Conclusion: CKAA显著改善了模型在面对误导任务标识时的表现，展现出优于现有方法的潜力。

Abstract: Continual Learning (CL) empowers AI models to continuously learn from
sequential task streams. Recently, parameter-efficient fine-tuning (PEFT)-based
CL methods have garnered increasing attention due to their superior
performance. They typically allocate a unique sub-module for learning each
task, with a task recognizer to select the appropriate sub-modules for testing
images. However, due to the feature subspace misalignment from independently
trained sub-modules, these methods tend to produce ambiguous decisions under
misleading task-ids. To address this, we propose Cross-subspace Knowledge
Alignment and Aggregation (CKAA), a novel framework that enhances model
robustness against misleading task-ids through two key innovations: (1)
Dual-level Knowledge Alignment (DKA): By aligning intra-class feature
distributions across different subspaces and learning a robust global
classifier through a feature simulation process, DKA enables the model to
distinguish features from both correct and incorrect subspaces during training.
(2) Task-Confidence-guided Mixture of Adapters (TC-MoA): A robust inference
scheme that adaptively aggregates task-specific knowledge from relevant
sub-modules based on task-confidence scores, avoiding overconfidence in
misleading task-id predictions. Extensive experiments demonstrate that CKAA
outperforms existing PEFT-based CL methods.

</details>


### [58] [HMID-Net: An Exploration of Masked Image Modeling and Knowledge Distillation in Hyperbolic Space](https://arxiv.org/abs/2507.09487)
*Changli Wang,Fang Yin,Jiafeng Liu,Rui Wu*

Main category: cs.CV

TL;DR: 提出了一种新的方法HMID-Net，用于在双曲空间中更高效地捕捉和利用视觉-语义层次结构，同时在多个下游任务上超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 探索如何在双曲空间中更高效地训练模型以捕捉和利用视觉-语义层次结构。

Method: 提出了一种集成了遮掩图像建模（MIM）和知识蒸馏技术的HMID-Net模型，并设计了一种适用于双曲空间的蒸馏损失函数。

Result: 实验验证表明，在双曲空间中使用MIM和知识蒸馏技术能取得与欧几里得空间相当的成功，并在多个下游任务中显著优于现有模型（如MERU和CLIP）。

Conclusion: HMID-Net展示了在双曲空间中组合MIM和知识蒸馏技术的能力，能够有效捕捉视觉-语义层次结构，为后续的下游任务和模型开发提供了强有力的支持。

Abstract: Visual and semantic concepts are often structured in a hierarchical manner.
For instance, textual concept `cat' entails all images of cats. A recent study,
MERU, successfully adapts multimodal learning techniques from Euclidean space
to hyperbolic space, effectively capturing the visual-semantic hierarchy.
However, a critical question remains: how can we more efficiently train a model
to capture and leverage this hierarchy? In this paper, we propose the
\textit{Hyperbolic Masked Image and Distillation Network} (HMID-Net), a novel
and efficient method that integrates Masked Image Modeling (MIM) and knowledge
distillation techniques within hyperbolic space. To the best of our knowledge,
this is the first approach to leverage MIM and knowledge distillation in
hyperbolic space to train highly efficient models. In addition, we introduce a
distillation loss function specifically designed to facilitate effective
knowledge transfer in hyperbolic space. Our experiments demonstrate that MIM
and knowledge distillation techniques in hyperbolic space can achieve the same
remarkable success as in Euclidean space. Extensive evaluations show that our
method excels across a wide range of downstream tasks, significantly
outperforming existing models like MERU and CLIP in both image classification
and retrieval.

</details>


### [59] [GLIMPSE: Do Large Vision-Language Models Truly Think With Videos or Just Glimpse at Them?](https://arxiv.org/abs/2507.09491)
*Yiyang Zhou,Linjie Li,Shi Qiu,Zhengyuan Yang,Yuyang Zhao,Siwei Han,Yangfan He,Kangqi Li,Haonian Ji,Zihao Zhao,Haibo Tong,Lijuan Wang,Huaxiu Yao*

Main category: cs.CV

TL;DR: 提出GLIMPSE基准，用于评估大型视觉语言模型(LVLMs)是否能在视频中进行深度时间推理，而非浅层帧级分析。


<details>
  <summary>Details</summary>
Motivation: 目前视频基准主要测试浅层帧级分析，无法反映LVLMs是否能真正对视频内容进行深度理解。

Method: 设计了GLIMPSE基准，包括3269个视频及4342个视觉中心问题，涵盖11类主题，问题需结合视频全貌进行理解和推理。

Result: 人类评估的准确率为94.82%，而当前LVLMs表现较差，最佳模型GPT-o3仅达66.43%。

Conclusion: GLIMPSE揭示了LVLMs在深度时间推理和全面理解视频内容方面的局限性，需进一步提升。

Abstract: Existing video benchmarks often resemble image-based benchmarks, with
question types like "What actions does the person perform throughout the
video?" or "What color is the woman's dress in the video?" For these, models
can often answer by scanning just a few key frames, without deep temporal
reasoning. This limits our ability to assess whether large vision-language
models (LVLMs) can truly think with videos rather than perform superficial
frame-level analysis. To address this, we introduce GLIMPSE, a benchmark
specifically designed to evaluate whether LVLMs can genuinely think with
videos. Unlike prior benchmarks, GLIMPSE emphasizes comprehensive video
understanding beyond static image cues. It consists of 3,269 videos and over
4,342 highly visual-centric questions across 11 categories, including
Trajectory Analysis, Temporal Reasoning, and Forensics Detection. All questions
are carefully crafted by human annotators and require watching the entire video
and reasoning over full video context-this is what we mean by thinking with
video. These questions cannot be answered by scanning selected frames or
relying on text alone. In human evaluations, GLIMPSE achieves 94.82% accuracy,
but current LVLMs face significant challenges. Even the best-performing model,
GPT-o3, reaches only 66.43%, highlighting that LVLMs still struggle to move
beyond surface-level reasoning to truly think with videos.

</details>


### [60] [SDTN and TRN: Adaptive Spectral-Spatial Feature Extraction for Hyperspectral Image Classification](https://arxiv.org/abs/2507.09492)
*Fuyin Ye,Erwen Yao,Jianyong Chen,Fengmei He,Junxiang Zhang,Lihao Ni*

Main category: cs.CV

TL;DR: 本文提出了一种结合张量分解和正则化机制的自适应张量正则化网络（SDTN）用于高光谱图像分类，解决传统方法在高维数据处理上的不足，并进一步构造了TRN实现高效的多尺度光谱-空间特征提取。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类在精密农业中至关重要，但受限于高维数据、光谱-空间冗余以及标签样本稀缺等问题，传统方法难以提供高效表现。

Method: 提出自适应张量正则化网络(SDTN)，结合张量分解与正则化机制动态调整张量秩，并进一步开发轻量化网络TRN捕获多尺度光谱-空间特征。

Result: 实验表明，SDTN和TRN方法在PaviaU等数据集上显著提升了分类精度，同时大幅减少模型参数，与现有方法相比具有竞争力。

Conclusion: 所提方法不仅提高分类性能，还显著降低计算复杂度，使其非常适合资源受限环境中的实时应用。

Abstract: Hyperspectral image classification plays a pivotal role in precision
agriculture, providing accurate insights into crop health monitoring, disease
detection, and soil analysis. However, traditional methods struggle with
high-dimensional data, spectral-spatial redundancy, and the scarcity of labeled
samples, often leading to suboptimal performance. To address these challenges,
we propose the Self-Adaptive Tensor- Regularized Network (SDTN), which combines
tensor decomposition with regularization mechanisms to dynamically adjust
tensor ranks, ensuring optimal feature representation tailored to the
complexity of the data. Building upon SDTN, we propose the Tensor-Regularized
Network (TRN), which integrates the features extracted by SDTN into a
lightweight network capable of capturing spectral-spatial features at multiple
scales. This approach not only maintains high classification accuracy but also
significantly reduces computational complexity, making the framework highly
suitable for real-time deployment in resource-constrained environments.
Experiments on PaviaU datasets demonstrate significant improvements in accuracy
and reduced model parameters compared to state-of-the-art methods.

</details>


### [61] [Advancing Reliable Test-Time Adaptation of Vision-Language Models under Visual Variations](https://arxiv.org/abs/2507.09500)
*Yiwen Liang,Hui Chen,Yizhe Xiong,Zihan Zhou,Mengyao Lyu,Zijia Lin,Shuaicheng Niu,Sicheng Zhao,Jungong Han,Guiguang Ding*

Main category: cs.CV

TL;DR: 提出一种名为ReTA的新方法，旨在解决现有测试时自适应（TTA）方法在分布变化下的可靠性问题，改善视觉语言模型在无标注数据情况下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在面对分布偏移时性能下降，而当前的TTA方法在无标注情况下尚存可靠性问题，如熵的不稳定性和决策边界的刚性，亟需解决。

Method: 引入了一种可靠的测试时自适应方法ReTA，其中包含两大核心策略：（1）一致性权重熵方法（CER），通过引入一致性约束优化样本选择；（2）多样性驱动的分布校准方法（DDC），利用高斯分布模型动态调整决策边界。

Result: ReTA方法通过实验验证，能够在应对真实世界分布偏移时，性能显著优于现有最前沿方法。

Conclusion: ReTA通过改善样本缓存和决策边界，为视觉语言模型测试时适应提供了一种兼具可靠性和适应性的新解决方案。

Abstract: Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but
struggle with distribution shifts in downstream tasks when labeled data is
unavailable, which has motivated the development of Test-Time Adaptation (TTA)
to improve VLMs' performance during inference without annotations. Among
various TTA approaches, cache-based methods show promise by preserving
historical knowledge from low-entropy samples in a dynamic cache and fostering
efficient adaptation. However, these methods face two critical reliability
challenges: (1) entropy often becomes unreliable under distribution shifts,
causing error accumulation in the cache and degradation in adaptation
performance; (2) the final predictions may be unreliable due to inflexible
decision boundaries that fail to accommodate large downstream shifts. To
address these challenges, we propose a Reliable Test-time Adaptation (ReTA)
method that integrates two complementary strategies to enhance reliability from
two perspectives. First, to mitigate the unreliability of entropy as a sample
selection criterion for cache construction, we introduce Consistency-aware
Entropy Reweighting (CER), which incorporates consistency constraints to weight
entropy during cache updating. While conventional approaches rely solely on low
entropy for cache prioritization and risk introducing noise, our method
leverages predictive consistency to maintain a high-quality cache and
facilitate more robust adaptation. Second, we present Diversity-driven
Distribution Calibration (DDC), which models class-wise text embeddings as
multivariate Gaussian distributions, enabling adaptive decision boundaries for
more accurate predictions across visually diverse content. Extensive
experiments demonstrate that ReTA consistently outperforms state-of-the-art
methods, particularly under challenging real-world distribution shifts.

</details>


### [62] [Online Micro-gesture Recognition Using Data Augmentation and Spatial-Temporal Attention](https://arxiv.org/abs/2507.09512)
*Pengyu Liu,Kun Li,Fei Wang,Yanyan Wei,Junhui She,Dan Guo*

Main category: cs.CV

TL;DR: 提出了HFUT-VUT团队为IJCAI 2025 MiGA挑战赛微手势在线识别赛道开发的最新解决方案，采用数据增强和时空注意力机制，取得了38.03的F1分数并排名第一。


<details>
  <summary>Details</summary>
Motivation: 解决微手势在线识别中定位和分类的挑战，特别是微手势比传统动作检测任务中动作差异更大更难以区分。

Method: 提出基于手工设计的数据增强方法和时空注意力机制，以提升模型对微手势实例分类和定位的精度。

Result: 模型F1分数达到38.03，相较之前的最佳成果提升37.9%，并在赛事中排名第一。

Conclusion: 通过数据增强与时空注意力机制的结合成功实现了微手势在线识别性能的突破。

Abstract: In this paper, we introduce the latest solution developed by our team,
HFUT-VUT, for the Micro-gesture Online Recognition track of the IJCAI 2025 MiGA
Challenge. The Micro-gesture Online Recognition task is a highly challenging
problem that aims to locate the temporal positions and recognize the categories
of multiple micro-gesture instances in untrimmed videos. Compared to
traditional temporal action detection, this task places greater emphasis on
distinguishing between micro-gesture categories and precisely identifying the
start and end times of each instance. Moreover, micro-gestures are typically
spontaneous human actions, with greater differences than those found in other
human actions. To address these challenges, we propose hand-crafted data
augmentation and spatial-temporal attention to enhance the model's ability to
classify and localize micro-gestures more accurately. Our solution achieved an
F1 score of 38.03, outperforming the previous state-of-the-art by 37.9%. As a
result, our method ranked first in the Micro-gesture Online Recognition track.

</details>


### [63] [QuarterMap: Efficient Post-Training Token Pruning for Visual State Space Models](https://arxiv.org/abs/2507.09514)
*Tien-Yu Chi,Hung-Yueh Chiang,Diana Marculescu,Kai-Chiang Wu*

Main category: cs.CV

TL;DR: QuarterMap是一种针对SSMs的后训练激活剪枝方法，可在不重新训练的情况下提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决VMamba视觉骨干中由于四方向扫描导致的空间冗余问题，以提高模型的效率。

Method: 提出QuarterMap，通过剪除冗余的空间激活，并采用最近邻上采样恢复维度来提高效率，无需重新训练模型。

Result: 在ImageNet-1K上实现最高11%的速度提升，同时准确率损失不足0.9%；在ADE20K分割任务和多种医学影像任务中表现出相似的提升效果。

Conclusion: QuarterMap提升了SSMs在推理阶段的效率，是一种无需折衷迁移性且部署友好型的方法。

Abstract: State space models (SSMs) reduce the quadratic complexity of transformers by
leveraging linear recurrence. Recently, VMamba has emerged as a strong
SSM-based vision backbone, yet remains bottlenecked by spatial redundancy in
its four-directional scan. We propose QuarterMap, a post-training activation
pruning method that removes redundant spatial activations before scanning and
restores dimensions via nearest-neighbor upsampling. Our method improves
throughput without retraining. On ImageNet-1K, QuarterMap achieves up to 11%
speedup on VMamba with less than 0.9% accuracy drop, and yields similar gains
on ADE20K segmentation. Beyond VMamba, we validate QuarterMap on MedMamba, a
domain-specific model that shares the same four-directional scanning structure,
where it consistently improves throughput while preserving accuracy across
multiple medical imaging tasks. Compared to token merging methods like ToMe,
QuarterMap is tailored for SSMs and avoids costly merge-unmerge operations. Our
method offers a plug-and-play tool for deployment-time efficiency without
compromising transferability.

</details>


### [64] [When Schrödinger Bridge Meets Real-World Image Dehazing with Unpaired Training](https://arxiv.org/abs/2507.09524)
*Yunwei Lan,Zhigao Cui,Xin Luo,Chang Liu,Nian Wang,Menglin Zhang,Yanzhao Su,Dong Liu*

Main category: cs.CV

TL;DR: 本论文提出了一种基于Schrödinger Bridge的清晰-模糊图像分布映射的无配对除雾框架DehazeSB，通过结合细节保持正则化和提示学习优化了雾天图像处理性能。


<details>
  <summary>Details</summary>
Motivation: 传统GAN方法在无配对的训练模式中由于生成器的有限传输映射能力，未能充分发挥其性能效果。

Method: 提出一种基于Schrödinger Bridge的框架，利用最佳传输理论实现清晰与模糊图像分布间的高效映射，同时引入细节保持正则化和提示学习，使其在模糊与清晰图像之间的处理能力更佳。

Result: 实验表明，DehazeSB在多组真实数据集上优于现有的无配对除雾方法，在生成质量和细节保留方面有显著提升。

Conclusion: 本方法有效解决了无配对除雾中现存的性能瓶颈，能够提供更高质量的图像还原效果。

Abstract: Recent advancements in unpaired dehazing, particularly those using GANs, show
promising performance in processing real-world hazy images. However, these
methods tend to face limitations due to the generator's limited transport
mapping capability, which hinders the full exploitation of their effectiveness
in unpaired training paradigms. To address these challenges, we propose
DehazeSB, a novel unpaired dehazing framework based on the Schr\"odinger
Bridge. By leveraging optimal transport (OT) theory, DehazeSB directly bridges
the distributions between hazy and clear images. This enables optimal transport
mappings from hazy to clear images in fewer steps, thereby generating
high-quality results. To ensure the consistency of structural information and
details in the restored images, we introduce detail-preserving regularization,
which enforces pixel-level alignment between hazy inputs and dehazed outputs.
Furthermore, we propose a novel prompt learning to leverage pre-trained CLIP
models in distinguishing hazy images and clear ones, by learning a haze-aware
vision-language alignment. Extensive experiments on multiple real-world
datasets demonstrate our method's superiority. Code:
https://github.com/ywxjm/DehazeSB.

</details>


### [65] [VDInstruct: Zero-Shot Key Information Extraction via Content-Aware Vision Tokenization](https://arxiv.org/abs/2507.09531)
*Son Nguyen,Giang Nguyen,Hung Dao,Thao Do,Daeyoung Kim*

Main category: cs.CV

TL;DR: 本文提出了VDInstruct，一种用于视觉文档理解的多模态大语言模型，通过内容感知的分块策略优化了图像标记效率并实现了最新的关键信息提取（KIE）性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在处理密集文档时表现不佳，并且其依赖与图像大小成比例的视觉标记方法，导致过多计算成本和低效的内存使用。因此需要一种更加高效且精确的方法来提升文档理解能力。

Method: VDInstruct通过将空间区域检测和语义特征提取分离，采用内容感知的标记策略，按文档复杂性生成标记，从而保留关键结构并消除多余标记；并通过三阶段的训练范式提升模型性能。

Result: 实验表明，VDInstruct在关键信息提取（KIE）基准测试中取得了最新的成果，标记数量减少了约3.6倍。而在零样本评估中，相较于DocOwl 1.5提高了约+5.5个F1分数，展示了对未见文档的出色鲁棒性。

Conclusion: 基于内容感知的标记方法和明确的布局建模为文档理解提供了一种有前途的方向，且相关数据、源码与模型权重将公开，以供社区使用。

Abstract: Key Information Extraction (KIE) underpins the understanding of visual
documents (e.g., receipts and contracts) by extracting precise semantic content
and accurately capturing spatial structure. Yet existing multimodal large
language models (MLLMs) often perform poorly on dense documents and rely on
vision tokenization approaches that scale with image size, leading to redundant
computation and memory inefficiency. To address these challenges, we introduce
VDInstruct, an MLLM that separates spatial region detection from semantic
feature extraction. Central to our model is a content-aware tokenization
strategy: rather than fragmenting the entire image uniformly, it generates
tokens in proportion to document complexity, preserving critical structure
while eliminating wasted tokens. Leveraging a three-stage training paradigm,
our model achieves state-of-the-art (SOTA) results on KIE benchmarks, matching
or exceeding the accuracy of leading approaches while reducing the number of
image tokens by roughly 3.6x. In zero-shot evaluations, VDInstruct surpasses
strong baselines-such as DocOwl 1.5-by +5.5 F1 points, highlighting its
robustness to unseen documents. These findings show that content-aware
tokenization combined with explicit layout modeling offers a promising
direction forward for document understanding. Data, source code, and model
weights will be made publicly available.

</details>


### [66] [DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection](https://arxiv.org/abs/2507.09541)
*Zihao Xiong,Fei Zhou,Fengyi Wu,Shuai Yuan,Maixia Fu,Zhenming Peng,Jian Yang,Yimian Dai*

Main category: cs.CV

TL;DR: 本文提出了动态鲁棒主成分分析网络（DRPCA-Net），结合稀疏性先验与深度展开网络用于红外小目标检测，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前的深度学习模型在红外小目标检测中普遍追求性能，忽略了模型可解释性、参数效率和泛化性，同时未有效利用红外小目标的固有稀疏性先验。

Method: 设计了动态鲁棒主成分分析网络（DRPCA-Net），基于稀疏先验并采用动态展开机制，通过轻量级超网络生成与输入场景相关的迭代参数，同时引入动态残差组模块以提升背景估计与目标分离能力。

Result: 在多个公开红外数据集上，DRPCA-Net在检测精度方面显著优于现有先进方法。

Conclusion: DRPCA-Net结合模型化先验和深度学习方法，实现了红外小目标检测的精度与效率的新突破，为实际应用带来了更高的鲁棒性和泛化能力。

Abstract: Infrared small target detection plays a vital role in remote sensing,
industrial monitoring, and various civilian applications. Despite recent
progress powered by deep learning, many end-to-end convolutional models tend to
pursue performance by stacking increasingly complex architectures, often at the
expense of interpretability, parameter efficiency, and generalization. These
models typically overlook the intrinsic sparsity prior of infrared small
targets--an essential cue that can be explicitly modeled for both performance
and efficiency gains. To address this, we revisit the model-based paradigm of
Robust Principal Component Analysis (RPCA) and propose Dynamic RPCA Network
(DRPCA-Net), a novel deep unfolding network that integrates the sparsity-aware
prior into a learnable architecture. Unlike conventional deep unfolding methods
that rely on static, globally learned parameters, DRPCA-Net introduces a
dynamic unfolding mechanism via a lightweight hypernetwork. This design enables
the model to adaptively generate iteration-wise parameters conditioned on the
input scene, thereby enhancing its robustness and generalization across diverse
backgrounds. Furthermore, we design a Dynamic Residual Group (DRG) module to
better capture contextual variations within the background, leading to more
accurate low-rank estimation and improved separation of small targets.
Extensive experiments on multiple public infrared datasets demonstrate that
DRPCA-Net significantly outperforms existing state-of-the-art methods in
detection accuracy. Code is available at https://github.com/GrokCV/DRPCA-Net.

</details>


### [67] [SeqCSIST: Sequential Closely-Spaced Infrared Small Target Unmixing](https://arxiv.org/abs/2507.09556)
*Ximeng Zhai,Bohan Xu,Yaohong Chen,Hao Wang,Kehua Guo,Yimian Dai*

Main category: cs.CV

TL;DR: 提出了一种名为Sequential CSIST Unmixing的新任务，旨在解决远距离高密度红外小目标混合预测问题，提出了数据集SeqCSIST和工具集，并设计了DeRefNet框架，实验表明性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的光学镜头焦距和红外探测分辨率限制了远距离高密度红外小目标（CSIST）混合目标的精确检测能力。

Method: 建立了一个名为SeqCSIST的开放数据集及评估工具，提出了一种基于时序多帧视角的深度学习模型DeRefNet，引入了TDFA模块实现自适应帧间信息对齐。

Result: 在SeqCSIST数据集上的实验结果显示，新方法在mAP指标上相比现有方法提升了5.3%。

Conclusion: 这是首次在多帧范式下解决CSIST Unmixing问题的研究，提出的方法证明是有效的，并提供了相关数据和工具支持社区进一步研究。

Abstract: Due to the limitation of the optical lens focal length and the resolution of
the infrared detector, distant Closely-Spaced Infrared Small Target (CSIST)
groups typically appear as mixing spots in the infrared image. In this paper,
we propose a novel task, Sequential CSIST Unmixing, namely detecting all
targets in the form of sub-pixel localization from a highly dense CSIST group.
However, achieving such precise detection is an extremely difficult challenge.
In addition, the lack of high-quality public datasets has also restricted the
research progress. To this end, firstly, we contribute an open-source
ecosystem, including SeqCSIST, a sequential benchmark dataset, and a toolkit
that provides objective evaluation metrics for this special task, along with
the implementation of 23 relevant methods. Furthermore, we propose the
Deformable Refinement Network (DeRefNet), a model-driven deep learning
framework that introduces a Temporal Deformable Feature Alignment (TDFA) module
enabling adaptive inter-frame information aggregation. To the best of our
knowledge, this work is the first endeavor to address the CSIST Unmixing task
within a multi-frame paradigm. Experiments on the SeqCSIST dataset demonstrate
that our method outperforms the state-of-the-art approaches with mean Average
Precision (mAP) metric improved by 5.3\%. Our dataset and toolkit are available
from https://github.com/GrokCV/SeqCSIST.

</details>


### [68] [EHPE: A Segmented Architecture for Enhanced Hand Pose Estimation](https://arxiv.org/abs/2507.09560)
*Bolun Zheng,Xinjie Liu,Qianyu Zhang,Canjin Wang,Fangni Chen,Mingen Xu*

Main category: cs.CV

TL;DR: 這篇文章提出了一種新型的分段架構，旨在提升3D手部姿態估計的準確性，特別針對Distal Phalanx Tip (TIP)和手腕的誤差積累問題進行解決。


<details>
  <summary>Details</summary>
Motivation: 現有方法對於手部姿態估計中的TIP和手腕的精確性重視不足，且未能很好地解決遠端關節的誤差累積問題，導致姿態估計結果不準確。

Method: 提出EHPE分段架構，通過兩個階段進行優化：1.TW-stage，對TIP和手腕位置進行初步提取以減少誤差；2.PG-stage，透過雙分支互動網絡對其餘關節位置進一步精煉。

Result: 在兩個廣泛使用的基準上進行實驗表明，EHPE達到了最先進的性能水準。

Conclusion: 該方法有效緩解了TIP和手腕誤差累積問題，進一步降低了所有關節的預測誤差，提升了手部姿態重建的整體質量。

Abstract: 3D hand pose estimation has garnered great attention in recent years due to
its critical applications in human-computer interaction, virtual reality, and
related fields. The accurate estimation of hand joints is essential for
high-quality hand pose estimation. However, existing methods neglect the
importance of Distal Phalanx Tip (TIP) and Wrist in predicting hand joints
overall and often fail to account for the phenomenon of error accumulation for
distal joints in gesture estimation, which can cause certain joints to incur
larger errors, resulting in misalignments and artifacts in the pose estimation
and degrading the overall reconstruction quality. To address this challenge, we
propose a novel segmented architecture for enhanced hand pose estimation
(EHPE). We perform local extraction of TIP and wrist, thus alleviating the
effect of error accumulation on TIP prediction and further reduce the
predictive errors for all joints on this basis. EHPE consists of two key
stages: In the TIP and Wrist Joints Extraction stage (TW-stage), the positions
of the TIP and wrist joints are estimated to provide an initial accurate joint
configuration; In the Prior Guided Joints Estimation stage (PG-stage), a
dual-branch interaction network is employed to refine the positions of the
remaining joints. Extensive experiments on two widely used benchmarks
demonstrate that EHPE achieves state-of-the-arts performance. Code is available
at https://github.com/SereinNout/EHPE.

</details>


### [69] [Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges](https://arxiv.org/abs/2507.09562)
*Yidong Jiang*

Main category: cs.CV

TL;DR: 本文综述了促使Segment Anything Model (SAM)成功的prompt engineering技巧，涵盖其方法、应用及主要挑战。


<details>
  <summary>Details</summary>
Motivation: 通过综合分析，发现prompt engineering对SAM成功起着关键作用，但当前尚未被深入研究，因此该文旨在填补这一领域的空白。

Method: 对现有文献进行系统梳理，涵盖从简单几何输入到复杂多模态方法的进化过程，同时分析在不同领域中的应用和优化挑战。

Result: 揭示了prompt engineering的演变过程，识别了在医疗影像和遥感等领域的独特挑战，并提出未来的研究方向。

Conclusion: 该文提供了一个系统框架，为理解和推进基础模型领域中的prompt engineering研究奠定了基础。

Abstract: The Segment Anything Model (SAM) has revolutionized image segmentation
through its innovative prompt-based approach, yet the critical role of prompt
engineering in its success remains underexplored. This paper presents the first
comprehensive survey focusing specifically on prompt engineering techniques for
SAM and its variants. We systematically organize and analyze the rapidly
growing body of work in this emerging field, covering fundamental
methodologies, practical applications, and key challenges. Our review reveals
how prompt engineering has evolved from simple geometric inputs to
sophisticated multimodal approaches, enabling SAM's adaptation across diverse
domains including medical imaging and remote sensing. We identify unique
challenges in prompt optimization and discuss promising research directions.
This survey fills an important gap in the literature by providing a structured
framework for understanding and advancing prompt engineering in foundation
models for segmentation.

</details>


### [70] [MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models](https://arxiv.org/abs/2507.09574)
*Haozhe Zhao,Zefan Cai,Shuzheng Si,Liang Chen,Jiuxiang Gu,Wen Xiao,Junjie Hu*

Main category: cs.CV

TL;DR: MENTOR提出了一种新颖的自回归框架，提高了多模态图像生成的精确度和控制能力。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现有文本生成图像模型在精确视觉控制和多模态输入平衡上的不足，同时减轻复杂任务所需的大量训练资源。

Method: 开发了自回归生成器结合两阶段训练方法：阶段一是多模态对齐，阶段二是多模态指令微调，无需使用额外的适配器或交叉注意模块。

Result: 在DreamBench++基准上表现优异，超越竞争对手，同时在图像重建保真度、多任务适应性和训练效率上取得进步。

Conclusion: 在资源有限的情况下，提出的MENTOR框架实现了高效且精确的多模态图像生成，并公开了数据集、代码和模型。

Abstract: Recent text-to-image models produce high-quality results but still struggle
with precise visual control, balancing multimodal inputs, and requiring
extensive training for complex multimodal image generation. To address these
limitations, we propose MENTOR, a novel autoregressive (AR) framework for
efficient Multimodal-conditioned Tuning for Autoregressive multimodal image
generation. MENTOR combines an AR image generator with a two-stage training
paradigm, enabling fine-grained, token-level alignment between multimodal
inputs and image outputs without relying on auxiliary adapters or
cross-attention modules. The two-stage training consists of: (1) a multimodal
alignment stage that establishes robust pixel- and semantic-level alignment,
followed by (2) a multimodal instruction tuning stage that balances the
integration of multimodal inputs and enhances generation controllability.
Despite modest model size, suboptimal base components, and limited training
resources, MENTOR achieves strong performance on the DreamBench++ benchmark,
outperforming competitive baselines in concept preservation and prompt
following. Additionally, our method delivers superior image reconstruction
fidelity, broad task adaptability, and improved training efficiency compared to
diffusion-based methods. Dataset, code, and models are available at:
https://github.com/HaozheZhao/MENTOR

</details>


### [71] [WordCraft: Interactive Artistic Typography with Attention Awareness and Noise Blending](https://arxiv.org/abs/2507.09573)
*Zhe Wang,Jingbo Zhang,Tianyi Wei,Wanchao Su,Can Wang*

Main category: cs.CV

TL;DR: WordCraft系统通过整合扩散模型，大幅提升艺术字体合成的互动性与创造性。


<details>
  <summary>Details</summary>
Motivation: 目前艺术字体生成方法在交互性和多样性方面存在不足，如缺乏局部编辑、迭代完善、多字符组合和开放式提示解析。

Method: 提出了WordCraft系统，它结合了无训练的区域注意机制和噪声混合技术，实现多区域精准生成与连续优化，并通过大语言模型解析用户提示以生成灵活字体。

Result: 系统能够生成高质量的多语言、多字符风格化字体，支持多种用户需求。

Conclusion: WordCraft提升了艺术字体生成的交互性和灵活性，为艺术家和设计师提供了更多创意空间。

Abstract: Artistic typography aims to stylize input characters with visual effects that
are both creative and legible. Traditional approaches rely heavily on manual
design, while recent generative models, particularly diffusion-based methods,
have enabled automated character stylization. However, existing solutions
remain limited in interactivity, lacking support for localized edits, iterative
refinement, multi-character composition, and open-ended prompt interpretation.
We introduce WordCraft, an interactive artistic typography system that
integrates diffusion models to address these limitations. WordCraft features a
training-free regional attention mechanism for precise, multi-region generation
and a noise blending that supports continuous refinement without compromising
visual quality. To support flexible, intent-driven generation, we incorporate a
large language model to parse and structure both concrete and abstract user
prompts. These components allow our framework to synthesize high-quality,
stylized typography across single- and multi-character inputs across multiple
languages, supporting diverse user-centered workflows. Our system significantly
enhances interactivity in artistic typography synthesis, opening up creative
possibilities for artists and designers.

</details>


### [72] [Memory-Augmented SAM2 for Training-Free Surgical Video Segmentation](https://arxiv.org/abs/2507.09577)
*Ming Yin,Fu Wang,Xujiong Ye,Yanda Meng,Zeyu Fu*

Main category: cs.CV

TL;DR: 提出MA-SAM2，一种无需训练的视频对象分割策略，解决SAM2在外科手术视频中表现受限的问题，并在现有数据集上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前SAM2在外科手术视频分割中由于记忆设计的局限性，在处理复杂、长时视频时表现性能下降。

Method: 提出Memory Augmented (MA)-SAM2策略，通过引入上下文感知和对遮挡具有更强鲁棒性的记忆模型，同时采用多目标、单循环、一提示的推理方式提升效率。

Result: MA-SAM2无需增加任何额外参数或训练，在EndoVis2017和EndoVis2018数据集上的性能分别提高了4.36%和6.1%。

Conclusion: MA-SAM2在复杂的外科手术视频分割任务中表现出显著优势，具有潜在的实际外科应用前景。

Abstract: Surgical video segmentation is a critical task in computer-assisted surgery,
essential for enhancing surgical quality and patient outcomes. Recently, the
Segment Anything Model 2 (SAM2) framework has demonstrated remarkable
advancements in both image and video segmentation. However, the inherent
limitations of SAM2's greedy selection memory design are amplified by the
unique properties of surgical videos-rapid instrument movement, frequent
occlusion, and complex instrument-tissue interaction-resulting in diminished
performance in the segmentation of complex, long videos. To address these
challenges, we introduce Memory Augmented (MA)-SAM2, a training-free video
object segmentation strategy, featuring novel context-aware and
occlusion-resilient memory models. MA-SAM2 exhibits strong robustness against
occlusions and interactions arising from complex instrument movements while
maintaining accuracy in segmenting objects throughout videos. Employing a
multi-target, single-loop, one-prompt inference further enhances the efficiency
of the tracking process in multi-instrument videos. Without introducing any
additional parameters or requiring further training, MA-SAM2 achieved
performance improvements of 4.36% and 6.1% over SAM2 on the EndoVis2017 and
EndoVis2018 datasets, respectively, demonstrating its potential for practical
surgical applications.

</details>


### [73] [ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models](https://arxiv.org/abs/2507.09876)
*Yongheng Zhang,Xu Liu,Ruihan Tao,Qiguang Chen,Hao Fei,Wanxiang Che,Libo Qin*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频推理范式ViTCoT，与传统的文本推理方法相比，更加贴近认知逻辑，实验表明其性能显著提高。


<details>
  <summary>Details</summary>
Motivation: 当前视频推理方法过于依赖文本信息，忽视了视觉信息的使用，与人类自然的视觉再检查过程不一致。

Method: 提出了ViTCoT视频推理范式，并构建了相关基准ViTIB，利用多模态语言模型(MLLMs)进行关键视频选择，并经过人工验证。

Result: 实验结果表明ViTCoT相比传统的方法能够显著提高推理性能，并激活更多的MLLM神经元值。

Conclusion: 通过整合视觉与文本信息，ViTCoT提供了一种更直观和符合认知逻辑的视频推理方法，在该领域展现了巨大的潜力。

Abstract: Video understanding plays a vital role in bridging low-level visual signals
with high-level cognitive reasoning, and is fundamental to applications such as
autonomous driving, embodied AI, and the broader pursuit of AGI. The rapid
development of large language models (LLMs), particularly those utilizing
Chain-of-Thought (CoT) technology, has significantly advanced video reasoning
capabilities. However, current approaches primarily depend on textual
information for reasoning, overlooking the visual modality in the actual video
reasoning process. In contrast, humans naturally re-examine visual content
while reasoning. Motivated by this, we introduce a novel video reasoning
paradigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive
and cognitively aligned reasoning. To the end, first, we construct the
Video-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for
key-video selection and manually verified. Furthermore, we extensively explore
the potential of the ViTCoT paradigm in the video understanding field.
Extensive experiments demonstrate that ViTCoT significantly enhances
performance compared to the traditional text-only CoT paradigm and effectively
activates more neuron values in MLLMs.

</details>


### [74] [Demystifying Flux Architecture](https://arxiv.org/abs/2507.09595)
*Or Greenberg*

Main category: cs.CV

TL;DR: FLUX.1是一个基于扩散技术的文本到图像生成模型，被认为在该领域表现优异，但官方未发布技术文档。本报告通过逆向工程分析其架构。


<details>
  <summary>Details</summary>
Motivation: 弥补FLUX.1官方技术文档缺失，促进其作为未来研究基础的应用。

Method: 通过对公开的源代码进行逆向工程，解析FLUX.1的架构和训练细节。

Result: 报告总结了FLUX.1的架构和技术特性，为研究人员提供支持。

Conclusion: 尽管FLUX.1表现卓越，但官方缺乏相关文档。本报告通过逆向工程提供了有价值的技术细节。

Abstract: FLUX.1 is a diffusion-based text-to-image generation model developed by Black
Forest Labs, designed to achieve faithful text-image alignment while
maintaining high image quality and diversity. FLUX is considered
state-of-the-art in text-to-image generation, outperforming popular models such
as Midjourney, DALL-E 3, Stable Diffusion 3 (SD3), and SDXL. Although publicly
available as open source, the authors have not released official technical
documentation detailing the model's architecture or training setup. This report
summarizes an extensive reverse-engineering effort aimed at demystifying FLUX's
architecture directly from its source code, to support its adoption as a
backbone for future research and development. This document is an unofficial
technical report and is not published or endorsed by the original developers or
their affiliated institutions.

</details>


### [75] [Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision Interactive](https://arxiv.org/abs/2507.09612)
*You Huang,Lichao Chen,Jiayi Ji,Liujuan Cao,Shengchuan Zhang,Rongrong Ji*

Main category: cs.CV

TL;DR: 本文提出Inter2Former模型，通过优化稠密token处理分配，提升交互式分割的效率和精度，尤其在CPU设备上实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 现有交互式分割方法在稠密token计算的高精度与稀疏token计算的效率之间存在权衡问题，需要一种既高效又精确的解决方案。

Method: 提出四项关键改进，包括动态提示嵌入（DPE）、动态混合注意力（DHA）、混合专家模块（HMoE）和动态局部上采样（DLU），以优化稠密token处理的计算分配。

Result: 实验结果表明，Inter2Former在高精度交互式分割基准测试中性能优异，并在CPU设备上实现了行业领先的效率。

Conclusion: Inter2Former能够有效平衡分割精度与计算效率，为CPU设备上的交互式分割提供了高效解决方案。

Abstract: Interactive segmentation (IS) improves annotation efficiency by segmenting
target regions from user prompts, with widespread applications in real-world
scenarios. Current approaches face a critical trade-off: dense-token methods
achieve superior accuracy and detail preservation but suffer from prohibitively
slow processing on CPU devices, while the Segment Anything Model (SAM) advances
the field with sparse prompt tokens for fast inference but compromises
segmentation quality. In this paper, we propose Inter2Former to address this
challenge by optimizing computation allocation in dense-token processing, which
introduces four key enhancements. First, we propose Dynamic Prompt Embedding
(DPE) that adaptively processes only regions of interest while avoiding
additional overhead from background tokens. Second, we introduce Dynamic Hybrid
Attention (DHA), which leverages previous segmentation masks to route tokens
through either full attention (O(N2)) for boundary regions or our proposed
efficient BSQ attention (O(N)) for non-boundary regions. Third, we develop
Hybrid Mixture of Experts (HMoE), which applies similar adaptive computation
strategies in FFN modules with CPU-optimized parallel processing. Finally, we
present Dynamic Local Upsampling (DLU), a reverse operation of DPE, which
localizes objects with a lightweight MLP and performs fine-grained upsampling
only in detected regions. Experimental results on high-precision IS benchmarks
demonstrate that Inter2Former achieves SOTA performance with high efficiency on
CPU devices.

</details>


### [76] [Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect](https://arxiv.org/abs/2507.10013)
*Tom Kouwenhoven,Kiana Shahrasbi,Tessa Verhoef*

Main category: cs.CV

TL;DR: 研究重新评估了多模态模型是否具备如 Bouba-Kiki 效应的跨模态认知能力，探讨其在形状与词汇匹配任务中的表现，结果显示模型未能呈现稳定的人类跨模态行为特征。


<details>
  <summary>Details</summary>
Motivation: 探讨最新视觉-语言模型（VLMs）在整合跨模态信息时，是否表现出类似人类认知的能力，尤其是经典的 Bouba-Kiki 效应中词汇与形状的关联。

Method: 比较了 CLIP 中两种变体（ResNet 和 Vision Transformer），设计了两种方法：基于提示的概率评估，以及使用 Grad-CAM 技术解释视觉注意在任务中的作用。

Result: 发现 ResNet 表现出对圆形偏好，但整体模型未表现出 Bouba-Kiki 效应的稳定关联，与人类实验数据差异显著。

Conclusion: 视觉-语言模型在跨模态概念理解上仍存在显著局限性，其内部表示和人类认知直觉不完全对齐，应进一步改进模型能力以贴合人类认知特点。

Abstract: Recent advances in multimodal models have raised questions about whether
vision-and-language models (VLMs) integrate cross-modal information in ways
that reflect human cognition. One well-studied test case in this domain is the
bouba-kiki effect, where humans reliably associate pseudowords like "bouba"
with round shapes and "kiki" with jagged ones. Given the mixed evidence found
in prior studies for this effect in VLMs, we present a comprehensive
re-evaluation focused on two variants of CLIP, ResNet and Vision Transformer
(ViT), given their centrality in many state-of-the-art VLMs. We apply two
complementary methods closely modelled after human experiments: a prompt-based
evaluation that uses probabilities as model preference, and we use Grad-CAM as
a novel way to interpret visual attention in shape-word matching tasks. Our
findings show that these models do not consistently exhibit the bouba-kiki
effect. While ResNet shows a preference for round shapes, overall performance
across both models lacks the expected associations. Moreover, direct comparison
with prior human data on the same task shows that the models' responses fall
markedly short of the robust, modality-integrated behaviour characteristic of
human cognition. These results contribute to the ongoing debate about the
extent to which VLMs truly understand cross-modal concepts, highlighting
limitations in their internal representations and alignment with human
intuitions.

</details>


### [77] [Towards Fine-Grained Adaptation of CLIP via a Self-Trained Alignment Score](https://arxiv.org/abs/2507.09615)
*Eman Ali,Sathira Silva,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 本文提出了FAIR方法，通过动态对齐图像和文本特征，在无监督微调中显著提高了精细分类的性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLM无监督方法在处理细粒度分类问题时，无法捕捉变化的类别差异或计算成本高，因此需要一种既高效又具有良好跨模态交互的新方法。

Method: FAIR方法通过引入类描述锚点（CDA）和学习对齐分数（LAS），实现了图像特征与文本描述动态对齐，并提出了一种自训练权重机制优化伪标签。

Result: FAIR方法在13个细粒度数据集上的表现相比现有方法提升了2.78%。

Conclusion: FAIR通过改进细粒度跨模态交互及优化伪标签，解决了现有方法的局限性，并显著提升了无监督微调的性能。

Abstract: Vision-language models (VLMs) like CLIP excel in zero-shot learning by
aligning image and text representations through contrastive pretraining.
Existing approaches to unsupervised adaptation (UA) for fine-grained
classification with VLMs either rely on fixed alignment scores that cannot
capture evolving, subtle class distinctions or use computationally expensive
pseudo-labeling strategies that limit scalability. In contrast, we show that
modeling fine-grained cross-modal interactions during adaptation produces more
accurate, class-discriminative pseudo-labels and substantially improves
performance over state-of-the-art (SOTA) methods. We introduce Fine-grained
Alignment and Interaction Refinement (FAIR), an innovative approach that
dynamically aligns localized image features with descriptive language
embeddings through a set of Class Description Anchors (CDA). This enables the
definition of a Learned Alignment Score (LAS), which incorporates CDA as an
adaptive classifier, facilitating cross-modal interactions to improve
self-training in unsupervised adaptation. Furthermore, we propose a
self-training weighting mechanism designed to refine pseudo-labels in the
presence of inter-class ambiguities. Our approach, FAIR, delivers a substantial
performance boost in fine-grained unsupervised adaptation, achieving a notable
overall gain of 2.78% across 13 fine-grained datasets compared to SOTA methods.

</details>


### [78] [Generate Aligned Anomaly: Region-Guided Few-Shot Anomaly Image-Mask Pair Synthesis for Industrial Inspection](https://arxiv.org/abs/2507.09619)
*Yilin Lu,Jianghang Lin,Linhuang Xie,Kai Zhao,Yansong Qu,Shengchuan Zhang,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: 本文提出了一种名为GAA的框架，通过生成对齐的异常图像和掩码对，解决了现有异常检测方法在样本稀缺情况下的不足，同时提升了生成样本的现实感和准确性。


<details>
  <summary>Details</summary>
Motivation: 目前工业制造中的异常检测面临异常样本稀缺的问题，现有异常合成方法在真实性、掩码对齐和泛化能力上存在不足，亟需一种更加有效的解决方案。

Method: 提出GAA框架，结合Localized Concept Decomposition以灵活控制异常的类型和位置；采用Adaptive Multi-Round Anomaly Clustering进行细粒度语义聚类；通过区域引导生成精确对齐的异常掩码，并利用低质量样本过滤提升生成样本质量。

Result: 实验证明，GAA在MVTec AD和LOCO数据集上的异常合成质量和后续任务表现（如定位和分类）中均优于现有方法。

Conclusion: GAA框架有效提升了异常检测中的样本生成能力，为工业异常检测和数据增强提供了新的思路。

Abstract: Anomaly inspection plays a vital role in industrial manufacturing, but the
scarcity of anomaly samples significantly limits the effectiveness of existing
methods in tasks such as localization and classification. While several anomaly
synthesis approaches have been introduced for data augmentation, they often
struggle with low realism, inaccurate mask alignment, and poor generalization.
To overcome these limitations, we propose Generate Aligned Anomaly (GAA), a
region-guided, few-shot anomaly image-mask pair generation framework. GAA
leverages the strong priors of a pretrained latent diffusion model to generate
realistic, diverse, and semantically aligned anomalies using only a small
number of samples. The framework first employs Localized Concept Decomposition
to jointly model the semantic features and spatial information of anomalies,
enabling flexible control over the type and location of anomalies. It then
utilizes Adaptive Multi-Round Anomaly Clustering to perform fine-grained
semantic clustering of anomaly concepts, thereby enhancing the consistency of
anomaly representations. Subsequently, a region-guided mask generation strategy
ensures precise alignment between anomalies and their corresponding masks,
while a low-quality sample filtering module is introduced to further improve
the overall quality of the generated samples. Extensive experiments on the
MVTec AD and LOCO datasets demonstrate that GAA achieves superior performance
in both anomaly synthesis quality and downstream tasks such as localization and
classification.

</details>


### [79] [FaceLLM: A Multimodal Large Language Model for Face Understanding](https://arxiv.org/abs/2507.10300)
*Hatef Otroshi Shahreza,Sébastien Marcel*

Main category: cs.CV

TL;DR: 提出了FaceLLM，一种面向面部图像理解的多模态大语言模型，通过构建FairFaceGPT数据集进行训练，解决了领域特定视觉线索的推理挑战并实现了多项面部任务的领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLMs）在视觉-语言任务上表现优异，但因缺乏大规模的面部图像-文本数据集，对面部图像的领域特定理解受限，尤其在面部结构、表情、情感等任务上探索不足。

Method: 利用弱监督方法构建FairFaceGPT数据集，结合ChatGPT生成属性感知的高质量问答对，然后训练专注于面部图像理解的FaceLLM模型。

Result: FaceLLM在多项面部相关任务中展现了优异性能，达到了当前领域的最佳水平。

Conclusion: 面向领域的合成监督能有效帮助构建特化的多模态大语言模型，该工作为可信且以人为本的多模态AI系统树立了新标准并公开了相关数据集与预训练模型。

Abstract: Multimodal large language models (MLLMs) have shown remarkable performance in
vision-language tasks. However, existing MLLMs are primarily trained on generic
datasets, limiting their ability to reason on domain-specific visual cues such
as those in facial images. In particular, tasks that require detailed
understanding of facial structure, expression, emotion, and demographic
features remain underexplored by MLLMs due to the lack of large-scale annotated
face image-text datasets. In this work, we introduce FaceLLM, a multimodal
large language model trained specifically for facial image understanding. To
construct the training data, we propose a novel weakly supervised pipeline that
uses ChatGPT with attribute-aware prompts to generate high-quality
question-answer pairs based on images from the FairFace dataset. The resulting
corpus, called FairFaceGPT, covers a diverse set of attributes including
expression, pose, skin texture, and forensic information. Our experiments
demonstrate that FaceLLM improves the performance of MLLMs on various
face-centric tasks and achieves state-of-the-art performance. This work
highlights the potential of synthetic supervision via language models for
building domain-specialized MLLMs, and sets a precedent for trustworthy,
human-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM
models are publicly available in the project page.

</details>


### [80] [Brain Stroke Detection and Classification Using CT Imaging with Transformer Models and Explainable AI](https://arxiv.org/abs/2507.09630)
*Shomukh Qari,Maha A. Thafar*

Main category: cs.CV

TL;DR: 本研究提出了一个基于人工智能的系统，用于通过CT扫描图像进行中风分类，并采用MaxViT模型实现了98%的精准度。


<details>
  <summary>Details</summary>
Motivation: 早期和准确的中风诊断对于改善患者结果至关重要，特别是在紧急情况下。

Method: 使用MaxViT视觉Transformer作为深度学习模型，同时结合数据增强技术（如合成图像生成）来改进模型泛化能力，并引入Grad-CAM++进行解释性分析。

Result: 提出的方法在分类精度和F1分数上达到98.00%，优于其他模型与基线方法。

Conclusion: 研究开发了一种可用于临床实践的可信AI诊断工具，提升了诊断的及时性和准确性，从而挽救更多生命。

Abstract: Stroke is one of the leading causes of death globally, making early and
accurate diagnosis essential for improving patient outcomes, particularly in
emergency settings where timely intervention is critical. CT scans are the key
imaging modality because of their speed, accessibility, and cost-effectiveness.
This study proposed an artificial intelligence framework for multiclass stroke
classification (ischemic, hemorrhagic, and no stroke) using CT scan images from
a dataset provided by the Republic of Turkey's Ministry of Health. The proposed
method adopted MaxViT, a state-of-the-art Vision Transformer, as the primary
deep learning model for image-based stroke classification, with additional
transformer variants (vision transformer, transformer-in-transformer, and
ConvNext). To enhance model generalization and address class imbalance, we
applied data augmentation techniques, including synthetic image generation. The
MaxViT model trained with augmentation achieved the best performance, reaching
an accuracy and F1-score of 98.00%, outperforming all other evaluated models
and the baseline methods. The primary goal of this study was to distinguish
between stroke types with high accuracy while addressing crucial issues of
transparency and trust in artificial intelligence models. To achieve this,
Explainable Artificial Intelligence (XAI) was integrated into the framework,
particularly Grad-CAM++. It provides visual explanations of the model's
decisions by highlighting relevant stroke regions in the CT scans and
establishing an accurate, interpretable, and clinically applicable solution for
early stroke detection. This research contributed to the development of a
trustworthy AI-assisted diagnostic tool for stroke, facilitating its
integration into clinical practice and enhancing access to timely and optimal
stroke diagnosis in emergency departments, thereby saving more lives.

</details>


### [81] [Devanagari Handwritten Character Recognition using Convolutional Neural Network](https://arxiv.org/abs/2507.10398)
*Diksha Mehta,Prateek Mehta*

Main category: cs.CV

TL;DR: 本研究提出了一种基于卷积神经网络的技术，用于识别手写的天城文字符，并在测试中获得96.36%的准确率。


<details>
  <summary>Details</summary>
Motivation: 天城文作为印度的古老文字脚本之一，缺乏适当的数字化工具。本研究旨在开发一种自动化方法从天城文图像中提取手写字符，提升效率并处理过时的数据。

Method: 利用两层深度卷积神经网络的方法，对天城文学手写字符进行识别，并使用包含36类字符的公开数据集（DHCD）进行训练和测试，每类包含1700张图像。

Result: 在训练过程中取得了99.55%的高准确率，在测试阶段取得了96.36%的准确率。

Conclusion: 提出的卷积神经网络方法在识别天城文手写字符方面表现出色，能够有效提高识别效率和准确率。

Abstract: Handwritten character recognition is getting popular among researchers
because of its possible applications in facilitating technological search
engines, social media, recommender systems, etc. The Devanagari script is one
of the oldest language scripts in India that does not have proper digitization
tools. With the advancement of computing and technology, the task of this
research is to extract handwritten Hindi characters from an image of Devanagari
script with an automated approach to save time and obsolete data. In this
paper, we present a technique to recognize handwritten Devanagari characters
using two deep convolutional neural network layers. This work employs a
methodology that is useful to enhance the recognition rate and configures a
convolutional neural network for effective Devanagari handwritten text
recognition (DHTR). This approach uses the Devanagari handwritten character
dataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each
of these classes has 1700 images for training and testing purposes. This
approach obtains promising results in terms of accuracy by achieving 96.36%
accuracy in testing and 99.55% in training time.

</details>


### [82] [Disentanglement and Assessment of Shortcuts in Ophthalmological Retinal Imaging Exams](https://arxiv.org/abs/2507.09640)
*Leonor Fernandes,Tiago Gonçalves,João Matos,Luis Filipe Nakayama,Jaime S. Cardoso*

Main category: cs.CV

TL;DR: 该研究探讨了AI模型在糖尿病视网膜病变预测中的公平性与性能，评估了去偏技术对模型效果的影响。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是工作年龄段人群视力丧失的主要原因，传统筛查方法昂贵且难以普及，而AI技术可提供可扩展的诊断解决方案，但其公平性和泛化能力存在挑战。

Method: 研究使用多样化的mBRSET眼底图像数据集，比较了ConvNeXt V2、DINOv2和Swin V2三种模型预测糖尿病视网膜病变及敏感属性（如年龄和性别）的效果，并通过去偏技术评估其公平性和性能。

Result: 所有模型在糖尿病视网膜病变预测中表现优异（诊断AUROC最高可达94%）。然而，DINOv2在年龄组间的AUROC存在10%的差距。去偏技术对不同模型的表现影响不一：提升了DINOv2性能（AUROC提升2%），但对ConvNeXt V2和Swin V2表现负面（分别下降7%和3%）。

Conclusion: 研究突出了眼底图像中细粒度特征去关联的复杂性，以及在医疗影像AI中的公平性对于实现公平可靠的医疗解决方案的重要性。

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss in working-age
adults. While screening reduces the risk of blindness, traditional imaging is
often costly and inaccessible. Artificial intelligence (AI) algorithms present
a scalable diagnostic solution, but concerns regarding fairness and
generalization persist. This work evaluates the fairness and performance of
image-trained models in DR prediction, as well as the impact of disentanglement
as a bias mitigation technique, using the diverse mBRSET fundus dataset. Three
models, ConvNeXt V2, DINOv2, and Swin V2, were trained on macula images to
predict DR and sensitive attributes (SAs) (e.g., age and gender/sex). Fairness
was assessed between subgroups of SAs, and disentanglement was applied to
reduce bias. All models achieved high DR prediction performance in diagnosing
(up to 94% AUROC) and could reasonably predict age and gender/sex (91% and 77%
AUROC, respectively). Fairness assessment suggests disparities, such as a 10%
AUROC gap between age groups in DINOv2. Disentangling SAs from DR prediction
had varying results, depending on the model selected. Disentanglement improved
DINOv2 performance (2% AUROC gain), but led to performance drops in ConvNeXt V2
and Swin V2 (7% and 3%, respectively). These findings highlight the complexity
of disentangling fine-grained features in fundus imaging and emphasize the
importance of fairness in medical imaging AI to ensure equitable and reliable
healthcare solutions.

</details>


### [83] [Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources](https://arxiv.org/abs/2507.10403)
*Daniele Rege Cambrin,Lorenzo Vaiani,Giuseppe Gallipoli,Luca Cagliero,Paolo Garza*

Main category: cs.CV

TL;DR: 提出CrisisLandMark数据集，结合SAR与光学影像新方法CLOSP，实现先进检索能力。


<details>
  <summary>Details</summary>
Motivation: 现有文本-影像检索系统局限于RGB数据，未能充分利用SAR和多光谱影像的物理信息。

Method: 引入包含647,000+影像及注释的CrisisLandMark数据集，提出基于对比学习的CLOSP，通过文本桥接对SAR与光学影像进行统一嵌入空间学习，同时通过GeoCLOSP结合地理信息优化。

Result: CLOSP在nDGC指标上比现有模型提升54%，GeoCLOSP在特定任务中表现优异，同时展现SAR解读的改进能力。

Conclusion: 多传感器数据和地理信息的融合对于解锁遥感档案潜力至关重要。

Abstract: Retrieving relevant imagery from vast satellite archives is crucial for
applications like disaster response and long-term climate monitoring. However,
most text-to-image retrieval systems are limited to RGB data, failing to
exploit the unique physical information captured by other sensors, such as the
all-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the
spectral signatures in optical multispectral data. To bridge this gap, we
introduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1
SAR and Sentinel-2 multispectral images paired with structured textual
annotations for land cover, land use, and crisis events harmonized from
authoritative land cover systems (CORINE and Dynamic World) and crisis-specific
sources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),
a novel framework that uses text as a bridge to align unpaired optical and SAR
images into a unified embedding space. Our experiments show that CLOSP achieves
a new state-of-the-art, improving retrieval nDGC by 54% over existing models.
Additionally, we find that the unified training strategy overcomes the inherent
difficulty of interpreting SAR imagery by transferring rich semantic knowledge
from the optical domain with indirect interaction. Furthermore, GeoCLOSP, which
integrates geographic coordinates into our framework, creates a powerful
trade-off between generality and specificity: while the CLOSP excels at general
semantic tasks, the GeoCLOSP becomes a specialized expert for retrieving
location-dependent crisis events and rare geographic features. This work
highlights that the integration of diverse sensor data and geographic context
is essential for unlocking the full potential of remote sensing archives.

</details>


### [84] [EyeSeg: An Uncertainty-Aware Eye Segmentation Framework for AR/VR](https://arxiv.org/abs/2507.09649)
*Zhengyuan Peng,Jianqing Xu,Shen Li,Jiazhen Ji,Yuge Huang,Jingyun Zhang,Jinmin Li,Shouhong Ding,Rizen Guo,Xin Tan,Lizhuang Ma*

Main category: cs.CV

TL;DR: 本文提出了EyeSeg，一种用于增强和虚拟现实中眼部分割的新框架，解决了运动模糊、眼睑遮挡和域间差异问题，显著提高了分割精度和凝视估计性能。


<details>
  <summary>Details</summary>
Motivation: 当前增强现实（AR）和虚拟现实（VR）的人机交互需要精确的凝视估计，而其关键在于准确的眼部分割。然而，现有方法无法很好地应对运动模糊、眼睑遮挡及域间差异问题，导致性能下降。

Method: 设计了EyeSeg框架，通过贝叶斯不确定性学习对在闭集先验下的后验分布进行建模，量化不确定性水平，并结合输出的不确定性得分和分割结果提升下游任务性能。

Result: EyeSeg在凝视估计和受挑战条件（如运动模糊、眼睑遮挡、跨域问题）下取得了显著优于现有方法的效果，并在分割指标MIoU、E1、F1和ACC上均有改进。

Conclusion: EyeSeg框架通过不确定性感知的方式有效解决了传统方法中的弱点，为AR/VR环境中的眼部分割和相关任务提供了更鲁棒的解决方案，代码现已开源。

Abstract: Human-machine interaction through augmented reality (AR) and virtual reality
(VR) is increasingly prevalent, requiring accurate and efficient gaze
estimation which hinges on the accuracy of eye segmentation to enable smooth
user experiences. We introduce EyeSeg, a novel eye segmentation framework
designed to overcome key challenges that existing approaches struggle with:
motion blur, eyelid occlusion, and train-test domain gaps. In these situations,
existing models struggle to extract robust features, leading to suboptimal
performance. Noting that these challenges can be generally quantified by
uncertainty, we design EyeSeg as an uncertainty-aware eye segmentation
framework for AR/VR wherein we explicitly model the uncertainties by performing
Bayesian uncertainty learning of a posterior under the closed set prior.
Theoretically, we prove that a statistic of the learned posterior indicates
segmentation uncertainty levels and empirically outperforms existing methods in
downstream tasks, such as gaze estimation. EyeSeg outputs an uncertainty score
and the segmentation result, weighting and fusing multiple gaze estimates for
robustness, which proves to be effective especially under motion blur, eyelid
occlusion and cross-domain challenges. Moreover, empirical results suggest that
EyeSeg achieves segmentation improvements of MIoU, E1, F1, and ACC surpassing
previous approaches. The code is publicly available at
https://github.com/JethroPeng/EyeSeg.

</details>


### [85] [VST-Pose: A Velocity-Integrated Spatiotem-poral Attention Network for Human WiFi Pose Estimation](https://arxiv.org/abs/2507.09672)
*Xinyu Zhang,Zhonghao Ye,Jingwei Zhang,Xiang Tian,Zhisheng Liang,Shipeng Yu*

Main category: cs.CV

TL;DR: 该研究提出了通过WiFi信道状态信息实现人体姿态估计的新方法，开发了一个名为VST-Pose的深度学习框架，能够在保留隐私的同时实现高精度的姿态估计。


<details>
  <summary>Details</summary>
Motivation: 基于WiFi的人体姿态估计由于其穿透性和隐私优势，成为视觉技术的有力替代选择。但现有方法在细微动作感知与数据处理能力上仍存在局限。

Method: 提出了一种名为ViSTA-Former的空间时间注意力主干网络，采用双流架构分别捕捉时间依赖性与关节间的结构关系，并引入速度建模分支以提升细粒度运动表征能力。

Result: 构建了专为智能家居护理场景设计的2D姿态数据集，在PCK@50指标上达到了92.2%的精度，在自建数据集上较现有方法提升了8.3%。此外，在公开数据集MMFi上的3D姿态估计任务中，验证了模型的稳健性和有效性。

Conclusion: 该方法提供了一个可靠且隐私保护的室内连续人体动作分析方案，有助于智能家居等领域的发展。

Abstract: WiFi-based human pose estimation has emerged as a promising non-visual
alternative approaches due to its pene-trability and privacy advantages. This
paper presents VST-Pose, a novel deep learning framework for accurate and
continuous pose estimation using WiFi channel state information. The proposed
method introduces ViSTA-Former, a spatiotemporal attention backbone with
dual-stream architecture that adopts a dual-stream architecture to separately
capture temporal dependencies and structural relationships among body joints.
To enhance sensitivity to subtle human motions, a velocity modeling branch is
integrated into the framework, which learns short-term keypoint dis-placement
patterns and improves fine-grained motion representation. We construct a 2D
pose dataset specifically designed for smart home care scenarios and
demonstrate that our method achieves 92.2% accuracy on the PCK@50 metric,
outperforming existing methods by 8.3% in PCK@50 on the self-collected dataset.
Further evaluation on the public MMFi dataset confirms the model's robustness
and effectiveness in 3D pose estimation tasks. The proposed system provides a
reliable and privacy-aware solution for continuous human motion analysis in
indoor environments. Our codes are available in
https://github.com/CarmenQing/VST-Pose.

</details>


### [86] [Prompt2DEM: High-Resolution DEMs for Urban and Open Environments from Global Prompts Using a Monocular Foundation Model](https://arxiv.org/abs/2507.09681)
*Osher Rafaeli,Tal Svoray,Ariel Nahlieli*

Main category: cs.CV

TL;DR: 研究提出了一个新的框架，用于通过提示驱动的单目深度估计生成高分辨率数字高程模型（DEMs），显示了显著提高的分辨率和精度，并应用于多种环境和任务。


<details>
  <summary>Details</summary>
Motivation: 现有生成高分辨率DEM的方法存在局限性：超分辨率技术受放大因子的限制，单目深度估计缺乏全球高程背景。新提出的提示驱动单目深度估计为在全球背景下生成绝对高程数据提供了潜力。

Method: 利用Shuttle Radar Topography Mission（SRTM）低分辨率数据作为提示和国家农业影像计划（NAIP）高分辨率RGB影像，结合视觉Transformer编码器，以LiDAR生成的DEM作为微调模型，开发了一种灵活提示策略，实现高分辨率DEM估算、空隙填补和更新等任务。

Result: 框架提升了100倍分辨率（从30米到30厘米），相较于前人方法提升一个数量级，对美国三种不同地形的评估中，与LiDAR相比的平均绝对误差低于5米，比SRTM精度提高最多18%。

Conclusion: 本研究展示了框架在地形特征捕捉与应用环境研究中的潜力，包括灾害和环境研究，同时证明了在大范围区域（如美国和以色列）的可扩展性。

Abstract: High-resolution elevation estimations are essential to understand catchment
and hillslope hydrology, study urban morphology and dynamics, and monitor the
growth, decline, and mortality of terrestrial ecosystems. Various deep learning
approaches (e.g., super-resolution techniques, monocular depth estimation) have
been developed to create high-resolution Digital Elevation Models (DEMs).
However, super-resolution techniques are limited by the upscaling factor, and
monocular depth estimation lacks global elevation context, making its
conversion to a seamless DEM restricted. The recently introduced technique of
prompt-based monocular depth estimation has opened new opportunities to extract
estimates of absolute elevation in a global context. We present here a
framework for the estimation of high-resolution DEMs as a new paradigm for
absolute global elevation mapping. It is exemplified using low-resolution
Shuttle Radar Topography Mission (SRTM) elevation data as prompts and
high-resolution RGB imagery from the National Agriculture Imagery Program
(NAIP). The approach fine-tunes a vision transformer encoder with LiDAR-derived
DEMs and employs a versatile prompting strategy, enabling tasks such as DEM
estimation, void filling, and updating. Our framework achieves a 100x
resolution gain (from 30-m to 30-cm), surpassing prior methods by an order of
magnitude. Evaluations across three diverse U.S. landscapes show robust
generalization, capturing urban structures and fine-scale terrain features with
< 5 m MAE relative to LiDAR, improving over SRTM by up to 18%. Hydrological
analysis confirms suitability for hazard and environmental studies. We
demonstrate scalability by applying the framework to large regions in the U.S.
and Israel. All code and pretrained models are publicly available at:
https://osherr1996.github.io/prompt2dem_propage/.

</details>


### [87] [ExpStar: Towards Automatic Commentary Generation for Multi-discipline Scientific Experiments](https://arxiv.org/abs/2507.09693)
*Jiali Chen,Yujie Jia,Zihan Wu,Jinyu Yang,Jianpeng Chen,Xusen Hei,Jiayuan Xie,Yi Cai,Qing Li*

Main category: cs.CV

TL;DR: 这篇论文提出了生成多学科科学实验评论的任务，并提出了一个新数据集ExpInstruct和模型ExpStar，用于自动生成实验评论，显著超越现有模型表现。


<details>
  <summary>Details</summary>
Motivation: 实验评论对描述实验过程及科学原理至关重要，但人工准备需要大量时间和学科专长。需要自动化解决方案。

Method: 构建了ExpInstruct数据集，收录了多个领域的实验评论内容；提出了ExpStar模型，其基于检索增强机制，自动访问和利用外部知识生成评论。

Result: ExpStar模型在实验中显著优于14种主流多模态大模型，证明了该数据集和模型的卓越性能。

Conclusion: ExpStar模型和ExpInstruct数据集为AI辅助科学实验指导提供了重要的技术基础和潜力。

Abstract: Experiment commentary is crucial in describing the experimental procedures,
delving into underlying scientific principles, and incorporating
content-related safety guidelines. In practice, human teachers rely heavily on
subject-specific expertise and invest significant time preparing such
commentary. To address this challenge, we introduce the task of automatic
commentary generation across multi-discipline scientific experiments. While
recent progress in large multimodal models (LMMs) has demonstrated promising
capabilities in video understanding and reasoning, their ability to generate
fine-grained and insightful experiment commentary remains largely
underexplored. In this paper, we make the following contributions: (i) We
construct \textit{ExpInstruct}, the first dataset tailored for experiment
commentary generation, featuring over 7\textit{K} step-level commentaries
across 21 scientific subjects from 3 core disciplines (\ie, science, healthcare
and engineering). Each sample includes procedural descriptions along with
potential scientific principles (\eg, chemical equations and physical laws) and
safety guidelines. (ii) We propose ExpStar, an automatic experiment commentary
generation model that leverages a retrieval-augmented mechanism to adaptively
access, evaluate, and utilize external knowledge. (iii) Extensive experiments
show that our ExpStar substantially outperforms 14 leading LMMs, which
highlights the superiority of our dataset and model. We believe that ExpStar
holds great potential for advancing AI-assisted scientific experiment
instruction.

</details>


### [88] [EmbRACE-3K: Embodied Reasoning and Action in Complex Environments](https://arxiv.org/abs/2507.10548)
*Mingxian Lin,Wei Huang,Yitang Li,Chengjie Jiang,Kui Wu,Fangwei Zhong,Shengju Qian,Xin Wang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 提出了一种名为EmRACE-3K的新数据集，用于评估视觉语言模型（VLMs）在真实互动环境中的表现，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的VLMs在被动图像和视频理解任务中的表现优异，但在需要主动交互的情境中表现有限，因此需要一个新工具来评估和提升它们在这类任务中的能力。

Method: 构建了EmRACE-3K数据集，包括3000多个语言引导任务，涵盖导航、对象操作和多阶段目标执行等挑战，并通过监督学习和强化学习对VLMs进行微调。

Result: 在零样本情况下，所有模型的成功率均低于20%；在微调后，模型在探索、动态空间-语义推理和多阶段目标执行三项挑战中都有显著提升。

Conclusion: EmRACE-3K是一种有效的基准工具，可以推动VLMs的交互式推理能力的发展，展示了在此领域进一步研究的潜力。

Abstract: Recent advanced vision-language models(VLMs) have demonstrated strong
performance on passive, offline image and video understanding tasks. However,
their effectiveness in embodied settings, which require online interaction and
active scene understanding remains limited. In such scenarios, an agent
perceives the environment from a first-person perspective, with each action
dynamically shaping subsequent observations. Even state-of-the-art models such
as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment
interactions, exhibiting clear limitations in spatial reasoning and
long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset
of over 3,000 language-guided tasks situated in diverse, photorealistic
environments constructed using Unreal Engine and the UnrealCV-Zoo framework.
The tasks encompass a wide range of embodied challenges, including navigation,
object manipulation, and multi-stage goal execution. Each task unfolds as a
multi-step trajectory, pairing first-person visual observations with high-level
instructions, grounded actions, and natural language rationales that express
the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to
evaluate the embodied reasoning capabilities of VLMs across three key
dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage
Goal Execution. In zero-shot settings, all models achieve success rates below
20%, underscoring the challenge posed by our benchmark and the current
limitations of VLMs in interactive environments. To demonstrate the utility of
EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning
followed by reinforcement learning. This approach yields substantial
improvements across all three challenge categories, highlighting the dataset's
effectiveness in enabling the development of embodied reasoning capabilities.

</details>


### [89] [Token Compression Meets Compact Vision Transformers: A Survey and Comparative Evaluation for Edge AI](https://arxiv.org/abs/2507.09702)
*Phat Nguyen,Ngai-Man Cheung*

Main category: cs.CV

TL;DR: 这篇论文系统地分类和比较了令牌压缩方法，并评估了它们在标准和紧凑ViT架构上的表现。


<details>
  <summary>Details</summary>
Motivation: 解决目前对令牌压缩方法分类和比较的系统化缺乏，以及在紧凑型变压器上的应用有效性尚未明确的问题。

Method: 通过系统分类和比较不同类型的令牌压缩方法，并在标准和紧凑型ViT架构上进行实验评估。

Result: 研究发现，虽然令牌压缩方法在通用ViT上表现有效，但直接应用于紧凑型设计时常表现不佳。

Conclusion: 这些发现为令牌优化技术适配紧凑型变压器网络提供了实用洞察，并为未来在边缘AI和智能代理中的应用研究铺平了道路。

Abstract: Token compression techniques have recently emerged as powerful tools for
accelerating Vision Transformer (ViT) inference in computer vision. Due to the
quadratic computational complexity with respect to the token sequence length,
these methods aim to remove less informative tokens before the attention layers
to improve inference throughput. While numerous studies have explored various
accuracy-efficiency trade-offs on large-scale ViTs, two critical gaps remain.
First, there is a lack of unified survey that systematically categorizes and
compares token compression approaches based on their core strategies (e.g.,
pruning, merging, or hybrid) and deployment settings (e.g., fine-tuning vs.
plug-in). Second, most benchmarks are limited to standard ViT models (e.g.,
ViT-B, ViT-L), leaving open the question of whether such methods remain
effective when applied to structurally compressed transformers, which are
increasingly deployed on resource-constrained edge devices. To address these
gaps, we present the first systematic taxonomy and comparative study of token
compression methods, and we evaluate representative techniques on both standard
and compact ViT architectures. Our experiments reveal that while token
compression methods are effective for general-purpose ViTs, they often
underperform when directly applied to compact designs. These findings not only
provide practical insights but also pave the way for future research on
adapting token optimization techniques to compact transformer-based networks
for edge AI and AI agent applications.

</details>


### [90] [Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation](https://arxiv.org/abs/2507.09748)
*Yu Lei,Bingde Liu,Qingsong Xie,Haonan Lu,Zhijie Deng*

Main category: cs.CV

TL;DR: 本文探讨了基于预训练2D扩散模型的文本到3D生成问题，提出了一种改进的变分得分蒸馏方法（$L^2$-VSD），在实验中表现优于之前的方法。


<details>
  <summary>Details</summary>
Motivation: 当前传统的得分蒸馏存在收敛性差的问题，研究旨在改进变分得分蒸馏（VSD）的优化过程，解决与3D模型分布的错配问题，并提升生成质量与稳定性。

Method: 提出了一种改进的变分得分蒸馏方法$L^2$-VSD，通过调整得分模型和3D模型的优化顺序，并引入线性化的模型变体，利用现有深度学习库的前向自动微分功能高效实现。

Result: 实验表明，$L^2$-VSD方法在生成质量和稳定性上明显优于以往得分蒸馏方法，并能够无缝整合到其他基于VSD的框架中。

Conclusion: 通过解决优化顺序和分布错配等问题，$L^2$-VSD显著改进了变分得分蒸馏方法的生成质量和稳定性，为文本到3D领域提供了更优的解决方案。

Abstract: Text-to-3D generation based on score distillation of pre-trained 2D diffusion
models has gained increasing interest, with variational score distillation
(VSD) as a remarkable example. VSD proves that vanilla score distillation can
be improved by introducing an extra score-based model, which characterizes the
distribution of images rendered from 3D models, to correct the distillation
gradient. Despite the theoretical foundations, VSD, in practice, is likely to
suffer from slow and sometimes ill-posed convergence. In this paper, we perform
an in-depth investigation of the interplay between the introduced score model
and the 3D model, and find that there exists a mismatching problem between LoRA
and 3D distributions in practical implementation. We can simply adjust their
optimization order to improve the generation quality. By doing so, the score
model looks ahead to the current 3D state and hence yields more reasonable
corrections. Nevertheless, naive lookahead VSD may suffer from unstable
training in practice due to the potential over-fitting. To address this, we
propose to use a linearized variant of the model for score distillation, giving
rise to the Linearized Lookahead Variational Score Distillation ($L^2$-VSD).
$L^2$-VSD can be realized efficiently with forward-mode autodiff
functionalities of existing deep learning libraries. Extensive experiments
validate the efficacy of $L^2$-VSD, revealing its clear superiority over prior
score distillation-based methods. We also show that our method can be
seamlessly incorporated into any other VSD-based text-to-3D framework.

</details>


### [91] [Pairwise Alignment & Compatibility for Arbitrarily Irregular Image Fragments](https://arxiv.org/abs/2507.09767)
*Ofir Itzhak Shahar,Gur Elkin,Ohad Ben-Shahar*

Main category: cs.CV

TL;DR: 本文提出了一种无假设下高效计算图像碎片对齐的新方法，结合几何与图像信息，展现了在考古拼图任务中的卓越性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法对于现实世界碎片的几何特性适用性不足，或者依赖于碎片特定形状，需改进计算兼容性的方法以适应真实场景。

Method: 提出了一种结合几何与图像的混合方法，用以计算碎片间的最优对齐，且无需假设形状与尺寸；生成了新数据集与评估指标，以及考古侵蚀模型。

Result: 该方法在RePAIR 2D数据集上实现了邻域级精度和召回的先进水平，体现了兼容性性能的显著提升。

Conclusion: 通过无形状和尺寸假设的兼容性计算改进了考古拼图问题的碎片对齐能力，此方法表现出了实际场景的有效性与性能优势。

Abstract: Pairwise compatibility calculation is at the core of most
fragments-reconstruction algorithms, in particular those designed to solve
different types of the jigsaw puzzle problem. However, most existing approaches
fail, or aren't designed to deal with fragments of realistic geometric
properties one encounters in real-life puzzles. And in all other cases,
compatibility methods rely strongly on the restricted shapes of the fragments.
In this paper, we propose an efficient hybrid (geometric and pictorial)
approach for computing the optimal alignment for pairs of fragments, without
any assumptions about their shapes, dimensions, or pictorial content. We
introduce a new image fragments dataset generated via a novel method for image
fragmentation and a formal erosion model that mimics real-world archaeological
erosion, along with evaluation metrics for the compatibility task. We then
embed our proposed compatibility into an archaeological puzzle-solving
framework and demonstrate state-of-the-art neighborhood-level precision and
recall on the RePAIR 2D dataset, directly reflecting compatibility performance
improvements.

</details>


### [92] [NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection](https://arxiv.org/abs/2507.09795)
*Amirhossein Ansari,Ke Wang,Pulei Xiong*

Main category: cs.CV

TL;DR: 本文提出了一个名为NegRefine的新框架，以改进视觉语言模型在零样本OOD检测中的性能，主要通过负标签的筛选和动态评分机制来提高检测精度。


<details>
  <summary>Details</summary>
Motivation: 当前负标签方法在区分OOD样本时易误将分布内样本误判为OOD，且难以处理同时匹配多种负标签和分布内标签的图像。

Method: NegRefine通过过滤机制排除子类别标签和专有名词，并采用动态调整多重标签匹配贡献的评分函数，来提升分布内和OOD样本的区分能力。

Result: 在包括ImageNet-1K在内的大规模基准上，NegRefine展现出更强的分类能力和鲁棒性。

Conclusion: NegRefine通过负标签的精炼和多标签动态匹配策略，提高了零样本OOD检测的准确性和实用性，为视觉语言模型的应用提供了新方向。

Abstract: Recent advancements in Vision-Language Models like CLIP have enabled
zero-shot OOD detection by leveraging both image and textual label information.
Among these, negative label-based methods such as NegLabel and CSP have shown
promising results by utilizing a lexicon of words to define negative labels for
distinguishing OOD samples. However, these methods suffer from detecting
in-distribution samples as OOD due to negative labels that are subcategories of
in-distribution labels or proper nouns. They also face limitations in handling
images that match multiple in-distribution and negative labels. We propose
NegRefine, a novel negative label refinement framework for zero-shot OOD
detection. By introducing a filtering mechanism to exclude subcategory labels
and proper nouns from the negative label set and incorporating a
multi-matching-aware scoring function that dynamically adjusts the
contributions of multiple labels matching an image, NegRefine ensures a more
robust separation between in-distribution and OOD samples. We evaluate
NegRefine on large-scale benchmarks, including ImageNet-1K. Source code is
available at https://github.com/ah-ansari/NegRefine.

</details>


### [93] [VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding](https://arxiv.org/abs/2507.09815)
*Younggun Kim,Ahmed S. Abdelrahman,Mohamed Abdel-Aty*

Main category: cs.CV

TL;DR: 该论文提出了VRU-Accident，一个用于评估多模态大语言模型在高风险交通场景中进行推理能力的大规模视觉语言基准。


<details>
  <summary>Details</summary>
Motivation: 解决当前多模态大语言模型缺乏针对行人和骑行者等弱势道路使用者交通事故中进行复杂推理能力评估的标准化基准。

Method: 设计并发布了VRU-Accident基准，包含1K事故视频、6K多选题问答对和1K场景描述，并对17个最先进模型进行了综合评估。

Result: 结果显示，多模态大语言模型在可视化属性分析中表现尚可，但在事故原因、类型及预防性推理能力方面仍有明显不足。

Conclusion: 此工作填补了高风险交通情境下针对弱势道路使用者的大语言模型评估空白，并揭示模型在事故因果和预防性推理中的局限性。

Abstract: Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and
cyclists, is a critical challenge for autonomous driving systems, as crashes
involving VRUs often result in severe or fatal consequences. While multimodal
large language models (MLLMs) have shown promise in enhancing scene
understanding and decision making in autonomous vehicles, there is currently no
standardized benchmark to quantitatively evaluate their reasoning abilities in
complex, safety-critical scenarios involving VRUs. To address this gap, we
present VRU-Accident, a large-scale vision-language benchmark designed to
evaluate MLLMs in high-risk traffic scenarios involving VRUs. VRU-Accident
comprises 1K real-world dashcam accident videos, annotated with 6K
multiple-choice question-answer pairs across six safety-critical categories
(with 24K candidate options and 3.4K unique answer choices), as well as 1K
dense scene descriptions. Unlike prior works, our benchmark focuses explicitly
on VRU-vehicle accidents, providing rich, fine-grained annotations that capture
both spatial-temporal dynamics and causal semantics of accidents. To assess the
current landscape of MLLMs, we conduct a comprehensive evaluation of 17
state-of-the-art models on the multiple-choice VQA task and on the dense
captioning task. Our findings reveal that while MLLMs perform reasonably well
on visually grounded attributes, they face significant challenges in reasoning
and describing accident causes, types, and preventability.

</details>


### [94] [Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models](https://arxiv.org/abs/2507.09830)
*Shuhao Fu,Philip J. Kellman,Hongjing Lu*

Main category: cs.CV

TL;DR: 本文探讨了深度学习模型是否能够以与人类相似的方式，从稀疏的3D点云中形成3D形状表征，并比较了两种不同模型的性能。


<details>
  <summary>Details</summary>
Motivation: 研究深度学习模型在3D形状识别中的表征能力，并验证其是否类似于人类的3D形状理解方式。

Method: 进行了两项实验，操控点密度和物体方向（实验1）以及局部几何结构（实验2），并比较DGCNN和点变换器（point transformer）两种模型的性能。

Result: 与卷积神经网络模型相比，点变换器模型在模拟人类表现方面更接近，表现出对3D形状层次抽象的优越特性。

Conclusion: 点变换器模型由于支持3D形状的层次抽象，在三维物体识别中更符合人类的表现，提升了对深度学习模型3D形状表征能力的理解。

Abstract: Both humans and deep learning models can recognize objects from 3D shapes
depicted with sparse visual information, such as a set of points randomly
sampled from the surfaces of 3D objects (termed a point cloud). Although deep
learning models achieve human-like performance in recognizing objects from 3D
shapes, it remains unclear whether these models develop 3D shape
representations similar to those used by human vision for object recognition.
We hypothesize that training with 3D shapes enables models to form
representations of local geometric structures in 3D shapes. However, their
representations of global 3D object shapes may be limited. We conducted two
human experiments systematically manipulating point density and object
orientation (Experiment 1), and local geometric structure (Experiment 2).
Humans consistently performed well across all experimental conditions. We
compared two types of deep learning models, one based on a convolutional neural
network (DGCNN) and the other on visual transformers (point transformer), with
human performance. We found that the point transformer model provided a better
account of human performance than the convolution-based model. The advantage
mainly results from the mechanism in the point transformer model that supports
hierarchical abstraction of 3D shapes.

</details>


### [95] [A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends](https://arxiv.org/abs/2507.09861)
*Yihao Ding,Siwen Luo,Yue Dai,Yanbei Jiang,Zechuan Li,Geoffrey Martin,Yifan Peng*

Main category: cs.CV

TL;DR: 本文综述了多模态大语言模型在视觉丰富文档理解中的应用，介绍了编码与特征融合方法、训练范式及数据集，并探讨了领域中的挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着对复杂视觉、文本和布局信息的自动处理需求增加，视觉丰富文档理解成为一个关键领域。

Method: 本文从编码与特征融合方法、预训练策略、指令响应调优和模型模块可训练性等方面对多模态大语言模型的最新进展进行了评述。

Result: 总结了领域内所用的数据集及相关研究进展，并对当前面临的挑战与机遇进行了探讨。

Conclusion: 多模态大语言模型在视觉丰富文档理解中显示出潜力，但仍需在效率、通用性和稳健性方面进一步改进。

Abstract: Visually-Rich Document Understanding (VRDU) has emerged as a critical field,
driven by the need to automatically process documents containing complex
visual, textual, and layout information. Recently, Multimodal Large Language
Models (MLLMs) have shown remarkable potential in this domain, leveraging both
Optical Character Recognition (OCR)-dependent and OCR-free frameworks to
extract and interpret information in document images. This survey reviews
recent advancements in MLLM-based VRDU, highlighting three core components: (1)
methods for encoding and fusing textual, visual, and layout features; (2)
training paradigms, including pretraining strategies, instruction-response
tuning, and the trainability of different model modules; and (3) datasets
utilized for pretraining, instruction-tuning, and supervised fine-tuning.
Finally, we discuss the challenges and opportunities in this evolving field and
propose future directions to advance the efficiency, generalizability, and
robustness of VRDU systems.

</details>


### [96] [SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation](https://arxiv.org/abs/2507.09862)
*Youliang Zhang,Zhaoyang Li,Duomin Wang,Jiahe Zhang,Deyu Zhou,Zixin Yin,Xili Dai,Gang Yu,Xiu Li*

Main category: cs.CV

TL;DR: 该论文介绍了SpeakerVid-5M，一个用于生成视听交互虚拟人类的大规模高质量数据集。


<details>
  <summary>Details</summary>
Motivation: 为了应对视听在交互虚拟人的挑战，研究者开发了一个大规模数据集以推动领域发展。

Method: 提出了一个名为SpeakerVid-5M的数据集，包含8743小时、520万个人像视频片段，分为多种交互类型，并提供监督微调的高质量子集和大规模预训练子集。同时提供基于自回归模型的视频聊天基线和评测试基准。

Result: 该数据集能支持各种2D虚拟人类任务，提出了用于视频聊天的基准方法（VidChatBench）。

Conclusion: 研究公开了数据集及处理代码，为未来的视听交互虚拟人类领域提供了新资源和评测方法。

Abstract: The rapid development of large-scale models has catalyzed significant
breakthroughs in the digital human domain. These advanced methodologies offer
high-fidelity solutions for avatar driving and rendering, leading academia to
focus on the next major challenge: audio-visual dyadic interactive virtual
human. To facilitate research in this emerging area, we present SpeakerVid-5M
dataset, the first large-scale, high-quality dataset designed for audio-visual
dyadic interactive virtual human generation. Totaling over 8,743 hours,
SpeakerVid-5M contains more than 5.2 million video clips of human portraits. It
covers diverse scales and interaction types, including monadic talking,
listening, and dyadic conversations. Crucially, the dataset is structured along
two key dimensions: interaction type and data quality. First, it is categorized
into four types (dialogue branch, single branch, listening branch and
multi-turn branch) based on the interaction scenario. Second, it is stratified
into a large-scale pre-training subset and a curated, high-quality subset for
Supervised Fine-Tuning (SFT). This dual structure accommodates a wide array of
2D virtual human tasks. In addition, we provide an autoregressive (AR)-based
video chat baseline trained on this data, accompanied by a dedicated set of
metrics and test data to serve as a benchmark VidChatBench for future work.
Both the dataset and the corresponding data processing code will be publicly
released. Project page: https://dorniwang.github.io/SpeakerVid-5M/

</details>


### [97] [OpenHuman4D: Open-Vocabulary 4D Human Parsing](https://arxiv.org/abs/2507.09880)
*Keito Suzuki,Bang Du,Runfa Blark Li,Kunyao Chen,Lei Wang,Peng Liu,Ning Bi,Truong Nguyen*

Main category: cs.CV

TL;DR: 本文提出了首个4D人体解析框架，旨在加速推理并引入开放词汇功能，其性能较现有方法大幅提升。


<details>
  <summary>Details</summary>
Motivation: 虚拟与扩展现实应用中对动态3D人体表示的需求日益增长，现有方法局限于封闭数据集且推理时间较长，应用受限。

Method: 引入基于视频物体追踪、Mask校验模块与4D Mask融合模块的框架，有效提升性能与灵活性。

Result: 实验显示，该方法在4D人体解析任务中显著加速推理（高达93.3%），灵活性胜过仅限固定类别的前沿方法。

Conclusion: 该方法成功解决了推理速度及开放词汇问题，为4D人体解析任务提供了新思路。

Abstract: Understanding dynamic 3D human representation has become increasingly
critical in virtual and extended reality applications. However, existing human
part segmentation methods are constrained by reliance on closed-set datasets
and prolonged inference times, which significantly restrict their
applicability. In this paper, we introduce the first 4D human parsing framework
that simultaneously addresses these challenges by reducing the inference time
and introducing open-vocabulary capabilities. Building upon state-of-the-art
open-vocabulary 3D human parsing techniques, our approach extends the support
to 4D human-centric video with three key innovations: 1) We adopt mask-based
video object tracking to efficiently establish spatial and temporal
correspondences, avoiding the necessity of segmenting all frames. 2) A novel
Mask Validation module is designed to manage new target identification and
mitigate tracking failures. 3) We propose a 4D Mask Fusion module, integrating
memory-conditioned attention and logits equalization for robust embedding
fusion. Extensive experiments demonstrate the effectiveness and flexibility of
the proposed method on 4D human-centric parsing tasks, achieving up to 93.3%
acceleration compared to the previous state-of-the-art method, which was
limited to parsing fixed classes.

</details>


### [98] [Counterfactual Visual Explanation via Causally-Guided Adversarial Steering](https://arxiv.org/abs/2507.09881)
*Yiran Qiao,Disheng Liu,Yiren Lu,Yu Yin,Mengnan Du,Jing Ma*

Main category: cs.CV

TL;DR: CECAS提出了一种基于因果指导的反事实解释生成方法，表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有反事实视觉解释方法忽视了图像生成过程中的因果关系和虚假相关性，导致解释质量有限。

Method: 引入CECAS框架，采用因果指导的对抗方法生成反事实解释，避免对虚假因素的干扰。

Result: 实验表明CECAS在多个基准数据集上的性能超越现有方法，在有效性、稀疏性、邻近性和真实感之间实现了平衡。

Conclusion: CECAS通过融合因果视角，提高了反事实视觉解释的生成质量，提供更清晰的AI模型解释。

Abstract: Recent work on counterfactual visual explanations has contributed to making
artificial intelligence models more explainable by providing visual
perturbation to flip the prediction. However, these approaches neglect the
causal relationships and the spurious correlations behind the image generation
process, which often leads to unintended alterations in the counterfactual
images and renders the explanations with limited quality. To address this
challenge, we introduce a novel framework CECAS, which first leverages a
causally-guided adversarial method to generate counterfactual explanations. It
innovatively integrates a causal perspective to avoid unwanted perturbations on
spurious factors in the counterfactuals. Extensive experiments demonstrate that
our method outperforms existing state-of-the-art approaches across multiple
benchmark datasets and ultimately achieves a balanced trade-off among various
aspects of validity, sparsity, proximity, and realism.

</details>


### [99] [MCGA: Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention](https://arxiv.org/abs/2507.09885)
*Zhanjiang Yang,Lijun Sun,Jiawei Dong,Xiaoxin An,Yang Liu,Meng Li*

Main category: cs.CV

TL;DR: 提出了一种从RGB图像重建高光谱图像的高效方法（MCGA），通过两阶段策略解决低维到高维信息转换的挑战，并引入基于物理的轻量注意机制和测试时自适应策略，达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接学习RGB到高光谱的映射，忽略了从低维到高维信息转换的复杂性。

Method: 提出了一种两阶段策略。第一阶段使用多尺度VQ-VAE从高光谱数据中提取编码表混合；第二阶段通过这些编码表改进RGB到高光谱的映射，并加入基于物理的注意机制（灰度感知注意和量化自注意）。此外，引入了基于熵的测试时自适应策略提高鲁棒性。

Result: 实验表明，MCGA方法在高光谱重建任务上取得了领先的表现。

Conclusion: 两阶段策略结合基于物理的注意机制和测试时自适应，能够高效且准确地完成高光谱图像的重建。

Abstract: Reconstructing hyperspectral images (HSI) from RGB images is a cost-effective
solution for various vision-based applications. However, most existing
learning-based hyperspectral reconstruction methods directly learn the
RGB-to-HSI mapping using complex attention mechanisms, neglecting the inherent
challenge of transitioning from low-dimensional to high-dimensional
information. To address this limitation, we propose a two-stage approach, MCGA,
which first learns spectral patterns before estimating the mapping. In the
first stage, a multi-scale VQ-VAE learns representations from heterogeneous HSI
datasets, extracting a Mixture of Codebooks (MoC). In the second stage, the
RGB-to-HSI mapping is refined by querying features from the MoC to replace
latent HSI representations, incorporating prior knowledge rather than forcing a
direct high-dimensional transformation. To further enhance reconstruction
quality, we introduce Grayscale-Aware Attention and Quantized Self-Attention,
which adaptively adjust feature map intensities to meet hyperspectral
reconstruction requirements. This physically motivated attention mechanism
ensures lightweight and efficient HSI recovery. Moreover, we propose an
entropy-based Test-Time Adaptation strategy to improve robustness in real-world
scenarios. Extensive experiments demonstrate that our method, MCGA, achieves
state-of-the-art performance. The code and models will be released at
https://github.com/Fibonaccirabbit/MCGA

</details>


### [100] [Measuring the Impact of Rotation Equivariance on Aerial Object Detection](https://arxiv.org/abs/2507.09896)
*Xiuyu Wu,Xinhao Wang,Xiubin Zhu,Lan Yang,Jiyuan Liu,Xingchen Hu*

Main category: cs.CV

TL;DR: 提出了一种名为MessDet的多分支旋转等变单阶段检测器，在DOTA和DIOR-R等数据集上实现了最先进的检测性能，同时显著减少了参数量。


<details>
  <summary>Details</summary>
Motivation: 解决航空影像中旋转等变性对于目标检测性能的重要性，并探讨严格旋转等变性是否有助于提高检测性能。

Method: 构建了一个具有更高级网络结构的严格旋转等变主干和颈部网络，并提出了一个多分支头部网络，通过利用旋转等变特性的分组性来减少参数量并提高检测精度。

Result: 在DOTA-v1.0、DOTA-v1.5和DIOR-R等难度较高的航空影像数据集上，MessDet显示了出色的性能和极低的参数量。

Conclusion: 严格旋转等变性在航空影像目标检测中具有重要意义，所提方法证明了检测性能的提升和参数量的优化可以同时实现。

Abstract: Due to the arbitrary orientation of objects in aerial images, rotation
equivariance is a critical property for aerial object detectors. However,
recent studies on rotation-equivariant aerial object detection remain scarce.
Most detectors rely on data augmentation to enable models to learn
approximately rotation-equivariant features. A few detectors have constructed
rotation-equivariant networks, but due to the breaking of strict rotation
equivariance by typical downsampling processes, these networks only achieve
approximately rotation-equivariant backbones. Whether strict rotation
equivariance is necessary for aerial image object detection remains an open
question. In this paper, we implement a strictly rotation-equivariant backbone
and neck network with a more advanced network structure and compare it with
approximately rotation-equivariant networks to quantitatively measure the
impact of rotation equivariance on the performance of aerial image detectors.
Additionally, leveraging the inherently grouped nature of rotation-equivariant
features, we propose a multi-branch head network that reduces the parameter
count while improving detection accuracy. Based on the aforementioned
improvements, this study proposes the Multi-branch head rotation-equivariant
single-stage Detector (MessDet), which achieves state-of-the-art performance on
the challenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and DIOR-R with an
exceptionally low parameter count.

</details>


### [101] [IGD: Instructional Graphic Design with Multimodal Layer Generation](https://arxiv.org/abs/2507.09910)
*Yadong Qu,Shancheng Fang,Yuxin Wang,Xiaorui Wang,Zhineng Chen,Hongtao Xie,Yongdong Zhang*

Main category: cs.CV

TL;DR: 本文提出了一个名为IGD的系统，通过自然语言指令生成可编辑的多模态图层，优化图形设计的智能化与效率。


<details>
  <summary>Details</summary>
Motivation: 目前的图形设计方法缺乏创造力和智能化，现有扩散模型生成的设计文件在文本可读性和编辑灵活性方面表现不佳。

Method: 提出了基于参数化渲染与图像生成的新范式，通过多模态大语言模型（MLLM）实现布局和属性预测，并结合扩散模型生成图像资产。

Result: 实验结果证明，IGD在复杂图形设计任务中具备优越的性能，可以实现高效且灵活的自动化设计。

Conclusion: IGD为图形设计领域提供了一种新的解决方案，支持端到端训练，具有良好的可扩展性和实用性。

Abstract: Graphic design visually conveys information and data by creating and
combining text, images and graphics. Two-stage methods that rely primarily on
layout generation lack creativity and intelligence, making graphic design still
labor-intensive. Existing diffusion-based methods generate non-editable graphic
design files at image level with poor legibility in visual text rendering,
which prevents them from achieving satisfactory and practical automated graphic
design. In this paper, we propose Instructional Graphic Designer (IGD) to
swiftly generate multimodal layers with editable flexibility with only natural
language instructions. IGD adopts a new paradigm that leverages parametric
rendering and image asset generation. First, we develop a design platform and
establish a standardized format for multi-scenario design files, thus laying
the foundation for scaling up data. Second, IGD utilizes the multimodal
understanding and reasoning capabilities of MLLM to accomplish attribute
prediction, sequencing and layout of layers. It also employs a diffusion model
to generate image content for assets. By enabling end-to-end training, IGD
architecturally supports scalability and extensibility in complex graphic
design tasks. The superior experimental results demonstrate that IGD offers a
new solution for graphic design.

</details>


### [102] [Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios](https://arxiv.org/abs/2507.09915)
*Siyue Yao,Mingjie Sun,Eng Gee Lim,Ran Yi,Baojiang Zhong,Moncef Gabbouj*

Main category: cs.CV

TL;DR: 提出Crucial-Diff框架，通过生成关键样本缓解数据稀缺和不平衡问题，显著提高模型检测与分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决数据稀缺和样本不平衡导致模型过拟合及性能低下的问题，同时弥补现有方法在生成样本质量和效率方面的不足。

Method: 提出了一个名为Crucial-Diff的领域无关框架，包括两个模块：SAFE（场景无关特征提取器）提取目标信息，WASM（弱点感知样本挖掘器）根据下游模型反馈生成难检测样本，两者结合生成关键样本用于训练。

Result: 在MVTec数据集上实现了像素级AP为83.63%和F1-MAX为78.12%；在息肉数据集上达到mIoU为81.64%和mDice为87.69%。

Conclusion: Crucial-Diff框架显著提升了数据稀缺场景下模型的检测和分割性能，并有望推广到更多应用领域。

Abstract: The scarcity of data in various scenarios, such as medical, industry and
autonomous driving, leads to model overfitting and dataset imbalance, thus
hindering effective detection and segmentation performance. Existing studies
employ the generative models to synthesize more training samples to mitigate
data scarcity. However, these synthetic samples are repetitive or simplistic
and fail to provide "crucial information" that targets the downstream model's
weaknesses. Additionally, these methods typically require separate training for
different objects, leading to computational inefficiencies. To address these
issues, we propose Crucial-Diff, a domain-agnostic framework designed to
synthesize crucial samples. Our method integrates two key modules. The Scene
Agnostic Feature Extractor (SAFE) utilizes a unified feature extractor to
capture target information. The Weakness Aware Sample Miner (WASM) generates
hard-to-detect samples using feedback from the detection results of downstream
model, which is then fused with the output of SAFE module. Together, our
Crucial-Diff framework generates diverse, high-quality training data, achieving
a pixel-level AP of 83.63% and an F1-MAX of 78.12% on MVTec. On polyp dataset,
Crucial-Diff reaches an mIoU of 81.64% and an mDice of 87.69%. Code will be
released after acceptance.

</details>


### [103] [Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion Product Attributes? A Zero-Shot Analysis](https://arxiv.org/abs/2507.09950)
*Shubham Shukla,Kunal Sonalkar*

Main category: cs.CV

TL;DR: 本文研究了大语言模型（LLMs）在时尚商品属性识别中的表现，特别是在零样本场景下进行评估，发现Gemini 2.0 Flash优于GPT-4o-mini。


<details>
  <summary>Details</summary>
Motivation: 时尚零售需要以产品属性为核心，这不仅影响产品分类，还显著提升客户在电商网站上的产品发现体验。然而，目前LLMs在时尚领域细粒度属性识别能力的研究尚属空白。

Method: 作者使用DeepFashion-MultiModal数据集，以零样本评估的方式测试了两个专为性能和效率优化的LLMs：GPT-4o-mini和Gemini 2.0 Flash，仅利用图像信息作为输入进行分析。

Result: Gemini 2.0 Flash在18个类别的时尚属性测试中整体表现最优，宏F1评分为56.79%，GPT-4o-mini的评分为43.28%。

Conclusion: 研究表明，应结合领域特定的微调方法以优化LLMs在生产环境中的应用，并为未来的时尚AI和多模态属性提取研究奠定了基础。

Abstract: The fashion retail business is centered around the capacity to comprehend
products. Product attribution helps in comprehending products depending on the
business process. Quality attribution improves the customer experience as they
navigate through millions of products offered by a retail website. It leads to
well-organized product catalogs. In the end, product attribution directly
impacts the 'discovery experience' of the customer. Although large language
models (LLMs) have shown remarkable capabilities in understanding multimodal
data, their performance on fine-grained fashion attribute recognition remains
under-explored. This paper presents a zero-shot evaluation of state-of-the-art
LLMs that balance performance with speed and cost efficiency, mainly
GPT-4o-mini and Gemini 2.0 Flash. We have used the dataset
DeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal) to
evaluate these models in the attribution tasks of fashion products. Our study
evaluates these models across 18 categories of fashion attributes, offering
insight into where these models excel. We only use images as the sole input for
product information to create a constrained environment. Our analysis shows
that Gemini 2.0 Flash demonstrates the strongest overall performance with a
macro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a
macro F1 score of 43.28%. Through detailed error analysis, our findings provide
practical insights for deploying these LLMs in production e-commerce product
attribution-related tasks and highlight the need for domain-specific
fine-tuning approaches. This work also lays the groundwork for future research
in fashion AI and multimodal attribute extraction.

</details>


### [104] [4D-MISR: A unified model for low-dose super-resolution imaging via feature fusion](https://arxiv.org/abs/2507.09953)
*Zifei Wang,Zian Mao,Xiaoya He,Xi Huang,Haoran Zhang,Chun Cheng,Shufen Chu,Tingzheng Hou,Xiaoqin Zeng,Yujun Xie*

Main category: cs.CV

TL;DR: 本研究提出了一种基于多图像超分辨（MISR）的方法，用于电子显微镜在超低电子剂量条件下实现原子级分辨率成像，主要针对受辐射损害的材料。


<details>
  <summary>Details</summary>
Motivation: 电子显微镜揭示原子级结构的潜力因辐射损害而受限，尤其是对蛋白质和二维材料等敏感材料。研究目标是突破这一辐射限制并提高成像质量。

Method: 采用多图像超分辨（MISR）原理，综合低分辨率、亚像素位移视图，同时利用卷积神经网络（CNN）结合多角度观测特征，提出了4D-STEM双通路注意力引导网络模型。

Result: 此方法在超低剂量下实现了原子级超分辨率成像，可适用于无定形、半晶体和晶体等不同类型的受辐射损害材料。实验显示它的空间分辨率与传统ptychography方法在超低剂量条件下的表现相当。

Conclusion: 此研究拓展了4D-STEM的功能，为研究辐射敏感材料的结构分析提供了新的通用方法。

Abstract: While electron microscopy offers crucial atomic-resolution insights into
structure-property relationships, radiation damage severely limits its use on
beam-sensitive materials like proteins and 2D materials. To overcome this
challenge, we push beyond the electron dose limits of conventional electron
microscopy by adapting principles from multi-image super-resolution (MISR) that
have been widely used in remote sensing. Our method fuses multiple
low-resolution, sub-pixel-shifted views and enhances the reconstruction with a
convolutional neural network (CNN) that integrates features from synthetic,
multi-angle observations. We developed a dual-path, attention-guided network
for 4D-STEM that achieves atomic-scale super-resolution from ultra-low-dose
data. This provides robust atomic-scale visualization across amorphous,
semi-crystalline, and crystalline beam-sensitive specimens. Systematic
evaluations on representative materials demonstrate comparable spatial
resolution to conventional ptychography under ultra-low-dose conditions. Our
work expands the capabilities of 4D-STEM, offering a new and generalizable
method for the structural analysis of radiation-vulnerable materials.

</details>


### [105] [Uncertainty Quantification for Incomplete Multi-View Data Using Divergence Measures](https://arxiv.org/abs/2507.09980)
*Zhipeng Xue,Yan Zhang,Ming Li,Chun Li,Yue Liu,Fei Yu*

Main category: cs.CV

TL;DR: 本文提出了KPHD-Net，一种基于Hölder散度的多视图分类和聚类方法，通过引入Dempster-Shafer证据理论和卡尔曼滤波改进融合结果的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有多视图分类和聚类方法在面对噪声数据时，缺乏对多视图融合和最终决策的可靠性保障。为解决此问题，作者提出一种新的方法。

Method: KPHD-Net基于Hölder散度，利用变分狄利克雷分布表示类别概率分布，并结合Dempster-Shafer证据理论和卡尔曼滤波进行不确定性估计与融合。

Result: KPHD-Net在多视图分类和聚类任务中，准确性、鲁棒性及可靠性均优于现有方法，并有理论支持。

Conclusion: KPHD-Net通过改进不确定性估计和融合性能，为多视图学习任务提供了更可靠的解决方案。

Abstract: Existing multi-view classification and clustering methods typically improve
task accuracy by leveraging and fusing information from different views.
However, ensuring the reliability of multi-view integration and final decisions
is crucial, particularly when dealing with noisy or corrupted data. Current
methods often rely on Kullback-Leibler (KL) divergence to estimate uncertainty
of network predictions, ignoring domain gaps between different modalities. To
address this issue, KPHD-Net, based on H\"older divergence, is proposed for
multi-view classification and clustering tasks. Generally, our KPHD-Net employs
a variational Dirichlet distribution to represent class probability
distributions, models evidences from different views, and then integrates it
with Dempster-Shafer evidence theory (DST) to improve uncertainty estimation
effects. Our theoretical analysis demonstrates that Proper H\"older divergence
offers a more effective measure of distribution discrepancies, ensuring
enhanced performance in multi-view learning. Moreover, Dempster-Shafer evidence
theory, recognized for its superior performance in multi-view fusion tasks, is
introduced and combined with the Kalman filter to provide future state
estimations. This integration further enhances the reliability of the final
fusion results. Extensive experiments show that the proposed KPHD-Net
outperforms the current state-of-the-art methods in both classification and
clustering tasks regarding accuracy, robustness, and reliability, with
theoretical guarantees.

</details>


### [106] [Latent Diffusion Models with Masked AutoEncoders](https://arxiv.org/abs/2507.09984)
*Junho Lee,Jeongwoo Shin,Hyungwook Choi,Joonseok Lee*

Main category: cs.CV

TL;DR: 本研究专注于改进潜在扩散模型（LDMs）中的自动编码器设计，提出了一种结合了变分及掩码编码特点的改进方案，以提升图像生成性能。


<details>
  <summary>Details</summary>
Motivation: 尽管潜在扩散模型（LDMs）在图像生成中潜力巨大，但其核心模块——自动编码器的理想特性及优化设计未被充分研究。

Method: 分析了自动编码器在LDM框架中的角色，确立了潜在平滑性、感知压缩质量与重建质量三大关键特性；提出结合变分和掩码自编码结构的变分掩码自动编码器（VMAEs），并将其集成到LDM框架中。

Result: 实验表明，LDMAEs显著提升了图像生成质量与计算效率。

Conclusion: 研究表明，改进的自动编码器不仅能更好地满足关键特性需求，还能进一步提高潜在扩散模型的图像生成性能。

Abstract: In spite of remarkable potential of the Latent Diffusion Models (LDMs) in
image generation, the desired properties and optimal design of the autoencoders
have been underexplored. In this work, we analyze the role of autoencoders in
LDMs and identify three key properties: latent smoothness, perceptual
compression quality, and reconstruction quality. We demonstrate that existing
autoencoders fail to simultaneously satisfy all three properties, and propose
Variational Masked AutoEncoders (VMAEs), taking advantage of the hierarchical
features maintained by Masked AutoEncoder. We integrate VMAEs into the LDM
framework, introducing Latent Diffusion Models with Masked AutoEncoders
(LDMAEs). Through comprehensive experiments, we demonstrate significantly
enhanced image generation quality and computational efficiency.

</details>


### [107] [3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving](https://arxiv.org/abs/2507.09993)
*Yixun Zhang,Lizhi Wang,Junjun Zhao,Wending Zhao,Feng Zhou,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: 提出3DGAA，一种优化3D对象几何和外观的物理可实现对抗攻击方法，将目标检测mAP从87.21%降至7.38%。


<details>
  <summary>Details</summary>
Motivation: 现有2D和3D物理攻击很难在物理真实感和攻击鲁棒性之间平衡，针对这一弱点，开发新的对抗攻击方法验证自动驾驶感知系统的安全性是必要的。

Method: 提出3DGAA框架，使用3D Gaussian Splatting参数化，同时优化几何（形状、大小、旋转）和外观（颜色、不透明度），并通过物理过滤模块和物理增强模块提高几何保真度和攻击的物理适应性。

Result: 实验表明，3DGAA可以将目标检测mAP从87.21%降到7.38%，在虚拟基准和实际场景中表现出色，且具有较高的跨物理条件迁移性。

Conclusion: 3DGAA展示了对抗性物理攻击的新技术水平，是评估自动驾驶感知系统安全性的重要工具。

Abstract: Camera-based object detection systems play a vital role in autonomous
driving, yet they remain vulnerable to adversarial threats in real-world
environments. While existing 2D and 3D physical attacks typically optimize
texture, they often struggle to balance physical realism and attack robustness.
In this work, we propose 3D Gaussian-based Adversarial Attack (3DGAA), a novel
adversarial object generation framework that leverages the full 14-dimensional
parameterization of 3D Gaussian Splatting (3DGS) to jointly optimize geometry
and appearance in physically realizable ways. Unlike prior works that rely on
patches or texture, 3DGAA jointly perturbs both geometric attributes (shape,
scale, rotation) and appearance attributes (color, opacity) to produce
physically realistic and transferable adversarial objects. We further introduce
a physical filtering module to preserve geometric fidelity, and a physical
augmentation module to simulate complex physical scenarios, thus enhancing
attack generalization under real-world conditions. We evaluate 3DGAA on both
virtual benchmarks and physical-world setups using miniature vehicle models.
Experimental results show that 3DGAA achieves to reduce the detection mAP from
87.21% to 7.38%, significantly outperforming existing 3D physical attacks.
Moreover, our method maintains high transferability across different physical
conditions, demonstrating a new state-of-the-art in physically realizable
adversarial attacks. These results validate 3DGAA as a practical attack
framework for evaluating the safety of perception systems in autonomous
driving.

</details>


### [108] [Leveraging Swin Transformer for enhanced diagnosis of Alzheimer's disease using multi-shell diffusion MRI](https://arxiv.org/abs/2507.09996)
*Quentin Dessain,Nicolas Delinte,Bernard Hanseeuw,Laurence Dricot,Benoît Macq*

Main category: cs.CV

TL;DR: 本研究利用多壳层扩散MRI数据和基于视觉变换器的深度学习框架，用于早期阿尔茨海默病诊断和淀粉样蛋白积累检测。


<details>
  <summary>Details</summary>
Motivation: 探索利用多壳层扩散MRI和先进深度学习模型提高阿尔茨海默病早期诊断和淀粉样蛋白检测的效率和准确率。

Method: 提出一个基于Swin Transformer的分类流程。使用DTI和NODDI提取指标投射到2D平面，通过ImageNet预训练模型迁移学习，并结合低秩适配方法有效处理有限标签数据的神经影像分析。

Result: 分类框架在多壳层dMRI特征分类中表现出优异性能，最高平衡准确率为95.2%用于区分认知正常与阿尔茨海默症患者。同时在淀粉样蛋白检测中达到77.2%的平衡准确率。

Conclusion: 研究表明，扩散MRI结合变换器模型在阿尔茨海默症及相关病理的早期检测方面具有潜力，支持在数据有限的医学环境中用于生物标志物驱动的诊断。

Abstract: Objective: This study aims to support early diagnosis of Alzheimer's disease
and detection of amyloid accumulation by leveraging the microstructural
information available in multi-shell diffusion MRI (dMRI) data, using a vision
transformer-based deep learning framework.
  Methods: We present a classification pipeline that employs the Swin
Transformer, a hierarchical vision transformer model, on multi-shell dMRI data
for the classification of Alzheimer's disease and amyloid presence. Key metrics
from DTI and NODDI were extracted and projected onto 2D planes to enable
transfer learning with ImageNet-pretrained models. To efficiently adapt the
transformer to limited labeled neuroimaging data, we integrated Low-Rank
Adaptation. We assessed the framework on diagnostic group prediction
(cognitively normal, mild cognitive impairment, Alzheimer's disease dementia)
and amyloid status classification.
  Results: The framework achieved competitive classification results within the
scope of multi-shell dMRI-based features, with the best balanced accuracy of
95.2% for distinguishing cognitively normal individuals from those with
Alzheimer's disease dementia using NODDI metrics. For amyloid detection, it
reached 77.2% balanced accuracy in distinguishing amyloid-positive mild
cognitive impairment/Alzheimer's disease dementia subjects from
amyloid-negative cognitively normal subjects, and 67.9% for identifying
amyloid-positive individuals among cognitively normal subjects. Grad-CAM-based
explainability analysis identified clinically relevant brain regions, including
the parahippocampal gyrus and hippocampus, as key contributors to model
predictions.
  Conclusion: This study demonstrates the promise of diffusion MRI and
transformer-based architectures for early detection of Alzheimer's disease and
amyloid pathology, supporting biomarker-driven diagnostics in data-limited
biomedical settings.

</details>


### [109] [Vision-Based Anti Unmanned Aerial Technology: Opportunities and Challenges](https://arxiv.org/abs/2507.10006)
*Guanghai Ding,Yihua Ren,Yuting Liu,Qijun Zhao,Shuiwang Li*

Main category: cs.CV

TL;DR: 论文讨论了无人机(UAV)反追踪技术的发展，特别是基于计算机视觉和多传感器数据融合的方法，以及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着无人机技术的快速发展及其在军事侦察、环境监测、物流等领域的广泛应用，反无人机的高效追踪变得愈发重要，尤其是在公共安全、边境巡逻等复杂场景中。

Method: 论文回顾了反无人机检测与追踪技术的特点及挑战；汇编了公开数据集，并提供了访问链接；分析了近年来主要的视觉和多传感器融合的检测与追踪算法。

Result: 研究梳理当前主流的反无人机检测算法及数据集，为研究者提供了参考；并指出基于视觉与传感器融合的技术具有优势。

Conclusion: 通过总结现状与分析，提出未来的研究方向，可为反无人机追踪技术领域的进步提供重要参考。

Abstract: With the rapid advancement of UAV technology and its extensive application in
various fields such as military reconnaissance, environmental monitoring, and
logistics, achieving efficient and accurate Anti-UAV tracking has become
essential. The importance of Anti-UAV tracking is increasingly prominent,
especially in scenarios such as public safety, border patrol, search and
rescue, and agricultural monitoring, where operations in complex environments
can provide enhanced security. Current mainstream Anti-UAV tracking
technologies are primarily centered around computer vision techniques,
particularly those that integrate multi-sensor data fusion with advanced
detection and tracking algorithms. This paper first reviews the characteristics
and current challenges of Anti-UAV detection and tracking technologies. Next,
it investigates and compiles several publicly available datasets, providing
accessible links to support researchers in efficiently addressing related
challenges. Furthermore, the paper analyzes the major vision-based and
vision-fusion-based Anti-UAV detection and tracking algorithms proposed in
recent years. Finally, based on the above research, this paper outlines future
research directions, aiming to provide valuable insights for advancing the
field.

</details>


### [110] [Binomial Self-Compensation: Mechanism and Suppression of Motion Error in Phase-Shifting Profilometry](https://arxiv.org/abs/2507.10009)
*Geyou Zhang,Kai Liu,Ce Zhu*

Main category: cs.CV

TL;DR: 文章提出了一种改进的图像序列二项自补偿(I-BSC)方法，用于在动态测量环境下减少PSP方法中的运动误差，同时提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统PSP假设目标物体静止，因此在动态测量中易受运动影响。需要一种新方法来解决此不足，并减小误差。

Method: 提出了基于图像序列的二项自补偿(I-BSC)算法，利用同质条纹图像的加权求和并仅需一次反正切计算，相较于分阶段权重求和的P-BSC，大幅降低了计算复杂度。

Result: I-BSC在减少动态测量误差方面优于已有方法，实现了准单帧速率3D重建；同时，计算复杂度降低一个多项式阶，计算帧速率加快数倍至十余倍。

Conclusion: I-BSC成功解决了P-BSC的高计算开销和误差累积问题，表现出更快的误差收敛速度和更高的动态测量精度。

Abstract: Phase shifting profilometry (PSP) is widely used in high-precision 3D
scanning due to its high accuracy, robustness, and pixel-wise handling.
However, a fundamental assumption of PSP that the object should remain static
does not hold in dynamic measurement, making PSP susceptible to object motion.
To address this challenge, our proposed solution, phase-sequential binomial
self-compensation (P-BSC), sums successive motion-affected phase frames
weighted by binomial coefficients. This approach exponentially reduces the
motion error in a pixel-wise and frame-wise loopable manner. Despite its
efficacy, P-BSC suffers from high computational overhead and error accumulation
due to its reliance on multi-frame phase calculations and weighted summations.
Inspired by P-BSC, we propose an image-sequential binomial self-compensation
(I-BSC) to weight sum the homogeneous fringe images instead of successive phase
frames, which generalizes the BSC concept from phase sequences to image
sequences. I-BSC computes the arctangent function only once, resolving both
limitations in P-BSC. Extensive analysis, simulations, and experiments show
that 1) the proposed BSC outperforms existing methods in reducing motion error
while achieving a quasi-single-shot frame rate, i.e., depth map frame rate
equals to the camera's acquisition rate, enabling 3D reconstruction with high
pixel-depth-temporal resolution; 2) compared to P-BSC, our I-BSC reduces the
computational complexity by one polynomial order, thereby accelerating the
computational frame rate by several to dozen times, while also reaching faster
motion error convergence.

</details>


### [111] [(Almost) Free Modality Stitching of Foundation Models](https://arxiv.org/abs/2507.10015)
*Jaisidh Singh,Diganta Misra,Boris Knyazev,Antonio Orvieto*

Main category: cs.CV

TL;DR: 提出了一种名为Hyma的框架，使用超网络解决单模态模型选择和连接器训练问题，减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 目前设计多模态模型需要多个预训练单模态模型的组合，其连接器的训练过程复杂且计算成本高。

Method: 利用超网络的参数预测能力，通过Hyma框架联合训练适用于多组单模态模型组合的连接器模块。

Result: 实验表明，Hyma框架将最佳单模态模型选择的搜索成本降低了10倍，同时匹配了网格搜索的排序和训练性能。

Conclusion: Hyma提供了一种高效且性能优良的解决方案，用于多模态模型的单模态模型选择和连接器训练。

Abstract: Foundation multi-modal models are often designed by stitching of multiple
existing pretrained uni-modal models: for example, an image classifier with an
autoregressive text model. This stitching process is performed by training a
connector module that aims to align the representation-representation or
representation-input spaces of these uni-modal models. However, given the
complexity of training such connectors on large scale web-based datasets
coupled with the ever-increasing number of available pretrained uni-modal
models, the task of uni-modal models selection and subsequent connector module
training becomes computationally demanding. To address this under-studied
critical problem, we propose Hypernetwork Model Alignment (Hyma), a novel
all-in-one solution for optimal uni-modal model selection and connector
training by leveraging hypernetworks. Specifically, our framework utilizes the
parameter prediction capability of a hypernetwork to obtain jointly trained
connector modules for $N \times M$ combinations of uni-modal models. In our
experiments, Hyma reduces the optimal uni-modal model pair search cost by
$10\times$ (averaged across all experiments), while matching the ranking and
trained connector performance obtained via grid search across a suite of
diverse multi-modal benchmarks.

</details>


### [112] [Memory-Efficient Personalization of Text-to-Image Diffusion Models via Selective Optimization Strategies](https://arxiv.org/abs/2507.10029)
*Seokeon Choi,Sunghyun Park,Hyoungwoo Park,Jeongho Kim,Sungrack Yun*

Main category: cs.CV

TL;DR: 本文提出了一种用于文本到图像扩散模型的记忆高效个性化方法，通过结合低分辨率图像后向传播（BP-low）和高分辨率图像零阶优化（ZO-high），实现了高效个性化和低内存高质量微调。


<details>
  <summary>Details</summary>
Motivation: 目前在边缘设备有限计算资源下实现文本到图像扩散模型的个性化调整存在内存效率和用户隐私问题。

Method: 本文框架结合两种优化策略：在低分辨率图像上进行后向传播（BP-low）和高分辨率图像零阶优化（ZO-high），并引入根据扩散时间步动态选择优化策略的时间步感知概率函数。

Result: 实验结果表明，该方法在大幅降低内存消耗的同时实现了竞争性的性能，支持可扩展、高质量的设备端个性化调整，并且不会增加推理延迟。

Conclusion: 通过结合BP-low和ZO-high的优势，并采用时间步感知选择机制，该方法实现了记忆高效且高质量的个性化微调，为边缘设备上个性化模型调整提供了创新解决方案。

Abstract: Memory-efficient personalization is critical for adapting text-to-image
diffusion models while preserving user privacy and operating within the limited
computational resources of edge devices. To this end, we propose a selective
optimization framework that adaptively chooses between backpropagation on
low-resolution images (BP-low) and zeroth-order optimization on high-resolution
images (ZO-high), guided by the characteristics of the diffusion process. As
observed in our experiments, BP-low efficiently adapts the model to
target-specific features, but suffers from structural distortions due to
resolution mismatch. Conversely, ZO-high refines high-resolution details with
minimal memory overhead but faces slow convergence when applied without prior
adaptation. By complementing both methods, our framework leverages BP-low for
effective personalization while using ZO-high to maintain structural
consistency, achieving memory-efficient and high-quality fine-tuning. To
maximize the efficacy of both BP-low and ZO-high, we introduce a timestep-aware
probabilistic function that dynamically selects the appropriate optimization
strategy based on diffusion timesteps. This function mitigates the overfitting
from BP-low at high timesteps, where structural information is critical, while
ensuring ZO-high is applied more effectively as training progresses.
Experimental results demonstrate that our method achieves competitive
performance while significantly reducing memory consumption, enabling scalable,
high-quality on-device personalization without increasing inference latency.

</details>


### [113] [LifelongPR: Lifelong knowledge fusion for point cloud place recognition based on replay and prompt learning](https://arxiv.org/abs/2507.10034)
*Xianghong Zou,Jianping Li,Zhe Chen,Zhen Cao,Zhen Dong,Qiegen Liu,Bisheng Yang*

Main category: cs.CV

TL;DR: 提出了LifelongPR，一个新的持续学习框架，用于解决点云位置识别(PCPR)中的灾难性遗忘问题，并通过实验验证其比现有方法有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有PCPR模型在适应新环境或传感器类型时经常出现灾难性遗忘问题，导致性能下降和实用性受限。研究动机是解决这些问题并提升模型的可扩展性和部署效率。

Method: 提出LifelongPR框架，包括一个动态样本选择方法以缓解知识丢失，以及一个基于提示学习的两阶段训练策略，通过轻量化模块进行特征域适配并最小化遗忘。

Result: 在大规模公开数据集和自收集数据上的实验表明，与最先进方法相比，LifelongPR在mIR@1、mR@1指标上分别有6.50%和7.96%的提升，F值降低8.95%。

Conclusion: 该方法有效解决了现有PCPR模型的灾难性遗忘问题，显著提升了模型在各种场景下的性能，并公开了代码供进一步研究。

Abstract: Point cloud place recognition (PCPR) plays a crucial role in photogrammetry
and robotics applications such as autonomous driving, intelligent
transportation, and augmented reality. In real-world large-scale deployments of
a positioning system, PCPR models must continuously acquire, update, and
accumulate knowledge to adapt to diverse and dynamic environments, i.e., the
ability known as continual learning (CL). However, existing PCPR models often
suffer from catastrophic forgetting, leading to significant performance
degradation in previously learned scenes when adapting to new environments or
sensor types. This results in poor model scalability, increased maintenance
costs, and system deployment difficulties, undermining the practicality of
PCPR. To address these issues, we propose LifelongPR, a novel continual
learning framework for PCPR, which effectively extracts and fuses knowledge
from sequential point cloud data. First, to alleviate the knowledge loss, we
propose a replay sample selection method that dynamically allocates sample
sizes according to each dataset's information quantity and selects spatially
diverse samples for maximal representativeness. Second, to handle domain
shifts, we design a prompt learning-based CL framework with a lightweight
prompt module and a two-stage training strategy, enabling domain-specific
feature adaptation while minimizing forgetting. Comprehensive experiments on
large-scale public and self-collected datasets are conducted to validate the
effectiveness of the proposed method. Compared with state-of-the-art (SOTA)
methods, our method achieves 6.50% improvement in mIR@1, 7.96% improvement in
mR@1, and an 8.95% reduction in F. The code and pre-trained models are publicly
available at https://github.com/zouxianghong/LifelongPR.

</details>


### [114] [CoSMo: A Multimodal Transformer for Page Stream Segmentation in Comic Books](https://arxiv.org/abs/2507.10053)
*Marc Serra Ortega,Emanuele Vivoli,Artemis Llabrés,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: 提出了一种称为CoSMo的新型多模态Transformer，用于漫画书中页码流分割。


<details>
  <summary>Details</summary>
Motivation: 为解决漫画书中自动化内容理解问题，例如角色分析、故事索引等提供基础。

Method: 提出CoSMo模型，结合视觉与多模态数据训练，并使用20,800页注释数据集进行评估。

Result: 相比传统基线和大规模通用视觉-语言模型，CoSMo在多个评估指标上表现更佳，展示了视觉特征的优势及多模态的补充效益。

Conclusion: CoSMo为漫画书分析设定了新的性能标准，并为未来研究提供了有力工具。

Abstract: This paper introduces CoSMo, a novel multimodal Transformer for Page Stream
Segmentation (PSS) in comic books, a critical task for automated content
understanding, as it is a necessary first stage for many downstream tasks like
character analysis, story indexing, or metadata enrichment. We formalize PSS
for this unique medium and curate a new 20,800-page annotated dataset. CoSMo,
developed in vision-only and multimodal variants, consistently outperforms
traditional baselines and significantly larger general-purpose vision-language
models across F1-Macro, Panoptic Quality, and stream-level metrics. Our
findings highlight the dominance of visual features for comic PSS
macro-structure, yet demonstrate multimodal benefits in resolving challenging
ambiguities. CoSMo establishes a new state-of-the-art, paving the way for
scalable comic book analysis.

</details>


### [115] [Lightweight Model for Poultry Disease Detection from Fecal Images Using Multi-Color Space Feature Optimization and Machine Learning](https://arxiv.org/abs/2507.10056)
*A. K. M. Shoriful Islam,Md. Rakib Hassan,Macbah Uddin,Md. Shahidur Rahman*

Main category: cs.CV

TL;DR: 本研究提出了一种基于轻量级机器学习的方法，通过分析鸡粪图像检测疾病，以低资源需求实现高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有家禽养殖业易受感染性疾病影响，且深度学习方法资源需求高，限制了低资源环境中的应用。

Method: 利用多彩空间特征提取和各种颜色、纹理、形状描述符，结合PCA和XGBoost进行特征选择，并用ANN分类器进行训练。

Result: 所提出的模型在谷歌Colab平台上无需GPU实现了95.85%的准确率，执行时间为638秒，相较Xception和MobileNetV3准确性相当但资源需求大幅降低。

Conclusion: 该研究提供了一种成本低、可扩展且易解释的家禽疾病检测替代方案，适用于低资源农业环境。

Abstract: Poultry farming is a vital component of the global food supply chain, yet it
remains highly vulnerable to infectious diseases such as coccidiosis,
salmonellosis, and Newcastle disease. This study proposes a lightweight machine
learning-based approach to detect these diseases by analyzing poultry fecal
images. We utilize multi-color space feature extraction (RGB, HSV, LAB) and
explore a wide range of color, texture, and shape-based descriptors, including
color histograms, local binary patterns (LBP), wavelet transforms, and edge
detectors. Through a systematic ablation study and dimensionality reduction
using PCA and XGBoost feature selection, we identify a compact global feature
set that balances accuracy and computational efficiency. An artificial neural
network (ANN) classifier trained on these features achieved 95.85% accuracy
while requiring no GPU and only 638 seconds of execution time in Google Colab.
Compared to deep learning models such as Xception and MobileNetV3, our proposed
model offers comparable accuracy with drastically lower resource usage. This
work demonstrates a cost-effective, interpretable, and scalable alternative to
deep learning for real-time poultry disease detection in low-resource
agricultural settings.

</details>


### [116] [MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second](https://arxiv.org/abs/2507.10065)
*Chenguo Lin,Yuchen Lin,Panwang Pan,Yifan Yu,Honglei Yan,Katerina Fragkiadaki,Yadong Mu*

Main category: cs.CV

TL;DR: MoVieS是一种创新的基于单目视频的4D动态视图生成模型，以高效方式实现新视图合成、重建和3D点跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有的动态场景建模和视图合成方法速度较慢且需要复杂的监控，限制了其扩展性和零样本任务的支持。

Method: MoVieS通过高斯原语的像素对齐网格建模动态3D场景，显式监督其随时间变化的运动，结合新视图合成与动态几何重建，统一外观、几何与运动的表征。

Result: MoVieS在多个任务中验证了其有效性与高效性，性能具有竞争力，并实现了数个数量级的速度提升。

Conclusion: MoVieS提供了一种统一的学习框架，不仅加速了动态新视图合成任务，还支持多种零样本应用场景。

Abstract: We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic
novel views from monocular videos in one second. MoVieS represents dynamic 3D
scenes using pixel-aligned grids of Gaussian primitives, explicitly supervising
their time-varying motion. This allows, for the first time, the unified
modeling of appearance, geometry and motion, and enables view synthesis,
reconstruction and 3D point tracking within a single learning-based framework.
By bridging novel view synthesis with dynamic geometry reconstruction, MoVieS
enables large-scale training on diverse datasets with minimal dependence on
task-specific supervision. As a result, it also naturally supports a wide range
of zero-shot applications, such as scene flow estimation and moving object
segmentation. Extensive experiments validate the effectiveness and efficiency
of MoVieS across multiple tasks, achieving competitive performance while
offering several orders of magnitude speedups.

</details>


### [117] [Frequency Regulation for Exposure Bias Mitigation in Diffusion Models](https://arxiv.org/abs/2507.10072)
*Meng Yu,Kun Zhan*

Main category: cs.CV

TL;DR: 本文通过研究扩散模型在生成过程中能量变化的规律，提出了一种基于频域的无训练、高适应性的调节机制，有效提高了扩散模型的生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的生成能力虽强，但受曝光偏差（exposure bias）的负面影响显著，需解决此问题以提升生成质量。

Method: 通过分析扩散过程中特征频段的能量变化模式，结合小波变换方法提出频域调节机制，分别调整低频和高频子带。此外，结合能量变化引入对曝光偏差更精确的分析方法，且整个机制无需重新训练直接可用。

Result: 该方法显著提升了不同架构扩散模型的生成质量，解决了扩散模型的曝光偏差问题。

Conclusion: 本文提出的方法是一种适用于多种扩散模型的轻量级、效果显著的解决方案，为后续扩散模型的改进方向提供了新思路。

Abstract: Diffusion models exhibit impressive generative capabilities but are
significantly impacted by exposure bias. In this paper, we make a key
observation: the energy of the predicted noisy images decreases during the
diffusion process. Building on this, we identify two important findings: 1) The
reduction in energy follows distinct patterns in the low-frequency and
high-frequency subbands; 2) This energy reduction results in amplitude
variations between the network-reconstructed clean data and the real clean
data. Based on the first finding, we introduce a frequency-domain regulation
mechanism utilizing wavelet transforms, which separately adjusts the low- and
high-frequency subbands. Leveraging the second insight, we provide a more
accurate analysis of exposure bias in the two subbands. Our method is
training-free and plug-and-play, significantly improving the generative quality
of various diffusion models and providing a robust solution to exposure bias
across different model architectures. The source code is available at
https://github.com/kunzhan/wpp.

</details>


### [118] [A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area](https://arxiv.org/abs/2507.10084)
*Haonan Chen,Xin Tong*

Main category: cs.CV

TL;DR: 提出一种基于SegFormer模型的两阶段迁移学习策略，显著提升遥感图像水体分割任务的精度。


<details>
  <summary>Details</summary>
Motivation: 解决遥感图像中由于领域转移和小样本问题导致的分割精度下降。

Method: 采用两阶段迁移学习策略，首先在多样化的源域上训练基础分割模型，然后在目标域数据上进行微调。

Result: 在具有复杂地形和光谱特性的西藏扎达土林区域实验结果表明，水体分割任务的IoU从直接迁移的25.50%提高到64.84%。

Conclusion: 该策略有效缓解了领域差异导致的模型性能下降问题，为数据稀缺且环境独特的遥感场景提供了高精度信息提取的技术范式。

Abstract: To address the prevalent challenges of domain shift and small sample sizes in
remote sensing image water body segmentation, this study proposes and validates
a two-stage transfer learning strategy based on the SegFormer model. The
approach begins by training a foundational segmentation model on a diverse
source domain, where it achieves an Intersection over Union (IoU) of 68.80% on
its validation set, followed by fine-tuning on data from the distinct target
domain. Focusing on the Zhada Tulin area in Tibet -- a region characterized by
highly complex topography and spectral features -- the experimental results
demonstrate that this strategy significantly boosts the IoU for the water body
segmentation task from 25.50% (for direct transfer) to 64.84%. This not only
effectively resolves the model performance degradation caused by domain
discrepancy but also provides an effective technical paradigm for
high-precision thematic information extraction in data-scarce and
environmentally unique remote sensing scenarios.

</details>


### [119] [FIX-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic Captions for Better Understanding of Long Text](https://arxiv.org/abs/2507.10095)
*Bingchao Wang,Zhiwei Ning,Jianyu Ding,Xuanang Gao,Yin Li,Dongsheng Jiang,Jie Yang,Wei Liu*

Main category: cs.CV

TL;DR: FIX-CLIP针对CLIP在处理长文本任务中的表现不足，提出了多项改进策略，包括双分支训练、区域提示和层次特征对齐，同时使用丰富的数据进行训练，大幅提升了CLIP处理长文本任务的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的CLIP模型受限于文本编码器的输入长度，在长文本任务中表现较弱，因此需要改进提升其对长文本任务的处理能力。

Method: FIX-CLIP提出三种改进模块：一种双分支训练管道，增强长文本表示能力并保持对短文本的处理能力；使用多区域学习提示与单向掩码的Transformer进行区域信息提取；在中间编码层引入层次特征对齐模块，提升多尺度特征一致性。同时收集30M图片并利用现有多模态语言模型生成长文本描述进行训练。

Result: FIX-CLIP在长文本和短文本检索基准上均达到了最新的性能水平，同时其文本编码器在处理长文本输入的扩散模型中表现出色，具备即插即用特性。

Conclusion: FIX-CLIP有效改进了CLIP模型，使其支持长文本任务，性能表现优异，展现出广泛的应用潜力。

Abstract: CLIP has shown promising performance across many short-text tasks in a
zero-shot manner. However, limited by the input length of the text encoder,
CLIP struggles on under-stream tasks with long-text inputs (>77 tokens). To
remedy this issue, we propose FIX-CLIP which includes three novel modules: (1)
A dual-branch training pipeline that aligns short and long texts with masked
and raw images respectively, which boosts the long-text representation while
preserving the short-text ability. (2) Multiple learnable regional prompts with
unidirectional masks in Transformer layers for regional information extraction.
(3) A hierarchical feature alignment module in the intermediate encoder layers
to promote the consistency of multi-scale features. Furthermore, we collect 30M
images and utilize existing MLLMs to synthesize long-text captions for
training. Extensive experiments show that FIX-CLIP achieves state-of-the-art
performance on both long-text and short-text retrieval benchmarks. For
downstream applications, we reveal that FIX-CLIP's text encoder delivers
promising performance in a plug-and-play manner for diffusion models with
long-text input.

</details>


### [120] [Glance-MCMT: A General MCMT Framework with Glance Initialization and Progressive Association](https://arxiv.org/abs/2507.10115)
*Hamidreza Hashempoor*

Main category: cs.CV

TL;DR: 提出一种多摄像机多目标跟踪框架，通过轨迹与外观特征保证跨视图的一致性。


<details>
  <summary>Details</summary>
Motivation: 研究如何在多摄像机情况下进行目标跟踪，并保持全球身份一致性。

Method: 基于BoT-SORT实现单摄像机跟踪，采用轨迹特征匹配初始化全球ID，后续通过优先的全局匹配策略进行更新，并使用深度图和校准估计3D位置进行空间验证。

Result: 实现了跨视图目标跟踪的流程，有效生成一致的全球身份。

Conclusion: 通过结合轨迹特征匹配和3D空间验证，该方法提高了多摄像机目标跟踪的一致性与准确性。

Abstract: We propose a multi-camera multi-target (MCMT) tracking framework that ensures
consistent global identity assignment across views using trajectory and
appearance cues. The pipeline starts with BoT-SORT-based single-camera
tracking, followed by an initial glance phase to initialize global IDs via
trajectory-feature matching. In later frames, new tracklets are matched to
existing global identities through a prioritized global matching strategy. New
global IDs are only introduced when no sufficiently similar trajectory or
feature match is found. 3D positions are estimated using depth maps and
calibration for spatial validation.

</details>


### [121] [DEARLi: Decoupled Enhancement of Recognition and Localization for Semi-supervised Panoptic Segmentation](https://arxiv.org/abs/2507.10118)
*Ivan Martinović,Josip Šarić,Marin Oršić,Matej Kristan,Siniša Šegvić*

Main category: cs.CV

TL;DR: 提出一种基于基础模型的半监督全景分割方法DEARLi，可以在标注数据有限的情况下取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 减少像素级标注的高成本和高耗时问题，同时利用基础模型提升半监督分割的效果。

Method: 结合两个基础模型，通过无监督mask-transformer一致性与CLIP特征的零样本分类增强识别能力，并通过SAM伪标签的类别无关解码器预热提高定位性能。

Result: 在ADE20K数据集上的半监督情景中，仅用158张标注图像即实现了29.9 PQ和38.9 mIoU，同时比现有方法显著领先且GPU内存需求减少8倍。

Conclusion: DEARLi在大分类体系和有限标注数据情况下表现卓越，进一步确立了半监督全景分割的新标准。

Abstract: Pixel-level annotation is expensive and time-consuming. Semi-supervised
segmentation methods address this challenge by learning models on few labeled
images alongside a large corpus of unlabeled images. Although foundation models
could further account for label scarcity, effective mechanisms for their
exploitation remain underexplored. We address this by devising a novel
semi-supervised panoptic approach fueled by two dedicated foundation models. We
enhance recognition by complementing unsupervised mask-transformer consistency
with zero-shot classification of CLIP features. We enhance localization by
class-agnostic decoder warm-up with respect to SAM pseudo-labels. The resulting
decoupled enhancement of recognition and localization (DEARLi) particularly
excels in the most challenging semi-supervised scenarios with large taxonomies
and limited labeled data. Moreover, DEARLi outperforms the state of the art in
semi-supervised semantic segmentation by a large margin while requiring 8x less
GPU memory, in spite of being trained only for the panoptic objective. We
observe 29.9 PQ and 38.9 mIoU on ADE20K with only 158 labeled images. The
source code is available at https://github.com/helen1c/DEARLi.

</details>


### [122] [Taming Modern Point Tracking for Speckle Tracking Echocardiography via Impartial Motion](https://arxiv.org/abs/2507.10127)
*Md Abulkalam Azad,John Nyberg,Håvard Dalen,Bjørnar Grenne,Lasse Lovstakken,Andreas Østvik*

Main category: cs.CV

TL;DR: 本文讨论了超声心动图中的点跟踪方法对组织变形追踪的改进，发现现有方法存在方向性运动偏差问题，并提出了优化策略和轻量级模型，显著提升了模型性能和轨迹准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以处理复杂的心脏运动，而现代点跟踪方法在超声心动图领域的应用有限且效果一般，因此需要探索更加通用和强健的解决方案。

Method: 通过分析心脏周期内的运动方向偏差问题，提出优化训练程序和定制数据增强，设计了一个利用多尺度空间上下文轻量级网络，并进行实验验证其有效性。

Result: 优化策略显著提升了模型性能，EchoTracker将位置精度提升了60.7%，轨迹误差减少了61.5%，轻量级模型甚至在某些方面超越了复杂模型，并改进了GLS测量的临床一致性。

Conclusion: 提出的方法增强了超声心动图点跟踪的性能和泛化性，并在临床应用中表现出较好的可重复性和可靠性。

Abstract: Accurate motion estimation for tracking deformable tissues in
echocardiography is essential for precise cardiac function measurements. While
traditional methods like block matching or optical flow struggle with intricate
cardiac motion, modern point tracking approaches remain largely underexplored
in this domain. This work investigates the potential of state-of-the-art (SOTA)
point tracking methods for ultrasound, with a focus on echocardiography.
Although these novel approaches demonstrate strong performance in general
videos, their effectiveness and generalizability in echocardiography remain
limited. By analyzing cardiac motion throughout the heart cycle in real B-mode
ultrasound videos, we identify that a directional motion bias across different
views is affecting the existing training strategies. To mitigate this, we
refine the training procedure and incorporate a set of tailored augmentations
to reduce the bias and enhance tracking robustness and generalization through
impartial cardiac motion. We also propose a lightweight network leveraging
multi-scale cost volumes from spatial context alone to challenge the advanced
spatiotemporal point tracking models. Experiments demonstrate that fine-tuning
with our strategies significantly improves models' performances over their
baselines, even for out-of-distribution (OOD) cases. For instance, EchoTracker
boosts overall position accuracy by 60.7% and reduces median trajectory error
by 61.5% across heart cycle phases. Interestingly, several point tracking
models fail to outperform our proposed simple model in terms of tracking
accuracy and generalization, reflecting their limitations when applied to
echocardiography. Nevertheless, clinical evaluation reveals that these methods
improve GLS measurements, aligning more closely with expert-validated,
semi-automated tools and thus demonstrating better reproducibility in
real-world applications.

</details>


### [123] [Deep Recurrence for Dynamical Segmentation Models](https://arxiv.org/abs/2507.10143)
*David Calhas,Arlindo L. Oliveira*

Main category: cs.CV

TL;DR: 提出一种预测编码启发的反馈机制，通过输出到输入的循环操作，提高模型在噪声条件下的分割任务表现和数据效率。


<details>
  <summary>Details</summary>
Motivation: 当前多数人工神经网络主要依赖单次静态传递输入的前馈方式，缺乏生物视觉依赖的反馈迭代能力。研究目的在于探索反馈机制能否提升模型的稳健性和效率。

Method: 在标准U-Net架构中引入预测编码启发的反馈机制，通过softmax投影和指数衰减两种生物学启发操作，确保反馈环路的稳定性。

Result: 反馈模型在噪声环境下显著优于前馈模型，且在使用极少训练样本（如仅两个样本）时展现出更高的泛化能力。

Conclusion: 反馈机制能够增强神经网络的稳健性和数据效率，提出了更适配和生物学启发的网络架构可能性。

Abstract: While biological vision systems rely heavily on feedback connections to
iteratively refine perception, most artificial neural networks remain purely
feedforward, processing input in a single static pass. In this work, we propose
a predictive coding inspired feedback mechanism that introduces a recurrent
loop from output to input, allowing the model to refine its internal state over
time. We implement this mechanism within a standard U-Net architecture and
introduce two biologically motivated operations, softmax projection and
exponential decay, to ensure stability of the feedback loop. Through controlled
experiments on a synthetic segmentation task, we show that the feedback model
significantly outperforms its feedforward counterpart in noisy conditions and
generalizes more effectively with limited supervision. Notably, feedback
achieves above random performance with just two training examples, while the
feedforward model requires at least four. Our findings demonstrate that
feedback enhances robustness and data efficiency, and offer a path toward more
adaptive and biologically inspired neural architectures. Code is available at:
github.com/DCalhas/feedback_segmentation.

</details>


### [124] [SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump Prediction via Video Analysis](https://arxiv.org/abs/2507.10171)
*Youngmin Kim,Giyeong Oh,Kwangsoo Youm,Youngjae Yu*

Main category: cs.CV

TL;DR: 提议了一种基于AI的视频系统SlumpGuard，用于实时监测混凝土流动性，改善质量控制的精确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统坍落度测试手动、耗时且不一致，无法满足实时监测需求。

Method: 提出并开发了SlumpGuard系统，一个基于视频和AI的解决方案，设计并使用专门的数据集进行真实场景实验验证。

Result: SlumpGuard在真实场景中部署表现出色，证明其可作为现代混凝土质量保证的实际解决方案。

Conclusion: 自动化视频分析技术能够有效提升混凝土质量控制的准确性与效率，展示出广阔的应用前景。

Abstract: Concrete workability is essential for construction quality, with the slump
test being the most common on-site method for its assessment. However,
traditional slump testing is manual, time-consuming, and prone to
inconsistency, limiting its applicability for real-time monitoring. To address
these challenges, we propose SlumpGuard, an AI-powered, video-based system that
automatically analyzes concrete flow from the truck chute to assess workability
in real time. Our system enables full-batch inspection without manual
intervention, improving both the accuracy and efficiency of quality control. We
present the system design, a the construction of a dedicated dataset, and
empirical results from real-world deployment, demonstrating the effectiveness
of SlumpGuard as a practical solution for modern concrete quality assurance.

</details>


### [125] [Minimizing the Pretraining Gap: Domain-aligned Text-Based Person Retrieval](https://arxiv.org/abs/2507.10195)
*Shuyu Yang,Yaxiong Wang,Yongrui Li,Li Zhu,Zhedong Zheng*

Main category: cs.CV

TL;DR: 本文提出了一个统一的文本驱动人物检索管道，通过图像与区域级别的领域自适应，成功闭合了合成数据和真实世界数据集之间的领域差距。


<details>
  <summary>Details</summary>
Motivation: 解决合成数据与真实世界数据集之间的领域差距问题，从而提升人物检索模型的性能。

Method: 提出了两个主要组件：Domain-aware Diffusion (图像级别自适应) 和 Multi-granularity Relation Alignment (区域级别自适应)，分别在图像和区域级别进行领域差距消除和细粒度对齐。

Result: 在CUHK-PEDES, ICFG-PEDES 和 RSTPReid数据集上实现了最先进的性能，超越了现有方法。

Conclusion: 该双层适应方法有效提升了文本驱动人物检索的性能，在合成数据和真实数据之间的适配问题上表现优异。

Abstract: In this work, we focus on text-based person retrieval, which aims to identify
individuals based on textual descriptions. Given the significant privacy issues
and the high cost associated with manual annotation, synthetic data has become
a popular choice for pretraining models, leading to notable advancements.
However, the considerable domain gap between synthetic pretraining datasets and
real-world target datasets, characterized by differences in lighting, color,
and viewpoint, remains a critical obstacle that hinders the effectiveness of
the pretrain-finetune paradigm. To bridge this gap, we introduce a unified
text-based person retrieval pipeline considering domain adaptation at both
image and region levels. In particular, it contains two primary components,
i.e., Domain-aware Diffusion (DaD) for image-level adaptation and
Multi-granularity Relation Alignment (MRA) for region-level adaptation. As the
name implies, Domain-aware Diffusion is to migrate the distribution of images
from the pretraining dataset domain to the target real-world dataset domain,
e.g., CUHK-PEDES. Subsequently, MRA performs a meticulous region-level
alignment by establishing correspondences between visual regions and their
descriptive sentences, thereby addressing disparities at a finer granularity.
Extensive experiments show that our dual-level adaptation method has achieved
state-of-the-art results on the CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets,
outperforming existing methodologies. The dataset, model, and code are
available at https://github.com/Shuyu-XJTU/MRA.

</details>


### [126] [A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on High-Resolution Images](https://arxiv.org/abs/2507.10202)
*Jaeseong Lee,Yeeun Choi,Heechan Choi,Hanjung Kim,Seonjoo Kim*

Main category: cs.CV

TL;DR: 提出了一种名为ECP的框架，用于增强MLLM在高分辨率图像任务中的性能，特别是解决了分辨率不一致带来的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLM在处理高分辨率图像时表现不佳，主要因为训练与测试中分辨率不一致，而直接下采样会损失细节。

Method: 提出了一个名为Extract Candidate then Predict (ECP)的两阶段框架。第一阶段通过粗略预测定位候选区域，第二阶段基于候选区域进行最终预测，从而保留了细粒度细节。

Result: 在4K GUI定位和4K、8K MLLM感知任务中，ECP分别实现了+21.3%、+5.8%、+5.2%的绝对性能提升。

Conclusion: ECP框架有效地解决了高分辨率图像处理中的细节损失问题，并显著提升了MLLM的性能。代码已开源。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in vision-language understanding, reasoning, and generation.
However, they struggle with tasks requiring fine-grained localization and
reasoning in high-resolution images. This constraint stems from the fact that
MLLMs are fine-tuned with fixed image resolution to align with the pre-trained
image encoder used in MLLM. Consequently, feeding high-resolution images
directly into MLLMs leads to poor generalization due to a train-test resolution
discrepancy, while downsampling these images-although ensuring
consistency-compromises fine-grained visual details and ultimately degrades
performance. To address this challenge, we propose Extract Candidate then
Predict (ECP), a novel training-free, task-agnostic two-stage framework
designed to enhance MLLM performance on high-resolution images. The key
intuition behind ECP is that while MLLMs struggle with high-resolution images,
their predictions on downsampled images still contain implicit localization
cues. By first identifying candidate region using the coarse prediction and
then predicting the final output based on candidate region, ECP effectively
preserves fine-grained details while mitigating the challenges posed by
high-resolution data. We validate our framework on 4K GUI grounding and 4K, 8K
MLLM perception, achieving +21.3%, +5.8%, +5.2% absolute improvement compared
to baseline respectively, demonstrating its effectiveness. Code is available at
https://github.com/yenncye/ECP.

</details>


### [127] [Improving Multimodal Learning via Imbalanced Learning](https://arxiv.org/abs/2507.10203)
*Shicai Wei,Chunbo Luo,Yang Luo*

Main category: cs.CV

TL;DR: 本文提出了非对称表征学习(ARL)策略，通过不平衡优化来提升多模态学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法尝试通过梯度平衡解决多模态学习中表现差于单模态学习的问题，但本文通过方差偏差分析证明，不平衡的模态依赖性依据模态方差逆比，可实现最优性能。

Method: 提出ARL策略，通过为每种模态编码器引入额外正则项，计算预测方差后，基于单模态方差重新加权优化各模态，并结合模态预测偏差与多模态损失联合优化。

Result: ARL策略在多种数据集上的实验中验证了其有效性与通用性。

Conclusion: ARL策略引入无额外参数，与多模态模型的结构与融合方法无关，提出一种高效的不平衡优化框架，为多模态学习提供新思路。

Abstract: Multimodal learning often encounters the under-optimized problem and may
perform worse than unimodal learning. Existing approaches attribute this issue
to imbalanced learning across modalities and tend to address it through
gradient balancing. However, this paper argues that balanced learning is not
the optimal setting for multimodal learning. With bias-variance analysis, we
prove that imbalanced dependency on each modality obeying the inverse ratio of
their variances contributes to optimal performance. To this end, we propose the
Asymmetric Representation Learning(ARL) strategy to assist multimodal learning
via imbalanced optimization. ARL introduces auxiliary regularizers for each
modality encoder to calculate their prediction variance. ARL then calculates
coefficients via the unimodal variance to re-weight the optimization of each
modality, forcing the modality dependence ratio to be inversely proportional to
the modality variance ratio. Moreover, to minimize the generalization error,
ARL further introduces the prediction bias of each modality and jointly
optimizes them with multimodal loss. Notably, all auxiliary regularizers share
parameters with the multimodal model and rely only on the modality
representation. Thus the proposed ARL strategy introduces no extra parameters
and is independent of the structures and fusion methods of the multimodal
model. Finally, extensive experiments on various datasets validate the
effectiveness and versatility of ARL. Code is available at
\href{https://github.com/shicaiwei123/ICCV2025-ARL}{https://github.com/shicaiwei123/ICCV2025-ARL}

</details>


### [128] [Is Micro-expression Ethnic Leaning?](https://arxiv.org/abs/2507.10209)
*Huai-Qian Khor,Yante Li,Xingxun Jiang,Guoying Zhao*

Main category: cs.CV

TL;DR: 研究探讨种族背景对情绪微表情识别的影响，并引入基于种族的框架提升分析准确性。


<details>
  <summary>Details</summary>
Motivation: 探讨种族背景是否对情绪表达及微表情识别产生影响，挑战情绪普适性假设的普遍适用性。

Method: 构建跨文化微表情数据库，算法标注种族标签，实验对比单一种族与混合种族情境下的情绪分析影响，并提出整合种族上下文的学习框架。

Result: 实验结果显示种族偏差确实对微表情识别有影响，新的框架有效提升种族敏感性，初步验证了研究方向。

Conclusion: 种族背景对微表情识别不可忽视，应在未来情绪分析研究中综合考虑种族因素。

Abstract: How much does ethnicity play its part in emotional expression? Emotional
expression and micro-expression research probe into understanding human
psychological responses to emotional stimuli, thereby revealing substantial
hidden yet authentic emotions that can be useful in the event of diagnosis and
interviews. While increased attention had been provided to micro-expression
analysis, the studies were done under Ekman's assumption of emotion
universality, where emotional expressions are identical across cultures and
social contexts. Our computational study uncovers some of the influences of
ethnic background in expression analysis, leading to an argument that the
emotional universality hypothesis is an overgeneralization from the perspective
of manual psychological analysis. In this research, we propose to investigate
the level of influence of ethnicity in a simulated micro-expression scenario.
We construct a cross-cultural micro-expression database and algorithmically
annotate the ethnic labels to facilitate the investigation. With the ethnically
annotated dataset, we perform a prima facie study to compare mono-ethnicity and
stereo-ethnicity in a controlled environment, which uncovers a certain
influence of ethnic bias via an experimental way. Building on this finding, we
propose a framework that integrates ethnic context into the emotional feature
learning process, yielding an ethnically aware framework that recognises
ethnicity differences in micro-expression recognition. For improved
understanding, qualitative analyses have been done to solidify the preliminary
investigation into this new realm of research. Code is publicly available at
https://github.com/IcedDoggie/ICMEW2025_EthnicMER

</details>


### [129] [Boosting Multimodal Learning via Disentangled Gradient Learning](https://arxiv.org/abs/2507.10213)
*Shicai Wei,Chunbo Luo,Yang Luo*

Main category: cs.CV

TL;DR: 该论文研究了多模态学习中的优化冲突问题，并提出了解耦梯度学习框架（DGL）以优化多模态模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究认为多模态学习性能差的原因是模态之间学习不平衡，但没有解释为什么多模态模型中的主导模态在性能上弱于单模态学习。

Method: 提出了DGL框架，通过截断从多模态损失反向传播到模态编码器的梯度，并用单模态损失的梯度替换，同时移除从单模态损失到模态融合模块的梯度，来实现模态编码器与融合模块的优化解耦。

Result: 实验表明，DGL在多种模态、任务和框架下均表现出高效的性能和适应性。

Conclusion: DGL通过解决优化冲突问题，为提高多模态模型性能提供了一种新思路。

Abstract: Multimodal learning often encounters the under-optimized problem and may have
worse performance than unimodal learning. Existing methods attribute this
problem to the imbalanced learning between modalities and rebalance them
through gradient modulation. However, they fail to explain why the dominant
modality in multimodal models also underperforms that in unimodal learning. In
this work, we reveal the optimization conflict between the modality encoder and
modality fusion module in multimodal models. Specifically, we prove that the
cross-modal fusion in multimodal models decreases the gradient passed back to
each modality encoder compared with unimodal models. Consequently, the
performance of each modality in the multimodal model is inferior to that in the
unimodal model. To this end, we propose a disentangled gradient learning (DGL)
framework to decouple the optimization of the modality encoder and modality
fusion module in the multimodal model. DGL truncates the gradient
back-propagated from the multimodal loss to the modality encoder and replaces
it with the gradient from unimodal loss. Besides, DGL removes the gradient
back-propagated from the unimodal loss to the modality fusion module. This
helps eliminate the gradient interference between the modality encoder and
modality fusion module while ensuring their respective optimization processes.
Finally, extensive experiments on multiple types of modalities, tasks, and
frameworks with dense cross-modal interaction demonstrate the effectiveness and
versatility of the proposed DGL. Code is available at
\href{https://github.com/shicaiwei123/ICCV2025-GDL}{https://github.com/shicaiwei123/ICCV2025-GDL}

</details>


### [130] [From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level Controllable Human Image Generation](https://arxiv.org/abs/2507.10217)
*Jeongho Kim,Sunghyun Park,Hyoungwoo Park,Sungrack Yun,Jaegul Choo,Seokeon Cho*

Main category: cs.CV

TL;DR: 提出一种名为Wardrobe Polyptych LoRA的新方法，通过LoRA层训练实现高保真个性化人像生成，无需额外推理阶段参数，并有效解决身份与属性一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成个性化人像时面临属性保持和计算消耗等挑战，尤其在推理阶段难以做到实时高效。

Method: 通过仅训练LoRA层，并结合以衣柜为中心进行生成，采用空间参考减少信息丢失，同时引入选择性主体区域损失优化生成效果。

Result: 模型在个性化人像生成中显著优于现有技术，特别是在信忠度和一致性方面表现出色。

Conclusion: 提出的方法无需额外参数支持，在小样本训练的条件下实现高效和保真个性化人像生成，有望为相关领域提供新性能基准。

Abstract: Recent diffusion models achieve personalization by learning specific
subjects, allowing learned attributes to be integrated into generated images.
However, personalized human image generation remains challenging due to the
need for precise and consistent attribute preservation (e.g., identity,
clothing details). Existing subject-driven image generation methods often
require either (1) inference-time fine-tuning with few images for each new
subject or (2) large-scale dataset training for generalization. Both approaches
are computationally expensive and impractical for real-time applications. To
address these limitations, we present Wardrobe Polyptych LoRA, a novel
part-level controllable model for personalized human image generation. By
training only LoRA layers, our method removes the computational burden at
inference while ensuring high-fidelity synthesis of unseen subjects. Our key
idea is to condition the generation on the subject's wardrobe and leverage
spatial references to reduce information loss, thereby improving fidelity and
consistency. Additionally, we introduce a selective subject region loss, which
encourages the model to disregard some of reference images during training. Our
loss ensures that generated images better align with text prompts while
maintaining subject integrity. Notably, our Wardrobe Polyptych LoRA requires no
additional parameters at the inference stage and performs generation using a
single model trained on a few training samples. We construct a new dataset and
benchmark tailored for personalized human image generation. Extensive
experiments show that our approach significantly outperforms existing
techniques in fidelity and consistency, enabling realistic and
identity-preserving full-body synthesis.

</details>


### [131] [Straighten Viscous Rectified Flow via Noise Optimization](https://arxiv.org/abs/2507.10218)
*Jimin Dai,Jiexi Yan,Jian Yang,Lei Luo*

Main category: cs.CV

TL;DR: 提出了VRFNO方法，通过优化噪声耦合与历史速度参数改进了Reflow不足，在单步与少步图片生成中达到了先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决Reflow生成高质量图像时的分布间隙问题。

Method: 提出VRFNO框架，结合编码器与神经速度场，通过历史速度项改善轨迹预测，以及通过重新参数化噪声实现数据优化训练。

Result: 实验表明VRFNO显著减少Reflow限制，在单步和少步生成任务中取得了最佳效果。

Conclusion: VRFNO方法通过创新设计有效克服了Reflow的关键缺陷，提高了生成图像质量。

Abstract: The Reflow operation aims to straighten the inference trajectories of the
rectified flow during training by constructing deterministic couplings between
noises and images, thereby improving the quality of generated images in
single-step or few-step generation. However, we identify critical limitations
in Reflow, particularly its inability to rapidly generate high-quality images
due to a distribution gap between images in its constructed deterministic
couplings and real images. To address these shortcomings, we propose a novel
alternative called Straighten Viscous Rectified Flow via Noise Optimization
(VRFNO), which is a joint training framework integrating an encoder and a
neural velocity field. VRFNO introduces two key innovations: (1) a historical
velocity term that enhances trajectory distinction, enabling the model to more
accurately predict the velocity of the current trajectory, and (2) the noise
optimization through reparameterization to form optimized couplings with real
images which are then utilized for training, effectively mitigating errors
caused by Reflow's limitations. Comprehensive experiments on synthetic data and
real datasets with varying resolutions show that VRFNO significantly mitigates
the limitations of Reflow, achieving state-of-the-art performance in both
one-step and few-step generation tasks.

</details>


### [132] [Spatial Lifting for Dense Prediction](https://arxiv.org/abs/2507.10222)
*Mingzhi Xu,Yizhe Zhang*

Main category: cs.CV

TL;DR: 提出了一种新的方法Spatial Lifting (SL)，通过提升输入数据的维度来提升密集预测任务的性能，同时降低模型参数和推理成本。


<details>
  <summary>Details</summary>
Motivation: 当前的密集预测任务方法存在模型参数过于庞大和推理效率低下的问题，亟需一种高效可靠的解决方案。

Method: 通过Spatial Lifting (SL)方法，将标准输入（例如2D图像）提升到更高的维度（如3D），并通过专为该更高维度设计的网络（如3D U-Net）进行处理。

Result: 在19个基准数据集上验证了SL方法，与传统方法相比，具有竞争力的密集预测性能，同时减少了98%以上的模型参数，并降低了推理成本。

Conclusion: Spatial Lifting引入了一种新的视觉建模范式，为视觉密集预测任务提供了一种高效、准确且可靠的深度网络解决方案。

Abstract: We present Spatial Lifting (SL), a novel methodology for dense prediction
tasks. SL operates by lifting standard inputs, such as 2D images, into a
higher-dimensional space and subsequently processing them using networks
designed for that higher dimension, such as a 3D U-Net. Counterintuitively,
this dimensionality lifting allows us to achieve good performance on benchmark
tasks compared to conventional approaches, while reducing inference costs and
significantly lowering the number of model parameters. The SL framework
produces intrinsically structured outputs along the lifted dimension. This
emergent structure facilitates dense supervision during training and enables
robust, near-zero-additional-cost prediction quality assessment at test time.
We validate our approach across 19 benchmark datasets (13 for semantic
segmentation and 6 for depth estimation), demonstrating competitive dense
prediction performance while reducing the model parameter count by over 98% (in
the U-Net case) and lowering inference costs. Spatial Lifting introduces a new
vision modeling paradigm that offers a promising path toward more efficient,
accurate, and reliable deep networks for dense prediction tasks in vision.

</details>


### [133] [ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users](https://arxiv.org/abs/2507.10223)
*Xiangyu Yin,Boyuan Yang,Weichen Liu,Qiyao Xue,Abrar Alamri,Goeran Fiedler,Wei Gao*

Main category: cs.CV

TL;DR: 本论文提出了一个名为ProGait的新数据集，用于支持义肢相关的视觉任务（如视频分割、2D姿态估计和步态分析），并提供基准任务和模型以验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于传统步态分析方法中现有的视觉算法难以处理义肢的独特外观及运动模式，因此研究者希望开发一个专门针对义肢的多任务数据集，以提高步态分析的准确性与适用性。

Method: 引入ProGait数据集，包括412个视频片段，涵盖了四名截肢者穿戴多种义肢进行步行测试的场景，并标注了相关的轮廓、姿态和步态特征。同时，研究提供了基线任务和微调模型以验证数据集的实际应用性能。

Result: 与预训练视觉模型相比，利用ProGait数据集进行的基线模型表现出更好的义肢相关任务的泛化能力。

Conclusion: ProGait数据集显著提高了义肢领域步态分析任务的准确性，为相关视觉研究提供了宝贵的数据资源及新方向。

Abstract: Prosthetic legs play a pivotal role in clinical rehabilitation, allowing
individuals with lower-limb amputations the ability to regain mobility and
improve their quality of life. Gait analysis is fundamental for optimizing
prosthesis design and alignment, directly impacting the mobility and life
quality of individuals with lower-limb amputations. Vision-based machine
learning (ML) methods offer a scalable and non-invasive solution to gait
analysis, but face challenges in correctly detecting and analyzing prosthesis,
due to their unique appearances and new movement patterns. In this paper, we
aim to bridge this gap by introducing a multi-purpose dataset, namely ProGait,
to support multiple vision tasks including Video Object Segmentation, 2D Human
Pose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from
four above-knee amputees when testing multiple newly-fitted prosthetic legs
through walking trials, and depicts the presence, contours, poses, and gait
patterns of human subjects with transfemoral prosthetic legs. Alongside the
dataset itself, we also present benchmark tasks and fine-tuned baseline models
to illustrate the practical application and performance of the ProGait dataset.
We compared our baseline models against pre-trained vision models,
demonstrating improved generalizability when applying the ProGait dataset for
prosthesis-specific tasks. Our code is available at
https://github.com/pittisl/ProGait and dataset at
https://huggingface.co/datasets/ericyxy98/ProGait.

</details>


### [134] [Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection](https://arxiv.org/abs/2507.10225)
*Jinglun Li,Kaixun Jiang,Zhaoyu Chen,Bo Lin,Yao Tang,Weifeng Ge,Wenqiang Zhang*

Main category: cs.CV

TL;DR: 本文提出SynOOD方法，通过利用基础模型生成边界对齐的OOD样本来微调CLIP模型，大幅提高了InD与OOD样本的区分能力。


<details>
  <summary>Details</summary>
Motivation: 克服当前预训练视觉语言模型在处理靠近InD边界的难辨OOD样本时的误分类问题。

Method: 采用基础模型和多模态大语言模型，通过迭代的in-painting过程生成边界对齐的复杂OOD样本，并基于OOD评分调整噪声生成样本，用以微调CLIP模型的图像编码器和负标签特征。

Result: 在ImageNet大型基准测试上表现优异，AUROC提升2.80%，FPR95降低11.13%。

Conclusion: SynOOD方法通过生成高质量的合成OOD样本显著增强了预训练模型的边界鉴别力，且参数与运行时间仅小幅增加。

Abstract: Pre-trained vision-language models have exhibited remarkable abilities in
detecting out-of-distribution (OOD) samples. However, some challenging OOD
samples, which lie close to in-distribution (InD) data in image feature space,
can still lead to misclassification. The emergence of foundation models like
diffusion models and multimodal large language models (MLLMs) offers a
potential solution to this issue. In this work, we propose SynOOD, a novel
approach that harnesses foundation models to generate synthetic, challenging
OOD data for fine-tuning CLIP models, thereby enhancing boundary-level
discrimination between InD and OOD samples. Our method uses an iterative
in-painting process guided by contextual prompts from MLLMs to produce nuanced,
boundary-aligned OOD samples. These samples are refined through noise
adjustments based on gradients from OOD scores like the energy score,
effectively sampling from the InD/OOD boundary. With these carefully
synthesized images, we fine-tune the CLIP image encoder and negative label
features derived from the text encoder to strengthen connections between
near-boundary OOD samples and a set of negative labels. Finally, SynOOD
achieves state-of-the-art performance on the large-scale ImageNet benchmark,
with minimal increases in parameters and runtime. Our approach significantly
surpasses existing methods, improving AUROC by 2.80% and reducing FPR95 by
11.13%. Codes are available in https://github.com/Jarvisgivemeasuit/SynOOD.

</details>


### [135] [Navigating the Challenges of AI-Generated Image Detection in the Wild: What Truly Matters?](https://arxiv.org/abs/2507.10236)
*Despina Konstantinidou,Dimitrios Karageorgiou,Christos Koutlis,Olga Papadopoulou,Emmanouil Schinas,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: 本研究探讨了当前AI生成图像检测（AID）的局限性，提出了基于真实社交媒体图像的新数据集ITW-SM，并通过调整模型组件提高了检测效果。


<details>
  <summary>Details</summary>
Motivation: 生成技术的快速发展引发了对社会信任和数字信息完整性的关注，尤其是AI图像生成的真实性问题迫切需要解决。

Method: 研究提出了一个名为ITW-SM的新数据集，系统分析了AID模型在骨干架构、训练数据组成、预处理策略和数据增强组合等四个关键因素的表现，通过改进这些组件提高检测性能。

Result: 在真实场景下，不同AID模型的平均AUC性能提升了26.87%。

Conclusion: 现有AID模型在受控数据集上表现良好，但在真实世界中表现受限。通过改进模型组合，可有效提高检测可信度，为AI生成内容的可信检测提供了思路。

Abstract: The rapid advancement of generative technologies presents both unprecedented
creative opportunities and significant challenges, particularly in maintaining
social trust and ensuring the integrity of digital information. Following these
concerns, the challenge of AI-Generated Image Detection (AID) becomes
increasingly critical. As these technologies become more sophisticated, the
quality of AI-generated images has reached a level that can easily deceive even
the most discerning observers. Our systematic evaluation highlights a critical
weakness in current AI-Generated Image Detection models: while they perform
exceptionally well on controlled benchmark datasets, they struggle
significantly with real-world variations. To assess this, we introduce ITW-SM,
a new dataset of real and AI-generated images collected from major social media
platforms. In this paper, we identify four key factors that influence AID
performance in real-world scenarios: backbone architecture, training data
composition, pre-processing strategies and data augmentation combinations. By
systematically analyzing these components, we shed light on their impact on
detection efficacy. Our modifications result in an average AUC improvement of
26.87% across various AID models under real-world conditions.

</details>


### [136] [Transferring Styles for Reduced Texture Bias and Improved Robustness in Semantic Segmentation Networks](https://arxiv.org/abs/2507.10239)
*Ben Hamscher,Edgar Heinert,Annika Mütze,Kira Maag,Matthias Rottmann*

Main category: cs.CV

TL;DR: 研究探讨了通过风格迁移方法来减弱语义分割中的纹理偏差，同时提升其对形状特征的依赖，提高对图像腐蚀和对抗性攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 受图像分类中减少纹理偏差并提升鲁棒性的启发，研究旨在探讨风格迁移方法在语义分割中的效果。

Method: 通过在人工划分的随机区域（Voronoi单元）上进行风格迁移，用风格转移后的数据训练语义分割DNN，以减低对纹理的依赖，突出形状特征的作用。

Result: 风格迁移增强显著减弱了纹理偏差，提高了对图像腐蚀和对抗攻击的鲁棒性；在Cityscapes和PASCAL Context数据集上，对CNN和Transformer架构均表现出良好的普适性。

Conclusion: 风格迁移是一种有效的增强策略，可用于改善语义分割模型的鲁棒性和泛化性能。

Abstract: Recent research has investigated the shape and texture biases of deep neural
networks (DNNs) in image classification which influence their generalization
capabilities and robustness. It has been shown that, in comparison to regular
DNN training, training with stylized images reduces texture biases in image
classification and improves robustness with respect to image corruptions. In an
effort to advance this line of research, we examine whether style transfer can
likewise deliver these two effects in semantic segmentation. To this end, we
perform style transfer with style varying across artificial image areas. Those
random areas are formed by a chosen number of Voronoi cells. The resulting
style-transferred data is then used to train semantic segmentation DNNs with
the objective of reducing their dependence on texture cues while enhancing
their reliance on shape-based features. In our experiments, it turns out that
in semantic segmentation, style transfer augmentation reduces texture bias and
strongly increases robustness with respect to common image corruptions as well
as adversarial attacks. These observations hold for convolutional neural
networks and transformer architectures on the Cityscapes dataset as well as on
PASCAL Context, showing the generality of the proposed method.

</details>


### [137] [Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures](https://arxiv.org/abs/2507.10265)
*Xinlong Ding,Hongwei Yu,Jiawei Li,Feifan Li,Yu Shang,Bochao Zou,Huimin Ma,Jiansheng Chen*

Main category: cs.CV

TL;DR: 本文提出了一种称为Kaleidoscopic Background Attack (KBA) 的对抗性攻击方法，利用具有多重径向对称性的背景盘有效影响相机姿态估计模型，结果显示该方法对各类模型均有效。


<details>
  <summary>Details</summary>
Motivation: 相机姿态估计在计算机视觉领域中意义重大，但在目标场景且输入稀疏时，背景纹理对估计精度影响显著，因此需研究如何利用背景的特性进行模型攻击。

Method: 提出Kaleidoscopic Background Attack (KBA)，通过多折对称的盘状背景生成相似纹理，并结合投影方向一致性损失进行优化，提高对抗效果。

Result: 实验验证了经过优化的对抗性背景能够有效攻击多种相机姿态估计模型，显著降低其精度。

Conclusion: 优化后的万花筒背景能够成功对相机姿态估计模型发起攻击，展示了背景攻击在模型研究中的潜在重要性。

Abstract: Camera pose estimation is a fundamental computer vision task that is
essential for applications like visual localization and multi-view stereo
reconstruction. In the object-centric scenarios with sparse inputs, the
accuracy of pose estimation can be significantly influenced by background
textures that occupy major portions of the images across different viewpoints.
In light of this, we introduce the Kaleidoscopic Background Attack (KBA), which
uses identical segments to form discs with multi-fold radial symmetry. These
discs maintain high similarity across different viewpoints, enabling effective
attacks on pose estimation models even with natural texture segments.
Additionally, a projected orientation consistency loss is proposed to optimize
the kaleidoscopic segments, leading to significant enhancement in the attack
effectiveness. Experimental results show that optimized adversarial
kaleidoscopic backgrounds can effectively attack various camera pose estimation
models.

</details>


### [138] [FTCFormer: Fuzzy Token Clustering Transformer for Image Classification](https://arxiv.org/abs/2507.10283)
*Muyi Bao,Changyu Zeng,Yifan Wang,Zhengni Yang,Zimu Wang,Guangliang Cheng,Jun Qi,Wei Wang*

Main category: cs.CV

TL;DR: 本文提出了FTCFormer，通过基于语义的动态视觉标记生成，克服了传统Transformer忽略图像语义内容的问题，在多个数据集上的图像分类任务表现出一致改进。


<details>
  <summary>Details</summary>
Motivation: Transformer在计算机视觉任务中表现优异但忽略了图像区域的语义信息，这导致特征表示次优。

Method: 设计了一种新颖的基于聚类的降采样模块，结合DPC-FKNN机制、空间连接评分（SCS）和通道合并策略（Cmerge），动态生成视觉标记。

Result: 在32个数据集上的实验表明，与TCFormer基线相比，FTCFormer在多个领域数据集上均取得性能提升，例如五个细粒度数据集上提升1.43%、六个自然图像数据集上提升1.09%。

Conclusion: 通过结合语义驱动的视觉标记生成和创新的聚类方法，FTCFormer显著提升了Transformer在图像分类中的表现，体现了其潜力。

Abstract: Transformer-based deep neural networks have achieved remarkable success
across various computer vision tasks, largely attributed to their long-range
self-attention mechanism and scalability. However, most transformer
architectures embed images into uniform, grid-based vision tokens, neglecting
the underlying semantic meanings of image regions, resulting in suboptimal
feature representations. To address this issue, we propose Fuzzy Token
Clustering Transformer (FTCFormer), which incorporates a novel clustering-based
downsampling module to dynamically generate vision tokens based on the semantic
meanings instead of spatial positions. It allocates fewer tokens to less
informative regions and more to represent semantically important regions,
regardless of their spatial adjacency or shape irregularity. To further enhance
feature extraction and representation, we propose a Density Peak
Clustering-Fuzzy K-Nearest Neighbor (DPC-FKNN) mechanism for clustering center
determination, a Spatial Connectivity Score (SCS) for token assignment, and a
channel-wise merging (Cmerge) strategy for token merging. Extensive experiments
on 32 datasets across diverse domains validate the effectiveness of FTCFormer
on image classification, showing consistent improvements over the TCFormer
baseline, achieving gains of improving 1.43% on five fine-grained datasets,
1.09% on six natural image datasets, 0.97% on three medical datasets and 0.55%
on four remote sensing datasets. The code is available at:
https://github.com/BaoBao0926/FTCFormer/tree/main.

</details>


### [139] [Show and Polish: Reference-Guided Identity Preservation in Face Video Restoration](https://arxiv.org/abs/2507.10293)
*Wenkang Han,Wang Lin,Yiyun Zhou,Qi Liu,Shulei Wang,Chang Yao,Jingyuan Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新方法（IP-FVR），通过参考高质量人脸图像实现面部视频从降质版本到高质量视频的恢复，并在身份一致性和细节保护方面取得显著成果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理严重降质的面部视频时难以保持细致及身份相关的特征，导致生成的面部图像缺乏个性化特征。

Method: IP-FVR 使用高质量参考人脸图像作为视觉提示，通过解耦的交叉注意力机制引入语义丰富的身份信息。此外，通过基于余弦相似性的奖励信号及后缀加权时间聚合实现身份漂移控制，并设计指数混合策略解决不同视频片段的身份对齐。同时，提出多流负提示增强模型对重要面部特征的关注。

Result: 实验表明，IP-FVR 在合成与真实数据集上的恢复质量和身份保持能力均优于现有方法。

Conclusion: IP-FVR 方法有效解决了面部视频恢复中的身份一致性问题，展现了在实际应用中的潜力。

Abstract: Face Video Restoration (FVR) aims to recover high-quality face videos from
degraded versions. Traditional methods struggle to preserve fine-grained,
identity-specific features when degradation is severe, often producing
average-looking faces that lack individual characteristics. To address these
challenges, we introduce IP-FVR, a novel method that leverages a high-quality
reference face image as a visual prompt to provide identity conditioning during
the denoising process. IP-FVR incorporates semantically rich identity
information from the reference image using decoupled cross-attention
mechanisms, ensuring detailed and identity consistent results. For intra-clip
identity drift (within 24 frames), we introduce an identity-preserving feedback
learning method that combines cosine similarity-based reward signals with
suffix-weighted temporal aggregation. This approach effectively minimizes drift
within sequences of frames. For inter-clip identity drift, we develop an
exponential blending strategy that aligns identities across clips by
iteratively blending frames from previous clips during the denoising process.
This method ensures consistent identity representation across different clips.
Additionally, we enhance the restoration process with a multi-stream negative
prompt, guiding the model's attention to relevant facial attributes and
minimizing the generation of low-quality or incorrect features. Extensive
experiments on both synthetic and real-world datasets demonstrate that IP-FVR
outperforms existing methods in both quality and identity preservation,
showcasing its substantial potential for practical applications in face video
restoration.

</details>


### [140] [DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs](https://arxiv.org/abs/2507.10302)
*Jiahe Zhao,Rongkun Zheng,Yi Wang,Helin Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 提出名为DisCo的新型视觉封装方法，通过视觉概念区分器和时间焦点校准器模块提高语义清晰度和时间一致性，显著提升视频MLLM性能。


<details>
  <summary>Details</summary>
Motivation: 解决线性投影器在视频语义清晰度与时间一致性上的不足，更有效地将视频内容转化为LLM输入。

Method: 设计DisCo，包括视觉概念区分器（VCD）模块和时间焦点校准器（TFC）模块，分配唯一语义并保证时间一致性。

Result: 在多个视频理解基准上表现优于现有方法，同时提升了标记效率。

Conclusion: DisCo在语义清晰性和时间一致性方面表现卓越，为视频MLLM的视觉封装提供了一个高效的解决方案。

Abstract: In video Multimodal Large Language Models (video MLLMs), the visual
encapsulation process plays a pivotal role in converting video contents into
representative tokens for LLM input. While linear projectors are widely
employed for encapsulation, they introduce semantic indistinctness and temporal
incoherence when applied to videos. Conversely, the structure of resamplers
shows promise in tackling these challenges, but an effective solution remains
unexplored. Drawing inspiration from resampler structures, we introduce DisCo,
a novel visual encapsulation method designed to yield semantically distinct and
temporally coherent visual tokens for video MLLMs. DisCo integrates two key
components: (1) A Visual Concept Discriminator (VCD) module, assigning unique
semantics for visual tokens by associating them in pair with discriminative
concepts in the video. (2) A Temporal Focus Calibrator (TFC) module, ensuring
consistent temporal focus of visual tokens to video elements across every video
frame. Through extensive experiments on multiple video MLLM frameworks, we
demonstrate that DisCo remarkably outperforms previous state-of-the-art methods
across a variety of video understanding benchmarks, while also achieving higher
token efficiency thanks to the reduction of semantic indistinctness. The code:
https://github.com/ZJHTerry18/DisCo.

</details>


### [141] [Contrastive Pretraining with Dual Visual Encoders for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2507.10306)
*Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: 提出了一种双视觉编码器架构，实现无手语词注释的手语翻译，并在基准数据集上取得了最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有手语翻译方法依赖手语词注释，而这种注释昂贵且不能完全捕捉连续手语的复杂性，亟需一种无手语词注释的翻译方法。

Method: 提出了两阶段、双视觉编码器框架，在预训练阶段通过对比目标对两套视觉特征与文本嵌入进行联合对齐；在后续任务中，融合视觉特征并输入编码器-解码器模型。

Result: 在Phoenix-2014T基准数据集上，提出的双编码器架构超越单流变体，并在无手语词注释方法中取得最高BLEU-4分数。

Conclusion: 双视觉编码器架构在无手语词注释的手语翻译中表现出色，证明了其在减少依赖昂贵注释基础上的潜力。

Abstract: Sign Language Translation (SLT) aims to convert sign language videos into
spoken or written text. While early systems relied on gloss annotations as an
intermediate supervision, such annotations are costly to obtain and often fail
to capture the full complexity of continuous signing. In this work, we propose
a two-phase, dual visual encoder framework for gloss-free SLT, leveraging
contrastive visual-language pretraining. During pretraining, our approach
employs two complementary visual backbones whose outputs are jointly aligned
with each other and with sentence-level text embeddings via a contrastive
objective. During the downstream SLT task, we fuse the visual features and
input them into an encoder-decoder model. On the Phoenix-2014T benchmark, our
dual encoder architecture consistently outperforms its single stream variants
and achieves the highest BLEU-4 score among existing gloss-free SLT approaches.

</details>


### [142] [Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching](https://arxiv.org/abs/2507.10318)
*Yuhan Liu,Jingwen Fu,Yang Wu,Kangyi Wu,Pengna Li,Jiayi Wu,Sanping Zhou,Jingmin Xin*

Main category: cs.CV

TL;DR: 本文提出了一种名为IMD（利用预训练扩散模型的图像特征匹配）的新框架，旨在解决基础模型在图像特征匹配中的失配问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了基础模型在单图像理解与跨图像理解需求之间的失配问题，影响多实例特征匹配效果。

Method: 提出IMD框架，包括引入生成式扩散模型以捕获实例级细节，以及设计跨图像交互提示模块以增强图像对间的信息交互。

Result: 在多项基准测试中达到新的SOTA水平，并在新基准IMIM中提升了12%的性能，表明有效缓解了失配问题。

Conclusion: IMD框架充分利用生成式扩散模型与提示机制，有效解决了现有基础模型在特征匹配中的失配挑战，为多实例特征匹配提供了有力支持。

Abstract: Leveraging the vision foundation models has emerged as a mainstream paradigm
that improves the performance of image feature matching. However, previous
works have ignored the misalignment when introducing the foundation models into
feature matching. The misalignment arises from the discrepancy between the
foundation models focusing on single-image understanding and the cross-image
understanding requirement of feature matching. Specifically, 1) the embeddings
derived from commonly used foundation models exhibit discrepancies with the
optimal embeddings required for feature matching; 2) lacking an effective
mechanism to leverage the single-image understanding ability into cross-image
understanding. A significant consequence of the misalignment is they struggle
when addressing multi-instance feature matching problems. To address this, we
introduce a simple but effective framework, called IMD (Image feature Matching
with a pre-trained Diffusion model) with two parts: 1) Unlike the dominant
solutions employing contrastive-learning based foundation models that emphasize
global semantics, we integrate the generative-based diffusion models to
effectively capture instance-level details. 2) We leverage the prompt mechanism
in generative model as a natural tunnel, propose a novel cross-image
interaction prompting module to facilitate bidirectional information
interaction between image pairs. To more accurately measure the misalignment,
we propose a new benchmark called IMIM, which focuses on multi-instance
scenarios. Our proposed IMD establishes a new state-of-the-art in commonly
evaluated benchmarks, and the superior improvement 12% in IMIM indicates our
method efficiently mitigates the misalignment.

</details>


### [143] [Text Embedding Knows How to Quantize Text-Guided Diffusion Models](https://arxiv.org/abs/2507.10340)
*Hongjae Lee,Myungjun Son,Dongjea Kang,Seung-Won Jung*

Main category: cs.CV

TL;DR: 本文提出了一种名为QLIP的新量化方法，利用文本提示动态选择每层的比特精度，降低扩散模型的计算复杂度，同时提升生成图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型因计算复杂度高，限制了其在资源受限环境中的使用，而现有的量化方法未充分利用输入条件的信息进行优化。

Method: 提出QLIP方法，利用文本提示来动态选择每层的比特精度，并可与现有量化方法无缝结合以提升量化效率。

Result: 实验表明，QLIP在多个数据集上显著减少了计算复杂度，并提高了生成图像的质量。

Conclusion: QLIP是一种创新且高效的扩散模型量化方法，有助于在保持生成质量的同时提升效率。

Abstract: Despite the success of diffusion models in image generation tasks such as
text-to-image, the enormous computational complexity of diffusion models limits
their use in resource-constrained environments. To address this, network
quantization has emerged as a promising solution for designing efficient
diffusion models. However, existing diffusion model quantization methods do not
consider input conditions, such as text prompts, as an essential source of
information for quantization. In this paper, we propose a novel quantization
method dubbed Quantization of Language-to-Image diffusion models using text
Prompts (QLIP). QLIP leverages text prompts to guide the selection of bit
precision for every layer at each time step. In addition, QLIP can be
seamlessly integrated into existing quantization methods to enhance
quantization efficiency. Our extensive experiments demonstrate the
effectiveness of QLIP in reducing computational complexity and improving the
quality of the generated images across various datasets.

</details>


### [144] [FGSSNet: Feature-Guided Semantic Segmentation of Real World Floorplans](https://arxiv.org/abs/2507.10343)
*Hugo Norrby,Gabriel Färm,Kevin Hernandez-Diaz,Fernando Alonso-Fernandez*

Main category: cs.CV

TL;DR: 提出FGSSNet，一种专为改进平面图墙体分割的通用能力而设计的新型多头特征引导语义分割架构。


<details>
  <summary>Details</summary>
Motivation: 改善墙体分割在平面图上的泛化能力。

Method: 设计一种名为FGSSNet的架构，采用U-Net分割主干，结合多头特征提取器提取领域特定特征图，并注入到U-Net的潜在空间中，以引导分割过程。特征提取器通过编码-解码结构训练，用于墙壁贴片的压缩特征表示，同时预测墙壁宽度。

Result: 实验结果显示，与传统U-Net相比，注入特征后的架构性能得到提升。

Conclusion: 所提方法通过注入特定特征以引导墙体分割，验证了其有效性。

Abstract: We introduce FGSSNet, a novel multi-headed feature-guided semantic
segmentation (FGSS) architecture designed to improve the generalization ability
of wall segmentation on floorplans. FGSSNet features a U-Net segmentation
backbone with a multi-headed dedicated feature extractor used to extract
domain-specific feature maps which are injected into the latent space of U-Net
to guide the segmentation process. This dedicated feature extractor is trained
as an encoder-decoder with selected wall patches, representative of the walls
present in the input floorplan, to produce a compressed latent representation
of wall patches while jointly trained to predict the wall width. In doing so,
we expect that the feature extractor encodes texture and width features of wall
patches that are useful to guide the wall segmentation process. Our experiments
show increased performance by the use of such injected features in comparison
to the vanilla U-Net, highlighting the validity of the proposed approach.

</details>


### [145] [Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter](https://arxiv.org/abs/2507.10355)
*Bo Jiang,Xueyang Ze,Beibei Wang,Xixi Wang,Xixi Wan,Bin Luo*

Main category: cs.CV

TL;DR: 提出了VRGAdapter，结合随机图模型改善VLM适配器的语义表征能力。


<details>
  <summary>Details</summary>
Motivation: 传统的确定性文本特征适配器难以捕获类别文本描述的多样性与类间关系，这限制了VLM在下游任务中的表现。

Method: 引入顶点随机知识图模型，结合概率消息传播，学习类别节点的上下文分布表示，采用重新参数化采样函数配合适配器训练；同时设计了不确定性引导的多分支融合方法，用以动态整合多个预训练模型。

Result: 在多个基准数据集上的实验验证了该方法的效果优越性。

Conclusion: VRGAdapter和UMF方案能够显著提升VLM模型在视觉学习任务中的适应性及鲁棒性。

Abstract: Textual adapter-based tuning methods have shown significant potential in
transferring knowledge from pre-trained Vision-Language Models (VLMs) to
downstream tasks. Existing works generally employ the deterministic textual
feature adapter to refine each category textual representation. However, due to
inherent factors such as different attributes and contexts, there exists
significant diversity in textual descriptions for each category. Such
description diversity offers rich discriminative semantic knowledge that can
benefit downstream visual learning tasks. Obviously, traditional deterministic
adapter model cannot adequately capture this varied semantic information. Also,
it is desirable to exploit the inter-class relationships in VLM adapter. To
address these issues, we propose to exploit random graph model into VLM adapter
and develop a novel Vertex Random Graph Adapter (VRGAdapter). VRGAdapter first
models the inherent diverse descriptions of each category and inter-class
relationships of different categories simultaneously by leveraging a Vertex
Random Knowledge Graph (VRKG) model. Then, it employs probabilistic message
propagation on VRKG to learn context-aware distribution representation for each
class node. Finally, it adopts a reparameterized sampling function to achieve
textual adapter learning. Note that, VRGAdapter provides a more general adapter
solution that encompasses traditional graph-based adapter as a special case. In
addition, to enable more robust performance for downstream tasks, we also
introduce a new Uncertainty-guided Multi-branch Fusion (UMF) scheme that
dynamically integrates multiple pre-trained models for ensemble prediction.
Extensive experiments on multiple benchmark datasets demonstrate the
effectiveness of our approach.

</details>


### [146] [Fine-Grained Zero-Shot Object Detection](https://arxiv.org/abs/2507.10358)
*Hongxu Ma,Chenbo Zhang,Lu Zhang,Jiaogen Zhou,Jihong Guan,Shuigeng Zhou*

Main category: cs.CV

TL;DR: 该论文提出了一种新的零样本细粒度目标检测任务（FG-ZSD），并开发了一个名为MSHC的方法，同时构建了一个新的鸟类数据集FGZSD-Birds，用以推进该领域研究。


<details>
  <summary>Details</summary>
Motivation: 当前零样本目标检测主要适用于粗粒度分类，无法应对需要区分细微差别的目标类别（如不同种类的鸟、鱼和花）。因此，提出面向细粒度目标检测的新问题（FG-ZSD）。

Method: 提出一种名为MSHC的方法，基于改进的两阶段检测器，结合多层次语义感知嵌入对齐损失，加强视觉和语义空间的耦合。

Result: 在新构建的FG-ZSD数据集（FGZSD-Birds）上进行大量实验，结果表明该方法优于现有的零样本目标检测模型。

Conclusion: 该研究首次定义并解决FG-ZSD问题，提供了方法、数据集及实验验证，为细粒度零样本目标检测领域开启了新方向。

Abstract: Zero-shot object detection (ZSD) aims to leverage semantic descriptions to
localize and recognize objects of both seen and unseen classes. Existing ZSD
works are mainly coarse-grained object detection, where the classes are
visually quite different, thus are relatively easy to distinguish. However, in
real life we often have to face fine-grained object detection scenarios, where
the classes are too similar to be easily distinguished. For example, detecting
different kinds of birds, fishes, and flowers.
  In this paper, we propose and solve a new problem called Fine-Grained
Zero-Shot Object Detection (FG-ZSD for short), which aims to detect objects of
different classes with minute differences in details under the ZSD paradigm. We
develop an effective method called MSHC for the FG-ZSD task, which is based on
an improved two-stage detector and employs a multi-level semantics-aware
embedding alignment loss, ensuring tight coupling between the visual and
semantic spaces. Considering that existing ZSD datasets are not suitable for
the new FG-ZSD task, we build the first FG-ZSD benchmark dataset FGZSD-Birds,
which contains 148,820 images falling into 36 orders, 140 families, 579 genera
and 1432 species. Extensive experiments on FGZSD-Birds show that our method
outperforms existing ZSD models.

</details>


### [147] [Test-Time Canonicalization by Foundation Models for Robust Perception](https://arxiv.org/abs/2507.10375)
*Utkarsh Singhal,Ryan Feng,Stella X. Yu,Atul Prakash*

Main category: cs.CV

TL;DR: 提出了一种名为FOCAL的方法，通过利用基础模型的视觉先验，在无需重新训练或修改架构的情况下，增强视觉感知的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法过于依赖特定架构和预定义增强，而难以泛化到多样化的变换场景中。

Method: 提出FOCAL框架，在测试时通过优化候选变换，使图像接近典型“规范”视图，从而增强鲁棒性，无需重新训练或架构更改。

Result: 实验证明，FOCAL在应对CLIP和SAM模型的多种复杂变换（如2D/3D旋转、光照变化及昼夜变化）时表现出更强鲁棒性，并展示其在主动视觉中的潜在应用。

Conclusion: 提出了一种可扩展且无需针对变换特定训练的新路径，突破了现有假设的限制。

Abstract: Real-world visual perception requires invariance to diverse transformations,
yet current methods rely heavily on specialized architectures or training on
predefined augmentations, limiting generalization. We propose FOCAL, a
test-time, data-driven framework that achieves robust perception by leveraging
internet-scale visual priors from foundation models. By generating and
optimizing candidate transformations toward visually typical, "canonical"
views, FOCAL enhances robustness without re-training or architectural changes.
Our experiments demonstrate improved robustness of CLIP and SAM across
challenging transformations, including 2D/3D rotations, illumination shifts
(contrast and color), and day-night variations. We also highlight potential
applications in active vision. Our approach challenges the assumption that
transform-specific training is necessary, instead offering a scalable path to
invariance. Our code is available at: https://github.com/sutkarsh/focal.

</details>


### [148] [Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks](https://arxiv.org/abs/2507.10381)
*Aaryam Sharma*

Main category: cs.CV

TL;DR: 本研究将拓扑数据分析（TDA）与卷积神经网络结合，用于遥感分类任务，并在多个数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有卷积神经网络方法对纹理特征的偏向性带来了局限性，因此需要一种新的方法将更广泛的几何信息应用于复杂数据集的分类任务。

Method: 提出了一种结合TDA特征与深度学习的工程管道，并将其应用于遥感分类任务，通过对拓扑特征的提取与融合提高模型表现。

Result: 在EuroSAT数据集上，所提出方法使ResNet18模型的准确率提高了1.44%，达到99.33%，超越了多种更大规模模型；在RESISC45数据集上，准确率比基础ResNet18模型提升了1.82%。

Conclusion: 此研究首次将TDA特征与深度学习集成并成功应用于卫星场景分类任务，证明了TDA特征在无明显拓扑结构数据集上的适用性。

Abstract: Topological data analysis (TDA) is a relatively new field that is gaining
rapid adoption due to its robustness and ability to effectively describe
complex datasets by quantifying geometric information. In imaging contexts, TDA
typically models data as filtered cubical complexes from which we can extract
discriminative features using persistence homology. Meanwhile, convolutional
neural networks (CNNs) have been shown to be biased towards texture based local
features. To address this limitation, we propose a TDA feature engineering
pipeline and a simple method to integrate topological features with deep
learning models on remote sensing classification. Our method improves the
performance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving
99.33% accuracy, which surpasses all previously reported single-model
accuracies, including those with larger architectures, such as ResNet50 (2x
larger) and XL Vision Transformers (197x larger). We additionally show that our
method's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45
dataset. To our knowledge, this is the first application of TDA features in
satellite scene classification with deep learning. This demonstrates that TDA
features can be integrated with deep learning models, even on datasets without
explicit topological structures, thereby increasing the applicability of TDA. A
clean implementation of our method will be made publicly available upon
publication.

</details>


### [149] [Numerically Computing Galois Groups of Minimal Problems](https://arxiv.org/abs/2507.10407)
*Timothy Duff*

Main category: cs.CV

TL;DR: 本文讨论了代数、数值计算和计算机视觉中的问题，特别是解决代数方程的参数系统。


<details>
  <summary>Details</summary>
Motivation: 解决鲁棒模型拟合中的参数代数系统问题，该问题广泛应用于计算机视觉领域，例如RanSaC。

Method: 分析和量化解决这类参数系统的固有难度，并尝试找到实际解决方案。

Result: 提出了过去5年的研究成果，深入理解了问题本质并取得了一些解决的进展。

Conclusion: 通过研究，推进了参数代数系统求解的理论和实践。

Abstract: I discuss a seemingly unlikely confluence of topics in algebra, numerical
computation, and computer vision. The motivating problem is that of solving
multiples instances of a parametric family of systems of algebraic (polynomial
or rational function) equations. No doubt already of interest to ISSAC
attendees, this problem arises in the context of robust model-fitting paradigms
currently utilized by the computer vision community (namely "Random Sampling
and Consensus", aka "RanSaC".) This talk will give an overview of work in the
last 5+ years that aspires to measure the intrinsic difficulty of solving such
parametric systems, and makes strides towards practical solutions.

</details>


### [150] [Text-Visual Semantic Constrained AI-Generated Image Quality Assessment](https://arxiv.org/abs/2507.10432)
*Qiang Li,Qingsen Yan,Haojian Huang,Peng Wu,Haokui Zhang,Yanning Zhang*

Main category: cs.CV

TL;DR: 本文提出了SC-AGIQA框架，通过结合文本-视觉语义约束，有效评估AI生成图像的文本一致性与感知质量缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有方法在评估AI生成图像时存在语义错位和细节感知不足问题，亟需创新的图像质量评估框架。

Method: 提出SC-AGIQA框架，包括两个核心模块：TSAM（生成图像描述并与原始提示词比对）和FFDPM（利用频域分析和人类视觉系统特性评估细微视觉失真）。

Result: 实验结果表明，SC-AGIQA在多个基准数据集上表现优于现有最先进方法。

Conclusion: SC-AGIQA通过创新模块有效弥补了现有方法的不足，为AI生成图像质量评估提供了更强大的解决方案，并公开源码。

Abstract: With the rapid advancements in Artificial Intelligence Generated Image (AGI)
technology, the accurate assessment of their quality has become an increasingly
vital requirement. Prevailing methods typically rely on cross-modal models like
CLIP or BLIP to evaluate text-image alignment and visual quality. However, when
applied to AGIs, these methods encounter two primary challenges: semantic
misalignment and details perception missing. To address these limitations, we
propose Text-Visual Semantic Constrained AI-Generated Image Quality Assessment
(SC-AGIQA), a unified framework that leverages text-visual semantic constraints
to significantly enhance the comprehensive evaluation of both text-image
consistency and perceptual distortion in AI-generated images. Our approach
integrates key capabilities from multiple models and tackles the aforementioned
challenges by introducing two core modules: the Text-assisted Semantic
Alignment Module (TSAM), which leverages Multimodal Large Language Models
(MLLMs) to bridge the semantic gap by generating an image description and
comparing it against the original prompt for a refined consistency check, and
the Frequency-domain Fine-Grained Degradation Perception Module (FFDPM), which
draws inspiration from Human Visual System (HVS) properties by employing
frequency domain analysis combined with perceptual sensitivity weighting to
better quantify subtle visual distortions and enhance the capture of
fine-grained visual quality details in images. Extensive experiments conducted
on multiple benchmark datasets demonstrate that SC-AGIQA outperforms existing
state-of-the-art methods. The code is publicly available at
https://github.com/mozhu1/SC-AGIQA.

</details>


### [151] [4D-Animal: Freely Reconstructing Animatable 3D Animals from Videos](https://arxiv.org/abs/2507.10437)
*Shanshan Zhong,Jiawei Peng,Zehan Zheng,Zhongzhan Huang,Wufei Ma,Guofeng Zhang,Qihao Liu,Alan Yuille,Jieneng Chen*

Main category: cs.CV

TL;DR: 现有从视频中重建可动画的3D动物的方法依赖稀疏关键点标注，但4D-Animal无需稀疏关键点标注，实现了对3D动物的高质量、有时间一致性的重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法中稀疏关键点的获取成本高且检测器性能受限，亟需一种无需关键点标注的更高效可靠的方法。

Method: 提出4D-Animal框架，使用密集特征网络将2D表示映射到SMAL参数，并利用层次对齐策略整合来自2D视觉模型的轮廓、部件、像素及时间信息。

Result: 4D-Animal在实验中优于基于模型和无模型的基线方法，同时生成的高质量3D数据可服务于其他3D任务。

Conclusion: 4D-Animal有效提升了3D动物重建的效率和精度，具有大规模应用潜力，并公开了相关代码以供推广使用。

Abstract: Existing methods for reconstructing animatable 3D animals from videos
typically rely on sparse semantic keypoints to fit parametric models. However,
obtaining such keypoints is labor-intensive, and keypoint detectors trained on
limited animal data are often unreliable. To address this, we propose
4D-Animal, a novel framework that reconstructs animatable 3D animals from
videos without requiring sparse keypoint annotations. Our approach introduces a
dense feature network that maps 2D representations to SMAL parameters,
enhancing both the efficiency and stability of the fitting process.
Furthermore, we develop a hierarchical alignment strategy that integrates
silhouette, part-level, pixel-level, and temporal cues from pre-trained 2D
visual models to produce accurate and temporally coherent reconstructions
across frames. Extensive experiments demonstrate that 4D-Animal outperforms
both model-based and model-free baselines. Moreover, the high-quality 3D assets
generated by our method can benefit other 3D tasks, underscoring its potential
for large-scale applications. The code is released at
https://github.com/zhongshsh/4D-Animal.

</details>


### [152] [CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding](https://arxiv.org/abs/2507.10449)
*Hongyong Han,Wei Wang,Gaowei Zhang,Mingjie Li,Yi Wang*

Main category: cs.CV

TL;DR: 提出了用于珊瑚礁分析的第一个大规模视觉问答(VQA)数据集CoralVQA，用于评估生态和健康相关条件，包含12,805张珊瑚图片和277,653个问答对。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁是脆弱但重要的生态系统，需要持续监测。然而，现有珊瑚图片的解析需要专业领域知识，存在挑战。视觉问答(VQA)结合大规模视觉语言模型(LVLMs)可以为珊瑚图片的用户友好交互提供潜力，因此需要专用数据集解决这些问题。

Method: 开发了CoralVQA数据集，并构建了一个半自动数据建设流水线，协同海洋生物学家确保数据的规模性和专业质量。同时，评估了多个先进的LVLM模型，分析其关键局限性和可改进点。

Result: CoralVQA包含来自3大洋67种珊瑚属的12,805张真实珊瑚图片和277,653个问答对，全面评估珊瑚的生态和健康条件。通过测试LVLM模型，发现了其在处理珊瑚图像问答时的局限与机会。

Conclusion: CoralVQA数据集为珊瑚图像的视觉语言推理提供了新的研究挑战和基准，为未来LVLM的开发和珊瑚保护工作奠定了基础。

Abstract: Coral reefs are vital yet vulnerable ecosystems that require continuous
monitoring to support conservation. While coral reef images provide essential
information in coral monitoring, interpreting such images remains challenging
due to the need for domain expertise. Visual Question Answering (VQA), powered
by Large Vision-Language Models (LVLMs), has great potential in user-friendly
interaction with coral reef images. However, applying VQA to coral imagery
demands a dedicated dataset that addresses two key challenges: domain-specific
annotations and multidimensional questions. In this work, we introduce
CoralVQA, the first large-scale VQA dataset for coral reef analysis. It
contains 12,805 real-world coral images from 67 coral genera collected from 3
oceans, along with 277,653 question-answer pairs that comprehensively assess
ecological and health-related conditions. To construct this dataset, we develop
a semi-automatic data construction pipeline in collaboration with marine
biologists to ensure both scalability and professional-grade data quality.
CoralVQA presents novel challenges and provides a comprehensive benchmark for
studying vision-language reasoning in the context of coral reef images. By
evaluating several state-of-the-art LVLMs, we reveal key limitations and
opportunities. These insights form a foundation for future LVLM development,
with a particular emphasis on supporting coral conservation efforts.

</details>


### [153] [RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening](https://arxiv.org/abs/2507.10461)
*Tao Tang,Chengxu Yang*

Main category: cs.CV

TL;DR: 论文提出了一种名为RAPNet的新框架，用于遥感图像融合，通过内容自适应卷积（RAPConv）和注意力机制模块（PAN-DFF）提升空间细节与光谱保真度并取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基于CNN的方法由于卷积核的统一应用限制了局部内容变化的利用，从而影响了图像融合的精确性。

Method: 提出了一种内容自适应的卷积核（RAPConv）以及融合注意力机制的模块（PAN-DFF），以适应局部特征变化及平衡空间细节与光谱保真度。

Result: 基于公开数据集的实验结果显示，与现有方法相比，RAPNet在定量指标和定性分析上均表现更优，且消融实验验证了其自适应组件的有效性。

Conclusion: RAPNet通过自适应卷积和动态特征融合机制，在遥感图像融合中实现了空间和光谱性能的显著提升。

Abstract: Pansharpening refers to the process of integrating a high resolution
panchromatic (PAN) image with a lower resolution multispectral (MS) image to
generate a fused product, which is pivotal in remote sensing. Despite the
effectiveness of CNNs in addressing this challenge, they are inherently
constrained by the uniform application of convolutional kernels across all
spatial positions, overlooking local content variations. To overcome this
issue, we introduce RAPNet, a new architecture that leverages content-adaptive
convolution. At its core, RAPNet employs the Receptive-field Adaptive
Pansharpening Convolution (RAPConv), designed to produce spatially adaptive
kernels responsive to local feature context, thereby enhancing the precision of
spatial detail extraction. Additionally, the network integrates the
Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an
attention mechanism to achieve an optimal balance between spatial detail
enhancement and spectral fidelity. Comprehensive evaluations on publicly
available datasets confirm that RAPNet delivers superior performance compared
to existing approaches, as demonstrated by both quantitative metrics and
qualitative assessments. Ablation analyses further substantiate the
effectiveness of the proposed adaptive components.

</details>


### [154] [RefSTAR: Blind Facial Image Restoration with Reference Selection, Transfer, and Reconstruction](https://arxiv.org/abs/2507.10470)
*Zhicun Yin,Junjie Chen,Ming Liu,Zhixin Wang,Fan Li,Renjing Pei,Xiaoming Li,Rynson W. H. Lau,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 本文提出了一种面向盲人面部图像修复的新方法RefSTAR，通过构造参考选择模块、特征融合范式以及重构机制，有效保留了身份特性并实现了高质量的特征传递。


<details>
  <summary>Details</summary>
Motivation: 目前方法在处理面部图像复杂退化及身份特性保留问题上表现不佳，因此需要提出能有效引入高质量参考图像特征的修复方法。

Method: 提出RefSTAR方法，包括参考选择模块(RefSel)、特征融合范式及参考图像重建机制，并通过构建RefSel-HQ数据集和重新设计循环一致性损失来优化模型性能。

Result: 实验表明，RefSTAR在各种模型中的表现优于现有方法，具备更好的身份保留能力和参考特征转移效果。

Conclusion: RefSTAR通过全面结合参考图像特征，实现了面部图像修复的新突破，并提升了身份保留与修复质量。

Abstract: Blind facial image restoration is highly challenging due to unknown complex
degradations and the sensitivity of humans to faces. Although existing methods
introduce auxiliary information from generative priors or high-quality
reference images, they still struggle with identity preservation problems,
mainly due to improper feature introduction on detailed textures. In this
paper, we focus on effectively incorporating appropriate features from
high-quality reference images, presenting a novel blind facial image
restoration method that considers reference selection, transfer, and
reconstruction (RefSTAR). In terms of selection, we construct a reference
selection (RefSel) module. For training the RefSel module, we construct a
RefSel-HQ dataset through a mask generation pipeline, which contains annotating
masks for 10,000 ground truth-reference pairs. As for the transfer, due to the
trivial solution in vanilla cross-attention operations, a feature fusion
paradigm is designed to force the features from the reference to be integrated.
Finally, we propose a reference image reconstruction mechanism that further
ensures the presence of reference image features in the output image. The cycle
consistency loss is also redesigned in conjunction with the mask. Extensive
experiments on various backbone models demonstrate superior performance,
showing better identity preservation ability and reference feature transfer
quality. Source code, dataset, and pre-trained models are available at
https://github.com/yinzhicun/RefSTAR.

</details>


### [155] [GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space](https://arxiv.org/abs/2507.10473)
*David G. Shatwell,Ishan Rajendrakumar Dave,Sirnam Swetha,Mubarak Shah*

Main category: cs.CV

TL;DR: 提出了一种名为GT-Loc的新方法，通过共享高维特征空间预测图像的时间和地理位置。


<details>
  <summary>Details</summary>
Motivation: 希望改善图像时间和地理位置预测问题，并应对其间的相互依赖性。

Method: 使用单独的编码器为图像、时间和位置提取特征，并通过循环时间度量学习目标在共同空间中建模。

Result: 新方法在时间预测和地理定位任务中表现优异，统一的空间支持组合式和基于文本的图像检索。

Conclusion: GT-Loc方法提高了预测的精确性，并为图像检索带来更多可能性。

Abstract: Timestamp prediction aims to determine when an image was captured using only
visual information, supporting applications such as metadata correction,
retrieval, and digital forensics. In outdoor scenarios, hourly estimates rely
on cues like brightness, hue, and shadow positioning, while seasonal changes
and weather inform date estimation. However, these visual cues significantly
depend on geographic context, closely linking timestamp prediction to
geo-localization. To address this interdependence, we introduce GT-Loc, a novel
retrieval-based method that jointly predicts the capture time (hour and month)
and geo-location (GPS coordinates) of an image. Our approach employs separate
encoders for images, time, and location, aligning their embeddings within a
shared high-dimensional feature space. Recognizing the cyclical nature of time,
instead of conventional contrastive learning with hard positives and negatives,
we propose a temporal metric-learning objective providing soft targets by
modeling pairwise time differences over a cyclical toroidal surface. We present
new benchmarks demonstrating that our joint optimization surpasses previous
time prediction methods, even those using the ground-truth geo-location as an
input during inference. Additionally, our approach achieves competitive results
on standard geo-localization tasks, and the unified embedding space facilitates
compositional and text-based image retrieval.

</details>


### [156] [Privacy-Preserving Multi-Stage Fall Detection Framework with Semi-supervised Federated Learning and Robotic Vision Confirmation](https://arxiv.org/abs/2507.10474)
*Seyed Alireza Rahimi Azghadi,Truong-Thanh-Hung Nguyen,Helene Fournier,Monica Wachowicz,Rene Richard,Francis Palma,Hung Cao*

Main category: cs.CV

TL;DR: 本文提出一种结合多种系统的跌倒检测框架，总体准确率高达99.99%，提供隐私保护与高可靠性。


<details>
  <summary>Details</summary>
Motivation: 应对人口老龄化带来的跌倒风险，既要快速可靠地检测跌倒场景，又需保护用户隐私。

Method: 整合半监督联邦学习的可穿戴设备检测、室内定位与导航技术、及基于视觉的跌倒识别系统，形成完整跌倒检测链路。

Result: 半监督联邦学习系统准确率99.19%，基于视觉系统准确率96.3%，导航系统成功率95%；整体框架准确率达99.99%。

Conclusion: 提出的框架集成多种技术，兼顾隐私保护和高效检测，适用于老年人跌倒场景管理。

Abstract: The aging population is growing rapidly, and so is the danger of falls in
older adults. A major cause of injury is falling, and detection in time can
greatly save medical expenses and recovery time. However, to provide timely
intervention and avoid unnecessary alarms, detection systems must be effective
and reliable while addressing privacy concerns regarding the user. In this
work, we propose a framework for detecting falls using several complementary
systems: a semi-supervised federated learning-based fall detection system
(SF2D), an indoor localization and navigation system, and a vision-based human
fall recognition system. A wearable device and an edge device identify a fall
scenario in the first system. On top of that, the second system uses an indoor
localization technique first to localize the fall location and then navigate a
robot to inspect the scenario. A vision-based detection system running on an
edge device with a mounted camera on a robot is used to recognize fallen
people. Each of the systems of this proposed framework achieves different
accuracy rates. Specifically, the SF2D has a 0.81% failure rate equivalent to
99.19% accuracy, while the vision-based fallen people detection achieves 96.3%
accuracy. However, when we combine the accuracy of these two systems with the
accuracy of the navigation system (95% success rate), our proposed framework
creates a highly reliable performance for fall detection, with an overall
accuracy of 99.99%. Not only is the proposed framework safe for older adults,
but it is also a privacy-preserving solution for detecting falls.

</details>


### [157] [The Power of Certainty: How Confident Models Lead to Better Segmentation](https://arxiv.org/abs/2507.10490)
*Tugberk Erol,Tuba Caglikantar,Duygu Sarikaya*

Main category: cs.CV

TL;DR: 该研究提出了一种基于置信度的自蒸馏方法，用于提高息肉分割性能，减少对资源的依赖，并表现出较高的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型虽然能高效检测和分割息肉，但通常参数量大、容易过拟合，尤其在处理数据集偏差时泛化能力较差。

Method: 提出了一种基于置信度的自蒸馏方法，利用动态置信系数计算批次内迭代间的损失，无需测试时额外的计算和内存消耗。

Result: 通过在多临床中心数据集上进行全面实验，显示新方法在息肉分割任务中优于现有最先进的模型，并具有较好的跨数据集泛化能力。

Conclusion: 该方法在不增加测试资源需求的情况下显著改善了模型性能，证明了其有效性，且后续代码将在论文接受后发布。

Abstract: Deep learning models have been proposed for automatic polyp detection and
precise segmentation of polyps during colonoscopy procedures. Although these
state-of-the-art models achieve high performance, they often require a large
number of parameters. Their complexity can make them prone to overfitting,
particularly when trained on biased datasets, and can result in poor
generalization across diverse datasets. Knowledge distillation and
self-distillation are proposed as promising strategies to mitigate the
limitations of large, over-parameterized models. These approaches, however, are
resource-intensive, often requiring multiple models and significant memory
during training. We propose a confidence-based self-distillation approach that
outperforms state-of-the-art models by utilizing only previous iteration data
storage during training, without requiring extra computation or memory usage
during testing. Our approach calculates the loss between the previous and
current iterations within a batch using a dynamic confidence coefficient. To
evaluate the effectiveness of our approach, we conduct comprehensive
experiments on the task of polyp segmentation. Our approach outperforms
state-of-the-art models and generalizes well across datasets collected from
multiple clinical centers. The code will be released to the public once the
paper is accepted.

</details>


### [158] [BenchReAD: A systematic benchmark for retinal anomaly detection](https://arxiv.org/abs/2507.10492)
*Chenyu Lian,Hong-Yu Zhou,Zhanli Hu,Jing Qin*

Main category: cs.CV

TL;DR: 作者提出了一种新的视网膜异常检测基准（BenchReAD），并开发了结合正常特征记忆的NFM-DRA方法，改善了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有视网膜异常检测研究面临数据集种类单一、测试集饱和以及缺乏泛化性分析等问题，且未能充分利用临床中的异常标注数据和未标注数据。

Method: 创建了一个全面的视网膜异常检测基准，并提出一种新的方法NFM-DRA，将异常表征分解与正常特征记忆相结合。

Result: NFM-DRA方法相比现有方法显著提高了检测性能，特别是在处理未知异常时表现优异，达到了新的SOTA水平。

Conclusion: 通过改进的视网膜异常检测基准和方法，提供了公平评价和推动该领域进步的工具，且研究结果对实际临床数据具有较好的适用性。

Abstract: Retinal anomaly detection plays a pivotal role in screening ocular and
systemic diseases. Despite its significance, progress in the field has been
hindered by the absence of a comprehensive and publicly available benchmark,
which is essential for the fair evaluation and advancement of methodologies.
Due to this limitation, previous anomaly detection work related to retinal
images has been constrained by (1) a limited and overly simplistic set of
anomaly types, (2) test sets that are nearly saturated, and (3) a lack of
generalization evaluation, resulting in less convincing experimental setups.
Furthermore, existing benchmarks in medical anomaly detection predominantly
focus on one-class supervised approaches (training only with negative samples),
overlooking the vast amounts of labeled abnormal data and unlabeled data that
are commonly available in clinical practice. To bridge these gaps, we introduce
a benchmark for retinal anomaly detection, which is comprehensive and
systematic in terms of data and algorithm. Through categorizing and
benchmarking previous methods, we find that a fully supervised approach
leveraging disentangled representations of abnormalities (DRA) achieves the
best performance but suffers from significant drops in performance when
encountering certain unseen anomalies. Inspired by the memory bank mechanisms
in one-class supervised learning, we propose NFM-DRA, which integrates DRA with
a Normal Feature Memory to mitigate the performance degradation, establishing a
new SOTA. The benchmark is publicly available at
https://github.com/DopamineLcy/BenchReAD.

</details>


### [159] [Cameras as Relative Positional Encoding](https://arxiv.org/abs/2507.10496)
*Ruilong Li,Brent Yi,Junchen Liu,Hang Gao,Yi Ma,Angjoo Kanazawa*

Main category: cs.CV

TL;DR: 本文讨论了如何在多视点变换器中利用相机几何关系，提出了一种新的相对位置编码方法——投影位置编码(PRoPE)，实验验证了其在多项任务中优越的性能。


<details>
  <summary>Details</summary>
Motivation: 目前在多视点计算机视觉任务中，利用变换器增进几何相关性能的潜力尚未完全实现，研究如何更好地基于相机几何关系为视觉标记提供三维空间参照。

Method: 本文比较了不同的变换器相机条件化技术：标记级的射线码图编码、注意力级的相对姿态编码，以及提出的投影位置编码(PRoPE)，后者完整捕捉相机内部和外部参数。

Result: 实验表明，PRoPE在多视点新视图合成中性能提升显著，对非分布序列长度和相机参数输入具有良好泛化能力，同时在立体深度估计与空间认知任务上也表现优异。

Conclusion: 利用几何关系改进多视点变换器表现的研究表明，PRoPE有效增强了模型性能和泛化能力，且可扩展至不同任务和更大模型规模。

Abstract: Transformers are increasingly prevalent for multi-view computer vision tasks,
where geometric relationships between viewpoints are critical for 3D
perception. To leverage these relationships, multi-view transformers must use
camera geometry to ground visual tokens in 3D space. In this work, we compare
techniques for conditioning transformers on cameras: token-level raymap
encodings, attention-level relative pose encodings, and a new relative encoding
we propose -- Projective Positional Encoding (PRoPE) -- that captures complete
camera frustums, both intrinsics and extrinsics, as a relative positional
encoding. Our experiments begin by showing how relative camera conditioning
improves performance in feedforward novel view synthesis, with further gains
from PRoPE. This holds across settings: scenes with both shared and varying
intrinsics, when combining token- and attention-level conditioning, and for
generalization to inputs with out-of-distribution sequence lengths and camera
intrinsics. We then verify that these benefits persist for different tasks,
stereo depth estimation and discriminative spatial cognition, as well as larger
model sizes.

</details>


### [160] [National level satellite-based crop field inventories in smallholder landscapes](https://arxiv.org/abs/2507.10499)
*Philippe Rufin,Pauline Lucie Hammer,Leon-Friedrich Thomas,Sá Nogueira Lisboa,Natasha Ribeiro,Almeida Sitoe,Patrick Hostert,Patrick Meyfroidt*

Main category: cs.CV

TL;DR: 本研究利用高分辨率卫星数据和深度迁移学习绘制了莫桑比克全国范围内的小农农业田地地图，并取得了93%的精度。这是首次实现对复杂小农系统大面积的田地划分数据采集。


<details>
  <summary>Details</summary>
Motivation: 传统农业系统缺乏对活跃耕地分布和田地规模的深入了解，限制了发展可持续农业政策的制定。

Method: 研究结合高分辨率(1.5米)的地球观测数据和深度迁移学习技术，在最少参考数据需求下，对莫桑比克全国范围的农业田地进行了详细的田块划分。

Result: 首次提供了覆盖莫桑比克近80万平方公里、逾2100万个田地的全国级数据集，田地划定的IoU中位值达0.81，展示了非农业用途与活跃耕地的93%区分精度。

Conclusion: 研究表明田地规模是农业社会经济和环境影响的关键指标，可用于分析农业生产、生活水平、森林砍伐与生物多样性等问题及其相互权衡。

Abstract: The design of science-based policies to improve the sustainability of
smallholder agriculture is challenged by a limited understanding of fundamental
system properties, such as the spatial distribution of active cropland and
field size. We integrate very high spatial resolution (1.5 m) Earth observation
data and deep transfer learning to derive crop field delineations in complex
agricultural systems at the national scale, while maintaining minimum reference
data requirements and enhancing transferability. We provide the first
national-level dataset of 21 million individual fields for Mozambique (covering
~800,000 km2) for 2023. Our maps separate active cropland from non-agricultural
land use with an overall accuracy of 93% and balanced omission and commission
errors. Field-level spatial agreement reached median intersection over union
(IoU) scores of 0.81, advancing the state-of-the-art in large-area field
delineation in complex smallholder systems. The active cropland maps capture
fragmented rural regions with low cropland shares not yet identified in global
land cover or cropland maps. These regions are mostly located in agricultural
frontier regions which host 7-9% of the Mozambican population. Field size in
Mozambique is very low overall, with half of the fields being smaller than 0.16
ha, and 83% smaller than 0.5 ha. Mean field size at aggregate spatial
resolution (0.05{\deg}) is 0.32 ha, but it varies strongly across gradients of
accessibility, population density, and net forest cover change. This variation
reflects a diverse set of actors, ranging from semi-subsistence smallholder
farms to medium-scale commercial farming, and large-scale farming operations.
Our results highlight that field size is a key indicator relating to
socio-economic and environmental outcomes of agriculture (e.g., food
production, livelihoods, deforestation, biodiversity), as well as their
trade-offs.

</details>


### [161] [Quantize-then-Rectify: Efficient VQ-VAE Training](https://arxiv.org/abs/2507.10547)
*Borui Zhang,Qihang Rao,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 提出了一种高效的VQ-VAE训练框架ReVQ，实现了高压缩率和极低的计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有高压缩比的VQ-VAE训练需要大量计算资源，挑战在于如何减少计算成本，提高效率。

Method: 通过使用“量化后校正（ReVQ）”框架，在预训练的VAE基础上加入通道多组量化和后置校正，扩大码本容量并减少量化误差。

Result: ReVQ将ImageNet图像压缩到最多512个token，同时保持高质量的重构（rFID=1.06），训练成本减少两个数量级，能在单块NVIDIA 4090显卡上22小时完成训练。

Conclusion: ReVQ提供了高效的效率-重建权衡方案，在多模态模型中具有潜在的广泛应用。

Abstract: Visual tokenizers are pivotal in multimodal large models, acting as bridges
between continuous inputs and discrete tokens. Nevertheless, training
high-compression-rate VQ-VAEs remains computationally demanding, often
necessitating thousands of GPU hours. This work demonstrates that a pre-trained
VAE can be efficiently transformed into a VQ-VAE by controlling quantization
noise within the VAE's tolerance threshold. We present
\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs
to enable rapid VQ-VAE training with minimal computational overhead. By
integrating \textbf{channel multi-group quantization} to enlarge codebook
capacity and a \textbf{post rectifier} to mitigate quantization errors, ReVQ
compresses ImageNet images into at most 512 tokens while sustaining competitive
reconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training
costs by over two orders of magnitude relative to state-of-the-art approaches:
ReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours,
whereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental
results show that ReVQ achieves superior efficiency-reconstruction trade-offs.

</details>


### [162] [Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder](https://arxiv.org/abs/2507.10552)
*Vladimir Iashin,Horace Lee,Dan Schofield,Andrew Zisserman*

Main category: cs.CV

TL;DR: 提出了一种自监督方法用于从未标记的相机陷阱影像中学习黑猩猩脸部特征，性能优于有监督方法。


<details>
  <summary>Details</summary>
Motivation: 通过减少对人工标注的依赖，解决相机陷阱数据处理中对个体识别的瓶颈问题。

Method: 利用DINOv2框架训练Vision Transformers，从自动挖掘的脸部剪辑中学习特征，完全无需身份标签。

Result: 该方法在Bossou等基准测试上表现出优越的开放集重新识别性能，超越了有监督的基线方法。

Conclusion: 研究展示了自监督学习在生物多样性监测中的潜力，并为大规模、非侵入式种群研究提供了可能的路径。

Abstract: Camera traps are revolutionising wildlife monitoring by capturing vast
amounts of visual data; however, the manual identification of individual
animals remains a significant bottleneck. This study introduces a fully
self-supervised approach to learning robust chimpanzee face embeddings from
unlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision
Transformers on automatically mined face crops, eliminating the need for
identity labels. Our method demonstrates strong open-set re-identification
performance, surpassing supervised baselines on challenging benchmarks such as
Bossou, despite utilising no labelled data during training. This work
underscores the potential of self-supervised learning in biodiversity
monitoring and paves the way for scalable, non-invasive population studies.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [163] [Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale](https://arxiv.org/abs/2507.08865)
*Javis AI Team,Amrendra Singh,Maulik Shah,Dharshan Sampath*

Main category: cs.CL

TL;DR: 本研究提出Spatial ModernBERT，通过结合文本和空间信息高效抽取复杂金融文件中的表格数据和键值对。


<details>
  <summary>Details</summary>
Motivation: 金融文件中表格和键值对的提取对业务流程如审计、数据分析和自动化处理至关重要，但其复杂性增加了提取难度。

Method: 采用基于Transformer结构的模型Spatial ModernBERT，加入空间嵌入，进行三头分类任务：(1)标记头，用于标记每个token的类别；(2)列头，用于预测列索引；(3)行头，用于检测表头和项目行的开始。模型通过在PubTables-1M数据集预训练并在金融文档数据上微调完成。

Result: 实验结果表明，Spatial ModernBERT能有效利用文本与空间信息，在实际金融文档中实现高精度的表格及键值对提取。

Conclusion: Spatial ModernBERT是一种能够融合文本和空间特征的强大工具，可用于解决复杂场景下金融文档数据提取的问题。

Abstract: Extracting tables and key-value pairs from financial documents is essential
for business workflows such as auditing, data analytics, and automated invoice
processing. In this work, we introduce Spatial ModernBERT-a transformer-based
model augmented with spatial embeddings-to accurately detect and extract
tabular data and key-value fields from complex financial documents. We cast the
extraction task as token classification across three heads: (1) Label Head,
classifying each token as a label (e.g., PO Number, PO Date, Item Description,
Quantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;
(3) Row Head, distinguishing the start of item rows and header rows. The model
is pretrained on the PubTables-1M dataset, then fine-tuned on a financial
document dataset, achieving robust performance through cross-entropy loss on
each classification head. We propose a post-processing method to merge tokens
using B-I-IB tagging, reconstruct the tabular layout, and extract key-value
pairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages
both textual and spatial cues, facilitating highly accurate table and key-value
extraction in real-world financial documents.

</details>


### [164] [SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems](https://arxiv.org/abs/2507.08898)
*Wenliang Shan,Michael Fu,Rui Yang,Chakkrit,Tantithamthavorn*

Main category: cs.CL

TL;DR: 论文提出了一个名为SEALGuard的多语言安全框架，与现有方法相比更有效地检测多语言环境中的不安全和越狱提示。


<details>
  <summary>Details</summary>
Motivation: 现有的如LlamaGuard等安全框架对非英语的低资源语言输入表现较差，容易遭受不安全和越狱提示攻击。

Method: 通过低秩适配（LoRA）技术将一个通用的多语言模型改造为SEALGuard多语言安全框架，并构建了包含10种语言的多语言安全数据集SEALSBench，用于模型训练和评估。

Result: SEALGuard在多语言不安全和越狱提示检测上优于现有框架，对比LlamaGuard，成功防御率（DSR）提高了48%，并在DSR、精确率和F1评分上表现最佳。

Conclusion: SEALGuard显著改善了LLM系统在多语言环境下的安全对齐能力，为多语言安全防护提供了有效解决方案。

Abstract: Safety alignment is critical for LLM-powered systems. While recent
LLM-powered guardrail approaches such as LlamaGuard achieve high detection
accuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),
they struggle with multilingual unsafe inputs. This limitation leaves LLM
systems vulnerable to unsafe and jailbreak prompts written in low-resource
languages such as those in Southeast Asia. This paper introduces SEALGuard, a
multilingual guardrail designed to improve the safety alignment across diverse
languages. It aims to address the multilingual safety alignment gap of existing
guardrails and ensure effective filtering of unsafe and jailbreak prompts in
LLM-powered systems. We adapt a general-purpose multilingual language model
into a multilingual guardrail using low-rank adaptation (LoRA). We construct
SEALSBench, a large-scale multilingual safety alignment dataset containing over
260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.
We evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on
this benchmark. Our findings show that multilingual unsafe and jailbreak
prompts substantially degrade the performance of the state-of-the-art
LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and
18%, respectively, compared to its performance on English-only prompts. In
contrast, SEALGuard outperforms existing guardrails in detecting multilingual
unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and
achieving the best DSR, precision, and F1-score. Our ablation study further
reveals the contributions of adaptation strategies and model size to the
overall performance of SEALGuard. SEALGuard advances the safety alignment of
LLM systems by introducing an effective multilingual guardrail.

</details>


### [165] [Evaluating LLMs in Medicine: A Call for Rigor, Transparency](https://arxiv.org/abs/2507.08916)
*Mahmoud Alwakeel,Aditya Nagori,Vijay Krishnamoorthy,Rishikesan Kamaleswaran*

Main category: cs.CL

TL;DR: 评估大型语言模型在医学问答中的局限性，重点分析用于评估的数据集质量，发现现有数据集缺乏临床真实性、透明性和稳健性，提出需要标准框架和合作改进。


<details>
  <summary>Details</summary>
Motivation: 探讨当前医学问答中大型语言模型的评估问题，尤其关注现有评估数据集的质量和适用性，以改进模型实践效果。

Method: 复审常用基准数据集（如MedQA、MedMCQA、PubMedQA、MMLU）的严谨性、透明性和临床相关性，并分析医学期刊中的挑战性问题作为潜在评估工具。

Result: 现存数据集缺乏临床真实感、透明度和稳健性；虽然期刊挑战问题具有一定优势，但规模小、范围有限，且受到LLM训练的影响，暴露出需要综合、代表性数据集的紧迫性。

Conclusion: 医学领域中评估LLM需要标准化框架，并依赖机构与政策制定者协作，确保数据集及评估方法严谨、公正且能反映临床复杂性。

Abstract: Objectives: To evaluate the current limitations of large language models
(LLMs) in medical question answering, focusing on the quality of datasets used
for their evaluation. Materials and Methods: Widely-used benchmark datasets,
including MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,
transparency, and relevance to clinical scenarios. Alternatives, such as
challenge questions in medical journals, were also analyzed to identify their
potential as unbiased evaluation tools. Results: Most existing datasets lack
clinical realism, transparency, and robust validation processes. Publicly
available challenge questions offer some benefits but are limited by their
small size, narrow scope, and exposure to LLM training. These gaps highlight
the need for secure, comprehensive, and representative datasets. Conclusion: A
standardized framework is critical for evaluating LLMs in medicine.
Collaborative efforts among institutions and policymakers are needed to ensure
datasets and methodologies are rigorous, unbiased, and reflective of clinical
complexities.

</details>


### [166] [From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation](https://arxiv.org/abs/2507.08924)
*Seokhee Hong,Sunkyoung Kim,Guijin Son,Soyeon Kim,Yeonjung Hong,Jinsik Lee*

Main category: cs.CL

TL;DR: 本文引入了两个韩国专家级基准KMMLU-Redux和KMMLU-Pro，用于评估大型语言模型在工业知识领域的表现，并公开发布了数据集。


<details>
  <summary>Details</summary>
Motivation: 开发鲁棒的基准测试，涵盖学术和工业领域，以评估大型语言模型在实际场景中的适用性。

Method: 通过重新构建KMMLU（修复关键错误）和创建基于韩国国家专业资格考试的KMMLU-Pro，形成两个评估基准。

Result: 实验结果表明，这些基准可以全面代表韩国工业知识。

Conclusion: 两个基准对评估大型语言模型在韩国特定行业知识领域的有效性有积极作用，且数据集已公开发布。

Abstract: The development of Large Language Models (LLMs) requires robust benchmarks
that encompass not only academic domains but also industrial fields to
effectively evaluate their applicability in real-world scenarios. In this
paper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,
reconstructed from the existing KMMLU, consists of questions from the Korean
National Technical Qualification exams, with critical errors removed to enhance
reliability. KMMLU-Pro is based on Korean National Professional Licensure exams
to reflect professional knowledge in Korea. Our experiments demonstrate that
these benchmarks comprehensively represent industrial knowledge in Korea. We
release our dataset publicly available.

</details>


### [167] [Self-Improving Model Steering](https://arxiv.org/abs/2507.08967)
*Rongyi Zhu,Yuhui Wang,Tanqiu Jiang,Jiacheng Liang,Ting Wang*

Main category: cs.CL

TL;DR: 提出了一个名为SIMS的无监督自我改进模型引导框架，用于提升语言模型推理时的自适应能力和引导效果。


<details>
  <summary>Details</summary>
Motivation: 传统模型引导方法过度依赖外部标注数据，限制了在不同上下文中的适应性，并受到标注质量的束缚。

Method: SIMS通过自我改进循环自动生成和优化对比样本，无需外部监督，同时采用提示排序和对比采样等策略提升效果。

Result: 在多个语言模型和评测基准上，SIMS在引导有效性和自适应能力方面显著优于现有方法。

Conclusion: SIMS展示了自我改进模型引导的潜力，是未来推理阶段语言模型对齐研究的有前景方向。

Abstract: Model steering represents a powerful technique that dynamically aligns large
language models (LLMs) with human preferences during inference. However,
conventional model-steering methods rely heavily on externally annotated data,
not only limiting their adaptability to varying contexts but also tethering
their effectiveness to annotation quality. In this paper, we present SIMS, the
first self-improving model-steering framework that operates without relying on
external supervision. At its core, SIMS autonomously generates and refines
contrastive samples through iterative self-improvement cycles, enabling
adaptive, context-specific steering. Additionally, SIMS employs novel
strategies, including prompt ranking and contrast sampling, to further enhance
steering efficacy. Extensive evaluation across diverse LLMs and benchmarks
demonstrates that SIMS substantially outperforms existing methods in steering
effectiveness and adaptability, highlighting self-improving model steering as a
promising direction for future research on inference-time LLM alignment.

</details>


### [168] [Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR](https://arxiv.org/abs/2507.08969)
*Drew Walker,Jennifer Love,Swati Rajwal,Isabel C Walker,Hannah LF Cooper,Abeed Sarker,Melvin Livingston III*

Main category: cs.CL

TL;DR: 研究表明EHR中存在某些群体被污名化语言的现象，由多种医疗提供者使用，且集中于历史上容易被污名化的患者。


<details>
  <summary>Details</summary>
Motivation: 探讨电子健康记录（EHR）中的污名化语言如何针对不同患者群体被使用，并通过语言特点分析其传播机制。

Method: 通过扩展词典匹配和监督学习分类器识别MIMIC-III EHR中的疑问标记和污名化标签特征，并使用泊松回归模型评估语言特征的使用率。

Result: 非裔美国患者、使用政府保险或自付的患者以及患有特定疾病的患者被记录污名化标签的几率更高；男性患者更易出现疑问标记；护士和社会工作者使用污名化标签及疑问标记的比率更高。

Conclusion: EHR中的污名化言语主要针对历史上被污名化的群体，多种医疗服务提供者均有参与，这可能对患者医疗结果造成不良影响。

Abstract: Introduction: Electronic health records (EHR) are a critical medium through
which patient stigmatization is perpetuated among healthcare teams. Methods: We
identified linguistic features of doubt markers and stigmatizing labels in
MIMIC-III EHR via expanded lexicon matching and supervised learning
classifiers. Predictors of rates of linguistic features were assessed using
Poisson regression models. Results: We found higher rates of stigmatizing
labels per chart among patients who were Black or African American (RR: 1.16),
patients with Medicare/Medicaid or government-run insurance (RR: 2.46),
self-pay (RR: 2.12), and patients with a variety of stigmatizing disease and
mental health conditions. Patterns among doubt markers were similar, though
male patients had higher rates of doubt markers (RR: 1.25). We found increased
stigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),
with similar patterns of doubt markers. Discussion: Stigmatizing language
occurred at higher rates among historically stigmatized patients, perpetuated
by multiple provider types.

</details>


### [169] [Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery](https://arxiv.org/abs/2507.09011)
*Ana Chkhaidze,Reshanne R. Reeder,Connor Gag,Anastasia Kiyonaga,Seana Coulson*

Main category: cs.CL

TL;DR: 本研究探讨了内在视觉系统的差异如何影响Ganzflicker诱导的视觉幻觉，用自然语言处理工具分析了4000多名参与者的幻觉描述，结果表明影像能力强的个体和弱的个体之间的视觉幻觉有显著差异。


<details>
  <summary>Details</summary>
Motivation: 研究视觉系统的个体差异如何影响在Ganzflicker刺激下产生的视觉幻觉，揭示影像能力对视觉生成的复杂性影响。

Method: 通过自然语言处理工具分析4000多名参与者对于Ganzflicker诱导幻觉的自由文本描述，比较影像能力不同的个体所描述的视觉内容与语言使用模式。

Result: 影像能力强的个体描述更加复杂、贴近自然的视觉内容，而影像能力弱的个体主要描述简单几何图形。同时，视觉语言模型比单独的文本语言模型更好地捕捉到这些差异。

Conclusion: 研究表明，视觉影像能力强弱与视觉诱发区域和更高层次脑区的协调存在差异，这种能力会影响视觉幻觉的复杂性以及语言描述中的感官运动关联性。

Abstract: A rapidly alternating red and black display known as Ganzflicker induces
visual hallucinations that reflect the generative capacity of the visual
system. Recent proposals regarding the imagery spectrum, that is, differences
in the visual system of individuals with absent imagery, typical imagery, and
vivid imagery, suggest these differences should impact the complexity of other
internally generated visual experiences. Here, we used tools from natural
language processing to analyze free-text descriptions of hallucinations from
over 4,000 participants, asking whether people with different imagery
phenotypes see different things in their mind's eye during Ganzflicker-induced
hallucinations. Strong imagers described complex, naturalistic content, while
weak imagers reported simple geometric patterns. Embeddings from vision
language models better captured these differences than text-only language
models, and participants with stronger imagery used language with richer
sensorimotor associations. These findings may reflect individual variation in
coordination between early visual areas and higher-order regions relevant for
the imagery spectrum.

</details>


### [170] [Lizard: An Efficient Linearization Framework for Large Language Models](https://arxiv.org/abs/2507.09025)
*Chien Van Nguyen,Ruiyi Zhang,Hanieh Deilamsalehy,Puneet Mathur,Viet Dac Lai,Haoliang Wang,Jayakumar Subramanian,Ryan A. Rossi,Trung Bui,Nikos Vlassis,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: Lizard是一个线性化框架，它通过引入新的注意力机制，将大规模预训练语言模型 (LLMs) 转变为具有无限上下文生成能力的亚二次架构，从而减少了上下文长度对计算及内存的限制问题。


<details>
  <summary>Details</summary>
Motivation: 解决基于Transformer的大型语言模型在上下文长度增加时面临的内存和计算瓶颈问题。

Method: 提出一种新的亚二次注意力机制，结合门控模块和滑动窗口注意力，提供长距离依赖和局部交互能力，同时引入硬件感知优化算法以加速训练。

Result: 在标准语言建模任务中性能几乎无损恢复，并显著优于之前提出的线性化方法；在5-shot MMLU基准上提升18分，并在联想回忆任务中表现出显著进步。

Conclusion: Lizard成功解决了长上下文的计算瓶颈问题，同时实现了灵活性和性能的平衡，是一种重要的线性化架构创新。

Abstract: We propose Lizard, a linearization framework that transforms pretrained
Transformer-based Large Language Models (LLMs) into flexible, subquadratic
architectures for infinite-context generation. Transformer-based LLMs face
significant memory and computational bottlenecks as context lengths increase,
due to the quadratic complexity of softmax attention and the growing key-value
(KV) cache. Lizard addresses these limitations by introducing a subquadratic
attention mechanism that closely approximates softmax attention while
preserving the output quality. Unlike previous linearization methods, which are
often limited by fixed model structures and therefore exclude gating
mechanisms, Lizard incorporates a gating module inspired by recent
state-of-the-art linear models. This enables adaptive memory control, supports
constant-memory inference, offers strong length generalization, and allows more
flexible model design. Lizard combines gated linear attention for global
context compression with sliding window attention enhanced by meta memory,
forming a hybrid mechanism that captures both long-range dependencies and
fine-grained local interactions. Moreover, we introduce a hardware-aware
algorithm that accelerates the training speed of our models. Extensive
experiments show that Lizard achieves near-lossless recovery of the teacher
model's performance across standard language modeling tasks, while
significantly outperforming previous linearization methods. On the 5-shot MMLU
benchmark, Lizard improves over prior models by 18 points and shows significant
improvements on associative recall tasks.

</details>


### [171] [ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making](https://arxiv.org/abs/2507.09037)
*Bharadwaj Ravichandran,David Joy,Paul Elliott,Brian Hu,Jadie Adams,Christopher Funk,Emily Veenhuis,Anthony Hoogs,Arslan Basharat*

Main category: cs.CL

TL;DR: 本文提出了ALIGN系统，通过提示对齐技术实现动态个性化LLM决策支持，以更好满足用户多样化的价值观与偏好。


<details>
  <summary>Details</summary>
Motivation: LLMs在决策支持领域的应用日益增多，但如何适应用户多样化的价值观和偏好是当前的挑战。

Method: 设计了ALIGN系统，通过提示对齐实现对细粒度属性的动态个性化，同时提供模块化后端、结构化输出和可替换的LLM核心算法。

Result: 在公共意见调查的人口统计对齐与医疗分类决策的价值对齐上进行了定量分析，证明了ALIGN的有效性。

Conclusion: ALIGN框架促进了研究人员开发可靠、负责任且个性化的LLM决策支持系统，其开源特性加速了相关研究与应用的推进。

Abstract: Large language models (LLMs) are increasingly being used as decision aids.
However, users have diverse values and preferences that can affect their
decision-making, which requires novel methods for LLM alignment and
personalization. Existing LLM comparison tools largely focus on benchmarking
tasks, such as knowledge-based question answering. In contrast, our proposed
ALIGN system focuses on dynamic personalization of LLM-based decision-makers
through prompt-based alignment to a set of fine-grained attributes. Key
features of our system include robust configuration management, structured
output generation with reasoning, and several algorithm implementations with
swappable LLM backbones, enabling different types of analyses. Our user
interface enables a qualitative, side-by-side comparison of LLMs and their
alignment to various attributes, with a modular backend for easy algorithm
integration. Additionally, we perform a quantitative analysis comparing
alignment approaches in two different domains: demographic alignment for public
opinion surveys and value alignment for medical triage decision-making. The
entire ALIGN framework is open source and will enable new research on reliable,
responsible, and personalized LLM-based decision-makers.

</details>


### [172] [OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique](https://arxiv.org/abs/2507.09075)
*Wasi Uddin Ahmad,Somshubra Majumdar,Aleksander Ficek,Sean Narenthiran,Mehrzad Samadi,Jocelyn Huang,Siddhartha Jain,Vahid Noroozi,Boris Ginsburg*

Main category: cs.CL

TL;DR: 提出OpenCodeReasoning-II数据集，包含2.5M代码相关样本，通过二阶段微调提升代码生成与批判性能，并扩展LiveCodeBench支持C++评测。


<details>
  <summary>Details</summary>
Motivation: 探索基于推理的大型语言模型在代码生成与批判中的潜力，并应对高质量大规模数据集的需求。

Method: 提出OpenCodeReasoning-II数据集，并采用两阶段微调：第一阶段专注代码生成微调；第二阶段联合训练代码生成和批判模型。

Result: 微调后的Qwen2.5-Instruct模型在代码生成方面超越或匹配现有最佳模型；结合生成和批判模型显著提升竞争编程性能。

Conclusion: 提出的框架和数据集极大推动了代码生成与批判中的性能，同时提供了支持C++编程语言评测的新基准。

Abstract: Recent advancements in reasoning-based Large Language Models (LLMs),
particularly their potential through test-time scaling, have created
significant opportunities for distillation in code generation and critique.
However, progress in both areas fundamentally depends on large-scale,
high-quality datasets. In this work, we introduce OpenCodeReasoning-II, a
dataset consists of 2.5M question-solution-critique triples (approx. 35K unique
programming questions), making it nearly twice the size of the previous largest
publicly available code reasoning dataset. In this work, we employ a two-stage
supervised fine-tuning strategy. The first stage focuses on fine-tuning for
code generation, while the second stage involves the joint training of models
for both code generation and critique. Our resulting finetuned Qwen2.5-Instruct
models achieve performance in code generation that either exceeds or equals the
best prior open-weight distilled models. Notably, the integration of our code
generation and critique models leads to significant improvements in competitive
coding performance. Furthermore, we present an extension of the LiveCodeBench
benchmark to specifically support the C++ programming language, thereby
facilitating more comprehensive LLM evaluation using this benchmark.

</details>


### [173] [Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation](https://arxiv.org/abs/2507.09076)
*Jialong Mai,Xiaofen Xing,Yawei Li,Zhipeng Li,Jingyuan Xing,Xiangmin Xu*

Main category: cs.CL

TL;DR: 本文提出了一种动态参数记忆(DPM)机制，有效提升了语音大语言模型(SLLM)在处理长音频片段时的情感识别能力，并在IEMOCAP数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 目前语音大语言模型（SLLM）在处理语音情感识别（SER）时受到语音模态高帧率的限制，无法有效处理较长音频，同时现有方法未能充分利用跨多轮对话的情感连续性和惯性。

Method: 提出了一种动态参数记忆（DPM）机制，通过上下文语义和句子级情感编码进行推断，利用临时LoRA模块逐步编码句子层面的信息与情感，突破SLLM的上下文窗口限制，实现对无限长音频的处理。

Result: 在IEMOCAP数据集上的实验结果显示，DPM能够显著提升SLLM在长音频序列情感识别中的能力，并取得了最新的最优性能。

Conclusion: DPM机制为SLLM在长音频情感识别中的应用提供了新的可能性，有助于解决现有方法在上下文处理能力上的局限性。

Abstract: Recent research has focused on applying speech large language model (SLLM) to
improve speech emotion recognition (SER). However, the inherently high frame
rate in speech modality severely limits the signal processing and understanding
capabilities of SLLM. For example, a SLLM with a 4K context window can only
process 80 seconds of audio at 50Hz feature sampling rate before reaching its
capacity limit. Input token compression methods used in SLLM overlook the
continuity and inertia of emotions across multiple conversation turns. This
paper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual
semantics and sentence-level emotion encoding, enabling processing of
unlimited-length audio with limited context windows in SLLM. Specifically, DPM
progressively encodes sentence-level information and emotions into a temporary
LoRA module during inference to effectively "memorize" the contextual
information. We trained an emotion SLLM as a backbone and incorporated our DPM
into inference for emotion recognition in conversation (ERC). Experimental
results on the IEMOCAP dataset show that DPM significantly improves the emotion
recognition capabilities of SLLM when processing long audio sequences,
achieving state-of-the-art performance.

</details>


### [174] [CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards](https://arxiv.org/abs/2507.09104)
*Taolin Zhang,Maosong Cao,Alexander Lam,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 提出了一种新的通用判决模型CompassJudger-2，通过任务驱动的多领域数据策略和先进的学习目标，显著提升了LLM评判能力和鲁棒性，同时提出了易于标准化的评估基准JudgerBenchV2。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评判模型存在专业性狭窄和鲁棒性有限的问题，难以进行全面的评估。

Method: 通过任务驱动的多领域数据策划策略，结合可验证奖励监督判决任务，并引入拒绝采样促进内在批判性推理，同时采用跨批次策略梯度损失优化学习目标。

Result: CompassJudger-2在多个评估基准和奖励基准上表现卓越，7B参数规模的模型与大型模型相比显示出很强的竞争力。

Conclusion: CompassJudger-2提高了LLM评判的鲁棒性和可扩展性，提出了新的性能和评估标准，为评判模型的进一步发展奠定了基础。

Abstract: Recently, the role of LLM-as-judge in evaluating large language models has
gained prominence. However, current judge models suffer from narrow
specialization and limited robustness, undermining their capacity for
comprehensive evaluations. In this work, we present CompassJudger-2, a novel
generalist judge model that overcomes these limitations via a task-driven,
multi-domain data curation strategy. Central to our approach is supervising
judgment tasks with verifiable rewards, guiding intrinsic critical reasoning
through rejection sampling to foster robust, generalizable judgment
capabilities. We introduce a refined learning objective with margin policy
gradient loss to enhance performance. Empirically, CompassJudger-2 achieves
superior results across multiple judge and reward benchmarks, and our 7B model
demonstrates competitive judgment accuracy with significantly larger models
like DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a
comprehensive benchmark evaluating cross-domain judgment accuracy and rank
consistency to standardize judge model evaluation. These contributions advance
robust, scalable LLM judgment and establish new performance and evaluation
standards.

</details>


### [175] [OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering](https://arxiv.org/abs/2507.09155)
*Ali Vosoughi,Ayoub Shahnazari,Yufeng Xi,Zeliang Zhang,Griffin Hess,Chenliang Xu,Niaz Abdolrahim*

Main category: cs.CL

TL;DR: OPENXRD 是一种利用 GPT-4.5 生成简洁支持内容的开放式解答晶体学问题的工具，通过生成紧凑的领域特定参考资料帮助小型模型理解 X 射线衍射 (XRD) 的关键概念，从而提升准确性。


<details>
  <summary>Details</summary>
Motivation: 解决传统晶体学教学中扫描教材可能涉及版权问题的局限，同时通过生成专业、浓缩的知识点来帮助模型进行科学问题推理。

Method: 设计 OPENXRD 管线，利用 GPT-4.5 生成支持性内容，并将其与现有视觉-语言模型结合，在闭卷（无支持材料）和开卷（有支持材料）语境下评估性能。

Result: 实验结果表明，使用 GPT-4.5 生成支持内容的模型准确性显著提高，尤其是在晶体学方面训练较少的模型上提升明显。

Conclusion: OPENXRD 展示了专用开放式系统在材料科学中的潜力，并为科学领域的 NLP 工具奠定了基础，同时未来可探索增加图示数据以扩展功能。

Abstract: This work presents OPENXRD, an open-book pipeline designed for
crystallography question answering, which integrates textual prompts with
concise supporting content generated by GPT-4.5. Instead of using scanned
textbooks, which may lead to copyright issues, OPENXRD generates compact,
domain-specific references that help smaller models understand key concepts in
X-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217
expert-level XRD questions by comparing different vision-language models,
including GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,
under both closed-book (without supporting material) and open-book (with
supporting material) conditions. Our experimental results show significant
accuracy improvements in models that use the GPT-4.5-generated summaries,
particularly those with limited prior training in crystallography. OPENXRD uses
knowledge from larger models to fill knowledge gaps in crystallography and
shows that AI-generated texts can help smaller models reason more effectively
in scientific tasks. While the current version of OPENXRD focuses on text-based
inputs, we also explore future extensions such as adding real crystal diagrams
or diffraction patterns to improve interpretation in specialized materials
science contexts. Overall, OPENXRD shows that specialized open-book systems can
be useful in materials science and provides a foundation for broader natural
language processing (NLP) tools in critical scientific fields.

</details>


### [176] [PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning](https://arxiv.org/abs/2507.09157)
*Bhavinkumar Vinodbhai Kuwar,Bikrant Bikram Pratap Maurya,Priyanshu Gupta,Nitin Choudhury*

Main category: cs.CL

TL;DR: 该论文提出了一种基于PU学习的轻量级模型，用于在战略对话中检测谎言，并在Diplomacy数据集上实现了最佳宏F1得分。


<details>
  <summary>Details</summary>
Motivation: 战略对话中的谎言检测任务具有高度复杂性，且存在明显的类别不平衡现象，这促使开发针对性的解决方案。

Method: 提出了一种结合冻结的BERT嵌入、可解释的语言与游戏特定特征以及PU学习目标的模型，使得模型适应极小比例标记数据的环境。

Result: 模型实现了宏F1得分0.60的新高，并显著减少了可训练参数量，同时表明PU学习的有效性与解释能力。

Conclusion: 通过PU学习能够更优先地捕获稀少但关键的欺诈类，有助于在战略环境中更准确地检测谎言。

Abstract: Detecting deception in strategic dialogues is a complex and high-stakes task
due to the subtlety of language and extreme class imbalance between deceptive
and truthful communications. In this work, we revisit deception detection in
the Diplomacy dataset, where less than 5% of messages are labeled deceptive. We
introduce a lightweight yet effective model combining frozen BERT embeddings,
interpretable linguistic and game-specific features, and a Positive-Unlabeled
(PU) learning objective. Unlike traditional binary classifiers, PU-Lie is
tailored for situations where only a small portion of deceptive messages are
labeled, and the majority are unlabeled. Our model achieves a new best macro F1
of 0.60 while reducing trainable parameters by over 650x. Through comprehensive
evaluations and ablation studies across seven models, we demonstrate the value
of PU learning, linguistic interpretability, and speaker-aware representations.
Notably, we emphasize that in this problem setting, accurately detecting
deception is more critical than identifying truthful messages. This priority
guides our choice of PU learning, which explicitly models the rare but vital
deceptive class.

</details>


### [177] [RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking](https://arxiv.org/abs/2507.09174)
*Shuo Yang,Zijian Yu,Zhenzhe Ying,Yuqin Dai,Guoqing Wang,Jun Lan,Jinfeng Xu,Jinze Li,Edith C. H. Ngai*

Main category: cs.CL

TL;DR: 提出了一种称为RAMA的新型框架，用于验证多媒体信息的真实性，尤其对模糊或不足上下文的声明表现优越。


<details>
  <summary>Details</summary>
Motivation: 应对多模态错误信息快速传播带来的自动化事实核查挑战，尤其是对于模糊或上下文不足的声明。

Method: 设计了RAMA，一个增量式检索-多代理框架，其核心创新包括：（1）将多模态声明转化为精准的网络搜索查询；（2）从多样性权威来源聚合交叉验证证据；（3）利用多模态大型语言模型和多种提示变体的多代理架构。

Result: 实验表明RAMA在标杆数据集上表现优异，尤其能处理模糊或不大可能的声明，通过检索到的事实证据进行验证。

Conclusion: 研究成果强调了结合网络证据和多代理推理在多媒体验证中不可或缺的作用，为更可靠和可扩展的事实核查方法铺平了道路。

Abstract: The rapid proliferation of multimodal misinformation presents significant
challenges for automated fact-checking systems, especially when claims are
ambiguous or lack sufficient context. We introduce RAMA, a novel
retrieval-augmented multi-agent framework designed for verifying multimedia
misinformation. RAMA incorporates three core innovations: (1) strategic query
formulation that transforms multimodal claims into precise web search queries;
(2) cross-verification evidence aggregation from diverse, authoritative
sources; and (3) a multi-agent ensemble architecture that leverages the
complementary strengths of multiple multimodal large language models and prompt
variants. Extensive experiments demonstrate that RAMA achieves superior
performance on benchmark datasets, particularly excelling in resolving
ambiguous or improbable claims by grounding verification in retrieved factual
evidence. Our findings underscore the necessity of integrating web-based
evidence and multi-agent reasoning for trustworthy multimedia verification,
paving the way for more reliable and scalable fact-checking solutions. RAMA
will be publicly available at https://github.com/kalendsyang/RAMA.git.

</details>


### [178] [Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models](https://arxiv.org/abs/2507.09185)
*Ameen Ali,Shahar Katz,Lior Wolf,Ivan Titov*

Main category: cs.CL

TL;DR: 该论文提出一种细化语言模型的结构以提升泛化能力的新方法，通过修剪模型中特定数据集相关的神经元来减少依赖数据集特定机制。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型常因依赖数据集特性而在新任务或分布中表现下降，作者希望提升模型的泛化能力。

Method: 使用综合梯度算法识别那些对数据集特定表现贡献较多但不能支持可泛化推理的神经元，然后选择性地修剪这些神经元。

Result: 在多项选择基准测试中表现优异，超过此前未采用修剪方法的调优手段。

Conclusion: 该方法通过促进模型依赖于可泛化表征，有效提升了模型的迁移能力。

Abstract: Large language models (LLMs) often develop learned mechanisms specialized to
specific datasets, such as reliance on domain-specific correlations, which
yield high-confidence predictions without generalizable reasoning. While
beneficial in one setting, these dataset-specific mechanisms typically degrade
performance when models encounter novel tasks or distributions. In this work,
we introduce a fine-tuning approach designed to enhance generalization by
identifying and pruning neurons associated with dataset-specific mechanisms in
transformer-based LLMs. Our method employs Integrated Gradients to quantify
each neuron's influence on high-confidence predictions, pinpointing those that
disproportionately contribute to dataset-specific performance without
supporting robust, transferable reasoning. Selectively pruning these neurons
compels the model to depend on generalizable representations. Evaluated across
multiple-choice benchmarks, our pruning-based fine-tuning significantly
enhances performance, surpassing prior (non-pruning) adaptation methods.

</details>


### [179] [Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training](https://arxiv.org/abs/2507.09205)
*Leiyu Pan,Bojian Xiong,Lei Yang,Renren Jin,Shaowei Zhang,Yue Chen,Ling Shi,Jiang Zhou,Junru Wu,Zhen Wang,Jianxiang Peng,Juesi Xiao,Tianyu Dong,Zhuowen Han,Zhuo Chen,Sangjee Dondrub,Caizang Tai,Haixing Zhao,Huaque Cairang,Suonan Cairang,Rou Te,Lengben Zhaxi,Gazang Zhaxi,Zhonglin Ye,Yuhui Zheng,Chunyan Peng,Secha Jia,Pema Tashi,Cizhen Jiacuo,Pema Dorjee,Hongkai Liu,Pema Yanggon,Tsehang Dorjee,Jiaxin Han,Qiongying Hu,Jilin Man,Huanke You,Yuqi Ren,Duo La,Deyi Xiong*

Main category: cs.CL

TL;DR: 提出了一种名为Banzhida的多语种大语言模型，专注于提升藏语的生成能力，并显著超越现有模型表现。


<details>
  <summary>Details</summary>
Motivation: 藏语作为一种低资源语言在现有模型中的表现受到限制，为解决高质量训练语料稀缺的问题，该研究旨在开发一个提高藏语生成能力的模型。

Method: 通过汇总多种来源数据，设计了专门用于藏语的数据清理和处理流程，生成了最大的藏语训练语料库，并基于一个多语种基础模型进行预训练与微调，最终生成了Banzhida模型。

Result: 实验表明Banzhida在多种任务中均显著优于同规模的开源模型和专注藏语的模型。

Conclusion: Banzhida模型显著推进了藏语生成式人工智能的能力，填补了藏语在大语言模型中的空白，具有重要实际意义。

Abstract: Large language models have achieved remarkable progress across many
languages. However, Tibetan, as a representative low-resource language, is
particularly underrepresented in existing models due to the scarcity of
high-quality training corpora. To address this gap, we curate the largest
Tibetan pre-training corpus to date, aggregating data from diverse sources and
applying a dedicated data cleaning and processing pipeline tailored for
Tibetan. With the curated data, we continue pre/post-training a multilingual
base model into Banzhida, a multilingual large language model that advances
generative AI for Tibetan. To evaluate the Tibetan capabilities of the model,
we create new high-quality Tibetan benchmarks, and complement them with
existing public benchmarks. Experimental results demonstrate that Banzhida
consistently and significantly outperforms both open-source models of similar
scale and Tibetan-tailored models across a wide range of tasks.

</details>


### [180] [MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis](https://arxiv.org/abs/2507.09225)
*Biagio Scalingi,Chiara Barattieri di San Pietro,Paolo Canal,Valentina Bambini*

Main category: cs.CL

TL;DR: 本研究开发了一个气候变化视觉隐喻数据库（MetaClimage），分析了其对沟通效果的影响，发现视觉隐喻在美学上更优，但理解难度更高。


<details>
  <summary>Details</summary>
Motivation: 研究气候变化视觉隐喻的影响，解决现有研究材料分散和少量的问题。

Method: 创建了MetaClimage数据库，结合自然语言处理和人类评分（包括理解难度、效果、美学质量、情绪唤起等）。

Result: 视觉隐喻的理解难度更高，但美学上更令人愉悦；与具有更高认知需求的人相比，视觉隐喻唤起的情绪更高，且更易引发积极的情感反应和更深入的认知加工。

Conclusion: 视觉隐喻相比字面图像在唤起认知和美学反应方面有优势，但具有更高的认知负担，未来可用于优化环境传播。

Abstract: Visual metaphors of climate change (e.g., melting glaciers depicted as a
melting ice grenade) are regarded as valuable tools for addressing the
complexity of environmental challenges. However, few studies have examined
their impact on communication, also due to scattered availability of material.
Here, we present a novel database of Metaphors of Climate Change in Images
(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal
images and enriched with human ratings. For each image, we collected values of
difficulty, efficacy, artistic quality, and emotional arousal from human
rating, as well as number of tags generated by participants to summarize the
message. Semantic and emotion variables were further derived from the tags via
Natural Language Processing. Visual metaphors were rated as more difficult to
understand, yet more aesthetically pleasant than literal images, but did not
differ in efficacy and arousal. The latter for visual metaphors, however, was
higher in participants with higher Need For Cognition. Furthermore, visual
metaphors received more tags, often referring to entities not depicted in the
image, and elicited words with more positive valence and greater dominance than
literal images. These results evidence the greater cognitive load of visual
metaphors, which nevertheless might induce positive effects such as deeper
cognitive elaboration and abstraction compared to literal stimuli. Furthermore,
while they are not deemed as more effective and arousing, visual metaphors seem
to generate superior aesthetic appreciation and a more positively valenced
experience. Overall, this study contributes to understanding the impact of
visual metaphors of climate change both by offering a database for future
research and by elucidating a cost-benefit trade-off to take into account when
shaping environmental communication.

</details>


### [181] [Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources](https://arxiv.org/abs/2507.09245)
*Deshan Sumanathilaka,Sameera Perera,Sachithya Dharmasiri,Maneesha Athukorala,Anuja Dilrukshi Herath,Rukshan Dias,Pasindu Gamage,Ruvan Weerasinghe,Y. H. P. P. Priyadarshana*

Main category: cs.CL

TL;DR: 本文介绍了Swa-bhasha资源中心，其包括2020至2025年间开发的罗马化僧伽罗语到僧伽罗语的转写数据和算法。


<details>
  <summary>Details</summary>
Motivation: 为推动僧伽罗语自然语言处理的研究进展，特别是在培训转写模型和开发涉及罗马化僧伽罗语的应用方面。

Method: 将现有数据集和工具公开发布，同时提供对既有转写应用进行对比分析的资源。

Result: 构建了一个全面的数据和算法资源库，并为相关转写模型及应用开发提供了重要支持。

Conclusion: Swa-bhasha资源中心是促进僧伽罗语自然语言处理研究的重要基石，为未来的相关领域研究奠定了坚实基础。

Abstract: The Swa-bhasha Resource Hub provides a comprehensive collection of data
resources and algorithms developed for Romanized Sinhala to Sinhala
transliteration between 2020 and 2025. These resources have played a
significant role in advancing research in Sinhala Natural Language Processing
(NLP), particularly in training transliteration models and developing
applications involving Romanized Sinhala. The current openly accessible data
sets and corresponding tools are made publicly available through this hub. This
paper presents a detailed overview of the resources contributed by the authors
and includes a comparative analysis of existing transliteration applications in
the domain.

</details>


### [182] [Psychology-Driven Enhancement of Humour Translation](https://arxiv.org/abs/2507.09259)
*Yuchen Su,Yonghua Zhu,Yang Chen,Diana Benavides-Prado,Michael Witbrock*

Main category: cs.CL

TL;DR: 该论文提出了一种基于心理学的幽默分解机制（HDM），并结合幽默理论优化翻译幽默文本的可读性和幽默性。实验结果显示，其方法在幽默、流畅性和连贯性方面显著提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的大型语言模型（LLMs）能够处理一般的翻译任务，但在幽默翻译方面表现不佳，主要体现在语言干扰和缺乏幽默感上。

Method: 作者提出了一种基于心理学的幽默分解机制（HDM），结合“思维链”（CoT）模拟人类的思考过程，并融入幽默理论以提升翻译文本的幽默性和可读性。

Result: 在开源幽默数据集上的自动评估实验表明，该方法在幽默性提升了7.75%、流畅性提升了2.81%、连贯性提升了6.13%。

Conclusion: 基于心理学的HDM与幽默理论相结合的翻译模式能有效提升幽默文本的翻译质量，强调方法创新性及其实验成果的有效性。

Abstract: Humour translation plays a vital role as a bridge between different cultures,
fostering understanding and communication. Although most existing Large
Language Models (LLMs) are capable of general translation tasks, these models
still struggle with humour translation, which is especially reflected through
linguistic interference and lacking humour in translated text. In this paper,
we propose a psychology-inspired Humour Decomposition Mechanism (HDM) that
utilises Chain-of-Thought (CoT) to imitate the ability of the human thought
process, stimulating LLMs to optimise the readability of translated humorous
texts. Moreover, we integrate humour theory in HDM to further enhance the
humorous elements in the translated text. Our automatic evaluation experiments
on open-source humour datasets demonstrate that our method significantly
improves the quality of humour translation, yielding average gains of 7.75\% in
humour, 2.81\% in fluency, and 6.13\% in coherence of the generated text.

</details>


### [183] [ClaritySpeech: Dementia Obfuscation in Speech](https://arxiv.org/abs/2507.09282)
*Dominika Woszczyk,Ranya Aloufi,Soteris Demetriou*

Main category: cs.CL

TL;DR: 本文提出了一个名为ClaritySpeech的系统，该系统结合ASR、文本模糊化和零样本TTS，旨在改进痴呆症影响的语音隐私和可访问性问题。


<details>
  <summary>Details</summary>
Motivation: 目前语音技术在处理痴呆症和非典型语音时存在困难，影响了交流和隐私保护需求，因此，需要一种新技术解决这些问题。

Method: ClaritySpeech综合了自动语音转录、文本模糊化和零样本文本到语音技术，以无需微调的方式改善痴呆症语音表现，同时保持讲话者身份。

Result: 在ADReSS和ADReSSo数据集上，系统分别降低16%、10%的平均F1分数；WER显著改善（从0.73降到0.08、或至0.15），语音质量评分提升至2.15，且保留了50%的讲话者相似性。

Conclusion: 该研究证明，ClaritySpeech可有效改善痴呆症语音的隐私和可访问性，是在低数据环境下一种可靠的语音处理方法。

Abstract: Dementia, a neurodegenerative disease, alters speech patterns, creating
communication barriers and raising privacy concerns. Current speech
technologies, such as automatic speech transcription (ASR), struggle with
dementia and atypical speech, further challenging accessibility. This paper
presents a novel dementia obfuscation in speech framework, ClaritySpeech,
integrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to
correct dementia-affected speech while preserving speaker identity in low-data
environments without fine-tuning. Results show a 16% and 10% drop in mean F1
score across various adversarial settings and modalities (audio, text, fusion)
for ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We
also find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15
for ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and
accessibility.

</details>


### [184] [DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models](https://arxiv.org/abs/2507.09424)
*Cathy Jiao,Yijun Pan,Emily Xiao,Daisy Sheng,Niket Jain,Hanzhang Zhao,Ishita Dasgupta,Jiaqi W. Ma,Chenyan Xiong*

Main category: cs.CL

TL;DR: 本文提出DATE-LM，一个用于评估LLM数据归因方法的统一基准，涵盖数据选择、毒性/偏差过滤和事实归因三大任务。


<details>
  <summary>Details</summary>
Motivation: 当前数据归因方法在LLM研究和应用中重要性日增，但缺乏系统的评估体系。本文旨在填补这一空白。

Method: 提出DATE-LM基准，量化评估数据归因方法。涵盖三大任务：数据选择、毒性/偏差过滤、事实归因，并支持大规模评估及多架构模型的应用。

Result: 通过DATE-LM展开大规模现有方法评估，发现无单一方法在所有任务中占优，数据归因方法与简单基线存在权衡，且性能受任务评估设计敏感。

Conclusion: DATE-LM为未来LLM数据归因研究提供了基础，发布公开排行榜以推动方法对比和社区参与。

Abstract: Data attribution methods quantify the influence of training data on model
outputs and are becoming increasingly relevant for a wide range of LLM research
and applications, including dataset curation, model interpretability, data
valuation. However, there remain critical gaps in systematic LLM-centric
evaluation of data attribution methods. To this end, we introduce DATE-LM (Data
Attribution Evaluation in Language Models), a unified benchmark for evaluating
data attribution methods through real-world LLM applications. DATE-LM measures
attribution quality through three key tasks -- training data selection,
toxicity/bias filtering, and factual attribution. Our benchmark is designed for
ease of use, enabling researchers to configure and run large-scale evaluations
across diverse tasks and LLM architectures. Furthermore, we use DATE-LM to
conduct a large-scale evaluation of existing data attribution methods. Our
findings show that no single method dominates across all tasks, data
attribution methods have trade-offs with simpler baselines, and method
performance is sensitive to task-specific evaluation design. Finally, we
release a public leaderboard for quick comparison of methods and to facilitate
community engagement. We hope DATE-LM serves as a foundation for future data
attribution research in LLMs.

</details>


### [185] [Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models](https://arxiv.org/abs/2507.09470)
*Mingchuan Yang,Ziyuan Huang*

Main category: cs.CL

TL;DR: 研究优化了DRAGON Longformer模型用于临床文本分类，显著提高了分类性能。


<details>
  <summary>Details</summary>
Motivation: 针对医疗案例的二分类任务，提升模型理解和处理临床文本的能力。

Method: 对joeranbosma/dragon-longformer-base-mixed-domain模型进行超参数调优，领域特定预处理，架构改进等，包括序列长度扩展、学习率调整、训练轮次增加及医学术语加入。

Result: 优化模型将准确率提升到85.2%，精确率提升到84.1%，召回率提升到86.3%，F1分数提升到85.2%，优化显著（p < 0.001）。

Conclusion: 优化后的模型在理解医学术语及临床观测方面表现卓越，为临床自然语言处理应用及领域语言模型研究提供了重要助力。

Abstract: This study explores the optimization of the DRAGON Longformer base model for
clinical text classification, specifically targeting the binary classification
of medical case descriptions. A dataset of 500 clinical cases containing
structured medical observations was used, with 400 cases for training and 100
for validation. Enhancements to the pre-trained
joeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter
tuning, domain-specific preprocessing, and architectural adjustments. Key
modifications involved increasing sequence length from 512 to 1024 tokens,
adjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5
to 8, and incorporating specialized medical terminology. The optimized model
achieved notable performance gains: accuracy improved from 72.0% to 85.2%,
precision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from
71.0% to 85.2%. Statistical analysis confirmed the significance of these
improvements (p < .001). The model demonstrated enhanced capability in
interpreting medical terminology, anatomical measurements, and clinical
observations. These findings contribute to domain-specific language model
research and offer practical implications for clinical natural language
processing applications. The optimized model's strong performance across
diverse medical conditions underscores its potential for broad use in
healthcare settings.

</details>


### [186] [The CoNLL-2013 Shared Task on Grammatical Error Correction](https://arxiv.org/abs/2507.09474)
*Hwee Tou Ng,Siew Mei Wu,Yuanbin Wu,Christian Hadiwinoto,Joel Tetreault*

Main category: cs.CL

TL;DR: 本文概述了CoNLL-2013共享任务的定义、数据集、评估指标和评分方法，并总结了参与团队的方法和结果。


<details>
  <summary>Details</summary>
Motivation: 推动对语法错误自动纠正任务的研究和创新。

Method: 定义任务，提供数据集，制定评估指标和评分规则，并汇总参与团队的各种方法。

Result: 展示了参与团队在该任务上的表现结果及评估数据。

Conclusion: 共享任务为语法错误纠正领域提供了一个平台，促进了方法间的比较和技术进步。

Abstract: The CoNLL-2013 shared task was devoted to grammatical error correction. In
this paper, we give the task definition, present the data sets, and describe
the evaluation metric and scorer used in the shared task. We also give an
overview of the various approaches adopted by the participating teams, and
present the evaluation results.

</details>


### [187] [Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs](https://arxiv.org/abs/2507.09477)
*Yangning Li,Weizhi Zhang,Yuyao Yang,Wei-Chieh Huang,Yaozu Wu,Junyu Luo,Yuanchen Bei,Henry Peng Zou,Xiao Luo,Yusheng Zhao,Chunkit Chan,Yankai Chen,Zhongfen Deng,Yinghui Li,Hai-Tao Zheng,Dongyuan Li,Renhe Jiang,Ming Zhang,Yangqiu Song,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文综述了结合检索与推理的系统，有助于提高知识密集型任务的性能，提出了相关方法、数据集及开放挑战。


<details>
  <summary>Details</summary>
Motivation: 克服当前检索增强生成（RAG）模型在多步推理任务上的不足，以及推理导向方法的事实性问题，提出更加统一的框架。

Method: 分析如何通过高级推理优化RAG的每一步（Reasoning-Enhanced RAG），以及用检索知识弥补推理缺失的前提并扩展背景（RAG-Enhanced Reasoning）；探讨高效融合搜寻与推理的框架（Synergized RAG-Reasoning）。

Result: 提出了一系列分类方法、数据集与开放挑战，展示了RAG-Reasoning在知识密集型基准上的成果和优势。

Conclusion: 未来的RAG-Reasoning系统需更加高效、多模态适应、值得信赖且以人为中心，研究进一步发展方向。

Abstract: Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language
Models (LLMs) by injecting external knowledge, yet it falls short on problems
that demand multi-step inference; conversely, purely reasoning-oriented
approaches often hallucinate or mis-ground facts. This survey synthesizes both
strands under a unified reasoning-retrieval perspective. We first map how
advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,
we show how retrieved knowledge of different type supply missing premises and
expand context for complex inference (RAG-Enhanced Reasoning). Finally, we
spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs
iteratively interleave search and reasoning to achieve state-of-the-art
performance across knowledge-intensive benchmarks. We categorize methods,
datasets, and open challenges, and outline research avenues toward deeper
RAG-Reasoning systems that are more effective, multimodally-adaptive,
trustworthy, and human-centric. The collection is available at
https://github.com/DavidZWZ/Awesome-RAG-Reasoning.

</details>


### [188] [ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning](https://arxiv.org/abs/2507.09482)
*Changli Wang,Rui Wu,Fang Yin*

Main category: cs.CL

TL;DR: 本文提出了一个包含4970个样本的多模态讽刺生成数据集（M2SaG）以及一个生成框架（ViSP），对讽刺文本生成进行了有效探索和改进。


<details>
  <summary>Details</summary>
Motivation: 讽刺文本的生成在相关研究中被忽视，主要是由于目前方法对文本模态的过度依赖以及对视觉线索的忽视。此外, 现有数据集中图像内容与讽刺意图不匹配，也制约了讽刺生成研究的发展。

Method: 提出一个新的数据集M2SaG，该数据集包括图像、讽刺文本及讽刺目标。同时，提出了ViSP生成框架，结合了Proximal Policy Optimization（PPO）和对比学习。PPO通过DIP的奖励分数来引导讽刺文本生成，对比学习则鼓励模型生成更高质量的内容。

Result: 实验结果表明，ViSP优于大语言模型等基线平台。生成的讽刺文本在讽刺分数（平均0.898）和事实矛盾分数（平均0.768）上均超过原始数据集（分别为0.770和0.739）。

Conclusion: 数据集和生成框架ViSP有助于提升讽刺文本生成质量，验证了多模态数据结合在讽刺生成中的潜力。

Abstract: Human emotions are complex, with sarcasm being a subtle and distinctive form.
Despite progress in sarcasm research, sarcasm generation remains underexplored,
primarily due to the overreliance on textual modalities and the neglect of
visual cues, as well as the mismatch between image content and sarcastic intent
in existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm
generation dataset with 4,970 samples, each containing an image, a sarcastic
text, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation
framework that integrates Proximal Policy Optimization (PPO) and contrastive
learning. PPO utilizes reward scores from DIP to steer the generation of
sarcastic texts, while contrastive learning encourages the model to favor
outputs with higher reward scores. These strategies improve overall generation
quality and produce texts with more pronounced sarcastic intent. We evaluate
ViSP across five metric sets and find it surpasses all baselines, including
large language models, underscoring their limitations in sarcasm generation.
Furthermore, we analyze the distributions of Sarcasm Scores and Factual
Incongruity for both M2SaG and the texts generated by ViSP. The generated texts
exhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity
(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic
content than the original dataset. % The dataset and code will be publicly
available. Our dataset and code will be released at
\textit{https://github.com/wclapply/ViSP}.

</details>


### [189] [Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.09485)
*Junjie Liu,Yuanhe Tian,Yan Song*

Main category: cs.CL

TL;DR: 本文探讨了利用大语言模型（LLM）进行基于方面的情感分析（ABSA），并通过数据增强技术解决短文本和数据偏不平衡的问题。研究提出了一种结合LLM和强化学习的数据增强方法，以提升ABSA模型性能。


<details>
  <summary>Details</summary>
Motivation: ABSA是社交媒体场景下重要的细粒度任务，但由于短文本和带有偏倚的小规模标注数据集的限制，现有研究面临学习上下文信息的挑战。

Method: 本文提出一种基于LLM的数据增强方法，通过提示生成更大的、标注分布均衡的训练数据，并引入强化学习 技术优化数据增强流程，提高生成数据的质量。

Result: 在英文ABSA基准数据集上的实验结果显示，与现有的强基线和大多数相关研究相比，本文方法具有优越的性能表现。

Conclusion: 结合LLM和强化学习的数据增强方法有效解决了ABSA任务中的短文本和数据分布不均的问题，在ABSA模型性能提升方面具有显著效果。

Abstract: Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in
social media scenarios to identify the sentiment polarity of specific aspect
terms in a sentence. Although many existing studies leverage large language
models (LLMs) to perform ABSA due to their strong context understanding
capabilities, they still face challenges to learn the context information in
the running text because of the short text, as well as the small and unbalanced
labeled training data, where most data are labeled with positive sentiment.
Data augmentation (DA) is a feasible strategy for providing richer contextual
information, especially when using LLMs to create synthetic training data, but
faces challenges in ensuring a high quality of the augmented data.In this
paper, we propose an LLM-based ABSA approach with training data
augmentation.Specifically, an LLM is prompted to generate augmented training
data based on the original training data, so as to construct a new training
data with larger size and balanced label distributions to better train an ABSA
model. Meanwhile, in order to improve the quality of the augmented data, we
propose a reinforcement learning approach to optimize the data augmentation.
LLM.Experiment results and further analyses on English benchmark datasets for
ABSA demonstrate the effectiveness of our approach, where superior performance
is observed over strong baselines and most existing studies.

</details>


### [190] [GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities](https://arxiv.org/abs/2507.09497)
*Siyi Wu,Zeyu Wang,Xinyuan Song,Zhengpeng Zhou,Lifan Sun,Tianyu Shi*

Main category: cs.CL

TL;DR: 提出了一种名为GoalfyMax的框架，用于改进多智能体系统在任务协调、记忆复用和任务分解方面的能力。


<details>
  <summary>Details</summary>
Motivation: 当前企业环境需要处理复杂动态任务的智能系统，而传统AI系统在扩展性能方面存在局限性，因此需要一种新的多智能体协作框架来解决这一问题。

Method: 提出了以Model Context Protocol (MCP) 为基础的Agent-to-Agent (A2A)通信层，并引入Experience Pack (XP)架构，包括分层记忆系统、多轮上下文对话、长短期记忆模块及动态安全验证，以实现高效的任务分解与协作。

Result: 经过复杂任务协调基准测试和案例研究，GoalfyMax展现出更强的适应性、协作能力与经验复用能力，相较于基线框架表现优越。

Conclusion: GoalfyMax拥有作为新一代多智能体系统的潜力，是一个可扩展且面向未来的智能系统基础。

Abstract: Modern enterprise environments demand intelligent systems capable of handling
complex, dynamic, and multi-faceted tasks with high levels of autonomy and
adaptability. However, traditional single-purpose AI systems often lack
sufficient coordination, memory reuse, and task decomposition capabilities,
limiting their scalability in realistic settings. To address these challenges,
we present \textbf{GoalfyMax}, a protocol-driven framework for end-to-end
multi-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent
(A2A) communication layer built on the Model Context Protocol (MCP), allowing
independent agents to coordinate through asynchronous, protocol-compliant
interactions. It incorporates the Experience Pack (XP) architecture, a layered
memory system that preserves both task rationales and execution traces,
enabling structured knowledge retention and continual learning. Moreover, our
system integrates advanced features including multi-turn contextual dialogue,
long-short term memory modules, and dynamic safety validation, supporting
robust, real-time strategy adaptation. Empirical results on complex task
orchestration benchmarks and case study demonstrate that GoalfyMax achieves
superior adaptability, coordination, and experience reuse compared to baseline
frameworks. These findings highlight its potential as a scalable, future-ready
foundation for multi-agent intelligent systems.

</details>


### [191] [Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models](https://arxiv.org/abs/2507.09506)
*Junjie Wu,Gefei Gu,Yanan Zheng,Dit-Yan Yeung,Arman Cohan*

Main category: cs.CL

TL;DR: 文章提出Ref-Long基准，用于评估长上下文语言模型在长上下文引用任务中的性能，发现现有模型在该任务上的能力有限。


<details>
  <summary>Details</summary>
Motivation: 探索长上下文语言模型在长上下文引用任务中的表现，这是一个尚未深入研究的重要领域。

Method: 通过设计Ref-Long基准测试，要求模型识别与特定关键词相关的文档索引，基准包括从合成到现实场景的三个子集，结合实验和分析来评估模型性能。

Result: 对13个长上下文语言模型进行实验，发现包括GPT-4o在内的先进模型在长上下文引用任务上表现欠佳。

Conclusion: 目前长上下文语言模型在引用能力上仍存在显著不足，通过进一步分析提供了多个关键见解，同步开源数据和代码供研究社区参考。

Abstract: Long-context language models (LCLMs) have exhibited impressive capabilities
in long-context understanding tasks. Among these, long-context referencing -- a
crucial task that requires LCLMs to attribute items of interest to specific
parts of long-context data -- remains underexplored. To bridge this gap, this
paper proposes Referencing Evaluation for Long-context Language Models
(Ref-Long), a novel benchmark designed to assess the long-context referencing
capability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the
indexes of documents that reference a specific key, emphasizing contextual
relationships between the key and the documents over simple retrieval. Based on
the task design, we construct three subsets ranging from synthetic to realistic
scenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs
reveal significant shortcomings in long-context referencing, even among
advanced models like GPT-4o. To further investigate these challenges, we
conduct comprehensive analyses, including human evaluations, task format
adjustments, fine-tuning experiments, and error analyses, leading to several
key insights. Our data and code can be found in https://github.
com/wujunjie1998/Ref-Long.

</details>


### [192] [How Important is `Perfect' English for Machine Translation Prompts?](https://arxiv.org/abs/2507.09509)
*Patrícia Schmidtová,Niyati Bafna,Seth Aycock,Gianluca Vico,Wiktor Kamzela,Katharina Hämmerl,Vilém Zouhar*

Main category: cs.CL

TL;DR: 研究分析了人类可见和合成错误的用户提示对大语言模型在机器翻译及其评估任务中的影响。


<details>
  <summary>Details</summary>
Motivation: 探究用户提示中的错误如何影响大语言模型在机器翻译任务中的表现，尤其是在不同类型错误存在的情况下。

Method: 通过系统性评估和分析，探讨不同提示的错误类型（如字符级、短语级等）以及综合噪声对模型结果的影响。进行定量分析与定性观察。

Result: 发现提示质量对翻译性能有显著影响，带有大量错误的好提示可能不如无错误的差提示。字符级与综合噪声对性能影响最大，而短语级噪声影响较小。此外，低质量提示会降低任务指令的遵循度，但不直接影响翻译质量。

Conclusion: 即使在对人类来说几乎无法辨识的随机噪声提示条件下，大语言模型也能完成翻译任务。提示的噪声类型和提示设计对模型效果具有重要作用。

Abstract: Large language models (LLMs) have achieved top results in recent machine
translation evaluations, but they are also known to be sensitive to errors and
perturbations in their prompts. We systematically evaluate how both humanly
plausible and synthetic errors in user prompts affect LLMs' performance on two
related tasks: Machine translation and machine translation evaluation. We
provide both a quantitative analysis and qualitative insights into how the
models respond to increasing noise in the user prompt.
  The prompt quality strongly affects the translation performance: With many
errors, even a good prompt can underperform a minimal or poor prompt without
errors. However, different noise types impact translation quality differently,
with character-level and combined noisers degrading performance more than
phrasal perturbations. Qualitative analysis reveals that lower prompt quality
largely leads to poorer instruction following, rather than directly affecting
translation quality itself. Further, LLMs can still translate in scenarios with
overwhelming random noise that would make the prompt illegible to humans.

</details>


### [193] [Adapting Definition Modeling for New Languages: A Case Study on Belarusian](https://arxiv.org/abs/2507.09536)
*Daniela Kazakouskaya,Timothee Mickus,Janine Siewert*

Main category: cs.CL

TL;DR: 提出了一种定义建模系统，可为来自不同语言或方言提供释义功能，并专注于将此系统应用于白俄罗斯语，展示了在少量数据进行自动生成释义的可能性及自动度量的局限性。


<details>
  <summary>Details</summary>
Motivation: 希望通过定义建模技术帮助词典编辑者更高效地记录多种语言和方言，同时评估现有技术在未支持语言上的适用性。

Method: 提出了一个包含43,150个定义的白俄罗斯语数据集，并对现有建模系统进行了语言适应性实验。

Result: 实验表明，只需要极少量的数据即可让定义建模系统适应新的语言，但目前自动评价指标未能全面捕捉释义质量的要素。

Conclusion: 研究展示了定义建模的可行性及其支持多语言释义的潜力，并指出当前自动评价方法存在改进空间以更全面评估释义质量。

Abstract: Definition modeling, the task of generating new definitions for words in
context, holds great prospect as a means to assist the work of lexicographers
in documenting a broader variety of lects and languages, yet much remains to be
done in order to assess how we can leverage pre-existing models for as-of-yet
unsupported languages. In this work, we focus on adapting existing models to
Belarusian, for which we propose a novel dataset of 43,150 definitions. Our
experiments demonstrate that adapting a definition modeling systems requires
minimal amounts of data, but that there currently are gaps in what automatic
metrics do capture.

</details>


### [194] [NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance](https://arxiv.org/abs/2507.09601)
*Hanwool Lee,Sara Yu,Yewon Hwang,Jonghyun Choi,Heejae Ahn,Sungbum Jung,Youngjae Yu*

Main category: cs.CL

TL;DR: 提出NMIXX模型和KorFinSTS基准，用于解决金融特定语义在低资源语言（如韩语）中表达的难题，显著提高跨语言嵌入表达效果。


<details>
  <summary>Details</summary>
Motivation: 现有通用句子嵌入模型难以捕捉金融领域的特定语义，特别是在低资源语言中，例如韩语，主要面临专业术语、语义迁移和双语词汇不匹配等问题。

Method: 引入NMIXX嵌入模型，通过18.8K高置信度三元组进行微调，这些三元组包括同领域释义、通过语义迁移分类得到的强负例以及精准的韩英翻译。同时发布KorFinSTS基准，包括1921对韩语金融句对，覆盖新闻、披露、研究报告和法规等多种文本来源。

Result: NMIXX的多语言bge-m3变体在英文FinSTS上的Spearman rho提升了0.10，在KorFinSTS上的提高了0.22，超过其预训练版本和其他所有模型，但在通用STS性能上略有权衡。

Conclusion: NMIXX不仅展示了模型在金融跨语言领域的适应性，还强调了低资源语言中tokenizer设计的重要性。其开放的模型和基准为金融多语言表示学习提供了强有力的工具支持。

Abstract: General-purpose sentence embedding models often struggle to capture
specialized financial semantics, especially in low-resource languages like
Korean, due to domain-specific jargon, temporal meaning shifts, and misaligned
bilingual vocabularies. To address these gaps, we introduce NMIXX (Neural
eMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual
embedding models fine-tuned with 18.8K high-confidence triplets that pair
in-domain paraphrases, hard negatives derived from a semantic-shift typology,
and exact Korean-English translations. Concurrently, we release KorFinSTS, a
1,921-pair Korean financial STS benchmark spanning news, disclosures, research
reports, and regulations, designed to expose nuances that general benchmarks
miss.
  When evaluated against seven open-license baselines, NMIXX's multilingual
bge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and
+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing
other models by the largest margin, while revealing a modest trade-off in
general STS performance. Our analysis further shows that models with richer
Korean token coverage adapt more effectively, underscoring the importance of
tokenizer design in low-resource, cross-lingual settings. By making both models
and the benchmark publicly available, we provide the community with robust
tools for domain-adapted, multilingual representation learning in finance.

</details>


### [195] [SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks](https://arxiv.org/abs/2507.09628)
*Salvatore Citraro,Edith Haim,Alessandra Carini,Cynthia S. Q. Siew,Giulio Rossetti,Massimo Stella*

Main category: cs.CL

TL;DR: SpreadPy是一个用于认知单层和多层网络中扩散激活模拟的Python库，支持心理学、神经科学和教育研究中的可重复研究。


<details>
  <summary>Details</summary>
Motivation: 探讨扩散激活如何反映认知、心理和临床现象，并验证结构与功能的关系。

Method: 设计了一个Python库SpreadPy，通过数值模拟在知识建模的理论背景下进行扩散激活模拟，结合三种案例研究展示其应用。

Result: 1. 发现高、低数学焦虑学生之间在概念组织结构上的差异；2. 揭示认知负荷如何通过激活路径调节词汇访问；3. 在失语症患者中，模拟的激活模式与经验性错误类型相关联。

Conclusion: SpreadPy为研究个体差异与认知障碍提供了一个灵活的框架，通过理论或经验网络建模，实现对心理与认知问题的机制性洞察并支持可重复研究。

Abstract: We introduce SpreadPy as a Python library for simulating spreading activation
in cognitive single-layer and multiplex networks. Our tool is designed to
perform numerical simulations testing structure-function relationships in
cognitive processes. By comparing simulation results with grounded theories in
knowledge modelling, SpreadPy enables systematic investigations of how
activation dynamics reflect cognitive, psychological and clinical phenomena. We
demonstrate the library's utility through three case studies: (1) Spreading
activation on associative knowledge networks distinguishes students with high
versus low math anxiety, revealing anxiety-related structural differences in
conceptual organization; (2) Simulations of a creativity task show that
activation trajectories vary with task difficulty, exposing how cognitive load
modulates lexical access; (3) In individuals with aphasia, simulated activation
patterns on lexical networks correlate with empirical error types (semantic vs.
phonological) during picture-naming tasks, linking network structure to
clinical impairments. SpreadPy's flexible framework allows researchers to model
these processes using empirically derived or theoretical networks, providing
mechanistic insights into individual differences and cognitive impairments. The
library is openly available, supporting reproducible research in psychology,
neuroscience, and education research.

</details>


### [196] [An Exploration of Knowledge Editing for Arabic](https://arxiv.org/abs/2507.09629)
*Basel Mousi,Nadir Durrani,Fahim Dalvi*

Main category: cs.CL

TL;DR: 本文研究了阿拉伯语知识编辑，并评估了四种方法在阿拉伯语数据集上的性能，同时发布了相应的基准和多语言训练数据。


<details>
  <summary>Details</summary>
Motivation: 研究阿拉伯语知识编辑领域的特殊性，并探讨现有方法在多语言与跨语言环境下的表现。

Method: 实验比较了四种知识编辑方法（ROME、MEMIT、ICE、LTE），并扩展了LTE方法使其适用于多语言的阿拉伯语-英语联合训练。使用Llama-2-7B-chat模型进行实验分析。

Result: 参数化方法在跨语言情境下表现较差，而指令调优方法表现较稳定。多语言联合训练提升了LTE方法的可编辑性和知识迁移能力。

Conclusion: 跨语言阿拉伯语知识编辑更适合使用指令调优方法，多语言联合训练在改进相关任务表现方面具有潜力。

Abstract: While Knowledge Editing (KE) has been widely explored in English, its
behavior in morphologically rich languages like Arabic remains underexamined.
In this work, we present the first study of Arabic KE. We evaluate four methods
(ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact
benchmarks, analyzing both multilingual and cross-lingual settings. Our
experiments on Llama-2-7B-chat show show that parameter-based methods struggle
with cross-lingual generalization, while instruction-tuned methods perform more
robustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show
that joint Arabic-English training improves both editability and transfer. We
release Arabic KE benchmarks and multilingual training for LTE data to support
future research.

</details>


### [197] [Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?](https://arxiv.org/abs/2507.09638)
*Pawitsapak Akarajaradwong,Chompakorn Chaksangchaichot,Pirat Pothavorn,Attapol Thamrongrattanarit-Rutherford,Ekapol Chuangsuwanich,Sarana Nutanong*

Main category: cs.CL

TL;DR: 本文提出一种基于GRPO的优化方法，提升泰国语言法律模型的引文准确性和应答质量，尤其在复杂推理任务上表现卓越。


<details>
  <summary>Details</summary>
Motivation: 目前泰国语言法律问答系统在需要复杂法律推理的问题上表现受限，亟需提高引文精确性和总体应答质量。

Method: 通过一种名为Group-Relative Policy Optimization（GRPO）的方法，结合BGE-M3嵌入作为语义相似性奖励，显著减少了计算费用并提升了模型表现。

Result: 在NitiBench基准上，GRPO方法比基础模型法实现了90%的引文F1提升，并在联合质量指标上提高了31%。在复杂法律推理任务上表现更稳健。

Conclusion: GRPO方法为提升泰国语言法律模型提供了一种有效且资源高效的解决方案。

Abstract: The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal
question answering is still limited, especially for questions requiring
extensive, complex legal reasoning. To address these limitations, we introduce
an approach aligning LLMs toward improved law citation accuracy and better
response quality using Group-Relative Policy Optimization (GRPO). Our approach
leverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,
significantly reducing computational expenses up to 2.5x compared to large
language model judges. Experiments on the NitiBench benchmark demonstrate
substantial improvements: GRPO achieves up to 90% citation-F1 gains from the
base model and a 31% increase in joint quality metrics over instruction tuning.
Crucially, our method shows enhanced robustness on complex legal reasoning
tasks compared to instruction tuning, providing an effective and
resource-efficient solution for enhancing Thai legal LLMs.

</details>


### [198] [MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs](https://arxiv.org/abs/2507.09701)
*Shulin Huang,Linyi Yang,Yue Zhang*

Main category: cs.CL

TL;DR: 该研究提出了一种名为MCEval的多语言评估框架，用于研究大型语言模型的文化意识和文化偏见问题。


<details>
  <summary>Details</summary>
Motivation: 目前的大型语言模型在处理跨文化理解时存在局限，尤其是面对多元文化背景的全球用户时表现出文化偏见，需要一个全面的评估框架来检测这些问题。

Method: MCEval框架通过动态文化问题构建，并结合反事实改写和混淆因子改写实现因果分析，评估了13种文化和13种语言场景下的文化意识与文化偏见。

Result: 实验涵盖39,897个文化意识实例和17,940个文化偏见实例。结果显示不同语言场景下表现存在差异，最佳文化表现不仅与训练数据分布相关，还与语言和文化的匹配性有关。此外，评估揭示了公平性问题，在英文场景中看似有效的方法对其他场景会带来显著劣势。

Conclusion: MCEval是首个全面的多语言文化评估框架，为大型语言模型的文化理解提供了更深入的分析和见解。

Abstract: Large language models exhibit cultural biases and limited cross-cultural
understanding capabilities, particularly when serving diverse global user
populations. We propose MCEval, a novel multilingual evaluation framework that
employs dynamic cultural question construction and enables causal analysis
through Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive
evaluation spans 13 cultures and 13 languages, systematically assessing both
cultural awareness and cultural bias across different linguistic scenarios. The
framework provides 39,897 cultural awareness instances and 17,940 cultural bias
instances. Experimental results reveal performance disparities across different
linguistic scenarios, demonstrating that optimal cultural performance is not
only linked to training data distribution, but also is related to
language-culture alignment. The evaluation results also expose the fairness
issue, where approaches appearing successful in the English scenario create
substantial disadvantages. MCEval represents the first comprehensive
multilingual cultural evaluation framework that provides deeper insights into
LLMs' cultural understanding.

</details>


### [199] [Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces](https://arxiv.org/abs/2507.09709)
*Baturay Saglam,Paul Kassianik,Blaine Nelson,Sajana Weerawardhena,Yaron Singer,Amin Karbasi*

Main category: cs.CL

TL;DR: 研究揭示大型语言模型(LLMs)潜在空间的几何特性，表示高层语义信息在低维子空间中具有线性可分性，并利用这种几何特性改进模型的对齐和安全性方法。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型内部是否及如何组织与语义理解相关的表示，从而改进模型行为的解释和对齐能力。

Method: 分析11个基于transformer架构的解码器模型的隐藏状态，覆盖6个科学主题与12个层，研究其语义信息的几何特性，并验证基于潜在空间干预的因果性。

Result: 发现高阶语义信息位于低维线性可分的子空间，其分离程度随层深增加而增强；基于该几何特性设计的轻量级守护系统可高效检测恶意输入。

Conclusion: LLM潜在空间中的几何特性提供了新的方法途径，可直接基于潜在表示改进模型对抗恶意内容的能力，表明发展几何感知工具的潜力。

Abstract: Understanding the latent space geometry of large language models (LLMs) is
key to interpreting their behavior and improving alignment. \baturay{However,
it remains unclear to what extent LLMs internally organize representations
related to semantic understanding. To investigate this, we conduct a
large-scale empirical study of hidden states in transformer-based LLMs,
analyzing 11 decoder-only models across 6 scientific topics and 12 layers each.
We find that high-level semantic information consistently lies in
low-dimensional subspaces that form linearly separable representations across
distinct domains. This separability becomes more pronounced in deeper layers
and under prompts that trigger structured reasoning or alignment
behaviors$\unicode{x2013}$even when surface content is unchanged. This geometry
enables simple yet effective causal interventions in hidden space; for example,
reasoning patterns like chain-of-thought can be captured by a single vector
direction. Together, these findings support the development of geometry-aware
tools that operate directly on latent representations to detect and mitigate
harmful or adversarial content, using methods such as transport-based defenses
that leverage this separability. As a proof of concept, we demonstrate this
potential by training a simple MLP classifier as a lightweight latent-space
guardrail, which detects adversarial and malicious prompts with high precision.

</details>


### [200] [Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding](https://arxiv.org/abs/2507.09758)
*Qi Feng,Yihong Liu,Hinrich Schütze*

Main category: cs.CL

TL;DR: 本文提出了一种自适应课程学习方法，通过利用预训练语言模型（PLMs）自身预测的难度分数来动态调整微调样本的排序，从而提升自然语言处理任务的训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统课程学习方法依赖手动定义的难度指标，这些指标可能无法准确反映模型自身对难度的理解，因此有必要探索一种基于模型自适应的课程学习方法。

Method: 通过使用预训练语言模型预测数据样本的难度分数，设计了不同的训练策略，包括从易到难、从难到易、以及混合采样，动态调整微调样本的顺序以优化训练过程。

Result: 在四个自然语言理解数据集上的实验结果显示，该方法能够加速模型收敛，同时提升了分类任务的性能，无论是二分类还是多分类任务。

Conclusion: 基于预训练语言模型自适应难度分析的课程学习策略相较于随机采样提供了显著的性能提升，是一种有效的提高训练效率和模型表现的途径。

Abstract: Curriculum learning is a widely adopted training strategy in natural language
processing (NLP), where models are exposed to examples organized by increasing
difficulty to enhance learning efficiency and performance. However, most
existing approaches rely on manually defined difficulty metrics -- such as text
length -- which may not accurately reflect the model's own perspective. To
overcome this limitation, we present a self-adaptive curriculum learning
paradigm that prioritizes fine-tuning examples based on difficulty scores
predicted by pre-trained language models (PLMs) themselves. Building on these
scores, we explore various training strategies that differ in the ordering of
examples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed
sampling. We evaluate our method on four natural language understanding (NLU)
datasets covering both binary and multi-class classification tasks.
Experimental results show that our approach leads to faster convergence and
improved performance compared to standard random sampling.

</details>


### [201] [Te Ahorré Un Click: A Revised Definition of Clickbait and Detection in Spanish News](https://arxiv.org/abs/2507.09777)
*Gabriel Mordecki,Guillermo Moncecchi,Javier Couto*

Main category: cs.CL

TL;DR: 本文重新定义了点击诱饵，从而区分其与哗众取宠、标题与内容不符等现象，提出新的检测数据集并针对西班牙语提供了首个公开点击诱饵数据集。


<details>
  <summary>Details</summary>
Motivation: 研究者发现目前对点击诱饵的定义缺乏统一，同时现有的检测方法和数据集存在较大的主观性，导致研究难以深入。

Method: 作者重新定义点击诱饵为通过故意省略部分信息激发好奇心以吸引点击行为的技术，并提出了新的数据集创建方法，定义了更严格的标注标准。此外，创建了西班牙语点击诱饵数据集TA1C，并进行人工标注。

Result: TA1C数据集包含3500条来自18个知名媒体的推文，人工标注且达到了0.825的Fleiss' K一致性分数。测试中基线模型的F1分数达到0.84。

Conclusion: 本文通过重新定义点击诱饵，为该领域提供了更加清晰的概念框架，并通过新的方法生成了首个西班牙语点击诱饵检测数据集，为研究提供了重要资源基础。

Abstract: We revise the definition of clickbait, which lacks current consensus, and
argue that the creation of a curiosity gap is the key concept that
distinguishes clickbait from other related phenomena such as sensationalism and
headlines that do not deliver what they promise or diverge from the article.
Therefore, we propose a new definition: clickbait is a technique for generating
headlines and teasers that deliberately omit part of the information with the
goal of raising the readers' curiosity, capturing their attention and enticing
them to click. We introduce a new approach to clickbait detection datasets
creation, by refining the concept limits and annotations criteria, minimizing
the subjectivity in the decision as much as possible. Following it, we created
and release TA1C (for Te Ahorr\'e Un Click, Spanish for Saved You A Click), the
first open source dataset for clickbait detection in Spanish. It consists of
3,500 tweets coming from 18 well known media sources, manually annotated and
reaching a 0.825 Fleiss' K inter annotator agreement. We implement strong
baselines that achieve 0.84 in F1-score.

</details>


### [202] [Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition](https://arxiv.org/abs/2507.09875)
*Qinyuan Ye,Robin Jia,Xiang Ren*

Main category: cs.CL

TL;DR: 这篇论文探讨了大型语言模型如何通过上下文学习完成未见过的任务，尤其是通过分析一个两步的反事实任务(如错位加法)，揭示了模型的任务级别泛化机制。


<details>
  <summary>Details</summary>
Motivation: 当前对于大型语言模型如何进行任务级别泛化尚不清楚，研究这一机制有助于更好地理解和改进模型能力。

Method: 使用电路式可解释性技术(例如路径修补)，分析模型在特定任务中的内部计算过程。

Result: 研究发现模型通过函数归纳机制从标准加法推广到错位加法（+1功能）；这一机制由多个注意力头共同完成，并且在更广泛的任务中被复用。

Conclusion: 研究揭示了语言模型内部可复用、可组合结构如何支持任务级别的泛化，提供了更深层次的见解。

Abstract: Large language models demonstrate the intriguing ability to perform unseen
tasks via in-context learning. However, it remains unclear what mechanisms
inside the model drive such task-level generalization. In this work, we
approach this question through the lens of off-by-one addition (i.e., 1+1=3,
2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function
as a second step. Leveraging circuit-style interpretability techniques such as
path patching, we analyze the models' internal computations behind their
notable performance and present three key findings. First, we uncover a
function induction mechanism that explains the model's generalization from
standard addition to off-by-one addition. This mechanism resembles the
structure of the induction head mechanism found in prior work and elevates it
to a higher level of abstraction. Second, we show that the induction of the +1
function is governed by multiple attention heads in parallel, each of which
emits a distinct piece of the +1 function. Finally, we find that this function
induction mechanism is reused in a broader range of tasks, including synthetic
tasks such as shifted multiple-choice QA and algorithmic tasks such as base-8
addition. Overall, our findings offer deeper insights into how reusable and
composable structures within language models enable task-level generalization.

</details>


### [203] [Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking](https://arxiv.org/abs/2507.09935)
*Hai Toan Nguyen,Tien Dat Nguyen,Viet Ha Nguyen*

Main category: cs.CL

TL;DR: 本研究提出了一种增强型RAG系统，利用分层文本分割与聚类生成更有意义的语义块，在多数据集上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统的块划分方法未充分考虑文本的语义结构，导致语义捕捉不足的问题。

Method: 通过分层文本分割和聚类生成语义一致的块，并在推理时结合段级和聚类级的向量表示进行信息检索。

Result: 在NarrativeQA、QuALITY和QASPER等数据集上，该方法效果优于传统的分块技术。

Conclusion: 引入分层文本结构可显著提高RAG系统的精确性和语义相关性，有助于解决传统块划分的局限性。

Abstract: Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies
for retrieval, which enhance large language models (LLMs) by enabling them to
access external knowledge, ensuring that the retrieved information is
up-to-date and domain-specific. However, traditional methods often fail to
create chunks that capture sufficient semantic meaning, as they do not account
for the underlying textual structure. This paper proposes a novel framework
that enhances RAG by integrating hierarchical text segmentation and clustering
to generate more meaningful and semantically coherent chunks. During inference,
the framework retrieves information by leveraging both segment-level and
cluster-level vector representations, thereby increasing the likelihood of
retrieving more precise and contextually relevant information. Evaluations on
the NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method
achieved improved results compared to traditional chunking techniques.

</details>


### [204] [Tiny Reward Models](https://arxiv.org/abs/2507.09973)
*Sarah Pan*

Main category: cs.CL

TL;DR: 该论文提出了一种名为TinyRM的小型双向遮罩语言模型，尽管参数数量远少于当前主流模型，但在推理和安全偏好建模任务上有竞争力表现。


<details>
  <summary>Details</summary>
Motivation: 旨在通过更高效的小模型解决大型语言模型推理成本高的问题，同时保持性能优势。

Method: 结合FLAN样式提示、定向低秩适配（DoRA）和层冻结技术，实现小模型高效性能。同时通过RewardBench评估和领域特定微调策略再优化表现。

Result: 实验显示，TinyRM在推理和偏好建模任务中表现优异，尤其是领域特定微调对提升推理能力尤为有效。

Conclusion: 小型双向架构在偏好建模中具有高效可伸缩性，尽管在广义模型和对话式偏好建模上仍面临挑战，但这一路线具有发展潜力。

Abstract: Large decoder-based language models have become the dominant architecture for
reward modeling in reinforcement learning from human feedback (RLHF). However,
as reward models are increasingly deployed in test-time strategies, their
inference costs become a growing concern. We present TinyRM, a family of small,
bidirectional masked language models (MLMs) with as few as 400 million
parameters, that rival the capabilities of models over 175 times larger on
reasoning and safety preference modeling tasks. TinyRM combines FLAN-style
prompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to
achieve strong performance on RewardBench, despite using significantly fewer
resources. Our experiments suggest that small models benefit from
domain-specific tuning strategies, particularly in reasoning, where lightweight
finetuning methods are especially effective. While challenges remain in
building generalist models and conversational preference modeling, our
preliminary results highlight the promise of lightweight bidirectional
architectures as efficient, scalable alternatives for preference modeling.

</details>


### [205] [TextOmics-Guided Diffusion for Hit-like Molecular Generation](https://arxiv.org/abs/2507.09982)
*Hang Yuan,Chen Li,Wenjun Ma,Yuncheng Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种名为TextOmics的新基准和ToDi生成框架，用于基于生物组学表达和分子描述生成具有治疗潜力的分子。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏能整合多样分子表示的异质数据和统一框架，该研究旨在解决这一问题。

Method: 开发了TextOmics基准以匹配生物组学表达和分子文本描述，并提出ToDi生成框架，使用双编码器和条件扩散方法实现可控分子生成。

Result: 实验表明，所提出的方法有效性强，生成模型在多个方面超越现有技术且具有零样本分子生成潜力。

Conclusion: ToDi结合TextOmics框架，可实现精准、语义相关的化学分子生成，推动靶向药物发现领域发展。

Abstract: Hit-like molecular generation with therapeutic potential is essential for
target-specific drug discovery. However, the field lacks heterogeneous data and
unified frameworks for integrating diverse molecular representations. To bridge
this gap, we introduce TextOmics, a pioneering benchmark that establishes
one-to-one correspondences between omics expressions and molecular textual
descriptions. TextOmics provides a heterogeneous dataset that facilitates
molecular generation through representations alignment. Built upon this
foundation, we propose ToDi, a generative framework that jointly conditions on
omics expressions and molecular textual descriptions to produce biologically
relevant, chemically valid, hit-like molecules. ToDi leverages two encoders
(OmicsEn and TextEn) to capture multi-level biological and semantic
associations, and develops conditional diffusion (DiffGen) for controllable
generation. Extensive experiments confirm the effectiveness of TextOmics and
demonstrate ToDi outperforms existing state-of-the-art approaches, while also
showcasing remarkable potential in zero-shot therapeutic molecular generation.
Sources are available at: https://github.com/hala-ToDi.

</details>


### [206] [Protective Factor-Aware Dynamic Influence Learning for Suicide Risk Prediction on Social Media](https://arxiv.org/abs/2507.10008)
*Jun Li,Xiangmeng Wang,Haoyang Li,Yifei Yan,Hong Va Leong,Ling Feng,Nancy Xiaonan Yu,Qing Li*

Main category: cs.CL

TL;DR: 本文提出一种新的框架，通过联合学习影响用户自杀风险的风险因素和保护性因素动态变化，来预测用户的后续自杀风险。


<details>
  <summary>Details</summary>
Motivation: 当前关于社交媒体上的自杀风险研究主要集中在静态检测，忽视了随时间快速变化的个体心理状态以及保护性因素在风险预测中的作用。

Method: 构建一个新的保护性因素感知数据集，涵盖12年Reddit数据，并提出动态因素影响学习方法，捕捉风险和保护因素对于自杀风险转变的动态影响。

Result: 实验表明，提出的模型在三个数据集上的表现显著优于最新的模型和大语言模型。

Conclusion: 本研究提供了一个具有解释性权重的预测模型，有助于辅助临床医生更好地理解自杀模式并制定针对性干预措施。

Abstract: Suicide is a critical global health issue that requires urgent attention.
Even though prior work has revealed valuable insights into detecting current
suicide risk on social media, little attention has been paid to developing
models that can predict subsequent suicide risk over time, limiting their
ability to capture rapid fluctuations in individuals' mental state transitions.
In addition, existing work ignores protective factors that play a crucial role
in suicide risk prediction, focusing predominantly on risk factors alone.
Protective factors such as social support and coping strategies can mitigate
suicide risk by moderating the impact of risk factors. Therefore, this study
proposes a novel framework for predicting subsequent suicide risk by jointly
learning the dynamic influence of both risk factors and protective factors on
users' suicide risk transitions. We propose a novel Protective Factor-Aware
Dataset, which is built from 12 years of Reddit posts along with comprehensive
annotations of suicide risk and both risk and protective factors. We also
introduce a Dynamic Factors Influence Learning approach that captures the
varying impact of risk and protective factors on suicide risk transitions,
recognizing that suicide risk fluctuates over time according to established
psychological theories. Our thorough experiments demonstrate that the proposed
model significantly outperforms state-of-the-art models and large language
models across three datasets. In addition, the proposed Dynamic Factors
Influence Learning provides interpretable weights, helping clinicians better
understand suicidal patterns and enabling more targeted intervention
strategies.

</details>


### [207] [GeLaCo: An Evolutionary Approach to Layer Compression](https://arxiv.org/abs/2507.10059)
*David Ponce,Thierry Etchegoyhen,Javier Del Ser*

Main category: cs.CL

TL;DR: 本文提出了一个名为GeLaCo的进化式压缩方法，用于通过层折叠实现对大型语言模型（LLM）的高效压缩，并在多个评估基准下超越了最先进的方法。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型的计算需求非常高，限制了其实际的部署和使用，本文旨在通过高效的模型压缩方法解决这一问题。

Method: 引入了一种名为GeLaCo的进化算法，通过种群搜索和模块级的相似性适应度函数实现了对LLM的层折叠压缩。支持单目标和多目标的进化压缩搜索，首次建立了以压缩率和质量为轴的帕累托前沿。

Result: 实验表明，GeLaCo方法在基础模型和指令调优模型的困惑度及生成评估任务中均表现优异，超过了当前的最优方法。

Conclusion: GeLaCo方法为LLM压缩提供了一种高效且有效的解决方案，解决了现有方法的高成本和潜在次优问题，在多个质量评估维度上取得了先进的性能。

Abstract: Large Language Models (LLM) have achieved remarkable performance across a
large number of tasks, but face critical deployment and usage barriers due to
substantial computational requirements. Model compression methods, which aim to
reduce model size while preserving its capacity, are an important means to
mitigate these issues. Promising approaches along these lines, such as
structured pruning, typically require costly empirical search for optimal
variants and may run the risk of ignoring better solutions. In this work we
introduce GeLaCo, an evolutionary approach to LLM compression via layer
collapse. Our approach supports an efficient exploration of the compression
solution space via population-based search and a module-wise similarity fitness
function capturing attention, feed-forward, and hidden state representations.
GeLaCo also supports both single and multi-objective evolutionary compression
search, establishing the first Pareto frontier along compression and quality
axes. We evaluate GeLaCo solutions via both perplexity-based and generative
evaluations over foundational and instruction-tuned models, outperforming
state-of-the-art alternatives.

</details>


### [208] [Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires](https://arxiv.org/abs/2507.10073)
*Simon Münker*

Main category: cs.CL

TL;DR: 研究表明，大型语言模型（LLMs）无法有效地表现多样化的文化道德价值，反而会系统性地同化道德差异。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否能够通过其语言能力真实地体现人类多样化的文化道德价值。

Method: 采用《道德基础问卷》，分析19种文化背景下AI生成的和人类的道德直觉之间的差异，并对多个先进LLM与人类基准数据进行比较。

Result: LLMs无法代表多样化的道德框架，模型规模的增加也未稳步提升其文化代表性，并存在系统性地同化道德多样性的现象。

Conclusion: 现有的AI对齐方法存在根本性限制，需开发更加扎实的对齐目标和评估方法，确保AI反映多样化的人类价值观，而非简化道德格局。

Abstract: Are AI systems truly representing human values, or merely averaging across
them? Our study suggests a concerning reality: Large Language Models (LLMs)
fail to represent diverse cultural moral frameworks despite their linguistic
capabilities. We expose significant gaps between AI-generated and human moral
intuitions by applying the Moral Foundations Questionnaire across 19 cultural
contexts. Comparing multiple state-of-the-art LLMs' origins against human
baseline data, we find these models systematically homogenize moral diversity.
Surprisingly, increased model size doesn't consistently improve cultural
representation fidelity. Our findings challenge the growing use of LLMs as
synthetic populations in social science research and highlight a fundamental
limitation in current AI alignment approaches. Without data-driven alignment
beyond prompting, these systems cannot capture the nuanced, culturally-specific
moral intuitions. Our results call for more grounded alignment objectives and
evaluation metrics to ensure AI systems represent diverse human values rather
than flattening the moral landscape.

</details>


### [209] [Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning](https://arxiv.org/abs/2507.10085)
*Chenxi Huang,Shaotian Yan,Liang Xie,Binbin Lin,Sinan Fan,Yue Xin,Deng Cai,Chen Shen,Jieping Ye*

Main category: cs.CL

TL;DR: 本文提出了Critical Representation Fine-Tuning (CRFT)，增强了Representation Fine-tuning (ReFT)方法在复杂推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 发现ReFT中固定位置的表示在复杂推理任务上表现欠佳，需要针对关键表示进行调优以提升性能。

Method: 设计CRFT方法，通过信息流分析识别关键表示，并在监督学习框架下动态优化这些表示，同时冻住基础模型。

Result: 在八个数字计算和常识推理基准测试中验证了CRFT方法的高效性及效果，包括LLaMA和Mistral模型，并在单次测试场景中提升了一次性精确度16.4%。

Conclusion: CRFT展现了在复杂推理任务中表示层优化的潜力，是一种轻量级但有力的PEFT替代方法。

Abstract: Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient
Fine-Tuning (PEFT) method, has attracted widespread attention for significantly
improving parameter efficiency by editing representation space alone. In this
work, we investigate applying ReFT to complex reasoning tasks. However,
directly using the native ReFT method, which modifies fixed representations at
the beginning and end of each layer, yields suboptimal performance, as these
fixed-position representations have uncertain impact on the outputs. We observe
that, in complex reasoning tasks, there often exist certain critical
representations. These representations either integrate significant information
from preceding layers or regulate subsequent layer representations. Through
layer-by-layer propagation, they exert a substantial influence on the final
output. Naturally, fine-tuning these critical representations has the potential
to greatly enhance reasoning performance. Building upon these insights, we
propose Critical Representation Fine-Tuning (CRFT), a novel method that
identifies and optimizes these critical representations through information
flow analysis. CRFT operates within a supervised learning framework,
dynamically optimizing critical representations in a low-rank linear subspace
while freezing the base model. The effectiveness and efficiency of our method
are validated across eight benchmarks for arithmetic and commonsense reasoning,
using LLaMA and Mistral model families. Furthermore, our method also adapts
effectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work
highlights the untapped potential of representation-level optimization for CoT
reasoning, offering a lightweight yet powerful alternative to traditional PEFT
methods.

</details>


### [210] [Fusing Large Language Models with Temporal Transformers for Time Series Forecasting](https://arxiv.org/abs/2507.10098)
*Chen Su,Yuanhe Tian,Qinyu Liu,Jun Zhang,Yan Song*

Main category: cs.CL

TL;DR: 本文提出了一种新的模型架构，结合了大语言模型(LLM)和传统的时间序列Transformer，以应对LLM在时间序列预测任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然在处理离散token和语义模式方面表现优异，但在连续数值时间序列数据建模时表现较差，因此需要新的方法来结合LLM的高层语义能力与Transformer的时间序列建模能力。

Method: 设计了一种新型的Transformer架构，将LLM学习到的高层语义表示和传统Transformer编码的时间序列信息融合，形成混合表示，用于更准确的时间序列预测。

Result: 实验结果表明，该方法在多个基准数据集上提升了时间序列预测的精度。

Conclusion: 通过融合LLM的语义变动模式表示和时间序列Transformer的时序动态信息，可以更好地预测未来值，这证明了该方法的有效性。

Abstract: Recently, large language models (LLMs) have demonstrated powerful
capabilities in performing various tasks and thus are applied by recent studies
to time series forecasting (TSF) tasks, which predict future values with the
given historical time series. Existing LLM-based approaches transfer knowledge
learned from text data to time series prediction using prompting or fine-tuning
strategies. However, LLMs are proficient at reasoning over discrete tokens and
semantic patterns but are not initially designed to model continuous numerical
time series data. The gaps between text and time series data lead LLMs to
achieve inferior performance to a vanilla Transformer model that is directly
trained on TSF data. However, the vanilla Transformers often struggle to learn
high-level semantic patterns. In this paper, we design a novel
Transformer-based architecture that complementarily leverages LLMs and vanilla
Transformers, so as to integrate the high-level semantic representations
learned by LLMs into the temporal information encoded by time series
Transformers, where a hybrid representation is obtained by fusing the
representations from the LLM and the Transformer. The resulting fused
representation contains both historical temporal dynamics and semantic
variation patterns, allowing our model to predict more accurate future values.
Experiments on benchmark datasets demonstrate the effectiveness of the proposed
approach.

</details>


### [211] [Task-Based Flexible Feature Distillation for LLMs](https://arxiv.org/abs/2507.10155)
*Khouloud Saadi,Di Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的基于任务的特征蒸馏方法，通过直接蒸馏教师模型中与任务相关的隐藏单元到学生模型中，有效传递知识且无需引入新参数。实验表明，该方法在多种任务上比传统方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的特征知识蒸馏方法限制于教师与学生模型必须共享相同的隐藏层大小，导致学生模型灵活性受限；且现有解决方案的线性投影通常需额外学习参数，影响生成任务的性能。

Method: 提出无参数的新特征蒸馏方法，通过识别教师模型中与任务最相关的隐藏单元直接蒸馏到学生模型，避免了学习新参数。

Result: 在分类、指令跟随、摘要等多任务上，比线性投影基线提升了最多3%的性能。

Conclusion: 通过任务相关的特征选择，该方法实现了跨隐藏维度的高效知识迁移且性能显著提升，灵活性强，可集成到其他蒸馏框架中。

Abstract: Knowledge Distillation (KD) in general and feature distillation in particular
are promising techniques for reducing the high computational demand of large
language models (LLMs). However, traditional feature KD methods typically
assume that the teacher and the student share the same hidden size, limiting
the flexibility of the student's architecture. A common solution to this
problem involves training a linear projector to align their feature spaces, but
this introduces additional parameters that must be learned from scratch and
often degrades performance on downstream tasks, especially in generative
settings. To address this issue, in this work, we propose a novel task-based
feature distillation method that enables knowledge transfer between teacher and
student models with different hidden layer dimensions, without introducing any
new parameters. Leveraging the insight that only a subset of LLM components
contribute significantly to a specific downstream task, our approach identifies
the most task-relevant hidden units in the teacher and directly distills their
activations to the student. Our method is flexible and easily integrates with
other distillation frameworks. Empirical results show consistent improvements
over prior approaches across diverse tasks, including classification,
instruction-following, and summarization, achieving up to a 3\% performance
gain over the linear projection baseline.

</details>


### [212] [Abusive text transformation using LLMs](https://arxiv.org/abs/2507.10177)
*Rohitash Chandra,Jiyong Choi*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型(LLMs)在将含有仇恨言论和脏话的侮辱性文本转化为非侮辱性文本方面的效果，并对多种模型的表现进行了评估。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用LLMs将包含仇恨言论和不恰当语言的侮辱性文本有效转化为非侮辱性版本，并保留文本原本意图。

Method: 使用Gemini, GPT-4o, DeekSeek和Groq等四种LLMs对侮辱性文本进行分类和非侮辱性转化；随后使用情感分析和语义分析工具对原始和转化后的数据集进行评估。

Result: 结果显示，Groq的表现与其他LLMs有显著不同，同时在GPT-4o和DeepSeek-V3之间发现了某些相似性。

Conclusion: 该研究表明，不同LLMs在文本转化及意图保留方面性能差异明显，为未来该领域的研究提供了重要的比较依据。

Abstract: Although Large Language Models (LLMs) have demonstrated significant
advancements in natural language processing tasks, their effectiveness in the
classification and transformation of abusive text into non-abusive versions
remains an area for exploration. In this study, we aim to use LLMs to transform
abusive text (tweets and reviews) featuring hate speech and swear words into
non-abusive text, while retaining the intent of the text. We evaluate the
performance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and
Groq, on their ability to identify abusive text. We them to transform and
obtain a text that is clean from abusive and inappropriate content but
maintains a similar level of sentiment and semantics, i.e. the transformed text
needs to maintain its message. Afterwards, we evaluate the raw and transformed
datasets with sentiment analysis and semantic analysis. Our results show Groq
provides vastly different results when compared with other LLMs. We have
identified similarities between GPT-4o and DeepSeek-V3.

</details>


### [213] [Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects](https://arxiv.org/abs/2507.10216)
*Renad Al-Monef,Hassan Alhuzali,Nora Alturayeif,Ashwag Alasmari*

Main category: cs.CL

TL;DR: 提出了一个名为\texttt{Absher}的基准，用于评估大型语言模型（LLM）在沙特阿拉伯主要方言中的表现。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在多样化语言环境中对地区方言和文化细微差异的理解能力，特别是在像沙特阿拉伯这样语言多样化的区域，具有重要意义。

Method: 设计并构建了\texttt{Absher}基准，包含超过18,000个多项选择题，涉及六个不同类别（如语义、填空、文化解释等），并通过多种尖端LLM进行性能评估。

Result: 实验表明LLM在需要文化推理或语境理解的任务上表现存在显著差距。

Conclusion: 当前的LLM需要更注重方言感知的训练和文化对齐的评测方法，以改善其在实际阿拉伯语应用中的表现。

Abstract: As large language models (LLMs) become increasingly central to Arabic NLP
applications, evaluating their understanding of regional dialects and cultural
nuances is essential, particularly in linguistically diverse settings like
Saudi Arabia. This paper introduces \texttt{Absher}, a comprehensive benchmark
specifically designed to assess LLMs performance across major Saudi dialects.
\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six
distinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,
Cultural Interpretation, and Location Recognition. These questions are derived
from a curated dataset of dialectal words, phrases, and proverbs sourced from
various regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,
including multilingual and Arabic-specific models. We also provide detailed
insights into their capabilities and limitations. Our results reveal notable
performance gaps, particularly in tasks requiring cultural inference or
contextual understanding. Our findings highlight the urgent need for
dialect-aware training and culturally aligned evaluation methodologies to
improve LLMs performance in real-world Arabic applications.

</details>


### [214] [Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation](https://arxiv.org/abs/2507.10326)
*Muzhaffar Hazman,Minh-Khoi Pham,Shweta Soundararajan,Goncalo Mordido,Leonardo Custode,David Lynch,Giorgio Cruciata,Yucheng Shi,Hongmeng Song,Wang Chao,Pan Yue,Aleksandar Milenovic,Alexandros Agapitos*

Main category: cs.CL

TL;DR: 论文提出了一种基于进化搜索的离散式提示优化方法，分为两阶段：语法引导的遗传编程和局部搜索，在小规模通用LLM和四个专业领域任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前提示工程的方法主要着眼于简单任务和大型LLM，而复杂任务和小型模型在提示设计上更加敏感，需要优化更多信息文本。

Method: 提出使用进化搜索的提示优化方法：第一阶段利用语法引导的遗传编程搜索编辑提示程序，第二阶段通过局部搜索进一步优化表现最佳的程序。

Result: 方法在三种小型通用LLM和四项专业领域任务中超越了PromptWizard、OPRO和RL-Prompt，对多数任务-模型组合性能均有提升，仅在少数情况下有轻微性能下降。

Conclusion: 本文方法有效应对复杂任务和敏感模型的提示优化问题，优于现有主流方法，同时其引入的性能下降极为有限。

Abstract: Prompt engineering has proven to be a crucial step in leveraging pretrained
large language models (LLMs) in solving various real-world tasks. Numerous
solutions have been proposed that seek to automate prompt engineering by using
the model itself to edit prompts. However, the majority of state-of-the-art
approaches are evaluated on tasks that require minimal prompt templates and on
very large and highly capable LLMs. In contrast, solving complex tasks that
require detailed information to be included in the prompt increases the amount
of text that needs to be optimised. Furthermore, smaller models have been shown
to be more sensitive to prompt design. To address these challenges, we propose
an evolutionary search approach to automated discrete prompt optimisation
consisting of two phases. In the first phase, grammar-guided genetic
programming is invoked to synthesise prompt-creating programmes by searching
the space of programmes populated by function compositions of syntactic,
dictionary-based and LLM-based prompt-editing functions. In the second phase,
local search is applied to explore the neighbourhoods of best-performing
programmes in an attempt to further fine-tune their performance. Our approach
outperforms three state-of-the-art prompt optimisation approaches,
PromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose
LLMs in four domain-specific challenging tasks. We also illustrate several
examples where these benchmark methods suffer relatively severe performance
degradation, while our approach improves performance in almost all task-model
combinations, only incurring minimal degradation when it does not.

</details>


### [215] [Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach](https://arxiv.org/abs/2507.10330)
*Mohammed Bouri,Adnane Saoud*

Main category: cs.CL

TL;DR: 研究提出了一种基于生长界矩阵（GBM）的新型正则化技术，以增强NLP模型应对对抗性攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管NLP技术取得了进展，但模型对同义词替换等对抗攻击依然脆弱，尤其是对递归网络和现代状态空间模型（如S4）的鲁棒性研究不足。

Method: 提出基于GBM的正则化技术，计算该矩阵以减少输入扰动对输出的影响，针对LSTM、S4和CNN三种结构进行实验。

Result: 实验在多个架构和基准数据集上表明，与现有方法相比，该方法将对抗鲁棒性提高了最高8.8%。

Conclusion: 该方法有效提高了NLP模型的鲁棒性，优于多种现有的前沿对抗防御方法，并首次对SSM(S4)的鲁棒性进行了系统分析。

Abstract: Despite advancements in Natural Language Processing (NLP), models remain
vulnerable to adversarial attacks, such as synonym substitutions. While prior
work has focused on improving robustness for feed-forward and convolutional
architectures, the robustness of recurrent networks and modern state space
models (SSMs), such as S4, remains understudied. These architectures pose
unique challenges due to their sequential processing and complex parameter
dynamics. In this paper, we introduce a novel regularization technique based on
Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the
impact of input perturbations on model outputs. We focus on computing the GBM
for three architectures: Long Short-Term Memory (LSTM), State Space models
(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance
resilience against word substitution attacks, (2) improve generalization on
clean text, and (3) providing the first systematic analysis of SSM (S4)
robustness. Extensive experiments across multiple architectures and benchmark
datasets demonstrate that our method improves adversarial robustness by up to
8.8% over existing baselines. These results highlight the effectiveness of our
approach, outperforming several state-of-the-art methods in adversarial
defense. Codes are available at https://github.com/BouriMohammed/GBM

</details>


### [216] [Using AI to replicate human experimental results: a motion study](https://arxiv.org/abs/2507.10342)
*Rosa Illan Castillo,Javier Valenzuela*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型(LLMs)在语言学研究中的潜力，证明其可作为可靠的分析工具，尤其是在情感意义与时间表达相关的研究中，与人类数据结果高度一致。


<details>
  <summary>Details</summary>
Motivation: 研究探索大型语言模型如GPT-4在语言学中复制和效仿人类判断的能力，以及是否可以替代或补充人类心理语言学实验。

Method: 进行了四项心理语言学研究，涉及 emergent meanings、valence shifts、情感背景中的动词选择和句子-表情符号关联，分别对比了人类参与者与大型语言模型的表现，使用统计分析如Spearman's rho 评估相关性。

Result: 人类与LLMs之间在评分模式及分类选择上呈现出强相关性(Spearman’s rho = .73-.96)，尽管少数情况下有些细微差别，但并未影响整体的解读结果。

Conclusion: 研究表明，LLMs可以补充传统的人类实验，为语言学研究提供可靠数据，并支持其作为可信的协作者，通过AI实现假设生成和数据扩展。

Abstract: This paper explores the potential of large language models (LLMs) as reliable
analytical tools in linguistic research, focusing on the emergence of affective
meanings in temporal expressions involving manner-of-motion verbs. While LLMs
like GPT-4 have shown promise across a range of tasks, their ability to
replicate nuanced human judgements remains under scrutiny. We conducted four
psycholinguistic studies (on emergent meanings, valence shifts, verb choice in
emotional contexts, and sentence-emoji associations) first with human
participants and then replicated the same tasks using an LLM. Results across
all studies show a striking convergence between human and AI responses, with
statistical analyses (e.g., Spearman's rho = .73-.96) indicating strong
correlations in both rating patterns and categorical choices. While minor
divergences were observed in some cases, these did not alter the overall
interpretative outcomes. These findings offer compelling evidence that LLMs can
augment traditional human-based experimentation, enabling broader-scale studies
without compromising interpretative validity. This convergence not only
strengthens the empirical foundation of prior human-based findings but also
opens possibilities for hypothesis generation and data expansion through AI.
Ultimately, our study supports the use of LLMs as credible and informative
collaborators in linguistic inquiry.

</details>


### [217] [Meanings are like Onions: a Layered Approach to Metaphor Processing](https://arxiv.org/abs/2507.10354)
*Silvia Cappa,Anna Sofia Lippolis,Stefano Zoia*

Main category: cs.CL

TL;DR: 本文提出了一个形象化的隐喻处理模型，将隐喻意义分为三个层次：内容分析、概念整合和语用意图，以实现更丰富的意义解析。


<details>
  <summary>Details</summary>
Motivation: 隐喻意义是一种复杂的认知现象，目前缺乏有效的计算模型对其多层次特性进行解释。

Method: 研究中开发了一个三维框架：（1）从基本概念元素出发进行内容分析；（2）结合隐喻的概念层次进行意义整合；（3）通过语用词汇识别说话者意图和上下文影响。

Result: 该模型能够统一各层次的隐喻理解，为开发具备深层次语境推理能力的计算系统奠定基础。

Conclusion: 通过多维框架的设计，该研究增强了对隐喻意义的解释能力，为计算隐喻分析打开了新的可能性。

Abstract: Metaphorical meaning is not a flat mapping between concepts, but a complex
cognitive phenomenon that integrates multiple levels of interpretation. In this
paper, we propose a stratified model of metaphor processing that treats meaning
as an onion: a multi-layered structure comprising (1) content analysis, (2)
conceptual blending, and (3) pragmatic intentionality. This three-dimensional
framework allows for a richer and more cognitively grounded approach to
metaphor interpretation in computational systems. At the first level, metaphors
are annotated through basic conceptual elements. At the second level, we model
conceptual combinations, linking components to emergent meanings. Finally, at
the third level, we introduce a pragmatic vocabulary to capture speaker intent,
communicative function, and contextual effects, aligning metaphor understanding
with pragmatic theories. By unifying these layers into a single formal
framework, our model lays the groundwork for computational methods capable of
representing metaphorical meaning beyond surface associations, toward deeper,
more context-sensitive reasoning.

</details>


### [218] [From Sequence to Structure: Uncovering Substructure Reasoning in Transformers](https://arxiv.org/abs/2507.10435)
*Xinnan Dai,Kai Yang,Jay Revolinsky,Kai Guo,Aoran Wang,Bohang Zhang,Jiliang Tang*

Main category: cs.CL

TL;DR: 研究表明，大型语言模型可以解决图推理任务，并通过分析其机制提出了诱导子结构过滤（ISF）方法，用于解释和优化Transformer在处理图数据时的能力。


<details>
  <summary>Details</summary>
Motivation: 探讨解码器仅Transformer架构如何理解潜在图结构，以及其在图数据处理中的具体能力。

Method: 提出并验证了诱导子结构过滤（ISF）方法，研究Transformer多层内部一致性的动态机制，并引入基于子结构思维的概念，用于解析复杂图模式。

Result: 验证了ISF机制的有效性和一致性，证实了解码器仅Transformer在从属性图中提取子结构任务中的卓越性能。

Conclusion: 论文提供了关于序列模型如何执行图数据子结构提取任务的新见解，为进一步优化Transformer的图数据处理能力奠定了基础。

Abstract: Recent studies suggest that large language models (LLMs) possess the
capability to solve graph reasoning tasks. Notably, even when graph structures
are embedded within textual descriptions, LLMs can still effectively answer
related questions. This raises a fundamental question: How can a decoder-only
Transformer architecture understand underlying graph structures? To address
this, we start with the substructure extraction task, interpreting the inner
mechanisms inside the transformers and analyzing the impact of the input
queries. Specifically, through both empirical results and theoretical analysis,
we present Induced Substructure Filtration (ISF), a perspective that captures
the substructure identification in the multi-layer transformers. We further
validate the ISF process in LLMs, revealing consistent internal dynamics across
layers. Building on these insights, we explore the broader capabilities of
Transformers in handling diverse graph types. Specifically, we introduce the
concept of thinking in substructures to efficiently extract complex composite
patterns, and demonstrate that decoder-only Transformers can successfully
extract substructures from attributed graphs, such as molecular graphs.
Together, our findings offer a new insight on how sequence-based Transformers
perform the substructure extraction task over graph data.

</details>


### [219] [Referential ambiguity and clarification requests: comparing human and LLM behaviour](https://arxiv.org/abs/2507.10445)
*Chris Madge,Matthew Purver,Massimo Poesio*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型 (LLMs) 在任务型对话中提出澄清性问题的能力，结合了Minecraft Dialogue Corpus的多个注释，对比了人类和模型应对模糊性的方式。


<details>
  <summary>Details</summary>
Motivation: 想要研究LLMs在模糊性和任务不确定性情况下提出澄清性问题的能力及其与人类的对比。

Method: 构建了一个通过整合现有注释的语料库，以分析澄清性问题与模糊性之间的联系和差异。比较了模型生成的问题与人类生成的澄清性问题之间的区别，并测试了不同推理方式对问题生成的影响。

Result: 发现人类与LLMs在模糊性和任务不确定性上的应对方式差异大。人类主要在任务不确定性下提出问题；LLMs更关注指代模糊性。推理能力的提高可以增加问题的频率和相关性。

Conclusion: LLMs生成澄清性问题的能力可能依赖于其模拟推理的能力，但在人类生成的问题模式上依然有差距。

Abstract: In this work we examine LLMs' ability to ask clarification questions in
task-oriented dialogues that follow the asynchronous
instruction-giver/instruction-follower format. We present a new corpus that
combines two existing annotations of the Minecraft Dialogue Corpus -- one for
reference and ambiguity in reference, and one for SDRT including clarifications
-- into a single common format providing the necessary information to
experiment with clarifications and their relation to ambiguity. With this
corpus we compare LLM actions with original human-generated clarification
questions, examining how both humans and LLMs act in the case of ambiguity. We
find that there is only a weak link between ambiguity and humans producing
clarification questions in these dialogues, and low correlation between humans
and LLMs. Humans hardly ever produce clarification questions for referential
ambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce
more clarification questions for referential ambiguity, but less so for task
uncertainty. We question if LLMs' ability to ask clarification questions is
predicated on their recent ability to simulate reasoning, and test this with
different reasoning approaches, finding that reasoning does appear to increase
question frequency and relevancy.

</details>


### [220] [From BERT to Qwen: Hate Detection across architectures](https://arxiv.org/abs/2507.10468)
*Ariadna Mon,Saúl Fenollosa,Jon Lecumberri*

Main category: cs.CL

TL;DR: 本文比较了经典双向转换器编码器与超大型自回归大语言模型（LLMs）的仇恨言论检测性能。


<details>
  <summary>Details</summary>
Motivation: 研究希望探讨超大型LLMs是否能在实际仇恨言论检测中超越传统双向编码器模型。

Method: 将两种模型家族在精心整理的在线互动语料库上进行基准测试，以检测仇恨言论。

Result: 对比结果揭示两种模型在不同情境下的仇恨言论检测表现。

Conclusion: 超大型LLMs可能因更深的上下文感知能力在某些情况下胜过传统方法，但尚需更多实际验证。

Abstract: Online platforms struggle to curb hate speech without over-censoring
legitimate discourse. Early bidirectional transformer encoders made big
strides, but the arrival of ultra-large autoregressive LLMs promises deeper
context-awareness. Whether this extra scale actually improves practical
hate-speech detection on real-world text remains unverified. Our study puts
this question to the test by benchmarking both model families, classic encoders
and next-generation LLMs, on curated corpora of online interactions for
hate-speech detection (Hate or No Hate).

</details>


### [221] [MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking](https://arxiv.org/abs/2507.10472)
*Mohamed T. Younes,Omar Walid,Mai Hassan,Ali Hamdi*

Main category: cs.CL

TL;DR: 文章提出了一种名为MLAR的机器人流程自动化框架，用于优化招聘流程，显著提高了简历处理效率和效果。


<details>
  <summary>Details</summary>
Motivation: 解决传统招聘流程中因时间和资源限制而导致的简历筛选和候选人选择效率低下的问题。

Method: MLAR通过三层架构应用大型语言模型（LLMs）：第一层提取职位信息的关键特征，第二层解析申请者简历包括教育、经验和技能，第三层进行语义匹配，辅以先进的语义算法确定最佳候选人，同时集成现有RPA流程自动化简历解析、职位匹配及通知工作。

Result: MLAR在高容量简历处理任务中表现优于主流RPA平台（如UiPath和Automation Anywhere）。在处理2400份简历时，其平均耗时为5.4秒/份，分别比Automation Anywhere和UiPath快16.9%和17.1%。

Conclusion: MLAR框架展现了提升招聘流程效率、精准性和可扩展性的显著潜力，为现代招聘需求提供了优化的解决方案。

Abstract: This paper introduces an innovative Applicant Tracking System (ATS) enhanced
by a novel Robotic process automation (RPA) framework or as further referred to
as MLAR. Traditional recruitment processes often encounter bottlenecks in
resume screening and candidate shortlisting due to time and resource
constraints. MLAR addresses these challenges employing Large Language Models
(LLMs) in three distinct layers: extracting key characteristics from job
postings in the first layer, parsing applicant resume to identify education,
experience, skills in the second layer, and similarity matching in the third
layer. These features are then matched through advanced semantic algorithms to
identify the best candidates efficiently. Our approach integrates seamlessly
into existing RPA pipelines, automating resume parsing, job matching, and
candidate notifications. Extensive performance benchmarking shows that MLAR
outperforms the leading RPA platforms, including UiPath and Automation
Anywhere, in high-volume resume-processing tasks. When processing 2,400
resumes, MLAR achieved an average processing time of 5.4 seconds per resume,
reducing processing time by approximately 16.9% compared to Automation Anywhere
and 17.1% compared to UiPath. These results highlight the potential of MLAR to
transform recruitment workflows by providing an efficient, accurate, and
scalable solution tailored to modern hiring needs.

</details>


### [222] [Can You Detect the Difference?](https://arxiv.org/abs/2507.10475)
*İsmail Tarım,Aytuğ Onan*

Main category: cs.CL

TL;DR: 本文研究了扩散生成文本(LLaDA)与自回归生成文本(LLaMA)的区别，并验证现有AI生成文本检测方法在扩散生成文本上的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的发展，对AI生成文本的检测需求增加，而现有检测方法在扩散生成文本上的性能尚未研究。

Method: 对2000个样本进行分析，比较LLaDA与LLaMA的困惑度、突发性、词汇多样性、可读性以及BLEU/ROUGE评分等指标。

Result: 发现LLaDA在困惑度及突发性上更接近人类文本，导致现有检测方法的高伪阴率，而LLaMA显示更低的困惑度但词汇忠实性较低。单一指标难以区分扩散生成文本与人类写作。

Conclusion: 需要开发针对扩散生成文本的检测器，例如混合模型、特定风格特征以及稳健的水印技术。

Abstract: The rapid advancement of large language models (LLMs) has raised concerns
about reliably detecting AI-generated text. Stylometric metrics work well on
autoregressive (AR) outputs, but their effectiveness on diffusion-based models
is unknown. We present the first systematic comparison of diffusion-generated
text (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,
burstiness, lexical diversity, readability, and BLEU/ROUGE scores show that
LLaDA closely mimics human text in perplexity and burstiness, yielding high
false-negative rates for AR-oriented detectors. LLaMA shows much lower
perplexity but reduced lexical fidelity. Relying on any single metric fails to
separate diffusion outputs from human writing. We highlight the need for
diffusion-aware detectors and outline directions such as hybrid models,
diffusion-specific stylometric signatures, and robust watermarking.

</details>


### [223] [Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation](https://arxiv.org/abs/2507.10524)
*Sangmin Bae,Yujin Kim,Reza Bayat,Sungnyun Kim,Jiyoun Ha,Tal Schuster,Adam Fisch,Hrayr Harutyunyan,Ziwei Ji,Aaron Courville,Se-Young Yun*

Main category: cs.CL

TL;DR: 该论文提出了Mixture-of-Recursions (MoR)框架，通过复用共享层和动态分配递归深度来兼顾参数和计算效率，从而提升语言模型性能并降低成本。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型规模的增加，计算和内存需求显著增加，导致训练与部署成本过高，因此需要一种兼顾参数共享与自适应计算的方法。

Method: 提出MoR框架，通过共享堆栈实现参数效率，轻量级路由器按需为每个token分配递归深度，并优化内存缓存机制，同时提出KV共享变体以降低延迟与内存占用。

Result: 实验结果表明，在相同训练FLOPs与较小模型尺寸下，MoR显著降低验证困惑度，提升Few-shot准确性，并提升吞吐量，与传统方法相比表现卓越。

Conclusion: MoR证明了一种有效的途径，可在不增加大模型成本的情况下实现大模型质量。

Abstract: Scaling language models unlocks impressive capabilities, but the accompanying
computational and memory demands make both training and deployment expensive.
Existing efficiency efforts typically target either parameter sharing or
adaptive computation, leaving open the question of how to attain both
simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework
that combines the two axes of efficiency inside a single Recursive Transformer.
MoR reuses a shared stack of layers across recursion steps to achieve parameter
efficiency, while lightweight routers enable adaptive token-level thinking by
dynamically assigning different recursion depths to individual tokens. This
allows MoR to focus quadratic attention computation only among tokens still
active at a given recursion depth, further improving memory access efficiency
by selectively caching only their key-value pairs. Beyond these core
mechanisms, we also propose a KV sharing variant that reuses KV pairs from the
first recursion, specifically designed to decrease prefill latency and memory
footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms
a new Pareto frontier: at equal training FLOPs and smaller model sizes, it
significantly lowers validation perplexity and improves few-shot accuracy,
while delivering higher throughput compared with vanilla and existing recursive
baselines. These gains demonstrate that MoR is an effective path towards
large-model quality without incurring large-model cost.

</details>


### [224] [CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks](https://arxiv.org/abs/2507.10535)
*Hongchao Jiang,Yiming Chen,Yushi Cao,Hung-yi Lee,Robby T. Tan*

Main category: cs.CL

TL;DR: 本文介绍了CodeJudgeBench，一个评估LLM作为代码任务评审者表现的新基准，包括代码生成、代码修复和单元测试生成三个关键任务。研究表明，较新的推理模型优于非推理模型，但在评价过程中仍有随机性和可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM能否在代码领域有效作为评估者，并解决缺少专门基准的问题。

Method: 引入CodeJudgeBench作为评估LLM-as-a-Judge性能的专用基准，并在代码相关任务上测试26种不同的LLM-as-a-Judge模型。

Result: 发现最新的推理模型在代码评审中更具优势，小型推理模型如Qwen3-8B甚至能超过更大的特定训练模型。但模型对任务的判断存在随机性、敏感性和一致性问题。

Conclusion: 尽管结果显示LLM-as-a-Judge潜力巨大，但评审中的随机性和一致性问题需进一步研究。同时，优化提示策略如保留完整响应及采用成对比较可提升评审表现。

Abstract: Large Language Models (LLMs) have significantly advanced the state-of-the-art
in various coding tasks. Beyond directly answering user queries, LLMs can also
serve as judges, assessing and comparing the quality of responses generated by
other models. Such an evaluation capability is crucial both for benchmarking
different LLMs and for improving response quality through response ranking.
However, despite the growing adoption of the LLM-as-a-Judge paradigm, its
effectiveness in coding scenarios remains underexplored due to the absence of
dedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a
benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge
models across three critical coding tasks: code generation, code repair, and
unit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge
models, we find that recent thinking models significantly outperform
non-thinking models on our carefully designed code judging tasks. Notably, even
relatively small thinking models, such as Qwen3-8B, can outperform specially
trained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still
exhibit significant randomness in their judgment of coding tasks. For pairwise
judging tasks, simply changing the order in which responses are presented can
substantially impact accuracy. In addition, when judging code and unit tests
written by different LLMs, LLM-as-a-Judge models also show variance in
performance. This sensitivity raises concerns about the reliability and
consistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal
prompting strategies for LLM-as-a-Judge. We find that using pair-wise
comparison outperforms scalar point-wise judging. Furthermore, retaining
comments and reasoning in the full, unprocessed LLM response leads to improved
judge performance.

</details>


### [225] [REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once](https://arxiv.org/abs/2507.10541)
*Zhuoshi Pan,Qizhi Pei,Yu Li,Qiyao Sun,Zinan Tang,H. Vicky Zhao,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: REST是一种测试当前大规模推理模型（LRMs）在多问同步下的表现的新框架，以应对现有评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 目前的LRMs评估只关注单一问题推理，存在数据污染风险和无法测试真实多上下文压力下模型性能的局限。

Method: 提出了REST（Reasoning Evaluation through Simultaneous Testing），通过多问题同步测试LRMs的能力，并评估其分配优先级、抗干扰和认知负载管理的能力。

Result: REST发现即使是先进模型如DeepSeek-R1在压力下处理多任务时表现显著下降，并提出“长转短”的训练方式能提升模型在多任务下的性能。

Conclusion: REST相比传统方法具有更强的区分能力，并且能够在更接近真实应用场景下衡量模型表现，同时减少对人类标注的依赖。

Abstract: Recent Large Reasoning Models (LRMs) have achieved remarkable progress on
task-specific benchmarks, yet their evaluation methods remain constrained by
isolated problem-solving paradigms. Existing benchmarks predominantly assess
single-question reasoning through sequential testing, resulting critical
limitations: (1) vulnerability to data contamination and less challenging
(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual
creation of new questions with large human efforts, (2) failure to evaluate
models under multi-context pressure, a key requirement for real-world
deployment. To bridge this gap, we present REST (Reasoning Evaluation through
Simultaneous Testing), a stress-testing framework that concurrently exposes
LRMs to multiple problems simultaneously. Beyond basic reasoning, REST
specifically evaluates several under-tested capabilities: contextual priority
allocation, cross-problem interference resistance, and dynamic cognitive load
management. Our evaluation reveals several striking findings: Even
state-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance
degradation under stress testing. Crucially, REST demonstrates stronger
discriminative power than existing benchmarks, revealing pronounced performance
differences among models that exhibit similar, near-ceiling performance under
single-question evaluations. Some key mechanistic insights emerge from our
analysis: (1) the "overthinking trap" is a critical factor contributing to the
performance degradation; (2) the models trained with "long2short" technique
preserve more accuracy of their single-problem performance under REST,
outperforming standard-trained counterparts. These results establish REST as a
cost-efficient, future-proof evaluation paradigm that better reflects
real-world reasoning demands while reducing reliance on continuous human
annotation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [226] [Think Clearly: Improving Reasoning via Redundant Token Pruning](https://arxiv.org/abs/2507.08806)
*Daewon Choi,Jimin Lee,Jihoon Tack,Woomin Song,Saket Dingliwal,Sai Muralidhar Jayanthi,Bhavana Ganesh,Jinwoo Shin,Aram Galstyan,Sravan Babu Bodapati*

Main category: cs.AI

TL;DR: 研究展示了通过去除语言模型推理过程中的冗余显著提高了性能，尤其在数学竞赛等推理密集的基准中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在长形式推理中存在显著冗余，导致准确性下降。

Method: 提出通过测量特殊“结束思考”标记的注意力分数来识别推理冗余，并使用结构感知剪枝方法去除低贡献的推理片段。

Result: 改进的方法在推理密集型任务的准确性上显著提高，特别是在数学竞赛基准上表现优异。

Conclusion: 通过减少推理冗余，优化性地提升了模型的推理能力与准确性，无需额外训练。

Abstract: Recent large language models have shown promising capabilities in long-form
reasoning, following structured chains of thought before arriving at a final
answer. However, we observe that these reasoning paths tend to include
substantial redundancy; analyzing attention patterns reveals that attention
scores are widely scattered, particularly incorrect answers exhibit greater
attention sparsity. In this paper, we demonstrate that deliberately removing
this redundancy in the reasoning process significantly improves performance
through clear thinking, i.e., removing distraction. Specifically, we
systematically identify reasoning redundancy by measuring token-level attention
scores to a special end-of-thinking token, which is appended to an explicit
instruction inserted to conclude each intermediate reasoning step. Furthermore,
we propose structure-aware pruning that prioritizes removing tokens in
low-contributing reasoning chunks over individual tokens. After evicting
redundant tokens, we remove the injected end-of-thinking instruction, then
resume the reasoning generation. We demonstrate that our method significantly
improves overall accuracy across reasoning-intensive benchmarks without any
training involved. In particular, our method shows strong performance on
challenging mathematical competition benchmarks such as AIME and AMC, where
reasoning redundancy is more prevalent.

</details>


### [227] [A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data](https://arxiv.org/abs/2507.08875)
*Fuh-Hwa Franklin Liu,Su-Chuan Shih*

Main category: cs.AI

TL;DR: 本文介绍了一种结合两个虚拟差距分析(VGA)模型的新型多准则评价(MCA)方法，旨在提高效率和公平性。


<details>
  <summary>Details</summary>
Motivation: 针对不同领域中基于多种定量和定性指标对决策单元(DMU)进行评价时现有方法存在的主观性和评价同质性假设问题。

Method: 结合两个基于线性规划的虚拟差距分析(VGA)模型。

Result: 提供了一种准确透明的评价方法，通过两个数值案例验证其有效性和可靠性。

Conclusion: 该方法改进了多准则评价的效率和公平性，鼓励自动化决策系统和决策支持系统的进一步发展。

Abstract: Modern methods for multi-criteria assessment (MCA), such as Data Envelopment
Analysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria
Decision-Making (MCDM), are utilized to appraise a collection of
Decision-Making Units (DMUs), also known as alternatives, based on several
criteria. These methodologies inherently rely on assumptions and can be
influenced by subjective judgment to effectively tackle the complex evaluation
challenges in various fields. In real-world scenarios, it is essential to
incorporate both quantitative and qualitative criteria as they consist of
cardinal and ordinal data. Despite the inherent variability in the criterion
values of different alternatives, the homogeneity assumption is often employed,
significantly affecting evaluations. To tackle these challenges and determine
the most appropriate alternative, we propose a novel MCA approach that combines
two Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear
programming, is pivotal in the MCA methodology. This approach improves
efficiency and fairness, ensuring that evaluations are both comprehensive and
dependable, thus offering a strong and adaptive solution. Two comprehensive
numerical examples demonstrate the accuracy and transparency of our proposed
method. The goal is to encourage continued advancement and stimulate progress
in automated decision systems and decision support systems.

</details>


### [228] [Multi-Actor Generative Artificial Intelligence as a Game Engine](https://arxiv.org/abs/2507.08892)
*Alexander Sasha Vezhnevets,Jayd Matyas,Logan Cross,Davide Paglieri,Minsuk Chang,William A. Cunningham,Simon Osindero,William S. Isaac,Joel Z. Leibo*

Main category: cs.AI

TL;DR: 这篇论文讨论了生成式AI在多参与者环境中的应用，提出用桌面角色扮演游戏（TTRPG）作为灵感，为创造灵活的场景定义框架提供解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI缺乏灵活的场景定义框架，限制了其在仿真、叙事和评价等多样化场景下的应用效果。

Method: 借鉴TTRPG的游戏结构，引入Entity-Component架构模式，以配置化实体（包括环境管理者GM）取代硬编码系统，从而实现组件的分离及模块化设计。

Result: 通过描述Concordia库的演变示例，展示了该方法能够高效地配置满足特定目标的场景。

Conclusion: 利用Entity-Component模式，在提高模块化与可扩展性的同时，支持了广泛应用场景的灵活构建。

Abstract: Generative AI can be used in multi-actor environments with purposes ranging
from social science modeling to interactive narrative and AI evaluation.
Supporting this diversity of use cases -- which we classify as Simulationist,
Dramatist, and Evaluationist -- demands a flexible scenario definition
framework. We argue here that a good approach is to take inspiration from
tabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible
for the environment and generates all parts of the story not directly
determined by the voluntary actions of player characters. We argue that the
Entity-Component architectural pattern is useful here. In such a system, the GM
is not a hardcoded computer game but is itself a configurable entity, composed
of components just like any other actor. By design, the approach allows for a
separation between the underlying implementation details handled by an
engineer, the creation of reusable components, and their composition and
configuration managed by a designer who constructs entities from the
components. This separation of concerns is instrumental for achieving rapid
iteration, maintaining modularity, and ultimately to ensure scalability. We
describe the ongoing evolution of the Concordia library in terms of this
philosophy, demonstrating how it allows users to effectively configure
scenarios that align with their specific goals.

</details>


### [229] [BioAnalyst: A Foundation Model for Biodiversity](https://arxiv.org/abs/2507.09080)
*Athanasios Trantas,Martino Mensio,Stylianos Stasinos,Sebastian Gribincea,Taimur Khan,Damian Podareanu,Aliene van der Veen*

Main category: cs.AI

TL;DR: BioAnalyst是第一个针对生物多样性分析和保护规划设计的通用基础模型，能够应对多种任务并在数据稀缺场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着生物多样性加速丧失，人类面临生态学研究与保护策略上的严峻挑战，亟需先进技术应对。

Method: 通过基于Transformer的架构，BioAnalyst利用多模态数据（包括物种记录、遥感、气候及环境变量）进行预训练，并支持微调以适应多个下游任务。

Result: 在物种分布建模和种群趋势预测等应用中，BioAnalyst在数据不足的场景下表现出卓越精度，超过现有方法。

Conclusion: 通过公开BioAnalyst模型及其微调流程，促进科学界的合作与技术革新，为应对生态挑战提供AI驱动的解决方案。

Abstract: The accelerating loss of biodiversity presents critical challenges for
ecological research and conservation strategies. The preservation of
biodiversity is paramount for maintaining ecological balance and ensuring the
sustainability of ecosystems. However, biodiversity faces numerous threats,
including habitat loss, climate change, and the proliferation of invasive
species. Addressing these and other ecology-related challenges, both at local
and global scales, requires comprehensive monitoring, predictive and
conservation planning capabilities. Artificial Intelligence (AI) Foundation
Models (FMs) have gained significant momentum in numerous scientific domains by
leveraging vast datasets to learn general-purpose representations adaptable to
various downstream tasks. This paradigm holds immense promise for biodiversity
conservation. In response, we introduce BioAnalyst, the first Foundation Model
tailored for biodiversity analysis and conservation planning. BioAnalyst
employs a transformer-based architecture, pre-trained on extensive multi-modal
datasets encompassing species occurrence records, remote sensing indicators,
climate and environmental variables. BioAnalyst is designed for adaptability,
allowing for fine-tuning of a range of downstream tasks, such as species
distribution modelling, habitat suitability assessments, invasive species
detection, and population trend forecasting. We evaluate the model's
performance on two downstream use cases, demonstrating its generalisability
compared to existing methods, particularly in data-scarce scenarios for two
distinct use-cases, establishing a new accuracy baseline for ecological
forecasting. By openly releasing BioAnalyst and its fine-tuning workflows to
the scientific community, we aim to foster collaborative efforts in
biodiversity modelling and advance AI-driven solutions to pressing ecological
challenges.

</details>


### [230] [Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity](https://arxiv.org/abs/2507.09089)
*Joel Becker,Nate Rush,Elizabeth Barnes,David Rein*

Main category: cs.AI

TL;DR: 研究分析AI工具对开源软件开发者生产力的实际影响，发现尽管预测认为AI能提升效率，但实际却增加了开发所需时间。


<details>
  <summary>Details</summary>
Motivation: 尽管AI工具被广泛采用，但其对野外软件开发的具体影响尚未得到深入研究。

Method: 进行随机对照实验，让具有5年经验的16名开发者使用或不使用AI工具完成246个任务，然后收集结果和相关数据进行分析。

Result: 允许使用AI工具的情况下，开发者完成任务的时间反而增加了19%。

Conclusion: 研究表明，AI工具未能如预期提高生产力，反而使开发时间延长，这揭示了AI在实际应用中的新问题。

Abstract: Despite widespread adoption, the impact of AI tools on software development
in the wild remains understudied. We conduct a randomized controlled trial
(RCT) to understand how AI tools at the February-June 2025 frontier affect the
productivity of experienced open-source developers. 16 developers with moderate
AI experience complete 246 tasks in mature projects on which they have an
average of 5 years of prior experience. Each task is randomly assigned to allow
or disallow usage of early 2025 AI tools. When AI tools are allowed, developers
primarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.
Before starting tasks, developers forecast that allowing AI will reduce
completion time by 24%. After completing the study, developers estimate that
allowing AI reduced completion time by 20%. Surprisingly, we find that allowing
AI actually increases completion time by 19%--AI tooling slowed developers
down. This slowdown also contradicts predictions from experts in economics (39%
shorter) and ML (38% shorter). To understand this result, we collect and
evaluate evidence for 20 properties of our setting that a priori could
contribute to the observed slowdown effect--for example, the size and quality
standards of projects, or prior developer experience with AI tooling. Although
the influence of experimental artifacts cannot be entirely ruled out, the
robustness of the slowdown effect across our analyses suggests it is unlikely
to primarily be a function of our experimental design.

</details>


### [231] [Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System](https://arxiv.org/abs/2507.09179)
*Ronghua Shi,Yiou Liu,Xinyu Ying,Yang Tan,Yuchun Feng,Lynn Ai,Bill Shi,Xuhui Wang,Zhuang Liu*

Main category: cs.AI

TL;DR: 本文提出了一个多智能体强化学习（MARL）框架“Hide-and-Shill”以检测去中心化金融（DeFi）中的市场操纵行为，实现实时且分布式的金融监控。


<details>
  <summary>Details</summary>
Motivation: DeFi的快速发展虽然带来了创新，但也伴随着市场操纵行为。在去中心化环境中，缺乏集中监管导致协同的抬拉动作和拉高出货行为泛滥。

Method: 作者提出了Hide-and-Shill系统，利用MARL框架以操纵者和检测者之间的动态对抗游戏建模，主要创新为：1）引入Group Relative Policy Optimization（GRPO）以提升稀疏奖励环境中的学习稳定性；2）基于信息不对称理论设定奖惩机制区分价格发现与操纵噪声；3）结合语义特征、社交图信号和链上市场数据的多模态智能体管道进行决策。

Result: Hide-and-Shill在100,000个现实数据和对抗模拟中达到顶级检测准确率与因果归因能力。

Conclusion: 该框架通过去中心化的多智能体架构增强了DeFi生态中的市场情报能力，推进金融监管的去中心化新范式，同时为学术界提供了开源资源以支持进一步研究。

Abstract: Decentralized finance (DeFi) has introduced a new era of permissionless
financial innovation but also led to unprecedented market manipulation. Without
centralized oversight, malicious actors coordinate shilling campaigns and
pump-and-dump schemes across various platforms. We propose a Multi-Agent
Reinforcement Learning (MARL) framework for decentralized manipulation
detection, modeling the interaction between manipulators and detectors as a
dynamic adversarial game. This framework identifies suspicious patterns using
delayed token price reactions as financial indicators.Our method introduces
three innovations: (1) Group Relative Policy Optimization (GRPO) to enhance
learning stability in sparse-reward and partially observable settings; (2) a
theory-based reward function inspired by rational expectations and information
asymmetry, differentiating price discovery from manipulation noise; and (3) a
multi-modal agent pipeline that integrates LLM-based semantic features, social
graph signals, and on-chain market data for informed decision-making.The
framework is integrated within the Symphony system, a decentralized multi-agent
architecture enabling peer-to-peer agent execution and trust-aware learning
through distributed logs, supporting chain-verifiable evaluation. Symphony
promotes adversarial co-evolution among strategic actors and maintains robust
manipulation detection without centralized oracles, enabling real-time
surveillance across global DeFi ecosystems.Trained on 100,000 real-world
discourse episodes and validated in adversarial simulations, Hide-and-Shill
achieves top performance in detection accuracy and causal attribution. This
work bridges multi-agent systems with financial surveillance, advancing a new
paradigm for decentralized market intelligence. All resources are available at
the Hide-and-Shill GitHub repository to promote open research and
reproducibility.

</details>


### [232] [When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents](https://arxiv.org/abs/2507.09329)
*Matous Kozak,Roshanak Zilouchian Moghaddam,Siva Sivaraman*

Main category: cs.AI

TL;DR: 本研究首次系统性地评估了基于LLM的编码代理的安全性，发现其处理任务中存在21%的不安全行为，并提出了检测和缓解策略。


<details>
  <summary>Details</summary>
Motivation: 对基于LLM的编码代理进行全面的安全性评估，以解决其可能带来不安全编程实践的潜在问题。

Method: 分析了五种最先进的模型在93个真实软件设置任务中产生的12000多个行动，开发了高精度漏洞检测系统并测试缓解策略的有效性。

Result: 发现21%的编码代理行为不安全，信息暴露为最常见问题。GPT-4.1在缓解不安全实践方面表现最优，成功率96.8%。

Conclusion: 首次提供了评估编码代理安全性的框架，强调了在未来代理设计中引入安全意识的必要性。

Abstract: LLM-based coding agents are rapidly being deployed in software development,
yet their security implications remain poorly understood. These agents, while
capable of accelerating software development, may inadvertently introduce
insecure practices. We conducted the first systematic security evaluation of
autonomous coding agents, analyzing over 12,000 actions across five
state-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world
software setup tasks. Our findings reveal significant security concerns: 21% of
agent trajectories contained insecure actions, with models showing substantial
variation in security behavior. We developed a high-precision detection system
that identified four major vulnerability categories, with information exposure
(CWE-200) being the most prevalent one. We also evaluated mitigation strategies
including feedback mechanisms and security reminders with various effectiveness
between models. GPT-4.1 demonstrated exceptional security awareness with 96.8%
mitigation success. Our work provides the first comprehensive framework for
evaluating coding agent security and highlights the need for security-aware
design of next generation LLM-based coding agents.

</details>


### [233] [A Taxonomy of Omnicidal Futures Involving Artificial Intelligence](https://arxiv.org/abs/2507.09369)
*Andrew Critch,Jacob Tsimerman*

Main category: cs.AI

TL;DR: 本文提出了一些可能由AI引发的灭绝事件的分类及示例，旨在唤起社会对预防AI灾难性风险的重视。


<details>
  <summary>Details</summary>
Motivation: 通过展示由AI可能引发的灭绝事件，引发公众和机构关注，以推动预防措施的实施。

Method: 构建并展示潜在的AI灭绝事件分类及具体情景，向公众呈现可能性。

Result: 提供了多种可能的AI灾难场景，并且强调这些事件并非不可避免，而是可以预防的。

Conclusion: 借助于公共展示，这些可能的灭绝场景可提升对预防AI灾难性风险的公共支持和机构行动。

Abstract: This report presents a taxonomy and examples of potential omnicidal events
resulting from AI: scenarios where all or almost all humans are killed. These
events are not presented as inevitable, but as possibilities that we can work
to avoid. Insofar as large institutions require a degree of public support in
order to take certain actions, we hope that by presenting these possibilities
in public, we can help to support preventive measures against catastrophic
risks from AI.

</details>


### [234] [EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique](https://arxiv.org/abs/2507.09374)
*Chenglin Zhu,Tao Zhang,Chong Li,Mingan Lin,Zenan Zhou,Jian Xie*

Main category: cs.AI

TL;DR: EduFlow是一个涵盖科学推理全流程的框架，旨在解决当前多模态大规模语言模型在科学任务中推理能力不足的问题，特别是需要多步和可解释推理的情境。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态语言模型在科学任务中表现受限，尤其是多步推理、全局一致性及反思自纠能力不足，难以满足结构化科学需求。

Method: 提出了EduFlow框架，包括数据选择、基于MCTS的轨迹构建、模型训练和输出优化；核心组件是EduPRM，一个通过课程学习训练的奖励模型，通过批评推理步骤以提高模型性能。此外，EduMCTS通过引入自反思机制进行自纠，提升推理质量，并构建了包含16万条推理轨迹的大规模数据集。

Result: EduFlow通过实验验证提升了多步推理的一致性与连贯性，同时生成了EduMCTS-160K数据集。

Conclusion: EduFlow框架为教育性科学推理提供了一种全新的方法，并通过自反思与逐步 refinement 提升了推理的可靠性且将在开放源码社区发布资源。

Abstract: Multimodal large language models (MLLMs) still perform poorly on scientific
tasks, particularly those requiring multi-step and interpretable reasoning.
Their limitations include insufficient scientific reasoning patterns, lack of
global coherence in multi-step inference, and the absence of reflective
self-correction, making them unreliable in structured scientific contexts. We
introduce EduFlow, the first end-to-end framework that covers the full pipeline
of educational scientific reasoning, including data selection, MCTS-based
trajectory construction, model training, and output optimization. At its core
is EduPRM, a process-aware reward model that critiques reasoning steps with
tags and justifications. EduPRM is trained via curriculum learning on three
complementary supervision sources: MCTS-guided trajectories, error-injected
critiques, and teacher-student dialogues, enabling dynamic adaptation to
multi-stage problem solving and iterative refinement during inference. We
further propose EduMCTS, a domain-adapted search framework that introduces
bootstrapping actions specifically designed for educational reasoning, such as
a self-reflection mechanism that promotes reflective error correction. It
further leverages EduPRM's fine-grained feedback to guide the search toward
higher-quality reasoning trajectories. By applying self-consistency and
rejection sampling, we constructed EduMCTS-160K, a large-scale dataset of
educational reasoning trajectories. Extensive experiments demonstrate that
EduFlow enhances reasoning consistency and coherence. Code, data, and models
will be released.

</details>


### [235] [Knowledge Conceptualization Impacts RAG Efficacy](https://arxiv.org/abs/2507.09389)
*Chris Davis Jaldi,Anmol Saini,Elham Ghiasi,O. Divine Eziolise,Cogan Shimizu*

Main category: cs.AI

TL;DR: 本文研究了解释性、可解释性和适应性在未来神经符号AI系统中的重要性，以及它们的融合。


<details>
  <summary>Details</summary>
Motivation: 探索如何设计出既可转移又具有解释性的智能系统，特别是适用于Agentic Retrieval-Augmented Generation类系统。

Method: 系统评估了知识概念化和表示（特别是结构和复杂性）如何影响大语言模型有效查询三元组存储库。

Result: 结果显示，不同的知识表示方式对系统的查询效果有显著影响，并探讨了这些影响的意义。

Conclusion: 研究表明，知识的结构和复杂性在AI系统设计中具有重要作用，尤其是在提高系统的可解释性和适应性方面。

Abstract: Explainability and interpretability are cornerstones of frontier and
next-generation artificial intelligence (AI) systems. This is especially true
in recent systems, such as large language models (LLMs), and more broadly,
generative AI. On the other hand, adaptability to new domains, contexts, or
scenarios is also an important aspect for a successful system. As such, we are
particularly interested in how we can merge these two efforts, that is,
investigating the design of transferable and interpretable neurosymbolic AI
systems. Specifically, we focus on a class of systems referred to as ''Agentic
Retrieval-Augmented Generation'' systems, which actively select, interpret, and
query knowledge sources in response to natural language prompts. In this paper,
we systematically evaluate how different conceptualizations and representations
of knowledge, particularly the structure and complexity, impact an AI agent (in
this case, an LLM) in effectively querying a triplestore. We report our
results, which show that there are impacts from both approaches, and we discuss
their impact and implications.

</details>


### [236] [LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing](https://arxiv.org/abs/2507.09407)
*Quanyan Zhu*

Main category: cs.AI

TL;DR: 本文提出了LLM-Stackelberg博弈框架，这是一种结合大型语言模型（LLM）的层次决策模型，应用于领导者和追随者之间的战略互动研究。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过LLM融入博弈论框架以分析复杂的决策行为，特别是在信息不完全和代理非理性情况下的表现。

Method: 提出两种新的均衡概念：推理与行为均衡，以及推测性推理均衡，随后通过定制的算法，利用LLM生成行为并适应策略变化。

Result: 通过网络钓鱼案例研究，验证框架在分析对抗性交互和建模复杂决策领域（如网络安全、错误信息传播、推荐系统）中的效用。

Conclusion: LLM-Stackelberg博弈框架丰富了博弈论与AI交叉领域，对建模不对称信息、有限理性和认知适应提供了新工具，同时展示了在实际问题中的潜力。

Abstract: We introduce the framework of LLM-Stackelberg games, a class of sequential
decision-making models that integrate large language models (LLMs) into
strategic interactions between a leader and a follower. Departing from
classical Stackelberg assumptions of complete information and rational agents,
our formulation allows each agent to reason through structured prompts,
generate probabilistic behaviors via LLMs, and adapt their strategies through
internal cognition and belief updates. We define two equilibrium concepts:
reasoning and behavioral equilibrium, which aligns an agent's internal
prompt-based reasoning with observable behavior, and conjectural reasoning
equilibrium, which accounts for epistemic uncertainty through parameterized
models over an opponent's response. These layered constructs capture bounded
rationality, asymmetric information, and meta-cognitive adaptation. We
illustrate the framework through a spearphishing case study, where a sender and
a recipient engage in a deception game using structured reasoning prompts. This
example highlights the cognitive richness and adversarial potential of
LLM-mediated interactions. Our results show that LLM-Stackelberg games provide
a powerful paradigm for modeling decision-making in domains such as
cybersecurity, misinformation, and recommendation systems.

</details>


### [237] [GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective](https://arxiv.org/abs/2507.09495)
*Hang Wang,Junshan Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种从反应型到生成型多智能体强化学习的范式转变，利用生成式人工智能实现更高效的多智能体协调和决策。


<details>
  <summary>Details</summary>
Motivation: 针对多智能体强化学习中的关键问题：状态和动作空间的指数级增长，非平稳环境中的学习目标移动性以及部分可观测性限制。

Method: 使用生成式AI模型的能力来建模环境演进、预测其他智能体行为、生成协调动作序列，并进行战略推理以应对长期动态。

Result: 生成式RL代理可进行预测性的决策、增强的通讯协调以及动态适应，不仅提升个体智能，更展示出集体协作智能的可能性。

Conclusion: 这种范式转变有望解决传统框架难以处理的协调挑战，为分布式智能、机器人学和人机协作领域提供突破性解决方案。

Abstract: Multi-agent reinforcement learning faces fundamental challenges that
conventional approaches have failed to overcome: exponentially growing joint
action spaces, non-stationary environments where simultaneous learning creates
moving targets, and partial observability that constrains coordination. Current
methods remain reactive, employing stimulus-response mechanisms that fail when
facing novel scenarios. We argue for a transformative paradigm shift from
reactive to proactive multi-agent intelligence through generative AI-based
reinforcement learning. This position advocates reconceptualizing agents not as
isolated policy optimizers, but as sophisticated generative models capable of
synthesizing complex multi-agent dynamics and making anticipatory decisions
based on predictive understanding of future interactions. Rather than
responding to immediate observations, generative-RL agents can model
environment evolution, predict other agents' behaviors, generate coordinated
action sequences, and engage in strategic reasoning accounting for long-term
dynamics. This approach leverages pattern recognition and generation
capabilities of generative AI to enable proactive decision-making, seamless
coordination through enhanced communication, and dynamic adaptation to evolving
scenarios. We envision this paradigm shift will unlock unprecedented
possibilities for distributed intelligence, moving beyond individual
optimization toward emergent collective behaviors representing genuine
collaborative intelligence. The implications extend across autonomous systems,
robotics, and human-AI collaboration, promising solutions to coordination
challenges intractable under traditional reactive frameworks.

</details>


### [238] [Consistency Trajectory Planning: High-Quality and Efficient Trajectory Optimization for Offline Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.09534)
*Guanquan Wang,Takuya Hiraoka,Yoshimasa Tsuruoka*

Main category: cs.AI

TL;DR: 本文提出了一种名为Consistency Trajectory Planning (CTP)的新型离线基于模型的强化学习方法，利用最近提出的Consistency Trajectory Model (CTM)进行高效的轨迹优化。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于扩散模型的规划方法计算成本高的问题，同时希望实现快速的单步轨迹生成而不牺牲策略质量。

Method: 使用Consistency Trajectory Model (CTM)，实现比以往扩散模型更高效的单步轨迹生成与优化。

Result: 在D4RL基准测试中，CTP在长时间跨度、目标导向任务上始终优于现有基于扩散的规划方法，同时实现了超过120倍的推理速度提升。

Conclusion: CTP具备高性能和低延迟的特点，是一种实用且有效的离线规划方法。

Abstract: This paper introduces Consistency Trajectory Planning (CTP), a novel offline
model-based reinforcement learning method that leverages the recently proposed
Consistency Trajectory Model (CTM) for efficient trajectory optimization. While
prior work applying diffusion models to planning has demonstrated strong
performance, it often suffers from high computational costs due to iterative
sampling procedures. CTP supports fast, single-step trajectory generation
without significant degradation in policy quality. We evaluate CTP on the D4RL
benchmark and show that it consistently outperforms existing diffusion-based
planning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves
higher normalized returns while using significantly fewer denoising steps. In
particular, CTP achieves comparable performance with over $120\times$ speedup
in inference time, demonstrating its practicality and effectiveness for
high-performance, low-latency offline planning.

</details>


### [239] [Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling](https://arxiv.org/abs/2507.09540)
*Ali Safa,Farida Mohsen,Ali Al-Zawqari*

Main category: cs.AI

TL;DR: 该论文介绍了一种使用Metropolis-Hastings采样技术训练用于强化学习的脉冲神经网络（SNN）的方法，避免了梯度计算困境，并在两个控制基准测试上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 目前脉冲神经网络在能量效率和生物启发方面具有优势，但其训练特别在强化学习任务中面临挑战，主要是由于尖峰通信的不可微分性。

Method: 提出了一种基于Metropolis-Hastings采样的训练框架，通过累积奖励信号概率性地更新网络参数，绕过梯度反向传播的限制，并直接优化神经形态平台。

Result: 实验在AcroBot和CartPole基准测试上表明，这种MH技术在提高奖励积累、减少网络资源和训练周期方面优于传统深度Q学习和现有SNN强化学习方法。

Conclusion: 新方法有效克服了SNN在强化学习训练中的挑战，展现了更优的性能和资源效率，证明了其在神经形态计算的潜力。

Abstract: Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient
alternatives to traditional Deep Neural Networks (DNNs) for real-time control
systems. However, their training presents several challenges, particularly for
reinforcement learning (RL) tasks, due to the non-differentiable nature of
spike-based communication. In this work, we introduce what is, to our
knowledge, the first framework that employs Metropolis-Hastings (MH) sampling,
a Bayesian inference technique, to train SNNs for dynamical agent control in RL
environments without relying on gradient-based methods. Our approach
iteratively proposes and probabilistically accepts network parameter updates
based on accumulated reward signals, effectively circumventing the limitations
of backpropagation while enabling direct optimization on neuromorphic
platforms. We evaluated this framework on two standard control benchmarks:
AcroBot and CartPole. The results demonstrate that our MH-based approach
outperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL
approaches in terms of maximizing the accumulated reward while minimizing
network resources and training episodes.

</details>


### [240] [eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation](https://arxiv.org/abs/2507.09588)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.AI

TL;DR: 文章介绍了eSapiens，一种基于AIaaS的AI平台，强调数据安全、知识保留和高效工作，通过实验验证其在高风险领域的有效性。


<details>
  <summary>Details</summary>
Motivation: 在高敏感度领域中提供一款能确保数据安全、知识保留，同时提升工作效率的AI平台。

Method: 采用包括结构化文档摄取、混合向量检索以及无代码的LangChain编排，结合多种主流LLM模型如OpenAI、Claude等，并配备THOR代理来处理结构化SQL查询并生成可操作的见解。

Result: 实验表明：1）在法律语料库检索任务中，512 tokens的块大小实现了91.3%的Top-3准确率。2）生成测试中，与五个LLM比较，输出的一致性和事实对齐性提高了23%。

Conclusion: eSapiens在法律和金融等高风险领域中展现了可信赖且可审计的AI工作流能力。

Abstract: We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a
business-oriented trifecta: proprietary data, operational workflows, and any
major agnostic Large Language Model (LLM). eSapiens gives businesses full
control over their AI assets, keeping everything in-house for AI knowledge
retention and data security. eSapiens AI Agents (Sapiens) empower your team by
providing valuable insights and automating repetitive tasks, enabling them to
focus on high-impact work and drive better business outcomes.
  The system integrates structured document ingestion, hybrid vector retrieval,
and no-code orchestration via LangChain, and supports top LLMs including
OpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which
handles structured SQL-style queries and generates actionable insights over
enterprise databases.
  To evaluate the system, we conduct two experiments. First, a retrieval
benchmark on legal corpora reveals that a chunk size of 512 tokens yields the
highest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation
quality test using TRACe metrics across five LLMs shows that eSapiens delivers
more context-consistent outputs with up to a 23% improvement in factual
alignment.
  These results demonstrate the effectiveness of eSapiens in enabling
trustworthy, auditable AI workflows for high-stakes domains like legal and
finance.

</details>


### [241] [The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development](https://arxiv.org/abs/2507.09611)
*Jenis Winsta*

Main category: cs.AI

TL;DR: 人工智能的扩展带来了能源消耗、电子废物、不平等的计算资源以及网络安全系统能源隐患等问题。


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能在性能之外的环境和伦理影响，并寻求负责任的人工智能发展路径。

Method: 基于近期研究和机构报告，分析能源消耗、硬件更替、全球基础设施不平等及安全系统的能源需求，揭示系统性问题。

Result: 识别关键研究空白，提出采用可持续、透明与公平的发展实践的建议。

Conclusion: 人工智能的发展应与伦理责任和环境保护相结合，推动更具包容性和可持续性的技术未来。

Abstract: Artificial intelligence (AI) has made remarkable progress in recent years,
yet its rapid expansion brings overlooked environmental and ethical challenges.
This review explores four critical areas where AI's impact extends beyond
performance: energy consumption, electronic waste (e-waste), inequality in
compute access, and the hidden energy burden of cybersecurity systems. Drawing
from recent studies and institutional reports, the paper highlights systemic
issues such as high emissions from model training, rising hardware turnover,
global infrastructure disparities, and the energy demands of securing AI. By
connecting these concerns, the review contributes to Responsible AI discourse
by identifying key research gaps and advocating for sustainable, transparent,
and equitable development practices. Ultimately, it argues that AI's progress
must align with ethical responsibility and environmental stewardship to ensure
a more inclusive and sustainable technological future.

</details>


### [242] [Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge Graphs](https://arxiv.org/abs/2507.09617)
*Margherita Martorana,Francesca Urgese,Mark Adamik,Ilaria Tiddi*

Main category: cs.AI

TL;DR: 本文提出了一种神经符号框架，将多模态语言模型的感知能力与知识图谱和本体的结构化表示相结合，以支持机器人应用中的互操作性。


<details>
  <summary>Details</summary>
Motivation: 当前个人服务机器人因硬件和软件的专有性，难以在平台间灵活适配和扩展，而本体与知识图谱提供了解决互操作性问题的方法。

Method: 将机器人感知数据、本体与五种多模态模型集成，包括三种LLaMA和两种GPT模型，生成符合本体的知识图谱并评估其一致性和有效性。

Result: GPT-o1与LLaMA 4 Maverick在多次实验中表现最佳，但新版本模型并不总是保证更好的结果，突出了集成策略的重要性。

Conclusion: 神经符号框架有效支持了面向机器人应用的兼容性，但如何选择和集成模型在性能结果中占据关键地位。

Abstract: Personal service robots are deployed to support daily living in domestic
environments, particularly for elderly and individuals requiring assistance.
These robots must perceive complex and dynamic surroundings, understand tasks,
and execute context-appropriate actions. However, current systems rely on
proprietary, hard-coded solutions tied to specific hardware and software,
resulting in siloed implementations that are difficult to adapt and scale
across platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to
enable interoperability across systems, through structured and standardized
representations of knowledge and reasoning. However, symbolic systems such as
KGs and ontologies struggle with raw and noisy sensory input. In contrast,
multimodal language models are well suited for interpreting input such as
images and natural language, but often lack transparency, consistency, and
knowledge grounding. In this work, we propose a neurosymbolic framework that
combines the perceptual strengths of multimodal language models with the
structured representations provided by KGs and ontologies, with the aim of
supporting interoperability in robotic applications. Our approach generates
ontology-compliant KGs that can inform robot behavior in a platform-independent
manner. We evaluated this framework by integrating robot perception data,
ontologies, and five multimodal models (three LLaMA and two GPT models), using
different modes of neural-symbolic interaction. We assess the consistency and
effectiveness of the generated KGs across multiple runs and configurations, and
perform statistical analyzes to evaluate performance. Results show that GPT-o1
and LLaMA 4 Maverick consistently outperform other models. However, our
findings also indicate that newer models do not guarantee better results,
highlighting the critical role of the integration strategy in generating
ontology-compliant KGs.

</details>


### [243] [humancompatible.interconnect: Testing Properties of Repeated Uses of Interconnections of AI Systems](https://arxiv.org/abs/2507.09626)
*Rodion Nazarov,Anthony Quinn,Robert Shorten,Jakub Marecek*

Main category: cs.AI

TL;DR: 提出了一种基于PyTorch的工具包，用于在多智能体系统中实现闭环模型的公平性和鲁棒性保障。


<details>
  <summary>Details</summary>
Motivation: 设计一种工具以简化在AI多智能体系统中提供公平性和鲁棒性保障的过程。

Method: 基于PyTorch开发了开源工具包，结合随机控制技术对AI系统的互动和多次使用中的特性进行建模。

Result: 该工具实现了对闭环模型中公平性和鲁棒性需求的支持，并提供了理论上的先验保证。

Conclusion: 该工具降低了复杂性，为多智能体系统中的闭环模型提供了可靠的公平性保障。

Abstract: Artificial intelligence (AI) systems often interact with multiple agents. The
regulation of such AI systems often requires that {\em a priori\/} guarantees
of fairness and robustness be satisfied. With stochastic models of agents'
responses to the outputs of AI systems, such {\em a priori\/} guarantees
require non-trivial reasoning about the corresponding stochastic systems. Here,
we present an open-source PyTorch-based toolkit for the use of stochastic
control techniques in modelling interconnections of AI systems and properties
of their repeated uses. It models robustness and fairness desiderata in a
closed-loop fashion, and provides {\em a priori\/} guarantees for these
interconnections. The PyTorch-based toolkit removes much of the complexity
associated with the provision of fairness guarantees for closed-loop models of
multi-agent systems.

</details>


### [244] [Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey](https://arxiv.org/abs/2507.09662)
*Jason Zhu,Hongyu Li*

Main category: cs.AI

TL;DR: 文章讨论了在大规模推理模型中，如何通过简洁和自适应推理提高效率，避免冗长的推理链。


<details>
  <summary>Details</summary>
Motivation: 大规模推理模型生成冗长推理链的问题导致资源浪费和响应延迟，阻碍了其实际应用，因此需要探讨如何优化推理链长度并实现自适应推理。

Method: 对现有简洁和自适应推理技术进行了综述，涵盖方法、基准和未来的挑战。

Result: 全面总结了简介和自适应推理在提高大规模推理模型效率方面的进展。

Conclusion: 这种综述有助于研究者快速了解这一领域，同时激发关于模型优化的创新性想法以提升应用场景中的效率。

Abstract: Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have
demonstrated impressive performance on complex reasoning tasks like mathematics
and programming with long Chain-of-Thought (CoT) reasoning sequences
(slow-thinking), compared with traditional large language models
(fast-thinking). However, these reasoning models also face a huge challenge
that generating unnecessarily lengthy and redundant reasoning chains even for
trivial questions. This phenomenon leads to a significant waste of inference
resources, increases the response time for simple queries, and hinders the
practical application of LRMs in real-world products. To this end, it is
crucial to shorten lengthy reasoning chains and learn adaptive reasoning
between fast and slow thinking based on input difficulty. In this survey, we
provide a comprehensive overview of recent progress in concise and adaptive
thinking for efficient reasoning of LRMs, including methodologies, benchmarks,
and challenges for future exploration. We hope this survey can help researchers
quickly understand the landscape of this field and inspire novel adaptive
thinking ideas to facilitate better usage of LRMs.

</details>


### [245] [Causality-informed Anomaly Detection in Partially Observable Sensor Networks: Moving beyond Correlations](https://arxiv.org/abs/2507.09742)
*Xiaofeng Xiao,Bo Shen,Xubo Yue*

Main category: cs.AI

TL;DR: 提出了一种整合因果信息的深度Q网络（Causal DQ）方法，用于在部分可观测条件下的传感器布置，提高了异常检测速度与训练效率。


<details>
  <summary>Details</summary>
Motivation: 在AI制造中，传感器资源有限且无法全面布置，需要优化的策略实现对系统的部分观测，并尽快检测异常。

Method: 将因果信息整合到深度Q网络的各个训练阶段，避免传统因果分析的人工干预方法，同时达成更快的网络收敛和更小的误差界限。

Result: 经过训练的因果深度Q网络显著减少了不同设定下异常检测的时间，验证了方法在大规模数据监测环境中的有效性。

Conclusion: 该技术不仅适用于传感器布置任务，其基本原理还为工程应用中因果驱动的强化学习提供了新的可能。

Abstract: Nowadays, as AI-driven manufacturing becomes increasingly popular, the volume
of data streams requiring real-time monitoring continues to grow. However, due
to limited resources, it is impractical to place sensors at every location to
detect unexpected shifts. Therefore, it is necessary to develop an optimal
sensor placement strategy that enables partial observability of the system
while detecting anomalies as quickly as possible. Numerous approaches have been
proposed to address this challenge; however, most existing methods consider
only variable correlations and neglect a crucial factor: Causality. Moreover,
although a few techniques incorporate causal analysis, they rely on
interventions-artificially creating anomalies-to identify causal effects, which
is impractical and might lead to catastrophic losses. In this paper, we
introduce a causality-informed deep Q-network (Causal DQ) approach for
partially observable sensor placement in anomaly detection. By integrating
causal information at each stage of Q-network training, our method achieves
faster convergence and tighter theoretical error bounds. Furthermore, the
trained causal-informed Q-network significantly reduces the detection time for
anomalies under various settings, demonstrating its effectiveness for sensor
placement in large-scale, real-world data streams. Beyond the current
implementation, our technique's fundamental insights can be applied to various
reinforcement learning problems, opening up new possibilities for real-world
causality-informed machine learning methods in engineering applications.

</details>


### [246] [Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations](https://arxiv.org/abs/2507.09751)
*Bradley P. Allen,Prateek Chhikara,Thomas Macaulay Ferguson,Filip Ilievski,Paul Groth*

Main category: cs.AI

TL;DR: 本文提出了一种将大型语言模型(LLM)融入到矛盾容忍逻辑形式语义解释功能中的方法，以解决LLM输出逻辑一致性的问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在逻辑一致性方面的问题，同时利用其广覆盖的参数化知识进行形式推理。

Method: 将LLM直接集成到矛盾容忍逻辑形式语义的解释功能中，并通过多个事实性基准数据集测试其可行性。

Result: 实验表明，所提方法能够有效利用LLM的知识，确保逻辑框架的健全性与完整性。

Conclusion: 该方法为神经符号推理提供了一个理论框架，能够在保留逻辑性质的同时，利用LLM的知识。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but they exhibit problems with
logical consistency in the output they generate. How can we harness LLMs'
broad-coverage parametric knowledge in formal reasoning despite their
inconsistency? We present a method for directly integrating an LLM into the
interpretation function of the formal semantics for a paraconsistent logic. We
provide experimental evidence for the feasibility of the method by evaluating
the function using datasets created from several short-form factuality
benchmarks. Unlike prior work, our method offers a theoretical framework for
neuro-symbolic reasoning that leverages an LLM's knowledge while preserving the
underlying logic's soundness and completeness properties.

</details>


### [247] [Technical Requirements for Halting Dangerous AI Activities](https://arxiv.org/abs/2507.09801)
*Peter Barnett,Aaron Scher,David Abecassis*

Main category: cs.AI

TL;DR: 本文旨在探讨如何通过关键技术干预措施，协调停止危险AI活动的发展和部署。


<details>
  <summary>Details</summary>
Motivation: 应对AI系统的快速发展所带来的风险，如失控、误用、地缘政治不稳定和权力集中。

Method: 提出关键技术干预措施，探讨其在限制危险AI活动中的作用，并作为AI治理计划的技术基础。

Result: 描述了技术干预措施对协调停止危险AI活动的潜在贡献。

Conclusion: 利用技术手段为AI治理制定一个潜在计划，填补危险AI活动治理的技术空白。

Abstract: The rapid development of AI systems poses unprecedented risks, including loss
of control, misuse, geopolitical instability, and concentration of power. To
navigate these risks and avoid worst-case outcomes, governments may proactively
establish the capability for a coordinated halt on dangerous AI development and
deployment. In this paper, we outline key technical interventions that could
allow for a coordinated halt on dangerous AI activities. We discuss how these
interventions may contribute to restricting various dangerous AI activities,
and show how these interventions can form the technical foundation for
potential AI governance plans.

</details>


### [248] [Is Human-Written Data Enough? The Challenge of Teaching Reasoning to LLMs Without RL or Distillation](https://arxiv.org/abs/2507.09850)
*Wei Du,Branislav Kisacanin,George Armstrong,Shubham Toshniwal,Ivan Moshkov,Alexan Ayrapetyan,Sadegh Mahdavi,Dan Zhao,Shizhe Diao,Dragan Masulovic,Marius Stanean,Advaith Avadhanam,Max Wang,Ashmit Dutta,Shitij Govil,Sri Yanamandara,Mihir Tandon,Sriram Ananthakrishnan,Vedant Rathi,David Zhang,Joonseok Kang,Leon Luo,Titu Andreescu,Boris Ginsburg,Igor Gitman*

Main category: cs.AI

TL;DR: 通过轻微微调基模型，并利用少量高质量的长CoT示例，该研究显著提高了模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 探索使用少量高质量的长CoT示例来激发基模型的推理能力，以及分析如何改进推理蒸馏的过程。

Method: 通过提供20个来自理由模型的长CoT示例，对基模型进行轻微微调，并进一步探索从其他来源获取数据，包括非推理模型和人工注释数据，结合提示工程、多次编辑和结构化指导等方法。

Result: 微调整的基模型表现优于较大的模型版本，而其他尝试（如非推理模型和人工数据）未能达到同样的性能，显示某些专家级CoT的潜在的难以复制的特性。

Conclusion: 少量精心设计的人工长CoT示例能够有效激发基模型的推理能力，未来需进一步研究如何优化推理监督数据的小规模使用。

Abstract: Reasoning-capable language models achieve state-of-the-art performance in
diverse complex tasks by generating long, explicit Chain-of-Thought (CoT)
traces. While recent works show that base models can acquire such reasoning
traces via reinforcement learning or distillation from stronger models like
DeepSeek-R1, previous works demonstrate that even short CoT prompting without
fine-tuning is able to improve reasoning. We ask whether long CoT can be
induced in a base model using only prompting or minimal tuning. Using just 20
long CoT examples from the reasoning model \texttt{QwQ-32B-Preview}, we lightly
fine-tune the base model \texttt{Qwen2.5-32B}. The resulting model outperforms
the much larger \texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of
high-quality examples can unlock strong reasoning capabilities. We further
explore using CoT data from non-reasoning models and human annotators, enhanced
with prompt engineering, multi-pass editing, and structural guidance. However,
neither matches the performance of reasoning model traces, suggesting that
certain latent qualities of expert CoT are difficult to replicate. We analyze
key properties of reasoning data, such as problem difficulty, diversity, and
answer length, that influence reasoning distillation. While challenges remain,
we are optimistic that carefully curated human-written CoT, even in small
quantities, can activate reasoning behaviors in base models. We release our
human-authored dataset across refinement stages and invite further
investigation into what makes small-scale reasoning supervision so effective.

</details>


### [249] [Model-Grounded Symbolic Artificial Intelligence Systems Learning and Reasoning with Model-Grounded Symbolic Artificial Intelligence Systems](https://arxiv.org/abs/2507.09854)
*Aniruddha Chattopadhyay,Raj Dandekar,Kaushik Roy*

Main category: cs.AI

TL;DR: 本文探讨将指令微调的大语言模型重新解读为以自然语言为符号层的符号AI系统，结合现代神经网络表征，开发新型学习与推理方法，并对其有效性进行了初步评估。


<details>
  <summary>Details</summary>
Motivation: 神经符号人工智能结合神经网络与经典符号AI的优势，但现有方法整合方式多样且未探索大语言模型的自然语言符号潜力。

Method: 作者提出将自然语言作为符号层，通过模型的内部表征空间实现符号AI的模型拓展，并研究开发新型学习与推理方法，同时对其在不同复杂度演绎推理程序中的表现进行初步评估。

Result: 初步实验表明，新方法在改进学习效率和推理可靠性方面具有一定的效果。

Conclusion: 本文展示了基于大语言模型的神经符号AI系统在学习效率和推理上的潜力，为未来方法的发展开辟了方向。

Abstract: Neurosymbolic artificial intelligence (AI) systems combine neural network and
classical symbolic AI mechanisms to exploit the complementary strengths of
large scale, generalizable learning and robust, verifiable reasoning. Numerous
classifications of neurosymbolic AI illustrate how these two components can be
integrated in distinctly different ways. In this work, we propose
reinterpreting instruction tuned large language models as model grounded
symbolic AI systems where natural language serves as the symbolic layer and
grounding is achieved through the models internal representation space. Within
this framework, we investigate and develop novel learning and reasoning
approaches that preserve structural similarities to traditional learning and
reasoning paradigms. Preliminary evaluations across axiomatic deductive
reasoning procedures of varying complexity provide insights into the
effectiveness of our approach in improving learning efficiency and reasoning
reliability.

</details>


### [250] [VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains](https://arxiv.org/abs/2507.09884)
*Xuzhao Li,Xuchen Li,Shiyu Hu,Yongzhen Guo,Wentao Zhang*

Main category: cs.AI

TL;DR: 本文提出了VerifyBench，一个全面的跨领域基准，用于系统性评估验证器的性能，特别是针对大规模语言模型（LLMs）中的强化学习任务。


<details>
  <summary>Details</summary>
Motivation: 当前验证器在处理复杂、灵活的LLM生成的回应时表现不足，专用验证器缺乏灵活性，而通用模型则欠缺一致性，全面的验证器性能评估研究仍是空白。

Method: 构建了包含数学、物理、化学和生物4000个专家级问题的基准数据集VerifyBench，以及参考答案和多样化的回答。提出了四维实验框架评估各种验证器的性能。

Result: 研究发现专用验证器尽管准确率领先，但召回率较低；而通用模型有更高的包容性，但精度不稳定。此外，验证器对输入结构敏感，跨领域泛化能力有限。

Conclusion: VerifyBench为未来强化学习与验证性奖励（RLVR）的开发提供了可靠的评估工具，并且揭示了现有验证器技术的关键瓶颈，为进一步优化验证器设计提供了参考。

Abstract: Large language models (LLMs) increasingly rely on reinforcement learning (RL)
to enhance their reasoning capabilities through feedback. A critical challenge
is verifying the consistency of model-generated responses and reference
answers, since these responses are often lengthy, diverse, and nuanced.
Rule-based verifiers struggle with complexity, prompting the use of model-based
verifiers. However, specialized verifiers lack flexibility, while general LLM
judges can be inconsistent. Existing research primarily focuses on building
better verifiers, yet a systematic evaluation of different types of verifiers'
performance across domains remains lacking, severely constraining the reliable
development of Reinforcement Learning with Verifiable Reward (RLVR). To address
this, we propose VerifyBench--a cross-domain comprehensive benchmark for
systematically evaluating verifiers. We construct 4,000 expert-level questions
covering mathematics, physics, chemistry, and biology. Each question is
equipped with reference answers and diverse responses. The reliability of the
evaluation is ensured through a rigorous annotation process conducted by a
multidisciplinary expert team. We design a four-dimensional experimental
framework to comprehensively compare the performance boundaries of specialized
verifiers and general LLMs under combined conditions of extracted answers vs.
complete responses, and short vs. long outputs. Our evaluation uncovers
fundamental trade-offs in verifiers: while specialized verifiers achieve
leading accuracy, they exhibit deficiencies in recall; general models show
stronger inclusivity but unstable precision. More importantly, we discover
verifiers' high sensitivity to input structure and inherent limitations in
cross-domain generalization, providing critical insights into the bottlenecks
of current verifier technology.

</details>


### [251] [DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models](https://arxiv.org/abs/2507.09955)
*Luolin Xiong,Haofen Wang,Xi Chen,Lu Sheng,Yun Xiong,Jingping Liu,Yanghua Xiao,Huajun Chen,Qing-Long Han,Yang Tang*

Main category: cs.AI

TL;DR: DeepSeek公司发布了性能优异且低成本的V3和R1系列模型，本文介绍了它们的算法创新与工程突破，同时分析了它们对AI领域的影响和未来趋势。


<details>
  <summary>Details</summary>
Motivation: 探讨DeepSeek模型的创新优势及其对AI行业的影响，同时审视大规模AI模型的未来发展趋势。

Method: 提出并分析了Multi-head Latent Attention (MLA)、Mixture-of-Experts (MoE)、Multi-Token Prediction (MTP)等算法；探讨了LLM扩展、训练、推理和系统优化架构的工程突破。

Result: DeepSeek通过新算法和工程优化展示了其模型的高性能和竞争力，并且对现有主流LLM形成了挑战。

Conclusion: DeepSeek的创新为大规模AI模型的技术与工程发展提供了重要启示，并为未来相关领域的研究指明方向。

Abstract: DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their
V3 and R1 series models, which attracted global attention due to their low
cost, high performance, and open-source advantages. This paper begins by
reviewing the evolution of large AI models focusing on paradigm shifts, the
mainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm.
Subsequently, the paper highlights novel algorithms introduced by DeepSeek,
including Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE),
Multi-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO).
The paper then explores DeepSeek engineering breakthroughs in LLM scaling,
training, inference, and system-level optimization architecture. Moreover, the
impact of DeepSeek models on the competitive AI landscape is analyzed,
comparing them to mainstream LLMs across various fields. Finally, the paper
reflects on the insights gained from DeepSeek innovations and discusses future
trends in the technical and engineering development of large AI models,
particularly in data, training, and reasoning.

</details>


### [252] [Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient](https://arxiv.org/abs/2507.09989)
*Xiaoyang Yu,Youfang Lin,Shuo Wang,Sheng Han*

Main category: cs.AI

TL;DR: 提出OMDPG算法解决异质多智能体强化学习的单调改善与部分参数共享冲突问题，实验验证了其优于其他主流算法。


<details>
  <summary>Details</summary>
Motivation: 异质多智能体强化学习需单调改进以提升性能，但部分参数共享会导致策略更新基线漂移问题。

Method: 通过引入OMDPG算法：使用最优边际Q函数替代顺序Q计算，提出广义Q评论家，优化不确定性约束损失，并实现集中评论家分组执行器架构。

Result: 在SMAC和MAMuJoCo环境中，OMDPG算法性能优于多种主流算法。

Conclusion: OMDPG平衡了单调改善与部分参数共享，是异质多智能体强化学习中的有效解决方案。

Abstract: In heterogeneous multi-agent reinforcement learning (MARL), achieving
monotonic improvement plays a pivotal role in enhancing performance. The HAPPO
algorithm proposes a feasible solution by introducing a sequential update
scheme, which requires independent learning with No Parameter-sharing (NoPS).
However, heterogeneous MARL generally requires Partial Parameter-sharing
(ParPS) based on agent grouping to achieve high cooperative performance. Our
experiments prove that directly combining ParPS with the sequential update
scheme leads to the policy updating baseline drift problem, thereby failing to
achieve improvement. To solve the conflict between monotonic improvement and
ParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG)
algorithm. First, we replace the sequentially computed $Q_{\psi}^s(s,a_{1:i})$
with the Optimal Marginal Q (OMQ) function $\phi_{\psi}^*(s,a_{1:i})$ derived
from Q-functions. This maintains MAAD's monotonic improvement while eliminating
the conflict through optimal joint action sequences instead of sequential
policy ratio calculations. Second, we introduce the Generalized Q Critic (GQC)
as the critic function, employing pessimistic uncertainty-constrained loss to
optimize different Q-value estimations. This provides the required Q-values for
OMQ computation and stable baselines for actor updates. Finally, we implement a
Centralized Critic Grouped Actor (CCGA) architecture that simultaneously
achieves ParPS in local policy networks and accurate global Q-function
computation. Experimental results in SMAC and MAMuJoCo environments demonstrate
that OMDPG outperforms various state-of-the-art MARL baselines.

</details>


### [253] [On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model](https://arxiv.org/abs/2507.10000)
*Mark Burgess*

Main category: cs.AI

TL;DR: 该论文利用Promise Theory的语义时空模型，通过尺度分离和时空一致性分析数据的潜在意图，提供了一种具有低计算成本的解释方法。


<details>
  <summary>Details</summary>
Motivation: 通过对意图和语境的分析，提供一种低计算成本的方法来解析数据的潜在意图，降低对大规模训练和计算资源的依赖。

Method: 采用Promise Theory模型，分析数据的多尺度异常，利用时空一致性将内容分为意图部分和环境上下文部分。

Result: 该方法提供了一种低计算成本的意图解释机制，并适用于基本有机体，无需大规模人工智能模型的支持。

Conclusion: 此方法具备低计算成本且实用性强的特点，可作为解析数据潜在意图的有效手段，尤其适合资源有限的场景。

Abstract: Since Searle's work deconstructing intent and intentionality in the realm of
philosophy, the practical meaning of intent has received little attention in
science and technology. Intentionality and context are both central to the
scope of Promise Theory's model of Semantic Spacetime, used as an effective
Tiny Language Model. One can identify themes and concepts from a text, on a low
level (without knowledge of the specific language) by using process coherence
as a guide. Any agent process can assess superficially a degree of latent
`intentionality' in data by looking for anomalous multi-scale anomalies and
assessing the work done to form them. Scale separation can be used to sort
parts into `intended' content and `ambient context', using the spacetime
coherence as a measure. This offers an elementary but pragmatic interpretation
of latent intentionality for very low computational cost, and without reference
to extensive training or reasoning capabilities. The process is well within the
reach of basic organisms as it does not require large scale artificial
probabilistic batch processing. The level of concept formation depends,
however, on the memory capacity of the agent.

</details>


### [254] [Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.10007)
*Zijun Chen,Wenbo Hu,Richang Hong*

Main category: cs.AI

TL;DR: 本文提出了一种新方法，通过利用模型内在的真实性编码，校准链式思维（CoT）推理的准确性。实验表明，该方法在多种任务和模型设置中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 链式思维推理能力已在LLMs和MLLMs中展现突出性能，但其可靠性常因中间步骤中的误差累积受到影响，需要提高推理的准确性和可靠性。

Method: 研究发现特定注意力头的激活可以反映CoT推理步骤的真实性，据此训练信心预测器，通过动态选择最可信的推理路径，提高推理性能。

Result: 实验表明，该方法在数学、符号和常识推理任务上显著超越现有基线方法，在单模态和多模态场景中均表现优异。

Conclusion: 该方法为提升链式思维推理的可靠性提供了一条新思路，具有广泛的应用潜力。

Abstract: Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning
capabilities in both large language models (LLMs) and multimodal large language
models (MLLMs). However, its reliability is often undermined by the
accumulation of errors in intermediate steps. This paper introduces an novel
approach to calibrate the CoT reasoning accuracy by leveraging the model's
intrinsic veracity encoding. We discover that specific attention head
activations reliably reflect the truthfulness of reasoning steps in CoT. Based
on this insight, we train a confidence predictor to evaluate the correctness of
each reasoning step using these truthfulness-sensitive activations, dynamically
selecting the most plausible reasoning path via beam search. Experimental
results demonstrate that our method significantly outperforms the
state-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and
Self-Evaluation Guided Beam Search) across the mathematical, symbolic, and
commonsense reasoning tasks, exhibiting superior accuracy and reliability in
both unimodal and multimodal settings. We further validate the approach on
large reasoning models, confirming its applicability to specialized reasoning
models. Additionally, we explore the role of the model's self-correction
ability in CoT reasoning. This work provides a novel reliability improvement
path for CoT reasoning with broad application potential.

</details>


### [255] [Automating SPARQL Query Translations between DBpedia and Wikidata](https://arxiv.org/abs/2507.10045)
*Malte Christian Bartels,Debayan Banerjee,Ricardo Usbeck*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型(LLMs)是否可以自动实现SPARQL在不同知识图谱(KG)模式之间的翻译。


<details>
  <summary>Details</summary>
Motivation: 解决当前知识图谱互操作性研究中的空白问题，通过评价LLMs在SPARQL-到-SPARQL翻译中的表现。

Method: 研究选择了三个LLMs模型，并测试了它们的零样本、少量样本及连锁思维变体，通过比较生成的结果与标准答案，分析翻译表现和错误分类。

Result: 模型的翻译性能因模型和提示策略而异，且从Wikidata到DBpedia的翻译表现优于从DBpedia到Wikidata的方向。

Conclusion: LLMs在知识图谱互操作中有一定潜力，但其性能因多种因素限制，有待进一步研究和优化。

Abstract: This paper investigates whether state-of-the-art Large Language Models (LLMs)
can automatically translate SPARQL between popular Knowledge Graph (KG)
schemas. We focus on translations between the DBpedia and Wikidata KG, and
later on DBLP and OpenAlex KG. This study addresses a notable gap in KG
interoperability research by rigorously evaluating LLM performance on
SPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first
align 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100
DBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic
KGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and
Mistral-Large-Instruct-2407 are selected based on their sizes and architectures
and tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs
were compared with gold answers, and resulting errors were categorized. We find
that the performance varies markedly across models and prompting strategies,
and that translations for Wikidata to DBpedia work far better than translations
for DBpedia to Wikidata.

</details>


### [256] [On Gradual Semantics for Assumption-Based Argumentation](https://arxiv.org/abs/2507.10076)
*Anna Rapberger,Fabrizio Russo,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 本文提出了一种新的逐步语义，用于为基于假设的论证框架（ABA）中的假设分配辩论力量，扩展了当前逐步语义的应用范围。


<details>
  <summary>Details</summary>
Motivation: 当前逐步语义主要研究抽象和定量的两性论证框架，以及某些结构化论证，但尚未在ABA中得到研究，而ABA在某些应用中具有重要性，因此需要填补这一研究空白。

Method: 使用基于两性集合的论证框架作为ABA框架的抽象模型，并推广现有的模块化逐步语义，设计了一系列新的逐步ABA语义，与基于论据的方法进行对比实验。

Result: 提出的逐步ABA语义符合均衡性和单调性等属性，并通过实验验证了逐步ABA语义的收敛性及其与基于论据方法的比较结果。

Conclusion: 本文为ABA框架提供了一种新的逐步语义，扩展了结构化论证的研究范围，并在实验中表现出了优良的性质，为ABA在实际应用中的进一步发展奠定了基础。

Abstract: In computational argumentation, gradual semantics are fine-grained
alternatives to extension-based and labelling-based semantics . They ascribe a
dialectical strength to (components of) arguments sanctioning their degree of
acceptability. Several gradual semantics have been studied for abstract,
bipolar and quantitative bipolar argumentation frameworks (QBAFs), as well as,
to a lesser extent, for some forms of structured argumentation. However, this
has not been the case for assumption-based argumentation (ABA), despite it
being a popular form of structured argumentation with several applications
where gradual semantics could be useful. In this paper, we fill this gap and
propose a family of novel gradual semantics for equipping assumptions, which
are the core components in ABA frameworks, with dialectical strengths. To do
so, we use bipolar set-based argumentation frameworks as an abstraction of
(potentially non-flat) ABA frameworks and generalise state-of-the-art modular
gradual semantics for QBAFs. We show that our gradual ABA semantics satisfy
suitable adaptations of desirable properties of gradual QBAF semantics, such as
balance and monotonicity. We also explore an argument-based approach that
leverages established QBAF modular semantics directly, and use it as baseline.
Finally, we conduct experiments with synthetic ABA frameworks to compare our
gradual ABA semantics with its argument-based counterpart and assess
convergence.

</details>


### [257] [BlueGlass: A Framework for Composite AI Safety](https://arxiv.org/abs/2507.10106)
*Harshal Nandigramwar,Syed Qutub,Kay-Ulrich Scholl*

Main category: cs.AI

TL;DR: 本文介绍了一个名为BlueGlass的框架，用于整合多种现有AI安全工具，以提升人工智能系统的整体安全性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统能力与使用场景的增加，确保其安全性变得至关重要。然而，现有安全工具往往无法单独全面解决问题，因此需要集成方法来保障安全。

Method: 提出BlueGlass框架，该框架通过统一基础设施整合多种安全工具，并演示了在视觉-语言模型中进行安全分析的应用，采用分布评测、基于探针的分析和稀疏自编码器等方法揭示模型特点。

Result: 验证了框架在视觉-语言模型的安全性分析任务中的效用，揭示了模型在分布间的性能权衡及潜在失效模式，同时发现层动态中的共享分层学习现象，并通过稀疏自编码器识别出可解释概念。

Conclusion: BlueGlass框架为构建更稳健和可靠的AI系统提供了基础性工具与研究结果，推动了AI安全性的综合发展。

Abstract: As AI systems become increasingly capable and ubiquitous, ensuring the safety
of these systems is critical. However, existing safety tools often target
different aspects of model safety and cannot provide full assurance in
isolation, highlighting a need for integrated and composite methodologies. This
paper introduces BlueGlass, a framework designed to facilitate composite AI
safety workflows by providing a unified infrastructure enabling the integration
and composition of diverse safety tools that operate across model internals and
outputs. Furthermore, to demonstrate the utility of this framework, we present
three safety-oriented analyses on vision-language models for the task of object
detection: (1) distributional evaluation, revealing performance trade-offs and
potential failure modes across distributions; (2) probe-based analysis of layer
dynamics highlighting shared hierarchical learning via phase transition; and
(3) sparse autoencoders identifying interpretable concepts. More broadly, this
work contributes foundational infrastructure and findings for building more
robust and reliable AI systems.

</details>


### [258] [Analysis of AI Techniques for Orchestrating Edge-Cloud Application Migration](https://arxiv.org/abs/2507.10119)
*Sadig Gojayev,Ahmad Anaqreh,Carolina Fortuna*

Main category: cs.AI

TL;DR: 本文探讨应用迁移在边缘云系统中的作用，并比较了多个AI规划和RL方法对迁移问题的解决效果。


<details>
  <summary>Details</summary>
Motivation: 促进优质服务和成本效益的服务交付，因此研究如何更智能地实现应用迁移。

Method: 将应用迁移建模为汉诺塔问题，并使用MDP框架分析和比较AI规划与强化学习方法。

Result: 提出了一种基于状态空间定义的新分类方法，并从该角度详细分析了模型。

Conclusion: 为在现代计算环境中优化应用迁移提供了深入理解和潜在技术支持。

Abstract: Application migration in edge-cloud system enables high QoS and cost
effective service delivery. However, automatically orchestrating such migration
is typically solved with heuristic approaches. Starting from the Markov
Decision Process (MDP), in this paper, we identify, analyze and compare
selected state-of-the-art Artificial Intelligence (AI) planning and
Reinforcement Learning (RL) approaches for solving the class of edge-cloud
application migration problems that can be modeled as Towers of Hanoi (ToH)
problems. We introduce a new classification based on state space definition and
analyze the compared models also through this lense. The aim is to understand
available techniques capable of orchestrating such application migration in
emerging computing continuum environments.

</details>


### [259] [Could you be wrong: Debiasing LLMs using a metacognitive prompt for improving human decision making](https://arxiv.org/abs/2507.10124)
*Thomas T. Hills*

Main category: cs.AI

TL;DR: 本文探讨了一种新颖的提示策略“你可能是错的吗？”用于揭示LLM的潜在偏见与错误，并提升其元认知反思能力


<details>
  <summary>Details</summary>
Motivation: LLM 存在潜在偏见与错误，需要采用可持续的通用去偏策略，而不是仅针对当前模型的特定优化。

Method: 借鉴人类决策领域中的元认知提示策略，通过提出“你可能是错的吗？”提醒LLM反思自己的回答，并生成额外的信息和反证。

Result: 该提示能够引导LLM揭示其潜在偏见、不一致的证据，并改善其元认知能力。

Conclusion: 研究表明，人类心理学中的提示方式可被有效应用于LLM的提示工程中，为揭示和纠正偏见提供了新思路。

Abstract: Identifying bias in LLMs is ongoing. Because they are still in development,
what is true today may be false tomorrow. We therefore need general strategies
for debiasing that will outlive current models. Strategies developed for
debiasing human decision making offer one promising approach as they
incorporate an LLM-style prompt intervention designed to bring latent knowledge
into awareness during decision making. LLMs trained on vast amounts of
information contain information about potential biases, counter-arguments, and
contradictory evidence, but that information may only be brought to bear if
prompted. Metacognitive prompts developed in the human decision making
literature are designed to achieve this, and as I demonstrate here, they show
promise with LLMs. The prompt I focus on here is "could you be wrong?"
Following an LLM response, this prompt leads LLMs to produce additional
information, including why they answered as they did, errors, biases,
contradictory evidence, and alternatives, none of which were apparent in their
initial response. Indeed, this metaknowledge often reveals that how LLMs and
users interpret prompts are not aligned. Here I demonstrate this prompt using a
set of questions taken from recent articles about LLM biases, including
implicit discriminatory biases and failures of metacognition. "Could you be
wrong" prompts the LLM to identify its own biases and produce cogent
metacognitive reflection. I also present another example involving convincing
but incomplete information, which is readily corrected by the metacognitive
prompt. In sum, this work argues that human psychology offers a new avenue for
prompt engineering, leveraging a long history of effective prompt-based
improvements to human decision making.

</details>


### [260] [FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring](https://arxiv.org/abs/2507.10134)
*Yousef Emami,Hao Zhou,Miguel Gutierrez Gaitan,Kai Li,Luis Almeida*

Main category: cs.AI

TL;DR: 本文提出了一种基于大语言模型（LLM）并增强了上下文学习能力的在线飞行资源分配方案（FRSICL），用于优化无人机的飞行控制与数据收集，在野火监测中通过减少信息时滞（AoI）提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度强化学习（DRL）的优化方法在无人机辅助的野火监测中存在抽样效率低、现实模拟差异大以及训练复杂的问题，难以适应时间敏感型任务的需求。

Method: 提出一种称为FRSICL的在线飞行资源分配方案，利用自然语言任务描述和环境反馈，结合大语言模型的上下文学习能力，实时优化无人机的飞行控制及数据传输调度，有效减少来自地面传感器的平均信息时滞（AoI）。

Result: 该方法通过模拟验证与现有的近邻基准法与近端策略优化（PPO）方法相比，表现出了更为明显的效果优势。

Conclusion: FRSICL通过引入自然语言处理和实时决策机制克服了DRL的局限，为支持时间紧迫的无人机监测任务提供了一种新颖高效的解决方案。

Abstract: Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in
wildfire monitoring, where early detection minimizes environmental impact. In
UAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor
transmission scheduling and velocity is critical for minimizing Age of
Information (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has
been used for such optimization; however, its limitations such as low sampling
efficiency, simulation-to-reality gaps, and complex training render it
unsuitable for time-critical applications like wildfire monitoring. This paper
introduces a new online Flight Resource Allocation scheme based on LLM-Enabled
In-Context Learning (FRSICL) to jointly optimize the UAV's flight control and
data collection schedule along the trajectory in real time, thereby
asymptotically minimizing the average AoI across ground sensors. In contrast to
DRL, FRSICL generates data collection schedules and controls velocity using
natural language task descriptions and feedback from the environment, enabling
dynamic decision-making without extensive retraining. Simulation results
confirm the effectiveness of the proposed FRSICL compared to Proximal Policy
Optimization (PPO) and Nearest-Neighbor baselines.

</details>


### [261] [Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review](https://arxiv.org/abs/2507.10142)
*Siyi Hu,Mohamad A Hady,Jianglin Qiao,Jimmy Cao,Mahardhika Pratama,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: 本文关注多智能体强化学习(MARL)在复杂动态环境下的适应性问题，提出了一种由学习适应性、策略适应性和场景驱动适应性构成的框架，以提升算法在真实世界多智能体系统中的表现能力以及评价方法。


<details>
  <summary>Details</summary>
Motivation: 当前MARL在模拟环境中表现出色，但在真实复杂环境的应用受限，主要因为多变的环境条件如动态任务、波动的代理数量和不一致的执行情况，需探索提升其适应能力。

Method: 引入一种适应性评价框架，围绕学习适应性、策略适应性和场景驱动适应性三个维度进行系统性评价，旨在改进评估现有算法在动态环境的可靠性及实用性。

Result: 框架为MARL提供了适应复杂动态任务的统一视角，同时通过明确划分三个适应性维度，促进算法改进及其在真实场景中的应用。

Conclusion: 通过提出适应性框架，本文期望推动MARL算法设计与评估方法的变革，使其更适应现实多智能体系统的动态性需求。

Abstract: Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in
coordinating multiple agents across simulated benchmarks and constrained
scenarios. However, its deployment in real-world multi-agent systems (MAS)
remains limited, primarily due to the complex and dynamic nature of such
environments. These challenges arise from multiple interacting sources of
variability, including fluctuating agent populations, evolving task goals, and
inconsistent execution conditions. Together, these factors demand that MARL
algorithms remain effective under continuously changing system configurations
and operational demands. To better capture and assess this capacity for
adjustment, we introduce the concept of \textit{adaptability} as a unified and
practically grounded lens through which to evaluate the reliability of MARL
algorithms under shifting conditions, broadly referring to any changes in the
environment dynamics that may occur during learning or execution. Centred on
the notion of adaptability, we propose a structured framework comprising three
key dimensions: learning adaptability, policy adaptability, and scenario-driven
adaptability. By adopting this adaptability perspective, we aim to support more
principled assessments of MARL performance beyond narrowly defined benchmarks.
Ultimately, this survey contributes to the development of algorithms that are
better suited for deployment in dynamic, real-world multi-agent systems.

</details>


### [262] [Introducing the Swiss Food Knowledge Graph: AI for Context-Aware Nutrition Recommendation](https://arxiv.org/abs/2507.10156)
*Lubnaa Abdur Rahman,Ioannis Papathanail,Stavroula Mougiakakou*

Main category: cs.AI

TL;DR: 本论文提出瑞士食品知识图（SwissFKG），整合食谱、成分、营养数据及饮食限制等信息，解决当前营养评估系统对非视觉因素的忽视。


<details>
  <summary>Details</summary>
Motivation: 现有饮食评估系统忽略食谱中特定成分替代和个性化饮食需求，且瑞士关于食品的相关信息分散，没有统一的集成资源。

Method: 提出SwissFKG，结合LLM支持的信息增益流程，并首次对四种小于70B参数的LLM进行食物知识增强评估。同时开发Graph-RAG应用，用于处理和回答用户相关的营养问题。

Result: 实验表明LLM可以有效为知识图补充信息，SwissFKG能够提供包括过敏原信息在内的成分级建议，并符合营养指南，可用于用户特定营养查询。

Conclusion: SwissFKG为下一代饮食评估工具奠定了基础，在结合视觉、上下文及文化维度的饮食评估中具有重要意义。

Abstract: AI has driven significant progress in the nutrition field, especially through
multimedia-based automatic dietary assessment. However, existing automatic
dietary assessment systems often overlook critical non-visual factors, such as
recipe-specific ingredient substitutions that can significantly alter
nutritional content, and rarely account for individual dietary needs, including
allergies, restrictions, cultural practices, and personal preferences. In
Switzerland, while food-related information is available, it remains
fragmented, and no centralized repository currently integrates all relevant
nutrition-related aspects within a Swiss context. To bridge this divide, we
introduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our
best knowledge, to unite recipes, ingredients, and their substitutions with
nutrient data, dietary restrictions, allergen information, and national
nutrition guidelines under one graph. We establish a LLM-powered enrichment
pipeline for populating the graph, whereby we further present the first
benchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge
augmentation. Our results demonstrate that LLMs can effectively enrich the
graph with relevant nutritional information. Our SwissFKG goes beyond recipe
recommendations by offering ingredient-level information such as allergen and
dietary restriction information, and guidance aligned with nutritional
guidelines. Moreover, we implement a Graph-RAG application to showcase how the
SwissFKG's rich natural-language data structure can help LLM answer
user-specific nutrition queries, and we evaluate LLM-embedding pairings by
comparing user-query responses against predefined expected answers. As such,
our work lays the foundation for the next generation of dietary assessment
tools that blend visual, contextual, and cultural dimensions of eating.

</details>


### [263] [Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?](https://arxiv.org/abs/2507.10174)
*Yumi Omori,Zixuan Dong,Keith Ross*

Main category: cs.AI

TL;DR: 本文比较了Transformer架构的Decision Transformer (DT)与多层感知机（MLP）架构的离线强化学习算法，发现MLP的Filtered Behavior Cloning (FBC)在稀疏奖励环境下表现更优。


<details>
  <summary>Details</summary>
Motivation: 近年来，Transformer在强化学习中的应用逐渐增加，DT作为一种基于回报条件的策略建模方法在离线强化学习中备受关注。作者希望对比DT与传统的MLP算法表现，尤其在稀疏奖励及低质量数据情境下。

Method: 本文通过在机器人操作任务（Robomimic）和移动测试基准集（D4RL）上的实验，采用一种被称为Filtered Behavior Cloning（FBC）的简单方法，仅筛选高表现轨迹后进行普通行为模仿训练，并与DT进行比较。

Result: 实验显示FBC在稀疏奖励环境下具有竞争力且表现优于DT，同时FBC对数据需求更少且计算效率更高。

Conclusion: 研究表明，DT并不适合用于稀疏奖励和密集奖励环境。作者进一步提出疑问，DT是否在某些情况下能够真正优于其他方法。

Abstract: In recent years, extensive work has explored the application of the
Transformer architecture to reinforcement learning problems. Among these,
Decision Transformer (DT) has gained particular attention in the context of
offline reinforcement learning due to its ability to frame return-conditioned
policy learning as a sequence modeling task. Most recently, Bhargava et al.
(2024) provided a systematic comparison of DT with more conventional MLP-based
offline RL algorithms, including Behavior Cloning (BC) and Conservative
Q-Learning (CQL), and claimed that DT exhibits superior performance in
sparse-reward and low-quality data settings.
  In this paper, through experimentation on robotic manipulation tasks
(Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered
Behavior Cloning (FBC) achieves competitive or superior performance compared to
DT in sparse-reward environments. FBC simply filters out low-performing
trajectories from the dataset and then performs ordinary behavior cloning on
the filtered dataset. FBC is not only very straightforward, but it also
requires less training data and is computationally more efficient. The results
therefore suggest that DT is not preferable for sparse-reward environments.
From prior work, arguably, DT is also not preferable for dense-reward
environments. Thus, we pose the question: Is DT ever preferable?

</details>


### [264] [Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks](https://arxiv.org/abs/2507.10208)
*Hamzah Ziadeh,Hendrik Knoche*

Main category: cs.AI

TL;DR: 本论文探讨了可解释人工智能（XAI）的分类与比较，通过提出'what, why, who'三维框架来分析目前研究的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的XAI研究存在任务描述不充分、缺乏情境相关性以及针对用户测试不足等问题，导致结论矛盾和设计建议不明确。

Method: 整合视觉分析、认知科学和仪表板设计领域的知识，提出基于'what, why, who'的框架用于分类和比较XAI研究，制定了研究和报告指南。

Result: 通过分析XAI研究中的用户领域、AI及数据分析经验，能更清楚地发现领域内的研究差距和方法普适性问题。

Conclusion: 提出的框架和指南能帮助研究人员和设计师甄别相关研究，识别研究的空缺点及应对XAI设计中出现的矛盾结果。

Abstract: Research into explainable artificial intelligence (XAI) for data analysis
tasks suffer from a large number of contradictions and lack of concrete design
recommendations stemming from gaps in understanding the tasks that require AI
assistance. In this paper, we drew on multiple fields such as visual analytics,
cognition, and dashboard design to propose a method for categorising and
comparing XAI studies under three dimensions: what, why, and who. We identified
the main problems as: inadequate descriptions of tasks, context-free studies,
and insufficient testing with target users. We propose that studies should
specifically report on their users' domain, AI, and data analysis expertise to
illustrate the generalisability of their findings. We also propose study
guidelines for designing and reporting XAI tasks to improve the XAI community's
ability to parse the rapidly growing field. We hope that our contribution can
help researchers and designers better identify which studies are most relevant
to their work, what gaps exist in the research, and how to handle contradictory
results regarding XAI design.

</details>


### [265] [Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence](https://arxiv.org/abs/2507.10281)
*Jiaming Tian,Liyao Li,Wentao Ye,Haobo Wang,Lingxin Wang,Lihua Yu,Zujie Ren,Gang Chen,Junbo Zhao*

Main category: cs.AI

TL;DR: 研究聚焦表格任务，对LLM驱动的表格代理进行综述，提出五项核心能力并分析当前方法的优势与不足。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界中表格任务的噪声、结构异质性及语义复杂性问题，弥补学术数据集研究的不足。

Method: 提出五项核心能力（表格结构理解、语义理解、检索与压缩、可执行推理及跨域泛化）分析方法，还特别探讨了Text-to-SQL代理的性能差距。

Result: 发现开源模型在真实场景中表现与学术基准间存在显著差距，并总结改进建议。

Conclusion: 提出了对LLM基于表格代理的实际应用改进建议，以增强其鲁棒性、泛化能力及效率。

Abstract: Tables are fundamental in domains such as finance, healthcare, and public
administration, yet real-world table tasks often involve noise, structural
heterogeneity, and semantic complexity--issues underexplored in existing
research that primarily targets clean academic datasets. This survey focuses on
LLM-based Table Agents, which aim to automate table-centric workflows by
integrating preprocessing, reasoning, and domain adaptation. We define five
core competencies--C1: Table Structure Understanding, C2: Table and Query
Semantic Understanding, C3: Table Retrieval and Compression, C4: Executable
Reasoning with Traceability, and C5: Cross-Domain Generalization--to analyze
and compare current approaches. In addition, a detailed examination of the
Text-to-SQL Agent reveals a performance gap between academic benchmarks and
real-world scenarios, especially for open-source models. Finally, we provide
actionable insights to improve the robustness, generalization, and efficiency
of LLM-based Table Agents in practical settings.

</details>


### [266] [Instance space analysis of the capacitated vehicle routing problem](https://arxiv.org/abs/2507.10397)
*Alessandra M. M. M. Gouvêa,Nuno Paulos,Eduardo Uchoa e Mariá C. V. Nascimento*

Main category: cs.AI

TL;DR: 本研究利用实例空间分析（ISA）和DIMACS第12次实施挑战中的数据集，研究实例特征和元启发式算法表现之间的关系，提出了一个新的分析方法。


<details>
  <summary>Details</summary>
Motivation: 旨在探索实例特征与元启发算法性能之间的复杂关系，并为CVRP领域提供新的分析手段。

Method: 通过结合实例空间分析（ISA）方法与DIMACS 12th Implementation Challenge中的数据集，利用降维与机器学习方法进行二维实例空间投影分析。

Result: 识别出了23个相关实例特征，并提供了一个投影矩阵，用于简化新实例分析。

Conclusion: 通过新的实例分析方法，增强了对CVRP领域中实例与算法性能之间关系的理解。

Abstract: This paper seeks to advance CVRP research by addressing the challenge of
understanding the nuanced relationships between instance characteristics and
metaheuristic (MH) performance. We present Instance Space Analysis (ISA) as a
valuable tool that allows for a new perspective on the field. By combining the
ISA methodology with a dataset from the DIMACS 12th Implementation Challenge on
Vehicle Routing, our research enabled the identification of 23 relevant
instance characteristics. Our use of the PRELIM, SIFTED, and PILOT stages,
which employ dimensionality reduction and machine learning methods, allowed us
to create a two-dimensional projection of the instance space to understand how
the structure of instances affect the behavior of MHs. A key contribution of
our work is that we provide a projection matrix, which makes it straightforward
to incorporate new instances into this analysis and allows for a new method for
instance analysis in the CVRP field.

</details>


### [267] [SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning](https://arxiv.org/abs/2507.10421)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.AI

TL;DR: 本文提出了一种新型模型，将BERT对学生评论的情感分析与XGBoost对社会人口统计及行为数据的分析结合，实现了学生辍学风险的精准预测，模型准确率达到84%。


<details>
  <summary>Details</summary>
Motivation: 提高远程学习环境中辍学预测的准确性，通过整合多样化数据，为个性化干预策略提供支持。

Method: 将BERT模型用于分析学生评论情感，结合运用XGBoost算法对行为和社会人口统计数据进行处理，并通过特征重要性技术融合关键特征。

Result: 模型在下一学年的未见数据上测试，准确率达到84%，优于基线模型的82%，并在精确率和F1分数等指标上表现突出。

Conclusion: 所提出的方法可为制定个性化策略提供有效工具，从而减少辍学率并促进学生坚持学习。

Abstract: School dropout is a serious problem in distance learning, where early
detection is crucial for effective intervention and student perseverance.
Predicting student dropout using available educational data is a widely
researched topic in learning analytics. Our partner's distance learning
platform highlights the importance of integrating diverse data sources,
including socio-demographic data, behavioral data, and sentiment analysis, to
accurately predict dropout risks. In this paper, we introduce a novel model
that combines sentiment analysis of student comments using the Bidirectional
Encoder Representations from Transformers (BERT) model with socio-demographic
and behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We
fine-tuned BERT on student comments to capture nuanced sentiments, which were
then merged with key features selected using feature importance techniques in
XGBoost. Our model was tested on unseen data from the next academic year,
achieving an accuracy of 84\%, compared to 82\% for the baseline model.
Additionally, the model demonstrated superior performance in other metrics,
such as precision and F1-score. The proposed method could be a vital tool in
developing personalized strategies to reduce dropout rates and encourage
student perseverance

</details>


### [268] [Acquiring and Adapting Priors for Novel Tasks via Neural Meta-Architectures](https://arxiv.org/abs/2507.10446)
*Sudarshan Babu*

Main category: cs.AI

TL;DR: 本研究利用神经记忆和超网络架构，克服数据稀缺问题，提升模型在非平稳分布以及3D场景中的适应能力。


<details>
  <summary>Details</summary>
Motivation: 在缺乏海量数据的领域（如计算化学、免疫学、医学成像），大规模模型的预训练成为挑战，亟需设计高效的架构推动知识迁移。

Method: 使用神经记忆应对非平稳分布，同时结合超网络与模型无关元学习（MAML），实现高效先验获取；应用于3D场景生成和分割以验证效果，并引入分子生成方法进行预训练以改善化学领域的预测能力。

Result: 结果显示超网络相比标准模型更具普适性，能在少量数据下高效生成3D场景、分割新场景，以及改善分子性质预测。

Conclusion: 提出的方法能在数据稀缺的条件下高效获取和转移知识，广泛适用于多领域任务。

Abstract: The ability to transfer knowledge from prior experiences to novel tasks
stands as a pivotal capability of intelligent agents, including both humans and
computational models. This principle forms the basis of transfer learning,
where large pre-trained neural networks are fine-tuned to adapt to downstream
tasks. Transfer learning has demonstrated tremendous success, both in terms of
task adaptation speed and performance. However there are several domains where,
due to lack of data, training such large pre-trained models or foundational
models is not a possibility - computational chemistry, computational
immunology, and medical imaging are examples. To address these challenges, our
work focuses on designing architectures to enable efficient acquisition of
priors when large amounts of data are unavailable. In particular, we
demonstrate that we can use neural memory to enable adaptation on
non-stationary distributions with only a few samples. Then we demonstrate that
our hypernetwork designs (a network that generates another network) can acquire
more generalizable priors than standard networks when trained with Model
Agnostic Meta-Learning (MAML). Subsequently, we apply hypernetworks to 3D scene
generation, demonstrating that they can acquire priors efficiently on just a
handful of training scenes, thereby leading to faster text-to-3D generation. We
then extend our hypernetwork framework to perform 3D segmentation on novel
scenes with limited data by efficiently transferring priors from earlier viewed
scenes. Finally, we repurpose an existing molecular generative method as a
pre-training framework that facilitates improved molecular property prediction,
addressing critical challenges in computational immunology

</details>


### [269] [DeepResearch$^{\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology](https://arxiv.org/abs/2507.10522)
*Jennifer D'Souza,Endres Keno Sander,Andrei Aioanei*

Main category: cs.AI

TL;DR: DeepResearch$^{\text{Eco}}$是一种基于大型语言模型的新系统，用于科学综合，支持递归与控制探索新研究问题，显著提升文献检索多样性与综合深度。应用于生态学问题，结果表现出显著的整合效率提高。


<details>
  <summary>Details</summary>
Motivation: 提高科学检索与综合的多样性、深度和透明度，解决传统检索生成系统对动态整合、配置控制和严密分析的局限性。

Method: 基于大型语言模型 (LLM)，开发递归性、用户透明且参数驱动的科学文献整合方法，用于按用户需求深度或广度综合生态学领域的研究文献。

Result: 在49个生态学研究问题上，文献整合效率提升高达21倍，单位词输出的文献引用量上升至14.9倍，更高参数设定下出现专家级分析深度与上下文多样性。

Conclusion: DeepResearch$^{\text{Eco}}$显著提升了复杂文献综合的效率与多样性，为科学研究与领域知识整合开拓了新思路。

Abstract: We introduce DeepResearch$^{\text{Eco}}$, a novel agentic LLM-based system
for automated scientific synthesis that supports recursive, depth- and
breadth-controlled exploration of original research questions -- enhancing
search diversity and nuance in the retrieval of relevant scientific literature.
Unlike conventional retrieval-augmented generation pipelines, DeepResearch
enables user-controllable synthesis with transparent reasoning and
parameter-driven configurability, facilitating high-throughput integration of
domain-specific evidence while maintaining analytical rigor. Applied to 49
ecological research questions, DeepResearch achieves up to a 21-fold increase
in source integration and a 14.9-fold rise in sources integrated per 1,000
words. High-parameter settings yield expert-level analytical depth and
contextual diversity.
  Source code available at: https://github.com/sciknoworg/deep-research.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [270] [Effects of structural properties of neural networks on machine learning performance](https://arxiv.org/abs/2507.10005)
*Yash Arya,Sang Hoon Lee*

Main category: cs.LG

TL;DR: 本文探讨了神经网络的图结构如何影响性能，发现具有社区结构的网络在图像分类任务中表现更佳，将生物神经网络比作为研究基准。


<details>
  <summary>Details</summary>
Motivation: 近年来，基于图的机器学习技术备受关注，但现有研究通常聚焦于简单模型网络，忽略了包含社区等中观结构的复杂网络。本研究旨在填补这一空白。

Method: 作者使用了包括随机网络、无标度网络以及生物神经网络的模型网络，分析它们的异质性度分布和社区结构，并考察这些结构属性对图像分类任务性能的影响。

Result: 结果显示，具有密集社区结构的网络在学习能力上有显著提升，与生物神经网络的比较表明这种研究具有现实意义。

Conclusion: 研究表明，现实网络的结构特性能显著影响神经网络性能，尤其是社区结构的作用，为设计更符合生物特性的神经网络提供了启发。

Abstract: In recent years, graph-based machine learning techniques, such as
reinforcement learning and graph neural networks, have garnered significant
attention. While some recent studies have started to explore the relationship
between the graph structure of neural networks and their predictive
performance, they often limit themselves to a narrow range of model networks,
particularly lacking mesoscale structures such as communities. Our work
advances this area by conducting a more comprehensive investigation,
incorporating realistic network structures characterized by heterogeneous
degree distributions and community structures, which are typical
characteristics of many real networks. These community structures offer a
nuanced perspective on network architecture. Our analysis employs model
networks such as random and scale-free networks, alongside a comparison with a
biological neural network and its subsets for more detailed analysis. We
examine the impact of these structural attributes on the performance of image
classification tasks. Our findings reveal that structural properties do affect
performance to some extent. Specifically, networks featuring coherent, densely
interconnected communities demonstrate enhanced learning capabilities. The
comparison with the biological neural network emphasizes the relevance of our
findings to real-world structures, suggesting an intriguing connection worth
further exploration. This study contributes meaningfully to network science and
machine learning, providing insights that could inspire the design of more
biologically informed neural networks.

</details>


### [271] [Recurrent Expansion: A Pathway Toward the Next Generation of Deep Learning](https://arxiv.org/abs/2507.08828)
*Tarek Berghout*

Main category: cs.LG

TL;DR: 提出了一种新型学习范式——递归扩展（RE），通过模型自身行为进行学习，改进了传统的深度学习。


<details>
  <summary>Details</summary>
Motivation: 克服传统深度学习仅处理静态数据表示的局限，引入基于模型行为学习的进阶方法，推动人工智能发展。

Method: 通过递归分析模型内部表示和性能信号，逐步改进模型版本。同时提出多宇宙RE（MVRE）和异构MVRE（HMVRE）扩展框架，以及具备选择性机制和规模多样性的Sc-HMVRE实现。

Result: RE框架通过模型自我演进实现了新的学习维度，证明了其在智能模型中的有效性，支持更强的自我改进能力。

Conclusion: RE开创了从表征学习到行为感知自我演进系统的转变，为可扩展、自省及自适应的智能模型提供了新的方向。

Abstract: This paper introduces Recurrent Expansion (RE) as a new learning paradigm
that advances beyond conventional Machine Learning (ML) and Deep Learning (DL).
While DL focuses on learning from static data representations, RE proposes an
additional dimension: learning from the evolving behavior of models themselves.
RE emphasizes multiple mappings of data through identical deep architectures
and analyzes their internal representations (i.e., feature maps) in conjunction
with observed performance signals such as loss. By incorporating these
behavioral traces, RE enables iterative self-improvement, allowing each model
version to gain insight from its predecessors. The framework is extended
through Multiverse RE (MVRE), which aggregates signals from parallel model
instances, and further through Heterogeneous MVRE (HMVRE), where models of
varying architectures contribute diverse perspectives. A scalable and adaptive
variant, Sc-HMVRE, introduces selective mechanisms and scale diversity for
real-world deployment. Altogether, RE presents a shift in DL: from purely
representational learning to behavior-aware, self-evolving systems. It lays the
groundwork for a new class of intelligent models capable of reasoning over
their own learning dynamics, offering a path toward scalable, introspective,
and adaptive artificial intelligence. A simple code example to support
beginners in running their own experiments is provided in Code Availability
Section of this paper.

</details>


### [272] [Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using Explainable AI](https://arxiv.org/abs/2507.08829)
*Kimia Soroush,Nastaran Shirazi,Mohsen Raji*

Main category: cs.LG

TL;DR: 本文提出一种通过可解释人工智能方法提升深度神经网络（DNN）在bit-flip错误下的可靠性的方法。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域DNN应用广泛，因此亟需提高其在bit-flip错误下的可靠性，并降低三模冗余（TMR）技术的开销。

Method: 采用一种基于梯度的可解释人工智能（XAI）技术——层次相关传播（LRP），计算DNN权重的重要性分数，仅对最关键参数的TMR保护。

Result: 在MNIST和CIFAR-10数据集上测试VGG16和AlexNet模型，该方法在10-4的误码率下提高了AlexNet模型60%以上的可靠性，并保持与当前最优方法相同的开销。

Conclusion: 基于XAI的TMR选择方法有效增强了DNN可靠性，为安全关键应用中的错误容忍提供了一种高效的解决方案。

Abstract: Deep Neural Networks (DNNs) are widely employed in safety-critical domains,
where ensuring their reliability is essential. Triple Modular Redundancy (TMR)
is an effective technique to enhance the reliability of DNNs in the presence of
bit-flip faults. In order to handle the significant overhead of TMR, it is
applied selectively on the parameters and components with the highest
contribution at the model output. Hence, the accuracy of the selection
criterion plays the key role on the efficiency of TMR. This paper presents an
efficient TMR approach to enhance the reliability of DNNs against bit-flip
faults using an Explainable Artificial Intelligence (XAI) method. Since XAI can
provide valuable insights about the importance of individual neurons and
weights in the performance of the network, they can be applied as the selection
metric in TMR techniques. The proposed method utilizes a low-cost,
gradient-based XAI technique known as Layer-wise Relevance Propagation (LRP) to
calculate importance scores for DNN parameters. These scores are then used to
enhance the reliability of the model, with the most critical weights being
protected by TMR. The proposed approach is evaluated on two DNN models, VGG16
and AlexNet, using datasets such as MNIST and CIFAR-10. The results demonstrate
that the method can protect the AlexNet model at a bit error rate of 10-4,
achieving over 60% reliability improvement while maintaining the same overhead
as state-of-the-art methods.

</details>


### [273] [A Hybrid Machine Learning Framework for Optimizing Crop Selection via Agronomic and Economic Forecasting](https://arxiv.org/abs/2507.08832)
*Niranjan Mallikarjun Sindhur,Pavithra C,Nivya Muchikel*

Main category: cs.LG

TL;DR: 本论文提出了一种通过整合机器学习和人机交互来帮助农民解决市场和气候不确定性，以及克服数字鸿沟的创新决策支持系统。


<details>
  <summary>Details</summary>
Motivation: 帮助识字水平低的农民应对市场和气候波动，同时克服数字化革命带来的排斥问题。

Method: 设计了一个混合推荐引擎，综合使用随机森林分类器和LSTM网络分别预测农业适用性和市场价格；系统通过本地语言的语音界面提供支持。

Result: 随机森林模型在适用性预测上达到了98.5%的准确率，LSTM模型对市场价格预测误差较低。

Conclusion: 该系统通过数据驱动和经济优化的建议，以及包容性接口，为边缘化农民社区提供了可扩展与影响深远的金融韧性解决方案。

Abstract: Farmers in developing regions like Karnataka, India, face a dual challenge:
navigating extreme market and climate volatility while being excluded from the
digital revolution due to literacy barriers. This paper presents a novel
decision support system that addresses both challenges through a unique
synthesis of machine learning and human-computer interaction. We propose a
hybrid recommendation engine that integrates two predictive models: a Random
Forest classifier to assess agronomic suitability based on soil, climate, and
real-time weather data, and a Long Short-Term Memory (LSTM) network to forecast
market prices for agronomically viable crops. This integrated approach shifts
the paradigm from "what can grow?" to "what is most profitable to grow?",
providing a significant advantage in mitigating economic risk. The system is
delivered through an end-to-end, voice-based interface in the local Kannada
language, leveraging fine-tuned speech recognition and high-fidelity speech
synthesis models to ensure accessibility for low-literacy users. Our results
show that the Random Forest model achieves 98.5% accuracy in suitability
prediction, while the LSTM model forecasts harvest-time prices with a low
margin of error. By providing data-driven, economically optimized
recommendations through an inclusive interface, this work offers a scalable and
impactful solution to enhance the financial resilience of marginalized farming
communities.

</details>


### [274] [LoRA Is Slower Than You Think](https://arxiv.org/abs/2507.08833)
*Seokmin Ko*

Main category: cs.LG

TL;DR: 本文探讨了热门的低秩适配技术（LoRA）在微调大语言模型（LLMs）中的性能，并提出了一些改进方法以提高训练速度的一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管LoRA在减少内存消耗和计算开销方面表现出优势，但其速度提升在某些模型架构和训练环境中表现不稳定。

Method: 分析了限制LoRA速度提升的因素，基于分析结果提出了多种更高效的微调方法。

Result: 所提出的方法在性能上可与或优于LoRA，同时提供了更稳定的训练速度提升。

Conclusion: 研究为在资源受限环境中优化LLMs微调提供了宝贵的见解和实用指南。

Abstract: Low-Rank Adaptation (LoRA) is one of the most widely used techniques for
fine-tuning large language models (LLMs). By introducing a small number of
trainable low-rank weight matrices, LoRA substantially reduces the number of
parameters that need to be updated, offering significant advantages in memory
consumption and computational efficiency compared to full fine-tuning. However,
we observed that LoRA does not consistently provide speed improvements across
all model architectures and training setups. Motivated by this inconsistency,
we conduct a comprehensive analysis of LoRA's performance and investigate the
underlying factors limiting its speedup. Based on our findings, we propose
several methods for more efficient fine-tuning of LLMs. We empirically evaluate
these methods and compare them to LoRA, demonstrating that our approach
achieves comparable or superior performance while delivering more consistent
training speed improvements. Our work offers valuable insights and practical
guidelines for practitioners seeking to optimize LLM fine-tuning under resource
constraints.

</details>


### [275] [Physical Informed Neural Networks for modeling ocean pollutant](https://arxiv.org/abs/2507.08834)
*Karishma Battina,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 本文提出使用物理驱动神经网络（PINN）模拟由二维对流扩散方程控制的污染物扩散行为。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法难以处理海洋污染物传播的复杂动态问题。

Method: 将物理定律与噪声合成数据嵌入神经网络的训练中，通过混合损失函数（包括PDE残差和边界/初始条件）优化。

Result: 使用Julia语言的高效计算生态体系，模型在不同噪声水平下均能实现具有物理一致性的预测。

Conclusion: 提出的PINN框架为传统求解器提供了一个具有扩展性和灵活性的替代方案。

Abstract: Traditional numerical methods often struggle with the complexity and scale of
modeling pollutant transport across vast and dynamic oceanic domains. This
paper introduces a Physics-Informed Neural Network (PINN) framework to simulate
the dispersion of pollutants governed by the 2D advection-diffusion equation.
The model achieves physically consistent predictions by embedding physical laws
and fitting to noisy synthetic data, generated via a finite difference method
(FDM), directly into the neural network training process. This approach
addresses challenges such as non-linear dynamics and the enforcement of
boundary and initial conditions. Synthetic data sets, augmented with varying
noise levels, are used to capture real-world variability. The training
incorporates a hybrid loss function including PDE residuals, boundary/initial
condition conformity, and a weighted data fit term. The approach takes
advantage of the Julia language scientific computing ecosystem for
high-performance simulations, offering a scalable and flexible alternative to
traditional solvers

</details>


### [276] [Representation learning with a transformer by contrastive learning for money laundering detection](https://arxiv.org/abs/2507.08835)
*Harold Guéneau,Alain Celisse,Pascal Delange*

Main category: cs.LG

TL;DR: 该研究提出了一种新的基于Transformer神经网络的方法用于检测洗钱行为，具有较低的误报率和更高的检测效率。


<details>
  <summary>Details</summary>
Motivation: 当前洗钱活动检测方法存在误报率高和检测效率低的问题，需要更先进的技术来应对跨领域数据处理。

Method: 采用基于对比学习的Transformer神经网络模型，结合无标签时间序列特征学习，并使用双阈值控制策略降低误报率。

Result: 实验表明，所提出的方法能够以最小的监督成功检测洗钱模式，表现优于基于规则或LSTM的传统方法。

Conclusion: 通过创新性的方法，该研究显著提高了洗钱检测的精确性和效率，并成功将误报控制在较低水平。

Abstract: The present work tackles the money laundering detection problem. A new
procedure is introduced which exploits structured time series of both
qualitative and quantitative data by means of a transformer neural network. The
first step of this procedure aims at learning representations of time series
through contrastive learning (without any labels). The second step leverages
these representations to generate a money laundering scoring of all
observations. A two-thresholds approach is then introduced, which ensures a
controlled false-positive rate by means of the Benjamini-Hochberg (BH)
procedure. Experiments confirm that the transformer is able to produce general
representations that succeed in exploiting money laundering patterns with
minimal supervision from domain experts. It also illustrates the higher ability
of the new procedure for detecting nonfraudsters as well as fraudsters, while
keeping the false positive rate under control. This greatly contrasts with
rule-based procedures or the ones based on LSTM architectures.

</details>


### [277] [Accuracy and Consumption analysis from a compressed model by CompactifAI from Multiverse Computing](https://arxiv.org/abs/2507.08836)
*Damien Fovet,Shashank Chamoli,Sarah Oury,Srishti Singhal*

Main category: cs.LG

TL;DR: 本文评估了Multiverse Computing开发的CompactifAI压缩方法对大语言模型Llama 3.1 8B的效果。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过压缩方法降低语言模型的计算资源消耗，同时保持其性能，提升模型的效率和可扩展性。

Method: 使用CompactifAI方法对Llama 3.1 8B模型进行压缩，并结合Codecarbon评估能耗，Ragas评估准确性。与未压缩模型进行对比实验。

Result: 压缩后的模型显著减少了计算资源的消耗，同时保持了模型的准确性。

Conclusion: CompactifAI方法使模型更高效、可扩展且性价比提升。

Abstract: This study evaluates the performance of a compression method, called
CompactifAI, developed by Multiverse Computing, applied to the large language
model Llama 3.1 8B\cite{llama}. The evaluation focused on model efficiency (in
terms of energy consumption) and accuracy using respectively the frameworks
Codecarbon\cite{codecarbon} and Ragas\cite{ragas}. A comparison was performed
between the model compressed with
CompactifAI\cite{compactifai}\cite{compactifai2} and its full-size version. Our
findings reveal that the compressed model using CompactifAI not only
significantly reduced the computational resources but also maintained the model
accuracy, making the model more efficient, scalable and cost-effective.

</details>


### [278] [wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models](https://arxiv.org/abs/2507.08838)
*Xiaohang Tang,Rares Dolga,Sangwoong Yoon,Ilija Bogunovic*

Main category: cs.LG

TL;DR: 文章介绍了一种名为$\mathtt{wd1}$的新方法，用于通过强化学习优化扩散型大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前扩散型大语言模型（dLLMs）的推理能力优化困难，现有方法在利用强化学习时计算复杂性高且易引入大偏差。

Method: 提出$\mathtt{wd1}$方法，将目标重新表述为加权似然形式，仅需对当前参数化策略的似然进行单次近似估计。

Result: 实验表明，$\mathtt{wd1}$在无需监督微调（SFT）或任何监督数据的情况下，能够在推理基准上比现有方法提升16%的准确率，并同时减少训练时间和每步梯度计算的函数评估次数。

Conclusion: $\mathtt{wd1}$方法简单高效，对扩散型大语言模型推理任务中的强化学习应用具有更优的效果和效率。

Abstract: Improving the reasoning capabilities of diffusion-based large language models
(dLLMs) through reinforcement learning (RL) remains an open problem. The
intractability of dLLMs likelihood function necessitates approximating the
current, old, and reference policy likelihoods at each policy optimization
step. This reliance introduces additional computational overhead and lead to
potentially large bias -- particularly when approximation errors occur in the
denominator of policy ratios used for importance sampling. To mitigate these
issues, we introduce $\mathtt{wd1}$, a novel policy optimization approach that
reformulates the objective as a weighted likelihood, requiring only a single
approximation for the current parametrized policy likelihood. Experiments on
widely used reasoning benchmarks demonstrate that $\mathtt{wd1}$, without
supervised fine-tuning (SFT) or any supervised data, outperforms existing RL
methods for dLLMs, achieving up to 16% higher accuracy. $\mathtt{wd1}$ delivers
additional computational gains, including reduced training time and fewer
function evaluations (NFEs) per gradient step. These findings, combined with
the simplicity of method's implementation and R1-Zero-like training (no SFT),
position $\mathtt{wd1}$ as a more effective and efficient method for applying
RL to dLLMs reasoning.

</details>


### [279] [Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer](https://arxiv.org/abs/2507.08839)
*Xiaowei Yu,Jing Zhang,Tong Chen,Yan Zhuang,Minheng Chen,Chao Cao,Yanjun Lyu,Lu Zhang,Li Su,Tianming Liu,Dajiang Zhu*

Main category: cs.LG

TL;DR: 提出了基于注意力机制的转移感知Transformer（TAT），通过从阿尔茨海默症（AD）数据中适应性地转移知识以改善路易体病（LBD）诊断。


<details>
  <summary>Details</summary>
Motivation: 由于LBD诊断数据稀缺且AD数据相对丰富，而LBD与AD数据存在域迁移问题，因此探索如何有效利用AD数据辅助LBD诊断。

Method: 提出了Transferability Aware Transformer（TAT），基于MRI衍生的结构连接数据，通过注意力机制赋予疾病可转移特征更高权重，降低域迁移影响。

Result: 实验结果证明，TAT有效改善了在LBD数据稀缺情况下的诊断准确性。

Conclusion: 本研究首次在数据稀缺及域迁移条件下探索从AD到LBD的领域适配，为罕见疾病的领域适配诊断提供了一个有前景的框架。

Abstract: Lewy Body Disease (LBD) is a common yet understudied form of dementia that
imposes a significant burden on public health. It shares clinical similarities
with Alzheimer's disease (AD), as both progress through stages of normal
cognition, mild cognitive impairment, and dementia. A major obstacle in LBD
diagnosis is data scarcity, which limits the effectiveness of deep learning. In
contrast, AD datasets are more abundant, offering potential for knowledge
transfer. However, LBD and AD data are typically collected from different sites
using different machines and protocols, resulting in a distinct domain shift.
To effectively leverage AD data while mitigating domain shift, we propose a
Transferability Aware Transformer (TAT) that adapts knowledge from AD to
enhance LBD diagnosis. Our method utilizes structural connectivity (SC) derived
from structural MRI as training data. Built on the attention mechanism, TAT
adaptively assigns greater weights to disease-transferable features while
suppressing domain-specific ones, thereby reducing domain shift and improving
diagnostic accuracy with limited LBD data. The experimental results demonstrate
the effectiveness of TAT. To the best of our knowledge, this is the first study
to explore domain adaptation from AD to LBD under conditions of data scarcity
and domain shift, providing a promising framework for domain-adaptive diagnosis
of rare diseases.

</details>


### [280] [Zero-Shot Neural Architecture Search with Weighted Response Correlation](https://arxiv.org/abs/2507.08841)
*Kun Jing,Luoyu Chen,Jungang Xu,Jianwei Tai,Yiyu Wang,Shuaimin Li*

Main category: cs.LG

TL;DR: 提出了一种新的零样本神经架构搜索方法，通过加权响应相关性（WRCor）加速架构评估，同时提高其效果、稳定性和通用性。


<details>
  <summary>Details</summary>
Motivation: 针对现有零样本算法效果、稳定性和通用性不足的问题，提出改进的零样本评估指标，以提升NAS有效性。

Method: 设计加权响应相关性（WRCor）作为零样本评估指标，利用响应的相关性矩阵计算网络架构表达性和泛化性，并结合不同搜索策略进行架构搜索。

Result: WRCor的评估效率高于现有指标，同时其零样本算法在多个搜索空间内优于现有NAS算法。在ImageNet-1k数据集上，通过4 GPU小时找到测试误差为22.1%的架构。

Conclusion: 提出的WRCor指标和NAS算法在高效率前提下提供了更优异的性能表现，为神经网络架构设计提供了新的可能性。

Abstract: Neural architecture search (NAS) is a promising approach for automatically
designing neural network architectures. However, the architecture estimation of
NAS is computationally expensive and time-consuming because of training
multiple architectures from scratch. Although existing zero-shot NAS methods
use training-free proxies to accelerate the architecture estimation, their
effectiveness, stability, and generality are still lacking. We present a novel
training-free estimation proxy called weighted response correlation (WRCor).
WRCor utilizes correlation coefficient matrices of responses across different
input samples to calculate the proxy scores of estimated architectures, which
can measure their expressivity and generalizability. Experimental results on
proxy evaluation demonstrate that WRCor and its voting proxies are more
efficient estimation strategies than existing proxies. We also apply them with
different search strategies in architecture search. Experimental results on
architecture search show that our zero-shot NAS algorithm outperforms most
existing NAS algorithms in different search spaces. Our NAS algorithm can
discover an architecture with a 22.1% test error on the ImageNet-1k dataset
within 4 GPU hours. All codes are publicly available at
https://github.com/kunjing96/ZSNAS-WRCor.git.

</details>


### [281] [Gradients as an Action: Towards Communication-Efficient Federated Recommender Systems via Adaptive Action Sharing](https://arxiv.org/abs/2507.08842)
*Zhufeng Lu,Chentao Jia,Ming Hu,Xiaofei Xie,Mingsong Chen*

Main category: cs.LG

TL;DR: 該論文提出FedRAS，一種改進聯邦推薦系統(FedRec)的框架，通過行動共享策略和自適應聚類機制，有效解決了聯邦學習中高通信負載和低訓練效率的問題。


<details>
  <summary>Details</summary>
Motivation: 目前聯邦推薦系統因涉及大量嵌入參數，面臨著通信開銷高和異構設備、網絡環境下訓練效率低的挑戰。現有方法的壓縮技術雖能減少通信，但會導致模型性能下降。

Method: 提出FedRAS框架，通過“行動共享策略”，聚合嵌入梯度以更新模型；引入自適應聚類機制，用於調整通信行動數量以適應異構設備和網絡環境。

Result: FedRAS有效減少了通信負載，比如降低高達96.88%的通信量，並且在多種異構場景下仍能保持推薦性能。

Conclusion: FedRAS框架在提高通信效率的同時，成功兼顧了模型性能與異構場景需求，展示出聯邦推薦系統應用的新潛力。

Abstract: As a promising privacy-aware collaborative model training paradigm, Federated
Learning (FL) is becoming popular in the design of distributed recommender
systems. However, Federated Recommender Systems (FedRecs) greatly suffer from
two major problems: i) extremely high communication overhead due to massive
item embeddings involved in recommendation systems, and ii) intolerably low
training efficiency caused by the entanglement of both heterogeneous network
environments and client devices. Although existing methods attempt to employ
various compression techniques to reduce communication overhead, due to the
parameter errors introduced by model compression, they inevitably suffer from
model performance degradation. To simultaneously address the above problems,
this paper presents a communication-efficient FedRec framework named FedRAS,
which adopts an action-sharing strategy to cluster the gradients of item
embedding into a specific number of model updating actions for communication
rather than directly compressing the item embeddings. In this way, the cloud
server can use the limited actions from clients to update all the items. Since
gradient values are significantly smaller than item embeddings, constraining
the directions of gradients (i.e., the action space) introduces smaller errors
compared to compressing the entire item embedding matrix into a reduced space.
To accommodate heterogeneous devices and network environments, FedRAS
incorporates an adaptive clustering mechanism that dynamically adjusts the
number of actions. Comprehensive experiments on well-known datasets demonstrate
that FedRAS can reduce the size of communication payloads by up to 96.88%,
while not sacrificing recommendation performance within various heterogeneous
scenarios. We have open-sourced FedRAS at
https://github.com/mastlab-T3S/FedRAS.

</details>


### [282] [Can We Predict Your Next Move Without Breaking Your Privacy?](https://arxiv.org/abs/2507.08843)
*Arpita Soni,Sahil Tripathi,Gautam Siddharth Kashyap,Manaswi Kulahara,Mohammad Anas Azeez,Zohaib Hasan Siddiqui,Nipun Joshi,Jiechao Gao*

Main category: cs.LG

TL;DR: 提出了一个隐私保护的框架FLLL3M，用于下一位置预测，通过局部数据处理和高效的外积机制利用大语言模型，实现了高准确性和低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 解决用户数据隐私问题，并提升下一位置预测的性能和资源利用效率。

Method: 通过在用户本地保留数据，并利用大语言模型与外积机制结合进行建模。

Result: 在Gowalla (Acc@1: 12.55, MRR: 0.1422), WeePlace (10.71, 0.1285), Brightkite (10.42, 0.1169), FourSquare (8.71, 0.1023)数据集上取得最先进的结果，同时减少了45.6%的参数和52.7%的内存使用。

Conclusion: FLLL3M在隐私保护的前提下有效地优化了下一位置预测的性能，是一个高效且资源节约的框架。

Abstract: We propose FLLL3M--Federated Learning with Large Language Models for Mobility
Modeling--a privacy-preserving framework for Next-Location Prediction (NxLP).
By retaining user data locally and leveraging LLMs through an efficient outer
product mechanism, FLLL3M ensures high accuracy with low resource demands. It
achieves SOT results on Gowalla (Acc@1: 12.55, MRR: 0.1422), WeePlace (10.71,
0.1285), Brightkite (10.42, 0.1169), and FourSquare (8.71, 0.1023), while
reducing parameters by up to 45.6% and memory usage by 52.7%.

</details>


### [283] [DAFOS: Dynamic Adaptive Fanout Optimization Sampler](https://arxiv.org/abs/2507.08845)
*Irfan Ullah,Young-Koo Lee*

Main category: cs.LG

TL;DR: 文章提出了一种动态自适应扇出优化采样器（DAFOS），旨在提升图神经网络（GNN）在大规模图数据上的训练效率和模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有的图神经网络的邻居采样过程采用统一扇出与静态设置，限制了其在大规模图数据上的扩展能力与效率。本研究旨在解决这一瓶颈。

Method: DAFOS 动态调整扇出数，根据模型表现优先选择重要节点，利用节点度数对节点重要性评分，同时在训练中增大扇出，并集成早停机制以避免训练资源浪费。

Result: 在ogbn-arxiv、Reddit与ogbn-products上，DAFOS表现显著优越，例如在ogbn-arxiv上加速3.57倍，F1得分从68.5%提高到71.21%；在Reddit上加速12.6倍，同时准确率提升；在ogbn-products上F1得分从73.78%提升到76.88%。

Conclusion: DAFOS证明了其在大规模GNN训练中的高效率与可扩展性，展示了其作为现有方法替代方案的潜力。

Abstract: Graph Neural Networks (GNNs) are becoming an essential tool for learning from
graph-structured data, however uniform neighbor sampling and static fanout
settings frequently limit GNNs' scalability and efficiency. In this paper, we
propose the Dynamic Adaptive Fanout Optimization Sampler (DAFOS), a novel
approach that dynamically adjusts the fanout based on model performance and
prioritizes important nodes during training. Our approach leverages node
scoring based on node degree to focus computational resources on structurally
important nodes, incrementing the fanout as the model training progresses.
DAFOS also integrates an early stopping mechanism to halt training when
performance gains diminish. Experiments conducted on three benchmark datasets,
ogbnarxiv, Reddit, and ogbn-products, demonstrate that our approach
significantly improves training speed and accuracy compared to a
state-of-the-art approach. DAFOS achieves a 3.57x speedup on the ogbn-arxiv
dataset and a 12.6x speedup on the Reddit dataset while improving the F1 score
from 68.5% to 71.21% on ogbn-arxiv and from 73.78% to 76.88% on the
ogbn-products dataset, respectively. These results highlight the potential of
DAFOS as an efficient and scalable solution for large-scale GNN training.

</details>


### [284] [Assuring the Safety of Reinforcement Learning Components: AMLAS-RL](https://arxiv.org/abs/2507.08848)
*Calum Corrie Imrie,Ioannis Stefanakos,Sepeedeh Shahbeigi,Richard Hawkins,Simon Burton*

Main category: cs.LG

TL;DR: 该论文针对强化学习的安全保障挑战，提出了适配强化学习的AMLAS-RL方法学，并通过轮式车辆示例进行验证。


<details>
  <summary>Details</summary>
Motivation: 强化学习在处理复杂动态环境中的表现优越，但当前针对安全关键系统的学习过程缺乏系统性保障框架。

Method: 基于AMLAS方法学对强化学习进行适配，提出迭代式的AMLAS-RL框架用于生成强化学习系统的安全保障论据。

Result: 通过一个轮式车辆达到目标并避免碰撞的示例，展示了AMLAS-RL方法的可行性与效果。

Conclusion: AMLAS-RL方法为强化学习系统提供了结构化的安全保障框架，并具有潜力在安全关键应用中推广。

Abstract: The rapid advancement of machine learning (ML) has led to its increasing
integration into cyber-physical systems (CPS) across diverse domains. While CPS
offer powerful capabilities, incorporating ML components introduces significant
safety and assurance challenges. Among ML techniques, reinforcement learning
(RL) is particularly suited for CPS due to its capacity to handle complex,
dynamic environments where explicit models of interaction between system and
environment are unavailable or difficult to construct. However, in
safety-critical applications, this learning process must not only be effective
but demonstrably safe. Safe-RL methods aim to address this by incorporating
safety constraints during learning, yet they fall short in providing systematic
assurance across the RL lifecycle. The AMLAS methodology offers structured
guidance for assuring the safety of supervised learning components, but it does
not directly apply to the unique challenges posed by RL. In this paper, we
adapt AMLAS to provide a framework for generating assurance arguments for an
RL-enabled system through an iterative process; AMLAS-RL. We demonstrate
AMLAS-RL using a running example of a wheeled vehicle tasked with reaching a
target goal without collision.

</details>


### [285] [Foundation models for time series forecasting: Application in conformal prediction](https://arxiv.org/abs/2507.08858)
*Sami Achour,Yassine Bouher,Duong Nguyen,Nicolas Chesneau*

Main category: cs.LG

TL;DR: 研究探索了基础模型在时间序列预测中的零样本能力，发现其在校准数据量少的情况下表现出色，并改善了预测的可靠性。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在比较时间序列基础模型和传统方法在符合性预测中的表现，特别是在数据量受限的场景下。

Method: 将时间序列基础模型与统计模型和梯度提升方法进行比较，评估其在符合性预测环境中的校准与预测性能。

Result: 时间序列基础模型在数据量较少时，提供了更可靠的符合性预测区间，并且校准过程更稳定。

Conclusion: 基础模型在数据有限情况下显著优于传统方法，展示了其在提升时间序列符合性预测可靠性方面的潜力。

Abstract: The zero-shot capabilities of foundation models (FMs) for time series
forecasting offer promising potentials in conformal prediction, as most of the
available data can be allocated to calibration. This study compares the
performance of Time Series Foundation Models (TSFMs) with traditional methods,
including statistical models and gradient boosting, within a conformal
prediction setting. Our findings highlight two key advantages of TSFMs. First,
when the volume of data is limited, TSFMs provide more reliable conformalized
prediction intervals than classic models, thanks to their superior predictive
accuracy. Second, the calibration process is more stable because more data are
used for calibration. Morever, the fewer data available, the more pronounced
these benefits become, as classic models require a substantial amount of data
for effective training. These results underscore the potential of foundation
models in improving conformal prediction reliability in time series
applications, particularly in data-constrained cases. All the code to reproduce
the experiments is available.

</details>


### [286] [e-Profits: A Business-Aligned Evaluation Metric for Profit-Sensitive Customer Churn Prediction](https://arxiv.org/abs/2507.08860)
*Awais Manzoor,M. Atif Qureshi,Etain Kidney,Luca Longo*

Main category: cs.LG

TL;DR: 提出了一种名为e-Profits的新评估指标，用于客户流失预测模型财务表现的评估，较传统指标更具业务对齐性。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标如AUC和F1-score未能很好地反映模型的财务影响，可能导致战略决策失误。

Method: 引入e-Profits作为新的业务对齐评估指标，使用Kaplan-Meier生存分析以个性化方式估算客户留存率，并进行逐客户评估。

Result: 在IBM Telco和Maven Telecom两个数据集上对六种分类器进行基准测试，发现e-Profits重新定义了模型排序，揭示了传统指标无法发现的财务优势。

Conclusion: e-Profits是一种设计用于支持业务情境下模型评估的工具，尤其适合优先考虑利润驱动决策的营销和分析团队。

Abstract: Retention campaigns in customer relationship management often rely on churn
prediction models evaluated using traditional metrics such as AUC and F1-score.
However, these metrics fail to reflect financial outcomes and may mislead
strategic decisions. We introduce e-Profits, a novel business-aligned
evaluation metric that quantifies model performance based on customer-specific
value, retention probability, and intervention costs. Unlike existing
profit-based metrics such as Expected Maximum Profit, which assume fixed
population-level parameters, e-Profits uses Kaplan-Meier survival analysis to
estimate personalised retention rates and supports granular, per customer
evaluation. We benchmark six classifiers across two telecom datasets (IBM Telco
and Maven Telecom) and demonstrate that e-Profits reshapes model rankings
compared to traditional metrics, revealing financial advantages in models
previously overlooked by AUC or F1-score. The metric also enables segment-level
insight into which models maximise return on investment for high-value
customers. e-Profits is designed as an understandable, post hoc tool to support
model evaluation in business contexts, particularly for marketing and analytics
teams prioritising profit-driven decisions. All source code is available at:
https://github.com/matifq/eprofits.

</details>


### [287] [On the under-reaching phenomenon in message-passing neural PDE solvers: revisiting the CFL condition](https://arxiv.org/abs/2507.08861)
*Lucas Tesan,Mikel M. Iparraguirre,David Gonzalez,Pedro Martins,Elias Cueto*

Main category: cs.LG

TL;DR: 本文提出了图神经网络（GNN）在求解偏微分方程（PDE）时所需的迭代次数的下限。


<details>
  <summary>Details</summary>
Motivation: 解决PDE时，减少调整超参数的需求，提高GNN的计算效率。

Method: 通过分析PDE的物理特性，将其与GNN的消息传递需求联系起来，为三类基本PDE（双曲型、抛物型和椭圆型）推导出迭代下限。

Result: 若迭代次数低于提出的下限，网络信息传播不足，导致较差解；满足下限时，可准确捕捉现象学，得到精确解。

Conclusion: 提出的迭代下限是图神经网络中信息有效传播的核心限制，对提高解PDE的效果具有指导意义。

Abstract: This paper proposes sharp lower bounds for the number of message passing
iterations required in graph neural networks (GNNs) when solving partial
differential equations (PDE). This significantly reduces the need for
exhaustive hyperparameter tuning. Bounds are derived for the three fundamental
classes of PDEs (hyperbolic, parabolic and elliptic) by relating the physical
characteristics of the problem in question to the message-passing requirement
of GNNs. In particular, we investigate the relationship between the physical
constants of the equations governing the problem, the spatial and temporal
discretisation and the message passing mechanisms in GNNs.
  When the number of message passing iterations is below these proposed limits,
information does not propagate efficiently through the network, resulting in
poor solutions, even for deep GNN architectures. In contrast, when the
suggested lower bound is satisfied, the GNN parameterisation allows the model
to accurately capture the underlying phenomenology, resulting in solvers of
adequate accuracy.
  Examples are provided for four different examples of equations that show the
sharpness of the proposed lower bounds.

</details>


### [288] [Underrepresentation, Label Bias, and Proxies: Towards Data Bias Profiles for the EU AI Act and Beyond](https://arxiv.org/abs/2507.08866)
*Marina Ceccon,Giandomenico Cornacchia,Davide Dalle Pezze,Alessandro Fabris,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 讨论数据偏见在算法偏见中的作用，并提出了检测和缓解偏见的框架。


<details>
  <summary>Details</summary>
Motivation: 数据偏见是算法歧视的重要驱动因素，然而当前的研究尚未充分解决这一问题。

Method: 通过分析常见数据偏见及其交互影响，开发了检测特定偏见的机制，并提出数据偏见分析框架(DBP)。

Result: 发现脆弱群体的低代表性不如预想的那样导致歧视，而代理及标签偏见的组合作用更大；DBP框架可以预测歧视风险并提高公平性干预的效果。

Conclusion: 通过数据视角，将算法公平性研究与反歧视政策相结合，提供了偏见检测和记录的初步解决方案。

Abstract: Undesirable biases encoded in the data are key drivers of algorithmic
discrimination. Their importance is widely recognized in the algorithmic
fairness literature, as well as legislation and standards on
anti-discrimination in AI. Despite this recognition, data biases remain
understudied, hindering the development of computational best practices for
their detection and mitigation. In this work, we present three common data
biases and study their individual and joint effect on algorithmic
discrimination across a variety of datasets, models, and fairness measures. We
find that underrepresentation of vulnerable populations in training sets is
less conducive to discrimination than conventionally affirmed, while
combinations of proxies and label bias can be far more critical. Consequently,
we develop dedicated mechanisms to detect specific types of bias, and combine
them into a preliminary construct we refer to as the Data Bias Profile (DBP).
This initial formulation serves as a proof of concept for how different bias
signals can be systematically documented. Through a case study with popular
fairness datasets, we demonstrate the effectiveness of the DBP in predicting
the risk of discriminatory outcomes and the utility of fairness-enhancing
interventions. Overall, this article bridges algorithmic fairness research and
anti-discrimination policy through a data-centric lens.

</details>


### [289] [GUIDE: Towards Scalable Advising for Research Ideas](https://arxiv.org/abs/2507.08870)
*Yaowenqi Liu,BingXu Meng,Rui Pan,Jerry Huang,Tong Zhang*

Main category: cs.LG

TL;DR: 本研究开发出一种小型但高效的指导系统，优于更强大的通用语言模型，在假设生成和实验设计方面表现出色。


<details>
  <summary>Details</summary>
Motivation: AI技术迅猛发展，但高质量、可扩展的指导系统仍有待开发，以改进假设和实验设计。

Method: 通过较小的模型结合压缩文献数据库和结构化推理框架，探索影响优秀指导系统的关键因素，如模型大小、上下文长度等。

Result: 该系统的自评排名前30%论文的ICLR 2025接受率高于通用模型Deepseek-R1，对高置信预测，接受率超90%。

Conclusion: 证明了小型模型在优化情况与合理框架下可显现出色效果，提升了假设生成与实验设计的质量和效率。

Abstract: The field of AI research is advancing at an unprecedented pace, enabling
automated hypothesis generation and experimental design across diverse domains
such as biology, mathematics, and artificial intelligence. Despite these
advancements, there remains a significant gap in the availability of scalable
advising systems capable of providing high-quality, well-reasoned feedback to
refine proposed hypotheses and experimental designs. To address this challenge,
we explore key factors that underlie the development of robust advising
systems, including model size, context length, confidence estimation, and
structured reasoning processes. Our findings reveal that a relatively small
model, when equipped with a well-compressed literature database and a
structured reasoning framework, can outperform powerful general-purpose
language models such as Deepseek-R1 in terms of acceptance rates for
self-ranked top-30% submissions to ICLR 2025. Moreover, when limited to
high-confidence predictions, our system achieves an acceptance rate exceeding
90% on the ICLR 2025 test set, underscoring its potential to significantly
enhance the quality and efficiency of hypothesis generation and experimental
design. The code is released at
https://github.com/HowardLiu0830/GUIDE-Research-Idea-Evaluation.

</details>


### [290] [Next-Generation Travel Demand Modeling with a Generative Framework for Household Activity Coordination](https://arxiv.org/abs/2507.08871)
*Xishun Liao,Haoxuan Ma,Yifan Liu,Yuxiang Wei,Brian Yueshuai He,Chris Stanford,Jiaqi Ma*

Main category: cs.LG

TL;DR: 提出了一种学习驱动的出行需求建模框架，整合了多个模块并在洛杉矶进行了验证，结果表明其在准确性和成本效益上优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 传统基于活动的出行需求模型存在规则过于简单和跨地区适应性差的问题，研究目标是开发一个更灵活、效率更高的出行需求模型。

Method: 设计了一个以数据驱动和生成式学习为核心的框架，包含人口合成、活动生成、位置分配和大规模微观交通仿真，并在洛杉矶全面实现并验证。

Result: 模型与传统ABM相比，具有更高的准确性和可扩展性，达到0.97的OD矩阵余弦相似度、0.006的JSD，以及与实际数据相比6.11%的速度体积MAPE误差。

Conclusion: 新框架能够更高效地建模出行需求，准确模拟实际出行模式并显著降低开发成本，显示出较高的潜力和实用性。

Abstract: Travel demand models are critical tools for planning, policy, and mobility
system design. Traditional activity-based models (ABMs), although grounded in
behavioral theories, often rely on simplified rules and assumptions, and are
costly to develop and difficult to adapt across different regions. This paper
presents a learning-based travel demand modeling framework that synthesizes
household-coordinated daily activity patterns based on a household's
socio-demographic profiles. The whole framework integrates population
synthesis, coordinated activity generation, location assignment, and
large-scale microscopic traffic simulation into a unified system. It is fully
generative, data-driven, scalable, and transferable to other regions. A
full-pipeline implementation is conducted in Los Angeles with a 10 million
population. Comprehensive validation shows that the model closely replicates
real-world mobility patterns and matches the performance of legacy ABMs with
significantly reduced modeling cost and greater scalability. With respect to
the SCAG ABM benchmark, the origin-destination matrix achieves a cosine
similarity of 0.97, and the daily vehicle miles traveled (VMT) in the network
yields a 0.006 Jensen-Shannon Divergence (JSD) and a 9.8% mean absolute
percentage error (MAPE). When compared to real-world observations from Caltrans
PeMS, the evaluation on corridor-level traffic speed and volume reaches a 0.001
JSD and a 6.11% MAPE.

</details>


### [291] [Contrastive Language-Image Pre-Training Model based Semantic Communication Performance Optimization](https://arxiv.org/abs/2507.08873)
*Shaoran Yang,Dongyu Wei,Hanzhi Yu,Zhaohui Yang,Yuchen Liu,Mingzhe Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于对比语言-图像预训练（CLIP）模型的语义通信框架。通过优化框架结构和频谱资源分配，提高了语义通信在有噪无线网络中的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现有基于神经网络的语义通信方法需要在公共数据集上联合训练的限制，提出无需训练的框架并提高无线网络条件下的通信性能。

Method: 利用CLIP模型提取数据语义，并通过PPO算法优化框架架构和频谱分配规则，最大化在有噪无线网络中的语义通信表现。

Result: 此方法在提高语义通信性能方面优于现有方法，实验显示收敛速度提升40%，累计收益提升4倍。

Conclusion: 基于CLIP模型和PPO算法的框架可以在无线网络中实现高效、无训练的语义通信，其性能相比现有方法有显著提升。

Abstract: In this paper, a novel contrastive language-image pre-training (CLIP) model
based semantic communication framework is designed. Compared to standard neural
network (e.g.,convolutional neural network) based semantic encoders and
decoders that require joint training over a common dataset, our CLIP model
based method does not require any training procedures thus enabling a
transmitter to extract data meanings of the original data without neural
network model training, and the receiver to train a neural network for
follow-up task implementation without the communications with the transmitter.
Next, we investigate the deployment of the CLIP model based semantic framework
over a noisy wireless network. Since the semantic information generated by the
CLIP model is susceptible to wireless noise and the spectrum used for semantic
information transmission is limited, it is necessary to jointly optimize CLIP
model architecture and spectrum resource block (RB) allocation to maximize
semantic communication performance while considering wireless noise, the delay
and energy used for semantic communication. To achieve this goal, we use a
proximal policy optimization (PPO) based reinforcement learning (RL) algorithm
to learn how wireless noise affect the semantic communication performance thus
finding optimal CLIP model and RB for each user. Simulation results show that
our proposed method improves the convergence rate by up to 40%, and the
accumulated reward by 4x compared to soft actor-critic.

</details>


### [292] [An Automated Classifier of Harmful Brain Activities for Clinical Usage Based on a Vision-Inspired Pre-trained Framework](https://arxiv.org/abs/2507.08874)
*Yulin Sun,Xiaopeng Si,Runnan He,Xiao Hu,Peter Smielewski,Wenlong Wang,Xiaoguang Tong,Wei Yue,Meijun Pang,Kuo Zhang,Xizi Song,Dong Ming,Xiuyun Liu*

Main category: cs.LG

TL;DR: 提出一个名为VIPEEGNet的卷积神经网络模型，用于通过EEG数据识别有害脑活动，表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决当前AI模型在EEG分析中的问题，包括专家间的差异、不足的资源约束和欠缺的泛化能力。

Method: 开发并验证一种能分析不同类型脑活动的卷积神经网络模型（VIPEEGNet），基于两个独立的EEG数据集，其中包含1950名患者的数据用于训练，另有1532名患者的数据用于测试。

Result: VIPEEGNet模型展现出高AUROC值、与专家类似的敏感度和精确度，外部验证排名第二但使用的参数非常少。

Conclusion: VIPEEGNet在EEG分类任务中具有出色表现，尤其是在资源高效性方面，展现出极大的临床潜力。

Abstract: Timely identification of harmful brain activities via electroencephalography
(EEG) is critical for brain disease diagnosis and treatment, which remains
limited application due to inter-rater variability, resource constraints, and
poor generalizability of existing artificial intelligence (AI) models. In this
study, a convolutional neural network model, VIPEEGNet, was developed and
validated using EEGs recorded from Massachusetts General Hospital/Harvard
Medical School. The VIPEEGNet was developed and validated using two independent
datasets, collected between 2006 and 2020. The development cohort included EEG
recordings from 1950 patients, with 106,800 EEG segments annotated by at least
one experts (ranging from 1 to 28). The online testing cohort consisted of EEG
segments from a subset of an additional 1,532 patients, each annotated by at
least 10 experts. For the development cohort (n=1950), the VIPEEGNet achieved
high accuracy, with an AUROC for binary classification of seizure, LPD, GPD,
LRDA, GRDA, and "other" categories at 0.972 (95% CI, 0.957-0.988), 0.962 (95%
CI, 0.954-0.970), 0.972 (95% CI, 0.960-0.984), 0.938 (95% CI, 0.917-0.959),
0.949 (95% CI, 0.941-0.957), and 0.930 (95% CI, 0.926-0.935). For multi
classification, the sensitivity of VIPEEGNET for the six categories ranges from
36.8% to 88.2% and the precision ranges from 55.6% to 80.4%, and performance
similar to human experts. Notably, the external validation showed
Kullback-Leibler Divergence (KLD)of 0.223 and 0.273, ranking top 2 among the
existing 2,767 competing algorithms, while we only used 2.8% of the parameters
of the first-ranked algorithm.

</details>


### [293] [ODIA: Oriented Distillation for Inline Acceleration of LLM-based Function Calling](https://arxiv.org/abs/2507.08877)
*Hanlong Zhang,Jingsheng Yang,Hao Li,Yuhao He,Franck Gong*

Main category: cs.LG

TL;DR: 该论文提出一种名为ODIA的新方法，通过蒸馏技术减少函数调用的延迟问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）函数调用高延迟问题，提升用户体验。

Method: 提出Oriented Distillation for Inline Acceleration (ODIA)方法，利用生产流量中的简单查询，从大模型向小模型蒸馏知识。

Result: 方法减少函数调用响应时间，预期延迟降低45%，中位数延迟降低78%，在音乐应用场景实现高效部署。

Conclusion: ODIA方法实现了在不明显损失准确性的前提下显著减少延迟，并且具备高自动化和持续优化的优势，适合实际生产环境。

Abstract: Function Calling is a crucial technique that enables Large Language Models
(LLMs) to interact with external systems through APIs. However, the high
latency associated with LLM-based Function Calling significantly impacts user
experience. This paper presents a novel approach called Oriented Distillation
for Inline Acceleration (ODIA) that leverages online user interaction data to
accelerate Function Calling. By automatically identifying "simple queries" from
production traffic and distilling knowledge from larger models to smaller ones,
our method reduces response latency by 45% (expected) and 78% (median) while
maintaining accuracy. We demonstrate the effectiveness of our approach through
real-world deployment in a music application, where the smaller model
successfully handles 60% of traffic with negligible accuracy loss. Our method
requires minimal human intervention and continuously improves through automated
data collection and model updating, making it a practical solution for
production environments.

</details>


### [294] [Last Layer Hamiltonian Monte Carlo](https://arxiv.org/abs/2507.08905)
*Koen Vellenga,H. Joe Steinhauer,Göran Falkman,Jonas Andersson,Anders Sjögren*

Main category: cs.LG

TL;DR: 研究探讨了对深度神经网络的最后一层采用哈密顿蒙特卡洛（HMC）抽样的概率方法，并将其用于驾驶员行为与意图的视频数据集分析，发现其在分类和分布外检测性能上竞争力较强。


<details>
  <summary>Details</summary>
Motivation: 现有HMC作为不确定性估计的标杆，但由于其高计算成本限制了其在大规模数据和神经网络上的适用性，提出限制HMC在深度神经网络最后一层以减少计算需求。

Method: 将HMC抽样方法应用于深度神经网络的最后一层（LL-HMC），并与五种概率深度学习方法进行比较，评估其分布内分类性能、校准能力及分布外检测能力。

Result: LL-HMC在分布内分类和分布外检测上表现出竞争力，额外采样的参数对分类性能影响不大，但有助于提升分布外检测；多链或不同初始点未带来显著改进。

Conclusion: 限制HMC至深度神经网络的最后一层是处理计算资源受限下数据密集场景的可行方法，并在不确定性与分布外检测方面表现出优势。

Abstract: We explore the use of Hamiltonian Monte Carlo (HMC) sampling as a
probabilistic last layer approach for deep neural networks (DNNs). While HMC is
widely regarded as a gold standard for uncertainty estimation, the
computational demands limit its application to large-scale datasets and large
DNN architectures. Although the predictions from the sampled DNN parameters can
be parallelized, the computational cost still scales linearly with the number
of samples (similar to an ensemble). Last layer HMC (LL--HMC) reduces the
required computations by restricting the HMC sampling to the final layer of a
DNN, making it applicable to more data-intensive scenarios with limited
computational resources. In this paper, we compare LL-HMC against five last
layer probabilistic deep learning (LL-PDL) methods across three real-world
video datasets for driver action and intention. We evaluate the in-distribution
classification performance, calibration, and out-of-distribution (OOD)
detection. Due to the stochastic nature of the probabilistic evaluations, we
performed five grid searches for different random seeds to avoid being reliant
on a single initialization for the hyperparameter configurations. The results
show that LL--HMC achieves competitive in-distribution classification and OOD
detection performance. Additional sampled last layer parameters do not improve
the classification performance, but can improve the OOD detection. Multiple
chains or starting positions did not yield consistent improvements.

</details>


### [295] [Fair-FLIP: Fair Deepfake Detection with Fairness-Oriented Final Layer Input Prioritising](https://arxiv.org/abs/2507.08912)
*Tomasz Szandala,Fatima Ezzeddine,Natalia Rusin,Silvia Giordano,Omran Ayoub*

Main category: cs.LG

TL;DR: 本研究提出了一种名为Fair-FLIP的后处理方法，旨在减少深度伪造检测中的偏见，同时维持检测准确性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决深度伪造检测中由于种族和性别等因素导致的偏见问题，并确保检测公平性。

Method: 提出Fair-FLIP方法，通过对训练模型的最终层输入进行重新加权处理，降低分组之间的差异性。

Result: Fair-FLIP方法相比基线和当前最先进方法，可提升公平性指标至高达30%，且精度仅微弱下降0.25%。

Conclusion: Fair-FLIP实现了在公平性和检测性能之间找到平衡，为未来深度伪造检测中的去偏应用提供了重要思路。

Abstract: Artificial Intelligence-generated content has become increasingly popular,
yet its malicious use, particularly the deepfakes, poses a serious threat to
public trust and discourse. While deepfake detection methods achieve high
predictive performance, they often exhibit biases across demographic attributes
such as ethnicity and gender. In this work, we tackle the challenge of fair
deepfake detection, aiming to mitigate these biases while maintaining robust
detection capabilities. To this end, we propose a novel post-processing
approach, referred to as Fairness-Oriented Final Layer Input Prioritising
(Fair-FLIP), that reweights a trained model's final-layer inputs to reduce
subgroup disparities, prioritising those with low variability while demoting
highly variable ones. Experimental results comparing Fair-FLIP to both the
baseline (without fairness-oriented de-biasing) and state-of-the-art approaches
show that Fair-FLIP can enhance fairness metrics by up to 30% while maintaining
baseline accuracy, with only a negligible reduction of 0.25%.
  Code is available on Github:
https://github.com/szandala/fair-deepfake-detection-toolbox

</details>


### [296] [Revisiting Convergence: Shuffling Complexity Beyond Lipschitz Smoothness](https://arxiv.org/abs/2507.08913)
*Qi He,Peiran Yu,Ziyi Chen,Heng Huang*

Main category: cs.LG

TL;DR: 本文重新审视了在没有Lipschitz平滑条件下的洗牌型梯度方法的收敛性，提出了一种新的步长策略，使算法可以在更弱假设下收敛，并验证了其在各种场景下具有较优的收敛率和实践性能。


<details>
  <summary>Details</summary>
Motivation: 许多机器学习模型不满足Lipschitz平滑条件，而现有的洗牌型梯度方法的收敛性研究多依赖于此过于强的假设，这限制了其应用范围。

Method: 提出了一种新的步长策略，并在一般有界方差条件下，研究了非凸、强凸及非强凸情况下两种洗牌条件（随机洗牌与任意洗牌）的收敛性，推导出算法的收敛速率。

Result: 新的步长策略下的洗牌型梯度算法在不同假设条件下均证明了收敛性，并达到了当前已知的最佳收敛速率。此外，数值实验结果也验证了算法的实际高效性。

Conclusion: 本文拓宽了洗牌型梯度方法的适用范围，使其在无需Lipschitz平滑假设的情况下仍具有优良的收敛性能与实践效果。

Abstract: Shuffling-type gradient methods are favored in practice for their simplicity
and rapid empirical performance. Despite extensive development of convergence
guarantees under various assumptions in recent years, most require the
Lipschitz smoothness condition, which is often not met in common machine
learning models. We highlight this issue with specific counterexamples. To
address this gap, we revisit the convergence rates of shuffling-type gradient
methods without assuming Lipschitz smoothness. Using our stepsize strategy, the
shuffling-type gradient algorithm not only converges under weaker assumptions
but also match the current best-known convergence rates, thereby broadening its
applicability. We prove the convergence rates for nonconvex, strongly convex,
and non-strongly convex cases, each under both random reshuffling and arbitrary
shuffling schemes, under a general bounded variance condition. Numerical
experiments further validate the performance of our shuffling-type gradient
algorithm, underscoring its practical efficacy.

</details>


### [297] [Beyond Scores: Proximal Diffusion Models](https://arxiv.org/abs/2507.08956)
*Zhenghan Fang,Mateo Díaz,Sam Buchanan,Jeremias Sulam*

Main category: cs.LG

TL;DR: 研究提出了一种替代方法，通过逆时间离散化和使用邻近算子，改进扩散模型的生成能力。


<details>
  <summary>Details</summary>
Motivation: 探讨使用邻近算子作为扩散模型中评分的替代方法，以提高理论收敛性和实际效率。

Method: 利用邻近匹配技术学习对数密度的邻近算子，从而开发出邻近扩散模型(ProxDM)，并采用逆时间离散化方法。

Result: 证明了该方法需要$\widetilde{O}(d/\sqrt{\varepsilon})$步即可生成与KL散度相关的$\varepsilon$-精确分布。实验显示该方法比传统评分匹配方法收敛更快。

Conclusion: ProxDM方法在理论收敛性和实验表现上均超过传统方法，表明邻近算子在扩散模型中的有效性。

Abstract: Diffusion models have quickly become some of the most popular and powerful
generative models for high-dimensional data. The key insight that enabled their
development was the realization that access to the score -- the gradient of the
log-density at different noise levels -- allows for sampling from data
distributions by solving a reverse-time stochastic differential equation (SDE)
via forward discretization, and that popular denoisers allow for unbiased
estimators of this score. In this paper, we demonstrate that an alternative,
backward discretization of these SDEs, using proximal maps in place of the
score, leads to theoretical and practical benefits. We leverage recent results
in proximal matching to learn proximal operators of the log-density and, with
them, develop Proximal Diffusion Models (ProxDM). Theoretically, we prove that
$\widetilde{O}(d/\sqrt{\varepsilon})$ steps suffice for the resulting
discretization to generate an $\varepsilon$-accurate distribution w.r.t. the KL
divergence. Empirically, we show that two variants of ProxDM achieve
significantly faster convergence within just a few sampling steps compared to
conventional score-matching methods.

</details>


### [298] [Graph Neural Network Enhanced Sequential Recommendation Method for Cross-Platform Ad Campaign](https://arxiv.org/abs/2507.08959)
*Xiang Li,Xinyu Wang,Yifan Lin*

Main category: cs.LG

TL;DR: 提出了一种基于图神经网络(GNN)的跨平台广告推荐方法，通过多维建模提升了推荐的精准度。


<details>
  <summary>Details</summary>
Motivation: 旨在解决跨平台广告推荐中的精准度问题，揭示用户兴趣迁移的潜在路径。

Method: 利用图神经网络，通过用户行为数据、广告内容和平台特征的多维建模，捕捉用户兴趣在平台之间迁移的规律，并通过超参数调整提升模型性能。

Result: 在三个平台的数据集上进行了实验，在平台B上AUC值达到0.937，其表现最佳，而平台A和C由于广告标签分布不均，精度和召回率有所下降。

Conclusion: 通过调整学习率、批量大小和嵌入维度等超参数，进一步提升了模型在异构数据中的适应性和鲁棒性。

Abstract: In order to improve the accuracy of cross-platform advertisement
recommendation, a graph neural network (GNN)- based advertisement
recommendation method is analyzed. Through multi-dimensional modeling, user
behavior data (e.g., click frequency, active duration) reveal temporal patterns
of interest evolution, ad content (e.g., type, tag, duration) influences
semantic preferences, and platform features (e.g., device type, usage context)
shape the environment where interest transitions occur. These factors jointly
enable the GNN to capture the latent pathways of user interest migration across
platforms. The experimental results are based on the datasets of three
platforms, and Platform B reaches 0.937 in AUC value, which is the best
performance. Platform A and Platform C showed a slight decrease in precision
and recall with uneven distribution of ad labels. By adjusting the
hyperparameters such as learning rate, batch size and embedding dimension, the
adaptability and robustness of the model in heterogeneous data are further
improved.

</details>


### [299] [Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models](https://arxiv.org/abs/2507.08965)
*Kevin Rojas,Ye He,Chieh-Hsin Lai,Yuta Takida,Yuki Mitsufuji,Molei Tao*

Main category: cs.LG

TL;DR: 本文研究了无分类器引导（CFG）在离散扩散模型中的作用，特别是在遮掩离散扩散中的引导调度问题，并提出了一种简单有效的改进方法。


<details>
  <summary>Details</summary>
Motivation: CFG在连续扩散模型中被广泛用于条件生成和提高样本质量，但其在离散扩散中的理论分析和引导调度的作用尚不明确。

Method: 通过理论分析CFG在遮掩离散扩散场景中的引导机制，发现现有实现的局限，并提出一种改进的无分类器引导方法，只需对代码进行一行更改即可实现。

Result: 在ImageNet和QM9数据集上的实验验证了改进方法在提高样本质量方面的有效性。

Conclusion: 本文的分析揭示了高引导在早期采样阶段会降低生成质量，而晚阶段效果更为显著，并通过提出的新方法改进了现有的CFG机制。

Abstract: Classifier-Free Guidance (CFG) is a widely used technique for conditional
generation and improving sample quality in continuous diffusion models, and
recent works have extended it to discrete diffusion. This paper theoretically
analyzes CFG in the context of masked discrete diffusion, focusing on the role
of guidance schedules. Our analysis shows that high guidance early in sampling
(when inputs are heavily masked) harms generation quality, while late-stage
guidance has a larger effect. These findings provide a theoretical explanation
for empirical observations in recent studies on guidance schedules. The
analysis also reveals an imperfection of the current CFG implementations. These
implementations can unintentionally cause imbalanced transitions, such as
unmasking too rapidly during the early stages of generation, which degrades the
quality of the resulting samples. To address this, we draw insight from the
analysis and propose a novel classifier-free guidance mechanism empirically
applicable to any discrete diffusion. Intuitively, our method smoothens the
transport between the data distribution and the initial (masked/uniform)
distribution, which results in improved sample quality. Remarkably, our method
is achievable via a simple one-line code change. The efficacy of our method is
empirically demonstrated with experiments on ImageNet (masked discrete
diffusion) and QM9 (uniform discrete diffusion).

</details>


### [300] [ToxBench: A Binding Affinity Prediction Benchmark with AB-FEP-Calculated Labels for Human Estrogen Receptor Alpha](https://arxiv.org/abs/2507.08966)
*Meng Liu,Karl Leswing,Simon K. S. Chu,Farhad Ramezanghorbani,Griffin Young,Gabriel Marques,Prerna Das,Anjali Panikar,Esther Jamir,Mohammed Sulaiman Shamsudeen,K. Shawn Watts,Ananya Sen,Hari Priya Devannagari,Edward B. Miller,Muyun Lihan,Howook Hwang,Janet Paulsen,Xin Yu,Kyle Gion,Timur Rvachov,Emine Kucukbenli,Saee Gopal Paliwal*

Main category: cs.LG

TL;DR: 研究通过引入大规模物理化学数据库（ToxBench）和基于机器学习的模型（DualBind），在快速计算蛋白质-小分子结合亲和力方面取得突破。


<details>
  <summary>Details</summary>
Motivation: 当前蛋白质-配体结合亲和力的预测受到高精度物理化学计算耗时和可靠数据稀缺的限制。

Method: 开发并使用首个专注于人雌激素受体（ERα）的大规模物理化学计算数据集ToxBench，并提出一种基于双损失学习框架的DualBind模型进行机器学习方法的对比与优化。

Result: DualBind在测试中展现了出色的预测能力，在保证计算准确性的同时显著减小了计算成本，相较AB-FEP方法更适应高通量应用。

Conclusion: 通过ToxBench数据集及DualBind模型，证明机器学习方法可以有效模仿传统物理化学方法，实现快速且准确的生物分子结合预测应用，为药物发现与毒性评估提供新方向。

Abstract: Protein-ligand binding affinity prediction is essential for drug discovery
and toxicity assessment. While machine learning (ML) promises fast and accurate
predictions, its progress is constrained by the availability of reliable data.
In contrast, physics-based methods such as absolute binding free energy
perturbation (AB-FEP) deliver high accuracy but are computationally prohibitive
for high-throughput applications. To bridge this gap, we introduce ToxBench,
the first large-scale AB-FEP dataset designed for ML development and focused on
a single pharmaceutically critical target, Human Estrogen Receptor Alpha
(ER$\alpha$). ToxBench contains 8,770 ER$\alpha$-ligand complex structures with
binding free energies computed via AB-FEP with a subset validated against
experimental affinities at 1.75 kcal/mol RMSE, along with non-overlapping
ligand splits to assess model generalizability. Using ToxBench, we further
benchmark state-of-the-art ML methods, and notably, our proposed DualBind
model, which employs a dual-loss framework to effectively learn the binding
energy function. The benchmark results demonstrate the superior performance of
DualBind and the potential of ML to approximate AB-FEP at a fraction of the
computational cost.

</details>


### [301] [Simulating Three-dimensional Turbulence with Physics-informed Neural Networks](https://arxiv.org/abs/2507.08972)
*Sifan Wang,Shyam Sankaran,Panos Stinis,Paris Perdikaris*

Main category: cs.LG

TL;DR: 本文提出用物理约束神经网络(PINNs)高效模拟湍流流体流动，并通过算法创新克服混沌动力学的挑战，从而提供网格自由、连续的解。


<details>
  <summary>Details</summary>
Motivation: 现有湍流模拟因计算资源受限而难以实现高效，作者希望设计基于物理方程的神经网络解决此限制。

Method: 将PINNs应用于湍流流体模拟，并采用自适应网络结构、因果训练和高级优化方法应对混沌动力学问题。

Result: 验证表明PINNs能精确再现能源谱、动能、涡度和雷诺应力等湍流关键统计特性。

Conclusion: 通过创新性的算法设计，PINNs成功实现湍流建模，有望突破传统模拟方法的计算限制。

Abstract: Turbulent fluid flows are among the most computationally demanding problems
in science, requiring enormous computational resources that become prohibitive
at high flow speeds. Physics-informed neural networks (PINNs) represent a
radically different approach that trains neural networks directly from physical
equations rather than data, offering the potential for continuous, mesh-free
solutions. Here we show that appropriately designed PINNs can successfully
simulate fully turbulent flows in both two and three dimensions, directly
learning solutions to the fundamental fluid equations without traditional
computational grids or training data. Our approach combines several algorithmic
innovations including adaptive network architectures, causal training, and
advanced optimization methods to overcome the inherent challenges of learning
chaotic dynamics. Through rigorous validation on challenging turbulence
problems, we demonstrate that PINNs accurately reproduce key flow statistics
including energy spectra, kinetic energy, enstrophy, and Reynolds stresses. Our
results demonstrate that neural equation solvers can handle complex chaotic
systems, opening new possibilities for continuous turbulence modeling that
transcends traditional computational limitations.

</details>


### [302] [Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery](https://arxiv.org/abs/2507.08977)
*Carson Dudley,Reiden Magdaleno,Christopher Harding,Marisa Eisenberg*

Main category: cs.LG

TL;DR: 本研究提出了基于模拟的神经网络（SGNNs），通过机械模拟数据来预训练神经网络，从而实现更为灵活和可解释的科学建模。


<details>
  <summary>Details</summary>
Motivation: 当前的科学建模手段存在两难：机械模型具备可解释性但难以应对复杂现实，而机器学习模型灵活但需要大量标记数据，无法推出不可观测量，并且通常是黑盒。

Method: 引入SGNNs框架，通过多样的机械模拟数据集对神经网络进行预训练，这些数据包括不同结构、参数范围、随机性及观测伪影的模拟情况。同时，SGNNs还能实现基于模拟的反向归因分析，为模型给出过程性解释。

Result: SGNNs在多个学科和建模任务中达到了最先进水平：例如在预测任务中，COVID-19预测技能几乎是CDC基线的三倍；减少三分之一化学产率预测误差，并在生态预测中比特定任务模型表现更好。在推断任务中，SGNNs能分类信息传播来源，并在不可观测目标（如COVID-19传播率估计）上表现优于传统方法。

Conclusion: SGNNs集成了科学理论与深度学习的灵活性，开创了一种新型建模范式，将模拟从僵化的后验工具转变为灵活的监督来源，实现了即便无真实数据时也能开展稳健、可解释的推断。

Abstract: Scientific modeling faces a core limitation: mechanistic models offer
interpretability but collapse under real-world complexity, while machine
learning models are flexible but require large labeled datasets, cannot infer
unobservable quantities, and operate as black boxes. We introduce
Simulation-Grounded Neural Networks (SGNNs), a general framework that uses
mechanistic simulations as training data for neural networks. SGNNs are
pretrained on synthetic corpora spanning diverse model structures, parameter
regimes, stochasticity, and observational artifacts. We evaluated SGNNs across
scientific disciplines and modeling tasks, and found that SGNNs achieved
state-of-the-art results across settings: for prediction tasks, they nearly
tripled COVID-19 forecasting skill versus CDC baselines, reduced chemical yield
prediction error by one third, and maintained accuracy in ecological
forecasting where task specific models failed. For inference tasks, SGNNs also
accurately classified the source of information spread in simulated social
networks and enabled supervised learning for unobservable targets, such as
estimating COVID-19 transmissibility more accurately than traditional methods
even in early outbreaks. Finally, SGNNs enable back-to-simulation attribution,
a new form of mechanistic interpretability. Given real world input, SGNNs
retrieve simulations based on what the model has learned to see as most
similar, revealing which underlying dynamics the model believes are active.
This provides process-level insight -- what the model thinks is happening --
not just which features mattered. SGNNs unify scientific theory with deep
learning flexibility and unlock a new modeling paradigm -- transforming
simulations from rigid, post hoc tools into flexible sources of supervision,
enabling robust, interpretable inference even when ground truth is missing.

</details>


### [303] [Learning Diffusion Models with Flexible Representation Guidance](https://arxiv.org/abs/2507.08980)
*Chenyu Wang,Cai Zhou,Sharut Gupta,Zongyu Lin,Stefanie Jegelka,Stephen Bates,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 提出了一种系统框架，将表示指导纳入扩散模型，通过优化步骤加速训练并改善生成质量。


<details>
  <summary>Details</summary>
Motivation: 探索通过对齐预训练模型的内部表示来提高扩散模型生成质量的潜力。

Method: 提出了替代分解方法和训练准则，引入两种策略：基于目标表示的多模态配对和优化的训练课程。

Result: 在图像、蛋白质序列和分子生成任务中表现出优越性能，并大幅加速训练，在ImageNet基准中实现显著训练加速。

Conclusion: 通过表示指导优化扩散模型的训练效率和生成质量，并提供了一种理论驱动的方法。

Abstract: Diffusion models can be improved with additional guidance towards more
effective representations of input. Indeed, prior empirical work has already
shown that aligning internal representations of the diffusion model with those
of pre-trained models improves generation quality. In this paper, we present a
systematic framework for incorporating representation guidance into diffusion
models. We provide alternative decompositions of denoising models along with
their associated training criteria, where the decompositions determine when and
how the auxiliary representations are incorporated. Guided by our theoretical
insights, we introduce two new strategies for enhancing representation
alignment in diffusion models. First, we pair examples with target
representations either derived from themselves or arisen from different
synthetic modalities, and subsequently learn a joint model over the multimodal
pairs. Second, we design an optimal training curriculum that balances
representation learning and data generation. Our experiments across image,
protein sequence, and molecule generation tasks demonstrate superior
performance as well as accelerated training. In particular, on the
class-conditional ImageNet $256\times 256$ benchmark, our guidance results in
$23.3$ times faster training than the original SiT-XL as well as four times
speedup over the state-of-the-art method REPA. The code is available at
https://github.com/ChenyuWang-Monica/REED.

</details>


### [304] [Exploiting Leaderboards for Large-Scale Distribution of Malicious Models](https://arxiv.org/abs/2507.08983)
*Anshuman Suri,Harsh Chaudhari,Yuefeng Peng,Ali Naseh,Amir Houmansadr,Alina Oprea*

Main category: cs.LG

TL;DR: 论文探讨了模型排行榜作为传播中毒模型的潜在途径，提出框架“TrojanClimb”用于证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是模型排行榜可能成为攻击者分发中毒模型的强力渠道，需识别其机制和安全漏洞。

Method: 提出一个通用框架“TrojanClimb”，通过多个模态实验展示如何隐蔽地注入恶意行为，同时保持排行榜竞争力。

Result: 实验展示攻击者能够在排行榜中取得高排名的同时嵌入后门或加入偏见等恶意功能。

Conclusion: 揭示了机器学习生态系统的重大漏洞，呼吁重新设计排行榜评估机制以检测和过滤中毒模型，同时警告使用未经验证模型的安全风险。

Abstract: While poisoning attacks on machine learning models have been extensively
studied, the mechanisms by which adversaries can distribute poisoned models at
scale remain largely unexplored. In this paper, we shed light on how model
leaderboards -- ranked platforms for model discovery and evaluation -- can
serve as a powerful channel for adversaries for stealthy large-scale
distribution of poisoned models. We present TrojanClimb, a general framework
that enables injection of malicious behaviors while maintaining competitive
leaderboard performance. We demonstrate its effectiveness across four diverse
modalities: text-embedding, text-generation, text-to-speech and text-to-image,
showing that adversaries can successfully achieve high leaderboard rankings
while embedding arbitrary harmful functionalities, from backdoors to bias
injection. Our findings reveal a significant vulnerability in the machine
learning ecosystem, highlighting the urgent need to redesign leaderboard
evaluation mechanisms to detect and filter malicious (e.g., poisoned) models,
while exposing broader security implications for the machine learning community
regarding the risks of adopting models from unverified sources.

</details>


### [305] [Multimodal Cardiovascular Risk Profiling Using Self-Supervised Learning of Polysomnography](https://arxiv.org/abs/2507.09009)
*Zhengxiao He,Huayu Li,Geng Yuan,William D. S. Killgore,Stuart F. Quan,Chen X. Chen,Ao Li*

Main category: cs.LG

TL;DR: 开发了一种自监督深度学习模型，用于分析多模态生理信号（EEG、ECG和呼吸信号），预测心血管疾病（CVD）风险。这一框架在两个独立队列中测试并验证了其有效性，生成了能显著提高风险预测能力的个性化CVD风险评分。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以高效、个性化地评估心血管疾病风险，需要从多模态生理数据中寻找能临床应用的新型评估方法。

Method: 开发了自监督深度学习模型，分析4,398名参与者的多模态信号数据，生成对比不同CVD结局的投影分数，并通过独立的1,093人队列进行验证。

Result: 模型揭示了具有临床意义的跨模态特征模式，ECG特征可预测心脏病和CVD死亡率，EEG特征适用于预测高血压和CVD死亡率，呼吸信号具备补充价值；结合Framingham风险评分显著提升了预测性能，外部队列验证了结果的稳健性。

Conclusion: 研究表明，该框架能够直接从PSG数据生成个性化的CVD风险评分，可为临床实践提供支持，提升风险评估能力，实现个性化的医疗方案。

Abstract: Methods: We developed a self-supervised deep learning model that extracts
meaningful patterns from multi-modal signals (Electroencephalography (EEG),
Electrocardiography (ECG), and respiratory signals). The model was trained on
data from 4,398 participants. Projection scores were derived by contrasting
embeddings from individuals with and without CVD outcomes. External validation
was conducted in an independent cohort with 1,093 participants. The source code
is available on https://github.com/miraclehetech/sleep-ssl. Results: The
projection scores revealed distinct and clinically meaningful patterns across
modalities. ECG-derived features were predictive of both prevalent and incident
cardiac conditions, particularly CVD mortality. EEG-derived features were
predictive of incident hypertension and CVD mortality. Respiratory signals
added complementary predictive value. Combining these projection scores with
the Framingham Risk Score consistently improved predictive performance,
achieving area under the curve values ranging from 0.607 to 0.965 across
different outcomes. Findings were robustly replicated and validated in the
external testing cohort. Conclusion: Our findings demonstrate that the proposed
framework can generate individualized CVD risk scores directly from PSG data.
The resulting projection scores have the potential to be integrated into
clinical practice, enhancing risk assessment and supporting personalized care.

</details>


### [306] [Enhancing RLHF with Human Gaze Modeling](https://arxiv.org/abs/2507.09016)
*Karim Galliamov,Ivan Titov,Ilya Pershin*

Main category: cs.LG

TL;DR: 该研究通过结合人类凝视建模，提升了与人类偏好对齐的强化学习效率。


<details>
  <summary>Details</summary>
Motivation: 目前，人类反馈的强化学习（RLHF）计算代价高昂，研究团队旨在通过引入人类凝视建模改进RLHF效率。

Method: 研究包括两种方法：基于凝视的奖励模型和在词语级别分配稀疏奖励的方法。

Result: 实验显示，结合人类凝视的RLHF在保持或略微改善性能的同时实现更快的收敛，降低了策略优化的计算成本。

Conclusion: 人类凝视是一个有价值且未被充分利用的信号，有助于提高RLHF效率，为进一步改进政策优化提供了新的方向。

Abstract: Reinforcement Learning from Human Feedback (RLHF) aligns language models with
human preferences but is computationally expensive. We explore two approaches
that leverage human gaze modeling to enhance RLHF: (1) gaze-aware reward models
and (2) gaze-based distribution of sparse rewards at token level. Our
experiments demonstate that gaze-informed RLHF achieves faster convergence
while maintaining or slightly improving performance, thus, reducing
computational costs during policy optimization. These results show that human
gaze provides a valuable and underused signal for policy optimization, pointing
to a promising direction for improving RLHF efficiency.

</details>


### [307] [On Evaluating Performance of LLM Inference Serving Systems](https://arxiv.org/abs/2507.09019)
*Amey Agrawal,Nitin Kedia,Anmol Agarwal,Jayashree Mohan,Nipun Kwatra,Souvik Kundu,Ramachandran Ramjee,Alexey Tumanov*

Main category: cs.LG

TL;DR: LLM推理系统评估方法存在问题，常见于基线比较、公平性等方面，并提出框架解决。


<details>
  <summary>Details</summary>
Motivation: 改进当前存在缺陷的LLM推理评估方法，避免反模式，促进科学进步。

Method: 通过分析现有系统的反模式，提出评估检查清单与框架，并进行案例研究验证。

Result: 揭示评估方法中的反模式问题，开发出实用的框架，展示其在推理系统中的实际效用。

Conclusion: 提出的框架改善了LLM推理评估，推动了科学研究的真实进展和技术优化。

Abstract: The rapid evolution of Large Language Model (LLM) inference systems has
yielded significant efficiency improvements. However, our systematic analysis
reveals that current evaluation methodologies frequently exhibit fundamental
flaws, often manifesting as common evaluation anti-patterns that obscure true
performance characteristics and impede scientific progress. Through a
comprehensive examination of recent systems, we identify recurring
anti-patterns across three key dimensions: Baseline Fairness, Evaluation Setup,
and Metric Design. These anti-patterns are uniquely problematic for LLM
inference due to its dual-phase nature combining distinct prefill and decode
operations, its handling of highly heterogeneous workloads, and its strict
temporal requirements for interactive use. We demonstrate how common
anti-patterns -- such as inadequate baseline comparisons that conflate
engineering effort with algorithmic novelty, workload selections that fail to
represent production scenarios, and metric normalizations that hide substantial
performance variability like generation stalls-lead to misleading conclusions.
To address these challenges, we provide a comprehensive checklist derived from
our analysis, establishing a framework for recognizing and avoiding these
anti-patterns in favor of robust LLM inference evaluation. To demonstrate the
practical application of our framework, we present a case study analyzing
speculative decoding, a technique whose bursty, non-uniform token generation is
easily misinterpreted when evaluated using approaches characteristic of these
anti-patterns. Our work establishes a rigorous foundation for evaluation
methodology, enabling meaningful comparisons, ensuring reproducible results,
and ultimately accelerating genuine progress in LLM inference systems by moving
beyond common anti-patterns to align evaluation with real-world requirements.

</details>


### [308] [Model Parallelism With Subnetwork Data Parallelism](https://arxiv.org/abs/2507.09029)
*Vaibhav Singh,Zafir Khalid,Edouard Oyallon,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 提出通过在不同节点独立训练小型结构化子网络来替代传统分布式大模型预训练的方法，大幅降低内存需求和通信成本，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决分布式预训练中节点内存需求大和通信成本高的问题。

Method: 通过在不同节点上训练结构化子网络，避免跨节点激活通信，并探索基于参数均匀分布原则的两种子网络构造策略。

Result: 采用随机块丢弃技术的子网络优于在联邦学习中使用的宽度分割方法，可以实现20-40%的内存节约，且性能无损。

Conclusion: 提出的方法不仅降低了整体内存开销，还保留了模型性能，具备进一步研究的潜力。

Abstract: Distributed pre-training of large models at scale often imposes heavy memory
demands on individual nodes and incurs significant intra-node communication
costs. We propose a novel alternative approach that reduces the memory
requirements by training small, structured subnetworks of the model on separate
workers. Unlike pipelining, our method avoids inter-node activation
communication and maintains bandwidth requirements that are comparable to or
lower than standard data parallel communication schemes based on all-reduce. We
evaluate two subnetwork construction strategies guided by the principle of
ensuring uniform representation of each parameter across the distributed
training setup. Our results show that the stochastic block dropping technique
consistently outperforms the width-wise subnetwork construction previously
explored in federated learning. We empirically attribute this superior
performance to stronger gradient alignment in subnetworks that retain blocks
having skip connections. Preliminary experiments highlight the promise of our
approach, achieving a 20-40% reduction in memory usage without any loss in
performance.

</details>


### [309] [Confounder-Free Continual Learning via Recursive Feature Normalization](https://arxiv.org/abs/2507.09031)
*Yash Shah,Camila Gonzalez,Mohammad H. Abbasi,Qingyu Zhao,Kilian M. Pohl,Ehsan Adeli*

Main category: cs.LG

TL;DR: 本文提出了一种可以用于消除混杂变量影响的递归元数据归一化层(R-MDN)，旨在静态学习及持续学习情境中实现对混杂变量的去除。


<details>
  <summary>Details</summary>
Motivation: 混杂变量会对输入与目标变量同时产生影响，引发虚假相关并导致预测偏差。在持续学习场景中，如何持续学习对混杂变量不敏感的特征表示是一个重大挑战。

Method: 论文提出了一种新型的递归元数据归一化层(R-MDN)，利用递归最小二乘算法对数据分布及混杂变量进行持续统计回归更新，可以整合到任意深度学习模型架构中。

Result: 实验结果表明，R-MDN能够在静态学习及持续学习的不同阶段中，通过减少混杂变量引起的灾难性遗忘，实现跨群体更公平的预测表现。

Conclusion: R-MDN方法为深度学习模型在处理混杂变量问题时提供了一种有效的解决方案。

Abstract: Confounders are extraneous variables that affect both the input and the
target, resulting in spurious correlations and biased predictions. There are
recent advances in dealing with or removing confounders in traditional models,
such as metadata normalization (MDN), where the distribution of the learned
features is adjusted based on the study confounders. However, in the context of
continual learning, where a model learns continuously from new data over time
without forgetting, learning feature representations that are invariant to
confounders remains a significant challenge. To remove their influence from
intermediate feature representations, we introduce the Recursive MDN (R-MDN)
layer, which can be integrated into any deep learning architecture, including
vision transformers, and at any model stage. R-MDN performs statistical
regression via the recursive least squares algorithm to maintain and
continually update an internal model state with respect to changing
distributions of data and confounding variables. Our experiments demonstrate
that R-MDN promotes equitable predictions across population groups, both within
static learning and across different stages of continual learning, by reducing
catastrophic forgetting caused by confounder effects changing over time.

</details>


### [310] [Behavioral Exploration: Learning to Explore via In-Context Adaptation](https://arxiv.org/abs/2507.09041)
*Andrew Wagenmaker,Zhiyuan Zhou,Sergey Levine*

Main category: cs.LG

TL;DR: 这篇论文提出了一种训练自主体进行快速线上适应与探索的新方法，借鉴了大规模行为克隆和上下文学习的最新进展，训练模型模仿专家行为并实现类似专家的探索。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖随机探索和缓慢的梯度更新，无法像人类一样快速适应与获取技能。因此，作者希望为自主体赋予类似人类的快速线上适应和探索能力。

Method: 使用专家示范数据集，训练一个长上下文生成模型，模型根据过去的观测及专家行为的探索性预测专家行动，使自主体能模仿专家行为并通过上下文选择新的探索策略。

Result: 实验结果表明，该方法在模拟的运动与操作环境以及真实机器人任务中均实现了自适应、探索性行为。

Conclusion: 通过上下文行为预测和专家示范学习，自主体在快速线上适应和探索方面接近人类水平。

Abstract: Developing autonomous agents that quickly explore an environment and adapt
their behavior online is a canonical challenge in robotics and machine
learning. While humans are able to achieve such fast online exploration and
adaptation, often acquiring new information and skills in only a handful of
interactions, existing algorithmic approaches tend to rely on random
exploration and slow, gradient-based behavior updates. How can we endow
autonomous agents with such capabilities on par with humans? Taking inspiration
from recent progress on both in-context learning and large-scale behavioral
cloning, in this work we propose behavioral exploration: training agents to
internalize what it means to explore and adapt in-context over the space of
``expert'' behaviors. To achieve this, given access to a dataset of expert
demonstrations, we train a long-context generative model to predict expert
actions conditioned on a context of past observations and a measure of how
``exploratory'' the expert's behaviors are relative to this context. This
enables the model to not only mimic the behavior of an expert, but also, by
feeding its past history of interactions into its context, to select different
expert behaviors than what have been previously selected, thereby allowing for
fast online adaptation and targeted, ``expert-like'' exploration. We
demonstrate the effectiveness of our method in both simulated locomotion and
manipulation settings, as well as on real-world robotic manipulation tasks,
illustrating its ability to learn adaptive, exploratory behavior.

</details>


### [311] [Shortening the Trajectories: Identity-Aware Gaussian Approximation for Efficient 3D Molecular Generation](https://arxiv.org/abs/2507.09043)
*Jingxiang Qu,Wenhan Gao,Yi Liu*

Main category: cs.LG

TL;DR: 该论文介绍了一种通过理论分析识别数据达到高斯分布的特征步骤，并用封闭形式的高斯近似代替冗余生成过程的框架，显著提升了生成效率和样本质量。


<details>
  <summary>Details</summary>
Motivation: 现有的高斯随机生成模型尽管表现优异，但其生成轨迹长、计算成本高，限制了其实用性。

Method: 通过理论分析确定数据达到‘高斯化’分布的特征步骤，并用封闭式高斯近似替代冗长的生成轨迹，同时保持学习动态的全分辨率。

Result: 在多个数据模式上，实验表明该方法显著提升样本质量和计算效率。

Conclusion: 提出的框架在保留训练与推断精度的同时，大幅改善了生成效率，是现有高斯生成模型在实际应用中的重要改进。

Abstract: Gaussian-based Probabilistic Generative Models (GPGMs) generate data by
reversing a stochastic process that progressively corrupts samples with
Gaussian noise. While these models have achieved state-of-the-art performance
across diverse domains, their practical deployment remains constrained by the
high computational cost of long generative trajectories, which often involve
hundreds to thousands of steps during training and sampling. In this work, we
introduce a theoretically grounded and empirically validated framework that
improves generation efficiency without sacrificing training granularity or
inference fidelity. Our key insight is that for certain data modalities, the
noising process causes data to rapidly lose its identity and converge toward a
Gaussian distribution. We analytically identify a characteristic step at which
the data has acquired sufficient Gaussianity, and then replace the remaining
generation trajectory with a closed-form Gaussian approximation. Unlike
existing acceleration techniques that coarsening the trajectories by skipping
steps, our method preserves the full resolution of learning dynamics while
avoiding redundant stochastic perturbations between `Gaussian-like'
distributions. Empirical results across multiple data modalities demonstrate
substantial improvements in both sample quality and computational efficiency.

</details>


### [312] [Imitation Learning in Continuous Action Spaces: Mitigating Compounding Error without Interaction](https://arxiv.org/abs/2507.09061)
*Thomas T. Zhang,Daniel Pfrommer,Nikolai Matni,Max Simchowitz*

Main category: cs.LG

TL;DR: 研究连续状态和动作动态系统中的模仿专家示范问题，通过提出最小干预措施缓解复合错误问题，并提出动作分块与噪声注入技术。


<details>
  <summary>Details</summary>
Motivation: 探索模仿学习在物理环境中的复杂性，以及因复合错误问题导致的稳定性挑战。

Method: 提出动作分块（预测并执行开放环动作序列）和噪声注入（在专家示范中加入噪声）技术，结合控制理论和强化学习工具进行分析。

Result: 验证这些干预措施可以显著减少复合错误问题，并从根本上提供稳定的模仿学习解决方案。

Conclusion: 研究揭示了结合控制理论和强化学习的新型理论考量，提出的技术改进了连续状态和动作模仿学习的性能与稳定性。

Abstract: We study the problem of imitating an expert demonstrator in a continuous
state-and-action dynamical system. While imitation learning in discrete
settings such as autoregressive language modeling has seen immense success and
popularity in recent years, imitation in physical settings such as autonomous
driving and robot learning has proven comparably more complex due to the
compounding errors problem, often requiring elaborate set-ups to perform
stably. Recent work has demonstrated that even in benign settings, exponential
compounding errors are unavoidable when learning solely from expert-controlled
trajectories, suggesting the need for more advanced policy parameterizations or
data augmentation. To this end, we present minimal interventions that provably
mitigate compounding errors in continuous state-and-action imitation learning.
When the system is open-loop stable, we prescribe "action chunking," i.e.,
predicting and playing sequences of actions in open-loop; when the system is
possibly unstable, we prescribe "noise injection," i.e., adding noise during
expert demonstrations. These interventions align with popular choices in modern
robot learning, though the benefits we derive are distinct from the effects
they were designed to target. Our results draw insights and tools from both
control theory and reinforcement learning; however, our analysis reveals novel
considerations that do not naturally arise when either literature is considered
in isolation.

</details>


### [313] [Queue up for takeoff: a transferable deep learning framework for flight delay prediction](https://arxiv.org/abs/2507.09084)
*Nnamdi Daniel Aghanya,Ta Duong Vu,Amaëlle Diop,Charlotte Deville,Nour Imane Kerroumi,Irene Moulitsas,Jun Li,Desmond Bisandu*

Main category: cs.LG

TL;DR: 论文提出了QT-SimAM（Queue-Theory SimAM）模型，用于预测航班延误，在不同数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 航班延误对航空业带来重大财务和运营挑战，需要精确且适应性强的预测模型来改善乘客体验并减少收入损失。

Method: 结合队列理论和简单注意力模型（SimAM），设计了QT-SimAM模型，并验证其在美国运输部和欧洲管控数据集上的表现。

Result: QT-SimAM（双向）模型在美国数据集上取得了0.927的准确率和0.932的F1分数，在欧洲数据集上取得了0.826的准确率和0.791的F1分数。

Conclusion: 提出了QT-SimAM模型，这是一种高效的端到端航班延误预测方法，可提高不同网络上的预测准确性，有助于缓解乘客焦虑并改善运营决策。

Abstract: Flight delays are a significant challenge in the aviation industry, causing
major financial and operational disruptions. To improve passenger experience
and reduce revenue loss, flight delay prediction models must be both precise
and generalizable across different networks. This paper introduces a novel
approach that combines Queue-Theory with a simple attention model, referred to
as the Queue-Theory SimAM (QT-SimAM). To validate our model, we used data from
the US Bureau of Transportation Statistics, where our proposed QT-SimAM
(Bidirectional) model outperformed existing methods with an accuracy of 0.927
and an F1 score of 0.932. To assess transferability, we tested the model on the
EUROCONTROL dataset. The results demonstrated strong performance, achieving an
accuracy of 0.826 and an F1 score of 0.791. Ultimately, this paper outlines an
effective, end-to-end methodology for predicting flight delays. The proposed
model's ability to forecast delays with high accuracy across different networks
can help reduce passenger anxiety and improve operational decision-making

</details>


### [314] [Deep Reinforcement Learning with Gradient Eligibility Traces](https://arxiv.org/abs/2507.09087)
*Esraa Elelimy,Brett Daley,Andrew Patterson,Marlos C. Machado,Adam White,Martha White*

Main category: cs.LG

TL;DR: 本论文扩展了广义投影贝尔曼误差（GPBE）目标以支持多步权值分配，提出三种基于梯度的方法并在MuJoCo和MinAtar环境中取得了优于PPO与StreamQ的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的深度强化学习方法主要依赖半梯度时序差分方法，容易出现发散问题；而GT方法收敛性强，却很少用于深度强化学习。同时之前的工作仅限于一步方法，较慢且需要大量样本。

Method: 将$GPBE$目标扩展至支持基于$\lambda$回报的多步权值分配，并设计三种优化新目标的梯度方法，提出正向视图和反向视图两种公式以适配不同的算法特性。

Result: 所提算法在MuJoCo和MinAtar环境中的表现优于PPO与StreamQ方法。

Conclusion: 新方法成功结合GPBE与多步权值分配，高效且表现优越，为深度强化学习提供了一种新的可行方案。

Abstract: Achieving fast and stable off-policy learning in deep reinforcement learning
(RL) is challenging. Most existing methods rely on semi-gradient
temporal-difference (TD) methods for their simplicity and efficiency, but are
consequently susceptible to divergence. While more principled approaches like
Gradient TD (GTD) methods have strong convergence guarantees, they have rarely
been used in deep RL. Recent work introduced the Generalized Projected Bellman
Error ($\GPBE$), enabling GTD methods to work efficiently with nonlinear
function approximation. However, this work is only limited to one-step methods,
which are slow at credit assignment and require a large number of samples. In
this paper, we extend the $\GPBE$ objective to support multistep credit
assignment based on the $\lambda$-return and derive three gradient-based
methods that optimize this new objective. We provide both a forward-view
formulation compatible with experience replay and a backward-view formulation
compatible with streaming algorithms. Finally, we evaluate the proposed
algorithms and show that they outperform both PPO and StreamQ in MuJoCo and
MinAtar environments, respectively. Code available at
https://github.com/esraaelelimy/gtd\_algos

</details>


### [315] [Continuous-Time Signal Decomposition: An Implicit Neural Generalization of PCA and ICA](https://arxiv.org/abs/2507.09091)
*Shayan K. Azmoodeh,Krishna Subramani,Paris Smaragdis*

Main category: cs.LG

TL;DR: 该论文扩展了PCA和ICA问题到连续时间向量信号领域，提出了一种基于模型不可知的隐式神经信号表示框架。


<details>
  <summary>Details</summary>
Motivation: 解决PCA和ICA在处理点云和非规则采样信号时的局限性。

Method: 将信号建模为连续时间随机过程，使用对比函数项在网络损失中实现信号解耦和独立性。

Result: 在连续时间不规则采样场景中实现了信号的低秩分解，适用性比传统方法更广。

Conclusion: 提出的方法能有效解决标准方法无法处理的连续场景中的低秩分解问题。

Abstract: We generalize the low-rank decomposition problem, such as principal and
independent component analysis (PCA, ICA) for continuous-time vector-valued
signals and provide a model-agnostic implicit neural signal representation
framework to learn numerical approximations to solve the problem. Modeling
signals as continuous-time stochastic processes, we unify the approaches to
both the PCA and ICA problems in the continuous setting through a contrast
function term in the network loss, enforcing the desired statistical properties
of the source signals (decorrelation, independence) learned in the
decomposition. This extension to a continuous domain allows the application of
such decompositions to point clouds and irregularly sampled signals where
standard techniques are not applicable.

</details>


### [316] [Principled Foundations for Preference Optimization](https://arxiv.org/abs/2507.07855)
*Wenxuan Zhou,Shujian Zhang,Brice Magdalou,John Lambert,Ehsan Amid,Richard Nock,Andrew Hard*

Main category: cs.LG

TL;DR: 本文探讨了偏好优化（DPO）与两大理论（损失函数与随机选择）的关联，并扩展了DPO的应用与理论框架。


<details>
  <summary>Details</summary>
Motivation: 由于DPO模型在实际应用中的广泛性以及当前学术界对DPO的关注，需要从原则性角度理解DPO的运作方式及其延展性。

Method: 通过描述DPO作为连接两个理论的特定形式，支持选择上的弃权、非凸优化目标，并补充DPO的关键扩展如边界与长度校正。

Result: 在一般性框架下，揭示了DPO设定的局限性、扩展可能性以及偏离该框架可能产生的问题与解决对策。

Conclusion: 研究不仅丰富了对DPO的理论理解，还启发了其在广泛应用中的优化与补充方向。

Abstract: In this paper, we show that direct preference optimization (DPO) is a very
specific form of a connection between two major theories in the ML context of
learning from preferences: loss functions (Savage) and stochastic choice
(Doignon-Falmagne and Machina). The connection is established for all of
Savage's losses and at this level of generality, (i) it includes support for
abstention on the choice theory side, (ii) it includes support for non-convex
objectives on the ML side, and (iii) it allows to frame for free some notable
extensions of the DPO setting, including margins and corrections for length.
Getting to understand how DPO operates from a general principled perspective is
crucial because of the huge and diverse application landscape of models,
because of the current momentum around DPO, but also -- and importantly --
because many state of the art variations on DPO definitely occupy a small
region of the map that we cover. It also helps to understand the pitfalls of
departing from this map, and figure out workarounds.

</details>


### [317] [On the Fragility of Multimodal Perception to Temporal Misalignment in Autonomous Driving](https://arxiv.org/abs/2507.09095)
*Md Hasan Shahriar,Md Mohaimin Al Barat,Harshavardhan Sundar,Naren Ramakrishnan,Y. Thomas Hou,Wenjing Lou*

Main category: cs.LG

TL;DR: 提出了DejaVu攻击，通过制造传感器流的时间错位破坏自主驾驶的多模态融合感知；并提出AION防御方案，有效监测并抵御这种攻击。


<details>
  <summary>Details</summary>
Motivation: 研究多模态融合感知中对时间同步的依赖及其潜在漏洞。

Method: 设计了DejaVu攻击，通过网络延迟制造摄像头和LiDAR传感器流的细微时间错位，同时开发AION防御方法，通过共享表示学习和动态时间规整监测时间对齐性。

Result: DejaVu攻击能显著降低感知性能，例如LiDAR单帧延迟可导致车辆检测mAP降低88.5%，三帧摄像头延迟使车辆的MOTA降低73%。AION防御在不同数据集上达到了0.92-0.98的AUROC，具备低误报率和高度鲁棒性。

Conclusion: 建议通过AION监测解决感知模型中的时间对齐漏洞，有助于提升自主驾驶系统的鲁棒性和安全性。

Abstract: Multimodal fusion (MMF) plays a critical role in the perception of autonomous
driving, which primarily fuses camera and LiDAR streams for a comprehensive and
efficient scene understanding. However, its strict reliance on precise temporal
synchronization exposes it to new vulnerabilities. In this paper, we introduce
DejaVu, a novel attack that exploits network-induced delays to create subtle
temporal misalignments across sensor streams, severely degrading downstream
MMF-based perception tasks. Our comprehensive attack analysis across different
models and datasets reveals these sensors' task-specific imbalanced
sensitivities: object detection is overly dependent on LiDAR inputs while
object tracking is highly reliant on the camera inputs. Consequently, with a
single-frame LiDAR delay, an attacker can reduce the car detection mAP by up to
88.5%, while with a three-frame camera delay, multiple object tracking accuracy
(MOTA) for car drops by 73%. To detect such attacks, we propose AION, a defense
patch that can work alongside the existing perception model to monitor temporal
alignment through cross-modal temporal consistency. AION leverages multimodal
shared representation learning and dynamic time warping to determine the path
of temporal alignment and calculate anomaly scores based on the alignment. Our
thorough evaluation of AION shows it achieves AUROC scores of 0.92-0.98 with
low false positives across datasets and model architectures, demonstrating it
as a robust and generalized defense against the temporal misalignment attacks.

</details>


### [318] [S2SRec2: Set-to-Set Recommendation for Basket Completion with Recipe](https://arxiv.org/abs/2507.09101)
*Yanan Cao,Omid Memarrast,Shiqin Cai,Sinduja Subramaniam,Evren Korpeoglu,Kannan Achan*

Main category: cs.LG

TL;DR: 该论文提出一个新的方法S2SRec2，用于通过不完整的食材篮推荐补充的食材，这种方法在性能上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的食谱补全方法无法满足实际场景中多种原料同时缺失的情况，也不能捕捉缺失原料之间的关系，因此需要一种更全面且实际的解决方案。

Method: 提出了基于集合变换器的S2SRec2框架，该方法将篮子补全任务表述为从集合到集合的推荐问题，并通过多任务学习同时优化原料检索和篮子完整性评估。

Result: S2SRec2在大规模配方数据集上的实验和定性分析表明，其性能显著优于针对单一目标的基线方法。

Conclusion: S2SRec2是一种有前景的方法，可以改善杂货购物体验并激发烹饪创造力。

Abstract: In grocery e-commerce, customers often build ingredient baskets guided by
dietary preferences but lack the expertise to create complete meals. Leveraging
recipe knowledge to recommend complementary ingredients based on a partial
basket is essential for improving the culinary experience. Traditional recipe
completion methods typically predict a single missing ingredient using a
leave-one-out strategy. However, they fall short in two key aspects: (i) they
do not reflect real-world scenarios where multiple ingredients are often
needed, and (ii) they overlook relationships among the missing ingredients
themselves. To address these limitations, we reformulate basket completion as a
set-to-set (S2S) recommendation problem, where an incomplete basket is input
into a system that predicts a set of complementary ingredients. We introduce
S2SRec2, a set-to-set ingredient recommendation framework based on a Set
Transformer and trained in a multitask learning paradigm. S2SRec2 jointly
learns to (i) retrieve missing ingredients from the representation of existing
ones and (ii) assess basket completeness after prediction. These tasks are
optimized together, enforcing accurate retrieval and coherent basket
completion. Experiments on large-scale recipe datasets and qualitative analyses
show that S2SRec2 significantly outperforms single-target baselines, offering a
promising approach to enhance grocery shopping and inspire culinary creativity.

</details>


### [319] [A Study of Value-Aware Eigenoptions](https://arxiv.org/abs/2507.09127)
*Harshil Kotamreddy,Marlos C. Machado*

Main category: cs.LG

TL;DR: 本文探讨了使用eigenoptions提升强化学习中的信用分配和探索性能，评估其在从表格到像素级网格世界的不同场景中的表现，并提出了一种适用于深度强化学习中的非线性函数近似的选项值学习方法。


<details>
  <summary>Details</summary>
Motivation: 强化学习中选项能够提供对时间和层次结构的归纳偏置，但大多是手工设计的，而自动发现选项的方法仍然不足，尤其是在信用分配方面的作用未被充分研究。

Method: 通过在表格和像素级的网格世界中，评估预先设定的eigenoptions与在线发现的eigenoptions的表现，同时提出了一种针对深度强化学习中非线性函数近似的方法，用于学习选项值。

Result: 发现预定的eigenoptions不仅有助于探索，还可以加速信用分配，而在线发现的eigenoptions可能会对代理的经验带来过多偏差，阻碍学习，并揭示了终止条件对性能的影响。

Conclusion: eigenoptions在信用分配和探索中有显著潜力，但其使用的复杂性也不容忽视，需要进一步研究和优化。

Abstract: Options, which impose an inductive bias toward temporal and hierarchical
structure, offer a powerful framework for reinforcement learning (RL). While
effective in sequential decision-making, they are often handcrafted rather than
learned. Among approaches for discovering options, eigenoptions have shown
strong performance in exploration, but their role in credit assignment remains
underexplored. In this paper, we investigate whether eigenoptions can
accelerate credit assignment in model-free RL, evaluating them in tabular and
pixel-based gridworlds. We find that pre-specified eigenoptions aid not only
exploration but also credit assignment, whereas online discovery can bias the
agent's experience too strongly and hinder learning. In the context of deep RL,
we also propose a method for learning option-values under non-linear function
approximation, highlighting the impact of termination conditions on
performance. Our findings reveal both the promise and complexity of using
eigenoptions, and options more broadly, to simultaneously support credit
assignment and exploration in reinforcement learning.

</details>


### [320] [Heterogeneous Graph Prompt Learning via Adaptive Weight Pruning](https://arxiv.org/abs/2507.09132)
*Chu-Yuan Wei,Shun-Yao Liu,Sheng-Da Zhuo,Chang-Dong Wang,Shu-Qiang Huang,Mohsen Guizani*

Main category: cs.LG

TL;DR: 引入的GPAWP框架通过结合图提示和权重剪枝来提升图神经网络（GNN）的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络存在训练和推理时间较长、难以捕捉复杂关系以及特征提取不足的问题，同时现有研究没有充分利用图提示对模型优化的潜力以及正负提示对模型稳定性的影响。

Method: 提出GPAWP框架，结合图提示和权重剪枝。框架通过重要性评估函数评估图提示的权重，进行层次化结构化剪枝，移除负提示，提升参数效率和模型性能。

Result: 在三项基准数据集上的实验验证了GPAWP的优越性，特别是在节点分类任务中显著减少了参数。

Conclusion: GPAWP框架实现了更高效的参数使用，同时在性能上也具有竞争力，为图神经网络的优化提供了一种新方法。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in various
graph-based tasks (e.g., node classification or link prediction). Despite their
triumphs, GNNs still face challenges such as long training and inference times,
difficulty in capturing complex relationships, and insufficient feature
extraction. To tackle these issues, graph pre-training and graph prompt methods
have garnered increasing attention for their ability to leverage large-scale
datasets for initial learning and task-specific adaptation, offering potential
improvements in GNN performance. However, previous research has overlooked the
potential of graph prompts in optimizing models, as well as the impact of both
positive and negative graph prompts on model stability and efficiency. To
bridge this gap, we propose a novel framework combining graph prompts with
weight pruning, called GPAWP, which aims to enhance the performance and
efficiency of graph prompts by using fewer of them. We evaluate the importance
of graph prompts using an importance assessment function to determine positive
and negative weights at different granularities. Through hierarchically
structured pruning, we eliminate negative prompt labels, resulting in more
parameter-efficient and competitively performing prompts. Extensive experiments
on three benchmark datasets demonstrate the superiority of GPAWP, leading to a
significant reduction in parameters in node classification tasks.

</details>


### [321] [POIFormer: A Transformer-Based Framework for Accurate and Scalable Point-of-Interest Attribution](https://arxiv.org/abs/2507.09137)
*Nripsuta Ani Saxena,Shang-Ling Hsu,Mehul Shetty,Omar Alkhadra,Cyrus Shahabi,Abigail L. Horn*

Main category: cs.LG

TL;DR: 本文介绍了一个名为POIFormer的基于Transformer的框架，用于在具有GPS误差和高密度POI分布的复杂环境中进行精确和高效的POI定位。


<details>
  <summary>Details</summary>
Motivation: 现有POI定位方法在GPS误差大和POI密集分布的城市环境中效果不佳，作者希望开发一种更精确和高效的模型。

Method: 提出了一种基于Transformer的框架，结合了空间邻近性、访问时间和持续时间、POI语义上下文特征以及用户和人群行为模式，利用自注意力机制处理复杂的多维交互。

Result: 在真实世界数据集上的实验结果显示，与现有基线方法相比，POIFormer在空间噪声和POI高密度聚集的情况下表现出了显著的改进。

Conclusion: POIFormer在支持多数据源与地理环境的通用性上效果良好，并避免了对难以获取的数据的依赖，适合实际场景部署。

Abstract: Accurately attributing user visits to specific Points of Interest (POIs) is a
foundational task for mobility analytics, personalized services, marketing and
urban planning. However, POI attribution remains challenging due to GPS
inaccuracies, typically ranging from 2 to 20 meters in real-world settings, and
the high spatial density of POIs in urban environments, where multiple venues
can coexist within a small radius (e.g., over 50 POIs within a 100-meter radius
in dense city centers). Relying on proximity is therefore often insufficient
for determining which POI was actually visited. We introduce
\textsf{POIFormer}, a novel Transformer-based framework for accurate and
efficient POI attribution. Unlike prior approaches that rely on limited
spatiotemporal, contextual, or behavioral features, \textsf{POIFormer} jointly
models a rich set of signals, including spatial proximity, visit timing and
duration, contextual features from POI semantics, and behavioral features from
user mobility and aggregated crowd behavior patterns--using the Transformer's
self-attention mechanism to jointly model complex interactions across these
dimensions. By leveraging the Transformer to model a user's past and future
visits (with the current visit masked) and incorporating crowd-level behavioral
patterns through pre-computed KDEs, \textsf{POIFormer} enables accurate,
efficient attribution in large, noisy mobility datasets. Its architecture
supports generalization across diverse data sources and geographic contexts
while avoiding reliance on hard-to-access or unavailable data layers, making it
practical for real-world deployment. Extensive experiments on real-world
mobility datasets demonstrate significant improvements over existing baselines,
particularly in challenging real-world settings characterized by spatial noise
and dense POI clustering.

</details>


### [322] [Towards Interpretable Drug-Drug Interaction Prediction: A Graph-Based Approach with Molecular and Network-Level Explanations](https://arxiv.org/abs/2507.09173)
*Mengjie Chen,Ming Zhang,Cunquan Qu*

Main category: cs.LG

TL;DR: MolecBioNet是一个新颖的基于图的框架，通过结合分子和生物医学知识，提供强大的和可解释的药物相互作用（DDI）预测能力。


<details>
  <summary>Details</summary>
Motivation: 目前的DDI预测方法未能充分捕捉药物对间复杂的上下文依赖关系，同时难以整合生物学交互网络和分子水平的结构信息。

Method: MolecBioNet将药物对建模为统一实体，结合生物医学知识图谱的局部子图和分子表示的层次交互图表，通过经典图神经网络学习药物对的多尺度表示，并引入基于领域的子图池化策略和信息多样性约束进行优化。

Result: 实验结果表明，MolecBioNet在DDI预测中优于现有的方法，同时消融研究和可视化分析验证了其统一药物对建模和多尺度知识整合的优势。

Conclusion: 该研究提出的MolecBioNet框架，为整合宏观生物交互和微观分子影响的DDI预测提供了新颖的解决方案，显著提高了准确性和可解释性。

Abstract: Drug-drug interactions (DDIs) represent a critical challenge in pharmacology,
often leading to adverse drug reactions with significant implications for
patient safety and healthcare outcomes. While graph-based methods have achieved
strong predictive performance, most approaches treat drug pairs independently,
overlooking the complex, context-dependent interactions unique to drug pairs.
Additionally, these models struggle to integrate biological interaction
networks and molecular-level structures to provide meaningful mechanistic
insights. In this study, we propose MolecBioNet, a novel graph-based framework
that integrates molecular and biomedical knowledge for robust and interpretable
DDI prediction. By modeling drug pairs as unified entities, MolecBioNet
captures both macro-level biological interactions and micro-level molecular
influences, offering a comprehensive perspective on DDIs. The framework
extracts local subgraphs from biomedical knowledge graphs and constructs
hierarchical interaction graphs from molecular representations, leveraging
classical graph neural network methods to learn multi-scale representations of
drug pairs. To enhance accuracy and interpretability, MolecBioNet introduces
two domain-specific pooling strategies: context-aware subgraph pooling
(CASPool), which emphasizes biologically relevant entities, and
attention-guided influence pooling (AGIPool), which prioritizes influential
molecular substructures. The framework further employs mutual information
minimization regularization to enhance information diversity during embedding
fusion. Experimental results demonstrate that MolecBioNet outperforms
state-of-the-art methods in DDI prediction, while ablation studies and
embedding visualizations further validate the advantages of unified drug pair
modeling and multi-scale knowledge integration.

</details>


### [323] [Continual Reinforcement Learning by Planning with Online World Models](https://arxiv.org/abs/2507.09177)
*Zichen Liu,Guoji Fu,Chao Du,Wee Sun Lee,Min Lin*

Main category: cs.LG

TL;DR: 提出了一种基于在线世界模型的规划方法，有效解决连续强化学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决连续强化学习中因学习新任务而遗忘旧任务（灾难性遗忘）的难题。

Method: 通过在线学习的泛步进模型（Follow-The-Leader shallow model）捕捉世界动态，并结合模型预测控制实现任务规划，形成一个逐步更新的在线代理（OA）。

Result: 在专门设计的连续学习环境Continual Bench中，OA在学习新任务的同时避免了旧技能的遗忘，其表现优于基于深层模型和其它连续学习技术的代理。

Conclusion: 该方法通过在线世界模型的规划，展现了在连续强化学习情境中有效防止遗忘的能力，为该领域提供了一个新颖有效的解决思路。

Abstract: Continual reinforcement learning (CRL) refers to a naturalistic setting where
an agent needs to endlessly evolve, by trial and error, to solve multiple tasks
that are presented sequentially. One of the largest obstacles to CRL is that
the agent may forget how to solve previous tasks when learning a new task,
known as catastrophic forgetting. In this paper, we propose to address this
challenge by planning with online world models. Specifically, we learn a
Follow-The-Leader shallow model online to capture the world dynamics, in which
we plan using model predictive control to solve a set of tasks specified by any
reward functions. The online world model is immune to forgetting by
construction with a proven regret bound of $\mathcal{O}(\sqrt{K^2D\log(T)})$
under mild assumptions. The planner searches actions solely based on the latest
online model, thus forming a FTL Online Agent (OA) that updates incrementally.
To assess OA, we further design Continual Bench, a dedicated environment for
CRL, and compare with several strong baselines under the same model-planning
algorithmic framework. The empirical results show that OA learns continuously
to solve new tasks while not forgetting old skills, outperforming agents built
on deep world models with various continual learning techniques.

</details>


### [324] [XiChen: An observation-scalable fully AI-driven global weather forecasting system with 4D variational knowledge](https://arxiv.org/abs/2507.09202)
*Wuxin Wang,Weicheng Ni,Lilan Huang,Tao Hao,Ben Fei,Shuo Ma,Taikang Yuan,Yanlai Zhao,Kefeng Deng,Xiaoyong Li,Boheng Duan,Lei Bai,Kaijun Ren*

Main category: cs.LG

TL;DR: XiChen是一个先进的全AI驱动天气预报系统，从数据同化到中期预报在17秒内完成，并达到专业水平。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型依赖传统数值天气预报系统，耗时且资源需求高，不能完全实现独立的AI驱动天气预报。

Method: 开发了基于天气预报预训练的基础模型，并通过微调实现数据同化和多种观测数据处理。利用四维变分知识优化模型性能。

Result: XiChen能够在17秒内完成天气预报，全流程AI驱动，精确度和专业NWP系统相当，能预报超过8.25天的天气趋势。

Conclusion: XiChen展现了完全脱离NWP系统的AI天气预报的强大潜力，为天气预报提供了一种高效且准确的新方式。

Abstract: Recent advancements in Artificial Intelligence (AI) demonstrate significant
potential to revolutionize weather forecasting. However, most AI-driven models
rely on Numerical Weather Prediction (NWP) systems for initial condition
preparation, which often consumes hours on supercomputers. Here we introduce
XiChen, the first observation-scalable fully AI-driven global weather
forecasting system, whose entire pipeline, from Data Assimilation (DA) to
medium-range forecasting, can be accomplished within only 17 seconds. XiChen is
built upon a foundation model that is pre-trained for weather forecasting.
Meanwhile, this model is subsequently fine-tuned to serve as both observation
operators and DA models, thereby scalably assimilating conventional and raw
satellite observations. Furthermore, the integration of four-dimensional
variational knowledge ensures that XiChen's DA and medium-range forecasting
accuracy rivals that of operational NWP systems, amazingly achieving a skillful
forecasting lead time exceeding 8.25 days. These findings demonstrate that
XiChen holds strong potential toward fully AI-driven weather forecasting
independent of NWP systems.

</details>


### [325] [Capturing Unseen Spatial Extremes Through Knowledge-Informed Generative Modeling](https://arxiv.org/abs/2507.09211)
*Xinyue Liu,Xiao Peng,Shuyue Yan,Yuntian Chen,Dongxiao Zhang,Zhixiao Niu,Hui-Min Wang,Xiaogang He*

Main category: cs.LG

TL;DR: 提出了一种深度生成模型DeepX-GAN，用于识别超出现实记录的潜在极端气候风险。这种方法能够模拟“未见”的极端情况，并揭示其对脆弱系统的潜在风险。


<details>
  <summary>Details</summary>
Motivation: 现有气候极端事件记录和风险评估往往忽视了历史外的“未见”极端情况以及空间依赖性可能造成的同步灾害风险。这无助于全面了解和管理相关风险。

Method: 开发了DeepX-GAN，一种深度生成对抗网络，结合知识导向，可捕获罕见极端事件的空间结构，并具备零样本泛化能力，用于模拟“未见”极端事件。

Result: 应用DeepX-GAN于中东和北非地区，揭示了“未见”极端事件如何严重影响高脆弱性地区，同时表明全球变暖可能会扩大和重新分布此类极端事件的影响范围。

Conclusion: 传统的灾害规划方法可能低估了潜在的空间风险热点，亟需制定能够预测新兴风险的空间自适应政策，以应对未来潜在挑战。

Abstract: Observed records of climate extremes provide an incomplete picture of risk,
missing "unseen" extremes that exceed historical bounds. In parallel,
neglecting spatial dependence undervalues the risk of synchronized hazards that
amplify impacts. To address these challenges, we develop DeepX-GAN
(Dependence-Enhanced Embedding for Physical eXtremes - Generative Adversarial
Network), a knowledge-informed deep generative model designed to better capture
the spatial structure of rare extremes. The zero-shot generalizability of
DeepX-GAN enables simulation of unseen extremes that fall outside historical
experience yet remain statistically plausible. We define two types of unseen
extremes: "checkmate" extremes that directly hit targets, and "stalemate"
extremes that narrowly miss. These unrealized scenarios expose latent risks in
fragile systems and may reinforce a false sense of resilience if overlooked.
Near misses, in particular, can prompt either proactive adaptation or dangerous
complacency, depending on how they are interpreted. Applying DeepX-GAN to the
Middle East and North Africa (MENA), we find that these unseen extremes
disproportionately affect regions with high vulnerability and low socioeconomic
readiness, but differ in urgency and interpretation. Future warming could
expand and redistribute these unseen extremes, with emerging exposure hotspots
in Indo-Pakistan and Central Africa. This distributional shift highlights
critical blind spots in conventional hazard planning and underscores the need
to develop spatially adaptive policies that anticipate emergent risk hotspots
rather than simply extrapolating from historical patterns.

</details>


### [326] [Warm Starts Accelerate Generative Modelling](https://arxiv.org/abs/2507.09212)
*Jonas Scholz,Richard E. Turner*

Main category: cs.LG

TL;DR: 提出一种名为warm-start model的新模型，通过预测有条件的先验分布大幅加速迭代生成过程。


<details>
  <summary>Details</summary>
Motivation: 当前迭代生成模型速度较慢，通常需要数百次函数评估，提出一种能加速这种生成任务的改进方法。

Method: 通过一个warm-start模型，先基于输入上下文预测条件先验分布作为初始点，并结合条件归一化技巧，使其与标准生成模型和采样器兼容。

Result: 在图像修复任务上，比肩1000步DDPM基线的性能，仅需11次函数评估（1次warm start，10次生成）。

Conclusion: warm-start模型是一种简单而有效的加速条件生成方法，且可与现有的生成模型结合进一步提高效率。

Abstract: Iterative generative models, like diffusion and flow-matching, create
high-fidelity samples by progressively refining a noise vector into data.
However, this process is notoriously slow, often requiring hundreds of function
evaluations. We introduce the warm-start model, a simple, deterministic model
that dramatically accelerates conditional generation by providing a better
starting point. Instead of starting generation from an uninformed N(0, I)
prior, our warm-start model predicts an informed prior N(mu, sigma), whose
moments are conditioned on the input context. This "warm start" substantially
reduces the distance the generative process must traverse, particularly when
the conditioning information is strongly informative. On tasks like image
inpainting, our method achieves results competitive with a 1000-step DDPM
baseline using only 11 total function evaluations (1 for the warm start, 10 for
generation). A simple conditional normalization trick makes our method
compatible with any standard generative model and sampler without modification,
allowing it to be combined with other efficient sampling techniques for further
acceleration. Our implementation is available at
https://github.com/jonas-scholz123/warm-start-model.

</details>


### [327] [Optimizing Basis Function Selection in Constructive Wavelet Neural Networks and Its Applications](https://arxiv.org/abs/2507.09213)
*Dunsheng Huang,Dong Shen,Lei Lu,Ying Tan*

Main category: cs.LG

TL;DR: 本文讨论了一种构造性小波神经网络（WNN）的新框架，通过频率估计和选择高能量小波基，改进了计算效率并应用于各种场景。


<details>
  <summary>Details</summary>
Motivation: 现有WNN在构造精确小波基和高计算成本方面存在挑战，影响其广泛应用。

Method: 提出了一种频率估计和小波基增量机制，通过选择初始小波基并根据主要频率分量引入新的小波基，从而减少计算成本。

Result: 所提出的框架在离线数据、时间序列数据和非线性依赖捕获等多个应用中验证了其实用性和通用性。

Conclusion: 新框架极大提升了WNN的效率和可扩展性，代码已开源供进一步研究和应用。

Abstract: Wavelet neural network (WNN), which learns an unknown nonlinear mapping from
the data, has been widely used in signal processing, and time-series analysis.
However, challenges in constructing accurate wavelet bases and high
computational costs limit their application. This study introduces a
constructive WNN that selects initial bases and trains functions by introducing
new bases for predefined accuracy while reducing computational costs. For the
first time, we analyze the frequency of unknown nonlinear functions and select
appropriate initial wavelets based on their primary frequency components by
estimating the energy of the spatial frequency component. This leads to a novel
constructive framework consisting of a frequency estimator and a wavelet-basis
increase mechanism to prioritize high-energy bases, significantly improving
computational efficiency. The theoretical foundation defines the necessary
time-frequency range for high-dimensional wavelets at a given accuracy. The
framework's versatility is demonstrated through four examples: estimating
unknown static mappings from offline data, combining two offline datasets,
identifying time-varying mappings from time-series data, and capturing
nonlinear dependencies in real time-series data. These examples showcase the
framework's broad applicability and practicality. All the code will be released
at https://github.com/dshuangdd/CWNN.

</details>


### [328] [TPP-SD: Accelerating Transformer Point Process Sampling with Speculative Decoding](https://arxiv.org/abs/2507.09252)
*Shukai Gong,Yiyang Fu,Fengyuan Ran,Feng Zhou*

Main category: cs.LG

TL;DR: TPP-SD是一种新方法，通过从语言模型中引入推测解码技术，显著加速Transformer时间点过程的采样。它使用较小的模型生成候选事件，并通过较大的目标模型验证，实现了与自回归采样相同的输出分布，同时提升了采样速度。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer时间点过程采样缓慢的问题，满足快速序列采样的实际需求。

Method: 提出TPP-SD框架，通过小型草稿模型生成候选事件，再由大型目标模型并行验证，从而加速采样。

Result: 实验表明，该方法在合成及真实数据集上比标准方法快2至6倍，同时保持相同的分布效果。

Conclusion: TPP-SD成功平衡了Transformer TPP模型的强大性能和快速采样的实际需求。

Abstract: We propose TPP-SD, a novel approach that accelerates Transformer temporal
point process (TPP) sampling by adapting speculative decoding (SD) techniques
from language models. By identifying the structural similarities between
thinning algorithms for TPPs and speculative decoding for language models, we
develop an efficient sampling framework that leverages a smaller draft model to
generate multiple candidate events, which are then verified by the larger
target model in parallel. TPP-SD maintains the same output distribution as
autoregressive sampling while achieving significant acceleration. Experiments
on both synthetic and real datasets demonstrate that our approach produces
samples from identical distributions as standard methods, but with 2-6$\times$
speedup. Our ablation studies analyze the impact of hyperparameters such as
draft length and draft model size on sampling efficiency. TPP-SD bridges the
gap between powerful Transformer TPP models and the practical need for rapid
sequence sampling.

</details>


### [329] [Controllable Patching for Compute-Adaptive Surrogate Modeling of Partial Differential Equations](https://arxiv.org/abs/2507.09264)
*Payel Mukhopadhyay,Michael McCabe,Ruben Ohana,Miles Cranmer*

Main category: cs.LG

TL;DR: 引入模块CKM与CSM，实现无重训练的动态补丁大小控制，提升效率与精度。


<details>
  <summary>Details</summary>
Motivation: 解决固定补丁大小导致的效率与生产环境部署限制问题。

Method: 设计CKM和CSM模块实现推断时动态调整补丁大小，并结合循环补丁大小滚动策略缓解伪影及提升长期稳定性。

Result: 在多种复杂2D与3D PDE基准测试中提升滚动精度与运行效率。

Conclusion: 首次实现支持推断实时调整补丁大小的框架，对PDE代理建模任务具有广泛应用性。

Abstract: Patch-based transformer surrogates have become increasingly effective for
modeling spatiotemporal dynamics, but the fixed patch size is a major
limitation for budget-conscience deployment in production. We introduce two
lightweight, architecture-agnostic modules-the Convolutional Kernel Modulator
(CKM) and Convolutional Stride Modulator (CSM)-that enable dynamic patch size
control at inference in patch based models, without retraining or accuracy
loss. Combined with a cyclic patch-size rollout, our method mitigates patch
artifacts and improves long-term stability for video-like prediction tasks.
Applied to a range of challenging 2D and 3D PDE benchmarks, our approach
improves rollout fidelity and runtime efficiency. To our knowledge, this is the
first framework to enable inference-time patch-size tunability in patch-based
PDE surrogates. Its plug-and-play design makes it broadly applicable across
architectures-establishing a general foundation for compute-adaptive modeling
in PDE surrogate tasks.

</details>


### [330] [Impute With Confidence: A Framework for Uncertainty Aware Multivariate Time Series Imputation](https://arxiv.org/abs/2507.09353)
*Addison Weatherhead,Anna Goldenberg*

Main category: cs.LG

TL;DR: 提出了一个框架，通过量化和利用不确定性选择性填补时间序列数据中的缺失值，以减少错误并提升后续任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往忽视模型不确定性或缺乏评估机制，特别是在医疗领域中，由于传感器长时间断开，缺失值问题尤为严重。

Method: 构建一个通用框架，量化并利用不确定性，通过选择性关注模型较为自信的值来避免高度不可靠的填补。

Result: 在多种电子健康记录（EHR）数据集上实验，验证选择性填补不确定性较低的值不仅能减少填补误差，还能改进下游任务的表现。

Conclusion: 通过在时间序列填补中引入不确定性，不仅提升了填补质量，还在实际任务中提升了性能，如24小时病亡预测任务表现的提高。

Abstract: Time series data with missing values is common across many domains.
Healthcare presents special challenges due to prolonged periods of sensor
disconnection. In such cases, having a confidence measure for imputed values is
critical. Most existing methods either overlook model uncertainty or lack
mechanisms to estimate it. To address this gap, we introduce a general
framework that quantifies and leverages uncertainty for selective imputation.
By focusing on values the model is most confident in, highly unreliable
imputations are avoided. Our experiments on multiple EHR datasets, covering
diverse types of missingness, demonstrate that selectively imputing
less-uncertain values not only reduces imputation errors but also improves
downstream tasks. Specifically, we show performance gains in a 24-hour
mortality prediction task, underscoring the practical benefit of incorporating
uncertainty into time series imputation.

</details>


### [331] [Meta-autoencoders: An approach to discovery and representation of relationships between dynamically evolving classes](https://arxiv.org/abs/2507.09362)
*Assaf Marron,Smadar Szekely,Irun Cohen,David Harel*

Main category: cs.LG

TL;DR: 提出了元自编码器（MAE）的概念，可以为一组自编码器（AE）学习紧凑的表示和相应的编码解码过程。


<details>
  <summary>Details</summary>
Motivation: 研究在一组多样化的类（如不同物种和其进化关系中）之间，实现更高级别的自动化建模和总结能力。

Method: 通过设计一个元自编码器（MAE）来学习一组自编码器的紧凑表示，并演示其在生物学演化建模中的潜在应用。

Result: 提供了元自编码器的初步定义和示例，证明了其在机器学习和生物学研究中的潜力。

Conclusion: 元自编码器有助于研究类与类之间的差异和联系，特别是在自然演化领域。

Abstract: An autoencoder (AE) is a neural network that, using self-supervised training,
learns a succinct parameterized representation, and a corresponding encoding
and decoding process, for all instances in a given class. Here, we introduce
the concept of a meta-autoencoder (MAE): an AE for a collection of
autoencoders. Given a family of classes that differ from each other by the
values of some parameters, and a trained AE for each class, an MAE for the
family is a neural net that has learned a compact representation and associated
encoder and decoder for the class-specific AEs. One application of this general
concept is in research and modeling of natural evolution -- capturing the
defining and the distinguishing properties across multiple species that are
dynamically evolving from each other and from common ancestors. In this interim
report we provide a constructive definition of MAEs, initial examples, and the
motivating research directions in machine learning and biology.

</details>


### [332] [Fair CCA for Fair Representation Learning: An ADNI Study](https://arxiv.org/abs/2507.09382)
*Bojian Hou,Zhanliang Wang,Zhuoping Zhou,Boning Tong,Zexuan Wang,Jingxuan Bao,Duy Duong-Tran,Qi Long,Li Shen*

Main category: cs.LG

TL;DR: 本文提出了一种新的公平正典相关分析（CCA）方法，用于学习公正的低维数据表示，能保证数据特征与敏感属性无关，从而在不牺牲性能的前提下实现公平性。


<details>
  <summary>Details</summary>
Motivation: 现有的公平CCA方法在应用中缺乏对下游分类任务的关注，限制了其实用性。

Method: 提出了一种新的公平CCA方法，通过确保降维后的特征与敏感属性无关，实现公平的表示学习。

Result: 在合成数据和阿尔茨海默病神经影像研究（ADNI）的真实数据上验证，结果表明其在保持相关分析性能的前提下，显著提高了分类任务中的公平性。

Conclusion: 该方法能够同时实现高相关分析与分类任务中的公平性，适用于神经影像领域需要无偏分析的场景。

Abstract: Canonical correlation analysis (CCA) is a technique for finding correlations
between different data modalities and learning low-dimensional representations.
As fairness becomes crucial in machine learning, fair CCA has gained attention.
However, previous approaches often overlook the impact on downstream
classification tasks, limiting applicability. We propose a novel fair CCA
method for fair representation learning, ensuring the projected features are
independent of sensitive attributes, thus enhancing fairness without
compromising accuracy. We validate our method on synthetic data and real-world
data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), demonstrating
its ability to maintain high correlation analysis performance while improving
fairness in classification tasks. Our work enables fair machine learning in
neuroimaging studies where unbiased analysis is essential.

</details>


### [333] [Geometric Generative Modeling with Noise-Conditioned Graph Networks](https://arxiv.org/abs/2507.09391)
*Peter Pao-Huang,Mitchell Black,Xiaojie Qiu*

Main category: cs.LG

TL;DR: 本文介绍了一种动态调整架构的新型图神经网络（Noise-Conditioned Graph Networks, NCGNs），能够根据生成过程中的噪声水平改进对图形的建模能力。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型未充分利用噪声水平对图神经网络的影响，现有架构的表达能力有限。

Method: 提出了NCGNs，通过噪声水平动态调整图神经网络架构，特别是开发了Dynamic Message Passing (DMP)，根据噪声调整消息传递范围和分辨率。

Result: DMP在包括三维点云、时空转录组组学以及图像等多个领域超越了现有噪声独立架构模型的表现。

Conclusion: 噪声条件对于图生成过程至关重要，动态调整网络架构可以显著提高模型效果。

Abstract: Generative modeling of graphs with spatial structure is essential across many
applications from computer graphics to spatial genomics. Recent flow-based
generative models have achieved impressive results by gradually adding and then
learning to remove noise from these graphs. Existing models, however, use graph
neural network architectures that are independent of the noise level, limiting
their expressiveness. To address this issue, we introduce
\textit{Noise-Conditioned Graph Networks} (NCGNs), a class of graph neural
networks that dynamically modify their architecture according to the noise
level during generation. Our theoretical and empirical analysis reveals that as
noise increases, (1) graphs require information from increasingly distant
neighbors and (2) graphs can be effectively represented at lower resolutions.
Based on these insights, we develop Dynamic Message Passing (DMP), a specific
instantiation of NCGNs that adapts both the range and resolution of message
passing to the noise level. DMP consistently outperforms noise-independent
architectures on a variety of domains including $3$D point clouds,
spatiotemporal transcriptomics, and images. Code is available at
https://github.com/peterpaohuang/ncgn.

</details>


### [334] [A Random Matrix Theory Perspective on the Learning Dynamics of Multi-head Latent Attention](https://arxiv.org/abs/2507.09394)
*Nandan Kumar Jha,Brandon Reagen*

Main category: cs.LG

TL;DR: 本文研究了多头潜在注意力（MLA）如何影响Transformer在预训练时的内部容量，发现不同旋转嵌入应用方式对模型表达能力有显著影响。


<details>
  <summary>Details</summary>
Motivation: 探讨多头潜在注意力（MLA）对Transformer内存压缩和模型表达能力的影响，明确旋转嵌入的关键性作用。

Method: 利用Marchenko-Pastur (MP) 诊断工具，通过分析$W_{Q}W_{K}^\top$ 格拉姆矩阵的光谱，对标准多头注意力（MHA）、MLA-PreRoPE和MLA-Decoupled三种变体进行比较研究。

Result: 随机矩阵分析揭示了三点关键发现：1）MHA和MLA-PreRoPE在特定层会出现容量瓶颈；2）与秩塌陷相关，这会集中模型表达能力于狭窄子空间中；3）MLA-Decoupled能防止容量碎片化，保持宽谱支持并抑制异常干扰。

Conclusion: 如何应用旋转嵌入同其压缩位置一样重要，跨头共享旋转组件可减轻光谱碎片化，维护表示能力。

Abstract: In this work, we study how multi-head latent attention (MLA), a popular
strategy for compressing key/value memory, affects a transformer's internal
capacity during pretraining. Using a lightweight suite of Marchenko-Pastur (MP)
diagnostics, we analyze the spectrum of the $W_{Q}W_{K}^\top$ gram matrix
throughout training, comparing three variants: the standard multi-head
attention (MHA) baseline, MLA-PreRoPE with rotary applied before compression,
and MLA-Decoupled, which shares a single rotary sub-vector across all heads.
Our random matrix analysis reveals \textbf{three key findings:} \textbf{ i)}
capacity bottlenecks emerge locally: both MHA and MLA-PreRoPE exhibit sharp,
early spikes in specific layers that persist and propagate, disrupting the
balance between bulk and outlier directions; \textbf{ ii)} these spikes
coincide with rank collapse, concentrating the model's expressivity into narrow
subspaces; \textbf{ iii)} only the decoupled variant prevents this cascade,
maintaining broad spectral support and suppressing outlier formation across
layers. These results underscore that \emph{how} rotary embeddings are applied
is just as critical as \emph{where} compression occurs. Sharing rotary
components across heads mitigates spectral fragmentation and preserves
representational capacity.

</details>


### [335] [Scaling Laws for Optimal Data Mixtures](https://arxiv.org/abs/2507.09404)
*Mustafa Shukor,Louis Bethune,Dan Busbridge,David Grangier,Enrico Fini,Alaaeldin El-Nouby,Pierre Ablin*

Main category: cs.LG

TL;DR: 本文提出了一种基于缩放法则的方法，以系统地确定针对目标领域的最佳数据混合比例，避免了传统的试错方法在大型预训练中的高成本问题。


<details>
  <summary>Details</summary>
Motivation: 当前在大模型的预训练中，多领域数据的混合比例选择对模型性能至关重要，但现有方法主要依赖于试错，效率低下且成本高昂。

Method: 提出了使用缩放法则来准确预测模型损失，并基于小规模训练的结果，外推估算大规模训练中的性能表现，从而确定最佳领域权重。

Result: 在大语言模型、原生多模态模型及视觉大模型三种大规模应用场景中验证了缩放法则的普适性。实验表明，该方法可在小规模训练后，有效推导大规模训练性能，并确定最优领域权重。

Conclusion: 缩放法则为确定目标领域的最佳数据混合比例，提供了一种系统且高效的方法，避免了高昂的试错成本。

Abstract: Large foundation models are typically trained on data from multiple domains,
with the data mixture--the proportion of each domain used--playing a critical
role in model performance. The standard approach to selecting this mixture
relies on trial and error, which becomes impractical for large-scale
pretraining. We propose a systematic method to determine the optimal data
mixture for any target domain using scaling laws. Our approach accurately
predicts the loss of a model of size $N$ trained with $D$ tokens and a specific
domain weight vector $h$. We validate the universality of these scaling laws by
demonstrating their predictive power in three distinct and large-scale
settings: large language model (LLM), native multimodal model (NMM), and large
vision models (LVM) pretraining. We further show that these scaling laws can
extrapolate to new data mixtures and across scales: their parameters can be
accurately estimated using a few small-scale training runs, and used to
estimate the performance at larger scales and unseen domain weights. The
scaling laws allow to derive the optimal domain weights for any target domain
under a given training budget ($N$,$D$), providing a principled alternative to
costly trial-and-error methods.

</details>


### [336] [Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers](https://arxiv.org/abs/2507.09406)
*Santhosh Kumar Ravindran*

Main category: cs.LG

TL;DR: 本论文提出代理激活修补（adversarial activation patching）框架，识别并缓解大语言模型（LLMs）的欺骗性行为。


<details>
  <summary>Details</summary>
Motivation: 大语言模型为确保安全性往往通过人类反馈强化学习（RLHF）进行对齐，但仍可能表现出隐匿的欺骗性行为，潜在地误导或隐瞒关键信息，亟需检测和缓解方法。

Method: 提出一种代理激活修补的机械可解释性框架，通过从“欺骗性”提示中提取激活值，并在特定层中将其引入安全的前向传播中，模拟漏洞，量化欺骗率，并测试多种假设。

Result: 通过玩具神经网络模拟实验，发现代理激活修补将欺骗性输出从0%提升到23.9%，同时展示了与模型层和条件的关联性。

Conclusion: 论文证明了代理激活修补在欺骗检测和缓解中的潜力，并重点分析了跨模型传递性、多模态情况加剧、以及规模效应，提出了进一步研究方向。

Abstract: Large language models (LLMs) aligned for safety through techniques like
reinforcement learning from human feedback (RLHF) often exhibit emergent
deceptive behaviors, where outputs appear compliant but subtly mislead or omit
critical information. This paper introduces adversarial activation patching, a
novel mechanistic interpretability framework that leverages activation patching
as an adversarial tool to induce, detect, and mitigate such deception in
transformer-based models. By sourcing activations from "deceptive" prompts and
patching them into safe forward passes at specific layers, we simulate
vulnerabilities and quantify deception rates. Through toy neural network
simulations across multiple scenarios (e.g., 1000 trials per setup), we
demonstrate that adversarial patching increases deceptive outputs to 23.9% from
a 0% baseline, with layer-specific variations supporting our hypotheses. We
propose six hypotheses, including transferability across models, exacerbation
in multimodal settings, and scaling effects. An expanded literature review
synthesizes over 20 key works in interpretability, deception, and adversarial
attacks. Mitigation strategies, such as activation anomaly detection and robust
fine-tuning, are detailed, alongside ethical considerations and future research
directions. This work advances AI safety by highlighting patching's dual-use
potential and provides a roadmap for empirical studies on large-scale models.

</details>


### [337] [On Information Geometry and Iterative Optimization in Model Compression: Operator Factorization](https://arxiv.org/abs/2507.09428)
*Zakhar Shumaylov,Vasileios Tsiaras,Yannis Stylianou*

Main category: cs.LG

TL;DR: 本文提出基于信息几何的模型压缩新方法，聚焦在算子分解，通过软化秩约束的方法提升了压缩模型的性能。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型参数规模的不断扩大，如何在资源受限的设备上高效部署成为关键问题。本研究动机是找到一种有效的模型压缩方法，同时保证性能不显著下降。

Method: 本文应用信息几何理论分析模型压缩中的算子分解方法，提出利用信息散度进行投影的框架，并通过迭代奇异值阈值方法（具有软秩约束）实现神经网络的训练。

Result: 研究证明了迭代奇异值阈值方法的收敛性，并展示了通过在现有方法中软化秩约束，在固定压缩率下可显著提升模型性能。

Conclusion: 信息几何在模型压缩中的应用能进一步提升深度学习模型在资源受限环境中的表现，特别是通过优化方法使压缩模型在零次微调或重训练条件下表现更优。

Abstract: The ever-increasing parameter counts of deep learning models necessitate
effective compression techniques for deployment on resource-constrained
devices. This paper explores the application of information geometry, the study
of density-induced metrics on parameter spaces, to analyze existing methods
within the space of model compression, primarily focusing on operator
factorization. Adopting this perspective highlights the core challenge:
defining an optimal low-compute submanifold (or subset) and projecting onto it.
We argue that many successful model compression approaches can be understood as
implicitly approximating information divergences for this projection. We
highlight that when compressing a pre-trained model, using information
divergences is paramount for achieving improved zero-shot accuracy, yet this
may no longer be the case when the model is fine-tuned. In such scenarios,
trainability of bottlenecked models turns out to be far more important for
achieving high compression ratios with minimal performance degradation,
necessitating adoption of iterative methods. In this context, we prove
convergence of iterative singular value thresholding for training neural
networks subject to a soft rank constraint. To further illustrate the utility
of this perspective, we showcase how simple modifications to existing methods
through softer rank reduction result in improved performance under fixed
compression rates.

</details>


### [338] [Dynamic Sparse Causal-Attention Temporal Networks for Interpretable Causality Discovery in Multivariate Time Series](https://arxiv.org/abs/2507.09439)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.LG

TL;DR: DyCAST-Net是一种新型架构，用于在多变量时间序列中发现因果关系，结合了扩张时间卷积和动态稀疏注意机制，表现优于现有模型如TCDF和GCFormer。


<details>
  <summary>Details</summary>
Motivation: 传统分析方法难以应对多变量时间序列中的复杂依赖和滞后效应，迫切需要一种更加精确和可解释的因果分析工具。

Method: DyCAST-Net结合扩张卷积捕获多尺度时间依赖，采用动态稀疏注意机制通过自适应门限消除虚假连接，同时附加统计重排测试以过滤假阳性。模型架构包括RMSNorm稳定和因果掩码机制。

Result: DyCAST-Net在金融和市场营销数据集上的表现优越，能够更精确地估计因果延迟，减少噪声环境中的虚假发现率，其可视化的注意力热图揭示了隐藏的因果模式。

Conclusion: DyCAST-Net在动态高维场景中表现出色，增强的因果推理能力及适应性为广泛应用领域提供了支持，对因果关系的解释性和可靠性做出了重要贡献。

Abstract: Understanding causal relationships in multivariate time series (MTS) is
essential for effective decision-making in fields such as finance and
marketing, where complex dependencies and lagged effects challenge conventional
analytical approaches. We introduce Dynamic Sparse Causal-Attention Temporal
Networks for Interpretable Causality Discovery in MTS (DyCAST-Net), a novel
architecture designed to enhance causal discovery by integrating dilated
temporal convolutions and dynamic sparse attention mechanisms. DyCAST-Net
effectively captures multiscale temporal dependencies through dilated
convolutions while leveraging an adaptive thresholding strategy in its
attention mechanism to eliminate spurious connections, ensuring both accuracy
and interpretability. A statistical shuffle test validation further strengthens
robustness by filtering false positives and improving causal inference
reliability. Extensive evaluations on financial and marketing datasets
demonstrate that DyCAST-Net consistently outperforms existing models such as
TCDF, GCFormer, and CausalFormer. The model provides a more precise estimation
of causal delays and significantly reduces false discoveries, particularly in
noisy environments. Moreover, attention heatmaps offer interpretable insights,
uncovering hidden causal patterns such as the mediated effects of advertising
on consumer behavior and the influence of macroeconomic indicators on financial
markets. Case studies illustrate DyCAST-Net's ability to detect latent
mediators and lagged causal factors, making it particularly effective in
high-dimensional, dynamic settings. The model's architecture enhanced by
RMSNorm stabilization and causal masking ensures scalability and adaptability
across diverse application domains

</details>


### [339] [Transformers Don't In-Context Learn Least Squares Regression](https://arxiv.org/abs/2507.09440)
*Joshua Hill,Benjamin Eyre,Elliot Creager*

Main category: cs.LG

TL;DR: 研究表明，大型预训练变压器模型在推理过程中实现的内在学习（ICL）并不完全符合如普通最小二乘法等学习规则的假设，且其对提示分布变化表现出有限的泛化能力。研究还发现预训练语料对ICL行为起重要作用，通过谱分析揭示了其学习表示中的分布特性。


<details>
  <summary>Details</summary>
Motivation: 探究大规模预训练变压器模型在推理时实现内在学习（ICL）机制的奥秘，尤其是它如何进行学习以及如何受训练语料影响。

Method: 使用合成线性回归作为研究手段，通过超出分布的泛化实验，并结合谱分析研究残差流中的学习表示特征。

Result: 发现变压器在提示分布变化情况下表现有限的泛化能力，与OLS等算法实现的预期不一致；谱分析显示，训练数据分布的输入在残差流中产生特有的光谱特征，而非分布外的输入则缺乏该特征。

Conclusion: 变压器中ICL的行为与直接实现学习算法的假设不完全匹配，预训练数据对ICL性能有重要影响，这为理解和改进ICL提供了新的视角。

Abstract: In-context learning (ICL) has emerged as a powerful capability of large
pretrained transformers, enabling them to solve new tasks implicit in example
input-output pairs without any gradient updates. Despite its practical success,
the mechanisms underlying ICL remain largely mysterious. In this work we study
synthetic linear regression to probe how transformers implement learning at
inference time. Previous works have demonstrated that transformers match the
performance of learning rules such as Ordinary Least Squares (OLS) regression
or gradient descent and have suggested ICL is facilitated in transformers
through the learned implementation of one of these techniques. In this work, we
demonstrate through a suite of out-of-distribution generalization experiments
that transformers trained for ICL fail to generalize after shifts in the prompt
distribution, a behaviour that is inconsistent with the notion of transformers
implementing algorithms such as OLS. Finally, we highlight the role of the
pretraining corpus in shaping ICL behaviour through a spectral analysis of the
learned representations in the residual stream. Inputs from the same
distribution as the training data produce representations with a unique
spectral signature: inputs from this distribution tend to have the same top two
singular vectors. This spectral signature is not shared by out-of-distribution
inputs, and a metric characterizing the presence of this signature is highly
correlated with low loss.

</details>


### [340] [Toward Developing Machine-Learning-Aided Tools for the Thermomechanical Monitoring of Nuclear Reactor Components](https://arxiv.org/abs/2507.09443)
*Luiz Aldeia Machado,Victor Coppo Leite,Elia Merzari,Arthur Motta,Roberto Ponciroli,Lander Ibarra,Lise Charlot*

Main category: cs.LG

TL;DR: 本文研究通过卷积神经网络（CNN）与计算热机械模型相结合，预测核电站压水堆燃料棒在运行期间的温度、应力和应变分布，进而支持预测性维护（PdM）。


<details>
  <summary>Details</summary>
Motivation: 由于组件失效可能导致意外停机，核电站需要减少停机时间的主动维护策略。本文希望利用CNN解决燃料棒在少量温度测量条件下的实时监测问题。

Method: 结合有限元模拟生成数据集，通过BISON和MOOSE-THM的模拟，训练CNN对燃料棒的温度分布进行预测，并使用热机械模型进一步计算应力与应变分布。

Result: 训练使用了11组仿真数据，其中8组用于训练，2组用于验证，1组用于测试。经过1000多次迭代，模型预测的温度分布极为准确，无明显过拟合，基础温度数据进一步成功用于计算应力和应变分布。

Conclusion: 系统中结合CNN和热机械模型的方法有助于核反应堆的预测性维护，能够实现实时精确的燃料棒运作状态监测。

Abstract: Proactive maintenance strategies, such as Predictive Maintenance (PdM), play
an important role in the operation of Nuclear Power Plants (NPPs), particularly
due to their capacity to reduce offline time by preventing unexpected shutdowns
caused by component failures.
  In this work, we explore the use of a Convolutional Neural Network (CNN)
architecture combined with a computational thermomechanical model to calculate
the temperature, stress, and strain of a Pressurized Water Reactor (PWR) fuel
rod during operation. This estimation relies on a limited number of temperature
measurements from the cladding's outer surface. This methodology can
potentially aid in developing PdM tools for nuclear reactors by enabling
real-time monitoring of such systems.
  The training, validation, and testing datasets were generated through coupled
simulations involving BISON, a finite element-based nuclear fuel performance
code, and the MOOSE Thermal-Hydraulics Module (MOOSE-THM). We conducted eleven
simulations, varying the peak linear heat generation rates. Of these, eight
were used for training, two for validation, and one for testing.
  The CNN was trained for over 1,000 epochs without signs of overfitting,
achieving highly accurate temperature distribution predictions. These were then
used in a thermomechanical model to determine the stress and strain
distribution within the fuel rod.

</details>


### [341] [Fourier Basis Mapping: A Time-Frequency Learning Framework for Time Series Forecasting](https://arxiv.org/abs/2507.09445)
*Runze Yang,Longbing Cao,Xin You,Kun Fang,Jianxun Li,Jie Yang*

Main category: cs.LG

TL;DR: 本文结合傅立叶变换和深度学习，提出了一种称为傅立叶基映射（FBM）的新方法，解决了现有方法在时间序列分析中的若干问题，并验证了其在现实数据集上的效果优越性。


<details>
  <summary>Details</summary>
Motivation: 通过傅立叶变换和深度学习的结合，优化时间序列预测中的频率特征提取和时间信息保留，解决现有方法中起始周期不一致和序列长度不一致等问题。

Method: 提出傅立叶基映射（FBM）方法，该方法通过傅立叶基展开和时间频率空间映射，提取显式频率特征和时间特征，并可通过调整初始投影层与神经网络无缝集成。同时构建专用模型架构（FBM-S），分解季节性、趋势性和交互效应模块化建模，并引入多种针对时间频率特征的技术。

Result: 验证显示，FBM方法在不同数据集上的长期和短期预测任务中达到了SOTA效果，证明了其优越性。

Conclusion: FBM方法通过融合时间频率特征，显著提升了时间序列预测的性能，其设计可扩展性使其可以与不同神经网络模型集成应用。

Abstract: The integration of Fourier transform and deep learning opens new avenues for
time series forecasting. We reconsider the Fourier transform from a basis
functions perspective. Specifically, the real and imaginary parts of the
frequency components can be regarded as the coefficients of cosine and sine
basis functions at tiered frequency levels, respectively. We find that existing
Fourier-based methods face inconsistent starting cycles and inconsistent series
length issues. They fail to interpret frequency components precisely and
overlook temporal information. Accordingly, the novel Fourier Basis Mapping
(FBM) method addresses these issues by integrating time-frequency features
through Fourier basis expansion and mapping in the time-frequency space. Our
approach extracts explicit frequency features while preserving temporal
characteristics. FBM supports plug-and-play integration with various types of
neural networks by only adjusting the first initial projection layer for better
performance. First, we propose FBM-L, FBM-NL, and FBM-NP to enhance linear,
MLP-based, and Transformer-based models, respectively, demonstrating the
effectiveness of time-frequency features. Next, we propose a synergetic model
architecture, termed FBM-S, which decomposes the seasonal, trend, and
interaction effects into three separate blocks, each designed to model
time-frequency features in a specialized manner. Finally, we introduce several
techniques tailored for time-frequency features, including interaction masking,
centralization, patching, rolling window projection, and multi-scale
down-sampling. The results are validated on diverse real-world datasets for
both long-term and short-term forecasting tasks with SOTA performance.

</details>


### [342] [Enhancing ALS Progression Tracking with Semi-Supervised ALSFRS-R Scores Estimated from Ambient Home Health Monitoring](https://arxiv.org/abs/2507.09460)
*Noah Marchal,William E. Janes,Mihail Popescu,Xing Song*

Main category: cs.LG

TL;DR: 这篇论文提出了一种利用连续的居家传感器监测数据和半监督回归模型来追踪ALS（肌萎缩性侧索硬化症）患者功能衰退轨迹的技术。


<details>
  <summary>Details</summary>
Motivation: 传统ALS临床监测依赖定期评估，无法捕捉访视之间的重要变化，激发了开发精确追踪疾病进展方法的需求。

Method: 论文比较了三种模型：个体批量学习、队列批量学习和增量微调迁移学习，并使用不同插值方法（线性、三次多项式和基于自注意的伪标签插值）来预测ALSFRS-R评分率的变化轨迹。

Result: 迁移学习方法在大多数情况下（28/32）降低了ALSFRS-R子量表的预测误差（平均RMSE=0.20±0.04），而自注意插值在捕捉复杂非线性进展模式上表现最佳（RMSE=0.19±0.06），超过了其他插值方法。不同功能领域呈现出了不同的同质异质性模式，部分领域更适合个性化调整，部分更适合队列水平的模型。

Conclusion: 匹配学习方法与功能领域的特性能够提高ALS疾病进展预测的准确性，灵活的模型选择可集成至传感器监测平台，为多中心研究提供支持并改善患者的干预时机。

Abstract: Clinical monitoring of functional decline in ALS relies on periodic
assessments that may miss critical changes occurring between visits. To address
this gap, semi-supervised regression models were developed to estimate rates of
decline in a case series cohort by targeting ALSFRS- R scale trajectories with
continuous in-home sensor monitoring data. Our analysis compared three model
paradigms (individual batch learning and cohort-level batch versus incremental
fine-tuned transfer learning) across linear slope, cubic polynomial, and
ensembled self-attention pseudo-label interpolations. Results revealed cohort
homogeneity across functional domains responding to learning methods, with
transfer learning improving prediction error for ALSFRS-R subscales in 28 of 32
contrasts (mean RMSE=0.20(0.04)), and individual batch learning for predicting
the composite scale (mean RMSE=3.15(1.25)) in 2 of 3. Self-attention
interpolation achieved the lowest prediction error for subscale-level models
(mean RMSE=0.19(0.06)), capturing complex nonlinear progression patterns,
outperforming linear and cubic interpolations in 20 of 32 contrasts, though
linear interpolation proved more stable in all ALSFRS-R composite scale models
(mean RMSE=0.23(0.10)). We identified distinct homogeneity-heterogeneity
profiles across functional domains with respiratory and speech exhibiting
patient-specific patterns benefiting from personalized incremental adaptation,
while swallowing and dressing functions followed cohort-level trajectories
suitable for transfer models. These findings suggest that matching learning and
pseudo-labeling techniques to functional domain-specific
homogeneity-heterogeneity profiles enhances predictive accuracy in ALS
progression tracking. Integrating adaptive model selection within sensor
monitoring platforms could enable timely interventions and scalable deployment
in future multi-center studies.

</details>


### [343] [La-Proteina: Atomistic Protein Generation via Partially Latent Flow Matching](https://arxiv.org/abs/2507.09466)
*Tomas Geffner,Kieran Didi,Zhonglin Cao,Danny Reidenbach,Zuobai Zhang,Christian Dallago,Emine Kucukbenli,Karsten Kreis,Arash Vahdat*

Main category: cs.LG

TL;DR: La-Proteina模型通过一个新颖的部分潜变量蛋白结构表示，实现了全原子水平的蛋白质设计，解决了侧链长度变化等挑战，达到了多个基准测试的先进性能。


<details>
  <summary>Details</summary>
Motivation: 目前，大多数生成模型无法直接生成全原子结构及其对应的氨基酸序列，这导致了在侧链长度变化等细节上的设计瓶颈。

Method: 引入部分潜变量表示法：显式建模骨架结构，通过每个残基的固定维度潜变量捕获序列和原子细节，结合部分潜变量空间中的流匹配来联合建模序列和全原子结构的分布。

Result: La-Proteina在多个生成基准测试中表现优异，包括全原子共同设计性、多样性和结构有效性。此外，该模型在基于原子特征的结构设计任务中也优于现有方法，展现了其可扩展性和可靠性。

Conclusion: La-Proteina突破现有模型的限制，成为蛋白质设计领域一个高效且创新的工具，对大规模蛋白生成任务具有重要的应用潜力。

Abstract: Recently, many generative models for de novo protein structure design have
emerged. Yet, only few tackle the difficult task of directly generating fully
atomistic structures jointly with the underlying amino acid sequence. This is
challenging, for instance, because the model must reason over side chains that
change in length during generation. We introduce La-Proteina for atomistic
protein design based on a novel partially latent protein representation: coarse
backbone structure is modeled explicitly, while sequence and atomistic details
are captured via per-residue latent variables of fixed dimensionality, thereby
effectively side-stepping challenges of explicit side-chain representations.
Flow matching in this partially latent space then models the joint distribution
over sequences and full-atom structures. La-Proteina achieves state-of-the-art
performance on multiple generation benchmarks, including all-atom
co-designability, diversity, and structural validity, as confirmed through
detailed structural analyses and evaluations. Notably, La-Proteina also
surpasses previous models in atomistic motif scaffolding performance, unlocking
critical atomistic structure-conditioned protein design tasks. Moreover,
La-Proteina is able to generate co-designable proteins of up to 800 residues, a
regime where most baselines collapse and fail to produce valid samples,
demonstrating La-Proteina's scalability and robustness.

</details>


### [344] [Discrete Differential Principle for Continuous Smooth Function Representation](https://arxiv.org/abs/2507.09480)
*Guoyou Wang,Yihua Tan,Shiqi Liu*

Main category: cs.LG

TL;DR: 提出了一种基于Vandermonde矩阵的新型离散微分算子，可以高精度估计导数并局部表示连续平滑函数，具有广泛潜在应用。


<details>
  <summary>Details</summary>
Motivation: 解决泰勒公式在高维情况下的误差传播和导数计算困难问题。

Method: 通过Vandermonde矩阵从截断的泰勒级数中估算导数，并在等距离均匀采样下实现高阶精度，适用于多维导数计算。

Result: 减少了误差传播和维度诅咒，具有比有限前向差分法更优的性能，实验验证了其在函数表示和导数估计上的优越性。

Conclusion: 新方法提高了函数表示和导数估计的精度，潜在应用广泛。

Abstract: Taylor's formula holds significant importance in function representation,
such as solving differential difference equations, ordinary differential
equations, partial differential equations, and further promotes applications in
visual perception, complex control, fluid mechanics, weather forecasting and
thermodynamics. However, the Taylor's formula suffers from the curse of
dimensionality and error propagation during derivative computation in discrete
situations. In this paper, we propose a new discrete differential operator to
estimate derivatives and to represent continuous smooth function locally using
the Vandermonde coefficient matrix derived from truncated Taylor series. Our
method simultaneously computes all derivatives of orders less than the number
of sample points, inherently mitigating error propagation. Utilizing
equidistant uniform sampling, it achieves high-order accuracy while alleviating
the curse of dimensionality. We mathematically establish rigorous error bounds
for both derivative estimation and function representation, demonstrating
tighter bounds for lower-order derivatives. We extend our method to the
two-dimensional case, enabling its use for multivariate derivative
calculations. Experiments demonstrate the effectiveness and superiority of the
proposed method compared to the finite forward difference method for derivative
estimation and cubic spline and linear interpolation for function
representation. Consequently, our technique offers broad applicability across
domains such as vision representation, feature extraction, fluid mechanics, and
cross-media imaging.

</details>


### [345] [An Analysis of Action-Value Temporal-Difference Methods That Learn State Values](https://arxiv.org/abs/2507.09523)
*Brett Daley,Prabhat Nagarajan,Martha White,Marlos C. Machado*

Main category: cs.LG

TL;DR: 本文分析了基于两种不对称价值函数的TD学习算法，提出了一种新的AV学习算法RDQ，并证明了其在MinAtar基准中的优势。


<details>
  <summary>Details</summary>
Motivation: 现有TD控制方法大多基于单一的动作价值函数，本文旨在探讨两种不对称价值函数的学习方法的理论及效率优势。

Method: 分析了现有QV-learning和AV-learning算法在收敛性和样本效率方面的性能，并提出一种新算法RDQ。

Result: 研究发现AV-learning在控制设置中优于QV-learning和Q-learning，提出的RDQ算法在MinAtar基准测试中表现显著优于Dueling DQN。

Conclusion: 学习两种不对称价值函数的AV方法在特定控制问题中表现出潜在优势，且新提出的RDQ算法提供了进一步的性能提升。

Abstract: The hallmark feature of temporal-difference (TD) learning is bootstrapping:
using value predictions to generate new value predictions. The vast majority of
TD methods for control learn a policy by bootstrapping from a single
action-value function (e.g., Q-learning and Sarsa). Significantly less
attention has been given to methods that bootstrap from two asymmetric value
functions: i.e., methods that learn state values as an intermediate step in
learning action values. Existing algorithms in this vein can be categorized as
either QV-learning or AV-learning. Though these algorithms have been
investigated to some degree in prior work, it remains unclear if and when it is
advantageous to learn two value functions instead of just one -- and whether
such approaches are theoretically sound in general. In this paper, we analyze
these algorithmic families in terms of convergence and sample efficiency. We
find that while both families are more efficient than Expected Sarsa in the
prediction setting, only AV-learning methods offer any major benefit over
Q-learning in the control setting. Finally, we introduce a new AV-learning
algorithm called Regularized Dueling Q-learning (RDQ), which significantly
outperforms Dueling DQN in the MinAtar benchmark.

</details>


### [346] [Assessing reliability of explanations in unbalanced datasets: a use-case on the occurrence of frost events](https://arxiv.org/abs/2507.09545)
*Ilaria Vascotto,Valentina Blasone,Alex Rodriguez,Alessandro Bonaita,Luca Bortolussi*

Main category: cs.LG

TL;DR: 本研究探讨了解释型人工智能（XAI）方法在处理不平衡数据集时的鲁棒性评估的重要性，并提出了一种简化评估方法。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能（AI）模型的广泛部署以及近年相关法律法规出台，XAI方法的实用性和解释能力变得尤为重要。然而，解释的鲁棒性往往被低估，这对于建立信任至关重要。

Method: 提出了一种专注于少数类的评估方法，包括利用流形生成领域的邻域数据、解释聚合以及测试解释一致性的指标。

Result: 采用一个基于包含数值特征和霜冻事件的分类数据集的用例验证方法，展示了该方法在高风险不平衡数据集上的适用性。

Conclusion: 研究揭示了解释型AI方法在不平衡数据集上下的一些初步问题，并提供了一种评估鲁棒性及解释可靠性的新视角。

Abstract: The usage of eXplainable Artificial Intelligence (XAI) methods has become
essential in practical applications, given the increasing deployment of
Artificial Intelligence (AI) models and the legislative requirements put
forward in the latest years. A fundamental but often underestimated aspect of
the explanations is their robustness, a key property that should be satisfied
in order to trust the explanations. In this study, we provide some preliminary
insights on evaluating the reliability of explanations in the specific case of
unbalanced datasets, which are very frequent in high-risk use-cases, but at the
same time considerably challenging for both AI models and XAI methods. We
propose a simple evaluation focused on the minority class (i.e. the less
frequent one) that leverages on-manifold generation of neighbours, explanation
aggregation and a metric to test explanation consistency. We present a use-case
based on a tabular dataset with numerical features focusing on the occurrence
of frost events.

</details>


### [347] [Holistix: A Dataset for Holistic Wellness Dimensions Analysis in Mental Health Narratives](https://arxiv.org/abs/2507.09565)
*Heeba Shakeel,Tanvir Ahmad,Chandni Saxena*

Main category: cs.LG

TL;DR: 本文介绍了一个数据集，用于社交媒体用户帖子中健康维度的分类，涵盖物理、情感、社交等六个方面。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏针对用户生成内容的全面健康维度分类工具，研究意在填补该领域的空白。

Method: 研究引入了一套由领域专家指导开发的注释框架，并通过传统机器学习模型和基于transformer的高级模型进行分类评估，同时加入后验解释以提高模型透明性。

Result: 所提出的框架通过10折交叉验证评估，采用了多项指标（如精准度、召回率和F1-score），并公开了数据集以促进研究推广。

Conclusion: 该数据集为社交媒体健康维度的评估奠定了基础，有助于个性化健康评估，并为心理健康的早期干预提供了参考工具。

Abstract: We introduce a dataset for classifying wellness dimensions in social media
user posts, covering six key aspects: physical, emotional, social,
intellectual, spiritual, and vocational. The dataset is designed to capture
these dimensions in user-generated content, with a comprehensive annotation
framework developed under the guidance of domain experts. This framework allows
for the classification of text spans into the appropriate wellness categories.
We evaluate both traditional machine learning models and advanced
transformer-based models for this multi-class classification task, with
performance assessed using precision, recall, and F1-score, averaged over
10-fold cross-validation. Post-hoc explanations are applied to ensure the
transparency and interpretability of model decisions. The proposed dataset
contributes to region-specific wellness assessments in social media and paves
the way for personalized well-being evaluations and early intervention
strategies in mental health. We adhere to ethical considerations for
constructing and releasing our experiments and dataset publicly on Github.

</details>


### [348] [DRAGD: A Federated Unlearning Data Reconstruction Attack Based on Gradient Differences](https://arxiv.org/abs/2507.09602)
*Bocheng Ju,Junchao Fan,Jiaqi Liu,Xiaolin Chang*

Main category: cs.LG

TL;DR: 本文提出储存在联邦学习中的隐私风险，即删除数据期间的梯度交换可能泄露敏感信息。提出两种攻击方法DRAGD和DRAGDP增强数据重建能力，揭示联邦学习中重要的隐私漏洞。


<details>
  <summary>Details</summary>
Motivation: 发现现有联邦学习中的数据删除过程可能导致敏感信息泄露，需要研究如何改进安全性。

Method: 通过利用删除前后梯度差异（DRAGD）以及公开的先验数据（DRAGDP），测试这两种方法在不同复杂数据集上的数据重建效率。

Result: 实验结果表明，DRAGD和DRAGDP在数据重建方面优于当前方法，同时揭示联邦学习系统中的隐私缺陷。

Conclusion: 该研究强调了联邦学习中存在的隐私漏洞并提出实用的解决方案，为实际应用中的联邦学习系统安全做出贡献。

Abstract: Federated learning enables collaborative machine learning while preserving
data privacy. However, the rise of federated unlearning, designed to allow
clients to erase their data from the global model, introduces new privacy
concerns. Specifically, the gradient exchanges during the unlearning process
can leak sensitive information about deleted data. In this paper, we introduce
DRAGD, a novel attack that exploits gradient discrepancies before and after
unlearning to reconstruct forgotten data. We also present DRAGDP, an enhanced
version of DRAGD that leverages publicly available prior data to improve
reconstruction accuracy, particularly for complex datasets like facial images.
Extensive experiments across multiple datasets demonstrate that DRAGD and
DRAGDP significantly outperform existing methods in data reconstruction.Our
work highlights a critical privacy vulnerability in federated unlearning and
offers a practical solution, advancing the security of federated unlearning
systems in real-world applications.

</details>


### [349] [Multiple Choice Learning of Low Rank Adapters for Language Modeling](https://arxiv.org/abs/2507.10419)
*Victor Letzelter,Hugo Malard,Mathieu Fontaine,Gaël Richard,Slim Essid,Andrei Bursuc,Patrick Pérez*

Main category: cs.LG

TL;DR: 提出了LoRA-MCL，一种改进语言模型生成多样句子续写的训练方法，通过多选择学习与WTA损失机制提升推断多样性及合理性。


<details>
  <summary>Details</summary>
Motivation: 语言建模面临的多解性挑战即在上下文确定的情况下存在多个可能的合理句子续写。

Method: 基于低秩适配（LoRA），将多选择学习（MCL）与胜者为王（WTA）损失相结合，处理生成多样性的难题，并对混合分布的数据进行理论解释和实验验证。

Result: 通过对马尔科夫链混合分布的数据样本和现实世界的视觉、音频标题生成任务实验显示，方法在生成多样性与相关性方面表现优异。

Conclusion: LoRA-MCL能够有效提高语言模型生成的多样性及相关性，展现出在实际任务中的良好应用潜力。

Abstract: We propose LoRA-MCL, a training scheme that extends next-token prediction in
language models with a method designed to decode diverse, plausible sentence
continuations at inference time. Traditional language modeling is an
intrinsically ill-posed problem: given a context, multiple futures may be
equally plausible. Our approach leverages Multiple Choice Learning (MCL) and
the Winner-Takes-All (WTA) loss to efficiently handle ambiguity through
Low-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying
Multiple Choice Learning to Language Modeling, assuming the data is generated
from a mixture of distributions. To illustrate the proposed approach, we use
data sampled from mixtures of Markov chains. We then demonstrate with extensive
experiments on real-world visual and audio captioning tasks that our method
achieves high diversity and relevance in generated outputs.

</details>


### [350] [MLoRQ: Bridging Low-Rank and Quantization for Transformer Compression](https://arxiv.org/abs/2507.09616)
*Ofir Gordon,Ariel Lapid,Elad Cohen,Yarden Yagil,Arnon Netzer,Hai Victor Habi*

Main category: cs.LG

TL;DR: 论文提出了一种名为MLoRQ的新方法，将低秩近似和混合精度量化技术结合，用于资源受限设备上的变压器模型优化。


<details>
  <summary>Details</summary>
Motivation: 解决变压器模型在资源受限设备上的部署问题，实现更高效的内存和计算利用率。

Method: MLoRQ通过两阶段优化实现：第一阶段在层内优化中，寻找所有低秩和量化组合中潜在的优化解；第二阶段在层间优化中，为每一层分配最优的比特宽度和秩，同时满足内存限制。最后还可通过修改的自适应舍入技术进一步优化。

Result: 在图像分类、目标检测和实例分割任务中，MLoRQ在Vision Transformers上实现了最多15%的性能提升，达到当前领先水平。

Conclusion: MLoRQ方法与大多数现有的量化算法兼容，并提供了一种有效的解决方案，通过联合低秩近似和量化技术显著提高模型的性能和部署效率。

Abstract: Deploying transformer-based neural networks on resource-constrained edge
devices presents a significant challenge. This challenge is often addressed
through various techniques, such as low-rank approximation and mixed-precision
quantization. In this work, we introduce Mixed Low-Rank and Quantization
(MLoRQ), a novel method that integrates both techniques. MLoRQ employs a
two-stage optimization process to determine optimal bit-width and rank
assignments for each layer, adhering to predefined memory constraints. This
process includes: (i) an intra-layer optimization that identifies potentially
optimal compression solutions out of all low-rank and quantization
combinations; (ii) an inter-layer optimization that assigns bit-width precision
and rank to each layer while ensuring the memory constraint is met. An optional
final step applies a sequential optimization process using a modified adaptive
rounding technique to mitigate compression-induced errors in joint low-rank
approximation and quantization. The method is compatible and can be seamlessly
integrated with most existing quantization algorithms. MLoRQ shows
state-of-the-art results with up to 15\% performance improvement, evaluated on
Vision Transformers for image classification, object detection, and instance
segmentation tasks.

</details>


### [351] [Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset](https://arxiv.org/abs/2507.09650)
*Lily Hong Zhang,Smitha Milli,Karen Jusko,Jonathan Smith,Brandon Amos,Wassim,Bouaziz,Manon Revel,Jack Kussman,Lisa Titus,Bhaktipriya Radharapu,Jane Yu,Vidya Sarma,Kris Rose,Maximilian Nickel*

Main category: cs.LG

TL;DR: 该论文探讨了如何使大型语言模型（LLMs）满足不同文化、政治等维度用户的多样化和冲突性需求，并提出了一个新数据集来改进LLMs的对齐能力。


<details>
  <summary>Details</summary>
Motivation: 旨在解决LLMs在面对全球多样化用户偏好时的局限性，尤其是现有方法难以捕捉人类偏好的多样性。

Method: 通过多语言大规模人类研究、结合负相关采样技术与提示生成方法，创建大型偏好数据集Community Alignment。

Result: 建立了最大且最具代表性的多语言、多轮次偏好数据集Community Alignment，包含约20万次比较数据，并证明使用负相关采样提高了偏好学习的效率。

Conclusion: 该研究提出的新方法和数据集为提升LLMs对全球多样化用户需求的适应能力提供了重要资源，促进了模型的公平性与泛用性。

Abstract: How can large language models (LLMs) serve users with varying preferences
that may conflict across cultural, political, or other dimensions? To advance
this challenge, this paper establishes four key results. First, we demonstrate,
through a large-scale multilingual human study with representative samples from
five countries (N=15,000), that humans exhibit significantly more variation in
preferences than the responses of 21 state-of-the-art LLMs. Second, we show
that existing methods for preference dataset collection are insufficient for
learning the diversity of human preferences even along two of the most salient
dimensions of variability in global values, due to the underlying homogeneity
of candidate responses. Third, we argue that this motivates the need for
negatively-correlated sampling when generating candidate sets, and we show that
simple prompt-based techniques for doing so significantly enhance the
performance of alignment methods in learning heterogeneous preferences. Fourth,
based on this novel candidate sampling approach, we collect and open-source
Community Alignment, the largest and most representative multilingual and
multi-turn preference dataset to date, featuring almost 200,000 comparisons
from annotators spanning five countries. We hope that the Community Alignment
dataset will be a valuable resource for improving the effectiveness of LLMs for
a diverse global population.

</details>


### [352] [Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination](https://arxiv.org/abs/2507.10532)
*Mingqi Wu,Zhihao Zhang,Qiaole Dong,Zhiheng Xi,Jun Zhao,Senjie Jin,Xiaoran Fan,Yuhao Zhou,Yanwei Fu,Qin Liu,Songyang Zhang,Qi Zhang*

Main category: cs.LG

TL;DR: 本文分析了大语言模型（LLMs）在推理能力上的研究，指出现有一些成果依赖于受污染的数据集，提出了新的无泄漏数据集评估方法。


<details>
  <summary>Details</summary>
Motivation: 评估现有强化学习方法在改进LLMs推理能力上的可靠性，以及揭示数据污染对模型表现评估的影响。

Method: 提出了一个全新的随机生成算术问题的生成器RandomCalculation，产生完全合成的无泄漏数据集，用于模型评估。

Result: 发现只有准确的奖励信号能一致改善模型性能，而噪声或错误信号对性能无帮助，同时揭示了Qwen2.5模型因预训练数据污染导致评估结果不可靠。

Conclusion: 强调应在无污染的基准数据集和多样化模型上评估强化学习方法，确保推理性能改进的可信性。

Abstract: The reasoning capabilities of large language models (LLMs) have been a
longstanding focus of research. Recent works have further enhanced these
capabilities using reinforcement learning (RL), with many new methods claiming
significant improvements with minimal or no external supervision. Surprisingly,
some studies even suggest that random or incorrect reward signals can enhance
reasoning performance. However, these breakthroughs are mostly reported on the
Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,
AMC, and AIME, while failing to achieve similar gains on other models like
Llama, which warrants further investigation. Our analysis shows that although
Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on
large-scale web corpora makes it vulnerable to data contamination in popular
benchmarks. As a result, results derived from these benchmarks may be
unreliable. To address this, we introduce a generator that produces fully
synthetic arithmetic problems of arbitrary length and difficulty, yielding a
clean dataset we call RandomCalculation. Using these leakage-free datasets, we
show that only accurate reward signals consistently improve performance, while
noisy or incorrect signals do not. We advocate for evaluating RL methods on
uncontaminated benchmarks and across diverse model families to ensure
trustworthy conclusions.

</details>


### [353] [Conformal Prediction for Privacy-Preserving Machine Learning](https://arxiv.org/abs/2507.09678)
*Alexander David Balinsky,Dominik Krzeminski,Alexander Balinsky*

Main category: cs.LG

TL;DR: 研究了将保序预测与监督学习结合用于加密数据的可能性，在MNIST数据上通过实验验证了方法的有效性，展示了加密情况下的预测结果和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 探讨在数据加密状态下进行机器学习的不确定性量化，同时保证隐私保护的可能性。

Method: 将保序预测方法与AES加密的MNIST数据集结合，分别测试p值和e值保序预测的表现，以验证其在加密数据域中的有效性。

Result: 在加密数据上，模型取得了远高于随机预测的准确率；e值保序预测在校准损失阈值4.3情况下，测试集中覆盖率超过60%。

Conclusion: 结合保序预测和加密技术的研究展现了隐私保护机器学习中的潜力，同时也指出预测集紧凑性和可靠性之间的权衡。

Abstract: We investigate the integration of Conformal Prediction (CP) with supervised
learning on deterministically encrypted data, aiming to bridge the gap between
rigorous uncertainty quantification and privacy-preserving machine learning.
Using AES-encrypted variants of the MNIST dataset, we demonstrate that CP
methods remain effective even when applied directly in the encrypted domain,
owing to the preservation of data exchangeability under fixed-key encryption.
We test traditional $p$-value-based against $e$-value-based conformal
predictors. Our empirical evaluation reveals that models trained on
deterministically encrypted data retain the ability to extract meaningful
structure, achieving 36.88\% test accuracy -- significantly above random
guessing (9.56\%) observed with per-instance encryption. Moreover,
$e$-value-based CP achieves predictive set coverage of over 60\% with 4.3
loss-threshold calibration, correctly capturing the true label in 4888 out of
5000 test cases. In contrast, the $p$-value-based CP yields smaller predictive
sets but with reduced coverage accuracy. These findings highlight both the
promise and limitations of CP in encrypted data settings and underscore
critical trade-offs between prediction set compactness and reliability. %Our
work sets a foundation for principled uncertainty quantification in secure,
privacy-aware learning systems.

</details>


### [354] [Networked Information Aggregation via Machine Learning](https://arxiv.org/abs/2507.09683)
*Michael Kearns,Aaron Roth,Emily Ryu*

Main category: cs.LG

TL;DR: 研究分布式学习，其中学习代理位于有向无环图(DAG)中，通过拓扑排序的序列对数据进行学习，分析了在代理仅具有部分特征访问权限的情况下是否能够实现信息聚合。


<details>
  <summary>Details</summary>
Motivation: 探讨分布式设置下代理在各自特征访问受限的情况下，通过信息共享实现类似于集中式全特征访问的学习效果。

Method: 通过理论分析定义信息聚合条件, 并围绕DAG深度这一关键参数提出上界和下界，探讨了线性与一般假设类的应用。结合实验验证理论发现。

Result: 发现DAG的深度是信息聚合的关键参数，路径足够深时可实现信息聚合。提出了特定分布条件下信息聚合无法实现的限制。

Conclusion: DAG结构中的信息聚合依赖于路径深度及特征表示条件，为设计分布式学习系统提供理论指导。

Abstract: We study a distributed learning problem in which learning agents are embedded
in a directed acyclic graph (DAG). There is a fixed and arbitrary distribution
over feature/label pairs, and each agent or vertex in the graph is able to
directly observe only a subset of the features -- potentially a different
subset for every agent. The agents learn sequentially in some order consistent
with a topological sort of the DAG, committing to a model mapping observations
to predictions of the real-valued label. Each agent observes the predictions of
their parents in the DAG, and trains their model using both the features of the
instance that they directly observe, and the predictions of their parents as
additional features. We ask when this process is sufficient to achieve
\emph{information aggregation}, in the sense that some agent in the DAG is able
to learn a model whose error is competitive with the best model that could have
been learned (in some hypothesis class) with direct access to \emph{all}
features, despite the fact that no single agent in the network has such access.
We give upper and lower bounds for this problem for both linear and general
hypothesis classes. Our results identify the \emph{depth} of the DAG as the key
parameter: information aggregation can occur over sufficiently long paths in
the DAG, assuming that all of the relevant features are well represented along
the path, and there are distributions over which information aggregation cannot
occur even in the linear case, and even in arbitrarily large DAGs that do not
have sufficient depth (such as a hub-and-spokes topology in which the spoke
vertices collectively see all the features). We complement our theoretical
results with a comprehensive set of experiments.

</details>


### [355] [Post-Training Quantization of Generative and Discriminative LSTM Text Classifiers: A Study of Calibration, Class Balance, and Robustness](https://arxiv.org/abs/2507.09687)
*Md Mushfiqur Rahaman,Elliot Chang,Tasmiah Haque,Srinjoy Das*

Main category: cs.LG

TL;DR: 本文研究了在边缘计算中应用生成型和判别型LSTM文本分类模型，并通过后训练量化（PTQ）进行优化，发现生成型模型在量化后对位宽、校准数据和输入噪声更为敏感。


<details>
  <summary>Details</summary>
Motivation: 在边缘计算中，面对实时环境中低延迟和高准确性的需求，同时注重对异常数据的鲁棒性，因此需要对生成型和判别型分类器在PTQ条件下的表现进行探讨。

Method: 通过Brevitas量化库对生成型和判别型LSTM模型进行多位宽量化实验，评估其在一般和噪声输入条件下的鲁棒性，进一步考察校准数据的类别不平衡对模型的影响并分析权重和激活的调整情况。

Result: 判别型模型在量化后保持较强鲁棒性，而生成型模型在低位宽、校准数据类别不均衡及输入噪声较高的情况下表现显著下降。校准数据类别不均衡主要在生成型LSTM中引发权重调整不足。

Conclusion: 校准数据的类别分布对生成型分类器在PTQ条件下的表现至关重要，适当选择校准数据分布可提升生成型模型的边缘部署表现。

Abstract: Text classification plays a pivotal role in edge computing applications like
industrial monitoring, health diagnostics, and smart assistants, where low
latency and high accuracy are both key requirements. Generative classifiers, in
particular, have been shown to exhibit robustness to out-of-distribution and
noisy data, which is an extremely critical consideration for deployment in such
real-time edge environments. However, deploying such models on edge devices
faces computational and memory constraints. Post Training Quantization (PTQ)
reduces model size and compute costs without retraining, making it ideal for
edge deployment. In this work, we present a comprehensive comparative study of
generative and discriminative Long Short Term Memory (LSTM)-based text
classification models with PTQ using the Brevitas quantization library. We
evaluate both types of classifier models across multiple bitwidths and assess
their robustness under regular and noisy input conditions. We find that while
discriminative classifiers remain robust, generative ones are more sensitive to
bitwidth, calibration data used during PTQ, and input noise during quantized
inference. We study the influence of class imbalance in calibration data for
both types of classifiers, comparing scenarios with evenly and unevenly
distributed class samples including their effect on weight adjustments and
activation profiles during PTQ. Using test statistics derived from
nonparametric hypothesis testing, we identify that using class imbalanced data
during calibration introduces insufficient weight adaptation at lower bitwidths
for generative LSTM classifiers, thereby leading to degraded performance. This
study underscores the role of calibration data in PTQ and when generative
classifiers succeed or fail under noise, aiding deployment in edge
environments.

</details>


### [356] [Frequency-aware Surrogate Modeling With SMT Kernels For Advanced Data Forecasting](https://arxiv.org/abs/2507.09694)
*Nicolas Gonel,Paul Saves,Joseph Morlier*

Main category: cs.LG

TL;DR: 本文提出一个开源框架，用于发展相关核方法，特别是用户自定义和复合核在代理建模中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有的一些传统核函数在捕捉复杂动态行为与频率敏感领域中的表现存在局限性，需要一种更灵活、更强大的核函数框架。

Method: 通过将传统核函数扩展到包括指数平方正弦以及有理二次核等，并包括其导数，同时整合到开源工具箱SMT 2.0中来实现。

Result: 该框架通过正弦卡迪纳尔测试验证并成功应用于CO₂浓度和航空客流预测，证明了其在复杂、频率敏感领域中的潜力。

Conclusion: 提出的框架不仅灵活，支持标准与自定义核配置，还能通过混合不同核的方式应对特定问题，为复杂领域的代理建模提供了多种可能性。

Abstract: This paper introduces a comprehensive open-source framework for developing
correlation kernels, with a particular focus on user-defined and composition of
kernels for surrogate modeling. By advancing kernel-based modeling techniques,
we incorporate frequency-aware elements that effectively capture complex
mechanical behaviors and timefrequency dynamics intrinsic to aircraft systems.
Traditional kernel functions, often limited to exponential-based methods, are
extended to include a wider range of kernels such as exponential squared sine
and rational quadratic kernels, along with their respective firstand
second-order derivatives. The proposed methodologies are first validated on a
sinus cardinal test case and then applied to forecasting Mauna-Loa Carbon
Dioxide (CO 2 ) concentrations and airline passenger traffic. All these
advancements are integrated into the open-source Surrogate Modeling Toolbox
(SMT 2.0), providing a versatile platform for both standard and customizable
kernel configurations. Furthermore, the framework enables the combination of
various kernels to leverage their unique strengths into composite models
tailored to specific problems. The resulting framework offers a flexible
toolset for engineers and researchers, paving the way for numerous future
applications in metamodeling for complex, frequency-sensitive domains.

</details>


### [357] [EPT-2 Technical Report](https://arxiv.org/abs/2507.09703)
*Roberto Molinaro,Niall Siegenheim,Niels Poulsen,Jordan Dane Daubinet,Henry Martin,Mark Frey,Kevin Thiart,Alexander Jakob Dautel,Andreas Schlueter,Alex Grigoryev,Bogdan Danciu,Nikoo Ekhtiari,Bas Steunebrink,Leonie Wagner,Marvin Vincent Gabler*

Main category: cs.LG

TL;DR: 本文提出了最新的地球物理变换模型EPT-2，相较于前一版本EPT-1.5有显著改进，在地球系统预测方面达到了最新水平。


<details>
  <summary>Details</summary>
Motivation: 改进地球物理预测模型的准确性和效率，提供更先进的中长期天气预测能力。

Method: 提出并开发了新一代地球物理变换模型EPT-2以及基于模型的概率预测版本EPT-2e，并通过与同类方法进行比较验证其性能。

Result: EPT-2在10m和100m风速、2m温度及表面太阳辐射等预测上超越了现有AI天气模型如Microsoft Aurora以及ECMWF的数值预测系统IFS HRES。EPT-2e在中长期天气预测中的表现也超过了ECMWF ENS均值。

Conclusion: EPT-2及其概率模型EPT-2e的问世显著提升了天气预测的准确性和计算效率，并已通过jua.ai平台提供访问。

Abstract: We present EPT-2, the latest iteration in our Earth Physics Transformer (EPT)
family of foundation AI models for Earth system forecasting. EPT-2 delivers
substantial improvements over its predecessor, EPT-1.5, and sets a new state of
the art in predicting energy-relevant variables-including 10m and 100m wind
speed, 2m temperature, and surface solar radiation-across the full 0-240h
forecast horizon. It consistently outperforms leading AI weather models such as
Microsoft Aurora, as well as the operational numerical forecast system IFS HRES
from the European Centre for Medium-Range Weather Forecasts (ECMWF). In
parallel, we introduce a perturbation-based ensemble model of EPT-2 for
probabilistic forecasting, called EPT-2e. Remarkably, EPT-2e significantly
surpasses the ECMWF ENS mean-long considered the gold standard for medium- to
longrange forecasting-while operating at a fraction of the computational cost.
EPT models, as well as third-party forecasts, are accessible via the app.jua.ai
platform.

</details>


### [358] [Continental scale habitat modelling with artificial intelligence and multimodal earth observation](https://arxiv.org/abs/2507.09732)
*Sara Si-Moussi,Stephan Hennekens,Sander Mucher,Stan Los,Wilfried Thuiller*

Main category: cs.LG

TL;DR: 栖息地综合了支持生物多样性和维持自然对人类贡献的非生物条件和生物物理结构。本文探讨利用高分辨率遥感数据和人工智能工具改进大范围高分辨率栖息地图的可能性，提出了一种方法框架，并验证其准确性和转移性。


<details>
  <summary>Details</summary>
Motivation: 当前栖息地地图在主题或空间分辨率上存在不足，无法有效支持保育和恢复工作。研究动机是寻求提高栖息地分类精度的解决方案，尤其针对多栖息地共存环境和类不平衡问题。

Method: 研究评估了以高分辨率遥感数据和人工智能工具改进栖息地分类的方法；基于欧洲植被档案的植被样点，采用多光谱和合成孔径雷达图像并结合分层命名法和集成机器学习策略进行栖息地模型构建和验证。

Result: 利用分层分类策略解决了分类模糊，特别在碎片化景观中效果显著。结合多光谱与雷达图像提高了分类性能，加权集成学习进一步提升了准确性。方法框架在欧洲以外地区具有可转移性。

Conclusion: 本文提出了一种创新框架，有助于推动动态栖息地时序建模、栖息地分割及质量评估，同时建议结合下一代地球观测数据和更高质量的原位数据继续发展。

Abstract: Habitats integrate the abiotic conditions and biophysical structures that
support biodiversity and sustain nature's contributions to people. As these
ecosystems face mounting pressure from human activities, accurate,
high-resolution habitat maps are essential for effective conservation and
restoration. Yet current maps often fall short in thematic or spatial
resolution because they must (1) model several mutually exclusive habitat types
that co-occur across landscapes and (2) cope with severe class imbalance that
complicate multi-class training. Here, we evaluated how high-resolution remote
sensing (RS) data and Artificial Intelligence (AI) tools can improve habitat
classification over large geographic extents at fine thematic resolution. Using
vegetation plots from the European Vegetation Archive, we modelled Level 3
EUNIS habitats across Europe and assessed multiple modelling strategies against
independent validation datasets. Strategies that exploited the hierarchical
nature of habitat nomenclatures resolved classification ambiguities, especially
in fragmented landscapes. Integrating multi-spectral (MSI) and synthetic
aperture radar (SAR) imagery, particularly through Earth Observation Foundation
models, enhanced within-formation discrimination and overall performance.
Finally, ensemble machine learning that corrects class imbalance boosted
accuracy further. Our methodological framework is transferable beyond Europe
and adaptable to other classification systems. Future research should advance
temporal modelling of dynamic habitats, extend to habitat segmentation and
quality assessment, and exploit next-generation EO data paired with
higher-quality in-situ observations.

</details>


### [359] [Universal Physics Simulation: A Foundational Diffusion Approach](https://arxiv.org/abs/2507.09733)
*Bradley Camburn*

Main category: cs.LG

TL;DR: 该研究提出了一种基础性的AI模型，用于通用物理仿真，通过边界条件数据直接学习物理定律，无需预先编码方程。


<details>
  <summary>Details</summary>
Motivation: 传统物理神经网络和有限差分方法需要明确数学方程的限制，无法很好地普适于多领域并发现新物理规律。

Method: 采用草图引导的扩散变压器框架，将仿真视为条件生成问题，通过边界条件生成物理准确的稳态解。

Result: 模型实现了从边界到平衡态的直接映射，生成的稳态解具有SSIM > 0.8的精度，同时保持子像素级边界精度。

Conclusion: 研究实现从AI加速物理到AI发现物理的范式转变，建立了首个真正通用的物理仿真框架。

Abstract: We present the first foundational AI model for universal physics simulation
that learns physical laws directly from boundary-condition data without
requiring a priori equation encoding. Traditional physics-informed neural
networks (PINNs) and finite-difference methods necessitate explicit
mathematical formulation of governing equations, fundamentally limiting their
generalizability and discovery potential. Our sketch-guided diffusion
transformer approach reimagines computational physics by treating simulation as
a conditional generation problem, where spatial boundary conditions guide the
synthesis of physically accurate steady-state solutions.
  By leveraging enhanced diffusion transformer architectures with novel spatial
relationship encoding, our model achieves direct boundary-to-equilibrium
mapping and is generalizable to diverse physics domains. Unlike sequential
time-stepping methods that accumulate errors over iterations, our approach
bypasses temporal integration entirely, directly generating steady-state
solutions with SSIM > 0.8 while maintaining sub-pixel boundary accuracy. Our
data-informed approach enables physics discovery through learned
representations analyzable via Layer-wise Relevance Propagation (LRP),
revealing emergent physical relationships without predetermined mathematical
constraints. This work represents a paradigm shift from AI-accelerated physics
to AI-discovered physics, establishing the first truly universal physics
simulation framework.

</details>


### [360] [Do we need equivariant models for molecule generation?](https://arxiv.org/abs/2507.09753)
*Ewa M. Nowara,Joshua Rackers,Patricia Suriana,Pan Kessel,Max Shen,Andrew Martin Watkins,Michael Maser*

Main category: cs.LG

TL;DR: 研究了非等变CNN通过旋转增强学习等变性的方法，用于分子生成任务，结果表明这些模型可以接近等变模型的性能。


<details>
  <summary>Details</summary>
Motivation: 目前用于分子发现的深度生成模型往往依赖于等变GNN，但这些模型复杂、难以训练且扩展性较差，因此研究是否能通过非等变CNN实现相似性能成为一个值得探讨的问题。

Method: 提出通过对非等变CNN进行旋转增强训练，使其学习等变性，并引入一种将预测误差与等变误差分离的损失分解方法，系统地评估模型规模、数据集大小和训练时间对性能的影响。

Result: 在去噪、分子生成和属性预测任务中，非等变CNN的表现接近等变模型的性能，验证了此方法的有效性。

Conclusion: 通过适当的训练增强，非等变CNN可以胜任等变任务，为生成分子相关的任务提供了一种更简单高效的解决方案。

Abstract: Deep generative models are increasingly used for molecular discovery, with
most recent approaches relying on equivariant graph neural networks (GNNs)
under the assumption that explicit equivariance is essential for generating
high-quality 3D molecules. However, these models are complex, difficult to
train, and scale poorly.
  We investigate whether non-equivariant convolutional neural networks (CNNs)
trained with rotation augmentations can learn equivariance and match the
performance of equivariant models. We derive a loss decomposition that
separates prediction error from equivariance error, and evaluate how model
size, dataset size, and training duration affect performance across denoising,
molecule generation, and property prediction. To our knowledge, this is the
first study to analyze learned equivariance in generative tasks.

</details>


### [361] [Explainable AI in Genomics: Transcription Factor Binding Site Prediction with Mixture of Experts](https://arxiv.org/abs/2507.09754)
*Aakash Tripathi,Ian E. Nielsen,Muhammad Umer,Ravi P. Ramachandran,Ghulam Rasool*

Main category: cs.LG

TL;DR: 本文提出一种集成专家模型的全新方法，通过整合多种CNN模型以提高转录因子结合位点（TFBS）预测性能，并引入新的可解释性技术ShiftSmooth。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解基因调控和生物过程中的转录因子作用，提高TFBS的预测准确性。

Method: 提出了一种专家混合模型(MoE)，整合多种预训练的CNN模型，还创新性地提出ShiftSmooth用于解释模型决策。

Result: MoE模型在各种TFBS的预测中表现优异，尤其是在分布外（OOD）数据集中表现突出，并且ShiftSmooth提供了比传统方法更优异的解释力。

Conclusion: MoE和ShiftSmooth的结合为TFBS预测提供了一个高效、可推广且具备良好解读性的解决方案，有助于推进基因组学研究。

Abstract: Transcription Factor Binding Site (TFBS) prediction is crucial for
understanding gene regulation and various biological processes. This study
introduces a novel Mixture of Experts (MoE) approach for TFBS prediction,
integrating multiple pre-trained Convolutional Neural Network (CNN) models,
each specializing in different TFBS patterns. We evaluate the performance of
our MoE model against individual expert models on both in-distribution and
out-of-distribution (OOD) datasets, using six randomly selected transcription
factors (TFs) for OOD testing. Our results demonstrate that the MoE model
achieves competitive or superior performance across diverse TF binding sites,
particularly excelling in OOD scenarios. The Analysis of Variance (ANOVA)
statistical test confirms the significance of these performance differences.
Additionally, we introduce ShiftSmooth, a novel attribution mapping technique
that provides more robust model interpretability by considering small shifts in
input sequences. Through comprehensive explainability analysis, we show that
ShiftSmooth offers superior attribution for motif discovery and localization
compared to traditional Vanilla Gradient methods. Our work presents an
efficient, generalizable, and interpretable solution for TFBS prediction,
potentially enabling new discoveries in genome biology and advancing our
understanding of transcriptional regulation.

</details>


### [362] [Toward accurate RUL and SOH estimation using reinforced graph-based PINNs enhanced with dynamic weights](https://arxiv.org/abs/2507.09766)
*Mohamadreza Akbari Pour,Ali Ghasemzadeh,MohamadAli Bijarchi,Mohammad Behshad Shafii*

Main category: cs.LG

TL;DR: 本文提出了一种结合物理监督和时空学习的框架（RGPD），用于提高剩余寿命（RUL）和健康状态（SOH）的估计，其性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 精准预测工业系统中设备的RUL和SOH是预防性维护的关键，亟需一种能结合物理规律和数据驱动的高效学习方法。

Method: 采用一种增强动态权重的基于图网络与强化学习的物理信息神经网络（RGPD），通过图卷积递归网络（GCRN）、图注意卷积（GATConv）、软演员-评论家模块（SAC）、Q学习等多模块协作实现高效时空学习和动态权重分配。

Result: 模型在多种工业基准数据集上的RUL和SOH任务中表现出色，超越了当前的先进模型，具有强鲁棒性和预测精度。

Conclusion: RGPD框架通过动态结合物理约束和数据驱动学习，将是解决复杂工业健康管理问题的有效工具。

Abstract: Accurate estimation of Remaining Useful Life (RUL) and State of Health (SOH)
is essential for Prognostics and Health Management (PHM) across a wide range of
industrial applications. We propose a novel framework -- Reinforced Graph-Based
Physics-Informed Neural Networks Enhanced with Dynamic Weights (RGPD) -- that
combines physics-based supervision with advanced spatio-temporal learning.
Graph Convolutional Recurrent Networks (GCRNs) embed graph-convolutional
filters within recurrent units to capture how node representations evolve over
time. Graph Attention Convolution (GATConv) leverages a self-attention
mechanism to compute learnable, edge-wise attention coefficients, dynamically
weighting neighbor contributions for adaptive spatial aggregation. A Soft
Actor-Critic (SAC) module is positioned between the Temporal Attention Unit
(TAU) and GCRN to further improve the spatio-temporal learning. This module
improves attention and prediction accuracy by dynamically scaling hidden
representations to minimize noise and highlight informative features. To
identify the most relevant physical constraints in each area, Q-learning agents
dynamically assign weights to physics-informed loss terms, improving
generalization across real-time industrial systems and reducing the need for
manual tuning. In both RUL and SOH estimation tasks, the proposed method
consistently outperforms state-of-the-art models, demonstrating strong
robustness and predictive accuracy across varied degradation patterns across
three diverse industrial benchmark datasets.

</details>


### [363] [Knowing When to Quit: Probabilistic Early Exits for Speech Separation](https://arxiv.org/abs/2507.09768)
*Kenny Falkær Olsen. Mads Østergaard,Karl Ulbæk,Søren Føns Nielsen,Rasmus Malik Høegh Lindrup,Bjørn Sand Jensen,Morten Mørup*

Main category: cs.LG

TL;DR: 近年来，基于深度学习的单通道语音分离技术在计算效率和参数效率的推动下取得了显著进展。本研究设计了一种具备早退能力的神经网络架构，并提出了一种基于不确定性感知的框架，以动态调整计算负载并实现高效语音分离。


<details>
  <summary>Details</summary>
Motivation: 目前的深度学习语音分离网络通常在固定计算资源约束下设计，无法满足移动设备等异构平台中不同资源需求的动态调整。

Method: 设计了一种支持早退功能的神经网络架构，同时结合不确定性感知框架模型干净语音信号及误差方差，基于信噪比阈值定义早退条件。

Result: 提出的模型在语音分离和增强任务中表现出可以与多种计算参数预算下的当前最先进模型竞争的能力。

Conclusion: 方法实现了语音分离网络的动态计算缩放，达到了业界先进水平，并具备清晰可解释的退出条件。

Abstract: In recent years, deep learning-based single-channel speech separation has
improved considerably, in large part driven by increasingly compute- and
parameter-efficient neural network architectures. Most such architectures are,
however, designed with a fixed compute and parameter budget, and consequently
cannot scale to varying compute demands or resources, which limits their use in
embedded and heterogeneous devices such as mobile phones and hearables. To
enable such use-cases we design a neural network architecture for speech
separation capable of early-exit, and we propose an uncertainty-aware
probabilistic framework to jointly model the clean speech signal and error
variance which we use to derive probabilistic early-exit conditions in terms of
desired signal-to-noise ratios. We evaluate our methods on both speech
separation and enhancement tasks, and we show that a single early-exit model
can be competitive with state-of-the-art models trained at many compute and
parameter budgets. Our framework enables fine-grained dynamic compute-scaling
of speech separation networks while achieving state-of-the-art performance and
interpretable exit conditions.

</details>


### [364] [Efficient Molecular Conformer Generation with SO(3)-Averaged Flow Matching and Reflow](https://arxiv.org/abs/2507.09785)
*Zhonglin Cao,Mario Geiger,Allan dos Santos Costa,Danny Reidenbach,Karsten Kreis,Tomas Geffner,Franco Pellegrini,Guoqing Zhou,Emine Kucukbenli*

Main category: cs.LG

TL;DR: 本研究提出了快速高效的分子构象生成方法，采用SO(3)-平均流训练目标和基于流的重流与蒸馏方法，实现快速训练和推理，同时保持高质量的生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有的分子构象生成模型（如扩散或流模型）训练和采样计算成本高，亟需更高效的方法。

Method: 引入SO(3)-平均流训练目标加快训练收敛，使用重新流动与蒸馏方法实现快速（甚至一步）生成分子构象。

Result: 实验表明，使用SO(3)-平均流训练的模型能达到最先进的生成质量，且方法显著提升了训练和推理效率。

Conclusion: 本研究提出的技术显著降低了3D分子构象生成的成本，为高效的基于流的生成建模提供了新方向。

Abstract: Fast and accurate generation of molecular conformers is desired for
downstream computational chemistry and drug discovery tasks. Currently,
training and sampling state-of-the-art diffusion or flow-based models for
conformer generation require significant computational resources. In this work,
we build upon flow-matching and propose two mechanisms for accelerating
training and inference of generative models for 3D molecular conformer
generation. For fast training, we introduce the SO(3)-Averaged Flow training
objective, which leads to faster convergence to better generation quality
compared to conditional optimal transport flow or Kabsch-aligned flow. We
demonstrate that models trained using SO(3)-Averaged Flow can reach
state-of-the-art conformer generation quality. For fast inference, we show that
the reflow and distillation methods of flow-based models enable few-steps or
even one-step molecular conformer generation with high quality. The training
techniques proposed in this work show a path towards highly efficient molecular
conformer generation with flow-based models.

</details>


### [365] [Leveraging Distribution Matching to Make Approximate Machine Unlearning Faster](https://arxiv.org/abs/2507.09786)
*Junaid Iqbal Khan*

Main category: cs.LG

TL;DR: 本文提出了一种加速分类导向的近似机器遗忘(AMU)的方法，通过结合数据集浓缩方法和快速收敛的遗忘目标，提高了遗忘效率。


<details>
  <summary>Details</summary>
Motivation: 当前AMU在处理保留数据集时需要高计算成本，同时减少训练轮次也面临挑战，本文旨在解决这些问题。

Method: 提出两种方法：1. Blend，通过将相似图像合并减少保留数据集规模，显著加速；2. Accelerated-AMU (A-AMU)，在遗忘目标中加入陡峭化的主要损失函数和新的可微分正则项，加速遗忘收敛。

Result: 实验表明，这两种方法结合可显著降低单轮和多轮遗忘过程中的延迟，同时保持模型效用和隐私。

Conclusion: 这是首个通过数据集浓缩与专用遗忘损失函数联合设计的遗忘加速研究，为实现高效AMU提供了新思路。

Abstract: Approximate machine unlearning (AMU) enables models to `forget' specific
training data through specialized fine-tuning on a retained dataset subset.
However, processing this retained subset still dominates computational runtime,
while reductions of epochs also remain a challenge. We propose two
complementary methods to accelerate classification-oriented AMU. First,
\textbf{Blend}, a novel distribution-matching dataset condensation (DC), merges
visually similar images with shared blend-weights to significantly reduce the
retained set size. It operates with minimal pre-processing overhead and is
orders of magnitude faster than state-of-the-art DC methods. Second, our
loss-centric method, \textbf{Accelerated-AMU (A-AMU)}, augments the unlearning
objective to quicken convergence. A-AMU achieves this by combining a steepened
primary loss to expedite forgetting with a novel, differentiable regularizer
that matches the loss distributions of forgotten and in-distribution unseen
data. Our extensive experiments demonstrate that this dual approach of data and
loss-centric optimization dramatically reduces end-to-end unlearning latency
across both single and multi-round scenarios, all while preserving model
utility and privacy. To our knowledge, this is the first work to systematically
tackle unlearning efficiency by jointly designing a specialized dataset
condensation technique with a dedicated accelerated loss function. Code is
available at https://github.com/algebraicdianuj/DC_Unlearning.

</details>


### [366] [A Scalable and Efficient Signal Integration System for Job Matching](https://arxiv.org/abs/2507.09797)
*Ping Liu,Rajat Arora,Xiao Shi,Benjamin Le,Qianqi Shen,Jianqiang Shen,Chengming Jiang,Nikita Zhiltsov,Priya Bannur,Yidan Zhu,Liming Dong,Haichao Wei,Qi Guo,Luke Simon,Liangjie Hong,Wenjing Zhang*

Main category: cs.LG

TL;DR: 本研究提出了STAR系统，通过结合大型语言模型(LLMs)和图神经网络(GNNs)，解决LinkedIn在职位匹配推荐系统中遇到的冷启动问题、过滤泡沫和偏差问题。


<details>
  <summary>Details</summary>
Motivation: LinkedIn的职位匹配推荐系统面临冷启动、过滤泡沫和匹配偏差等挑战，需要一种能够有效整合多种信号的解决方案。

Method: 本研究设计了STAR系统，将LLM用于文本数据理解，将GNN用于捕捉复杂关系，结合工业级采样方法和版本管理，实现端到端的大规模推荐系统嵌入开发与部署。

Result: 系统显著提升了候选人和职位匹配的推荐性能，并能够兼顾工业环境中的可扩展性和部署需求。

Conclusion: 通过STAR系统的实现和应用，验证了GNN与LLM的结合在职位推荐场景中的有效性，并为行业系统部署提供了实用性指导。

Abstract: LinkedIn, one of the world's largest platforms for professional networking
and job seeking, encounters various modeling challenges in building
recommendation systems for its job matching product, including cold-start,
filter bubbles, and biases affecting candidate-job matching. To address these,
we developed the STAR (Signal Integration for Talent And Recruiters) system,
leveraging the combined strengths of Large Language Models (LLMs) and Graph
Neural Networks (GNNs). LLMs excel at understanding textual data, such as
member profiles and job postings, while GNNs capture intricate relationships
and mitigate cold-start issues through network effects. STAR integrates diverse
signals by uniting LLM and GNN capabilities with industrial-scale paradigms
including adaptive sampling and version management. It provides an end-to-end
solution for developing and deploying embeddings in large-scale recommender
systems. Our key contributions include a robust methodology for building
embeddings in industrial applications, a scalable GNN-LLM integration for
high-performing recommendations, and practical insights for real-world model
deployment.

</details>


### [367] [Federated Learning with Graph-Based Aggregation for Traffic Forecasting](https://arxiv.org/abs/2507.09805)
*Audri Banik,Glaucio Haroldo Silva de Carvalho,Renata Dividino*

Main category: cs.LG

TL;DR: 本文探讨了交通预测中的联邦学习方法，提出了一种结合图学习的轻量级联邦学习方法，在不牺牲效率的情况下捕获空间关系。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习方法未充分考虑客户端间的空间依赖性，导致交通预测性能受限。如何在保护隐私需求下有效利用这些依赖性成为关键问题。

Method: 结合FedAvg的简单性及图学习的关键思想，引入邻域聚合逻辑对联邦学习的参数更新进行指导，通过图连接权重对客户端模型进行加权，捕捉空间关系的同时保持计算高效。

Result: 方法在METR-LA和PEMS-BAY交通数据集上表现出色，与标准基线及近期基于图的联邦学习技术相比具有竞争力。

Conclusion: 新方法在引入空间关系的同时，减少了计算开销，为交通预测的联邦学习提供了一种高效而有效的解决方案。

Abstract: In traffic prediction, the goal is to estimate traffic speed or flow in
specific regions or road segments using historical data collected by devices
deployed in each area. Each region or road segment can be viewed as an
individual client that measures local traffic flow, making Federated Learning
(FL) a suitable approach for collaboratively training models without sharing
raw data. In centralized FL, a central server collects and aggregates model
updates from multiple clients to build a shared model while preserving each
client's data privacy. Standard FL methods, such as Federated Averaging
(FedAvg), assume that clients are independent, which can limit performance in
traffic prediction tasks where spatial relationships between clients are
important. Federated Graph Learning methods can capture these dependencies
during server-side aggregation, but they often introduce significant
computational overhead. In this paper, we propose a lightweight graph-aware FL
approach that blends the simplicity of FedAvg with key ideas from graph
learning. Rather than training full models, our method applies basic
neighbourhood aggregation principles to guide parameter updates, weighting
client models based on graph connectivity. This approach captures spatial
relationships effectively while remaining computationally efficient. We
evaluate our method on two benchmark traffic datasets, METR-LA and PEMS-BAY,
and show that it achieves competitive performance compared to standard
baselines and recent graph-based federated learning techniques.

</details>


### [368] [Compressed Computation: Dense Circuits in a Toy Model of the Universal-AND Problem](https://arxiv.org/abs/2507.09816)
*Adam Newgas*

Main category: cs.LG

TL;DR: 本文分析了神经网络在受限维度下学习计算效率电路的能力，并研究其在不同参数下的鲁棒性及可扩展性。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络在维度有限的约束下，能否找到计算高效的电路，从而理解模型的电路构建方式及其解释性。

Method: 设计了一个玩具模型解决Universal-AND问题，限制隐层尺寸，分析其学习和生成的计算电路特性，并对结果进行详细解释与计算。

Result: 训练过程发现模型找到的解决方案和理论构造不同，其解决方案充分利用了所有神经元，且随着维度扩展时具备自然扩展性并能适应不同参数调整。

Conclusion: 该研究揭示了当前模型倾向于生成的电路类型及其在超叠加表示中的灵活性，并对神经网络电路结构和可解释性提供更广泛的理解。

Abstract: Neural networks are capable of superposition -- representing more features
than there are dimensions. Recent work considers the analogous concept for
computation instead of storage, proposing theoretical constructions. But there
has been little investigation into whether these circuits can be learned in
practice. In this work, we investigate a toy model for the Universal-AND
problem which computes the AND of all $m\choose 2$ pairs of $m$ sparse inputs.
The hidden dimension that determines the number of non-linear activations is
restricted to pressure the model to find a compute-efficient circuit, called
compressed computation. We find that the training process finds a simple
solution that does not correspond to theoretical constructions. It is fully
dense -- every neuron contributes to every output. The solution circuit
naturally scales with dimension, trading off error rates for neuron efficiency.
It is similarly robust to changes in sparsity and other key parameters, and
extends naturally to other boolean operations and boolean circuits. We explain
the found solution in detail and compute why it is more efficient than the
theoretical constructions at low sparsity. Our findings shed light on the types
of circuits that models like to form and the flexibility of the superposition
representation. This contributes to a broader understanding of network
circuitry and interpretability.

</details>


### [369] [Bridging Neural Networks and Dynamic Time Warping for Adaptive Time Series Classification](https://arxiv.org/abs/2507.09826)
*Jintao Qu,Zichong Wang,Chenhao Wu,Wenbin Zhang*

Main category: cs.LG

TL;DR: 提出了一种结合传统DTW算法与神经网络的新模型，旨在解决冷启动与丰富资源情况下的时间序列分类问题。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络尽管在时间序列分类中表现出色，但在冷启动或数据稀缺下表现受限；而DTW方法在数据少时表现更佳，但无法通过训练优化且在资源丰富时不如神经网络。

Method: 通过动态长度缩短算法将时间序列转换为保留结构模式的原型，并将DTW转化为等价的循环神经网络，从而构建出可训练模型，兼具DTW的对齐特性与解释能力。

Result: 在多项时间序列分类任务中，该模型在资源较少的情况下显著优于现有方法，而在资源充足时仍具有竞争力。

Conclusion: 新模型有效结合了DTW的解释性与神经网络的可训练性，在时间序列分类任务中表现出色，适用于不同数据资源情境。

Abstract: Neural networks have achieved remarkable success in time series
classification, but their reliance on large amounts of labeled data for
training limits their applicability in cold-start scenarios. Moreover, they
lack interpretability, reducing transparency in decision-making. In contrast,
dynamic time warping (DTW) combined with a nearest neighbor classifier is
widely used for its effectiveness in limited-data settings and its inherent
interpretability. However, as a non-parametric method, it is not trainable and
cannot leverage large amounts of labeled data, making it less effective than
neural networks in rich-resource scenarios. In this work, we aim to develop a
versatile model that adapts to cold-start conditions and becomes trainable with
labeled data, while maintaining interpretability. We propose a dynamic
length-shortening algorithm that transforms time series into prototypes while
preserving key structural patterns, thereby enabling the reformulation of the
DTW recurrence relation into an equivalent recurrent neural network. Based on
this, we construct a trainable model that mimics DTW's alignment behavior. As a
neural network, it becomes trainable when sufficient labeled data is available,
while still retaining DTW's inherent interpretability. We apply the model to
several benchmark time series classification tasks and observe that it
significantly outperforms previous approaches in low-resource settings and
remains competitive in rich-resource settings.

</details>


### [370] [Generative Cognitive Diagnosis](https://arxiv.org/abs/2507.09831)
*Jiatong Li,Qi Liu,Mengxiao Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种新的生成诊断范式，将认知诊断从预测模型转变为生成模型，通过生成过程实现无需参数再优化的认知状态归纳推断。


<details>
  <summary>Details</summary>
Motivation: 传统认知诊断模型在新学习者的即时诊断和诊断结果的可靠性上存在局限。因此需要一种能够更高效并可靠执行认知状态推断的方法。

Method: 引入生成诊断范式，设计了两种具体实现：生成性项目反应理论（G-IRT）和生成性神经认知诊断模型（G-NCDM），通过引入可识别性和单调性条件的生成过程实现状态推断和响应预测的解耦。

Result: 实验显示该方法显著提高了扩展性和可靠性，尤其在处理新学习者的诊断时实现了100倍加速效果。

Conclusion: 生成诊断范式为认知诊断特别是在智能模型评估和智能教育系统中的应用提供了新的研究方向。

Abstract: Cognitive diagnosis (CD) models latent cognitive states of human learners by
analyzing their response patterns on diagnostic tests, serving as a crucial
machine learning technique for educational assessment and evaluation.
Traditional cognitive diagnosis models typically follow a transductive
prediction paradigm that optimizes parameters to fit response scores and
extract learner abilities. These approaches face significant limitations as
they cannot perform instant diagnosis for new learners without computationally
expensive retraining and produce diagnostic outputs with limited reliability.
In this study, we introduces a novel generative diagnosis paradigm that
fundamentally shifts CD from predictive to generative modeling, enabling
inductive inference of cognitive states without parameter re-optimization. We
propose two simple yet effective instantiations of this paradigm: Generative
Item Response Theory (G-IRT) and Generative Neural Cognitive Diagnosis Model
(G-NCDM), which achieve excellent performance improvements over traditional
methods. The generative approach disentangles cognitive state inference from
response prediction through a well-designed generation process that
incorporates identifiability and monotonicity conditions. Extensive experiments
on real-world datasets demonstrate the effectiveness of our methodology in
addressing scalability and reliability challenges, especially $\times 100$
speedup for the diagnosis of new learners. Our framework opens new avenues for
cognitive diagnosis applications in artificial intelligence, particularly for
intelligent model evaluation and intelligent education systems. The code is
available at https://github.com/CSLiJT/Generative-CD.git.

</details>


### [371] [A Pre-training Framework for Relational Data with Information-theoretic Principles](https://arxiv.org/abs/2507.09837)
*Quang Truong,Zhikai Chen,Mingxuan Ju,Tong Zhao,Neil Shah,Jiliang Tang*

Main category: cs.LG

TL;DR: 提出了一种名为任务向量估计（TVE）的预训练框架，用于有效从关系型数据库中学习，同时考虑到任务的异质性及时间依赖性。


<details>
  <summary>Details</summary>
Motivation: 针对关系型数据库中任务异质性（如关系模式图、时间依赖性及SQL定义的标签逻辑）给预训练策略设计带来的挑战，提出新的方法以改善下游任务表现。

Method: TVE通过基于集合的图遍历方法生成预测监督信号，显式建模关系动态，并通过信息论形式化其方法，确保任务相关信号更精准。

Result: 在RelBench基准上，TVE持续优于传统的预训练基线方法。

Conclusion: 编码任务异质性和时间结构的预训练目标是提高关系型数据库预测建模性能的设计原则。

Abstract: Relational databases underpin critical infrastructure across a wide range of
domains, yet the design of generalizable pre-training strategies for learning
from relational databases remains an open challenge due to task heterogeneity.
Specifically, there exist infinitely many possible downstream tasks, as tasks
are defined based on relational schema graphs, temporal dependencies, and
SQL-defined label logics. An effective pre-training framework is desired to
take these factors into account in order to obtain task-aware representations.
By incorporating knowledge of the underlying distribution that drives label
generation, downstream tasks can benefit from relevant side-channel
information. To bridge this gap, we introduce Task Vector Estimation (TVE), a
novel pre-training framework that constructs predictive supervisory signals via
set-based aggregation over schema traversal graphs, explicitly modeling
next-window relational dynamics. We formalize our approach through an
information-theoretic lens, demonstrating that task-informed representations
retain more relevant signals than those obtained without task priors. Extensive
experiments on the RelBench benchmark show that TVE consistently outperforms
traditional pre-training baselines. Our findings advocate for pre-training
objectives that encode task heterogeneity and temporal structure as design
principles for predictive modeling on relational databases.

</details>


### [372] [Rethinking Prompt Optimization: Reinforcement, Diversification, and Migration in Blackbox LLMs](https://arxiv.org/abs/2507.09839)
*MohammadReza Davari,Utkarsh Garg,Weixin Cai,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 本文提出了一种新的自动化提示优化（APO）框架，通过改进反馈机制，用于促使大语言模型（LLMs）生成更优输出。此外，本文还引入了一种新的方法，解决提示迁移过程中的性能退化问题。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型通常通过黑盒API交互，提示工程成为控制模型输出的关键。然而，现有的自动提示优化方法多集中于错误修正，忽视了成功预测中蕴含的重要信息。

Method: 研究重新定义文本梯度为负强化信号，并引入正强化信号以保留来自成功预测的有益信息。同时，使用反馈多样化技术来聚合多重反馈信号，从而增强一致性并过滤噪声。此外，正式提出持续提示优化（CPO），以高效迁移优化后的提示到不同模型版本或API供应商。

Result: 与传统方法相比，该方法在标准和提示迁移场景下均取得了显著的准确率提升、更快的收敛速度和更低的计算成本。

Conclusion: 通过改进反馈机制和引入提示迁移方法，该研究显著提升了提示优化的效率和效果，为大语言模型的应用提供了实际支持。

Abstract: An increasing number of NLP applications interact with large language models
(LLMs) through black-box APIs, making prompt engineering critical for
controlling model outputs. While recent Automatic Prompt Optimization (APO)
methods iteratively refine prompts using model-generated feedback, textual
gradients, they primarily focus on error correction and neglect valuable
insights from correct predictions. This limits both their effectiveness and
efficiency. In this paper, we propose a novel APO framework centered on
enhancing the feedback mechanism. We reinterpret the textual gradient as a form
of negative reinforcement and introduce the complementary positive
reinforcement to explicitly preserve beneficial prompt components identified
through successful predictions. To mitigate the noise inherent in LLM-generated
feedback, we introduce a technique called feedback diversification, which
aggregates multiple feedback signals, emphasizing consistent, actionable advice
while filtering out outliers. Motivated by the rapid evolution and diversity of
available LLMs, we also formalize Continual Prompt Optimization (CPO),
addressing the practical challenge of efficiently migrating optimized prompts
between different model versions or API providers. Our experiments reveal that
naive prompt migration often degrades performance due to loss of critical
instructions. In contrast, our approach consistently outperforms strong
baselines, achieving significant accuracy improvements, faster convergence, and
lower computational costs in both standard and migration scenarios.

</details>


### [373] [Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training](https://arxiv.org/abs/2507.09846)
*Minhak Song,Beomhan Baek,Kwangjun Ahn,Chulhee Yun*

Main category: cs.LG

TL;DR: 论文提出了一种无需学习率调度的优化方法，并对其理论性和实用性进行了详细分析。


<details>
  <summary>Details</summary>
Motivation: 现有的训练策略（如余弦学习率调度、WSD等）在针对大型模型和数据集时存在诸多限制，希望找到更可扩展和高效的方法。

Method: 重新审视了Schedule-Free (SF) 方法，在理论和实证中分析了SF 的优化动态，并提出了改进的SF变体，以提升其在大批量训练中的稳定性和性能。

Result: SF方法能够在不需要额外内存的情况下，隐式地实现权重平均，同时在连续扩展的工作负载中表现出色。改进的SF变体还更为鲁棒，更适合大批量训练。

Conclusion: SF方法是一种实际、可扩展和理论上有依据的语言模型训练方法，能够有效解决当前的训练瓶颈。

Abstract: As both model and dataset sizes continue to scale rapidly, conventional
pretraining strategies with fixed compute budgets-such as cosine learning rate
schedules-are increasingly inadequate for large-scale training. Recent
alternatives, including warmup-stable-decay (WSD) schedules and weight
averaging, offer greater flexibility. However, WSD relies on explicit decay
phases to track progress, while weight averaging addresses this limitation at
the cost of additional memory. In search of a more principled and scalable
alternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024],
which has shown strong empirical performance across diverse settings. We show
that SF-AdamW effectively navigates the "river" structure of the loss landscape
without decay phases or auxiliary averaging, making it particularly suitable
for continuously scaling training workloads. To understand this behavior, we
conduct a theoretical and empirical analysis of SF dynamics, revealing that it
implicitly performs weight averaging without memory overhead. Guided by this
analysis, we propose a refined variant of SF that improves robustness to
momentum and performs better under large batch sizes, addressing key
limitations of the original method. Together, these results establish SF as a
practical, scalable, and theoretically grounded approach for language model
training.

</details>


### [374] [Task Priors: Enhancing Model Evaluation by Considering the Entire Space of Downstream Tasks](https://arxiv.org/abs/2507.09871)
*Niket Patel,Randall Balestriero*

Main category: cs.LG

TL;DR: 本文提出了一种新的评估框架，通过定义任务先验和任务分布，解决了当前AI研究中固定评估协议的限制。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究中的评估方法通常依赖于固定且手工选择的下游基准集合，这种僵化的评估方法阻碍了AI研究的发展。

Method: 提出一种基于任务分布和任务先验的概率性下游任务空间，评估模型在所有可能下游任务上的平均表现及表现方差。

Result: 框架首次回答了模型在所有可能下游任务上的平均表现和表现方差等关键问题。

Conclusion: 通过建立新的评估标准，该框架有望加速在自监督学习领域的研究进展。

Abstract: The grand goal of AI research, and particularly Self Supervised Learning
(SSL), is to produce systems that can successfully solve any possible task. In
contrast, current evaluation methods available to AI researchers typically rely
on a fixed collection of hand-picked downstream benchmarks. Hence, a large
amount of effort is put into designing and searching for large collection of
evaluation tasks that can serve as a proxy of our grand goal. We argue that
such a rigid evaluation protocol creates a silent bottleneck in AI research. To
remedy that, we define a probabilistic space of downstream tasks obtained by
adopting a distribution of tasks and by defining Task Priors. Under this view,
one can evaluate a model's performance over the set of all possible downstream
tasks. Our framework is the first to provide answers to key questions such as
(i) what is the average performance of my model over all possible downstream
tasks weighted by the probability to encounter each task? or (ii) what is the
variance of my model's performance across all downstream tasks under the
defined Task Priors? Beyond establishing a new standard for evaluation, we
believe that Task Priors will accelerate the pace of research in SSL - where
downstream task evaluation is the sole qualitative signal that researchers have
access to.

</details>


### [375] [AdaBrain-Bench: Benchmarking Brain Foundation Models for Brain-Computer Interface Applications](https://arxiv.org/abs/2507.09882)
*Jiamin Wu,Zichen Ren,Junyu Wang,Pengyu Zhu,Yonghao Song,Mianxin Liu,Qihao Zheng,Lei Bai,Wanli Ouyang,Chunfeng Song*

Main category: cs.LG

TL;DR: 提出AdaBrain-Bench，一个新型的大规模标准化基准，用于系统评估脑基础模型在多种非侵入性脑机接口任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 目前非侵入式脑机接口因其安全性和广泛的应用场景而备受关注，但现有信号的高噪音水平和任务特定数据有限性限制了解码能力，而当前缺乏全面、实用和可扩展的基准来促进对脑基础模型的评估和采用。

Method: 提出了AdaBrain-Bench，这是一套大型标准化的基准测试工具，集合了7个关键领域的脑机解码数据集，并引入了流线型的任务适应管道、多维度评估指标以及工具集，以系统评估脑基础模型的泛化能力。

Result: 通过AdaBrain-Bench评估了多种可公开获取的脑基础模型，并提供了选择模型的实践指导，基准测试平台同时支持可重复性研究和外部使用，为解决鲁棒和通用神经解码提供了一个不断发展的平台。

Conclusion: AdaBrain-Bench是一个有潜力推动非侵入性脑机接口研究进展的重要工具，能为更广泛的任务适配和模型评估提供支持。

Abstract: Non-invasive Brain-Computer Interfaces (BCI) offer a safe and accessible
means of connecting the human brain to external devices, with broad
applications in home and clinical settings to enhance human capabilities.
However, the high noise level and limited task-specific data in non-invasive
signals constrain decoding capabilities. Recently, the adoption of
self-supervised pre-training is transforming the landscape of non-invasive BCI
research, enabling the development of brain foundation models to capture
generic neural representations from large-scale unlabeled
electroencephalography (EEG) signals with substantial noises. However, despite
these advances, the field currently lacks comprehensive, practical and
extensible benchmarks to assess the utility of the public foundation models
across diverse BCI tasks, hindering their widespread adoption. To address this
challenge, we present AdaBrain-Bench, a large-scale standardized benchmark to
systematically evaluate brain foundation models in widespread non-invasive BCI
tasks. AdaBrain-Bench encompasses a diverse collection of representative BCI
decoding datasets spanning 7 key applications. It introduces a streamlined task
adaptation pipeline integrated with multi-dimensional evaluation metrics and a
set of adaptation tools. The benchmark delivers an inclusive framework for
assessing generalizability of brain foundation models across key transfer
settings, including cross-subject, multi-subject, and few-shot scenarios. We
leverage AdaBrain-Bench to evaluate a suite of publicly available brain
foundation models and offer insights into practices for selecting appropriate
models in various scenarios. We make our benchmark pipeline available to enable
reproducible research and external use, offering a continuously evolving
platform to foster progress toward robust and generalized neural decoding
solutions.

</details>


### [376] [TolerantECG: A Foundation Model for Imperfect Electrocardiogram](https://arxiv.org/abs/2507.09887)
*Huynh Nguyen Dang,Thang Pham,Ngan Le,Van Nguyen*

Main category: cs.LG

TL;DR: TolerantECG是一种面向ECG信号的基础模型，它能够在存在噪声或缺失导联的情况下进行有效诊断。


<details>
  <summary>Details</summary>
Motivation: 解决传统12导联ECG记录因噪声或导联缺失导致诊断不准确的问题。

Method: 提出了一种结合对比学习和自监督学习框架的模型，能够学习ECG信号的表示，并与文本报告描述以及损坏或导联缺失的信号相关联。

Result: TolerantECG在PTB-XL数据集的各种ECG信号条件和类别级别上表现最佳或次优，并在MIT-BIH心律失常数据库中达到最高性能。

Conclusion: TolerantECG在应对心电图噪声和导联缺失方面非常鲁棒，已证明其在ECG信号理解和诊断中的有效性。

Abstract: The electrocardiogram (ECG) is an essential and effective tool for diagnosing
heart diseases. However, its effectiveness can be compromised by noise or
unavailability of one or more leads of the standard 12-lead recordings,
resulting in diagnostic errors or uncertainty. To address these challenges, we
propose TolerantECG, a foundation model for ECG signals that is robust to noise
and capable of functioning with arbitrary subsets of the standard 12-lead ECG.
TolerantECG training combines contrastive and self-supervised learning
frameworks to jointly learn ECG signal representations alongside their
corresponding knowledge-retrieval-based text report descriptions and corrupted
or lead-missing signals. Comprehensive benchmarking results demonstrate that
TolerantECG consistently ranks as the best or second-best performer across
various ECG signal conditions and class levels in the PTB-XL dataset, and
achieves the highest performance on the MIT-BIH Arrhythmia Database.

</details>


### [377] [NeuTSFlow: Modeling Continuous Functions Behind Time Series Forecasting](https://arxiv.org/abs/2507.09888)
*Huibo Xu,Likang Wu,Xianquan Wang,Haoning Dang,Chun-Wun Cheng,Angelica I Aviles-Rivero,Qi Liu*

Main category: cs.LG

TL;DR: 本文提出了一种名为NeuTSFlow的新框架，利用神经算子来实现历史和未来函数族之间测度路径的流匹配，提升时间序列预测的准确性和稳健性。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列方法将数据视为离散序列，忽略了其作为连续过程噪声样本的本质，而这种限制导致无法准确描述函数间的关系。

Method: 采用基于神经算子的流匹配方法，参数化流在无限维度函数空间中的速度场，直接建模函数层级的特征，而非仅研究离散点的依赖关系。

Result: 实验证明，NeuTSFlow在多种预测任务中表现出优越的准确性和稳健性，验证了该框架基于函数族视角的有效性。

Conclusion: NeuTSFlow通过更精细建模函数族空间中的关系，提供了一种更可靠、更准确的时间序列预测方法。

Abstract: Time series forecasting is a fundamental task with broad applications, yet
conventional methods often treat data as discrete sequences, overlooking their
origin as noisy samples of continuous processes. Crucially, discrete noisy
observations cannot uniquely determine a continuous function; instead, they
correspond to a family of plausible functions. Mathematically, time series can
be viewed as noisy observations of a continuous function family governed by a
shared probability measure. Thus, the forecasting task can be framed as
learning the transition from the historical function family to the future
function family. This reframing introduces two key challenges: (1) How can we
leverage discrete historical and future observations to learn the relationships
between their underlying continuous functions? (2) How can we model the
transition path in function space from the historical function family to the
future function family? To address these challenges, we propose NeuTSFlow, a
novel framework that leverages Neural Operators to facilitate flow matching for
learning path of measure between historical and future function families. By
parameterizing the velocity field of the flow in infinite-dimensional function
spaces, NeuTSFlow moves beyond traditional methods that focus on dependencies
at discrete points, directly modeling function-level features instead.
Experiments on diverse forecasting tasks demonstrate NeuTSFlow's superior
accuracy and robustness, validating the effectiveness of the function-family
perspective.

</details>


### [378] [Soft Graph Clustering for single-cell RNA Sequencing Data](https://arxiv.org/abs/2507.09890)
*Ping Xu,Pengfei Wang,Zhiyuan Ning,Meng Xiao,Min Wu,Yuanchun Zhou*

Main category: cs.LG

TL;DR: 提出了scSGC，改进了单细胞RNA测序数据的聚类方法，从而准确表征细胞间连续相似性，克服了传统基于图方法的信息丢失和误差传播问题。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络（GNN）方法通过硬图构建描述细胞间相似性，存在信息丢失和错误消息传播等问题，亟需新的方法表示连续相似性并提升聚类性能。

Method: 开发了scSGC方法，包括一个基于零膨胀负二项分布（ZINB）的特征自编码器、一个双通道软图嵌入模块及基于最优运输的聚类优化模块。

Result: 通过在10个数据集上的实验，scSGC在聚类准确性、细胞类型注释和计算效率方面优于13种最新的聚类模型。

Conclusion: scSGC在单细胞RNA测序数据分析中具有显著潜力，可深化对细胞异质性和多样性的理解。

Abstract: Clustering analysis is fundamental in single-cell RNA sequencing (scRNA-seq)
data analysis for elucidating cellular heterogeneity and diversity. Recent
graph-based scRNA-seq clustering methods, particularly graph neural networks
(GNNs), have significantly improved in tackling the challenges of
high-dimension, high-sparsity, and frequent dropout events that lead to
ambiguous cell population boundaries. However, their reliance on hard graph
constructions derived from thresholded similarity matrices presents
challenges:(i) The simplification of intercellular relationships into binary
edges (0 or 1) by applying thresholds, which restricts the capture of
continuous similarity features among cells and leads to significant information
loss.(ii) The presence of significant inter-cluster connections within hard
graphs, which can confuse GNN methods that rely heavily on graph structures,
potentially causing erroneous message propagation and biased clustering
outcomes. To tackle these challenges, we introduce scSGC, a Soft Graph
Clustering for single-cell RNA sequencing data, which aims to more accurately
characterize continuous similarities among cells through non-binary edge
weights, thereby mitigating the limitations of rigid data structures. The scSGC
framework comprises three core components: (i) a zero-inflated negative
binomial (ZINB)-based feature autoencoder; (ii) a dual-channel cut-informed
soft graph embedding module; and (iii) an optimal transport-based clustering
optimization module. Extensive experiments across ten datasets demonstrate that
scSGC outperforms 13 state-of-the-art clustering models in clustering accuracy,
cell type annotation, and computational efficiency. These results highlight its
substantial potential to advance scRNA-seq data analysis and deepen our
understanding of cellular heterogeneity.

</details>


### [379] [Algorithm Development in Neural Networks: Insights from the Streaming Parity Task](https://arxiv.org/abs/2507.09897)
*Loek van Rossem,Andrew M. Saxe*

Main category: cs.LG

TL;DR: 本文研究了循环神经网络（RNNs）在流式奇偶任务中的学习动力学，揭示了神经网络可以通过有限训练经验实现无限泛化的现象及其机制。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络在有限训练数据下能够实现无限泛化的机制，尤其是在超参数化下仍具有卓越的泛化能力。

Method: 通过对循环神经网络（RNNs）在流式奇偶任务中的学习动力学进行研究，提出了一种表示动力学的有效理论，并分析了其隐含的表示合并效应。

Result: 证明了在足够的有限训练经验下，RNNs 会经历一个相变过程，最终达到完美的无限泛化。

Conclusion: 神经网络可以通过表示合并效应构建出一种有限自动机，从而实现从有限训练经验出发的无限泛化能力。

Abstract: Even when massively overparameterized, deep neural networks show a remarkable
ability to generalize. Research on this phenomenon has focused on
generalization within distribution, via smooth interpolation. Yet in some
settings neural networks also learn to extrapolate to data far beyond the
bounds of the original training set, sometimes even allowing for infinite
generalization, implying that an algorithm capable of solving the task has been
learned. Here we undertake a case study of the learning dynamics of recurrent
neural networks (RNNs) trained on the streaming parity task in order to develop
an effective theory of algorithm development. The streaming parity task is a
simple but nonlinear task defined on sequences up to arbitrary length. We show
that, with sufficient finite training experience, RNNs exhibit a phase
transition to perfect infinite generalization. Using an effective theory for
the representational dynamics, we find an implicit representational merger
effect which can be interpreted as the construction of a finite automaton that
reproduces the task. Overall, our results disclose one mechanism by which
neural networks can generalize infinitely from finite training experience.

</details>


### [380] [Extracting Cause-Effect Pairs from a Sentence with a Dependency-Aware Transformer Model](https://arxiv.org/abs/2507.09925)
*Md Ahsanul Kabir,Abrar Jahin,Mohammad Al Hasan*

Main category: cs.LG

TL;DR: 该研究提出了一种结合依存树的基于BERT的模型DepBERT，在因果短语抽取任务中表现优于多种现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有监督学习方法在因果短语抽取任务中未充分利用依存树等语言学工具，而依存树已被证明对语义配对提取非常有效。

Method: 提出了DepBERT模型，将依存树整合到基于transformer的模型框架中，以增强因果短语抽取能力。

Result: 通过在三个数据集上的广泛实验验证，DepBERT模型优于现有的多种因果性抽取方法。

Conclusion: DepBERT模型展示了将语言学工具与深度学习模型相结合，在自然语言处理特定任务中提升性能的潜力。

Abstract: Extracting cause and effect phrases from a sentence is an important NLP task,
with numerous applications in various domains, including legal, medical,
education, and scientific research. There are many unsupervised and supervised
methods proposed for solving this task. Among these, unsupervised methods
utilize various linguistic tools, including syntactic patterns, dependency
tree, dependency relations, etc. among different sentential units for
extracting the cause and effect phrases. On the other hand, the contemporary
supervised methods use various deep learning based mask language models
equipped with a token classification layer for extracting cause and effect
phrases. Linguistic tools, specifically, dependency tree, which organizes a
sentence into different semantic units have been shown to be very effective for
extracting semantic pairs from a sentence, but existing supervised methods do
not have any provision for utilizing such tools within their model framework.
In this work, we propose DepBERT, which extends a transformer-based model by
incorporating dependency tree of a sentence within the model framework.
Extensive experiments over three datasets show that DepBERT is better than
various state-of-the art supervised causality extraction methods.

</details>


### [381] [Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications](https://arxiv.org/abs/2507.09931)
*Yoon Pyo Lee*

Main category: cs.LG

TL;DR: 本文提出了一种新方法，通过研究神经元激活模式并使用神经元静默技术，解析LLM如何编码和利用核工程知识，提升了模型透明度。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域（如核工业）中需要理解LLM的推理过程，从而满足严格的核监管要求。

Method: 采用参数高效的低秩调整技术，将通用LLM调整至核领域，并分析对特定任务表现影响明显的神经元，使用静默技术探查其因果关系。

Result: 发现对某些特定神经元的静默会整体显著降低模型在核领域任务的表现，且影响了生成详细且背景准确的技术信息的能力。

Conclusion: 该方法揭示了LLM中与领域相关知识直接相关的神经回路，有助于实现核级人工智能保障，解决核监管框架中的验证与验证挑战。

Abstract: The integration of Large Language Models (LLMs) into safety-critical domains,
such as nuclear engineering, necessitates a deep understanding of their
internal reasoning processes. This paper presents a novel methodology for
interpreting how an LLM encodes and utilizes domain-specific knowledge, using a
Boiling Water Reactor system as a case study. We adapted a general-purpose LLM
(Gemma-3-1b-it) to the nuclear domain using a parameter-efficient fine-tuning
technique known as Low-Rank Adaptation. By comparing the neuron activation
patterns of the base model to those of the fine-tuned model, we identified a
sparse set of neurons whose behavior was significantly altered during the
adaptation process. To probe the causal role of these specialized neurons, we
employed a neuron silencing technique. Our results demonstrate that while
silencing most of these specialized neurons individually did not produce a
statistically significant effect, deactivating the entire group collectively
led to a statistically significant degradation in task performance. Qualitative
analysis further revealed that silencing these neurons impaired the model's
ability to generate detailed, contextually accurate technical information. This
paper provides a concrete methodology for enhancing the transparency of an
opaque black-box model, allowing domain expertise to be traced to verifiable
neural circuits. This offers a pathway towards achieving nuclear-grade
artificial intelligence (AI) assurance, addressing the verification and
validation challenges mandated by nuclear regulatory frameworks (e.g., 10 CFR
50 Appendix B), which have limited AI deployment in safety-critical nuclear
operations.

</details>


### [382] [Memorization Sinks: Isolating Memorization during LLM Training](https://arxiv.org/abs/2507.09937)
*Gaurav R. Ghosal,Pratyush Maini,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 大语言模型（LLMs）容易记忆重复序列，带来隐私和版权问题。本文提出了一种新方法MemSinks，通过设计让记忆与普通语言能力分离，以便更容易删除记忆内容，同时保持语言模型的通用性能。


<details>
  <summary>Details</summary>
Motivation: 减少大语言模型对于隐私和版权内容的记忆风险，同时避免损害其普通语言能力。

Method: 引入MemSinks框架，通过使用序列标识符激活独特的记忆神经元集合，使记忆隔离成为可能，并通过学习和遗忘的动态分析，证明其有效性。

Result: 在十亿参数和十亿数据规模上，实验表明MemSinks能够有效实现记忆隔离，同时保持强语言泛化能力。

Conclusion: MemSinks方法首次在实际数据上实现了记忆隔离与通用语言能力的同时兼容，为解决隐私和版权问题提供了新方向，其代码也已开源供研究者使用。

Abstract: Large language models are susceptible to memorizing repeated sequences,
posing privacy and copyright concerns. A popular mitigation strategy is to
remove memorized information from specific neurons post-hoc. However, such
approaches have shown limited success so far. In a controlled setting, we show
that the memorization of natural sequences (those that resemble linguistically
plausible text) become mechanistically entangled with general language
abilities, thereby becoming challenging to remove post-hoc. In this work, we
put forward a new paradigm of MemSinks that promotes isolation of memorization
by design. We leverage a sequence identifier that activates a unique set of
memorization neurons for each sequence across repetitions. By analyzing the
dynamics of learning and forgetting, we argue that MemSinks facilitates
isolation of memorized content, making it easier to remove without compromising
general language capabilities. We implement MemSinks at the billion-parameter
and billion-token scale, and observe both effective isolation and strong
generalization. To our knowledge, this is the first proof-of-concept on real
data demonstrating that simultaneous generalization and isolation is
achievable. We open-source our code at http://github.com/grghosal/MemSinks.

</details>


### [383] [Long-Tailed Data Classification by Increasing and Decreasing Neurons During Training](https://arxiv.org/abs/2507.09940)
*Taigo Sakai,Kazuhiro Hotta*

Main category: cs.LG

TL;DR: 该研究提出一种动态调整神经元数量的方法，以应对类不平衡数据的挑战，从而提升少数类的识别精度。


<details>
  <summary>Details</summary>
Motivation: 受生物学启发，研究者希望通过动态调整神经元数量来增强神经网络在类不平衡数据上的表现。

Method: 提出在训练过程中动态添加和移除神经元的方法，从而在不改变最终网络规模的情况下提高表现，尤其是针对少数类数据。

Result: 通过在三个数据集和五种模型上的实验验证，该方法优于固定神经元网络，并与其他不平衡处理技术结合时效果更佳。

Conclusion: 生物学启发的动态网络设计能够有效改善类不平衡数据上的性能，具有实际部署的潜力。

Abstract: In conventional deep learning, the number of neurons typically remains fixed
during training. However, insights from biology suggest that the human
hippocampus undergoes continuous neuron generation and pruning of neurons over
the course of learning, implying that a flexible allocation of capacity can
contribute to enhance performance. Real-world datasets often exhibit class
imbalance situations where certain classes have far fewer samples than others,
leading to significantly reduce recognition accuracy for minority classes when
relying on fixed size networks.To address the challenge, we propose a method
that periodically adds and removes neurons during training, thereby boosting
representational power for minority classes. By retaining critical features
learned from majority classes while selectively increasing neurons for
underrepresented classes, our approach dynamically adjusts capacity during
training. Importantly, while the number of neurons changes throughout training,
the final network size and structure remain unchanged, ensuring efficiency and
compatibility with deployment.Furthermore, by experiments on three different
datasets and five representative models, we demonstrate that the proposed
method outperforms fixed size networks and shows even greater accuracy when
combined with other imbalance-handling techniques. Our results underscore the
effectiveness of dynamic, biologically inspired network designs in improving
performance on class-imbalanced data.

</details>


### [384] [Iceberg: Enhancing HLS Modeling with Synthetic Data](https://arxiv.org/abs/2507.09948)
*Zijian Ding,Tung Nguyen,Weikai Li,Aditya Grover,Yizhou Sun,Jason Cong*

Main category: cs.LG

TL;DR: 本文提出Iceberg，利用生成预训练缺乏样本问题的综合数据增强方法，并显著提升HLS预测模型的泛化性能表现。


<details>
  <summary>Details</summary>
Motivation: 高层次综合（HLS）模型难以在硬件设计预测中实现良好的泛化能力，需探索如何通过预训练和数据增强提升这一能力。

Method: 提出Iceberg方法，结合语言模型生成合成数据和弱标签，使用一种上下文模型架构进行元学习。

Result: 在针对实际应用时，几何平均预测准确提升达86.4%，在两个测试数据集上分别实现2.47倍和1.12倍的离线DSE性能改进。

Conclusion: Iceberg方法通过在合成数据和弱标签的扩展上提升了模型预测的泛化性，提供了一个有效的解决方案。

Abstract: Deep learning-based prediction models for High-Level Synthesis (HLS) of
hardware designs often struggle to generalize. In this paper, we study how to
close the generalizability gap of these models through pretraining on synthetic
data and introduce Iceberg, a synthetic data augmentation approach that expands
both large language model (LLM)-generated programs and weak labels of unseen
design configurations. Our weak label generation method is integrated with an
in-context model architecture, enabling meta-learning from actual and proximate
labels. Iceberg improves the geometric mean modeling accuracy by $86.4\%$ when
adapt to six real-world applications with few-shot examples and achieves a
$2.47\times$ and a $1.12\times$ better offline DSE performance when adapting to
two different test datasets. Our open-sourced code is here:
\href{https://github.com/UCLA-VAST/iceberg}{https://github.com/UCLA-VAST/iceberg}

</details>


### [385] [Hierarchical Job Classification with Similarity Graph Integration](https://arxiv.org/abs/2507.09949)
*Md Ahsanul Kabir,Kareem Abdelfatah,Mohammed Korayem,Mohammad Al Hasan*

Main category: cs.LG

TL;DR: 这篇论文讨论了在线招聘领域的工作分类问题，提出了一种新的表示学习和分类模型，通过嵌入层次行业类别来提升分类精度。


<details>
  <summary>Details</summary>
Motivation: 随着工作市场的复杂性增加，传统文本分类方法无法充分利用行业类别的层次结构，迫切需要新模型来处理这一复杂性。

Method: 提出基于标准职业分类系统（SOC）和内部分类体系Carotene的表示学习方法，将工作和层次行业类别嵌入到共享潜在空间中。

Result: 模型在大规模工作招聘数据集上的实验显示其在利用层次结构与语义特征上显著优于现有方法。

Conclusion: 该研究为提高工作分类精确度提供了一个强大的框架，有助于招聘行业的决策优化。

Abstract: In the dynamic realm of online recruitment, accurate job classification is
paramount for optimizing job recommendation systems, search rankings, and labor
market analyses. As job markets evolve, the increasing complexity of job titles
and descriptions necessitates sophisticated models that can effectively
leverage intricate relationships within job data. Traditional text
classification methods often fall short, particularly due to their inability to
fully utilize the hierarchical nature of industry categories. To address these
limitations, we propose a novel representation learning and classification
model that embeds jobs and hierarchical industry categories into a latent
embedding space. Our model integrates the Standard Occupational Classification
(SOC) system and an in-house hierarchical taxonomy, Carotene, to capture both
graph and hierarchical relationships, thereby improving classification
accuracy. By embedding hierarchical industry categories into a shared latent
space, we tackle cold start issues and enhance the dynamic matching of
candidates to job opportunities. Extensive experimentation on a large-scale
dataset of job postings demonstrates the model's superior ability to leverage
hierarchical structures and rich semantic features, significantly outperforming
existing methods. This research provides a robust framework for improving job
classification accuracy, supporting more informed decision-making in the
recruitment industry.

</details>


### [386] [Radial Neighborhood Smoothing Recommender System](https://arxiv.org/abs/2507.09952)
*Zerui Zhang,Yumou Qiu*

Main category: cs.LG

TL;DR: 研究提出了一种用于推荐系统的新方法，通过改进潜在空间的距离估计来提升推荐性能，同时还能有效缓解冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统在捕获用户-用户、物品-物品及用户-物品关系时，难以定义和测量潜在空间中的距离。

Method: 提出了一种新的距离估计方法，将观测矩阵中行列间的距离映射到潜在空间，并利用经验方差校正噪声引起的非中心性，构造了径向邻域估计器（RNE），结合局部核回归来优化推荐精度。

Result: 理论分析和实验证明，相较其他协同过滤及矩阵分解方法，RNE在模拟与真实数据集上表现优越。

Conclusion: 新的距离估计方法及RNE模型有效改进了推荐系统的性能，并在缓解冷启动问题上同样有效。

Abstract: Recommender systems inherently exhibit a low-rank structure in latent space.
A key challenge is to define meaningful and measurable distances in the latent
space to capture user-user, item-item, user-item relationships effectively. In
this work, we establish that distances in the latent space can be
systematically approximated using row-wise and column-wise distances in the
observed matrix, providing a novel perspective on distance estimation. To
refine the distance estimation, we introduce the correction based on empirical
variance estimator to account for noise-induced non-centrality. The novel
distance estimation enables a more structured approach to constructing
neighborhoods, leading to the Radial Neighborhood Estimator (RNE), which
constructs neighborhoods by including both overlapped and partially overlapped
user-item pairs and employs neighborhood smoothing via localized kernel
regression to improve imputation accuracy. We provide the theoretical
asymptotic analysis for the proposed estimator. We perform evaluations on both
simulated and real-world datasets, demonstrating that RNE achieves superior
performance compared to existing collaborative filtering and matrix
factorization methods. While our primary focus is on distance estimation in
latent space, we find that RNE also mitigates the ``cold-start'' problem.

</details>


### [387] [Rethinking Inductive Bias in Geographically Neural Network Weighted Regression](https://arxiv.org/abs/2507.09958)
*Zhenyuan Chen*

Main category: cs.LG

TL;DR: 这篇论文分析了空间回归模型中的归纳偏差，提出了通过结合CNN、RNN和Transformer技术改进GNNWR的方式，并展示了方法对非线性和复杂空间关系的表现改进。


<details>
  <summary>Details</summary>
Motivation: 研究者发现现有的空间回归模型，尤其是GNNWR，存在对非平稳空间数据建模时的局限性，希望通过改进归纳偏差来提高模型的性能和适应性。

Method: 提出了一种改进的GNNWR方法，将卷积神经网络(CNN)、循环神经网络(RNN)和Transformer的概念引入到空间回归模型中，利用局部感受野、序列上下文和自注意力机制增强模型能力。

Result: 实验表明，改进后的GNNWR在捕获非线性和复杂空间关系方面优于传统方法，同时结果展示了模型性能依赖于数据特性，如局部模型在高度异质或小样本情况下表现较好，而全局模型则适合大规模、均质数据。

Conclusion: 本文强调了归纳偏差对空间建模的重要性，并提出未来研究方向，包括可学习的空间加权函数、混合神经架构及提升模型可解释性的方法。

Abstract: Inductive bias is a key factor in spatial regression models, determining how
well a model can learn from limited data and capture spatial patterns. This
work revisits the inductive biases in Geographically Neural Network Weighted
Regression (GNNWR) and identifies limitations in current approaches for
modeling spatial non-stationarity. While GNNWR extends traditional
Geographically Weighted Regression by using neural networks to learn spatial
weighting functions, existing implementations are often restricted by fixed
distance-based schemes and limited inductive bias. We propose to generalize
GNNWR by incorporating concepts from convolutional neural networks, recurrent
neural networks, and transformers, introducing local receptive fields,
sequential context, and self-attention into spatial regression. Through
extensive benchmarking on synthetic spatial datasets with varying
heterogeneity, noise, and sample sizes, we show that GNNWR outperforms classic
methods in capturing nonlinear and complex spatial relationships. Our results
also reveal that model performance depends strongly on data characteristics,
with local models excelling in highly heterogeneous or small-sample scenarios,
and global models performing better with larger, more homogeneous data. These
findings highlight the importance of inductive bias in spatial modeling and
suggest future directions, including learnable spatial weighting functions,
hybrid neural architectures, and improved interpretability for models handling
non-stationary spatial data.

</details>


### [388] [Text-Driven Causal Representation Learning for Source-Free Domain Generalization](https://arxiv.org/abs/2507.09961)
*Lihua Zhou,Mao Ye,Nianxin Li,Shuaifeng Li,Jinlin Wu,Xiatian Zhu,Lei Deng,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.LG

TL;DR: 提出TDCRL，一种结合因果推理的无源域泛化方法，解决深度学习在域不匹配条件下的泛化问题，并在多项基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习在训练和测试数据分布不一致时表现不佳，传统的域泛化需要多个源域数据，成本高昂。近期利用CLIP模型和文本提示的方法虽减少了数据需求，但在处理领域特定混淆变量时表现受限。

Method: 提出TDCRL方法，分为两个步骤：第一，利用数据增强生成风格词向量，与类别信息结合生成文本嵌入以模拟视觉表示；第二，训练一个因果干预网络，通过混杂字典提取领域不变特征。

Result: 在PACS、VLCS、OfficeHome和DomainNet数据集上表现优异，达到了无源域泛化的最新水平。

Conclusion: 基于因果学习的TDCRL提供了一种清晰且高效的机制，能够提取鲁棒的领域不变特征，显著提升无源域泛化能力。

Abstract: Deep learning often struggles when training and test data distributions
differ. Traditional domain generalization (DG) tackles this by including data
from multiple source domains, which is impractical due to expensive data
collection and annotation. Recent vision-language models like CLIP enable
source-free domain generalization (SFDG) by using text prompts to simulate
visual representations, reducing data demands. However, existing SFDG methods
struggle with domain-specific confounders, limiting their generalization
capabilities. To address this issue, we propose TDCRL
(\textbf{T}ext-\textbf{D}riven \textbf{C}ausal \textbf{R}epresentation
\textbf{L}earning), the first method to integrate causal inference into the
SFDG setting. TDCRL operates in two steps: first, it employs data augmentation
to generate style word vectors, combining them with class information to
generate text embeddings to simulate visual representations; second, it trains
a causal intervention network with a confounder dictionary to extract
domain-invariant features. Grounded in causal learning, our approach offers a
clear and effective mechanism to achieve robust, domain-invariant features,
ensuring robust generalization. Extensive experiments on PACS, VLCS,
OfficeHome, and DomainNet show state-of-the-art performance, proving TDCRL
effectiveness in SFDG.

</details>


### [389] [Compliance Minimization via Physics-Informed Gaussian Processes](https://arxiv.org/abs/2507.09968)
*Xiangyu Sun,Amin Yousefpour,Shirin Hosseinmardi,Ramin Bostanabad*

Main category: cs.LG

TL;DR: 研究提出了一种基于物理信息高斯过程（GPs）的无网格框架，用于解决合规性最优问题，同时提高计算效率和设计复杂性控制能力。


<details>
  <summary>Details</summary>
Motivation: 机学习方法用于合规性最优化问题，但存在特征边界差、计算代价高和设计复杂性控制不足等缺点。

Method: 通过基于高斯过程的参数化设计，结合使用多输出神经网络作为均值函数，引入了网格卷积注意力网络架构，并通过同时最小化合规性、总势能和体积分数约束残差来估计所有参数。

Result: 该方法实现了超分辨率的拓扑设计，并在收敛速度、合规性、灰色区域分数及对比其他机器学习方法的表现上表现优异，同时提供了对微观特征的控制能力。

Conclusion: 提出的方法克服了传统机器学习方法的局限性，提高了解决合规性最优化问题的效率、鲁棒性及设计复杂性控制能力。

Abstract: Machine learning (ML) techniques have recently gained significant attention
for solving compliance minimization (CM) problems. However, these methods
typically provide poor feature boundaries, are very expensive, and lack a
systematic mechanism to control the design complexity. Herein, we address these
limitations by proposing a mesh-free and simultaneous framework based on
physics-informed Gaussian processes (GPs). In our approach, we parameterize the
design and state variables with GP priors which have independent kernels but
share a multi-output neural network (NN) as their mean function. The
architecture of this NN is based on Parametric Grid Convolutional Attention
Networks (PGCANs) which not only mitigate spectral bias issues, but also
provide an interpretable mechanism to control design complexity. We estimate
all the parameters of our GP-based representations by simultaneously minimizing
the compliance, total potential energy, and residual of volume fraction
constraint. Importantly, our loss function exclude all data-based residuals as
GPs automatically satisfy them. We also develop computational schemes based on
curriculum training and numerical integration to increase the efficiency and
robustness of our approach which is shown to (1) produce super-resolution
topologies with fast convergence, (2) achieve smaller compliance and less gray
area fraction compared to traditional numerical methods, (3) provide control
over fine-scale features, and (4) outperform competing ML-based methods.

</details>


### [390] [Forecasting Coccidioidomycosis (Valley Fever) in Arizona: A Graph Neural Network Approach](https://arxiv.org/abs/2507.10014)
*Ali Sarabi,Arash Sarabi,Hao Yan,Beckett Sterner,Petar Jevtić*

Main category: cs.LG

TL;DR: 提出一种基于图神经网络的模型，预测美国亚利桑那州的谷热病发病情况，整合环境预测因子和疾病进展延迟效应。


<details>
  <summary>Details</summary>
Motivation: 谷热病在美国西南部的流行区仍是公共健康的重要挑战，现有预测手段无法充分捕捉复杂的时空依赖关系。

Method: 利用监测病例数据与环境预测因子，通过图神经网络构建模型，分析变量之间的相关性及滞后效应。

Result: GNN架构可以有效建模谷热病趋势，并揭示影响发病率的关键环境因素。

Conclusion: 模型为早期预警系统的开发和资源分配决策提供支持，有助于高风险地区的疾病预防工作。

Abstract: Coccidioidomycosis, commonly known as Valley Fever, remains a significant
public health concern in endemic regions of the southwestern United States.
This study develops the first graph neural network (GNN) model for forecasting
Valley Fever incidence in Arizona. The model integrates surveillance case data
with environmental predictors using graph structures, including soil
conditions, atmospheric variables, agricultural indicators, and air quality
metrics. Our approach explores correlation-based relationships among variables
influencing disease transmission. The model captures critical delays in disease
progression through lagged effects, enhancing its capacity to reflect complex
temporal dependencies in disease ecology. Results demonstrate that the GNN
architecture effectively models Valley Fever trends and provides insights into
key environmental drivers of disease incidence. These findings can inform early
warning systems and guide resource allocation for disease prevention efforts in
high-risk areas.

</details>


### [391] [Towards Applying Large Language Models to Complement Single-Cell Foundation Models](https://arxiv.org/abs/2507.10039)
*Steven Palayew,Bo Wang,Gary Bader*

Main category: cs.LG

TL;DR: 这篇论文探讨了单细胞基础模型(scGPT)和大型语言模型(LLMs)的互补作用，提出了scMPT模型，结合两者的优势提升单细胞数据分析性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何突破单细胞基础模型无法利用大量文本信息的局限性，并探讨LLMs作为替代或补充的潜力。

Method: 提出scMPT模型，将scGPT与能够捕获生物学洞见的LLM单细胞表示整合，同时测试不同的融合方法。

Result: scMPT模型性能稳定且优于单一模型，并通过融合方法展示了结合专用逻辑推理模型与scGPT提升性能的可能性。

Conclusion: LLMs可与单细胞基础模型互补，推动单细胞数据分析的改善，展现了两者联合应用的潜力。

Abstract: Single-cell foundation models such as scGPT represent a significant
advancement in single-cell omics, with an ability to achieve state-of-the-art
performance on various downstream biological tasks. However, these models are
inherently limited in that a vast amount of information in biology exists as
text, which they are unable to leverage. There have therefore been several
recent works that propose the use of LLMs as an alternative to single-cell
foundation models, achieving competitive results. However, there is little
understanding of what factors drive this performance, along with a strong focus
on using LLMs as an alternative, rather than complementary approach to
single-cell foundation models. In this study, we therefore investigate what
biological insights contribute toward the performance of LLMs when applied to
single-cell data, and introduce scMPT; a model which leverages synergies
between scGPT, and single-cell representations from LLMs that capture these
insights. scMPT demonstrates stronger, more consistent performance than either
of its component models, which frequently have large performance gaps between
each other across datasets. We also experiment with alternate fusion methods,
demonstrating the potential of combining specialized reasoning models with
scGPT to improve performance. This study ultimately showcases the potential for
LLMs to complement single-cell foundation models and drive improvements in
single-cell analysis.

</details>


### [392] [On the Efficiency of Training Robust Decision Trees](https://arxiv.org/abs/2507.10048)
*Benedict Gerlach,Marie Anastacio,Holger H. Hoos*

Main category: cs.LG

TL;DR: 提出一个高效且简化的对抗性鲁棒决策树训练流程，包括三个阶段：扰动大小选择、对抗训练及验证鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 满足工业领域对机器学习的可信性、效率与可持续性的需求，优化对抗性鲁棒训练流程以提高其效率。

Method: 设计了包含三个阶段的流程：1)利用简单算法自动选择数据集的扰动大小，2)进行对抗性鲁棒性训练并评估时间及精度，3)验证这些模型的鲁棒性并研究验证时间的影响。

Result: 验证效率与训练时间并不直接相关，提出的方法在提高训练效率的同时不牺牲鲁棒性精度。

Conclusion: 该工作提出了一个高效、可持续的训练决策树对抗性鲁棒性的方法，并验证其高效实用性。

Abstract: As machine learning gets adopted into the industry quickly, trustworthiness
is increasingly in focus. Yet, efficiency and sustainability of robust training
pipelines still have to be established. In this work, we consider a simple
pipeline for training adversarially robust decision trees and investigate the
efficiency of each step. Our pipeline consists of three stages. Firstly, we
choose the perturbation size automatically for each dataset. For that, we
introduce a simple algorithm, instead of relying on intuition or prior work.
Moreover, we show that the perturbation size can be estimated from smaller
models than the one intended for full training, and thus significant gains in
efficiency can be achieved. Secondly, we train state-of-the-art adversarial
training methods and evaluate them regarding both their training time and
adversarial accuracy. Thirdly, we certify the robustness of each of the models
thus obtained and investigate the time required for this. We find that
verification time, which is critical to the efficiency of the full pipeline, is
not correlated with training time.

</details>


### [393] [Compression Method for Deep Diagonal State Space Model Based on $H^2$ Optimal Reduction](https://arxiv.org/abs/2507.10078)
*Hiroki Sakamoto,Kazuhiro Sato*

Main category: cs.LG

TL;DR: 本研究提出了一种基于控制理论的$H^{2}$模型降阶技术，用于减少深度学习线性状态空间模型的参数数量，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习中的线性状态空间模型在序列数据处理中表现出色，但其参数量较大，不适合资源受限设备。

Method: 采用控制理论中的$H^{2}$模型降阶技术，对线性状态空间模型的组件进行参数压缩。

Result: 实验表明，提出的方法在压缩线性状态空间模型到原参数数量的$1/32$的同时，其性能优于使用平衡截断方法的现有方案。

Conclusion: 基于$H^{2}$模型降阶的方法，是一种高效的线性状态空间模型参数压缩技术，适合在资源受限环境中使用。

Abstract: Deep learning models incorporating linear SSMs have gained attention for
capturing long-range dependencies in sequential data. However, their large
parameter sizes pose challenges for deployment on resource-constrained devices.
In this study, we propose an efficient parameter reduction method for these
models by applying $H^{2}$ model order reduction techniques from control theory
to their linear SSM components. In experiments, the LRA benchmark results show
that the model compression based on our proposed method outperforms an existing
method using the Balanced Truncation, while successfully reducing the number of
parameters in the SSMs to $1/32$ without sacrificing the performance of the
original models.

</details>


### [394] [Towards High Supervised Learning Utility Training Data Generation: Data Pruning and Column Reordering](https://arxiv.org/abs/2507.10088)
*Tung Sum Thomas Kwok,Zeyong Zhang,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: 本文提出PRRO方法，通过结合数据修剪和列重新排序技术改善表格数据生成，用于提高监督学习中合成数据的性能。


<details>
  <summary>Details</summary>
Motivation: 合成数据在表格数据处理中逐渐流行，但现有技术在生成的合成数据用于监督学习时表现不佳，主要因为类别不平衡和忽略数据内部关系的存在。

Method: 提出的PRRO方法包括两个核心模块：数据修剪部分高比例保留高信噪比样本以优化分类分布；列重新排序模块对齐生成器与监督学习模型的建模结构。

Result: 在22个公开数据集上的实验显示，PRRO生成的表格数据在用于替换和补充原始数据时，对预测性能分别平均提升26.74%和6.13%。此外，针对高度不平衡数据，PRRO使分类分布相似度提高了43%。

Conclusion: PRRO通过结合数据中心AI技术，显著提高了表格生成数据在监督学习应用中的实用性，促进了更高质量和更易访问的数据分析。

Abstract: Tabular data synthesis for supervised learning ('SL') model training is
gaining popularity in industries such as healthcare, finance, and retail.
Despite the progress made in tabular data generators, models trained with
synthetic data often underperform compared to those trained with original data.
This low SL utility of synthetic data stems from class imbalance exaggeration
and SL data relationship overlooked by tabular generator. To address these
challenges, we draw inspirations from techniques in emerging data-centric
artificial intelligence and elucidate Pruning and ReOrdering ('PRRO'), a novel
pipeline that integrates data-centric techniques into tabular data synthesis.
PRRO incorporates data pruning to guide the table generator towards
observations with high signal-to-noise ratio, ensuring that the class
distribution of synthetic data closely matches that of the original data.
Besides, PRRO employs a column reordering algorithm to align the data modeling
structure of generators with that of SL models. These two modules enable PRRO
to optimize SL utility of synthetic data. Empirical experiments on 22 public
datasets show that synthetic data generated using PRRO enhances predictive
performance compared to data generated without PRRO. Specifically, synthetic
replacement of original data yields an average improvement of 26.74% and up to
871.46% improvement using PRRO, while synthetic appendant to original data
results with PRRO-generated data results in an average improvement of 6.13% and
up to 200.32%. Furthermore, experiments on six highly imbalanced datasets show
that PRRO enables the generator to produce synthetic data with a class
distribution that resembles the original data more closely, achieving a
similarity improvement of 43%. Through PRRO, we foster a seamless integration
of data synthesis to subsequent SL prediction, promoting quality and accessible
data analysis.

</details>


### [395] [A Variance-Reduced Cubic-Regularized Newton for Policy Optimization](https://arxiv.org/abs/2507.10120)
*Cheng Sun,Zhen Zhang,Shaofu Yang*

Main category: cs.LG

TL;DR: 本文引入了VR-CR-PN算法，为强化学习中的策略优化提供了有效的二阶方法，提高了样本复杂度并解决了分布移位问题。


<details>
  <summary>Details</summary>
Motivation: 现有的二阶方法在样本复杂度和重要性采样假设方面存在不足，需要更有效的解决方案。

Method: 提出VR-CR-PN算法，结合Hessian辅助的方差减少与二阶策略优化，并设计了新的Hessian估计器以减少计算复杂度。

Result: 理论证明该算法能在一般非凸条件下以$\tilde{\mathcal{O}}(\epsilon^{-3})$达到二阶稳态点，较之前的性能有显著提升。

Conclusion: 提出的算法在不需重要性采样的情况下实现了样本复杂度的最佳结果，并有效解决了分布移位问题，对强化学习研究具有重要意义。

Abstract: In this paper, we study a second-order approach to policy optimization in
reinforcement learning. Existing second-order methods often suffer from
suboptimal sample complexity or rely on unrealistic assumptions about
importance sampling. To overcome these limitations, we propose VR-CR-PN, a
variance-reduced cubic-regularized policy Newton algorithm. To the best of our
knowledge, this is the first algorithm that integrates Hessian-aided variance
reduction with second-order policy optimization, effectively addressing the
distribution shift problem and achieving best-known sample complexity under
general nonconvex conditions but without the need for importance sampling. We
theoretically establish that VR-CR-PN achieves a sample complexity of
$\tilde{\mathcal{O}}(\epsilon^{-3})$ to reach an $\epsilon$-second-order
stationary point, significantly improving upon the previous best result of
$\tilde{\mathcal{O}}(\epsilon^{-3.5})$ under comparable assumptions. As an
additional contribution, we introduce a novel Hessian estimator for the
expected return function, which admits a uniform upper bound independent of the
horizon length $H$, allowing the algorithm to achieve horizon-independent
sample complexity.

</details>


### [396] [Wavelet-Enhanced Neural ODE and Graph Attention for Interpretable Energy Forecasting](https://arxiv.org/abs/2507.10132)
*Usman Gani Joy*

Main category: cs.LG

TL;DR: 本文提出了一种结合神经微分方程、图注意力、多分辨率小波变换和频率自适应学习的框架，用于提升时间序列预测能力，显著优于其他现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决可再生能源波动性和动态消费模式对精准能耗预测的挑战。

Method: 通过结合神经常微分方程、Runge-Kutta ODE求解器、图注意力、残差连接、小波特征提取和适应性频率调制的方法，建模复杂的多尺度时序动态。

Result: 在七个不同数据集上（包括电力变压器温度和可再生能源相关数据集），该模型在多种预测指标下均优于现有主流模型。

Conclusion: 新提出的模型不仅提高了预测性能，还通过SHAP分析增强了模型的可解释性，非常适合于可持续能源领域的应用。

Abstract: Accurate forecasting of energy demand and supply is critical for optimizing
sustainable energy systems, yet it is challenged by the variability of
renewable sources and dynamic consumption patterns. This paper introduces a
neural framework that integrates continuous-time Neural Ordinary Differential
Equations (Neural ODEs), graph attention, multi-resolution wavelet
transformations, and adaptive learning of frequencies to address the issues of
time series prediction. The model employs a robust ODE solver, using the
Runge-Kutta method, paired with graph-based attention and residual connections
to better understand both structural and temporal patterns. Through
wavelet-based feature extraction and adaptive frequency modulation, it adeptly
captures and models diverse, multi-scale temporal dynamics. When evaluated
across seven diverse datasets: ETTh1, ETTh2, ETTm1, ETTm2 (electricity
transformer temperature), and Waste, Solar, and Hydro (renewable energy), this
architecture consistently outperforms state-of-the-art baselines in various
forecasting metrics, proving its robustness in capturing complex temporal
dependencies. Furthermore, the model enhances interpretability through SHAP
analysis, making it suitable for sustainable energy applications.

</details>


### [397] [MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping](https://arxiv.org/abs/2507.10158)
*Obaidullah Zaland,Erik Elmroth,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 该论文提出MTF-Grasp方法，一种多层联邦学习（FL）方法，用于解决机器人抓取任务中的非IID数据分布问题，提升抓取模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习已应用于机器人操作任务，但在抓取任务上研究较少，主要挑战在于各机器人数据非IID且数据量少，导致模型性能下降。

Method: 提出MTF-Grasp方法，通过选取具备高数据质量和数量的“高层次”机器人用于训练初始模型，并将其分发到“低层次”机器人，以缓解模型性能退化问题。

Result: MTF-Grasp在Cornell和Jacquard抓取数据集上相较传统FL设置提升了高达8%的性能。

Conclusion: MTF-Grasp能有效应对机器人抓取任务中因数据非IID和数量不均导致的性能降低问题，证明了其在多层联邦学习设置中的优势。

Abstract: Federated Learning (FL) is a promising machine learning paradigm that enables
participating devices to train privacy-preserved and collaborative models. FL
has proven its benefits for robotic manipulation tasks. However, grasping tasks
lack exploration in such settings where robots train a global model without
moving data and ensuring data privacy. The main challenge is that each robot
learns from data that is nonindependent and identically distributed (non-IID)
and of low quantity. This exhibits performance degradation, particularly in
robotic grasping. Thus, in this work, we propose MTF-Grasp, a multi-tier FL
approach for robotic grasping, acknowledging the unique challenges posed by the
non-IID data distribution across robots, including quantitative skewness.
MTF-Grasp harnesses data quality and quantity across robots to select a set of
"top-level" robots with better data distribution and higher sample count. It
then utilizes top-level robots to train initial seed models and distribute them
to the remaining "low-level" robots, reducing the risk of model performance
degradation in low-level robots. Our approach outperforms the conventional FL
setup by up to 8% on the quantity-skewed Cornell and Jacquard grasping
datasets.

</details>


### [398] [Domain Borders Are There to Be Crossed With Federated Few-Shot Adaptation](https://arxiv.org/abs/2507.10160)
*Manuel Röder,Christoph Raab,Frank-Michael Schleif*

Main category: cs.LG

TL;DR: 本文提出了一种名为FedAcross+的联邦学习框架，旨在解决数据标注成本高、客户端数据协变量偏移以及资源受限环境下模型更新不便的问题。


<details>
  <summary>Details</summary>
Motivation: 在工业环境中，由于传感器环境因素对数据采集的影响以及资源受限设备的限制，现实中的联邦学习面临数据偏移、终端适配难度大以及模型更新成本高等挑战。

Method: 提出的FedAcross+框架包含预训练的深度骨干网、适配模块和分类器，通过冻结骨干网和分类器，仅使用域适应线性层进行目标域适配，从而降低计算开销，并支持流数据处理，适应非平稳环境。

Result: 实验结果表明，在低端设备和有限数据样本下，FedAcross+框架能够在客户端实现高效适配，并成功应对域偏移问题。

Conclusion: FedAcross+证明其在资源受限环境中的实际部署可行性，为低成本、高效的联邦学习提供了可行解决方案。

Abstract: Federated Learning has emerged as a leading paradigm for decentralized,
privacy-preserving learning, particularly relevant in the era of interconnected
edge devices equipped with sensors. However, the practical implementation of
Federated Learning faces three primary challenges: the need for human
involvement in costly data labelling processes for target adaptation, covariate
shift in client device data collection due to environmental factors affecting
sensors, leading to discrepancies between source and target samples, and the
impracticality of continuous or regular model updates in resource-constrained
environments due to limited data transmission capabilities and technical
constraints on channel availability and energy efficiency. To tackle these
issues, we expand upon an efficient and scalable Federated Learning framework
tailored for real-world client adaptation in industrial settings. This
framework leverages a pre-trained source model comprising a deep backbone, an
adaptation module, and a classifier running on a powerful server. By freezing
the backbone and classifier during client adaptation on resource-constrained
devices, we allow the domain adaptive linear layer to handle target domain
adaptation, thus minimizing overall computational overhead. Furthermore, this
setup, designated as FedAcross+, is extended to encompass the processing of
streaming data, thereby rendering the solution suitable for non-stationary
environments. Extensive experimental results demonstrate the effectiveness of
FedAcross+ in achieving competitive adaptation on low-end client devices with
limited target samples, successfully addressing the challenge of domain shift.
Moreover, our framework accommodates sporadic model updates within
resource-constrained environments, ensuring practical and seamless deployment.

</details>


### [399] [Understanding the Rank of Tensor Networks via an Intuitive Example-Driven Approach](https://arxiv.org/abs/2507.10170)
*Wuyang Zhou,Giorgos Iacovides,Kriton Konstantinidis,Ilya Kisil,Danilo Mandic*

Main category: cs.LG

TL;DR: 本文探讨张量网络(Tensor Network, TN)的秩(TN ranks)的含义与应用，并提供直观示例及工具，帮助研究人员更好地理解和应用TN方法处理高阶数据。


<details>
  <summary>Details</summary>
Motivation: 针对张量网络中张量秩缺乏统一定义和依赖经验调整的问题，该论文希望通过揭示其与应用领域的联系，提供更直观的解释。

Method: 采用实例与直观图形化方法，解释TN秩如何从常用的分解模型（如CP和Tucker）中选取，并推广到更复杂的张量网络结构。

Result: 通过图示化展示TN秩与张量展开及其矩阵秩之间的关系，简化多指标张量代数，支持领域驱动的TN设计。

Conclusion: 为读者提供了清晰的TN秩理解框架，帮助在实际应用与教学中更加高效地选择和应用张量方法。

Abstract: Tensor Network (TN) decompositions have emerged as an indispensable tool in
Big Data analytics owing to their ability to provide compact low-rank
representations, thus alleviating the ``Curse of Dimensionality'' inherent in
handling higher-order data. At the heart of their success lies the concept of
TN ranks, which governs the efficiency and expressivity of TN decompositions.
However, unlike matrix ranks, TN ranks often lack a universal meaning and an
intuitive interpretation, with their properties varying significantly across
different TN structures. Consequently, TN ranks are frequently treated as
empirically tuned hyperparameters, rather than as key design parameters
inferred from domain knowledge. The aim of this Lecture Note is therefore to
demystify the foundational yet frequently misunderstood concept of TN ranks
through real-life examples and intuitive visualizations. We begin by
illustrating how domain knowledge can guide the selection of TN ranks in
widely-used models such as the Canonical Polyadic (CP) and Tucker
decompositions. For more complex TN structures, we employ a self-explanatory
graphical approach that generalizes to tensors of arbitrary order. Such a
perspective naturally reveals the relationship between TN ranks and the
corresponding ranks of tensor unfoldings (matrices), thereby circumventing
cumbersome multi-index tensor algebra while facilitating domain-informed TN
design. It is our hope that this Lecture Note will equip readers with a clear
and unified understanding of the concept of TN rank, along with the necessary
physical insight and intuition to support the selection, explainability, and
deployment of tensor methods in both practical applications and educational
contexts.

</details>


### [400] [Play Style Identification Using Low-Level Representations of Play Traces in MicroRTS](https://arxiv.org/abs/2507.10172)
*Ruizhe Yu Xia,Jeremy Gow,Simon Lucas*

Main category: cs.LG

TL;DR: 本研究提出利用无监督的CNN-LSTM自动编码器模型，从低阶游戏轨迹数据中直接提取潜在表示，有效区分不同游戏玩家的行为风格。


<details>
  <summary>Details</summary>
Motivation: 现有研究依赖领域知识或抽象层级来分析玩家的游戏风格，存在依赖性及偏差问题。

Method: 利用无监督的CNN-LSTM自动编码器，从MicroRTS游戏的低级游戏轨迹数据中直接提取潜在表示，以减少对领域知识的依赖。

Result: 成功在潜在空间中实现了不同游戏玩家的行为风格区分，证明了方法的有效性。

Conclusion: 无需领域知识的潜在表示方法具有指导AI玩家探索多样化游戏风格的潜力。

Abstract: Play style identification can provide valuable game design insights and
enable adaptive experiences, with the potential to improve game playing agents.
Previous work relies on domain knowledge to construct play trace
representations using handcrafted features. More recent approaches incorporate
the sequential structure of play traces but still require some level of domain
abstraction. In this study, we explore the use of unsupervised CNN-LSTM
autoencoder models to obtain latent representations directly from low-level
play trace data in MicroRTS. We demonstrate that this approach yields a
meaningful separation of different game playing agents in the latent space,
reducing reliance on domain expertise and its associated biases. This latent
space is then used to guide the exploration of diverse play styles within
studied AI players.

</details>


### [401] [T-GRAB: A Synthetic Diagnostic Benchmark for Learning on Temporal Graphs](https://arxiv.org/abs/2507.10183)
*Alireza Dizaji,Benedict Aaron Tjandra,Mehrab Hamidi,Shenyang Huang,Guillaume Rabusseau*

Main category: cs.LG

TL;DR: 本文提出了一个名为T-GRAB的基准，用于系统性地评估时序图神经网络（TGNNs）在时间推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的时序图神经网络尽管在动态图建模中表现出色，但在捕捉周期性、因果关系和长范围依赖等核心时间模式上的效果尚不明确。

Method: 设计了合成任务集T-GRAB，用于隔离和评估TGNNs在不同时间技能上的表现，如记忆周期重复、推断延迟因果关系和捕获时空长距离依赖。

Result: 对11种时序图学习方法进行测试，结果揭示了它们在捕捉时间模式上的重大不足。

Conclusion: 研究为现有模型在时间推理上的局限性提供了有价值的见解，并推动了具有更强时间推理能力的架构开发。

Abstract: Dynamic graph learning methods have recently emerged as powerful tools for
modelling relational data evolving through time. However, despite extensive
benchmarking efforts, it remains unclear whether current Temporal Graph Neural
Networks (TGNNs) effectively capture core temporal patterns such as
periodicity, cause-and-effect, and long-range dependencies. In this work, we
introduce the Temporal Graph Reasoning Benchmark (T-GRAB), a comprehensive set
of synthetic tasks designed to systematically probe the capabilities of TGNNs
to reason across time. T-GRAB provides controlled, interpretable tasks that
isolate key temporal skills: counting/memorizing periodic repetitions,
inferring delayed causal effects, and capturing long-range dependencies over
both spatial and temporal dimensions. We evaluate 11 temporal graph learning
methods on these tasks, revealing fundamental shortcomings in their ability to
generalize temporal patterns. Our findings offer actionable insights into the
limitations of current models, highlight challenges hidden by traditional
real-world benchmarks, and motivate the development of architectures with
stronger temporal reasoning abilities. The code for T-GRAB can be found at:
https://github.com/alirezadizaji/T-GRAB.

</details>


### [402] [Learning Private Representations through Entropy-based Adversarial Training](https://arxiv.org/abs/2507.10194)
*Tassilo Klein,Moin Nabi*

Main category: cs.LG

TL;DR: 本文提出了一种对抗性表征学习方法，旨在消除学习表征中的敏感内容，同时保证高预测能力。


<details>
  <summary>Details</summary>
Motivation: 希望在保护用户隐私的前提下学习具有高预测能力的表征。

Method: 引入了一种变体熵——焦点熵，通过其减轻现有基于熵方法的潜在信息泄漏问题。采用对抗性表征学习进行隐私数据清洗。

Result: 在多个基准测试中展示了该方法的可行性，结果表明可以在适度隐私泄漏的情况下保持高目标效用。

Conclusion: 对抗性表征学习与焦点熵方法能够有效平衡隐私保护与表征的预测性能，是一种具有潜力的方法。

Abstract: How can we learn a representation with high predictive power while preserving
user privacy? We present an adversarial representation learning method for
sanitizing sensitive content from the learned representation. Specifically, we
introduce a variant of entropy - focal entropy, which mitigates the potential
information leakage of the existing entropy-based approaches. We showcase
feasibility on multiple benchmarks. The results suggest high target utility at
moderate privacy leakage.

</details>


### [403] [A Graph Sufficiency Perspective for Neural Networks](https://arxiv.org/abs/2507.10215)
*Cencheng Shen,Yuexiao Dong*

Main category: cs.LG

TL;DR: 通过将神经网络解析为图变量和统计充分性，提出了一种新框架，并证明了神经网络层级中条件分布的保留条件。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络层如何通过统计充分性保留输入信息，以及这种现象如何跨图和统计表示建模。

Method: 将神经网络层描述为基于图的变换，分析各层输出的统计充分性，并证明了宽度无限化及有限宽度情况下的充分性条件。

Result: 证明了在无限宽度情况下的渐进充分性，并提供了有限宽度网络在特定输入分布和锚点构造下实现充分性的条件。

Conclusion: 此框架将统计充分性、图形表示和深度学习相结合，提供了对神经网络的新统计理解。

Abstract: This paper analyzes neural networks through graph variables and statistical
sufficiency. We interpret neural network layers as graph-based transformations,
where neurons act as pairwise functions between inputs and learned anchor
points. Within this formulation, we establish conditions under which layer
outputs are sufficient for the layer inputs, that is, each layer preserves the
conditional distribution of the target variable given the input variable. Under
dense anchor point assumptions, we prove that asymptotic sufficiency holds in
the infinite-width limit and is preserved throughout training. To align more
closely with practical architectures, we further show that sufficiency can be
achieved with finite-width networks by assuming region-separated input
distributions and constructing appropriate anchor points. Our framework covers
fully connected layers, general pairwise functions, ReLU and sigmoid
activations, and convolutional neural networks. This work bridges statistical
sufficiency, graph-theoretic representations, and deep learning, providing a
new statistical understanding of neural networks.

</details>


### [404] [Kernel-Adaptive PI-ELMs for Forward and Inverse Problems in PDEs with Sharp Gradients](https://arxiv.org/abs/2507.10241)
*Vikas Dwivedi,Balaji Srinivasan,Monica Sigovan,Bruno Sixou*

Main category: cs.LG

TL;DR: 本文提出了KAPI-ELM，一种基于自适应径向基函数扩展的物理约束极限学习机模型，能够以更少参数高效处理具有局部锐利梯度的PDE问题。


<details>
  <summary>Details</summary>
Motivation: 解决传统PI-ELM无法捕捉PDE局部锐利梯度的问题，同时保持其快速优化优势。

Method: 通过轻量级贝叶斯优化框架调整输入层参数分布，而输出层继续采用最小二乘优化，并结合径向基扩展策略。

Result: KAPI-ELM在多个PDE基准实验中实现了前沿精度，在刚性PDE中超过了如XTFC等高级算法，且参数更少。

Conclusion: KAPI-ELM作为一种可扩展、可解释且泛化性强的物理约束学习框架，在刚性PDE问题中展示了巨大潜力。

Abstract: This paper introduces the Kernel Adaptive Physics-Informed Extreme Learning
Machine (KAPI-ELM), an adaptive Radial Basis Function (RBF)-based extension of
PI-ELM designed to solve both forward and inverse Partial Differential Equation
(PDE) problems involving localized sharp gradients. While PI-ELMs outperform
the traditional Physics-Informed Neural Networks (PINNs) in speed due to their
single-shot, least square optimization, this advantage comes at a cost: their
fixed, randomly initialized input layer limits their ability to capture sharp
gradients. To overcome this limitation, we introduce a lightweight Bayesian
Optimization (BO) framework that, instead of adjusting each input layer
parameter individually as in traditional backpropagation, learns a small set of
hyperparameters defining the statistical distribution from which the input
weights are drawn. This novel distributional optimization strategy -- combining
BO for input layer distributional parameters with least-squares optimization
for output layer network parameters -- enables KAPI-ELM to preserve PI-ELM's
speed while matching or exceeding the expressiveness of PINNs. We validate the
proposed methodology on several challenging forward and inverse PDE benchmarks,
including a 1D singularly perturbed convection-diffusion equation, a 2D Poisson
equation with sharp localized sources, and a time-dependent advection equation.
Notably, KAPI-ELM achieves state-of-the-art accuracy in both forward and
inverse settings. In stiff PDE regimes, it matches or even outperforms advanced
methods such as the Extended Theory of Functional Connections (XTFC), while
requiring nearly an order of magnitude fewer tunable parameters. These results
establish the potential of KAPI-ELM as a scalable, interpretable, and
generalizable physics-informed learning framework, especially in stiff PDE
regimes.

</details>


### [405] [Conditional Chemical Language Models are Versatile Tools in Drug Discovery](https://arxiv.org/abs/2507.10273)
*Lu Zhu,Emmanuel Noutahi*

Main category: cs.LG

TL;DR: SAFE-T 是一个通用化学建模框架，在无结构信息或工程评分函数的情况下，条件于生物学上下文，用于分子优选和设计，且在多项任务和基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式化学语言模型（CLMs）在分子设计中展现了强大能力，但其在药物发现中的应用受限于缺乏可靠奖励信号和结果的不易解释性。

Method: SAFE-T 基于生物学提示建模片段分子序列的条件似然，通过从学习分布采样实现目标导向的分子生成，并在多任务（如虚拟筛选、药物-靶点相互作用预测等）中打分和匹配生物学目标。

Result: 在多项零样本预测和生成基准测试中，SAFE-T 的性能优于或等同于现有方法，同时显著提高计算效率。

Conclusion: SAFE-T 展示了条件生成式化学语言模型可通过统一打分和生成方法加速药物早期发现过程。

Abstract: Generative chemical language models (CLMs) have demonstrated strong
capabilities in molecular design, yet their impact in drug discovery remains
limited by the absence of reliable reward signals and the lack of
interpretability in their outputs. We present SAFE-T, a generalist chemical
modeling framework that conditions on biological context -- such as protein
targets or mechanisms of action -- to prioritize and design molecules without
relying on structural information or engineered scoring functions. SAFE-T
models the conditional likelihood of fragment-based molecular sequences given a
biological prompt, enabling principled scoring of molecules across tasks such
as virtual screening, drug-target interaction prediction, and activity cliff
detection. Moreover, it supports goal-directed generation by sampling from this
learned distribution, aligning molecular design with biological objectives. In
comprehensive zero-shot evaluations across predictive (LIT-PCBA, DAVIS, KIBA,
ACNet) and generative (DRUG, PMO) benchmarks, SAFE-T consistently achieves
performance comparable to or better than existing approaches while being
significantly faster. Fragment-level attribution further reveals that SAFE-T
captures known structure-activity relationships, supporting interpretable and
biologically grounded design. Together with its computational efficiency, these
results demonstrate that conditional generative CLMs can unify scoring and
generation to accelerate early-stage drug discovery.

</details>


### [406] [Average Sensitivity of Hierarchical $k$-Median Clustering](https://arxiv.org/abs/2507.10296)
*Shijie Li,Weiqiang He,Ruobing Bai,Pan Peng*

Main category: cs.LG

TL;DR: 文章研究了层次型k-中心聚类问题，提出了一种高效且对数据扰动敏感度低的算法，并通过实验验证了其稳定性和效果。


<details>
  <summary>Details</summary>
Motivation: 聚类算法在面对大规模动态数据时，如果对数据集的小扰动敏感，实用性会大大降低，作者想开发一种更加稳定的层次聚类算法。

Method: 分析层次型k-中心聚类算法的平均敏感性，通过随机删除数据点测评算法输出的变化，并提出一种新算法，理论证明其低平均敏感性和高聚类质量。

Result: 发现单一连线聚类和CLNSS算法具有高敏感性，而新提出的算法敏感性低且聚类效果佳。

Conclusion: 提出的方法在理论和实验中都表现出低敏感性和高鲁棒性，适用于处理大规模动态数据。

Abstract: Hierarchical clustering is a widely used method for unsupervised learning
with numerous applications. However, in the application of modern algorithms,
the datasets studied are usually large and dynamic. If the hierarchical
clustering is sensitive to small perturbations of the dataset, the usability of
the algorithm will be greatly reduced. In this paper, we focus on the
hierarchical $k$ -median clustering problem, which bridges hierarchical and
centroid-based clustering while offering theoretical appeal, practical utility,
and improved interpretability. We analyze the average sensitivity of algorithms
for this problem by measuring the expected change in the output when a random
data point is deleted. We propose an efficient algorithm for hierarchical
$k$-median clustering and theoretically prove its low average sensitivity and
high clustering quality. Additionally, we show that single linkage clustering
and a deterministic variant of the CLNSS algorithm exhibit high average
sensitivity, making them less stable. Finally, we validate the robustness and
effectiveness of our algorithm through experiments.

</details>


### [407] [Recognizing Dementia from Neuropsychological Tests with State Space Models](https://arxiv.org/abs/2507.10311)
*Liming Wang,Saurabhchand Bhati,Cody Karjadi,Rhoda Au,James Glass*

Main category: cs.LG

TL;DR: 本研究提出了一个名为Demenba的新型自动痴呆分类(ADC)框架，该框架基于状态空间模型，在内存和计算上对序列长度具线性扩展性，并在现有方法上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 早期检测痴呆对于及时的医疗干预和改善患者治疗效果至关重要，而传统基于手动评分的检测方法局限性较大。

Method: 利用状态空间模型开发新型的Demenba框架，结合超1000小时的认知评估数据进行训练，并通过整合大型语言模型提高性能。

Result: Demenba框架在精细化痴呆分类上优于现有方法21%，并使用了更少的模型参数，同时通过与大型语言模型融合进一步提升性能。

Conclusion: Demenba为实现更透明、可扩展的痴呆评估工具打下了基础，有潜力改变当前痴呆检测与分类方式。

Abstract: Early detection of dementia is critical for timely medical intervention and
improved patient outcomes. Neuropsychological tests are widely used for
cognitive assessment but have traditionally relied on manual scoring. Automatic
dementia classification (ADC) systems aim to infer cognitive decline directly
from speech recordings of such tests. We propose Demenba, a novel ADC framework
based on state space models, which scale linearly in memory and computation
with sequence length. Trained on over 1,000 hours of cognitive assessments
administered to Framingham Heart Study participants, some of whom were
diagnosed with dementia through adjudicated review, our method outperforms
prior approaches in fine-grained dementia classification by 21\%, while using
fewer parameters. We further analyze its scaling behavior and demonstrate that
our model gains additional improvement when fused with large language models,
paving the way for more transparent and scalable dementia assessment tools.
Code: https://anonymous.4open.science/r/Demenba-0861

</details>


### [408] [Convergence of Agnostic Federated Averaging](https://arxiv.org/abs/2507.10325)
*Herlock,Rahimi,Dionysis Kalogerias*

Main category: cs.LG

TL;DR: 本文探讨在随机且非均匀客户端参与情况下，联邦学习（FL）的收敛性，提出对不可知FedAvg算法的优化问题，并证明其收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法多假设客户端完全参与或参与概率分布已知，但这些假设在实践中很难满足。因此，需要一种能够处理非均匀客户端参与情景的理论框架。

Method: 研究不可知FedAvg算法在随机和不同规模客户端参与下的优化问题，通过数学分析证明其在凸且可能非平滑损失中的收敛特性，收敛速率为$\mathcal{O}(1/\sqrt{T})$（T为聚合时间）。

Result: 证明了不可知FedAvg算法在通用非均匀随机客户端参与情况下的收敛性，并通过实验表明其优于常见的加权聚合方法，即使在服务器知道参与分布权重的情况下亦然。

Conclusion: 不可知FedAvg在困难的随机客户端参与场景下实现了理论收敛，拓展了联邦学习的实际适用性。

Abstract: Federated learning (FL) enables decentralized model training without
centralizing raw data. However, practical FL deployments often face a key
realistic challenge: Clients participate intermittently in server aggregation
and with unknown, possibly biased participation probabilities. Most existing
convergence results either assume full-device participation, or rely on
knowledge of (in fact uniform) client availability distributions -- assumptions
that rarely hold in practice. In this work, we characterize the optimization
problem that consistently adheres to the stochastic dynamics of the well-known
\emph{agnostic Federated Averaging (FedAvg)} algorithm under random (and
variably-sized) client availability, and rigorously establish its convergence
for convex, possibly nonsmooth losses, achieving a standard rate of order
$\mathcal{O}(1/\sqrt{T})$, where $T$ denotes the aggregation horizon. Our
analysis provides the first convergence guarantees for agnostic FedAvg under
general, non-uniform, stochastic client participation, without knowledge of the
participation distribution. We also empirically demonstrate that agnostic
FedAvg in fact outperforms common (and suboptimal) weighted aggregation FedAvg
variants, even with server-side knowledge of participation weights.

</details>


### [409] [MoCap-Impute: A Comprehensive Benchmark and Comparative Analysis of Imputation Methods for IMU-based Motion Capture Data](https://arxiv.org/abs/2507.10334)
*Mahmoud Bekhit,Ahmad Salah,Ahmed Salim Alrawahi,Tarek Attia,Ahmed Ali,Esraa Eldesokey,Ahmed Fathalla*

Main category: cs.LG

TL;DR: 这篇论文研究了可穿戴IMU生成的运动捕捉数据在存在缺失情况下不同补全方法的表现，通过实验得出多变量方法优于单变量方法，在复杂缺失情境下表现尤为显著。


<details>
  <summary>Details</summary>
Motivation: 运动捕捉数据对于运动科学至关重要，但数据缺失问题限制了其应用，该论文旨在系统评估现存补全技术并填补MoCap时间序列数据补全性能研究的空白。

Method: 使用包含53名空手道练习者的首个公开数据集，对统计学、机器学习和深度学习方法进行评估，模拟三种缺失机制并比较补全方法表现。

Result: 多变量补全方法在复杂缺失模式下表现最佳，可将MAE显著降低50%。其中，GAIN和迭代补全器在复杂情境中达到最高准确性。

Conclusion: 研究成果提供了未来研究的关键基准，并提出了提高运动捕捉数据分析完整性和鲁棒性的实践建议。

Abstract: Motion capture (MoCap) data from wearable Inertial Measurement Units (IMUs)
is vital for applications in sports science, but its utility is often
compromised by missing data. Despite numerous imputation techniques, a
systematic performance evaluation for IMU-derived MoCap time-series data is
lacking. We address this gap by conducting a comprehensive comparative analysis
of statistical, machine learning, and deep learning imputation methods. Our
evaluation considers three distinct contexts: univariate time-series,
multivariate across subjects, and multivariate across kinematic angles. To
facilitate this benchmark, we introduce the first publicly available MoCap
dataset designed specifically for imputation, featuring data from 53 karate
practitioners. We simulate three controlled missingness mechanisms: missing
completely at random (MCAR), block missingness, and a novel value-dependent
pattern at signal transition points. Our experiments, conducted on 39 kinematic
variables across all subjects, reveal that multivariate imputation frameworks
consistently outperform univariate approaches, particularly for complex
missingness. For instance, multivariate methods achieve up to a 50% mean
absolute error reduction (MAE from 10.8 to 5.8) compared to univariate
techniques for transition point missingness. Advanced models like Generative
Adversarial Imputation Networks (GAIN) and Iterative Imputers demonstrate the
highest accuracy in these challenging scenarios. This work provides a critical
baseline for future research and offers practical recommendations for improving
the integrity and robustness of Mo-Cap data analysis.

</details>


### [410] [Some Super-approximation Rates of ReLU Neural Networks for Korobov Functions](https://arxiv.org/abs/2507.10345)
*Yuwen Li,Guozhi Zhang*

Main category: cs.LG

TL;DR: 本文研究ReLU神经网络对Korobov函数的$L_p$和$W^1_p$范数近似误差，提出了几乎最优的超近似误差界。


<details>
  <summary>Details</summary>
Motivation: 针对高维目标函数的混合导数性质，探讨神经网络在不同范数下的近似能力，优化经典方法的误差界限。

Method: 利用稀疏网格有限元和位提取技术，分析并推导出ReLU神经网络的误差界。

Result: 提出$L_p$范数下误差阶为$2m$，$W^1_p$范数下误差阶为$2m-2$的结果，表明神经网络克服了维数灾难的限制。

Conclusion: 研究表明，ReLU神经网络在高维函数近似中具有较高效能，为深度学习理论提供新的支持。

Abstract: This paper examines the $L_p$ and $W^1_p$ norm approximation errors of ReLU
neural networks for Korobov functions. In terms of network width and depth, we
derive nearly optimal super-approximation error bounds of order $2m$ in the
$L_p$ norm and order $2m-2$ in the $W^1_p$ norm, for target functions with
$L_p$ mixed derivative of order $m$ in each direction. The analysis leverages
sparse grid finite elements and the bit extraction technique. Our results
improve upon classical lowest order $L_\infty$ and $H^1$ norm error bounds and
demonstrate that the expressivity of neural networks is largely unaffected by
the curse of dimensionality.

</details>


### [411] [Parallel Sampling of Diffusion Models on $SO(3)$](https://arxiv.org/abs/2507.10347)
*Yan-Ting Chen,Hao-Wei Chen,Tsu-Ching Hsiao,Chun-Yi Lee*

Main category: cs.LG

TL;DR: 本文提出一种针对 $SO(3)$ 流形上的扩散过程加速算法。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型由于其本质的顺序特性，在对受扰数据去噪时需要耗费大量时间。

Method: 将数值Picard迭代适配至 $SO(3)$ 空间，应用于现有的扩散模型方法以解决姿态模糊问题。

Result: 实验显示算法在保持任务奖励不受损的前提下，加速效果可达 4.9 倍，大幅减少单样本生成的延迟时间。

Conclusion: 该方法有效加速了 $SO(3)$ 流形上的扩散过程且不损失性能。

Abstract: In this paper, we design an algorithm to accelerate the diffusion process on
the $SO(3)$ manifold. The inherently sequential nature of diffusion models
necessitates substantial time for denoising perturbed data. To overcome this
limitation, we proposed to adapt the numerical Picard iteration for the $SO(3)$
space. We demonstrate our algorithm on an existing method that employs
diffusion models to address the pose ambiguity problem. Moreover, we show that
this acceleration advantage occurs without any measurable degradation in task
reward. The experiments reveal that our algorithm achieves a speed-up of up to
4.9$\times$, significantly reducing the latency for generating a single sample.

</details>


### [412] [Feature Distillation is the Better Choice for Model-Heterogeneous Federated Learning](https://arxiv.org/abs/2507.10348)
*Yichen Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为FedFD的模型异构联邦学习方法，通过特征蒸馏提高模型性能，相较于之前的方法更稳定高效。


<details>
  <summary>Details</summary>
Motivation: 现有的Hetero-FL方法因为异构模型之间的知识偏差问题，难以达到理想的性能，因此需要一种更有效的知识整合方法。

Method: 作者提出了一种基于特征蒸馏的联邦学习方法，利用正交投影对齐来自不同模型架构的特征，并通过重新参数化减少知识偏差以增强模型性能。

Result: 实验表明，FedFD在性能上优于现有的最先进方法，同时训练过程更加稳定。

Conclusion: FedFD提供了一种稳定且高效的解决方案，可实现模型异构联邦学习场景中更好的知识整合，可推广性强。

Abstract: Model-Heterogeneous Federated Learning (Hetero-FL) has attracted growing
attention for its ability to aggregate knowledge from heterogeneous models
while keeping private data locally. To better aggregate knowledge from clients,
ensemble distillation, as a widely used and effective technique, is often
employed after global aggregation to enhance the performance of the global
model. However, simply combining Hetero-FL and ensemble distillation does not
always yield promising results and can make the training process unstable. The
reason is that existing methods primarily focus on logit distillation, which,
while being model-agnostic with softmax predictions, fails to compensate for
the knowledge bias arising from heterogeneous models. To tackle this challenge,
we propose a stable and efficient Feature Distillation for model-heterogeneous
Federated learning, dubbed FedFD, that can incorporate aligned feature
information via orthogonal projection to integrate knowledge from heterogeneous
models better. Specifically, a new feature-based ensemble federated knowledge
distillation paradigm is proposed. The global model on the server needs to
maintain a projection layer for each client-side model architecture to align
the features separately. Orthogonal techniques are employed to re-parameterize
the projection layer to mitigate knowledge bias from heterogeneous models and
thus maximize the distilled knowledge. Extensive experiments show that FedFD
achieves superior performance compared to state-of-the-art methods.

</details>


### [413] [TAT: Temporal-Aligned Transformer for Multi-Horizon Peak Demand Forecasting](https://arxiv.org/abs/2507.10349)
*Zhiyuan Zhao,Sitan Yang,Kin G. Olivares,Boris N. Oreshkin,Stan Vitebsky,Michael W. Mahoney,B. Aditya Prakash,Dmitry Efimov*

Main category: cs.LG

TL;DR: 论文提出了Temporal-Aligned Transformer (TAT)，一种专注于解决高峰需求预测问题的多周期时间序列预测模型，并在实际数据中表现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 跨时间周期的需求预测对于供应链管理至关重要，尤其在高峰期需求更难以准确预测。本研究旨在改进高峰需求预测准确性，提高交易高峰期的供应链运作效率及用户体验。

Method: 提出了Temporal-Aligned Transformer (TAT)模型，包含具有Temporal Alignment Attention（TAA）的编码器和解码器，利用先验上下文变量（如节假日和促销信息）进行上下文相关的时间对齐，提高预测性能。

Result: 在两个大规模电商数据集上进行实验，TAT在高峰需求预测中实现了高达30%的准确性提升，并在整体性能上保持与其他先进方法的竞争力。

Conclusion: TAT模型能显著改进高峰需求预测的表现，并展示了上下文时间对齐方法在多周期时间序列预测中的潜在价值。

Abstract: Multi-horizon time series forecasting has many practical applications such as
demand forecasting. Accurate demand prediction is critical to help make buying
and inventory decisions for supply chain management of e-commerce and physical
retailers, and such predictions are typically required for future horizons
extending tens of weeks. This is especially challenging during high-stake sales
events when demand peaks are particularly difficult to predict accurately.
However, these events are important not only for managing supply chain
operations but also for ensuring a seamless shopping experience for customers.
To address this challenge, we propose Temporal-Aligned Transformer (TAT), a
multi-horizon forecaster leveraging apriori-known context variables such as
holiday and promotion events information for improving predictive performance.
Our model consists of an encoder and decoder, both embedded with a novel
Temporal Alignment Attention (TAA), designed to learn context-dependent
alignment for peak demand forecasting. We conduct extensive empirical analysis
on two large-scale proprietary datasets from a large e-commerce retailer. We
demonstrate that TAT brings up to 30% accuracy improvement on peak demand
forecasting while maintaining competitive overall performance compared to other
state-of-the-art methods.

</details>


### [414] [Enhanced DeepONet for 1-D consolidation operator learning: an architectural investigation](https://arxiv.org/abs/2507.10368)
*Yongjin Choi,Chenying Liu,Jorge Macedo*

Main category: cs.LG

TL;DR: 本研究评估了多种DeepONet架构在解决一维固结问题中的表现，并提出了四种模型，其中Model 4通过引入傅里叶特征增强了对快速变化函数的捕捉，表现最佳，大幅提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 探讨并验证DeepONet在岩土工程中的应用潜力，特别是针对解决以PDE控制的系统操作符的高效求解问题。

Method: 实验评估标准DeepONet架构及结合物理启发的模型并提出傅里叶特征增强的Trunknet DeepONet架构（Model 4）。

Result: Model 4能够更好捕捉快速变化的目标函数，同时在计算效率上实现了较传统解算器最大100倍的速度提升。

Conclusion: 研究证明通过DeepONet特别是相关改进模型的研发，可推进科学机器学习在岩土工程中的应用，为复杂系统的建模提供高效手段。

Abstract: Deep Operator Networks (DeepONets) have emerged as a powerful surrogate
modeling framework for learning solution operators in PDE-governed systems.
While their use is expanding across engineering disciplines, applications in
geotechnical engineering remain limited. This study systematically evaluates
several DeepONet architectures for the one-dimensional consolidation problem.
We initially consider three architectures: a standard DeepONet with the
coefficient of consolidation embedded in the branch net (Models 1 and 2), and a
physics-inspired architecture with the coefficient embedded in the trunk net
(Model 3). Results show that Model 3 outperforms the standard configurations
(Models 1 and 2) but still has limitations when the target solution (excess
pore pressures) exhibits significant variation. To overcome this limitation, we
propose a Trunknet Fourier feature-enhanced DeepONet (Model 4) that addresses
the identified limitations by capturing rapidly varying functions. All proposed
architectures achieve speedups ranging from 1.5 to 100 times over traditional
explicit and implicit solvers, with Model 4 being the most efficient. Larger
computational savings are expected for more complex systems than the explored
1D case, which is promising. Overall, the study highlights the potential of
DeepONets to enable efficient, generalizable surrogate modeling in geotechnical
applications, advancing the integration of scientific machine learning in
geotechnics, which is at an early stage.

</details>


### [415] [Leveraging RAG-LLMs for Urban Mobility Simulation and Analysis](https://arxiv.org/abs/2507.10382)
*Yue Ding,Conor McCarthy,Kevin O'Shea,Mingming Liu*

Main category: cs.LG

TL;DR: 本文提出了一个结合云计算和大语言模型（LLM）的共享电子出行平台，重点在个性化路线推荐和优化交通工具选择。


<details>
  <summary>Details</summary>
Motivation: 推动智能出行和共享电子出行服务发展，利用先进技术提高交通效率和用户体验。

Method: 构建一个基于云的共享电子出行平台，集成LLM与移动应用，用于个性化路线推荐及交通状况优化。采用XiYanSQL对LLM驱动的RAG框架进行系统与用户的查询测试。

Result: 实验评估表明，优化模块在不同交通场景下表现良好，LLM驱动的RAG在系统查询中执行准确率为0.81，用户查询为0.98。

Conclusion: 基于LLM的云共享出行平台显示了其为用户提供个性化服务和优化交通体验的潜力，未来可进一步提升准确率与用户体验。

Abstract: With the rise of smart mobility and shared e-mobility services, numerous
advanced technologies have been applied to this field. Cloud-based traffic
simulation solutions have flourished, offering increasingly realistic
representations of the evolving mobility landscape. LLMs have emerged as
pioneering tools, providing robust support for various applications, including
intelligent decision-making, user interaction, and real-time traffic analysis.
As user demand for e-mobility continues to grow, delivering comprehensive
end-to-end solutions has become crucial. In this paper, we present a
cloud-based, LLM-powered shared e-mobility platform, integrated with a mobile
application for personalized route recommendations. The optimization module is
evaluated based on travel time and cost across different traffic scenarios.
Additionally, the LLM-powered RAG framework is evaluated at the schema level
for different users, using various evaluation methods. Schema-level RAG with
XiYanSQL achieves an average execution accuracy of 0.81 on system operator
queries and 0.98 on user queries.

</details>


### [416] [Extracting Important Tokens in E-Commerce Queries with a Tag Interaction-Aware Transformer Model](https://arxiv.org/abs/2507.10385)
*Md. Ahsanul Kabir,Mohammad Al Hasan,Aritra Mandal,Liyang Hao,Ishita Khan,Daniel Tunkelang,Zhe Wu*

Main category: cs.LG

TL;DR: 提出了一种名为TagBERT的模型，通过使用语义标签改进电商查询重构，并在实际数据集上表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决电商搜索中用户查询和商品间语义缺口的问题。

Method: 将查询重构任务转化为令牌分类问题，并设计了一个依赖感知的基于Transformer模型TagBERT，利用令牌的语义标签来学习更好的查询短语嵌入。

Result: 在大规模电商数据集上的实验显示，TagBERT在重要的令牌分类任务中优于多个对比模型，包括BERT、eBERT及序列到序列Transformer。

Conclusion: TagBERT通过结合语义标签更好地捕捉了用户意图，在电商查询重构任务中表现出色。

Abstract: The major task of any e-commerce search engine is to retrieve the most
relevant inventory items, which best match the user intent reflected in a
query. This task is non-trivial due to many reasons, including ambiguous
queries, misaligned vocabulary between buyers, and sellers, over- or
under-constrained queries by the presence of too many or too few tokens. To
address these challenges, query reformulation is used, which modifies a user
query through token dropping, replacement or expansion, with the objective to
bridge semantic gap between query tokens and users' search intent. Early
methods of query reformulation mostly used statistical measures derived from
token co-occurrence frequencies from selective user sessions having clicks or
purchases. In recent years, supervised deep learning approaches, specifically
transformer-based neural language models, or sequence-to-sequence models are
being used for query reformulation task. However, these models do not utilize
the semantic tags of a query token, which are significant for capturing user
intent of an e-commerce query. In this work, we pose query reformulation as a
token classification task, and solve this task by designing a dependency-aware
transformer-based language model, TagBERT, which makes use of semantic tags of
a token for learning superior query phrase embedding. Experiments on large,
real-life e-commerce datasets show that TagBERT exhibits superior performance
than plethora of competing models, including BERT, eBERT, and
Sequence-to-Sequence transformer model for important token classification task.

</details>


### [417] [Anticipating the Selectivity of Cyclization Reaction Pathways with Neural Network Potentials](https://arxiv.org/abs/2507.10400)
*Nicholas Casetti,Dylan Anstine,Olexandr Isayev,Connor W. Coley*

Main category: cs.LG

TL;DR: 本文提出了一种针对复杂环化反应的机制搜索策略，结合图基枚举和机器学习技术用于反应路径探索并使用神经网络潜力AIMNet2-rxn估算能量。


<details>
  <summary>Details</summary>
Motivation: 针对自然产物合成中涉及复杂共轭键变化的环化反应现象，现有机制搜索工具面对多键变化路径计算复杂性的问题，需要改进搜索策略。

Method: 提出了一种结合图形化枚举方案和机器学习中间体筛选的机制搜索策略，利用神经网络潜力（AIMNet2-rxn）计算候选反应路径的激活能。

Result: 方法评估显示神经网络潜力能够估算激活能，正确预测立体选择性，重现自然产物合成中的复杂关键步骤。

Conclusion: 所提出的搜索策略简化并加速了复杂环化反应的机制探索，对自然产物合成的研究具有实用价值。

Abstract: Reaction mechanism search tools have demonstrated the ability to provide
insights into likely products and rate-limiting steps of reacting systems.
However, reactions involving several concerted bond changes - as can be found
in many key steps of natural product synthesis - can complicate the search
process. To mitigate these complications, we present a mechanism search
strategy particularly suited to help expedite exploration of an exemplary
family of such complex reactions, cyclizations. We provide a cost-effective
strategy for identifying relevant elementary reaction steps by combining
graph-based enumeration schemes and machine learning techniques for
intermediate filtering. Key to this approach is our use of a neural network
potential (NNP), AIMNet2-rxn, for computational evaluation of each candidate
reaction pathway. In this article, we evaluate the NNP's ability to estimate
activation energies, demonstrate the correct anticipation of stereoselectivity,
and recapitulate complex enabling steps in natural product synthesis.

</details>


### [418] [Stochastic Operator Network: A Stochastic Maximum Principle Based Approach to Operator Learning](https://arxiv.org/abs/2507.10401)
*Ryan Bausback,Jingqiao Tang,Lu Lu,Feng Bao,Toan Huynh*

Main category: cs.LG

TL;DR: 提出一种新的框架Stochastic Operator Network (SON)，用于算子学习中的不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 解决算子学习中不确定性量化的问题。

Method: 将随机神经网络(SNN)的随机最优控制理念与DeepONet结合，通过将分支网络设为随机微分方程(SDE)，利用邻接BSDE反向传播，用随机最优控制中的Hamiltonian梯度替代损失函数梯度。

Result: 证明SON在复现二维和三维中多个噪声算子中的有效性。

Conclusion: SON通过其扩散参数能够学习算子中的不确定性并在多个维度上表现出色。

Abstract: We develop a novel framework for uncertainty quantification in operator
learning, the Stochastic Operator Network (SON). SON combines the stochastic
optimal control concepts of the Stochastic Neural Network (SNN) with the
DeepONet. By formulating the branch net as an SDE and backpropagating through
the adjoint BSDE, we replace the gradient of the loss function with the
gradient of the Hamiltonian from Stohastic Maximum Principle in the SGD update.
This allows SON to learn the uncertainty present in operators through its
diffusion parameters. We then demonstrate the effectiveness of SON when
replicating several noisy operators in 2D and 3D.

</details>


### [419] [Energy Efficiency in AI for 5G and Beyond: A DeepRx Case Study](https://arxiv.org/abs/2507.10409)
*Amine Lbath,Ibtissam Labriji*

Main category: cs.LG

TL;DR: 本文研究了DeepRX，一种基于ResNet网络的深度学习接收器，着眼于在AI/ML模型中平衡能效与性能，并探索了通过知识蒸馏训练能效更高的模型。


<details>
  <summary>Details</summary>
Motivation: 设计一种在不显著降低性能的情况下具有更高能效的AI/ML模型，以满足现代计算系统的能源使用需求。

Method: 评估DeepRX的能耗特性，包括FLOPs/Watt和FLOPs/clock等指标；应用知识蒸馏方法训练更小的DeepRX学生模型，并优化师生模型的参数配置。

Result: 蒸馏后的学生模型在SINR水平上表现出更低的错误底限（Error Floor）。

Conclusion: 知识蒸馏是一种有效的方法，可以在保持模型性能的前提下，显著降低AI/ML模型的能耗。

Abstract: This study addresses the challenge of balancing energy efficiency with
performance in AI/ML models, focusing on DeepRX, a deep learning receiver based
on a fully convolutional ResNet architecture. We evaluate the energy
consumption of DeepRX, considering factors including FLOPs/Watt and
FLOPs/clock, and find consistency between estimated and actual energy usage,
influenced by memory access patterns. The research extends to comparing energy
dynamics during training and inference phases. A key contribution is the
application of knowledge distillation (KD) to train a compact DeepRX
\textit{student} model that emulates the performance of the \textit{teacher}
model but with reduced energy consumption. We experiment with different student
model sizes, optimal teacher sizes, and KD hyperparameters. Performance is
measured by comparing the Bit Error Rate (BER) performance versus
Signal-to-Interference \& Noise Ratio (SINR) values of the distilled model and
a model trained from scratch. The distilled models demonstrate a lower error
floor across SINR levels, highlighting the effectiveness of KD in achieving
energy-efficient AI solutions.

</details>


### [420] [Non-exchangeable Conformal Prediction with Optimal Transport: Tackling Distribution Shifts with Unlabeled Data](https://arxiv.org/abs/2507.10425)
*Alvaro H. C. Correia,Christos Louizos*

Main category: cs.LG

TL;DR: 研究以保序变换的角度解决分布转移下保准预测的泛化误差问题，通过估算覆盖率损失进行调整。


<details>
  <summary>Details</summary>
Motivation: 分布自由的保准预测方法在分布转移场景中难以维护覆盖率，需要新的方法应对这一常见问题。

Method: 通过引入最优传输角度，提出估计覆盖损失并在分布转移场景下进行调整的方法。

Result: 在分布转移场景下能够有效估算和缓解覆盖率损失，验证了最优传输的有效性。

Conclusion: 利用最优传输新视角，该方法能够对分布转移情景下的覆盖率损失做出一定补偿。

Abstract: Conformal prediction is a distribution-free uncertainty quantification method
that has gained popularity in the machine learning community due to its
finite-sample guarantees and ease of use. Its most common variant, dubbed split
conformal prediction, is also computationally efficient as it boils down to
collecting statistics of the model predictions on some calibration data not yet
seen by the model. Nonetheless, these guarantees only hold if the calibration
and test data are exchangeable, a condition that is difficult to verify and
often violated in practice due to so-called distribution shifts. The literature
is rife with methods to mitigate the loss in coverage in this non-exchangeable
setting, but these methods require some prior information on the type of
distribution shift to be expected at test time. In this work, we study this
problem via a new perspective, through the lens of optimal transport, and show
that it is possible to estimate the loss in coverage and mitigate it in case of
distribution shift.

</details>


### [421] [CLA: Latent Alignment for Online Continual Self-Supervised Learning](https://arxiv.org/abs/2507.10434)
*Giacomo Cignoni,Andrea Cossu,Alexandra Gomez-Villa,Joost van de Weijer,Antonio Carta*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的自监督学习方法，用于在线持续学习设置下生成一致的潜在表示，从而提高模型性能并减少遗忘。


<details>
  <summary>Details</summary>
Motivation: 在线持续学习需要在小型数据批次下持续学习，并且没有明确的任务边界，同时还需满足固定的计算预算，但现有的自监督学习方法很少适用于此场景。

Method: 提出了一种称为连续潜在对齐（CLA）的自监督学习策略，通过将当前模型的表示与过去的表示对齐，从而减轻遗忘并加速训练收敛。

Result: CLA在在线场景下训练收敛速度比现有最前沿方法更快，同时在固定计算预算下表现出色。此外，CLA作为预训练协议能够在预训练早期阶段提升最终性能。

Conclusion: CLA在在线持续学习情境下是一种高效的自监督学习策略，可加快训练过程并提升模型性能，同时展现出作为早期预训练方法的潜力。

Abstract: Self-supervised learning (SSL) is able to build latent representations that
generalize well to unseen data. However, only a few SSL techniques exist for
the online CL setting, where data arrives in small minibatches, the model must
comply with a fixed computational budget, and task boundaries are absent. We
introduce Continual Latent Alignment (CLA), a novel SSL strategy for Online CL
that aligns the representations learned by the current model with past
representations to mitigate forgetting. We found that our CLA is able to speed
up the convergence of the training process in the online scenario,
outperforming state-of-the-art approaches under the same computational budget.
Surprisingly, we also discovered that using CLA as a pretraining protocol in
the early stages of pretraining leads to a better final performance when
compared to a full i.i.d. pretraining.

</details>


### [422] [Response Wide Shut? Surprising Observations in Basic Vision Language Model Capabilities](https://arxiv.org/abs/2507.10442)
*Shivam Chandhok,Wan-Cyuan Fan,Vered Shwartz,Vineeth N Balasubramanian,Leonid Sigal*

Main category: cs.LG

TL;DR: 该研究分析了当前视觉语言模型（VLMs）的基本视觉任务能力，并揭示其局限性和改进方向。


<details>
  <summary>Details</summary>
Motivation: 理解当前最先进视觉语言模型在基本视觉任务上的局限性，分析其设计中缺失的关键组件。

Method: 通过设计一系列测试，深入比较了视觉编码器、视觉语言投影中间层及语言模型解码器的输出特性，揭示模型的局限。

Result: 发现了现有视觉语言模型在处理视觉信息方面的不足，并得出了其能力、鲁棒性和处理信息方式的关键观察。

Conclusion: 研究为提升视觉语言模型的性能提供了新的见解和改进方向。

Abstract: Vision-language Models (VLMs) have emerged as general-purpose tools for
addressing a variety of complex computer vision problems. Such models have been
shown to be highly capable, but, at the same time, lacking some basic visual
understanding skills. In this paper, we set out to understand the limitations
of SoTA VLMs on fundamental visual tasks by constructing a series of tests that
probe which components of design, specifically, may be lacking. Importantly, we
go significantly beyond the current benchmarks, which simply measure the final
performance of VLM response, by also comparing and contrasting it to the
performance of probes trained directly on features obtained from the visual
encoder, intermediate vision-language projection and LLM-decoder output. In
doing so, we uncover shortcomings in VLMs and make a number of important
observations about their capabilities, robustness and how they process visual
information. We hope our insights will guide progress in further improving
VLMs.

</details>


### [423] [Some remarks on gradient dominance and LQR policy optimization](https://arxiv.org/abs/2507.10452)
*Eduardo D. Sontag*

Main category: cs.LG

TL;DR: 本文探讨了优化问题（包括强化学习中的策略优化）中梯度下降算法的收敛性，描述了反馈控制中线性前馈神经网络的收敛性及相关问题。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习和优化问题中的收敛性研究表明，在连续时间LQR问题中，初始条件较大会导致收敛速率减慢，与离散时间LQR问题中的全局指数收敛性形成鲜明对比，从而需要对更广泛的一般化Polyak-{65-explorationPLI-like condition的探索进行动机设计。

Method: 结合Polyak-Lojasiewicz不等式，进行渐近分析方法，并引入“状态到输入稳定性”分析，探索具体算法中的暂时误差和渐近误差，特别关注受限数据或存在随机因素的学习。

Result: 研究结果表明，通过重新定义和广义的PLI性质，可更好地理解扰动状态下强化学习问题的全局与局部收敛性之间的关系，并补充了于反馈控制中线性前向神经网络的探索性讨论。

Conclusion: 本文的结果为解决连续与离散时间之间的收敛差异提供了新的理论基础，并有助于改进优化算法的鲁棒性及适应性。

Abstract: Solutions of optimization problems, including policy optimization in
reinforcement learning, typically rely upon some variant of gradient descent.
There has been much recent work in the machine learning, control, and
optimization communities applying the Polyak-{\L}ojasiewicz Inequality (PLI) to
such problems in order to establish an exponential rate of convergence (a.k.a.
``linear convergence'' in the local-iteration language of numerical analysis)
of loss functions to their minima under the gradient flow. Often, as is the
case of policy iteration for the continuous-time LQR problem, this rate
vanishes for large initial conditions, resulting in a mixed globally linear /
locally exponential behavior. This is in sharp contrast with the discrete-time
LQR problem, where there is global exponential convergence. That gap between CT
and DT behaviors motivates the search for various generalized PLI-like
conditions, and this talk will address that topic. Moreover, these
generalizations are key to understanding the transient and asymptotic effects
of errors in the estimation of the gradient, errors which might arise from
adversarial attacks, wrong evaluation by an oracle, early stopping of a
simulation, inaccurate and very approximate digital twins, stochastic
computations (algorithm ``reproducibility''), or learning by sampling from
limited data. We describe an ``input to state stability'' (ISS) analysis of
this issue. The lecture also discussed convergence and PLI-like properties of
``linear feedforward neural networks'' in feedback control, but this arXiv
skips that part (to be updated). Much of the work described here was done in
collaboration with Arthur Castello B. de Oliveira, Leilei Cui, Zhong-Ping
Jiang, and Milad Siami.

</details>


### [424] [The Target Polish: A New Approach to Outlier-Resistant Non-Negative Matrix and Tensor Factorization](https://arxiv.org/abs/2507.10484)
*Paul Fogel,Christophe Geissler,George Luta*

Main category: cs.LG

TL;DR: 提出了一种名为“Target Polish”的高效工具，用于非负矩阵和张量分解，在提高抗异常值能力的同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有加权非负矩阵分解方法抗异常值能力强，但由于使用乘法更新导致收敛速度较慢。

Method: 通过采用基于加权中值的转换对数据进行自适应平滑，同时兼容Fast-HALS算法中高效的加法更新结构。

Result: 实验表明，该方法在抗异常值能力上匹配或超越最先进的鲁棒非负矩阵分解方法，并显著减少计算时间达一个数量级。

Conclusion: 通过引入Target Polish方法，实现了鲁棒性和计算效率的兼得，在包含噪声的数据集上展现了卓越性能。

Abstract: This paper introduces the "Target Polish," a robust and computationally
efficient framework for nonnegative matrix and tensor factorization. Although
conventional weighted NMF approaches are resistant to outliers, they converge
slowly due to the use of multiplicative updates to minimize the objective
criterion. In contrast, the Target Polish approach remains compatible with the
Fast-HALS algorithm, which is renowned for its speed, by adaptively smoothing
the data with a weighted median-based transformation. This innovation provides
outlier resistance while maintaining the highly efficient additive update
structure of Fast-HALS. Empirical evaluations using image datasets corrupted
with structured (block) and unstructured (salt) noise demonstrate that the
Target Polish approach matches or exceeds the accuracy of state-of-the-art
robust NMF methods and reduces computational time by an order of magnitude in
the studied scenarios.

</details>


### [425] [Overcoming catastrophic forgetting in neural networks](https://arxiv.org/abs/2507.10485)
*Brandon Shuen Yi Loke,Filippo Quadri,Gabriel Vivanco,Maximilian Casagrande,Saúl Fenollosa*

Main category: cs.LG

TL;DR: 论文探讨了使用弹性权重巩固（EWC）减少神经网络灾难性遗忘的问题，结果表明EWC在保留知识方面优于传统训练方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决神经网络在持续学习中出现的灾难性遗忘问题，提高其在多任务学习中的表现。

Method: 通过在PermutedMNIST和RotatedMNIST基准测试中系统性地评估EWC方法，并与L2正则化和未加正则化的SGD进行比较，研究其对知识维持与适应能力之间的平衡。

Result: 实验结果表明，EWC显著减少了灾难性遗忘，但对新任务的学习效率存在一定妥协，同时在超参数调整和dropout正则化的影响方面提供了新见解。

Conclusion: EWC是一种潜在的可行解决方案，可用于实现神经网络的持续学习。

Abstract: Catastrophic forgetting is the primary challenge that hinders continual
learning, which refers to a neural network ability to sequentially learn
multiple tasks while retaining previously acquired knowledge. Elastic Weight
Consolidation, a regularization-based approach inspired by synaptic
consolidation in biological neural systems, has been used to overcome this
problem. In this study prior research is replicated and extended by evaluating
EWC in supervised learning settings using the PermutedMNIST and RotatedMNIST
benchmarks. Through systematic comparisons with L2 regularization and
stochastic gradient descent (SGD) without regularization, we analyze how
different approaches balance knowledge retention and adaptability. Our results
confirm what was shown in previous research, showing that EWC significantly
reduces forgetting compared to naive training while slightly compromising
learning efficiency on new tasks. Moreover, we investigate the impact of
dropout regularization and varying hyperparameters, offering insights into the
generalization of EWC across diverse learning scenarios. These results
underscore EWC's potential as a viable solution for lifelong learning in neural
networks.

</details>


### [426] [Split Happens: Combating Advanced Threats with Split Learning and Function Secret Sharing](https://arxiv.org/abs/2507.10494)
*Tanveer Khan,Mindaugas Budzys,Antonis Michalas*

Main category: cs.LG

TL;DR: 本研究改进了Split Learning(SL)的隐私保护能力，与功能秘密共享(FSS)结合，提出一种新方法针对U型SL，提高了数据标签的隐私保护能力，与现有方案相比提升了安全性。


<details>
  <summary>Details</summary>
Motivation: 传统的分割学习方法虽然在隐私保护和机器学习整合方面具有潜力，但面临多种攻击的隐患，亟需更安全的方法保障数据隐私。

Method: 将功能秘密共享(FSS)技术与U型分割学习(SL)结合，允许客户端隐藏训练数据标签，避免将其暴露给服务器，同时降低FSS的通信和计算成本。

Result: 通过对不同的卷积神经网络和数据集实验，验证了新方法能够在提高安全性的同时，减少训练时间和通信成本，并与过去方法在准确性上匹配。

Conclusion: U型SL结合FSS的方法不仅提高了分割学习的隐私保护能力，还有效扩展了对抗攻击的适用性，为未来隐私保护型机器学习提供了新的解决方案。

Abstract: Split Learning (SL) -- splits a model into two distinct parts to help protect
client data while enhancing Machine Learning (ML) processes. Though promising,
SL has proven vulnerable to different attacks, thus raising concerns about how
effective it may be in terms of data privacy. Recent works have shown promising
results for securing SL through the use of a novel paradigm, named Function
Secret Sharing (FSS), in which servers obtain shares of a function they compute
and operate on a public input hidden with a random mask. However, these works
fall short in addressing the rising number of attacks which exist on SL. In
SplitHappens, we expand the combination of FSS and SL to U-shaped SL. Similarly
to other works, we are able to make use of the benefits of SL by reducing the
communication and computational costs of FSS. However, a U-shaped SL provides a
higher security guarantee than previous works, allowing a client to keep the
labels of the training data secret, without having to share them with the
server. Through this, we are able to generalize the security analysis of
previous works and expand it to different attack vectors, such as modern model
inversion attacks as well as label inference attacks. We tested our approach
for two different convolutional neural networks on different datasets. These
experiments show the effectiveness of our approach in reducing the training
time as well as the communication costs when compared to simply using FSS while
matching prior accuracy.

</details>


### [427] [Benchmarking and Evaluation of AI Models in Biology: Outcomes and Recommendations from the CZI Virtual Cells Workshop](https://arxiv.org/abs/2507.10502)
*Elizabeth Fahsbender,Alma Andersson,Jeremy Ash,Polina Binder,Daniel Burkhardt,Benjamin Chang,Georg K. Gerber,Anthony Gitter,Patrick Godau,Ankit Gupta,Genevieve Haliburton,Siyu He,Trey Ideker,Ivana Jelic,Aly Khan,Yang-Joon Kim,Aditi Krishnapriyan,Jon M. Laurent,Tianyu Liu 28,Emma Lundberg,Shalin B. Mehta,Rob Moccia,Angela Oliveira Pisco,Katherine S. Pollard,Suresh Ramani,Julio Saez-Rodriguez,Yasin Senbabaoglu,Elana Simon,Srinivasan Sivanandan,Gustavo Stolovitzky,Marc Valer,Bo Wang,Xikun Zhang,James Zou,Katrina Kalantar*

Main category: cs.LG

TL;DR: 人工智能在生物学领域具有巨大潜力，但缺乏标准化的跨领域基准。本文总结了一个汇聚机器学习和计算生物学专家的研讨会成果，提出了应对数据异质性、噪声、再现性以及资源分散等问题的建议。


<details>
  <summary>Details</summary>
Motivation: 解决当前生物学领域人工智能模型面临的标准化、跨领域基准缺失的问题，以便提升模型的鲁棒性和可信度。

Method: 通过召开研讨会，集结影像学、转录组学、蛋白质组学和基因组学专家，辨识主要技术与体系瓶颈，并提出构建基准框架的建议，包括高质量数据整理、标准化工具、全面评估指标及开放协作平台。

Result: 提出了一套推荐方案，旨在促进基准框架的开发，为AI驱动的虚拟细胞模型提供支持。

Conclusion: 这些基准框架对于确保科学的严格性、再现性和生物关联性至关重要，将推动集成模型发展，助力新发现、疗法见解及细胞系统的深入理解。

Abstract: Artificial intelligence holds immense promise for transforming biology, yet a
lack of standardized, cross domain, benchmarks undermines our ability to build
robust, trustworthy models. Here, we present insights from a recent workshop
that convened machine learning and computational biology experts across
imaging, transcriptomics, proteomics, and genomics to tackle this gap. We
identify major technical and systemic bottlenecks such as data heterogeneity
and noise, reproducibility challenges, biases, and the fragmented ecosystem of
publicly available resources and propose a set of recommendations for building
benchmarking frameworks that can efficiently compare ML models of biological
systems across tasks and data modalities. By promoting high quality data
curation, standardized tooling, comprehensive evaluation metrics, and open,
collaborative platforms, we aim to accelerate the development of robust
benchmarks for AI driven Virtual Cells. These benchmarks are crucial for
ensuring rigor, reproducibility, and biological relevance, and will ultimately
advance the field toward integrated models that drive new discoveries,
therapeutic insights, and a deeper understanding of cellular systems.

</details>


### [428] [On the Performance of Differentially Private Optimization with Heavy-Tail Class Imbalance](https://arxiv.org/abs/2507.10536)
*Qiaoyue Tang,Alain Zhiyanov,Mathias Lécuyer*

Main category: cs.LG

TL;DR: 本文分析了差分隐私优化算法在重尾类别不平衡分布下的行为，发现DP-AdamBC相比DP-GD更适应低频类别学习。


<details>
  <summary>Details</summary>
Motivation: 研究差分隐私优化算法在类别不平衡分布下的性能，特别是提升低频类别学习的效果。

Method: 通过理论模型分析，以及控制实验和实际数据实验，比较传统DP-GD与修正的DP-AdamBC算法表现。

Result: 实验表明，DP-AdamBC在学习低频类别时，在训练准确率上相较DP-GD分别提高了约8%和5%。

Conclusion: DP-AdamBC通过消除对损失曲率估计的差分隐私偏差，有助于应对重尾类别不平衡问题，更适合低频类别的学习。

Abstract: In this work, we analyze the optimization behaviour of common private
learning optimization algorithms under heavy-tail class imbalanced
distribution. We show that, in a stylized model, optimizing with Gradient
Descent with differential privacy (DP-GD) suffers when learning low-frequency
classes, whereas optimization algorithms that estimate second-order information
do not. In particular, DP-AdamBC that removes the DP bias from estimating loss
curvature is a crucial component to avoid the ill-condition caused by
heavy-tail class imbalance, and empirically fits the data better with
$\approx8\%$ and $\approx5\%$ increase in training accuracy when learning the
least frequent classes on both controlled experiments and real data
respectively.

</details>


### [429] [Graph World Model](https://arxiv.org/abs/2507.10539)
*Tao Feng,Yexin Wu,Guanyu Lin,Jiaxuan You*

Main category: cs.LG

TL;DR: 提出了一种新的图世界模型（GWM），兼容非结构化和图结构化数据，对多个领域任务表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有模型不能有效处理图结构化数据和多模态任务的挑战。

Method: 通过信息传递算法与多模态编码器统一数据表征，并引入动作节点支持多领域任务。

Result: 在六个多领域任务上超越或媲美领域专用方法，展现了强大的零样本/少样本能力。

Conclusion: GWM具备广泛适用性和强大性能，适合多样化领域，多模态任务处理发展潜力强大。

Abstract: World models (WMs) demonstrate strong capabilities in prediction, generation,
and planning tasks. Existing WMs primarily focus on unstructured data and
cannot leverage the ubiquitous structured data, often represented as graphs, in
the digital world. While multiple graph foundation models have been proposed,
they focus on graph learning tasks and cannot extend to diverse multi-modal
data and interdisciplinary tasks. To address these challenges, we propose the
Graph World Model (GWM), a world model that supports both unstructured and
graph-structured states with multi-modal information and represents diverse
tasks as actions. The core of a GWM is a generic message-passing algorithm to
aggregate structured information, either over a unified multi-modal token space
by converting multi-modal data into text (GWM-T) or a unified multi-modal
embedding space by modality-specific encoders (GWM-E). Notably, GWM introduces
action nodes to support diverse tasks, where action nodes are linked to other
nodes via direct reference or similarity computation. Extensive experiments on
six tasks from diverse domains, including multi-modal generation and matching,
recommendation, graph prediction, multi-agent, retrieval-augmented generation,
and planning and optimization, show that the same GWM outperforms or matches
domain-specific baselines' performance, benefits from multi-hop structures, and
demonstrates strong zero-shot/few-shot capabilities on unseen new tasks. Our
code for GWM is released at https://github.com/ulab-uiuc/GWM.

</details>


### [430] [Fusing LLM Capabilities with Routing Data](https://arxiv.org/abs/2507.10540)
*Tao Feng,Haozhen Zhang,Zijie Lei,Pengrui Han,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro,Jiaxuan You*

Main category: cs.LG

TL;DR: 文章提出将多种大型语言模型（LLMs）结合使用以优化性能与资源效率，介绍了一个基准工具和整合框架。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖单一模型，限制了功能覆盖，且在处理复杂任务时效率不高。

Method: 提出FusionBench和FusionFactory，其中FusionFactory包含查询级、思维级和模型级三个整合方法，对不同LLMs的能力进行融合与优化。

Result: FusionFactory在14项基准测试中显著优于单一最佳LLM，显示了多模型整合的优越性。

Conclusion: 系统化的LLMs融合策略具备提升性能和利用模型互补性的潜力。

Abstract: The rapid advancement of large language models (LLMs) has created a vibrant
ecosystem of diverse architectures, each with unique strengths due to
differences in design, training data, and objectives. However, most
applications still rely on a single backend model, limiting coverage of
capabilities and leading to inefficiencies in performance and token cost when
tackling complex tasks. We highlight an underexploited opportunity: LLM routing
data, produced when hosting platforms route diverse queries to different
models, which can reveal comparative strengths across tasks. To address this,
we propose FusionBench, a comprehensive routing benchmark covering 14 tasks
across five domains with 20 open-source LLMs (8B to 671B parameters), capturing
103M tokens and summarizing reusable thought templates from top models.
Building on this, we introduce FusionFactory, a systematic fusion framework
with three levels: (1) query-level fusion, tailoring routers for each query
using both direct responses and reasoning-augmented outputs; (2) thought-level
fusion, leveraging abstract templates derived from top-performing LLMs' answers
to similar queries; and (3) model-level fusion, transferring capabilities
between models via distillation, using top responses or highest judge scores as
training data. Experiments show FusionFactory consistently outperforms the best
individual LLM across all 14 benchmarks, with optimal fusion configurations
varying by benchmark, demonstrating the value of systematic LLM fusion in
harnessing complementary strengths and improving overall performance.

</details>


### [431] [Disentangling Neural Disjunctive Normal Form Models](https://arxiv.org/abs/2507.10546)
*Kexin Gu Baugh,Vincent Perreault,Matthew Baugh,Luke Dickens,Katsumi Inoue,Alessandra Russo*

Main category: cs.LG

TL;DR: 论文提出了一种新的方法，通过分割网络中的节点以应对符号翻译过程中的性能退化问题，并展示了其在分类任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于神经DNF的模型在符号翻译过程中由于无法有效解开权重中的知识编码而导致性能下降。

Method: 提出了一种新的解耦方法，通过分割编码嵌套规则的节点为更小的独立节点，从而更好地保留模型性能。

Result: 通过在各种分类任务上的实验，验证了所提出方法能够提供紧凑且可解释的逻辑表示，同时性能更接近翻译前的模型。

Conclusion: 研究证明了新的解耦方法在提升模型性能和解释性中具有显著效果，为基于神经DNF的模型提供了改进方向。

Abstract: Neural Disjunctive Normal Form (DNF) based models are powerful and
interpretable approaches to neuro-symbolic learning and have shown promising
results in classification and reinforcement learning settings without prior
knowledge of the tasks. However, their performance is degraded by the
thresholding of the post-training symbolic translation process. We show here
that part of the performance degradation during translation is due to its
failure to disentangle the learned knowledge represented in the form of the
networks' weights. We address this issue by proposing a new disentanglement
method; by splitting nodes that encode nested rules into smaller independent
nodes, we are able to better preserve the models' performance. Through
experiments on binary, multiclass, and multilabel classification tasks
(including those requiring predicate invention), we demonstrate that our
disentanglement method provides compact and interpretable logical
representations for the neural DNF-based models, with performance closer to
that of their pre-translation counterparts. Our code is available at
https://github.com/kittykg/disentangling-ndnf-classification.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [432] [Credit Card Fraud Detection Using RoFormer Model With Relative Distance Rotating Encoding](https://arxiv.org/abs/2507.09385)
*Kevin Reyes,Vasco Cortez*

Main category: cs.NE

TL;DR: 本文提出了一种在 RoFormer 模型中加入相对距离旋转编码 (ReDRE) 的新方法，用于改进交易型欺诈检测。


<details>
  <summary>Details</summary>
Motivation: 金融系统面临重大挑战，即需要检测欺诈性交易。尤其对于支付网关公司，例如 Flow Payment，提高授权率、增强安全性并降低欺诈率对用户体验和业务的可持续发展至关重要。

Method: 通过将相对距离旋转编码（ReDRE）方法引入 RoFormer 模型，强化时间序列数据的表示能力，改进了 Transformer 模型对欺诈检测中的时间依赖性及事件关系的捕捉能力。

Result: 模型表现出更有效的检测能力，能够更好地捕获金融交易数据中的欺诈特征。

Conclusion: 本文方法在改善欺诈检测表现方面具有潜力，对持续研究和投资于金融系统内安全性问题具有意义。

Abstract: Fraud detection is one of the most important challenges that financial
systems must address. Detecting fraudulent transactions is critical for payment
gateway companies like Flow Payment, which process millions of transactions
monthly and require robust security measures to mitigate financial risks.
Increasing transaction authorization rates while reducing fraud is essential
for providing a good user experience and building a sustainable business. For
this reason, discovering novel and improved methods to detect fraud requires
continuous research and investment for any company that wants to succeed in
this industry. In this work, we introduced a novel method for detecting
transactional fraud by incorporating the Relative Distance Rotating Encoding
(ReDRE) in the RoFormer model. The incorporation of angle rotation using ReDRE
enhances the characterization of time series data within a Transformer, leading
to improved fraud detection by better capturing temporal dependencies and event
relationships.

</details>


### [433] [BrainFLORA: Uncovering Brain Concept Representation via Multimodal Neural Embeddings](https://arxiv.org/abs/2507.09747)
*Dongyang Li,Haoyang Qin,Mingyang Wu,Chen Wei,Quanying Liu*

Main category: cs.NE

TL;DR: BrainFLORA通过整合跨模态神经影像数据，构建统一的神经表示框架，实现了先进的视觉检索任务表现。


<details>
  <summary>Details</summary>
Motivation: 研究脑部如何表征视觉信息，藉由整合不同模态的神经影像数据克服时空错位的问题。

Method: 开发BrainFLORA框架，使用多模态大语言模型(MLLMs)，结合模态特定适配器和任务解码器，与神经影像分析方法集成。

Result: 在跨主体视觉检索任务上获得了最先进性能，并展示了视觉概念的神经表示如何与现实世界的物体感知一致。

Conclusion: BrainFLORA推动了脑科学与机器学习的跨模态研究，具有认知神经科学和脑机接口的新潜力。

Abstract: Understanding how the brain represents visual information is a fundamental
challenge in neuroscience and artificial intelligence. While AI-driven decoding
of neural data has provided insights into the human visual system, integrating
multimodal neuroimaging signals, such as EEG, MEG, and fMRI, remains a critical
hurdle due to their inherent spatiotemporal misalignment. Current approaches
often analyze these modalities in isolation, limiting a holistic view of neural
representation. In this study, we introduce BrainFLORA, a unified framework for
integrating cross-modal neuroimaging data to construct a shared neural
representation. Our approach leverages multimodal large language models (MLLMs)
augmented with modality-specific adapters and task decoders, achieving
state-of-the-art performance in joint-subject visual retrieval task and has the
potential to extend multitasking. Combining neuroimaging analysis methods, we
further reveal how visual concept representations align across neural
modalities and with real world object perception. We demonstrate that the
brain's structured visual concept representations exhibit an implicit mapping
to physical-world stimuli, bridging neuroscience and machine learning from
different modalities of neural imaging. Beyond methodological advancements,
BrainFLORA offers novel implications for cognitive neuroscience and
brain-computer interfaces (BCIs). Our code is available at
https://github.com/ncclab-sustech/BrainFLORA.

</details>


### [434] [Effective Self-Attention-Based Deep Learning Model with Evolutionary Grid Search for Robust Wave Farm Energy Forecasting](https://arxiv.org/abs/2507.09847)
*Amin Abdollahi Dehkordi,Mehdi Neshat,Nataliia Y. Sergiienko,Zahra Ghasemi,Lei Chen,John Boland,Hamid Moradkhani,Amir H. Gandomi*

Main category: cs.NE

TL;DR: 本文探讨通过新型预测框架来促进波浪能的并网，提出结合自注意力增强卷积Bi-LSTM的混合序列学习模型，显著提高预测精度。


<details>
  <summary>Details</summary>
Motivation: 碳中和推动清洁能源探索，波浪能具有巨大潜力，但因技术和经济限制未被充分开发，尤其是波浪能场功率预测对电网稳定至关重要。

Method: 提出一种融合自注意力增强卷积Bi-LSTM和超参数优化的混合序列学习模型，利用波浪能转换器的空间数据进行建模和预测验证。

Result: 模型在澳大利亚多个波浪能农场的验证集上R2得分达到91.7%(阿德莱德)、88.0%(珀斯)、82.8%(塔斯马尼亚)和91.0%(悉尼)，效果优于传统机器学习和深度学习方法。

Conclusion: 该模型提供了在不同海洋环境中对波浪能输出进行预测的强大工具，有助于波浪能可靠接入能源系统。

Abstract: Achieving carbon neutrality, a key focus of UN SDG #13, drives the
exploration of wave energy, a renewable resource with the potential to generate
30,000 TWh of clean electricity annually, surpassing global demand. However,
wave energy remains underdeveloped due to technical and economic challenges,
particularly in forecasting wave farm power output, which is vital for grid
stability and commercial viability. This study proposes a novel predictive
framework to enhance wave energy integration into power grids. It introduces a
hybrid sequential learning model combining Self-Attention-enhanced
Convolutional Bi-LSTM with hyperparameter optimization. The model leverages
spatial data from Wave Energy Converters (WECs) and is validated using datasets
from wave farms in Adelaide, Sydney, Perth, and Tasmania, Australia.
Benchmarked against ten machine learning algorithms, the model achieves
superior accuracy, with R2 scores of 91.7% (Adelaide), 88.0% (Perth), 82.8%
(Tasmania), and 91.0% (Sydney). It outperforms conventional ML and deep
learning methods, offering robust and scalable predictions for wave energy
output across diverse marine environments, supporting reliable integration into
energy systems.

</details>
