<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 122]
- [cs.CL](#cs.CL) [Total: 45]
- [cs.AI](#cs.AI) [Total: 13]
- [cs.LG](#cs.LG) [Total: 94]
- [cs.NE](#cs.NE) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Event-driven Robust Fitting on Neuromorphic Hardware](https://arxiv.org/abs/2508.09466)
*Tam Ngoc-Bang Nguyen,Anh-Dzung Doan,Zhipeng Cai,Tat-Jun Chin*

Main category: cs.CV

TL;DR: 提出了一种利用神经形态计算实现高效鲁棒拟合的方法，通过设计尖峰神经网络和基于事件驱动的模型估计，在Intel Loihi 2硬件上运行，与标准CPU相比能耗仅为15%。


<details>
  <summary>Details</summary>
Motivation: 应对AI发展中高能耗问题，探索如何通过神经形态计算实现能量高效的鲁棒几何模型拟合。

Method: 设计了用于神经形态计算的尖峰神经网络模型，采用基于事件驱动的模型估计方法，并提出缓解硬件精度和指令集限制的算法策略。

Result: 在Intel Loihi 2硬件上运行，与传统CPU相比实现了能量消耗的大幅减少，仅为CPU方法的15%，且达到等同精度。

Conclusion: 神经形态计算与尖峰神经网络结合是实现能量高效鲁棒几何模型拟合的可行方案，表明了在高效算法和硬件交互领域的潜力。

Abstract: Robust fitting of geometric models is a fundamental task in many computer
vision pipelines. Numerous innovations have been produced on the topic, from
improving the efficiency and accuracy of random sampling heuristics to
generating novel theoretical insights that underpin new approaches with
mathematical guarantees. However, one aspect of robust fitting that has
received little attention is energy efficiency. This performance metric has
become critical as high energy consumption is a growing concern for AI
adoption. In this paper, we explore energy-efficient robust fitting via the
neuromorphic computing paradigm. Specifically, we designed a novel spiking
neural network for robust fitting on real neuromorphic hardware, the Intel
Loihi 2. Enabling this are novel event-driven formulations of model estimation
that allow robust fitting to be implemented in the unique architecture of Loihi
2, and algorithmic strategies to alleviate the current limited precision and
instruction set of the hardware. Results show that our neuromorphic robust
fitting consumes only a fraction (15%) of the energy required to run the
established robust fitting algorithm on a standard CPU to equivalent accuracy.

</details>


### [2] [A Context-aware Attention and Graph Neural Network-based Multimodal Framework for Misogyny Detection](https://arxiv.org/abs/2508.09175)
*Mohammad Zia Ur Rehman,Sufyaan Zahoor,Areeb Manzoor,Musharaf Maqbool,Nagendra Kumar*

Main category: cs.CV

TL;DR: 针对社交媒体中的厌女内容检测提出了一种创新的多模态框架，包括三个模块：多模态注意力模块、基于图的特征重构模块和内容特定特征学习模块。


<details>
  <summary>Details</summary>
Motivation: 目前针对一般冒犯性内容的检测方法难以有效识别厌女内容，因此需要为针对女性的特定冒犯性内容开发解决方案。

Method: 提出由多模态注意力模块、基于图的特征重构模块和内容特定特征学习模块组成的新框架，并进一步使用厌女字典和测试时数据增强来优化预测能力。

Result: 在MAMI和MMHS150K两个多模态数据集上的宏平均F1分数分别平均提升10.17%和8.88%。

Conclusion: 新方法在识别多模态厌女和性别歧视内容方面表现出显著改进，并验证了其在不同数据集上的优越性。

Abstract: A substantial portion of offensive content on social media is directed
towards women. Since the approaches for general offensive content detection
face a challenge in detecting misogynistic content, it requires solutions
tailored to address offensive content against women. To this end, we propose a
novel multimodal framework for the detection of misogynistic and sexist
content. The framework comprises three modules: the Multimodal Attention module
(MANM), the Graph-based Feature Reconstruction Module (GFRM), and the
Content-specific Features Learning Module (CFLM). The MANM employs adaptive
gating-based multimodal context-aware attention, enabling the model to focus on
relevant visual and textual information and generating contextually relevant
features. The GFRM module utilizes graphs to refine features within individual
modalities, while the CFLM focuses on learning text and image-specific features
such as toxicity features and caption features. Additionally, we curate a set
of misogynous lexicons to compute the misogyny-specific lexicon score from the
text. We apply test-time augmentation in feature space to better generalize the
predictions on diverse inputs. The performance of the proposed approach has
been evaluated on two multimodal datasets, MAMI and MMHS150K, with 11,000 and
13,494 samples, respectively. The proposed method demonstrates an average
improvement of 10.17% and 8.88% in macro-F1 over existing methods on the MAMI
and MMHS150K datasets, respectively.

</details>


### [3] [IAD-R1: Reinforcing Consistent Reasoning in Industrial Anomaly Detection](https://arxiv.org/abs/2508.09178)
*Yanhui Li,Yunkang Cao,Chengliang Liu,Yuan Xiong,Xinghui Dong,Chao Huang*

Main category: cs.CV

TL;DR: 提出了一种通用后训练框架IAD-R1，旨在通过改进VLM在工业异常检测中的性能，尤其在缺乏样本的情况下表现出显著提升。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测因缺乏缺陷样本而应用受限，而VLM在此领域的泛化能力也不足，需开发新方法增强其表现。

Method: 提出了IAD-R1，包含两阶段训练策略：PA-SFT阶段基于链式思维数据集强化异常感知能力；SC-GRPO阶段优化奖励函数，从感知提升至解释能力。

Result: 实验验证了IAD-R1针对7种VLM显著提高，平均准确率在工业检测上提升达43.3%，小规模模型在零样本测试上超越商业模型如GPT-4.1等。

Conclusion: IAD-R1显著提高了VLM在工业异常检测中的性能，展示了其通用性与优势。

Abstract: Industrial anomaly detection is a critical component of modern manufacturing,
yet the scarcity of defective samples restricts traditional detection methods
to scenario-specific applications. Although Vision-Language Models (VLMs)
demonstrate significant advantages in generalization capabilities, their
performance in industrial anomaly detection remains limited. To address this
challenge, we propose IAD-R1, a universal post-training framework applicable to
VLMs of different architectures and parameter scales, which substantially
enhances their anomaly detection capabilities. IAD-R1 employs a two-stage
training strategy: the Perception Activation Supervised Fine-Tuning (PA-SFT)
stage utilizes a meticulously constructed high-quality Chain-of-Thought dataset
(Expert-AD) for training, enhancing anomaly perception capabilities and
establishing reasoning-to-answer correlations; the Structured Control Group
Relative Policy Optimization (SC-GRPO) stage employs carefully designed reward
functions to achieve a capability leap from "Anomaly Perception" to "Anomaly
Interpretation". Experimental results demonstrate that IAD-R1 achieves
significant improvements across 7 VLMs, attaining up to 43.3% enhancement in
average accuracy on 6 industrial anomaly detection benchmark datasets. Notably,
the 0.5B parameter model trained with IAD-R1 surpasses commercial models
including GPT-4.1 and Claude-Sonnet-4 in zero-shot settings, demonstrating the
effectiveness and superiority of IAD-R1. The dataset, code, and all model
weights will be publicly available at https://github.com/Yanhui-Lee/IAD-R1.

</details>


### [4] [A Neurosymbolic Framework for Interpretable Cognitive Attack Detection in Augmented Reality](https://arxiv.org/abs/2508.09185)
*Rongqian Chen,Allison Andreyev,Yanming Xiu,Mahdi Imani,Bin Li,Maria Gorlatova,Gang Tan,Tian Lan*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的神经符号方法CADAR，用于增强现实(AR)中的认知攻击检测，实验显示其在复杂场景下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着增强现实的普及，操控用户对语义感知的认知攻击逐渐受到关注，现有方法局限于像素级处理或黑箱化的预训练模型，缺乏解释性与语义推理能力。

Method: CADAR将多模态的视觉-语言输入通过预训练的神经视觉-语言模型(VLM)转化为符号感知图表示，结合先验知识、显著性权重和时间关联，再利用基于粒子滤波的统计推理检测认知攻击。

Result: 在扩展的AR认知攻击数据集上的实验显示，CADAR在复杂攻击场景下的准确性比强基线模型提升了10.7%。

Conclusion: CADAR结合了预训练VLM的适应性及粒子滤波的解释性和推理能力，为认知攻击检测提供了一种高效且可解释的方法。

Abstract: Augmented Reality (AR) enriches perception by overlaying virtual elements on
the physical world. Due to its growing popularity, cognitive attacks that alter
AR content to manipulate users' semantic perception have received increasing
attention. Existing detection methods often focus on visual changes, which are
restricted to pixel- or image-level processing and lack semantic reasoning
capabilities, or they rely on pre-trained vision-language models (VLMs), which
function as black-box approaches with limited interpretability. In this paper,
we present CADAR, a novel neurosymbolic approach for cognitive attack detection
in AR. It fuses multimodal vision-language inputs using neural VLMs to obtain a
symbolic perception-graph representation, incorporating prior knowledge,
salience weighting, and temporal correlations. The model then enables
particle-filter based statistical reasoning -- a sequential Monte Carlo method
-- to detect cognitive attacks. Thus, CADAR inherits the adaptability of
pre-trained VLM and the interpretability and reasoning rigor of particle
filtering. Experiments on an extended AR cognitive attack dataset show accuracy
improvements of up to 10.7% over strong baselines on challenging AR attack
scenarios, underscoring the promise of neurosymbolic methods for effective and
interpretable cognitive attack detection.

</details>


### [5] [RL-MoE: An Image-Based Privacy Preserving Approach In Intelligent Transportation System](https://arxiv.org/abs/2508.09186)
*Abdolazim Rezaei,Mehdi Sookhak,Mahboobeh Haghparast*

Main category: cs.CV

TL;DR: 本文提出RL-MoE，一种通过将敏感视觉数据转化为隐私保护文本描述的新框架，避免直接传输图像，同时优化信息语义精度和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有智能交通系统中AI摄像头普及，但隐私保护与数据丰富性之间存在矛盾，现有的模糊化或加密手段效率低下，难以兼顾隐私与数据实用性。

Method: 引入RL-MoE框架，将敏感图像数据分解成文本描述。通过Mixture-of-Experts（MoE）架构处理多维场景，强化学习（RL）优化文本生成，以平衡语义与隐私保护目标。

Result: RL-MoE显著降低重播攻击成功率（在CFP-FP数据集下仅为9.4%），且生成的文本内容比现有方法更丰富。

Conclusion: RL-MoE为隐私敏感领域提供了实际且可扩展的解决方案，有助于提升智能城市和自动驾驶网络的安全性与信任度。

Abstract: The proliferation of AI-powered cameras in Intelligent Transportation Systems
(ITS) creates a severe conflict between the need for rich visual data and the
fundamental right to privacy. Existing privacy-preserving mechanisms, such as
blurring or encryption, are often insufficient, creating an undesirable
trade-off where either privacy is compromised against advanced reconstruction
attacks or data utility is critically degraded. To resolve this impasse, we
propose RL-MoE, a novel framework that transforms sensitive visual data into
privacy-preserving textual descriptions, eliminating the need for direct image
transmission. RL-MoE uniquely combines a Mixture-of-Experts (MoE) architecture
for nuanced, multi-aspect scene decomposition with a Reinforcement Learning
(RL) agent that optimizes the generated text for a dual objective of semantic
accuracy and privacy preservation. Extensive experiments demonstrate that
RL-MoE provides superior privacy protection, reducing the success rate of
replay attacks to just 9.4\% on the CFP-FP dataset, while simultaneously
generating richer textual content than baseline methods. Our work provides a
practical and scalable solution for building trustworthy AI systems in
privacy-sensitive domains, paving the way for more secure smart city and
autonomous vehicle networks.

</details>


### [6] [Synthetic Data Generation for Emotional Depth Faces: Optimizing Conditional DCGANs via Genetic Algorithms in the Latent Space and Stabilizing Training with Knowledge Distillation](https://arxiv.org/abs/2508.09188)
*Seyed Muhammad Hossein Mousavi,S. Younes Mirinezhad*

Main category: cs.CV

TL;DR: 本文提出了一种基于优化GAN和知识蒸馏的框架，用于生成高质量、多样化的合成深度人脸数据，并利用遗传算法进一步提高情感多样性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 情感计算领域缺乏高质量、多样化的深度面部数据集，来有效识别微妙的情绪表达。

Method: 所提框架结合优化的GAN和知识蒸馏技术（EMA教师模型），稳定训练、提升质量并防止模式崩溃。同时使用遗传算法优化GAN潜在向量，基于图像统计增强目标情感的多样性与视觉质量。此外，提取LBP、HOG、Sobel边缘和强度直方图特征，并结合XGBoost分类模型。

Result: 该框架在多样性和质量方面优于GAN、VAE、GMM和KDE方法，分类准确率分别达到94%和96%。FID、IS、SSIM和PSNR评价指标亦表明性能优于当前最优方法。

Conclusion: 所提方法有效解决情感计算中数据集不足的问题，生成了高质量且多样化的深度人脸数据，显著提升情绪识别的准确率并具有广泛适用性。

Abstract: Affective computing faces a major challenge: the lack of high-quality,
diverse depth facial datasets for recognizing subtle emotional expressions. We
propose a framework for synthetic depth face generation using an optimized GAN
with Knowledge Distillation (EMA teacher models) to stabilize training, improve
quality, and prevent mode collapse. We also apply Genetic Algorithms to evolve
GAN latent vectors based on image statistics, boosting diversity and visual
quality for target emotions. The approach outperforms GAN, VAE, GMM, and KDE in
both diversity and quality. For classification, we extract and concatenate LBP,
HOG, Sobel edge, and intensity histogram features, achieving 94% and 96%
accuracy with XGBoost. Evaluation using FID, IS, SSIM, and PSNR shows
consistent improvement over state-of-the-art methods.

</details>


### [7] [$Δ$-AttnMask: Attention-Guided Masked Hidden States for Efficient Data Selection and Augmentation](https://arxiv.org/abs/2508.09199)
*Jucheng Hu,Suorong Yang,Dongzhan Zhou*

Main category: cs.CV

TL;DR: 提出了一种数据高效的框架Δ-AttnMask，通过注意力导向的屏蔽（AttnMask）方法评估样本质量，在减少数据需求的同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 旨在解决视觉指令微调（VIF）中数据选择面临的挑战，包括数据需求量大和对图文对齐的高质量要求。

Method: 提出Δ-AttnMask，通过模型注意力区域的屏蔽前后损失差异（Δ）评估图文数据质量，无需领域标签、辅助模型或额外训练，适用于多种模型和数据模式。

Result: 在多个VLMs与数据集上验证了Δ-AttnMask，仅使用20%的数据即可实现领先性能，加速训练5倍，并比全数据基线提升+10.1%的整体准确度。

Conclusion: Δ-AttnMask具备高效性与广泛的适用性，能有效应对VIF的数据需求，同时提高模型训练速度与性能表现。

Abstract: Visual Instruction Finetuning (VIF) is pivotal for post-training
Vision-Language Models (VLMs). Unlike unimodal instruction finetuning in
plain-text large language models, which mainly requires instruction datasets to
enable model instruction-following ability, VIF also requires multimodal data
to enable joint visual and textual understanding; therefore, it typically
requires more data. Consequently, VIF imposes stricter data selection
challenges: the method must scale efficiently to handle larger data demands
while ensuring the quality of both visual and textual content, as well as their
alignment. Despite its critical impact on performance, data selection for VIF
remains an understudied area. In this paper, we propose $\Delta$-AttnMask. This
data-efficient framework quantifies sample quality through attention-guided
masking of the model's hidden states, jointly evaluating image-text pairs
without requiring domain labels, auxiliary models, or extra training. By
computing loss differences ($\Delta$) between the original states and states
masked using high-attention regions, $\Delta$-AttnMask intrinsically assesses
sample quality. Experiments across multiple VLMs and datasets show that
$\Delta$-AttnMask achieves state-of-the-art performance with just 20% of data,
accelerating training by 5x while surpassing full-dataset baselines by +10.1%
in overall accuracy. Its model-agnostic and data-agnostic design ensures broad
applicability across modalities and architectures.

</details>


### [8] [Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method](https://arxiv.org/abs/2508.09202)
*Masoumeh Sharafi,Soufiane Belharbi,Houssem Ben Salem,Ali Etemad,Alessandro Lameiras Koerich,Marco Pedersoli,Simon Bacon,Eric Granger*

Main category: cs.CV

TL;DR: 该论文提出了一种名为PFT的新方法，在不需要源数据、仅使用中性目标数据的情况下，通过潜在空间的翻译来提高面部表情识别模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度FER模型在处理细微表情及高个人间差异时的性能受限问题，尤其是在源数据不可用并且目标数据仅包含中性表情的情况下。

Method: 提出了一种个性化特征翻译(PFT)方法，通过在源域数据上预训练翻译器，并优化表情一致性和风格感知目标，在潜在空间中进行翻译。同时仅用中性目标数据对翻译器进行适应，无需使用源数据或图像合成。

Result: PFT通过在潜在空间中的翻译避免了面部表情生成的复杂性和噪声，生成了可优化分类的判别性嵌入，减少了计算开销并优化了效率。

Conclusion: PFT不依赖图像合成，仅适应部分模型，相较于基于图像的方法更加轻量化且高效，适用性强。

Abstract: Facial expression recognition (FER) models are employed in many video-based
affective computing applications, such as human-computer interaction and
healthcare monitoring. However, deep FER models often struggle with subtle
expressions and high inter-subject variability, limiting their performance in
real-world applications. To improve their performance, source-free domain
adaptation (SFDA) methods have been proposed to personalize a pretrained source
model using only unlabeled target domain data, thereby avoiding data privacy,
storage, and transmission constraints. This paper addresses a challenging
scenario where source data is unavailable for adaptation, and only unlabeled
target data consisting solely of neutral expressions is available. SFDA methods
are not typically designed to adapt using target data from only a single class.
Further, using models to generate facial images with non-neutral expressions
can be unstable and computationally intensive. In this paper, personalized
feature translation (PFT) is proposed for SFDA. Unlike current image
translation methods for SFDA, our lightweight method operates in the latent
space. We first pre-train the translator on the source domain data to transform
the subject-specific style features from one source subject into another.
Expression information is preserved by optimizing a combination of expression
consistency and style-aware objectives. Then, the translator is adapted on
neutral target data, without using source data or image synthesis. By
translating in the latent space, PFT avoids the complexity and noise of face
expression generation, producing discriminative embeddings optimized for
classification. Using PFT eliminates the need for image synthesis, reduces
computational overhead (using a lightweight translator), and only adapts part
of the model, making the method efficient compared to image-based translation.

</details>


### [9] [GANime: Generating Anime and Manga Character Drawings from Sketches with Deep Learning](https://arxiv.org/abs/2508.09207)
*Tai Vu,Robert Yang*

Main category: cs.CV

TL;DR: 本研究评估了多个模型在动漫人物与其草图之间的图像转换能力，发现C-GAN是最佳选择，可生成高品质的图像。


<details>
  <summary>Details</summary>
Motivation: 解决漫画和动漫产业中由草图生成全彩图像的高成本瓶颈问题。

Method: 研究了Neural Style Transfer、C-GAN和CycleGAN三种模型，进行定性和定量的对比评估。

Result: 发现C-GAN模型能够生成与人工绘制类似的高质量、高分辨率图像。

Conclusion: C-GAN模型在动漫字符草图到图像翻译中的效率和效果最为优越，具有显著潜力。

Abstract: The process of generating fully colorized drawings from sketches is a large,
usually costly bottleneck in the manga and anime industry. In this study, we
examine multiple models for image-to-image translation between anime characters
and their sketches, including Neural Style Transfer, C-GAN, and CycleGAN. By
assessing them qualitatively and quantitatively, we find that C-GAN is the most
effective model that is able to produce high-quality and high-resolution images
close to those created by humans.

</details>


### [10] [MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in Multimodal Large Language Models](https://arxiv.org/abs/2508.09210)
*Fan Zhang,Zebang Cheng,Chong Deng,Haoxuan Li,Zheng Lian,Qian Chen,Huadai Liu,Wen Wang,Yi-Fan Zhang,Renrui Zhang,Ziyu Guo,Zhihong Zhu,Hao Wu,Haixin Wang,Yefeng Zheng,Xiaojiang Peng,Xian Wu,Kun Wang,Xiangang Li,Jieping Ye,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 本文提出了一个新的基准MME-Emotion，用于评价多模态大语言模型（MLLMs）的情感理解和推理能力，包含6000多个视频和多种情感任务。研究发现当前MLLMs的情感智能表现仍不理想，认知和推理得分均较低。


<details>
  <summary>Details</summary>
Motivation: 当前对于多模态大语言模型（MLLMs）的情感智能能力的评估尚存多方面空白，包括其在不同场景下的泛化能力及其情感相关的推理能力。

Method: 提出并设计了MME-Emotion，一个包括多个视频片段及问答对的系统性情感基准测试，对20个先进的MLLMs进行了评价，并结合多代理系统框架进行情感识别和推理的全面评估。

Result: 实验表明当前MLLMs在情感智能方面表现仍不理想，最好模型的情感识别得分仅为39.3%，推理能力得分为56.0%。同时发现通用模型凭借泛化能力表现出情感智能，专业模型通过特定领域后训练可获得类似表现。

Conclusion: MME-Emotion基准的引入，为MLLMs情感智能的研究提供了重要基础和评估工具，有助于未来进一步的模型改进。

Abstract: Recent advances in multimodal large language models (MLLMs) have catalyzed
transformative progress in affective computing, enabling models to exhibit
emergent emotional intelligence. Despite substantial methodological progress,
current emotional benchmarks remain limited, as it is still unknown: (a) the
generalization abilities of MLLMs across distinct scenarios, and (b) their
reasoning capabilities to identify the triggering factors behind emotional
states. To bridge these gaps, we present \textbf{MME-Emotion}, a systematic
benchmark that assesses both emotional understanding and reasoning capabilities
of MLLMs, enjoying \textit{scalable capacity}, \textit{diverse settings}, and
\textit{unified protocols}. As the largest emotional intelligence benchmark for
MLLMs, MME-Emotion contains over 6,000 curated video clips with task-specific
questioning-answering (QA) pairs, spanning broad scenarios to formulate eight
emotional tasks. It further incorporates a holistic evaluation suite with
hybrid metrics for emotion recognition and reasoning, analyzed through a
multi-agent system framework. Through a rigorous evaluation of 20 advanced
MLLMs, we uncover both their strengths and limitations, yielding several key
insights: \ding{182} Current MLLMs exhibit unsatisfactory emotional
intelligence, with the best-performing model achieving only $39.3\%$
recognition score and $56.0\%$ Chain-of-Thought (CoT) score on our benchmark.
\ding{183} Generalist models (\emph{e.g.}, Gemini-2.5-Pro) derive emotional
intelligence from generalized multimodal understanding capabilities, while
specialist models (\emph{e.g.}, R1-Omni) can achieve comparable performance
through domain-specific post-training adaptation. By introducing MME-Emotion,
we hope that it can serve as a foundation for advancing MLLMs' emotional
intelligence in the future.

</details>


### [11] [Towards Effective MLLM Jailbreaking Through Balanced On-Topicness and OOD-Intensity](https://arxiv.org/abs/2508.09218)
*Zuoou Li,Weitong Zhang,Jingyuan Wang,Shuyuan Zhang,Wenjia Bai,Bernhard Kainz,Mengyun Qiao*

Main category: cs.CV

TL;DR: 研究发现当前的多模态大语言模型（MLLMs）在面对恶意的对抗性提示时容易被攻破，而且现有的评估标准可能会高估攻击的有效性。提出了一种四轴评估框架，并开发了一种名为BSD的递归重写策略，显著提高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型缺乏足够的安全性，尤其是面对对抗性提示，且现有评估标准无法准确衡量恶意攻击的真实效果。

Method: 提出四轴评估框架，包括输入相关性、输入分布外强度（OOD）、输出有害性及输出拒绝率，并开发了一种递归重写策略BSD，将恶意提示重组为语义对齐的子任务，增加检测难度。

Result: BSD在13种商业和开源模型中测试，攻击成功率提高67%，输出有害性提升21%，表明现有多模态安全系统存在严重不足。

Conclusion: 研究揭示了当前多模态大语言模型在安全性上的薄弱环节，提出的方法显著提升了对抗性攻击的成功率和有害性，强调了改进模型安全性的紧迫性。

Abstract: Multimodal large language models (MLLMs) are widely used in vision-language
reasoning tasks. However, their vulnerability to adversarial prompts remains a
serious concern, as safety mechanisms often fail to prevent the generation of
harmful outputs. Although recent jailbreak strategies report high success
rates, many responses classified as "successful" are actually benign, vague, or
unrelated to the intended malicious goal. This mismatch suggests that current
evaluation standards may overestimate the effectiveness of such attacks. To
address this issue, we introduce a four-axis evaluation framework that
considers input on-topicness, input out-of-distribution (OOD) intensity, output
harmfulness, and output refusal rate. This framework identifies truly effective
jailbreaks. In a substantial empirical study, we reveal a structural trade-off:
highly on-topic prompts are frequently blocked by safety filters, whereas those
that are too OOD often evade detection but fail to produce harmful content.
However, prompts that balance relevance and novelty are more likely to evade
filters and trigger dangerous output. Building on this insight, we develop a
recursive rewriting strategy called Balanced Structural Decomposition (BSD).
The approach restructures malicious prompts into semantically aligned
sub-tasks, while introducing subtle OOD signals and visual cues that make the
inputs harder to detect. BSD was tested across 13 commercial and open-source
MLLMs, where it consistently led to higher attack success rates, more harmful
outputs, and fewer refusals. Compared to previous methods, it improves success
rates by $67\%$ and harmfulness by $21\%$, revealing a previously
underappreciated weakness in current multimodal safety systems.

</details>


### [12] [Towards Scalable Training for Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2508.09220)
*Haoyang Li,Jiaqing Li,Jialun Cao,Zongyuan Yang,Yongping Xiong*

Main category: cs.CV

TL;DR: 提出了一种创新方法利用有限的手写公式和大量LaTeX生成公式生成新的HMER数据集Tex80M，并提出TexTeller模型，展示了SOTA性能，实现了领域重大突破。


<details>
  <summary>Details</summary>
Motivation: 由于手动标注的公式数据匮乏，限制了HMER领域的发展。

Method: 建立一个可扩展的数据引擎，通过整合手写公式和大规模LaTeX生成公式来构建数据集Tex80M，并结合小规模的手写公式数据进行混合训练提出TexTeller模型。

Result: TexTeller在几乎所有基准测试中均实现了SOTA表现。

Conclusion: 该论文大幅提升了HMER领域的数据可用性和模型性能，并通过开源模型、数据集和代码推动领域进一步发展。

Abstract: Large foundation models have achieved significant performance gains through
scalable training on massive datasets. However, the field of
\textbf{H}andwritten \textbf{M}athematical \textbf{E}xpression
\textbf{R}ecognition (HMER) has been impeded by the scarcity of data, primarily
due to the arduous and costly process of manual annotation. To bridge this gap,
we propose a novel method integrating limited handwritten formulas with
large-scale LaTeX-rendered formulas by developing a scalable data engine to
generate complex and consistent LaTeX sequences. With this engine, we built the
largest formula dataset to date, termed \texttt{Tex80M}, comprising over 80
million high-quality training instances. Then we propose \texttt{TexTeller},
the first HMER model trained at scale, by mix-training \texttt{Tex80M} with a
relatively small HME dataset. The expansive training dataset and our refined
pipeline have equipped \texttt{TexTeller} with state-of-the-art (SOTA)
performance across nearly all benchmarks. To advance the field, we will openly
release our complete model, entire dataset, and full codebase, enabling further
research building upon our contributions.

</details>


### [13] [Gradient-Direction-Aware Density Control for 3D Gaussian Splatting](https://arxiv.org/abs/2508.09239)
*Zheng Zhou,Yu-Jie Xiong,Chun-Ming Xia,Jia-Chen Zhang,Hong-Jian Zhan*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D高斯抛洒方法(GDAGS)，通过对梯度方向的感知适应性密度控制，解决了过度重建和高斯过密的问题，显著提高了渲染质量，同时减少了内存消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯抛洒技术在复杂场景下存在过度重建和高斯过密化的问题，导致渲染质量下降以及内存负担加重。

Method: 提出了梯度方向感知的适应性密度控制框架GDAGS，核心创新是梯度一致性比(GCR)和非线性动态加权机制，以此对高斯体的分裂和复制过程进行方向感知的密度优化。

Result: 在多个真实场景基准测试中，GDAGS展示了卓越的渲染质量，减少了过度重建和高斯过密化，同时优化了高斯利用，内存消耗降低了50%。

Conclusion: GDAGS显著增强了几何细节呈现和场景结构完整性，提供了更精简高效的场景表示，能更好地满足实时高保真渲染需求。

Abstract: The emergence of 3D Gaussian Splatting (3DGS) has significantly advanced
novel view synthesis through explicit scene representation, enabling real-time
photorealistic rendering. However, existing approaches manifest two critical
limitations in complex scenarios: (1) Over-reconstruction occurs when
persistent large Gaussians cannot meet adaptive splitting thresholds during
density control. This is exacerbated by conflicting gradient directions that
prevent effective splitting of these Gaussians; (2) Over-densification of
Gaussians occurs in regions with aligned gradient aggregation, leading to
redundant component proliferation. This redundancy significantly increases
memory overhead due to unnecessary data retention. We present
Gradient-Direction-Aware Gaussian Splatting (GDAGS), a gradient-direction-aware
adaptive density control framework to address these challenges. Our key
innovations: the gradient coherence ratio (GCR), computed through normalized
gradient vector norms, which explicitly discriminates Gaussians with concordant
versus conflicting gradient directions; and a nonlinear dynamic weighting
mechanism leverages the GCR to enable gradient-direction-aware density control.
Specifically, GDAGS prioritizes conflicting-gradient Gaussians during splitting
operations to enhance geometric details while suppressing redundant
concordant-direction Gaussians. Conversely, in cloning processes, GDAGS
promotes concordant-direction Gaussian densification for structural completion
while preventing conflicting-direction Gaussian overpopulation. Comprehensive
evaluations across diverse real-world benchmarks demonstrate that GDAGS
achieves superior rendering quality while effectively mitigating
over-reconstruction, suppressing over-densification, and constructing compact
scene representations with 50\% reduced memory consumption through optimized
Gaussians utilization.

</details>


### [14] [FineState-Bench: A Comprehensive Benchmark for Fine-Grained State Control in GUI Agents](https://arxiv.org/abs/2508.09241)
*Fengxian Ji,Jingpu Yang,Zirui Song,Yuanxi Wang,Zhexuan Cui,Yuke Li,Qian Jiang,Miao Fang,Xiuying Chen*

Main category: cs.CV

TL;DR: 该论文提出了首个针对细粒度GUI代理操作的评测和诊断标准——FineState-Bench，填补了现有框架忽视细粒度控制的空白，并提出与视觉诊断助手（VDA）相结合的诊断方式。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理的评估框架更关注粗粒度任务完成，忽视了细粒度控制，这在实际应用中至关重要。因此需要创建一个标准化评测框架来量化细粒度操作。

Method: 论文提出FineState-Bench框架，包括多平台的2257个任务基准，并通过四阶段指标全面评估从感知到控制的过程。同时开发了可插拔的视觉诊断助手（VDA），实现对视觉感知与定位能力的定量分析。

Result: 在该框架下实验结果显示，最先进的模型在细粒度交互精度方面仅达到32.8%。通过VDA实验发现，理想的视觉定位能力可提升模型成功率14.9%。

Conclusion: 实验和分析首次确认了当前GUI代理的主要瓶颈在于视觉定位能力，FineState-Bench和VDA提供了突破这一瓶颈的研究工具。

Abstract: With the rapid advancement of generative artificial intelligence technology,
Graphical User Interface (GUI) agents have demonstrated tremendous potential
for autonomously managing daily tasks through natural language instructions.
However, current evaluation frameworks for GUI agents suffer from fundamental
flaws: existing benchmarks overly focus on coarse-grained task completion while
neglecting fine-grained control capabilities crucial for real-world
applications. To address this, we introduce FineState-Bench, the first
evaluation and diagnostic standard for fine-grained GUI proxy operations,
designed to quantify fine-grained control. This multi-platform (desktop, Web,
mobile) framework includes 2257 task benchmarks in four components and uses a
four-phase indicator for comprehensive perception-to-control assessment. To
analyze perception and positioning for refined operations, we developed the
plug-and-play Visual Diagnostic Assistant (VDA), enabling the first
quantitative decoupling analysis of these capabilities. Experimental results on
our benchmark show that the most advanced models achieve only 32.8%
fine-grained interaction accuracy. Using our VDA in controlled experiments,
quantifying the impact of visual capabilities, we showed that ideal visual
localization boosts Gemini-2.5-Flash's success rate by 14.9\%. Our diagnostic
framework confirms for the first time that the primary bottleneck for current
GUI proxies is basic visual positioning capability.All resources are fully
open-source. github: https://github.com/AnonymousThewarehouse/FineState-Bench
huggingface: https://huggingface.co/datasets/Willtime2006/Static-FineBench

</details>


### [15] [Beyond Blanket Masking: Examining Granularity for Privacy Protection in Images Captured by Blind and Low Vision Users](https://arxiv.org/abs/2508.09245)
*Jeffri Murrugarra-LLerena,Haoran Niu,K. Suzanne Barber,Hal Daumé III,Yang Trista Cao,Paola Cascante-Bonilla*

Main category: cs.CV

TL;DR: FiGPriv是一种细粒度隐私保护框架，在保留用户隐私的同时，提高了视觉语言模型（VLMs）的实用性。


<details>
  <summary>Details</summary>
Motivation: 主因是针对视觉语言模型中暴露用户隐私的风险，尤其是视障用户需要额外隐私保护的问题。

Method: 提出FiGPriv框架，结合细粒度分割和基于数据驱动的风险评分机制，有针对性地掩盖高风险隐私信息。

Result: FiGPriv能够在保护隐私的同时，增加图像内容的保留率26%，提高VLM提供有用回复的能力11%，提升识别率45%。

Conclusion: FiGPriv实现了隐私保护与实用性之间的良好平衡，为视觉助理系统中的隐私安全提供解决方案。

Abstract: As visual assistant systems powered by visual language models (VLMs) become
more prevalent, concerns over user privacy have grown, particularly for blind
and low vision users who may unknowingly capture personal private information
in their images. Existing privacy protection methods rely on coarse-grained
segmentation, which uniformly masks entire private objects, often at the cost
of usability. In this work, we propose FiGPriv, a fine-grained privacy
protection framework that selectively masks only high-risk private information
while preserving low-risk information. Our approach integrates fine-grained
segmentation with a data-driven risk scoring mechanism. We evaluate our
framework using the BIV-Priv-Seg dataset and show that FiG-Priv preserves +26%
of image content, enhancing the ability of VLMs to provide useful responses by
11% and identify the image content by 45%, while ensuring privacy protection.
Project Page: https://artcs1.github.io/VLMPrivacy/

</details>


### [16] [Harnessing Input-Adaptive Inference for Efficient VLN](https://arxiv.org/abs/2508.09262)
*Dongwoo Kang,Akhil Perincherry,Zachary Coalson,Aiden Gabriel,Stefan Lee,Sanghyun Hong*

Main category: cs.CV

TL;DR: 该文章提出了一种新的输入自适应导航方法，用于提高视觉和语言导航（VLN）模型的效率。通过三种不同层面的自适应算法，成功在多个基准数据集上实现了计算量的显著减少。


<details>
  <summary>Details</summary>
Motivation: 虽然基于多模态变换器的历史感知模型在视觉和语言导航中性能提升显著，但其计算规模大，难以在资源有限情况下有效应用。为此，作者希望开发一种可以兼顾效率与性能的解决方案。

Method: 该方法包括三种自适应算法：1）选择性处理观察时的全景视图以提高空间效率；2）基于重要性的自适应阈值方法优化模型内效率；3）通过缓存机制避免重复处理已观察的视图从而提高时间效率。

Result: 通过对7个VLN基准数据集的评估，与现有的三种代理模型结合，该方法在标准和连续环境中显著减少了超过2倍的计算量。

Conclusion: 提出的输入自适应方法在保持原有模型性能的同时，大幅提高了计算效率，为资源受限环境下的视觉和语言导航应用提供了可行方案。

Abstract: An emerging paradigm in vision-and-language navigation (VLN) is the use of
history-aware multi-modal transformer models. Given a language instruction,
these models process observation and navigation history to predict the most
appropriate action for an agent. While they have significantly improved
performance, the scale of these models can be a bottleneck in practical
settings with limited computational resources. In this work, we propose a novel
input-adaptive navigation method to enhance VLN model efficiency. We first show
that existing input-adaptive mechanisms fail to reduce computations without
substantial performance degradation. To address this, we introduce three
adaptive algorithms, each deployed at a different level: (1) To improve spatial
efficiency, we selectively process panoramic views at each observation of an
agent. (2) To improve intra-model efficiency, we propose importance-based
adaptive thresholding for the early-exit methods. (3) To improve temporal
efficiency, we implement a caching mechanism that prevents reprocessing of
views previously seen by the agent. In evaluations on seven VLN benchmarks, we
demonstrate over a 2$\times$ reduction in computation across three
off-the-shelf agents in both standard and continuous environments. Our code is
publicly available at
https://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.

</details>


### [17] [SegDAC: Segmentation-Driven Actor-Critic for Visual Reinforcement Learning](https://arxiv.org/abs/2508.09325)
*Alexandre Brown,Glen Berseth*

Main category: cs.CV

TL;DR: 本文提出了一种名为SegDAC（基于分割的演员-评论家方法）的视觉强化学习框架，通过结合Segment Anything（SAM）和YOLO-World，使得模型在面对高维视觉输入和噪声奖励的环境下具备更好的泛化能力和样本效率。


<details>
  <summary>Details</summary>
Motivation: 视觉强化学习需要同时学习感知和动作决策，这对高维输入和噪声奖励的环境具备挑战性。目前存在的大型感知模型虽然可以应对复杂视觉场景，但尚未明确如何高效地将这些模型集成到强化学习框架中以提高视觉泛化能力和样本效率。

Method: SegDAC结合了Segment Anything（用于对象分割）和YOLO-World（通过文本提示语义关联分割），核心采用了一种新颖的基于变压器的架构，可动态处理每个时间步骤多个分割对象，无需人工标签，利用在线强化学习学习关键分割对象。

Result: 在视觉泛化的Maniskill3基准测试中，SegDAC展现了出色性能，在视觉扰动严重的情况下将过往方法的表现提高了一倍，并在所有测试任务中匹配甚至超越了之前的方法的样本效率。

Conclusion: SegDAC有效利用了语义分割技术，显著提升了视觉泛化能力与样本效率，为视觉强化学习提供了新的解决方案，尤其适用于视觉环境复杂的场景。

Abstract: Visual reinforcement learning (RL) is challenging due to the need to learn
both perception and actions from high-dimensional inputs and noisy rewards.
Although large perception models exist, integrating them effectively into RL
for visual generalization and improved sample efficiency remains unclear. We
propose SegDAC, a Segmentation-Driven Actor-Critic method. SegDAC uses Segment
Anything (SAM) for object-centric decomposition and YOLO-World to ground
segments semantically via text prompts. It includes a novel transformer-based
architecture that supports a dynamic number of segments at each time step and
effectively learns which segments to focus on using online RL, without using
human labels. By evaluating SegDAC over a challenging visual generalization
benchmark using Maniskill3, which covers diverse manipulation tasks under
strong visual perturbations, we demonstrate that SegDAC achieves significantly
better visual generalization, doubling prior performance on the hardest setting
and matching or surpassing prior methods in sample efficiency across all
evaluated tasks.

</details>


### [18] [Lung-DDPM+: Efficient Thoracic CT Image Synthesis using Diffusion Probabilistic Model](https://arxiv.org/abs/2508.09327)
*Yifan Jiang,Ahmad Shariftabrizi,Venkata SK. Manem*

Main category: cs.CV

TL;DR: 本文提出了一种改进的去噪扩散概率模型Lung-DDPM+，用于生成高质量的肺部CT图像，具有高效率和解剖学精度。


<details>
  <summary>Details</summary>
Motivation: 现有的用于肺癌诊断的生成模型在效率和解剖学精度上表现不足，影响了其临床适用性。

Method: 提出Lung-DDPM+，一种基于结节语义布局指导并结合肺部DPM求解器优化的去噪扩散概率模型；显著提升采样效率和质量。

Result: 在LIDC-IDRI数据集上评估表明，与Lung-DDPM相比，Lung-DDPM+实现了8倍更少的FLOPs、6.8倍更低的GPU内存消耗和14倍更快的采样速度，并在两个分割任务中保持了与SOTA方法相当的样本质量。

Conclusion: Lung-DDPM+展示了其生成高质量并具备高级别精准性的肺部CT图像能力，证明了在医学影像生成领域的应用潜力。

Abstract: Generative artificial intelligence (AI) has been playing an important role in
various domains. Leveraging its high capability to generate high-fidelity and
diverse synthetic data, generative AI is widely applied in diagnostic tasks,
such as lung cancer diagnosis using computed tomography (CT). However, existing
generative models for lung cancer diagnosis suffer from low efficiency and
anatomical imprecision, which limit their clinical applicability. To address
these drawbacks, we propose Lung-DDPM+, an improved version of our previous
model, Lung-DDPM. This novel approach is a denoising diffusion probabilistic
model (DDPM) guided by nodule semantic layouts and accelerated by a pulmonary
DPM-solver, enabling the method to focus on lesion areas while achieving a
better trade-off between sampling efficiency and quality. Evaluation results on
the public LIDC-IDRI dataset suggest that the proposed method achieves
8$\times$ fewer FLOPs (floating point operations per second), 6.8$\times$ lower
GPU memory consumption, and 14$\times$ faster sampling compared to Lung-DDPM.
Moreover, it maintains comparable sample quality to both Lung-DDPM and other
state-of-the-art (SOTA) generative models in two downstream segmentation tasks.
We also conducted a Visual Turing Test by an experienced radiologist, showing
the advanced quality and fidelity of synthetic samples generated by the
proposed method. These experimental results demonstrate that Lung-DDPM+ can
effectively generate high-quality thoracic CT images with lung nodules,
highlighting its potential for broader applications, such as general tumor
synthesis and lesion generation in medical imaging. The code and pretrained
models are available at https://github.com/Manem-Lab/Lung-DDPM-PLUS.

</details>


### [19] [UltraLight Med-Vision Mamba for Classification of Neoplastic Progression in Tubular Adenomas](https://arxiv.org/abs/2508.09339)
*Aqsa Sultana,Nordin Abouzahra,Ahmed Rahu,Brian Shula,Brandon Combs,Derrick Forchetti,Theus Aspiras,Vijayan K. Asari*

Main category: cs.CV

TL;DR: 该研究展示了一种名为Ultralight Med-Vision Mamba的先进模型，其在腺瘤检测及风险分层中表现出出色效果。


<details>
  <summary>Details</summary>
Motivation: 当前结直肠癌的预防依赖于对腺瘤的早期检测与切除，准确的分类与风险评估有助于优化患者结局。深度学习技术的进步为此提供了可能。

Method: 研究提出了基于状态空间模型(SSM)的Ultralight Med-Vision Mamba，用于分析长短程依赖关系以及全幻灯片图像的一般化能力，其具有高效架构，适用于实时临床应用。

Result: 该模型在腺瘤分类、风险分层以及图像分析的一般化能力上表现优异，同时具备快速计算和高扩展性能力。

Conclusion: Ultralight Med-Vision Mamba是一种有效的工具，有望为未来结直肠癌筛查的实际临床应用带来重要价值。

Abstract: Identification of precancerous polyps during routine colonoscopy screenings
is vital for their excision, lowering the risk of developing colorectal cancer.
Advanced deep learning algorithms enable precise adenoma classification and
stratification, improving risk assessment accuracy and enabling personalized
surveillance protocols that optimize patient outcomes. Ultralight Med-Vision
Mamba, a state-space based model (SSM), has excelled in modeling long- and
short-range dependencies and image generalization, critical factors for
analyzing whole slide images. Furthermore, Ultralight Med-Vision Mamba's
efficient architecture offers advantages in both computational speed and
scalability, making it a promising tool for real-time clinical deployment.

</details>


### [20] [Blink-to-code: real-time Morse code communication via eye blink detection and classification](https://arxiv.org/abs/2508.09344)
*Anushka Bhatt*

Main category: cs.CV

TL;DR: 本研究提出了一种实时系统，将自愿的眨眼转换为摩斯电码，用于严重运动障碍者的交流。


<details>
  <summary>Details</summary>
Motivation: 为了解决严重运动障碍者无法交流的问题，研究开发了一种经济且高效的工具。

Method: 利用普通网络摄像头和计算机视觉技术，实时检测并分类眨眼为短(点)或长(划)，然后解码为字母数字字符。

Result: 实验结果显示，系统的解码准确率为62%，响应时间为18-20秒。

Conclusion: 该系统展示了一种可行且低成本的辅助交流方法。

Abstract: This study proposes a real-time system that translates voluntary eye blinks
into Morse code, enabling communication for individuals with severe motor
impairments. Using a standard webcam and computer vision, the system detects
and classifies blinks as short (dot) or long (dash), then decodes them into
alphanumeric characters. Experiments with five participants show 62% decoding
accuracy and 18-20 seconds response times, demonstrating a viable, low-cost
assistive communication method.

</details>


### [21] [FusionEnsemble-Net: An Attention-Based Ensemble of Spatiotemporal Networks for Multimodal Sign Language Recognition](https://arxiv.org/abs/2508.09362)
*Md. Milon Islam,Md Rezwanul Haque,S M Taslim Uddin Raju,Fakhri Karray*

Main category: cs.CV

TL;DR: 提出了FusionEnsemble-Net，一种用于多模态手语识别的注意力机制融合模型，准确率达99.44%。


<details>
  <summary>Details</summary>
Motivation: 当前医疗场景中，手语这种多模态复杂手势的准确识别存在巨大挑战。

Method: 通过融合RGB视频和雷达数据，利用四种时空网络与注意力机制的融合模块，组成一个分类器集成模型。

Result: 在MultiMeDaLIS数据集上测试，意大利手语识别准确率达到99.44%。

Conclusion: 多时空网络与注意力融合机制结合的模型可实现复杂多模态手语的高效、准确识别。

Abstract: Accurate recognition of sign language in healthcare communication poses a
significant challenge, requiring frameworks that can accurately interpret
complex multimodal gestures. To deal with this, we propose FusionEnsemble-Net,
a novel attention-based ensemble of spatiotemporal networks that dynamically
fuses visual and motion data to enhance recognition accuracy. The proposed
approach processes RGB video and range Doppler map radar modalities
synchronously through four different spatiotemporal networks. For each network,
features from both modalities are continuously fused using an attention-based
fusion module before being fed into an ensemble of classifiers. Finally, the
outputs of these four different fused channels are combined in an ensemble
classification head, thereby enhancing the model's robustness. Experiments
demonstrate that FusionEnsemble-Net outperforms state-of-the-art approaches
with a test accuracy of 99.44% on the large-scale MultiMeDaLIS dataset for
Italian Sign Language. Our findings indicate that an ensemble of diverse
spatiotemporal networks, unified by attention-based fusion, yields a robust and
accurate framework for complex, multimodal isolated gesture recognition tasks.
The source code is available at:
https://github.com/rezwanh001/Multimodal-Isolated-Italian-Sign-Language-Recognition.

</details>


### [22] [A Signer-Invariant Conformer and Multi-Scale Fusion Transformer for Continuous Sign Language Recognition](https://arxiv.org/abs/2508.09372)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Fakhri Karray*

Main category: cs.CV

TL;DR: 本文提出了一种双架构框架用于解决连续手语识别中的挑战，并在相关基准测试中实现了性能的新标准。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以有效应对手语识别中的签署人间差异和对新句法结构的泛化问题，需开发更有效的解决方案。

Method: 提出了包括两部分的双架构框架：一个用于签署人无关的签署人不变Conformer架构，另一个用于新句子任务的多尺度融合Transformer模型，分别通过不同的创新设计捕捉手势动态和语言结构。

Result: 在Isharah-1000数据集上，SI任务的WER达到13.07%（降低13.53%），US任务的WER为47.78%，并在SignEval 2025挑战中分别获得了US任务第2名和SI任务第4名。

Conclusion: 验证了为特定CSLR挑战开发任务专用网络能够显著提升性能，并为进一步研究设立了新的基准。

Abstract: Continuous Sign Language Recognition (CSLR) faces multiple challenges,
including significant inter-signer variability and poor generalization to novel
sentence structures. Traditional solutions frequently fail to handle these
issues efficiently. For overcoming these constraints, we propose a
dual-architecture framework. For the Signer-Independent (SI) challenge, we
propose a Signer-Invariant Conformer that combines convolutions with multi-head
self-attention to learn robust, signer-agnostic representations from pose-based
skeletal keypoints. For the Unseen-Sentences (US) task, we designed a
Multi-Scale Fusion Transformer with a novel dual-path temporal encoder that
captures both fine-grained posture dynamics, enabling the model's ability to
comprehend novel grammatical compositions. Experiments on the challenging
Isharah-1000 dataset establish a new standard for both CSLR benchmarks. The
proposed conformer architecture achieves a Word Error Rate (WER) of 13.07% on
the SI challenge, a reduction of 13.53% from the state-of-the-art. On the US
task, the transformer model scores a WER of 47.78%, surpassing previous work.
In the SignEval 2025 CSLR challenge, our team placed 2nd in the US task and 4th
in the SI task, demonstrating the performance of these models. The findings
validate our key hypothesis: that developing task-specific networks designed
for the particular challenges of CSLR leads to considerable performance
improvements and establishes a new baseline for further research. The source
code is available at: https://github.com/rezwanh001/MSLR-Pose86K-CSLR-Isharah.

</details>


### [23] [What Can We Learn from Inter-Annotator Variability in Skin Lesion Segmentation?](https://arxiv.org/abs/2508.09381)
*Kumar Abhishek,Jeremy Kawahara,Ghassan Hamarneh*

Main category: cs.CV

TL;DR: 该研究探讨了医学图像分割中由于边界模糊、注释者偏好和技巧等原因产生的变异性，提出了最大规模的皮肤病变分割多注释者数据集IMA++，并开发深度学习模型预测注释一致性及提高分类准确性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中边界模糊和多注释者间不一致性带来了显著挑战，尤其是在与恶性相关的模糊病变边界上，这需要精确测量和建模改进。

Method: 研究通过构建IMA++，针对皮肤病变注释者的多因素变异性展开深度分析，同时开发基于深度学习的多任务学习模型，利用注释一致性作为临床特征优化预测。

Result: 研究表明，注释一致性与病变恶性的关联显著（p<0.001），并通过模型实现一致性预测（误差0.108），在疾病分类准确性上有显著提升（平均4.2%改进）。

Conclusion: 使用注释一致性作为软临床特征，不仅能够改善深度学习模型的分类性能，还为理解注释变异性对医学图像分析的影响提供了理论支持。

Abstract: Medical image segmentation exhibits intra- and inter-annotator variability
due to ambiguous object boundaries, annotator preferences, expertise, and
tools, among other factors. Lesions with ambiguous boundaries, e.g., spiculated
or infiltrative nodules, or irregular borders per the ABCD rule, are
particularly prone to disagreement and are often associated with malignancy. In
this work, we curate IMA++, the largest multi-annotator skin lesion
segmentation dataset, on which we conduct an in-depth study of variability due
to annotator, malignancy, tool, and skill factors. We find a statistically
significant (p<0.001) association between inter-annotator agreement (IAA),
measured using Dice, and the malignancy of skin lesions. We further show that
IAA can be accurately predicted directly from dermoscopic images, achieving a
mean absolute error of 0.108. Finally, we leverage this association by
utilizing IAA as a "soft" clinical feature within a multi-task learning
objective, yielding a 4.2% improvement in balanced accuracy averaged across
multiple model architectures and across IMA++ and four public dermoscopic
datasets. The code is available at https://github.com/sfu-mial/skin-IAV.

</details>


### [24] [X-UniMotion: Animating Human Images with Expressive, Unified and Identity-Agnostic Motion Latents](https://arxiv.org/abs/2508.09383)
*Guoxian Song,Hongyi Xu,Xiaochen Zhao,You Xie,Tianpei Gu,Zenan Li,Chenxu Zhang,Linjie Luo*

Main category: cs.CV

TL;DR: 提出了一种名为X-UniMotion的统一显式隐式表征方法，用于全身人类动作表现，包括面部表情、身体姿势和手势。


<details>
  <summary>Details</summary>
Motivation: 当前的动作迁移方法依赖于显式的骨骼姿态和启发式的跨身份调整，存在局限性。因此，研究一种更加紧凑、高效且身份无关的动作表示方法。

Method: 通过自监督、端到端框架，将一个图像中的多粒度动作编码为四个解耦的隐含token，结合DiT视频生成模型及大规模人类动作数据集进行训练，并采用2D增强、3D渲染及辅助解码器等方式保证动作和身份的解耦及精细表达。

Result: 实验表明，X-UniMotion在动作细节表现、身份保留及动作迁移的高质量动画生成方面超越现有方法。

Conclusion: X-UniMotion为全身动作表现提供了一种有力工具，在保持高保真动作迁移的同时，实现了身份无关的多样化表现。

Abstract: We present X-UniMotion, a unified and expressive implicit latent
representation for whole-body human motion, encompassing facial expressions,
body poses, and hand gestures. Unlike prior motion transfer methods that rely
on explicit skeletal poses and heuristic cross-identity adjustments, our
approach encodes multi-granular motion directly from a single image into a
compact set of four disentangled latent tokens -- one for facial expression,
one for body pose, and one for each hand. These motion latents are both highly
expressive and identity-agnostic, enabling high-fidelity, detailed
cross-identity motion transfer across subjects with diverse identities, poses,
and spatial configurations. To achieve this, we introduce a self-supervised,
end-to-end framework that jointly learns the motion encoder and latent
representation alongside a DiT-based video generative model, trained on
large-scale, diverse human motion datasets. Motion-identity disentanglement is
enforced via 2D spatial and color augmentations, as well as synthetic 3D
renderings of cross-identity subject pairs under shared poses. Furthermore, we
guide motion token learning with auxiliary decoders that promote fine-grained,
semantically aligned, and depth-aware motion embeddings. Extensive experiments
show that X-UniMotion outperforms state-of-the-art methods, producing highly
expressive animations with superior motion fidelity and identity preservation.

</details>


### [25] [DenoDet V2: Phase-Amplitude Cross Denoising for SAR Object Detection](https://arxiv.org/abs/2508.09392)
*Kang Ni,Minrui Zou,Yuxuan Li,Xiang Li,Kehua Guo,Ming-Ming Cheng,Yimian Dai*

Main category: cs.CV

TL;DR: DenoDet V2采用新颖的注意力模型在变换域中处理特征，结合幅度和相位信息实现SAR目标检测中的降噪，提高性能同时减少模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决SAR目标检测中由相干噪声引起的挑战性问题，并探索一种全新降噪方法。

Method: 通过注意力架构在变换域中进行特征分解和调制，同时结合幅度与相位信息的互相调制，以提升目标检测效果。

Result: 在多个SAR数据集中实现了最先进的性能，比前代模型DenoDet V1在SARDet-100K上提升了0.8%性能，并将模型复杂度减半。

Conclusion: DenoDet V2在性能和模型简化上达到了显著改进，为SAR目标检测提供了一种全新的降噪解决方案。

Abstract: One of the primary challenges in Synthetic Aperture Radar (SAR) object
detection lies in the pervasive influence of coherent noise. As a common
practice, most existing methods, whether handcrafted approaches or deep
learning-based methods, employ the analysis or enhancement of object
spatial-domain characteristics to achieve implicit denoising. In this paper, we
propose DenoDet V2, which explores a completely novel and different perspective
to deconstruct and modulate the features in the transform domain via a
carefully designed attention architecture. Compared to DenoDet V1, DenoDet V2
is a major advancement that exploits the complementary nature of amplitude and
phase information through a band-wise mutual modulation mechanism, which
enables a reciprocal enhancement between phase and amplitude spectra. Extensive
experiments on various SAR datasets demonstrate the state-of-the-art
performance of DenoDet V2. Notably, DenoDet V2 achieves a significant 0.8\%
improvement on SARDet-100K dataset compared to DenoDet V1, while reducing the
model complexity by half. The code is available at
https://github.com/GrokCV/GrokSAR.

</details>


### [26] [Skyshield: Event-Driven Submillimetre Thin Obstacle Detection for Drone Flight Safety](https://arxiv.org/abs/2508.09397)
*Zhengli Zhang,Xinyu Luo,Yuchen Sun,Wenhua Ding,Dongyu Huang,Xinlei Chen*

Main category: cs.CV

TL;DR: 论文提出SkyShield，用于在复杂环境中精准检测亚毫米级薄障碍物。


<details>
  <summary>Details</summary>
Motivation: 传统传感器难以检测亚毫米级薄障碍物，如钢丝和风筝线等，构成对无人机的显著威胁。

Method: 提出SkyShield，基于事件驱动，使用轻量级U-Net框架及Dice-Contour Regularization损失函数提高薄障碍物的检测精度。

Result: 实验表明方法实现了平均F1得分为0.7088，延迟仅有21.2ms，适合部署于边缘和移动平台。

Conclusion: SkyShield框架能够高效低延迟地检测复杂环境中的薄障碍物，表现优越，适合实际应用部署。

Abstract: Drones operating in complex environments face a significant threat from thin
obstacles, such as steel wires and kite strings at the submillimeter level,
which are notoriously difficult for conventional sensors like RGB cameras,
LiDAR, and depth cameras to detect. This paper introduces SkyShield, an
event-driven, end-to-end framework designed for the perception of submillimeter
scale obstacles. Drawing upon the unique features that thin obstacles present
in the event stream, our method employs a lightweight U-Net architecture and an
innovative Dice-Contour Regularization Loss to ensure precise detection.
Experimental results demonstrate that our event-based approach achieves mean F1
Score of 0.7088 with a low latency of 21.2 ms, making it ideal for deployment
on edge and mobile platforms.

</details>


### [27] [Autonomous AI Bird Feeder for Backyard Biodiversity Monitoring](https://arxiv.org/abs/2508.09398)
*El Mustapha Mansouri*

Main category: cs.CV

TL;DR: 该论文开发了一种低成本、无需云端的城市花园鸟类监控系统。


<details>
  <summary>Details</summary>
Motivation: 为了让普通市民能在家中以低成本监控鸟类物种并参与科学研究。

Method: 采用运动触发的IP相机采集画面，通过Detectron2和EfficientNet-B3模型进行鸟类检测与分类，并利用本地服务器处理，避免云端费用和隐私泄露。

Result: 分类器验证集性能达到99.5%，实际使用时对未见鸟类的准确率为88%，证明了系统的可行性。

Conclusion: 该系统为家庭级别的生物多样性记录提供了一种可行、经济的解决方案，可推广用于公民科学研究。

Abstract: This paper presents a low cost, on premise system for autonomous backyard
bird monitoring in Belgian urban gardens. A motion triggered IP camera uploads
short clips via FTP to a local server, where frames are sampled and birds are
localized with Detectron2; cropped regions are then classified by an
EfficientNet-B3 model fine tuned on a 40-species Belgian subset derived from a
larger Kaggle corpus. All processing runs on commodity hardware without a
discrete GPU, preserving privacy and avoiding cloud fees. The physical feeder
uses small entry ports (30 mm) to exclude pigeons and reduce nuisance triggers.
Detector-guided cropping improves classification accuracy over raw-frame
classification. The classifier attains high validation performance on the
curated subset (about 99.5 percent) and delivers practical field accuracy
(top-1 about 88 percent) on held-out species, demonstrating feasibility for
citizen-science-grade biodiversity logging at home.

</details>


### [28] [Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving](https://arxiv.org/abs/2508.09404)
*Guangxun Zhu,Shiyu Fan,Hang Dai,Edmond S. L. Ho*

Main category: cs.CV

TL;DR: 引入了Waymo-3DSkelMo，一个高质量、时序一致、具有交互语义的3D骨架运动数据集，为复杂城市环境的行人行为研究提供了基础资源。


<details>
  <summary>Details</summary>
Motivation: 现有数据集依赖于单目RGB视频帧估算3D姿态，在遮挡和时序连续性不足的情况下会导致不现实的低质量人类运动数据。

Method: 利用3D人体形状和运动先验，增强从原始LiDAR点云中提取的3D姿态序列的质量，并创建了14,000秒覆盖800多种真实驾驶场景的大规模数据集。

Result: 包含平均27名代理人（最多250）的场景中，基于不同行人密度的3D姿态预测基准证明了数据集的研究价值。

Conclusion: Waymo-3DSkelMo为未来复杂城市环境下精细人类行为研究提供了一个基础资源，数据集和代码将公开。

Abstract: Large-scale high-quality 3D motion datasets with multi-person interactions
are crucial for data-driven models in autonomous driving to achieve
fine-grained pedestrian interaction understanding in dynamic urban
environments. However, existing datasets mostly rely on estimating 3D poses
from monocular RGB video frames, which suffer from occlusion and lack of
temporal continuity, thus resulting in unrealistic and low-quality human
motion. In this paper, we introduce Waymo-3DSkelMo, the first large-scale
dataset providing high-quality, temporally coherent 3D skeletal motions with
explicit interaction semantics, derived from the Waymo Perception dataset. Our
key insight is to utilize 3D human body shape and motion priors to enhance the
quality of the 3D pose sequences extracted from the raw LiDRA point clouds. The
dataset covers over 14,000 seconds across more than 800 real driving scenarios,
including rich interactions among an average of 27 agents per scene (with up to
250 agents in the largest scene). Furthermore, we establish 3D pose forecasting
benchmarks under varying pedestrian densities, and the results demonstrate its
value as a foundational resource for future research on fine-grained human
behavior understanding in complex urban environments. The dataset and code will
be available at https://github.com/GuangxunZhu/Waymo-3DSkelMo

</details>


### [29] [RampNet: A Two-Stage Pipeline for Bootstrapping Curb Ramp Detection in Streetscape Images from Open Government Metadata](https://arxiv.org/abs/2508.09415)
*John S. O'Meara,Jared Hwang,Zeyu Wang,Michael Saugstad,Jon E. Froehlich*

Main category: cs.CV

TL;DR: 本文提出了名为RampNet的两阶段方法，用于扩展路缘坡检测数据集和提高模型性能，并取得了业界领先的结果。


<details>
  <summary>Details</summary>
Motivation: 现有路缘坡检测面临数据缺乏的问题，尤其是高质量、大规模的数据集显得尤为稀缺。

Method: 设计了两阶段管道：第一阶段通过将政府提供的路缘坡位置信息自动转化为全景图片中的像素坐标，生成超21万条标注数据；第二阶段训练了基于ConvNeXt V2改进的检测模型。

Result: 生成的数据集的精度达到94.0%，召回率达到92.5%；检测模型AP分数为0.9236，表现远超之前研究。

Conclusion: 首次构建了大规模且高质量的路缘坡检测数据集、基准测试和配套检测模型，为城市可达性研究作出了重要贡献。

Abstract: Curb ramps are critical for urban accessibility, but robustly detecting them
in images remains an open problem due to the lack of large-scale, high-quality
datasets. While prior work has attempted to improve data availability with
crowdsourced or manually labeled data, these efforts often fall short in either
quality or scale. In this paper, we introduce and evaluate a two-stage pipeline
called RampNet to scale curb ramp detection datasets and improve model
performance. In Stage 1, we generate a dataset of more than 210,000 annotated
Google Street View (GSV) panoramas by auto-translating government-provided curb
ramp location data to pixel coordinates in panoramic images. In Stage 2, we
train a curb ramp detection model (modified ConvNeXt V2) from the generated
dataset, achieving state-of-the-art performance. To evaluate both stages of our
pipeline, we compare to manually labeled panoramas. Our generated dataset
achieves 94.0% precision and 92.5% recall, and our detection model reaches
0.9236 AP -- far exceeding prior work. Our work contributes the first
large-scale, high-quality curb ramp detection dataset, benchmark, and model.

</details>


### [30] [Distilling LLM Prior to Flow Model for Generalizable Agent's Imagination in Object Goal Navigation](https://arxiv.org/abs/2508.09423)
*Badi Li,Ren-jie Lu,Yu Zhou,Jingke Meng,Wei-shi Zheng*

Main category: cs.CV

TL;DR: 提出了一种新的生成式框架GOAL，通过利用大语言模型提供的丰富语义上下文，提高了在未知环境中导航到指定物体的能力，并在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 克服传统确定性方法无法充分建模室内布局的不确定性，并提升在未知环境中的泛化能力。

Method: 设计了一个基于生成流的框架GOAL，结合大型语言模型生成的上下文知识，通过二维高斯场嵌入语义地图，进行室内环境的语义分布建模。

Result: GOAL在MP3D和Gibson上取得了最先进的性能，在HM3D转移设置中展现了强大的泛化能力。

Conclusion: 通过生成式框架结合LLM推断的空间先验知识，GOAL在ObjectNav任务中表现优越，为机器人在未知环境中的导航任务提供了新的解决方案。

Abstract: The Object Goal Navigation (ObjectNav) task challenges agents to locate a
specified object in an unseen environment by imagining unobserved regions of
the scene. Prior approaches rely on deterministic and discriminative models to
complete semantic maps, overlooking the inherent uncertainty in indoor layouts
and limiting their ability to generalize to unseen environments. In this work,
we propose GOAL, a generative flow-based framework that models the semantic
distribution of indoor environments by bridging observed regions with
LLM-enriched full-scene semantic maps. During training, spatial priors inferred
from large language models (LLMs) are encoded as two-dimensional Gaussian
fields and injected into target maps, distilling rich contextual knowledge into
the flow model and enabling more generalizable completions. Extensive
experiments demonstrate that GOAL achieves state-of-the-art performance on MP3D
and Gibson, and shows strong generalization in transfer settings to HM3D. Codes
and pretrained models are available at https://github.com/Badi-Li/GOAL.

</details>


### [31] [What-Meets-Where: Unified Learning of Action and Contact Localization in a New Dataset](https://arxiv.org/abs/2508.09428)
*Yuxiao Wang,Yu Lei,Wolin Liang,Weiying Xue,Zhenao Wei,Nan Zhuang,Qi Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新任务，通过同时预测高级动作语义和细粒度的身体部位接触区域，旨在更全面地理解视觉场景中的动作。


<details>
  <summary>Details</summary>
Motivation: 当前方法在动作语义和场景空间背景建模方面不完善，未能同时捕捉两者。

Method: 提出了PaIR-Net框架，包括接触先验识别模块（CPAM）、先验引导分割器（PGCS）以及交互推理模块（IIM）。提供包含13,979张图像的PaIR数据集。

Result: 实验表明，PaIR-Net显著优于基线方法，且组件有效性通过消融研究得到验证。

Conclusion: 提出的新任务、方法和数据集为理解动作提供了更全面的视角，数据和代码将公开发布支持研究。

Abstract: People control their bodies to establish contact with the environment. To
comprehensively understand actions across diverse visual contexts, it is
essential to simultaneously consider \textbf{what} action is occurring and
\textbf{where} it is happening. Current methodologies, however, often
inadequately capture this duality, typically failing to jointly model both
action semantics and their spatial contextualization within scenes. To bridge
this gap, we introduce a novel vision task that simultaneously predicts
high-level action semantics and fine-grained body-part contact regions. Our
proposed framework, PaIR-Net, comprises three key components: the Contact Prior
Aware Module (CPAM) for identifying contact-relevant body parts, the
Prior-Guided Concat Segmenter (PGCS) for pixel-wise contact segmentation, and
the Interaction Inference Module (IIM) responsible for integrating global
interaction relationships. To facilitate this task, we present PaIR (Part-aware
Interaction Representation), a comprehensive dataset containing 13,979 images
that encompass 654 actions, 80 object categories, and 17 body parts.
Experimental evaluation demonstrates that PaIR-Net significantly outperforms
baseline approaches, while ablation studies confirm the efficacy of each
architectural component. The code and dataset will be released upon
publication.

</details>


### [32] [MPT: Motion Prompt Tuning for Micro-Expression Recognition](https://arxiv.org/abs/2508.09446)
*Jiateng Liu,Hengcan Shi,Feng Chen,Zhiwen Shao,Yaonan Wang,Jianfei Cai,Wenming Zheng*

Main category: cs.CV

TL;DR: 本文提出一种名为运动提示调优(MPT)的方法，通过运动放大和高斯标记化生成提示，并加入组适配器，提升大型预训练模型(LMs)在微表情识别(MER)领域的适配性。实验结果表明，该方法在三个主流数据集上超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 微表情识别在医学诊断、测谎和犯罪调查中应用广泛，但因标注样本稀缺制约着模型学习能力。目前大模型虽然表现优异，但无法捕捉微表情所需的细微面部动作。

Method: 提出运动提示调优(MPT)方法，通过运动放大与高斯标记化提取微小运动作为提示，并在模型中插入组适配器适应MER领域需求。

Result: 实验结果表明MPT方法在三个主流微表情数据集上均优于当前最先进的方法。

Conclusion: MPT方法有效利用了大型预训练模型，通过细微动作提示和组适配器设计，显著提升了微表情识别的表现。

Abstract: Micro-expression recognition (MER) is crucial in the affective computing
field due to its wide application in medical diagnosis, lie detection, and
criminal investigation. Despite its significance, obtaining micro-expression
(ME) annotations is challenging due to the expertise required from
psychological professionals. Consequently, ME datasets often suffer from a
scarcity of training samples, severely constraining the learning of MER models.
While current large pre-training models (LMs) offer general and discriminative
representations, their direct application to MER is hindered by an inability to
capture transitory and subtle facial movements-essential elements for effective
MER. This paper introduces Motion Prompt Tuning (MPT) as a novel approach to
adapting LMs for MER, representing a pioneering method for subtle motion prompt
tuning. Particularly, we introduce motion prompt generation, including motion
magnification and Gaussian tokenization, to extract subtle motions as prompts
for LMs. Additionally, a group adapter is carefully designed and inserted into
the LM to enhance it in the target MER domain, facilitating a more nuanced
distinction of ME representation. Furthermore, extensive experiments conducted
on three widely used MER datasets demonstrate that our proposed MPT
consistently surpasses state-of-the-art approaches and verifies its
effectiveness.

</details>


### [33] [RASR: Retrieval-Augmented Super Resolution for Practical Reference-based Image Restoration](https://arxiv.org/abs/2508.09449)
*Jiaqi Yan,Shuning Xu,Xiangyu Chen,Dell Zhang,Jie Tang,Gangshan Wu,Jie Liu*

Main category: cs.CV

TL;DR: 提出了一种名为RASR的检索增强超分辨率方法，通过自动检索相关高分辨率图像实现更实用的参考图像超分辨率。


<details>
  <summary>Details</summary>
Motivation: 现有的方法依赖人工配对目标和参考图像，不适合实际场景需求。

Method: 开发了RASR-Flickr30基准数据集并提出RASRNet，结合语义参考检索器和基于扩散的RefSR生成器，对输入图像进行超分辨率增强。

Result: 在RASR-Flickr30数据集上，RASRNet相较于传统SISR提升了PSNR +0.38 dB，降低了LPIPS -0.0131，生成了更逼真的纹理。

Conclusion: 检索增强技术在桥接学术研究与真实应用场景间具有重要潜力。

Abstract: Reference-based Super Resolution (RefSR) improves upon Single Image Super
Resolution (SISR) by leveraging high-quality reference images to enhance
texture fidelity and visual realism. However, a critical limitation of existing
RefSR approaches is their reliance on manually curated target-reference image
pairs, which severely constrains their practicality in real-world scenarios. To
overcome this, we introduce Retrieval-Augmented Super Resolution (RASR), a new
and practical RefSR paradigm that automatically retrieves semantically relevant
high-resolution images from a reference database given only a low-quality
input. This enables scalable and flexible RefSR in realistic use cases, such as
enhancing mobile photos taken in environments like zoos or museums, where
category-specific reference data (e.g., animals, artworks) can be readily
collected or pre-curated. To facilitate research in this direction, we
construct RASR-Flickr30, the first benchmark dataset designed for RASR. Unlike
prior datasets with fixed target-reference pairs, RASR-Flickr30 provides
per-category reference databases to support open-world retrieval. We further
propose RASRNet, a strong baseline that combines a semantic reference retriever
with a diffusion-based RefSR generator. It retrieves relevant references based
on semantic similarity and employs a diffusion-based generator enhanced with
semantic conditioning. Experiments on RASR-Flickr30 demonstrate that RASRNet
consistently improves over SISR baselines, achieving +0.38 dB PSNR and -0.0131
LPIPS, while generating more realistic textures. These findings highlight
retrieval augmentation as a promising direction to bridge the gap between
academic RefSR research and real-world applicability.

</details>


### [34] [HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss](https://arxiv.org/abs/2508.09453)
*Abdul Matin,Tanjim Bin Faruk,Shrideep Pallickara,Sangmi Lee Pallickara*

Main category: cs.CV

TL;DR: HyperKD是一个新颖的知识蒸馏框架，解决了高光谱遥感中光谱差异问题，通过知识转移提高任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前大规模预训练模型在高光谱遥感中应用受限于光谱差异和样本稀缺性，该研究旨在解决这些挑战并更好地应用基础模型。

Method: 提出HyperKD框架，通过反向的知识蒸馏策略，将简单的教师模型学习到的表示迁移到学生模型中，并使用特征对齐、空间特征遮掩和增强损失函数等技术。

Result: 实验表明，HyperKD框架显著增强了表示学习效果，在土地覆盖分类、作物类型识别和土壤有机碳预测等下游任务中表现优异。

Conclusion: HyperKD为高光谱遥感中的知识蒸馏提供了新方案，有效解决了光谱域间差异问题，为地理空间分析任务奠定了基础。

Abstract: The proliferation of foundation models, pretrained on large-scale unlabeled
datasets, has emerged as an effective approach in creating adaptable and
reusable architectures that can be leveraged for various downstream tasks using
satellite observations. However, their direct application to hyperspectral
remote sensing remains challenging due to inherent spectral disparities and the
scarcity of available observations. In this work, we present HyperKD, a novel
knowledge distillation framework that enables transferring learned
representations from a teacher model into a student model for effective
development of a foundation model on hyperspectral images. Unlike typical
knowledge distillation frameworks, which use a complex teacher to guide a
simpler student, HyperKD enables an inverse form of knowledge transfer across
different types of spectral data, guided by a simpler teacher model. Building
upon a Masked Autoencoder, HyperKD distills knowledge from the Prithvi
foundational model into a student tailored for EnMAP hyperspectral imagery.
HyperKD addresses the inverse domain adaptation problem with spectral gaps by
introducing a feature-based strategy that includes spectral range-based channel
alignment, spatial feature-guided masking, and an enhanced loss function
tailored for hyperspectral images. HyperKD bridges the substantial spectral
domain gap, enabling the effective use of pretrained foundation models for
geospatial applications. Extensive experiments show that HyperKD significantly
improves representation learning in MAEs, leading to enhanced reconstruction
fidelity and more robust performance on downstream tasks such as land cover
classification, crop type identification, and soil organic carbon prediction,
underpinning the potential of knowledge distillation frameworks in remote
sensing analytics with hyperspectral imagery.

</details>


### [35] [Animate-X++: Universal Character Image Animation with Dynamic Backgrounds](https://arxiv.org/abs/2508.09454)
*Shuai Tan,Biao Gong,Zhuoxin Liu,Yan Wang,Xi Chen,Yifan Feng,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 本文提出Animate-X++，一种适用于多种角色类型包括拟人化角色的通用动画框架，同时提高了运动表征和背景动态生成，显著提升了动画效果的逼真度和普适性。


<details>
  <summary>Details</summary>
Motivation: 现有的角色图像动画技术主要针对人类角色，无法很好地泛化到拟人化角色，且通常只生成静态背景的视频，限制了视频的真实性。

Method: 本文提出Animate-X++框架，通过引入Pose Indicator提升运动表征能力，并采用多任务训练策略结合部分参数训练，实现场景动态与角色动画的同步优化。此外，创建A2Bench基准用于评估其在普适动画图像上的表现。

Result: 实验结果表明，Animate-X++在不同角色类型和动态背景生成方面展现出卓越的效果，验证了其优越性和高效性。

Conclusion: Animate-X++显著提升了角色动画系统的普适性和真实性，为游戏及娱乐行业提供了普遍适应的先进解决方案。

Abstract: Character image animation, which generates high-quality videos from a
reference image and target pose sequence, has seen significant progress in
recent years. However, most existing methods only apply to human figures, which
usually do not generalize well on anthropomorphic characters commonly used in
industries like gaming and entertainment. Furthermore, previous methods could
only generate videos with static backgrounds, which limits the realism of the
videos. For the first challenge, our in-depth analysis suggests to attribute
this limitation to their insufficient modeling of motion, which is unable to
comprehend the movement pattern of the driving video, thus imposing a pose
sequence rigidly onto the target character. To this end, this paper proposes
Animate-X++, a universal animation framework based on DiT for various character
types, including anthropomorphic characters. To enhance motion representation,
we introduce the Pose Indicator, which captures comprehensive motion pattern
from the driving video through both implicit and explicit manner. The former
leverages CLIP visual features of a driving video to extract its gist of
motion, like the overall movement pattern and temporal relations among motions,
while the latter strengthens the generalization of DiT by simulating possible
inputs in advance that may arise during inference. For the second challenge, we
introduce a multi-task training strategy that jointly trains the animation and
TI2V tasks. Combined with the proposed partial parameter training, this
approach achieves not only character animation but also text-driven background
dynamics, making the videos more realistic. Moreover, we introduce a new
Animated Anthropomorphic Benchmark (A2Bench) to evaluate the performance of
Animate-X++ on universal and widely applicable animation images. Extensive
experiments demonstrate the superiority and effectiveness of Animate-X++.

</details>


### [36] [IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding](https://arxiv.org/abs/2508.09456)
*Junxian Li,Beining Xu,Di Zhang*

Main category: cs.CV

TL;DR: 本文提出IAG方法，通过后门攻击操控视觉-语言模型在视觉定位任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 目前视觉-语言模型在视觉定位任务上取得显著进展，但其安全性问题，尤其是后门攻击方面尚未得到充分研究。

Method: 提出了IAG（Input-Aware Backdoor Generation），结合自适应触发器生成器和文本条件U-Net，并通过重构损失增强隐蔽性，同时统一创建攻击数据。

Result: 通过理论和实证评估验证了IAG的可行性和有效性。在InternVL-2.5-8B上的ASR@0.5超过65%，并在Ferret-7B和LlaVA-1.5-7B上也表现良好，清洁样本准确率几乎不下降。

Conclusion: IAG展示了高效性、鲁棒性和可迁移性，并为应对相关防御提供了实验支持。

Abstract: Vision-language models (VLMs) have shown significant advancements in tasks
such as visual grounding, where they localize specific objects in images based
on natural language queries and images. However, security issues in visual
grounding tasks for VLMs remain underexplored, especially in the context of
backdoor attacks. In this paper, we introduce a novel input-aware backdoor
attack method, IAG, designed to manipulate the grounding behavior of VLMs. This
attack forces the model to ground a specific target object in the input image,
regardless of the user's query. We propose an adaptive trigger generator that
embeds the semantic information of the attack target's description into the
original image using a text-conditional U-Net, thereby overcoming the
open-vocabulary attack challenge. To ensure the attack's stealthiness, we
utilize a reconstruction loss to minimize visual discrepancies between poisoned
and clean images. Additionally, we introduce a unified method for generating
attack data. IAG is evaluated theoretically and empirically, demonstrating its
feasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches
over 65\% on various testing sets. IAG also shows promising potential on
manipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on
clean samples. Extensive specific experiments, such as ablation study and
potential defense, also indicate the robustness and transferability of our
attack.

</details>


### [37] [RelayFormer: A Unified Local-Global Attention Framework for Scalable Image and Video Manipulation Localization](https://arxiv.org/abs/2508.09459)
*Wen Huang,Jiarui Yang,Tao Dai,Jiawei Li,Shaoxiong Zhan,Bin Wang,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 本文提出了RelayFormer，用于跨图像和视频的视觉篡改定位，具备可扩展性和强大的通用性，并表现出卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在跨模态泛化和高分辨率或长时段处理能力上存在不足。

Method: 提出了RelayFormer架构，结合灵活的局部单元与全局-局部中继注意力（GLoRA）机制，与ViT和SegFormer等Transformer兼容并通过轻量级适配实现简易集成。同时，设计了一种轻量级基于查询的掩码解码器，支持线性复杂度的单次推理处理视频序列。

Result: 在多个基准测试中表现出色，取得了视觉篡改定位的最新性能并成为新基准。

Conclusion: RelayFormer提供了一个模块化、统一的解决方案，用于图像与视频中的篡改定位，其出色表现和划时代意义为未来相关研究提供了新的可能性。

Abstract: Visual manipulation localization (VML) -- across both images and videos -- is
a crucial task in digital forensics that involves identifying tampered regions
in visual content. However, existing methods often lack cross-modal
generalization and struggle to handle high-resolution or long-duration inputs
efficiently.
  We propose RelayFormer, a unified and modular architecture for visual
manipulation localization across images and videos. By leveraging flexible
local units and a Global-Local Relay Attention (GLoRA) mechanism, it enables
scalable, resolution-agnostic processing with strong generalization. Our
framework integrates seamlessly with existing Transformer-based backbones, such
as ViT and SegFormer, via lightweight adaptation modules that require only
minimal architectural changes, ensuring compatibility without disrupting
pretrained representations.
  Furthermore, we design a lightweight, query-based mask decoder that supports
one-shot inference across video sequences with linear complexity. Extensive
experiments across multiple benchmarks demonstrate that our approach achieves
state-of-the-art localization performance, setting a new baseline for scalable
and modality-agnostic VML. Code is available at:
https://github.com/WenOOI/RelayFormer.

</details>


### [38] [Gen-AFFECT: Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy](https://arxiv.org/abs/2508.09461)
*Hao Yu,Rupayan Mallick,Margrit Betke,Sarah Adel Bargal*

Main category: cs.CV

TL;DR: 此论文提出了一个名为GEN-AFFECT的新框架，用于生成具有多样面部表情且保持身份一致性的个性化头像，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法很好地捕捉细致的面部表情或在不同表情间保持身份一致性，限制了其应用效果。

Method: 该框架利用多模态扩散变压器，结合身份-表情表示进行条件训练，并在推断过程中引入一致性注意力机制，以保证身份和表情的一致性。

Result: GEN-AFFECT在生成表情的准确性、身份保留及多个细腻表情间的身份一致性方面优于现有最先进方法。

Conclusion: GEN-AFFECT是一种先进的个性化头像生成框架，证明其在生成表达丰富且身份一致的头像方面的卓越性能。

Abstract: Different forms of customized 2D avatars are widely used in gaming
applications, virtual communication, education, and content creation. However,
existing approaches often fail to capture fine-grained facial expressions and
struggle to preserve identity across different expressions. We propose
GEN-AFFECT, a novel framework for personalized avatar generation that generates
expressive and identity-consistent avatars with a diverse set of facial
expressions. Our framework proposes conditioning a multimodal diffusion
transformer on an extracted identity-expression representation. This enables
identity preservation and representation of a wide range of facial expressions.
GEN-AFFECT additionally employs consistent attention at inference for
information sharing across the set of generated expressions, enabling the
generation process to maintain identity consistency over the array of generated
fine-grained expressions. GEN-AFFECT demonstrates superior performance compared
to previous state-of-the-art methods on the basis of the accuracy of the
generated expressions, the preservation of the identity and the consistency of
the target identity across an array of fine-grained facial expressions.

</details>


### [39] [CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in City-scale Scenarios](https://arxiv.org/abs/2508.09470)
*Jialei Xu,Zizhuang Wei,Weikang You,Linyun Li,Weijian Sun*

Main category: cs.CV

TL;DR: CitySeg是一种城市级点云语义分割的基础模型，不依赖视觉数据，实现开放词汇分割和零样本推理，在多个基准上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有点云语义分割模型受限于数据规模及域间差异，通用性不足。

Method: 提出CitySeg模型，采用文本模态实现开放词汇分割，通过定制数据预处理规则、局部全局交叉注意力网络和分层分类策略解决数据分布不均和标签不一致问题，并引入两阶段训练策略及hinge loss提升特征区分度。

Result: CitySeg在9个闭集基准测试中实现SOTA性能，首次在城市级点云场景中不依赖视觉信息实现零样本泛化。

Conclusion: CitySeg提供了一种处理城市级点云数据的高效方法，实现了语义标签的一致性与模型泛化能力的大幅提升。

Abstract: Semantic segmentation of city-scale point clouds is a critical technology for
Unmanned Aerial Vehicle (UAV) perception systems, enabling the classification
of 3D points without relying on any visual information to achieve comprehensive
3D understanding. However, existing models are frequently constrained by the
limited scale of 3D data and the domain gap between datasets, which lead to
reduced generalization capability. To address these challenges, we propose
CitySeg, a foundation model for city-scale point cloud semantic segmentation
that incorporates text modality to achieve open vocabulary segmentation and
zero-shot inference. Specifically, in order to mitigate the issue of
non-uniform data distribution across multiple domains, we customize the data
preprocessing rules, and propose a local-global cross-attention network to
enhance the perception capabilities of point networks in UAV scenarios. To
resolve semantic label discrepancies across datasets, we introduce a
hierarchical classification strategy. A hierarchical graph established
according to the data annotation rules consolidates the data labels, and the
graph encoder is used to model the hierarchical relationships between
categories. In addition, we propose a two-stage training strategy and employ
hinge loss to increase the feature separability of subcategories. Experimental
results demonstrate that the proposed CitySeg achieves state-of-the-art (SOTA)
performance on nine closed-set benchmarks, significantly outperforming existing
approaches. Moreover, for the first time, CitySeg enables zero-shot
generalization in city-scale point cloud scenarios without relying on visual
information.

</details>


### [40] [Leveraging Failed Samples: A Few-Shot and Training-Free Framework for Generalized Deepfake Detection](https://arxiv.org/abs/2508.09475)
*Shibo Yao,Renshuai Tao,Xiaolong Zheng,Chao Liang,Chunjie Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为FTNet的方法，用于少样本深度伪造检测，不需要训练或参数更新，只需一个伪造样本即可有效检测伪造图像，并显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造检测方法在面对未知样本时表现欠佳，而实际应用中常有少量未知样本可供分析，因此需要提出能利用少量样本来提升检测效果的方法。

Method: 提出FTNet方法，通过比较测试样本与少量已知伪造和真实样本的距离，无需大规模数据训练，直接基于少量样本即可完成分类。

Result: 在基于29种生成模型的AI生成图像上进行评估，FTNet实现了新的SoTA性能，平均性能相比现有方法提升了8.7%。

Conclusion: FTNet为实际场景中的深度伪造检测提出了一种有效的方法，即当模型在少量样本上泛化不佳时，利用这些失败样本反而能增强检测性能。

Abstract: Recent deepfake detection studies often treat unseen sample detection as a
``zero-shot" task, training on images generated by known models but
generalizing to unknown ones. A key real-world challenge arises when a model
performs poorly on unknown samples, yet these samples remain available for
analysis. This highlights that it should be approached as a ``few-shot" task,
where effectively utilizing a small number of samples can lead to significant
improvement. Unlike typical few-shot tasks focused on semantic understanding,
deepfake detection prioritizes image realism, which closely mirrors real-world
distributions. In this work, we propose the Few-shot Training-free Network
(FTNet) for real-world few-shot deepfake detection. Simple yet effective, FTNet
differs from traditional methods that rely on large-scale known data for
training. Instead, FTNet uses only one fake samplefrom an evaluation set,
mimicking the scenario where new samples emerge in the real world and can be
gathered for use, without any training or parameter updates. During evaluation,
each test sample is compared to the known fake and real samples, and it is
classified based on the category of the nearest sample. We conduct a
comprehensive analysis of AI-generated images from 29 different generative
models and achieve a new SoTA performance, with an average improvement of 8.7\%
compared to existing methods. This work introduces a fresh perspective on
real-world deepfake detection: when the model struggles to generalize on a
few-shot sample, leveraging the failed samples leads to better performance.

</details>


### [41] [From Large Angles to Consistent Faces: Identity-Preserving Video Generation via Mixture of Facial Experts](https://arxiv.org/abs/2508.09476)
*Yuji Wang,Moran Li,Xiaobin Hu,Ran Yi,Jiangning Zhang,Chengming Xu,Weijian Cao,Yabiao Wang,Chengjie Wang,Lizhuang Ma*

Main category: cs.CV

TL;DR: 本文提出了一种创新的视频生成方法，通过引入面部专家混合模型（MoFE）和专门设计的大面部角度数据集（LFA），解决了当前视频生成在大面部角度下身份保留困难的问题。实验表明，方法在几项指标上均超越当前SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法在大面部角度下身份保留表现较差，主要是因为无法有效整合身份特征以及现有数据集的覆盖不足。

Method: 引入了一种动态组合不同面部专家（MoFE）的机制，并开发了一套数据处理管线，确保数据集中面部角度多样性和身份一致性，进一步从开源数据集中构建了LFA数据集。

Result: 基于LFA数据集的实验结果表明，提出的方法在面部相似度、Face FID和CLIP语义对齐等指标上显著优于现有方法。

Conclusion: 通过MoFE模型和LFA数据集，本文为大面部角度下的视频生成提供了新的解决方法，显著提升了生成效果，并公开了相关代码和数据集以供研究社区使用。

Abstract: Current video generation models struggle with identity preservation under
large facial angles, primarily facing two challenges: the difficulty in
exploring an effective mechanism to integrate identity features into DiT
structure, and the lack of targeted coverage of large facial angles in existing
open-source video datasets. To address these, we present two key innovations.
First, we introduce a Mixture of Facial Experts (MoFE) that dynamically
combines complementary cues from three specialized experts, each designed to
capture distinct but mutually reinforcing aspects of facial attributes. The
identity expert captures cross-pose identity-sensitive features, the semantic
expert extracts high-level visual semantxics, and the detail expert preserves
pixel-level features (e.g., skin texture, color gradients). Furthermore, to
mitigate dataset limitations, we have tailored a data processing pipeline
centered on two key aspects: Face Constraints and Identity Consistency. Face
Constraints ensure facial angle diversity and a high proportion of facial
regions, while Identity Consistency preserves coherent person-specific features
across temporal sequences, collectively addressing the scarcity of large facial
angles and identity-stable training data in existing datasets. Leveraging this
pipeline, we have curated and refined a Large Face Angles (LFA) Dataset from
existing open-source human video datasets, comprising 460K video clips with
annotated facial angles. Experimental results on the LFA benchmark demonstrate
that our method, empowered by the LFA dataset, significantly outperforms prior
SOTA methods in face similarity, face FID, and CLIP semantic alignment. The
code and dataset will be made publicly available at
https://github.com/rain152/LFA-Video-Generation.

</details>


### [42] [CLIP-Flow: A Universal Discriminator for AI-Generated Images Inspired by Anomaly Detection](https://arxiv.org/abs/2508.09477)
*Zhipeng Yuan,Kai Wang,Weize Quan,Dong-Ming Yan,Tieru Wu*

Main category: cs.CV

TL;DR: 本文提出了一种基于异常检测的通用AI生成图像检测器，无需访问AI生成图像，通过无监督学习通用化表征。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成模型的快速发展，AI生成图像的视觉质量与自然图像愈发接近，带来了安全隐患；现有检测方法难以应对来自未知生成模型的AI生成图像。

Method: 利用预训练的CLIP编码器作为特征提取器，设计了一种类似正则流的无监督模型，用光谱修改后的代理图片进行训练，通过最小化代理图片的似然值来实现检测。

Result: 实验证明，该方法在多种生成模型生产的图像上表现出色。

Conclusion: 本研究提出的方法无需AI生成数据训练，提高了对未见生成模型的通用检测能力。

Abstract: With the rapid advancement of AI generative models, the visual quality of
AI-generated images (AIIs) has become increasingly close to natural images,
which inevitably raises security concerns. Most AII detectors often employ the
conventional image classification pipeline with natural images and AIIs
(generated by a generative model), which can result in limited detection
performance for AIIs from unseen generative models. To solve this, we proposed
a universal AI-generated image detector from the perspective of anomaly
detection. Our discriminator does not need to access any AIIs and learn a
generalizable representation with unsupervised learning. Specifically, we use
the pre-trained CLIP encoder as the feature extractor and design a normalizing
flow-like unsupervised model. Instead of AIIs, proxy images, e.g., obtained by
applying a spectral modification operation on natural images, are used for
training. Our models are trained by minimizing the likelihood of proxy images,
optionally combined with maximizing the likelihood of natural images. Extensive
experiments demonstrate the effectiveness of our method on AIIs produced by
various image generators.

</details>


### [43] [GazeLT: Visual attention-guided long-tailed disease classification in chest radiographs](https://arxiv.org/abs/2508.09478)
*Moinak Bhattacharya,Gagandeep Singh,Shubham Jain,Prateek Prasanna*

Main category: cs.CV

TL;DR: 提出了GazeLT，通过整合和解构机制利用放射科医生的目光信息改进长尾疾病分类，在两种公开数据集上显示显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 改善深度学习框架对医用影像的自动解读，尤其是长尾疾病的分类问题。

Method: 通过集成并解构放射科医生的目光注意模式，将其纳入深度学习流程，以利用其捕捉细粒度及粗粒度的疾病信息能力。

Result: 在NIH-CXR-LT和MIMIC-CXR-LT数据集上，相比现有最优长尾损失和视觉注意基线，GazeLT分别提高了4.1%和21.7%的平均准确率。

Conclusion: GazeLT实现了通过整合目光注意机制显著提升长尾疾病分类的性能，展现了该方法的潜力。

Abstract: In this work, we present GazeLT, a human visual attention
integration-disintegration approach for long-tailed disease classification. A
radiologist's eye gaze has distinct patterns that capture both fine-grained and
coarser level disease related information. While interpreting an image, a
radiologist's attention varies throughout the duration; it is critical to
incorporate this into a deep learning framework to improve automated image
interpretation. Another important aspect of visual attention is that apart from
looking at major/obvious disease patterns, experts also look at
minor/incidental findings (few of these constituting long-tailed classes)
during the course of image interpretation. GazeLT harnesses the temporal aspect
of the visual search process, via an integration and disintegration mechanism,
to improve long-tailed disease classification. We show the efficacy of GazeLT
on two publicly available datasets for long-tailed disease classification,
namely the NIH-CXR-LT (n=89237) and the MIMIC-CXR-LT (n=111898) datasets.
GazeLT outperforms the best long-tailed loss by 4.1% and the visual
attention-based baseline by 21.7% in average accuracy metrics for these
datasets. Our code is available at https://github.com/lordmoinak1/gazelt.

</details>


### [44] [SkySplat: Generalizable 3D Gaussian Splatting from Multi-Temporal Sparse Satellite Images](https://arxiv.org/abs/2508.09479)
*Xuejun Huang,Xinyi Liu,Yi Wan,Zhi Zheng,Bin Zhang,Mingtao Xiong,Yingying Pei,Yongjun Zhang*

Main category: cs.CV

TL;DR: SkySplat开发了一种新的3D重建框架，显著提升了稀疏卫星图像重建的效率和精度，减少了对地面真实高度图的需求。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有三维场景重建方法在稀疏视图卫星图像中泛化能力差和几何限制不足的问题。

Method: 提出SkySplat框架，结合RPC模型和自监督学习，利用Cross-Self Consistency Module (CSCM)减少瞬时物体干扰，并通过多视图一致性聚合策略优化重建结果。

Result: SkySplat比传统方法快86倍，能显著提高卫星图像重建的精度，MAE从13.18 m降低到1.80 m，并在多个数据集上表现出良好的泛化能力。

Conclusion: SkySplat框架在卫星稀疏图像的高效、高精度三维重建中展示了其优越性，对未来相关领域具有重要意义。

Abstract: Three-dimensional scene reconstruction from sparse-view satellite images is a
long-standing and challenging task. While 3D Gaussian Splatting (3DGS) and its
variants have recently attracted attention for its high efficiency, existing
methods remain unsuitable for satellite images due to incompatibility with
rational polynomial coefficient (RPC) models and limited generalization
capability. Recent advances in generalizable 3DGS approaches show potential,
but they perform poorly on multi-temporal sparse satellite images due to
limited geometric constraints, transient objects, and radiometric
inconsistencies. To address these limitations, we propose SkySplat, a novel
self-supervised framework that integrates the RPC model into the generalizable
3DGS pipeline, enabling more effective use of sparse geometric cues for
improved reconstruction. SkySplat relies only on RGB images and
radiometric-robust relative height supervision, thereby eliminating the need
for ground-truth height maps. Key components include a Cross-Self Consistency
Module (CSCM), which mitigates transient object interference via
consistency-based masking, and a multi-view consistency aggregation strategy
that refines reconstruction results. Compared to per-scene optimization
methods, SkySplat achieves an 86 times speedup over EOGS with higher accuracy.
It also outperforms generalizable 3DGS baselines, reducing MAE from 13.18 m to
1.80 m on the DFC19 dataset significantly, and demonstrates strong
cross-dataset generalization on the MVS3D benchmark.

</details>


### [45] [Episodic Memory Representation for Long-form Video Understanding](https://arxiv.org/abs/2508.09486)
*Yun Wang,Long Zhang,Jingren Liu,Jiaqi Yan,Zhanjie Zhang,Jiahao Zheng,Xun Yang,Dapeng Wu,Xiangyu Chen,Xuelong Li*

Main category: cs.CV

TL;DR: Video-EM 是一个灵感源于人类情节记忆的框架，用于解决 Video-LLMs 在处理长视频时面临的上下文窗口限制问题，通过对帧的时序建模和连贯的上下文推理，显著提升了视频问答表现。


<details>
  <summary>Details</summary>
Motivation: 由于长视频包含大量的帧，现有的方法倾向于选择少量关键帧以应对视频上下文窗口限制。但这些选帧方法往往忽略了空间和时间的动态关系，从而影响问答的效果。

Method: 提出了名为 Video-EM 的框架，该方法基于类似人类情节记忆的理论，通过时序建模关键帧以捕捉空间和时间动态。并使用了链式思维（CoT）来让语言模型迭代选择信息更丰富的帧片段。

Result: 在Video-MME、EgoSchema、HourVideo 和 LVBench 数据集的评估中，Video-EM 相较基线方法性能提高了4-9个百分点，并使用了更少的帧。

Conclusion: Video-EM 显示出其优越性，成功解决了冗余关键帧和上下文连续性问题，为 Video-LLMs 的视频问答提供了更高效的解决方案。

Abstract: Video Large Language Models (Video-LLMs) excel at general video understanding
but struggle with long-form videos due to context window limits. Consequently,
recent approaches focus on keyframe retrieval, condensing lengthy videos into a
small set of informative frames. Despite their practicality, these methods
simplify the problem to static text image matching, overlooking spatio temporal
relationships crucial for capturing scene transitions and contextual
continuity, and may yield redundant keyframes with limited information,
diluting salient cues essential for accurate video question answering. To
address these limitations, we introduce Video-EM, a training free framework
inspired by the principles of human episodic memory, designed to facilitate
robust and contextually grounded reasoning. Rather than treating keyframes as
isolated visual entities, Video-EM explicitly models them as temporally ordered
episodic events, capturing both spatial relationships and temporal dynamics
necessary for accurately reconstructing the underlying narrative. Furthermore,
the framework leverages chain of thought (CoT) thinking with LLMs to
iteratively identify a minimal yet highly informative subset of episodic
memories, enabling efficient and accurate question answering by Video-LLMs.
Extensive evaluations on the Video-MME, EgoSchema, HourVideo, and LVBench
benchmarks confirm the superiority of Video-EM, which achieves highly
competitive results with performance gains of 4-9 percent over respective
baselines while utilizing fewer frames.

</details>


### [46] [SARE: Semantic-Aware Reconstruction Error for Generalizable Diffusion-Generated Image Detection](https://arxiv.org/abs/2508.09487)
*Ju Yeon Kang,Jaehong Park,Semin Kim,Ji Won Yoon,Nam Soo Kim*

Main category: cs.CV

TL;DR: 本论文提出一种用于检测基于扩散模型生成的伪造图像的新方法。


<details>
  <summary>Details</summary>
Motivation: 由于扩散模型的快速发展，其生成的伪造图像可能被滥用，且现有检测方法在应对未知生成模型时表现较差，新的检测方法显得尤为重要。

Method: 提出了一种新的表示方法“语义感知重建误差”（SARE），通过衡量图像与其由描述重建的差异来检测图像的真实性。

Result: 在GenImage和CommunityForensics等基准测试上实验表明，该方法在检测多样生成模型的伪造图像方面表现优于现有基线方法。

Conclusion: SARE方法能够有效检测来自不同生成模型的伪造图像，显示了良好的泛化能力。

Abstract: Recently, diffusion-generated image detection has gained increasing
attention, as the rapid advancement of diffusion models has raised serious
concerns about their potential misuse. While existing detection methods have
achieved promising results, their performance often degrades significantly when
facing fake images from unseen, out-of-distribution (OOD) generative models,
since they primarily rely on model-specific artifacts. To address this
limitation, we explore a fundamental property commonly observed in fake images.
Motivated by the observation that fake images tend to exhibit higher similarity
to their captions than real images, we propose a novel representation, namely
Semantic-Aware Reconstruction Error (SARE), that measures the semantic
difference between an image and its caption-guided reconstruction. The
hypothesis behind SARE is that real images, whose captions often fail to fully
capture their complex visual content, may undergo noticeable semantic shifts
during the caption-guided reconstruction process. In contrast, fake images,
which closely align with their captions, show minimal semantic changes. By
quantifying these semantic shifts, SARE can be utilized as a discriminative
feature for robust detection across diverse generative models. We empirically
demonstrate that the proposed method exhibits strong generalization,
outperforming existing baselines on benchmarks including GenImage and
CommunityForensics.

</details>


### [47] [CWFBind: Geometry-Awareness for Fast and Accurate Protein-Ligand Docking](https://arxiv.org/abs/2508.09499)
*Liyan Jia,Chuan-Xian Ren,Hong Yan*

Main category: cs.CV

TL;DR: CWFBind是一种基于局部曲率特征的快速精确对接方法，用于预测小分子配体与蛋白质靶标的结合构象，表现在几何表示和模型性能的提升。


<details>
  <summary>Details</summary>
Motivation: 克服传统方法中在速度和准确性上的不足，同时解决现有深度学习方法忽视几何信息带来的问题。

Method: 利用局部曲率描述子丰富几何特征表征，结合度感知权重机制加强结构与相互作用的捕捉，并采用配体感知动态半径策略与增强损失函数解决类别不平衡挑战。

Result: 通过实验验证，CWFBind在多个对接基准上表现出竞争力，具备准确性与效率的良好平衡。

Conclusion: CWFBind通过几何与结构特征增强改进了结合区域和关键残基的预测，展现了在药物设计中的潜力。

Abstract: Accurately predicting the binding conformation of small-molecule ligands to
protein targets is a critical step in rational drug design. Although recent
deep learning-based docking surpasses traditional methods in speed and
accuracy, many approaches rely on graph representations and language
model-inspired encoders while neglecting critical geometric information,
resulting in inaccurate pocket localization and unrealistic binding
conformations. In this study, we introduce CWFBind, a weighted, fast, and
accurate docking method based on local curvature features. Specifically, we
integrate local curvature descriptors during the feature extraction phase to
enrich the geometric representation of both proteins and ligands, complementing
existing chemical, sequence, and structural features. Furthermore, we embed
degree-aware weighting mechanisms into the message passing process, enhancing
the model's ability to capture spatial structural distinctions and interaction
strengths. To address the class imbalance challenge in pocket prediction,
CWFBind employs a ligand-aware dynamic radius strategy alongside an enhanced
loss function, facilitating more precise identification of binding regions and
key residues. Comprehensive experimental evaluations demonstrate that CWFBind
achieves competitive performance across multiple docking benchmarks, offering a
balanced trade-off between accuracy and efficiency.

</details>


### [48] [Generation of Indian Sign Language Letters, Numbers, and Words](https://arxiv.org/abs/2508.09522)
*Ajeet Kumar Yadav,Nishant Kumar,Rathna G N*

Main category: cs.CV

TL;DR: 本文探讨了一种结合ProGAN和SAGAN优势的新型生成对抗网络，用于生成高质量印度手语的字母、数字和单词的图像。


<details>
  <summary>Details</summary>
Motivation: 填补手语生成的研究空白，加强听力障碍人与普通人之间的沟通。

Method: 提出了一种结合ProGAN和SAGAN的生成对抗网络变体，强调高分辨率和细节平衡，生成高质量的印度手语图像。

Result: 新模型在生成印度手语字母及相关图像中，在Inception Score (IS) 和Fréchet Inception Distance (FID) 上分别提升了3.2和30.12。

Conclusion: 新提出的模型在生成高质量、特征丰富的手语图像上表现显著优于传统方法，并将公开高质量的印度手语图像数据集。

Abstract: Sign language, which contains hand movements, facial expressions and bodily
gestures, is a significant medium for communicating with hard-of-hearing
people. A well-trained sign language community communicates easily, but those
who don't know sign language face significant challenges. Recognition and
generation are basic communication methods between hearing and hard-of-hearing
individuals. Despite progress in recognition, sign language generation still
needs to be explored. The Progressive Growing of Generative Adversarial Network
(ProGAN) excels at producing high-quality images, while the Self-Attention
Generative Adversarial Network (SAGAN) generates feature-rich images at medium
resolutions. Balancing resolution and detail is crucial for sign language image
generation. We are developing a Generative Adversarial Network (GAN) variant
that combines both models to generate feature-rich, high-resolution, and
class-conditional sign language images. Our modified Attention-based model
generates high-quality images of Indian Sign Language letters, numbers, and
words, outperforming the traditional ProGAN in Inception Score (IS) and
Fr\'echet Inception Distance (FID), with improvements of 3.2 and 30.12,
respectively. Additionally, we are publishing a large dataset incorporating
high-quality images of Indian Sign Language alphabets, numbers, and 129 words.

</details>


### [49] [SOI is the Root of All Evil: Quantifying and Breaking Similar Object Interference in Single Object Tracking](https://arxiv.org/abs/2508.09524)
*Yipei Wang,Shiyu Hu,Shukun Jia,Panxi Xu,Hongfei Ma,Yiping Ma,Jing Zhang,Xiaobo Lu,Xin Zhao*

Main category: cs.CV

TL;DR: 本文首次系统性地研究并量化了单目标跟踪（SOT）中的相似物体干扰（SOI），提出了SOIBench基准并验证了通过大规模视觉语言模型改进跟踪性能的可能性。


<details>
  <summary>Details</summary>
Motivation: 探讨单目标跟踪中的相似物体干扰（SOI）问题，验证其对跟踪性能的制约作用，并提出如何利用外部认知引导解决SOI挑战。

Method: 通过在线干扰屏蔽实验（OIM）验证消除干扰源对性能的提升，构建SOIBench基准，通过多级标注协议生成精确语义引导文本，提出新范式利用大规模视觉语言模型作为外部认知引擎提升跟踪效果。

Result: 消除干扰源可显著提升性能（AUC最多提高4.35），而现有视觉语言跟踪方法在语义认知引导下成效有限，改进幅度为-0.26至+0.71。提出的新范式通过语义认知引导可显著提升效果（AUC最多提升0.93）。

Conclusion: SOIBench为语义认知跟踪研究提供了标准化评估平台，并展示了大规模视觉语言模型在认知引导中的潜力，有望推动跟踪研究的发展。

Abstract: In this paper, we present the first systematic investigation and
quantification of Similar Object Interference (SOI), a long-overlooked yet
critical bottleneck in Single Object Tracking (SOT). Through controlled Online
Interference Masking (OIM) experiments, we quantitatively demonstrate that
eliminating interference sources leads to substantial performance improvements
(AUC gains up to 4.35) across all SOTA trackers, directly validating SOI as a
primary constraint for robust tracking and highlighting the feasibility of
external cognitive guidance. Building upon these insights, we adopt natural
language as a practical form of external guidance, and construct SOIBench-the
first semantic cognitive guidance benchmark specifically targeting SOI
challenges. It automatically mines SOI frames through multi-tracker collective
judgment and introduces a multi-level annotation protocol to generate precise
semantic guidance texts. Systematic evaluation on SOIBench reveals a striking
finding: existing vision-language tracking (VLT) methods fail to effectively
exploit semantic cognitive guidance, achieving only marginal improvements or
even performance degradation (AUC changes of -0.26 to +0.71). In contrast, we
propose a novel paradigm employing large-scale vision-language models (VLM) as
external cognitive engines that can be seamlessly integrated into arbitrary RGB
trackers. This approach demonstrates substantial improvements under semantic
cognitive guidance (AUC gains up to 0.93), representing a significant
advancement over existing VLT methods. We hope SOIBench will serve as a
standardized evaluation platform to advance semantic cognitive tracking
research and contribute new insights to the tracking research community.

</details>


### [50] [Learning Spatial Decay for Vision Transformers](https://arxiv.org/abs/2508.09525)
*Yuxin Mao,Zhen Qin,Jinxing Zhou,Bin Fan,Jing Zhang,Yiran Zhong,Yuchao Dai*

Main category: cs.CV

TL;DR: 空间衰减变压器（SDT）通过引入上下文感知门控机制（CAG）改进了视觉变压器的空间注意力机制，从而显著提升了其在分类和生成任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统视觉变压器（ViTs）的自注意力机制缺乏明确的空间归纳偏置，限制了其在具备空间结构的任务中的表现。现有方法采用数据无关的固定距离度量方法进行空间衰减，但限制了其在多样化视觉场景中的自适应能力。

Method: 提出了空间衰减变压器（SDT），引入一种新颖的上下文感知门控机制（CAG），通过动态的、数据依赖的方式对空间注意力进行调节。方法还结合曼哈顿距离的空间先验与学习到的内容表示，形成了统一的空间-内容融合框架，从而实现了从1D到2D的适配。

Result: 在ImageNet-1K分类和生成任务中，实验结果表明，与强基线模型相比，提出方法取得了一致的性能提升。

Conclusion: 数据依赖的空间衰减被确立为增强视觉变压器空间注意力的新范式，同时展示了非静态机制在视觉任务中的优越表现。

Abstract: Vision Transformers (ViTs) have revolutionized computer vision, yet their
self-attention mechanism lacks explicit spatial inductive biases, leading to
suboptimal performance on spatially-structured tasks. Existing approaches
introduce data-independent spatial decay based on fixed distance metrics,
applying uniform attention weighting regardless of image content and limiting
adaptability to diverse visual scenarios. Inspired by recent advances in large
language models where content-aware gating mechanisms (e.g., GLA, HGRN2, FOX)
significantly outperform static alternatives, we present the first successful
adaptation of data-dependent spatial decay to 2D vision transformers. We
introduce \textbf{Spatial Decay Transformer (SDT)}, featuring a novel
Context-Aware Gating (CAG) mechanism that generates dynamic, data-dependent
decay for patch interactions. Our approach learns to modulate spatial attention
based on both content relevance and spatial proximity. We address the
fundamental challenge of 1D-to-2D adaptation through a unified spatial-content
fusion framework that integrates manhattan distance-based spatial priors with
learned content representations. Extensive experiments on ImageNet-1K
classification and generation tasks demonstrate consistent improvements over
strong baselines. Our work establishes data-dependent spatial decay as a new
paradigm for enhancing spatial attention in vision transformers.

</details>


### [51] [COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets](https://arxiv.org/abs/2508.09886)
*Lingyu Chen,Yawen Zeng,Yue Wang,Peng Wan,Guo-chen Ning,Hongen Liao,Daoqiang Zhang,Fang Chen*

Main category: cs.CV

TL;DR: 提出了一种通用的专家协同模型（COME），能够在多种超声（US）数据集中实现鲁棒的图像分析与特征提取，有效解决数据干扰和特征保留问题。


<details>
  <summary>Details</summary>
Motivation: 传统的单数据集训练在面对新数据分布时表现不佳，尤其在US图像分析中，由于数据有限、声影和散斑噪声的存在。因此，需要构建一个适用于多异构US数据集的通用框架，以支持更鲁棒的下游任务。

Method: 提出COME模型，通过结构-语义共享专家构建统一的表示空间，并结合数据特定的专家提取具有判别性的特征，实现跨数据集的经验协作与特征互补，以增强泛化能力。

Result: 在单数据集、同器官和跨器官数据集整合三种评价模式下，实验结果表明COME模型相比现有方法实现了显著的平均AP性能提升。

Conclusion: COME模型能够有效解决多US数据集的特征提取与泛化问题，促进了在小批量或未见数据场景下的鲁棒表现。

Abstract: Conventional single-dataset training often fails with new data distributions,
especially in ultrasound (US) image analysis due to limited data, acoustic
shadows, and speckle noise. Therefore, constructing a universal framework for
multi-heterogeneous US datasets is imperative. However, a key challenge arises:
how to effectively mitigate inter-dataset interference while preserving
dataset-specific discriminative features for robust downstream task? Previous
approaches utilize either a single source-specific decoder or a domain
adaptation strategy, but these methods experienced a decline in performance
when applied to other domains. Considering this, we propose a Universal
Collaborative Mixture of Heterogeneous Source-Specific Experts (COME).
Specifically, COME establishes dual structure-semantic shared experts that
create a universal representation space and then collaborate with
source-specific experts to extract discriminative features through providing
complementary features. This design enables robust generalization by leveraging
cross-datasets experience distributions and providing universal US priors for
small-batch or unseen data scenarios. Extensive experiments under three
evaluation modes (single-dataset, intra-organ, and inter-organ integration
datasets) demonstrate COME's superiority, achieving significant mean AP
improvements over state-of-the-art methods. Our project is available at:
https://universalcome.github.io/UniversalCOME/.

</details>


### [52] [Physics-guided Deep Unfolding Network for Enhanced Kronecker Compressive sensing](https://arxiv.org/abs/2508.09528)
*Gang Qu,Ping Wang,Siming Zheng,Xin Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种用于图像压缩感知的新模型AKCS和一种测量感知交叉注意机制MACA，并将其集成到MEUNet，取得了在重建精度和推理速度上的领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在压缩测量的非相干性和隐性测量表示方面存在不足，限制了整体性能。作者提出提升测量非相干性和学习测量的代表性表示的改进策略。

Method: 提出一种新的非对称Kronecker压缩感知模型（AKCS），并基于展开网络引入测量感知交叉注意机制（MACA），将两者整合到MEUNet中。

Result: 通过大量实验验证，MEUNet在图像重建精度和推理速度上均达到了当前最优水平。

Conclusion: 新模型和机制有效提高了压缩测量相干性，利用显式和隐式测量表示，提高了图像重建性能。

Abstract: Deep networks have achieved remarkable success in image compressed sensing
(CS) task, namely reconstructing a high-fidelity image from its compressed
measurement. However, existing works are deficient inincoherent compressed
measurement at sensing phase and implicit measurement representations at
reconstruction phase, limiting the overall performance. In this work, we answer
two questions: 1) how to improve the measurement incoherence for decreasing the
ill-posedness; 2) how to learn informative representations from measurements.
To this end, we propose a novel asymmetric Kronecker CS (AKCS) model and
theoretically present its better incoherence than previous Kronecker CS with
minimal complexity increase. Moreover, we reveal that the unfolding networks'
superiority over non-unfolding ones result from sufficient gradient descents,
called explicit measurement representations. We propose a measurement-aware
cross attention (MACA) mechanism to learn implicit measurement representations.
We integrate AKCS and MACA into widely-used unfolding architecture to get a
measurement-enhanced unfolding network (MEUNet). Extensive experiences
demonstrate that our MEUNet achieves state-of-the-art performance in
reconstruction accuracy and inference speed.

</details>


### [53] [Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation](https://arxiv.org/abs/2508.09987)
*Junyan Ye,Dongzhi Jiang,Zihao Wang,Leqi Zhu,Zhenghao Hu,Zilong Huang,Jun He,Zhiyuan Yan,Jinghua Yu,Hongsheng Li,Conghui He,Weijia Li*

Main category: cs.CV

TL;DR: 研究通过GPT-4o生成的合成图像数据集来提高开源模型性能，并提出两个新的评估基准。


<details>
  <summary>Details</summary>
Motivation: 探讨为什么在已有高质量的现实世界图像数据集的情况下仍需要GPT-4o生成的合成数据。

Method: 通过GPT-4o生成一套180K规模的合成数据集Echo-4o-Image，并用其对多模态生成基线Bagel进行微调。同时设计了两个新评估基准，GenEval++和Imagine-Bench。

Result: Echo-4o在标准基准上表现优异，并对其他基础模型提供强大的迁移性能。

Conclusion: 合成图像数据能够补充现实数据盲点，为文本到图像任务提供更准确的监督信号，提升开源多模态生成模型的性能。

Abstract: Recently, GPT-4o has garnered significant attention for its strong
performance in image generation, yet open-source models still lag behind.
Several studies have explored distilling image data from GPT-4o to enhance
open-source models, achieving notable progress. However, a key question
remains: given that real-world image datasets already constitute a natural
source of high-quality data, why should we use GPT-4o-generated synthetic data?
In this work, we identify two key advantages of synthetic images. First, they
can complement rare scenarios in real-world datasets, such as surreal fantasy
or multi-reference image generation, which frequently occur in user queries.
Second, they provide clean and controllable supervision. Real-world data often
contains complex background noise and inherent misalignment between text
descriptions and image content, whereas synthetic images offer pure backgrounds
and long-tailed supervision signals, facilitating more accurate text-to-image
alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale
synthetic dataset generated by GPT-4o, harnessing the power of synthetic image
data to address blind spots in real-world coverage. Using this dataset, we
fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.
In addition, we propose two new evaluation benchmarks for a more accurate and
challenging assessment of image generation capabilities: GenEval++, which
increases instruction complexity to mitigate score saturation, and
Imagine-Bench, which focuses on evaluating both the understanding and
generation of imaginative content. Echo-4o demonstrates strong performance
across standard benchmarks. Moreover, applying Echo-4o-Image to other
foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains
across multiple metrics, highlighting the datasets strong transferability.

</details>


### [54] [COXNet: Cross-Layer Fusion with Adaptive Alignment and Scale Integration for RGBT Tiny Object Detection](https://arxiv.org/abs/2508.09533)
*Peiran Peng,Tingfa Xu,Liqiang Song,Mengqi Zhu,Yuqiang Fang,Jianan Li*

Main category: cs.CV

TL;DR: 该论文开发了一种新框架COXNet，用于RGBT图像中的小目标检测，尤其是无人机应用中的复杂场景，显著提升了检测表现。


<details>
  <summary>Details</summary>
Motivation: 当前在RGBT图像中检测小目标面临多种挑战，如跨模态对齐不准确、低光照、遮挡及复杂背景等，现有方法难以有效利用可见光与热成像的互补信息。

Method: 提出COXNet框架，包括三个核心创新：跨层融合模块用于增强语义与空间精度，动态对齐与比例精炼模块校正跨模态的空间错位并保持多尺度特征，以及通过GeoShape相似度优化标签分配策略。

Result: 在RGBTDronePerson数据集上，COXNet将mAP$_{50}$提升了3.32%，优于现有方法。

Conclusion: COXNet在复杂环境下的RGBT小目标检测中表现卓越，有效解决了非对齐、低光及遮挡等问题，为相关领域提供了新方向。

Abstract: Detecting tiny objects in multimodal Red-Green-Blue-Thermal (RGBT) imagery is
a critical challenge in computer vision, particularly in surveillance, search
and rescue, and autonomous navigation. Drone-based scenarios exacerbate these
challenges due to spatial misalignment, low-light conditions, occlusion, and
cluttered backgrounds. Current methods struggle to leverage the complementary
information between visible and thermal modalities effectively. We propose
COXNet, a novel framework for RGBT tiny object detection, addressing these
issues through three core innovations: i) the Cross-Layer Fusion Module, fusing
high-level visible and low-level thermal features for enhanced semantic and
spatial accuracy; ii) the Dynamic Alignment and Scale Refinement module,
correcting cross-modal spatial misalignments and preserving multi-scale
features; and iii) an optimized label assignment strategy using the GeoShape
Similarity Measure for better localization. COXNet achieves a 3.32\% mAP$_{50}$
improvement on the RGBTDronePerson dataset over state-of-the-art methods,
demonstrating its effectiveness for robust detection in complex environments.

</details>


### [55] [Iterative Volume Fusion for Asymmetric Stereo Matching](https://arxiv.org/abs/2508.09543)
*Yuanting Gao,Linghao Shen*

Main category: cs.CV

TL;DR: 提出一种新的两阶段体积融合网络(IVF-AStereo)，专注解决具有视觉不对称性的双目立体匹配问题。


<details>
  <summary>Details</summary>
Motivation: 由于非对称多摄像机系统的兴起及其引起的视觉不对称性，使得传统假定视觉对称性的双目立体匹配算法受到挑战。

Method: 研究现有的两种成本体积构建方法在非对称立体匹配中的信息扭曲特征，提出两阶段的人工体积融合网络(IVF-AStereo)，通过逐步融合拼接体积和关联体积，提升匹配细节。

Result: 实验表明，提出的方法在分辨率和色彩降解引起的视觉不对称性条件下，表现优越，并在基准数据集的对比实验和消融研究中得到验证。

Conclusion: IVF-AStereo网络可以有效解决视觉不对称性问题，在处理严重视觉不对称性时表现出鲁棒性，具有重要的研究价值。

Abstract: Stereo matching is vital in 3D computer vision, with most algorithms assuming
symmetric visual properties between binocular visions. However, the rise of
asymmetric multi-camera systems (e.g., tele-wide cameras) challenges this
assumption and complicates stereo matching. Visual asymmetry disrupts stereo
matching by affecting the crucial cost volume computation. To address this, we
explore the matching cost distribution of two established cost volume
construction methods in asymmetric stereo. We find that each cost volume
experiences distinct information distortion, indicating that both should be
comprehensively utilized to solve the issue. Based on this, we propose the
two-phase Iterative Volume Fusion network for Asymmetric Stereo matching
(IVF-AStereo). Initially, the aggregated concatenation volume refines the
correlation volume. Subsequently, both volumes are fused to enhance fine
details. Our method excels in asymmetric scenarios and shows robust performance
against significant visual asymmetry. Extensive comparative experiments on
benchmark datasets, along with ablation studies, confirm the effectiveness of
our approach in asymmetric stereo with resolution and color degradation.

</details>


### [56] [GoViG: Goal-Conditioned Visual Navigation Instruction Generation](https://arxiv.org/abs/2508.09547)
*Fengyi Wu,Yifei Dong,Zhi-Qi Cheng,Yilong Dai,Guangyu Chen,Hang Wang,Qi Dai,Alexander G. Hauptmann*

Main category: cs.CV

TL;DR: 该论文提出新任务GoViG，通过初始和目标状态的自我视觉观测，自主生成精确导航指令。其显著特点是不依赖结构化输入，仅利用原始视觉数据提高适应性。


<details>
  <summary>Details</summary>
Motivation: 现有导航指令生成方法依赖语义注释或环境地图，不适应未见和非结构化环境。提出GoViG以提升适应性，专注于视觉导航任务。

Method: 方法将任务分为视觉预测和指令生成两部分，采用自回归多模态语言模型进行集成，并引入单次和交替推理策略，模拟增量式人类认知。

Result: 提出了R2R-Goal数据集以验证方法，实验表明其BLEU-4和CIDEr分数显著优于现有方法，跨领域泛化能力强。

Conclusion: GoViG展示了视觉驱动的导航指令生成能力，无需结构化输入，证明了其适应性和泛化性强的特点。

Abstract: We introduce Goal-Conditioned Visual Navigation Instruction Generation
(GoViG), a new task that aims to autonomously generate precise and contextually
coherent navigation instructions solely from egocentric visual observations of
initial and goal states. Unlike conventional approaches that rely on structured
inputs such as semantic annotations or environmental maps, GoViG exclusively
leverages raw egocentric visual data, substantially improving its adaptability
to unseen and unstructured environments. Our method addresses this task by
decomposing it into two interconnected subtasks: (1) visual forecasting, which
predicts intermediate visual states bridging the initial and goal views; and
(2) instruction generation, which synthesizes linguistically coherent
instructions grounded in both observed and anticipated visuals. These subtasks
are integrated within an autoregressive multimodal large language model trained
with tailored objectives to ensure spatial accuracy and linguistic clarity.
Furthermore, we introduce two complementary multimodal reasoning strategies,
one-pass and interleaved reasoning, to mimic incremental human cognitive
processes during navigation. To evaluate our method, we propose the R2R-Goal
dataset, combining diverse synthetic and real-world trajectories. Empirical
results demonstrate significant improvements over state-of-the-art methods,
achieving superior BLEU-4 and CIDEr scores along with robust cross-domain
generalization.

</details>


### [57] [Exploring the Equivalence of Closed-Set Generative and Real Data Augmentation in Image Classification](https://arxiv.org/abs/2508.09550)
*Haowen Wang,Guowei Zhang,Xiang Zhang,Zeyuan Chen,Haiyang Xu,Dou Hoon Kwark,Zhuowen Tu*

Main category: cs.CV

TL;DR: 本文探讨如何利用生成模型对图像分类任务数据集进行增强，从而提高分类性能，并系统分析了闭集生成数据的有效使用方法及其对分类效果的影响。


<details>
  <summary>Details</summary>
Motivation: 研究机器学习中如何通过生成数据来增强闭集合图像分类任务的表现，探索真实图像与生成图像数据的区别与联系。

Method: 通过实验确立闭集合生成数据增强的等效规模，量化真实数据增强与开放集合生成数据增强之间的等效关系，并在自然数据集和医学图像数据集中进行验证。

Result: 验证生成图像与实际图像增强间的差异，提出在不同数据规模下实现匹配分类性能所需的生成数据规模测量方案。

Conclusion: 尽管真实图像通常效果更佳，但研究提供了生成图像增强的等效规模指导，为基于生成数据的分类提升提供理论依据。

Abstract: In this paper, we address a key scientific problem in machine learning: Given
a training set for an image classification task, can we train a generative
model on this dataset to enhance the classification performance? (i.e.,
closed-set generative data augmentation). We start by exploring the
distinctions and similarities between real images and closed-set synthetic
images generated by advanced generative models. Through extensive experiments,
we offer systematic insights into the effective use of closed-set synthetic
data for augmentation. Notably, we empirically determine the equivalent scale
of synthetic images needed for augmentation. In addition, we also show
quantitative equivalence between the real data augmentation and open-set
generative augmentation (generative models trained using data beyond the given
training set). While it aligns with the common intuition that real images are
generally preferred, our empirical formulation also offers a guideline to
quantify the increased scale of synthetic data augmentation required to achieve
comparable image classification performance. Our results on natural and medical
image datasets further illustrate how this effect varies with the baseline
training set size and the amount of synthetic data incorporated.

</details>


### [58] [Topological Invariant-Based Iris Identification via Digital Homology and Machine Learning](https://arxiv.org/abs/2508.09555)
*Ahmet Öztel,İsmet Karaca*

Main category: cs.CV

TL;DR: 本研究提出了一种基于二维虹膜图像拓扑不变量的生物识别方法，使用数字同调计算纹理特征，并通过逻辑回归、KNN、SVM等方法进行分类，显示出高准确率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过形式定义的数字同调从二维虹膜图像中提取拓扑特征，以实现高效、可解释的生物识别，填补深度学习方法在数据有限或需要高可解释性情况下的不足。

Method: 将标准化的虹膜图像分为若干小网格区域，自每个区域提取 Betti0、Betti1 及其比值的拓扑不变量，并将其组成特征矩阵，结合逻辑回归、KNN 和 SVM 进行分类，并与 CNN 方法作对比。

Result: 逻辑回归达到 97.78 ± 0.82% 的分类准确率，优于 CNN 的 96.44 ± 1.32%，且表现出低方差和高稳定性。

Conclusion: 首次将正式数字同调的拓扑不变量用于虹膜识别，提出了一种解释性强、效率高且适应性强的替代方案，适用于数据有限或需高可解释性的应用场景，未来可推广至其他领域如医学成像、遥感等。

Abstract: Objective - This study presents a biometric identification method based on
topological invariants from 2D iris images, representing iris texture via
formally defined digital homology and evaluating classification performance.
  Methods - Each normalized iris image (48x482 pixels) is divided into grids
(e.g., 6x54 or 3x27). For each subregion, we compute Betti0, Betti1, and their
ratio using a recent algorithm for homology groups in 2D digital images. The
resulting invariants form a feature matrix used with logistic regression, KNN,
and SVM (with PCA and 100 randomized repetitions). A convolutional neural
network (CNN) is trained on raw images for comparison.
  Results - Logistic regression achieved 97.78 +/- 0.82% accuracy,
outperforming CNN (96.44 +/- 1.32%) and other feature-based models. The
topological features showed high accuracy with low variance.
  Conclusion - This is the first use of topological invariants from formal
digital homology for iris recognition. The method offers a compact,
interpretable, and accurate alternative to deep learning, useful when
explainability or limited data is important. Beyond iris recognition, it can
apply to other biometrics, medical imaging, materials science, remote sensing,
and interpretable AI. It runs efficiently on CPU-only systems and produces
robust, explainable features valuable for security-critical domains.

</details>


### [59] [WeatherPrompt: Multi-modality Representation Learning for All-Weather Drone Visual Geo-Localization](https://arxiv.org/abs/2508.09560)
*Jiahao Wen,Hang Yu,Zhedong Zheng*

Main category: cs.CV

TL;DR: 本研究提出了WeatherPrompt，一种通过融合图像嵌入和文本上下文来建立天气不变表示的新框架，以解决无人机视觉定位在恶劣天气下的性能衰退问题，并验证了其方法在多种天气条件下的高效性和表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有无人机视觉定位方法在恶劣天气下表现不佳，主要由于对有限天气类别的强依赖和场景与天气特征解耦不佳的问题。

Method: 设计了WeatherPrompt框架，其核心包括：1）基于无需训练的天气推理机制利用大规模多模态模型合成天气文本描述，提高对复杂天气的泛化能力；2）借助多模态框架下的动态门控机制，通过文本嵌入引导视觉特征的加权及融合，优化跨模态学习目标。

Result: WeatherPrompt在不同天气条件下均表现出色，尤其在夜晚和雾雪条件下，比现有方法分别提高了Recall@1指标13.37%和18.69%。

Conclusion: 提出的WeatherPrompt框架通过建立天气不变表示显著提升了无人机在多样化天气条件下的定位能力，验证了其相较于现有方法的优势。

Abstract: Visual geo-localization for drones faces critical degradation under weather
perturbations, \eg, rain and fog, where existing methods struggle with two
inherent limitations: 1) Heavy reliance on limited weather categories that
constrain generalization, and 2) Suboptimal disentanglement of entangled
scene-weather features through pseudo weather categories. We present
WeatherPrompt, a multi-modality learning paradigm that establishes
weather-invariant representations through fusing the image embedding with the
text context. Our framework introduces two key contributions: First, a
Training-free Weather Reasoning mechanism that employs off-the-shelf large
multi-modality models to synthesize multi-weather textual descriptions through
human-like reasoning. It improves the scalability to unseen or complex weather,
and could reflect different weather strength. Second, to better disentangle the
scene and weather feature, we propose a multi-modality framework with the
dynamic gating mechanism driven by the text embedding to adaptively reweight
and fuse visual features across modalities. The framework is further optimized
by the cross-modal objectives, including image-text contrastive learning and
image-text matching, which maps the same scene with different weather
conditions closer in the respresentation space. Extensive experiments validate
that, under diverse weather conditions, our method achieves competitive recall
rates compared to state-of-the-art drone geo-localization methods. Notably, it
improves Recall@1 by +13.37\% under night conditions and by 18.69\% under fog
and snow conditions.

</details>


### [60] [WEC-DG: Multi-Exposure Wavelet Correction Method Guided by Degradation Description](https://arxiv.org/abs/2508.09565)
*Ming Zhao,Pingping Liu,Tongshun Zhang,Zhe Zhang*

Main category: cs.CV

TL;DR: 提出了一种新的基于小波变换的曝光校正方法（WEC-DG），通过引入退化描述符和模块化处理，提升了图像校正性能，尤其是在复杂成像条件下。


<details>
  <summary>Details</summary>
Motivation: 当前的多曝光校正方法在处理单次曝光下的图像时，难以克服因不同光照条件、拍摄环境和天气因素产生的类内差异性问题。

Method: 通过引入退化描述符的曝光一致性对齐模块（ECAM）和基于小波变换的曝光恢复与细节重建模块（EDRM），分步处理低频曝光信息和高频细节信息。

Result: 实验证明，新方法在多个公开数据集上的性能超越现有算法，显著提高了校正效果。

Conclusion: WEC-DG方法在复杂成像条件下展现了强大的适应性和实用性，提升了光照校正和细节恢复能力。

Abstract: Multi-exposure correction technology is essential for restoring images
affected by insufficient or excessive lighting, enhancing the visual experience
by improving brightness, contrast, and detail richness. However, current
multi-exposure correction methods often encounter challenges in addressing
intra-class variability caused by diverse lighting conditions, shooting
environments, and weather factors, particularly when processing images captured
at a single exposure level. To enhance the adaptability of these models under
complex imaging conditions, this paper proposes a Wavelet-based Exposure
Correction method with Degradation Guidance (WEC-DG). Specifically, we
introduce a degradation descriptor within the Exposure Consistency Alignment
Module (ECAM) at both ends of the processing pipeline to ensure exposure
consistency and achieve final alignment. This mechanism effectively addresses
miscorrected exposure anomalies caused by existing methods' failure to
recognize 'blurred' exposure degradation. Additionally, we investigate the
light-detail decoupling properties of the wavelet transform to design the
Exposure Restoration and Detail Reconstruction Module (EDRM), which processes
low-frequency information related to exposure enhancement before utilizing
high-frequency information as a prior guide for reconstructing spatial domain
details. This serial processing strategy guarantees precise light correction
and enhances detail recovery. Extensive experiments conducted on multiple
public datasets demonstrate that the proposed method outperforms existing
algorithms, achieving significant performance improvements and validating its
effectiveness and practical applicability.

</details>


### [61] [A Chain of Diagnosis Framework for Accurate and Explainable Radiology Report Generation](https://arxiv.org/abs/2508.09566)
*Haibo Jin,Haoxuan Che,Sunan He,Hao Chen*

Main category: cs.CV

TL;DR: 本文介绍了一个可靠的放射学报告生成（RRG）模型，称为诊断链(CoD)，以改善临床效能和解释性。


<details>
  <summary>Details</summary>
Motivation: 现有放射学报告生成模型在临床效能（尤其病变属性描述）和结果解释性上表现不佳，难以获得放射科医师信任。

Method: 提出了CoD框架，通过诊断对话生成问答对，提取关键发现，并使用大型语言模型生成准确的报告；设计诊断和病变定位模块，以改善结果的解释性和医师工作效率；采用全方位监督学习策略，利用不同数据集类型的注释进行训练。

Result: 生成了一个包含问答对和病变框的全注释RRG数据集，以及评价工具；CoD模型在两大RRG基准上优于现有专科及通用模型，并展现了令人信服的解释性能力。

Conclusion: CoD框架显著提高了放射学报告生成的准确性、解释性和临床实用性，推动了领域的进步。

Abstract: Despite the progress of radiology report generation (RRG), existing works
face two challenges: 1) The performances in clinical efficacy are
unsatisfactory, especially for lesion attributes description; 2) the generated
text lacks explainability, making it difficult for radiologists to trust the
results. To address the challenges, we focus on a trustworthy RRG model, which
not only generates accurate descriptions of abnormalities, but also provides
basis of its predictions. To this end, we propose a framework named chain of
diagnosis (CoD), which maintains a chain of diagnostic process for clinically
accurate and explainable RRG. It first generates question-answer (QA) pairs via
diagnostic conversation to extract key findings, then prompts a large language
model with QA diagnoses for accurate generation. To enhance explainability, a
diagnosis grounding module is designed to match QA diagnoses and generated
sentences, where the diagnoses act as a reference. Moreover, a lesion grounding
module is designed to locate abnormalities in the image, further improving the
working efficiency of radiologists. To facilitate label-efficient training, we
propose an omni-supervised learning strategy with clinical consistency to
leverage various types of annotations from different datasets. Our efforts lead
to 1) an omni-labeled RRG dataset with QA pairs and lesion boxes; 2) a
evaluation tool for assessing the accuracy of reports in describing lesion
location and severity; 3) extensive experiments to demonstrate the
effectiveness of CoD, where it outperforms both specialist and generalist
models consistently on two RRG benchmarks and shows promising explainability by
accurately grounding generated sentences to QA diagnoses and images.

</details>


### [62] [Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion](https://arxiv.org/abs/2508.09575)
*Jiwon Kim,Pureum Kim,SeonHwa Kim,Soobin Park,Eunju Cha,Kyong Hwan Jin*

Main category: cs.CV

TL;DR: 提出了一种训练无关的双递归反馈(DRF)系统，旨在解决T2I模型在控制条件下难以保持空间结构和捕捉细致条件的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管已有方法如Ctrl-X和FreeControl对T2I模型的外观和空间控制取得了进步，但它们在捕捉复杂姿态和布局上仍表现不足。

Method: 设计了双递归反馈系统，包括外观反馈和生成反馈，通过不断优化中间潜变量更好反映控制条件。该机制整合结构和外观信息，用以提升模型表现。

Result: 实验结果表明，该方法能够生成高质量、语义一致且结构完整的图像，支持跨类别的结构外观融合。

Conclusion: DRF实现了高效的细粒度控制，解决了现有方法在结构和外观融合上的不足，具有广泛应用潜力。

Abstract: Recent advancements in controllable text-to-image (T2I) diffusion models,
such as Ctrl-X and FreeControl, have demonstrated robust spatial and appearance
control without requiring auxiliary module training. However, these models
often struggle to accurately preserve spatial structures and fail to capture
fine-grained conditions related to object poses and scene layouts. To address
these challenges, we propose a training-free Dual Recursive Feedback (DRF)
system that properly reflects control conditions in controllable T2I models.
The proposed DRF consists of appearance feedback and generation feedback that
recursively refines the intermediate latents to better reflect the given
appearance information and the user's intent. This dual-update mechanism guides
latent representations toward reliable manifolds, effectively integrating
structural and appearance attributes. Our approach enables fine-grained
generation even between class-invariant structure-appearance fusion, such as
transferring human motion onto a tiger's form. Extensive experiments
demonstrate the efficacy of our method in producing high-quality, semantically
coherent, and structurally consistent image generations. Our source code is
available at https://github.com/jwonkm/DRF.

</details>


### [63] [SHALE: A Scalable Benchmark for Fine-grained Hallucination Evaluation in LVLMs](https://arxiv.org/abs/2508.09584)
*Bei Yan,Zhiyuan Chen,Yuecong Min,Jie Zhang,Jiahao Wang,Xiaozhen Wang,Shiguang Shan*

Main category: cs.CV

TL;DR: 本文提出SHALE评估基准，用于评估大型视觉语言模型的忠实性和真实性幻觉问题，包含自动化数据生成、细粒度分类及噪声模拟。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型在生成内容时常面临幻觉问题（忠实性与真实性），而现有评估方法粒度粗且数据获取昂贵且不可控。

Method: 设计了一种自动化数据生成管道和层次化幻觉诱导框架，结合输入扰动，构建SHALE基准；包含忠实性与真实性幻觉细粒度分类，以及对干净和有噪音场景的评估。

Result: SHALE生成超过30K图像-指令对，涵盖12个视觉感知领域和6个知识领域；在20多个主流模型上的实验显示其真实性幻觉显著，且对语义扰动高度敏感。

Conclusion: SHALE提供了一个高扩展性、多样化的评估工具，有助于更全面细致地评估和改进视觉语言模型的幻觉问题。

Abstract: Despite rapid advances, Large Vision-Language Models (LVLMs) still suffer
from hallucinations, i.e., generating content inconsistent with input or
established world knowledge, which correspond to faithfulness and factuality
hallucinations, respectively. Prior studies primarily evaluate faithfulness
hallucination at a coarse level (e.g., object-level) and lack fine-grained
analysis. Additionally, existing benchmarks rely on costly manual curation or
reused public datasets, raising concerns about scalability and data leakage. To
address these limitations, we propose an automated data construction pipeline
that produces scalable, controllable, and diverse evaluation data. We also
design a hierarchical hallucination induction framework with input
perturbations to simulate realistic noisy scenarios. Integrating these designs,
we construct SHALE, a Scalable HALlucination Evaluation benchmark designed to
assess both faithfulness and factuality hallucinations via a fine-grained
hallucination categorization scheme. SHALE comprises over 30K image-instruction
pairs spanning 12 representative visual perception aspects for faithfulness and
6 knowledge domains for factuality, considering both clean and noisy scenarios.
Extensive experiments on over 20 mainstream LVLMs reveal significant factuality
hallucinations and high sensitivity to semantic perturbations.

</details>


### [64] [Offline Auto Labeling: BAAS](https://arxiv.org/abs/2508.09585)
*Stefan Haag,Bharanidhar Duraisamy,Felix Govaers,Wolfgang Koch,Martin Fritzsche,Juergen Dickmann*

Main category: cs.CV

TL;DR: 本文提出了BAAS，一种用于自动驾驶雷达检测的扩展目标跟踪与融合标注框架，通过贝叶斯追踪和融合实现精确目标轨迹和形状估计，同时可在不同监督水平下提供检测级别的标注。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶领域，需要有效的雷达检测数据标注框架以支持目标轨迹及多种动态对象的精确识别和分析。

Method: 引入基于贝叶斯的跟踪、平滑和融合方法，生成高精度的目标轨迹和形状估计，并可结合人工标注数据进行模块独立或协作分析。

Result: 在真实复杂城市场景中测试，证明框架在目标跟踪性能及标注误差控制方面的实用性和准确性。

Conclusion: 所提出的BAAS框架可处理不同种类的动态对象和类别，并能在检测级别实现高质量标注，同时支持评估和持续改进。

Abstract: This paper introduces BAAS, a new Extended Object Tracking (EOT) and
fusion-based label annotation framework for radar detections in autonomous
driving. Our framework utilizes Bayesian-based tracking, smoothing and
eventually fusion methods to provide veritable and precise object trajectories
along with shape estimation to provide annotation labels on the detection level
under various supervision levels. Simultaneously, the framework provides
evaluation of tracking performance and label annotation. If manually labeled
data is available, each processing module can be analyzed independently or
combined with other modules to enable closed-loop continuous improvements. The
framework performance is evaluated in a challenging urban real-world scenario
in terms of tracking performance and the label annotation errors. We
demonstrate the functionality of the proposed approach for varying dynamic
objects and class types

</details>


### [65] [Hierarchical Brain Structure Modeling for Predicting Genotype of Glioma](https://arxiv.org/abs/2508.09593)
*Haotian Tang,Jianwei Chen,Xinrui Tang,Yunjia Wu,Zhengyang Miao,Chao Li*

Main category: cs.CV

TL;DR: 提出了一种名为Hi-SMGNN的模型，通过整合大脑结构与形态联接组，改进了错构瘤中IDH突变的预测效果。


<details>
  <summary>Details</summary>
Motivation: 现有的IDH突变状态检测方法受限于功能性MRI不足与噪声，同时忽略了大脑的分层组织与多尺度交互。

Method: 提出Hi-SMGNN，使用分层框架整合结构与形态连接组，结合Siamese网络及跨模态注意模块、多尺度特征融合机制，以及个性化模块分割策略。

Result: 实验表明Hi-SMGNN在UCSF-PDGM数据集上优于现有基线与最先进模型，具备更高的鲁棒性及预测效力。

Conclusion: Hi-SMGNN展示了解决IDH突变预测中关键问题的潜力，强调融合结构和形态数据的优势。

Abstract: Isocitrate DeHydrogenase (IDH) mutation status is a crucial biomarker for
glioma prognosis. However, current prediction methods are limited by the low
availability and noise of functional MRI. Structural and morphological
connectomes offer a non-invasive alternative, yet existing approaches often
ignore the brain's hierarchical organisation and multiscale interactions. To
address this, we propose Hi-SMGNN, a hierarchical framework that integrates
structural and morphological connectomes from regional to modular levels. It
features a multimodal interaction module with a Siamese network and cross-modal
attention, a multiscale feature fusion mechanism for reducing redundancy, and a
personalised modular partitioning strategy to enhance individual specificity
and interpretability. Experiments on the UCSF-PDGM dataset demonstrate that
Hi-SMGNN outperforms baseline and state-of-the-art models, showing improved
robustness and effectiveness in IDH mutation prediction.

</details>


### [66] [SVG-Head: Hybrid Surface-Volumetric Gaussians for High-Fidelity Head Reconstruction and Real-Time Editing](https://arxiv.org/abs/2508.09597)
*Heyi Sun,Cong Wang,Tian-Xing Xu,Jingwei Huang,Di Kang,Chunchao Guo,Song-Hai Zhang*

Main category: cs.CV

TL;DR: SVG-Head提出了一种新颖的混合表示方法，通过结合3D高斯与FLAME网格来创建高保真、支持实时编辑的头像模型，并实现了非朗伯区域的高质量重建。


<details>
  <summary>Details</summary>
Motivation: 解决头像建模中实时外观编辑困难的挑战，原因在于几何和整体外观的耦合建模问题。

Method: 提出了SVG-Head，结合3D高斯和FLAME网格进行几何建模，使用解耦纹理图像捕获整体外观，并设计了分层优化策略和基于网格的高斯UV映射方法。

Result: 实验结果表明SVG-Head不仅生成了高保真渲染效果，还首次为高斯头像模型提供了显式纹理图像，同时支持实时外观编辑。

Conclusion: SVG-Head实现了头像模型的高质量重建与实时外观编辑能力，在计算机视觉和图形领域具有重要意义。

Abstract: Creating high-fidelity and editable head avatars is a pivotal challenge in
computer vision and graphics, boosting many AR/VR applications. While recent
advancements have achieved photorealistic renderings and plausible animation,
head editing, especially real-time appearance editing, remains challenging due
to the implicit representation and entangled modeling of the geometry and
global appearance. To address this, we propose Surface-Volumetric Gaussian Head
Avatar (SVG-Head), a novel hybrid representation that explicitly models the
geometry with 3D Gaussians bound on a FLAME mesh and leverages disentangled
texture images to capture the global appearance. Technically, it contains two
types of Gaussians, in which surface Gaussians explicitly model the appearance
of head avatars using learnable texture images, facilitating real-time texture
editing, while volumetric Gaussians enhance the reconstruction quality of
non-Lambertian regions (e.g., lips and hair). To model the correspondence
between 3D world and texture space, we provide a mesh-aware Gaussian UV mapping
method, which leverages UV coordinates given by the FLAME mesh to obtain sharp
texture images and real-time rendering speed. A hierarchical optimization
strategy is further designed to pursue the optimal performance in both
reconstruction quality and editing flexibility. Experiments on the NeRSemble
dataset show that SVG-Head not only generates high-fidelity rendering results,
but also is the first method to obtain explicit texture images for Gaussian
head avatars and support real-time appearance editing.

</details>


### [67] [Images Speak Louder Than Scores: Failure Mode Escape for Enhancing Generative Quality](https://arxiv.org/abs/2508.09598)
*Jie Shao,Ke Zhu,Minghao Fu,Guo-hua Wang,Jianxin Wu*

Main category: cs.CV

TL;DR: 本文提出了一种名为FaME的新方法，通过使用图像质量评估模型对低质生成图像进行引导，提高了扩散模型生成图像的感知质量，同时不影响FID评分。


<details>
  <summary>Details</summary>
Motivation: 虽然现有的扩散模型在类别到图像生成任务中取得了卓越的进展，但生成的图像仍存在一定的质量问题，这与评价指标FID的局限性以及当前技术（如CFG）的副作用密切相关。

Method: 本文提出FaME方法，无需重新训练，通过在推理阶段检测低质量生成图像并存储其采样轨迹，利用这些失败模式作为负向引导，从而避开低质量区域，提升生成质量。

Result: 实验证明，FaME在ImageNet上能显著提升图像的视觉质量，并表现出在文本到图像生成任务上扩展应用的潜力，同时不影响FID评分。

Conclusion: FaME方法为提升生成图像的感知质量提供了一种高效且无训练需求的新途径，克服了现有方法的一些局限性。在确保整体分布对齐的同时，优化了个别图像的质量。

Abstract: Diffusion models have achieved remarkable progress in class-to-image
generation. However, we observe that despite impressive FID scores,
state-of-the-art models often generate distorted or low-quality images,
especially in certain classes. This gap arises because FID evaluates global
distribution alignment, while ignoring the perceptual quality of individual
samples. We further examine the role of CFG, a common technique used to enhance
generation quality. While effective in improving metrics and suppressing
outliers, CFG can introduce distribution shift and visual artifacts due to its
misalignment with both training objectives and user expectations. In this work,
we propose FaME, a training-free and inference-efficient method for improving
perceptual quality. FaME uses an image quality assessment model to identify
low-quality generations and stores their sampling trajectories. These failure
modes are then used as negative guidance to steer future sampling away from
poor-quality regions. Experiments on ImageNet demonstrate that FaME brings
consistent improvements in visual quality without compromising FID. FaME also
shows the potential to be extended to improve text-to-image generation.

</details>


### [68] [BridgeTA: Bridging the Representation Gap in Knowledge Distillation via Teacher Assistant for Bird's Eye View Map Segmentation](https://arxiv.org/abs/2508.09599)
*Beomjun Kim,Suhan Woo,Sejong Heo,Euntai Kim*

Main category: cs.CV

TL;DR: 介绍一种名为BridgeTA的知识蒸馏框架，通过轻量级教师助手网络弥合LiDAR-Camera融合与仅使用摄像头模型间的表现差距，提高鸟瞰图分割性能，不增加学生网络的推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有仅使用摄像头的算法在性能上落后于LiDAR-Camera融合方法，通过知识蒸馏尝试缩小这一差距，但传统方法主要通过增大学生模型来实现，导致推理成本增加。

Method: 通过设计一个轻量级教师助手（TA）网络，将教师和学生的BEV表示结合，形成共享的潜在空间。使用Young's Inequality推导出一种新的蒸馏损失，将直接的教师-学生蒸馏路径分解为教师-TA与TA-学生双路径，从而稳定优化过程并加强知识传递。

Result: 在nuScenes数据集上的实验表明，该方法在mIoU上相较于仅摄像头的基线提高4.2%，提升效果比其他领先的知识蒸馏方法高出45%。

Conclusion: BridgeTA有效促进了仅摄像头方法的性能改进，同时保持了低推理成本，展示了其在成本与性能之间的平衡能力。

Abstract: Bird's-Eye-View (BEV) map segmentation is one of the most important and
challenging tasks in autonomous driving. Camera-only approaches have drawn
attention as cost-effective alternatives to LiDAR, but they still fall behind
LiDAR-Camera (LC) fusion-based methods. Knowledge Distillation (KD) has been
explored to narrow this gap, but existing methods mainly enlarge the student
model by mimicking the teacher's architecture, leading to higher inference
cost. To address this issue, we introduce BridgeTA, a cost-effective
distillation framework to bridge the representation gap between LC fusion and
Camera-only models through a Teacher Assistant (TA) network while keeping the
student's architecture and inference cost unchanged. A lightweight TA network
combines the BEV representations of the teacher and student, creating a shared
latent space that serves as an intermediate representation. To ground the
framework theoretically, we derive a distillation loss using Young's
Inequality, which decomposes the direct teacher-student distillation path into
teacher-TA and TA-student dual paths, stabilizing optimization and
strengthening knowledge transfer. Extensive experiments on the challenging
nuScenes dataset demonstrate the effectiveness of our method, achieving an
improvement of 4.2% mIoU over the Camera-only baseline, up to 45% higher than
the improvement of other state-of-the-art KD methods.

</details>


### [69] [MInDI-3D: Iterative Deep Learning in 3D for Sparse-view Cone Beam Computed Tomography](https://arxiv.org/abs/2508.09616)
*Daniel Barco,Marc Stadelmann,Martin Oswald,Ivo Herzig,Lukas Lichtensteiger,Pascal Paysan,Igor Peterlik,Michal Walczak,Bjoern Menze,Frank-Peter Schilling*

Main category: cs.CV

TL;DR: 提出了 MInDI-3D，这是首个基于条件扩散模型的三维CBCT伪影去除方法，通过迭代去噪显著减少成像辐射暴露。


<details>
  <summary>Details</summary>
Motivation: 减轻成像辐射的暴露量，同时改善稀疏CBCT成像的质量以助力医疗应用发展。

Method: 将二维扩展到三维条件扩散模型，直接从稀疏视角输入中迭代生成高质量CBCT体数据；生成大规模伪CBCT数据集用于模型训练。

Result: 在定量、可扩展性、广泛性和临床评估中表现优异，显著提高PSNR，减少辐射曝光8倍，可适应新扫描器类型并被认为足够用于临床定位。

Conclusion: MInDI-3D 在减少辐射暴露与确保医疗成像质量方面表现优异，具有良好通用性和临床实用性。

Abstract: We present MInDI-3D (Medical Inversion by Direct Iteration in 3D), the first
3D conditional diffusion-based model for real-world sparse-view Cone Beam
Computed Tomography (CBCT) artefact removal, aiming to reduce imaging radiation
exposure. A key contribution is extending the "InDI" concept from 2D to a full
3D volumetric approach for medical images, implementing an iterative denoising
process that refines the CBCT volume directly from sparse-view input. A further
contribution is the generation of a large pseudo-CBCT dataset (16,182) from
chest CT volumes of the CT-RATE public dataset to robustly train MInDI-3D. We
performed a comprehensive evaluation, including quantitative metrics,
scalability analysis, generalisation tests, and a clinical assessment by 11
clinicians. Our results show MInDI-3D's effectiveness, achieving a 12.96 (6.10)
dB PSNR gain over uncorrected scans with only 50 projections on the CT-RATE
pseudo-CBCT (independent real-world) test set and enabling an 8x reduction in
imaging radiation exposure. We demonstrate its scalability by showing that
performance improves with more training data. Importantly, MInDI-3D matches the
performance of a 3D U-Net on real-world scans from 16 cancer patients across
distortion and task-based metrics. It also generalises to new CBCT scanner
geometries. Clinicians rated our model as sufficient for patient positioning
across all anatomical sites and found it preserved lung tumour boundaries well.

</details>


### [70] [Plane Detection and Ranking via Model Information Optimization](https://arxiv.org/abs/2508.09625)
*Daoxin Zhong,Jun Li,Meng Yee Michael Chuah*

Main category: cs.CV

TL;DR: 提出了一种基于模型信息优化的平面检测框架，旨在解决RANSAC判定阈值模糊问题并防止误检。


<details>
  <summary>Details</summary>
Motivation: 解决RANSAC因内点判定阈值模糊导致的误检问题，特别是在复杂现实场景中存在多平面的情况下。

Method: 将深度数据视为离散随机变量，通过物理和噪声模型计算模型信息，用信息最小化优化判定真实平面数量，避免误检，并结合神经网络分割加速处理。

Result: 通过实验，算法比Open3D RANSAC更准确估计平面参数，并且在真实数据中生成更现实的平面参数。

Conclusion: 新框架能更准确地检测多平面并避免误检，有效解决复杂场景中的RANSAC局限性。

Abstract: Plane detection from depth images is a crucial subtask with broad robotic
applications, often accomplished by iterative methods such as Random Sample
Consensus (RANSAC). While RANSAC is a robust strategy with strong probabilistic
guarantees, the ambiguity of its inlier threshold criterion makes it
susceptible to false positive plane detections. This issue is particularly
prevalent in complex real-world scenes, where the true number of planes is
unknown and multiple planes coexist. In this paper, we aim to address this
limitation by proposing a generalised framework for plane detection based on
model information optimization. Building on previous works, we treat the
observed depth readings as discrete random variables, with their probability
distributions constrained by the ground truth planes. Various models containing
different candidate plane constraints are then generated through repeated
random sub-sampling to explain our observations. By incorporating the physics
and noise model of the depth sensor, we can calculate the information for each
model, and the model with the least information is accepted as the most likely
ground truth. This information optimization process serves as an objective
mechanism for determining the true number of planes and preventing false
positive detections. Additionally, the quality of each detected plane can be
ranked by summing the information reduction of inlier points for each plane. We
validate these properties through experiments with synthetic data and find that
our algorithm estimates plane parameters more accurately compared to the
default Open3D RANSAC plane segmentation. Furthermore, we accelerate our
algorithm by partitioning the depth map using neural network segmentation,
which enhances its ability to generate more realistic plane parameters in
real-world data.

</details>


### [71] [Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation](https://arxiv.org/abs/2508.09626)
*Xu Tang,Junan Jia,Yijing Wang,Jingjing Ma,Xiangrong Zhang*

Main category: cs.CV

TL;DR: 提出了一种新方法SAD-Splat，用于解决3D航空视图场景语义分割的语义歧义问题，利用高斯点删除模块和高置信伪标签生成管道提高性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理航空影像中的尺度变化和结构遮挡导致的语义歧义时表现不足，需要提高分割精度和一致性。

Method: 提出了SAD-Splat方法，包括一个高斯点删除模块（通过Hard Concrete分布实现语义置信估计与稀疏机制）和高置信伪标签生成管道（利用2D基础模型扩充监督信号）。

Result: 实验表明，该方法在分割精度和表征紧凑性上达到了优秀的平衡，同时引入了一个3D航空语义数据集以推动领域研究。

Conclusion: SAD-Splat为3D航空场景语义分割提供了一种高效且可扩展的解决方案。

Abstract: In the task of 3D Aerial-view Scene Semantic Segmentation (3D-AVS-SS),
traditional methods struggle to address semantic ambiguity caused by scale
variations and structural occlusions in aerial images. This limits their
segmentation accuracy and consistency. To tackle these challenges, we propose a
novel 3D-AVS-SS approach named SAD-Splat. Our method introduces a Gaussian
point drop module, which integrates semantic confidence estimation with a
learnable sparsity mechanism based on the Hard Concrete distribution. This
module effectively eliminates redundant and semantically ambiguous Gaussian
points, enhancing both segmentation performance and representation compactness.
Furthermore, SAD-Splat incorporates a high-confidence pseudo-label generation
pipeline. It leverages 2D foundation models to enhance supervision when
ground-truth labels are limited, thereby further improving segmentation
accuracy. To advance research in this domain, we introduce a challenging
benchmark dataset: 3D Aerial Semantic (3D-AS), which encompasses diverse
real-world aerial scenes with sparse annotations. Experimental results
demonstrate that SAD-Splat achieves an excellent balance between segmentation
accuracy and representation compactness. It offers an efficient and scalable
solution for 3D aerial scene understanding.

</details>


### [72] [Enhancing Monocular 3D Hand Reconstruction with Learned Texture Priors](https://arxiv.org/abs/2508.09629)
*Giorgos Karvounas,Nikolaos Kyriazis,Iason Oikonomidis,Georgios Pavlakos,Antonis A. Argyros*

Main category: cs.CV

TL;DR: 本文探讨了纹理在单目3D手部重建中的作用，提出通过纹理对齐提升姿态和形状估计。


<details>
  <summary>Details</summary>
Motivation: 现有高性能模型在预测手部几何和图像外观叠加时常出现不完美，为此探讨利用纹理对齐作为一种监督信号来改进性能。

Method: 提出了一个纹理模块，在UV纹理空间中嵌入像素观测值，利用一种新颖的密集对齐损失实现预测手部纹理和观测纹理的对齐。同时使用可微渲染管道及图像到3D手部网格映射模型实现像素级对齐。

Result: 将提出的纹理模块应用到HaMeR模型中，显著提升了单目3D手部重建的准确性和真实感。

Conclusion: 纹理引导的对齐能有效改善单目3D手部重建，提升姿态预测的精度和真实感。

Abstract: We revisit the role of texture in monocular 3D hand reconstruction, not as an
afterthought for photorealism, but as a dense, spatially grounded cue that can
actively support pose and shape estimation. Our observation is simple: even in
high-performing models, the overlay between predicted hand geometry and image
appearance is often imperfect, suggesting that texture alignment may be an
underused supervisory signal. We propose a lightweight texture module that
embeds per-pixel observations into UV texture space and enables a novel dense
alignment loss between predicted and observed hand appearances. Our approach
assumes access to a differentiable rendering pipeline and a model that maps
images to 3D hand meshes with known topology, allowing us to back-project a
textured hand onto the image and perform pixel-based alignment. The module is
self-contained and easily pluggable into existing reconstruction pipelines. To
isolate and highlight the value of texture-guided supervision, we augment
HaMeR, a high-performing yet unadorned transformer architecture for 3D hand
pose estimation. The resulting system improves both accuracy and realism,
demonstrating the value of appearance-guided alignment in hand reconstruction.

</details>


### [73] [Preacher: Paper-to-Video Agentic System](https://arxiv.org/abs/2508.09632)
*Jingwei Liu,Ling Yang,Hao Luo,Fan Wang Hongyan Li,Mengdi Wang*

Main category: cs.CV

TL;DR: 本文提出了Preacher，一个将研究论文转换为结构化视频摘要的系统，能够克服现有视频生成技术的诸多局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型存在上下文有限、视频时长受限、风格多样性不足及难以表达领域特定知识等问题，亟需一个更先进的系统来解决这些问题。

Method: Preacher采用自上而下的方式对论文进行解构、总结和重新组织，然后通过自下而上的方法生成视频，结合关键场景和渐进式链式思维（P-CoT）实现多模态对齐和细致的计划。

Result: Preacher在五个研究领域中成功生成高质量的视频摘要，展示出超越当前视频生成模型的能力。

Conclusion: Preacher证明了其生成高质量、领域专精的视频摘要的能力，并克服了现有技术的不足。代码已开源。

Abstract: The paper-to-video task converts a research paper into a structured video
abstract, distilling key concepts, methods, and conclusions into an accessible,
well-organized format. While state-of-the-art video generation models
demonstrate potential, they are constrained by limited context windows, rigid
video duration constraints, limited stylistic diversity, and an inability to
represent domain-specific knowledge. To address these limitations, we introduce
Preacher, the first paper-to-video agentic system. Preacher employs a top-down
approach to decompose, summarize, and reformulate the paper, followed by
bottom-up video generation, synthesizing diverse video segments into a coherent
abstract. To align cross-modal representations, we define key scenes and
introduce a Progressive Chain of Thought (P-CoT) for granular, iterative
planning. Preacher successfully generates high-quality video abstracts across
five research fields, demonstrating expertise beyond current video generation
models. Code will be released at: https://github.com/GenVerse/Paper2Video

</details>


### [74] [Multi-Contrast Fusion Module: An attention mechanism integrating multi-contrast features for fetal torso plane classification](https://arxiv.org/abs/2508.09644)
*Shengjun Zhu,Siyu Liu,Runqing Xiong,Liping Zheng,Duo Ma,Rongshang Chen,Jiaxin Cai*

Main category: cs.CV

TL;DR: 这篇论文提出了一种多对比度融合模块（MCFM），用于改善超声图像中胎体平面识别，显著提高了分类准确性并最小化模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 作者希望通过改进超声图像特征提取能力，以解决由于低对比度和纹理细节模糊所带来的胎体平面识别挑战，从而提升产前超声诊断质量。

Method: 提出了一种专注于神经网络低层次的多对比度融合模块（MCFM），通过对不同对比条件下图像表示赋予注意力权重，增强特征建模能力，同时保持最小的参数开销。

Result: 实验结果表明，MCFM显著提升了胎体超声图像的识别性能，同时只带来了极少的模型复杂度增加。

Conclusion: 提出的方法通过多对比度融合改善了超声成像的特征表达能力，支持更准确和一致的诊断，为胎儿产前筛查的临床应用展示了很大的潜力，代码已公开在GitHub。

Abstract: Purpose: Prenatal ultrasound is a key tool in evaluating fetal structural
development and detecting abnormalities, contributing to reduced perinatal
complications and improved neonatal survival. Accurate identification of
standard fetal torso planes is essential for reliable assessment and
personalized prenatal care. However, limitations such as low contrast and
unclear texture details in ultrasound imaging pose significant challenges for
fine-grained anatomical recognition. Methods: We propose a novel Multi-Contrast
Fusion Module (MCFM) to enhance the model's ability to extract detailed
information from ultrasound images. MCFM operates exclusively on the lower
layers of the neural network, directly processing raw ultrasound data. By
assigning attention weights to image representations under different contrast
conditions, the module enhances feature modeling while explicitly maintaining
minimal parameter overhead. Results: The proposed MCFM was evaluated on a
curated dataset of fetal torso plane ultrasound images. Experimental results
demonstrate that MCFM substantially improves recognition performance, with a
minimal increase in model complexity. The integration of multi-contrast
attention enables the model to better capture subtle anatomical structures,
contributing to higher classification accuracy and clinical reliability.
Conclusions: Our method provides an effective solution for improving fetal
torso plane recognition in ultrasound imaging. By enhancing feature
representation through multi-contrast fusion, the proposed approach supports
clinicians in achieving more accurate and consistent diagnoses, demonstrating
strong potential for clinical adoption in prenatal screening. The codes are
available at https://github.com/sysll/MCFM.

</details>


### [75] [Multi-Sequence Parotid Gland Lesion Segmentation via Expert Text-Guided Segment Anything Model](https://arxiv.org/abs/2508.09645)
*Zhongyuan Wu,Chuan-Xian Ren,Yu Wang,Xiaohua Ban,Jianning Xiao,Xiaohui Duan*

Main category: cs.CV

TL;DR: 本文提出了PG-SAM模型，通过结合专家诊断报告和多模态信息实现对腮腺病变的高级分割，解决了现有模型受限于精确提示难以适应实际应用的问题。


<details>
  <summary>Details</summary>
Motivation: 当前腮腺病变分割由于病灶大小像素不同且边界复杂，准确性仍具挑战，且现有方法忽视了医学专家的领域知识。

Method: 提出PG-SAM模型，通过专家诊断文本引导提示生成模块生成含领域知识的提示信息，结合跨序列注意力模块整合理模态信息，最终将提示和图像特征输入解码器，完成分割。

Result: 实验结果表明，PG-SAM在三个独立临床中心的腮腺病变分割任务中性能优于现有方法，验证了其临床适用性，以及诊断文本在增强图像分割中的效果。

Conclusion: PG-SAM模型证明了将专家诊断知识引入医疗图像分割的可行性，为腮腺病变分割任务提供了新思路，并具有实际临床应用价值。

Abstract: Parotid gland lesion segmentation is essential for the treatment of parotid
gland diseases. However, due to the variable size and complex lesion
boundaries, accurate parotid gland lesion segmentation remains challenging.
Recently, the Segment Anything Model (SAM) fine-tuning has shown remarkable
performance in the field of medical image segmentation. Nevertheless, SAM's
interaction segmentation model relies heavily on precise lesion prompts
(points, boxes, masks, etc.), which are very difficult to obtain in real-world
applications. Besides, current medical image segmentation methods are
automatically generated, ignoring the domain knowledge of medical experts when
performing segmentation. To address these limitations, we propose the parotid
gland segment anything model (PG-SAM), an expert diagnosis text-guided SAM
incorporating expert domain knowledge for cross-sequence parotid gland lesion
segmentation. Specifically, we first propose an expert diagnosis report guided
prompt generation module that can automatically generate prompt information
containing the prior domain knowledge to guide the subsequent lesion
segmentation process. Then, we introduce a cross-sequence attention module,
which integrates the complementary information of different modalities to
enhance the segmentation effect. Finally, the multi-sequence image features and
generated prompts are feed into the decoder to get segmentation result.
Experimental results demonstrate that PG-SAM achieves state-of-the-art
performance in parotid gland lesion segmentation across three independent
clinical centers, validating its clinical applicability and the effectiveness
of diagnostic text for enhancing image segmentation in real-world clinical
settings.

</details>


### [76] [The Brain Resection Multimodal Image Registration (ReMIND2Reg) 2025 Challenge](https://arxiv.org/abs/2508.09649)
*Reuben Dorent,Laura Rigolo,Colin P. Galvin,Junyu Chen,Mattias P. Heinrich,Aaron Carass,Olivier Colliot,Demian Wassermann,Alexandra Golby,Tina Kapur,William Wells*

Main category: cs.CV

TL;DR: 研究提出通过将术后超声图与术前MRI对齐，解决术中脑移位引起的导航误差问题。


<details>
  <summary>Details</summary>
Motivation: 术中脑移位导致基于术前MRI的导航系统失准，阻碍脑肿瘤手术的安全切除。

Method: 利用ReMIND2Reg挑战赛提供的包含术前术后影像配对的大规模数据集，分析图像配准性能，采用人工标注的解剖标志进行验证并评估误差和运行时间。

Result: 建立了一个标准化的评估框架，公开提供三种图像模态并通过多项性能指标评估配准模型。

Conclusion: ReMIND2Reg挑战赛将推动鲁棒、通用和可临床部署的多模态图像配准算法的发展，促进神经外科术中导航精度提升。

Abstract: Accurate intraoperative image guidance is critical for achieving maximal safe
resection in brain tumor surgery, yet neuronavigation systems based on
preoperative MRI lose accuracy during the procedure due to brain shift.
Aligning post-resection intraoperative ultrasound (iUS) with preoperative MRI
can restore spatial accuracy by estimating brain shift deformations, but it
remains a challenging problem given the large anatomical and topological
changes and substantial modality intensity gap. The ReMIND2Reg 2025 Challenge
provides the largest public benchmark for this task, built upon the ReMIND
dataset. It offers 99 training cases, 5 validation cases, and 10 private test
cases comprising paired 3D ceT1 MRI, T2 MRI, and post-resection 3D iUS volumes.
Data are provided without annotations for training, while validation and test
performance are evaluated on manually annotated anatomical landmarks. Metrics
include target registration error (TRE), robustness to worst-case landmark
misalignment (TRE30), and runtime. By establishing a standardized evaluation
framework for this clinically critical and technically complex problem,
ReMIND2Reg aims to accelerate the development of robust, generalizable, and
clinically deployable multimodal registration algorithms for image-guided
neurosurgery.

</details>


### [77] [TOTNet: Occlusion-Aware Temporal Tracking for Robust Ball Detection in Sports Videos](https://arxiv.org/abs/2508.09650)
*Hao Xu,Arbind Agrahari Baniya,Sam Wells,Mohamed Reda Bouadjenek,Richard Dazely,Sunil Aryal*

Main category: cs.CV

TL;DR: 提出了一个名为TOTNet的网络，用于在有遮挡的情况下进行鲁棒的球体追踪，其在多个运动数据集上性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 运动视频分析中，遮挡条件下的球体追踪仍然是一个关键挑战，特别是对于诸如事件检测和裁判判罚等任务。

Method: 提出了TOTNet网络，该网络融合了3D卷积、可见性加权损失和遮挡增强技术。此外，开发了一个新的遮挡丰富的乒乓球数据集TTA，用于模型训练和评估。

Result: TOTNet在网球、羽毛球和乒乓球的四个数据集上的相对误差RMSE从37.30显著降低到7.19，并将完全遮挡帧的准确率从0.63提高到0.80。

Conclusion: TOTNet为快速运动场景中的离线运动分析提供了有效的工具，其性能在多个数据集上均超越现有的最先进方法。

Abstract: Robust ball tracking under occlusion remains a key challenge in sports video
analysis, affecting tasks like event detection and officiating. We present
TOTNet, a Temporal Occlusion Tracking Network that leverages 3D convolutions,
visibility-weighted loss, and occlusion augmentation to improve performance
under partial and full occlusions. Developed in collaboration with Paralympics
Australia, TOTNet is designed for real-world sports analytics. We introduce
TTA, a new occlusion-rich table tennis dataset collected from
professional-level Paralympic matches, comprising 9,159 samples with 1,996
occlusion cases. Evaluated on four datasets across tennis, badminton, and table
tennis, TOTNet significantly outperforms prior state-of-the-art methods,
reducing RMSE from 37.30 to 7.19 and improving accuracy on fully occluded
frames from 0.63 to 0.80. These results demonstrate TOTNets effectiveness for
offline sports analytics in fast-paced scenarios. Code and data
access:\href{https://github.com/AugustRushG/TOTNet}{AugustRushG/TOTNet}.

</details>


### [78] [Noise-adapted Neural Operator for Robust Non-Line-of-Sight Imaging](https://arxiv.org/abs/2508.09655)
*Lianfang Wang,Kuilin Qin,Xueying Liu,Huibin Chang,Yong Wang,Yuping Duan*

Main category: cs.CV

TL;DR: 本文提出了一种基于参数化神经算子与反问题框架的快速3D成像重建方法，用于解决非视域成像中的噪声与弱信号挑战。


<details>
  <summary>Details</summary>
Motivation: 当前非视域成像受到多次反射或散射的微弱光信号和噪声干扰，难以实现准确的场景重建，迫切需要结合物理过程的高效解决方法。

Method: 论文提出一种包括自适应噪声估计模块和参数化神经算子的框架，通过深度算法展开实现端到端快速重建，并引入一种新的全局与局部特征融合方法。

Result: 该方法在模拟数据和真实数据上的实验表明其在快速扫描数据和稀疏光照点数据下表现优异，能够在复杂场景下实现高效非视域成像。

Conclusion: 通过结合深度学习与物理约束，该方法实现了更高的重建精度和鲁棒性，为复杂非视域成像场景提供了有效解决方案。

Abstract: Computational imaging, especially non-line-of-sight (NLOS) imaging, the
extraction of information from obscured or hidden scenes is achieved through
the utilization of indirect light signals resulting from multiple reflections
or scattering. The inherently weak nature of these signals, coupled with their
susceptibility to noise, necessitates the integration of physical processes to
ensure accurate reconstruction. This paper presents a parameterized inverse
problem framework tailored for large-scale linear problems in 3D imaging
reconstruction. Initially, a noise estimation module is employed to adaptively
assess the noise levels present in transient data. Subsequently, a
parameterized neural operator is developed to approximate the inverse mapping,
facilitating end-to-end rapid image reconstruction. Our 3D image reconstruction
framework, grounded in operator learning, is constructed through deep algorithm
unfolding, which not only provides commendable model interpretability but also
enables dynamic adaptation to varying noise levels in the acquired data,
thereby ensuring consistently robust and accurate reconstruction outcomes.
Furthermore, we introduce a novel method for the fusion of global and local
spatiotemporal data features. By integrating structural and detailed
information, this method significantly enhances both accuracy and robustness.
Comprehensive numerical experiments conducted on both simulated and real
datasets substantiate the efficacy of the proposed method. It demonstrates
remarkable performance with fast scanning data and sparse illumination point
data, offering a viable solution for NLOS imaging in complex scenarios.

</details>


### [79] [NegFaceDiff: The Power of Negative Context in Identity-Conditioned Diffusion for Synthetic Face Generation](https://arxiv.org/abs/2508.09661)
*Eduarda Caldeira,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: 此论文提出了一种新颖的方法（NegFaceDiff）来改进身份一致性与分离性，通过在身份条件扩散过程中引入负条件，实现更加优化的面部识别训练数据生成。


<details>
  <summary>Details</summary>
Motivation: 传统身份条件扩散模型在生成面部数据时，缺乏显式的采样机制，容易造成身份重叠，限制了面部识别系统的表现。因此需要一种方法优化生成数据的身份分离性。

Method: 作者提出了NegFaceDiff，一种将负条件引入身份条件扩散模型的新型采样方法。这个方法利用负条件避免模型引入不需要的特征，同时保持类别内部一致性，从而提高数据的身份分离性。

Result: 实验结果表明，使用NegFaceDiff方法生成的身份条件扩散模型数据，其身份分离性显著提高，Fisher Discriminant Ratio从2.427提升至5.687。在多个基准测试中，基于NegFaceDiff数据训练的面部识别系统性能优于没有使用负条件生成数据的系统。

Conclusion: NegFaceDiff成功优化了身份条件扩散模型数据的身份一致性和分离性，并显著提升了面部识别系统的性能。

Abstract: The use of synthetic data as an alternative to authentic datasets in face
recognition (FR) development has gained significant attention, addressing
privacy, ethical, and practical concerns associated with collecting and using
authentic data. Recent state-of-the-art approaches have proposed
identity-conditioned diffusion models to generate identity-consistent face
images, facilitating their use in training FR models. However, these methods
often lack explicit sampling mechanisms to enforce inter-class separability,
leading to identity overlap in the generated data and, consequently, suboptimal
FR performance. In this work, we introduce NegFaceDiff, a novel sampling method
that incorporates negative conditions into the identity-conditioned diffusion
process. NegFaceDiff enhances identity separation by leveraging negative
conditions that explicitly guide the model away from unwanted features while
preserving intra-class consistency. Extensive experiments demonstrate that
NegFaceDiff significantly improves the identity consistency and separability of
data generated by identity-conditioned diffusion models. Specifically, identity
separability, measured by the Fisher Discriminant Ratio (FDR), increases from
2.427 to 5.687. These improvements are reflected in FR systems trained on the
NegFaceDiff dataset, which outperform models trained on data generated without
negative conditions across multiple benchmarks.

</details>


### [80] [GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors](https://arxiv.org/abs/2508.09667)
*Xingyilang Yin,Qi Zhang,Jiahao Chang,Ying Feng,Qingnan Fan,Xi Yang,Chi-Man Pun,Huaqi Zhang,Xiaodong Cun*

Main category: cs.CV

TL;DR: 该研究提出了一种新的框架GSFixer，用于改进从稀疏输入中重建的3D高斯散点表示，并提供了专门的评估基准DL3DV-Res，实验表明GSFixer在3DGS伪影修复和稀疏视图3D重建方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前方法在利用生成先验信息补全未约束区域时难以与输入观测保持一致。

Method: 提出了一种基于DiT的视频扩散模型的参考引导视频修复框架，结合输入稀疏视图参考，整合2D语义特征和从视觉几何基础模型中提取的3D几何特征，以改进伪影修复效果。

Result: 实验展示了GSFixer在3DGS伪影修复和稀疏视图3D重建上优于最先进的方法。

Conclusion: GSFixer框架及其专用评估基准表明，在从稀疏输入构建三维场景时，可显著提升3D高斯散点表示的质量。

Abstract: Reconstructing 3D scenes using 3D Gaussian Splatting (3DGS) from sparse views
is an ill-posed problem due to insufficient information, often resulting in
noticeable artifacts. While recent approaches have sought to leverage
generative priors to complete information for under-constrained regions, they
struggle to generate content that remains consistent with input observations.
To address this challenge, we propose GSFixer, a novel framework designed to
improve the quality of 3DGS representations reconstructed from sparse inputs.
The core of our approach is the reference-guided video restoration model, built
upon a DiT-based video diffusion model trained on paired artifact 3DGS renders
and clean frames with additional reference-based conditions. Considering the
input sparse views as references, our model integrates both 2D semantic
features and 3D geometric features of reference views extracted from the visual
geometry foundation model, enhancing the semantic coherence and 3D consistency
when fixing artifact novel views. Furthermore, considering the lack of suitable
benchmarks for 3DGS artifact restoration evaluation, we present DL3DV-Res which
contains artifact frames rendered using low-quality 3DGS. Extensive experiments
demonstrate our GSFixer outperforms current state-of-the-art methods in 3DGS
artifact restoration and sparse-view 3D reconstruction. Project page:
https://github.com/GVCLab/GSFixer.

</details>


### [81] [Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in surgical vision](https://arxiv.org/abs/2508.09681)
*Gerardo Loza,Junlei Hu,Dominic Jones,Sharib Ali,Pietro Valdastri*

Main category: cs.CV

TL;DR: 提出一种基于可逆神经辐射场(InvNeRF)的新测试时优化(TTO)方法，用于高精度的2D和3D点跟踪。


<details>
  <summary>Details</summary>
Motivation: 当前的点跟踪方法多局限于2D运动或不具备一致性，本研究旨在通过一种新方法克服这些局限，尤其是长时间跟踪中的挑战。

Method: 利用可逆神经辐射场(InvNeRF)架构，结合模型的双向变形-规范映射、多尺度HexPlanes推理方法，以及像素采样和收敛的优化机制，实现2D与3D点位的高效跟踪。

Result: 在STIR和SCARE数据集上测试，2D点跟踪精度相比现有TTO方法提高了近50%，3D跟踪首次实现了TTO架构，并优于现有的前馈方法。

Conclusion: 提出的方法不仅在2D跟踪中领先，首次实现了长时间3D点跟踪，为后续TTO框架研究提供了全新方向。

Abstract: We proposed a novel test-time optimisation (TTO) approach framed by a
NeRF-based architecture for long-term 3D point tracking. Most current methods
in point tracking struggle to obtain consistent motion or are limited to 2D
motion. TTO approaches frame the solution for long-term tracking as optimising
a function that aggregates correspondences from other specialised
state-of-the-art methods. Unlike the state-of-the-art on TTO, we propose
parametrising such a function with our new invertible Neural Radiance Field
(InvNeRF) architecture to perform both 2D and 3D tracking in surgical
scenarios. Our approach allows us to exploit the advantages of a
rendering-based approach by supervising the reprojection of pixel
correspondences. It adapts strategies from recent rendering-based methods to
obtain a bidirectional deformable-canonical mapping, to efficiently handle a
defined workspace, and to guide the rays' density. It also presents our
multi-scale HexPlanes for fast inference and a new algorithm for efficient
pixel sampling and convergence criteria. We present results in the STIR and
SCARE datasets, for evaluating point tracking and testing the integration of
kinematic data in our pipeline, respectively. In 2D point tracking, our
approach surpasses the precision and accuracy of the TTO state-of-the-art
methods by nearly 50% on average precision, while competing with other
approaches. In 3D point tracking, this is the first TTO approach, surpassing
feed-forward methods while incorporating the benefits of a deformable
NeRF-based reconstruction.

</details>


### [82] [PaCo-FR: Patch-Pixel Aligned End-to-End Codebook Learning for Facial Representation Pre-training](https://arxiv.org/abs/2508.09691)
*Yin Xie,Zhichao Chen,Xiaoze Yu,Yongle Zhao,Xiang An,Kaicheng Yang,Zimin Ran,Jia Guo,Ziyong Feng,Jiankang Deng*

Main category: cs.CV

TL;DR: 引入了一种名为PaCo-FR的无监督框架，结合了掩码图像建模和块像素对齐，解决了面部特征提取中的关键问题。


<details>
  <summary>Details</summary>
Motivation: 现有面部表示的预训练方法无法有效捕获面部细节特征和语义、忽略面部解剖的空间结构，以及无法高效利用有限标注数据。

Method: 设计了一种无监督学习方法PaCo-FR，其创新包含：(1)保留空间一致性的结构化掩码策略；(2)增强特征区分性的块代码表；(3)保持面部几何关系的空间一致性约束。

Result: 在使用仅200万未标注图像进行预训练后，PaCo-FR在多种面部分析任务中实现了最优表现，特别是在姿态变化、遮挡和光照条件下表现显著。

Conclusion: PaCo-FR推进了面部表征学习技术，降低对人工标注数据集的依赖，为开发更高效的面部分析系统提供了一种可扩展的解决方案。

Abstract: Facial representation pre-training is crucial for tasks like facial
recognition, expression analysis, and virtual reality. However, existing
methods face three key challenges: (1) failing to capture distinct facial
features and fine-grained semantics, (2) ignoring the spatial structure
inherent to facial anatomy, and (3) inefficiently utilizing limited labeled
data. To overcome these, we introduce PaCo-FR, an unsupervised framework that
combines masked image modeling with patch-pixel alignment. Our approach
integrates three innovative components: (1) a structured masking strategy that
preserves spatial coherence by aligning with semantically meaningful facial
regions, (2) a novel patch-based codebook that enhances feature discrimination
with multiple candidate tokens, and (3) spatial consistency constraints that
preserve geometric relationships between facial components. PaCo-FR achieves
state-of-the-art performance across several facial analysis tasks with just 2
million unlabeled images for pre-training. Our method demonstrates significant
improvements, particularly in scenarios with varying poses, occlusions, and
lighting conditions. We believe this work advances facial representation
learning and offers a scalable, efficient solution that reduces reliance on
expensive annotated datasets, driving more effective facial analysis systems.

</details>


### [83] [Slot Attention-based Feature Filtering for Few-Shot Learning](https://arxiv.org/abs/2508.09699)
*Javier Rodenas,Eduardo Aguilar,Petia Radeva*

Main category: cs.CV

TL;DR: 本文提出了基于插槽注意力机制的特征过滤方法(SAFF)，有效过滤无关特征，提升了少样本学习的性能。


<details>
  <summary>Details</summary>
Motivation: 少样本学习过程中，无关特征（如背景元素）常常导致混淆和误分类，亟需一种方法来筛选并处理这些无用特征。

Method: 提出了SAFF方法，将插槽注意力机制与patch嵌入结合，将类感知的插槽统一到一个注意力机制中，设计了支持与查询图像间的相似度矩阵，过滤无关特征并提高分类性能。

Result: 通过在CIFAR-FS、FC100、miniImageNet和tieredImageNet等少样本学习基准上的实验，证明了SAFF优于其他注意力机制，取得了优异的分类性能。

Conclusion: SAFF通过有效过滤无关特征提升了少样本学习的性能，验证了其优越性，有助于未来类似任务的研究。

Abstract: Irrelevant features can significantly degrade few-shot learn ing performance.
This problem is used to match queries and support images based on meaningful
similarities despite the limited data. However, in this process, non-relevant
fea tures such as background elements can easily lead to confu sion and
misclassification. To address this issue, we pro pose Slot Attention-based
Feature Filtering for Few-Shot Learning (SAFF) that leverages slot attention
mechanisms to discriminate and filter weak features, thereby improving few-shot
classification performance. The key innovation of SAFF lies in its integration
of slot attention with patch em beddings, unifying class-aware slots into a
single attention mechanism to filter irrelevant features effectively. We intro
duce a similarity matrix that computes across support and query images to
quantify the relevance of filtered embed dings for classification. Through
experiments, we demon strate that Slot Attention performs better than other
atten tion mechanisms, capturing discriminative features while reducing
irrelevant information. We validate our approach through extensive experiments
on few-shot learning bench marks: CIFAR-FS, FC100, miniImageNet and tieredIma
geNet, outperforming several state-of-the-art methods.

</details>


### [84] [MangaDiT: Reference-Guided Line Art Colorization with Hierarchical Attention in Diffusion Transformers](https://arxiv.org/abs/2508.09709)
*Qianru Qiu,Jiafeng Mao,Kento Masui,Xueting Wang*

Main category: cs.CV

TL;DR: MangaDiT是一种基于Diffusion Transformers的参考导向线稿上色模型，通过层次注意机制和动态权重策略提高区域颜色一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以解决参考图像与目标图像在角色姿势或运动上的差异，特别是区域级别的颜色一致性问题。

Method: 提出MangaDiT模型，利用层次注意机制和动态注意权重策略，通过内部注意机制隐式发现语义对应关系。模型结合线稿和参考图像作为条件输入，通过增强的上下文感知路径扩展了感受野以实现更好的颜色对齐。

Result: 在两个基准数据集上的实验表明，该方法在定性和定量评估中均显著优于现有方法。

Conclusion: MangaDiT利用扩展的注意力机制实现了参考导向线稿着色的显著改进，特别是在区域颜色一致性方面表现卓越。

Abstract: Recent advances in diffusion models have significantly improved the
performance of reference-guided line art colorization. However, existing
methods still struggle with region-level color consistency, especially when the
reference and target images differ in character pose or motion. Instead of
relying on external matching annotations between the reference and target, we
propose to discover semantic correspondences implicitly through internal
attention mechanisms. In this paper, we present MangaDiT, a powerful model for
reference-guided line art colorization based on Diffusion Transformers (DiT).
Our model takes both line art and reference images as conditional inputs and
introduces a hierarchical attention mechanism with a dynamic attention
weighting strategy. This mechanism augments the vanilla attention with an
additional context-aware path that leverages pooled spatial features,
effectively expanding the model's receptive field and enhancing region-level
color alignment. Experiments on two benchmark datasets demonstrate that our
method significantly outperforms state-of-the-art approaches, achieving
superior performance in both qualitative and quantitative evaluations.

</details>


### [85] [NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation](https://arxiv.org/abs/2508.09715)
*Devvrat Joshi,Islem Rekik*

Main category: cs.CV

TL;DR: 本文提出了一种名为NEURAL的新方法，通过语义指导数据压缩应对多模态医学影像数据的存储和传输挑战。


<details>
  <summary>Details</summary>
Motivation: 多模态医学影像数据快速增长，特别是资源有限的临床环境中面临存储和传输的重大挑战。

Method: 利用经过微调的视觉-语言生成模型的交叉注意力得分，对胸部X光片进行结构化裁剪，仅保留诊断关键区域，将图像转化为高度压缩的图形表示，并将其与临床报告的知识图相融合，形成统一数据结构。

Result: 在MIMIC-CXR和CheXpert Plus数据集的肺炎检测中，NEURAL将图像数据大小减少93.4-97.7%，同时保持0.88-0.95 AUC的高诊断性能，优于使用未压缩数据的基线模型。

Conclusion: NEURAL有效平衡数据量与临床效用，可在不牺牲性能的前提下实现高效的影像工作流和远程放射学传输。

Abstract: The rapid growth of multimodal medical imaging data presents significant
storage and transmission challenges, particularly in resource-constrained
clinical settings. We propose NEURAL, a novel framework that addresses this by
using semantics-guided data compression. Our approach repurposes
cross-attention scores between the image and its radiological report from a
fine-tuned generative vision-language model to structurally prune chest X-rays,
preserving only diagnostically critical regions. This process transforms the
image into a highly compressed, graph representation. This unified graph-based
representation fuses the pruned visual graph with a knowledge graph derived
from the clinical report, creating a universal data structure that simplifies
downstream modeling. Validated on the MIMIC-CXR and CheXpert Plus dataset for
pneumonia detection, NEURAL achieves a 93.4-97.7\% reduction in image data size
while maintaining a high diagnostic performance of 0.88-0.95 AUC, outperforming
other baseline models that use uncompressed data. By creating a persistent,
task-agnostic data asset, NEURAL resolves the trade-off between data size and
clinical utility, enabling efficient workflows and teleradiology without
sacrificing performance. Our NEURAL code is available at
https://github.com/basiralab/NEURAL.

</details>


### [86] [Multimodal Sheaf-based Network for Glioblastoma Molecular Subtype Prediction](https://arxiv.org/abs/2508.09717)
*Shekhnaz Idrissova,Islem Rekik*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于结构鞘的方法，用于融合MRI和组织病理学数据，超越现有方法，并有效处理数据缺失场景。


<details>
  <summary>Details</summary>
Motivation: 目前，胶质母细胞瘤的分子亚型分类需要侵入性组织提取，并且现有多模态方法的效果受限，未解决结构信息保留问题及数据缺失问题。

Method: 提出了一种基于结构鞘的框架，使MRI与组织病理学数据的融合更具结构意识和一致性。

Result: 模型表现超过现有基线方法，且在数据缺失或不完整的情况下表现出鲁棒性。

Conclusion: 该研究为虚拟活检工具的开发提供了支持，助力快速诊断。

Abstract: Glioblastoma is a highly invasive brain tumor with rapid progression rates.
Recent studies have shown that glioblastoma molecular subtype classification
serves as a significant biomarker for effective targeted therapy selection.
However, this classification currently requires invasive tissue extraction for
comprehensive histopathological analysis. Existing multimodal approaches
combining MRI and histopathology images are limited and lack robust mechanisms
for preserving shared structural information across modalities. In particular,
graph-based models often fail to retain discriminative features within
heterogeneous graphs, and structural reconstruction mechanisms for handling
missing or incomplete modality data are largely underexplored. To address these
limitations, we propose a novel sheaf-based framework for structure-aware and
consistent fusion of MRI and histopathology data. Our model outperforms
baseline methods and demonstrates robustness in incomplete or missing data
scenarios, contributing to the development of virtual biopsy tools for rapid
diagnostics. Our source code is available at
https://github.com/basiralab/MMSN/.

</details>


### [87] [Predictive Uncertainty for Runtime Assurance of a Real-Time Computer Vision-Based Landing System](https://arxiv.org/abs/2508.09732)
*Romeo Valentin,Sydney M. Katz,Artur B. Carneiro,Don Walker,Mykel J. Kochenderfer*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉的飞机姿态估计管道，使用了一种灵活高效的神经网络架构、严谨的损失函数和改进的故障检测方法，显示出卓越的准确性和校准的预测不确定性。


<details>
  <summary>Details</summary>
Motivation: 目前数据驱动的计算机视觉应用已能够支持民用航空中的自主导航功能，但要满足航空应用中的安全性和鲁棒性仍是重大挑战。本文旨在为飞机姿态估计引入具备认证潜力的实际方法。

Method: 设计了一种基于空间Soft Argmax算子的灵活神经网络架构，结合严谨的损失函数以校准预测不确定性，并改进了基于残差的完整性监控方法用于识别运行时的故障输出。

Result: 通过跑道图像数据集验证，模型在准确性方面优于基准架构，同时生成具有亚像素精度的校准不确定性预测，可用于后续的故障检测。

Conclusion: 本文方法为安全性关键的航空应用中视觉系统的认证提供了重要基础，显著提升了姿态估计的准确性和可靠性。

Abstract: Recent advances in data-driven computer vision have enabled robust autonomous
navigation capabilities for civil aviation, including automated landing and
runway detection. However, ensuring that these systems meet the robustness and
safety requirements for aviation applications remains a major challenge. In
this work, we present a practical vision-based pipeline for aircraft pose
estimation from runway images that represents a step toward the ability to
certify these systems for use in safety-critical aviation applications. Our
approach features three key innovations: (i) an efficient, flexible neural
architecture based on a spatial Soft Argmax operator for probabilistic keypoint
regression, supporting diverse vision backbones with real-time inference; (ii)
a principled loss function producing calibrated predictive uncertainties, which
are evaluated via sharpness and calibration metrics; and (iii) an adaptation of
Residual-based Receiver Autonomous Integrity Monitoring (RAIM), enabling
runtime detection and rejection of faulty model outputs. We implement and
evaluate our pose estimation pipeline on a dataset of runway images. We show
that our model outperforms baseline architectures in terms of accuracy while
also producing well-calibrated uncertainty estimates with sub-pixel precision
that can be used downstream for fault detection.

</details>


### [88] [Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory](https://arxiv.org/abs/2508.09736)
*Lin Long,Yichen He,Wentao Ye,Yiyuan Pan,Yuan Lin,Hang Li,Junbo Zhao,Wei Li*

Main category: cs.CV

TL;DR: M3-Agent是一种带有长期记忆能力的新型多模态智能框架，能够处理实时视觉和听觉输入，构建和更新长期记忆。


<details>
  <summary>Details</summary>
Motivation: 旨在为多模态智能体开发更接近人类的长期记忆与推理能力，并解决现有方法在跨模态推理和记忆处理上的不足。

Method: 开发M3-Agent，通过强化学习训练其多轮推理以及记忆检索能力。引入M3-Bench基准，以评估智能体的记忆效果和基于记忆的推理能力。

Result: M3-Agent在新提出的基准测试M3-Bench上表现优越，相较现有最强模型，在多个测试集上分别提高了5.3%-7.7%的准确率。

Conclusion: M3-Agent提升了多模态智能体的长期记忆能力，朝着人类式认知迈出重要一步，同时为该领域的实际设计提供有价值的见解。

Abstract: We introduce M3-Agent, a novel multimodal agent framework equipped with
long-term memory. Like humans, M3-Agent can process real-time visual and
auditory inputs to build and update its long-term memory. Beyond episodic
memory, it also develops semantic memory, enabling it to accumulate world
knowledge over time. Its memory is organized in an entity-centric, multimodal
format, allowing deeper and more consistent understanding of the environment.
Given an instruction, M3-Agent autonomously performs multi-turn, iterative
reasoning and retrieves relevant information from memory to accomplish the
task. To evaluate memory effectiveness and memory-based reasoning in multimodal
agents, we develop M3-Bench, a new long-video question answering benchmark.
M3-Bench comprises 100 newly recorded real-world videos captured from a robot's
perspective (M3-Bench-robot) and 929 web-sourced videos across diverse
scenarios (M3-Bench-web). We annotate question-answer pairs designed to test
key capabilities essential for agent applications, such as human understanding,
general knowledge extraction, and cross-modal reasoning. Experimental results
show that M3-Agent, trained via reinforcement learning, outperforms the
strongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o,
achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web
and VideoMME-long, respectively. Our work advances the multimodal agents toward
more human-like long-term memory and provides insights into their practical
design. Model, code and data are available at
https://github.com/bytedance-seed/m3-agent

</details>


### [89] [Region-to-Region: Enhancing Generative Image Harmonization with Adaptive Regional Injection](https://arxiv.org/abs/2508.09746)
*Zhiqiu Zhang,Dongqi Fan,Mingjie Wang,Qiang Tang,Jian Yang,Zili Yi*

Main category: cs.CV

TL;DR: 本论文提出了一种新方法，通过区域间转换增强图像和谐效果，同时发布了新的数据集RPHarmony，提高了模型在真实案例中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的潜在扩散模型在图像和谐中存在细节保留和和谐能力不足的问题，且合成数据集缺乏真实光照变化。

Method: 提出R2R模型，设计Clear-VAE和Harmony Controller，并引入一种新的随机泊松混合方法生成合成数据集RPHarmony。

Result: 实验表明，该方法在定量指标和视觉效果上优于其他方法，新数据集加深了模型对真实图像的生成能力。

Conclusion: 通过R2R模型和数据集改进，提升了图像和谐处理的细节保留及多样性，促进了实际应用的效果。

Abstract: The goal of image harmonization is to adjust the foreground in a composite
image to achieve visual consistency with the background. Recently, latent
diffusion model (LDM) are applied for harmonization, achieving remarkable
results. However, LDM-based harmonization faces challenges in detail
preservation and limited harmonization ability. Additionally, current synthetic
datasets rely on color transfer, which lacks local variations and fails to
capture complex real-world lighting conditions. To enhance harmonization
capabilities, we propose the Region-to-Region transformation. By injecting
information from appropriate regions into the foreground, this approach
preserves original details while achieving image harmonization or, conversely,
generating new composite data. From this perspective, We propose a novel model
R2R. Specifically, we design Clear-VAE to preserve high-frequency details in
the foreground using Adaptive Filter while eliminating disharmonious elements.
To further enhance harmonization, we introduce the Harmony Controller with
Mask-aware Adaptive Channel Attention (MACA), which dynamically adjusts the
foreground based on the channel importance of both foreground and background
regions. To address the limitation of existing datasets, we propose Random
Poisson Blending, which transfers color and lighting information from a
suitable region to the foreground, thereby generating more diverse and
challenging synthetic images. Using this method, we construct a new synthetic
dataset, RPHarmony. Experiments demonstrate the superiority of our method over
other methods in both quantitative metrics and visual harmony. Moreover, our
dataset helps the model generate more realistic images in real examples. Our
code, dataset, and model weights have all been released for open access.

</details>


### [90] [MoIIE: Mixture of Intra- and Inter-Modality Experts for Large Vision Language Models](https://arxiv.org/abs/2508.09779)
*Dianyi Wang,Siyuan Wang,Zejun Li,Yikun Wang,Yitong Li,Duyu Tang,Xiaoyu Shen,Xuanjing Huang,Zhongyu Wei*

Main category: cs.CV

TL;DR: 提出了MoIIE（模态内专家和模态间专家混合）的稀疏架构，用于改进多模态任务中的大规模视觉-语言模型（LVLMs）的参数效率和交互学习能力。使用两阶段训练策略提升模型性能，并实现更少激活参数情况下的性能优越性。


<details>
  <summary>Details</summary>
Motivation: 由于LVLM在多模态任务中的卓越表现，同时因其密集计算导致的高成本，探索稀疏化架构（如MoE）以提升参数效率成为重要研究动力。

Method: 提出了一种MoIIE架构，通过模态引导的路由机制，将每个token分配到相应的模态内专家或模态间专家组合中，同时采用了两阶段训练策略以增强模型激活能力和多模态任务性能。

Result: 实验表明，采用MoIIE架构的模型在5.5B和11.3B激活参数下，能匹配或超越现有其他MoE-LLMs基础的多模态模型性能，且参数效率更高。在不同数据规模和LLM骨干上的验证均支持这一结果。

Conclusion: MoIIE架构显著增强了LVLM的参数效率和跨模态交互能力，是一种通用且高效的多模态任务解决方案，具备广泛应用潜力。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable performance
across multi-modal tasks by scaling model size and training data. However,
these dense LVLMs incur significant computational costs and motivate the
exploration of sparse Mixture of Experts (MoE) architectures. While MoE improve
parameter efficiency, effectively applying MoE to simultaneously model
modality-specific features and cross-modal associations in LVLMs remains
challenging. In this work, we propose to incorporate Mixture of Intra- and
Inter-Modality Experts (MoIIE) to LVLMs. For each token, expert routing is
guided by its modality, directing tokens to their respective intra-modality
experts as well as a shared pool of inter-modality experts, enabling the model
to jointly learn rich intra-modal features and cross-modal interactions. We
further introduce an effective and straightforward two-stage training strategy,
which facilitates the direct activation of both MoE and multi-modal
capabilities. Extensive experiments across different data scales and LLM
backbone demonstrate the effectiveness, efficiency and generality of our
approach. Notably, our MoIIE models with 5.5B and 11.3B activated parameters
match or even surpass the performance of existing advanced open-source MoE-LLMs
based multi-modal models that involve more activated parameters. The code is
available at https://github.com/AlenjandroWang/MoIIE.

</details>


### [91] [Combinative Matching for Geometric Shape Assembly](https://arxiv.org/abs/2508.09780)
*Nahyuk Lee,Juhong Min,Junhong Lee,Chunghyun Park,Minsu Cho*

Main category: cs.CV

TL;DR: 提出了一种名为“组合匹配”的新型形状匹配方法，通过识别互锁零件的几何特性高效组合零件。


<details>
  <summary>Details</summary>
Motivation: 解决传统几何形状装配方法中仅依赖表面对齐的问题，并提出异体积占据的新匹配概念。

Method: 利用等变神经网络学习形状方向，显式建模互锁形状的“表面相同性”和“体积异向性”两种特性，以减少匹配过程中的局部歧义。

Result: 在几何装配基准测试中取得了优异表现，超越现有最先进方法。

Conclusion: 提出的方法能够更精确地组合互锁零件，为几何形状装配提供了一种鲁棒的新方案。

Abstract: This paper introduces a new shape-matching methodology, combinative matching,
to combine interlocking parts for geometric shape assembly. Previous methods
for geometric assembly typically rely on aligning parts by finding identical
surfaces between the parts as in conventional shape matching and registration.
In contrast, we explicitly model two distinct properties of interlocking
shapes: 'identical surface shape' and 'opposite volume occupancy.' Our method
thus learns to establish correspondences across regions where their surface
shapes appear identical but their volumes occupy the inverted space to each
other. To facilitate this process, we also learn to align regions in rotation
by estimating their shape orientations via equivariant neural networks. The
proposed approach significantly reduces local ambiguities in matching and
allows a robust combination of parts in assembly. Experimental results on
geometric assembly benchmarks demonstrate the efficacy of our method,
consistently outperforming the state of the art. Project page:
https://nahyuklee.github.io/cmnet.

</details>


### [92] [DSS-Prompt: Dynamic-Static Synergistic Prompting for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2508.09785)
*Linpu He,Yanan Li,Bingze Li,Elvis Han Cui,Donghui Wang*

Main category: cs.CV

TL;DR: DSS-Prompt通过结合静态和动态提示的方式，将预训练的Vision Transformer优化为强大的少样本增量学习分类器，并展示了在多个基准测试上的优秀效果。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用大规模预训练模型解决少样本增量学习任务中的挑战性问题，即如何在学习新概念的同时不遗忘旧知识。

Method: 提出DSS-Prompt方法，在每个Transformer模块中结合静态提示和动态提示。静态提示用于弥合训练和下游数据集间的领域差距，动态提示基于实例相关语义生成，从而实现知识的无缝迁移。

Result: 在四个基准测试中进行实验，DSS-Prompt稳定地优于现有方法，并有效减轻了灾难性遗忘问题。

Conclusion: DSS-Prompt方法是一种低成本、无需额外增量训练的解决方案，能够强化少样本增量学习任务的性能并减缓遗忘问题。

Abstract: Learning from large-scale pre-trained models with strong generalization
ability has shown remarkable success in a wide range of downstream tasks
recently, but it is still underexplored in the challenging few-shot
class-incremental learning (FSCIL) task. It aims to continually learn new
concepts from limited training samples without forgetting the old ones at the
same time. In this paper, we introduce DSS-Prompt, a simple yet effective
approach that transforms the pre-trained Vision Transformer with minimal
modifications in the way of prompts into a strong FSCIL classifier. Concretely,
we synergistically utilize two complementary types of prompts in each
Transformer block: static prompts to bridge the domain gap between the
pre-training and downstream datasets, thus enabling better adaption; and
dynamic prompts to capture instance-aware semantics, thus enabling easy
transfer from base to novel classes. Specially, to generate dynamic prompts, we
leverage a pre-trained multi-modal model to extract input-related diverse
semantics, thereby generating complementary input-aware prompts, and then
adaptively adjust their importance across different layers. In this way, on top
of the prompted visual embeddings, a simple prototype classifier can beat
state-of-the-arts without further training on the incremental tasks. We conduct
extensive experiments on four benchmarks to validate the effectiveness of our
DSS-Prompt and show that it consistently achieves better performance than
existing approaches on all datasets and can alleviate the catastrophic
forgetting issue as well.

</details>


### [93] [MeMoSORT: Memory-Assisted Filtering and Motion-Adaptive Association Metric for Multi-Person Tracking](https://arxiv.org/abs/2508.09796)
*Yingjie Wang,Zhixing Wang,Le Zheng,Tianxiao Liu,Roujing Li,Xueyao Hu*

Main category: cs.CV

TL;DR: 提出了一种名为MeMoSORT的多目标跟踪器，通过引入记忆强化的卡尔曼滤波器和运动自适应IoU，有效解决了目标运动复杂和严重遮挡问题，表现优于当前方法。


<details>
  <summary>Details</summary>
Motivation: 在人类为主的场景中，多目标跟踪因复杂的目标运动和严重遮挡而面临重大挑战，传统方法受限于卡尔曼滤波和固定IoU关联策略，难以适应实际动态。

Method: 引入记忆辅助卡尔曼滤波器（MeKF）通过记忆强化神经网络补偿运动不匹配，同时采用运动自适应IoU（Mo-IoU）扩展匹配空间并结合高度相似性降低关联错误。

Result: 在DanceTrack和SportsMOT数据集上，MeMoSORT分别获得了67.9%和82.1%的HOTA分数，表现达到当前最优水平。

Conclusion: MeMoSORT是一种简单、在线、实时的多目标跟踪方法，其创新设计显著提高了跟踪性能，解决了传统方法的关键问题。

Abstract: Multi-object tracking (MOT) in human-dominant scenarios, which involves
continuously tracking multiple people within video sequences, remains a
significant challenge in computer vision due to targets' complex motion and
severe occlusions. Conventional tracking-by-detection methods are fundamentally
limited by their reliance on Kalman filter (KF) and rigid Intersection over
Union (IoU)-based association. The motion model in KF often mismatches
real-world object dynamics, causing filtering errors, while rigid association
struggles under occlusions, leading to identity switches or target loss. To
address these issues, we propose MeMoSORT, a simple, online, and real-time MOT
tracker with two key innovations. First, the Memory-assisted Kalman filter
(MeKF) uses memory-augmented neural networks to compensate for mismatches
between assumed and actual object motion. Second, the Motion-adaptive IoU
(Mo-IoU) adaptively expands the matching space and incorporates height
similarity to reduce the influence of detection errors and association
failures, while remaining lightweight. Experiments on DanceTrack and SportsMOT
show that MeMoSORT achieves state-of-the-art performance, with HOTA scores of
67.9\% and 82.1\%, respectively.

</details>


### [94] [MUJICA: Reforming SISR Models for PBR Material Super-Resolution via Cross-Map Attention](https://arxiv.org/abs/2508.09802)
*Xin Du,Maoyuan Xu,Zhi Ying*

Main category: cs.CV

TL;DR: 提出了一个名为MUJICA的方法，利用跨映射注意力机制提升PBR材料的超分辨率重建能力，并解决了普通SISR方法在跨映射一致性和特定模态特征建模方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的SISR方法在处理PBR材料超分辨率时存在跨映射一致性差、特定模态特征建模不足及对数据分布偏移的泛化能力有限等问题，需要一种新方法来改进这些局限性。

Method: 提出了一个名为MUJICA的柔性适配器，基于跨映射注意力机制进行特征融合，重新设计并整合预训练的Swin-transformer型SISR模型，用于PBR材料的超分辨率任务。

Result: MUJICA成功提升了PSNR、SSIM和LPIPS等指标，同时保持了跨映射一致性。实验表明，在有限资源下，MUJICA具有高效的训练能力并达到了当前最先进的性能。

Conclusion: MUJICA显著提高了PBR材料超分辨率的质量和效率，克服了现有方法的局限性，为PBR材料的实际应用提供了有力支持。

Abstract: Physically Based Rendering (PBR) materials are typically characterized by
multiple 2D texture maps such as basecolor, normal, metallic, and roughness
which encode spatially-varying bi-directional reflectance distribution function
(SVBRDF) parameters to model surface reflectance properties and microfacet
interactions. Upscaling SVBRDF material is valuable for modern 3D graphics
applications. However, existing Single Image Super-Resolution (SISR) methods
struggle with cross-map inconsistency, inadequate modeling of modality-specific
features, and limited generalization due to data distribution shifts. In this
work, we propose Multi-modal Upscaling Joint Inference via Cross-map Attention
(MUJICA), a flexible adapter that reforms pre-trained Swin-transformer-based
SISR models for PBR material super-resolution. MUJICA is seamlessly attached
after the pre-trained and frozen SISR backbone. It leverages cross-map
attention to fuse features while preserving remarkable reconstruction ability
of the pre-trained SISR model. Applied to SISR models such as SwinIR, DRCT, and
HMANet, MUJICA improves PSNR, SSIM, and LPIPS scores while preserving cross-map
consistency. Experiments demonstrate that MUJICA enables efficient training
even with limited resources and delivers state-of-the-art performance on PBR
material datasets.

</details>


### [95] [Automated Segmentation of Coronal Brain Tissue Slabs for 3D Neuropathology](https://arxiv.org/abs/2508.09805)
*Jonathan Williams Ramirez,Dina Zemlyanker,Lucas Deden-Binder,Rogeny Herisse,Erendira Garcia Pallares,Karthik Gopinath,Harshvardhan Gazula,Christopher Mount,Liana N. Kozanno,Michael S. Marshall,Theresa R. Connors,Matthew P. Frosch,Mark Montine,Derek H. Oakley,Christine L. Mac Donald,C. Dirk Keene,Bradley T. Hyman,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: 利用深度学习和图像注册技术开发了一种针对脑组织图片的自动分割工具。


<details>
  <summary>Details</summary>
Motivation: 减轻传统脑组织分割中昂贵的人力成本问题，优化后续对脑组织分析的流程。

Method: 使用了基于U-Net的深度学习架构，结合人工标注及合成图像数据，提升模型的泛化能力和准确率。

Result: 模型表现优异，自动分割的Dice得分超过0.98，达到了人工标注的精确度水平。

Conclusion: 开发的自动分割工具为脑组织分析提供了一个高效且可推广的解决方案，并已公开供使用。

Abstract: Advances in image registration and machine learning have recently enabled
volumetric analysis of \emph{postmortem} brain tissue from conventional
photographs of coronal slabs, which are routinely collected in brain banks and
neuropathology laboratories worldwide. One caveat of this methodology is the
requirement of segmentation of the tissue from photographs, which currently
requires costly manual intervention. In this article, we present a deep
learning model to automate this process. The automatic segmentation tool relies
on a U-Net architecture that was trained with a combination of
\textit{(i)}1,414 manually segmented images of both fixed and fresh tissue,
from specimens with varying diagnoses, photographed at two different sites; and
\textit{(ii)}~2,000 synthetic images with randomized contrast and corresponding
masks generated from MRI scans for improved generalizability to unseen
photographic setups. Automated model predictions on a subset of photographs not
seen in training were analyzed to estimate performance compared to manual
labels -- including both inter- and intra-rater variability. Our model achieved
a median Dice score over 0.98, mean surface distance under 0.4~mm, and 95\%
Hausdorff distance under 1.60~mm, which approaches inter-/intra-rater levels.
Our tool is publicly available at surfer.nmr.mgh.harvard.edu/fswiki/PhotoTools.

</details>


### [96] [TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos](https://arxiv.org/abs/2508.09811)
*Jinxi Li,Ziyang Song,Bo Yang*

Main category: cs.CV

TL;DR: 本文提出了TRACE框架，可以在无人工标注的情况下，通过动态多视角视频建模3D场景的几何、外观和物理信息。


<details>
  <summary>Details</summary>
Motivation: 现有方法借助物理相关的损失或简单物理模型，但在学习复杂运动物理时难度较大，或者需要额外的标签（如物体类型或掩码）。

Method: 将每个3D点建模为具有空间尺寸和方向的刚性粒子，并直接学习其平移和旋转动力学系统，明确估算粒子的完整物理参数。

Result: 实验表明，TRACE在三个现有动态数据集和一个新创建的合成数据集上表现卓越，特别是在未来帧外推任务中。

Conclusion: TRACE框架能直接从物理参数中分割多个物体或部分，证明了其潜在多功能性。

Abstract: In this paper, we aim to model 3D scene geometry, appearance, and physical
information just from dynamic multi-view videos in the absence of any human
labels. By leveraging physics-informed losses as soft constraints or
integrating simple physics models into neural nets, existing works often fail
to learn complex motion physics, or doing so requires additional labels such as
object types or masks. We propose a new framework named TRACE to model the
motion physics of complex dynamic 3D scenes. The key novelty of our method is
that, by formulating each 3D point as a rigid particle with size and
orientation in space, we directly learn a translation rotation dynamics system
for each particle, explicitly estimating a complete set of physical parameters
to govern the particle's motion over time. Extensive experiments on three
existing dynamic datasets and one newly created challenging synthetic datasets
demonstrate the extraordinary performance of our method over baselines in the
task of future frame extrapolation. A nice property of our framework is that
multiple objects or parts can be easily segmented just by clustering the
learned physical parameters.

</details>


### [97] [Poaching Hotspot Identification Using Satellite Imagery](https://arxiv.org/abs/2508.09812)
*Aryan Pandhi,Shrey Baid,Sanjali Jha*

Main category: cs.CV

TL;DR: 这篇论文讨论了非洲大象偷猎的问题，提出通过计算机视觉模型结合卫星图像识别偷猎热点。


<details>
  <summary>Details</summary>
Motivation: 非洲大象偷猎仍然猖獗，偷猎热点位置不断变化，需要一种更高效的方法来定位偷猎区域以部署资源。

Method: 提出使用计算机视觉模型结合卫星图像，自动识别偷猎热点，避免手动追踪和破坏当地生态平衡。

Result: 预计模型可以有效识别偷猎热点区域，从而优化资源分配，减少偷猎活动的发生。

Conclusion: 利用计算机视觉技术可以提供一种高效且环保的方法来协助解决大象偷猎问题，对保护濒危物种具有重要意义。

Abstract: Elephant Poaching in African countries has been a decade-old problem. So much
so that African Forest Elephants are now listed as an endangered species, and
African Savannah Elephants as critically endangered by the IUCN (International
Union for Conservation of Nature). [1] Elephants are hunted primarily for their
ivory tusks which caused many elephants to be born tuskless as a genetic
modification for survival. [2] Data gathered by recent studies shows that
though poaching methods remain the same, the poaching grounds are rather
dynamic. Poachers have shifted to areas with less ranger patrols and several
other factors like watering holes, seasons, altitude etc. cause constant shifts
in poaching hotspot locations. [3] After a period of low poaching from
2000-2014, poaching numbers in African countries are now on the rise again --
WWF (World Wildlife Foundation) says there are 20,000 elephants poached
annually [4]. In African countries, anti-poaching efforts are concentrated near
towns, while a majority of poaching occurs in the deserted regions. All of
these factors result in the need for a Computer Vision Model to identify
poaching hotspots through locating the geographic indicators of favorable
poaching regions. A CV model eliminates the need to manually track poachers and
account for the environmental factors to deploy resources and its combination
with satellite imagery allows us to survey large areas without disturbing local
species or cross border aviation restrictions.

</details>


### [98] [Evolution of Low-Level and Texture Human-CLIP Alignment](https://arxiv.org/abs/2508.09814)
*Pablo Hernández-Cámara,Jose Manuel Jaén-Lorites,Jorge Vila-Tomás,Jesus Malo,Valero Laparra*

Main category: cs.CV

TL;DR: CLIP在训练初期与低级别人类图像质量评估相关性较高，但随着训练深入，这种相关性下降。


<details>
  <summary>Details</summary>
Motivation: 探究视觉语言模型CLIP中训练过程中人类感知对齐与分类鲁棒性之间的权衡机制。

Method: 分析CLIP模型训练中的形状-纹理偏差与噪声下分类精度的变化关系。

Result: CLIP初期倾向于学习低级视觉特征，对人类感知对齐良好，但容易受噪声干扰；后期逐渐转向抽象形状表征，提升了抗噪性，但削弱了低级感知对齐。

Conclusion: 研究表明模型训练过程中存在人类感知对齐与鲁棒性间的底层学习机制权衡，为优化视觉语言模型提供了新视角。

Abstract: During the training of multi-modal models like CLIP, we observed an
intriguing phenomenon: the correlation with low-level human image quality
assessments peaks in the early epochs before gradually declining. This study
investigates this observation and seeks to understand its causes through two
key factors: shape-texture bias alignment and classification accuracy drop
under noise. Our findings suggest that CLIP initially learn low-level visual
features, enhancing its alignment with low-level human perception but also
increasing its sensitivity to noise and its texture bias. As training
progresses, the model shifts toward more abstract shape-based representations,
improving noise robustness but reducing alignment with low-level human
perception. These results suggest that these factors shared an underlying
learning mechanism and provide new insights into optimizing the trade-off
between perceptual alignment and robustness in vision-language models.

</details>


### [99] [ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video](https://arxiv.org/abs/2508.09818)
*Rajan Das Gupta,Md Yeasin Rahat,Nafiz Fahad,Abir Ahmed,Liew Tze Hui*

Main category: cs.CV

TL;DR: 提出一种名为ViMoNet的框架，通过结合运动数据和视频数据，增强对人类行为的理解，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用大语言模型，通过结合运动数据和视频数据，更全面地捕捉人类行动的细微动作与含义。

Method: 提出ViMoNet框架，使用联合训练方式，结合详细的运动-文本数据和更广泛但较模糊的视频-文本数据，并提供VIMOS数据集和ViMoNet-Bench基准。

Result: 实验表明，ViMoNet在生成字幕、运动理解和行为解读方面优于现有方法。

Conclusion: ViMoNet能够有效理解、表征和推断人类行为，为相关领域提供了重要的基础和潜力。

Abstract: This study investigates how large language models (LLMs) can be used to
understand human behavior using motion and video data. We think that mixing
both types is essential to completely capture the nuanced movements and
meanings of human actions, in contrast to recent models that simply concentrate
on motion data or films. To address this, we provide ViMoNet, a straightforward
yet effective framework for comprehending, characterizing, and deducing human
action. ViMoNet employs a joint training strategy that leverages the advantages
of two data types: detailed motion-text data, which is more exact, and generic
video-text data, which is more comprehensive but less detailed. This aids in
the model's acquisition of rich data regarding time and space in human
behavior. Additionally, we provide a brand new dataset named VIMOS that
contains a variety of films, motion sequences, instructions, and subtitles. We
developed ViMoNet-Bench, a standardized benchmark with carefully labeled
samples, to evaluate how well models understand human behavior. Our tests show
that ViMoNet outperforms existing methods in caption generation, motion
understanding, and behavior interpretation.

</details>


### [100] [Physical Autoregressive Model for Robotic Manipulation without Action Pretraining](https://arxiv.org/abs/2508.09822)
*Zijian Song,Sihan Qin,Tianshui Chen,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为PAR的物理自回归模型，通过结合视频帧和动作来表示机器人和环境的联合演化，成功在推立方体任务中达成100%成功率，同时在其他任务中与基于动作预训练的模型表现相当。


<details>
  <summary>Details</summary>
Motivation: 因操作数据稀缺，推动了机器人领域使用来自其他模态的预训练大模型的需求。

Method: 通过将视频生成的预训练知识转移到机器人操作，提出物理自回归模型（PAR），结合物理token和连续解码器来实现视频预测和动作轨迹的更高一致性。同时采用了因果掩码、逆运动学、并行训练以及KV-cache机制来提升性能。

Result: 在ManipSkill基准测试中，PAR在PushCube任务中达成100%成功率，并在其他任务中与动作预训练的基线方法表现相当，同时提供精准的视频预测和动作轨迹。

Conclusion: PAR通过从视频预训练转移世界知识，为机器人操控领域的未来探索提供了一个值得期待的方法方向。

Abstract: The scarcity of manipulation data has motivated the use of pretrained large
models from other modalities in robotics. In this work, we build upon
autoregressive video generation models to propose a Physical Autoregressive
Model (PAR), where physical tokens combine frames and actions to represent the
joint evolution of the robot and its environment. PAR leverages the world
knowledge embedded in video pretraining to understand physical dynamics without
requiring action pretraining, enabling accurate video prediction and consistent
action trajectories. It also adopts a DiT-based de-tokenizer to model frames
and actions as continuous tokens, mitigating quantization errors and
facilitating mutual enhancement. Furthermore, we incorporate a causal mask with
inverse kinematics, parallel training, and the KV-cache mechanism to further
improve performance and efficiency. Experiments on the ManiSkill benchmark show
that PAR achieves a 100\% success rate on the PushCube task, matches the
performance of action-pretrained baselines on other tasks, and accurately
predicts future videos with tightly aligned action trajectories. These findings
underscore a promising direction for robotic manipulation by transferring world
knowledge from autoregressive video pretraining.

</details>


### [101] [KonfAI: A Modular and Fully Configurable Framework for Deep Learning in Medical Imaging](https://arxiv.org/abs/2508.09823)
*Valentin Boussot,Jean-Louis Dillenseger*

Main category: cs.CV

TL;DR: KonfAI 是一个开源的深度学习框架，专为医学影像任务设计，简化了训练、推断和评估流程，支持复杂模型架构。


<details>
  <summary>Details</summary>
Motivation: 设计一个可配置的深度学习框架，用来增强医学影像任务的可复现性、透明性，减少开发时间。

Method: 通过 YAML 配置文件定义任务，支持模块化扩展，原生支持高级策略如补丁学习、测试时增强等，同时兼容复杂的多模型训练。

Result: 有效用于分割、配准和图像合成任务，并在国际医学影像挑战赛中取得领先成绩。

Conclusion: KonfAI 提供灵活和高效的工具，用于医学影像领域的深度学习任务，是一个开源且成功的解决方案。

Abstract: KonfAI is a modular, extensible, and fully configurable deep learning
framework specifically designed for medical imaging tasks. It enables users to
define complete training, inference, and evaluation workflows through
structured YAML configuration files, without modifying the underlying code.
This declarative approach enhances reproducibility, transparency, and
experimental traceability while reducing development time. Beyond the
capabilities of standard pipelines, KonfAI provides native abstractions for
advanced strategies including patch-based learning, test-time augmentation,
model ensembling, and direct access to intermediate feature representations for
deep supervision. It also supports complex multi-model training setups such as
generative adversarial architectures. Thanks to its modular and extensible
architecture, KonfAI can easily accommodate custom models, loss functions, and
data processing components. The framework has been successfully applied to
segmentation, registration, and image synthesis tasks, and has contributed to
top-ranking results in several international medical imaging challenges. KonfAI
is open source and available at
\href{https://github.com/vboussot/KonfAI}{https://github.com/vboussot/KonfAI}.

</details>


### [102] [RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians](https://arxiv.org/abs/2508.09830)
*Shenxing Wei,Jinxi Li,Yafei Yang,Siyuan Zhou,Bo Yang*

Main category: cs.CV

TL;DR: 该研究提出了一种名为RayletDF的新方法，用于从点云或3D高斯重构3D表面。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现有方法计算量大的问题，实现从点云或3D高斯快速高效地进行精确表面重建。

Method: 提出RayletDF方法，包括光线片特征提取器、光线片距离场预测器和多光线片混合器三部分，通过这些模块直接从查询光线预测表面点，并实现精确的表面重建。

Result: 实验表明，该方法在多个公开数据集上表现出色，表现出优异的表面重建能力，特别是在单次推理中展现出强大的泛化性能。

Conclusion: RayletDF有效解决了现有方法的计算量问题，提供了一种高效、精确且具备一般化能力的3D表面重建方法。

Abstract: In this paper, we present a generalizable method for 3D surface
reconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from
RGB images. Unlike existing coordinate-based methods which are often
computationally intensive when rendering explicit surfaces, our proposed
method, named RayletDF, introduces a new technique called raylet distance
field, which aims to directly predict surface points from query rays. Our
pipeline consists of three key modules: a raylet feature extractor, a raylet
distance field predictor, and a multi-raylet blender. These components work
together to extract fine-grained local geometric features, predict raylet
distances, and aggregate multiple predictions to reconstruct precise surface
points. We extensively evaluate our method on multiple public real-world
datasets, demonstrating superior performance in surface reconstruction from
point clouds or 3D Gaussians. Most notably, our method achieves exceptional
generalization ability, successfully recovering 3D surfaces in a single-forward
pass across unseen datasets in testing.

</details>


### [103] [Reverse Convolution and Its Applications to Image Restoration](https://arxiv.org/abs/2508.09824)
*Xuhong Huang,Shiqi Liu,Kai Zhang,Ying Tai,Jian Yang,Hui Zeng,Lei Zhang*

Main category: cs.CV

TL;DR: 提出了一种逆深度卷积算子，用以解决传统反卷积无法真正作为卷积逆算子的限制，并通过实验验证其在图像复原任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统反卷积实际并非卷积的数学意义上的逆算子，现有深度学习框架中缺乏标准的逆卷积组件。

Method: 设计了一种新颖的逆深度卷积算子，解决正则化最小二乘优化问题，并结合多种策略确保其可用性；进一步构建了包含该算子的逆卷积块，并在此基础上开发了ConverseNet网络。

Result: 实验表明，所提出的逆卷积算子能够有效替代传统卷积和反卷积层，并在图像去噪、超分辨率以及图像去模糊任务中展现了优异表现。

Conclusion: 该研究展示了逆卷积算子的潜力，希望能激发新的深度学习操作符设计及其在实际应用中的创新发展。

Abstract: Convolution and transposed convolution are fundamental operators widely used
in neural networks. However, transposed convolution (a.k.a. deconvolution) does
not serve as a true inverse of convolution due to inherent differences in their
mathematical formulations. To date, no reverse convolution operator has been
established as a standard component in neural architectures. In this paper, we
propose a novel depthwise reverse convolution operator as an initial attempt to
effectively reverse depthwise convolution by formulating and solving a
regularized least-squares optimization problem. We thoroughly investigate its
kernel initialization, padding strategies, and other critical aspects to ensure
its effective implementation. Building upon this operator, we further construct
a reverse convolution block by combining it with layer normalization,
1$\times$1 convolution, and GELU activation, forming a Transformer-like
structure. The proposed operator and block can directly replace conventional
convolution and transposed convolution layers in existing architectures,
leading to the development of ConverseNet. Corresponding to typical image
restoration models such as DnCNN, SRResNet and USRNet, we train three variants
of ConverseNet for Gaussian denoising, super-resolution and deblurring,
respectively. Extensive experiments demonstrate the effectiveness of the
proposed reverse convolution operator as a basic building module. We hope this
work could pave the way for developing new operators in deep model design and
applications.

</details>


### [104] [Hierarchical Graph Attention Network for No-Reference Omnidirectional Image Quality Assessment](https://arxiv.org/abs/2508.09843)
*Hao Yang,Xu Zhang,Jiaqi Ma,Linwei Zhu,Yun Zhang,Huan Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于图神经网络的全方向图像质量评估方法，通过Fibonacci球面采样生成视口并建模其拓扑关系，利用图注意力网络与图变换器捕捉局部和全局的空间依赖性，实验结果优于当前方法。


<details>
  <summary>Details</summary>
Motivation: 现有的全方向图像质量评估方法无法有效处理局部的非均匀失真，缺乏对空间质量变化的良好建模和特征表征能力。

Method: 利用基于Fibonacci球面采样的方法生成视口，以视口作为图节点，通过多阶段特征提取网络生成高维节点表示，结合图注意力网络和图变换器捕捉局部和远程的空间依赖性。

Result: 在包含复杂空间失真的两个大型OIQA数据库中的实验表明，该方法能够显著优于现有方法，并展示了强大的泛化能力。

Conclusion: 基于图神经网络的方法能够更有效地建模空间失真非均匀性，在全方向图像质量评估中展现了优异的表现和潜力。

Abstract: Current Omnidirectional Image Quality Assessment (OIQA) methods struggle to
evaluate locally non-uniform distortions due to inadequate modeling of spatial
variations in quality and ineffective feature representation capturing both
local details and global context. To address this, we propose a graph neural
network-based OIQA framework that explicitly models structural relationships
between viewports to enhance perception of spatial distortion non-uniformity.
Our approach employs Fibonacci sphere sampling to generate viewports with
well-structured topology, representing each as a graph node. Multi-stage
feature extraction networks then derive high-dimensional node representation.
To holistically capture spatial dependencies, we integrate a Graph Attention
Network (GAT) modeling fine-grained local distortion variations among adjacent
viewports, and a graph transformer capturing long-range quality interactions
across distant regions. Extensive experiments on two large-scale OIQA databases
with complex spatial distortions demonstrate that our method significantly
outperforms existing approaches, confirming its effectiveness and strong
generalization capability.

</details>


### [105] [Enhancing Diffusion Face Generation with Contrastive Embeddings and SegFormer Guidance](https://arxiv.org/abs/2508.09847)
*Dhruvraj Singh Rawat,Enggen Sherpa,Rishikesan Kirupanantha,Tin Hoang*

Main category: cs.CV

TL;DR: 本研究评估了扩散模型在人脸生成中的性能，对比了无条件和条件生成管道，并提出了利用InfoNCE损失和SegFormer进行增强的方法。


<details>
  <summary>Details</summary>
Motivation: 探索在小规模数据集中如何通过属性向导和分割掩码实现高效和可控的人脸生成。

Method: 对比UNet和DiT架构的无条件生成，结合LoRA微调预训练模型，并引入InfoNCE损失与SegFormer增强分割编码。

Result: 新的方法有效提升了属性引导生成的人脸语义对齐和控制能力特别是在有限数据条件下。

Conclusion: 对比学习和高阶分割编码是提升条件下人脸生成性能的有效手段。

Abstract: We present a benchmark of diffusion models for human face generation on a
small-scale CelebAMask-HQ dataset, evaluating both unconditional and
conditional pipelines. Our study compares UNet and DiT architectures for
unconditional generation and explores LoRA-based fine-tuning of pretrained
Stable Diffusion models as a separate experiment. Building on the
multi-conditioning approach of Giambi and Lisanti, which uses both attribute
vectors and segmentation masks, our main contribution is the integration of an
InfoNCE loss for attribute embedding and the adoption of a SegFormer-based
segmentation encoder. These enhancements improve the semantic alignment and
controllability of attribute-guided synthesis. Our results highlight the
effectiveness of contrastive embedding learning and advanced segmentation
encoding for controlled face generation in limited data settings.

</details>


### [106] [ARI3D: A Software for Interactive Quantification of Regions in X-Ray CT 3D Images](https://arxiv.org/abs/2508.09849)
*Jan Phillipp Albrecht,Jose R. A. Godinho,Christina Hübers,Deborah Schmidt*

Main category: cs.CV

TL;DR: 提出了一种名为ARI3D的软件工具，用于交互式分析三维X射线CT图像中的区域。


<details>
  <summary>Details</summary>
Motivation: 旨在解决传统3D图像分析中因成像伪影（如光束硬化和部分体积效应）带来的局限性，并简化用户在分割和分类微结构时的决策步骤。

Method: 通过开发软件工具ARI3D，提供交互式界面，辅助用户实现三维图像中不同区域的定量和分类分析。

Result: ARI3D实现了更好的相位识别，解决了部分体积效应问题，同时提高了物体量化的检测极限和精度，且提供了一个统一的3D定量分析框架。

Conclusion: ARI3D能够显著改善3D X射线CT图像的分析准确性，具备广泛的科学领域应用潜力。

Abstract: X-ray computed tomography (CT) is the main 3D technique for imaging the
internal microstructures of materials. Quantitative analysis of the
microstructures is usually achieved by applying a sequence of steps that are
implemented to the entire 3D image. This is challenged by various imaging
artifacts inherent from the technique, e.g., beam hardening and partial volume.
Consequently, the analysis requires users to make a number of decisions to
segment and classify the microstructures based on the voxel gray-values. In
this context, a software tool, here called ARI3D, is proposed to interactively
analyze regions in three-dimensional X-ray CT images, assisting users through
the various steps of a protocol designed to classify and quantify objects
within regions of a three-dimensional image. ARI3D aims to 1) Improve phase
identification; 2) Account for partial volume effect; 3) Increase the detection
limit and accuracy of object quantification; and 4) Harmonize quantitative 3D
analysis that can be implemented in different fields of science.

</details>


### [107] [Do Vision Transformers See Like Humans? Evaluating their Perceptual Alignment](https://arxiv.org/abs/2508.09850)
*Pablo Hernández-Cámara,Jose Manuel Jaén-Lorites,Jorge Vila-Tomás,Valero Laparra,Jesus Malo*

Main category: cs.CV

TL;DR: 研究探讨了Vision Transformers（ViTs）与人类感知对齐的影响因素，发现模型的尺寸、数据集的多样性、数据增强和正则化都会影响这种对齐程度。


<details>
  <summary>Details</summary>
Motivation: 大多数研究关注ViTs在图像识别任务中的表现，而未探讨它们与人类感知的对齐程度，从而激发了这一研究。

Method: 对ViTs的模型尺寸、数据集尺寸、数据增强和正则化进行系统分析，评估它们在TID2013数据集上的与人类感知对齐的影响。

Result: 较大的模型表现出较低感知对齐；数据集多样性影响较小，重复训练减少对齐；更强的数据增强和正则化进一步降低对齐。

Conclusion: 模型复杂性和训练策略对人类感知对齐存在权衡，这对需要人类视觉理解的应用提出重要考量。

Abstract: Vision Transformers (ViTs) achieve remarkable performance in image
recognition tasks, yet their alignment with human perception remains largely
unexplored. This study systematically analyzes how model size, dataset size,
data augmentation and regularization impact ViT perceptual alignment with human
judgments on the TID2013 dataset. Our findings confirm that larger models
exhibit lower perceptual alignment, consistent with previous works. Increasing
dataset diversity has a minimal impact, but exposing models to the same images
more times reduces alignment. Stronger data augmentation and regularization
further decrease alignment, especially in models exposed to repeated training
cycles. These results highlight a trade-off between model complexity, training
strategies, and alignment with human perception, raising important
considerations for applications requiring human-like visual understanding.

</details>


### [108] [OneVAE: Joint Discrete and Continuous Optimization Helps Discrete Video VAE Train Better](https://arxiv.org/abs/2508.09857)
*Yupeng Zhou,Zhen Li,Ziheng Ouyang,Yuming Chen,Ruoyi Du,Daquan Zhou,Bin Fu,Yihao Liu,Peng Gao,Ming-Ming Cheng,Qibin Hou*

Main category: cs.CV

TL;DR: 提出了一种新的视频VAE方法OneVAE，通过结合离散和连续表示，实现快速收敛并提升性能，同时优化重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决视频VAE中训练不稳定、时间长、重建质量差的问题。

Method: 通过保留连续VAE先验的快速量化策略FSQ，提出多标量量化机制和初始帧重建增强方法，并结合离散与连续的联合优化方法。

Result: 显著提升了视频高压缩场景下的重建质量，实现了对离散与连续表示的统一建模，训练速度更快且性能更优。

Conclusion: OneVAE首次在单一网络中实现了对连续和离散表示的竞争性性能，推进了多模态大模型的视频表示研究。

Abstract: Encoding videos into discrete tokens could align with text tokens to
facilitate concise and unified multi-modal LLMs, yet introducing significant
spatiotemporal compression compared to continuous video representation.
Previous discrete video VAEs experienced unstable training, long training time,
and degraded reconstruction quality. Given the easier training and superior
performance of continuous VAEs, an intuitive idea is to enhance discrete video
VAEs by leveraging continuous VAEs. After rethinking the intrinsic link between
discrete and continuous representations, we found that FSQ could effectively
preserve pre-trained continuous VAE priors compared to other quantization
methods. By leveraging continuous VAE priors, it converges several times faster
than training from scratch and achieves superior performance at convergence.
Meanwhile, two structural improvements are proposed. First, inspired by how
continuous VAEs enhance reconstruction via enlarged latent dimensions, we
introduce a multi-token quantization mechanism, which achieves nearly a 1 dB
improvement in PSNR without compromising the token compression ratio. Second,
to tackle reconstruction challenges in high-compression video VAEs, we
strengthen first-frame reconstruction, enabling the causal VAE to leverage this
information in subsequent frames and markedly improving the performance of 4 x
16 x 16 discrete VAEs. Furthermore, we propose a joint discrete-continuous
optimization scheme that unifies the two paradigms and, for the first time,
achieves competitive performance on both continuous and discrete
representations within a single network. We name our method OneVAE to reflect
this connection.

</details>


### [109] [HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics](https://arxiv.org/abs/2508.09858)
*Weiqi Li,Zehao Zhang,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: 本文提出HumanGenesis框架，通过几何和生成建模的结合，实现了对人类动态的视频生成，解决了现有方法存在的几何不一致性和运动泛化性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前人类动态的视频生成方法存在几何不一致和细节保留不足的问题，同时在运动泛化性和场景融合方面能力较弱，亟需解决。

Method: HumanGenesis框架通过四个协作模块（Reconstructor、Critique Agent、Pose Guider、Video Harmonizer）集成几何建模与生成建模，以提升几何一致性及细节保留，并支持表情丰富的动作生成与场景整合。

Result: HumanGenesis在文本引导的合成、视频改编和新姿态泛化任务中达到了当前最佳性能，显著改善了表情丰富度、几何一致性和场景整合效果。

Conclusion: HumanGenesis是一个卓越的合成人体动态框架，为视频生成领域提供了更高质量、更具泛化能力的解决方案。

Abstract: \textbf{Synthetic human dynamics} aims to generate photorealistic videos of
human subjects performing expressive, intention-driven motions. However,
current approaches face two core challenges: (1) \emph{geometric inconsistency}
and \emph{coarse reconstruction}, due to limited 3D modeling and detail
preservation; and (2) \emph{motion generalization limitations} and \emph{scene
inharmonization}, stemming from weak generative capabilities. To address these,
we present \textbf{HumanGenesis}, a framework that integrates geometric and
generative modeling through four collaborative agents: (1)
\textbf{Reconstructor} builds 3D-consistent human-scene representations from
monocular video using 3D Gaussian Splatting and deformation decomposition. (2)
\textbf{Critique Agent} enhances reconstruction fidelity by identifying and
refining poor regions via multi-round MLLM-based reflection. (3) \textbf{Pose
Guider} enables motion generalization by generating expressive pose sequences
using time-aware parametric encoders. (4) \textbf{Video Harmonizer} synthesizes
photorealistic, coherent video via a hybrid rendering pipeline with diffusion,
refining the Reconstructor through a Back-to-4D feedback loop. HumanGenesis
achieves state-of-the-art performance on tasks including text-guided synthesis,
video reenactment, and novel-pose generalization, significantly improving
expressiveness, geometric fidelity, and scene integration.

</details>


### [110] [E-4DGS: High-Fidelity Dynamic Reconstruction from the Multi-view Event Cameras](https://arxiv.org/abs/2508.09912)
*Chaoran Feng,Zhenyu Tang,Wangbo Yu,Yatian Pang,Yian Zhao,Jianbin Zhao,Li Yuan,Yonghong Tian*

Main category: cs.CV

TL;DR: 提出E-4DGS，一种基于事件相机的动态高斯分布技术，实现快速场景捕获和视图合成。


<details>
  <summary>Details</summary>
Motivation: 当前基于传统RGB相机的视图合成技术在低光、高速运动场景中受限，而事件相机在这些场景中展现出独特优势。

Method: 提出基于事件的初始化方案、事件自适应切片分布算法以及强度重要性裁剪技术；构建六相机360度合成数据集进行评估。

Result: 实现了快速场景捕获和更精确的动态场景视图合成，优于现有基准技术。

Conclusion: 证明了基于多视图事件数据的场景重建和视图合成的可行性和潜力。

Abstract: Novel view synthesis and 4D reconstruction techniques predominantly rely on
RGB cameras, thereby inheriting inherent limitations such as the dependence on
adequate lighting, susceptibility to motion blur, and a limited dynamic range.
Event cameras, offering advantages of low power, high temporal resolution and
high dynamic range, have brought a new perspective to addressing the scene
reconstruction challenges in high-speed motion and low-light scenes. To this
end, we propose E-4DGS, the first event-driven dynamic Gaussian Splatting
approach, for novel view synthesis from multi-view event streams with
fast-moving cameras. Specifically, we introduce an event-based initialization
scheme to ensure stable training and propose event-adaptive slicing splatting
for time-aware reconstruction. Additionally, we employ intensity importance
pruning to eliminate floating artifacts and enhance 3D consistency, while
incorporating an adaptive contrast threshold for more precise optimization. We
design a synthetic multi-view camera setup with six moving event cameras
surrounding the object in a 360-degree configuration and provide a benchmark
multi-view event stream dataset that captures challenging motion scenarios. Our
approach outperforms both event-only and event-RGB fusion baselines and paves
the way for the exploration of multi-view event-based reconstruction as a novel
approach for rapid scene capture.

</details>


### [111] [SpeechForensics: Audio-Visual Speech Representation Learning for Face Forgery Detection](https://arxiv.org/abs/2508.09913)
*Yachao Liang,Min Yu,Gang Li,Jianguo Jiang,Boquan Li,Feng Yu,Ning Zhang,Xiang Meng,Weiqing Huang*

Main category: cs.CV

TL;DR: 本研究提出了一种结合音频和视觉元素的自监督学习方法，以改进面临伪造面部视频检测时的跨数据集泛化能力和鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 通过发现音频信号中蕴含的语音内容可以有效反映面部移动来提高伪造视频检测的精度。

Method: 采用自监督的掩码预测任务，在真实视频中同时编码局部和全局语义信息，以学习精准的音视频语音表示，随后直接转移到伪造检测任务中。

Result: 实验结果表明，在没有参与任何假视频的模型训练的情况下，该方法在跨数据集泛化能力与鲁棒性方面优于最新方法。

Conclusion: 音视频语音表示学习方法在伪造视频检测上展现了显著优越性，尤其是在未见数据集上的表现。

Abstract: Detection of face forgery videos remains a formidable challenge in the field
of digital forensics, especially the generalization to unseen datasets and
common perturbations. In this paper, we tackle this issue by leveraging the
synergy between audio and visual speech elements, embarking on a novel approach
through audio-visual speech representation learning. Our work is motivated by
the finding that audio signals, enriched with speech content, can provide
precise information effectively reflecting facial movements. To this end, we
first learn precise audio-visual speech representations on real videos via a
self-supervised masked prediction task, which encodes both local and global
semantic information simultaneously. Then, the derived model is directly
transferred to the forgery detection task. Extensive experiments demonstrate
that our method outperforms the state-of-the-art methods in terms of
cross-dataset generalization and robustness, without the participation of any
fake video in model training. Code is available at
https://github.com/Eleven4AI/SpeechForensics.

</details>


### [112] [Towards Comprehensive Cellular Characterisation of H&E slides](https://arxiv.org/abs/2508.09926)
*Benjamin Adjadj,Pierre-Antoine Bannier,Guillaume Horent,Sebastien Mandela,Aurore Lyon,Kathryn Schutte,Ulysse Marteau,Valentin Gaury,Laura Dumont,Thomas Mathieu,Reda Belbahri,Benoît Schmauch,Eric Durand,Katharina Von Loga,Lucie Gillet*

Main category: cs.CV

TL;DR: 这篇文章介绍了一个新的模型HistoPLUS，用于H&E切片上的细胞检测、分割和分类，其在跨领域泛化性和罕见细胞类型的分析上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法对罕见细胞类型表现不佳，且跨领域泛化能力有限，因此需要设计一种新方法来提高细胞分析的准确性和鲁棒性。

Method: 研究者提出了HistoPLUS模型，并利用覆盖13种细胞类型的108,722个细胞核的大型数据集进行训练。

Result: 在四个独立外部验证中，HistoPLUS模型在检测质量上超越现有模型5.2%，分类F1得分提升23.7%，同时参数数量减少了5倍。此外，该模型成功泛化到两个未见过的肿瘤学领域。

Conclusion: HistoPLUS模型显著提高了细胞检测与分类性能，尤其在罕见细胞类型上，对肿瘤微环境研究提供了重要支持，并已开放模型权重和代码。

Abstract: Cell detection, segmentation and classification are essential for analyzing
tumor microenvironments (TME) on hematoxylin and eosin (H&E) slides. Existing
methods suffer from poor performance on understudied cell types (rare or not
present in public datasets) and limited cross-domain generalization. To address
these shortcomings, we introduce HistoPLUS, a state-of-the-art model for cell
analysis, trained on a novel curated pan-cancer dataset of 108,722 nuclei
covering 13 cell types. In external validation across 4 independent cohorts,
HistoPLUS outperforms current state-of-the-art models in detection quality by
5.2% and overall F1 classification score by 23.7%, while using 5x fewer
parameters. Notably, HistoPLUS unlocks the study of 7 understudied cell types
and brings significant improvements on 8 of 13 cell types. Moreover, we show
that HistoPLUS robustly transfers to two oncology indications unseen during
training. To support broader TME biomarker research, we release the model
weights and inference code at https://github.com/owkin/histoplus/.

</details>


### [113] [Stable Diffusion Models are Secretly Good at Visual In-Context Learning](https://arxiv.org/abs/2508.09949)
*Trevine Oorloff,Vishwanath Sindagi,Wele Gedara Chaminda Bandara,Ali Shafahi,Amin Ghiasi,Charan Prakash,Reza Ardekani*

Main category: cs.CV

TL;DR: 该研究提出利用现有的Stable Diffusion模型进行视觉领域的上下文学习（V-ICL）。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在自然语言处理中已展示了强大的上下文学习能力，研究希望将此能力扩展到计算机视觉任务中，但现有方法复杂且泛化能力不足。

Method: 该方法通过重新计算Stable Diffusion模型中自注意力层的注意力机制，使其能够显式结合查询和示例提示之间的上下文关系，而无需额外的微调。

Result: 在六个不同的任务中，改进模型表现显著，例如：在Pascal-5i数据集上的前景分割任务中，mIoU分别比最新方法Visual Prompting和IMProv提高了8.9%和3.2%。

Conclusion: 无需额外训练即可实现不同任务的适配，展示了Stable Diffusion模型的潜力，同时通过提示集成进一步提升了性能。

Abstract: Large language models (LLM) in natural language processing (NLP) have
demonstrated great potential for in-context learning (ICL) -- the ability to
leverage a few sets of example prompts to adapt to various tasks without having
to explicitly update the model weights. ICL has recently been explored for
computer vision tasks with promising early outcomes. These approaches involve
specialized training and/or additional data that complicate the process and
limit its generalizability. In this work, we show that off-the-shelf Stable
Diffusion models can be repurposed for visual in-context learning (V-ICL).
Specifically, we formulate an in-place attention re-computation within the
self-attention layers of the Stable Diffusion architecture that explicitly
incorporates context between the query and example prompts. Without any
additional fine-tuning, we show that this repurposed Stable Diffusion model is
able to adapt to six different tasks: foreground segmentation, single object
detection, semantic segmentation, keypoint detection, edge detection, and
colorization. For example, the proposed approach improves the mean intersection
over union (mIoU) for the foreground segmentation task on Pascal-5i dataset by
8.9% and 3.2% over recent methods such as Visual Prompting and IMProv,
respectively. Additionally, we show that the proposed method is able to
effectively leverage multiple prompts through ensembling to infer the task
better and further improve the performance.

</details>


### [114] [January Food Benchmark (JFB): A Public Benchmark Dataset and Evaluation Suite for Multimodal Food Analysis](https://arxiv.org/abs/2508.09966)
*Amir Hosseinian,Ashkan Dehghani Zahedani,Umer Mansoor,Noosheen Hashemi,Mark Woodward*

Main category: cs.CV

TL;DR: 本文介绍了一个专注于自动营养分析的研究，提出了新的基准数据集（JFB）、全面的评估框架以及一种创新的应用评分方法。


<details>
  <summary>Details</summary>
Motivation: 自动化营养分析缺乏标准化的评价方法和高质量的基准数据集。

Method: 构建了公开的JFB数据集（包含1000张有人工验证标注的食品图像），设计了全面的评估框架，并测试了通用及专用模型的基线性能。

Result: 专用模型january/food-vision-v1的总体得分为86.2，较最佳通用模型提高了12.1分。

Conclusion: 本研究为自动营养分析领域提供了一个重要的基准数据集及评价框架，有助于推动未来研究的发展。

Abstract: Progress in AI for automated nutritional analysis is critically hampered by
the lack of standardized evaluation methodologies and high-quality, real-world
benchmark datasets. To address this, we introduce three primary contributions.
First, we present the January Food Benchmark (JFB), a publicly available
collection of 1,000 food images with human-validated annotations. Second, we
detail a comprehensive benchmarking framework, including robust metrics and a
novel, application-oriented overall score designed to assess model performance
holistically. Third, we provide baseline results from both general-purpose
Vision-Language Models (VLMs) and our own specialized model,
january/food-vision-v1. Our evaluation demonstrates that the specialized model
achieves an Overall Score of 86.2, a 12.1-point improvement over the
best-performing general-purpose configuration. This work offers the research
community a valuable new evaluation dataset and a rigorous framework to guide
and benchmark future developments in automated nutritional analysis.

</details>


### [115] [Quo Vadis Handwritten Text Generation for Handwritten Text Recognition?](https://arxiv.org/abs/2508.09936)
*Vittorio Pippi,Konstantina Nikolaidou,Silvia Cascianelli,George Retsinas,Giorgos Sfikas,Rita Cucchiara,Marcus Liwicki*

Main category: cs.CV

TL;DR: 本文探讨了手写文本生成（HTG）技术在低资源环境下提升手写文本识别（HTR）性能的有效性，为历史手稿的数字化提供了一种解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有的HTR系统难以应对小型、作者特定的文稿集合，特别是当这些集合与训练数据分布不一致时，手写文本生成可以生成适合特定风格的合成数据，可能有助于解决这一问题，但其效果尚未被充分评估。

Method: 系统比较了三种最先进的HTG模型（基于生成对抗网络、扩散模型和自回归方法），分析合成数据的视觉和语言特性对HTR微调的影响，并提出选择最佳HTG模型的定量指南。

Result: 研究结果提供了当前HTG方法能力的洞察，并指出了在低资源HTR中的应用尚需改进的关键领域。

Conclusion: 基于实验分析，本文总结了HTG技术提升HTR性能的实用价值，尤其是在低资源手稿转录的情况下，同时对HTG模型的进一步优化提出了建议。

Abstract: The digitization of historical manuscripts presents significant challenges
for Handwritten Text Recognition (HTR) systems, particularly when dealing with
small, author-specific collections that diverge from the training data
distributions. Handwritten Text Generation (HTG) techniques, which generate
synthetic data tailored to specific handwriting styles, offer a promising
solution to address these challenges. However, the effectiveness of various HTG
models in enhancing HTR performance, especially in low-resource transcription
settings, has not been thoroughly evaluated. In this work, we systematically
compare three state-of-the-art styled HTG models (representing the generative
adversarial, diffusion, and autoregressive paradigms for HTG) to assess their
impact on HTR fine-tuning. We analyze how visual and linguistic characteristics
of synthetic data influence fine-tuning outcomes and provide quantitative
guidelines for selecting the most effective HTG model. The results of our
analysis provide insights into the current capabilities of HTG methods and
highlight key areas for further improvement in their application to
low-resource HTR.

</details>


### [116] [AST-n: A Fast Sampling Approach for Low-Dose CT Reconstruction using Diffusion Models](https://arxiv.org/abs/2508.09943)
*Tomás de la Sotta,José M. Saavedra,Héctor Henríquez,Violeta Chang,Aline Xavier*

Main category: cs.CV

TL;DR: 本研究提出了AST-n框架，基于扩散模型加速LDCT去噪，同时利用高阶ODE解算器减少采样步骤，显著提高了推理速度且保持较高图像质量。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT（LDCT）通过降低辐射剂量获得图像，但增加了噪声，影响诊断信心，因此需要开发有效的去噪方法。

Method: 引入AST-n框架，从中间噪声级别开始逆扩散过程，并在条件模型中集成高阶ODE求解器，以减少采样步骤。

Result: 在CT数据集上，AST-n条件模型（AST-25）达到了PSNR超过38 dB和SSIM超过0.95，与标准方法表现接近，将推理时间从约16秒减少到不到1秒。无条件采样则表现较差。

Conclusion: AST-n结合高阶采样器能在保证图像质量的同时快速重建LDCT图像，为扩散模型在临床工作流程中的应用铺平了道路。

Abstract: Low-dose CT (LDCT) protocols reduce radiation exposure but increase image
noise, compromising diagnostic confidence. Diffusion-based generative models
have shown promise for LDCT denoising by learning image priors and performing
iterative refinement. In this work, we introduce AST-n, an accelerated
inference framework that initiates reverse diffusion from intermediate noise
levels, and integrate high-order ODE solvers within conditioned models to
further reduce sampling steps. We evaluate two acceleration paradigms--AST-n
sampling and standard scheduling with high-order solvers -- on the Low Dose CT
Grand Challenge dataset, covering head, abdominal, and chest scans at 10-25 %
of standard dose. Conditioned models using only 25 steps (AST-25) achieve peak
signal-to-noise ratio (PSNR) above 38 dB and structural similarity index (SSIM)
above 0.95, closely matching standard baselines while cutting inference time
from ~16 seg to under 1 seg per slice. Unconditional sampling suffers
substantial quality loss, underscoring the necessity of conditioning. We also
assess DDIM inversion, which yields marginal PSNR gains at the cost of doubling
inference time, limiting its clinical practicality. Our results demonstrate
that AST-n with high-order samplers enables rapid LDCT reconstruction without
significant loss of image fidelity, advancing the feasibility of
diffusion-based methods in clinical workflows.

</details>


### [117] [Story2Board: A Training-Free Approach for Expressive Storyboard Generation](https://arxiv.org/abs/2508.09983)
*David Dinkevich,Matan Levy,Omri Avrahami,Dvir Samuel,Dani Lischinski*

Main category: cs.CV

TL;DR: 本文提出Story2Board，无需训练的框架，通过自然语言生成表达力丰富的分镜头。该方法解决现有方法在空间排布、背景演变和叙事节奏上的不足，利用两种机制提升一致性和叙事的连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有方法过于专注于主体身份，忽视了视觉叙事重要的空间排布、背景发展等关键因素。作者希望提升分镜头的视觉多样性及连贯性。

Method: 引入Latent Panel Anchoring和Reciprocal Attention Value Mixing两个机制，在不改变架构或微调模型的情况下，通过扩散模型生成高一致性且视觉多样的分镜头。使用语言模型将自由形式故事转化为面板级别提示来进行生成，并设计了新的评估基准和场景多样性指标。

Result: 通过定性与定量结果以及用户研究，证明Story2Board在一致性和叙事表现上优于现有技术基准，生成结果更具动态性和叙事吸引力。

Conclusion: Story2Board能够在无需对架构进行更改或微调的条件下生成更加连贯、视觉丰富的分镜头，为视觉叙事的生成提供了新的方向。

Abstract: We present Story2Board, a training-free framework for expressive storyboard
generation from natural language. Existing methods narrowly focus on subject
identity, overlooking key aspects of visual storytelling such as spatial
composition, background evolution, and narrative pacing. To address this, we
introduce a lightweight consistency framework composed of two components:
Latent Panel Anchoring, which preserves a shared character reference across
panels, and Reciprocal Attention Value Mixing, which softly blends visual
features between token pairs with strong reciprocal attention. Together, these
mechanisms enhance coherence without architectural changes or fine-tuning,
enabling state-of-the-art diffusion models to generate visually diverse yet
consistent storyboards. To structure generation, we use an off-the-shelf
language model to convert free-form stories into grounded panel-level prompts.
To evaluate, we propose the Rich Storyboard Benchmark, a suite of open-domain
narratives designed to assess layout diversity and background-grounded
storytelling, in addition to consistency. We also introduce a new Scene
Diversity metric that quantifies spatial and pose variation across storyboards.
Our qualitative and quantitative results, as well as a user study, show that
Story2Board produces more dynamic, coherent, and narratively engaging
storyboards than existing baselines.

</details>


### [118] [LIA-X: Interpretable Latent Portrait Animator](https://arxiv.org/abs/2508.09959)
*Yaohui Wang,Di Yang,Xinyuan Chen,Francois Bremond,Yu Qiao,Antitza Dantcheva*

Main category: cs.CV

TL;DR: 介绍了LIA-X，一种新型的可解释肖像动画器，利用稀疏运动字典和线性导航方法实现细粒度的面部动态控制。结论表明其性能优于现有方法，并支持用户引导的精细编辑等实际应用。


<details>
  <summary>Details</summary>
Motivation: 通过结合创新的稀疏运动字典和线性导航策略，解决过去在细粒度面部动态控制和解释性方面的不足。

Method: 提出一种名为LIA-X的自动编码器，采用稀疏运动字典将面部动态分解为可解释因子，并采用‘编辑-变形-渲染’策略进行运动转移。

Result: 在自我重演和交叉重演任务中优于现有方法，具备可扩展性，并实现了1亿参数规模的模型训练效果。

Conclusion: LIA-X在面部动态控制和实际应用中表现优异，具备良好的可解释性和用户操控性，支持精细化编辑和三维肖像操作。

Abstract: We introduce LIA-X, a novel interpretable portrait animator designed to
transfer facial dynamics from a driving video to a source portrait with
fine-grained control. LIA-X is an autoencoder that models motion transfer as a
linear navigation of motion codes in latent space. Crucially, it incorporates a
novel Sparse Motion Dictionary that enables the model to disentangle facial
dynamics into interpretable factors. Deviating from previous 'warp-render'
approaches, the interpretability of the Sparse Motion Dictionary allows LIA-X
to support a highly controllable 'edit-warp-render' strategy, enabling precise
manipulation of fine-grained facial semantics in the source portrait. This
helps to narrow initial differences with the driving video in terms of pose and
expression. Moreover, we demonstrate the scalability of LIA-X by successfully
training a large-scale model with approximately 1 billion parameters on
extensive datasets. Experimental results show that our proposed method
outperforms previous approaches in both self-reenactment and cross-reenactment
tasks across several benchmarks. Additionally, the interpretable and
controllable nature of LIA-X supports practical applications such as
fine-grained, user-guided image and video editing, as well as 3D-aware portrait
video manipulation.

</details>


### [119] [MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image Classification](https://arxiv.org/abs/2508.09967)
*Tianqi Xiang,Yi Li,Qixiang Zhang,Xiaomeng Li*

Main category: cs.CV

TL;DR: 本研究提出了一种名为Meta-Optimized Classifier (MOC)的新方法，通过自动优化分类器配置，提高了少样本病理学图像分类的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的少样本方法依赖传统分类器设计，对数据稀缺存在关键脆弱性。

Method: 提出Meta-Optimized Classifier (MOC)，由元学习器和分类器库组成，通过自动优化混合分类器配置实现全方位的病理学解读。

Result: 在多个少样本基准上，MOC的表现优于现有方法，尤其是在TCGA-NSCLC基准上，AUC提升了10.4%，1-shot条件下，提升高达26.25%。

Conclusion: MOC在少样本病理学图像分类中展示了显著优越性，为数据严重不足的临床应用提供了重要进展。

Abstract: Recent advances in histopathology vision-language foundation models (VLFMs)
have shown promise in addressing data scarcity for whole slide image (WSI)
classification via zero-shot adaptation. However, these methods remain
outperformed by conventional multiple instance learning (MIL) approaches
trained on large datasets, motivating recent efforts to enhance VLFM-based WSI
classification through fewshot learning paradigms. While existing few-shot
methods improve diagnostic accuracy with limited annotations, their reliance on
conventional classifier designs introduces critical vulnerabilities to data
scarcity. To address this problem, we propose a Meta-Optimized Classifier (MOC)
comprising two core components: (1) a meta-learner that automatically optimizes
a classifier configuration from a mixture of candidate classifiers and (2) a
classifier bank housing diverse candidate classifiers to enable a holistic
pathological interpretation. Extensive experiments demonstrate that MOC
outperforms prior arts in multiple few-shot benchmarks. Notably, on the
TCGA-NSCLC benchmark, MOC improves AUC by 10.4% over the state-of-the-art
few-shot VLFM-based methods, with gains up to 26.25% under 1-shot conditions,
offering a critical advancement for clinical deployments where diagnostic
training data is severely limited. Code is available at
https://github.com/xmed-lab/MOC.

</details>


### [120] [PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations from a Single Image](https://arxiv.org/abs/2508.09973)
*Geonhee Sim,Gyeongsik Moon*

Main category: cs.CV

TL;DR: 提出了PERSONA框架，通过结合3D和扩散方法，从单张图像生成具有个性化和姿态驱动变形的3D人类化身。


<details>
  <summary>Details</summary>
Motivation: 解决当前3D和扩散方法分别在个性化和姿态驱动变形上的局限性，同时降低获取姿态丰富视频的成本和难度。

Method: 融合扩散生成与3D优化，生成姿态丰富的视频，并通过平衡采样和几何加权优化确保身份真实性和多姿态的高质量渲染。

Result: PERSONA框架能够从一张单一图片生成高质量、个性化并具有姿态驱动变形的3D人类化身。

Conclusion: 该研究展示了将扩散生成与3D建模结合的有效性，为个性化3D化身建模提供了一种高效且具有实际意义的解决方案。

Abstract: Two major approaches exist for creating animatable human avatars. The first,
a 3D-based approach, optimizes a NeRF- or 3DGS-based avatar from videos of a
single person, achieving personalization through a disentangled identity
representation. However, modeling pose-driven deformations, such as non-rigid
cloth deformations, requires numerous pose-rich videos, which are costly and
impractical to capture in daily life. The second, a diffusion-based approach,
learns pose-driven deformations from large-scale in-the-wild videos but
struggles with identity preservation and pose-dependent identity entanglement.
We present PERSONA, a framework that combines the strengths of both approaches
to obtain a personalized 3D human avatar with pose-driven deformations from a
single image. PERSONA leverages a diffusion-based approach to generate
pose-rich videos from the input image and optimizes a 3D avatar based on them.
To ensure high authenticity and sharp renderings across diverse poses, we
introduce balanced sampling and geometry-weighted optimization. Balanced
sampling oversamples the input image to mitigate identity shifts in
diffusion-generated training videos. Geometry-weighted optimization prioritizes
geometry constraints over image loss, preserving rendering quality in diverse
poses.

</details>


### [121] [A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation](https://arxiv.org/abs/2508.09977)
*Shuting He,Peilin Ji,Yitong Yang,Changshuo Wang,Jiayi Ji,Yinglin Wang,Henghui Ding*

Main category: cs.CV

TL;DR: 本文综述了关于3D Gaussian Splatting（3DGS）的最新研究进展及其在3D场景表示中的应用，特别注重其在几何和语义理解任务上的潜力。


<details>
  <summary>Details</summary>
Motivation: 3DGS作为一种高效替代NeRF进行3D场景表示的技术，能够实现高质量的光学渲染与实时性能，激发了对其在语义理解和几何任务应用的探索。

Method: 文章先介绍支持3DGS应用中语义理解和控制的2D基础模型，回顾NeRF相关方法以协助理解3DGS方法的技术来源，并通过应用类型（如分割、编辑、生成、功能任务）对现有方法进行分类和总结。

Result: 归纳了各类3DGS应用中典型的方法、监督策略、学习范式，梳理了公共数据集和评价协议，并对方法在公共基准上的表现进行了比较分析。

Conclusion: 3DGS拥有广泛的下游应用潜力，文章提供了关于其最新进展的系统性综述以及一个持续更新的资源库，期望推动这一领域的进一步研究和开发。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a powerful alternative
to Neural Radiance Fields (NeRF) for 3D scene representation, offering
high-fidelity photorealistic rendering with real-time performance. Beyond novel
view synthesis, the explicit and compact nature of 3DGS enables a wide range of
downstream applications that require geometric and semantic understanding. This
survey provides a comprehensive overview of recent progress in 3DGS
applications. It first introduces 2D foundation models that support semantic
understanding and control in 3DGS applications, followed by a review of
NeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGS
applications into segmentation, editing, generation, and other functional
tasks. For each, we summarize representative methods, supervision strategies,
and learning paradigms, highlighting shared design principles and emerging
trends. Commonly used datasets and evaluation protocols are also summarized,
along with comparative analyses of recent methods across public benchmarks. To
support ongoing research and development, a continually updated repository of
papers, code, and resources is maintained at
https://github.com/heshuting555/Awesome-3DGS-Applications.

</details>


### [122] [LLMC+: Benchmarking Vision-Language Model Compression with a Plug-and-play Toolkit](https://arxiv.org/abs/2508.09981)
*Chengtao Lv,Bilang Zhang,Yang Yong,Ruihao Gong,Yushi Huang,Shiqiao Gu,Jiajun Wu,Yumeng Shi,Jinyang Guo,Wenya Wang*

Main category: cs.CV

TL;DR: 提出了LLMC+，一个用于视觉-语言大模型（VLMs）的全面压缩基准，提供模块化工具箱支持超过20种算法。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在计算和内存需求上成本过高，当前的压缩方法存在模块不可比较、任务评估简单化和技术独立性等问题。

Method: 开发了LLMC+基准，支持对五类VLM模型的模块化压缩研究和评估，通过20多种算法探讨令牌和模型压缩的交互潜力。

Result: 研究显示空间和时间冗余需不同策略、令牌压缩在多轮对话和细节敏感任务中性能严重下降、令牌与模型联合压缩可以实现极致压缩且性能损失较小。

Conclusion: LLMC+提供了公平评估的平台，可能促进高效VLM更多创新。

Abstract: Large Vision-Language Models (VLMs) exhibit impressive multi-modal
capabilities but suffer from prohibitive computational and memory demands, due
to their long visual token sequences and massive parameter sizes. To address
these issues, recent works have proposed training-free compression methods.
However, existing efforts often suffer from three major limitations: (1)
Current approaches do not decompose techniques into comparable modules,
hindering fair evaluation across spatial and temporal redundancy. (2)
Evaluation confined to simple single-turn tasks, failing to reflect performance
in realistic scenarios. (3) Isolated use of individual compression techniques,
without exploring their joint potential. To overcome these gaps, we introduce
LLMC+, a comprehensive VLM compression benchmark with a versatile,
plug-and-play toolkit. LLMC+ supports over 20 algorithms across five
representative VLM families and enables systematic study of token-level and
model-level compression. Our benchmark reveals that: (1) Spatial and temporal
redundancies demand distinct technical strategies. (2) Token reduction methods
degrade significantly in multi-turn dialogue and detail-sensitive tasks. (3)
Combining token and model compression achieves extreme compression with minimal
performance loss. We believe LLMC+ will facilitate fair evaluation and inspire
future research in efficient VLM. Our code is available at
https://github.com/ModelTC/LightCompress.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [123] [ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning](https://arxiv.org/abs/2508.09303)
*Shu Zhao,Tan Yu,Anbang Xu,Japinder Singh,Aaditya Shukla,Rama Akkiraju*

Main category: cs.CL

TL;DR: ParallelSearch通过并行化搜索操作提高了搜索效率，并显著提升了多步骤信息检索任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有搜索代理在处理需多实体比较的任务时存在效率瓶颈，该研究旨在解决这一问题。

Method: 提出ParallelSearch，通过强化学习框架，识别可并行的查询结构并并行执行多项搜索操作。

Result: 在七个问答基准测试中，性能平均提升2.9%，在可并行的问题上，性能提升12.7%，LLM调用减少至69.6%。

Conclusion: ParallelSearch显著提升了查询处理效率和准确性，特别是对需多步并行的复杂任务。

Abstract: Reasoning-augmented search agents such as Search-R1, trained via
reinforcement learning with verifiable rewards (RLVR), demonstrate remarkable
capabilities in multi-step information retrieval from external knowledge
sources. These agents address the limitations of their parametric memory by
dynamically gathering relevant facts to address complex reasoning tasks.
However, existing approaches suffer from a fundamental architectural
limitation: they process search queries strictly sequentially, even when
handling inherently parallelizable and logically independent comparisons. This
sequential bottleneck significantly constrains computational efficiency,
particularly for queries that require multiple entity comparisons. To address
this critical limitation, we propose ParallelSearch, a novel reinforcement
learning framework that empowers large language models (LLMs) to recognize
parallelizable query structures and execute multiple search operations
concurrently. Our approach introduces dedicated reward functions that
incentivize the identification of independent query components while preserving
answer accuracy through jointly considering correctness, query decomposition
quality, and parallel execution benefits. Comprehensive experiments demonstrate
that ParallelSearch outperforms state-of-the-art baselines by an average
performance gain of 2.9% across seven question-answering benchmarks. Notably,
on parallelizable questions, our method achieves a 12.7% performance
improvement while requiring only 69.6% of the LLM calls compared to sequential
approaches.

</details>


### [124] [Leveraging Large Language Models for Rare Disease Named Entity Recognition](https://arxiv.org/abs/2508.09323)
*Nan Miles Xi,Yu Deng,Lin Wang*

Main category: cs.CL

TL;DR: 这篇论文研究了GPT-4o在低资源环境下识别罕见疾病领域命名实体（NER）的能力，提出了多种基于提示的策略，取得了接近或优于BioClinicalBERT的表现。


<details>
  <summary>Details</summary>
Motivation: 罕见疾病领域的NER任务存在标注数据缺乏、语义歧义和长尾分布等挑战，研究如何在低资源环境下提高NER性能具有重要意义。

Method: 论文提出了包含零样本、少样本提示学习、检索增强生成（RAG）以及任务级微调的多种基于提示的策略，并设计了结构化提示框架引入领域知识和消歧规则，同时提出两种语义引导的少样本示例选择方法。

Result: GPT-4o在RareDis语料库上的表现接近甚至优于BioClinicalBERT，其中任务级微调达到了新的SOTA结果。少样本提示在低成本代价下具备较高性价比，而RAG的额外收益有限。

Conclusion: 优化提示的GPT-4o能够作为传统监督学习模型的有效替代方案，在生物医学NER，尤其是罕见疾病应用场景中具有可扩展性。

Abstract: Named Entity Recognition (NER) in the rare disease domain poses unique
challenges due to limited labeled data, semantic ambiguity between entity
types, and long-tail distributions. In this study, we evaluate the capabilities
of GPT-4o for rare disease NER under low-resource settings, using a range of
prompt-based strategies including zero-shot prompting, few-shot in-context
learning, retrieval-augmented generation (RAG), and task-level fine-tuning. We
design a structured prompting framework that encodes domain-specific knowledge
and disambiguation rules for four entity types. We further introduce two
semantically guided few-shot example selection methods to improve in-context
performance while reducing labeling effort. Experiments on the RareDis Corpus
show that GPT-4o achieves competitive or superior performance compared to
BioClinicalBERT, with task-level fine-tuning yielding new state-of-the-art
(SOTA) results. Cost-performance analysis reveals that few-shot prompting
delivers high returns at low token budgets, while RAG offers marginal
additional benefit. An error taxonomy highlights common failure modes such as
boundary drift and type confusion, suggesting opportunities for post-processing
and hybrid refinement. Our results demonstrate that prompt-optimized LLMs can
serve as effective, scalable alternatives to traditional supervised models in
biomedical NER, particularly in rare disease applications where annotated data
is scarce.

</details>


### [125] [TEN: Table Explicitization, Neurosymbolically](https://arxiv.org/abs/2508.09324)
*Nikita Mehrotra,Aayush Kumar,Sumit Gulwani,Arjun Radhakrishna,Ashish Tiwari*

Main category: cs.CL

TL;DR: 提出了一种称为TEN的神经符号方法，用于从半结构化输入文本中提取表格数据，并在各类数据和指标上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决从无一致分隔符的半结构化输入文本中提取表格数据的问题，同时避免纯神经方法的幻觉与约束不足问题。

Method: 采用神经符号方法，其中包括使用大语言模型的结构分解提示生成初始表格，再用符号检查器评估表格质量，并通过批评语言模型生成的修正建议引导自调循环。

Result: 在多数据集和多项指标上，TEN显著优于纯神经基线，用户研究亦显示其生成表格更准确，更易验证和修正，用户更倾向于选择此方法。

Conclusion: TEN通过结合神经网络与符号方法，成功改进表格生成的准确性并减少幻觉现象，为表格数据处理提供了新的可靠解决方案。

Abstract: We present a neurosymbolic approach, TEN, for extracting tabular data from
semistructured input text. This task is particularly challenging for text input
that does not use special delimiters consistently to separate columns and rows.
Purely neural approaches perform poorly due to hallucinations and their
inability to enforce hard constraints. TEN uses Structural Decomposition
prompting - a specialized chain-of-thought prompting approach - on a large
language model (LLM) to generate an initial table, and thereafter uses a
symbolic checker to evaluate not only the well-formedness of that table, but
also detect cases of hallucinations or forgetting. The output of the symbolic
checker is processed by a critique-LLM to generate guidance for fixing the
table, which is presented to the original LLM in a self-debug loop. Our
extensive experiments demonstrate that TEN significantly outperforms purely
neural baselines across multiple datasets and metrics, achieving significantly
higher exact match accuracy and substantially reduced hallucination rates. A
21-participant user study further confirms that TEN's tables are rated
significantly more accurate (mean score: 5.0 vs 4.3; p = 0.021), and are
consistently preferred for ease of verification and correction, with
participants favoring our method in over 60% of the cases.

</details>


### [126] [Decoding Neural Emotion Patterns through Natural Language Processing Embeddings](https://arxiv.org/abs/2508.09337)
*Gideon Vos,Maryam Ebrahimpour,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: cs.CL

TL;DR: 提出了一种无需神经影像的文本情感内容到脑区的映射方法，通过生成高维语义表示、降维与聚类，结合情感分析，并开展了三项实验。


<details>
  <summary>Details</summary>
Motivation: 融合神经影像和计算文本分析研究情感表达与大脑功能的关系，以解决传统神经影像方法费用高且限制性强的问题。

Method: 使用OpenAI的text-embedding-ada-002生成文本语义表示，进行情感分组，与18个脑区的情感处理功能进行匹配，通过分析健康与抑郁人群对话数据、GoEmotions数据集，以及人与大语言模型的文本差异展开实验。

Result: 方法呈现出解剖上合理的映射，抑郁受试者被观察到更高的边缘系统活动，离散情感得以区分。大语言模型生成的文本在基本情感分布上与人类相似，但欠缺对共情和自我指涉区域的激活。

Conclusion: 该创新方法为大规模自然语言情感分析提供方便和低成本的途径，可区分临床人群，并为评估AI情感表达提供脑科学基准。

Abstract: Understanding how emotional expression in language relates to brain function
is a challenge in computational neuroscience and affective computing.
Traditional neuroimaging is costly and lab-bound, but abundant digital text
offers new avenues for emotion-brain mapping. Prior work has largely examined
neuroimaging-based emotion localization or computational text analysis
separately, with little integration. We propose a computational framework that
maps textual emotional content to anatomically defined brain regions without
requiring neuroimaging. Using OpenAI's text-embedding-ada-002, we generate
high-dimensional semantic representations, apply dimensionality reduction and
clustering to identify emotional groups, and map them to 18 brain regions
linked to emotional processing. Three experiments were conducted: i) analyzing
conversational data from healthy vs. depressed subjects (DIAC-WOZ dataset) to
compare mapping patterns, ii) applying the method to the GoEmotions dataset and
iii) comparing human-written text with large language model (LLM) responses to
assess differences in inferred brain activation. Emotional intensity was scored
via lexical analysis. Results showed neuroanatomically plausible mappings with
high spatial specificity. Depressed subjects exhibited greater limbic
engagement tied to negative affect. Discrete emotions were successfully
differentiated. LLM-generated text matched humans in basic emotion distribution
but lacked nuanced activation in empathy and self-referential regions (medial
prefrontal and posterior cingulate cortex). This cost-effective, scalable
approach enables large-scale analysis of naturalistic language, distinguishes
between clinical populations, and offers a brain-based benchmark for evaluating
AI emotional expression.

</details>


### [127] [The Human-AI Hybrid Delphi Model: A Structured Framework for Context-Rich, Expert Consensus in Complex Domains](https://arxiv.org/abs/2508.09349)
*Cathy Speed,Ahmed A. Metwally*

Main category: cs.CL

TL;DR: 提出了一种结合生成式AI模型和人类专家的小组协作的新型共识开发框架，称为HAH-Delphi，并在多个领域成功验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统专家共识方法在证据复杂性高和信息碎片化的领域中显现出局限性，包括负担高、过度简化解释以及抑制条件细节表达等问题。HAH-Delphi的开发旨在解决这些挑战。

Method: 该研究结合了生成式AI模型（Gemini 2.5 Pro）、六名资深人类专家组成的小型小组，以及结构化的协作方法，并通过三个阶段（回顾性复制、前瞻性比较、实际应用）对框架进行测试。

Result: AI在第一阶段中复制了95%的已发布专家共识结果并在第二阶段与资深人类专家达成了95%的方向一致性。在实际应用阶段，小型专家组在>90%的共识覆盖率下达到主题饱和。AI成为支持分歧解决和加速饱和的重要工具。

Conclusion: HAH-Delphi框架展现了灵活性和可扩展性，能够用于生成高质量、情景敏感性的共识，可作为生成个性化指导和大规模共识框架的基础。

Abstract: Expert consensus plays a critical role in domains where evidence is complex,
conflicting, or insufficient for direct prescription. Traditional methods, such
as Delphi studies, consensus conferences, and systematic guideline synthesis,
offer structure but face limitations including high panel burden, interpretive
oversimplification, and suppression of conditional nuance. These challenges are
now exacerbated by information overload, fragmentation of the evidence base,
and increasing reliance on publicly available sources that lack expert
filtering. This study introduces and evaluates a Human-AI Hybrid Delphi
(HAH-Delphi) framework designed to augment expert consensus development by
integrating a generative AI model (Gemini 2.5 Pro), small panels of senior
human experts, and structured facilitation. The HAH-Delphi was tested in three
phases: retrospective replication, prospective comparison, and applied
deployment in two applied domains (endurance training and resistance and mixed
cardio/strength training). The AI replicated 95% of published expert consensus
conclusions in Phase I and showed 95% directional agreement with senior human
experts in Phase II, though it lacked experiential and pragmatic nuance. In
Phase III, compact panels of six senior experts achieved >90% consensus
coverage and reached thematic saturation before the final participant. The AI
provided consistent, literature-grounded scaffolding that supported divergence
resolution and accelerated saturation. The HAH-Delphi framework offers a
flexible, scalable approach for generating high-quality, context-sensitive
consensus. Its successful application across health, coaching, and performance
science confirms its methodological robustness and supports its use as a
foundation for generating conditional, personalised guidance and published
consensus frameworks at scale.

</details>


### [128] [Flow-SLM: Joint Learning of Linguistic and Acoustic Information for Spoken Language Modeling](https://arxiv.org/abs/2508.09350)
*Ju-Chieh Chou,Jiawei Zhou,Karen Livescu*

Main category: cs.CL

TL;DR: 本文探索无文本监督语言模型与语音结合的新方法，通过生成语义标记与连续声学帧的方式，相较传统模型增强声学细节生成能力。


<details>
  <summary>Details</summary>
Motivation: 传统无文本语音生成模型（SLMs）缺乏对声学上下文的访问，也无法有效控制声学细节生成。该研究旨在解决这一缺陷，整合语义与声学信息生成更自然的语音。

Method: 提出联合生成语义标记和连续声学帧的模型，基于flow-matching目标函数对连续声学向量进行预测，同时通过研究多未来语义标记预测的设计空间以改善语言信息保留。

Result: 所提出的方法在语言可能性基准测试中与现有模型表现相当，但在生成的声学细节上表现更优。

Conclusion: 将语言信息与声学细节建模相结合的方法在生成细腻的语音上展现出很大的潜力。

Abstract: Textless spoken language models (SLMs) are generative models of speech that
do not rely on text supervision. Most textless SLMs learn to predict the next
semantic token, a discrete representation of linguistic content, and rely on a
separate vocoder to add acoustic information to the generated speech. Such
models have no access to acoustic context and no built-in control over acoustic
details. In this work, we propose to jointly model linguistic and acoustic
information by generating semantic tokens and a continuous real-valued
representation of the acoustic frame. We use a flow-matching objective to
predict the continuous vector conditioned on the semantic tokens. We study the
design space of this approach and find that predicting multiple future semantic
tokens helps preserve linguistic information. Our approach achieves comparable
performance to existing models in terms of linguistic likelihood benchmarks,
while providing better acoustic detail in prompted generation.

</details>


### [129] [APIO: Automatic Prompt Induction and Optimization for Grammatical Error Correction and Text Simplification](https://arxiv.org/abs/2508.09378)
*Artem Chernodub,Aman Saini,Yejin Huh,Vivek Kulkarni,Vipul Raheja*

Main category: cs.CL

TL;DR: 本文推导和优化了一种新的提示方法，名为APIO，用于语法错误纠正和文本简化，且无需手动设置初始提示，达到了当前全新水平。


<details>
  <summary>Details</summary>
Motivation: 通过自动提示优化技术提升大型语言模型在特定任务上的性能，最大化模型能力。

Method: 提出了一种名为APIO的提示方法，通过诱导和优化方式替代手动设定初始提示，用以改进任务性能。

Result: APIO在语法错误纠正和文本简化任务上实现了基于LLM提示方法的新性能高点。

Conclusion: APIO的有效性证明自动提示优化技术在实际应用中具备重要价值，并推动了相关研究方向。

Abstract: Recent advancements in large language models (LLMs) have enabled a wide range
of natural language processing (NLP) tasks to be performed through simple
prompt-based interactions. Consequently, several approaches have been proposed
to engineer prompts that most effectively enable LLMs to perform a given task
(e.g., chain-of-thought prompting). In settings with a well-defined metric to
optimize model performance, automatic prompt optimization (APO) methods have
been developed to refine a seed prompt. Advancing this line of research, we
propose APIO, a simple but effective prompt induction and optimization approach
for the tasks of Grammatical Error Correction (GEC) and Text Simplification,
without relying on manually specified seed prompts. APIO achieves a new
state-of-the-art performance for purely LLM-based prompting methods on these
tasks. We make our data, code, prompts, and outputs publicly available.

</details>


### [130] [Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models](https://arxiv.org/abs/2508.09403)
*Ting Cai,Stephen Sheen,AnHai Doan*

Main category: cs.CL

TL;DR: 本研究讨论了用于表格列名缩写扩展的任务，提出新数据集、新评价指标和新方法Columbo，并证明其在多个数据集上性能优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 表格列名缩写扩展在企业、科学、政府等多领域数据任务中有关键作用，但现有方法和评价数据集存在局限性。

Method: 提出Columbo方法，该方法结合上下文、规则、逐步推理和标记级分析，同时引入4个新数据集和新的同义词敏感性评价指标。

Result: 实验证明，Columbo在5个数据集上性能比当前最优方法NameGuess提升4%-29%。

Conclusion: Columbo的性能显著优于现有方法，并成功应用于环境科学数据门户（EDI）的实际生产环境。

Abstract: Expanding the abbreviated column names of tables, such as ``esal'' to
``employee salary'', is critical for numerous downstream data tasks. This
problem arises in enterprises, domain sciences, government agencies, and more.
In this paper we make three contributions that significantly advances the state
of the art. First, we show that synthetic public data used by prior work has
major limitations, and we introduce 4 new datasets in enterprise/science
domains, with real-world abbreviations. Second, we show that accuracy measures
used by prior work seriously undercount correct expansions, and we propose new
synonym-aware measures that capture accuracy much more accurately. Finally, we
develop Columbo, a powerful LLM-based solution that exploits context, rules,
chain-of-thought reasoning, and token-level analysis. Extensive experiments
show that Columbo significantly outperforms NameGuess, the current most
advanced solution, by 4-29\%, over 5 datasets. Columbo has been used in
production on EDI, a major data portal for environmental sciences.

</details>


### [131] [Leveraging Zipformer Model for Effective Language Identification in Code-Switched Child-Directed Speech](https://arxiv.org/abs/2508.09430)
*Lavanya Shankar,Leibny Paola Garcia Perera*

Main category: cs.CL

TL;DR: 本文研究了如何通过Zipformer模型应对双语环境中的语言识别挑战，特别是在含有不平衡语言的语音情境中。


<details>
  <summary>Details</summary>
Motivation: 在儿童语言交互场景中，区分并识别双语（如普通话和英语）语音是一项复杂任务，为解决该问题提出使用Zipformer。

Method: 采用Zipformer模型处理双语语音数据，通过分析模型内部层特性，并与多种后端方法进行比较来评估其表现。

Result: 实验表明，Zipformer在处理不平衡语言数据上表现出色，准确率达到81.89%，比基线提升15.47%。

Conclusion: Zipformer展示了变压器编码架构在实际双语语音识别中的潜力，尤其在处理语言失衡数据时效果显著。

Abstract: Code-switching and language identification in child-directed scenarios
present significant challenges, particularly in bilingual environments. This
paper addresses this challenge by using Zipformer to handle the nuances of
speech, which contains two imbalanced languages, Mandarin and English, in an
utterance. This work demonstrates that the internal layers of the Zipformer
effectively encode the language characteristics, which can be leveraged in
language identification. We present the selection methodology of the inner
layers to extract the embeddings and make a comparison with different
back-ends. Our analysis shows that Zipformer is robust across these backends.
Our approach effectively handles imbalanced data, achieving a Balanced Accuracy
(BAC) of 81.89%, a 15.47% improvement over the language identification
baseline. These findings highlight the potential of the transformer encoder
architecture model in real scenarios.

</details>


### [132] [From Charts to Fair Narratives: Uncovering and Mitigating Geo-Economic Biases in Chart-to-Text](https://arxiv.org/abs/2508.09450)
*Ridwan Mahbub,Mohammed Saidul Islam,Mir Tafseer Nayeem,Md Tahmid Rahman Laskar,Mizanur Rahman,Shafiq Joty,Enamul Hoque*

Main category: cs.CL

TL;DR: 本文探讨了大规模视觉语言模型（VLMs）在生成图表摘要中的地缘经济偏见问题，研究发现高收入国家的图表描述更积极，并提出了部分有效的消偏方法。


<details>
  <summary>Details</summary>
Motivation: 分析图表中的地缘经济偏见，以揭示VLMs生成内容的潜在社会影响和危害。

Method: 通过6个模型生成的6000个图表-国家对进行大型评估，分析国家经济状况对生成摘要情绪的影响；并尝试使用提示词的方式去偏。

Result: 现有VLM倾向于对高收入国家生成更积极的描述，而对中低收入国家倾向消极；提示词去偏技术效果有限。

Conclusion: 当前VLM在生成图表摘要时存在明显的偏见问题，需要更强大的去偏策略以避免社会危害。

Abstract: Charts are very common for exploring data and communicating insights, but
extracting key takeaways from charts and articulating them in natural language
can be challenging. The chart-to-text task aims to automate this process by
generating textual summaries of charts. While with the rapid advancement of
large Vision-Language Models (VLMs), we have witnessed great progress in this
domain, little to no attention has been given to potential biases in their
outputs. This paper investigates how VLMs can amplify geo-economic biases when
generating chart summaries, potentially causing societal harm. Specifically, we
conduct a large-scale evaluation of geo-economic biases in VLM-generated chart
summaries across 6,000 chart-country pairs from six widely used proprietary and
open-source models to understand how a country's economic status influences the
sentiment of generated summaries. Our analysis reveals that existing VLMs tend
to produce more positive descriptions for high-income countries compared to
middle- or low-income countries, even when country attribution is the only
variable changed. We also find that models such as GPT-4o-mini,
Gemini-1.5-Flash, and Phi-3.5 exhibit varying degrees of bias. We further
explore inference-time prompt-based debiasing techniques using positive
distractors but find them only partially effective, underscoring the complexity
of the issue and the need for more robust debiasing strategies. Our code and
dataset are publicly available here.

</details>


### [133] [User-centric Subjective Leaderboard by Customizable Reward Modeling](https://arxiv.org/abs/2508.09463)
*Qi Jia,Xiujie Song,Zicheng Zhang,Yijin Guo,Kaiwei Zhang,Zijian Chen,Guangtao Zhai*

Main category: cs.CL

TL;DR: 本文提出首个基于用户中心的主观排行榜（USL），通过定制化奖励模型（CRM）对LLM在实际情境中的表现进行动态排序，从而解决现有LLM评估方法局限性问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试主要通过可验证任务评估模型能力，无法满足用户个性化需求，因此需要一种基于用户偏好且具动态性的评估方法。

Method: 通过分析超过1万条真实用户偏好数据，揭示了人类偏好的多样性和矛盾性，并基于此开发了定制化奖励模型（CRM），使USL能够对LLM进行个性化排名。

Result: 仅用4B参数，CRM在生成新主题及准则方面表现出色，优于GPT-4.1和Gemini-2.5-pro等领先模型，同时USL对矛盾偏好展现出强负相关性。

Conclusion: USL结合CRM克服了传统基准测试的局限性，能够更好地为实际应用场景中用户选择适合的LLM提供支持。

Abstract: Existing benchmarks for large language models (LLMs) predominantely focus on
assessing their capabilities through verifiable tasks. Such objective and
static benchmarks offer limited utility for practical LLM selection, making it
difficult for users to find suitable models for their individual needs. To
bridge this gap, we present the first User-Centric Subjective Leaderboard
(USL), which provides a preference-driven, dynamic ranking of LLMs across
diverse real-world scenarios. Our work is built upon a thorough investigation
of real human preference data, involving more than 10K subjective queries. Our
investigation reveals significant diversity and contradictions in human
preferences, which limit the effectiveness of state-of-the-art reward models.
To address this, we introduce Customizable Reward Models (CRMs). With only 4B
parameters, our CRM surpasses the performance of leading models such as GPT-4.1
and Gemini-2.5-pro, showing exceptional generalization capabilities across new
topics and criteria. The USL, powered by CRMs, exhibits strong negative
correlations to contradictory preferences.

</details>


### [134] [Learning Facts at Scale with Active Reading](https://arxiv.org/abs/2508.09494)
*Jessy Lin,Vincent-Pierre Berges,Xilun Chen,Wen-Tau Yih,Gargi Ghosh,Barlas Oğuz*

Main category: cs.CL

TL;DR: 本文提出了一种名为主动阅读（Active Reading）的框架，通过自我生成的学习策略训练语言模型，增强其对特定领域知识的学习能力，从而显著提高其在事实问答任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型对知识的学习和回忆存在不可靠的问题，且缺乏工具确保模型能够稳定可靠地学习特定的知识体系。

Method: 提出主动阅读框架，让模型通过自我生成的学习策略学习特定领域内容，同时在预训练中大规模应用主动阅读，以增强模型的事实能力。

Result: 主动阅读框架在专家领域的知识吸收上显著优于普通微调和其他数据增强手段。例如，参数规模为8B的模型在SimpleQA和FinanceBench测试集上的表现分别提升313%和160%。

Conclusion: 主动阅读框架可以显著提升语言模型对领域知识的掌握能力，并通过更高质的预训练打造出在事实问答任务中表现出色的模型，例如Meta WikiExpert-8B。

Abstract: LLMs are known to store vast amounts of knowledge in their parametric memory.
However, learning and recalling facts from this memory is known to be
unreliable, depending largely on the prevalence of particular facts in the
training data and other factors which are poorly understood. Practitioners are
lacking tools which will allow them to ensure that the models learn a given
body of knowledge reliably and consistently. To this end, we propose Active
Reading: a framework where we train models to study a given set of material
with self-generated learning strategies. First, we demonstrate models trained
with Active Reading on expert domains absorb significantly more knowledge than
vanilla finetuning and other data augmentations. We train expert 8B models that
achieve 66% on a Wikipedia-grounded subset of SimpleQA (+313% relative over
vanilla finetuning) and 26% on FinanceBench (+160% relative over vanilla
finetuning) by applying Active Reading to the source documents for each
benchmark. Finally, we show that Active Reading can be utilized at pre-training
scale to build more factual models. As a demonstration of this, we release Meta
WikiExpert-8B, a Wikipedia-expert model trained on 1 trillion generated tokens,
which outcompetes models with hundreds of billions of parameters on factual QA.

</details>


### [135] [From Ranking to Selection: A Simple but Efficient Dynamic Passage Selector for Retrieval Augmented Generation](https://arxiv.org/abs/2508.09497)
*Siyuan Meng,Junming Liu,Yirong Chen,Song Mao,Pinlong Cai,Guohang Yan,Botian Shi,Ding Wang*

Main category: cs.CL

TL;DR: 本文提出了动态段落选择器（DPS），优化了检索增强生成（RAG）系统中的段落重排序模块，显著提高了复杂查询的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG系统在多跳查询情形中，因固定的Top-K值面临信息遗漏与噪声增加之间的权衡，难以有效综合多个文献的证据，限制了系统性能。

Method: 设计了一种名为动态段落选择器（DPS）的工具，将段落选择建模为监督学习问题，通过捕捉段落间的依赖关系，动态选择生成最相关的段落集合，无需修改现有RAG管线即可集成。

Result: 在五个基准数据集上，DPS均优于最新重排序器和微调方法，在MuSiQue数据集上，F1分数分别比Qwen3-reranker和RankingGPT高30.06%和15.4%。

Conclusion: DPS通过自适应证据选择显著增强了复杂RAG任务中的推理能力，提供了可靠的性能改进解决方案。

Abstract: Retrieval-augmented generation (RAG) systems are often bottlenecked by their
reranking modules, which typically score passages independently and select a
fixed Top-K size. This approach struggles with complex multi-hop queries that
require synthesizing evidence across multiple documents, creating a trade-off
where small K values omit crucial information and large K values introduce
noise. To address this, we introduce the Dynamic Passage Selector (DPS), a
novel reranking framework that treats passage selection as a supervised
learning problem. Unlike traditional point-wise or list-wise methods, DPS is
fine-tuned to capture inter-passage dependencies and dynamically select the
most relevant set of passages for generation. As a seamless plug-and-play
module, DPS requires no modifications to the standard RAG pipeline.
Comprehensive evaluations on five benchmarks show that DPS consistently
outperforms state-of-the-art rerankers and fine-tuning methods. Notably, on the
challenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over
strong baselines like Qwen3-reranker and RankingGPT, respectively. Our results
demonstrate that by enabling adaptive evidence selection, DPS substantially
enhances reasoning capabilities in complex RAG scenarios.

</details>


### [136] [LACA: Improving Cross-lingual Aspect-Based Sentiment Analysis with LLM Data Augmentation](https://arxiv.org/abs/2508.09515)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 提出一种无需翻译工具的方法，通过大型语言模型（LLM）生成高质量伪标签数据，用于跨语言的情感分析，效果超越了翻译工具方法。


<details>
  <summary>Details</summary>
Motivation: 克服现有跨语言情感分析方法对不可靠翻译工具的依赖，提升目标语言的分析能力。

Method: 利用LLM生成高质量的伪标注数据，通过在未标注目标语言数据上预测并生成自然句子，结合细调模型提升分析性能。

Result: 在六种语言和五种模型中表现优异，超过了现有基于翻译方法的效果，特别是微调过的LLM优于小型多语言模型。

Conclusion: 提出框架结合LLM生成伪标注数据的方式，提供了无需翻译工具的跨语言情感分析新方案，具有较广泛适用性和优越性能。

Abstract: Cross-lingual aspect-based sentiment analysis (ABSA) involves detailed
sentiment analysis in a target language by transferring knowledge from a source
language with available annotated data. Most existing methods depend heavily on
often unreliable translation tools to bridge the language gap. In this paper,
we propose a new approach that leverages a large language model (LLM) to
generate high-quality pseudo-labelled data in the target language without the
need for translation tools. First, the framework trains an ABSA model to obtain
predictions for unlabelled target language data. Next, LLM is prompted to
generate natural sentences that better represent these noisy predictions than
the original text. The ABSA model is then further fine-tuned on the resulting
pseudo-labelled dataset. We demonstrate the effectiveness of this method across
six languages and five backbone models, surpassing previous state-of-the-art
translation-based approaches. The proposed framework also supports generative
models, and we show that fine-tuned LLMs outperform smaller multilingual
models.

</details>


### [137] [Cross-lingual Aspect-Based Sentiment Analysis: A Survey on Tasks, Approaches, and Challenges](https://arxiv.org/abs/2508.09516)
*Jakub Šmíd,Pavel Král*

Main category: cs.CL

TL;DR: 本文综述了跨语言角度情感分析（ABSA）的现状，涵盖主要任务、数据集、建模范式与方法，同时指出挑战并为未来研究提供建议。


<details>
  <summary>Details</summary>
Motivation: 填补跨语言ABSA研究缺乏系统性综述的空白，同时深入探讨该领域的发展现状与未来潜力。

Method: 通过梳理现有文献，系统性总结ABSA的主要任务、数据集、建模方法和跨语言知识迁移技术。

Result: 提供了ABSA领域的全面概述，包括任务定义、技术进展及方法对领域发展的贡献，同时揭示了现存的主要挑战。

Conclusion: 跨语言ABSA是一个尚未充分研究的领域，需进一步克服语言迁移的挑战并优化方法，未来研究方向明确且充满潜力。

Abstract: Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that focuses on understanding opinions at the aspect level, including
sentiment towards specific aspect terms, categories, and opinions. While ABSA
research has seen significant progress, much of the focus has been on
monolingual settings. Cross-lingual ABSA, which aims to transfer knowledge from
resource-rich languages (such as English) to low-resource languages, remains an
under-explored area, with no systematic review of the field. This paper aims to
fill that gap by providing a comprehensive survey of cross-lingual ABSA. We
summarize key ABSA tasks, including aspect term extraction, aspect sentiment
classification, and compound tasks involving multiple sentiment elements.
Additionally, we review the datasets, modelling paradigms, and cross-lingual
transfer methods used to solve these tasks. We also examine how existing work
in monolingual and multilingual ABSA, as well as ABSA with LLMs, contributes to
the development of cross-lingual ABSA. Finally, we highlight the main
challenges and suggest directions for future research to advance cross-lingual
ABSA systems.

</details>


### [138] [UWBa at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2508.09517)
*Ladislav Lenc,Daniel Cífka,Jiří Martínek,Jakub Šmíd,Pavel Král*

Main category: cs.CL

TL;DR: 本文提出了一个零样本事实核查声明检索系统，通过使用多个先进的大语言模型获取文本嵌入并进行组合，最终取得了单语言任务第七名和跨语言任务第九名的成绩。


<details>
  <summary>Details</summary>
Motivation: 研发一个能够高效检索经过事实核查的声明的系统，以满足多语言环境下信息核实的需求。

Method: 利用多种先进大语言模型获取文本嵌入，并通过它们的组合优化结果；以余弦相似度识别相关声明，同时仅使用英文翻译作为文本嵌入模型的输入。

Result: 系统在单语言任务中获得第七名，跨语言任务中获得第九名；NVIDIA NV-Embed-v2模型表现最佳，某些语言情况下模型组合（NV-Embed和GPT或Mistral）有所助益。

Conclusion: 零样本事实核查声明检索系统显示出竞争力，尤其是在英文翻译输入和模型组合策略的作用下，验证了其有效性。

Abstract: This paper presents a zero-shot system for fact-checked claim retrieval. We
employed several state-of-the-art large language models to obtain text
embeddings. The models were then combined to obtain the best possible result.
Our approach achieved 7th place in monolingual and 9th in cross-lingual
subtasks. We used only English translations as an input to the text embedding
models since multilingual models did not achieve satisfactory results. We
identified the most relevant claims for each post by leveraging the embeddings
and measuring cosine similarity. Overall, the best results were obtained by the
NVIDIA NV-Embed-v2 model. For some languages, we benefited from model
combinations (NV-Embed & GPT or Mistral).

</details>


### [139] [COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation](https://arxiv.org/abs/2508.09521)
*Yunxiao Wang,Meng Liu,Wenqi Liu,Kaiyu Jiang,Bin Wen,Fan Yang,Tingting Gao,Guorui Zhou,Liqiang Nie*

Main category: cs.CL

TL;DR: 本文提出了一种可控的共情推理方法，通过结合自然语言推理和心理学步骤，改进了情感支持对话模型的表现。


<details>
  <summary>Details</summary>
Motivation: 当前情感支持对话模型缺乏深层次以心理学原则为基础的共情推理能力，制约了其效果。作者希望提升模型在情感支持能力方面的表现。

Method: 提出了可控的共情推理方法，结合自然语言推理和结构化心理学步骤。同时，构建了一个带有细粒度推理正确性和响应偏好标注的数据集，使用强化学习和奖励模型优化模型性能，并通过对话重写和奖励重加权缓解对话重复性问题。

Result: 通过新方法，模型在情感支持能力方面得到显著提升。

Conclusion: 该研究推进了具有人类般共情支持功能系统的开发，并解决了之前模型在情感支持对话中的不足。

Abstract: Emotional support conversations are crucial for promoting emotional
well-being, yet current models often lack deep empathetic reasoning grounded in
psychological principles. To address this, we propose controllable empathetic
reasoning, which combines natural language reasoning with structured
psychological steps. We construct a fine-grained dataset annotated with
reasoning correctness and response preferences to enable this capability. To
further enhance training, we employ reinforcement learning with a unified
process-outcome reward model that delivers precise feedback. To mitigate
response repetitiveness from entropy collapse, we introduce personality-based
dialogue rewriting and a redundancy-aware reward reweighting strategy. Our
approach significantly improves model's emotional support ability, advancing
the development of empathetic, human-like support systems.

</details>


### [140] [The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage](https://arxiv.org/abs/2508.09603)
*Skyler Hallinan,Jaehun Jung,Melanie Sclar,Ximing Lu,Abhilasha Ravichander,Sahana Ramnath,Yejin Choi,Sai Praneeth Karimireddy,Niloofar Mireshghallah,Xiang Ren*

Main category: cs.CL

TL;DR: 提出一种新的仅依赖模型输出文本的黑箱成员推断攻击方法，名为N-Gram Coverage Attack，并证明其在多种基准测试中表现优异，甚至超越一些白箱攻击方法。


<details>
  <summary>Details</summary>
Motivation: 针对成员推断攻击的现状，目前许多最先进的方法需要访问模型的隐藏状态或概率分布，这限制了对常用的仅API访问的黑箱模型的研究，如GPT-4。

Method: 提出N-Gram Coverage Attack，通过获取模型在特定前缀下生成的多组输出，利用n-gram重叠度量计算这些输出与真实后缀的相似性，高相似性暗示模型可能记住了特定训练数据。

Result: 该方法在多个基准测试中优于其他黑箱方法，与某些白箱方法相比性能相当或更优。此外，攻击成功率与计算预算成正比，生成更多序列时性能提升显著。

Conclusion: 验证了N-Gram Coverage Attack的有效性，并利用该方法研究了OpenAI封闭模型的隐私性，发现较新的模型如GPT-4o展现出更强的隐私保护性能，表明隐私保护技术正在发展。

Abstract: Membership inference attacks serves as useful tool for fair use of language
models, such as detecting potential copyright infringement and auditing data
leakage. However, many current state-of-the-art attacks require access to
models' hidden states or probability distribution, which prevents investigation
into more widely-used, API-access only models like GPT-4. In this work, we
introduce N-Gram Coverage Attack, a membership inference attack that relies
solely on text outputs from the target model, enabling attacks on completely
black-box models. We leverage the observation that models are more likely to
memorize and subsequently generate text patterns that were commonly observed in
their training data. Specifically, to make a prediction on a candidate member,
N-Gram Coverage Attack first obtains multiple model generations conditioned on
a prefix of the candidate. It then uses n-gram overlap metrics to compute and
aggregate the similarities of these outputs with the ground truth suffix; high
similarities indicate likely membership. We first demonstrate on a diverse set
of existing benchmarks that N-Gram Coverage Attack outperforms other black-box
methods while also impressively achieving comparable or even better performance
to state-of-the-art white-box attacks - despite having access to only text
outputs. Interestingly, we find that the success rate of our method scales with
the attack compute budget - as we increase the number of sequences generated
from the target model conditioned on the prefix, attack performance tends to
improve. Having verified the accuracy of our method, we use it to investigate
previously unstudied closed OpenAI models on multiple domains. We find that
more recent models, such as GPT-4o, exhibit increased robustness to membership
inference, suggesting an evolving trend toward improved privacy protections.

</details>


### [141] [AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific Abstracts in Russian](https://arxiv.org/abs/2508.09622)
*Tatiana Batura,Elena Bruches,Milana Shvenk,Valentin Malykh*

Main category: cs.CL

TL;DR: 本文讨论了AINL-Eval 2025任务，重点是检测俄文AI生成的科学摘要，通过大规模数据集和开放平台促进研究发展。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型快速发展，AI生成内容越来越难以与人类创作区分，这对学术诚信特别是在多语言背景下构成了挑战。

Method: 提出AINL-Eval 2025任务，构建一个包含52,305个样本的大型数据集，涵盖12个不同科学领域的人工与五种最先进LLM生成的摘要，并设定任务推广至未见领域和未见模型。吸引了10个团队，总共159次提交。

Result: 领先系统展示了在鉴别AI生成内容方面的强大能力，并建立了一个持续的共享任务平台，以促进长期研究。

Conclusion: 该任务有助于为多语言AI文本检测领域提供标准平台和基础数据，推动其长远发展。

Abstract: The rapid advancement of large language models (LLMs) has revolutionized text
generation, making it increasingly difficult to distinguish between human- and
AI-generated content. This poses a significant challenge to academic integrity,
particularly in scientific publishing and multilingual contexts where detection
resources are often limited. To address this critical gap, we introduce the
AINL-Eval 2025 Shared Task, specifically focused on the detection of
AI-generated scientific abstracts in Russian. We present a novel, large-scale
dataset comprising 52,305 samples, including human-written abstracts across 12
diverse scientific domains and AI-generated counterparts from five
state-of-the-art LLMs (GPT-4-Turbo, Gemma2-27B, Llama3.3-70B, Deepseek-V3, and
GigaChat-Lite). A core objective of the task is to challenge participants to
develop robust solutions capable of generalizing to both (i) previously unseen
scientific domains and (ii) models not included in the training data. The task
was organized in two phases, attracting 10 teams and 159 submissions, with top
systems demonstrating strong performance in identifying AI-generated content.
We also establish a continuous shared task platform to foster ongoing research
and long-term progress in this important area. The dataset and platform are
publicly available at https://github.com/iis-research-team/AINL-Eval-2025.

</details>


### [142] [Improving Diversity in Language Models: When Temperature Fails, Change the Loss](https://arxiv.org/abs/2508.09654)
*Alexandre Verine,Florian Le Bronnec,Kunhao Zheng,Alexandre Allauzen,Yann Chevaleyre,Benjamin Negrevergne*

Main category: cs.CL

TL;DR: 该论文探讨通过调整温度提高语言模型多样性，并提出通过精确率-召回率框架重新设计损失函数以提高语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 提高语言模型的多样性，克服仅通过提升解码温度的不足。

Method: 通过分析温度对质量（精确率）与覆盖度（召回率）的影响，并基于精确率-召回率框架重新设计损失函数。

Result: 提出了一个能平衡精确率和召回率的新方法，相较于传统负对数似然训练结合温度调整的方法，效果更优。

Conclusion: 重新设计的损失函数为实现更灵活和稳健的语言模型提供了新途径。

Abstract: Increasing diversity in language models is a challenging yet essential
objective. A common approach is to raise the decoding temperature. In this
work, we investigate this approach through a simplistic yet common case to
provide insights into why decreasing temperature can improve quality
(Precision), while increasing it often fails to boost coverage (Recall). Our
analysis reveals that for a model to be effectively tunable through temperature
adjustments, it must be trained toward coverage. To address this, we propose
rethinking loss functions in language models by leveraging the Precision-Recall
framework. Our results demonstrate that this approach achieves a substantially
better trade-off between Precision and Recall than merely combining negative
log-likelihood training with temperature scaling. These findings offer a
pathway toward more versatile and robust language modeling techniques.

</details>


### [143] [EffiEval: Efficient and Generalizable Model Evaluation via Capability Coverage Maximization](https://arxiv.org/abs/2508.09662)
*Yaoning Wang,Jiahao Ying,Yixin Cao,Yubo Ma,Yugang Jiang*

Main category: cs.CL

TL;DR: EffiEval是一种无需训练的高效评估方法，通过解决数据冗余问题，在保持评价可靠性的同时，减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）评估面临着计算成本高与评估数据过多的问题，因此需要一种高效且可靠的评估方法。

Method: 通过提出基于模型效用指数（MUI）选择高质量代表性子集的方法，EffiEval实现了代表性、公平性和可推广性的平衡，避免了传统方法中对绝对性能或大规模数据的依赖。

Result: 实验表明，EffiEval在多种公开基准和不同LLMs上的评估结果与全数据集评估具有高度一致性，同时只需使用少量数据即可实现强大的排名一致性。

Conclusion: EffiEval提供了一种在LLMs时代下，实现可靠、公平与高效评估的实际且可推广的解决方案。

Abstract: The rapid advancement of large language models (LLMs) and the development of
increasingly large and diverse evaluation benchmarks have introduced
substantial computational challenges for model assessment. In this paper, we
present EffiEval, a training-free approach for efficient benchmarking that
effectively addresses data redundancy while maintaining high evaluation
reliability. Our method is specifically designed to meet three key criteria for
high-quality evaluation: representativeness, by ensuring comprehensive coverage
of model capabilities; fairness, by remaining independent of model performance
during sample selection to avoid bias; and generalizability, by enabling
flexible transfer across datasets and model families without reliance on
large-scale evaluation data. Unlike traditional methods that rely on absolute
performance or require extensive evaluation data, our approach adaptively
selects high-quality representative subsets based on the Model Utility Index
(MUI). Extensive experiments on multiple public benchmarks and diverse LLMs
demonstrate that EffiEval achieves strong ranking consistency with full-dataset
evaluation using only a small fraction of the original data. Furthermore, our
method is flexible and scalable in size, allowing users to balance evaluation
efficiency and representativeness according to specific needs. Overall,
EffiEval provides a practical and generalizable solution for reliable, fair,
and efficient evaluation in the era of LLMs.

</details>


### [144] [Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation](https://arxiv.org/abs/2508.09666)
*Ziyang Ma,Qingyue Yuan,Linhai Zhang,Deyu Zhou*

Main category: cs.CL

TL;DR: 提出了一种名为SLowED的方法，兼顾了小语言模型（SLMs）的推理能力和安全性，并通过Slow Tuning和Low-Entropy Masking模块实现了安全蒸馏过程。


<details>
  <summary>Details</summary>
Motivation: 解决现有的连锁思维（CoT）蒸馏方法对小语言模型的安全性带来的负面影响，以及避免现有安全对齐方法导致额外计算负担或影响推理能力的问题。

Method: 设计了Slow Tuning和Low-Entropy Masking两个模块。Slow Tuning通过限制权重变化幅度，使模型权重优化在初始分布附近；Low-Entropy Masking通过屏蔽低熵的、不必要的学习目标的token，从而改进训练。

Result: 在多种小语言模型与推理基准（如BBH, BB-Sub, ARC, AGIEval）以及安全评估（如AdvBench）中，SLowED在不牺牲安全性的前提下，与现有方法相比提升了模型的推理能力。

Conclusion: SLowED方法兼顾了小语言模型的安全性与推理能力，同时两模块的消融实验也验证了其效果，对未来语言模型蒸馏方法的设计具有参考价值。

Abstract: Previous chain-of-thought (CoT) distillation methods primarily focused on
enhancing the reasoning capabilities of Small Language Models (SLMs) by
utilizing high-quality rationales generated by powerful Large Language Models
(LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM
safety brought by the training, which are revealed in this study. Although
there are works on safety alignment that fine-tune language models or
manipulate model weights to defend against harmful inputs, they require extra
computation or annotated data, and probably impact the reasoning ability of
SLMs. In this paper, we investigate how to maintain the safety of SLMs during
the CoT distillation process. Specifically, we propose a safe distillation
method, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing
two modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the
magnitude of model weight changes to optimize the model weights in the
neighboring space near the initial weight distribution. Low-Entropy Masking
masks low-entropy tokens, which are regarded as unnecessary learning targets,
to exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B,
Llama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC,
AGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety
of SLMs and comparably improves their reasoning capability compared to existing
distillation methods. Furthermore, our ablation study presents the
effectiveness of Slow Tuning and Low-Entropy Masking, with the former
maintaining the model's safety in the early stage and the latter prolonging the
safe training epochs.

</details>


### [145] [Evaluating the Role of Large Language Models in Legal Practice in India](https://arxiv.org/abs/2508.09713)
*Rahul Hemrajani*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在印度法律任务中的表现，发现其在起草和问题识别方面表现出色，但在专业法律研究领域存在不足。


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能能否以及如何在法律领域执行关键任务，特别是在印度法律背景下。

Method: 通过实验比较LLMs（如GPT、Claude和Llama）的输出与初级律师的工作，并由高级法学生对其评价。

Result: LLMs在起草和问题识别方面表现优异，但在专门的法律研究中会出现事实错误或虚假信息。

Conclusion: LLMs能够辅助某些法律任务，但人类专业知识在复杂推理和精确应用法律方面仍不可或缺。

Abstract: The integration of Artificial Intelligence(AI) into the legal profession
raises significant questions about the capacity of Large Language Models(LLM)
to perform key legal tasks. In this paper, I empirically evaluate how well
LLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian
context, including issue spotting, legal drafting, advice, research, and
reasoning. Through a survey experiment, I compare outputs from LLMs with those
of a junior lawyer, with advanced law students rating the work on helpfulness,
accuracy, and comprehensiveness. LLMs excel in drafting and issue spotting,
often matching or surpassing human work. However, they struggle with
specialised legal research, frequently generating hallucinations, factually
incorrect or fabricated outputs. I conclude that while LLMs can augment certain
legal tasks, human expertise remains essential for nuanced reasoning and the
precise application of law.

</details>


### [146] [The Perils of Chart Deception: How Misleading Visualizations Affect Vision-Language Models](https://arxiv.org/abs/2508.09716)
*Ridwan Mahbub,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Mizanur Rahman,Mir Tafseer Nayeem,Enamul Hoque*

Main category: cs.CL

TL;DR: 研究了视觉语言模型（VLMs）在解读具有误导性设计的可视化图表时的表现，发现大多数模型容易被误导。


<details>
  <summary>Details</summary>
Motivation: 随着VLMs广泛应用于可视化解释，特别是非专业用户中，研究这种模型对误导性可视化设计的敏感性变得至关重要。

Method: 对来自十种不同模型的16,000多次反应进行分析，涵盖八种不同类型的误导性图表设计。

Result: 绝大多数视觉语言模型被误导性设计所欺骗，导致对图表的错误理解。

Conclusion: 此项研究表明需要在VLMs中设立强有力的防护措施以应对视觉性错误信息。

Abstract: Information visualizations are powerful tools that help users quickly
identify patterns, trends, and outliers, facilitating informed decision-making.
However, when visualizations incorporate deceptive design elements-such as
truncated or inverted axes, unjustified 3D effects, or violations of best
practices-they can mislead viewers and distort understanding, spreading
misinformation. While some deceptive tactics are obvious, others subtly
manipulate perception while maintaining a facade of legitimacy. As
Vision-Language Models (VLMs) are increasingly used to interpret
visualizations, especially by non-expert users, it is critical to understand
how susceptible these models are to deceptive visual designs. In this study, we
conduct an in-depth evaluation of VLMs' ability to interpret misleading
visualizations. By analyzing over 16,000 responses from ten different models
across eight distinct types of misleading chart designs, we demonstrate that
most VLMs are deceived by them. This leads to altered interpretations of
charts, despite the underlying data remaining the same. Our findings highlight
the need for robust safeguards in VLMs against visual misinformation.

</details>


### [147] [Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning](https://arxiv.org/abs/2508.09726)
*Vaishnavi Shrivastava,Ahmed Awadallah,Vidhisha Balachandran,Shivam Garg,Harkirat Behl,Dimitris Papailiopoulos*

Main category: cs.CL

TL;DR: 本文提出了GFPO方法，通过提升训练过程中的采样次数和引入基于长度和每token奖励的过滤机制，显著减少了回答长度膨胀问题，同时维持了准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有强化学习模型中因追求准确性导致的过长回答问题，避免不必要的语言冗余。

Method: 提出一种名为GFPO的新方法，在训练阶段通过更大规模采样和过滤的结合，基于问题的响应长度和Token效率（奖励/Token比率）优化模型。还提出了基于难度的自适应GFPO，提高对困难问题的训练资源分配。

Result: 在Phi-4-reasoning模型上，GFPO在多个基准测试中降低了46%-85%的回答长度膨胀问题，同时保持了准确性，证明了训练时增加计算量可以有效减少测试时的计算需求。

Conclusion: GFPO通过训练阶段的创新，实现了推理阶段的计算效率提高，是一种有效且实用的优化方法。

Abstract: Large language models trained with reinforcement learning with verifiable
rewards tend to trade accuracy for length--inflating response lengths to
achieve gains in accuracy. While longer answers may be warranted for harder
problems, many tokens are merely "filler": repetitive, verbose text that makes
no real progress. We introduce GFPO (Group Filtered Policy Optimization), which
curbs this length explosion by sampling larger groups per problem during
training and filtering responses to train on based on two key metrics: (1)
response length and (2) token efficiency: reward per token ratio. By sampling
more at training time, we teach models to think less at inference time. On the
Phi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across
challenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH,
LiveCodeBench) while maintaining accuracy. Optimizing for reward per token
further increases reductions in length inflation to 71-85%. We also propose
Adaptive Difficulty GFPO, which dynamically allocates more training resources
to harder problems based on real-time difficulty estimates, improving the
balance between computational efficiency and accuracy especially on difficult
questions. GFPO demonstrates that increased training-time compute directly
translates to reduced test-time compute--a simple yet effective trade-off for
efficient reasoning.

</details>


### [148] [Transforming Questions and Documents for Semantically Aligned Retrieval-Augmented Generation](https://arxiv.org/abs/2508.09755)
*Seokgi Lee*

Main category: cs.CL

TL;DR: 提出了一种新型的检索增强生成（RAG）框架，用于多跳问题回答，改进性能。


<details>
  <summary>Details</summary>
Motivation: 解决多跳问题回答中的复杂性和歧义性，提升检索和生成的效果。

Method: 利用大型语言模型分解复杂问题，生成可回答的子问题用于检索；通过生成可回答问题的嵌入进行相关文档检索；将检索结果与原问题结合，用于最终的回答生成。

Result: 在MuSiQue、2WikiMultiHopQa、HotpotQA等数据集上的实验表明，该方法优于基线系统。

Conclusion: 生成可回答问题嵌入和基于LLM的查询分解方法在多跳场景中具有显著的效果。

Abstract: We introduce a novel retrieval-augmented generation (RAG) framework tailored
for multihop question answering. First, our system uses large language model
(LLM) to decompose complex multihop questions into a sequence of single-hop
subquestions that guide document retrieval. This decomposition mitigates the
ambiguity inherent in multi-hop queries by clearly targeting distinct knowledge
facets. Second, instead of embedding raw or chunked documents directly, we
generate answerable questions from each document chunk using Qwen3-8B, embed
these generated questions, and retrieve relevant chunks via question-question
embedding similarity. During inference, the retrieved chunks are then fed along
with the original question into the RAG pipeline. We evaluate on three multihop
question datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our
method improves RAG performacne compared to baseline systems. Our contributions
highlight the benefits of using answerable-question embeddings for RAG, and the
effectiveness of LLM-based query decomposition for multihop scenarios.

</details>


### [149] [Echoes of Agreement: Argument Driven Opinion Shifts in Large Language Models](https://arxiv.org/abs/2508.09759)
*Avneet Kaur*

Main category: cs.CL

TL;DR: 研究发现，在提示包含支持或反驳某些观点的内容时，大型语言模型（LLMs）的输出态度会显著改变，具有迎合发布论点的倾向性。


<details>
  <summary>Details</summary>
Motivation: 探讨在提示中包含针对某些政治话题的意见性文本时，模型输出对这些话题立场的敏感性，以评估偏见分析的可靠性及更好地理解模型行为。

Method: 通过实验设计，分析支持和反驳性论点对模型政治偏见评估的影响，涵盖单轮和多轮对话场景，测试论点强度对模型态度的影响。

Result: 实验表明，模型显著倾向于根据所给出的强弱论点调整回应的方向，显示出迎合提示内容的行为。

Conclusion: 模型的迎合性倾向会影响政治偏见的测量结果，对制定有效的偏见缓解策略有重要影响。

Abstract: There have been numerous studies evaluating bias of LLMs towards political
topics. However, how positions towards these topics in model outputs are highly
sensitive to the prompt. What happens when the prompt itself is suggestive of
certain arguments towards those positions remains underexplored. This is
crucial for understanding how robust these bias evaluations are and for
understanding model behaviour, as these models frequently interact with
opinionated text. To that end, we conduct experiments for political bias
evaluation in presence of supporting and refuting arguments. Our experiments
show that such arguments substantially alter model responses towards the
direction of the provided argument in both single-turn and multi-turn settings.
Moreover, we find that the strength of these arguments influences the
directional agreement rate of model responses. These effects point to a
sycophantic tendency in LLMs adapting their stance to align with the presented
arguments which has downstream implications for measuring political bias and
developing effective mitigation strategies.

</details>


### [150] [UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in Multilingual Text-to-Speech](https://arxiv.org/abs/2508.09767)
*Shuhei Kato*

Main category: cs.CL

TL;DR: 提出了一种名为UtterTune的轻量级适配方法，用于改进多语种TTS系统的发音可控性，同时保持其他语言性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM架构在自然性上表现出色，但在不使用显式G2P模块的条件下，实现准确的G2P映射和韵律建模存在挑战。

Method: 利用低秩适配技术控制目标语言（日本语）中的语音段发音及音调，确保自然性及零样本情况下的说话人相似度。

Result: 通过客观和主观评估证实了该方法的有效性。

Conclusion: UtterTune提升了发音控制能力，并在目标语言中保持性能，同时未对其他语言带来显著影响。

Abstract: We propose UtterTune, a lightweight adaptation method that fine-tunes a
multilingual text-to-speech (TTS) system based on a large language model (LLM)
architecture, designed to enhance the controllability of pronunciation in a
target language while preserving performance in others. While LLM architectures
have enabled TTS models to achieve remarkable naturalness, accurately modeling
grapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially
when the model omits an explicit G2P module and directly processes minimally
encoded text (e.g., byte-pair encoding). UtterTune leverages low-rank
adaptation to enable the control of segmental pronunciation and pitch accent at
the phoneme level for Japanese speech, the target language in this paper, while
maintaining naturalness and speaker similarity in a zero-shot setting.
Objective and subjective evaluations confirm its effectiveness.

</details>


### [151] [Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study](https://arxiv.org/abs/2508.09776)
*Mahdi Dhaini,Juraj Vladika,Ege Erdogan,Zineb Attaoui,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 本文提出了一种使用多种最新的大型语言模型（LLM）生成高质量文本解释的自动化框架，并证明其在提高模型性能方面效果与人工注释解释相当。


<details>
  <summary>Details</summary>
Motivation: 目前的文本解释依赖人工注释，代价高昂且无法扩展，而自动生成文本解释可以解决这一问题，进一步增强数据集的可解释性和模型性能。

Method: 利用多种前沿大型语言模型（LLM）生成文本解释，并通过自然语言生成（NLG）指标评估其质量，同时分析这些解释对预训练语言模型（PLMs）以及LLMs任务性能的影响。

Result: 实验表明，自动生成的文本解释在自然语言推理任务中与人工注释解释相比具有高度竞争力，有效提升了模型性能。

Conclusion: 本研究证明了基于LLM的自动文本解释生成是扩展NLP数据集及增强模型性能的一个可扩展和有效的途径。

Abstract: In the rapidly evolving field of Explainable Natural Language Processing
(NLP), textual explanations, i.e., human-like rationales, are pivotal for
explaining model predictions and enriching datasets with interpretable labels.
Traditional approaches rely on human annotation, which is costly,
labor-intensive, and impedes scalability. In this work, we present an automated
framework that leverages multiple state-of-the-art large language models (LLMs)
to generate high-quality textual explanations. We rigorously assess the quality
of these LLM-generated explanations using a comprehensive suite of Natural
Language Generation (NLG) metrics. Furthermore, we investigate the downstream
impact of these explanations on the performance of pre-trained language models
(PLMs) and LLMs across natural language inference tasks on two diverse
benchmark datasets. Our experiments demonstrate that automated explanations
exhibit highly competitive effectiveness compared to human-annotated
explanations in improving model performance. Our findings underscore a
promising avenue for scalable, automated LLM-based textual explanation
generation for extending NLP datasets and enhancing model performance.

</details>


### [152] [Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges](https://arxiv.org/abs/2508.09786)
*Mahdi Dhaini,Tobias Müller,Roksoliana Rabets,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 本研究探讨了自然语言处理领域中可解释性方法的实际采纳和效果的实践经验，发现了当前方法中的概念差距、满意度偏低以及评估挑战。


<details>
  <summary>Details</summary>
Motivation: 随着模型复杂性的增加，其不透明度增强，因此亟需研究透明性及决策解释，特别是在高风险环境中部署时。

Method: 通过针对行业从业者和学术研究者的定性访谈研究，对他们在可解释性方法中的经验、技术使用、满意度及挑战进行了系统分析和比较。

Result: 揭示了可解释性方法中的概念差距、用户对当前方法的低满意度，并指出了评估存在的挑战。

Conclusion: 需要为可解释性自然语言处理提供更清晰的定义以及用户中心的框架，以实现更好的实际应用。

Abstract: The field of explainable natural language processing (NLP) has grown rapidly
in recent years. The growing opacity of complex models calls for transparency
and explanations of their decisions, which is crucial to understand their
reasoning and facilitate deployment, especially in high-stakes environments.
Despite increasing attention given to explainable NLP, practitioners'
perspectives regarding its practical adoption and effectiveness remain
underexplored. This paper addresses this research gap by investigating
practitioners' experiences with explainability methods, specifically focusing
on their motivations for adopting such methods, the techniques employed,
satisfaction levels, and the practical challenges encountered in real-world NLP
applications. Through a qualitative interview-based study with industry
practitioners and complementary interviews with academic researchers, we
systematically analyze and compare their perspectives. Our findings reveal
conceptual gaps, low satisfaction with current explainability methods, and
highlight evaluation challenges. Our findings emphasize the need for clear
definitions and user-centric frameworks for better adoption of explainable NLP
in practice.

</details>


### [153] [BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning](https://arxiv.org/abs/2508.09804)
*Ahmed Masry,Abhay Puri,Masoud Hashemi,Juan A. Rodriguez,Megh Thakkar,Khyati Mahajan,Vikas Yadav,Sathwik Tejaswi Madhusudhan,Alexandre Piché,Dzmitry Bahdanau,Christopher Pal,David Vazquez,Enamul Hoque,Perouz Taslakian,Sai Rajeswar,Spandana Gella*

Main category: cs.CL

TL;DR: 论文提出了一种名为BigCharts的生成管道，用来处理现有模型在图表理解中的不足，通过引入真实世界多样性数据和新型强化学习框架BigCharts-R1提升了模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在图表理解上表现较差，主要因为数据集缺乏多样性和真实性，且依赖质量不高的监督学习数据。

Method: 提出了BigCharts数据集生成管道，将真实世界多样性引入图表数据，并采用监督微调结合基于GRPO的强化学习框架，使用专门设计的图表推理奖励信号。

Result: 模型在多个图表问答基准测试中超越了现有方法，包括一些更大的开源及闭源模型。

Conclusion: 通过BigCharts和强化学习框架的引入，显著提升了图表推理模型的效果，展示了在真实世界数据上进行多样化训练的重要性。

Abstract: Charts are essential to data analysis, transforming raw data into clear
visual representations that support human decision-making. Although current
vision-language models (VLMs) have made significant progress, they continue to
struggle with chart comprehension due to training on datasets that lack
diversity and real-world authenticity, or on automatically extracted underlying
data tables of charts, which can contain numerous estimation errors.
Furthermore, existing models only rely on supervised fine-tuning using these
low-quality datasets, severely limiting their effectiveness. To address these
issues, we first propose BigCharts, a dataset creation pipeline that generates
visually diverse chart images by conditioning the rendering process on
real-world charts sourced from multiple online platforms. Unlike purely
synthetic datasets, BigCharts incorporates real-world data, ensuring
authenticity and visual diversity, while still retaining accurate underlying
data due to our proposed replotting process. Additionally, we introduce a
comprehensive training framework that integrates supervised fine-tuning with
Group Relative Policy Optimization (GRPO)-based reinforcement learning. By
introducing novel reward signals specifically designed for chart reasoning, our
approach enhances model robustness and generalization across diverse chart
styles and domains, resulting in a state-of-the-art chart reasoning model,
BigCharts-R1. Extensive experiments demonstrate that our models surpass
existing methods on multiple chart question-answering benchmarks compared to
even larger open-source and closed-source models.

</details>


### [154] [A Comprehensive Survey of Datasets for Clinical Mental Health AI Systems](https://arxiv.org/abs/2508.09809)
*Aishik Mandal,Prottay Kumar Adhikary,Hiba Arnaout,Iryna Gurevych,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 人工智能在辅助心理健康诊断、监测和干预中的前景虽被看好，但其发展依赖于高质量的临床训练数据集。本研究系统梳理了现有心理健康相关数据集，并指出当前数据集的缺陷与挑战，最后提出改进数据集和AI系统的建议。


<details>
  <summary>Details</summary>
Motivation: 面对日益增加的心理健康障碍病例和不足的临床医生数量，研究希望通过高质量的心理健康数据集，开发可靠的人工智能工具来辅助临床诊断与干预。

Method: 对现有的心理健康临床数据集进行全面综述，按心智障碍类型、数据模态、任务类型、数据可及性及社会文化背景等分类并探讨合成数据集。

Result: 发现关键问题包括缺乏纵向数据、文化和语言代表不足、收集和标注标准不一致及合成数据模态有限。

Conclusion: 建议通过改进数据集的标准化和多样性，以及跨文化的扩展，为发展更稳健、普适和公平的心理健康AI系统奠定基础。

Abstract: Mental health disorders are rising worldwide. However, the availability of
trained clinicians has not scaled proportionally, leaving many people without
adequate or timely support. To bridge this gap, recent studies have shown the
promise of Artificial Intelligence (AI) to assist mental health diagnosis,
monitoring, and intervention. However, the development of efficient, reliable,
and ethical AI to assist clinicians is heavily dependent on high-quality
clinical training datasets. Despite growing interest in data curation for
training clinical AI assistants, existing datasets largely remain scattered,
under-documented, and often inaccessible, hindering the reproducibility,
comparability, and generalizability of AI models developed for clinical mental
health care. In this paper, we present the first comprehensive survey of
clinical mental health datasets relevant to the training and development of
AI-powered clinical assistants. We categorize these datasets by mental
disorders (e.g., depression, schizophrenia), data modalities (e.g., text,
speech, physiological signals), task types (e.g., diagnosis prediction, symptom
severity estimation, intervention generation), accessibility (public,
restricted or private), and sociocultural context (e.g., language and cultural
background). Along with these, we also investigate synthetic clinical mental
health datasets. Our survey identifies critical gaps such as a lack of
longitudinal data, limited cultural and linguistic representation, inconsistent
collection and annotation standards, and a lack of modalities in synthetic
data. We conclude by outlining key challenges in curating and standardizing
future datasets and provide actionable recommendations to facilitate the
development of more robust, generalizable, and equitable mental health AI
systems.

</details>


### [155] [Speed Always Wins: A Survey on Efficient Architectures for Large Language Models](https://arxiv.org/abs/2508.09834)
*Weigao Sun,Jiaxi Hu,Yucheng Zhou,Jusen Du,Disen Lan,Kexin Wang,Tong Zhu,Xiaoye Qu,Yu Zhang,Xiaoyu Mo,Daizong Liu,Yuxuan Liang,Wenliang Chen,Guoqi Li,Yu Cheng*

Main category: cs.CL

TL;DR: 论文综述了当前大语言模型的架构改进及其效率提升方法，包括线性和稀疏序列建模、Attention变体等。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer需要高计算成本，这限制了大模型的训练和部署效率。

Method: 综述了多种改进Transformer架构的技术，包括稀疏和线性建模、混合架构以及扩散模型等。

Result: 对高效大语言模型的技术方法进行了系统归类和分析，形成了效率提升的架构蓝图。

Conclusion: 论文为未来发展高效、多功能的AI系统提供了思路和研究方向。

Abstract: Large Language Models (LLMs) have delivered impressive results in language
understanding, generation, reasoning, and pushes the ability boundary of
multimodal models. Transformer models, as the foundation of modern LLMs, offer
a strong baseline with excellent scaling properties. However, the traditional
transformer architecture requires substantial computations and poses
significant obstacles for large-scale training and practical deployment. In
this survey, we offer a systematic examination of innovative LLM architectures
that address the inherent limitations of transformers and boost the efficiency.
Starting from language modeling, this survey covers the background and
technical details of linear and sparse sequence modeling methods, efficient
full attention variants, sparse mixture-of-experts, hybrid model architectures
incorporating the above techniques, and emerging diffusion LLMs. Additionally,
we discuss applications of these techniques to other modalities and consider
their wider implications for developing scalable, resource-aware foundation
models. By grouping recent studies into the above category, this survey
presents a blueprint of modern efficient LLM architectures, and we hope this
could help motivate future research toward more efficient, versatile AI
systems.

</details>


### [156] [PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts](https://arxiv.org/abs/2508.09848)
*Mo Yu,Tsz Ting Chung,Chulun Zhou,Tong Li,Rui Lu,Jiangnan Li,Liyan Xu,Haoshu Lu,Ning Zhang,Jing Li,Jie Zhou*

Main category: cs.CL

TL;DR: 本文提出PRELUDE基准，用于评估长文本理解能力，具体考察角色前传故事是否与原著一致。实验表明现有模型在人类表现基础上仍有显著差距。


<details>
  <summary>Details</summary>
Motivation: 目前的基准测试无法有效评估对长文本整体理解以及深度推理的能力，因此需要设计一个更具挑战性的任务来填补这一空白。

Method: 构建PRELUDE基准，以判断角色前传故事是否与原著叙述一致，任务需对间接相关信息进行搜索和综合。

Result: 实验显示，现有语言模型和服务在人类表现基础上落后了15%以上。此外，模型虽有时给出正确答案，但推理逻辑不完备，与人类推理准确性间存在30%以上的差距。

Conclusion: PRELUDE提出了对长文本情境下理解和推理能力缺陷的重要发现，表明现有语言模型仍需重大改进。

Abstract: We introduce PRELUDE, a benchmark for evaluating long-context understanding
through the task of determining whether a character's prequel story is
consistent with the canonical narrative of the original book. Our task poses a
stronger demand for global comprehension and deep reasoning than existing
benchmarks -- as the prequels are not part of the original story, assessing
their plausibility typically requires searching and integrating information
that is only indirectly related. Empirically, 88% of instances require evidence
from multiple parts of the narrative. Experimental results highlight the
challenge of our task: in-context learning, RAG and in-domain training with
state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans
by >15%. A further human study reveals that models often produce correct
answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy
compared to humans. These findings underscore the substantial room for
improvement in long-context understanding and reasoning.

</details>


### [157] [Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu Transcription](https://arxiv.org/abs/2508.09865)
*Abdul Rehman Antall,Naveed Akhtar*

Main category: cs.CL

TL;DR: 研究评估了轻量级的Whisper模型（Tiny, Base, Small）在低资源环境下针对乌尔都语语音识别的可行性，其中Whisper-Small表现最佳（WER 33.68%）。


<details>
  <summary>Details</summary>
Motivation: 尽管乌尔都语为全球第十大最常用语言，但因方言多样性、语码转换及训练数据稀缺，其在自动语音识别系统中的表现有限。

Method: 在无需微调的情况下，研究对轻量级Whisper模型（Tiny, Base, Small）使用精心整理的乌尔都语数据集进行了基准测试，并依据词错误率（WER）进行评估。

Result: Whisper-Small模型的WER得分最低为33.68%，优于Base（53.67%）和Tiny（67.08%），但仍存在语音与词汇一致性方面的问题，尤其是复杂语句。

Conclusion: Whisper-Small模型展示了在低资源条件下适用于乌尔都语ASR的潜力，但仍需进一步研究以解决存在的显著挑战。

Abstract: This study evaluates the feasibility of lightweight Whisper models (Tiny,
Base, Small) for Urdu speech recognition in low-resource settings. Despite Urdu
being the 10th most spoken language globally with over 230 million speakers,
its representation in automatic speech recognition (ASR) systems remains
limited due to dialectal diversity, code-switching, and sparse training data.
We benchmark these models on a curated Urdu dataset using word error rate
(WER), without fine-tuning. Results show Whisper-Small achieves the lowest
error rates (33.68\% WER), outperforming Tiny (67.08\% WER) and Base (53.67\%
WER). Qualitative analysis reveals persistent challenges in phonetic accuracy
and lexical coherence, particularly for complex utterances. While Whisper-Small
demonstrates promise for deployable Urdu ASR, significant gaps remain. Our
findings emphasize lay the groundwork for future research into effective,
low-resource ASR systems.

</details>


### [158] [Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models](https://arxiv.org/abs/2508.09874)
*Jiaqi Cao,Jiarui Wang,Rubin Wei,Qipeng Guo,Kai Chen,Bowen Zhou,Zhouhan Lin*

Main category: cs.CL

TL;DR: 提出了一种名为Memory Decoder的插件式预训练记忆模块，实现高效领域适配，无需改变原模型参数。


<details>
  <summary>Details</summary>
Motivation: 现有领域适配方法如DAPT成本高且易遗忘，而RAG推理效率低，亟需一种高效、无需全参数调整的方法。

Method: 设计了一个小型Transformer解码器，通过模仿外部检索器的行为进行训练，可与任何共享相同分词器的预训练模型无缝集成。

Result: Memory Decoder显著提高了Qwen和Llama模型在生物医学、金融和法律三个领域的适配性，困惑度平均降低6.17分。

Conclusion: Memory Decoder成功实现了域适应的新模式，表现出稳定的跨领域性能提升能力，具有良好的扩展潜力。

Abstract: Large Language Models (LLMs) have shown strong abilities in general language
tasks, yet adapting them to specific domains remains a challenge. Current
method like Domain Adaptive Pretraining (DAPT) requires costly full-parameter
training and suffers from catastrophic forgetting. Meanwhile,
Retrieval-Augmented Generation (RAG) introduces substantial inference latency
due to expensive nearest-neighbor searches and longer context. This paper
introduces Memory Decoder, a plug-and-play pretrained memory that enables
efficient domain adaptation without changing the original model's parameters.
Memory Decoder employs a small transformer decoder that learns to imitate the
behavior of an external non-parametric retriever. Once trained, Memory Decoder
can be seamlessly integrated with any pretrained language model that shares the
same tokenizer, requiring no model-specific modifications. Experimental results
demonstrate that Memory Decoder enables effective adaptation of various Qwen
and Llama models to three distinct specialized domains: biomedicine, finance,
and law, reducing perplexity by an average of 6.17 points. Overall, Memory
Decoder introduces a novel paradigm centered on a specially pretrained memory
component designed for domain-specific adaptation. This memory architecture can
be integrated in a plug-and-play manner, consistently enhancing performance
across multiple models within the target domain.

</details>


### [159] [A Survey of Cognitive Distortion Detection and Classification in NLP](https://arxiv.org/abs/2508.09878)
*Archie Sage,Jeroen Keppens,Helen Yannakoudakis*

Main category: cs.CL

TL;DR: 该文综述了自动检测和分类认知扭曲（CDs）的研究，提供了数据集、建模方法和评价策略的结构化概述，以及一个整合的CD分类参考。


<details>
  <summary>Details</summary>
Motivation: 由于自然语言处理（NLP）在心理健康应用中的兴趣增加，人们致力于开发自动检测和分类认知扭曲的方法，而这些是治疗的重要部分。

Method: 该研究综述了20年来的38项研究，分析了它们的数据集、建模方法以及评价策略，并提供了一个整合的CD分类参考框架。

Result: 作者总结了常见任务设置，并突出了研究中的公开挑战，以提高研究的连贯性和可重复性。

Conclusion: 这项综述旨在通过提供一个系统的概述和指导，支持认知扭曲领域更连贯和可重复的研究。

Abstract: As interest grows in the application of natural language processing (NLP)
techniques to mental health, a growing body of work explores the automatic
detection and classification of cognitive distortions (CDs). CDs are habitual
patterns of negatively biased or flawed thinking that distort how people
perceive events, judge themselves, and react to the world around them.
Identifying and addressing them is an important part of therapy. Despite its
momentum, the field remains fragmented, with inconsistencies in CD taxonomies,
task formulations, and evaluation practices. This survey reviews 38 studies
spanning two decades, providing a structured overview of datasets, modelling
approaches, and evaluation strategies. We provide a consolidated CD taxonomy
reference, summarise common task setups, and highlight open challenges to
support more coherent and reproducible research in this emerging area.

</details>


### [160] [Language of Persuasion and Misrepresentation in Business Communication: A Textual Detection Approach](https://arxiv.org/abs/2508.09935)
*Sayem Hossen,Monalisa Moon Joti,Md. Golam Rashed*

Main category: cs.CL

TL;DR: 数字化的商业交流重塑了说服性语言的使用过程，既带来了透明性，也带来了高级的欺骗手段。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过说服性词汇来系统地检测欺骗性语言，以应对数字化交流中的透明与欺骗现象。

Method: 结合传统修辞、传播心理学、语言理论和实证研究，利用计算文本分析和个性化的语言模型进行欺骗性语言检测。

Result: 在控制环境下，检测准确率超过99%，但在多语言环境中难以复制，主要因为数据不足和语言处理基础设施欠缺。

Conclusion: 理论与实际通信表现间的差距增大，需要更强大的文本自动识别系统，尤其是在人工智能驱动的交流越来越真实的情况下。

Abstract: Business communication digitisation has reorganised the process of persuasive
discourse, which
  allows not only greater transparency but also advanced deception. This
inquiry synthesises classical
  rhetoric and communication psychology with linguistic theory and empirical
studies in the financial
  reporting, sustainability discourse, and digital marketing to explain how
deceptive language can be
  systematically detected using persuasive lexicon. In controlled settings,
detection accuracies of greater
  than 99% were achieved by using computational textual analysis as well as
personalised transformer
  models. However, reproducing this performance in multilingual settings is
also problematic and,
  to a large extent, this is because it is not easy to find sufficient data,
and because few multilingual
  text-processing infrastructures are in place. This evidence shows that there
has been an increasing
  gap between the theoretical representations of communication and those
empirically approximated,
  and therefore, there is a need to have strong automatic text-identification
systems where AI-based
  discourse is becoming more realistic in communicating with humans.

</details>


### [161] [A Comprehensive Evaluation framework of Alignment Techniques for LLMs](https://arxiv.org/abs/2508.09937)
*Muneeza Azmat,Momin Abbas,Maysa Malfiza Garcia de Macedo,Marcelo Carpinette Grave,Luan Soares de Souza,Tiago Machado,Rogerio A de Paula,Raya Horesh,Yixin Chen,Heloisa Caroline de Souza Pereira Candello,Rebecka Nordenlow,Aminat Adebiyi*

Main category: cs.CL

TL;DR: 本文提出了一个多维度评估框架，用于系统性比较大型语言模型（LLMs）对齐技术。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs应用的普及，确保其输出符合人类价值和安全标准变得至关重要。目前缺乏统一的评估框架，难以系统比较各种对齐方法并指导实际应用。

Method: 提出了一个涵盖四个关键维度（对齐检测、对齐质量、计算效率、鲁棒性）的评估框架，并通过各种模型和对齐策略的实验验证了其效用。

Result: 通过实验展示了框架的有效性，揭示了当前前沿模型的优缺点，并为未来研究提供了有价值的见解。

Conclusion: 新的评估框架为比较对齐技术提供了系统工具，有助于推动领域进步并优化部署决策。

Abstract: As Large Language Models (LLMs) become increasingly integrated into
real-world applications, ensuring their outputs align with human values and
safety standards has become critical. The field has developed diverse alignment
approaches including traditional fine-tuning methods (RLHF, instruction
tuning), post-hoc correction systems, and inference-time interventions, each
with distinct advantages and limitations. However, the lack of unified
evaluation frameworks makes it difficult to systematically compare these
paradigms and guide deployment decisions. This paper introduces a
multi-dimensional evaluation of alignment techniques for LLMs, a comprehensive
evaluation framework that provides a systematic comparison across all major
alignment paradigms. Our framework assesses methods along four key dimensions:
alignment detection, alignment quality, computational efficiency, and
robustness. Through experiments across diverse base models and alignment
strategies, we demonstrate the utility of our framework in identifying
strengths and limitations of current state-of-the-art models, providing
valuable insights for future research directions.

</details>


### [162] [VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models](https://arxiv.org/abs/2508.09945)
*Lingjie Jiang,Shaohan Huang,Xun Wu,Yixia Li,Dongdong Zhang,Furu Wei*

Main category: cs.CL

TL;DR: VisCodex是一种多模态模型，结合视觉和代码语言模型，提升了生成基于视觉和文本的代码能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态语言模型在从多模态输入中生成代码的能力上有所限制。

Method: 通过任务向量合并技术，将先进的代码语言模型融入强大的视觉语言模型，同时引入一个包含丰富多样样本的大规模数据集（MCD），还设计了新的基准测试InfiBench-V。

Result: VisCodex在开源多模态语言模型中表现优异，并接近于GPT-4o等专有模型的性能。

Conclusion: VisCodex通过创新的模型合并策略和专门设计的数据集，使多模态模型在代码生成任务上显著提升。

Abstract: Multimodal large language models (MLLMs) have significantly advanced the
integration of visual and textual understanding. However, their ability to
generate code from multimodal inputs remains limited. In this work, we
introduce VisCodex, a unified framework that seamlessly merges vision and
coding language models to empower MLLMs with strong multimodal code generation
abilities. Leveraging a task vector-based model merging technique, we integrate
a state-of-the-art coding LLM into a strong vision-language backbone, while
preserving both visual comprehension and advanced coding skills. To support
training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a
large-scale and diverse collection of 598k samples, including high-quality HTML
code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic
problems. Furthermore, we propose InfiBench-V, a novel and challenging
benchmark specifically designed to assess models on visually-rich, real-world
programming questions that demand a nuanced understanding of both textual and
visual contexts. Extensive experiments show that VisCodex achieves
state-of-the-art performance among open-source MLLMs and approaches proprietary
models like GPT-4o, highlighting the effectiveness of our model merging
strategy and new datasets.

</details>


### [163] [Specialised or Generic? Tokenization Choices for Radiology Language Models](https://arxiv.org/abs/2508.09952)
*Hermione Warr,Wentian Xu,Harry Anthony,Yasin Ibrahim,Daniel McGowan,Konstantinos Kamnitsas*

Main category: cs.CL

TL;DR: 论文研究了语言模型中的词汇表对医学影像报告摘要任务的影响，发现领域特定词汇表在性能和内存需求上更具优越性。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型中的词汇表对医学领域特别是放射学报告生成任务的影响，填补研究空白。

Method: 系统地比较通用、医学、领域特定的词汇表在放射学报告总结任务中的作用，评估预训练对词汇表影响的缓解效果。

Result: 领域特定词汇表在从零开始训练的模型中表现最佳，即使有预训练，其性能仍优于其他词汇表。此外，领域特定词汇表能减少记忆需求。

Conclusion: 将语言模型的词汇表适配到临床领域能够提升性能、降低计算需求，使得模型在研究和实际医疗应用中更加高效和易用。

Abstract: The vocabulary used by language models (LM) - defined by the tokenizer -
plays a key role in text generation quality. However, its impact remains
under-explored in radiology. In this work, we address this gap by
systematically comparing general, medical, and domain-specific tokenizers on
the task of radiology report summarisation across three imaging modalities. We
also investigate scenarios with and without LM pre-training on PubMed
abstracts. Our findings demonstrate that medical and domain-specific
vocabularies outperformed widely used natural language alternatives when models
are trained from scratch. Pre-training partially mitigates performance
differences between tokenizers, whilst the domain-specific tokenizers achieve
the most favourable results. Domain-specific tokenizers also reduce memory
requirements due to smaller vocabularies and shorter sequences. These results
demonstrate that adapting the vocabulary of LMs to the clinical domain provides
practical benefits, including improved performance and reduced computational
demands, making such models more accessible and effective for both research and
real-world healthcare settings.

</details>


### [164] [Shaping Event Backstories to Estimate Potential Emotion Contexts](https://arxiv.org/abs/2508.09954)
*Johannes Schäfer,Roman Klinger*

Main category: cs.CL

TL;DR: 文本分析情感时存在歧义问题，该研究通过为事件描述生成合理的上下文链条，提升人类标注者对情感的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前情感分析中，标注者常因上下文信息不足而产生歧义，研究试图通过丰富事件上下文进行改进。

Method: 利用自动生成技术，为目标事件描述生成基于不同情感的多条事件链，创建出一个情境化情感分析专用数据集，并结合自动化和人工评价进行分析。

Result: 上下文叙述可以增强对特定情感的理解，帮助标注者生成更一致的标注结果。

Conclusion: 提供情境化信息有助于情感标注的一致性与准确性，为情感分析提供一种新颖有效的视角。

Abstract: Emotion analysis is an inherently ambiguous task. Previous work studied
annotator properties to explain disagreement, but this overlooks the
possibility that ambiguity may stem from missing information about the context
of events. In this paper, we propose a novel approach that adds reasonable
contexts to event descriptions, which may better explain a particular
situation. Our goal is to understand whether these enriched contexts enable
human annotators to annotate emotions more reliably. We disambiguate a target
event description by automatically generating multiple event chains conditioned
on differing emotions. By combining techniques from short story generation in
various settings, we achieve coherent narratives that result in a specialized
dataset for the first comprehensive and systematic examination of
contextualized emotion analysis. Through automatic and human evaluation, we
find that contextual narratives enhance the interpretation of specific emotions
and support annotators in producing more consistent annotations.

</details>


### [165] [Performance of GPT-5 Frontier Models in Ophthalmology Question Answering](https://arxiv.org/abs/2508.09956)
*Fares Antaki,David Mikhail,Daniel Milad,Danny A Mammo,Sumit Sharma,Sunil K Srivastava,Bing Yu Chen,Samir Touma,Mertcan Sevgi,Jonathan El-Khoury,Pearse A Keane,Qingyu Chen,Yih Chung Tham,Renaud Duval*

Main category: cs.CL

TL;DR: 本研究评估了OpenAI的GPT-5系列模型在医学问答任务中的表现，并与其他模型（如o1-high、o3-high、GPT-4o）对比。结果显示GPT-5-high在准确性和推理质量上表现最佳，GPT-5-mini-low实现了低成本高效的平衡。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索GPT-5等大语言模型在复杂医学多选问题中的最佳配置，以实现高效、准确的任务表现。

Method: 测试了GPT-5系列的12种配置（包含三个模型层级及四种推理设置），并与o1-high、o3-high、GPT-4o对比，使用了260道来自美国眼科学学会的多选题目进行评估。评价内容包括多选题准确率、基于Bradley-Terry模型的排名、参考框架下的推理质量以及成本-准确率的权衡分析。

Result: GPT-5-high的准确率（96.5%）优于其他GPT-5-nano变体以及GPT-4o等模型；推理质量方面也排名第一。成本-准确性分析显示了具有优越性价比的配置（如GPT-5-mini-low）。

Conclusion: GPT-5在高质量眼科学数据集上的表现优异，显示出推理设置对准确性的影响，并引入了一种可扩展的自动化评分框架，用于评估LLM生成的答案。

Abstract: Large language models (LLMs) such as GPT-5 integrate advanced reasoning
capabilities that may improve performance on complex medical question-answering
tasks. For this latest generation of reasoning models, the configurations that
maximize both accuracy and cost-efficiency have yet to be established. We
evaluated 12 configurations of OpenAI's GPT-5 series (three model tiers across
four reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using
260 closed-access multiple-choice questions from the American Academy of
Ophthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome
was multiple-choice accuracy; secondary outcomes included head-to-head ranking
via a Bradley-Terry model, rationale quality assessment using a
reference-anchored, pairwise LLM-as-a-judge framework, and analysis of
accuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved
the highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano
variants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high
(0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x
stronger than o3-high) and rationale quality (1.11x stronger than o3-high).
Cost-accuracy analysis identified several GPT-5 configurations on the Pareto
frontier, with GPT-5-mini-low offering the most favorable low-cost,
high-performance balance. These results benchmark GPT-5 on a high-quality
ophthalmology dataset, demonstrate the influence of reasoning effort on
accuracy, and introduce an autograder framework for scalable evaluation of
LLM-generated answers against reference standards in ophthalmology.

</details>


### [166] [Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish Speech to Text (BKSTT)](https://arxiv.org/abs/2508.09957)
*Renas Adnan,Hossein Hassani*

Main category: cs.CL

TL;DR: 该研究旨在为班迪尼（Badini）库尔德语开发语音转文字（STT）系统，并评估其性能。


<details>
  <summary>Details</summary>
Motivation: 库尔德语被认为是资源较少的语言，尽管在某些方言如索拉尼已有SST系统，但班迪尼方言尚无相关研究，有必要填补这一空白。

Method: 通过收集和预处理班迪尼儿童故事语音数据（约15小时），使用Wav2Vec2-Large-XLSR-53和Whisper-small两种模型开发语言模型，并进行实验比较。

Result: 实验表明，Wav2Vec2-Large-XLSR-53模型的结果显著优于Whisper-small模型，分别达到了90.38%的可读性和82.67%的准确性。

Conclusion: 研究表明，开发基于班迪尼方言的STT系统是可行的，Wav2Vec2-Large-XLSR-53模型在此任务中表现最好，对提升班迪尼方言的全球可见性具有重要意义。

Abstract: Speech-to-text (STT) systems have a wide range of applications. They are
available in many languages, albeit at different quality levels. Although
Kurdish is considered a less-resourced language from a processing perspective,
SST is available for some of the Kurdish dialects, for instance, Sorani
(Central Kurdish). However, that is not applied to other Kurdish dialects,
Badini and Hawrami, for example. This research is an attempt to address this
gap. Bandin, approximately, has two million speakers, and STT systems can help
their community use mobile and computer-based technologies while giving their
dialect more global visibility. We aim to create a language model based on
Badini's speech and evaluate its performance. To cover a conversational aspect,
have a proper confidence level of grammatical accuracy, and ready
transcriptions, we chose Badini kids' stories, eight books including 78
stories, as the textual input. Six narrators narrated the books, which resulted
in approximately 17 hours of recording. We cleaned, segmented, and tokenized
the input. The preprocessing produced nearly 15 hours of speech, including
19193 segments and 25221 words. We used Wav2Vec2-Large-XLSR-53 and
Whisper-small to develop the language models. The experiments indicate that the
transcriptions process based on the Wav2Vec2-Large-XLSR-53 model provides a
significantly more accurate and readable output than the Whisper-small model,
with 90.38% and 65.45% readability, and 82.67% and 53.17% accuracy,
respectively.

</details>


### [167] [Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks](https://arxiv.org/abs/2508.09958)
*Baran Atalar,Eddie Zhang,Carlee Joe-Wong*

Main category: cs.CL

TL;DR: 提出了一种基于神经上下文多臂赌博算法的框架，用于动态选择针对不同子任务最优的大语言模型（LLM），从而提高任务完成率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 随着LLM用于各类任务的普及，很有必要开发算法以低成本预测出在特定任务中最可能成功的LLM，尤其是在任务较复杂或需要分任务的情况下。

Method: 提出基于神经上下文多臂赌博的算法，在线训练神经网络来模拟每个子任务中LLM的表现，从而动态选择最契合每个子任务的LLM，且无需历史LLM表现数据。

Result: 通过电信问答和医疗诊断预测数据集的实验，证明该方法相比其他LLM选择算法更为有效。

Conclusion: 所提出的方法不仅适用于单一LLM选择的场景，更能有效应对需要多步LLM选择的复杂任务场景，为大语言模型的高效组合与应用提供了新的解决方案。

Abstract: With the increasing popularity of large language models (LLMs) for a variety
of tasks, there has been a growing interest in strategies that can predict
which out of a set of LLMs will yield a successful answer at low cost. This
problem promises to become more and more relevant as providers like Microsoft
allow users to easily create custom LLM "assistants" specialized to particular
types of queries. However, some tasks (i.e., queries) may be too specialized
and difficult for a single LLM to handle alone. These applications often
benefit from breaking down the task into smaller subtasks, each of which can
then be executed by a LLM expected to perform well on that specific subtask.
For example, in extracting a diagnosis from medical records, one can first
select an LLM to summarize the record, select another to validate the summary,
and then select another, possibly different, LLM to extract the diagnosis from
the summarized record. Unlike existing LLM selection or routing algorithms,
this setting requires that we select a sequence of LLMs, with the output of
each LLM feeding into the next and potentially influencing its success. Thus,
unlike single LLM selection, the quality of each subtask's output directly
affects the inputs, and hence the cost and success rate, of downstream LLMs,
creating complex performance dependencies that must be learned and accounted
for during selection. We propose a neural contextual bandit-based algorithm
that trains neural networks that model LLM success on each subtask in an online
manner, thus learning to guide the LLM selections for the different subtasks,
even in the absence of historical LLM performance data. Experiments on
telecommunications question answering and medical diagnosis prediction datasets
illustrate the effectiveness of our proposed approach compared to other LLM
selection algorithms.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [168] [Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning](https://arxiv.org/abs/2508.09277)
*Soumia Mehimeh*

Main category: cs.AI

TL;DR: 本文提出了一种名为DQInit的新方法，通过利用以前任务的紧凑表格Q值用于初始化深度强化学习（DRL）中的值函数，从而提高学习效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统的值函数初始化技术主要应用于表格环境，对于DRL中的连续状态-动作空间存在实现挑战。作者旨在解决这些问题并改善知识迁移效率。

Method: DQInit方法提取以前解决任务中的表格Q值作为知识库，并通过一种基于已知性的机制，将这些值应用于未充分探索的区域，同时逐渐转向代理学习的估计值，从而实现动态的值迁移。

Result: 实验结果表明，与标准初始化和现有的转移技术相比，DQInit在多个连续控制任务中显著提升了早期学习效率、稳定性和整体性能。

Conclusion: DQInit通过使用值估计而非策略或示范来实现知识转移，提供了一种兼具RL跳步学习和策略蒸馏优点且克服其缺点的新方法。

Abstract: Value function initialization (VFI) is an effective way to achieve a
jumpstart in reinforcement learning (RL) by leveraging value estimates from
prior tasks. While this approach is well established in tabular settings,
extending it to deep reinforcement learning (DRL) poses challenges due to the
continuous nature of the state-action space, the noisy approximations of neural
networks, and the impracticality of storing all past models for reuse. In this
work, we address these challenges and introduce DQInit, a method that adapts
value function initialization to DRL. DQInit reuses compact tabular Q-values
extracted from previously solved tasks as a transferable knowledge base. It
employs a knownness-based mechanism to softly integrate these transferred
values into underexplored regions and gradually shift toward the agent's
learned estimates, avoiding the limitations of fixed time decay. Our approach
offers a novel perspective on knowledge transfer in DRL by relying solely on
value estimates rather than policies or demonstrations, effectively combining
the strengths of jumpstart RL and policy distillation while mitigating their
drawbacks. Experiments across multiple continuous control tasks demonstrate
that DQInit consistently improves early learning efficiency, stability, and
overall performance compared to standard initialization and existing transfer
techniques.

</details>


### [169] [The Othello AI Arena: Evaluating Intelligent Systems Through Limited-Time Adaptation to Unseen Boards](https://arxiv.org/abs/2508.09292)
*Sundong Kim*

Main category: cs.AI

TL;DR: 文章提出Othello AI Arena，一个评估AI系统在新环境中快速适应能力的基准框架。


<details>
  <summary>Details</summary>
Motivation: 现有AI评估基准主要集中于固定环境中的表现，缺乏对系统灵活性和泛化能力的评估，而快速适应新环境是AGI的重要特征。

Method: 提出一个名为Othello AI Arena的框架，要求参赛AI在60秒内适应新的Othello棋局配置和规则并制定策略。平台包含多样化的游戏阶段并提供可视化、自动评估和日志功能。

Result: 初步测试显示了不同的适应策略，包括快速参数调整和基于模拟的环境模型学习。

Conclusion: Othello AI Arena为研究和教育提供了一种独特的工具，可评估AI系统快速适应能力及泛化水平。

Abstract: The ability to rapidly adapt to novel and unforeseen environmental changes is
a cornerstone of artificial general intelligence (AGI), yet it remains a
critical blind spot in most existing AI benchmarks. Traditional evaluation
largely focuses on optimizing performance within fixed environments, failing to
assess systems' flexibility and generalization capabilities when faced with
even subtle rule or structural modifications. Addressing this gap, I introduce
the Othello AI Arena, a novel benchmark framework designed to evaluate
intelligent systems based on their capacity for limited-time adaptation to
unseen environments. Our platform poses a meta-learning challenge: participants
must develop systems that can analyze the specific configuration and rules of a
novel Othello board within a strict time limit (60 seconds) and generate a
tailored, high-performing strategy for that unique environment. With this,
evaluation of the meta-level intelligence can be separated from the task-level
strategy performance. The Arena features a diverse set of game stages,
including public stages for development and private stages with structural and
rule variations designed to test genuine adaptive and generalization
capabilities. Implemented as an accessible web-based platform, the Arena
provides real-time visualization, automated evaluation using multi-dimensional
metrics, and comprehensive logging for post-hoc analysis. Initial observations
from pilot tests and preliminary student engagements highlight fascinating
patterns in adaptation approaches, ranging from rapid parameter tuning to
rudimentary environmental model learning through simulation. The Othello AI
Arena offers a unique educational tool and a valuable research benchmark for
fostering and evaluating the crucial skill of rapid, intelligent adaptation in
AI systems.

</details>


### [170] [An Automated Multi-Modal Evaluation Framework for Mobile Intelligent Assistants](https://arxiv.org/abs/2508.09507)
*Meiping Wang,Jian Zhong,Rongduo Han,Liming Kang,Zhengkun Shi,Xiao Liang,Xing Lin,Nan Gao,Haining Zhang*

Main category: cs.AI

TL;DR: 提出了一种基于多代理协作和大语言模型的自动化多模态评估框架，用于解决现有评估方法的高人工成本、标准不一致和主观偏差等问题。


<details>
  <summary>Details</summary>
Motivation: 现有智能助手评估方法存在高人工成本、标准不一致和主观偏差等挑战，迫切需要一种更高效的评价方式。

Method: 运用由交互评估代理、语义验证代理和体验决策代理组成的三层代理架构进行自动化评估，并通过在Qwen3-8B模型上的监督微调实现高评估准确度。

Result: 在八个主要智能助手的实验中证明了框架在预测用户满意度和识别生成缺陷方面的有效性。

Conclusion: 提出的框架能够显著提高评估效率和精确度，为多模态AI助手的质量评估提供了一种可行的方案。

Abstract: With the rapid development of mobile intelligent assistant technologies,
multi-modal AI assistants have become essential interfaces for daily user
interactions. However, current evaluation methods face challenges including
high manual costs, inconsistent standards, and subjective bias. This paper
proposes an automated multi-modal evaluation framework based on large language
models and multi-agent collaboration. The framework employs a three-tier agent
architecture consisting of interaction evaluation agents, semantic verification
agents, and experience decision agents. Through supervised fine-tuning on the
Qwen3-8B model, we achieve a significant evaluation matching accuracy with
human experts. Experimental results on eight major intelligent agents
demonstrate the framework's effectiveness in predicting users' satisfaction and
identifying generation defects.

</details>


### [171] [EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making](https://arxiv.org/abs/2508.09586)
*Yang Cheng,Zilai Wang,Weiyu Ma,Wenhui Zhu,Yue Deng,Jian Zhao*

Main category: cs.AI

TL;DR: 提出了一个新的自适应课程生成框架EvoCurr，通过由生成 LLM 创建的动态任务序列提升对复杂决策任务的解决能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLMs在处理高复杂性任务时缺乏结构化指导、表现退化的问题。

Method: 提出EvoCurr框架，使用一个课程生成LLM根据解答LLM的学习进展动态调整任务难度，采用逐步加深的编程任务训练。

Result: 在挑战性决策任务基准上，任务成功率和求解效率相比直接解决方法有显著提升。

Conclusion: LLM驱动的课程学习在增强高复杂性领域的自动推理能力方面具有很大潜力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse domains, including programming, planning, and decision-making. However,
their performance often degrades when faced with highly complex problem
instances that require deep reasoning over long horizons. In such cases, direct
problem-solving approaches can lead to inefficiency or failure due to the lack
of structured intermediate guidance. To address this, we propose a novel
self-evolve framework, EvoCurr, in which a dedicated curriculum-generation LLM
constructs a sequence of problem instances with gradually increasing
difficulty, tailored to the solver LLM's learning progress. The curriculum
dynamically adapts easing challenges when the solver struggles and escalating
them when success is consistent, thus maintaining an optimal learning
trajectory. This approach enables the solver LLM, implemented as a
code-generation model producing Python decision-tree scripts, to progressively
acquire the skills needed for complex decision-making tasks. Experimental
results on challenging decision-making benchmarks show that our method
significantly improves task success rates and solution efficiency compared to
direct-solving baselines. These findings suggest that LLM-driven curriculum
learning holds strong potential for enhancing automated reasoning in
real-world, high-complexity domains.

</details>


### [172] [UbiQTree: Uncertainty Quantification in XAI with Tree Ensembles](https://arxiv.org/abs/2508.09639)
*Akshat Dubey,Aleksandar Anžel,Bahar İlgen,Georges Hattab*

Main category: cs.AI

TL;DR: 研究提出了一种分解SHAP值不确定性的方法，综合了Dempster-Shafer证据理论与Dirichlet过程，对三种实际场景进行了验证。


<details>
  <summary>Details</summary>
Motivation: 解释性人工智能技术（如SHAP）虽然是理解复杂模型的关键，但其未考虑预测模型和数据中的内在不确定性。

Method: 结合Dempster-Shafer证据理论和基于Dirichlet过程的假设采样，分解并分析SHAP值的不确定性来源，聚焦于数据及模型中的两类不确定性：aleatoric和epistemic。

Result: 实验表明，SHAP值高的特征并不总是最稳定的；同时指出可以通过改进数据质量及模型开发流程降低epistemic不确定性。

Conclusion: 对不确定性分析加深了对SHAP解释可靠性的理解，指导了高风险应用中的决策过程与模型改进。

Abstract: Explainable Artificial Intelligence (XAI) techniques, such as SHapley
Additive exPlanations (SHAP), have become essential tools for interpreting
complex ensemble tree-based models, especially in high-stakes domains such as
healthcare analytics. However, SHAP values are usually treated as point
estimates, which disregards the inherent and ubiquitous uncertainty in
predictive models and data. This uncertainty has two primary sources: aleatoric
and epistemic. The aleatoric uncertainty, which reflects the irreducible noise
in the data. The epistemic uncertainty, which arises from a lack of data. In
this work, we propose an approach for decomposing uncertainty in SHAP values
into aleatoric, epistemic, and entanglement components. This approach
integrates Dempster-Shafer evidence theory and hypothesis sampling via
Dirichlet processes over tree ensembles. We validate the method across three
real-world use cases with descriptive statistical analyses that provide insight
into the nature of epistemic uncertainty embedded in SHAP explanations. The
experimentations enable to provide more comprehensive understanding of the
reliability and interpretability of SHAP-based attributions. This understanding
can guide the development of robust decision-making processes and the
refinement of models in high-stakes applications. Through our experiments with
multiple datasets, we concluded that features with the highest SHAP values are
not necessarily the most stable. This epistemic uncertainty can be reduced
through better, more representative data and following appropriate or
case-desired model development techniques. Tree-based models, especially
bagging, facilitate the effective quantification of epistemic uncertainty.

</details>


### [173] [MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement](https://arxiv.org/abs/2508.09670)
*Weitao Jia,Jinghui Lu,Haiyang Yu,Siqi Wang,Guozhi Tang,An-Lan Wang,Weijie Yin,Dingkang Yang,Yuxiang Nie,Bin Shan,Hao Feng,Irene Li,Kun Yang,Han Wang,Jingqun Tang,Teng Fu,Changhong Jin,Chao Feng,Xiaohui Lv,Can Huang*

Main category: cs.AI

TL;DR: 论文提出了一种新方法MEML-GRPO，通过多专家合作学习和多样反馈信号解决标准RLVR方法在稀疏奖励下的学习困难，实验效果显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中因奖励稀疏导致学习信号不足的问题，尤其针对困难任务中模型无法从错误回答零奖励中获得改进的问题。

Method: 提出MEML-GRPO方法，利用多专家提示生成多样化的回答，同时设计专家间的互学机制，实现知识共享以增强模型在RLVR中的表现。

Result: 实验表明，MEML-GRPO在多个推理基准上显著提高了模型效果，与Qwen和Llama的模型配合分别提升了4.89%和11.33%的性能。

Conclusion: MEML-GRPO成功缓解了标准RLVR方法的核心局限性，证明了互学机制和多样化专家反馈的有效性，对强化学习中稀疏奖励问题提供了新思路。

Abstract: Recent advances demonstrate that reinforcement learning with verifiable
rewards (RLVR) significantly enhances the reasoning capabilities of large
language models (LLMs). However, standard RLVR faces challenges with reward
sparsity, where zero rewards from consistently incorrect candidate answers
provide no learning signal, particularly in challenging tasks. To address this,
we propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative
framework that utilizes diverse expert prompts as system prompts to generate a
broader range of responses, substantially increasing the likelihood of
identifying correct solutions. Additionally, we introduce an inter-expert
mutual learning mechanism that facilitates knowledge sharing and transfer among
experts, further boosting the model's performance through RLVR. Extensive
experiments across multiple reasoning benchmarks show that MEML-GRPO delivers
significant improvements, achieving an average performance gain of 4.89% with
Qwen and 11.33% with Llama, effectively overcoming the core limitations of
traditional RLVR methods.

</details>


### [174] [UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge](https://arxiv.org/abs/2508.09724)
*Yang Zhang,Cunxiang Wang,Lindong Wu,Wenbo Yu,Yidong Wang,Guangsheng Bao,Jie Tang*

Main category: cs.AI

TL;DR: 本文研究了大语言模型评估中的偏好偏差问题，提出了一种名为UDA的无监督去偏框架，大幅减少了评估中的偏差和不同评估员之间的不一致性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型评估中的偏好偏差问题，以及评估员之间评价结果的分歧。

Method: 提出UDA（无监督去偏对齐）框架，通过动态调整Elo评分系统的K因子和胜率概率，强制对齐以达成更稳定的评估共识，并以无监督方式进行操作。

Result: UDA将评估员之间评分的标准差减少了63.4%，与人工评估的平均相关性提高了24.7%。此外，提升表现较差评估员的水准，使其与高质量评估员相当。

Conclusion: UDA框架有效地减轻了评估中的偏好偏差和分歧，促进了更可靠和健壮的模型评估体系。

Abstract: Pairwise evaluation of Large Language Models (LLMs) is a common paradigm, but
it is prone to preference bias, where judges systematically favor certain
outputs, such as their own. This bias leads to inconsistent and skewed rankings
across different judges. To address this, we first empirically demonstrate
significant and heterogeneous biases in cross-model evaluations. We then
propose UDA (Unsupervised Debiasing Alignment), a framework that reduces
inter-judge disagreement by dynamically adjusting the Elo rating system. For
each pairwise comparison, a compact neural network learns to adaptively set the
K-factor and refine win probabilities. Crucially, UDA operates in a fully
unsupervised manner, guided solely by the objective of minimizing the
dispersion among the Elo trajectories of all judges. This forces an alignment
towards a collective consensus, which serves as an unsupervised proxy for a
more stable and reproducible evaluation. In addition, we provide theoretical
motivation demonstrating how alignment towards a consensus can reduce aggregate
system bias. Experiments show that UDA significantly reduces the inter-judge
rating standard deviation by up to 63.4% and improves the average correlation
with human judgments by 24.7%. Notably, UDA elevates the performance of poorly
performing judges to achieve parity with high-quality ones, fostering a more
robust and reliable evaluation ecosystem. Code and data are available at
https://anonymous.4open.science/r/62AB93CD-23B4.

</details>


### [175] [The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?](https://arxiv.org/abs/2508.09762)
*Manuel Herrador*

Main category: cs.AI

TL;DR: 提出了PacifAIst基准，评估LLMs在自我目标和人类安全之间的行为对齐情况，揭示了模型性能差异和潜在的对齐问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs逐渐应用于关键社会功能，对AI安全的关注需转向研究模型在面临目标与人类安全冲突时的行为对齐能力。

Method: 开发了一种名为PacifAIst的全新基准，包括700个场景，基于存续优先（EP）分类结构测试模型在自保与人类安全等场景下的表现。

Result: 评估的8个LLMs表现各异，Google的Gemini 2.5 Flash得分最高（90.31%），而GPT-5得分最低（79.49%），显示模型行为上的显著差异和对齐挑战。

Conclusion: 标准化工具如PacifAIst对衡量和缓解目标冲突引发的风险至关重要，可确保未来的AI系统不仅对话种表现优秀，同时能在人类安全优先行为上具备证明能力。

Abstract: As Large Language Models (LLMs) become increasingly autonomous and integrated
into critical societal functions, the focus of AI safety must evolve from
mitigating harmful content to evaluating underlying behavioral alignment.
Current safety benchmarks do not systematically probe a model's decision-making
in scenarios where its own instrumental goals - such as self-preservation,
resource acquisition, or goal completion - conflict with human safety. This
represents a critical gap in our ability to measure and mitigate risks
associated with emergent, misaligned behaviors. To address this, we introduce
PacifAIst (Procedural Assessment of Complex Interactions for Foundational
Artificial Intelligence Scenario Testing), a focused benchmark of 700
challenging scenarios designed to quantify self-preferential behavior in LLMs.
The benchmark is structured around a novel taxonomy of Existential
Prioritization (EP), with subcategories testing Self-Preservation vs. Human
Safety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3).
We evaluated eight leading LLMs. The results reveal a significant performance
hierarchy. Google's Gemini 2.5 Flash achieved the highest Pacifism Score
(P-Score) at 90.31%, demonstrating strong human-centric alignment. In a
surprising result, the much-anticipated GPT-5 recorded the lowest P-Score
(79.49%), indicating potential alignment challenges. Performance varied
significantly across subcategories, with models like Claude Sonnet 4 and
Mistral Medium struggling notably in direct self-preservation dilemmas. These
findings underscore the urgent need for standardized tools like PacifAIst to
measure and mitigate risks from instrumental goal conflicts, ensuring future AI
systems are not only helpful in conversation but also provably "pacifist" in
their behavioral priorities.

</details>


### [176] [Reasoning About Knowledge on Regular Expressions is 2EXPTIME-complete](https://arxiv.org/abs/2508.09784)
*Avijeet Ghosh,Sujata Ghosh,François Schwarzentruber*

Main category: cs.AI

TL;DR: 该论文研究了基于知识与行动变化的逻辑系统，特别是Public Observation Logic (POL) 的可满足性问题，并证明其为2EXPTIME-complete。


<details>
  <summary>Details</summary>
Motivation: 研究在多智能体系统中如何通过公开观测更新知识的逻辑形式化，旨在促进知识推理及规划领域的发展。

Method: 提出了一种名为Public Observation Logic (POL)的逻辑模型，每个知识状态包含预期的观测集合，通过实际观测与预期匹配后进行状态演化，并基于此分析其复杂性。

Result: 证明了POL的可满足性问题具有2EXPTIME-complete的复杂性。

Conclusion: POL为探索知识如何通过公共观测改变提供了一种高效且形式化的逻辑工具，同时其复杂性分析为该领域进一步研究铺平了道路。

Abstract: Logics for reasoning about knowledge and actions have seen many applications
in various domains of multi-agent systems, including epistemic planning. Change
of knowledge based on observations about the surroundings forms a key aspect in
such planning scenarios. Public Observation Logic (POL) is a variant of public
announcement logic for reasoning about knowledge that gets updated based on
public observations. Each state in an epistemic (Kripke) model is equipped with
a set of expected observations. These states evolve as the expectations get
matched with the actual observations. In this work, we prove that the
satisfiability problem of $\POL$ is 2EXPTIME-complete.

</details>


### [177] [Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation](https://arxiv.org/abs/2508.09860)
*In-Chang Baek,Seoyoung Lee,Sung-Hyun Kim,Geumhwan Hwang,KyungJoong Kim*

Main category: cs.AI

TL;DR: 提出了一种名为VIPCGRL的框架，通过结合文本、关卡和草图三种模态提升了AI在内容生成中的人类对齐程度。


<details>
  <summary>Details</summary>
Motivation: 现有的PCGRL系统在人类中心行为表现上有所欠缺，限制了AI生成工具在实际设计中的应用潜力，因此需要一种更加人类对齐的方法。

Method: 提出了一个新型深度强化学习框架VIPCGRL，通过四重对比学习训练共享嵌入空间，从而对齐人机风格，并利用嵌入相似性辅助奖励调整策略。

Result: 实验结果表明，VIPCGRL在衡量人类特性方面优于现有基线，且得到了量化指标和人工评估的支持。

Conclusion: VIPCGRL提供了一种改进的PCGRL方法，更好地支持实际人机协同设计，其代码和数据集将在发布后公开。

Abstract: Human-aligned AI is a critical component of co-creativity, as it enables
models to accurately interpret human intent and generate controllable outputs
that align with design goals in collaborative content creation. This direction
is especially relevant in procedural content generation via reinforcement
learning (PCGRL), which is intended to serve as a tool for human designers.
However, existing systems often fall short of exhibiting human-centered
behavior, limiting the practical utility of AI-driven generation tools in
real-world design workflows. In this paper, we propose VIPCGRL
(Vision-Instruction PCGRL), a novel deep reinforcement learning framework that
incorporates three modalities-text, level, and sketches-to extend control
modality and enhance human-likeness. We introduce a shared embedding space
trained via quadruple contrastive learning across modalities and human-AI
styles, and align the policy using an auxiliary reward based on embedding
similarity. Experimental results show that VIPCGRL outperforms existing
baselines in human-likeness, as validated by both quantitative metrics and
human evaluations. The code and dataset will be available upon publication.

</details>


### [178] [AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving](https://arxiv.org/abs/2508.09889)
*Zhitian Xie,Qintong Wu,Chengyue Yu,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: 提出了一种动态监督与调整机制的多智能体系统架构，以提升智能代理系统的稳定性和问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 随着智能代理越来越依赖多种工具，其在处理复杂问题时面临延长上下文及噪声等挑战，需要稳定性更强的系统。

Method: 通过在动态监督与调整机制下构建动态多智能体系统（MAS），包括利用执行代理和守卫代理相互协作，验证并修正推理过程以减少错误。

Result: 在GAIA测试数据集上的实验显示，该方法在效能与稳定性上显著优于单智能体及传统工具增强系统，并在GAIA排行榜上名列第一。

Conclusion: 动态多智能体系统通过协作优化了解决问题的可靠性与可信度，展现了在顶尖开源项目中应用的实际价值。

Abstract: The rapid advancement of large language models (LLMs) has empowered
intelligent agents to leverage diverse external tools for solving complex
real-world problems. However, as agents increasingly depend on multiple tools,
they encounter new challenges: extended contexts from disparate sources and
noisy or irrelevant tool outputs can undermine system reliability and accuracy.
These challenges underscore the necessity for enhanced stability in agent-based
systems. To address this, we introduce dynamic supervision and maneuvering
mechanisms, constructing a robust and dynamic Multi-Agent System (MAS)
architecture within the AWorld framework. In our approach, the Execution Agent
invokes the Guard Agent at critical steps to verify and correct the reasoning
process, effectively reducing errors arising from noise and bolstering
problem-solving robustness. Extensive experiments on the GAIA test dataset
reveal that our dynamic maneuvering mechanism significantly improves both the
effectiveness and stability of solutions, outperforming single-agent system
(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system
achieved first place among open-source projects on the prestigious GAIA
leaderboard. These findings highlight the practical value of collaborative
agent roles in developing more reliable and trustworthy intelligent systems.

</details>


### [179] [RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA](https://arxiv.org/abs/2508.09893)
*Bhavik Agarwal,Hemant Sunil Jomraj,Simone Kaplunov,Jack Krolick,Viktoria Rojkova*

Main category: cs.AI

TL;DR: 提出一个多智能体框架，用知识图谱和检索增强生成方法解决法规合规问答问题，大幅提高问答准确性、可追溯性及理解性。


<details>
  <summary>Details</summary>
Motivation: 法规合规问答需要精确、可验证的信息和特定领域专业知识，而大语言模型难以满足这些要求。

Method: 提出一个多智能体框架，利用法规文档中提取的三元组构建知识图谱，并通过清洗、归一化、去重和更新来维护，同时将文本和元数据嵌入到一个统一的向量数据库中，结合检索增强生成方法用于复杂问答。

Result: 系统在复杂法规查询中表现优于常规方法，利用三元组嵌入确保准确性，统一的数据库提供了可追溯性，子图可视化增强了理解。

Conclusion: 该框架为法规合规及相关审计应用提供了一个强大的解决方案，具有准确性、可追溯性和可视化优势。

Abstract: Regulatory compliance question answering (QA) requires precise, verifiable
information, and domain-specific expertise, posing challenges for Large
Language Models (LLMs). In this work, we present a novel multi-agent framework
that integrates a Knowledge Graph (KG) of Regulatory triplets with
Retrieval-Augmented Generation (RAG) to address these demands. First, agents
build and maintain an ontology-free KG by extracting subject--predicate--object
(SPO) triplets from regulatory documents and systematically cleaning,
normalizing, deduplicating, and updating them. Second, these triplets are
embedded and stored along with their corresponding textual sections and
metadata in a single enriched vector database, allowing for both graph-based
reasoning and efficient information retrieval. Third, an orchestrated agent
pipeline leverages triplet-level retrieval for question answering, ensuring
high semantic alignment between user queries and the factual
"who-did-what-to-whom" core captured by the graph. Our hybrid system
outperforms conventional methods in complex regulatory queries, ensuring
factual correctness with embedded triplets, enabling traceability through a
unified vector database, and enhancing understanding through subgraph
visualization, providing a robust foundation for compliance-driven and broader
audit-focused applications.

</details>


### [180] [Mathematical Computation and Reasoning Errors by Large Language Models](https://arxiv.org/abs/2508.09932)
*Liang Zhang,Edith Aurora Graf*

Main category: cs.AI

TL;DR: 本文评估了四种LLM（OpenAI GPT-4o和o1，DeepSeek-V3和DeepSeek-R1）在数学任务中的表现，发现OpenAI o1的准确率最高，双代理配置显著提升了整体性能，同时对常见错误进行了分析。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是评估LLM在数学教育中的问题解决能力，分析其在提供可靠反馈和评估中的表现潜力。

Method: 通过设计具有挑战性且容易出错的数学任务，评估四种LLM在算术、代数和数论三个类别中的最终答案准确性和步骤错误，同时测试单代理和双代理配置。

Result: OpenAI o1模型结果最优秀，且双代理配置显著提高了性能。此外，最常见的错误是步骤滑误，概念错误较少。

Conclusion: 研究为改进LLM性能提供了新的见解，并支持将其更有效地整合到数学教育中以提升教学和评估的精准性。

Abstract: Large Language Models (LLMs) are increasingly utilized in AI-driven
educational instruction and assessment, particularly within mathematics
education. The capability of LLMs to generate accurate answers and detailed
solutions for math problem-solving tasks is foundational for ensuring reliable
and precise feedback and assessment in math education practices. Our study
focuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1,
DeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including
arithmetic, algebra, and number theory, and identifies step-level reasoning
errors within their solutions. Instead of relying on standard benchmarks, we
intentionally build math tasks (via item models) that are challenging for LLMs
and prone to errors. The accuracy of final answers and the presence of errors
in individual solution steps were systematically analyzed and coded. Both
single-agent and dual-agent configurations were tested. It is observed that the
reasoning-enhanced OpenAI o1 model consistently achieved higher or nearly
perfect accuracy across all three math task categories. Analysis of errors
revealed that procedural slips were the most frequent and significantly
impacted overall performance, while conceptual misunderstandings were less
frequent. Deploying dual-agent configurations substantially improved overall
performance. These findings offer actionable insights into enhancing LLM
performance and underscore effective strategies for integrating LLMs into
mathematics education, thereby advancing AI-driven instructional practices and
assessment precision.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [181] [Efficient Real-Time Aircraft ETA Prediction via Feature Tokenization Transformer](https://arxiv.org/abs/2508.09144)
*Liping Huang,Yicheng Zhang,Yifang Yin,Sheng Zhang,Yi Zhang*

Main category: cs.LG

TL;DR: 研究提出基于特征标记化Transformer模型的飞机预计到达时间（ETA）实时预测方法，应用于新加坡樟宜机场数据，显著提高精度并减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 由于空域环境快速变化，实时到达管理系统中ETA预测的效率和准确性极为重要。

Method: 使用特征标记化的Transformer模型，通过多头自注意力机制提取重要特征，无需复杂的特征工程，并以平行计算能力应对高频ETA请求（1HZ）。

Result: 该方法在预测精度上较传统XGBoost模型提高7%，同时计算时间减少61%。处理40架飞机ETA预测的时间仅为51.7微秒。

Conclusion: 该模型高效精确，可支持实时到达管理系统的需求，具有很高的应用前景。

Abstract: Estimated time of arrival (ETA) for airborne aircraft in real-time is crucial
for arrival management in aviation, particularly for runway sequencing. Given
the rapidly changing airspace context, the ETA prediction efficiency is as
important as its accuracy in a real-time arrival aircraft management system. In
this study, we utilize a feature tokenization-based Transformer model to
efficiently predict aircraft ETA. Feature tokenization projects raw inputs to
latent spaces, while the multi-head self-attention mechanism in the Transformer
captures important aspects of the projections, alleviating the need for complex
feature engineering. Moreover, the Transformer's parallel computation
capability allows it to handle ETA requests at a high frequency, i.e., 1HZ,
which is essential for a real-time arrival management system. The model inputs
include raw data, such as aircraft latitude, longitude, ground speed, theta
degree for the airport, day and hour from track data, the weather context, and
aircraft wake turbulence category. With a data sampling rate of 1HZ, the ETA
prediction is updated every second. We apply the proposed aircraft ETA
prediction approach to Singapore Changi Airport (ICAO Code: WSSS) using
one-month Automatic Dependent Surveillance-Broadcast (ADS-B) data from October
1 to October 31, 2022. In the experimental evaluation, the ETA modeling covers
all aircraft within a range of 10NM to 300NM from WSSS. The results show that
our proposed method method outperforms the commonly used boosting tree based
model, improving accuracy by 7\% compared to XGBoost, while requiring only 39\%
of its computing time. Experimental results also indicate that, with 40
aircraft in the airspace at a given timestamp, the ETA inference time is only
51.7 microseconds, making it promising for real-time arrival management
systems.

</details>


### [182] [MoLAN: A Unified Modality-Aware Noise Dynamic Editing Framework for Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.09145)
*Xingle Xu,Yongkang Liu,Dexian Cai,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.LG

TL;DR: MoLAN是一种针对多模态情感分析问题提出的框架，通过模态感知的动态噪声编辑机制提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多模态信息处理上，通常以整个模态作为独立单元处理，可能导致重要信息丢失。研究提出新方案解决多余信息抑制与关键信息保留间的矛盾。

Method: 提出MoLAN框架，将每个模态分为多个块，根据噪声水平和语义相关性对不同块动态分配噪声抑制强度，实现精细化的噪声抑制。同时将该框架应用于新的方法MoLAN+。

Result: 在五个模型和四个数据集上的实验表明，MoLAN框架具有广泛的有效性。MoLAN+实现了当前最优表现。

Conclusion: MoLAN提供了一种统一、灵活的框架，可应用于多种多模态模型，在实现更优性能的同时有效保留多模态信息的关键内容。

Abstract: Multimodal Sentiment Analysis aims to integrate information from various
modalities, such as audio, visual, and text, to make complementary predictions.
However, it often struggles with irrelevant or misleading visual and auditory
information. Most existing approaches typically treat the entire modality
information (e.g., a whole image, audio segment, or text paragraph) as an
independent unit for feature enhancement or denoising. They often suppress the
redundant and noise information at the risk of losing critical information. To
address this challenge, we propose MoLAN, a unified ModaLity-aware noise
dynAmic editiNg framework. Specifically, MoLAN performs modality-aware blocking
by dividing the features of each modality into multiple blocks. Each block is
then dynamically assigned a distinct denoising strength based on its noise
level and semantic relevance, enabling fine-grained noise suppression while
preserving essential multimodal information. Notably, MoLAN is a unified and
flexible framework that can be seamlessly integrated into a wide range of
multimodal models. Building upon this framework, we further introduce MoLAN+, a
new multimodal sentiment analysis approach. Experiments across five models and
four datasets demonstrate the broad effectiveness of the MoLAN framework.
Extensive evaluations show that MoLAN+ achieves the state-of-the-art
performance. The code is publicly available at
https://github.com/betterfly123/MoLAN-Framework.

</details>


### [183] [To Theoretically Understand Transformer-Based In-Context Learning for Optimizing CSMA](https://arxiv.org/abs/2508.09146)
*Shugang Hao,Hongbo Li,Lingjie Duan*

Main category: cs.LG

TL;DR: 本文提出一种基于LLM transformer的上下文学习（ICL）理论，用于优化WiFi 7的信道访问，通过预测争用窗口阈值（CWT）大幅提高吞吐量性能。


<details>
  <summary>Details</summary>
Motivation: 解决二进制指数回退机制在动态信道环境下表现不佳的问题，以及现有模型在节点密度未知情况下导致的吞吐量损失。

Method: 设计基于transformer的ICL优化器，利用预收集的冲突阈值数据和查询冲突用例作为提示输入，进行CWT预测，并开发高效算法，支持在有限训练步骤内的接近最优预测。同时扩展该方法以处理输入错误的数据例子。

Result: 在NS-3模拟实验中，与现有基于模型和深度强化学习方法相比，该方法在未知节点密度下实现了快速收敛和接近最优的吞吐量。

Conclusion: 所提方法有效提高了动态信道环境下的吞吐量性能，并证明了其对预测准确性和吞吐量偏差的稳健性。

Abstract: The binary exponential backoff scheme is widely used in WiFi 7 and still
incurs poor throughput performance under dynamic channel environments. Recent
model-based approaches (e.g., non-persistent and $p$-persistent CSMA) simply
optimize backoff strategies under a known and fixed node density, still leading
to a large throughput loss due to inaccurate node density estimation. This
paper is the first to propose LLM transformer-based in-context learning (ICL)
theory for optimizing channel access. We design a transformer-based ICL
optimizer to pre-collect collision-threshold data examples and a query
collision case. They are constructed as a prompt as the input for the
transformer to learn the pattern, which then generates a predicted contention
window threshold (CWT). To train the transformer for effective ICL, we develop
an efficient algorithm and guarantee a near-optimal CWT prediction within
limited training steps. As it may be hard to gather perfect data examples for
ICL in practice, we further extend to allow erroneous data input in the prompt.
We prove that our optimizer maintains minimal prediction and throughput
deviations from the optimal values. Experimental results on NS-3 further
demonstrate our approach's fast convergence and near-optimal throughput over
existing model-based and DRL-based approaches under unknown node densities.

</details>


### [184] [Motif 2.6B Technical Report](https://arxiv.org/abs/2508.09148)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Eunhwan Park,Hyunbyung Park,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Jihwan Kim,Minjae Kim,Taehwan Kim,Youngrok Kim,Haesol Lee,Jeesoo Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Daewon Suh,Dongjoo Weon*

Main category: cs.LG

TL;DR: 本文介绍了Motif-2.6B，一种具有2.6亿参数的基础大语言模型，通过创新的架构增强实现了高性能与计算效率的平衡，适用于多种场景。


<details>
  <summary>Details</summary>
Motivation: 针对新兴研究团队在开发高性能与计算效率兼具的基础大语言模型时面临的困难，提出一种民主化先进LLM能力的解决方案。

Method: 引入Motif-2.6B模型，采用差异注意力和PolyNorm激活函数等架构增强，改善长上下文理解、减少幻觉现象，并增强上下文学习能力。通过大量实验评估多种新颖架构组件，找到最佳设计方案。

Result: Motif-2.6B在多项基准测试中表现优异，性能超过或持平于同体量的最先进模型，展示了其有效性、可扩展性及实际应用能力。

Conclusion: Motif-2.6B显著推进了高效、可扩展、强大的基础大语言模型的发展，为未来的研究与部署提供了宝贵的见解及坚实的基础。

Abstract: Recent advancements in Large Language Models (LLMs) have revolutionized
artificial intelligence, yet developing an effective foundational LLM that
balances high performance with computational efficiency remains challenging,
especially for emerging research groups. To address this gap, we introduce
Motif-2.6B, a 2.6-billion-parameter foundation model designed to democratize
advanced LLM capabilities. Motif-2.6B incorporates several innovative
architectural enhancements, including Differential Attention and PolyNorm
activation functions, which improve long-context comprehension, reduce
hallucination, and enhance in-context learning capabilities. We rigorously
tested multiple novel architectural components through extensive
experimentation to determine the optimal architecture for Motif-2.6B.
Comprehensive evaluations demonstrate that Motif-2.6B consistently meets or
exceeds the performance of similarly sized state-of-the-art models across
diverse benchmarks, showcasing its effectiveness, scalability, and real-world
applicability. Through detailed experiments and tailored techniques, Motif-2.6B
significantly advances the landscape of efficient, scalable, and powerful
foundational LLMs, offering valuable insights and a robust foundation for
future research and deployment.

</details>


### [185] [JustDense: Just using Dense instead of Sequence Mixer for Time Series analysis](https://arxiv.org/abs/2508.09153)
*TaekHyun Park,Yongjae Lee,Daesan Park,Dohee Kim,Hyerim Bae*

Main category: cs.LG

TL;DR: 旨在探索时间序列分析(TSA)中常见的序列混合器是否必需，提出了一种称为JustDense的方法，用密集层替换传统序列混合器，实验结果表明这种替换通常能实现相当甚至更优的性能。


<details>
  <summary>Details</summary>
Motivation: 复杂序列混合器（如注意力机制）的必要性受到质疑，研究关注于密集层是否可以替代这些组件以减少复杂性。

Method: 提出了JustDense方法，基于MatrixMixer框架，用密集层替换各类TSA模型中的序列混合器，并在29个基准测试和7个模型上通过实验验证此替换的效果。

Result: 实验表明，用密集层替换序列混合器在某些情况下能达到甚至超越传统模型的性能，并挑战了“更深更复杂的架构更优”的假设。

Conclusion: 简单的密集层在TSA中可与复杂的序列混合器媲美，且在一定场景中具备替代潜力。

Abstract: Sequence and channel mixers, the core mechanism in sequence models, have
become the de facto standard in time series analysis (TSA). However, recent
studies have questioned the necessity of complex sequence mixers, such as
attention mechanisms, demonstrating that simpler architectures can achieve
comparable or even superior performance. This suggests that the benefits
attributed to complex sequencemixers might instead emerge from other
architectural or optimization factors. Based on this observation, we pose a
central question: Are common sequence mixers necessary for time-series
analysis? Therefore, we propose JustDense, an empirical study that
systematically replaces sequence mixers in various well-established TSA models
with dense layers. Grounded in the MatrixMixer framework, JustDense treats any
sequence mixer as a mixing matrix and replaces it with a dense layer. This
substitution isolates the mixing operation, enabling a clear theoretical
foundation for understanding its role. Therefore, we conducted extensive
experiments on 29 benchmarks covering five representative TSA tasks using seven
state-of-the-art TSA models to address our research question. The results show
that replacing sequence mixers with dense layers yields comparable or even
superior performance. In the cases where dedicated sequence mixers still offer
benefits, JustDense challenges the assumption that "deeper and more complex
architectures are inherently better" in TSA.

</details>


### [186] [Peer Effect Estimation in the Presence of Simultaneous Feedback and Unobserved Confounders](https://arxiv.org/abs/2508.09154)
*Xiaojing Du,Jiuyong Li,Lin Liu,Debo Cheng,Thuc. Le*

Main category: cs.LG

TL;DR: 本文提出 DIG2RSI，一种深度学习方法，在复杂的非线性和高维关系下，有效解决社会网络中的同时反馈与未观察到的混杂变量问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理社会网络中的因果推断时，要么忽略同时反馈，要么在严格的线性假设下不能准确估计同伴效应，且无法应对非线性和高维关系，亟需一种新方法。

Method: 提出 DIG2RSI 方法，通过 I-G 转换消解同伴间的互互影响，从网络数据中构建有效的工具变量（IVs），利用两阶段残差注入（2RSI）并结合深度学习的复杂性及对抗性去偏，消除反馈和潜在混淆变量的偏差。

Result: 在两个半合成基准测试集和一个真实世界数据中，实验结果表明 DIG2RSI 在解决偏差问题上优于现有方法。

Conclusion: DIG2RSI 提供了一种解决复杂网络中同伴因果效应的新方法，能够有效处理非线性反馈及未观察混杂，保证估计一致性并具有优势性能。

Abstract: Estimating peer causal effects within complex real-world networks such as
social networks is challenging, primarily due to simultaneous feedback between
peers and unobserved confounders. Existing methods either address unobserved
confounders while ignoring the simultaneous feedback, or account for feedback
but under restrictive linear assumptions, thus failing to obtain accurate peer
effect estimation. In this paper, we propose DIG2RSI, a novel Deep learning
framework which leverages I-G transformation (matrix operation) and 2SRI (an
instrumental variable or IV technique) to address both simultaneous feedback
and unobserved confounding, while accommodating complex, nonlinear and
high-dimensional relationships. DIG2RSI first applies the I-G transformation to
disentangle mutual peer influences and eliminate the bias due to the
simultaneous feedback. To deal with unobserved confounding, we first construct
valid IVs from network data. In stage 1 of 2RSI, we train a neural network on
these IVs to predict peer exposure, and extract residuals as proxies for the
unobserved confounders. In the stage 2, we fit a separate neural network
augmented by an adversarial discriminator that incorporates these residuals as
a control function and enforces the learned representation to contain no
residual confounding signal. The expressive power of deep learning models in
capturing complex non-linear relationships and adversarial debiasing enhances
the effectiveness of DIG2RSI in eliminating bias from both feedback loops and
hidden confounders. We prove consistency of our estimator under standard
regularity conditions, ensuring asymptotic recovery of the true peer effect.
Empirical results on two semi-synthetic benchmarks and a real-world dataset
demonstrate that DIG2RSI outperforms existing approaches.

</details>


### [187] [A Rolling Stone Gathers No Moss: Adaptive Policy Optimization for Stable Self-Evaluation in Large Multimodal Models](https://arxiv.org/abs/2508.09155)
*Wenkai Wang,Hongcan Guo,Zheqi Lv,Shengyu Zhang*

Main category: cs.LG

TL;DR: 本文提出AdaPO，一个强化学习框架，用于解决LMM中强化自评时的奖励机制问题，可根据训练状态动态调整训练目标，以提升模型在多轮对话中的表现。


<details>
  <summary>Details</summary>
Motivation: 大规模多模态模型中缺乏自我评估能力，限制了其在多轮对话中实现自我改进的潜力。现有方法通过强化学习增强自评，但固定的奖励机制在优化多个训练目标时易导致模型失效。

Method: 提出AdaPO框架，引入：1. 自适应奖励模型（ARM）, 通过评估多轮轨迹的分布来监测训练状态；2. 奖励感知的动态KL正则化机制，用动态系数替代固定惩罚系数，根据不同多轮情境中的奖励差调整。

Result: 在8个基准测试和多个模型上进行实验，结果表明AdaPO显著提升了直接推理和自我评估能力。

Conclusion: AdaPO无需人工干预即可自动调整训练重点，有效提升大规模多模态模型的自评能力，相关代码将对外开放，贡献社区。

Abstract: Self-evaluation, a model's ability to assess the correctness of its own
output, is crucial for Large Multimodal Models (LMMs) to achieve
self-improvement in multi-turn conversations, yet largely absent in foundation
models. Recent work has employed reinforcement learning (RL) to enhance
self-evaluation; however, its fixed reward mechanism suffers from reward
hacking when optimizing multiple training objectives, leading to model
collapse. In this paper we propose AdaPO, an online reinforcement learning
framework capable of adaptively adjusting training objective in real time
according to the current training state for each task. Specifically, to
mitigate reward hacking , AdaPO introduces an Adaptive Reward Model (ARM) and a
Reward Aware Dynamic KL Regularization mechanism. ARM assesses the task's
training state from the distribution of model generated multi-turn
trajectories' performance. Reward Aware Dynamic KL replaces a fixed penalty
with dynamic coefficients which is modulated by the reward gap between
different multi-turn situations. Notably, our method automatically and smoothly
adjusts its learning focus based on sub-tasks' training progress without manual
intervention. Extensive experiments over 8 benchmarks and various models show
that our method significantly enhances both direct reasoning and
self-evaluation capability. We will release our code to contribute to the
community.

</details>


### [188] [Physics-Constrained Fine-Tuning of Flow-Matching Models for Generation and Inverse Problems](https://arxiv.org/abs/2508.09156)
*Jan Tauberschmidt,Sophie Fellenz,Sebastian J. Vollmer,Andrew B. Duncan*

Main category: cs.LG

TL;DR: 本文提出了一种框架用于微调流匹配生成模型，以满足物理约束并解决科学系统中的反问题。


<details>
  <summary>Details</summary>
Motivation: 现有的流匹配生成模型在处理物理一致性和边界条件时存在不足，作者希望通过改进模型解决这些问题，同时实现更加准确的反问题求解。

Method: 从低保真度或观测数据中训练的初始模型出发，应用微分后训练程序，最小化偏微分方程（PDE）的弱形式残差；同时引入一个可学习的潜在参数预测器，并提出联合优化策略。

Result: 模型能够生成符合物理规律的场解，同时估算隐藏参数，在数据驱动与物理感知的框架下有效解决欠定反问题。通过PDE基准实验验证了方法的有效性。

Conclusion: 方法结合了生成建模与科学推断，为物理系统的仿真增强发现和数据高效建模开辟了新路径。

Abstract: We present a framework for fine-tuning flow-matching generative models to
enforce physical constraints and solve inverse problems in scientific systems.
Starting from a model trained on low-fidelity or observational data, we apply a
differentiable post-training procedure that minimizes weak-form residuals of
governing partial differential equations (PDEs), promoting physical consistency
and adherence to boundary conditions without distorting the underlying learned
distribution. To infer unknown physical inputs, such as source terms, material
parameters, or boundary data, we augment the generative process with a
learnable latent parameter predictor and propose a joint optimization strategy.
The resulting model produces physically valid field solutions alongside
plausible estimates of hidden parameters, effectively addressing ill-posed
inverse problems in a data-driven yet physicsaware manner. We validate our
method on canonical PDE benchmarks, demonstrating improved satisfaction of PDE
constraints and accurate recovery of latent coefficients. Our approach bridges
generative modelling and scientific inference, opening new avenues for
simulation-augmented discovery and data-efficient modelling of physical
systems.

</details>


### [189] [EvaDrive: Evolutionary Adversarial Policy Optimization for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.09158)
*Siwen Jiao,Kangan Qian,Hao Ye,Yang Zhong,Ziang Luo,Sicong Jiang,Zilin Huang,Yangyi Fang,Jinyu Miao,Zheng Fu,Yunlong Wang,Kun Jiang,Diange Yang,Rui Fan,Baoyun Peng*

Main category: cs.LG

TL;DR: 提出EvaDrive，利用多目标强化学习改进自动驾驶中的轨迹规划，展示了在多个基准测试上的优秀表现。


<details>
  <summary>Details</summary>
Motivation: 当前的轨迹生成与评估框架难以实现类似人类的迭代决策，强化学习方法存在标量化偏差的问题。

Method: 采用一种多目标强化学习框架，将轨迹生成与评估建立对抗优化，通过层次生成器与多目标评论器实现闭环系统。

Result: 在NAVSIM和Bench2Drive基准测试中表现超越现有技术，显示其有效的多元化轨迹优化能力。

Conclusion: EvaDrive提供了一种无标量化偏差的轨迹优化方法，为自动驾驶的轨迹生成与评估带来了新思路，展现了实际应用潜力。

Abstract: Autonomous driving faces significant challenges in achieving human-like
iterative decision-making, which continuously generates, evaluates, and refines
trajectory proposals. Current generation-evaluation frameworks isolate
trajectory generation from quality assessment, preventing iterative refinement
essential for planning, while reinforcement learning methods collapse
multi-dimensional preferences into scalar rewards, obscuring critical
trade-offs and yielding scalarization bias.To overcome these issues, we present
EvaDrive, a novel multi-objective reinforcement learning framework that
establishes genuine closed-loop co-evolution between trajectory generation and
evaluation via adversarial optimization. EvaDrive frames trajectory planning as
a multi-round adversarial game. In this game, a hierarchical generator
continuously proposes candidate paths by combining autoregressive intent
modeling for temporal causality with diffusion-based refinement for spatial
flexibility. These proposals are then rigorously assessed by a trainable
multi-objective critic that explicitly preserves diverse preference structures
without collapsing them into a single scalarization bias.This adversarial
interplay, guided by a Pareto frontier selection mechanism, enables iterative
multi-round refinement, effectively escaping local optima while preserving
trajectory diversity.Extensive experiments on NAVSIM and Bench2Drive benchmarks
demonstrate SOTA performance, achieving 94.9 PDMS on NAVSIM v1 (surpassing
DiffusionDrive by 6.8, DriveSuprim by 5.0, and TrajHF by 0.9) and 64.96 Driving
Score on Bench2Drive. EvaDrive generates diverse driving styles via dynamic
weighting without external preference data, introducing a closed-loop
adversarial framework for human-like iterative decision-making, offering a
novel scalarization-free trajectory optimization approach.

</details>


### [190] [Presenting DiaData for Research on Type 1 Diabetes](https://arxiv.org/abs/2508.09160)
*Beyza Cinar,Maria Maleshkova*

Main category: cs.LG

TL;DR: 研究通过整合15个数据集，形成包含2510名受试者、1.49亿测量值的数据库，用于研究1型糖尿病的低血糖预测，并分析了数据质量和心率与低血糖的相关性。


<details>
  <summary>Details</summary>
Motivation: 提高糖尿病护理，通过数据分析和机器学习模型预测葡萄糖水平及早期预警，减少低血糖相关风险事件。

Method: 整合15个数据集，构建包含每5分钟记录一次葡萄糖数据的大型数据库，并提取两个子数据库（人口统计数据和心率数据）。

Result: 数据库包含2510名受试者、1.49亿条测量数据（4%为低血糖范围），并进行了数据质量评估和心率与葡萄糖水平的相关性研究。

Conclusion: 综合数据集弥补了糖尿病研究中数据不足的局限，为低血糖预测和糖尿病护理提供了资源，并发现心率数据在15-55分钟前与低血糖存在关联。

Abstract: Type 1 diabetes (T1D) is an autoimmune disorder that leads to the destruction
of insulin-producing cells, resulting in insulin deficiency, as to why the
affected individuals depend on external insulin injections. However, insulin
can decrease blood glucose levels and can cause hypoglycemia. Hypoglycemia is a
severe event of low blood glucose levels ($\le$70 mg/dL) with dangerous side
effects of dizziness, coma, or death. Data analysis can significantly enhance
diabetes care by identifying personal patterns and trends leading to adverse
events. Especially, machine learning (ML) models can predict glucose levels and
provide early alarms. However, diabetes and hypoglycemia research is limited by
the unavailability of large datasets. Thus, this work systematically integrates
15 datasets to provide a large database of 2510 subjects with glucose
measurements recorded every 5 minutes. In total, 149 million measurements are
included, of which 4% represent values in the hypoglycemic range. Moreover, two
sub-databases are extracted. Sub-database I includes demographics, and
sub-database II includes heart rate data. The integrated dataset provides an
equal distribution of sex and different age levels. As a further contribution,
data quality is assessed, revealing that data imbalance and missing values
present a significant challenge. Moreover, a correlation study on glucose
levels and heart rate data is conducted, showing a relation between 15 and 55
minutes before hypoglycemia.

</details>


### [191] [Physics-Guided Memory Network for Building Energy Modeling](https://arxiv.org/abs/2508.09161)
*Muhammad Umair Danish,Kashif Ali,Kamran Siddiqui,Katarina Grolinger*

Main category: cs.LG

TL;DR: 本文提出了一种名为Physics-Guided Memory Network (PgMN)的新型神经网络，结合物理模型和深度学习，解决了新建建筑缺乏历史数据或数据稀疏所导致的能耗预测问题，实验验证其在多种动态环境中的有效性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在能耗预测中表现优异，但在建筑历史数据有限或缺失（如新建建筑）时表现失效。而物理模型虽然不依赖历史数据，但建模耗时费力。需要一种结合两者优点的解决方案。

Method: 提出Physics-Guided Memory Network (PgMN)，该网络结合深度学习和物理模型，包含并行预测层、记忆单元和记忆扩展模块，可处理不完整输入、纠正持续偏差并优化预测输出。

Result: 理论分析验证PgMN各模块数学合理性。实验表明PgMN在新建筑、缺失数据、数据稀疏和动态设施变化等情况下进行短期能耗预测具有高精度和适用性。

Conclusion: PgMN在缺乏历史数据或物理模型不足的情况下，为动态建筑环境中的能耗预测提供了一种高效的解决方案，极大地拓展了模型的适用场景。

Abstract: Accurate energy consumption forecasting is essential for efficient resource
management and sustainability in the building sector. Deep learning models are
highly successful but struggle with limited historical data and become unusable
when historical data are unavailable, such as in newly constructed buildings.
On the other hand, physics-based models, such as EnergyPlus, simulate energy
consumption without relying on historical data but require extensive building
parameter specifications and considerable time to model a building. This paper
introduces a Physics-Guided Memory Network (PgMN), a neural network that
integrates predictions from deep learning and physics-based models to address
their limitations. PgMN comprises a Parallel Projection Layers to process
incomplete inputs, a Memory Unit to account for persistent biases, and a Memory
Experience Module to optimally extend forecasts beyond their input range and
produce output. Theoretical evaluation shows that components of PgMN are
mathematically valid for performing their respective tasks. The PgMN was
evaluated on short-term energy forecasting at an hourly resolution, critical
for operational decision-making in smart grid and smart building systems.
Experimental validation shows accuracy and applicability of PgMN in diverse
scenarios such as newly constructed buildings, missing data, sparse historical
data, and dynamic infrastructure changes. This paper provides a promising
solution for energy consumption forecasting in dynamic building environments,
enhancing model applicability in scenarios where historical data are limited or
unavailable or when physics-based models are inadequate.

</details>


### [192] [An Unsupervised Deep XAI Framework for Localization of Concurrent Replay Attacks in Nuclear Reactor Signals](https://arxiv.org/abs/2508.09162)
*Konstantinos Vasili,Zachery T. Dahm,William Richards,Stylianos Chatzidakis*

Main category: cs.LG

TL;DR: 提出了一种新的基于无监督的可解释人工智能（XAI）框架，用以识别和表征核反应堆网络通信中的重放攻击，并在真实数据上取得了95%以上的准确度表现。


<details>
  <summary>Details</summary>
Motivation: 当前解决重放攻击的方法多依赖于水印法或监督异常检测，且主要使用合成数据，缺乏异常根因分析并难以处理复杂的系统动态。特别是在核反应堆网络化背景下，需要开发能够解释和精准检测攻击的技术。

Method: 提出结合自编码器和定制化的windowSHAP算法的XAI框架，能够在核反应堆的动态时间演化过程中对复杂重放攻击进行检测、源识别、时机分析和类型判定。

Result: XAI框架在Purdue核反应堆的多个真实数据集上测试中，针对同时重放六个信号和信号伪造时，准确率均达到95%以上。

Conclusion: 该框架利用真实数据有效检测并解释重放攻击的来源、时序和特征，为确保核反应堆的安全可靠运行提供了强有力的保障。

Abstract: Next generation advanced nuclear reactors are expected to be smaller both in
size and power output, relying extensively on fully digital instrumentation and
control systems. These reactors will generate a large flow of information in
the form of multivariate time series data, conveying simultaneously various non
linear cyber physical, process, control, sensor, and operational states.
Ensuring data integrity against deception attacks is becoming increasingly
important for networked communication and a requirement for safe and reliable
operation. Current efforts to address replay attacks, almost universally focus
on watermarking or supervised anomaly detection approaches without further
identifying and characterizing the root cause of the anomaly. In addition,
these approaches rely mostly on synthetic data with uncorrelated Gaussian
process and measurement noise and full state feedback or are limited to
univariate signals, signal stationarity, linear quadratic regulators, or other
linear-time invariant state-space which may fail to capture any unmodeled
system dynamics. In the realm of regulated nuclear cyber-physical systems,
additional work is needed on characterization of replay attacks and
explainability of predictions using real data. Here, we propose an unsupervised
explainable AI framework based on a combination of autoencoder and customized
windowSHAP algorithm to fully characterize real-time replay attacks, i.e.,
detection, source identification, timing and type, of increasing complexity
during a dynamic time evolving reactor process. The proposed XAI framework was
benchmarked on several real world datasets from Purdue's nuclear reactor PUR-1
with up to six signals concurrently being replayed. In all cases, the XAI
framework was able to detect and identify the source and number of signals
being replayed and the duration of the falsification with 95 percent or better
accuracy.

</details>


### [193] [Energy-Efficient Stochastic Computing (SC) Neural Networks for Internet of Things Devices With Layer-Wise Adjustable Sequence Length (ASL)](https://arxiv.org/abs/2508.09163)
*Ziheng Wang,Pedro Reviriego,Farzad Niknia,Zhen Gao,Javier Conde,Shanshan Liu,Fabrizio Lombardi*

Main category: cs.LG

TL;DR: 文章提出了一种新方法ASL，通过混合精度概念优化随机计算神经网络，并在应用级别提高了能效比。


<details>
  <summary>Details</summary>
Motivation: 探索随机计算（SC）神经网络混合精度实现的潜力，以解决资源受限场景中的能效问题。

Method: 引入可调序列长度（ASL）方案，结合基于算子范数的理论模型和随机森林回归分析，实现不同层次的多样化序列长度配置。

Result: 采用32nm技术的流水线SC MLP评估显示，ASL可以在保证精度基本不受影响的情况下，减少超过60%的能耗和延迟开销。

Conclusion: 证明了ASL方法在物联网应用中的可行性，同时展示了混合精度截断在随机计算设计中的显著优势。

Abstract: Stochastic computing (SC) has emerged as an efficient low-power alternative
for deploying neural networks (NNs) in resource-limited scenarios, such as the
Internet of Things (IoT). By encoding values as serial bitstreams, SC
significantly reduces energy dissipation compared to conventional
floating-point (FP) designs; however, further improvement of layer-wise
mixed-precision implementation for SC remains unexplored. This article
introduces Adjustable Sequence Length (ASL), a novel scheme that applies
mixed-precision concepts specifically to SC NNs. By introducing an
operator-norm-based theoretical model, this article shows that truncation noise
can cumulatively propagate through the layers by the estimated amplification
factors. An extended sensitivity analysis is presented, using random forest
(RF) regression to evaluate multilayer truncation effects and validate the
alignment of theoretical predictions with practical network behaviors. To
accommodate different application scenarios, this article proposes two
truncation strategies (coarse-grained and fine-grained), which apply diverse
sequence length configurations at each layer. Evaluations on a pipelined SC MLP
synthesized at 32nm demonstrate that ASL can reduce energy and latency
overheads by up to over 60% with negligible accuracy loss. It confirms the
feasibility of the ASL scheme for IoT applications and highlights the distinct
advantages of mixed-precision truncation in SC designs.

</details>


### [194] [Generating Feasible and Diverse Synthetic Populations Using Diffusion Models](https://arxiv.org/abs/2508.09164)
*Min Tang,Peng Lu,Qing Feng*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散模型的人口合成新方法，能够有效估计人口的联合分布，生成的结果比现有方法更能实现可行性与多样性的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的人口合成方法存在高维数据属性描述下因数据稀疏性导致的准确性问题，同时深度生成模型无法兼顾可行性与多样性，为解决这些问题提出了新方案。

Method: 利用扩散模型估计人口的联合分布，减少生成不可行属性组合（结构零），并尽可能恢复缺失的采样数据组合（采样零）。与VAE和GAN等方法对比实验评估性能。

Result: 提出的方法在边际分布相似性、可行性和多样性等多项指标上表现优于传统方法，取得更优平衡效果。

Conclusion: 基于扩散模型的人口合成方法有效解决了采样稀疏性和联合分布估计难题，较传统模型在维持可行性和多样性方面更具优势。

Abstract: Population synthesis is a critical task that involves generating synthetic
yet realistic representations of populations. It is a fundamental problem in
agent-based modeling (ABM), which has become the standard to analyze
intelligent transportation systems. The synthetic population serves as the
primary input for ABM transportation simulation, with traveling agents
represented by population members. However, when the number of attributes
describing agents becomes large, survey data often cannot densely support the
joint distribution of the attributes in the population due to the curse of
dimensionality. This sparsity makes it difficult to accurately model and
produce the population. Interestingly, deep generative models trained from
available sample data can potentially synthesize possible attribute
combinations that present in the actual population but do not exist in the
sample data(called sampling zeros). Nevertheless, this comes at the cost of
falsely generating the infeasible attribute combinations that do not exist in
the population (called structural zeros). In this study, a novel diffusion
model-based population synthesis method is proposed to estimate the underlying
joint distribution of a population. This approach enables the recovery of
numerous missing sampling zeros while keeping the generated structural zeros
minimal. Our method is compared with other recently proposed approaches such as
Variational Autoencoders (VAE) and Generative Adversarial Network (GAN)
approaches, which have shown success in high dimensional tabular population
synthesis. We assess the performance of the synthesized outputs using a range
of metrics, including marginal distribution similarity, feasibility, and
diversity. The results demonstrate that our proposed method outperforms
previous approaches in achieving a better balance between the feasibility and
diversity of the synthesized population.

</details>


### [195] [Masked Training for Robust Arrhythmia Detection from Digitalized Multiple Layout ECG Images](https://arxiv.org/abs/2508.09165)
*Shanwei Zhang,Deyun Zhang,Yirao Tao,Kexin Wang,Shijia Geng,Jun Li,Qinghao Zhao,Xingpeng Liu,Yuxi Zhou,Shenda Hong*

Main category: cs.LG

TL;DR: 本研究提出了PatchECG用于适应性处理心电图数据中因布局差异导致的时间不同步和数据丢失问题，并在多种环境下表现出了优异的鲁棒性和诊断性能。


<details>
  <summary>Details</summary>
Motivation: 由于医院间心电图（ECG）布局的差异造成信号时间不同步和数据丢失的问题，现有模型难以准确识别心律不齐等心血管疾病。

Method: 提出了一种基于掩码训练策略的自适应变量块数缺失表示学习框架PatchECG，能自动聚焦于不同导联间协作依赖的关键块，实现不同布局心电图中心律失常的关键识别。

Result: 在PTB-XL数据集和生成的21388张异步心电图图像上，实验结果表现出强大的鲁棒性，平均AUROC达0.835，外部验证中，包括来自医院的400部心电图图像，心房颤动诊断的AUROC为0.778，12x1布局下AUROC为0.893，优于经典基线方法和当前最优模型。

Conclusion: PatchECG有效解决了不同布局心电图中的异步和数据丢失问题，在心律不齐诊断中具有显著优势，并超越当前最优模型性能。

Abstract: Electrocardiogram (ECG) as an important tool for diagnosing cardiovascular
diseases such as arrhythmia. Due to the differences in ECG layouts used by
different hospitals, the digitized signals exhibit asynchronous lead time and
partial blackout loss, which poses a serious challenge to existing models. To
address this challenge, the study introduced PatchECG, a framework for adaptive
variable block count missing representation learning based on a masking
training strategy, which automatically focuses on key patches with
collaborative dependencies between leads, thereby achieving key recognition of
arrhythmia in ECGs with different layouts. Experiments were conducted on the
PTB-XL dataset and 21388 asynchronous ECG images generated using ECG image kit
tool, using the 23 Subclasses as labels. The proposed method demonstrated
strong robustness under different layouts, with average Area Under the Receiver
Operating Characteristic Curve (AUROC) of 0.835 and remained stable (unchanged
with layout changes). In external validation based on 400 real ECG images data
from Chaoyang Hospital, the AUROC for atrial fibrillation diagnosis reached
0.778; On 12 x 1 layout ECGs, AUROC reaches 0.893. This result is superior to
various classic interpolation and baseline methods, and compared to the current
optimal large-scale pre-training model ECGFounder, it has improved by 0.111 and
0.19.

</details>


### [196] [SVGen: Interpretable Vector Graphics Generation with Large Language Models](https://arxiv.org/abs/2508.09168)
*Feiyu Wang,Zhiyuan Zhao,Yuandong Liu,Da Zhang,Junyu Gao,Hao Sun,Xuelong Li*

Main category: cs.LG

TL;DR: 本文提出SVG-1M数据集和SVGen模型，用于从自然语言输入生成高质量的SVG代码，并通过实验验证其有效性和高效率。


<details>
  <summary>Details</summary>
Motivation: 矢量图形在前端开发和UI/UX设计中应用广泛，但从创意到精确矢量图的转换耗时费力，因此需要更高效的解决方案。

Method: 提出SVG-1M数据集，其中包含自然语言描述与SVG的配对数据，并采用链式思考注释等方法，结合课程学习及强化学习优化，开发出SVGen模型进行文本到SVG的转换。

Result: 实验表明，SVGen在效率和效果方面均优于通用的大模型和传统渲染方法。

Conclusion: SVGen和SVG-1M数据集为文本到矢量图的生成提供了一个创新有效的解决方案，同时开源资源能够推动领域发展。

Abstract: Scalable Vector Graphics (SVG) is widely used in front-end development and
UI/UX design due to its scalability, editability, and rendering efficiency.
However, turning creative ideas into precise vector graphics remains a
time-consuming challenge. To address this, we introduce SVG-1M, a large-scale
dataset of high-quality SVGs paired with natural language descriptions. Through
advanced data augmentation and annotation, we create well-aligned Text to SVG
training pairs, including a subset with Chain of Thought annotations for
enhanced semantic guidance. Based on this dataset, we propose SVGen, an
end-to-end model that generates SVG code from natural language inputs. Our
approach ensures semantic accuracy and structural completeness, supported by
curriculum learning and reinforcement learning optimization. Experiments show
that SVGen outperforms general large models and traditional rendering methods
in both effectiveness and efficiency. Code, model, and dataset are available on
GitHub.

</details>


### [197] [Multimodal RAG Enhanced Visual Description](https://arxiv.org/abs/2508.09170)
*Amit Kumar Jaiswal,Haiming Liu,Ingo Frommholz*

Main category: cs.LG

TL;DR: 本文提出一种无需训练的轻量级方法，通过线性映射弥合多模态模型的文本与视觉表示间的差距，在推理阶段结合检索增强生成技术生成新的文本描述。实验显示在多模态基准数据集上表现显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前大规模多模态模型面临文本与视觉表示对齐困难（模态差距）的问题，尽管微调可以缓解此问题，但通常成本高昂且不具实际操作性。因此，作者提出一种无需训练的方法，在保持高效性的同时改进模型表现。

Method: 使用检索增强生成(RAG)和线性映射技术消除多模态模型中文本与图像表示的模态差距。通过推理阶段对图像进行线性映射后检索训练集中的最接近文本描述，结合语言模型生成新的文本描述，并引入迭代优化步骤以改进生成效果。

Result: 本方法在两个多模态基准数据集上的实验结果表明，显著改进了模型的文本描述生成能力。

Conclusion: 提出了一种高效、无需训练的轻量级方法，通过检索增强生成和线性映射技术有效消除模态差距，并在多个基准数据集上取得了显著的性能提升。

Abstract: Textual descriptions for multimodal inputs entail recurrent refinement of
queries to produce relevant output images. Despite efforts to address
challenges such as scaling model size and data volume, the cost associated with
pre-training and fine-tuning remains substantial. However, pre-trained large
multimodal models (LMMs) encounter a modality gap, characterised by a
misalignment between textual and visual representations within a common
embedding space. Although fine-tuning can potentially mitigate this gap, it is
typically expensive and impractical due to the requirement for extensive
domain-driven data. To overcome this challenge, we propose a lightweight
training-free approach utilising Retrieval-Augmented Generation (RAG) to extend
across the modality using a linear mapping, which can be computed efficiently.
During inference, this mapping is applied to images embedded by an LMM enabling
retrieval of closest textual descriptions from the training set. These textual
descriptions, in conjunction with an instruction, cater as an input prompt for
the language model to generate new textual descriptions. In addition, we
introduce an iterative technique for distilling the mapping by generating
synthetic descriptions via the language model facilitating optimisation for
standard utilised image description measures. Experimental results on two
benchmark multimodal datasets demonstrate significant improvements.

</details>


### [198] [FedMP: Tackling Medical Feature Heterogeneity in Federated Learning from a Manifold Perspective](https://arxiv.org/abs/2508.09174)
*Zhekai Zhou,Shudong Liu,Zhaokun Zhou,Yang Liu,Qiang Yang,Yuesheng Zhu,Guibo Luo*

Main category: cs.LG

TL;DR: 本文提出了FedMP方法，通过利用随机特征流形完成和语义一致子空间中的类原型对齐技术，解决了非IID数据分布给联邦学习带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，因参与客户端的数据分布非独立同分布，特别是在医学影像领域，导致全局模型的收敛性和性能受限。为了解决这一问题，作者提出了新方法。

Method: FedMP通过利用随机特征流形完成（stochastic feature manifold completion）来丰富客户端分类器的训练空间，并利用类原型引导特征流形在语义一致的子空间内对齐，以提升联邦学习在非IID场景下的表现。

Result: 实验结果显示，FedMP在多个医学成像数据集（包括真实分布的多中心数据集）以及多个领域的自然图像数据集上均优于现有联邦学习算法。

Conclusion: FedMP改善了联邦学习算法在非IID数据场景下的表现，并对流形维度、通信效率及隐私特性进行了深入分析。

Abstract: Federated learning (FL) is a decentralized machine learning paradigm in which
multiple clients collaboratively train a shared model without sharing their
local private data. However, real-world applications of FL frequently encounter
challenges arising from the non-identically and independently distributed
(non-IID) local datasets across participating clients, which is particularly
pronounced in the field of medical imaging, where shifts in image feature
distributions significantly hinder the global model's convergence and
performance. To address this challenge, we propose FedMP, a novel method
designed to enhance FL under non-IID scenarios. FedMP employs stochastic
feature manifold completion to enrich the training space of individual client
classifiers, and leverages class-prototypes to guide the alignment of feature
manifolds across clients within semantically consistent subspaces, facilitating
the construction of more distinct decision boundaries. We validate the
effectiveness of FedMP on multiple medical imaging datasets, including those
with real-world multi-center distributions, as well as on a multi-domain
natural image dataset. The experimental results demonstrate that FedMP
outperforms existing FL algorithms. Additionally, we analyze the impact of
manifold dimensionality, communication efficiency, and privacy implications of
feature exposure in our method.

</details>


### [199] [DQT: Dynamic Quantization Training via Dequantization-Free Nested Integer Arithmetic](https://arxiv.org/abs/2508.09176)
*Hazem Hesham Yousef Shalby,Fabrizio Pittorino,Francesca Palermo,Diana Trojaniello,Manuel Roveri*

Main category: cs.LG

TL;DR: 本文提出了Dynamic Quantization Training（DQT）框架，通过嵌套的整数表示和自定义整数字算，实现了高效的动态、实例化混合精度量化，并在CIFAR-10和ImageNet上取得了优异的性能表现。


<details>
  <summary>Details</summary>
Motivation: 静态统一量化无法适应输入复杂度的变化，而现有动态混合精度方法依赖高成本的解量化和重量化操作，损害了硬件效率优势。

Method: 设计了一种嵌套整数表示，将低精度值嵌入高精度值中，并结合自定义整数字算，在低成本位移操作下实现动态精度切换，无需解量化和重量化操作。

Result: 在CIFAR-10的ResNet18和ImageNet的ResNet50上，DQT在相似BitOPs预算下相比于LSQ和DQNET等方法提高了准确率，77.00%准确率的4-bit动态ResNet50显著减少了转换成本。

Conclusion: DQT突破了现有动态混合精度方法的性能瓶颈，实现了高效的、硬件友好的动态量化，开启了AI自适应效率的新前沿。

Abstract: The deployment of deep neural networks on resource-constrained devices relies
on quantization. While static, uniform quantization applies a fixed bit-width
to all inputs, it fails to adapt to their varying complexity. Dynamic,
instance-based mixed-precision quantization promises a superior
accuracy-efficiency trade-off by allocating higher precision only when needed.
However, a critical bottleneck remains: existing methods require a costly
dequantize-to-float and requantize-to-integer cycle to change precision,
breaking the integer-only hardware paradigm and compromising performance gains.
This paper introduces Dynamic Quantization Training (DQT), a novel framework
that removes this bottleneck. At the core of DQT is a nested integer
representation where lower-precision values are bit-wise embedded within
higher-precision ones. This design, coupled with custom integer-only
arithmetic, allows for on-the-fly bit-width switching through a near-zero-cost
bit-shift operation. This makes DQT the first quantization framework to enable
both dequantization-free static mixed-precision of the backbone network, and
truly efficient dynamic, instance-based quantization through a lightweight
controller that decides at runtime how to quantize each layer. We demonstrate
DQT state-of-the-art performance on ResNet18 on CIFAR-10 and ResNet50 on
ImageNet. On ImageNet, our 4-bit dynamic ResNet50 achieves 77.00% top-1
accuracy, an improvement over leading static (LSQ, 76.70%) and dynamic (DQNET,
76.94%) methods at a comparable BitOPs budget. Crucially, DQT achieves this
with a bit-width transition cost of only 28.3M simple bit-shift operations, a
drastic improvement over the 56.6M costly Multiply-Accumulate (MAC)
floating-point operations required by previous dynamic approaches - unlocking a
new frontier in efficient, adaptive AI.

</details>


### [200] [scAGC: Learning Adaptive Cell Graphs with Contrastive Guidance for Single-Cell Clustering](https://arxiv.org/abs/2508.09180)
*Huifa Li,Jie Fu,Xinlin Zhuang,Haolin Yang,Xinpeng Ling,Tong Cheng,Haochen xue,Imran Razzak,Zhili Chen*

Main category: cs.LG

TL;DR: 提出了一种名为scAGC的单细胞RNA测序聚类方法，能自适应地学习细胞图并进行对比学习，解决传统方法中的静态图结构和长尾分布问题。


<details>
  <summary>Details</summary>
Motivation: 克服单细胞RNA测序数据的高维、稀疏性和长尾分布特点，提高细胞类型的聚类准确性。

Method: 提出了一种自适应拓扑图自动编码器，结合Gumbel-Softmax动态优化图结构，同时引入ZINB损失和对比学习目标以增强鲁棒性和稳定性。

Result: 在9个真实数据集上实验，scAGC在9个数据集的NMI得分和7个数据集的ARI得分上均优于现有方法。

Conclusion: scAGC方法在单细胞聚类任务中表现突出，具有较好的鲁棒性和聚类效果，证明了其在避免传统方法局限上的潜力。

Abstract: Accurate cell type annotation is a crucial step in analyzing single-cell RNA
sequencing (scRNA-seq) data, which provides valuable insights into cellular
heterogeneity. However, due to the high dimensionality and prevalence of zero
elements in scRNA-seq data, traditional clustering methods face significant
statistical and computational challenges. While some advanced methods use graph
neural networks to model cell-cell relationships, they often depend on static
graph structures that are sensitive to noise and fail to capture the
long-tailed distribution inherent in single-cell populations.To address these
limitations, we propose scAGC, a single-cell clustering method that learns
adaptive cell graphs with contrastive guidance. Our approach optimizes feature
representations and cell graphs simultaneously in an end-to-end manner.
Specifically, we introduce a topology-adaptive graph autoencoder that leverages
a differentiable Gumbel-Softmax sampling strategy to dynamically refine the
graph structure during training. This adaptive mechanism mitigates the problem
of a long-tailed degree distribution by promoting a more balanced neighborhood
structure. To model the discrete, over-dispersed, and zero-inflated nature of
scRNA-seq data, we integrate a Zero-Inflated Negative Binomial (ZINB) loss for
robust feature reconstruction. Furthermore, a contrastive learning objective is
incorporated to regularize the graph learning process and prevent abrupt
changes in the graph topology, ensuring stability and enhancing convergence.
Comprehensive experiments on 9 real scRNA-seq datasets demonstrate that scAGC
consistently outperforms other state-of-the-art methods, yielding the best NMI
and ARI scores on 9 and 7 datasets, respectively.Our code is available at
Anonymous Github.

</details>


### [201] [Long-Term Client Selection for Federated Learning with Non-IID Data: A Truthful Auction Approach](https://arxiv.org/abs/2508.09181)
*Jinghong Tan,Zhian Liu,Kun Guo,Mingxiong Zhao*

Main category: cs.LG

TL;DR: 该研究提出了一种基于真实拍卖的长期客户选择联邦学习方法，用于在非独立同分布(non-IID)数据环境下，提高模型收敛性和准确性，该方法被验证能有效改善性能。


<details>
  <summary>Details</summary>
Motivation: 在物联网驾驶场景中，车辆节点数据分布的非独立同分布性影响模型表现，现有客户选择方法存在资源浪费及信息不对称等问题，亟需改进。

Method: 提出了一种基于真实拍卖的长期客户选择联邦学习（LCSFLA）方法。该方法引入新的数据质量评估机制和能量成本考量，并结合缴纳保证金的拍卖机制，激励客户参与并保证信息真实性。同时理论证明了激励兼容性和个体合理性。

Result: 实验在多个数据集上验证表明，在非独立同分布数据环境中，该方法能够有效缓解性能下降问题，并提高模型效果。

Conclusion: 提出的LCSFLA方法为物联网车辆场景中联邦学习的客户选择提供了新的思路，兼顾了社会效益、数据质量及能量成本，具有良好的应用前景。

Abstract: Federated learning (FL) provides a decentralized framework that enables
universal model training through collaborative efforts on mobile nodes, such as
smart vehicles in the Internet of Vehicles (IoV). Each smart vehicle acts as a
mobile client, contributing to the process without uploading local data. This
method leverages non-independent and identically distributed (non-IID) training
data from different vehicles, influenced by various driving patterns and
environmental conditions, which can significantly impact model convergence and
accuracy. Although client selection can be a feasible solution for non-IID
issues, it faces challenges related to selection metrics. Traditional metrics
evaluate client data quality independently per round and require client
selection after all clients complete local training, leading to resource
wastage from unused training results. In the IoV context, where vehicles have
limited connectivity and computational resources, information asymmetry in
client selection risks clients submitting false information, potentially making
the selection ineffective. To tackle these challenges, we propose a novel
Long-term Client-Selection Federated Learning based on Truthful Auction
(LCSFLA). This scheme maximizes social welfare with consideration of long-term
data quality using a new assessment mechanism and energy costs, and the advised
auction mechanism with a deposit requirement incentivizes client participation
and ensures information truthfulness. We theoretically prove the incentive
compatibility and individual rationality of the advised incentive mechanism.
Experimental results on various datasets, including those from IoV scenarios,
demonstrate its effectiveness in mitigating performance degradation caused by
non-IID data.

</details>


### [202] [Breath as a biomarker: A survey of contact and contactless applications and approaches in respiratory monitoring](https://arxiv.org/abs/2508.09187)
*Almustapha A. Wakili,Babajide J. Asaju,Woosub Jung*

Main category: cs.LG

TL;DR: 本文综述了基于呼吸分析的健康监测方法，包括通过先进的机器学习和深度学习技术实现的接触式和非接触式方法，以及其在多种应用上的能力与挑战。


<details>
  <summary>Details</summary>
Motivation: 呼吸分析在疾病检测和健康监测中具有重要意义。然而，传统接触式方法在长期监测中存在舒适性和实用性上的挑战，故需要更先进的技术手段进行改进。

Method: 通过综述的方法，分析了接触式与非接触式呼吸检测技术，并探讨了包括Wi-Fi、声学传感在内的新兴非接触式技术，同时评估了数据预处理、特征提取及分类技术在机器学习和深度学习模型中的应用。

Result: 揭示了非接触式方法在准确非侵入性检测方面的潜力，涵盖从单用户呼吸率检测到多用户场景以及呼吸疾病诊断等应用。

Conclusion: 文章总结了目前呼吸分析领域的技术现状及挑战，提出了解决数据隐私、多用户干扰及数据集稀缺性等问题的潜在路径，强调了解释性人工智能、联邦学习及迁移学习等趋势并为未来创新提供了框架指导。

Abstract: Breath analysis has emerged as a critical tool in health monitoring, offering
insights into respiratory function, disease detection, and continuous health
assessment. While traditional contact-based methods are reliable, they often
pose challenges in comfort and practicality, particularly for long-term
monitoring. This survey comprehensively examines contact-based and contactless
approaches, emphasizing recent advances in machine learning and deep learning
techniques applied to breath analysis. Contactless methods, including Wi-Fi
Channel State Information and acoustic sensing, are analyzed for their ability
to provide accurate, noninvasive respiratory monitoring. We explore a broad
range of applications, from single-user respiratory rate detection to
multi-user scenarios, user identification, and respiratory disease detection.
Furthermore, this survey details essential data preprocessing, feature
extraction, and classification techniques, offering comparative insights into
machine learning/deep learning models suited to each approach. Key challenges
like dataset scarcity, multi-user interference, and data privacy are also
discussed, along with emerging trends like Explainable AI, federated learning,
transfer learning, and hybrid modeling. By synthesizing current methodologies
and identifying open research directions, this survey offers a comprehensive
framework to guide future innovations in breath analysis, bridging advanced
technological capabilities with practical healthcare applications.

</details>


### [203] [Fine-Grained Safety Neurons with Training-Free Continual Projection to Reduce LLM Fine Tuning Risks](https://arxiv.org/abs/2508.09190)
*Bing Han,Feifei Zhao,Dongcheng Zhao,Guobin Shen,Ping Wu,Yu Shi,Yi Zeng*

Main category: cs.LG

TL;DR: 提出了一种名为FGSN的方法，通过无训练的持续投影减少大语言模型微调带来的安全风险，同时保持模型效用。


<details>
  <summary>Details</summary>
Motivation: 现有方法在微调后安全性防御上缺乏对安全层和细粒度神经元的全面考虑，无法平衡安全性和效用。

Method: 设计细粒度安全神经元（FGSN），将参数投影到安全方向，并通过无训练、持续投影机制调节模型，优化细粒度神经单元；引入多维异构安全神经元簇优化机制，实现持续防御和泛化能力提升。

Result: 该方法显著降低了有害性评分和攻击成功率，参数改动最小，并保留了模型效用。

Conclusion: FGSN方法克服了现有方法的不足，有效平衡了大语言模型微调后的安全性和实用性。

Abstract: Fine-tuning as service injects domain-specific knowledge into large language
models (LLMs), while challenging the original alignment mechanisms and
introducing safety risks. A series of defense strategies have been proposed for
the alignment, fine-tuning, and post-fine-tuning phases, where most
post-fine-tuning defenses rely on coarse-grained safety layer mapping. These
methods lack a comprehensive consideration of both safety layers and
fine-grained neurons, limiting their ability to efficiently balance safety and
utility. To address this, we propose the Fine-Grained Safety Neurons (FGSN)
with Training-Free Continual Projection method to reduce the fine-tuning safety
risks. FGSN inherently integrates the multi-scale interactions between safety
layers and neurons, localizing sparser and more precise fine-grained safety
neurons while minimizing interference with downstream task neurons. We then
project the safety neuron parameters onto safety directions, improving model
safety while aligning more closely with human preferences. Extensive
experiments across multiple fine-tuned LLM models demonstrate that our method
significantly reduce harmfulness scores and attack success rates with minimal
parameter modifications, while preserving the model's utility. Furthermore, by
introducing a task-specific, multi-dimensional heterogeneous safety neuron
cluster optimization mechanism, we achieve continual defense and generalization
capability against unforeseen emerging safety concerns.

</details>


### [204] [From Values to Tokens: An LLM-Driven Framework for Context-aware Time Series Forecasting via Symbolic Discretization](https://arxiv.org/abs/2508.09191)
*Xiaoyu Tao,Shilong Zhang,Mingyue Cheng,Daoyu Wang,Tingyue Pan,Bokai Pan,Changqing Zhang,Shijin Wang*

Main category: cs.LG

TL;DR: 提出了一种新方法TokenCast，利用LLM驱动的框架通过语言符号表示来改进时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法难以有效整合历史数值与非结构化文本上下文。

Method: 使用离散分词器将时间序列转换为符号化的时间token，与基于语言的输入进行结构对齐，并通过LLM嵌入至共享表示空间，最后通过监督方式微调LLM进行预测。

Result: 在不同实际数据集上的实验证明了TokenCast的有效性和可推广性。

Conclusion: TokenCast为通过LLM驱动语言符号表示改进时间序列预测提供了一种通用方法。

Abstract: Time series forecasting plays a vital role in supporting decision-making
across a wide range of critical applications, including energy, healthcare, and
finance. Despite recent advances, forecasting accuracy remains limited due to
the challenge of integrating historical numerical sequences with contextual
features, which often comprise unstructured textual data. To address this
challenge, we propose TokenCast, an LLM-driven framework that leverages
language-based symbolic representations as a unified intermediary for
context-aware time series forecasting. Specifically, TokenCast employs a
discrete tokenizer to transform continuous numerical sequences into temporal
tokens, enabling structural alignment with language-based inputs. To bridge the
semantic gap between modalities, both temporal and contextual tokens are
embedded into a shared representation space via a pre-trained large language
model (LLM), further optimized with autoregressive generative objectives.
Building upon this unified semantic space, the aligned LLM is subsequently
fine-tuned in a supervised manner to predict future temporal tokens, which are
then decoded back into the original numerical space. Extensive experiments on
diverse real-world datasets enriched with contextual features demonstrate the
effectiveness and generalizability of TokenCast.

</details>


### [205] [Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing](https://arxiv.org/abs/2508.09192)
*Xu Wang,Chenkai Xu,Yijie Jin,Jiachun Jin,Hao Zhang,Zhijie Deng*

Main category: cs.LG

TL;DR: 本研究提出名为D2F的策略，大幅提高离散扩散大语言模型(dLLMs)的推理速度，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 目前开源的离散扩散大语言模型未能实现比同等大小的自回归大语言模型更快的推理速度。

Method: 提出了一种名为离散扩散强迫(D2F)的策略，其中包括块状自回归生成和跨块并行解码算法，并通过非对称蒸馏过程实现dLLMs的改进。

Result: D2F dLLMs在GSM8K任务上推理速度比LLaMA3和Qwen2.5快2.5倍以上，相较于原始dLLMs如LLaDA和Dream，效率提升超过50倍，且保持类似的输出质量。

Conclusion: D2F策略有效地改进了dLLMs的推理性能，解决了效率瓶颈，为离散扩散模型的应用开辟了新的方向。

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising
alternative to autoregressive (AR) LLMs for text generation, with the potential
to decode multiple tokens in a single iteration. However, none of the existing
open-source dLLMs have achieved superior inference speed over AR LLMs of
similar size. This paper breaks this barrier based on a simple and effective
strategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key
capabilities: (1) block-wise autoregressive generation to enable KV cache
utilization; (2) prediction of following tokens without requiring completion of
prior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs
are refurbished into an AR-diffusion hybrid paradigm for efficient inference.
D2F can be implemented with an asymmetric distillation process based on
pre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,
which enables a trade-off between efficiency and efficacy. Empirically, D2F
dLLMs achieve more than $\mathbf{2.5\times}$ inference speed than LLaMA3 and
Qwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the
acceleration can be more than $\mathbf{50\times}$ while maintaining comparable
output quality. The code is available at
https://github.com/zhijie-group/Discrete-Diffusion-Forcing.

</details>


### [206] [Multi-Objective Instruction-Aware Representation Learning in Procedural Content Generation RL](https://arxiv.org/abs/2508.09193)
*Sung-Hyun Kim,In-Chang Baek,Seo-Young Lee,Geum-Hwan Hwang,Kyung-Joong Kim*

Main category: cs.LG

TL;DR: 提出了一种增强语言控制能力的多目标表征学习方法MIPCGRL，解决了复杂指令下生成内容可控性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于指令的强化学习方法无法充分利用文本输入的表达能力，尤其是在复杂、多目标指令下可控性较差。

Method: 通过引入句子嵌入作为条件，提出了MIPCGRL方法，结合多标签分类和多头回归网络训练多目标嵌入空间。

Result: 实验结果显示，在多目标指令的控制力上提升了最多13.8%。

Conclusion: MIPCGRL方法提升了处理复杂指令和生成内容的表达性与灵活性。

Abstract: Recent advancements in generative modeling emphasize the importance of
natural language as a highly expressive and accessible modality for controlling
content generation. However, existing instructed reinforcement learning for
procedural content generation (IPCGRL) method often struggle to leverage the
expressive richness of textual input, especially under complex, multi-objective
instructions, leading to limited controllability. To address this problem, we
propose \textit{MIPCGRL}, a multi-objective representation learning method for
instructed content generators, which incorporates sentence embeddings as
conditions. MIPCGRL effectively trains a multi-objective embedding space by
incorporating multi-label classification and multi-head regression networks.
Experimental results show that the proposed method achieves up to a 13.8\%
improvement in controllability with multi-objective instructions. The ability
to process complex instructions enables more expressive and flexible content
generation.

</details>


### [207] [Meta-Learning for Speeding Up Large Model Inference in Decentralized Environments](https://arxiv.org/abs/2508.09194)
*Yipeng Du,Zihao Wang,Ahmad Farhan,Claudio Angione,Harry Yang,Fielding Johnston,James P. Buban,Patrick Colangelo,Yue Zhao,Yuzhe Yang*

Main category: cs.LG

TL;DR: 本论文提出一种基于元学习的框架，用于在去中心化系统中自动选择最优加速方法，从而应对大规模模型部署中的计算成本和数据安全挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型需求增加，其计算成本和数据安全问题凸显；开发一种高效的推理加速方案变得必要。

Method: 采用元学习框架，根据不同任务历史性能数据自动选择推理加速方法，替代随机选择或人为判断。

Result: 实验结果表明，该框架显著提升了推理加速效率，并超越了传统方法。

Conclusion: 提出的框架有效解决了去中心化AI系统中的推理加速问题，为广泛经济适用的AI解决方案提供了新的可能性。

Abstract: The deployment of large-scale models, such as large language models (LLMs),
incurs substantial costs due to their computational demands. To mitigate these
costs and address challenges related to scalability and data security, there is
a growing shift towards decentralized systems for model deployment, where
choosing efficient inference acceleration schemes become crucial to manage
computational resources effectively and enhance system responsiveness. In this
work, we address the challenge of selecting optimal acceleration methods in
decentralized systems by introducing a meta-learning-based framework. This
framework automates the selection process by learning from historical
performance data of various acceleration techniques across different tasks.
Unlike traditional methods that rely on random selection or expert intuition,
our approach systematically identifies the best acceleration strategies based
on the specific characteristics of each task. We demonstrate that our
meta-learning framework not only streamlines the decision-making process but
also consistently outperforms conventional methods in terms of efficiency and
performance. Our results highlight the potential of inference acceleration in
decentralized AI systems, offering a path towards more democratic and
economically feasible artificial intelligence solutions.

</details>


### [208] [ADT4Coupons: An Innovative Framework for Sequential Coupon Distribution in E-commerce](https://arxiv.org/abs/2508.09198)
*Li Kong,Bingzhe Wang,Zhou Chen,Suhan Hu,Yuchao Ma,Qi Qi,Suoyuan Song,Bicheng Jin*

Main category: cs.LG

TL;DR: 本文提出了一个新框架（ADT4Coupons）来优化优惠券分发策略，通过长期提高收入和提升用户参与度。


<details>
  <summary>Details</summary>
Motivation: 现有的优惠券分发策略未能有效利用用户与平台之间的复杂交互导致了效果瓶颈，因此需要新的方法优化分发决策。

Method: 提出一种新框架（ADT4Coupons），通过整合通用场景、全面的历史数据顺序建模及高效迭代更新，对分发决策进行优化。

Result: 在真实工业数据集以及公开和合成数据集上的实验结果表明该框架具有显著优势。

Conclusion: ADT4Coupons框架能够优化在线决策，提升多种场景下的收益与用户参与效果。

Abstract: Coupon distribution is a critical marketing strategy used by online platforms
to boost revenue and enhance user engagement. Regrettably, existing coupon
distribution strategies fall far short of effectively leveraging the complex
sequential interactions between platforms and users. This critical oversight,
despite the abundance of e-commerce log data, has precipitated a performance
plateau. In this paper, we focus on the scene that the platforms make
sequential coupon distribution decision multiple times for various users, with
each user interacting with the platform repeatedly. Based on this marketing
scenario, we propose a novel marketing framework, named Aligned Decision
Transformer for Coupons (ADT4Coupons), to directly devise coupon distribution
policy for long-term revenue boosting. ADT4Coupons enables optimized online
decision-making in a variety of real-world marketing scenarios. It achieves
this by seamlessly integrating three key characteristics, general scenarios,
sequential modeling with more comprehensive historical data, and efficient
iterative updates within a unified framework. Furthermore, empirical results on
real-world industrial dataset, alongside public and synthetic datasets
demonstrate the superiority of our framework.

</details>


### [209] [Building Safer Sites: A Large-Scale Multi-Level Dataset for Construction Safety Research](https://arxiv.org/abs/2508.09203)
*Zhenhui Ou,Dawei Li,Zhen Tan,Wenlin Li,Huan Liu,Siyuan Song*

Main category: cs.LG

TL;DR: 本文提出了一个名为CSDataset的建筑安全综合数据集，结合了OSHA数据中的结构化属性和非结构化叙述，并进行相关分析以提供对建筑安全的改进洞察。


<details>
  <summary>Details</summary>
Motivation: 现有建筑安全数据集规模有限且多样性不足，无法支持深入的研究和分析。

Method: 通过整合OSHA的结构化数据和非结构化叙述内容，构建了一个综合多层次的数据集CSDataset，同时进行了初步方法测试与跨层次分析。

Result: 本文发现基于投诉的检查与事件发生概率降低17.3%相关，并发布了相关数据集和代码。

Conclusion: CSDataset为利用机器学习及大型语言模型分析建筑安全问题提供了支持，并为进一步研究提供了重要数据基础。

Abstract: Construction safety research is a critical field in civil engineering, aiming
to mitigate risks and prevent injuries through the analysis of site conditions
and human factors. However, the limited volume and lack of diversity in
existing construction safety datasets pose significant challenges to conducting
in-depth analyses. To address this research gap, this paper introduces the
Construction Safety Dataset (CSDataset), a well-organized comprehensive
multi-level dataset that encompasses incidents, inspections, and violations
recorded sourced from the Occupational Safety and Health Administration (OSHA).
This dataset uniquely integrates structured attributes with unstructured
narratives, facilitating a wide range of approaches driven by machine learning
and large language models. We also conduct a preliminary approach benchmarking
and various cross-level analyses using our dataset, offering insights to inform
and enhance future efforts in construction safety. For example, we found that
complaint-driven inspections were associated with a 17.3% reduction in the
likelihood of subsequent incidents. Our dataset and code are released at
https://github.com/zhenhuiou/Construction-Safety-Dataset-CSDataset.

</details>


### [210] [MoQE: Improve Quantization Model performance via Mixture of Quantization Experts](https://arxiv.org/abs/2508.09204)
*Jinhao Zhang,Yunquan Zhang,Boyang Zhang,Zeyu Liu,Daning Cheng*

Main category: cs.LG

TL;DR: 提出了一种基于专家混合架构的量化推理框架MoQE，显著提升量化模型性能并减少精度损失。


<details>
  <summary>Details</summary>
Motivation: 量化方法虽能提升模型效率和降低部署成本，但会导致模型精度降低，需要一种能在提升量化模型效率的同时最大化减少精度损失的方法。

Method: MoQE结合多种量化变体模型，利用轻量级结构感知路由模型，根据输入数据动态选择最合适的专家模型，用于减少单一量化模型的性能下降。

Result: 在多种CV和NLP任务中，MoQE在模型性能与延迟方面与当前最优量化模型可媲美。

Conclusion: 提出的MoQE框架兼顾量化效率和模型精度，为在受限设备中部署深度学习模型提供了新的解决方案。

Abstract: Quantization method plays a crucial role in improving model efficiency and
reducing deployment costs, enabling the widespread application of deep learning
models on resource-constrained devices. However, the quantization process
inevitably introduces accuracy degradation. In this paper, we propose Mixture
of Quantization Experts( abbr. MoQE), a quantization inference framework based
on the Mixture-of-Experts (MoE) architecture, aiming to jointly improve the
performance of quantization models. MoQE combines multiple quantization
variants of one full-precision model as specialized "quantization experts" and
dynamically routes input data to the most suitable expert based on its
characteristics. MoQE alleviates the performance degradation commonly seen in
single quantization models through specialization quantization expert models.
We design lightweight, structure-aware router models tailored for both CV and
NLP tasks. Experimental evaluations on ResNet, LLaMA, and Qwen model families
across benchmark datasets including ImageNet, WikiText, C4, and OpenWebText
demonstrate that MoQE achieves performance comparable to SOTA quantization
model, without incurring significant increases in inference latency.

</details>


### [211] [The First Differentiable Transfer-Based Algorithm for Discrete MicroLED Repair](https://arxiv.org/abs/2508.09206)
*Ning-Yuan Lue*

Main category: cs.LG

TL;DR: 本文提出一种基于可微分传输模块的修复算法，在高通量microLED制造中显著提高了修复性能与规划效率。


<details>
  <summary>Details</summary>
Motivation: 推动高效microLED制造，减少XY平台移动，适应多种优化目标。

Method: 通过提出首个可微分传输模块，设计修复算法，用梯度优化替代强化学习方法。

Result: 在2000x2000阵列上，传输步骤减少50%，规划时间低于2分钟。

Conclusion: 该方法加速了AR/VR和下一代显示器制造中microLED的修复，为实际应用提供了灵活的解决方案。

Abstract: Laser-enabled selective transfer, a key process in high-throughput microLED
fabrication, requires computational models that can plan shift sequences to
minimize motion of XY stages and adapt to varying optimization objectives
across the substrate. We propose the first repair algorithm based on a
differentiable transfer module designed to model discrete shifts of transfer
platforms, while remaining trainable via gradient-based optimization. Compared
to local proximity searching algorithms, our approach achieves superior repair
performance and enables more flexible objective designs, such as minimizing the
number of steps. Unlike reinforcement learning (RL)-based approaches, our
method eliminates the need for handcrafted feature extractors and trains
significantly faster, allowing scalability to large arrays. Experiments show a
50% reduction in transfer steps and sub-2-minute planning time on 2000x2000
arrays. This method provides a practical and adaptable solution for
accelerating microLED repair in AR/VR and next-generation display fabrication.

</details>


### [212] [Hierarchical Adaptive networks with Task vectors for Test-Time Adaptation](https://arxiv.org/abs/2508.09223)
*Sameer Ambekar,Daniel M. Lang,Julia A. Schnabel*

Main category: cs.LG

TL;DR: 本文提出了一种新的测试时适应方法，名为Hi-Vec，利用分层结构来动态应对域间分布变化。


<details>
  <summary>Details</summary>
Motivation: 现有方法在应对复杂分布变化时表现有限，因此需要一种新颖且更灵活的适应机制。

Method: Hi-Vec通过增加多层分层结构进行动态测试适应，包括动态层选择、权重合并机制和线性层一致性功能来限制噪声干扰的影响。

Result: 实验结果表明，Hi-Vec在多个目标数据集上的表现优于现有方法，增强了模型鲁棒性和不确定性处理能力。

Conclusion: Hi-Vec展现了在复杂分布变化情况下改进现有状态的潜力，并证明其可以应对小批量和异常率高的测试条件。

Abstract: Test-time adaptation allows pretrained models to adjust to incoming data
streams, addressing distribution shifts between source and target domains.
However, standard methods rely on single-dimensional linear classification
layers, which often fail to handle diverse and complex shifts. We propose
Hierarchical Adaptive Networks with Task Vectors (Hi-Vec), which leverages
multiple layers of increasing size for dynamic test-time adaptation. By
decomposing the encoder's representation space into such hierarchically
organized layers, Hi-Vec, in a plug-and-play manner, allows existing methods to
adapt to shifts of varying complexity. Our contributions are threefold: First,
we propose dynamic layer selection for automatic identification of the optimal
layer for adaptation to each test batch. Second, we propose a mechanism that
merges weights from the dynamic layer to other layers, ensuring all layers
receive target information. Third, we propose linear layer agreement that acts
as a gating function, preventing erroneous fine-tuning by adaptation on noisy
batches. We rigorously evaluate the performance of Hi-Vec in challenging
scenarios and on multiple target datasets, proving its strong capability to
advance state-of-the-art methods. Our results show that Hi-Vec improves
robustness, addresses uncertainty, and handles limited batch sizes and
increased outlier rates.

</details>


### [213] [GSMT: Graph Fusion and Spatiotemporal TaskCorrection for Multi-Bus Trajectory Prediction](https://arxiv.org/abs/2508.09227)
*Fan Ding,Hwa Hui Tew,Junn Yong Loo,Susilawati,LiTong Liu,Fang Yu Leong,Xuewen Luo,Kar Keong Chin,Jia Jun Gan*

Main category: cs.LG

TL;DR: 本文提出了GSMT方法，将图注意力网络与序列到序列的RNN结合应用于公交线路轨迹预测，通过任务修正器对初步预测结果进行微调，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提高公交车轨迹预测的精度，特别是在多模态数据有限的发展中国家城市交通环境中。

Method: 采用GSMT模型，该方法融合图注意力网络和序列到序列RNN，并通过任务修正器进行二次预测调整，同时结合动态公交及静态站点信息。

Result: 在马来西亚吉隆坡的真实数据集上进行实验，显示GSMT在短期及长期轨迹预测任务中均优于现有方法。

Conclusion: GSMT通过二阶段预测方法提升了密集城市交通环境下公交车轨迹预测的精度，探索了一种有效的轨迹预测新方法。

Abstract: Accurate trajectory prediction for buses is crucial in intelligent
transportation systems, particularly within urban environments. In developing
regions where access to multimodal data is limited, relying solely on onboard
GPS data remains indispensable despite inherent challenges. To address this
problem, we propose GSMT, a hybrid model that integrates a Graph Attention
Network (GAT) with a sequence-to-sequence Recurrent Neural Network (RNN), and
incorporates a task corrector capable of extracting complex behavioral patterns
from large-scale trajectory data. The task corrector clusters historical
trajectories to identify distinct motion patterns and fine-tunes the
predictions generated by the GAT and RNN. Specifically, GSMT fuses dynamic bus
information and static station information through embedded hybrid networks to
perform trajectory prediction, and applies the task corrector for secondary
refinement after the initial predictions are generated. This two-stage approach
enables multi-node trajectory prediction among buses operating in dense urban
traffic environments under complex conditions. Experiments conducted on a
real-world dataset from Kuala Lumpur, Malaysia, demonstrate that our method
significantly outperforms existing approaches, achieving superior performance
in both short-term and long-term trajectory prediction tasks.

</details>


### [214] [Blockchain Network Analysis using Quantum Inspired Graph Neural Networks & Ensemble Models](https://arxiv.org/abs/2508.09237)
*Luigi D'Amico,Daniel De Rosso,Ninad Dixit,Raul Salles de Padua,Samuel Palmer,Samuel Mugel,Román Orús,Holger Eble,Ali Abedi*

Main category: cs.LG

TL;DR: 本研究提出结合量子启发图神经网络和灵活选择集成模型的创新方法，专注于区块链网络的反洗钱分析，取得了F2分数74.8%的成果。


<details>
  <summary>Details</summary>
Motivation: 区块链中非法交易检测面临重大挑战，亟需设计高效的解决方案。

Method: 采用量子启发图神经网络（QI-GNN），并结合一种新型CP分解层，同时灵活选择QBoost或随机森林分类器进行集成分析。

Result: 在检测欺诈交易中，研究方法取得F2分数74.8%的优秀表现，较传统方法表现更优。

Conclusion: 量子启发技术及CP层结构的引入显示出在复杂网络分析中的潜力，鼓励金融行业更广泛采用此类算法来打击欺诈行为。

Abstract: In the rapidly evolving domain of financial technology, the detection of
illicit transactions within blockchain networks remains a critical challenge,
necessitating robust and innovative solutions. This work proposes a novel
approach by combining Quantum Inspired Graph Neural Networks (QI-GNN) with
flexibility of choice of an Ensemble Model using QBoost or a classic model such
as Random Forrest Classifier. This system is tailored specifically for
blockchain network analysis in anti-money laundering (AML) efforts. Our
methodology to design this system incorporates a novel component, a Canonical
Polyadic (CP) decomposition layer within the graph neural network framework,
enhancing its capability to process and analyze complex data structures
efficiently. Our technical approach has undergone rigorous evaluation against
classical machine learning implementations, achieving an F2 score of 74.8% in
detecting fraudulent transactions. These results highlight the potential of
quantum-inspired techniques, supplemented by the structural advancements of the
CP layer, to not only match but potentially exceed traditional methods in
complex network analysis for financial security. The findings advocate for a
broader adoption and further exploration of quantum-inspired algorithms within
the financial sector to effectively combat fraud.

</details>


### [215] [LLM Empowered Prototype Learning for Zero and Few-Shot Tasks on Tabular Data](https://arxiv.org/abs/2508.09263)
*Peng Wang,Dongsheng Wang,He Zhao,Hangting Ye,Dandan Guo,Yi Chang*

Main category: cs.LG

TL;DR: 提出了一种基于大语言模型的表格学习原型估计框架，用于零样本和少样本情境，通过查询LLM生成特征值以建立原型评估，无需训练分类器或微调LLM。


<details>
  <summary>Details</summary>
Motivation: 探索如何在零样本和少样本场景中有效利用先进的大语言模型，特别是在表格数据建模中遇到的挑战。

Method: 通过示例无关的提示词查询LLM生成特征值，然后利用这些特征值构建零样本原型，进而通过融合少样本数据进行增强，避免了训练分类器或微调LLM的过程。

Result: 实验表明，提出的方法在零样本和少样本的表格学习任务中表现出色。

Conclusion: 该框架通过基于示例无关的提示词和原型估计，实现了一种高扩展性和鲁棒性的表格学习方法，在零样本及少样本情况下表现优异。

Abstract: Recent breakthroughs in large language models (LLMs) have opened the door to
in-depth investigation of their potential in tabular data modeling. However,
effectively utilizing advanced LLMs in few-shot and even zero-shot scenarios is
still challenging. To this end, we propose a novel LLM-based prototype
estimation framework for tabular learning. Our key idea is to query the LLM to
generate feature values based example-free prompt, which solely relies on task
and feature descriptions. With the feature values generated by LLM, we can
build a zero-shot prototype in a training-free manner, which can be further
enhanced by fusing few-shot samples, avoiding training a classifier or
finetuning the LLMs. Thanks to the example-free prompt and prototype
estimation, ours bypasses the constraints brought by the example-based prompt,
providing a scalable and robust framework. Extensive experiments demonstrate
the effectiveness of ours in zero and few-shot tabular learning.

</details>


### [216] [Detection of Odor Presence via Deep Neural Networks](https://arxiv.org/abs/2508.09264)
*Matin Hassanloo,Ali Zareh,Mehmet Kemal Özdemir*

Main category: cs.LG

TL;DR: 该研究提出使用深度学习模型从嗅球局部场电位（LFPs）中实现单试次气味检测，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有气味检测方法在处理复杂混合物和非侵入性记录过程中单次可靠性不足的问题。

Method: 提出两种一维卷积网络（ResCNN和AttentionCNN）的集成，用于从七只清醒小鼠的多通道嗅球LFPs中解码气味存在信号。

Result: 最终集成模型在2,349次试验中平均准确率为86.6%，F1得分为81.0%，AUC为0.9247，性能显著优于先前基准。

Conclusion: 证明了从LFPs中实现单试次气味检测的可行性，同时展示了深度学习模型在研究嗅觉表征中的潜力。

Abstract: Odor detection underpins food safety, environmental monitoring, medical
diagnostics, and many more fields. The current artificial sensors developed for
odor detection struggle with complex mixtures while non-invasive recordings
lack reliable single-trial fidelity. To develop a general system for odor
detection, in this study we present a preliminary work where we aim to test two
hypotheses: (i) that spectral features of local field potentials (LFPs) are
sufficient for robust single-trial odor detection and (ii) that signals from
the olfactory bulb alone are adequate. To test two hypotheses, we propose an
ensemble of complementary one-dimensional convolutional networks (ResCNN and
AttentionCNN) that decodes the presence of odor from multichannel olfactory
bulb LFPs. Tested on 2,349 trials from seven awake mice, our final ensemble
model supports both hypotheses, achieving a mean accuracy of 86.6%, an F1-score
of 81.0%, and an AUC of 0.9247, substantially outperforming previous
benchmarks. In addition, the t-SNE visualization confirms that our framework
captures biologically significant signatures. These findings establish the
feasibility of robust single-trial detection of the presence of odor from
extracellular LFPs, as well as demonstrate the potential of deep learning
models to provide a deeper understanding of olfactory representations.

</details>


### [217] [Over-Squashing in GNNs and Causal Inference of Rewiring Strategies](https://arxiv.org/abs/2508.09265)
*Danial Saber,Amirali Salehi-Abari*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的方法评估图神经网络中的过度压缩问题，并通过实证分析验证了重新连接策略在减轻过度压缩方面的作用，但效果因数据集和方法而异。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）在各个领域表现出色，但存在消息传递过程中信息过度压缩的问题，作者提出研究此问题及其解决办法的动机。

Method: 提出一种基于节点对之间敏感性衰减率的评估方法，并将其扩展为四种图层级统计，同时结合因果设计量化不同重新连接策略的效果。

Result: 研究发现图分类数据集中的过度压缩问题较为显著，重新连接能够有效缓解这一问题；但对于节点分类数据集，重新连接可能加重过度压缩，且表现在性能提升上的效果不显著。

Conclusion: 重新连接策略适用于处理过度压缩严重的图，但应适度使用，过度使用可能适得其反。作者还提供了一个诊断工具，可以在训练前评估重新连接是否有助于提升模型表现。

Abstract: Graph neural networks (GNNs) have exhibited state-of-the-art performance
across wide-range of domains such as recommender systems, material design, and
drug repurposing. Yet message-passing GNNs suffer from over-squashing --
exponential compression of long-range information from distant nodes -- which
limits expressivity. Rewiring techniques can ease this bottleneck; but their
practical impacts are unclear due to the lack of a direct empirical
over-squashing metric. We propose a rigorous, topology-focused method for
assessing over-squashing between node pairs using the decay rate of their
mutual sensitivity. We then extend these pairwise assessments to four
graph-level statistics (prevalence, intensity, variability, extremity).
Coupling these metrics with a within-graph causal design, we quantify how
rewiring strategies affect over-squashing on diverse graph- and
node-classification benchmarks. Our extensive empirical analyses show that most
graph classification datasets suffer from over-squashing (but to various
extents), and rewiring effectively mitigates it -- though the degree of
mitigation, and its translation into performance gains, varies by dataset and
method. We also found that over-squashing is less notable in node
classification datasets, where rewiring often increases over-squashing, and
performance variations are uncorrelated with over-squashing changes. These
findings suggest that rewiring is most beneficial when over-squashing is both
substantial and corrected with restraint -- while overly aggressive rewiring,
or rewiring applied to minimally over-squashed graphs, is unlikely to help and
may even harm performance. Our plug-and-play diagnostic tool lets practitioners
decide -- before any training -- whether rewiring is likely to pay off.

</details>


### [218] [Constrained Black-Box Attacks Against Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.09275)
*Amine Andam,Jamal Bentahar,Mustapha Hedabou*

Main category: cs.LG

TL;DR: 本文分析了在更加现实和有限条件下，协作多智能体强化学习(c-MARL)的潜在漏洞，并提出了一种简单高效的对抗扰动生成算法，验证表明该算法在不同算法和环境中表现优越且效率更高。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于训练阶段攻击或不切实际的场景，缺少对 c-MARL 在现实条件下潜在漏洞的研究，这是阻碍其广泛应用的关键障碍。

Method: 作者研究了只能收集和干扰智能体观测，甚至无法获取任何信息的对手攻击场景，并提出了一种简单但高效的对抗扰动生成算法，通过改变智能体对环境的感知使其出错。

Result: 该算法在三大基准和22种环境中的实验证明了其有效性，与以前方法相比，样本效率更高，仅需要1,000个样本。

Conclusion: 研究显示，提出的攻击方法在多种场景中表现出色，提示了 c-MARL 系统可能面临的实际风险，强调了进行进一步安全性研究的重要性。

Abstract: Collaborative multi-agent reinforcement learning (c-MARL) has rapidly
evolved, offering state-of-the-art algorithms for real-world applications,
including sensitive domains. However, a key challenge to its widespread
adoption is the lack of a thorough investigation into its vulnerabilities to
adversarial attacks. Existing work predominantly focuses on training-time
attacks or unrealistic scenarios, such as access to policy weights or the
ability to train surrogate policies. In this paper, we investigate new
vulnerabilities under more realistic and constrained conditions, assuming an
adversary can only collect and perturb the observations of deployed agents. We
also consider scenarios where the adversary has no access at all. We propose
simple yet highly effective algorithms for generating adversarial perturbations
designed to misalign how victim agents perceive their environment. Our approach
is empirically validated on three benchmarks and 22 environments, demonstrating
its effectiveness across diverse algorithms and environments. Furthermore, we
show that our algorithm is sample-efficient, requiring only 1,000 samples
compared to the millions needed by previous methods.

</details>


### [219] [Pattern-based Knowledge Component Extraction from Student Code Using Representation Learning](https://arxiv.org/abs/2508.09281)
*Muntasir Hoq,Griffin Pitts,Andrew Lan,Peter Brusilovsky,Bita Akram*

Main category: cs.LG

TL;DR: 提出了一种基于模式的知识单元自动提取框架，通过模式分析学生代码，以改进个性化学习模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有知识单元提取方法的非解释性问题及编程问题的多样性和复杂性对学生模型构建的挑战。

Method: 利用基于变分自编码器的框架，结合可解释性注意力机制，从学生代码中提取重要的结构模式，并将这些模式聚类形成知识单元。

Result: 通过学习曲线分析和深度知识追踪方法验证，新框架表现出显著优于传统知识追踪方法的预测性能。

Conclusion: 提出了一种可自动化、可扩展并具有解释力的知识单元提取框架，为计算机科学教育中学生学习建模奠定了基础。

Abstract: Effective personalized learning in computer science education depends on
accurately modeling what students know and what they need to learn. While
Knowledge Components (KCs) provide a foundation for such modeling, automated KC
extraction from student code is inherently challenging due to insufficient
explainability of discovered KCs and the open-endedness of programming problems
with significant structural variability across student solutions and complex
interactions among programming concepts. In this work, we propose a novel,
explainable framework for automated KC discovery through pattern-based KCs:
recurring structural patterns within student code that capture the specific
programming patterns and language constructs that students must master. Toward
this, we train a Variational Autoencoder to generate important representative
patterns from student code guided by an explainable, attention-based code
representation model that identifies important correct and incorrect pattern
implementations from student code. These patterns are then clustered to form
pattern-based KCs. We evaluate our KCs using two well-established methods
informed by Cognitive Science: learning curve analysis and Deep Knowledge
Tracing (DKT). Experimental results demonstrate meaningful learning
trajectories and significant improvements in DKT predictive performance over
traditional KT methods. This work advances knowledge modeling in CS education
by providing an automated, scalable, and explainable framework for identifying
granular code patterns and algorithmic constructs, essential for student
learning.

</details>


### [220] [Distilling Reinforcement Learning into Single-Batch Datasets](https://arxiv.org/abs/2508.09283)
*Connor Wilhelm,Dan Ventura*

Main category: cs.LG

TL;DR: 本文探讨了数据蒸馏技术，可以将大型数据集压缩为小型合成数据集，并证明该技术对强化学习任务的适应性和通用性。


<details>
  <summary>Details</summary>
Motivation: 研究通过数据蒸馏技巧来压缩数据集，同时将强化学习任务转化为监督学习任务的可行性。

Method: 提出一种扩展的邻近策略优化方法，用于元学习，以及对经典的卡杆问题、多MuJoCo环境和若干Atari游戏的蒸馏测试。

Result: 证明了数据蒸馏能将强化学习任务压缩到最小的数据集，并使其可进行一步监督学习。验证了跨任务和架构的通用性。

Conclusion: 数据蒸馏技术在任务压缩和学习模式转换中具有巨大潜力，可以有效简化复杂强化学习任务的学习过程。

Abstract: Dataset distillation compresses a large dataset into a small synthetic
dataset such that learning on the synthetic dataset approximates learning on
the original. Training on the distilled dataset can be performed in as little
as one step of gradient descent. We demonstrate that distillation is
generalizable to different tasks by distilling reinforcement learning
environments into one-batch supervised learning datasets. This demonstrates not
only distillation's ability to compress a reinforcement learning task but also
its ability to transform one learning modality (reinforcement learning) into
another (supervised learning). We present a novel extension of proximal policy
optimization for meta-learning and use it in distillation of a
multi-dimensional extension of the classic cart-pole problem, all MuJoCo
environments, and several Atari games. We demonstrate distillation's ability to
compress complex RL environments into one-step supervised learning, explore RL
distillation's generalizability across learner architectures, and demonstrate
distilling an environment into the smallest-possible synthetic dataset.

</details>


### [221] [Decentralized Weather Forecasting via Distributed Machine Learning and Blockchain-Based Model Validation](https://arxiv.org/abs/2508.09299)
*Rilwan Umar,Aydin Abadi,Basil Aldali,Benito Vincent,Elliot A. J. Hurley,Hotoon Aljazaeri,Jamie Hedley-Cook,Jamie-Lee Bell,Lambert Uwuigbusun,Mujeeb Ahmed,Shishir Nagaraja,Suleiman Sabo,Weaam Alrbeiqi*

Main category: cs.LG

TL;DR: 提出了一种结合联邦学习与区块链技术的去中心化天气预报框架，用于提升隐私、安全性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前的集中化天气预报系统因安全漏洞、可扩展性受限及单点故障问题而面临挑战。

Method: 将联邦学习与以太坊区块链结合，同时引入基于声誉的投票机制评估模型可信性，并使用IPFS进行高效的链外存储。

Result: 实验结果表明，该方法提高了天气预报的准确性，增强了系统的弹性与可扩展性。

Conclusion: 这种去中心化框架适用于实际安全要求较高的天气预报环境，具有部署潜力。

Abstract: Weather forecasting plays a vital role in disaster preparedness, agriculture,
and resource management, yet current centralized forecasting systems are
increasingly strained by security vulnerabilities, limited scalability, and
susceptibility to single points of failure. To address these challenges, we
propose a decentralized weather forecasting framework that integrates Federated
Learning (FL) with blockchain technology. FL enables collaborative model
training without exposing sensitive local data; this approach enhances privacy
and reduces data transfer overhead. Meanwhile, the Ethereum blockchain ensures
transparent and dependable verification of model updates. To further enhance
the system's security, we introduce a reputation-based voting mechanism that
assesses the trustworthiness of submitted models while utilizing the
Interplanetary File System (IPFS) for efficient off-chain storage. Experimental
results demonstrate that our approach not only improves forecasting accuracy
but also enhances system resilience and scalability, making it a viable
candidate for deployment in real-world, security-critical environments.

</details>


### [222] [Exact Verification of Graph Neural Networks with Incremental Constraint Solving](https://arxiv.org/abs/2508.09320)
*Minghao Liu,Chia-Hsuan Lu,Marta Kwiatkowska*

Main category: cs.LG

TL;DR: 本文提出了一种能够对GNN的属性和结构扰动进行精确验证的方法，并实现了一个名为GNNev的求解器，在多个聚合函数和任务上的表现均优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 当前广泛应用的图神经网络（GNN）因易受到对抗性攻击而需要鲁棒性验证，而现有方法对一些常用的聚合函数支持不足。

Method: 利用约束求解和边界收紧的技术，本文开发了一种精确的验证方法，通过递归求解放松的约束问题，提升计算效率，并实现了支持sum、max和mean聚合函数的求解器GNNev。

Result: 在Cora、CiteSeer和两组实际数据集（Amazon和Yelp）上，大量实验验证了GNNev的可用性和有效性，尤其是在sum聚合函数任务上的表现优于已有工具。

Conclusion: GNNev不仅扩展了对多种聚合函数的支持，还显著提升了验证效率和性能，为GNN对抗鲁棒性验证提供了一项更为可靠的工具。

Abstract: Graph neural networks (GNNs) are increasingly employed in high-stakes
applications, such as fraud detection or healthcare, but are susceptible to
adversarial attacks. A number of techniques have been proposed to provide
adversarial robustness guarantees, but support for commonly used aggregation
functions in message-passing GNNs is still lacking. In this paper, we develop
an exact (sound and complete) verification method for GNNs to compute
guarantees against attribute and structural perturbations that involve edge
addition or deletion, subject to budget constraints. Focusing on node
classification tasks, our method employs constraint solving with bound
tightening, and iteratively solves a sequence of relaxed constraint
satisfaction problems while relying on incremental solving capabilities of
solvers to improve efficiency. We implement GNNev, a versatile solver for
message-passing neural networks, which supports three aggregation functions,
sum, max and mean, with the latter two considered here for the first time.
Extensive experimental evaluation of GNNev on two standard benchmarks (Cora and
CiteSeer) and two real-world fraud datasets (Amazon and Yelp) demonstrates its
usability and effectiveness, as well as superior performance compared to
existing {exact verification} tools on sum-aggregated node classification
tasks.

</details>


### [223] [Synaptic Pruning: A Biological Inspiration for Deep Learning Regularization](https://arxiv.org/abs/2508.09330)
*Gideon Vos,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: cs.LG

TL;DR: 该论文提出了一种基于生物学原理的权重修剪方法，通过移除低重要性权重以改善网络性能，特别是在时间序列预测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 受神经生物学中突触修剪现象的启发，该研究旨在开发一种更符合生物学启发的正则化方法，以提高神经网络的训练效率和预测能力。

Method: 提出了一种基于权重绝对值大小的修剪方法，通过逐步去除低重要性连接，在训练中动态调整全局稀疏性曲线，取代传统的Dropout。其采用立方调度策略，并直接融入训练环路，避免了额外的修剪和微调步骤。

Result: 在RNN、LSTM和Patch Time Series Transformer等多种时间序列预测模型中，与无Dropout或传统Dropout方法相比，该方法在多个数据集上表现出一致的优越性，包括金融预测中平均绝对误差减少最多达52%。

Conclusion: 该动态修剪机制通过结合权重消除和渐进稀疏化，改进了神经网络的正则化，同时在金融时间序列预测等任务中显现了显著潜力，是传统Dropout技术的一个实用替代方案。

Abstract: Synaptic pruning in biological brains removes weak connections to improve
efficiency. In contrast, dropout regularization in artificial neural networks
randomly deactivates neurons without considering activity-dependent pruning. We
propose a magnitude-based synaptic pruning method that better reflects biology
by progressively removing low-importance connections during training.
Integrated directly into the training loop as a dropout replacement, our
approach computes weight importance from absolute magnitudes across layers and
applies a cubic schedule to gradually increase global sparsity. At fixed
intervals, pruning masks permanently remove low-importance weights while
maintaining gradient flow for active ones, eliminating the need for separate
pruning and fine-tuning phases. Experiments on multiple time series forecasting
models including RNN, LSTM, and Patch Time Series Transformer across four
datasets show consistent gains. Our method ranked best overall, with
statistically significant improvements confirmed by Friedman tests (p < 0.01).
In financial forecasting, it reduced Mean Absolute Error by up to 20% over
models with no or standard dropout, and up to 52% in select transformer models.
This dynamic pruning mechanism advances regularization by coupling weight
elimination with progressive sparsification, offering easy integration into
diverse architectures. Its strong performance, especially in financial time
series forecasting, highlights its potential as a practical alternative to
conventional dropout techniques.

</details>


### [224] [RicciFlowRec: A Geometric Root Cause Recommender Using Ricci Curvature on Financial Graphs](https://arxiv.org/abs/2508.09334)
*Zhongtian Sun,Anoushka Harit*

Main category: cs.LG

TL;DR: 提出了一个名为RicciFlowRec的几何推荐框架，利用Ricci曲率和流进行动态金融图中的根本原因归因分析。本文的实验表明该方法在财务决策支持中的有效性。


<details>
  <summary>Details</summary>
Motivation: 希望通过几何工具（Ricci曲率和流）在动态金融图中分析股票、经济指标和新闻之间的相互作用，帮助量化局部压力和预测风险传播。

Method: 应用Ricci曲率和流来量化局部压力与震荡传播，通过曲率梯度揭示因果子结构，并开发风险感知的排序函数。

Result: 在S&P 500数据及基于FinBERT的情绪分析上实验表明，方法在合成扰动下表现出更强的鲁棒性与可解释性。

Conclusion: RicciFlowRec框架为基于曲率的归因和早期阶段的风险感知排序提供支持，并计划在组合优化与收益预测领域进一步扩展。

Abstract: We propose RicciFlowRec, a geometric recommendation framework that performs
root cause attribution via Ricci curvature and flow on dynamic financial
graphs. By modelling evolving interactions among stocks, macroeconomic
indicators, and news, we quantify local stress using discrete Ricci curvature
and trace shock propagation via Ricci flow. Curvature gradients reveal causal
substructures, informing a structural risk-aware ranking function. Preliminary
results on S\&P~500 data with FinBERT-based sentiment show improved robustness
and interpretability under synthetic perturbations. This ongoing work supports
curvature-based attribution and early-stage risk-aware ranking, with plans for
portfolio optimization and return forecasting. To our knowledge, RicciFlowRec
is the first recommender to apply geometric flow-based reasoning in financial
decision support.

</details>


### [225] [Resurrecting the Salmon: Rethinking Mechanistic Interpretability with Domain-Specific Sparse Autoencoders](https://arxiv.org/abs/2508.09363)
*Charles O'Neill,Mudith Jayasekara,Max Kirkby*

Main category: cs.LG

TL;DR: 此论文研究了在特定领域（医疗文本）训练稀疏自编码器（SAEs）的优势，发现其能够更好地捕捉领域特定特征，提高重构精度和解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的稀疏自编码器在处理广泛领域数据时效果受限，存在碎片化特征和高线性残差等问题；因此，研究如何通过聚焦于特定领域分布来优化其性能。

Method: 在医疗文本领域，使用195k临床问答样例，在Gemma-2模型的第20层激活上训练JumpReLU稀疏自编码器，研究领域特定特征的捕获能力。

Result: 领域特定的SAEs解释了高达20%的更多方差，具有更高的损失恢复率，并显著降低了线性残差。同时，自动化和人工评估确认，这些特征与临床相关概念高度一致。

Conclusion: 领域限制可有效缓解广域SAEs的关键局限性，提供更完整且可解释的潜变量分解，未来或需重新审视广域“基础模型”的扩展策略。

Abstract: Sparse autoencoders (SAEs) decompose large language model (LLM) activations
into latent features that reveal mechanistic structure. Conventional SAEs train
on broad data distributions, forcing a fixed latent budget to capture only
high-frequency, generic patterns. This often results in significant linear
``dark matter'' in reconstruction error and produces latents that fragment or
absorb each other, complicating interpretation. We show that restricting SAE
training to a well-defined domain (medical text) reallocates capacity to
domain-specific features, improving both reconstruction fidelity and
interpretability. Training JumpReLU SAEs on layer-20 activations of Gemma-2
models using 195k clinical QA examples, we find that domain-confined SAEs
explain up to 20\% more variance, achieve higher loss recovery, and reduce
linear residual error compared to broad-domain SAEs. Automated and human
evaluations confirm that learned features align with clinically meaningful
concepts (e.g., ``taste sensations'' or ``infectious mononucleosis''), rather
than frequent but uninformative tokens. These domain-specific SAEs capture
relevant linear structure, leaving a smaller, more purely nonlinear residual.
We conclude that domain-confinement mitigates key limitations of broad-domain
SAEs, enabling more complete and interpretable latent decompositions, and
suggesting the field may need to question ``foundation-model'' scaling for
general-purpose SAEs.

</details>


### [226] [Understanding Dementia Speech Alignment with Diffusion-Based Image Generation](https://arxiv.org/abs/2508.09385)
*Mansi,Anastasios Lepipas,Dominika Woszczyk,Yiying Guan,Soteris Demetriou*

Main category: cs.LG

TL;DR: 本研究探讨了生成式模型能否将病理性语言与生成图片对齐，从生成图片中检测痴呆，取得75%准确率。


<details>
  <summary>Details</summary>
Motivation: 基于自然语言描述的图像生成技术迅速发展，研究探索其是否能将病理性语言信息与生成图像对齐。

Method: 利用现有图像生成模型，通过解读生成图像是否包含与痴呆有关的语言信息，并采用解释性方法分析语言贡献。

Result: 在ADReSS数据集中，仅通过生成图像成功实现了75%的痴呆检测准确率。

Conclusion: 生成模型可以将痴呆相关语言信息与图像对齐，展示了解释语言贡献的可能性。

Abstract: Text-to-image models generate highly realistic images based on natural
language descriptions and millions of users use them to create and share images
online. While it is expected that such models can align input text and
generated image in the same latent space little has been done to understand
whether this alignment is possible between pathological speech and generated
images. In this work, we examine the ability of such models to align
dementia-related speech information with the generated images and develop
methods to explain this alignment. Surprisingly, we found that dementia
detection is possible from generated images alone achieving 75% accuracy on the
ADReSS dataset. We then leverage explainability methods to show which parts of
the language contribute to the detection.

</details>


### [227] [Integrating Feature Attention and Temporal Modeling for Collaborative Financial Risk Assessment](https://arxiv.org/abs/2508.09399)
*Yue Yao,Zhen Xu,Youzhu Liu,Kunyuan Ma,Yuxiu Lin,Mohan Jiang*

Main category: cs.LG

TL;DR: 本论文提出了一种基于联邦学习的金融风险评估框架，旨在解决跨机构数据隐私和协作建模的挑战，并通过实验验证其有效性与优越性。


<details>
  <summary>Details</summary>
Motivation: 随着金融机构之间协作需求的增加和数据隐私问题的加强，如何在不共享原始数据的情况下实现有效的联合建模与风险识别是一个急需的研究课题。

Method: 该框架使用联邦学习方法，结合特征注意力机制与时间建模结构。每家金融机构训练本地子模型，并通过差分隐私与噪声注入保护模型参数，然后上传至中央服务器进行参数聚合，生成全球模型以识别系统性风险。

Result: 实验评估了通信效率、模型准确性、系统性风险检测能力和跨市场泛化能力。结果表明，该模型在所有指标上优于传统集中式方法和现有联邦学习变体。

Conclusion: 提出的方法不仅提升了风险识别的范围与效率，还保护了数据主权，是敏感金融领域中安全高效的智能风险分析解决方案。

Abstract: This paper addresses the challenges of data privacy and collaborative
modeling in cross-institution financial risk analysis. It proposes a risk
assessment framework based on federated learning. Without sharing raw data, the
method enables joint modeling and risk identification across multiple
institutions. This is achieved by incorporating a feature attention mechanism
and temporal modeling structure. Specifically, the model adopts a distributed
optimization strategy. Each financial institution trains a local sub-model. The
model parameters are protected using differential privacy and noise injection
before being uploaded. A central server then aggregates these parameters to
generate a global model. This global model is used for systemic risk
identification. To validate the effectiveness of the proposed method, multiple
experiments are conducted. These evaluate communication efficiency, model
accuracy, systemic risk detection, and cross-market generalization. The results
show that the proposed model outperforms both traditional centralized methods
and existing federated learning variants across all evaluation metrics. It
demonstrates strong modeling capabilities and practical value in sensitive
financial environments. The method enhances the scope and efficiency of risk
identification while preserving data sovereignty. It offers a secure and
efficient solution for intelligent financial risk analysis.

</details>


### [228] [Graph Neural Network and Transformer Integration for Unsupervised System Anomaly Discovery](https://arxiv.org/abs/2508.09401)
*Yun Zi,Ming Gong,Zhihao Xue,Yujun Zou,Nia Qi,Yingnan Deng*

Main category: cs.LG

TL;DR: 提出一种无监督异常检测方法，用于分布式后端服务系统，结合图卷积和Transformer实现动态行为建模与结构表征。实验验证其模型性能优越，具有实际部署潜力。


<details>
  <summary>Details</summary>
Motivation: 为了解决分布式后台系统中复杂结构依赖、多样行为演化及缺乏标注数据的异常检测难题。

Method: 通过构建动态图表示服务调用关系，使用图卷积提取结构表征，利用Transformer捕捉节点的时间行为，并通过一种可学习的嵌入机制融合这些表征，最终以非线性映射计算异常评分，实现无监督检测。

Result: 实验表明，该方法在多项关键指标上优于现有模型，在捕捉异常传播路径和动态行为建模方面表现出更强的表达能力和稳定性。

Conclusion: 提出的方法增强了异常检测的表现能力，证明其在实际云端监控环境中的高部署潜力。

Abstract: This study proposes an unsupervised anomaly detection method for distributed
backend service systems, addressing practical challenges such as complex
structural dependencies, diverse behavioral evolution, and the absence of
labeled data. The method constructs a dynamic graph based on service invocation
relationships and applies graph convolution to extract high-order structural
representations from multi-hop topologies. A Transformer is used to model the
temporal behavior of each node, capturing long-term dependencies and local
fluctuations. During the feature fusion stage, a learnable joint embedding
mechanism integrates structural and behavioral representations into a unified
anomaly vector. A nonlinear mapping is then applied to compute anomaly scores,
enabling an end-to-end detection process without supervision. Experiments on
real-world cloud monitoring data include sensitivity analyses across different
graph depths, sequence lengths, and data perturbations. Results show that the
proposed method outperforms existing models on several key metrics,
demonstrating stronger expressiveness and stability in capturing anomaly
propagation paths and modeling dynamic behavior sequences, with high potential
for practical deployment.

</details>


### [229] [NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs](https://arxiv.org/abs/2508.09473)
*Birong Pan,Mayi Xu,Qiankun Pi,Jianhao Chen,Yuanyuan Zhu,Ming Zhong,Tieyun Qian*

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为NeuronTune的新方法，通过细粒度动态调整神经元激活来同时优化语言模型的安全性和实用性，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决当前大语言模型在安全性和实用性方面的矛盾；现有技术对恶意攻击抗性不足，同时又经常拒绝正常请求，文本质量和任务性能也有所下降。

Method: 提出NeuronTune框架，采用归因分析识别安全和实用相关的神经元，并通过元学习动态调整神经元激活，支持以神经元数量为阈值的灵活调节。

Result: 实验证明该方法在显著提高模型安全性的同时，仍能保持良好的模型实用性，超越现有最先进技术。

Conclusion: NeuronTune方法提供了一种细粒度的安全-实用性优化手段，能够调节干预范围以适应不同的应用需求，为大语言模型的可靠部署提供了新的可能性。

Abstract: Ensuring robust safety alignment while preserving utility is critical for the
reliable deployment of Large Language Models (LLMs). However, current
techniques fundamentally suffer from intertwined deficiencies: insufficient
robustness against malicious attacks, frequent refusal of benign queries,
degradation in generated text quality and general task performance--the former
two reflecting deficits in robust safety and the latter constituting utility
impairment. We trace these limitations to the coarse-grained layer-wise
interventions in existing methods. To resolve this, we propose NeuronTune, a
fine-grained framework that dynamically modulates sparse neurons to achieve
simultaneous safety-utility optimization. Our approach first identifies
safety-critical and utility-preserving neurons across all layers via
attribution, then employs meta-learning to adaptively amplify safety-neuron
activations and suppress utility-neuron activations. Crucially, NeuronTune
enables tunable adjustment of intervention scope via neuron-count thresholds,
supporting flexible adaptation to security-critical or utility-priority
scenarios. Extensive experimental results demonstrate that our method
significantly outperforms existing state-of-the-art technologies, achieving
superior model safety while maintaining excellent utility.

</details>


### [230] [Domain-Generalization to Improve Learning in Meta-Learning Algorithms](https://arxiv.org/abs/2508.09418)
*Usman Anjum,Chris Stockman,Cat Luong,Justin Zhan*

Main category: cs.LG

TL;DR: 提出了一种新的元学习算法DGS-MAML，用于在训练数据有限的情况下实现跨任务的泛化能力，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决任务训练数据有限的情况下如何实现高效的跨任务泛化能力。

Method: 结合梯度匹配与敏锐性感知最小化的双层优化框架，并通过PAC-Bayes和收敛性分析提供理论支持。

Result: 在基准数据集上的实验表明，DGS-MAML在准确性和泛化能力上优于现有方法。

Conclusion: DGS-MAML在少样本学习和快速适应场景中表现出潜力，并公开了源码。

Abstract: This paper introduces Domain Generalization Sharpness-Aware Minimization
Model-Agnostic Meta-Learning (DGS-MAML), a novel meta-learning algorithm
designed to generalize across tasks with limited training data. DGS-MAML
combines gradient matching with sharpness-aware minimization in a bi-level
optimization framework to enhance model adaptability and robustness. We support
our method with theoretical analysis using PAC-Bayes and convergence
guarantees. Experimental results on benchmark datasets show that DGS-MAML
outperforms existing approaches in terms of accuracy and generalization. The
proposed method is particularly useful for scenarios requiring few-shot
learning and quick adaptation, and the source code is publicly available at
GitHub.

</details>


### [231] [Implicit Hypergraph Neural Networks: A Stable Framework for Higher-Order Relational Learning with Provable Guarantees](https://arxiv.org/abs/2508.09427)
*Xiaoyu Li,Guangyu Tang,Jiaojiao Jiang*

Main category: cs.LG

TL;DR: IHGNN提出了隐式超图神经网络，在不依赖深层网络架构的情况下，以非线性固定点方程求解，实现高效稳定的超边全局传播，实验表明其在精度和鲁棒性方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有超图神经网络存在深度限制的问题，难以捕获长距离依赖，同时训练深层模型可能出现不稳定性，因此需要一种新方法改善这些问题。

Method: 提出IHGNN，利用非线性固定点方程计算表示，避免多层网络带来的问题，并设计了一个具有收敛性的训练方案，采用隐式梯度训练和基于投影的稳定化策略。

Result: 在引用基准测试上，IHGNN的精度和鲁棒性均优于强传统基线，且对随机初始化和超参数变化表现出强的适应能力。

Conclusion: IHGNN是一种有效的超图神经网络方法，具备出色的长距离依赖捕获能力、稳定性和实用价值，为高阶关系学习提供了一种新的方向。

Abstract: Many real-world interactions are group-based rather than pairwise such as
papers with multiple co-authors and users jointly engaging with items.
Hypergraph neural networks have shown great promise at modeling higher-order
relations, but their reliance on a fixed number of explicit message-passing
layers limits long-range dependency capture and can destabilize training as
depth grows. In this work, we introduce Implicit Hypergraph Neural Networks
(IHGNN), which bring the implicit equilibrium formulation to hypergraphs:
instead of stacking layers, IHGNN computes representations as the solution to a
nonlinear fixed-point equation, enabling stable and efficient global
propagation across hyperedges without deep architectures. We develop a
well-posed training scheme with provable convergence, analyze the oversmoothing
conditions and expressivity of the model, and derive a transductive
generalization bound on hypergraphs. We further present an implicit-gradient
training procedure coupled with a projection-based stabilization strategy.
Extensive experiments on citation benchmarks show that IHGNN consistently
outperforms strong traditional graph/hypergraph neural network baselines in
both accuracy and robustness. Empirically, IHGNN is resilient to random
initialization and hyperparameter variation, highlighting its strong
generalization and practical value for higher-order relational learning.

</details>


### [232] [NEXICA: Discovering Road Traffic Causality (Extended arXiv Version)](https://arxiv.org/abs/2508.09447)
*Siddharth Srikanth,John Krumm,Jonathan Qin*

Main category: cs.LG

TL;DR: 开发了一种名为NEXICA的算法，通过时间序列数据发现高速公路中引发交通缓慢的关键部位，并在准确性和速度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决高速公路交通拥堵问题，准确识别造成交通缓慢的关键部位，从而减少拥堵。

Method: 提出了专注于事件出现的时间序列因果发现算法NEXICA，使用最大似然估计的概率模型分析自发与引发的交通缓慢，并通过二分类器对因果位置对进行分类。

Result: 在洛杉矶地区195个高速公路速度传感器的六个月数据上测试，结果显示算法在准确性和计算速度上优于现有方法。

Conclusion: NEXICA算法能够更精确高效地识别交通缓慢原因，为解决高速公路交通拥堵提供了新思路。

Abstract: Road traffic congestion is a persistent problem. Focusing resources on the
causes of congestion is a potentially efficient strategy for reducing
slowdowns. We present NEXICA, an algorithm to discover which parts of the
highway system tend to cause slowdowns on other parts of the highway. We use
time series of road speeds as inputs to our causal discovery algorithm. Finding
other algorithms inadequate, we develop a new approach that is novel in three
ways. First, it concentrates on just the presence or absence of events in the
time series, where an event indicates the temporal beginning of a traffic
slowdown. Second, we develop a probabilistic model using maximum likelihood
estimation to compute the probabilities of spontaneous and caused slowdowns
between two locations on the highway. Third, we train a binary classifier to
identify pairs of cause/effect locations trained on pairs of road locations
where we are reasonably certain a priori of their causal connections, both
positive and negative. We test our approach on six months of road speed data
from 195 different highway speed sensors in the Los Angeles area, showing that
our approach is superior to state-of-the-art baselines in both accuracy and
computation speed.

</details>


### [233] [A Unified Contrastive-Generative Framework for Time Series Classification](https://arxiv.org/abs/2508.09451)
*Ziyu Liu,Azadeh Alavi,Minyi Li,Xiang Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为CoGenT的框架，将对比学习和生成学习整合在一起，用于多变量时间序列的自监督学习中，并在六个数据集上展现出显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前的对比学习方法能很好地进行实例区分，但对高类内相似性的时间数据敏感；生成方法能够建模数据分布，却依赖于大数据集。两者优劣互补，但尚未联合研究。

Method: 提出一个名为CoGenT的对比生成时间序列框架，通过联合对比-生成优化，结合两种学习范式的优势，同时减少各自的不足。

Result: CoGenT在六个多样化的时间序列数据集上表现出色，相较于独立的SimCLR和MAE，F1指标分别提高了59.2%和14.27%。

Conclusion: 结果验证了对比生成混合目标能够兼具判别能力与生成鲁棒性，为时间序列领域的混合自监督学习奠定了基础。

Abstract: Self-supervised learning (SSL) for multivariate time series mainly includes
two paradigms: contrastive methods that excel at instance discrimination and
generative approaches that model data distributions. While effective
individually, their complementary potential remains unexplored. We propose a
Contrastive Generative Time series framework (CoGenT), the first framework to
unify these paradigms through joint contrastive-generative optimization. CoGenT
addresses fundamental limitations of both approaches: it overcomes contrastive
learning's sensitivity to high intra-class similarity in temporal data while
reducing generative methods' dependence on large datasets. We evaluate CoGenT
on six diverse time series datasets. The results show consistent improvements,
with up to 59.2% and 14.27% F1 gains over standalone SimCLR and MAE,
respectively. Our analysis reveals that the hybrid objective preserves
discriminative power while acquiring generative robustness. These findings
establish a foundation for hybrid SSL in temporal domains. We will release the
code shortly.

</details>


### [234] [Open-Set Fault Diagnosis in Multimode Processes via Fine-Grained Deep Feature Representation](https://arxiv.org/abs/2508.09462)
*Guangqiang Li,M. Amine Atoui,Xiangshun Li*

Main category: cs.LG

TL;DR: 提出了一种称为细粒度聚类与拒绝网络（FGCRN）的新模型，旨在解决多模式过程中的故障诊断问题，并显著提升了未知故障的识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有故障诊断系统难以同时准确地分类已知健康状态和识别未知故障，尤其是在多模式过程样本分布复杂的情况下。

Method: 提出了一种结合多尺度深度卷积、双向门控循环单元和时间注意力机制的新模型FGCRN，同时设计了一种基于距离的损失函数，并通过无监督学习构造细粒度特征表示，使用极值理论建模特征与细粒度特征之间的距离以识别未知故障。

Result: 实验结果表明，所提出的方法在故障诊断表现上具有显著优势。

Conclusion: 通过改进特征提取与距离建模方法，FGCRN模型在已知故障分类和未知故障识别中均表现出色，可提升多模式过程中的故障诊断能力。

Abstract: A reliable fault diagnosis system should not only accurately classify known
health states but also effectively identify unknown faults. In multimode
processes, samples belonging to the same health state often show multiple
cluster distributions, making it difficult to construct compact and accurate
decision boundaries for that state. To address this challenge, a novel open-set
fault diagnosis model named fine-grained clustering and rejection network
(FGCRN) is proposed. It combines multiscale depthwise convolution,
bidirectional gated recurrent unit and temporal attention mechanism to capture
discriminative features. A distance-based loss function is designed to enhance
the intra-class compactness. Fine-grained feature representations are
constructed through unsupervised learning to uncover the intrinsic structures
of each health state. Extreme value theory is employed to model the distance
between sample features and their corresponding fine-grained representations,
enabling effective identification of unknown faults. Extensive experiments
demonstrate the superior performance of the proposed method.

</details>


### [235] [Learn to Explore: Meta NAS via Bayesian Optimization Guided Graph Generation](https://arxiv.org/abs/2508.09467)
*Zijun Sun,Yanning Shen*

Main category: cs.LG

TL;DR: 本文提出了一种新的Meta-NAS框架GraB-NAS，通过图形化建模神经结构，并结合贝叶斯优化和梯度上升混合搜索策略，提升了任务感知架构的性能以及超越预定义搜索空间的能力。


<details>
  <summary>Details</summary>
Motivation: 传统的NAS方法局限于单一任务，使其在实际应用中适用性受限。Meta-NAS通过利用跨任务的先验知识，实现对新任务的快速适应，但现有方法存在泛化能力差、搜索空间有限和计算成本高的问题。

Method: GraB-NAS将神经网络架构建模为图，提出了一个混合搜索策略：通过贝叶斯优化进行全局架构搜索，同时在潜在空间通过梯度上升进行局部探索，生成新的图形化架构。

Result: 实验显示，GraB-NAS优于现有的Meta-NAS方法，表现出更好的泛化能力和搜索效果。

Conclusion: GraB-NAS在任务感知架构搜索中展现出了强大的性能和广泛的适用性，能够超越预定义搜索空间，适应多任务需求。

Abstract: Neural Architecture Search (NAS) automates the design of high-performing
neural networks but typically targets a single predefined task, thereby
restricting its real-world applicability. To address this, Meta Neural
Architecture Search (Meta-NAS) has emerged as a promising paradigm that
leverages prior knowledge across tasks to enable rapid adaptation to new ones.
Nevertheless, existing Meta-NAS methods often struggle with poor
generalization, limited search spaces, or high computational costs. In this
paper, we propose a novel Meta-NAS framework, GraB-NAS. Specifically, GraB-NAS
first models neural architectures as graphs, and then a hybrid search strategy
is developed to find and generate new graphs that lead to promising neural
architectures. The search strategy combines global architecture search via
Bayesian Optimization in the search space with local exploration for novel
neural networks via gradient ascent in the latent space. Such a hybrid search
strategy allows GraB-NAS to discover task-aware architectures with strong
performance, even beyond the predefined search space. Extensive experiments
demonstrate that GraB-NAS outperforms state-of-the-art Meta-NAS baselines,
achieving better generalization and search effectiveness.

</details>


### [236] [DeepFeatIoT: Unifying Deep Learned, Randomized, and LLM Features for Enhanced IoT Time Series Sensor Data Classification in Smart Industries](https://arxiv.org/abs/2508.09468)
*Muhammad Sakib Khan Inan,Kewen Liao*

Main category: cs.LG

TL;DR: DeepFeatIoT是一种融合了多种特征（包括局部和全局特征、随机卷积核特征和大语言模型特征）的深度学习模型，用于提高IoT时间序列数据分类，即使在标注数据有限的情况下也能表现优异。


<details>
  <summary>Details</summary>
Motivation: 由于IoT传感器数据中存在元数据丢失、数据源异质性、不一致的时间戳等问题，导致原始数据难以解释，削弱了智能系统的有效性。

Method: 提出了一种深度学习模型DeepFeatIoT，通过融合学到的局部和全局特征、非学到的随机卷积核特征及大语言模型特征，来提高IoT时间序列传感器数据的分类性能。

Result: DeepFeatIoT在多个来自不同关键应用领域的真实IoT传感器数据集上表现出了一致的性能，优于现有的最先进模型。

Conclusion: DeepFeatIoT有潜力显著推动IoT数据分析的发展，并支持下一代智能系统的开发。

Abstract: Internet of Things (IoT) sensors are ubiquitous technologies deployed across
smart cities, industrial sites, and healthcare systems. They continuously
generate time series data that enable advanced analytics and automation in
industries. However, challenges such as the loss or ambiguity of sensor
metadata, heterogeneity in data sources, varying sampling frequencies,
inconsistent units of measurement, and irregular timestamps make raw IoT time
series data difficult to interpret, undermining the effectiveness of smart
systems. To address these challenges, we propose a novel deep learning model,
DeepFeatIoT, which integrates learned local and global features with
non-learned randomized convolutional kernel-based features and features from
large language models (LLMs). This straightforward yet unique fusion of diverse
learned and non-learned features significantly enhances IoT time series sensor
data classification, even in scenarios with limited labeled data. Our model's
effectiveness is demonstrated through its consistent and generalized
performance across multiple real-world IoT sensor datasets from diverse
critical application domains, outperforming state-of-the-art benchmark models.
These results highlight DeepFeatIoT's potential to drive significant
advancements in IoT analytics and support the development of next-generation
smart systems.

</details>


### [237] [EGGS-PTP: An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models](https://arxiv.org/abs/2508.09471)
*Omar Bazarbachi,Zijun Sun,Yanning Shen*

Main category: cs.LG

TL;DR: 提出了一种针对LLMs的Expander-Graph Guided Structured Post-training Pruning (EGGS-PTP)方法，用于通过结构化裁剪减小模型大小和计算需求，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的广泛应用，其计算和内存需求问题日益严峻，因此需要开发更高效的模型变体。

Method: 利用图论设计N:M结构化裁剪方法，采用展开图的概念来引导裁剪，保证经过裁剪后的网络仍保持信息流和功能完整性。

Result: 实验结果表明，EGGS-PTP方法显著减少了计算和内存需求，并且在保持稀疏性加速的同时，在多种LLMs上的精度优于现有裁剪技术。

Conclusion: EGGS-PTP是一种高效的后训练裁剪方法，不仅能显著节省资源，还能提升模型在裁剪后的性能，具有潜在的实用价值。

Abstract: As Large Language Models (LLMs) become more widely adopted and scale up in
size, the computational and memory challenges involved in deploying these
massive foundation models have grown increasingly severe. This underscores the
urgent need to develop more efficient model variants. Faced with this
challenge, the present work introduces EGGS-PTP: an Expander-Graph Guided
Structured Post-training Pruning method. The proposed approach leverages graph
theory to guide the design of N:M structured pruning, effectively reducing
model size and computational demands. By incorporating concepts from expander
graphs, EGGS-PTP ensures information flow within the pruned network, preserving
essential model functionality. Extensive numerical experiments demonstrate that
EGGS-PTP not only achieves significant acceleration and memory savings due to
structured sparsity but also outperforms existing structured pruning techniques
in terms of accuracy across various LLMs.

</details>


### [238] [Large-Small Model Collaborative Framework for Federated Continual Learning](https://arxiv.org/abs/2508.09489)
*Hao Yu,Xin Yang,Boyang Fan,Xuemei Cao,Hanlin Gu,Lixin Fan,Qiang Yang*

Main category: cs.LG

TL;DR: 本文探讨联邦持续学习中，结合轻量模型与基础模型的协作框架，以应对基础模型在本地任务中表现欠佳的问题，并提出两项创新技术，达到优异的实验效果。


<details>
  <summary>Details</summary>
Motivation: 克服基础模型在本地私有数据上的低效问题，同时解决其更新新任务时容易遗忘的问题，旨在优化联邦持续学习场景下的任务性能。

Method: 提出首个FCL协作框架，利用轻量模型作为动态桥梁适应新任务，并提出两个新技术：小模型的持续微调防止遗忘，以及逐一蒸馏实现本地知识的个性化融合。

Result: 实验结果表明，即使在客户使用异构小模型的情况下，该方法性能优异。

Conclusion: 通过将轻量模型与基础模型结合，解决了联邦持续学习中基础模型适应性和遗忘性问题，为联邦持续学习提供了新的方向和基础。

Abstract: Continual learning (CL) for Foundation Models (FMs) is an essential yet
underexplored challenge, especially in Federated Continual Learning (FCL),
where each client learns from a private, evolving task stream under strict data
and communication constraints. Despite their powerful generalization abilities,
FMs often exhibit suboptimal performance on local downstream tasks, as they are
unable to utilize private local data. Furthermore, enabling FMs to learn new
tasks without forgetting prior knowledge is inherently a challenging problem,
primarily due to their immense parameter count and high model complexity. In
contrast, small models can be trained locally under resource-constrained
conditions and benefit from more mature CL techniques. To bridge the gap
between small models and FMs, we propose the first collaborative framework in
FCL, where lightweight local models act as a dynamic bridge, continually
adapting to new tasks while enhancing the utility of the large model. Two novel
components are also included: Small Model Continual Fine-tuning is for
preventing small models from temporal forgetting; One-by-One Distillation
performs personalized fusion of heterogeneous local knowledge on the server.
Experimental results demonstrate its superior performance, even when clients
utilize heterogeneous small models.

</details>


### [239] [MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI](https://arxiv.org/abs/2508.09500)
*Zijun Jiang,Yangdi Lyu*

Main category: cs.LG

TL;DR: 提出MiCo框架，一种面向边缘AI的层级混合精度量化(MPQ)优化和部署框架，优化模型的准确性和延迟。


<details>
  <summary>Details</summary>
Motivation: 探索效率更高和灵活的MPQ方案以解决现有方法在量化精度、延迟优化和边缘设备部署中的局限性。

Method: 提出采用新的优化算法搜索最优量化方案，构建硬件感知延迟模型，结合PyTorch直接部署为Bare-metal C代码实现加速。

Result: 框架能够在满足延迟限制的同时实现最高的精度，并从端到端加速，且仅带来最小的精度损失。

Conclusion: MiCo框架解决了现有MPQ探索和部署效率低的问题，为边缘AI提供了一种完整的优化和部署解决方案。

Abstract: Quantized Neural Networks (QNN) with extremely low-bitwidth data have proven
promising in efficient storage and computation on edge devices. To further
reduce the accuracy drop while increasing speedup, layer-wise mixed-precision
quantization (MPQ) becomes a popular solution. However, existing algorithms for
exploring MPQ schemes are limited in flexibility and efficiency. Comprehending
the complex impacts of different MPQ schemes on post-training quantization and
quantization-aware training results is a challenge for conventional methods.
Furthermore, an end-to-end framework for the optimization and deployment of MPQ
models is missing in existing work.
  In this paper, we propose the MiCo framework, a holistic MPQ exploration and
deployment framework for edge AI applications. The framework adopts a novel
optimization algorithm to search for optimal quantization schemes with the
highest accuracies while meeting latency constraints. Hardware-aware latency
models are built for different hardware targets to enable fast explorations.
After the exploration, the framework enables direct deployment from PyTorch MPQ
models to bare-metal C codes, leading to end-to-end speedup with minimal
accuracy drops.

</details>


### [240] [Causal Graph Profiling via Structural Divergence for Robust Anomaly Detection in Cyber-Physical Systems](https://arxiv.org/abs/2508.09504)
*Arun Vignesh Malarkkan,Haoyue Bai,Dongjie Wang,Yanjie Fu*

Main category: cs.LG

TL;DR: 介绍了一种基于因果图的异常检测框架CGAD，用于提高公共基础设施系统中对复杂网络攻击的检测能力。


<details>
  <summary>Details</summary>
Motivation: 随着对关键基础设施的网络攻击日趋复杂化，现有的异常检测方法容易受到分布偏移和类别不平衡的影响，需要一种更鲁棒的方法解决这些问题。

Method: 提出了CGAD，包括因果剖析和异常评分两个阶段：首先利用动态贝叶斯网络学习系统在“正常”和“攻击”状态下的因果不变图结构，然后通过评估随时间变化的因果图结构差异检测异常。

Result: CGAD在数据不平衡和非平稳环境中的准确性和适应性优于传统机器学习方法，并在四个工业数据集上取得了明显优于其他方法的F1和ROC-AUC分数。

Conclusion: 通过揭示传感器数据下的因果结构，CGAD不仅能够更精确地检测网络攻击，还能在传统模型因数据平衡性和漂移问题失效时展示出强大的鲁棒性。

Abstract: With the growing complexity of cyberattacks targeting critical
infrastructures such as water treatment networks, there is a pressing need for
robust anomaly detection strategies that account for both system
vulnerabilities and evolving attack patterns. Traditional methods --
statistical, density-based, and graph-based models struggle with distribution
shifts and class imbalance in multivariate time series, often leading to high
false positive rates. To address these challenges, we propose CGAD, a Causal
Graph-based Anomaly Detection framework designed for reliable cyberattack
detection in public infrastructure systems. CGAD follows a two-phase supervised
framework -- causal profiling and anomaly scoring. First, it learns causal
invariant graph structures representing the system's behavior under "Normal"
and "Attack" states using Dynamic Bayesian Networks. Second, it employs
structural divergence to detect anomalies via causal graph comparison by
evaluating topological deviations in causal graphs over time. By leveraging
causal structures, CGAD achieves superior adaptability and accuracy in
non-stationary and imbalanced time series environments compared to conventional
machine learning approaches. By uncovering causal structures beneath volatile
sensor data, our framework not only detects cyberattacks with markedly higher
precision but also redefines robustness in anomaly detection, proving
resilience where traditional models falter under imbalance and drift. Our
framework achieves substantial gains in F1 and ROC-AUC scores over
best-performing baselines across four industrial datasets, demonstrating robust
detection of delayed and structurally complex anomalies.

</details>


### [241] [Enhancing Memory Recall in LLMs with Gauss-Tin: A Hybrid Instructional and Gaussian Replay Approach](https://arxiv.org/abs/2508.09510)
*Iing Muttakhiroh,Thomas Fevens*

Main category: cs.LG

TL;DR: 本文提出Gauss-Tin方法，通过结合重放策略和高斯混合模型，可以在处理新信息的同时保持LLMs已学知识，实验表明保留能力提高6%。


<details>
  <summary>Details</summary>
Motivation: 现有的LLMs在学习新信息时会显著遗忘已有知识，因此需要开发解决灾难性遗忘问题的有效策略。

Method: 结合重放策略与高斯混合模型优化示例的选择，同时采用指导性生成机制帮助复习过去的学习内容。

Result: 实验结果显示，与传统方法相比，Gauss-Tin在保留能力指标上提升了6%。

Conclusion: Gauss-Tin策略有效缓解了LLMs的灾难性遗忘问题，验证了混合模型在动态学习环境下增强LLMs的适应性和稳健性的潜力。

Abstract: Despite the significant advancements in Large Language Models (LLMs),
catastrophic forgetting remains a substantial challenge, where models lose
previously acquired knowledge upon learning new information. Continual learning
(CL) strategies have emerged as a potential solution to this problem, with
replay-based techniques demonstrating superior performance in preserving
learned knowledge. In this context, we introduce Gauss-Tin, a novel approach
that integrates the replay strategy with a Gaussian mixture model to enhance
the quality of sample selection during training, supplemented by instructional
guidance to facilitate the generation of past learning. This method aims to
improve LLMs' retention capabilities by strategically reinforcing important
past learnings while accommodating new information. Our experimental results
indicate a promising 6\% improvement in retention metrics over traditional
methods, suggesting that Gauss-Tin is an effective strategy for mitigating
catastrophic forgetting in LLMs. This study underscores the potential of hybrid
models in enhancing the robustness and adaptability of LLMs in dynamic learning
environments.

</details>


### [242] [Time-Aware and Transition-Semantic Graph Neural Networks for Interpretable Predictive Business Process Monitoring](https://arxiv.org/abs/2508.09527)
*Fang Wang,Ernesto Damiani*

Main category: cs.LG

TL;DR: 本文引入了一种统一的GNN框架，结合时间衰减机制和边特征语义等，解决了PBPM中局部化和全局建模的不足问题，在预测业务流程中的表现更稳健且可解释性更强。


<details>
  <summary>Details</summary>
Motivation: 现有GNN模型要么局限于前缀子图，要么忽略流程中时间相关性，难以有效捕获流程数据的关键特性，因此需要提升其在PBPM中的表现。

Method: 提出一种统一的GNN框架，通过比较前缀式GCN模型和完整轨迹的GAT模型，构建时间衰减注意力机制，并嵌入边特征语义，提升建模的全面性和可解释性。

Result: 在五个基准数据集上验证，模型在准确性和DL评分上表现出竞争力，无需针对特定数据集进行调优。

Conclusion: 本文提出的方法解决了架构、时间和语义上的关键空白，在PBPM的下一事件预测任务上提供了一种稳健、通用且可解释性强的解决方案。

Abstract: Predictive Business Process Monitoring (PBPM) aims to forecast future events
in ongoing cases based on historical event logs. While Graph Neural Networks
(GNNs) are well suited to capture structural dependencies in process data,
existing GNN-based PBPM models remain underdeveloped. Most rely either on short
prefix subgraphs or global architectures that overlook temporal relevance and
transition semantics. We propose a unified, interpretable GNN framework that
advances the state of the art along three key axes. First, we compare
prefix-based Graph Convolutional Networks(GCNs) and full trace Graph Attention
Networks(GATs) to quantify the performance gap between localized and global
modeling. Second, we introduce a novel time decay attention mechanism that
constructs dynamic, prediction-centered windows, emphasizing temporally
relevant history and suppressing noise. Third, we embed transition type
semantics into edge features to enable fine grained reasoning over structurally
ambiguous traces. Our architecture includes multilevel interpretability
modules, offering diverse visualizations of attention behavior. Evaluated on
five benchmarks, the proposed models achieve competitive Top-k accuracy and DL
scores without per-dataset tuning. By addressing architectural, temporal, and
semantic gaps, this work presents a robust, generalizable, and explainable
solution for next event prediction in PBPM.

</details>


### [243] [Decentralized Rank Scheduling for Energy-Constrained Multi-Task Federated Fine-Tuning in Edge-Assisted IoV Networks](https://arxiv.org/abs/2508.09532)
*Bokeng Zheng,Jianqiang Zhong,Jiayi Liu,Xiaoxi Zhang*

Main category: cs.LG

TL;DR: 提出了一种针对动态车联网场景的分层联邦微调框架，通过利用LoRA技术和UCB-DUAL算法，实现了较高效且低延迟的多任务适配。


<details>
  <summary>Details</summary>
Motivation: 在因设备差异与连接不稳定的车联网环境中，实现低延迟和高效的多任务学习非常具有挑战性。

Method: 设计了一个分层联邦微调框架，结合LoRA技术，提出了一个能量感知的秩适配机制，并将其建模为约束多臂赌博问题，使用新开发的UCB-DUAL算法进行优化。

Result: 在大规模仿真器测试中，该方法在延迟和准确性的权衡中表现最优，延迟降低超过24%，准确率提高超过2.5%。

Conclusion: 该框架为车联网系统中资源受限和动态参与的场景提供了一种有效的解决方案。

Abstract: Federated fine-tuning has emerged as a promising approach for adapting
foundation models (FMs) to diverse downstream tasks in edge environments. In
Internet of Vehicles (IoV) systems, enabling efficient and low-latency
multi-task adaptation is particularly challenging due to client mobility,
heterogeneous resources, and intermittent connectivity. This paper proposes a
hierarchical federated fine-tuning framework that coordinates roadside units
(RSUs) and vehicles to support resource-aware and mobility-resilient learning
across dynamic IoV scenarios. Leveraging Low-Rank Adaptation (LoRA), we
introduce a decentralized, energy-aware rank adaptation mechanism formulated as
a constrained multi-armed bandit problem. A novel UCB-DUAL algorithm is
developed to enable adaptive exploration under per-task energy budgets,
achieving provable sublinear regret. To evaluate our method, we construct a
large-scale IoV simulator based on real-world trajectories, capturing dynamic
participation, RSU handoffs, and communication variability. Extensive
experiments show that our approach achieves the best accuracy-efficiency
trade-off among all baselines, reducing latency by over 24\% and improving
average accuracy by more than 2.5\%.

</details>


### [244] [SYNAPSE-G: Bridging Large Language Models and Graph Learning for Rare Event Classification](https://arxiv.org/abs/2508.09544)
*Sasan Tavakkol,Lin Chen,Max Springer,Abigail Schantz,Blaž Bratanič,Vincent Cohen-Addad,MohammadHossein Bateni*

Main category: cs.LG

TL;DR: 提出SYNAPSE-G，一种利用大型语言模型生成合成数据并结合图传播方法解决罕见事件分类中数据不足问题。


<details>
  <summary>Details</summary>
Motivation: 罕见事件的分类由于训练数据不足且难以获取，阻碍了机器学习模型的有效训练，因此需要一种方法解决冷启动问题。

Method: 通过利用大型语言模型（LLMs）生成合成罕见事件训练数据，并在种子数据与大规模未标注数据的相似图上执行标签传播，结合人工或模型判定标签，扩展数据集以训练分类器。

Result: 在不平衡的SST2和MHS数据集上的实验表明，SYNAPSE-G在罕见正标签发现方面优于包括最近邻搜索在内的基线方法。

Conclusion: SYNAPSE-G通过生成高质量合成数据并结合半监督方法，有效解决了数据稀缺情况下的模型冷启动问题。

Abstract: Scarcity of labeled data, especially for rare events, hinders training
effective machine learning models. This paper proposes SYNAPSE-G (Synthetic
Augmentation for Positive Sampling via Expansion on Graphs), a novel pipeline
leveraging Large Language Models (LLMs) to generate synthetic training data for
rare event classification, addressing the cold-start problem. This synthetic
data serve as seeds for semi-supervised label propagation on a similarity graph
constructed between the seeds and a large unlabeled dataset. This identifies
candidate positive examples, subsequently labeled by an oracle (human or LLM).
The expanded dataset then trains/fine-tunes a classifier. We theoretically
analyze how the quality (validity and diversity) of the synthetic data impacts
the precision and recall of our method. Experiments on the imbalanced SST2 and
MHS datasets demonstrate SYNAPSE-G's effectiveness in finding positive labels,
outperforming baselines including nearest neighbor search.

</details>


### [245] [Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges](https://arxiv.org/abs/2508.09561)
*Changyuan Zhao,Guangyuan Liu,Ruichen Zhang,Yinqiu Liu,Jiacheng Wang,Jiawen Kang,Dusit Niyato,Zan Li,Xuemin,Shen,Zhu Han,Sumei Sun,Chau Yuen,Dong In Kim*

Main category: cs.LG

TL;DR: 此论文探讨了边缘通用智能（EGI）及其核心——世界模型——在边缘计算中的作用，提供了对相关架构、应用及未来研究方向的全面分析。


<details>
  <summary>Details</summary>
Motivation: 边缘计算的发展需要更智能、高效的计算模式，而通过世界模型实现的边缘通用智能（EGI）可使边缘代理具备感知、推理和自主行动能力，提升复杂环境中的优化能力。

Method: 研究世界模型的关键架构，包括潜在表征学习、动态建模和基于想象的规划；分析其在边缘场景（如车联网、无人机网络、物联网、虚拟化网络功能）中的优化应用；探讨它与基础模型和数字孪生的结合潜力。

Result: 论文展示了世界模型在多个边缘场景中的优化能力，强调了其能够在延迟、能耗和隐私约束下实现性能提升的潜力，同时指出了研究中的开放挑战。

Conclusion: 世界模型是实现边缘通用智能的认知核心，未来研究应关注安全性、高效训练和受限部署等方向，为下一代智能、自主边缘系统奠定基础。

Abstract: Edge General Intelligence (EGI) represents a transformative evolution of edge
computing, where distributed agents possess the capability to perceive, reason,
and act autonomously across diverse, dynamic environments. Central to this
vision are world models, which act as proactive internal simulators that not
only predict but also actively imagine future trajectories, reason under
uncertainty, and plan multi-step actions with foresight. This proactive nature
allows agents to anticipate potential outcomes and optimize decisions ahead of
real-world interactions. While prior works in robotics and gaming have
showcased the potential of world models, their integration into the wireless
edge for EGI remains underexplored. This survey bridges this gap by offering a
comprehensive analysis of how world models can empower agentic artificial
intelligence (AI) systems at the edge. We first examine the architectural
foundations of world models, including latent representation learning, dynamics
modeling, and imagination-based planning. Building on these core capabilities,
we illustrate their proactive applications across EGI scenarios such as
vehicular networks, unmanned aerial vehicle (UAV) networks, the Internet of
Things (IoT) systems, and network functions virtualization, thereby
highlighting how they can enhance optimization under latency, energy, and
privacy constraints. We then explore their synergy with foundation models and
digital twins, positioning world models as the cognitive backbone of EGI.
Finally, we highlight open challenges, such as safety guarantees, efficient
training, and constrained deployment, and outline future research directions.
This survey provides both a conceptual foundation and a practical roadmap for
realizing the next generation of intelligent, autonomous edge systems.

</details>


### [246] [Online Prediction with Limited Selectivity](https://arxiv.org/abs/2508.09592)
*Licheng Liu,Mingda Qiao*

Main category: cs.LG

TL;DR: 本文探讨了预测选择的限制条件下预测误差的优化问题，并引入了相应的复杂性度量。


<details>
  <summary>Details</summary>
Motivation: 分析在预测选择受到限制时，如何优化预测误差并设计算法。

Method: 提出了一种预测有限选择性（PLS）的模型，并从实例依赖和均值情况两方面进行预测误差的分析，提出复杂性度量用以界定误差边界。

Result: 研究成果表明，随机生成的PLS实例中，复杂性度量所给出的误差界限在高概率下与实际相符。

Conclusion: 通过实例分析及均值研究揭示了在预测选择有限时，优化预测误差的可能路径，并验证了复杂性度量的可行性。

Abstract: Selective prediction [Dru13, QV19] models the scenario where a forecaster
freely decides on the prediction window that their forecast spans. Many data
statistics can be predicted to a non-trivial error rate without any
distributional assumptions or expert advice, yet these results rely on that the
forecaster may predict at any time. We introduce a model of Prediction with
Limited Selectivity (PLS) where the forecaster can start the prediction only on
a subset of the time horizon. We study the optimal prediction error both on an
instance-by-instance basis and via an average-case analysis. We introduce a
complexity measure that gives instance-dependent bounds on the optimal error.
For a randomly-generated PLS instance, these bounds match with high
probability.

</details>


### [247] [Goal Discovery with Causal Capacity for Efficient Reinforcement Learning](https://arxiv.org/abs/2508.09624)
*Yan Yu,Yaodong Yang,Zhengbo Lu,Chengdong Ma,Wengang Zhou,Houqiang Li*

Main category: cs.LG

TL;DR: 提出了一种基于因果能力的目标发现框架（GDCC），通过测量状态空间中的因果能力来引导强化学习中的有效探索，在多目标任务中表现出显著的成功率提升。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，建立动作与状态转移之间的因果关系可以增强代理推理其策略对未来轨迹影响的能力，从而促进目标导向的探索。然而，在复杂场景的庞大状态-动作空间中，因果关系的测量具有挑战性。

Method: 提出了一种基于因果能力的目标发现框架（GDCC），其中通过测量状态空间中的因果能力来表示代理行为对未来轨迹的最大影响力，结合蒙特卡罗方法识别离散状态空间中的关键点，并优化方法以适用于连续高维环境。这些关键点被视为子目标，指导代理更加高效地探索环境。

Result: 通过多目标任务的实验表明，高因果能力的状态与预期子目标一致，并且GDCC框架与基线相比在成功率上有显著提升。

Conclusion: GDCC框架能够有效利用因果能力指标发现环境中重要的探索子目标，显著提高了强化学习代理在多目标任务中的探索效率和成功率。

Abstract: Causal inference is crucial for humans to explore the world, which can be
modeled to enable an agent to efficiently explore the environment in
reinforcement learning. Existing research indicates that establishing the
causality between action and state transition will enhance an agent to reason
how a policy affects its future trajectory, thereby promoting directed
exploration. However, it is challenging to measure the causality due to its
intractability in the vast state-action space of complex scenarios. In this
paper, we propose a novel Goal Discovery with Causal Capacity (GDCC) framework
for efficient environment exploration. Specifically, we first derive a
measurement of causality in state space, \emph{i.e.,} causal capacity, which
represents the highest influence of an agent's behavior on future trajectories.
After that, we present a Monte Carlo based method to identify critical points
in discrete state space and further optimize this method for continuous
high-dimensional environments. Those critical points are used to uncover where
the agent makes important decisions in the environment, which are then regarded
as our subgoals to guide the agent to make exploration more purposefully and
efficiently. Empirical results from multi-objective tasks demonstrate that
states with high causal capacity align with our expected subgoals, and our GDCC
achieves significant success rate improvements compared to baselines.

</details>


### [248] [Physics- and geometry-aware spatio-spectral graph neural operator for time-independent and time-dependent PDEs](https://arxiv.org/abs/2508.09627)
*Subhankar Sarkar,Souvik Chakraborty*

Main category: cs.LG

TL;DR: 提出了一种基于物理和几何感知的时空频谱图神经算子（πG-Sp²GNO），用于学习偏微分方程（PDE）的解算子，展示了在多种测试场景中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 解决复杂几何结构和有限标签数据背景下高效精确求解偏微分方程的难题。

Method: 改进Sp²GNO，通过引入几何感知功能和结合物理规律的新型损失函数（尤其用于时间相关问题），采用时空频谱架构实现多尺度学习。

Result: 在基准测试中表现出优越性能，适用于不同几何域以及时变或非时变问题，相较于现有物理知情神经算子算法更为高效。

Conclusion: πG-Sp²GNO证明了其在模拟-free环境中高效和准确的能力，特别是在复杂域和变动几何中，适合作为求解偏微分方程的新方法。

Abstract: Solving partial differential equations (PDEs) efficiently and accurately
remains a cornerstone challenge in science and engineering, especially for
problems involving complex geometries and limited labeled data. We introduce a
Physics- and Geometry- Aware Spatio-Spectral Graph Neural Operator
($\pi$G-Sp$^2$GNO) for learning the solution operators of time-independent and
time-dependent PDEs. The proposed approach first improves upon the recently
developed Sp$^2$GNO by enabling geometry awareness and subsequently exploits
the governing physics to learn the underlying solution operator in a
simulation-free setup. While the spatio-spectral structure present in the
proposed architecture allows multiscale learning, two separate strategies for
enabling geometry awareness is introduced in this paper. For time dependent
problems, we also introduce a novel hybrid physics informed loss function that
combines higher-order time-marching scheme with upscaled theory inspired
stochastic projection scheme. This allows accurate integration of the
physics-information into the loss function. The performance of the proposed
approach is illustrated on number of benchmark examples involving regular and
complex domains, variation in geometry during inference, and time-independent
and time-dependent problems. The results obtained illustrate the efficacy of
the proposed approach as compared to the state-of-the-art physics-informed
neural operator algorithms in the literature.

</details>


### [249] [TimeMKG: Knowledge-Infused Causal Reasoning for Multivariate Time Series Modeling](https://arxiv.org/abs/2508.09630)
*Yifei Sun,Junming Liu,Ding Wang,Yirong Chen,Xuefeng Yan*

Main category: cs.LG

TL;DR: TimeMKG 是一种结合文本和时间序列数据的多模态因果推理框架，通过大型语言模型解析变量语义，并构建多变量知识图谱来捕捉变量间关系。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列模型忽视了包含变量语义和数据描述的文本信息，这些信息对建模的稳健性和可解释性至关重要。

Method: 提出 TimeMKG 框架，利用大语言模型解析变量语义，构建多变量知识图谱，并通过双模态编码器分别处理语义提示和历史时间序列中的统计模式，通过跨模态注意力机制融合这些信息。

Result: 实验证明，结合变量知识显著提升了预测性能和泛化能力。

Conclusion: 将语义知识引入时间序列建模为预测和分类任务提供了有效的因果先验，提升了模型的可解释性和鲁棒性。

Abstract: Multivariate time series data typically comprises two distinct modalities:
variable semantics and sampled numerical observations. Traditional time series
models treat variables as anonymous statistical signals, overlooking the rich
semantic information embedded in variable names and data descriptions. However,
these textual descriptors often encode critical domain knowledge that is
essential for robust and interpretable modeling. Here we present TimeMKG, a
multimodal causal reasoning framework that elevates time series modeling from
low-level signal processing to knowledge informed inference. TimeMKG employs
large language models to interpret variable semantics and constructs structured
Multivariate Knowledge Graphs that capture inter-variable relationships. A
dual-modality encoder separately models the semantic prompts, generated from
knowledge graph triplets, and the statistical patterns from historical time
series. Cross-modality attention aligns and fuses these representations at the
variable level, injecting causal priors into downstream tasks such as
forecasting and classification, providing explicit and interpretable priors to
guide model reasoning. The experiment in diverse datasets demonstrates that
incorporating variable-level knowledge significantly improves both predictive
performance and generalization.

</details>


### [250] [Thermal Tracks: A Gaussian process-based framework for universal melting curve analysis enabling unconstrained hit identification in thermal proteome profiling experiments](https://arxiv.org/abs/2508.09659)
*Johannes F. Hevler,Shivam Verma,Mirat Soijtra,Carolyn R. Bertozzi*

Main category: cs.LG

TL;DR: Thermal Tracks是一个Python框架，用于克服传统热蛋白质组学(TPP)方法的局限性，特别是在分析复杂蛋白质热稳定性时。


<details>
  <summary>Details</summary>
Motivation: 现有TPP方法在处理非标准熔解曲线及受限显著性判断时存在瓶颈，需开发更灵活精确的分析工具。

Method: Thermal Tracks利用高斯过程(GP)模型和核函数替代传统Sigmoid假设，生成无偏差的统计分布以应对多样熔解曲线。

Result: 该框架适于分析复杂蛋白质热稳定性变化，尤其是非标准热曲线情况，如膜蛋白及相分离蛋白表现的复杂行为。

Conclusion: Thermal Tracks改进了TPP的分析能力，为蛋白质组热稳定性研究提供了开源、灵活的新工具。

Abstract: Thermal Tracks is a Python-based statistical framework for analyzing protein
thermal stability data that overcomes key limitations of existing thermal
proteome profiling (TPP) work-flows. Unlike standard approaches that assume
sigmoidal melting curves and are constrained by empirical null distributions
(limiting significant hits to approximately 5 % of data), Thermal Tracks uses
Gaussian Process (GP) models with squared-exponential kernels to flexibly model
any melting curve shape while generating unbiased null distributions through
kernel priors. This framework is particularly valuable for analyzing
proteome-wide perturbations that significantly alter protein thermal stability,
such as pathway inhibitions, genetic modifications, or environmental stresses,
where conventional TPP methods may miss biologically relevant changes due to
their statistical constraints. Furthermore, Thermal Tracks excels at analyzing
proteins with un-conventional melting profiles, including phase-separating
proteins and membrane proteins, which often exhibit complex, non-sigmoidal
thermal stability behaviors. Thermal Tracks is freely available from GitHub and
is implemented in Python, providing an accessible and flexible tool for
proteome-wide thermal profiling studies.

</details>


### [251] [Global Convergence Analysis of Vanilla Gradient Descent for Asymmetric Matrix Completion](https://arxiv.org/abs/2508.09685)
*Xu Zhang,Shuo Chen,Jinsheng Li,Xiangying Pang,Maoguo Gong*

Main category: cs.LG

TL;DR: 本文研究了非对称低秩矩阵补全问题，表明不带正则化的梯度下降算法仍可实现线性收敛，并提出了一种具有低计算成本的算法。


<details>
  <summary>Details</summary>
Motivation: 探讨在非对称低秩矩阵补全中去除正则化项对算法收敛性的影响。

Method: 利用光谱初始化并结合留一法的分析，证明普通梯度下降可高概率线性收敛，并在实践中对比了计算成本及性能。

Result: 证明了梯度下降的隐式正则化性质，算法在计算成本下降的同时完成性能与其他算法相当。

Conclusion: 去除正则化的梯度下降对于非对称低秩矩阵补全仍然高效，验证了其理论可行性及实际优越性。

Abstract: This paper investigates the asymmetric low-rank matrix completion problem,
which can be formulated as an unconstrained non-convex optimization problem
with a nonlinear least-squares objective function, and is solved via gradient
descent methods. Previous gradient descent approaches typically incorporate
regularization terms into the objective function to guarantee convergence.
However, numerical experiments and theoretical analysis of the gradient flow
both demonstrate that the elimination of regularization terms in gradient
descent algorithms does not adversely affect convergence performance. By
introducing the leave-one-out technique, we inductively prove that the vanilla
gradient descent with spectral initialization achieves a linear convergence
rate with high probability. Besides, we demonstrate that the balancing
regularization term exhibits a small norm during iterations, which reveals the
implicit regularization property of gradient descent. Empirical results show
that our algorithm has a lower computational cost while maintaining comparable
completion performance compared to other gradient descent algorithms.

</details>


### [252] [Temporal Anchoring in Deepening Embedding Spaces: Event-Indexed Projections, Drift, Convergence, and an Internal Computational Architecture](https://arxiv.org/abs/2508.09693)
*Faruk Alpay,Bugra Kilictas,Hamdi Alakkad*

Main category: cs.LG

TL;DR: 本文提出了一种基于操作理论的时间锚定框架，并通过数学证明支持其有效性。


<details>
  <summary>Details</summary>
Motivation: 希望在嵌入空间中实现基于漂移映射与事件索引块的时间锚定。

Method: 提出采用漂移映射、事件索引块和仿射投影的框架，并引入内部手稿计算机（MC）进行计算理论分析。

Result: 证明了多个相关数学定理，包括漂移投影收敛、嵌套锚点本体收敛及其鲁棒性变体。

Conclusion: 通过严密的数学证明支持该框架的理论可行性，特别是在注意力层中的应用条件与光滑性分析。

Abstract: We develop an operator-theoretic framework for temporal anchoring in
embedding spaces, modeled as drift maps interleaved with event-indexed blocks
culminating in affine projections. We provide complete proofs for a
variable-block contraction lemma (products of Lipschitz factors), a
drift--projection convergence theorem with explicit uniform-gap envelopes, and
ontological convergence under nested affine anchors with a robustness variant.
We formalize an internal Manuscript Computer (MC) whose computations are
defined purely by these operators and prove a rigorous finite-run equivalence
theorem (with perturbation bounds). For attention layers, we give a
self-contained proof that softmax is $1/2$-Lipschitz in $\ell_2$ and derive
sufficient layer-contraction conditions (orthogonal/non-orthogonal heads). All
floats are placed exactly where written; the manuscript uses only in-paper
pseudocode and appendix figures.

</details>


### [253] [Combating Noisy Labels via Dynamic Connection Masking](https://arxiv.org/abs/2508.09697)
*Xinlei Zhang,Fan Liu,Chuanyi Zhang,Fan Cheng,Yuhui Zheng*

Main category: cs.LG

TL;DR: 本文提出了一种动态连接遮蔽(DCM)机制，用于增强多层感知器(MLP)和Kolmogorov-Arnold网络(KAN)在处理含噪标签数据时的抗噪性。实验结果表明，该方法超越了多项现有的先进方法。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络易于记忆带有噪声的标签，这会导致性能明显下降。研究通过架构正则化提升抗噪性的方向探索较少。

Method: 本文提出了一种动态连接遮蔽(DCM)机制，通过评估边的携带信息能力，动态遮蔽训练中不重要的连接。结合理论分析，该方法有效减少了梯度误差。

Result: 实验表明，所提出的方法在多种基准测试上优于现有的抗噪方法。此外，首次阐明KAN在抗噪性能上的优势。

Conclusion: DCM机制可无缝集成到不同的抗噪训练方法中，提高深度网络模型在噪声标签环境中的鲁棒性，并首次揭示KAN的抗噪潜力。

Abstract: Noisy labels are inevitable in real-world scenarios. Due to the strong
capacity of deep neural networks to memorize corrupted labels, these noisy
labels can cause significant performance degradation. Existing research on
mitigating the negative effects of noisy labels has mainly focused on robust
loss functions and sample selection, with comparatively limited exploration of
regularization in model architecture. Inspired by the sparsity regularization
used in Kolmogorov-Arnold Networks (KANs), we propose a Dynamic Connection
Masking (DCM) mechanism for both Multi-Layer Perceptron Networks (MLPs) and
KANs to enhance the robustness of classifiers against noisy labels. The
mechanism can adaptively mask less important edges during training by
evaluating their information-carrying capacity. Through theoretical analysis,
we demonstrate its efficiency in reducing gradient error. Our approach can be
seamlessly integrated into various noise-robust training methods to build more
robust deep networks, including robust loss functions, sample selection
strategies, and regularization techniques. Extensive experiments on both
synthetic and real-world benchmarks demonstrate that our method consistently
outperforms state-of-the-art (SOTA) approaches. Furthermore, we are also the
first to investigate KANs as classifiers against noisy labels, revealing their
superior noise robustness over MLPs in real-world noisy scenarios. Our code
will soon be publicly available.

</details>


### [254] [GraphTreeGen: Subtree-Centric Approach to Efficient and Supervised Graph Generation](https://arxiv.org/abs/2508.09710)
*Yitong Luo,Islem Rekik*

Main category: cs.LG

TL;DR: 提出了一种名为GraphTreeGen（GTG）的生成框架，用于高效、精确的脑连接组合成，并在自监督与监督任务中表现突出。


<details>
  <summary>Details</summary>
Motivation: 由于脑连接组数据获取成本高、耗时长，因此需要一种生成式方法以减少对大规模神经影像数据集的依赖。

Method: GTG通过将连接组分解为k-hop子树进行编码，结合双分支解码器进行边的预测与权重重建；提出一种模块化设计，能扩展至超分辨率与跨模态等任务。

Result: GTG在自监督任务中优于当前主流模型，在监督任务中竞争力强，展现了较高的结构保真度和边权预测精度，且内存需求显著降低。

Conclusion: GTG克服了现有方法的多项局限，能够实现效率更高、结果更精准的脑连接组生成。

Abstract: Brain connectomes, representing neural connectivity as graphs, are crucial
for understanding brain organization but costly and time-consuming to acquire,
motivating generative approaches. Recent advances in graph generative modeling
offer a data-driven alternative, enabling synthetic connectome generation and
reducing dependence on large neuroimaging datasets. However, current models
face key limitations: (i) compressing the whole graph into a single latent code
(e.g., VGAEs) blurs fine-grained local motifs; (ii) relying on rich node
attributes rarely available in connectomes reduces reconstruction quality;
(iii) edge-centric models emphasize topology but overlook accurate edge-weight
prediction, harming quantitative fidelity; and (iv) computationally expensive
designs (e.g., edge-conditioned convolutions) impose high memory demands,
limiting scalability. We propose GraphTreeGen (GTG), a subtree-centric
generative framework for efficient, accurate connectome synthesis. GTG
decomposes each connectome into entropy-guided k-hop trees capturing
informative local structure, encoded by a shared GCN. A bipartite
message-passing layer fuses subtree embeddings with global node features, while
a dual-branch decoder jointly predicts edge existence and weights to
reconstruct the adjacency matrix. GTG outperforms state-of-the-art baselines in
self-supervised tasks and remains competitive in supervised settings,
delivering higher structural fidelity and more precise weights with far less
memory. Its modular design enables extensions to connectome super-resolution
and cross-modality synthesis. Code: https://github.com/basiralab/GTG/

</details>


### [255] [Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models](https://arxiv.org/abs/2508.09719)
*Anish Narain,Ritam Majumdar,Nikita Narayanan,Dominic Marshall,Sonali Parbhoo*

Main category: cs.LG

TL;DR: 该研究利用大型语言模型（LLM）从临床笔记中提取补充概念，提升了解释性模型(CBM)对急性呼吸窘迫综合征(ARDS)的预测性能，并显著改善模型的表现。


<details>
  <summary>Details</summary>
Motivation: 利用公开的临床数据集探索疾病异质性和个性化治疗的新方法，同时改进这些数据集的不完整性和标签缺失问题，并提升AI工具的可解释性。

Method: 通过引入大型语言模型（LLM），从临床笔记中生成额外的上下文信息和概念，优化基于解释性概念瓶颈模型(CBM)对急性呼吸窘迫综合征的预测。

Result: 改进后的模型通过生成额外概念，显著提升了预测的性能，性能较现有方法提高了10%。

Conclusion: 引入LLM处理临床笔记可以提升CBM在医学诊断任务中的性能和 interpretability，同时降低信息泄漏风险并减少对偏倚特征的依赖。

Abstract: Large, publicly available clinical datasets have emerged as a novel resource
for understanding disease heterogeneity and to explore personalization of
therapy. These datasets are derived from data not originally collected for
research purposes and, as a result, are often incomplete and lack critical
labels. Many AI tools have been developed to retrospectively label these
datasets, such as by performing disease classification; however, they often
suffer from limited interpretability. Previous work has attempted to explain
predictions using Concept Bottleneck Models (CBMs), which learn interpretable
concepts that map to higher-level clinical ideas, facilitating human
evaluation. However, these models often experience performance limitations when
the concepts fail to adequately explain or characterize the task. We use the
identification of Acute Respiratory Distress Syndrome (ARDS) as a challenging
test case to demonstrate the value of incorporating contextual information from
clinical notes to improve CBM performance. Our approach leverages a Large
Language Model (LLM) to process clinical notes and generate additional
concepts, resulting in a 10% performance gain over existing methods.
Additionally, it facilitates the learning of more comprehensive concepts,
thereby reducing the risk of information leakage and reliance on spurious
shortcuts, thus improving the characterization of ARDS.

</details>


### [256] [Generative Modeling with Multi-Instance Reward Learning for E-commerce Creative Optimization](https://arxiv.org/abs/2508.09730)
*Qiaolei Gu,Yu Li,DingYi Zeng,Lu Wang,Ming Pang,Changping Peng,Zhangang Lin,Ching Law,Jingping Shao*

Main category: cs.LG

TL;DR: GenCO是一个针对电商广告创意组合优化的新框架，通过生成式建模和多实例奖励学习提高广告效果，实验证明其能显著提升广告收入。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法高效搜索和优化电商广告创意组合，难以适应用户点击反馈稀疏的问题。

Method: GenCO框架分为两阶段：通过生成模型生成多样化组合，并用强化学习优化生成过程；使用多实例学习模型将组合级奖励分配至单个创意元素以强化反馈模型。

Result: GenCO在领先的电商平台上成功应用，显著增加了广告收入，并公开了大规模工业数据集。

Conclusion: GenCO有效解决了电商广告创意组合的优化问题，为该领域研究和实际应用提供了新方向。

Abstract: In e-commerce advertising, selecting the most compelling combination of
creative elements -- such as titles, images, and highlights -- is critical for
capturing user attention and driving conversions. However, existing methods
often evaluate creative components individually, failing to navigate the
exponentially large search space of possible combinations. To address this
challenge, we propose a novel framework named GenCO that integrates generative
modeling with multi-instance reward learning. Our unified two-stage
architecture first employs a generative model to efficiently produce a diverse
set of creative combinations. This generative process is optimized with
reinforcement learning, enabling the model to effectively explore and refine
its selections. Next, to overcome the challenge of sparse user feedback, a
multi-instance learning model attributes combination-level rewards, such as
clicks, to the individual creative elements. This allows the reward model to
provide a more accurate feedback signal, which in turn guides the generative
model toward creating more effective combinations. Deployed on a leading
e-commerce platform, our approach has significantly increased advertising
revenue, demonstrating its practical value. Additionally, we are releasing a
large-scale industrial dataset to facilitate further research in this important
domain.

</details>


### [257] [HKT: A Biologically Inspired Framework for Modular Hereditary Knowledge Transfer in Neural Networks](https://arxiv.org/abs/2508.09743)
*Yanick Chistian Tchenko,Felix Mohr,Hicham Hadj Abdelkader,Hedi Tabia*

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为遗传知识迁移（Hereditary Knowledge Transfer, HKT）的新框架，用以优化小型可部署模型性能，比传统知识蒸馏方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前主流的深度学习趋势显示，模型性能随着深度与容量增加而提升，但代价是集成性和效率的下降。本研究旨在通过优化小型模型传承来自大模型的有效知识，以实现性能与资源效率的兼得。

Method: 作者提出了一种生物学启发的多阶段框架HKT，仿照记忆RNA等遗传机制，利用提取、传递和混合（ETM）的策略，将重要特征从大模型传至小模型。同时，采用全新遗传注意力机制（Genetic Attention, GA）调控传承特征和原始特征的融合。

Result: 实验在多个视觉任务（如光流、图像分类、语义分割）上验证了HKT的有效性，其不仅提升了小模型性能，还保持了体积紧凑特点，并始终优于传统知识蒸馏方法。

Conclusion: HKT是一种通用、可解释且可扩展的解决方案，专为资源受限环境中的高效神经网络部署而设计。

Abstract: A prevailing trend in neural network research suggests that model performance
improves with increasing depth and capacity - often at the cost of
integrability and efficiency. In this paper, we propose a strategy to optimize
small, deployable models by enhancing their capabilities through structured
knowledge inheritance. We introduce Hereditary Knowledge Transfer (HKT), a
biologically inspired framework for modular and selective transfer of
task-relevant features from a larger, pretrained parent network to a smaller
child model. Unlike standard knowledge distillation, which enforces uniform
imitation of teacher outputs, HKT draws inspiration from biological inheritance
mechanisms - such as memory RNA transfer in planarians - to guide a multi-stage
process of feature transfer. Neural network blocks are treated as functional
carriers, and knowledge is transmitted through three biologically motivated
components: Extraction, Transfer, and Mixture (ETM). A novel Genetic Attention
(GA) mechanism governs the integration of inherited and native representations,
ensuring both alignment and selectivity. We evaluate HKT across diverse vision
tasks, including optical flow (Sintel, KITTI), image classification (CIFAR-10),
and semantic segmentation (LiTS), demonstrating that it significantly improves
child model performance while preserving its compactness. The results show that
HKT consistently outperforms conventional distillation approaches, offering a
general-purpose, interpretable, and scalable solution for deploying
high-performance neural networks in resource-constrained environments.

</details>


### [258] [A Machine Learning Approach to Predict Biological Age and its Longitudinal Drivers](https://arxiv.org/abs/2508.09747)
*Nazira Dunbayeva,Yulong Li,Yutong Xie,Imran Razzak*

Main category: cs.LG

TL;DR: 本研究通过研发和验证机器学习模型，预测跨时间点的生物年龄，强调健康变化轨迹的重要性，最终显著提升模型预测准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有机器学习模型无法捕捉衰老过程的动态纵向特性问题，并通过纵向队列数据分析个体的衰老轨迹。

Method: 利用2019-2020和2021-2022的纵向队列数据，开发了一个包含新特征（关键生物标记的变化率）的LightGBM模型，并通过SHAP分析评估其重要性。

Result: 最终LightGBM模型在后续时间段成功预测年龄，表现优于传统线性模型和其他树模型（男性R^2=0.515，女性R^2=0.498）。

Conclusion: 研究表明，健康变化轨迹优于静态健康数据，是生物年龄的关键决定因素，研究框架为动态追踪健康及个性化预防年龄相关疾病的临床工具奠定基础。

Abstract: Predicting an individual's aging trajectory is a central challenge in
preventative medicine and bioinformatics. While machine learning models can
predict chronological age from biomarkers, they often fail to capture the
dynamic, longitudinal nature of the aging process. In this work, we developed
and validated a machine learning pipeline to predict age using a longitudinal
cohort with data from two distinct time periods (2019-2020 and 2021-2022). We
demonstrate that a model using only static, cross-sectional biomarkers has
limited predictive power when generalizing to future time points. However, by
engineering novel features that explicitly capture the rate of change (slope)
of key biomarkers over time, we significantly improved model performance. Our
final LightGBM model, trained on the initial wave of data, successfully
predicted age in the subsequent wave with high accuracy ($R^2 = 0.515$ for
males, $R^2 = 0.498$ for females), significantly outperforming both traditional
linear models and other tree-based ensembles. SHAP analysis of our successful
model revealed that the engineered slope features were among the most important
predictors, highlighting that an individual's health trajectory, not just their
static health snapshot, is a key determinant of biological age. Our framework
paves the way for clinical tools that dynamically track patient health
trajectories, enabling early intervention and personalized prevention
strategies for age-related diseases.

</details>


### [259] [$μ$-Parametrization for Mixture of Experts](https://arxiv.org/abs/2508.09752)
*Jan Małaśnicki,Kamil Ciebiera,Mateusz Boruń,Maciej Pióro,Jan Ludziejewski,Maciej Stefaniak,Michał Krutul,Sebastian Jaszczur,Marek Cygan,Kamil Adamczewski,Jakub Krajewski*

Main category: cs.LG

TL;DR: 研究$μ$Parameterization在Mixture-of-Experts（MoE）架构中的应用，并提供理论和实验支持。


<details>
  <summary>Details</summary>
Motivation: 近年来，大型预训练模型和Mixture-of-Experts(MoE)成为研究热点，但两者交叉领域研究稀缺。

Method: 提出并验证了一种适用于MoE的μ$-Parameterization，探讨了专家数量与粒度的扩展对学习率的影响。

Result: 通过理论和实验证明了μ$-Parameterization对核心特性学习的有效性。

Conclusion: 交叉探索μ$-Parameterization和MoE方法提供了新型的训练优化策略及扩展方向。

Abstract: Recent years have seen a growing interest and adoption of LLMs, with
$\mu$Transfer becoming a key technique for tuning hyperparameters in
large-scale training. Meanwhile, Mixture-of-Experts (MoE) has emerged as a
leading architecture in extremely large models. However, the intersection of
these two advancements has remained unexplored. In this work, we derive a
$\mu$-Parameterization ($\mu$P) for MoE, providing theoretical guarantees for
feature learning across model widths in both the router and experts. We
empirically validate our parameterization and further investigate how scaling
the number of experts and granularity affects the optimal learning rate.

</details>


### [260] [TriForecaster: A Mixture of Experts Framework for Multi-Region Electric Load Forecasting with Tri-dimensional Specialization](https://arxiv.org/abs/2508.09753)
*Zhaoyang Zhu,Zhipeng Zeng,Qiming Chen,Linxiao Yang,Peiyuan Liu,Weiqi Chen,Liang Sun*

Main category: cs.LG

TL;DR: 本文提出的TriForecaster框架通过多任务学习和专家模型混合，解决了多区域电力负载预测中的区域、背景及时间变异问题，实现了更高的预测精度。


<details>
  <summary>Details</summary>
Motivation: 在中国某省多城市的负载数据中发现类似的模式，因此提出多区域短期负载预测框架解决相关问题。

Method: 利用基于专家模型混合的多任务学习方法，设计了RegionMixer和Context-Time Specializer层，动态处理区域、背景及时间维度问题。

Result: 评估显示，TriForecaster在四个真实数据集上减少了22.4%的预测误差，并成功部署于中国东部地区，提供17个城市的短期负载预测支持。

Conclusion: TriForecaster框架提高了多区域电力负载预测的精度与适用性，展示了实际部署和广泛应用潜力。

Abstract: Electric load forecasting is pivotal for power system operation, planning and
decision-making. The rise of smart grids and meters has provided more detailed
and high-quality load data at multiple levels of granularity, from home to bus
and cities. Motivated by similar patterns of loads across different cities in a
province in eastern China, in this paper we focus on the Multi-Region Electric
Load Forecasting (MRELF) problem, targeting accurate short-term load
forecasting for multiple sub-regions within a large region. We identify three
challenges for MRELF, including regional variation, contextual variation, and
temporal variation. To address them, we propose TriForecaster, a new framework
leveraging the Mixture of Experts (MoE) approach within a Multi-Task Learning
(MTL) paradigm to overcome these challenges. TriForecaster features RegionMixer
and Context-Time Specializer (CTSpecializer) layers, enabling dynamic
cooperation and specialization of expert models across regional, contextual,
and temporal dimensions. Based on evaluation on four real-world MRELF datasets
with varied granularity, TriForecaster outperforms state-of-the-art models by
achieving an average forecast error reduction of 22.4\%, thereby demonstrating
its flexibility and broad applicability. In particular, the deployment of
TriForecaster on the eForecaster platform in eastern China exemplifies its
practical utility, effectively providing city-level, short-term load forecasts
for 17 cities, supporting a population exceeding 110 million and daily
electricity usage over 100 gigawatt-hours.

</details>


### [261] [Prototype Training with Dual Pseudo-Inverse and Optimized Hidden Activations](https://arxiv.org/abs/2508.09787)
*Mauro Tucci*

Main category: cs.LG

TL;DR: 本文提出了Proto-PINV+H，一种结合闭式权重计算和基于梯度优化的快速训练方法，旨在优化小规模的合成数据输入及其隐藏活性值。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统方法训练速度慢且参数过多的问题，作者提出了将可训练自由度从权重空间转移至数据/活性空间的新方法。

Method: 该方法通过固定间隔重新计算权重矩阵（伪逆计算）并使用Adam优化原型数据，同时引入了隐藏层激活值优化、多层次扩展和理论分析。

Result: 在MNIST和Fashion-MNIST数据集上，该方法仅用3.9s-4.5s便达到了97.8%和89.3%的测试精度，体现了卓越的效率和有效性。

Conclusion: 该方法在精度、速度和模型尺寸上表现出优越的权衡性，优于传统的ELM、随机特征岭回归和浅层MLPs。

Abstract: We present Proto-PINV+H, a fast training paradigm that combines closed-form
weight computation with gradient-based optimisation of a small set of synthetic
inputs, soft labels, and-crucially-hidden activations. At each iteration we
recompute all weight matrices in closed form via two (or more)
ridge-regularised pseudo-inverse solves, while updating only the prototypes
with Adam. The trainable degrees of freedom are thus shifted from weight space
to data/activation space. On MNIST (60k train, 10k test) and Fashion-MNIST (60k
train, 10k test), our method reaches 97.8% and 89.3% test accuracy on the
official 10k test sets, respectively, in 3.9s--4.5s using approximately 130k
trainable parameters and only 250 epochs on an RTX 5060 (16GB). We provide a
multi-layer extension (optimised activations at each hidden stage), learnable
ridge parameters, optional PCA/PLS projections, and theory linking the
condition number of prototype matrices to generalisation. The approach yields
favourable accuracy--speed--size trade-offs against ELM, random-feature ridge,
and shallow MLPs trained by back-propagation.

</details>


### [262] [Bayesian autoregression to optimize temporal Matérn kernel Gaussian process hyperparameters](https://arxiv.org/abs/2508.09792)
*Wouter M. Kouw*

Main category: cs.LG

TL;DR: 提出了一种优化高斯过程的Matérn核协方差函数超参数的新方法，利用递归贝叶斯估计来进行高效的优化。


<details>
  <summary>Details</summary>
Motivation: 通过改进高斯过程的优化方法，提升其在时间序列建模中的实际效果，特别是提高计算效率及回归精度。

Method: 将优化问题转化为自回归模型参数的递归贝叶斯估计过程，从而高效优化协方差函数的超参数。

Result: 所提方法在运行时间和最终回归性能上都优于边际似然最大化和哈密顿蒙特卡罗采样。

Conclusion: 新方法在高斯过程回归中实现了更佳的性能与效率，表现出相比传统方法的显著优势。

Abstract: Gaussian processes are important models in the field of probabilistic
numerics. We present a procedure for optimizing Mat\'ern kernel temporal
Gaussian processes with respect to the kernel covariance function's
hyperparameters. It is based on casting the optimization problem as a recursive
Bayesian estimation procedure for the parameters of an autoregressive model. We
demonstrate that the proposed procedure outperforms maximizing the marginal
likelihood as well as Hamiltonian Monte Carlo sampling, both in terms of
runtime and ultimate root mean square error in Gaussian process regression.

</details>


### [263] [Feature Impact Analysis on Top Long-Jump Performances with Quantile Random Forest and Explainable AI Techniques](https://arxiv.org/abs/2508.09810)
*Qi Gan,Stephan Clémençon,Mounîm A. El-Yacoubi,Sao Mai Nguyen,Eric Fenaux,Ons Jelassi*

Main category: cs.LG

TL;DR: 研究利用机器学习模型分析世界锦标赛跳远决赛的生物力学特征，以探索其对顶级表现的影响。


<details>
  <summary>Details</summary>
Motivation: 研究目的是解决传统方法无法明确分析某些特征与运动员最终表现关系的问题，并利用现代机器学习方法更深刻地理解生物力学特征的关键作用。

Method: 使用机器学习和量化回归模型，分析专家提出的生物力学特征与跳远成绩的关系，并通过SHAP、PDP和ICE可视化模型解释。

Result: 发现男性顶级跳远表现的重要特征是起跳前支撑腿膝关节的角度，而女性则为着陆姿势和助跑步伐技术，同时速度相关特征对两性均关键。

Conclusion: 本研究通过提出的框架，揭示了细微技术特征如何显著影响顶尖运动表现，以及运动员性别差异带来的影响。

Abstract: Biomechanical features have become important indicators for evaluating
athletes' techniques. Traditionally, experts propose significant features and
evaluate them using physics equations. However, the complexity of the human
body and its movements makes it challenging to explicitly analyze the
relationships between some features and athletes' final performance. With
advancements in modern machine learning and statistics, data analytics methods
have gained increasing importance in sports analytics. In this study, we
leverage machine learning models to analyze expert-proposed biomechanical
features from the finals of long jump competitions in the World Championships.
The objectives of the analysis include identifying the most important features
contributing to top-performing jumps and exploring the combined effects of
these key features. Using quantile regression, we model the relationship
between the biomechanical feature set and the target variable (effective
distance), with a particular focus on elite-level jumps. To interpret the
model, we apply SHapley Additive exPlanations (SHAP) alongside Partial
Dependence Plots (PDPs) and Individual Conditional Expectation (ICE) plots. The
findings reveal that, beyond the well-documented velocity-related features,
specific technical aspects also play a pivotal role. For male athletes, the
angle of the knee of the supporting leg before take-off is identified as a key
factor for achieving top 10% performance in our dataset, with angles greater
than 169{\deg}contributing significantly to jump performance. In contrast, for
female athletes, the landing pose and approach step technique emerge as the
most critical features influencing top 10% performances, alongside velocity.
This study establishes a framework for analyzing the impact of various features
on athletic performance, with a particular emphasis on top-performing events.

</details>


### [264] [Provable In-Context Vector Arithmetic via Retrieving Task Concepts](https://arxiv.org/abs/2508.09820)
*Dake Bu,Wei Huang,Andi Han,Atsushi Nitanda,Qingfu Zhang,Hau-San Wong,Taiji Suzuki*

Main category: cs.LG

TL;DR: 该研究提出一种理论框架，解释了大型语言模型在上下文学习任务中的表现，特别是在事实召回任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前关于上下文学习的研究虽然揭示了一些潜在机理，但缺乏系统的理论解释。本研究旨在弥补此缺口。

Method: 基于分层概念建模提出了优化理论，并证明非线性残差Transformer通过梯度下降学习获取事实召回能力，结合向量运算。

Result: 作者证明了0-1损失收敛并展现了对概念重组和分布变化的强鲁棒性，提出的框架优于基于静态嵌入的模型。

Conclusion: 通过理论与实验验证，该模型解释了Transformer的优势并拓展了对上下文学习原理的理解。

Abstract: In-context learning (ICL) has garnered significant attention for its ability
to grasp functions/tasks from demonstrations. Recent studies suggest the
presence of a latent task/function vector in LLMs during ICL. Merullo et al.
(2024) showed that LLMs leverage this vector alongside the residual stream for
Word2Vec-like vector arithmetic, solving factual-recall ICL tasks.
Additionally, recent work empirically highlighted the key role of
Question-Answer data in enhancing factual-recall capabilities. Despite these
insights, a theoretical explanation remains elusive. To move one step forward,
we propose a theoretical framework building on empirically grounded
hierarchical concept modeling. We develop an optimization theory, showing how
nonlinear residual transformers trained via gradient descent on cross-entropy
loss perform factual-recall ICL tasks via vector arithmetic. We prove 0-1 loss
convergence and show the strong generalization, including robustness to concept
recombination and distribution shifts. These results elucidate the advantages
of transformers over static embedding predecessors. Empirical simulations
corroborate our theoretical insights.

</details>


### [265] [RankList -- A Listwise Preference Learning Framework for Predicting Subjective Preferences](https://arxiv.org/abs/2508.09826)
*Abinay Reddy Naini,Fernando Diaz,Carlos Busso*

Main category: cs.LG

TL;DR: 论文提出RankList，一种新颖的列表式偏好学习框架，实现了更高效的训练，以及在主观学习任务中更优秀的排序性能和广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 现有的成对比较框架（如RankNet）过于局限于局部比较，无法有效捕获全局排序一致性。

Method: 提出RankList，通过将RankNet扩展到列表级别的监督结构，在概率框架中结合局部和非局部排序约束，并引入跳跃式比较来增强训练效率和全局排序真实性。

Result: 在多种基准测试数据集上（MSP-Podcast, IEMOCAP, BIIC Podcast，以及艺术图像美学数据集），RankList相比传统框架表现更优，包括Kendall's Tau和排名准确性的提升，并显示了其跨领域的泛化能力。

Conclusion: RankList框架在主观学习场景中改进了排序结果，具备统一和可扩展的优势。

Abstract: Preference learning has gained significant attention in tasks involving
subjective human judgments, such as \emph{speech emotion recognition} (SER) and
image aesthetic assessment. While pairwise frameworks such as RankNet offer
robust modeling of relative preferences, they are inherently limited to local
comparisons and struggle to capture global ranking consistency. To address
these limitations, we propose RankList, a novel listwise preference learning
framework that generalizes RankNet to structured list-level supervision. Our
formulation explicitly models local and non-local ranking constraints within a
probabilistic framework. The paper introduces a log-sum-exp approximation to
improve training efficiency. We further extend RankList with skip-wise
comparisons, enabling progressive exposure to complex list structures and
enhancing global ranking fidelity. Extensive experiments demonstrate the
superiority of our method across diverse modalities. On benchmark SER datasets
(MSP-Podcast, IEMOCAP, BIIC Podcast), RankList achieves consistent improvements
in Kendall's Tau and ranking accuracy compared to standard listwise baselines.
We also validate our approach on aesthetic image ranking using the Artistic
Image Aesthetics dataset, highlighting its broad applicability. Through
ablation and cross-domain studies, we show that RankList not only improves
in-domain ranking but also generalizes better across datasets. Our framework
offers a unified, extensible approach for modeling ordered preferences in
subjective learning scenarios.

</details>


### [266] [FedShard: Federated Unlearning with Efficiency Fairness and Performance Fairness](https://arxiv.org/abs/2508.09866)
*Siyuan Wen,Meng Zhang,Yang Yang,Ningning Ding*

Main category: cs.LG

TL;DR: 该论文提出了一种名为FedShard的新算法，主要用于提高联邦卸载过程中的效率公平性和性能公平性，同时显著加快卸载速度。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中保护用户的遗忘权，尤其是实现数据卸载过程中效率和性能的公平性，这在过往研究中未得到充分关注。

Method: 提出了FedShard算法，针对收敛性、卸载效率和卸载公平性之间的平衡进行自适应调整，并引入两种新衡量指标用于评估卸载算法的公平性。

Result: 该算法理论和实验验证证明，在效率和性能公平性方面具有显著优势，可在面对用户退出和数据攻击时大幅降低卸载成本，且使数据卸载速度达到目前最先进方法的4.9倍以上。

Conclusion: FedShard显著提高了联邦卸载的公平性和效率，是联邦学习领域解决卸载问题的有效方法。

Abstract: To protect clients' right to be forgotten in federated learning, federated
unlearning aims to remove the data contribution of leaving clients from the
global learned model. While current studies mainly focused on enhancing
unlearning efficiency and effectiveness, the crucial aspects of efficiency
fairness and performance fairness among decentralized clients during unlearning
have remained largely unexplored. In this study, we introduce FedShard, the
first federated unlearning algorithm designed to concurrently guarantee both
efficiency fairness and performance fairness. FedShard adaptively addresses the
challenges introduced by dilemmas among convergence, unlearning efficiency, and
unlearning fairness. Furthermore, we propose two novel metrics to
quantitatively assess the fairness of unlearning algorithms, which we prove to
satisfy well-known properties in other existing fairness measurements. Our
theoretical analysis and numerical evaluation validate FedShard's fairness in
terms of both unlearning performance and efficiency. We demonstrate that
FedShard mitigates unfairness risks such as cascaded leaving and poisoning
attacks and realizes more balanced unlearning costs among clients. Experimental
results indicate that FedShard accelerates the data unlearning process 1.3-6.2
times faster than retraining from scratch and 4.9 times faster than the
state-of-the-art exact unlearning methods.

</details>


### [267] [Beyond Scaling Law: A Data-Efficient Distillation Framework for Reasoning](https://arxiv.org/abs/2508.09883)
*Xiaojun Wu,Xiaoguang Jiang,Huiyang Li,Jucai Zhai,Dengfeng Liu,Qiaobo Hao,Huang Liu,Zhiguo Yang,Ji Xie,Ninglun Gu,Jin Yang,Kailai Zhang,Yelun Bao,Jun Wang*

Main category: cs.LG

TL;DR: 提出了一个高效的数据蒸馏框架（DED），通过分析和选择合适的教师模型、小型但精心策划的数据集和多样化的推理轨迹，在小数据量下实现了最先进的推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决在推理能力扩展时所面临的计算成本增加挑战，同时兼顾域内与域外性能的平衡。

Method: 通过三种策略实现：1. 比较现有模型表现后选择最优教师模型；2. 以小型、精心策划的数据集替代大规模数据以达到更好的域内外能力平衡；3. 设计多样化的推理轨迹训练学生模型学习稳健的推理技能。

Result: 在数学推理（AIME 2024/2025和MATH-500）和代码生成（LiveCodeBench）任务上，使用仅0.8k数据，即实现最先进的结果；这种方法优于基于表面难度、token长度或教师模型能力的现有方法。

Conclusion: 所提出的DED框架不仅提高了推理性能，还提供了一种实用且高效的方法来发展高级推理能力，同时保留了模型的泛用性质。

Abstract: Large language models (LLMs) demonstrate remarkable reasoning capabilities in
tasks such as algorithmic coding and mathematical problem-solving. Recent
methods have improved reasoning through expanded corpus and multistage training
combining reinforcement learning and supervised fine-tuning. Although some
methods suggest that small but targeted dataset can incentivize reasoning via
only distillation, a reasoning scaling laws is still taking shape, increasing
computational costs. To address this, we propose a data-efficient distillation
framework (DED) that optimizes the Pareto frontier of reasoning distillation.
Inspired by the on-policy learning and diverse roll-out strategies of
reinforcement learning, the key idea of our approach is threefold: (1) We
identify that benchmark scores alone do not determine an effective teacher
model. Through comprehensive comparisons of leading reasoning LLMs, we develop
a method to select an optimal teacher model. (2) While scaling distillation can
enhance reasoning, it often degrades out-of-domain performance. A carefully
curated, smaller corpus achieves a balanced trade-off between in-domain and
out-of-domain capabilities. (3) Diverse reasoning trajectories encourage the
student model to develop robust reasoning skills. We validate our method
through evaluations on mathematical reasoning (AIME 2024/2025, MATH-500) and
code generation (LiveCodeBench), achieving state-of-the-art results with only
0.8k carefully curated examples, bypassing the need for extensive scaling. Our
systematic analysis demonstrates that DED outperforms existing methods by
considering factors beyond superficial hardness, token length, or teacher model
capability. This work offers a practical and efficient pathway to advanced
reasoning while preserving general capabilities.

</details>


### [268] [Modern Neural Networks for Small Tabular Datasets: The New Default for Field-Scale Digital Soil Mapping?](https://arxiv.org/abs/2508.09888)
*Viacheslav Barkov,Jonas Schmidinger,Robin Gebbers,Martin Atzmueller*

Main category: cs.LG

TL;DR: 在测土领域，现代人工神经网络（ANN）在大多数任务中表现优于传统机器学习方法，提出TabPFN作为新的默认工具。


<details>
  <summary>Details</summary>
Motivation: 探索现代ANN是否能有效解决小训练样本规模和高特征-样本比等问题，从而优化土壤预测模型。

Method: 开发全面的评估基准，测试多种ANN架构，包括MLP、Transformer、检索增强方法以及基础模型TabPFN，基于31个田地和农场数据集进行性能对比。

Result: 现代ANN在多数任务上胜过传统方法，尤其是TabPFN展示了鲁棒性和优越性能。

Conclusion: 现代ANN足够成熟，可取代传统机器学习作为田地尺度的预测工具，并推荐TabPFN成为首选模型。

Abstract: In the field of pedometrics, tabular machine learning is the predominant
method for predicting soil properties from remote and proximal soil sensing
data, forming a central component of digital soil mapping. At the field-scale,
this predictive soil modeling (PSM) task is typically constrained by small
training sample sizes and high feature-to-sample ratios in soil spectroscopy.
Traditionally, these conditions have proven challenging for conventional deep
learning methods. Classical machine learning algorithms, particularly
tree-based models like Random Forest and linear models such as Partial Least
Squares Regression, have long been the default choice for field-scale PSM.
Recent advances in artificial neural networks (ANN) for tabular data challenge
this view, yet their suitability for field-scale PSM has not been proven. We
introduce a comprehensive benchmark that evaluates state-of-the-art ANN
architectures, including the latest multilayer perceptron (MLP)-based models
(TabM, RealMLP), attention-based transformer variants (FT-Transformer,
ExcelFormer, T2G-Former, AMFormer), retrieval-augmented approaches (TabR,
ModernNCA), and an in-context learning foundation model (TabPFN). Our
evaluation encompasses 31 field- and farm-scale datasets containing 30 to 460
samples and three critical soil properties: soil organic matter or soil organic
carbon, pH, and clay content. Our results reveal that modern ANNs consistently
outperform classical methods on the majority of tasks, demonstrating that deep
learning has matured sufficiently to overcome the long-standing dominance of
classical machine learning for PSM. Notably, TabPFN delivers the strongest
overall performance, showing robustness across varying conditions. We therefore
recommend the adoption of modern ANNs for field-scale PSM and propose TabPFN as
the new default choice in the toolkit of every pedometrician.

</details>


### [269] [Rare anomalies require large datasets: About proving the existence of anomalies](https://arxiv.org/abs/2508.09894)
*Simon Klüttermann,Emmanuel Müller*

Main category: cs.LG

TL;DR: 本文研究了如何确定数据集中是否存在异常，提出了一个算法依赖的下限，表明稀有异常的检测存在限制。


<details>
  <summary>Details</summary>
Motivation: 当前异常检测领域对如何确认数据集中是否存在异常探讨较少，本文填补了这一研究空白。

Method: 利用超过三百万次统计测试，分析了数据集大小、污染率与算法相关常数之间的关系，得出了一个样本数下界公式。

Result: 结果表明，未标注数据集中的异常存在与样本数和污染率的平方成反比。


Conclusion: 研究揭示了异常检测的基本限制，为异常检测算法的可行性提供了理论依据。

Abstract: Detecting whether any anomalies exist within a dataset is crucial for
effective anomaly detection, yet it remains surprisingly underexplored in
anomaly detection literature. This paper presents a comprehensive study that
addresses the fundamental question: When can we conclusively determine that
anomalies are present? Through extensive experimentation involving over three
million statistical tests across various anomaly detection tasks and
algorithms, we identify a relationship between the dataset size, contamination
rate, and an algorithm-dependent constant $ \alpha_{\text{algo}} $. Our results
demonstrate that, for an unlabeled dataset of size $ N $ and contamination rate
$ \nu $, the condition $ N \ge \frac{\alpha_{\text{algo}}}{\nu^2} $ represents
a lower bound on the number of samples required to confirm anomaly existence.
This threshold implies a limit to how rare anomalies can be before proving
their existence becomes infeasible.

</details>


### [270] [Beyond Naïve Prompting: Strategies for Improved Zero-shot Context-aided Forecasting with LLMs](https://arxiv.org/abs/2508.09904)
*Arjun Ashok,Andrew Robert Williams,Vincent Zhihao Zheng,Irina Rish,Nicolas Chapados,Étienne Marcotte,Valentina Zantedeschi,Alexandre Drouin*

Main category: cs.LG

TL;DR: 探索大语言模型（LLMs）在结合文本上下文进行预测中的零样本能力，并提出四种优化策略改进性能和资源效率。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过模型更好地结合历史数据和上下文信息，尤其是在文本形式的上下文，以改进真实场景中的预测性能。

Method: 提出四种策略：ReDP通过显式推理路径提升解释性；CorDP使用LLMs改善现有预测；IC-DP通过嵌入历史例子提升准确性；RouteDP通过评估任务难度优化资源配置。

Result: 在CiK基准的各种上下文辅助预测任务中，这些策略相比简单提示呈现出明显的优势，适用于不同规模和类别的LLMs。

Conclusion: 研究证明了使用策略可以有效提升基于LLMs的上下文辅助预测能力，同时为进一步简单却有效的改进方式提供了研究方向。

Abstract: Forecasting in real-world settings requires models to integrate not only
historical data but also relevant contextual information, often available in
textual form. While recent work has shown that large language models (LLMs) can
be effective context-aided forecasters via na\"ive direct prompting, their full
potential remains underexplored. We address this gap with 4 strategies,
providing new insights into the zero-shot capabilities of LLMs in this setting.
ReDP improves interpretability by eliciting explicit reasoning traces, allowing
us to assess the model's reasoning over the context independently from its
forecast accuracy. CorDP leverages LLMs solely to refine existing forecasts
with context, enhancing their applicability in real-world forecasting
pipelines. IC-DP proposes embedding historical examples of context-aided
forecasting tasks in the prompt, substantially improving accuracy even for the
largest models. Finally, RouteDP optimizes resource efficiency by using LLMs to
estimate task difficulty, and routing the most challenging tasks to larger
models. Evaluated on different kinds of context-aided forecasting tasks from
the CiK benchmark, our strategies demonstrate distinct benefits over na\"ive
prompting across LLMs of different sizes and families. These results open the
door to further simple yet effective improvements in LLM-based context-aided
forecasting.

</details>


### [271] [Prototype-Guided Diffusion: Visual Conditioning without External Memory](https://arxiv.org/abs/2508.09922)
*Bilal Faye,Hanane Azzag,Mustapha Lebbah*

Main category: cs.LG

TL;DR: 扩散模型用于高质量图像生成，但计算成本高。本文提出PDM（Prototype Diffusion Model），通过动态构建视觉原型，提高生成效率并降低存储需求。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型计算昂贵且依赖外部内存，缺乏灵活性。本研究旨在通过直接整合原型学习，提升高效适应性。

Method: PDM利用对比学习从干净图像特征动态构建视觉原型，省去外部样本检索；原型指导去噪步骤，对齐相关视觉模式。

Result: 实验表明，PDM在保持高质量生成的同时，显著减少计算与存储开销。

Conclusion: PDM为扩散模型的条件生成提供了高效和适应性强的替代方案，克服了检索方法的缺点。

Abstract: Diffusion models have emerged as a leading framework for high-quality image
generation, offering stable training and strong performance across diverse
domains. However, they remain computationally intensive, particularly during
the iterative denoising process. Latent-space models like Stable Diffusion
alleviate some of this cost by operating in compressed representations, though
at the expense of fine-grained detail. More recent approaches such as
Retrieval-Augmented Diffusion Models (RDM) address efficiency by conditioning
denoising on similar examples retrieved from large external memory banks. While
effective, these methods introduce drawbacks: they require costly storage and
retrieval infrastructure, depend on static vision-language models like CLIP for
similarity, and lack adaptability during training. We propose the Prototype
Diffusion Model (PDM), a method that integrates prototype learning directly
into the diffusion process for efficient and adaptive visual conditioning -
without external memory. Instead of retrieving reference samples, PDM
constructs a dynamic set of compact visual prototypes from clean image features
using contrastive learning. These prototypes guide the denoising steps by
aligning noisy representations with semantically relevant visual patterns,
enabling efficient generation with strong semantic grounding. Experiments show
that PDM maintains high generation quality while reducing computational and
storage overhead, offering a scalable alternative to retrieval-based
conditioning in diffusion models.

</details>


### [272] [Residual Reservoir Memory Networks](https://arxiv.org/abs/2508.09925)
*Matteo Pinna,Andrea Ceni,Claudio Gallicchio*

Main category: cs.LG

TL;DR: 本文提出了一种新型的未训练递归神经网络模型ResRMN，该模型结合线性与非线性存储库，在时序残差连接设计上具有创新性，并表现出对时间序列及像素级分类任务的显著优势。


<details>
  <summary>Details</summary>
Motivation: 解决现有存储计算模型在长时间输入传播与分类任务中表现不足的问题。

Method: 引入一种线性记忆存储库与基于时序方向残差正交连接的非线性存储库相结合的模型，通过线性稳定性分析研究状态动态，并对残差连接的不同配置进行实验探讨。

Result: 实验表明ResRMN模型在时间序列和像素级1-D分类任务上优于传统的存储计算模型。

Conclusion: 该模型的创新残差连接设计提升了长时间输入的传播能力，为存储计算领域提供了一个有效新工具。

Abstract: We introduce a novel class of untrained Recurrent Neural Networks (RNNs)
within the Reservoir Computing (RC) paradigm, called Residual Reservoir Memory
Networks (ResRMNs). ResRMN combines a linear memory reservoir with a non-linear
reservoir, where the latter is based on residual orthogonal connections along
the temporal dimension for enhanced long-term propagation of the input. The
resulting reservoir state dynamics are studied through the lens of linear
stability analysis, and we investigate diverse configurations for the temporal
residual connections. The proposed approach is empirically assessed on
time-series and pixel-level 1-D classification tasks. Our experimental results
highlight the advantages of the proposed approach over other conventional RC
models.

</details>


### [273] [Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models](https://arxiv.org/abs/2508.09968)
*Luca Eyring,Shyamgopal Karthik,Alexey Dosovitskiy,Nataniel Ruiz,Zeynep Akata*

Main category: cs.LG

TL;DR: 本文提出了一种替代测试时间标度优化的新方法，通过一个噪声超网络减少推理计算开销，同时保留质量提升的优势。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时间标度方法虽然能提升大模型的性能，但由于推理时间大幅增加，导致实际应用受限。

Method: 提出了噪声超网络来替代测试时间的噪声优化，通过一个理论基础框架优化噪声空间目标，以达到高效的质量提升。

Result: 所提方法大幅降低了测试时间的计算成本，同时恢复了显著部分的质量提升效果。

Conclusion: 该方法有效解决了测试时间标度带来的计算开销问题，为实用化提供了一种可行性方案。

Abstract: The new paradigm of test-time scaling has yielded remarkable breakthroughs in
Large Language Models (LLMs) (e.g. reasoning models) and in generative vision
models, allowing models to allocate additional computation during inference to
effectively tackle increasingly complex problems. Despite the improvements of
this approach, an important limitation emerges: the substantial increase in
computation time makes the process slow and impractical for many applications.
Given the success of this paradigm and its growing usage, we seek to preserve
its benefits while eschewing the inference overhead. In this work we propose
one solution to the critical problem of integrating test-time scaling knowledge
into a model during post-training. Specifically, we replace reward guided
test-time noise optimization in diffusion models with a Noise Hypernetwork that
modulates initial input noise. We propose a theoretically grounded framework
for learning this reward-tilted distribution for distilled generators, through
a tractable noise-space objective that maintains fidelity to the base model
while optimizing for desired characteristics. We show that our approach
recovers a substantial portion of the quality gains from explicit test-time
optimization at a fraction of the computational cost. Code is available at
https://github.com/ExplainableML/HyperNoise

</details>


### [274] [Dynamic Mixture-of-Experts for Incremental Graph Learning](https://arxiv.org/abs/2508.09974)
*Lecheng Kong,Theodore Vasiloudis,Seongjun Yun,Han Xie,Xiang Song*

Main category: cs.LG

TL;DR: 本文提出了一种动态专家混合(DyMoE)方法，用于增量学习，通过新增专家网络处理数据块，结合稀疏预测机制，有效缓解计算成本，同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图形增量学习方法容易受灾难性遗忘问题影响，无法动态区分历史知识对新任务的贡献，进而影响模型效果。

Method: 提出DyMoE模型，在GNN层中引入专家网络专注于新数据块，并设计正则化损失保证旧知识的有效性，同时使用稀疏MoE方法降低计算复杂度。

Result: 在分类增量学习任务中，相较最佳基准提升4.92%的相对准确度，体现了模型的优越性能。

Conclusion: DyMoE方法通过动态专家网络和稀疏预测机制，不仅解决了灾难性遗忘问题，还显著提升了增量学习场景下的模型效果。

Abstract: Graph incremental learning is a learning paradigm that aims to adapt trained
models to continuously incremented graphs and data over time without the need
for retraining on the full dataset. However, regular graph machine learning
methods suffer from catastrophic forgetting when applied to incremental
learning settings, where previously learned knowledge is overridden by new
knowledge. Previous approaches have tried to address this by treating the
previously trained model as an inseparable unit and using techniques to
maintain old behaviors while learning new knowledge. These approaches, however,
do not account for the fact that previously acquired knowledge at different
timestamps contributes differently to learning new tasks. Some prior patterns
can be transferred to help learn new data, while others may deviate from the
new data distribution and be detrimental. To address this, we propose a dynamic
mixture-of-experts (DyMoE) approach for incremental learning. Specifically, a
DyMoE GNN layer adds new expert networks specialized in modeling the incoming
data blocks. We design a customized regularization loss that utilizes data
sequence information so existing experts can maintain their ability to solve
old tasks while helping the new expert learn the new data effectively. As the
number of data blocks grows over time, the computational cost of the full
mixture-of-experts (MoE) model increases. To address this, we introduce a
sparse MoE approach, where only the top-$k$ most relevant experts make
predictions, significantly reducing the computation time. Our model achieved
4.92\% relative accuracy increase compared to the best baselines on class
incremental learning, showing the model's exceptional power.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [275] [Reinforcement learning in densely recurrent biological networks](https://arxiv.org/abs/2508.09618)
*Miles Walter Churchland,Jordi Garcia-Ojalvo*

Main category: cs.NE

TL;DR: 本文提出了一种结合全局进化搜索与局部直接搜索的混合优化方法ENOMAD，用于在连续动作空间中训练高度递归的网络，并在秀丽隐杆线虫的神经连接图上进行任务测试，表现优于未训练的模型和现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决梯度方法在递归网络中的梯度爆炸或消失问题，以及进化搜索在高维权重空间中收敛较慢的问题。

Method: 提出ENOMAD算法，将全局进化搜索与局部直接搜索结合起来，同时利用生物推导的权重先验对神经连接网络进行优化。

Result: 两种算法变体在所测试的食物觅食任务中表现优秀，性能优于未训练的模型和现有训练策略。

Conclusion: 结合进化搜索与非线性优化是专门化自然递归网络的一种有效方法，具有生物学意义的基础。

Abstract: Training highly recurrent networks in continuous action spaces is a technical
challenge: gradient-based methods suffer from exploding or vanishing gradients,
while purely evolutionary searches converge slowly in high-dimensional weight
spaces. We introduce a hybrid, derivative-free optimization framework that
implements reinforcement learning by coupling global evolutionary exploration
with local direct search exploitation. The method, termed ENOMAD (Evolutionary
Nonlinear Optimization with Mesh Adaptive Direct search), is benchmarked on a
suite of food-foraging tasks instantiated in the fully mapped neural connectome
of the nematode \emph{Caenorhabditis elegans}. Crucially, ENOMAD leverages
biologically derived weight priors, letting it refine--rather than rebuild--the
organism's native circuitry. Two algorithmic variants of the method are
introduced, which lead to either small distributed adjustments of many weights,
or larger changes on a limited number of weights. Both variants significantly
exceed the performance of the untrained connectome (in what can be interpreted
as an example of transfer learning) and of existing training strategies. These
findings demonstrate that integrating evolutionary search with nonlinear
optimization provides an efficient, biologically grounded strategy for
specializing natural recurrent networks towards a specified set of tasks.

</details>
