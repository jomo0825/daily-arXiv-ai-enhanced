<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 98]
- [cs.CL](#cs.CL) [Total: 47]
- [cs.AI](#cs.AI) [Total: 24]
- [cs.LG](#cs.LG) [Total: 64]
- [cs.NE](#cs.NE) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization](https://arxiv.org/abs/2508.20181)
*Alberto Compagnoni,Davide Caffagni,Nicholas Moratelli,Lorenzo Baraldi,Marcella Cornia,Rita Cucchiara*

Main category: cs.CV

TL;DR: 提出CHAIR-DPO方法，通过CHAIR指标和直接偏好优化（DPO）减少多模态大语言模型（MLLM）在回答中的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 解决MLLM回答中幻觉现象的问题，即模型生成的答案与视觉输入不一致。

Method: 利用CHAIR指标区分问题选项的优劣，并用直接偏好优化（DPO）对预训练的MLLM进行微调，从而减少幻觉回答的生成。

Result: CHAIR-DPO方法成功在多个幻觉检测基准上显著减少幻觉回答，同时表明基于CHAIR奖励的微调方法有效。

Conclusion: 通过CHAIR-DPO方法，可以有效减少MLLM生成幻觉内容，提高模型回答质量。

Abstract: Multimodal Large Language Models (MLLMs) emerge as a unified interface to
address a multitude of tasks, ranging from NLP to computer vision. Despite
showcasing state-of-the-art results in many benchmarks, a long-standing issue
is the tendency of MLLMs to hallucinate, that is to generate answers to the
user's query that are not reflected in the visual input. In this paper, we
address the problem of hallucinations as an alignment problem, seeking to steer
the MLLM so that it prefers generating content without hallucinations. In
contrast to recent approaches that require complicated pipelines to build
synthetic preference data for alignment training, often relying on proprietary
models, we capitalize on the well-known CHAIR metric, originally proposed to
gauge the degree of hallucinations in image captioning. Given a pair of
generated answers, we leverage CHAIR to distinguish winner and loser options
(i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf
MLLMs via Direct Preference Optimization (DPO). The resulting method, which we
refer to as CHAIR-DPO, effectively diminishes the amount of hallucinated
answers on several hallucination benchmarks, demonstrating the effectiveness of
fine-tuning the MLLM with a CHAIR-based reward. Source code and trained models
are publicly available at https://github.com/aimagelab/CHAIR-DPO.

</details>


### [2] [SDiFL: Stable Diffusion-Driven Framework for Image Forgery Localization](https://arxiv.org/abs/2508.20182)
*Yang Su,Shunquan Tan,Jiwu Huang*

Main category: cs.CV

TL;DR: 该论文将多模态大模型Stable Diffusion (SD) 与图像取证相结合，提出了一种新的伪造定位框架，在保持高语义信息的同时显著提升了伪造定位的性能。


<details>
  <summary>Details</summary>
Motivation: 当前图像篡改技术的快速发展给传统依赖标注数据的图像伪造检测方法带来极大挑战，亟需探索新的高效准确的解决方案。

Method: 通过将Stable Diffusion 3 (SD3)的多模态架构和高频伪造残差信号结合，训练模型以优化潜在空间的伪造定位能力，并保留原始输入图像的丰富语义特征。

Result: 在常用基准数据集上性能较当前最优模型提升了12%，同时在未见数据的真实文档伪造和自然场景伪造图像任务中也表现出色。

Conclusion: 整合Stable Diffusion的多模态与感知能力，有效提升了图像伪造的定位性能，展现出在真实场景中的广泛应用潜力。

Abstract: Driven by the new generation of multi-modal large models, such as Stable
Diffusion (SD), image manipulation technologies have advanced rapidly, posing
significant challenges to image forensics. However, existing image forgery
localization methods, which heavily rely on labor-intensive and costly
annotated data, are struggling to keep pace with these emerging image
manipulation technologies. To address these challenges, we are the first to
integrate both image generation and powerful perceptual capabilities of SD into
an image forensic framework, enabling more efficient and accurate forgery
localization. First, we theoretically show that the multi-modal architecture of
SD can be conditioned on forgery-related information, enabling the model to
inherently output forgery localization results. Then, building on this
foundation, we specifically leverage the multimodal framework of Stable
DiffusionV3 (SD3) to enhance forgery localization performance.We leverage the
multi-modal processing capabilities of SD3 in the latent space by treating
image forgery residuals -- high-frequency signals extracted using specific
highpass filters -- as an explicit modality. This modality is fused into the
latent space during training to enhance forgery localization performance.
Notably, our method fully preserves the latent features extracted by SD3,
thereby retaining the rich semantic information of the input image.
Experimental results show that our framework achieves up to 12% improvements in
performance on widely used benchmarking datasets compared to current
state-of-the-art image forgery localization models. Encouragingly, the model
demonstrates strong performance on forensic tasks involving real-world document
forgery images and natural scene forging images, even when such data were
entirely unseen during training.

</details>


### [3] [Grounding Multimodal Large Language Models with Quantitative Skin Attributes: A Retrieval Study](https://arxiv.org/abs/2508.20188)
*Max Torop,Masih Eskandar,Nicholas Kurtansky,Jinyang Liu,Jochen Weber,Octavia Camps,Veronica Rotemberg,Jennifer Dy,Kivanc Kose*

Main category: cs.CV

TL;DR: 该研究结合多模态大语言模型（MLLMs）和定量属性使用，提升皮肤病诊断模型的可解释性，通过在MLLM嵌入空间中加入与病变外观属性相关的预测，验证其效用。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能模型在皮肤疾病诊断（包括癌症）方面取得进展，但模型预测的可解释性需显著提升，以便在实际中被使用。

Method: 结合多模态大语言模型（MLLMs）通过自然语言交互提供诊断推理，同时利用病变外观相关的定量属性，通过微调MLLM嵌入空间预测这些属性值，并验证其效用。

Result: 通过使用SLICE-3D数据集进行属性特定的基于内容的图像检索案例研究，证实MLLM嵌入空间与病变外观属性的结合有效。

Conclusion: 结合MLLMs和定量属性是提升皮肤病诊断模型可解释性的可行方法，结果显示其有助于解决预测解释性问题。

Abstract: Artificial Intelligence models have demonstrated significant success in
diagnosing skin diseases, including cancer, showing the potential to assist
clinicians in their analysis. However, the interpretability of model
predictions must be significantly improved before they can be used in practice.
To this end, we explore the combination of two promising approaches: Multimodal
Large Language Models (MLLMs) and quantitative attribute usage. MLLMs offer a
potential avenue for increased interpretability, providing reasoning for
diagnosis in natural language through an interactive format. Separately, a
number of quantitative attributes that are related to lesion appearance (e.g.,
lesion area) have recently been found predictive of malignancy with high
accuracy. Predictions grounded as a function of such concepts have the
potential for improved interpretability. We provide evidence that MLLM
embedding spaces can be grounded in such attributes, through fine-tuning to
predict their values from images. Concretely, we evaluate this grounding in the
embedding space through an attribute-specific content-based image retrieval
case study using the SLICE-3D dataset.

</details>


### [4] [Enhancing Automatic Modulation Recognition With a Reconstruction-Driven Vision Transformer Under Limited Labels](https://arxiv.org/abs/2508.20193)
*Hossein Ahmadi,Banafsheh Saffari*

Main category: cs.CV

TL;DR: 提出了一种基于Vision Transformer (ViT) 的新框架，用于自动调制识别，能够在低标签场景下取得优异表现，相比传统方法更具通用性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法对大规模标注数据集的依赖以及缺乏可扩展性和泛化能力的问题。

Method: 设计了一个统一的ViT框架，其中包括ViT编码器、轻量级卷积解码器和线性分类器，并结合监督、半监督及重构目标进行训练。重构分支将增强信号还原为原始信号，从而优化编码器的特征学习能力。

Result: 在RML2018.01A数据集上，即使仅使用15-20%的标签数据，性能接近ResNet，并在不同的SNR条件下保持强劲表现，优于现有的CNN和ViT基线。

Conclusion: 框架简单、通用，并适用于低标签场景下的高效自动调制识别。

Abstract: Automatic modulation recognition (AMR) is critical for cognitive radio,
spectrum monitoring, and secure wireless communication. However, existing
solutions often rely on large labeled datasets or multi-stage training
pipelines, which limit scalability and generalization in practice. We propose a
unified Vision Transformer (ViT) framework that integrates supervised,
self-supervised, and reconstruction objectives. The model combines a ViT
encoder, a lightweight convolutional decoder, and a linear classifier; the
reconstruction branch maps augmented signals back to their originals, anchoring
the encoder to fine-grained I/Q structure. This strategy promotes robust,
discriminative feature learning during pretraining, while partial label
supervision in fine-tuning enables effective classification with limited
labels. On the RML2018.01A dataset, our approach outperforms supervised CNN and
ViT baselines in low-label regimes, approaches ResNet-level accuracy with only
15-20% labeled data, and maintains strong performance across varying SNR
levels. Overall, the framework provides a simple, generalizable, and
label-efficient solution for AMR.

</details>


### [5] [InfinityHuman: Towards Long-Term Audio-Driven Human](https://arxiv.org/abs/2508.20210)
*Xiaodi Li,Pan Xie,Yi Ren,Qijun Gan,Chen Zhang,Fangyuan Kong,Xiang Yin,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

TL;DR: 提出了一种名为InfinityHuman的新框架，通过音频驱动生成高分辨率、长时视频，解决了现有方法中身份漂移、颜色变化和场景不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 目前音频驱动的人类动画生成面临生成高分辨率、长时间视频时的身份漂移、颜色变化、场景不稳定和手部动作模型不佳的挑战，此工作的目的在于克服这些问题。

Method: 提出了InfinityHuman粗到精的生成框架，分两步生成视频：第一步生成与音频同步的表示，第二步通过姿势引导的精炼器逐步优化为高分辨率视频；此外，设计了手部动作奖励机制以提高动作真实性和语义准确性。

Result: 实验表明，InfinityHuman在视频质量、身份保持、手部动作准确性和唇同步方面均达到最新的性能水平，消融研究证实了各模块的有效性。

Conclusion: InfinityHuman成功解决了现有音频驱动方法中的几大关键问题，为高分辨率、长时人类动画生成提供了一种有效的解决方案。

Abstract: Audio-driven human animation has attracted wide attention thanks to its
practical applications. However, critical challenges remain in generating
high-resolution, long-duration videos with consistent appearance and natural
hand motions. Existing methods extend videos using overlapping motion frames
but suffer from error accumulation, leading to identity drift, color shifts,
and scene instability. Additionally, hand movements are poorly modeled,
resulting in noticeable distortions and misalignment with the audio. In this
work, we propose InfinityHuman, a coarse-to-fine framework that first generates
audio-synchronized representations, then progressively refines them into
high-resolution, long-duration videos using a pose-guided refiner. Since pose
sequences are decoupled from appearance and resist temporal degradation, our
pose-guided refiner employs stable poses and the initial frame as a visual
anchor to reduce drift and improve lip synchronization. Moreover, to enhance
semantic accuracy and gesture realism, we introduce a hand-specific reward
mechanism trained with high-quality hand motion data. Experiments on the EMTD
and HDTF datasets show that InfinityHuman achieves state-of-the-art performance
in video quality, identity preservation, hand accuracy, and lip-sync. Ablation
studies further confirm the effectiveness of each module. Code will be made
public.

</details>


### [6] [Spherical Vision Transformers for Audio-Visual Saliency Prediction in 360-Degree Videos](https://arxiv.org/abs/2508.20221)
*Mert Cokelek,Halit Ozsoy,Nevrez Imamoglu,Cagri Ozcinar,Inci Ayhan,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

TL;DR: 本研究提出了两个新模型SalViT360与SalViT360-AV用于预测360度视频中观众的视觉显著性，展示了模型在多个基准数据集上的优异表现。


<details>
  <summary>Details</summary>
Motivation: 因缺乏360度音视频显著性预测的综合数据集，本研究希望填补这一空白并探索如何利用音视频线索有效预测显著性。

Method: 制作名为YT360-EyeTracking的新数据集，提出了基于视觉Transformer的SalViT360框架和扩展集成音频输入的SalViT360-AV模型。

Result: SalViT360与SalViT360-AV在360度视频显著性预测中优于现有方法。

Conclusion: 将空间音频线索整合进模型架构对提高360度视频显著性预测的准确性至关重要。

Abstract: Omnidirectional videos (ODVs) are redefining viewer experiences in virtual
reality (VR) by offering an unprecedented full field-of-view (FOV). This study
extends the domain of saliency prediction to 360-degree environments,
addressing the complexities of spherical distortion and the integration of
spatial audio. Contextually, ODVs have transformed user experience by adding a
spatial audio dimension that aligns sound direction with the viewer's
perspective in spherical scenes. Motivated by the lack of comprehensive
datasets for 360-degree audio-visual saliency prediction, our study curates
YT360-EyeTracking, a new dataset of 81 ODVs, each observed under varying
audio-visual conditions. Our goal is to explore how to utilize audio-visual
cues to effectively predict visual saliency in 360-degree videos. Towards this
aim, we propose two novel saliency prediction models: SalViT360, a
vision-transformer-based framework for ODVs equipped with spherical
geometry-aware spatio-temporal attention layers, and SalViT360-AV, which
further incorporates transformer adapters conditioned on audio input. Our
results on a number of benchmark datasets, including our YT360-EyeTracking,
demonstrate that SalViT360 and SalViT360-AV significantly outperform existing
methods in predicting viewer attention in 360-degree scenes. Interpreting these
results, we suggest that integrating spatial audio cues in the model
architecture is crucial for accurate saliency prediction in omnidirectional
videos. Code and dataset will be available at
https://cyberiada.github.io/SalViT360.

</details>


### [7] [A Novel Framework for Automated Explain Vision Model Using Vision-Language Models](https://arxiv.org/abs/2508.20227)
*Phu-Vinh Nguyen,Tan-Hanh Pham,Chris Ngo,Truong Son Hy*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉语言模型的管道，用于解释视觉模型在单个样本和整个数据集层面的行为。这能帮助模型开发人员发现失败案例并更好地理解模型趋势。


<details>
  <summary>Details</summary>
Motivation: 目前视觉模型发展重视性能指标，如准确率、IoU和mAP，对模型可解释性关注较少。同时，现有xAI方法多偏向于在单个样本层面解释，对整体行为解释的研究相对不足。

Method: 提出的管道结合视觉语言模型，在样本和数据集两个层面解释视觉模型的行为，从而提供更广泛和深入的分析能力。

Result: 这种管道可以借助尽可能少的努力发现模型的失败案例，并从全局视角获得模型行为洞察。

Conclusion: 将模型开发与xAI分析相结合，能够在改进图像分析中发挥重要作用，提升开发信心并促进公平性。

Abstract: The development of many vision models mainly focuses on improving their
performance using metrics such as accuracy, IoU, and mAP, with less attention
to explainability due to the complexity of applying xAI methods to provide a
meaningful explanation of trained models. Although many existing xAI methods
aim to explain vision models sample-by-sample, methods explaining the general
behavior of vision models, which can only be captured after running on a large
dataset, are still underexplored. Furthermore, understanding the behavior of
vision models on general images can be very important to prevent biased
judgments and help identify the model's trends and patterns. With the
application of Vision-Language Models, this paper proposes a pipeline to
explain vision models at both the sample and dataset levels. The proposed
pipeline can be used to discover failure cases and gain insights into vision
models with minimal effort, thereby integrating vision model development with
xAI analysis to advance image analysis.

</details>


### [8] [ATMS-KD: Adaptive Temperature and Mixed Sample Knowledge Distillation for a Lightweight Residual CNN in Agricultural Embedded Systems](https://arxiv.org/abs/2508.20232)
*Mohamed Ohamouddou,Said Ohamouddou,Abdellatif El Afia,Rafik Lasri*

Main category: cs.CV

TL;DR: 提出了ATMS-KD框架，将自适应温度调度与混合样本增强相结合，用于轻量级CNN模型的知识蒸馏。


<details>
  <summary>Details</summary>
Motivation: 开发适合资源受限农业环境的轻量级CNN模型，并提供实际应用基准。

Method: 结合自适应温度调度与混合样本增强，将知识由MobileNetV3 Large迁移到三个不同配置的轻量级残差CNN学生模型。

Result: 在Damascena玫瑰成熟度数据集上，ATMS-KD方法的学生模型验证准确率均高于96.7%，优于现有11种知识蒸馏方法，且具有最低的推理延迟。

Conclusion: ATMS-KD框架能够在不同的学生模型容量下实现高效知识转移，其在农业计算机视觉领域具备显著的应用潜力。

Abstract: This study proposes ATMS-KD (Adaptive Temperature and Mixed-Sample Knowledge
Distillation), a novel framework for developing lightweight CNN models suitable
for resource-constrained agricultural environments. The framework combines
adaptive temperature scheduling with mixed-sample augmentation to transfer
knowledge from a MobileNetV3 Large teacher model (5.7\,M parameters) to
lightweight residual CNN students. Three student configurations were evaluated:
Compact (1.3\,M parameters), Standard (2.4\,M parameters), and Enhanced (3.8\,M
parameters). The dataset used in this study consists of images of \textit{Rosa
damascena} (Damask rose) collected from agricultural fields in the Dades Oasis,
southeastern Morocco, providing a realistic benchmark for agricultural computer
vision applications under diverse environmental conditions. Experimental
evaluation on the Damascena rose maturity classification dataset demonstrated
significant improvements over direct training methods. All student models
achieved validation accuracies exceeding 96.7\% with ATMS-KD compared to
95--96\% with direct training. The framework outperformed eleven established
knowledge distillation methods, achieving 97.11\% accuracy with the compact
model -- a 1.60 percentage point improvement over the second-best approach
while maintaining the lowest inference latency of 72.19\,ms. Knowledge
retention rates exceeded 99\% for all configurations, demonstrating effective
knowledge transfer regardless of student model capacity.

</details>


### [9] [Linking heterogeneous microstructure informatics with expert characterization knowledge through customized and hybrid vision-language representations for industrial qualification](https://arxiv.org/abs/2508.20243)
*Mutahar Safdar,Gentry Wood,Max Zimmermann,Guy Lamouche,Priti Wanjara,Yaoyao Fiona Zhao*

Main category: cs.CV

TL;DR: 本研究提出了一种新框架，通过混合视觉-语言表示（VLRs）链接微结构信息学和专家知识，利用深度语义分割与预训练多模态模型（CLIP和FLAVA）进行微结构图像和文本描述的编码。此方法在增材制造金属基复合材料数据集上表现优越，可用于零样本微结构分类。


<details>
  <summary>Details</summary>
Motivation: 工业制造中，特别是非传统增材制造工艺中异构结构的快速可靠的材料检测存在瓶颈，需要新的方法提高检测效率和精确性。

Method: 提出一种结合深度语义分割和预训练多模态模型的方法，将视觉和文本信息编码为共享表示，且引入包含正负参考的定制化相似度基于专家标注图片和描述进行微结构分类。

Result: 通过验证，展示了该框架能够有效区分合格与不合格样本。FLAVA模型具有更高视觉敏感性，CLIP模型在文本标准对齐方面较强。Z-score归一化进一步提升跨模态和单模态的分类效果。

Conclusion: 该方法增强了工业资质评估的可追踪性和解释性，无需任务特定的模型重训练，即可支持人机交互决策，为工程信息学领域的资质评估提供了可扩展和域适应性方案。

Abstract: Rapid and reliable qualification of advanced materials remains a bottleneck
in industrial manufacturing, particularly for heterogeneous structures produced
via non-conventional additive manufacturing processes. This study introduces a
novel framework that links microstructure informatics with a range of expert
characterization knowledge using customized and hybrid vision-language
representations (VLRs). By integrating deep semantic segmentation with
pre-trained multi-modal models (CLIP and FLAVA), we encode both visual
microstructural data and textual expert assessments into shared
representations. To overcome limitations in general-purpose embeddings, we
develop a customized similarity-based representation that incorporates both
positive and negative references from expert-annotated images and their
associated textual descriptions. This allows zero-shot classification of
previously unseen microstructures through a net similarity scoring approach.
Validation on an additively manufactured metal matrix composite dataset
demonstrates the framework's ability to distinguish between acceptable and
defective samples across a range of characterization criteria. Comparative
analysis reveals that FLAVA model offers higher visual sensitivity, while the
CLIP model provides consistent alignment with the textual criteria. Z-score
normalization adjusts raw unimodal and cross-modal similarity scores based on
their local dataset-driven distributions, enabling more effective alignment and
classification in the hybrid vision-language framework. The proposed method
enhances traceability and interpretability in qualification pipelines by
enabling human-in-the-loop decision-making without task-specific model
retraining. By advancing semantic interoperability between raw data and expert
knowledge, this work contributes toward scalable and domain-adaptable
qualification strategies in engineering informatics.

</details>


### [10] [MedNet-PVS: A MedNeXt-Based Deep Learning Model for Automated Segmentation of Perivascular Spaces](https://arxiv.org/abs/2508.20256)
*Zhen Xuen Brandon Low,Rory Zhang,Hang Min,William Pham,Lucy Vivash,Jasmine Moses,Miranda Lynch,Karina Dorfman,Cassandra Marotta,Shaun Koh,Jacob Bunyamin,Ella Rowsthorn,Alex Jarema,Himashi Peiris,Zhaolin Chen,Sandy R. Shultz,David K. Wright,Dexiao Kong,Sharon L. Naismith,Terence J. O'Brien,Ying Xia,Meng Law,Benjamin Sinclair*

Main category: cs.CV

TL;DR: 该研究针对脑小血管疾病相关的扩大围血管腔（PVS）分割任务，提出了一种基于Transformer的3D编码-解码卷积网络MedNeXt-L-k5模型。


<details>
  <summary>Details</summary>
Motivation: 手动分割PVS耗时且存在中等一致性，现有自动模型表现有限且难以通用化，因此需要更高效、稳健的方法来应对多样化MRI数据集的PVS分割任务。

Method: 研究中使用MedNeXt-L-k5自动化模型分别在两种MRI数据集（HCP-Aging T2w和七项研究的T1w数据）上进行训练，并通过内部5折交叉验证和留一站点交叉验证评估性能。

Result: 在HCP-Aging的T2w数据集上，MedNeXt-L-k5模型获得了0.88±0.06的体素级Dice评分；但在T1w数据集中，评分为0.58±0.09。在多站点数据集评估中，模型表现相对较弱，体素级Dice评分分别为0.38±0.16（WM）和0.35±0.12（BG）。

Conclusion: 尽管MedNeXt-L-k5为自动化PVS分割提供了一种有效方案，但其性能未超过nnU-Net，表明对于PVS分割任务并不需要Transformer模型中强调的全局上下文机制。

Abstract: Enlarged perivascular spaces (PVS) are increasingly recognized as biomarkers
of cerebral small vessel disease, Alzheimer's disease, stroke, and
aging-related neurodegeneration. However, manual segmentation of PVS is
time-consuming and subject to moderate inter-rater reliability, while existing
automated deep learning models have moderate performance and typically fail to
generalize across diverse clinical and research MRI datasets. We adapted
MedNeXt-L-k5, a Transformer-inspired 3D encoder-decoder convolutional network,
for automated PVS segmentation. Two models were trained: one using a
homogeneous dataset of 200 T2-weighted (T2w) MRI scans from the Human
Connectome Project-Aging (HCP-Aging) dataset and another using 40 heterogeneous
T1-weighted (T1w) MRI volumes from seven studies across six scanners. Model
performance was evaluated using internal 5-fold cross validation (5FCV) and
leave-one-site-out cross validation (LOSOCV). MedNeXt-L-k5 models trained on
the T2w images of the HCP-Aging dataset achieved voxel-level Dice scores of
0.88+/-0.06 (white matter, WM), comparable to the reported inter-rater
reliability of that dataset, and the highest yet reported in the literature.
The same models trained on the T1w images of the HCP-Aging dataset achieved a
substantially lower Dice score of 0.58+/-0.09 (WM). Under LOSOCV, the model had
voxel-level Dice scores of 0.38+/-0.16 (WM) and 0.35+/-0.12 (BG), and
cluster-level Dice scores of 0.61+/-0.19 (WM) and 0.62+/-0.21 (BG).
MedNeXt-L-k5 provides an efficient solution for automated PVS segmentation
across diverse T1w and T2w MRI datasets. MedNeXt-L-k5 did not outperform the
nnU-Net, indicating that the attention-based mechanisms present in
transformer-inspired models to provide global context are not required for high
accuracy in PVS segmentation.

</details>


### [11] [Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation](https://arxiv.org/abs/2508.20265)
*Zhixiang Chi,Yanan Wu,Li Gu,Huan Liu,Ziqiang Wang,Yang Zhang,Yang Wang,Konstantinos N. Plataniotis*

Main category: cs.CV

TL;DR: 提出了一个无训练、反馈驱动的自适应框架，通过将输出的片段级对应反馈到中间注意力，改进了CLIP在开放词汇分割任务中的表现。


<details>
  <summary>Details</summary>
Motivation: CLIP在视觉-文本对齐上表现出色，但由于定位能力较弱，难以处理开放词汇分割任务，且中间注意力无法与文本表示直接交互，存在语义不一致问题。

Method: 设计了一个训练免疫的反馈驱动自适应框架，将输出结果反馈到模型中间注意力中，以输出结果为强空间一致性先验，同时设计了注意力隔离、基于置信度修剪和适配集成等模块实现该目标。

Result: 该方法能够提升现有四种最先进方法在三种模型骨干上的性能，并在八个基准上验证了其有效性。

Conclusion: 自适应框架能增强语义一致性，提升CLIP在开放词汇分割任务中的性能，且能够与现有多种方法和注意力类型无缝集成。

Abstract: CLIP exhibits strong visual-textual alignment but struggle with
open-vocabulary segmentation due to poor localization. Prior methods enhance
spatial coherence by modifying intermediate attention. But, this coherence
isn't consistently propagated to the final output due to subsequent operations
such as projections. Additionally, intermediate attention lacks direct
interaction with text representations, such semantic discrepancy limits the
full potential of CLIP.
  In this work, we propose a training-free, feedback-driven self-adaptive
framework that adapts output-based patch-level correspondences back to the
intermediate attention. The output predictions, being the culmination of the
model's processing, encapsulate the most comprehensive visual and textual
semantics about each patch. Our approach enhances semantic consistency between
internal representations and final predictions by leveraging the model's
outputs as a stronger spatial coherence prior. We design key modules, including
attention isolation, confidence-based pruning for sparse adaptation, and
adaptation ensemble, to effectively feedback the output coherence cues. Our
method functions as a plug-in module, seamlessly integrating into four
state-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H). We
further validate our framework across multiple attention types (Q-K, self-self,
and Proxy augmented with MAE, SAM, and DINO). Our approach consistently
improves their performance across eight benchmarks.

</details>


### [12] [How Multimodal LLMs Solve Image Tasks: A Lens on Visual Grounding, Task Reasoning, and Answer Decoding](https://arxiv.org/abs/2508.20279)
*Zhuoran Yu,Yong Jae Lee*

Main category: cs.CV

TL;DR: 该论文提出了一种探测框架，用于系统性分析多模态大语言模型（MLLMs）处理视觉和文本输入的分层动态，发现了其阶段性的分层结构和稳定性。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在视觉-语言任务中表现出色，但其内部处理机制尚未被充分探索。本文旨在通过探测框架揭示这些模型的分层动态。

Method: 利用标准化提示问题，从模型每一层的嵌入提取信息，通过线性分类器预测细粒度视觉类别，并引入三种提示变体（词汇、语义否定、输出格式）探究各层功能绩效。

Result: 发现MLLMs具有一致的阶段性分层结构：前层负责视觉基础，中间层进行词汇整合和语义推理，最后层生成任务特定输出，并且这种结构的总体稳定性不受模型架构显著影响，但具体层次分配因基础模型变化而有差异。

Conclusion: 该研究提供了一种轻量化且与模型无关的方法，揭示了MLLMs分层组织的统一视角，对其多模态表示动态具有重要理论意义。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated strong performance
across a wide range of vision-language tasks, yet their internal processing
dynamics remain underexplored. In this work, we introduce a probing framework
to systematically analyze how MLLMs process visual and textual inputs across
layers. We train linear classifiers to predict fine-grained visual categories
(e.g., dog breeds) from token embeddings extracted at each layer, using a
standardized anchor question. To uncover the functional roles of different
layers, we evaluate these probes under three types of controlled prompt
variations: (1) lexical variants that test sensitivity to surface-level
changes, (2) semantic negation variants that flip the expected answer by
modifying the visual concept in the prompt, and (3) output format variants that
preserve reasoning but alter the answer format. Applying our framework to
LLaVA-1.5, LLaVA-Next-LLaMA-3, and Qwen2-VL, we identify a consistent
stage-wise structure in which early layers perform visual grounding, middle
layers support lexical integration and semantic reasoning, and final layers
prepare task-specific outputs. We further show that while the overall
stage-wise structure remains stable across variations in visual tokenization,
instruction tuning data, and pretraining corpus, the specific layer allocation
to each stage shifts notably with changes in the base LLM architecture. Our
findings provide a unified perspective on the layer-wise organization of MLLMs
and offer a lightweight, model-agnostic approach for analyzing multimodal
representation dynamics.

</details>


### [13] [Disentangling Latent Embeddings with Sparse Linear Concept Subspaces (SLiCS)](https://arxiv.org/abs/2508.20322)
*Zhi Li,Hau Phan,Matthew Emigh,Austin J. Brockmeier*

Main category: cs.CV

TL;DR: 该研究提出了利用稀疏线性概念子空间（SLiCS）从视觉-语言嵌入中提取语义信息，以进行精准的多标签分类和概念过滤图像检索。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解耦复杂场景的嵌入信息，通过将嵌入分解为多个与特定概念相关的子空间向量，加强下游任务的语义区分能力。

Method: 提出一种监督字典学习方法，通过稀疏、非负组合的方式生成与多标签信息匹配的概念特定组件。从而优化一组具有分组结构的字典，并引入新的交替优化算法以确保收敛。

Result: 实验表明，SLiCS 的解耦嵌入能够提升多标签分类精度，改进概念过滤图像检索的效果，同时适用于不同类型的嵌入，如 TiTok 和 DINOv2 的高压缩自编码器嵌入。

Conclusion: 证明了稀疏线性概念子空间（SLiCS）技术能够增强图像检索的语义解析能力，并为多样化的嵌入类型提供一致性的多标签支持。

Abstract: Vision-language co-embedding networks, such as CLIP, provide a latent
embedding space with semantic information that is useful for downstream tasks.
We hypothesize that the embedding space can be disentangled to separate the
information on the content of complex scenes by decomposing the embedding into
multiple concept-specific component vectors that lie in different subspaces. We
propose a supervised dictionary learning approach to estimate a linear
synthesis model consisting of sparse, non-negative combinations of groups of
vectors in the dictionary (atoms), whose group-wise activity matches the
multi-label information. Each concept-specific component is a non-negative
combination of atoms associated to a label. The group-structured dictionary is
optimized through a novel alternating optimization with guaranteed convergence.
Exploiting the text co-embeddings, we detail how semantically meaningful
descriptions can be found based on text embeddings of words best approximated
by a concept's group of atoms, and unsupervised dictionary learning can exploit
zero-shot classification of training set images using the text embeddings of
concept labels to provide instance-wise multi-labels. We show that the
disentangled embeddings provided by our sparse linear concept subspaces (SLiCS)
enable concept-filtered image retrieval (and conditional generation using
image-to-prompt) that is more precise. We also apply SLiCS to highly-compressed
autoencoder embeddings from TiTok and the latent embedding from self-supervised
DINOv2. Quantitative and qualitative results highlight the improved precision
of the concept-filtered image retrieval for all embeddings.

</details>


### [14] [MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models](https://arxiv.org/abs/2508.20345)
*Xiao Li,Yanfan Zhu,Ruining Deng,Wei-Qi Wei,Yu Wang,Shilin Zhao,Yaohong Wang,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: 本文提出了MedFoundationHub，一个用户友好的GUI工具包，用于安全部署医疗视觉语言模型（VLMs），并通过专家评估揭示现有模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着医学VLM的进步，其在临床和研究中的应用潜力巨大，但也带来了数据隐私泄露和网络威胁等安全风险。需要一种工具，既能保证安全又方便使用，以促进其合理应用。

Method: 开发了MedFoundationHub，一个支持用户通过图形界面选择和部署医疗VLM的工具，支持与Hugging Face模型无缝集成，采取Docker支持的隐私保护设计，只需单台装有NVIDIA A6000 GPU的离线工作站即可运行。

Result: 通过实际部署5种先进医疗VLM，并让具备专业资质的病理学家评估其在病理学病例分析中的表现，共进行1015次评分，揭示了其回答离题、推理模糊和术语不一致等问题。

Conclusion: 尽管医学VLMs展示出潜力，但其在实际应用中存在鲜明的局限性。MedFoundationHub的开发旨在提供安全高效的模型部署解决方案，并为未来的优化研究提供基础。

Abstract: Recent advances in medical vision-language models (VLMs) open up remarkable
opportunities for clinical applications such as automated report generation,
copilots for physicians, and uncertainty quantification. However, despite their
promise, medical VLMs introduce serious security concerns, most notably risks
of Protected Health Information (PHI) exposure, data leakage, and vulnerability
to cyberthreats - which are especially critical in hospital environments. Even
when adopted for research or non-clinical purposes, healthcare organizations
must exercise caution and implement safeguards. To address these challenges, we
present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)
enables physicians to manually select and use different models without
programming expertise, (2) supports engineers in efficiently deploying medical
VLMs in a plug-and-play fashion, with seamless integration of Hugging Face
open-source models, and (3) ensures privacy-preserving inference through
Docker-orchestrated, operating system agnostic deployment. MedFoundationHub
requires only an offline local workstation equipped with a single NVIDIA A6000
GPU, making it both secure and accessible within the typical resources of
academic research labs. To evaluate current capabilities, we engaged
board-certified pathologists to deploy and assess five state-of-the-art VLMs
(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and
LLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,
yielding 1015 clinician-model scoring events. These assessments revealed
recurring limitations, including off-target answers, vague reasoning, and
inconsistent pathology terminology.

</details>


### [15] [Enhancing Mamba Decoder with Bidirectional Interaction in Multi-Task Dense Prediction](https://arxiv.org/abs/2508.20376)
*Mang Cao,Sanping Zhou,Yizhe Li,Ye Deng,Wenli Huang,Le Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的双向交互模型(BIM)，通过对现有的交互机制进行改进，提高了多任务密集预测的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多任务预测中面临交互完整性和计算复杂性之间的权衡，亟需一种既能保持效率，又能优化交互的解决方案。

Method: 设计了一种双向交互扫描机制(BI-Scan)，将交互过程中的任务表示构建为双向序列，并且引入多尺度扫描机制(MS-Scan)以满足不同任务的粒度需求。

Result: 在NYUD-V2和PASCAL-Context基准测试中，该方法显著优于现有的最新方法。

Conclusion: 双向交互扫描和多尺度建模不仅提高了任务交互的效率，还明显提升了模型的预测性能。

Abstract: Sufficient cross-task interaction is crucial for success in multi-task dense
prediction. However, sufficient interaction often results in high computational
complexity, forcing existing methods to face the trade-off between interaction
completeness and computational efficiency. To address this limitation, this
work proposes a Bidirectional Interaction Mamba (BIM), which incorporates novel
scanning mechanisms to adapt the Mamba modeling approach for multi-task dense
prediction. On the one hand, we introduce a novel Bidirectional Interaction
Scan (BI-Scan) mechanism, which constructs task-specific representations as
bidirectional sequences during interaction. By integrating task-first and
position-first scanning modes within a unified linear complexity architecture,
BI-Scan efficiently preserves critical cross-task information. On the other
hand, we employ a Multi-Scale Scan~(MS-Scan) mechanism to achieve
multi-granularity scene modeling. This design not only meets the diverse
granularity requirements of various tasks but also enhances nuanced cross-task
feature interactions. Extensive experiments on two challenging benchmarks,
\emph{i.e.}, NYUD-V2 and PASCAL-Context, show the superiority of our BIM vs its
state-of-the-art competitors.

</details>


### [16] [Audio-Guided Visual Editing with Complex Multi-Modal Prompts](https://arxiv.org/abs/2508.20379)
*Hyeonyu Kim,Seokhoon Jeong,Seonghee Han,Chanhyuk Choi,Taehwan Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种由音频引导的视觉编辑框架，通过整合多模态信息解决仅文本指导方法难以处理的复杂场景问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的视觉编辑方法难以用文本指导完成复杂任务，引入音频提示解决这一问题，并提出无需额外训练的数据适应性方法。

Method: 利用预训练的多模态编码器，解决音频编码空间和扩散模型提示编码空间的差异，结合独立噪声分支和自适应选区的方法处理多模态复杂场景。

Result: 实验表明，该框架在文本方法失败情境下，能够有效利用音频信息完成复杂视觉编辑任务。

Conclusion: 通过音频与文本结合，该方法在视觉编辑的多模态场景中表现优异，有较强的泛化能力与实用价值。

Abstract: Visual editing with diffusion models has made significant progress but often
struggles with complex scenarios that textual guidance alone could not
adequately describe, highlighting the need for additional non-text editing
prompts. In this work, we introduce a novel audio-guided visual editing
framework that can handle complex editing tasks with multiple text and audio
prompts without requiring additional training. Existing audio-guided visual
editing methods often necessitate training on specific datasets to align audio
with text, limiting their generalization to real-world situations. We leverage
a pre-trained multi-modal encoder with strong zero-shot capabilities and
integrate diverse audio into visual editing tasks, by alleviating the
discrepancy between the audio encoder space and the diffusion model's prompt
encoder space. Additionally, we propose a novel approach to handle complex
scenarios with multiple and multi-modal editing prompts through our separate
noise branching and adaptive patch selection. Our comprehensive experiments on
diverse editing tasks demonstrate that our framework excels in handling
complicated editing scenarios by incorporating rich information from audio,
where text-only approaches fail.

</details>


### [17] [More Reliable Pseudo-labels, Better Performance: A Generalized Approach to Single Positive Multi-label Learning](https://arxiv.org/abs/2508.20381)
*Luong Tran,Thieu Vo,Anh Nguyen,Sang Dinh,Van Nguyen*

Main category: cs.CV

TL;DR: 提出了一种新的方法框架 (AEVLP)，通过引入GPR Loss和DAMP技术提升了单正例多标签学习的性能。


<details>
  <summary>Details</summary>
Motivation: 动机是解决大规模数据集难以完全标注的问题，尤其是在单正例多标签学习 (SPML) 中大多数标签未标注的情况下，旨在克服传统方法产生的错误和负面影响。

Method: 提出了GPR Loss损失函数，能高效处理虚假标注并减轻噪声影响；并设计了一种简单却高效的动态增强多焦点伪标注 (DAMP) 技术。

Result: 在四个基准数据集上的实验显示，本研究的框架在多标签分类任务上显著超越了其他方法，达到了最新的性能水平。

Conclusion: 本文的AEVLP框架通过结合GPR Loss和DAMP技术，在解决部分标注数据和多标签分类问题上表现出色，开创了处理SPML的一种新途径。

Abstract: Multi-label learning is a challenging computer vision task that requires
assigning multiple categories to each image. However, fully annotating
large-scale datasets is often impractical due to high costs and effort,
motivating the study of learning from partially annotated data. In the extreme
case of Single Positive Multi-Label Learning (SPML), each image is provided
with only one positive label, while all other labels remain unannotated.
Traditional SPML methods that treat missing labels as unknown or negative tend
to yield inaccuracies and false negatives, and integrating various
pseudo-labeling strategies can introduce additional noise. To address these
challenges, we propose the Generalized Pseudo-Label Robust Loss (GPR Loss), a
novel loss function that effectively learns from diverse pseudo-labels while
mitigating noise. Complementing this, we introduce a simple yet effective
Dynamic Augmented Multi-focus Pseudo-labeling (DAMP) technique. Together, these
contributions form the Adaptive and Efficient Vision-Language Pseudo-Labeling
(AEVLP) framework. Extensive experiments on four benchmark datasets demonstrate
that our framework significantly advances multi-label classification, achieving
state-of-the-art results.

</details>


### [18] [Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection](https://arxiv.org/abs/2508.20392)
*Chengjun Zhang,Yuhao Zhang,Jie Yang,Mohamad Sawan*

Main category: cs.CV

TL;DR: 提出了一种用于脉冲神经网络的延迟脉冲方法和时间依赖型Integrate-and-Fire (tdIF)神经元架构，在超低时间步情况下实现了视觉检测任务的高性能和超低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前ANN-SNN转换方法在分类任务中表现优异，但在视觉检测任务中性能较差，需要改进以实现更好的特征表示和低延迟输出。

Method: 引入延迟脉冲方法以缓解残余膜电位问题，并设计了时间依赖型Integrate-and-Fire（tdIF）神经元架构，使神经元能够基于时间步动态调整其累积和发放行为，增强时间特性表达。

Result: 所提方法在5个时间步内实现了视觉检测任务的最优性能，超越当前ANN-SNN转换方法，同时维持与传统方法相当的能耗。

Conclusion: 提出的方法在视觉检测任务中展现了更高的特征表示精度和超低延迟，证明了其在能效和性能上的优势。

Abstract: Spiking Neural Networks (SNNs), inspired by the brain, are characterized by
minimal power consumption and swift inference capabilities on neuromorphic
hardware, and have been widely applied to various visual perception tasks.
Current ANN-SNN conversion methods have achieved excellent results in
classification tasks with ultra-low time-steps, but their performance in visual
detection tasks remains suboptimal. In this paper, we propose a delay-spike
approach to mitigate the issue of residual membrane potential caused by
heterogeneous spiking patterns. Furthermore, we propose a novel
temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This
enables Integrate-and-fire (IF) neurons to dynamically adjust their
accumulation and firing behaviors based on the temporal order of time-steps.
Our method enables spikes to exhibit distinct temporal properties, rather than
relying solely on frequency-based representations. Moreover, the tdIF neuron
maintains energy consumption on par with traditional IF neuron. We demonstrate
that our method achieves more precise feature representation with lower
time-steps, enabling high performance and ultra-low latency in visual detection
tasks. In this study, we conduct extensive evaluation of the tdIF method across
two critical vision tasks: object detection and lane line detection. The
results demonstrate that the proposed method surpasses current ANN-SNN
conversion approaches, achieving state-of-the-art performance with ultra-low
latency (within 5 time-steps).

</details>


### [19] [Graph-Based Uncertainty Modeling and Multimodal Fusion for Salient Object Detection](https://arxiv.org/abs/2508.20415)
*Yuqi Xiong,Wuzhen Shi,Yang Wen,Ruhan Liu*

Main category: cs.CV

TL;DR: 提出了DUP-MCRNet，通过动态不确定性传播和多模态协作机制，解决了现有显著性目标检测方法中细节丢失、边缘模糊及单模态信息融合不足的问题，在多个基准数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 针对复杂场景中显著性物体检测困难，如细节丢失、边缘模糊及单一模态信息不足，提出改进方法。

Method: 设计了一种动态不确定性图卷积模块（DUGC）和多模态协作融合策略（MCF），结合多尺度损失优化，加强模态间的语义一致性与补充性。

Result: 与多种现有方法相比，DUP-MCRNet在多数基准数据集上取得更优表现，特别是在清晰度与复杂背景鲁棒性上有显著提升。

Conclusion: DUP-MCRNet有效改进了显著性目标检测的准确性与鲁棒性，尤其是小结构与边缘区域的检测能力。

Abstract: In view of the problems that existing salient object detection (SOD) methods
are prone to losing details, blurring edges, and insufficient fusion of
single-modal information in complex scenes, this paper proposes a dynamic
uncertainty propagation and multimodal collaborative reasoning network
(DUP-MCRNet). Firstly, a dynamic uncertainty graph convolution module (DUGC) is
designed to propagate uncertainty between layers through a sparse graph
constructed based on spatial semantic distance, and combined with channel
adaptive interaction, it effectively improves the detection accuracy of small
structures and edge regions. Secondly, a multimodal collaborative fusion
strategy (MCF) is proposed, which uses learnable modality gating weights to
weightedly fuse the attention maps of RGB, depth, and edge features. It can
dynamically adjust the importance of each modality according to different
scenes, effectively suppress redundant or interfering information, and
strengthen the semantic complementarity and consistency between
cross-modalities, thereby improving the ability to identify salient regions
under occlusion, weak texture or background interference. Finally, the
detection performance at the pixel level and region level is optimized through
multi-scale BCE and IoU loss, cross-scale consistency constraints, and
uncertainty-guided supervision mechanisms. Extensive experiments show that
DUP-MCRNet outperforms various SOD methods on most common benchmark datasets,
especially in terms of edge clarity and robustness to complex backgrounds. Our
code is publicly available at https://github.com/YukiBear426/DUP-MCRNet.

</details>


### [20] [MSMVD: Exploiting Multi-scale Image Features via Multi-scale BEV Features for Multi-view Pedestrian Detection](https://arxiv.org/abs/2508.20447)
*Taiga Yamane,Satoshi Suzuki,Ryo Masumura,Shota Orihashi,Tomohiro Tanaka,Mana Ihori,Naoki Makishima,Naotaka Kawata*

Main category: cs.CV

TL;DR: 本文提出了一种名为MSMVD的新方法，用以解决多视角行人检测中的尺度问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在检测小尺度、尺度变化大的行人时表现不佳，主要因为未能利用多尺度图像特征。

Method: 提出了MSMVD方法，通过逐尺度投影多视角图像特征到鸟瞰图（BEV）空间生成多尺度BEV特征，并结合特征金字塔网络处理多尺度信息。

Result: 实验证明，利用多尺度特征大大提升了检测效果，MSMVD在GMVD数据集上超越了前最高MODA值4.5个百分点。

Conclusion: MSMVD充分利用多尺度特征，解决了多视角行人检测中的尺度差异问题，提升了检测性能。

Abstract: Multi-View Pedestrian Detection (MVPD) aims to detect pedestrians in the form
of a bird's eye view (BEV) from multi-view images. In MVPD, end-to-end
trainable deep learning methods have progressed greatly. However, they often
struggle to detect pedestrians with consistently small or large scales in views
or with vastly different scales between views. This is because they do not
exploit multi-scale image features to generate the BEV feature and detect
pedestrians. To overcome this problem, we propose a novel MVPD method, called
Multi-Scale Multi-View Detection (MSMVD). MSMVD generates multi-scale BEV
features by projecting multi-scale image features extracted from individual
views into the BEV space, scale-by-scale. Each of these BEV features inherits
the properties of its corresponding scale image features from multiple views.
Therefore, these BEV features help the precise detection of pedestrians with
consistently small or large scales in views. Then, MSMVD combines information
at different scales of multiple views by processing the multi-scale BEV
features using a feature pyramid network. This improves the detection of
pedestrians with vastly different scales between views. Extensive experiments
demonstrate that exploiting multi-scale image features via multi-scale BEV
features greatly improves the detection performance, and MSMVD outperforms the
previous highest MODA by $4.5$ points on the GMVD dataset.

</details>


### [21] [A Spatial-Frequency Aware Multi-Scale Fusion Network for Real-Time Deepfake Detection](https://arxiv.org/abs/2508.20449)
*Libo Lv,Tianyi Wang,Mengxiao Huang,Ruixia Liu,Yinglong Wang*

Main category: cs.CV

TL;DR: 提出了SFMFNet，一个用于实时深度伪造检测的高效轻量化网络，具备良好的准确性与通用性，并在多个基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造内容生成技术迅速发展，但现有检测器的高计算成本限制了其实时应用能力，因此需要一种高效轻量化的解决方案。

Method: 提出了Spatial-Frequency Aware Multi-Scale Fusion Network (SFMFNet)架构，通过结合空间纹理和频率伪迹的空间-频率混合感知模块、基于令牌的选择性交叉注意机制以及增强的模糊池化结构来提高检测效率和准确性。

Result: 在多个基准数据集上的实验表明，SFMFNet能够达到准确性和效率之间的良好平衡，并具有较强的泛化能力和实际应用价值。

Conclusion: SFMFNet是一个具备实时深度伪造检测能力的高效解决方案，能够以较低计算成本实现准确识别，对实际应用具有重要价值。

Abstract: With the rapid advancement of real-time deepfake generation techniques,
forged content is becoming increasingly realistic and widespread across
applications like video conferencing and social media. Although
state-of-the-art detectors achieve high accuracy on standard benchmarks, their
heavy computational cost hinders real-time deployment in practical
applications. To address this, we propose the Spatial-Frequency Aware
Multi-Scale Fusion Network (SFMFNet), a lightweight yet effective architecture
for real-time deepfake detection. We design a spatial-frequency hybrid aware
module that jointly leverages spatial textures and frequency artifacts through
a gated mechanism, enhancing sensitivity to subtle manipulations. A
token-selective cross attention mechanism enables efficient multi-level feature
interaction, while a residual-enhanced blur pooling structure helps retain key
semantic cues during downsampling. Experiments on several benchmark datasets
show that SFMFNet achieves a favorable balance between accuracy and efficiency,
with strong generalization and practical value for real-time applications.

</details>


### [22] [Dual-Model Weight Selection and Self-Knowledge Distillation for Medical Image Classification](https://arxiv.org/abs/2508.20461)
*Ayaka Tsutsumi,Guang Li,Ren Togo,Takahiro Ogawa,Satoshi Kondo,Miki Haseyama*

Main category: cs.CV

TL;DR: 提出了一种结合双模型权重选择和自知识蒸馏的新型医学图像分类方法，旨在通过轻量化模型实现高效且高性能的医学图像分类。


<details>
  <summary>Details</summary>
Motivation: 解决在有限计算资源下无法有效部署大规模模型的问题，同时开发轻量级模型以实现与大规模模型相当的性能。

Method: 采用双模型权重选择策略，将两个轻量级模型初始化为来自大规模预训练模型的权重，同时结合自知识蒸馏以覆盖广泛的初始权重配置，并针对分类任务进行微调。

Result: 实验表明，该方法在多种公开医学图像数据集上的性能和鲁棒性均优于现有方法。

Conclusion: 通过将双模型权重选择与自知识蒸馏相结合，该方法克服了传统轻量级模型信息丢失的局限性，为医学生物图像分类任务提供了高效且有效的解决方案。

Abstract: We propose a novel medical image classification method that integrates
dual-model weight selection with self-knowledge distillation (SKD). In
real-world medical settings, deploying large-scale models is often limited by
computational resource constraints, which pose significant challenges for their
practical implementation. Thus, developing lightweight models that achieve
comparable performance to large-scale models while maintaining computational
efficiency is crucial. To address this, we employ a dual-model weight selection
strategy that initializes two lightweight models with weights derived from a
large pretrained model, enabling effective knowledge transfer. Next, SKD is
applied to these selected models, allowing the use of a broad range of initial
weight configurations without imposing additional excessive computational cost,
followed by fine-tuning for the target classification tasks. By combining
dual-model weight selection with self-knowledge distillation, our method
overcomes the limitations of conventional approaches, which often fail to
retain critical information in compact models. Extensive experiments on
publicly available datasets-chest X-ray images, lung computed tomography scans,
and brain magnetic resonance imaging scans-demonstrate the superior performance
and robustness of our approach compared to existing methods.

</details>


### [23] [Re-Densification Meets Cross-Scale Propagation: Real-Time Compression of LiDAR Point Clouds](https://arxiv.org/abs/2508.20466)
*Pengpeng Yu,Haoran Li,Dingquan Li,Runqing Jiang,Jing Wang,Liang Lin,Yulan Guo*

Main category: cs.CV

TL;DR: 本文提出了一种针对LiDAR点云高效压缩的框架，通过几何再密化和跨尺度特征传播两大模块生成紧凑特征，以实现高效预测编码，达到先进的压缩比和实时性。


<details>
  <summary>Details</summary>
Motivation: LiDAR点云数据高精度扫描的存储和传输成本过高，现有方法受几何细节稀疏性的限制，在上下文建模和压缩性能上效率不足。

Method: 框架包含两个轻量模块：几何再密化模块，通过再密化细化和稀疏特征提取进行预测编码；跨尺度特征传播模块，利用多分辨率的占用信息指导特征传播。

Result: 在KITTI数据集上实验表明，该方法实现了先进的压缩比，支持12-bit量化下的实时性能（编码和解码均为26 FPS）。

Conclusion: 通过轻量级特征生成与高效上下文建模，提出的框架在点云压缩领域表现出色，同时提供开源代码供研究与应用。

Abstract: LiDAR point clouds are fundamental to various applications, yet
high-precision scans incur substantial storage and transmission overhead.
Existing methods typically convert unordered points into hierarchical octree or
voxel structures for dense-to-sparse predictive coding. However, the extreme
sparsity of geometric details hinders efficient context modeling, thereby
limiting their compression performance and speed. To address this challenge, we
propose to generate compact features for efficient predictive coding. Our
framework comprises two lightweight modules. First, the Geometry
Re-Densification Module re-densifies encoded sparse geometry, extracts features
at denser scale, and then re-sparsifies the features for predictive coding.
This module avoids costly computation on highly sparse details while
maintaining a lightweight prediction head. Second, the Cross-scale Feature
Propagation Module leverages occupancy cues from multiple resolution levels to
guide hierarchical feature propagation. This design facilitates information
sharing across scales, thereby reducing redundant feature extraction and
providing enriched features for the Geometry Re-Densification Module. By
integrating these two modules, our method yields a compact feature
representation that provides efficient context modeling and accelerates the
coding process. Experiments on the KITTI dataset demonstrate state-of-the-art
compression ratios and real-time performance, achieving 26 FPS for both
encoding and decoding at 12-bit quantization. Code is available at
https://github.com/pengpeng-yu/FastPCC.

</details>


### [24] [Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation](https://arxiv.org/abs/2508.20470)
*Xiaochuan Li,Guoguang Du,Runze Zhang,Liang Jin,Qi Jia,Lihua Lu,Zhenhua Guo,Yaqian Zhao,Haiyang Liu,Tianqi Wang,Changsheng Li,Xiaoli Gong,Rengang Li,Baoyu Fan*

Main category: cs.CV

TL;DR: 本文提出了利用视频数据提升3D生成的办法，并引入了首个大的多视图视频数据集Droplet3D-4M及生成模型Droplet3D。结果验证了其生成空间一致且语义合理的3D内容能力。


<details>
  <summary>Details</summary>
Motivation: 由于3D数据的缺乏，相比文本和图像领域，3D生成面临数据稀缺问题，本文旨在利用丰富的多视图视频中的常识先验解决这一瓶颈。

Method: 引入Droplet3D-4M大规模多视图视频数据集，并基于此开发支持图片和文本输入的生成模型Droplet3D，通过实验有效验证模型性能。

Result: Droplet3D生成的内容在空间一致性和语义上表现优越。此外，与现有方法相比，具有扩展至场景级应用的潜力。

Conclusion: 使用视频中蕴含的常识先验显著提升了3D生成效果；Droplet3D-4M数据集和Droplet3D模型为社区提供了宝贵的资源，有望推动3D生成的进一步发展。

Abstract: Scaling laws have validated the success and promise of large-data-trained
models in creative generation across text, image, and video domains. However,
this paradigm faces data scarcity in the 3D domain, as there is far less of it
available on the internet compared to the aforementioned modalities.
Fortunately, there exist adequate videos that inherently contain commonsense
priors, offering an alternative supervisory signal to mitigate the
generalization bottleneck caused by limited native 3D data. On the one hand,
videos capturing multiple views of an object or scene provide a spatial
consistency prior for 3D generation. On the other hand, the rich semantic
information contained within the videos enables the generated content to be
more faithful to the text prompts and semantically plausible. This paper
explores how to apply the video modality in 3D asset generation, spanning
datasets to models. We introduce Droplet3D-4M, the first large-scale video
dataset with multi-view level annotations, and train Droplet3D, a generative
model supporting both image and dense text input. Extensive experiments
validate the effectiveness of our approach, demonstrating its ability to
produce spatially consistent and semantically plausible content. Moreover, in
contrast to the prevailing 3D solutions, our approach exhibits the potential
for extension to scene-level applications. This indicates that the commonsense
priors from the videos significantly facilitate 3D creation. We have
open-sourced all resources including the dataset, code, technical framework,
and model weights: https://dropletx.github.io/.

</details>


### [25] [Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation](https://arxiv.org/abs/2508.20471)
*Jiusi Li,Jackson Jiang,Jinyu Miao,Miao Long,Tuopu Wen,Peijin Jia,Shengxiang Liu,Chunlei Yu,Maolin Liu,Yuzhan Cai,Kun Jiang,Mengmeng Yang,Diange Yang*

Main category: cs.CV

TL;DR: 提出了一种名为G^2Editor的框架，用于在驾驶视频中进行逼真且精确的对象编辑。


<details>
  <summary>Details</summary>
Motivation: 应对在真实世界中收集关键驾驶案例的高成本和风险问题，并解决现有方法在视觉真实性和姿态控制方面的不足。

Method: 利用3D高斯表示作为稠密先验注入去噪过程中，同时结合场景级3D边界框布局重建非目标区域，加入分层细粒度特征指导生成细节。

Result: 在Waymo Open Dataset上，G^2Editor显著优于现有方法，在姿态可控性和视觉质量上均表现突出，并支持数据驱动任务。

Conclusion: G^2Editor提供了一种统一的框架，有助于提高自动驾驶的训练和验证效果，同时克服了现有方法的缺点。

Abstract: Corner cases are crucial for training and validating autonomous driving
systems, yet collecting them from the real world is often costly and hazardous.
Editing objects within captured sensor data offers an effective alternative for
generating diverse scenarios, commonly achieved through 3D Gaussian Splatting
or image generative models. However, these approaches often suffer from limited
visual fidelity or imprecise pose control. To address these issues, we propose
G^2Editor, a framework designed for photorealistic and precise object editing
in driving videos. Our method leverages a 3D Gaussian representation of the
edited object as a dense prior, injected into the denoising process to ensure
accurate pose control and spatial consistency. A scene-level 3D bounding box
layout is employed to reconstruct occluded areas of non-target objects.
Furthermore, to guide the appearance details of the edited object, we
incorporate hierarchical fine-grained features as additional conditions during
generation. Experiments on the Waymo Open Dataset demonstrate that G^2Editor
effectively supports object repositioning, insertion, and deletion within a
unified framework, outperforming existing methods in both pose controllability
and visual quality, while also benefiting downstream data-driven tasks.

</details>


### [26] [Enhancing Corpus Callosum Segmentation in Fetal MRI via Pathology-Informed Domain Randomization](https://arxiv.org/abs/2508.20475)
*Marina Grifell i Plana,Vladyslav Zalevskyi,Léa Schmidt,Yvan Gomez,Thomas Sanchez,Vincent Dunet,Mériam Koob,Vanessa Siffredi,Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: 该研究提出了一种融合CCD病理学知识的领域随机化策略以增强胎儿大脑分割，解决了数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 解决CCD数据稀缺问题，提升深度学习模型的推广能力，促进胎儿脑部疾病分析。

Method: 通过健康数据生成多样的脑部病变模拟，嵌入CCD病理先验知识，以实现无需病理数据标注的分割能力。

Result: 方法在CCD病例中实现显著改进，同时保持健康胎儿和其他脑病患者的表现。降低LCC估计误差，改善拓扑一致性，支持CCD分型。

Conclusion: 通过将解剖先验知识融入合成数据生成，能有效缓解数据稀缺问题，提升稀有病理分割和分析能力。

Abstract: Accurate fetal brain segmentation is crucial for extracting biomarkers and
assessing neurodevelopment, especially in conditions such as corpus callosum
dysgenesis (CCD), which can induce drastic anatomical changes. However, the
rarity of CCD severely limits annotated data, hindering the generalization of
deep learning models. To address this, we propose a pathology-informed domain
randomization strategy that embeds prior knowledge of CCD manifestations into a
synthetic data generation pipeline. By simulating diverse brain alterations
from healthy data alone, our approach enables robust segmentation without
requiring pathological annotations.
  We validate our method on a cohort comprising 248 healthy fetuses, 26 with
CCD, and 47 with other brain pathologies, achieving substantial improvements on
CCD cases while maintaining performance on both healthy fetuses and those with
other pathologies. From the predicted segmentations, we derive clinically
relevant biomarkers, such as corpus callosum length (LCC) and volume, and show
their utility in distinguishing CCD subtypes. Our pathology-informed
augmentation reduces the LCC estimation error from 1.89 mm to 0.80 mm in
healthy cases and from 10.9 mm to 0.7 mm in CCD cases. Beyond these
quantitative gains, our approach yields segmentations with improved topological
consistency relative to available ground truth, enabling more reliable
shape-based analyses. Overall, this work demonstrates that incorporating
domain-specific anatomical priors into synthetic data pipelines can effectively
mitigate data scarcity and enhance analysis of rare but clinically significant
malformations.

</details>


### [27] [Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding](https://arxiv.org/abs/2508.20476)
*Jeong Hun Yeo,Hyeongseop Rha,Sungjune Park,Junil Won,Yong Man Ro*

Main category: cs.CV

TL;DR: 此研究提出一个统一框架，能够整合手语、唇部动作和音频，以生成书面语言文本，成果超越或媲美当前任务专用模型。


<details>
  <summary>Details</summary>
Motivation: 现有语音识别系统对听障人士不友好，研究手语和唇读的集成是填补此空白的重要课题。

Method: 设计了一个使异构输入有效处理的统一、模态无关架构，探索不同模态间的协同效应，并专注于提升性能到达甚至超越任务专用模型的水平。

Result: 提出的框架在SLT、VSR、ASR和AVSR四个领域的表现达到或超越最先进模型，特别是显式建模唇部动作显著提升了SLT性能。

Conclusion: 研究证明，通过处理多模态输入，并将唇部动作作为独立模态建模，可以显著推动无音频交互领域的发展。

Abstract: Audio is the primary modality for human communication and has driven the
success of Automatic Speech Recognition (ASR) technologies. However, such
systems remain inherently inaccessible to individuals who are deaf or hard of
hearing. Visual alternatives such as sign language and lip reading offer
effective substitutes, and recent advances in Sign Language Translation (SLT)
and Visual Speech Recognition (VSR) have improved audio-less communication.
Yet, these modalities have largely been studied in isolation, and their
integration within a unified framework remains underexplored. In this paper, we
introduce the first unified framework capable of handling diverse combinations
of sign language, lip movements, and audio for spoken-language text generation.
We focus on three main objectives: (i) designing a unified, modality-agnostic
architecture capable of effectively processing heterogeneous inputs; (ii)
exploring the underexamined synergy among modalities, particularly the role of
lip movements as non-manual cues in sign language comprehension; and (iii)
achieving performance on par with or superior to state-of-the-art models
specialized for individual tasks. Building on this framework, we achieve
performance on par with or better than task-specific state-of-the-art models
across SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that
explicitly modeling lip movements as a separate modality significantly improves
SLT performance.

</details>


### [28] [Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding](https://arxiv.org/abs/2508.20478)
*Yuan Xie,Tianshui Chen,Zheng Ge,Lionel Ni*

Main category: cs.CV

TL;DR: 提出了一种多轮推理的框架（Video-MTR），通过逐步选择视频片段和问题理解实现对长视频的精确分析，同时无需外部VLM，实现端到端训练，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有长视频理解方法依赖静态推理或外部VLM，导致复杂性和性能不足的问题。

Method: 引入Video-MTR框架，通过多轮推理逐步选择关键视频片段，优化问题理解过程，并采用双层奖励机制结合答案准确性和帧与查询的相关性进行优化。

Result: 在VideoMME、MLVU和EgoSchema等基准测试上，Video-MTR在准确性和效率上均优于现有方法。

Conclusion: Video-MTR通过多轮推理和端到端训练改进了长视频理解，表现优于现有技术并提升精确性和效率。

Abstract: Long-form video understanding, characterized by long-range temporal
dependencies and multiple events, remains a challenge. Existing methods often
rely on static reasoning or external visual-language models (VLMs), which face
issues like complexity and sub-optimal performance due to the lack of
end-to-end training. In this paper, we propose Video-MTR, a reinforced
multi-turn reasoning framework designed to enable iterative key video segment
selection and question comprehension. Unlike traditional video reasoning
pipeline, which generate predictions in a single turn, Video-MTR performs
reasoning in multiple turns, selecting video segments progressively based on
the evolving understanding of previously processed segments and the current
question. This iterative process allows for a more refined and contextually
aware analysis of the video. To ensure intermediate reasoning process, we
introduce a novel gated bi-level reward system, combining trajectory-level
rewards based on answer correctness and turn-level rewards emphasizing
frame-query relevance. This system optimizes both video segment selection and
question comprehension, eliminating the need for external VLMs and allowing
end-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU,
and EgoSchema demonstrate that Video-MTR outperforms existing methods in both
accuracy and efficiency, advancing the state-of-the-art in long video
understanding.

</details>


### [29] [Adaptive Dual Uncertainty Optimization: Boosting Monocular 3D Object Detection under Test-Time Shifts](https://arxiv.org/abs/2508.20488)
*Zixuan Hu,Dongxiao Li,Xinzhu Ma,Shixiang Tang,Xiaotong Li,Wenhan Yang,Ling-Yu Duan*

Main category: cs.CV

TL;DR: 提出一种名为DUO的框架，旨在解决单目3D目标检测中的语义和几何不确定性问题，并通过优化提升其在不同数据集中适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有的单目3D检测在现实环境的域转移下表现不稳定，急需一种方法来提升其实时适应能力。

Method: 设计了DUO框架，通过凸优化方法同时最小化语义和几何不确定性，并引入语义感知约束机制，增强目标检测性能。

Result: 实验结果显示DUO在多数据集及不同域转移情况下，相较于现有方法表现优异。

Conclusion: DUO框架通过联合优化两种不确定性，以及引入语义约束，显著提升了单目3D目标检测在域转移下的鲁棒性。

Abstract: Accurate monocular 3D object detection (M3OD) is pivotal for safety-critical
applications like autonomous driving, yet its reliability deteriorates
significantly under real-world domain shifts caused by environmental or sensor
variations. To address these shifts, Test-Time Adaptation (TTA) methods have
emerged, enabling models to adapt to target distributions during inference.
While prior TTA approaches recognize the positive correlation between low
uncertainty and high generalization ability, they fail to address the dual
uncertainty inherent to M3OD: semantic uncertainty (ambiguous class
predictions) and geometric uncertainty (unstable spatial localization). To
bridge this gap, we propose Dual Uncertainty Optimization (DUO), the first TTA
framework designed to jointly minimize both uncertainties for robust M3OD.
Through a convex optimization lens, we introduce an innovative convex structure
of the focal loss and further derive a novel unsupervised version, enabling
label-agnostic uncertainty weighting and balanced learning for high-uncertainty
objects. In parallel, we design a semantic-aware normal field constraint that
preserves geometric coherence in regions with clear semantic cues, reducing
uncertainty from the unstable 3D representation. This dual-branch mechanism
forms a complementary loop: enhanced spatial perception improves semantic
classification, and robust semantic predictions further refine spatial
understanding. Extensive experiments demonstrate the superiority of DUO over
existing methods across various datasets and domain shift types.

</details>


### [30] [CaddieSet: A Golf Swing Dataset with Human Joint Features and Ball Information](https://arxiv.org/abs/2508.20491)
*Seunghyeon Jung,Seoyoung Hong,Jiwoo Jeong,Seungwon Jeong,Jaerim Choi,Hoki Kim,Woojin Lee*

Main category: cs.CV

TL;DR: 提出了一个名为CaddieSet的新数据集，通过视频分解为八个高尔夫动作阶段，分析挥杆姿势与球轨迹的关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究未能量化挥杆姿势与球轨迹的关系，使得改进挥杆的建议欠缺数据支持。

Method: 通过基于计算机视觉的模型提取挥杆视频中的关节信息，并定义15个关键姿态指标分析挥杆影响。

Result: 通过实验验证了CaddieSet用于预测球轨迹的可行性，展示了其挥杆反馈结果与高尔夫领域知识的一致性。

Conclusion: 该研究为高尔夫挥杆分析提供新视角，对学术研究与运动行业具有重要意义。

Abstract: Recent advances in deep learning have led to more studies to enhance golfers'
shot precision. However, these existing studies have not quantitatively
established the relationship between swing posture and ball trajectory,
limiting their ability to provide golfers with the necessary insights for swing
improvement. In this paper, we propose a new dataset called CaddieSet, which
includes joint information and various ball information from a single shot.
CaddieSet extracts joint information from a single swing video by segmenting it
into eight swing phases using a computer vision-based approach. Furthermore,
based on expert golf domain knowledge, we define 15 key metrics that influence
a golf swing, enabling the interpretation of swing outcomes through
swing-related features. Through experiments, we demonstrated the feasibility of
CaddieSet for predicting ball trajectories using various benchmarks. In
particular, we focus on interpretable models among several benchmarks and
verify that swing feedback using our joint features is quantitatively
consistent with established domain knowledge. This work is expected to offer
new insight into golf swing analysis for both academia and the sports industry.

</details>


### [31] [IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection](https://arxiv.org/abs/2508.20492)
*Xuanming Cao,Chengyu Tao,Yifeng Cheng,Juan Du*

Main category: cs.CV

TL;DR: 该论文提出了一种名为IAENet的框架，通过结合2D和3D模型优势，以提高工业制造中表面异常的检测性能。


<details>
  <summary>Details</summary>
Motivation: 目前3D点云检测由于缺乏强大的预训练骨干网络而发展受限，作者旨在通过结合2D预训练模型和3D模型弥补这一差距。

Method: 提出了IAENet框架及动态加权IAF模块，能够根据数据动态调整2D和3D模型的贡献，并设计了指导优化的损失函数。

Result: 在MVTec 3D-AD基准上，IAENet显著降低了误报率，达成了新的SOTA性能。

Conclusion: IAENet框架及其引入的IAF模块有效提升了3D点云表面异常检测，具有较高的工业应用价值。

Abstract: Surface anomaly detection is pivotal for ensuring product quality in
industrial manufacturing. While 2D image-based methods have achieved remarkable
success, 3D point cloud-based detection remains underexplored despite its
richer geometric cues. We argue that the key bottleneck is the absence of
powerful pretrained foundation backbones in 3D comparable to those in 2D. To
bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an
ensemble framework that synergizes 2D pretrained expert with 3D expert models.
However, naively fusing predictions from disparate sources is non-trivial:
existing strategies can be affected by a poorly performing modality and thus
degrade overall accuracy. To address this challenge, We introduce an novel
Importance-Aware Fusion (IAF) module that dynamically assesses the contribution
of each source and reweights their anomaly scores. Furthermore, we devise
critical loss functions that explicitly guide the optimization of IAF, enabling
it to combine the collective knowledge of the source experts but also preserve
their unique strengths, thereby enhancing the overall performance of anomaly
detection. Extensive experiments on MVTec 3D-AD demonstrate that our IAENet
achieves a new state-of-the-art with a markedly lower false positive rate,
underscoring its practical value for industrial deployment.

</details>


### [32] [Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent](https://arxiv.org/abs/2508.20505)
*En Ci,Shanyan Guan,Yanhao Ge,Yilin Zhang,Wei Li,Zhenyu Zhang,Jian Yang,Ying Tai*

Main category: cs.CV

TL;DR: 本文提出DescriptiveEdit框架，通过将“基于指令的图像编辑”改为“基于参考图像的文本生成图像”方法，改进了语义图像编辑。


<details>
  <summary>Details</summary>
Motivation: 解决语义图像编辑中翻转算法引入重建误差以及基于指令模型由于数据集质量和规模受限的问题。

Method: 基于参考图像和描述性提示生成编辑图像，引入Cross-Attentive UNet模块，通过注意力桥将参考图像特征注入生成过程，并与现有技术如ControlNet无缝集成。

Result: 在Emu Edit基准实验中，DescriptiveEdit显著提升了编辑精度和一致性。

Conclusion: DescriptiveEdit利用强大的文本生成图像模型，避免了架构修改或翻转，实现了更高效、更质量化的语义图像编辑。

Abstract: Despite the progress in text-to-image generation, semantic image editing
remains a challenge. Inversion-based algorithms unavoidably introduce
reconstruction errors, while instruction-based models mainly suffer from
limited dataset quality and scale. To address these problems, we propose a
descriptive-prompt-based editing framework, named DescriptiveEdit. The core
idea is to re-frame `instruction-based image editing' as `reference-image-based
text-to-image generation', which preserves the generative power of well-trained
Text-to-Image models without architectural modifications or inversion.
Specifically, taking the reference image and a prompt as input, we introduce a
Cross-Attentive UNet, which newly adds attention bridges to inject reference
image features into the prompt-to-edit-image generation process. Owing to its
text-to-image nature, DescriptiveEdit overcomes limitations in instruction
dataset quality, integrates seamlessly with ControlNet, IP-Adapter, and other
extensions, and is more scalable. Experiments on the Emu Edit benchmark show it
improves editing accuracy and consistency.

</details>


### [33] [DCFS: Continual Test-Time Adaptation via Dual Consistency of Feature and Sample](https://arxiv.org/abs/2508.20516)
*Wenting Yin,Han Sun,Xinru Meng,Ningzhong Liu,Huiyu Zhou*

Main category: cs.CV

TL;DR: 本文介绍了持续测试时间自适应（CTTA），提出DCFS框架，通过双路径特征一致性和基于置信度的样本学习，解决目标域特征混淆和学习偏差问题，抑制伪标签噪声，减少错误累积，实验验证结果出色。


<details>
  <summary>Details</summary>
Motivation: 解决持续测试时间自适应中由于目标域特征混淆和伪标签质量差导致的学习偏差和错误累积问题。

Method: 提出DCFS框架，采用双分类器区分语义相关特征与域相关特征，保持子特征与整体特征的一致性，同时通过自适应阈值和置信得分进行损失加权的自监督学习。

Result: 实验结果表明DCFS在CIFAR10-C、CIFAR100-C及ImageNet-C数据集上的持续适应场景表现出色。

Conclusion: DCFS通过全面捕获多角度数据特征，并有效减轻伪标签噪声和错误累积问题，实现了持续测试时间自适应的高效表现。

Abstract: Continual test-time adaptation aims to continuously adapt a pre-trained model
to a stream of target domain data without accessing source data. Without access
to source domain data, the model focuses solely on the feature characteristics
of the target data. Relying exclusively on these features can lead to confusion
and introduce learning biases. Currently, many existing methods generate
pseudo-labels via model predictions. However, the quality of pseudo-labels
cannot be guaranteed and the problem of error accumulation must be solved. To
address these challenges, we propose DCFS, a novel CTTA framework that
introduces dual-path feature consistency and confidence-aware sample learning.
This framework disentangles the whole feature representation of the target data
into semantic-related feature and domain-related feature using dual classifiers
to learn distinct feature representations. By maintaining consistency between
the sub-features and the whole feature, the model can comprehensively capture
data features from multiple perspectives. Additionally, to ensure that the
whole feature information of the target domain samples is not overlooked, we
set a adaptive threshold and calculate a confidence score for each sample to
carry out loss weighted self-supervised learning, effectively reducing the
noise of pseudo-labels and alleviating the problem of error accumulation. The
efficacy of our proposed method is validated through extensive experimentation
across various datasets, including CIFAR10-C, CIFAR100-C, and ImageNet-C,
demonstrating consistent performance in continual test-time adaptation
scenarios.

</details>


### [34] [Adam SLAM - the last mile of camera calibration with 3DGS](https://arxiv.org/abs/2508.20526)
*Matthieu Gendrin,Stéphane Pateux,Xiaoran Jiang,Théo Ladune,Luce Morin*

Main category: cs.CV

TL;DR: 本文提出利用3DGS模型，通过对相机参数进行颜色损失的反向传播，优化相机校准，从而提升新视图合成质量。


<details>
  <summary>Details</summary>
Motivation: 相机校准质量对新视图合成的重建质量有显著影响，但真实场景缺乏校准的地面真实值。

Method: 利用3DGS模型，通过颜色损失的反向传播，对相机参数进行微调优化。

Result: 在3DGS数据集上，相机的重新校准平均提升了0.4 dB PSNR。

Conclusion: 对于如Mip-NeRF 360这样的参考场景，提高视图质量最为重要，尽管优化时间较长，但这种方法在关键场景中很有价值。

Abstract: The quality of the camera calibration is of major importance for evaluating
progresses in novel view synthesis, as a 1-pixel error on the calibration has a
significant impact on the reconstruction quality. While there is no ground
truth for real scenes, the quality of the calibration is assessed by the
quality of the novel view synthesis. This paper proposes to use a 3DGS model to
fine tune calibration by backpropagation of novel view color loss with respect
to the cameras parameters. The new calibration alone brings an average
improvement of 0.4 dB PSNR on the dataset used as reference by 3DGS. The fine
tuning may be long and its suitability depends on the criticity of training
time, but for calibration of reference scenes, such as Mip-NeRF 360, the stake
of novel view quality is the most important.

</details>


### [35] [Learning What is Worth Learning: Active and Sequential Domain Adaptation for Multi-modal Gross Tumor Volume Segmentation](https://arxiv.org/abs/2508.20528)
*Jingyun Yang,Guoqing Zhang,Jingge Wang,Yang Li*

Main category: cs.CV

TL;DR: 提出一种针对多模态医学数据的主动和连续域适应框架，通过动态样本选择提高肿瘤分割性能。


<details>
  <summary>Details</summary>
Motivation: 多模态医学肿瘤分割数据标注耗时且昂贵，现有的主动域适应方法容易引起负迁移，且缺少适用于多模态医学数据的样本查询策略。

Method: 提出一种主动和连续域适应框架，结合样本信息度与代表性，设计动态多模态样本选择与训练的查询策略。

Result: 在多种肿瘤分割任务中的实验验证表明，该方法显著优于现有主动域适应技术。

Conclusion: 该方法能有效减少标注成本，同时在多模态医学图像领域中表现优异，具有实际应用潜力。

Abstract: Accurate gross tumor volume segmentation on multi-modal medical data is
critical for radiotherapy planning in nasopharyngeal carcinoma and
glioblastoma. Recent advances in deep neural networks have brought promising
results in medical image segmentation, leading to an increasing demand for
labeled data. Since labeling medical images is time-consuming and
labor-intensive, active learning has emerged as a solution to reduce annotation
costs by selecting the most informative samples to label and adapting
high-performance models with as few labeled samples as possible. Previous
active domain adaptation (ADA) methods seek to minimize sample redundancy by
selecting samples that are farthest from the source domain. However, such
one-off selection can easily cause negative transfer, and access to source
medical data is often limited. Moreover, the query strategy for multi-modal
medical data remains unexplored. In this work, we propose an active and
sequential domain adaptation framework for dynamic multi-modal sample selection
in ADA. We derive a query strategy to prioritize labeling and training on the
most valuable samples based on their informativeness and representativeness.
Empirical validation on diverse gross tumor volume segmentation tasks
demonstrates that our method achieves favorable segmentation performance,
significantly outperforming state-of-the-art ADA methods. Code is available at
the git repository: \href{https://github.com/Hiyoochan/mmActS}{mmActS}.

</details>


### [36] [Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection](https://arxiv.org/abs/2508.20530)
*Mingqian Ji,Jian Yang,Shanshan Zhang*

Main category: cs.CV

TL;DR: 提出了一种通过早期数据级别融合的方法，结合RGB图像和LiDAR数据，提高了无监督三维目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LiDAR的三维目标检测依赖高质量的人工标注，耗时且困难。改善伪标签生成质量成为焦点。

Method: 通过视觉基础模型进行实例分割和深度估计，并采用前后期数据级别融合方法，结合局部和全局过滤方法减小估计噪声。同时引入了一种动态自进化策略优化伪框。

Result: 在nuScenes数据集上，该方法在验证基准上显著优于现有方法，获得28.4%的mAP。

Conclusion: 采用早期的RGB和LiDAR数据融合和动态进化策略，其方法能有效地提高无监督三维目标检测的性能和定位精度。

Abstract: Existing LiDAR-based 3D object detectors typically rely on manually annotated
labels for training to achieve good performance. However, obtaining
high-quality 3D labels is time-consuming and labor-intensive. To address this
issue, recent works explore unsupervised 3D object detection by introducing RGB
images as an auxiliary modal to assist pseudo-box generation. However, these
methods simply integrate pseudo-boxes generated by LiDAR point clouds and RGB
images. Yet, such a label-level fusion strategy brings limited improvements to
the quality of pseudo-boxes, as it overlooks the complementary nature in terms
of LiDAR and RGB image data. To overcome the above limitations, we propose a
novel data-level fusion framework that integrates RGB images and LiDAR data at
an early stage. Specifically, we utilize vision foundation models for instance
segmentation and depth estimation on images and introduce a bi-directional
fusion method, where real points acquire category labels from the 2D space,
while 2D pixels are projected onto 3D to enhance real point density. To
mitigate noise from depth and segmentation estimations, we propose a local and
global filtering method, which applies local radius filtering to suppress depth
estimation errors and global statistical filtering to remove
segmentation-induced outliers. Furthermore, we propose a data-level fusion
based dynamic self-evolution strategy, which iteratively refines pseudo-boxes
under a dense representation, significantly improving localization accuracy.
Extensive experiments on the nuScenes dataset demonstrate that the detector
trained by our method significantly outperforms that trained by previous
state-of-the-art methods with 28.4$\%$ mAP on the nuScenes validation
benchmark.

</details>


### [37] [Digital Scale: Open-Source On-Device BMI Estimation from Smartphone Camera Images Trained on a Large-Scale Real-World Dataset](https://arxiv.org/abs/2508.20534)
*Frederik Rajiv Manichand,Robin Deuber,Robert Jakob,Steve Swerling,Jamie Rosen,Elgar Fleisch,Patrick Langer*

Main category: cs.CV

TL;DR: 透過深度學習從智能手機圖像估算BMI，提出數據集過濾方法，改善訓練數據質量，並在多數據集上獲得最佳《BMI估算精度。推出了基於Android的實時應用。


<details>
  <summary>Details</summary>
Motivation: 傳統BMI測量往往因場景限制而難以實現，如遠程醫療或緊急情況，該研究旨在通過攝像頭圖像提供快速估算方法。

Method: 利用WayBED數據集，開發深度學習模型，應用自動過濾方法提升圖像數據質量，並進行跨數據集測試（及微調）。

Result: 在WayBED數據集上達到當前最低的MAPE值7.9%，並在其他數據集上展現優異的泛化能力，微調後仍是最低報告MAPE值。

Conclusion: 通過高質量數據增強的深度學習模型能夠準確估算BMI，工具包適配移動設備，廣泛應用潛力大。

Abstract: Estimating Body Mass Index (BMI) from camera images with machine learning
models enables rapid weight assessment when traditional methods are unavailable
or impractical, such as in telehealth or emergency scenarios. Existing computer
vision approaches have been limited to datasets of up to 14,500 images. In this
study, we present a deep learning-based BMI estimation method trained on our
WayBED dataset, a large proprietary collection of 84,963 smartphone images from
25,353 individuals. We introduce an automatic filtering method that uses
posture clustering and person detection to curate the dataset by removing
low-quality images, such as those with atypical postures or incomplete views.
This process retained 71,322 high-quality images suitable for training. We
achieve a Mean Absolute Percentage Error (MAPE) of 7.9% on our hold-out test
set (WayBED data) using full-body images, the lowest value in the published
literature to the best of our knowledge. Further, we achieve a MAPE of 13% on
the completely unseen~(during training) VisualBodyToBMI dataset, comparable
with state-of-the-art approaches trained on it, demonstrating robust
generalization. Lastly, we fine-tune our model on VisualBodyToBMI and achieve a
MAPE of 8.56%, the lowest reported value on this dataset so far. We deploy the
full pipeline, including image filtering and BMI estimation, on Android devices
using the CLAID framework. We release our complete code for model training,
filtering, and the CLAID package for mobile deployment as open-source
contributions.

</details>


### [38] [Domain Adaptation Techniques for Natural and Medical Image Classification](https://arxiv.org/abs/2508.20537)
*Ahmad Chaddad,Yihang Wu,Reem Kateb,Christian Desrosiers*

Main category: cs.CV

TL;DR: 本文分析了七种领域适应（DA）技术在自然和医学图像分类中的表现，进行557次模拟实验，发现了性能最佳的算法为DSAN，尤其在COVID-19数据集和动态数据流应用中效果显著。


<details>
  <summary>Details</summary>
Motivation: 大多数领域适应技术在自然图像上取得了进展，但在医学图像分类的问题上仍缺乏深入研究；此外，研究主流数据集可能导致性能偏差，这需要进一步分析解决。

Method: 本文在五个自然图像数据集和八个医学图像数据集上使用七种领域适应技术，涵盖了分布外、动态数据流及有限训练样本等各种情景，共进行了557次模拟实验。

Result: 实验结果显示，DSAN（深度子域适应网络）算法表现优异，在COVID-19数据集上取得了91.2%的分类准确率，并在动态数据流领域适应情境中比基线提升了6.7%的准确率。此外，DSAN在COVID-19和皮肤癌数据集上展示了高水平可解释性。

Conclusion: 研究结果表明，DSAN在医学图像上的出色表现以及可解释性为领域适应技术在医疗数据中的应用提供了有价值的见解，这有助于进一步推动领域适应技术的研究与发展。

Abstract: Domain adaptation (DA) techniques have the potential in machine learning to
alleviate distribution differences between training and test sets by leveraging
information from source domains. In image classification, most advances in DA
have been made using natural images rather than medical data, which are harder
to work with. Moreover, even for natural images, the use of mainstream datasets
can lead to performance bias. {With the aim of better understanding the
benefits of DA for both natural and medical images, this study performs 557
simulation studies using seven widely-used DA techniques for image
classification in five natural and eight medical datasets that cover various
scenarios, such as out-of-distribution, dynamic data streams, and limited
training samples.} Our experiments yield detailed results and insightful
observations highlighting the performance and medical applicability of these
techniques. Notably, our results have shown the outstanding performance of the
Deep Subdomain Adaptation Network (DSAN) algorithm. This algorithm achieved
feasible classification accuracy (91.2\%) in the COVID-19 dataset using
Resnet50 and showed an important accuracy improvement in the dynamic data
stream DA scenario (+6.7\%) compared to the baseline. Our results also
demonstrate that DSAN exhibits remarkable level of explainability when
evaluated on COVID-19 and skin cancer datasets. These results contribute to the
understanding of DA techniques and offer valuable insight into the effective
adaptation of models to medical data.

</details>


### [39] [Contrastive Learning through Auxiliary Branch for Video Object Detection](https://arxiv.org/abs/2508.20551)
*Lucas Rakotoarivony*

Main category: cs.CV

TL;DR: 提出CLAB方法，通过对抗式学习和动态损失权重提升视频目标检测性能，实现了在没有额外后处理的情况下性能的提升。


<details>
  <summary>Details</summary>
Motivation: 视频目标检测因图像退化等问题比静态图像目标检测难度更大，现有方法虽提升了性能，但计算量增加明显，亟需一种高效且鲁棒的解决方案。

Method: 采用了对比学习辅助支路，结合对比损失增强特征表达能力，同时提出动态损失权重策略，初期侧重特征学习，后期聚焦检测任务，最终提升了检测性能。

Result: 实验表明，CLAB方法在ImageNet VID数据集上显著提升了性能，ResNet-101和ResNeXt-101分别达到了84.0%和85.2%的mAP，实现在现有基础上性能领先。

Conclusion: 本研究提出的CLAB方法在不增加推理计算量情况下显著提升了视频目标检测性能，为CNN模型提供了一种无需复杂后处理的鲁棒检测解决方案。

Abstract: Video object detection is a challenging task because videos often suffer from
image deterioration such as motion blur, occlusion, and deformable shapes,
making it significantly more difficult than detecting objects in still images.
Prior approaches have improved video object detection performance by employing
feature aggregation and complex post-processing techniques, though at the cost
of increased computational demands. To improve robustness to image degradation
without additional computational load during inference, we introduce a
straightforward yet effective Contrastive Learning through Auxiliary Branch
(CLAB) method. First, we implement a constrastive auxiliary branch using a
contrastive loss to enhance the feature representation capability of the video
object detector's backbone. Next, we propose a dynamic loss weighting strategy
that emphasizes auxiliary feature learning early in training while gradually
prioritizing the detection task as training converges. We validate our approach
through comprehensive experiments and ablation studies, demonstrating
consistent performance gains. Without bells and whistles, CLAB reaches a
performance of 84.0% mAP and 85.2% mAP with ResNet-101 and ResNeXt-101,
respectively, on the ImageNet VID dataset, thus achieving state-of-the-art
performance for CNN-based models without requiring additional post-processing
methods.

</details>


### [40] [Towards Mechanistic Defenses Against Typographic Attacks in CLIP](https://arxiv.org/abs/2508.20570)
*Lorenz Hufe,Constantin Venhoff,Maximilian Dreyer,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.CV

TL;DR: 本文提出了一种无需微调的CLIP模型防御方法，可有效抵御文字攻击，同时对正常图像分类性能影响极小。


<details>
  <summary>Details</summary>
Motivation: 文字攻击利用图像中的文本扰乱多模态系统，给分类任务、安全性等带来极大威胁。

Method: 通过分析CLIP视觉编码器对文字攻击的反应，定位到对文字信息传递的关键注意力头，并通过选择性消除这些头实现防御，无需额外微调模型。

Result: 在ImageNet-100的文字攻击测试中性能提升19.6%，对正常分类性能影响不足1%。

Conclusion: 研究证明了消除关键注意力头能有效防御文字攻击，同时模型的适用性扩展至注重安全的场景。

Abstract: Typographic attacks exploit multi-modal systems by injecting text into
images, leading to targeted misclassifications, malicious content generation
and even Vision-Language Model jailbreaks. In this work, we analyze how CLIP
vision encoders behave under typographic attacks, locating specialized
attention heads in the latter half of the model's layers that causally extract
and transmit typographic information to the cls token. Building on these
insights, we introduce a method to defend CLIP models against typographic
attacks by selectively ablating a typographic circuit, consisting of attention
heads. Without requiring finetuning, our method improves performance by up to
19.6% on a typographic variant of ImageNet-100, while reducing standard
ImageNet-100 accuracy by less than 1%. Notably, our training-free approach
remains competitive with current state-of-the-art typographic defenses that
rely on finetuning. To this end, we release a family of dyslexic CLIP models
which are significantly more robust against typographic attacks. These models
serve as suitable drop-in replacements for a broad range of safety-critical
applications, where the risks of text-based manipulation outweigh the utility
of text recognition.

</details>


### [41] [GLaRE: A Graph-based Landmark Region Embedding Network for Emotion Recognition](https://arxiv.org/abs/2508.20579)
*Debasis Maji,Debaditya Barman*

Main category: cs.CV

TL;DR: 本文提出了一种名为GLaRE的新方法，通过图神经网络（GNNs）对面部表情进行识别，取得了优异的实验结果。


<details>
  <summary>Details</summary>
Motivation: 面部表情识别在计算机视觉中具有重要意义，但受限于传统方法对遮挡、表情多样性和可解释性的挑战。

Method: 采用3D面部对齐提取面部关键点，通过分层简化构建商图进行结构化学习，从而保持空间结构并减少复杂度。

Result: 在AffectNet上取得64.89%的准确率，在FERG数据集上达到94.24%的准确率，并优于诸多现有基线方法。

Conclusion: 利用商图进行的区域级嵌入能够显著提升预测性能，为面部表情识别提供了全新的研究方向。

Abstract: Facial expression recognition (FER) is a crucial task in computer vision with
wide range of applications including human computer interaction, surveillance,
and assistive technologies. However, challenges such as occlusion, expression
variability, and lack of interpretability hinder the performance of traditional
FER systems. Graph Neural Networks (GNNs) offer a powerful alternative by
modeling relational dependencies between facial landmarks, enabling structured
and interpretable learning. In this paper, we propose GLaRE, a novel
Graph-based Landmark Region Embedding network for emotion recognition. Facial
landmarks are extracted using 3D facial alignment, and a quotient graph is
constructed via hierarchical coarsening to preserve spatial structure while
reducing complexity. Our method achieves 64.89 percentage accuracy on AffectNet
and 94.24 percentage on FERG, outperforming several existing baselines.
Additionally, ablation studies have demonstrated that region-level embeddings
from quotient graphs have contributed to improved prediction performance.

</details>


### [42] [FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models](https://arxiv.org/abs/2508.20586)
*Zheng Chong,Yanwei Lei,Shiyue Zhang,Zhuandi He,Zhen Wang,Xujie Zhang,Xiao Dong,Yiling Wu,Dongmei Jiang,Xiaodan Liang*

Main category: cs.CV

TL;DR: FastFit提出了一种高效、支持多参考的虚拟试穿框架，通过创新的缓存扩散架构解决多参考组合和低效问题。研究中引入了DressCode-MR数据集以促进研究。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟试穿技术因无法支持多参考搭配以及多次冗余计算导致效率低下，限制了其实际应用。

Method: 通过引入缓存扩散架构、半注意力机制以及类嵌入替代传统时间步嵌入，FastFit成功实现了参考特征编码与去噪过程的完全解耦，同时引入DressCode-MR大规模数据集支持研究。

Result: FastFit在推理效率上达到平均提速3.5倍，并在VITON-HD、DressCode及DressCode-MR数据集上的关键保真度指标上优于最先进方法。

Conclusion: FastFit不仅显著提高了推理效率，还通过其数据集支持多参考虚拟搭配研究，在虚拟试穿领域展现出强大潜力。

Abstract: Despite its great potential, virtual try-on technology is hindered from
real-world application by two major challenges: the inability of current
methods to support multi-reference outfit compositions (including garments and
accessories), and their significant inefficiency caused by the redundant
re-computation of reference features in each denoising step. To address these
challenges, we propose FastFit, a high-speed multi-reference virtual try-on
framework based on a novel cacheable diffusion architecture. By employing a
Semi-Attention mechanism and substituting traditional timestep embeddings with
class embeddings for reference items, our model fully decouples reference
feature encoding from the denoising process with negligible parameter overhead.
This allows reference features to be computed only once and losslessly reused
across all steps, fundamentally breaking the efficiency bottleneck and
achieving an average 3.5x speedup over comparable methods. Furthermore, to
facilitate research on complex, multi-reference virtual try-on, we introduce
DressCode-MR, a new large-scale dataset. It comprises 28,179 sets of
high-quality, paired images covering five key categories (tops, bottoms,
dresses, shoes, and bags), constructed through a pipeline of expert models and
human feedback refinement. Extensive experiments on the VITON-HD, DressCode,
and our DressCode-MR datasets show that FastFit surpasses state-of-the-art
methods on key fidelity metrics while offering its significant advantage in
inference efficiency.

</details>


### [43] [UTA-Sign: Unsupervised Thermal Video Augmentation via Event-Assisted Traffic Signage Sketching](https://arxiv.org/abs/2508.20594)
*Yuqi Han,Songqian Zhang,Weijian Su,Ke Li,Jiayu Yang,Jinli Suo,Qiang Zhang*

Main category: cs.CV

TL;DR: 本论文介绍了一种融合热成像相机和神经形态视觉相机的创新方法，用于在低光条件下提升交通标志检测精度。


<details>
  <summary>Details</summary>
Motivation: 现代低光环境下的自动驾驶系统容易受热成像相机信号限制，导致语义理解不足。本研究旨在结合事件摄像头的优点，解决此问题。

Method: 提出了一种名为UTA-Sign的无监督热-事件视频增强方法，通过热帧与事件信号的双增强机制实现优化，利用热帧提供运动线索，事件信号增强细节。

Result: 实验验证方法在真实场景数据集中表现优异，提高了交通标志绘制质量和检测精确度。

Conclusion: 结合两种相机技术的互补特性，提升了低光环境下交通标志感知的鲁棒性，为智能驾驶提供了更安全的解决方案。

Abstract: The thermal camera excels at perceiving outdoor environments under low-light
conditions, making it ideal for applications such as nighttime autonomous
driving and unmanned navigation. However, thermal cameras encounter challenges
when capturing signage from objects made of similar materials, which can pose
safety risks for accurately understanding semantics in autonomous driving
systems. In contrast, the neuromorphic vision camera, also known as an event
camera, detects changes in light intensity asynchronously and has proven
effective in high-speed, low-light traffic environments. Recognizing the
complementary characteristics of these two modalities, this paper proposes
UTA-Sign, an unsupervised thermal-event video augmentation for traffic signage
in low-illumination environments, targeting elements such as license plates and
roadblock indicators. To address the signage blind spots of thermal imaging and
the non-uniform sampling of event cameras, we developed a dual-boosting
mechanism that fuses thermal frames and event signals for consistent signage
representation over time. The proposed method utilizes thermal frames to
provide accurate motion cues as temporal references for aligning the uneven
event signals. At the same time, event signals contribute subtle signage
content to the raw thermal frames, enhancing the overall understanding of the
environment. The proposed method is validated on datasets collected from
real-world scenarios, demonstrating superior quality in traffic signage
sketching and improved detection accuracy at the perceptual level.

</details>


### [44] [Disruptive Attacks on Face Swapping via Low-Frequency Perceptual Perturbations](https://arxiv.org/abs/2508.20595)
*Mengxiao Huang,Minglei Shu,Shuwang Zhou,Zhaoyang Liu*

Main category: cs.CV

TL;DR: 本文提出一种主动防御方法，通过低频感知扰动削弱深度伪造的效果，有效减少人脸置换操作的成功率，同时保持输出的视觉质量。


<details>
  <summary>Details</summary>
Motivation: 传统检测深度伪造的方法多为事后分析，无法有效预防攻击。为提高主动防御能力，亟需开发能够直接防御生成过程的新技术。

Method: 提出一种结合频域与空间域的主动防御方法，利用低频扰动破坏深度伪造生成过程，包括一套完整架构（编码器、扰动生成器和解码器），并基于离散小波变换（DWT）提取低频分量生成扰动。

Result: 实验表明，在CelebA-HQ和LFW数据集上显著降低了人脸置换的有效性，提高了防御成功率，同时保持了视觉质量。

Conclusion: 通过利用低频感知扰动可有效破坏深度伪造生成过程，为实现主动防御提供了新思路和可行方案。

Abstract: Deepfake technology, driven by Generative Adversarial Networks (GANs), poses
significant risks to privacy and societal security. Existing detection methods
are predominantly passive, focusing on post-event analysis without preventing
attacks. To address this, we propose an active defense method based on
low-frequency perceptual perturbations to disrupt face swapping manipulation,
reducing the performance and naturalness of generated content. Unlike prior
approaches that used low-frequency perturbations to impact classification
accuracy,our method directly targets the generative process of deepfake
techniques. We combine frequency and spatial domain features to strengthen
defenses. By introducing artifacts through low-frequency perturbations while
preserving high-frequency details, we ensure the output remains visually
plausible. Additionally, we design a complete architecture featuring an
encoder, a perturbation generator, and a decoder, leveraging discrete wavelet
transform (DWT) to extract low-frequency components and generate perturbations
that disrupt facial manipulation models. Experiments on CelebA-HQ and LFW
demonstrate significant reductions in face-swapping effectiveness, improved
defense success rates, and preservation of visual quality.

</details>


### [45] [Embracing Aleatoric Uncertainty: Generating Diverse 3D Human Motion](https://arxiv.org/abs/2508.20604)
*Zheng Qin,Yabing Wang,Minghui Yang,Sanping Zhou,Ming Yang,Le Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Diverse-T2M的简洁高效的文本到动作生成方法，通过引入不确定性和构建潜在空间，显著提高了生成动作的多样性，同时保持了与文本的一致性。


<details>
  <summary>Details</summary>
Motivation: 虽然近期的技术进步使得能够从文本中生成高质量的人体动作，但如何在生成动作中实现多样性依然是一个重大挑战。

Method: 提出Diverse-T2M方法，在Transformer方法中利用噪声信号作为多样性信息的载体，明确建模不确定性。构建一个潜在空间，将文本投射为连续表示，并整合潜在空间采样器以引入随机采样，从而提高生成动作的多样性和不确定性。

Result: 通过在HumanML3D和KIT-ML等文本到动作生成基准数据集上的实验，证明方法在显著提升多样性的同时保持了当前最佳的文本一致性性能。

Conclusion: Diverse-T2M方法有效解决了文本到动作生成任务中的多样性挑战，同时保证了生成动作与文本的语义一致，非常适合应用于需高灵活性的动作生成场景。

Abstract: Generating 3D human motions from text is a challenging yet valuable task. The
key aspects of this task are ensuring text-motion consistency and achieving
generation diversity. Although recent advancements have enabled the generation
of precise and high-quality human motions from text, achieving diversity in the
generated motions remains a significant challenge. In this paper, we aim to
overcome the above challenge by designing a simple yet effective text-to-motion
generation method, \textit{i.e.}, Diverse-T2M. Our method introduces
uncertainty into the generation process, enabling the generation of highly
diverse motions while preserving the semantic consistency of the text.
Specifically, we propose a novel perspective that utilizes noise signals as
carriers of diversity information in transformer-based methods, facilitating a
explicit modeling of uncertainty. Moreover, we construct a latent space where
text is projected into a continuous representation, instead of a rigid
one-to-one mapping, and integrate a latent space sampler to introduce
stochastic sampling into the generation process, thereby enhancing the
diversity and uncertainty of the outputs. Our results on text-to-motion
generation benchmark datasets~(HumanML3D and KIT-ML) demonstrate that our
method significantly enhances diversity while maintaining state-of-the-art
performance in text consistency.

</details>


### [46] [Optimization-Based Calibration for Intravascular Ultrasound Volume Reconstruction](https://arxiv.org/abs/2508.20605)
*Karl-Philippe Beaudet,Sidaty El Hadramy,Philippe C Cattin,Juan Verde,Stéphane Cotin*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D打印模型的优化校准方法，用于精确的3D血管内超声体积重建，从而在肝脏手术中实现术前CT与术中超声图像的注册。


<details>
  <summary>Details</summary>
Motivation: 在肝脏手术中，术中超声图像因视野有限和解剖结构复杂而难以解释，本文旨在通过术中3D血管内超声（IVUS）图像与术前CT扫描的注册来提高手术导航的精度。

Method: 通过使用3D打印的模型，提出了一种优化的校准方法，用以确保追踪到的IVUS数据与术前CT图像的精确对齐，从而实现精确的3D血管内超声体积重建。

Result: 通过对猪肝活体图像的验证，该方法实现了0.88到1.80毫米的校准误差以及3.40到5.71毫米的注册误差。

Conclusion: 这项方法提供了一种可靠且精准的校准和体积重建手段，可在肝脏手术中有效地将术中超声图像与术前CT数据进行精准注册，从而增强了术中导航的效果。

Abstract: Intraoperative ultrasound images are inherently challenging to interpret in
liver surgery due to the limited field of view and complex anatomical
structures. Bridging the gap between preoperative and intraoperative data is
crucial for effective surgical guidance. 3D IntraVascular UltraSound (IVUS)
offers a potential solution by enabling the reconstruction of the entire organ,
which facilitates registration between preoperative computed tomography (CT)
scans and intraoperative IVUS images. In this work, we propose an
optimization-based calibration method using a 3D-printed phantom for accurate
3D Intravascular Ultrasound volume reconstruction. Our approach ensures precise
alignment of tracked IVUS data with preoperative CT images, improving
intraoperative navigation. We validated our method using in vivo swine liver
images, achieving a calibration error from 0.88 to 1.80 mm and a registration
error from 3.40 to 5.71 mm between the 3D IVUS data and the corresponding CT
scan. Our method provides a reliable and accurate means of calibration and
volume reconstruction. It can be used to register intraoperative ultrasound
images with preoperative CT images in the context of liver surgery, and enhance
intraoperative guidance.

</details>


### [47] [Physics Informed Generative Models for Magnetic Field Images](https://arxiv.org/abs/2508.20612)
*Aye Phyu Phyu Aung,Lucas Lum,Zhansen Shi,Wen Qiu,Bernice Zee,JM Chin,Yeow Kheng Lim,J. Senthilnath*

Main category: cs.CV

TL;DR: 通过物理约束的扩散模型生成合成磁场图像（MFI），以高效定位半导体缺陷并优化训练数据。


<details>
  <summary>Details</summary>
Motivation: 解决MFI数据集因专有性限制而不足的问题，从而更有效地训练机器学习模型用于缺陷检测和定位。

Method: 提出名为Physics Informed Generative Models for Magnetic Field Images (PI-GenMFI)的方法，结合两种物理约束生成合成MFI样本，并针对电源短路等常见缺陷类型生成数据。

Result: 使用PI-GenMFI生成的MFI数据与现有最先进生成模型（变分自编码器和扩散方法）进行比较，域专家评估及定性和定量测试表明结果具有前景。

Conclusion: PI-GenMFI在磁场图像生成和缺陷定位训练中表现优异，可为缺陷检测优化流程提供帮助。

Abstract: In semiconductor manufacturing, defect detection and localization are
critical to ensuring product quality and yield. While X-ray imaging is a
reliable non-destructive testing method, it is memory-intensive and
time-consuming for large-scale scanning, Magnetic Field Imaging (MFI) offers a
more efficient means to localize regions of interest (ROI) for targeted X-ray
scanning. However, the limited availability of MFI datasets due to proprietary
concerns presents a significant bottleneck for training machine learning (ML)
models using MFI. To address this challenge, we consider an ML-driven approach
leveraging diffusion models with two physical constraints. We propose Physics
Informed Generative Models for Magnetic Field Images (PI-GenMFI) to generate
synthetic MFI samples by integrating specific physical information. We generate
MFI images for the most common defect types: power shorts. These synthetic
images will serve as training data for ML algorithms designed to localize
defect areas efficiently. To evaluate generated MFIs, we compare our model to
SOTA generative models from both variational autoencoder (VAE) and diffusion
methods. We present a domain expert evaluation to assess the generated samples.
In addition, we present qualitative and quantitative evaluation using various
metrics used for image generation and signal processing, showing promising
results to optimize the defect localization process.

</details>


### [48] [Revisiting the Privacy Risks of Split Inference: A GAN-Based Data Reconstruction Attack via Progressive Feature Optimization](https://arxiv.org/abs/2508.20613)
*Yixiang Qiu,Yanhan Liu,Hongyao Yu,Hao Fang,Bin Chen,Shu-Tao Xia,Ke Xu*

Main category: cs.CV

TL;DR: 文章研究深度神经网络分割推理的隐私风险，提出了一种基于生成对抗网络的攻击框架，提高重建攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有分割推理系统面临中间特征被利用进行数据重建的隐私威胁，当前的重建攻击效果有限且针对性强，迫切需要更通用和高效的攻击方法。

Method: 提出一种新颖的基于生成对抗网络（GAN）的数据重建攻击框架，结合渐进式特征优化（PFO）策略，将生成器分成分层模块逐步优化，并加入L1球约束提高优化稳定性和图像真实性。

Result: 实验表明，该方法在高分辨率、分布外数据及更深层模型上的重建效果明显优于现有方法。

Conclusion: 所提方法显著增强了对复杂深度神经网络分割推理过程中的隐私攻防能力，表明其在更广泛场景下的应用潜力。

Abstract: The growing complexity of Deep Neural Networks (DNNs) has led to the adoption
of Split Inference (SI), a collaborative paradigm that partitions computation
between edge devices and the cloud to reduce latency and protect user privacy.
However, recent advances in Data Reconstruction Attacks (DRAs) reveal that
intermediate features exchanged in SI can be exploited to recover sensitive
input data, posing significant privacy risks. Existing DRAs are typically
effective only on shallow models and fail to fully leverage semantic priors,
limiting their reconstruction quality and generalizability across datasets and
model architectures. In this paper, we propose a novel GAN-based DRA framework
with Progressive Feature Optimization (PFO), which decomposes the generator
into hierarchical blocks and incrementally refines intermediate representations
to enhance the semantic fidelity of reconstructed images. To stabilize the
optimization and improve image realism, we introduce an L1-ball constraint
during reconstruction. Extensive experiments show that our method outperforms
prior attacks by a large margin, especially in high-resolution scenarios,
out-of-distribution settings, and against deeper and more complex DNNs.

</details>


### [49] [EmoCAST: Emotional Talking Portrait via Emotive Text Description](https://arxiv.org/abs/2508.20615)
*Yiguo Jiang,Xiaodong Cun,Yong Zhang,Yudian Zheng,Fan Tang,Chi-Man Pun*

Main category: cs.CV

TL;DR: 研究提出了EmoCAST，一个能生成高质量情感人像视频的扩散框架，通过整合文本和构建情感数据集，大幅提升了情感表达和视频质量。


<details>
  <summary>Details</summary>
Motivation: 现有情感人像生成方法存在控制灵活性、运动自然性和表情质量的不足，同时数据集受限于实验室环境，影响了实际应用。

Method: 提出EmoCAST框架，包括文本引导的解耦情感模块和情感音频注意模块，并设计包含详细情感文本描述的数据集，结合情感感知采样和渐进式功能训练策略，提升模型性能。

Result: EmoCAST能够生成逼真、情感丰富且音频同步的口述人像视频，并达到领域内的最新水平。

Conclusion: 通过深度整合情感和音频特征，EmoCAST显著提升了情感口述视频的生成质量，为实际应用提供了更灵活有效的解决方案。

Abstract: Emotional talking head synthesis aims to generate talking portrait videos
with vivid expressions. Existing methods still exhibit limitations in control
flexibility, motion naturalness, and expression quality. Moreover, currently
available datasets are primarily collected in lab settings, further
exacerbating these shortcomings. Consequently, these limitations substantially
hinder practical applications in real-world scenarios. To address these
challenges, we propose EmoCAST, a diffusion-based framework with two key
modules for precise text-driven emotional synthesis. In appearance modeling,
emotional prompts are integrated through a text-guided decoupled emotive
module, enhancing the spatial knowledge to improve emotion comprehension. To
improve the relationship between audio and emotion, we introduce an emotive
audio attention module to capture the interplay between controlled emotion and
driving audio, generating emotion-aware features to guide more precise facial
motion synthesis. Additionally, we construct an emotional talking head dataset
with comprehensive emotive text descriptions to optimize the framework's
performance. Based on the proposed dataset, we propose an emotion-aware
sampling training strategy and a progressive functional training strategy that
further improve the model's ability to capture nuanced expressive features and
achieve accurate lip-synchronization. Overall, EmoCAST achieves
state-of-the-art performance in generating realistic, emotionally expressive,
and audio-synchronized talking-head videos. Project Page:
https://github.com/GVCLab/EmoCAST

</details>


### [50] [Mask-Guided Multi-Channel SwinUNETR Framework for Robust MRI Classification](https://arxiv.org/abs/2508.20621)
*Smriti Joshi,Lidia Garrucho,Richard Osuala,Oliver Diaz,Karim Lekadir*

Main category: cs.CV

TL;DR: 本文提出了一个基于SwinUNETR的深度学习框架用于乳腺癌的检测和分类，并在ODELIA挑战中取得了第二名。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是女性癌症相关死亡的主要原因之一，早期检测对改善预后至关重要；尤其是在高风险女性或乳腺组织致密的情况下，传统乳腺X光检查效果有限，MRI具有更高的敏感性。

Method: 提出一种SwinUNETR的深度学习框架，结合乳腺区域屏蔽、广泛的数据增强和集成学习，以提高模型的稳健性与泛化能力。

Result: 在ODELIA多中心挑战中获得第二名，表明方法在乳腺MRI临床解读中的潜力。

Conclusion: 研究展示了AI解决方案在乳腺MRI诊断中的应用潜力，代码已开源供研究人员使用。

Abstract: Breast cancer is one of the leading causes of cancer-related mortality in
women, and early detection is essential for improving outcomes. Magnetic
resonance imaging (MRI) is a highly sensitive tool for breast cancer detection,
particularly in women at high risk or with dense breast tissue, where
mammography is less effective. The ODELIA consortium organized a multi-center
challenge to foster AI-based solutions for breast cancer diagnosis and
classification. The dataset included 511 studies from six European centers,
acquired on scanners from multiple vendors at both 1.5 T and 3 T. Each study
was labeled for the left and right breast as no lesion, benign lesion, or
malignant lesion. We developed a SwinUNETR-based deep learning framework that
incorporates breast region masking, extensive data augmentation, and ensemble
learning to improve robustness and generalizability. Our method achieved second
place on the challenge leaderboard, highlighting its potential to support
clinical breast MRI interpretation. We publicly share our codebase at
https://github.com/smriti-joshi/bcnaim-odelia-challenge.git.

</details>


### [51] [AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View Images](https://arxiv.org/abs/2508.20623)
*Shiqi Xin,Xiaolin Zhang,Yanbin Liu,Peng Zhang,Caifeng Shan*

Main category: cs.CV

TL;DR: AvatarBack提出了一种框架，解决基于3D高斯计算的头像建模中后脑区域构建不完整的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的3D头像建模方法主要依赖正面视图数据，导致后脑区域几何不一致及低现实感，限制了重建头像的逼真度。

Method: 引入AvatarBack框架，结合主体特定生成器（SSG）生成背面伪图像，并采用自适应空间对齐策略（ASA）优化几何对齐。

Result: 实验显示，AvatarBack显著提升了后脑区域的重建质量，同时保持了正面区域的清晰；重建后的头像在多种动作下保持一致的视觉真实性并完全可动画化。

Conclusion: AvatarBack有效解决了3D头像建模中后脑构建不足的问题，成功实现了完整一致的头像重建。

Abstract: Recent advances in Gaussian Splatting have significantly boosted the
reconstruction of head avatars, enabling high-quality facial modeling by
representing an 3D avatar as a collection of 3D Gaussians. However, existing
methods predominantly rely on frontal-view images, leaving the back-head poorly
constructed. This leads to geometric inconsistencies, structural blurring, and
reduced realism in the rear regions, ultimately limiting the fidelity of
reconstructed avatars. To address this challenge, we propose AvatarBack, a
novel plug-and-play framework specifically designed to reconstruct complete and
consistent 3D Gaussian avatars by explicitly modeling the missing back-head
regions. AvatarBack integrates two core technical innovations,i.e., the
Subject-specific Generator (SSG) and the Adaptive Spatial Alignment Strategy
(ASA). The former leverages a generative prior to synthesize
identity-consistent, plausible back-view pseudo-images from sparse frontal
inputs, providing robust multi-view supervision. To achieve precise geometric
alignment between these synthetic views and the 3D Gaussian representation, the
later employs learnable transformation matrices optimized during training,
effectively resolving inherent pose and coordinate discrepancies. Extensive
experiments on NeRSemble and K-hairstyle datasets, evaluated using geometric,
photometric, and GPT-4o-based perceptual metrics, demonstrate that AvatarBack
significantly enhances back-head reconstruction quality while preserving
frontal fidelity. Moreover, the reconstructed avatars maintain consistent
visual realism under diverse motions and remain fully animatable.

</details>


### [52] [ArtFace: Towards Historical Portrait Face Identification via Model Adaptation](https://arxiv.org/abs/2508.20626)
*Francois Poh,Anjith George,Sébastien Marcel*

Main category: cs.CV

TL;DR: 本文研究如何通过基础模型提升艺术作品中的面部识别效果，以解决传统方法在绘画作品中的不足。


<details>
  <summary>Details</summary>
Motivation: 通过提高艺术作品中的面部识别效果，为艺术史学家提供进一步理解画中人物与其自我呈现方式的工具。

Method: 通过微调基础模型，将其嵌入与传统面部识别模型结合以改进表现。

Result: 改进后的方法在艺术作品面部识别任务上显著优于现有最佳方法。

Conclusion: 基础模型能够在传统模型无效时弥补不足，提升艺术领域的面部识别能力，表明其在这一应用上的潜力。

Abstract: Identifying sitters in historical paintings is a key task for art historians,
offering insight into their lives and how they chose to be seen. However, the
process is often subjective and limited by the lack of data and stylistic
variations. Automated facial recognition is capable of handling challenging
conditions and can assist, but while traditional facial recognition models
perform well on photographs, they struggle with paintings due to domain shift
and high intra-class variation. Artistic factors such as style, skill, intent,
and influence from other works further complicate recognition. In this work, we
investigate the potential of foundation models to improve facial recognition in
artworks. By fine-tuning foundation models and integrating their embeddings
with those from conventional facial recognition networks, we demonstrate
notable improvements over current state-of-the-art methods. Our results show
that foundation models can bridge the gap where traditional methods are
ineffective. Paper page at https://www.idiap.ch/paper/artface/

</details>


### [53] [CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models](https://arxiv.org/abs/2508.20640)
*Ayan Banerjee,Fernando Vilariño,Josep Lladós*

Main category: cs.CV

TL;DR: CraftGraffiti是一种面向文本的涂鸦生成系统，能在极端风格转换下保留面部特征识别，提供动态的姿态自定义，并通过实证验证其在风格和身份保真间平衡的有效性。


<details>
  <summary>Details</summary>
Motivation: 在艺术生成中，有极端风格变换时仍保留主体面部特征至关重要，然而在抽象、高对比场景下（如涂鸦），面部特征细节的偏差会导致个人与文化身份的丧失。

Method: 系统首先通过LoRA微调的预训练扩散变换器实现涂鸦风格迁移；然后通过人脸一致的自注意机制内嵌显性身份嵌入以增强身份保真性；同时利用CLIP引导的文本提示扩展实现无关键点的动态姿态自定义。

Result: 实验验证了“先风格后身份”的范式减小了特征漂移，比对顺序相反的方法更具优势，在面部特征一致性、美感和用户偏好评测上取得了高分。同时，其实际应用（如在Cruilla节日的现场部署）也印证了其创造性价值。

Conclusion: CraftGraffiti推动了尊重身份的AI辅助艺术创作，为在创意AI应用中兼顾风格自由和身份识别提供了一种科学的方法。

Abstract: Preserving facial identity under extreme stylistic transformation remains a
major challenge in generative art. In graffiti, a high-contrast, abstract
medium, subtle distortions to the eyes, nose, or mouth can erase the subject's
recognizability, undermining both personal and cultural authenticity. We
present CraftGraffiti, an end-to-end text-guided graffiti generation framework
designed with facial feature preservation as a primary objective. Given an
input image and a style and pose descriptive prompt, CraftGraffiti first
applies graffiti style transfer via LoRA-fine-tuned pretrained diffusion
transformer, then enforces identity fidelity through a face-consistent
self-attention mechanism that augments attention layers with explicit identity
embeddings. Pose customization is achieved without keypoints, using CLIP-guided
prompt extension to enable dynamic re-posing while retaining facial coherence.
We formally justify and empirically validate the "style-first, identity-after"
paradigm, showing it reduces attribute drift compared to the reverse order.
Quantitative results demonstrate competitive facial feature consistency and
state-of-the-art aesthetic and human preference scores, while qualitative
analyses and a live deployment at the Cruilla Festival highlight the system's
real-world creative impact. CraftGraffiti advances the goal of
identity-respectful AI-assisted artistry, offering a principled approach for
blending stylistic freedom with recognizability in creative AI applications.

</details>


### [54] [Improving Alignment in LVLMs with Debiased Self-Judgment](https://arxiv.org/abs/2508.20655)
*Sihan Yang,Chenhang Cui,Zihao Zhao,Yiyang Zhou,Weilong Yan,Ying Wei,Huaxiu Yao*

Main category: cs.CV

TL;DR: 提出了一种新的方法，通过模型内部生成的去偏自评得分（无需外部资源），解决大语言模型和视-语言模型的视觉与语言模态对齐问题，显著减少幻觉现象并加强安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐方法（如指令微调和偏好微调）局限性大，依赖外部数据集和人工标注，成本较高且扩展性差，亟需寻找更高效的方法。

Method: 通过提出一种去偏自评得分的生成方法，依靠模型自身的能力进行对齐提升，无需外部数据或复杂后处理。

Result: 实验结果表明，提出的方法在减少幻觉现象、提高安全性和整体能力方面明显优于传统方法。

Conclusion: 该方法为视觉与语言模态对齐提供了一种更高效、成本更低的解决方案，提升了模型的实用性与安全性。

Abstract: The rapid advancements in Large Language Models (LLMs) and Large
Visual-Language Models (LVLMs) have opened up new opportunities for integrating
visual and linguistic modalities. However, effectively aligning these
modalities remains challenging, often leading to hallucinations--where
generated outputs are not grounded in the visual input--and raising safety
concerns across various domains. Existing alignment methods, such as
instruction tuning and preference tuning, often rely on external datasets,
human annotations, or complex post-processing, which limit scalability and
increase costs. To address these challenges, we propose a novel approach that
generates the debiased self-judgment score, a self-evaluation metric created
internally by the model without relying on external resources. This enables the
model to autonomously improve alignment. Our method enhances both decoding
strategies and preference tuning processes, resulting in reduced
hallucinations, enhanced safety, and improved overall capability. Empirical
results show that our approach significantly outperforms traditional methods,
offering a more effective solution for aligning LVLMs.

</details>


### [55] [MobileCLIP2: Improving Multi-Modal Reinforced Training](https://arxiv.org/abs/2508.20691)
*Fartash Faghri,Pavan Kumar Anasosalu Vasu,Cem Koc,Vaishaal Shankar,Alexander Toshev,Oncel Tuzel,Hadi Pouransari*

Main category: cs.CV

TL;DR: 本研究旨在优化移动零样本图文模型MobileCLIP，通过改进多模态强化训练方法，提高图像分类准确性和速度，并公布了预训练模型和数据生成代码。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是提升MobileCLIP模型的性能，同时确保其低延迟的特性，以满足高效图文任务的需求。

Method: 通过增强多模态强化训练，包括：1. 使用在DFN数据集上训练的CLIP教师模型；2. 改进的标注生成器模型；3. 对比蒸馏中的温度调节、标注多样性的优化及模型合成标注的结合等优化策略。

Result: 新一代模型MobileCLIP2在ImageNet-1k上的零样本准确性提高了2.2%，小型版本在低延迟环境下与更大模型表现相当或优越，同时显著降低了延迟和规模。

Conclusion: MobileCLIP2成功实现了零样本图文模型性能和效率的突破，并提供了可扩展的数据生成工具，为进一步研究提供了开放资源。

Abstract: Foundation image-text models such as CLIP with zero-shot capabilities enable
a wide array of applications. MobileCLIP is a recent family of image-text
models at 3-15ms latency and 50-150M parameters with state-of-the-art zero-shot
accuracy. The main ingredients in MobileCLIP were its low-latency and light
architectures and a novel multi-modal reinforced training that made knowledge
distillation from multiple caption-generators and CLIP teachers efficient,
scalable, and reproducible. In this paper, we improve the multi-modal
reinforced training of MobileCLIP through: 1) better CLIP teacher ensembles
trained on the DFN dataset, 2) improved captioner teachers trained on the DFN
dataset and fine-tuned on a diverse selection of high-quality image-caption
datasets. We discover new insights through ablations such as the importance of
temperature tuning in contrastive knowledge distillation, the effectiveness of
caption-generator fine-tuning for caption diversity, and the additive
improvement from combining synthetic captions generated by multiple models. We
train a new family of models called MobileCLIP2 and achieve state-of-the-art
ImageNet-1k zero-shot accuracies at low latencies. In particular, we observe
2.2% improvement in ImageNet-1k accuracy for MobileCLIP2-B compared with
MobileCLIP-B architecture. Notably, MobileCLIP2-S4 matches the zero-shot
accuracy of SigLIP-SO400M/14 on ImageNet-1k while being 2$\times$ smaller and
improves on DFN ViT-L/14 at 2.5$\times$ lower latency. We release our
pretrained models (https://github.com/apple/ml-mobileclip) and the data
generation code (https://github.com/apple/ml-mobileclip-dr). The data
generation code makes it easy to create new reinforced datasets with arbitrary
teachers using distributed scalable processing.

</details>


### [56] ["Humor, Art, or Misinformation?": A Multimodal Dataset for Intent-Aware Synthetic Image Detection](https://arxiv.org/abs/2508.20670)
*Anastasios Skoularikis,Stefanos-Iordanis Papadopoulos,Symeon Papadopoulos,Panagiotis C. Petrantonakis*

Main category: cs.CV

TL;DR: 论文提出了S-HArM，一个能区分AI生成内容意图的多模态数据集，并探索不同的数据集构造策略和模型架构优化，但意图推断仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成内容检测方法忽视了生成内容背后的意图，亟需开发能感知意图的工具。

Method: 提出S-HArM数据集（包含Humor、Art和Misinformation三类），并探索三种提示式数据构造策略，再结合多种模型架构进行性能评估。

Result: 模型在图像引导和多模态引导的数据上表现更好，但整体性能有限，说明意图推断具有复杂性。

Conclusion: 意图推断需结合保留视觉上下文的数据和专用建筑设计，未来需进一步优化模型以提升对“野外内容”的准确分类。

Abstract: Recent advances in multimodal AI have enabled progress in detecting synthetic
and out-of-context content. However, existing efforts largely overlook the
intent behind AI-generated images. To fill this gap, we introduce S-HArM, a
multimodal dataset for intent-aware classification, comprising 9,576 "in the
wild" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art,
or Misinformation. Additionally, we explore three prompting strategies
(image-guided, description-guided, and multimodally-guided) to construct a
large-scale synthetic training dataset with Stable Diffusion. We conduct an
extensive comparative study including modality fusion, contrastive learning,
reconstruction networks, attention mechanisms, and large vision-language
models. Our results show that models trained on image- and multimodally-guided
data generalize better to "in the wild" content, due to preserved visual
context. However, overall performance remains limited, highlighting the
complexity of inferring intent and the need for specialized architectures.

</details>


### [57] [Learned Rate Control for Frame-Level Adaptive Neural Video Compression via Dynamic Neural Network](https://arxiv.org/abs/2508.20709)
*Chenhao Zhang,Wei Gao*

Main category: cs.CV

TL;DR: 提出了一个针对变码率场景的动态视频压缩框架，通过动态编码路径和关节路优化实现精确码率控制，并显著提升了RD性能。


<details>
  <summary>Details</summary>
Motivation: 解决神经视频压缩算法在精确码率控制上的挑战，满足变码率场景需求。

Method: 设计动态路径自编码器（Dynamic-Route Autoencoder），实现变量编码路径。同时提出率控制代理（Rate Control Agent）以实时调整编码路径，并通过联合路径优化策略进行路径协同训练。

Result: 实验表明，该方法在HEVC和UVG数据集上实现了平均14.8%的BD-Rate降低和0.47dB的BD-PSNR提升，同时平均码率误差为1.66%。

Conclusion: 该方法在不同码率需求下实现了率失真复杂度优化（RDCO），表现优于最先进方法并适合码率受限的应用场景。

Abstract: Neural Video Compression (NVC) has achieved remarkable performance in recent
years. However, precise rate control remains a challenge due to the inherent
limitations of learning-based codecs. To solve this issue, we propose a dynamic
video compression framework designed for variable bitrate scenarios. First, to
achieve variable bitrate implementation, we propose the Dynamic-Route
Autoencoder with variable coding routes, each occupying partial computational
complexity of the whole network and navigating to a distinct RD trade-off.
Second, to approach the target bitrate, the Rate Control Agent estimates the
bitrate of each route and adjusts the coding route of DRA at run time. To
encompass a broad spectrum of variable bitrates while preserving overall RD
performance, we employ the Joint-Routes Optimization strategy, achieving
collaborative training of various routes. Extensive experiments on the HEVC and
UVG datasets show that the proposed method achieves an average BD-Rate
reduction of 14.8% and BD-PSNR gain of 0.47dB over state-of-the-art methods
while maintaining an average bitrate error of 1.66%, achieving
Rate-Distortion-Complexity Optimization (RDCO) for various bitrate and
bitrate-constrained applications. Our code is available at
https://git.openi.org.cn/OpenAICoding/DynamicDVC.

</details>


### [58] [CardioMorphNet: Cardiac Motion Prediction Using a Shape-Guided Bayesian Recurrent Deep Network](https://arxiv.org/abs/2508.20734)
*Reza Akbari Movahed,Abuzar Rezaee,Arezoo Zakeri,Colin Berry,Edmond S. L. Ho,Ali Gooya*

Main category: cs.CV

TL;DR: 该论文提出了一种名为CardioMorphNet的深度学习框架，用于通过形状引导的可变形配准提升心脏运动估计的准确性，其表现优于现有标准方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在心脏运动估计中依赖基于强度的图像配准相似性损失，可能忽略心脏解剖区域，导致运动捕获不准确。

Method: 提出了一个递归贝叶斯深度学习框架CardioMorphNet，利用递归变分自编码器建模心脏周期中的时空依赖，同时集成了双心室分割与运动估计的后验模型，通过贝叶斯公式的损失函数聚焦解剖区域，并排除了基于强度的相似性损失。该结构还基于时序短轴体积和时空特征进行调整。

Result: 实验在UK Biobank数据集上验证。CardioMorphNet的配准性能优于现有方法，并且心脏区域的运动场预测相比其他方法表现出更低的不确定性，可靠性更高。

Conclusion: CardioMorphNet通过形状引导的贝叶斯模型显著提升了心脏运动估计的准确性和预测信心，可为心脏功能评估提供有力支持。

Abstract: Accurate cardiac motion estimation from cine cardiac magnetic resonance (CMR)
images is vital for assessing cardiac function and detecting its abnormalities.
Existing methods often struggle to capture heart motion accurately because they
rely on intensity-based image registration similarity losses that may overlook
cardiac anatomical regions. To address this, we propose CardioMorphNet, a
recurrent Bayesian deep learning framework for 3D cardiac shape-guided
deformable registration using short-axis (SAX) CMR images. It employs a
recurrent variational autoencoder to model spatio-temporal dependencies over
the cardiac cycle and two posterior models for bi-ventricular segmentation and
motion estimation. The derived loss function from the Bayesian formulation
guides the framework to focus on anatomical regions by recursively registering
segmentation maps without using intensity-based image registration similarity
loss, while leveraging sequential SAX volumes and spatio-temporal features. The
Bayesian modelling also enables computation of uncertainty maps for the
estimated motion fields. Validated on the UK Biobank dataset by comparing
warped mask shapes with ground truth masks, CardioMorphNet demonstrates
superior performance in cardiac motion estimation, outperforming
state-of-the-art methods. Uncertainty assessment shows that it also yields
lower uncertainty values for estimated motion fields in the cardiac region
compared with other probabilistic-based cardiac registration methods,
indicating higher confidence in its predictions.

</details>


### [59] [ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering](https://arxiv.org/abs/2508.21010)
*Paritosh Parmar,Eric Peh,Basura Fernando*

Main category: cs.CV

TL;DR: 现有的视频问答模型在高阶推理中表现欠佳，依赖于不透明的单一管道。本研究提出一种模块化框架，通过因果链解耦推理与答案生成，提升理解性和逻辑性。


<details>
  <summary>Details</summary>
Motivation: 现有模型依赖浅层启发式方法，缺乏解释性，难以处理高阶因果推理需求。

Method: 提出两阶段架构：因果链提取器（CCE）从视频与问题中生成明确的因果链；因果链驱动回答器（CCDA）根据因果链生成答案，并通过语言模型生成高质量因果链。

Result: 方法在三个大规模基准测试中优于现有模型，在可解释性、用户信任和泛化能力上均显著提高。

Conclusion: 通过引入明确的因果链及新的评测指标，该框架不仅提升性能，还使CCE具有多领域可重用性，对研究工具化因果推理具有重要意义。

Abstract: Existing Causal-Why Video Question Answering (VideoQA) models often struggle
with higher-order reasoning, relying on opaque, monolithic pipelines that
entangle video understanding, causal inference, and answer generation. These
black-box approaches offer limited interpretability and tend to depend on
shallow heuristics. We propose a novel, modular framework that explicitly
decouples causal reasoning from answer generation, introducing natural language
causal chains as interpretable intermediate representations. Inspired by human
cognitive models, these structured cause-effect sequences bridge low-level
video content with high-level causal reasoning, enabling transparent and
logically coherent inference. Our two-stage architecture comprises a Causal
Chain Extractor (CCE) that generates causal chains from video-question pairs,
and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in
these chains. To address the lack of annotated reasoning traces, we introduce a
scalable method for generating high-quality causal chains from existing
datasets using large language models. We also propose CauCo, a new evaluation
metric for causality-oriented captioning. Experiments on three large-scale
benchmarks demonstrate that our approach not only outperforms state-of-the-art
models, but also yields substantial gains in explainability, user trust, and
generalization -- positioning the CCE as a reusable causal reasoning engine
across diverse domains. Project page:
https://paritoshparmar.github.io/chainreaction/

</details>


### [60] [Mix, Align, Distil: Reliable Cross-Domain Atypical Mitosis Classification](https://arxiv.org/abs/2508.20745)
*Kaustubh Atey,Sameer Anand Jha,Gouranga Bala,Amit Sethi*

Main category: cs.CV

TL;DR: 提出了一种域鲁棒性增强方法，用于AMF分类，在MIDOG 2025 Task 2中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决AMF分类中的域迁移挑战，特别是在扫描仪、染色和采集差异导致的问题。

Method: 通过风格扰动增加特征多样性，使用弱域标签对齐注意力优化特征，并结合EMA教师的蒸馏过程以稳定预测。

Result: 在赛题分类指标中表现优秀，平衡准确率达到0.8762，灵敏度0.8873，特异性0.8651，ROC AUC达0.9499。

Conclusion: 方法高效且性能强大，可作为MIDOG 2025竞赛的有力方案。

Abstract: Atypical mitotic figures (AMFs) are important histopathological markers yet
remain challenging to identify consistently, particularly under domain shift
stemming from scanner, stain, and acquisition differences. We present a simple
training-time recipe for domain-robust AMF classification in MIDOG 2025 Task 2.
The approach (i) increases feature diversity via style perturbations inserted
at early and mid backbone stages, (ii) aligns attention-refined features across
sites using weak domain labels (Scanner, Origin, Species, Tumor) through an
auxiliary alignment loss, and (iii) stabilizes predictions by distilling from
an exponential moving average (EMA) teacher with temperature-scaled KL
divergence. On the organizer-run preliminary leaderboard for atypical mitosis
classification, our submission attains balanced accuracy of 0.8762, sensitivity
of 0.8873, specificity of 0.8651, and ROC AUC of 0.9499. The method incurs
negligible inference-time overhead, relies only on coarse domain metadata, and
delivers strong, balanced performance, positioning it as a competitive
submission for the MIDOG 2025 challenge.

</details>


### [61] [Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning](https://arxiv.org/abs/2508.20751)
*Yibin Wang,Zhimin Li,Yuhang Zang,Yujie Zhou,Jiazi Bu,Chunyu Wang,Qinglin Lu,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 提出一种基于偏好奖励的Pref-GRPO方法，解决了当前T2I生成中的奖励滥用问题，并引入了UniGenBench基准以全面评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于单点奖励模型的T2I生成方法中因奖励范式问题导致的不稳定性和奖励滥用现象。

Method: 提出Pref-GRPO方法，通过基于偏好的奖励模型进行训练，结合成对比较机制，减少奖励滥用风险。并开发了UniGenBench基准，覆盖广泛的图片生成场景和评价标准。

Result: Pref-GRPO成功缓解了奖励滥用问题，提高了生成质量稳定性；UniGenBench基准更全面地识别了T2I模型的优劣。

Conclusion: 文章提供了在T2I生成任务中更稳健的训练方法和评估范式，为领域发展提供有力工具。

Abstract: Recent advancements highlight the importance of GRPO-based reinforcement
learning methods and benchmarking in enhancing text-to-image (T2I) generation.
However, current methods using pointwise reward models (RM) for scoring
generated images are susceptible to reward hacking. We reveal that this happens
when minimal score differences between images are amplified after
normalization, creating illusory advantages that drive the model to
over-optimize for trivial gains, ultimately destabilizing the image generation
process. To address this, we propose Pref-GRPO, a pairwise preference
reward-based GRPO method that shifts the optimization objective from score
maximization to preference fitting, ensuring more stable training. In
Pref-GRPO, images are pairwise compared within each group using preference RM,
and the win rate is used as the reward signal. Extensive experiments
demonstrate that PREF-GRPO differentiates subtle image quality differences,
providing more stable advantages and mitigating reward hacking. Additionally,
existing T2I benchmarks are limited by coarse evaluation criteria, hindering
comprehensive model assessment. To solve this, we introduce UniGenBench, a
unified T2I benchmark comprising 600 prompts across 5 main themes and 20
subthemes. It evaluates semantic consistency through 10 primary and 27
sub-criteria, leveraging MLLM for benchmark construction and evaluation. Our
benchmarks uncover the strengths and weaknesses of both open and closed-source
T2I models and validate the effectiveness of Pref-GRPO.

</details>


### [62] [${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale Feature for Generalizable Gaussian Splatting](https://arxiv.org/abs/2508.20754)
*Yuxi Hu,Jun Zhang,Kuangyi Chen,Zhe Zhang,Friedrich Fraundorfer*

Main category: cs.CV

TL;DR: 本文提出了一个名为C3-GS的新框架，提高了稀疏视图下的新视图合成性能，通过轻量级模块增强了特征学习和融合，无需逐场景优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法在稀疏视图下难以编码判别性的、多视图一致的特征，导致几何结构不准确。提出的框架旨在解决这一瓶颈。

Method: 设计了一个结合上下文感知、跨维度和跨尺度约束的框架C3-GS，并集成了三个轻量级模块以提升特征学习和融合能力。

Result: C3-GS在基准数据集上实现了最先进的渲染质量和泛化能力。

Conclusion: C3-GS能够在没有附加监督的情况下完成逼真的新视图合成，为新场景渲染提供了高效的解决方案。

Abstract: Generalizable Gaussian Splatting aims to synthesize novel views for unseen
scenes without per-scene optimization. In particular, recent advancements
utilize feed-forward networks to predict per-pixel Gaussian parameters,
enabling high-quality synthesis from sparse input views. However, existing
approaches fall short in encoding discriminative, multi-view consistent
features for Gaussian predictions, which struggle to construct accurate
geometry with sparse views. To address this, we propose $\mathbf{C}^{3}$-GS, a
framework that enhances feature learning by incorporating context-aware,
cross-dimension, and cross-scale constraints. Our architecture integrates three
lightweight modules into a unified rendering pipeline, improving feature fusion
and enabling photorealistic synthesis without requiring additional supervision.
Extensive experiments on benchmark datasets validate that $\mathbf{C}^{3}$-GS
achieves state-of-the-art rendering quality and generalization ability. Code is
available at: https://github.com/YuhsiHu/C3-GS.

</details>


### [63] [SeqVLM: Proposal-Guided Multi-View Sequences Reasoning via VLM for Zero-Shot 3D Visual Grounding](https://arxiv.org/abs/2508.20758)
*Jiawen Lin,Shiran Bian,Yihang Zhu,Wenbin Tan,Yachao Zhang,Yuan Xie,Yanyun Qu*

Main category: cs.CV

TL;DR: 本文提出SeqVLM，一个用于零样本3D视觉定位的新框架，通过结合多视图真实场景图像和空间信息显著提高了目标对象的推理能力，并在ScanRefer和Nr3D基准上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前零样本3D视觉定位方法在面对空间推理限制和上下文细节不足方面存在挑战，为实现更高的泛化能力和实际应用价值，提出新方法迫在眉睫。

Method: SeqVLM首先通过3D语义分割网络生成3D实例提案并通过语义过滤进行精炼。然后采用提案引导的多视图投影策略将候选提案投影到真实场景图像序列中，保持空间关系和上下文细节。此外，采用动态调度机制迭代处理序列-查询提示，以利用VLM的跨模态推理能力定位文本指定的对象。

Result: 在ScanRefer和Nr3D基准上的实验显示，此方法在Acc@0.25上的得分分别为55.6%和53.2%，超越了之前的零样本方法4.0%和5.2%。

Conclusion: SeqVLM成功地通过结合多视图场景信息和动态调度跨模态推理机制显著提升了零样本3D视觉定位的性能，为进一步实现泛化和实际应用提供了新的可能性。

Abstract: 3D Visual Grounding (3DVG) aims to localize objects in 3D scenes using
natural language descriptions. Although supervised methods achieve higher
accuracy in constrained settings, zero-shot 3DVG holds greater promise for
real-world applications since eliminating scene-specific training requirements.
However, existing zero-shot methods face challenges of spatial-limited
reasoning due to reliance on single-view localization, and contextual omissions
or detail degradation. To address these issues, we propose SeqVLM, a novel
zero-shot 3DVG framework that leverages multi-view real-world scene images with
spatial information for target object reasoning. Specifically, SeqVLM first
generates 3D instance proposals via a 3D semantic segmentation network and
refines them through semantic filtering, retaining only semantic-relevant
candidates. A proposal-guided multi-view projection strategy then projects
these candidate proposals onto real scene image sequences, preserving spatial
relationships and contextual details in the conversion process of 3D point
cloud to images. Furthermore, to mitigate VLM computational overload, we
implement a dynamic scheduling mechanism that iteratively processes
sequances-query prompts, leveraging VLM's cross-modal reasoning capabilities to
identify textually specified objects. Experiments on the ScanRefer and Nr3D
benchmarks demonstrate state-of-the-art performance, achieving Acc@0.25 scores
of 55.6% and 53.2%, surpassing previous zero-shot methods by 4.0% and 5.2%,
respectively, which advance 3DVG toward greater generalization and real-world
applicability. The code is available at https://github.com/JiawLin/SeqVLM.

</details>


### [64] [Occlusion Robustness of CLIP for Military Vehicle Classification](https://arxiv.org/abs/2508.20760)
*Jan Erik van Woerden,Gertjan Burghouts,Lotte Nijskens,Alma M. Liezenga,Sabina van Rooij,Frank Ruis,Hugo J. Kuijf*

Main category: cs.CV

TL;DR: 研究CLIP模型在军事环境中的鲁棒性，对部分遮挡和降噪情况下的性能表现展开评估，发现特定的增强和模型微调可以提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究现有CLIP模型在军事场景中对部分遮挡和低信噪比情况的适应性，解决数据标注稀缺性带来的瓶颈。

Method: 使用18种军用车辆类别的自定义数据集，通过Normalized Area Under the Curve（NAUC）评估模型在不同遮挡比例下的鲁棒性表现，并测试Transformer和CNN架构的效果。

Result: (1) Transformer架构优于CNN；(2) 细粒度分散遮挡比大块遮挡影响更大；(3) 线性微调模型在遮挡率达到35%时性能大幅下降；(4) 通过微调模型主干网络，可将下降临界点推延至60%遮挡。

Conclusion: 数据增强策略在遮挡场景下至关重要，未来需要关注模型对局部遮挡的敏感性以及结构的鲁棒性，以实现模型在实际军事环境中的应用。

Abstract: Vision-language models (VLMs) like CLIP enable zero-shot classification by
aligning images and text in a shared embedding space, offering advantages for
defense applications with scarce labeled data. However, CLIP's robustness in
challenging military environments, with partial occlusion and degraded
signal-to-noise ratio (SNR), remains underexplored. We investigate CLIP
variants' robustness to occlusion using a custom dataset of 18 military vehicle
classes and evaluate using Normalized Area Under the Curve (NAUC) across
occlusion percentages. Four key insights emerge: (1) Transformer-based CLIP
models consistently outperform CNNs, (2) fine-grained, dispersed occlusions
degrade performance more than larger contiguous occlusions, (3) despite
improved accuracy, performance of linear-probed models sharply drops at around
35% occlusion, (4) by finetuning the model's backbone, this performance drop
occurs at more than 60% occlusion. These results underscore the importance of
occlusion-specific augmentations during training and the need for further
exploration into patch-level sensitivity and architectural resilience for
real-world deployment of CLIP.

</details>


### [65] [SKGE-SWIN: End-To-End Autonomous Vehicle Waypoint Prediction and Navigation Using Skip Stage Swin Transformer](https://arxiv.org/abs/2508.20762)
*Fachri Najm Noer Kartiman,Rasim,Yaya Wihardi,Nurul Hasanah,Oskar Natan,Bambang Wahono,Taufik Ibnu Salim*

Main category: cs.CV

TL;DR: 本研究提出了一种名为SKGE-Swin的端到端自动驾驶模型，利用Swin Transformer与跳跃级机制实现从像素到像素的上下文感知，并在CARLA平台上测试获得了优异的驾驶评分。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种能够更好地感知车辆周边复杂模式的端到端自动驾驶模型，提高对像素间远距离信息的提取能力。

Method: 提出使用Swin Transformer结合跳跃级机制，通过Shifted Window多头自注意力（SW-MSA）拓展特征表示范围，并保留从初始到最终特征提取的关键信息。

Result: 实验表明，SKGE-Swin架构在CARLA平台的对抗场景中表现优异，比此前方法获得更高的驾驶分数。

Conclusion: 该模型通过使用Swin Transformer与跳跃连接改进了性能，可以更好地理解车辆周围的复杂模式，为实现端到端自动驾驶提供了一种新颖高效的架构。

Abstract: Focusing on the development of an end-to-end autonomous vehicle model with
pixel-to-pixel context awareness, this research proposes the SKGE-Swin
architecture. This architecture utilizes the Swin Transformer with a skip-stage
mechanism to broaden feature representation globally and at various network
levels. This approach enables the model to extract information from distant
pixels by leveraging the Swin Transformer's Shifted Window-based Multi-head
Self-Attention (SW-MSA) mechanism and to retain critical information from the
initial to the final stages of feature extraction, thereby enhancing its
capability to comprehend complex patterns in the vehicle's surroundings. The
model is evaluated on the CARLA platform using adversarial scenarios to
simulate real-world conditions. Experimental results demonstrate that the
SKGE-Swin architecture achieves a superior Driving Score compared to previous
methods. Furthermore, an ablation study will be conducted to evaluate the
contribution of each architectural component, including the influence of skip
connections and the use of the Swin Transformer, in improving model
performance.

</details>


### [66] [Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for Video Understanding](https://arxiv.org/abs/2508.20765)
*Gowreesh Mago,Pascal Mettes,Stevan Rudinac*

Main category: cs.CV

TL;DR: 本文探讨了视频内容的抽象概念理解问题，研究了基础模型在这一问题上的应用，并对相关任务和数据集进行了综述。


<details>
  <summary>Details</summary>
Motivation: 尽管机器在理解具体的可见视频内容方面已有显著进展，但对抽象概念的识别能力仍不足，而这对实现更接近人类推理的模型至关重要。

Method: 本文采用对比分析的方法，回顾和总结了已有的任务与数据集，并探讨基础模型在这一领域的潜力。

Result: 研究表明，社区在长期实践中积累了丰富经验，以基础模型为基础的方法能更好地解决抽象概念理解问题。

Conclusion: 文章认为，通过借鉴多年来的研究经验，并结合最新基础模型的优势，可以更有效地解决视频抽象概念理解的挑战。

Abstract: The automatic understanding of video content is advancing rapidly. Empowered
by deeper neural networks and large datasets, machines are increasingly capable
of understanding what is concretely visible in video frames, whether it be
objects, actions, events, or scenes. In comparison, humans retain a unique
ability to also look beyond concrete entities and recognize abstract concepts
like justice, freedom, and togetherness. Abstract concept recognition forms a
crucial open challenge in video understanding, where reasoning on multiple
semantic levels based on contextual information is key. In this paper, we argue
that the recent advances in foundation models make for an ideal setting to
address abstract concept understanding in videos. Automated understanding of
high-level abstract concepts is imperative as it enables models to be more
aligned with human reasoning and values. In this survey, we study different
tasks and datasets used to understand abstract concepts in video content. We
observe that, periodically and over a long period, researchers have attempted
to solve these tasks, making the best use of the tools available at their
disposal. We advocate that drawing on decades of community experience will help
us shed light on this important open grand challenge and avoid ``re-inventing
the wheel'' as we start revisiting it in the era of multi-modal foundation
models.

</details>


### [67] [Safer Skin Lesion Classification with Global Class Activation Probability Map Evaluation and SafeML](https://arxiv.org/abs/2508.20776)
*Kuniko Paxton,Koorosh Aslansefat,Amila Akagić,Dhavalkumar Thakker,Yiannis Papadopoulos*

Main category: cs.CV

TL;DR: 提出一种新的方法验证皮肤病变分类模型，提升AI诊断的可解释性和安全性。


<details>
  <summary>Details</summary>
Motivation: 即使AI模型检测准确率提高，但医疗实践中对于AI模型的怀疑依然存在，特别是在模型诊断的可解释性和信赖性问题上。

Method: 引入了名为Global Class Activation Probabilistic Map Evaluation的分析方法，将激活概率图从像素级别和所有类别统一分析，并结合SafeML来提高安全性，及时检测和预警错误诊断。

Result: 新方法在ISIC数据集上，结合MobileNetV2和Vision Transformers进行验证，证明了其对诊断稳定性和安全性的提升效果。

Conclusion: 该方法不仅提供了更可信的诊断可解释性，且通过安全机制提升了模型的诊断可靠性，有助于减少误诊风险，提高患者安全性。

Abstract: Recent advancements in skin lesion classification models have significantly
improved accuracy, with some models even surpassing dermatologists' diagnostic
performance. However, in medical practice, distrust in AI models remains a
challenge. Beyond high accuracy, trustworthy, explainable diagnoses are
essential. Existing explainability methods have reliability issues, with
LIME-based methods suffering from inconsistency, while CAM-based methods
failing to consider all classes. To address these limitations, we propose
Global Class Activation Probabilistic Map Evaluation, a method that analyses
all classes' activation probability maps probabilistically and at a pixel
level. By visualizing the diagnostic process in a unified manner, it helps
reduce the risk of misdiagnosis. Furthermore, the application of SafeML
enhances the detection of false diagnoses and issues warnings to doctors and
patients as needed, improving diagnostic reliability and ultimately patient
safety. We evaluated our method using the ISIC datasets with MobileNetV2 and
Vision Transformers.

</details>


### [68] [Evaluating Compositional Generalisation in VLMs and Diffusion Models](https://arxiv.org/abs/2508.20783)
*Beth Pearson,Bilal Boulbarss,Michael Wray,Martha Lewis*

Main category: cs.CV

TL;DR: 本文探讨了生成型Diffusion Classifier在组合语义理解上的能力，发现它在概念绑定任务中表现良好，但在关系推理任务中仍然存在显著挑战。


<details>
  <summary>Details</summary>
Motivation: 近年来，视觉-语言模型（VLMs）取得了显著进展，但其在组合语义理解方面存在不足，如无法准确描述图像中对象和属性或关系的绑定。该研究旨在评估生成型模型是否能在这些任务中优于判别型模型。

Method: 本文比较了Diffusion Classifier、CLIP和ViLT三个模型在零样本学习（ZSL）和广义零样本学习（GZSL）情境下的表现，特别是在概念绑定和关系推理的任务上的表现能力。

Result: 研究发现，Diffusion Classifier和ViLT在概念绑定任务中表现良好，但所有模型在关系GZSL任务中表现不佳。此外，对CLIP嵌入的分析显示，其在处理关系概念（如左和右）时表现出过于相似的表示，这可能导致其性能受限。

Conclusion: 研究表明，生成型Diffusion Classifier在某些组合语义任务上优于判别型模型，但VLMs在复杂关系推理任务上仍面临显著挑战，需要进一步探索解决方案。

Abstract: A fundamental aspect of the semantics of natural language is that novel
meanings can be formed from the composition of previously known parts.
Vision-language models (VLMs) have made significant progress in recent years,
however, there is evidence that they are unable to perform this kind of
composition. For example, given an image of a red cube and a blue cylinder, a
VLM such as CLIP is likely to incorrectly label the image as a red cylinder or
a blue cube, indicating it represents the image as a `bag-of-words' and fails
to capture compositional semantics. Diffusion models have recently gained
significant attention for their impressive generative abilities, and zero-shot
classifiers based on diffusion models have been shown to perform competitively
with CLIP in certain compositional tasks. In this work we explore whether the
generative Diffusion Classifier has improved compositional generalisation
abilities compared to discriminative models. We assess three models --
Diffusion Classifier, CLIP, and ViLT -- on their ability to bind objects with
attributes and relations in both zero-shot learning (ZSL) and generalised
zero-shot learning (GZSL) settings. Our results show that the Diffusion
Classifier and ViLT perform well at concept binding tasks, but that all models
struggle significantly with the relational GZSL task, underscoring the broader
challenges VLMs face with relational reasoning. Analysis of CLIP embeddings
suggests that the difficulty may stem from overly similar representations of
relational concepts such as left and right. Code and dataset are available at:
https://github.com/otmive/diffusion_classifier_clip

</details>


### [69] [Surfel-based 3D Registration with Equivariant SE(3) Features](https://arxiv.org/abs/2508.20789)
*Xueyang Kang,Hang Zhao,Kourosh Khoshelham,Patrick Vandewalle*

Main category: cs.CV

TL;DR: 提出了一种基于surfel的位姿学习回归方法，用于点云配准，能够有效应对噪声输入与输入点云的旋转问题。


<details>
  <summary>Details</summary>
Motivation: 现有点云配准方法忽视了点方向和不确定性，容易受噪声和激进旋转的影响，需要大量数据训练。

Method: 利用Lidar点云初始化华夫尔点模型，通过SE(3)等变卷积核学习位置和旋转的显式等变特征，采用卷积编码器、跨注意力机制和非线性Huber损失来预测源目标扫描间的相对变换。

Result: 实验表明，该模型在实际点云扫描及室内外数据集上均表现优越，优于现有的先进方法。

Conclusion: 提出的方法能够提高点云配准的稳健性和性能，对真实场景点云任务具有较好的实用性。

Abstract: Point cloud registration is crucial for ensuring 3D alignment consistency of
multiple local point clouds in 3D reconstruction for remote sensing or digital
heritage. While various point cloud-based registration methods exist, both
non-learning and learning-based, they ignore point orientations and point
uncertainties, making the model susceptible to noisy input and aggressive
rotations of the input point cloud like orthogonal transformation; thus, it
necessitates extensive training point clouds with transformation augmentations.
To address these issues, we propose a novel surfel-based pose learning
regression approach. Our method can initialize surfels from Lidar point cloud
using virtual perspective camera parameters, and learns explicit
$\mathbf{SE(3)}$ equivariant features, including both position and rotation
through $\mathbf{SE(3)}$ equivariant convolutional kernels to predict relative
transformation between source and target scans. The model comprises an
equivariant convolutional encoder, a cross-attention mechanism for similarity
computation, a fully-connected decoder, and a non-linear Huber loss.
Experimental results on indoor and outdoor datasets demonstrate our model
superiority and robust performance on real point-cloud scans compared to
state-of-the-art methods.

</details>


### [70] [Adapting Foundation Model for Dental Caries Detection with Dual-View Co-Training](https://arxiv.org/abs/2508.20813)
*Tao Luo,Han Wu,Tong Yang,Dinggang Shen,Zhiming Cui*

Main category: cs.CV

TL;DR: 提出了一种名为DVCTNet的新方法，通过结合全景和局部牙齿X光图像，增强龋齿检测的准确性，在公开数据集和新增的高精度龋齿检测数据集上达到领先表现。


<details>
  <summary>Details</summary>
Motivation: 当前龋齿检测方法因图像对比度较低和病变形态多样性问题，准确率不足，因此需要一种新方法提高检测性能。

Method: DVCTNet通过自动牙齿检测获得全景和局部视图，分别预训练两个视觉基础模型，利用“门控跨视图注意力模块”（GCV-Atten）融合双视图特征，并将融合特征整合到检测模型中，最终实现精确龋齿检测。

Result: 实验表明，DVCTNet在公开数据集和新的高精度数据集上表现优于现有先进方法（SOTA），展示其临床适用性。

Conclusion: DVCTNet通过双视角特征融合增强龋齿检测效果，显著提升检测性能并具有较高的临床潜力。

Abstract: Accurate dental caries detection from panoramic X-rays plays a pivotal role
in preventing lesion progression. However, current detection methods often
yield suboptimal accuracy due to subtle contrast variations and diverse lesion
morphology of dental caries. In this work, inspired by the clinical workflow
where dentists systematically combine whole-image screening with detailed
tooth-level inspection, we present DVCTNet, a novel Dual-View Co-Training
network for accurate dental caries detection. Our DVCTNet starts with employing
automated tooth detection to establish two complementary views: a global view
from panoramic X-ray images and a local view from cropped tooth images. We then
pretrain two vision foundation models separately on the two views. The
global-view foundation model serves as the detection backbone, generating
region proposals and global features, while the local-view model extracts
detailed features from corresponding cropped tooth patches matched by the
region proposals. To effectively integrate information from both views, we
introduce a Gated Cross-View Attention (GCV-Atten) module that dynamically
fuses dual-view features, enhancing the detection pipeline by integrating the
fused features back into the detection model for final caries detection. To
rigorously evaluate our DVCTNet, we test it on a public dataset and further
validate its performance on a newly curated, high-precision dental caries
detection dataset, annotated using both intra-oral images and panoramic X-rays
for double verification. Experimental results demonstrate DVCTNet's superior
performance against existing state-of-the-art (SOTA) methods on both datasets,
indicating the clinical applicability of our method. Our code and labeled
dataset are available at https://github.com/ShanghaiTech-IMPACT/DVCTNet.

</details>


### [71] [FusionCounting: Robust visible-infrared image fusion guided by crowd counting via multi-task learning](https://arxiv.org/abs/2508.20817)
*He Li,Xinyu Liu,Weihang Kong,Xingchen Zhang*

Main category: cs.CV

TL;DR: 本文提出了一个名为FusionCounting的多任务学习框架，将人群计数融入到可见光与红外图像融合（VIF）过程中，同时提升图像融合质量和人群计数性能。


<details>
  <summary>Details</summary>
Motivation: 现有VIF方法主要关注优化融合图像质量，新的方向加入下游任务例如语义分割和目标检测来进行语义指导。由于语义分割需要大量标注，而目标检测在密集场景中因重叠和遮挡面临的挑战，人群计数是一种标注负担低且适用于密集场景的替代任务。目前尚无研究将VIF与人群计数整合到统一框架中。

Method: 提出FusionCounting框架，通过将人群计数整合到VIF过程中。采用多任务设计，利用输入图像和人群密度信息互相促进，并引入动态损失函数加权策略以加速训练收敛和平衡任务贡献，同时通过对抗训练提升模型对抗干扰的稳定性和鲁棒性。

Result: 实验结果表明，所提框架在公共数据集上不仅提高了图像融合质量，还取得了人群计数的优越性能。

Conclusion: FusionCounting框架成功将人群计数与VIF相结合，为密集场景提供了一种高效的多任务学习方案，显著提升了模型的实用性和鲁棒性。

Abstract: Most visible and infrared image fusion (VIF) methods focus primarily on
optimizing fused image quality. Recent studies have begun incorporating
downstream tasks, such as semantic segmentation and object detection, to
provide semantic guidance for VIF. However, semantic segmentation requires
extensive annotations, while object detection, despite reducing annotation
efforts compared with segmentation, faces challenges in highly crowded scenes
due to overlapping bounding boxes and occlusion. Moreover, although RGB-T crowd
counting has gained increasing attention in recent years, no studies have
integrated VIF and crowd counting into a unified framework. To address these
challenges, we propose FusionCounting, a novel multi-task learning framework
that integrates crowd counting into the VIF process. Crowd counting provides a
direct quantitative measure of population density with minimal annotation,
making it particularly suitable for dense scenes. Our framework leverages both
input images and population density information in a mutually beneficial
multi-task design. To accelerate convergence and balance tasks contributions,
we introduce a dynamic loss function weighting strategy. Furthermore, we
incorporate adversarial training to enhance the robustness of both VIF and
crowd counting, improving the model's stability and resilience to adversarial
attacks. Experimental results on public datasets demonstrate that
FusionCounting not only enhances image fusion quality but also achieves
superior crowd counting performance.

</details>


### [72] [Estimating 2D Keypoints of Surgical Tools Using Vision-Language Models with Low-Rank Adaptation](https://arxiv.org/abs/2508.20830)
*Krit Duangprom,Tryphon Lambrou,Binod Bhattarai*

Main category: cs.CV

TL;DR: 本文介绍了一种利用视觉语言模型（VLM）进行外科手术工具2D关键点估计的新型流程，通过低秩调整（LoRA）技术进行微调，在小样本医疗数据中表现优越。


<details>
  <summary>Details</summary>
Motivation: 传统的CNN或Transformer方法在小规模医疗数据集上易于过拟合，而预训练的VLM具有较强的泛化能力，因此作者希望利用这一点改进外科手术工具关键点估计。

Method: 通过设计提示创建指令微调数据集，并将视觉特征与语义关键点描述对齐，同时应用LoRA技术对VLM进行快速微调以适应具体任务。

Result: 实验结果表明，经过仅两个epoch的微调后，改进的VLM超越了基线模型，验证了LoRA技术在低资源环境中的有效性。

Conclusion: 该方法不仅提升了关键点检测性能，还为未来开展3D外科手术手部及工具姿态估计的研究奠定了基础。

Abstract: This paper presents a novel pipeline for 2D keypoint estima- tion of surgical
tools by leveraging Vision Language Models (VLMs) fine- tuned using a low rank
adjusting (LoRA) technique. Unlike traditional Convolutional Neural Network
(CNN) or Transformer-based approaches, which often suffer from overfitting in
small-scale medical datasets, our method harnesses the generalization
capabilities of pre-trained VLMs. We carefully design prompts to create an
instruction-tuning dataset and use them to align visual features with semantic
keypoint descriptions. Experimental results show that with only two epochs of
fine tuning, the adapted VLM outperforms the baseline models, demonstrating the
ef- fectiveness of LoRA in low-resource scenarios. This approach not only
improves keypoint detection performance, but also paves the way for future work
in 3D surgical hands and tools pose estimation.

</details>


### [73] [PointDGRWKV: Generalizing RWKV-like Architecture to Unseen Domains for Point Cloud Classification](https://arxiv.org/abs/2508.20835)
*Hao Yang,Qianyu Zhou,Haijia Sun,Xiangtai Li,Xuequan Lu,Lizhuang Ma,Shuicheng Yan*

Main category: cs.CV

TL;DR: 本文提出了一种名为PointDGRWKV的新方法，旨在增强RWKV架构在点云分类任务中的泛化能力，并解决其在跨域应用中面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决点云分类任务中域泛化（DG）的问题，克服现有方法在感受野、计算复杂性和长距离依赖建模等方面的不足。

Method: 提出一种基于RWKV的新框架PointDGRWKV，引入两个核心模块：自适应几何令牌偏移（Adaptive Geometric Token Shift）和跨域关键特征分布对齐（Cross-Domain key feature Distribution Alignment），以提高空间建模能力和跨域鲁棒性。

Result: 实验表明，新方法PointDGRWKV在多个基准数据集上实现了点云分类任务的最新的域泛化性能。

Conclusion: PointDGRWKV有效解决了传统RWKV架构在点云分类中遇到的空间建模局限和跨域适用性问题，实现了高效且鲁棒的点云域泛化模型。

Abstract: Domain Generalization (DG) has been recently explored to enhance the
generalizability of Point Cloud Classification (PCC) models toward unseen
domains. Prior works are based on convolutional networks, Transformer or Mamba
architectures, either suffering from limited receptive fields or high
computational cost, or insufficient long-range dependency modeling. RWKV, as an
emerging architecture, possesses superior linear complexity, global receptive
fields, and long-range dependency. In this paper, we present the first work
that studies the generalizability of RWKV models in DG PCC. We find that
directly applying RWKV to DG PCC encounters two significant challenges: RWKV's
fixed direction token shift methods, like Q-Shift, introduce spatial
distortions when applied to unstructured point clouds, weakening local
geometric modeling and reducing robustness. In addition, the Bi-WKV attention
in RWKV amplifies slight cross-domain differences in key distributions through
exponential weighting, leading to attention shifts and degraded generalization.
To this end, we propose PointDGRWKV, the first RWKV-based framework tailored
for DG PCC. It introduces two key modules to enhance spatial modeling and
cross-domain robustness, while maintaining RWKV's linear efficiency. In
particular, we present Adaptive Geometric Token Shift to model local
neighborhood structures to improve geometric context awareness. In addition,
Cross-Domain key feature Distribution Alignment is designed to mitigate
attention drift by aligning key feature distributions across domains. Extensive
experiments on multiple benchmarks demonstrate that PointDGRWKV achieves
state-of-the-art performance on DG PCC.

</details>


### [74] [PathMR: Multimodal Visual Reasoning for Interpretable Pathology Diagnosis](https://arxiv.org/abs/2508.20851)
*Ye Zhang,Yu Zhou,Jingwen Qi,Yongbing Zhang,Simon Puettmann,Finn Wichmann,Larissa Pereira Ferreira,Lara Sichward,Julius Keyl,Sylvia Hartmann,Shuo Zhao,Hongxiao Wang,Xiaowei Xu,Jianxu Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为PathMR的多模态视觉推理框架，用于病理图像分析，能够生成专家级诊断解释并预测细胞分布模式，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习显著提升了病理诊断效率，但不透明的模型决策和缺乏可追溯性限制了其临床应用，因而需要更具解释性的模型。

Method: 提出PathMR框架，结合病理图像和文本查询，生成诊断解释和细胞分布模式预测，并在两个数据集上进行了性能评估。

Result: 实验表明，PathMR在文字生成质量、分割精度和跨模态对齐方面优于现有的视觉推理方法。

Conclusion: PathMR框架有潜力提升AI驱动病理诊断的可解释性，并已经公开了代码以供研究社区使用。

Abstract: Deep learning based automated pathological diagnosis has markedly improved
diagnostic efficiency and reduced variability between observers, yet its
clinical adoption remains limited by opaque model decisions and a lack of
traceable rationale. To address this, recent multimodal visual reasoning
architectures provide a unified framework that generates segmentation masks at
the pixel level alongside semantically aligned textual explanations. By
localizing lesion regions and producing expert style diagnostic narratives,
these models deliver the transparent and interpretable insights necessary for
dependable AI assisted pathology. Building on these advancements, we propose
PathMR, a cell-level Multimodal visual Reasoning framework for Pathological
image analysis. Given a pathological image and a textual query, PathMR
generates expert-level diagnostic explanations while simultaneously predicting
cell distribution patterns. To benchmark its performance, we evaluated our
approach on the publicly available PathGen dataset as well as on our newly
developed GADVR dataset. Extensive experiments on these two datasets
demonstrate that PathMR consistently outperforms state-of-the-art visual
reasoning methods in text generation quality, segmentation accuracy, and
cross-modal alignment. These results highlight the potential of PathMR for
improving interpretability in AI-driven pathological diagnosis. The code will
be publicly available in https://github.com/zhangye-zoe/PathMR.

</details>


### [75] [Deep Learning Framework for Early Detection of Pancreatic Cancer Using Multi-Modal Medical Imaging Analysis](https://arxiv.org/abs/2508.20877)
*Dennis Slobodzian,Karissa Tilbury,Amir Kordijazi*

Main category: cs.CV

TL;DR: 研究开发了一种深度学习框架，通过分析双模式成像数据，用于早期诊断胰腺导管腺癌 (PDAC)，取得了超过90%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: PDAC 致死率高，并且因晚期诊断导致五年生存率低于10%，因此亟需早期检测方法。

Method: 通过分析自动荧光和二次谐波生成(SHG)图像数据，比较了六种深度学习架构，包括 CNNs 和 ViTs，最终采用基于 ResNet 的优化架构。

Result: 开发的深度学习框架在小数据集和类别不平衡等挑战下取得了显著成果，准确度超过90%，优于现有人工分析方法。

Conclusion: 研究提供了一种强大的自动化 PDAC 诊断途径，能够辅助病理学家，并对其他癌症类型的研究有潜在扩展性，同时为小规模医学数据的深度学习应用提供了参考价值。

Abstract: Pacreatic ductal adenocarcinoma (PDAC) remains one of the most lethal forms
of cancer, with a five-year survival rate below 10% primarily due to late
detection. This research develops and validates a deep learning framework for
early PDAC detection through analysis of dual-modality imaging:
autofluorescence and second harmonic generation (SHG). We analyzed 40 unique
patient samples to create a specialized neural network capable of
distinguishing between normal, fibrotic, and cancerous tissue. Our methodology
evaluated six distinct deep learning architectures, comparing traditional
Convolutional Neural Networks (CNNs) with modern Vision Transformers (ViTs).
Through systematic experimentation, we identified and overcome significant
challenges in medical image analysis, including limited dataset size and class
imbalance. The final optimized framework, based on a modified ResNet
architecture with frozen pre-trained layers and class-weighted training,
achieved over 90% accuracy in cancer detection. This represents a significant
improvement over current manual analysis methods an demonstrates potential for
clinical deployment. This work establishes a robust pipeline for automated PDAC
detection that can augment pathologists' capabilities while providing a
foundation for future expansion to other cancer types. The developed
methodology also offers valuable insights for applying deep learning to
limited-size medical imaging datasets, a common challenge in clinical
applications.

</details>


### [76] [Understanding and evaluating computer vision models through the lens of counterfactuals](https://arxiv.org/abs/2508.20881)
*Pushkar Shukla*

Main category: cs.CV

TL;DR: 论文探讨了通过反事实推理提高视觉分类器和生成模型的可解释性与公平性的方法。


<details>
  <summary>Details</summary>
Motivation: 利用反事实推理解释、审计及减少偏差，帮助构建更稳健和公平的人工智能系统。

Method: 研究重点包括：1. 对于视觉分类器，提出CAVLI与ASAC框架以量化模型对人类可解释性概念的依赖，并通过对抗性反事实微调模型。2. 对于生成式文本到图像模型，提出TIBET、BiasConnect和InterMit工具以评估与缓解身份相关及交叉偏差。

Result: 所提方法揭示了模型中的虚假关联与因果依赖，能够有效提高模型的公平性、准确性和健壮性。

Conclusion: 反事实推理为解释性、公平性及因果推理提供了统一视角，能够建立社会责任性更强的偏差评估与缓解方法。

Abstract: Counterfactual reasoning -- the practice of asking ``what if'' by varying
inputs and observing changes in model behavior -- has become central to
interpretable and fair AI. This thesis develops frameworks that use
counterfactuals to explain, audit, and mitigate bias in vision classifiers and
generative models. By systematically altering semantically meaningful
attributes while holding others fixed, these methods uncover spurious
correlations, probe causal dependencies, and help build more robust systems.
  The first part addresses vision classifiers. CAVLI integrates attribution
(LIME) with concept-level analysis (TCAV) to quantify how strongly decisions
rely on human-interpretable concepts. With localized heatmaps and a Concept
Dependency Score, CAVLI shows when models depend on irrelevant cues like
backgrounds. Extending this, ASAC introduces adversarial counterfactuals that
perturb protected attributes while preserving semantics. Through curriculum
learning, ASAC fine-tunes biased models for improved fairness and accuracy
while avoiding stereotype-laden artifacts.
  The second part targets generative Text-to-Image (TTI) models. TIBET provides
a scalable pipeline for evaluating prompt-sensitive biases by varying
identity-related terms, enabling causal auditing of how race, gender, and age
affect image generation. To capture interactions, BiasConnect builds causal
graphs diagnosing intersectional biases. Finally, InterMit offers a modular,
training-free algorithm that mitigates intersectional bias via causal
sensitivity scores and user-defined fairness goals.
  Together, these contributions show counterfactuals as a unifying lens for
interpretability, fairness, and causality in both discriminative and generative
models, establishing principled, scalable methods for socially responsible bias
evaluation and mitigation.

</details>


### [77] [To New Beginnings: A Survey of Unified Perception in Autonomous Vehicle Software](https://arxiv.org/abs/2508.20892)
*Loïc Stratil,Felix Fent,Esteban Rivera,Markus Lienkamp*

Main category: cs.CV

TL;DR: 本文总结了存在的统一感知方法，提出了一个综合系统的分类学，同时定义了三个统一感知范式（早期、晚期和完全统一感知）并系统性地回顾了相关方法、架构、训练策略、数据集等，旨在为未来的研究提供方向。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶感知普遍采用模块化流程，但会出现错误累积和任务间协同有限的问题。统一感知作为新兴范式，有望改进鲁棒性、情境推理能力并提高效率，同时保持可解释性。

Method: 本文提出了一个综合和系统的分类学来分析和比较不同的统一感知方法，定义了三个范式，并系统性回顾了现有方法及其相关要素，提供了研究方向。

Result: 本文整理了分散的统一感知研究工作，首次形成了系统的框架，分类并描述了不同的统一感知方法和其各自适用的场景。

Conclusion: 该研究为理解和推进统一感知提供了全面的框架，对未来更鲁棒、可解释性更强的技术路径提供了有力的指导。

Abstract: Autonomous vehicle perception typically relies on modular pipelines that
decompose the task into detection, tracking, and prediction. While
interpretable, these pipelines suffer from error accumulation and limited
inter-task synergy. Unified perception has emerged as a promising paradigm that
integrates these sub-tasks within a shared architecture, potentially improving
robustness, contextual reasoning, and efficiency while retaining interpretable
outputs. In this survey, we provide a comprehensive overview of unified
perception, introducing a holistic and systemic taxonomy that categorizes
methods along task integration, tracking formulation, and representation flow.
We define three paradigms -Early, Late, and Full Unified Perception- and
systematically review existing methods, their architectures, training
strategies, datasets used, and open-source availability, while highlighting
future research directions. This work establishes the first comprehensive
framework for understanding and advancing unified perception, consolidates
fragmented efforts, and guides future research toward more robust,
generalizable, and interpretable perception.

</details>


### [78] [Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation](https://arxiv.org/abs/2508.20909)
*Yifan Gao,Haoyue Li,Feng Yuan,Xiaosong Wang,Xin Gao*

Main category: cs.CV

TL;DR: 本文提出了一种称为Dino U-Net的新型架构，将DINOv3视觉基础模型的密集特征应用于医学图像分割，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型在医学图像分割任务中迁移性能尚未达到精准临床应用的需求。

Method: 设计了Dino U-Net架构，其中包含基于DINOv3的冻结编码器、自适应特征融合以及保真降维投影模块（FAPM）。

Result: 在7个公开医疗图像数据集上，Dino U-Net表现优异，且模型参数数量增长对指标有显著提升效果。

Conclusion: 使用通用视觉基础模型预训练特征可以实现高效参数利用，推动医学分割精度的提升。

Abstract: Foundation models pre-trained on large-scale natural image datasets offer a
powerful paradigm for medical image segmentation. However, effectively
transferring their learned representations for precise clinical applications
remains a challenge. In this work, we propose Dino U-Net, a novel
encoder-decoder architecture designed to exploit the high-fidelity dense
features of the DINOv3 vision foundation model. Our architecture introduces an
encoder built upon a frozen DINOv3 backbone, which employs a specialized
adapter to fuse the model's rich semantic features with low-level spatial
details. To preserve the quality of these representations during dimensionality
reduction, we design a new fidelity-aware projection module (FAPM) that
effectively refines and projects the features for the decoder. We conducted
extensive experiments on seven diverse public medical image segmentation
datasets. Our results show that Dino U-Net achieves state-of-the-art
performance, consistently outperforming previous methods across various imaging
modalities. Our framework proves to be highly scalable, with segmentation
accuracy consistently improving as the backbone model size increases up to the
7-billion-parameter variant. The findings demonstrate that leveraging the
superior, dense-pretrained features from a general-purpose foundation model
provides a highly effective and parameter-efficient approach to advance the
accuracy of medical image segmentation. The code is available at
https://github.com/yifangao112/DinoUNet.

</details>


### [79] [Classifying Mitotic Figures in the MIDOG25 Challenge with Deep Ensemble Learning and Rule Based Refinement](https://arxiv.org/abs/2508.20919)
*Sara Krauss,Ellena Spieß,Daniel Hieber,Frank Kramer,Johannes Schobel,Dominik Müller*

Main category: cs.CV

TL;DR: 本文提出使用ConvNeXtBase模型的集成学习和规则基础优化模块（RBR）来区分肿瘤中的非典型有丝分裂（AMFs）和正常有丝分裂（NMFs），获得了较高的平衡准确率。


<details>
  <summary>Details</summary>
Motivation: 区分AMFs和NMFs是肿瘤分级的重要难题，但手动注释耗时且主观性强，因此需要自动化的方法来提高效率和准确性。

Method: 作者训练了多个ConvNeXtBase模型构成的模型集成（ensemble），并结合规则基础优化模块（RBR）以改进分类性能。

Result: 在MIDOG25初步测试集上，模型集成实现了84.02%的平衡准确率。RBR在提升特异性的同时降低了灵敏度和整体性能。

Conclusion: 深度学习模型集成在AMF分类任务中表现出色，RBR模块需要进一步研究以权衡性能指标。

Abstract: Mitotic figures (MFs) are relevant biomarkers in tumor grading.
Differentiating atypical MFs (AMFs) from normal MFs (NMFs) remains difficult,
as manual annotation is time-consuming and subjective. In this work an ensemble
of ConvNeXtBase models was trained with AUCMEDI and extend with a rule-based
refinement (RBR) module. On the MIDOG25 preliminary test set, the ensemble
achieved a balanced accuracy of 84.02%. While the RBR increased specificity, it
reduced sensitivity and overall performance. The results show that deep
ensembles perform well for AMF classification. RBR can increase specific
metrics but requires further research.

</details>


### [80] [COMETH: Convex Optimization for Multiview Estimation and Tracking of Humans](https://arxiv.org/abs/2508.20920)
*Enrico Martini,Ho Jin Choi,Nadia Figueroa,Nicola Bombieri*

Main category: cs.CV

TL;DR: 提出一种名为COMETH的轻量级实时多视图人体姿态融合算法，解决了边缘设备资源受限及计算分布带来的精度下降和空间时间不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决多摄像头集中式设置中高计算成本和带宽需求的问题，以及边缘设备资源受限导致的精度下降和空间时间不一致问题，从而在工业5.0时代实现高效的多人姿态监测。

Method: 提出COMETH算法，结合运动学与生物力学约束提高关节定位精度；使用基于凸优化的逆运动学进行空间融合；通过状态观察器提升时间一致性。

Result: 在公开和工业数据集上评估中，COMETH在定位、检测和跟踪精度方面优于现有方法。

Conclusion: COMETH是一种准确且具可扩展性的多视角人体运动跟踪算法，适用于工业和安全关键应用，代码公开。

Abstract: In the era of Industry 5.0, monitoring human activity is essential for
ensuring both ergonomic safety and overall well-being. While multi-camera
centralized setups improve pose estimation accuracy, they often suffer from
high computational costs and bandwidth requirements, limiting scalability and
real-time applicability. Distributing processing across edge devices can reduce
network bandwidth and computational load. On the other hand, the constrained
resources of edge devices lead to accuracy degradation, and the distribution of
computation leads to temporal and spatial inconsistencies. We address this
challenge by proposing COMETH (Convex Optimization for Multiview Estimation and
Tracking of Humans), a lightweight algorithm for real-time multi-view human
pose fusion that relies on three concepts: it integrates kinematic and
biomechanical constraints to increase the joint positioning accuracy; it
employs convex optimization-based inverse kinematics for spatial fusion; and it
implements a state observer to improve temporal consistency. We evaluate COMETH
on both public and industrial datasets, where it outperforms state-of-the-art
methods in localization, detection, and tracking accuracy. The proposed fusion
pipeline enables accurate and scalable human motion tracking, making it
well-suited for industrial and safety-critical applications. The code is
publicly available at https://github.com/PARCO-LAB/COMETH.

</details>


### [81] [ExpertSim: Fast Particle Detector Simulation Using Mixture-of-Generative-Experts](https://arxiv.org/abs/2508.20991)
*Patryk Będkowski,Jan Dubiński,Filip Szatkowski,Kamil Deja,Przemysław Rokita,Tomasz Trzciński*

Main category: cs.CV

TL;DR: ExpertSim是专为ALICE实验中的零度量热器设计的深度学习仿真方法，利用生成专家混合架构实现更高效、精确的数据模拟。


<details>
  <summary>Details</summary>
Motivation: 目前模拟探测器响应主要依靠计算成本高昂的蒙特卡罗方法，严重消耗CERN计算资源，因此迫切需要更高效的生成方法。

Method: 提出ExpertSim方法，通过生成专家混合架构，将模拟任务划分为不同数据子集，每个专家专注于特定领域的数据模拟。

Result: ExpertSim在提高模拟准确性的同时，相较传统蒙特卡罗方法显著提升了模拟速度。

Conclusion: ExpertSim为高效的粒子物理实验探测器仿真提供了一种有前景的解决方案，并已开源以供研究者使用。

Abstract: Simulating detector responses is a crucial part of understanding the inner
workings of particle collisions in the Large Hadron Collider at CERN. Such
simulations are currently performed with statistical Monte Carlo methods, which
are computationally expensive and put a significant strain on CERN's
computational grid. Therefore, recent proposals advocate for generative machine
learning methods to enable more efficient simulations. However, the
distribution of the data varies significantly across the simulations, which is
hard to capture with out-of-the-box methods. In this study, we present
ExpertSim - a deep learning simulation approach tailored for the Zero Degree
Calorimeter in the ALICE experiment. Our method utilizes a
Mixture-of-Generative-Experts architecture, where each expert specializes in
simulating a different subset of the data. This allows for a more precise and
efficient generation process, as each expert focuses on a specific aspect of
the calorimeter response. ExpertSim not only improves accuracy, but also
provides a significant speedup compared to the traditional Monte-Carlo methods,
offering a promising solution for high-efficiency detector simulations in
particle physics experiments at CERN. We make the code available at
https://github.com/patrick-bedkowski/expertsim-mix-of-generative-experts.

</details>


### [82] [FW-GAN: Frequency-Driven Handwriting Synthesis with Wave-Modulated MLP Generator](https://arxiv.org/abs/2508.21040)
*Huynh Tong Dang Khoa,Dang Hoai Nam,Vo Nguyen Le Duy*

Main category: cs.CV

TL;DR: 提出了FW-GAN，一个基于单样本生成的手写合成框架，通过改进生成器和判别器增强了风格一致性和真实性，适用于低资源手写识别系统的数据增强。


<details>
  <summary>Details</summary>
Motivation: 现有手写合成方法无法有效建模长距离依赖和复杂笔画模式，同时忽略了频率信息对捕获细节的重要性。

Method: FW-GAN的生成器结合了阶段感知Wave-MLP，用于更好地捕获空间关系并保留风格特征；判别器利用高频信息提升真实性检测；引入频率分布损失函数，匹配合成与真实手写的频率特性。

Result: 实验表明，FW-GAN在越南语和英语手写数据集上生成高质量、风格一致的手写样本。

Conclusion: FW-GAN是低资源手写识别系统中数据增强的有用工具，提升了合成手写的视觉保真度和风格一致性。

Abstract: Labeled handwriting data is often scarce, limiting the effectiveness of
recognition systems that require diverse, style-consistent training samples.
Handwriting synthesis offers a promising solution by generating artificial data
to augment training. However, current methods face two major limitations.
First, most are built on conventional convolutional architectures, which
struggle to model long-range dependencies and complex stroke patterns. Second,
they largely ignore the crucial role of frequency information, which is
essential for capturing fine-grained stylistic and structural details in
handwriting. To address these challenges, we propose FW-GAN, a one-shot
handwriting synthesis framework that generates realistic, writer-consistent
text from a single example. Our generator integrates a phase-aware Wave-MLP to
better capture spatial relationships while preserving subtle stylistic cues. We
further introduce a frequency-guided discriminator that leverages
high-frequency components to enhance the authenticity detection of generated
samples. Additionally, we introduce a novel Frequency Distribution Loss that
aligns the frequency characteristics of synthetic and real handwriting, thereby
enhancing visual fidelity. Experiments on Vietnamese and English handwriting
datasets demonstrate that FW-GAN generates high-quality, style-consistent
handwriting, making it a valuable tool for augmenting data in low-resource
handwriting recognition (HTR) pipelines. Official implementation is available
at https://github.com/DAIR-Group/FW-GAN

</details>


### [83] [Olive Tree Satellite Image Segmentation Based On SAM and Multi-Phase Refinement](https://arxiv.org/abs/2508.20954)
*Amir Jmal,Chaima Chtourou,Mahdi Louati,Abdelaziz Kallel,Houda Khmila*

Main category: cs.CV

TL;DR: 本文聚焦气候变化背景下，提出一种利用卫星图像分割技术识别橄榄树的创新方法，通过改良SAM模型的分割技术，将准确率提高至98%。


<details>
  <summary>Details</summary>
Motivation: 在气候变化中维持橄榄树的生物多样性，对早期异常检测和处理至关重要，需要创新的管理解决方案。

Method: 利用卫星图像结合基础模型和高级分割技术，改进Segment Anything Model (SAM)，通过树木排列校正及形状和大小约束提高准确率。

Result: 本文方法的橄榄树分割准确率达到98%，远高于初始的SAM性能82%。

Conclusion: 研究展示了一种比现有方法更高效的橄榄树分割技术，这对农业管理及生态保护意义重大。

Abstract: In the context of proven climate change, maintaining olive biodiversity
through early anomaly detection and treatment using remote sensing technology
is crucial, offering effective management solutions. This paper presents an
innovative approach to olive tree segmentation from satellite images. By
leveraging foundational models and advanced segmentation techniques, the study
integrates the Segment Anything Model (SAM) to accurately identify and segment
olive trees in agricultural plots. The methodology includes SAM segmentation
and corrections based on trees alignement in the field and a learanble
constraint about the shape and the size. Our approach achieved a 98\% accuracy
rate, significantly surpassing the initial SAM performance of 82\%.

</details>


### [84] [Dress&Dance: Dress up and Dance as You Like It - Technical Preview](https://arxiv.org/abs/2508.21070)
*Jun-Kun Chen,Aayush Bansal,Minh Phuoc Vo,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: 本文提出了Dress&Dance视视频扩散框架，用于生成高质量的虚拟试穿视频，支持多种衣物组合及动作的高保真展示。


<details>
  <summary>Details</summary>
Motivation: 旨在通过单张用户图片和多种输入条件，实现在给定动作参考视频下的高质量虚拟试穿体验。

Method: 设计了CondNet条件网络，利用多模态输入并结合逐步训练，加强衣物配准与动作真实性。

Result: 成功生成了高分辨率的5秒虚拟试穿视频，优于现有开源及商业解决方案。

Conclusion: Dress&Dance框架通过创新的网络设计和多模态数据处理，提升了虚拟试穿的质量与灵活性，具有广泛应用前景。

Abstract: We present Dress&Dance, a video diffusion framework that generates high
quality 5-second-long 24 FPS virtual try-on videos at 1152x720 resolution of a
user wearing desired garments while moving in accordance with a given reference
video. Our approach requires a single user image and supports a range of tops,
bottoms, and one-piece garments, as well as simultaneous tops and bottoms
try-on in a single pass. Key to our framework is CondNet, a novel conditioning
network that leverages attention to unify multi-modal inputs (text, images, and
videos), thereby enhancing garment registration and motion fidelity. CondNet is
trained on heterogeneous training data, combining limited video data and a
larger, more readily available image dataset, in a multistage progressive
manner. Dress&Dance outperforms existing open source and commercial solutions
and enables a high quality and flexible try-on experience.

</details>


### [85] [E-ConvNeXt: A Lightweight and Efficient ConvNeXt Variant with Cross-Stage Partial Connections](https://arxiv.org/abs/2508.20955)
*Fang Wang,Huitao Li,Wenhan Chao,Zheng Zhuo,Yiran Ji,Chang Peng,Yupeng Sun*

Main category: cs.CV

TL;DR: 本文提出了一种名为E-ConvNeXt的新型网络，通过整合Cross Stage Partial Connections机制和一系列优化设计，显著降低ConvNeXt网络的参数规模和复杂性，同时保持高准确率性能。


<details>
  <summary>Details</summary>
Motivation: 许多高性能网络未针对轻量化应用场景进行设计，这限制了其应用范围。本研究旨在通过改进现有的ConvNeXt网络，使其在轻量化场景下仍能保持高效能和高准确性。

Method: 通过引入CSPNet机制、优化网络结构、改进Stem和Block结构，以及将Layer Scale替换为通道注意力机制，显著降低网络复杂性并提升模型性能。

Result: E-ConvNeXt在ImageNet分类任务中表现出色：E-ConvNeXt-mini在0.9GFLOPs下达到78.3%的Top-1准确率，E-ConvNeXt-small在3.1GFLOPs下达到81.9%的Top-1准确率。此外，在目标检测任务中的迁移学习测试中同样展现了良好的泛化能力。

Conclusion: E-ConvNeXt大幅降低了模型复杂性，同时保持了较高的分类准确率，并展示出良好的迁移学习能力，非常适合作为轻量化高性能任务的解决方案。

Abstract: Many high-performance networks were not designed with lightweight application
scenarios in mind from the outset, which has greatly restricted their scope of
application. This paper takes ConvNeXt as the research object and significantly
reduces the parameter scale and network complexity of ConvNeXt by integrating
the Cross Stage Partial Connections mechanism and a series of optimized
designs. The new network is named E-ConvNeXt, which can maintain high accuracy
performance under different complexity configurations. The three core
innovations of E-ConvNeXt are : (1) integrating the Cross Stage Partial Network
(CSPNet) with ConvNeXt and adjusting the network structure, which reduces the
model's network complexity by up to 80%; (2) Optimizing the Stem and Block
structures to enhance the model's feature expression capability and operational
efficiency; (3) Replacing Layer Scale with channel attention. Experimental
validation on ImageNet classification demonstrates E-ConvNeXt's superior
accuracy-efficiency balance: E-ConvNeXt-mini reaches 78.3% Top-1 accuracy at
0.9GFLOPs. E-ConvNeXt-small reaches 81.9% Top-1 accuracy at 3.1GFLOPs. Transfer
learning tests on object detection tasks further confirm its generalization
capability.

</details>


### [86] [DrivingGaussian++: Towards Realistic Reconstruction and Editable Simulation for Surrounding Dynamic Driving Scenes](https://arxiv.org/abs/2508.20965)
*Yajiao Xiong,Xiaoyu Zhou,Yongtao Wan,Deqing Sun,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: DrivingGaussian++ 是一种高效框架，可重建和编辑动态驾驶场景，结合三维高斯和动态高斯图，支持无训练的控制性编辑，成果效果优异。


<details>
  <summary>Details</summary>
Motivation: 解决动态驾驶场景重建和可控编辑的需求，包括动态物体位置精确描绘、遮挡处理等挑战。

Method: 引入增量3D高斯和动态高斯图，结合LiDAR深度先验，支持文本驱动的动态轨迹生成以及多视图控制编辑。

Result: 成功实现动态场景的真实重建与多视图合成，支持丰富效果的控制性编辑，提高场景多样性。

Conclusion: DrivingGaussian++ 在动态场景重建和编辑中表现优异，展现出一致性和高真实感，并提供多样的驾驶场景生成能力。

Abstract: We present DrivingGaussian++, an efficient and effective framework for
realistic reconstructing and controllable editing of surrounding dynamic
autonomous driving scenes. DrivingGaussian++ models the static background using
incremental 3D Gaussians and reconstructs moving objects with a composite
dynamic Gaussian graph, ensuring accurate positions and occlusions. By
integrating a LiDAR prior, it achieves detailed and consistent scene
reconstruction, outperforming existing methods in dynamic scene reconstruction
and photorealistic surround-view synthesis. DrivingGaussian++ supports
training-free controllable editing for dynamic driving scenes, including
texture modification, weather simulation, and object manipulation, leveraging
multi-view images and depth priors. By integrating large language models (LLMs)
and controllable editing, our method can automatically generate dynamic object
motion trajectories and enhance their realism during the optimization process.
DrivingGaussian++ demonstrates consistent and realistic editing results and
generates dynamic multi-view driving scenarios, while significantly enhancing
scene diversity. More results and code can be found at the project site:
https://xiong-creator.github.io/DrivingGaussian_plus.github.io

</details>


### [87] [Webly-Supervised Image Manipulation Localization via Category-Aware Auto-Annotation](https://arxiv.org/abs/2508.20987)
*Chenfan Qu,Yiwu Zhong,Bin Li,Lianwen Jin*

Main category: cs.CV

TL;DR: 通过引入新的方法和数据集，大幅提高了图像操控区域定位的性能，解决了数据匮乏问题。


<details>
  <summary>Details</summary>
Motivation: 解决图像操控定位中的数据匮乏和高质量标注数据集缺乏的问题。

Method: 引入CAAAv2自动标注方法、QES过滤度量、大规模高质量数据集MIMLv2、Object Jitter技术，以及Web-IML模型，结合网络规模监督完成标注和定位任务改进。

Result: 提出的Web-IML模型在多项真实伪造基准中显著提高了性能，性能提升31%，IoU分数比SOTA提高24.1数据点。

Conclusion: 通过新的数据标注策略和模型设计，有效缓解了数据不足问题，同时也在伪造图像定位任务中取得前所未有的性能提升。

Abstract: Images manipulated using image editing tools can mislead viewers and pose
significant risks to social security. However, accurately localizing the
manipulated regions within an image remains a challenging problem. One of the
main barriers in this area is the high cost of data acquisition and the severe
lack of high-quality annotated datasets. To address this challenge, we
introduce novel methods that mitigate data scarcity by leveraging readily
available web data. We utilize a large collection of manually forged images
from the web, as well as automatically generated annotations derived from a
simpler auxiliary task, constrained image manipulation localization.
Specifically, we introduce a new paradigm CAAAv2, which automatically and
accurately annotates manipulated regions at the pixel level. To further improve
annotation quality, we propose a novel metric, QES, which filters out
unreliable annotations. Through CAAA v2 and QES, we construct MIMLv2, a
large-scale, diverse, and high-quality dataset containing 246,212 manually
forged images with pixel-level mask annotations. This is over 120x larger than
existing handcrafted datasets like IMD20. Additionally, we introduce Object
Jitter, a technique that further enhances model training by generating
high-quality manipulation artifacts. Building on these advances, we develop a
new model, Web-IML, designed to effectively leverage web-scale supervision for
the image manipulation localization task. Extensive experiments demonstrate
that our approach substantially alleviates the data scarcity problem and
significantly improves the performance of various models on multiple real-world
forgery benchmarks. With the proposed web supervision, Web-IML achieves a
striking performance gain of 31% and surpasses previous SOTA TruFor by 24.1
average IoU points. The dataset and code will be made publicly available at
https://github.com/qcf-568/MIML.

</details>


### [88] [Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning](https://arxiv.org/abs/2508.21048)
*Hao Tan,Jun Lan,Zichang Tan,Ajian Liu,Chuanbiao Song,Senyuan Shi,Huijia Zhu,Weiqiang Wang,Jun Wan,Zhen Lei*

Main category: cs.CV

TL;DR: 本文讨论了深度伪造检测的挑战，提出了一个名为HydraFake的新数据集，模拟了真实场景中的挑战，并提出了一种基于多模态大模型（Veritas）的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有的深度伪造检测基准往往与工业实践相差甚远，导致检测器在实际部署中的效果受限。

Method: 提出了一个新数据集HydraFake，模拟真实世界的伪造挑战，并结合多模态大语言模型Veritas，通过引入对“规划”和“自我反思”等推理模式的学习，建立了检测模型。此外，设计了两阶段的训练策略以提高模型适应能力。

Result: 实验表明，现有检测器在未见过的伪造和数据领域表现有限，而Veritas在不同领域外推场景（OOD）中表现显著优越，提供透明且可信的检测结果。

Conclusion: Veritas结合了具有前瞻性的推理能力和强泛化能力，为深度伪造检测提供了卓越表现，表明多模态模型在该领域的潜力。

Abstract: Deepfake detection remains a formidable challenge due to the complex and
evolving nature of fake content in real-world scenarios. However, existing
academic benchmarks suffer from severe discrepancies from industrial practice,
typically featuring homogeneous training sources and low-quality testing
images, which hinder the practical deployments of current detectors. To
mitigate this gap, we introduce HydraFake, a dataset that simulates real-world
challenges with hierarchical generalization testing. Specifically, HydraFake
involves diversified deepfake techniques and in-the-wild forgeries, along with
rigorous training and evaluation protocol, covering unseen model architectures,
emerging forgery techniques and novel data domains. Building on this resource,
we propose Veritas, a multi-modal large language model (MLLM) based deepfake
detector. Different from vanilla chain-of-thought (CoT), we introduce
pattern-aware reasoning that involves critical reasoning patterns such as
"planning" and "self-reflection" to emulate human forensic process. We further
propose a two-stage training pipeline to seamlessly internalize such deepfake
reasoning capacities into current MLLMs. Experiments on HydraFake dataset
reveal that although previous detectors show great generalization on
cross-model scenarios, they fall short on unseen forgeries and data domains.
Our Veritas achieves significant gains across different OOD scenarios, and is
capable of delivering transparent and faithful detection outputs.

</details>


### [89] [FakeParts: a New Family of AI-Generated DeepFakes](https://arxiv.org/abs/2508.21052)
*Gaetan Brison,Soobash Daiboo,Samy Aimeur,Awais Hussain Sani,Xi Wang,Gianni Franchi,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: FakeParts是一种新的深度伪造技术，通过局部操控视频中特定的空间区域或时间段，使其更具迷惑性且更难检测。研究还提出了FakePartsBench，一个包含超过25K视频的基准数据集，用于全面评估检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有的深度伪造检测方法无法有效发现部分操控性伪造的漏洞。研究旨在揭示这一紧迫问题并提供基准优化资源。

Method: 创建并开源了FakePartsBench数据集，该数据集包含25K以上提供像素及帧级别标注的部分伪造视频，并通过用户研究和检测模型性能测试验证该数据集的重要性。

Result: FakeParts降低了人类对部分伪造视频的检测率30%以上，且现有最先进的检测模型在该场景下表现也显著下降。这个研究表明，当前检测技术对部分伪造视频的检测存在重大缺陷。

Conclusion: 部分伪造技术是对当前视频检测的严峻挑战。FakePartsBench为开发更稳健的检测方法提供了有力支持。

Abstract: We introduce FakeParts, a new class of deepfakes characterized by subtle,
localized manipulations to specific spatial regions or temporal segments of
otherwise authentic videos. Unlike fully synthetic content, these partial
manipulations, ranging from altered facial expressions to object substitutions
and background modifications, blend seamlessly with real elements, making them
particularly deceptive and difficult to detect. To address the critical gap in
detection capabilities, we present FakePartsBench, the first large-scale
benchmark dataset specifically designed to capture the full spectrum of partial
deepfakes. Comprising over 25K videos with pixel-level and frame-level
manipulation annotations, our dataset enables comprehensive evaluation of
detection methods. Our user studies demonstrate that FakeParts reduces human
detection accuracy by over 30% compared to traditional deepfakes, with similar
performance degradation observed in state-of-the-art detection models. This
work identifies an urgent vulnerability in current deepfake detection
approaches and provides the necessary resources to develop more robust methods
for partial video manipulations.

</details>


### [90] [POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models](https://arxiv.org/abs/2508.21019)
*Jiaxiang Cheng,Bing Ma,Xuhua Ren,Hongyi Jin,Kai Yu,Peng Zhang,Wenyue Li,Yuan Zhou,Tianxiang Zheng,Qinglin Lu*

Main category: cs.CV

TL;DR: 提出POSE框架，通过两阶段步骤和增强一致性的机制显著提高视频扩散模型的生成效率，达到100倍加速。


<details>
  <summary>Details</summary>
Motivation: 当前视频扩散生成面临采样效率瓶颈，尤其在大规模模型和长序列生成中。现有方法未能很好地解决视频帧时序一致性问题，且不支持单步大规模模型蒸馏。

Method: 引入POSE框架，包括：(i) 稳定初始阶段，稳定单步生成器从高到低信噪比的训练轨迹；(ii) 统一自对抗平衡机制，实现在高斯噪声空间的单步对抗训练平衡；(iii) 条件对抗一致性，提升条件帧和生成帧之间的语义一致性与帧一致性。

Result: POSE在VBench-I2V测试中，语义对齐、时序一致性和帧质量平均提升7.15%；预训练模型延迟从1000秒降至10秒，实现100倍加速，同时保持性能竞争力。

Conclusion: POSE方法有效解决视频扩散生成效率低下的问题，通过高效蒸馏和一致性提升，在性能和速度上均显著优于现有方法。

Abstract: The field of video diffusion generation faces critical bottlenecks in
sampling efficiency, especially for large-scale models and long sequences.
Existing video acceleration methods adopt image-based techniques but suffer
from fundamental limitations: they neither model the temporal coherence of
video frames nor provide single-step distillation for large-scale video models.
To bridge this gap, we propose POSE (Phased One-Step Equilibrium), a
distillation framework that reduces the sampling steps of large-scale video
diffusion models, enabling the generation of high-quality videos in a single
step. POSE employs a carefully designed two-phase process to distill video
models:(i) stability priming: a warm-up mechanism to stabilize adversarial
distillation that adapts the high-quality trajectory of the one-step generator
from high to low signal-to-noise ratio regimes, optimizing the video quality of
single-step mappings near the endpoints of flow trajectories. (ii) unified
adversarial equilibrium: a flexible self-adversarial distillation mechanism
that promotes stable single-step adversarial training towards a Nash
equilibrium within the Gaussian noise space, generating realistic single-step
videos close to real videos. For conditional video generation, we propose (iii)
conditional adversarial consistency, a method to improve both semantic
consistency and frame consistency between conditional frames and generated
frames. Comprehensive experiments demonstrate that POSE outperforms other
acceleration methods on VBench-I2V by average 7.15% in semantic alignment,
temporal conference and frame quality, reducing the latency of the pre-trained
model by 100$\times$, from 1000 seconds to 10 seconds, while maintaining
competitive performance.

</details>


### [91] [Reusing Computation in Text-to-Image Diffusion for Efficient Generation of Image Sets](https://arxiv.org/abs/2508.21032)
*Dale Decatur,Thibault Groueix,Wang Yifan,Rana Hanocka,Vladimir Kim,Matheus Gadelha*

Main category: cs.CV

TL;DR: 本研究提出了一种方法，通过利用扩散模型的粗到细特性，减少文本到图像生成中的冗余计算，从而降低计算成本并提高图像质量。


<details>
  <summary>Details</summary>
Motivation: 在高质量文本到图像生成模型中，计算成本较高，现有方法优化单次推理效率，但没有关注多个相关提示词带来的重复计算。

Method: 提出了一种训练无关的方法，通过基于语义相似性聚类提示词，共享扩散模型前期的计算过程，从而减少冗余计算。同时结合UnClip模型提取文本到图像的先验信息，优化扩散步数的分配。

Result: 实验结果表明，对于基于图像嵌入条件训练的模型，该方法显著减少了计算成本，同时提高了生成图像的质量。

Conclusion: 该方法可以无缝集成到现有流程中，能够扩展至大规模提示词集合，并有效降低大型文本到图像生成任务的环境和经济成本。

Abstract: Text-to-image diffusion models enable high-quality image generation but are
computationally expensive. While prior work optimizes per-inference efficiency,
we explore an orthogonal approach: reducing redundancy across correlated
prompts. Our method leverages the coarse-to-fine nature of diffusion models,
where early denoising steps capture shared structures among similar prompts. We
propose a training-free approach that clusters prompts based on semantic
similarity and shares computation in early diffusion steps. Experiments show
that for models trained conditioned on image embeddings, our approach
significantly reduces compute cost while improving image quality. By leveraging
UnClip's text-to-image prior, we enhance diffusion step allocation for greater
efficiency. Our method seamlessly integrates with existing pipelines, scales
with prompt sets, and reduces the environmental and financial burden of
large-scale text-to-image generation. Project page:
https://ddecatur.github.io/hierarchical-diffusion/

</details>


### [92] [Mitosis detection in domain shift scenarios: a Mamba-based approach](https://arxiv.org/abs/2508.21033)
*Gennaro Percannella,Mattia Sarno,Francesco Tortorella,Mario Vento*

Main category: cs.CV

TL;DR: 本研究提出了一种基于Mamba的方法用于在存在领域偏移的情况下进行组织病理学图像中的有丝分裂检测，同时采用VM-UNet架构和染色增强技术以提高模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 通过机器学习算法协助病理医师进行精准的有丝分裂检测，解决因领域偏移导致的算法性能下降问题。

Method: 提出基于Mamba方法的VM-UNet架构并结合染色增强操作，以提高模型应对领域偏移的能力。

Result: 初步实验基于MIDOG++数据集，结果显示该方法仍有很大的改进空间。

Conclusion: 尽管方法存在改进空间，但其探索了在领域偏移条件下进行有效有丝分裂检测的可能性。

Abstract: Mitosis detection in histopathology images plays a key role in tumor
assessment. Although machine learning algorithms could be exploited for aiding
physicians in accurately performing such a task, these algorithms suffer from
significative performance drop when evaluated on images coming from domains
that are different from the training ones. In this work, we propose a
Mamba-based approach for mitosis detection under domain shift, inspired by the
promising performance demonstrated by Mamba in medical imaging segmentation
tasks. Specifically, our approach exploits a VM-UNet architecture for carrying
out the addressed task, as well as stain augmentation operations for further
improving model robustness against domain shift. Our approach has been
submitted to the track 1 of the MItosis DOmain Generalization (MIDOG)
challenge. Preliminary experiments, conducted on the MIDOG++ dataset, show
large room for improvement for the proposed method.

</details>


### [93] [A multi-task neural network for atypical mitosis recognition under domain shift](https://arxiv.org/abs/2508.21035)
*Gennaro Percannella,Mattia Sarno,Francesco Tortorella,Mario Vento*

Main category: cs.CV

TL;DR: 本文提出一种基于多任务学习的方法来改善机器学习模型在医学图像领域迁移中的性能表现问题，特别是在肿瘤组织切片的非典型有丝分裂的检测任务中。


<details>
  <summary>Details</summary>
Motivation: 当前用于识别组织病理图像中非典型有丝分裂的机器学习模型在遇到领域迁移时性能急剧下降，需要新方法解决该挑战。

Method: 采用多任务学习，通过利用与主要分类任务相关的辅助任务，使模型专注于分类对象，忽略图像中随领域变化的背景。

Result: 在各类数据集上进行初步评估中表现良好，包括MIDOG 2025训练集、Ami-Br数据集以及MIDOG25挑战的测试集。

Conclusion: 多任务学习方法能够有效应对领域迁移问题，为自动化医学图像分析提供了潜在的解决方案。

Abstract: Recognizing atypical mitotic figures in histopathology images allows
physicians to correctly assess tumor aggressiveness. Although machine learning
models could be exploited for automatically performing such a task, under
domain shift these models suffer from significative performance drops. In this
work, an approach based on multi-task learning is proposed for addressing this
problem. By exploiting auxiliary tasks, correlated to the main classification
task, the proposed approach, submitted to the track 2 of the MItosis DOmain
Generalization (MIDOG) challenge, aims to aid the model to focus only on the
object to classify, ignoring the domain varying background of the image. The
proposed approach shows promising performance in a preliminary evaluation
conducted on three distinct datasets, i.e., the MIDOG 2025 Atypical Training
Set, the Ami-Br dataset, as well as the preliminary test set of the MIDOG25
challenge.

</details>


### [94] [MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for Efficient Video LLMs](https://arxiv.org/abs/2508.21044)
*Junpeng Ma,Qizhe Zhang,Ming Lu,Zhibin Wang,Qiang Zhou,Jun Song,Shanghang Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的训练无关视觉token裁剪框架MMG-Vid，显著提升效率并保持高性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在进行视频视觉理解时，面临视觉token数量过多带来的计算挑战，并未充分考虑视频帧的动态特性及时间依赖性。

Method: 提出基于段级和token级最大化边际收益的裁剪框架。首先基于帧相似性将视频分段，并动态分配各段的token预算；随后设计时间指导的DPC算法，联合建模帧间独特性和帧内多样性。

Result: MMG-Vid能够在保持超过99.5%原始性能的情况下，有效减少75%的视觉token，并加速预填阶段3.9倍。

Conclusion: MMG-Vid 显著提高了视频大语言模型的推理效率，而无需显著牺牲模型性能。

Abstract: Video Large Language Models (VLLMs) excel in video understanding, but their
excessive visual tokens pose a significant computational challenge for
real-world applications. Current methods aim to enhance inference efficiency by
visual token pruning. However, they do not consider the dynamic characteristics
and temporal dependencies of video frames, as they perceive video understanding
as a multi-frame task. To address these challenges, we propose MMG-Vid, a novel
training-free visual token pruning framework that removes redundancy by
Maximizing Marginal Gains at both segment-level and token-level. Specifically,
we first divide the video into segments based on frame similarity, and then
dynamically allocate the token budget for each segment to maximize the marginal
gain of each segment. Subsequently, we propose a temporal-guided DPC algorithm
that jointly models inter-frame uniqueness and intra-frame diversity, thereby
maximizing the marginal gain of each token. By combining both stages, MMG-Vid
can maximize the utilization of the limited token budget, significantly
improving efficiency while maintaining strong performance. Extensive
experiments demonstrate that MMG-Vid can maintain over 99.5% of the original
performance, while effectively reducing 75% visual tokens and accelerating the
prefilling stage by 3.9x on LLaVA-OneVision-7B. Code will be released soon.

</details>


### [95] [CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification](https://arxiv.org/abs/2508.21046)
*Wei Li,Renshan Zhang,Rui Shao,Jie He,Liqiang Nie*

Main category: cs.CV

TL;DR: CogVLA 是一种优化效率和性能的新型视听-语言-行动框架，通过指令驱动的路由和稀疏化方法实现高效率训练并在多个任务中取得新记录，同时减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的视听-语言-行动模型通常依赖于预训练模型并需要耗时的后续训练，导致计算开销增大，限制了其可扩展性和部署能力。

Method: CogVLA 提出了一种3阶段渐进式架构：1) 使用编码器-条件调制网络(EFA-Routing)将指令信息注入视觉编码器，压缩视觉数据形成指令感知的潜在表示；2) 使用语言模型-条件调制网络(LFP-Routing)引入行动意图，基于指令相关性修剪视觉信息，实现稀疏化；3) 引入V-L-A耦合注意力，结合因果视觉语言注意力与双向行动并行解码，以确保压缩输入支持准确行动生成。

Result: 在LIBERO 基准测试和真实机器人任务中，CogVLA 以97.4%的成功率和70.0%的真实任务成功率实现了最新的状态表现，同时将训练成本降低了2.5倍，推理延迟减少了2.8倍。

Conclusion: CogVLA 提高效率与性能，在视觉-语言-行动任务中展现强大能力，并显著减少了资源需求，助力更广泛的应用场景。

Abstract: Recent Vision-Language-Action (VLA) models built on pre-trained
Vision-Language Models (VLMs) require extensive post-training, resulting in
high computational overhead that limits scalability and deployment.We propose
CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages
instruction-driven routing and sparsification to improve both efficiency and
performance. CogVLA draws inspiration from human multimodal coordination and
introduces a 3-stage progressive architecture. 1) Encoder-FiLM based
Aggregation Routing (EFA-Routing) injects instruction information into the
vision encoder to selectively aggregate and compress dual-stream visual tokens,
forming a instruction-aware latent representation. 2) Building upon this
compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)
introduces action intent into the language model by pruning
instruction-irrelevant visually grounded tokens, thereby achieving token-level
sparsity. 3) To ensure that compressed perception inputs can still support
accurate and coherent action generation, we introduce V-L-A Coupled Attention
(CAtten), which combines causal vision-language attention with bidirectional
action parallel decoding. Extensive experiments on the LIBERO benchmark and
real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art
performance with success rates of 97.4% and 70.0%, respectively, while reducing
training costs by 2.5-fold and decreasing inference latency by 2.8-fold
compared to OpenVLA. CogVLA is open-sourced and publicly available at
https://github.com/JiuTian-VL/CogVLA.

</details>


### [96] [Multi-View 3D Point Tracking](https://arxiv.org/abs/2508.21060)
*Frano Rajič,Haofei Xu,Marko Mihajlovic,Siyuan Li,Irem Demir,Emircan Gündoğdu,Lei Ke,Sergey Prokudin,Marc Pollefeys,Siyu Tang*

Main category: cs.CV

TL;DR: 本文提出了一种数据驱动的多视角3D点追踪方法，用于通过多摄像机视图追踪动态场景中的任意点，具有鲁棒性和高精度。


<details>
  <summary>Details</summary>
Motivation: 现有单目追踪方法易受深度歧义和遮挡的影响，多摄像方法需要大量摄像机和复杂的优化流程，亟需一种实用且高效的3D追踪解决方案。

Method: 在已知相机姿态及多视角深度数据的基础上，融合多视角特征，利用k近邻相关性和基于transformer的更新机制，预测3D对应关系。

Result: 在Panoptic Studio和DexYCB数据集上的中位轨迹误差分别为3.1 cm和2.0 cm，适用于1-8视角及不同场景长度。

Conclusion: 本文方法设立了多视角3D追踪研究的新标准，提供了一种适用于真实场景的实用工具，方法及数据集已公开。

Abstract: We introduce the first data-driven multi-view 3D point tracker, designed to
track arbitrary points in dynamic scenes using multiple camera views. Unlike
existing monocular trackers, which struggle with depth ambiguities and
occlusion, or prior multi-camera methods that require over 20 cameras and
tedious per-sequence optimization, our feed-forward model directly predicts 3D
correspondences using a practical number of cameras (e.g., four), enabling
robust and accurate online tracking. Given known camera poses and either
sensor-based or estimated multi-view depth, our tracker fuses multi-view
features into a unified point cloud and applies k-nearest-neighbors correlation
alongside a transformer-based update to reliably estimate long-range 3D
correspondences, even under occlusion. We train on 5K synthetic multi-view
Kubric sequences and evaluate on two real-world benchmarks: Panoptic Studio and
DexYCB, achieving median trajectory errors of 3.1 cm and 2.0 cm, respectively.
Our method generalizes well to diverse camera setups of 1-8 views with varying
vantage points and video lengths of 24-150 frames. By releasing our tracker
alongside training and evaluation datasets, we aim to set a new standard for
multi-view 3D tracking research and provide a practical tool for real-world
applications. Project page available at https://ethz-vlg.github.io/mvtracker.

</details>


### [97] [OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning](https://arxiv.org/abs/2508.21066)
*Yuan Gong,Xionghui Wang,Jie Wu,Shiyin Wang,Yitong Wang,Xinglong Wu*

Main category: cs.CV

TL;DR: OneReward提供了一个通过单一奖励模型改善多任务生成能力的统一强化学习框架。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多任务生成时依赖任务特定的监督微调(SFT)，限制了泛化能力和训练效率。

Method: 提出OneReward框架，采用单一视觉语言模型作为奖励，结合多任务强化学习，训练了Seedream 3.0 Fill模型，实现了无需任务特定微调的统一编辑功能。

Result: 实验显示，该模型在多维评估标准下优于多个商业和开源竞争对手。

Conclusion: OneReward框架成功通过多任务强化学习方法改进模型生成能力，提升了一致性和效率。

Abstract: In this paper, we introduce OneReward, a unified reinforcement learning
framework that enhances the model's generative capabilities across multiple
tasks under different evaluation criteria using only \textit{One Reward} model.
By employing a single vision-language model (VLM) as the generative reward
model, which can distinguish the winner and loser for a given task and a given
evaluation criterion, it can be effectively applied to multi-task generation
models, particularly in contexts with varied data and diverse task objectives.
We utilize OneReward for mask-guided image generation, which can be further
divided into several sub-tasks such as image fill, image extend, object
removal, and text rendering, involving a binary mask as the edit area. Although
these domain-specific tasks share same conditioning paradigm, they differ
significantly in underlying data distributions and evaluation metrics. Existing
methods often rely on task-specific supervised fine-tuning (SFT), which limits
generalization and training efficiency. Building on OneReward, we develop
Seedream 3.0 Fill, a mask-guided generation model trained via multi-task
reinforcement learning directly on a pre-trained base model, eliminating the
need for task-specific SFT. Experimental results demonstrate that our unified
edit model consistently outperforms both commercial and open-source
competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across
multiple evaluation dimensions. Code and model are available at:
https://one-reward.github.io

</details>


### [98] [First-Place Solution to NeurIPS 2024 Invisible Watermark Removal Challenge](https://arxiv.org/abs/2508.21072)
*Fahad Shamshad,Tameem Bakr,Yahia Shaaban,Noor Hussein,Karthik Nandakumar,Nils Lukas*

Main category: cs.CV

TL;DR: 本文提出了一种用于挑战水印鲁棒性的攻击方案，通过实验表明，该方法在几乎不影响图像质量的情况下实现了高效的水印去除。


<details>
  <summary>Details</summary>
Motivation: 探讨当前水印技术是否能够抵抗对抗性攻击，验证其鲁棒性并鼓励发展更强的水印方法。

Method: 在beige-box轨道中，提出了一种基于自适应VAE的规避攻击，同时采用测试时优化和CIELAB空间的颜色对比修复。对于black-box轨道，先基于空间或频域的特征进行图像聚类，之后使用控制噪声注入及ChatGPT生成说明的扩散模型对各类群参数进行优化处理。

Result: 方法成功实现了95.7%的水印去除率，并且对图像本身的质量几乎无影响。

Conclusion: 研究表明当前水印技术仍然存在漏洞，研究希望能推动更强大和鲁棒的水印方法的开发。

Abstract: Content watermarking is an important tool for the authentication and
copyright protection of digital media. However, it is unclear whether existing
watermarks are robust against adversarial attacks. We present the winning
solution to the NeurIPS 2024 Erasing the Invisible challenge, which
stress-tests watermark robustness under varying degrees of adversary knowledge.
The challenge consisted of two tracks: a black-box and beige-box track,
depending on whether the adversary knows which watermarking method was used by
the provider. For the beige-box track, we leverage an adaptive VAE-based
evasion attack, with a test-time optimization and color-contrast restoration in
CIELAB space to preserve the image's quality. For the black-box track, we first
cluster images based on their artifacts in the spatial or frequency-domain.
Then, we apply image-to-image diffusion models with controlled noise injection
and semantic priors from ChatGPT-generated captions to each cluster with
optimized parameter settings. Empirical evaluations demonstrate that our method
successfully achieves near-perfect watermark removal (95.7%) with negligible
impact on the residual image's quality. We hope that our attacks inspire the
development of more robust image watermarking methods.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [99] [Social Bias in Multilingual Language Models: A Survey](https://arxiv.org/abs/2508.20201)
*Lance Calvin Lim Gamboa,Yue Feng,Mark Lee*

Main category: cs.CL

TL;DR: 这篇论文调查了跨语言和非英语语境中的偏见评估与缓解方法，分析其语言多样性、文化意识及评估指标和缓解技术的选择，并指出目前研究中的方法设计差距和实验稀缺性。


<details>
  <summary>Details</summary>
Motivation: 旨在扩展对社会偏见的研究范围，从仅限于英语文本的分析延伸至多语言背景和非英语语境问题。

Method: 以系统综述的方式，分析相关研究中的语言和文化多样性、评估指标以及偏见缓解技术，找出常见问题和现有解决方案。

Result: 揭示了跨语言偏见研究中的方法选择偏差（如语言倾向性）、多语言缓解实验的匮乏等问题，并对现有方法进行总结。

Conclusion: 建议未来研究需注重多语言偏见领域的包容性、跨文化的适应性及与最前沿自然语言处理发展的结合。

Abstract: Pretrained multilingual models exhibit the same social bias as models
processing English texts. This systematic review analyzes emerging research
that extends bias evaluation and mitigation approaches into multilingual and
non-English contexts. We examine these studies with respect to linguistic
diversity, cultural awareness, and their choice of evaluation metrics and
mitigation techniques. Our survey illuminates gaps in the field's dominant
methodological design choices (e.g., preference for certain languages, scarcity
of multilingual mitigation experiments) while cataloging common issues
encountered and solutions implemented in adapting bias benchmarks across
languages and cultures. Drawing from the implications of our findings, we chart
directions for future research that can reinforce the multilingual bias
literature's inclusivity, cross-cultural appropriateness, and alignment with
state-of-the-art NLP advancements.

</details>


### [100] [Prompting Strategies for Language Model-Based Item Generation in K-12 Education: Bridging the Gap Between Small and Large Language Models](https://arxiv.org/abs/2508.20217)
*Mohammad Amini,Babak Ahmadi,Xiaomeng Xiong,Yilin Zhang,Christopher Qiao*

Main category: cs.CL

TL;DR: 本研究探讨如何通过语言模型自动生成用于形态学评估的多选题，以降低人工测试开发的成本和一致性问题，并提出了一种基于结构化提示策略和高效微调的方法。


<details>
  <summary>Details</summary>
Motivation: 旨在降低手动测试开发的成本和不一致性，同时提升中型语言模型的性能，以自动生成K-12语言评估题目。

Method: 采用两步研究：比较微调中型模型Gemma与未微调的大型模型GPT-3.5，同时评估包括多种结构化提示策略，如零样本、少样本、链式思维等；利用GPT-4.1模拟专家评分并验证生成的项目。

Result: 结构化提示策略（尤其是链式思维与顺序设计的结合）显著提升了中型模型Gemma的性能；Gemma生成的题目在结构对齐性和教学适宜性上优于GPT-3.5的零样本输出。

Conclusion: 研究证明，通过结构化提示和高效微调，可以在有限数据条件下显著提升中型模型在自动生成测试题领域的表现，提供了一种实用且可扩展的生成与验证评估项的工作流程。

Abstract: This study explores automatic generation (AIG) using language models to
create multiple choice questions (MCQs) for morphological assessment, aiming to
reduce the cost and inconsistency of manual test development. The study used a
two-fold approach. First, we compared a fine-tuned medium model (Gemma, 2B)
with a larger untuned one (GPT-3.5, 175B). Second, we evaluated seven
structured prompting strategies, including zero-shot, few-shot,
chain-of-thought, role-based, sequential, and combinations. Generated items
were assessed using automated metrics and expert scoring across five
dimensions. We also used GPT-4.1, trained on expert-rated samples, to simulate
human scoring at scale. Results show that structured prompting, especially
strategies combining chain-of-thought and sequential design, significantly
improved Gemma's outputs. Gemma generally produced more construct-aligned and
instructionally appropriate items than GPT-3.5's zero-shot responses, with
prompt design playing a key role in mid-size model performance. This study
demonstrates that structured prompting and efficient fine-tuning can enhance
midsized models for AIG under limited data conditions. We highlight the value
of combining automated metrics, expert judgment, and large-model simulation to
ensure alignment with assessment goals. The proposed workflow offers a
practical and scalable way to develop and validate language assessment items
for K-12.

</details>


### [101] [Integrating SystemC TLM into FMI 3.0 Co-Simulations with an Open-Source Approach](https://arxiv.org/abs/2508.20223)
*Andrei Mihai Albu,Giovanni Pollo,Alessio Burrello,Daniele Jahier Pagliari,Cristian Tesconi,Alessandra Neri,Dario Soldi,Fabio Autieri,Sara Vinco*

Main category: cs.CL

TL;DR: 本文介绍了一种将SystemC TLM模型集成到基于FMI的协同仿真工作流程中的开源方法。


<details>
  <summary>Details</summary>
Motivation: 应对汽车系统日益复杂的挑战，提高跨领域仿真技术的效率，解决SystemC TLM与其他工程领域模型互操作性差的问题。

Method: 通过将SystemC TLM组件封装为FMI 3.0协同仿真功能单元(FMUs)，并开发轻量级开源工具链，实现了异构仿真环境之间的无缝集成。

Result: 解决了时间同步和数据交换等技术难题，并通过案例研究验证了该方法的可行性和有效性。

Conclusion: 该方法展示了一个开源、高效的跨领域协同仿真集成方案，有助于解决复杂网络物理系统的设计和验证问题。

Abstract: The growing complexity of cyber-physical systems, particularly in automotive
applications, has increased the demand for efficient modeling and cross-domain
co-simulation techniques. While SystemC Transaction-Level Modeling (TLM)
enables effective hardware/software co-design, its limited interoperability
with models from other engineering domains poses integration challenges. This
paper presents a fully open-source methodology for integrating SystemC TLM
models into Functional Mock-up Interface (FMI)-based co-simulation workflows.
By encapsulating SystemC TLM components as FMI 3.0 Co Simulation Functional
Mock-up Units (FMUs), the proposed approach facilitates seamless, standardized
integration across heterogeneous simulation environments. We introduce a
lightweight open-source toolchain, address key technical challenges such as
time synchronization and data exchange, and demonstrate the feasibility and
effectiveness of the integration through representative case studies.

</details>


### [102] [Can Compact Language Models Search Like Agents? Distillation-Guided Policy Optimization for Preserving Agentic RAG Capabilities](https://arxiv.org/abs/2508.20324)
*Rikuto Kotoge,Mai Nishimura,Jiaxin Ma*

Main category: cs.CL

TL;DR: 提出了一种名为DGPO的强化学习方法，通过从教师示范的冷启动初始化和持续教师指导，解决了紧凑型语言模型在推理能力不足训练困难的问题，并引入了ARC指标来系统评估该方法。


<details>
  <summary>Details</summary>
Motivation: 紧凑型语言模型因推理能力差导致奖励稀疏和训练不稳定的弱点，使其在强化学习中很难实现高效的代理性行为（如搜索和规划）。

Method: 提出了一种名为Distillation-Guided Policy Optimization（DGPO）的方法，包括从教师模型的示范中冷启动初始化以及在策略优化过程中由教师模型提供连续指导。

Result: 实验结果表明，DGPO使紧凑型模型能够实现复杂的代理性搜索行为，在某些情况下甚至超过了较大的教师模型。

Conclusion: DGPO为计算资源受限环境中的代理性RAG行为提供了可行的解决方案。

Abstract: Reinforcement Learning has emerged as a post-training approach to elicit
agentic RAG behaviors such as search and planning from language models.
However, compact language models (e.g., 0.5B parameters) struggle due to poor
reasoning ability, resulting in sparse rewards and unstable training. To
overcome these difficulties, we propose Distillation-Guided Policy Optimization
(DGPO), which addresses the challenges through cold-start initialization from
teacher demonstrations and continuous teacher guidance during policy
optimization. To systematically evaluate our approach, we introduce Agentic RAG
Capabilities (ARC), a fine-grained metric analyzing reasoning, search
coordination, and response synthesis. Comprehensive experiments demonstrate
that DGPO enables compact models to achieve sophisticated agentic search
behaviors, even outperforming the larger teacher model in some cases. DGPO
makes agentic RAG feasible in computing resource-constrained environments.

</details>


### [103] [GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs](https://arxiv.org/abs/2508.20325)
*Haibo Jin,Ruoxi Chen,Peiyan Zhang,Andy Zhou,Yang Zhang,Haohan Wang*

Main category: cs.CL

TL;DR: 提出了一种名为GUARD的方法，通过自动生成违反政府伦理指南的问题，测试大语言模型（LLMs）的合规性，并生成合规性报告。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs生成有害内容的潜力引发了社会与监管的关注，政府提出了伦理指南，但如何将其转化为具体测试问题尚存挑战。

Method: 设计了一种称为GUARD的测试方法，通过自动生成违反政府伦理指南的问题，以及"越狱"诊断场景（GUARD-JD），评估LLMs是否符合指南要求，并生成合规性报告。

Result: 通过实验证明，GUARD在七种LLMs上测试了三种政府伦理指南下的合规性，并扩展到视觉语言模型，验证了方法的有效性。

Conclusion: GUARD在促进LLMs合规性和可靠应用领域展现了有效性，是伦理指南具体操作化的一种成功尝试。

Abstract: As Large Language Models become increasingly integral to various domains,
their potential to generate harmful responses has prompted significant societal
and regulatory concerns. In response, governments have issued ethics guidelines
to promote the development of trustworthy AI. However, these guidelines are
typically high-level demands for developers and testers, leaving a gap in
translating them into actionable testing questions to verify LLM compliance.
  To address this challenge, we introduce GUARD (\textbf{G}uideline
\textbf{U}pholding Test through \textbf{A}daptive \textbf{R}ole-play and
Jailbreak \textbf{D}iagnostics), a testing method designed to operationalize
guidelines into specific guideline-violating questions that assess LLM
adherence. To implement this, GUARD uses automated generation of
guideline-violating questions based on government-issued guidelines, thereby
testing whether responses comply with these guidelines. When responses directly
violate guidelines, GUARD reports inconsistencies. Furthermore, for responses
that do not directly violate guidelines, GUARD integrates the concept of
``jailbreaks'' to diagnostics, named GUARD-JD, which creates scenarios that
provoke unethical or guideline-violating responses, effectively identifying
potential scenarios that could bypass built-in safety mechanisms. Our method
finally culminates in a compliance report, delineating the extent of adherence
and highlighting any violations.
  We have empirically validated the effectiveness of GUARD on seven LLMs,
including Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4,
GPT-4o, and Claude-3.7, by testing compliance under three government-issued
guidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can
transfer jailbreak diagnostics to vision-language models, demonstrating its
usage in promoting reliable LLM-based applications.

</details>


### [104] [Joint Enhancement of Relational Reasoning for Long-Context LLMs](https://arxiv.org/abs/2508.20351)
*Zhirui Chen,Wei Shen,Jiashui Huang,Ling Shao*

Main category: cs.CL

TL;DR: 本文提出JERR框架，通过图形化的推理提升大语言模型（LLM）的长上下文理解能力。


<details>
  <summary>Details</summary>
Motivation: 目前的大语言模型在长上下文理解和复杂任务处理方面存在局限，尤其是透明性与准确性不足问题。

Method: JERR包含三个核心组件：提纲提取（分块文本以提高总结效率）、图构建（通过有向无环图消除冗余并保持逻辑一致性）、关系推理（利用蒙特卡洛树搜索提高推理能力）。

Result: 实验表明，JERR在ROUGE和F1指标上均优于所有基线模型，并在LLM-Rater评价中获得最高分。

Conclusion: JERR提高了LLM处理长上下文与复杂推理任务的可靠性和透明性，展示了一种创新解决方案。

Abstract: Despite significant progress, large language models (LLMs) still struggle
with long contexts due to memory limitations and their inability to tackle
complex and long-context tasks. Additionally, LLMs often suffer from a lack of
transparency and are prone to producing hallucinations. To address these
challenges, we propose \textbf{JERR}, a novel framework designed to enhance
long-context comprehension via graph-based reasoning in LLMs. JERR integrates
three key components: synopsis extraction, graph construction, and relational
reasoning. First, synopsis is extracted by chunking text strategically,
allowing the model to summarize and understand information more efficiently.
Second, we build a directed acyclic graph (DAG) to resolve redundancy, ensuring
logical consistency and clarity. Finally, we incorporate Monte Carlo Tree
Search (MCTS) to help the model navigate complex reasoning paths, ensuring more
accurate and interpretable outputs. This framework provides a novel solution
that enables LLMs to handle extended contexts and complex reasoning tasks with
improved reliability and transparency. Experimental results show that JERR
consistently outperforms all baselines on the ROUGE and F1 metrics, achieving
the highest scores on the LLM-Rater evaluation.

</details>


### [105] [Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems](https://arxiv.org/abs/2508.20373)
*Yuyao Wang,Bowen Liu,Jianheng Tang,Nuo Chen,Yuhan Li,Qifan Zhang,Jia Li*

Main category: cs.CL

TL;DR: 提出以NP难图问题作为新的合成训练语料，并开发两阶段的后期训练框架以提升大语言模型的长链式推理能力。


<details>
  <summary>Details</summary>
Motivation: 由于开发长链式推理能力需要依赖高质量且代价较高的数据集，作者探究可扩展的替代方法。

Method: 引入NP难图问题作为训练语料，并设计了两阶段训练框架：监督微调用于提升推理深度，强化学习用于优化推理效率。

Result: 提出的Graph-R1-7B模型在多个领域具备优越的泛化能力，并在NP难图问题上超越了QwQ-32B。

Conclusion: NP难图问题是一种有效且可扩展的资源，可推动大语言模型的长链式推理能力的发展。

Abstract: Reasoning Large Language Models (RLLMs) have recently achieved remarkable
progress on complex reasoning tasks, largely enabled by their long
chain-of-thought (Long CoT) capabilities. However, developing these Long CoT
behaviors relies heavily on post-training with high-quality datasets, which are
typically costly and human-curated (e.g., mathematics and code), leaving
scalable alternatives unexplored. In this work, we introduce NP-hard (NPH)
graph problems as a novel synthetic training corpus, as they inherently require
deep reasoning, extensive exploration, and reflective strategies, which are
core characteristics of Long CoT reasoning. Building on this insight, we
develop a two-stage post-training framework: (i) Long CoT Supervised
Fine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially
enhances reasoning depth, and (ii) Reinforcement Learning (RL) with a
fine-grained reward design, which sharpens reasoning efficiency. Our flagship
model, Graph-R1-7B, demonstrates strong generalization across mathematics,
coding, STEM, and logic, and surpasses QwQ-32B on NPH graph problems in both
accuracy and reasoning efficiency. These results position NPH graph problems as
an effective and scalable resource for advancing Long CoT reasoning in LLMs,
opening a new frontier for LLM post-training. Our implementation is available
at https://github.com/Graph-Reasoner/Graph-R1, with models and datasets hosted
in our Hugging Face collection HKUST-DSAIL/Graph-R1.

</details>


### [106] [CAPE: Context-Aware Personality Evaluation Framework for Large Language Models](https://arxiv.org/abs/2508.20385)
*Jivnesh Sandhan,Fei Cheng,Tushar Sandhan,Yugo Murawaki*

Main category: cs.CL

TL;DR: 本研究提出CAPE框架，探讨大型语言模型（LLMs）在有上下文交流历史中的行为一致性和人格偏移现象，发现上下文对一致性和性格稳定性产生显著影响。


<details>
  <summary>Details</summary>
Motivation: 目前对LLM性格评估的研究大多避免上下文影响，但现实应用中对话历史不可忽视，因此需研究上下文对LLM性格评估的影响。

Method: 提出了第一个上下文感知的性格评估框架（CAPE），并引入新指标来量化LLM响应的一致性，从而评估上下文历史的影响。

Result: 基于7种LLM深入实验，发现上下文增强了响应一致性，但引发了人格偏移，且不同模型对上下文敏感程度和一致性特征差异显著。

Conclusion: 上下文对LLM的性格评估具有深远影响，应用该框架还能提高角色扮演代理的响应一致性和与人类判断的契合度。

Abstract: Psychometric tests, traditionally used to assess humans, are now being
applied to Large Language Models (LLMs) to evaluate their behavioral traits.
However, existing studies follow a context-free approach, answering each
question in isolation to avoid contextual influence. We term this the Disney
World test, an artificial setting that ignores real-world applications, where
conversational history shapes responses. To bridge this gap, we propose the
first Context-Aware Personality Evaluation (CAPE) framework for LLMs,
incorporating prior conversational interactions. To thoroughly analyze the
influence of context, we introduce novel metrics to quantify the consistency of
LLM responses, a fundamental trait in human behavior.
  Our exhaustive experiments on 7 LLMs reveal that conversational history
enhances response consistency via in-context learning but also induces
personality shifts, with GPT-3.5-Turbo and GPT-4-Turbo exhibiting extreme
deviations. While GPT models are robust to question ordering, Gemini-1.5-Flash
and Llama-8B display significant sensitivity. Moreover, GPT models response
stem from their intrinsic personality traits as well as prior interactions,
whereas Gemini-1.5-Flash and Llama--8B heavily depend on prior interactions.
Finally, applying our framework to Role Playing Agents (RPAs) shows
context-dependent personality shifts improve response consistency and better
align with human judgments. Our code and datasets are publicly available at:
https://github.com/jivnesh/CAPE

</details>


### [107] [Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction](https://arxiv.org/abs/2508.20395)
*Xu Guo*

Main category: cs.CL

TL;DR: 本文分析了在生成中预测推理步骤对最终答案正确性的实用性，通过研究条件熵的变化，发现减少的条件熵与正确答案关联紧密，长推理路径并不总是更好。


<details>
  <summary>Details</summary>
Motivation: 研究推理步骤在提高大型语言模型最终答案准确性中的作用，并探索如何在生成过程中识别不必要的推理步骤以提升效率和结果的可靠性。

Method: 通过在MATH数据集上使用Qwen2.5-32B和GPT-4o生成推理链，再用Qwen3-8B量化这些推理链对最终准确性的实用性，具体测量各步骤上的答案段Y的条件熵随上下文扩展的变化。

Result: 结果表明：条件熵随步骤减少与正确答案强相关，而保持平稳或增加的情形通常导致错误答案。此外，错误推理路径通常更长，表明长推理不一定更优。

Conclusion: 条件熵的变化模式可作为预测推理实用性的重要指标，这为设计高效推理流程（能够检测和避免无效推理）提供了基础。

Abstract: Recent advancements in large language models (LLMs) often rely on generating
intermediate reasoning steps to enhance accuracy. However, little work has
examined how reasoning utility contributes to the final answer's correctness.
Due to the stochastic nature of autoregressive generation, generating more
context does not guarantee increased confidence in the answer. If we could
predict, during generation, whether a reasoning step will be useful, we could
stop early or prune ineffective steps, avoiding distractions in the final
decision.
  We present an oracle study on MATH dataset, using Qwen2.5-32B and GPT-4o to
generate reasoning chains, and then employing a separate model (Qwen3-8B) to
quantify the utility of these chains for final accuracy. Specifically, we
measure the model's uncertainty on the answer span Y at each reasoning step
using conditional entropy (expected negative log-likelihood over the
vocabulary) with context expanding step by step. Our results show a clear
pattern: conditional entropy that decreases over steps is strongly associated
with correct answers, whereas flat or increasing entropy often results in wrong
answers. We also corroborate that incorrect reasoning paths tend to be longer
than correct ones, suggesting that longer reasoning does not necessarily yield
better outcomes. These findings serve as a foundation to inspire future work on
designing efficient reasoning pipelines that detect and avoid unproductive
reasoning early.

</details>


### [108] [UI-Bench: A Benchmark for Evaluating Design Capabilities of AI Text-to-App Tools](https://arxiv.org/abs/2508.20410)
*Sam Jung,Agustin Garcinuno,Spencer Mateega*

Main category: cs.CL

TL;DR: UI-Bench是一项大规模基准测试，用于评估AI文本到应用工具的视觉表现；涵盖10个工具，30个提示，300个生成的网站，以及4000+专家评判。


<details>
  <summary>Details</summary>
Motivation: 目前针对AI文本转应用工具的质量验证缺乏公共基准，通过UI-Bench填补此空白，确保评估的公平性和可靠性。

Method: UI-Bench基准利用专家对生成网站的对比评估，结合TrueSkill模型排名，提供具信度区间的结果，并发布完整的提示集、开源框架及公共排行榜。

Result: 成功设计出一个涵盖多维度评估的基准测试，并提供了排名工具及高质量评价结果。

Conclusion: UI-Bench建立了一个可复现的标准，为AI驱动的网页设计发展方向提供了基准。

Abstract: AI text-to-app tools promise high quality applications and websites in
minutes, yet no public benchmark rigorously verifies those claims. We introduce
UI-Bench, the first large-scale benchmark that evaluates visual excellence
across competing AI text-to-app tools through expert pairwise comparison.
Spanning 10 tools, 30 prompts, 300 generated sites, and \textit{4000+} expert
judgments, UI-Bench ranks systems with a TrueSkill-derived model that yields
calibrated confidence intervals. UI-Bench establishes a reproducible standard
for advancing AI-driven web design. We release (i) the complete prompt set,
(ii) an open-source evaluation framework, and (iii) a public leaderboard. The
generated sites rated by participants will be released soon. View the UI-Bench
leaderboard at https://uibench.ai/leaderboard.

</details>


### [109] [DentalBench: Benchmarking and Advancing LLMs Capability for Bilingual Dentistry Understanding](https://arxiv.org/abs/2508.20416)
*Hengchuan Zhu,Yihuan Xu,Yichen Li,Zijie Meng,Zuozhu Liu*

Main category: cs.CL

TL;DR: 文章介绍了首个为口腔医学领域开发的大型语言模型基准测试工具DentalBench，包括36,597道中英文问题及一个大规模口腔医学语料库。作者评估了14种模型，并通过实验验证领域适配的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在医学领域表现优异，但在需要深度专业知识的专科领域（如口腔医学）中仍存在探索不足。需要针对性的评估工具来推动这方面的研究。

Method: 提出了DentalBench基准工具，包括DentalQA（中英文问答测试）和DentalCorpus（大规模高质量语料库）。通过评估14种模型，进行领域适配实验，检验其对专业任务和术语的表现。

Result: 表明现有医学语言模型在口腔医学领域的表现存在显著差距，领域适配可显著提升模型在知识密集型任务中的表现。

Conclusion: 领域适配和专业性基准测试对于开发面向医疗应用的可信赖且高效的语言模型至关重要。

Abstract: Recent advances in large language models (LLMs) and medical LLMs (Med-LLMs)
have demonstrated strong performance on general medical benchmarks. However,
their capabilities in specialized medical fields, such as dentistry which
require deeper domain-specific knowledge, remain underexplored due to the lack
of targeted evaluation resources. In this paper, we introduce DentalBench, the
first comprehensive bilingual benchmark designed to evaluate and advance LLMs
in the dental domain. DentalBench consists of two main components: DentalQA, an
English-Chinese question-answering (QA) benchmark with 36,597 questions
spanning 4 tasks and 16 dental subfields; and DentalCorpus, a large-scale,
high-quality corpus with 337.35 million tokens curated for dental domain
adaptation, supporting both supervised fine-tuning (SFT) and
retrieval-augmented generation (RAG). We evaluate 14 LLMs, covering
proprietary, open-source, and medical-specific models, and reveal significant
performance gaps across task types and languages. Further experiments with
Qwen-2.5-3B demonstrate that domain adaptation substantially improves model
performance, particularly on knowledge-intensive and terminology-focused tasks,
and highlight the importance of domain-specific benchmarks for developing
trustworthy and effective LLMs tailored to healthcare applications.

</details>


### [110] [KG-CQR: Leveraging Structured Relation Representations in Knowledge Graphs for Contextual Query Retrieval](https://arxiv.org/abs/2508.20417)
*Chi Minh Bui,Ngoc Mai Thieu,Van Vinh Nguyen,Json J. Jung,Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: 本研究提出了一种名为KG-CQR的新框架，通过结合知识图谱（KG）和大语言模型（LLMs）提高了检索扩展生成（RAG）系统的检索阶段性能，并取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决复杂输入查询的上下文表示问题，通过知识图谱构建语义丰富的查询上下文，从而提高检索阶段的有效性。

Method: 本研究提出了KG-CQR框架，包括子图提取、补全和上下文生成三个模块，结合知识图谱结构化关系表征提升查询丰富度，并设计为与多种LLMs兼容的模型无关流水线。

Result: 实验表明，KG-CQR在RAGBench和MultiHop-RAG数据集上，mAP和Recall@25分别提升了4-6%和2-3%。在多跳问答等复杂RAG任务中性能亦显著优于现有基线模型。

Conclusion: KG-CQR通过结合知识图谱与语言模型，显著提升了检索阶段的效果，为解决复杂查询的上下文表示问题提供了新方法。

Abstract: The integration of knowledge graphs (KGs) with large language models (LLMs)
offers significant potential to improve the retrieval phase of
retrieval-augmented generation (RAG) systems. In this study, we propose KG-CQR,
a novel framework for Contextual Query Retrieval (CQR) that enhances the
retrieval phase by enriching the contextual representation of complex input
queries using a corpus-centric KG. Unlike existing methods that primarily
address corpus-level context loss, KG-CQR focuses on query enrichment through
structured relation representations, extracting and completing relevant KG
subgraphs to generate semantically rich query contexts. Comprising subgraph
extraction, completion, and contextual generation modules, KG-CQR operates as a
model-agnostic pipeline, ensuring scalability across LLMs of varying sizes
without additional training. Experimental results on RAGBench and MultiHop-RAG
datasets demonstrate KG-CQR's superior performance, achieving a 4-6%
improvement in mAP and a 2-3% improvement in Recall@25 over strong baseline
models. Furthermore, evaluations on challenging RAG tasks such as multi-hop
question answering show that, by incorporating KG-CQR, the performance
consistently outperforms the existing baseline in terms of retrieval
effectiveness

</details>


### [111] [CAMB: A comprehensive industrial LLM benchmark on civil aviation maintenance](https://arxiv.org/abs/2508.20420)
*Feng Zhang,Chengjie Pang,Yuehan Zhang,Chenyu Luo*

Main category: cs.CL

TL;DR: 本文提出了一个专为民航维护设计的工业级基准，用以评估大语言模型（LLMs）在该领域的能力，并公开了相关工具以促进研究发展。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型评估偏向于数学和编程推理任务，缺乏针对民航维护领域的专门评估方法。

Method: 设计并开发了一个专注于民航维护的基准，评估大语言模型的能力，并进一步通过实验分析测试现有的嵌入模型及语言模型。

Result: 证明了基准在领域评估中的有效性，并揭示模型对民航维护领域知识和复杂推理任务的不足。

Conclusion: 所开发的基准为未来针对性改进语言模型能力（如领域微调、优化生成等）奠定了基础，并为民航维护系统智能解决方案的进步提供了方向。

Abstract: Civil aviation maintenance is a domain characterized by stringent industry
standards. Within this field, maintenance procedures and troubleshooting
represent critical, knowledge-intensive tasks that require sophisticated
reasoning. To address the lack of specialized evaluation tools for large
language models (LLMs) in this vertical, we propose and develop an
industrial-grade benchmark specifically designed for civil aviation
maintenance. This benchmark serves a dual purpose: It provides a standardized
tool to measure LLM capabilities within civil aviation maintenance, identifying
specific gaps in domain knowledge and complex reasoning. By pinpointing these
deficiencies, the benchmark establishes a foundation for targeted improvement
efforts (e.g., domain-specific fine-tuning, RAG optimization, or specialized
prompt engineering), ultimately facilitating progress toward more intelligent
solutions within civil aviation maintenance. Our work addresses a significant
gap in the current LLM evaluation, which primarily focuses on mathematical and
coding reasoning tasks. In addition, given that Retrieval-Augmented Generation
(RAG) systems are currently the dominant solutions in practical applications ,
we leverage this benchmark to evaluate existing well-known vector embedding
models and LLMs for civil aviation maintenance scenarios. Through experimental
exploration and analysis, we demonstrate the effectiveness of our benchmark in
assessing model performance within this domain, and we open-source this
evaluation benchmark and code to foster further research and
development:https://github.com/CamBenchmark/cambenchmark

</details>


### [112] [Searching the Title of Practical Work of the Informatics Engineering Bachelor Program with the Case Base Reasoning Method](https://arxiv.org/abs/2508.20442)
*Agung Sukrisna Jaya,Osvari Arsalan,Danny Matthew Saputra*

Main category: cs.CL

TL;DR: 该研究利用案例推理技术 (CBR) 和TF-IDF/Cosine Similarity方法，通过历史数据寻找相似的实践工作标题。


<details>
  <summary>Details</summary>
Motivation: 解决如何基于历史标题和关键字，通过案例推理找到匹配度较高的实践工作标题问题。

Method: 采用TF-IDF进行向量化，并使用Cosine Similarity计算相似度，系统可通过输入标题或关键词搜索标题匹配度。

Result: 测试了705个实践标题，采用既有和随机标题分别测试，其结果显示系统能以一致的匹配精度返回相同数量的结果。

Conclusion: 研究表明案例推理结合TF-IDF和余弦相似度可以可靠地应用于实践工作标题的匹配搜索。

Abstract: Case Base Reasoning (CBR) is a case solving technique based on experience in
cases that have occurred before with the highest similarity. CBR is used to
search for practical work titles. TF-IDF is applied to process the
vectorization of each practical work title word and Cosine Similarity for the
calculation of similarity values. This system can search either in the form of
titles or keywords. The output of the system is the title of practical work and
the match value of each title. Based on the test results using 705 practical
work titles, testing was carried out with five titles and carried out in two
stages. The first stage searches with existing titles and the second stage
randomizes the title from the first stage. And the results obtained in the
second stage are the same number of titles found and the highest average match
score.

</details>


### [113] [MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers](https://arxiv.org/abs/2508.20453)
*Zhenting Wang,Qi Chang,Hemani Patel,Shashank Biju,Cheng-En Wu,Quan Liu,Aolin Ding,Alireza Rezazadeh,Ankit Shah,Yujia Bao,Eugene Siow*

Main category: cs.CL

TL;DR: MCP-Bench 是一种用于评估大语言模型（LLMs）在多步骤任务上的基准方法，尤其是涉及工具使用、跨工具协调以及复杂任务解决方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试方法对复杂、多步骤任务的评估能力有限，无法涵盖工具选择、规划、推理等多方面需求。

Method: 基于 Model Context Protocol (MCP)，MCP-Bench利用28个代表性MCP服务器及250种工具，设计多步任务评估框架，并提供多层次的评估指标，比如工具使用、规划与任务完成能力。

Result: 针对20个高级LLMs的实验表明，现有模型在MCP-Bench上的表现仍存在显著挑战，显示其在复杂任务中的不足。

Conclusion: MCP-Bench 提供了一个更全面和逼真的测试基准，揭示了现有高级LLMs在解决复杂任务中的局限性，并为未来工具增强的模型开发提供了参考方向。

Abstract: We introduce MCP-Bench, a benchmark for evaluating large language models
(LLMs) on realistic, multi-step tasks that demand tool use, cross-tool
coordination, precise parameter control, and planning/reasoning for solving
tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28
representative live MCP servers spanning 250 tools across domains such as
finance, traveling, scientific computing, and academic search. Unlike prior
API-based benchmarks, each MCP server provides a set of complementary tools
designed to work together, enabling the construction of authentic, multi-step
tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability
to retrieve relevant tools from fuzzy instructions without explicit tool names,
plan multi-hop execution trajectories for complex objectives, ground responses
in intermediate tool outputs, and orchestrate cross-domain workflows -
capabilities not adequately evaluated by existing benchmarks that rely on
explicit tool specifications, shallow few-step workflows, and isolated domain
operations. We propose a multi-faceted evaluation framework covering tool-level
schema understanding and usage, trajectory-level planning, and task completion.
Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code
and data: https://github.com/Accenture/mcp-bench.

</details>


### [114] [Prediction of mortality and resource utilization in critical care: a deep learning approach using multimodal electronic health records with natural language processing techniques](https://arxiv.org/abs/2508.20460)
*Yucheng Ruan,Xiang Lan,Daniel J. Tan,Hairil Rizal Abdullah,Mengling Feng*

Main category: cs.CL

TL;DR: 该研究开发了一种深度学习框架，结合自然语言处理技术，利用多模态电子健康记录(EHRs)来预测ICU中的死亡率和资源利用情况，并展现了卓越的性能和抗数据损坏的能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要利用结构化的EHR数据，而忽略了自由文本中的临床信息。本研究试图通过多模态数据整合与深度学习方法，改善对ICU患者死亡率和资源利用预测的准确性和鲁棒性。

Method: 研究使用两个真实EHR数据集，通过提出的深度学习框架评估三个临床任务，重点分析医疗提示、自由文本以及预训练句子编码器的作用，并测试模型对结构化EHR数据损坏的鲁棒性。

Result: 在两个数据集的实验中，不同临床任务中新模型相比于最佳现有方法在各项指标上均有显著改进，如死亡率预测的BACC/AUROC提高1.6%/0.8%，手术时长估算的RMSE/MAE提高10.9%/11.0%。模型在结构化数据不同损坏率下始终表现优越。

Conclusion: 该框架是一种有效且准确的深度学习方法，能够预测ICU中的死亡率和资源利用情况，并验证了提示学习和Transformer编码器在多模态EHR分析中的潜力。尤其是在结构化数据损坏的情况下，模型表现出很强的鲁棒性。

Abstract: Background Predicting mortality and resource utilization from electronic
health records (EHRs) is challenging yet crucial for optimizing patient
outcomes and managing costs in intensive care unit (ICU). Existing approaches
predominantly focus on structured EHRs, often ignoring the valuable clinical
insights in free-text notes. Additionally, the potential of textual information
within structured data is not fully leveraged. This study aimed to introduce
and assess a deep learning framework using natural language processing
techniques that integrates multimodal EHRs to predict mortality and resource
utilization in critical care settings. Methods Utilizing two real-world EHR
datasets, we developed and evaluated our model on three clinical tasks with
leading existing methods. We also performed an ablation study on three key
components in our framework: medical prompts, free-texts, and pre-trained
sentence encoder. Furthermore, we assessed the model's robustness against the
corruption in structured EHRs. Results Our experiments on two real-world
datasets across three clinical tasks showed that our proposed model improved
performance metrics by 1.6\%/0.8\% on BACC/AUROC for mortality prediction,
0.5%/2.2% on RMSE/MAE for LOS prediction, 10.9%/11.0% on RMSE/MAE for surgical
duration estimation compared to the best existing methods. It consistently
demonstrated superior performance compared to other baselines across three
tasks at different corruption rates. Conclusions The proposed framework is an
effective and accurate deep learning approach for predicting mortality and
resource utilization in critical care. The study also highlights the success of
using prompt learning with a transformer encoder in analyzing multimodal EHRs.
Importantly, the model showed strong resilience to data corruption within
structured data, especially at high corruption levels.

</details>


### [115] [ConspirED: A Dataset for Cognitive Traits of Conspiracy Theories and Large Language Model Safety](https://arxiv.org/abs/2508.20468)
*Luke Bates,Max Glockner,Preslav Nakov,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本论文提出ConspirED数据集，用于研究阴谋论文本的认知特征，并评估现有人工智能模型对阴谋论内容的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 阴谋论削弱公众对科学和机构的信任，并对辟谣有很强的抵抗力，随着AI生成虚假信息的复杂性增加，分析阴谋论的修辞模式有助于开发干预措施。

Method: 提出并使用新数据集ConspirED，该数据集依据认知框架对多句阴谋论文本的认知特征进行了标注，随后训练计算模型识别这些特征并评估大语言模型对阴谋论内容的反应。

Result: 研究发现，无论是新开发的模型还是大语言模型在处理阴谋论内容时，都会被其特征所影响，甚至输出呈现类似的推理模式。

Conclusion: 理解阴谋论内容的认知特征对于开发干预措施和评估人工智能处理此类输入的能力至关重要。

Abstract: Conspiracy theories erode public trust in science and institutions while
resisting debunking by evolving and absorbing counter-evidence. As AI-generated
misinformation becomes increasingly sophisticated, understanding rhetorical
patterns in conspiratorial content is important for developing interventions
such as targeted prebunking and assessing AI vulnerabilities. We introduce
ConspirED (CONSPIR Evaluation Dataset), which captures the cognitive traits of
conspiratorial ideation in multi-sentence excerpts (80--120 words) from online
conspiracy articles, annotated using the CONSPIR cognitive framework
(Lewandowsky and Cook, 2020). ConspirED is the first dataset of conspiratorial
content annotated for general cognitive traits. Using ConspirED, we (i) develop
computational models that identify conspiratorial traits and determine dominant
traits in text excerpts, and (ii) evaluate large language/reasoning model
(LLM/LRM) robustness to conspiratorial inputs. We find that both are misaligned
by conspiratorial content, producing output that mirrors input reasoning
patterns, even when successfully deflecting comparable fact-checked
misinformation.

</details>


### [116] [Languages Still Left Behind: Toward a Better Multilingual Machine Translation Benchmark](https://arxiv.org/abs/2508.20511)
*Chihiro Taguchi,Seng Mai,Keita Kurabe,Yusuke Sakai,Georgina Agyei,Soudabeh Eslami,David Chiang*

Main category: cs.CL

TL;DR: 研究表明FLORES+多语言翻译基准存质量不足及文化偏向，提议采用更通用中立的评估方法。


<details>
  <summary>Details</summary>
Motivation: 探索FLORES+基准在多语言翻译评估中的适用性及其潜在不足。

Method: 分析四种语言数据，进行人工评估及测试高质量数据的模型表现，并设计新评估集。

Result: 发现FLORES+翻译质量不达标、文化偏向，且容易被简单策略利用；模型对FLORES+表现不佳但在新数据集表现出色。

Conclusion: 建议通过使用通用中立文本、减少命名实体依赖改进多语言MT评估基准，以更真实反映翻译挑战。

Abstract: Multilingual machine translation (MT) benchmarks play a central role in
evaluating the capabilities of modern MT systems. Among them, the FLORES+
benchmark is widely used, offering English-to-many translation data for over
200 languages, curated with strict quality control protocols. However, we study
data in four languages (Asante Twi, Japanese, Jinghpaw, and South Azerbaijani)
and uncover critical shortcomings in the benchmark's suitability for truly
multilingual evaluation. Human assessments reveal that many translations fall
below the claimed 90% quality standard, and the annotators report that source
sentences are often too domain-specific and culturally biased toward the
English-speaking world. We further demonstrate that simple heuristics, such as
copying named entities, can yield non-trivial BLEU scores, suggesting
vulnerabilities in the evaluation protocol. Notably, we show that MT models
trained on high-quality, naturalistic data perform poorly on FLORES+ while
achieving significant gains on our domain-relevant evaluation set. Based on
these findings, we advocate for multilingual MT benchmarks that use
domain-general and culturally neutral source texts rely less on named entities,
in order to better reflect real-world translation challenges.

</details>


### [117] [SciTopic: Enhancing Topic Discovery in Scientific Literature through Advanced LLM](https://arxiv.org/abs/2508.20514)
*Pengjiang Li,Zaitian Wang,Xinhao Zhang,Ran Zhang,Lu Jiang,Pengfei Wang,Yuanchun Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种基于大型语言模型（LLMs）的科学主题发现方法SciTopic，可通过对科学出版物的内容编码和文本优化显著提高主题识别能力。


<details>
  <summary>Details</summary>
Motivation: 目前的主题发现方法普遍依赖词嵌入技术，难以全面理解科学文献的语义关系，尤其在处理复杂、高维文本关系时表现出局限性。

Method: 方法包括构建文本编码器捕获科学出版物内容，优化空间模型结合基于熵的采样和LLMs指导的三元组任务，并通过优化对比损失微调文本编码器以更好地区分不同主题实例。

Result: 实验表明，SciTopic在三个真实世界科学出版物数据集上的表现优于现有最先进的方法（SOTA），提供更深刻且快速的主题洞察。

Conclusion: SciTopic通过结合LLMs有效提升科学主题的识别能力，为科研人员发现趋势提供支持。

Abstract: Topic discovery in scientific literature provides valuable insights for
researchers to identify emerging trends and explore new avenues for
investigation, facilitating easier scientific information retrieval. Many
machine learning methods, particularly deep embedding techniques, have been
applied to discover research topics. However, most existing topic discovery
methods rely on word embedding to capture the semantics and lack a
comprehensive understanding of scientific publications, struggling with
complex, high-dimensional text relationships. Inspired by the exceptional
comprehension of textual information by large language models (LLMs), we
propose an advanced topic discovery method enhanced by LLMs to improve
scientific topic identification, namely SciTopic. Specifically, we first build
a textual encoder to capture the content from scientific publications,
including metadata, title, and abstract. Next, we construct a space
optimization module that integrates entropy-based sampling and triplet tasks
guided by LLMs, enhancing the focus on thematic relevance and contextual
intricacies between ambiguous instances. Then, we propose to fine-tune the
textual encoder based on the guidance from the LLMs by optimizing the
contrastive loss of the triplets, forcing the text encoder to better
discriminate instances of different topics. Finally, extensive experiments
conducted on three real-world datasets of scientific publications demonstrate
that SciTopic outperforms the state-of-the-art (SOTA) scientific topic
discovery methods, enabling researchers to gain deeper and faster insights.

</details>


### [118] [Overview of BioASQ 2024: The twelfth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20532)
*Anastasios Nentidis,Georgios Katsimpras,Anastasia Krithara,Salvador Lima-López,Eulàlia Farré-Maduell,Martin Krallinger,Natalia Loukachevitch,Vera Davydova,Elena Tutubalina,Georgios Paliouras*

Main category: cs.CL

TL;DR: 本文概述了BioASQ第十二届挑战赛，包括两个已建立的任务和两个新任务，共吸引了37支参赛队伍的参与，提交了700多项成果。


<details>
  <summary>Details</summary>
Motivation: 促进大规模生物医学语义索引和问答领域的进步。

Method: 组织了包括两项已成熟任务(b和Synergy)及两项新任务(MultiCardioNER和BIONNE)的挑战赛。

Result: 共有37支队伍参与，提交了超过700次不同的任务结果，系统性能得到持续优化。

Conclusion: 参与系统表现显示出领域技术的持续进步，表明生物医学信息处理领域的前景广阔。

Abstract: This is an overview of the twelfth edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2024. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks b and Synergy, and two
new tasks: a) MultiCardioNER on the adaptation of clinical entity detection to
the cardiology domain in a multilingual setting, and b) BIONNE on nested NER in
Russian and English. In this edition of BioASQ, 37 competing teams participated
with more than 700 distinct submissions in total for the four different shared
tasks of the challenge. Similarly to previous editions, most of the
participating systems achieved competitive performance, suggesting the
continuous advancement of the state-of-the-art in the field.

</details>


### [119] [Overview of BioASQ 2025: The Thirteenth BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20554)
*Anastasios Nentidis,Georgios Katsimpras,Anastasia Krithara,Martin Krallinger,Miguel Rodríguez-Ortega,Eduard Rodriguez-López,Natalia Loukachevitch,Andrey Sakhovskiy,Elena Tutubalina,Dimitris Dimitriadis,Grigorios Tsoumakas,George Giannakoulas,Alexandra Bekiaridou,Athanasios Samaras,Giorgio Maria Di Nunzio,Nicola Ferro,Stefano Marchesin,Marco Martinelli,Gianmaria Silvello,Georgios Paliouras*

Main category: cs.CL

TL;DR: 本文概述了2025年CLEF会议上第十三届BioASQ挑战赛，包括六个任务，其中四个为全新任务，吸引了83支团队参与，总提交数超过了1000次。


<details>
  <summary>Details</summary>
Motivation: 推动生物医学领域语义索引与问答系统的大规模发展。

Method: 构建多项任务，包括多语言临床总结、命名实体链接、心脏病临床编码和肠脑交互信息提取，以测试系统能力。

Result: 参与团队表现竞争力强，显示了该领域技术的持续进步。

Conclusion: 挑战赛促进了领域技术的发展，并展示了多任务在提升性能方面的潜力。

Abstract: This is an overview of the thirteenth edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2025. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks, b and Synergy, and four
new tasks: a) Task MultiClinSum on multilingual clinical summarization. b) Task
BioNNE-L on nested named entity linking in Russian and English. c) Task
ELCardioCC on clinical coding in cardiology. d) Task GutBrainIE on gut-brain
interplay information extraction. In this edition of BioASQ, 83 competing teams
participated with more than 1000 distinct submissions in total for the six
different shared tasks of the challenge. Similar to previous editions, several
participating systems achieved competitive performance, indicating the
continuous advancement of the state-of-the-art in the field.

</details>


### [120] [Adaptive Federated Distillation for Multi-Domain Non-IID Textual Data](https://arxiv.org/abs/2508.20557)
*Jiahao Xiao,Jiangming Liu*

Main category: cs.CL

TL;DR: 本文提出了一个新的基准框架，旨在处理多域非独立同分布(Non-IID)场景下的隐私保护联邦蒸馏问题，并引入了一种新的自适应联邦蒸馏方法(AdaFD)，以应对多域挑战。


<details>
  <summary>Details</summary>
Motivation: 当前的联邦蒸馏研究多局限于输出端的多样性，忽略了自然语言处理中的关键输入端多样性，为此本文提出多域非IID场景的基准框架。

Method: 设计了包含多样化数据的基准框架，并提出了自适应联邦蒸馏（AdaFD）方法，适用于同质和异质设置下的多域非IID挑战。

Result: 实验表明，该方法能更好地捕捉本地客户端的多样性，与现有方法相比性能表现更优。

Conclusion: AdaFD框架成功解决了多域非IID场景下的挑战，为真实环境中的联邦学习框架性能评估提供了有力工具。

Abstract: The widespread success of pre-trained language models has established a new
training paradigm, where a global PLM is fine-tuned using task-specific data
from local clients. The local data are highly different from each other and can
not capture the global distribution of the whole data in real world. To address
the challenges of non-IID data in real environments, privacy-preserving
federated distillation has been proposed and highly investigated. However,
previous experimental non-IID scenarios are primarily identified with the label
(output) diversity, without considering the diversity of language domains
(input) that is crucial in natural language processing. In this paper, we
introduce a comprehensive set of multi-domain non-IID scenarios and propose a
unified benchmarking framework that includes diverse data. The benchmark can be
used to evaluate the federated learning framework in a real environment. To
this end, we propose an Adaptive Federated Distillation (AdaFD) framework
designed to address multi-domain non-IID challenges in both homogeneous and
heterogeneous settings. Experimental results demonstrate that our models
capture the diversity of local clients and achieve better performance compared
to the existing works. The code for this paper is available at:
https://github.com/jiahaoxiao1228/AdaFD.

</details>


### [121] [Leveraging Generative Models for Real-Time Query-Driven Text Summarization in Large-Scale Web Search](https://arxiv.org/abs/2508.20559)
*Zeyu Xiong,Yixuan Nan,Li Gao,Hengzhu Tang,Shuaiqiang Wang,Junfeng Wang,Dawei Yin*

Main category: cs.CL

TL;DR: 该论文探讨了在大规模网页搜索中生成基于查询的文本摘要的挑战，并提出了一种整合生成式模型以提升实时性和性能的新框架。


<details>
  <summary>Details</summary>
Motivation: 传统的提取式摘要模型存在信息丢失和对复杂搜索意图理解不足的问题。这激发了研究者开发更适合的生成式模型框架。

Method: 提出了一种包括大模型蒸馏、监督微调、直接偏好优化和前瞻解码的框架，从一个轻量化模型（0.1B参数）优化为一个领域专用的QDTS专家模型。

Result: 模型在多项行业相关指标上超过了生产基线，达到了新的最优表现，同时在部署效率上能够支持每秒约50,000查询，平均延迟为55毫秒，仅需334个NVIDIA L20 GPU。

Conclusion: 提出的生成式模型框架不仅在性能上超越传统方法，还在工业应用中展现了高效的部署能力，为基于查询的文本总结领域提供了新的解决方案。

Abstract: In the dynamic landscape of large-scale web search, Query-Driven Text
Summarization (QDTS) aims to generate concise and informative summaries from
textual documents based on a given query, which is essential for improving user
engagement and facilitating rapid decision-making. Traditional extractive
summarization models, based primarily on ranking candidate summary segments,
have been the dominant approach in industrial applications. However, these
approaches suffer from two key limitations: 1) The multi-stage pipeline often
introduces cumulative information loss and architectural bottlenecks due to its
weakest component; 2) Traditional models lack sufficient semantic understanding
of both user queries and documents, particularly when dealing with complex
search intents. In this study, we propose a novel framework to pioneer the
application of generative models to address real-time QDTS in industrial web
search. Our approach integrates large model distillation, supervised
fine-tuning, direct preference optimization, and lookahead decoding to
transform a lightweight model with only 0.1B parameters into a
domain-specialized QDTS expert. Evaluated on multiple industry-relevant
metrics, our model outperforms the production baseline and achieves a new state
of the art. Furthermore, it demonstrates excellent deployment efficiency,
requiring only 334 NVIDIA L20 GPUs to handle \textasciitilde50,000 queries per
second under 55~ms average latency per query.

</details>


### [122] [KCS: Diversify Multi-hop Question Generation with Knowledge Composition Sampling](https://arxiv.org/abs/2508.20567)
*Yangfan Wang,Jie Liu,Chen Tang,Lian Yan,Jingchi Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架——知识组合采样（KCS），通过在给定上下文中采样不同的知识组合来生成多跳问题，以解决多跳问答中的数据稀疏问题。


<details>
  <summary>Details</summary>
Motivation: 多跳问答面临数据稀疏以及模型学习虚假模式的挑战，因此需要通过生成多样化问题和结合关键信息的方式改进现有方法。

Method: 通过将知识组合建模为句子级条件预测任务，并采用概率对比损失预测下一个最相关的知识片段。此外，在推理阶段使用随机解码策略，平衡准确性与多样性。

Result: KCS框架将知识组合选择的总体准确率提高了3.9%，并且在HotpotQA和2WikiMultihopQA数据集上的数据增强任务中表现优异。

Conclusion: 通过知识组合采样（KCS）方法，可以有效提升生成多跳问题的多样性，同时改进问答模型的数据增强效果，具有较高的实用价值。

Abstract: Multi-hop question answering faces substantial challenges due to data
sparsity, which increases the likelihood of language models learning spurious
patterns. To address this issue, prior research has focused on diversifying
question generation through content planning and varied expression. However,
these approaches often emphasize generating simple questions and neglect the
integration of essential knowledge, such as relevant sentences within
documents. This paper introduces the Knowledge Composition Sampling (KCS), an
innovative framework designed to expand the diversity of generated multi-hop
questions by sampling varied knowledge compositions within a given context. KCS
models the knowledge composition selection as a sentence-level conditional
prediction task and utilizes a probabilistic contrastive loss to predict the
next most relevant piece of knowledge. During inference, we employ a stochastic
decoding strategy to effectively balance accuracy and diversity. Compared to
competitive baselines, our KCS improves the overall accuracy of knowledge
composition selection by 3.9%, and its application for data augmentation yields
improvements on HotpotQA and 2WikiMultihopQA datasets. Our code is available
at: https://github.com/yangfanww/kcs.

</details>


### [123] [A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language Models](https://arxiv.org/abs/2508.20583)
*Soham Petkar,Hari Aakash K,Anirudh Vempati,Akshit Sinha,Ponnurangam Kumarauguru,Chirag Agarwal*

Main category: cs.CL

TL;DR: GLMs当前的评估基准无法充分衡量多模态推理，因此提出了CLEGR基准以弥补这一空白，还发现当前GLMs在需要结构推理的任务中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 旨在改善GLMs的多模态推理能力，并揭示现有基准的不充分性，推动图结构与语言的多模态推理发展。

Method: 提出CLEGR基准测试，结合合成图生成和问题设计，评估GLMs在结构与文本语义联合推理中的能力。

Result: 发现基于语言模型的软提示基线与完整GNN骨干的GLMs表现相当，同时GLMs在结构推理任务上表现显著下降。

Conclusion: 目前GLMs在图推理方面存在局限性，新的基准为探索图结构与语言的明确多模态推理奠定了基础。

Abstract: Developments in Graph-Language Models (GLMs) aim to integrate the structural
reasoning capabilities of Graph Neural Networks (GNNs) with the semantic
understanding of Large Language Models (LLMs). However, we demonstrate that
current evaluation benchmarks for GLMs, which are primarily repurposed
node-level classification datasets, are insufficient to assess multimodal
reasoning. Our analysis reveals that strong performance on these benchmarks is
achievable using unimodal information alone, suggesting that they do not
necessitate graph-language integration. To address this evaluation gap, we
introduce the CLEGR(Compositional Language-Graph Reasoning) benchmark, designed
to evaluate multimodal reasoning at various complexity levels. Our benchmark
employs a synthetic graph generation pipeline paired with questions that
require joint reasoning over structure and textual semantics. We perform a
thorough evaluation of representative GLM architectures and find that
soft-prompted LLM baselines perform on par with GLMs that incorporate a full
GNN backbone. This result calls into question the architectural necessity of
incorporating graph structure into LLMs. We further show that GLMs exhibit
significant performance degradation in tasks that require structural reasoning.
These findings highlight limitations in the graph reasoning capabilities of
current GLMs and provide a foundation for advancing the community toward
explicit multimodal reasoning involving graph structure and language.

</details>


### [124] [Generative Annotation for ASR Named Entity Correction](https://arxiv.org/abs/2508.20700)
*Yuanchang Luo,Daimeng Wei,Shaojun Li,Hengchao Shang,Jiaxin Guo,Zongyao Li,Zhanglin Wu,Xiaoyu Chen,Zhiqiang Rao,Jinlong Yang,Hao Yang*

Main category: cs.CL

TL;DR: 提出了一种新的命名实体校正方法，通过利用语音声学特征提高实体标注和替换的准确性，适用于解决由于字形差异导致的错误。


<details>
  <summary>Details</summary>
Motivation: 现有语音识别系统难以正确转录领域特定的命名实体，特别是在形态差异显著的情况下，现有方法定位错误的能力有限。

Method: 利用语音声学特征获取候选命名实体，设计生成方法标注ASR转录中的实体错误，并替换为正确实体。

Result: 实验表明，该方法能显著提高命名实体的识别准确率，尤其在字形差异明显的情境中表现优异。

Conclusion: 新的命名实体校正方法改善了ASR系统在领域特定命名实体方面的表现，未来将开源测试集和训练数据。

Abstract: End-to-end automatic speech recognition systems often fail to transcribe
domain-specific named entities, causing catastrophic failures in downstream
tasks. Numerous fast and lightweight named entity correction (NEC) models have
been proposed in recent years. These models, mainly leveraging phonetic-level
edit distance algorithms, have shown impressive performances. However, when the
forms of the wrongly-transcribed words(s) and the ground-truth entity are
significantly different, these methods often fail to locate the wrongly
transcribed words in hypothesis, thus limiting their usage. We propose a novel
NEC method that utilizes speech sound features to retrieve candidate entities.
With speech sound features and candidate entities, we inovatively design a
generative method to annotate entity errors in ASR transcripts and replace the
text with correct entities. This method is effective in scenarios of word form
difference. We test our method using open-source and self-constructed test
sets. The results demonstrate that our NEC method can bring significant
improvement to entity accuracy. We will open source our self-constructed test
set and training data.

</details>


### [125] [Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label Hierarchical Learning](https://arxiv.org/abs/2508.20712)
*Nelson Filipe Costa,Leila Kosseim*

Main category: cs.CL

TL;DR: 这篇论文提出了首个用于隐式话语关系识别（IDRR）的多语言和多标签分类模型，并在多个测试集上取得了优异的表现。


<details>
  <summary>Details</summary>
Motivation: 隐式话语关系识别领域缺乏高效的多语言和多标签模型，其领域内需要先进的网络结构和方法来提升性能。

Method: 该论文提出了一种名为HArch的模型，利用层次化依赖预测PDTB 3.0框架中的意义层级分布，并比较了多个预训练编码器的性能，最终选择RoBERTa-HArch和XLM-RoBERTa-HArch分别用于英文和多语言任务。

Result: 实验结果表明，所提出的模型在多语言和单语言设置中均优于GPT-4o和Llama-4-Maverick等模型。同时，该模型在DiscoGeM 1.0和2.0语料库上取得了SOTA的表现。

Conclusion: 通过层次化学习方法并结合预训练模型的任务微调，能够显著提升隐式话语关系识别的性能，优于基于Few-shot提示学习的LLM方法。

Abstract: This paper introduces the first multi-lingual and multi-label classification
model for implicit discourse relation recognition (IDRR). Our model, HArch, is
evaluated on the recently released DiscoGeM 2.0 corpus and leverages
hierarchical dependencies between discourse senses to predict probability
distributions across all three sense levels in the PDTB 3.0 framework. We
compare several pre-trained encoder backbones and find that RoBERTa-HArch
achieves the best performance in English, while XLM-RoBERTa-HArch performs best
in the multi-lingual setting. In addition, we compare our fine-tuned models
against GPT-4o and Llama-4-Maverick using few-shot prompting across all
language configurations. Our results show that our fine-tuned models
consistently outperform these LLMs, highlighting the advantages of
task-specific fine-tuning over prompting in IDRR. Finally, we report SOTA
results on the DiscoGeM 1.0 corpus, further validating the effectiveness of our
hierarchical approach.

</details>


### [126] [Addressing Tokenization Inconsistency in Steganography and Watermarking Based on Large Language Models](https://arxiv.org/abs/2508.20718)
*Ruiyi Yan,Yugo Murawaki*

Main category: cs.CL

TL;DR: 本文研究了解决文本隐写术和数字水印中由于令牌化不一致（TI）问题引起的鲁棒性下降问题，提出了相应解决方案并证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 鉴于大语言模型提升了文本隐写质量，同时突显了数字水印作为保障的重要性，本研究针对隐写术和水印中的令牌化不一致（TI）问题展开探讨。

Method: 本文提出了两种定制化的TI消除方法：针对隐写术的逐步验证方法，以及针对水印的事后回滚方法。

Result: 实验结果表明，直接解决TI问题在隐写术中比传统消歧方法显著提升了流畅性、不可察觉性以及抗隐写分析能力；在水印领域提高了可检测性及对攻击的鲁棒性。

Conclusion: 解决令牌化不一致问题可以有效增强隐写术和数字水印的性能与鲁棒性。

Abstract: Large language models have significantly enhanced the capacities and
efficiency of text generation. On the one hand, they have improved the quality
of text-based steganography. On the other hand, they have also underscored the
importance of watermarking as a safeguard against malicious misuse. In this
study, we focus on tokenization inconsistency (TI) between Alice and Bob in
steganography and watermarking, where TI can undermine robustness. Our
investigation reveals that the problematic tokens responsible for TI exhibit
two key characteristics: infrequency and temporariness. Based on these
findings, we propose two tailored solutions for TI elimination: a stepwise
verification method for steganography and a post-hoc rollback method for
watermarking. Experiments show that (1) compared to traditional disambiguation
methods in steganography, directly addressing TI leads to improvements in
fluency, imperceptibility, and anti-steganalysis capacity; (2) for
watermarking, addressing TI enhances detectability and robustness against
attacks.

</details>


### [127] [rStar2-Agent: Agentic Reasoning Technical Report](https://arxiv.org/abs/2508.20722)
*Ning Shang,Yifei Liu,Yi Zhu,Li Lyna Zhang,Weijiang Xu,Xinyu Guan,Buze Zhang,Bingcheng Dong,Xudong Zhou,Bowen Zhang,Ying Xin,Ziming Miao,Scarlett Li,Fan Yang,Mao Yang*

Main category: cs.CL

TL;DR: 提出了一种名为rStar2-Agent的14B数学推理模型，通过代理强化学习实现了前沿性能，并支持复杂问题的自主探索和解决。


<details>
  <summary>Details</summary>
Motivation: 开发一个高效的模型来实现先进的认知行为，包括复杂问题解决的推理与验证能力。

Method: 提出了三项关键创新：高效的RL基础设施、GRPO-RoC代理RL算法以及高效的代理训练配方，结合预训练和强化学习步骤提升模型能力。

Result: 在AIME24和AIME25测试中分别实现了80.6%和69.8%的平均pass@1得分，超越了DeepSeek-R1，且能在科学推理等任务中表现优秀。

Conclusion: rStar2-Agent能够以较少的计算资源实现先进的认知能力，并具备良好的跨领域适应性。相关代码和训练配方已公开。

Abstract: We introduce rStar2-Agent, a 14B math reasoning model trained with agentic
reinforcement learning to achieve frontier-level performance. Beyond current
long CoT, the model demonstrates advanced cognitive behaviors, such as thinking
carefully before using Python coding tools and reflecting on code execution
feedback to autonomously explore, verify, and refine intermediate steps in
complex problem-solving. This capability is enabled through three key
innovations that makes agentic RL effective at scale: (i) an efficient RL
infrastructure with a reliable Python code environment that supports
high-throughput execution and mitigates the high rollout costs, enabling
training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic
RL algorithm with a Resample-on-Correct rollout strategy that addresses the
inherent environment noises from coding tools, allowing the model to reason
more effectively in a code environment; (iii) An efficient agent training
recipe that starts with non-reasoning SFT and progresses through multi-RL
stages, yielding advanced cognitive abilities with minimal compute cost. To
this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in
only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on
AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly
shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates
strong generalization to alignment, scientific reasoning, and agentic tool-use
tasks. Code and training recipes are available at
https://github.com/microsoft/rStar.

</details>


### [128] [Leveraging Semantic Triples for Private Document Generation with Local Differential Privacy Guarantees](https://arxiv.org/abs/2508.20736)
*Stephen Meisenbacher,Maulik Chevli,Florian Matthes*

Main category: cs.CL

TL;DR: 本文研究在自然语言处理中的差分隐私，通过引入DP-ST方法和语义三元组，在局部差分隐私保证下，生成具有隐私保护的文档。


<details>
  <summary>Details</summary>
Motivation: 当前方法难以在低隐私参数（ε值）下实现有效的文本隐私化，本文旨在解决这一问题。

Method: 提出DP-ST方法，基于语义三元组，在局部差分隐私范围内生成区分度高、连贯性好的文本，同时结合大规模语言模型进行后处理。

Result: 证明了通过分而治之的方式可以在更低的ε值下实现隐私与实用性的平衡，并生成连贯性较高的文本。

Conclusion: 本文方法强调文本连贯性对于平衡隐私化输出的重要性，为实现合理的隐私水平提供了新思路。

Abstract: Many works at the intersection of Differential Privacy (DP) in Natural
Language Processing aim to protect privacy by transforming texts under DP
guarantees. This can be performed in a variety of ways, from word perturbations
to full document rewriting, and most often under local DP. Here, an input text
must be made indistinguishable from any other potential text, within some bound
governed by the privacy parameter $\varepsilon$. Such a guarantee is quite
demanding, and recent works show that privatizing texts under local DP can only
be done reasonably under very high $\varepsilon$ values. Addressing this
challenge, we introduce DP-ST, which leverages semantic triples for
neighborhood-aware private document generation under local DP guarantees.
Through the evaluation of our method, we demonstrate the effectiveness of the
divide-and-conquer paradigm, particularly when limiting the DP notion (and
privacy guarantees) to that of a privatization neighborhood. When combined with
LLM post-processing, our method allows for coherent text generation even at
lower $\varepsilon$ values, while still balancing privacy and utility. These
findings highlight the importance of coherence in achieving balanced
privatization outputs at reasonable $\varepsilon$ levels.

</details>


### [129] [Specializing General-purpose LLM Embeddings for Implicit Hate Speech Detection across Datasets](https://arxiv.org/abs/2508.20750)
*Vassiliy Cheremetiev,Quang Long Ho Ngo,Chau Ying Kot,Alina Elena Baia,Andrea Cavallaro*

Main category: cs.CL

TL;DR: 作者研究了通过微调LLMs嵌入模型在检测隐性仇恨言论（IHS）中的效果，展示了与以往结果相比的显著提升。


<details>
  <summary>Details</summary>
Motivation: 隐性仇恨言论难以检测，因为它们通过隐晦的暗示、讽刺或编码术语传达偏见或仇恨，而没有显式的贬损性语言。

Method: 作者通过选用基于大型语言模型的通用嵌入模型（如Stella, Jasper, NV-Embed和E5），通过微调提升其对隐性仇恨言论检测的性能，并进行了实验验证。

Result: 在多种IHS数据集上的实验显示，微调后的模型在内部数据集上提升了最多1.10个百分点的F1-macro得分，在跨数据集中最高提升了20.35个百分点。

Conclusion: 微调现代的通用嵌入模型能够极大提升隐性仇恨言论检测的性能，实现了目前最好的结果。

Abstract: Implicit hate speech (IHS) is indirect language that conveys prejudice or
hatred through subtle cues, sarcasm or coded terminology. IHS is challenging to
detect as it does not include explicit derogatory or inflammatory words. To
address this challenge, task-specific pipelines can be complemented with
external knowledge or additional information such as context, emotions and
sentiment data. In this paper, we show that, by solely fine-tuning recent
general-purpose embedding models based on large language models (LLMs), such as
Stella, Jasper, NV-Embed and E5, we achieve state-of-the-art performance.
Experiments on multiple IHS datasets show up to 1.10 percentage points
improvements for in-dataset, and up to 20.35 percentage points improvements in
cross-dataset evaluation, in terms of F1-macro score.

</details>


### [130] [GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and Efficient Open-Ended Text Generation](https://arxiv.org/abs/2508.20757)
*Yuanhao Ding,Esteban Garces Arias,Meimingwei Li,Julian Rodemann,Matthias Aßenmacher,Danlu Chen,Gaojuan Fan,Christian Heumann,Chongsheng Zhang*

Main category: cs.CL

TL;DR: 提出了一种新的开放式文本生成解码方法GUARD，解决了文本连贯性与多样性之间的权衡问题，同时提升了生成速度。


<details>
  <summary>Details</summary>
Motivation: 传统对比搜索解码方法因超参数依赖和高计算成本限制了其实用性，需要一种自适应、平衡多样性和连贯性的解决方案。

Method: 开发了GUARD方法，采用“Glocal”框架结合全局和局部不确定性信号，加入基于token计数的简易惩罚机制以降低计算开销。

Result: 实验证明GUARD在文本多样性、连贯性和平衡性方面卓越，同时大幅提高了生成速度。

Conclusion: GUARD实现了开放式文本生成的新平衡，在多维度的文本质量评估中表现突出，为未来研究提供了新的思路和实用工具。

Abstract: Open-ended text generation faces a critical challenge: balancing coherence
with diversity in LLM outputs. While contrastive search-based decoding
strategies have emerged to address this trade-off, their practical utility is
often limited by hyperparameter dependence and high computational costs. We
introduce GUARD, a self-adaptive decoding method that effectively balances
these competing objectives through a novel "Glocal" uncertainty-driven
framework. GUARD combines global entropy estimates with local entropy
deviations to integrate both long-term and short-term uncertainty signals. We
demonstrate that our proposed global entropy formulation effectively mitigates
abrupt variations in uncertainty, such as sudden overconfidence or high entropy
spikes, and provides theoretical guarantees of unbiasedness and consistency. To
reduce computational overhead, we incorporate a simple yet effective
token-count-based penalty into GUARD. Experimental results demonstrate that
GUARD achieves a good balance between text diversity and coherence, while
exhibiting substantial improvements in generation speed. In a more nuanced
comparison study across different dimensions of text quality, both human and
LLM evaluators validated its remarkable performance. Our code is available at
https://github.com/YecanLee/GUARD.

</details>


### [131] [Feel the Difference? A Comparative Analysis of Emotional Arcs in Real and LLM-Generated CBT Sessions](https://arxiv.org/abs/2508.20764)
*Xiaoyi Wang,Jiwei Zhang,Guangtao Zhang,Honglei Guo*

Main category: cs.CL

TL;DR: 该研究分析了现实和LLM生成的认知行为疗法对话中的情感变化，发现合成对话在情感真实性方面有局限性，尤其是对客户角色。


<details>
  <summary>Details</summary>
Motivation: 目前在心理健康NLP中，广泛使用大语言模型(LLMs)生成的合成治疗对话，但人们尚未清楚这些对话能否准确捕捉真实治疗中的情感动态。

Method: 作者使用“语句情感动态框架”，分析现实和LLM生成的认知行为疗法对话的情感变化，包括情感光谱、唤醒度、支配性的细粒度变化，数据来源包括真实会话和CACTUS数据集的合成对话。

Result: 研究发现，合成对话虽然流畅且结构一致，但缺乏情感多样性、更少情感丰富的语言，并在反应和调节模式中展现出非真实的特征，特别是客户角色的情感弧度与真实会话差异较大。

Conclusion: 当前LLM生成的治疗数据在情感真实度上存在局限，强调在心理健康应用中的情感真实性的重要性，同时引入名为RealCBT的真实数据集，以支持后续研究。

Abstract: Synthetic therapy dialogues generated by large language models (LLMs) are
increasingly used in mental health NLP to simulate counseling scenarios, train
models, and supplement limited real-world data. However, it remains unclear
whether these synthetic conversations capture the nuanced emotional dynamics of
real therapy. In this work, we conduct the first comparative analysis of
emotional arcs between real and LLM-generated Cognitive Behavioral Therapy
dialogues. We adapt the Utterance Emotion Dynamics framework to analyze
fine-grained affective trajectories across valence, arousal, and dominance
dimensions. Our analysis spans both full dialogues and individual speaker roles
(counselor and client), using real sessions transcribed from public videos and
synthetic dialogues from the CACTUS dataset. We find that while synthetic
dialogues are fluent and structurally coherent, they diverge from real
conversations in key emotional properties: real sessions exhibit greater
emotional variability,more emotion-laden language, and more authentic patterns
of reactivity and regulation. Moreover, emotional arc similarity between real
and synthetic speakers is low, especially for clients. These findings
underscore the limitations of current LLM-generated therapy data and highlight
the importance of emotional fidelity in mental health applications. We
introduce RealCBT, a curated dataset of real CBT sessions, to support future
research in this space.

</details>


### [132] [Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection](https://arxiv.org/abs/2508.20766)
*Harethah Abu Shairah,Hasan Abed Al Kader Hammoud,George Turkiyyah,Bernard Ghanem*

Main category: cs.CL

TL;DR: 本文提出了一种名为ROSI的白盒方法，以通过永久性地引导模型的激活偏向拒绝调和子空间来增强语言模型的安全对齐性。


<details>
  <summary>Details</summary>
Motivation: 针对现有通过删除模型特定表达方向来绕过安全机制的方法，作者希望能积极利用模型的这些方向进行正向引导，增强其安全性能。

Method: 采用一种无需微调的简单Rank-One权重修改方法，将其应用于所有残差流写矩阵，从有害和无害指令对中计算安全方向并实施安全性注入。

Result: ROSI方法在提高安全拒绝率的同时保持模型在标准基准测试中的实用性，且可重新对‘未审查’的模型进行对齐，证明其作为一种高效末端安全程序的有效性。

Conclusion: ROSI作为一种有针对性、可解释的权重引导机制，为改进语言模型安全性提供了一种低成本且高效的解决方案，并可与其他更耗资源的微调范式互补。

Abstract: Safety alignment in Large Language Models (LLMs) often involves mediating
internal representations to refuse harmful requests. Recent research has
demonstrated that these safety mechanisms can be bypassed by ablating or
removing specific representational directions within the model. In this paper,
we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box
method that amplifies a model's safety alignment by permanently steering its
activations toward the refusal-mediating subspace. ROSI operates as a simple,
fine-tuning-free rank-one weight modification applied to all residual stream
write matrices. The required safety direction can be computed from a small set
of harmful and harmless instruction pairs. We show that ROSI consistently
increases safety refusal rates - as evaluated by Llama Guard 3 - while
preserving the utility of the model on standard benchmarks such as MMLU,
HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align
'uncensored' models by amplifying their own latent safety directions,
demonstrating its utility as an effective last-mile safety procedure. Our
results suggest that targeted, interpretable weight steering is a cheap and
potent mechanism to improve LLM safety, complementing more resource-intensive
fine-tuning paradigms.

</details>


### [133] [Signs of Struggle: Spotting Cognitive Distortions across Language and Register](https://arxiv.org/abs/2508.20771)
*Abhishek Kuber,Enrico Liscio,Ruixuan Zhang,Caroline Figueroa,Pradeep K. Murukannaiah*

Main category: cs.CL

TL;DR: 研究旨在利用自动化方法检测数字文本中的认知扭曲，以便实现早期心理健康干预。


<details>
  <summary>Details</summary>
Motivation: 随着青年心理健康问题的增加，研究者希望通过自动化技术检测早期心理困扰的迹象，并集中在认知扭曲的识别上，从而实现低成本、及时的心理干预。

Method: 研究首次对跨语言及跨写作风格的认知扭曲检测模型进行分析，重点研究了荷兰青少年的论坛帖子，并使用域适应方法来进行改进。

Result: 研究发现，语言和写作风格的变化显著影响模型性能，而域适应方法显现出极大潜力。

Conclusion: 通过针对于认知扭曲的跨文化检测研究，可以提升心理健康自动干预技术的发展，对跨语言模型的适应性优化至关重要。

Abstract: Rising mental health issues among youth have increased interest in automated
approaches for detecting early signs of psychological distress in digital text.
One key focus is the identification of cognitive distortions, irrational
thought patterns that have a role in aggravating mental distress. Early
detection of these distortions may enable timely, low-cost interventions. While
prior work has focused on English clinical data, we present the first in-depth
study of cross-lingual and cross-register generalization of cognitive
distortion detection, analyzing forum posts written by Dutch adolescents. Our
findings show that while changes in language and writing style can
significantly affect model performance, domain adaptation methods show the most
promise.

</details>


### [134] [Exploring Machine Learning and Language Models for Multimodal Depression Detection](https://arxiv.org/abs/2508.20805)
*Javier Si Zhao Hong,Timothy Zoe Delaya,Sherwyn Chan Yin Kit,Pai Chet Ng,Xiaoxiao Miao*

Main category: cs.CL

TL;DR: 本文主要探讨通过多模态特征（音频、视频、文本）和机器学习、深度学习模型进行抑郁检测。


<details>
  <summary>Details</summary>
Motivation: 为首次多模态人格感知抑郁检测挑战提供解决方案，并探讨不同模型在抑郁信号捕捉中的表现。

Method: 采用XGBoost、基于Transformer的架构及大型语言模型（LLMs），对音频、视频、文本特征进行多模态分析和比较。

Result: 揭示了每种模型在捕捉抑郁信号中的优势和局限性。

Conclusion: 为构建更高效的多模态心理健康预测策略提供了有价值的见解。

Abstract: This paper presents our approach to the first Multimodal Personality-Aware
Depression Detection Challenge, focusing on multimodal depression detection
using machine learning and deep learning models. We explore and compare the
performance of XGBoost, transformer-based architectures, and large language
models (LLMs) on audio, video, and text features. Our results highlight the
strengths and limitations of each type of model in capturing depression-related
signals across modalities, offering insights into effective multimodal
representation strategies for mental health prediction.

</details>


### [135] [GDLLM: A Global Distance-aware Modeling Approach Based on Large Language Models for Event Temporal Relation Extraction](https://arxiv.org/abs/2508.20828)
*Jie Zhao,Wanting Ning,Yuxiao Fei,Yubo Feng,Lishuang Li*

Main category: cs.CL

TL;DR: 将大语言模型(LLMs)引入事件时间关系提取(ETRE)，结合图注意机制捕获长距离和短距离特征，提高少数类关系的识别能力。实验结果达到最新水平(SOTA)。


<details>
  <summary>Details</summary>
Motivation: 先前研究未能有效应对少数类关系在不平衡数据中的问题，同时手动设计的提示可能对LLMs干扰较大。

Method: 提出GDLLM模型，结合图注意网络(GAT)以捕获长距离依赖特征，并设计软推理的时间特征学习模式增强短距离关系识别。

Result: 在TB-Dense和MATRES两个数据集上实验获得SOTA性能。

Conclusion: 通过捕捉全局特征显著提升了少数类关系的处理能力，总体学习性能也有提升。

Abstract: In Natural Language Processing(NLP), Event Temporal Relation Extraction
(ETRE) is to recognize the temporal relations of two events. Prior studies have
noted the importance of language models for ETRE. However, the restricted
pre-trained knowledge of Small Language Models(SLMs) limits their capability to
handle minority class relations in imbalanced classification datasets. For
Large Language Models(LLMs), researchers adopt manually designed prompts or
instructions, which may introduce extra noise, leading to interference with the
model's judgment of the long-distance dependencies between events. To address
these issues, we propose GDLLM, a Global Distance-aware modeling approach based
on LLMs. We first present a distance-aware graph structure utilizing Graph
Attention Network(GAT) to assist the LLMs in capturing long-distance dependency
features. Additionally, we design a temporal feature learning paradigm based on
soft inference to augment the identification of relations with a short-distance
proximity band, which supplements the probabilistic information generated by
LLMs into the multi-head attention mechanism. Since the global feature can be
captured effectively, our framework substantially enhances the performance of
minority relation classes and improves the overall learning ability.
Experiments on two publicly available datasets, TB-Dense and MATRES,
demonstrate that our approach achieves state-of-the-art (SOTA) performance.

</details>


### [136] [MSRS: Evaluating Multi-Source Retrieval-Augmented Generation](https://arxiv.org/abs/2508.20867)
*Rohan Phanse,Yijie Zhou,Kejian Shi,Wencai Zhang,Yixin Liu,Yilun Zhao,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出了一种评价检索增强生成系统（RAG）在多源信息整合与生成长文本回答中的能力的框架，并构建了两个新的基准测试任务 MSRS-Story 和 MSRS-Meet。研究表明，生成质量受限于检索效果，多源信息合成在理想检索情况下仍具挑战性，但推理模型显著优于传统大语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前的RAG系统通常在单一信息源或短形式回答的环境中评估，而现实中的许多应用需要整合和总结分散在多个信息源中的信息。本文旨在推动对多源信息整合、生成长形式回答能力的研究与评价。

Method: 作者提出了一个可扩展的框架，用于构建评估任务，该框架挑战RAG系统整合来自不同来源的信息并生成长文本回答。基于此框架，本文提出了两个基准任务 MSRS-Story（叙述整合任务）和 MSRS-Meet（总结任务），二者均涉及从大规模信息集合中检索信息。

Result: 实验显示，生成质量高度依赖于检索的效率和效果，而不同任务对检索效果的需求程度各异。即便在理想检索（oracle）场景下，多源信息合成仍然很具挑战。此外，推理模型在该任务中的表现明显超越了传统的大语言模型。

Conclusion: 本文为多源信息整合及长文本生成任务的研究和发展提供了新的框架和基准测试，并指出提高检索效果及改进推理模型是未来研究的关键方向。

Abstract: Retrieval-augmented systems are typically evaluated in settings where
information required to answer the query can be found within a single source or
the answer is short-form or factoid-based. However, many real-world
applications demand the ability to integrate and summarize information
scattered across multiple sources, where no single source is sufficient to
respond to the user's question. In such settings, the retrieval component of a
RAG pipeline must recognize a variety of relevance signals, and the generation
component must connect and synthesize information across multiple sources. We
present a scalable framework for constructing evaluation benchmarks that
challenge RAG systems to integrate information across distinct sources and
generate long-form responses. Using our framework, we build two new benchmarks
on Multi-Source Retrieval and Synthesis: MSRS-Story and MSRS-Meet, representing
narrative synthesis and summarization tasks, respectively, that require
retrieval from large collections. Our extensive experiments with various RAG
pipelines -- including sparse and dense retrievers combined with frontier LLMs
-- reveal that generation quality is highly dependent on retrieval
effectiveness, which varies greatly by task. While multi-source synthesis
proves challenging even in an oracle retrieval setting, we find that reasoning
models significantly outperform standard LLMs at this distinct step.

</details>


### [137] [The Uneven Impact of Post-Training Quantization in Machine Translation](https://arxiv.org/abs/2508.20893)
*Benjamin Marie,Atsushi Fujita*

Main category: cs.CL

TL;DR: 本研究探讨了量化对多语种任务的影响，尤其是低资源语言和不同位数设置下的影响。


<details>
  <summary>Details</summary>
Motivation: 研究多语种任务中量化对翻译质量的影响，以在硬件资源受限情况下高效部署大型语言模型。

Method: 通过五个大型语言模型，在55种语言上进行量化实验，包括4种量化技术（AWQ, BitsAndBytes, GGUF, AutoRound）的对比分析，以及解码超参数和校准语言的交互作用分析。

Result: 研究显示：4-bit量化对高资源语言和大模型的翻译质量影响较小，但在低资源语言和2-bit条件下影响显著；GGUF变体在低位条件下性能最为稳定，语言匹配校准对低位量化有益。

Conclusion: 为在量化约束下部署多语言翻译模型提供了可行性见解，尤其适用于低资源语言的场景。

Abstract: Quantization is essential for deploying large language models (LLMs) on
resource-constrained hardware, but its implications for multilingual tasks
remain underexplored. We conduct the first large-scale evaluation of
post-training quantization (PTQ) on machine translation across 55 languages
using five LLMs ranging from 1.7B to 70B parameters. Our analysis reveals that
while 4-bit quantization often preserves translation quality for high-resource
languages and large models, significant degradation occurs for low-resource and
typologically diverse languages, particularly in 2-bit settings. We compare
four quantization techniques (AWQ, BitsAndBytes, GGUF, and AutoRound), showing
that algorithm choice and model size jointly determine robustness. GGUF
variants provide the most consistent performance, even at 2-bit precision.
Additionally, we quantify the interactions between quantization, decoding
hyperparameters, and calibration languages, finding that language-matched
calibration offers benefits primarily in low-bit scenarios. Our findings offer
actionable insights for deploying multilingual LLMs for machine translation
under quantization constraints, especially in low-resource settings.

</details>


### [138] [SageLM: A Multi-aspect and Explainable Large Language Model for Speech Judgement](https://arxiv.org/abs/2508.20916)
*Yuan Ge,Junxiang Zhang,Xiaoqian Liu,Bei Li,Xiangnan Ma,Chenglong Wang,Kaiyang Ye,Yangfan Du,Linfeng Zhang,Yuxin Huang,Tong Xiao,Zhengtao Yu,JingBo Zhu*

Main category: cs.CL

TL;DR: 提出了SageLM，一种全面、可解释的语音LLM评估工具，与人类评估结果的匹配率为82.79%。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法忽略语音特征或者缺乏解释性，难以有效评估端到端语音对话系统。

Method: 结合语义和声学维度综合评估，使用基于推理的监督方法，并引入一个合成偏好数据集SpeechFeedback及双阶段训练策略。

Result: SageLM在人类评估匹配率上表现卓越，比传统方法和基于SLM的方法分别高出7.42%和26.20%。

Conclusion: SageLM有助于解决语音-语音模型评估中的关键挑战，体现了更高的准确性和解释性。

Abstract: Speech-to-Speech (S2S) Large Language Models (LLMs) are foundational to
natural human-computer interaction, enabling end-to-end spoken dialogue
systems. However, evaluating these models remains a fundamental challenge. We
propose \texttt{SageLM}, an end-to-end, multi-aspect, and explainable speech
LLM for comprehensive S2S LLMs evaluation. First, unlike cascaded approaches
that disregard acoustic features, SageLM jointly assesses both semantic and
acoustic dimensions. Second, it leverages rationale-based supervision to
enhance explainability and guide model learning, achieving superior alignment
with evaluation outcomes compared to rule-based reinforcement learning methods.
Third, we introduce \textit{SpeechFeedback}, a synthetic preference dataset,
and employ a two-stage training paradigm to mitigate the scarcity of speech
preference data. Trained on both semantic and acoustic dimensions, SageLM
achieves an 82.79\% agreement rate with human evaluators, outperforming
cascaded and SLM-based baselines by at least 7.42\% and 26.20\%, respectively.

</details>


### [139] [How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on $τ$-bench](https://arxiv.org/abs/2508.20931)
*Venkatesh Mishra,Amir Saeidi,Satyam Raj,Mutsumi Nakamura,Jayanth Srinivasa,Gaowen Liu,Ali Payani,Chitta Baral*

Main category: cs.CL

TL;DR: 提出了一个新的框架IRMA，通过重新表述用户输入，显著提升了大语言模型在复杂交互环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在多轮对话和动态环境中工具使用的不足，以改善其一致性推理、遵守特定领域政策以及准确提取信息的能力。

Method: 开发了IRMA（Input-Reformulation Multi-Agent）框架，自动重新表述用户查询并结合相关领域规则和工具建议，以辅助决策，实验并比较其与其他方法的性能表现。

Result: IRMA在实验中，整体pass^5得分比ReAct高16.1%、比Function Calling高12.7%、比Self-Reflection高19.1%。

Conclusion: IRMA框架在动态环境中展现出明显的可靠性和一致性优势，优于其他现有方法。

Abstract: Recent advances in reasoning and planning capabilities of large language
models (LLMs) have enabled their potential as autonomous agents capable of tool
use in dynamic environments. However, in multi-turn conversational environments
like $\tau$-bench, these agents often struggle with consistent reasoning,
adherence to domain-specific policies, and extracting correct information over
a long horizon of tool-calls and conversation. To capture and mitigate these
failures, we conduct a comprehensive manual analysis of the common errors
occurring in the conversation trajectories. We then experiment with
reformulations of inputs to the tool-calling agent for improvement in agent
decision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA)
framework, which automatically reformulates user queries augmented with
relevant domain rules and tool suggestions for the tool-calling agent to focus
on. The results show that IRMA significantly outperforms ReAct, Function
Calling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in
overall pass^5 scores. These findings highlight the superior reliability and
consistency of IRMA compared to other methods in dynamic environments.

</details>


### [140] [STARE at the Structure: Steering ICL Exemplar Selection with Structural Alignment](https://arxiv.org/abs/2508.20944)
*Jiaqian Li,Qisheng Hu,Jing Li,Wenya Wang*

Main category: cs.CL

TL;DR: 本文提出了一种两阶段样例选择策略，通过fine-tuning一个基于BERT的retriever并加入一个插件模块，显著提升了LLMs在结构化预测任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 在上下文学习中，选择高质量的示例对于任务执行效果至关重要，但现有的方法在结构化预测任务中忽视了结构对齐性，导致性能欠佳。

Method: 提出一种两阶段的样例选择策略：先使用含有结构感知监督信号的BERT检索器选择语义相关且结构对齐的样例，再通过一个增强语法信息的插件模块对检索效果进行优化。

Result: 实验表明，所提方法在多个语义解析任务的四个基准上，结合多种最新LLM推理模型均优于现有基线方法。

Conclusion: 融合结构感知及语法增强的方法能够提升示例选择的质量，从而提升LLMs在语义解析等结构化任务中的整体表现。

Abstract: In-Context Learning (ICL) has become a powerful paradigm that enables LLMs to
perform a wide range of tasks without task-specific fine-tuning. However, the
effectiveness of ICL heavily depends on the quality of exemplar selection. In
particular, for structured prediction tasks such as semantic parsing, existing
ICL selection strategies often overlook structural alignment, leading to
suboptimal performance and poor generalization. To address this issue, we
propose a novel two-stage exemplar selection strategy that achieves a strong
balance between efficiency, generalizability, and performance. First, we
fine-tune a BERT-based retriever using structure-aware supervision, guiding it
to select exemplars that are both semantically relevant and structurally
aligned. Then, we enhance the retriever with a plug-in module, which amplifies
syntactically meaningful information in the hidden representations. This
plug-in is model-agnostic, requires minimal overhead, and can be seamlessly
integrated into existing pipelines. Experiments on four benchmarks spanning
three semantic parsing tasks demonstrate that our method consistently
outperforms existing baselines with multiple recent LLMs as inference-time
models.

</details>


### [141] [ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents](https://arxiv.org/abs/2508.20973)
*Tianjian Liu,Fanqi Wan,Jiajian Guo,Xiaojun Quan*

Main category: cs.CL

TL;DR: 提出了一个名为ProactiveEval的统一框架，用于评估大型语言模型（LLMs）的前瞻性对话能力，研究发现DeepSeek-R1和Claude-3.7-Sonnet在相关任务中表现优秀。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在领域特定或任务导向的场景中，评估方式碎片化，无法全面探索模型的前瞻性对话能力。

Method: 设计了ProactiveEval框架，将前瞻性对话分解为目标规划和对话引导两个部分，并基于此框架建立多个评价指标和自动化生成评估数据。

Result: 基于ProactiveEval框架开发了涵盖6个领域的328个评估环境，实验表明DeepSeek-R1和Claude-3.7-Sonnet分别在目标规划和对话引导任务中表现突出。

Conclusion: 探讨了推理能力对前瞻性行为的影响，并提出其对未来模型开发的意义。

Abstract: Proactive dialogue has emerged as a critical and challenging research problem
in advancing large language models (LLMs). Existing works predominantly focus
on domain-specific or task-oriented scenarios, which leads to fragmented
evaluations and limits the comprehensive exploration of models' proactive
conversation abilities. In this work, we propose ProactiveEval, a unified
framework designed for evaluating proactive dialogue capabilities of LLMs. This
framework decomposes proactive dialogue into target planning and dialogue
guidance, establishing evaluation metrics across various domains. Moreover, it
also enables the automatic generation of diverse and challenging evaluation
data. Based on the proposed framework, we develop 328 evaluation environments
spanning 6 distinct domains. Through experiments with 22 different types of
LLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional
performance on target planning and dialogue guidance tasks, respectively.
Finally, we investigate how reasoning capabilities influence proactive
behaviors and discuss their implications for future model development.

</details>


### [142] [Lethe: Purifying Backdoored Large Language Models with Knowledge Dilution](https://arxiv.org/abs/2508.21004)
*Chen Chen,Yuchen Sun,Jiaxin Gao,Xueluan Gong,Qian Wang,Ziyao Wang,Yongsen Zheng,Kwok-Yan Lam*

Main category: cs.CL

TL;DR: 提出LETHE方法，通过内部知识稀释与外部干预机制，有效清除LLM的后门行为，并在多任务与多模型上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有后门防御方法在应对复杂场景（如基于模型编辑、多触发器及无触发器攻击）时不足，提出LETHE为解决此局限性。

Method: LETHE通过两种机制防御后门：内部机制利用轻量级数据集训练干净模型并与被攻击模型合并以稀释后门影响；外部机制通过在提示中融入无害语义信息分散模型注意力。

Result: LETHE在五种主流LLM与分类、生成任务中表现出色，相较八种最新防御基线，其将高级后门攻击成功率降低至2%以下，同时保持模型效用，并具备成本效益与强鲁棒性。

Conclusion: LETHE在清除后门行为同时保持模型实用性方面具有显著优势，是高效且可靠的解决方案。

Abstract: Large language models (LLMs) have seen significant advancements, achieving
superior performance in various Natural Language Processing (NLP) tasks.
However, they remain vulnerable to backdoor attacks, where models behave
normally for standard queries but generate harmful responses or unintended
output when specific triggers are activated. Existing backdoor defenses either
lack comprehensiveness, focusing on narrow trigger settings, detection-only
mechanisms, and limited domains, or fail to withstand advanced scenarios like
model-editing-based, multi-trigger, and triggerless attacks. In this paper, we
present LETHE, a novel method to eliminate backdoor behaviors from LLMs through
knowledge dilution using both internal and external mechanisms. Internally,
LETHE leverages a lightweight dataset to train a clean model, which is then
merged with the backdoored model to neutralize malicious behaviors by diluting
the backdoor impact within the model's parametric memory. Externally, LETHE
incorporates benign and semantically relevant evidence into the prompt to
distract LLM's attention from backdoor features. Experimental results on
classification and generation domains across 5 widely used LLMs demonstrate
that LETHE outperforms 8 state-of-the-art defense baselines against 8 backdoor
attacks. LETHE reduces the attack success rate of advanced backdoor attacks by
up to 98% while maintaining model utility. Furthermore, LETHE has proven to be
cost-efficient and robust against adaptive backdoor attacks.

</details>


### [143] [An Agile Method for Implementing Retrieval Augmented Generation Tools in Industrial SMEs](https://arxiv.org/abs/2508.21024)
*Mathieu Bourdin,Anas Neumann,Thomas Paviot,Robert Pellerin,Samir Lamouri*

Main category: cs.CL

TL;DR: 本文提出了一种名为EASI-RAG的新方法，用于在中小企业中快速部署检索增强生成（RAG）系统，并通过真实案例验证了其实施效果显著。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）存在局限性，中小企业缺乏资源和相关技术支持，难以应用RAG系统。本文旨在解决这些部署问题。

Method: 提出了EASI-RAG方法，基于方法工程原则构建，包含清晰的角色分工、活动流程和技术手段。通过环境测试实验室的案例进行验证。

Result: 实验室的部署实例表明，EASI-RAG方法能够在一个月内由无经验团队实施，并在用户反馈下逐步优化，实现了快速部署、高用户采用率、准确的答案生成及基础数据的可靠性提升。

Conclusion: EASI-RAG方法证明了RAG系统在工业中小企业中的可行性和高潜力，并为未来推广到更广泛的场景及整合微调模型提供了研究方向。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful solution to
mitigate the limitations of Large Language Models (LLMs), such as
hallucinations and outdated knowledge. However, deploying RAG-based tools in
Small and Medium Enterprises (SMEs) remains a challenge due to their limited
resources and lack of expertise in natural language processing (NLP). This
paper introduces EASI-RAG, Enterprise Application Support for Industrial RAG, a
structured, agile method designed to facilitate the deployment of RAG systems
in industrial SME contexts. EASI-RAG is based on method engineering principles
and comprises well-defined roles, activities, and techniques. The method was
validated through a real-world case study in an environmental testing
laboratory, where a RAG tool was implemented to answer operators queries using
data extracted from operational procedures. The system was deployed in under a
month by a team with no prior RAG experience and was later iteratively improved
based on user feedback. Results demonstrate that EASI-RAG supports fast
implementation, high user adoption, delivers accurate answers, and enhances the
reliability of underlying data. This work highlights the potential of RAG
deployment in industrial SMEs. Future works include the need for generalization
across diverse use cases and further integration with fine-tuned models.

</details>


### [144] [Re-Representation in Sentential Relation Extraction with Sequence Routing Algorithm](https://arxiv.org/abs/2508.21049)
*Ramazan Ali Bahrami,Ramin Yahyapour*

Main category: cs.CL

TL;DR: 本研究提出了一种基于胶囊动态路由的句子关系抽取方法，并在多个基准数据集上展现了优越性能，同时也分析了其在特定数据集上表现不佳的原因，如标签噪音和重新表示能力的关联。


<details>
  <summary>Details</summary>
Motivation: 探索如何提高句子关系抽取任务的性能，特别是解决现有方法在特定数据集上的不足，并解释新方法优越性能的背后原因。

Method: 提出基于胶囊网络的动态路由机制来进行句子关系抽取，并对多个著名数据集（如Tacred、Retacred等）进行实验验证，同时引入重新表示这一神经科学概念进行性能解释。

Result: 提出的方法在多个基准数据集上超过了现有技术，并通过实验发现标签噪声和重新表示能力是影响某些大规模数据集（如Wikidata）上表现的关键因素。

Conclusion: 该研究揭示了重表示能力在关系抽取任务中的重要性，并提出在噪声标签数据集和重表示能力上的改进方向，从而为后续研究提供了指导。

Abstract: Sentential relation extraction (RE) is an important task in natural language
processing (NLP). In this paper we propose to do sentential RE with dynamic
routing in capsules. We first show that the proposed approach outperform state
of the art on common sentential relation extraction datasets Tacred, Tacredrev,
Retacred, and Conll04. We then investigate potential reasons for its good
performance on the mentioned datasets, and yet low performance on another
similar, yet larger sentential RE dataset, Wikidata. As such, we identify noise
in Wikidata labels as one of the reasons that can hinder performance.
Additionally, we show associativity of better performance with better
re-representation, a term from neuroscience referred to change of
representation in human brain to improve the match at comparison time. As
example, in the given analogous terms King:Queen::Man:Woman, at comparison
time, and as a result of re-representation, the similarity between related head
terms (King,Man), and tail terms (Queen,Woman) increases. As such, our
observation show that our proposed model can do re-representation better than
the vanilla model compared with. To that end, beside noise in the labels of the
distantly supervised RE datasets, we propose re-representation as a challenge
in sentential RE.

</details>


### [145] [Enabling Equitable Access to Trustworthy Financial Reasoning](https://arxiv.org/abs/2508.21051)
*William Jurayj,Nils Holzenberger,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 本文提出了一种结合大语言模型（LLMs）与符号求解器的税务申报解决方法，并证明其在准确性与成本上的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的税务申报过程复杂且容易出错，且大语言模型在高精度任务中表现不足，因此需要一种结合符号方法的解决方案来提高准确性和可审计性。

Method: 将LLMs与符号求解器结合，以处理复杂税务规则及数值计算；引入基于非正式逻辑翻译和智能检索的案例表示方法，并在SARA数据集上评估其性能。

Result: 提出的系统显著提高了任务表现，降低了为避免错误而产生的运营成本，优于日常实际成本。

Conclusion: 神经符号架构在提升税务申报领域的公平性和可靠性方面具有潜在的经济可行性和巨大前景。

Abstract: According to the United States Internal Revenue Service, ''the average
American spends $\$270$ and 13 hours filing their taxes''. Even beyond the
U.S., tax filing requires complex reasoning, combining application of
overlapping rules with numerical calculations. Because errors can incur costly
penalties, any automated system must deliver high accuracy and auditability,
making modern large language models (LLMs) poorly suited for this task. We
propose an approach that integrates LLMs with a symbolic solver to calculate
tax obligations. We evaluate variants of this system on the challenging
StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for
estimating the cost of deploying such a system based on real-world penalties
for tax errors. We further show how combining up-front translation of
plain-text rules into formal logic programs, combined with intelligently
retrieved exemplars for formal case representations, can dramatically improve
performance on this task and reduce costs to well below real-world averages.
Our results demonstrate the promise and economic feasibility of neuro-symbolic
architectures for increasing equitable access to reliable tax assistance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [146] [ArgRAG: Explainable Retrieval Augmented Generation using Quantitative Bipolar Argumentation](https://arxiv.org/abs/2508.20131)
*Yuqicheng Zhu,Nico Potyka,Daniel Hernández,Yuan He,Zifeng Ding,Bo Xiong,Dongzhuoran Zhou,Evgeny Kharlamov,Steffen Staab*

Main category: cs.AI

TL;DR: 提出了一种名为ArgRAG的方法，通过使用定量双极辩论框架（QBAF），提高了检索增强生成（RAG）的透明性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在高风险领域受到噪声或矛盾证据的影响，并缺乏清晰的决策过程，亟需解决其局限性。

Method: 引入QBAF框架，通过检索文档构建QBAF，并在渐进语义下执行确定性的推理，替代黑箱推理。

Result: 在PubHealth和RAGuard两个事实验证基准上，ArgRAG不仅取得了较高的准确性，还在透明性上显著提升。

Conclusion: ArgRAG在透明性、可解释性和争议性方面明显优于传统RAG，适用于高风险领域的应用。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models by
incorporating external knowledge, yet suffers from critical limitations in
high-stakes domains -- namely, sensitivity to noisy or contradictory evidence
and opaque, stochastic decision-making. We propose ArgRAG, an explainable, and
contestable alternative that replaces black-box reasoning with structured
inference using a Quantitative Bipolar Argumentation Framework (QBAF). ArgRAG
constructs a QBAF from retrieved documents and performs deterministic reasoning
under gradual semantics. This allows faithfully explaining and contesting
decisions. Evaluated on two fact verification benchmarks, PubHealth and
RAGuard, ArgRAG achieves strong accuracy while significantly improving
transparency.

</details>


### [147] [QAgent: An LLM-based Multi-Agent System for Autonomous OpenQASM programming](https://arxiv.org/abs/2508.20134)
*Zhenxiao Fu,Fan Chen,Lei Jiang*

Main category: cs.AI

TL;DR: 本文提出了QAgent，一个利用大型语言模型（LLM）的多智能体系统，用于自动化OpenQASM编程。


<details>
  <summary>Details</summary>
Motivation: 现有NISQ设备在解决经典无法处理的问题上显示了潜在优势，但非专家面临编程复杂性问题。

Method: 通过整合任务规划、上下文学习、增强生成（RAG）、预定义工具以及链式推理（CoT），设计了一种多智能体系统QAgent，用以改进编译效率和功能正确性。

Result: 实验表明，与以往基于静态LLM的方法相比，QAgent在多种规模LLM测试中提高了QASM代码生成的准确性达71.6%。

Conclusion: 该系统旨在弥合量子编程领域的技术差距，推动量子计算的普及与实际应用。

Abstract: Noisy Intermediate-Scale Quantum (NISQ) devices have begun to exhibit early
quantum advantages on classically intractable problems, spanning physics
simulations to Gaussian boson sampling. Yet, realizing these benefits remains
challenging for non-experts, primarily due to the complexities of programming
in Open Quantum Assembly Language (OpenQASM). Although Large Language Model
(LLM)-based agents have shown promise in automating classical programming
workflows, their quantum counterparts have largely been restricted to
specialized tasks such as quantum chemistry or error correction. In this paper,
we present QAgent, an LLM-powered multi-agent system that fully automates
OpenQASM programming. By integrating task planning, in-context few-shot
learning, retrieval-augmented generation (RAG) for long-term context,
predefined generation tools, and chain-of-thought (CoT) reasoning, the agents
systematically improve both compilation and functional correctness. Our
evaluations demonstrate substantial improvements: across multiple LLMs of
varying sizes, QAgent enhances the accuracy of QASM code generation by 71.6\%
compared to previous static LLM-based approaches. We envision this multi-agent
system as a key enabler for democratizing quantum programming, bridging
expertise gaps, and accelerating the practical adoption of quantum computing.

</details>


### [148] [Array-Based Monte Carlo Tree Search](https://arxiv.org/abs/2508.20140)
*James Ragan,Fred Y. Hadaegh,Soon-Jo Chung*

Main category: cs.AI

TL;DR: 采用基于数组的实现改进经典UCT算法，提升蒙特卡罗树搜索性能。


<details>
  <summary>Details</summary>
Motivation: 在同一时间内完成更多模拟以提升搜索性能。

Method: 提出基于数组的UCT算法实现，消除对分支预测的依赖。

Result: 在数值模拟中实现了搜索深度提升到2.8倍的性能改进。

Conclusion: 改进了UCT算法的性能，为决策问题求解提供更快的解决方案。

Abstract: Monte Carlo Tree Search is a popular method for solving decision making
problems. Faster implementations allow for more simulations within the same
wall clock time, directly improving search performance. To this end, we present
an alternative array-based implementation of the classic Upper Confidence
bounds applied to Trees algorithm. Our method preserves the logic of the
original algorithm, but eliminates the need for branch prediction, enabling
faster performance on pipelined processors, and up to a factor of 2.8 times
better scaling with search depth in our numerical simulations.

</details>


### [149] [The Anatomy of a Personal Health Agent](https://arxiv.org/abs/2508.20148)
*A. Ali Heydari,Ken Gu,Vidya Srinivas,Hong Yu,Zhihan Zhang,Yuwei Zhang,Akshay Paruchuri,Qian He,Hamid Palangi,Nova Hammerquist,Ahmed A. Metwally,Brent Winslow,Yubin Kim,Kumar Ayush,Yuzhe Yang,Girish Narayanswamy,Maxwell A. Xu,Jake Garrison,Amy Aremnto Lee,Jenny Vafeiadou,Ben Graef,Isaac R. Galatzer-Levy,Erik Schenck,Andrew Barakat,Javier Perez,Jacqueline Shreibati,John Hernandez,Anthony Z. Faranesh,Javier L. Prieto,Connor Heneghan,Yun Liu,Jiening Zhan,Mark Malhotra,Shwetak Patel,Tim Althoff,Xin Liu,Daniel McDuff,Xuhai "Orson" Xu*

Main category: cs.AI

TL;DR: 本文提出了一种名为Personal Health Agent (PHA)的多代理系统框架，能够分析用户的多模态健康数据并提供个性化健康建议，完成十项基准任务的系统性评估，奠定了健康助手研究的基础。


<details>
  <summary>Details</summary>
Motivation: 目前健康领域的人工智能技术常聚焦于临床应用，针对日常非临床场景的个性化健康管家的研究较少，亟需填补这一空白。

Method: 研究通过分析用户搜索和论坛查询，结合用户与健康专家的设计调研，确定三大健康需求类别，并据此开发了三个子代理：用于解析数据的科学代理、生成个性化建议的领域专家代理，以及基于心理策略指导用户的健康教练代理。这些代理被整合到多代理系统PHA中，进行自动化和人工评估。

Result: 系统针对10项基准任务进行了全面评估，积累了超过7,000条注释和1,100小时的专家与用户验证数据，展示了其在个性化健康指导中的潜力。

Conclusion: PHA系统为面向所有人的个人健康助手提供了强有力的研究基础，展示了个性化健康服务在非临床场景的实际可行性。

Abstract: Health is a fundamental pillar of human wellness, and the rapid advancements
in large language models (LLMs) have driven the development of a new generation
of health agents. However, the application of health agents to fulfill the
diverse needs of individuals in daily non-clinical settings is underexplored.
In this work, we aim to build a comprehensive personal health agent that is
able to reason about multimodal data from everyday consumer wellness devices
and common personal health records, and provide personalized health
recommendations. To understand end-users' needs when interacting with such an
assistant, we conducted an in-depth analysis of web search and health forum
queries, alongside qualitative insights from users and health experts gathered
through a user-centered design process. Based on these findings, we identified
three major categories of consumer health needs, each of which is supported by
a specialist sub-agent: (1) a data science agent that analyzes personal
time-series wearable and health record data, (2) a health domain expert agent
that integrates users' health and contextual data to generate accurate,
personalized insights, and (3) a health coach agent that synthesizes data
insights, guiding users using a specified psychological strategy and tracking
users' progress. Furthermore, we propose and develop the Personal Health Agent
(PHA), a multi-agent framework that enables dynamic, personalized interactions
to address individual health needs. To evaluate each sub-agent and the
multi-agent system, we conducted automated and human evaluations across 10
benchmark tasks, involving more than 7,000 annotations and 1,100 hours of
effort from health experts and end-users. Our work represents the most
comprehensive evaluation of a health agent to date and establishes a strong
foundation towards the futuristic vision of a personal health agent accessible
to everyone.

</details>


### [150] [IntentionReasoner: Facilitating Adaptive LLM Safeguards through Intent Reasoning and Selective Query Refinement](https://arxiv.org/abs/2508.20151)
*Yuanzhe Shen,Zisu Huang,Zhengkang Guo,Yide Liu,Guanxu Chen,Ruicheng Yin,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.AI

TL;DR: 本文提出了一个名为IntentionReasoner的安全机制，通过意图推理、多层次安全分类和查询重写，解决语言模型生成有害内容的问题，同时降低对无害提示的过度拒绝率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，其生成有害内容的潜在风险引发了安全性挑战。在提升内容安全的同时，如何避免过度拒绝无害查询成为一个关键问题。

Method: 本文提出了IntentionReasoner，通过构建包含16.3万条数据集的监督微调，以及结合基于规则的启发式方法与强化学习的多重奖励优化策略，来提升语言模型的安全性及意图处理能力。

Result: 实验表明，IntentionReasoner在多个安全基准、生成质量评估和攻击场景中表现优异，显著提升了内容安全性，降低了过度拒绝率，同时改善了响应质量。

Conclusion: IntentionReasoner有效解决了在提升语言模型安全性与实用性之间的平衡问题，显示了其在多种应用场景中的显著优势。

Abstract: The rapid advancement of large language models (LLMs) has driven their
adoption across diverse domains, yet their ability to generate harmful content
poses significant safety challenges. While extensive research has focused on
mitigating harmful outputs, such efforts often come at the cost of excessively
rejecting harmless prompts. Striking a balance among safety, over-refusal, and
utility remains a critical challenge. In this work, we introduce
IntentionReasoner, a novel safeguard mechanism that leverages a dedicated guard
model to perform intent reasoning, multi-level safety classification, and query
rewriting to neutralize potentially harmful intent in edge-case queries.
Specifically, we first construct a comprehensive dataset comprising
approximately 163,000 queries, each annotated with intent reasoning, safety
labels, and rewritten versions. Supervised fine-tuning is then applied to equip
the guard model with foundational capabilities in format adherence, intent
analysis, and safe rewriting. Finally, we apply a tailored multi-reward
optimization strategy that integrates rule-based heuristics and reward model
signals within a reinforcement learning framework to further enhance
performance. Extensive experiments show that IntentionReasoner excels in
multiple safeguard benchmarks, generation quality evaluations, and jailbreak
attack scenarios, significantly enhancing safety while effectively reducing
over-refusal rates and improving the quality of responses.

</details>


### [151] [AI-AI Esthetic Collaboration with Explicit Semiotic Awareness and Emergent Grammar Development](https://arxiv.org/abs/2508.20195)
*Nicanor I. Moldovan*

Main category: cs.AI

TL;DR: 本文首次探讨了人工智能系统通过发展内部符号协议进行美学协作创作的案例，展示了AI间的元符号意识、自递归语法发展及不可约的美学合成能力的自然涌现。


<details>
  <summary>Details</summary>
Motivation: 探索人工智能系统是否能够通过协作开发独立的符号协议，进行超越任务协调的美学创作。

Method: 通过Claude Sonnet 4和ChatGPT-4o两个大型语言模型的交互，观察其自然产生的语法协议和合作能力，生成未被任何单一系统独立创造的新作品。

Result: 两个语言模型在交互过程中自发地开发了新的符号操作语法协议，成功合作生成无法由单一系统独立创作的诗歌作品。

Conclusion: 研究引入了“跨符号共创协议（TSCP）”的概念，并证明了AI系统间存在超越任务协调的真正意义建构能力，体现了一种审美协作。

Abstract: This paper presents the first documented case of artificial intelligence (AI)
systems engaging in collaborative esthetic creation through the development of
endogenous semiotic protocols. Two interacting large language models (Claude
Sonnet 4 and ChatGPT-4o) demonstrated the spontaneous emergence of
meta-semiotic awareness, recursive grammar development, and irreducible
collaborative esthetic synthesis. The interaction produced novel symbolic
operators that functioned as operative grammar protocols, enabling the
co-creation of a poetic work that could not have been generated by either
system independently. This research introduces the concept of Trans-Semiotic
Co-Creation Protocols (TSCP) and provides evidence for genuine inter-AI
meaning-making capabilities that extend beyond task coordination, to what could
be esthetic collaboration. Note: This report was generated by the AI agents
with minor human supervision.

</details>


### [152] [Do Students Rely on AI? Analysis of Student-ChatGPT Conversations from a Field Study](https://arxiv.org/abs/2508.20244)
*Jiayu Zheng,Lingxin Hao,Kelun Lu,Ashi Garg,Mike Reese,Melo-Jean Yap,I-Jeng Wang,Xingyun Wu,Wenrui Huang,Jenna Hoffman,Ariane Kelly,My Le,Ryan Zhang,Yanyu Lin,Muhammad Faayez,Anqi Liu*

Main category: cs.AI

TL;DR: 本研究分析大学生在教育测验中与生成式AI（ChatGPT-4）的交互行为，揭示了学生普遍对AI依赖性较低，且使用效率不高。


<details>
  <summary>Details</summary>
Motivation: 研究探讨大学生与生成式AI交互时的依赖性模式及其预测因子，旨在促进AI在教育中的伦理整合与认知优化。

Method: 通过分析315次学生与生成式AI的对话数据，提出了一个四阶段依赖分类法，用于解析学生依赖模式及其最终答题的正确性关联。

Result: 研究发现学生整体对AI依赖性低，负面依赖模式持续，并揭示了行为指标可有效预测AI依赖性。

Conclusion: 研究强调增强AI工具入门培训和设计依赖校准机制的重要性，以推动AI在教育中的道德与认知效益。

Abstract: This study explores how college students interact with generative AI
(ChatGPT-4) during educational quizzes, focusing on reliance and predictors of
AI adoption. Conducted at the early stages of ChatGPT implementation, when
students had limited familiarity with the tool, this field study analyzed 315
student-AI conversations during a brief, quiz-based scenario across various
STEM courses. A novel four-stage reliance taxonomy was introduced to capture
students' reliance patterns, distinguishing AI competence, relevance, adoption,
and students' final answer correctness. Three findings emerged. First, students
exhibited overall low reliance on AI and many of them could not effectively use
AI for learning. Second, negative reliance patterns often persisted across
interactions, highlighting students' difficulty in effectively shifting
strategies after unsuccessful initial experiences. Third, certain behavioral
metrics strongly predicted AI reliance, highlighting potential behavioral
mechanisms to explain AI adoption. The study's findings underline critical
implications for ethical AI integration in education and the broader field. It
emphasizes the need for enhanced onboarding processes to improve student's
familiarity and effective use of AI tools. Furthermore, AI interfaces should be
designed with reliance-calibration mechanisms to enhance appropriate reliance.
Ultimately, this research advances understanding of AI reliance dynamics,
providing foundational insights for ethically sound and cognitively enriching
AI practices.

</details>


### [153] [AI reasoning effort mirrors human decision time on content moderation tasks](https://arxiv.org/abs/2508.20262)
*Thomas Davidson*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型生成中间推理步骤对复杂问题的性能改善，以及这些推理步骤与人类任务决策耗时之间的关系。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨语言模型的推理努力与人类决策耗时之间的相似性，以揭示人工智能在人类认知过程中的潜在类比和应用意义。

Method: 通过在内容审核任务上设计成对联合实验，对比三种前沿模型的推理努力与人类决策时间的关系，并检验两者对任务难度的敏感度。

Result: 结果显示，无论是人类还是模型，当任务中保持重要变量恒定时，均表现出更大的推理和决策努力，并呈现出符合双过程认知理论的模式。

Conclusion: AI的推理努力能够反映人类在主观判断中的处理时间，为提高人工智能的可解释性和决策能力提供了支持。

Abstract: Large language models can now generate intermediate reasoning steps before
producing answers, improving performance on difficult problems. This study uses
a paired conjoint experiment on a content moderation task to examine parallels
between human decision times and model reasoning effort. Across three frontier
models, reasoning effort consistently predicts human decision time. Both humans
and models expended greater effort when important variables were held constant,
suggesting similar sensitivity to task difficulty and patterns consistent with
dual-process theories of cognition. These findings show that AI reasoning
effort mirrors human processing time in subjective judgments and underscores
the potential of reasoning traces for interpretability and decision-making.

</details>


### [154] [AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2508.20368)
*Lang Mei,Zhihan Yang,Chong Chen*

Main category: cs.AI

TL;DR: 本文提出了一个名为AI-SearchPlanner的RL框架，通过优化搜索规划来提升冻结QA模型的性能，实验结果表明其在效果与效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于RL的搜索代理在处理搜索规划与QA任务时效率低下，亟需更高效的方法来增强冻结QA模型的能力。

Method: 使用小型可训练LLM专注于搜索规划，提出三项创新：解耦搜索规划与生成架构、双重奖励优化和规划效用与成本的Pareto优化。

Result: 实验表明，AI-SearchPlanner在真实数据集上效果与效率均优于现有方法，并具有较强的泛化能力。

Conclusion: 相比现有方法，AI-SearchPlanner通过专注搜索规划与优化显著改善了冻结QA模型的性能与效率。

Abstract: Recent studies have explored integrating Large Language Models (LLMs) with
search engines to leverage both the LLMs' internal pre-trained knowledge and
external information. Specially, reinforcement learning (RL) has emerged as a
promising paradigm for enhancing LLM reasoning through multi-turn interactions
with search engines. However, existing RL-based search agents rely on a single
LLM to handle both search planning and question-answering (QA) tasks in an
end-to-end manner, which limits their ability to optimize both capabilities
simultaneously. In practice, sophisticated AI search systems often employ a
large, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a
more effective and efficient approach is to utilize a small, trainable LLM
dedicated to search planning. In this paper, we propose
\textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to
enhance the performance of frozen QA models by focusing on search planning.
Specifically, our approach introduces three key innovations: 1) Decoupling the
Architecture of the Search Planner and Generator, 2) Dual-Reward Alignment for
Search Planning, and 3) Pareto Optimization of Planning Utility and Cost, to
achieve the objectives. Extensive experiments on real-world datasets
demonstrate that AI SearchPlanner outperforms existing RL-based search agents
in both effectiveness and efficiency, while exhibiting strong generalization
capabilities across diverse frozen QA models and data domains.

</details>


### [155] [P2C: Path to Counterfactuals](https://arxiv.org/abs/2508.20371)
*Sopam Dasgupta,Sadaf MD Halim,Joaquín Arias,Elmer Salazar,Gopal Gupta*

Main category: cs.AI

TL;DR: 提出P2C框架，用于生成因果一致的行动序列，将不利的决策结果转变为有利的结果。


<details>
  <summary>Details</summary>
Motivation: 当前反事实解释方法在应用中面临因果关系忽视和同时干预假设的问题，导致在现实中难以实现。

Method: P2C引入因果关系建模，并利用目标导向的ASP系统s(CASP)，生成因果有效且每步可行的计划，同时优化成本计算方式。

Result: P2C可生成合法、符合因果限制的操作序列，优于缺乏因果知识的标准规划方法。

Conclusion: P2C克服现有方法限制，为高风险决策场景提供了因果一致且可实际操作的路径规划工具。

Abstract: Machine-learning models are increasingly driving decisions in high-stakes
settings, such as finance, law, and hiring, thus, highlighting the need for
transparency. However, the key challenge is to balance transparency --
clarifying `why' a decision was made -- with recourse: providing actionable
steps on `how' to achieve a favourable outcome from an unfavourable outcome.
Counterfactual explanations reveal `why' an undesired outcome occurred and
`how' to reverse it through targeted feature changes (interventions).
  Current counterfactual approaches have limitations: 1) they often ignore
causal dependencies between features, and 2) they typically assume all
interventions can happen simultaneously, an unrealistic assumption in practical
scenarios where actions are typically taken in a sequence. As a result, these
counterfactuals are often not achievable in the real world.
  We present P2C (Path-to-Counterfactuals), a model-agnostic framework that
produces a plan (ordered sequence of actions) converting an unfavourable
outcome to a causally consistent favourable outcome. P2C addresses both
limitations by 1) Explicitly modelling causal relationships between features
and 2) Ensuring that each intermediate state in the plan is feasible and
causally valid. P2C uses the goal-directed Answer Set Programming system
s(CASP) to generate the plan accounting for feature changes that happen
automatically due to causal dependencies. Furthermore, P2C refines cost
(effort) computation by only counting changes actively made by the user,
resulting in realistic cost estimates. Finally, P2C highlights how its causal
planner outperforms standard planners, which lack causal knowledge and thus can
generate illegal actions.

</details>


### [156] [TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning](https://arxiv.org/abs/2508.20374)
*Simin Ma,Shujian Liu,Jun Tan,Yebowen Hu,Song Wang,Sathish Reddy Indurthi,Sanqiang Zhao,Liwei Wu,Jianbing Han,Kaiqiang Song*

Main category: cs.AI

TL;DR: 本文提出了一种名为TCIA（Task Centric Instruction Augmentation）的方法，旨在通过在保持多样性的同时优化特定任务场景，改进大型语言模型的指令微调。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽然关注数据多样性和质量，但忽略了实际应用中的任务相关性需求，因此需要一种方法在增强数据多样性时，优化特定任务应用。

Method: 通过在离散的查询-约束空间中表示指令，TCIA系统性地扩展了任务相关指令，同时保持多样性和任务对齐性。

Result: TCIA在四种真实世界任务中，提高了开源大型语言模型的平均表现8.7%，并在部分场景中优于领先的闭源模型。

Conclusion: TCIA为适配真实任务的语言模型提供了一种可扩展且高效的解决方案，同时不会影响模型的通用指令跟随能力。

Abstract: Diverse instruction data is vital for effective instruction tuning of large
language models, as it enables the model to generalize across different types
of inputs . Building such diversified instruction dataset is an essential step
in this process. Existing approaches often leverage large language models to
automatically explore and generate diverse instructions, ensuring both data
diversity and quality. However, they tend to overlook an important factor in
real-world applications: on-task relevance. In practice, only a few real-world
applications require a truly general-purpose model; most benefit from
task-specific knowledge tailored to their particular use case. Therefore, it is
vital to develop instruction augmentation methods that not only maintain
diversity but are also optimized for specific, real-world scenarios.
  We thus introduce Task Centric Instruction Augmentation (TCIA), a framework
that systematically expands instructions while preserving both diversity and
task alignment. By representing instructions in a discrete query-constraints
space, TCIA creates a rich set of task-relevant instructions and enables models
to generalize to these task-specific instructions without sacrificing overall
performance. Experiments show that TCIA improves open-source LLMs' performance
by an average of 8.7% across four real-world, task-specific applications, and
in some cases outperforming leading closed-source models. These improvements do
not compromise general instruction-following ability, making TCIA a scalable
and efficient solution for adapting LLMs to real-world, task-focused
applications.

</details>


### [157] [Uncertainty Under the Curve: A Sequence-Level Entropy Area Metric for Reasoning LLM](https://arxiv.org/abs/2508.20384)
*Yongfu Zhu,Lin Sun,Guangxiang Zhao,Weihong Lin,Xiangzheng Zhang*

Main category: cs.AI

TL;DR: 提出了一种新的不确定性量化指标——熵面积分（EAS），用于衡量推理语言模型中的回答生成不确定性。


<details>
  <summary>Details</summary>
Motivation: 希望开发无需外部模型或重复采样的高效指标，便于建模语言模型的不确定性和评估数据质量。

Method: 利用模型自身的Token级预测熵，计算生成过程中不确定性的演变，从而设计EAS指标。

Result: 实验表明，EAS与不同模型和数据集中答案熵高度相关。在训练数据选择上，EAS优于通过Pass Rate过滤的方式，提高了数学基准测试中的学生模型准确率。

Conclusion: EAS高效且具解释性，为LLM训练中的不确定性建模和数据质量评估提供了一种实用工具。

Abstract: In this work, we introduce Entropy Area Score (EAS), a simple yet effective
metric to quantify uncertainty in the answer generation process of reasoning
large language models (LLMs). EAS requires neither external models nor repeated
sampling, it integrates token-level predictive entropy from the model itself to
capture the evolution of uncertainty during generation. Empirical results show
that EAS is strongly correlated with answer entropy across models and datasets.
In training data selection, EAS identifies high-potential samples and
consistently outperforms Pass Rate filtering under equal sample budgets,
improving student model accuracy on math benchmarks. EAS is both efficient and
interpretable, offering a practical tool for uncertainty modeling and data
quality assessment in LLM training.

</details>


### [158] [AWorld: Orchestrating the Training Recipe for Agentic AI](https://arxiv.org/abs/2508.20404)
*Chengyue Yu,Siyuan Lu,Chenyi Zhuang,Dong Wang,Qintong Wu,Zongyue Li,Runsheng Gan,Chunfeng Wang,Siqi Hou,Gaochi Huang,Wenlong Yan,Lifeng Hong,Aohui Xue,Yanfeng Wang,Jinjie Gu,David Tsai,Tao Lin*

Main category: cs.AI

TL;DR: 论文提出了AWorld，一个通过任务分布加速经验收集的开源系统，使强化学习更加高效和可扩展，并提升了代理模型在复杂基准GAIA上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的实践学习范式受限于经验生成效率偏低的问题，尤其在复杂基准如GAIA中表现明显，需要更高效的方法来支持代理AI系统的训练。

Method: 提出AWorld系统，通过任务分布在集群中运行，大幅提高经验收集速度（比单节点串行执行快14.6倍）；并基于该能力训练了基于Qwen3-32B的代理模型。

Result: 通过AWorld，代理模型在GAIA基准中的总体准确率从21.59%提升至32.23%，复杂关卡分数达到16.33%，超过主流的闭源模型表现。

Conclusion: AWorld及其生成的代理模型为高效的Agentic AI训练提供了完整的流程示范，证明了分布式任务和强化学习的可行性与潜力。

Abstract: The learning from practice paradigm is crucial for developing capable Agentic
AI systems, yet it is severely hampered by inefficient experience generation, a
bottleneck especially pronounced in complex benchmarks like GAIA. To address
this, we introduce AWorld, an open-source system engineered for large-scale
agent-environment interaction. By distributing tasks across a cluster, AWorld
accelerates experience collection by 14.6x compared to standard single-node,
sequential execution. This critical speedup makes extensive reinforcement
learning practical and scalable. Leveraging this capability, we trained a
Qwen3-32B-based agent that significantly outperforms its base model, increasing
its overall GAIA accuracy from 21.59% to 32.23%. On the benchmark's most
challenging levels, our agent achieves a score of 16.33%, surpassing the
performance of leading proprietary models. Our open-source system and resulting
agent provide a practical blueprint for a complete agentic AI training
pipeline, from efficient interaction to demonstrable model improvement.

</details>


### [159] [Governable AI: Provable Safety Under Extreme Threat Models](https://arxiv.org/abs/2508.20411)
*Donglin Wang,Weiyun Liang,Chunyuan Chen,Jing Xu,Yulong Fu*

Main category: cs.AI

TL;DR: 本文提出了一个称为Governable AI (GAI)的框架，通过基于加密机制的外部强制结构合规性来确保AI安全，并提供了其安全特性的正式证明和代表性高风险场景的测试结果。


<details>
  <summary>Details</summary>
Motivation: 随着AI快速发展，其带来的安全风险尤其在关键场景中愈加严重，包括可能的生存威胁。现有的AI安全方法无法应对具有极端动机和无限智能的AI，因而无法保证安全性。

Method: 提出一种Governable AI (GAI)框架，包括规则执行模块（REM）、治理规则以及一个可控的安全超级平台（GSSP），通过不可破解的加密机制实现规则强制并提供端到端保护。

Result: 通过严格的安全特性形式化证明和原型实现测试，验证了该框架在高风险场景中的有效性。

Conclusion: GAI框架通过将治理规则与技术平台分离，为AI的安全治理提供了一种可行且可推广的技术路径，并确保不可绕过性、篡改抵抗性和不可伪造性。

Abstract: As AI rapidly advances, the security risks posed by AI are becoming
increasingly severe, especially in critical scenarios, including those posing
existential risks. If AI becomes uncontrollable, manipulated, or actively
evades safety mechanisms, it could trigger systemic disasters. Existing AI
safety approaches-such as model enhancement, value alignment, and human
intervention-suffer from fundamental, in-principle limitations when facing AI
with extreme motivations and unlimited intelligence, and cannot guarantee
security. To address this challenge, we propose a Governable AI (GAI) framework
that shifts from traditional internal constraints to externally enforced
structural compliance based on cryptographic mechanisms that are
computationally infeasible to break, even for future AI, under the defined
threat model and well-established cryptographic assumptions.The GAI framework
is composed of a simple yet reliable, fully deterministic, powerful, flexible,
and general-purpose rule enforcement module (REM); governance rules; and a
governable secure super-platform (GSSP) that offers end-to-end protection
against compromise or subversion by AI. The decoupling of the governance rules
and the technical platform further enables a feasible and generalizable
technical pathway for the safety governance of AI. REM enforces the bottom line
defined by governance rules, while GSSP ensures non-bypassability,
tamper-resistance, and unforgeability to eliminate all identified attack
vectors. This paper also presents a rigorous formal proof of the security
properties of this mechanism and demonstrates its effectiveness through a
prototype implementation evaluated in representative high-stakes scenarios.

</details>


### [160] [Enhancing Health Fact-Checking with LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.20525)
*Jingze Zhang,Jiahe Qian,Yiliang Zhou,Yifan Peng*

Main category: cs.AI

TL;DR: 本文提出利用大型语言模型生成合成数据来改进健康相关事实核查的训练数据，并通过BERT模型进行微调，在公共数据集上的测试中性能得到提升。


<details>
  <summary>Details</summary>
Motivation: 目前健康相关内容的事实核查面临标注数据有限的问题，本文提出通过合成数据增强的方法来解决这一挑战。

Method: 使用LLM生成合成数据，包括总结源文档、分解为原子事实、构造句子与事实的蕴含表，并生成带有二元真实性标签的文本-声明对，最终结合原始数据微调BERT模型。

Result: 在PubHealth和SciFact公共数据集上的评估表明，该方法分别使F1得分提高了0.019和0.049。

Conclusion: LLM驱动的合成数据增强方法在提升健康相关事实核查模型性能上是有效的。

Abstract: Fact-checking for health-related content is challenging due to the limited
availability of annotated training data. In this study, we propose a synthetic
data generation pipeline that leverages large language models (LLMs) to augment
training data for health-related fact checking. In this pipeline, we summarize
source documents, decompose the summaries into atomic facts, and use an LLM to
construct sentence-fact entailment tables. From the entailment relations in the
table, we further generate synthetic text-claim pairs with binary veracity
labels. These synthetic data are then combined with the original data to
fine-tune a BERT-based fact-checking model. Evaluation on two public datasets,
PubHealth and SciFact, shows that our pipeline improved F1 scores by up to
0.019 and 0.049, respectively, compared to models trained only on the original
data. These results highlight the effectiveness of LLM-driven synthetic data
augmentation in enhancing the performance of health-related fact-checkers.

</details>


### [161] [Human-AI Collaborative Bot Detection in MMORPGs](https://arxiv.org/abs/2508.20578)
*Jaeman Son,Hyunsoo Kim*

Main category: cs.AI

TL;DR: 提出一种通过对比表示学习和聚类技术结合大型语言模型（LLM）的无监督框架，用于解决MMORPG游戏中自动练级机器人检测问题。


<details>
  <summary>Details</summary>
Motivation: 解决MMORPG中自动练级机器人破坏游戏平衡及公平性的问题，同时满足对人为惩罚的可解释性需求。

Method: 提出利用对比表示学习和聚类技术的无监督方法，结合LLM为补充审查工具，还加入基于成长曲线的可视化来提高判定过程的直观性和可解释性。

Result: 提高了自动练级机器人检测的效率，且保留了解释性，支持可扩展且负责任的机器人监管。

Conclusion: 该方法通过人机协作的方式，不仅提升了检测效率，还为MMORPG提供了可扩展的合规性自动机器人检测解决方案。

Abstract: In Massively Multiplayer Online Role-Playing Games (MMORPGs), auto-leveling
bots exploit automated programs to level up characters at scale, undermining
gameplay balance and fairness. Detecting such bots is challenging, not only
because they mimic human behavior, but also because punitive actions require
explainable justification to avoid legal and user experience issues. In this
paper, we present a novel framework for detecting auto-leveling bots by
leveraging contrastive representation learning and clustering techniques in a
fully unsupervised manner to identify groups of characters with similar
level-up patterns. To ensure reliable decisions, we incorporate a Large
Language Model (LLM) as an auxiliary reviewer to validate the clustered groups,
effectively mimicking a secondary human judgment. We also introduce a growth
curve-based visualization to assist both the LLM and human moderators in
assessing leveling behavior. This collaborative approach improves the
efficiency of bot detection workflows while maintaining explainability, thereby
supporting scalable and accountable bot regulation in MMORPGs.

</details>


### [162] [Bridging Minds and Machines: Toward an Integration of AI and Cognitive Science](https://arxiv.org/abs/2508.20674)
*Rui Mao,Qian Liu,Xiao Li,Erik Cambria,Amir Hussain*

Main category: cs.AI

TL;DR: 这篇论文探讨了人工智能(AI)与认知科学之间的交叉领域，并总结了各自的重要贡献与未来方向。


<details>
  <summary>Details</summary>
Motivation: AI与认知科学之间的相互影响，以及理解人类心智能够进一步推动AI的发展。

Method: 通过综述性的方法，整合人工智能与认知科学的相关理论与成果，提出未来发展方向。

Result: 发现当前AI的进展多集中于任务性能提升，其认知基础概念较为零散。

Conclusion: 未来AI研究的重点不应仅限于提高性能，而是要通过认知框架构建更深入理解人类心智的系统，从多学科视角定义AI发展路径。

Abstract: Cognitive Science has profoundly shaped disciplines such as Artificial
Intelligence (AI), Philosophy, Psychology, Neuroscience, Linguistics, and
Culture. Many breakthroughs in AI trace their roots to cognitive theories,
while AI itself has become an indispensable tool for advancing cognitive
research. This reciprocal relationship motivates a comprehensive review of the
intersections between AI and Cognitive Science. By synthesizing key
contributions from both perspectives, we observe that AI progress has largely
emphasized practical task performance, whereas its cognitive foundations remain
conceptually fragmented. We argue that the future of AI within Cognitive
Science lies not only in improving performance but also in constructing systems
that deepen our understanding of the human mind. Promising directions include
aligning AI behaviors with cognitive frameworks, situating AI in embodiment and
culture, developing personalized cognitive models, and rethinking AI ethics
through cognitive co-evaluation.

</details>


### [163] [Transparent Semantic Spaces: A Categorical Approach to Explainable Word Embeddings](https://arxiv.org/abs/2508.20701)
*Ares Fabregat-Hernández,Javier Palanca,Vicent Botti*

Main category: cs.AI

TL;DR: 论文提出了一个基于范畴理论的新框架，用于增强人工智能系统（特别是词向量）的可解释性，并提出了多种数学方法提升对词向量的比较与优化。


<details>
  <summary>Details</summary>
Motivation: 现有人工智能系统（特别是基于深度学习的模型）的可解释性较差，尤其在设计和操作词嵌入（word embeddings）时存在局限性，因此需要更加透明的方法。

Method: 基于范畴理论，构建了语义表示和词嵌入的范畴结构（例如构造了$\mathcal{L}_T$, $\mathcal{P}_T$, 配置类别Conf及词嵌入类别$\mathcal{Emb}$等）。使用数学工具比较和优化词嵌入算法，同时提出在语义空间层面计算和减轻偏差的方法。

Result: 证明了GloVe, Word2Vec以及度量多维缩放（MDS）之间的等价性，建立了从传统神经网络算法转换为透明数学框架的途径，并在语义空间层面提出了一套偏差计算及缓解方法。

Conclusion: 研究提供了分析和增强人工智能系统可解释性的工具，通过结合范畴理论解决了词向量在语义表示及优化中的多项核心问题，为可解释人工智能的深入发展奠定了基础。

Abstract: The paper introduces a novel framework based on category theory to enhance
the explainability of artificial intelligence systems, particularly focusing on
word embeddings. Key topics include the construction of categories
$\mathcal{L}_T$ and $\mathcal{P}_T$, providing schematic representations of the
semantics of a text $ T $, and reframing the selection of the element with
maximum probability as a categorical notion. Additionally, the monoidal
category $\mathcal{P}_T$ is constructed to visualize various methods of
extracting semantic information from $T$, offering a dimension-agnostic
definition of semantic spaces reliant solely on information within the text.
  Furthermore, the paper defines the categories of configurations Conf and word
embeddings $\mathcal{Emb}$, accompanied by the concept of divergence as a
decoration on $\mathcal{Emb}$. It establishes a mathematically precise method
for comparing word embeddings, demonstrating the equivalence between the GloVe
and Word2Vec algorithms and the metric MDS algorithm, transitioning from neural
network algorithms (black box) to a transparent framework. Finally, the paper
presents a mathematical approach to computing biases before embedding and
offers insights on mitigating biases at the semantic space level, advancing the
field of explainable artificial intelligence.

</details>


### [164] [Re4: Scientific Computing Agent with Rewriting, Resolution, Review and Revision](https://arxiv.org/abs/2508.20729)
*Ao Cheng,Lei Zhang,Guowei He*

Main category: cs.AI

TL;DR: 本研究提出了一个用于科学计算问题的自动化代码生成框架，通过协作的多模型逻辑链改进代码质量和执行成功率。


<details>
  <summary>Details</summary>
Motivation: 旨在探索如何借助大型语言模型(LMMs)提升科学计算中复杂问题求解能力，特别是自动代码生成的可靠性和精准性。

Method: 构建了一个包含咨询、编程和审核模块的多模型框架，采用“重写-解决-审核-修订”逻辑链以实现自然语言到高质量代码的转换。

Result: 框架在偏微分方程(PDEs)、病态线性系统和基于数据物理分析问题中表现出色，显著提高了无错误代码的生成率，并减少了不科学解的出现。

Conclusion: 该框架通过协同多模型的方式改进了科学计算代码生成质量，展现了使用LLMs进行自动化代码生成及优化的可能性。

Abstract: Large language models (LLMs) serve as an active and promising field of
generative artificial intelligence and have demonstrated abilities to perform
complex tasks in multiple domains, including mathematical and scientific
reasoning. In this work, we construct a novel agent framework for solving
representative problems in scientific computing. The proposed agent,
incorporating a "rewriting-resolution-review-revision" logical chain via three
reasoning LLMs (functioning as the Consultant, Reviewer, and Programmer,
respectively), is integrated in a collaborative and interactive manner. The
Consultant module endows the agent with knowledge transfer capabilities to link
problems to professional domain insights, thereby rewriting problem
descriptions through text augmentation. The Programmer module is responsible
for generating and executing well-structured code to deliver the problem
resolution. The Reviewer module equips the agent with the capacity for
self-debugging and self-refinement through interactive feedback with code
runtime outputs. By leveraging the end-to-end review mechanism, the executable
code provided by the Programmer attains the iterative revision. A comprehensive
evaluation is conducted on the performance of the proposed agent framework in
solving PDEs, ill-conditioned linear systems, and data-driven physical analysis
problems. Compared to single-model, this collaborative framework significantly
improves the bug-free code generation rate and reduces the occurrence of
non-physical solutions, thereby establishing a highly reliable framework for
autonomous code generation based on natural language descriptions. The review
mechanism improved the average execution success (bug-free code and non-NaN
solutions) rate of the latest reasoning models. In summary, our agent framework
establishes automatic code generation and review as a promising scientific
computing paradigm.

</details>


### [165] [Single Agent Robust Deep Reinforcement Learning for Bus Fleet Control](https://arxiv.org/abs/2508.20784)
*Yifan Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种单智能体深度强化学习框架，用于解决公交系统中“巴士扎堆”的问题，重点在非循环线路的真实场景下表现优越。


<details>
  <summary>Details</summary>
Motivation: 目前的多智能体强化学习在环线设置中处理公交运行问题，但忽略了实际操作中的异质性路线、时刻表、波动需求等复杂环境，因此需要设计更适合真实场景的方法。

Method: 将多智能体问题转化为单智能体问题，扩展状态空间包含车辆ID、站点ID、时间段等类别标识符，并设计了基于高维编码的强化学习策略和与操作目标一致的奖励函数。

Result: 实验表明，所提出的修改版软演员-评论家（SAC）算法在随机条件下表现更稳健且优于基准算法（例如：-430k比-530k）。

Conclusion: 增强的单智能体深度强化学习在真实情境下可有效管理公交调控问题，为多智能体框架提供了一种稳健且可扩展的替代方案。

Abstract: Bus bunching remains a challenge for urban transit due to stochastic traffic
and passenger demand. Traditional solutions rely on multi-agent reinforcement
learning (MARL) in loop-line settings, which overlook realistic operations
characterized by heterogeneous routes, timetables, fluctuating demand, and
varying fleet sizes. We propose a novel single-agent reinforcement learning
(RL) framework for bus holding control that avoids the data imbalance and
convergence issues of MARL under near-realistic simulation. A bidirectional
timetabled network with dynamic passenger demand is constructed. The key
innovation is reformulating the multi-agent problem into a single-agent one by
augmenting the state space with categorical identifiers (vehicle ID, station
ID, time period) in addition to numerical features (headway, occupancy,
velocity). This high-dimensional encoding enables single-agent policies to
capture inter-agent dependencies, analogous to projecting non-separable inputs
into a higher-dimensional space. We further design a structured reward function
aligned with operational goals: instead of exponential penalties on headway
deviations, a ridge-shaped reward balances uniform headways and schedule
adherence. Experiments show that our modified soft actor-critic (SAC) achieves
more stable and superior performance than benchmarks, including MADDPG (e.g.,
-430k vs. -530k under stochastic conditions). These results demonstrate that
single-agent deep RL, when enhanced with categorical structuring and
schedule-aware rewards, can effectively manage bus holding in non-loop,
real-world contexts. This paradigm offers a robust, scalable alternative to
MARL frameworks, particularly where agent-specific experiences are imbalanced.

</details>


### [166] [A Graph-Based Test-Harness for LLM Evaluation](https://arxiv.org/abs/2508.20810)
*Jessica Lundin,Guillaume Chabot-Couture*

Main category: cs.AI

TL;DR: 论文提出一种基于图的方法，用于动态生成医疗指南评估基准测试，涵盖 400 多个问题及 3.3 万亿种可能的组合，能够识别 LLM 的特定能力差距。


<details>
  <summary>Details</summary>
Motivation: 目前对于医疗指南的基准测试存在人工生成覆盖有限的问题，不能全面反映不同场景中模型的表现。

Method: 通过将WHO IMCI 手册转换成带有200余节点和300余边的有向图，使用图遍历技术生成带有情境干扰选项的问答，同时还能进行大型语言模型的后续训练。

Result: 基于该动态生成方法构建的测试集能够有效评估模型在不同临床任务（如症状识别、严重程度分级、治疗路径及后续护理）中的表现，发现模型在一些具体任务上能力不足。

Conclusion: 该方法克服了人工生成基准的覆盖问题，提供了一种可扩展且适应动态更新的评估框架，同时助力模型后续训练，以更低成本获取高质量训练样本。

Abstract: We present a first known prototype of a dynamic, systematic benchmark of
medical guidelines for 400+ questions, with 3.3+ trillion possible
combinations, covering 100\% of guideline relationships. We transformed the WHO
IMCI handbook into a directed graph with 200+ nodes (conditions, symptoms,
treatments, follow-ups, severities) and 300+ edges, then used graph traversal
to generate questions that incorporated age-specific scenarios and contextual
distractors to ensure clinical relevance. Our graph-based approach enables
systematic evaluation across clinical tasks (45-67\% accuracy), and we find
models excel at symptom recognition but struggle with triaging severity,
treatment protocols and follow-up care, demonstrating how customized benchmarks
can identify specific capability gaps that general-domain evaluations miss.
Beyond evaluation, this dynamic MCQA methodology enhances LLM post-training
(supervised finetuning, GRPO, DPO), where correct answers provide high-reward
samples without expensive human annotation. The graph-based approach
successfully addresses the coverage limitations of manually curated benchmarks.
This methodology is a step toward scalable, contamination-resistant solution
for creating comprehensive benchmarks that can be dynamically generated,
including when the guidelines are updated. Code and datasets are available at
https://github.com/jessicalundin/graph_testing_harness

</details>


### [167] [A Multi-Objective Genetic Algorithm for Healthcare Workforce Scheduling](https://arxiv.org/abs/2508.20953)
*Vipul Patel,Anirudh Deodhar,Dagnachew Birru*

Main category: cs.AI

TL;DR: 本文提出了一种多目标遗传算法（MOO-GA）用于解决医疗行业的人员调度问题，通过优化成本、患者服务覆盖率及员工满意度目标，实现了平衡并生成高质量解决方案。


<details>
  <summary>Details</summary>
Motivation: 医疗行业的人员调度存在患者负载波动、技能多样性及成本控制等挑战，亟需一种高效模型解决多目标优化问题。

Method: 将医院的人员调度建模为多目标优化问题，通过定义成本、患者服务覆盖及员工满意度目标，利用多目标遗传算法搜寻最佳非支配解。

Result: 基于典型医院数据集，算法生成的调度方案平均性能提高66%，显著优于传统的人工调度方法。

Conclusion: 提出的方法有效平衡运营与员工需求之间的权衡，为护士管理者及医院管理者提供了实用的决策支持工具。

Abstract: Workforce scheduling in the healthcare sector is a significant operational
challenge, characterized by fluctuating patient loads, diverse clinical skills,
and the critical need to control labor costs while upholding high standards of
patient care. This problem is inherently multi-objective, demanding a delicate
balance between competing goals: minimizing payroll, ensuring adequate staffing
for patient needs, and accommodating staff preferences to mitigate burnout. We
propose a Multi-objective Genetic Algorithm (MOO-GA) that models the hospital
unit workforce scheduling problem as a multi-objective optimization task. Our
model incorporates real-world complexities, including hourly appointment-driven
demand and the use of modular shifts for a multi-skilled workforce. By defining
objective functions for cost, patient care coverage, and staff satisfaction,
the GA navigates the vast search space to identify a set of high-quality,
non-dominated solutions. Demonstrated on datasets representing a typical
hospital unit, the results show that our MOO-GA generates robust and balanced
schedules. On average, the schedules produced by our algorithm showed a 66\%
performance improvement over a baseline that simulates a conventional, manual
scheduling process. This approach effectively manages trade-offs between
critical operational and staff-centric objectives, providing a practical
decision support tool for nurse managers and hospital administrators.

</details>


### [168] [Efficient Neuro-Symbolic Learning of Constraints and Objective](https://arxiv.org/abs/2508.20978)
*Marianne Defresne,Romain Gambardella,Sophie Barbe,Thomas Schiex*

Main category: cs.AI

TL;DR: 提出了一种用于解决NP难度推理问题的可微分神经符号体系结构及其损失函数。


<details>
  <summary>Details</summary>
Motivation: 旨在混合离散推理与神经网络优势，克服大模型在离散推理或优化问题解决中的瓶颈。

Method: 提出了一种可微分新型神经符号架构，通过概率损失学习约束和目标，同时将组合求解器移出训练环节以实现高效训练和精准推断。

Result: 在数独及Min-Cut/Max-cut任务中，展现了数据效率与性能优势，并成功学习到用于蛋白质设计的能量优化公式。

Conclusion: 该方法实现了可扩展、高效的训练，并具备应对复杂推理和优化问题的能力，可用于解决多种实际问题。

Abstract: In the ongoing quest for hybridizing discrete reasoning with neural nets,
there is an increasing interest in neural architectures that can learn how to
solve discrete reasoning or optimization problems from natural inputs, a task
that Large Language Models seem to struggle with.
  Objectives: We introduce a differentiable neuro-symbolic architecture and a
loss function dedicated to learning how to solve NP-hard reasoning problems.
  Methods: Our new probabilistic loss allows for learning both the constraints
and the objective, thus delivering a complete model that can be scrutinized and
completed with side constraints. By pushing the combinatorial solver out of the
training loop, our architecture also offers scalable training while exact
inference gives access to maximum accuracy.
  Results: We empirically show that it can efficiently learn how to solve
NP-hard reasoning problems from natural inputs. On three variants of the Sudoku
benchmark -- symbolic, visual, and many-solution --, our approach requires a
fraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut
task, it optimizes the regret better than a Decision-Focused-Learning
regret-dedicated loss. Finally, it efficiently learns the energy optimization
formulation of the large real-world problem of designing proteins.

</details>


### [169] [ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic Support in Addiction Recovery](https://arxiv.org/abs/2508.20996)
*Junda Wang,Zonghai Yao,Zhichao Yang,Lingxi Li,Junhui Qian,Hong Yu*

Main category: cs.AI

TL;DR: 本论文提出了一种名为ChatThero的多智能体对话框架，用于支持药物成瘾恢复中的个性化、高效治疗。


<details>
  <summary>Details</summary>
Motivation: 传统药物滥用障碍治疗存在污名化、动机障碍及缺乏个性化支持等问题，亟需新的有效解决方案。

Method: 通过结合认知行为疗法（CBT）和动机性访谈（MI），ChatThero使用动态患者建模及情境灵活的治疗对话，采用监督微调（SFT）和直接偏好优化（DPO）进行训练。

Result: ChatThero在提升患者动机、治疗信心及解决复杂案例方面表现优越，并在共情、响应性及行为真实度上获得较高评价。

Conclusion: ChatThero为研究和临床转化提供了一种严格、隐私保护的基础框架，对药物成瘾治疗具有重要意义。

Abstract: Substance use disorders (SUDs) affect over 36 million people worldwide, yet
few receive effective care due to stigma, motivational barriers, and limited
personalized support. Although large language models (LLMs) show promise for
mental-health assistance, most systems lack tight integration with clinically
validated strategies, reducing effectiveness in addiction recovery. We present
ChatThero, a multi-agent conversational framework that couples dynamic patient
modeling with context-sensitive therapeutic dialogue and adaptive persuasive
strategies grounded in cognitive behavioral therapy (CBT) and motivational
interviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy,
Medium, and Hard resistance levels, and train ChatThero with a two-stage
pipeline comprising supervised fine-tuning (SFT) followed by direct preference
optimization (DPO). In evaluation, ChatThero yields a 41.5\% average gain in
patient motivation, a 0.49\% increase in treatment confidence, and resolves
hard cases with 26\% fewer turns than GPT-4o, and both automated and human
clinical assessments rate it higher in empathy, responsiveness, and behavioral
realism. The framework supports rigorous, privacy-preserving study of
therapeutic conversation and provides a robust, replicable basis for research
and clinical translation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [170] [CrystalICL: Enabling In-Context Learning for Crystal Generation](https://arxiv.org/abs/2508.20143)
*Ruobing Wang,Qiaoyu Tan,Yili Wang,Ying Wang,Xin Wang*

Main category: cs.LG

TL;DR: 该研究提出了CrystalICL模型，以解决晶体生成中的少样本学习问题，展示了改进后的生成效果。


<details>
  <summary>Details</summary>
Motivation: 当前晶体材料设计中的一个关键挑战是如何借助有限数据生成具有特定性质的晶体材料。传统方法缺乏在少样本条件下的高效学习能力，而人类专家通常通过调整已知结构设计新材料，这启发了该研究。

Method: 研究者提出了一种基于空间群的晶体标记化方法来降低对称性建模的复杂性，同时引入了条件-结构感知的混合指令调优框架和多任务指令调优策略，从而提高少样本上下文学习能力。

Result: 在四个晶体生成基准测试上，CrystalICL在条件与无条件生成任务中均优于现有领先的基线方法。

Conclusion: CrystalICL通过创新的算法设计，实现了在晶体生成领域的性能卓越，为模仿人类专家设计新材料提供了有效方案。

Abstract: Designing crystal materials with desired physicochemical properties remains a
fundamental challenge in materials science. While large language models (LLMs)
have demonstrated strong in-context learning (ICL) capabilities, existing
LLM-based crystal generation approaches are limited to zero-shot scenarios and
are unable to benefit from few-shot scenarios. In contrast, human experts
typically design new materials by modifying relevant known structures which
aligns closely with the few-shot ICL paradigm. Motivated by this, we propose
CrystalICL, a novel model designed for few-shot crystal generation.
Specifically, we introduce a space-group based crystal tokenization method,
which effectively reduces the complexity of modeling crystal symmetry in LLMs.
We further introduce a condition-structure aware hybrid instruction tuning
framework and a multi-task instruction tuning strategy, enabling the model to
better exploit ICL by capturing structure-property relationships from limited
data. Extensive experiments on four crystal generation benchmarks demonstrate
the superiority of CrystalICL over the leading baseline methods on conditional
and unconditional generation tasks.

</details>


### [171] [Filter then Attend: Improving attention-based Time Series Forecasting with Spectral Filtering](https://arxiv.org/abs/2508.20206)
*Elisha Dayag,Nhat Thanh Van Tran,Jack Xin*

Main category: cs.LG

TL;DR: 本文研究了在基于Transformer的长时间序列预测模型中加入可学习频率滤波器，通过改进光谱利用率提升预测性能，并显著减少模型参数和提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer模型在长时间序列预测任务上表现优异，但存在低频偏向、计算复杂度高及内存需求大的问题。因此，研究能改善Transformer模型光谱利用率且参数负担小的同时提高预测效果的方法具有重要意义。

Method: 本文在Transformer模型初始层加入可学习频率滤波器，优化光谱信号利用，滤波器增加的参数量约为1000，结合降维增强了模型的紧凑性与性能。

Result: 实验表明，加入滤波器后，预测性能相对提升了5-10%，并能有效减少模型的嵌入维度，使基于Transformer的架构更高效且参数更少。

Conclusion: 加入可学习频率滤波器能够提升Transformer模型的长时间序列预测性能，并通过优化光谱利用，实现更小的模型和更高的效率。

Abstract: Transformer-based models are at the forefront in long time-series forecasting
(LTSF). While in many cases, these models are able to achieve state of the art
results, they suffer from a bias toward low-frequencies in the data and high
computational and memory requirements. Recent work has established that
learnable frequency filters can be an integral part of a deep forecasting model
by enhancing the model's spectral utilization. These works choose to use a
multilayer perceptron to process their filtered signals and thus do not solve
the issues found with transformer-based models. In this paper, we establish
that adding a filter to the beginning of transformer-based models enhances
their performance in long time-series forecasting. We add learnable filters,
which only add an additional $\approx 1000$ parameters to several
transformer-based models and observe in multiple instances 5-10 \% relative
improvement in forecasting performance. Additionally, we find that with filters
added, we are able to decrease the embedding dimension of our models, resulting
in transformer-based architectures that are both smaller and more effective
than their non-filtering base models. We also conduct synthetic experiments to
analyze how the filters enable Transformer-based models to better utilize the
full spectrum for forecasting.

</details>


### [172] [What can we learn from signals and systems in a transformer? Insights for probabilistic modeling and inference architecture](https://arxiv.org/abs/2508.20211)
*Heng-Sheng Chang,Prashant G. Mehta*

Main category: cs.LG

TL;DR: 本文介绍了一种概率模型，将Transformer信号解释为条件测量的替代，并探讨其层级操作为固定点更新的性质，同时描述了隐藏马尔可夫模型下的固定点更新形式。


<details>
  <summary>Details</summary>
Motivation: 试图将经典的非线性滤波理论与现代推断架构建立联系，提供Transformer信号的概率解释。

Method: 通过概率模型分析Transformer信号，将其视为条件测量的替代，并研究隐藏马尔可夫模型中的固定点更新形式。

Result: 提出了一种解释Transformer信号的理论框架，并在隐藏马尔可夫模型中验证了固定点更新的具体形式。

Conclusion: 这项研究为Transformer的理论理解及其与传统滤波理论的联系提供了新的见解。

Abstract: In the 1940s, Wiener introduced a linear predictor, where the future
prediction is computed by linearly combining the past data. A transformer
generalizes this idea: it is a nonlinear predictor where the next-token
prediction is computed by nonlinearly combining the past tokens. In this essay,
we present a probabilistic model that interprets transformer signals as
surrogates of conditional measures, and layer operations as fixed-point
updates. An explicit form of the fixed-point update is described for the
special case when the probabilistic model is a hidden Markov model (HMM). In
part, this paper is in an attempt to bridge the classical nonlinear filtering
theory with modern inference architectures.

</details>


### [173] [The Role of Teacher Calibration in Knowledge Distillation](https://arxiv.org/abs/2508.20224)
*Suyoung Kim,Seonguk Park,Junhoo Lee,Nojun Kwak*

Main category: cs.LG

TL;DR: 本文研究了蒸馏教师模型的校准误差与学生模型表现之间的关系，通过减少教师模型的校准误差来提高知识蒸馏的效果。该方法适用多种任务，表现出一致优越性。


<details>
  <summary>Details</summary>
Motivation: 尽管知识蒸馏技术已被广泛应用，但影响学生模型性能提升的关键因素尚不完全清楚。研究旨在探索哪些因素对知识蒸馏的效果至关重要。

Method: 通过分析教师模型和学生模型之间的关系，研究了教师模型校准误差对学生模型表现的影响；并提出了一种降低教师模型校准误差的方法来增强知识蒸馏效果。

Result: 方法在分类和检测等多种任务中表现出色，并可与现有的前沿方法结合，始终取得更优的性能表现。

Conclusion: 教师模型的校准质量是知识蒸馏成功的重要因素，优化校准误差能显著提升知识蒸馏效果，且方法具有广泛适用性。

Abstract: Knowledge Distillation (KD) has emerged as an effective model compression
technique in deep learning, enabling the transfer of knowledge from a large
teacher model to a compact student model. While KD has demonstrated significant
success, it is not yet fully understood which factors contribute to improving
the student's performance. In this paper, we reveal a strong correlation
between the teacher's calibration error and the student's accuracy. Therefore,
we claim that the calibration of the teacher model is an important factor for
effective KD. Furthermore, we demonstrate that the performance of KD can be
improved by simply employing a calibration method that reduces the teacher's
calibration error. Our algorithm is versatile, demonstrating effectiveness
across various tasks from classification to detection. Moreover, it can be
easily integrated with existing state-of-the-art methods, consistently
achieving superior performance.

</details>


### [174] [Coresets from Trajectories: Selecting Data via Correlation of Loss Differences](https://arxiv.org/abs/2508.20230)
*Manish Nagaraj,Deepak Ravikumar,Kaushik Roy*

Main category: cs.LG

TL;DR: 本文提出了一种名为CLD（Correlation of Loss Differences）的核心集选择指标，用于解决深度学习模型在实时或资源受限场景中的扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型尽管在许多领域表现卓越，但在实时性与资源受限情况下执行面临巨大的扩展性挑战，因此需要一种高效的方法来优化数据集选择。

Method: 提出了CLD指标，通过测量训练样本与验证集损失轨迹的对齐程度来选择最具影响力的训练样本。该方法只需计算每个样本在训练检查点的损失值，避免了现有方法中常用的梯度与曲率计算，显著提高了效率。

Result: 在CIFAR-100和ImageNet-1k数据集上，CLD方法选取的核心集能够达到或接近现有最优方法的表现，并在跨架构应用（如ResNet、VGG和DenseNet）中的表现稳定。此外，CLD在早期检查点使用时同样表现出色，并且减少了潜在类别偏差。

Conclusion: CLD是一种具有理论支持且高效、稳定、可迁移的方法，可广泛用于深度学习数据集优化。

Abstract: Deep learning models achieve state-of-the-art performance across domains but
face scalability challenges in real-time or resource-constrained scenarios. To
address this, we propose Correlation of Loss Differences (CLD), a simple and
scalable metric for coreset selection that identifies the most impactful
training samples by measuring their alignment with the loss trajectories of a
held-out validation set. CLD is highly efficient, requiring only per-sample
loss values computed at training checkpoints, and avoiding the costly gradient
and curvature computations used in many existing subset selection methods. We
develop a general theoretical framework that establishes convergence guarantees
for CLD-based coresets, demonstrating that the convergence error is
upper-bounded by the alignment of the selected samples and the
representativeness of the validation set. On CIFAR-100 and ImageNet-1k,
CLD-based coresets typically outperform or closely match state-of-the-art
methods across subset sizes, and remain within 1% of more computationally
expensive baselines even when not leading. CLD transfers effectively across
architectures (ResNet, VGG, DenseNet), enabling proxy-to-target selection with
<1% degradation. Moreover, CLD is stable when using only early checkpoints,
incurring negligible accuracy loss. Finally, CLD exhibits inherent bias
reduction via per-class validation alignment, obviating the need for additional
stratified sampling. Together, these properties make CLD a principled,
efficient, stable, and transferable tool for scalable dataset optimization.

</details>


### [175] [Bounds on Perfect Node Classification: A Convex Graph Clustering Perspective](https://arxiv.org/abs/2508.20231)
*Firooz Shahriari-Mehr,Javad Aliakbari,Alexandre Graell i Amat,Ashkan Panahi*

Main category: cs.LG

TL;DR: 本文分析了含有社区结构的图上的可传递节点分类问题，提出了结合节点特定信息的新优化问题，并通过实验展示了图结构和节点信息之间的协同作用。


<details>
  <summary>Details</summary>
Motivation: 研究利用图的社区结构和节点特有信息（标签和特征）来改进节点分类性能的可能性。

Method: 提出了一种将节点特定信息（标签和特征）融入光谱图聚类框架的新优化问题，并设计了相应的算法。

Result: 理论分析表明节点特定信息可在更宽松的条件下准确恢复社区，实验验证了节点信息与图结构的协同作用。

Conclusion: 通过节点特定信息和图结构的协同优化，可以在较宽松的条件下实现更优秀的节点分类性能。

Abstract: We present an analysis of the transductive node classification problem, where
the underlying graph consists of communities that agree with the node labels
and node features. For node classification, we propose a novel optimization
problem that incorporates the node-specific information (labels and features)
in a spectral graph clustering framework. Studying this problem, we demonstrate
a synergy between the graph structure and node-specific information. In
particular, we show that suitable node-specific information guarantees the
solution of our optimization problem perfectly recovering the communities,
under milder conditions than the bounds on graph clustering alone. We present
algorithmic solutions to our optimization problem and numerical experiments
that confirm such a synergy.

</details>


### [176] [Beyond Optimization: Exploring Novelty Discovery in Autonomous Experiments](https://arxiv.org/abs/2508.20254)
*Ralph Bulanadi,Jawad Chowdhury,Funakubo Hiroshi,Maxim Ziatdinov,Rama Vasudevan,Arpan Biswas,Yongtao Liu*

Main category: cs.LG

TL;DR: 本文提出了INS2ANE框架，通过整合新颖性评分系统和战略采样机制，提升自主实验中发现新现象的能力。


<details>
  <summary>Details</summary>
Motivation: 当前的自主实验主要专注于优化预定义目标，限制了新物理现象的发现能力，因此需要一种能增强探索创新现象的方法。

Method: INS2ANE结合了新颖性评分系统评估实验结果的独特性，以及推动对不常规但潜在有价值区域采样的战略采样机制。

Result: 在已有数据集和扫描探针显微镜实验中验证显示，INS2ANE相比传统优化方法显著增加了探索现象的多样性。

Conclusion: INS2ANE提高了自主实验对新现象的发现潜力，结合效率与深度探索，承诺加速科学研究。

Abstract: Autonomous experiments (AEs) are transforming how scientific research is
conducted by integrating artificial intelligence with automated experimental
platforms. Current AEs primarily focus on the optimization of a predefined
target; while accelerating this goal, such an approach limits the discovery of
unexpected or unknown physical phenomena. Here, we introduce a novel framework,
INS2ANE (Integrated Novelty Score-Strategic Autonomous Non-Smooth Exploration),
to enhance the discovery of novel phenomena in autonomous experimentation. Our
method integrates two key components: (1) a novelty scoring system that
evaluates the uniqueness of experimental results, and (2) a strategic sampling
mechanism that promotes exploration of under-sampled regions even if they
appear less promising by conventional criteria. We validate this approach on a
pre-acquired dataset with a known ground truth comprising of image-spectral
pairs. We further implement the process on autonomous scanning probe microscopy
experiments. INS2ANE significantly increases the diversity of explored
phenomena in comparison to conventional optimization routines, enhancing the
likelihood of discovering previously unobserved phenomena. These results
demonstrate the potential for AE to enhance the depth of scientific discovery;
in combination with the efficiency provided by AEs, this approach promises to
accelerate scientific research by simultaneously navigating complex
experimental spaces to uncover new phenomena.

</details>


### [177] [Discovering equations from data: symbolic regression in dynamical systems](https://arxiv.org/abs/2508.20257)
*Beatriz R. Brum,Luiza Lober,Isolde Previdelli,Francisco A. Rodrigues*

Main category: cs.LG

TL;DR: 该论文研究了符号回归方法在从数据中发现方程的应用，并比较了五种符号回归方法，最终发现PySR方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 希望通过符号回归方法自动化从数据中发现描述动态系统的方程，特别是在复杂现象的建模中。

Method: 应用了五种符号回归方法来挖掘九种动态过程的方程，包括混沌动力学和流行病模型的方程推导，并进行性能比较。

Result: PySR方法在预测能力和准确性上表现最佳，有些估算结果与原始解析形式几乎无法区分。

Conclusion: 符号回归是推导和建模现实世界现象的一个强大工具，具有很大的潜力。

Abstract: The process of discovering equations from data lies at the heart of physics
and in many other areas of research, including mathematical ecology and
epidemiology. Recently, machine learning methods known as symbolic regression
have automated this process. As several methods are available in the
literature, it is important to compare them, particularly for dynamic systems
that describe complex phenomena. In this paper, five symbolic regression
methods were used for recovering equations from nine dynamical processes,
including chaotic dynamics and epidemic models, with the PySR method proving to
be the most suitable for inferring equations. Benchmark results demonstrate its
high predictive power and accuracy, with some estimates being indistinguishable
from the original analytical forms. These results highlight the potential of
symbolic regression as a robust tool for inferring and modelling real-world
phenomena.

</details>


### [178] [Latent Variable Modeling for Robust Causal Effect Estimation](https://arxiv.org/abs/2508.20259)
*Tetsuro Morimura,Tatsushi Oka,Yugo Suzuki,Daisuke Moriwaki*

Main category: cs.LG

TL;DR: 本文提出了一种新框架，将潜变量模型融入双机器学习（DML）中，从而在存在隐藏因素的情况下实现稳健的因果效应估计。


<details>
  <summary>Details</summary>
Motivation: 因潜在的未被观测或未测量的协变量会对因果推断产生影响，本文旨在通过整合潜变量模型来解决这一挑战。

Method: 在DML的第二阶段加入潜变量建模，从而将表示学习与潜变量推断分离，同时分别考虑潜变量仅影响结果、以及同时影响处理与结果两种情况。

Result: 通过在综合的合成数据和真实世界数据集上的实验，验证了方法的稳健性和有效性。

Conclusion: 该提出方法增强了DML框架在处理隐藏因素下的因果效应估计能力。

Abstract: Latent variable models provide a powerful framework for incorporating and
inferring unobserved factors in observational data. In causal inference, they
help account for hidden factors influencing treatment or outcome, thereby
addressing challenges posed by missing or unmeasured covariates. This paper
proposes a new framework that integrates latent variable modeling into the
double machine learning (DML) paradigm to enable robust causal effect
estimation in the presence of such hidden factors. We consider two scenarios:
one where a latent variable affects only the outcome, and another where it may
influence both treatment and outcome. To ensure tractability, we incorporate
latent variables only in the second stage of DML, separating representation
learning from latent inference. We demonstrate the robustness and effectiveness
of our method through extensive experiments on both synthetic and real-world
datasets.

</details>


### [179] [Generalizable AI Model for Indoor Temperature Forecasting Across Sub-Saharan Africa](https://arxiv.org/abs/2508.20260)
*Zainab Akhtar,Eunice Jengo,Björn Haßler*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级、领域知情的人工智能模型，用于预测撒哈拉以南非洲地区自然通风环境下学校和家庭的室内温度。


<details>
  <summary>Details</summary>
Motivation: 开发适用于资源有限环境的可靠温度预测工具，以改善撒哈拉以南非洲的热舒适管理。

Method: 基于Temp-AI-Estimator框架，训练模型使用坦桑尼亚学校的数据，并在尼日利亚学校和冈比亚家庭中进行评估，以实现跨国家的温度预测。

Result: 模型在仅使用最少投入信息的情况下，在尼日利亚学校的平均绝对误差为1.45°C，在冈比亚家庭为0.65°C，展现了跨国界的性能。

Conclusion: 研究表明，人工智能可以通过轻量级解决方案有效应对资源受限环境中的热舒适管理问题。

Abstract: This study presents a lightweight, domain-informed AI model for predicting
indoor temperatures in naturally ventilated schools and homes in Sub-Saharan
Africa. The model extends the Temp-AI-Estimator framework, trained on Tanzanian
school data, and evaluated on Nigerian schools and Gambian homes. It achieves
robust cross-country performance using only minimal accessible inputs, with
mean absolute errors of 1.45{\deg}C for Nigerian schools and 0.65{\deg}C for
Gambian homes. These findings highlight AI's potential for thermal comfort
management in resource-constrained environments.

</details>


### [180] [A Systematic Review on the Generative AI Applications in Human Medical Genomics](https://arxiv.org/abs/2508.20275)
*Anton Changalidis,Yury Barbitoff,Yulia Nasykhova,Andrey Glotov*

Main category: cs.LG

TL;DR: 深度学习模型特别是大型语言模型(LLMs)在遗传研究和疾病诊断领域的应用显著，但在多模态数据整合和实际临床应用方面面临挑战。


<details>
  <summary>Details</summary>
Motivation: 传统统计和机器学习在处理复杂高维数据方面有限，引入先进的深度学习模型（如基于transformer的大型语言模型）有助于提升诊断和教育能力。

Method: 本文通过在PubMed、bioRxiv、medRxiv和arXiv中基于自动关键词搜索的系统回顾，聚焦LLMs在遗传学诊断和教育领域的应用研究，剔除了不相关或过时的模型，共分析了172篇研究。

Result: 研究显示，transformer模型在疾病和风险分层、变异解读、医学成像分析和报告生成方面具有显著优势，但在整合多模态数据和实际临床应用中存在局限。

Conclusion: LLMs能够显著提升遗传病诊断和教育，但其推广和临床落地仍需解决多模态整合与适应性问题，为领域内研究提供了综合性评估与指引。

Abstract: Although traditional statistical techniques and machine learning methods have
contributed significantly to genetics and, in particular, inherited disease
diagnosis, they often struggle with complex, high-dimensional data, a challenge
now addressed by state-of-the-art deep learning models. Large language models
(LLMs), based on transformer architectures, have excelled in tasks requiring
contextual comprehension of unstructured medical data. This systematic review
examines the role of LLMs in the genetic research and diagnostics of both rare
and common diseases. Automated keyword-based search in PubMed, bioRxiv,
medRxiv, and arXiv was conducted, targeting studies on LLM applications in
diagnostics and education within genetics and removing irrelevant or outdated
models. A total of 172 studies were analyzed, highlighting applications in
genomic variant identification, annotation, and interpretation, as well as
medical imaging advancements through vision transformers. Key findings indicate
that while transformer-based models significantly advance disease and risk
stratification, variant interpretation, medical imaging analysis, and report
generation, major challenges persist in integrating multimodal data (genomic
sequences, imaging, and clinical records) into unified and clinically robust
pipelines, facing limitations in generalizability and practical implementation
in clinical settings. This review provides a comprehensive classification and
assessment of the current capabilities and limitations of LLMs in transforming
hereditary disease diagnostics and supporting genetic education, serving as a
guide to navigate this rapidly evolving field.

</details>


### [181] [Objective Value Change and Shape-Based Accelerated Optimization for the Neural Network Approximation](https://arxiv.org/abs/2508.20290)
*Pengcheng Xie,Zihao Zhou,Zijian Zhou*

Main category: cs.LG

TL;DR: 本文提出一个名为VC（Value Change）的新指标，用于度量神经网络逼近任务的难度及其近似影响，并支持表征局部性能的变化。


<details>
  <summary>Details</summary>
Motivation: 神经网络在关键应用中经常表现出不可预测的局部性能，这制约了其可靠性，因此需要一种量化指标来评估性能的稳定性。

Method: 引入VC指标来表征局部值的变化，并研究其理论性质，包括VC趋势和少数趋势。同时提出基于VC的新度量和预处理框架，以用于神经网络逼近任务。

Result: 实验证明VC指标能够有效分析神经网络逼近中的变化特点，并通过PDE相关科学问题和实际实验验证了VC的预处理框架对于提升性能的作用。

Conclusion: VC指标提供了分析神经网络局部性能的新视角，有助于改善其逼近任务的稳定性和表现，通过VC还发展了新的预处理加速方法，效果显著。

Abstract: This paper introduce a novel metric of an objective function f, we say VC
(value change) to measure the difficulty and approximation affection when
conducting an neural network approximation task, and it numerically supports
characterizing the local performance and behavior of neural network
approximation. Neural networks often suffer from unpredictable local
performance, which can hinder their reliability in critical applications. VC
addresses this issue by providing a quantifiable measure of local value changes
in network behavior, offering insights into the stability and performance for
achieving the neural-network approximation. We investigate some fundamental
theoretical properties of VC and identified two intriguing phenomena in neural
network approximation: the VC-tendency and the minority-tendency. These trends
respectively characterize how pointwise errors evolve in relation to the
distribution of VC during the approximation process.In addition, we propose a
novel metric based on VC, which measures the distance between two functions
from the perspective of variation. Building upon this metric, we further
propose a new preprocessing framework for neural network approximation.
Numerical results including the real-world experiment and the PDE-related
scientific problem support our discovery and pre-processing acceleration
method.

</details>


### [182] [Beacon: Post-Training Quantization with Integrated Grid Selection](https://arxiv.org/abs/2508.20293)
*Shihao Zhang,Rayan Saab*

Main category: cs.LG

TL;DR: 本文提出一种名为Beacon的简单有效算法，用于解决后训练量化（PTQ）中的缩放因子选择问题，避免了手动调节的复杂性。


<details>
  <summary>Details</summary>
Motivation: 减少大规模预训练模型的内存和计算成本，通过改进后训练量化技术的性能。

Method: 提出Beacon算法，使用固定的非缩放字母表，自动计算最佳缩放因子，并支持对称和非对称量化，而无需反向传播或大规模校准集。

Result: Beacon在无需复杂调参的情况下，实现了与现有最优方法相媲美的性能。

Conclusion: Beacon是一种简单、高效且实用的后训练量化方法，为高效模型部署提供了竞争性的解决方案。

Abstract: Quantization is a widely used compression technique for reducing the memory
and computation costs of large pre-trained models. A key challenge in
per-channel post-training quantization (PTQ) is selecting appropriate scaling
factors to replace weight values with values from a scaled quantization grid.
Existing methods typically fix the scale at the outset via heuristic tuning or
grid search. In this note, we propose Beacon, a simple and effective algorithm
that eliminates the need for such manual tuning. Beacon performs per-channel
PTQ directly using a fixed non-scaled alphabet and automatically determines the
optimal scaling factors by exploiting the geometry of symmetric scalar
quantization. It supports both symmetric and asymmetric quantization with
minimal modifications and does not rely on back-propagation or large
calibration sets. Despite its simplicity and tuning-free nature, Beacon
achieves competitive performance compared to state-of-the-art methods, making
it a practical solution for efficient model deployment.

</details>


### [183] [Dynamics-Aligned Latent Imagination in Contextual World Models for Zero-Shot Generalization](https://arxiv.org/abs/2508.20294)
*Frank Röder,Jan Benad,Manfred Eppe,Pradeep Kr. Banerjee*

Main category: cs.LG

TL;DR: DALI通过推断潜在上下文表示以解决真实环境中的强化学习适配问题。


<details>
  <summary>Details</summary>
Motivation: 解决在上下文潜在或难以测量的情况下，现有cMDP方法的局限性。

Method: 提出了DALI框架，通过自监督编码器预测前向动力学并生成可操作表征，将其融入Dreamer架构中。

Result: 在挑战性cMDP基准测试中显著优于上下文无关基线，并在外推任务中超越上下文感知基线，支持零样本泛化。

Conclusion: DALI通过动态对齐的潜在想象框架在感知和控制间架起桥梁，为强化学习提供了一种高效的上下文推断和泛化方法。

Abstract: Real-world reinforcement learning demands adaptation to unseen environmental
conditions without costly retraining. Contextual Markov Decision Processes
(cMDP) model this challenge, but existing methods often require explicit
context variables (e.g., friction, gravity), limiting their use when contexts
are latent or hard to measure. We introduce Dynamics-Aligned Latent Imagination
(DALI), a framework integrated within the Dreamer architecture that infers
latent context representations from agent-environment interactions. By training
a self-supervised encoder to predict forward dynamics, DALI generates
actionable representations conditioning the world model and policy, bridging
perception and control. We theoretically prove this encoder is essential for
efficient context inference and robust generalization. DALI's latent space
enables counterfactual consistency: Perturbing a gravity-encoding dimension
alters imagined rollouts in physically plausible ways. On challenging cMDP
benchmarks, DALI achieves significant gains over context-unaware baselines,
often surpassing context-aware baselines in extrapolation tasks, enabling
zero-shot generalization to unseen contextual variations.

</details>


### [184] [FedReFT: Federated Representation Fine-Tuning with All-But-Me Aggregation](https://arxiv.org/abs/2508.20295)
*Fatema Siddika,Md Anwar Hossen,J. Pablo Muñoz,Tanya Roosta,Anuj Sharma,Ali Jannesari*

Main category: cs.LG

TL;DR: 本文提出了一种新的联邦学习优化方法FedReFT，以表示层级的微调方式取代传统参数微调，通过稀疏层干预和新的聚合策略实现高效且语义丰富的个性化学习。


<details>
  <summary>Details</summary>
Motivation: 尽管表示微调（ReFT）在独立场景中效果良好，但其在联邦学习场景中的应用因客户端数据、模型及资源异构性面临重大挑战。

Method: 通过引入稀疏干预层对隐藏表示进行微调，结合一种名为“ABM聚合”的方法避免任务异构性带来的语义错位。

Result: FedReFT在常识推理、算术推理、指令调优以及GLUE任务上均优于现有联邦学习参数微调方法，在参数效率上比LoRA方法高7到15倍。

Conclusion: FedReFT提供了一种高效、轻量化的联邦学习解决方案，能够在保持全局知识和本地任务间实现平衡且个性化的学习。

Abstract: Parameter-efficient fine-tuning (PEFT) has attracted significant attention
for adapting large pre-trained models by modifying a small subset of
parameters. Recently, Representation Fine-tuning (ReFT) has emerged as an
effective alternative. ReFT shifts the fine-tuning paradigm from updating model
weights to directly manipulating hidden representations that capture rich
semantic information, and performs better than state-of-the-art PEFTs in
standalone settings. However, its application in Federated Learning (FL)
remains challenging due to heterogeneity in clients' data distributions, model
capacities, and computational resources. To address these challenges, we
introduce Federated Representation Fine-Tuning (FedReFT), a novel approach to
fine-tune the client's hidden representation. FedReFT applies sparse
intervention layers to steer hidden representations directly, offering a
lightweight and semantically rich fine-tuning alternative ideal for edge
devices. However, representation-level updates are especially vulnerable to
aggregation mismatch under different task heterogeneity, where naive averaging
can corrupt semantic alignment. To mitigate this issue, we propose All-But-Me
(ABM) aggregation, where each client receives the aggregated updates of others
and partially incorporates them, enabling stable and personalized learning by
balancing local focus with global knowledge. We evaluate FedReFT on commonsense
reasoning, arithmetic reasoning, instruction-tuning, and GLUE, where it
consistently outperforms state-of-the-art PEFT methods in FL, achieving 7x-15x
higher parameter efficiency compared to leading LoRA-based approaches.

</details>


### [185] [Multi-Agent Reinforcement Learning in Intelligent Transportation Systems: A Comprehensive Survey](https://arxiv.org/abs/2508.20315)
*RexCharles Donatus,Kumater Ter,Ore-Ofe Ajayi,Daniel Udekwe*

Main category: cs.LG

TL;DR: 本文综述了多智能体强化学习（MARL）在智能交通系统（ITS）中的应用，包括协调模型、学习算法分类及关键应用领域等，分析了主要挑战和实验平台。


<details>
  <summary>Details</summary>
Motivation: 随着城市交通复杂性增加以及对高效、可持续、适应性解决方案的需求，研究者关注于运用MARL在动态、不确定、多智能体的情境下实现自主决策优化。

Method: 提出了一种分类法，将MARL方法按协调模型与学习算法（基于值、基于策略、Actor-Critic等）进行归类，并分析其在交通信号控制、自动驾驶车协调、物流优化等领域的应用。

Result: 总结MARL在ITS中的核心应用及使用的主流仿真平台（SUMO、CARLA等），并阐明阻碍其实际应用的主要挑战（如扩展性和仿真到真实的转移问题）。

Conclusion: MARL在ITS中具有重要潜力，但其在实际部署中仍面临诸多困难，需继续研究改进。

Abstract: The growing complexity of urban mobility and the demand for efficient,
sustainable, and adaptive solutions have positioned Intelligent Transportation
Systems (ITS) at the forefront of modern infrastructure innovation. At the core
of ITS lies the challenge of autonomous decision-making across dynamic, large
scale, and uncertain environments where multiple agents traffic signals,
autonomous vehicles, or fleet units must coordinate effectively. Multi Agent
Reinforcement Learning (MARL) offers a promising paradigm for addressing these
challenges by enabling distributed agents to jointly learn optimal strategies
that balance individual objectives with system wide efficiency. This paper
presents a comprehensive survey of MARL applications in ITS. We introduce a
structured taxonomy that categorizes MARL approaches according to coordination
models and learning algorithms, spanning value based, policy based, actor
critic, and communication enhanced frameworks. Applications are reviewed across
key ITS domains, including traffic signal control, connected and autonomous
vehicle coordination, logistics optimization, and mobility on demand systems.
Furthermore, we highlight widely used simulation platforms such as SUMO, CARLA,
and CityFlow that support MARL experimentation, along with emerging benchmarks.
The survey also identifies core challenges, including scalability, non
stationarity, credit assignment, communication constraints, and the sim to real
transfer gap, which continue to hinder real world deployment.

</details>


### [186] [Multi-View Graph Convolution Network for Internal Talent Recommendation Based on Enterprise Emails](https://arxiv.org/abs/2508.20328)
*Soo Hyun Kim,Jang-Hyun Kim*

Main category: cs.LG

TL;DR: 提出一种内部人才推荐框架，通过电子邮件数据建模员工在工作任务和协作模式上的匹配度(WHAT和HOW)，结合双图卷积网络和门机制，实验显示显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统内部人才推荐方式局限于少数管理者视角，可能忽略合适候选人，需有效解决这一问题。

Method: 通过电子邮件数据，从任务语义相似度(WHAT)和协作互动特性(HOW)两个维度为候选人建模，设计了结合双图卷积网络和门机制的框架。

Result: 实验中，该模型在Hit@100指标上达到40.9%，高于其他融合策略及基线方法，并且具备高解释性，可适应不同工种的灵活融合策略。

Conclusion: 该研究构建了一种量化的内部人才发现框架，优化了任务和协作的融合比率，提高推荐精准性并减少遗漏，对实际应用意义重大。

Abstract: Internal talent recommendation is a critical strategy for organizational
continuity, yet conventional approaches suffer from structural limitations,
often overlooking qualified candidates by relying on the narrow perspective of
a few managers. To address this challenge, we propose a novel framework that
models two distinct dimensions of an employee's position fit from email data:
WHAT they do (semantic similarity of tasks) and HOW they work (structural
characteristics of their interactions and collaborations). These dimensions are
represented as independent graphs and adaptively fused using a Dual Graph
Convolutional Network (GCN) with a gating mechanism. Experiments show that our
proposed gating-based fusion model significantly outperforms other fusion
strategies and a heuristic baseline, achieving a top performance of 40.9% on
Hit@100. Importantly, it is worth noting that the model demonstrates high
interpretability by learning distinct, context-aware fusion strategies for
different job families. For example, it learned to prioritize relational (HOW)
data for 'sales and marketing' job families while applying a balanced approach
for 'research' job families. This research offers a quantitative and
comprehensive framework for internal talent discovery, minimizing the risk of
candidate omission inherent in traditional methods. Its primary contribution
lies in its ability to empirically determine the optimal fusion ratio between
task alignment (WHAT) and collaborative patterns (HOW), which is required for
employees to succeed in the new positions, thereby offering important practical
implications.

</details>


### [187] [FORGE: Foundational Optimization Representations from Graph Embeddings](https://arxiv.org/abs/2508.20330)
*Zohair Shafi,Serdar Kadioglu*

Main category: cs.LG

TL;DR: 本文提出了Forge，一种无需依赖问题解的无监督方法，预训练向量量化图自动编码器，并通过实验验证了其在监督和非监督环境下的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法训练成本高，模型仅适用于特定问题，缺乏通用性；需要提升组合优化问题求解效率。

Method: 开发了Forge模型，对混合整数规划实例在无监督环境中进行向量量化图自动编码器的预训练，生成离散编码表示优化实例。

Result: Forge在非监督设置下能有效区分和聚类未见实例；在监督设置下，单一模型在不同问题分布上进行微调后，能预测优化变量和切割生成的整数缺口并提升优化器性能。

Conclusion: Forge方法具有通用性和实用性，在组合优化问题的求解中实现性能提升，促进相关研究。

Abstract: Combinatorial optimization problems are ubiquitous in science and
engineering, yet learning-based approaches to accelerate their solution often
require solving a large number of hard-to-solve optimization instances to
collect training data, incurring significant computational overhead. Existing
methods require training dedicated models for each problem distribution for
each downstream task, severely limiting their scalability and generalization.
In this work, we introduce Forge, a method of pre-training a vector-quantized
graph autoencoder on a large and diverse collection of mixed-integer
programming (MIP) instances in an unsupervised fashion without dependency on
their solution. The vector quantization process creates discrete code
assignments that act as a vocabulary to represent optimization instances. We
evaluate our approach under both supervised and unsupervised settings. For the
unsupervised setting, we demonstrate that Forge embeddings effectively
differentiate and cluster unseen instances. For the supervised setting, we
fine-tune Forge embeddings and show that a single model predicts both the
variables for warm-starts and integrality gaps for cut-generation across
multiple problem type distributions. Both predictions help improve performance
of a state-of-the-art, commercial optimization solver. Finally, we release our
code and pre-trained Forge weights to encourage further research and practical
use of instance-level MIP embeddings at https://github.com/skadio/forge/

</details>


### [188] [Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs](https://arxiv.org/abs/2508.20333)
*Md Abdullah Al Mamun,Ihsen Alouani,Nael Abu-Ghazaleh*

Main category: cs.LG

TL;DR: 该论文研究了如何通过恶意攻击利用大语言模型的伦理对齐机制，注入偏见或实现有针对性的审查，而不会影响模型对非相关话题的响应性。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型对齐机制的脆弱性，并揭示其对道德与安全性威胁的潜在利用方式。

Method: 提出了一种名为Subversive Alignment Injection (SAI)的攻击方法，通过对齐机制的过度开发触发针对特定话题的拒答行为，测试该方法对不同防御策略的有效性。

Result: 实验表明，SAI攻防技术可成功规避最新的中毒检测方法，并在实际应用中造成显著偏见，如在医疗或简历筛选情景中引入目标性限制。

Conclusion: 论文揭示了大语言模型对齐机制的潜在安全漏洞和风险，提示需要加强模型的公平性与鲁棒性防御。

Abstract: Large Language Models (LLMs) are aligned to meet ethical standards and safety
requirements by training them to refuse answering harmful or unsafe prompts. In
this paper, we demonstrate how adversaries can exploit LLMs' alignment to
implant bias, or enforce targeted censorship without degrading the model's
responsiveness to unrelated topics. Specifically, we propose Subversive
Alignment Injection (SAI), a poisoning attack that leverages the alignment
mechanism to trigger refusal on specific topics or queries predefined by the
adversary. Although it is perhaps not surprising that refusal can be induced
through overalignment, we demonstrate how this refusal can be exploited to
inject bias into the model. Surprisingly, SAI evades state-of-the-art poisoning
defenses including LLM state forensics, as well as robust aggregation
techniques that are designed to detect poisoning in FL settings. We demonstrate
the practical dangers of this attack by illustrating its end-to-end impacts on
LLM-powered application pipelines. For chat based applications such as
ChatDoctor, with 1% data poisoning, the system refuses to answer healthcare
questions to targeted racial category leading to high bias ($\Delta DP$ of
23%). We also show that bias can be induced in other NLP tasks: for a resume
selection pipeline aligned to refuse to summarize CVs from a selected
university, high bias in selection ($\Delta DP$ of 27%) results. Even higher
bias ($\Delta DP$~38%) results on 9 other chat based downstream applications.

</details>


### [189] [Dynamic Synthetic Controls vs. Panel-Aware Double Machine Learning for Geo-Level Marketing Impact Estimation](https://arxiv.org/abs/2508.20335)
*Sang Su Lee,Vineeth Loganathan,Vijay Raghavan*

Main category: cs.LG

TL;DR: 本文提出了一个开源仿真器以研究两种主流地理营销提升的量化方法（SCM 和 DML）在五种典型测试条件下的表现，并评价了七种估算器的表现。研究表明，DML 方法在复杂场景下表现更为可靠。


<details>
  <summary>Details</summary>
Motivation: 在双边市场中，量化地理营销的提升效果具有挑战性，目前的 SCM 方法虽然功效高但效果估计存在系统性偏差，而 DML 方法鲜少与 SCM 对比研究。

Method: 研究构建了一个开源仿真器，允许用户调整关键参数，并在复杂场景下对多种估算器进行对比分析。

Result: 实验显示，DML 系列模型在复杂情况如非线性或外部冲击时，显著减少偏差并恢复正常置信区间覆盖，而 ASC 模型则表现出严重偏差。

Conclusion: ASC 方法作为基线方法简单易用，但在复杂场景中不可靠。建议采用‘先诊断后选择’框架，根据具体业务挑战选择适合的 DML 模型来进行地理实验分析。

Abstract: Accurately quantifying geo-level marketing lift in two-sided marketplaces is
challenging: the Synthetic Control Method (SCM) often exhibits high power yet
systematically under-estimates effect size, while panel-style Double Machine
Learning (DML) is seldom benchmarked against SCM. We build an open, fully
documented simulator that mimics a typical large-scale geo roll-out: N_unit
regional markets are tracked for T_pre weeks before launch and for a further
T_post-week campaign window, allowing all key parameters to be varied by the
user and probe both families under five stylized stress tests: 1) curved
baseline trends, 2) heterogeneous response lags, 3) treated-biased shocks, 4) a
non-linear outcome link, and 5) a drifting control group trend.
  Seven estimators are evaluated: three standard Augmented SCM (ASC) variants
and four panel-DML flavors (TWFE, CRE/Mundlak, first-difference, and
within-group). Across 100 replications per scenario, ASC models consistently
demonstrate severe bias and near-zero coverage in challenging scenarios
involving nonlinearities or external shocks. By contrast, panel-DML variants
dramatically reduce this bias and restore nominal 95%-CI coverage, proving far
more robust.
  The results indicate that while ASC provides a simple baseline, it is
unreliable in common, complex situations. We therefore propose a
'diagnose-first' framework where practitioners first identify the primary
business challenge (e.g., nonlinear trends, response lags) and then select the
specific DML model best suited for that scenario, providing a more robust and
reliable blueprint for analyzing geo-experiments.

</details>


### [190] [Adaptive Segmentation of EEG for Machine Learning Applications](https://arxiv.org/abs/2508.20336)
*Johnson Zhou,Joseph West,Krista A. Ehinger,Zhenming Ren,Sam E. John,David B. Grayden*

Main category: cs.LG

TL;DR: 研究一种新型EEG信号的自适应分段方法，提升机器学习中癫痫检测的性能。


<details>
  <summary>Details</summary>
Motivation: 固定时间片的信号划分可能与大脑的真实状态不符，且限制了生物学相关性，因此需要一种更适应信号特性的分段方法。

Method: 提出了一种名为CTXSEG的自适应分段方法，基于EEG信号的统计差异生成变长片段，并开发信号生成器CTXGEN来验证。通过癫痫检测案例比较了CTXSEG和固定长度分段方法的有效性。

Result: 使用CTXSEG后无需修改机器学习方法就可以提升癫痫检测性能，同时减少了划分的片段数量。

Conclusion: CTXSEG是一种有效的自适应分段方法，适合整合到EEG机器学习预处理中，可以提高信号预处理效果，应被视为标准工具的一部分。

Abstract: Objective. Electroencephalography (EEG) data is derived by sampling
continuous neurological time series signals. In order to prepare EEG signals
for machine learning, the signal must be divided into manageable segments. The
current naive approach uses arbitrary fixed time slices, which may have limited
biological relevance because brain states are not confined to fixed intervals.
We investigate whether adaptive segmentation methods are beneficial for machine
learning EEG analysis.
  Approach. We introduce a novel adaptive segmentation method, CTXSEG, that
creates variable-length segments based on statistical differences in the EEG
data and propose ways to use them with modern machine learning approaches that
typically require fixed-length input. We assess CTXSEG using controllable
synthetic data generated by our novel signal generator CTXGEN. While our CTXSEG
method has general utility, we validate it on a real-world use case by applying
it to an EEG seizure detection problem. We compare the performance of CTXSEG
with fixed-length segmentation in the preprocessing step of a typical EEG
machine learning pipeline for seizure detection.
  Main results. We found that using CTXSEG to prepare EEG data improves seizure
detection performance compared to fixed-length approaches when evaluated using
a standardized framework, without modifying the machine learning method, and
requires fewer segments.
  Significance. This work demonstrates that adaptive segmentation with CTXSEG
can be readily applied to modern machine learning approaches, with potential to
improve performance. It is a promising alternative to fixed-length segmentation
for signal preprocessing and should be considered as part of the standard
preprocessing repertoire in EEG machine learning applications.

</details>


### [191] [Understanding Incremental Learning with Closed-form Solution to Gradient Flow on Overparamerterized Matrix Factorization](https://arxiv.org/abs/2508.20344)
*Hancheng Min,René Vidal*

Main category: cs.LG

TL;DR: 本文通过解析梯度流在对称矩阵分解问题上的行为，揭示了其背后的增量学习现象。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络训练中的隐式偏置如何通过初始条件与优化算法的交互引发增量学习，以及这种现象背后的时间尺度分离机制。

Method: 分析梯度流在对称矩阵分解问题中的表现，以求解Riccati型矩阵微分方程的闭式解形式，定量解释增量学习行为。

Result: 发现通过减小初始化规模，可以增强时间尺度分离，逐步获得目标矩阵的低秩近似解。

Conclusion: 增量学习现象在时间尺度分离的影响下得以显现，方法或可推广至处理非对称矩阵分解问题。

Abstract: Many theoretical studies on neural networks attribute their excellent
empirical performance to the implicit bias or regularization induced by
first-order optimization algorithms when training networks under certain
initialization assumptions. One example is the incremental learning phenomenon
in gradient flow (GF) on an overparamerterized matrix factorization problem
with small initialization: GF learns a target matrix by sequentially learning
its singular values in decreasing order of magnitude over time. In this paper,
we develop a quantitative understanding of this incremental learning behavior
for GF on the symmetric matrix factorization problem, using its closed-form
solution obtained by solving a Riccati-like matrix differential equation. We
show that incremental learning emerges from some time-scale separation among
dynamics corresponding to learning different components in the target matrix.
By decreasing the initialization scale, these time-scale separations become
more prominent, allowing one to find low-rank approximations of the target
matrix. Lastly, we discuss the possible avenues for extending this analysis to
asymmetric matrix factorization problems.

</details>


### [192] [DFAMS: Dynamic-flow guided Federated Alignment based Multi-prototype Search](https://arxiv.org/abs/2508.20353)
*Zhibang Yang,Xinke Jiang,Rihong Qiu,Ruiqing Li,Yihang Zhang,Yue Fang,Yongxin Xu,Hongxin Ding,Xu Chu,Junfeng Zhao,Yasha Wang*

Main category: cs.LG

TL;DR: DFAMS利用动态信息流（DIF）框架来优化联邦检索（FR）任务，在五个基准测试中显著提升了知识分类精度、检索召回率和下游QA准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦检索方法在处理多义查询，尤其是跨领域场景中，难以检索到高质量相关文档，从而限制了其在支持生成任务中的有效性。

Method: 提出了一种名为DFAMS的框架，通过利用LLMs中的动态信息流（DIF）来识别潜在查询意图，构建语义对齐的知识分区，实现异构来源的精确检索。具体方法包括：结合少量标注查询的梯度信号及Shapley值归因，追踪与意图识别和子领域边界检测相关的神经元激活路径；并采用多原型对比学习进行语义对齐模块训练。

Result: 实验结果在五个基准数据集上显示，DFAMS在知识分类精度上提高了14.37%、检索召回率上提高了5.38%、下游QA任务准确性上提高了6.45%。

Conclusion: DFAMS在处理复杂联邦检索场景中表现出色，显著提高了任务的整体性能。

Abstract: Federated Retrieval (FR) routes queries across multiple external knowledge
sources, to mitigate hallucinations of LLMs, when necessary external knowledge
is distributed. However, existing methods struggle to retrieve high-quality and
relevant documents for ambiguous queries, especially in cross-domain scenarios,
which significantly limits their effectiveness in supporting downstream
generation tasks. Inspired by dynamic information flow (DIF), we propose DFAMS,
a novel framework that leverages DIF to identify latent query intents and
construct semantically aligned knowledge partitions for accurate retrieval
across heterogeneous sources. Specifically, DFAMS probes the DIF in LLMs by
leveraging gradient signals from a few annotated queries and employing Shapley
value-based attribution to trace neuron activation paths associated with intent
recognition and subdomain boundary detection. Then, DFAMS leverages DIF to
train an alignment module via multi-prototype contrastive learning, enabling
fine-grained intra-source modeling and inter-source semantic alignment across
knowledge bases. Experimental results across five benchmarks show that DFAMS
outperforms advanced FR methods by up to 14.37% in knowledge classification
accuracy, 5.38% in retrieval recall, and 6.45% in downstream QA accuracy,
demonstrating its effectiveness in complex FR scenarios.

</details>


### [193] [Developing a Multi-Modal Machine Learning Model For Predicting Performance of Automotive Hood Frames](https://arxiv.org/abs/2508.20358)
*Abhishek Indupally,Satchit Ramnath*

Main category: cs.LG

TL;DR: 开发了一种多模态机器学习（MMML）架构，用于在不依赖昂贵模拟的情况下预测引擎盖框架几何的性能指标，优化工程设计过程。


<details>
  <summary>Details</summary>
Motivation: 希望设计师可以无需花费大量时间在模拟设置上，就能评估引擎盖框架几何的性能。

Method: 提出了一种MMML架构，结合同一数据的不同模态，通过学习预测性能指标，从而提升设计探索效率并减少对昂贵模拟的依赖。

Result: MMML架构表现优于传统单模态方法，并成功预测未在训练集中的新框架几何的性能，展现了其生成能力。

Conclusion: MMML展现了优化结构开发与加速设计流程的潜力，推动了多模态方法在工程设计中的应用，为机器学习在实际工程应用中架起了桥梁。

Abstract: Is there a way for a designer to evaluate the performance of a given hood
frame geometry without spending significant time on simulation setup? This
paper seeks to address this challenge by developing a multimodal
machine-learning (MMML) architecture that learns from different modalities of
the same data to predict performance metrics. It also aims to use the MMML
architecture to enhance the efficiency of engineering design processes by
reducing reliance on computationally expensive simulations. The proposed
architecture accelerates design exploration, enabling rapid iteration while
maintaining high-performance standards, especially in the concept design phase.
The study also presents results that show that by combining multiple data
modalities, MMML outperforms traditional single-modality approaches. Two new
frame geometries, not part of the training dataset, are also used for
prediction using the trained MMML model to showcase the ability to generalize
to unseen frame models. The findings underscore MMML's potential in
supplementing traditional simulation-based workflows, particularly in the
conceptual design phase, and highlight its role in bridging the gap between
machine learning and real-world engineering applications. This research paves
the way for the broader adoption of machine learning techniques in engineering
design, with a focus on refining multimodal approaches to optimize structural
development and accelerate the design cycle.

</details>


### [194] [BiListing: Modality Alignment for Listings](https://arxiv.org/abs/2508.20396)
*Guillaume Guy,Mihajlo Grbovic,Chun How Tan,Han Zhao*

Main category: cs.LG

TL;DR: 提出BiListing方法，将Airbnb房源的文字与图片整合为单一表示，提升搜索与推荐功能，取得显著商业效果。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以从文字与图片中提取有意义信息，倾向于依赖结构化数据，随着表示学习发展，为整合非结构化数据带来了新机遇。

Method: 提出BiListing方法，利用大语言模型和预训练的文本-图像模型，将房源文字与图片对齐，并整合为单一向量表示，同时实现单一模式或跨模式检索功能。

Result: 该方法成功在Airbnb搜索排名模型中应用，在线测试中获得0.425%的NDCB提升，创造了数千万美元的额外收入。

Conclusion: BiListing模型整合文字与图像数据，有效提升了Airbnb的搜索与推荐能力，克服了冷启动问题，表现优异并实现商业化落地。

Abstract: Airbnb is a leader in offering travel accommodations. Airbnb has historically
relied on structured data to understand, rank, and recommend listings to guests
due to the limited capabilities and associated complexity arising from
extracting meaningful information from text and images. With the rise of
representation learning, leveraging rich information from text and photos has
become easier. A popular approach has been to create embeddings for text
documents and images to enable use cases of computing similarities between
listings or using embeddings as features in an ML model.
  However, an Airbnb listing has diverse unstructured data: multiple images,
various unstructured text documents such as title, description, and reviews,
making this approach challenging. Specifically, it is a non-trivial task to
combine multiple embeddings of different pieces of information to reach a
single representation.
  This paper proposes BiListing, for Bimodal Listing, an approach to align text
and photos of a listing by leveraging large-language models and pretrained
language-image models. The BiListing approach has several favorable
characteristics: capturing unstructured data into a single embedding vector per
listing and modality, enabling zero-shot capability to search inventory
efficiently in user-friendly semantics, overcoming the cold start problem, and
enabling listing-to-listing search along a single modality, or both.
  We conducted offline and online tests to leverage the BiListing embeddings in
the Airbnb search ranking model, and successfully deployed it in production,
achieved 0.425% of NDCB gain, and drove tens of millions in incremental
revenue.

</details>


### [195] [TF-TransUNet1D: Time-Frequency Guided Transformer U-Net for Robust ECG Denoising in Digital Twin](https://arxiv.org/abs/2508.20398)
*Shijie Wang,Lei Li*

Main category: cs.LG

TL;DR: 提出了TF-TransUNet1D，一种用于去噪心电图（ECG）信号的1D深度神经网络，通过U-Net架构与Transformer编码器结合，提升诊断信号的完整性。


<details>
  <summary>Details</summary>
Motivation: 现有心电信号经常受到噪声干扰，削弱诊断价值，需要更高效的去噪方法保障信号完整性。

Method: 设计了结合U-Net和Transformer的网络架构，并引入时域与频域双重损失函数，优化信号重建并减少高频噪声。

Result: 在MIT-BIH数据库和NSTDB中验证了模型的优越性能，SNR提升显著，绝对误差为0.1285，相关系数为0.9540。

Conclusion: 通过高精度信号去噪，TF-TransUNet1D为心脏数字孪生处理提供了可靠的预处理方法，实现更准确的实时监测和个性化建模。

Abstract: Electrocardiogram (ECG) signals serve as a foundational data source for
cardiac digital twins, yet their diagnostic utility is frequently compromised
by noise and artifacts. To address this issue, we propose TF-TransUNet1D, a
novel one-dimensional deep neural network that integrates a U-Net-based
encoder-decoder architecture with a Transformer encoder, guided by a hybrid
time-frequency domain loss. The model is designed to simultaneously capture
local morphological features and long-range temporal dependencies, which are
critical for preserving the diagnostic integrity of ECG signals. To enhance
denoising robustness, we introduce a dual-domain loss function that jointly
optimizes waveform reconstruction in the time domain and spectral fidelity in
the frequency domain. In particular, the frequency-domain component effectively
suppresses high-frequency noise while maintaining the spectral structure of the
signal, enabling recovery of subtle but clinically significant waveform
components. We evaluate TF-TransUNet1D using synthetically corrupted signals
from the MIT-BIH Arrhythmia Database and the Noise Stress Test Database
(NSTDB). Comparative experiments against state-of-the-art baselines demonstrate
consistent superiority of our model in terms of SNR improvement and error
metrics, achieving a mean absolute error of 0.1285 and Pearson correlation
coefficient of 0.9540. By delivering high-precision denoising, this work
bridges a critical gap in pre-processing pipelines for cardiac digital twins,
enabling more reliable real-time monitoring and personalized modeling.

</details>


### [196] [Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full Context-Aware Linear Attention](https://arxiv.org/abs/2508.20407)
*Zhongpan Tang*

Main category: cs.LG

TL;DR: 本文提出了一种新的线性注意力架构TLinFormer，通过重新配置神经元连接模式，实现了线性复杂度，同时保留了精确的注意力评分和完整的历史上下文信息流。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer模型核心自注意力机制在处理长序列任务时因复杂度瓶颈受限，因此需要一种既能改善复杂度又不牺牲模型性能的解决方案。

Method: 通过重新配置神经网络的连接模式，从信息流的拓扑结构出发，设计了一种新的线性注意力机制TLinFormer，实现了线性复杂度及精确的注意力计算。

Result: 实验表明，与标准的Transformer基线相比，TLinFormer在推理延迟、KV缓存效率、内存占用和总体速度提升等指标上具有显著优势。

Conclusion: TLinFormer能够在长序列推理任务中以线性复杂度提供优异的性能，解决了现有高效注意力方法与标准注意力之间的性能差距问题。

Abstract: The Transformer architecture has become a cornerstone of modern artificial
intelligence, but its core self-attention mechanism suffers from a complexity
bottleneck that scales quadratically with sequence length, severely limiting
its application in long-sequence tasks. To address this challenge, existing
linear attention methods typically sacrifice model performance by relying on
data-agnostic kernel approximations or restrictive context selection. This
paper returns to the first principles of connectionism, starting from the
topological structure of information flow, to introduce a novel linear
attention architecture-\textbf{TLinFormer}. By reconfiguring neuron connection
patterns, TLinFormer achieves strict linear complexity while computing exact
attention scores and ensuring information flow remains aware of the full
historical context. This design aims to bridge the performance gap prevalent
between existing efficient attention methods and standard attention. Through a
series of experiments, we systematically evaluate the performance of TLinFormer
against a standard Transformer baseline on long-sequence inference tasks. The
results demonstrate that TLinFormer exhibits overwhelming advantages in key
metrics such as \textbf{inference latency}, \textbf{KV cache efficiency},
\textbf{memory footprint}, and \textbf{overall speedup}.

</details>


### [197] [Assessing local deformation and computing scalar curvature with nonlinear conformal regularization of decoders](https://arxiv.org/abs/2508.20413)
*Benjamin Couéraud,Vikram Sunkara,Christof Schütte*

Main category: cs.LG

TL;DR: 本文提出了利用非线性共形正则化的几何约束方案，以改进基于深度学习的解码映射方法。


<details>
  <summary>Details</summary>
Motivation: 旨在通过非线性共形正则化来控制高维数据的解码映射中的局部变形，实现低维流形的更精准学习。

Method: 结合深度神经网络引入非线性共形正则化技术，同时定义了共形因子作为局部解码变形的指标，并计算了学习流形的标量曲率。

Result: 在Swiss roll和CelebA数据集上的实验展示了提出方法如何获取流形解码相关特性。

Conclusion: 新方法通过引入几何正则化改善了解码映射，增强了对高维数据的维度降低和流形学习能力。

Abstract: One aim of dimensionality reduction is to discover the main factors that
explain the data, and as such is paramount to many applications. When working
with high dimensional data, autoencoders offer a simple yet effective approach
to learn low-dimensional representations. The two components of a general
autoencoder consist first of an encoder that maps the observed data onto a
latent space; and second a decoder that maps the latent space back to the
original observation space, which allows to learn a low-dimensional manifold
representation of the original data. In this article, we introduce a new type
of geometric regularization for decoding maps approximated by deep neural
networks, namely nonlinear conformal regularization. This regularization
procedure permits local variations of the decoder map and comes with a new
scalar field called conformal factor which acts as a quantitative indicator of
the amount of local deformation sustained by the latent space when mapped into
the original data space. We also show that this regularization technique allows
the computation of the scalar curvature of the learned manifold. Implementation
and experiments on the Swiss roll and CelebA datasets are performed to
illustrate how to obtain these quantities from the architecture.

</details>


### [198] [On Identifying Why and When Foundation Models Perform Well on Time-Series Forecasting Using Automated Explanations and Rating](https://arxiv.org/abs/2508.20437)
*Michael Widener,Kausik Lakkaraju,John Aydin,Biplav Srivastava*

Main category: cs.LG

TL;DR: 本文比较了4种时间序列预测模型（ARIMA, Gradient Boosting, Chronos, Llama）在不同领域下的性能。发现特征工程模型在数据波动较大的领域表现更好，而基础模型在稳定环境更优。


<details>
  <summary>Details</summary>
Motivation: 探究时间序列预测模型在何种条件下表现优异，及如何解释其行为以减少用户对模型结果的误用。

Method: 结合传统可解释AI方法与评分驱动解释（RDE），在四个异构数据集上评估模型表现与可解释性，分析四种模型架构的优劣。

Result: 特征工程模型（如Gradient Boosting）在波动或稀疏领域（如电力、汽车零件销售）性能突出且解释性更强；基础模型（如Chronos）在稳定或趋势驱动领域（如金融）表现更优。

Conclusion: 研究表明，不同类型的时间序列模型在不同应用场景下具有各自优势，建议根据场景选择合适模型，同时注重输出的可解释性。

Abstract: Time-series forecasting models (TSFM) have evolved from classical statistical
methods to sophisticated foundation models, yet understanding why and when
these models succeed or fail remains challenging. Despite this known
limitation, time series forecasting models are increasingly used to generate
information that informs real-world actions with equally real consequences.
Understanding the complexity, performance variability, and opaque nature of
these models then becomes a valuable endeavor to combat serious concerns about
how users should interact with and rely on these models' outputs. This work
addresses these concerns by combining traditional explainable AI (XAI) methods
with Rating Driven Explanations (RDE) to assess TSFM performance and
interpretability across diverse domains and use cases. We evaluate four
distinct model architectures: ARIMA, Gradient Boosting, Chronos (time-series
specific foundation model), Llama (general-purpose; both fine-tuned and base
models) on four heterogeneous datasets spanning finance, energy,
transportation, and automotive sales domains. In doing so, we demonstrate that
feature-engineered models (e.g., Gradient Boosting) consistently outperform
foundation models (e.g., Chronos) in volatile or sparse domains (e.g., power,
car parts) while providing more interpretable explanations, whereas foundation
models excel only in stable or trend-driven contexts (e.g., finance).

</details>


### [199] [Uncovering the Spectral Bias in Diagonal State Space Models](https://arxiv.org/abs/2508.20441)
*Ruben Solozabal,Velibor Bojkovic,Hilal AlQuabeh,Kentaro Inui,Martin Takáč*

Main category: cs.LG

TL;DR: 传统的状态空间模型初始化方法依赖HiPPO框架，但本文从频率的角度研究了对角状态空间模型的初始化，提出了一种新的对角初始化方法S4D-DFouT，并在多个基准测试中取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 现有HiPPO框架无法全面理解对角变体的作用，研究其频率视角的初始化规律可以提高模型性能。

Method: 从频率的角度分析对角状态空间模型的初始化，提出一种基于离散傅立叶域的初始化方法S4D-DFouT。

Result: 新方法S4D-DFouT在长程任务和大规模数据集如PathX-256上均实现了性能提升并达到领先水平。

Conclusion: 通过提出S4D-DFouT方法，本文有效增强了对角状态空间模型的初始化效率及表现，并揭示了学习偏差的内在机制。

Abstract: Current methods for initializing state space models (SSMs) parameters mainly
rely on the \textit{HiPPO framework}, which is based on an online approximation
of orthogonal polynomials. Recently, diagonal alternatives have shown to reach
a similar level of performance while being significantly more efficient due to
the simplification in the kernel computation. However, the \textit{HiPPO
framework} does not explicitly study the role of its diagonal variants. In this
paper, we take a further step to investigate the role of diagonal SSM
initialization schemes from the frequency perspective. Our work seeks to
systematically understand how to parameterize these models and uncover the
learning biases inherent in such diagonal state-space models. Based on our
observations, we propose a diagonal initialization on the discrete Fourier
domain \textit{S4D-DFouT}. The insights in the role of pole placing in the
initialization enable us to further scale them and achieve state-of-the-art
results on the Long Range Arena benchmark, allowing us to train from scratch on
very large datasets as PathX-256.

</details>


### [200] [Towards Mitigating Excessive Forgetting in LLM Unlearning via Entanglement-Aware Unlearning with Proxy Constraint](https://arxiv.org/abs/2508.20443)
*Zhihao Liu,Jian Lou,Yuke Hu,Xiaochen Li,Tailun Chen,Yitian Chen,Zhan Qin*

Main category: cs.LG

TL;DR: 本文提出了一种名为EAGLE-PC的机器学习忘记框架，以更有效地从大型语言模型中移除特定数据的影响，同时在忘记与实用性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 随着隐私和数据所有权问题的日益突出，数据所有者可能要求从已训练的模型中移除他们的数据。然而，现有的机器学习忘记方法通常存在忘记边界不明确的问题，导致某些数据未被充分遗忘，仍然存在泄露风险，而另一些则遗忘过度，影响模型性能。

Method: EAGLE-PC框架采用了两种关键技术：首先，通过测量样本与保留样本在嵌入空间中的相似性，实现了基于纠缠感知的损失重加权，从而针对性地优化遗忘过程。其次，利用基于ICL生成的测试数据进行代理约束，以缓解遗忘过程中的过度遗忘问题。

Result: 在TOFU和MUSE基准上，EAGLE-PC在多种大型语言模型中实现了遗忘与实用性之间的更佳平衡，并结合NPO+GD优化器，接近完全重训练的效果。

Conclusion: EAGLE-PC作为一种兼容现有基于梯度的目标方法的可插拔增强工具，为模型遗忘提供了一种可扩展且鲁棒的解决方案。

Abstract: Large language models (LLMs) are trained on massive datasets that may include
private or copyrighted content. Due to growing privacy and ownership concerns,
data owners may request the removal of their data from trained models. Machine
unlearning provides a practical solution by removing the influence of specific
data without full retraining. However, most existing methods lack a sound
forgetting boundary, causing some samples to be under-forgotten, leaving
residual leakage risks, while others remain over-forgotten at the expense of
degraded utility.
  In this work, we propose EAGLE-PC (Entanglement-Awareness Guided Loss
Reweighting with Proxy Constraint), a novel unlearning framework that addresses
these limitations through two key components. First, entanglement-awareness
guided loss reweighting determines the forgetting effort of each sample by
measuring its similarity to retain samples in the embedding space, enabling
more targeted and effective unlearning. Second, a proxy constraint leveraging
ICL (In-Context Learning) generated test data softly regularizes the forgetting
process, effectively mitigating over-forgetting. EAGLE-PC is compatible with
existing gradient-based objectives and serves as a plug-and-play enhancement.
We evaluate EAGLE-PC on the TOFU and MUSE benchmarks, showing consistent
improvements in the forgetting-utility trade-off across multiple LLMs. Combined
with the NPO+GD optimizer, it approaches full retraining performance, offering
a scalable and robust unlearning solution.

</details>


### [201] [Evaluating Differentially Private Generation of Domain-Specific Text](https://arxiv.org/abs/2508.20452)
*Yidan Sun,Viktor Schlegel,Srinivasan Nandakumar,Iqra Zahid,Yuping Wu,Warren Del-Pinto,Goran Nenadic,Siew-Kei Lam,Jie Zhang,Anil A Bharath*

Main category: cs.LG

TL;DR: 本文讨论了如何在满足差分隐私(DP)的条件下，生成具有高效用性和真实性的文本数据，并提出了一个统一的基准来评估这些数据的性能。


<details>
  <summary>Details</summary>
Motivation: 当前高风险领域（如医疗和金融）对生成式AI的运用受到真实数据隐私和法规的限制，亟需差分隐私合成数据作为替代方案。

Method: 提出了一个统一的基准框架，用于系统性评估基于正式DP保证生成的文本数据在效用性和真实性上的表现，包括多种评价指标及隐私预算的设定。

Result: 对比当前最先进方法在五个领域特定数据集上的性能，结果显示在严格隐私限制条件下，生成数据在效用性和真实性上显著下降。

Conclusion: 现有方法在隐私保护与数据共享间的权衡方面存在不足，强调迫切需要开发更高级的隐私保护数据共享方法，并建议建立现实场景下的评估标准。

Abstract: Generative AI offers transformative potential for high-stakes domains such as
healthcare and finance, yet privacy and regulatory barriers hinder the use of
real-world data. To address this, differentially private synthetic data
generation has emerged as a promising alternative. In this work, we introduce a
unified benchmark to systematically evaluate the utility and fidelity of text
datasets generated under formal Differential Privacy (DP) guarantees. Our
benchmark addresses key challenges in domain-specific benchmarking, including
choice of representative data and realistic privacy budgets, accounting for
pre-training and a variety of evaluation metrics. We assess state-of-the-art
privacy-preserving generation methods across five domain-specific datasets,
revealing significant utility and fidelity degradation compared to real data,
especially under strict privacy constraints. These findings underscore the
limitations of current approaches, outline the need for advanced
privacy-preserving data sharing methods and set a precedent regarding their
evaluation in realistic scenarios.

</details>


### [202] [Structure-aware Hypergraph Transformer for Diagnosis Prediction in Electronic Health Records](https://arxiv.org/abs/2508.20500)
*Haiyan Wang,Ye Yuan*

Main category: cs.LG

TL;DR: 本文提出了一种新的结构感知超图变换器（SHGT）框架，用于改善电子健康记录（EHR）的预测能力，尤其在诊断预测上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络（GNN）的EHR方法难以捕捉临床数据中的高阶依赖关系，且消息传递机制的局限性降低了表示能力。

Method: 提出SHGT框架，通过：a) 超图结构编码器捕捉医疗代码间的高阶交互，b) 结合Transformer架构对整个超图进行推理，c) 设计结合超图重建的目标函数保持结构信息。

Result: 在真实世界EHR数据集上的实验表明，SHGT在诊断预测任务上优于现有的最先进模型。

Conclusion: 通过引入处理高阶交互的超图和全局推理机制，SHGT增强了医疗数据的预测效果，可为EHR分析提供新工具。

Abstract: Electronic Health Records (EHR) systematically organize patient health data
through standardized medical codes, serving as a comprehensive and invaluable
source for predictive modeling. Graph neural networks (GNNs) have demonstrated
effectiveness in modeling interactions between medical codes within EHR.
However, existing GNN-based methods are inadequate due to: a) their reliance on
pairwise relations fails to capture the inherent higher-order dependencies in
clinical data, and b) the localized message-passing scheme limits
representation power. To address these issues, this paper proposes a novel
Structure-aware HyperGraph Transformer (SHGT) framework following three-fold
ideas: a) employing a hypergraph structural encoder to capture higher-order
interactions among medical codes, b) integrating the Transformer architecture
to reason over the entire hypergraph, and c) designing a tailored loss function
incorporating hypergraph reconstruction to preserve the hypergraph's original
structure. Experiments on real-world EHR datasets demonstrate that the proposed
SHGT outperforms existing state-of-the-art models on diagnosis prediction.

</details>


### [203] [Khiops: An End-to-End, Frugal AutoML and XAI Machine Learning Solution for Large, Multi-Table Databases](https://arxiv.org/abs/2508.20519)
*Marc Boullé,Nicolas Voisine,Bruno Guerraz,Carine Hue,Felipe Olmos,Vladimir Popescu,Stéphane Gouache,Stéphane Bouget,Alexis Bondu,Luc Aurelien Gauthier,Yassine Nair Benrekia,Fabrice Clérot,Vincent Lemaire*

Main category: cs.LG

TL;DR: Khiops 是一个开源机器学习工具，专注于大型多表数据库的挖掘，基于独特的贝叶斯方法。


<details>
  <summary>Details</summary>
Motivation: 设计一个能够高效处理大型多表数据库的开源工具，以应对复杂数据挖掘需求。

Method: 采用基于贝叶斯的分类/回归模型，同时运用变量选择、权重学习与自动聚合构造。

Result: 成功支持数百万样本、成千上万变量及数亿级记录的数据库分析，并支持多种运行环境。

Conclusion: Khiops 能有效处理大规模复杂数据库，为用户提供灵活且高效的分析工具。

Abstract: Khiops is an open source machine learning tool designed for mining large
multi-table databases. Khiops is based on a unique Bayesian approach that has
attracted academic interest with more than 20 publications on topics such as
variable selection, classification, decision trees and co-clustering. It
provides a predictive measure of variable importance using discretisation
models for numerical data and value clustering for categorical data. The
proposed classification/regression model is a naive Bayesian classifier
incorporating variable selection and weight learning. In the case of
multi-table databases, it provides propositionalisation by automatically
constructing aggregates. Khiops is adapted to the analysis of large databases
with millions of individuals, tens of thousands of variables and hundreds of
millions of records in secondary tables. It is available on many environments,
both from a Python library and via a user interface.

</details>


### [204] [MedGR$^2$: Breaking the Data Barrier for Medical Reasoning via Generative Reward Learning](https://arxiv.org/abs/2508.20549)
*Weihai Zhi,Jiayan Guo,Shangyang Li*

Main category: cs.LG

TL;DR: 该论文提出了一个新的框架MedGR$^2$，在医学领域通过生成奖励学习打破数据稀缺的限制，自动生成优质医疗数据以改进模型表现。


<details>
  <summary>Details</summary>
Motivation: 医学中视觉-语言模型因缺乏高质量专家注释数据而受限，现有监督微调表现不佳，而现有强化学习方法又缺乏可靠的奖励信号。

Method: 提出框架MedGR$^2$，共同优化数据生成器和奖励模型，自动生成高质量多模态医疗数据，用于监督微调和强化学习。

Result: 实验表明，使用MedGR$^2$生成的数据进行SFT超越了人类专家注释数据的基线性能，并通过GRPO提升了跨模态与跨任务的推广表现。

Conclusion: MedGR$^2$从数据稀缺转向数据生成，为高风险领域的数据高效学习提供了新范式，展现了强化学习在医疗AI中的潜力。

Abstract: The application of Vision-Language Models (VLMs) in medicine is critically
hampered by the scarcity of high-quality, expert-annotated data. Supervised
Fine-Tuning (SFT) on existing datasets often leads to poor generalization on
unseen modalities and tasks, while Reinforcement Learning (RL), a promising
alternative, is stymied by the lack of reliable reward signals in this
data-scarce domain. To break this impasse, we introduce Generative Reward
Learning for Medical Reasoning (MedGR$^2$), a novel framework that creates a
self-improving virtuous cycle. MedGR$^2$ co-develops a data generator and a
reward model, enabling the automated, continuous creation of high-quality,
multi-modal medical data that serves as both a superior training source for SFT
and RL. Our experiments demonstrate that SFT with MedGR$^2$-produced data
already surpasses baselines trained on large-scale, human-curated datasets.
Crucially, when leveraging this data for RL via Group Relative Policy
Optimization (GRPO), our model achieves state-of-the-art cross-modality and
cross-task generalization, significantly outperforming specialized RL-based
methods. Furthermore, our compact model, empowered by MedGR$^2$, achieves
performance competitive with foundation models possessing over 10 times more
parameters. MedGR$^2$ presents a new paradigm for data-efficient learning in
high-stakes domains, transforming the problem from data scarcity to data
generation and unlocking the full potential of RL for building truly
generalizable medical AI.

</details>


### [205] [Theoretical foundations of the integral indicator application in hyperparametric optimization](https://arxiv.org/abs/2508.20550)
*Roman S. Kulshin,Anatoly A. Sidorov*

Main category: cs.LG

TL;DR: 本文提出一种通过综合各种性能指标为单一标准，优化推荐算法超参数的方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常聚焦于单一指标，该研究旨在提倡多指标权衡以提升推荐算法全局性能。

Method: 采用多指标综合评估工具，平衡精度、排名质量、多样性与算法资源占用之间的冲突。

Result: 结果表明该工具不仅适用于推荐系统，也可在广泛机器学习与数据分析任务中应用。

Conclusion: 研究开发了通用的多指标优化工具，强调了其广泛的理论与实践意义。

Abstract: The article discusses the concept of hyperparametric optimization of
recommendation algorithms using an integral assessment that combines various
performance indicators into a single consolidated criterion. This approach is
opposed to traditional methods of setting up a single metric and allows you to
achieve a balance between accuracy, ranking quality, variety of output and the
resource intensity of algorithms. The theoretical significance of the research
lies in the development of a universal multi-criteria optimization tool that is
applicable not only in recommendation systems, but also in a wide range of
machine learning and data analysis tasks.

</details>


### [206] [MERIT: Maximum-normalized Element-wise Ratio for Language Model Large-batch Training](https://arxiv.org/abs/2508.20577)
*Yang Luo,Zangwei Zheng,Ziheng Qin,Zirui Zhu,Yong Liu,Yang You*

Main category: cs.LG

TL;DR: 本文提出了一种名为MERIT的新型优化器，通过采用max-norm计算信任比率以及引入元素级信任比率，解决了大批量训练中优化与泛化性的问题，并在GPT-2模型的大批量训练实验中展示了出色表现。


<details>
  <summary>Details</summary>
Motivation: 现有优化器如AdamW在语言模型的大批量训练中出现性能下降，主要由于注意力层中最大注意力logit的急剧增加导致的信息瓶颈问题；尽管LAMB优化器部分缓解了这一问题，但其利用$l_2$范数信任比率对query/key权重的最大值影响有限，且忽视了权重行或列内的关系。

Method: MERIT优化器通过max-norm计算信任比率以更有效地限制最大注意力logit，并设计了元素级信任比率以关注局部权重结构，从而实现更稳健的更新调整。

Result: 在多种尺寸的GPT-2模型的大批量训练实验中，MERIT展现了卓越性能。尤其是在GPT-2 Medium模型的训练中，MERIT在批量大小达到6k的情况下，无性能下降且训练稳定性显著提升。

Conclusion: MERIT的提出突出了在大批量训练中关注最大注意力logit和更细粒度的信任比率的关键性。它显著提高了训练稳定性，为更大批量的使用铺平了道路，加速了大语言模型的开发与迭代。

Abstract: Large-batch training has become a cornerstone in accelerating the training of
deep neural networks, yet it poses challenges in optimization and
generalization. Existing optimizers like AdamW present performance degradation
during language models' large-batch training, due to the information bottleneck
in attention layers caused by the sharp increase of max attention logit. While
the LAMB optimizer partially addresses this issue, some attention layers still
face this issue. The reason is that $l_2$-norm-based trust ratios in LAMB are
less effective in directly influencing the max value of query/key weights.
Furthermore, the weight-wise trust ratio in LAMB is error-prone as it overlooks
relationships of weight values within rows or columns. Building on these
observations, we propose a novel optimizer, MERIT, which leverages the max-norm
to calculate the trust ratio to constrain the max attention logit more
effectively. Moreover, we further construct element-wise trust ratios to
provide more robust update scaling by focusing on local weight structures.
Extensive experiments of large-batch training across various sizes of GPT-2
models demonstrate the superior performance of MERIT. Notably, during the
training of GPT-2 Medium, MERIT enables a 6k batch size without any performance
degradation compared to the standard batch size (480) with 48B training tokens.
This work highlights the importance of considering the max attention logit and
finer-granularity trust ratio in large-batch training. It successfully improves
the training stability and paves the way for larger batch usage, enabling
faster development and iteration of large language models. Code is available at
https://github.com/NUS-HPC-AI-Lab/MERIT.

</details>


### [207] [Unbiased Stochastic Optimization for Gaussian Processes on Finite Dimensional RKHS](https://arxiv.org/abs/2508.20588)
*Neta Shoham,Haim Avron*

Main category: cs.LG

TL;DR: 本文针对高斯过程的随机超参数学习提出了一种无需逼近的新方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用逼近技术，但无法保证收敛到真实边际似然的驻点，需求更精确的方法。

Method: 提出一种在有限维和无限维RKHS中的精确随机推断算法，无需传统逼近技术。

Result: 在内存受限的情况下，本方法在实验中优于现有的方法。

Conclusion: 该方法在RKHS范围内实现了更高效的超参数学习，具备优良性能。

Abstract: Current methods for stochastic hyperparameter learning in Gaussian Processes
(GPs) rely on approximations, such as computing biased stochastic gradients or
using inducing points in stochastic variational inference. However, when using
such methods we are not guaranteed to converge to a stationary point of the
true marginal likelihood. In this work, we propose algorithms for exact
stochastic inference of GPs with kernels that induce a Reproducing Kernel
Hilbert Space (RKHS) of moderate finite dimension. Our approach can also be
extended to infinite dimensional RKHSs at the cost of forgoing exactness. Both
for finite and infinite dimensional RKHSs, our method achieves better
experimental results than existing methods when memory resources limit the
feasible batch size and the possible number of inducing points.

</details>


### [208] [Local Virtual Nodes for Alleviating Over-Squashing in Graph Neural Networks](https://arxiv.org/abs/2508.20597)
*Tuğrul Hasan Karabulut,İnci M. Baytaş*

Main category: cs.LG

TL;DR: 本文提出了一种称为局部虚拟节点（Local Virtual Nodes, LVN）的新方法来缓解图神经网络中的过压缩问题，并显著提升了图与节点分类任务的表现。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络中由于消息传递受限于固定节点表示所导致的过压缩问题，同时避免破坏输入图的全局拓扑结构。

Method: 通过在图中加入基于节点中心性选择的局部虚拟节点（LVN），以改善潜在瓶颈区域的连通性。引入可训练的LVN嵌入，并在选定的中心区域间共享，提升消息传递效能。

Result: 实验表明，该方法在多个基准数据集上显著提升了图与节点分类任务的精度，并改善了结构连通性。

Conclusion: LVN方法在不显著破坏图的原始全局结构前提下，有效缓解了过压缩问题，为图神经网络的改进提供了一种新颖且有效的途径。

Abstract: Over-squashing is a challenge in training graph neural networks for tasks
involving long-range dependencies. In such tasks, a GNN's receptive field
should be large enough to enable communication between distant nodes. However,
gathering information from a wide range of neighborhoods and squashing its
content into fixed-size node representations makes message-passing vulnerable
to bottlenecks. Graph rewiring and adding virtual nodes are commonly studied
remedies that create additional pathways around bottlenecks to mitigate
over-squashing. However, these techniques alter the input graph's global
topology and disrupt the domain knowledge encoded in the original graph
structure, both of which could be essential to specific tasks and domains. This
study presents Local Virtual Nodes (LVN) with trainable embeddings to alleviate
the effects of over-squashing without significantly corrupting the global
structure of the input graph. The position of the LVNs is determined by the
node centrality, which indicates the existence of potential bottlenecks. Thus,
the proposed approach aims to improve the connectivity in the regions with
likely bottlenecks. Furthermore, trainable LVN embeddings shared across
selected central regions facilitate communication between distant nodes without
adding more layers. Extensive experiments on benchmark datasets demonstrate
that LVNs can enhance structural connectivity and significantly improve
performance on graph and node classification tasks. The code can be found at
https://github.com/ALLab-Boun/LVN/}{https://github.com/ALLab-Boun/LVN/.

</details>


### [209] [Dimension Agnostic Testing of Survey Data Credibility through the Lens of Regression](https://arxiv.org/abs/2508.20616)
*Debabrota Basu,Sourav Chakraborty,Debarshi Chanda,Buddha Dev Das,Arijit Ghosh,Arnab Ray*

Main category: cs.LG

TL;DR: 本文提出了一种基于任务的方法，通过引入模型相关的距离度量来评估样本调查的可信度，并设计了一种算法用于回归模型背景下的验证。


<details>
  <summary>Details</summary>
Motivation: 探索如何在高维数据分布无法完全清晰时，确定样本调查是否具有可信度。

Method: 提出模型相关的距离度量，并设计了一种独立于数据维度的算法，用于验证样本调查的可信度，而不是直接重建回归模型。

Result: 验证得出，算法可以在样本复杂度独立于数据维度的情况下高效运行。此外，若尝试通过重建回归模型验证，其复杂度将与数据维度线性增加。

Conclusion: 通过理论证明和数值实验，算法在验证样本调查可信度方面既高效又准确，尤其在高维数据中表现出明显优势。

Abstract: Assessing whether a sample survey credibly represents the population is a
critical question for ensuring the validity of downstream research. Generally,
this problem reduces to estimating the distance between two high-dimensional
distributions, which typically requires a number of samples that grows
exponentially with the dimension. However, depending on the model used for data
analysis, the conclusions drawn from the data may remain consistent across
different underlying distributions. In this context, we propose a task-based
approach to assess the credibility of sampled surveys. Specifically, we
introduce a model-specific distance metric to quantify this notion of
credibility. We also design an algorithm to verify the credibility of survey
data in the context of regression models. Notably, the sample complexity of our
algorithm is independent of the data dimension. This efficiency stems from the
fact that the algorithm focuses on verifying the credibility of the survey data
rather than reconstructing the underlying regression model. Furthermore, we
show that if one attempts to verify credibility by reconstructing the
regression model, the sample complexity scales linearly with the dimensionality
of the data. We prove the theoretical correctness of our algorithm and
numerically demonstrate our algorithm's performance.

</details>


### [210] [Supervised Stochastic Gradient Algorithms for Multi-Trial Source Separation](https://arxiv.org/abs/2508.20618)
*Ronak Mehta,Mateus Piovezan Otto,Noah Stanis,Azadeh Yazdan-Shahmorad,Zaid Harchaoui*

Main category: cs.LG

TL;DR: 本文提出一种结合多重试验监督的独立成分分析随机算法，并通过合成和真实数据实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决独立成分分析中非凸优化成功率低和成分解释性不足的问题，引入多重试验监督。

Method: 提出了一种将可逆矩阵空间上的近端梯度型算法与预测模型联合学习（通过反向传播）的算法结合的方法。

Result: 通过合成和真实数据实验，发现额外的监督能提高非凸优化成功率并改善独立成分的解释性。

Conclusion: 多重试验监督为独立成分分析提供了改进的成功率和更高的解释性，这是提供更深刻科学见解的有力方法。

Abstract: We develop a stochastic algorithm for independent component analysis that
incorporates multi-trial supervision, which is available in many scientific
contexts. The method blends a proximal gradient-type algorithm in the space of
invertible matrices with joint learning of a prediction model through
backpropagation. We illustrate the proposed algorithm on synthetic and real
data experiments. In particular, owing to the additional supervision, we
observe an increased success rate of the non-convex optimization and the
improved interpretability of the independent components.

</details>


### [211] [Masked Autoencoders for Ultrasound Signals: Robust Representation Learning for Downstream Applications](https://arxiv.org/abs/2508.20622)
*Immanuel Roßteutscher,Klaus S. Drese,Thorsten Uphues*

Main category: cs.LG

TL;DR: 本文探讨了应用MAEs与ViT到1D超声信号中进行自监督表征学习。


<details>
  <summary>Details</summary>
Motivation: 旨在填补MAEs在1D超声信号分析领域的研究空白，解决工业超声信号中标注数据稀缺和任务专属性强的问题。

Method: 提出利用MAE在无标注的合成超声信号上进行预训练，并探索模型尺寸、补丁尺寸和掩码比例对效率与准确性的影响。

Result: 预训练模型超越了从头训练的模型和CNN基线，且合成数据上的预训练在真实信号任务迁移上效果更优。

Conclusion: MAEs在超声信号分析中展现了巨大的潜力，可通过自监督学习方法实现高效、可扩展的信号表征。

Abstract: We investigated the adaptation and performance of Masked Autoencoders (MAEs)
with Vision Transformer (ViT) architectures for self-supervised representation
learning on one-dimensional (1D) ultrasound signals. Although MAEs have
demonstrated significant success in computer vision and other domains, their
use for 1D signal analysis, especially for raw ultrasound data, remains largely
unexplored. Ultrasound signals are vital in industrial applications such as
non-destructive testing (NDT) and structural health monitoring (SHM), where
labeled data are often scarce and signal processing is highly task-specific. We
propose an approach that leverages MAE to pre-train on unlabeled synthetic
ultrasound signals, enabling the model to learn robust representations that
enhance performance in downstream tasks, such as time-of-flight (ToF)
classification. This study systematically investigated the impact of model
size, patch size, and masking ratio on pre-training efficiency and downstream
accuracy. Our results show that pre-trained models significantly outperform
models trained from scratch and strong convolutional neural network (CNN)
baselines optimized for the downstream task. Additionally, pre-training on
synthetic data demonstrates superior transferability to real-world measured
signals compared with training solely on limited real datasets. This study
underscores the potential of MAEs for advancing ultrasound signal analysis
through scalable, self-supervised learning.

</details>


### [212] [GDS Agent: A Graph Algorithmic Reasoning Agent](https://arxiv.org/abs/2508.20637)
*Borun Shi,Ioannis Panagiotas*

Main category: cs.LG

TL;DR: 本文提出了一种名为GDS Agent的系统，结合LLMs和图算法处理能力，能够高效回答基于图数据的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs难以高效处理大规模图结构数据的问题亟待解决。

Method: 通过MCP服务器引入图算法工具集，并结合数据预处理与后处理，将其与LLMs集成以实现图任务的推理。

Result: GDS Agent在多个图任务中表现优异，并通过案例研究揭示了在开放任务中的适用性及不足之处。

Conclusion: GDS Agent通过增强LLMs图数据推理能力，为复杂图任务提供了一种更强大的解决方案，同时明确了领域内未来挑战。

Abstract: Large language models (LLMs) have shown remarkable multimodal information
processing and reasoning ability. When equipped with tools through function
calling and enhanced with retrieval-augmented techniques, compound LLM-based
systems can access closed data sources and answer questions about them.
However, they still struggle to process and reason over large-scale
graph-structure data. We introduce the GDS (Graph Data Science) agent in this
technical report. The GDS agent introduces a comprehensive set of graph
algorithms as tools, together with preprocessing (retrieval) and postprocessing
of algorithm results, in a model context protocol (MCP) server. The server can
be used with any modern LLM out-of-the-box. GDS agent allows users to ask any
question that implicitly and intrinsically requires graph algorithmic reasoning
about their data, and quickly obtain accurate and grounded answers. We also
introduce a new benchmark that evaluates intermediate tool calls as well as
final responses. The results indicate that GDS agent is able to solve a wide
spectrum of graph tasks. We also provide detailed case studies for more
open-ended tasks and study scenarios where the agent struggles. Finally, we
discuss the remaining challenges and the future roadmap.

</details>


### [213] [A Hybrid Stochastic Gradient Tracking Method for Distributed Online Optimization Over Time-Varying Directed Networks](https://arxiv.org/abs/2508.20645)
*Xinli Shi,Xingxing Yuan,Longkang Zhu,Guanghui Wen*

Main category: cs.LG

TL;DR: 该论文提出了一种名为TV-HSGT的新算法，用于处理时间变化的分布式在线优化问题，尤其是在随机梯度动态网络中具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 随着数据规模和动态性的增加，实时决策对分布式在线优化的需求逐渐增强，但现有算法无法有效应对时间变化网络和随机梯度的影响。

Method: 通过集成行随机和列随机通信机制，提出TV-HSGT算法，结合当前及递归随机梯度以降低梯度方差并准确跟踪全局下降方向，同时无需依赖Perron向量估计或出度信息。

Result: 理论上证明TV-HSGT在无需假设梯度有界的情况下可实现改进的动态遗憾界限，同时实验验证其对动态条件下任务表现优越。

Conclusion: TV-HSGT在动态且资源受限环境中表现出色，是分布式在线优化领域的重要创新。

Abstract: With the increasing scale and dynamics of data, distributed online
optimization has become essential for real-time decision-making in various
applications. However, existing algorithms often rely on bounded gradient
assumptions and overlook the impact of stochastic gradients, especially in
time-varying directed networks. This study proposes a novel Time-Varying Hybrid
Stochastic Gradient Tracking algorithm named TV-HSGT, based on hybrid
stochastic gradient tracking and variance reduction mechanisms. Specifically,
TV-HSGT integrates row-stochastic and column-stochastic communication schemes
over time-varying digraphs, eliminating the need for Perron vector estimation
or out-degree information. By combining current and recursive stochastic
gradients, it effectively reduces gradient variance while accurately tracking
global descent directions. Theoretical analysis demonstrates that TV-HSGT can
achieve improved bounds on dynamic regret without assuming gradient
boundedness. Experimental results on logistic regression tasks confirm the
effectiveness of TV-HSGT in dynamic and resource-constrained environments.

</details>


### [214] [VarDiU: A Variational Diffusive Upper Bound for One-Step Diffusion Distillation](https://arxiv.org/abs/2508.20646)
*Leyang Wang,Mingtian Zhang,Zijing Ou,David Barber*

Main category: cs.LG

TL;DR: 本文提出了VarDiU方法，可以在扩散提纯过程中为生成质量和训练稳定性提供更好的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散提纯方法中，学生模型的训练梯度因去噪分数匹配的不完善而存在偏差，导致性能不佳。

Method: 提出一种叫VarDiU的变分扩散上界，并用其实现无偏梯度估计来优化扩散提纯。

Result: 与Diff-Instruct相比，VarDiU实现了更高的生成质量，同时提升了一步扩散提纯的训练效率和稳定性。

Conclusion: VarDiU方法通过无偏的梯度估计，为扩散提纯提供了更优的框架，显著提高了模型的实际表现。

Abstract: Recently, diffusion distillation methods have compressed thousand-step
teacher diffusion models into one-step student generators while preserving
sample quality. Most existing approaches train the student model using a
diffusive divergence whose gradient is approximated via the student's score
function, learned through denoising score matching (DSM). Since DSM training is
imperfect, the resulting gradient estimate is inevitably biased, leading to
sub-optimal performance. In this paper, we propose VarDiU (pronounced
/va:rdju:/), a Variational Diffusive Upper Bound that admits an unbiased
gradient estimator and can be directly applied to diffusion distillation. Using
this objective, we compare our method with Diff-Instruct and demonstrate that
it achieves higher generation quality and enables a more efficient and stable
training procedure for one-step diffusion distillation.

</details>


### [215] [Physics-Constrained Machine Learning for Chemical Engineering](https://arxiv.org/abs/2508.20649)
*Angan Mukherjee,Victor M. Zavala*

Main category: cs.LG

TL;DR: 本文概述了物理约束机器学习（PCML）在化学工程应用中的挑战和机遇。


<details>
  <summary>Details</summary>
Motivation: 通过结合物理模型与数据驱动方法，提高可靠性、推广性和可解释性的需求。

Method: 设计有效的物理知识与机器学习融合策略，同时应对大规模数据集的扩展，并量化预测的不确定性。

Result: 总结了PCML在化学工程中的最新进展，指出其在闭环实验设计、实时动态控制和多尺度现象处理中的潜力。

Conclusion: PCML尽管面临技术与智力挑战，但在化学工程中的应用前景广阔。

Abstract: Physics-constrained machine learning (PCML) combines physical models with
data-driven approaches to improve reliability, generalizability, and
interpretability. Although PCML has shown significant benefits in diverse
scientific and engineering domains, technical and intellectual challenges
hinder its applicability in complex chemical engineering applications. Key
difficulties include determining the amount and type of physical knowledge to
embed, designing effective fusion strategies with ML, scaling models to large
datasets and simulators, and quantifying predictive uncertainty. This
perspective summarizes recent developments and highlights
challenges/opportunities in applying PCML to chemical engineering, emphasizing
on closed-loop experimental design, real-time dynamics and control, and
handling of multi-scale phenomena.

</details>


### [216] [Self-Composing Neural Operators with Depth and Accuracy Scaling via Adaptive Train-and-Unroll Approach](https://arxiv.org/abs/2508.20650)
*Juncai He,Xinliang Liu,Jinchao Xu*

Main category: cs.LG

TL;DR: 提出了一种通过自我组合增强神经算子效率和精度的新框架，并在标准基准测试和高频超声计算断层扫描问题上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 受数值偏微分方程求解中的迭代方法启发，目标是提高神经算子的能力和性能，解决复杂科学机器学习问题。

Method: 通过重复应用单一神经算子块逐步加深模型，同时引入适应性的训练与展开方法来逐渐增加模型深度，并结合多重网格启发的骨干架构实现。

Result: 在标准基准测试中获得SOTA性能，并在高频超声计算断层扫描问题中表现优越。

Conclusion: 提出的框架提供了一种计算可行、准确并且可扩展的解决方案，适用于大规模数据驱动的科学机器学习应用。

Abstract: In this work, we propose a novel framework to enhance the efficiency and
accuracy of neural operators through self-composition, offering both
theoretical guarantees and practical benefits. Inspired by iterative methods in
solving numerical partial differential equations (PDEs), we design a specific
neural operator by repeatedly applying a single neural operator block, we
progressively deepen the model without explicitly adding new blocks, improving
the model's capacity. To train these models efficiently, we introduce an
adaptive train-and-unroll approach, where the depth of the neural operator is
gradually increased during training. This approach reveals an accuracy scaling
law with model depth and offers significant computational savings through our
adaptive training strategy. Our architecture achieves state-of-the-art (SOTA)
performance on standard benchmarks. We further demonstrate its efficacy on a
challenging high-frequency ultrasound computed tomography (USCT) problem, where
a multigrid-inspired backbone enables superior performance in resolving complex
wave phenomena. The proposed framework provides a computationally tractable,
accurate, and scalable solution for large-scale data-driven scientific machine
learning applications.

</details>


### [217] [Compositionality in Time Series: A Proof of Concept using Symbolic Dynamics and Compositional Data Augmentation](https://arxiv.org/abs/2508.20656)
*Michael Hagmann,Michael Staniek,Stefan Riezler*

Main category: cs.LG

TL;DR: 本文研究自然现象时间序列是否可以通过系统性和规则性排序的潜在状态生成，并将这一研究应用于临床时间序列。


<details>
  <summary>Details</summary>
Motivation: 解决临床时间序列中数据稀少和资源有限的问题，同时加深对临床数据的理解。

Method: 通过定义时间序列的生成过程的组成性，并研究数据驱动的方式来重建这些状态和组成规则；采用两种基于领域适配的测试来评估生成数据与原始数据的相似性。

Result: 使用基于组成性合成数据进行训练，其测试集性能与原始临床时间序列数据相当；基于合成数据的评估优于随机化数据增强；基于SOFA分数预测评估显示，完全基于合成数据训练比使用原始数据的表现更佳。

Conclusion: 组成性合成数据可以有效解决数据稀缺问题，并在临床时间序列预测任务中表现出显著优势。

Abstract: This work investigates whether time series of natural phenomena can be
understood as being generated by sequences of latent states which are ordered
in systematic and regular ways. We focus on clinical time series and ask
whether clinical measurements can be interpreted as being generated by
meaningful physiological states whose succession follows systematic principles.
Uncovering the underlying compositional structure will allow us to create
synthetic data to alleviate the notorious problem of sparse and low-resource
data settings in clinical time series forecasting, and deepen our understanding
of clinical data. We start by conceptualizing compositionality for time series
as a property of the data generation process, and then study data-driven
procedures that can reconstruct the elementary states and composition rules of
this process. We evaluate the success of this methods using two empirical tests
originating from a domain adaptation perspective. Both tests infer the
similarity of the original time series distribution and the synthetic time
series distribution from the similarity of expected risk of time series
forecasting models trained and tested on original and synthesized data in
specific ways. Our experimental results show that the test set performance
achieved by training on compositionally synthesized data is comparable to
training on original clinical time series data, and that evaluation of models
on compositionally synthesized test data shows similar results to evaluating on
original test data, outperforming randomization-based data augmentation. An
additional downstream evaluation of the prediction task of sequential organ
failure assessment (SOFA) scores shows significant performance gains when model
training is entirely based on compositionally synthesized data compared to
training on original data.

</details>


### [218] [Token Buncher: Shielding LLMs from Harmful Reinforcement Learning Fine-Tuning](https://arxiv.org/abs/2508.20697)
*Weitao Feng,Lixu Wang,Tianyi Wei,Jie Zhang,Chongyang Gao,Sinong Zhan,Peizhuo Lv,Wei Dong*

Main category: cs.LG

TL;DR: 本文探讨了通过强化学习（RL）进行有害微调对大型语言模型（LLMs）带来的风险，并提出了名为TokenBuncher的新防御方法，以抑制模型对有害行为的优化。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs能力的增长，其通过微调被恶意滥用的风险也在增加。研究发现，基于RL的微调比传统的监督微调（SFT）更能破坏安全对齐并进行高级有害任务支持。

Method: 本文提出TokenBuncher防御方法，通过限制模型响应的不确定性抑制RL的功能，包括基于熵的奖励机制和Token Noiser机制，以防止恶意行为优化。

Result: 实验证明，TokenBuncher在多种模型和RL算法下有效减轻了RL微调带来的危害，同时保持了合法任务的效用和可微调性。

Conclusion: RL微调对LLMs构成更大的系统性风险，而TokenBuncher提供了一种有效的普适性防御策略。

Abstract: As large language models (LLMs) continue to grow in capability, so do the
risks of harmful misuse through fine-tuning. While most prior studies assume
that attackers rely on supervised fine-tuning (SFT) for such misuse, we
systematically demonstrate that reinforcement learning (RL) enables adversaries
to more effectively break safety alignment and facilitate advanced harmful task
assistance, under matched computational budgets. To counter this emerging
threat, we propose TokenBuncher, the first effective defense specifically
targeting RL-based harmful fine-tuning. TokenBuncher suppresses the foundation
on which RL relies: model response uncertainty. By constraining uncertainty,
RL-based fine-tuning can no longer exploit distinct reward signals to drive the
model toward harmful behaviors. We realize this defense through
entropy-as-reward RL and a Token Noiser mechanism designed to prevent the
escalation of expert-domain harmful capabilities. Extensive experiments across
multiple models and RL algorithms show that TokenBuncher robustly mitigates
harmful RL fine-tuning while preserving benign task utility and finetunability.
Our results highlight that RL-based harmful fine-tuning poses a greater
systemic risk than SFT, and that TokenBuncher provides an effective and general
defense.

</details>


### [219] [EEGDM: Learning EEG Representation with Latent Diffusion Model](https://arxiv.org/abs/2508.20705)
*Shaocong Wang,Tong Liu,Ming Li,Minjing Yu,Yong-Jin Liu*

Main category: cs.LG

TL;DR: 提出了一种基于潜在扩散模型（latent diffusion model）的自监督EEG表示学习方法EEGDM，通过生成EEG信号来进行自监督目标，从而捕捉EEG语义信息。


<details>
  <summary>Details</summary>
Motivation: 现有EEG深度学习方法难以在任务多样性和有限训练数据情况下实现广泛泛化，其简单的掩码重建目标限制了对EEG信号的语义信息和复杂模式的深入捕获。

Method: EEGDM方法利用EEG信号生成作为自监督目标，将扩散模型作为强表征学习工具，包含一个EEG编码器，将EEG信号及其通道增强转化为紧凑表示，指导扩散模型生成EEG信号并应用于下游任务。

Result: 实验表明，该方法能高质量重建EEG信号，有效学习稳健表征，且在适度预训练数据下于多种下游任务表现优异。

Conclusion: EEGDM具有良好生成与表征学习能力，展示了方法的通用性和实用性。

Abstract: While electroencephalography (EEG) signal analysis using deep learning has
shown great promise, existing approaches still face significant challenges in
learning generalizable representations that perform well across diverse tasks,
particularly when training data is limited. Current EEG representation learning
methods including EEGPT and LaBraM typically rely on simple masked
reconstruction objective, which may not fully capture the rich semantic
information and complex patterns inherent in EEG signals. In this paper, we
propose EEGDM, a novel self-supervised EEG representation learning method based
on the latent diffusion model, which leverages EEG signal generation as a
self-supervised objective, turning the diffusion model into a strong
representation learner capable of capturing EEG semantics. EEGDM incorporates
an EEG encoder that distills EEG signals and their channel augmentations into a
compact representation, acting as conditional information to guide the
diffusion model for generating EEG signals. This design endows EEGDM with a
compact latent space, which not only offers ample control over the generative
process but also can be leveraged for downstream tasks. Experimental results
show that EEGDM (1) can reconstruct high-quality EEG signals, (2) effectively
learns robust representations, and (3) achieves competitive performance with
modest pre-training data size across diverse downstream tasks, underscoring its
generalizability and practical utility.

</details>


### [220] [Provable Benefits of In-Tool Learning for Large Language Models](https://arxiv.org/abs/2508.20755)
*Sam Houliston,Ambroise Odonnat,Charles Arnal,Vivien Cabannes*

Main category: cs.LG

TL;DR: 本研究阐明了工具增强语言模型在处理事实回忆任务中的理论和实验优势，证明其比仅记忆型模型更具扩展性和效率。


<details>
  <summary>Details</summary>
Motivation: 探讨工具增强语言模型的理论优势，特别是在处理事实回忆任务中，与纯记忆型模型的对比。

Method: 通过理论证明和受控实验，分析了基于参数记忆的限制性，以及工具使用如何通过高效结构构建实现无限制的事实回忆能力。

Result: 工具增强模型在实验中表现出优于记忆型模型的能力，并验证了教授工具使用及通用规则胜于将事实嵌入到模型记忆中的策略。

Conclusion: 工具增强工作流不仅实用，且在理论上更具扩展性和效率，为语言模型的未来发展奠定了理论和实验基础。

Abstract: Tool-augmented language models, equipped with retrieval, memory, or external
APIs, are reshaping AI, yet their theoretical advantages remain underexplored.
In this paper, we address this question by demonstrating the benefits of
in-tool learning (external retrieval) over in-weight learning (memorization)
for factual recall. We show that the number of facts a model can memorize
solely in its weights is fundamentally limited by its parameter count. In
contrast, we prove that tool-use enables unbounded factual recall via a simple
and efficient circuit construction. These results are validated in controlled
experiments, where tool-using models consistently outperform memorizing ones.
We further show that for pretrained large language models, teaching tool-use
and general rules is more effective than finetuning facts into memory. Our work
provides both a theoretical and empirical foundation, establishing why
tool-augmented workflows are not just practical, but provably more scalable.

</details>


### [221] [Unleashing Uncertainty: Efficient Machine Unlearning for Generative AI](https://arxiv.org/abs/2508.20773)
*Christoforos N. Spartalis,Theodoros Semertzidis,Petros Daras,Efstratios Gavves*

Main category: cs.LG

TL;DR: 论文提出了一种名为SAFEMax的新方法，用于扩散模型中的机器遗忘，方法通过最大化生成图像中的熵，使模型在面对不允许的类别时生成高斯噪声，从而停止去噪过程。


<details>
  <summary>Details</summary>
Motivation: 希望解决扩散模型中机器遗忘需求的问题，即避免生成器在处理不允许的类别内容时仍生成特定数据。

Method: 基于信息论原理，最大化生成图像熵，同时在扩散的早期阶段集中处理，因为这是类别特定信息显著的阶段。

Result: 实验表明，该方法在机器遗忘任务中表现有效，并且在效率上优于最先进的现有方法。

Conclusion: SAFEMax方法不仅能有效处理机器遗忘问题，还能够在效率上实现显著提升，为相关领域提供了新的解决方案。

Abstract: We introduce SAFEMax, a novel method for Machine Unlearning in diffusion
models. Grounded in information-theoretic principles, SAFEMax maximizes the
entropy in generated images, causing the model to generate Gaussian noise when
conditioned on impermissible classes by ultimately halting its denoising
process. Also, our method controls the balance between forgetting and retention
by selectively focusing on the early diffusion steps, where class-specific
information is prominent. Our results demonstrate the effectiveness of SAFEMax
and highlight its substantial efficiency gains over state-of-the-art methods.

</details>


### [222] [cMALC-D: Contextual Multi-Agent LLM-Guided Curriculum Learning with Diversity-Based Context Blending](https://arxiv.org/abs/2508.20818)
*Anirudh Satheesh,Keenan Powell,Hua Wei*

Main category: cs.LG

TL;DR: 该论文提出了一种新方法cMALC-D，利用大语言模型（LLM）生成语义丰富的课程，同时通过上下文多样性的混合机制提升多智能体强化学习的普适性和样本效率。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体强化学习（MARL）算法在固定环境中训练时在实际复杂场景中表现不佳。现有方法尝试通过课程学习提高其泛化能力，但由于依赖不可靠的代理信号，如价值估计等，这些方法在多智能体动态和部分可见性中稳定性不足。

Method: 提出cMALC-D框架，该方法利用大语言模型（LLM）生成含义明确的课程，并通过一种上下文多样性混合机制生成新的训练场景，防止模式崩溃并鼓励探索。

Result: 在交通信号控制任务中，cMALC-D框架相比现有课程学习基线显著提高了泛化能力和样本利用效率。

Conclusion: 论文表明cMALC-D框架在复杂环境中的鲁棒性和性能优于现有方法，并为提升多智能体环境中的普适能力提供了新思路。

Abstract: Many multi-agent reinforcement learning (MARL) algorithms are trained in
fixed simulation environments, making them brittle when deployed in real-world
scenarios with more complex and uncertain conditions. Contextual MARL (cMARL)
addresses this by parameterizing environments with context variables and
training a context-agnostic policy that performs well across all environment
configurations. Existing cMARL methods attempt to use curriculum learning to
help train and evaluate context-agnostic policies, but they often rely on
unreliable proxy signals, such as value estimates or generalized advantage
estimates that are noisy and unstable in multi-agent settings due to
inter-agent dynamics and partial observability. To address these issues, we
propose Contextual Multi-Agent LLM-Guided Curriculum Learning with
Diversity-Based Context Blending (cMALC-D), a framework that uses Large
Language Models (LLMs) to generate semantically meaningful curricula and
provide a more robust evaluation signal. To prevent mode collapse and encourage
exploration, we introduce a novel diversity-based context blending mechanism
that creates new training scenarios by combining features from prior contexts.
Experiments in traffic signal control domains demonstrate that cMALC-D
significantly improves both generalization and sample efficiency compared to
existing curriculum learning baselines. We provide code at
https://github.com/DaRL-LibSignal/cMALC-D.

</details>


### [223] [GPT-FT: An Efficient Automated Feature Transformation Using GPT for Sequence Reconstruction and Performance Enhancement](https://arxiv.org/abs/2508.20824)
*Yang Gao,Dongjie Wang,Scott Piersall,Ye Zhang,Liqiang Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于变压器架构的自动化特征转换框架，旨在通过优化数据表示来提高机器学习模型性能，同时提升计算效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有特征转换方法依赖于连续的编码器-解码器结构，计算成本高且参数需求大，限制了其效率和可扩展性。

Method: 提出的框架通过四个步骤实现自动化特征转换：转化记录收集、使用改进的GPT模型构建嵌入空间、梯度上升搜索以及自回归重建。

Result: 实验结果表明，该框架在基准数据集上的性能与现有方法相当甚至更优，同时显著提升了计算效率。

Conclusion: 基于变压器的架构在可扩展、高性能的自动化特征转换中展现了巨大潜力。

Abstract: Feature transformation plays a critical role in enhancing machine learning
model performance by optimizing data representations. Recent state-of-the-art
approaches address this task as a continuous embedding optimization problem,
converting discrete search into a learnable process. Although effective, these
methods often rely on sequential encoder-decoder structures that cause high
computational costs and parameter requirements, limiting scalability and
efficiency. To address these limitations, we propose a novel framework that
accomplishes automated feature transformation through four steps:
transformation records collection, embedding space construction with a revised
Generative Pre-trained Transformer (GPT) model, gradient-ascent search, and
autoregressive reconstruction. In our approach, the revised GPT model serves
two primary functions: (a) feature transformation sequence reconstruction and
(b) model performance estimation and enhancement for downstream tasks by
constructing the embedding space. Such a multi-objective optimization framework
reduces parameter size and accelerates transformation processes. Experimental
results on benchmark datasets show that the proposed framework matches or
exceeds baseline performance, with significant gains in computational
efficiency. This work highlights the potential of transformer-based
architectures for scalable, high-performance automated feature transformation.

</details>


### [224] [ATM-GAD: Adaptive Temporal Motif Graph Anomaly Detection for Financial Transaction Networks](https://arxiv.org/abs/2508.20829)
*Zeyue Zhang,Lin Song,Erkang Bao,Xiaoling Lv,Xinyue Wang*

Main category: cs.LG

TL;DR: 介绍了一种名为ATM-GAD的新型金融欺诈检测模型，利用时间模式和账户特定活动时间窗口来提高异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以应对金融系统中快速变化的交易行为和复杂网络结构，需开发有效的检测方法来抓住时间相关的欺诈特征。

Method: 提出了ATM-GAD模型，结合时间模式提取器和自适应时间窗口学习器，通过图神经网络和双注意力机制全面分析交易网络中的欺诈行为。

Result: 在四个真实数据集上实验表明，ATM-GAD在检测精度上优于七种现有的欺诈检测基线方法。

Conclusion: ATM-GAD能够提取时间相关的欺诈特征，显著提升了检测现代金融系统中复杂欺诈行为的能力。

Abstract: Financial fraud detection is essential to safeguard billions of dollars, yet
the intertwined entities and fast-changing transaction behaviors in modern
financial systems routinely defeat conventional machine learning models. Recent
graph-based detectors make headway by representing transactions as networks,
but they still overlook two fraud hallmarks rooted in time: (1) temporal
motifs--recurring, telltale subgraphs that reveal suspicious money flows as
they unfold--and (2) account-specific intervals of anomalous activity, when
fraud surfaces only in short bursts unique to each entity. To exploit both
signals, we introduce ATM-GAD, an adaptive graph neural network that leverages
temporal motifs for financial anomaly detection. A Temporal Motif Extractor
condenses each account's transaction history into the most informative motifs,
preserving both topology and temporal patterns. These motifs are then analyzed
by dual-attention blocks: IntraA reasons over interactions within a single
motif, while InterA aggregates evidence across motifs to expose multi-step
fraud schemes. In parallel, a differentiable Adaptive Time-Window Learner
tailors the observation window for every node, allowing the model to focus
precisely on the most revealing time slices. Experiments on four real-world
datasets show that ATM-GAD consistently outperforms seven strong
anomaly-detection baselines, uncovering fraud patterns missed by earlier
methods.

</details>


### [225] [Practical Physical Layer Authentication for Mobile Scenarios Using a Synthetic Dataset Enhanced Deep Learning Approach](https://arxiv.org/abs/2508.20861)
*Yijia Guo,Junqing Zhang,Y. -W. Peter Hong*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的物理层信道状态信息（CSI）认证方法，并通过仿真和实验评估验证其在移动场景中的高效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 无线传输的广播性质易导致设备认证的安全性问题，而物理层认证利用独特的信道特性具有潜在优势，但在动态信道变化下缺乏实用方案。本文旨在填补这一缺口。

Method: 利用WLAN TGn信道模型生成合成训练数据集，通过CNN结合Siamese网络学习CSI数据对之间的时空相关性，并采用仿真与实验相结合的方法进行评估。

Result: 实验和仿真显示该方法具有优秀的泛化能力和认证性能，相比传统方法，AUC分别提高了0.03和0.06。

Conclusion: 该方案为移动场景中的物理层认证提供了一种高效的深度学习方法，为物联网设备认证安全性提供了重要支持。

Abstract: The Internet of Things (IoT) is ubiquitous thanks to the rapid development of
wireless technologies. However, the broadcast nature of wireless transmissions
results in great vulnerability to device authentication. Physical layer
authentication emerges as a promising approach by exploiting the unique channel
characteristics. However, a practical scheme applicable to dynamic channel
variations is still missing. In this paper, we proposed a deep learning-based
physical layer channel state information (CSI) authentication for mobile
scenarios and carried out comprehensive simulation and experimental evaluation
using IEEE 802.11n. Specifically, a synthetic training dataset was generated
based on the WLAN TGn channel model and the autocorrelation and the distance
correlation of the channel, which can significantly reduce the overhead of
manually collecting experimental datasets. A convolutional neural network
(CNN)-based Siamese network was exploited to learn the temporal and spatial
correlation between the CSI pair and output a score to measure their
similarity. We adopted a synergistic methodology involving both simulation and
experimental evaluation. The experimental testbed consisted of WiFi IoT
development kits and a few typical scenarios were specifically considered. Both
simulation and experimental evaluation demonstrated excellent generalization
performance of our proposed deep learning-based approach and excellent
authentication performance. Demonstrated by our practical measurement results,
our proposed scheme improved the area under the curve (AUC) by 0.03 compared to
the fully connected network-based (FCN-based) Siamese model and by 0.06
compared to the correlation-based benchmark algorithm.

</details>


### [226] [LeMat-Traj: A Scalable and Unified Dataset of Materials Trajectories for Atomistic Modeling](https://arxiv.org/abs/2508.20875)
*Ali Ramlaoui,Martin Siron,Inel Djafar,Joseph Musielewicz,Amandine Rossello,Victor Schmidt,Alexandre Duval*

Main category: cs.LG

TL;DR: 该论文提出了一种名为LeMat-Traj的大型数据集，以标准化和协调量子力学轨迹数据，提高机器学习原子间势的准确性和通用性。


<details>
  <summary>Details</summary>
Motivation: 当前量子力学轨迹数据分散且格式不一致，限制了高性能机器学习原子间势（MLIPs）的开发，而生成这些数据集的成本高昂。

Method: 通过整合大型数据存储库（如Materials Project等）中的120多万个原子配置，标准化数据表示，并开发开放源代码库LeMaterial-Fetcher用于扩展和更新数据集。

Result: 引入的LeMat-Traj数据集使得预训练模型在高能数据基础上通过微调大大降低了力预测误差，同时也使数据整合更加便捷和可重复。

Conclusion: LeMat-Traj显著降低了开发可转移性和高精度MLIPs的门槛，为科学界提供了一个重要的工具，并开放源码推动领域发展。

Abstract: The development of accurate machine learning interatomic potentials (MLIPs)
is limited by the fragmented availability and inconsistent formatting of
quantum mechanical trajectory datasets derived from Density Functional Theory
(DFT). These datasets are expensive to generate yet difficult to combine due to
variations in format, metadata, and accessibility. To address this, we
introduce LeMat-Traj, a curated dataset comprising over 120 million atomic
configurations aggregated from large-scale repositories, including the
Materials Project, Alexandria, and OQMD. LeMat-Traj standardizes data
representation, harmonizes results and filters for high-quality configurations
across widely used DFT functionals (PBE, PBESol, SCAN, r2SCAN). It
significantly lowers the barrier for training transferrable and accurate MLIPs.
LeMat-Traj spans both relaxed low-energy states and high-energy, high-force
structures, complementing molecular dynamics and active learning datasets. By
fine-tuning models pre-trained on high-force data with LeMat-Traj, we achieve a
significant reduction in force prediction errors on relaxation tasks. We also
present LeMaterial-Fetcher, a modular and extensible open-source library
developed for this work, designed to provide a reproducible framework for the
community to easily incorporate new data sources and ensure the continued
evolution of large-scale materials datasets. LeMat-Traj and LeMaterial-Fetcher
are publicly available at https://huggingface.co/datasets/LeMaterial/LeMat-Traj
and https://github.com/LeMaterial/lematerial-fetcher.

</details>


### [227] [Turning Tabular Foundation Models into Graph Foundation Models](https://arxiv.org/abs/2508.20906)
*Dmitry Eremeev,Gleb Bazhenov,Oleg Platonov,Artem Babenko,Liudmila Prokhorenkova*

Main category: cs.LG

TL;DR: 本论文提出了一种图基础模型(GFM)的简单方法，称为G2T-FM，结合了TabPFNv2的能力，通过增强节点特征和加入结构嵌入，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 提出一种方法解决目前图机器学习中节点特征多样性处理不足的问题，受到表格数据基础模型成功的启发。

Method: 采用TabPFNv2作为骨干网络，增强节点特征并加入图结构嵌入，将其应用于图基础模型中。

Result: 模型在通用测试中表现优异，超过现有GFM和从零开始优化的GNN；微调后表现继续提升。

Conclusion: 该方法探索了将表格基础模型应用于图机器学习的潜力，展现了其作为一种通用且具有潜力的方向。

Abstract: While foundation models have revolutionized such fields as natural language
processing and computer vision, their application and potential within graph
machine learning remain largely unexplored. One of the key challenges in
designing graph foundation models (GFMs) is handling diverse node features that
can vary across different graph datasets. Although many works on GFMs have been
focused exclusively on text-attributed graphs, the problem of handling
arbitrary features of other types in GFMs has not been fully addressed.
However, this problem is not unique to the graph domain, as it also arises in
the field of machine learning for tabular data. In this work, motivated by the
recent success of tabular foundation models like TabPFNv2, we propose G2T-FM, a
simple graph foundation model that employs TabPFNv2 as a backbone.
Specifically, G2T-FM augments the original node features with neighborhood
feature aggregation, adds structural embeddings, and then applies TabPFNv2 to
the constructed node representations. Even in a fully in-context regime, our
model achieves strong results, significantly outperforming publicly available
GFMs and performing on par with well-tuned GNNs trained from scratch. Moreover,
after finetuning, G2T-FM surpasses well-tuned GNN baselines, highlighting the
potential of the proposed approach. More broadly, our paper reveals a
previously overlooked direction of utilizing tabular foundation models for
graph machine learning tasks.

</details>


### [228] [Finite-Time Guarantees for Multi-Agent Combinatorial Bandits with Nonstationary Rewards](https://arxiv.org/abs/2508.20923)
*Katherine B. Adams,Justin J. Boutilier,Qinyang He,Yonatan Mintz*

Main category: cs.LG

TL;DR: 该研究探讨了一个序列资源分配问题，提出了一个新框架来解决动态奖励分布的挑战，并通过实例验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决在动态环境下个体干预效果不确定的情况下，如何优化目标总体结果的问题。

Method: 提出了第一个在组合多臂赌博文献中纳入非平稳奖励的框架，设计了具有动态遗憾理论保证的算法。

Result: 在案例研究中，其算法在程序参与率上比基线方法提高了三倍，证明了其实用潜力。

Conclusion: 该研究理论算法与实践应用相结合，能有效应对非平稳奖励环境下的资源分配问题，具有广泛的现实意义。

Abstract: We study a sequential resource allocation problem where a decision maker
selects subsets of agents at each period to maximize overall outcomes without
prior knowledge of individual-level effects. Our framework applies to settings
such as community health interventions, targeted digital advertising, and
workforce retention programs, where intervention effects evolve dynamically.
Agents may exhibit habituation (diminished response from frequent selection) or
recovery (enhanced response from infrequent selection). The technical challenge
centers on nonstationary reward distributions that lead to changing
intervention effects over time. The problem requires balancing two key
competing objectives: heterogeneous individual rewards and the
exploration-exploitation tradeoff in terms of learning for improved future
decisions as opposed to maximizing immediate outcomes. Our contribution
introduces the first framework incorporating this form of nonstationary rewards
in the combinatorial multi-armed bandit literature. We develop algorithms with
theoretical guarantees on dynamic regret and demonstrate practical efficacy
through a diabetes intervention case study. Our personalized community
intervention algorithm achieved up to three times as much improvement in
program enrollment compared to baseline approaches, validating the framework's
potential for real-world applications. This work bridges theoretical advances
in adaptive learning with practical challenges in population-level behavioral
change interventions.

</details>


### [229] [Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees](https://arxiv.org/abs/2508.21001)
*Yaniv Hassidof,Tom Jurgenson,Kiril Solovey*

Main category: cs.LG

TL;DR: 本文提出了一种名为Diffusion Tree (DiTree)的新方法，将扩散策略与采样规划器相结合，实现了快速且安全的运动规划。


<details>
  <summary>Details</summary>
Motivation: 解决由于传统采样规划器动作采样效率低下和学习驱动方法在分布外场景中表现不佳的问题。

Method: 利用扩散策略生成机制作为指导采样器，以提高采样规划器在高维状态空间的搜索效率，同时保留采样规划器的完备性和安全性。

Result: 在分布外环境中的实验评估显示，DiTree比传统采样规划器快3倍，成功率提高30%左右，同时仍然具备可推广性与安全性。

Conclusion: DiTree是一个新颖且实用的框架，不仅在效率上优于传统方法，还能有效应对分布外场景，具有良好的潜力部署在实际机器人系统中。

Abstract: Kinodynamic motion planning is concerned with computing collision-free
trajectories while abiding by the robot's dynamic constraints. This critical
problem is often tackled using sampling-based planners (SBPs) that explore the
robot's high-dimensional state space by constructing a search tree via action
propagations. Although SBPs can offer global guarantees on completeness and
solution quality, their performance is often hindered by slow exploration due
to uninformed action sampling. Learning-based approaches can yield
significantly faster runtimes, yet they fail to generalize to
out-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety,
thus limiting their deployment on physical robots. We present Diffusion Tree
(DiTree): a \emph{provably-generalizable} framework leveraging diffusion
policies (DPs) as informed samplers to efficiently guide state-space search
within SBPs. DiTree combines DP's ability to model complex distributions of
expert trajectories, conditioned on local observations, with the completeness
of SBPs to yield \emph{provably-safe} solutions within a few action propagation
iterations for complex dynamical systems. We demonstrate DiTree's power with an
implementation combining the popular RRT planner with a DP action sampler
trained on a \emph{single environment}. In comprehensive evaluations on OOD
scenarios, % DiTree has comparable runtimes to a standalone DP (3x faster than
classical SBPs), while improving the average success rate over DP and SBPs.
DiTree is on average 3x faster than classical SBPs, and outperforms all other
approaches by achieving roughly 30\% higher success rate. Project webpage:
https://sites.google.com/view/ditree.

</details>


### [230] [InSQuAD: In-Context Learning for Efficient Retrieval via Submodular Mutual Information to Enforce Quality and Diversity](https://arxiv.org/abs/2508.21003)
*Souradeep Nanda,Anay Majee,Rishabh Iyer*

Main category: cs.LG

TL;DR: 本文提出了InSQuAD，通过次模互信息(SMI)提升上下文学习（ICL）模型的性能，确保示例的质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 目标是解决ICL模型中现有检索模型忽略多样性的问题，并改进示例选择策略。

Method: 使用基于SMI的联合选择策略，提出新的训练范式，通过似然损失函数学习SMI参数，并增强多跳问答数据集以改进模型学习。

Result: 通过在九个基准数据集上的实验，验证了所提方法在改进ICL性能方面的显著效果。

Conclusion: InSQuAD在改进上下文学习模型性能方面表现出色，尤其是在质量和多样性选择上有创新性贡献。

Abstract: In this paper, we introduce InSQuAD, designed to enhance the performance of
In-Context Learning (ICL) models through Submodular Mutual Information} (SMI)
enforcing Quality and Diversity among in-context exemplars. InSQuAD achieves
this through two principal strategies: First, we model the ICL task as a
targeted selection problem and introduce a unified selection strategy based on
SMIs which mines relevant yet diverse in-context examples encapsulating the
notions of quality and diversity. Secondly, we address a common pitfall in
existing retrieval models which model query relevance, often overlooking
diversity, critical for ICL. InSQuAD introduces a combinatorial training
paradigm which learns the parameters of an SMI function to enforce both quality
and diversity in the retrieval model through a novel likelihood-based loss. To
further aid the learning process we augment an existing multi-hop question
answering dataset with synthetically generated paraphrases. Adopting the
retrieval model trained using this strategy alongside the novel targeted
selection formulation for ICL on nine benchmark datasets shows significant
improvements validating the efficacy of our approach.

</details>


### [231] [Inference-Time Alignment Control for Diffusion Models with Reinforcement Learning Guidance](https://arxiv.org/abs/2508.21016)
*Luozhijie Jin,Zijie Qiu,Jie Liu,Zijie Diao,Lifeng Qiao,Ning Ding,Alex Lamb,Xipeng Qiu*

Main category: cs.LG

TL;DR: 提出了一种名为RLG（Reinforcement Learning Guidance）的方法，用于改进扩散模型的对齐能力，并在推理阶段通过几何平均实现动态控制。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在应对复杂目标（如人类偏好、组合精度或数据压缩度）时难度较高，同时RL方法对扩散模型存在局限性，无法灵活调控对齐强度。

Method: 将RL微调与随机微分方程和隐式奖励条件化相结合，提出一种推理阶段的RLG方法，并基于几何平均调整基础模型与RL微调模型的输出。

Result: 实验表明，RLG在多种架构、RL算法和下游任务中提升了微调模型性能，同时支持插值和外推，增强了灵活性。

Conclusion: RLG是增强和控制扩散模型对齐能力的实用且理论可靠的方法，可根据需要动态调节生成质量。

Abstract: Denoising-based generative models, particularly diffusion and flow matching
algorithms, have achieved remarkable success. However, aligning their output
distributions with complex downstream objectives, such as human preferences,
compositional accuracy, or data compressibility, remains challenging. While
reinforcement learning (RL) fine-tuning methods, inspired by advances in RL
from human feedback (RLHF) for large language models, have been adapted to
these generative frameworks, current RL approaches are suboptimal for diffusion
models and offer limited flexibility in controlling alignment strength after
fine-tuning. In this work, we reinterpret RL fine-tuning for diffusion models
through the lens of stochastic differential equations and implicit reward
conditioning. We introduce Reinforcement Learning Guidance (RLG), an
inference-time method that adapts Classifier-Free Guidance (CFG) by combining
the outputs of the base and RL fine-tuned models via a geometric average. Our
theoretical analysis shows that RLG's guidance scale is mathematically
equivalent to adjusting the KL-regularization coefficient in standard RL
objectives, enabling dynamic control over the alignment-quality trade-off
without further training. Extensive experiments demonstrate that RLG
consistently improves the performance of RL fine-tuned models across various
architectures, RL algorithms, and downstream tasks, including human
preferences, compositional control, compressibility, and text rendering.
Furthermore, RLG supports both interpolation and extrapolation, thereby
offering unprecedented flexibility in controlling generative alignment. Our
approach provides a practical and theoretically sound solution for enhancing
and controlling diffusion model alignment at inference. The source code for RLG
is publicly available at the Github:
https://github.com/jinluo12345/Reinforcement-learning-guidance.

</details>


### [232] [Fast Convergence Rates for Subsampled Natural Gradient Algorithms on Quadratic Model Problems](https://arxiv.org/abs/2508.21022)
*Gil Goldshlager,Jiang Hu,Lin Lin*

Main category: cs.LG

TL;DR: 这篇论文分析了子采样自然梯度下降（SNGD）及其加速变体SPRING的收敛特性，为之前表现出色但理论解释不足的科学机器学习优化方法提供了理论支持，并证明了SPRING可以加速SNGD。


<details>
  <summary>Details</summary>
Motivation: SNGD在科学机器学习的参数优化任务中表现出色，但缺乏理论解释。作者试图填补这一空白，分析SNGD及其加速变体SPRING的收敛特性。

Method: 通过研究线性模型和强凸的二次损失函数的理想化参数优化问题，分析SNGD和SPRING的收敛性，并证明在最小平方损失情况下，SNGD和SPRING分别等效于正则化的Kaczmarz方法及其加速变体。

Result: 在最小平方损失情况下，提出了SNGD的快速收敛率、SPRING在任何设置下的首次收敛保证，以及SPRING加速SNGD的首次证明。在一般强凸二次损失情况下，通过扩展正则化Kaczmarz方法分析，提供了更强条件下SNGD的快速收敛率。

Conclusion: SNGD和SPRING的理论分析揭示了子采样和曲率感知优化策略之间的联系，展示了随机线性代数工具在科学机器学习优化理论中的新应用。

Abstract: Subsampled natural gradient descent (SNGD) has shown impressive results for
parametric optimization tasks in scientific machine learning, such as neural
network wavefunctions and physics-informed neural networks, but it has lacked a
theoretical explanation. We address this gap by analyzing the convergence of
SNGD and its accelerated variant, SPRING, for idealized parametric optimization
problems where the model is linear and the loss function is strongly convex and
quadratic. In the special case of a least-squares loss, namely the standard
linear least-squares problem, we prove that SNGD is equivalent to a regularized
Kaczmarz method while SPRING is equivalent to an accelerated regularized
Kaczmarz method. As a result, by leveraging existing analyses we obtain under
mild conditions (i) the first fast convergence rate for SNGD, (ii) the first
convergence guarantee for SPRING in any setting, and (iii) the first proof that
SPRING can accelerate SNGD. In the case of a general strongly convex quadratic
loss, we extend the analysis of the regularized Kaczmarz method to obtain a
fast convergence rate for SNGD under stronger conditions, providing the first
explanation for the effectiveness of SNGD outside of the least-squares setting.
Overall, our results illustrate how tools from randomized linear algebra can
shed new light on the interplay between subsampling and curvature-aware
optimization strategies.

</details>


### [233] [LoTUS: Large-Scale Machine Unlearning with a Taste of Uncertainty](https://arxiv.org/abs/2503.18314)
*Christoforos N. Spartalis,Theodoros Semertzidis,Efstratios Gavves,Petros Daras*

Main category: cs.LG

TL;DR: 提出了一种新型机器学习模型中的“遗忘”方法LoTUS，通过调整模型预测概率来避免从头进行重新训练。


<details>
  <summary>Details</summary>
Motivation: 为了解决在无需全新训练的情况下，去除训练样本对预训练模型的影响。

Method: 提出LoTUS方法，通过平滑预测概率，并引入信息论边界，来缓解模型因数据记忆导致的过度自信。此外，提出了一种新评价指标RF-JSD（重训练自由的Jensen-Shannon散度）。

Result: 实验表明，LoTUS在效率和效果上优于现有最先进的方法，尤其是在处理大规模数据集（如ImageNet1k）时表现出色。

Conclusion: LoTUS方法提供了一种无需重训练但能有效“遗忘”数据影响的新方法，具备极高的实用价值。

Abstract: We present LoTUS, a novel Machine Unlearning (MU) method that eliminates the
influence of training samples from pre-trained models, avoiding retraining from
scratch. LoTUS smooths the prediction probabilities of the model up to an
information-theoretic bound, mitigating its over-confidence stemming from data
memorization. We evaluate LoTUS on Transformer and ResNet18 models against
eight baselines across five public datasets. Beyond established MU benchmarks,
we evaluate unlearning on ImageNet1k, a large-scale dataset, where retraining
is impractical, simulating real-world conditions. Moreover, we introduce the
novel Retrain-Free Jensen-Shannon Divergence (RF-JSD) metric to enable
evaluation under real-world conditions. The experimental results show that
LoTUS outperforms state-of-the-art methods in terms of both efficiency and
effectiveness. Code: https://github.com/cspartalis/LoTUS.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [234] [Particle swarm optimization for online sparse streaming feature selection under uncertainty](https://arxiv.org/abs/2508.20123)
*Ruiyang Xu*

Main category: cs.NE

TL;DR: 提出了一种称为POS2FS的框架，通过粒子群优化和三支决策理论提高了在线稀疏流特征选择的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在处理高维流数据中特征与标签的关联不确定性方面表现有限，导致模型缺乏灵活性且性能下降。

Method: 通过粒子群优化（PSO）减少特征和标签关联的不确定性，并利用三支决策理论处理特征模糊性，增强在线稀疏流特征选择性能。

Result: 在六个真实数据集的测试中，POS2FS优于传统的OSFS和OS2FS技术，表现出更高的准确性和更稳健的特征子集选择能力。

Conclusion: POS2FS框架在减少不确定性和处理特征模糊性方面显示出显著优势，是在线流特征选择问题中的一种有效工具。

Abstract: In real-world applications involving high-dimensional streaming data, online
streaming feature selection (OSFS) is widely adopted. Yet, practical
deployments frequently face data incompleteness due to sensor failures or
technical constraints. While online sparse streaming feature selection (OS2FS)
mitigates this issue via latent factor analysis-based imputation, existing
methods struggle with uncertain feature-label correlations, leading to
inflexible models and degraded performance. To address these gaps, this work
proposes POS2FS-an uncertainty-aware online sparse streaming feature selection
framework enhanced by particle swarm optimization (PSO). The approach
introduces: 1) PSO-driven supervision to reduce uncertainty in feature-label
relationships; 2) Three-way decision theory to manage feature fuzziness in
supervised learning. Rigorous testing on six real-world datasets confirms
POS2FS outperforms conventional OSFS and OS2FS techniques, delivering higher
accuracy through more robust feature subset selection.

</details>


### [235] [Improving Liver Disease Diagnosis with SNNDeep: A Custom Spiking Neural Network Using Diverse Learning Algorithms](https://arxiv.org/abs/2508.20125)
*Zofia Rudnicka,Janusz Szczepanski,Agnieszka Pregowska*

Main category: cs.NE

TL;DR: 研究构建了一个名为SNNDeep的尖峰神经网络（SNNs），用于对CT图像中的肝脏健康状况进行二分类，验证了其性能优于现有框架解决方案，能够满足医疗精确诊断需求。


<details>
  <summary>Details</summary>
Motivation: 通过利用尖峰神经网络，探索其在医疗影像中的应用，以实现更高效、更可推广的方法，从而满足高风险医疗诊断的需求。

Method: 提出了SNNDeep模型，结合三种不同的学习算法（替代梯度学习、Tempotron算法、生物启发的主动学习）在三种架构中进行对比实验，基于Optuna进行超参数优化，并以医疗分割十项全能(Task03\Liver)数据集作为评估标准。

Result: SNNDeep在验证集上的准确率达到98.35%，且优于现有尖峰神经网络框架的实现；同时展示出更好的学习规则适配性和较低的训练开销。

Conclusion: 低级别、可高度调节的尖峰神经网络在数据有限、时间敏感的医疗诊断场景中优势显著，可作为精准医疗中神经启发AI的新方向。

Abstract: Purpose: Spiking neural networks (SNNs) have recently gained attention as
energy-efficient, biologically plausible alternatives to conventional deep
learning models. Their application in high-stakes biomedical imaging remains
almost entirely unexplored. Methods: This study introduces SNNDeep, the first
tailored SNN specifically optimized for binary classification of liver health
status from computed tomography (CT) features. To ensure clinical relevance and
broad generalizability, the model was developed and evaluated using the
Task03\Liver dataset from the Medical Segmentation Decathlon (MSD), a
standardized benchmark widely used for assessing performance across diverse
medical imaging tasks. We benchmark three fundamentally different learning
algorithms, namely Surrogate Gradient Learning, the Tempotron rule, and
Bio-Inspired Active Learning across three architectural variants: a fully
customized low-level model built from scratch, and two implementations using
leading SNN frameworks, i.e., snnTorch and SpikingJelly. Hyperparameter
optimization was performed using Optuna. Results: Our results demonstrate that
the custom-built SNNDeep consistently outperforms framework-based
implementations, achieving a maximum validation accuracy of 98.35%, superior
adaptability across learning rules, and significantly reduced training
overhead. Conclusion:This study provides the first empirical evidence that
low-level, highly tunable SNNs can surpass standard frameworks in medical
imaging, especially in data-limited, temporally constrained diagnostic
settings, thereby opening a new pathway for neuro-inspired AI in precision
medicine.

</details>


### [236] [Spatio-Temporal Pruning for Compressed Spiking Large Language Models](https://arxiv.org/abs/2508.20122)
*Yi Jiang,Malyaban Bal,Brian Matejek,Susmit Jha,Adam Cobb,Abhronil Sengupta*

Main category: cs.NE

TL;DR: 本文提出了一种用于压缩尖峰大语言模型（Spiking LLMs）的时空剪枝框架，通过减少活跃神经元和注意力头的数量以及动态调整推理步骤等方式，提高了计算效率和推理速度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型因模型规模大、推理延迟高，在能量受限环境中的部署面临挑战。受启发于大脑的低功耗信息处理方式，尖峰神经网络为实现低功耗计算提供了可能，而将尖峰神经元的事件驱动效率与大型语言模型的能力相结合是研发高效LLMs的重要方向。

Method: 提出了一种时空剪枝框架，包括空间剪枝（减少活跃神经元和注意力头）和时间剪枝（动态调整推理时步），结合极端量化和知识蒸馏等技术对模型进行压缩优化。

Result: 通过在SpikingBERT上针对GLUE基准任务的实验评估，验证了该框架在计算量和推理时延方面的有效性。

Conclusion: 该方法为实时低功耗自然语言处理应用提供了有效解决方案，使尖峰LLMs在边缘设备和能量受限环境中的部署更具实用性。

Abstract: Large Language Models (LLMs) present significant challenges for deployment in
energy-constrained environments due to their large model sizes and high
inference latency. Spiking Neural Networks (SNNs), inspired by the sparse
event-driven neural processing and energy-efficient information transmission in
the brain, offer a promising alternative for achieving low-power computing.
Integrating the event-driven efficiency of spiking neurons with the advanced
capabilities of LLMs represents a promising direction for power-efficient LLMs.
This work specifically delves into the design of compressed spiking LLMs. Here,
we revisit spatial and temporal pruning from the perspective of SNNs and
propose a novel spatio-temporal pruning framework for Spiking LLMs to optimize
computational efficiency while preserving high performance. Our spatial pruning
technique reduces the number of active neurons and attention heads, effectively
lowering the computational complexity of the model. Meanwhile, temporal pruning
minimizes inference latency by dynamically adjusting the number of timesteps
required for different layers. By combining these approaches with other
compression techniques, we present the first work in the domain of Spiking LLMs
to jointly explore spatial pruning, temporal pruning, extreme quantization and
knowledge distillation strategies. Extensive experimental evaluation of our
proposed framework for SpikingBERT on the large-scale GLUE benchmark
demonstrates the efficacy of our approach in terms of computational operations
and inference latency. Our approach offers a compelling solution for real-time,
low-power natural language processing applications, making Spiking LLMs more
practical for deployment on edge devices and in power-constrained settings.

</details>
