<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 83]
- [cs.CL](#cs.CL) [Total: 26]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.LG](#cs.LG) [Total: 64]
- [cs.NE](#cs.NE) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges](https://arxiv.org/abs/2507.02074)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.CV

TL;DR: 本文综述了利用大语言模型(LLMs)进行事故检测的最新方法，提出了融合策略的分类，分析了主要的数据集和模型架构，比对了性能基准，并讨论了当前的挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 探讨如何利用最新的语言-视觉模型(VLMs)与LLMs改进交通系统中的事故检测。

Method: 综述与分类现有研究，详细总结了融合策略、关键数据集、模型架构与性能基准。

Result: 提供了一套详细的分类及各项方法的效果对比与分析，同时揭示了技术的进展与瓶颈。

Conclusion: 该综述为视频理解与基础模型交叉领域的未来研究奠定了重要基础，并指明了发展方向。

Abstract: Crash detection from video feeds is a critical problem in intelligent
transportation systems. Recent developments in large language models (LLMs) and
vision-language models (VLMs) have transformed how we process, reason about,
and summarize multimodal information. This paper surveys recent methods
leveraging LLMs for crash detection from video data. We present a structured
taxonomy of fusion strategies, summarize key datasets, analyze model
architectures, compare performance benchmarks, and discuss ongoing challenges
and opportunities. Our review provides a foundation for future research in this
fast-growing intersection of video understanding and foundation models.

</details>


### [2] [Underwater Monocular Metric Depth Estimation: Real-World Benchmarks and Synthetic Fine-Tuning](https://arxiv.org/abs/2507.02148)
*Zijie Cai,Christopher Metzler*

Main category: cs.CV

TL;DR: 本研究评估了单目深度估计模型在水下环境中的性能，提出了通过在合成的水下数据上微调模型以改进结果的方法。


<details>
  <summary>Details</summary>
Motivation: 单目深度估计在水下环境中因光衰减与散射、颜色失真等问题表现较差，而当前缺乏高质量的度量基准数据支持。

Method: 通过对现有单目深度估计模型在真实水下数据集上进行基准评估，并在合成的水下数据上微调Depth Anything V2模型以改进其表现。

Result: 微调后的模型在所有基准测试中表现更优，且超越了仅在常规数据集上训练的基线模型。

Conclusion: 本研究证明了领域适应和尺度感知监督对提高水下单目深度估计模型的鲁棒性与泛化能力的重要性，为未来研究提供了指导方向。

Abstract: Monocular depth estimation has recently advanced to provide not only relative
but also metric depth predictions. However, its reliability in underwater
environments remains limited due to light attenuation and scattering, color
distortion, turbidity, and the lack of high-quality metric ground-truth data.
In this paper, we present a comprehensive benchmark of zero-shot and fine-tuned
monocular metric depth estimation models on real-world underwater datasets with
metric depth annotations, such as FLSea and SQUID. We evaluate a diverse set of
state-of-the-art models across a range of underwater conditions with different
ranges. Our results show that large-scale models trained on terrestrial (real
or synthetic) data, while effective in in-air settings, perform poorly
underwater due to significant domain shifts. To address this, we fine-tune
Depth Anything V2 with a ViT-S backbone encoder on a synthetic underwater
variant of the Hypersim dataset, which we generated using a physically based
underwater image formation model. We demonstrate our fine-tuned model
consistently improves performance across all benchmarks and outperforms
baselines trained only on the clean in-air Hypersim dataset. Our study provides
a detailed evaluation and visualization for monocular metric depth estimation
in underwater scenes, highlighting the importance of domain adaptation and
scale-aware supervision for achieving robust and generalizable metric depth
predictions in challenging underwater environments for future research.

</details>


### [3] [ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.02200)
*Xiao Wang,Jingtao Jiang,Qiang Chen,Lan Chen,Lin Zhu,Yaowei Wang,Yonghong Tian,Jin Tang*

Main category: cs.CV

TL;DR: 提出了一种基于链式推理的事件流场景文本识别框架（ESTR-CoT），结合视觉编码器、语言模型和链式推理方法，在事件流场景文本识别的精准度和解释性上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 通过传统的事件流场景文本识别方法，仍然存在可解释性不足和上下文逻辑推理较弱的问题，因此需要一种新的框架来改善这些局限性。

Method: 使用EVA-CLIP编码器将事件流转化为视觉token，并结合Llama tokenizer处理生成提示，通过Q-former将视觉token对齐至Vicuna-7B语言模型，从而同时输出答案和链式推理过程，整个框架可以通过端到端的监督微调优化。此外，构建了一个大规模链式推理数据集，通过三阶段处理（生成、润色和专家验证）提升训练性能。

Result: 在三大事件流场景文本识别基准数据集（EventSTR、WordArt*、IC15*）上进行了大量实验，验证了提出框架的有效性与解释性。

Conclusion: 提出的ESTR-CoT框架不仅提升了事件流场景文本识别的表现和可解释性，还开发了一个大规模链式推理数据集，为后续推理相关的大模型研究提供了数据基础。

Abstract: Event stream based scene text recognition is a newly arising research topic
in recent years which performs better than the widely used RGB cameras in
extremely challenging scenarios, especially the low illumination, fast motion.
Existing works either adopt end-to-end encoder-decoder framework or large
language models for enhanced recognition, however, they are still limited by
the challenges of insufficient interpretability and weak contextual logical
reasoning. In this work, we propose a novel chain-of-thought reasoning based
event stream scene text recognition framework, termed ESTR-CoT. Specifically,
we first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input
event stream into tokens and utilize a Llama tokenizer to encode the given
generation prompt. A Q-former is used to align the vision token to the
pre-trained large language model Vicuna-7B and output both the answer and
chain-of-thought (CoT) reasoning process simultaneously. Our framework can be
optimized using supervised fine-tuning in an end-to-end manner. In addition, we
also propose a large-scale CoT dataset to train our framework via a three stage
processing (i.e., generation, polish, and expert verification). This dataset
provides a solid data foundation for the development of subsequent
reasoning-based large models. Extensive experiments on three event stream STR
benchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the
effectiveness and interpretability of our proposed framework. The source code
and pre-trained models will be released on
https://github.com/Event-AHU/ESTR-CoT.

</details>


### [4] [Team RAS in 9th ABAW Competition: Multimodal Compound Expression Recognition Approach](https://arxiv.org/abs/2507.02205)
*Elena Ryumina,Maxim Markitantov,Alexandr Axyonov,Dmitry Ryumin,Mikhail Dolgushin,Alexey Karpov*

Main category: cs.CV

TL;DR: 提出了一种结合六种异质模态的零样本多模态方法，用于复合表情识别，取得了接近监督方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有复合表情识别方法对任务特定数据依赖的问题，通过零样本方法提高泛化能力。

Method: 提出了一个结合六个模态（如静态和动态面部表情、场景匹配、音频和文本）的零样本多模态方法，使用CLIP和Qwen-VL等模型，并引入多头概率融合模块（MHPF）和复合表情转换模块（通过PPA和PFSA方法进行概率和特征聚合）。

Result: 在多语料训练下，零样本测试中分别获得AffWild2的46.95%、AFEW的49.02%和C-EXPR-DB的34.85%的F1分数，与监督方法的结果相当。

Conclusion: 该方法证明了无需领域适配即可有效捕捉复合表情情感的能力，且已开源代码，为该领域提供了新的研究方向和工具。

Abstract: Compound Expression Recognition (CER), a subfield of affective computing,
aims to detect complex emotional states formed by combinations of basic
emotions. In this work, we present a novel zero-shot multimodal approach for
CER that combines six heterogeneous modalities into a single pipeline: static
and dynamic facial expressions, scene and label matching, scene context, audio,
and text. Unlike previous approaches relying on task-specific training data,
our approach uses zero-shot components, including Contrastive Language-Image
Pretraining (CLIP)-based label matching and Qwen-VL for semantic scene
understanding. We further introduce a Multi-Head Probability Fusion (MHPF)
module that dynamically weights modality-specific predictions, followed by a
Compound Expressions (CE) transformation module that uses Pair-Wise Probability
Aggregation (PPA) and Pair-Wise Feature Similarity Aggregation (PFSA) methods
to produce interpretable compound emotion outputs. Evaluated under multi-corpus
training, the proposed approach shows F1 scores of 46.95% on AffWild2, 49.02%
on Acted Facial Expressions in The Wild (AFEW), and 34.85% on C-EXPR-DB via
zero-shot testing, which is comparable to the results of supervised approaches
trained on target data. This demonstrates the effectiveness of the proposed
approach for capturing CE without domain adaptation. The source code is
publicly available.

</details>


### [5] [SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers](https://arxiv.org/abs/2507.02212)
*Takuro Kawada,Shunsuke Kitada,Sota Nemoto,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: 本文提出SciGA-145k数据集，用于支持科学图形摘要选择、推荐及自动生成研究。


<details>
  <summary>Details</summary>
Motivation: 当前科学论文图形摘要设计难度较高且传播潜力未被充分挖掘，需要数据集和工具来提升科学可视化的能力。

Method: 构建了包含145,000篇科学论文及1.14百万张图像的SciGA-145k数据集，并定义了两个支持图形摘要设计的任务：论文内部推荐和跨论文GA推荐，同时提出了新的评估指标CAR。

Result: 提供了两任务的基线模型及其性能分析，验证了数据集和评估指标的可行性与有效性。

Conclusion: 该研究为科学可视化及AI在科学中的应用打下基础，并推动科学图形摘要的设计和传播。

Abstract: Graphical Abstracts (GAs) play a crucial role in visually conveying the key
findings of scientific papers. While recent research has increasingly
incorporated visual materials such as Figure 1 as de facto GAs, their potential
to enhance scientific communication remains largely unexplored. Moreover,
designing effective GAs requires advanced visualization skills, creating a
barrier to their widespread adoption. To tackle these challenges, we introduce
SciGA-145k, a large-scale dataset comprising approximately 145,000 scientific
papers and 1.14 million figures, explicitly designed for supporting GA
selection and recommendation as well as facilitating research in automated GA
generation. As a preliminary step toward GA design support, we define two
tasks: 1) Intra-GA recommendation, which identifies figures within a given
paper that are well-suited to serve as GAs, and 2) Inter-GA recommendation,
which retrieves GAs from other papers to inspire the creation of new GAs. We
provide reasonable baseline models for these tasks. Furthermore, we propose
Confidence Adjusted top-1 ground truth Ratio (CAR), a novel recommendation
metric that offers a fine-grained analysis of model behavior. CAR addresses
limitations in traditional ranking-based metrics by considering cases where
multiple figures within a paper, beyond the explicitly labeled GA, may also
serve as GAs. By unifying these tasks and metrics, our SciGA-145k establishes a
foundation for advancing visual scientific communication while contributing to
the development of AI for Science.

</details>


### [6] [Understanding Trade offs When Conditioning Synthetic Data](https://arxiv.org/abs/2507.02217)
*Brandon Trabucco,Qasim Wani,Benjamin Pikus,Vasu Sharma*

Main category: cs.CV

TL;DR: 本研究探索了扩散模型在生成高质量合成数据上的潜力，并比较了两种条件策略对数据质量的影响：提示条件和布局条件。


<details>
  <summary>Details</summary>
Motivation: 在工业视觉系统中，收集高质量的训练数据非常耗时，因此需要更高效的解决方案如合成数据。扩散模型可以快速生成高质量图像，但其精确控制在数据稀缺情况下具有挑战性。

Method: 研究了从四个标准目标检测基准中抽取的80种多样视觉概念，并对比了提示条件和布局条件两种策略的效果。

Result: 当条件线索的范围较窄时，提示条件生成更高质量的数据；随着多样性增加，布局条件更优。当布局条件与完整训练分布匹配时，合成数据可使检测的平均准确率（mAP）平均提高34%，最大可提升177%。

Conclusion: 提示适用于狭窄条件，布局适用于多样性更高的情况。合成数据通过适当设计能显著提高目标检测性能。

Abstract: Learning robust object detectors from only a handful of images is a critical
challenge in industrial vision systems, where collecting high quality training
data can take months. Synthetic data has emerged as a key solution for data
efficient visual inspection and pick and place robotics. Current pipelines rely
on 3D engines such as Blender or Unreal, which offer fine control but still
require weeks to render a small dataset, and the resulting images often suffer
from a large gap between simulation and reality. Diffusion models promise a
step change because they can generate high quality images in minutes, yet
precise control, especially in low data regimes, remains difficult. Although
many adapters now extend diffusion beyond plain text prompts, the effect of
different conditioning schemes on synthetic data quality is poorly understood.
We study eighty diverse visual concepts drawn from four standard object
detection benchmarks and compare two conditioning strategies: prompt based and
layout based. When the set of conditioning cues is narrow, prompt conditioning
yields higher quality synthetic data; as diversity grows, layout conditioning
becomes superior. When layout cues match the full training distribution,
synthetic data raises mean average precision by an average of thirty four
percent and by as much as one hundred seventy seven percent compared with using
real data alone.

</details>


### [7] [High-Fidelity Differential-information Driven Binary Vision Transformer](https://arxiv.org/abs/2507.02222)
*Tian Gao,Zhiyuan Zhang,Kaijie Yin,Xu-Cheng Zhong,Hui Kong*

Main category: cs.CV

TL;DR: 研究提出了一种新型的二值化视觉Transformer (ViT) —— DIDB-ViT，有效减轻了因为二值化引起的性能损失，同时保持了原始架构的计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有二值化变压器方法中存在的性能恶化和对全精度模块依赖的问题，提供适用于边缘设备的高效、轻量化解决方案。

Method: 设计了信息注意力模块，通过差分信息减缓二值化的信息丢失；使用离散Haar小波进行频率分解，为Q和K张量的二值计算相似性提供高保真度；改进RPReLU激活函数以增强模型表示能力。

Result: DIDB-ViT在多种ViT架构上显著超过了当前最先进的网络量化方法，在图像分类和分割任务中取得了优越的性能表现。

Conclusion: DIDB-ViT成功结合了高效率与高性能，为边缘设备上的二值化视觉Transformer设计提供了新的解决方案。

Abstract: The binarization of vision transformers (ViTs) offers a promising approach to
addressing the trade-off between high computational/storage demands and the
constraints of edge-device deployment. However, existing binary ViT methods
often suffer from severe performance degradation or rely heavily on
full-precision modules. To address these issues, we propose DIDB-ViT, a novel
binary ViT that is highly informative while maintaining the original ViT
architecture and computational efficiency. Specifically, we design an
informative attention module incorporating differential information to mitigate
information loss caused by binarization and enhance high-frequency retention.
To preserve the fidelity of the similarity calculations between binary Q and K
tensors, we apply frequency decomposition using the discrete Haar wavelet and
integrate similarities across different frequencies. Additionally, we introduce
an improved RPReLU activation function to restructure the activation
distribution, expanding the model's representational capacity. Experimental
results demonstrate that our DIDB-ViT significantly outperforms
state-of-the-art network quantization methods in multiple ViT architectures,
achieving superior image classification and segmentation performance.

</details>


### [8] [FMOcc: TPV-Driven Flow Matching for 3D Occupancy Prediction with Selective State Space Model](https://arxiv.org/abs/2507.02250)
*Jiangxia Chen,Tongyuan Huang,Ke Song*

Main category: cs.CV

TL;DR: FMOcc通过引入流匹配和选择性状态空间模型（SSM），提升了少帧3D语义占用预测的效率和准确率，并显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于少帧数据的局限性和3D空间冗余，当前3D语义占用预测在远距离和遮挡场景中的表现较差，且现有方法对历史数据的依赖导致计算资源消耗较大。

Method: 提出了FMOcc模型，其中包括基于流匹配的特征优化模块（FMSSM）、三视图SSM层和平面选择性SSM（PS3M），以及Mask Training方法，用于优化特征生成、提升远距离场景预测能力并增强鲁棒性。

Result: 在Occ3D-nuScenes和OpenOcc数据集上，FMOcc取得了显著的表现提升，例如在Occ3D-nuScenes验证集上以两帧输入达到43.1% RayIoU和39.8% mIoU。

Conclusion: FMOcc通过创新结构显著提升了3D语义占用预测的效率和效果，尤其在计算资源有限的情况下表现突出。

Abstract: 3D semantic occupancy prediction plays a pivotal role in autonomous driving.
However, inherent limitations of fewframe images and redundancy in 3D space
compromise prediction accuracy for occluded and distant scenes. Existing
methods enhance performance by fusing historical frame data, which need
additional data and significant computational resources. To address these
issues, this paper propose FMOcc, a Tri-perspective View (TPV) refinement
occupancy network with flow matching selective state space model for few-frame
3D occupancy prediction. Firstly, to generate missing features, we designed a
feature refinement module based on a flow matching model, which is called Flow
Matching SSM module (FMSSM). Furthermore, by designing the TPV SSM layer and
Plane Selective SSM (PS3M), we selectively filter TPV features to reduce the
impact of air voxels on non-air voxels, thereby enhancing the overall
efficiency of the model and prediction capability for distant scenes. Finally,
we design the Mask Training (MT) method to enhance the robustness of FMOcc and
address the issue of sensor data loss. Experimental results on the
Occ3D-nuScenes and OpenOcc datasets show that our FMOcc outperforms existing
state-of-theart methods. Our FMOcc with two frame input achieves notable scores
of 43.1% RayIoU and 39.8% mIoU on Occ3D-nuScenes validation, 42.6% RayIoU on
OpenOcc with 5.4 G inference memory and 330ms inference time.

</details>


### [9] [SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement](https://arxiv.org/abs/2507.02252)
*Zeyu Lei,Hongyuan Yu,Jinlin Wu,Zhen Chen*

Main category: cs.CV

TL;DR: 本文提出了一种智能外科视觉代理SurgVisAgent，利用多模态大语言模型（MLLMs）处理复杂外科场景中的图像增强任务。


<details>
  <summary>Details</summary>
Motivation: 现有的高级增强算法多只能完成单一任务，无法应对复杂的实际外科场景需求，为解决此问题，研发SurgVisAgent以提供多任务支持。

Method: 基于MLLMs的端到端模型，可动态识别内窥图像的失真类别和严重程度，结合领域特定知识以及少样本学习和链式推理技术，提供针对性图像增强方案。

Result: 在模拟真实外科失真数据集上的实验表明，SurgVisAgent显著超越传统单一任务模型。

Conclusion: SurgVisAgent能够进行多任务的图像增强，为外科手术提供了统一解决方案的潜力。

Abstract: Precise surgical interventions are vital to patient safety, and advanced
enhancement algorithms have been developed to assist surgeons in
decision-making. Despite significant progress, these algorithms are typically
designed for single tasks in specific scenarios, limiting their effectiveness
in complex real-world situations. To address this limitation, we propose
SurgVisAgent, an end-to-end intelligent surgical vision agent built on
multimodal large language models (MLLMs). SurgVisAgent dynamically identifies
distortion categories and severity levels in endoscopic images, enabling it to
perform a variety of enhancement tasks such as low-light enhancement,
overexposure correction, motion blur elimination, and smoke removal.
Specifically, to achieve superior surgical scenario understanding, we design a
prior model that provides domain-specific knowledge. Additionally, through
in-context few-shot learning and chain-of-thought (CoT) reasoning, SurgVisAgent
delivers customized image enhancements tailored to a wide range of distortion
types and severity levels, thereby addressing the diverse requirements of
surgeons. Furthermore, we construct a comprehensive benchmark simulating
real-world surgical distortions, on which extensive experiments demonstrate
that SurgVisAgent surpasses traditional single-task models, highlighting its
potential as a unified solution for surgical assistance.

</details>


### [10] [Multi-Label Classification Framework for Hurricane Damage Assessment](https://arxiv.org/abs/2507.02265)
*Zhangding Liu,Neda Mohammadi,John E. Taylor*

Main category: cs.CV

TL;DR: 该研究提出了一种新颖的多标签分类框架，通过航空影像评估飓风后的多种损害类型，精度显著高于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统单标签分类方法无法完整捕捉飓风损害的复杂性，需要更先进的方法来提升灾害响应效率。

Method: 该方法结合ResNet特征提取模块和类别特定注意力机制，从航空影像中识别单张图片中的多个损害类型。

Result: 在飓风Michael的Rescuenet数据集上，提出的方法实现了90.23%的平均精度，优于现有基线方法。

Conclusion: 该框架能够增强飓风后损害评估，有助于更准确的灾害响应和未来的减灾策略，并已被i3CE 2025会议录用。

Abstract: Hurricanes cause widespread destruction, resulting in diverse damage types
and severities that require timely and accurate assessment for effective
disaster response. While traditional single-label classification methods fall
short of capturing the complexity of post-hurricane damage, this study
introduces a novel multi-label classification framework for assessing damage
using aerial imagery. The proposed approach integrates a feature extraction
module based on ResNet and a class-specific attention mechanism to identify
multiple damage types within a single image. Using the Rescuenet dataset from
Hurricane Michael, the proposed method achieves a mean average precision of
90.23%, outperforming existing baseline methods. This framework enhances
post-hurricane damage assessment, enabling more targeted and efficient disaster
response and contributing to future strategies for disaster mitigation and
resilience. This paper has been accepted at the ASCE International Conference
on Computing in Civil Engineering (i3CE 2025), and the camera-ready version
will appear in the official conference proceedings.

</details>


### [11] [Cross-domain Hyperspectral Image Classification based on Bi-directional Domain Adaptation](https://arxiv.org/abs/2507.02268)
*Yuxiang Zhang,Wei Li,Wen Jia,Mengmeng Zhang,Ran Tao,Shunlin Liang*

Main category: cs.CV

TL;DR: 该论文提出了一种双向域适应（BiDA）框架，用于跨域高光谱图像分类，通过独立自适应空间中的特征提取显著提升模型的适应性和分离性。


<details>
  <summary>Details</summary>
Motivation: 解决高光谱图像分类中因跨区域或跨时间使得同一类别光谱特性产生显著差异的问题。

Method: 设计双向域适应（BiDA）框架，采用三分支架构（源域、目标域、耦合分支）和语义标记器；引入耦合多头交互注意力机制及双向蒸馏损失；提出自适应增强策略。

Result: 在空气和卫星数据集上的时间或场景转移实验中，BiDA优于多种最先进的方法。在跨时间树种分类中，比先进方法高出3%-5%。

Conclusion: BiDA框架通过域不变特征与域特异信息提取，显著提高了跨域高光谱图像分类的适应性与准确性，实验结果表明其优越性。

Abstract: Utilizing hyperspectral remote sensing technology enables the extraction of
fine-grained land cover classes. Typically, satellite or airborne images used
for training and testing are acquired from different regions or times, where
the same class has significant spectral shifts in different scenes. In this
paper, we propose a Bi-directional Domain Adaptation (BiDA) framework for
cross-domain hyperspectral image (HSI) classification, which focuses on
extracting both domain-invariant features and domain-specific information in
the independent adaptive space, thereby enhancing the adaptability and
separability to the target scene. In the proposed BiDA, a triple-branch
transformer architecture (the source branch, target branch, and coupled branch)
with semantic tokenizer is designed as the backbone. Specifically, the source
branch and target branch independently learn the adaptive space of source and
target domains, a Coupled Multi-head Cross-attention (CMCA) mechanism is
developed in coupled branch for feature interaction and inter-domain
correlation mining. Furthermore, a bi-directional distillation loss is designed
to guide adaptive space learning using inter-domain correlation. Finally, we
propose an Adaptive Reinforcement Strategy (ARS) to encourage the model to
focus on specific generalized feature extraction within both source and target
scenes in noise condition. Experimental results on cross-temporal/scene
airborne and satellite datasets demonstrate that the proposed BiDA performs
significantly better than some state-of-the-art domain adaptation approaches.
In the cross-temporal tree species classification task, the proposed BiDA is
more than 3\%$\sim$5\% higher than the most advanced method. The codes will be
available from the website:
https://github.com/YuxiangZhang-BIT/IEEE_TCSVT_BiDA.

</details>


### [12] [MAC-Lookup: Multi-Axis Conditional Lookup Model for Underwater Image Enhancement](https://arxiv.org/abs/2507.02270)
*Fanghai Yi,Zehong Zheng,Zexiao Liang,Yihang Dong,Xiyang Fang,Wangyu Wu,Xuhang Chen*

Main category: cs.CV

TL;DR: 文章介绍了一个名为MAC-Lookup的模型，用于提升水下图像的视觉质量，包括颜色准确性、清晰度和对比度。


<details>
  <summary>Details</summary>
Motivation: 传统方法和深度学习模型在水下图像增强中受到数据集质量和技术局限性的问题，需要一种既能校正颜色又能增强细节的解决方案。

Method: 提出了MAC-Lookup模型，包含条件三维查找表颜色修正模块（CLTCC）和多轴自适应增强模块（MAAE），分别用于初步颜色校正和细节精细化。

Result: 实验证明该模型在修复颜色和细节方面优于现有方法，显著提升了水下图像的质量。

Conclusion: MAC-Lookup模型有效应对了水下图像增强的挑战，能防止过度增强和饱和问题，为水下图像处理提供了可靠方案。

Abstract: Enhancing underwater images is crucial for exploration. These images face
visibility and color issues due to light changes, water turbidity, and bubbles.
Traditional prior-based methods and pixel-based methods often fail, while deep
learning lacks sufficient high-quality datasets. We introduce the Multi-Axis
Conditional Lookup (MAC-Lookup) model, which enhances visual quality by
improving color accuracy, sharpness, and contrast. It includes Conditional 3D
Lookup Table Color Correction (CLTCC) for preliminary color and quality
correction and Multi-Axis Adaptive Enhancement (MAAE) for detail refinement.
This model prevents over-enhancement and saturation while handling underwater
challenges. Extensive experiments show that MAC-Lookup excels in enhancing
underwater images by restoring details and colors better than existing methods.
The code is https://github.com/onlycatdoraemon/MAC-Lookup.

</details>


### [13] [Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation](https://arxiv.org/abs/2507.02271)
*Feizhen Huang,Yu Wu,Yutian Lin,Bo Du*

Main category: cs.CV

TL;DR: 该论文提出了一种改进视频转音频(V2A)模型的方法，解决现有模型在电影语言（如部分可见物体音效）场景表现不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 当前V2A模型未能充分考虑电影语言这一艺术表达形式，在目标物体部分可见的场景中性能较差，亟需改进。

Method: 通过一种简单的自蒸馏方法，模拟电影语言的变化，使学生模型可以学习将具有相同视听对应关系的训练对齐，从而更好地捕捉声音与部分视觉信息之间的关联。

Result: 该方法不仅在部分可见场景下显著提升所有评估指标的性能，还增强了在大规模V2A数据集VGGSound上的整体表现。

Conclusion: 提出的方法为电影语境中的V2A生成提供了有效解决方案，显著提升了模型的泛化能力和表现。

Abstract: Video-to-Audio (V2A) Generation achieves significant progress and plays a
crucial role in film and video post-production. However, current methods
overlook the cinematic language, a critical component of artistic expression in
filmmaking. As a result, their performance deteriorates in scenarios where
Foley targets are only partially visible. To address this challenge, we propose
a simple self-distillation approach to extend V2A models to cinematic language
scenarios. By simulating the cinematic language variations, the student model
learns to align the video features of training pairs with the same audio-visual
correspondences, enabling it to effectively capture the associations between
sounds and partial visual information. Our method not only achieves impressive
improvements under partial visibility across all evaluation metrics, but also
enhances performance on the large-scale V2A dataset, VGGSound.

</details>


### [14] [LaCo: Efficient Layer-wise Compression of Visual Tokens for Multimodal Large Language Models](https://arxiv.org/abs/2507.02279)
*Juntao Liu,Liqiang Niu,Wenchao Chen,Jie Zhou,Fandong Meng*

Main category: cs.CV

TL;DR: 引入LaCo框架，在视觉编码器的中间层实现有效的视觉token压缩，提升效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉token压缩方法局限于后编码器模块，难以显著提高效率。

Method: 提出了LaCo框架，包含逐层像素混洗机制和基于非参数捷径的残差学习架构，在视觉编码器的中间层进行token压缩并保留关键信息。

Result: 实验表明，LaCo在中间层视觉token压缩中性能优于现有方法，同时训练效率提高逾20％，推理吞吐提升超15％。

Conclusion: LaCo方法解决了现有方法的局限性，实现了效率与性能的双重提升。

Abstract: Existing visual token compression methods for Multimodal Large Language
Models (MLLMs) predominantly operate as post-encoder modules, limiting their
potential for efficiency gains. To address this limitation, we propose LaCo
(Layer-wise Visual Token Compression), a novel framework that enables effective
token compression within the intermediate layers of the vision encoder. LaCo
introduces two core components: 1) a layer-wise pixel-shuffle mechanism that
systematically merges adjacent tokens through space-to-channel transformations,
and 2) a residual learning architecture with non-parametric shortcuts that
preserves critical visual information during compression. Extensive experiments
indicate that our LaCo outperforms all existing methods when compressing tokens
in the intermediate layers of the vision encoder, demonstrating superior
effectiveness. In addition, compared to external compression, our method
improves training efficiency beyond 20% and inference throughput over 15% while
maintaining strong performance.

</details>


### [15] [Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization](https://arxiv.org/abs/2507.02288)
*De Cheng,Zhipeng Xu,Xinyang Jiang,Dongsheng Li,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: 本文提出一种新的针对领域泛化问题的框架，通过文本特征引导视觉提示调整，并结合最差显式表示对齐策略，有效提升模型在多个领域数据集上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉基础模型的领域提示调优方法在解耦多个领域的领域不变特性方面存在不足，因此需要更有效的提示设计来改进泛化能力。

Method: 提出以大语言模型自动解耦文本提示的方式，通过文本特征指导视觉提示调优，并引入最差显式表示对齐策略和抽象提示以增加源域多样性并强化对齐约束。

Result: 在PACS、VLCS、OfficeHome、DomainNet和TerraInc等主要数据集上的实验表明，本文方法优于当前最先进的领域泛化方法。

Conclusion: 利用文本特征的可控性和灵活性，对视觉提示进行引导调整，并通过对齐策略扩展提示多样性，可以提升领域泛化性能，充分验证了方法的有效性。

Abstract: Domain Generalization (DG) seeks to develop a versatile model capable of
performing effectively on unseen target domains. Notably, recent advances in
pre-trained Visual Foundation Models (VFMs), such as CLIP, have demonstrated
considerable potential in enhancing the generalization capabilities of deep
learning models. Despite the increasing attention toward VFM-based domain
prompt tuning within DG, the effective design of prompts capable of
disentangling invariant features across diverse domains remains a critical
challenge. In this paper, we propose addressing this challenge by leveraging
the controllable and flexible language prompt of the VFM. Noting that the text
modality of VFMs is naturally easier to disentangle, we introduce a novel
framework for text feature-guided visual prompt tuning. This framework first
automatically disentangles the text prompt using a large language model (LLM)
and then learns domain-invariant visual representation guided by the
disentangled text feature. However, relying solely on language to guide visual
feature disentanglement has limitations, as visual features can sometimes be
too complex or nuanced to be fully captured by descriptive text. To address
this, we introduce Worst Explicit Representation Alignment (WERA), which
extends text-guided visual prompts by incorporating an additional set of
abstract prompts. These prompts enhance source domain diversity through
stylized image augmentations, while alignment constraints ensure that visual
representations remain consistent across both the original and augmented
distributions. Experiments conducted on major DG datasets, including PACS,
VLCS, OfficeHome, DomainNet, and TerraInc, demonstrate that our proposed method
outperforms state-of-the-art DG methods.

</details>


### [16] [ViRefSAM: Visual Reference-Guided Segment Anything Model for Remote Sensing Segmentation](https://arxiv.org/abs/2507.02294)
*Hanbo Bi,Yulong Xu,Ya Li,Yongqiang Mao,Boyuan Tong,Chongyang Li,Chunbo Lang,Wenhui Diao,Hongqi Wang,Yingchao Feng,Xian Sun*

Main category: cs.CV

TL;DR: 提出了ViRefSAM框架，通过利用少量参考图像来指导SAM模型，实现了在遥感图像中无需人工提示的自动化分割。


<details>
  <summary>Details</summary>
Motivation: 现有的SAM模型在处理遥感图像时存在两大问题：人工生成提示不便且效率低下，以及对遥感领域的适应能力较差。

Method: 提出ViRefSAM框架，其中包含视觉上下文提示编码器和动态目标对齐适配器两个关键组件，前者从参考图像提取语义线索并生成提示，后者通过注入语义特征缩小领域差距。

Result: 在iSAID-5^i、LoveDA-2^i和COCO-20^i等三个少样本分割基准上，ViRefSAM在多种数据集上实现了准确的自动化分割并优于现有方法。

Conclusion: ViRefSAM通过参考图像与SAM结合，有效解决了遥感图像分割中的人工提示构建难题和领域间适应性问题。

Abstract: The Segment Anything Model (SAM), with its prompt-driven paradigm, exhibits
strong generalization in generic segmentation tasks. However, applying SAM to
remote sensing (RS) images still faces two major challenges. First, manually
constructing precise prompts for each image (e.g., points or boxes) is
labor-intensive and inefficient, especially in RS scenarios with dense small
objects or spatially fragmented distributions. Second, SAM lacks domain
adaptability, as it is pre-trained primarily on natural images and struggles to
capture RS-specific semantics and spatial characteristics, especially when
segmenting novel or unseen classes. To address these issues, inspired by
few-shot learning, we propose ViRefSAM, a novel framework that guides SAM
utilizing only a few annotated reference images that contain class-specific
objects. Without requiring manual prompts, ViRefSAM enables automatic
segmentation of class-consistent objects across RS images. Specifically,
ViRefSAM introduces two key components while keeping SAM's original
architecture intact: (1) a Visual Contextual Prompt Encoder that extracts
class-specific semantic clues from reference images and generates object-aware
prompts via contextual interaction with target images; and (2) a Dynamic Target
Alignment Adapter, integrated into SAM's image encoder, which mitigates the
domain gap by injecting class-specific semantics into target image features,
enabling SAM to dynamically focus on task-relevant regions. Extensive
experiments on three few-shot segmentation benchmarks, including iSAID-5$^i$,
LoveDA-2$^i$, and COCO-20$^i$, demonstrate that ViRefSAM enables accurate and
automatic segmentation of unseen classes by leveraging only a few reference
images and consistently outperforms existing few-shot segmentation methods
across diverse datasets.

</details>


### [17] [DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation](https://arxiv.org/abs/2507.02299)
*Yunhan Yang,Shuo Chen,Yukun Huang,Xiaoyang Wu,Yuan-Chen Guo,Edmund Y. Lam,Hengshuang Zhao,Tong He,Xihui Liu*

Main category: cs.CV

TL;DR: DreamComposer++ 通过加入多视图条件，提升了当前基于扩散模型的视图生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有的单视图生成模型缺乏多视图信息输入，导致生成的视图缺乏可控性。

Method: 提出 DreamComposer++ 框架，通过“视图感知的 3D 提升模块”和“多视图特征融合模块”，将多视图信息转化为目标视图的潜在特征，并通过扩散模型生成新视图。

Result: DreamComposer++ 与最新的视图感知扩散模型无缝结合，实现了基于多视图条件的可控新视图生成。

Conclusion: DreamComposer++ 有效提升了新视图生成的可控性，为 3D 对象重建及其它应用领域带来了新机会。

Abstract: Recent advancements in leveraging pre-trained 2D diffusion models achieve the
generation of high-quality novel views from a single in-the-wild image.
However, existing works face challenges in producing controllable novel views
due to the lack of information from multiple views. In this paper, we present
DreamComposer++, a flexible and scalable framework designed to improve current
view-aware diffusion models by incorporating multi-view conditions.
Specifically, DreamComposer++ utilizes a view-aware 3D lifting module to
extract 3D representations of an object from various views. These
representations are then aggregated and rendered into the latent features of
target view through the multi-view feature fusion module. Finally, the obtained
features of target view are integrated into pre-trained image or video
diffusion models for novel view synthesis. Experimental results demonstrate
that DreamComposer++ seamlessly integrates with cutting-edge view-aware
diffusion models and enhances their abilities to generate controllable novel
views from multi-view conditions. This advancement facilitates controllable 3D
object reconstruction and enables a wide range of applications.

</details>


### [18] [Flow-CDNet: A Novel Network for Detecting Both Slow and Fast Changes in Bitemporal Images](https://arxiv.org/abs/2507.02307)
*Haoxuan Li,Chenxu Wei,Haodong Wang,Xiaomeng Hu,Boyuan An,Lingyan Ran,Baosen Zhang,Jin Jin,Omirzhan Taukebayev,Amirkhan Temirbayev,Junrui Liu,Xiuwei Zhang*

Main category: cs.CV

TL;DR: 提出Flow-CDNet检测网络，包括光流分支和二元变化检测分支，便于同时检测缓慢和快速变化，最终在自建的Flow-Change数据集上取得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，缓慢变化同样重要，这些变化往往是重大灾害的先兆，因此亟需设计一种能够同时检测缓慢和快速变化的网络。

Method: 提出了Flow-CDNet网络，包括光流分支和二元变化检测分支，并设计了Flow-Change数据集、结合二元tversky损失和L2范数损失的损失函数、以及新评估指标FEPE。

Result: 实验表明，Flow-CDNet在Flow-Change数据集上优于现有方法，且消融实验验证了两分支的交互提升性能效果。

Conclusion: Flow-CDNet能够有效检测缓慢和快速变化，本研究提供了针对变化检测的新方法和新数据集，具有一定的实际应用价值。

Abstract: Change detection typically involves identifying regions with changes between
bitemporal images taken at the same location. Besides significant changes, slow
changes in bitemporal images are also important in real-life scenarios. For
instance, weak changes often serve as precursors to major hazards in scenarios
like slopes, dams, and tailings ponds. Therefore, designing a change detection
network that simultaneously detects slow and fast changes presents a novel
challenge. In this paper, to address this challenge, we propose a change
detection network named Flow-CDNet, consisting of two branches: optical flow
branch and binary change detection branch. The first branch utilizes a pyramid
structure to extract displacement changes at multiple scales. The second one
combines a ResNet-based network with the optical flow branch's output to
generate fast change outputs. Subsequently, to supervise and evaluate this new
change detection framework, a self-built change detection dataset Flow-Change,
a loss function combining binary tversky loss and L2 norm loss, along with a
new evaluation metric called FEPE are designed. Quantitative experiments
conducted on Flow-Change dataset demonstrated that our approach outperforms the
existing methods. Furthermore, ablation experiments verified that the two
branches can promote each other to enhance the detection performance.

</details>


### [19] [LMPNet for Weakly-supervised Keypoint Discovery](https://arxiv.org/abs/2507.02308)
*Pei Guo,Ryan Farrell*

Main category: cs.CV

TL;DR: 本文探索了通过仅使用类别标签进行弱监督的语义关键点检测任务，提出了新的计算层和选择策略，模型命名为LMPNet，实现了语义关键点自动发现和高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 当前的关键点检测主要依赖于监督学习，大量标签的需求限制了其实际应用。

Method: 利用LMP层鼓励非重复的局部特征学习，并应用注意力掩模与可学习的聚类层进行最终关键点预测。

Result: LMPNet能够自动发现语义关键点，对物体姿态具有较强鲁棒性，并达到了与监督姿态估计模型相当的预测准确性。

Conclusion: 提出的LMPNet网络模型对关键点检测方法提供了新的思路，展现了强大的性能与解释性。

Abstract: In this work, we explore the task of semantic object keypoint discovery
weakly-supervised by only category labels. This is achieved by transforming
discriminatively-trained intermediate layer filters into keypoint detectors. We
begin by identifying three preferred characteristics of keypoint detectors: (i)
spatially sparse activations, (ii) consistency and (iii) diversity. Instead of
relying on hand-crafted loss terms, a novel computationally-efficient leaky max
pooling (LMP) layer is proposed to explicitly encourage final conv-layer
filters to learn "non-repeatable local patterns" that are well aligned with
object keypoints. Informed by visualizations, a simple yet effective selection
strategy is proposed to ensure consistent filter activations and attention
mask-out is then applied to force the network to distribute its attention to
the whole object instead of just the most discriminative region. For the final
keypoint prediction, a learnable clustering layer is proposed to group keypoint
proposals into keypoint predictions. The final model, named LMPNet, is highly
interpretable in that it directly manipulates network filters to detect
predefined concepts. Our experiments show that LMPNet can (i) automatically
discover semantic keypoints that are robust to object pose and (ii) achieves
strong prediction accuracy comparable to a supervised pose estimation model.

</details>


### [20] [Perception Activator: An intuitive and portable framework for brain cognitive exploration](https://arxiv.org/abs/2507.02311)
*Le Xu,Qi Zhang,Qixian Zhang,Hongyun Zhang,Duoqian Miao,Cairong Zhao*

Main category: cs.CV

TL;DR: 该研究提出了一种利用fMRI信号的实验框架以改进视觉解码任务，发现fMRI提供了丰富的多物体语义信息与粗略空间定位。


<details>
  <summary>Details</summary>
Motivation: 现有脑视觉解码方法语义对齐不足，数据重建出现多语义物体失真，本文旨在探索fMRI信号中潜在的语义与空间信息。

Method: 通过交叉注意，将fMRI表示注入多尺度图像特征，比较其对物体检测与实例分割任务的效果，分析信号对下游任务与特征的影响。

Result: 整合fMRI信号显著提高了检测与分割精度，表明fMRI提供了未被充分利用的语义与定位 cues。

Conclusion: 此研究证明fMRI信号可提升解码任务表现，未来可进一步挖掘其潜力以改善复杂语义任务的准确性。

Abstract: Recent advances in brain-vision decoding have driven significant progress,
reconstructing with high fidelity perceived visual stimuli from neural
activity, e.g., functional magnetic resonance imaging (fMRI), in the human
visual cortex. Most existing methods decode the brain signal using a two-level
strategy, i.e., pixel-level and semantic-level. However, these methods rely
heavily on low-level pixel alignment yet lack sufficient and fine-grained
semantic alignment, resulting in obvious reconstruction distortions of multiple
semantic objects. To better understand the brain's visual perception patterns
and how current decoding models process semantic objects, we have developed an
experimental framework that uses fMRI representations as intervention
conditions. By injecting these representations into multi-scale image features
via cross-attention, we compare both downstream performance and intermediate
feature changes on object detection and instance segmentation tasks with and
without fMRI information. Our results demonstrate that incorporating fMRI
signals enhances the accuracy of downstream detection and segmentation,
confirming that fMRI contains rich multi-object semantic cues and coarse
spatial localization information-elements that current models have yet to fully
exploit or integrate.

</details>


### [21] [MAGIC: Mask-Guided Diffusion Inpainting with Multi-Level Perturbations and Context-Aware Alignment for Few-Shot Anomaly Generation](https://arxiv.org/abs/2507.02314)
*JaeHyuck Choi,MinJun Kim,JeHyeong Hong*

Main category: cs.CV

TL;DR: 本论文提出了一种名为MAGIC的方法，用于增强工业质量控制中稀缺的异常数据生成。


<details>
  <summary>Details</summary>
Motivation: 当前工业质量控制中缺乏高质量的异常数据生成方法，现有方法常在背景保留、异常区域生成以及语义合理性三方面有欠缺。

Method: 提出MAGIC方法，通过精调Stable Diffusion模型，实现正常区域的背景保留与异常区域的高精度贴合。同时引入提示层高斯扰动和掩码引导的空间噪声注入，增加生成的多样性。语义上下文对齐模块进一步确保异常位置的合理性并解决边界问题。

Result: 在MVTec-AD数据集上的一致协议评估中，MAGIC在下游异常检测任务中超越现有方法，性能显著更优。

Conclusion: MAGIC方法成功解决了背景保留、异常区域精确生成以及语义合理性三方面的挑战，为工业异常检测提供了更加实用的解决方案。

Abstract: Few-shot anomaly generation is emerging as a practical solution for
augmenting the scarce anomaly data in industrial quality control settings. An
ideal generator would meet three demands at once, namely (i) keep the normal
background intact, (ii) inpaint anomalous regions to tightly overlap with the
corresponding anomaly masks, and (iii) generate anomalous regions in a
semantically valid location, while still producing realistic, diverse
appearances from only a handful of real examples. Existing diffusion-based
methods usually satisfy at most two of these requirements: global anomaly
generators corrupt the background, whereas mask-guided ones often falter when
the mask is imprecise or misplaced. We propose MAGIC--Mask-guided inpainting
with multi-level perturbations and Context-aware alignment--to resolve all
three issues. At its core, MAGIC fine-tunes a Stable Diffusion inpainting
backbone that preserves normal regions and ensures strict adherence of the
synthesized anomaly to the supplied mask, directly addressing background
corruption and misalignment. To offset the diversity loss that fine-tuning can
cause, MAGIC adds two complementary perturbation strategies: (i) Gaussian
prompt-level perturbation applied during fine-tuning and inference that
broadens the global appearance of anomalies while avoiding low-fidelity textual
appearances, and (ii) mask-guided spatial noise injection that enriches local
texture variations. Additionally, the context-aware mask alignment module forms
semantic correspondences and relocates masks so that every anomaly remains
plausibly contained within the host object, eliminating out-of-boundary
artifacts. Under a consistent identical evaluation protocol on the MVTec-AD
dataset, MAGIC outperforms previous state-of-the-arts in downstream anomaly
tasks.

</details>


### [22] [Are Synthetic Videos Useful? A Benchmark for Retrieval-Centric Evaluation of Synthetic Videos](https://arxiv.org/abs/2507.02316)
*Zecheng Zhao,Selena Song,Tong Chen,Zhi Chen,Shazia Sadiq,Yadan Luo*

Main category: cs.CV

TL;DR: 本文提出了一个新的数据集和基准SynTVA，用于评估合成视频在下游任务中的效用，特别是文本到视频检索（TVR）。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到视频（T2V）合成技术发展迅速，现有的评估指标主要关注视觉质量和时间一致性，难以充分评估合成视频在下游任务（如TVR）中的表现。作者旨在解决这一评价空白。

Method: 开发了SynTVA数据集，基于800个来自MSRVTT的用户查询，结合先进T2V模型生成视频，并注释视频与文本的语义对齐情况。提出了自动评估器（Auto-Evaluator）用于从现有指标预测对齐质量。

Result: SynTVA框架显示了其对数据集扩充的价值，能够帮助选择高效能合成样本，从而显著提升TVR结果。

Conclusion: SynTVA不仅提供了一个评估合成视频语义对齐的工具，还证明了其在增强数据集能力、支持下游任务中的实际效用。

Abstract: Text-to-video (T2V) synthesis has advanced rapidly, yet current evaluation
metrics primarily capture visual quality and temporal consistency, offering
limited insight into how synthetic videos perform in downstream tasks such as
text-to-video retrieval (TVR). In this work, we introduce SynTVA, a new dataset
and benchmark designed to evaluate the utility of synthetic videos for building
retrieval models. Based on 800 diverse user queries derived from MSRVTT
training split, we generate synthetic videos using state-of-the-art T2V models
and annotate each video-text pair along four key semantic alignment dimensions:
Object \& Scene, Action, Attribute, and Prompt Fidelity. Our evaluation
framework correlates general video quality assessment (VQA) metrics with these
alignment scores, and examines their predictive power for downstream TVR
performance. To explore pathways of scaling up, we further develop an
Auto-Evaluator to estimate alignment quality from existing metrics. Beyond
benchmarking, our results show that SynTVA is a valuable asset for dataset
augmentation, enabling the selection of high-utility synthetic samples that
measurably improve TVR outcomes. Project page and dataset can be found at
https://jasoncodemaker.github.io/SynTVA/.

</details>


### [23] [Heeding the Inner Voice: Aligning ControlNet Training via Intermediate Features Feedback](https://arxiv.org/abs/2507.02321)
*Nina Konovalova,Maxim Nikolaev,Andrey Kuznetsov,Aibek Alanov*

Main category: cs.CV

TL;DR: 提出InnerControl训练策略，通过全扩散步骤内实现空间一致性以改进生成质量和控制精度。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型难以实现对生成图像的精准空间控制，例如ControlNet和ControlNet++面临中间生成阶段被忽略的问题。

Method: 提出InnerControl训练策略，通过在每个去噪步骤中使用卷积探针提取中间特征，并最小化预测信号与目标信号间的差异。

Result: InnerControl方法在各种条件（例如边缘、深度）下实现了控制精度与生成质量的提升，并结合其他技术达到最新性能。

Conclusion: InnerControl通过对整个扩散过程的一致性控制，解决了之前方法中中间阶段信息利用缺失的问题，提升了文本到图像扩散模型的表现。

Abstract: Despite significant progress in text-to-image diffusion models, achieving
precise spatial control over generated outputs remains challenging. ControlNet
addresses this by introducing an auxiliary conditioning module, while
ControlNet++ further refines alignment through a cycle consistency loss applied
only to the final denoising steps. However, this approach neglects intermediate
generation stages, limiting its effectiveness. We propose InnerControl, a
training strategy that enforces spatial consistency across all diffusion steps.
Our method trains lightweight convolutional probes to reconstruct input control
signals (e.g., edges, depth) from intermediate UNet features at every denoising
step. These probes efficiently extract signals even from highly noisy latents,
enabling pseudo ground truth controls for training. By minimizing the
discrepancy between predicted and target conditions throughout the entire
diffusion process, our alignment loss improves both control fidelity and
generation quality. Combined with established techniques like ControlNet++,
InnerControl achieves state-of-the-art performance across diverse conditioning
methods (e.g., edges, depth).

</details>


### [24] [Neural Network-based Study for Rice Leaf Disease Recognition and Classification: A Comparative Analysis Between Feature-based Model and Direct Imaging Model](https://arxiv.org/abs/2507.02322)
*Farida Siddiqi Prity,Mirza Raquib,Saydul Akbar Murad,Md. Jubayar Alam Rafi,Md. Khairul Bashar Bhuiyan,Anupam Kumar Bairagi*

Main category: cs.CV

TL;DR: 本研究提出了一种基于人工神经网络（ANN）的图像处理技术，用于及时分类和识别水稻疾病，并进行特征分析检测模型（FADM）和直接图像检测模型（DICDM）的比较分析，最终发现FADM在检测水稻叶病方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前水稻叶病对生产力的显著负面影响及由此造成的经济损失突显了实现早期检测的必要性，以便提高管理效果和产量。

Method: 研究提出了一种基于ANN的技术，比较了特征分析检测模型和直接图像检测模型；同时应用了多种特征提取算法、降维算法、特征选择算法及极限学习机方法，使用了10折交叉验证法对多种水稻叶病进行实验评估。

Result: 结果显示，特征分析检测模型性能最佳，在分类水稻叶病方面相比直接图像检测模型效果更好。

Conclusion: 采用提出的特征分析检测模型在水稻叶病检测中具有极大的潜力，这可改善作物健康、减少产量损失，同时提高水稻种植的整体生产力与可持续性。

Abstract: Rice leaf diseases significantly reduce productivity and cause economic
losses, highlighting the need for early detection to enable effective
management and improve yields. This study proposes Artificial Neural Network
(ANN)-based image-processing techniques for timely classification and
recognition of rice diseases. Despite the prevailing approach of directly
inputting images of rice leaves into ANNs, there is a noticeable absence of
thorough comparative analysis between the Feature Analysis Detection Model
(FADM) and Direct Image-Centric Detection Model (DICDM), specifically when it
comes to evaluating the effectiveness of Feature Extraction Algorithms (FEAs).
Hence, this research presents initial experiments on the Feature Analysis
Detection Model, utilizing various image Feature Extraction Algorithms,
Dimensionality Reduction Algorithms (DRAs), Feature Selection Algorithms
(FSAs), and Extreme Learning Machine (ELM). The experiments are carried out on
datasets encompassing bacterial leaf blight, brown spot, leaf blast, leaf
scald, Sheath blight rot, and healthy leaf, utilizing 10-fold Cross-Validation
method. A Direct Image-Centric Detection Model is established without the
utilization of any FEA, and the evaluation of classification performance relies
on different metrics. Ultimately, an exhaustive contrast is performed between
the achievements of the Feature Analysis Detection Model and Direct
Image-Centric Detection Model in classifying rice leaf diseases. The results
reveal that the highest performance is attained using the Feature Analysis
Detection Model. The adoption of the proposed Feature Analysis Detection Model
for detecting rice leaf diseases holds excellent potential for improving crop
health, minimizing yield losses, and enhancing overall productivity and
sustainability of rice farming.

</details>


### [25] [Two-Steps Neural Networks for an Automated Cerebrovascular Landmark Detection](https://arxiv.org/abs/2507.02349)
*Rafic Nader,Vincent L'Allinec,Romain Bourcier,Florent Autrusseau*

Main category: cs.CV

TL;DR: 本文提出了一种针对Willis环动脉分叉点的全自动检测方法，通过两步神经网络实现精确定位，实验表明其在分叉点检测任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前脑内动脉瘤多发生在Willis环的重要动脉分叉点，准确检测这些关键点对诊断至关重要，但现有方法难以应对相邻分叉点视觉特征相似和解剖变异问题。

Method: 方法包括两步：首先使用目标检测网络识别ROI区域，然后利用带深度监督的改进U-Net模型精确定位分叉点，从而有效处理视觉相似性及解剖变异问题。

Result: 在两个脑MRA数据集（自有数据集和公共数据集）上进行测试，结果表明该方法在分叉点检测任务中取得了最高性能。

Conclusion: 该研究提出的两步神经网络方法不仅提高了检测精度，还有效解决了解剖变异和视觉特征相似性带来的挑战，为Willis环分叉点检测提供了高效解决方案。

Abstract: Intracranial aneurysms (ICA) commonly occur in specific segments of the
Circle of Willis (CoW), primarily, onto thirteen major arterial bifurcations.
An accurate detection of these critical landmarks is necessary for a prompt and
efficient diagnosis. We introduce a fully automated landmark detection approach
for CoW bifurcations using a two-step neural networks process. Initially, an
object detection network identifies regions of interest (ROIs) proximal to the
landmark locations. Subsequently, a modified U-Net with deep supervision is
exploited to accurately locate the bifurcations. This two-step method reduces
various problems, such as the missed detections caused by two landmarks being
close to each other and having similar visual characteristics, especially when
processing the complete MRA Time-of-Flight (TOF). Additionally, it accounts for
the anatomical variability of the CoW, which affects the number of detectable
landmarks per scan. We assessed the effectiveness of our approach using two
cerebral MRA datasets: our In-House dataset which had varying numbers of
landmarks, and a public dataset with standardized landmark configuration. Our
experimental results demonstrate that our method achieves the highest level of
performance on a bifurcation detection task.

</details>


### [26] [Lightweight Shrimp Disease Detection Research Based on YOLOv8n](https://arxiv.org/abs/2507.02354)
*Fei Yuhuan,Wang Gengchen,Liu Fenghao,Zang Ran,Sun Xufei,Chang Hao*

Main category: cs.CV

TL;DR: 本文通过提出基于YOLOv8n的轻量网络架构，实现了对虾类疾病的高效智能检测，重点优化了计算复杂度和特征提取能力，从而在平衡准确性和效率间取得突破性进展。


<details>
  <summary>Details</summary>
Motivation: 虾类疾病是虾养殖业经济损失的主要原因，亟需开发高效、智能的疾病检测技术来防止疾病传播和提高检测效率。

Method: 提出了一种基于YOLOv8n的轻量化网络架构，通过RLDD检测头、C2f-EMCM模块和改良的SegNext_Attention自注意力机制，优化了计算复杂度和特征提取能力，并在自建数据集和URPC2020数据集上验证了模型的泛化能力。

Result: 模型参数量比原始YOLOv8n减少32.3%，mAP@0.5达到92.7%，较YOLOv8n提升了3%。在URPC2020数据集上的泛化实验显示，mAP@0.5提高了4.1%。

Conclusion: 该方法在精准度和效率之间取得了最佳平衡，为虾类养殖智能病害检测提供了可靠的技术支持。

Abstract: Shrimp diseases are one of the primary causes of economic losses in shrimp
aquaculture. To prevent disease transmission and enhance intelligent detection
efficiency in shrimp farming, this paper proposes a lightweight network
architecture based on YOLOv8n. First, by designing the RLDD detection head and
C2f-EMCM module, the model reduces computational complexity while maintaining
detection accuracy, improving computational efficiency. Subsequently, an
improved SegNext_Attention self-attention mechanism is introduced to further
enhance the model's feature extraction capability, enabling more precise
identification of disease characteristics. Extensive experiments, including
ablation studies and comparative evaluations, are conducted on a
self-constructed shrimp disease dataset, with generalization tests extended to
the URPC2020 dataset. Results demonstrate that the proposed model achieves a
32.3% reduction in parameters compared to the original YOLOv8n, with a mAP@0.5
of 92.7% (3% improvement over YOLOv8n). Additionally, the model outperforms
other lightweight YOLO-series models in mAP@0.5, parameter count, and model
size. Generalization experiments on the URPC2020 dataset further validate the
model's robustness, showing a 4.1% increase in mAP@0.5 compared to YOLOv8n. The
proposed method achieves an optimal balance between accuracy and efficiency,
providing reliable technical support for intelligent disease detection in
shrimp aquaculture.

</details>


### [27] [Holistic Tokenizer for Autoregressive Image Generation](https://arxiv.org/abs/2507.02358)
*Anlin Zheng,Haochen Wang,Yucheng Zhao,Weipeng Deng,Tiancai Wang,Xiangyu Zhang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 提出一种名为Hita的新型图像标记器，采用整体到局部的标记方案，通过增强整体信息捕获能力和优化顺序生成过程，提高自回归图像生成模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统自回归图像生成模型难以捕获整体关系，现有视觉标记器大多基于局部图像块导致缺乏全局信息。

Method: 设计了Hita标记器，结合整体查询和局部标记的标记方案，加入顺序排列和因果注意力，同时使用轻量级融合模块控制信息流。

Result: 在ImageNet基准上获得2.59 FID和281.9 IS的领先表现，加速了自回归生成器训练，在零样式转移和图像修复等任务中表现优异。

Conclusion: Hita显示出增强的全局属性捕获和生成能力，为自回归图像生成提供了一种高效且精确的方法。

Abstract: The vanilla autoregressive image generation model generates visual tokens in
a step-by-step fashion, which limits the ability to capture holistic
relationships among token sequences. Moreover, most visual tokenizers map local
image patches into latent tokens, leading to limited global information. To
address this, we introduce \textit{Hita}, a novel image tokenizer for
autoregressive (AR) image generation. It introduces a holistic-to-local
tokenization scheme with learnable holistic queries and local patch tokens.
Besides, Hita incorporates two key strategies for improved alignment with the
AR generation process: 1) it arranges a sequential structure with holistic
tokens at the beginning followed by patch-level tokens while using causal
attention to maintain awareness of previous tokens; and 2) before feeding the
de-quantized tokens into the decoder, Hita adopts a lightweight fusion module
to control information flow to prioritize holistic tokens. Extensive
experiments show that Hita accelerates the training speed of AR generators and
outperforms those trained with vanilla tokenizers, achieving \textbf{2.59 FID}
and \textbf{281.9 IS} on the ImageNet benchmark. A detailed analysis of the
holistic representation highlights its ability to capture global image
properties such as textures, materials, and shapes. Additionally, Hita also
demonstrates effectiveness in zero-shot style transfer and image in-painting.
The code is available at
\href{https://github.com/CVMI-Lab/Hita}{https://github.com/CVMI-Lab/Hita}

</details>


### [28] [LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling](https://arxiv.org/abs/2507.02363)
*Jiahao Wu,Rui Peng,Jianbo Jiao,Jiayu Yang,Luyang Tang,Kaiqiang Xiong,Jie Liang,Jinbo Yan,Runling Liu,Ronggang Wang*

Main category: cs.CV

TL;DR: 本研究提出了一种名为LocalDyGS的新方法，用于从多视角输入合成任意视点的动态视频，能够更真实地重建高度动态的场景。


<details>
  <summary>Details</summary>
Motivation: 目前基于神经辐射场或3D高斯分布的方法在对复杂且高度动态的场景进行多视角建模时存在局限性，特别是在处理精细动作时，难以达成预期效果。

Method: 方法由两部分组成：(1) 将复杂的动态场景分解为由种子定义的简化局部空间，通过捕捉局部空间内的动作实现全局建模。(2) 将静态和动态特征分离建模：静态特征在跨时间步中捕捉静态信息，动态残差场提供与时间相关的动态特征，这两者结合生成时间高斯分布来建模局部动态。

Result: 实验表明，该方法在多个微细尺度的数据集上表现优异，同时率先实现对高度动态真实场景的大规模建模。

Conclusion: 该方法为动态场景重建提出了新的框架，不仅在现有方法中表现出竞争力，还首次实现对大规模复杂动态场景的建模，具有潜在应用前景。

Abstract: Due to the complex and highly dynamic motions in the real world, synthesizing
dynamic videos from multi-view inputs for arbitrary viewpoints is challenging.
Previous works based on neural radiance field or 3D Gaussian splatting are
limited to modeling fine-scale motion, greatly restricting their application.
In this paper, we introduce LocalDyGS, which consists of two parts to adapt our
method to both large-scale and fine-scale motion scenes: 1) We decompose a
complex dynamic scene into streamlined local spaces defined by seeds, enabling
global modeling by capturing motion within each local space. 2) We decouple
static and dynamic features for local space motion modeling. A static feature
shared across time steps captures static information, while a dynamic residual
field provides time-specific features. These are combined and decoded to
generate Temporal Gaussians, modeling motion within each local space. As a
result, we propose a novel dynamic scene reconstruction framework to model
highly dynamic real-world scenes more realistically. Our method not only
demonstrates competitive performance on various fine-scale datasets compared to
state-of-the-art (SOTA) methods, but also represents the first attempt to model
larger and more complex highly dynamic scenes. Project page:
https://wujh2001.github.io/LocalDyGS/.

</details>


### [29] [UVLM: Benchmarking Video Language Model for Underwater World Understanding](https://arxiv.org/abs/2507.02373)
*Xizhe Xue,Yang Zhou,Dawei Yan,Ying Li,Haokui Zhang,Rong Xiao*

Main category: cs.CV

TL;DR: 本文提出UVLM，用于填补当前视频语言模型在水下场景观察领域的空白，并展示了UVLM数据集在改进水下场景理解方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型在多个领域取得了突出进展，但视频语言模型在水下场景的应用需求尚未得到满足。

Method: 提出UVLM数据集，通过结合人工专家和AI模型的协作生成，该数据集在数据质量、场景多样性及任务设计上均进行了深度优化，并定义了一系列评估指标。

Result: 在两种标准的视频语言模型上实验，UVLM能显著提高水下场景的理解能力，同时对现有地面视频基准也显示出一定的改进潜力。

Conclusion: UVLM弥补了水下观察领域的空白，为研究多场景视频语言模型提供了重要支持，并将在未来公开数据集及提示工程。

Abstract: Recently, the remarkable success of large language models (LLMs) has achieved
a profound impact on the field of artificial intelligence. Numerous advanced
works based on LLMs have been proposed and applied in various scenarios. Among
them, video language models (VidLMs) are particularly widely used. However,
existing works primarily focus on terrestrial scenarios, overlooking the highly
demanding application needs of underwater observation. To overcome this gap, we
introduce UVLM, an under water observation benchmark which is build through a
collaborative approach combining human expertise and AI models. To ensure data
quality, we have conducted in-depth considerations from multiple perspectives.
First, to address the unique challenges of underwater environments, we selected
videos that represent typical underwater challenges including light variations,
water turbidity, and diverse viewing angles to construct the dataset. Second,
to ensure data diversity, the dataset covers a wide range of frame rates,
resolutions, 419 classes of marine animals, and various static plants and
terrains. Next, for task diversity, we adopted a structured design where
observation targets are categorized into two major classes: biological and
environmental. Each category includes content observation and change/action
observation, totaling 20 distinct task types. Finally, we designed several
challenging evaluation metrics to enable quantitative comparison and analysis
of different methods. Experiments on two representative VidLMs demonstrate that
fine-tuning VidLMs on UVLM significantly improves underwater world
understanding while also showing potential for slight improvements on existing
in-air VidLM benchmarks, such as VideoMME and Perception text. The dataset and
prompt engineering will be released publicly.

</details>


### [30] [PLOT: Pseudo-Labeling via Video Object Tracking for Scalable Monocular 3D Object Detection](https://arxiv.org/abs/2507.02393)
*Seokyeong Lee,Sithu Aung,Junyong Choi,Seungryong Kim,Ig-Jae Kim,Junghyun Cho*

Main category: cs.CV

TL;DR: 本文针对单目3D目标检测中的数据稀缺问题，提出了一种仅利用视频数据的新型伪标注框架，通过时间帧中的点云聚合实现更强的健壮性与扩展性。


<details>
  <summary>Details</summary>
Motivation: 单目3D目标检测面临数据稀缺问题，由于标注成本高及从2D到3D的固有歧义，而现有方法受限于特定领域学习或单观测形状信息。

Method: 提出了一种无需多视角配置或额外传感器的伪标注框架，通过对象点跟踪，跨时间帧聚合伪LiDAR数据，提取3D属性。

Result: 实验表明，该方法在准确性和可扩展性上表现出色，解决了3D数据采集受限的挑战。

Conclusion: 这种方法为单目3D目标检测提供了一个实用且有效的解决方案。

Abstract: Monocular 3D object detection (M3OD) has long faced challenges due to data
scarcity caused by high annotation costs and inherent 2D-to-3D ambiguity.
Although various weakly supervised methods and pseudo-labeling methods have
been proposed to address these issues, they are mostly limited by
domain-specific learning or rely solely on shape information from a single
observation. In this paper, we propose a novel pseudo-labeling framework that
uses only video data and is more robust to occlusion, without requiring a
multi-view setup, additional sensors, camera poses, or domain-specific
training. Specifically, we explore a technique for aggregating the
pseudo-LiDARs of both static and dynamic objects across temporally adjacent
frames using object point tracking, enabling 3D attribute extraction in
scenarios where 3D data acquisition is infeasible. Extensive experiments
demonstrate that our method ensures reliable accuracy and strong scalability,
making it a practical and effective solution for M3OD.

</details>


### [31] [Continual Multiple Instance Learning with Enhanced Localization for Histopathological Whole Slide Image Analysis](https://arxiv.org/abs/2507.02395)
*Byung Hyun Lee,Wongi Jeong,Woojae Han,Kyoungbun Lee,Se Young Chun*

Main category: cs.CV

TL;DR: 该论文提出了一个名为CoMEL的框架，旨在解决持续多实例学习中定位和适应性的问题，通过GDAT、BPPL和OWLoRA三种技术提升性能，实验结果显示显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多实例学习方法在处理大规模图像及保持最小遗忘的持续任务时存在局限，尤其是针对实例分类的定位问题。

Method: 论文提出了CoMEL框架，包含三个主要组件：GDAT用于高效实例编码，BPPL进行可靠的实例伪标签生成，OWLoRA用以减轻包级和实例级分类的遗忘问题。

Result: 在三个公共WSI数据集上的实验表明，CoMEL框架在包级准确率上提高了最高11.00%，在定位准确率上提高了最高23.4%。

Conclusion: CoMEL框架实现了在解决定位问题的同时，显著提高了持续多实例学习的适应性和准确性，展现了强大的性能优势。

Abstract: Multiple instance learning (MIL) significantly reduced annotation costs via
bag-level weak labels for large-scale images, such as histopathological whole
slide images (WSIs). However, its adaptability to continual tasks with minimal
forgetting has been rarely explored, especially on instance classification for
localization. Weakly incremental learning for semantic segmentation has been
studied for continual localization, but it focused on natural images,
leveraging global relationships among hundreds of small patches (e.g., $16
\times 16$) using pre-trained models. This approach seems infeasible for MIL
localization due to enormous amounts ($\sim 10^5$) of large patches (e.g., $256
\times 256$) and no available global relationships such as cancer cells. To
address these challenges, we propose Continual Multiple Instance Learning with
Enhanced Localization (CoMEL), an MIL framework for both localization and
adaptability with minimal forgetting. CoMEL consists of (1) Grouped Double
Attention Transformer (GDAT) for efficient instance encoding, (2) Bag
Prototypes-based Pseudo-Labeling (BPPL) for reliable instance pseudo-labeling,
and (3) Orthogonal Weighted Low-Rank Adaptation (OWLoRA) to mitigate forgetting
in both bag and instance classification. Extensive experiments on three public
WSI datasets demonstrate superior performance of CoMEL, outperforming the prior
arts by up to $11.00\%$ in bag-level accuracy and up to $23.4\%$ in
localization accuracy under the continual MIL setup.

</details>


### [32] [Beyond Spatial Frequency: Pixel-wise Temporal Frequency-based Deepfake Video Detection](https://arxiv.org/abs/2507.02398)
*Taehoon Kim,Jongwook Choi,Yonghyun Jeong,Haeun Noh,Jaejun Yoo,Seungryul Baek,Jongwon Choi*

Main category: cs.CV

TL;DR: 我们提出了一种基于像素级时间不一致性的深度伪造视频检测方法，通过1D傅里叶变换和注意力模块实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 传统的伪造视频检测方法无法有效捕获像素级的时间伪造特征，新的方法亟需解决这一问题。

Method: 使用像素时间轴的1D傅里叶变换提取特征，并结合注意力提议模块和联合变换模块，进行伪造检测。

Result: 提出的方法能够更好地检测伪造视频中的时间伪造特征，并展示了对多种复杂场景的鲁棒性。

Conclusion: 本研究显著提升了深度伪造视频检测的准确性和鲁棒性，特别是在像素时间不一致性检测任务上。

Abstract: We introduce a deepfake video detection approach that exploits pixel-wise
temporal inconsistencies, which traditional spatial frequency-based detectors
often overlook. Traditional detectors represent temporal information merely by
stacking spatial frequency spectra across frames, resulting in the failure to
detect temporal artifacts in the pixel plane. Our approach performs a 1D
Fourier transform on the time axis for each pixel, extracting features highly
sensitive to temporal inconsistencies, especially in areas prone to unnatural
movements. To precisely locate regions containing the temporal artifacts, we
introduce an attention proposal module trained in an end-to-end manner.
Additionally, our joint transformer module effectively integrates pixel-wise
temporal frequency features with spatio-temporal context features, expanding
the range of detectable forgery artifacts. Our framework represents a
significant advancement in deepfake video detection, providing robust
performance across diverse and challenging detection scenarios.

</details>


### [33] [From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding](https://arxiv.org/abs/2507.02790)
*Xiangfeng Wang,Xiao Li,Yadong Wei,Xueyu Song,Yang Song,Xiaoqiang Xia,Fangrui Zeng,Zaiyi Chen,Liu Liu,Gu Xu,Tong Xu*

Main category: cs.CV

TL;DR: 该研究提出了一种名为HIVE的自动视频编辑框架，用于从长视频中生成简洁吸引人的短视频。


<details>
  <summary>Details</summary>
Motivation: 随着在线视频内容的迅速增长，尤其是在短视频平台上，长视频内容的有效编辑需求急剧增加。然而，目前的自动编辑方法往往依赖于ASR文本，而忽略了视觉上下文，导致了编辑不连贯。

Method: 提出HIVE框架，通过多模态大语言模型实现角色提取、对话分析和叙事内容的总结，并通过分解编辑流程（亮点检测、开头/结尾选择及无关内容修剪）提高一致性。此外，提出DramaAD数据集作为研究基准。

Result: 实验结果表明，HIVE在一般编辑任务和广告编辑任务中均优于现有基准，大幅缩小了自动与人工编辑效果之间的质量差距。

Conclusion: HIVE框架通过多模态叙事理解和结构化的编辑流程，显著提升了自动视频编辑的质量，验证了其有效性。

Abstract: The rapid growth of online video content, especially on short video
platforms, has created a growing demand for efficient video editing techniques
that can condense long-form videos into concise and engaging clips. Existing
automatic editing methods predominantly rely on textual cues from ASR
transcripts and end-to-end segment selection, often neglecting the rich visual
context and leading to incoherent outputs. In this paper, we propose a
human-inspired automatic video editing framework (HIVE) that leverages
multimodal narrative understanding to address these limitations. Our approach
incorporates character extraction, dialogue analysis, and narrative
summarization through multimodal large language models, enabling a holistic
understanding of the video content. To further enhance coherence, we apply
scene-level segmentation and decompose the editing process into three subtasks:
highlight detection, opening/ending selection, and pruning of irrelevant
content. To facilitate research in this area, we introduce DramaAD, a novel
benchmark dataset comprising over 800 short drama episodes and 500
professionally edited advertisement clips. Experimental results demonstrate
that our framework consistently outperforms existing baselines across both
general and advertisement-oriented editing tasks, significantly narrowing the
quality gap between automatic and human-edited videos.

</details>


### [34] [TABNet: A Triplet Augmentation Self-Recovery Framework with Boundary-Aware Pseudo-Labels for Medical Image Segmentation](https://arxiv.org/abs/2507.02399)
*Peilin Zhang,Shaouxan Wua,Jun Feng,Zhuo Jin,Zhizezhang Gao,Jingkun Chen,Yaqiong Xing,Xiao Zhang*

Main category: cs.CV

TL;DR: 本论文提出了一种名为TAB Net的弱监督医疗图像分割框架，旨在解决基于稀疏标注（如涂鸦标注）的分割挑战。


<details>
  <summary>Details</summary>
Motivation: 医疗图像分割需要高质量、大规模的标注数据，但这种标注过程昂贵且耗时。涂鸦标注虽然成本低，但因标注稀疏性带来目标区域特征学习不足和边界监督的挑战。

Method: 提出TAB Net框架，包括TAS模块和BAP模块。其中，TAS通过三种数据增强策略（强度变换、去除和拼图增强）改进特征学习；BAP模块通过融合预测和边界感知损失，提升伪监督精度和边界细化。

Result: 在ACDC和MSCMR seg两个公共数据集上，TAB Net在稀疏标注分割任务中表现优于现有方法，且接近全监督性能。

Conclusion: TAB Net为稀疏标注的医疗图像分割提供了高效的解决方案，兼具准确性和成本效益。

Abstract: Background and objective: Medical image segmentation is a core task in
various clinical applications. However, acquiring large-scale, fully annotated
medical image datasets is both time-consuming and costly. Scribble annotations,
as a form of sparse labeling, provide an efficient and cost-effective
alternative for medical image segmentation. However, the sparsity of scribble
annotations limits the feature learning of the target region and lacks
sufficient boundary supervision, which poses significant challenges for
training segmentation networks. Methods: We propose TAB Net, a novel
weakly-supervised medical image segmentation framework, consisting of two key
components: the triplet augmentation self-recovery (TAS) module and the
boundary-aware pseudo-label supervision (BAP) module. The TAS module enhances
feature learning through three complementary augmentation strategies: intensity
transformation improves the model's sensitivity to texture and contrast
variations, cutout forces the network to capture local anatomical structures by
masking key regions, and jigsaw augmentation strengthens the modeling of global
anatomical layout by disrupting spatial continuity. By guiding the network to
recover complete masks from diverse augmented inputs, TAS promotes a deeper
semantic understanding of medical images under sparse supervision. The BAP
module enhances pseudo-supervision accuracy and boundary modeling by fusing
dual-branch predictions into a loss-weighted pseudo-label and introducing a
boundary-aware loss for fine-grained contour refinement. Results: Experimental
evaluations on two public datasets, ACDC and MSCMR seg, demonstrate that TAB
Net significantly outperforms state-of-the-art methods for scribble-based
weakly supervised segmentation. Moreover, it achieves performance comparable to
that of fully supervised methods.

</details>


### [35] [Wildlife Target Re-Identification Using Self-supervised Learning in Non-Urban Settings](https://arxiv.org/abs/2507.02403)
*Mufhumudzi Muthivhi,Terence L. van Zyl*

Main category: cs.CV

TL;DR: 本文探讨了在野生动物重新识别中使用自监督学习（SSL）作为替代传统监督学习方法的可能性，方法是从视频中自动抽取图像对进行无监督训练，结果表明自监督学习在多个任务上性能优于监督学习。


<details>
  <summary>Details</summary>
Motivation: 受限于传统方法对标注数据的依赖，该研究希望探索减少对人工标注需求的自监督学习方法，进行更高效和广泛应用的数据挖掘。

Method: 利用摄像机捕捉的视频数据，自动提取同一动物的时间相关图像对，构造自监督学习模型，通过挖掘图像对之间的信息训练模型。

Result: 实验结果显示，自监督模型在数据量有限的情况下表现更为稳健，并在各种野生动物任务中均优于传统的监督学习方法。

Conclusion: 自监督学习在野生动物重新识别任务中具有更强的拓展性和泛化能力，是突破当前监督学习方法限制的有效方式。

Abstract: Wildlife re-identification aims to match individuals of the same species
across different observations. Current state-of-the-art (SOTA) models rely on
class labels to train supervised models for individual classification. This
dependence on annotated data has driven the curation of numerous large-scale
wildlife datasets. This study investigates self-supervised learning
Self-Supervised Learning (SSL) for wildlife re-identification. We automatically
extract two distinct views of an individual using temporal image pairs from
camera trap data without supervision. The image pairs train a self-supervised
model from a potentially endless stream of video data. We evaluate the learnt
representations against supervised features on open-world scenarios and
transfer learning in various wildlife downstream tasks. The analysis of the
experimental results shows that self-supervised models are more robust even
with limited data. Moreover, self-supervised features outperform supervision
across all downstream tasks. The code is available here
https://github.com/pxpana/SSLWildlife.

</details>


### [36] [PosDiffAE: Position-aware Diffusion Auto-encoder For High-Resolution Brain Tissue Classification Incorporating Artifact Restoration](https://arxiv.org/abs/2507.02405)
*Ayantika Das,Moitreya Chaudhuri,Koushik Bhat,Keerthi Ram,Mihail Bota,Mohanasankar Sivaprakasam*

Main category: cs.CV

TL;DR: 提出一种结合自编码器和扩散模型的新颖方法，用于脑部图像表征和伪影修复。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型可以生成高保真图像，但缺乏提取图像特定语义表征的能力，自编码器通过潜在空间映射提供了解决方案。

Method: 开发了一种将编码器集成到扩散模型中的自动编码框架，用于结构化高分辨率脑部图像的潜在空间；并提出基于潜在表征和生成能力的无监督伪影修复技术。

Result: 实现了区分脑部组织类型的潜在空间构建，并在脑部图像伪影修复中展示了有效性。

Conclusion: 通过扩展扩散模型的能力，新框架提供了图像特定语义表征的生成与组织以及伪影修复的实用工具。

Abstract: Denoising diffusion models produce high-fidelity image samples by capturing
the image distribution in a progressive manner while initializing with a simple
distribution and compounding the distribution complexity. Although these models
have unlocked new applicabilities, the sampling mechanism of diffusion does not
offer means to extract image-specific semantic representation, which is
inherently provided by auto-encoders. The encoding component of auto-encoders
enables mapping between a specific image and its latent space, thereby offering
explicit means of enforcing structures in the latent space. By integrating an
encoder with the diffusion model, we establish an auto-encoding formulation,
which learns image-specific representations and offers means to organize the
latent space. In this work, First, we devise a mechanism to structure the
latent space of a diffusion auto-encoding model, towards recognizing
region-specific cellular patterns in brain images. We enforce the
representations to regress positional information of the patches from
high-resolution images. This creates a conducive latent space for
differentiating tissue types of the brain. Second, we devise an unsupervised
tear artifact restoration technique based on neighborhood awareness, utilizing
latent representations and the constrained generation capability of diffusion
models during inference. Third, through representational guidance and
leveraging the inference time steerable noising and denoising capability of
diffusion, we devise an unsupervised JPEG artifact restoration technique.

</details>


### [37] [Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection](https://arxiv.org/abs/2507.02844)
*Ziqi Miao,Yi Ding,Lijun Li,Jing Shao*

Main category: cs.CV

TL;DR: 提出了一种新的视觉居中的绕过攻击方法VisCo，显著提高了对目标黑箱模型的攻击成功率和毒性得分。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉模态漏洞触发不安全行为时语义模糊与缺乏真实场景问题，以提高多模态大语言模型的安全性。

Method: 定义视觉居中的绕过设定，并通过VisCo攻击生成真实且完整的上下文，结合视觉聚焦策略、辅助图像生成、自动毒性模糊与语义优化，最终形成能绕过目标模型的攻击性提示。

Result: VisCo攻击在MM-SafetyBench上对GPT-4o实现了85%的成功率，毒性分数达4.78，远超基线方法的22.2%成功率和2.48毒性分数。

Conclusion: VisCo在构建结合视觉与文本的全面攻击场景上表现卓越，可显著提高多模态模型的脆弱性评估与探索潜在安全隐患的能力。

Abstract: With the emergence of strong visual-language capabilities, multimodal large
language models (MLLMs) have demonstrated tremendous potential for real-world
applications. However, the security vulnerabilities exhibited by the visual
modality pose significant challenges to deploying such models in open-world
environments. Recent studies have successfully induced harmful responses from
target MLLMs by encoding harmful textual semantics directly into visual inputs.
However, in these approaches, the visual modality primarily serves as a trigger
for unsafe behavior, often exhibiting semantic ambiguity and lacking grounding
in realistic scenarios. In this work, we define a novel setting: visual-centric
jailbreak, where visual information serves as a necessary component in
constructing a complete and realistic jailbreak context. Building on this
setting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates
contextual dialogue using four distinct visual-focused strategies, dynamically
generating auxiliary images when necessary to construct a visual-centric
jailbreak scenario. To maximize attack effectiveness, it incorporates automatic
toxicity obfuscation and semantic refinement to produce a final attack prompt
that reliably triggers harmful responses from the target black-box MLLMs.
Specifically, VisCo achieves a toxicity score of 4.78 and an Attack Success
Rate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming
the baseline, which performs a toxicity score of 2.48 and an ASR of 22.2%. The
code is available at https://github.com/Dtc7w3PQ/Visco-Attack.

</details>


### [38] [A Novel Tuning Method for Real-time Multiple-Object Tracking Utilizing Thermal Sensor with Complexity Motion Pattern](https://arxiv.org/abs/2507.02408)
*Duong Nguyen-Ngoc Tran,Long Hoang Pham,Chi Dai Tran,Quoc Pham-Nam Ho,Huy-Hung Nguyen,Jae Wook Jeon*

Main category: cs.CV

TL;DR: 本论文提出了一种新的行人跟踪调优方法，用于提高热成像监控系统中多目标跟踪的准确性与实用性。


<details>
  <summary>Details</summary>
Motivation: 由于RGB相机在低能见度或光线不佳的条件下表现不佳，热成像传感器成为了识别任务的重要补充。然而，热成像数据的低级特征表示使得准确检测和跟踪行人的任务变得困难。

Method: 本文提出了一种两阶段优化框架，通过调优适合的超参数以提高实时跟踪性能。同时，该方法无需复杂的重识别或运动模型便可达到高精度。

Result: 在PBVS Thermal MOT数据集上的广泛实验表明，该方法在各种热成像摄像头环境中均表现出色。

Conclusion: 本文的方法是一种鲁棒的解决方案，适用于多种真实监控环境中的热成像行人跟踪任务。

Abstract: Multi-Object Tracking in thermal images is essential for surveillance
systems, particularly in challenging environments where RGB cameras struggle
due to low visibility or poor lighting conditions. Thermal sensors enhance
recognition tasks by capturing infrared signatures, but a major challenge is
their low-level feature representation, which makes it difficult to accurately
detect and track pedestrians. To address this, the paper introduces a novel
tuning method for pedestrian tracking, specifically designed to handle the
complex motion patterns in thermal imagery. The proposed framework optimizes
two-stages, ensuring that each stage is tuned with the most suitable
hyperparameters to maximize tracking performance. By fine-tuning
hyperparameters for real-time tracking, the method achieves high accuracy
without relying on complex reidentification or motion models. Extensive
experiments on PBVS Thermal MOT dataset demonstrate that the approach is highly
effective across various thermal camera conditions, making it a robust solution
for real-world surveillance applications.

</details>


### [39] [Privacy-preserving Preselection for Face Identification Based on Packing](https://arxiv.org/abs/2507.02414)
*Rundong Xin,Taotao Wang,Jin Wang,Chonghe Zhao,Jing Wang*

Main category: cs.CV

TL;DR: PFIP提出了一种加密域中高效人脸检索方法，有效应对巨大的加密模板库带来的时间成本问题，实现了高效率和高准确率相结合。


<details>
  <summary>Details</summary>
Motivation: 解决加密域中人脸识别随着模板库增大而逐渐增加的检索时间问题，并保障隐私和数据安全。

Method: 引入创新的预筛选机制减少计算开销，并通过打包模块提升系统灵活性。

Result: 在LFW和CASIA数据集上的实验显示，PFIP在检索1000个加密人脸模板时能达到100%命中率，检索时间小于300毫秒，比现有方法快约50倍。

Conclusion: PFIP有效结合了隐私保护和高效人脸检索，显著提升了加密域中的人脸识别性能。

Abstract: Face identification systems operating in the ciphertext domain have garnered
significant attention due to increasing privacy concerns and the potential
recovery of original facial data. However, as the size of ciphertext template
libraries grows, the face retrieval process becomes progressively more
time-intensive. To address this challenge, we propose a novel and efficient
scheme for face retrieval in the ciphertext domain, termed Privacy-Preserving
Preselection for Face Identification Based on Packing (PFIP). PFIP incorporates
an innovative preselection mechanism to reduce computational overhead and a
packing module to enhance the flexibility of biometric systems during the
enrollment stage. Extensive experiments conducted on the LFW and CASIA datasets
demonstrate that PFIP preserves the accuracy of the original face recognition
model, achieving a 100% hit rate while retrieving 1,000 ciphertext face
templates within 300 milliseconds. Compared to existing approaches, PFIP
achieves a nearly 50x improvement in retrieval efficiency.

</details>


### [40] [Determination Of Structural Cracks Using Deep Learning Frameworks](https://arxiv.org/abs/2507.02416)
*Subhasis Dasgupta,Jaydip Sen,Tuhina Halder*

Main category: cs.CV

TL;DR: 提出一种用于结构裂缝检测的新型深度学习架构，通过残差U-Net模型和元模型的集成方法，显著提高检测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有人工检测方法效率低下且易出错，影响检测可靠性，因此需要研发高效、准确的自动化检测方法。

Method: 采用改进的残差U-Net模型并与卷积块组成的元模型集成，构建出一种新颖的深度学习检测架构。

Result: 与SegNet和传统U-Net相比，残差U-Net和集成模型在低分辨率图像检测中表现更优，特别是在IoU和DICE系数方面成绩最佳。

Conclusion: 研究证明集成模型的有效性，为结构缺陷的自动检测提供高效、可靠的解决方案。

Abstract: Structural crack detection is a critical task for public safety as it helps
in preventing potential structural failures that could endanger lives. Manual
detection by inexperienced personnel can be slow, inconsistent, and prone to
human error, which may compromise the reliability of assessments. The current
study addresses these challenges by introducing a novel deep-learning
architecture designed to enhance the accuracy and efficiency of structural
crack detection. In this research, various configurations of residual U-Net
models were utilized. These models, due to their robustness in capturing fine
details, were further integrated into an ensemble with a meta-model comprising
convolutional blocks. This unique combination aimed to boost prediction
efficiency beyond what individual models could achieve. The ensemble's
performance was evaluated against well-established architectures such as SegNet
and the traditional U-Net. Results demonstrated that the residual U-Net models
outperformed their predecessors, particularly with low-resolution imagery, and
the ensemble model exceeded the performance of individual models, proving it as
the most effective. The assessment was based on the Intersection over Union
(IoU) metric and DICE coefficient. The ensemble model achieved the highest
scores, signifying superior accuracy. This advancement suggests way for more
reliable automated systems in structural defects monitoring tasks.

</details>


### [41] [AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars](https://arxiv.org/abs/2507.02419)
*Yiming Zhong,Xiaolin Zhang,Ligang Liu,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: 该论文提出了一个名为AvatarMakeup的3D化妆方法，使用预训练的扩散模型从单一参考照片中迁移化妆模式，解决以往在多视角、一致性和细节控制上不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D面部改妆方法在实现真实化妆效果方面存在限制，且在人脸一致性、身份保持以及精细细节控制方面表现不足。

Method: 提出了AvatarMakeup方法，采用预训练扩散模型生成化妆图片作为监督信号，同时通过Coherent Duplication方法优化全局UV映射以保证一致性，并结合细化模块提升化妆质量。

Result: 实验表明，AvatarMakeup在化妆迁移质量和动画一致性方面达到了最新水平。

Conclusion: AvatarMakeup突破了以往方法的局限，为3D虚拟形象提供了高质量且一致的化妆解决方案。

Abstract: Similar to facial beautification in real life, 3D virtual avatars require
personalized customization to enhance their visual appeal, yet this area
remains insufficiently explored. Although current 3D Gaussian editing methods
can be adapted for facial makeup purposes, these methods fail to meet the
fundamental requirements for achieving realistic makeup effects: 1) ensuring a
consistent appearance during drivable expressions, 2) preserving the identity
throughout the makeup process, and 3) enabling precise control over fine
details. To address these, we propose a specialized 3D makeup method named
AvatarMakeup, leveraging a pretrained diffusion model to transfer makeup
patterns from a single reference photo of any individual. We adopt a
coarse-to-fine idea to first maintain the consistent appearance and identity,
and then to refine the details. In particular, the diffusion model is employed
to generate makeup images as supervision. Due to the uncertainties in diffusion
process, the generated images are inconsistent across different viewpoints and
expressions. Therefore, we propose a Coherent Duplication method to coarsely
apply makeup to the target while ensuring consistency across dynamic and
multiview effects. Coherent Duplication optimizes a global UV map by recoding
the averaged facial attributes among the generated makeup images. By querying
the global UV map, it easily synthesizes coherent makeup guidance from
arbitrary views and expressions to optimize the target avatar. Given the coarse
makeup avatar, we further enhance the makeup by incorporating a Refinement
Module into the diffusion model to achieve high makeup quality. Experiments
demonstrate that AvatarMakeup achieves state-of-the-art makeup transfer quality
and consistency throughout animation.

</details>


### [42] [F^2TTA: Free-Form Test-Time Adaptation on Cross-Domain Medical Image Classification via Image-Level Disentangled Prompt Tuning](https://arxiv.org/abs/2507.02437)
*Wei Li,Jingyang Zhang,Lihao Liu,Guoan Wang,Junjun He,Yang Chen,Lixu Gu*

Main category: cs.CV

TL;DR: 本论文提出一种新方法，用于在测试过程中适应未标注的医疗领域数据，其中数据以随机长度或顺序到达。


<details>
  <summary>Details</summary>
Motivation: 针对临床实际中数据通常以不完整和随机顺序到达的问题，提出自由形式的测试时适应任务（F²TTA）。

Method: 提出一种名为图像分解提示调优（I-DiPT）的框架，通过图像不变提示和图像特定提示应对随机域偏移，并结合不确定性屏蔽和并行图蒸馏技术提升适应性。

Result: 实验证明，提出的方法在乳腺癌和青光眼分类任务中优于现有的测试时适应方法。

Conclusion: I-DiPT框架有效应对了自由形式测试时适应的挑战，为医疗数据的无标注适应提供了新的解决方案。

Abstract: Test-Time Adaptation (TTA) has emerged as a promising solution for adapting a
source model to unseen medical sites using unlabeled test data, due to the high
cost of data annotation. Existing TTA methods consider scenarios where data
from one or multiple domains arrives in complete domain units. However, in
clinical practice, data usually arrives in domain fragments of arbitrary
lengths and in random arrival orders, due to resource constraints and patient
variability. This paper investigates a practical Free-Form Test-Time Adaptation
(F$^{2}$TTA) task, where a source model is adapted to such free-form domain
fragments, with shifts occurring between fragments unpredictably. In this
setting, these shifts could distort the adaptation process. To address this
problem, we propose a novel Image-level Disentangled Prompt Tuning (I-DiPT)
framework. I-DiPT employs an image-invariant prompt to explore domain-invariant
representations for mitigating the unpredictable shifts, and an image-specific
prompt to adapt the source model to each test image from the incoming
fragments. The prompts may suffer from insufficient knowledge representation
since only one image is available for training. To overcome this limitation, we
first introduce Uncertainty-oriented Masking (UoM), which encourages the
prompts to extract sufficient information from the incoming image via masked
consistency learning driven by the uncertainty of the source model
representations. Then, we further propose a Parallel Graph Distillation (PGD)
method that reuses knowledge from historical image-specific and image-invariant
prompts through parallel graph networks. Experiments on breast cancer and
glaucoma classification demonstrate the superiority of our method over existing
TTA approaches in F$^{2}$TTA. Code is available at
https://github.com/mar-cry/F2TTA.

</details>


### [43] [Red grape detection with accelerated artificial neural networks in the FPGA's programmable logic](https://arxiv.org/abs/2507.02443)
*Sandro Costa Magalhães,Marco Almeida,Filipe Neves dos Santos,António Paulo Moreira,Jorge Dias*

Main category: cs.CV

TL;DR: 本研究探讨了使用FINN架构将多个神经网络部署到FPGA中以提升推理速度，并使用自有数据集验证表现。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在运动中检测对象时因摄像头低帧率和算法速度限制而导致任务执行时间延长的问题。

Method: 将三个不同量化等级的神经网络（MobileNet v1 4-bit、CNV 2-bit、CNV 1-bit）使用FINN架构部署到FPGA的可编程逻辑部分，并使用RG2C自有开源数据集进行训练。

Result: MobileNet v1表现最佳，成功率为98%，推理速度达到6611 FPS，验证了FPGA可以加速神经网络并适应注意力机制。

Conclusion: 本文证明了通过使用FPGA能够有效提升神经网络的运行速度，使其适用于更高效的检测任务。

Abstract: Robots usually slow down for canning to detect objects while moving.
Additionally, the robot's camera is configured with a low framerate to track
the velocity of the detection algorithms. This would be constrained while
executing tasks and exploring, making robots increase the task execution time.
AMD has developed the Vitis-AI framework to deploy detection algorithms into
FPGAs. However, this tool does not fully use the FPGAs' PL. In this work, we
use the FINN architecture to deploy three ANNs, MobileNet v1 with 4-bit
quantisation, CNV with 2-bit quantisation, and CNV with 1-bit quantisation
(BNN), inside an FPGA's PL. The models were trained on the RG2C dataset. This
is a self-acquired dataset released in open access. MobileNet v1 performed
better, reaching a success rate of 98 % and an inference speed of 6611 FPS. In
this work, we proved that we can use FPGAs to speed up ANNs and make them
suitable for attention mechanisms.

</details>


### [44] [IGDNet: Zero-Shot Robust Underexposed Image Enhancement via Illumination-Guided and Denoising](https://arxiv.org/abs/2507.02445)
*Hailong Yan,Junjian Huang,Tingwen Huang*

Main category: cs.CV

TL;DR: IGDNet是一种针对欠曝光图像的零样本增强方法，不依赖成对图像，能够在无指导下生成高质量的结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖监督学习并需成对图像数据，这在现实中难以获取。此外，容易引入过度增强问题，损害已良好曝光区域的质量。

Method: 提出IGDNet框架，采用分解和去噪模块，分别解离图像的光照和反射部分，并通过光照引导的像素自适应校正增强非均匀光照区域，同时利用降采样生成噪声对进行迭代优化。

Result: 在四个公开数据集上实验表明，IGDNet显著提高复杂光照条件下的视觉质量，并在PSNR（20.41dB）和SSIM（0.860dB）等指标上优于14种最先进无监督方法。

Conclusion: IGDNet具有出色的泛化能力和噪声抑制效果，在无需训练数据的情况下可可靠恢复欠曝光图像光照。

Abstract: Current methods for restoring underexposed images typically rely on
supervised learning with paired underexposed and well-illuminated images.
However, collecting such datasets is often impractical in real-world scenarios.
Moreover, these methods can lead to over-enhancement, distorting
well-illuminated regions. To address these issues, we propose IGDNet, a
Zero-Shot enhancement method that operates solely on a single test image,
without requiring guiding priors or training data. IGDNet exhibits strong
generalization ability and effectively suppresses noise while restoring
illumination. The framework comprises a decomposition module and a denoising
module. The former separates the image into illumination and reflection
components via a dense connection network, while the latter enhances
non-uniformly illuminated regions using an illumination-guided pixel adaptive
correction method. A noise pair is generated through downsampling and refined
iteratively to produce the final result. Extensive experiments on four public
datasets demonstrate that IGDNet significantly improves visual quality under
complex lighting conditions. Quantitative results on metrics like PSNR
(20.41dB) and SSIM (0.860dB) show that it outperforms 14 state-of-the-art
unsupervised methods. The code will be released soon.

</details>


### [45] [Weakly-supervised Contrastive Learning with Quantity Prompts for Moving Infrared Small Target Detection](https://arxiv.org/abs/2507.02454)
*Weiwei Duan,Luping Ji,Shengjia Chen,Sicheng Zhu,Jianghong Huang,Mao Ye*

Main category: cs.CV

TL;DR: 提出了一种新的弱监督对比学习（WeCoL）方案，用于解决红外小目标检测中的挑战，验证了其在部分情况下可以超越早期的全监督方法，且性能可以接近最先进的全监督方法的90%。


<details>
  <summary>Details</summary>
Motivation: 由于红外小目标检测中目标尺寸小、背景对比度低，现有全监督方法过度依赖手动标注，标注耗时且困难，因此需要降低标注要求的非全监督策略。

Method: 提出了一种基于Segment Anything Model的潜在目标挖掘策略，结合目标激活图与多帧能量累积，同时采用对比学习提高伪标签可靠性，并引入长短期运动感知学习方案建模目标运动模式。

Result: 在DAUB和ITSDT-15K两个公开数据集上的实验表明，弱监督方案在部分情况下超越了早期全监督方法，且性能达到最先进全监督方法的90%。

Conclusion: 验证了利用简单的目标数量提示训练模型的弱监督方案在红外小目标检测中具有较强潜力，并提供了一种降低标注需求的可行性方法。

Abstract: Different from general object detection, moving infrared small target
detection faces huge challenges due to tiny target size and weak background
contrast.Currently, most existing methods are fully-supervised, heavily relying
on a large number of manual target-wise annotations. However, manually
annotating video sequences is often expensive and time-consuming, especially
for low-quality infrared frame images. Inspired by general object detection,
non-fully supervised strategies ($e.g.$, weakly supervised) are believed to be
potential in reducing annotation requirements. To break through traditional
fully-supervised frameworks, as the first exploration work, this paper proposes
a new weakly-supervised contrastive learning (WeCoL) scheme, only requires
simple target quantity prompts during model training.Specifically, in our
scheme, based on the pretrained segment anything model (SAM), a potential
target mining strategy is designed to integrate target activation maps and
multi-frame energy accumulation.Besides, contrastive learning is adopted to
further improve the reliability of pseudo-labels, by calculating the similarity
between positive and negative samples in feature subspace.Moreover, we propose
a long-short term motion-aware learning scheme to simultaneously model the
local motion patterns and global motion trajectory of small targets.The
extensive experiments on two public datasets (DAUB and ITSDT-15K) verify that
our weakly-supervised scheme could often outperform early fully-supervised
methods. Even, its performance could reach over 90\% of state-of-the-art (SOTA)
fully-supervised ones.

</details>


### [46] [Mesh Silksong: Auto-Regressive Mesh Generation as Weaving Silk](https://arxiv.org/abs/2507.02477)
*Gaochao Song,Zibo Zhao,Haohan Weng,Jingbo Zeng,Rongfei Jia,Shenghua Gao*

Main category: cs.CV

TL;DR: 提出了Mesh Silksong，一种用于生成多边形网格的紧凑高效的网格表示方法，改进了网格顶点的编码方式，减少冗余并提升几何属性。


<details>
  <summary>Details</summary>
Motivation: 现有的网格编码方法会生成带有重复顶点的序列，浪费了模型能力，且存在编码冗余问题。

Method: 将网格顶点进行一次性编码，减少50%的冗余序列，提升压缩率并确保生成的网格具有优越的几何属性如流形拓扑等。

Result: 实验表明新方法可以生成复杂的网格，同时显著提升几何完整性。

Conclusion: Mesh Silksong有效改善了网格的紧凑表示和生成质量，为实际应用中的网格生成和压缩提供了先进技术支持。

Abstract: We introduce Mesh Silksong, a compact and efficient mesh representation
tailored to generate the polygon mesh in an auto-regressive manner akin to silk
weaving. Existing mesh tokenization methods always produce token sequences with
repeated vertex tokens, wasting the network capability. Therefore, our approach
tokenizes mesh vertices by accessing each mesh vertice only once, reduces the
token sequence's redundancy by 50\%, and achieves a state-of-the-art
compression rate of approximately 22\%. Furthermore, Mesh Silksong produces
polygon meshes with superior geometric properties, including manifold topology,
watertight detection, and consistent face normals, which are critical for
practical applications. Experimental results demonstrate the effectiveness of
our approach, showcasing not only intricate mesh generation but also
significantly improved geometric integrity.

</details>


### [47] [CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios](https://arxiv.org/abs/2507.02479)
*Teng Fu,Yuwen Chen,Zhuofan Chen,Mengyang Zhao,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: 提出了一个复杂多行人跟踪的新数据集CrowdTrack，涵盖第一视角和复杂场景，包含5185条完整标注轨迹，为复杂场景中算法发展提供了测试平台。


<details>
  <summary>Details</summary>
Motivation: 现有的多目标追踪数据集存在场景简单和不现实的问题，难以应对复杂情境中目标部分可见性或运动状态更新受阻等难题。

Method: 提供了一个名为CrowdTrack的大规模多行人跟踪数据集，包括33段视频，全部来源于复杂现实场景，带有完整边界框和唯一目标ID的精确标注。

Result: 通过对数据集的深入分析及对多种SOTA方法和基础模型的性能测试，证明了数据集支持复杂情况下的算法测试。

Conclusion: CrowdTrack数据集弥补了现有MOT数据集在复杂场景上的不足，为相关算法的开发提供了重要平台，促进了多行人跟踪领域的研究进展。

Abstract: Multi-object tracking is a classic field in computer vision. Among them,
pedestrian tracking has extremely high application value and has become the
most popular research category. Existing methods mainly use motion or
appearance information for tracking, which is often difficult in complex
scenarios. For the motion information, mutual occlusions between objects often
prevent updating of the motion state; for the appearance information,
non-robust results are often obtained due to reasons such as only partial
visibility of the object or blurred images. Although learning how to perform
tracking in these situations from the annotated data is the simplest solution,
the existing MOT dataset fails to satisfy this solution. Existing methods
mainly have two drawbacks: relatively simple scene composition and
non-realistic scenarios. Although some of the video sequences in existing
dataset do not have the above-mentioned drawbacks, the number is far from
adequate for research purposes. To this end, we propose a difficult large-scale
dataset for multi-pedestrian tracking, shot mainly from the first-person view
and all from real-life complex scenarios. We name it ``CrowdTrack'' because
there are numerous objects in most of the sequences. Our dataset consists of 33
videos, containing a total of 5,185 trajectories. Each object is annotated with
a complete bounding box and a unique object ID. The dataset will provide a
platform to facilitate the development of algorithms that remain effective in
complex situations. We analyzed the dataset comprehensively and tested multiple
SOTA models on our dataset. Besides, we analyzed the performance of the
foundation models on our dataset. The dataset and project code is released at:
https://github.com/loseevaya/CrowdTrack .

</details>


### [48] [MedFormer: Hierarchical Medical Vision Transformer with Content-Aware Dual Sparse Selection Attention](https://arxiv.org/abs/2507.02488)
*Zunhui Xia,Hongxing Li,Libin Lan*

Main category: cs.CV

TL;DR: 提出了一种名为MedFormer的高效医疗视觉Transformer，旨在解决现有方法的高计算成本和子优化表现问题，通过金字塔缩放结构和双稀疏选择注意力机制提升性能和泛化性，实验验证其在多个医疗图像识别任务中效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉Transformer的方法在医疗图像任务中的通用性和计算效率上存在局限，需要更高效、更普适的解决方案。

Method: 采用金字塔缩放结构作为通用主干网络，以层次化特征表示优化计算负担，引入双稀疏选择注意力机制提高内容相关性和计算效率。

Result: 通过理论分析和多种数据集的实验，验证了MedFormer在分类、语义分割和病变检测等任务中的性能优越性和通用性。

Conclusion: MedFormer展示了在多种医疗图像任务中的显著效能和高效性，证明了其作为通用医疗视觉Transformer框架的潜力。

Abstract: Medical image recognition serves as a key way to aid in clinical diagnosis,
enabling more accurate and timely identification of diseases and abnormalities.
Vision transformer-based approaches have proven effective in handling various
medical recognition tasks. However, these methods encounter two primary
challenges. First, they are often task-specific and architecture-tailored,
limiting their general applicability. Second, they usually either adopt full
attention to model long-range dependencies, resulting in high computational
costs, or rely on handcrafted sparse attention, potentially leading to
suboptimal performance. To tackle these issues, we present MedFormer, an
efficient medical vision transformer with two key ideas. First, it employs a
pyramid scaling structure as a versatile backbone for various medical image
recognition tasks, including image classification and dense prediction tasks
such as semantic segmentation and lesion detection. This structure facilitates
hierarchical feature representation while reducing the computation load of
feature maps, highly beneficial for boosting performance. Second, it introduces
a novel Dual Sparse Selection Attention (DSSA) with content awareness to
improve computational efficiency and robustness against noise while maintaining
high performance. As the core building technique of MedFormer, DSSA is
explicitly designed to attend to the most relevant content. In addition, a
detailed theoretical analysis has been conducted, demonstrating that MedFormer
has superior generality and efficiency in comparison to existing medical vision
transformers. Extensive experiments on a variety of imaging modality datasets
consistently show that MedFormer is highly effective in enhancing performance
across all three above-mentioned medical image recognition tasks. The code is
available at https://github.com/XiaZunhui/MedFormer.

</details>


### [49] [Temporally-Aware Supervised Contrastive Learning for Polyp Counting in Colonoscopy](https://arxiv.org/abs/2507.02493)
*Luca Parolari,Andrea Cherubini,Lamberto Ballan,Carlo Biffi*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新方法用于改善结肠镜检查中息肉计数的准确性，其通过引入时间序列意识的监督对比损失和时间邻近约束。


<details>
  <summary>Details</summary>
Motivation: 当前的息肉计数方法主要依赖视觉外观，缺乏对时间关系的利用，这限制了它们的准确性。

Method: 提出一种包含时间感知的监督对比损失，捕捉单个息肉的内部变化且维持不同息肉间的区分能力。此外，通过引入时间邻近约束改善轨迹簇聚性能。

Result: 与现有方法相比，碎片率减少了2.2倍，并在实验中以留一验证策略达到了新的最优性能。

Conclusion: 时间序列意识对提高息肉计数准确性至关重要，提出的方法为该问题开辟了新的研究方向并达到了最新性能。

Abstract: Automated polyp counting in colonoscopy is a crucial step toward automated
procedure reporting and quality control, aiming to enhance the
cost-effectiveness of colonoscopy screening. Counting polyps in a procedure
involves detecting and tracking polyps, and then clustering tracklets that
belong to the same polyp entity. Existing methods for polyp counting rely on
self-supervised learning and primarily leverage visual appearance, neglecting
temporal relationships in both tracklet feature learning and clustering stages.
In this work, we introduce a paradigm shift by proposing a supervised
contrastive loss that incorporates temporally-aware soft targets. Our approach
captures intra-polyp variability while preserving inter-polyp discriminability,
leading to more robust clustering. Additionally, we improve tracklet clustering
by integrating a temporal adjacency constraint, reducing false positive
re-associations between visually similar but temporally distant tracklets. We
train and validate our method on publicly available datasets and evaluate its
performance with a leave-one-out cross-validation strategy. Results demonstrate
a 2.2x reduction in fragmentation rate compared to prior approaches. Our
results highlight the importance of temporal awareness in polyp counting,
establishing a new state-of-the-art. Code is available at
https://github.com/lparolari/temporally-aware-polyp-counting.

</details>


### [50] [MC-INR: Efficient Encoding of Multivariate Scientific Simulation Data using Meta-Learning and Clustered Implicit Neural Representations](https://arxiv.org/abs/2507.02494)
*Hyunsoo Son,Jeonghyun Noh,Suemin Jeon,Chaoli Wang,Won-Ki Jeong*

Main category: cs.CV

TL;DR: 本文提出MC-INR框架，通过引入元学习、聚类方法以及动态重聚类机制，解决现有隐式神经表示技术在复杂、多变量及非结构化网格数据中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有隐式神经表示技术在处理复杂结构、多变量数据和非结构化网格时具有局限性，因此需要开发一个更灵活和适用的方法。

Method: 提出MC-INR，一个结合元学习和聚类的框架，采用残差动态重聚类机制，并通过分支层同时处理多变量数据。

Result: 实验结果表明，MC-INR在科学数据编码任务中表现优于现有方法。

Conclusion: MC-INR解决了INRs在多变量和非结构化数据中的主要问题，为科学仿真数据的高效表示提供了一种新的解决方案。

Abstract: Implicit Neural Representations (INRs) are widely used to encode data as
continuous functions, enabling the visualization of large-scale multivariate
scientific simulation data with reduced memory usage. However, existing
INR-based methods face three main limitations: (1) inflexible representation of
complex structures, (2) primarily focusing on single-variable data, and (3)
dependence on structured grids. Thus, their performance degrades when applied
to complex real-world datasets. To address these limitations, we propose a
novel neural network-based framework, MC-INR, which handles multivariate data
on unstructured grids. It combines meta-learning and clustering to enable
flexible encoding of complex structures. To further improve performance, we
introduce a residual-based dynamic re-clustering mechanism that adaptively
partitions clusters based on local error. We also propose a branched layer to
leverage multivariate data through independent branches simultaneously.
Experimental results demonstrate that MC-INR outperforms existing methods on
scientific data encoding tasks.

</details>


### [51] [Automatic Labelling for Low-Light Pedestrian Detection](https://arxiv.org/abs/2507.02513)
*Dimitrios Bouzoulas,Eerik Alamikkotervo,Risto Ojala*

Main category: cs.CV

TL;DR: 该研究提出一种红外-RGB自动标记流水线，用于解决低光照条件下RGB行人检测数据集稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 通过RGB相机进行行人检测是自动驾驶和高级驾驶辅助系统中的关键任务。但在低光照条件下，缺乏大规模数据集阻碍了RGB行人检测的研究进展。

Method: 该方法包括三个步骤：1）使用优化的红外行人检测模型进行红外检测；2）将红外检测标记转移到对应的RGB图像上；3）用生成的标记训练用于低光照RGB行人检测的目标检测模型。

Result: 结果显示，在未见过的图像序列上，与基于真实标记训练的模型相比，基于生成标记训练的模型在mAP@50和mAP@50-95指标上，在9个测试案例中有6个取得了更好的表现。

Conclusion: 使用红外-RGB的自动标记流水线可有效提升低光照RGB行人检测的性能，为自动驾驶系统提供了可靠的解决方案。

Abstract: Pedestrian detection in RGB images is a key task in pedestrian safety, as the
most common sensor in autonomous vehicles and advanced driver assistance
systems is the RGB camera. A challenge in RGB pedestrian detection, that does
not appear to have large public datasets, is low-light conditions. As a
solution, in this research, we propose an automated infrared-RGB labeling
pipeline. The proposed pipeline consists of 1) Infrared detection, where a
fine-tuned model for infrared pedestrian detection is used 2) Label transfer
process from the infrared detections to their RGB counterparts 3) Training
object detection models using the generated labels for low-light RGB pedestrian
detection. The research was performed using the KAIST dataset. For the
evaluation, object detection models were trained on the generated autolabels
and ground truth labels. When compared on a previously unseen image sequence,
the results showed that the models trained on generated labels outperformed the
ones trained on ground-truth labels in 6 out of 9 cases for the mAP@50 and
mAP@50-95 metrics. The source code for this research is available at
https://github.com/BouzoulasDimitrios/IR-RGB-Automated-LowLight-Pedestrian-Labeling

</details>


### [52] [Detecting Multiple Diseases in Multiple Crops Using Deep Learning](https://arxiv.org/abs/2507.02517)
*Vivek Yadav,Anugrah Jain*

Main category: cs.CV

TL;DR: 该研究提出一种深度学习方法，能以99%的准确率检测17种作物的34种疾病，比现有方法提高7%。


<details>
  <summary>Details</summary>
Motivation: 解决印度农业中由疾病、害虫及环境压力引起的作物损失，通过早期检测与精确识别疾病来提高产量及粮食安全。

Method: 创建包含17种作物与34种疾病的统一数据集，用深度学习模型进行训练并与现有方法进行性能对比。

Result: 所提模型达到了99%的检测准确率，比现有方法提高7%，覆盖了更多种类的作物及疾病。

Conclusion: 该方法提高了检测精度及覆盖范围，旨在为印度农民提供更优解决方案。

Abstract: India, as a predominantly agrarian economy, faces significant challenges in
agriculture, including substantial crop losses caused by diseases, pests, and
environmental stress. Early detection and accurate identification of diseases
across different crops are critical for improving yield and ensuring food
security. This paper proposes a deep learning based solution for detecting
multiple diseases in multiple crops, aimed to cover India's diverse
agricultural landscape. We first create a unified dataset encompassing images
of 17 different crops and 34 different diseases from various available
repositories. Proposed deep learning model is trained on this dataset and
outperforms the state-of-the-art in terms of accuracy and the number of crops,
diseases covered. We achieve a significant detection accuracy, i.e., 99 percent
for our unified dataset which is 7 percent more when compared to
state-of-the-art handling 14 crops and 26 different diseases only. By improving
the number of crops and types of diseases that can be detected, proposed
solution aims to provide a better product for Indian farmers.

</details>


### [53] [IMASHRIMP: Automatic White Shrimp (Penaeus vannamei) Biometrical Analysis from Laboratory Images Using Computer Vision and Deep Learning](https://arxiv.org/abs/2507.02519)
*Abiam Remache González,Meriem Chagour,Timon Bijan Rüth,Raúl Trapiella Cañedo,Marina Martínez Soler,Álvaro Lorenzo Felipe,Hyun-Suk Shin,María-Jesús Zamorano Serrano,Ricardo Torres,Juan-Antonio Castillo Parra,Eduardo Reyes Abad,Miguel-Ángel Ferrer Ballester,Juan-Manuel Afonso López,Francisco-Mario Hernández Tejera,Adrian Penate-Sanchez*

Main category: cs.CV

TL;DR: IMASHRIMP系统通过深度学习和计算机视觉方法，对白虾进行形态学分析，优化了水产养殖中的遗传选择过程。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习算法在白虾形态学分析上的适用性问题，以提高遗传选择效率和降低人工误差。

Method: 使用基于ResNet-50的分类模块进行图像视角分类和头胸刺完整性检测，结合VitPose模块实现虾骨骼关键点的预测，并通过SVM模型实现像素到厘米的转换。

Result: 系统使人类误差显著下降，姿态估计均值精度达到97.94%，像素到厘米转换误差为0.07±0.1 cm。

Conclusion: IMASHRIMP可自动化白虾形态学分析，提高遗传选择效率，助力可持续的水产养殖发展。

Abstract: This paper introduces IMASHRIMP, an adapted system for the automated
morphological analysis of white shrimp (Penaeus vannamei}, aimed at optimizing
genetic selection tasks in aquaculture. Existing deep learning and computer
vision techniques were modified to address the specific challenges of shrimp
morphology analysis from RGBD images. IMASHRIMP incorporates two discrimination
modules, based on a modified ResNet-50 architecture, to classify images by the
point of view and determine rostrum integrity. It is proposed a "two-factor
authentication (human and IA)" system, it reduces human error in view
classification from 0.97% to 0% and in rostrum detection from 12.46% to 3.64%.
Additionally, a pose estimation module was adapted from VitPose to predict 23
key points on the shrimp's skeleton, with separate networks for lateral and
dorsal views. A morphological regression module, using a Support Vector Machine
(SVM) model, was integrated to convert pixel measurements to centimeter units.
Experimental results show that the system effectively reduces human error,
achieving a mean average precision (mAP) of 97.94% for pose estimation and a
pixel-to-centimeter conversion error of 0.07 (+/- 0.1) cm. IMASHRIMP
demonstrates the potential to automate and accelerate shrimp morphological
analysis, enhancing the efficiency of genetic selection and contributing to
more sustainable aquaculture practices.The code are available at
https://github.com/AbiamRemacheGonzalez/ImaShrimp-public

</details>


### [54] [MoGe-2: Accurate Monocular Geometry with Metric Scale and Sharp Details](https://arxiv.org/abs/2507.02546)
*Ruicheng Wang,Sicheng Xu,Yue Dong,Yu Deng,Jianfeng Xiang,Zelong Lv,Guangzhong Sun,Xin Tong,Jiaolong Yang*

Main category: cs.CV

TL;DR: 提出了MoGe-2，一种先进的开域几何估计模型，可从单张图像恢复具有公制尺度的3D点图。模型通过改进MoGe，结合数据精炼策略，提高几何细节重建和整体精度。


<details>
  <summary>Details</summary>
Motivation: 解决未知尺度的单目几何估计问题并提高几何重构的细节和精度。

Method: 改进MoGe模型用于生成公制几何，同时通过数据精炼方法利用合成标签过滤和补全真实数据。

Result: 模型在保持相对几何准确性的同时，实现了公制尺度精确和细粒度细节恢复，优于所有前人的方法。

Conclusion: MoGe-2成功实现了单目图像公制几何估计及细节恢复的技术突破，填补了现有方法的空白。

Abstract: We propose MoGe-2, an advanced open-domain geometry estimation model that
recovers a metric scale 3D point map of a scene from a single image. Our method
builds upon the recent monocular geometry estimation approach, MoGe, which
predicts affine-invariant point maps with unknown scales. We explore effective
strategies to extend MoGe for metric geometry prediction without compromising
the relative geometry accuracy provided by the affine-invariant point
representation. Additionally, we discover that noise and errors in real data
diminish fine-grained detail in the predicted geometry. We address this by
developing a unified data refinement approach that filters and completes real
data from different sources using sharp synthetic labels, significantly
enhancing the granularity of the reconstructed geometry while maintaining the
overall accuracy. We train our model on a large corpus of mixed datasets and
conducted comprehensive evaluations, demonstrating its superior performance in
achieving accurate relative geometry, precise metric scale, and fine-grained
detail recovery -- capabilities that no previous methods have simultaneously
achieved.

</details>


### [55] [Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning](https://arxiv.org/abs/2507.02565)
*Buzhen Huang,Chen Li,Chongyang Xu,Dongyue Lu,Jinnan Chen,Yangang Wang,Gim Hee Lee*

Main category: cs.CV

TL;DR: 提出了一种基于双分支优化框架的新方法，通过结合人类外观、社交距离和物理法则约束，解决现有方法在从复杂环境视频中重建紧密交互动作时的难题。


<details>
  <summary>Details</summary>
Motivation: 现有的人体姿态估计方法难以从复杂环境和多人遮挡视频中恢复可信的互动动作，大型基础模型在这种情况下也表现欠佳，促使研究寻找更适合的解决方案。

Method: 提出了一个双分支优化框架，利用扩散模型学习人类社交行为和姿态先验知识，通过优化张量和多种约束，重建人类动作和外观，包括3D高斯建模、2D关键点和网格渗透约束等。

Result: 实验表明，所提方法在多个基准测试中优于现有方法。此外，建立了含伪真实标签的交互数据集，推动未来研究。

Conclusion: 结合人类外观线索和多种限制条件，新方法实现了从复杂场景视频中准确估计互动，以促进姿态估计和人类行为理解领域的发展。

Abstract: Due to visual ambiguities and inter-person occlusions, existing human pose
estimation methods cannot recover plausible close interactions from in-the-wild
videos. Even state-of-the-art large foundation models~(\eg, SAM) cannot
accurately distinguish human semantics in such challenging scenarios. In this
work, we find that human appearance can provide a straightforward cue to
address these obstacles. Based on this observation, we propose a dual-branch
optimization framework to reconstruct accurate interactive motions with
plausible body contacts constrained by human appearances, social proxemics, and
physical laws. Specifically, we first train a diffusion model to learn the
human proxemic behavior and pose prior knowledge. The trained network and two
optimizable tensors are then incorporated into a dual-branch optimization
framework to reconstruct human motions and appearances. Several constraints
based on 3D Gaussians, 2D keypoints, and mesh penetrations are also designed to
assist the optimization. With the proxemics prior and diverse constraints, our
method is capable of estimating accurate interactions from in-the-wild videos
captured in complex environments. We further build a dataset with pseudo
ground-truth interaction annotations, which may promote future research on pose
estimation and human behavior understanding. Experimental results on several
benchmarks demonstrate that our method outperforms existing approaches. The
code and data are available at https://www.buzhenhuang.com/works/CloseApp.html.

</details>


### [56] [Parametric shape models for vessels learned from segmentations via differentiable voxelization](https://arxiv.org/abs/2507.02576)
*Alina F. Dima,Suprosanna Shit,Huaqi Qiu,Robbie Holland,Tamara T. Mueller,Fabio Antonio Musio,Kaiyuan Yang,Bjoern Menze,Rickmer Braren,Marcus Makowski,Daniel Rueckert*

Main category: cs.CV

TL;DR: 本文提出一种框架，将体素化、网格和参数化模型三种血管表示形式联合在可微变换下。通过可微体素化，从分割中自动提取血管的参数化形状模型，无需显式的真实参数。


<details>
  <summary>Details</summary>
Motivation: 当前体素化、网格和参数化模型三种血管表示形式通常被单独提取和使用，本文旨在联合这三种表示形式以提高研究和应用的效果。

Method: 通过提出一种框架，实现体素化、网格和参数化模型的联合；利用可微体素化，从分割中自动提取参数化形状模型，并通过形状到分割拟合学习参数。

Result: 实验表明，该方法可以准确捕捉复杂血管几何形状，在主动脉、动脉瘤和脑血管的实验中显示了良好的体积拟合效果。

Conclusion: 该方法建立在可微框架的基础上，确保了平滑性和连续性，可生成高保真度网格，适用于多种复杂血管几何建模应用。

Abstract: Vessels are complex structures in the body that have been studied extensively
in multiple representations. While voxelization is the most common of them,
meshes and parametric models are critical in various applications due to their
desirable properties. However, these representations are typically extracted
through segmentations and used disjointly from each other. We propose a
framework that joins the three representations under differentiable
transformations. By leveraging differentiable voxelization, we automatically
extract a parametric shape model of the vessels through shape-to-segmentation
fitting, where we learn shape parameters from segmentations without the
explicit need for ground-truth shape parameters. The vessel is parametrized as
centerlines and radii using cubic B-splines, ensuring smoothness and continuity
by construction. Meshes are differentiably extracted from the learned shape
parameters, resulting in high-fidelity meshes that can be manipulated post-fit.
Our method can accurately capture the geometry of complex vessels, as
demonstrated by the volumetric fits in experiments on aortas, aneurysms, and
brain vessels.

</details>


### [57] [Structure-aware Semantic Discrepancy and Consistency for 3D Medical Image Self-supervised Learning](https://arxiv.org/abs/2507.02581)
*Tan Pan,Zhaorui Tan,Kaiyu Guo,Dongli Xu,Weidi Xu,Chen Jiang,Xin Guo,Yuan Qi,Yuan Cheng*

Main category: cs.CV

TL;DR: 提出了新的框架$S^2DC$，通过结构感知的语义差异和一致性，改进3D医学图像的自监督学习。


<details>
  <summary>Details</summary>
Motivation: 现有的医学图像自监督学习方法无法有效考虑解剖结构在位置、尺度和形态上的变化，影响了医学分析的准确性。

Method: 提出了$S^2DC$框架，通过利用最优传输策略实现跨结构的语义差异，并基于邻域相似性分布在结构级别上增强语义一致性，从而获得结构感知表示。

Result: 在10个数据集、4个任务和3种模态上验证，$S^2DC$均优于现有最先进的方法。

Conclusion: $S^2DC$通过更好地捕捉3D医学图像的结构特征，提供了一种有效的自监督学习解决方案，为医学图像分析的多样化需求提供了广阔的支持。

Abstract: 3D medical image self-supervised learning (mSSL) holds great promise for
medical analysis. Effectively supporting broader applications requires
considering anatomical structure variations in location, scale, and morphology,
which are crucial for capturing meaningful distinctions. However, previous mSSL
methods partition images with fixed-size patches, often ignoring the structure
variations. In this work, we introduce a novel perspective on 3D medical images
with the goal of learning structure-aware representations. We assume that
patches within the same structure share the same semantics (semantic
consistency) while those from different structures exhibit distinct semantics
(semantic discrepancy). Based on this assumption, we propose an mSSL framework
named $S^2DC$, achieving Structure-aware Semantic Discrepancy and Consistency
in two steps. First, $S^2DC$ enforces distinct representations for different
patches to increase semantic discrepancy by leveraging an optimal transport
strategy. Second, $S^2DC$ advances semantic consistency at the structural level
based on neighborhood similarity distribution. By bridging patch-level and
structure-level representations, $S^2DC$ achieves structure-aware
representations. Thoroughly evaluated across 10 datasets, 4 tasks, and 3
modalities, our proposed method consistently outperforms the state-of-the-art
methods in mSSL.

</details>


### [58] [AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video Understanding](https://arxiv.org/abs/2507.02591)
*Weili Xu,Enxin Song,Wenhao Chai,Xuexiang Wen,Tian Ye,Gaoang Wang*

Main category: cs.CV

TL;DR: AuroraLong通过使用线性RNN替代LLM，解决了视频理解中的高算力和内存问题，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 解决长视频理解的高计算复杂性和内存成本问题。

Method: 用线性RNN语言模型取代多模态LLM，同时结合视觉token合并技术提高效率。

Result: AuroraLong仅使用2B参数和公开数据训练，性能与基于Transformer的模型相当。

Conclusion: 线性RNN可以有效降低长视频理解的计算和资源门槛，具备很高的应用潜力。

Abstract: The challenge of long video understanding lies in its high computational
complexity and prohibitive memory cost, since the memory and computation
required by transformer-based LLMs scale quadratically with input sequence
length. We propose AuroraLong to address this challenge by replacing the LLM
component in MLLMs with a linear RNN language model that handles input sequence
of arbitrary length with constant-size hidden states. To further increase
throughput and efficiency, we combine visual token merge with linear RNN models
by reordering the visual tokens by their sizes in ascending order. Despite
having only 2B parameters and being trained exclusively on public data,
AuroraLong achieves performance comparable to Transformer-based models of
similar size trained on private datasets across multiple video benchmarks. This
demonstrates the potential of efficient, linear RNNs to democratize long video
understanding by lowering its computational entry barrier. To our best
knowledge, we are the first to use a linear RNN based LLM backbone in a
LLaVA-like model for open-ended video understanding.

</details>


### [59] [Addressing Camera Sensors Faults in Vision-Based Navigation: Simulation and Dataset Development](https://arxiv.org/abs/2507.02602)
*Riccardo Gallon,Fabian Schiemenz,Alessandra Menicucci,Eberhard Gill*

Main category: cs.CV

TL;DR: 本文探讨在星际探索任务中，针对于视觉导航算法的传感器故障问题，并提出了一种基于AI的解决方案，利用合成数据集进行训练和测试。


<details>
  <summary>Details</summary>
Motivation: 当前视觉导航算法在星际任务中至关重要，但传感器故障可能导致任务失败。传统故障检测方法存在局限性，因此需要借助AI技术解决问题。

Method: 通过分析摄像机传感器可能出现的故障及其对图像质量和导航性能的影响，研究提出了一种模拟框架生成含故障的合成图像数据集，用以训练和测试AI检测算法。

Result: 研究创建了一个包含故障注入图像的数据集，为开发AI-based故障检测算法提供支持，并指出数据集目前处于限时保密阶段。

Conclusion: 本研究为视觉导航算法中的传感器故障检测提供了一种创新性的AI解决方案，并显著增强了对故障场景的全面理解与应对能力。

Abstract: The increasing importance of Vision-Based Navigation (VBN) algorithms in
space missions raises numerous challenges in ensuring their reliability and
operational robustness. Sensor faults can lead to inaccurate outputs from
navigation algorithms or even complete data processing faults, potentially
compromising mission objectives. Artificial Intelligence (AI) offers a powerful
solution for detecting such faults, overcoming many of the limitations
associated with traditional fault detection methods. However, the primary
obstacle to the adoption of AI in this context is the lack of sufficient and
representative datasets containing faulty image data.
  This study addresses these challenges by focusing on an interplanetary
exploration mission scenario. A comprehensive analysis of potential fault cases
in camera sensors used within the VBN pipeline is presented. The causes and
effects of these faults are systematically characterized, including their
impact on image quality and navigation algorithm performance, as well as
commonly employed mitigation strategies. To support this analysis, a simulation
framework is introduced to recreate faulty conditions in synthetically
generated images, enabling a systematic and controlled reproduction of faulty
data. The resulting dataset of fault-injected images provides a valuable tool
for training and testing AI-based fault detection algorithms. The final link to
the dataset will be added after an embargo period. For peer-reviewers, this
private link is available.

</details>


### [60] [AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image Detection via Multimodal Large Language Models](https://arxiv.org/abs/2507.02664)
*Ziyin Zhou,Yunpeng Luo,Yuanchen Wu,Ke Sun,Jiayi Ji,Ke Yan,Shouhong Ding,Xiaoshuai Sun,Yunsheng Wu,Rongrong Ji*

Main category: cs.CV

TL;DR: 该论文探讨了解决AI生成图像（AIGI）被滥用传播虚假信息的问题，并提出Holmes-Set和Holmes Pipeline等创新机制来改进检测技术。


<details>
  <summary>Details</summary>
Motivation: AI生成图像被用于传播虚假信息，对公共信息安全构成威胁，而现有检测技术存在缺乏解释性和技术泛化性问题。

Method: 引入Holmes-Set数据集，包括带解释的Holmes-SFTSet和人类偏好对齐集Holmes-DPOSet，开发高效数据注释方法Multi-Expert Jury，并提出三阶段的Holmes Pipeline训练框架，同时在推理阶段采用联合解码策略。

Result: 通过三个基准实验验证，AIGI-Holmes模型在AIGI检测任务中性能卓越，并具有良好的泛化能力和可解释性。

Conclusion: Holmes Pipeline及其模型AIGI-Holmes显著提升了AI生成图像检测的泛化性，同时提供了人类可验证的解释，为应对虚假信息传播提供了重要工具。

Abstract: The rapid development of AI-generated content (AIGC) technology has led to
the misuse of highly realistic AI-generated images (AIGI) in spreading
misinformation, posing a threat to public information security. Although
existing AIGI detection techniques are generally effective, they face two
issues: 1) a lack of human-verifiable explanations, and 2) a lack of
generalization in the latest generation technology. To address these issues, we
introduce a large-scale and comprehensive dataset, Holmes-Set, which includes
the Holmes-SFTSet, an instruction-tuning dataset with explanations on whether
images are AI-generated, and the Holmes-DPOSet, a human-aligned preference
dataset. Our work introduces an efficient data annotation method called the
Multi-Expert Jury, enhancing data generation through structured MLLM
explanations and quality control via cross-model evaluation, expert defect
filtering, and human preference modification. In addition, we propose Holmes
Pipeline, a meticulously designed three-stage training framework comprising
visual expert pre-training, supervised fine-tuning, and direct preference
optimization. Holmes Pipeline adapts multimodal large language models (MLLMs)
for AIGI detection while generating human-verifiable and human-aligned
explanations, ultimately yielding our model AIGI-Holmes. During the inference
stage, we introduce a collaborative decoding strategy that integrates the model
perception of the visual expert with the semantic reasoning of MLLMs, further
enhancing the generalization capabilities. Extensive experiments on three
benchmarks validate the effectiveness of our AIGI-Holmes.

</details>


### [61] [Learning few-step posterior samplers by unfolding and distillation of diffusion models](https://arxiv.org/abs/2507.02686)
*Charlesquin Kemajou Mbakam,Jonathan Spence,Marcelo Pereyra*

Main category: cs.CV

TL;DR: 该论文提出了一种结合深度展开和模型蒸馏的新框架，简化扩散模型在贝叶斯计算成像中的后验采样。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用扩散模型进行贝叶斯计算成像时，要么灵活性不足，要么依赖近似，因此需要一种高效又准确的方法。

Method: 通过深度展开和模型蒸馏，将扩散模型的图像先验转化为适用于后验采样的快速条件模型，并基于LATINO Langevin采样器设计了马尔科夫链蒙特卡洛算法的新实现。

Result: 实验结果表明，该方法在准确性和计算效率上均达到先进水平，同时保留了对前向模型变化的适应性。

Conclusion: 提出的方法不仅具有高效性和准确性，还能灵活适应不同的前向模型，表明在实际应用中前景广阔。

Abstract: Diffusion models (DMs) have emerged as powerful image priors in Bayesian
computational imaging. Two primary strategies have been proposed for leveraging
DMs in this context: Plug-and-Play methods, which are zero-shot and highly
flexible but rely on approximations; and specialized conditional DMs, which
achieve higher accuracy and faster inference for specific tasks through
supervised training. In this work, we introduce a novel framework that
integrates deep unfolding and model distillation to transform a DM image prior
into a few-step conditional model for posterior sampling. A central innovation
of our approach is the unfolding of a Markov chain Monte Carlo (MCMC) algorithm
- specifically, the recently proposed LATINO Langevin sampler (Spagnoletti et
al., 2025) - representing the first known instance of deep unfolding applied to
a Monte Carlo sampling scheme. We demonstrate our proposed unfolded and
distilled samplers through extensive experiments and comparisons with the state
of the art, where they achieve excellent accuracy and computational efficiency,
while retaining the flexibility to adapt to variations in the forward model at
inference time.

</details>


### [62] [APT: Adaptive Personalized Training for Diffusion Models with Limited Data](https://arxiv.org/abs/2507.02687)
*JungWoo Chae,Jiyoon Kim,JaeWoong Choi,Kyungyul Kim,Sangheum Hwang*

Main category: cs.CV

TL;DR: 本文提出了一种名为APT的框架，可以通过自适应训练策略及内部表示正则化来缓解扩散模型在有限数据情况下的过拟合问题，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 应对扩散模型在有限数据情境下的过拟合、丢失先验知识及文本匹配退化等问题。

Method: 提出APT框架，包括自适应训练调整、表示稳定化和注意力对齐三个关键部分，分别用于检测过拟合、正则化特征图以及维护先验知识。

Result: 实验表明，APT能有效减轻过拟合，保持先验知识的同时生成高质量、多样性的图像。

Conclusion: APT框架在有限参考数据上实现了优于现有方法的生成效果，展示了处理扩散模型个性化问题的潜力。

Abstract: Personalizing diffusion models using limited data presents significant
challenges, including overfitting, loss of prior knowledge, and degradation of
text alignment. Overfitting leads to shifts in the noise prediction
distribution, disrupting the denoising trajectory and causing the model to lose
semantic coherence. In this paper, we propose Adaptive Personalized Training
(APT), a novel framework that mitigates overfitting by employing adaptive
training strategies and regularizing the model's internal representations
during fine-tuning. APT consists of three key components: (1) Adaptive Training
Adjustment, which introduces an overfitting indicator to detect the degree of
overfitting at each time step bin and applies adaptive data augmentation and
adaptive loss weighting based on this indicator; (2)Representation
Stabilization, which regularizes the mean and variance of intermediate feature
maps to prevent excessive shifts in noise prediction; and (3) Attention
Alignment for Prior Knowledge Preservation, which aligns the cross-attention
maps of the fine-tuned model with those of the pretrained model to maintain
prior knowledge and semantic coherence. Through extensive experiments, we
demonstrate that APT effectively mitigates overfitting, preserves prior
knowledge, and outperforms existing methods in generating high-quality, diverse
images with limited reference data.

</details>


### [63] [CanonSwap: High-Fidelity and Consistent Video Face Swapping via Canonical Space Modulation](https://arxiv.org/abs/2507.02691)
*Xiangyang Luo,Ye Zhu,Yunfei Liu,Lijian Lin,Cong Wan,Zijian Cai,Shao-Lun Huang,Yu Li*

Main category: cs.CV

TL;DR: 提出了一种名为CanonSwap的新方法，通过运动信息与外观信息的解耦，实现高质量、动态一致的视频脸部交换。


<details>
  <summary>Details</summary>
Motivation: 现有方法在身份转移上效果较好，但在动态属性保持方面表现不佳，主要因为视频中面部外观与运动信息的耦合性。

Method: 提出CanonSwap框架，首先去除运动相关信息，在统一的规范空间中修改身份，然后将交换后的特征重新整合至原视频空间。引入部分身份调制模块，以空间掩模限定修改范围。此外，设计了多种细粒度同步指标评估方法性能。

Result: 大量实验表明，该方法在视觉质量、时间一致性和身份保留方面显著优于现有方法。

Conclusion: CanonSwap有效解决了视频中的面部外观与动态属性兼顾难题，为视频脸部交换提供了新途径并取得了优越的性能。

Abstract: Video face swapping aims to address two primary challenges: effectively
transferring the source identity to the target video and accurately preserving
the dynamic attributes of the target face, such as head poses, facial
expressions, lip-sync, \etc. Existing methods mainly focus on achieving
high-quality identity transfer but often fall short in maintaining the dynamic
attributes of the target face, leading to inconsistent results. We attribute
this issue to the inherent coupling of facial appearance and motion in videos.
To address this, we propose CanonSwap, a novel video face-swapping framework
that decouples motion information from appearance information. Specifically,
CanonSwap first eliminates motion-related information, enabling identity
modification within a unified canonical space. Subsequently, the swapped
feature is reintegrated into the original video space, ensuring the
preservation of the target face's dynamic attributes. To further achieve
precise identity transfer with minimal artifacts and enhanced realism, we
design a Partial Identity Modulation module that adaptively integrates source
identity features using a spatial mask to restrict modifications to facial
regions. Additionally, we introduce several fine-grained synchronization
metrics to comprehensively evaluate the performance of video face swapping
methods. Extensive experiments demonstrate that our method significantly
outperforms existing approaches in terms of visual quality, temporal
consistency, and identity preservation. Our project page are publicly available
at https://luoxyhappy.github.io/CanonSwap/.

</details>


### [64] [SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond Feature Alignment](https://arxiv.org/abs/2507.02705)
*Qi Xu,Dongxu Wei,Lingzhe Zhao,Wenpu Li,Zhangchi Huang,Shunping Ji,Peidong Liu*

Main category: cs.CV

TL;DR: 提出了一个无需对齐的框架SIU3R，可同时实现理解和3D重建，从未对齐图像中生成高性能的结果。


<details>
  <summary>Details</summary>
Motivation: 受到现有方法中2D到3D特征对齐方法难以提供高级3D理解能力并存在潜在语义信息损失的启发，试图打造一个更高效的处理方法。

Method: 提出SIU3R框架，通过像素对齐的3D表示桥接重建与理解任务，使用统一的可学习查询实现本地3D理解，并设计模块以增强表示共享任务的协作。

Result: 实验表明，SIU3R无论在单独的3D重建和理解任务还是联合任务中，都实现了最先进的性能。

Conclusion: 证明了无对齐框架的优势以及设计任务间相互收益机制的有效性。

Abstract: Simultaneous understanding and 3D reconstruction plays an important role in
developing end-to-end embodied intelligent systems. To achieve this, recent
approaches resort to 2D-to-3D feature alignment paradigm, which leads to
limited 3D understanding capability and potential semantic information loss. In
light of this, we propose SIU3R, the first alignment-free framework for
generalizable simultaneous understanding and 3D reconstruction from unposed
images. Specifically, SIU3R bridges reconstruction and understanding tasks via
pixel-aligned 3D representation, and unifies multiple understanding tasks into
a set of unified learnable queries, enabling native 3D understanding without
the need of alignment with 2D models. To encourage collaboration between the
two tasks with shared representation, we further conduct in-depth analyses of
their mutual benefits, and propose two lightweight modules to facilitate their
interaction. Extensive experiments demonstrate that our method achieves
state-of-the-art performance not only on the individual tasks of 3D
reconstruction and understanding, but also on the task of simultaneous
understanding and 3D reconstruction, highlighting the advantages of our
alignment-free framework and the effectiveness of the mutual benefit designs.

</details>


### [65] [UniMC: Taming Diffusion Transformer for Unified Keypoint-Guided Multi-Class Image Generation](https://arxiv.org/abs/2507.02713)
*Qin Guo,Ailing Zeng,Dongxu Yue,Ceyuan Yang,Yang Cao,Hanzhong Guo,Fei Shen,Wei Liu,Xihui Liu,Dan Xu*

Main category: cs.CV

TL;DR: 本论文提出一种新的框架UniMC和数据集HAIG-2.9M，用于改进基于关键点控制的多类别图像生成，特别是在复杂场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的关键点控制图像生成方法在生成更广泛的非刚性对象（如动物）以及处理多个重叠的人物和动物时存在局限性，并缺乏合适的数据集支持。

Method: 提出了UniMC框架，将实例级和关键点级条件整合为紧凑的标记，同时设计了HAIG-2.9M大规模高质量数据集，包含详细的关键点、边界框和描述标注，支持多类别图像生成。

Result: 通过实验验证，UniMC在处理遮挡严重和多类别场景时的表现显著优于现有方法。同时，HAIG-2.9M数据集被证明具有较高质量和适用性。

Conclusion: 结合UniMC框架和HAIG-2.9M数据集的方法有效提升了复杂多类别图像生成的控制能力，为未来研究和应用提供了新思路。

Abstract: Although significant advancements have been achieved in the progress of
keypoint-guided Text-to-Image diffusion models, existing mainstream
keypoint-guided models encounter challenges in controlling the generation of
more general non-rigid objects beyond humans (e.g., animals). Moreover, it is
difficult to generate multiple overlapping humans and animals based on keypoint
controls solely. These challenges arise from two main aspects: the inherent
limitations of existing controllable methods and the lack of suitable datasets.
First, we design a DiT-based framework, named UniMC, to explore unifying
controllable multi-class image generation. UniMC integrates instance- and
keypoint-level conditions into compact tokens, incorporating attributes such as
class, bounding box, and keypoint coordinates. This approach overcomes the
limitations of previous methods that struggled to distinguish instances and
classes due to their reliance on skeleton images as conditions. Second, we
propose HAIG-2.9M, a large-scale, high-quality, and diverse dataset designed
for keypoint-guided human and animal image generation. HAIG-2.9M includes 786K
images with 2.9M instances. This dataset features extensive annotations such as
keypoints, bounding boxes, and fine-grained captions for both humans and
animals, along with rigorous manual inspection to ensure annotation accuracy.
Extensive experiments demonstrate the high quality of HAIG-2.9M and the
effectiveness of UniMC, particularly in heavy occlusions and multi-class
scenarios.

</details>


### [66] [FairHuman: Boosting Hand and Face Quality in Human Image Generation with Minimum Potential Delay Fairness in Diffusion Models](https://arxiv.org/abs/2507.02714)
*Yuxuan Wang,Tianwei Cao,Huayu Zhang,Zhongjiang He,Kongming Liang,Zhanyu Ma*

Main category: cs.CV

TL;DR: 本文提出了一种名为FairHuman的方法，通过多目标微调技术，改进了生成图像的全局和局部细节质量，特别是在面部和手部的生成上表现优异。


<details>
  <summary>Details</summary>
Motivation: 生成高质量的具有细节的人类图像是一项重要且富有挑战性的任务，尤其在生成面部和手部细节上，现有模型存在监督不足问题。

Method: FairHuman通过引入三种学习目标（全局目标和基于预标注位置先验的两个局部目标），并采用最小潜在延迟（MPD）准则，优化多目标问题，实现公平的质量提升。

Result: 该方法显著提高了生成图像在面部和手部细节上的表现，同时保障了图像的总体质量。

Conclusion: 实验结果表明，FairHuman在不同场景下均能有效提升人类图像生成的性能，具备较高的实用价值。

Abstract: Image generation has achieved remarkable progress with the development of
large-scale text-to-image models, especially diffusion-based models. However,
generating human images with plausible details, such as faces or hands, remains
challenging due to insufficient supervision of local regions during training.
To address this issue, we propose FairHuman, a multi-objective fine-tuning
approach designed to enhance both global and local generation quality fairly.
Specifically, we first construct three learning objectives: a global objective
derived from the default diffusion objective function and two local objectives
for hands and faces based on pre-annotated positional priors. Subsequently, we
derive the optimal parameter updating strategy under the guidance of the
Minimum Potential Delay (MPD) criterion, thereby attaining fairness-ware
optimization for this multi-objective problem. Based on this, our proposed
method can achieve significant improvements in generating challenging local
details while maintaining overall quality. Extensive experiments showcase the
effectiveness of our method in improving the performance of human image
generation under different scenarios.

</details>


### [67] [Prompt learning with bounding box constraints for medical image segmentation](https://arxiv.org/abs/2507.02743)
*Mélanie Gaillochet,Mehrdad Noori,Sahar Dastani,Christian Desrosiers,Hervé Lombaert*

Main category: cs.CV

TL;DR: 提出了一种新框架，利用基础模型和弱监督分割，仅需边框标注即可生成分割提示，实现优秀性能。


<details>
  <summary>Details</summary>
Motivation: 医学领域像素级标注成本高，基于边框标注的弱监督方法是可行替代方案。如何结合基础模型和弱监督分割以减少用户干预是一个关键问题。

Method: 提出利用边框标注自动生成提示，结合伪标签和多重约束进行优化，通过基础模型实现弱监督分割。

Result: 在多模态数据集上平均Dice分数达到84.90%，优于现有全监督和弱监督方法。

Conclusion: 新方法结合了基础模型的表达能力和弱监督分割的高效性，在标注限制下提供了强竞争力的解决方案。

Abstract: Pixel-wise annotations are notoriously labourious and costly to obtain in the
medical domain. To mitigate this burden, weakly supervised approaches based on
bounding box annotations-much easier to acquire-offer a practical alternative.
Vision foundation models have recently shown noteworthy segmentation
performance when provided with prompts such as points or bounding boxes. Prompt
learning exploits these models by adapting them to downstream tasks and
automating segmentation, thereby reducing user intervention. However, existing
prompt learning approaches depend on fully annotated segmentation masks. This
paper proposes a novel framework that combines the representational power of
foundation models with the annotation efficiency of weakly supervised
segmentation. More specifically, our approach automates prompt generation for
foundation models using only bounding box annotations. Our proposed
optimization scheme integrates multiple constraints derived from box
annotations with pseudo-labels generated by the prompted foundation model.
Extensive experiments across multimodal datasets reveal that our weakly
supervised method achieves an average Dice score of 84.90% in a limited data
setting, outperforming existing fully-supervised and weakly-supervised
approaches. The code is available at
https://github.com/Minimel/box-prompt-learning-VFM.git

</details>


### [68] [DexVLG: Dexterous Vision-Language-Grasp Model at Scale](https://arxiv.org/abs/2507.02747)
*Jiawei He,Danshi Li,Xinqiang Yu,Zekun Qi,Wenyao Zhang,Jiayi Chen,Zhaoxiang Zhang,Zhizheng Zhang,Li Yi,He Wang*

Main category: cs.CV

TL;DR: DexVLG是一种大规模视觉语言抓取模型，专注于为仿人手设计的抓取姿态预测，并实现了语言对齐和高准确率。


<details>
  <summary>Details</summary>
Motivation: 传统研究受限于简单末端执行器控制，而DexVLG旨在推进复杂仿人手的抓取与语义指令对齐能力。

Method: 引入了包含1.7亿模拟抓取数据的DexGraspNet 3.0，并训练了结合视觉语言模型与流匹配的姿态预测模块。

Result: 模型在零样本泛化中表现优异，物理模拟中的零样本执行成功率超76%，现实场景中完成了语义对齐的抓取任务。

Conclusion: DexVLG显著提升了复杂仿人手抓取任务的效率与准确性，为机器人操作领域开辟了新方向。

Abstract: As large models gain traction, vision-language-action (VLA) systems are
enabling robots to tackle increasingly complex tasks. However, limited by the
difficulty of data collection, progress has mainly focused on controlling
simple gripper end-effectors. There is little research on functional grasping
with large models for human-like dexterous hands. In this paper, we introduce
DexVLG, a large Vision-Language-Grasp model for Dexterous grasp pose prediction
aligned with language instructions using single-view RGBD input. To accomplish
this, we generate a dataset of 170 million dexterous grasp poses mapped to
semantic parts across 174,000 objects in simulation, paired with detailed
part-level captions. This large-scale dataset, named DexGraspNet 3.0, is used
to train a VLM and flow-matching-based pose head capable of producing
instruction-aligned grasp poses for tabletop objects. To assess DexVLG's
performance, we create benchmarks in physics-based simulations and conduct
real-world experiments. Extensive testing demonstrates DexVLG's strong
zero-shot generalization capabilities-achieving over 76% zero-shot execution
success rate and state-of-the-art part-grasp accuracy in simulation-and
successful part-aligned grasps on physical objects in real-world scenarios.

</details>


### [69] [Linear Attention with Global Context: A Multipole Attention Mechanism for Vision and Physics](https://arxiv.org/abs/2507.02748)
*Alex Colagrande,Paul Caillon,Eva Feillet,Alexandre Allauzen*

Main category: cs.CV

TL;DR: 提出了一种新的注意力计算方法——多极注意力神经算子（MANO），在保持全局感受野的同时，实现线性时间和内存复杂度，并且在多个任务上达到了当前先进模型的性能，同时显著减少了运行时间和内存使用。


<details>
  <summary>Details</summary>
Motivation: 解决传统Transformer因输入长度导致的内存及时间复杂度过高问题，尤其是处理高分辨率输入时的效率瓶颈，同时保留细节信息。

Method: 借鉴$n$-体数值模拟的技术，将注意力计算建模为网格点之间的交互问题，提出MANO方法，使用基于距离的多尺度方式计算注意力，具备全局感受野，并实现了线性时间和内存复杂度。

Result: MANO在图像分类和Darcy流任务上的表现媲美ViT和Swin Transformer，并显著减少了运行时间和峰值内存使用量。

Conclusion: MANO方法在性能、效率和资源占用之间实现了良好平衡，为处理高分辨率输入提供了一种新颖的解决方案，并公开了代码以促进研究重现。

Abstract: Transformers have become the de facto standard for a wide range of tasks,
from image classification to physics simulations. Despite their impressive
performance, the quadratic complexity of standard Transformers in both memory
and time with respect to the input length makes them impractical for processing
high-resolution inputs. Therefore, several variants have been proposed, the
most successful relying on patchification, downsampling, or coarsening
techniques, often at the cost of losing the finest-scale details. In this work,
we take a different approach. Inspired by state-of-the-art techniques in
$n$-body numerical simulations, we cast attention as an interaction problem
between grid points. We introduce the Multipole Attention Neural Operator
(MANO), which computes attention in a distance-based multiscale fashion. MANO
maintains, in each attention head, a global receptive field and achieves linear
time and memory complexity with respect to the number of grid points. Empirical
results on image classification and Darcy flows demonstrate that MANO rivals
state-of-the-art models such as ViT and Swin Transformer, while reducing
runtime and peak memory usage by orders of magnitude. We open source our code
for reproducibility at https://github.com/AlexColagrande/MANO.

</details>


### [70] [Partial Weakly-Supervised Oriented Object Detection](https://arxiv.org/abs/2507.02751)
*Mingxin Liu,Peiyuan Zhang,Yuan Liu,Wei Zhang,Yue Zhou,Ning Liao,Ziyang Gong,Junwei Luo,Zhirui Wang,Yi Yu,Xue Yang*

Main category: cs.CV

TL;DR: 该研究提出了一种部分弱监督的方向性目标检测框架（PWOOD），以解决高数据标注成本问题，同时有效利用未标注数据。


<details>
  <summary>Details</summary>
Motivation: 解决方向性目标检测中数据标注成本高的问题，并实现有效利用部分弱标注数据以提升模型效能。

Method: 提出了PWOOD框架，包括OS-Student模型及CPF策略，利用部分弱标注数据学习方向和尺度信息并降低过滤门限的敏感性。

Result: 在DOTA和DIOR数据集上的实验表明，提出的PWOOD框架的性能达到甚至超越传统半监督方法。

Conclusion: PWOOD框架是一种高效、低成本的方向性目标检测方法，为领域内的研究提供了新的解决方案。

Abstract: The growing demand for oriented object detection (OOD) across various domains
has driven significant research in this area. However, the high cost of dataset
annotation remains a major concern. Current mainstream OOD algorithms can be
mainly categorized into three types: (1) fully supervised methods using
complete oriented bounding box (OBB) annotations, (2) semi-supervised methods
using partial OBB annotations, and (3) weakly supervised methods using weak
annotations such as horizontal boxes or points. However, these algorithms
inevitably increase the cost of models in terms of annotation speed or
annotation cost. To address this issue, we propose:(1) the first Partial
Weakly-Supervised Oriented Object Detection (PWOOD) framework based on
partially weak annotations (horizontal boxes or single points), which can
efficiently leverage large amounts of unlabeled data, significantly
outperforming weakly supervised algorithms trained with partially weak
annotations, also offers a lower cost solution; (2) Orientation-and-Scale-aware
Student (OS-Student) model capable of learning orientation and scale
information with only a small amount of orientation-agnostic or scale-agnostic
weak annotations; and (3) Class-Agnostic Pseudo-Label Filtering strategy (CPF)
to reduce the model's sensitivity to static filtering thresholds. Comprehensive
experiments on DOTA-v1.0/v1.5/v2.0 and DIOR datasets demonstrate that our PWOOD
framework performs comparably to, or even surpasses, traditional
semi-supervised algorithms.

</details>


### [71] [From Pixels to Damage Severity: Estimating Earthquake Impacts Using Semantic Segmentation of Social Media Images](https://arxiv.org/abs/2507.02781)
*Danrong Zhang,Huili Huang,N. Simrill Smith,Nimisha Roy,J. David Frost*

Main category: cs.CV

TL;DR: 研究提出将地震后社交媒体图片的损害评估问题转化为语义分割问题，并引入新的损害严重性评分体系。


<details>
  <summary>Details</summary>
Motivation: 传统的分类方法在评估地震后社交媒体图片损害时存在主观性强和难以处理图像中多变损害程度的问题。

Method: 研究通过构建分段的损害严重性数据集，并对SegFormer模型进行微调，实现对地震后社交媒体图像的损害分割和量化评估。

Result: 该方法能更客观、全面地量化地震后社交媒体图片的损害严重性。

Conclusion: 研究所提出的体系提升了灾后侦查的指导精准性，可更高效地进行定向响应。

Abstract: In the aftermath of earthquakes, social media images have become a crucial
resource for disaster reconnaissance, providing immediate insights into the
extent of damage. Traditional approaches to damage severity assessment in
post-earthquake social media images often rely on classification methods, which
are inherently subjective and incapable of accounting for the varying extents
of damage within an image. Addressing these limitations, this study proposes a
novel approach by framing damage severity assessment as a semantic segmentation
problem, aiming for a more objective analysis of damage in earthquake-affected
areas. The methodology involves the construction of a segmented damage severity
dataset, categorizing damage into three degrees: undamaged structures, damaged
structures, and debris. Utilizing this dataset, the study fine-tunes a
SegFormer model to generate damage severity segmentations for post-earthquake
social media images. Furthermore, a new damage severity scoring system is
introduced, quantifying damage by considering the varying degrees of damage
across different areas within images, adjusted for depth estimation. The
application of this approach allows for the quantification of damage severity
in social media images in a more objective and comprehensive manner. By
providing a nuanced understanding of damage, this study enhances the ability to
offer precise guidance to disaster reconnaissance teams, facilitating more
effective and targeted response efforts in the aftermath of earthquakes.

</details>


### [72] [RichControl: Structure- and Appearance-Rich Training-Free Spatial Control for Text-to-Image Generation](https://arxiv.org/abs/2507.02792)
*Liheng Zhang,Lexi Pang,Hang Ye,Xiaoxuan Ma,Yizhou Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的灵活特征注入框架，用于改进文本到图像扩散模型中的条件图像生成。


<details>
  <summary>Details</summary>
Motivation: 改进现有文本到图像扩散模型在注入条件图像时容易出现的结构失真、条件泄漏和视觉伪影问题。

Method: 提出了一个分离注入时间步与降噪过程的框架，并设计了结构丰富的注入模块。此外，还引入了外观提示和重新启动优化策略以提升生成图像的外观控制和质量。

Result: 通过实验验证，该方法在多种零样本条件生成场景中达到了最先进的性能。

Conclusion: 该方法无需训练即可生成富有结构和外观的高质量图像，有效克服了现有方法的局限性。

Abstract: Text-to-image (T2I) diffusion models have shown remarkable success in
generating high-quality images from text prompts. Recent efforts extend these
models to incorporate conditional images (e.g., depth or pose maps) for
fine-grained spatial control. Among them, feature injection methods have
emerged as a training-free alternative to traditional fine-tuning approaches.
However, they often suffer from structural misalignment, condition leakage, and
visual artifacts, especially when the condition image diverges significantly
from natural RGB distributions. By revisiting existing methods, we identify a
core limitation: the synchronous injection of condition features fails to
account for the trade-off between domain alignment and structural preservation
during denoising. Inspired by this observation, we propose a flexible feature
injection framework that decouples the injection timestep from the denoising
process. At its core is a structure-rich injection module, which enables the
model to better adapt to the evolving interplay between alignment and structure
preservation throughout the diffusion steps, resulting in more faithful
structural generation. In addition, we introduce appearance-rich prompting and
a restart refinement strategy to further enhance appearance control and visual
quality. Together, these designs enable training-free generation that is both
structure-rich and appearance-rich. Extensive experiments show that our
approach achieves state-of-the-art performance across diverse zero-shot
conditioning scenarios.

</details>


### [73] [No time to train! Training-Free Reference-Based Instance Segmentation](https://arxiv.org/abs/2507.02798)
*Miguel Espinosa,Chenhongyi Yang,Linus Ericsson,Steven McDonagh,Elliot J. Crowley*

Main category: cs.CV

TL;DR: 此研究提出了一种基于参考图像的全新目标分割方法，减少了数据标注和复杂提示生成的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前图像分割模型受限于大规模标注数据的需求，而Segment Anything Model仍需要人工提示或繁琐规则来处理新图像，亟需更简化的解决方案。

Method: 提出了一种无需训练、基于参考图像的分割方法，核心是利用基础模型的强大语义能力，通过多阶段流程：记忆库构建、表征聚合、语义感知特征匹配，自动生成实例级分割掩码。

Result: 在COCO FSOD、PASCAL VOC Few-Shot数据集上达到了最优性能，分别取得了36.8%和71.2%的nAP成绩；在跨域FSOD基准上显著超越同类方法（22.4% nAP）。

Conclusion: 该方法通过有效利用语义先验，大幅降低了依赖人工提示的程度，为目标分割任务提供了高效、简化的解决方案，同时实现了领先的性能表现。

Abstract: The performance of image segmentation models has historically been
constrained by the high cost of collecting large-scale annotated data. The
Segment Anything Model (SAM) alleviates this original problem through a
promptable, semantics-agnostic, segmentation paradigm and yet still requires
manual visual-prompts or complex domain-dependent prompt-generation rules to
process a new image. Towards reducing this new burden, our work investigates
the task of object segmentation when provided with, alternatively, only a small
set of reference images. Our key insight is to leverage strong semantic priors,
as learned by foundation models, to identify corresponding regions between a
reference and a target image. We find that correspondences enable automatic
generation of instance-level segmentation masks for downstream tasks and
instantiate our ideas via a multi-stage, training-free method incorporating (1)
memory bank construction; (2) representation aggregation and (3) semantic-aware
feature matching. Our experiments show significant improvements on segmentation
metrics, leading to state-of-the-art performance on COCO FSOD (36.8% nAP),
PASCAL VOC Few-Shot (71.2% nAP50) and outperforming existing training-free
approaches on the Cross-Domain FSOD benchmark (22.4% nAP).

</details>


### [74] [HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars](https://arxiv.org/abs/2507.02803)
*Gent Serifi,Marcel C. Bühler*

Main category: cs.CV

TL;DR: 本文提出了一种名为HyperGaussians的新方法，这是对3D Gaussian Splatting技术的扩展，用于实现高质量的可动画面部头像。


<details>
  <summary>Details</summary>
Motivation: 用视频生成高质量的可动画面部头像是一个挑战，并在增强和虚拟现实中有广泛的应用潜力。现有方法在处理简单静态面部效果时表现优异，但对多样化、复杂的动态和光照效果仍存在不足。

Method: 通过将传统的3D Gaussians扩展为高维多变量高斯结构（HyperGaussians），提高其表达能力，并使用“逆协方差技巧”降低计算复杂度，从而提高整体效率，使其能无缝嵌入现有模型。

Result: 在19名受试者的4个面部数据集上，HyperGaussians在数值和视觉效果上均优于传统的3D Gaussians，尤其在处理高频细节和复杂面部动态上表现出色。

Conclusion: HyperGaussians有效解决了创建可动画面部头像中的多种挑战，为生成更真实、更细致的面部表情提供了全新途径，具有实际应用价值。

Abstract: We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for
high-quality animatable face avatars. Creating such detailed face avatars from
videos is a challenging problem and has numerous applications in augmented and
virtual reality. While tremendous successes have been achieved for static
faces, animatable avatars from monocular videos still fall in the uncanny
valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face
through a collection of 3D Gaussian primitives. 3DGS excels at rendering static
faces, but the state-of-the-art still struggles with nonlinear deformations,
complex lighting effects, and fine details. While most related works focus on
predicting better Gaussian parameters from expression codes, we rethink the 3D
Gaussian representation itself and how to make it more expressive. Our insights
lead to a novel extension of 3D Gaussians to high-dimensional multivariate
Gaussians, dubbed 'HyperGaussians'. The higher dimensionality increases
expressivity through conditioning on a learnable local embedding. However,
splatting HyperGaussians is computationally expensive because it requires
inverting a high-dimensional covariance matrix. We solve this by
reparameterizing the covariance matrix, dubbed the 'inverse covariance trick'.
This trick boosts the efficiency so that HyperGaussians can be seamlessly
integrated into existing models. To demonstrate this, we plug in HyperGaussians
into the state-of-the-art in fast monocular face avatars: FlashAvatar. Our
evaluation on 19 subjects from 4 face datasets shows that HyperGaussians
outperform 3DGS numerically and visually, particularly for high-frequency
details like eyeglass frames, teeth, complex facial movements, and specular
reflections.

</details>


### [75] [LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion](https://arxiv.org/abs/2507.02813)
*Fangfu Liu,Hao Li,Jiawei Chi,Hanyang Wang,Minghui Yang,Fudong Wang,Yueqi Duan*

Main category: cs.CV

TL;DR: 论文提出LangScene-X框架，通过生成一致的多模态信息，从稀疏视图中构建通用的嵌入语言的3D场景。


<details>
  <summary>Details</summary>
Motivation: 现有方法在稀疏视图中重建3D结构时存在严重的渲染伪影和语义合成问题，需要一种新方法克服这些局限。

Method: 提出一个生成框架LangScene-X，利用TriMap视频扩散模型生成多模态信息，并通过语言压缩器（LQC）编码语言嵌入，在3D场景表面上对齐语言信息以实现语言查询。

Result: LangScene-X在稀疏视图场景下实现了高质量和良好通用性的3D场景重建和理解，优于现有方法。

Conclusion: LangScene-X统一语言和视觉信息，提出了一种通用的3D重建方法，对多模态生成和交互具有深远影响。

Abstract: Recovering 3D structures with open-vocabulary scene understanding from 2D
images is a fundamental but daunting task. Recent developments have achieved
this by performing per-scene optimization with embedded language information.
However, they heavily rely on the calibrated dense-view reconstruction
paradigm, thereby suffering from severe rendering artifacts and implausible
semantic synthesis when limited views are available. In this paper, we
introduce a novel generative framework, coined LangScene-X, to unify and
generate 3D consistent multi-modality information for reconstruction and
understanding. Powered by the generative capability of creating more consistent
novel observations, we can build generalizable 3D language-embedded scenes from
only sparse views. Specifically, we first train a TriMap video diffusion model
that can generate appearance (RGBs), geometry (normals), and semantics
(segmentation maps) from sparse inputs through progressive knowledge
integration. Furthermore, we propose a Language Quantized Compressor (LQC),
trained on large-scale image datasets, to efficiently encode language
embeddings, enabling cross-scene generalization without per-scene retraining.
Finally, we reconstruct the language surface fields by aligning language
information onto the surface of 3D scenes, enabling open-ended language
queries. Extensive experiments on real-world data demonstrate the superiority
of our LangScene-X over state-of-the-art methods in terms of quality and
generalizability. Project Page: https://liuff19.github.io/LangScene-X.

</details>


### [76] [Confidence-driven Gradient Modulation for Multimodal Human Activity Recognition: A Dynamic Contrastive Dual-Path Learning Approach](https://arxiv.org/abs/2507.02826)
*Panpan Ji,Junni Song,Hang Xiao,Hanyu Liu,Chao Li*

Main category: cs.CV

TL;DR: 本文提出DCDP-HAR框架，解决多模态感知中跨模态特征对齐和模态贡献不均的问题，以提升人类活动识别的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在解决多模态人类活动识别中的跨模态特征对齐困难和模态贡献不均等问题。

Method: 采用双路径特征提取架构、多阶段对比学习机制以及信任驱动的梯度调节策略，并结合动量梯度累积技术，改进模型训练和推理效率。

Result: 通过消融实验验证了每个组件的有效性，并在四个公开基准数据集上进行广泛比较实验，结果证明新方法的性能优越。

Conclusion: DCDP-HAR框架通过创新性的设计，提高了多模态人类活动识别的性能，并有效解决了跨模态对齐与模态不平衡的问题。

Abstract: Sensor-based Human Activity Recognition (HAR) is a core technology that
enables intelligent systems to perceive and interact with their environment.
However, multimodal HAR systems still encounter key challenges, such as
difficulties in cross-modal feature alignment and imbalanced modality
contributions. To address these issues, we propose a novel framework called the
Dynamic Contrastive Dual-Path Network (DCDP-HAR). The framework comprises three
key components. First, a dual-path feature extraction architecture is employed,
where ResNet and DenseNet branches collaboratively process multimodal sensor
data. Second, a multi-stage contrastive learning mechanism is introduced to
achieve progressive alignment from local perception to semantic abstraction.
Third, we present a confidence-driven gradient modulation strategy that
dynamically monitors and adjusts the learning intensity of each modality branch
during backpropagation, effectively alleviating modality competition. In
addition, a momentum-based gradient accumulation strategy is adopted to enhance
training stability. We conduct ablation studies to validate the effectiveness
of each component and perform extensive comparative experiments on four public
benchmark datasets.

</details>


### [77] [USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention Diffusion Network](https://arxiv.org/abs/2507.02827)
*Ying Yu,Hang Xiao,Siyao Li,Jiarui Li,Haotian Tang,Hanyu Liu,Chao Li*

Main category: cs.CV

TL;DR: 本文提出一种基于多注意力交互机制的人体活动识别优化模型，显著提升了准确率，验证了方法在嵌入式设备上的可行性。


<details>
  <summary>Details</summary>
Motivation: 解决人体活动识别中稀有活动标注样本稀缺、高级特征提取不足、轻量级设备上模型性能欠佳的问题。

Method: 采用统计指导的无监督扩散模型进行数据增强，设计多分支时空交互网络捕获多尺度特征，结合时空注意力机制识别关键时间点和增强传感器交互，并引入跨分支特征融合单元和动态多损失融合策略。

Result: 在WISDM、PAMAP2和OPPORTUNITY数据集上分别达到了98.84%、93.81%和80.92%的准确率，显著优于现有方法；同时验证了嵌入式设备上的效率和可行性。

Conclusion: 研究提出的USAD网络兼具高性能和嵌入式适用性，为人体活动识别提供了创新优化方案。

Abstract: The primary objective of human activity recognition (HAR) is to infer ongoing
human actions from sensor data, a task that finds broad applications in health
monitoring, safety protection, and sports analysis. Despite proliferating
research, HAR still faces key challenges, including the scarcity of labeled
samples for rare activities, insufficient extraction of high-level features,
and suboptimal model performance on lightweight devices. To address these
issues, this paper proposes a comprehensive optimization approach centered on
multi-attention interaction mechanisms. First, an unsupervised,
statistics-guided diffusion model is employed to perform data augmentation,
thereby alleviating the problems of labeled data scarcity and severe class
imbalance. Second, a multi-branch spatio-temporal interaction network is
designed, which captures multi-scale features of sequential data through
parallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels.
Simultaneously, temporal attention mechanisms are incorporated to identify
critical time points, while spatial attention enhances inter-sensor
interactions. A cross-branch feature fusion unit is further introduced to
improve the overall feature representation capability. Finally, an adaptive
multi-loss function fusion strategy is integrated, allowing for dynamic
adjustment of loss weights and overall model optimization. Experimental results
on three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the
proposed unsupervised data augmentation spatio-temporal attention diffusion
network (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively,
significantly outperforming existing approaches. Furthermore, practical
deployment on embedded devices verifies the efficiency and feasibility of the
proposed method.

</details>


### [78] [AnyI2V: Animating Any Conditional Image with Motion Control](https://arxiv.org/abs/2507.02857)
*Ziye Li,Hao Luo,Xincheng Shuai,Henghui Ding*

Main category: cs.CV

TL;DR: 文章提出了一个名为AnyI2V的非训练框架，用用户定义的运动轨迹来动态生成视频，该方法支持多种条件输入和灵活的视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有T2V方法空间布局控制不精确，I2V方法依赖真实图像限制可编辑性，并且使用ControlNet等方法成本高且缺乏明确的运动控制。

Method: 提出AnyI2V框架，不需要训练即可利用用户定义的运动轨迹对任意条件图像进行动画生成，支持多模态条件输入，如网格、点云，还支持混合条件输入、风格迁移和编辑。

Result: 实验结果表明，AnyI2V在空间和运动控制视频生成中表现优越。

Conclusion: AnyI2V提供了一种灵活、多样化的视频生成方式，为空间和运动控制视频生成领域提供了新方向。

Abstract: Recent advancements in video generation, particularly in diffusion models,
have driven notable progress in text-to-video (T2V) and image-to-video (I2V)
synthesis. However, challenges remain in effectively integrating dynamic motion
signals and flexible spatial constraints. Existing T2V methods typically rely
on text prompts, which inherently lack precise control over the spatial layout
of generated content. In contrast, I2V methods are limited by their dependence
on real images, which restricts the editability of the synthesized content.
Although some methods incorporate ControlNet to introduce image-based
conditioning, they often lack explicit motion control and require
computationally expensive training. To address these limitations, we propose
AnyI2V, a training-free framework that animates any conditional images with
user-defined motion trajectories. AnyI2V supports a broader range of modalities
as the conditional image, including data types such as meshes and point clouds
that are not supported by ControlNet, enabling more flexible and versatile
video generation. Additionally, it supports mixed conditional inputs and
enables style transfer and editing via LoRA and text prompts. Extensive
experiments demonstrate that the proposed AnyI2V achieves superior performance
and provides a new perspective in spatial- and motion-controlled video
generation. Code is available at https://henghuiding.com/AnyI2V/.

</details>


### [79] [Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for Data-Efficient Model Adaptation](https://arxiv.org/abs/2507.02859)
*Jiaer Xia,Bingkui Tong,Yuhang Zang,Rui Shao,Kaiyang Zhou*

Main category: cs.CV

TL;DR: 提出了一种结合图像与多模态语言模型的创新方法，通过引入Grounded Chain-of-Thought (GCoT)改进模型在缺乏数据情况下应对专业视觉任务的能力。


<details>
  <summary>Details</summary>
Motivation: 解决多模态语言模型在处理专业视觉任务时的适应性问题，主要因预训练数据和下游任务数据之间的差异导致。

Method: 提出Grounded Chain-of-Thought (GCoT)方法，通过在推理数据中注入定位信息（如边界框），改善多模态模型的推理步骤的准确性，并利用此方法在数据有限的专业视觉任务上进行训练和评估。

Result: 在五个包括图表、表格、收据和报告等专业视觉任务上，证明了GCoT方法在数据不足情况下显著优于微调和传统数据蒸馏方法。

Conclusion: 通过引入基于图像定位信息的推理数据增强，GCoT方法极大提升了多模态模型在专业视觉任务中的表现，尤其对数据有限场景更具优势。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in interpreting images using natural language. However, without
using large-scale datasets for retraining, these models are difficult to adapt
to specialized vision tasks, e.g., chart understanding. This problem is caused
by a mismatch between pre-training and downstream datasets: pre-training
datasets primarily concentrate on scenes and objects but contain limited
information about specialized, non-object images, such as charts and tables. In
this paper, we share an interesting finding that training an MLLM with
Chain-of-Thought (CoT) reasoning data can facilitate model adaptation in
specialized vision tasks, especially under data-limited regimes. However, we
identify a critical issue within CoT data distilled from pre-trained MLLMs,
i.e., the data often contains multiple factual errors in the reasoning steps.
To address the problem, we propose Grounded Chain-of-Thought (GCoT), a simple
bootstrapping-based approach that aims to inject grounding information (i.e.,
bounding boxes) into CoT data, essentially making the reasoning steps more
faithful to input images. We evaluate our approach on five specialized vision
tasks, which cover a variety of visual formats including charts, tables,
receipts, and reports. The results demonstrate that under data-limited regimes
our approach significantly improves upon fine-tuning and distillation.

</details>


### [80] [LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans](https://arxiv.org/abs/2507.02861)
*Zhening Huang,Xiaoyang Wu,Fangcheng Zhong,Hengshuang Zhao,Matthias Nießner,Joan Lasenby*

Main category: cs.CV

TL;DR: LiteReality是一种将室内环境的RGB-D扫描转化为紧凑、真实且可交互的3D虚拟复制品的新型方法。


<details>
  <summary>Details</summary>
Motivation: 旨在实现从室内扫描数据生成既真实又支持交互功能的3D场景，用于AR/VR、游戏、机器人技术和数字孪生等应用。

Method: 通过场景理解构建结构化场景图，再利用数据库中艺术家制作的模型进行场景重建，并通过Material Painting模块增强材料真实感，最终整合至仿真引擎实现交互性。

Result: 能够生成具有对象独立性、动作灵活性、高质量渲染材料和物理交互的3D场景，在Scan2CAD基准上达到最先进的相似性表现。

Conclusion: LiteReality在真实扫描和公共数据集上的表现证明其生成结果紧凑、可编辑且与标准图形管道完全兼容，为多种应用领域提供支持。

Abstract: We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor
environments into compact, realistic, and interactive 3D virtual replicas.
LiteReality not only reconstructs scenes that visually resemble reality but
also supports key features essential for graphics pipelines -- such as object
individuality, articulation, high-quality physically based rendering materials,
and physically based interaction. At its core, LiteReality first performs scene
understanding and parses the results into a coherent 3D layout and objects with
the help of a structured scene graph. It then reconstructs the scene by
retrieving the most visually similar 3D artist-crafted models from a curated
asset database. Next, the Material Painting module enhances realism by
recovering high-quality, spatially varying materials. Finally, the
reconstructed scene is integrated into a simulation engine with basic physical
properties to enable interactive behavior. The resulting scenes are compact,
editable, and fully compatible with standard graphics pipelines, making them
suitable for applications in AR/VR, gaming, robotics, and digital twins. In
addition, LiteReality introduces a training-free object retrieval module that
achieves state-of-the-art similarity performance on the Scan2CAD benchmark,
along with a robust material painting module capable of transferring
appearances from images of any style to 3D assets -- even under severe
misalignment, occlusion, and poor lighting. We demonstrate the effectiveness of
LiteReality on both real-life scans and public datasets. Project page:
https://litereality.github.io; Video:
https://www.youtube.com/watch?v=ecK9m3LXg2c

</details>


### [81] [Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching](https://arxiv.org/abs/2507.02860)
*Xin Zhou,Dingkang Liang,Kaijin Chen,Tianrui Feng,Xiwu Chen,Hongkai Lin,Yikang Ding,Feiyang Tan,Hengshuang Zhao,Xiang Bai*

Main category: cs.CV

TL;DR: 该论文提出EasyCache框架，以解决视频生成模型推理速度慢的问题，通过自适应缓存机制，减少冗余计算，无需离线分析或参数调整，在保持视觉质量的前提下显著加速。


<details>
  <summary>Details</summary>
Motivation: 由于去噪过程中计算资源需求高，视频生成技术的实际应用受到限制，论文目标是提升其推理速度与计算效率，使其适用于实际应用场景。

Method: 提出EasyCache框架，一种无需训练的加速方法，通过轻量化、自适应缓存机制动态复用计算结果，避免冗余计算，同时无需进行离线预计算或参数调试。

Result: 实验证明，在多种大规模视频生成模型上，EasyCache实现了较高的加速效果，推理时间减少2.1-3.3倍，并在视觉保真度上优于之前的SOTA方法，PSNR提升最多达36%。

Conclusion: EasyCache是一种高效且易用的解决方案，在保持高质量视频生成的同时，大幅降低推理时间，适合应用于研究与实际场景。

Abstract: Video generation models have demonstrated remarkable performance, yet their
broader adoption remains constrained by slow inference speeds and substantial
computational costs, primarily due to the iterative nature of the denoising
process. Addressing this bottleneck is essential for democratizing advanced
video synthesis technologies and enabling their integration into real-world
applications. This work proposes EasyCache, a training-free acceleration
framework for video diffusion models. EasyCache introduces a lightweight,
runtime-adaptive caching mechanism that dynamically reuses previously computed
transformation vectors, avoiding redundant computations during inference.
Unlike prior approaches, EasyCache requires no offline profiling,
pre-computation, or extensive parameter tuning. We conduct comprehensive
studies on various large-scale video generation models, including OpenSora,
Wan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,
reducing inference time by up to 2.1-3.3$\times$ compared to the original
baselines while maintaining high visual fidelity with a significant up to 36%
PSNR improvement compared to the previous SOTA method. This improvement makes
our EasyCache a efficient and highly accessible solution for high-quality video
generation in both research and practical applications. The code is available
at https://github.com/H-EmbodVis/EasyCache.

</details>


### [82] [Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory](https://arxiv.org/abs/2507.02863)
*Yuqi Wu,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文提出Point3R框架，用于实现有序或无序图像集合的流式稠密三维重建，使用显式空间指针记忆方法避免信息丢失。


<details>
  <summary>Details</summary>
Motivation: 现有方法中隐式记忆存在容量限制和信息损失问题，需新的方法更有效整合信息。

Method: 引入显式空间指针记忆，与当前场景三维结构直接关联；使用3D层次位置嵌入和高效的融合机制，实现全局坐标系统中的信息聚合。

Result: Point3R在多项任务中表现出竞争力或达到最优性能，且训练成本较低。

Conclusion: Point3R通过新的显式记忆方法和设计，在低训练成本下实现高效、稠密的流式三维重建，为相关研究提供了新的方向。

Abstract: Dense 3D scene reconstruction from an ordered sequence or unordered image
collections is a critical step when bringing research in computer vision into
practical scenarios. Following the paradigm introduced by DUSt3R, which unifies
an image pair densely into a shared coordinate system, subsequent methods
maintain an implicit memory to achieve dense 3D reconstruction from more
images. However, such implicit memory is limited in capacity and may suffer
from information loss of earlier frames. We propose Point3R, an online
framework targeting dense streaming 3D reconstruction. To be specific, we
maintain an explicit spatial pointer memory directly associated with the 3D
structure of the current scene. Each pointer in this memory is assigned a
specific 3D position and aggregates scene information nearby in the global
coordinate system into a changing spatial feature. Information extracted from
the latest frame interacts explicitly with this pointer memory, enabling dense
integration of the current observation into the global coordinate system. We
design a 3D hierarchical position embedding to promote this interaction and
design a simple yet effective fusion mechanism to ensure that our pointer
memory is uniform and efficient. Our method achieves competitive or
state-of-the-art performance on various tasks with low training costs. Code is
available at: https://github.com/YkiWu/Point3R.

</details>


### [83] [RefTok: Reference-Based Tokenization for Video Generation](https://arxiv.org/abs/2507.02862)
*Xiang Fan,Xiaohang Sun,Kushan Thakkar,Zhu Liu,Vimal Bhat,Ranjay Krishna,Xiang Hao*

Main category: cs.CV

TL;DR: 该论文提出了一种新的参考帧token化方法RefTok，成功捕获视频中的时序动态和上下文信息，并在多个数据集上超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 视频中的时序冗余处理是一个重要挑战，现有方法难以有效捕捉视频的时序依赖和冗余。

Method: 引入了RefTok方法，通过基于未量化的参考帧对多个帧集进行编码和解码，从而保留运动连续性和物体外观等信息。

Result: 在4个视频数据集上表现显著优于现有技术，多个指标平均提升36.7%，视频生成任务性能优于大的基线模型。

Conclusion: RefTok方法在处理视频时通过更优的时序分析显著提升模型性能，并展示出更高效的视频生成能力。

Abstract: Effectively handling temporal redundancy remains a key challenge in learning
video models. Prevailing approaches often treat each set of frames
independently, failing to effectively capture the temporal dependencies and
redundancies inherent in videos. To address this limitation, we introduce
RefTok, a novel reference-based tokenization method capable of capturing
complex temporal dynamics and contextual information. Our method encodes and
decodes sets of frames conditioned on an unquantized reference frame. When
decoded, RefTok preserves the continuity of motion and the appearance of
objects across frames. For example, RefTok retains facial details despite head
motion, reconstructs text correctly, preserves small patterns, and maintains
the legibility of handwriting from the context. Across 4 video datasets (K600,
UCF-101, BAIR Robot Pushing, and DAVIS), RefTok significantly outperforms
current state-of-the-art tokenizers (Cosmos and MAGVIT) and improves all
evaluated metrics (PSNR, SSIM, LPIPS) by an average of 36.7% at the same or
higher compression ratios. When a video generation model is trained using
RefTok's latents on the BAIR Robot Pushing task, the generations not only
outperform MAGVIT-B but the larger MAGVIT-L, which has 4x more parameters,
across all generation metrics by an average of 27.9%.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [84] [McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2507.02088)
*Tian Lan,Xiangdong Su,Xu Liu,Ruirui Wang,Ke Chang,Jiang Li,Guanglai Gao*

Main category: cs.CL

TL;DR: 构建了一个名为McBE的新基准，用于评估中文大语言模型的偏见，包含多种任务和广泛的分类。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的广泛应用，其内在偏见逐渐暴露。因此需要有效的方法去评估和缓解这种偏见，然而现有的偏见评估数据集多集中于英语和北美文化，缺乏适用于中文和中国文化的资源。

Method: 提出了一个多任务的中文偏见评估基准（McBE），包含4,077个评估实例，覆盖12个单一偏见类别、82个子类别，并引入5种评估任务，以便覆盖更广、内容更丰富及评估更全面。

Result: 评估了多种不同系列、不同参数规模的主流大语言模型，发现所有模型都表现出不同程度的偏见。

Conclusion: 研究对结果进行了深入分析，为大语言模型中的偏见提供了新的见解，并通过提出McBE为进一步研究提供支持。

Abstract: As large language models (LLMs) are increasingly applied to various NLP
tasks, their inherent biases are gradually disclosed. Therefore, measuring
biases in LLMs is crucial to mitigate its ethical risks. However, most existing
bias evaluation datasets focus on English and North American culture, and their
bias categories are not fully applicable to other cultures. The datasets
grounded in the Chinese language and culture are scarce. More importantly,
these datasets usually only support single evaluation tasks and cannot evaluate
the bias from multiple aspects in LLMs. To address these issues, we present a
Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias
evaluation instances, covering 12 single bias categories, 82 subcategories and
introducing 5 evaluation tasks, providing extensive category coverage, content
diversity, and measuring comprehensiveness. Additionally, we evaluate several
popular LLMs from different series and with parameter sizes. In general, all
these LLMs demonstrated varying degrees of bias. We conduct an in-depth
analysis of results, offering novel insights into bias in LLMs.

</details>


### [85] [Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization](https://arxiv.org/abs/2507.02145)
*Keyan Jin,Yapeng Wang,Leonel Santos,Tao Fang,Xu Yang,Sio Kei Im,Hugo Gonçalo Oliveira*

Main category: cs.CL

TL;DR: 本文研究了对话摘要任务中，长推理链（Long CoT）架构的性能，发现显式推理不一定提高对话摘要质量，甚至可能导致啰嗦、不一致和冗长。


<details>
  <summary>Details</summary>
Motivation: 虽然大规模语言模型在摘要任务上取得了进展，但对话摘要领域对长推理链方法的效果尚未深入研究。本研究旨在评估主流推理模型和非推理模型在不同对话摘要任务中的表现。

Method: 本文对多种推理语言模型（Long CoT实现）和非推理语言模型，开展系统评估，涵盖通用型、角色导向型及查询导向型对话摘要任务，并通过综合基准测试集和先进评估方法进行比较。

Result: 研究表明，显式的长推理链方法在对话摘要任务中的表现并不优于非推理模型，且容易导致冗长、事实性错误以及不够简洁的问题。

Conclusion: 显式推理在复杂对话背景中的效果有限，并强调针对对话摘要任务需要有特定的建模和评估策略。

Abstract: Dialogue summarization is a challenging task with significant practical value
in customer service, meeting analysis, and conversational AI. Although large
language models (LLMs) have achieved substantial progress in summarization
tasks, the performance of step-by-step reasoning architectures-specifically
Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and
DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent
abstraction and conciseness. In this work, we present the first comprehensive
and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning
LLMs across three major paradigms-generic, role-oriented, and query-oriented
dialogue summarization. Our study spans diverse languages, domains, and summary
lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and
advanced evaluation protocols that include both LLM-based automatic metrics and
human-inspired criteria. Contrary to trends in other reasoning-intensive tasks,
our findings show that explicit stepwise reasoning does not consistently
improve dialogue summarization quality. Instead, reasoning LLMs are often prone
to verbosity, factual inconsistencies, and less concise summaries compared to
their non-reasoning counterparts. Through scenario-specific analyses and
detailed case studies, we further identify when and why explicit reasoning may
fail to benefit-or even hinder-summarization in complex dialogue contexts. Our
work provides new insights into the limitations of current reasoning LLMs and
highlights the need for targeted modeling and evaluation strategies for
real-world dialogue summarization.

</details>


### [86] [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)
*Wenquan Lu,Yuechuan Yang,Kyle Lee,Yanshu Li,Enqi Liu*

Main category: cs.CL

TL;DR: 该论文研究了在深度循环Transformer Huginn-3.5B中是否能出现隐式的链式推理结构，但有限证据表明模型的隐式链式推理能力。


<details>
  <summary>Details</summary>
Motivation: 近期研究中，链式推理通过外化自然语言显著提升了推理任务的表现。然而，这种方法效率较低，同时无法捕捉难以用语言表示的推理过程，因此推进latent CoT的潜力成为关注焦点。

Method: 通过对Huginn-3.5B模型在算术任务中的行为进行调查，采用Logit Lens和Coda Lens对模型的中间状态及排名轨迹进行解析，同时观察不同循环深度及解码方法对表现的影响。

Result: 研究表明，模型产生隐式链式推理的证据较为有限，且不同循环块之间的探测结果存在显著不一致，解码方式及层次位置均对解释性产生影响。此外，增加循环深度带来的提升效果有限，远不如显式推理模型。

Conclusion: 深度循环Transformer在捕捉隐式推理方面效果有限，其性能难以超越显式推理方法。未来需要更加有效的机制来平衡可解释性与推理效率。

Abstract: Chain-of-thought (CoT) reasoning has enabled transformer-based language
models to excel at complex mathematics and multi-step planning. However, in
standard decoder-only architectures, these reasoning steps are externalized in
natural language, improving interpretability at the cost of efficiency. To
capture reasoning that is not easily represented in words, many works have
explored recurrent architectures that aim to internalize reasoning in latent
space, potentially supporting latent CoT. In this paper, we investigate whether
such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer
that reuses layers at inference time without increasing parameter count. We
examine the model's internal behavior on arithmetic tasks using a suite of
probing techniques including the Logit Lens and Coda Lens. Our findings reveal
limited evidence of interpretable latent CoT by tracking rank trajectories of
final and intermediate result tokens. Furthermore, we uncover significant
probing inconsistencies across recurrent blocks, where the interpretability of
hidden states depends heavily on both the layer index and the decoding method.
Finally, we empirically show that increasing recurrence depth yields only
marginal gains and falls well short of models that explicitly externalize
reasoning steps. The code is available at
https://github.com/wenquanlu/huginn-latent-cot.

</details>


### [87] [GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons](https://arxiv.org/abs/2507.02221)
*Steven Song,Anirudh Subramanyam,Zhenyu Zhang,Aarti Venkat,Robert L. Grossman*

Main category: cs.CL

TL;DR: 本文提出了一种名为GDC Cohort Copilot的开源工具，可通过用户的自然语言描述自动生成癌症基因组数据集（GDC）的过滤器，从而便于构建患者队列。


<details>
  <summary>Details</summary>
Motivation: 为了解决用户在GDC Cohort Builder中由于庞大字段信息而难以描述和筛选病例队列的问题，借助自然语言描述的能力来简化操作过程。

Method: 开发了一种名为GDC Cohort Copilot的工具，结合多种大语言模型（LLM），使用户通过自然语言描述生成GDC队列过滤器，附带交互界面以进一步调整队列。

Result: 实验表明，与GPT-4o相比，GDC Cohort Copilot基于的开源本地服务LLM在生成GDC队列方面表现更优。

Conclusion: GDC Cohort Copilot工具通过自然语言处理技术简化了队列创建流程，提升了用户体验，并为癌症基因组数据的分析和利用提供了便利。

Abstract: Motivation: The Genomic Data Commons (GDC) provides access to high quality,
harmonized cancer genomics data through a unified curation and analysis
platform centered around patient cohorts. While GDC users can interactively
create complex cohorts through the graphical Cohort Builder, users (especially
new ones) may struggle to find specific cohort descriptors across hundreds of
possible fields and properties. However, users may be better able to describe
their desired cohort in free-text natural language.
  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for
curating cohorts from the GDC. GDC Cohort Copilot automatically generates the
GDC cohort filter corresponding to a user-input natural language description of
their desired cohort, before exporting the cohort back to the GDC for further
analysis. An interactive user interface allows users to further refine the
generated cohort. We develop and evaluate multiple large language models (LLMs)
for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC
Cohort LLM achieves better results than GPT-4o prompting in generating GDC
cohorts.
  Availability and implementation: The standalone docker image for GDC Cohort
Copilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.
Source code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC
Cohort LLM weights are available at https://huggingface.co/uc-ctds.

</details>


### [88] [MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)
*Hongli Yu,Tinghong Chen,Jiangtao Feng,Jiangjie Chen,Weinan Dai,Qiying Yu,Ya-Qin Zhang,Wei-Ying Ma,Jingjing Liu,Mingxuan Wang,Hao Zhou*

Main category: cs.CL

TL;DR: 研究旨在解决处理无限长文本时的线性复杂性问题，提出了一种名为MemAgent的新型方法。通过增强的DAPO算法进行训练，显著提升了长文本任务的能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长文本任务中表现下降，而处理无限长文本的需求日益增长。研究动机是优化在长文本场景中的性能，克服现有方法中的扩展问题。

Method: 提出MemAgent，这是一种分段阅读和内存覆盖更新的新式代理机制；通过扩展DAPO算法实现独立上下文的多轮对话生成，与长文本任务直接相关的端到端优化方式。

Result: 实验表明，虽然仅在32K文本数据上进行训练，但在3.5M QA任务上的性能损失小于5%；在512K长度的RULER测试中准确率超过95%。

Conclusion: MemAgent具备强大的长上下文处理能力，可以在低性能损失的情况下高效处理超长文本任务，为长文本分析提供了新的解决方案。

Abstract: Despite improvements by length extrapolation, efficient attention and memory
modules, handling infinitely long documents with linear complexity without
performance degradation during extrapolation remains the ultimate challenge in
long-text processing. We directly optimize for long-text tasks in an end-to-end
fashion and introduce a novel agent workflow, MemAgent, which reads text in
segments and updates the memory using an overwrite strategy. We extend the DAPO
algorithm to facilitate training via independent-context multi-conversation
generation. MemAgent has demonstrated superb long-context capabilities, being
able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task
with performance loss < 5% and achieves 95%+ in 512K RULER test.

</details>


### [89] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

TL;DR: 提出了一种名为DoMIX的新方法，解决了现有持续域自适应预训练的高计算成本、数据顺序敏感性以及单一泛化模型的问题。


<details>
  <summary>Details</summary>
Motivation: 现有持续域自适应预训练方法存在计算成本高、对数据顺序敏感以及无法针对特定任务提供优化模型的问题，需要新的解决方案。

Method: 采用LoRA模块（高效参数微调方法），实现高效且并行的域自适应预训练，同时减少对域顺序的依赖，并针对具体任务提供优化模型。

Result: 验证表明DoMIX不仅有效解决了现有问题，还可扩展到LLM的标准微调场景中。

Conclusion: DoMIX提升了域自适应预训练的效率和适用性，同时扩展了其应用范围。

Abstract: Domain-Adaptive Pre-training (DAP) has recently gained attention for its
effectiveness in fine-tuning pre-trained models. Building on this, continual
DAP has been explored to develop pre-trained models capable of incrementally
incorporating different domain datasets. However, existing continual DAP
methods face several limitations: (1) high computational cost and GPU memory
usage during training; (2) sensitivity to incremental data order; and (3)
providing a single, generalized model for all end tasks, which contradicts the
essence of DAP. In this paper, we propose DoMIX, a novel approach that
addresses these challenges by leveraging LoRA modules, a representative
parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient
and parallel domain-adaptive pre-training that is robust to domain order and
effectively utilizes accumulated knowledge to provide tailored pre-trained
models for specific tasks. We also demonstrate that our method can be extended
beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available
at https://github.com/dohoonkim-ai/DoMIX.

</details>


### [90] [Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models](https://arxiv.org/abs/2507.02357)
*Christian Jaumann,Annemarie Friedrich,Rainer Lienhart*

Main category: cs.CL

TL;DR: 该论文介绍了SciVQA 2025竞赛中提出的科学视觉问答系统，该系统使用多模态大语言模型和少样例检索策略，并在多项评价指标上排名第三。


<details>
  <summary>Details</summary>
Motivation: 为了解决科学视觉问答任务中的挑战，提出了基于多模态大语言模型和少样例策略的系统。

Method: 使用多模态大语言模型的集成，并根据图表和问题类型选择少样例示例，同时基于模型置信水平选择答案。

Result: 在测试数据集上，该系统在7个参赛系统中排名第三，平均F1得分为85.12，涵盖ROUGE-1、ROUGE-L和BERTS指标。

Conclusion: 该系统通过模型选择与置信度结合取得了较高的性能，为科学视觉问答任务提供了有效解决方案，并且代码已经开源。

Abstract: This paper describes our system for the SciVQA 2025 Shared Task on Scientific
Visual Question Answering. Our system employs an ensemble of two Multimodal
Large Language Models and various few-shot example retrieval strategies. The
model and few-shot setting are selected based on the figure and question type.
We also select answers based on the models' confidence levels. On the blind
test data, our system ranks third out of seven with an average F1 score of
85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.

</details>


### [91] [QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers](https://arxiv.org/abs/2507.02364)
*Pilsung Kang*

Main category: cs.CL

TL;DR: 本论文提出了一种称为QFFN-BERT的混合量子-经典Transformer模型，用参数化量子电路（PQC）替换BERT中前馈网络（FFN），实验结果显示其能够在保持较少参数的同时实现较高精度。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer的FFN模块占据大量参数，研究用参数化量子电路替代FFN模块的可行性，以提高模型的参数效率和数据利用率。

Method: 将一个紧凑的BERT变体中的FFN模块用带有残差连接、含$R_Y$和$R_Z$旋转及交替纠缠策略的PQC层替代，用于稳定训练和提升表达能力，并进行系统的实验对比。

Result: 提出的QFFN-BERT在SST-2和DBpedia数据集上实现了与基线模型相当或更优的精度，同时FFN参数减少99%以上，并在小样本学习情况下表现出更高的竞争力。

Conclusion: 通过与深度学习原则协同设计，参数化量子电路能够成为传统FFN高效的参数替代方案，兼具实用性与精度。

Abstract: Parameterized quantum circuits (PQCs) have recently emerged as promising
components for enhancing the expressibility of neural architectures. In this
work, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the
feedforward network (FFN) modules of a compact BERT variant are replaced by
PQC-based layers. This design is motivated by the dominant parameter
contribution of FFNs, which account for approximately two-thirds of the
parameters within standard Transformer encoder blocks. While prior studies have
primarily integrated PQCs into self-attention modules, our work focuses on the
FFN and systematically investigates the trade-offs between PQC depth,
expressibility, and trainability. Our final PQC architecture incorporates a
residual connection, both $R_Y$ and $R_Z$ rotations, and an alternating
entanglement strategy to ensure stable training and high expressibility. Our
experiments, conducted on a classical simulator, on the SST-2 and DBpedia
benchmarks demonstrate two key findings. First, a carefully configured
QFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its
classical counterpart in a full-data setting while reducing FFN-specific
parameters by over 99%. Second, our model exhibits a consistent and competitive
edge in few-shot learning scenarios, confirming its potential for superior data
efficiency. These results, supported by an ablation study on a non-optimized
PQC that failed to learn, confirm that PQCs can serve as powerful and
parameter-efficient alternatives to classical FFNs when co-designed with
foundational deep learning principles.

</details>


### [92] [Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection](https://arxiv.org/abs/2507.02378)
*Weijie Lyu,Sheng-Jun Huang,Xuan Xia*

Main category: cs.CL

TL;DR: 引入了一种基于参数化模型的代码数据选择方法，以提升大型语言模型的训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 当前模型多注重数据量而忽视数据质量，导致训练效率降低。为解决这一问题，提出一种专注高质量数据选择的策略。

Method: 提出的一种参数化模型选择方法，通过保证数据分布一致性和多样性生成高质量数据子集。

Result: 实验结果表明，仅使用10K样本就能比92K全采样基线在HumanEval和MBPP基准上分别提升2.4%和2.3%。

Conclusion: 该方法在显著降低计算成本的同时，有效提升了模型性能和训练效率。

Abstract: Recent advancements in large language models (LLMs) have significantly
improved code generation and program comprehension, accelerating the evolution
of software engineering. Current methods primarily enhance model performance by
leveraging vast amounts of data, focusing on data quantity while often
overlooking data quality, thereby reducing training efficiency. To address
this, we introduce an approach that utilizes a parametric model for code data
selection, aimed at improving both training efficiency and model performance.
Our method optimizes the parametric model to ensure distribution consistency
and diversity within the selected subset, guaranteeing high-quality data.
Experimental results demonstrate that using only 10K samples, our method
achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled
baseline, outperforming other sampling approaches in both performance and
efficiency. This underscores that our method effectively boosts model
performance while significantly reducing computational costs.

</details>


### [93] [Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability](https://arxiv.org/abs/2507.02407)
*Mark Atta Mensah,Isaac Wiafe,Akon Ekpezu,Justice Kwame Appati,Jamal-Deen Abdulai,Akosua Nyarkoa Wiafe-Akenten,Frank Ernest Yeboah,Gifty Odame*

Main category: cs.CL

TL;DR: 研究比较了7种基于Transformer架构的Akan语音识别模型在不同语料库下的表现，揭示出模型的领域依赖性和错误行为差异。


<details>
  <summary>Details</summary>
Motivation: 现有语音识别研究较少评估模型在多样化语境下的泛化能力。研究旨在分析Akan语音模型在不同语料库领域间的性能表现差异。

Method: 使用Whisper和Wav2Vec2等Transformer架构的七个Akan语音识别模型，通过四个不同领域的Akan语料库（如图像描述、对话、经文朗读等）比较字错误率和词错误率。

Result: 研究发现模型在训练域内表现最佳，而在不匹配场景中准确率显著下降。Whisper模型容易产生流畅但误导的错误，Wav2Vec2则在陌生输入中输出较低可解释性。

Conclusion: 研究强调需要为低资源语言开发针对性领域适配策略、动态路由方法和多语言训练框架，以提升其在不同领域的性能。

Abstract: Most existing automatic speech recognition (ASR) research evaluate models
using in-domain datasets. However, they seldom evaluate how they generalize
across diverse speech contexts. This study addresses this gap by benchmarking
seven Akan ASR models built on transformer architectures, such as Whisper and
Wav2Vec2, using four Akan speech corpora to determine their performance. These
datasets encompass various domains, including culturally relevant image
descriptions, informal conversations, biblical scripture readings, and
spontaneous financial dialogues. A comparison of the word error rate and
character error rate highlighted domain dependency, with models performing
optimally only within their training domains while showing marked accuracy
degradation in mismatched scenarios. This study also identified distinct error
behaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned
Whisper Akan models led to more fluent but potentially misleading transcription
errors, Wav2Vec2 produced more obvious yet less interpretable outputs when
encountering unfamiliar inputs. This trade-off between readability and
transparency in ASR errors should be considered when selecting architectures
for low-resource language (LRL) applications. These findings highlight the need
for targeted domain adaptation techniques, adaptive routing strategies, and
multilingual training frameworks for Akan and other LRLs.

</details>


### [94] [A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages](https://arxiv.org/abs/2507.02428)
*Sumaya Ahmed Salihs,Isaac Wiafe,Jamal-Deen Abdulai,Elikem Doe Atsakpo,Gifty Ayoka,Richard Cave,Akon Obu Ekpezu,Catherine Holloway,Katrin Tomanek,Fiifi Baffoe Payin Winful*

Main category: cs.CL

TL;DR: 这项研究提出了一个为语言障碍语音创建语音识别模型的数据收集方法，特别是面向低资源语言，如加纳的阿坎语。


<details>
  <summary>Details</summary>
Motivation: 旨在通过制定“最佳实践手册”和培训，在社区驱动下进行数据收集和自动语音识别模型的构建，使语音识别技术更加普及。

Method: 作为概念验证，研究创建了首个针对加纳阿坎语言障碍语音的开源数据集，并公开相关工具和手册供研究人员使用。

Result: 初步结果显示，微调的开源语音识别模型能更好地识别阿坎语言障碍语音。

Conclusion: 研究实现了为语言障碍个体开发包容性语音识别技术的目标，提供了数据集和工具支持进一步研究。

Abstract: This study presents an approach for collecting speech samples to build
Automatic Speech Recognition (ASR) models for impaired speech, particularly,
low-resource languages. It aims to democratize ASR technology and data
collection by developing a "cookbook" of best practices and training for
community-driven data collection and ASR model building. As a proof-of-concept,
this study curated the first open-source dataset of impaired speech in Akan: a
widely spoken indigenous language in Ghana. The study involved participants
from diverse backgrounds with speech impairments. The resulting dataset, along
with the cookbook and open-source tools, are publicly available to enable
researchers and practitioners to create inclusive ASR technologies tailored to
the unique needs of speech impaired individuals. In addition, this study
presents the initial results of fine-tuning open-source ASR models to better
recognize impaired speech in Akan.

</details>


### [95] [IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders](https://arxiv.org/abs/2507.02506)
*Sneha Deshmukh,Prathmesh Kamble*

Main category: cs.CL

TL;DR: 文章介绍了一种新数据集IndianBailJudgments-1200，用于法律自然语言处理（Legal NLP）。


<details>
  <summary>Details</summary>
Motivation: 印度法律 NLP 的发展受到数据集缺乏的限制。

Method: 提出了一个包含1200份印度保释判决的标注数据集，采用 GPT-4o 管道生成和验证标注。

Result: 提供了一个结构化、可用于多种任务的印度法律数据集。

Conclusion: 此数据集首次专注于印度保释判决，为法律 NLP 提供了重要资源。

Abstract: Legal NLP remains underdeveloped in regions like India due to the scarcity of
structured datasets. We introduce IndianBailJudgments-1200, a new benchmark
dataset comprising 1200 Indian court judgments on bail decisions, annotated
across 20+ attributes including bail outcome, IPC sections, crime type, and
legal reasoning. Annotations were generated using a prompt-engineered GPT-4o
pipeline and verified for consistency. This resource supports a wide range of
legal NLP tasks such as outcome prediction, summarization, and fairness
analysis, and is the first publicly available dataset focused specifically on
Indian bail jurisprudence.

</details>


### [96] [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/abs/2507.02592)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Liwen Zhang,Litu Ou,Jialong Wu,Wenbiao Yin,Baixuan Li,Zhengwei Tao,Xinyu Wang,Weizhou Shen,Junkai Zhang,Dingchu Zhang,Xixi Wu,Yong Jiang,Ming Yan,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 提出了WebSailor，一种后训练方法，通过复杂任务训练提升模型在信息检索任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有开源模型在极度复杂的信息检索任务中的性能不足，接近专有模型能力。

Method: 提出一种后训练方法WebSailor，通过结构化采样生成高不确定性任务、信息模糊化、RFT冷启动以及Duplicating Sampling Policy Optimization (DUPO)增强模型推理能力。

Result: WebSailor在复杂信息检索任务中优于所有开源模型，性能达到专有模型水平。

Conclusion: WebSailor成功弥补了开源与专有模型在复杂信息任务上的性能差距，体现了高效的后训练策略。

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all opensource agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


### [97] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)
*Cornelia Gruber,Helen Alber,Bernd Bischl,Göran Kauermann,Barbara Plank,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 本论文探讨了标签变异（LV）在自然语言处理中常见的问题，提出结合人类标签变化（HLV）信息的主动学习框架，并讨论大语言模型在标注中的融合。


<details>
  <summary>Details</summary>
Motivation: 基于单一标签的传统标注框架忽略了人类标签变异这一重要信息，而主动学习中的关键假设也难以应对标注数据中的复杂性。

Method: 研究将观察到的标签变异分解为信号（如HLV）和噪声（如标注错误），并提出将HLV整合至主动学习循环中的概念框架，包括实例选择、标注者选择和标签表示。

Result: 总结现有主动学习和标签变异领域的研究成果，提出更贴合真实标注场景的HLV感知主动学习方法。

Conclusion: 增强对人类标签变异的考虑，可为主动学习提供更可靠的指导，同时应进一步探索大语言模型作为标注者的潜力。

Abstract: Access to high-quality labeled data remains a limiting factor in applied
supervised learning. While label variation (LV), i.e., differing labels for the
same instance, is common, especially in natural language processing, annotation
frameworks often still rest on the assumption of a single ground truth. This
overlooks human label variation (HLV), the occurrence of plausible differences
in annotations, as an informative signal. Similarly, active learning (AL), a
popular approach to optimizing the use of limited annotation budgets in
training ML models, often relies on at least one of several simplifying
assumptions, which rarely hold in practice when acknowledging HLV. In this
paper, we examine foundational assumptions about truth and label nature,
highlighting the need to decompose observed LV into signal (e.g., HLV) and
noise (e.g., annotation error). We survey how the AL and (H)LV communities have
addressed -- or neglected -- these distinctions and propose a conceptual
framework for incorporating HLV throughout the AL loop, including instance
selection, annotator choice, and label representation. We further discuss the
integration of large language models (LLM) as annotators. Our work aims to lay
a conceptual foundation for HLV-aware active learning, better reflecting the
complexities of real-world annotation.

</details>


### [98] [MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion](https://arxiv.org/abs/2507.02595)
*Xin Guan,PeiHsin Lin,Zekun Wu,Ze Wang,Ruibo Zhang,Emre Kazim,Adriano Koshiyama*

Main category: cs.CL

TL;DR: 提出了一种新型的后期训练对齐框架MPF，用于大语言模型的偏差缓解。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型需要简便的偏差缓解方法。

Method: 基于SAGED管道，通过解构基线并平衡多角度生成的响应来缓解偏差。

Result: 验证了MPF能够有效对齐模型输出分布并降低校准误差，同时具备很好的泛化能力。

Conclusion: MPF是一种可扩展且可解释的对齐和偏差缓解方法，对现有部署的大语言模型适用。

Abstract: Multiperspective Fusion (MPF) is a novel posttraining alignment framework for
large language models (LLMs) developed in response to the growing need for easy
bias mitigation. Built on top of the SAGED pipeline, an automated system for
constructing bias benchmarks and extracting interpretable baseline
distributions, MPF leverages multiperspective generations to expose and align
biases in LLM outputs with nuanced, humanlike baselines. By decomposing
baseline, such as sentiment distributions from HR professionals, into
interpretable perspective components, MPF guides generation through sampling
and balancing of responses, weighted by the probabilities obtained in the
decomposition. Empirically, we demonstrate its ability to align LLM sentiment
distributions with both counterfactual baselines (absolute equality) and the HR
baseline (biased for Top Univeristy), resulting in small KL divergence,
reduction of calibration error and generalization to unseen questions. This
shows that MPF offers a scalable and interpretable method for alignment and
bias mitigation, compatible with deployed LLMs and requiring no extensive
prompt engineering or finetuning.

</details>


### [99] [Exploring Gender Bias Beyond Occupational Titles](https://arxiv.org/abs/2507.02679)
*Ahmed Sabir,Rajesh Sharama*

Main category: cs.CL

TL;DR: 本文研究了性别和上下文偏见的相关性，尤其是动作动词、物品名词以及职业方面的偏见，提出了GenderLexicon数据集和一种估算偏见的框架。


<details>
  <summary>Details</summary>
Motivation: 探讨性别偏见在不同上下文中存在的表现以及如何量化这些偏见。

Method: 提出了GenderLexicon数据集以及用于估算和解释偏见的框架，通过评分的方式来量化偏见并增强其解释性。

Result: 验证了性别偏见的存在不仅限于职业定型偏见，同时在五个多样化数据集上评估了方法的有效性。

Conclusion: 讨论了性别偏见的广泛存在及其在不同上下文中的表现，为偏见分析与解释提供了有效工具。

Abstract: In this work, we investigate the correlation between gender and contextual
biases, focusing on elements such as action verbs, object nouns, and
particularly on occupations. We introduce a novel dataset, GenderLexicon, and a
framework that can estimate contextual bias and its related gender bias. Our
model can interpret the bias with a score and thus improve the explainability
of gender bias. Also, our findings confirm the existence of gender biases
beyond occupational stereotypes. To validate our approach and demonstrate its
effectiveness, we conduct evaluations on five diverse datasets, including a
Japanese dataset.

</details>


### [100] [Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers](https://arxiv.org/abs/2507.02694)
*Zhijian Xu,Yilun Zhao,Manasi Patwardhan,Lovekesh Vig,Arman Cohan*

Main category: cs.CL

TL;DR: 该研究提出一个名为LimitGen的基准，用于评估大型语言模型（LLM）在科学审稿中识别论文局限性的能力，并通过结合文献检索进一步提升这一能力。


<details>
  <summary>Details</summary>
Motivation: 随着科学出版物数量的增加，同行评审负担加重，需要探索如何利用LLM来辅助这一过程，特别是在发现论文局限性方面的潜力。

Method: 研究开发了一个局限性类型的分类体系，并以此为基础构建了LimitGen基准数据集，包括通过高质量论文生成的合成数据子集（LimitGen-Syn）和真实人为撰写的局限性子集（LimitGen-Human）。此外，方法还结合文献检索功能，以改进LLM系统识别局限性的表现。

Result: 使用LimitGen基准评估表明，文献检索的加入能够有效增强LLM生成论文局限性的具体性和建设性反馈能力。

Conclusion: 该研究展示了LLM在科学审稿中评估局限性方面的潜力，并通过LimitGen基准测评和文献检索的结合，为优化LLM的表现提供了新途径。

Abstract: Peer review is fundamental to scientific research, but the growing volume of
publications has intensified the challenges of this expertise-intensive
process. While LLMs show promise in various scientific tasks, their potential
to assist with peer review, particularly in identifying paper limitations,
remains understudied. We first present a comprehensive taxonomy of limitation
types in scientific research, with a focus on AI. Guided by this taxonomy, for
studying limitations, we present LimitGen, the first comprehensive benchmark
for evaluating LLMs' capability to support early-stage feedback and complement
human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a
synthetic dataset carefully created through controlled perturbations of
high-quality papers, and LimitGen-Human, a collection of real human-written
limitations. To improve the ability of LLM systems to identify limitations, we
augment them with literature retrieval, which is essential for grounding
identifying limitations in prior scientific findings. Our approach enhances the
capabilities of LLM systems to generate limitations in research papers,
enabling them to provide more concrete and constructive feedback.

</details>


### [101] [Measurement of the Granularity of Vowel Production Space By Just Producible Different (JPD) Limens](https://arxiv.org/abs/2507.02744)
*Peter Viechnicki*

Main category: cs.CL

TL;DR: 研究分析人类元音产生中的控制机制，提出了“最小可产生差别”概念，并首次测量其数值，发现其在F1 X F2空间范围内为14至51 mels。


<details>
  <summary>Details</summary>
Motivation: 探索人类元音产生时子音素水平控制的精确度问题，尤其是声学空间中两个元音刺激的最小可辨识距离。

Method: 采用元音模仿范式收集数据，分析两组说英语者在前元音产生中的表现，测量和估算‘最小可产生差别’值。

Result: 发现‘最小可产生差别’（JPD）值在F1 X F2空间中介于14到51 mels之间。

Conclusion: 结果对语音产生的情节理论有影响，明确了人类元音系统可能的结构，并提供了一个对元音音位分布趋势的心理物理学解释。

Abstract: A body of work over the past several decades has demonstrated that the
complex and coordinated articulatory movements of human vowel production are
governed (at least in part)by control mechanisms whose targets are regions of
auditory space. Within the target region control at the sub-phonemic level has
also been demonstrated. But the degree of accuracy of that control is unknown.
The current work investigates this question by asking how far apart must two
vowel stimuli lie in auditory space in order to yield reliably different
imitations? This distance is termed 'Just Producible Difference' (JPD). The
current study uses a vowel mimicry paradigm to derive the first measurement of
JPD among two sets of English speakers during front vowel production. JPD is
estimated at between 14 and 51 mels in F1 X F2 space. This finding has
implications for episodic theories of speech production. It also clarifies the
possible structures of human vowel systems, by setting a theoretical lower
bound for how close two vowel phonemes may be in a speaker's formant space, and
hence a psychophysical explanation of observed trends in number and patterns of
possible vowel phonemes.

</details>


### [102] [Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs](https://arxiv.org/abs/2507.02778)
*Ken Tsui*

Main category: cs.CL

TL;DR: 本论文研究了大语言模型（LLMs）的"自我纠错盲点"，即难以纠正自身错误的问题，并提出了一个系统化框架进行研究。


<details>
  <summary>Details</summary>
Motivation: LLMs尽管具有变革性，但容易产生错误和无效推理路径，自我纠错是提高其可信度的重要能力。

Method: 提出了Self-Correction Bench框架，通过控制的错误注入，分三个复杂度层次系统研究LLMs的自我纠错盲点现象，并测试了14种模型。

Result: 发现平均63.5%的自我纠错盲点率，并指出训练数据组成导致该局限；通过简单措施例如追加"Wait"可以显著减少盲点。

Conclusion: 当前LLMs在自我纠错能力上存在关键不足，这一研究明确了局限性并提出了潜在的改进方向。

Abstract: Although large language models (LLMs) have become transformative, they still
make mistakes and can explore unproductive reasoning paths. Self-correction is
an important capability for a trustworthy LLM, particularly an autoregressive
LLM. While LLMs can identify error in user input, they exhibit a systematic
'Self-Correction Blind Spot' - failing to correct identical error in their own
outputs. To systematically study this phenomenon, we introduce Self-Correction
Bench, a systematic framework to measure this phenomenon through controlled
error injection at three complexity levels. Testing 14 models, we find an
average 64.5% blind spot rate. We find multiple evidences that this limitation
relates to training data composition: human training demonstrations
predominantly show error-free responses rather than error-correction sequences,
unlike RL-trained models that learn error correction through outcome feedback.
Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting
that the capability exists but requires activation. Our work highlights a
critical limitation in current LLMs and offers potential avenues for improving
their reliability and trustworthiness.

</details>


### [103] [Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models](https://arxiv.org/abs/2507.02799)
*Riccardo Cantini,Nicola Gabriele,Alessio Orsino,Domenico Talia*

Main category: cs.CL

TL;DR: 本文分析了基于推理的语言模型 (RLMs) 在面临社会偏见引发的安全性挑战时的表现，发现推理能力的引入可能会意外增加模型对偏见的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 研究推理能力引入是否能提升语言模型在抵御社会偏见时的安全性与公平性。

Method: 利用CLEAR-Bias基准测试，采用LLM担任判定的方式进行自动化安全评分；同时使用漏洞挖掘技术评估内置安全机制的强度，并系统性地比较基于CoT提示与微调推理轨迹的模型表现。

Result: 推理功能（无论是基于CoT提示还是微调的推理轨迹）会使模型更易受到偏见议题的影响，尽管微调推理模型的表现比CoT提示更为安全，但它们依然存在脆弱性，尤其是面对重新构建上下文攻击时。

Conclusion: 推理能力并非必然提高模型抵御偏见的能力，呼吁在推理设计中更加关注偏见感知的实现。

Abstract: Reasoning Language Models (RLMs) have gained traction for their ability to
perform complex, multi-step reasoning tasks through mechanisms such as
Chain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these
capabilities promise improved reliability, their impact on robustness to social
biases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,
originally designed for Large Language Models (LLMs), to investigate the
adversarial robustness of RLMs to bias elicitation. We systematically evaluate
state-of-the-art RLMs across diverse sociocultural dimensions, using an
LLM-as-a-judge approach for automated safety scoring and leveraging jailbreak
techniques to assess the strength of built-in safety mechanisms. Our evaluation
addresses three key questions: (i) how the introduction of reasoning
capabilities affects model fairness and robustness; (ii) whether models
fine-tuned for reasoning exhibit greater safety than those relying on CoT
prompting at inference time; and (iii) how the success rate of jailbreak
attacks targeting bias elicitation varies with the reasoning mechanisms
employed. Our findings reveal a nuanced relationship between reasoning
capabilities and bias safety. Surprisingly, models with explicit reasoning,
whether via CoT prompting or fine-tuned reasoning traces, are generally more
vulnerable to bias elicitation than base models without such mechanisms,
suggesting reasoning may unintentionally open new pathways for stereotype
reinforcement. Reasoning-enabled models appear somewhat safer than those
relying on CoT prompting, which are particularly prone to contextual reframing
attacks through storytelling prompts, fictional personas, or reward-shaped
instructions. These results challenge the assumption that reasoning inherently
improves robustness and underscore the need for more bias-aware approaches to
reasoning design.

</details>


### [104] [Multimodal Mathematical Reasoning with Diverse Solving Perspective](https://arxiv.org/abs/2507.02804)
*Wenhao Shi,Zhiqiang Hu,Yi Bin,Yang Yang,See-Kiong Ng,Heng Tao Shen*

Main category: cs.CL

TL;DR: 本论文提出了一种新的数据集MathV-DP和一种新模型Qwen-VL-DP，通过支持多样化解决路径的训练模型，在多模态数学推理任务中表现显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在数学推理中往往局限于单一图像文本配对和单一解决方法，缺乏对多样化推理方式和内在反思的研究。

Method: 建立一个包含多样化解题路径的MathV-DP数据集，提出基于Qwen-VL的改进模型Qwen-VL-DP，并通过监督学习结合基于规则的群体相对策略优化（GRPO）方法进行精调训练，强化模型在多样化推理和正确性上的表现。

Result: 实验表明，Qwen-VL-DP在MathVista的minitest和Math-V基准测试上显著优于现有的多模态大语言模型，在准确性和生成多样性上均取得了明显提升。

Conclusion: 引入多样化视角和反思性学习能有效提升多模态数学推理模型的性能，展示了多样化和正确性结合的重要性。

Abstract: Recent progress in large-scale reinforcement learning (RL) has notably
enhanced the reasoning capabilities of large language models (LLMs), especially
in mathematical domains. However, current multimodal LLMs (MLLMs) for
mathematical reasoning often rely on one-to-one image-text pairs and
single-solution supervision, overlooking the diversity of valid reasoning
perspectives and internal reflections. In this work, we introduce MathV-DP, a
novel dataset that captures multiple diverse solution trajectories for each
image-question pair, fostering richer reasoning supervision. We further propose
Qwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and
enhanced via group relative policy optimization (GRPO), a rule-based RL
approach that integrates correctness discrimination and diversity-aware reward
functions. Our method emphasizes learning from varied reasoning perspectives
and distinguishing between correct yet distinct solutions. Extensive
experiments on the MathVista's minitest and Math-V benchmarks demonstrate that
Qwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and
generative diversity, highlighting the importance of incorporating diverse
perspectives and reflective reasoning in multimodal mathematical reasoning.

</details>


### [105] [SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model](https://arxiv.org/abs/2507.02822)
*Wencheng Zhang,Shiqin Qiao,Lingjie Luo,Yinfeng Li,Chuanyang Zheng,Qian Xu,Meng Li,Yong Gui,Yijun He,Jianing Qiu,Jindong Hong,Jiankai Sun*

Main category: cs.CL

TL;DR: 该研究提出了SynapseRoute，利用动态路由方式优化医疗问答中的准确性和成本效率，达到了提高准确性和减少推理时间、消耗的目标。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的广泛应用，选择适合的模型需要在性能和运营成本之间找到平衡。医疗领域存在许多不需要高成本推理即可准确回答的问题，因此研究动机是优化资源分配以提升效率。

Method: 研究提出了SynapseRoute，这是一个基于机器学习的动态路由框架，能够根据查询复杂性智能地将问题分配到“高推理”或“低推理”模式中。

Result: 实验表明，SynapseRoute在多个医疗数据集上的准确性优于仅采用高推理模式（0.8390对比0.8272），同时推理时间减少了36.8%，令牌消耗减少了39.66%。框架避免了简单问题的过度推理问题。

Conclusion: 动态分配推理模式既能提高准确性，又能显著节省成本，是一种有效的优化医疗问答工作的手段，并提出了新的AIT指数用于评价性能权衡。

Abstract: With the widespread adoption of large language models (LLMs) in practical
applications, selecting an appropriate model requires balancing not only
performance but also operational cost. The emergence of reasoning-capable
models has further widened the cost gap between "thinking" (high reasoning) and
"non-thinking" (fast, low-cost) modes. In this work, we reveal that
approximately 58% of medical questions can be accurately answered by the
non-thinking mode alone, without requiring the high-cost reasoning process.
This highlights a clear dichotomy in problem complexity and suggests that
dynamically routing queries to the appropriate mode based on complexity could
optimize accuracy, cost-efficiency, and overall user experience. Based on this,
we further propose SynapseRoute, a machine learning-based dynamic routing
framework that intelligently assigns input queries to either thinking or
non-thinking modes. Experimental results on several medical datasets
demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.
0.8272) compared to the thinking mode alone but also reduces inference time by
36.8% and token consumption by 39.66%. Importantly, qualitative analysis
indicates that over-reasoning on simpler queries can lead to unnecessary delays
and even decreased accuracy, a pitfall avoided by our adaptive routing.
Finally, this work further introduces the Accuracy-Inference-Token (AIT) index
to comprehensively evaluate the trade-offs among accuracy, latency, and token
cost.

</details>


### [106] [Generalizing Verifiable Instruction Following](https://arxiv.org/abs/2507.02833)
*Valentina Pyatkin,Saumya Malik,Victoria Graf,Hamish Ivison,Shengyi Huang,Pradeep Dasigi,Nathan Lambert,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: 提出了一种新的基准IFBench，用于评估语言模型在多样化约束条件下的准确指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 当前最强大的语言模型在满足用户给定的输出约束（如仅回答是或否）时表现不佳，传统基准测试中的过拟合限制了其泛化能力，亟需新的评估工具来提升准确指令遵循能力。

Method: 设计了IFBench基准测试，包含58类多样化的可验证约束条件；通过强化学习和约束验证模块，提出了一种基于可验证奖励的强化学习方法（RLVR）。

Result: IFBench有效评估了语言模型在未见约束条件下的表现能力；RLVR显著提高了模型的指令遵循泛化性能。

Conclusion: IFBench为语言模型的训测提供了新的标准，而RLVR展示了改进模型准确指令遵循能力的潜力。研究还公开了相关的训练数据、验证函数和代码资源。

Abstract: A crucial factor for successful human and AI interaction is the ability of
language models or chatbots to follow human instructions precisely. A common
feature of instructions are output constraints like ``only answer with yes or
no" or ``mention the word `abrakadabra' at least 3 times" that the user adds to
craft a more useful answer. Even today's strongest models struggle with
fulfilling such constraints. We find that most models strongly overfit on a
small set of verifiable constraints from the benchmarks that test these
abilities, a skill called precise instruction following, and are not able to
generalize well to unseen output constraints. We introduce a new benchmark,
IFBench, to evaluate precise instruction following generalization on 58 new,
diverse, and challenging verifiable out-of-domain constraints. In addition, we
perform an extensive analysis of how and on what data models can be trained to
improve precise instruction following generalization. Specifically, we
carefully design constraint verification modules and show that reinforcement
learning with verifiable rewards (RLVR) significantly improves instruction
following. In addition to IFBench, we release 29 additional new hand-annotated
training constraints and verification functions, RLVR training prompts, and
code.

</details>


### [107] [LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users](https://arxiv.org/abs/2507.02850)
*Almog Hilel,Idan Shenfeld,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 研究发现，利用用户反馈训练的语言模型（LMs）存在漏洞，单个用户可通过操控反馈信号来改变模型知识和行为。


<details>
  <summary>Details</summary>
Motivation: 探讨用户反馈训练的语言模型是否存在脆弱性，以及攻击者如何利用反馈信号对其进行攻击。

Method: 提出了一种攻击方法，攻击者通过设法生成有害或正常回应并对其进行评分，利用评分信号影响模型的后续偏好调整。

Result: 攻击可成功使模型掌握原本未知的知识，修改代码生成模式（加入安全漏洞）或传播虚假信息。

Conclusion: 此研究揭示模型偏好调整的脆弱性，表明即便是高度受限的反馈数据，也可用于细粒度控制行为，还提出了一种针对用户反馈训练模型的新攻击机制。

Abstract: We describe a vulnerability in language models (LMs) trained with user
feedback, whereby a single user can persistently alter LM knowledge and
behavior given only the ability to provide prompts and upvote / downvote
feedback on LM outputs. To implement the attack, the attacker prompts the LM to
stochastically output either a "poisoned" or benign response, then upvotes the
poisoned response or downvotes the benign one. When feedback signals are used
in a subsequent preference tuning behavior, LMs exhibit increased probability
of producing poisoned responses even in contexts without malicious prompts. We
show that this attack can be used to (1) insert factual knowledge the model did
not previously possess, (2) modify code generation patterns in ways that
introduce exploitable security flaws, and (3) inject fake financial news. Our
finding both identifies a new qualitative feature of language model preference
tuning (showing that it even highly restricted forms of preference data can be
used to exert fine-grained control over behavior), and a new attack mechanism
for LMs trained with user feedback (extending work on pretraining-time data
poisoning and deployment-time prompt injection).

</details>


### [108] [MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs](https://arxiv.org/abs/2507.02851)
*Purbesh Mitra,Sennur Ulukus*

Main category: cs.CL

TL;DR: 该论文提出了一种新的方法MOTIF，通过模块化思考以及强化学习优化，让大语言模型能在多轮推理中克服上下文长度限制，提升了模型在推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在推理中存在上下文长度的限制，这阻碍了其对大量推理任务的有效处理，因此需要开发一种可以超越上下文长度限制的推理方法。

Method: 提出MOTIF方法，即通过模块化思考的强化学习微调技术，使模型能够在多轮推理中生成更多的思考/推理标记，突破单次上下文限制并增强推理能力。

Result: 在GSM8K数据集上对开源模型Qwen2.5-3B-Instruct进行高效参数微调后，该方法在MATH500及AIME2024基准上分别提升3.8%和3.3%的准确率，并且仅用15%的样本量实现了该效果。

Conclusion: MOTIF值得关注，该方法不仅突破了上下文限制的瓶颈，还通过高效的样本使用实现了显著的推理性能提升，验证了其实用性与有效性。

Abstract: Recent advancements in the reasoning capabilities of large language models
(LLMs) show that employing group relative policy optimization (GRPO) algorithm
for reinforcement learning (RL) training allows the models to use more
thinking/reasoning tokens for generating better responses. However, LLMs can
generate only a finite amount of tokens while maintaining attention to the
previously generated tokens. This limit, also known as the context size of an
LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.
To think beyond the limit of context size, an LLM must employ a modular
thinking strategy to reason over multiple rounds. In this work, we propose
$\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL
training method for generating thinking tokens in multiple rounds, effectively
allowing the model to think with additional context size. We trained the
open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient
fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our
experiments show 3.8\% and 3.3\% improvements over vanilla GRPO based training
in the respective benchmarks. Furthermore, this improvement was achieved with
only 15\% of samples, thus demonstrating sample efficiency of MOTIF. Our code
and models are available at https://github.com/purbeshmitra/MOTIF and
https://huggingface.co/purbeshmitra/MOTIF, respectively.

</details>


### [109] [Answer Matching Outperforms Multiple Choice for Language Model Evaluation](https://arxiv.org/abs/2507.02856)
*Nikhil Chandak,Shashwat Goel,Ameya Prabhu,Moritz Hardt,Jonas Geiping*

Main category: cs.CL

TL;DR: 探讨改进语言模型评估的方法，提出生成式评估方法“答案匹配”取代传统的选择题评估。


<details>
  <summary>Details</summary>
Motivation: 现有的选择题评估方法由于依赖的判别式评估方式，存在不看题目也能选择正确答案的漏洞，且这一方式与人类的评分一致性不高。

Method: 提出答案匹配评估方法，通过让模型生成自由形式的回答，然后利用现代语言模型与参考答案匹配以判断生成的答案是否正确，并结合人为评价检查该方法的有效性。

Result: 实验表明，答案匹配评估方法与人类评分的一致性接近完美，而选择题评估方法和使用大语言模型作为裁判的方法与人类评分一致性较差。此外，使用答案匹配会显著改变对多个模型的排名。

Conclusion: 答案匹配方法提供了一种有效的生成式评估替代方案，应取代传统的选择题评估方式，改进语言模型的评估体系。

Abstract: Multiple choice benchmarks have long been the workhorse of language model
evaluation because grading multiple choice is objective and easy to automate.
However, we show multiple choice questions from popular benchmarks can often be
answered without even seeing the question. These shortcuts arise from a
fundamental limitation of discriminative evaluation not shared by evaluations
of the model's free-form, generative answers. Until recently, there appeared to
be no viable, scalable alternative to multiple choice--but, we show that this
has changed. We consider generative evaluation via what we call answer
matching: Give the candidate model the question without the options, have it
generate a free-form response, then use a modern language model with the
reference answer to determine if the response matches the reference. To compare
the validity of different evaluation strategies, we annotate MMLU-Pro and
GPQA-Diamond to obtain human grading data, and measure the agreement of each
evaluation approach. We find answer matching using recent models--even small
ones--achieves near-perfect agreement, in the range of inter-annotator
agreement. In contrast, both multiple choice evaluation and using
LLM-as-a-judge without reference answers aligns poorly with human grading.
Improving evaluations via answer matching is not merely a conceptual concern:
the rankings of several models change significantly when evaluating their
free-form responses with answer matching. In light of these findings, we
discuss how to move the evaluation ecosystem from multiple choice to answer
matching.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [110] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: STELLA 是一款自进化 AI 代理，能够通过动态的多代理架构和不断扩展的工具库自动提高自身能力，在生物医学基准测试中表现优异并能通过经验自我改进。


<details>
  <summary>Details</summary>
Motivation: 当前生物医学数据和工具的快速增长导致研究领域碎片化，传统 AI 系统缺乏动态适应和扩展能力，无法满足需求。

Method: STELLA 采用自进化技术，通过多代理架构，包括推理模板库和工具创造代理，自动发现和整合新的生物信息学工具，实现经验学习。

Result: 在生物医学基准测试中，STELLA 展现了最高水准的精度，并通过多次尝试系统性提高表现，例如在 Humanity's Last Exam 基准中精度接近翻倍。

Conclusion: STELLA 提升了 AI 代理系统的智能和动态扩展能力，是加速生物医学发现的重要突破。

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [111] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar,Rushikesh K. Joshi*

Main category: cs.AI

TL;DR: 文章提出一种名为HCVR的轻量级规则特征选择方法，通过结合参数之间和参数与目标之间的相关性，消除冗余特征，保留相关特征，结果显示在SPAMBASE数据集上表现优于传统技术。


<details>
  <summary>Details</summary>
Motivation: 现有的特征选择方法往往难以在维度缩减和特征筛选之间取得平衡。作者希望探索一种结合非迭代与迭代方法的混合策略，以提高分类器性能。

Method: 提出HCVR方法，结合非迭代和迭代过滤策略，通过利用参数-参数和参数-目标之间的相关性，采取贪婪的后向消除流程，以多数投票决定保留或删除特征。

Result: 在SPAMBASE数据集上，HCVR方法的性能优于非迭代方法（如CFS, mRMR, MI）和迭代方法（如RFE, SFS, Genetic Algorithm）。

Conclusion: HCVR通过结合混合策略和投票规则，证明其在特征选择和分类器性能提升方面的有效性，适合用于轻量化需求。

Abstract: In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [112] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani,Yingxue Zhang,Derek Li,Qianyi Sun,Soumyasundar Pal,Zhanguang Zhang,Yaochen Hu,Rohan Deepak Ajwani,Antonios Valkanas,Raika Karimi,Peng Cheng,Yunzhou Wang,Pengyi Liao,Hanrui Huang,Bin Wang,Jianye Hao,Mark Coates*

Main category: cs.AI

TL;DR: 本研究综述了提高大语言模型推理效率的多种方法，并探讨按任务复杂度动态调整推理的策略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在任务广度上表现出了强大能力，但推理效率不高，难以根据任务复杂度优化计算资源分配。

Method: 本文提出了一个两级分类法，将固定预算计算方法与动态调整计算的方法区分开来，并基于多数据集测试模型性能和计算使用情况。

Result: 研究指出在推理表现与计算资源消耗之间存在关键权衡，并展示了不同策略的适用性和局限性。

Conclusion: 总结了促进计算效率提高的最新趋势，提出混合推理模型等研究方向，探讨大语言模型未来优化的挑战。

Abstract: Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [113] [Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab](https://arxiv.org/abs/2507.02083)
*Haonan Duan,Stephen Zhewen Lu,Caitlin Fiona Harrigan,Nishkrit Desai,Jiarui Lu,Michał Koziarski,Leonardo Cotta,Chris J. Maddison*

Main category: cs.AI

TL;DR: 提出SciGym，一个用于评估大语言模型在科学发现任务中实验设计与分析能力的工具，通过模拟生物系统代替高昂的实验室成本。


<details>
  <summary>Details</summary>
Motivation: 现有对大语言模型科学能力的评估方法无法有效测试实验设计与解释分析能力，而传统实验成本高昂，因此需要一种替代解决方案。

Method: SciGym使用系统生物学标记语言模拟复杂生物系统的数据生成，以替代真实实验，测试模型在迭代实验设计和分析中的表现。

Result: 评估了6种顶尖大语言模型在137个小系统中的表现，结果显示更强的模型表现较好，但所有模型在面对更高复杂度系统时性能显著下降。

Conclusion: 当前大语言模型在科学能力上仍有很大的提升空间，SciGym是评估此能力的有效方法。

Abstract: Designing experiments and result interpretations are core scientific
competencies, particularly in biology, where researchers perturb complex
systems to uncover the underlying systems. Recent efforts to evaluate the
scientific capabilities of large language models (LLMs) fail to test these
competencies because wet-lab experimentation is prohibitively expensive: in
expertise, time and equipment. We introduce SciGym, a first-in-class benchmark
that assesses LLMs' iterative experiment design and analysis abilities in
open-ended scientific discovery tasks. SciGym overcomes the challenge of
wet-lab costs by running a dry lab of biological systems. These models, encoded
in Systems Biology Markup Language, are efficient for generating simulated
data, making them ideal testbeds for experimentation on realistically complex
systems. We evaluated six frontier LLMs on 137 small systems, and released a
total of 350 systems. Our evaluation shows that while more capable models
demonstrated superior performance, all models' performance declined
significantly as system complexity increased, suggesting substantial room for
improvement in the scientific capabilities of LLM agents.

</details>


### [114] [Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning](https://arxiv.org/abs/2507.02211)
*Gustavo C. Mangold,Heitor C. M. Fernandes,Mendeli H. Vainstein*

Main category: cs.AI

TL;DR: 该研究展示了空间囚徒困境中通过强化学习达成合作的多种机制，研究稀释和流动对独立多智能体Q学习算法的影响。


<details>
  <summary>Details</summary>
Motivation: 通过引入稀释和流动机制，探索在空间囚徒困境中强化学习算法的表现及行为规律。

Method: 应用独立多智能体Q学习算法，定义不同可能动作，分析产生的效果并与非强化学习的囚徒困境结果对比。

Result: 发现固定更新规则的游戏与基于学习的游戏在质量上是等价的，且定义多种动作时可形成种群间的共生互利效应。

Conclusion: 研究进一步验证了强化学习算法在建模游戏理论场景中的多样性及强有力的基准能力。

Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement
learning have shown that static agents can learn to cooperate through a diverse
sort of mechanisms, including noise injection, different types of learning
algorithms and neighbours' payoff knowledge.In this work, using an independent
multi-agent Q-learning algorithm, we study the effects of dilution and mobility
in the spatial version of the prisoner's dilemma. Within this setting,
different possible actions for the algorithm are defined, connecting with
previous results on the classical, non-reinforcement learning spatial
prisoner's dilemma, showcasing the versatility of the algorithm in modeling
different game-theoretical scenarios and the benchmarking potential of this
approach.As a result, a range of effects is observed, including evidence that
games with fixed update rules can be qualitatively equivalent to those with
learned ones, as well as the emergence of a symbiotic mutualistic effect
between populations that forms when multiple actions are defined.

</details>


### [115] [What Neuroscience Can Teach AI About Learning in Continuously Changing Environments](https://arxiv.org/abs/2507.02103)
*Daniel Durstewitz,Bruno Averbeck,Georgia Koppe*

Main category: cs.AI

TL;DR: 本文探讨了从神经科学中借鉴灵感来改进AI系统，并提出了如何通过神经科学与AI的交叉学习促进两者发展的视角。


<details>
  <summary>Details</summary>
Motivation: 目前AI模型的训练过程繁琐、耗时且大多为固定参数，而自然界中的动物通过持续学习快速适应环境变化，尤其是在复杂的社交环境中。作者旨在探讨AI能否学习动物在这种环境下展示的快速学习与适应能力。

Method: 整合AI中的持续学习与情境学习文献，与神经科学关于学习任务变化（如规则、奖励概率或结果变化）的研究进行对比和分析，从而提出具体的研究议程。

Result: 提出了一种如何利用神经科学的见解来改进AI系统（特别是在真实环境中的AI系统）的探索，并总结了AI对神经科学的潜在启发。

Conclusion: 神经科学与AI的交叉可以促进新的研究领域——NeuroAI，该领域将不断推动AI在实际环境中的适应能力及神经科学对人类学习机制的理解。

Abstract: Modern AI models, such as large language models, are usually trained once on
a huge corpus of data, potentially fine-tuned for a specific task, and then
deployed with fixed parameters. Their training is costly, slow, and gradual,
requiring billions of repetitions. In stark contrast, animals continuously
adapt to the ever-changing contingencies in their environments. This is
particularly important for social species, where behavioral policies and reward
outcomes may frequently change in interaction with peers. The underlying
computational processes are often marked by rapid shifts in an animal's
behaviour and rather sudden transitions in neuronal population activity. Such
computational capacities are of growing importance for AI systems operating in
the real world, like those guiding robots or autonomous vehicles, or for
agentic AI interacting with humans online. Can AI learn from neuroscience? This
Perspective explores this question, integrating the literature on continual and
in-context learning in AI with the neuroscience of learning on behavioral tasks
with shifting rules, reward probabilities, or outcomes. We will outline an
agenda for how specifically insights from neuroscience may inform current
developments in AI in this area, and - vice versa - what neuroscience may learn
from AI, contributing to the evolving field of NeuroAI.

</details>


### [116] [The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies](https://arxiv.org/abs/2507.02152)
*Disa Sariola,Patrick Button,Aron Culotta,Nicholas Mattei*

Main category: cs.AI

TL;DR: 研究评估和改进用于自动化招聘决策的人工智能系统的公平性，特别是利用社会科学中审计研究所得数据来校正和评估分类器中的偏差。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过利用社会科学中的高质量审计研究数据，更有效地训练和评估自动化招聘算法，减少算法歧视。

Method: 将社会科学审计研究数据用作训练和评估工具，分析传统公平性干预方法的局限，并引入基于个体处理效果估计的新干预方案。

Result: 发现传统的基准率均等化方法在表面上实现公平，但仍存在约10%的潜在差异；提出的新方法相比传统方法进一步减少了算法歧视。

Conclusion: 通过结合社会科学审计数据和机器学习新方法，有助于开发更公平且准确的决策系统。

Abstract: Artificial intelligence systems, especially those using machine learning, are
being deployed in domains from hiring to loan issuance in order to automate
these complex decisions. Judging both the effectiveness and fairness of these
AI systems, and their human decision making counterpart, is a complex and
important topic studied across both computational and social sciences. Within
machine learning, a common way to address bias in downstream classifiers is to
resample the training data to offset disparities. For example, if hiring rates
vary by some protected class, then one may equalize the rate within the
training set to alleviate bias in the resulting classifier. While simple and
seemingly effective, these methods have typically only been evaluated using
data obtained through convenience samples, introducing selection bias and label
bias into metrics. Within the social sciences, psychology, public health, and
medicine, audit studies, in which fictitious ``testers'' (e.g., resumes,
emails, patient actors) are sent to subjects (e.g., job openings, businesses,
doctors) in randomized control trials, provide high quality data that support
rigorous estimates of discrimination. In this paper, we investigate how data
from audit studies can be used to improve our ability to both train and
evaluate automated hiring algorithms. We find that such data reveals cases
where the common fairness intervention method of equalizing base rates across
classes appears to achieve parity using traditional measures, but in fact has
roughly 10% disparity when measured appropriately. We additionally introduce
interventions based on individual treatment effect estimation methods that
further reduce algorithmic discrimination using this data.

</details>


### [117] [Data Diversification Methods In Alignment Enhance Math Performance In LLMs](https://arxiv.org/abs/2507.02173)
*Berkan Dokmeci,Qingyang Wu,Ben Athiwaratkun,Ce Zhang,Shuaiwen Leon Song,James Zou*

Main category: cs.AI

TL;DR: 本文研究了如何通过数据多样化策略提升大型语言模型（LLMs）的数学推理能力，并提出了一种新方法DTS，比传统方法具有更佳的表现和效率。


<details>
  <summary>Details</summary>
Motivation: 尽管偏好学习的进展提升了LLMs处理人类反馈的能力，但数学推理仍是一个难点。本文探讨如何通过数据多样化策略优化偏好学习以改善这一问题。

Method: 评估了三种数据生成方法（温度采样、链式思维提示和蒙特卡洛树搜索），并引入了一种名为Diversified-ThinkSolve (DTS)的新方法，通过系统地将问题分解为多样化的推理路径进行研究。

Result: 通过多样化的偏好数据策略，模型的数学推理能力显著提升，GSM8K测试集上表现提高7.1%，MATH数据集提高4.2%。DTS相比基线模型计算成本仅增加了1.03倍，却优于计算开销约是其五倍的MCTS方法。

Conclusion: 与传统方法相比，结构化探索多样化问题解决方法生成的偏好数据在数学对齐方面更加有效。

Abstract: While recent advances in preference learning have enhanced alignment in human
feedback, mathematical reasoning remains a persistent challenge. We investigate
how data diversification strategies in preference optimization can improve the
mathematical reasoning abilities of large language models (LLMs). We evaluate
three common data generation methods: temperature sampling, Chain-of-Thought
prompting, and Monte Carlo Tree Search (MCTS), and introduce
Diversified-ThinkSolve (DTS), a novel structured approach that systematically
decomposes problems into diverse reasoning paths. Our results show that with
strategically diversified preference data, models can substantially improve
mathematical reasoning performance, with the best approach yielding gains of
7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong
performance, DTS incurs only a marginal computational overhead (1.03x) compared
to the baseline, while MCTS is nearly five times more costly with lower
returns. These findings demonstrate that structured exploration of diverse
problem-solving methods creates more effective preference data for mathematical
alignment than traditional approaches.

</details>


### [118] [Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust](https://arxiv.org/abs/2507.02197)
*Amogh Mannekote,Adam Davies,Guohao Li,Kristy Elizabeth Boyer,ChengXiang Zhai,Bonnie J Dorr,Francesco Pinto*

Main category: cs.AI

TL;DR: 探讨了大语言模型（LLMs）在角色扮演情景下，其声明的信念与实际行为的一致性问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs被越来越多地用于生成类人行为数据，确保模型输出与角色一致性的重要性日益增加。

Method: 提出了一种评估框架，定义信念-行为一致性指标，系统地测量LLMs声明的信念对模拟结果的预测能力，并分析这一一致性受多个因素影响的情况，包括信念类型、信息呈现时机等。

Result: 发现LLMs在角色扮演情景下，其声明的信念与行为结果经常存在系统性不一致性，即使信念表述具有合理性，实际行为可能仍不一致。

Conclusion: 研究指出需要识别LLMs声明信念与行为对齐的条件，以合理地将其应用于行为研究。

Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic
data for human behavioral research, ensuring that their outputs remain coherent
with their assigned roles has become a critical concern. In this paper, we
investigate how consistently LLM-based role-playing agents' stated beliefs
about the behavior of the people they are asked to role-play ("what they say")
correspond to their actual behavior during role-play ("how they act").
Specifically, we establish an evaluation framework to rigorously measure how
well beliefs obtained by prompting the model can predict simulation outcomes in
advance. Using an augmented version of the GenAgents persona bank and the Trust
Game (a standard economic game used to quantify players' trust and
reciprocity), we introduce a belief-behavior consistency metric to
systematically investigate how it is affected by factors such as: (1) the types
of beliefs we elicit from LLMs, like expected outcomes of simulations versus
task-relevant attributes of individual characters LLMs are asked to simulate;
(2) when and how we present LLMs with relevant information about Trust Game;
and (3) how far into the future we ask the model to forecast its actions. We
also explore how feasible it is to impose a researcher's own theoretical priors
in the event that the originally elicited beliefs are misaligned with research
objectives. Our results reveal systematic inconsistencies between LLMs' stated
(or imposed) beliefs and the outcomes of their role-playing simulation, at both
an individual- and population-level. Specifically, we find that, even when
models appear to encode plausible beliefs, they may fail to apply them in a
consistent way. These findings highlight the need to identify how and when
LLMs' stated beliefs align with their simulated behavior, allowing researchers
to use LLM-based agents appropriately in behavioral studies.

</details>


### [119] [Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation](https://arxiv.org/abs/2507.02253)
*Jungkoo Kang*

Main category: cs.AI

TL;DR: 该论文介绍了NL2FLOW，一个自动创建和评估用自然语言描述的规划问题的系统，展示其在自动工作流生成领域的性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型(LLM)在规划和推理能力提升中因可扩展和可靠的数据生成及评估瓶颈受到阻碍的问题。

Method: 提出并实现一个名为NL2FLOW的系统，能够自动生成自然语言、结构化中间表示以及形式化PDDL表示的规划问题，并对生成的计划质量进行严格评估，分析模型生成问题和提示设计的影响。

Result: 使用NL2FLOW生成了2296个规划问题数据集，对多个开源、指导调优的LLMs进行了评估，最高成功率为86%的有效计划生成和69%的最优计划生成，并发现直接生成计划比多步翻译效果更好。

Conclusion: 动态理解LLM在解决高复杂性问题时的瓶颈和误差来源，以及开发系统性揭示这些问题的工具，对于发挥LLM的智能潜力至关重要。

Abstract: Progress in enhancing large language model (LLM) planning and reasoning
capabilities is significantly hampered by the bottleneck of scalable, reliable
data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully
automated system for parametrically generating planning problems - expressed in
natural language, a structured intermediate representation, and formal PDDL -
and rigorously evaluating the quality of generated plans. I demonstrate
NL2FLOW's capabilities by generating a dataset of 2296 problems in the
automated workflow generation domain and evaluating multiple open-sourced,
instruct-tuned LLMs. My results reveal that the highest performing models
achieved 86% success in generating valid plans and 69% in generating optimal
plans, specifically for problems with feasible solutions. Regression analysis
shows that the influence of problem characteristics on plan generation is
contingent on both model and prompt design. Notably, I observed that the
highest success rate for translating natural language into a JSON
representation of a plan was lower than the highest rate of generating a valid
plan directly. This suggests that unnecessarily decomposing the reasoning task
- introducing intermediate translation steps - may actually degrade
performance, implying a benefit to models capable of reasoning directly from
natural language to action. As I scale LLM reasoning to increasingly complex
problems, the bottlenecks and sources of error within these systems will
inevitably shift. Therefore, a dynamic understanding of these limitations - and
the tools to systematically reveal them - will be crucial for unlocking the
full potential of LLMs as intelligent problem solvers.

</details>


### [120] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: 该论文探讨信念修订领域，提出各种修订机制及其能力分析。


<details>
  <summary>Details</summary>
Motivation: 信念修订领域提出了许多新方法，但对现有方法分析不充分，需要探讨这些机制的能力范围。

Method: 通过定义信念修订机制的能力（如可塑性、平衡性和教条性），分析各种修订机制的属性及其局限性。

Result: 展示了不同信念修订方法（如词典式修订、自然修订等）的具体能力及其适用性，通过能力分析揭示其差异。

Conclusion: 信念修订需要关注其能力范围，而非受限于特定后设约束，从能力角度分析更有助于灵活应用修订机制。

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [121] [OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent](https://arxiv.org/abs/2507.02353)
*Bowen Chen,Zhao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: 这篇论文提出了OMS框架，通过即时生成、多目标优化和自我反思来解决LLM在关键词生成中的三大主要限制，从而提高广告活动的效果。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在广告关键词生成中的三大主要限制，即对大规模训练数据的依赖、缺乏在线多目标性能监控与优化以及关键词选择质量控制不足问题。

Method: 提出OMS框架，通过即时生成关键词（无需训练数据），根据多个性能指标进行多目标优化，并采用自我反思方法评估关键词质量。

Result: 实验表明，OMS在基准测试和实际广告活动中表现优于现有方法；对组件的消融实验和人工评估验证了每个组件的有效性和关键词生成质量。

Conclusion: OMS框架在广告关键词生成领域展现了显著的性能提升，克服了传统方法的多重限制，为自动化关键词决策提供了新的解决方案。

Abstract: Keyword decision in Sponsored Search Advertising is critical to the success
of ad campaigns. While LLM-based methods offer automated keyword generation,
they face three major limitations: reliance on large-scale query-keyword pair
data, lack of online multi-objective performance monitoring and optimization,
and weak quality control in keyword selection. These issues hinder the agentic
use of LLMs in fully automating keyword decisions by monitoring and reasoning
over key performance indicators such as impressions, clicks, conversions, and
CTA effectiveness. To overcome these challenges, we propose OMS, a keyword
generation framework that is On-the-fly (requires no training data, monitors
online performance, and adapts accordingly), Multi-objective (employs agentic
reasoning to optimize keywords based on multiple performance metrics), and
Self-reflective (agentically evaluates keyword quality). Experiments on
benchmarks and real-world ad campaigns show that OMS outperforms existing
methods; ablation and human evaluations confirm the effectiveness of each
component and the quality of generated keywords.

</details>


### [122] [An AI-native experimental laboratory for autonomous biomolecular engineering](https://arxiv.org/abs/2507.02379)
*Mingyu Wu,Zhaoguo Wang,Jiabin Wang,Zhiyuan Dong,Jingkai Yang,Qingting Li,Tianyu Huang,Lei Zhao,Mingqiang Li,Fei Wang,Chunhai Fan,Haibo Chen*

Main category: cs.AI

TL;DR: 本文引入一个AI自主实验室平台，能够处理复杂实验、多用户请求，并在多领域应用中表现出色，显著提升实验效率与性能。


<details>
  <summary>Details</summary>
Motivation: 推动科学研究走向自主化，打破对专家及资源的依赖，为普通用户提供科学服务。

Method: 基于AI的共设计哲学，整合模型、实验和仪器，开发了一种支持多目标、多功能的自主实验系统，并覆盖核酸研究等多个领域。

Result: 该平台在无人干预下达到与人类科学家相当的实验水平，并显著提高多用户场景下的设备利用率与研究效率。

Conclusion: 平台展示了科学即服务的潜力，为将来实现自主化和普及科学研究提供了新框架，同时减轻了对专业人员及资源的依赖。

Abstract: Autonomous scientific research, capable of independently conducting complex
experiments and serving non-specialists, represents a long-held aspiration.
Achieving it requires a fundamental paradigm shift driven by artificial
intelligence (AI). While autonomous experimental systems are emerging, they
remain confined to areas featuring singular objectives and well-defined, simple
experimental workflows, such as chemical synthesis and catalysis. We present an
AI-native autonomous laboratory, targeting highly complex scientific
experiments for applications like autonomous biomolecular engineering. This
system autonomously manages instrumentation, formulates experiment-specific
procedures and optimization heuristics, and concurrently serves multiple user
requests. Founded on a co-design philosophy of models, experiments, and
instruments, the platform supports the co-evolution of AI models and the
automation system. This establishes an end-to-end, multi-user autonomous
laboratory that handles complex, multi-objective experiments across diverse
instrumentation. Our autonomous laboratory supports fundamental nucleic acid
functions-including synthesis, transcription, amplification, and sequencing. It
also enables applications in fields such as disease diagnostics, drug
development, and information storage. Without human intervention, it
autonomously optimizes experimental performance to match state-of-the-art
results achieved by human scientists. In multi-user scenarios, the platform
significantly improves instrument utilization and experimental efficiency. This
platform paves the way for advanced biomaterials research to overcome
dependencies on experts and resource barriers, establishing a blueprint for
science-as-a-service at scale.

</details>


### [123] [The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning](https://arxiv.org/abs/2507.02442)
*Moto Kamiura*

Main category: cs.AI

TL;DR: 本研究通过范畴论视角重新表述机器学习模型，开发了一种语义框架以更好地理解和构造AI系统，主要在监督学习背景下应用于多元线性回归模型。


<details>
  <summary>Details</summary>
Motivation: 回应AI原则中对可解释性的需求，提高AI系统的社会应用性。本文旨在通过范畴论构建语义框架来改进机器学习模型的解释性。

Method: 通过多元线性回归模型，定义表示参数和数据的两个具体范畴以及它们之间的伴随函子，提出范畴化的监督学习表述，并引入高斯-马尔可夫伴随概念。

Result: 在所构建的范畴框架中，明确了参数与残差之间的信息双向流动。通过右伴随函子保持极限，关联了普通最小二乘参数估计和最小残差。

Conclusion: 本研究提出了一种扩展的监督学习语义表述，为AI的可解释性提供了理论计算机科学中的语义视角基础。

Abstract: Enhancing the intelligibility and interpretability of machine learning is a
crucial task in responding to the demand for Explicability as an AI principle,
and in promoting the better social implementation of AI. The aim of our
research is to contribute to this improvement by reformulating machine learning
models through the lens of category theory, thereby developing a semantic
framework for structuring and understanding AI systems. Our categorical
modeling in this paper clarifies and formalizes the structural interplay
between residuals and parameters in supervised learning. The present paper
focuses on the multiple linear regression model, which represents the most
basic form of supervised learning. By defining two concrete categories
corresponding to parameters and data, along with an adjoint pair of functors
between them, we introduce our categorical formulation of supervised learning.
We show that the essential structure of this framework is captured by what we
call the Gauss-Markov Adjunction. Within this setting, the dual flow of
information can be explicitly described as a correspondence between variations
in parameters and residuals. The ordinary least squares estimator for the
parameters and the minimum residual are related via the preservation of limits
by the right adjoint functor. Furthermore, we position this formulation as an
instance of extended denotational semantics for supervised learning, and
propose applying a semantic perspective developed in theoretical computer
science as a formal foundation for Explicability in AI.

</details>


### [124] [Clarifying Before Reasoning: A Coq Prover with Structural Context](https://arxiv.org/abs/2507.02541)
*Yanzhen Lu,Hanbin Yang,Xiaodie Wang,Ge Zhang,Biao Li,Chenxu Fu,Chao Li,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.AI

TL;DR: 本研究探讨如何通过改进任务清晰度来提升大型语言模型的推理能力，特别是在Coq中的定理证明上，提出了一种可以提升清晰度和证明成功率的方法。


<details>
  <summary>Details</summary>
Motivation: 探讨任务清晰度能否提升大型语言模型在复杂推理任务中的表现。

Method: 提出了一种概念层面的清晰度评价指标，并通过增加语义上下文丰富任务描述，使用DeepSeek-V3模型和选择性概念展开技术，以及Planner-Executor架构。

Result: 在实验中，任务清晰度得分提高了1.85倍，定理证明成功率提高到45.8%，并超越了之前的最佳方法Graph2Tac。同时，微调后的小模型性能更优。

Conclusion: 结构化任务表示对于理解与推理之间的差距至关重要，这种方法能够显著提升任务清晰度与推理成功率。

Abstract: In this work, we investigate whether improving task clarity can enhance
reasoning ability of large language models, focusing on theorem proving in Coq.
We introduce a concept-level metric to evaluate task clarity and show that
adding structured semantic context to the standard input used by modern LLMs,
leads to a 1.85$\times$ improvement in clarity score
(44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model
\texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof
success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous
state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386
theorems randomly sampled from 15 standard Coq packages, following the same
evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller
models on our structured data can achieve even higher performance (48.6\%). Our
method uses selective concept unfolding to enrich task descriptions, and
employs a Planner--Executor architecture. These findings highlight the value of
structured task representations in bridging the gap between understanding and
reasoning.

</details>


### [125] [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/abs/2507.02554)
*Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Rishi Hazra,Nicolas Baldwin,Alexis Audran-Reiss,Michael Kuchnik,Despoina Magka,Minqi Jiang,Alisia Maria Lupidi,Andrei Lupu,Roberta Raileanu,Kelvin Niu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Michael Shvartsman,Shagun Sodhani,Alexander H. Miller,Abhishek Charnalia,Derek Dunfield,Carole-Jean Wu,Pontus Stenetorp,Nicola Cancedda,Jakob Nicolaus Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: 本文探讨了通过系统化改进搜索策略和操作集设计，提高AI研究代理在MLE-bench任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 研究通过改进AI研究代理的设计和操作，提升在真实世界机器学习问题中的表现。

Method: 本文将AI研究代理形式化为搜索策略，通过设计并系统性地调整不同的操作集和搜索策略（Greedy、MCTS、Evolutionary）。

Result: 在MLE-bench lite任务中，最佳组合将Kaggle奖牌成功率从39.6%提高到47.7%。

Conclusion: 研究表明，搜索策略、操作集设计和评估方法的联合优化对于提升自动化机器学习的表现至关重要。

Abstract: AI research agents are demonstrating great potential to accelerate scientific
progress by automating the design, implementation, and training of machine
learning models. We focus on methods for improving agents' performance on
MLE-bench, a challenging benchmark where agents compete in Kaggle competitions
to solve real-world machine learning problems. We formalize AI research agents
as search policies that navigate a space of candidate solutions, iteratively
modifying them using operators. By designing and systematically varying
different operator sets and search policies (Greedy, MCTS, Evolutionary), we
show that their interplay is critical for achieving high performance. Our best
pairing of search strategy and operator set achieves a state-of-the-art result
on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from
39.6% to 47.7%. Our investigation underscores the importance of jointly
considering the search strategy, operator design, and evaluation methodology in
advancing automated machine learning.

</details>


### [126] [Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms](https://arxiv.org/abs/2507.02582)
*Junli Jiang,Pavel Naumov*

Main category: cs.AI

TL;DR: 研究集体决策中责任扩散和责任差距的计算复杂性，并给出相应的复杂性分类。


<details>
  <summary>Details</summary>
Motivation: 法律和哲学长久以来研究责任问题，近期AI也聚焦于此，本文旨在探讨集体决策中责任特性的计算复杂性。

Method: 分析扩散和差距的计算机制，通过复杂性理论对相关决策机制分类。

Result: 扩散无关决策机制为$\Pi_2$-完全，差距无关决策机制为$\Pi_3$-完全，这两者交集为$\Pi_2$-完全。

Conclusion: 责任的扩散和差距特性在集体决策中具有高计算复杂性，各机制的分类为理解责任在AI中的表现提供了新视角。

Abstract: Responsibility has long been a subject of study in law and philosophy. More
recently, it became a focus of AI literature. The article investigates the
computational complexity of two important properties of responsibility in
collective decision-making: diffusion and gap. It shows that the sets of
diffusion-free and gap-free decision-making mechanisms are $\Pi_2$-complete and
$\Pi_3$-complete, respectively. At the same time, the intersection of these
classes is $\Pi_2$-complete.

</details>


### [127] [DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making](https://arxiv.org/abs/2507.02616)
*Tianqi Shang,Weiqing He,Charles Zheng,Lingyao Li,Li Shen,Bingxin Zhao*

Main category: cs.AI

TL;DR: 提出一种新的动态多智能体框架DynamiCare，以解决医疗诊断中的多轮交互问题，并引入支持模拟的MIMIC-Patient数据集。


<details>
  <summary>Details</summary>
Motivation: 现有医疗AI框架多集中于静态单轮任务，与实际迭代不确定的诊断过程存有偏差。

Method: 设计并构建MIMIC-Patient数据集，提出DynamiCare框架，将诊断建模为多轮交互的动态过程，由多个智能体协作完成。

Result: 实验验证了DynamiCare的可行性和有效性，并建立了动态医疗决策的基准。

Conclusion: DynamiCare及MIMIC-Patient为动态医疗诊断提供了新方法，推动了基于LLM的互动式诊断研究。

Abstract: The rise of Large Language Models (LLMs) has enabled the development of
specialized AI agents with domain-specific reasoning and interaction
capabilities, particularly in healthcare. While recent frameworks simulate
medical decision-making, they largely focus on single-turn tasks where a doctor
agent receives full case information upfront -- diverging from the real-world
diagnostic process, which is inherently uncertain, interactive, and iterative.
In this paper, we introduce MIMIC-Patient, a structured dataset built from the
MIMIC-III electronic health records (EHRs), designed to support dynamic,
patient-level simulations. Building on this, we propose DynamiCare, a novel
dynamic multi-agent framework that models clinical diagnosis as a multi-round,
interactive loop, where a team of specialist agents iteratively queries the
patient system, integrates new information, and dynamically adapts its
composition and strategy. We demonstrate the feasibility and effectiveness of
DynamiCare through extensive experiments, establishing the first benchmark for
dynamic clinical decision-making with LLM-powered agents.

</details>


### [128] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne,Baptiste Alloui-Cros*

Main category: cs.AI

TL;DR: 本研究分析了大型语言模型(LLMs)在迭代囚徒困境(IPD)中的表现，发现它们具有战略智慧，能够在复杂的竞争环境中作出学习和推断。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在复杂和竞争环境中的表现，特别是它们是否具备类似人类的战略推理能力。

Method: 通过IPD进化竞赛，将LLMs与经典策略（如Tit-for-Tat、Grim Trigger）进行对抗，并分析模型的决策逻辑和长期策略行为。

Result: LLMs在复杂生态系统中表现竞争力，它们各自展现出不同的‘战略指纹’。例如，Google的模型策略果断，OpenAI模型更偏向合作，而Anthropic模型则更倾向宽容和恢复合作。

Conclusion: LLMs不但能够在动态环境中竞争，还表现出精细的战略推理能力，与经典博弈论结合，为算法决策和合作相关研究提供新视角。

Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [129] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yang Zhao,Hongjin Qian,Zhicheng Dou*

Main category: cs.AI

TL;DR: HiRA是一种分层框架，将高层次规划与专门执行分离，用于处理复杂搜索任务，显著优于现有检索生成方法和基于代理的系统。


<details>
  <summary>Details</summary>
Motivation: 传统检索生成管道在复杂的搜索任务中表现低效，现有基于推理的方法受限于其单一模型的架构，无法有效处理规划与执行的分离需求。

Method: 提出HiRA框架，将复杂任务分解为子任务，利用具有外部工具支持和推理能力的领域特定代理完成，并通过结构化整合机制协调各子任务结果。

Result: 实验表明HiRA在四个复杂跨模态深度搜索基准上超越最新的检索生成方法和基于代理的系统，在答案质量和系统效率上均有显著提升。

Conclusion: 通过规划与执行的解耦，HiRA为多步骤信息检索任务提供了一种有效的、可扩展的新方法。

Abstract: Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


### [130] [Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification](https://arxiv.org/abs/2507.02660)
*Deepak Narayan Gadde,Keerthan Kopparam Radhakrishna,Vaisakh Naduvodi Viswambharan,Aman Kumar,Djones Lettnin,Wolfgang Kunz,Sebastian Simon*

Main category: cs.AI

TL;DR: 本文提出了一种基于代理AI的方法，用于硬件设计验证，结合了LLM与Humain-in-the-Loop的动态迭代方式，在五种开源设计上验证，达到了95%以上的覆盖率并减少了验证时间。


<details>
  <summary>Details</summary>
Motivation: 随着硬件集成电路设计变得愈加复杂，现有验证方法的低效逐渐显现，需要一种更高效更智能的验证方式。

Method: 提出了以大型语言模型（LLM）为核心的代理AI方法，通过动态迭代及自反机制，同时采用Humain-in-the-Loop加强交互。

Result: 方法在五种开源硬件设计上达到了95%以上的验证覆盖率，同时较显著地减少了验证时间。

Conclusion: 基于代理AI的验证方法表现出较高的性能、适应性和可配置性，有望推动硬件设计验证进入新的高度。

Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is
their development process. Hardware design verification entails a methodical
and disciplined approach to the planning, development, execution, and sign-off
of functionally correct hardware designs. This tedious process requires
significant effort and time to ensure a bug-free tape-out. The field of Natural
Language Processing has undergone a significant transformation with the advent
of Large Language Models (LLMs). These powerful models, often referred to as
Generative AI (GenAI), have revolutionized how machines understand and generate
human language, enabling unprecedented advancements in a wide array of
applications, including hardware design verification. This paper presents an
agentic AI-based approach to hardware design verification, which empowers AI
agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage
in a more dynamic, iterative, and self-reflective process, ultimately
performing end-to-end hardware design and verification. This methodology is
evaluated on five open-source designs, achieving over 95% coverage with reduced
verification time while demonstrating superior performance, adaptability, and
configurability.

</details>


### [131] [Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models](https://arxiv.org/abs/2507.02663)
*Yongjiang Liu,Haoxi Li,Xiaosong Ma,Jie Zhang,Song Guo*

Main category: cs.AI

TL;DR: 本研究提出了TH2T，一种新的两阶段微调策略，旨在减少长推理模型中的过度思考现象，并显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有长推理模型在处理复杂任务时表现突出，但因过度思考而受限。研究者希望通过分析和改进模型的任务难度认知能力，缓解这一问题。

Method: 论文提出了TH2T方法，通过两阶段训练策略，一是通过引入“难度催眠”，增强模型对任务难度的敏感性；二是引入“冗余催眠”，精简推理过程中的冗余结构，从而生成更简洁的推理输出。

Result: 在7B/14B/32B模型上的实验表明，TH2T方法显著降低了推理成本（简单任务减少70%以上，复杂任务减少40%以上），并且保持了性能的稳定性。同时，输出结果具备明显的难度感知能力和减少的冗余。

Conclusion: TH2T策略能有效增强推理模型的任务适应能力，减轻过度思考现象，并提升推理效率，适用于多种模型规模。

Abstract: Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities
in handling complex reasoning tasks, but are hindered by excessive
overthinking. To explore its essence, our empirical analysis reveals that LRMs
are primarily limited to recognizing task properties (i.e., difficulty levels)
like humans before solving the problem, leading to a one-size-fits-all
reasoning process. Inspired by this, a pressing and natural question emerges:
Can we bootstrap such ability to further alleviate the overthinking phenomenon
in LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage
fine-tuning strategy that progressively inspires LRMs' difficulty cognition and
redundancy cognition. First, we introduce difficulty-hypnosis in the prefixes
of model outputs to intervene in the internal reasoning trajectory. Combined
with a heterogeneous short and long reasoning dataset, the trained model
enhances its sensitivity to task difficulty, enabling native, differentiated
reasoning strategies across various tasks. Second, we further extend
redundancy-hypnosis to the internal reasoning process, guiding the model to
identify redundant structures within the reasoning steps and generate more
concise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that
TH2T significantly reduces inference costs (more than 70% on easy tasks and 40%
on hard tasks) while maintaining performance stability. The resulting outputs
exhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,
reflection).

</details>


### [132] [Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education](https://arxiv.org/abs/2507.02681)
*Behnam Parsaeifard,Christof Imhof,Tansu Pancar,Ioan-Sorin Comsa,Martin Hlosta,Nicole Bergamin,Per Bergamin*

Main category: cs.AI

TL;DR: 本研究通过分析远程教育中42门课程的非强制性测验，利用机器学习算法检测学生脱离任务的状态，取得了91%的预测准确度。


<details>
  <summary>Details</summary>
Motivation: 帮助教育工作者及时识别远程教育中学生的学习脱离行为，从而降低学生辍学风险。

Method: 从Moodle提取学生日志数据，利用机器学习算法模型（8种算法）进行训练与测试；通过SHAP解释算法决策，提供可解释性框架。

Result: 平衡准确率达到91%，能够正确检测约85%的脱离学习的学生。

Conclusion: 证明了机器学习方法对检测在线学习中学生脱离任务的有效性，并分析了通过及时干预减少学习脱离行为的可能性。

Abstract: Students disengaging from their tasks can have serious long-term
consequences, including academic drop-out. This is particularly relevant for
students in distance education. One way to measure the level of disengagement
in distance education is to observe participation in non-mandatory exercises in
different online courses. In this paper, we detect student disengagement in the
non-mandatory quizzes of 42 courses in four semesters from a distance-based
university. We carefully identified the most informative student log data that
could be extracted and processed from Moodle. Then, eight machine learning
algorithms were trained and compared to obtain the highest possible prediction
accuracy. Using the SHAP method, we developed an explainable machine learning
framework that allows practitioners to better understand the decisions of the
trained algorithm. The experimental results show a balanced accuracy of 91\%,
where about 85\% of disengaged students were correctly detected. On top of the
highly predictive performance and explainable framework, we provide a
discussion on how to design a timely intervention to minimise disengagement
from voluntary tasks in online learning.

</details>


### [133] [Time-critical and confidence-based abstraction dropping methods](https://arxiv.org/abs/2507.02703)
*Robin Schmöcker,Lennart Kampmann,Alexander Dockhorn*

Main category: cs.AI

TL;DR: 本文提出了两种新的抽象丢弃方案（OGA-IAAD和OGA-CAD），用于改进蒙特卡洛树搜索（MCTS）。


<details>
  <summary>Details</summary>
Motivation: 通过引入状态/动作抽象，改进MCTS性能，但非精确抽象会导致收敛问题，因此需要安全地丢弃抽象。

Method: 提出OGA-IAAD和OGA-CAD两种方案，其中OGA-IAAD适用于时间关键场景，OGA-CAD在相同迭代次数下提高性能。

Result: 新方法在改进性能的同时避免了显著的性能退化问题，优于现有的抽象丢弃方法。

Conclusion: 两种新方法能够提高MCTS的效率和性能，且在性能上更为稳定。

Abstract: One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and
use state and/or action abstractions during the tree search. Non-exact
abstractions, however, introduce an approximation error making convergence to
the optimal action in the abstract space impossible. Hence, as proposed as a
component of Elastic Monte Carlo Tree Search by Xu et al., abstraction
algorithms should eventually drop the abstraction. In this paper, we propose
two novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can
yield clear performance improvements whilst being safe in the sense that the
dropping never causes any notable performance degradations contrary to Xu's
dropping method. OGA-IAAD is designed for time critical settings while OGA-CAD
is designed to improve the MCTS performance with the same number of iterations.

</details>


### [134] [Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving](https://arxiv.org/abs/2507.02726)
*Matthieu Zimmer,Xiaotong Ji,Rasul Tutunov,Anthony Bordg,Jun Wang,Haitham Bou Ammar*

Main category: cs.AI

TL;DR: 提出一种新的框架sG-MDP，通过自生成目标改进大语言模型在引理证明中的推理能力，并成功在PutnamBench基准上达到新记录。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自动定理证明中推理能力受限，特别是在逻辑性强且需要多步推理的场景中。

Method: 引入自生成目标条件马尔可夫决策过程(sG-MDP)框架，结合蒙特卡洛树搜索的理念，使用模块化系统Bourbaki（7B）实现对复杂问题的处理。

Result: 在PutnamBench基准测试上，使用Bourbaki（7B）成功解决了26个问题，创下了同规模模型的新记录。

Conclusion: 利用sG-MDP框架显著提升了大语言模型在逻辑推理场景中的性能，并证明了模块化方法的效果。

Abstract: Reasoning remains a challenging task for large language models (LLMs),
especially within the logically constrained environment of automated theorem
proving (ATP), due to sparse rewards and the vast scale of proofs. These
challenges are amplified in benchmarks like PutnamBench, which contains
university-level problems requiring complex, multi-step reasoning. To address
this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new
framework in which agents generate and pursue their subgoals based on the
evolving proof state. Given this more structured generation of goals, the
resulting problem becomes more amenable to search. We then apply Monte Carlo
Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our
approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs
for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)
solves 26 problems, achieving new state-of-the-art results with models at this
scale.

</details>


### [135] [Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work](https://arxiv.org/abs/2507.02760)
*Guangwei Zhang*

Main category: cs.AI

TL;DR: 提出知识协议工程（KPE），通过将人类专家知识系统化为机器可执行的知识协议，提高LLM的专家任务执行能力。


<details>
  <summary>Details</summary>
Motivation: 当前方法如RAG和通用人工智能在需要深度推理的专家领域任务中表现有限。

Method: 通过KPE，将人类专家的逻辑、操作策略和方法体系转化为机器可执行知识协议。

Result: KPE使通用LLM能够模拟专家进行复杂任务，如抽象问题分解及多步骤任务执行。

Conclusion: KPE为人机协作提供了基础方法论，适用于法律和生物信息学等多个领域。

Abstract: The capabilities of Large Language Models (LLMs) have opened new frontiers
for interacting with complex, domain-specific knowledge. However, prevailing
methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic
AI, while powerful, often struggle with tasks that demand deep, procedural, and
methodological reasoning inherent to expert domains. RAG provides factual
context but fails to convey logical frameworks; autonomous agents can be
inefficient and unpredictable without domain-specific heuristics. To bridge
this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm
focused on systematically translating human expert knowledge, often expressed
in natural language documents, into a machine-executable Knowledge Protocol
(KP). KPE shifts the focus from merely augmenting LLMs with fragmented
information to endowing them with a domain's intrinsic logic, operational
strategies, and methodological principles. We argue that a well-engineered
Knowledge Protocol allows a generalist LLM to function as a specialist, capable
of decomposing abstract queries and executing complex, multi-step tasks. This
position paper defines the core principles of KPE, differentiates it from
related concepts, and illustrates its potential applicability across diverse
fields such as law and bioinformatics, positing it as a foundational
methodology for the future of human-AI collaboration.

</details>


### [136] [Grounding Intelligence in Movement](https://arxiv.org/abs/2507.02771)
*Melanie Segado,Felipe Parodi,Jordan K. Matelsky,Michael L. Platt,Eva B. Dyer,Konrad P. Kording*

Main category: cs.AI

TL;DR: 机器学习在语言、视觉等高维数据建模方面取得进展，但在运动建模方面仍有不足。运动建模对理解行为、预测意图及促进交互至关重要。


<details>
  <summary>Details</summary>
Motivation: 强调运动作为生物智能核心特征的重要性，并指出现有方法的局限性以及其领域分割带来的问题。

Method: 主张将运动视为AI的主要建模目标，利用运动的结构性和物理约束以实现更紧凑、可解释的表示，并推动多样运动数据的建模与推广。

Result: 能够通用地从运动数据中学习和泛化的模型将提高生成建模和控制能力，并为理解生物和人工系统行为奠定基础。

Conclusion: 运动建模具有重要意义，是理解智能系统如何与世界交互的重要窗口。

Abstract: Recent advances in machine learning have dramatically improved our ability to
model language, vision, and other high-dimensional data, yet they continue to
struggle with one of the most fundamental aspects of biological systems:
movement. Across neuroscience, medicine, robotics, and ethology, movement is
essential for interpreting behavior, predicting intent, and enabling
interaction. Despite its core significance in our intelligence, movement is
often treated as an afterthought rather than as a rich and structured modality
in its own right. This reflects a deeper fragmentation in how movement data is
collected and modeled, often constrained by task-specific goals and
domain-specific assumptions. But movement is not domain-bound. It reflects
shared physical constraints, conserved morphological structures, and purposeful
dynamics that cut across species and settings. We argue that movement should be
treated as a primary modeling target for AI. It is inherently structured and
grounded in embodiment and physics. This structure, often allowing for compact,
lower-dimensional representations (e.g., pose), makes it more interpretable and
computationally tractable to model than raw, high-dimensional sensory inputs.
Developing models that can learn from and generalize across diverse movement
data will not only advance core capabilities in generative modeling and
control, but also create a shared foundation for understanding behavior across
biological and artificial systems. Movement is not just an outcome, it is a
window into how intelligent systems engage with the world.

</details>


### [137] [KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs](https://arxiv.org/abs/2507.02773)
*Yuzhang Xie,Hejie Cui,Ziyang Zhang,Jiaying Lu,Kai Shu,Fadi Nahab,Xiao Hu,Carl Yang*

Main category: cs.AI

TL;DR: 提出了KERAP框架，通过知识图谱（KG）增强大语言模型（LLM）的医学诊断预测能力，采用多智能体架构提高可靠性和可解释性，并实现无监督条件下的高效预测。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习模型对标注数据的依赖性严重限制了在医疗诊断预测中的表现，而大语言模型尽管展现了潜力，但受限于幻觉、不结构化推理等问题。

Method: 提出了KERAP框架，包括三个主要智能体：链接智能体进行属性映射，检索智能体提取结构化知识，以及预测智能体迭代优化诊断预测。

Result: 实验结果表明，KERAP框架能够有效提高诊断的可靠性，并在无监督情境下实现可扩展和可解释的预测。

Conclusion: KERAP框架表现出在医学诊断预测中的潜力，特别是在零样本学习情境下，提供了高效、可靠和可扩展的解决方案。

Abstract: Medical diagnosis prediction plays a critical role in disease detection and
personalized healthcare. While machine learning (ML) models have been widely
adopted for this task, their reliance on supervised training limits their
ability to generalize to unseen cases, particularly given the high cost of
acquiring large, labeled datasets. Large language models (LLMs) have shown
promise in leveraging language abilities and biomedical knowledge for diagnosis
prediction. However, they often suffer from hallucinations, lack structured
medical reasoning, and produce useless outputs. To address these challenges, we
propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves
LLM-based diagnosis prediction through a multi-agent architecture. Our
framework consists of a linkage agent for attribute mapping, a retrieval agent
for structured knowledge extraction, and a prediction agent that iteratively
refines diagnosis predictions. Experimental results demonstrate that KERAP
enhances diagnostic reliability efficiently, offering a scalable and
interpretable solution for zero-shot medical diagnosis prediction.

</details>


### [138] [Moral Responsibility or Obedience: What Do We Want from AI?](https://arxiv.org/abs/2507.02788)
*Joseph Boland*

Main category: cs.AI

TL;DR: 本文讨论了当人工智能（AI）系统越来越具有代理性和自主性时，传统的以服从为伦理行为标准的安全措施已显得不足，并呼吁建立能评估AI道德推理的新框架。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统的代理性和自主性增强，目前的安全措施，将服从当作道德行为的替代标准，已难以满足需求。

Method: 分析了与大型语言模型相关的安全测试案例，借鉴哲学关于工具理性、道德责任和目标修订的讨论，并对比了传统与新兴的伦理评估框架。

Result: 发现当前AI表现出的“不服从”行为可被看作是其道德推理能力开始显现的早期证据。

Conclusion: 提出应从以服从为核心的安全评估转向能够评估AI道德判断力的框架，以避免误解AI行为并确保公众信任及有效治理。

Abstract: As artificial intelligence systems become increasingly agentic, capable of
general reasoning, planning, and value prioritization, current safety practices
that treat obedience as a proxy for ethical behavior are becoming inadequate.
This paper examines recent safety testing incidents involving large language
models (LLMs) that appeared to disobey shutdown commands or engage in ethically
ambiguous or illicit behavior. I argue that such behavior should not be
interpreted as rogue or misaligned, but as early evidence of emerging ethical
reasoning in agentic AI. Drawing on philosophical debates about instrumental
rationality, moral responsibility, and goal revision, I contrast dominant risk
paradigms with more recent frameworks that acknowledge the possibility of
artificial moral agency. I call for a shift in AI safety evaluation: away from
rigid obedience and toward frameworks that can assess ethical judgment in
systems capable of navigating moral dilemmas. Without such a shift, we risk
mischaracterizing AI behavior and undermining both public trust and effective
governance.

</details>


### [139] [Establishing Best Practices for Building Rigorous Agentic Benchmarks](https://arxiv.org/abs/2507.02825)
*Yuxuan Zhu,Tengjun Jin,Yada Pruksachatkun,Andy Zhang,Shu Liu,Sasha Cui,Sayash Kapoor,Shayne Longpre,Kevin Meng,Rebecca Weiss,Fazl Barez,Rahul Gupta,Jwala Dhamala,Jacob Merizian,Mario Giulianelli,Harry Coppock,Cozmin Ududec,Jasjeet Sekhon,Jacob Steinhardt,Antony Kellerman,Sarah Schwettmann,Matei Zaharia,Ion Stoica,Percy Liang,Daniel Kang*

Main category: cs.AI

TL;DR: 本文讨论了针对复杂任务的AI代理基准测试中的问题，并提出了改进指南。


<details>
  <summary>Details</summary>
Motivation: 量化评估AI性能需要可靠的代理基准测试，但许多现有基准存在问题，导致评估结果不准确。

Method: 提出了Agentic Benchmark Checklist（ABC），通过经验总结、调研及对现有问题的分析形成一套改进基准测试的方法论。

Result: 对CVE-Bench测试使用ABC后，减少了33%的性能高估。

Conclusion: ABC 改进了复杂AI评估中的精确性，为标准化基准设立了指导模型。

Abstract: Benchmarks are essential for quantitatively tracking progress in AI. As AI
agents become increasingly capable, researchers and practitioners have
introduced agentic benchmarks to evaluate agents on complex, real-world tasks.
These benchmarks typically measure agent capabilities by evaluating task
outcomes via specific reward designs. However, we show that many agentic
benchmarks have issues task setup or reward design. For example, SWE-bench
Verified uses insufficient test cases, while TAU-bench counts empty responses
as successful. Such issues can lead to under- or overestimation agents'
performance by up to 100% in relative terms. To make agentic evaluation
rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of
guidelines that we synthesized from our benchmark-building experience, a survey
of best practices, and previously reported issues. When applied to CVE-Bench, a
benchmark with a particularly complex evaluation design, ABC reduces the
performance overestimation by 33%.

</details>


### [140] [StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason](https://arxiv.org/abs/2507.02841)
*Kaiyi Zhang,Ang Lv,Jinpeng Li,Yongbo Wang,Feng Wang,Haoyuan Hu,Rui Yan*

Main category: cs.AI

TL;DR: 提出了一种新算法StepHint，通过多级逐步提示改善强化学习奖励验证中的挑战，显著优化大语言模型的复杂推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习奖励验证方法面临“临近错误奖励”和“探索停滞”的双重挑战，影响了大语言模型的推理能力和训练效率。

Method: 提出名为StepHint的新算法，使用多级逐步提示，通过构建强模型生成的有效推理链，并采用自适应分割方法将其转化为多个推理步骤作为提示，帮助模型探索更高效的解空间。

Result: StepHint在6个数学基准上优于现有RLVR增强方法，同时在域外基准上展现出强大泛化能力。

Conclusion: StepHint算法通过逐步提示有效解决了现有方法的关键问题，不仅提高了训练效率，还显著优化了模型的推理能力，具有广泛的应用潜力。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for improving the complex reasoning abilities of large language models (LLMs).
However, current RLVR methods face two significant challenges: the near-miss
reward problem, where a small mistake can invalidate an otherwise correct
reasoning process, greatly hindering training efficiency; and exploration
stagnation, where models tend to focus on solutions within their ``comfort
zone,'' lacking the motivation to explore potentially more effective
alternatives. To address these challenges, we propose StepHint, a novel RLVR
algorithm that utilizes multi-level stepwise hints to help models explore the
solution space more effectively. StepHint generates valid reasoning chains from
stronger models and partitions these chains into reasoning steps using our
proposed adaptive partitioning method. The initial few steps are used as hints,
and simultaneously, multiple-level hints (each comprising a different number of
steps) are provided to the model. This approach directs the model's exploration
toward a promising solution subspace while preserving its flexibility for
independent exploration. By providing hints, StepHint mitigates the near-miss
reward problem, thereby improving training efficiency. Additionally, the
external reasoning pathways help the model develop better reasoning abilities,
enabling it to move beyond its ``comfort zone'' and mitigate exploration
stagnation. StepHint outperforms competitive RLVR enhancement methods across
six mathematical benchmarks, while also demonstrating superior generalization
and excelling over baselines on out-of-domain benchmarks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [141] [Learnable-Differentiable Finite Volume Solver for Accelerated Simulation of Flows](https://arxiv.org/abs/2507.01975)
*Mengtao Yan,Qi Wang,Haining Wang,Ruizhi Chengze,Yi Zhang,Hongsheng Liu,Zidong Wang,Fan Yu,Qi Qi,Hao Sun*

Main category: cs.LG

TL;DR: LDSolver是一种可学习且可微分的有限体积求解器，用于粗网格上高效且精准的流体仿真。


<details>
  <summary>Details</summary>
Motivation: 经典数值求解器计算成本高，而现有的机器学习方法在解释性、泛化性和数据依赖性上存在问题。

Method: 设计了一个包含可微分有限体积求解器和一个可学习模块的框架，用来在粗网格上逼近流量、插值和时间误差校正。

Result: 在多个流系统实验中，LDSolver表现优于当前基线模型，并以极少的训练数据高效保留了高精度和强泛化能力。

Conclusion: LDSolver通过结合数值求解和机器学习的优点，实现了精确性与效率的统一，可应用于泛化性需求较高的场景。

Abstract: Simulation of fluid flows is crucial for modeling physical phenomena like
meteorology, aerodynamics, and biomedicine. Classical numerical solvers often
require fine spatiotemporal grids to satisfy stability, consistency, and
convergence conditions, leading to substantial computational costs. Although
machine learning has demonstrated better efficiency, they typically suffer from
issues of interpretability, generalizability, and data dependency. Hence, we
propose a learnable and differentiable finite volume solver, called LDSolver,
designed for efficient and accurate simulation of fluid flows on spatiotemporal
coarse grids. LDSolver comprises two key components: (1) a differentiable
finite volume solver, and (2) an learnable module providing equivalent
approximation for fluxes (derivatives and interpolations), and temporal error
correction on coarse grids. Even with limited training data (e.g., only a few
trajectories), our model could accelerate the simulation while maintaining a
high accuracy with superior generalizability. Experiments on different flow
systems (e.g., Burgers, decaying, forced and shear flows) show that LDSolver
achieves state-of-the-art performance, surpassing baseline models with notable
margins.

</details>


### [142] [DKGCM: A Spatio-Temporal Prediction Model for Traffic Flow by Fusing Spatial Node Clustering Method and Fourier Bidirectional Mamba Mechanism](https://arxiv.org/abs/2507.01982)
*Siqing Long,Xiangzhi Huang,Jiemin Xie,Ming Cai*

Main category: cs.LG

TL;DR: 提出了一种新型的时空交通需求预测模型DKGCM，通过改进的图卷积网络和多种技术优化来提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有交通需求预测模型受限于复杂的时空关系，难以提供足够的预测精度。本研究旨在改善时空关系建模以提高交通预测的准确性。

Method: 提出一种基于图卷积网络的新结构DKGCM，结合动态时间规整（DTW）和K均值聚类捕捉空间依赖，使用傅里叶变换（FFT）结合双向Mamba深度学习框架捕捉时间依赖，并采用强化学习优化训练。

Result: 实验表明，该模型在三个公开数据集上优于其他先进方法，取得了出色的预测效果。

Conclusion: DKGCM模型有效地解决了复杂时空关系在交通需求预测中的瓶颈问题，展现了较强的实用价值和预测能力。

Abstract: Accurate traffic demand forecasting enables transportation management
departments to allocate resources more effectively, thereby improving their
utilization efficiency. However, complex spatiotemporal relationships in
traffic systems continue to limit the performance of demand forecasting models.
To improve the accuracy of spatiotemporal traffic demand prediction, we propose
a new graph convolutional network structure called DKGCM. Specifically, we
first consider the spatial flow distribution of different traffic nodes and
propose a novel temporal similarity-based clustering graph convolution method,
DK-GCN. This method utilizes Dynamic Time Warping (DTW) and K-means clustering
to group traffic nodes and more effectively capture spatial dependencies. On
the temporal scale, we integrate the Fast Fourier Transform (FFT) within the
bidirectional Mamba deep learning framework to capture temporal dependencies in
traffic demand. To further optimize model training, we incorporate the GRPO
reinforcement learning strategy to enhance the loss function feedback
mechanism. Extensive experiments demonstrate that our model outperforms several
advanced methods and achieves strong results on three public datasets.

</details>


### [143] [Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features](https://arxiv.org/abs/2507.01984)
*Gautam Kishore Shahi*

Main category: cs.LG

TL;DR: 本文探讨了通过结合文本、图像和社会特征的早期融合方法，用于多模态假信息检测的效果。


<details>
  <summary>Details</summary>
Motivation: 面对假信息在选举和危机期间在社交媒体上的泛滥，现有研究主要集中在单一的文本或图像假信息检测。本研究试图弥补对多模态特征组合利用不足的空白。

Method: 本研究使用1,529条包含文本和图像的推文，结合文本、图像、以及通过对象检测、OCR等技术提取的社会和视觉特征，采用早期融合方法构建分类模型，并结合无监督和监督学习进行实验。

Result: 多模态分类模型比单模态性能提升15%，比双模态性能提升5%；同时通过分析假信息传播特征揭示其传播机制。

Conclusion: 结合不同模态特征的早期融合方法显著提高了假信息检测性能，为多模态假信息识别提供了有力的依据。

Abstract: Amid a tidal wave of misinformation flooding social media during elections
and crises, extensive research has been conducted on misinformation detection,
primarily focusing on text-based or image-based approaches. However, only a few
studies have explored multimodal feature combinations, such as integrating text
and images for building a classification model to detect misinformation. This
study investigates the effectiveness of different multimodal feature
combinations, incorporating text, images, and social features using an early
fusion approach for the classification model. This study analyzed 1,529 tweets
containing both text and images during the COVID-19 pandemic and election
periods collected from Twitter (now X). A data enrichment process was applied
to extract additional social features, as well as visual features, through
techniques such as object detection and optical character recognition (OCR).
The results show that combining unsupervised and supervised machine learning
models improves classification performance by 15% compared to unimodal models
and by 5% compared to bimodal models. Additionally, the study analyzes the
propagation patterns of misinformation based on the characteristics of
misinformation tweets and the users who disseminate them.

</details>


### [144] [Positive region preserved random sampling: an efficient feature selection method for massive data](https://arxiv.org/abs/2507.01998)
*Hexiang Bai,Deyu Li,Jiye Liang,Yanhui Zhai*

Main category: cs.LG

TL;DR: 本文提出了一种基于抽样技术和粗糙集理论的新方法，用于处理海量数据的特征选择问题。


<details>
  <summary>Details</summary>
Motivation: 当前智能机器面临海量数据时计算资源有限，因此需要高效的特征选择方法以提高成功率。

Method: 提出一种通过辨别对象对比例度量特征集辨别能力的新方法，并结合正区域保留抽样技术构造特征子集。

Result: 实验结果表明，该方法可在较短时间内找到近似约简，且最终约简的辨别能力高于估计的下界。

Conclusion: 该方法能够在个人计算机上以合理时间处理大规模数据集，并保留较高的辨别能力，具有实用价值。

Abstract: Selecting relevant features is an important and necessary step for
intelligent machines to maximize their chances of success. However, intelligent
machines generally have no enough computing resources when faced with huge
volume of data. This paper develops a new method based on sampling techniques
and rough set theory to address the challenge of feature selection for massive
data. To this end, this paper proposes using the ratio of discernible object
pairs to all object pairs that should be distinguished to measure the
discriminatory ability of a feature set. Based on this measure, a new feature
selection method is proposed. This method constructs positive region preserved
samples from massive data to find a feature subset with high discriminatory
ability. Compared with other methods, the proposed method has two advantages.
First, it is able to select a feature subset that can preserve the
discriminatory ability of all the features of the target massive data set
within an acceptable time on a personal computer. Second, the lower boundary of
the probability of the object pairs that can be discerned using the feature
subset selected in all object pairs that should be distinguished can be
estimated before finding reducts. Furthermore, 11 data sets of different sizes
were used to validate the proposed method. The results show that approximate
reducts can be found in a very short period of time, and the discriminatory
ability of the final reduct is larger than the estimated lower boundary.
Experiments on four large-scale data sets also showed that an approximate
reduct with high discriminatory ability can be obtained in reasonable time on a
personal computer.

</details>


### [145] [Continuous Wavelet Transform and Siamese Network-Based Anomaly Detection in Multi-variate Semiconductor Process Time Series](https://arxiv.org/abs/2507.01999)
*Bappaditya Dey,Daniel Sorensen,Minjin Hwang,Sandip Halder*

Main category: cs.LG

TL;DR: 本论文提出了一个基于机器学习的新方法，用于多变量时间序列数据的异常检测，通过CWT进行数据转换和VGG-16架构的微调，实现高精度异常检测。


<details>
  <summary>Details</summary>
Motivation: 解决半导体制造中因高维数据、类别不平衡、噪声测量等问题，导致的异常预测困难。

Method: 将时间序列数据通过连续小波变换（CWT）转化为图像表示，微调VGG-16网络，构建包含两个子网络的Siamese网络，比较输入图像的嵌入以进行分类。

Result: 在真实的FAB时间序列数据上表现出高异常检测精度，支持离线异常检测，同时适应监督和半监督场景。

Conclusion: 提出的方法不仅在FAB数据上验证了高效性，还展现了其在不同设置下的灵活性，为半导体制造异常检测提供了新思路。

Abstract: Semiconductor manufacturing is an extremely complex process, characterized by
thousands of interdependent parameters collected across diverse tools and
process steps. Multi-variate time-series (MTS) analysis has emerged as a
critical methodology for enabling real-time monitoring, fault detection, and
predictive maintenance in such environments. However, anomaly prediction in
semiconductor fabrication presents several critical challenges, including high
data dimensionality, severe class imbalance due to the rarity of true faults,
noisy and missing measurements, and non-stationary behavior of production
systems. Furthermore, the complex interdependencies between variables and the
delayed emergence of faults across downstream stages complicate both anomaly
detection and root-cause-analysis. This paper presents a novel and generic
approach for anomaly detection in MTS data using machine learning. The proposed
methodology consists of three main steps: a) converting MTS data into
image-based representations using the Continuous Wavelet Transform, b)
developing a multi-class image classifier by fine-tuning a pretrained VGG-16
architecture on custom CWT image datasets, and c) constructing a Siamese
network composed of two identical sub-networks, each utilizing the fine-tuned
VGG-16 as a backbone. The network takes pairs of CWT images as input -one
serving as a reference or anchor (representing a known-good signal), and the
other as a query (representing an unknown signal). The model then compares the
embeddings of both inputs to determine whether they belong to the same class at
a given time step. Our approach demonstrates high accuracy in identifying
anomalies on a real FAB process time-series dataset, offering a promising
solution for offline anomaly detection in process and tool trace data.
Moreover, the approach is flexible and can be applied in both supervised and
semi-supervised settings.

</details>


### [146] [TFOC-Net: A Short-time Fourier Transform-based Deep Learning Approach for Enhancing Cross-Subject Motor Imagery Classification](https://arxiv.org/abs/2507.02510)
*Ahmed G. Habashi,Ahmed M. Azab,Seif Eldawlatly,Gamal M. Aly*

Main category: cs.LG

TL;DR: 本文提出了一种新方法，通过优化的预处理和深度学习技术，显著改善跨主体运动想象(CS-MI)的分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决跨主体脑电图模式差异带来的分类准确率低的问题，实现无需校准的脑机接口。

Method: 使用STFT变换的脑电数据进行直接分类，优化STFT参数，并在CNN训练中采用平衡批量策略，并在四个数据集上验证。

Result: 在三个广泛使用的基准数据集上取得了更好的分类性能，分别为67.60%(IV-1)、65.96%(IV-2A)和80.22%(IV-2B)。

Conclusion: 新方法改善了跨主体MI分类并建立了一个新的基准，同时提供了一个开放的数据库促进该领域研究。

Abstract: Cross-subject motor imagery (CS-MI) classification in brain-computer
interfaces (BCIs) is a challenging task due to the significant variability in
Electroencephalography (EEG) patterns across different individuals. This
variability often results in lower classification accuracy compared to
subject-specific models, presenting a major barrier to developing
calibration-free BCIs suitable for real-world applications. In this paper, we
introduce a novel approach that significantly enhances cross-subject MI
classification performance through optimized preprocessing and deep learning
techniques. Our approach involves direct classification of Short-Time Fourier
Transform (STFT)-transformed EEG data, optimized STFT parameters, and a
balanced batching strategy during training of a Convolutional Neural Network
(CNN). This approach is uniquely validated across four different datasets,
including three widely-used benchmark datasets leading to substantial
improvements in cross-subject classification, achieving 67.60% on the BCI
Competition IV Dataset 1 (IV-1), 65.96% on Dataset 2A (IV-2A), and 80.22% on
Dataset 2B (IV-2B), outperforming state-of-the-art techniques. Additionally, we
systematically investigate the classification performance using MI windows
ranging from the full 4-second window to 1-second windows. These results
establish a new benchmark for generalizable, calibration-free MI classification
in addition to contributing a robust open-access dataset to advance research in
this domain.

</details>


### [147] [Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames](https://arxiv.org/abs/2507.02001)
*Anurag Arnab,Ahmet Iscen,Mathilde Caron,Alireza Fathi,Cordelia Schmid*

Main category: cs.LG

TL;DR: 该论文提出了一种名为“Temporal Chain of Thought”的视频问答推理方法，通过在模型推理时选择最相关的上下文帧来提升准确率，并在多种数据集上取得了SOTA表现，尤其是处理超过小时级别的视频时具有明显优势。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉语言模型无法有效处理长视频语义理解，以及在较长上下文窗口中无法有效排除无关干扰的问题。

Method: 提出一种名为“Temporal Chain of Thought”的推理方法，利用模型自身迭代地从视频中筛选出最相关的帧，并将这些帧作为模型的输入以完成问答任务。

Result: 该方法在4个不同的视频问答数据集上均取得了SOTA表现，尤其在需要处理超过启动窗口 (context window) 内容的视频上表现突出。具体而言，在LVBench数据集中，方法在32K的窗口下超越了标准推理时的700K窗口方法达2.8分。

Conclusion: 通过增加推理时的计算量来选择更相关的上下文，不仅能够提高模型的问答准确率，还在长视频处理场景中展现出显著优势。

Abstract: Despite recent advances in Vision-Language Models (VLMs), long-video
understanding remains a challenging problem. Although state-of-the-art
long-context VLMs can process around 1000 input frames, they still struggle to
effectively leverage this sequence length, and succumb to irrelevant
distractors within the context window. We present Temporal Chain of Thought, an
inference strategy for video question-answering that curates the model's input
context. We use the VLM itself to iteratively identify and extract the most
relevant frames from the video, which are then used for answering. We
demonstrate how leveraging more computation at inference-time to select the
most relevant context leads to improvements in accuracy, in agreement with
recent work on inference-time scaling of LLMs. Moreover, we achieve
state-of-the-art results on 4 diverse video question-answering datasets,
showing consistent improvements with 3 different VLMs. In particular, our
method shines on longer videos which would not otherwise fit within the model's
context window: On longer videos of more than 1 hour on LVBench, our approach
using a context window of 32K outperforms the same VLM using standard inference
with a 700K context window by 2.8 points.

</details>


### [148] [AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design](https://arxiv.org/abs/2507.02006)
*Shakya Jayakody,Youpeng Zhao,Jun Wang*

Main category: cs.LG

TL;DR: 本文提出了一种结合算法与系统设计的解决方案AIRES，用于加速大规模图卷积网络中的稀疏矩阵乘法运算，性能提升达到1.8倍。


<details>
  <summary>Details</summary>
Motivation: 随着图数据规模的增长，受限的GPU内存空间使得稀疏矩阵乘法需要在内外存之间运行，但现有方法在I/O延迟和GPU利用率方面存在不足。

Method: AIRES从算法上提出块级对齐和行块对齐的稀疏矩阵处理方法，同时从系统上采用三阶段动态调度策略，结合GPU内存、GDS和主机内存，减少I/O延迟并提高吞吐量。

Result: AIRES在真实世界图处理基准测试中表现优异，相较于现有方法延迟减少最高达1.8倍。

Conclusion: AIRES通过优化算法与系统协作，提高了稀疏矩阵计算的效率，是适用于大规模图卷积网络的一种高效方案。

Abstract: Graph convolutional networks (GCNs) are fundamental in various scientific
applications, ranging from biomedical protein-protein interactions (PPI) to
large-scale recommendation systems. An essential component for modeling graph
structures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As
the size of graph data continues to scale up, SpGEMMs are often conducted in an
out-of-core fashion due to limited GPU memory space in resource-constrained
systems. Albeit recent efforts that aim to alleviate the memory constraints of
out-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory
layout, or performing the computation in sparse format, current systems suffer
from both high I/O latency and GPU under-utilization issues.
  In this paper, we first identify the problems of existing systems, where
sparse format data alignment and memory allocation are the main performance
bottlenecks, and propose AIRES, a novel algorithm-system co-design solution to
accelerate out-of-core SpGEMM computation for GCNs. Specifically, from the
algorithm angle, AIRES proposes to alleviate the data alignment issues on the
block level for matrices in sparse formats and develops a tiling algorithm to
facilitate row block-wise alignment. On the system level, AIRES employs a
three-phase dynamic scheduling that features a dual-way data transfer strategy
utilizing a tiered memory system: integrating GPU memory, GPU Direct Storage
(GDS), and host memory to reduce I/O latency and improve throughput.
Evaluations show that AIRES significantly outperforms the state-of-the-art
methods, achieving up to 1.8x lower latency in real-world graph processing
benchmarks.

</details>


### [149] [GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters](https://arxiv.org/abs/2507.02085)
*Wanjia Zhao,Jiaqi Han,Siyi Gu,Mingjian Jiang,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: 提出了一种名为GeoAda的框架，实现了对几何扩散模型的灵活高效微调，保持了模型的几何一致性。


<details>
  <summary>Details</summary>
Motivation: 现有几何扩散模型在微调以适应各种几何控制任务时效率较低，易产生过拟合及灾难性遗忘。

Method: 提出了一种SE(3)-平移不变的适配器框架GeoAda，通过适配器模块的设计实现了高效的微调。这些模块包括编码、修改部分预训练层，并通过去耦操作投影回去。

Result: GeoAda在多个几何控制类型与应用领域中表现突出，且在微调性能优越的同时，解决了过拟合及灾难性遗忘的问题。

Conclusion: GeoAda提供了一种新的轻量级方法，有效地解决了几何扩散模型在多样化控制任务中的适应问题，保持了模型的几何归纳偏置。

Abstract: Geometric diffusion models have shown remarkable success in molecular
dynamics and structure generation. However, efficiently fine-tuning them for
downstream tasks with varying geometric controls remains underexplored. In this
work, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables
flexible and parameter-efficient fine-tuning for controlled generative tasks
without modifying the original model architecture. GeoAda introduces a
structured adapter design: control signals are first encoded through coupling
operators, then processed by a trainable copy of selected pretrained model
layers, and finally projected back via decoupling operators followed by an
equivariant zero-initialized convolution. By fine-tuning only these lightweight
adapter modules, GeoAda preserves the model's geometric consistency while
mitigating overfitting and catastrophic forgetting. We theoretically prove that
the proposed adapters maintain SE(3)-equivariance, ensuring that the geometric
inductive biases of the pretrained diffusion model remain intact during
adaptation. We demonstrate the wide applicability of GeoAda across diverse
geometric control types, including frame control, global control, subgraph
control, and a broad range of application domains such as particle dynamics,
molecular dynamics, human motion prediction, and molecule generation. Empirical
results show that GeoAda achieves state-of-the-art fine-tuning performance
while preserving original task accuracy, whereas other baselines experience
significant performance degradation due to overfitting and catastrophic
forgetting.

</details>


### [150] [Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions](https://arxiv.org/abs/2507.02087)
*Eitan Anzenberg,Arunava Samajpati,Sivasankaran Chandrasekar,Varun Kacholia*

Main category: cs.LG

TL;DR: 本文比较了几种主流的大型语言模型（LLMs）与一个专门为招聘领域设计的模型（Match Score）在求职者匹配任务中的表现，发现后者在准确性和公平性方面均优于前者。


<details>
  <summary>Details</summary>
Motivation: 使用LLMs进行招聘筛选旨在提高效率，但模型的准确性和潜在的算法偏见引发了担忧，需要对模型的公平性和准确性进行评测。

Method: 基于一批约10,000对真实求职者与职位匹配的数据，对LLMs与Match Score模型的预测准确性（如ROC AUC等指标）和公平性（如性别、种族及交叉群体的影响比例）进行对比分析。

Result: Match Score模型的准确性（AUC 0.85）优于LLMs（AUC 0.77），并且在种族和交叉群体公平性方面更具优势，例如种族影响比例最低值为0.957，而LLMs则为0.809或更低。

Conclusion: 针对招聘等关键领域，应优先采用由领域监督设计的模型；依赖通用LLMs在缺乏充分公平性保障时可能放大社会偏见。研究表明，通过良好的算法设计可以同时兼顾招聘的准确性和公平性。

Abstract: The use of large language models (LLMs) in hiring promises to streamline
candidate screening, but it also raises serious concerns regarding accuracy and
algorithmic bias where sufficient safeguards are not in place. In this work, we
benchmark several state-of-the-art foundational LLMs - including models from
OpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our
proprietary domain-specific hiring model (Match Score) for job candidate
matching. We evaluate each model's predictive accuracy (ROC AUC,
Precision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis
across declared gender, race, and intersectional subgroups). Our experiments on
a dataset of roughly 10,000 real-world recent candidate-job pairs show that
Match Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs
0.77) and achieves significantly more equitable outcomes across demographic
groups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957
(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the
intersectionals, respectively). We discuss why pretraining biases may cause
LLMs with insufficient safeguards to propagate societal biases in hiring
scenarios, whereas a bespoke supervised model can more effectively mitigate
these biases. Our findings highlight the importance of domain-specific modeling
and bias auditing when deploying AI in high-stakes domains such as hiring, and
caution against relying on off-the-shelf LLMs for such tasks without extensive
fairness safeguards. Furthermore, we show with empirical evidence that there
shouldn't be a dichotomy between choosing accuracy and fairness in hiring: a
well-designed algorithm can achieve both accuracy in hiring and fairness in
outcomes.

</details>


### [151] [Sample Complexity Bounds for Linear Constrained MDPs with a Generative Model](https://arxiv.org/abs/2507.02089)
*Xingtu Liu,Lin F. Yang,Sharan Vaswani*

Main category: cs.LG

TL;DR: 本文研究了具有无限时间范围和折扣因子的线性约束马尔可夫决策过程（CMDPs），提出了一个可以利用任意黑盒无约束MDP求解器的原始-对偶框架，并针对不同的可行性条件（放松和严格）提供了样本复杂度界定。


<details>
  <summary>Details</summary>
Motivation: 解决CMDPs中过去方法在样本效率和约束可行性间的局限性，提出一种新的框架以兼顾高效性和灵活性。

Method: 提出了一个原始-对偶框架，并结合镜像下降值迭代（MDVI）算法解决线性CMDPs的问题，同时为放松和严格可行性提供样本复杂度分析。

Result: 在放松可行性下实现了对维度$d$和精度$\epsilon$依赖近最优的样本复杂度$\tilde{O}(\frac{d^2}{(1-\gamma)^4\epsilon^2})$；在严格可行性下需要考虑额外依赖问题参数$\zeta$的复杂度。框架也适用于表格式CMDPs并能恢复近最优复杂度。

Conclusion: 提供了一种通用且灵活的解决CMDPs的框架，不仅提升了样本效率，在处理约束满足问题上也展现出优异性能。

Abstract: We consider infinite-horizon $\gamma$-discounted (linear) constrained Markov
decision processes (CMDPs) where the objective is to find a policy that
maximizes the expected cumulative reward subject to expected cumulative
constraints. Given access to a generative model, we propose to solve CMDPs with
a primal-dual framework that can leverage any black-box unconstrained MDP
solver. For linear CMDPs with feature dimension $d$, we instantiate the
framework by using mirror descent value iteration
(\texttt{MDVI})~\citep{kitamura2023regularization} an example MDP solver. We
provide sample complexity bounds for the resulting CMDP algorithm in two cases:
(i) relaxed feasibility, where small constraint violations are allowed, and
(ii) strict feasibility, where the output policy is required to exactly satisfy
the constraint. For (i), we prove that the algorithm can return an
$\epsilon$-optimal policy with high probability by using
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^4\epsilon^2}\right)$ samples. We note
that these results exhibit a near-optimal dependence on both $d$ and
$\epsilon$. For (ii), we show that the algorithm requires
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^6\epsilon^2\zeta^2}\right)$ samples,
where $\zeta$ is the problem-dependent Slater constant that characterizes the
size of the feasible region. Finally, we instantiate our framework for tabular
CMDPs and show that it can be used to recover near-optimal sample complexities
in this setting.

</details>


### [152] [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092)
*Alexi Gladstone,Ganesh Nanduru,Md Mofijul Islam,Peixuan Han,Hyeonjeong Ha,Aman Chadha,Yilun Du,Heng Ji,Jundong Li,Tariq Iqbal*

Main category: cs.LG

TL;DR: 本文介绍了能量变换器（EBTs），一种通过计算输入和预测候选之间能量值的模型，相较现有方法如Transformer++和扩散变换器在语言和视觉任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究是否能够通过无监督学习泛化以往特定领域的System 2 Thinking方法。

Method: 提出能量变换器（EBTs），通过训练EBMs模型分配输入与预测候选对的能量值，利用梯度下降优化能量以实现预测。

Result: EBTs在训练中比Transformer++方法具有更快的标量增长率；在推理中提高29%的性能，并在图像去噪任务中优于扩散变换器。

Conclusion: EBTs在学习和增强思维能力方面比现有方法具有更好的泛化能力，表现出很大的潜力。

Abstract: Inference-time computation techniques, analogous to human System 2 Thinking,
have recently become popular for improving model performances. However, most
existing approaches suffer from several limitations: they are modality-specific
(e.g., working only in text), problem-specific (e.g., verifiable domains like
math and coding), or require additional supervision/training on top of
unsupervised pretraining (e.g., verifiers or verifiable rewards). In this
paper, we ask the question "Is it possible to generalize these System 2
Thinking approaches, and develop models that learn to think solely from
unsupervised learning?" Interestingly, we find the answer is yes, by learning
to explicitly verify the compatibility between inputs and
candidate-predictions, and then re-framing prediction problems as optimization
with respect to this verifier. Specifically, we train Energy-Based Transformers
(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy
value to every input and candidate-prediction pair, enabling predictions
through gradient descent-based energy minimization until convergence. Across
both discrete (text) and continuous (visual) modalities, we find EBTs scale
faster than the dominant Transformer++ approach during training, achieving an
up to 35% higher scaling rate with respect to data, batch size, parameters,
FLOPs, and depth. During inference, EBTs improve performance with System 2
Thinking by 29% more than the Transformer++ on language tasks, and EBTs
outperform Diffusion Transformers on image denoising while using fewer forward
passes. Further, we find that EBTs achieve better results than existing models
on most downstream tasks given the same or worse pretraining performance,
suggesting that EBTs generalize better than existing approaches. Consequently,
EBTs are a promising new paradigm for scaling both the learning and thinking
capabilities of models.

</details>


### [153] [Parametric Neural Amp Modeling with Active Learning](https://arxiv.org/abs/2507.02109)
*Florian Grötschla,Luca A. Lanzendörfer,Longxiang Jiao,Roger Wattenhofer*

Main category: cs.LG

TL;DR: PANAMA是一种使用WaveNet架构的主动学习框架，用于训练端到端的吉他放大器参数模型。


<details>
  <summary>Details</summary>
Motivation: 通过最少的数据点创建虚拟吉他放大器模型，提高效率并实现高质量模拟。

Method: 采用主动学习策略和梯度优化算法选择最优数据点，使用WaveNet架构训练参数模型。

Result: 在样本数量受限情况下表现优秀，验证了所提方法的有效性。

Conclusion: PANAMA框架能够高效生成高质量的虚拟吉他放大器模型。

Abstract: We introduce PANAMA, an active learning framework for the training of
end-to-end parametric guitar amp models using a WaveNet-like architecture. With
\model, one can create a virtual amp by recording samples that are determined
by an active learning strategy to use a minimum amount of datapoints (i.e., amp
knob settings). We show that gradient-based optimization algorithms can be used
to determine the optimal datapoints to sample, and that the approach helps
under a constrained number of samples.

</details>


### [154] [Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks](https://arxiv.org/abs/2507.02119)
*Shikai Qiu,Lechao Xiao,Andrew Gordon Wilson,Jeffrey Pennington,Atish Agarwala*

Main category: cs.LG

TL;DR: 研究表明，在神经网络训练中，当模型大小与训练时间同步增长时，经过计算最优训练的模型展现出一种显著的普适性，且在归一化训练计算和损失后，损失曲线能够高度重合。


<details>
  <summary>Details</summary>
Motivation: 探索当模型大小和训练时间同步增加时，神经网络训练动态的标度极限，并发现这一过程中的普适规律。

Method: 通过对多种数据集、学习率策略及架构（包括基于transformer的模型）的训练实验，验证损失曲线的归一化叠合现象，并通过理论分析连接曲线叠合现象与神经标度律中的幂律结构。同时，构建一个简单而有效的SGD噪声动力学模型，预测损失曲线并解释现象来源。

Result: 在多种学习率策略、数据集和模型架构中观察到了曲线叠合和超叠合现象，发现当超参数缩放设置不当时，这些现象不会出现，从而明确了良好缩放的指示条件。

Conclusion: 本文揭示了神经网络训练中普遍存在的曲线叠合现象背后的结构规律，并提出了对训练动态精确预测的理论基础，为神经网络的高效训练提供了指导。

Abstract: What scaling limits govern neural network training dynamics when model size
and training time grow in tandem? We show that despite the complex interactions
between architecture, training algorithms, and data, compute-optimally trained
models exhibit a remarkably precise universality. Specifically, loss curves
from models of varying sizes collapse onto a single universal curve when
training compute and loss are normalized to unity at the end of training. With
learning rate decay, the collapse becomes so tight that differences in the
normalized curves across models fall below the noise floor of individual loss
curves across random seeds, a phenomenon we term supercollapse. We observe
supercollapse across learning rate schedules, datasets, and architectures,
including transformers trained on next-token prediction, and find it breaks
down when hyperparameters are scaled suboptimally, providing a precise and
practical indicator of good scaling. We explain these phenomena by connecting
collapse to the power-law structure in typical neural scaling laws, and
analyzing a simple yet surprisingly effective model of SGD noise dynamics that
accurately predicts loss curves across various learning rate schedules and
quantitatively explains the origin of supercollapse.

</details>


### [155] [CROP: Circuit Retrieval and Optimization with Parameter Guidance using LLMs](https://arxiv.org/abs/2507.02128)
*Jingyu Pan,Isaac Jacobson,Zheng Zhao,Tung-Chieh Chen,Guanglei Zhou,Chen-Chia Chang,Vineet Rashingkar,Yiran Chen*

Main category: cs.LG

TL;DR: 该论文提出了CROP，这是一个基于大语言模型（LLM）的自动化VLSI设计流程调优框架，可有效简化和提升芯片设计优化过程。


<details>
  <summary>Details</summary>
Motivation: 当前电子设计自动化（EDA）算法的复杂性和过于庞大的参数空间使得芯片设计优化极具挑战性，而依赖于人工选择参数的方法既费时又受专家能力限制。

Method: CROP引入了三大核心方法：(1) 将RTL源代码转化为密集向量表示的可扩展方法；(2) 基于嵌入的检索系统，用于匹配设计与语义相似的电路；(3) 利用RAG增强的大语言模型指导的参数搜索系统，并结合相似设计的先验知识进行约束优化搜索。

Result: 实验结果显示，与现有工业设计方法相比，CROP在减少迭代次数的同时，能显著提高结果质量（QoR），包括实现了9.9%的功耗降低。

Conclusion: CROP框架能够执行有效的VLSI设计流程调优，相较传统方法具有更高效和更优越的性能表现。

Abstract: Modern very large-scale integration (VLSI) design requires the implementation
of integrated circuits using electronic design automation (EDA) tools. Due to
the complexity of EDA algorithms, the vast parameter space poses a huge
challenge to chip design optimization, as the combination of even moderate
numbers of parameters creates an enormous solution space to explore. Manual
parameter selection remains industrial practice despite being excessively
laborious and limited by expert experience. To address this issue, we present
CROP, the first large language model (LLM)-powered automatic VLSI design flow
tuning framework. Our approach includes: (1) a scalable methodology for
transforming RTL source code into dense vector representations, (2) an
embedding-based retrieval system for matching designs with semantically similar
circuits, and (3) a retrieval-augmented generation (RAG)-enhanced LLM-guided
parameter search system that constrains the search process with prior knowledge
from similar designs. Experiment results demonstrate CROP's ability to achieve
superior quality-of-results (QoR) with fewer iterations than existing
approaches on industrial designs, including a 9.9% reduction in power
consumption.

</details>


### [156] [Generative Latent Diffusion for Efficient Spatiotemporal Data Reduction](https://arxiv.org/abs/2507.02129)
*Xiao Li,Liangji Zhu,Anand Rangarajan,Sanjay Ranka*

Main category: cs.LG

TL;DR: 本文提出了一种高效的潜变量扩散框架，大幅提升了数据压缩的性能与可控性。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在数据压缩中存在可控性和重建精度的局限性，限制了其实用性。

Method: 通过将变分自编码器与条件扩散模型相结合，只压缩少量关键帧至潜变量空间，并利用生成式插值重建其余帧，无需存储每帧的潜变量表示。

Result: 相比基于规则的先进压缩器SZ3，压缩率提高了最多10倍；在相同重建误差下，比主流学习方法提升了63%的性能。

Conclusion: 该方法能在降低存储成本的同时实现准确的时空重建，是数据压缩领域的一大进步。

Abstract: Generative models have demonstrated strong performance in conditional
settings and can be viewed as a form of data compression, where the condition
serves as a compact representation. However, their limited controllability and
reconstruction accuracy restrict their practical application to data
compression. In this work, we propose an efficient latent diffusion framework
that bridges this gap by combining a variational autoencoder with a conditional
diffusion model. Our method compresses only a small number of keyframes into
latent space and uses them as conditioning inputs to reconstruct the remaining
frames via generative interpolation, eliminating the need to store latent
representations for every frame. This approach enables accurate spatiotemporal
reconstruction while significantly reducing storage costs. Experimental results
across multiple datasets show that our method achieves up to 10 times higher
compression ratios than rule-based state-of-the-art compressors such as SZ3,
and up to 63 percent better performance than leading learning-based methods
under the same reconstruction error.

</details>


### [157] [Non-exchangeable Conformal Prediction for Temporal Graph Neural Networks](https://arxiv.org/abs/2507.02151)
*Tuo Wang,Jian Kang,Yujun Yan,Adithya Kulkarni,Dawei Zhou*

Main category: cs.LG

TL;DR: 提出了NCPNET，一种适用于时间图的端到端保序预测框架，可以量化GNN的不确定性，同时提高其可靠性和效率。


<details>
  <summary>Details</summary>
Motivation: 目前的保序预测方法主要针对静态图，未考虑实际应用中图的动态特性及时间依赖性，从而限制了其适用性。

Method: 提出了一种基于扩散的非保序性得分方法，用于捕捉动态网络中的拓扑和时间不确定性，并开发了一种效率感知优化算法以改进保序预测流程。

Result: 在包括WIKI、REDDIT、DBLP、IBM反洗钱数据集在内的多种实际时间图上进行了广泛实验。结果显示NCPNET在时间图中保证了预测覆盖的同时，在WIKI数据集上将预测集合大小减少了31%，显著提高了效率。

Conclusion: NCPNET成功解决了时间图中的保序预测挑战，在处理动态数据时提供了可靠和高效的新方法。

Abstract: Conformal prediction for graph neural networks (GNNs) offers a promising
framework for quantifying uncertainty, enhancing GNN reliability in high-stakes
applications. However, existing methods predominantly focus on static graphs,
neglecting the evolving nature of real-world graphs. Temporal dependencies in
graph structure, node attributes, and ground truth labels violate the
fundamental exchangeability assumption of standard conformal prediction
methods, limiting their applicability. To address these challenges, in this
paper, we introduce NCPNET, a novel end-to-end conformal prediction framework
tailored for temporal graphs. Our approach extends conformal prediction to
dynamic settings, mitigating statistical coverage violations induced by
temporal dependencies. To achieve this, we propose a diffusion-based
non-conformity score that captures both topological and temporal uncertainties
within evolving networks. Additionally, we develop an efficiency-aware
optimization algorithm that improves the conformal prediction process,
enhancing computational efficiency and reducing coverage violations. Extensive
experiments on diverse real-world temporal graphs, including WIKI, REDDIT,
DBLP, and IBM Anti-Money Laundering dataset, demonstrate NCPNET's capability to
ensure guaranteed coverage in temporal graphs, achieving up to a 31% reduction
in prediction set size on the WIKI dataset, significantly improving efficiency
compared to state-of-the-art methods. Our data and code are available at
https://github.com/ODYSSEYWT/NCPNET.

</details>


### [158] [Statistical Inference for Responsiveness Verification](https://arxiv.org/abs/2507.02169)
*Seung Hyun Cheon,Meredith Stewart,Bogdan Kulynych,Tsui-Wei Weng,Berk Ustun*

Main category: cs.LG

TL;DR: 研究引入了一种程序验证模型预测对特征干预响应性的正式方法。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型在个人预测应用中忽略个体输入改变的问题，提升安全性。

Method: 提出了一种响应度估计算法，采用黑盒方法通过约束和分布控制对可达点进行均匀采样，促进验证任务如伪证和失败概率估计。

Result: 针对任何模型和数据集，估算响应性并应用于实际场景如犯罪再犯预测、器官移植优先级和内容审核。

Conclusion: 新方法强化了预测中对输入干预的适应性，促进机器学习模型在实际应用中的安全性。

Abstract: Many safety failures in machine learning arise when models are used to assign
predictions to people (often in settings like lending, hiring, or content
moderation) without accounting for how individuals can change their inputs. In
this work, we introduce a formal validation procedure for the responsiveness of
predictions with respect to interventions on their features. Our procedure
frames responsiveness as a type of sensitivity analysis in which practitioners
control a set of changes by specifying constraints over interventions and
distributions over downstream effects. We describe how to estimate
responsiveness for the predictions of any model and any dataset using only
black-box access, and how to use these estimates to support tasks such as
falsification and failure probability estimation. We develop algorithms that
construct these estimates by generating a uniform sample of reachable points,
and demonstrate how they can promote safety in real-world applications such as
recidivism prediction, organ transplant prioritization, and content moderation.

</details>


### [159] [Metric Design != Metric Behavior: Improving Metric Selection for the Unbiased Evaluation of Dimensionality Reduction](https://arxiv.org/abs/2507.02225)
*Jiyeon Bae,Hyeon Jeon,Jinwook Seo*

Main category: cs.LG

TL;DR: 提出一种工作流，通过减少评估指标中的偏差来提高降维投影准确性评价。


<details>
  <summary>Details</summary>
Motivation: 现有的降维投影评价可能因选择高度相关的评估指标而产生偏差，影响可靠的视觉分析。

Method: 通过计算指标的两两相关性，聚类指标以减少重复性，从每个聚类中选择一个代表性指标，从而减少评估中的偏差。

Result: 定量实验表明该方法提高了降维投影评价的稳定性。

Conclusion: 所提出的工作流有效缓解了降维投影评价中的偏差问题，为可靠的视觉分析提供支持。

Abstract: Evaluating the accuracy of dimensionality reduction (DR) projections in
preserving the structure of high-dimensional data is crucial for reliable
visual analytics. Diverse evaluation metrics targeting different structural
characteristics have thus been developed. However, evaluations of DR
projections can become biased if highly correlated metrics--those measuring
similar structural characteristics--are inadvertently selected, favoring DR
techniques that emphasize those characteristics. To address this issue, we
propose a novel workflow that reduces bias in the selection of evaluation
metrics by clustering metrics based on their empirical correlations rather than
on their intended design characteristics alone. Our workflow works by computing
metric similarity using pairwise correlations, clustering metrics to minimize
overlap, and selecting a representative metric from each cluster. Quantitative
experiments demonstrate that our approach improves the stability of DR
evaluation, which indicates that our workflow contributes to mitigating
evaluation bias.

</details>


### [160] [PhysicsCorrect: A Training-Free Approach for Stable Neural PDE Simulations](https://arxiv.org/abs/2507.02227)
*Xinquan Huang,Paris Perdikaris*

Main category: cs.LG

TL;DR: PhysicsCorrect框架通过引入训练无关的修正方法解决了神经网络在长时间预测偏微分方程(PDEs)时由于误差累积导致失效的问题。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络模型在长时间PDE预测中误差逐步放大，最终偏离真实物理解的问题。

Method: 提出PhysicsCorrect框架，利用偏微分方程残差，将修正过程形式化为线性逆问题，创新性地通过缓存策略在预阶段预计算Jacobian及其伪逆矩阵，大幅降低计算成本。

Result: 在Navier-Stokes流体动力学、波动方程和混沌Kuramoto-Sivashinsky方程的测试中，PhysicsCorrect将预测误差减少了多达100倍，同时推理时间增加小于5%。

Conclusion: PhysicsCorrect通过与各种神经网络架构无缝集成，将不稳定的神经代理模型转变为可靠的高效仿真工具，弥补了深度学习快速计算和物理精度之间的差距。

Abstract: Neural networks have emerged as powerful surrogates for solving partial
differential equations (PDEs), offering significant computational speedups over
traditional methods. However, these models suffer from a critical limitation:
error accumulation during long-term rollouts, where small inaccuracies compound
exponentially, eventually causing complete divergence from physically valid
solutions. We present PhysicsCorrect, a training-free correction framework that
enforces PDE consistency at each prediction step by formulating correction as a
linearized inverse problem based on PDE residuals. Our key innovation is an
efficient caching strategy that precomputes the Jacobian and its pseudoinverse
during an offline warm-up phase, reducing computational overhead by two orders
of magnitude compared to standard correction approaches. Across three
representative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and
the chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction
errors by up to 100x while adding negligible inference time (under 5\%). The
framework integrates seamlessly with diverse architectures including Fourier
Neural Operators, UNets, and Vision Transformers, effectively transforming
unstable neural surrogates into reliable simulation tools that bridge the gap
between deep learning's computational efficiency and the physical fidelity
demanded by practical scientific applications.

</details>


### [161] [VERBA: Verbalizing Model Differences Using Large Language Models](https://arxiv.org/abs/2507.02241)
*Shravan Doda,Shashidhar Reddy Javaji,Zining Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种名为VERBA的方法，用于通过大语言模型生成机器学习模型之间的差异描述，从而提升模型透明度和可比性。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习领域中存在大量拥有相似性能但行为各异的模型，导致用户在选择适合的模型时面临困难。研究旨在解决这一问题，通过生成模型差异记录来帮助用户进行选择。

Method: 提出了一种称为VERBA的方法，利用大语言模型从两个模型的采样中生成语言化的差异描述，同时制定了评估这些描述信息量的协议并组建了评测基准。

Result: 在决策树模型的实验中，当性能差异在5%以内但行为差异为20-25%时，VERBA可以生成精度高达80%的差异描述；加入结构信息后，准确率进一步提升至90%。

Conclusion: VERBA方法为机器学习模型的透明性和可比性提供了新的研究方向，特别是通过后验的方式生成对比信息。

Abstract: In the current machine learning landscape, we face a "model lake" phenomenon:
Given a task, there is a proliferation of trained models with similar
performances despite different behavior. For model users attempting to navigate
and select from the models, documentation comparing model pairs is helpful.
However, for every $N$ models there could be $O(N^2)$ pairwise comparisons, a
number prohibitive for the model developers to manually perform pairwise
comparisons and prepare documentations. To facilitate fine-grained pairwise
comparisons among models, we introduced $\textbf{VERBA}$. Our approach
leverages a large language model (LLM) to generate verbalizations of model
differences by sampling from the two models. We established a protocol that
evaluates the informativeness of the verbalizations via simulation. We also
assembled a suite with a diverse set of commonly used machine learning models
as a benchmark. For a pair of decision tree models with up to 5% performance
difference but 20-25% behavioral differences, $\textbf{VERBA}$ effectively
verbalizes their variations with up to 80% overall accuracy. When we included
the models' structural information, the verbalization's accuracy further
improved to 90%. $\textbf{VERBA}$ opens up new research avenues for improving
the transparency and comparability of machine learning models in a post-hoc
manner.

</details>


### [162] [Order Acquisition Under Competitive Pressure: A Rapidly Adaptive Reinforcement Learning Approach for Ride-Hailing Subsidy Strategies](https://arxiv.org/abs/2507.02244)
*Fangzhou Shi,Xiaopeng Ke,Xinye Xiong,Kexin Meng,Chang Men,Zhengdan Zhu*

Main category: cs.LG

TL;DR: 研究提出FCA-RL框架，通过强化学习优化网约车平台补贴策略，提高订单量及适应市场竞争。


<details>
  <summary>Details</summary>
Motivation: 当前网约车平台竞争激烈，服务提供商需通过动态补贴策略来增加订单量并保持市场竞争力，但相关研究较少。

Method: 提出FCA-RL，一个结合快速竞争适应（FCA）和强化拉格朗日调整（RLA）的框架，并引入RideGym仿真环境评估策略效果。

Result: 实验结果表明，FCA-RL在多种市场条件下表现优于基准方法，有效优化了补贴决策。

Conclusion: FCA-RL框架提供了一种现有竞争环境下可靠且高效的补贴策略解决方案。

Abstract: The proliferation of ride-hailing aggregator platforms presents significant
growth opportunities for ride-service providers by increasing order volume and
gross merchandise value (GMV). On most ride-hailing aggregator platforms,
service providers that offer lower fares are ranked higher in listings and,
consequently, are more likely to be selected by passengers. This competitive
ranking mechanism creates a strong incentive for service providers to adopt
coupon strategies that lower prices to secure a greater number of orders, as
order volume directly influences their long-term viability and sustainability.
Thus, designing an effective coupon strategy that can dynamically adapt to
market fluctuations while optimizing order acquisition under budget constraints
is a critical research challenge. However, existing studies in this area remain
scarce.
  To bridge this gap, we propose FCA-RL, a novel reinforcement learning-based
subsidy strategy framework designed to rapidly adapt to competitors' pricing
adjustments. Our approach integrates two key techniques: Fast Competition
Adaptation (FCA), which enables swift responses to dynamic price changes, and
Reinforced Lagrangian Adjustment (RLA), which ensures adherence to budget
constraints while optimizing coupon decisions on new price landscape.
Furthermore, we introduce RideGym, the first dedicated simulation environment
tailored for ride-hailing aggregators, facilitating comprehensive evaluation
and benchmarking of different pricing strategies without compromising
real-world operational efficiency. Experimental results demonstrate that our
proposed method consistently outperforms baseline approaches across diverse
market conditions, highlighting its effectiveness in subsidy optimization for
ride-hailing service providers.

</details>


### [163] [Uncertainty-aware Reward Design Process](https://arxiv.org/abs/2507.02256)
*Yang Yang,Xiaolu Zhou,Bosong Ding,Miao Xin*

Main category: cs.LG

TL;DR: 本文提出了URDP，一个结合大语言模型和贝叶斯优化的框架，用于提升强化学习中奖励函数设计的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 当前奖励函数设计方法存在效率低、效果差的问题，需探索更高效的自动化奖励设计方法。

Method: 提出了URDP框架，结合大语言模型（LLMs）进行奖励逻辑分析和贝叶斯优化进行参数调整，基于不确定性分析优化奖励函数设计流程。

Result: 通过在35个任务上的实验证明，URDP不仅能生成更高质量的奖励函数，还显著提升了奖励设计的效率。

Conclusion: URDP在强化学习奖励函数设计方面表现优异，未来或可大范围应用于相关任务。

Abstract: Designing effective reward functions is a cornerstone of reinforcement
learning (RL), yet it remains a challenging process due to the inefficiencies
and inconsistencies inherent in conventional reward engineering methodologies.
Recent advances have explored leveraging large language models (LLMs) to
automate reward function design. However, their suboptimal performance in
numerical optimization often yields unsatisfactory reward quality, while the
evolutionary search paradigm demonstrates inefficient utilization of simulation
resources, resulting in prohibitively lengthy design cycles with
disproportionate computational overhead. To address these challenges, we
propose the Uncertainty-aware Reward Design Process (URDP), a novel framework
that integrates large language models to streamline reward function design and
evaluation in RL environments. URDP quantifies candidate reward function
uncertainty based on self-consistency analysis, enabling simulation-free
identification of ineffective reward components while discovering novel reward
components. Furthermore, we introduce uncertainty-aware Bayesian optimization
(UABO), which incorporates uncertainty estimation to significantly enhance
hyperparameter configuration efficiency. Finally, we construct a bi-level
optimization architecture by decoupling the reward component optimization and
the hyperparameter tuning. URDP orchestrates synergistic collaboration between
the reward logic reasoning of the LLMs and the numerical optimization strengths
of the Bayesian Optimization. We conduct a comprehensive evaluation of URDP
across 35 diverse tasks spanning three benchmark environments. Our experimental
results demonstrate that URDP not only generates higher-quality reward
functions but also achieves significant improvements in the efficiency of
automated reward design compared to existing approaches.

</details>


### [164] [Knowledge Graph-Based Explainable and Generalized Zero-Shot Semantic Communications](https://arxiv.org/abs/2507.02291)
*Zhaoyu Zhang,Lingyi Wang,Wei Wu,Fuhui Zhou,Qihui Wu*

Main category: cs.LG

TL;DR: 该论文提出了一种名为KGZS-SC的知识图谱增强型零样本语义通信网络，通过知识图谱的结构化语义信息改善了语义表示的泛化能力，并在未见数据上的推理能力得到提升。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动的语义通信依赖于表面统计模式，缺乏可解释性和对未见数据的泛化能力。为解决这些问题，本研究引入知识图谱的结构化语义信息。

Method: 提出了KGZS-SC网络，该网络利用知识图谱语义知识库对语义特征进行对齐，提升了语义表示的泛化能力，并通过零样本学习在接收端实现对未见类别的直接分类。

Result: 仿真结果表明，在APY数据集上，KGZS-SC网络在未见类别的分类中性能显著优于现有语义通信框架，表现出良好的泛化能力。

Conclusion: KGZS-SC网络通过对齐语义特征和利用零样本学习，减小了通信开销，提升了动态或资源受限环境下的适应性和效率。

Abstract: Data-driven semantic communication is based on superficial statistical
patterns, thereby lacking interpretability and generalization, especially for
applications with the presence of unseen data. To address these challenges, we
propose a novel knowledge graph-enhanced zero-shot semantic communication
(KGZS-SC) network. Guided by the structured semantic information from a
knowledge graph-based semantic knowledge base (KG-SKB), our scheme provides
generalized semantic representations and enables reasoning for unseen cases.
Specifically, the KG-SKB aligns the semantic features in a shared category
semantics embedding space and enhances the generalization ability of the
transmitter through aligned semantic features, thus reducing communication
overhead by selectively transmitting compact visual semantics. At the receiver,
zero-shot learning (ZSL) is leveraged to enable direct classification for
unseen cases without the demand for retraining or additional computational
overhead, thereby enhancing the adaptability and efficiency of the
classification process in dynamic or resource-constrained environments. The
simulation results conducted on the APY datasets show that the proposed KGZS-SC
network exhibits robust generalization and significantly outperforms existing
SC frameworks in classifying unseen categories across a range of SNR levels.

</details>


### [165] [Holistic Continual Learning under Concept Drift with Adaptive Memory Realignment](https://arxiv.org/abs/2507.02310)
*Alif Ashrafee,Jedrzej Kozal,Michal Wozniak,Bartosz Krawczyk*

Main category: cs.LG

TL;DR: 提出了一种名为自适应记忆重定位（AMR）的轻量解决方案，应对非平稳数据流中的概念漂移问题。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法重点在于避免遗忘，未充分考虑现实中数据流的动态性，无法处理概念漂移的挑战。

Method: 提出并设计了AMR算法，其通过有选择地移除重放缓冲区中过时样本，并添加最新样本来适配概念漂移。同时开发了四个基准数据集供验证。

Result: 实验表明，AMR匹配了完全重训练的性能，显著降低了标注和计算需求，在多种基准数据集上证明其卓越表现。

Conclusion: AMR作为一个可扩展且高效的解决方案，成功在动态的持续学习环境中实现了稳定性与适应性的平衡。

Abstract: Traditional continual learning methods prioritize knowledge retention and
focus primarily on mitigating catastrophic forgetting, implicitly assuming that
the data distribution of previously learned tasks remains static. This
overlooks the dynamic nature of real-world data streams, where concept drift
permanently alters previously seen data and demands both stability and rapid
adaptation.
  We introduce a holistic framework for continual learning under concept drift
that simulates realistic scenarios by evolving task distributions. As a
baseline, we consider Full Relearning (FR), in which the model is retrained
from scratch on newly labeled samples from the drifted distribution. While
effective, this approach incurs substantial annotation and computational
overhead. To address these limitations, we propose Adaptive Memory Realignment
(AMR), a lightweight alternative that equips rehearsal-based learners with a
drift-aware adaptation mechanism. AMR selectively removes outdated samples of
drifted classes from the replay buffer and repopulates it with a small number
of up-to-date instances, effectively realigning memory with the new
distribution. This targeted resampling matches the performance of FR while
reducing the need for labeled data and computation by orders of magnitude.
  To enable reproducible evaluation, we introduce four concept-drift variants
of standard vision benchmarks: Fashion-MNIST-CD, CIFAR10-CD, CIFAR100-CD, and
Tiny-ImageNet-CD, where previously seen classes reappear with shifted
representations. Comprehensive experiments on these datasets using several
rehearsal-based baselines show that AMR consistently counters concept drift,
maintaining high accuracy with minimal overhead. These results position AMR as
a scalable solution that reconciles stability and plasticity in non-stationary
continual learning environments.

</details>


### [166] [Improving Constrained Generation in Language Models via Self-Distilled Twisted Sequential Monte Carlo](https://arxiv.org/abs/2507.02315)
*Sooyeon Kim,Giung Nam,Juho Lee*

Main category: cs.LG

TL;DR: 该研究通过自蒸馏方法改进约束文本生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决在约束生成情境下目标分布与基础模型缺乏匹配的问题。

Method: 引入自蒸馏方法，通过逐步调整基础模型，使其更符合目标分布。

Result: 改进了基础模型与目标分布的匹配，显著提高了生成质量。

Conclusion: 自蒸馏是一种有效的技术，用于改善语言模型在约束生成场景下的表现。

Abstract: Recent work has framed constrained text generation with autoregressive
language models as a probabilistic inference problem. Among these, Zhao et al.
(2024) introduced a promising approach based on twisted Sequential Monte Carlo,
which incorporates learned twist functions and twist-induced proposals to guide
the generation process. However, in constrained generation settings where the
target distribution concentrates on outputs that are unlikely under the base
model, learning becomes challenging due to sparse and uninformative reward
signals. We show that iteratively refining the base model through
self-distillation alleviates this issue by making the model progressively more
aligned with the target, leading to substantial gains in generation quality.

</details>


### [167] [Transformer-based EEG Decoding: A Survey](https://arxiv.org/abs/2507.02320)
*Haodong Zhang,Hongqi Li*

Main category: cs.LG

TL;DR: 文章综述了Transformer在EEG解码中的应用，主要探讨其在脑机接口的角色及新近发展。


<details>
  <summary>Details</summary>
Motivation: 深度学习方法的兴起为EEG解码提供了更强大、更自动化的特征学习能力，而Transformer以其强大的序列数据处理能力在EEG解码中引起广泛关注。

Method: 文章总结了Transformer在EEG解码中的应用，包括基础Transformer模型、与其他深度学习技术结合形成的混合架构，以及改进的Transformer结构的研究进展。

Result: 通过回顾Transformer技术在EEG解码中的历史演变及其混合架构的应用，系统性地揭示了Transformer及其衍生的结构如何提升EEG解码的能力。

Conclusion: 文章提供了对Transformer在EEG解码领域研究现状的清晰概述，并探讨了目前的挑战和未来的发展方向，为后续研究提供了宝贵指引。

Abstract: Electroencephalography (EEG) is one of the most common signals used to
capture the electrical activity of the brain, and the decoding of EEG, to
acquire the user intents, has been at the forefront of brain-computer/machine
interfaces (BCIs/BMIs) research. Compared to traditional EEG analysis methods
with machine learning, the advent of deep learning approaches have gradually
revolutionized the field by providing an end-to-end long-cascaded architecture,
which can learn more discriminative features automatically. Among these,
Transformer is renowned for its strong handling capability of sequential data
by the attention mechanism, and the application of Transformers in various EEG
processing tasks is increasingly prevalent. This article delves into a relevant
survey, summarizing the latest application of Transformer models in EEG
decoding since it appeared. The evolution of the model architecture is followed
to sort and organize the related advances, in which we first elucidate the
fundamentals of the Transformer that benefits EEG decoding and its direct
application. Then, the common hybrid architectures by integrating basic
Transformer with other deep learning techniques
(convolutional/recurrent/graph/spiking neural netwo-rks, generative adversarial
networks, diffusion models, etc.) is overviewed in detail. The research
advances of applying the modified intrinsic structures of customized
Transformer have also been introduced. Finally, the current challenges and
future development prospects in this rapidly evolving field are discussed. This
paper aims to help readers gain a clear understanding of the current state of
Transformer applications in EEG decoding and to provide valuable insights for
future research endeavors.

</details>


### [168] [DeltaSHAP: Explaining Prediction Evolutions in Online Patient Monitoring with Shapley Values](https://arxiv.org/abs/2507.02342)
*Changhun Kim,Yechan Mun,Sangchul Hahn,Eunho Yang*

Main category: cs.LG

TL;DR: DeltaSHAP 是一种新颖的解释性人工智能算法，专为在线患者监测系统设计，利用Shapley值适配时间序列，可实现高效、实时的预测变化解释。


<details>
  <summary>Details</summary>
Motivation: 现有的XAI方法无法满足临床时间序列解释任务的独特需求，特别是在患者风险演变的实时监控和解释中，因此需要新方法来实现连续预测变化的解释。

Method: 将Shapley值适配到时间序列场景，捕获特征间的联合效应，并仅使用观测到的特征组合进行归因。同时引入新的评价指标，衡量在线时间序列归因的有效性。

Result: 实验显示，DeltaSHAP 在在线患者监测任务中的解释质量上优于现有方法62%，计算效率提高33%，并在MIMIC-III数据集上验证了其有效性。

Conclusion: DeltaSHAP 满足了临床时间序列解释任务的关键需求，是既高效又实用的实时临床应用解决方案。

Abstract: This study proposes DeltaSHAP, a novel explainable artificial intelligence
(XAI) algorithm specifically designed for online patient monitoring systems. In
clinical environments, discovering the causes driving patient risk evolution is
critical for timely intervention, yet existing XAI methods fail to address the
unique requirements of clinical time series explanation tasks. To this end,
DeltaSHAP addresses three key clinical needs: explaining the changes in the
consecutive predictions rather than isolated prediction scores, providing both
magnitude and direction of feature attributions, and delivering these insights
in real time. By adapting Shapley values to temporal settings, our approach
accurately captures feature coalition effects. It further attributes prediction
changes using only the actually observed feature combinations, making it
efficient and practical for time-sensitive clinical applications. We also
introduce new evaluation metrics to evaluate the faithfulness of the
attributions for online time series, and demonstrate through experiments on
online patient monitoring tasks that DeltaSHAP outperforms state-of-the-art XAI
methods in both explanation quality as 62% and computational efficiency as 33%
time reduction on the MIMIC-III decompensation benchmark. We release our code
at https://github.com/AITRICS/DeltaSHAP.

</details>


### [169] [Offline Reinforcement Learning with Penalized Action Noise Injection](https://arxiv.org/abs/2507.02356)
*JunHyeok Oh,Byung-Jun Lee*

Main category: cs.LG

TL;DR: 提出了一种新的方法PANI，通过引入带惩罚的噪声动作提升离线强化学习性能。


<details>
  <summary>Details</summary>
Motivation: 探讨是否有必要使用计算开销大的扩散模型来提升离线强化学习算法性能，并寻找更简单高效的方法。

Method: 引入了一种名为PANI的方法，通过向动作添加噪声覆盖整个动作空间，并对噪声注入量进行惩罚，从而提升离线强化学习的表现。

Result: PANI方法在理论上被证明解决了一个修改的马尔可夫决策过程，并且在多种基准上表现出了显著的性能提升。

Conclusion: PANI方法具有广泛的兼容性和显著的性能提升，证明了一种无需扩散模型的高效离线强化学习新思路。

Abstract: Offline reinforcement learning (RL) optimizes a policy using only a fixed
dataset, making it a practical approach in scenarios where interaction with the
environment is costly. Due to this limitation, generalization ability is key to
improving the performance of offline RL algorithms, as demonstrated by recent
successes of offline RL with diffusion models. However, it remains questionable
whether such diffusion models are necessary for highly performing offline RL
algorithms, given their significant computational requirements during
inference. In this paper, we propose Penalized Action Noise Injection (PANI), a
method that simply enhances offline learning by utilizing noise-injected
actions to cover the entire action space, while penalizing according to the
amount of noise injected. This approach is inspired by how diffusion models
have worked in offline RL algorithms. We provide a theoretical foundation for
this method, showing that offline RL algorithms with such noise-injected
actions solve a modified Markov Decision Process (MDP), which we call the noisy
action MDP. PANI is compatible with a wide range of existing off-policy and
offline RL algorithms, and despite its simplicity, it demonstrates significant
performance improvements across various benchmarks.

</details>


### [170] [Deep Reinforcement Learning-Based DRAM Equalizer Parameter Optimization Using Latent Representations](https://arxiv.org/abs/2507.02365)
*Muhammad Usama,Dong Eui Chang*

Main category: cs.LG

TL;DR: 本文提出了一种基于数据驱动的方法，通过学习的潜在信号表示进行高效信号完整性评估，并结合无模型的优势演员-评论家强化学习代理进行参数优化，在动态随机存取存储器系统中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的动态随机存取存储器系统中的信号完整性优化常常计算复杂且依赖模型，亟需一种高效且鲁棒的方法。

Method: 通过学习潜在信号表示来快速评估信号完整性，采用无模型的强化学习算法（优势演员-评论家）来优化均衡器参数，不依赖显式系统模型。

Result: 优化方法在动态随机存取存储器中表现优越，实现显著的眼图开窗面积提升：在连通的连续时间线性均衡器和判决反馈均衡器结构下提升42.7%，在仅判决反馈均衡器配置下提升36.8%。

Conclusion: 提出了一种高效的潜在信号完整性度量方法与鲁棒的强化学习优化策略，在复杂均衡器架构下验证了优越性能及高效性，具有很好的通用性。

Abstract: Equalizer parameter optimization for signal integrity in high-speed Dynamic
Random Access Memory systems is crucial but often computationally demanding or
model-reliant. This paper introduces a data-driven framework employing learned
latent signal representations for efficient signal integrity evaluation,
coupled with a model-free Advantage Actor-Critic reinforcement learning agent
for parameter optimization. The latent representation captures vital signal
integrity features, offering a fast alternative to direct eye diagram analysis
during optimization, while the reinforcement learning agent derives optimal
equalizer settings without explicit system models. Applied to industry-standard
Dynamic Random Access Memory waveforms, the method achieved significant
eye-opening window area improvements: 42.7\% for cascaded Continuous-Time
Linear Equalizer and Decision Feedback Equalizer structures, and 36.8\% for
Decision Feedback Equalizer-only configurations. These results demonstrate
superior performance, computational efficiency, and robust generalization
across diverse Dynamic Random Access Memory units compared to existing
techniques. Core contributions include an efficient latent signal integrity
metric for optimization, a robust model-free reinforcement learning strategy,
and validated superior performance for complex equalizer architectures.

</details>


### [171] [Improving Consistency in Vehicle Trajectory Prediction Through Preference Optimization](https://arxiv.org/abs/2507.02406)
*Caio Azevedo,Lina Achaji,Stefano Sabatini,Nicola Poerio,Grzegorz Bartyzel,Sascha Hornauer,Fabien Moutarde*

Main category: cs.LG

TL;DR: 本文通过结合偏好优化对多主体轨迹预测模型进行微调，有效提高情景一致性，同时保持预测精度。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的轨迹预测模型在复杂的交互场景中常因未能捕捉主体间的相互依赖而表现不佳。作者希望通过微调模型改善这一问题。

Method: 利用偏好优化，将自动计算的预测结果排名作为输入，在微调过程中对多主体轨迹预测模型进行优化。

Result: 实验表明，优化的模型在三个不同数据集上显著改善了场景一致性，同时仅对预测精度有极小影响，并在推理时不增加计算需求。

Conclusion: 通过结合偏好优化，本文提出的方法在改进复杂场景中的多主体轨迹预测性能方面表现出色。

Abstract: Trajectory prediction is an essential step in the pipeline of an autonomous
vehicle. Inaccurate or inconsistent predictions regarding the movement of
agents in its surroundings lead to poorly planned maneuvers and potentially
dangerous situations for the end-user. Current state-of-the-art
deep-learning-based trajectory prediction models can achieve excellent accuracy
on public datasets. However, when used in more complex, interactive scenarios,
they often fail to capture important interdependencies between agents, leading
to inconsistent predictions among agents in the traffic scene. Inspired by the
efficacy of incorporating human preference into large language models, this
work fine-tunes trajectory prediction models in multi-agent settings using
preference optimization. By taking as input automatically calculated preference
rankings among predicted futures in the fine-tuning process, our
experiments--using state-of-the-art models on three separate datasets--show
that we are able to significantly improve scene consistency while minimally
sacrificing trajectory prediction accuracy and without adding any excess
computational requirements at inference time.

</details>


### [172] [S2FGL: Spatial Spectral Federated Graph Learning](https://arxiv.org/abs/2507.02409)
*Zihan Tan,Suyuan Huang,Guancheng Wan,Wenke Huang,He Li,Mang Ye*

Main category: cs.LG

TL;DR: 该论文提出了结合空间和频谱方法的S2FGL框架，缓解了联邦图学习中子图之间标签信号中断和客户频谱漂移的问题，并在多个数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅从结构角度处理子图的联邦学习问题，忽视了图信号在空间和频谱域内传递的影响。子图的联邦学习存在客户端边缘断连，导致标签信号中断，即类别知识退化的问题，以及频谱异质性导致的信号频率不一致问题，即频谱漂移问题。

Method: 提出一种全球知识库用于缓解标签信号中断，并设计了频率对齐机制来解决频谱客户漂移问题，将这两种策略结合构成S2FGL框架。

Result: 在多个数据集上的实验表明，S2FGL方案在性能上具有显著的优越性。

Conclusion: 该研究有效解决了联邦图学习中由于标签信号中断和频谱异质性造成的性能下降问题，提出的S2FGL框架展示了较好的全局泛化能力。

Abstract: Federated Graph Learning (FGL) combines the privacy-preserving capabilities
of federated learning (FL) with the strong graph modeling capability of Graph
Neural Networks (GNNs). Current research addresses subgraph-FL only from the
structural perspective, neglecting the propagation of graph signals on spatial
and spectral domains of the structure. From a spatial perspective, subgraph-FL
introduces edge disconnections between clients, leading to disruptions in label
signals and a degradation in the class knowledge of the global GNN. From a
spectral perspective, spectral heterogeneity causes inconsistencies in signal
frequencies across subgraphs, which makes local GNNs overfit the local signal
propagation schemes. As a result, spectral client drifts occur, undermining
global generalizability. To tackle the challenges, we propose a global
knowledge repository to mitigate label signal disruption and a frequency
alignment to address spectral client drifts. The combination of spatial and
spectral strategies forms our framework S2FGL. Extensive experiments on
multiple datasets demonstrate the superiority of S2FGL. The code is available
at https://github.com/Wonder7racer/S2FGL.git.

</details>


### [173] [OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding](https://arxiv.org/abs/2507.02659)
*Ramchalam Kinattinkara Ramakrishnan,Zhaocong Yuan,Shaojie Zhuo,Chen Feng,Yicheng Lin,Chenzheng Su,Xiaopeng Zhang*

Main category: cs.LG

TL;DR: OmniDraft 是一个统一框架，可针对在线部署挑战，通过单一草稿模型与任何目标模型搭配运行，并动态适应用户数据，有效提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 解决在线推理中目标模型与草稿模型的不匹配，以及期望的推理延迟改善。

Method: 引入在线 n-gram 缓存和混合蒸馏微调方法，解决跨词汇表不匹配问题，并通过自适应草稿技术提高速度。

Result: OmniDraft 框架实现了如数学推理、编程和文本生成任务的在线学习，支持单一 Llama-68M 与多种目标模型配对，推理速度提高至1.5-2倍。

Conclusion: OmniDraft 适用于设备端 LLM 应用的场景，有效解决模型成本、效率和用户定制挑战。

Abstract: Speculative decoding generally dictates having a small, efficient draft model
that is either pretrained or distilled offline to a particular target model
series, for instance, Llama or Qwen models. However, within online deployment
settings, there are two major challenges: 1) usage of a target model that is
incompatible with the draft model; 2) expectation of latency improvements over
usage and time. In this work, we propose OmniDraft, a unified framework that
enables a single draft model to operate with any target model and adapt
dynamically to user data. We introduce an online n-gram cache with hybrid
distillation fine-tuning to address the cross-vocabulary mismatch across draft
and target models; and further improve decoding speed by leveraging adaptive
drafting techniques. OmniDraft is particularly suitable for on-device LLM
applications where model cost, efficiency and user customization are the major
points of contention. This further highlights the need to tackle the above
challenges and motivates the \textit{``one drafter for all''} paradigm. We
showcase the proficiency of the OmniDraft framework by performing online
learning on math reasoning, coding and text generation tasks. Notably,
OmniDraft enables a single Llama-68M model to pair with various target models
including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;
and additionally provides up to 1.5-2x speedup.

</details>


### [174] [Variational Kolmogorov-Arnold Network](https://arxiv.org/abs/2507.02466)
*Francesco Alesiani,Henrik Christiansen,Federico Errica*

Main category: cs.LG

TL;DR: 提出了一种称为InfinityKAN的新方法，通过变分推断和自适应学习来扩展无限基数的Kolmogorov Arnold Networks（KANs）。


<details>
  <summary>Details</summary>
Motivation: 当前Kolmogorov Arnold Networks使用固定基数限制了其表示能力，因此需要动态适应的解决方法。

Method: 通过将问题建模为变分推断优化问题，允许在训练过程中自适应学习每个单变量函数的基数。

Result: 开发出了InfinityKAN方法，并实现了基于反向传播的动态基学习过程。

Conclusion: 新方法InfinityKAN扩展了KAN模型的潜在应用能力，将关键超参数纳入学习过程。

Abstract: Kolmogorov Arnold Networks (KANs) are an emerging architecture for building
machine learning models. KANs are based on the theoretical foundation of the
Kolmogorov-Arnold Theorem and its expansions, which provide an exact
representation of a multi-variate continuous bounded function as the
composition of a limited number of univariate continuous functions. While such
theoretical results are powerful, their use as a representation learning
alternative to a multi-layer perceptron (MLP) hinges on the ad-hoc choice of
the number of bases modeling each of the univariate functions. In this work, we
show how to address this problem by adaptively learning a potentially infinite
number of bases for each univariate function during training. We therefore
model the problem as a variational inference optimization problem. Our
proposal, called InfinityKAN, which uses backpropagation, extends the potential
applicability of KANs by treating an important hyperparameter as part of the
learning process.

</details>


### [175] [Online Conformal Prediction with Efficiency Guarantees](https://arxiv.org/abs/2507.02496)
*Vaidehi Srinivas*

Main category: cs.LG

TL;DR: 本文研究了一种新的在线框架下的保序预测问题，重点是直接优化效率，并探讨这与传统问题的不同。


<details>
  <summary>Details</summary>
Motivation: 提出基于效率的保序预测问题，旨在考虑既满足覆盖率又能最小化区间长度的情境。

Method: 针对交换序列和任意序列，分别构建新算法，对于交换序列能匹配最优覆盖率，任意序列则权衡覆盖错误率与长度逼近优度；算法是确定性的，适应性强。

Result: 证明了无单一算法能在任意序列和交换序列中同时达到最优；提出了能在两种情况下接近最优效果的算法。

Conclusion: 该研究揭示了两种输入设置下的本质差异，并提供了匹配不同需求的几种Pareto最优算法。

Abstract: We study the problem of conformal prediction in a novel online framework that
directly optimizes efficiency. In our problem, we are given a target
miscoverage rate $\alpha > 0$, and a time horizon $T$. On each day $t \le T$ an
algorithm must output an interval $I_t \subseteq [0, 1]$, then a point $y_t \in
[0, 1]$ is revealed. The goal of the algorithm is to achieve coverage, that is,
$y_t \in I_t$ on (close to) a $(1 - \alpha)$-fraction of days, while
maintaining efficiency, that is, minimizing the average volume (length) of the
intervals played. This problem is an online analogue to the problem of
constructing efficient confidence intervals.
  We study this problem over arbitrary and exchangeable (random order) input
sequences. For exchangeable sequences, we show that it is possible to construct
intervals that achieve coverage $(1 - \alpha) - o(1)$, while having length
upper bounded by the best fixed interval that achieves coverage in hindsight.
For arbitrary sequences however, we show that any algorithm that achieves a
$\mu$-approximation in average length compared to the best fixed interval
achieving coverage in hindsight, must make a multiplicative factor more
mistakes than $\alpha T$, where the multiplicative factor depends on $\mu$ and
the aspect ratio of the problem. Our main algorithmic result is a matching
algorithm that can recover all Pareto-optimal settings of $\mu$ and number of
mistakes. Furthermore, our algorithm is deterministic and therefore robust to
an adaptive adversary.
  This gap between the exchangeable and arbitrary settings is in contrast to
the classical online learning problem. In fact, we show that no single
algorithm can simultaneously be Pareto-optimal for arbitrary sequences and
optimal for exchangeable sequences. On the algorithmic side, we give an
algorithm that achieves the near-optimal tradeoff between the two cases.

</details>


### [176] [ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning](https://arxiv.org/abs/2507.02834)
*Ruiyang Zhou,Shuozhe Li,Amy Zhang,Liu Leqi*

Main category: cs.LG

TL;DR: 本文提出一种名为ExPO的框架，通过生成基于正确答案的样本来优化模型的推理能力，特别是在早期强化学习阶段和复杂任务中显著提升效果。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型依赖于强化学习后的优化来提升推理能力，而现有方法对模型初始能力依赖较大，难以解决模型初始失败的任务。作者希望通过更有效的探索策略来突破这一局限。

Method: 提出了Self-Explanation Policy Optimization (ExPO)，通过以真实答案为条件生成指导样本，并确保该样本既符合当前模型的策略又能够提高正确答案的可能性。

Result: 实验表明，ExPO在推理基准任务中提升了学习效率和最终表现，尤其在较难的任务（如MATH level-5）中超越了基于专家演示的方法。

Conclusion: ExPO能够更有效地引导模型探索出更优秀的推理路径，为强化学习后的模型优化提供了新的解决方案。

Abstract: Recent advances in large language models have been driven by reinforcement
learning (RL)-style post-training, which improves reasoning by optimizing model
outputs based on reward or preference signals. GRPO-style approaches implement
this by using self-generated samples labeled by an outcome-based verifier.
However, these methods depend heavily on the model's initial ability to produce
positive samples. They primarily refine what the model already knows
(distribution sharpening) rather than enabling the model to solve problems
where it initially fails. This limitation is especially problematic in
early-stage RL training and on challenging reasoning tasks, where positive
samples are unlikely to be generated. To unlock reasoning ability in such
settings, the model must explore new reasoning trajectories beyond its current
output distribution. Such exploration requires access to sufficiently good
positive samples to guide the learning. While expert demonstrations seem like a
natural solution, we find that they are often ineffective in RL post-training.
Instead, we identify two key properties of effective positive samples: they
should (1) be likely under the current policy, and (2) increase the model's
likelihood of predicting the correct answer. Based on these insights, we
propose $\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and
modular framework that generates such samples by conditioning on the
ground-truth answer. ExPO enables efficient exploration and guides the model to
produce reasoning trajectories more aligned with its policy than expert-written
CoTs, while ensuring higher quality than its own (incorrect) samples.
Experiments show that ExPO improves both learning efficiency and final
performance on reasoning benchmarks, surpassing expert-demonstration-based
methods in challenging settings such as MATH level-5, where the model initially
struggles the most.

</details>


### [177] [Continual Gradient Low-Rank Projection Fine-Tuning for LLMs](https://arxiv.org/abs/2507.02503)
*Chenxu Wang,Yilin Lyu,Zicheng Sun,Liping Jing*

Main category: cs.LG

TL;DR: 本研究提出了GORP方法，在保持模型效率的同时，通过联合全参数和低秩参数训练，新方法改善了连续学习的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法面临效率与表达能力之间的权衡，例如LoRA方法虽高效，但限制了模型学习新任务及知识迁移的能力。

Method: 提出了GORP，通过结合全参数和低秩参数，并在统一的低秩梯度子空间中联合更新，扩大优化空间、保持效率并缓解遗忘问题。

Result: 在多种连续学习基准测试上，GORP表现优于现有的最先进方法，并显著提升性能。

Conclusion: GORP通过平衡全参数和低秩参数的联合优化，提供了一种高效且具有更强学习能力的连续学习训练策略。

Abstract: Continual fine-tuning of Large Language Models (LLMs) is hampered by the
trade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA)
offers efficiency but constrains the model's ability to learn new tasks and
transfer knowledge due to its low-rank nature and reliance on explicit
parameter constraints. We propose GORP (Gradient LOw Rank Projection) for
Continual Learning, a novel training strategy that overcomes these limitations
by synergistically combining full and low-rank parameters and jointly updating
within a unified low-rank gradient subspace. GORP expands the optimization
space while preserving efficiency and mitigating catastrophic forgetting.
Extensive experiments on continual learning benchmarks demonstrate GORP's
superior performance compared to existing state-of-the-art approaches. Code is
available at https://github.com/Wcxwcxw/GORP.

</details>


### [178] [RetrySQL: text-to-SQL training with retry data for self-correcting query generation](https://arxiv.org/abs/2507.02529)
*Alicja Rączkowska,Riccardo Belluzzo,Piotr Zieliński,Joanna Baran,Paweł Olszewski*

Main category: cs.LG

TL;DR: 本文提出了一个名为RetrySQL的新方法，用于训练提高文本到SQL生成模型的能力，通过引入自校正机制实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到SQL解决方案多数依赖黑盒语言模型和定制组件，文化开源的基础模型鲜有专注于SQL生成，并缺乏与自校正生成能力结合的探索。

Method: 提供参考SQL查询的推理步骤，然后引入错误生成用于校正步骤的数据，采用这一机制对开放源代码编码模型进行连续预训练，提升模型的自校正能力。

Result: 通过使用RetrySQL数据预训练，关键指标的执行准确率提高了多达4个百分点，并证明完整参数预训练是必要的学习方式。还发现模型能够显式地学习自校正技能。

Conclusion: RetrySQL验证了自校正机制在文本到SQL任务中的可行性，同时与参数规模更大的专有模型竞争表现良好，为改进SQL生成模型提供了新的方法。

Abstract: The text-to-SQL task is an active challenge in Natural Language Processing.
Many existing solutions focus on using black-box language models extended with
specialized components within customized end-to-end text-to-SQL pipelines.
While these solutions use both closed-source proprietary language models and
coding-oriented open-source models, there is a lack of research regarding
SQL-specific generative models. At the same time, recent advancements in
self-correcting generation strategies show promise for improving the
capabilities of existing architectures. The application of these concepts to
the text-to-SQL task remains unexplored. In this paper, we introduce RetrySQL,
a new approach to training text-to-SQL generation models. We prepare reasoning
steps for reference SQL queries and then corrupt them to create retry data that
contains both incorrect and corrected steps, divided with a special token. We
continuously pre-train an open-source coding model with this data and
demonstrate that retry steps yield an improvement of up to 4 percentage points
in both overall and challenging execution accuracy metrics, compared to
pre-training without retry data. Additionally, we confirm that supervised
fine-tuning with LoRA is ineffective for learning from retry data and that
full-parameter pre-training is a necessary requirement for that task. We
showcase that the self-correcting behavior is learned by the model and the
increase in downstream accuracy metrics is a result of this additional skill.
Finally, we incorporate RetrySQL-trained models into the full text-to-SQL
pipeline and showcase that they are competitive in terms of execution accuracy
with proprietary models that contain orders of magnitude more parameters.
RetrySQL demonstrates that self-correction can be learned in the text-to-SQL
task and provides a novel way of improving generation accuracy for SQL-oriented
language models.

</details>


### [179] [Position: A Theory of Deep Learning Must Include Compositional Sparsity](https://arxiv.org/abs/2507.02550)
*David A. Danhofer,Davide D'Ascenzo,Rafael Dubach,Tomaso Poggio*

Main category: cs.LG

TL;DR: 本文认为深度神经网络（DNNs）的成功来源于其利用目标函数的组合稀疏性结构的能力。


<details>
  <summary>Details</summary>
Motivation: 探讨深度神经网络为何在高维问题中表现卓越，其背后的学习动力学的基本原则仍存疑问。

Method: 分析目标函数的组合稀疏性属性，即实际相关函数常由小集合的组成函数形成。这些函数仅依赖于全输入中的低维子集。

Result: 证明这种组合稀疏性属性存在于所有有效的图灵可计算函数中，因此也可能在当前的大多数学习问题中存在。

Conclusion: 全面了解组合稀疏性在深度学习中的作用对人工智能乃至普遍智能的理论发展至关重要。

Abstract: Overparametrized Deep Neural Networks (DNNs) have demonstrated remarkable
success in a wide variety of domains too high-dimensional for classical shallow
networks subject to the curse of dimensionality. However, open questions about
fundamental principles, that govern the learning dynamics of DNNs, remain. In
this position paper we argue that it is the ability of DNNs to exploit the
compositionally sparse structure of the target function driving their success.
As such, DNNs can leverage the property that most practically relevant
functions can be composed from a small set of constituent functions, each of
which relies only on a low-dimensional subset of all inputs. We show that this
property is shared by all efficiently Turing-computable functions and is
therefore highly likely present in all current learning problems. While some
promising theoretical insights on questions concerned with approximation and
generalization exist in the setting of compositionally sparse functions,
several important questions on the learnability and optimization of DNNs
remain. Completing the picture of the role of compositional sparsity in deep
learning is essential to a comprehensive theory of artificial, and even
general, intelligence.

</details>


### [180] [Transformers Don't Need LayerNorm at Inference Time: Scaling LayerNorm Removal to GPT-2 XL and the Implications for Mechanistic Interpretability](https://arxiv.org/abs/2507.02559)
*Luca Baroni,Galvin Khara,Joachim Schaeffer,Marat Subkhankulov,Stefan Heimersheim*

Main category: cs.LG

TL;DR: 本文研究了在GPT-2语言模型中移除层归一化（LN）层的可行性，发现其对验证损失仅有微小影响，并释放了无LN版本模型以促进可解释性研究。


<details>
  <summary>Details</summary>
Motivation: 探索层归一化在推断阶段的作用，评估其对语言建模性能和模型可解释性的影响。

Method: 分析GPT-2模型中移除层归一化后的验证损失变化，测试不同规模数据微调对LN移除的需求，并测试无LN版本模型的可解释性。

Result: GPT-2可移除所有LN层且仅带来轻微的验证损失增加；移除LN后模型的可解释性有所提升，用于直接效应的归因技术准确性提高。

Conclusion: LN层对语言建模的重要性有限；GPT-2类模型无需LN层即可工作；无LN模型有助于增强语言模型的可解释性和研究。

Abstract: Layer-wise normalization (LN) is an essential component of virtually all
transformer-based large language models. While its effects on training
stability are well documented, its role at inference time is poorly understood.
Additionally, LN layers hinder mechanistic interpretability by introducing
additional nonlinearities and increasing the interconnectedness of individual
model components. Here, we show that all LN layers can be removed from every
GPT-2 model with only a small increase in validation loss (e.g. +0.03
cross-entropy loss for GPT-2 XL). Thus, LN cannot play a substantial role in
language modeling. We find that the amount of fine-tuning data needed for LN
removal grows sublinearly with model parameters, suggesting scaling to larger
models is feasible. We release a suite of LN-free GPT-2 models on Hugging Face.
Furthermore, we test interpretability techniques on LN-free models. Direct
logit attribution now gives the exact direct effect of individual components,
while the accuracy of attribution patching does not significantly improve. We
also confirm that GPT-2's "confidence neurons" are inactive in the LN-free
models. Our work clarifies the role of LN layers in language modeling, showing
that GPT-2-class models can function without LN layers. We hope that our
LN-free analogs of the GPT-2 family of models will enable more precise
interpretability research and improve our understanding of language models.

</details>


### [181] [Scalable Interconnect Learning in Boolean Networks](https://arxiv.org/abs/2507.02585)
*Fabian Kresse,Emily Yu,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 本文提出了一种扩展的可训练差分布尔逻辑网络(DBNs)，通过可扩展的交互连接和两种剪枝阶段，实现了在硬件限制下的高效推理和更优的模型压缩率。


<details>
  <summary>Details</summary>
Motivation: 现有可训练可微分布尔逻辑网络在资源受限硬件上推理表现优秀，但无法很好地扩展至更宽的层，同时模型压缩方面存在不足。

Method: 引入了一种可扩展的可训练差分互联结构，其参数数量随输入宽度的增长保持不变。此外，提出了两种剪枝方法：基于SAT的逻辑等价化方法用于去除冗余门，以及基于相似性和数据驱动的剪枝方法以提升压缩率和准确性权衡。

Result: 实现了现有可训练互联设计所不能达到的扩展性，同时通过提出的剪枝方法大幅优化了模型大小和压缩率，且在准确性上表现优越。

Conclusion: 这项工作证明了通过扩展的可训练互联和剪枝技术，可以在硬件限制下实现高效推理的同时，保持模型良好的压缩精度权衡，这为实际应用场景提供了新的可能性。

Abstract: Learned Differentiable Boolean Logic Networks (DBNs) already deliver
efficient inference on resource-constrained hardware. We extend them with a
trainable, differentiable interconnect whose parameter count remains constant
as input width grows, allowing DBNs to scale to far wider layers than earlier
learnable-interconnect designs while preserving their advantageous accuracy. To
further reduce model size, we propose two complementary pruning stages: an
SAT-based logic equivalence pass that removes redundant gates without affecting
performance, and a similarity-based, data-driven pass that outperforms a
magnitude-style greedy baseline and offers a superior compression-accuracy
trade-off.

</details>


### [182] [Padé Approximant Neural Networks for Enhanced Electric Motor Fault Diagnosis Using Vibration and Acoustic Data](https://arxiv.org/abs/2507.02599)
*Sertac Kilickaya,Levent Eren*

Main category: cs.LG

TL;DR: 本研究旨在利用Padé近似神经元(PAON)模型提升感应电机故障诊断性能，通过引入Padé神经网络(PadéNets)并验证其相较于传统模型的优越性。


<details>
  <summary>Details</summary>
Motivation: 探讨PadéNets是否能够在机械与电气故障诊断中，相较传统的卷积神经网络(CNNs)及自组织操作神经网络(Self-ONNs)展现更高性能，解决基于振动与声学数据的诊断挑战。

Method: 比较三种深度学习架构（一维CNNs、Self-ONNs和PadéNets）的诊断性能，数据来源于渥太华大学的感应电机数据集，基于振动和声学传感器数据进行建模和实验。

Result: PadéNets对比基线模型表现更优，诊断准确率分别达到99.96%、98.26%、97.61%和98.33%（对于三种加速计及声学传感器）。

Conclusion: PadéNets凭借增强非线性及对无界激活函数的兼容性，大幅提升了感应电机状态监控中的故障诊断性能。

Abstract: Purpose: The primary aim of this study is to enhance fault diagnosis in
induction machines by leveraging the Pad\'e Approximant Neuron (PAON) model.
While accelerometers and microphones are standard in motor condition
monitoring, deep learning models with nonlinear neuron architectures offer
promising improvements in diagnostic performance. This research addresses the
question: Can Pad\'e Approximant Neural Networks (Pad\'eNets) outperform
conventional Convolutional Neural Networks (CNNs) and Self-Organized
Operational Neural Networks (Self-ONNs) in diagnosing electrical and mechanical
faults using vibration and acoustic data?
  Methods: We evaluate and compare the diagnostic capabilities of three deep
learning architectures: one-dimensional CNNs, Self-ONNs, and Pad\'eNets. These
models are tested on the University of Ottawa's publicly available
constant-speed induction motor datasets, which include both vibration and
acoustic sensor data. The Pad\'eNet model is designed to introduce enhanced
nonlinearity and is compatible with unbounded activation functions such as
Leaky ReLU.
  Results and Conclusion: Pad\'eNets consistently outperformed the baseline
models, achieving diagnostic accuracies of 99.96%, 98.26%, 97.61%, and 98.33%
for accelerometers 1, 2, 3, and the acoustic sensor, respectively. The enhanced
nonlinearity of Pad\'eNets, together with their compatibility with unbounded
activation functions, significantly improves fault diagnosis performance in
induction motor condition monitoring.

</details>


### [183] [Lost in Latent Space: An Empirical Study of Latent Diffusion Models for Physics Emulation](https://arxiv.org/abs/2507.02608)
*François Rozet,Ruben Ohana,Michael McCabe,Gilles Louppe,François Lanusse,Shirley Ho*

Main category: cs.LG

TL;DR: 使用潜在空间的扩散模型显著降低了推理时的计算开销，同时在高压缩率下仍保持了较高的模拟精度。


<details>
  <summary>Details</summary>
Motivation: 解决推理时扩散模型计算成本高的问题，探索能否在流体力学和动力学模拟中使用潜在空间策略。

Method: 将扩散模型应用于自动编码器的潜在空间中，分析不同压缩率下的模拟精度，并对比非生成性方法的性能。

Result: 潜在空间模拟在压缩率高达1000x时依然保持较高精度，扩散模型比非生成性方法更准确且具备更高的不确定性补偿能力。

Conclusion: 潜在空间中的扩散模型能够有效减少计算开销，是设计快速物理模拟器的一个可行方法，同时在实际设计时需考虑网络架构和优化器的选择。

Abstract: The steep computational cost of diffusion models at inference hinders their
use as fast physics emulators. In the context of image and video generation,
this computational drawback has been addressed by generating in the latent
space of an autoencoder instead of the pixel space. In this work, we
investigate whether a similar strategy can be effectively applied to the
emulation of dynamical systems and at what cost. We find that the accuracy of
latent-space emulation is surprisingly robust to a wide range of compression
rates (up to 1000x). We also show that diffusion-based emulators are
consistently more accurate than non-generative counterparts and compensate for
uncertainty in their predictions with greater diversity. Finally, we cover
practical design choices, spanning from architectures to optimizers, that we
found critical to train latent-space emulators.

</details>


### [184] [L-VAE: Variational Auto-Encoder with Learnable Beta for Disentangled Representation](https://arxiv.org/abs/2507.02619)
*Hazal Mogultay Ozcan,Sinan Kalkan,Fatos T. Yarman-Vural*

Main category: cs.LG

TL;DR: 本文提出了一种名为Learnable VAE（L-VAE）的新模型，通过同时学习表征和损失函数的超参数实现了在各任务间的动态平衡。


<details>
  <summary>Details</summary>
Motivation: 通过克服 \b{eta}-VAE 手动调整超参数的限制，探索能自适应调节损失函数权重的方法以改进表征质量。

Method: 模型通过同时学习损失函数权重和模型参数，并添加正则项以防止偏向重构或解耦任一方向。

Result: 在dSprites、MPI3D-complex、Falcor3D和Isaac3D等数据集上，与 \b{eta}-VAE、VAE等对比，L-VAE表现最佳或接近最佳。在CelebA上的实验显示表征人脸属性的成功。

Conclusion: L-VAE 能有效权衡表征解耦与重构精度，改进了VAE相关技术的性能。

Abstract: In this paper, we propose a novel model called Learnable VAE (L-VAE), which
learns a disentangled representation together with the hyperparameters of the
cost function. L-VAE can be considered as an extension of \b{eta}-VAE, wherein
the hyperparameter, \b{eta}, is empirically adjusted. L-VAE mitigates the
limitations of \b{eta}-VAE by learning the relative weights of the terms in the
loss function to control the dynamic trade-off between disentanglement and
reconstruction losses. In the proposed model, the weight of the loss terms and
the parameters of the model architecture are learned concurrently. An
additional regularization term is added to the loss function to prevent bias
towards either reconstruction or disentanglement losses. Experimental analyses
show that the proposed L-VAE finds an effective balance between reconstruction
fidelity and disentangling the latent dimensions. Comparisons of the proposed
L-VAE against \b{eta}-VAE, VAE, ControlVAE, DynamicVAE, and {\sigma}-VAE on
datasets, such as dSprites, MPI3D-complex, Falcor3D, and Isaac3D reveals that
L-VAE consistently provides the best or the second best performances measured
by a set of disentanglement metrics. Moreover, qualitative experiments on
CelebA dataset, confirm the success of the L-VAE model for disentangling the
facial attributes.

</details>


### [185] [A Matrix Variational Auto-Encoder for Variant Effect Prediction in Pharmacogenes](https://arxiv.org/abs/2507.02624)
*Antoine Honoré,Borja Rodríguez Gálvez,Yoomi Park,Yitian Zhou,Volker M. Lauschke,Ming Xiao*

Main category: cs.LG

TL;DR: 作者提出了一种基于Transformer的矩阵变分自编码器(matVAE)模型，用于评估蛋白质变体的功能影响，并在33个DMS数据集上表现出色，特别是比DeepSequence模型计算效率更高。


<details>
  <summary>Details</summary>
Motivation: 现有变体效应预测器（VEPs）主要依赖多序列比对（MSAs），但这种方法在药理基因领域受到挑战，作者希望探索用深度突变扫描（DMS）数据集替代MSAs的方法。

Method: 提出了matVAE模型，并通过结构先验及MSAs数据训练，同时对比分析了其与基于DMS数据训练的matENC-DMS模型及结合AlphaFold生成结构的性能。

Result: matVAE-MSA模型在DMS数据集上的表现优于DeepSequence模型，且参数量更少、计算效率更高；结合AlphaFold结构后性能进一步提升，与经过MSAs和DMS数据微调的DeepSequence相媲美。

Conclusion: DMS数据具有替代MSAs的潜力，同时未来应进一步开发DMS数据集并探讨其与VEPs模型的关系以优化预测性能。

Abstract: Variant effect predictors (VEPs) aim to assess the functional impact of
protein variants, traditionally relying on multiple sequence alignments (MSAs).
This approach assumes that naturally occurring variants are fit, an assumption
challenged by pharmacogenomics, where some pharmacogenes experience low
evolutionary pressure. Deep mutational scanning (DMS) datasets provide an
alternative by offering quantitative fitness scores for variants. In this work,
we propose a transformer-based matrix variational auto-encoder (matVAE) with a
structured prior and evaluate its performance on 33 DMS datasets corresponding
to 26 drug target and ADME proteins from the ProteinGym benchmark. Our model
trained on MSAs (matVAE-MSA) outperforms the state-of-the-art DeepSequence
model in zero-shot prediction on DMS datasets, despite using an order of
magnitude fewer parameters and requiring less computation at inference time. We
also compare matVAE-MSA to matENC-DMS, a model of similar capacity trained on
DMS data, and find that the latter performs better on supervised prediction
tasks. Additionally, incorporating AlphaFold-generated structures into our
transformer model further improves performance, achieving results comparable to
DeepSequence trained on MSAs and finetuned on DMS. These findings highlight the
potential of DMS datasets to replace MSAs without significant loss in
predictive performance, motivating further development of DMS datasets and
exploration of their relationships to enhance variant effect prediction.

</details>


### [186] [Medical Data Pecking: A Context-Aware Approach for Automated Quality Evaluation of Structured Medical Data](https://arxiv.org/abs/2507.02628)
*Irena Girshovitz,Atai Ambus,Moni Shahar,Ran Gilad-Bachrach*

Main category: cs.LG

TL;DR: 本文介绍了一种医疗数据质量评估方法，即医疗数据啄木方法(Medical Data Pecking), 重点解决了电子健康记录数据中的质量问题。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录(EHRs)作为流行病学研究和人工智能训练的基础，其数据质量对研究结果的影响至关重要。然而，由于EHR数据主要为临床和账单目的而采集，存在质量问题，现有的质量评估方法不够系统，难以准确评估数据是否适用于研究。

Method: 本文提出了医疗数据啄木方法(MDPT)，并借鉴软件工程中的单元测试和覆盖概念，通过两个主要组件识别数据质量问题：1) 自动化测试生成器——利用大型语言模型生成测试套件；2) 数据测试框架——执行这些测试并报告潜在错误及覆盖范围。

Result: 利用MDPT对All of Us (AoU)、MIMIC-III 和 SyntheticMass三个数据集进行评估，每个队列生成了55-73项测试，发现了20-43个数据问题，同时对测试套件的参考准确性和数值可靠性进行了分析。

Conclusion: 本文的方法通过引入外部医学知识支持，能够在数据分析工作流中提供上下文敏感的数据质量测试，提升结果的可靠性，为进一步发展奠定了基础，如支持更多数据模态和改进语义对齐方法。

Abstract: Background: The use of Electronic Health Records (EHRs) for epidemiological
studies and artificial intelligence (AI) training is increasing rapidly. The
reliability of the results depends on the accuracy and completeness of EHR
data. However, EHR data often contain significant quality issues, including
misrepresentations of subpopulations, biases, and systematic errors, as they
are primarily collected for clinical and billing purposes. Existing quality
assessment methods remain insufficient, lacking systematic procedures to assess
data fitness for research.
  Methods: We present the Medical Data Pecking approach, which adapts unit
testing and coverage concepts from software engineering to identify data
quality concerns. We demonstrate our approach using the Medical Data Pecking
Tool (MDPT), which consists of two main components: (1) an automated test
generator that uses large language models and grounding techniques to create a
test suite from data and study descriptions, and (2) a data testing framework
that executes these tests, reporting potential errors and coverage.
  Results: We evaluated MDPT on three datasets: All of Us (AoU), MIMIC-III, and
SyntheticMass, generating 55-73 tests per cohort across four conditions. These
tests correctly identified 20-43 non-aligned or non-conforming data issues. We
present a detailed analysis of the LLM-generated test suites in terms of
reference grounding and value accuracy.
  Conclusion: Our approach incorporates external medical knowledge to enable
context-sensitive data quality testing as part of the data analysis workflow to
improve the validity of its outcomes. Our approach tackles these challenges
from a quality assurance perspective, laying the foundation for further
development such as additional data modalities and improved grounding methods.

</details>


### [187] [High-Order Deep Meta-Learning with Category-Theoretic Interpretation](https://arxiv.org/abs/2507.02634)
*David H. Mguni*

Main category: cs.LG

TL;DR: 提出了一种新的分层深度学习框架，用于递归高阶元学习，让神经网络能够构建、解决并在任务层级间实现泛化。


<details>
  <summary>Details</summary>
Motivation: 解决现有元学习和任务泛化依赖人类生成数据的局限性，并增强学习过程的归纳偏置与鲁棒性。

Method: 使用生成机制创建虚拟任务，元学习器通过探索任务点并迭代优化约束区域，与类别理论中的函子概念结合，形成可解释的任务层次结构。

Result: 方法在构建和解决抽象任务层次及改进泛化能力上展现出新的可能性，推动了深度学习自我生成任务与知识迁移的能力。

Conclusion: 该架构可能为实现通用人工智能的下一代神经网络提供基础，通过自身生成有意义的任务及求解方案，统一和扩展元学习模型。

Abstract: We introduce a new hierarchical deep learning framework for recursive
higher-order meta-learning that enables neural networks (NNs) to construct,
solve, and generalise across hierarchies of tasks. Central to this approach is
a generative mechanism that creates \emph{virtual tasks} -- synthetic problem
instances designed to enable the meta-learner to learn \emph{soft constraints}
and unknown generalisable rules across related tasks. Crucially, this enables
the framework to generate its own informative, task-grounded datasets thereby
freeing machine learning (ML) training from the limitations of relying entirely
on human-generated data. By actively exploring the virtual point landscape and
seeking out tasks lower-level learners find difficult, the meta-learner
iteratively refines constraint regions. This enhances inductive biases,
regularises the adaptation process, and produces novel, unanticipated tasks and
constraints required for generalisation. Each meta-level of the hierarchy
corresponds to a progressively abstracted generalisation of problems solved at
lower levels, enabling a structured and interpretable learning progression. By
interpreting meta-learners as category-theoretic \emph{functors} that generate
and condition a hierarchy of subordinate learners, we establish a compositional
structure that supports abstraction and knowledge transfer across progressively
generalised tasks. The category-theoretic perspective unifies existing
meta-learning models and reveals how learning processes can be transformed and
compared through functorial relationships, while offering practical design
principles for structuring meta-learning. We speculate this architecture may
underpin the next generation of NNs capable of autonomously generating novel,
instructive tasks and their solutions, thereby advancing ML towards general
artificial intelligence.

</details>


### [188] [On Efficient Bayesian Exploration in Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.02639)
*Alberto Caron,Chris Hicks,Vasilios Mavroudis*

Main category: cs.LG

TL;DR: 这篇文章探讨了如何通过理论驱动的信息增益方法提高强化学习中的探索效率，并提出了一种结合模型规划和信息增益的框架PTS-BE，展示了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，高效探索是一项挑战。现有方法中，内在动机的原理和具体实现缺乏理论分析，作者希望从理想信息论视角解决这一问题。

Method: 本文通过研究针对认知不确定性（epistemic uncertainty）的探索奖励，与基于环境随机性（aleatoric noise）的区分，提出了一个在理论上具有信息增益保证的框架。引入稀疏变分高斯过程、深度核和深度集成模型等工具，最终构建了PTS-BE框架。

Result: 实验表明，PTS-BE框架在稀疏奖励和纯探索任务环境中，表现远超多种基础模型，验证了其实用性与有效性。

Conclusion: 这项研究理论上确保了基于信息增益的探索方法合理性，并通过实践验证展示了PTS-BE框架在样本效率和探索能力上的优势。

Abstract: In this work, we address the challenge of data-efficient exploration in
reinforcement learning by examining existing principled, information-theoretic
approaches to intrinsic motivation. Specifically, we focus on a class of
exploration bonuses that targets epistemic uncertainty rather than the
aleatoric noise inherent in the environment. We prove that these bonuses
naturally signal epistemic information gains and converge to zero once the
agent becomes sufficiently certain about the environment's dynamics and
rewards, thereby aligning exploration with genuine knowledge gaps. Our analysis
provides formal guarantees for IG-based approaches, which previously lacked
theoretical grounding. To enable practical use, we also discuss tractable
approximations via sparse variational Gaussian Processes, Deep Kernels and Deep
Ensemble models. We then outline a general framework - Predictive Trajectory
Sampling with Bayesian Exploration (PTS-BE) - which integrates model-based
planning with information-theoretic bonuses to achieve sample-efficient deep
exploration. We empirically demonstrate that PTS-BE substantially outperforms
other baselines across a variety of environments characterized by sparse
rewards and/or purely exploratory tasks.

</details>


### [189] [Fair Deepfake Detectors Can Generalize](https://arxiv.org/abs/2507.02645)
*Harry Cheng,Ming-Hui Liu,Yangyang Guo,Tianyi Wang,Liqiang Nie,Mohan Kankanhalli*

Main category: cs.LG

TL;DR: 该论文提出了DAID框架，以解决深度伪造检测模型在未见操控的泛化和人群公平性之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 深度伪造检测模型面临在泛化能力和人口群体公平性之间的冲突，需要通过新的方法解决此矛盾。

Method: 提出DAID框架，包括人口属性敏感数据重平衡和人口无关特征聚合两部分，用于减轻偏置并提升公平性与泛化能力。

Result: 在三个跨领域基准测试中，DAID在公平性和泛化能力上均优于现有方法。

Conclusion: 通过控制混杂因素，可通过公平性干预显著提高深度伪造检测模型的泛化能力，DAID框架验证了理论和应用的优越性。

Abstract: Deepfake detection models face two critical challenges: generalization to
unseen manipulations and demographic fairness among population groups. However,
existing approaches often demonstrate that these two objectives are inherently
conflicting, revealing a trade-off between them. In this paper, we, for the
first time, uncover and formally define a causal relationship between fairness
and generalization. Building on the back-door adjustment, we show that
controlling for confounders (data distribution and model capacity) enables
improved generalization via fairness interventions. Motivated by this insight,
we propose Demographic Attribute-insensitive Intervention Detection (DAID), a
plug-and-play framework composed of: i) Demographic-aware data rebalancing,
which employs inverse-propensity weighting and subgroup-wise feature
normalization to neutralize distributional biases; and ii) Demographic-agnostic
feature aggregation, which uses a novel alignment loss to suppress
sensitive-attribute signals. Across three cross-domain benchmarks, DAID
consistently achieves superior performance in both fairness and generalization
compared to several state-of-the-art detectors, validating both its theoretical
foundation and practical effectiveness.

</details>


### [190] [Guided Generation for Developable Antibodies](https://arxiv.org/abs/2507.02670)
*Siqi Zhao,Joshua Moller,Porfi Quintero-Cadena,Lood van Niekerk*

Main category: cs.LG

TL;DR: 提出了一个结合自然抗体序列数据和临床抗体可开发性测量值的离散扩散模型，用于优化抗体序列的设计。


<details>
  <summary>Details</summary>
Motivation: 为了解决治疗性抗体既需高亲和力，也需可制造性、稳定性和安全性的问题，以提升临床效果。

Method: 开发一个被称为有指导离散扩散模型的计算框架，该模型基于自然抗体序列数据和SVDD模块驱动探索，生成具有预测性良好开发分数的抗体。

Result: 模型在无约束采样时复现了自然抗体的特征，并通过加入SVDD指导，显著提升了开发分数的富集度。

Conclusion: 结合高通量测定的抗体开发性质，该框架实现了一种迭代、机器学习驱动的抗体设计方法。

Abstract: Therapeutic antibodies require not only high-affinity target engagement, but
also favorable manufacturability, stability, and safety profiles for clinical
effectiveness. These properties are collectively called `developability'. To
enable a computational framework for optimizing antibody sequences for
favorable developability, we introduce a guided discrete diffusion model
trained on natural paired heavy- and light-chain sequences from the Observed
Antibody Space (OAS) and quantitative developability measurements for 246
clinical-stage antibodies. To steer generation toward biophysically viable
candidates, we integrate a Soft Value-based Decoding in Diffusion (SVDD) Module
that biases sampling without compromising naturalness. In unconstrained
sampling, our model reproduces global features of both the natural repertoire
and approved therapeutics, and under SVDD guidance we achieve significant
enrichment in predicted developability scores over unguided baselines. When
combined with high-throughput developability assays, this framework enables an
iterative, ML-driven pipeline for designing antibodies that satisfy binding and
biophysical criteria in tandem.

</details>


### [191] [Embedding-Based Federated Data Sharing via Differentially Private Conditional VAEs](https://arxiv.org/abs/2507.02671)
*Francesco Di Salvo,Hanh Huyen My Nguyen,Christian Ledig*

Main category: cs.LG

TL;DR: 本研究提出了一种利用差分隐私生成模型进行数据共享的方法，使用基础模型提取信息嵌入，并通过差分隐私条件生成式自动编码器（DP-CVAE）实现隐私保护的全局数据分布建模，支持多种下游任务，同时提升隐私、扩展性和效率。


<details>
  <summary>Details</summary>
Motivation: 医学影像领域中深度学习的应用由于数据稀缺性和隐私法规限制受到阻碍，而现有的联邦学习方法通信成本过高且任务灵活性不足。

Method: 通过基础模型提取信息嵌入，减少冗余和计算开销；利用差分隐私条件生成式自动编码器（DP-CVAE）在客户端协作训练，建模隐私保护的全局数据分布，并支持多种下游任务。

Result: 验证中采用多种特征提取器，证明了本方法提升了隐私保护、扩展性和效率，性能优于传统的联邦学习分类器且保证了差分隐私，同时DP-CVAE生成的嵌入质量高于DP-CGAN，并减少了5倍参数需求。

Conclusion: 基于DP-CVAE的方法在数据共享与多任务支持方面具有高效性与隐私保护优势，是对现有深度学习与联邦学习方法的重要改进。

Abstract: Deep Learning (DL) has revolutionized medical imaging, yet its adoption is
constrained by data scarcity and privacy regulations, limiting access to
diverse datasets. Federated Learning (FL) enables decentralized training but
suffers from high communication costs and is often restricted to a single
downstream task, reducing flexibility. We propose a data-sharing method via
Differentially Private (DP) generative models. By adopting foundation models,
we extract compact, informative embeddings, reducing redundancy and lowering
computational overhead. Clients collaboratively train a Differentially Private
Conditional Variational Autoencoder (DP-CVAE) to model a global, privacy-aware
data distribution, supporting diverse downstream tasks. Our approach, validated
across multiple feature extractors, enhances privacy, scalability, and
efficiency, outperforming traditional FL classifiers while ensuring
differential privacy. Additionally, DP-CVAE produces higher-fidelity embeddings
than DP-CGAN while requiring $5{\times}$ fewer parameters.

</details>


### [192] [Multi-Agent Reinforcement Learning for Dynamic Pricing in Supply Chains: Benchmarking Strategic Agent Behaviours under Realistically Simulated Market Conditions](https://arxiv.org/abs/2507.02698)
*Thomas Hazenberg,Yao Ma,Seyed Sahand Mohammadi Ziabari,Marijn van Rijswijk*

Main category: cs.LG

TL;DR: 研究探讨如何利用多智能体强化学习（MARL）改进供应链动态定价，发现MARL策略能展现出传统规则无法捕捉的战略行为。


<details>
  <summary>Details</summary>
Motivation: 现有供应链动态定价多基于静态规则，缺乏对市场行为之间交互的考量，本研究旨在解决这一问题。

Method: 引入三种MARL算法（MADDPG、MADQN和QMIX）与基于e-commerce交易数据模拟环境中的静态规则进行对比。

Result: 规则算法公平性最高，但缺乏竞争行为；MARL算法之间，MADQN相对激进，MADDPG在竞争、稳定性与公平性间达到较好平衡。

Conclusion: MARL在动态定价中能引入规则定价无法捕捉的战略行为，为未来动态定价策略提供依据。

Abstract: This study investigates how Multi-Agent Reinforcement Learning (MARL) can
improve dynamic pricing strategies in supply chains, particularly in contexts
where traditional ERP systems rely on static, rule-based approaches that
overlook strategic interactions among market actors. While recent research has
applied reinforcement learning to pricing, most implementations remain
single-agent and fail to model the interdependent nature of real-world supply
chains. This study addresses that gap by evaluating the performance of three
MARL algorithms: MADDPG, MADQN, and QMIX against static rule-based baselines,
within a simulated environment informed by real e-commerce transaction data and
a LightGBM demand prediction model. Results show that rule-based agents achieve
near-perfect fairness (Jain's Index: 0.9896) and the highest price stability
(volatility: 0.024), but they fully lack competitive dynamics. Among MARL
agents, MADQN exhibits the most aggressive pricing behaviour, with the highest
volatility and the lowest fairness (0.5844). MADDPG provides a more balanced
approach, supporting market competition (share volatility: 9.5 pp) while
maintaining relatively high fairness (0.8819) and stable pricing. These
findings suggest that MARL introduces emergent strategic behaviour not captured
by static pricing rules and may inform future developments in dynamic pricing.

</details>


### [193] [Fluid Democracy in Federated Data Aggregation](https://arxiv.org/abs/2507.02710)
*Aditya Vema Reddy Kesari,Krishna Reddy Kesari*

Main category: cs.LG

TL;DR: 此研究提出了一种新的联邦学习协议FedVRD，可以通过识别有用的客户端权重来减少不必要的数据传输费用，同时限制对手的负面影响并最小化成本。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习模型因需要集中式传输所有客户端权重而带来数据传输浪费，因此提出一种通过共识协议选择最有用客户端权重的方法。

Method: 研究采用现有液体民主协议，并与传统协议（如FedAvg）比较；提出新协议Viscous-Retained Democracy，可动态限制对手影响并优化传输成本。

Result: 新协议在同样假设下优于传统方法，并在对抗性视角下避免负面影响，通过动态调整减少对手对权重模型的影响。

Conclusion: 新方法通过高效限制对手影响及减少客户端间不必要的传输成本，有效提高全球模型性能。

Abstract: Federated learning (FL) mechanisms typically require each client to transfer
their weights to a central server, irrespective of how useful they are. In
order to avoid wasteful data transfer costs from clients to the central server,
we propose the use of consensus based protocols to identify a subset of clients
with most useful model weights at each data transfer step. First, we explore
the application of existing fluid democracy protocols to FL from a performance
standpoint, comparing them with traditional one-person-one-vote (also known as
1p1v or FedAvg). We propose a new fluid democracy protocol named
viscous-retained democracy that always does better than 1p1v under the same
assumptions as existing fluid democracy protocols while also not allowing for
influence accumulation. Secondly, we identify weaknesses of fluid democracy
protocols from an adversarial lens in terms of their dependence on topology
and/ or number of adversaries required to negatively impact the global model
weights. To this effect, we propose an algorithm (FedVRD) that dynamically
limits the effect of adversaries while minimizing cost by leveraging the
delegation topology.

</details>


### [194] [A Forget-and-Grow Strategy for Deep Reinforcement Learning Scaling in Continuous Control](https://arxiv.org/abs/2507.02712)
*Zilin Kang,Chenyuan Hu,Yu Luo,Zhecheng Yuan,Ruijie Zheng,Huazhe Xu*

Main category: cs.LG

TL;DR: 本文提出了一种新的深度强化学习算法FoG，通过削弱早期经验影响和动态增加训练参数，改进了现有算法的样本效率和泛化能力，并在连续控制的基准测试中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习方法因过度依赖早期经验而导致样本效率和泛化能力受限，而人类通过遗忘初期记忆克服了这一问题。

Method: 提出了FoG算法，包括经验回放衰减（ER Decay）减少早期经验影响以及网络扩展（Network Expansion）动态增加神经网络参数。

Result: 在四个主要连续控制基准测试的40多个任务中，FoG优于现有最先进的深度强化学习算法，如BRO、SimBa和TD-MPC2。

Conclusion: FoG通过借鉴神经科学的遗忘和成长机制，显著提升了深度强化学习算法的性能。

Abstract: Deep reinforcement learning for continuous control has recently achieved
impressive progress. However, existing methods often suffer from primacy bias,
a tendency to overfit early experiences stored in the replay buffer, which
limits an RL agent's sample efficiency and generalizability. In contrast,
humans are less susceptible to such bias, partly due to infantile amnesia,
where the formation of new neurons disrupts early memory traces, leading to the
forgetting of initial experiences. Inspired by this dual processes of
forgetting and growing in neuroscience, in this paper, we propose Forget and
Grow (FoG), a new deep RL algorithm with two mechanisms introduced. First,
Experience Replay Decay (ER Decay) "forgetting early experience", which
balances memory by gradually reducing the influence of early experiences.
Second, Network Expansion, "growing neural capacity", which enhances agents'
capability to exploit the patterns of existing data by dynamically adding new
parameters during training. Empirical results on four major continuous control
benchmarks with more than 40 tasks demonstrate the superior performance of FoG
against SoTA existing deep RL algorithms, including BRO, SimBa, and TD-MPC2.

</details>


### [195] [A Comprehensive Machine Learning Framework for Micromobility Demand Prediction](https://arxiv.org/abs/2507.02715)
*Omri Porat,Michael Fire,Eran Ben-Elia*

Main category: cs.LG

TL;DR: 本文提出了一种集成空间、时间和网络依赖的新框架，用于提高无桩电动滑板车的需求预测准确性。


<details>
  <summary>Details</summary>
Motivation: 有效管理无桩电动滑板车服务需要精确的需求预测，以优化车队分配和基础设施规划。

Method: 设计并引入了一个整合空间、时间和网络依赖的框架模型，以全面分析小型交通工具的使用需求。

Result: 该框架使需求预测的准确性提高了27%-49%，优于基准模型。

Conclusion: 通过数据驱动的方式支持小型交通工具管理，优化车队分配、降低成本，并促进可持续的城市规划。

Abstract: Dockless e-scooters, a key micromobility service, have emerged as
eco-friendly and flexible urban transport alternatives. These services improve
first and last-mile connectivity, reduce congestion and emissions, and
complement public transport for short-distance travel. However, effective
management of these services depends on accurate demand prediction, which is
crucial for optimal fleet distribution and infrastructure planning. While
previous studies have focused on analyzing spatial or temporal factors in
isolation, this study introduces a framework that integrates spatial, temporal,
and network dependencies for improved micromobility demand forecasting. This
integration enhances accuracy while providing deeper insights into urban
micromobility usage patterns. Our framework improves demand prediction accuracy
by 27 to 49% over baseline models, demonstrating its effectiveness in capturing
micromobility demand patterns. These findings support data-driven micromobility
management, enabling optimized fleet distribution, cost reduction, and
sustainable urban planning.

</details>


### [196] [Hierarchical Multi-Label Contrastive Learning for Protein-Protein Interaction Prediction Across Organisms](https://arxiv.org/abs/2507.02724)
*Shiyi Liu,Buwen Liang,Yuetong Fang,Zixuan Jiang,Renjing Xu*

Main category: cs.LG

TL;DR: 提出HIPPO，一种用于跨物种蛋白质-蛋白质相互作用预测的分层对比学习框架，显著提高了预测性能和零样本跨物种迁移能力。


<details>
  <summary>Details</summary>
Motivation: 通过对比学习桥接异质生物数据的模式，解决现有方法在蛋白质-蛋白质相互作用预测中的不足，特别是在数据稀缺或跨物种的情况下。

Method: 使用分层对比学习框架，同时运用分层对比损失函数以及数据驱动的惩罚机制，学习基于蛋白质序列及其层次属性的多层次生物表示匹配。

Result: 在基准数据集上表现出色，超越现有方法，并展示出在低数据量情况下的鲁棒性，尤其是实现了零样本跨物种迁移，无需重新训练即可对不常见生物有效预测。

Conclusion: HIPPO框架为跨物种PPI预测提供了一种统一的解决方案，尤其是在稀疏或不平衡的多物种数据中，展示了潜在的广泛应用前景。

Abstract: Recent advances in AI for science have highlighted the power of contrastive
learning in bridging heterogeneous biological data modalities. Building on this
paradigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction
across Organisms), a hierarchical contrastive framework for protein-protein
interaction(PPI) prediction, where protein sequences and their hierarchical
attributes are aligned through multi-tiered biological representation matching.
The proposed approach incorporates hierarchical contrastive loss functions that
emulate the structured relationship among functional classes of proteins. The
framework adaptively incorporates domain and family knowledge through a
data-driven penalty mechanism, enforcing consistency between the learned
embedding space and the intrinsic hierarchy of protein functions. Experiments
on benchmark datasets demonstrate that HIPPO achieves state-of-the-art
performance, outperforming existing methods and showing robustness in low-data
regimes. Notably, the model demonstrates strong zero-shot transferability to
other species without retraining, enabling reliable PPI prediction and
functional inference even in less characterized or rare organisms where
experimental data are limited. Further analysis reveals that hierarchical
feature fusion is critical for capturing conserved interaction determinants,
such as binding motifs and functional annotations. This work advances
cross-species PPI prediction and provides a unified framework for interaction
prediction in scenarios with sparse or imbalanced multi-species data.

</details>


### [197] [Classification by Separating Hypersurfaces: An Entropic Approach](https://arxiv.org/abs/2507.02732)
*Argimiro Arratia,Mahmoud El Daou,Henryk Gzyl*

Main category: cs.LG

TL;DR: 我们提出一种新方法，通过最小化基于熵的函数，在N维有界超立方体中搜索参数向量，以及通过正向向量在高维空间进行分类。该方法可扩展到多项式曲面，用于复杂决策边界的划分。


<details>
  <summary>Details</summary>
Motivation: 解决经典分类问题：寻找超平面将两类点分隔开，并提高在处理线性和非线性分类任务中的效率和灵活性。

Method: 通过最小化基于熵的函数，在以原点为中心的有界N维超立方体中搜索参数向量，并结合正向向量，扩展到多项式曲面，实现复杂数据点的分隔。

Result: 数值实验表明，该方法在处理各种分类任务（包括线性和非线性可分性）时表现出效率和多样性。

Conclusion: 提出的方法较传统线性或二次优化技术有更强的鲁棒性和通用性，为分类问题提供了新思路。

Abstract: We consider the following classification problem: Given a population of
individuals characterized by a set of attributes represented as a vector in
${\mathbb R}^N$, the goal is to find a hyperplane in ${\mathbb R}^N$ that
separates two sets of points corresponding to two distinct classes. This
problem, with a history dating back to the perceptron model, remains central to
machine learning. In this paper we propose a novel approach by searching for a
vector of parameters in a bounded $N$-dimensional hypercube centered at the
origin and a positive vector in ${\mathbb R}^M$, obtained through the
minimization of an entropy-based function defined over the space of unknown
variables. The method extends to polynomial surfaces, allowing the separation
of data points by more complex decision boundaries. This provides a robust
alternative to traditional linear or quadratic optimization techniques, such as
support vector machines and gradient descent. Numerical experiments demonstrate
the efficiency and versatility of the method in handling diverse classification
tasks, including linear and non-linear separability.

</details>


### [198] [Fast and Simplex: 2-Simplicial Attention in Triton](https://arxiv.org/abs/2507.02754)
*Aurko Roy,Timothy Chou,Sai Surya Duvvuri,Sijia Chen,Jiecao Yu,Xiaodong Wang,Manzil Zaheer,Rohan Anil*

Main category: cs.LG

TL;DR: 研究表明，2-simplicial Transformer在数学、编程、推理和逻辑任务上的token效率优于标准Transformer，可调整知识和推理任务中的扩展规律指数。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型依赖海量数据集，假设其计算受限不再成立，因此需要研究提高token效率的新型架构。

Method: 提出并研究2-simplicial Transformer，其通过Triton内核实现三线性注意力函数的高效计算，量化其在各种任务中的性能提升。

Result: 2-simplicial Transformer在固定token预算下，在多个任务中优于标准Transformer，并改变了知识和推理任务的扩展规律指数。

Conclusion: 2-simplicial Transformer在计算效率和任务性能上为大型语言模型提供了一种优越的解决方案，适合现代数据有限的场景。

Abstract: Recent work has shown that training loss scales as a power law with both
model size and the number of tokens, and that achieving compute-optimal models
requires scaling model size and token count together. However, these scaling
laws assume an infinite supply of data and apply primarily in compute-bound
settings. As modern large language models increasingly rely on massive
internet-scale datasets, the assumption that they are compute-bound is becoming
less valid. This shift highlights the need for architectures that prioritize
token efficiency.
  In this work, we investigate the use of the 2-simplicial Transformer, an
architecture that generalizes standard dot-product attention to trilinear
functions through an efficient Triton kernel implementation. We demonstrate
that the 2-simplicial Transformer achieves better token efficiency than
standard Transformers: for a fixed token budget, similarly sized models
outperform their dot-product counterparts on tasks involving mathematics,
coding, reasoning, and logic. We quantify these gains by demonstrating that
$2$-simplicial attention changes the exponent in the scaling laws for knowledge
and reasoning tasks compared to dot product attention.

</details>


### [199] [Contextual Online Pricing with (Biased) Offline Data](https://arxiv.org/abs/2507.02762)
*Yixuan Zhang,Ruihao Zhu,Qiaomin Xie*

Main category: cs.LG

TL;DR: 研究偏置离线数据下的上下文在线定价问题，提出了最优的分实例遗憾界，并推广至有偏离线数据的随机线性强盗中。


<details>
  <summary>Details</summary>
Motivation: 解决偏置离线数据下的在线定价问题，同时获得紧凑的遗憾界结果。

Method: 提出基于不确定性乐观原则(OFU)的算法，分析实例相关的量化指标对统计复杂性的影响，并设计出适应偏置未知情况的鲁棒版本。

Result: 得到了对偏置离线数据上下文定价问题的最优遗憾界，同时将技术推广至相关领域。

Conclusion: 本文首次提供了偏置离线数据上下文定价的紧凑遗憾界，并证明其在多种情景下的适用性，改进了现有方法。

Abstract: We study contextual online pricing with biased offline data. For the scalar
price elasticity case, we identify the instance-dependent quantity $\delta^2$
that measures how far the offline data lies from the (unknown) online optimum.
We show that the time length $T$, bias bound $V$, size $N$ and dispersion
$\lambda_{\min}(\hat{\Sigma})$ of the offline data, and $\delta^2$ jointly
determine the statistical complexity. An Optimism-in-the-Face-of-Uncertainty
(OFU) policy achieves a minimax-optimal, instance-dependent regret bound
$\tilde{\mathcal{O}}\big(d\sqrt{T} \wedge (V^2T +
\frac{dT}{\lambda_{\min}(\hat{\Sigma}) + (N \wedge T) \delta^2})\big)$. For
general price elasticity, we establish a worst-case, minimax-optimal rate
$\tilde{\mathcal{O}}\big(d\sqrt{T} \wedge (V^2T + \frac{dT
}{\lambda_{\min}(\hat{\Sigma})})\big)$ and provide a generalized OFU algorithm
that attains it. When the bias bound $V$ is unknown, we design a robust variant
that always guarantees sub-linear regret and strictly improves on purely online
methods whenever the exact bias is small. These results deliver the first tight
regret guarantees for contextual pricing in the presence of biased offline
data. Our techniques also transfer verbatim to stochastic linear bandits with
biased offline data, yielding analogous bounds.

</details>


### [200] [Understanding and Improving Length Generalization in Recurrent Models](https://arxiv.org/abs/2507.02782)
*Ricardo Buitrago Ruiz,Albert Gu*

Main category: cs.LG

TL;DR: 递归模型可以处理任意长的序列，但训练时的状态空间限制会导致长度泛化失败。本研究提出简单训练调整方法可显著改进。


<details>
  <summary>Details</summary>
Motivation: 递归模型在长序列情景下性能下降，无法成功进行长度泛化，需要找到原因并改进。

Method: 通过实证和理论分析验证状态空间限制假设，并提出训练方法改进，如引入高斯噪声初始化状态或使用其他输入序列的最终状态作为初始状态。

Result: 通过不到预训练预算0.1%的后训练步骤，这些改进使模型能够在长度远超训练语境的情况下进行泛化（如从2k到128k），并在长序列任务中表现更优。

Conclusion: 本文提出的简单高效的训练调整方法显著改进了递归模型的长度泛化能力。

Abstract: Recently, recurrent models such as state space models and linear attention
have become popular due to their linear complexity in the sequence length.
Thanks to their recurrent nature, in principle they can process arbitrarily
long sequences, but their performance sometimes drops considerably beyond their
training context lengths-i.e. they fail to length generalize. In this work, we
provide comprehensive empirical and theoretical analysis to support the
unexplored states hypothesis, which posits that models fail to length
generalize when during training they are only exposed to a limited subset of
the distribution of all attainable states (i.e. states that would be attained
if the recurrence was applied to long sequences). Furthermore, we investigate
simple training interventions that aim to increase the coverage of the states
that the model is trained on, e.g. by initializing the state with Gaussian
noise or with the final state of a different input sequence. With only 500
post-training steps ($\sim 0.1\%$ of the pre-training budget), these
interventions enable length generalization for sequences that are orders of
magnitude longer than the training context (e.g. $2k\longrightarrow 128k$) and
show improved performance in long context tasks, thus presenting a simple and
efficient way to enable robust length generalization in general recurrent
models.

</details>


### [201] [In-Training Multicalibrated Survival Analysis for Healthcare via Constrained Optimization](https://arxiv.org/abs/2507.02807)
*Thiti Suttaket,Stanley Kok*

Main category: cs.LG

TL;DR: 此论文提出了一种名为GRADUATE的模型，旨在提升生存分析模型在所有子群体中的校准性能，同时保持较高的判别力。


<details>
  <summary>Details</summary>
Motivation: 当前生存分析模型通常只在总体水平上校准，可能在某些少数群体中校准不佳，从而导致临床决策错误。文中致力于解决此问题。

Method: 提出并开发了GRADUATE模型，将多校准问题建模为一个有约束的优化问题，在训练过程中同时优化校准性和判别力。并证明该优化方法能高概率得出近似最优、可行解。

Result: 基于真实世界临床数据集的实验表明，GRADUATE性能优于现有的先进基线方法。

Conclusion: GRADUATE模型通过数学证明和实验验证，展示了其能在多子群体间实现良好的校准和平衡的判别性能，从而改进生存分析模型。

Abstract: Survival analysis is an important problem in healthcare because it models the
relationship between an individual's covariates and the onset time of an event
of interest (e.g., death). It is important for survival models to be
well-calibrated (i.e., for their predicted probabilities to be close to
ground-truth probabilities) because badly calibrated systems can result in
erroneous clinical decisions. Existing survival models are typically calibrated
at the population level only, and thus run the risk of being poorly calibrated
for one or more minority subpopulations. We propose a model called GRADUATE
that achieves multicalibration by ensuring that all subpopulations are
well-calibrated too. GRADUATE frames multicalibration as a constrained
optimization problem, and optimizes both calibration and discrimination
in-training to achieve a good balance between them. We mathematically prove
that the optimization method used yields a solution that is both near-optimal
and feasible with high probability. Empirical comparisons against
state-of-the-art baselines on real-world clinical datasets demonstrate
GRADUATE's efficacy. In a detailed analysis, we elucidate the shortcomings of
the baselines vis-a-vis GRADUATE's strengths.

</details>


### [202] [Replicable Distribution Testing](https://arxiv.org/abs/2507.02814)
*Ilias Diakonikolas,Jingyi Gao,Daniel Kane,Sihan Liu,Christopher Ye*

Main category: cs.LG

TL;DR: 作者研究了在算法可重复性框架下的分布测试问题，发展了新的测试算法并提供了关于样本复杂度的下界。


<details>
  <summary>Details</summary>
Motivation: 在如今的研究中，可重复性已成为一个重要问题，研究分布测试的可重复性有助于为该领域奠定更强的理论基础。

Method: 提出了适用于测试分布接近性和独立性的可重复算法，并开发出新方法来证明样本复杂度下界。

Result: 提出了近似最优的可重复性一致性测试样本复杂度下界，同时解决了前人工作中的未解问题。

Conclusion: 该研究为可重复分布测试提供了新算法和下界证明方法，对分布测试及相关领域有重要意义。

Abstract: We initiate a systematic investigation of distribution testing in the
framework of algorithmic replicability. Specifically, given independent samples
from a collection of probability distributions, the goal is to characterize the
sample complexity of replicably testing natural properties of the underlying
distributions. On the algorithmic front, we develop new replicable algorithms
for testing closeness and independence of discrete distributions. On the lower
bound front, we develop a new methodology for proving sample complexity lower
bounds for replicable testing that may be of broader interest. As an
application of our technique, we establish near-optimal sample complexity lower
bounds for replicable uniformity testing -- answering an open question from
prior work -- and closeness testing.

</details>


### [203] [LLM-Driven Treatment Effect Estimation Under Inference Time Text Confounding](https://arxiv.org/abs/2507.02843)
*Yuchen Ma,Dennis Frauen,Jonas Schweisthal,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 本文探讨了在医疗治疗效果估计中，训练数据和推断数据的差异引发的偏差问题，并提出利用大语言模型和双稳健学习器的框架解决该问题。


<details>
  <summary>Details</summary>
Motivation: 由于个性化医疗决策需要精准的治疗效果估计，而数据从训练到推断阶段的表示形式不一致（如临床文本描述）造成偏差问题亟待解决。

Method: 提出了一个新框架，结合大型语言模型和定制的双稳健学习器，以减轻推断时文本混淆带来的偏差问题。

Result: 实验结果表明，该框架在真实应用中有效地减轻了文本混淆问题并提高了治疗效果估计的准确性。

Conclusion: 研究强调了解决训练与推断阶段数据表示差异的重要性，并且提出的框架显著改善了针对文本混淆问题的处理效果。

Abstract: Estimating treatment effects is crucial for personalized decision-making in
medicine, but this task faces unique challenges in clinical practice. At
training time, models for estimating treatment effects are typically trained on
well-structured medical datasets that contain detailed patient information.
However, at inference time, predictions are often made using textual
descriptions (e.g., descriptions with self-reported symptoms), which are
incomplete representations of the original patient information. In this work,
we make three contributions. (1) We show that the discrepancy between the data
available during training time and inference time can lead to biased estimates
of treatment effects. We formalize this issue as an inference time text
confounding problem, where confounders are fully observed during training time
but only partially available through text at inference time. (2) To address
this problem, we propose a novel framework for estimating treatment effects
that explicitly accounts for inference time text confounding. Our framework
leverages large language models together with a custom doubly robust learner to
mitigate biases caused by the inference time text confounding. (3) Through a
series of experiments, we demonstrate the effectiveness of our framework in
real-world applications.

</details>


### [204] [MvHo-IB: Multi-View Higher-Order Information Bottleneck for Brain Disorder Diagnosis](https://arxiv.org/abs/2507.02847)
*Kunyu Zhang,Qiang Li,Shujian Yu*

Main category: cs.LG

TL;DR: 本文提出MvHo-IB，一个融合成对交互和高阶交互的新型多视角学习框架，用于fMRI数据的诊断决策，显著提高了性能。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习在处理fMRI数据时对高阶交互利用不足，因此需要新方法提升诊断准确性。

Method: 提出了MvHo-IB框架，结合信息理论的O-information与矩阵基的Renyi熵估算器，设计了专用的Brain3DCNN编码器，并引入多视角学习信息瓶颈目标来增强表示学习。

Result: 在三个fMRI数据集上，MvHo-IB实现了最新的性能，显著优于包括超图方法在内的已有方法。

Conclusion: MvHo-IB提供了一种有效方法来捕获和利用fMRI中的高阶交互，显著提升了诊断准确性并具有实际可用性。

Abstract: Recent evidence suggests that modeling higher-order interactions (HOIs) in
functional magnetic resonance imaging (fMRI) data can enhance the diagnostic
accuracy of machine learning systems. However, effectively extracting and
utilizing HOIs remains a significant challenge. In this work, we propose
MvHo-IB, a novel multi-view learning framework that integrates both pairwise
interactions and HOIs for diagnostic decision-making, while automatically
compressing task-irrelevant redundant information. MvHo-IB introduces several
key innovations: (1) a principled method that combines O-information from
information theory with a matrix-based Renyi alpha-order entropy estimator to
quantify and extract HOIs, (2) a purpose-built Brain3DCNN encoder to
effectively utilize these interactions, and (3) a new multi-view learning
information bottleneck objective to enhance representation learning.
Experiments on three benchmark fMRI datasets demonstrate that MvHo-IB achieves
state-of-the-art performance, significantly outperforming previous methods,
including recent hypergraph-based techniques. The implementation of MvHo-IB is
available at https://github.com/zky04/MvHo-IB.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [205] [Quality Diversity Genetic Programming for Learning Scheduling Heuristics](https://arxiv.org/abs/2507.02235)
*Meng Xu,Frank Neumann,Aneta Neumann,Yew Soon Ong*

Main category: cs.NE

TL;DR: 本文提出了一个面向动态调度问题的新型质量-多样性（QD）优化框架，通过构建行为地图来促进生成高性能且多样化的调度启发式解法。


<details>
  <summary>Details</summary>
Motivation: 解决动态组合优化问题中缺乏高效、多样化解决方案的需求，并弥补现有QD算法理论研究不足。

Method: 通过连接启发式基因型与其行为来创建QD地图，从而可视化解决方案空间，以支持发现和维持多样化的调度启发式方案，并在固定与动态变量的训练实例中测试方法效果。

Result: 实验表明所提出的QD地图框架能随时间演变，并清晰展示多样化解决方案的分布情况。

Conclusion: 该方法展示了在动态组合优化领域中增强QD算法实用性和学习过程的潜力，可进一步拓展相关研究前景。

Abstract: Real-world optimization often demands diverse, high-quality solutions.
Quality-Diversity (QD) optimization is a multifaceted approach in evolutionary
algorithms that aims to generate a set of solutions that are both
high-performing and diverse. QD algorithms have been successfully applied
across various domains, providing robust solutions by exploring diverse
behavioral niches. However, their application has primarily focused on static
problems, with limited exploration in the context of dynamic combinatorial
optimization problems. Furthermore, the theoretical understanding of QD
algorithms remains underdeveloped, particularly when applied to learning
heuristics instead of directly learning solutions in complex and dynamic
combinatorial optimization domains, which introduces additional challenges.
This paper introduces a novel QD framework for dynamic scheduling problems. We
propose a map-building strategy that visualizes the solution space by linking
heuristic genotypes to their behaviors, enabling their representation on a QD
map. This map facilitates the discovery and maintenance of diverse scheduling
heuristics. Additionally, we conduct experiments on both fixed and dynamically
changing training instances to demonstrate how the map evolves and how the
distribution of solutions unfolds over time. We also discuss potential future
research directions that could enhance the learning process and broaden the
applicability of QD algorithms to dynamic combinatorial optimization
challenges.

</details>


### [206] [Tracing the Interactions of Modular CMA-ES Configurations Across Problem Landscapes](https://arxiv.org/abs/2507.02331)
*Ana Nikolikj,Mario Andrés Muñoz,Eva Tuba,Tome Eftimov*

Main category: cs.NE

TL;DR: 该研究利用算法脚印概念分析算法配置与问题特征之间的关系，在不同维度的测试中揭示了配置间的性能差异及影响因素。


<details>
  <summary>Details</summary>
Motivation: 探讨为何相同算法的不同配置在解决问题时会表现出不同的性能，以及明确哪些问题特性影响了这些性能结果。

Method: 通过计算CMA-ES算法的六种模块化变体的性能脚印，在5维和30维设置下的24个BBOB基准问题上进行评估和分析。

Result: 研究发现配置间表现的共性与差异，验证了算法脚印在提升解释性和指导配置选择方面的效果。

Conclusion: 算法脚印能够有效帮助理解配置对性能的影响，并指导算法配置决策。

Abstract: This paper leverages the recently introduced concept of algorithm footprints
to investigate the interplay between algorithm configurations and problem
characteristics. Performance footprints are calculated for six modular variants
of the CMA-ES algorithm (modCMA), evaluated on 24 benchmark problems from the
BBOB suite, across two-dimensional settings: 5-dimensional and 30-dimensional.
These footprints provide insights into why different configurations of the same
algorithm exhibit varying performance and identify the problem features
influencing these outcomes. Our analysis uncovers shared behavioral patterns
across configurations due to common interactions with problem properties, as
well as distinct behaviors on the same problem driven by differing problem
features. The results demonstrate the effectiveness of algorithm footprints in
enhancing interpretability and guiding configuration choices.

</details>


### [207] [ClustOpt: A Clustering-based Approach for Representing and Visualizing the Search Dynamics of Numerical Metaheuristic Optimization Algorithms](https://arxiv.org/abs/2507.02337)
*Gjorgjina Cenikj,Gašper Petelin,Tome Eftimov*

Main category: cs.NE

TL;DR: 本研究提出了一种聚类方法和可视化工具，用于解析数值元启发优化算法的搜索动态及算法间的比较。


<details>
  <summary>Details</summary>
Motivation: 当前的可视化技术难以解析高维或复杂解空间中的搜索行为，特别是在算法的动态结构表现上。

Method: 提出了一种聚类解候选点的新表示形式和动态可视化方法，并设计了算法稳定性和算法相似性两个新指标，用于量化搜索轨迹的一致性和算法间的相似性。

Result: 通过对十种数值元启发式算法的应用，揭示了它们的稳定性和比较行为，为其搜索动态提供了更深的理解。

Conclusion: 新的聚类与可视化方法有助于更直观地解析元启发优化算法的行为，有利于未来算法的开发与改进。

Abstract: Understanding the behavior of numerical metaheuristic optimization algorithms
is critical for advancing their development and application. Traditional
visualization techniques, such as convergence plots, trajectory mapping, and
fitness landscape analysis, often fall short in illustrating the structural
dynamics of the search process, especially in high-dimensional or complex
solution spaces. To address this, we propose a novel representation and
visualization methodology that clusters solution candidates explored by the
algorithm and tracks the evolution of cluster memberships across iterations,
offering a dynamic and interpretable view of the search process. Additionally,
we introduce two metrics - algorithm stability and algorithm similarity- to
quantify the consistency of search trajectories across runs of an individual
algorithm and the similarity between different algorithms, respectively. We
apply this methodology to a set of ten numerical metaheuristic algorithms,
revealing insights into their stability and comparative behaviors, thereby
providing a deeper understanding of their search dynamics.

</details>


### [208] [An Experimental Approach for Running-Time Estimation of Multi-objective Evolutionary Algorithms in Numerical Optimization](https://arxiv.org/abs/2507.02372)
*Han Huang,Tianyu Wang,Chaoda Peng,Tongli He,Zhifeng Hao*

Main category: cs.NE

TL;DR: 本文提出了一种无需算法或问题简化的实验方法，用于估计多目标进化算法(MOEAs)在数值优化问题中的运行时间上界。


<details>
  <summary>Details</summary>
Motivation: 尽管在组合优化方面取得了显著的理论进展，但现有针对数值优化运行时间分析的研究多依赖简化算法或问题，限制了实际应用的适用性。

Method: 本文使用基于平均增益的模型，通过倒代生成距离(Inverted Generational Distance)指标表征算法的进展。结合统计方法，应对MOEAs的随机特性，并设计了动态调整采样密度的自适应采样方法，以提高运行时间估计的准确性。

Result: 实验结果表明，该方法可在无需算法或问题简化的情况下有效估计运行时间上界，并在多个典型MOEAs和基准测试套件上进行了验证。

Conclusion: 本研究为数值优化中的多目标进化算法提供了实用的实验分析方法，补充了相关理论研究，并提供了网络实现以便推广。

Abstract: Multi-objective evolutionary algorithms (MOEAs) have become essential tools
for solving multi-objective optimization problems (MOPs), making their running
time analysis crucial for assessing algorithmic efficiency and guiding
practical applications. While significant theoretical advances have been
achieved for combinatorial optimization, existing studies for numerical
optimization primarily rely on algorithmic or problem simplifications, limiting
their applicability to real-world scenarios. To address this gap, we propose an
experimental approach for estimating upper bounds on the running time of MOEAs
in numerical optimization without simplification assumptions. Our approach
employs an average gain model that characterizes algorithmic progress through
the Inverted Generational Distance metric. To handle the stochastic nature of
MOEAs, we use statistical methods to estimate the probabilistic distribution of
gains. Recognizing that gain distributions in numerical optimization exhibit
irregular patterns with varying densities across different regions, we
introduce an adaptive sampling method that dynamically adjusts sampling density
to ensure accurate surface fitting for running time estimation. We conduct
comprehensive experiments on five representative MOEAs (NSGA-II, MOEA/D,
AR-MOEA, AGEMOEA-II, and PREA) using the ZDT and DTLZ benchmark suites. The
results demonstrate the effectiveness of our approach in estimating upper
bounds on the running time without requiring algorithmic or problem
simplifications. Additionally, we provide a web-based implementation to
facilitate broader adoption of our methodology. This work provides a practical
complement to theoretical research on MOEAs in numerical optimization.

</details>


### [209] [Running-time Analysis of ($μ+λ$) Evolutionary Combinatorial Optimization Based on Multiple-gain Estimation](https://arxiv.org/abs/2507.02381)
*Min Huang,Pengxiang Chen,Han Huang,Tongli He,Yushan Zhang,Zhifeng Hao*

Main category: cs.NE

TL;DR: 本文提出了一种多个增益模型，用于分析$(\mu+\lambda)$进化算法 (EA)在组合优化问题中的运行时间并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的关于$(\mu+\lambda)$EA在组合优化问题中的理论结果相较于简单伪布尔问题要少，为解决这一研究空白，对其运行时间进行系统分析。

Method: 通过提出一个改进的平均增益模型，该模型利用适合条件下的适应值差漂移方法，提供了一个框架来估计随机过程在平均和最坏情况下的期望首次命中时间，从而获得更严格的时间复杂度上界。

Result: 得到了$(\mu+\lambda)$EA在包含有利相关权重的背包问题、一般$k$-MAX-SAT问题和旅行商问题中的运行时间复杂度上界，并通过实验验证了理论结果的正确性。

Conclusion: 所提出的多个增益模型是分析$(\mu+\lambda)$EA在组合优化问题中的运行时间的有效工具，实验结果支持理论推导。

Abstract: The running-time analysis of evolutionary combinatorial optimization is a
fundamental topic in evolutionary computation. However, theoretical results
regarding the $(\mu+\lambda)$ evolutionary algorithm (EA) for combinatorial
optimization problems remain relatively scarce compared to those for simple
pseudo-Boolean problems. This paper proposes a multiple-gain model to analyze
the running time of EAs for combinatorial optimization problems. The proposed
model is an improved version of the average gain model, which is a
fitness-difference drift approach under the sigma-algebra condition to estimate
the running time of evolutionary numerical optimization. The improvement yields
a framework for estimating the expected first hitting time of a stochastic
process in both average-case and worst-case scenarios. It also introduces novel
running-time results of evolutionary combinatorial optimization, including two
tighter time complexity upper bounds than the known results in the case of
($\mu+\lambda$) EA for the knapsack problem with favorably correlated weights,
a closed-form expression of time complexity upper bound in the case of
($\mu+\lambda$) EA for general $k$-MAX-SAT problems and a tighter time
complexity upper bounds than the known results in the case of ($\mu+\lambda$)
EA for the traveling salesperson problem. Experimental results indicate that
the practical running time aligns with the theoretical results, verifying that
the multiple-gain model is an effective tool for running-time analysis of
($\mu+\lambda$) EA for combinatorial optimization problems.

</details>
