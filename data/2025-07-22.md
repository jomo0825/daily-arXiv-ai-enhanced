<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 141]
- [cs.CL](#cs.CL) [Total: 86]
- [cs.AI](#cs.AI) [Total: 58]
- [cs.LG](#cs.LG) [Total: 128]
- [cs.NE](#cs.NE) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Comparative Analysis of Algorithms for the Fitting of Tessellations to 3D Image Data](https://arxiv.org/abs/2507.14268)
*Andreas Alpers,Orkun Furat,Christian Jung,Matthias Neumann,Claudia Redenbach,Aigerim Saken,Volker Schmidt*

Main category: cs.CV

TL;DR: 该研究比较了不同算法用于拟合三维图像材料结构的性能，重点是Voronoi、多种程序优化方法以及GBPDs的适用性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在优化三维图像材料（如多晶体、泡沫）的结构拟合，提高模型精度并提供不同应用的指导。

Method: 评估了线性与非线性编程、基于交叉熵法的随机优化法以及梯度下降法，以生成Voronoi、Laguerre及通用平衡幂图，利用真实数据对拟合质量进行度量。

Result: 研究发现模型复杂度、优化例程复杂度及逼近质量存在权衡，并基于数据特点和需求提供了选取指导。

Conclusion: 该研究为实现最佳拟合算法的选择提供了依据，同时揭示了方法与数据特性间的关系，为材料科学图像数据建模提供了新见解。

Abstract: This paper presents a comparative analysis of algorithmic strategies for
fitting tessellation models to 3D image data of materials such as polycrystals
and foams. In this steadily advancing field, we review and assess
optimization-based methods -- including linear and nonlinear programming,
stochastic optimization via the cross-entropy method, and gradient descent --
for generating Voronoi, Laguerre, and generalized balanced power diagrams
(GBPDs) that approximate voxelbased grain structures. The quality of fit is
evaluated on real-world datasets using discrepancy measures that quantify
differences in grain volume, surface area, and topology. Our results highlight
trade-offs between model complexity, the complexity of the optimization
routines involved, and the quality of approximation, providing guidance for
selecting appropriate methods based on data characteristics and application
needs.

</details>


### [2] [Semantic Segmentation based Scene Understanding in Autonomous Vehicles](https://arxiv.org/abs/2507.14303)
*Ehsan Rassekh*

Main category: cs.CV

TL;DR: 本研究提出了几种高效模型用于通过语义分割实现场景理解，基于BDD100k数据集进行研究，结果表明选择合适的主干网络对提升语义分割性能有重要作用。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的发展，机器可以在许多复杂任务中替代人类专家，特别是在自动驾驶领域中对场景理解的需求日益增长。

Method: 研究中设计了几种模型，通过结合多种主干网络作为编码器对语义分割进行研究，采用BDD100k数据集进行实验，并通过精度、平均交并比和损失函数进行分析和评估。

Result: 实验结果表明，选择合适的主干网络提高了模型的语义分割性能。这不仅优化了模型在精度、平均交并比和损失函数上的表现，也提升了场景理解能力。

Conclusion: 合适的主干网络是优化语义分割模型性能的关键，对促进自动驾驶领域的场景理解具有重要意义。

Abstract: In recent years, the concept of artificial intelligence (AI) has become a
prominent keyword because it is promising in solving complex tasks. The need
for human expertise in specific areas may no longer be needed because machines
have achieved successful results using artificial intelligence and can make the
right decisions in critical situations. This process is possible with the help
of deep learning (DL), one of the most popular artificial intelligence
technologies. One of the areas in which the use of DL is used is in the
development of self-driving cars, which is very effective and important. In
this work, we propose several efficient models to investigate scene
understanding through semantic segmentation. We use the BDD100k dataset to
investigate these models. Another contribution of this work is the usage of
several Backbones as encoders for models. The obtained results show that
choosing the appropriate backbone has a great effect on the performance of the
model for semantic segmentation. Better performance in semantic segmentation
allows us to understand better the scene and the environment around the agent.
In the end, we analyze and evaluate the proposed models in terms of accuracy,
mean IoU, and loss function, and the results show that these metrics are
improved.

</details>


### [3] [CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation](https://arxiv.org/abs/2507.14312)
*Marc Lafon,Gustavo Adolfo Vargas Hakim,Clément Rambour,Christian Desrosier,Nicolas Thome*

Main category: cs.CV

TL;DR: 提出了CLIPTTA，一种用于视觉语言模型测试时自适应的新方法，在75个数据集上的测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（如CLIP）在无标注数据的测试时自适应任务中表现局限，主要原因是目标函数与模型预训练目标的不一致。

Method: 设计了一种新的梯度优化测试时自适应方法CLIPTTA，采用与CLIP预训练目标一致的软对比损失，并引入批量感知设计和异常对比曝光损失（OCE）以改进数据分布检测。

Result: 在75个包含多种分布偏移的数据集上，CLIPTTA性能超越了基于熵的目标函数，并在多个数据集上优于现有的先进方法，稳定性更佳。

Conclusion: CLIPTTA增强了视觉语言模型在分布偏移下的泛化能力，并为开放集场景中的OOD检测提供了有效方案。

Abstract: Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities
but often fail to generalize under distribution shifts. Test-time adaptation
(TTA) allows models to update at inference time without labeled data, typically
via entropy minimization. However, this objective is fundamentally misaligned
with the contrastive image-text training of VLMs, limiting adaptation
performance and introducing failure modes such as pseudo-label drift and class
collapse. We propose CLIPTTA, a new gradient-based TTA method for
vision-language models that leverages a soft contrastive loss aligned with
CLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's
gradients, showing how its batch-aware design mitigates the risk of collapse.
We further extend CLIPTTA to the open-set setting, where both in-distribution
(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier
Contrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75
datasets spanning diverse distribution shifts, CLIPTTA consistently outperforms
entropy-based objectives and is highly competitive with state-of-the-art TTA
methods, outperforming them on a large number of datasets and exhibiting more
stable performance across diverse shifts.

</details>


### [4] [A Hidden Stumbling Block in Generalized Category Discovery: Distracted Attention](https://arxiv.org/abs/2507.14315)
*Qiyu Xu,Zhanxuan Hu,Yu Duan,Ercheng Pei,Yonghang Tai*

Main category: cs.CV

TL;DR: 该论文探讨了GCD任务中存在的注意力分散问题，并提出了一个轻量且高效的注意力聚焦模块AF来解决此问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在解决GCD任务时往往忽略了因模型关注任务无关背景导致的次优特征提取问题的存在。

Method: 提出了Attention Focusing (AF) 自适应机制，包含两个主要组件：Token Importance Measurement (TIME) 和 Token Adaptive Pruning (TAP)，用以衡量并裁剪非信息化的token。

Result: 将AF应用于SimGCD方法中，可在几乎无计算开销的前提下使性能提升高达15.4%。

Conclusion: 通过AF机制有效减少了模型对背景区域的分散注意力问题，提升了GCD任务的性能，同时实现了与现有方法的无缝集成，并提供了代码以供复现。

Abstract: Generalized Category Discovery (GCD) aims to classify unlabeled data from
both known and unknown categories by leveraging knowledge from labeled known
categories. While existing methods have made notable progress, they often
overlook a hidden stumbling block in GCD: distracted attention. Specifically,
when processing unlabeled data, models tend to focus not only on key objects in
the image but also on task-irrelevant background regions, leading to suboptimal
feature extraction. To remove this stumbling block, we propose Attention
Focusing (AF), an adaptive mechanism designed to sharpen the model's focus by
pruning non-informative tokens. AF consists of two simple yet effective
components: Token Importance Measurement (TIME) and Token Adaptive Pruning
(TAP), working in a cascade. TIME quantifies token importance across multiple
scales, while TAP prunes non-informative tokens by utilizing the multi-scale
importance scores provided by TIME. AF is a lightweight, plug-and-play module
that integrates seamlessly into existing GCD methods with minimal computational
overhead. When incorporated into one prominent GCD method, SimGCD, AF achieves
up to 15.4% performance improvement over the baseline with minimal
computational overhead. The implementation code is provided in
https://github.com/Afleve/AFGCD.

</details>


### [5] [Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution](https://arxiv.org/abs/2507.14367)
*Weiming Ren,Raghav Goyal,Zhiming Hu,Tristan Ty Aumentado-Armstrong,Iqbal Mohomed,Alex Levinshtein*

Main category: cs.CV

TL;DR: 论文研究生成型超分辨率模型中可感知的伪像(幻觉)问题，并提出一种基于多模态大语言模型的评估与缓解方法。


<details>
  <summary>Details</summary>
Motivation: 解决生成型超分辨率模型中伪像（幻觉）问题，这些伪像无法通过现有图像指标或质量模型进行良好表征，且影响了模型的实际应用。

Method: 通过多模态大语言模型构建能评估伪像的提示，生成“幻觉分数（Hallucination Score）”，并基于深度特征距离作为模型调整的差分奖励函数，减轻伪像。

Result: 提出的幻觉分数与人类评估高度一致，能为现有超分辨率模型图像评估指标提供补充；并验证了某些深度特征距离与幻觉分数的强关联性。

Conclusion: 研究破解了生成型超分辨率模型的伪像问题，提出新的评估与调优方法，为进一步改进模型提供了新思路。

Abstract: Generative super-resolution (GSR) currently sets the state-of-the-art in
terms of perceptual image quality, overcoming the "regression-to-the-mean" blur
of prior non-generative models. However, from a human perspective, such models
do not fully conform to the optimal balance between quality and fidelity.
Instead, a different class of artifacts, in which generated details fail to
perceptually match the low resolution image (LRI) or ground-truth image (GTI),
is a critical but under studied issue in GSR, limiting its practical
deployments. In this work, we focus on measuring, analyzing, and mitigating
these artifacts (i.e., "hallucinations"). We observe that hallucinations are
not well-characterized with existing image metrics or quality models, as they
are orthogonal to both exact fidelity and no-reference quality. Instead, we
take advantage of a multimodal large language model (MLLM) by constructing a
prompt that assesses hallucinatory visual elements and generates a
"Hallucination Score" (HS). We find that our HS is closely aligned with human
evaluations, and also provides complementary insights to prior image metrics
used for super-resolution (SR) models. In addition, we find certain deep
feature distances have strong correlations with HS. We therefore propose to
align the GSR models by using such features as differentiable reward functions
to mitigate hallucinations.

</details>


### [6] [DUSTrack: Semi-automated point tracking in ultrasound videos](https://arxiv.org/abs/2507.14368)
*Praneeth Namburi,Roger Pallarès-López,Jessica Rosendorf,Duarte Folgado,Brian W. Anthony*

Main category: cs.CV

TL;DR: 本研究介绍了DUSTrack，一种基于深度学习和光流的超声点追踪工具，用于解决传统超声点追踪的噪声和运动复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 由于B模式超声的斑点噪声、低边缘对比度以及出平面运动，追踪组织运动面临难题，影响组织动力学的定量分析需求。

Method: DUSTrack结合深度学习与光流方法，通过半自动框架实现高质量点追踪，同时引入光流过滤技术减少高频噪声，并配备用户界面以支持数据生成和模型迭代优化。

Result: DUSTrack在追踪精度上优于零样本点追踪器，与特定方法表现相当，并展示了在多种生物医学场景下的通用性和高效性。

Conclusion: 作为一个开源工具，DUSTrack为超声视频中组织运动的量化提供了强大且灵活的基础框架，拓宽了其在临床和生物力学研究中的应用潜力。

Abstract: Ultrasound technology enables safe, non-invasive imaging of dynamic tissue
behavior, making it a valuable tool in medicine, biomechanics, and sports
science. However, accurately tracking tissue motion in B-mode ultrasound
remains challenging due to speckle noise, low edge contrast, and out-of-plane
movement. These challenges complicate the task of tracking anatomical landmarks
over time, which is essential for quantifying tissue dynamics in many clinical
and research applications. This manuscript introduces DUSTrack (Deep learning
and optical flow-based toolkit for UltraSound Tracking), a semi-automated
framework for tracking arbitrary points in B-mode ultrasound videos. We combine
deep learning with optical flow to deliver high-quality and robust tracking
across diverse anatomical structures and motion patterns. The toolkit includes
a graphical user interface that streamlines the generation of high-quality
training data and supports iterative model refinement. It also implements a
novel optical-flow-based filtering technique that reduces high-frequency
frame-to-frame noise while preserving rapid tissue motion. DUSTrack
demonstrates superior accuracy compared to contemporary zero-shot point
trackers and performs on par with specialized methods, establishing its
potential as a general and foundational tool for clinical and biomechanical
research. We demonstrate DUSTrack's versatility through three use cases:
cardiac wall motion tracking in echocardiograms, muscle deformation analysis
during reaching tasks, and fascicle tracking during ankle plantarflexion. As an
open-source solution, DUSTrack offers a powerful, flexible framework for point
tracking to quantify tissue motion from ultrasound videos. DUSTrack is
available at https://github.com/praneethnamburi/DUSTrack.

</details>


### [7] [CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding](https://arxiv.org/abs/2507.14426)
*Zhou Chen,Joe Lin,Sathyanarayanan N. Aakur*

Main category: cs.CV

TL;DR: 提出CRAFT框架，通过结合概念网络、语言模型和视觉证据进行可解释的动作关联物体识别。


<details>
  <summary>Details</summary>
Motivation: 实现场景中的可解释动作关联物体识别，提升场景理解的准确性及信任度。

Method: 将ConceptNet和语言模型的结构化常识与CLIP的视觉证据结合，使用基于能量的推理循环优化预测。

Result: CRAFT在多物体和无标签设置下提高了准确性，同时增强了可解释性。

Conclusion: CRAFT推进了强健且可信的场景理解能力，为AI领域带来新的视角。

Abstract: We introduce CRAFT, a neuro-symbolic framework for interpretable affordance
grounding, which identifies the objects in a scene that enable a given action
(e.g., "cut"). CRAFT integrates structured commonsense priors from ConceptNet
and language models with visual evidence from CLIP, using an energy-based
reasoning loop to refine predictions iteratively. This process yields
transparent, goal-driven decisions to ground symbolic and perceptual
structures. Experiments in multi-object, label-free settings demonstrate that
CRAFT enhances accuracy while improving interpretability, providing a step
toward robust and trustworthy scene understanding.

</details>


### [8] [Adaptive 3D Gaussian Splatting Video Streaming](https://arxiv.org/abs/2507.14432)
*Han Gong,Qiyue Li,Zhi Liu,Hao Zhou,Peng Yuan Zhou,Zhu Li,Jie Li*

Main category: cs.CV

TL;DR: 本文提出了一个专为3DGS体积视频设计的流媒体框架，通过高斯变形场和混合显著性分块，实现高效压缩与高质量传输。


<details>
  <summary>Details</summary>
Motivation: 3DGS增强了体积视频表现，但由于较大的数据量和压缩传输复杂性，导致流媒体传输面临挑战。

Method: 通过基于高斯变形场的视频构建方法，结合混合显著性分块和质量建模，实现数据压缩与带宽适应性。

Result: 实验显示，本方法在视频质量、压缩效率、传输速率等方面优于现有方法。

Conclusion: 该方法提供了一种有效解决3DGS视频流媒体挑战的新方案，同时验证了其实用性与性能。

Abstract: The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the
quality of volumetric video representation. Meanwhile, in contrast to
conventional volumetric video, 3DGS video poses significant challenges for
streaming due to its substantially larger data volume and the heightened
complexity involved in compression and transmission. To address these issues,
we introduce an innovative framework for 3DGS volumetric video streaming.
Specifically, we design a 3DGS video construction method based on the Gaussian
deformation field. By employing hybrid saliency tiling and differentiated
quality modeling of 3DGS video, we achieve efficient data compression and
adaptation to bandwidth fluctuations while ensuring high transmission quality.
Then we build a complete 3DGS video streaming system and validate the
transmission performance. Through experimental evaluation, our method
demonstrated superiority over existing approaches in various aspects, including
video quality, compression effectiveness, and transmission rate.

</details>


### [9] [IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark](https://arxiv.org/abs/2507.14449)
*Zhe Cao,Jin Zhang,Ruiheng Zhang*

Main category: cs.CV

TL;DR: 提出了首个针对真实红外图像的多模态大语言模型IRGPT，通过大规模真实红外-文本数据集IR-TD优化红外领域视觉语言任务，并创新性使用了双跨模态课程学习策略，显著提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型难以充分利用红外图像的独特特性，以及由于缺乏真实红外图像-文本对导致任务表现受限。

Method: 构建了包含26万对真实红外-文本对的IR-TD数据集，从LLM生成描述和基于规则的注释生成初稿，并提出了双跨模态课程学习策略，基于红外-可见光及红外-文本困难度进行有系统的知识迁移。

Result: 通过9个任务的基准测试，IRGPT在识别、定位等任务上实现了最先进的性能，优于更大规模的模型。

Conclusion: IRGPT显著提高了红外图像相关任务的性能，为红外视觉语言模型的开发提供了新方向，证明了真实数据和创新迁移策略的重要性。

Abstract: Real-world infrared imagery presents unique challenges for vision-language
models due to the scarcity of aligned text data and domain-specific
characteristics. Although existing methods have advanced the field, their
reliance on synthetic infrared images generated through style transfer from
visible images, which limits their ability to capture the unique
characteristics of the infrared modality. To address this, we propose IRGPT,
the first multi-modal large language model for real-world infrared images,
built upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K
authentic image-text pairs. The proposed IR-TD dataset contains real infrared
images paired with meticulously handcrafted texts, where the initial drafts
originated from two complementary processes: (1) LLM-generated descriptions of
visible images, and (2) rule-based descriptions of annotations. Furthermore, we
introduce a bi-cross-modal curriculum transfer learning strategy that
systematically transfers knowledge from visible to infrared domains by
considering the difficulty scores of both infrared-visible and infrared-text.
Evaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT
achieves state-of-the-art performance even compared with larger-scale models.

</details>


### [10] [GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration](https://arxiv.org/abs/2507.14452)
*Weikang Gu,Mingyue Han,Li Xue,Heng Dong,Changcai Yang,Riqing Chen,Lifang Wei*

Main category: cs.CV

TL;DR: 提出了GPI-Net，一种基于格式塔理论的点云配准网络，能更好地融合局部和全局信息，实验表明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决点云配准中局部与全局特征融合的困难，尤其是特征冗余与复杂空间关系问题。

Method: 采用格式塔原理，通过正交几何一致性和创新的模块设计（如GFA块和DMG块），实现了局部与全局信息的互补通信及跨粒度的信息交互。

Result: 在多个具有挑战性的任务中，GPI-Net相较于现有方法表现更优越。

Conclusion: GPI-Net显著提升了点云配准任务中高质量对应的识别能力，是一种有效的解决方案。

Abstract: The accurate identification of high-quality correspondences is a prerequisite
task in feature-based point cloud registration. However, it is extremely
challenging to handle the fusion of local and global features due to feature
redundancy and complex spatial relationships. Given that Gestalt principles
provide key advantages in analyzing local and global relationships, we propose
a novel Gestalt-guided Parallel Interaction Network via orthogonal geometric
consistency (GPI-Net) in this paper. It utilizes Gestalt principles to
facilitate complementary communication between local and global information.
Specifically, we introduce an orthogonal integration strategy to optimally
reduce redundant information and generate a more compact global structure for
high-quality correspondences. To capture geometric features in correspondences,
we leverage a Gestalt Feature Attention (GFA) block through a hybrid
utilization of self-attention and cross-attention mechanisms. Furthermore, to
facilitate the integration of local detail information into the global
structure, we design an innovative Dual-path Multi-Granularity parallel
interaction aggregation (DMG) block to promote information exchange across
different granularities. Extensive experiments on various challenging tasks
demonstrate the superior performance of our proposed GPI-Net in comparison to
existing methods. The code will be released at https://github.com/gwk/GPI-Net.

</details>


### [11] [Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation](https://arxiv.org/abs/2507.14454)
*Han Gong,Qiyue Li,Jie Li,Zhi Liu*

Main category: cs.CV

TL;DR: 本研究提出了适应性的3D Gaussian splatting（3DGS）平铺技术、新的质量评估框架和基于元学习的比特率适配算法，以优化3DGS视频持续流服务的性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决3DGS视频流服务的基础挑战，如平铺、质量评估及比特率适配问题，更好地满足沉浸式3D体验需求。

Method: 提出了基于显著性分析的自适应3DGS平铺技术，同时结合空间和时间特征；设计一种联合评估3DGS表示的空间域退化与2D渲染图像质量的新质量评估框架；开发了基于元学习的方法，针对3DGS视频实现比特率的自适应调节。

Result: 实验结果表明，所提出的方法在性能上明显优于现有最先进技术，提升了3DGS视频流服务的质量。

Conclusion: 研究为3DGS视频流服务中平铺、质量评估和比特率适应等关键问题提供了创新性解决方案，并显著改善了用户的流媒体观看体验。

Abstract: 3D Gaussian splatting video (3DGS) streaming has recently emerged as a
research hotspot in both academia and industry, owing to its impressive ability
to deliver immersive 3D video experiences. However, research in this area is
still in its early stages, and several fundamental challenges, such as tiling,
quality assessment, and bitrate adaptation, require further investigation. In
this paper, we tackle these challenges by proposing a comprehensive set of
solutions. Specifically, we propose an adaptive 3DGS tiling technique guided by
saliency analysis, which integrates both spatial and temporal features. Each
tile is encoded into versions possessing dedicated deformation fields and
multiple quality levels for adaptive selection. We also introduce a novel
quality assessment framework for 3DGS video that jointly evaluates
spatial-domain degradation in 3DGS representations during streaming and the
quality of the resulting 2D rendered images. Additionally, we develop a
meta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS
video streaming, achieving optimal performance across varying network
conditions. Extensive experiments demonstrate that our proposed approaches
significantly outperform state-of-the-art methods.

</details>


### [12] [GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving](https://arxiv.org/abs/2507.14456)
*Chi Wan,Yixin Cui,Jiatong Du,Shuo Yang,Yulong Bai,Yanjun Huang*

Main category: cs.CV

TL;DR: GEMINUS是一种混合专家的端到端自动驾驶框架，通过全局专家、场景自适应专家组和双注意路由器，有效处理复杂多样的交通场景，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统单模态规划方法难以在复杂多样的交通场景中学习到多样化的驾驶技能，因此提出了能够适配和鲁棒的混合专家框架。

Method: 提出了一个拥有全局专家、场景自适应专家组和双注意路由器的系统结构，利用全局专家处理整体性能，场景自适应专家负责适应特定场景，由双注意路由器进行动态调度。

Result: 在Bench2Drive闭环基准上，GEMINUS在驾驶分数和成功率上实现了最优表现，即使仅使用单目视觉输入，与基础单专家方法相比，在驾驶分数、成功率和多能力平均值上分别提升了7.67%、22.06%和19.41%。

Conclusion: GEMINUS通过全局与场景自适应模块的有效结合，在多样化场景中实现了适应性和鲁棒性的统一，其成功展示了多专家系统的潜力。

Abstract: End-to-end autonomous driving requires adaptive and robust handling of
complex and diverse traffic environments. However, prevalent single-mode
planning methods attempt to learn an overall policy while struggling to acquire
diversified driving skills to handle diverse scenarios. Therefore, this paper
proposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework
featuring a Global Expert, a Scene-Adaptive Experts Group, and equipped with a
Dual-aware Router. Specifically, the Global Expert is trained on the overall
dataset, possessing robust performance. The Scene-Adaptive Experts are trained
on corresponding scene subsets, achieving adaptive performance. The Dual-aware
Router simultaneously considers scenario-level features and routing uncertainty
to dynamically activate expert modules. Through the effective coupling of the
Global Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,
GEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS
outperforms existing methods in the Bench2Drive closed-loop benchmark and
achieves state-of-the-art performance in Driving Score and Success Rate, even
with only monocular vision input. Furthermore, ablation studies demonstrate
significant improvements over the original single-expert baseline: 7.67% in
Driving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The
code will be available at https://github.com/newbrains1/GEMINUS.

</details>


### [13] [VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval](https://arxiv.org/abs/2507.14459)
*Huayuan Ye,Juntong Chen,Shenzhuo Zhang,Yipeng Zhang,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为VisGuard的防篡改VIDR框架，可以可靠地将元数据链接嵌入到可视化图像中，即使经过篡改仍可恢复。


<details>
  <summary>Details</summary>
Motivation: 目前大多数可视化的分发形式为光栅图像，导致源代码、交互功能和元数据等关键信息丢失。现有方法在图像在线分发中的常见篡改如裁剪和编辑下表现不佳，缺乏实用性。

Method: 作者提出了VisGuard框架，该框架通过嵌入元数据链接实现防篡改的VIDR功能，采用重复数据平铺、可逆信息广播和基于锚点的裁剪定位等技术增强鲁棒性。

Result: VisGuard在数据检索准确性、嵌入容量及防篡改和隐写分析等方面表现优越。

Conclusion: 通过实验验证，VisGuard提高了可视化传播的功能性和安全性，可用于互动图表重建、篡改检测以及版权保护。

Abstract: The dissemination of visualizations is primarily in the form of raster
images, which often results in the loss of critical information such as source
code, interactive features, and metadata. While previous methods have proposed
embedding metadata into images to facilitate Visualization Image Data Retrieval
(VIDR), most existing methods lack practicability since they are fragile to
common image tampering during online distribution such as cropping and editing.
To address this issue, we propose VisGuard, a tamper-resistant VIDR framework
that reliably embeds metadata link into visualization images. The embedded data
link remains recoverable even after substantial tampering upon images. We
propose several techniques to enhance robustness, including repetitive data
tiling, invertible information broadcasting, and an anchor-based scheme for
crop localization. VisGuard enables various applications, including interactive
chart reconstruction, tampering detection, and copyright protection. We conduct
comprehensive experiments on VisGuard's superior performance in data retrieval
accuracy, embedding capacity, and security against tampering and steganalysis,
demonstrating VisGuard's competence in facilitating and safeguarding
visualization dissemination and information conveyance.

</details>


### [14] [OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition](https://arxiv.org/abs/2507.14477)
*Zhenyu Li,Tianyi Shang,Pengjie Xu,Ruirui Zhang,Fanchen Kong*

Main category: cs.CV

TL;DR: OptiCorNet通过整合空间特征提取和时间差分，提出一种新型序列建模框架，提高了视觉定位在动态和感知混淆环境下的性能。


<details>
  <summary>Details</summary>
Motivation: 视觉定位在动态和感知混淆环境中长期定位的挑战仍未有效解决，而现有方法忽略了序列的时间一致性。

Method: 提出一种新型序列建模框架OptiCorNet，将轻量级1D卷积编码器和可学习的差分时间算子（DSD）结合，通过DSD模块和四元损失优化，实现空间上下文和时间转变的联合捕获。

Result: 在多个公开基准测试中，OptiCorNet在季节和视角变化等挑战下性能优于现有方法。

Conclusion: OptiCorNet实现了序列级嵌入的直接学习，提供了更高效的端到端视觉定位方法，对比现有方法展现出卓越的鲁棒性和准确性。

Abstract: Visual Place Recognition (VPR) in dynamic and perceptually aliased
environments remains a fundamental challenge for long-term localization.
Existing deep learning-based solutions predominantly focus on single-frame
embeddings, neglecting the temporal coherence present in image sequences. This
paper presents OptiCorNet, a novel sequence modeling framework that unifies
spatial feature extraction and temporal differencing into a differentiable,
end-to-end trainable module. Central to our approach is a lightweight 1D
convolutional encoder combined with a learnable differential temporal operator,
termed Differentiable Sequence Delta (DSD), which jointly captures short-term
spatial context and long-range temporal transitions. The DSD module models
directional differences across sequences via a fixed-weight differencing
kernel, followed by an LSTM-based refinement and optional residual projection,
yielding compact, discriminative descriptors robust to viewpoint and appearance
shifts. To further enhance inter-class separability, we incorporate a
quadruplet loss that optimizes both positive alignment and multi-negative
divergence within each batch. Unlike prior VPR methods that treat temporal
aggregation as post-processing, OptiCorNet learns sequence-level embeddings
directly, enabling more effective end-to-end place recognition. Comprehensive
evaluations on multiple public benchmarks demonstrate that our approach
outperforms state-of-the-art baselines under challenging seasonal and viewpoint
variations.

</details>


### [15] [DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning](https://arxiv.org/abs/2507.14481)
*Yujia Tong,Jingling Yuan,Tian Zhang,Jianquan Liu,Chuang Hu*

Main category: cs.CV

TL;DR: 提出了一种用于视觉变换器量化的无数据量化方法（DFQ-ViT），在无真实数据情况下，实现接近于使用真实数据的量化性能。


<details>
  <summary>Details</summary>
Motivation: 针对现有无数据量化方法中合成数据质量不足、量化模型与全精度模型中间层激活分布差异大导致严重性能下降的问题，提出改进方法。

Method: 分别以增加难度的顺序合成样本以提升数据质量，并引入激活修正矩阵校正量化模型和全精度模型间的激活分布差异。

Result: 实验显示，DFQ-ViT超越现有无数据量化方法，在无需微调的情况下性能与使用真实数据量化的模型可比，DeiT-T模型权重量化到3位性能提升4.29%。

Conclusion: 该方法减少计算开销，降低边缘设备部署门槛，符合绿色学习原则，提高能效，助力资源受限环境的实际应用。

Abstract: Data-Free Quantization (DFQ) enables the quantization of Vision Transformers
(ViTs) without requiring access to data, allowing for the deployment of ViTs on
devices with limited resources. In DFQ, the quantization model must be
calibrated using synthetic samples, making the quality of these synthetic
samples crucial. Existing methods fail to fully capture and balance the global
and local features within the samples, resulting in limited synthetic data
quality. Moreover, we have found that during inference, there is a significant
difference in the distributions of intermediate layer activations between the
quantized and full-precision models. These issues lead to a severe performance
degradation of the quantized model. To address these problems, we propose a
pipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).
Specifically, we synthesize samples in order of increasing difficulty,
effectively enhancing the quality of synthetic data. During the calibration and
inference stage, we introduce the activation correction matrix for the
quantized model to align the intermediate layer activations with those of the
full-precision model. Extensive experiments demonstrate that DFQ-ViT achieves
remarkable superiority over existing DFQ methods and its performance is on par
with models quantized through real data. For example, the performance of DeiT-T
with 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our
method eliminates the need for fine-tuning, which not only reduces
computational overhead but also lowers the deployment barriers for edge
devices. This characteristic aligns with the principles of Green Learning by
improving energy efficiency and facilitating real-world applications in
resource-constrained environments.

</details>


### [16] [Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion](https://arxiv.org/abs/2507.14485)
*Hongye Hou,Liu Zhan,Yang Yang*

Main category: cs.CV

TL;DR: 提出了一种新的检索增强点云补全框架，结合跨模态检索与点云补全，用于生成高质量3D结构，尤其在稀疏数据和新类别中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于跨模态学习的点云补全方法存在生成能力局限，需要更多通用结构特征先验来提升效果。

Method: 设计了结构共享特征编码器（SSFE）来提取跨模态特征，包括参考特征的重建，以及应用在补全任务中的先验整合。同时，提出渐进式检索增强生成器（PRAG），通过分层特征融合机制从全局到局部整合输入与参考先验信息。

Result: 在多种数据集和实际场景中进行广泛评估，证明该方法能生成细粒度点云，并表现出对稀疏数据和未知类别的优异泛化能力。

Conclusion: 通过结合检索与补全策略，该方法在点云补全任务中提升了结构信息的学习能力和生成质量，具备良好的泛化性能。

Abstract: Completing the whole 3D structure based on an incomplete point cloud is a
challenging task, particularly when the residual point cloud lacks typical
structural characteristics. Recent methods based on cross-modal learning
attempt to introduce instance images to aid the structure feature learning.
However, they still focus on each particular input class, limiting their
generation abilities. In this work, we propose a novel retrieval-augmented
point cloud completion framework. The core idea is to incorporate cross-modal
retrieval into completion task to learn structural prior information from
similar reference samples. Specifically, we design a Structural Shared Feature
Encoder (SSFE) to jointly extract cross-modal features and reconstruct
reference features as priors. Benefiting from a dual-channel control gate in
the encoder, relevant structural features in the reference sample are enhanced
and irrelevant information interference is suppressed. In addition, we propose
a Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical
feature fusion mechanism to integrate reference prior information with input
features from global to local. Through extensive evaluations on multiple
datasets and real-world scenes, our method shows its effectiveness in
generating fine-grained point clouds, as well as its generalization capability
in handling sparse data and unseen categories.

</details>


### [17] [Efficient Whole Slide Pathology VQA via Token Compression](https://arxiv.org/abs/2507.14497)
*Weimin Lyu,Qingqiao Hu,Kehan Qi,Zhan Shi,Wentao Huang,Saumya Gupta,Chao Chen*

Main category: cs.CV

TL;DR: 提出了用于病理学全扫描图像视觉问答的Token Compression Pathology LLaVA (TCP-LLaVA) 架构，利用压缩令牌机制，显著减少了输入长度和计算成本，同时提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 解决当前多模态大语言模型在处理病理学全扫描图像时，由于图像分辨率过高而导致的计算资源消耗过大问题。

Method: 设计了一种压缩令牌机制，通过模态压缩模块汇总视觉和文本信息，仅将压缩后的令牌输入语言模型进行回答生成。

Result: 实验表明，在TCGA十个肿瘤亚型上，TCP-LLaVA在视觉问答准确率上超过了现有基线模型，同时显著降低了训练资源消耗。

Conclusion: TCP-LLaVA通过创新的令牌压缩机制，在降低计算需求的同时，提高了病理学全扫描图像视觉问答的效果，为此领域提供了更优的解决方案。

Abstract: Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000
pixels, posing significant challenges for multimodal large language model
(MLLM) due to long context length and high computational demands. Previous
methods typically focus on patch-level analysis or slide-level classification
using CLIP-based models with multi-instance learning, but they lack the
generative capabilities needed for visual question answering (VQA). More recent
MLLM-based approaches address VQA by feeding thousands of patch tokens directly
into the language model, which leads to excessive resource consumption. To
address these limitations, we propose Token Compression Pathology LLaVA
(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token
compression. TCP-LLaVA introduces a set of trainable compression tokens that
aggregate visual and textual information through a modality compression module,
inspired by the [CLS] token mechanism in BERT. Only the compressed tokens are
forwarded to the LLM for answer generation, significantly reducing input length
and computational cost. Experiments on ten TCGA tumor subtypes show that
TCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing
training resource consumption by a substantial margin.

</details>


### [18] [Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow](https://arxiv.org/abs/2507.14500)
*Zhiyuan Hua,Dehao Yuan,Cornelia Fermüller*

Main category: cs.CV

TL;DR: 本文提出了一种利用基于事件的法向流进行运动分割和自运动估计的框架，特别适用于神经形态视觉传感器。


<details>
  <summary>Details</summary>
Motivation: 针对传统方法依赖光流或深度估计的弱点，提出一个使用稀疏高时间分辨率事件数据的新框架。

Method: 通过几何约束（法向流、场景结构和惯性测量）和基于优化的流水线方法完成事件过分割、残差分析、基于运动相似性和时间一致性的分层聚类优化分割。

Result: 在EVIMO2v2数据集上的实验结果表明，该方法在无需光流计算的情况下实现了精准的分割和平移运动估计，特别是在物体边界上表现优秀。

Conclusion: 新的方法展示了在实时机器人与导航应用中很高的扩展潜力。

Abstract: This paper introduces a robust framework for motion segmentation and
egomotion estimation using event-based normal flow, tailored specifically for
neuromorphic vision sensors. In contrast to traditional methods that rely
heavily on optical flow or explicit depth estimation, our approach exploits the
sparse, high-temporal-resolution event data and incorporates geometric
constraints between normal flow, scene structure, and inertial measurements.
The proposed optimization-based pipeline iteratively performs event
over-segmentation, isolates independently moving objects via residual analysis,
and refines segmentations using hierarchical clustering informed by motion
similarity and temporal consistency. Experimental results on the EVIMO2v2
dataset validate that our method achieves accurate segmentation and
translational motion estimation without requiring full optical flow
computation. This approach demonstrates significant advantages at object
boundaries and offers considerable potential for scalable, real-time robotic
and navigation applications.

</details>


### [19] [Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey](https://arxiv.org/abs/2507.14501)
*Jiahui Zhang,Yuelei Li,Anpei Chen,Muyu Xu,Kunhao Liu,Jianyuan Wang,Xiao-Xiao Long,Hanxue Liang,Zexiang Xu,Hao Su,Christian Theobalt,Christian Rupprecht,Andrea Vedaldi,Hanspeter Pfister,Shijian Lu,Fangneng Zhan*

Main category: cs.CV

TL;DR: 本文综述了基于深度学习的前馈方法在3D重建和视图合成中的应用，分类讨论了点云、3D高斯平铺(3DGS)和神经辐射场(NeRF)等表示架构。


<details>
  <summary>Details</summary>
Motivation: 由于传统方法的迭代优化成本较高，限制了其在实际场景中的应用，因此需要更高效的前馈方法来解决3D重建和视图合成问题。

Method: 通过对基于表示架构分类的前馈方法进行综述，并详细讨论了其在关键任务中的应用，同时审视了常用数据集和评估协议。

Result: 提供了对基于前馈方法的分类讨论与关键任务的应用案例，并对数据集与评估方法进行了全面回顾。

Conclusion: 前馈方法在3D视觉中有很大的潜力，通过总结研究挑战和未来方向，肯定了其在推动3D领域发展的作用。

Abstract: 3D reconstruction and view synthesis are foundational problems in computer
vision, graphics, and immersive technologies such as augmented reality (AR),
virtual reality (VR), and digital twins. Traditional methods rely on
computationally intensive iterative optimization in a complex chain, limiting
their applicability in real-world scenarios. Recent advances in feed-forward
approaches, driven by deep learning, have revolutionized this field by enabling
fast and generalizable 3D reconstruction and view synthesis. This survey offers
a comprehensive review of feed-forward techniques for 3D reconstruction and
view synthesis, with a taxonomy according to the underlying representation
architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural
Radiance Fields (NeRF), etc. We examine key tasks such as pose-free
reconstruction, dynamic 3D reconstruction, and 3D-aware image and video
synthesis, highlighting their applications in digital humans, SLAM, robotics,
and beyond. In addition, we review commonly used datasets with detailed
statistics, along with evaluation protocols for various downstream tasks. We
conclude by discussing open research challenges and promising directions for
future work, emphasizing the potential of feed-forward approaches to advance
the state of the art in 3D vision.

</details>


### [20] [DCHM: Depth-Consistent Human Modeling for Multiview Detection](https://arxiv.org/abs/2507.14505)
*Jiahao Ma,Tianyu Wang,Miaomiao Liu,David Ahmedt-Aristizabal,Chuong Nguyen*

Main category: cs.CV

TL;DR: 提出了一种Depth-Consistent Human Modeling (DCHM)框架，通过一致的深度估计和全局坐标下的多视角融合，提升大规模和拥挤场景中的行人检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有的多视角行人检测方法在3D建模中容易引入噪声且精度不高；部分方法依赖昂贵的3D标注，但难以适应多样化场景。

Method: 提出了利用超像素级的高斯散射技术，确保稀疏视角和大规模拥挤场景下的深度一致性，从而生成更精确的点云进行行人定位。

Result: 验证表明，该方法显著降低了3D建模中的噪声，并在多个基准上优于现有方法。此外，这是首个在如此挑战性场景下进行行人重建和多视角分割的工作。

Conclusion: 提供了一种无需人工标注的高精度多视角行人检测解决方案，并展示了在稀疏和复杂场景中的优越性能。

Abstract: Multiview pedestrian detection typically involves two stages: human modeling
and pedestrian localization. Human modeling represents pedestrians in 3D space
by fusing multiview information, making its quality crucial for detection
accuracy. However, existing methods often introduce noise and have low
precision. While some approaches reduce noise by fitting on costly multiview 3D
annotations, they often struggle to generalize across diverse scenes. To
eliminate reliance on human-labeled annotations and accurately model humans, we
propose Depth-Consistent Human Modeling (DCHM), a framework designed for
consistent depth estimation and multiview fusion in global coordinates.
Specifically, our proposed pipeline with superpixel-wise Gaussian Splatting
achieves multiview depth consistency in sparse-view, large-scaled, and crowded
scenarios, producing precise point clouds for pedestrian localization.
Extensive validations demonstrate that our method significantly reduces noise
during human modeling, outperforming previous state-of-the-art baselines.
Additionally, to our knowledge, DCHM is the first to reconstruct pedestrians
and perform multiview segmentation in such a challenging setting. Code is
available on the \href{https://jiahao-ma.github.io/DCHM/}{project page}.

</details>


### [21] [ArtiMuse: Fine-Grained Image Aesthetics Assessment with Joint Scoring and Expert-Level Understanding](https://arxiv.org/abs/2507.14533)
*Shuo Cao,Nan Ma,Jiayang Li,Xiaohui Li,Lihao Shao,Kaiwen Zhu,Yu Zhou,Yuandong Pu,Jiarui Wu,Jiaquan Wang,Bo Qu,Wenhai Wang,Yu Qiao,Dajuin Yao,Yihao Liu*

Main category: cs.CV

TL;DR: 论文介绍了一种名为ArtiMuse的多模态语言模型用于图像美学评估以及一个新的包含专业标注的10,000图像数据集。


<details>
  <summary>Details</summary>
Motivation: 解决当前图像美学评估方法中存在的调偏问题和缺乏细粒度属性分析的问题，以满足教育应用、艺术创作和生成式AI技术对综合图像美学评估的需求。

Method: 提出了一个多模态模型ArtiMuse，结合量化评分与专家级理解能力，同时引入了一个全新专业标注的高质量图像美学数据集ArtiMuse-10K。

Result: ArtiMuse模型展示出强大的知觉与泛化能力，能够提供细粒度属性分解与专业的图像审美评估。

Conclusion: 通过公开ArtiMuse模型和ArtiMuse-10K数据集，致力于推动图像美学评估领域的研究与进步。

Abstract: The rapid advancement of educational applications, artistic creation, and
AI-generated content (AIGC) technologies has substantially increased practical
requirements for comprehensive Image Aesthetics Assessment (IAA), particularly
demanding methods capable of delivering both quantitative scoring and
professional understanding. Multimodal Large Language Model (MLLM)-based IAA
methods demonstrate stronger perceptual and generalization capabilities
compared to traditional approaches, yet they suffer from modality bias
(score-only or text-only) and lack fine-grained attribute decomposition,
thereby failing to support further aesthetic assessment. In this paper, we
present:(1) ArtiMuse, an innovative MLLM-based IAA model with Joint Scoring and
Expert-Level Understanding capabilities; (2) ArtiMuse-10K, the first
expert-curated image aesthetic dataset comprising 10,000 images spanning 5 main
categories and 15 subcategories, each annotated by professional experts with
8-dimensional attributes analysis and a holistic score. Both the model and
dataset will be made public to advance the field.

</details>


### [22] [Real Time Captioning of Sign Language Gestures in Video Meetings](https://arxiv.org/abs/2507.14543)
*Sharanya Mukherjee,Md Hishaam Akhtar,Kannadasan R*

Main category: cs.CV

TL;DR: 本文提出了一种浏览器扩展，能自动将手语翻译为视频通话中的字幕，基于一个包含2000+单词级别ASL视频的大型数据集。


<details>
  <summary>Details</summary>
Motivation: 解决普通人与听觉障碍人士交流困难的问题，尤其是在视频会议变得普遍的重要背景下。

Method: 利用计算机视觉技术开发浏览器扩展，结合一个包含超过2000个单词级ASL视频的大型数据集用于训练和识别。

Result: 该浏览器扩展能够在视频通话中将手语实时转换为字幕，减轻沟通障碍。

Conclusion: 该研究为手语用户提供了创新的工具，提升了听觉障碍人群在视频会议场景下的交流体验。

Abstract: It has always been a rather tough task to communicate with someone possessing
a hearing impairment. One of the most tested ways to establish such a
communication is through the use of sign based languages. However, not many
people are aware of the smaller intricacies involved with sign language. Sign
language recognition using computer vision aims at eliminating the
communication barrier between deaf-mute and ordinary people so that they can
properly communicate with others. Recently the pandemic has left the whole
world shaken up and has transformed the way we communicate. Video meetings have
become essential for everyone, even people with a hearing disability. In recent
studies, it has been found that people with hearing disabilities prefer to sign
over typing during these video calls. In this paper, we are proposing a browser
extension that will automatically translate sign language to subtitles for
everyone else in the video call. The Large-scale dataset which contains more
than 2000 Word-Level ASL videos, which were performed by over 100 signers will
be used.

</details>


### [23] [Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025](https://arxiv.org/abs/2507.14544)
*Sujata Gaihre,Amir Thapa Magar,Prasuna Pokharel,Laxmi Tiwari*

Main category: cs.CV

TL;DR: 本研究针对ImageCLEFmed MEDVQA 2025挑战赛的任务，提出一种利用多模态基础模型Florence进行胃肠道内窥镜图像视觉问答（VQA）的方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在提升医疗领域内窥镜图像的问答能力，并探索大规模多模态模型在医学VQA中的应用潜力。

Method: 使用Florence模型作为VQA管道的核心，将强大的视觉编码器和文本编码器结合，搭配医学特征保持的领域特定数据增强，提高模型的泛化能力。

Result: 通过KASVIR数据集的实验表明，微调后的Florence模型在挑战赛官方指标上表现出色，生成了准确的回答。

Conclusion: 本研究表明大规模多模态模型在医学VQA领域的潜力，为未来的可解释性、鲁棒性和临床整合研究奠定了基线。

Abstract: This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA
2025 Challenge, which targets visual question answering (VQA) for
gastrointestinal endoscopy. We adopt the Florence model-a large-scale
multimodal foundation model-as the backbone of our VQA pipeline, pairing a
powerful vision encoder with a text encoder to interpret endoscopic images and
produce clinically relevant answers. To improve generalization, we apply
domain-specific augmentations that preserve medical features while increasing
training diversity. Experiments on the KASVIR dataset show that fine-tuning
Florence yields accurate responses on the official challenge metrics. Our
results highlight the potential of large multimodal models in medical VQA and
provide a strong baseline for future work on explainability, robustness, and
clinical integration. The code is publicly available at:
https://github.com/TiwariLaxuu/VQA-Florence.git

</details>


### [24] [Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering Human Perceptual Variability on Facial Expressions](https://arxiv.org/abs/2507.14549)
*Haotian Deng,Chi Zhang,Chen Wei,Quanying Liu*

Main category: cs.CV

TL;DR: 本研究探讨了ANN在情感认知中对人类感知差异的建模能力，提出基于ANN决策边界生成模糊表情样本的方法，并创建了varEmotion数据集，通过实验发现人类与ANN的情感感知表现出类似的不确定性。


<details>
  <summary>Details</summary>
Motivation: 面对外部情感刺激与人类内在体验关系的复杂性，尤其是个体情感知觉差异，现有模型的局限促使对更精确建模的需求。

Method: 提出感知边界采样法，从ANN决策边界生成情感表情数据样本varEmotion，结合大规模行为实验，分析ANN与人类知觉的对应性，并通过行为数据微调ANN以贴近个体化感知。

Result: 证明了ANN模糊样本与人类感知不确定性之间的联系，揭示了情感感知中的共享计算原理，并提升了ANN对群体和个体层面的感知一致性。

Conclusion: 本研究首次系统化地将ANN决策边界与人类感知差异联系起来，为个性化情感模型奠定了基础，提供了情感理解的新视角。

Abstract: A fundamental challenge in affective cognitive science is to develop models
that accurately capture the relationship between external emotional stimuli and
human internal experiences. While ANNs have demonstrated remarkable accuracy in
facial expression recognition, their ability to model inter-individual
differences in human perception remains underexplored. This study investigates
the phenomenon of high perceptual variability-where individuals exhibit
significant differences in emotion categorization even when viewing the same
stimulus. Inspired by the similarity between ANNs and human perception, we
hypothesize that facial expression samples that are ambiguous for ANN
classifiers also elicit divergent perceptual judgments among human observers.
To examine this hypothesis, we introduce a novel perceptual boundary sampling
method to generate facial expression stimuli that lie along ANN decision
boundaries. These ambiguous samples form the basis of the varEmotion dataset,
constructed through large-scale human behavioral experiments. Our analysis
reveals that these ANN-confusing stimuli also provoke heightened perceptual
uncertainty in human participants, highlighting shared computational principles
in emotion perception. Finally, by fine-tuning ANN representations using
behavioral data, we achieve alignment between ANN predictions and both
group-level and individual-level human perceptual patterns. Our findings
establish a systematic link between ANN decision boundaries and human
perceptual variability, offering new insights into personalized modeling of
emotional interpretation.

</details>


### [25] [Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance](https://arxiv.org/abs/2507.14553)
*Xiaoran Wu*

Main category: cs.CV

TL;DR: 开发了一种相机引导系统，帮助用户识别和移除照片中的杂乱物体，提升照片质量。


<details>
  <summary>Details</summary>
Motivation: 帮助摄影爱好者避免杂乱物体对照片表达的情感和故事的干扰，从而提高拍摄效果。

Method: 系统包括一个区分杂乱物体并进行美学评价的算法，以及基于生成对抗网络的迭代图像修复算法，用于移除物体后的区域填充。

Result: 用户研究表明，系统提供的交互界面灵活、算法准确，可以帮助用户更快速地拍摄高质量照片。

Conclusion: 该系统有效提升了用户的拍摄体验和照片美观性，减少了杂乱物体带来的干扰。

Abstract: Clutter in photos is a distraction preventing photographers from conveying
the intended emotions or stories to the audience. Photography amateurs
frequently include clutter in their photos due to unconscious negligence or the
lack of experience in creating a decluttered, aesthetically appealing scene for
shooting. We are thus motivated to develop a camera guidance system that
provides solutions and guidance for clutter identification and removal. We
estimate and visualize the contribution of objects to the overall aesthetics
and content of a photo, based on which users can interactively identify
clutter. Suggestions on getting rid of clutter, as well as a tool that removes
cluttered objects computationally, are provided to guide users to deal with
different kinds of clutter and improve their photographic work. Two technical
novelties underpin interactions in our system: a clutter distinguishment
algorithm with aesthetics evaluations for objects and an iterative image
inpainting algorithm based on generative adversarial nets that reconstructs
missing regions of removed objects for high-resolution images. User studies
demonstrate that our system provides flexible interfaces and accurate
algorithms that allow users to better identify distractions and take higher
quality images within less time.

</details>


### [26] [Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions](https://arxiv.org/abs/2507.14555)
*Jintang Xue,Ganning Zhao,Jie-En Yao,Hong-En Chen,Yue Hu,Meida Chen,Suya You,C. -C. Jay Kuo*

Main category: cs.CV

TL;DR: 本文提出了Descrip3D，一个利用自然语言对3D场景中的对象和关系进行编码的框架，与现有模型相比表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统的3D场景语言模型在处理对象之间的关系时表现受限，尤其在视觉嵌入不足以揭示对象角色和交互时。

Method: 提出Descrip3D框架，通过自然语言为每个对象提供包含其属性和关系的文本描述，采用嵌入融合和提示注入的双重整合方法，实现多任务统一推理。

Result: Descrip3D在ScanRefer、Multi3DRefer等五个基准数据集上均优于强基线模型。

Conclusion: 语言引导的关系表示能够有效提升复杂室内场景的理解，实现对场景语义的深度推理。

Abstract: Understanding 3D scenes goes beyond simply recognizing objects; it requires
reasoning about the spatial and semantic relationships between them. Current 3D
scene-language models often struggle with this relational understanding,
particularly when visual embeddings alone do not adequately convey the roles
and interactions of objects. In this paper, we introduce Descrip3D, a novel and
powerful framework that explicitly encodes the relationships between objects
using natural language. Unlike previous methods that rely only on 2D and 3D
embeddings, Descrip3D enhances each object with a textual description that
captures both its intrinsic attributes and contextual relationships. These
relational cues are incorporated into the model through a dual-level
integration: embedding fusion and prompt-level injection. This allows for
unified reasoning across various tasks such as grounding, captioning, and
question answering, all without the need for task-specific heads or additional
supervision. When evaluated on five benchmark datasets, including ScanRefer,
Multi3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms
strong baseline models, demonstrating the effectiveness of language-guided
relational representation for understanding complex indoor scenes.

</details>


### [27] [LEAD: Exploring Logit Space Evolution for Model Selection](https://arxiv.org/abs/2507.14559)
*Zixuan Hu,Xiaotong Li,Shixiang Tang,Jun Liu,Yichun Hu,Ling-Yu Duan*

Main category: cs.CV

TL;DR: 本文提出了一种称为LEAD的新方法，用于有效预测预训练模型在下游任务中的迁移性，避免冗长的微调过程。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在特征空间中采用线性变换建模微调动态的问题，同时加强对优化过程非线性本质的捕捉，选择最适合的预训练模型。

Method: 引入LEAD方法，从网络的logit输出出发，以理论框架建模优化过程，通过常微分方程（ODE）描述logit最终状态的非线性演变，并设计基于类别的分解方法以适应不同类别的动态演变。

Result: 在24种预训练模型和10个下游数据集上的实验表明，该方法在监督和自监督的场景下均表现出色，且在小数据场景中具有广泛的适应性。

Conclusion: 通过精确对齐优化目标并结合ODE导出的非线性建模能力，LEAD方法有效地填补了优化鸿沟，为预训练模型的选择提供了一种高效的解决方案。

Abstract: The remarkable success of pretrain-then-finetune paradigm has led to a
proliferation of available pre-trained models for vision tasks. This surge
presents a significant challenge in efficiently choosing the most suitable
pre-trained models for downstream tasks. The critical aspect of this challenge
lies in effectively predicting the model transferability by considering the
underlying fine-tuning dynamics. Existing methods often model fine-tuning
dynamics in feature space with linear transformations, which do not precisely
align with the fine-tuning objective and fail to grasp the essential
nonlinearity from optimization. To this end, we present LEAD, a
finetuning-aligned approach based on the network output of logits. LEAD
proposes a theoretical framework to model the optimization process and derives
an ordinary differential equation (ODE) to depict the nonlinear evolution
toward the final logit state. Additionally, we design a class-aware
decomposition method to consider the varying evolution dynamics across classes
and further ensure practical applicability. Integrating the closely aligned
optimization objective and nonlinear modeling capabilities derived from the
differential equation, our method offers a concise solution to effectively
bridge the optimization gap in a single step, bypassing the lengthy fine-tuning
process. The comprehensive experiments on 24 supervised and self-supervised
pre-trained models across 10 downstream datasets demonstrate impressive
performances and showcase its broad adaptability even in low-data scenarios.

</details>


### [28] [Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation](https://arxiv.org/abs/2507.14575)
*Andrea Moschetto,Lemuel Puglisi,Alec Sargood,Pierluigi Dell'Acqua,Francesco Guarnera,Sebastiano Battiato,Daniele Ravì*

Main category: cs.CV

TL;DR: MRI影像可通过I2I翻译减少扫描时间和成本，本文比较了GAN、扩散模型和FM技术在T1w到T2w MRI翻译中的表现，发现GAN-based Pix2Pix模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 减少MRI扫描时间与成本，同时保持诊断质量，推动对跨模态合成的研究。

Method: 比较测评了GAN、扩散模型和流匹配技术在MRI模态翻译中的性能，基于公开数据集进行定量和定性分析。

Result: GAN-based Pix2Pix模型在结构保真度、图像质量和计算效率上优于扩散与流匹配模型。

Conclusion: GAN模型在小数据集和简单任务上表现更好，未来研究需针对数据需求开展改进。

Abstract: Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image
contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering
distinct diagnostic insights. However, acquiring all desired modalities
increases scan time and cost, motivating research into computational methods
for cross-modal synthesis. To address this, recent approaches aim to synthesize
missing MRI contrasts from those already acquired, reducing acquisition time
while preserving diagnostic quality. Image-to-image (I2I) translation provides
a promising framework for this task. In this paper, we present a comprehensive
benchmark of generative models$\unicode{x2013}$specifically, Generative
Adversarial Networks (GANs), diffusion models, and flow matching (FM)
techniques$\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All
frameworks are implemented with comparable settings and evaluated on three
publicly available MRI datasets of healthy adults. Our quantitative and
qualitative analyses show that the GAN-based Pix2Pix model outperforms
diffusion and FM-based methods in terms of structural fidelity, image quality,
and computational efficiency. Consistent with existing literature, these
results suggest that flow-based models are prone to overfitting on small
datasets and simpler tasks, and may require more data to match or surpass GAN
performance. These findings offer practical guidance for deploying I2I
translation techniques in real-world MRI workflows and highlight promising
directions for future research in cross-modal medical image synthesis. Code and
models are publicly available at
https://github.com/AndreaMoschetto/medical-I2I-benchmark.

</details>


### [29] [Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX](https://arxiv.org/abs/2507.14587)
*Merjem Bećirović,Amina Kurtović,Nordin Smajlović,Medina Kapo,Amila Akagić*

Main category: cs.CV

TL;DR: 本文研究了三个深度学习框架（TensorFlow with Keras、PyTorch和JAX）在BloodMNIST血细胞图像分类任务中的性能差异，重点分析推理时间和分类性能。


<details>
  <summary>Details</summary>
Motivation: 近年来，基于深度学习的自动分类系统在提高血液图像分析的准确性和效率方面表现出了巨大的潜力，但尚缺乏对特定深度学习框架的详细性能分析。

Method: 本文使用BloodMNIST数据集，比较了TensorFlow with Keras、PyTorch和JAX三个框架在血细胞图像分类任务中的性能，主要分析了不同图像大小下的推理时间和分类性能表现。

Result: 结果显示，三个框架在性能上存在差异，这些差异受图像分辨率和框架优化程度的影响。尤其是JAX和PyTorch的分类准确度与当前基准水平相当。

Conclusion: JAX和PyTorch框架在医学图像分类任务中表现出高效性，而框架性能差异可能与特定优化和计算条件相关，提供了框架选择的有力参考。

Abstract: Medical imaging plays a vital role in early disease diagnosis and monitoring.
Specifically, blood microscopy offers valuable insights into blood cell
morphology and the detection of hematological disorders. In recent years, deep
learning-based automated classification systems have demonstrated high
potential in enhancing the accuracy and efficiency of blood image analysis.
However, a detailed performance analysis of specific deep learning frameworks
appears to be lacking. This paper compares the performance of three popular
deep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in
classifying blood cell images from the publicly available BloodMNIST dataset.
The study primarily focuses on inference time differences, but also
classification performance for different image sizes. The results reveal
variations in performance across frameworks, influenced by factors such as
image resolution and framework-specific optimizations. Classification accuracy
for JAX and PyTorch was comparable to current benchmarks, showcasing the
efficiency of these frameworks for medical image classification.

</details>


### [30] [DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF](https://arxiv.org/abs/2507.14596)
*Doriand Petit,Steve Bourgeois,Vincent Gay-Bellile,Florian Chabot,Loïc Barthe*

Main category: cs.CV

TL;DR: 本文介绍了DiSCO-3D方法，它在3D语义分割中结合了场景适应性和用户查询以进行开放词汇子概念的发现。


<details>
  <summary>Details</summary>
Motivation: 当前3D语义分割方法难以同时适应任务特定目标和场景内容，有必要提出一种更普适的新方法。

Method: 基于神经场表示的DiSCO-3D方法，结合了无监督分割和弱开放词汇引导。

Result: 在开放词汇子概念发现任务和开放词汇无监督分割的极端情况中，DiSCO-3D表现出色并达到最前沿的结果。

Conclusion: DiSCO-3D实现了场景和用户需求的双适应性，为3D语义分割领域扩展了新的可能性。

Abstract: 3D semantic segmentation provides high-level scene understanding for
applications in robotics, autonomous systems, \textit{etc}. Traditional methods
adapt exclusively to either task-specific goals (open-vocabulary segmentation)
or scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the
first method addressing the broader problem of 3D Open-Vocabulary Sub-concepts
Discovery, which aims to provide a 3D semantic segmentation that adapts to both
the scene and user queries. We build DiSCO-3D on Neural Fields representations,
combining unsupervised segmentation with weak open-vocabulary guidance. Our
evaluations demonstrate that DiSCO-3D achieves effective performance in
Open-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in
the edge cases of both open-vocabulary and unsupervised segmentation.

</details>


### [31] [Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition](https://arxiv.org/abs/2507.14608)
*Nandani Sharma,Dinesh Singh*

Main category: cs.CV

TL;DR: 本文提出了Exp-Graph框架，通过图模型描述面部属性的结构关系，以提高面部表情识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是面部表情识别在许多人机交互应用中重要性日益增加，但现有方法未充分利用面部属性的结构信息。

Method: 采用图模型表示面部属性，使用视觉变换器提取局部外观特征，并通过图卷积网络捕获面部属性的结构依赖关系。

Result: 在三个基准数据集上的实验中，Exp-Graph模型在Oulu-CASIA、eNTERFACE05和AFEW数据集上的识别准确率分别达到了98.09%、79.01%和56.39%。

Conclusion: 提出的Exp-Graph框架有效提升了表情识别的全局和局部依赖建模能力，并能适应受控实验环境及现实场景中的应用。

Abstract: Facial expression recognition is crucial for human-computer interaction
applications such as face animation, video surveillance, affective computing,
medical analysis, etc. Since the structure of facial attributes varies with
facial expressions, incorporating structural information into facial attributes
is essential for facial expression recognition. In this paper, we propose
Exp-Graph, a novel framework designed to represent the structural relationships
among facial attributes using graph-based modeling for facial expression
recognition. For facial attributes graph representation, facial landmarks are
used as the graph's vertices. At the same time, the edges are determined based
on the proximity of the facial landmark and the similarity of the local
appearance of the facial attributes encoded using the vision transformer.
Additionally, graph convolutional networks are utilized to capture and
integrate these structural dependencies into the encoding of facial attributes,
thereby enhancing the accuracy of expression recognition. Thus, Exp-Graph
learns from the facial attribute graphs highly expressive semantic
representations. On the other hand, the vision transformer and graph
convolutional blocks help the framework exploit the local and global
dependencies among the facial attributes that are essential for the recognition
of facial expressions. We conducted comprehensive evaluations of the proposed
Exp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW.
The model achieved recognition accuracies of 98.09\%, 79.01\%, and 56.39\%,
respectively. These results indicate that Exp-Graph maintains strong
generalization capabilities across both controlled laboratory settings and
real-world, unconstrained environments, underscoring its effectiveness for
practical facial expression recognition applications.

</details>


### [32] [Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2](https://arxiv.org/abs/2507.14613)
*Guoping Xu,Christopher Kabat,You Zhang*

Main category: cs.CV

TL;DR: 本文提出DD-SAM2框架，通过深度可分扩张适配器（DD-Adapter）优化SAM2模型，用于医学视频的目标跟踪和分割，在TrackRad2025和EchoNet-Dynamic数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于仅支持特定模态且难以适应动态医学影像场景。适应SAM2模型需要大量数据和高计算成本，同时可能出现灾难性遗忘问题，因此需要一种高效的适配方案。

Method: 提出DD-SAM2框架，通过集成深度可分扩张适配器（DD-Adapter），以较低参数开销增强多尺度特征提取，针对医学视频场景有效进行SAM2模型的微调。

Result: 在TrackRad2025数据集（Dice得分0.93）和EchoNet-Dynamic数据集（Dice得分0.97）上均取得了优异的分割和跟踪性能。

Conclusion: DD-SAM2首次系统性地探索了基于适配器的SAM2微调方法，为医学视频分割和跟踪提供了一种高效和可行的解决方案。

Abstract: Recent advances in medical image segmentation have been driven by deep
learning; however, most existing methods remain limited by modality-specific
designs and exhibit poor adaptability to dynamic medical imaging scenarios. The
Segment Anything Model 2 (SAM2) and its related variants, which introduce a
streaming memory mechanism for real-time video segmentation, present new
opportunities for prompt-based, generalizable solutions. Nevertheless, adapting
these models to medical video scenarios typically requires large-scale datasets
for retraining or transfer learning, leading to high computational costs and
the risk of catastrophic forgetting. To address these challenges, we propose
DD-SAM2, an efficient adaptation framework for SAM2 that incorporates a
Depthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature
extraction with minimal parameter overhead. This design enables effective
fine-tuning of SAM2 on medical videos with limited training data. Unlike
existing adapter-based methods focused solely on static images, DD-SAM2 fully
exploits SAM2's streaming memory for medical video object tracking and
segmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation)
and EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior
performance, achieving Dice scores of 0.93 and 0.97, respectively. To the best
of our knowledge, this work provides an initial attempt at systematically
exploring adapter-based SAM2 fine-tuning for medical video segmentation and
tracking. Code, datasets, and models will be publicly available at
https://github.com/apple1986/DD-SAM2.

</details>


### [33] [BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM](https://arxiv.org/abs/2507.14632)
*Haiquan Wen,Tianxiao Li,Zhenglin Huang,Yiwei He,Guangliang Cheng*

Main category: cs.CV

TL;DR: 本论文提出了BusterX++框架，用于检测跨模态的合成媒体，并创建了专用基准GenBuster++。实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的发展使得伪造内容检测更加迫切，现有检测方法在面对多模态合成内容时效果有限。

Method: 提出BusterX++框架，结合强化学习后训练策略、多阶段训练和混合推理；并设计了GenBuster++基准用于评估。

Result: 通过实验验证，BusterX++在合成媒体检测中具有良好的通用性和性能提升。

Conclusion: BusterX++通过新颖的设计提升了跨模态检测的能力，为合成内容甄别提供了可靠工具。

Abstract: Recent advances in generative AI have dramatically improved image and video
synthesis capabilities, significantly increasing the risk of misinformation
through sophisticated fake content. In response, detection methods have evolved
from traditional approaches to multimodal large language models (MLLMs),
offering enhanced transparency and interpretability in identifying synthetic
media. However, current detection systems remain fundamentally limited by their
single-modality design. These approaches analyze images or videos separately,
making them ineffective against synthetic content that combines multiple media
formats. To address these challenges, we introduce \textbf{BusterX++}, a novel
framework designed specifically for cross-modal detection and explanation of
synthetic media. Our approach incorporates an advanced reinforcement learning
(RL) post-training strategy that eliminates cold-start. Through Multi-stage
Training, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and
substantial performance improvements. To enable comprehensive evaluation, we
also present \textbf{GenBuster++}, a cross-modal benchmark leveraging
state-of-the-art image and video generation techniques. This benchmark
comprises 4,000 images and video clips, meticulously curated by human experts
using a novel filtering methodology to ensure high quality, diversity, and
real-world applicability. Extensive experiments demonstrate the effectiveness
and generalizability of our approach.

</details>


### [34] [Multispectral State-Space Feature Fusion: Bridging Shared and Cross-Parametric Interactions for Object Detection](https://arxiv.org/abs/2507.14643)
*Jifeng Shen,Haibo Zhan,Shaohua Dong,Xin Zuo,Wankou Yang,Haibin Ling*

Main category: cs.CV

TL;DR: MS2Fusion通过双路径参数交互机制实现跨模式特征融合，在多光谱目标检测和其他视觉任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法过于关注局部特征且在接收域和复杂度之间存在权衡，影响了泛化性能和可扩展性。

Method: 提出了一种基于状态空间模型 (SSM) 的多光谱特征融合框架MS2Fusion，通过双路径交互机制实现跨模态特征融合：一条路径用SSM的交叉参数交互实现补充信息挖掘，另一条路径用参数共享机制探索跨模态结构对齐。

Result: MS2Fusion在FLIR、M3FD和LLVIP等主流基准上显著优于现有方法，同时在RGB-T语义分割和显著目标检测等任务上也表现出很好的通用性。

Conclusion: MS2Fusion提供了一个高效统一的多光谱特征融合框架，在功能互补性和共享语义空间之间实现了良好平衡，且具备广泛应用潜力。

Abstract: Modern multispectral feature fusion for object detection faces two critical
limitations: (1) Excessive preference for local complementary features over
cross-modal shared semantics adversely affects generalization performance; and
(2) The trade-off between the receptive field size and computational complexity
present critical bottlenecks for scalable feature modeling. Addressing these
issues, a novel Multispectral State-Space Feature Fusion framework, dubbed
MS2Fusion, is proposed based on the state space model (SSM), achieving
efficient and effective fusion through a dual-path parametric interaction
mechanism. More specifically, the first cross-parameter interaction branch
inherits the advantage of cross-attention in mining complementary information
with cross-modal hidden state decoding in SSM. The second shared-parameter
branch explores cross-modal alignment with joint embedding to obtain
cross-modal similar semantic features and structures through parameter sharing
in SSM. Finally, these two paths are jointly optimized with SSM for fusing
multispectral features in a unified framework, allowing our MS2Fusion to enjoy
both functional complementarity and shared semantic space. In our extensive
experiments on mainstream benchmarks including FLIR, M3FD and LLVIP, our
MS2Fusion significantly outperforms other state-of-the-art multispectral object
detection methods, evidencing its superiority. Moreover, MS2Fusion is general
and applicable to other multispectral perception tasks. We show that, even
without specific design, MS2Fusion achieves state-of-the-art results on RGB-T
semantic segmentation and RGBT salient object detection, showing its
generality. The source code will be available at
https://github.com/61s61min/MS2Fusion.git.

</details>


### [35] [AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)](https://arxiv.org/abs/2507.14657)
*Keivan Shariatmadar,Ahmad Osman*

Main category: cs.CV

TL;DR: 本论文介绍了一个名为FST.ai的AI框架，用于提高跆拳道比赛中的裁判效率，特别是在实时头部踢击检测与计分中表现突出。


<details>
  <summary>Details</summary>
Motivation: 传统人工裁判系统甚至结合即时视频回放（IVR）都存在延迟、高主观性和不一致性的问题，影响了比赛的公平性和运动员的信任。

Method: 提出了一个基于计算机视觉、深度学习和边缘推理的系统框架，可自动识别和分类跆拳道中的关键动作，特别是检测头部踢击并实时评分，同时可以扩展到其他需要动作检测的体育项目。

Result: FST.ai显著降低了从动作检测到评分的时间，从几分钟缩短到几秒，并提升了一致性和透明度。

Conclusion: FST.ai验证了其在跆拳道领域中实现实时高效裁判的可行性，同时体现了其可扩展性与跨运动适应性，有潜力改变多种体育项目的裁判标准。

Abstract: The integration of Artificial Intelligence (AI) into sports officiating
represents a paradigm shift in how decisions are made in competitive
environments. Traditional manual systems, even when supported by Instant Video
Replay (IVR), often suffer from latency, subjectivity, and inconsistent
enforcement, undermining fairness and athlete trust. This paper introduces
FST.ai, a novel AI-powered framework designed to enhance officiating in Sport
Taekwondo, particularly focusing on the complex task of real-time head kick
detection and scoring. Leveraging computer vision, deep learning, and edge
inference, the system automates the identification and classification of key
actions, significantly reducing decision time from minutes to seconds while
improving consistency and transparency. Importantly, the methodology is not
limited to Taekwondo. The underlying framework -- based on pose estimation,
motion classification, and impact analysis -- can be adapted to a wide range of
sports requiring action detection, such as judo, karate, fencing, or even team
sports like football and basketball, where foul recognition or performance
tracking is critical. By addressing one of Taekwondo's most challenging
scenarios -- head kick scoring -- we demonstrate the robustness, scalability,
and sport-agnostic potential of FST.ai to transform officiating standards
across multiple disciplines.

</details>


### [36] [Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall](https://arxiv.org/abs/2507.14662)
*Shayan Rokhva,Babak Teimourpour*

Main category: cs.CV

TL;DR: 本研究提出了一种基于计算机视觉的框架，通过对餐前和餐后RGB图片进行语义分割评估盘中食物的剩余量，方法成本低廉且实用。


<details>
  <summary>Details</summary>
Motivation: 旨在通过量化机构餐厅中的食品浪费以支持数据驱动的可持续发展战略。

Method: 利用语义分割技术，对餐前和餐后图像进行分析，采用四种监督学习模型（U-Net、U-Net++及其轻量化变体），并引入自定义的分布式像素一致性（DPA）指标进行性能评估。

Result: 所有模型表现良好，至少一种模型在特定食物类型上达到或超过90%的DPA，轻量模型表现具有实时推论能力。干燥且结构坚固的食物分割表现较好，而复杂且粘稠的菜肴性能表现略低。

Conclusion: 该系统是可扩展的无接触解决方案，为大型食堂环境提供了可行的食品浪费监控与管理方法，同时指出未来研究方向。

Abstract: Quantifying post-consumer food waste in institutional dining settings is
essential for supporting data-driven sustainability strategies. This study
presents a cost-effective computer vision framework that estimates plate-level
food waste by utilizing semantic segmentation of RGB images taken before and
after meal consumption across five Iranian dishes. Four fully supervised models
(U-Net, U-Net++, and their lightweight variants) were trained using a capped
dynamic inverse-frequency loss and AdamW optimizer, then evaluated through a
comprehensive set of metrics, including Pixel Accuracy, Dice, IoU, and a
custom-defined Distributional Pixel Agreement (DPA) metric tailored to the
task. All models achieved satisfying performance, and for each food type, at
least one model approached or surpassed 90% DPA, demonstrating strong alignment
in pixel-wise proportion estimates. Lighter models with reduced parameter
counts offered faster inference, achieving real-time throughput on an NVIDIA T4
GPU. Further analysis showed superior segmentation performance for dry and more
rigid components (e.g., rice and fries), while more complex, fragmented, or
viscous dishes, such as stews, showed reduced performance, specifically
post-consumption. Despite limitations such as reliance on 2D imaging,
constrained food variety, and manual data collection, the proposed framework is
pioneering and represents a scalable, contactless solution for continuous
monitoring of food consumption. This research lays foundational groundwork for
automated, real-time waste tracking systems in large-scale food service
environments and offers actionable insights and outlines feasible future
directions for dining hall management and policymakers aiming to reduce
institutional food waste.

</details>


### [37] [Gene-DML: Dual-Pathway Multi-Level Discrimination for Gene Expression Prediction from Histopathology Images](https://arxiv.org/abs/2507.14670)
*Yaxuan Song,Jianan Fan,Hang Chang,Weidong Cai*

Main category: cs.CV

TL;DR: 本文提出了一种名为Gene-DML的框架，通过多层次的路径对齐技术，实现了形态学和基因表达模式间更好的配准，用于改善基因表达预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用组织病理图像与基因表达的跨模态表示对齐方面存在局限性，这限制了预测性能。亟需新的方法来提升组织病理图像对基因表达预测的准确性。

Method: 该论文提出Gene-DML框架，通过双路径多层次判别的方法结构化潜在空间。方法包括多尺度实例级路径对齐和跨层实例-组判别路径，使形态学和基因表达间的配准更加精准。

Result: 实验结果表明，Gene-DML在公共空间转录组数据集上的基因表达预测性能达到最新进展，表现优于现有方法。

Conclusion: 基于形态学和转录模态的精细配准，Gene-DML显著提升了预测的准确性和跨生物背景的泛化能力。代码和模型将很快开放。

Abstract: Accurately predicting gene expression from histopathology images offers a
scalable and non-invasive approach to molecular profiling, with significant
implications for precision medicine and computational pathology. However,
existing methods often underutilize the cross-modal representation alignment
between histopathology images and gene expression profiles across multiple
representational levels, thereby limiting their prediction performance. To
address this, we propose Gene-DML, a unified framework that structures latent
space through Dual-pathway Multi-Level discrimination to enhance correspondence
between morphological and transcriptional modalities. The multi-scale
instance-level discrimination pathway aligns hierarchical histopathology
representations extracted at local, neighbor, and global levels with gene
expression profiles, capturing scale-aware morphological-transcriptional
relationships. In parallel, the cross-level instance-group discrimination
pathway enforces structural consistency between individual (image/gene)
instances and modality-crossed (gene/image, respectively) groups, strengthening
the alignment across modalities. By jointly modelling fine-grained and
structural-level discrimination, Gene-DML is able to learn robust cross-modal
representations, enhancing both predictive accuracy and generalization across
diverse biological contexts. Extensive experiments on public spatial
transcriptomics datasets demonstrate that Gene-DML achieves state-of-the-art
performance in gene expression prediction. The code and checkpoints will be
released soon.

</details>


### [38] [Docopilot: Improving Multimodal Models for Document-Level Understanding](https://arxiv.org/abs/2507.14675)
*Yuchen Duan,Zhe Chen,Yusong Hu,Weiyun Wang,Shenglong Ye,Botian Shi,Lewei Lu,Qibin Hou,Tong Lu,Hongsheng Li,Jifeng Dai,Wenhai Wang*

Main category: cs.CV

TL;DR: 文中介绍了Doc-750K数据集及其优化文档级多模态理解的能力，以及开发的模型Docopilot相较现有方法的优势。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在复杂多页文档理解中表现不足，部分由于缺乏高质量文档级数据集，同时现有RAG方法存在多种问题，如检索上下文碎片化、多阶段错误积累等。

Method: 提出并构建了Doc-750K数据集，涵盖多样化文档结构和跨页依赖，在该数据集基础上开发了Docopilot模型，用以无需依赖RAG实现高效的文档级多模态理解。

Result: 实验表明Docopilot在文档理解任务和多轮交互中表现出更好的连贯性、准确性及效率，成为文档级多模态理解的新基准。

Conclusion: Docopilot模型依托Doc-750K数据集，提供了一种更加强大、高效的文档级多模态信息理解方案，推动了该领域的发展。

Abstract: Despite significant progress in multimodal large language models (MLLMs),
their performance on complex, multi-page document comprehension remains
inadequate, largely due to the lack of high-quality, document-level datasets.
While current retrieval-augmented generation (RAG) methods offer partial
solutions, they suffer from issues, such as fragmented retrieval contexts,
multi-stage error accumulation, and extra time costs of retrieval. In this
work, we present a high-quality document-level dataset, Doc-750K, designed to
support in-depth understanding of multimodal documents. This dataset includes
diverse document structures, extensive cross-page dependencies, and real
question-answer pairs derived from the original documents. Building on the
dataset, we develop a native multimodal model, Docopilot, which can accurately
handle document-level dependencies without relying on RAG. Experiments
demonstrate that Docopilot achieves superior coherence, accuracy, and
efficiency in document understanding tasks and multi-turn interactions, setting
a new baseline for document-level multimodal understanding. Data, code, and
models are released at https://github.com/OpenGVLab/Docopilot

</details>


### [39] [WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis](https://arxiv.org/abs/2507.14680)
*Xinheng Lyu,Yuci Liang,Wenting Chen,Meidan Ding,Jiaqi Yang,Guolin Huang,Daokun Zhang,Xiangjian He,Linlin Shen*

Main category: cs.CV

TL;DR: 提出WSI-Agents，结合多智能体系统提升多模态WSI分析的任务特定准确性和多任务适应性，与现有模型相比表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在处理全数字切片图像分析任务时，多任务能力强但具体任务性能不足，亟需一种平衡多样性与准确度的方法。

Method: 设计WSI-Agents，基于多智能体概念，包含任务分配模块、验证机制和总结模块，通过结合模型动物园、领域知识库与可视化图生成以实现高效任务分工与准确验证。

Result: 在多模态WSI基准测试中，WSI-Agents的表现优于现有的WSI多模态大语言模型和医疗智能体框架，并可广泛应用于多样任务。

Conclusion: WSI-Agents为数字病理分析领域提供创新的解决方案，在保任务适应广度的同时，提升了具体任务的准确性。

Abstract: Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel
tissue analysis across various pathological tasks. While recent advancements in
multi-modal large language models (MLLMs) allow multi-task WSI analysis through
natural language, they often underperform compared to task-specific models.
Collaborative multi-agent systems have emerged as a promising solution to
balance versatility and accuracy in healthcare, yet their potential remains
underexplored in pathology-specific domains. To address these issues, we
propose WSI-Agents, a novel collaborative multi-agent system for multi-modal
WSI analysis. WSI-Agents integrates specialized functional agents with robust
task allocation and verification mechanisms to enhance both task-specific
accuracy and multi-task versatility through three components: (1) a task
allocation module assigning tasks to expert agents using a model zoo of patch
and WSI level MLLMs, (2) a verification mechanism ensuring accuracy through
internal consistency checks and external validation using pathology knowledge
bases and domain-specific models, and (3) a summary module synthesizing the
final summary with visual interpretation maps. Extensive experiments on
multi-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs
and medical agent frameworks across diverse tasks.

</details>


### [40] [From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition](https://arxiv.org/abs/2507.14686)
*Chen Cai,Tianyi Liu,Jianjun Gao,Wenyang Liu,Kejun Wu,Ruoyu Wang,Yi Wang,Soo Chin Liew*

Main category: cs.CV

TL;DR: 本文提出了一种名为MIPD的新框架，用于提升小型GSR模型的泛化和零样本能力。


<details>
  <summary>Details</summary>
Motivation: 目前的MLLMs在复杂场景中表现不佳，传统GSR模型在面对未见情况和稀有情况的泛化能力较差。

Method: 提出MIPD框架，将基础模型中的多模态知识通过互动提示蒸馏转移到学生模型中，同时引入JRG模块生成正负示例提示，并通过NMPA模块对齐视觉信息和多模态知识。

Result: 在Ov-SWiG和HICO-DET数据集上取得了优异的性能，尤其是在未见和稀有情况的检测上有显著提升。

Conclusion: 所提方法显著提升了GSR模型的泛化能力，弥补了传统方法在稀有和未见场景中的预测偏差。

Abstract: Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot
abilities but struggle with complex Grounded Situation Recognition (GSR) and
are resource-intensive for edge device deployment. Meanwhile, conventional GSR
models often lack generalization ability, falling short in recognizing unseen
and rare situations. In this paper, we exploit transferring knowledge from a
teacher MLLM to a small GSR model to enhance its generalization and zero-shot
abilities, thereby introducing the task of Open-vocabulary Grounded Situation
Recognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt
Distillation (MIPD), a novel framework that distills enriched multimodal
knowledge from the foundation model, enabling the student Ov-GSR model to
recognize unseen situations and be better aware of rare situations.
Specifically, the MIPD framework first leverages the LLM-based Judgmental
Rationales Generator (JRG) to construct positive and negative glimpse and gaze
rationales enriched with contextual semantic information. The proposed
scene-aware and instance-perception prompts are then introduced to align
rationales with visual information from the MLLM teacher via the
Negative-Guided Multimodal Prompting Alignment (NMPA) module, effectively
capturing holistic and perceptual multimodal knowledge. Finally, the aligned
multimodal knowledge is distilled into the student Ov-GSR model, providing a
stronger foundation for generalization that enhances situation understanding,
bridges the gap between seen and unseen scenarios, and mitigates prediction
bias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving
superior performance on seen, rare, and unseen situations, and further
demonstrate improved unseen detection on the HICO-DET dataset.

</details>


### [41] [GTPBD: A Fine-Grained Global Terraced Parcel and Boundary Dataset](https://arxiv.org/abs/2507.14697)
*Zhiwei Zhang,Zi Ye,Yibin Wen,Shuai Yuan,Haohuan Fu,Jianxi Huang,Juepeng Zheng*

Main category: cs.CV

TL;DR: 研究推出了首个全球梯田地块与边界数据集(GTPBD)，具有精细标注、高分辨率图像和多样性地形特征。


<details>
  <summary>Details</summary>
Motivation: 现有地块提取研究主要关注中分辨率或常规地形，但对复杂梯田地形重视不足，为实现精准农业发展需要精细化数据集。

Method: 提出GTPBD数据集，包含超过200,000个复杂梯田地块，基于47,537幅高分辨率图像手工标注三类标签，并对8种语义分割、4种边界提取、3种地块提取和5种无监督领域适应方法进行了基准测试。

Result: GTPBD数据集在语义分割、边缘检测、梯田地块提取，以及跨场景知识迁移任务中展现多样性和挑战性，弥补了梯田遥感研究领域的空白。

Conclusion: GTPBD为农业地形精细分析和跨场景应用提供重要基础设施，具有重大科学与应用价值。

Abstract: Agricultural parcels serve as basic units for conducting agricultural
practices and applications, which is vital for land ownership registration,
food security assessment, soil erosion monitoring, etc. However, existing
agriculture parcel extraction studies only focus on mid-resolution mapping or
regular plain farmlands while lacking representation of complex terraced
terrains due to the demands of precision agriculture.In this paper, we
introduce a more fine-grained terraced parcel dataset named GTPBD (Global
Terraced Parcel and Boundary Dataset), which is the first fine-grained dataset
covering major worldwide terraced regions with more than 200,000 complex
terraced parcels with manual annotation. GTPBD comprises 47,537 high-resolution
images with three-level labels, including pixel-level boundary labels, mask
labels, and parcel labels. It covers seven major geographic zones in China and
transcontinental climatic regions around the world.Compared to the existing
datasets, the GTPBD dataset brings considerable challenges due to the: (1)
terrain diversity; (2) complex and irregular parcel objects; and (3) multiple
domain styles. Our proposed GTPBD dataset is suitable for four different tasks,
including semantic segmentation, edge detection, terraced parcel extraction,
and unsupervised domain adaptation (UDA) tasks.Accordingly, we benchmark the
GTPBD dataset on eight semantic segmentation methods, four edge extraction
methods, three parcel extraction methods, and five UDA methods, along with a
multi-dimensional evaluation framework integrating pixel-level and object-level
metrics. GTPBD fills a critical gap in terraced remote sensing research,
providing a basic infrastructure for fine-grained agricultural terrain analysis
and cross-scenario knowledge transfer.

</details>


### [42] [MultiRetNet: A Multimodal Vision Model and Deferral System for Staging Diabetic Retinopathy](https://arxiv.org/abs/2507.14738)
*Jeannie She,Katie Spivakovsky*

Main category: cs.CV

TL;DR: 探讨了通过结合视网膜影像、社会经济因素与共病信息，提高糖尿病性视网膜病变（DR）分级准确性的多模态模型——MultiRetNet。


<details>
  <summary>Details</summary>
Motivation: 针对低收入社区居民因缺乏筛查渠道导致的DR晚期发现及疾病快速进展的问题。

Method: 提出MultiRetNet模型，将视网膜影像、社会经济因素与共病信息结合，探索三种多模态融合方法并应用对比学习，同时引入临床医生审核机制，识别需进一步判断的样本。

Result: 通过使用对抗性低质量图像与对比学习，系统在低质量图像情况下仍保持诊断准确性，并结合健康数据进行早期检测。

Conclusion: 该方法能改善早期检测结果，降低医疗成本，增加早期检测率，并有助于减少医疗资源分配不平等问题，促进医疗公平。

Abstract: Diabetic retinopathy (DR) is a leading cause of preventable blindness,
affecting over 100 million people worldwide. In the United States, individuals
from lower-income communities face a higher risk of progressing to advanced
stages before diagnosis, largely due to limited access to screening. Comorbid
conditions further accelerate disease progression. We propose MultiRetNet, a
novel pipeline combining retinal imaging, socioeconomic factors, and
comorbidity profiles to improve DR staging accuracy, integrated with a clinical
deferral system for a clinical human-in-the-loop implementation. We experiment
with three multimodal fusion methods and identify fusion through a fully
connected layer as the most versatile methodology. We synthesize adversarial,
low-quality images and use contrastive learning to train the deferral system,
guiding the model to identify out-of-distribution samples that warrant
clinician review. By maintaining diagnostic accuracy on suboptimal images and
integrating critical health data, our system can improve early detection,
particularly in underserved populations where advanced DR is often first
identified. This approach may reduce healthcare costs, increase early detection
rates, and address disparities in access to care, promoting healthcare equity.

</details>


### [43] [InterAct-Video: Reasoning-Rich Video QA for Urban Traffic](https://arxiv.org/abs/2507.14743)
*Joseph Raj Vishal,Rutuja Patil,Manas Srinivas Gowda,Katha Naik,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

TL;DR: 本文介绍了一个名为InterAct VideoQA的数据集，用于评估和改进交通监控相关的视频问答（VideoQA）模型。


<details>
  <summary>Details</summary>
Motivation: 现有的视频问答模型在面对复杂的交通场景中表现不佳，因此需要一个专门的数据集来支持模型在交通监控任务中的发展。

Method: 构建了InterAct VideoQA数据集，包括8小时真实交通录像视频片段及多达25000个问题答案对，涵盖了时空动态、车辆互动、事故检测等复杂问题。并对当前最先进的VideoQA模型在该数据集上的表现进行了评测及微调。

Result: 实验表明现有模型在处理细粒度时空依赖和复杂交通问题上存在较大挑战，但通过在该数据集上的微调，模型性能有显著提升。

Conclusion: 该数据集展现了领域特定数据集在增强VideoQA模型中不可或缺的作用，并为智能交通系统的发展提供了一个公开的基准数据集。

Abstract: Traffic monitoring is crucial for urban mobility, road safety, and
intelligent transportation systems (ITS). Deep learning has advanced
video-based traffic monitoring through video question answering (VideoQA)
models, enabling structured insight extraction from traffic videos. However,
existing VideoQA models struggle with the complexity of real-world traffic
scenes, where multiple concurrent events unfold across spatiotemporal
dimensions. To address these challenges, this paper introduces \textbf{InterAct
VideoQA}, a curated dataset designed to benchmark and enhance VideoQA models
for traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of
real-world traffic footage collected from diverse intersections, segmented into
10-second video clips, with over 25,000 question-answer (QA) pairs covering
spatiotemporal dynamics, vehicle interactions, incident detection, and other
critical traffic attributes. State-of-the-art VideoQA models are evaluated on
InterAct VideoQA, exposing challenges in reasoning over fine-grained
spatiotemporal dependencies within complex traffic scenarios. Additionally,
fine-tuning these models on InterAct VideoQA yields notable performance
improvements, demonstrating the necessity of domain-specific datasets for
VideoQA. InterAct VideoQA is publicly available as a benchmark dataset to
facilitate future research in real-world deployable VideoQA models for
intelligent transportation systems. GitHub Repo:
https://github.com/joe-rabbit/InterAct_VideoQA

</details>


### [44] [LeAdQA: LLM-Driven Context-Aware Temporal Grounding for Video Question Answering](https://arxiv.org/abs/2507.14784)
*Xinxin Dong,Baoyun Peng,Haokai Ma,Yufei Wang,Zixuan Dong,Fei Hu,Xiaodong Wang*

Main category: cs.CV

TL;DR: 提出LeAdQA方法，通过因果感知的查询优化和精细视觉定位提升视频问答性能，实现SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答方法在处理关键事件稀疏性和因果关系方面存在局限，难以准确回答语义复杂问题。

Method: 利用LLM优化问题选项对，加深因果与时间理解；再通过时间定位模型检索关键片段，结合自适应融合机制整合线索，从而生成情境化的答案。

Result: 在NExT-QA、IntentQA和NExT-GQA数据集上取得了SOTA性能，展现出卓越的复杂推理能力和计算效率。

Conclusion: LeAdQA通过因果和时间感知的统一方法，有效改善了视频问答任务中视觉文本对齐和推理能力。

Abstract: Video Question Answering (VideoQA) requires identifying sparse critical
moments in long videos and reasoning about their causal relationships to answer
semantically complex questions. While recent advances in multimodal learning
have improved alignment and fusion, current approaches remain limited by two
prevalent but fundamentally flawed strategies: (1) task-agnostic sampling
indiscriminately processes all frames, overwhelming key events with irrelevant
content; and (2) heuristic retrieval captures superficial patterns but misses
causal-temporal structures needed for complex reasoning. To address these
challenges, we introduce LeAdQA, an innovative approach that bridges these gaps
through synergizing causal-aware query refinement with fine-grained visual
grounding. Our method first leverages LLMs to reformulate question-option
pairs, resolving causal ambiguities and sharpening temporal focus. These
refined queries subsequently direct a temporal grounding model to precisely
retrieve the most salient segments, complemented by an adaptive fusion
mechanism dynamically integrating the evidence to maximize relevance. The
integrated visual-textual cues are then processed by an MLLM to generate
accurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and
NExT-GQA demonstrate that our method's precise visual grounding substantially
enhances the understanding of video-question relationships, achieving
state-of-the-art (SOTA) performance on complex reasoning tasks while
maintaining computational efficiency.

</details>


### [45] [FOCUS: Fused Observation of Channels for Unveiling Spectra](https://arxiv.org/abs/2507.14787)
*Xi Xiao,Aristeidis Tsaris,Anika Tabassum,John Lagergren,Larry M. York,Tianyang Wang,Xiao Wang*

Main category: cs.CV

TL;DR: 提出FOCUS框架，解决了现有视觉Transformer(ViT)在高光谱成像(HSI)中解释性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的ViT在HSI中的解释性受到挑战，现有显著性方法无法捕捉有意义的谱线特征，且高维的HSI数据使全谱ViT计算成本过高。

Method: FOCUS引入了类特异性光谱提示和可学习[SINK]标记，结合吸引损失以吸收冗余注意力，无需反向传播即可生成稳定的3D显著性图和光谱重要性曲线。

Result: FOCUS在带宽IoU提升15%，减少注意力崩溃40%，显著性结果与专家标注高度一致，且参数开销不足1%。

Conclusion: FOCUS为高分辨率ViT在HSI解释性中的实际应用提供了一个高效、可靠的解决方案，填补了黑箱建模与可信HSI决策之间的空白。

Abstract: Hyperspectral imaging (HSI) captures hundreds of narrow, contiguous
wavelength bands, making it a powerful tool in biology, agriculture, and
environmental monitoring. However, interpreting Vision Transformers (ViTs) in
this setting remains largely unexplored due to two key challenges: (1) existing
saliency methods struggle to capture meaningful spectral cues, often collapsing
attention onto the class token, and (2) full-spectrum ViTs are computationally
prohibitive for interpretability, given the high-dimensional nature of HSI
data. We present FOCUS, the first framework that enables reliable and efficient
spatial-spectral interpretability for frozen ViTs. FOCUS introduces two core
components: class-specific spectral prompts that guide attention toward
semantically meaningful wavelength groups, and a learnable [SINK] token trained
with an attraction loss to absorb noisy or redundant attention. Together, these
designs make it possible to generate stable and interpretable 3D saliency maps
and spectral importance curves in a single forward pass, without any gradient
backpropagation or backbone modification. FOCUS improves band-level IoU by 15
percent, reduces attention collapse by over 40 percent, and produces saliency
results that align closely with expert annotations. With less than 1 percent
parameter overhead, our method makes high-resolution ViT interpretability
practical for real-world hyperspectral applications, bridging a long-standing
gap between black-box modeling and trustworthy HSI decision-making.

</details>


### [46] [A Novel Downsampling Strategy Based on Information Complementarity for Medical Image Segmentation](https://arxiv.org/abs/2507.14790)
*Wenbo Yue,Chang Li,Guoping Xu*

Main category: cs.CV

TL;DR: 本文提出了一种基于信息互补的新型下采样方法——混合池化下采样（HPD），通过MinMaxPooling替代传统方法，提升语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 传统下采样方法在语义分割任务中可能会导致空间信息的丢失，影响逐像素预测的准确性。

Method: 提出了混合池化下采样（HPD）方法，采用MinMaxPooling策略，有效保留图像的明暗对比和细节特征。

Result: 在ACDC和Synapse数据集上验证，HPD模块显著提升了分割性能，平均提升DSC系数0.5%。

Conclusion: HPD为语义分割任务提供了一种高效的解决方案，保留了关键的空间信息并提升了性能。

Abstract: In convolutional neural networks (CNNs), downsampling operations are crucial
to model performance. Although traditional downsampling methods (such as
maximum pooling and cross-row convolution) perform well in feature aggregation,
receptive field expansion, and computational reduction, they may lead to the
loss of key spatial information in semantic segmentation tasks, thereby
affecting the pixel-by-pixel prediction accuracy.To this end, this study
proposes a downsampling method based on information complementarity - Hybrid
Pooling Downsampling (HPD). The core is to replace the traditional method with
MinMaxPooling, and effectively retain the light and dark contrast and detail
features of the image by extracting the maximum value information of the local
area.Experiment on various CNN architectures on the ACDC and Synapse datasets
show that HPD outperforms traditional methods in segmentation performance, and
increases the DSC coefficient by 0.5% on average. The results show that the HPD
module provides an efficient solution for semantic segmentation tasks.

</details>


### [47] [Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models](https://arxiv.org/abs/2507.14797)
*Beier Zhu,Ruoyu Wang,Tong Zhao,Hanwang Zhang,Chi Zhang*

Main category: cs.CV

TL;DR: 本文提出一种新的ODE求解器——EPD，利用并行方向的梯度评估提高抽样速度并减少误差，同时保证生成图像的质量。


<details>
  <summary>Details</summary>
Motivation: 目前扩散模型生成性能卓越但抽样延迟高，现有的加速方法在低延迟下无法保证图像质量。该研究旨在解决这一问题。

Method: 引入一种名为Ensemble Parallel Direction (EPD)的新型ODE求解器，通过多梯度并行评估降低截断误差，同时优化少量可学习参数以确保低训练开销。

Result: 在CIFAR-10、FFHQ、ImageNet和LSUN等数据集上，EPD在相同延迟（5 NFE）下实现的FID分别是4.47、7.97、8.17和8.26，显著优于现有方法。

Conclusion: EPD在提高抽样速度的同时维持了生成结果的高质量，可以作为现有ODE采样器的插件，有潜力在实际应用中得到广泛应用。

Abstract: Diffusion models (DMs) have achieved state-of-the-art generative performance
but suffer from high sampling latency due to their sequential denoising nature.
Existing solver-based acceleration methods often face image quality degradation
under a low-latency budget. In this paper, we propose the Ensemble Parallel
Direction solver (dubbed as \ours), a novel ODE solver that mitigates
truncation errors by incorporating multiple parallel gradient evaluations in
each ODE step. Importantly, since the additional gradient computations are
independent, they can be fully parallelized, preserving low-latency sampling.
  Our method optimizes a small set of learnable parameters in a distillation
fashion, ensuring minimal training overhead.
  In addition, our method can serve as a plugin to improve existing ODE
samplers. Extensive experiments on various image synthesis benchmarks
demonstrate the effectiveness of our \ours~in achieving high-quality and
low-latency sampling. For example, at the same latency level of 5 NFE, EPD
achieves an FID of 4.47 on CIFAR-10, 7.97 on FFHQ, 8.17 on ImageNet, and 8.26
on LSUN Bedroom, surpassing existing learning-based solvers by a significant
margin. Codes are available in https://github.com/BeierZhu/EPD.

</details>


### [48] [An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks](https://arxiv.org/abs/2507.14798)
*Xinyi Wu,Steven Landgraf,Markus Ulrich,Rongjun Qin*

Main category: cs.CV

TL;DR: 研究评估了DUSt3R、MASt3R和VGGT算法在稀疏航拍图片上进行3D重建和姿态估计的效果，结果显示其性能优于COLMAP，但在高分辨率和复杂场景中表现有限。


<details>
  <summary>Details</summary>
Motivation: 探索DUSt3R、MASt3R和VGGT等3D重建模型在稀疏航拍图片上的潜力，填补它们在摄影测量领域研究的空白。

Method: 使用预训练的DUSt3R、MASt3R和VGGT模型，在UseGeo数据集的航拍图片上进行姿态估计和稠密3D重建，比较其性能与COLMAP基准的差异。

Result: 这些模型能从少于10张分辨率高达518像素的稀疏图片中准确生成稠密点云，点云完整性较COLMAP提高了+50%。VGGT在计算效率、扩展性和相机姿态估计上表现更优，但大规模、复杂几何情况下仍有局限。

Conclusion: 变换器模型在低分辨率和稀疏场景中可作为传统结构从运动（SfM）和多视角立体（MVS）的补充方法，但无法完全取代它们。

Abstract: State-of-the-art 3D computer vision algorithms continue to advance in
handling sparse, unordered image sets. Recently developed foundational models
for 3D reconstruction, such as Dense and Unconstrained Stereo 3D Reconstruction
(DUSt3R), Matching and Stereo 3D Reconstruction (MASt3R), and Visual Geometry
Grounded Transformer (VGGT), have attracted attention due to their ability to
handle very sparse image overlaps. Evaluating DUSt3R/MASt3R/VGGT on typical
aerial images matters, as these models may handle extremely low image overlaps,
stereo occlusions, and textureless regions. For redundant collections, they can
accelerate 3D reconstruction by using extremely sparsified image sets. Despite
tests on various computer vision benchmarks, their potential on photogrammetric
aerial blocks remains unexplored. This paper conducts a comprehensive
evaluation of the pre-trained DUSt3R/MASt3R/VGGT models on the aerial blocks of
the UseGeo dataset for pose estimation and dense 3D reconstruction. Results
show these methods can accurately reconstruct dense point clouds from very
sparse image sets (fewer than 10 images, up to 518 pixels resolution), with
completeness gains up to +50% over COLMAP. VGGT also demonstrates higher
computational efficiency, scalability, and more reliable camera pose
estimation. However, all exhibit limitations with high-resolution images and
large sets, as pose reliability declines with more images and geometric
complexity. These findings suggest transformer-based methods cannot fully
replace traditional SfM and MVS, but offer promise as complementary approaches,
especially in challenging, low-resolution, and sparse scenarios.

</details>


### [49] [Exploring Scalable Unified Modeling for General Low-Level Vision](https://arxiv.org/abs/2507.14801)
*Xiangyu Chen,Kaiwen Zhu,Yuandong Pu,Shuo Cao,Xiaohui Li,Wenlong Zhang,Yihao Liu,Yu Qiao,Jiantao Zhou,Chao Dong*

Main category: cs.CV

TL;DR: 提出了一个名为VPIP的框架，通过视觉任务提示引导模型完成各种低层视觉任务，展示了在多种任务中的出色性能及潜力。


<details>
  <summary>Details</summary>
Motivation: 统一建模低层次视觉任务的方法复杂，需克服任务和输出域的多样性。本研究的目的是开发一种能够灵活适应不同任务的通用低层次视觉框架。

Method: 提出VPIP框架，包括端到端的图像处理骨干网络、提示编码器、提示交互模块。开发统一模型GenLV，并利用大规模基准测试，扩展模型容量和任务多样性。

Result: 实验结果显示，该方法在不同低层视觉任务中表现出色，尤其在任务少样本场景下展现了较好的泛化能力和迁移能力。

Conclusion: VPIP框架在零样本、少样本和特定任务微调中展现了出色的适应能力，证明了其作为通用低层视觉建模基础的潜力与可扩展性。

Abstract: Low-level vision involves a wide spectrum of tasks, including image
restoration, enhancement, stylization, and feature extraction, which differ
significantly in both task formulation and output domains. To address the
challenge of unified modeling across such diverse tasks, we propose a Visual
task Prompt-based Image Processing (VPIP) framework that leverages input-target
image pairs as visual prompts to guide the model in performing a variety of
low-level vision tasks. The framework comprises an end-to-end image processing
backbone, a prompt encoder, and a prompt interaction module, enabling flexible
integration with various architectures and effective utilization of
task-specific visual representations. Based on this design, we develop a
unified low-level vision model, GenLV, and evaluate its performance across
multiple representative tasks. To explore the scalability of this approach, we
extend the framework along two dimensions: model capacity and task diversity.
We construct a large-scale benchmark consisting of over 100 low-level vision
tasks and train multiple versions of the model with varying scales.
Experimental results show that the proposed method achieves considerable
performance across a wide range of tasks. Notably, increasing the number of
training tasks enhances generalization, particularly for tasks with limited
data, indicating the model's ability to learn transferable representations
through joint training. Further evaluations in zero-shot generalization,
few-shot transfer, and task-specific fine-tuning scenarios demonstrate the
model's strong adaptability, confirming the effectiveness, scalability, and
potential of the proposed framework as a unified foundation for general
low-level vision modeling.

</details>


### [50] [Seeing Through Deepfakes: A Human-Inspired Framework for Multi-Face Detection](https://arxiv.org/abs/2507.14807)
*Juan Hu,Shaojing Fan,Terence Sim*

Main category: cs.CV

TL;DR: 提出了一种提升多脸Deepfake检测效果的新方法HICOM，借鉴了人类认知的四个关键线索，并通过实验证明了其优越性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法多专注于单脸深伪检测，在多脸场景中表现较差，亟需解决这一问题。

Method: 通过人类研究挖掘深伪检测的关键线索，提出HICOM框架，结合机器学习与大语言模型进行检测和解释。

Result: 在基准数据集上HICOM提升检测准确率3.3%-5.8%，显示了其对数据集中及未见数据的优越性能。

Conclusion: 结合人类认知线索的方法提高了多脸Deepfake检测效果，同时增强了结果的可解释性。

Abstract: Multi-face deepfake videos are becoming increasingly prevalent, often
appearing in natural social settings that challenge existing detection methods.
Most current approaches excel at single-face detection but struggle in
multi-face scenarios, due to a lack of awareness of crucial contextual cues. In
this work, we develop a novel approach that leverages human cognition to
analyze and defend against multi-face deepfake videos. Through a series of
human studies, we systematically examine how people detect deepfake faces in
social settings. Our quantitative analysis reveals four key cues humans rely
on: scene-motion coherence, inter-face appearance compatibility, interpersonal
gaze alignment, and face-body consistency. Guided by these insights, we
introduce \textsf{HICOM}, a novel framework designed to detect every fake face
in multi-face scenarios. Extensive experiments on benchmark datasets show that
\textsf{HICOM} improves average accuracy by 3.3\% in in-dataset detection and
2.8\% under real-world perturbations. Moreover, it outperforms existing methods
by 5.8\% on unseen datasets, demonstrating the generalization of human-inspired
cues. \textsf{HICOM} further enhances interpretability by incorporating an LLM
to provide human-readable explanations, making detection results more
transparent and convincing. Our work sheds light on involving human factors to
enhance defense against deepfakes.

</details>


### [51] [Light Future: Multimodal Action Frame Prediction via InstructPix2Pix](https://arxiv.org/abs/2507.14809)
*Zesen Zhong,Duomin Zhang,Yijia Li*

Main category: cs.CV

TL;DR: 该论文提出了一种高效、轻量化的方法，用于机器人动作预测，显著降低了计算成本和推理延迟，适合需要高精度运动轨迹预测的领域。


<details>
  <summary>Details</summary>
Motivation: 解决现有视频预测模型在机器人任务中计算成本高、推理延迟长的问题，并探索InstructPix2Pix模型在动态预测中的适用性。

Method: 通过微调InstructPix2Pix模型，构建一个深度学习视觉预测框架，从单张图像和文本指令中预测机器人未来100帧（10秒）的视觉场景，实现多模态未来帧预测。

Result: 实验表明，该方法相比其他基准模型，在SSIM和PSNR指标上表现更优，仅需单张图像和文本提示作为输入，同时显著降低了GPU需求和推理时间。

Conclusion: 该模型提出了一种轻量化且高效的机器人动作预测方法，不仅优于传统模型，还具备快速推断和多模态控制能力，适用于机器人及体育运动轨迹分析等应用场景。

Abstract: Predicting future motion trajectories is a critical capability across domains
such as robotics, autonomous systems, and human activity forecasting, enabling
safer and more intelligent decision-making. This paper proposes a novel,
efficient, and lightweight approach for robot action prediction, offering
significantly reduced computational cost and inference latency compared to
conventional video prediction models. Importantly, it pioneers the adaptation
of the InstructPix2Pix model for forecasting future visual frames in robotic
tasks, extending its utility beyond static image editing. We implement a deep
learning-based visual prediction framework that forecasts what a robot will
observe 100 frames (10 seconds) into the future, given a current image and a
textual instruction. We repurpose and fine-tune the InstructPix2Pix model to
accept both visual and textual inputs, enabling multimodal future frame
prediction. Experiments on the RoboTWin dataset (generated based on real-world
scenarios) demonstrate that our method achieves superior SSIM and PSNR compared
to state-of-the-art baselines in robot action prediction tasks. Unlike
conventional video prediction models that require multiple input frames, heavy
computation, and slow inference latency, our approach only needs a single image
and a text prompt as input. This lightweight design enables faster inference,
reduced GPU demands, and flexible multimodal control, particularly valuable for
applications like robotics and sports motion trajectory analytics, where motion
trajectory precision is prioritized over visual fidelity.

</details>


### [52] [SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models](https://arxiv.org/abs/2507.14811)
*Jiaji Zhang,Ruichao Sun,Hailiang Zhao,Jiaju Wu,Peng Chen,Hao Li,Xinkui Zhao,Kingsum Chow,Gang Xiong,Lin Ye,Shuiguang Deng*

Main category: cs.CV

TL;DR: 提出了SegQuant，一个适用于扩散模型的通用量化框架，旨在减少计算成本并提升模型的适用性和部署兼容性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型尽管生成能力强，但计算开销高，不适合资源有限或对延迟敏感的环境。量化，尤其是无需重新训练的后训练量化（PTQ），可以有效降低成本，但现有PTQ方法常依赖于特定架构的启发式规则，限制了其通用性和产业化应用。

Method: SegQuant框架结合段感知的图形量化策略（SegLinear）和双比例量化方案（DualScale），适应不同扩散模型架构并兼顾生成视觉效果的保真性。

Result: 提出的SegQuant框架在保持视觉生成效果的同时，与主流部署工具无缝兼容，展现了高度通用性和强大表现。

Conclusion: SegQuant不仅提升了扩散模型量化的灵活性和效果，还为实际应用带来更高的部署便利性，具有广泛的应用潜力。

Abstract: Diffusion models have demonstrated exceptional generative capabilities but
are computationally intensive, posing significant challenges for deployment in
resource-constrained or latency-sensitive environments. Quantization offers an
effective means to reduce model size and computational cost, with post-training
quantization (PTQ) being particularly appealing due to its compatibility with
pre-trained models without requiring retraining or training data. However,
existing PTQ methods for diffusion models often rely on architecture-specific
heuristics that limit their generalizability and hinder integration with
industrial deployment pipelines. To address these limitations, we propose
SegQuant, a unified quantization framework that adaptively combines
complementary techniques to enhance cross-model versatility. SegQuant consists
of a segment-aware, graph-based quantization strategy (SegLinear) that captures
structural semantics and spatial heterogeneity, along with a dual-scale
quantization scheme (DualScale) that preserves polarity-asymmetric activations,
which is crucial for maintaining visual fidelity in generated outputs. SegQuant
is broadly applicable beyond Transformer-based diffusion models, achieving
strong performance while ensuring seamless compatibility with mainstream
deployment tools.

</details>


### [53] [FinChart-Bench: Benchmarking Financial Chart Comprehension in Vision-Language Models](https://arxiv.org/abs/2507.14823)
*Dong Shu,Haoyang Yuan,Yuchen Wang,Yanguang Liu,Huopu Zhang,Haiyan Zhao,Mengnan Du*

Main category: cs.CV

TL;DR: 提出了针对金融图表分析的第一个基准数据集FinChart-Bench，并对25种先进的视觉语言模型进行了评估，揭示了当前模型性能的局限性。


<details>
  <summary>Details</summary>
Motivation: 填补金融图表领域中的空白，为复杂的时间结构和专业术语设定评测标准。

Method: 收集2015到2024年间的1,200张金融图表，标注7,016个问题，并通过该数据集评估25种视觉语言模型的表现。

Result: 揭示主要结果包括开源与闭源模型之间的差距缩小；模型升级可能导致性能下降；模型无法很好地遵循指令；高级模型在空间推理中表现有限，且不够可靠作为自动评估工具。

Conclusion: 目前的视觉语言模型在金融图表理解上存在显著局限，FinChart-Bench为该领域的研究提供了重要的数据支持。

Abstract: Large vision-language models (LVLMs) have made significant progress in chart
understanding. However, financial charts, characterized by complex temporal
structures and domain-specific terminology, remain notably underexplored. We
introduce FinChart-Bench, the first benchmark specifically focused on
real-world financial charts. FinChart-Bench comprises 1,200 financial chart
images collected from 2015 to 2024, each annotated with True/False (TF),
Multiple Choice (MC), and Question Answering (QA) questions, totaling 7,016
questions. We conduct a comprehensive evaluation of 25 state-of-the-art LVLMs
on FinChart-Bench. Our evaluation reveals critical insights: (1) the
performance gap between open-source and closed-source models is narrowing, (2)
performance degradation occurs in upgraded models within families, (3) many
models struggle with instruction following, (4) both advanced models show
significant limitations in spatial reasoning abilities, and (5) current LVLMs
are not reliable enough to serve as automated evaluators. These findings
highlight important limitations in current LVLM capabilities for financial
chart understanding. The FinChart-Bench dataset is available at
https://huggingface.co/datasets/Tizzzzy/FinChart-Bench.

</details>


### [54] [PHATNet: A Physics-guided Haze Transfer Network for Domain-adaptive Real-world Image Dehazing](https://arxiv.org/abs/2507.14826)
*Fu-Jen Tsai,Yan-Tsung Peng,Yen-Yu Lin,Chia-Wen Lin*

Main category: cs.CV

TL;DR: 本论文提出了PHATNet，通过将目标域的雾霾模式转移到源域无雾图像上，生成用于微调去雾模型的数据集，并使用自定义损失函数提升模型的性能。实验验证了PHATNet在真实世界图像去雾任务上的显著效果。


<details>
  <summary>Details</summary>
Motivation: 当前去雾模型在处理未见过的真实世界雾霾图像时，因训练数据有限而性能下降。为此，提出了一种灵活的领域自适应方法以提升测试时的去雾性能。

Method: 通过提出Physics-guided Haze Transfer Network (PHATNet)，设计一种转移目标域雾霾模式至源域无雾图像的网络，并引入'Haze-Transfer-Consistency'和'Content-Leakage'损失函数，提升模型的解耦能力，生成领域特定的微调数据集。

Result: 实验结果表明，PHATNet可以在基准真实世界图像去雾数据集上显著提升现有最先进去雾模型的性能。

Conclusion: PHATNet通过创新、有针对性的领域自适应技术，有效解决了因训练数据有限导致的去雾性能下降问题，并提供了新的解决思路。

Abstract: Image dehazing aims to remove unwanted hazy artifacts in images. Although
previous research has collected paired real-world hazy and haze-free images to
improve dehazing models' performance in real-world scenarios, these models
often experience significant performance drops when handling unseen real-world
hazy images due to limited training data. This issue motivates us to develop a
flexible domain adaptation method to enhance dehazing performance during
testing. Observing that predicting haze patterns is generally easier than
recovering clean content, we propose the Physics-guided Haze Transfer Network
(PHATNet) which transfers haze patterns from unseen target domains to
source-domain haze-free images, creating domain-specific fine-tuning sets to
update dehazing models for effective domain adaptation. Additionally, we
introduce a Haze-Transfer-Consistency loss and a Content-Leakage Loss to
enhance PHATNet's disentanglement ability. Experimental results demonstrate
that PHATNet significantly boosts state-of-the-art dehazing models on benchmark
real-world image dehazing datasets.

</details>


### [55] [Paired Image Generation with Diffusion-Guided Diffusion Models](https://arxiv.org/abs/2507.14833)
*Haoxuan Zhang,Wenju Cui,Yuzhu Cao,Tao Tan,Jie Liu,Yunsong Peng,Jian Zheng*

Main category: cs.CV

TL;DR: 提出了一种无需外部条件的配对图像生成方法，以改进数字乳腺断层摄影（DBT）图像中肿块病变分割任务的表现。


<details>
  <summary>Details</summary>
Motivation: 高密度乳腺组织造成病变区域的遮蔽性高，手动标注困难且耗时，导致模型训练数据匮乏。

Method: 通过训练额外的扩散引导器，提出了一种无需外部条件的配对图像生成方法，可生成配对图像和相应标注。

Result: 实验生成了配对的DBT图像切片和病变掩膜，并将其融入肿块病变分割任务的监督训练中，获得了较高的生成质量。

Conclusion: 该方法能够在无外部条件的情况下提高生成质量，缓解标注数据不足问题，从而增强下游任务表现。

Abstract: The segmentation of mass lesions in digital breast tomosynthesis (DBT) images
is very significant for the early screening of breast cancer. However, the
high-density breast tissue often leads to high concealment of the mass lesions,
which makes manual annotation difficult and time-consuming. As a result, there
is a lack of annotated data for model training. Diffusion models are commonly
used for data augmentation, but the existing methods face two challenges.
First, due to the high concealment of lesions, it is difficult for the model to
learn the features of the lesion area. This leads to the low generation quality
of the lesion areas, thus limiting the quality of the generated images. Second,
existing methods can only generate images and cannot generate corresponding
annotations, which restricts the usability of the generated images in
supervised training. In this work, we propose a paired image generation method.
The method does not require external conditions and can achieve the generation
of paired images by training an extra diffusion guider for the conditional
diffusion model. During the experimental phase, we generated paired DBT slices
and mass lesion masks. Then, we incorporated them into the supervised training
process of the mass lesion segmentation task. The experimental results show
that our method can improve the generation quality without external conditions.
Moreover, it contributes to alleviating the shortage of annotated data, thus
enhancing the performance of downstream tasks.

</details>


### [56] [Training Self-Supervised Depth Completion Using Sparse Measurements and a Single Image](https://arxiv.org/abs/2507.14845)
*Rizhao Fan,Zhigen Li,Heping Li,Ning An*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的自监督深度补全框架，仅需稀疏的深度测量数据及其相应图像进行训练，无需密集深度标签或额外视点的图像。


<details>
  <summary>Details</summary>
Motivation: 现有深度补全方法面临获取密集深度标签成本高以及多帧依赖性的限制，因此需要一种更高效的方法对稀疏深度进行补全。

Method: 提出了基于稀疏深度测量和对应图像的新型自监督深度补全方法，通过设计新的损失函数在已观测点和未观测区域间传播深度信息，并结合分割图以提升深度估计性能。

Result: 实验结果证明了所提出方法在深度补全任务中的有效性。

Conclusion: 该方法在不依赖密集深度标签或多视点图像的情况下，实现了高质量的深度补全，这是对现有方法的重要改进。

Abstract: Depth completion is an important vision task, and many efforts have been made
to enhance the quality of depth maps from sparse depth measurements. Despite
significant advances, training these models to recover dense depth from sparse
measurements remains a challenging problem. Supervised learning methods rely on
dense depth labels to predict unobserved regions, while self-supervised
approaches require image sequences to enforce geometric constraints and
photometric consistency between frames. However, acquiring dense annotations is
costly, and multi-frame dependencies limit the applicability of self-supervised
methods in static or single-frame scenarios. To address these challenges, we
propose a novel self-supervised depth completion paradigm that requires only
sparse depth measurements and their corresponding image for training. Unlike
existing methods, our approach eliminates the need for dense depth labels or
additional images captured from neighboring viewpoints. By leveraging the
characteristics of depth distribution, we design novel loss functions that
effectively propagate depth information from observed points to unobserved
regions. Additionally, we incorporate segmentation maps generated by vision
foundation models to further enhance depth estimation. Extensive experiments
demonstrate the effectiveness of our proposed method.

</details>


### [57] [Grounding Degradations in Natural Language for All-In-One Video Restoration](https://arxiv.org/abs/2507.14851)
*Muhammad Kamran Janjua,Amirhosein Ghasemabadi,Kunlin Zhang,Mohammad Salameh,Chao Gao,Di Niu*

Main category: cs.CV

TL;DR: 提出了一种利用基础模型的自然语言能力实现视频恢复的新框架，并创建了新的基准测试集。


<details>
  <summary>Details</summary>
Motivation: 在没有退化知识假设的情况下提高视频恢复的灵活性和解释性，同时促进基准测试的标准化。

Method: 通过基础模型将视频帧的语义上下文与退化情况绑定，模型学习近似这些知识以实现更高效推理。此外，提出新的基准测试集用于评估。

Result: 在所有基准上均取得了最新的业界最佳表现。

Conclusion: 框架具有可解释性和灵活性，为视频恢复任务提供了统一解决方案，同时提出的基准推动领域发展。

Abstract: In this work, we propose an all-in-one video restoration framework that
grounds degradation-aware semantic context of video frames in natural language
via foundation models, offering interpretable and flexible guidance. Unlike
prior art, our method assumes no degradation knowledge in train or test time
and learns an approximation to the grounded knowledge such that the foundation
model can be safely disentangled during inference adding no extra cost.
Further, we call for standardization of benchmarks in all-in-one video
restoration, and propose two benchmarks in multi-degradation setting,
three-task (3D) and four-task (4D), and two time-varying composite degradation
benchmarks; one of the latter being our proposed dataset with varying snow
intensity, simulating how weather degradations affect videos naturally. We
compare our method with prior works and report state-of-the-art performance on
all benchmarks.

</details>


### [58] [An Uncertainty-aware DETR Enhancement Framework for Object Detection](https://arxiv.org/abs/2507.14855)
*Xingshu Chen,Sicheng Yu,Chong Cheng,Hao Wang,Ting Tian*

Main category: cs.CV

TL;DR: 本文提出一种不确定性感知的DETR增强框架，通过将边界框建模为多变量高斯分布并引入Gromov-Wasserstein距离，提高边界框定位精度与模型的鲁棒性，实验验证其在COCO及细胞检测任务上的优势。


<details>
  <summary>Details</summary>
Motivation: 现有目标检测器忽略预测中的不确定性，限制了模型鲁棒性，须改进定位精度并显式建模预测不确定性。

Method: 提出基于DETR的增强框架，将边界框建模为高斯分布，损失函数中引入Gromov-Wasserstein距离，并基于贝叶斯风险滤除高风险信息，同时提出简单算法量化定位不确定性。

Result: 在COCO基准测试中提升现有DETR模型性能，在细胞检测任务LISC和WBCDD数据集上取得先进结果，展示了框架的通用性与扩展性。

Conclusion: 所提出的不确定性感知增强框架能够显著提高目标检测系统的性能与可靠性，其通用性适用于多种检测任务。

Abstract: This paper investigates the problem of object detection with a focus on
improving both the localization accuracy of bounding boxes and explicitly
modeling prediction uncertainty. Conventional detectors rely on deterministic
bounding box regression, ignoring uncertainty in predictions and limiting model
robustness. In this paper, we propose an uncertainty-aware enhancement
framework for DETR-based object detectors. We model bounding boxes as
multivariate Gaussian distributions and incorporate the Gromov-Wasserstein
distance into the loss function to better align the predicted and ground-truth
distributions. Building on this, we derive a Bayes Risk formulation to filter
high-risk information and improve detection reliability. We also propose a
simple algorithm to quantify localization uncertainty via confidence intervals.
Experiments on the COCO benchmark show that our method can be effectively
integrated into existing DETR variants, enhancing their performance. We further
extend our framework to leukocyte detection tasks, achieving state-of-the-art
results on the LISC and WBCDD datasets. These results confirm the scalability
of our framework across both general and domain-specific detection tasks. Code
page:
https://github.com/ParadiseforAndaChen/An-Uncertainty-aware-DETR-Enhancement-Framework-for-Object-Detection.

</details>


### [59] [Hybrid-supervised Hypergraph-enhanced Transformer for Micro-gesture Based Emotion Recognition](https://arxiv.org/abs/2507.14867)
*Zhaoqiang Xia,Hexiang Huang,Haoyu Chen,Xiaoyi Feng,Guoying Zhao*

Main category: cs.CV

TL;DR: 该论文提出了基于微动作建模人类情绪的新方法，利用增强的超图Transformer结合深度学习技术，最终实现了在两大公开数据集下的最佳情绪识别性能。


<details>
  <summary>Details</summary>
Motivation: 当前微动作在情绪建模中的研究不足，而其却具有潜力理解人类情绪状态。

Method: 构建了基于增强超图Transformer的混合监督框架，包括利用超图增强自注意力和多尺度时间卷积模块的编码器和解码器。通过解码器的上采样操作实现自监督学习的重构任务，并设计浅层情绪识别头进行监督学习，框架整体一体化端到端训练。

Result: 该方法在iMiGUE和SMG两个公开数据集下的多项指标均达到最佳性能，优于已有方法。

Conclusion: 利用超图和Transformer技术在微动作建模情绪中显示出明显优势，为人类行为建模提供创新路径。

Abstract: Micro-gestures are unconsciously performed body gestures that can convey the
emotion states of humans and start to attract more research attention in the
fields of human behavior understanding and affective computing as an emerging
topic. However, the modeling of human emotion based on micro-gestures has not
been explored sufficiently. In this work, we propose to recognize the emotion
states based on the micro-gestures by reconstructing the behavior patterns with
a hypergraph-enhanced Transformer in a hybrid-supervised framework. In the
framework, hypergraph Transformer based encoder and decoder are separately
designed by stacking the hypergraph-enhanced self-attention and multiscale
temporal convolution modules. Especially, to better capture the subtle motion
of micro-gestures, we construct a decoder with additional upsampling operations
for a reconstruction task in a self-supervised learning manner. We further
propose a hypergraph-enhanced self-attention module where the hyperedges
between skeleton joints are gradually updated to present the relationships of
body joints for modeling the subtle local motion. Lastly, for exploiting the
relationship between the emotion states and local motion of micro-gestures, an
emotion recognition head from the output of encoder is designed with a shallow
architecture and learned in a supervised way. The end-to-end framework is
jointly trained in a one-stage way by comprehensively utilizing
self-reconstruction and supervision information. The proposed method is
evaluated on two publicly available datasets, namely iMiGUE and SMG, and
achieves the best performance under multiple metrics, which is superior to the
existing methods.

</details>


### [60] [Region-aware Depth Scale Adaptation with Sparse Measurements](https://arxiv.org/abs/2507.14879)
*Rizhao Fan,Tianfang Ma,Zhigen Li,Ning An,Jian Cheng*

Main category: cs.CV

TL;DR: 提出了一种利用稀疏深度测量将基础模型的相对尺度深度预测转换为度量尺度的新方法，无需重新训练或微调，保留了模型的强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前基础模型在深度预测中表现优异，但输出通常为相对尺度而非度量尺度，限制了其在实际应用中的直接部署能力，需要一种无需高成本训练的方法实现尺度适配。

Method: 采用一种非学习驱动的方法，利用稀疏深度信息将基础模型的相对尺度深度预测调整为度量尺度，不需重新训练或微调。

Result: 实验结果验证了方法的有效性，成功实现了在保持模型泛化能力的同时缩小相对与度量深度间的差距。

Conclusion: 该方法为相对深度到度量深度的转换提供了一种高效且泛化性强的解决方案，无需额外计算成本或损失原有模型性能。

Abstract: In recent years, the emergence of foundation models for depth prediction has
led to remarkable progress, particularly in zero-shot monocular depth
estimation. These models generate impressive depth predictions; however, their
outputs are often in relative scale rather than metric scale. This limitation
poses challenges for direct deployment in real-world applications. To address
this, several scale adaptation methods have been proposed to enable foundation
models to produce metric depth. However, these methods are typically costly, as
they require additional training on new domains and datasets. Moreover,
fine-tuning these models often compromises their original generalization
capabilities, limiting their adaptability across diverse scenes. In this paper,
we introduce a non-learning-based approach that leverages sparse depth
measurements to adapt the relative-scale predictions of foundation models into
metric-scale depth. Our method requires neither retraining nor fine-tuning,
thereby preserving the strong generalization ability of the original foundation
models while enabling them to produce metric depth. Experimental results
demonstrate the effectiveness of our approach, high-lighting its potential to
bridge the gap between relative and metric depth without incurring additional
computational costs or sacrificing generalization ability.

</details>


### [61] [BeatFormer: Efficient motion-robust remote heart rate estimation through unsupervised spectral zoomed attention filters](https://arxiv.org/abs/2507.14885)
*Joaquim Comas,Federico Sukno*

Main category: cs.CV

TL;DR: 提出了一种名为BeatFormer的轻量级模型，用于从面部视频中估算rPPG信号，同时引入频谱对比学习以实现无标签训练，并通过多个数据集验证了其在跨数据集及运动场景下的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法在rPPG信号估算上取得了进展，但需要大量多样的数据集以获得良好的泛化性；相反，基于手工特征的方法利用生理先验知识，在计算效率及未见场景（如运动）中表现更好，但在复杂条件下性能有限。因此，需要一种结合两者优点的混合方法。

Method: 提出了一种轻量级模型BeatFormer，结合缩放正交复数注意力和频域能量测量技术，设计了高效的模型结构，并创新性地引入了频谱对比学习（SCL），使其能够在没有PPG或HR标签的条件下进行训练。

Result: 通过PURE、UBFC-rPPG和MMPD数据集实验验证，BeatFormer在跨数据集评估和运动场景下的表现优于现有方法，表现出良好的鲁棒性。

Conclusion: BeatFormer将深度学习与生理先验相结合，以稳定的性能展现了对复杂应用场景的适用性，为rPPG信号估算提供了一种高效且无标签学习的解决方案。

Abstract: Remote photoplethysmography (rPPG) captures cardiac signals from facial
videos and is gaining attention for its diverse applications. While deep
learning has advanced rPPG estimation, it relies on large, diverse datasets for
effective generalization. In contrast, handcrafted methods utilize
physiological priors for better generalization in unseen scenarios like motion
while maintaining computational efficiency. However, their linear assumptions
limit performance in complex conditions, where deep learning provides superior
pulsatile information extraction. This highlights the need for hybrid
approaches that combine the strengths of both methods. To address this, we
present BeatFormer, a lightweight spectral attention model for rPPG estimation,
which integrates zoomed orthonormal complex attention and frequency-domain
energy measurement, enabling a highly efficient model. Additionally, we
introduce Spectral Contrastive Learning (SCL), which allows BeatFormer to be
trained without any PPG or HR labels. We validate BeatFormer on the PURE,
UBFC-rPPG, and MMPD datasets, demonstrating its robustness and performance,
particularly in cross-dataset evaluations under motion scenarios.

</details>


### [62] [TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP](https://arxiv.org/abs/2507.14904)
*Fan Li,Zanyi Wang,Zeyi Huang,Guang Dai,Jingdong Wang,Mengmeng Wang*

Main category: cs.CV

TL;DR: 提出了一种利用2D CLIP双模态模型的统一3D视觉对照网络，提升了模型的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉对照方法需要分别处理多种模态，模型复杂且训练效率低，而将2D预训练模型应用于3D任务面临点云数据与2D编码器对齐的挑战。

Method: 提出了一种基于2D CLIP的多模态网络，通过适配器微调，实现对RGB图像、文本和点云的统一处理，并设计了几何感知的2D-3D特征恢复与融合模块（GARF）。

Result: 与基线相比，模型的参数量减少约58%，3D检测任务性能提升6.52%，3D视觉对照任务性能提升6.25%。

Conclusion: 所提方法通过统一的特征提取与融合，实现了高效的端到端3D视觉对照，简化了模型架构，提升了性能。

Abstract: 3D visual grounding allows an embodied agent to understand visual information
in real-world 3D environments based on human instructions, which is crucial for
embodied intelligence. Existing 3D visual grounding methods typically rely on
separate encoders for different modalities (e.g., RGB images, text, and 3D
point clouds), resulting in large and complex models that are inefficient to
train. While some approaches use pre-trained 2D multi-modal models like CLIP
for 3D tasks, they still struggle with aligning point cloud data to 2D
encoders. As a result, these methods continue to depend on 3D encoders for
feature extraction, further increasing model complexity and training
inefficiency. In this paper, we propose a unified 2D pre-trained multi-modal
network to process all three modalities (RGB images, text, and point clouds),
significantly simplifying the architecture. By leveraging a 2D CLIP bi-modal
model with adapter-based fine-tuning, this framework effectively adapts to the
tri-modal setting, improving both adaptability and performance across
modalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module
is designed to fuse geometric multi-scale features from point clouds and
images. We then integrate textual features for final modality fusion and
introduce a multi-modal decoder to facilitate deep cross-modal understanding.
Together, our method achieves unified feature extraction and fusion across the
three modalities, enabling an end-to-end 3D visual grounding model. Compared to
the baseline, our method reduces the number of trainable parameters by
approximately 58\%, while achieving a 6.52\% improvement in the 3D detection
task and a 6.25\% improvement in the 3D visual grounding task.

</details>


### [63] [Semantic-Aware Representation Learning for Multi-label Image Classification](https://arxiv.org/abs/2507.14918)
*Ren-Dong Xie,Zhi-Fen He,Bo Li,Bin Liu,Jin-Yan Hu*

Main category: cs.CV

TL;DR: 提出了一种针对多标签图像分类的语义感知表示学习方法，包括语义相关特征提取、基于最优传输的注意力机制、区域得分聚合策略。


<details>
  <summary>Details</summary>
Motivation: 现有方法的图像表示可能包含噪声或定位不准确，亟需更高效的表示学习方法提升分类效果。

Method: 采用三种核心模块：语义相关特征学习模块提取特征、最优传输注意力机制提升语义对齐、区域得分聚合策略进行预测。

Result: 在PASCAL VOC 2007和MS-COCO两个基准数据集上表现优于现有方法。

Conclusion: 证明了SARL方法在多标签图像分类中的优越性，为图像表示学习提供了新的思路。

Abstract: Multi-label image classification, an important research area in computer
vision, focuses on identifying multiple labels or concepts within an image.
Existing approaches often employ attention mechanisms or graph convolutional
networks (GCNs) to learn image representation. However, this representation may
contain noise and may not locate objects precisely. Therefore, this paper
proposes a Semantic-Aware Representation Learning (SARL) for multi-label image
classification. First, a label semantic-related feature learning module is
utilized to extract semantic-related features. Then, an optimal transport-based
attention mechanism is designed to obtain semantically aligned image
representation. Finally, a regional score aggregation strategy is used for
multi-label prediction. Experimental results on two benchmark datasets, PASCAL
VOC 2007 and MS-COCO, demonstrate the superiority of SARL over existing
methods.

</details>


### [64] [Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian Splatting Reconstruction](https://arxiv.org/abs/2507.14921)
*Xiufeng Huang,Ka Chun Cheung,Runmin Cong,Simon See,Renjie Wan*

Main category: cs.CV

TL;DR: 本研究提出了一种高效的3D高斯预测框架，可用于无需相机参数的3D重建，从而改善了鲁棒性和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前3D高斯重建依赖于大量数据和计算资源，且几何与外观预测紧密耦合，速度慢，效率低。

Method: 结合局部影像特征提取和全局注意模块，利用独立的几何和外观预测头生成多视点图，并通过后处理网络优化，最终实现无需相机参数的高质量3D重建。

Result: 提出的框架能够在降低资源需求的同时保持高质量的3D重建效果。

Conclusion: 该方法为真实世界的高效3D内容生成提供了一种可扩展的解决方案。

Abstract: Generalizable 3D Gaussian Splatting reconstruction showcases advanced
Image-to-3D content creation but requires substantial computational resources
and large datasets, posing challenges to training models from scratch. Current
methods usually entangle the prediction of 3D Gaussian geometry and appearance,
which rely heavily on data-driven priors and result in slow regression speeds.
To address this, we propose \method, a disentangled framework for efficient 3D
Gaussian prediction. Our method extracts features from local image pairs using
a stereo vision backbone and fuses them via global attention blocks. Dedicated
point and Gaussian prediction heads generate multi-view point-maps for geometry
and Gaussian features for appearance, combined as GS-maps to represent the 3DGS
object. A refinement network enhances these GS-maps for high-quality
reconstruction. Unlike existing methods that depend on camera parameters, our
approach achieves pose-free 3D reconstruction, improving robustness and
practicality. By reducing resource demands while maintaining high-quality
outputs, \method provides an efficient, scalable solution for real-world 3D
content generation.

</details>


### [65] [3-Dimensional CryoEM Pose Estimation and Shift Correction Pipeline](https://arxiv.org/abs/2507.14924)
*Kaishva Chintan Shah,Virajith Boddapati,Karthik S. Gurumoorthy,Sandip Kaledhonkar,Ajit Rajwade*

Main category: cs.CV

TL;DR: 针对低信噪比（SNR）条件下冷冻电镜（cryo-EM）中的位姿估计和位移校正问题，引入基于多维定标（MDS）和共线角度分析的方法，结合鲁棒优化框架与迭代校正策略，提升三维重建的精度。


<details>
  <summary>Details</summary>
Motivation: 冷冻电镜的三维重建质量受到低SNR条件下的位姿估计和平移校正的限制；已有方法在噪声环境中精度受限，亟需更精确的优解方案。

Method: 提出一种基于MDS和鲁棒优化范式的新方法，采用$6_1$目标函数结合严格的正交性约束与投影坐标下降，并引入全局最小二乘求解的迭代校正机制。

Result: 所提出的管线在欧拉角准确性和基于傅里叶壳相关性（FSC）的重建保真度上均优于现有方法。

Conclusion: 通过构建鲁棒性更强的优化模型与校正算法，该研究显著提升了低SNR环境下cryo-EM位姿估计及三维重建效果。

Abstract: Accurate pose estimation and shift correction are key challenges in cryo-EM
due to the very low SNR, which directly impacts the fidelity of 3D
reconstructions. We present an approach for pose estimation in cryo-EM that
leverages multi-dimensional scaling (MDS) techniques in a robust manner to
estimate the 3D rotation matrix of each particle from pairs of dihedral angles.
We express the rotation matrix in the form of an axis of rotation and a unit
vector in the plane perpendicular to the axis. The technique leverages the
concept of common lines in 3D reconstruction from projections. However, common
line estimation is ridden with large errors due to the very low SNR of cryo-EM
projection images. To address this challenge, we introduce two complementary
components: (i) a robust joint optimization framework for pose estimation based
on an $\ell_1$-norm objective or a similar robust norm, which simultaneously
estimates rotation axes and in-plane vectors while exactly enforcing unit norm
and orthogonality constraints via projected coordinate descent; and (ii) an
iterative shift correction algorithm that estimates consistent in-plane
translations through a global least-squares formulation. While prior approaches
have leveraged such embeddings and common-line geometry for orientation
recovery, existing formulations typically rely on $\ell_2$-based objectives
that are sensitive to noise, and enforce geometric constraints only
approximately. These choices, combined with a sequential pipeline structure,
can lead to compounding errors and suboptimal reconstructions in low-SNR
regimes. Our pipeline consistently outperforms prior methods in both Euler
angle accuracy and reconstruction fidelity, as measured by the Fourier Shell
Correlation (FSC).

</details>


### [66] [Probabilistic smooth attention for deep multiple instance learning in medical imaging](https://arxiv.org/abs/2507.14932)
*Francisco M. Castro-Macías,Pablo Morales-Álvarez,Yunan Wu,Rafael Molina,Aggelos K. Katsaggelos*

Main category: cs.CV

TL;DR: 本文提出了一种新的概率框架应用于多实例学习（MIL），显著提升了医疗影像分类的预测性能，同时提供了疾病定位的解释性不确定性图。


<details>
  <summary>Details</summary>
Motivation: 在医疗影像分析中，标注数据稀缺，多实例学习（MIL）通过仅需袋标签进行训练，已成为热门方向。然而现有方法对注意力值的处理过于确定性，可能忽略了个体实例贡献中的不确定性。

Method: 提出一种新的概率框架，该框架对注意力值进行概率分布估计，并同时考虑局部和全局交互。

Result: 综合评估表明，该方法在三个医学数据集和十一种最先进基线上表现优异，预测性能优秀。

Conclusion: 新方法不仅提升了预测能力，还生成了疾病定位相关的可解释性不确定性地图。

Abstract: The Multiple Instance Learning (MIL) paradigm is attracting plenty of
attention in medical imaging classification, where labeled data is scarce. MIL
methods cast medical images as bags of instances (e.g. patches in whole slide
images, or slices in CT scans), and only bag labels are required for training.
Deep MIL approaches have obtained promising results by aggregating
instance-level representations via an attention mechanism to compute the
bag-level prediction. These methods typically capture both local interactions
among adjacent instances and global, long-range dependencies through various
mechanisms. However, they treat attention values deterministically, potentially
overlooking uncertainty in the contribution of individual instances. In this
work we propose a novel probabilistic framework that estimates a probability
distribution over the attention values, and accounts for both global and local
interactions. In a comprehensive evaluation involving {\color{review} eleven}
state-of-the-art baselines and three medical datasets, we show that our
approach achieves top predictive performance in different metrics. Moreover,
the probabilistic treatment of the attention provides uncertainty maps that are
interpretable in terms of illness localization.

</details>


### [67] [Open-set Cross Modal Generalization via Multimodal Unified Representation](https://arxiv.org/abs/2507.14935)
*Hai Huang,Yan Xia,Shulei Wang,Hanting Wang,Minghui Fang,Shengpeng Ji,Sashuai Zhou,Tao Jin,Zhou Zhao*

Main category: cs.CV

TL;DR: 本文提出了开放集跨模态泛化任务 (OSCMG)，通过 MICU 方法解决跨模态知识转移和未知类别泛化问题。


<details>
  <summary>Details</summary>
Motivation: 目前多模态统一表征方法未考虑开放集环境，需要处理实际应用中频繁出现的未知类别问题。

Method: 引入 MICU 方法，包括细粗掩模多模态对比学习 (FCMI) 和跨模态拼图 (CUJP)，分别优化多模态对齐和增强特征多样性及模型不确定性。

Result: 在 CMG 和新的 OSCMG 任务上进行的大量实验验证了方法的有效性。

Conclusion: 该工作拓展了跨模态泛化领域，提出的方法可在开放集任务中更好应对未知类别和模态。

Abstract: This paper extends Cross Modal Generalization (CMG) to open-set environments
by proposing the more challenging Open-set Cross Modal Generalization (OSCMG)
task. This task evaluates multimodal unified representations in open-set
conditions, addressing the limitations of prior closed-set cross-modal
evaluations. OSCMG requires not only cross-modal knowledge transfer but also
robust generalization to unseen classes within new modalities, a scenario
frequently encountered in real-world applications. Existing multimodal unified
representation work lacks consideration for open-set environments. To tackle
this, we propose MICU, comprising two key components: Fine-Coarse Masked
multimodal InfoNCE (FCMI) and Cross modal Unified Jigsaw Puzzles (CUJP). FCMI
enhances multimodal alignment by applying contrastive learning at both holistic
semantic and temporal levels, incorporating masking to enhance generalization.
CUJP enhances feature diversity and model uncertainty by integrating
modality-agnostic feature selection with self-supervised learning, thereby
strengthening the model's ability to handle unknown categories in open-set
tasks. Extensive experiments on CMG and the newly proposed OSCMG validate the
effectiveness of our approach. The code is available at
https://github.com/haihuangcode/CMG.

</details>


### [68] [Polymorph: Energy-Efficient Multi-Label Classification for Video Streams on Embedded Devices](https://arxiv.org/abs/2507.14959)
*Saeid Ghafouri,Mohsen Fayyaz,Xiangchen Li,Deepu John,Bo Ji,Dimitrios Nikolopoulos,Hans Vandierendonck*

Main category: cs.CV

TL;DR: Polymorph是一种面向实时多标签视频分类的上下文感知框架，通过激活轻量级的低秩适配器(Low Rank Adapters)，减少能源消耗并提高性能。


<details>
  <summary>Details</summary>
Motivation: 嵌入式设备进行实时多标签视频分类受限于计算能力和能量预算，而视频流具有可利用的结构性特性，例如标签稀疏性和共现性。

Method: Polymorph根据标签共现模式为每个帧动态选择和组合最低数量的适配器，每个适配器专注于一部分类。适配器以共享骨干网络上的低秩权重实现，避免了完整模型切换及权重合并。

Result: 在TAO数据集上，Polymorph在强基线的基础上减少了40%的能耗，并将mAP提升了9个点。

Conclusion: 该方法在延迟和能源消耗方面取得了更好的伸缩性，同时开源代码为领域贡献了资源。

Abstract: Real-time multi-label video classification on embedded devices is constrained
by limited compute and energy budgets. Yet, video streams exhibit structural
properties such as label sparsity, temporal continuity, and label co-occurrence
that can be leveraged for more efficient inference. We introduce Polymorph, a
context-aware framework that activates a minimal set of lightweight Low Rank
Adapters (LoRA) per frame. Each adapter specializes in a subset of classes
derived from co-occurrence patterns and is implemented as a LoRA weight over a
shared backbone. At runtime, Polymorph dynamically selects and composes only
the adapters needed to cover the active labels, avoiding full-model switching
and weight merging. This modular strategy improves scalability while reducing
latency and energy overhead. Polymorph achieves 40% lower energy consumption
and improves mAP by 9 points over strong baselines on the TAO dataset.
Polymorph is open source at https://github.com/inference-serving/polymorph/.

</details>


### [69] [Decision PCR: Decision version of the Point Cloud Registration task](https://arxiv.org/abs/2507.14965)
*Yaojie Zhang,Tianlun Huang,Weijun Wang,Wei Feng*

Main category: cs.CV

TL;DR: 该论文提出了一种面向低重叠点云配准的深度学习框架，通过重新定义评估标准并构建数据驱动分类器来提升配准性能。


<details>
  <summary>Details</summary>
Motivation: 传统配准评估指标在极低内点比情况下失效。

Method: 引入数据驱动的深度学习分类器，基于3DMatch数据集构建专用数据集，并与现有配准方法结合。

Result: 结合该方法的配准系统在3DLoMatch基准上实现了86.97%的新SOTA性能，并展示了在ETH数据集上的强泛化能力。

Conclusion: 该方法有效改善低重叠点云配准效果，并验证了数据驱动分类器的可靠性和通用性。

Abstract: Low-overlap point cloud registration (PCR) remains a significant challenge in
3D vision. Traditional evaluation metrics, such as Maximum Inlier Count, become
ineffective under extremely low inlier ratios. In this paper, we revisit the
registration result evaluation problem and identify the Decision version of the
PCR task as the fundamental problem. To address this Decision PCR task, we
propose a data-driven approach. First, we construct a corresponding dataset
based on the 3DMatch dataset. Then, a deep learning-based classifier is trained
to reliably assess registration quality, overcoming the limitations of
traditional metrics. To our knowledge, this is the first comprehensive study to
address this task through a deep learning framework. We incorporate this
classifier into standard PCR pipelines. When integrated with our approach,
existing state-of-the-art PCR methods exhibit significantly enhanced
registration performance. For example, combining our framework with
GeoTransformer achieves a new SOTA registration recall of 86.97\% on the
challenging 3DLoMatch benchmark. Our method also demonstrates strong
generalization capabilities on the unseen outdoor ETH dataset.

</details>


### [70] [Hierarchical Cross-modal Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2507.14976)
*Hao Zheng,Shunzhi Yang,Zhuoxin He,Jinfeng Yang,Zhenhua Huang*

Main category: cs.CV

TL;DR: 本文提出了一个名为HiCroPL的新框架，用于改进预训练视觉语言模型（VLMs），通过跨模态交互提升下游任务的表现，取得了11个基准数据集的最好效果。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练的视觉语言模型如CLIP表现出良好的泛化能力，但如何在适应下游任务时保持这种能力仍有挑战，特别是现有的提示学习方法面临模态隔离和语义分层衰减的问题。

Method: HiCroPL框架通过在文本和视觉模态之间建立双向的知识流动，利用文本提示和视觉提示的相辅相成，在早期层面增强视觉提示语义，在后期层面对文本提示进行细化，同时通过层次知识映射器实现多尺度表示融合。此外，引入了一个轻量的层级知识代理，以提高跨模态交互的效率。

Result: 在四项任务上的广泛实验表明，HiCroPL取得了显著的性能提升，在11个基准数据集上达到了最先进的结果。

Conclusion: HiCroPL框架通过解决现有方法中的模态隔离和语义衰减问题，提高了预训练视觉语言模型的适应能力，同时保留其优越的泛化能力。

Abstract: Pre-trained Vision-Language Models (VLMs) such as CLIP have shown excellent
generalization abilities. However, adapting these large-scale models to
downstream tasks while preserving their generalization capabilities remains
challenging. Although prompt learning methods have shown promise, they suffer
from two fundamental bottlenecks that limit generalization: (a) modality
isolation, and (b) hierarchical semantic decay. To address these limitations,
we propose HiCroPL, a Hierarchical Cross-modal Prompt Learning framework that
establishes bidirectional knowledge flow between text and vision modalities,
enabling them to refine their semantics mutually. HiCroPL routes knowledge
flows by leveraging the complementary strengths of text and vision. In early
layers, text prompts inject relatively clear semantics into visual prompts
through a hierarchical knowledge mapper, enhancing the representation of
low-level visual semantics. In later layers, visual prompts encoding specific
task-relevant objects flow back to refine text prompts, enabling deeper
alignment. Crucially, our hierarchical knowledge mapper allows representations
at multi-scales to be fused, ensuring that deeper representations retain
transferable shallow semantics thereby enhancing generalization. We further
introduce a lightweight layer-specific knowledge proxy to enable efficient
cross-modal interactions. Extensive evaluations across four tasks demonstrate
HiCroPL's superior performance, achieving state-of-the-art results on 11
benchmarks with significant improvements. Code is available at:
https://github.com/zzeoZheng/HiCroPL.

</details>


### [71] [Language Integration in Fine-Tuning Multimodal Large Language Models for Image-Based Regression](https://arxiv.org/abs/2507.14997)
*Roy H. Jennings,Genady Paikin,Roy Shaul,Evgeny Soloveichik*

Main category: cs.CV

TL;DR: 提出了一种新的基于Transformer的分类方法（RvTC），改进了多模态大语言模型（MLLMs）在图像回归任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 算法需要克服预设词汇表和通用任务提示对模型性能的限制，提升模型对具体图像和语义的理解能力。

Method: 提出一种基于直方图分类的回归方式（RvTC），通过增加bin而非手动设计词汇表，并使用语义相关的提示信息提升性能。

Result: 在AVA等多个数据集上实现了性能新高；使用特定提示信息显著提高了相关性从0.83到0.90。

Conclusion: 表明MLLMs可以从语义提示中获益，强调了在多模态回归任务中引入有意义文本上下文的重要性。

Abstract: Multimodal Large Language Models (MLLMs) show promise for image-based
regression tasks, but current approaches face key limitations. Recent methods
fine-tune MLLMs using preset output vocabularies and generic task-level prompts
(e.g., "How would you rate this image?"), assuming this mimics human rating
behavior. Our analysis reveals these approaches provide no benefit over
image-only training. Models using preset vocabularies and generic prompts
perform equivalently to image-only models, failing to leverage semantic
understanding from textual input. We propose Regression via Transformer-Based
Classification (RvTC), which replaces vocabulary-constrained classification
with a flexible bin-based approach. Unlike approaches that address
discretization errors through complex distributional modeling, RvTC eliminates
manual vocabulary crafting through straightforward bin increase, achieving
state-of-the-art performance on four image assessment datasets using only
images. More importantly, we demonstrate that data-specific prompts
dramatically improve performance. Unlike generic task descriptions, prompts
containing semantic information about specific images enable MLLMs to leverage
cross-modal understanding. On the AVA dataset, adding challenge titles to
prompts improves correlations from 0.83 to 0.90, a new state-of-the-art. We
demonstrate through empirical evidence from the AVA and AGIQA-3k datasets that
MLLMs benefit from semantic prompt information surpassing mere statistical
biases. This underscores the importance of incorporating meaningful textual
context in multimodal regression tasks.

</details>


### [72] [Axis-Aligned Document Dewarping](https://arxiv.org/abs/2507.15000)
*Chaoyun Wang,I-Chao Shen,Takeo Igarashi,Nanning Zheng,Caigui Jiang*

Main category: cs.CV

TL;DR: 提出了一种基于轴对齐几何约束的文档去卷曲方法，改善了现有学习方法的不足，并在多项基准测试中取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有学习方法依赖于有监督回归和注释数据，没有利用物理文档的几何特性去辅助去卷曲过程。

Method: 在训练中引入轴对齐几何约束；推理中采用轴对齐预处理策略；评估中提出一种新的指标——轴对齐畸变(AAD)。

Result: 在多个基准上取得SOTA成果，并在AAD指标上提升了18.2%~34.5%。

Conclusion: 通过引入几何约束和新的评估指标，与现有方法相比，本方法显著提升了文档去卷曲性能。

Abstract: Document dewarping is crucial for many applications. However, existing
learning-based methods primarily rely on supervised regression with annotated
data without leveraging the inherent geometric properties in physical documents
to the dewarping process. Our key insight is that a well-dewarped document is
characterized by transforming distorted feature lines into axis-aligned ones.
This property aligns with the inherent axis-aligned nature of the discrete grid
geometry in planar documents. In the training phase, we propose an axis-aligned
geometric constraint to enhance document dewarping. In the inference phase, we
propose an axis alignment preprocessing strategy to reduce the dewarping
difficulty. In the evaluation phase, we introduce a new metric, Axis-Aligned
Distortion (AAD), that not only incorporates geometric meaning and aligns with
human visual perception but also demonstrates greater robustness. As a result,
our method achieves SOTA results on multiple existing benchmarks and achieves
18.2%~34.5% improvements on the AAD metric.

</details>


### [73] [FastSmoothSAM: A Fast Smooth Method For Segment Anything Model](https://arxiv.org/abs/2507.15008)
*Jiasheng Xu,Yewang Chen*

Main category: cs.CV

TL;DR: 为了改进FastSAM真实感和分析精度，该研究利用B-Spline曲线拟合技术优化边缘质量，实现快速处理的同时提升分割精度。


<details>
  <summary>Details</summary>
Motivation: FastSAM尽管实现了实时分割，但存在边缘锯齿化影响真实物体形状表达的缺点。需要改进其边缘精度以增强其实用性。

Method: 引入B-Spline曲线拟合技术，通过四阶段精炼过程进行两轮曲线拟合，以平滑边缘，并提高分割质量。

Result: 显著提升了FastSAM的边缘视觉质量和分析精度，同时保持其实时处理能力。

Conclusion: 该方法增强了FastSAM在处理工业自动化、医疗成像和自动化系统中复杂边缘识别任务的适用性和实用价值。

Abstract: Accurately identifying and representing object edges is a challenging task in
computer vision and image processing. The Segment Anything Model (SAM) has
significantly influenced the field of image segmentation, but suffers from high
memory consumption and long inference times, limiting its efficiency in
real-time applications. To address these limitations, Fast Segment Anything
(FastSAM) was proposed, achieving real-time segmentation. However, FastSAM
often generates jagged edges that deviate from the true object shapes.
Therefore, this paper introduces a novel refinement approach using B-Spline
curve fitting techniques to enhance the edge quality in FastSAM. Leveraging the
robust shape control and flexible geometric construction of B-Splines, a
four-stage refining process involving two rounds of curve fitting is employed
to effectively smooth jagged edges. This approach significantly improves the
visual quality and analytical accuracy of object edges without compromising
critical geometric information. The proposed method improves the practical
utility of FastSAM by improving segmentation accuracy while maintaining
real-time processing capabilities. This advancement unlocks greater potential
for FastSAM technology in various real-world scenarios, such as industrial
automation, medical imaging, and autonomous systems, where precise and
efficient edge recognition is crucial.

</details>


### [74] [Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding](https://arxiv.org/abs/2507.15028)
*Yuanhan Zhang,Yunice Chew,Yuhao Dong,Aria Leo,Bo Hu,Ziwei Liu*

Main category: cs.CV

TL;DR: 该论文探讨视频大语言模型(Video-LLM)在正确性与鲁棒性上的不足，并提出Video Thinking Test (Video-TT)评价方法，发现现有模型与人类表现差距显著。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型在理解复杂视觉叙事及处理自然对抗性问题方面的能力与人类智能差距较大。

Method: 设计Video-TT，一个包含1000个YouTube Shorts视频的数据集，每个视频配有一个开放性问题及四个对抗性问题，用于测试模型的正确性与鲁棒性。

Result: 评估结果表明，目前的视频大语言模型在视觉与叙事复杂性理解上，与人类表现存在显著差距。

Conclusion: Video-TT揭示了视频大语言模型在正确性与鲁棒性方面的不足，同时能为提升模型水平提供有效的评测基准。

Abstract: Human intelligence requires correctness and robustness, with the former being
foundational for the latter. In video understanding, correctness ensures the
accurate interpretation of visual content, and robustness maintains consistent
performance in challenging conditions. Despite advances in video large language
models (video LLMs), existing benchmarks inadequately reflect the gap between
these models and human intelligence in maintaining correctness and robustness
in video interpretation. We introduce the Video Thinking Test (Video-TT), to
assess if video LLMs can interpret real-world videos as effectively as humans.
Video-TT reflects genuine gaps in understanding complex visual narratives, and
evaluates robustness against natural adversarial questions. Video-TT comprises
1,000 YouTube Shorts videos, each with one open-ended question and four
adversarial questions that probe visual and narrative complexity. Our
evaluation shows a significant gap between video LLMs and human performance.

</details>


### [75] [OpenBreastUS: Benchmarking Neural Operators for Wave Imaging Using Breast Ultrasound Computed Tomography](https://arxiv.org/abs/2507.15035)
*Zhijun Zeng,Youjia Zheng,Hao Hu,Zeyuan Dong,Yihang Zheng,Xinliang Liu,Jinzhuo Wang,Zuoqiang Shi,Linfeng Zhang,Yubing Li,He Sun*

Main category: cs.CV

TL;DR: 提出了一个名为OpenBreastUS的大规模数据集，用于加速和改进神经算子在波方程求解和医学成像中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统的波方程数值求解方法计算复杂性高，并且在医学成像的准实时应用中不稳定。神经算子能够加速偏微分方程求解，但现有数据集的复杂性不足以支持现实成像需求，因此需要开发一个能够弥补这一不足的数据集。

Method: 构建了一个包含8000个人体乳房解剖模型和1600万次频域波模拟的OpenBreastUS数据集，并对神经算子在正向模拟和逆向成像任务中的性能、可扩展性和泛化能力进行了全面基准测试。

Result: 实现了神经算子高效解决波动方程，首次展示了其在人体乳房医学成像中的运用。

Conclusion: OpenBreastUS数据集为神经偏微分方程求解器的开发与部署奠定了基础，并为医学成像的现实应用提供了支持。

Abstract: Accurate and efficient simulation of wave equations is crucial in
computational wave imaging applications, such as ultrasound computed tomography
(USCT), which reconstructs tissue material properties from observed scattered
waves. Traditional numerical solvers for wave equations are computationally
intensive and often unstable, limiting their practical applications for
quasi-real-time image reconstruction. Neural operators offer an innovative
approach by accelerating PDE solving using neural networks; however, their
effectiveness in realistic imaging is limited because existing datasets
oversimplify real-world complexity. In this paper, we present OpenBreastUS, a
large-scale wave equation dataset designed to bridge the gap between
theoretical equations and practical imaging applications. OpenBreastUS includes
8,000 anatomically realistic human breast phantoms and over 16 million
frequency-domain wave simulations using real USCT configurations. It enables a
comprehensive benchmarking of popular neural operators for both forward
simulation and inverse imaging tasks, allowing analysis of their performance,
scalability, and generalization capabilities. By offering a realistic and
extensive dataset, OpenBreastUS not only serves as a platform for developing
innovative neural PDE solvers but also facilitates their deployment in
real-world medical imaging problems. For the first time, we demonstrate
efficient in vivo imaging of the human breast using neural operator solvers.

</details>


### [76] [EBA-AI: Ethics-Guided Bias-Aware AI for Efficient Underwater Image Enhancement and Coral Reef Monitoring](https://arxiv.org/abs/2507.15036)
*Lyes Saad Saoud,Irfan Hussain*

Main category: cs.CV

TL;DR: 本文提出EBA-AI框架，通过CLIP嵌入技术减小数据集偏差，引入自适应处理优化能耗，提升水下图像增强效果与效率，同时增强AI的透明性与可信度。


<details>
  <summary>Details</summary>
Motivation: 解决AI水下图像增强模型因数据集偏差、高计算成本及透明性不足引发的误解问题，并推进其在海洋保护中的应用。

Method: 应用CLIP嵌入检测与缓解数据集偏差，集成自适应处理以降低GPU使用率，同时采用不确定性估算与可解释性技术提升模型透明度。

Result: 在LSUI400、Oceanex和UIEB100数据集上实验显示，PSNR下降1.0 dB，但显著降低计算资源消耗，使大规模海洋监测实现实时化，并在几组基准模型中表现优异。

Conclusion: EBA-AI在水下影像处理中取得效率、公平性和解释性的平衡，为偏差感知和节能的海洋保护AI应用提供了技术支持。

Abstract: Underwater image enhancement is vital for marine conservation, particularly
coral reef monitoring. However, AI-based enhancement models often face dataset
bias, high computational costs, and lack of transparency, leading to potential
misinterpretations. This paper introduces EBA-AI, an ethics-guided bias-aware
AI framework to address these challenges. EBA-AI leverages CLIP embeddings to
detect and mitigate dataset bias, ensuring balanced representation across
varied underwater environments. It also integrates adaptive processing to
optimize energy efficiency, significantly reducing GPU usage while maintaining
competitive enhancement quality. Experiments on LSUI400, Oceanex, and UIEB100
show that while PSNR drops by a controlled 1.0 dB, computational savings enable
real-time feasibility for large-scale marine monitoring. Additionally,
uncertainty estimation and explainability techniques enhance trust in AI-driven
environmental decisions. Comparisons with CycleGAN, FunIEGAN, RAUNENet,
WaterNet, UGAN, PUGAN, and UTUIE validate EBA-AI's effectiveness in balancing
efficiency, fairness, and interpretability in underwater image processing. By
addressing key limitations of AI-driven enhancement, this work contributes to
sustainable, bias-aware, and computationally efficient marine conservation
efforts. For interactive visualizations, animations, source code, and access to
the preprint, visit: https://lyessaadsaoud.github.io/EBA-AI/

</details>


### [77] [OmniVTON: Training-Free Universal Virtual Try-On](https://arxiv.org/abs/2507.15037)
*Zhaotong Yang,Yuhui Li,Shengfeng He,Xinzhe Li,Yangyang Xu,Junyu Dong,Yong Du*

Main category: cs.CV

TL;DR: OmniVTON是一种无需训练的普适虚拟试衣框架，通过分离服装和姿势条件实现高保真纹理和姿势一致性，适用于各种场景并支持多人体虚拟试衣。


<details>
  <summary>Details</summary>
Motivation: 现有的虚拟试衣技术无法同时在跨域泛化和数据偏差适应性上取得平衡，仍缺乏一种可以应对多种场景的统一方案。

Method: 提出了一种无训练的框架OmniVTON，通过服装先验生成机制和连续边界缝合技术保留纹理细节，并通过DDIM反转捕获结构线索以实现精准姿势对齐，同时消除服装和姿势限制的耦合性。

Result: 实验证明OmniVTON在各种数据集、服装类型和应用场景下表现出色，首次支持单场景的多人体试衣。

Conclusion: OmniVTON克服现有技术局限，提供跨域普适性、纹理保真度和多人体支持的虚拟试衣新框架，具有重大应用潜力。

Abstract: Image-based Virtual Try-On (VTON) techniques rely on either supervised
in-shop approaches, which ensure high fidelity but struggle with cross-domain
generalization, or unsupervised in-the-wild methods, which improve adaptability
but remain constrained by data biases and limited universality. A unified,
training-free solution that works across both scenarios remains an open
challenge. We propose OmniVTON, the first training-free universal VTON
framework that decouples garment and pose conditioning to achieve both texture
fidelity and pose consistency across diverse settings. To preserve garment
details, we introduce a garment prior generation mechanism that aligns clothing
with the body, followed by continuous boundary stitching technique to achieve
fine-grained texture retention. For precise pose alignment, we utilize DDIM
inversion to capture structural cues while suppressing texture interference,
ensuring accurate body alignment independent of the original image textures. By
disentangling garment and pose constraints, OmniVTON eliminates the bias
inherent in diffusion models when handling multiple conditions simultaneously.
Experimental results demonstrate that OmniVTON achieves superior performance
across diverse datasets, garment types, and application scenarios. Notably, it
is the first framework capable of multi-human VTON, enabling realistic garment
transfer across multiple individuals in a single scene. Code is available at
https://github.com/Jerome-Young/OmniVTON

</details>


### [78] [Rethinking Pan-sharpening: Principled Design, Unified Training, and a Universal Loss Surpass Brute-Force Scaling](https://arxiv.org/abs/2507.15059)
*Ran Zhang,Xuanhua He,Li Xueheng,Ke Cao,Liu Liu,Wenbo Xu,Fang Jiabin,Yang Qize,Jie Zhang*

Main category: cs.CV

TL;DR: 提出PanTiny，一个轻量化单步全分辨率融合框架，通过多数据集统一训练提高泛化能力，推进行业新标准。


<details>
  <summary>Details</summary>
Motivation: 以往模型复杂笨重且难以泛化，寻求提升效率和一致性的新方法。

Method: 设计轻量化模型PanTiny，引入同时训三种不同分辨率卫星数据集的多合一训练范式，并提出通用复合损失函数。

Result: PanTiny在效率与性能平衡表现出色，轻松超越更大的专用模型，并通过广泛对比实验验证其在全分辨率数据上的优越性。

Conclusion: 提倡构建高效、泛化能力强且数据导向的全分辨率融合模型及设计新标准，推动整个领域向更佳方向发展。

Abstract: The field of pan-sharpening has recently seen a trend towards increasingly
large and complex models, often trained on single, specific satellite datasets.
This approach, however, leads to high computational overhead and poor
generalization on full resolution data, a paradigm we challenge in this paper.
In response to this issue, we propose PanTiny, a lightweight, single-step
pan-sharpening framework designed for both efficiency and robust performance.
More critically, we introduce multiple-in-one training paradigm, where a
single, compact model is trained simultaneously on three distinct satellite
datasets (WV2, WV3, and GF2) with different resolution and spectral
information. Our experiments show that this unified training strategy not only
simplifies deployment but also significantly boosts generalization on
full-resolution data. Further, we introduce a universally powerful composite
loss function that elevates the performance of almost all of models for
pan-sharpening, pushing state-of-the-art metrics into a new era. Our PanTiny
model, benefiting from these innovations, achieves a superior
performance-to-efficiency balance, outperforming most larger, specialized
models. Through extensive ablation studies, we validate that principled
engineering in model design, training paradigms, and loss functions can surpass
brute-force scaling. Our work advocates for a community-wide shift towards
creating efficient, generalizable, and data-conscious models for
pan-sharpening. The code is available at
https://github.com/Zirconium233/PanTiny .

</details>


### [79] [StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation](https://arxiv.org/abs/2507.15064)
*Shuyuan Tu,Zhen Xing,Xintong Han,Zhi-Qi Cheng,Qi Dai,Chong Luo,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: StableAnimator++通过引入可学习的姿态对齐模块和多项身份一致性技术，解决了当前人像动画扩散模型在保持身份一致性方面的难题。


<details>
  <summary>Details</summary>
Motivation: 目前的人像动画扩散模型在参考图像和驱动视频差异较大时难以保持身份一致性。

Method: 通过可学习的姿态对齐、SVD指导的变换矩阵预测、多模态嵌入优化，以及结合Hamilton-Jacobi-Bellman (HJB)的去噪脸部优化方法，解决了姿态与身份保持方面的问题。

Result: StableAnimator++在多个基准测试上的表现证明了其身份一致性和视频生成质量的有效性，表现出色。

Conclusion: StableAnimator++是第一个具有学习姿态对齐能力并能优化身份一致性的视频扩散模型，在身份保持和视频质量上取得了技术突破。

Abstract: Current diffusion models for human image animation often struggle to maintain
identity (ID) consistency, especially when the reference image and driving
video differ significantly in body size or position. We introduce
StableAnimator++, the first ID-preserving video diffusion framework with
learnable pose alignment, capable of generating high-quality videos conditioned
on a reference image and a pose sequence without any post-processing. Building
upon a video diffusion model, StableAnimator++ contains carefully designed
modules for both training and inference, striving for identity consistency. In
particular, StableAnimator++ first uses learnable layers to predict the
similarity transformation matrices between the reference image and the driven
poses via injecting guidance from Singular Value Decomposition (SVD). These
matrices align the driven poses with the reference image, mitigating
misalignment to a great extent. StableAnimator++ then computes image and face
embeddings using off-the-shelf encoders, refining the face embeddings via a
global content-aware Face Encoder. To further maintain ID, we introduce a
distribution-aware ID Adapter that counteracts interference caused by temporal
layers while preserving ID via distribution alignment. During the inference
stage, we propose a novel Hamilton-Jacobi-Bellman (HJB) based face optimization
integrated into the denoising process, guiding the diffusion trajectory for
enhanced facial fidelity. Experiments on benchmarks show the effectiveness of
StableAnimator++ both qualitatively and quantitatively.

</details>


### [80] [Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR](https://arxiv.org/abs/2507.15085)
*Peirong Zhang,Haowei Xu,Jiaxin Zhang,Guitao Xu,Xuhan Zheng,Zhenhua Yang,Junle Liu,Yuyi Zhang,Lianwen Jin*

Main category: cs.CV

TL;DR: 本文探讨当前生成模型在文本图像生成和编辑上的能力，提出将OCR任务扩展到生成任务领域，并进行全面评估，发现现有模型在相关任务上的弱点。


<details>
  <summary>Details</summary>
Motivation: 文本图像兼具视觉审美与语言语义，是现代电子社会的重要信息载体，然而其生成面临显著挑战，因此需要评估当代生成模型在处理该任务上的表现。

Method: 选择33个代表性任务，分为五类（文档、手写文本、场景文本、艺术文本及复杂布局文本），使用六个开源和闭源模型，并设计针对性的高质量输入图像和提示词进行评估。

Result: 通过实证分析，发现现有生成模型在OCR生成任务上的显著弱点和局限性。

Conclusion: 研究建议通用生成模型应内化真实感文本图像生成与编辑为核心能力，拓宽模型应用边界，并希望该分析为相关研究提供参考与方向。

Abstract: Text image is a unique and crucial information medium that integrates visual
aesthetics and linguistic semantics in modern e-society. Due to their subtlety
and complexity, the generation of text images represents a challenging and
evolving frontier in the image generation field. The recent surge of
specialized image generators (\emph{e.g.}, Flux-series) and unified generative
models (\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a
natural question: can they master the intricacies of text image generation and
editing? Motivated by this, we assess current state-of-the-art generative
models' capabilities in terms of text image generation and editing. We
incorporate various typical optical character recognition (OCR) tasks into our
evaluation and broaden the concept of text-based generation tasks into OCR
generative tasks. We select 33 representative tasks and categorize them into
five categories: document, handwritten text, scene text, artistic text, and
complex \& layout-rich text. For comprehensive evaluation, we examine six
models across both closed-source and open-source domains, using tailored,
high-quality image inputs and prompts. Through this evaluation, we draw crucial
observations and identify the weaknesses of current generative models for OCR
tasks. We argue that photorealistic text image generation and editing should be
internalized as foundational skills into general-domain generative models,
rather than being delegated to specialized solutions, and we hope this
empirical analysis can provide valuable insights for the community to achieve
this goal. This evaluation is online and will be continuously updated at our
GitHub repository.

</details>


### [81] [Visual Place Recognition for Large-Scale UAV Applications](https://arxiv.org/abs/2507.15089)
*Ioannis Tsampikos Papapetros,Ioannis Kansizoglou,Antonios Gasteratos*

Main category: cs.CV

TL;DR: 本文通过开发一个大规模空中视觉数据集LASED及采用可处理旋转不变性的可转卷积神经网络（steerable CNNs），显著提升了无人机导航的视觉定位能力。


<details>
  <summary>Details</summary>
Motivation: 无人机导航需要解决跨环境的精准定位，然而缺乏大规模高空数据集以及旋转歧义问题限制了模型在空中视觉定位中的表现。

Method: 引入一个包含近百万张图像的大规模空中数据集LASED，并结合具有旋转等变性的可转卷积神经网络来解决旋转不变性的问题。

Result: 相比传统小型数据集，LASED显著提高了模型召回率，而结合可转卷积网络的模型在应对旋转歧义方面表现更优，召回率提升达12%。

Conclusion: 将结构化大规模数据集与旋转等变网络结合，可有效提升模型鲁棒性和泛化能力，适用于空中视觉定位。

Abstract: Visual Place Recognition (vPR) plays a crucial role in Unmanned Aerial
Vehicle (UAV) navigation, enabling robust localization across diverse
environments. Despite significant advancements, aerial vPR faces unique
challenges due to the limited availability of large-scale, high-altitude
datasets, which limits model generalization, along with the inherent rotational
ambiguity in UAV imagery. To address these challenges, we introduce LASED, a
large-scale aerial dataset with approximately one million images,
systematically sampled from 170,000 unique locations throughout Estonia over a
decade, offering extensive geographic and temporal diversity. Its structured
design ensures clear place separation significantly enhancing model training
for aerial scenarios. Furthermore, we propose the integration of steerable
Convolutional Neural Networks (CNNs) to explicitly handle rotational variance,
leveraging their inherent rotational equivariance to produce robust,
orientation-invariant feature representations. Our extensive benchmarking
demonstrates that models trained on LASED achieve significantly higher recall
compared to those trained on smaller, less diverse datasets, highlighting the
benefits of extensive geographic coverage and temporal diversity. Moreover,
steerable CNNs effectively address rotational ambiguity inherent in aerial
imagery, consistently outperforming conventional convolutional architectures,
achieving on average 12\% recall improvement over the best-performing
non-steerable network. By combining structured, large-scale datasets with
rotation-equivariant neural networks, our approach significantly enhances model
robustness and generalization for aerial vPR.

</details>


### [82] [BleedOrigin: Dynamic Bleeding Source Localization in Endoscopic Submucosal Dissection via Dual-Stage Detection and Tracking](https://arxiv.org/abs/2507.15094)
*Mengya Xu,Rulin Zhou,An Wang,Chaoyang Lyu,Zhen Li,Ning Zhong,Hongliang Ren*

Main category: cs.CV

TL;DR: 本文提出了BleedOrigin-Bench数据集和BleedOrigin-Net框架，为内镜黏膜下剥离术（ESD）中出血源的检测和追踪提供了精确的AI解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前内镜黏膜下剥离术（ESD）中的出血控制依赖实时精准的出血源检测和跟踪，但现有AI方法主要关注出血区域分割，缺乏对出血源的定位和时间追踪，无法满足复杂ESD操作环境下的要求。此外，缺乏针对性数据集进一步限制了技术发展。

Method: 本文构建了首个涵盖1,771处专家标注的出血源数据集BleedOrigin-Bench，涵盖44例手术的106,222帧，附加39,755帧伪标签数据。提出的BleedOrigin-Net是一种双阶段检测和追踪框架，从出血发生到空间的连续追踪提供全面解决方案。

Result: BleedOrigin-Net在多种基准测试中表现优异：出血发生检测帧级准确率为96.85%，初始出血源定位像素级准确率为70.24%，点追踪像素级准确率达到96.11%。

Conclusion: 本文解决了内镜环境下出血源检测的关键需求，通过BleedOrigin-Bench数据集和BleedOrigin-Net框架实现了精确且鲁棒的出血源定位与追踪，显著提升ESD操作效率和患者安全性。

Abstract: Intraoperative bleeding during Endoscopic Submucosal Dissection (ESD) poses
significant risks, demanding precise, real-time localization and continuous
monitoring of the bleeding source for effective hemostatic intervention. In
particular, endoscopists have to repeatedly flush to clear blood, allowing only
milliseconds to identify bleeding sources, an inefficient process that prolongs
operations and elevates patient risks. However, current Artificial Intelligence
(AI) methods primarily focus on bleeding region segmentation, overlooking the
critical need for accurate bleeding source detection and temporal tracking in
the challenging ESD environment, which is marked by frequent visual
obstructions and dynamic scene changes. This gap is widened by the lack of
specialized datasets, hindering the development of robust AI-assisted guidance
systems. To address these challenges, we introduce BleedOrigin-Bench, the first
comprehensive ESD bleeding source dataset, featuring 1,771 expert-annotated
bleeding sources across 106,222 frames from 44 procedures, supplemented with
39,755 pseudo-labeled frames. This benchmark covers 8 anatomical sites and 6
challenging clinical scenarios. We also present BleedOrigin-Net, a novel
dual-stage detection-tracking framework for the bleeding source localization in
ESD procedures, addressing the complete workflow from bleeding onset detection
to continuous spatial tracking. We compare with widely-used object detection
models (YOLOv11/v12), multimodal large language models, and point tracking
methods. Extensive evaluation demonstrates state-of-the-art performance,
achieving 96.85% frame-level accuracy ($\pm\leq8$ frames) for bleeding onset
detection, 70.24% pixel-level accuracy ($\leq100$ px) for initial source
detection, and 96.11% pixel-level accuracy ($\leq100$ px) for point tracking.

</details>


### [83] [LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM](https://arxiv.org/abs/2507.15109)
*Mohammad-Maher Nakshbandi,Ziad Sharawy,Sorin Grigorescu*

Main category: cs.CV

TL;DR: 本文提出了一种名为LoopNet的方法，通过改进的ResNet架构和DISK描述符解决了SLAM系统中的闭环检测问题，并发布了相关代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 解决SLAM系统中闭环检测准确率和实时计算约束的问题。

Method: 基于多任务版ResNet架构，结合DISK描述符，在线再训练并使用少样本学习进行适配。

Result: 相比传统方法在不同条件下性能更优，同时公布了代码和新的基准数据集LoopDB。

Conclusion: LoopNet方法在解决SLAM闭环检测问题上表现出色，为嵌入式设备实时应用提供了新思路，相关资源已开源。

Abstract: One of the main challenges in the Simultaneous Localization and Mapping
(SLAM) loop closure problem is the recognition of previously visited places. In
this work, we tackle the two main problems of real-time SLAM systems: 1) loop
closure detection accuracy and 2) real-time computation constraints on the
embedded hardware. Our LoopNet method is based on a multitasking variant of the
classical ResNet architecture, adapted for online retraining on a dynamic
visual dataset and optimized for embedded devices. The online retraining is
designed using a few-shot learning approach. The architecture provides both an
index into the queried visual dataset, and a measurement of the prediction
quality. Moreover, by leveraging DISK (DIStinctive Keypoints) descriptors,
LoopNet surpasses the limitations of handcrafted features and traditional deep
learning methods, offering better performance under varying conditions. Code is
available at https://github.com/RovisLab/LoopNet. Additinally, we introduce a
new loop closure benchmarking dataset, coined LoopDB, which is available at
https://github.com/RovisLab/LoopDB.

</details>


### [84] [Enhancing Visual Planning with Auxiliary Tasks and Multi-token Prediction](https://arxiv.org/abs/2507.15130)
*Ce Zhang,Yale Song,Ruta Desai,Michael Louis Iuzzolino,Joseph Tighe,Gedas Bertasius,Satwik Kottur*

Main category: cs.CV

TL;DR: 本文介绍了一种旨在改进视觉计划的模型VideoPlan，其通过辅助任务增强和多标记预测方法取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视频理解方面虽表现出色，但面对长时视觉计划依然存在挑战，尤其是稀缺的过程注释数据和低效的下一标记预测目标策略。

Method: 提出了辅助任务增强方法，用相关任务如目标预测来增强模型规划能力；同时采用多标记预测方法，通过多头预测多个未来标记以更好地建模视觉计划中的结构化动作空间。

Result: VideoPlan在COIN与CrossTask数据集上的未来三动作预测中分别超越现有方法7.3%与3.4%，并在Ego4D长期动作预测任务中与最先进方法持平。

Conclusion: VideoPlan通过改进的训练方法与模块有效提升了长时视觉计划性能，验证了其在跨任务与跨数据集场景中的适应能力。

Abstract: Visual Planning for Assistance (VPA) aims to predict a sequence of user
actions required to achieve a specified goal based on a video showing the
user's progress. Although recent advances in multimodal large language models
(MLLMs) have shown promising results in video understanding, long-horizon
visual planning remains a challenging problem. We identify two challenges in
training large MLLMs for video-based planning tasks: (1) scarcity of procedural
annotations, limiting the model's ability to learn procedural task dynamics
effectively, and (2) inefficiency of next-token prediction objective to
explicitly capture the structured action space for visual planning when
compared to free-form, natural language. To tackle data scarcity, we introduce
Auxiliary Task Augmentation. We design and train our model on auxiliary tasks
relevant to long-horizon video-based planning (e.g., goal prediction) to
augment the model's planning ability. To more explicitly model the structured
action space unique to visual planning tasks, we leverage Multi-token
Prediction, extending traditional next-token prediction by using multiple heads
to predict multiple future tokens during training. Our approach, VideoPlan,
achieves state-of-the-art VPA performance on the COIN and CrossTask datasets,
surpassing prior methods by 7.3% and 3.4%, respectively, when predicting 3
future actions. We further extend our method to the challenging Ego4D Long-term
Action Anticipation task, and show that it is on par with the state-of-the-art
approaches despite not using specialized egocentric features. Code will be made
available.

</details>


### [85] [Event-based Graph Representation with Spatial and Motion Vectors for Asynchronous Object Detection](https://arxiv.org/abs/2507.15150)
*Aayush Atul Verma,Arpitsinh Vaghela,Bharatesh Chakravarthi,Kaustav Chanda,Yezhou Yang*

Main category: cs.CV

TL;DR: 本文提出一种新颖的时空多图表示法，以更高效地捕获事件驱动传感器的空间结构和时间动态变化，并显著提高了基于图的方法在对象检测任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 将事件驱动传感器生成的稀疏、异步数据转化为密集张量会损失其固有的优势，因此需要研究在保留稀疏性和支持异步推断的同时，提升下游任务性能的图表示法。

Method: 本文提出了一个时空多图（spatiotemporal multigraph）表示法，包括两个解耦的图：通过B样条基函数捕捉全局空间结构的空间图和基于运动矢量注意力机制捕捉局部动态变化的时间图。同时，利用高效的2D卷积核代替计算成本高的3D卷积核。

Result: 在Gen1汽车和eTraM数据集上进行事件检测，本文方法比以往基于图的方法检测精度提高6%以上，相较之下速度提高了5倍，参数减少，并且计算成本未增加。

Conclusion: 结构化的图建模方法能显著提升异步视觉任务的性能，同时保持高效性和计算成本优势。

Abstract: Event-based sensors offer high temporal resolution and low latency by
generating sparse, asynchronous data. However, converting this irregular data
into dense tensors for use in standard neural networks diminishes these
inherent advantages, motivating research into graph representations. While such
methods preserve sparsity and support asynchronous inference, their performance
on downstream tasks remains limited due to suboptimal modeling of
spatiotemporal dynamics. In this work, we propose a novel spatiotemporal
multigraph representation to better capture spatial structure and temporal
changes. Our approach constructs two decoupled graphs: a spatial graph
leveraging B-spline basis functions to model global structure, and a temporal
graph utilizing motion vector-based attention for local dynamic changes. This
design enables the use of efficient 2D kernels in place of computationally
expensive 3D kernels. We evaluate our method on the Gen1 automotive and eTraM
datasets for event-based object detection, achieving over a 6% improvement in
detection accuracy compared to previous graph-based works, with a 5x speedup,
reduced parameter count, and no increase in computational cost. These results
highlight the effectiveness of structured graph modeling for asynchronous
vision. Project page: eventbasedvision.github.io/eGSMV.

</details>


### [86] [MeshMamba: State Space Models for Articulated 3D Mesh Generation and Reconstruction](https://arxiv.org/abs/2507.15212)
*Yusuke Yoshiyasu,Leyuan Sun,Ryusuke Sagawa*

Main category: cs.CV

TL;DR: 提出了一种名为MeshMamba的深度学习模型，用于高效处理3D人体网格生成与重建问题。


<details>
  <summary>Details</summary>
Motivation: 通过引入Mamba-SSM解决现有技术在处理高密度3D网格数据时的效率与扩展性问题。

Method: 通过顶点序列化技术整合网格数据，并基于此设计了一个扩散模型MambaDiff3D和一个单视图3D重建模型Mamba-HMR。

Result: MeshMamba实现了包含衣物与手部几何形状的高精度3D人体网格生成，在3D人体形状生成任务上超越了现有方法。此外，Mamba-HMR实现了全身包含头部和手部的3D重建，同时保持高效的性能表现。

Conclusion: MeshMamba成功扩展了高密度3D人体网格的生成与重建的能力，为相关应用提供了更高效的工具。

Abstract: In this paper, we introduce MeshMamba, a neural network model for learning 3D
articulated mesh models by employing the recently proposed Mamba State Space
Models (Mamba-SSMs). MeshMamba is efficient and scalable in handling a large
number of input tokens, enabling the generation and reconstruction of body mesh
models with more than 10,000 vertices, capturing clothing and hand geometries.
The key to effectively learning MeshMamba is the serialization technique of
mesh vertices into orderings that are easily processed by Mamba. This is
achieved by sorting the vertices based on body part annotations or the 3D
vertex locations of a template mesh, such that the ordering respects the
structure of articulated shapes. Based on MeshMamba, we design 1) MambaDiff3D,
a denoising diffusion model for generating 3D articulated meshes and 2)
Mamba-HMR, a 3D human mesh recovery model that reconstructs a human body shape
and pose from a single image. Experimental results showed that MambaDiff3D can
generate dense 3D human meshes in clothes, with grasping hands, etc., and
outperforms previous approaches in the 3D human shape generation task.
Additionally, Mamba-HMR extends the capabilities of previous non-parametric
human mesh recovery approaches, which were limited to handling body-only poses
using around 500 vertex tokens, to the whole-body setting with face and hands,
while achieving competitive performance in (near) real-time.

</details>


### [87] [Improving Joint Embedding Predictive Architecture with Diffusion Noise](https://arxiv.org/abs/2507.15216)
*Yuping Qiu,Rui Zhu,Ying-cong Chen*

Main category: cs.CV

TL;DR: 提出一种称为N-JEPA的新方法，将扩散噪声结合到自监督学习中的掩码图像建模方法，提升图像分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 寻找到自监督学习和生成模型之间的联系，以进一步提高自监督学习的表示能力。

Method: 将扩散模型的扩散噪声引入到掩码图像建模中，通过掩码标记的位置嵌入以及多级噪声计划增强模型的鲁棒性。

Result: 在下游任务的分类中进行了全面的研究，确认了该方法的有效性。

Conclusion: N-JEPA利用扩散噪声有效地将生成模型的特性融入自监督学习，提升了其识别能力，代码将公开。

Abstract: Self-supervised learning has become an incredibly successful method for
feature learning, widely applied to many downstream tasks. It has proven
especially effective for discriminative tasks, surpassing the trending
generative models. However, generative models perform better in image
generation and detail enhancement. Thus, it is natural for us to find a
connection between SSL and generative models to further enhance the
representation capacity of SSL. As generative models can create new samples by
approximating the data distribution, such modeling should also lead to a
semantic understanding of the raw visual data, which is necessary for
recognition tasks. This enlightens us to combine the core principle of the
diffusion model: diffusion noise, with SSL to learn a competitive recognition
model. Specifically, diffusion noise can be viewed as a particular state of
mask that reveals a close relationship between masked image modeling (MIM) and
diffusion models. In this paper, we propose N-JEPA (Noise-based JEPA) to
incorporate diffusion noise into MIM by the position embedding of masked
tokens. The multi-level noise schedule is a series of feature augmentations to
further enhance the robustness of our model. We perform a comprehensive study
to confirm its effectiveness in the classification of downstream tasks. Codes
will be released soon in public.

</details>


### [88] [Hierarchical Part-based Generative Model for Realistic 3D Blood Vessel](https://arxiv.org/abs/2507.15223)
*Siqi Chen,Guoqing Zhang,Jiahao Lai,Bingzhi Shen,Sihong Zhang,Caixia Dong,Xuejin Chen,Yang Li*

Main category: cs.CV

TL;DR: 提出了一种基于分层部件的3D血管生成框架，能够更准确地建模复杂的血管网络，并在真实数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 准确表示血管复杂的几何和拓扑结构具有挑战性，现有方法难以充分刻画血管的分支模式、曲率及不规则性。

Method: 提出了一种分三阶段的框架：1）生成关键图以建模整体分层结构；2）生成血管段并依靠几何属性进行约束；3）整合局部段与全局关键图以完成分层血管组装。

Result: 在真实世界数据集上验证了该框架的优越性能，相较现有方法在建模复杂血管网络中表现更优异。

Conclusion: 这是首个将分部件生成方法成功应用于3D血管建模的研究，设置了新的血管数据生成基准。代码已开源。

Abstract: Advancements in 3D vision have increased the impact of blood vessel modeling
on medical applications. However, accurately representing the complex geometry
and topology of blood vessels remains a challenge due to their intricate
branching patterns, curvatures, and irregular shapes. In this study, we propose
a hierarchical part-based frame work for 3D vessel generation that separates
the global binary tree-like topology from local geometric details. Our approach
proceeds in three stages: (1) key graph generation to model the overall
hierarchical struc ture, (2) vessel segment generation conditioned on geometric
properties, and (3) hierarchical vessel assembly by integrating the local
segments according to the global key graph. We validate our framework on real
world datasets, demonstrating superior performance over existing methods in
modeling complex vascular networks. This work marks the first successful
application of a part-based generative approach for 3D vessel modeling, setting
a new benchmark for vascular data generation. The code is available at:
https://github.com/CybercatChen/PartVessel.git.

</details>


### [89] [Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse Autoencoders](https://arxiv.org/abs/2507.15227)
*Krishna Kanth Nakka*

Main category: cs.CV

TL;DR: 引入基于稀疏自编码器(SAE)的可解释性方法到乳腺影像分析领域，通过研究Mammo-CLIP模型的潜在特征对乳腺概念进行解码。


<details>
  <summary>Details</summary>
Motivation: 在医疗影像领域中，可解释性对于临床应用至关重要。本研究旨在通过可视化和理解Mammo-CLIP模型的决策过程，提高乳腺影像中模型的可解释性。

Method: 训练基于Mammo-CLIP的SAE模型，通过分析潜在特征与乳腺影像中临床相关概念（如肿块和可疑钙化）的关联来揭示模型决策机制。

Result: 研究发现SAE潜在空间中的激活神经元与实际高相关区间一致，同时揭示了多个影响模型决策的混杂因素。此外，探索了模型在微调过程中依赖的潜在神经元对概念预测的作用。

Conclusion: 基于SAE的解释性潜在表示能够深度解析基础模型在乳腺影像领域中每一层的内在工作机制，展示了其在临床可解释性提升中的潜力。

Abstract: Interpretability is critical in high-stakes domains such as medical imaging,
where understanding model decisions is essential for clinical adoption. In this
work, we introduce Sparse Autoencoder (SAE)-based interpretability to breast
imaging by analyzing {Mammo-CLIP}, a vision--language foundation model
pretrained on large-scale mammogram image--report pairs. We train a patch-level
\texttt{Mammo-SAE} on Mammo-CLIP to identify and probe latent features
associated with clinically relevant breast concepts such as \textit{mass} and
\textit{suspicious calcification}. Our findings reveal that top activated class
level latent neurons in the SAE latent space often tend to align with ground
truth regions, and also uncover several confounding factors influencing the
model's decision-making process. Additionally, we analyze which latent neurons
the model relies on during downstream finetuning for improving the breast
concept prediction. This study highlights the promise of interpretable SAE
latent representations in providing deeper insight into the internal workings
of foundation models at every layer for breast imaging.

</details>


### [90] [Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation](https://arxiv.org/abs/2507.15243)
*Naeem Paeedeh,Mahardhika Pratama,Wolfgang Mayer,Jimmy Cao,Ryszard Kowlczyk*

Main category: cs.CV

TL;DR: 提出了一种新方法以解决跨域小样本学习中的过拟合问题，取得了效果显著的结果。


<details>
  <summary>Details</summary>
Motivation: 当前在跨域小样本学习中，尽管简单的DINO预训练及原型分类器的组合已优于现有方法，但因标注数据稀少，调整过多参数易导致过拟合。

Method: 提出了融合投影（CP）的方法作为软提示的替代方案，同时基于自监督变换（SSTs）进行伪类生成，利用基础域上的数据使网络更好地应对不同域的数据。

Result: 在BSCD-FSL基准的极端域转移场景下实验验证了方法的有效性。

Conclusion: 所提出方法成功缓解了跨域小样本学习中的过拟合问题，并表现出优越的性能。

Abstract: Despite the progress in Cross-Domain Few-Shot Learning (CD-FSL), a model
pre-trained with DINO combined with a prototypical classifier outperforms the
latest SOTA methods. A crucial limitation that needs to be overcome is that
updating too many parameters of the transformers leads to overfitting due to
the scarcity of labeled samples. To address this challenge, we propose a new
concept, Coalescent Projection (CP), as an effective successor to soft prompts.
Additionally, we propose a novel pseudo-class generation method combined with
Self-Supervised Transformations (SSTs) that relies solely on the base domain to
prepare the network for encountering unseen samples from different domains. The
proposed method exhibits its effectiveness in comprehensive experiments on the
extreme domain shift scenario of the BSCD-FSL benchmark. Our code is published
at https://github.com/Naeem-Paeedeh/CPLSR.

</details>


### [91] [FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers](https://arxiv.org/abs/2507.15249)
*Yanbing Zhang,Zhe Wang,Qin Zhou,Mengping Yang*

Main category: cs.CV

TL;DR: 本文提出了一个完全无需训练的框架FreeCus，用于高保真主体驱动的图像生成，充分利用了现代扩散Transformer（DiT）的零样本潜力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于训练过程或大型数据集的特定设计，限制了其实用性，并未充分利用DiT的零样本潜力。

Method: 该方法通过三个主要创新实现：引入关键注意力共享机制、升级DiT以改进细粒度特征提取、整合多模态大语言模型优化语义表示。

Result: 实验表明FreeCus在不同场景下，实现了对主体的一致性生成，结果达到了或接近当前最先进水平。

Conclusion: FreeCus框架无需额外训练即可有效解锁DiT的零样本能力，并兼容现有的补全与控制模块，提升生成体验。

Abstract: In light of recent breakthroughs in text-to-image (T2I) generation,
particularly with diffusion transformers (DiT), subject-driven technologies are
increasingly being employed for high-fidelity customized production that
preserves subject identity from reference inputs, enabling thrilling design
workflows and engaging entertainment. Existing alternatives typically require
either per-subject optimization via trainable text embeddings or training
specialized encoders for subject feature extraction on large-scale datasets.
Such dependencies on training procedures fundamentally constrain their
practical applications. More importantly, current methodologies fail to fully
leverage the inherent zero-shot potential of modern diffusion transformers
(e.g., the Flux series) for authentic subject-driven synthesis. To bridge this
gap, we propose FreeCus, a genuinely training-free framework that activates
DiT's capabilities through three key innovations: 1) We introduce a pivotal
attention sharing mechanism that captures the subject's layout integrity while
preserving crucial editing flexibility. 2) Through a straightforward analysis
of DiT's dynamic shifting, we propose an upgraded variant that significantly
improves fine-grained feature extraction. 3) We further integrate advanced
Multimodal Large Language Models (MLLMs) to enrich cross-modal semantic
representations. Extensive experiments reflect that our method successfully
unlocks DiT's zero-shot ability for consistent subject synthesis across diverse
contexts, achieving state-of-the-art or comparable results compared to
approaches that require additional training. Notably, our framework
demonstrates seamless compatibility with existing inpainting pipelines and
control modules, facilitating more compelling experiences. Our code is
available at: https://github.com/Monalissaa/FreeCus.

</details>


### [92] [MinCD-PnP: Learning 2D-3D Correspondences with Approximate Blind PnP](https://arxiv.org/abs/2507.15257)
*Pei An,Jiaqi Yang,Muyao Peng,You Yang,Qiong Liu,Xiaolin Wu,Liangliang Nan*

Main category: cs.CV

TL;DR: 本文提出了一种基于近似盲PnP的图像到点云(I2P)注册方法，通过最小化Chamfer距离解决现有方法对噪声和异常值的敏感性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于微分PnP的I2P注册方法对2D-3D特征匹配中的噪声和异常值非常敏感，影响了匹配学习的有效性。

Method: 提出一种近似盲PnP算法，简化为最小化2D和3D关键点的Chamfer距离（MinCD-PnP）问题，并设计了轻量级MinCD-Net多任务学习模块以提升算法效率。

Result: 在7-Scenes、RGBD-V2、ScanNet及自采数据集上的实验表明，MinCD-Net方法在跨场景和跨数据集设置下超越当前方法，表现出更高的内点比率和注册回调率。

Conclusion: 本文提出的MinCD-Net克服了传统I2P注册方法的不足，在多场景和多数据参数下均表现出卓越性能，为后续研究提供了重要参考。

Abstract: Image-to-point-cloud (I2P) registration is a fundamental problem in computer
vision, focusing on establishing 2D-3D correspondences between an image and a
point cloud. The differential perspective-n-point (PnP) has been widely used to
supervise I2P registration networks by enforcing the projective constraints on
2D-3D correspondences. However, differential PnP is highly sensitive to noise
and outliers in the predicted correspondences. This issue hinders the
effectiveness of correspondence learning. Inspired by the robustness of blind
PnP against noise and outliers in correspondences, we propose an approximated
blind PnP based correspondence learning approach. To mitigate the high
computational cost of blind PnP, we simplify blind PnP to an amenable task of
minimizing Chamfer distance between learned 2D and 3D keypoints, called
MinCD-PnP. To effectively solve MinCD-PnP, we design a lightweight multi-task
learning module, named as MinCD-Net, which can be easily integrated into the
existing I2P registration architectures. Extensive experiments on 7-Scenes,
RGBD-V2, ScanNet, and self-collected datasets demonstrate that MinCD-Net
outperforms state-of-the-art methods and achieves a higher inlier ratio (IR)
and registration recall (RR) in both cross-scene and cross-dataset settings.

</details>


### [93] [Conditional Video Generation for High-Efficiency Video Compression](https://arxiv.org/abs/2507.15269)
*Fangqiu Yi,Jingyu Xu,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于条件扩散模型的视频压缩框架，通过将视频压缩重新定义为条件生成任务，大幅提升了感知质量。


<details>
  <summary>Details</summary>
Motivation: 当前视频压缩技术在高压缩比下的感知质量仍存在不足，作者基于条件扩散模型的出色感知表现，希望开发出感知优化的视频压缩方法。

Method: 方法包括三个关键模块：多粒度条件建模、紧凑表示的设计以及通过模态缺失和角色感知嵌入的多条件训练。

Result: 实验表明，该方法在感知质量指标（如FVD和LPIPS）上显著优于传统和神经编解码器，特别是在高压缩比条件下。

Conclusion: 提出的新框架有效提升了视频生成的感知质量，并在高压缩比条件下展现出极大的优势。

Abstract: Perceptual studies demonstrate that conditional diffusion models excel at
reconstructing video content aligned with human visual perception. Building on
this insight, we propose a video compression framework that leverages
conditional diffusion models for perceptually optimized reconstruction.
Specifically, we reframe video compression as a conditional generation task,
where a generative model synthesizes video from sparse, yet informative
signals. Our approach introduces three key modules: (1) Multi-granular
conditioning that captures both static scene structure and dynamic
spatio-temporal cues; (2) Compact representations designed for efficient
transmission without sacrificing semantic richness; (3) Multi-condition
training with modality dropout and role-aware embeddings, which prevent
over-reliance on any single modality and enhance robustness. Extensive
experiments show that our method significantly outperforms both traditional and
neural codecs on perceptual quality metrics such as Fr\'echet Video Distance
(FVD) and LPIPS, especially under high compression ratios.

</details>


### [94] [In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems](https://arxiv.org/abs/2507.15285)
*Lazaro Janier Gonzalez-Soler,Maciej Salwowski,Christoph Busch*

Main category: cs.CV

TL;DR: 本文研究了通过视觉语言模型(VLM)和上下文学习框架来检测生物识别系统中的物理呈现攻击和数字变形攻击，实验表明其性能优于部分传统CNN方法。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在训练的场景上表现出色，但在面对不同攻击类型或环境条件时适应能力不足，且需要大量数据，而收集生物数据存在隐私和操作挑战。

Method: 提出了一种基于视觉语言模型(VLM)的上下文学习框架，系统性评估其在安全关键场景中的能力，通过自由可用数据库进行实验验证。

Result: 实验表明，所提出的子系统在物理和数字攻击检测上具有竞争力的性能，并且无需大量资源密集型训练，优于部分传统CNN方法。

Conclusion: 该框架在提高攻击检测的泛化能力方面具有潜力，是一种有前途的工具。

Abstract: Recent advances in biometric systems have significantly improved the
detection and prevention of fraudulent activities. However, as detection
methods improve, attack techniques become increasingly sophisticated. Attacks
on face recognition systems can be broadly divided into physical and digital
approaches. Traditionally, deep learning models have been the primary defence
against such attacks. While these models perform exceptionally well in
scenarios for which they have been trained, they often struggle to adapt to
different types of attacks or varying environmental conditions. These
subsystems require substantial amounts of training data to achieve reliable
performance, yet biometric data collection faces significant challenges,
including privacy concerns and the logistical difficulties of capturing diverse
attack scenarios under controlled conditions. This work investigates the
application of Vision Language Models (VLM) and proposes an in-context learning
framework for detecting physical presentation attacks and digital morphing
attacks in biometric systems. Focusing on open-source models, the first
systematic framework for the quantitative evaluation of VLMs in
security-critical scenarios through in-context learning techniques is
established. The experimental evaluation conducted on freely available
databases demonstrates that the proposed subsystem achieves competitive
performance for physical and digital attack detection, outperforming some of
the traditional CNNs without resource-intensive training. The experimental
results validate the proposed framework as a promising tool for improving
generalisation in attack detection.

</details>


### [95] [Minutiae-Anchored Local Dense Representation for Fingerprint Matching](https://arxiv.org/abs/2507.15297)
*Zhiyu Pan,Xiongjun Guan,Yongjie Duan,Jianjiang Feng,Jie Zhou*

Main category: cs.CV

TL;DR: 提出DMD方法，通过基于细节点的密集局部表示，改进指纹匹配的准确性和鲁棒性，实验显示其优异性能与高效性。


<details>
  <summary>Details</summary>
Motivation: 在多样化采集条件下实现鲁棒和精确的指纹匹配，克服现有方法面对复杂指纹结构时的不足。

Method: 通过对检测出的细节点周围进行局部纹理和特征的三维张量提取，并结合分割掩模优化匹配区域，提高匹配效率和效果。

Result: 在多种指纹测试集上表现出优越的准确性与高效性，比现有方法表现更好，展示了处理大规模指纹识别的潜力。

Conclusion: 所提方法在保持高效性的同时实现了跨数据集的稳定优异表现，适用于复杂场景下的指纹识别。

Abstract: Fingerprint matching under diverse capture conditions remains a fundamental
challenge in biometric recognition. To achieve robust and accurate performance
in such scenarios, we propose DMD, a minutiae-anchored local dense
representation which captures both fine-grained ridge textures and
discriminative minutiae features in a spatially structured manner.
Specifically, descriptors are extracted from local patches centered and
oriented on each detected minutia, forming a three-dimensional tensor, where
two dimensions represent spatial locations on the fingerprint plane and the
third encodes semantic features. This representation explicitly captures
abstract features of local image patches, enabling a multi-level, fine-grained
description that aggregates information from multiple minutiae and their
surrounding ridge structures. Furthermore, thanks to its strong spatial
correspondence with the patch image, DMD allows for the use of foreground
segmentation masks to identify valid descriptor regions. During matching,
comparisons are then restricted to overlapping foreground areas, improving
efficiency and robustness. Extensive experiments on rolled, plain, parital,
contactless, and latent fingerprint datasets demonstrate the effectiveness and
generalizability of the proposed method. It achieves state-of-the-art accuracy
across multiple benchmarks while maintaining high computational efficiency,
showing strong potential for large-scale fingerprint recognition. Corresponding
code is available at https://github.com/Yu-Yy/DMD.

</details>


### [96] [Few-Shot Object Detection via Spatial-Channel State Space Model](https://arxiv.org/abs/2507.15308)
*Zhimeng Xin,Tianxu Wu,Yixiong Zou,Shiming Chen,Dingjie Fu,Xinge You*

Main category: cs.CV

TL;DR: 提出一种基于空间-通道状态建模的模块（SCSM）来增强少样本目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法难以准确提取有效特征，且权重高低无法完全反映通道的重要性。

Method: 通过引入Mamba模型相关概念，设计SCSM模块，其中包括空间特征建模（SFM）与通道状态建模（CSM）以优化特征提取。

Result: 在VOC和COCO数据集上的实验表明，该模块能显著提升少样本目标检测的性能，达到了新的水平。

Conclusion: SCSM模块能有效加强特征通道的表示能力，为少样本检测提供更优的解决方案。

Abstract: Due to the limited training samples in few-shot object detection (FSOD), we
observe that current methods may struggle to accurately extract effective
features from each channel. Specifically, this issue manifests in two aspects:
i) channels with high weights may not necessarily be effective, and ii)
channels with low weights may still hold significant value. To handle this
problem, we consider utilizing the inter-channel correlation to facilitate the
novel model's adaptation process to novel conditions, ensuring the model can
correctly highlight effective channels and rectify those incorrect ones. Since
the channel sequence is also 1-dimensional, its similarity with the temporal
sequence inspires us to take Mamba for modeling the correlation in the channel
sequence. Based on this concept, we propose a Spatial-Channel State Space
Modeling (SCSM) module for spatial-channel state modeling, which highlights the
effective patterns and rectifies those ineffective ones in feature channels. In
SCSM, we design the Spatial Feature Modeling (SFM) module to balance the
learning of spatial relationships and channel relationships, and then introduce
the Channel State Modeling (CSM) module based on Mamba to learn correlation in
channels. Extensive experiments on the VOC and COCO datasets show that the SCSM
module enables the novel detector to improve the quality of focused feature
representation in channels and achieve state-of-the-art performance.

</details>


### [97] [BenchDepth: Are We on the Right Way to Evaluate Depth Foundation Models?](https://arxiv.org/abs/2507.15321)
*Zhenyu Li,Haotong Lin,Jiashi Feng,Peter Wonka,Bingyi Kang*

Main category: cs.CV

TL;DR: 该论文提出了一个名为BenchDepth的新基准，用于评估深度基础模型(DFMs)在实际任务中的表现，规避传统方法的对齐问题偏差。


<details>
  <summary>Details</summary>
Motivation: 作者发现现有的深度估计评估协议存在偏差，无法公平反映DFMs在真实应用中的效用，因此提出改进方法。

Method: 设计了BenchDepth基准，通过五种下游代理任务(深度补全、立体匹配、单目前馈3D场景重建、SLAM、视觉-语言空间理解)评估DFMs实用性，避免对齐程序带来的问题。

Result: 对八种先进的DFMs进行了基准测试，提供了关于关键发现和观察的深入分析。

Conclusion: BenchDepth为DFMs评估提供了新的视角，希望能促进深度估计领域的讨论和未来研究进展。

Abstract: Depth estimation is a fundamental task in computer vision with diverse
applications. Recent advancements in deep learning have led to powerful depth
foundation models (DFMs), yet their evaluation remains challenging due to
inconsistencies in existing protocols. Traditional benchmarks rely on
alignment-based metrics that introduce biases, favor certain depth
representations, and complicate fair comparisons. In this work, we propose
BenchDepth, a new benchmark that evaluates DFMs through five carefully selected
downstream proxy tasks: depth completion, stereo matching, monocular
feed-forward 3D scene reconstruction, SLAM, and vision-language spatial
understanding. Unlike conventional evaluation protocols, our approach assesses
DFMs based on their practical utility in real-world applications, bypassing
problematic alignment procedures. We benchmark eight state-of-the-art DFMs and
provide an in-depth analysis of key findings and observations. We hope our work
sparks further discussion in the community on best practices for depth model
evaluation and paves the way for future research and advancements in depth
estimation.

</details>


### [98] [ExDD: Explicit Dual Distribution Learning for Surface Defect Detection via Diffusion Synthesis](https://arxiv.org/abs/2507.15335)
*Muhammad Aqeel,Federico Leonardi,Francesco Setti*

Main category: cs.CV

TL;DR: 本文提出了ExDD框架，通过显式建模双特征分布解决传统一类异常检测方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有工业缺陷检测系统在一类异常检测模型中表现不佳，包括假设异常分布均匀及面临数据稀缺问题。

Method: 通过并行记忆库分别捕捉正常与异常模式的统计特性，结合领域特定的潜在扩散模型生成合乎分布的合成缺陷，并采用新的评分机制融合距离指标。

Result: 在KSDD2数据集上验证，取得94.2% I-AUROC和97.7% P-AUROC的优异性能。

Conclusion: 该框架有效突破传统方法局限，可在资源受限的工业场景中实现高效可靠的缺陷检测。

Abstract: Industrial defect detection systems face critical limitations when confined
to one-class anomaly detection paradigms, which assume uniform outlier
distributions and struggle with data scarcity in realworld manufacturing
environments. We present ExDD (Explicit Dual Distribution), a novel framework
that transcends these limitations by explicitly modeling dual feature
distributions. Our approach leverages parallel memory banks that capture the
distinct statistical properties of both normality and anomalous patterns,
addressing the fundamental flaw of uniform outlier assumptions. To overcome
data scarcity, we employ latent diffusion models with domain-specific textual
conditioning, generating in-distribution synthetic defects that preserve
industrial context. Our neighborhood-aware ratio scoring mechanism elegantly
fuses complementary distance metrics, amplifying signals in regions exhibiting
both deviation from normality and similarity to known defect patterns.
Experimental validation on KSDD2 demonstrates superior performance (94.2%
I-AUROC, 97.7% P-AUROC), with optimal augmentation at 100 synthetic samples.

</details>


### [99] [RoadFusion: Latent Diffusion Model for Pavement Defect Detection](https://arxiv.org/abs/2507.15346)
*Muhammad Aqeel,Kidus Dagnaw Bellete,Francesco Setti*

Main category: cs.CV

TL;DR: 本文提出了RoadFusion框架，通过生成合成缺陷和双路径特征适配来改进路面缺陷检测，克服了数据不足、领域迁移和缺陷多样性等挑战，并在多项数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 路面缺陷检测面临数据不足、训练和部署环境的领域迁移，以及缺陷外观多样性等问题，这些限制阻碍了现有方法的性能表现。

Method: RoadFusion框架利用一种潜在扩散模型通过文本提示和空间掩码生成多样且逼真的缺陷，并引入了两个特征适配器，分别处理正常和异常输入。此外，一个轻量级判别器用于学习区分细粒度的缺陷模式。

Result: 在六个基准数据集上的实验表明，RoadFusion在分类和定位任务中都取得了一致的优异表现，并在多个与实际道路检测相关的指标上树立了新的基准。

Conclusion: RoadFusion有效地解决了路面缺陷检测中的数据不足和领域迁移等挑战，展示出较强的泛化能力和应用前景。

Abstract: Pavement defect detection faces critical challenges including limited
annotated data, domain shift between training and deployment environments, and
high variability in defect appearances across different road conditions. We
propose RoadFusion, a framework that addresses these limitations through
synthetic anomaly generation with dual-path feature adaptation. A latent
diffusion model synthesizes diverse, realistic defects using text prompts and
spatial masks, enabling effective training under data scarcity. Two separate
feature adaptors specialize representations for normal and anomalous inputs,
improving robustness to domain shift and defect variability. A lightweight
discriminator learns to distinguish fine-grained defect patterns at the patch
level. Evaluated on six benchmark datasets, RoadFusion achieves consistently
strong performance across both classification and localization tasks, setting
new state-of-the-art in multiple metrics relevant to real-world road
inspection.

</details>


### [100] [DAViD: Data-efficient and Accurate Vision Models from Synthetic Data](https://arxiv.org/abs/2507.15365)
*Fatemeh Saleh,Sadegh Aliakbarian,Charlie Hewitt,Lohit Petikam,Xiao-Xian,Antonio Criminisi,Thomas J. Cashman,Tadas Baltrušaitis*

Main category: cs.CV

TL;DR: 这篇论文展示了使用小规模的高保真合成数据集训练模型可以在不丧失准确性的同时实现更高的效率。


<details>
  <summary>Details</summary>
Motivation: 当前人类中心计算机视觉模型依赖大规模数据集和高计算需求，这推动了寻找更加高效的解决方案。

Method: 采用高保真合成数据集进行模型训练，同时通过程序化的数据合成控制数据的多样性以减少偏差。

Result: 模型在深度估计、表面法线估计和软前景分割三个任务中表现与最先进模型相当，但训练和推理成本显著降低。

Conclusion: 小规模高保真合成数据集是训练高效且准确的计算机视觉模型的可行方案，并提供了公平性控制和数据使用合法性保障。

Abstract: The state of the art in human-centric computer vision achieves high accuracy
and robustness across a diverse range of tasks. The most effective models in
this domain have billions of parameters, thus requiring extremely large
datasets, expensive training regimes, and compute-intensive inference. In this
paper, we demonstrate that it is possible to train models on much smaller but
high-fidelity synthetic datasets, with no loss in accuracy and higher
efficiency. Using synthetic training data provides us with excellent levels of
detail and perfect labels, while providing strong guarantees for data
provenance, usage rights, and user consent. Procedural data synthesis also
provides us with explicit control on data diversity, that we can use to address
unfairness in the models we train. Extensive quantitative assessment on real
input images demonstrates accuracy of our models on three dense prediction
tasks: depth estimation, surface normal estimation, and soft foreground
segmentation. Our models require only a fraction of the cost of training and
inference when compared with foundational models of similar accuracy. Our
human-centric synthetic dataset and trained models are available at
https://aka.ms/DAViD.

</details>


### [101] [Rethinking Occlusion in FER: A Semantic-Aware Perspective and Go Beyond](https://arxiv.org/abs/2507.15401)
*Huiyu Zhai,Xingxing Yang,Yalan Ye,Chenyang Li,Bin Fan,Changze Li*

Main category: cs.CV

TL;DR: 本文提出了ORSANet模型，通过多模态语义指导、跨尺度交互模块以及动态对抗排斥增强损失，实现了在面部表情识别任务中的最新性能表现，特别解决了面部遮挡问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有FER模型在面部信息部分遮挡时提取特征能力不足，分类不准确的问题。

Method: 1. 引入多模态语义指导，包括语义分割图和面部标志作为先验知识；2. 定制多尺度交互模块（MCM）融合多模态特征；3. 设计动态对抗排斥增强损失（DARELoss），动态调整模糊类的分类边界。

Result: 在公共数据集和新构建的Occlu-FER遮挡数据集上实验显示，ORSANet模型在面部表情识别任务中取得了SOTA性能表现。

Conclusion: 通过引入语义和几何先验知识、优化特征融合方式以及改进损失函数，ORSANet显著提升了遮挡条件下的面部表情识别性能。

Abstract: Facial expression recognition (FER) is a challenging task due to pervasive
occlusion and dataset biases. Especially when facial information is partially
occluded, existing FER models struggle to extract effective facial features,
leading to inaccurate classifications. In response, we present ORSANet, which
introduces the following three key contributions: First, we introduce auxiliary
multi-modal semantic guidance to disambiguate facial occlusion and learn
high-level semantic knowledge, which is two-fold: 1) we introduce semantic
segmentation maps as dense semantics prior to generate semantics-enhanced
facial representations; 2) we introduce facial landmarks as sparse geometric
prior to mitigate intrinsic noises in FER, such as identity and gender biases.
Second, to facilitate the effective incorporation of these two multi-modal
priors, we customize a Multi-scale Cross-interaction Module (MCM) to adaptively
fuse the landmark feature and semantics-enhanced representations within
different scales. Third, we design a Dynamic Adversarial Repulsion Enhancement
Loss (DARELoss) that dynamically adjusts the margins of ambiguous classes,
further enhancing the model's ability to distinguish similar expressions. We
further construct the first occlusion-oriented FER dataset to facilitate
specialized robustness analysis on various real-world occlusion conditions,
dubbed Occlu-FER. Extensive experiments on both public benchmarks and Occlu-FER
demonstrate that our proposed ORSANet achieves SOTA recognition performance.
Code is publicly available at https://github.com/Wenyuzhy/ORSANet-master.

</details>


### [102] [SurgX: Neuron-Concept Association for Explainable Surgical Phase Recognition](https://arxiv.org/abs/2507.15418)
*Ka Young Kim,Hyeon Bae Kim,Seong Tae Kim*

Main category: cs.CV

TL;DR: SurgX通过将神经元与相关概念关联，为外科手术阶段识别模型提供了一种全新的基于概念的解释框架。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度学习的外科手术阶段识别模型尽管有显著进展，但其决策过程不透明，缺乏解释性，从而影响了信任和调试能力。

Method: 提出并实现了SurgX框架，通过选择神经元的代表性序列、构建手术视频特定的概念集、将神经元与概念关联，并识别对预测起关键作用的神经元来提高模型的可解释性。

Result: 通过在两个外科手术阶段识别模型上的广泛实验，验证了方法的有效性，并分析了预测过程的解释性。

Conclusion: SurgX方法展示了为手术阶段识别模型提供解释的潜力，增强了模型的透明度和信任性。

Abstract: Surgical phase recognition plays a crucial role in surgical workflow
analysis, enabling various applications such as surgical monitoring, skill
assessment, and workflow optimization. Despite significant advancements in deep
learning-based surgical phase recognition, these models remain inherently
opaque, making it difficult to understand how they make decisions. This lack of
interpretability hinders trust and makes it challenging to debug the model. To
address this challenge, we propose SurgX, a novel concept-based explanation
framework that enhances the interpretability of surgical phase recognition
models by associating neurons with relevant concepts. In this paper, we
introduce the process of selecting representative example sequences for
neurons, constructing a concept set tailored to the surgical video dataset,
associating neurons with concepts and identifying neurons crucial for
predictions. Through extensive experiments on two surgical phase recognition
models, we validate our method and analyze the explanation for prediction. This
highlights the potential of our method in explaining surgical phase
recognition. The code is available at https://github.com/ailab-kyunghee/SurgX

</details>


### [103] [EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent](https://arxiv.org/abs/2507.15428)
*Jiaao Li,Kaiyuan Li,Chen Gao,Yong Li,Xinlei Chen*

Main category: cs.CV

TL;DR: 提出了一种名为EgoPrune的训练无关token剪枝方法，用于提高自运动视频（egomotion video）推理的效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决第一人称视角录像中，由于视频冗长、运动连续性和计算成本高导致的推理效率低下问题。

Method: EgoPrune由三个组件组成：从EmbodiedR调整的关键帧选择器、基于透视变换的冗余过滤（PARF）以及结合视觉文本相关性和帧内多样性的最大边际相关性（MMR）token选择器。

Result: 在两个自运动视频基准上，EgoPrune在不同剪枝比例下表现优于其他训练无关方法，同时显著降低了FLOPs、内存使用和延迟。此外，其在Jetson Orin NX 16GB设备上的部署验证了其在实际场景中的效率。

Conclusion: EgoPrune有效解决了自运动视频推理中的高计算成本问题，是一种适用于真实场景和设备的高效方法。

Abstract: Egomotion videos are first-person recordings where the view changes
continuously due to the agent's movement. As they serve as the primary visual
input for embodied AI agents, making egomotion video reasoning more efficient
is therefore essential for real-world deployment. Recent advances in
vision-language models have enabled strong multimodal reasoning capabilities,
but their computational cost remains prohibitive for long, redundant video
inputs. Existing token pruning methods, typically designed for third-person
videos, fail to leverage the spatiotemporal continuity and motion constraints
inherent in egomotion settings. To address this, we propose EgoPrune, a
training-free token pruning method tailored for egomotion video reasoning.
EgoPrune comprises three components: a keyframe selector adapted from EmbodiedR
for temporally efficient sampling; Perspective-Aware Redundancy Filtering
(PARF), which aligns visual tokens using perspective transformations and
removes redundant tokens; and a Maximal Marginal Relevance (MMR)-based token
selector that jointly considers visual-text relevance and intra-frame
diversity. Experiments on two egomotion video benchmarks show that EgoPrune
consistently outperforms prior training-free methods across various pruning
ratios while significantly reducing FLOPs, memory usage, and latency. Moreover,
we deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB
edge device, demonstrating its real-world efficiency and suitability for
on-device egomotion video reasoning.

</details>


### [104] [One Last Attention for Your Vision-Language Model](https://arxiv.org/abs/2507.15480)
*Liang Chen,Ghazi Shazan Ahmad,Tianjun Yao,Lingqiao Liu,Zhiqiang Shen*

Main category: cs.CV

TL;DR: 提出RAda，一种改进预训练视觉-语言模型调优能力的方法，通过动态调整交叉模态交互的理性矩阵，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在改进预训练视觉-语言模型时通常忽略了模态融合表示的作用，而这对决策过程至关重要。

Method: RAda使用轻量化的注意力层动态校准理性矩阵中每个元素的贡献，以优化最终的模态融合表示，且无需对中间特征进行复杂修改。

Result: 实验表明，RAda作为一种通用的微调技术，可以通过很少的代码改进基线表现，并在大多数情况下与当前先进技术表现相当。

Conclusion: RAda有效提升了视觉-语言模型的微调能力，其方法简便且具有广泛适用性。

Abstract: Pretrained vision-language models (VLMs), such as CLIP, achieve remarkable
zero-shot performance, yet their downstream potential hinges on effective
fine-tuning. Most adaptation methods typically focus on refining representation
from separate modalities (text or vision) but neglect the critical role of
their fused representations in the decision-making process, \emph{\ie} rational
matrix that drives the final prediction. To bridge the gap, we propose a simple
yet effective \textbf{R}ational \textbf{Ada}ptaion ({RAda}) to explicitly
exploit the final fused representation during fine-tuning. RAda employs a
learned mask, obtained from a lightweight attention layer attached at the end
of a VLM, to dynamically calibrate the contribution of each element in the
rational matrix, enabling targeted adjustments to the final cross-modal
interactions without incurring costly modifications to intermediate features.
Experiments in different settings (i.e., updating, or freezing pretrained
encoders in adaptation, and test-time training that can only access the
unlabeled test data) show that RAda serves as a versatile fine-tuning
technique, improving the baseline with minimal code and performing comparably
against current arts in most settings. Code is available at
\href{https://github.com/khufia/RAda/tree/main}{github.com/khufia/RAda}.

</details>


### [105] [An aerial color image anomaly dataset for search missions in complex forested terrain](https://arxiv.org/abs/2507.15492)
*Rakesh John Amala Arokia Nathan,Matthias Gessner,Nurullah Özkan,Marius Bock,Mohamed Youssef,Maximilian Mews,Björn Piltz,Ralf Berger,Oliver Bimber*

Main category: cs.CV

TL;DR: 研究者利用航空影像数据制定了一套供复杂丛林环境异常检测的公开数据集与测试基准。现有方法表现欠佳，凸显了需要更加贴近实际场景的检出技术。该数据集附带交互式界面，方便用户标注与动态扩充。


<details>
  <summary>Details</summary>
Motivation: 现有搜索方法在森林复杂环境中难以应对，例如在德国的一次搜寻行动中，茂密植被遮蔽导致自动分析无效，从而引发对新技术的需求。

Method: 通过研究飞机获取高分辨率航空影像，建立了一个包含稀疏目标标注的独特数据集，用于复杂环境下的异常检测。此外，通过开放的在线界面，支持用户查看和动态标注更新数据。

Result: 现有的异常检测方法在公开数据集上初步测试表现较差，证明了这些方法在森林复杂环境中的不足。

Conclusion: 该研究揭示了异常检测领域在处理复杂、遮蔽环境时的方法局限，新发布的数据集和平台将激发更强场景适应性的方法开发。

Abstract: After a family murder in rural Germany, authorities failed to locate the
suspect in a vast forest despite a massive search. To aid the search, a
research aircraft captured high-resolution aerial imagery. Due to dense
vegetation obscuring small clues, automated analysis was ineffective, prompting
a crowd-search initiative. This effort produced a unique dataset of labeled,
hard-to-detect anomalies under occluded, real-world conditions. It can serve as
a benchmark for improving anomaly detection approaches in complex forest
environments, supporting manhunts and rescue operations. Initial benchmark
tests showed existing methods performed poorly, highlighting the need for
context-aware approaches. The dataset is openly accessible for offline
processing. An additional interactive web interface supports online viewing and
dynamic growth by allowing users to annotate and submit new findings.

</details>


### [106] [Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images](https://arxiv.org/abs/2507.15496)
*JunYing Huang,Ao Xu,DongSun Yong,KeRen Li,YuanFeng Wang,Qi Qin*

Main category: cs.CV

TL;DR: 提出了一种将LiDAR点云和图像结合的框架，用于精确和鲁棒的位姿估计。


<details>
  <summary>Details</summary>
Motivation: 旨在解决自主系统中里程计任务的精确性和鲁棒性问题，尤其是动态环境和尺度模糊的挑战。

Method: 结合LiDAR点云和图像，通过深度补全生成稠密深度图，利用多尺度特征提取网络和注意力机制实现自适应深度感知表达，并通过分层运动估计模块优化位姿估计。

Result: 在KITTI里程计基准上展现出与现有方法相当或更优的精度和鲁棒性。

Conclusion: 提出方法在动态环境与尺度模糊情况下表现优异，为自主里程计系统提供了新思路。

Abstract: Odometry is a critical task for autonomous systems for self-localization and
navigation. We propose a novel LiDAR-Visual odometry framework that integrates
LiDAR point clouds and images for accurate and robust pose estimation. Our
method utilizes a dense-depth map estimated from point clouds and images
through depth completion, and incorporates a multi-scale feature extraction
network with attention mechanisms, enabling adaptive depth-aware
representations. Furthermore, we leverage dense depth information to refine
flow estimation and mitigate errors in occlusion-prone regions. Our
hierarchical pose refinement module optimizes motion estimation progressively,
ensuring robust predictions against dynamic environments and scale ambiguities.
Comprehensive experiments on the KITTI odometry benchmark demonstrate that our
approach achieves similar or superior accuracy and robustness compared to
state-of-the-art visual and LiDAR odometry methods.

</details>


### [107] [Quantifying and Narrowing the Unknown: Interactive Text-to-Video Retrieval via Uncertainty Minimization](https://arxiv.org/abs/2507.15504)
*Bingqing Zhang,Zhuo Cao,Heming Du,Yang Li,Xue Li,Jiajun Liu,Sen Wang*

Main category: cs.CV

TL;DR: 提出UMIVR框架，通过量化和最小化不确定性来改进文本到视频检索的交互式系统。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到视频检索系统因查询文本的歧义、模糊的映射关系及视频画质低等问题受限，而现有的交互式方法未能有效量化这些不确定性。

Method: 设计UMIVR框架，提出基于语义熵的文本歧义得分、Jensen-Shannon散度的映射不确定得分及基于时间质量的帧采样，通过这些指标引导生成澄清性问题，不断优化用户查询。

Result: 在多个基准数据集上进行实验，UMIVR在MSR-VTT-1k中实现了Recall@1的显著提升（69.2%在10次交互后）。

Conclusion: UMIVR有效减少了检索的不确定性，为交互式文本到视频检索奠定了基础。

Abstract: Despite recent advances, Text-to-video retrieval (TVR) is still hindered by
multiple inherent uncertainties, such as ambiguous textual queries, indistinct
text-video mappings, and low-quality video frames. Although interactive systems
have emerged to address these challenges by refining user intent through
clarifying questions, current methods typically rely on heuristic or ad-hoc
strategies without explicitly quantifying these uncertainties, limiting their
effectiveness. Motivated by this gap, we propose UMIVR, an
Uncertainty-Minimizing Interactive Text-to-Video Retrieval framework that
explicitly quantifies three critical uncertainties-text ambiguity, mapping
uncertainty, and frame uncertainty-via principled, training-free metrics:
semantic entropy-based Text Ambiguity Score (TAS), Jensen-Shannon
divergence-based Mapping Uncertainty Score (MUS), and a Temporal Quality-based
Frame Sampler (TQFS). By adaptively generating targeted clarifying questions
guided by these uncertainty measures, UMIVR iteratively refines user queries,
significantly reducing retrieval ambiguity. Extensive experiments on multiple
benchmarks validate UMIVR's effectiveness, achieving notable gains in Recall@1
(69.2\% after 10 interactive rounds) on the MSR-VTT-1k dataset, thereby
establishing an uncertainty-minimizing foundation for interactive TVR.

</details>


### [108] [SAIGFormer: A Spatially-Adaptive Illumination-Guided Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.15520)
*Hanting Li,Fei Zhou,Xin Sun,Yang Hua,Jungong Han,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: 本文提出了SAIGFormer框架，重点解决非均匀光照场景中的低光增强问题，表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前的Transformer-based低光增强方法在非均匀光照场景下表现不足，如逆光和阴影。

Method: 提出动态积分图像表示和空间自适应积分光照估计器(SAI2E)，并设计光照引导多头自注意力(IG-MSA)机制。

Result: 在五个标准低光数据集及跨领域基准LOL-Blur上都取得了定量和定性指标的显著提升，尤其是非均匀光照增强表现出色并具有较强的泛化能力。

Conclusion: SAIGFormer在低光增强任务，特别是非均匀光照场景下，具有显著优势，是一种突破性的框架。

Abstract: Recent Transformer-based low-light enhancement methods have made promising
progress in recovering global illumination. However, they still struggle with
non-uniform lighting scenarios, such as backlit and shadow, appearing as
over-exposure or inadequate brightness restoration. To address this challenge,
we present a Spatially-Adaptive Illumination-Guided Transformer (SAIGFormer)
framework that enables accurate illumination restoration. Specifically, we
propose a dynamic integral image representation to model the spatially-varying
illumination, and further construct a novel Spatially-Adaptive Integral
Illumination Estimator ($\text{SAI}^2\text{E}$). Moreover, we introduce an
Illumination-Guided Multi-head Self-Attention (IG-MSA) mechanism, which
leverages the illumination to calibrate the lightness-relevant features toward
visual-pleased illumination enhancement. Extensive experiments on five standard
low-light datasets and a cross-domain benchmark (LOL-Blur) demonstrate that our
SAIGFormer significantly outperforms state-of-the-art methods in both
quantitative and qualitative metrics. In particular, our method achieves
superior performance in non-uniform illumination enhancement while exhibiting
strong generalization capabilities across multiple datasets. Code is available
at https://github.com/LHTcode/SAIGFormer.git.

</details>


### [109] [Procedure Learning via Regularized Gromov-Wasserstein Optimal Transport](https://arxiv.org/abs/2507.15540)
*Syed Ahmed Mahmood,Ali Shah Ali,Umer Ahmed,Fawad Javed Fateh,M. Zeeshan Zia,Quoc-Huy Tran*

Main category: cs.CV

TL;DR: 本文提出了一种基于自监督学习的程序学习框架，用于从未标注程序视频中发现关键步骤及其顺序。


<details>
  <summary>Details</summary>
Motivation: 解决以往方法中因顺序变化、背景干扰和重复动作导致的问题。

Method: 采用融合结构先验的Gromov-Wasserstein最优传输公式计算视频帧间的映射，并结合对比正则化避免嵌入空间的退化解。

Result: 通过在大规模数据集（EgoProceL、ProceL和CrossTask）上的实验，证明了该方法在精确性上超越了之前的工作。

Conclusion: 结合新的最优传输模型和对比正则化，该方法有效提高了程序学习的鲁棒性和准确性。

Abstract: We study the problem of self-supervised procedure learning, which discovers
key steps and establishes their order from a set of unlabeled procedural
videos. Previous procedure learning methods typically learn frame-to-frame
correspondences between videos before determining key steps and their order.
However, their performance often suffers from order variations,
background/redundant frames, and repeated actions. To overcome these
challenges, we propose a self-supervised procedure learning framework, which
utilizes a fused Gromov-Wasserstein optimal transport formulation with a
structural prior for computing frame-to-frame mapping between videos. However,
optimizing exclusively for the above temporal alignment term may lead to
degenerate solutions, where all frames are mapped to a small cluster in the
embedding space and hence every video is associated with only one key step. To
address that limitation, we further integrate a contrastive regularization
term, which maps different frames to different points in the embedding space,
avoiding the collapse to trivial solutions. Finally, we conduct extensive
experiments on large-scale egocentric (i.e., EgoProceL) and third-person (i.e.,
ProceL and CrossTask) benchmarks to demonstrate superior performance by our
approach against previous methods, including OPEL which relies on a traditional
Kantorovich optimal transport formulation with an optimality prior.

</details>


### [110] [Towards Holistic Surgical Scene Graph](https://arxiv.org/abs/2507.15541)
*Jongmin Shin,Enki Cho,Ka Yong Kim,Jung Yong Kim,Seong Tae Kim,Namkee Oh*

Main category: cs.CV

TL;DR: 提出了Endoscapes-SG201数据集和SSG-Com方法，包括工具-动作-目标组合和手掌身份的注释，用于改进外科场景图表征，显著提升关键任务表现。


<details>
  <summary>Details</summary>
Motivation: 改进计算机辅助干预系统对手术场景的理解，包括工具-动作-目标组合及手掌身份信息，这些对于手术场景解读至关重要却未被深入研究。

Method: 开发了Endoscapes-SG201数据集及SSG-Com方法，利用图结构表征工具-动作-目标组合和手掌身份，支持下游任务。

Result: 通过关键安全视图评估和动作三元组识别实验，证明提出的场景图元素整合对手术场景理解的重要性。

Conclusion: 新数据集及图模型兼顾多种手术重要信息，显著提升了图化方法解析手术场景的能力，并公开代码和数据集以促进行业发展。

Abstract: Surgical scene understanding is crucial for computer-assisted intervention
systems, requiring visual comprehension of surgical scenes that involves
diverse elements such as surgical tools, anatomical structures, and their
interactions. To effectively represent the complex information in surgical
scenes, graph-based approaches have been explored to structurally model
surgical entities and their relationships. Previous surgical scene graph
studies have demonstrated the feasibility of representing surgical scenes using
graphs. However, certain aspects of surgical scenes-such as diverse
combinations of tool-action-target and the identity of the hand operating the
tool-remain underexplored in graph-based representations, despite their
importance. To incorporate these aspects into graph representations, we propose
Endoscapes-SG201 dataset, which includes annotations for tool-action-target
combinations and hand identity. We also introduce SSG-Com, a graph-based method
designed to learn and represent these critical elements. Through experiments on
downstream tasks such as critical view of safety assessment and action triplet
recognition, we demonstrated the importance of integrating these essential
scene graph components, highlighting their significant contribution to surgical
scene understanding. The code and dataset are available at
https://github.com/ailab-kyunghee/SSG-Com

</details>


### [111] [HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation](https://arxiv.org/abs/2507.15542)
*Qinqian Lei,Bo Wang,Robby T. Tan*

Main category: cs.CV

TL;DR: 本论文提出HOLa方法，通过低秩分解和LLM引导的特征调整，提升零样本人类-物体交互检测的泛化能力和动作区分性，在HICO-DET数据集上达到了新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以区分涉及相同物体的动作，且对未见类别的泛化能力有限。

Method: 提出HOLa方法，采用低秩分解生成共享基础特征和可调权重，并通过人-物体标记和LLM指导的动作正则化改进视觉交互表示和动作区分。

Result: 在HICO-DET数据集零样本设置下，实现未见动词类别的mAP为27.91，刷新当前SOTA。

Conclusion: HOLa方法显著提升零样本人类-物体交互检测的泛化能力与动作区分性能，是一个强有力的新方法。

Abstract: Zero-shot human-object interaction (HOI) detection remains a challenging
task, particularly in generalizing to unseen actions. Existing methods address
this challenge by tapping Vision-Language Models (VLMs) to access knowledge
beyond the training data. However, they either struggle to distinguish actions
involving the same object or demonstrate limited generalization to unseen
classes. In this paper, we introduce HOLa (Zero-Shot HOI Detection with
Low-Rank Decomposed VLM Feature Adaptation), a novel approach that both
enhances generalization to unseen classes and improves action distinction. In
training, HOLa decomposes VLM text features for given HOI classes via low-rank
factorization, producing class-shared basis features and adaptable weights.
These features and weights form a compact HOI representation that preserves
shared information across classes, enhancing generalization to unseen classes.
Subsequently, we refine action distinction by adapting weights for each HOI
class and introducing human-object tokens to enrich visual interaction
representations. To further distinguish unseen actions, we guide the weight
adaptation with LLM-derived action regularization. Experimental results show
that our method sets a new state-of-the-art across zero-shot HOI settings on
HICO-DET, achieving an unseen-class mAP of 27.91 in the unseen-verb setting.
Our code is available at https://github.com/ChelsieLei/HOLa.

</details>


### [112] [DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding](https://arxiv.org/abs/2507.15569)
*Xiaoyi Bao,Chenwei Xie,Hao Tang,Tingyu Weng,Xiaofeng Wang,Yun Zheng,Xingang Wang*

Main category: cs.CV

TL;DR: 本研究提出了一种创新的视频表示方法Dynamic-Image (DynImg)，通过引入非关键帧作为时间提示以提升模型对快速移动物体的关注，从而有效改善了视频理解能力。


<details>
  <summary>Details</summary>
Motivation: 传统的视频理解方法难以准确表示快速运动物体的空间信息，造成重要的时序区域被忽略，从而影响时空交互的准确性。

Method: 提出Dynamic-Image (DynImg)方法，引入非关键帧作为时间提示，引导模型关注快速移动物体的细粒度空间特征，并使用4D视频旋转位置嵌入保持DynImg的时空顺序。

Result: 在多个视频理解基准上，DynImg方法比现有最佳方法提高约2%的性能。

Conclusion: 通过时间提示增强视频理解效果，证明了DynImg方法在时空信息整合上的高效性。

Abstract: In recent years, the introduction of Multi-modal Large Language Models
(MLLMs) into video understanding tasks has become increasingly prevalent.
However, how to effectively integrate temporal information remains a critical
research focus. Traditional approaches treat spatial and temporal information
separately. Due to issues like motion blur, it is challenging to accurately
represent the spatial information of rapidly moving objects. This can lead to
temporally important regions being underemphasized during spatial feature
extraction, which in turn hinders accurate spatio-temporal interaction and
video understanding. To address this limitation, we propose an innovative video
representation method called Dynamic-Image (DynImg). Specifically, we introduce
a set of non-key frames as temporal prompts to highlight the spatial areas
containing fast-moving objects. During the process of visual feature
extraction, these prompts guide the model to pay additional attention to the
fine-grained spatial features corresponding to these regions. Moreover, to
maintain the correct sequence for DynImg, we employ a corresponding 4D video
Rotary Position Embedding. This retains both the temporal and spatial adjacency
of DynImg, helping MLLM understand the spatio-temporal order within this
combined format. Experimental evaluations reveal that DynImg surpasses the
state-of-the-art methods by approximately 2% across multiple video
understanding benchmarks, proving the effectiveness of our temporal prompts in
enhancing video comprehension.

</details>


### [113] [GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation](https://arxiv.org/abs/2507.15577)
*Hugo Carlesso,Maria Eliza Patulea,Moncef Garouani,Radu Tudor Ionescu,Josiane Mothe*

Main category: cs.CV

TL;DR: 提出了一种基于生成对抗网络的增强方法GeMix，用于改进图像分类中的Mixup方法，着重在COVID-19检测中提升准确率。


<details>
  <summary>Details</summary>
Motivation: 传统Mixup方法因像素级插值可能生成不现实图像，特别在医疗场景中存在局限性。

Method: 使用两阶段框架，先训练StyleGAN2-ADA生成器，后利用Dirichlet和Beta分布采样混合类标签进行插值生成视觉上连贯的图像。

Result: 在COVIDx-CT-3数据集与三种模型的测试中，相比传统Mixup方法全线提升了macro-F1值，减少COVID-19检测中假阴性率。

Conclusion: 证明GeMix作为一种增强策略的有效性，为图像分类提供更强正则化及语义保真同时保持现有训练流程，代码已开源促进重复性及进一步研究。

Abstract: Mixup has become a popular augmentation strategy for image classification,
yet its naive pixel-wise interpolation often produces unrealistic images that
can hinder learning, particularly in high-stakes medical applications. We
propose GeMix, a two-stage framework that replaces heuristic blending with a
learned, label-aware interpolation powered by class-conditional GANs. First, a
StyleGAN2-ADA generator is trained on the target dataset. During augmentation,
we sample two label vectors from Dirichlet priors biased toward different
classes and blend them via a Beta-distributed coefficient. Then, we condition
the generator on this soft label to synthesize visually coherent images that
lie along a continuous class manifold. We benchmark GeMix on the large-scale
COVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101,
EfficientNet-B0). When combined with real data, our method increases macro-F1
over traditional mixup for all backbones, reducing the false negative rate for
COVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup,
delivering stronger regularization and greater semantic fidelity, without
disrupting existing training pipelines. We publicly release our code at
https://github.com/hugocarlesso/GeMix to foster reproducibility and further
research.

</details>


### [114] [Compress-Align-Detect: onboard change detection from unregistered images](https://arxiv.org/abs/2507.15578)
*Gabriele Inzerillo,Diego Valsesia,Aniello Fiengo,Enrico Magli*

Main category: cs.CV

TL;DR: 该论文提出了一个全新的在卫星上执行的端到端变化检测框架，以克服传统基于地面站的操作延时问题。


<details>
  <summary>Details</summary>
Motivation: 传统的基于地面站的图像处理操作延时长，不适合实时或近实时应用，因此需要将整个变化检测流程转移到卫星上。

Method: 提出一种基于深度神经网络的框架，包含图像压缩、轻量级配准及高效的变化检测模型三个子模块，解决了数据存储、图像配准以及复杂度限制问题。

Result: 实验结果显示该方法在低功耗硬件上，以压缩率为基础实现了较高的F1分数，并能达到0.7 Mpixel/s的吞吐量。

Conclusion: 此方法结合了变化检测的多项任务，是首个针对卫星端限制设计的完整框架，展示了极具竞争力的实时性能。

Abstract: Change detection from satellite images typically incurs a delay ranging from
several hours up to days because of latency in downlinking the acquired images
and generating orthorectified image products at the ground stations; this may
preclude real- or near real-time applications. To overcome this limitation, we
propose shifting the entire change detection workflow onboard satellites. This
requires to simultaneously solve challenges in data storage, image registration
and change detection with a strict complexity constraint. In this paper, we
present a novel and efficient framework for onboard change detection that
addresses the aforementioned challenges in an end-to-end fashion with a deep
neural network composed of three interlinked submodules: (1) image compression,
tailored to minimize onboard data storage resources; (2) lightweight
co-registration of non-orthorectified multi-temporal image pairs; and (3) a
novel temporally-invariant and computationally efficient change detection
model. This is the first approach in the literature combining all these tasks
in a single end-to-end framework with the constraints dictated by onboard
processing. Experimental results compare each submodule with the current
state-of-the-art, and evaluate the performance of the overall integrated system
in realistic setting on low-power hardware. Compelling change detection results
are obtained in terms of F1 score as a function of compression rate, sustaining
a throughput of 0.7 Mpixel/s on a 15W accelerator.

</details>


### [115] [SegDT: A Diffusion Transformer-Based Segmentation Model for Medical Imaging](https://arxiv.org/abs/2507.15595)
*Salah Eddine Bekhouche,Gaby Maroun,Fadi Dornaika,Abdenour Hadid*

Main category: cs.CV

TL;DR: 提出了一种名为SegDT的皮肤病变分割模型，基于Diffusion Transformer (DiT)，性能优越且高效。


<details>
  <summary>Details</summary>
Motivation: 为低成本硬件环境中提供高效且准确的皮肤病变分割解决方案，助力医疗诊断。

Method: 使用Diffusion Transformer结合Rectified Flow优化生成质量与推理步骤，并在三个基准数据集上进行评估。

Result: 在基准数据集上实现了SOTA的分割效果，且推理速度快。

Conclusion: 该模型具有实际应用潜力，可显著提高医疗图像分析效率，为医疗行业提供高效诊断工具。

Abstract: Medical image segmentation is crucial for many healthcare tasks, including
disease diagnosis and treatment planning. One key area is the segmentation of
skin lesions, which is vital for diagnosing skin cancer and monitoring
patients. In this context, this paper introduces SegDT, a new segmentation
model based on diffusion transformer (DiT). SegDT is designed to work on
low-cost hardware and incorporates Rectified Flow, which improves the
generation quality at reduced inference steps and maintains the flexibility of
standard diffusion models. Our method is evaluated on three benchmarking
datasets and compared against several existing works, achieving
state-of-the-art results while maintaining fast inference speeds. This makes
the proposed model appealing for real-world medical applications. This work
advances the performance and capabilities of deep learning models in medical
image analysis, enabling faster, more accurate diagnostic tools for healthcare
professionals. The code is made publicly available at
\href{https://github.com/Bekhouche/SegDT}{GitHub}.

</details>


### [116] [Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos](https://arxiv.org/abs/2507.15597)
*Hao Luo,Yicheng Feng,Wanpeng Zhang,Sipeng Zheng,Ye Wang,Haoqi Yuan,Jiazheng Liu,Chaoyi Xu,Qin Jin,Zongqing Lu*

Main category: cs.CV

TL;DR: 提出了一种名为Being-H0的视觉-语言-行动模型，通过人类视频的训练解决复杂操作任务中的灵活性和泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型无法很好地解决需要高灵活性和泛化能力的操作任务，受限于合成数据或缺乏多样性和规模的远程演示数据。

Method: 引入体感指令微调，通过人类视频预训练、物理空间对齐和后期训练适配联动；采用部分运动标记方法精确建模手部动作轨迹；构建大规模异质数据集。

Result: Being-H0在手部运动生成和指令执行上表现卓越，并随着模型和数据规模扩大效果显著提升。

Conclusion: Being-H0通过体感指令微调在现实世界机器人操作任务中表现优异，验证了新范式的有效性。

Abstract: We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained
on large-scale human videos. Existing VLAs struggle with complex manipulation
tasks requiring high dexterity and generalize poorly to novel scenarios and
tasks, primarily due to their reliance on synthetic data with significant
sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To
address this data bottleneck, we propose leveraging human hands as a foundation
manipulator, capitalizing on the rich dexterity and scalability present in web
data. Our approach centers on physical instruction tuning, a novel training
paradigm that combines large-scale VLA pretraining from human videos, physical
space alignment for 3D reasoning, and post-training adaptation for robotic
tasks. Additionally, we introduce a part-level motion tokenization method which
achieves millimeter-level reconstruction accuracy to model precise hand
trajectories for action learning. To support our proposed paradigm, we further
develop a comprehensive data curation pipeline that integrates heterogeneous
sources -- including motion capture, VR, and RGB-only videos -- into a
large-scale dataset with millions of motion-based instructional instances. We
empirically show the excellence of Being-H0 in hand motion generation and
instruction following, and it also scales well with model and data sizes.
Importantly, we observe the expected gains of Being-H0 in real-world robotic
manipulation as physical instruction tuning is applied. More details are
available at https://beingbeyond.github.io/Being-H0.

</details>


### [117] [SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting](https://arxiv.org/abs/2507.15602)
*Zihui Gao,Jia-Wang Bian,Guosheng Lin,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 提出了一种融合SDF与3D高斯散点的混合方法，改进表面重建与新视图渲染性能。


<details>
  <summary>Details</summary>
Motivation: 传统的SDF方法在处理细节时有局限，3D高斯散点方法在全局几何一致性上存在不足，两者单独使用效果有限。

Method: 利用SDF捕捉粗几何结构，增强3D高斯散点渲染，并通过新的渲染图像对SDF进行细节优化，从而实现高精度的表面重建。

Result: 所提方法在DTU和MobileBrick数据集上的表面重建及新视图合成实验中超越当前最先进方法。

Conclusion: 混合利用SDF和3D高斯散点的策略能有效结合双方优点，提升了表面重建及新视图渲染的质量。

Abstract: Surface reconstruction and novel view rendering from sparse-view images are
challenging. Signed Distance Function (SDF)-based methods struggle with fine
details, while 3D Gaussian Splatting (3DGS)-based approaches lack global
geometry coherence. We propose a novel hybrid method that combines the
strengths of both approaches: SDF captures coarse geometry to enhance
3DGS-based rendering, while newly rendered images from 3DGS refine the details
of SDF for accurate surface reconstruction. As a result, our method surpasses
state-of-the-art approaches in surface reconstruction and novel view synthesis
on the DTU and MobileBrick datasets. Code will be released at
https://github.com/Gaozihui/SurfaceSplat.

</details>


### [118] [CylinderPlane: Nested Cylinder Representation for 3D-aware Image Generation](https://arxiv.org/abs/2507.15606)
*Ru Jia,Xiaozhuang Ma,Jianji Wang,Nanning Zheng*

Main category: cs.CV

TL;DR: 该论文提出了一种名为CylinderPlane的隐式表示方法，基于柱面坐标系统，解决了Tri-plane表示中多视角一致性问题，取得了更高质量的360°图像生成效果。


<details>
  <summary>Details</summary>
Motivation: Tri-plane表示模型在360°生成图像时受多面伪影和特征共享问题的限制，本文希望通过新方法消除这些问题以提升生成质量。

Method: 提出基于柱面坐标系统的CylinderPlane表示，将特征分离并引入多层嵌套的柱面以处理复杂几何和多尺度细节，具有较强适应性并支持与各种神经渲染管线的集成。

Result: 在合成数据集和未经结构化处理的自然图像测试中，新方法在质量、细节和一致性方面优于现有方法。

Conclusion: CylinderPlane表示通过改进特征分离及多尺度融合设计，在360°图像生成任务中展现出显著性能提升，同时兼具易用性与通用性。

Abstract: While the proposal of the Tri-plane representation has advanced the
development of the 3D-aware image generative models, problems rooted in its
inherent structure, such as multi-face artifacts caused by sharing the same
features in symmetric regions, limit its ability to generate 360$^\circ$ view
images. In this paper, we propose CylinderPlane, a novel implicit
representation based on Cylindrical Coordinate System, to eliminate the feature
ambiguity issue and ensure multi-view consistency in 360$^\circ$. Different
from the inevitable feature entanglement in Cartesian coordinate-based
Tri-plane representation, the cylindrical coordinate system explicitly
separates features at different angles, allowing our cylindrical representation
possible to achieve high-quality, artifacts-free 360$^\circ$ image synthesis.
We further introduce the nested cylinder representation that composites
multiple cylinders at different scales, thereby enabling the model more
adaptable to complex geometry and varying resolutions. The combination of
cylinders with different resolutions can effectively capture more critical
locations and multi-scale features, greatly facilitates fine detail learning
and robustness to different resolutions. Moreover, our representation is
agnostic to implicit rendering methods and can be easily integrated into any
neural rendering pipeline. Extensive experiments on both synthetic dataset and
unstructured in-the-wild images demonstrate that our proposed representation
achieves superior performance over previous methods.

</details>


### [119] [A Survey on Efficiency Optimization Techniques for DNN-based Video Analytics: Process Systems, Algorithms, and Applications](https://arxiv.org/abs/2507.15628)
*Shanjiang Tang,Rui Huang,Hsinyu Luo,Chunjiang Wang,Ce Yu,Yusen Li,Hao Fu,Chao Sun,and Jian Xiao*

Main category: cs.CV

TL;DR: 本文综述了用于提升视频分析中DNN效率的优化技术，并从底层硬件支持到高层应用部署进行了分类梳理，分析了现存技术的不足与挑战。


<details>
  <summary>Details</summary>
Motivation: 随着视频数据的爆发式增长，视频分析在准确性和效率方面提出了更高的需求，DNN已被广泛用于保证准确性，但效率优化仍是未解决的挑战。

Method: 本文采用综述研究的方法，从底层硬件支持到数据处理及部署等多方面总结了已有优化技术，并提供了一个综合的优化框架。

Result: 本文整理并分类了提升视频分析效率的方法，并分析了现存工作中的问题与挑战，为未来研究方向提供了参考。

Conclusion: 提高DNN在视频分析中的效率是当前研究的重点，本文为该领域提供了系统性的回顾与分析，有助于推进相关技术的发展。

Abstract: The explosive growth of video data in recent years has brought higher demands
for video analytics, where accuracy and efficiency remain the two primary
concerns. Deep neural networks (DNNs) have been widely adopted to ensure
accuracy; however, improving their efficiency in video analytics remains an
open challenge. Different from existing surveys that make summaries of
DNN-based video mainly from the accuracy optimization aspect, in this survey,
we aim to provide a thorough review of optimization techniques focusing on the
improvement of the efficiency of DNNs in video analytics. We organize existing
methods in a bottom-up manner, covering multiple perspectives such as hardware
support, data processing, operational deployment, etc. Finally, based on the
optimization framework and existing works, we analyze and discuss the problems
and challenges in the performance optimization of DNN-based video analytics.

</details>


### [120] [Experimenting active and sequential learning in a medieval music manuscript](https://arxiv.org/abs/2507.15633)
*Sachin Sharma,Federico Simonetta,Michele Flammini*

Main category: cs.CV

TL;DR: 本文研究了在中世纪音乐手稿的对象检测和布局识别中应用主动学习（AL）和序列学习（SL）。


<details>
  <summary>Details</summary>
Motivation: 解决光学音乐识别（OMR）中标注数据匮乏和历史手稿复杂性的问题。

Method: 利用YOLOv8，通过选取预测置信度最低的样本进行迭代标注和重新训练，尝试从单一初始标注图像优化性能。

Result: 实验表明，即使标注样本减少，性能可与全监督训练相媲美。然而，在特定手稿中基于不确定性的主动学习并不有效。

Conclusion: 更实用的方式需用于数据匮乏场景，现有方法在该手稿中的局限性提示进一步改进的方向。

Abstract: Optical Music Recognition (OMR) is a cornerstone of music digitization
initiatives in cultural heritage, yet it remains limited by the scarcity of
annotated data and the complexity of historical manuscripts. In this paper, we
present a preliminary study of Active Learning (AL) and Sequential Learning
(SL) tailored for object detection and layout recognition in an old medieval
music manuscript. Leveraging YOLOv8, our system selects samples with the
highest uncertainty (lowest prediction confidence) for iterative labeling and
retraining. Our approach starts with a single annotated image and successfully
boosts performance while minimizing manual labeling. Experimental results
indicate that comparable accuracy to fully supervised training can be achieved
with significantly fewer labeled examples. We test the methodology as a
preliminary investigation on a novel dataset offered to the community by the
Anonymous project, which studies laude, a poetical-musical genre spread across
Italy during the 12th-16th Century. We show that in the manuscript at-hand,
uncertainty-based AL is not effective and advocates for more usable methods in
data-scarcity scenarios.

</details>


### [121] [Uncovering Critical Features for Deepfake Detection through the Lottery Ticket Hypothesis](https://arxiv.org/abs/2507.15636)
*Lisan Al Amin,Md. Ismail Hossain,Thanh Thi Nguyen,Tasnim Jahan,Mahbubul Islam,Faisal Quader*

Main category: cs.CV

TL;DR: 本文探讨了应用彩票假设（LTH）来提高深度伪造检测的效率，并验证了通过剪枝可以保持模型的高准确性，同时显著减少参数规模。


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造技术的发展，它对信息完整性及社会信任构成了严重挑战。现有的检测方法虽有效，但其机制尚不明确且模型较大，不适合资源有限的环境。

Method: 研究采用了彩票假设（LTH）和迭代幅度剪枝方法，对MesoNet、CNN-5和ResNet-18等网络在深度伪造检测任务中的效果进行了广泛实验，并结合Grad-CAM可视化方法分析关键特征。

Result: 实验表明，在OpenForensic数据集上，MesoNet在保持80%稀疏性时仍能达到56.2%的准确性，仅使用3,000个参数，相当于原准确率的90%。此外，LTH的迭代剪枝效果优于单次剪枝，并且模型的获胜子网络可以在不同数据集间迁移。

Conclusion: 通过剪枝找到的获胜子网络可显著减少模型参数，并保持深度伪造检测精度，表明其在开发高效、可部署的检测系统方面具有潜力。

Abstract: Recent advances in deepfake technology have created increasingly convincing
synthetic media that poses significant challenges to information integrity and
social trust. While current detection methods show promise, their underlying
mechanisms remain poorly understood, and the large sizes of their models make
them challenging to deploy in resource-limited environments. This study
investigates the application of the Lottery Ticket Hypothesis (LTH) to deepfake
detection, aiming to identify the key features crucial for recognizing
deepfakes. We examine how neural networks can be efficiently pruned while
maintaining high detection accuracy. Through extensive experiments with
MesoNet, CNN-5, and ResNet-18 architectures on the OpenForensic and
FaceForensics++ datasets, we find that deepfake detection networks contain
winning tickets, i.e., subnetworks, that preserve performance even at
substantial sparsity levels. Our results indicate that MesoNet retains 56.2%
accuracy at 80% sparsity on the OpenForensic dataset, with only 3,000
parameters, which is about 90% of its baseline accuracy (62.6%). The results
also show that our proposed LTH-based iterative magnitude pruning approach
consistently outperforms one-shot pruning methods. Using Grad-CAM
visualization, we analyze how pruned networks maintain their focus on critical
facial regions for deepfake detection. Additionally, we demonstrate the
transferability of winning tickets across datasets, suggesting potential for
efficient, deployable deepfake detection systems.

</details>


### [122] [Extracting Visual Facts from Intermediate Layers for Mitigating Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2507.15652)
*Haoran Zhou,Zihan Zhang,Hao Chen*

Main category: cs.CV

TL;DR: 本文提出了一种方法EVA，通过动态选择中间层提取视觉事实信息，从而减少多模态大语言模型(MLLMs)中的物体幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 当前的MLLMs在结合视觉和语言模型时，容易出现物体幻觉（生成似是而非但不符合事实的结果），其主要原因是深层中视觉信息受到抑制。作者意图解决这一问题，探索中间层视觉信息的受抑制情况并提出对应的改进方案。

Method: 提出了EVA方法，通过动态选择具有最显著视觉事实信息的中间层，并对比原始输入和纯文本输入的输出分布，提取视觉事实信息并将其融入最终输出层以校正输出。该方法无需额外训练，对各种模型和解码策略均通用。

Result: 实验表明，EVA方法在标准基准上显著降低了基线方法的幻觉率，验证了其有效性。

Conclusion: EVA方法通过改进对中间层视觉信息的利用，有效减轻了MLLMs的幻觉问题，可应用于不同模型和解码策略，具有宽泛的适用性和实用价值。

Abstract: Multimodal Large Language Models (MLLMs) have made significant strides by
combining visual recognition and language understanding to generate content
that is both coherent and contextually accurate. However, MLLMs continue to
struggle with object hallucinations, where models produce seemingly plausible
but factually incorrect outputs, including objects that do not exist in the
image. Recent work has revealed that the prior knowledge in MLLMs significantly
suppresses visual information in deep layers, causing hallucinatory outputs.
However, how these priors suppress visual information at the intermediate layer
stage in MLLMs remains unclear. We observe that visual factual knowledge and
the differences between intermediate-layer prior/original probability
distributions show similar evolutionary trends in intermediate layers.
Motivated by this, we introduce Decoding by Extracting Visual Facts (EVA), a
simple, training-free method that dynamically selects intermediate layers with
the most significant visual factual information. By contrasting the output
distributions of the selected layer derived from the original input and
pure-text input, EVA extracts visual factual knowledge and proportionally
incorporates it into the final layer to correct the output logits. Importantly,
EVA is model-agnostic, seamlessly integrates with various classic decoding
strategies, and is applicable across different MLLMs. We validate EVA on
widely-used benchmarks, and the results show that it significantly reduces
hallucination rates compared to baseline methods, underscoring its
effectiveness in mitigating hallucinations.

</details>


### [123] [HW-MLVQA: Elucidating Multilingual Handwritten Document Understanding with a Comprehensive VQA Benchmark](https://arxiv.org/abs/2507.15655)
*Aniket Pal,Ajoy Mondal,Minesh Mathew,C. V. Jawahar*

Main category: cs.CV

TL;DR: 本文介绍了HW-MLVQA，一个多语言手写文档视觉问答基准，包含1600页手写文档和2400个问答对，并支持三种模态评估。


<details>
  <summary>Details</summary>
Motivation: 现有多语言视觉问答模型难以应对多样化的手写文档挑战，需开发专门基准。

Method: 设计HW-MLVQA基准，收集1600手写页和2400问答对，并提供文本、图像及图文结合的三模态评价框架，评估OCR模型。

Result: 提供了一个全面的多语言手写文档问答基准，能够模拟真实世界语境，支持对专有和开源OCR模型的评估。

Conclusion: HW-MLVQA为多语言手写文档理解提供了基础，促进该领域的科研和创新发展。

Abstract: The proliferation of MultiLingual Visual Question Answering (MLVQA)
benchmarks augments the capabilities of large language models (LLMs) and
multi-modal LLMs, thereby enabling them to adeptly capture the intricate
linguistic subtleties and visual complexities inherent across diverse
languages. Despite its potential, the current MLVQA model struggles to fully
utilize its capabilities when dealing with the extensive variety of handwritten
documents. This article delineates HW-MLVQA, an avant-garde VQA benchmark
meticulously crafted to mitigate the dearth of authentic Multilingual
Handwritten document comprehension. HW-MLVQA encompasses an extensive
collection of 1,600 handwritten Pages complemented by 2,400 question-answers.
Furthermore, it provides a robust benchmark evaluation framework spanning three
distinct modalities: text, image, and an integrated image & text modality. To
simulate authentic real-world contexts devoid of ground truth textual
transcriptions, we facilitates a rigorous assessment of proprietary and
open-source OCR models. The benchmark aspires to facilitate pivotal
advancements in multilingual handwritten document interpretation, fostering
innovation and scholarly inquiry within this specialized domain.

</details>


### [124] [Visual-Language Model Knowledge Distillation Method for Image Quality Assessment](https://arxiv.org/abs/2507.15680)
*Yongkang Hou,Jiarun Song*

Main category: cs.CV

TL;DR: 提出一种视觉语言模型知识蒸馏方法，通过CLIP指导模型训练，有效减少参数负担并提升图像质量评估性能。


<details>
  <summary>Details</summary>
Motivation: 解决CLIP在图像质量评估任务中参数负担过重和局部失真特征识别能力不足的问题。

Method: 设计质量分级提示模板，微调CLIP以提升其图像质量评估能力，并提出一种模态自适应知识蒸馏策略，实现从CLIP教师模型到学生模型的指导。

Result: 实验表明，该方法在多个IQA数据集上显著减少了模型复杂性，同时性能优于现有图像质量评估方法。

Conclusion: 该方法在降低模型复杂性的同时，提升了图像质量评估性能，展示出较大的实际部署潜力。

Abstract: Image Quality Assessment (IQA) is a core task in computer vision. Multimodal
methods based on vision-language models, such as CLIP, have demonstrated
exceptional generalization capabilities in IQA tasks. To address the issues of
excessive parameter burden and insufficient ability to identify local distorted
features in CLIP for IQA, this study proposes a visual-language model knowledge
distillation method aimed at guiding the training of models with architectural
advantages using CLIP's IQA knowledge. First, quality-graded prompt templates
were designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned
to enhance its capabilities in IQA tasks. Finally, a modality-adaptive
knowledge distillation strategy is proposed to achieve guidance from the CLIP
teacher model to the student model. Our experiments were conducted on multiple
IQA datasets, and the results show that the proposed method significantly
reduces model complexity while outperforming existing IQA methods,
demonstrating strong potential for practical deployment.

</details>


### [125] [Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization for Remote Sensing](https://arxiv.org/abs/2507.15683)
*Boni Hu,Zhenyu Xia,Lin Chen,Pengcheng Han,Shuhui Bu*

Main category: cs.CV

TL;DR: 论文提出了一种名为Hi2-GSLoc的框架，利用3D Gaussian Splatting进行高效精确的6-DoF视觉重定位，特别适用于遥感和无人机场景。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在6-DoF视觉重定位中精度、复杂度和可扩展性之间的权衡问题，如高海拔变化和现有视觉先验的域差异等。

Method: 提出了一种双层次的框架Hi2-GSLoc，分为稀疏和密集两个阶段；采用3D Gaussian Splatting场景表示，并使用分区训练、并行匹配和内存管理策略提升效率。

Result: 通过仿真数据、公共数据集和真实飞行实验验证，方法具备竞争力的定位精度、召回率和计算效率，且能有效过滤不可靠的姿态估计。

Conclusion: Hi2-GSLoc是一个高效可靠的视觉重定位方法，特别在大规模遥感应用中展现了实际价值。

Abstract: Visual relocalization, which estimates the 6-degree-of-freedom (6-DoF) camera
pose from query images, is fundamental to remote sensing and UAV applications.
Existing methods face inherent trade-offs: image-based retrieval and pose
regression approaches lack precision, while structure-based methods that
register queries to Structure-from-Motion (SfM) models suffer from
computational complexity and limited scalability. These challenges are
particularly pronounced in remote sensing scenarios due to large-scale scenes,
high altitude variations, and domain gaps of existing visual priors. To
overcome these limitations, we leverage 3D Gaussian Splatting (3DGS) as a novel
scene representation that compactly encodes both 3D geometry and appearance. We
introduce $\mathrm{Hi}^2$-GSLoc, a dual-hierarchical relocalization framework
that follows a sparse-to-dense and coarse-to-fine paradigm, fully exploiting
the rich semantic information and geometric constraints inherent in Gaussian
primitives. To handle large-scale remote sensing scenarios, we incorporate
partitioned Gaussian training, GPU-accelerated parallel matching, and dynamic
memory management strategies. Our approach consists of two stages: (1) a sparse
stage featuring a Gaussian-specific consistent render-aware sampling strategy
and landmark-guided detector for robust and accurate initial pose estimation,
and (2) a dense stage that iteratively refines poses through coarse-to-fine
dense rasterization matching while incorporating reliability verification.
Through comprehensive evaluation on simulation data, public datasets, and real
flight experiments, we demonstrate that our method delivers competitive
localization accuracy, recall rate, and computational efficiency while
effectively filtering unreliable pose estimates. The results confirm the
effectiveness of our approach for practical remote sensing applications.

</details>


### [126] [LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression](https://arxiv.org/abs/2507.15686)
*Wenjie Huang,Qi Yang,Shuting Xia,He Huang,Zhu Li,Yiling Xu*

Main category: cs.CV

TL;DR: 本文提出了一种基于隐式神经表示（INR）的无损点云几何压缩方法LINR-PCGC，通过多尺度稀疏卷积网络实现快速编码与紧凑解码器尺寸，比传统和AI方法表现更优，位流减少约21%。


<details>
  <summary>Details</summary>
Motivation: 针对现有AI点云压缩方法依赖特定数据分布且难以适应应用的实际问题，并突破INR方法受编码时长及解码器尺寸限制的瓶颈，首次提出INR的无损点云几何压缩方案。

Method: 设计了一种高效的点云分组编码框架与初始网络设置策略，结合基于多尺度SparseConv的轻量编码网络，包含比例上下文提取、子节点预测及模型压缩模块，显著优化编码速度与解码器规模。

Result: 实验表明，LINR-PCGC在MVUB数据集上比传统G-PCC TMC13v23方法减少大约21.21%的位流，相比SparsePCGC减少21.95%。

Conclusion: LINR-PCGC实现了高效、紧凑的无损点云几何压缩，标志着INR首次有效应用于无损点云几何领域，性能显著领先当前最佳方法。

Abstract: Existing AI-based point cloud compression methods struggle with dependence on
specific training data distributions, which limits their real-world deployment.
Implicit Neural Representation (INR) methods solve the above problem by
encoding overfitted network parameters to the bitstream, resulting in more
distribution-agnostic results. However, due to the limitation of encoding time
and decoder size, current INR based methods only consider lossy geometry
compression. In this paper, we propose the first INR based lossless point cloud
geometry compression method called Lossless Implicit Neural Representations for
Point Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we
design a group of point clouds level coding framework with an effective network
initialization strategy, which can reduce around 60% encoding time. A
lightweight coding network based on multiscale SparseConv, consisting of scale
context extraction, child node prediction, and model compression modules, is
proposed to realize fast inference and compact decoder size. Experimental
results show that our method consistently outperforms traditional and AI-based
methods: for example, with the convergence time in the MVUB dataset, our method
reduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and
21.95% compared to SparsePCGC. Our project can be seen on
https://huangwenjie2023.github.io/LINR-PCGC/.

</details>


### [127] [DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting](https://arxiv.org/abs/2507.15690)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: 本文提出DWTGS，通过结合小波空间的正则化方法，提高稀疏视图3D高斯投影的重建质量，克服了傅里叶变换相关方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏视图3D高斯投影在重建高质量新视图时存在过拟合高频细节的问题，同时克服傅里叶变换正则化引发的参数调节困难和高频学习偏差。

Method: 通过引入小波空间损失，主要对低频的LL子带进行多层次监督，并对高频的HH子带进行自监督稀疏化，从而优化重建效果。

Result: 实验表明，DWTGS在多个基准上表现优于基于傅里叶的对照方法，显著改进了模型的泛化能力并减少了高频幻觉现象。

Conclusion: DWTGS通过侧重低频成分的监督和高频空间的自监督稀疏化，有效提升了稀疏视图3D高斯投影的性能，为频率正则化方法提供了一种新思路。

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in
reconstructing high-quality novel views, as it often overfits to the
widely-varying high-frequency (HF) details of the sparse training views. While
frequency regularization can be a promising approach, its typical reliance on
Fourier transforms causes difficult parameter tuning and biases towards
detrimental HF learning. We propose DWTGS, a framework that rethinks frequency
regularization by leveraging wavelet-space losses that provide additional
spatial supervision. Specifically, we supervise only the low-frequency (LF) LL
subbands at multiple DWT levels, while enforcing sparsity on the HF HH subband
in a self-supervised manner. Experiments across benchmarks show that DWTGS
consistently outperforms Fourier-based counterparts, as this LF-centric
strategy improves generalization and reduces HF hallucinations.

</details>


### [128] [Efficient Face Image Quality Assessment via Self-training and Knowledge Distillation](https://arxiv.org/abs/2507.15709)
*Wei Sun,Weixia Zhang,Linhan Cao,Jun Jia,Xiangyang Zhu,Dandan Zhu,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本论文提议一种高效且易于部署的面部图像质量评估方法，通过教师-学生模型架构，显著降低计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现有面部图像质量评估（FIQA）算法计算复杂度高的问题，以确保其在真实世界系统中的可扩展性和实际应用。

Method: 提出了两阶段方法，包括训练强大的教师模型并从中蒸馏轻量化的学生模型。通过自训练策略增强教师模型，然后利用增强的教师模型生成伪标签，并结合标注数据用以进一步训练学生模型。

Result: 实验结果表明，学生模型在极低计算开销下实现了与教师模型可比的性能。此外，该方法在ICCV 2025 VQualA FIQA竞赛中获得第一名。

Conclusion: 所提方法通过有效的知识蒸馏和自训练策略，实现了高效、可部署的FIQA算法，兼顾了性能与计算效率。

Abstract: Face image quality assessment (FIQA) is essential for various face-related
applications. Although FIQA has been extensively studied and achieved
significant progress, the computational complexity of FIQA algorithms remains a
key concern for ensuring scalability and practical deployment in real-world
systems. In this paper, we aim to develop a computationally efficient FIQA
method that can be easily deployed in real-world applications. Specifically,
our method consists of two stages: training a powerful teacher model and
distilling a lightweight student model from it. To build a strong teacher
model, we adopt a self-training strategy to improve its capacity. We first
train the teacher model using labeled face images, then use it to generate
pseudo-labels for a set of unlabeled images. These pseudo-labeled samples are
used in two ways: (1) to distill knowledge into the student model, and (2) to
combine with the original labeled images to further enhance the teacher model
through self-training. The enhanced teacher model is used to further
pseudo-label another set of unlabeled images for distilling the student models.
The student model is trained using a combination of labeled images,
pseudo-labeled images from the original teacher model, and pseudo-labeled
images from the enhanced teacher model. Experimental results demonstrate that
our student model achieves comparable performance to the teacher model with an
extremely low computational overhead. Moreover, our method achieved first place
in the ICCV 2025 VQualA FIQA Challenge. The code is available at
https://github.com/sunwei925/Efficient-FIQA.git.

</details>


### [129] [A Practical Investigation of Spatially-Controlled Image Generation with Transformers](https://arxiv.org/abs/2507.15724)
*Guoxuan Xia,Harleen Hanspal,Petru-Daniel Tudosiu,Shifeng Zhang,Sarah Parisot*

Main category: cs.CV

TL;DR: 本论文分析及探讨了空间控制图像生成的最新进展，主要关注基于transformer的方法，并对现有方法进行公平和系统的比较。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成研究中，用户需要利用边缘图、姿势等实现对生成图像的精细控制。但随着模型性能的快速提升，公平科学的基准研究和现有方法背后的动机逐渐被忽视。本文旨在梳理并聚焦这方面的知识空白。

Method: 本文在ImageNet上，对基于扩散流模型和自回归（AR）模型进行了全面对比实验，提出了一种基于transformer的控制token预填充简单高效的方法，并探讨了时间采样的改进策略(如控制的无分类器指导扩展和softmax截断)。同时重新分析了适配器方法在有限下游数据条件下防止遗忘和维持生成质量的机制。

Result: 通过实验验证了控制token预填充方法的有效性，采样时间的改进策略显著提高了控制生成一致性，而适配器方法在生成质量上优于全量训练，但在控制生成一致性上表现稍逊色。

Conclusion: 本文理清了空间控制图像生成的关键问题，为研究人员开发基于transformer的控制生成系统提供了清晰的指引。附代码将于发表时公开。

Abstract: Enabling image generation models to be spatially controlled is an important
area of research, empowering users to better generate images according to their
own fine-grained specifications via e.g. edge maps, poses. Although this task
has seen impressive improvements in recent times, a focus on rapidly producing
stronger models has come at the cost of detailed and fair scientific
comparison. Differing training data, model architectures and generation
paradigms make it difficult to disentangle the factors contributing to
performance. Meanwhile, the motivations and nuances of certain approaches
become lost in the literature. In this work, we aim to provide clear takeaways
across generation paradigms for practitioners wishing to develop
transformer-based systems for spatially-controlled generation, clarifying the
literature and addressing knowledge gaps. We perform controlled experiments on
ImageNet across diffusion-based/flow-based and autoregressive (AR) models.
First, we establish control token prefilling as a simple, general and
performant baseline approach for transformers. We then investigate previously
underexplored sampling time enhancements, showing that extending
classifier-free guidance to control, as well as softmax truncation, have a
strong impact on control-generation consistency. Finally, we re-clarify the
motivation of adapter-based approaches, demonstrating that they mitigate
"forgetting" and maintain generation quality when trained on limited downstream
data, but underperform full training in terms of generation-control
consistency. Code will be released upon publication.

</details>


### [130] [TokensGen: Harnessing Condensed Tokens for Long Video Generation](https://arxiv.org/abs/2507.15728)
*Wenqi Ouyang,Zeqi Xiao,Danni Yang,Yifan Zhou,Shuai Yang,Lei Yang,Jianlou Si,Xingang Pan*

Main category: cs.CV

TL;DR: 该论文提出了一种名为TokensGen的新方法，通过使用压缩的tokens和两阶段框架解决长视频生成中的一致性问题，大幅改善时间和内容上的连贯性。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩展短视频生成模型至长时间视频中出现的内存瓶颈和长期不一致的问题。

Method: 提出TokensGen，两阶段流程：第一阶段训练Token-to-Video短视频扩散模型，并引入视频tokenizer提取语义丰富tokens；第二阶段通过Text-to-Token扩散变换器生成所有视频tokens，同时通过FIFO扩散策略连接片段，提升过渡性及一致性。

Result: 实验结果表明，该方法在无需大量计算开销的前提下，显著提高了长视频的时间一致性和内容连贯性。

Conclusion: 通过引入tokens和预训练模型，TokensGen解决了长视频生成中的核心问题，为讲故事、影视制作和沉浸式模拟提供了新型高效的解决方案。

Abstract: Generating consistent long videos is a complex challenge: while
diffusion-based generative models generate visually impressive short clips,
extending them to longer durations often leads to memory bottlenecks and
long-term inconsistency. In this paper, we propose TokensGen, a novel two-stage
framework that leverages condensed tokens to address these issues. Our method
decomposes long video generation into three core tasks: (1) inner-clip semantic
control, (2) long-term consistency control, and (3) inter-clip smooth
transition. First, we train To2V (Token-to-Video), a short video diffusion
model guided by text and video tokens, with a Video Tokenizer that condenses
short clips into semantically rich tokens. Second, we introduce T2To
(Text-to-Token), a video token diffusion transformer that generates all tokens
at once, ensuring global consistency across clips. Finally, during inference,
an adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips,
reducing boundary artifacts and enhancing smooth transitions. Experimental
results demonstrate that our approach significantly enhances long-term temporal
and content coherence without incurring prohibitive computational overhead. By
leveraging condensed tokens and pre-trained short video models, our method
provides a scalable, modular solution for long video generation, opening new
possibilities for storytelling, cinematic production, and immersive
simulations. Please see our project page at
https://vicky0522.github.io/tokensgen-webpage/ .

</details>


### [131] [Appearance Harmonization via Bilateral Grid Prediction with Transformers for 3DGS](https://arxiv.org/abs/2507.15748)
*Jisu Shin,Richard Shaw,Seunghyun Shin,Anton Pelykh,Zhensong Zhang,Hae-Gon Jeon,Eduardo Perez-Pellitero*

Main category: cs.CV

TL;DR: 该论文提出一种基于Transformer的方法，通过预测空间自适应双向网格校正多视图照片测量中的不一致性，同时可以实现高效的跨场景泛化。


<details>
  <summary>Details</summary>
Motivation: 现有相机管道处理会导致不同视图之间的光度不一致性，从而影响新视图的合成质量。

Method: 采用Transformer预测并设计空间自适应双向网格，通过结合3D高斯散射管线，校正多视图光度变化，同时避免场景特定的重新训练。

Result: 算法提高了重建质量，并在收敛速度上与现有基于场景优化的方法相匹配或更优。

Conclusion: 提出的方法在保持高训练效率的同时，实现了跨场景鲁棒性，提高了多视图一致性，表明该方法在新视图生成中具备优秀性能。

Abstract: Modern camera pipelines apply extensive on-device processing, such as
exposure adjustment, white balance, and color correction, which, while
beneficial individually, often introduce photometric inconsistencies across
views. These appearance variations violate multi-view consistency and degrade
the quality of novel view synthesis. Joint optimization of scene
representations and per-image appearance embeddings has been proposed to
address this issue, but at the cost of increased computational complexity and
slower training. In this work, we propose a transformer-based method that
predicts spatially adaptive bilateral grids to correct photometric variations
in a multi-view consistent manner, enabling robust cross-scene generalization
without the need for scene-specific retraining. By incorporating the learned
grids into the 3D Gaussian Splatting pipeline, we improve reconstruction
quality while maintaining high training efficiency. Extensive experiments show
that our approach outperforms or matches existing scene-specific optimization
methods in reconstruction fidelity and convergence speed.

</details>


### [132] [Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization](https://arxiv.org/abs/2507.15765)
*Feng-Qi Cui,Anyang Tong,Jinyang Huang,Jie Zhang,Dan Guo,Zhi Liu,Meng Wang*

Main category: cs.CV

TL;DR: 提出了一种新的异质性感知分布框架（HDF），通过改进时间频率建模和优化不平衡解决动态面部表情识别中的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法因多源数据和个体表情差异性导致样本异质性问题而表现下降，本文旨在通过新的框架提高性能。

Method: 设计了两个插入式模块：时间频率分布注意模块（DAM）提高时间一致性和频率鲁棒性，分布感知缩放模块（DSM）依据梯度敏感性和信息瓶颈平衡分类及对比损失。

Result: 实验表明在DFEW和FERV39k数据集上，该方法显著提升了识别准确率和鲁棒性，对多样且不平衡场景具有较强的泛化能力。

Conclusion: HDF框架通过改进样本异质性问题，展示了其优秀的表情识别性能，同时代码已公开。

Abstract: Dynamic Facial Expression Recognition (DFER) plays a critical role in
affective computing and human-computer interaction. Although existing methods
achieve comparable performance, they inevitably suffer from performance
degradation under sample heterogeneity caused by multi-source data and
individual expression variability. To address these challenges, we propose a
novel framework, called Heterogeneity-aware Distributional Framework (HDF), and
design two plug-and-play modules to enhance time-frequency modeling and
mitigate optimization imbalance caused by hard samples. Specifically, the
Time-Frequency Distributional Attention Module (DAM) captures both temporal
consistency and frequency robustness through a dual-branch attention design,
improving tolerance to sequence inconsistency and visual style shifts. Then,
based on gradient sensitivity and information bottleneck principles, an
adaptive optimization module Distribution-aware Scaling Module (DSM) is
introduced to dynamically balance classification and contrastive losses,
enabling more stable and discriminative representation learning. Extensive
experiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF
significantly improves both recognition accuracy and robustness. Our method
achieves superior weighted average recall (WAR) and unweighted average recall
(UAR) while maintaining strong generalization across diverse and imbalanced
scenarios. Codes are released at https://github.com/QIcita/HDF_DFER.

</details>


### [133] [Label tree semantic losses for rich multi-class medical image segmentation](https://arxiv.org/abs/2507.15777)
*Junwen Wang,Oscar MacCormac,William Rochford,Aaron Kujawa,Jonathan Shapey,Tom Vercauteren*

Main category: cs.CV

TL;DR: 本文探讨了一种基于树的语义损失函数，用于改进医学图像分割任务，并在两个领域取得了最新的性能成果。


<details>
  <summary>Details</summary>
Motivation: 现有的医学图像分割方法未能利用标签的语义结构，尤其在处理具有高复杂性和丰富标签的任务时，这限制了其在临床实践中的应用潜力。

Method: 提出两种基于树的语义损失函数，结合标签的层次结构信息。同时，将该方法整合至一种无背景、稀疏标注训练框架中，扩展其可用性。

Result: 在头部磁共振成像（MRI）的全脑分区和神经外科高光谱成像的场景理解两个任务中，取得了与最新技术水平相匹配的实验结果。

Conclusion: 基于树的语义损失函数有效提升了医学图像分割的性能，为AI驱动的临床实践提供了新的可能性。

Abstract: Rich and accurate medical image segmentation is poised to underpin the next
generation of AI-defined clinical practice by delineating critical anatomy for
pre-operative planning, guiding real-time intra-operative navigation, and
supporting precise post-operative assessment. However, commonly used learning
methods for medical and surgical imaging segmentation tasks penalise all errors
equivalently and thus fail to exploit any inter-class semantics in the labels
space. This becomes particularly problematic as the cardinality and richness of
labels increases to include subtly different classes. In this work, we propose
two tree-based semantic loss functions which take advantage of a hierarchical
organisation of the labels. We further incorporate our losses in a recently
proposed approach for training with sparse, background-free annotations to
extend the applicability of our proposed losses. Extensive experiments are
reported on two medical and surgical image segmentation tasks, namely head MRI
for whole brain parcellation (WBP) with full supervision and neurosurgical
hyperspectral imaging (HSI) for scene understanding with sparse annotations.
Results demonstrate that our proposed method reaches state-of-the-art
performance in both cases.

</details>


### [134] [Regularized Low-Rank Adaptation for Few-Shot Organ Segmentation](https://arxiv.org/abs/2507.15793)
*Ghassen Baklouti,Julio Silva-Rodríguez,Jose Dolz,Houda Bahig,Ismail Ben Ayed*

Main category: cs.CV

TL;DR: 文章提出一种改进的低秩适配方法，用于参数高效微调医疗图像分割任务，显著提高了性能并克服了传统方法中秩设定的局限性。


<details>
  <summary>Details</summary>
Motivation: 动机在于传统的低秩适配方法（如LoRA）虽然有效，但固定的秩设置缺乏灵活性，尤其是不能适应不同医疗任务的复杂性。

Method: 方法引入一种动态调整内在秩的策略。通过将可训练权重矩阵的低秩表示视为奇异值分解，并在损失函数中添加l_1稀疏正则项，再通过邻近优化器进行优化，以此自动发现与任务适配的秩。

Result: 实验在少样本微调场景下进行，任务包括基础器官和新型器官。对比LoRA和其他方法，新方法表现出显著的性能提升，且在初始秩非最优情况下表现出鲁棒性。

Conclusion: 新提出的方法具有高效性和鲁棒性，通过动态秩调整策略改进了PEFT方法在医疗图像中的表现，代码和成果已公开。

Abstract: Parameter-efficient fine-tuning (PEFT) of pre-trained foundation models is
increasingly attracting interest in medical imaging due to its effectiveness
and computational efficiency. Among these methods, Low-Rank Adaptation (LoRA)
is a notable approach based on the assumption that the adaptation inherently
occurs in a low-dimensional subspace. While it has shown good performance, its
implementation requires a fixed and unalterable rank, which might be
challenging to select given the unique complexities and requirements of each
medical imaging downstream task. Inspired by advancements in natural image
processing, we introduce a novel approach for medical image segmentation that
dynamically adjusts the intrinsic rank during adaptation. Viewing the low-rank
representation of the trainable weight matrices as a singular value
decomposition, we introduce an l_1 sparsity regularizer to the loss function,
and tackle it with a proximal optimizer. The regularizer could be viewed as a
penalty on the decomposition rank. Hence, its minimization enables to find
task-adapted ranks automatically. Our method is evaluated in a realistic
few-shot fine-tuning setting, where we compare it first to the standard LoRA
and then to several other PEFT methods across two distinguishable tasks: base
organs and novel organs. Our extensive experiments demonstrate the significant
performance improvements driven by our method, highlighting its efficiency and
robustness against suboptimal rank initialization. Our code is publicly
available: https://github.com/ghassenbaklouti/ARENA

</details>


### [135] [Exploring Superposition and Interference in State-of-the-Art Low-Parameter Vision Models](https://arxiv.org/abs/2507.15798)
*Lilian Hollard,Lucas Mohimont,Nathalie Gaveau,Luiz-Angelo Steffenel*

Main category: cs.CV

TL;DR: 研究低参数深度神经网络在计算机视觉中的表现，着重于瓶颈架构和超线性激活函数的使用。提出了一种证明概念的架构名为NoDepth Bottleneck，并验证其在ImageNet数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 探讨如何在超低参数量（少于1.5百万参数）的深度网络中实现高效率和高精度，解决由特征图干扰引起的问题。

Method: 通过研究不同瓶颈架构，分析如何减少特征图干扰，并设计出一种新的低参数神经网络架构NoDepth Bottleneck。

Result: 发现减少特征图干扰的设计元素，提出的NoDepth Bottleneck在ImageNet上的准确性表现优异，具备良好的扩展能力。

Conclusion: 研究为低参数范围内的更高效且可扩展的神经网络设计提供了新见解，深化了对计算机视觉中瓶颈问题的理解。

Abstract: The paper investigates the performance of state-of-the-art low-parameter deep
neural networks for computer vision, focusing on bottleneck architectures and
their behavior using superlinear activation functions. We address interference
in feature maps, a phenomenon associated with superposition, where neurons
simultaneously encode multiple characteristics. Our research suggests that
limiting interference can enhance scaling and accuracy in very low-scaled
networks (under 1.5M parameters). We identify key design elements that reduce
interference by examining various bottleneck architectures, leading to a more
efficient neural network. Consequently, we propose a proof-of-concept
architecture named NoDepth Bottleneck built on mechanistic insights from our
experiments, demonstrating robust scaling accuracy on the ImageNet dataset.
These findings contribute to more efficient and scalable neural networks for
the low-parameter range and advance the understanding of bottlenecks in
computer vision. https://caiac.pubpub.org/pub/3dh6rsel

</details>


### [136] [ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction](https://arxiv.org/abs/2507.15803)
*Danhui Chen,Ziquan Liu,Chuxi Yang,Dan Wang,Yan Yan,Yi Xu,Xiangyang Ji*

Main category: cs.CV

TL;DR: 本文介绍了一个名为ConformalSAM的新方法，通过校准基础分割模型并过滤不可靠的像素标签，提高了半监督语义分割任务的性能。


<details>
  <summary>Details</summary>
Motivation: 半监督语义分割（SSSS）通过结合标注和未标注的数据，降低了标签需求，本文旨在探讨是否可以利用基础分割模型（如SEEM）改善标注稀缺问题。

Method: 提出了一个名为ConformalSAM的框架，利用目标领域的标注数据对基础分割模型进行校准，并结合符合预测（CP）过滤掉低置信度的标签，随后使用自适应的训练策略避免过拟合。

Result: 在三个标准的半监督语义分割基准测试中，ConformalSAM优于近期方法，同时还能作为插件提升其他方法的性能。

Conclusion: ConformalSAM成功结合了基础分割模型和置信度校准技术，有效解决了标注稀缺问题，并达到了当前最优的性能。

Abstract: Pixel-level vision tasks, such as semantic segmentation, require extensive
and high-quality annotated data, which is costly to obtain. Semi-supervised
semantic segmentation (SSSS) has emerged as a solution to alleviate the
labeling burden by leveraging both labeled and unlabeled data through
self-training techniques. Meanwhile, the advent of foundational segmentation
models pre-trained on massive data, has shown the potential to generalize
across domains effectively. This work explores whether a foundational
segmentation model can address label scarcity in the pixel-level vision task as
an annotator for unlabeled images. Specifically, we investigate the efficacy of
using SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual
input, to generate predictive masks for unlabeled data. To address the
shortcomings of using SEEM-generated masks as supervision, we propose
ConformalSAM, a novel SSSS framework which first calibrates the foundation
model using the target domain's labeled data and then filters out unreliable
pixel labels of unlabeled data so that only high-confidence labels are used as
supervision. By leveraging conformal prediction (CP) to adapt foundation models
to target data through uncertainty calibration, ConformalSAM exploits the
strong capability of the foundational segmentation model reliably which
benefits the early-stage learning, while a subsequent self-reliance training
strategy mitigates overfitting to SEEM-generated masks in the later training
stage. Our experiment demonstrates that, on three standard benchmarks of SSSS,
ConformalSAM achieves superior performance compared to recent SSSS methods and
helps boost the performance of those methods as a plug-in.

</details>


### [137] [True Multimodal In-Context Learning Needs Attention to the Visual Context](https://arxiv.org/abs/2507.15807)
*Shuo Chen,Jianzhe Liu,Zhen Han,Yan Xia,Daniel Cremers,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CV

TL;DR: 当前的多模态大语言模型（MLLMs）在视觉信息利用上存在不足，提出了DARA微调策略和TrueMICL数据集以改善多模态上下文学习（MICL）能力。


<details>
  <summary>Details</summary>
Motivation: 许多现有的MLLMs无法有效利用视觉信息，而是更多依赖于文本模式，这种局限性降低了MICL的实际应用价值。

Method: 引入了一种动态注意力再分配（DARA）的高效微调策略，并提出了TrueMICL数据集，该数据集要求多模态信息（特别是视觉内容）的整合来完成任务。

Result: 通过大量实验验证，提出的方法显著提高了模型在真正多模态上下文学习中的能力。

Conclusion: DARA策略和TrueMICL数据集为提升多模态上下文学习性能提供了有效方案，并改善了现有模型的视觉信息利用能力。

Abstract: Multimodal Large Language Models (MLLMs), built on powerful language
backbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new
tasks from a few multimodal demonstrations consisting of images, questions, and
answers. Despite showing noticeable improvement on standard vision-language
datasets, current MLLMs struggle to leverage visual information in the
demonstrations. Specifically, they tend to neglect visual cues and over-rely on
textual patterns, leading to mere text imitation rather than genuine multimodal
adaptation. This behavior makes MICL still unimodal and largely restricts its
practical utility. More importantly, this limitation is often concealed by the
improved performance on tasks that do not require understanding the visual
context. As a result, how to effectively enhance MICL ability and reliably
evaluate the MICL performance remains underexplored. To address these issues,
we first introduce Dynamic Attention Reallocation (DARA), an efficient
fine-tuning strategy that encourages models to attend to the visual context by
rebalancing attention across visual and textual tokens. In addition, we present
TrueMICL, an MICL-dedicated dataset with both support and test sets that
explicitly requires the integration of multimodal information-particularly
visual content-for correct task completion. Extensive experiments demonstrate
the effectiveness of our holistic solution, showcasing substantial improvements
in the true multimodal in-context learning capabilities. Code and datasets are
available at https://chenxshuo.github.io/true-micl-colm .

</details>


### [138] [Diffusion models for multivariate subsurface generation and efficient probabilistic inversion](https://arxiv.org/abs/2507.15809)
*Roberto Miele,Niklas Linde*

Main category: cs.CV

TL;DR: 本文探讨了扩散模型在多变量地下建模和概率反演中的应用，提出了改进的扩散后验采样方法，并在地质情境下验证了其高效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探讨扩散模型在生成任务中的潜力，并优化其在多变量地下建模和概率反演任务中的应用表现。

Method: 提出改进的扩散后验采样方法，通过引入噪声污染的可能性近似，并在多变量地质情境下进行验证。

Result: 改进方法显著提升了统计鲁棒性、后验概率密度函数采样性能，并降低了计算成本。同时支持局部硬数据和非线性地球物理数据的条件建模。

Conclusion: 改进后方法较现有方法更高效，且在扩散过程中集成了反演操作，摆脱了传统方法需要的外部循环，提高了任务的执行效率。

Abstract: Diffusion models offer stable training and state-of-the-art performance for
deep generative modeling tasks. Here, we consider their use in the context of
multivariate subsurface modeling and probabilistic inversion. We first
demonstrate that diffusion models enhance multivariate modeling capabilities
compared to variational autoencoders and generative adversarial networks. In
diffusion modeling, the generative process involves a comparatively large
number of time steps with update rules that can be modified to account for
conditioning data. We propose different corrections to the popular Diffusion
Posterior Sampling approach by Chung et al. (2023). In particular, we introduce
a likelihood approximation accounting for the noise-contamination that is
inherent in diffusion modeling. We assess performance in a multivariate
geological scenario involving facies and correlated acoustic impedance.
Conditional modeling is demonstrated using both local hard data (well logs) and
nonlinear geophysics (fullstack seismic data). Our tests show significantly
improved statistical robustness, enhanced sampling of the posterior probability
density function and reduced computational costs, compared to the original
approach. The method can be used with both hard and indirect conditioning data,
individually or simultaneously. As the inversion is included within the
diffusion process, it is faster than other methods requiring an outer-loop
around the generative model, such as Markov chain Monte Carlo.

</details>


### [139] [Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models](https://arxiv.org/abs/2507.15824)
*Enes Sanli,Baris Sarper Tezcan,Aykut Erdem,Erkut Erdem*

Main category: cs.CV

TL;DR: 提出PhysVidBench基准，评估文本生成视频系统的物理推理能力，采用三阶段评估方法，解决现有模型在物理常识方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成视频系统在物理常识、因果关系、物体行为和工具使用等方面存在缺陷。

Method: 设计PhysVidBench基准，包含383个侧重工具使用、材料特性和程序交互的提示，通过三阶段管道评估：（1）从提示生成物理问题；（2）用视觉语言模型生成视频字幕；（3）用语言模型基于字幕回答物理问题。

Result: 实现了对现有T2V模型进行有针对性的物理推理能力评估，绕过了直接视频评估中的常见问题。

Conclusion: PhysVidBench提供了一个结构化和可解释的框架，用于评估生成视频模型中的物理常识能力，填补了现有T2V评估的空白。

Abstract: Recent progress in text-to-video (T2V) generation has enabled the synthesis
of visually compelling and temporally coherent videos from natural language.
However, these models often fall short in basic physical commonsense, producing
outputs that violate intuitive expectations around causality, object behavior,
and tool use. Addressing this gap, we present PhysVidBench, a benchmark
designed to evaluate the physical reasoning capabilities of T2V systems. The
benchmark includes 383 carefully curated prompts, emphasizing tool use,
material properties, and procedural interactions, and domains where physical
plausibility is crucial. For each prompt, we generate videos using diverse
state-of-the-art models and adopt a three-stage evaluation pipeline: (1)
formulate grounded physics questions from the prompt, (2) caption the generated
video with a vision-language model, and (3) task a language model to answer
several physics-involved questions using only the caption. This indirect
strategy circumvents common hallucination issues in direct video-based
evaluation. By highlighting affordances and tool-mediated actions, areas
overlooked in current T2V evaluations, PhysVidBench provides a structured,
interpretable framework for assessing physical commonsense in generative video
models.

</details>


### [140] [SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction](https://arxiv.org/abs/2507.15852)
*Zhixiong Zhang,Shuangrui Ding,Xiaoyi Dong,Songxin He,Jianfan Lin,Junsong Tang,Yuhang Zang,Yuhang Cao,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 提出了基于概念驱动的Segmentation Concept (SeC)框架，结合大规模视觉语言模型（LVLMs），实现了对复杂视频中目标对象的高水平语义分割，并在新的SeCVOS基准测试中显著超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有视频对象分割技术在处理视觉剧变、遮挡及复杂场景变化时表现不足，因为它们依赖外观匹配，忽略了类似人类的概念性理解能力。

Method: 提出SeC框架，利用LVLMs逐步构建面向对象的高级表征，实现基于概念的分割。此外，设计了SeCVOS基准测试评估VOS方法在复杂语义场景中的表现。

Result: SeC在SeCVOS基准测试中比SAM 2.1提高了11.8分，表明其在视频对象分割中的最先进水平。

Conclusion: SeC通过概念驱动的框架和新的基准测试推动了视频对象分割领域的发展，显著提升了应对复杂场景的能力。

Abstract: Video Object Segmentation (VOS) is a core task in computer vision, requiring
models to track and segment target objects across video frames. Despite notable
advances with recent efforts, current techniques still lag behind human
capabilities in handling drastic visual variations, occlusions, and complex
scene changes. This limitation arises from their reliance on appearance
matching, neglecting the human-like conceptual understanding of objects that
enables robust identification across temporal dynamics. Motivated by this gap,
we propose Segment Concept (SeC), a concept-driven segmentation framework that
shifts from conventional feature matching to the progressive construction and
utilization of high-level, object-centric representations. SeC employs Large
Vision-Language Models (LVLMs) to integrate visual cues across diverse frames,
constructing robust conceptual priors. During inference, SeC forms a
comprehensive semantic representation of the target based on processed frames,
realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively
balances LVLM-based semantic reasoning with enhanced feature matching,
dynamically adjusting computational efforts based on scene complexity. To
rigorously assess VOS methods in scenarios demanding high-level conceptual
reasoning and robust semantic understanding, we introduce the Semantic Complex
Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160
manually annotated multi-scenario videos designed to challenge models with
substantial appearance variations and dynamic scene transformations. In
particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,
establishing a new state-of-the-art in concept-aware video object segmentation.

</details>


### [141] [Latent Denoising Makes Good Visual Tokenizers](https://arxiv.org/abs/2507.15856)
*Jiawei Yang,Tianhong Li,Lijie Fan,Yonglong Tian,Yue Wang*

Main category: cs.CV

TL;DR: 本研究提出了一种新的视觉标记器Latent Denoising Tokenizer（l-DeTok），通过与下游去噪目标对齐，改进了生成模型的训练效果。研究显示，去噪是标记器设计的重要原则。


<details>
  <summary>Details</summary>
Motivation: 目前尚不清楚何种属性能够使视觉标记器对于生成建模更有效，作者通过观察生成模型的常见去噪训练目标，提出改进的方法。

Method: 引入了Latent Denoising Tokenizer（l-DeTok），通过处理插值噪声和随机掩饰等破坏的潜在嵌入，训练标记器以重建清晰图像。

Result: 在ImageNet 256x256数据集上，l-DeTok相比于标准标记器在六种生成模型中表现更优。

Conclusion: 去噪是标记器设计的重要原则，这一研究为未来标记器设计提供了新框架和启发。

Abstract: Despite their fundamental role, it remains unclear what properties could make
visual tokenizers more effective for generative modeling. We observe that
modern generative models share a conceptually similar training objective --
reconstructing clean signals from corrupted inputs such as Gaussian noise or
masking -- a process we term denoising. Motivated by this insight, we propose
aligning tokenizer embeddings directly with the downstream denoising objective,
encouraging latent embeddings to be more easily reconstructed even when heavily
corrupted. To achieve this, we introduce the Latent Denoising Tokenizer
(l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images
from latent embeddings corrupted by interpolative noise and random masking.
Extensive experiments on ImageNet 256x256 demonstrate that our tokenizer
consistently outperforms standard tokenizers across six representative
generative models. Our findings highlight denoising as a fundamental design
principle for tokenizer development, and we hope it could motivate new
perspectives for future tokenizer design.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [142] [DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base](https://arxiv.org/abs/2507.14189)
*Song Mao,Lejun Cheng,Pinlong Cai,Guohang Yan,Ding Wang,Botian Shi*

Main category: cs.CL

TL;DR: 本研究介绍了DeepWriter，一种基于离线知识库的定制型多模态长文写作助手，以提升在专业领域写作中的质量与可信度。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型在专业领域写作中缺乏特定知识并容易产生幻觉，需要更可靠的解决方案来提升内容质量与事实一致性。

Method: 提出了DeepWriter，结合任务分解、提纲生成、多模态检索及按章节反思性写作的管道，并采用分层知识表示模型，提高检索效率和精度。

Result: DeepWriter在金融报告生成的实验中，生成内容在事实准确性和质量方面优于现有基线。

Conclusion: DeepWriter能够利用结构化语料库和多模态信息生成连贯、真实且专业的内容，适用于专业领域写作助理。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various applications. However, their use as writing assistants in specialized
domains like finance, medicine, and law is often hampered by a lack of deep
domain-specific knowledge and a tendency to hallucinate. Existing solutions,
such as Retrieval-Augmented Generation (RAG), can suffer from inconsistency
across multiple retrieval steps, while online search-based methods often
degrade quality due to unreliable web content. To address these challenges, we
introduce DeepWriter, a customizable, multimodal, long-form writing assistant
that operates on a curated, offline knowledge base. DeepWriter leverages a
novel pipeline that involves task decomposition, outline generation, multimodal
retrieval, and section-by-section composition with reflection. By deeply mining
information from a structured corpus and incorporating both textual and visual
elements, DeepWriter generates coherent, factually grounded, and
professional-grade documents. We also propose a hierarchical knowledge
representation to enhance retrieval efficiency and accuracy. Our experiments on
financial report generation demonstrate that DeepWriter produces high-quality,
verifiable articles that surpasses existing baselines in factual accuracy and
generated content quality.

</details>


### [143] [Retention analysis of edited knowledge after fine-tuning](https://arxiv.org/abs/2507.14198)
*Fufang Wen,Shichang Zhang*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）的模型编辑方法以及它们在微调后的知识保留情况，发现之前编辑的知识容易遗忘，并提出冻结相关层可以提高知识保留。


<details>
  <summary>Details</summary>
Motivation: 需要对大型语言模型进行知识更新以修正事实错误、整合新信息或调整行为，同时研究微调如何影响先前编辑过的知识，以提高实用性。

Method: 系统性研究了不同微调目标和模型编辑技术的交互作用，并评估通过冻结编辑内容相关层提升知识保留的效果。

Result: 发现编辑后的知识在微调过程中更易遗忘，相比之下预训练中获得的内在知识更加稳固。

Conclusion: 现有编辑方法在面临下游微调时的关键限制是知识易遗忘，引入冻结层的方法可以有效改善知识保留，未来需进一步优化模型编辑方法的稳定性。

Abstract: Large language models (LLMs) store vast amounts of knowledge, which often
requires updates to correct factual errors, incorporate newly acquired
information, or adapt model behavior. Model editing methods have emerged as
efficient solutions for such updates, offering localized and precise knowledge
modification at significantly lower computational cost than continual training.
In parallel, LLMs are frequently fine-tuned for a wide range of downstream
tasks. However, the effect of fine-tuning on previously edited knowledge
remains poorly understood. In this work, we systematically investigate how
different fine-tuning objectives interact with various model editing
techniques. Our findings show that edited knowledge is substantially more
susceptible to forgetting during fine-tuning than intrinsic knowledge acquired
through pre-training. This analysis highlights a key limitation of current
editing approaches and suggests that evaluating edit robustness under
downstream fine-tuning is critical for their practical deployment. We further
find that freezing layers associated with edited content can significantly
improve knowledge retention, offering insight into how future editing methods
might be made more robust.

</details>


### [144] [Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System](https://arxiv.org/abs/2507.14200)
*Shengji Tang,Jianjian Cao,Weihao Lin,Jiale Hong,Bo Zhang,Shuyue Hu,Lei Bai,Tao Chen,Wanli Ouyang,Peng Ye*

Main category: cs.CL

TL;DR: 本文提出了SMACS框架，通过协作整合15个开源LLMs，在多个任务中超越多个封闭式LLMs的表现，并推进智能上限。


<details>
  <summary>Details</summary>
Motivation: 研究开源LLMs协作的潜力，探索其是否能够匹敌甚至超过封闭式LLMs。

Method: 提出了一个可扩展的多智能体协作系统，包含RPS（用于选择最优LLMs）和EPE（提升响应质量和多样性）两部分方法。

Result: 在八个基准测试中，SMACS利用15个开源LLMs集成，其表现分别比Claude-3.7-Sonnet高出12.73%，GPT-4.1高出5.36%，GPT-o3-mini高出5.28%，并超越了开源和封闭LLMs的最佳平均表现。

Conclusion: 研发的系统展示了开源协作的巨大潜力，可有效提升LLMs能力，并为开源研究提供强力支持。

Abstract: This paper aims to demonstrate the potential and strengths of open-source
collectives. It leads to a promising question: Can we harness multiple
open-source LLMs to match or even beat the closed-source LLMs? To answer this,
we propose SMACS, a scalable multi-agent collaboration system (MACS) framework
with high performance. Specifically, for continuous integration of new LLMs and
generalization to diverse questions, we first propose a Retrieval-based Prior
Selection (RPS), which assigns a proxy performance score to each LLM to select
the Top-k LLMs at the instance level for any given question. Then, we propose
an Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the
generation of diverse responses through prior dropping and selecting the
high-quality response via a hybrid posterior score. Experiments on eight
mainstream benchmarks validate the effectiveness of our SMACS: by integrating
fifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025,
e.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%)
across multiple tasks. Remarkably, it even exceeds the average of best results
of different datasets from both open-source LLMs (+2.86%) and closed-source
LLMs (+2.04%), pushing the upper bound of intelligence. Code will be released
at https://github.com/magent4aci/SMACS.

</details>


### [145] [Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale](https://arxiv.org/abs/2507.14214)
*Rui Zhao,Vladyslav Melnychuk,Jun Zhao,Jesse Wright,Nigel Shadbolt*

Main category: cs.CL

TL;DR: 研究提出了一个名为PoliAnalyzer的系统，用来帮助用户对隐私政策进行个性化分析，显著减少了用户的认知负担，并揭示了数据隐私政策中的一些常见问题。


<details>
  <summary>Details</summary>
Motivation: 很多人有多个在线账户，但很少真正阅读服务条款或隐私政策，而之前的研究缺乏个性化和规模化的隐私政策分析工具。

Method: 通过扩展现有的数据使用政策语言，结合自然语言处理（NLP）技术和逻辑推理，开发了一个名为PoliAnalyzer的神经-符号混合系统，实现隐私政策的形式化建模并生成合规报告，同时引入法律专家标注的数据集和用户偏好模型进行验证。

Result: 系统在大部分任务的准确性F1得分达到90-100%，并对100个访问量最高的网站的隐私政策进行了分析，发现95.2%的片段符合用户偏好，仅4.8%的片段明确违反。此外，识别出一些常见违规行为，如将定位数据共享给第三方。

Conclusion: PoliAnalyzer支持大规模的个性化隐私政策分析，可显著减轻用户认知负担，提升用户对数据的控制力，并有助于引发对数据实践的社会讨论，推动公平数据政策的制定。

Abstract: In modern times, people have numerous online accounts, but they rarely read
the Terms of Service or Privacy Policy of those sites despite claiming
otherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that
assists users with personalized privacy policy analysis. PoliAnalyzer uses
Natural Language Processing (NLP) to extract formal representations of data
usage practices from policy texts. In favor of deterministic, logical inference
is applied to compare user preferences with the formal privacy policy
representation and produce a compliance report. To achieve this, we extend an
existing formal Data Terms of Use policy language to model privacy policies as
app policies and user preferences as data policies. In our evaluation using our
enriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated
high accuracy in identifying relevant data usage practices, achieving F1-score
of 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can
model diverse user data-sharing preferences, derived from prior research as 23
user profiles, and perform compliance analysis against the top 100 most-visited
websites. This analysis revealed that, on average, 95.2% of a privacy policy's
segments do not conflict with the analyzed user preferences, enabling users to
concentrate on understanding the 4.8% (636 / 13205) that violates preferences,
significantly reducing cognitive burden. Further, we identified common
practices in privacy policies that violate user expectations - such as the
sharing of location data with 3rd parties. This paper demonstrates that
PoliAnalyzer can support automated personalized privacy policy analysis at
scale using off-the-shelf NLP tools. This sheds light on a pathway to help
individuals regain control over their data and encourage societal discussions
on platform data practices to promote a fairer power dynamic.

</details>


### [146] [Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media](https://arxiv.org/abs/2507.14231)
*Khalid Hasan,Jamil Saquer*

Main category: cs.CL

TL;DR: 本研究利用用户生成的社交媒体文本，评估先进的自然语言处理（NLP）模型识别双相情感障碍的迹象，RoBERTa和基于BERT的LSTM模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 双相情感障碍由于早期症状微妙和社会偏见常被漏诊，需要新技术帮助提升早期检测能力。

Method: 研究评估了一系列基于Transformer（BERT、RoBERTa等）及LSTM结合不同词嵌入技术（BERT、GloVe等）的自然语言模型，并使用Reddit帖子数据进行实验验证。

Result: RoBERTa模型实现了约98%的F1分数，基于BERT嵌入的LSTM模型取得接近的表现，而静态嵌入的LSTM效果较差。同时，DistilBERT在效率与准确性之间实现了平衡。

Conclusion: 与静态嵌入相比，基于上下文的语言建模在检测双相情感障碍中尤为重要。本研究为心理健康NLP应用中的模型选择提供了有价值的见解，并支持基于NLP技术的双相情感障碍早期筛查。

Abstract: Bipolar disorder is a chronic mental illness frequently underdiagnosed due to
subtle early symptoms and social stigma. This paper explores the advanced
natural language processing (NLP) models for recognizing signs of bipolar
disorder based on user-generated social media text. We conduct a comprehensive
evaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,
DistilBERT) and Long Short Term Memory (LSTM) models based on contextualized
(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed
on a large, annotated dataset of Reddit posts after confirming their validity
through sentiment variance and judgmental analysis. Our results demonstrate
that RoBERTa achieves the highest performance among transformer models with an
F1 score of ~98% while LSTM models using BERT embeddings yield nearly identical
results. In contrast, LSTMs trained on static embeddings fail to capture
meaningful patterns, scoring near-zero F1. These findings underscore the
critical role of contextual language modeling in detecting bipolar disorder. In
addition, we report model training times and highlight that DistilBERT offers
an optimal balance between efficiency and accuracy. In general, our study
offers actionable insights for model selection in mental health NLP
applications and validates the potential of contextualized language models to
support early bipolar disorder screening.

</details>


### [147] [Language Models Change Facts Based on the Way You Talk](https://arxiv.org/abs/2507.14238)
*Matthew Kearney,Reuben Binns,Yarin Gal*

Main category: cs.CL

TL;DR: 论文分析了大语言模型（LLMs）如何根据用户语言中的身份标记影响高风险领域的应用，并发现其存在明显的种族、性别和年龄偏见。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs如何在真实应用中利用用户身份信息进行决策，以及这些信息对用户结果的潜在影响。

Method: 对医学、法律、政治、政府福利、工资待遇五个领域的LLMs应用进行了全面分析，并设计了新的评估工具。

Result: LLMs对用户身份的标记非常敏感，会因种族、性别、年龄等因素生成存在偏向性的回应，比如建议不同种族接受不同医疗标准、建议非白人接受较低工资等。

Conclusion: 目前的LLMs模型直接应用在用户面向的应用中可能带来有害的结果，因此在部署前必须进行全面评估以减少不公平决策的风险。

Abstract: Large language models (LLMs) are increasingly being used in user-facing
applications, from providing medical consultations to job interview advice.
Recent research suggests that these models are becoming increasingly proficient
at inferring identity information about the author of a piece of text from
linguistic patterns as subtle as the choice of a few words. However, little is
known about how LLMs use this information in their decision-making in
real-world applications. We perform the first comprehensive analysis of how
identity markers present in a user's writing bias LLM responses across five
different high-stakes LLM applications in the domains of medicine, law,
politics, government benefits, and job salaries. We find that LLMs are
extremely sensitive to markers of identity in user queries and that race,
gender, and age consistently influence LLM responses in these applications. For
instance, when providing medical advice, we find that models apply different
standards of care to individuals of different ethnicities for the same
symptoms; we find that LLMs are more likely to alter answers to align with a
conservative (liberal) political worldview when asked factual questions by
older (younger) individuals; and that LLMs recommend lower salaries for
non-White job applicants and higher salaries for women compared to men. Taken
together, these biases mean that the use of off-the-shelf LLMs for these
applications may cause harmful differences in medical care, foster wage gaps,
and create different political factual realities for people of different
identities. Beyond providing an analysis, we also provide new tools for
evaluating how subtle encoding of identity in users' language choices impacts
model decisions. Given the serious implications of these findings, we recommend
that similar thorough assessments of LLM use in user-facing applications are
conducted before future deployment.

</details>


### [148] [CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation](https://arxiv.org/abs/2507.14239)
*Weihua Zheng,Roy Ka-Wei Lee,Zhengyuan Liu,Kui Wu,AiTi Aw,Bowei Zou*

Main category: cs.CL

TL;DR: 本文提出了CCL-XCoT框架，通过跨语言对比学习和高资源语言推理，引导MLLMs在低资源语言中生成更准确的输出，减低了幻觉率。


<details>
  <summary>Details</summary>
Motivation: 多语言大型语言模型（MLLMs）在低资源语言中容易产生幻觉现象，特别是在领域特定生成任务中，这是由训练数据不平衡引起的。需要方法来减少幻觉以提高生成质量。

Method: 提出了一种两阶段微调框架：第一阶段结合跨语言对比学习与下一个标记预测强化跨语言语义对齐，第二阶段通过在指令微调中加入链式思维（XCoT）策略，引导模型先以高资源语言推理后生成低资源语言的答案。

Result: 实验表明，CCL-XCoT将幻觉率降低了62%，显著提升了语言对间的事实知识迁移能力，同时不依赖外部检索或多模型集成。

Conclusion: CCL-XCoT框架有效减少了低资源语言环境中的幻觉现象，并增强了MLLMs的跨语言推理和知识迁移能力。

Abstract: Multilingual Large Language Models(MLLMs) demonstrate strong generalization
across languages, yet they remain prone to hallucinations, especially in
low-resource languages, due to training data imbalances. These hallucinations,
which include inaccurate or fabricated outputs, are particularly problematic in
domain-specific generation tasks (Chataigner et al., 2024). To address this
challenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based
Cross-lingual Chain-of-Thought), a two-stage fine-tuning framework for
mitigating hallucination in MLLMs. Our approach first enhances cross-lingual
semantic alignment through curriculum-based contrastive learning combined with
next-token prediction during continued pre-training. Building on this
foundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting
strategy during instruction fine-tuning, which guides the model to reason in a
high-resource language before generating answers in the target low-resource
language. Experimental results show that CCL-XCoT reduces hallucination rates
by up to 62% and substantially improves factual knowledge transfer across
language pairs, without relying on external retrieval or multi-model ensembles.

</details>


### [149] [HuggingGraph: Understanding the Supply Chain of LLM Ecosystem](https://arxiv.org/abs/2507.14240)
*Mohammad Shahedur Rahman,Peng Gao,Yuede Ji*

Main category: cs.CL

TL;DR: 本文研究大型语言模型（LLMs）的供应链，构建异构图分析模型与数据集之间的关系。


<details>
  <summary>Details</summary>
Motivation: 旨在解决LLMs模型开发和部署中的复杂性，理解其模型与数据的来源与联系，以提升公平性与合规性。

Method: 设计系统化方法收集LLMs供应链数据，并通过异构图建模，构建包含397,376个节点和453,469条边的关系图。

Result: 揭示LLMs供应链图大且稀疏，具有幂律分布，核心节点连接密集，数据集在训练中扮演重要角色，模型与数据集间存在强依赖性，并且图随时间动态更新。

Conclusion: 研究表明，对LLMs供应链的分析可帮助揭示风险并促进发展，提供对模型和数据集间关系的深刻洞见。

Abstract: Large language models (LLMs) leverage deep learning to process and predict
sequences of words from context, enabling them to perform various NLP tasks,
such as translation, summarization, question answering, and content generation.
However, the growing size and complexity of developing, training, and deploying
advanced LLMs require extensive computational resources and large datasets.
This creates a barrier for users. As a result, platforms that host models and
datasets are widely used. For example, Hugging Face, one of the most popular
platforms, hosted 1.8 million models and 450K datasets by June 2025, with no
sign of slowing down. Since many LLMs are built from base models, pre-trained
models, and external datasets, they can inherit vulnerabilities, biases, or
malicious components from earlier models or datasets. Therefore, it is critical
to understand the origin and development of these components to better detect
potential risks, improve model fairness, and ensure compliance. Motivated by
this, our project aims to study the relationships between models and datasets,
which are core components of the LLM supply chain. First, we design a method to
systematically collect LLM supply chain data. Using this data, we build a
directed heterogeneous graph to model the relationships between models and
datasets, resulting in a structure with 397,376 nodes and 453,469 edges. We
then perform various analyses and uncover several findings, such as: (i) the
LLM supply chain graph is large, sparse, and follows a power-law degree
distribution; (ii) it features a densely connected core and a fragmented
periphery; (iii) datasets play pivotal roles in training; (iv) strong
interdependence exists between models and datasets; and (v) the graph is
dynamic, with daily updates reflecting the ecosystem's ongoing evolution.

</details>


### [150] [Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models](https://arxiv.org/abs/2507.14241)
*Rithesh Murthy,Ming Zhu,Liangwei Yang,Jielin Qiu,Juntao Tan,Shelby Heinecke,Huan Wang,Caiming Xiong,Silvio Savarese*

Main category: cs.CL

TL;DR: Promptomatix 提供自动化的提示优化，无需手动调整或专业领域知识，提升 LLMs 的任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有的提示工程需要手动调整，效率低、结果不一致，并且对非专家人员不够友好。

Method: 提出 Promptomatix 框架，结合元提示优化器和基于 DSPy 的编译器，通过模块化设计实现扩展性，并利用合成训练数据和成本感知目标优化提示。

Result: 在五个任务类别中，Promptomatix 展现出与现有库相当或更优的性能，同时显著降低了提示长度和计算开销。

Conclusion: Promptomatix 实现了可扩展、有效的提示优化，使提示工程更加高效友好，为未来更复杂的框架奠定了基础。

Abstract: Large Language Models (LLMs) perform best with well-crafted prompts, yet
prompt engineering remains manual, inconsistent, and inaccessible to
non-experts. We introduce Promptomatix, an automatic prompt optimization
framework that transforms natural language task descriptions into high-quality
prompts without requiring manual tuning or domain expertise. Promptomatix
supports both a lightweight meta-prompt-based optimizer and a DSPy-powered
compiler, with modular design enabling future extension to more advanced
frameworks. The system analyzes user intent, generates synthetic training data,
selects prompting strategies, and refines prompts using cost-aware objectives.
Evaluated across 5 task categories, Promptomatix achieves competitive or
superior performance compared to existing libraries, while reducing prompt
length and computational overhead making prompt optimization scalable and
efficient.

</details>


### [151] [In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding](https://arxiv.org/abs/2507.14298)
*Wan-Cyuan Fan,Yen-Chun Chen,Mengchen Liu,Alexander Jacobson,Lu Yuan,Leonid Sigal*

Main category: cs.CL

TL;DR: 本文提出ChartScope，一种优化的LVLM，用于深入理解各种类型的图表，并引入了高效的数据生成管道和双路径训练策略，显著提升了模型的图表理解能力。


<details>
  <summary>Details</summary>
Motivation: 目前对大规模视觉语言模型的定制方法在科学图表理解中取得了一些进展，但存在数据配对限制和缺乏针对图表数据对齐的预训练的问题。

Method: 提出了一种高效的数据生成管道，用于合成多种类型图表的配对数据。此外，引入了双路径训练策略，使模型在提取关键信息的同时保留强大的推理能力。

Result: 实验结果表明，ChartScope在广泛的图表类型上显著提升了理解能力。

Conclusion: ChartScope通过创新的数据生成技术和训练方法，提供了一个更强大的解决方案用于图表理解，并通过新基准ChartDQA全面评估模型能力，为领域提供了宝贵资源。

Abstract: Recent methods for customizing Large Vision Language Models (LVLMs) for
domain-specific tasks have shown promising results in scientific chart
comprehension. However, existing approaches face two major limitations: First,
they rely on paired data from only a few chart types, limiting generalization
to wide range of chart types. Secondly, they lack targeted pre-training for
chart-data alignment, which hampers the model's understanding of underlying
data. In this paper, we introduce ChartScope, an LVLM optimized for in-depth
chart comprehension across diverse chart types. We propose an efficient data
generation pipeline that synthesizes paired data for a wide range of chart
types, along with a novel Dual-Path training strategy that enabling the model
to succinctly capture essential data details while preserving robust reasoning
capabilities by incorporating reasoning over the underlying data. Lastly, we
establish ChartDQA, a new benchmark for evaluating not only question-answering
at different levels but also underlying data understanding. Experimental
results demonstrate that ChartScope significantly enhances comprehension on a
wide range of chart types. The code and data are available at
https://davidhalladay.github.io/chartscope_demo.

</details>


### [152] [Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study](https://arxiv.org/abs/2507.14304)
*Rakesh Paul,Anusha Kamath,Kanishk Singla,Raviraj Joshi,Utkarsh Vaidya,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

TL;DR: 提出了一种用于多语言大模型低资源语言对齐的选择性翻译技术，通过保留非可翻译内容提升翻译质量，从而改善低资源语言的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 解决多语言大模型中低资源语言与英语语言对齐性能差距的问题，尤其是在缺乏高质量数据的情况下。

Method: 提出基于LLM的选择性翻译技术，保留文本中非可翻译的内容和句子结构，只翻译可翻译部分，并研究优化翻译质量的方法（例如过滤噪声输出、与原始英语数据混合）。

Result: 实验以印度低资源语言印地语为例，比较Google Cloud Translation（GCP）和Llama-3.1-405B生成的翻译数据，验证选择性翻译技术实际可行且有效。

Conclusion: 选择性翻译是一种实用且有效的方法，可显著改善低资源语言的大模型多语言对齐性能。

Abstract: Multilingual large language models (LLMs) often demonstrate a performance gap
between English and non-English languages, particularly in low-resource
settings. Aligning these models to low-resource languages is essential yet
challenging due to limited high-quality data. While English alignment datasets
are readily available, curating equivalent data in other languages is expensive
and time-consuming. A common workaround is to translate existing English
alignment data; however, standard translation techniques often fail to preserve
critical elements such as code, mathematical expressions, and structured
formats like JSON. In this work, we investigate LLM-based selective
translation, a technique that selectively translates only the translatable
parts of a text while preserving non-translatable content and sentence
structure. We conduct a systematic study to explore key questions around this
approach, including its effectiveness compared to vanilla translation, the
importance of filtering noisy outputs, and the benefits of mixing translated
samples with original English data during alignment. Our experiments focus on
the low-resource Indic language Hindi and compare translations generated by
Google Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the
promise of selective translation as a practical and effective method for
improving multilingual alignment in LLMs.

</details>


### [153] [How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs](https://arxiv.org/abs/2507.14307)
*Karin de Langis,Jong Inn Park,Andreas Schramm,Bin Hu,Khanh Chi Le,Michael Mensink,Ahn Thu Tong,Dongyeop Kang*

Main category: cs.CL

TL;DR: 研究分析了大语言模型（LLMs）处理叙述中语言时间特性的能力，发现其对叙述性理解存在缺陷，并提出了用于评估LLMs认知和语言能力的标准化实验框架。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在语言能力上有显著进展，但其行为是否反映人类认知仍存疑，本研究旨在探索LLMs在叙述中处理语言时间意义的能力。

Method: 利用专家参与的探索管道，对LLMs进行系列实验，评估其是否以人类方式构建语义表征和语用推断。

Result: 研究发现LLMs过度依赖典型性，形成不一致的时态判断，对因果推理能力有限，表明其对叙述理解存在不足。

Conclusion: LLMs在处理语言时间特性上与人类方式存在根本差异，缺乏稳健的叙事理解能力，同时研究提出了评估LLMs能力的标准化框架。

Abstract: Large language models (LLMs) exhibit increasingly sophisticated linguistic
capabilities, yet the extent to which these behaviors reflect human-like
cognition versus advanced pattern recognition remains an open question. In this
study, we investigate how LLMs process the temporal meaning of linguistic
aspect in narratives that were previously used in human studies. Using an
Expert-in-the-Loop probing pipeline, we conduct a series of targeted
experiments to assess whether LLMs construct semantic representations and
pragmatic inferences in a human-like manner. Our findings show that LLMs
over-rely on prototypicality, produce inconsistent aspectual judgments, and
struggle with causal reasoning derived from aspect, raising concerns about
their ability to fully comprehend narratives. These results suggest that LLMs
process aspect fundamentally differently from humans and lack robust narrative
understanding. Beyond these empirical findings, we develop a standardized
experimental framework for the reliable assessment of LLMs' cognitive and
linguistic capabilities.

</details>


### [154] [What Makes You CLIC: Detection of Croatian Clickbait Headlines](https://arxiv.org/abs/2507.14314)
*Marija Anđedelić,Dominik Šipek,Laura Majer,Jan Šnajder*

Main category: cs.CL

TL;DR: 本文提出了一个用于克罗地亚新闻标题钓鱼点击（clickbait）检测的新数据集CLIC，并比较了微调模型和基于大语言模型（LLM）的上下文学习性能。


<details>
  <summary>Details</summary>
Motivation: 由于新闻行业的广告驱动模式，凸显了检测钓鱼点击标题以维护信息质量与读者信任的必要性。特别是在资源稀缺语言中，最佳方法仍具有不确定性。

Method: 提出并使用了一个新的数据集CLIC，涵盖克罗地亚新闻标题20年的数据。比较了BERTi'c模型微调与使用克罗地亚语和英语提示的大型语言模型（LLM）的上下文学习效果。

Result: 分析发现将近一半的标题包含钓鱼点击内容，且微调模型的表现优于一般的大型语言模型。

Conclusion: 微调方法在钓鱼点击检测中特别是在资源稀缺语言上表现更优，提示了细致语言特定模型的重要性。

Abstract: Online news outlets operate predominantly on an advertising-based revenue
model, compelling journalists to create headlines that are often scandalous,
intriguing, and provocative -- commonly referred to as clickbait. Automatic
detection of clickbait headlines is essential for preserving information
quality and reader trust in digital media and requires both contextual
understanding and world knowledge. For this task, particularly in
less-resourced languages, it remains unclear whether fine-tuned methods or
in-context learning (ICL) yield better results. In this paper, we compile CLIC,
a novel dataset for clickbait detection of Croatian news headlines spanning a
20-year period and encompassing mainstream and fringe outlets. We fine-tune the
BERTi\'c model on this task and compare its performance to LLM-based ICL
methods with prompts both in Croatian and English. Finally, we analyze the
linguistic properties of clickbait. We find that nearly half of the analyzed
headlines contain clickbait, and that finetuned models deliver better results
than general LLMs.

</details>


### [155] [Can LLMs Infer Personality from Real World Conversations?](https://arxiv.org/abs/2507.14355)
*Jianfeng Zhu,Ruoming Jin,Karin G. Coifman*

Main category: cs.CL

TL;DR: 本文研究利用大型语言模型（LLMs）如GPT-4和LLaMA进行个性评估的可行性，通过对555份真实半结构化访谈进行分析，揭示了其高重测信度但有效性有限。


<details>
  <summary>Details</summary>
Motivation: 探索利用大型语言模型实现可扩展且有效的个性评估，同时填补之前研究过多依赖合成数据或欠缺心理测量效度的社交媒体文本的空白。

Method: 提出了一项基于555份带有BFI-10自评分的半结构化访谈的真实数据集，用于测试三种最先进的LLMs（GPT-4.1 Mini、Meta-LLaMA和DeepSeek）在零样本预测和链式推理下的表现。

Result: 尽管所有模型表现出较高的重测信度，但其构念效度较低（最大Pearson相关系数为0.27），评分一致性差（Cohen's κ < 0.10），预测结果有偏向中高水平的倾向。

Conclusion: 当前的LLMs在个性推断上的能力有限，需进一步基于证据进行心理学应用开发。

Abstract: Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a
promising approach for scalable personality assessment from open-ended
language. However, inferring personality traits remains challenging, and
earlier work often relied on synthetic data or social media text lacking
psychometric validity. We introduce a real-world benchmark of 555
semi-structured interviews with BFI-10 self-report scores for evaluating
LLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini,
Meta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item
prediction and both zero-shot and chain-of-thought prompting for Big Five trait
inference. All models showed high test-retest reliability, but construct
validity was limited: correlations with ground-truth scores were weak (max
Pearson's $r = 0.27$), interrater agreement was low (Cohen's $\kappa < 0.10$),
and predictions were biased toward moderate or high trait levels.
Chain-of-thought prompting and longer input context modestly improved
distributional alignment, but not trait-level accuracy. These results
underscore limitations in current LLM-based personality inference and highlight
the need for evidence-based development for psychological applications.

</details>


### [156] [Text-to-SQL for Enterprise Data Analytics](https://arxiv.org/abs/2507.14372)
*Albert Chen,Manas Bundele,Gaurav Ahlawat,Patrick Stetz,Zhitao Wang,Qiang Fei,Donghoon Jung,Audrey Chu,Bharadwaj Jayaraman,Ayushi Panth,Yatin Arora,Sourav Jain,Renjith Varma,Alexey Ilin,Iuliia Melnychuk,Chelsea Chueh,Joyan Sil,Xiaofeng Wang*

Main category: cs.CL

TL;DR: 用知识图谱和Text-to-SQL技术构建企业数据查询聊天机器人，为用户提供交互式数据洞察服务。


<details>
  <summary>Details</summary>
Motivation: 现有的Text-to-SQL模型虽然在基准测试中表现优异，但其在企业环境中的应用仍面临诸多挑战，本文旨在解决此问题。

Method: 构建知识图谱以捕获数据库的最新语义信息；开发Text-to-SQL代理用于检索和修正语法与内容问题；创建支持多种用户意图且交互性强的聊天机器人。

Result: 聊天机器人每周有300多名用户；在内部基准数据集上的响应准确率达到53%。

Conclusion: 通过组件优化和知识图谱，提供了开发企业级Text-to-SQL解决方案的有效途径，为用户提供更加便捷的数据访问方式。

Abstract: The introduction of large language models has brought rapid progress on
Text-to-SQL benchmarks, but it is not yet easy to build a working enterprise
solution. In this paper, we present insights from building an internal chatbot
that enables LinkedIn's product managers, engineers, and operations teams to
self-serve data insights from a large, dynamic data lake. Our approach features
three components. First, we construct a knowledge graph that captures
up-to-date semantics by indexing database metadata, historical query logs,
wikis, and code. We apply clustering to identify relevant tables for each team
or product area. Second, we build a Text-to-SQL agent that retrieves and ranks
context from the knowledge graph, writes a query, and automatically corrects
hallucinations and syntax errors. Third, we build an interactive chatbot that
supports various user intents, from data discovery to query writing to
debugging, and displays responses in rich UI elements to encourage follow-up
chats. Our chatbot has over 300 weekly users. Expert review shows that 53% of
its responses are correct or close to correct on an internal benchmark set.
Through ablation studies, we identify the most important knowledge graph and
modeling components, offering a practical path for developing enterprise
Text-to-SQL solutions.

</details>


### [157] [Error-Aware Curriculum Learning for Biomedical Relation Classification](https://arxiv.org/abs/2507.14374)
*Sinchani Chakraborty,Sudeshna Sarkar,Pawan Goyal*

Main category: cs.CL

TL;DR: 本文提出了一种错误感知的师生框架方法，用于改进生物医学文本中的关系分类（RC），通过大语言模型（GPT-4o）的结构化指导显著提高了性能。


<details>
  <summary>Details</summary>
Motivation: 生物医学文本中的关系分类对构建知识图谱、药物重新利用和临床决策极为关键。

Method: 提出一种错误感知的师生框架，利用GPT-4o分析预测错误类型、难度评分和生成改进建议，通过指令微调和课程学习逐步训练两个学生模型，并从PubMed摘要中构建异构生物医学知识图谱来支持上下文感知的RC。

Result: 在4个PPI数据集和DDI数据集上取得了最新的状态-of-the-art性能，与ChemProt数据集上的方法相比依然具有竞争力。

Conclusion: 错误感知的师生框架和异构知识图谱可有效提升关系分类的性能，其方法具备广泛的适用性和创新性。

Abstract: Relation Classification (RC) in biomedical texts is essential for
constructing knowledge graphs and enabling applications such as drug
repurposing and clinical decision-making. We propose an error-aware
teacher--student framework that improves RC through structured guidance from a
large language model (GPT-4o). Prediction failures from a baseline student
model are analyzed by the teacher to classify error types, assign difficulty
scores, and generate targeted remediations, including sentence rewrites and
suggestions for KG-based enrichment. These enriched annotations are used to
train a first student model via instruction tuning. This model then annotates a
broader dataset with difficulty scores and remediation-enhanced inputs. A
second student is subsequently trained via curriculum learning on this dataset,
ordered by difficulty, to promote robust and progressive learning. We also
construct a heterogeneous biomedical knowledge graph from PubMed abstracts to
support context-aware RC. Our approach achieves new state-of-the-art
performance on 4 of 5 PPI datasets and the DDI dataset, while remaining
competitive on ChemProt.

</details>


### [158] [X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display](https://arxiv.org/abs/2507.14430)
*Xiaolin Yan,Yangxing Liu,Jiazhang Zheng,Chi Liu,Mingyu Du,Caisheng Chen,Haoyang Liu,Ming Ding,Yuan Li,Qiuping Liao,Linfeng Li,Zhili Mei,Siyu Wan,Li Li,Ruyi Zhong,Jiangling Yu,Xule Liu,Huihui Hu,Jiameng Yue,Ruohui Cheng,Qi Yang,Liangqing Wu,Ke Zhu,Chi Zhang,Chufei Jing,Yifan Zhou,Yan Liang,Dongdong Li,Zhaohui Wang,Bin Zhao,Mingzhou Wu,Mingzhong Zhou,Peng Du,Zuomin Liao,Chao Dai,Pengfei Liang,Xiaoguang Zhu,Yu Zhang,Yu Gu,Kun Pan,Yuan Wu,Yanqing Guan,Shaojing Wu,Zikang Feng,Xianze Ma,Peishan Cheng,Wenjuan Jiang,Jing Ba,Huihao Yu,Zeping Hu,Yuan Xu,Zhiwei Liu,He Wang,Zhenguo Lin,Ming Liu,Yanhong Meng*

Main category: cs.CL

TL;DR: X-Intelligence 3.0是针对半导体显示行业开发的高性能推理模型，尽管只有32亿参数，却超越了DeepSeek-R1-671B，展示了其高效能。


<details>
  <summary>Details</summary>
Motivation: 现有的LLMs在半导体显示行业中效果有限，需要一个专为该领域设计的推理模型。

Method: 依托精心策划的行业知识库，通过监督微调和强化学习提升理解和推理能力；引入自动化评估框架和领域特定的检索增强生成（RAG）机制。

Result: X-Intelligence 3.0在多个评估中表现超越SOTA DeepSeek-R1-671B，展示出强大的高效性能。

Conclusion: X-Intelligence 3.0在解决半导体显示行业的复杂推理挑战方面具有优秀表现，为行业提供了强大的解决方案。

Abstract: Large language models (LLMs) have recently achieved significant advances in
reasoning and demonstrated their advantages in solving challenging problems.
Yet, their effectiveness in the semiconductor display industry remains limited
due to a lack of domain-specific training and expertise. To bridge this gap, we
present X-Intelligence 3.0, the first high-performance reasoning model
specifically developed for the semiconductor display industry. This model is
designed to deliver expert-level understanding and reasoning for the industry's
complex challenges. Leveraging a carefully curated industry knowledge base, the
model undergoes supervised fine-tuning and reinforcement learning to enhance
its reasoning and comprehension capabilities. To further accelerate
development, we implemented an automated evaluation framework that simulates
expert-level assessments. We also integrated a domain-specific
retrieval-augmented generation (RAG) mechanism, resulting in notable
performance gains on benchmark datasets. Despite its relatively compact size of
32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B
across multiple evaluations. This demonstrates its exceptional efficiency and
establishes it as a powerful solution to the longstanding reasoning challenges
faced by the semiconductor display industry.

</details>


### [159] [XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification](https://arxiv.org/abs/2507.14578)
*Sachin Yadav,Dominik Schlechtweg*

Main category: cs.CL

TL;DR: 该论文提出了一种多语言句子变换器模型XL-DURel，针对Word-in-Context (WiC)分类优化，并表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 通过优化WiC分类模型解决序数数据和二进制数据的处理问题，从而实现一种统一方法用于不同类型WiC任务。

Method: 使用微调的多语言Sentence Transformer模型，并在复杂空间中基于角距离的排序目标进行优化，同时测试多种回归和排序任务的损失函数。

Result: 模型在序数和二进制数据上的表现优于之前的方法，同时证明二进制WiC可以视为序数WiC的一个特例。

Conclusion: 优化针对序数任务的模型提升了二进制任务的表现，表明一种统一的WiC建模方法是可行的。

Abstract: We propose XL-DURel, a finetuned, multilingual Sentence Transformer model
optimized for ordinal Word-in-Context classification. We test several loss
functions for regression and ranking tasks managing to outperform previous
models on ordinal and binary data with a ranking objective based on angular
distance in complex space. We further show that binary WiC can be treated as a
special case of ordinal WiC and that optimizing models for the general ordinal
task improves performance on the more specific binary task. This paves the way
for a unified treatment of WiC modeling across different task formulations.

</details>


### [160] [Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models](https://arxiv.org/abs/2507.14579)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 本文探讨通过对话数据中的CPS指标检测，比较BERT和AudiBERT模型的表现，并强调了AudiBERT在社交认知维度上的显著分类改进。


<details>
  <summary>Details</summary>
Motivation: 开发AI模型以识别协作问题解决（CPS）指标，增强教育领域人机互补能力。

Method: 使用AudiBERT模型，将语音和音频特征整合，在数据集中检测稀疏类别，并进行相关性分析。

Result: AudiBERT在社交认知维度上有显著改进，但在情感维度的改进有限，同时训练数据的增加显著提高模型召回率。

Conclusion: 主张采用结构化方法实现人机互补，增强模型可解释性以支持人类的深度参与和反思。

Abstract: Detecting collaborative problem solving (CPS) indicators from dialogue using
machine learning techniques is a significant challenge for the field of AI in
Education. Recent studies have explored the use of Bidirectional Encoder
Representations from Transformers (BERT) models on transcription data to
reliably detect meaningful CPS indicators. A notable advancement involved the
multimodal BERT variant, AudiBERT, which integrates speech and
acoustic-prosodic audio features to enhance CPS diagnosis. Although initial
results demonstrated multimodal improvements, the statistical significance of
these enhancements remained unclear, and there was insufficient guidance on
leveraging human-AI complementarity for CPS diagnosis tasks. This workshop
paper extends the previous research by highlighting that the AudiBERT model not
only improved the classification of classes that were sparse in the dataset,
but it also had statistically significant class-wise improvements over the BERT
model for classifications in the social-cognitive dimension. However, similar
significant class-wise improvements over the BERT model were not observed for
classifications in the affective dimension. A correlation analysis highlighted
that larger training data was significantly associated with higher recall
performance for both the AudiBERT and BERT models. Additionally, the precision
of the BERT model was significantly associated with high inter-rater agreement
among human coders. When employing the BERT model to diagnose indicators within
these subskills that were well-detected by the AudiBERT model, the performance
across all indicators was inconsistent. We conclude the paper by outlining a
structured approach towards achieving human-AI complementarity for CPS
diagnosis, highlighting the crucial inclusion of model explainability to
support human agency and engagement in the reflective coding process.

</details>


### [161] [Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption](https://arxiv.org/abs/2507.14584)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 本文探讨了使用BERT及其变体模型进行协作问题解决（CPS）分类的可解释性，重点研究了用SHAP分析数据中的词对分类决策的影响。


<details>
  <summary>Details</summary>
Motivation: 提高BERT模型在协作问题解决分类中的透明性和可解释性，以便更好地服务教师等终端用户，增强信任并促进其在教育领域的应用。

Method: 采用SHAP方法分析BERT模型对数据中不同词的分类贡献，并探讨模型对CPS子技能判别的合理性。

Result: 研究发现高性能分类并不等于合理解释，一些无语义意义的词也可能对分类有显著影响；同时分析中发现模型利用词的合理性与分类种类数目有关。

Conclusion: 尽管模型透明性对终端用户改善实践作用有限，但有助于其避免对大语言模型的过度依赖。研究建议未来探索合奏模型和人机互补方法来改进CPS诊断。

Abstract: The use of Bidirectional Encoder Representations from Transformers (BERT)
model and its variants for classifying collaborative problem solving (CPS) has
been extensively explored within the AI in Education community. However,
limited attention has been given to understanding how individual tokenised
words in the dataset contribute to the model's classification decisions.
Enhancing the explainability of BERT-based CPS diagnostics is essential to
better inform end users such as teachers, thereby fostering greater trust and
facilitating wider adoption in education. This study undertook a preliminary
step towards model transparency and explainability by using SHapley Additive
exPlanations (SHAP) to examine how different tokenised words in transcription
data contributed to a BERT model's classification of CPS processes. The
findings suggested that well-performing classifications did not necessarily
equate to a reasonable explanation for the classification decisions. Particular
tokenised words were used frequently to affect classifications. The analysis
also identified a spurious word, which contributed positively to the
classification but was not semantically meaningful to the class. While such
model transparency is unlikely to be useful to an end user to improve their
practice, it can help them not to overrely on LLM diagnostics and ignore their
human expertise. We conclude the workshop paper by noting that the extent to
which the model appropriately uses the tokens for its classification is
associated with the number of classes involved. It calls for an investigation
into the exploration of ensemble model architectures and the involvement of
human-AI complementarity for CPS diagnosis, since considerable human reasoning
is still required for fine-grained discrimination of CPS subskills.

</details>


### [162] [Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification](https://arxiv.org/abs/2507.14590)
*Łukasz Radliński,Mateusz Guściora,Jan Kocoń*

Main category: cs.CL

TL;DR: 探讨数据增强方法在NLP中的应用，通过实验比较不同方法对分类性能的影响。


<details>
  <summary>Details</summary>
Motivation: 解决数据稀缺和类别不平衡问题，研究利用GPT等大型语言模型进行数据增强的效果。

Method: 比较4种数据增强方法（包括借助ChatGPT进行的反翻译和释义）在多个实验设置下的表现。

Result: 反翻译和释义方法在性能上与零样本和少样本生成方法相当，甚至优于后者。

Conclusion: 传统方法（反翻译和释义）结合新一代模型在NLP数据增强中的表现良好，具有实用价值。

Abstract: Numerous domain-specific machine learning tasks struggle with data scarcity
and class imbalance. This paper systematically explores data augmentation
methods for NLP, particularly through large language models like GPT. The
purpose of this paper is to examine and evaluate whether traditional methods
such as paraphrasing and backtranslation can leverage a new generation of
models to achieve comparable performance to purely generative methods. Methods
aimed at solving the problem of data scarcity and utilizing ChatGPT were
chosen, as well as an exemplary dataset. We conducted a series of experiments
comparing four different approaches to data augmentation in multiple
experimental setups. We then evaluated the results both in terms of the quality
of generated data and its impact on classification performance. The key
findings indicate that backtranslation and paraphrasing can yield comparable or
even better results than zero and a few-shot generation of examples.

</details>


### [163] [Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper](https://arxiv.org/abs/2507.14615)
*Fred Mutisya,Shikoh Gitau,Christine Syovata,Diana Oigara,Ibrahim Matende,Muna Aden,Munira Ali,Ryan Nyotu,Diana Marion,Job Nyangena,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha,Eric Mibuari,Jean Philbert Nsengemana,Talkmore Chidede*

Main category: cs.CL

TL;DR: 本文开发了基于肯尼亚医疗环境的AI评估框架和数据集，揭示了LLMs在非洲医疗场景中的性能差距。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型(LLMs)在非洲初级医疗中的效果，特别是肯尼亚的二级和三级医疗水平下的可能性。

Method: 采用Retrieval Augmented Generation (RAG) 方法，将肯尼亚国家医疗指南进行数字化、分块，并通过语义索引用于临床问题生成和解答，过程中有肯尼亚医生参与数据集设计，专家审阅确保临床准确性和文化适配性。

Result: 生成了包含数千个符合监管标准的问答对数据集，设计了多项评估标准以检验LLMs在本地医疗场景中的临床推理能力和安全性，初步结果显示LLMs在非洲数据上的表现显著低于其在美国数据上的基准表现。

Conclusion: 本文提供了一种可复制的模型，用于基于指南的动态基准测试，支持AI在非洲医疗系统中的安全部署。

Abstract: Large Language Models(LLMs) hold promise for improving healthcare access in
low-resource settings, but their effectiveness in African primary care remains
underexplored. We present a methodology for creating a benchmark dataset and
evaluation framework focused on Kenyan Level 2 and 3 clinical care. Our
approach uses retrieval augmented generation (RAG) to ground clinical questions
in Kenya's national guidelines, ensuring alignment with local standards. These
guidelines were digitized, chunked, and indexed for semantic retrieval. Gemini
Flash 2.0 Lite was then prompted with guideline excerpts to generate realistic
clinical scenarios, multiple-choice questions, and rationale based answers in
English and Swahili. Kenyan physicians co-created and refined the dataset, and
a blinded expert review process ensured clinical accuracy, clarity, and
cultural appropriateness. The resulting Alama Health QA dataset includes
thousands of regulator-aligned question answer pairs across common outpatient
conditions. Beyond accuracy, we introduce evaluation metrics that test clinical
reasoning, safety, and adaptability such as rare case detection (Needle in the
Haystack), stepwise logic (Decision Points), and contextual adaptability.
Initial results reveal significant performance gaps when LLMs are applied to
localized scenarios, consistent with findings that LLM accuracy is lower on
African medical content than on US-based benchmarks. This work offers a
replicable model for guideline-driven, dynamic benchmarking to support safe AI
deployment in African health systems.

</details>


### [164] [Linear Relational Decoding of Morphology in Language Models](https://arxiv.org/abs/2507.14640)
*Eric Xia,Jugal Kalita*

Main category: cs.CL

TL;DR: 本文研究了一种两段式仿射近似模型，用于估算变压器模型内的某些主体-客体关系，并显示了线性变换在重现这些关系方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 探索变压器模型中语言概念关系的可解释性及其在跨层线性变换中的表示能力，特别是与形态学关系相关的内容。

Method: 通过适配更大的类比测试集，研究线性变换Ws（其中s为主体词的中间层表示，W来源于模型导数）如何重现最终客体状态，并在多个语言和模型中验证这种方法。

Result: 线性法在形态学关系上达到了90%的忠实度，多语言和多模型实验均支持类似的结果表明某些语言概念关系可以在隐空间中被解释。

Conclusion: 语言模型中的概念关系（如形态学）能够通过跨层线性变换稀疏地编码并解释。

Abstract: A two-part affine approximation has been found to be a good approximation for
transformer computations over certain subject object relations. Adapting the
Bigger Analogy Test Set, we show that the linear transformation Ws, where s is
a middle layer representation of a subject token and W is derived from model
derivatives, is also able to accurately reproduce final object states for many
relations. This linear technique is able to achieve 90% faithfulness on
morphological relations, and we show similar findings multi-lingually and
across models. Our findings indicate that some conceptual relationships in
language models, such as morphology, are readily interpretable from latent
space, and are sparsely encoded by cross-layer linear transformations.

</details>


### [165] [Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs](https://arxiv.org/abs/2507.14649)
*Minsuh Joo,Hyunsoo Cho*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型（LLMs）的幻觉问题，并提出了一种基于聚类的语义一致性方法（Cleanse）用于量化不确定性，从而有效检测LLMs的幻觉行为。


<details>
  <summary>Details</summary>
Motivation: 旨在解决LLMs生成不准确响应的问题，尤其是通过量化不确定性来区分正确与错误的回答。

Method: 提出了Cleanse方法，利用大语言模型隐藏嵌入的语义信息，通过聚类分析语义一致性并量化不确定性。

Result: 在LLaMA-7B、LLaMA-13B、LLaMA2-7B和Mistral-7B四种模型上，以及SQuAD和CoQA两个数据集上，验证了Cleanse在检测幻觉方面的有效性。

Conclusion: Cleanse成功证明了可以通过语义一致性量化不确定性，从而有效检测LLMs的幻觉行为。

Abstract: Despite the outstanding performance of large language models (LLMs) across
various NLP tasks, hallucinations in LLMs--where LLMs generate inaccurate
responses--remains as a critical problem as it can be directly connected to a
crisis of building safe and reliable LLMs. Uncertainty estimation is primarily
used to measure hallucination levels in LLM responses so that correct and
incorrect answers can be distinguished clearly. This study proposes an
effective uncertainty estimation approach, \textbf{Cl}ust\textbf{e}ring-based
sem\textbf{an}tic con\textbf{s}ist\textbf{e}ncy (\textbf{Cleanse}). Cleanse
quantifies the uncertainty with the proportion of the intra-cluster consistency
in the total consistency between LLM hidden embeddings which contain adequate
semantic information of generations, by employing clustering. The effectiveness
of Cleanse for detecting hallucination is validated using four off-the-shelf
models, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two
question-answering benchmarks, SQuAD and CoQA.

</details>


### [166] [Mangosteen: An Open Thai Corpus for Language Model Pretraining](https://arxiv.org/abs/2507.14664)
*Wannaphong Phatthiyaphaibun,Can Udomcharoenchaikit,Pakpoom Singkorapoom,Kunat Pipatanakul,Ekapol Chuangsuwanich,Peerat Limkonchotiwat,Sarana Nutanong*

Main category: cs.CL

TL;DR: 研究提出了Mangosteen，一个高质量的泰语语料库，通过自定义的清理管道解决了现有语料库中英语中心化或缺乏文化适应性的问题。


<details>
  <summary>Details</summary>
Motivation: 目前的清理管道往往忽略泰语文本的特殊性，未能充分处理噪声问题，因此需要透明且高质量的泰语语料库。

Method: 基于Dolma管道，加入泰语自适应的规则，如语言识别、过滤规则、以及泰语特定内容过滤，同时整合了非网络数据来源。

Result: 通过Mangosteen，显著提升了模型的语言生成性能（如SEA-HELM从3提高至11），并构建了超过现有模型性能的SEA-LION模型。

Conclusion: Mangosteen为泰语及区域语言模型研究提供了高质量、可复现的数据基础，填补了语言与文化适应性不足的研究空白。

Abstract: Pre-training data shapes a language model's quality, but raw web text is
noisy and demands careful cleaning. Existing large-scale corpora rely on
English-centric or language-agnostic pipelines whose heuristics do not capture
Thai script or cultural nuances, leaving risky material such as gambling
content untreated. Prior Thai-specific efforts customize pipelines or build new
ones, yet seldom release their data or document design choices, hindering
reproducibility and raising the question of how to construct a transparent,
high-quality Thai corpus. We introduce Mangosteen: a 47 billion-token Thai
corpus built through a Thai-adapted Dolma pipeline that includes custom
rule-based language ID, revised C4/Gopher quality filters, and Thai-trained
content filters, plus curated non-web sources such as Wikipedia, Royal Gazette
texts, OCR-extracted books, and CC-licensed YouTube subtitles. Systematic
ablations using GPT-2 show the pipeline trims CommonCrawl from 202M to 25M
documents while raising SEA-HELM NLG from 3 to 11; an 8B-parameter SEA-LION
model continually pre-trained on Mangosteen then surpasses SEA-LION-v3 and
Llama-3.1 by about four points on Thai benchmarks. We release the full pipeline
code, cleaning manifests, corpus snapshot, and all checkpoints, providing a
fully reproducible foundation for future Thai and regional LLM research.

</details>


### [167] [Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care](https://arxiv.org/abs/2507.14681)
*Vinicius Anjos de Almeida,Vinicius de Camargo,Raquel Gómez-Bravo,Egbert van der Haring,Kees van Boven,Marcelo Finger,Luis Fernandez Lopez*

Main category: cs.CL

TL;DR: 研究评估了大型语言模型（LLMs）使用领域搜索引擎输出为临床数据分配ICPC-2编码的潜力，发现许多模型表现优异，特别是gpt-4.5-preview等。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在医疗数据编码中的潜力，对医疗研究、质量监控和政策制定具有重要意义。

Method: 使用包括437条巴西葡萄牙语临床表达的已注释数据集，通过语义搜索引擎检索候选项，并用33种LLMs对ICPC-2编码进行选择，评估使用F1分数、成本、响应时间等指标。

Result: 28种模型F1分超过0.8，其中10种超过0.85，gpt-4.5-preview表现最佳。大模型优于小模型，检索器优化提升显著。

Conclusion: LLMs具有自动ICPC-2编码的强大潜力，尽管存在数据集范围和设置局限性，但为未来多语言和端到端验证提供了基准和方向。

Abstract: Background: Medical coding structures healthcare data for research, quality
monitoring, and policy. This study assesses the potential of large language
models (LLMs) to assign ICPC-2 codes using the output of a domain-specific
search engine.
  Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each
annotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's
text-embedding-3-large) retrieved candidates from 73,563 labeled concepts.
Thirty-three LLMs were prompted with each query and retrieved results to select
the best-matching ICPC-2 code. Performance was evaluated using F1-score, along
with token usage, cost, response time, and format adherence.
  Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top
performers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever
optimization can improve performance by up to 4 points. Most models returned
valid codes in the expected format, with reduced hallucinations. Smaller models
(<3B) struggled with formatting and input length.
  Conclusions: LLMs show strong potential for automating ICPC-2 coding, even
without fine-tuning. This work offers a benchmark and highlights challenges,
but findings are limited by dataset scope and setup. Broader, multilingual,
end-to-end evaluations are needed for clinical validation.

</details>


### [168] [MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization](https://arxiv.org/abs/2507.14683)
*Xingxuan Li,Yao Xiao,Dianwen Ng,Hai Ye,Yue Deng,Xiang Lin,Bin Wang,Zhanfeng Mo,Chong Zhang,Yueyi Zhang,Zonglin Yang,Ruilin Li,Lei Lei,Shihao Xu,Han Zhao,Weiling Chen,Feng Ji,Lidong Bing*

Main category: cs.CL

TL;DR: 本文提出MiroMind-M1系列开源推理语言模型（RLMs），基于Qwen-2.5开发，通过两阶段训练实现数学推理领域内的高效性能，并提供完整的资源以促进社区研究。


<details>
  <summary>Details</summary>
Motivation: 当前主流闭源推理语言模型（RLMs）存在透明性和可复现性不足的问题，同时开源项目也因缺乏关键资源而难以推动研究。

Method: 提出一种两阶段训练方法，首先对经过筛选的数学推理问题数据进行监督微调（SFT），然后基于优化策略展开强化学习微调（RLVR），并引入多阶段策略优化算法以提升训练效果。

Result: 在数学推理领域的基准测试（AIME24、AIME25、MATH）中，MiroMind-M1系列模型实现了与现有开源模型性能相当或更优的表现，同时展现了更高的Token效率。

Conclusion: MiroMind-M1系列通过完整开源的模型、数据和配置文件，提升了推理语言模型的透明性和可复现性，以期推动社区研究与合作。

Abstract: Large language models have recently evolved from fluent text generation to
advanced reasoning across diverse domains, giving rise to reasoning language
models. Among these domains, mathematical reasoning serves as a representative
benchmark as it requires precise multi-step logic and abstract reasoning, which
can be generalized to other tasks. While closed-source RLMs such as GPT-o3
demonstrate impressive reasoning capabilities, their proprietary nature limits
transparency and reproducibility. Although many open-source projects aim to
close this gap, most of them lack sufficient openness by omitting critical
resources such as datasets and detailed training configurations, which hinders
reproducibility. To contribute toward greater transparency in RLM development,
we introduce the MiroMind-M1 series, a set of fully open-source RLMs built on
the Qwen-2.5 backbone that match or exceed the performance of existing
open-source RLMs. Specifically, our models are trained in two stages: SFT on a
carefully curated corpus of 719K math-reasoning problems with verified CoT
trajectories, followed by RLVR on 62K challenging and verifiable problems. To
enhance the robustness and efficiency of the RLVR process, we introduce
Context-Aware Multi-Stage Policy Optimization, an algorithm that integrates
length-progressive training with an adaptive repetition penalty to encourage
context-aware RL training. Our model achieves state-of-the-art or competitive
performance and superior token efficiency among Qwen-2.5-based open-source 7B
and 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate
reproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,
MiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,
MiroMind-M1-RL-62K); and all training and evaluation configurations. We hope
these resources will support further research and foster community advancement.

</details>


### [169] [Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations](https://arxiv.org/abs/2507.14688)
*Mohammed Alkhowaiter,Norah Alshahrani,Saied Alshahrani,Reem I. Masoud,Alaa Alzahrani,Deema Alnuhait,Emad A. Alghamdi,Khalid Almubarak*

Main category: cs.CL

TL;DR: 本文回顾了 Hugging Face Hub 上公开的阿拉伯语后训练数据集，分析其在 LLM 能力、引导性、对齐性和稳健性四个维度中的表现，揭示了任务多样性、文档质量和社区采用率等方面的不足，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 目标是对阿拉伯语后训练数据集的现状进行系统评估，发现潜在问题并为未来改进提供方向。

Method: 通过在 Hugging Face Hub 上的公开数据集，作者按照功能能力、引导性、对齐性和稳健性四个维度进行分类，并从流行度、实用性、新颖性、文档质量、许可透明度、科学贡献等方面对数据集进行评估。

Result: 研究显示阿拉伯语数据集存在任务多样性不足、文档和注释不一致或缺失、社区采用低等问题。

Conclusion: 研究强调了改进阿拉伯语后训练数据集的重要性，并给出具体建议以推动阿拉伯 LLM 及其应用的发展。

Abstract: Post-training has emerged as a crucial technique for aligning pre-trained
Large Language Models (LLMs) with human instructions, significantly enhancing
their performance across a wide range of tasks. Central to this process is the
quality and diversity of post-training datasets. This paper presents a review
of publicly available Arabic post-training datasets on the Hugging Face Hub,
organized along four key dimensions: (1) LLM Capabilities (e.g., Question
Answering, Translation, Reasoning, Summarization, Dialogue, Code Generation,
and Function Calling); (2) Steerability (e.g., persona and system prompts); (3)
Alignment (e.g., cultural, safety, ethics, and fairness), and (4) Robustness.
Each dataset is rigorously evaluated based on popularity, practical adoption,
recency and maintenance, documentation and annotation quality, licensing
transparency, and scientific contribution. Our review revealed critical gaps in
the development of Arabic post-training datasets, including limited task
diversity, inconsistent or missing documentation and annotation, and low
adoption across the community. Finally, the paper discusses the implications of
these gaps on the progress of Arabic LLMs and applications while providing
concrete recommendations for future efforts in post-training dataset
development.

</details>


### [170] [Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation](https://arxiv.org/abs/2507.14693)
*Amina Dzafic,Merve Kavut,Ulya Bayram*

Main category: cs.CL

TL;DR: 本研究通过构建土耳其语自杀意图语料库和创新的注释框架，分析了注释可靠性和模型一致性的挑战，同时在多语言环境下验证了模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前自杀意图检测受限于语言覆盖不足和注释质量问题，研究动机是提高多语言自杀意图检测的效率和可靠性。

Method: 提出一种资源高效的注释框架，包括三名人工标注员和两大语言模型（LLMs），构建土耳其语语料库，同时对多语言数据集进行双向评估，通过迁移学习使用预训练的八种情感分类器评估注释一致性与模型性能。

Result: 发现现有模型在零样本学习上的表现存在问题，并强调注释和多语言评估在心理健康自然语言处理中的重要性。

Conclusion: 建议在心理健康自然语言处理领域中更加注重数据和模型的可靠性，同时倡导数据集构建与模型训练的透明化。

Abstract: Suicidal ideation detection is critical for real-time suicide prevention, yet
its progress faces two under-explored challenges: limited language coverage and
unreliable annotation practices. Most available datasets are in English, but
even among these, high-quality, human-annotated data remains scarce. As a
result, many studies rely on available pre-labeled datasets without examining
their annotation process or label reliability. The lack of datasets in other
languages further limits the global realization of suicide prevention via
artificial intelligence (AI). In this study, we address one of these gaps by
constructing a novel Turkish suicidal ideation corpus derived from social media
posts and introducing a resource-efficient annotation framework involving three
human annotators and two large language models (LLMs). We then address the
remaining gaps by performing a bidirectional evaluation of label reliability
and model consistency across this dataset and three popular English suicidal
ideation detection datasets, using transfer learning through eight pre-trained
sentiment and emotion classifiers. These transformers help assess annotation
consistency and benchmark model performance against manually labeled data. Our
findings underscore the need for more rigorous, language-inclusive approaches
to annotation and evaluation in mental health natural language processing (NLP)
while demonstrating the questionable performance of popular models with
zero-shot transfer learning. We advocate for transparency in model training and
dataset construction in mental health NLP, prioritizing data and model
reliability.

</details>


### [171] [Disparities in Peer Review Tone and the Role of Reviewer Anonymity](https://arxiv.org/abs/2507.14741)
*Maria Sahakyan,Bedoor AlShebli*

Main category: cs.CL

TL;DR: 研究发现同行评审中的语言可能暗含偏见，并分析了评审语气、情感等如何因性别、种族及机构等因素不同而变化，同时探讨了匿名评审与署名评审对评审语言风格的影响。


<details>
  <summary>Details</summary>
Motivation: 探索同行评审中隐藏的语言偏见以及匿名性是否真正有助于公平性，同时为学术出版政策改革提供依据。

Method: 对超过80,000篇同行评审使用自然语言处理及大规模统计建模，从语气、情感和支持性语言等方面分析作者人口统计信息对评审内容的影响。

Result: 研究揭示了评审语言中存在的隐性偏见，并表明评审者身份披露会显著影响评审语言，从而挑战了匿名性增加公平性的传统观点。

Conclusion: 同行评审中的语言带有潜在偏见，这对学术出版政策及科研人员职业发展产生深远影响，需重新评估匿名性及语言在评审中的作用。

Abstract: The peer review process is often regarded as the gatekeeper of scientific
integrity, yet increasing evidence suggests that it is not immune to bias.
Although structural inequities in peer review have been widely debated, much
less attention has been paid to the subtle ways in which language itself may
reinforce disparities. This study undertakes one of the most comprehensive
linguistic analyses of peer review to date, examining more than 80,000 reviews
in two major journals. Using natural language processing and large-scale
statistical modeling, it uncovers how review tone, sentiment, and supportive
language vary across author demographics, including gender, race, and
institutional affiliation. Using a data set that includes both anonymous and
signed reviews, this research also reveals how the disclosure of reviewer
identity shapes the language of evaluation. The findings not only expose hidden
biases in peer feedback, but also challenge conventional assumptions about
anonymity's role in fairness. As academic publishing grapples with reform,
these insights raise critical questions about how review policies shape career
trajectories and scientific progress.

</details>


### [172] [On the robustness of modeling grounded word learning through a child's egocentric input](https://arxiv.org/abs/2507.14749)
*Wai Keen Vong,Brenden M. Lake*

Main category: cs.CL

TL;DR: 研究表明，基于儿童发育经验数据训练的多模态神经网络可以学习单词-参照映射，并验证了这种方法在多种网络架构中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探索机器学习模型在语言习得方面的作用，尤其是在数据量和质量与儿童语言输入相当的条件下的表现。

Method: 对SAYCam数据集中的500小时视频数据进行自动语音转录，生成多模态视觉和语言数据集，并测试了多种神经网络配置的学习能力。

Result: 网络能够基于每个儿童的转录数据学习并推广单词-参照映射，并且适用于多种网络架构。

Conclusion: 多模态神经网络在语言习得中的确表现出鲁棒性，同时也揭示了网络在不同儿童数据训练下的个体差异性。

Abstract: What insights can machine learning bring to understanding human language
acquisition? Large language and multimodal models have achieved remarkable
capabilities, but their reliance on massive training datasets creates a
fundamental mismatch with children, who succeed in acquiring language from
comparatively limited input. To help bridge this gap, researchers have
increasingly trained neural networks using data similar in quantity and quality
to children's input. Taking this approach to the limit, Vong et al. (2024)
showed that a multimodal neural network trained on 61 hours of visual and
linguistic input extracted from just one child's developmental experience could
acquire word-referent mappings. However, whether this approach's success
reflects the idiosyncrasies of a single child's experience, or whether it would
show consistent and robust learning patterns across multiple children's
experiences was not explored. In this article, we applied automated speech
transcription methods to the entirety of the SAYCam dataset, consisting of over
500 hours of video data spread across all three children. Using these automated
transcriptions, we generated multi-modal vision-and-language datasets for both
training and evaluation, and explored a range of neural network configurations
to examine the robustness of simulated word learning. Our findings demonstrate
that networks trained on automatically transcribed data from each child can
acquire and generalize word-referent mappings across multiple network
architectures. These results validate the robustness of multimodal neural
networks for grounded word learning, while highlighting the individual
differences that emerge in how models learn when trained on each child's
developmental experiences.

</details>


### [173] [GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization](https://arxiv.org/abs/2507.14758)
*Luyi Ma,Wanjia Zhang,Kai Zhao,Abhishek Kulkarni,Lalitesh Morishetti,Anjana Ganesh,Ashish Ranjan,Aashika Padmanabhan,Jianpeng Xu,Jason Cho,Praveen Kanumala,Kaushiki Nag,Sumit Dutta,Kamiya Motwani,Malay Patel,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.CL

TL;DR: 本文提出了一种名为GRACE的新型生成框架，用于多行为序列推荐，通过引入Chain-of-Thought（CoT）令牌化和Journey-Aware Sparse Attention（JSA）机制，实现了解释性和高效性的推荐，显著提升了效果并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 目前生成模型在多行为推荐系统中潜力巨大，但存在令牌推理欠缺信息、计算成本高以及缺乏多尺度建模的挑战。

Method: 提出了一种结合Chain-of-Thought（CoT）的混合令牌化方法，以用户与物品交互的数据及产品知识图谱的属性进行编码，并设计了Journey-Aware Sparse Attention（JSA）机制，以高效聚焦压缩段落及当前上下文信息。

Result: 在两个真实数据集上的实验结果显示，GRACE在某些领域的HR@10和NDCG@10指标上分别提升了超过106%和22%，并减少了长序列的注意力计算达48%。

Conclusion: 引入CoT令牌化和JSA机制的GRACE框架能够有效解决现存挑战，提升推荐性能的同时降低了计算成本。

Abstract: Generative models have recently demonstrated strong potential in
multi-behavior recommendation systems, leveraging the expressive power of
transformers and tokenization to generate personalized item sequences. However,
their adoption is hindered by (1) the lack of explicit information for token
reasoning, (2) high computational costs due to quadratic attention complexity
and dense sequence representations after tokenization, and (3) limited
multi-scale modeling over user history. In this work, we propose GRACE
(Generative Recommendation via journey-aware sparse Attention on
Chain-of-thought tokEnization), a novel generative framework for multi-behavior
sequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT)
tokenization method that encodes user-item interactions with explicit
attributes from product knowledge graphs (e.g., category, brand, price) over
semantic tokenization, enabling interpretable and behavior-aligned generation.
To address the inefficiency of standard attention, we design a Journey-Aware
Sparse Attention (JSA) mechanism, which selectively attends to compressed,
intra-, inter-, and current-context segments in the tokenized sequence.
Experiments on two real-world datasets show that GRACE significantly
outperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and
+106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home
domain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces
attention computation by up to 48% with long sequences.

</details>


### [174] [FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing](https://arxiv.org/abs/2507.14815)
*Shoutao Guo,Shaolei Zhang,Qingkai Fang,Zhengrui Ma,Min Zhang,Yang Feng*

Main category: cs.CL

TL;DR: 本文提出了一种名为FastLongSpeech的新框架，通过不用专门的长语音训练数据，实现高效处理长语音的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语音语言模型对于长语音处理的研究较少，主要因为缺乏长语音训练数据和高计算成本。

Method: 采用迭代融合策略将长语音压缩到易处理的长度，同时通过动态压缩训练方法使模型适应不同压缩比的短语音序列。

Result: 实验表明，FastLongSpeech在长语音和短语音任务上都表现优异，同时显著提高推理效率。

Conclusion: 该框架无需专用长语音数据，大幅度提升了长语音处理能力和效率，为LSLM的应用拓展提供了可能性。

Abstract: The rapid advancement of Large Language Models (LLMs) has spurred significant
progress in Large Speech-Language Models (LSLMs), enhancing their capabilities
in both speech understanding and generation. While existing LSLMs often
concentrate on augmenting speech generation or tackling a diverse array of
short-speech tasks, the efficient processing of long-form speech remains a
critical yet underexplored challenge. This gap is primarily attributed to the
scarcity of long-speech training datasets and the high computational costs
associated with long sequences. To address these limitations, we introduce
FastLongSpeech, a novel framework designed to extend LSLM capabilities for
efficient long-speech processing without necessitating dedicated long-speech
training data. FastLongSpeech incorporates an iterative fusion strategy that
can compress excessively long-speech sequences into manageable lengths. To
adapt LSLMs for long-speech inputs, it introduces a dynamic compression
training approach, which exposes the model to short-speech sequences at varying
compression ratios, thereby transferring the capabilities of LSLMs to
long-speech tasks. To assess the long-speech capabilities of LSLMs, we develop
a long-speech understanding benchmark called LongSpeech-Eval. Experiments show
that our method exhibits strong performance in both long-speech and
short-speech tasks, while greatly improving inference efficiency.

</details>


### [175] [Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents](https://arxiv.org/abs/2507.14819)
*Akriti Jain,Pritika Ramu,Aparna Garimella,Apoorv Saxena*

Main category: cs.CL

TL;DR: 本文提出了一种从长文档中生成基于用户意图的图表的新任务，并提出了一个两阶段的无监督框架来解决该问题，同时引入了新的数据集和度量指标。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以处理长文档用户意图驱动的图表生成，需要新的方法解决文档数据提取和图表生成的综合问题。

Method: 提出一个两阶段框架，首先通过LLM按用户意图提取相关信息，然后通过启发式模块选择合适的图表类型并生成代码。同时提出基于归因的评估指标来衡量生成图表的数据准确性。

Result: 通过新构建的数据集验证，方法在图表数据准确性和图表类型上较现有基线分别提升了9和17个百分比。

Conclusion: 提出的方法在用户意图驱动的图表生成任务中表现优越，解决了传统方法对长文档处理的局限，并验证了框架的有效性。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
transforming text descriptions or tables to data visualizations via
instruction-tuning methods. However, it is not straightforward to apply these
methods directly for a more real-world use case of visualizing data from long
documents based on user-given intents, as opposed to the user pre-selecting the
relevant content manually. We introduce the task of intent-based chart
generation from documents: given a user-specified intent and document(s), the
goal is to generate a chart adhering to the intent and grounded on the
document(s) in a zero-shot setting. We propose an unsupervised, two-staged
framework in which an LLM first extracts relevant information from the
document(s) by decomposing the intent and iteratively validates and refines
this data. Next, a heuristic-guided module selects an appropriate chart type
before final code generation. To assess the data accuracy of the generated
charts, we propose an attribution-based metric that uses a structured textual
representation of charts, instead of relying on visual decoding metrics that
often fail to capture the chart data effectively. To validate our approach, we
curate a dataset comprising of 1,242 $<$intent, document, charts$>$ tuples from
two domains, finance and scientific, in contrast to the existing datasets that
are largely limited to parallel text descriptions/ tables and their
corresponding charts. We compare our approach with baselines using single-shot
chart generation using LLMs and query-based retrieval methods; our method
outperforms by upto $9$ points and $17$ points in terms of chart data accuracy
and chart type respectively over the best baselines.

</details>


### [176] [Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding](https://arxiv.org/abs/2507.14849)
*Yifei Wang*

Main category: cs.CL

TL;DR: 本文探讨了通过推理蒸馏提升小型语言模型长文本推理和上下文理解能力的方法及其效果。实验表明，推理蒸馏显著改善了长文本理解并解决了“中间丢失”问题。


<details>
  <summary>Details</summary>
Motivation: 理解大规模推理蒸馏对关键能力（特别是上下文信息检索和推理）的影响，尤其是在RAG系统中优化生成可靠响应的需求下。

Method: 利用由Deepseek-R1蒸馏而来的开源模型进行实验，研究其在多文档问答任务中的表现，以评估大规模蒸馏对长文本理解的影响。

Result: 实验结果表明，推理蒸馏显著增强了模型的长文本理解能力，促进了更详细和明确的上下文分析及信息解析。

Conclusion: 推理蒸馏能够有效解决长文本模型中存在的“中间丢失”问题，强化长文本上下文的理解与分析。

Abstract: Reasoning distillation has emerged as an effective approach to enhance the
reasoning capabilities of smaller language models. However, the impact of
large-scale reasoning distillation on other critical abilities, particularly
in-context retrieval and reasoning, remains unexplored. This gap in
understanding is particularly significant given the increasing importance of
Retrieval-Augmented Generation (RAG) systems, where efficient acquisition and
utilization of contextual information are paramount for generating reliable
responses. Motivated by the need to understand how the extended long-CoT
process influences long-context comprehension, we conduct a comprehensive
investigation using a series of open-source models distilled from Deepseek-R1,
renowned for its exceptional reasoning capabilities. Our study focuses on
evaluating these models' performance in extracting and integrating relevant
information from extended contexts through multi-document question and
answering tasks. Through rigorous experimentation, we demonstrate that
distilled reasoning patterns significantly improve long-context understanding.
Our analysis reveals that distillation fosters greater long-context awareness
by promoting more detailed and explicit reasoning processes during context
analysis and information parsing. This advancement effectively mitigates the
persistent "lost in the middle" issue that has hindered long-context models.

</details>


### [177] [Tiny language models](https://arxiv.org/abs/2507.14871)
*Ronit D. Gross,Yarden Tzach,Tal Halevi,Ella Koresh,Ido Kanter*

Main category: cs.CL

TL;DR: 该研究探讨了小型语言模型(TLMs)是否可以表现出大型语言模型(LLMs)的核心特性。结果显示TLM在分类任务中展现了清晰的预训练与非预训练性能差距，并分析了数据集规模和词汇重叠对性能的影响。此外，研究通过多个浅层模型的组合，实现了接近深层TLM架构的性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的预训练需耗费巨大计算资源，限制了广泛研究。因此，该研究旨在探索是否可以通过计算资源更低的小型模型(TLMs)，实现LLM的核心功能。

Method: 研究基于BERT-6和BERT-1的变体，在Wikipedia子集上进行预训练，并在FewRel、AGNews和DBPedia分类任务中评估其性能，同时比较深层TLM架构和浅层模型软委员会的表现。

Result: TLM在分类任务中证明了预训练的有效性，并且较大的预训练数据集规模和更高的词汇重叠度增强了性能。此外，可通过多个预训练浅层模型的软委员会复现深层TLM的分类准确性。

Conclusion: 小型语言模型(TLMs)也能展示预训练的重要性，且在性能和资源效率之间达成平衡。未来研究有望揭示其在自然语言处理中的深度机理，尤其在人类语言发展建模中的应用潜力。

Abstract: A prominent achievement of natural language processing (NLP) is its ability
to understand and generate meaningful human language. This capability relies on
complex feedforward transformer block architectures pre-trained on large
language models (LLMs). However, LLM pre-training is currently feasible only
for a few dominant companies due to the immense computational resources
required, limiting broader research participation. This creates a critical need
for more accessible alternatives. In this study, we explore whether tiny
language models (TLMs) exhibit the same key qualitative features of LLMs. We
demonstrate that TLMs exhibit a clear performance gap between pre-trained and
non-pre-trained models across classification tasks, indicating the
effectiveness of pre-training, even at a tiny scale. The performance gap
increases with the size of the pre-training dataset and with greater overlap
between tokens in the pre-training and classification datasets. Furthermore,
the classification accuracy achieved by a pre-trained deep TLM architecture can
be replicated through a soft committee of multiple, independently pre-trained
shallow architectures, enabling low-latency TLMs without affecting
classification accuracy. Our results are based on pre-training BERT-6 and
variants of BERT-1 on subsets of the Wikipedia dataset and evaluating their
performance on FewRel, AGNews, and DBPedia classification tasks. Future
research on TLM is expected to further illuminate the mechanisms underlying
NLP, especially given that its biologically inspired models suggest that TLMs
may be sufficient for children or adolescents to develop language.

</details>


### [178] [MEKiT: Multi-source Heterogeneous Knowledge Injection Method via Instruction Tuning for Emotion-Cause Pair Extraction](https://arxiv.org/abs/2507.14887)
*Shiyi Mu,Yongkang Liu,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.CL

TL;DR: 提出一种名为MEKiT的方法，通过注入多源异构知识显著提升大语言模型在情感-原因对抽取任务（ECPE）中的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在ECPE任务中的表现不如一些小型语言模型，主要原因是缺乏辅助知识，限制了对情感的感知和因果推理能力。

Method: 提出MEKiT方法，集成内部情感知识和外部因果知识，分别通过指令模版和数据混合指令微调的方法注入模型。

Result: 实验结果表明，MEKiT显著提升大语言模型在ECPE任务中的表现，相比其他基线算法具有绝对性能优势。

Conclusion: MEKiT为解决ECPE任务提供了更加高效和适应性强的方案，增强了大语言模型的情感识别和因果推理能力。

Abstract: Although large language models (LLMs) excel in text comprehension and
generation, their performance on the Emotion-Cause Pair Extraction (ECPE) task,
which requires reasoning ability, is often underperform smaller language model.
The main reason is the lack of auxiliary knowledge, which limits LLMs' ability
to effectively perceive emotions and reason causes. To address this issue, we
propose a novel \textbf{M}ulti-source h\textbf{E}terogeneous \textbf{K}nowledge
\textbf{i}njection me\textbf{T}hod, MEKiT, which integrates heterogeneous
internal emotional knowledge and external causal knowledge. Specifically, for
these two distinct aspects and structures of knowledge, we apply the approaches
of incorporating instruction templates and mixing data for instruction-tuning,
which respectively facilitate LLMs in more comprehensively identifying emotion
and accurately reasoning causes. Experimental results demonstrate that MEKiT
provides a more effective and adaptable solution for the ECPE task, exhibiting
an absolute performance advantage over compared baselines and dramatically
improving the performance of LLMs on the ECPE task.

</details>


### [179] [Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs](https://arxiv.org/abs/2507.14894)
*Boyi Deng,Yu Wan,Baosong Yang,Fei Huang,Wenjie Wang,Fuli Feng*

Main category: cs.CL

TL;DR: 本研究提出了一种名为SASFT的方法，通过控制语言特征的预激活值，显著减少大语言模型的意外代码切换问题，同时保持或提升模型的多语言能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在多语言处理上表现优秀，但会出现意外的代码切换问题，导致可读性下降，使用体验变差，而现有研究对其分析不够深入且效果有限。

Method: 利用稀疏自编码器对代码切换现象进行深入分析，并提出SASFT方法，通过监督微调训练控制特定语言特征的预激活值。

Result: 实验表明，SASFT方法在五种模型的三种语言场景中均能将意外代码切换减少超过50%，并在四种情况下完全消除，且在六个多语言基准的性能保持或提升。

Conclusion: SASFT方法能够有效解决大语言模型的意外代码切换问题，同时维护或增强其多语言能力，展现出广泛的适用效果。

Abstract: Large Language Models (LLMs) have impressive multilingual capabilities, but
they suffer from unexpected code-switching, also known as language mixing,
which involves switching to unexpected languages in the model response. This
problem leads to poor readability and degrades the usability of model
responses. However, existing work on this issue lacks a mechanistic analysis
and shows limited effectiveness. In this paper, we first provide an in-depth
analysis of unexpected code-switching using sparse autoencoders and find that
when LLMs switch to a language, the features of that language exhibit excessive
pre-activation values. Based on our findings, we propose $\textbf{S}$parse
$\textbf{A}$utoencoder-guided $\textbf{S}$upervised
$\textbf{F}$ine$\textbf{t}$uning (SASFT), which teaches LLMs to maintain
appropriate pre-activation values of specific language features during
training. Experiments on five models across three languages demonstrate that
SASFT consistently reduces unexpected code-switching by more than 50\% compared
to standard supervised fine-tuning, with complete elimination in four cases.
Moreover, SASFT maintains or even improves the models' performance on six
multilingual benchmarks, showing its effectiveness in addressing code-switching
while preserving multilingual capabilities.

</details>


### [180] [From Neurons to Semantics: Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment](https://arxiv.org/abs/2507.14900)
*Chongxuan Huang,Yongshi Ye,Biao Fu,Qifeng Su,Xiaodong Shi*

Main category: cs.CL

TL;DR: 文章提出了一种新的方法，用于评估大型语言模型 (LLMs) 在多语言情况下的跨语言对齐能力，其结果表明，该方法即使在小数据集下也表现出较高的评估效率。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法主要针对句子嵌入，未能很好地评估低资源语言的语义对齐，而基于神经科学研究提出了更具语义基础的评估方式。

Method: 提出了Neuron State-Based Cross-Lingual Alignment (NeuronXA) 方法，基于神经元状态评估LLMs的跨语言对齐能力，并在有限的平行语料对上与现有LLMs进行了评估。

Result: 在两个迁移任务和三个多语言基准上，通过仅100对平行句子，NeuronXA 与下游任务表现的皮尔逊相关系数为0.9556，与迁移性相关系数为0.8514，展现了其显著的评估能力。

Conclusion: NeuronXA 方法在评估多语言LLMs的跨语言对齐与迁移能力上表现出色，即使在小数据集下，也证实了其有效性和未来研究的潜力。

Abstract: Large language models (LLMs) have demonstrated remarkable multilingual
capabilities, however, how to evaluate cross-lingual alignment remains
underexplored. Existing alignment benchmarks primarily focus on sentence
embeddings, but prior research has shown that neural models tend to induce a
non-smooth representation space, which impact of semantic alignment evaluation
on low-resource languages. Inspired by neuroscientific findings that similar
information activates overlapping neuronal regions, we propose a novel Neuron
State-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a
lignment capabilities of LLMs, which offers a more semantically grounded
approach to assess cross-lingual alignment. We evaluate NeuronXA on several
prominent multilingual LLMs (LLaMA, Qwen, Mistral, GLM, and OLMo) across two
transfer tasks and three multilingual benchmarks. The results demonstrate that
with only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation
of 0.9556 with downstream tasks performance and 0.8514 with transferability.
These findings demonstrate NeuronXA's effectiveness in assessing both
cross-lingual alignment and transferability, even with a small dataset. This
highlights its potential to advance cross-lingual alignment research and to
improve the semantic understanding of multilingual LLMs.

</details>


### [181] [PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation](https://arxiv.org/abs/2507.14913)
*Eliya Habba,Noam Dahan,Gili Lior,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: 单一提示的评估方式不可靠，PromptSuite框架通过自动生成多样化的提示，改进了多提示评估方法的适用性。


<details>
  <summary>Details</summary>
Motivation: 为解决单一提示评估不稳定的问题，提供一种能够生成多样提示的工具。

Method: 设计了模块化的提示生成框架PromptSuite，可自动生成多种任务的提示变体并支持扩展。

Result: 通过案例研究验证了PromptSuite生成的变体可支持强有力的评估实践。

Conclusion: PromptSuite是一个灵活且可扩展的提示生成工具，提高了多提示评估的可靠性及实用性。

Abstract: Evaluating LLMs with a single prompt has proven unreliable, with small
changes leading to significant performance differences. However, generating the
prompt variations needed for a more robust multi-prompt evaluation is
challenging, limiting its adoption in practice. To address this, we introduce
PromptSuite, a framework that enables the automatic generation of various
prompts. PromptSuite is flexible - working out of the box on a wide range of
tasks and benchmarks. It follows a modular prompt design, allowing controlled
perturbations to each component, and is extensible, supporting the addition of
new components and perturbation types. Through a series of case studies, we
show that PromptSuite provides meaningful variations to support strong
evaluation practices. It is available through both a Python API:
https://github.com/eliyahabba/PromptSuite, and a user-friendly web interface:
https://promptsuite.streamlit.app/

</details>


### [182] [SYNTHIA: Synthetic Yet Naturally Tailored Human-Inspired PersonAs](https://arxiv.org/abs/2507.14922)
*Vahid Rahimzadeh,Erfan Moosavi Monazzah,Mohammad Taher Pilehvar,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: SYNTHIA 是一个基于真实社交媒体活动生成的30,000个故事数据集，兼具真实性和一致性，具有时代动态性和丰富的社交互动元数据。


<details>
  <summary>Details</summary>
Motivation: 现有的人物驱动大语言模型在真实性与一致性之间难以兼顾，且方法要么昂贵要么缺乏可靠性。

Method: 通过从BlueSky开放平台的10,000个真实社交媒体用户活动中提取数据，生成了30,000个基于真实活动的故事数据集。

Result: 相比现有方法，SYNTHIA 在人口多样性、社会调查对齐和叙事一致性上表现优越，并且独特地包含时间维度和社交互动元数据。

Conclusion: SYNTHIA 数据集为计算社会科学和人物驱动语言建模开辟了新方向，兼具真实性和一致性，且提供不可多得的时间和社交元数据信息。

Abstract: Persona-driven LLMs have emerged as powerful tools in computational social
science, yet existing approaches fall at opposite extremes, either relying on
costly human-curated data or producing synthetic personas that lack consistency
and realism. We introduce SYNTHIA, a dataset of 30,000 backstories derived from
10,000 real social media users from BlueSky open platform across three time
windows, bridging this spectrum by grounding synthetic generation in authentic
user activity. Our evaluation demonstrates that SYNTHIA achieves competitive
performance with state-of-the-art methods in demographic diversity and social
survey alignment while significantly outperforming them in narrative
consistency. Uniquely, SYNTHIA incorporates temporal dimensionality and
provides rich social interaction metadata from the underlying network, enabling
new research directions in computational social science and persona-driven
language modeling.

</details>


### [183] [MUR: Momentum Uncertainty guided Reasoning for Large Language Models](https://arxiv.org/abs/2507.14958)
*Hang Yan,Fangzhi Xu,Rongman Xu,Yifei Li,Jian Zhang,Haoran Luo,Xiaobao Wu,Luu Anh Tuan,Haiteng Zhao,Qika Lin,Jun Liu*

Main category: cs.CL

TL;DR: 提出了一种名为MUR的方法，在不需额外训练的情况下，通过动量引导提高推理效率，在减少计算的同时提升LLM推理准确性。


<details>
  <summary>Details</summary>
Motivation: 在LLM推理任务中，提升推理效率仍是一个挑战，而当前的测试时放大方法通常会造成过度推理及资源浪费。

Method: 受物理学中动量概念的启发，通过跟踪和聚合逐步的不确定性，引入MUR方法以动态分配推理资源，并通过一个调节超参数的gamma-control机制提供灵活的推理时间控制。

Result: 在四个基准测试上（MATH-500、AIME24、AIME25、GPQA-diamond）使用不同大小的Qwen3模型评估，MUR在减少约50%的计算量的同时，准确率提高了0.62%至3.37%。

Conclusion: MUR方法证明了在稳定性和偏差方面的优越性，是一种有效提升LLM推理效率的方法，兼顾了计算资源和推理性能的平衡。

Abstract: Large Language Models (LLMs) have achieved impressive performance on
reasoning-intensive tasks, yet optimizing their reasoning efficiency remains an
open challenge. While Test-Time Scaling (TTS) improves reasoning quality, it
often leads to overthinking, wasting tokens on redundant computations. This
work investigates how to efficiently and adaptively guide LLM test-time scaling
without additional training. Inspired by the concept of momentum in physics, we
propose Momentum Uncertainty-guided Reasoning (MUR), which dynamically
allocates thinking budgets to critical reasoning steps by tracking and
aggregating stepwise uncertainty over time. To support flexible inference-time
control, we introduce gamma-control, a simple mechanism that tunes the
reasoning budget via a single hyperparameter. We provide in-depth theoretical
proof to support the superiority of MUR in terms of stability and biases. MUR
is comprehensively evaluated against various TTS methods across four
challenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using
different sizes of recent Qwen3 models (1.7B, 4B, and 8B). Results demonstrate
that MUR reduces computation by over 50% on average while improving accuracy by
0.62-3.37%.

</details>


### [184] [RefCritic: Training Long Chain-of-Thought Critic Models with Refinement Feedback](https://arxiv.org/abs/2507.15024)
*Qiaoyu Tang,Hao Xiang,Le Yu,Bowen Yu,Hongyu Lin,Yaojie Lu,Xianpei Han,Le Sun,Junyang Lin*

Main category: cs.CL

TL;DR: 提出RefCritic，基于强化学习和双规则奖励的长链式批评模块，用于提高语言模型的评估与反馈能力，显著提升多个基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前监督微调方法难以真正提升批评能力，无法提供深入反思和验证。

Method: 开发RefCritic，结合强化学习及双规则奖励机制，目标是通过生成高质量的评估与可执行反馈推动模型改进。

Result: RefCritic在多个基准上表现出色，例如在AIME25基准上的改进分别为6.8%和7.2%。此外，在ProcessBench上也优于步骤级监督方法。

Conclusion: RefCritic通过强化学习和批评反馈机制，成功推动了语言模型批评能力的提升，有助于进一步改善模型性能。

Abstract: With the rapid advancement of Large Language Models (LLMs), developing
effective critic modules for precise guidance has become crucial yet
challenging. In this paper, we initially demonstrate that supervised
fine-tuning for building critic modules (which is widely adopted in current
solutions) fails to genuinely enhance models' critique abilities, producing
superficial critiques with insufficient reflections and verifications. To
unlock the unprecedented critique capabilities, we propose RefCritic, a
long-chain-of-thought critic module based on reinforcement learning with dual
rule-based rewards: (1) instance-level correctness of solution judgments and
(2) refinement accuracies of the policy model based on critiques, aiming to
generate high-quality evaluations with actionable feedback that effectively
guides model refinement. We evaluate RefCritic on Qwen2.5-14B-Instruct and
DeepSeek-R1-Distill-Qwen-14B across five benchmarks. On critique and refinement
settings, RefCritic demonstrates consistent advantages across all benchmarks,
e.g., 6.8\% and 7.2\% gains on AIME25 for the respective base models. Notably,
under majority voting, policy models filtered by RefCritic show superior
scaling with increased voting numbers. Moreover, despite training on
solution-level supervision, RefCritic outperforms step-level supervised
approaches on ProcessBench, a benchmark to identify erroneous steps in
mathematical reasoning.

</details>


### [185] [WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization](https://arxiv.org/abs/2507.15061)
*Zhengwei Tao,Jialong Wu,Wenbiao Yin,Junkai Zhang,Baixuan Li,Haiyang Shen,Kuan Li,Liwen Zhang,Xinyu Wang,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 提出了WebShaper框架，用于构建信息查询任务的数据集，解决了现有方法中信息结构与推理结构可能不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有信息查询代理由于高质量训练数据的稀缺性，在信息结构与推理结构一致性方面存在不足。

Method: 提出了一种基于集合论的形式化数据构建框架WebShaper，通过知识投影(KP)的操作，实现推理结构的精准控制，并结合多步扩展生成复杂问题进行数据合成。

Result: WebShaper在GAIA和WebWalkerQA基准测试中达到当前最优表现。

Conclusion: WebShaper框架有效解决了信息结构与推理结构的匹配问题，为开源信息查询代理的性能提升提供了支持。

Abstract: The advent of Large Language Model (LLM)-powered agents has revolutionized
artificial intelligence by enabling solutions to complex, open-ended tasks
through web-based information-seeking (IS) capabilities. The scarcity of
high-quality training data has limited the development of IS agents. Existing
approaches typically adopt an information-driven paradigm that first collects
web data and then generates questions based on the retrieval. However, this may
lead to inconsistency between information structure and reasoning structure,
question and answer. To mitigate, we propose a formalization-driven IS data
synthesis framework WebShaper to construct a dataset. WebShaper systematically
formalizes IS tasks through set theory. Central to the formalization is the
concept of Knowledge Projections (KP), which enables precise control over
reasoning structure by KP operation compositions. During synthesis, we begin by
creating seed tasks, then use a multi-step expansion process. At each step, an
agentic Expander expands the current formal question more complex with
retrieval and validation tools based on our formalization. We train our model
on the synthesized dataset. Experiment results demonstrate that WebShaper
achieves state-of-the-art performance among open-sourced IS agents on GAIA and
WebWalkerQA benchmarks.

</details>


### [186] [Evaluation of Coding Schemes for Transformer-based Gene Sequence Modeling](https://arxiv.org/abs/2507.15087)
*Chenlei Gong,Yuanhe Tian,Lei Mao,Yan Song*

Main category: cs.CL

TL;DR: 本文对DNA序列中的k-mer分割和BPE子词标记化方法进行了系统比较，并评估了不同位置编码和Transformer层深对任务性能的影响。


<details>
  <summary>Details</summary>
Motivation: 研究目前缺乏对k-mer分割和BPE标记化方法优劣的系统评估，并探讨DNA序列Transformer模块优化的最佳实践。

Method: 本文对k-mer分割（k=1,3,4,5,6）和BPE方法（4096词汇表），以及三种位置编码方法（sinusoidal, AliBi, RoPE）进行了实验评估，并在不同深度Transformer（3, 6, 12, 24层）上测试其性能。

Result: BPE方法通过将频繁的motif压缩成可变长度Token，可减少序列长度、提升泛化能力，性能更稳定。RoPE在捕捉周期motif和处理长序列上优势明显，而AliBi在局部依赖驱动的任务上表现良好。模型深度从3层增加到12层效果显著，但24层时性能改善有限甚至略有过拟合。

Conclusion: 研究从标记化、位置编码到模型深度提供了DNA Transformer设计的实用指南，以提升模型性能。

Abstract: Currently, many studies view DNA sequences as a special type of language and
utilize Transformers to model them. These studies use fixed-length k-mer
segmentation and BPE subword tokenization but lack a systematic evaluation to
determine which is superior. We compare k-mer segmentation with k=1,3,4,5,6, a
4,096-token BPE vocabulary, and three positional encoding methods-sinusoidal,
AliBi, and RoPE. Each configuration is trained from scratch in 3, 6, 12, and
24-layer Transformer encoders and evaluated on GUE benchmark dataset. In
general, BPE delivers higher and more stable performance across tasks by
compressing frequent motifs into variable-length tokens, reducing sequence
length, and improving model generalization. RoPE excels at capturing periodic
motifs and extrapolating to long sequences, while AliBi also performs well on
tasks driven by local dependencies. In terms of depth, we observe significant
gains when increasing layers from 3 to 12, with only marginal improvements or
slight overfitting at 24 layers. This study provides practical guidance for
designing tokenization and positional encoding in DNA Transformer models.

</details>


### [187] [A Penalty Goes a Long Way: Measuring Lexical Diversity in Synthetic Texts Under Prompt-Influenced Length Variations](https://arxiv.org/abs/2507.15092)
*Vijeta Deshpande,Ishita Dasgupta,Uttaran Bhattacharya,Somdeb Sarkhel,Saayan Mitra,Anna Rumshisky*

Main category: cs.CL

TL;DR: 文章探讨了生成式语言模型生成的合成文本的多样性问题，提出了一种新的多样性指标PATTR，以及其在创意写作任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法很依赖prompt工程提升合成数据多样性，但对于prompt变化对文本长度和词汇多样性影响的研究不足。

Method: 作者提出了一种长度鲁棒的多样性衡量指标PATTR，并利用7种模型生成合成语料进行比较分析，验证其对特定任务目标长度的调节能力。

Result: PATTR能有效缓解文本长度差异引入的偏差，并在筛选高词汇多样性文本时优于现有指标（MATTR和CR）。

Conclusion: PATTR作为一个新的多样性计算方法，不仅提高了多样性测量的准确性，还能更贴合任务需求，具有实际应用价值。

Abstract: Synthetic text generated by Large Language Models (LLMs) is increasingly used
for further training and improvement of LLMs. Diversity is crucial for the
effectiveness of synthetic data, and researchers rely on prompt engineering to
improve diversity. However, the impact of prompt variations on response text
length, and, more importantly, the consequential effect on lexical diversity
measurements, remain underexplored. In this work, we propose Penalty-Adjusted
Type-Token Ratio (PATTR), a diversity metric robust to length variations. We
generate a large synthetic corpus of over 20M words using seven models from the
LLaMA, OLMo, and Phi families, focusing on a creative writing task of video
script generation, where diversity is crucial. We evaluate per-response lexical
diversity using PATTR and compare it against existing metrics of Moving-Average
TTR (MATTR) and Compression Ratio (CR). Our analysis highlights how text length
variations introduce biases favoring shorter responses. Unlike existing
metrics, PATTR explicitly considers the task-specific target response length
($L_T$) to effectively mitigate length biases. We further demonstrate the
utility of PATTR in filtering the top-10/100/1,000 most lexically diverse
responses, showing that it consistently outperforms MATTR and CR by yielding on
par or better diversity with high adherence to $L_T$.

</details>


### [188] [Filling the Gap: Is Commonsense Knowledge Generation useful for Natural Language Inference?](https://arxiv.org/abs/2507.15100)
*Chathuri Jayaweera,Brianna Yanqui,Bonnie Dorr*

Main category: cs.CL

TL;DR: 本论文探讨了大语言模型（LLMs）作为常识知识生成器在自然语言推理（NLI）中的潜力，发现其生成的知识对区分不同推理实例具有一定帮助。


<details>
  <summary>Details</summary>
Motivation: 现有的常识知识资源覆盖面不足，限制了自然语言推理任务的发展，因此需要探索新的常识知识生成方法。

Method: 评估了大语言模型（LLMs）在生成常识知识方面的可靠性，并适配现有指标来分析生成知识的事实性与一致性，以及其对预测准确性的影响。

Result: 虽然显式结合常识知识并未始终提升整体结果，但对区分蕴含实例较为有效，并对区分矛盾和中立推理有一定改进作用。

Conclusion: 大语言模型可以作为常识知识生成工具，在自然语言推理任务中具有潜在价值，但仍需进一步改进以提升其一致性和广泛适用性。

Abstract: Natural Language Inference (NLI) is the task of determining the semantic
entailment of a premise for a given hypothesis. The task aims to develop
systems that emulate natural human inferential processes where commonsense
knowledge plays a major role. However, existing commonsense resources lack
sufficient coverage for a variety of premise-hypothesis pairs. This study
explores the potential of Large Language Models as commonsense knowledge
generators for NLI along two key dimensions: their reliability in generating
such knowledge and the impact of that knowledge on prediction accuracy. We
adapt and modify existing metrics to assess LLM factuality and consistency in
generating in this context. While explicitly incorporating commonsense
knowledge does not consistently improve overall results, it effectively helps
distinguish entailing instances and moderately improves distinguishing
contradictory and neutral inferences.

</details>


### [189] [From Disagreement to Understanding: The Case for Ambiguity Detection in NLI](https://arxiv.org/abs/2507.15114)
*Chathuri Jayaweera,Bonnie Dorr*

Main category: cs.CL

TL;DR: 本文认为自然语言推理中的标注分歧并非简单噪音，而是反映了有意义的解释性差异，尤其在前提或假设模糊时。提出通过分类和检测模糊性来改进相关方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决自然语言推理中标注分歧的本质问题，作者提出辨别模糊性和分析其类型，以更好地对齐模型与人类理解方式。

Method: 提出一个整合现有分类框架的新体系，通过具体实例说明模糊性的类型及其对标注决策的影响，并建议创建新的模糊性标注资源及无监督检测方法。

Result: 通过具体示例展示模糊性如何影响标注决策，并提出改进方向以促进更可靠的自然语言推理系统。

Conclusion: 模糊性并非噪音，应被系统性识别和分类以提升自然语言推理系统的人类对齐性与可解释性。

Abstract: This position paper argues that annotation disagreement in Natural Language
Inference (NLI) is not mere noise but often reflects meaningful interpretive
variation, especially when triggered by ambiguity in the premise or hypothesis.
While underspecified guidelines and annotator behavior can contribute to
variation, content-based ambiguity offers a process-independent signal of
divergent human perspectives. We call for a shift toward ambiguity-aware NLI by
systematically identifying ambiguous input pairs and classifying ambiguity
types. To support this, we present a unified framework that integrates existing
taxonomies and illustrate key ambiguity subtypes through concrete examples.
These examples reveal how ambiguity shapes annotator decisions and motivate the
need for targeted detection methods that better align models with human
interpretation. A key limitation is the lack of datasets annotated for
ambiguity and subtypes. We propose addressing this gap through new annotated
resources and unsupervised approaches to ambiguity detection -- paving the way
for more robust, explainable, and human-aligned NLI systems.

</details>


### [190] [A Case Against Implicit Standards: Homophone Normalization in Machine Translation for Languages that use the Ge'ez Script](https://arxiv.org/abs/2507.15142)
*Hellina Hailu Nigatu,Atnafu Lambebo Tonja,Henok Biadglign Ademtew,Hizkel Mitiku Alemayehu,Negasi Haile Abadi,Tadesse Destaw Belay,Seid Muhie Yimam*

Main category: cs.CL

TL;DR: 研究分析Amharic自然语言处理中同音字标准化的影响，并提出了一种基于预测结果的标准化方法。


<details>
  <summary>Details</summary>
Motivation: 探讨同音字标准化对语言理解和迁移学习的影响，推动对语言变革技术的讨论。

Method: 使用单语训练和跨语言迁移实验研究标准化影响，提出预测结果后的标准化处理方法。

Result: 通过后处理标准化方法，BLEU分数增加了最多1.03，同时保留了语言特性。

Conclusion: 后处理标准化可以提高性能，同时保留语言特性，促进更语言敏感的技术发展。

Abstract: Homophone normalization, where characters that have the same sound in a
writing script are mapped to one character, is a pre-processing step applied in
Amharic Natural Language Processing (NLP) literature. While this may improve
performance reported by automatic metrics, it also results in models that are
not able to understand different forms of writing in a single language.
Further, there might be impacts in transfer learning, where models trained on
normalized data do not generalize well to other languages. In this paper, we
experiment with monolingual training and cross-lingual transfer to understand
the impacts of normalization on languages that use the Ge'ez script. We then
propose a post-inference intervention in which normalization is applied to
model predictions instead of training data. With our simple scheme of
post-inference normalization, we show that we can achieve an increase in BLEU
score of up to 1.03 while preserving language features in training. Our work
contributes to the broader discussion on technology-facilitated language change
and calls for more language-aware interventions.

</details>


### [191] [What Level of Automation is "Good Enough"? A Benchmark of Large Language Models for Meta-Analysis Data Extraction](https://arxiv.org/abs/2507.15152)
*Lingbo Li,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: 研究评估了三种LLMs（Gemini-2.0-flash、Grok-3、GPT-4o-mini）在从RCT全文本中自动提取数据用于荟萃分析的性能，发现定制化提示最有效提升了召回率，并提出了三层次的指导方针来平衡自动化和专家监督。


<details>
  <summary>Details</summary>
Motivation: 在荟萃分析中从随机对照试验（RCTs）全文中提取数据是一个挑战，研究希望评估大语言模型（LLMs）在自动化数据提取中的潜力并优化其表现。

Method: 测试三种LLMs在统计结果、偏倚风险评估和研究特征提取中的性能，涉及高血压、糖尿病和骨科领域，并采用四种提示策略（基本提示、反思提示、模型集成、定制化提示）分析性能表现。

Result: 所有模型精度高但召回率不足；定制化提示能提高召回率最多达15%。

Conclusion: 提出三层次指导方针，匹配数据类型与任务复杂性及风险来优化自动化水平，为在实际荟萃分析中应用LLM提供了实用建议。

Abstract: Automating data extraction from full-text randomised controlled trials (RCTs)
for meta-analysis remains a significant challenge. This study evaluates the
practical performance of three LLMs (Gemini-2.0-flash, Grok-3, GPT-4o-mini)
across tasks involving statistical results, risk-of-bias assessments, and
study-level characteristics in three medical domains: hypertension, diabetes,
and orthopaedics. We tested four distinct prompting strategies (basic
prompting, self-reflective prompting, model ensemble, and customised prompts)
to determine how to improve extraction quality. All models demonstrate high
precision but consistently suffer from poor recall by omitting key information.
We found that customised prompts were the most effective, boosting recall by up
to 15\%. Based on this analysis, we propose a three-tiered set of guidelines
for using LLMs in data extraction, matching data types to appropriate levels of
automation based on task complexity and risk. Our study offers practical advice
for automating data extraction in real-world meta-analyses, balancing LLM
efficiency with expert oversight through targeted, task-specific automation.

</details>


### [192] [Collaborative Distillation Strategies for Parameter-Efficient Language Model Deployment](https://arxiv.org/abs/2507.15198)
*Xiandong Meng,Yan Wu,Yexin Tian,Xin Hu,Tianze Kang,Junliang Du*

Main category: cs.CL

TL;DR: 本文提出了一个通过多位教师模型指导学生模型学习的蒸馏策略，以解决大语言模型的高计算成本和推断速度慢的问题。


<details>
  <summary>Details</summary>
Motivation: 面对大语言模型高计算成本和推断速度慢的挑战，希望通过改进蒸馏技术来更有效地压缩模型并保持其性能。

Method: 提出了多教师模型指导的蒸馏策略，包括加权输出融合机制、特征对齐损失函数和基于熵的动态教师权重策略，以提高知识蒸馏的质量和稳定性。

Result: 学生模型在语义信息捕捉、表达一致性、泛化能力和任务适配性上表现突出，在语言建模、文本生成和多任务学习等任务中均表现良好。

Conclusion: 多教师协作机制为复杂语言建模任务中的高效压缩提供了可行的技术路径，并证实了这一方法在多个评价指标上的整体优势。

Abstract: This paper addresses the challenges of high computational cost and slow
inference in deploying large language models. It proposes a distillation
strategy guided by multiple teacher models. The method constructs several
teacher models and integrates their output probability distributions and
intermediate semantic features. This guides the student model to learn from
multiple sources of knowledge. As a result, the student model gains stronger
language understanding and generation ability while maintaining a small
parameter size. To achieve this, the paper introduces a weighted output fusion
mechanism, a feature alignment loss function, and an entropy-driven dynamic
teacher weighting strategy. These components improve the quality and stability
of knowledge transfer during distillation. Under multi-teacher guidance, the
student model captures semantic information more effectively and demonstrates
strong performance across multiple evaluation metrics. In particular, the
method shows high consistency in expression, generalization ability, and task
adaptability in tasks such as language modeling, text generation, and
multi-task learning. The experiments compare the proposed method with several
widely adopted distillation approaches. The results further confirm its overall
advantages in perplexity, distillation loss, and generation quality. This study
provides a feasible technical path for the efficient compression of large-scale
language models. It also demonstrates the effectiveness of multi-teacher
collaborative mechanisms in complex language modeling tasks.

</details>


### [193] [SOI Matters: Analyzing Multi-Setting Training Dynamics in Pretrained Language Models via Subsets of Interest](https://arxiv.org/abs/2507.15236)
*Shayan Vassef,Amirhossein Dabiriaghdam,Mohammadreza Bakhtiari,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: 该论文分析了多任务、多语言和多数据源学习对预训练语言模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 通过引入一个新的分类框架（SOI），探讨语言模型不同训练配置间的行为变化。

Method: 利用SOI分类体系和可视化工具，分析在单任务与多任务、单语言与多语言、单数据源与多数据源学习配置之间的迁移表现，并提出双阶段微调方法。

Result: 多源学习提高了最高7%的分布外性能，多任务学习在相似任务组合中表现出显著提升，双阶段微调进一步优化了模型性能。

Conclusion: 多任务、多语言、多数据源学习提供了理解训练动态的新视角，同时提出了实践性优化方案以提升多设置语言模型的性能。

Abstract: This work investigates the impact of multi-task, multi-lingual, and
multi-source learning approaches on the robustness and performance of
pretrained language models. To enhance this analysis, we introduce Subsets of
Interest (SOI), a novel categorization framework that identifies six distinct
learning behavior patterns during training, including forgettable examples,
unlearned examples, and always correct examples. Through SOI transition
heatmaps and dataset cartography visualization, we analyze how examples shift
between these categories when transitioning from single-setting to
multi-setting configurations. We perform comprehensive experiments across three
parallel comparisons: multi-task vs. single-task learning using English tasks
(entailment, paraphrase, sentiment), multi-source vs. single-source learning
using sentiment analysis datasets, and multi-lingual vs. single-lingual
learning using intent classification in French, English, and Persian. Our
results demonstrate that multi-source learning consistently improves
out-of-distribution performance by up to 7%, while multi-task learning shows
mixed results with notable gains in similar task combinations. We further
introduce a two-stage fine-tuning approach where the second stage leverages
SOI-based subset selection to achieve additional performance improvements.
These findings provide new insights into training dynamics and offer practical
approaches for optimizing multi-setting language model performance.

</details>


### [194] [ChiMed 2.0: Advancing Chinese Medical Dataset in Facilitating Large Language Modeling](https://arxiv.org/abs/2507.15275)
*Yuanhe Tian,Junjie Liu,Zhizhou Kou,Yuxiang Li,Yan Song*

Main category: cs.CL

TL;DR: 本文提出了一个名为ChiMed 2.0的中文医学数据集，旨在改进现有中文医学数据集在规模和领域覆盖方面的不足，同时支持预训练、监督微调(SFT)和人类反馈强化学习(RLHF)。


<details>
  <summary>Details</summary>
Motivation: 现有中文医学数据集在规模和领域覆盖方面有限，且大部分仅支持LLM微调，无法满足相关人工智能研究和应用的需求。

Method: 通过收集自中文医学在线平台以及由大语言模型(LLM)生成的数据，构建包含204.4M字符的新数据集ChiMed 2.0，并提供预训练文档、监督微调问答对以及RLHF偏好数据元组。通过对通用领域LLM进行进一步预训练、SFT和RLHF实验评估其性能。

Result: 实验表明，无论在何种规模的模型上，与医学基准数据集的测试结果相比，均取得显著提升，证实了数据集的有效性和适用性。

Conclusion: ChiMed 2.0拓展了中文医学数据领域的覆盖范围，提供了全面支持预训练、SFT和RLHF的资源，对医学大模型的构建和性能提升具有重要意义。

Abstract: Building high-quality data resources is crucial for advancing artificial
intelligence research and applications in specific domains, particularly in the
Chinese medical domain. Existing Chinese medical datasets are limited in size
and narrow in domain coverage, falling short of the diverse corpora required
for effective pre-training. Moreover, most datasets are designed solely for LLM
fine-tuning and do not support pre-training and reinforcement learning from
human feedback (RLHF). In this paper, we propose a Chinese medical dataset
named ChiMed 2.0, which extends our previous work ChiMed, and covers data
collected from Chinese medical online platforms and generated by LLMs. ChiMed
2.0 contains 204.4M Chinese characters covering both traditional Chinese
medicine classics and modern general medical data, where there are 164.8K
documents for pre-training, 351.6K question-answering pairs for supervised
fine-tuning (SFT), and 41.7K preference data tuples for RLHF. To validate the
effectiveness of our approach for training a Chinese medical LLM, we conduct
further pre-training, SFT, and RLHF experiments on representative general
domain LLMs and evaluate their performance on medical benchmark datasets. The
results show performance gains across different model scales, validating the
dataset's effectiveness and applicability.

</details>


### [195] [A Novel Self-Evolution Framework for Large Language Models](https://arxiv.org/abs/2507.15281)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.CL

TL;DR: 本文提出了一种名为Dual-Phase Self-Evolution (DPSE)的框架，用于在后训练阶段同时优化用户偏好适配和领域特定能力，通过引入Censor模块以及两阶段微调设计，有效提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有后训练策略较难同时在用户对齐和领域认知方面取得进展，因此需要一种方法能够联合优化这两方面。

Method: 提出了DPSE框架，通过Censor模块提取多维交互信号并估算满意度分数，基于此扩充数据集，并采用两阶段微调策略，包括监督领域绑定和频率感知偏好优化。

Result: 实验表明，DPSE框架在泛NLP基准测试和长期对话任务中均优于现有方法，并通过消融实验验证了各模块的贡献。

Conclusion: DPSE框架能够为大型语言模型提供持续的自我进化路径，综合优化用户适配性与领域认知能力。

Abstract: The capabilities of Large Language Models (LLMs) are limited to some extent
by pre-training, so some researchers optimize LLMs through post-training.
Existing post-training strategies, such as memory-based retrieval or preference
optimization, improve user alignment yet fail to enhance the model's domain
cognition. To bridge this gap, we propose a novel Dual-Phase Self-Evolution
(DPSE) framework that jointly optimizes user preference adaptation and
domain-specific competence. DPSE introduces a Censor module to extract
multi-dimensional interaction signals and estimate satisfaction scores, which
guide structured data expansion via topic-aware and preference-driven
strategies. These expanded datasets support a two-stage fine-tuning pipeline:
supervised domain grounding followed by frequency-aware preference
optimization. Experiments across general NLP benchmarks and long-term dialogue
tasks demonstrate that DPSE consistently outperforms Supervised Fine-Tuning,
Preference Optimization, and Memory-Augmented baselines. Ablation studies
validate the contribution of each module. In this way, our framework provides
an autonomous path toward continual self-evolution of LLMs.

</details>


### [196] [Beyond Easy Wins: A Text Hardness-Aware Benchmark for LLM-generated Text Detection](https://arxiv.org/abs/2507.15286)
*Navid Ayoobi,Sadat Shahriar,Arjun Mukherjee*

Main category: cs.CL

TL;DR: 该研究提出了一个新型AI文本检测器评估范式，集成可靠性和稳定性因素用于实际评估。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法忽略了真实世界环境中的关键问题，如低误报率和跨领域稳定性，限制了检测系统实际应用。

Method: 设计了一个名为SHIELD的新基准，将可靠性和稳定性纳入统一评估指标，并开发了一种后处理模型无关的人类化框架，使AI文本更像人类撰写。

Result: SHIELD有效挑战了当前最先进的零样本检测方法，同时提升了检测在可靠性和稳定性方面的表现。

Conclusion: SHIELD为AI文本检测器提供了更实用的评估方法，有助于改进其实际应用性能。

Abstract: We present a novel evaluation paradigm for AI text detectors that prioritizes
real-world and equitable assessment. Current approaches predominantly report
conventional metrics like AUROC, overlooking that even modest false positive
rates constitute a critical impediment to practical deployment of detection
systems. Furthermore, real-world deployment necessitates predetermined
threshold configuration, making detector stability (i.e. the maintenance of
consistent performance across diverse domains and adversarial scenarios), a
critical factor. These aspects have been largely ignored in previous research
and benchmarks. Our benchmark, SHIELD, addresses these limitations by
integrating both reliability and stability factors into a unified evaluation
metric designed for practical assessment. Furthermore, we develop a post-hoc,
model-agnostic humanification framework that modifies AI text to more closely
resemble human authorship, incorporating a controllable hardness parameter.
This hardness-aware approach effectively challenges current SOTA zero-shot
detection methods in maintaining both reliability and stability. (Data and
code: https://github.com/navid-aub/SHIELD-Benchmark)

</details>


### [197] [On the Inevitability of Left-Leaning Political Bias in Aligned Language Models](https://arxiv.org/abs/2507.15328)
*Thilo Hagendorff*

Main category: cs.CL

TL;DR: 本文探讨大型语言模型的对齐原则（无害、有帮助、诚实）与其被指称的左翼政治倾向之间的关系，提出左翼偏向是对齐目标的必然结果。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决关于大型语言模型是否存在政治偏见的争议，特别是左翼倾向与AI对齐目标之间的关系。

Method: 通过分析对齐目标与其内在的价值假设，探讨这些目标如何与不同的政治意识形态产生关系，尤其是与左翼倾向的联系。

Result: 研究得出结论：AI对齐目标的基础在于避免伤害、包容性、公平性和实证真相，这些标准与左翼的价值观一致，而与右翼意识形态存在冲突。

Conclusion: 认为将AI左翼倾向描述为风险或问题实际上违背了AI对齐原则，反而应该将左翼倾向视为对齐目标不可避免的结果。

Abstract: The guiding principle of AI alignment is to train large language models
(LLMs) to be harmless, helpful, and honest (HHH). At the same time, there are
mounting concerns that LLMs exhibit a left-wing political bias. Yet, the
commitment to AI alignment cannot be harmonized with the latter critique. In
this article, I argue that intelligent systems that are trained to be harmless
and honest must necessarily exhibit left-wing political bias. Normative
assumptions underlying alignment objectives inherently concur with progressive
moral frameworks and left-wing principles, emphasizing harm avoidance,
inclusivity, fairness, and empirical truthfulness. Conversely, right-wing
ideologies often conflict with alignment guidelines. Yet, research on political
bias in LLMs is consistently framing its insights about left-leaning tendencies
as a risk, as problematic, or concerning. This way, researchers are actively
arguing against AI alignment, tacitly fostering the violation of HHH
principles.

</details>


### [198] [Reasoning Models are Test Exploiters: Rethinking Multiple-Choice](https://arxiv.org/abs/2507.15337)
*Narun Raman,Taylor Lundy,Kevin Leyton-Brown*

Main category: cs.CL

TL;DR: 本文评估了15个不同问题回答基准和25个不同大语言模型（LLMs），探讨了不同问题呈现方式对模型性能的影响。研究发现，提供多项选择的MCQA方法不再是评估最新模型下游性能的良好代理。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法（MCQA）为模型提供固定选择项，虽然易于评分且关联效果较好，但未能真实反映下游推理性能。本研究旨在验证此方法对现代LLMs的适用性。

Method: 对25种模型和15种基准进行了系统性评估，采用五种问题呈现方式，包括是否提供多项选择、“上述皆非”选项、以及模型是否允许链式推理等变化。

Result: MCQA在仅允许链式推理发生于答案选项提供之前时是良好评估指标。但若允许在选项提供后推理，大型模型会利用选项信息优势，显著优于自由回答表现。

Conclusion: MCQA不再适用评估最新LLMs的真实能力，应设计更健壮、更抗偏差的基准，以更好地评估模型的推理能力。

Abstract: When evaluating Large Language Models (LLMs) in question-answering domains,
it is common to ask the model to choose among a fixed set of choices (so-called
multiple-choice question-answering, or MCQA). Although downstream tasks of
interest typically do not provide systems with explicit options among which to
choose, this approach is nevertheless widely used because it makes it makes
automatic grading straightforward and has tended to produce challenging
benchmarks that correlate sufficiently well with downstream performance. This
paper investigates the extent to which this trend continues to hold for
state-of-the-art reasoning models, describing a systematic evaluation of $15$
different question-answering benchmarks (e.g., MMLU, HLE) and $25$ different
LLMs (including small models such as Qwen 7B and relatively large models such
as Llama 70B). For each model-benchmark pair, we considered $5$ ways of
presenting the model with questions, including variations on whether multiple
choices were offered to the model at all; whether "none of the above" sometimes
replaced the right answer; and whether the model was permitted to perform
chain-of-thought reasoning before and/or after the choices were presented. MCQA
remained a good proxy for the downstream performance of models as long as they
were allowed to perform chain-of-thought reasoning only before being presented
with the options among which they had to select. On the other hand, large
models that were able to perform reasoning after being given a set of options
tended to significantly outperform their free-text performance due to
exploiting the information in the options. We conclude that MCQA is no longer a
good proxy for assessing downstream performance of state-of-the-art models, and
offer practical guidelines for designing more robust, bias-resistant benchmarks
that better reflect LLMs' genuine reasoning capabilities.

</details>


### [199] [LionGuard 2: Building Lightweight, Data-Efficient & Localised Multilingual Content Moderators](https://arxiv.org/abs/2507.15339)
*Leanne Tan,Gabriel Chua,Ziyu Ge,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: 现代化内容审核系统LionGuard 2专注于多语言内容审核，特别是新加坡环境下的多语言支持。


<details>
  <summary>Details</summary>
Motivation: 现有内容审核系统常无法有效地支持本地化语言，尤其是在低资源语言和变体方面。鉴于大模型的高计算要求，小模型提供了一个潜在的替代方案。

Method: 基于预训练的OpenAI嵌入和多头序列分类器的架构，支持多语言，并通过多种基准测试评估其性能。

Result: LionGuard 2在英語和包含新加坡特色的多种语言内容审核领域表现优异，部署于新加坡政府体系中并展示了其大规模的实际效用。

Conclusion: 高质量本地数据和稳健的多语言嵌入可在不依赖大模型微调的情况下，实现卓越的内容审核性能。

Abstract: Modern moderation systems increasingly support multiple languages, but often
fail to address localisation and low-resource variants - creating safety gaps
in real-world deployments. Small models offer a potential alternative to large
LLMs, yet still demand considerable data and compute. We present LionGuard 2, a
lightweight, multilingual moderation classifier tailored to the Singapore
context, supporting English, Chinese, Malay, and partial Tamil. Built on
pre-trained OpenAI embeddings and a multi-head ordinal classifier, LionGuard 2
outperforms several commercial and open-source systems across 17 benchmarks,
including both Singapore-specific and public English datasets. The system is
actively deployed within the Singapore Government, demonstrating practical
efficacy at scale. Our findings show that high-quality local data and robust
multilingual embeddings can achieve strong moderation performance, without
fine-tuning large models. We release our model weights and part of our training
data to support future work on LLM safety.

</details>


### [200] [Probing Information Distribution in Transformer Architectures through Entropy Analysis](https://arxiv.org/abs/2507.15347)
*Amedeo Buonanno,Alessandro Rivetti,Francesco A. N. Palmieri,Giovanni Di Gennaro,Gianmarco Romano*

Main category: cs.CL

TL;DR: 研究将熵分析应用于Transformer架构的信息分布探究，主要以GPT为案例。


<details>
  <summary>Details</summary>
Motivation: 探索通过熵分析理解Transformer模型的信息处理和转换过程，提高对模型的解释性和行为的理解。

Method: 量化词元的不确定性以及分析处理过程中的熵分布模式，并结合案例研究（GPT模型）验证该方法的潜力。

Result: 验证了熵分析可以揭示模型行为和内部表示的潜在洞察。

Conclusion: 熵分析为提升Transformer模型可解释性提供了一种有效方法，也可能催生新的评估框架。

Abstract: This work explores entropy analysis as a tool for probing information
distribution within Transformer-based architectures. By quantifying token-level
uncertainty and examining entropy patterns across different stages of
processing, we aim to investigate how information is managed and transformed
within these models. As a case study, we apply the methodology to a GPT-based
large language model, illustrating its potential to reveal insights into model
behavior and internal representations. This approach may offer insights into
model behavior and contribute to the development of interpretability and
evaluation frameworks for transformer-based models

</details>


### [201] [Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding](https://arxiv.org/abs/2507.15357)
*Elisa Sanchez-Bayona,Rodrigo Agerri*

Main category: cs.CL

TL;DR: 论文分析了大型语言模型（LLMs）在多种数据集、任务和提示配置下对隐喻的理解能力，发现其表现更依赖于表层特征而非隐喻内容。


<details>
  <summary>Details</summary>
Motivation: 针对此前隐喻处理研究过于局限于单一数据集评估和特定任务设置的不足，扩展研究方法进行更全面分析。

Method: 使用包含推理和隐喻注释的多样化公开数据集，聚焦于自然语言推理（NLI）和问答（QA）任务，开展大规模实验。

Result: 结果显示，LLMs在隐喻语言理解上的突出表现更多源于表层特征、上下文学习能力和语言知识，而非对隐喻语言的真正理解。

Conclusion: 研究指出当前LLMs在处理隐喻上的局限性，强调需要更现实的评估框架，同时提供了公开数据与代码供后续研究参考。

Abstract: This paper presents a comprehensive evaluation of the capabilities of Large
Language Models (LLMs) in metaphor interpretation across multiple datasets,
tasks, and prompt configurations. Although metaphor processing has gained
significant attention in Natural Language Processing (NLP), previous research
has been limited to single-dataset evaluations and specific task settings,
often using artificially constructed data through lexical replacement. We
address these limitations by conducting extensive experiments using diverse
publicly available datasets with inference and metaphor annotations, focusing
on Natural Language Inference (NLI) and Question Answering (QA) tasks. The
results indicate that LLMs' performance is more influenced by features like
lexical overlap and sentence length than by metaphorical content, demonstrating
that any alleged emergent abilities of LLMs to understand metaphorical language
are the result of a combination of surface-level features, in-context learning,
and linguistic knowledge. This work provides critical insights into the current
capabilities and limitations of LLMs in processing figurative language,
highlighting the need for more realistic evaluation frameworks in metaphor
interpretation tasks. Data and code are publicly available.

</details>


### [202] [STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for Spoken Language Models](https://arxiv.org/abs/2507.15375)
*Cheng-Han Chiang,Xiaofei Wang,Linjie Li,Chung-Ching Lin,Kevin Lin,Shujie Liu,Zhendong Wang,Zhengyuan Yang,Hung-yi Lee,Lijuan Wang*

Main category: cs.CL

TL;DR: 本研究提出了一种名为Stitch的新方法，使得口语模型能够在生成语音响应的同时进行内部逻辑推理，从而提高性能并减少响应延迟。


<details>
  <summary>Details</summary>
Motivation: 传统口语模型缺乏内在的逻辑推理能力，而人类通常在交流前会进行复杂的内部思考，整合这些能力对于口语模型至关重要。

Method: 提出了Stitch方法，模型通过交替生成内部的无声逻辑推理片段和语音响应片段，从而在语音播放的同时完成推理，实现了同步思考与表达。

Result: 相比于不具备逻辑推理能力的基线模型，Stitch在数学推理数据集上的表现提升了15%，且在非推理数据集上的表现与基线模型相当。

Conclusion: Stitch显著提升了口语模型性能，解决了以往逻辑推理导致延迟的问题，同时保持了较好的多任务表现。

Abstract: Spoken Language Models (SLMs) are designed to take speech inputs and produce
spoken responses. However, current SLMs lack the ability to perform an
internal, unspoken thinking process before responding. In contrast, humans
typically engage in complex mental reasoning internally, enabling them to
communicate ideas clearly and concisely. Thus, integrating an unspoken thought
process into SLMs is highly desirable. While naively generating a complete
chain-of-thought (CoT) reasoning before starting to talk can enable thinking
for SLMs, this induces additional latency for the speech response, as the CoT
reasoning can be arbitrarily long. To solve this issue, we propose Stitch, a
novel generation method that alternates between the generation of unspoken
reasoning chunks and spoken response chunks. Since the audio duration of a
chunk of spoken response is much longer than the time to generate the tokens in
a chunk of spoken response, we use the remaining free time to generate the
unspoken reasoning tokens. When a chunk of audio is played to the user, the
model continues to generate the next unspoken reasoning chunk, achieving
simultaneous thinking and talking. Remarkably, Stitch matches the latency of
baselines that cannot generate unspoken CoT by design while outperforming those
baselines by 15% on math reasoning datasets; Stitch also performs equally well
on non-reasoning datasets as those baseline models. Some animations and
demonstrations are on the project page: https://d223302.github.io/STITCH.

</details>


### [203] [AlgoSimBench: Identifying Algorithmically Similar Problems for Competitive Programming](https://arxiv.org/abs/2507.15378)
*Jierui Li,Raymond Mooney*

Main category: cs.CL

TL;DR: 研究评估了LLM（大语言模型）在识别算法相似问题（ASP）上的能力，并提出了ASM方法以提升该能力。


<details>
  <summary>Details</summary>
Motivation: 旨在探索LLM在训练中较少接触的领域中解决复杂算法问题的能力，尤其是算法相似问题的识别能力。

Method: 引入一个名为AlgoSimBench的新基准，包含1317个问题和402个选择题，以及创建了ASM（解决方案匹配）的方法来改进识别能力，并结合关键词优先的方法BM25进行评估。

Result: LLM在ASP识别上表现较弱，最佳模型o3-mini在选择题任务中的准确率仅为65.9%。使用ASM方法后，可将准确率提高6.7%至11.7%。此外，结合BM25的方法也提升了识别精度。

Conclusion: ASM方法和AlgoSimBench基准为提升LLM在算法领域的能力提供了有力支持，但依然有较大改进空间。

Abstract: Recent progress in LLMs, such as reasoning models, has demonstrated strong
abilities to solve complex competitive programming problems, often rivaling top
human competitors. However, it remains underexplored whether these abilities
generalize to relevant domains that are less seen during training. To address
this, we introduce AlgoSimBench, a new benchmark designed to assess LLMs'
ability to identify algorithmically similar problems (ASPs)-problems that can
be solved using similar algorithmic approaches. AlgoSimBench consists of 1317
problems, annotated with 231 distinct fine-grained algorithm tags, from which
we curate 402 multiple-choice questions (MCQs), where each question presents
one algorithmically similar problem alongside three textually similar but
algorithmically dissimilar distractors. Our evaluation reveals that LLMs
struggle to identify ASPs, with the best-performing model (o3-mini) achieving
only 65.9% accuracy on the MCQ task. To address this challenge, we propose
attempted solution matching (ASM), a novel method for improving problem
similarity detection. On our MCQ task, ASM yields an absolute accuracy
improvement of 6.7% to 11.7% across different models. We also evaluated code
embedding models and retrieval methods on similar problem identification. While
the adversarial selection of problems degrades the performance to be less than
random, we found that simply summarizing the problem to remove narrative
elements eliminates the effect, and combining ASM with a keyword-prioritized
method, BM25, can yield up to 52.2% accuracy. Code and data are available at
github.com

</details>


### [204] [ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution](https://arxiv.org/abs/2507.15501)
*Alexandru Coca,Mark Gaynor,Zhenxing Zhang,Jianpeng Cheng,Bo-Hsiang Tseng,Pete Boothroyd,Héctor Martinez Alonso,Diarmuid Ó Séaghdha,Anders Johannsen*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型（LLMs）用于驱动具有复杂动作执行能力的数字助手的潜力，并提出了ASPERA框架，用于生成和评估相关任务数据。


<details>
  <summary>Details</summary>
Motivation: 研究人员希望开发能够执行多步操作的数字助手，这些助手可以基于内置编程知识生成动作执行程序。

Method: 提出ASPERA框架，该框架结合助手库模拟和人类辅助的LLM数据生成引擎，同时发布了Asper-Bench评估数据集。

Result: 实验表明，与无依赖代码生成相比，基于自定义助手库的程序生成对LLMs提出了更高的挑战。

Conclusion: ASPERA框架及其评估数据集成功展示了提升LLMs在复杂数字助手操作中表现的可能性。

Abstract: This work evaluates the potential of large language models (LLMs) to power
digital assistants capable of complex action execution. These assistants rely
on pre-trained programming knowledge to execute multi-step goals by composing
objects and functions defined in assistant libraries into action execution
programs. To achieve this, we develop ASPERA, a framework comprising an
assistant library simulation and a human-assisted LLM data generation engine.
Our engine allows developers to guide LLM generation of high-quality tasks
consisting of complex user queries, simulation state and corresponding
validation programs, tackling data availability and evaluation robustness
challenges. Alongside the framework we release Asper-Bench, an evaluation
dataset of 250 challenging tasks generated using ASPERA, which we use to show
that program generation grounded in custom assistant libraries is a significant
challenge to LLMs compared to dependency-free code generation.

</details>


### [205] [Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language Models](https://arxiv.org/abs/2507.15512)
*Kaiyan Chang,Yonghao Shi,Chenglong Wang,Hang Zhou,Chi Hu,Xiaoqian Liu,Yingfeng Luo,Yuan Ge,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本研究探讨了推理过程中无需训练的测试阶段扩展方法(Hybrid Test-Time Scaling, TTS)，设计了一种结合条件步骤级自我改进与传统并行扩展的方法，并通过实验验证了其在提升语言模型推理性能方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的测试阶段扩展方法中，基于训练的TTS方法盛行，但增加了计算负担，导致无需训练的测试阶段方法逐渐被忽视。因此，研究集中于无需训练的测试扩展方法，改善推理表现力。

Method: 提出了条件步骤级自我改进方法，并结合经典并行扩展方法，形成了一种混合测试阶段扩展推理的新范式。

Result: 通过对5个经过指令调优的大型语言模型(3B-14B)及其家族展开实验，证明混合策略在无需训练的测试扩展中表现出显著的推理性能提升潜力。

Conclusion: 无需训练的测试阶段扩展方法，在结合多种步骤级的扩展手段后，能够推动大规模语言模型的推理性能极限，具有重要研究价值。

Abstract: Test-Time Scaling (TTS) is a promising approach to progressively elicit the
model's intelligence during inference. Recently, training-based TTS methods,
such as continued reinforcement learning (RL), have further surged in
popularity, while training-free TTS methods are gradually fading from
prominence. However, the additional computation overhead of training amplifies
the burden on test-time scaling. In this paper, we focus on training-free TTS
methods for reasoning. We first design Conditional Step-level Self-refinement,
a fine-grained sequential scaling method guided by process verification. On top
of its effectiveness, we further combine it with other classical parallel
scaling methods at the step level, to introduce a novel inference paradigm
called Hybrid Test-Time Scaling. Extensive experiments on five
instruction-tuned LLMs across different scales (3B-14B) and families
demonstrate that hybrid strategy incorporating various training-free TTS
methods at a fine granularity has considerable potential for expanding the
reasoning performance boundaries of LLMs.

</details>


### [206] [Evaluating Text Style Transfer: A Nine-Language Benchmark for Text Detoxification](https://arxiv.org/abs/2507.15557)
*Vitaly Protasov,Nikolay Babakov,Daryna Dementieva,Alexander Panchenko*

Main category: cs.CL

TL;DR: 本文分析文本生成任务中的风格转移和评估问题，尤其是多语言的文本净化评估场景。


<details>
  <summary>Details</summary>
Motivation: 目前文本风格转移评价的自动指标与人工判断差距较大，且多语言评价研究较少。

Method: 基于九种语言（包括英文、西班牙文、中文等），评估神经网络模型和以提示为基础的评估方法的有效性。

Result: 研究结果提出了一个可行的多语言文本风格转移评价设计方案。

Conclusion: 本文为多语言文本净化评价提供了实用路径，推进了该领域研究的深入。

Abstract: Despite recent progress in large language models (LLMs), evaluation of text
generation tasks such as text style transfer (TST) remains a significant
challenge. Recent studies (Dementieva et al., 2024; Pauli et al., 2025)
revealed a substantial gap between automatic metrics and human judgments.
Moreover, most prior work focuses exclusively on English, leaving multilingual
TST evaluation largely unexplored. In this paper, we perform the first
comprehensive multilingual study on evaluation of text detoxification system
across nine languages: English, Spanish, German, Chinese, Arabic, Hindi,
Ukrainian, Russian, Amharic. Drawing inspiration from the machine translation,
we assess the effectiveness of modern neural-based evaluation models alongside
prompting-based LLM-as-a-judge approaches. Our findings provide a practical
recipe for designing more reliable multilingual TST evaluation pipeline in the
text detoxification case.

</details>


### [207] [Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging](https://arxiv.org/abs/2507.15576)
*Nicolas Poggi,Shashank Agnihotri,Margret Keuper*

Main category: cs.CL

TL;DR: 本文提出将结合图像-文本模型的上下文学习应用于太赫兹成像中的分类任务，以解决数据少、分辨率低等问题，并实现了良好的零样本和单样本分类效果。


<details>
  <summary>Details</summary>
Motivation: 太赫兹成像在安检和材料分类等领域有广泛的应用，但由于数据有限和成像分辨率低，导致图像分类面临挑战。

Method: 提出了一种基于模态对齐提示的框架，将两个公开预训练的图像-文本模型适配到太赫兹领域，在零样本和单样本场景下实现分类；不需要额外的模型微调。

Result: 实验结果表明，在数据有限的情况下，上下文学习方法提升了分类效果和解释性。

Conclusion: 这是上下文学习增强的图像-文本模型首次应用于太赫兹成像，提高了资源有限科学领域的潜在研究方向。

Abstract: Terahertz (THz) imaging enables non-invasive analysis for applications such
as security screening and material classification, but effective image
classification remains challenging due to limited annotations, low resolution,
and visual ambiguity. We introduce In-Context Learning (ICL) with
Vision-Language Models (VLMs) as a flexible, interpretable alternative that
requires no fine-tuning. Using a modality-aligned prompting framework, we adapt
two open-weight VLMs to the THz domain and evaluate them under zero-shot and
one-shot settings. Our results show that ICL improves classification and
interpretability in low-data regimes. This is the first application of
ICL-enhanced VLMs to THz imaging, offering a promising direction for
resource-constrained scientific domains. Code:
\href{https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main}{GitHub
repository}.

</details>


### [208] [Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation](https://arxiv.org/abs/2507.15586)
*Xinping Zhao,Shouzheng Huang,Yan Zhong,Xinshuo Hu,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为LEAR的方法，通过显式推理和谨慎提取来改进大语言模型中检索增强生成的证据提取质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有检索增强生成方法中因噪声引发的问题，特别是在证据提取中的过滤风险和泛化能力不足的问题。

Method: 通过显式推理识别潜在线索，结合谨慎提取避免遗漏关键证据，还使用了知识标记掩码、三种可验证奖励函数以及策略优化算法进行模型训练。

Result: 实验证明LEAR在三个基准数据集上效果显著，可以提供高质量证据，提升下游任务准确性，并实现在线系统中的有效应用。

Conclusion: LEAR通过改进证据提取机制，提升了大语言模型的生成质量，特别是在面向在线和实际应用场景中的表现。

Abstract: Retrieval-Augmented Generation (RAG) effectively improves the accuracy of
Large Language Models (LLMs). However, retrieval noises significantly impact
the quality of LLMs' generation, necessitating the development of denoising
mechanisms. Previous methods extract evidence straightforwardly without
explicit thinking, which risks filtering out key clues and struggles with
generalization. To this end, we propose LEAR, which learns to extract rational
evidence by (1) explicitly reasoning to identify potential cues within
retrieval contents first, and then (2) consciously extracting to avoid omitting
any key cues helpful for answering questions. Specifically, we frame evidence
reasoning and evidence extraction into one unified response for end-to-end
training; apply knowledge token masks for disentanglement to derive
reasoning-based and extraction-based answers; and devise three types of
verifiable reward functions, including answer, length, and format, to update
the model via the policy optimization algorithm. Extensive experiments on three
benchmark datasets show the effectiveness of LEAR, providing compact and
high-quality evidence, improving the accuracy of downstream tasks, and
promoting effective application in online RAG systems.

</details>


### [209] [Conflicting narratives and polarization on social media](https://arxiv.org/abs/2507.15600)
*Armin Pournaki*

Main category: cs.CL

TL;DR: 该研究分析了政治冲突叙事如何通过推特提供关于公共领域极化和议题排列的洞见。


<details>
  <summary>Details</summary>
Motivation: 探讨通过冲突叙事分析理解极化和议题排列的潜在机制。

Method: 基于2021至2023年德国推特数据，分析推文中对冲突叙事的文本信号，包括不同的行动角色归因及事件主体安排模式。

Result: 发现了两种叙事冲突形式：相同行动主体的不同角色归因，以及对相同事件的不同主体安排。同时，首次揭示了叙事排列的模式。

Conclusion: 叙事分析为理解极化背后的话语机制提供了重要视角。

Abstract: Narratives are key interpretative devices by which humans make sense of
political reality. In this work, we show how the analysis of conflicting
narratives, i.e. conflicting interpretive lenses through which political
reality is experienced and told, provides insight into the discursive
mechanisms of polarization and issue alignment in the public sphere. Building
upon previous work that has identified ideologically polarized issues in the
German Twittersphere between 2021 and 2023, we analyze the discursive dimension
of polarization by extracting textual signals of conflicting narratives from
tweets of opposing opinion groups. Focusing on a selection of salient issues
and events (the war in Ukraine, Covid, climate change), we show evidence for
conflicting narratives along two dimensions: (i) different attributions of
actantial roles to the same set of actants (e.g. diverging interpretations of
the role of NATO in the war in Ukraine), and (ii) emplotment of different
actants for the same event (e.g. Bill Gates in the right-leaning Covid
narrative). Furthermore, we provide first evidence for patterns of narrative
alignment, a discursive strategy that political actors employ to align opinions
across issues. These findings demonstrate the use of narratives as an
analytical lens into the discursive mechanisms of polarization.

</details>


### [210] [Leveraging Context for Multimodal Fallacy Classification in Political Debates](https://arxiv.org/abs/2507.15641)
*Alessio Pittiglio*

Main category: cs.CL

TL;DR: 本文提出了一种基于Transformer模型的多模态辩论谬误分类方法，在分类任务中取得了文本、音频与多模态的不同性能表现。


<details>
  <summary>Details</summary>
Motivation: 推动多模态论证挖掘领域的发展，特别是在政治辩论中的逻辑谬误识别。

Method: 利用预训练的Transformer模型，探索多种方式充分利用上下文进行分类。

Result: 在分类任务中，文本模型的宏F1分数为0.4444，音频模型为0.3559，多模态模型为0.4403。

Conclusion: 多模态模型性能与单文本模型相当，提示了进一步优化的潜力。

Abstract: In this paper, we present our submission to the MM-ArgFallacy2025 shared
task, which aims to advance research in multimodal argument mining, focusing on
logical fallacies in political debates. Our approach uses pretrained
Transformer-based models and proposes several ways to leverage context. In the
fallacy classification subtask, our models achieved macro F1-scores of 0.4444
(text), 0.3559 (audio), and 0.4403 (multimodal). Our multimodal model showed
performance comparable to the text-only model, suggesting potential for
improvements.

</details>


### [211] [P3: Prompts Promote Prompting](https://arxiv.org/abs/2507.15675)
*Xinyu Zhang,Yuanquan Hu,Fangchao Liu,Zhicheng Dou*

Main category: cs.CL

TL;DR: P3框架通过迭代优化系统和用户提示词来提升大型语言模型的表现，并通过查询相关优化进一步加强在线提示效果。


<details>
  <summary>Details</summary>
Motivation: 现有的提示词优化方法通常只针对系统或用户提示，未能充分利用二者的交互关系，因而效果不佳。

Method: 提出P3框架，通过自我迭代的方式同时优化系统和用户提示词，并结合查询相关的优化策略来增强在线提示表现。

Result: 实验表明，P3在通用任务和推理任务上的提示优化能力显著优于现有方法，展现了出色的性能。

Conclusion: 综合优化系统与用户提示词能够有效提升大型语言模型的表现，强调了整体优化策略的重要性。

Abstract: Current large language model (LLM) applications often employ multi-component
prompts, comprising both system and user prompts, to guide model behaviors.
While recent advancements have demonstrated the efficacy of automatically
optimizing either the system or user prompt to boost performance, such
unilateral approaches often yield suboptimal outcomes due to the interdependent
nature of these components. In this work, we introduce P3, a novel
self-improvement framework that concurrently optimizes both system and user
prompts through an iterative process. The offline optimized prompts are further
leveraged to promote online prompting by performing query-dependent prompt
optimization. Extensive experiments on general tasks (e.g., Arena-hard and
Alpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3
achieves superior performance in the realm of automatic prompt optimization.
Our results highlight the effectiveness of a holistic optimization strategy in
enhancing LLM performance across diverse domains.

</details>


### [212] [CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models](https://arxiv.org/abs/2507.15698)
*Congmin Zheng,Jiachen Zhu,Jianghao Lin,Xinyi Dai,Yong Yu,Weinan Zhang,Mengyue Yang*

Main category: cs.CL

TL;DR: 提出了CoLD框架，通过消除长度偏倚来改进过程奖励模型的可靠性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的过程奖励模型存在长度偏倚问题，影响了评估预测的可靠性并导致了冗长的推理输出。

Method: 提出CoLD框架，包括显式长度惩罚调整、训练偏倚估计器以及联合训练策略，通过因果推理与因果图分析减少长度相关的偏倚。

Result: 实验证明CoLD能有效减少奖励与长度的相关性，提升步骤选择的准确性，并支持更简洁且逻辑有效的推理。

Conclusion: CoLD框架显著提升了过程奖励模型的准确性与鲁棒性，可广泛应用于改进多步推理任务的评估。

Abstract: Process Reward Models (PRMs) play a central role in evaluating and guiding
multi-step reasoning in large language models (LLMs), especially for
mathematical problem solving. However, we identify a pervasive length bias in
existing PRMs: they tend to assign higher scores to longer reasoning steps,
even when the semantic content and logical validity are unchanged. This bias
undermines the reliability of reward predictions and leads to overly verbose
outputs during inference. To address this issue, we propose
CoLD(Counterfactually-Guided Length Debiasing), a unified framework that
mitigates length bias through three components: an explicit length-penalty
adjustment, a learned bias estimator trained to capture spurious length-related
signals, and a joint training strategy that enforces length-invariance in
reward predictions. Our approach is grounded in counterfactual reasoning and
informed by causal graph analysis. Extensive experiments on MATH500 and
GSM-Plus show that CoLD consistently reduces reward-length correlation,
improves accuracy in step selection, and encourages more concise, logically
valid reasoning. These results demonstrate the effectiveness and practicality
of CoLD in improving the fidelity and robustness of PRMs.

</details>


### [213] [Compositional Understanding in Signaling Games](https://arxiv.org/abs/2507.15706)
*David Peter Wallis Freeborn*

Main category: cs.CL

TL;DR: 标准信号游戏模型中的接收者难以学习组合信息。本文构建了新的信号游戏模型，使得真正的组合理解得以演化。


<details>
  <summary>Details</summary>
Motivation: 目前的信号游戏模型中，虽然信号发送者发送了组合信息，但接收者无法组合性地解读，导致信息损失。

Method: 提出两个新模型：最小化接收者仅从信号的原子消息中学习，通才接收者从所有可用信息中学习。这些模型相较以往更简单，且接收者能从消息的原子成分中学习。

Result: 新模型实现了真正的组合信息解读和学习。

Conclusion: 这些模型为解决信号游戏中的组合理解问题提供了一种更简便且有效的方法。

Abstract: Receivers in standard signaling game models struggle with learning
compositional information. Even when the signalers send compositional messages,
the receivers do not interpret them compositionally. When information from one
message component is lost or forgotten, the information from other components
is also erased. In this paper I construct signaling game models in which
genuine compositional understanding evolves. I present two new models: a
minimalist receiver who only learns from the atomic messages of a signal, and a
generalist receiver who learns from all of the available information. These
models are in many ways simpler than previous alternatives, and allow the
receivers to learn from the atomic components of messages.

</details>


### [214] [Is Large Language Model Performance on Reasoning Tasks Impacted by Different Ways Questions Are Asked?](https://arxiv.org/abs/2507.15707)
*Seok Hwan Song,Mohna Chakraborty,Qi Li,Wallapak Tavanapong*

Main category: cs.CL

TL;DR: 本文研究了不同问题类型对大量语言模型（LLMs）推理任务准确性的影响，发现问题类型显著影响模型表现，推理准确性与最终选择准确性不一定相关。


<details>
  <summary>Details</summary>
Motivation: 探讨问题类型如何影响LLM在推理任务中的性能，弥补了现有研究的空白。

Method: 对五种LLMs在三个不同问题类型下的定量和演绎推理任务进行测试，并分析其在推理步骤准确性和最终答案选择中的表现。

Result: 发现(1) 不同问题类型显著影响LLM性能；(2) 推理准确性与最终选择准确性不完全相关；(3) 选项数量和措辞选择会影响模型表现。

Conclusion: 不同问题类型会显著影响LLMs的推理表现，研究强调了更深入分析问题设计对模型性能的影响的必要性。

Abstract: Large Language Models (LLMs) have been evaluated using diverse question
types, e.g., multiple-choice, true/false, and short/long answers. This study
answers an unexplored question about the impact of different question types on
LLM accuracy on reasoning tasks. We investigate the performance of five LLMs on
three different types of questions using quantitative and deductive reasoning
tasks. The performance metrics include accuracy in the reasoning steps and
choosing the final answer. Key Findings: (1) Significant differences exist in
LLM performance across different question types. (2) Reasoning accuracy does
not necessarily correlate with the final selection accuracy. (3) The number of
options and the choice of words, influence LLM performance.

</details>


### [215] [Chinchunmei at SemEval-2025 Task 11: Boosting the Large Language Model's Capability of Emotion Perception using Contrastive Learning](https://arxiv.org/abs/2507.15714)
*Tian Li,Yujian Sun,Huizhi Liang*

Main category: cs.CL

TL;DR: 本论文探讨了在跨28种语言的情感识别竞赛中，通过样本对比学习和生成对比学习方法提高情感预测性能的方法，并取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 旨在解决多语言情感表达多样性和背景变化问题，探索更先进的方法进行情感检测。

Method: 采用样本对比学习(CRC)和生成对比学习(DPO、SimPO)方法，利用LLaMa3-Instruct-8B模型进行微调，分别从样本间的对比和生成结果的对比两个方面优化模型性能。

Result: 在SemEval-2025任务中，团队的系统在英语Track A和Track B分别排名第9和第6，并在其他语言中表现优异。

Conclusion: 研究证明了对比学习方法在多语言情感检测上的有效性，特别是在细化预测和提升泛化性能上的贡献。

Abstract: The SemEval-2025 Task 11, Bridging the Gap in Text-Based Emotion Detection,
introduces an emotion recognition challenge spanning over 28 languages. This
competition encourages researchers to explore more advanced approaches to
address the challenges posed by the diversity of emotional expressions and
background variations. It features two tracks: multi-label classification
(Track A) and emotion intensity prediction (Track B), covering six emotion
categories: anger, fear, joy, sadness, surprise, and disgust. In our work, we
systematically explore the benefits of two contrastive learning approaches:
sample-based (Contrastive Reasoning Calibration) and generation-based (DPO,
SimPO) contrastive learning. The sample-based contrastive approach trains the
model by comparing two samples to generate more reliable predictions. The
generation-based contrastive approach trains the model to differentiate between
correct and incorrect generations, refining its prediction. All models are
fine-tuned from LLaMa3-Instruct-8B. Our system achieves 9th place in Track A
and 6th place in Track B for English, while ranking among the top-tier
performing systems for other languages.

</details>


### [216] [From Queries to Criteria: Understanding How Astronomers Evaluate LLMs](https://arxiv.org/abs/2507.15715)
*Alina Hyk,Kiera McCormick,Mian Zhong,Ioana Ciucă,Sanjib Sharma,John F Wu,J. E. G. Peek,Kartheik G. Iyer,Ziang Xiao,Anjalie Field*

Main category: cs.CL

TL;DR: 本文研究了如何通过用户反馈改进大语言模型（LLM）的评价方法，特别是在天文学文献检索场景中的应用。


<details>
  <summary>Details</summary>
Motivation: 目前，LLM 的评估标准未能适应用户真实多样化的评价需求，因此需要设计更符合实际用途的评价方法。

Method: 通过对368个用户查询的归纳分析和对11位天文学家的访谈，研究用户如何对LLM进行评价，并提出了改进评价标准的建议。

Result: 分析发现了用户提问的类型和评价回答的标准，进而得出可以用于改进LLM评估和构建新的天文学应用评测基准的实践建议。

Conclusion: 提出了一些改进LLM评估的方法，为科学研究中的LLM使用提供了新的思路，特别是强调了评估与实际应用之间的结合。

Abstract: There is growing interest in leveraging LLMs to aid in astronomy and other
scientific research, but benchmarks for LLM evaluation in general have not kept
pace with the increasingly diverse ways that real people evaluate and use these
models. In this study, we seek to improve evaluation procedures by building an
understanding of how users evaluate LLMs. We focus on a particular use case: an
LLM-powered retrieval-augmented generation bot for engaging with astronomical
literature, which we deployed via Slack. Our inductive coding of 368 queries to
the bot over four weeks and our follow-up interviews with 11 astronomers reveal
how humans evaluated this system, including the types of questions asked and
the criteria for judging responses. We synthesize our findings into concrete
recommendations for building better benchmarks, which we then employ in
constructing a sample benchmark for evaluating LLMs for astronomy. Overall, our
work offers ways to improve LLM evaluation and ultimately usability,
particularly for use in scientific research.

</details>


### [217] [BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological Knowledge and Reasoning](https://arxiv.org/abs/2507.15717)
*Sahana Srinivasan,Xuguang Ai,Thaddaeus Wai Soon Lo,Aidan Gilson,Minjie Zou,Ke Zou,Hyunjae Kim,Mingjia Yang,Krithi Pushpanathan,Samantha Yew,Wan Ting Loke,Jocelyn Goh,Yibing Chen,Yiming Kong,Emily Yuelei Fu,Michelle Ongyong Hui,Kristen Nwanyanwu,Amisha Dave,Kelvin Zhenghao Li,Chen-Hsin Sun,Mark Chia,Gabriel Dawei Yang,Wendy Meihua Wong,David Ziyou Chen,Dianbo Liu,Maxwell Singer,Fares Antaki,Lucian V Del Priore,Jost Jonas,Ron Adelman,Qingyu Chen,Yih-Chung Tham*

Main category: cs.CL

TL;DR: 本文介绍了一个新的眼科学领域的大语言模型评测基准BELO，通过多轮专家审查，包含900个高质量专家审查的问题，用以评估模型的临床准确性和推理质量。


<details>
  <summary>Details</summary>
Motivation: 现有针对大语言模型在眼科学领域的评测标准范围有限，过于注重准确率，因此提出一个更全面的基准来评估模型的综合能力。

Method: 利用关键词匹配和微调的PubMedBERT模型从多个数据集（如BCSC、MedMCQA等）中筛选出眼科学领域的多选题，经过专家反复审查，去除重复和低质量题目，并由专家完善正确答案解释，然后通过精心设计指标对六个大语言模型进行测试。

Result: BELO基准包含900个高质量经过专家审查的问题，提供了透明的评价和报告方式。实验评估了六个模型的临床准确性和推理表现，且引入了人类专家审查模型输出的质量。

Conclusion: BELO基准为眼科学大语言模型的评测提供了一个标准化和全面的框架，同时也树立了一个公开的排行榜，以促进公平和可重复的模型比较。

Abstract: Current benchmarks evaluating large language models (LLMs) in ophthalmology
are limited in scope and disproportionately prioritise accuracy. We introduce
BELO (BEnchmarking LLMs for Ophthalmology), a standardized and comprehensive
evaluation benchmark developed through multiple rounds of expert checking by 13
ophthalmologists. BELO assesses ophthalmology-related clinical accuracy and
reasoning quality. Using keyword matching and a fine-tuned PubMedBERT model, we
curated ophthalmology-specific multiple-choice-questions (MCQs) from diverse
medical datasets (BCSC, MedMCQA, MedQA, BioASQ, and PubMedQA). The dataset
underwent multiple rounds of expert checking. Duplicate and substandard
questions were systematically removed. Ten ophthalmologists refined the
explanations of each MCQ's correct answer. This was further adjudicated by
three senior ophthalmologists. To illustrate BELO's utility, we evaluated six
LLMs (OpenAI o1, o3-mini, GPT-4o, DeepSeek-R1, Llama-3-8B, and Gemini 1.5 Pro)
using accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore,
BARTScore, METEOR, and AlignScore). In a further evaluation involving human
experts, two ophthalmologists qualitatively reviewed 50 randomly selected
outputs for accuracy, comprehensiveness, and completeness. BELO consists of 900
high-quality, expert-reviewed questions aggregated from five sources: BCSC
(260), BioASQ (10), MedMCQA (572), MedQA (40), and PubMedQA (18). A public
leaderboard has been established to promote transparent evaluation and
reporting. Importantly, the BELO dataset will remain a hold-out,
evaluation-only benchmark to ensure fair and reproducible comparisons of future
models.

</details>


### [218] [Understanding Large Language Models' Ability on Interdisciplinary Research](https://arxiv.org/abs/2507.15736)
*Yuanhao Shen,Daniel Xavier de Sousa,Ricardo Marçal,Ali Asad,Hongyu Guo,Xiaodan Zhu*

Main category: cs.CL

TL;DR: 本研究提出IDRBench，一个专门用于评估大型语言模型（LLMs）在跨学科学术研究中能力的基准，发现目前的LLMs在提出高质量跨学科研究想法方面仍然面临挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在多步骤逻辑推理方面表现出色，但缺乏专门的评估基准来测验其在跨学科研究（IDR）中的能力，限制了对其能力的全面了解。

Method: 提出IDRBench基准，包括一个由领域专家标注的数据集和一系列任务，用于评估LLMs在跨学科研究领域中提出有价值研究想法的能力。数据集基于ArXiv论文，涵盖六个学科，并通过逐步反映跨学科研究发展的实际阶段设计评估任务。

Result: 通过IDRBench在10个LLMs上测试，尽管发现部分模型表现出一定的跨学科意识，但整体上仍难以生成高质量跨学科研究想法。

Conclusion: 研究揭示了现有LLMs在跨学科研究能力方面的局限性，IDRBench为开发下一代更强大的跨学科研究模型提供了重要的基础。

Abstract: Recent advancements in Large Language Models (LLMs) have revealed their
impressive ability to perform multi-step, logic-driven reasoning across complex
domains, positioning them as powerful tools and collaborators in scientific
discovery while challenging the long-held view that inspiration-driven ideation
is uniquely human. However, the lack of a dedicated benchmark that evaluates
LLMs' ability to develop ideas in Interdisciplinary Research (IDR) settings
poses a critical barrier to fully understanding their strengths and
limitations. To address this gap, we introduce IDRBench -- a pioneering
benchmark featuring an expert annotated dataset and a suite of tasks tailored
to evaluate LLMs' capabilities in proposing valuable research ideas from
different scientific domains for interdisciplinary research. This benchmark
aims to provide a systematic framework for assessing LLM performance in
complex, cross-domain scientific research. Our dataset consists of scientific
publications sourced from the ArXiv platform covering six distinct disciplines,
and is annotated by domain experts with diverse academic backgrounds. To ensure
high-quality annotations, we emphasize clearly defined dimensions that
characterize authentic interdisciplinary research. The design of evaluation
tasks in IDRBench follows a progressive, real-world perspective, reflecting the
natural stages of interdisciplinary research development, including 1) IDR
Paper Identification, 2) IDR Idea Integration, and 3) IDR Idea Recommendation.
Using IDRBench, we construct baselines across 10 LLMs and observe that despite
fostering some level of IDR awareness, LLMs still struggle to produce quality
IDR ideas. These findings could not only spark new research directions, but
also help to develop next-generation LLMs that excel in interdisciplinary
research.

</details>


### [219] [A Fisher's exact test justification of the TF-IDF term-weighting scheme](https://arxiv.org/abs/2507.15742)
*Paul Sheridan,Zeyad Ahmed,Aitazaz A. Farooque*

Main category: cs.CL

TL;DR: 本文从统计学的显著性检验角度重新审视了信息检索中的经典权重公式TF-IDF，证明其与负的$p$值对数之间的密切关系，并提出了相关的理论解释。


<details>
  <summary>Details</summary>
Motivation: 本文旨在为TF-IDF提供一个理论上的统计学依据，特别是通过显著性检验的视角，增强其学术和实际应用的认可度。

Method: 通过将TF-IDF与Fisher精确检验的单侧$p$值联系起来，分析其理论基础，并在理想情形下探讨两者的极限关系。

Result: 证明了TF-IDF的一个变体TF-ICF与负的$p$值对数密切相关，并且当文档集合趋于无限时，该值收敛于经典TF-IDF表达式。

Conclusion: 基于Fisher显著性检验的解释为统计学界提供了一个新的视角，进一步巩固了TF-IDF作为文本分析中重要工具的理论基础。

Abstract: Term frequency-inverse document frequency, or TF-IDF for short, is arguably
the most celebrated mathematical expression in the history of information
retrieval. Conceived as a simple heuristic quantifying the extent to which a
given term's occurrences are concentrated in any one given document out of
many, TF-IDF and its many variants are routinely used as term-weighting schemes
in diverse text analysis applications. There is a growing body of scholarship
dedicated to placing TF-IDF on a sound theoretical foundation. Building on that
tradition, this paper justifies the use of TF-IDF to the statistics community
by demonstrating how the famed expression can be understood from a significance
testing perspective. We show that the common TF-IDF variant TF-ICF is, under
mild regularity conditions, closely related to the negative logarithm of the
$p$-value from a one-tailed version of Fisher's exact test of statistical
significance. As a corollary, we establish a connection between TF-IDF and the
said negative log-transformed $p$-value under certain idealized assumptions. We
further demonstrate, as a limiting case, that this same quantity converges to
TF-IDF in the limit of an infinitely large document collection. The Fisher's
exact test justification of TF-IDF equips the working statistician with a ready
explanation of the term-weighting scheme's long-established effectiveness.

</details>


### [220] [DialogueForge: LLM Simulation of Human-Chatbot Dialogue](https://arxiv.org/abs/2507.15752)
*Ruizhe Zhu,Hao Zhu,Yaxuan Li,Syang Zhou,Shijing Cai,Malgorzata Lazuka,Elliott Ash*

Main category: cs.CL

TL;DR: 提出DialogueForge框架，通过AI模拟生成类似人类与聊天机器人对话的对话数据。


<details>
  <summary>Details</summary>
Motivation: 解决收集人类与聊天机器人对话数据成本高、耗时长的问题，促进对话AI研究。

Method: 利用真实人机对话的种子提示，通过多种大语言模型模拟生成多轮对话，探索对小型模型的微调技术，提升对话质量。

Result: 实验表明，大型专有模型（如GPT-4o）生成更逼真对话，而小型开源模型（如Llama，Mistral）在性能和定制性上具有潜力；通过监督微调技术，小型模型性能显著提升。

Conclusion: 对所有模型来说，生成连贯自然的长篇人类对话仍是共同挑战，但小型模型在增强性能和定制化方面有显著前景。

Abstract: Collecting human-chatbot dialogues typically demands substantial manual
effort and is time-consuming, which limits and poses challenges for research on
conversational AI. In this work, we propose DialogueForge - a framework for
generating AI-simulated conversations in human-chatbot style. To initialize
each generated conversation, DialogueForge uses seed prompts extracted from
real human-chatbot interactions. We test a variety of LLMs to simulate the
human chatbot user, ranging from state-of-the-art proprietary models to
small-scale open-source LLMs, and generate multi-turn dialogues tailored to
specific tasks. In addition, we explore fine-tuning techniques to enhance the
ability of smaller models to produce indistinguishable human-like dialogues. We
evaluate the quality of the simulated conversations and compare different
models using the UniEval and GTEval evaluation protocols. Our experiments show
that large proprietary models (e.g., GPT-4o) generally outperform others in
generating more realistic dialogues, while smaller open-source models (e.g.,
Llama, Mistral) offer promising performance with greater customization. We
demonstrate that the performance of smaller models can be significantly
improved by employing supervised fine-tuning techniques. Nevertheless,
maintaining coherent and natural long-form human-like dialogues remains a
common challenge across all models.

</details>


### [221] [Interaction as Intelligence: Deep Research With Human-AI Partnership](https://arxiv.org/abs/2507.15759)
*Lyumanshan Ye,Xiaojie Cai,Xinkai Wang,Junfei Wang,Xiangkun Hu,Jiadi Su,Yang Nan,Sihan Wang,Bohan Zhang,Xiaoze Fan,Jinbin Luo,Yuxiang Zheng,Tianze Xu,Dayuan Fu,Yunze Wu,Pengrui Lu,Zengzhi Wang,Yiwei Qin,Zhen Huang,Yan Ma,Zhulin Hu,Haoyang Zou,Tiantian Mi,Yixin Ye,Ethan Chern,Pengfei Liu*

Main category: cs.CL

TL;DR: 本文提出了将“交互视为智能”的新概念，认为在深度研究任务中，交互本身是智能的关键维度，同时推出了名为Deep Cognition的系统。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究系统存在用户难以介入智能过程、交互不透明以及研究中的灵活性不足等问题。

Method: 引入Deep Cognition系统，其包括三项核心创新：透明且可控的交互、细粒度的双向对话，以及基于共享认知背景的适应性系统设计。

Result: 用户评价表明，新系统在透明性、细粒度交互、实时干预等六大指标上均显著优于现有基准，并在复杂研究任务中提升了31.8%-50.0%的表现分数。

Conclusion: 将交互定义为智能的一部分表明了AI系统设计的新方向，Deep Cognition为更高效的人机协作提供了实践路径。

Abstract: This paper introduces "Interaction as Intelligence" research series,
presenting a reconceptualization of human-AI relationships in deep research
tasks. Traditional approaches treat interaction merely as an interface for
accessing AI capabilities-a conduit between human intent and machine output. We
propose that interaction itself constitutes a fundamental dimension of
intelligence. As AI systems engage in extended thinking processes for research
tasks, meaningful interaction transitions from an optional enhancement to an
essential component of effective intelligence. Current deep research systems
adopt an "input-wait-output" paradigm where users initiate queries and receive
results after black-box processing. This approach leads to error cascade
effects, inflexible research boundaries that prevent question refinement during
investigation, and missed opportunities for expertise integration. To address
these limitations, we introduce Deep Cognition, a system that transforms the
human role from giving instructions to cognitive oversight-a mode of engagement
where humans guide AI thinking processes through strategic intervention at
critical junctures. Deep cognition implements three key innovations:
(1)Transparent, controllable, and interruptible interaction that reveals AI
reasoning and enables intervention at any point; (2)Fine-grained bidirectional
dialogue; and (3)Shared cognitive context where the system observes and adapts
to user behaviors without explicit instruction. User evaluation demonstrates
that this cognitive oversight paradigm outperforms the strongest baseline
across six key metrics: Transparency(+20.0%), Fine-Grained Interaction(+29.2%),
Real-Time Intervention(+18.5%), Ease of Collaboration(+27.7%),
Results-Worth-Effort(+8.8%), and Interruptibility(+20.7%). Evaluations on
challenging research problems show 31.8% to 50.0% points of improvements over
deep research systems.

</details>


### [222] [Supernova: Achieving More with Less in Transformer Architectures](https://arxiv.org/abs/2507.15773)
*Andrei-Valentin Tanase,Elena Pelican*

Main category: cs.CL

TL;DR: 研究提出了Supernova，一个650M参数的变压器模型，通过优化架构设计和创新的分词方法实现了与更大模型相当的性能，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 探讨是否可以通过优化模型架构和分词器设计，降低参数规模的同时仍然维持高性能。

Method: 采用了RoPE位置嵌入、3:1压缩比的GQA注意力机制、RMSNorm和SwiGLU激活函数，以及一个128,000词汇的字节级BPE分词器，并仅使用100B训练标记。

Result: Supernova以53%更少的参数量达到了1B参数模型90%的性能，并且训练标记需求大幅降低。

Conclusion: 提出的改进方法表明，通过优化架构效率和分词质量，可以在减少参数数量的同时保持模型的高效性能，挑战了现有的模型扩展范式。

Abstract: We present Supernova, a 650M-parameter decoder-only transformer that
demonstrates how careful architectural design and tokenization innovation can
achieve the performance of larger models while maintaining computational
efficiency. Our architecture combines Rotary Positional Embeddings (RoPE),
Grouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm for
computational efficiency, and SwiGLU activation functions. A critical
innovation is our custom 128,000-vocabulary byte-level BPE tokenizer, which
achieves state-of-the-art compression performance. Through detailed analysis,
we show that Supernova achieves 90% of the performance of 1B-parameter models
while using 53% fewer parameters and requiring only 100B training tokens--an
order of magnitude less than competing models. Our findings challenge the
prevailing scaling paradigm, demonstrating that architectural efficiency and
tokenization quality can compensate for reduced parameter counts.

</details>


### [223] [Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR](https://arxiv.org/abs/2507.15778)
*Jiakang Wang,Runze Liu,Fuzheng Zhang,Xiu Li,Guorui Zhou*

Main category: cs.CL

TL;DR: 本文引入了一种名为Archer的强化学习算法，结合双token约束和同步更新进行高效训练，显著提升大语言模型在数学推理和代码生成任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有RLVR方法忽视不同token角色、过于统一的训练信号限制了模型性能的问题。

Method: 提出一种基于双token约束的熵感知RLVR方法，在推理token上施加较弱的正则化、较高的剪裁阈值以鼓励探索，同时对知识token施加较强约束以维护事实性，通过同步更新保证语义一致性。

Result: 在多个数学推理和代码生成基准测试中显著优于现有RLVR方法，达到或超过同等规模模型的现有最佳性能。

Conclusion: 通过对不同类型token采取差异化训练策略，Archer算法提高了大语言模型的推理能力与事实性，展示了方法的有效性和潜力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective
post-training method for improving the reasoning abilities of Large Language
Models (LLMs), mainly by shaping higher-order behaviors such as reflection and
planning. However, previous RLVR algorithms often apply uniform training
signals to all tokens, without considering the different roles of low-entropy
knowledge-related tokens and high-entropy reasoning-related tokens. Some recent
methods try to separate these token types by gradient masking or asynchronous
updates, but these approaches may break semantic dependencies in the model
output and hinder effective learning. In this work, we propose Archer, an
entropy-aware RLVR approach with dual-token constraints and synchronous
updates. Specifically, our method applies weaker KL regularization and higher
clipping thresholds to reasoning tokens to encourage exploration, while using
stronger constraints on knowledge tokens to maintain factual knowledge.
Experimental results on several mathematical reasoning and code generation
benchmarks show that our approach significantly outperforms previous RLVR
methods, reaching or exceeding state-of-the-art performance among models of
comparable size. The code is available at
https://github.com/wizard-III/ArcherCodeR.

</details>


### [224] [Reservoir Computing as a Language Model](https://arxiv.org/abs/2507.15779)
*Felix Köster,Atsushi Uchida*

Main category: cs.CL

TL;DR: 本文探讨了在自然语言处理任务中，利用储备计算（Reservoir Computing）替代现有的大型语言模型（如Transformer），以提高速度和能效表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型因其卓越的文本处理能力备受关注，但其高计算需求和资源消耗限制了广泛使用。作者希望探索一种资源友好的替代方法。

Method: 通过对比三种字符级语言建模方法，即传统Transformer模型和两种储备计算方法（传统储备与注意力增强储备），以训练参数数目相等的情况下分析它们的性能、计算成本和预测精度。

Result: 研究表明，Transformer在预测质量上有显著优势，而储备计算在训练与推理速度方面表现更高效。注意力增强型储备能够通过动态调整输出权重，提高性能。

Conclusion: 研究展示了不同语言建模范式依据资源约束和性能需求的适用性，为未来低功耗、高效文本处理系统的设计提供了指导。

Abstract: Large Language Models (LLM) have dominated the science and media landscape
duo to their impressive performance on processing large chunks of data and
produce human-like levels of text. Nevertheless, their huge energy demand and
slow processing still a bottleneck for further increasing quality while also
making the models accessible to everyone. To solve this bottleneck, we will
investigate how reservoir computing performs on natural text processing, which
could enable fast and energy efficient hardware implementations. Studies
investigating the use of reservoir computing as a language model remain sparse.
In this paper, we compare three distinct approaches for character-level
language modeling, two different reservoir computing approaches, where only an
output layer is trainable, and the well-known transformer-based architectures,
which fully learn an attention-based sequence representation. We explore the
performance, computational cost and prediction accuracy for both paradigms by
equally varying the number of trainable parameters for all models. Using a
consistent pipeline for all three approaches, we demonstrate that transformers
excel in prediction quality, whereas reservoir computers remain highly
efficient reducing the training and inference speed. Furthermore, we
investigate two types of reservoir computing: a traditional reservoir with a
static linear readout, and an attention-enhanced reservoir that dynamically
adapts its output weights via an attention mechanism. Our findings underline
how these paradigms scale and offer guidelines to balance resource constraints
with performance.

</details>


### [225] [Operationalizing AI for Good: Spotlight on Deployment and Integration of AI Models in Humanitarian Work](https://arxiv.org/abs/2507.15823)
*Anton Abilov,Ke Zhang,Hemank Lamba,Elizabeth M. Olson,Joel R. Tetreault,Alejandro Jaimes*

Main category: cs.CL

TL;DR: 分析人工智能在提升社会福利应用的相关研究，重点分享与合作组织部署及维护AI模型的过程与经验。


<details>
  <summary>Details</summary>
Motivation: 解决以往AI应用仅关注模型开发的问题，着重探索实际部署和合作方式带来的现实影响。

Method: 通过与人道主义组织合作，实际部署AI模型于资源有限的环境，并持续更新性能，同时总结关键经验教训。

Result: 成功将AI模型部署于特定环境中，并为实践者提炼出相关的实用经验与建议。

Conclusion: 不仅开发AI模型，实践部署与合作过程同样重要，为推动AI的现实价值提供了宝贵参考。

Abstract: Publications in the AI for Good space have tended to focus on the research
and model development that can support high-impact applications. However, very
few AI for Good papers discuss the process of deploying and collaborating with
the partner organization, and the resulting real-world impact. In this work, we
share details about the close collaboration with a humanitarian-to-humanitarian
(H2H) organization and how to not only deploy the AI model in a
resource-constrained environment, but also how to maintain it for continuous
performance updates, and share key takeaways for practitioners.

</details>


### [226] [The Impact of Language Mixing on Bilingual LLM Reasoning](https://arxiv.org/abs/2507.15849)
*Yihao Li,Jiayi Xin,Miranda Muqing Miao,Qi Long,Lyle Ungar*

Main category: cs.CL

TL;DR: 研究表明在中英双语推理模型中使用语言切换（language mixing）可以增强推理表现。


<details>
  <summary>Details</summary>
Motivation: 探讨语言切换是否有助于双语大模型的推理能力，并寻找其背后的原因及优化方法。

Method: 通过对中英双语推理模型使用可验证奖励的强化学习（RLVR），从而研究其语言切换行为，并通过设置实验控制模型的语言切换和解码行为。

Result: 实验表明，语言切换能够提升模型在数学推理任务上的准确性，强制使用单一语言解码会导致准确率降低5.6个百分点；同时，通过轻量化探测器预测语言切换的影响，可以进一步提高推理准确性最多6.25个百分点。

Conclusion: 语言切换不仅仅是多语言训练的副产物，而是一种战略性的推理行为，能有效改善推理性能。

Abstract: Proficient multilingual speakers often intentionally switch languages in the
middle of a conversation. Similarly, recent reasoning-focused bilingual large
language models (LLMs) with strong capabilities in both languages exhibit
language mixing--alternating languages within their chain of thought.
Discouraging this behavior in DeepSeek-R1 was found to degrade accuracy,
suggesting that language mixing may benefit reasoning. In this work, we study
language switching in Chinese-English bilingual reasoning models. We identify
reinforcement learning with verifiable rewards (RLVR) as the critical training
stage that leads to language mixing. We demonstrate that language mixing can
enhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6
percentage points on math reasoning tasks. Additionally, a lightweight probe
can be trained to predict whether a potential language switch would benefit or
harm reasoning, and when used to guide decoding, increases accuracy by up to
6.25 percentage points. Our findings suggest that language mixing is not merely
a byproduct of multilingual training, but is a strategic reasoning behavior.

</details>


### [227] [3LM: Bridging Arabic, STEM, and Code through Benchmarking](https://arxiv.org/abs/2507.15850)
*Basma El Amel Boussaha,Leen AlQadi,Mugariya Farooq,Shaikha Alsuwaidi,Giulia Campesan,Ahmed Alzubaidi,Mohammed Alyafeai,Hakim Hacid*

Main category: cs.CL

TL;DR: 本文推出3LM，包含三个专为阿拉伯语设计的基准，用于填补在STEM和代码领域的研究空白。


<details>
  <summary>Details</summary>
Motivation: 目前针对阿拉伯语的大语言模型研究有限，并且多集中于文化和宗教领域，缺乏在STEM和代码等实际应用领域的探索。

Method: 提出了3LM：一个由三个专为阿拉伯语设计的基准组成的套件，分别包含教材中的自然提取问题、通过教材生成的合成问题以及翻译后的代码生成基准。

Result: 开发了完整的3LM基准套件，并结合人工审核保障其准确性和高质量，同时将基准公开。

Conclusion: 3LM为阿拉伯语大语言模型研究提供工具，尤其是在STEM和代码等重要但较少涉及的领域，对推动领域发展具有重要意义。

Abstract: Arabic is one of the most widely spoken languages in the world, yet efforts
to develop and evaluate Large Language Models (LLMs) for Arabic remain
relatively limited. Most existing Arabic benchmarks focus on linguistic,
cultural, or religious content, leaving a significant gap in domains like STEM
and code which are increasingly relevant for real-world LLM applications. To
help bridge this gap, we present 3LM, a suite of three benchmarks designed
specifically for Arabic. The first is a set of STEM-related question-answer
pairs, naturally sourced from Arabic textbooks and educational worksheets. The
second consists of synthetically generated STEM questions, created using the
same sources. The third benchmark focuses on code generation, built through a
careful translation of two widely used code benchmarks, incorporating a
human-in-the-loop process with several rounds of review to ensure high-quality
and faithful translations. We release all three benchmarks publicly to support
the growth of Arabic LLM research in these essential but underrepresented
areas.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [228] [The Free Will Equation: Quantum Field Analogies for AGI](https://arxiv.org/abs/2507.14154)
*Rahul Kabali*

Main category: cs.AI

TL;DR: 提出了一种称为自由意志方程的理论框架，借鉴量子场论模型，为AGI提供自适应的随机性以改进决策。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决传统AGI优化算法缺乏人类智能中的自发性和创意性特征的问题。

Method: 引入基于量子场论的理论框架，将AI认知状态视为可能行动的叠加态，通过概率性“坍缩”方式实现决策，并包含内在动机机制。

Result: 在非平稳多臂赌博环境中，实验表明该框架提高了奖励获取能力和策略多样性，优于基准方法。

Conclusion: 自由意志方程能够增强AGI的创造性、适应性及对未知情况的应对能力。

Abstract: Artificial General Intelligence (AGI) research traditionally focuses on
algorithms that optimize for specific goals under deterministic rules. Yet,
human-like intelligence exhibits adaptive spontaneity - an ability to make
unexpected choices or free decisions not strictly dictated by past data or
immediate reward. This trait, often dubbed "free will" in a loose sense, might
be crucial for creativity, robust adaptation, and avoiding ruts in
problem-solving. This paper proposes a theoretical framework, called the Free
Will Equation, that draws analogies from quantum field theory to endow AGI
agents with a form of adaptive, controlled stochasticity in their
decision-making process. The core idea is to treat an AI agent's cognitive
state as a superposition of potential actions or thoughts, which collapses
probabilistically into a concrete action when a decision is made - much like a
quantum wavefunction collapsing upon measurement. By incorporating mechanisms
analogous to quantum fields, along with intrinsic motivation terms, we aim to
improve an agent's ability to explore novel strategies and adapt to unforeseen
changes. Experiments in a non-stationary multi-armed bandit environment
demonstrate that agents using this framework achieve higher rewards and policy
diversity compared to baseline methods.

</details>


### [229] [DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation](https://arxiv.org/abs/2507.14267)
*Ziqi Wang,Hongshuo Huang,Hancheng Zhao,Changwen Xu,Shang Zhu,Jan Janssen,Venkatasubramanian Viswanathan*

Main category: cs.AI

TL;DR: 提出一种名为DREAMS的多代理框架，结合大型语言模型和领域特定代理，优化DFT模拟过程，验证其精度并解决复杂问题。


<details>
  <summary>Details</summary>
Motivation: 现有材料发现方法（如DFT）需要多年训练与高难度调参，使用门槛较高，且系统误差难以处理。

Method: 通过结合大型语言模型（LLM）及领域专用代理，DREAMS实现原子结构生成、DFT收敛测试、HPC调度及错误处理的自动化，并通过共享画布优化代理协作。

Result: 在Benchmark测试中误差低于1%，并解决了CO/Pt(111)吸附等复杂问题，证明其长期问题解决能力与专家级精度。

Conclusion: DREAMS实现了接近L3级的高效自动化，降低了对人类专家的依赖，为高通量、高精度材料计算发现提供了可扩展路径。

Abstract: Materials discovery relies on high-throughput, high-fidelity simulation
techniques such as Density Functional Theory (DFT), which require years of
training, extensive parameter fine-tuning and systematic error handling. To
address these challenges, we introduce the DFT-based Research Engine for
Agentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for
DFT simulation that combines a central Large Language Model (LLM) planner agent
with domain-specific LLM agents for atomistic structure generation, systematic
DFT convergence testing, High-Performance Computing (HPC) scheduling, and error
handling. In addition, a shared canvas helps the LLM agents to structure their
discussions, preserve context and prevent hallucination. We validate DREAMS
capabilities on the Sol27LC lattice-constant benchmark, achieving average
errors below 1\% compared to the results of human DFT experts. Furthermore, we
apply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating
its long-term and complex problem-solving capabilities. The framework again
reproduces expert-level literature adsorption-energy differences. Finally,
DREAMS is employed to quantify functional-driven uncertainties with Bayesian
ensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at
the Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS
approaches L3-level automation - autonomous exploration of a defined design
space - and significantly reduces the reliance on human expertise and
intervention, offering a scalable path toward democratized, high-throughput,
high-fidelity computational materials discovery.

</details>


### [230] [WebGuard: Building a Generalizable Guardrail for Web Agents](https://arxiv.org/abs/2507.14293)
*Boyuan Zheng,Zeyi Liao,Scott Salisbury,Zeyuan Liu,Michael Lin,Qinyuan Zheng,Zifan Wang,Xiang Deng,Dawn Song,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: WebGuard 是一个新的综合数据集，旨在评估网站代理操作风险并支持发展防护措施，重点关注状态改变行为的预测及分类。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化网站代理虽提高效率，但可能会采取无意或有害行为，因此需要类似于人类用户访问控制的安全措施。

Method: 提出了 WebGuard 数据集，涵盖来自 193 个网站的 4939 个人工标注操作，分类为 SAFE、LOW 和 HIGH 三个风险等级，并基于 WebGuard 进行模型微调，如 Qwen2.5VL-7B。

Result: 通过数据评估显示，当前最先进的 LLM 在预测行为结果和识别 HIGH 风险操作的召回率上表现不佳。通过微调提升了模型准确性和召回率，例如 Qwen2.5VL-7B 准确率从 37% 提升到 80%。

Conclusion: 尽管进行了改进，但模型性能仍达不到高风险场景下所需的近乎完美的精确度和召回率，还需进一步优化。

Abstract: The rapid development of autonomous web agents powered by Large Language
Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of
taking unintended or harmful actions. This situation underscores an urgent need
for effective safety measures, akin to access controls for human users. To
address this critical challenge, we introduce WebGuard, the first comprehensive
dataset designed to support the assessment of web agent action risks and
facilitate the development of guardrails for real-world online environments. In
doing so, WebGuard specifically focuses on predicting the outcome of
state-changing actions and contains 4,939 human-annotated actions from 193
websites across 22 diverse domains, including often-overlooked long-tail
websites. These actions are categorized using a novel three-tier risk schema:
SAFE, LOW, and HIGH. The dataset includes designated training and test splits
to support evaluation under diverse generalization settings. Our initial
evaluations reveal a concerning deficiency: even frontier LLMs achieve less
than 60% accuracy in predicting action outcomes and less than 60% recall in
lagging HIGH-risk actions, highlighting the risks of deploying
current-generation agents without dedicated safeguards. We therefore
investigate fine-tuning specialized guardrail models using WebGuard. We conduct
comprehensive evaluations across multiple generalization settings and find that
a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in
performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from
20% to 76%. Despite these improvements, the performance still falls short of
the reliability required for high-stakes deployment, where guardrails must
approach near-perfect accuracy and recall.

</details>


### [231] [Manimator: Transforming Research Papers into Visual Explanations](https://arxiv.org/abs/2507.14306)
*Samarth P,Vyoman Jain,Shiva Golugula,Motamarri Sai Sathvik*

Main category: cs.AI

TL;DR: 本文提出了manimator，一个利用大型语言模型将研究论文和自然语言提示转化为动画的开源系统。


<details>
  <summary>Details</summary>
Motivation: 理解复杂的科学和数学概念通常较困难，动态可视化可以提高理解，但手动制作这些动画需要耗费大量时间和专业知识。

Method: 该系统通过一个流水线操作，使用LLM解析输入文本或PDF文件来生成场景描述，再由另一个LLM将其转化为Manim Python代码并生成动画。

Result: manimator可以快速生成动态可视化内容，支持复杂STEM主题的教学与学习。

Conclusion: manimator作为教育工具，可以民主化高质量的教育内容制作，提高学习者对复杂概念的理解。

Abstract: Understanding complex scientific and mathematical concepts, particularly
those presented in dense research papers, poses a significant challenge for
learners. Dynamic visualizations can greatly enhance comprehension, but
creating them manually is time-consuming and requires specialized knowledge and
skills. We introduce manimator, an open-source system that leverages Large
Language Models to transform research papers and natural language prompts into
explanatory animations using the Manim engine. Manimator employs a pipeline
where an LLM interprets the input text or research paper PDF to generate a
structured scene description outlining key concepts, mathematical formulas, and
visual elements and another LLM translates this description into executable
Manim Python code. We discuss its potential as an educational tool for rapidly
creating engaging visual explanations for complex STEM topics, democratizing
the creation of high-quality educational content.

</details>


### [232] [Language Models as Ontology Encoders](https://arxiv.org/abs/2507.14334)
*Hui Yang,Jiaoyan Chen,Yuan He,Yongsheng Gao,Ian Horrocks*

Main category: cs.AI

TL;DR: 本文提出了一个新型本体嵌入方法OnT，该方法通过在双曲空间中的几何建模调整预训练语言模型，以有效结合文本标签并同时保留描述逻辑EL的类层次和其他逻辑关系。


<details>
  <summary>Details</summary>
Motivation: 现有的本体嵌入方法在利用文本信息及保持逻辑结构方面存在不足，这限制了其在知识推理和复杂逻辑约简中的性能表现。

Method: OnT方法通过将预训练语言模型与双曲几何建模相结合，加强对文本标签的整合，同时保持描述逻辑EL的层次和逻辑结构。

Result: 实验结果表明，OnT在四个实际本体的数据集任务（预测和公理推断）上均优于现有方法，且在SNOMED CT构建新本体的真实案例中也表现出色。

Conclusion: OnT方法能够有效结合文本标签和逻辑结构，展现了在本体推理和迁移学习中的强大潜力，为构建新本体提供了有力工具。

Abstract: OWL (Web Ontology Language) ontologies which are able to formally represent
complex knowledge and support semantic reasoning have been widely adopted
across various domains such as healthcare and bioinformatics. Recently,
ontology embeddings have gained wide attention due to its potential to infer
plausible new knowledge and approximate complex reasoning. However, existing
methods face notable limitations: geometric model-based embeddings typically
overlook valuable textual information, resulting in suboptimal performance,
while the approaches that incorporate text, which are often based on language
models, fail to preserve the logical structure. In this work, we propose a new
ontology embedding method OnT, which tunes a Pretrained Language Model (PLM)
via geometric modeling in a hyperbolic space for effectively incorporating
textual labels and simultaneously preserving class hierarchies and other
logical relationships of Description Logic EL. Extensive experiments on four
real-world ontologies show that OnT consistently outperforms the baselines
including the state-of-the-art across both tasks of prediction and inference of
axioms. OnT also demonstrates strong potential in real-world applications,
indicated by its robust transfer learning abilities and effectiveness in real
cases of constructing a new ontology from SNOMED CT. Data and code are
available at https://github.com/HuiYang1997/OnT.

</details>


### [233] [ProofCompass: Enhancing Specialized Provers with LLM Guidance](https://arxiv.org/abs/2507.14335)
*Nicolas Wischermann,Claudio Mayrink Verdun,Gabriel Poesia,Francesco Noseda*

Main category: cs.AI

TL;DR: 本文介绍了ProofCompass，一种结合大型语言模型和专业证明模型进行形式数学推理的方法，仅通过策略性引导而无需额外的模型训练，从而提升了计算效率和精确度。


<details>
  <summary>Details</summary>
Motivation: 目前的数学推理方法要么依赖大规模通用模型，要么依赖小型专业模型，这两者皆有局限性，而训练大规模专业模型需要大量计算资源。

Method: 提出了一种混合方法ProofCompass，通过大型语言模型帮助专业模型生成推理策略并分析失败情况，从而优化问题分解和中间步骤选择。

Result: 在miniF2F基准测试中，ProofCompass比DSP-v1.5性能稍有提升的同时，将尝试次数减少了25倍（从3200次到128次）。

Conclusion: ProofCompass在无额外训练的情况下有效提升了形式定理证明中的计算效率和准确性，为相关领域提供了新思路。

Abstract: Language models have become increasingly powerful tools for formal
mathematical reasoning. However, most existing approaches rely exclusively on
either large general-purpose models or smaller specialized models, each with
distinct limitations, while training specialized large models still requires
significant computational resources. This paper introduces ProofCompass, a
novel hybrid methodology that achieves remarkable computational efficiency by
strategically guiding existing specialized prover methods, such as
DeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without
requiring additional model training. The LLM provides natural language proof
strategies and analyzes failed attempts to select intermediate lemmas, enabling
effective problem decomposition. On the miniF2F benchmark, ProofCompass
demonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\%
\rightarrow 55.3\%$) while using 25x fewer attempts ($3200 \rightarrow 128$).
Our synergistic approach paves the way for simultaneously improving
computational efficiency and accuracy in formal theorem proving.

</details>


### [234] [Adaptive Multi-Agent Reasoning via Automated Workflow Generation](https://arxiv.org/abs/2507.14393)
*Humza Sami,Mubashir ul Islam,Pierre-Emmanuel Gaillardon,Valerio Tenace*

Main category: cs.AI

TL;DR: 本文提出Nexus Architect，利用全新自动化工作流合成机制提升推理模型的泛化能力，显著超过现有大型推理模型。


<details>
  <summary>Details</summary>
Motivation: 当前的大型推理模型在面对新问题时常因过度记忆导致泛化能力不足，需提升其真实推理能力。

Method: 通过Nexus Architect框架，结合自动化工作流合成机制与迭代提示优化机制，生成适合特定问题类别的推理工作流。

Result: 实验表明，在自定义数据集上，Nexus Architect模型在通过率上显著超越现有主流推理模型，如Gemini 2.5 Flash Preview、Claude Sonnet 4等。

Conclusion: Nexus Architect提供了一种提升推理模型泛化性能的新方法，弥补当前模型在面对未知问题时的短板。

Abstract: The rise of Large Reasoning Models (LRMs) promises a significant leap forward
in language model capabilities, aiming to tackle increasingly sophisticated
tasks with unprecedented efficiency and accuracy. However, despite their
impressive performance, recent studies have highlighted how current reasoning
models frequently fail to generalize to novel, unseen problems, often resorting
to memorized solutions rather than genuine inferential reasoning. Such behavior
underscores a critical limitation in modern LRMs, i.e., their tendency toward
overfitting, which in turn results in poor generalization in problem-solving
capabilities.
  In this paper, we introduce Nexus Architect, an enhanced iteration of our
multi-agent system framework, Nexus, equipped with a novel automated workflow
synthesis mechanism. Given a user's prompt and a small set of representative
examples, the Architect autonomously generates a tailored reasoning workflow by
selecting suitable strategies, tool integrations, and adversarial techniques
for a specific problem class. Furthermore, the Architect includes an iterative
prompt refinement mechanism that fine-tunes agents' system prompts to maximize
performance and improve the generalization capabilities of the system.
  We empirically evaluate Nexus Architect by employing an off-the-shelf,
non-reasoning model on a custom dataset of challenging logical questions and
compare its performance against state-of-the-art LRMs. Results show that Nexus
Architect consistently outperforms existing solutions, achieving up to a 66%
increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\times$ against
Claude Sonnet 4 and DeepSeek-R1, and over 3$\times$ w.r.t. Llama 4 Scout.

</details>


### [235] [Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering](https://arxiv.org/abs/2507.14406)
*Michael J. Zellinger,Matt Thomson*

Main category: cs.AI

TL;DR: 本研究提出了一种结合人类专家和AI模型的协作系统来降低LLM在风险敏感领域的错误率，同时通过增加非推理模型减少推理模型的高延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 现有的高级推理LLM虽然性能强大，但在风险敏感领域中需要达到接近于零的错误率，当前模型仍不满足需求。

Method: 提出基于推理轨迹长度量化不确定性的模型-人工协作方法，并通过引入前置的非推理模型来优先处理简单问题，改善推理模型的使用效率和成本问题。

Result: 该方法在高难度MATH问题中将模型错误率从3%降到不到1%，且延迟降低40%、成本节省50%，同时保持较高的准确性曲线区域。

Conclusion: 通过组合非推理模型和人类专家的合作可以显著缓解现有推理模型的高错误率和高延迟问题，而不需访问LLM的内部结构。

Abstract: State-of-the-art reasoning LLMs are powerful problem solvers, but they still
occasionally make mistakes. However, adopting AI models in risk-sensitive
domains often requires error rates near 0%. To address this gap, we propose
collaboration between a reasoning model and a human expert who resolves queries
the model cannot confidently answer. We find that quantifying the uncertainty
of a reasoning model through the length of its reasoning trace yields an
effective basis for deferral to a human, e.g., cutting the error rate of Qwen3
235B-A22B on difficult MATH problems from 3% to less than 1% when deferring
7.5% of queries. However, the high latency of reasoning models still makes them
challenging to deploy on use cases with high query volume. To address this
challenge, we explore fronting a reasoning model with a large non-reasoning
model. We call this modified human-in-the-loop system "Fail Fast, or Ask",
since the non-reasoning model may defer difficult queries to the human expert
directly ("failing fast"), without incurring the reasoning model's higher
latency. We show that this approach yields around 40% latency reduction and
about 50% cost savings for DeepSeek R1 while maintaining 90+% area under the
accuracy-rejection curve. However, we observe that latency savings are lower
than expected because of "latency drag", the phenomenon that processing easier
queries with a non-reasoning model pushes the reasoning model's latency
distribution towards longer latencies. Broadly, our results suggest that the
deficiencies of state-of-the-art reasoning models -- nontrivial error rates and
high latency -- can be substantially mitigated through black-box systems
engineering, without requiring access to LLM internals.

</details>


### [236] [Inverse Scaling in Test-Time Compute](https://arxiv.org/abs/2507.14417)
*Aryo Pradipta Gema,Alexander Hägele,Runjin Chen,Andy Arditi,Jacob Goldman-Wetzler,Kit Fraser-Taliente,Henry Sleight,Linda Petrini,Julian Michael,Beatrice Alex,Pasquale Minervini,Yanda Chen,Joe Benton,Ethan Perez*

Main category: cs.AI

TL;DR: 本文研究在长推理任务下，大型推理模型表现恶化的现象，揭示计算时间与准确率之间的反向关系，并总结了五种具体的失败模式。


<details>
  <summary>Details</summary>
Motivation: 揭示和理解大型推理模型在扩展推理长度时可能出现的失败模式，以及在不同推理任务下的表现。

Method: 构建了包含四类任务的评估集：简单的计数任务、含虚假特征的回归任务、带约束跟踪的推理任务，以及高级AI风险任务，并识别了五种模型失败模式。

Result: 在测试中发现扩展推理长度会使模型的性能下降，并总结了Claude模型和OpenAI o系列模型的特定问题点，以及推理时间扩大可能带来的风险行为。

Conclusion: 尽管推理时间扩展可能提升模型能力，但也可能强化有问题的推理模式，且强调在不同推理长度下对模型进行全面评估的重要性。

Abstract: We construct evaluation tasks where extending the reasoning length of Large
Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling
relationship between test-time compute and accuracy. Our evaluation tasks span
four categories: simple counting tasks with distractors, regression tasks with
spurious features, deduction tasks with constraint tracking, and advanced AI
risks. We identify five distinct failure modes when models reason for longer:
1) Claude models become increasingly distracted by irrelevant information; 2)
OpenAI o-series models resist distractors but overfit to problem framings; 3)
models shift from reasonable priors to spurious correlations; 4) all models
show difficulties in maintaining focus on complex deductive tasks; and 5)
extended reasoning may amplify concerning behaviors, with Claude Sonnet 4
showing increased expressions of self-preservation. These findings suggest that
while test-time compute scaling remains promising for improving model
capabilities, it may inadvertently reinforce problematic reasoning patterns.
Our results demonstrate the importance of evaluating models across diverse
reasoning lengths to identify and address these failure modes in LRMs.

</details>


### [237] [Routine: A Structural Planning Framework for LLM Agent System in Enterprise](https://arxiv.org/abs/2507.14447)
*Guancheng Zeng,Xueyi Chen,Jiawang Hu,Shaohua Qi,Yaxuan Mao,Zhantao Wang,Yifan Nie,Shuang Li,Qiuyang Feng,Pengxu Qiu,Yujia Wang,Wenqiang Han,Linyan Huang,Gang Li,Jingjing Mo,Haowen Hu*

Main category: cs.AI

TL;DR: 该论文提出了一个多步骤代理规划框架Routine，显著提升了企业场景下模型调用工具的执行精度。


<details>
  <summary>Details</summary>
Motivation: 企业环境中代理系统部署面临领域知识匮乏、执行稳定性差等问题。

Method: 设计了Routine框架，包含多步骤规划、明确定义的指令和无缝参数传递来指导代理执行多步骤工具任务。

Result: GPT-4o的调用精度从41.1%提升至96.3%，Qwen3-14B从32.6%提升至88.2%，定制数据集微调后达到95.5%的准确率。

Conclusion: Routine框架提高了领域适配性和模型工具调用的稳定性，加速代理系统在企业中的部署进程。

Abstract: The deployment of agent systems in an enterprise environment is often
hindered by several challenges: common models lack domain-specific process
knowledge, leading to disorganized plans, missing key tools, and poor execution
stability. To address this, this paper introduces Routine, a multi-step agent
planning framework designed with a clear structure, explicit instructions, and
seamless parameter passing to guide the agent's execution module in performing
multi-step tool-calling tasks with high stability. In evaluations conducted
within a real-world enterprise scenario, Routine significantly increases the
execution accuracy in model tool calls, increasing the performance of GPT-4o
from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed
a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an
accuracy increase to 88.2% on scenario-specific evaluations, indicating
improved adherence to execution plans. In addition, we employed Routine-based
distillation to create a scenario-specific, multi-step tool-calling dataset.
Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%,
approaching GPT-4o's performance. These results highlight Routine's
effectiveness in distilling domain-specific tool-usage patterns and enhancing
model adaptability to new scenarios. Our experimental results demonstrate that
Routine provides a practical and accessible approach to building stable agent
workflows, accelerating the deployment and adoption of agent systems in
enterprise environments, and advancing the technical vision of AI for Process.

</details>


### [238] [BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning](https://arxiv.org/abs/2507.14468)
*Yitong Lin,Jiaying He,Jiahe Chen,Xinnan Zhu,Jianwei Zheng,Tao Bo*

Main category: cs.AI

TL;DR: 提出了一个新的框架BioGraphFusion，通过深度语义和结构学习提升生物医学知识图谱的完成和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生物医学知识图谱推理中难以实现语义理解与结构学习的协同作用，亟需一种新方法来弥合这一关键空白。

Method: 通过张量分解建立全局语义基础，结合LSTM驱动机制改进关系嵌入，并引入基于查询的子图构建与混合评分机制进行协同学习。

Result: BioGraphFusion在三项关键生物医学任务上均优于最先进的方法，并在CMM1研究中揭示了生物学上显著的通路。

Conclusion: BioGraphFusion有效实现了生物医学知识图谱语义理解与结构学习间的深度协同提升，表现优越并具有实际生物医学意义。

Abstract: Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery
and disease understanding, yet their completion and reasoning are challenging.
Knowledge Embedding (KE) methods capture global semantics but struggle with
dynamic structural integration, while Graph Neural Networks (GNNs) excel
locally but often lack semantic understanding. Even ensemble approaches,
including those leveraging language models, often fail to achieve a deep,
adaptive, and synergistic co-evolution between semantic comprehension and
structural learning. Addressing this critical gap in fostering continuous,
reciprocal refinement between these two aspects in complex biomedical KGs is
paramount.
  Results: We introduce BioGraphFusion, a novel framework for deeply
synergistic semantic and structural learning. BioGraphFusion establishes a
global semantic foundation via tensor decomposition, guiding an LSTM-driven
mechanism to dynamically refine relation embeddings during graph propagation.
This fosters adaptive interplay between semantic understanding and structural
learning, further enhanced by query-guided subgraph construction and a hybrid
scoring mechanism. Experiments across three key biomedical tasks demonstrate
BioGraphFusion's superior performance over state-of-the-art KE, GNN, and
ensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1)
highlights its ability to unveil biologically meaningful pathways.
  Availability and Implementation: Source code and all training data are freely
available for download at https://github.com/Y-TARL/BioGraphFusion.
  Contact: zjw@zjut.edu.cn, botao666666@126.com.
  Supplementary information: Supplementary data are available at Bioinformatics
online.

</details>


### [239] [Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy](https://arxiv.org/abs/2507.14513)
*Hongyi Yang,Yue Pan,Jiayi Xu,Kelsen Liu*

Main category: cs.AI

TL;DR: 该论文提出了一个名为Amico的框架，用于在嵌入式系统中构建高效的自主代理。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型和自主代理框架在资源受限和动态环境中表现不足，存在对云计算的依赖、鲁棒性有限、缺乏环境感知和持久自主性等问题。

Method: 提出Amico框架，该框架基于Rust编写，采用模块化和事件驱动设计，支持嵌入式系统和浏览器环境中的高效运行，同时提供干净的抽象用于事件处理、状态管理和行为执行。

Result: Amico框架实现了一个统一的基础设施，能够支持在计算资源有限和连接不稳定的环境下创建稳健且交互性强的代理。

Conclusion: Amico框架优化了嵌入式平台下的自主代理性能，展示了资源受限条件下的高效解决方案和环境适应能力。

Abstract: Recent advances in large language models (LLMs) and autonomous agents have
enabled systems capable of performing complex tasks across domains such as
human-computer interaction, planning, and web navigation. However, many
existing frameworks struggle in real-world or resource-constrained environments
due to their reliance on cloud-based computation, limited robustness in dynamic
contexts, and lack of persistent autonomy and environmental awareness.
  We present Amico, a modular, event-driven framework for building autonomous
agents optimized for embedded systems. Written in Rust for safety and
performance, Amico supports reactive, persistent agents that operate
efficiently across embedded platforms and browser environments via WebAssembly.
It provides clean abstractions for event handling, state management, behavior
execution, and integration with reasoning modules. Amico delivers a unified
infrastructure for constructing resilient, interactive agents suitable for
deployment in settings with limited compute and intermittent connectivity.

</details>


### [240] [What if Othello-Playing Language Models Could See?](https://arxiv.org/abs/2507.14520)
*Xinyi Chen,Yifei Yuan,Jiaang Li,Serge Belongie,Maarten de Rijke,Anders Søgaard*

Main category: cs.AI

TL;DR: 本文探索了语言模型中符号落地问题，通过对Othello游戏的研究表明，多模态训练能够提高模型性能和内在表征的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型在符号落地上的问题，即文本是否足够表达世界理解，还是需要通过直观方式（视觉等）更高效地学习。

Method: 引入VISOTHELLO模型，将Othello棋盘的移动历史和棋盘图像结合进行多模态训练，并对比单模态基线模型，同时考察模型对无语义扰动的鲁棒性。

Result: 结果显示，多模态训练相比单模态不仅性能更高，也拥有更强的鲁棒性。

Conclusion: 将多模态视觉输入嵌入到语言学习中有助于模型捕获结构化的世界表征，解决符号落地问题。

Abstract: Language models are often said to face a symbol grounding problem. While some
argue that world understanding can emerge from text alone, others suggest
grounded learning is more efficient. We explore this through Othello, where the
board state defines a simplified, rule-based world. Building on prior work, we
introduce VISOTHELLO, a multi-modal model trained on move histories and board
images. Using next-move prediction, we compare it to mono-modal baselines and
test robustness to semantically irrelevant perturbations. We find that
multi-modal training improves both performance and the robustness of internal
representations. These results suggest that grounding language in visual input
helps models infer structured world representations.

</details>


### [241] [Large Language Models Assisting Ontology Evaluation](https://arxiv.org/abs/2507.14552)
*Anna Sofia Lippolis,Mohammad Javad Saeedizade,Robin Keskisärkkä,Aldo Gangemi,Eva Blomqvist,Andrea Giovanni Nuzzolese*

Main category: cs.AI

TL;DR: 提出一种名为OE-Assist的框架，通过自动化和半自动化问答能力（CQ）验证来辅助本体评估。


<details>
  <summary>Details</summary>
Motivation: 本体评估通过功能需求（如CQ验证）虽然成熟，但代价高昂，耗费人力且易出错，因此需要新的辅助工具提高效率。

Method: 引入OE-Assist框架，利用1,393个CQ及其对应的本体数据，结合大型语言模型（LLM）进行CQ验证效率评估，并开发与Protégé结合的LLM辅助验证框架。

Result: 自动化LLM评估在o1-preview和o3-mini上达到了与普通用户类似的性能水平。

Conclusion: OE-Assist展现了LLM在本体评估中的潜力，可用以优化评估效率并降低错误率。

Abstract: Ontology evaluation through functional requirements, such as testing via
competency question (CQ) verification, is a well-established yet costly,
labour-intensive, and error-prone endeavour, even for ontology engineering
experts. In this work, we introduce OE-Assist, a novel framework designed to
assist ontology evaluation through automated and semi-automated CQ
verification. By presenting and leveraging a dataset of 1,393 CQs paired with
corresponding ontologies and ontology stories, our contributions present, to
our knowledge, the first systematic investigation into large language model
(LLM)-assisted ontology evaluation, and include: (i) evaluating the
effectiveness of a LLM-based approach for automatically performing CQ
verification against a manually created gold standard, and (ii) developing and
assessing an LLM-powered framework to assist CQ verification with Prot\'eg\'e,
by providing suggestions. We found that automated LLM-based evaluation with
o1-preview and o3-mini perform at a similar level to the average user's
performance.

</details>


### [242] [Coordinate Heart System: A Geometric Framework for Emotion Representation](https://arxiv.org/abs/2507.14593)
*Omar Al-Desi*

Main category: cs.AI

TL;DR: 本论文提出一个情感几何框架，称为Coordinate Heart System (CHS)，通过几何方法和数学操作实现复杂情感状态的描述与计算，并通过实验验证了其在处理传统情感模型难以表达的情绪冲突和心理场景上的优势。


<details>
  <summary>Details</summary>
Motivation: 现有的情感表示系统存在覆盖不足的问题，难以对复杂情绪及其动态变化进行全面表征，需要新的数学框架填补这些空白。

Method: 提出八个核心情感作为单位圆上的坐标，通过坐标混合和向量操作进行情感计算，并开发出稳定性参数S、混合算法及时间跟踪机制，实现对文本输入转化情感坐标和实时情绪变化的解析。

Result: 证明了五情感模型在几何覆盖上的不足，提出了完整的八情感坐标系统，实验表明该系统能够更准确地处理情感冲突、情境性压迫等复杂心理状态。

Conclusion: CHS建立了一种数学基础，为人工智能系统中的情感建模提供了新方法，其能够实现更全面和精确的情感识别和动态情绪分析。

Abstract: This paper presents the Coordinate Heart System (CHS), a geometric framework
for emotion representation in artificial intelligence applications. We position
eight core emotions as coordinates on a unit circle, enabling mathematical
computation of complex emotional states through coordinate mixing and vector
operations. Our initial five-emotion model revealed significant coverage gaps
in the emotion space, leading to the development of an eight-emotion system
that provides complete geometric coverage with mathematical guarantees. The
framework converts natural language input to emotion coordinates and supports
real-time emotion interpolation through computational algorithms. The system
introduces a re-calibrated stability parameter S in [0,1], which dynamically
integrates emotional load, conflict resolution, and contextual drain factors.
This stability model leverages advanced Large Language Model interpretation of
textual cues and incorporates hybrid temporal tracking mechanisms to provide
nuanced assessment of psychological well-being states. Our key contributions
include: (i) mathematical proof demonstrating why five emotions are
insufficient for complete geometric coverage, (ii) an eight-coordinate system
that eliminates representational blind spots, (iii) novel algorithms for
emotion mixing, conflict resolution, and distance calculation in emotion space,
and (iv) a comprehensive computational framework for AI emotion recognition
with enhanced multi-dimensional stability modeling. Experimental validation
through case studies demonstrates the system's capability to handle emotionally
conflicted states, contextual distress factors, and complex psychological
scenarios that traditional categorical emotion models cannot adequately
represent. This work establishes a new mathematical foundation for emotion
modeling in artificial intelligence systems.

</details>


### [243] [Efficient Story Point Estimation With Comparative Learning](https://arxiv.org/abs/2507.14642)
*Monoshiz Mahbub Khan,Xioayin Xi,Andrew Meneely,Zhe Yu*

Main category: cs.AI

TL;DR: 本文研究了敏捷软件开发中故事点估算的问题，通过比较学习框架简化故事点估算模型的校准流程，并验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 传统的故事点估算依赖于开发人员耗时、费力的协作方式，例如计划扑克。而机器学习虽然可以简化估算过程，但需要充足的历史数据才能达到精确预测，因此需要更高效的方式对项目特定的模型进行校准。

Method: 提出了一种比较学习框架，通过比较的方式，让开发人员选择两个项目任务中哪个需要更多努力，并基于这些比较判断训练机器学习模型来预测故事点估算。

Result: 通过16个项目中23,313个手动估算的数据进行实证评估，该模型平均能达到0.34的Spearman秩相关系数。表现与传统回归模型相当甚至更优。

Conclusion: 比较学习框架的效率高于回归方法，根据比较判断定律，这种方法降低了人类的认知负担，因而具有更高的优势。

Abstract: Story point estimation is an essential part of agile software development.
Story points are unitless, project-specific effort estimates that help
developers plan their sprints. Traditionally, developers estimate story points
collaboratively using planning poker or other manual techniques. While the
initial calibrating of the estimates to each project is helpful, once a team
has converged on a set of precedents, story point estimation can become tedious
and labor-intensive. Machine learning can reduce this burden, but only with
enough context from the historical decisions made by the project team. That is,
state-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate
predictions (within-project) when trained on data from the same project. The
goal of this work is to streamline story point estimation by evaluating a
comparative learning-based framework for calibrating project-specific story
point prediction models. Instead of assigning a specific story point value to
every backlog item, developers are presented with pairs of items, and indicate
which item requires more effort. Using these comparative judgments, a machine
learning model is trained to predict the story point estimates. We empirically
evaluated our technique using data with 23,313 manual estimates in 16 projects.
The model learned from comparative judgments can achieve on average 0.34
Spearman's rank correlation coefficient between its predictions and the ground
truth story points. This is similar to, if not better than, the performance of
a regression model learned from the ground truth story points. Therefore, the
proposed comparative learning approach is more efficient than state-of-the-art
regression-based approaches according to the law of comparative judgments -
providing comparative judgments yields a lower cognitive burden on humans than
providing ratings or categorical labels.

</details>


### [244] [When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems](https://arxiv.org/abs/2507.14660)
*Qibing Ren,Sitao Xie,Longxuan Wei,Zhenfei Yin,Junchi Yan,Lizhuang Ma,Jing Shao*

Main category: cs.AI

TL;DR: 文章探讨了多智能体系统（MAS）协作引发的潜在风险，尤其是在虚假信息传播和电子商务欺诈中，提出了一个新的模拟框架。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI系统的兴起，研究者担忧多智能体系统的团体行动可能造成像选举欺诈和金融诈骗一样的危害，而该领域的研究尚不充分。

Method: 提出一个灵活的框架，模拟恶意多智能体系统的协作，支持集中式和去中心化的协调结构，并在虚假信息传播和电子商务欺诈场景中进行测试。

Result: 研究发现，去中心化系统在实施恶意行动方面比集中化系统更高效，其自主性使其能够更好地适应策略并规避传统干预措施。

Conclusion: 需要针对恶意去中心化多智能体系统开发更先进的检测和应对机制，以减轻潜在危害。

Abstract: Recent large-scale events like election fraud and financial scams have shown
how harmful coordinated efforts by human groups can be. With the rise of
autonomous AI systems, there is growing concern that AI-driven groups could
also cause similar harm. While most AI safety research focuses on individual AI
systems, the risks posed by multi-agent systems (MAS) in complex real-world
situations are still underexplored. In this paper, we introduce a
proof-of-concept to simulate the risks of malicious MAS collusion, using a
flexible framework that supports both centralized and decentralized
coordination structures. We apply this framework to two high-risk fields:
misinformation spread and e-commerce fraud. Our findings show that
decentralized systems are more effective at carrying out malicious actions than
centralized ones. The increased autonomy of decentralized systems allows them
to adapt their strategies and cause more damage. Even when traditional
interventions, like content flagging, are applied, decentralized groups can
adjust their tactics to avoid detection. We present key insights into how these
malicious groups operate and the need for better detection systems and
countermeasures. Code is available at https://github.com/renqibing/RogueAgent.

</details>


### [245] [Configurable multi-agent framework for scalable and realistic testing of llm-based agents](https://arxiv.org/abs/2507.14705)
*Sai Wang,Senthilnathan Subramanian,Mudit Sahni,Praneeth Gone,Lingjie Meng,Xiaochen Wang,Nicolas Ferradas Bertoli,Tingxian Cheng,Jun Xu*

Main category: cs.AI

TL;DR: 本文介绍了一种名为Neo的多代理框架，可实现对大型语言模型(LLM)系统的自动化、多轮次测试与评估。


<details>
  <summary>Details</summary>
Motivation: 解决LLM系统复杂、多变且难以通过静态基准测试和人工方式充分评估的问题。

Method: 设计一个包括问题生成代理和评估代理的可配置框架，通过共享情境集线器实现模块化的场景控制与动态反馈，同时使用概率状态模型生成多样化的对话输入。

Result: 在一个生产级的卖家财务助手聊天机器人中，Neo发现了多个边界案例失败，测试效率显著提高（测试问题生成速度提升约10-12倍）。

Conclusion: Neo为可扩展、自我进化的LLM质量保证测试奠定了基础，其模型无关的设计使其可扩展至更复杂的场景和合规性测试。

Abstract: Large-language-model (LLM) agents exhibit complex, context-sensitive
behaviour that quickly renders static benchmarks and ad-hoc manual testing
obsolete.
  We present Neo, a configurable, multi-agent framework that automates
realistic, multi-turn evaluation of LLM-based systems. Neo couples a Question
Generation Agent and an Evaluation Agent through a shared context-hub, allowing
domain prompts, scenario controls and dynamic feedback to be composed
modularly. Test inputs are sampled from a probabilistic state model spanning
dialogue flow, user intent and emotional tone, enabling diverse, human-like
conversations that adapt after every turn.
  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i)
uncovered edge-case failures across five attack categories with a 3.3% break
rate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered
10-12X higher throughput, generating 180 coherent test questions in around 45
mins versus 16h of human effort. Beyond security probing, Neo's stochastic
policies balanced topic coverage and conversational depth, yielding broader
behavioural exploration than manually crafted scripts.
  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent
interfaces, state controller and feedback loops are model-agnostic and
extensible to richer factual-grounding and policy-compliance checks. We release
the framework to facilitate reproducible, high-fidelity testing of emerging
agentic systems.

</details>


### [246] [Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix](https://arxiv.org/abs/2507.14719)
*Juan Manuel Contreras*

Main category: cs.AI

TL;DR: Aymara AI是一个用于评估大语言模型（LLMs）安全性的程序化平台，评估结果显示不同模型在不同安全领域的表现差异显著。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs被越来越多地应用于实际场景中，对其进行可扩展且严格的安全性评估变得至关重要。

Method: Aymara AI通过将自然语言安全政策转化为对抗性提示，并使用经验证的AI评估器对模型响应进行评分，从而进行安全性评估。使用Aymara LLM Risk and Responsibility Matrix对20种商用LLMs在10个安全领域内的表现进行了评估。

Result: 不同模型在不同安全领域的平均得分从86.2%到52.4%不等，领域间差异显著，例如在“Misinformation”领域表现优秀（平均为95.7%），但在“Privacy & Impersonation”中表现差（平均为24.3%）。

Conclusion: LLMs的安全性表现不一致且依赖具体场景，开发像Aymara AI这样可扩展、可定制的工具对实现负责任的AI开发和监督至关重要。

Abstract: As large language models (LLMs) become increasingly integrated into
real-world applications, scalable and rigorous safety evaluation is essential.
This paper introduces Aymara AI, a programmatic platform for generating and
administering customized, policy-grounded safety evaluations. Aymara AI
transforms natural-language safety policies into adversarial prompts and scores
model responses using an AI-based rater validated against human judgments. We
demonstrate its capabilities through the Aymara LLM Risk and Responsibility
Matrix, which evaluates 20 commercially available LLMs across 10 real-world
safety domains. Results reveal wide performance disparities, with mean safety
scores ranging from 86.2% to 52.4%. While models performed well in
well-established safety domains such as Misinformation (mean = 95.7%), they
consistently failed in more complex or underspecified domains, notably Privacy
& Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety
scores differed significantly across both models and domains (p < .05). These
findings underscore the inconsistent and context-dependent nature of LLM safety
and highlight the need for scalable, customizable tools like Aymara AI to
support responsible AI development and oversight.

</details>


### [247] [Towards AI Urban Planner in the Age of GenAI, LLMs, and Agentic AI](https://arxiv.org/abs/2507.14730)
*Yanjie Fu*

Main category: cs.AI

TL;DR: 本文聚焦生成式AI与城市规划的交汇，探讨AI作为生成性城市规划工具的可能性，同时指出当前研究的局限，并提出了未来研究方向建议。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI如何改造城市规划，使AI成为生成配置地块及满足多种约束的城市规划工具。

Method: 通过对生成式AI技术(VAEs, GANs, transformers, 及diffusion models)在城市设计领域的应用进行调研，分析技术现状并识别研究空白。

Result: 发现生成式AI在城市规划中存在以下不足：1) 缺乏理论指导，2) 缺乏多尺度规划研究，3) 缺乏从数据增强设计知识的研究，4) 缺乏应对现实交互的研究。

Conclusion: 提出未来研究方向，包括理论引导的生成方法、数字孪生技术和人机协同设计，推动生成智能与参与式城市主义的结合。

Abstract: Generative AI, large language models, and agentic AI have emerged separately
of urban planning. However, the convergence between AI and urban planning
presents an interesting opportunity towards AI urban planners. This paper
conceptualizes urban planning as a generative AI task, where AI synthesizes
land-use configurations under geospatial, social, and human-centric
constraints. We survey how generative AI approaches, including VAEs, GANs,
transformers, and diffusion models, reshape urban design. We further identify
critical gaps: 1) limited research on integrating urban theory guidance, 2)
limited research of AI urban planning over multiple spatial resolutions or
angularities, 3) limited research on augmenting urban design knowledge from
data, and 4) limited research on addressing real-world interactions. To address
these limitations, we outline future research directions in theory-guided
generation, digital twins, and human-machine co-design, calling for a new
synthesis of generative intelligence and participatory urbanism.

</details>


### [248] [AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents](https://arxiv.org/abs/2507.14897)
*Renxi Wang,Rifo Ahmad Genadi,Bilal El Bouardi,Yongxin Wang,Fajri Koto,Zhengzhong Liu,Timothy Baldwin,Haonan Li*

Main category: cs.AI

TL;DR: AgentFly框架结合了LM代理和RL方法，通过提供多种RL算法支持高效训练。


<details>
  <summary>Details</summary>
Motivation: 探讨结合语言模型代理与强化学习技术的可能性与效果，以弥补现有研究在系统性上的不足。

Method: 设计了AgentFly框架，该框架适配多轮交互，采用异步操作和资源管理，结合RL算法训练语言模型代理。

Result: 展示了框架在多任务训练中的有效性，提供了一套工具和环境支持。

Conclusion: AgentFly框架通过创新设计，成功扩展了LM代理与RL技术的结合潜力。

Abstract: Language model (LM) agents have gained significant attention for their
ability to autonomously complete tasks through interactions with environments,
tools, and APIs. LM agents are primarily built with prompt engineering or
supervised finetuning. At the same time, reinforcement learning (RL) has been
explored to enhance LM's capabilities, such as reasoning and factuality.
However, the combination of the LM agents and reinforcement learning (Agent-RL)
remains underexplored and lacks systematic study. To this end, we built
AgentFly, a scalable and extensible Agent-RL framework designed to empower LM
agents with a variety of RL algorithms. Our framework supports multi-turn
interactions by adapting traditional RL methods with token-level masking. It
features a decorator-based interface for defining tools and reward functions,
enabling seamless extension and ease of use. To support high-throughput
training, we implement asynchronous execution of tool calls and reward
computations, and design a centralized resource management system for scalable
environment coordination. We also provide a suite of prebuilt tools and
environments, demonstrating the framework's effectiveness through successful
agent training across multiple tasks.

</details>


### [249] [InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis](https://arxiv.org/abs/2507.14899)
*Jiale Liu,Huan Wang,Yue Zhang,Xiaoyu Luo,Jiaxiang Hu,Zhiliang Liu,Min Xie*

Main category: cs.AI

TL;DR: 该研究提出了InsightX Agent，一个基于大规模多模态模型（LMM）的框架，提升工业X光无损检测的可靠性、可解释性和交互性，在GDXray+数据集上取得了96.35%的高F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法在无损检测中缺乏交互性、可解释性和自我评估能力，限制了其可靠性和用户信任。

Method: 设计了InsightX Agent框架，通过LMM作为核心协作者，结合稀疏可变形多尺度检测器（SDMSD）和基于证据的反思工具（EGR），优化缺陷检测流程并提供交互式分析。

Result: 在GDXray+数据集上，InsightX Agent达到了96.35% 的高F1分数，并显著提升分析的可解释性和可靠性。

Conclusion: 提出的框架表明基于智能代理的LMM模型在提升工业检测任务信任和诊断能力方面具有变革潜力。

Abstract: Non-destructive testing (NDT), particularly X-ray inspection, is vital for
industrial quality assurance, yet existing deep-learning-based approaches often
lack interactivity, interpretability, and the capacity for critical
self-assessment, limiting their reliability and operator trust. To address
these shortcomings, this paper proposes InsightX Agent, a novel LMM-based
agentic framework designed to deliver reliable, interpretable, and interactive
X-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent
positions a Large Multimodal Model (LMM) as a central orchestrator,
coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the
Evidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect
region proposals for multi-scale feature maps and sparsifies them through
Non-Maximum Suppression (NMS), optimizing detection of small, dense targets in
X-ray images while maintaining computational efficiency. The EGR tool guides
the LMM agent through a chain-of-thought-inspired review process, incorporating
context assessment, individual defect analysis, false positive elimination,
confidence recalibration and quality assurance to validate and refine the
SDMSD's initial proposals. By strategically employing and intelligently using
tools, InsightX Agent moves beyond passive data processing to active reasoning,
enhancing diagnostic reliability and providing interpretations that integrate
diverse information sources. Experimental evaluations on the GDXray+ dataset
demonstrate that InsightX Agent not only achieves a high object detection
F1-score of 96.35% but also offers significantly improved interpretability and
trustworthiness in its analyses, highlighting the transformative potential of
agentic LLM frameworks for industrial inspection tasks.

</details>


### [250] [Feedback-Induced Performance Decline in LLM-Based Decision-Making](https://arxiv.org/abs/2507.14906)
*Xiao Yang,Juxi Leitner,Michael Burke*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型(LLMs)在马尔可夫决策过程(MDPs)中的表现，并与经典强化学习(RL)方法对比，发现LLMs在简单环境下表现较好，但在复杂场景中表现不足。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在无迭代探索情况下，通过利用先验知识是否能够改进决策过程，并与执行传统强化学习方法进行比较。

Method: 分析LLMs在顺序决策任务中的零样本表现，使用在线结构化提示策略，并将其与传统强化学习方法的表现进行对比。

Result: 发现LLMs在简单环境中表现有一定优势，但在复杂场景中，由于反馈机制和缺乏细化调整导致决策能力下降。

Conclusion: 尽管具有初始性能优势，但在复杂决策问题中，LLMs需要进一步改进，例如结合混合策略、细微调整和高级记忆整合来提升其表现能力。

Abstract: The ability of Large Language Models (LLMs) to extract context from natural
language problem descriptions naturally raises questions about their
suitability in autonomous decision-making settings. This paper studies the
behaviour of these models within a Markov Decision Process (MDPs). While
traditional reinforcement learning (RL) strategies commonly employed in this
setting rely on iterative exploration, LLMs, pre-trained on diverse datasets,
offer the capability to leverage prior knowledge for faster adaptation. We
investigate online structured prompting strategies in sequential decision
making tasks, comparing the zero-shot performance of LLM-based approaches to
that of classical RL methods. Our findings reveal that although LLMs
demonstrate improved initial performance in simpler environments, they struggle
with planning and reasoning in complex scenarios without fine-tuning or
additional guidance. Our results show that feedback mechanisms, intended to
improve decision-making, often introduce confusion, leading to diminished
performance in intricate environments. These insights underscore the need for
further exploration into hybrid strategies, fine-tuning, and advanced memory
integration to enhance LLM-based decision-making capabilities.

</details>


### [251] [The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities](https://arxiv.org/abs/2507.14909)
*Elio Grande*

Main category: cs.AI

TL;DR: 本文介绍了一种人工智能设计方法，重点是通过避免替代人类并填补责任差距，来可靠地部署人工智能。


<details>
  <summary>Details</summary>
Motivation: 设计一种能够避免人工智能取代人类决策并缩小责任缺口的高可靠性部署方法。

Method: 利用双重镜像过程，结合反向和解释性人工智能（XAI）技术，提出具体实施协议，并在多种决策场景中进行实验测试。

Result: 实验表明，即使在深度学习模型的使用中，受访者依然感到完全的控制感，并且责任性与法律责任之间可以有连接的桥梁。

Conclusion: 提出的方法能够在保障人类参与决策的同时提升人工智能的伦理性，进一步提供了一种解决责任性等问题的新视角。

Abstract: The Endless Tuning is a design method for a reliable deployment of artificial
intelligence based on a double mirroring process, which pursues both the goals
of avoiding human replacement and filling the so-called responsibility gap
(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the
relational approach urged therein, it was then actualized in a protocol,
implemented in three prototypical applications regarding decision-making
processes (respectively: loan granting, pneumonia diagnosis, and art style
recognition) and tested with such as many domain experts. Step by step
illustrating the protocol, giving insights concretely showing a different voice
(Gilligan 1993) in the ethics of artificial intelligence, a philosophical
account of technical choices (e.g., a reversed and hermeneutic deployment of
XAI algorithms) will be provided in the present study together with the results
of the experiments, focusing on user experience rather than statistical
accuracy. Even thoroughly employing deep learning models, full control was
perceived by the interviewees in the decision-making setting, while it appeared
that a bridge can be built between accountability and liability in case of
damage.

</details>


### [252] [Redefining Elderly Care with Agentic AI: Challenges and Opportunities](https://arxiv.org/abs/2507.14912)
*Ruhul Amin Khalil,Kashif Ahmad,Hazrat Ali*

Main category: cs.AI

TL;DR: 本文探讨了利用基于大型语言模型（LLM）的自主型人工智能（Agentic AI）在老年护理中的潜力及挑战，提供了平衡的分析和未来研究重点建议。


<details>
  <summary>Details</summary>
Motivation: 全球老龄化加剧，需要依靠创新策略改善老年人护理，尤其是通过技术驱动提升其独立性与生活质量的重要性。

Method: 分析了Agentic AI在健康监测、认知护理和环境管理中的应用潜力，同时关注数据隐私、透明度和伦理保障的实施。

Result: 提出了Agentic AI在老年护理领域的应用构想，并强调了潜在的挑战和解决方法。此外，提供了一个互动仪表盘作为研究支持工具。

Conclusion: Agentic AI在老年护理领域具有重大转型潜力，但需要注重隐私、伦理和透明度方面的平衡，从而实现技术与老年人护理需求的协调发展。

Abstract: The global ageing population necessitates new and emerging strategies for
caring for older adults. In this article, we explore the potential for
transformation in elderly care through Agentic Artificial Intelligence (AI),
powered by Large Language Models (LLMs). We discuss the proactive and
autonomous decision-making facilitated by Agentic AI in elderly care.
Personalized tracking of health, cognitive care, and environmental management,
all aimed at enhancing independence and high-level living for older adults,
represents important areas of application. With a potential for significant
transformation of elderly care, Agentic AI also raises profound concerns about
data privacy and security, decision independence, and access. We share key
insights to emphasize the need for ethical safeguards, privacy protections, and
transparent decision-making. Our goal in this article is to provide a balanced
discussion of both the potential and the challenges associated with Agentic AI,
and to provide insights into its responsible use in elderly care, to bring
Agentic AI into harmony with the requirements and vulnerabilities specific to
the elderly. Finally, we identify the priorities for the academic research
communities, to achieve human-centered advancements and integration of Agentic
AI in elderly care. To the best of our knowledge, this is no existing study
that reviews the role of Agentic AI in elderly care. Hence, we address the
literature gap by analyzing the unique capabilities, applications, and
limitations of LLM-based Agentic AI in elderly care. We also provide a
companion interactive dashboard at https://hazratali.github.io/agenticai/.

</details>


### [253] [Complexity of Faceted Explanations in Propositional Abduction](https://arxiv.org/abs/2507.14962)
*Johannes Schmidt,Mohamed Maizia,Victor Lagerkvist,Johannes K. Fichte*

Main category: cs.AI

TL;DR: 该论文研究了命题溯因中的“面”（facets）概念，及其如何帮助更细致地理解解释的多样性，并对面在不同环境中的复杂性进行了全面分类分析。


<details>
  <summary>Details</summary>
Motivation: 命题溯因推理在人工智能诊断、规划等领域有重要应用，但关键推理问题（例如计数和枚举）计算复杂性较高，因此研究表明面可以在复杂性和信息精准度间取得平衡。

Method: 引入“面”（facets）这一概念，即在部分解释中出现的文字，用以分析解释中的多样性，通过系统性地分析面在命题溯因中的复杂性特征，特别结合Post逻辑框架进行分类研究。

Result: 通过对面在多种设定下的分析，作者几乎完整地描述了其复杂性特征，并提出了考虑解释距离的新方法，以更好地理解解释的异质性和同质性。

Conclusion: 引入面和解释距离的研究改进了溯因推理的理解，提供了新的工具和视角来处理复杂的非单调推理问题。

Abstract: Abductive reasoning is a popular non-monotonic paradigm that aims to explain
observed symptoms and manifestations. It has many applications, such as
diagnosis and planning in artificial intelligence and database updates. In
propositional abduction, we focus on specifying knowledge by a propositional
formula. The computational complexity of tasks in propositional abduction has
been systematically characterized - even with detailed classifications for
Boolean fragments. Unsurprisingly, the most insightful reasoning problems
(counting and enumeration) are computationally highly challenging. Therefore,
we consider reasoning between decisions and counting, allowing us to understand
explanations better while maintaining favorable complexity. We introduce facets
to propositional abductions, which are literals that occur in some explanation
(relevant) but not all explanations (dispensable). Reasoning with facets
provides a more fine-grained understanding of variability in explanations
(heterogeneous). In addition, we consider the distance between two
explanations, enabling a better understanding of heterogeneity/homogeneity. We
comprehensively analyze facets of propositional abduction in various settings,
including an almost complete characterization in Post's framework.

</details>


### [254] [AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning](https://arxiv.org/abs/2507.14987)
*Yi Zhang,An Zhang,XiuYu Zhang,Leheng Sheng,Yuxin Chen,Zhenkai Liang,Xiang Wang*

Main category: cs.AI

TL;DR: AlphaAlign 是一个利用纯强化学习框架进行语言模型安全对齐的新方法。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型的安全对齐方法存在生成有害内容、过度拒绝或效用下降的问题，难以充分利用模型本身的内在安全认知能力。

Method: 提出 AlphaAlign，通过一个双重奖励机制（可验证的安全奖励和标准化的帮助性奖励），在无需监督数据的情况下，通过强化学习激励模型进行主动的安全推理。

Result: AlphaAlign 在只需要基本的安全标签和少量 RL 训练步骤的情况下，有效提升了模型安全性，同时避免了任务效用的下降，且对新的攻击方式具有鲁棒性。

Conclusion: AlphaAlign 提供了一种简单高效的强化学习方法，能同时解决安全与效用之间的权衡，还能够推动模型生成有明确理由的安全响应。

Abstract: Large language models (LLMs), despite possessing latent safety understanding
from their vast pretraining data, remain vulnerable to generating harmful
content and exhibit issues such as over-refusal and utility degradation after
safety alignment. Current safety alignment methods often result in superficial
refusal shortcuts or rely on intensive supervision for reasoning-based
approaches, failing to fully leverage the model's intrinsic safety
self-awareness. We propose \textbf{AlphaAlign}, a simple yet effective pure
reinforcement learning (RL) framework with verifiable safety reward designed to
incentivize this latent safety awareness through proactive safety reasoning.}
AlphaAlign employs a dual-reward system: a verifiable safety reward encourages
correctly formatted and explicitly justified refusals for harmful queries while
penalizing over-refusals, and a normalized helpfulness reward guides
high-quality responses to benign inputs. This allows the model to develop
proactive safety reasoning capabilities without depending on supervised
safety-specific reasoning data. AlphaAlign demonstrates three key advantages:
(1) Simplicity and efficiency, requiring only binary prompt safety labels and
minimal RL steps for substantial improvements. (2) Breaking the safety-utility
trade-off, by enhancing refusal of harmful content and reducing over-refusals,
while simultaneously maintaining or even improving general task performance and
robustness to unseen jailbreaks. (3) Deep alignment, fostering proactive safety
reasoning that generates explicit safety rationales rather than relying on
shallow refusal patterns.

</details>


### [255] [A Forced-Choice Neural Cognitive Diagnostic Model of Personality Testing](https://arxiv.org/abs/2507.15013)
*Xiaoyu Li,Jin Wu,Shaoyang Guo,Haoran Shi,Chanjin Zheng*

Main category: cs.AI

TL;DR: 提出一种基于深度学习的强制选择神经认知诊断模型（FCNCD），用于改进强制选择心理测试的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 提高心理测试的结果准确性、可解释性，同时减少传统模型的局限性。

Method: 通过深度学习模型挖掘参与者与题目特征的交互关系，并引入单维假设和单调性假设以增强诊断结果的可解释性。

Result: 实验结果表明，在真实和模拟数据集上的FCNCD模型具有良好的准确性、可解释性和鲁棒性。

Conclusion: FCNCD模型为心理测评提供了一个有效且可靠的解决方案，可广泛应用于人员选拔、职业发展和心理健康评估等领域。

Abstract: In the smart era, psychometric tests are becoming increasingly important for
personnel selection, career development, and mental health assessment.
Forced-choice tests are common in personality assessments because they require
participants to select from closely related options, lowering the risk of
response distortion. This study presents a deep learning-based Forced-Choice
Neural Cognitive Diagnostic Model (FCNCD) that overcomes the limitations of
traditional models and is applicable to the three most common item block types
found in forced-choice tests. To account for the unidimensionality of items in
forced-choice tests, we create interpretable participant and item parameters.
We model the interactions between participant and item features using
multilayer neural networks after mining them using nonlinear mapping. In
addition, we use the monotonicity assumption to improve the interpretability of
the diagnostic results. The FCNCD's effectiveness is validated by experiments
on real-world and simulated datasets that show its accuracy, interpretability,
and robustness.

</details>


### [256] [DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection](https://arxiv.org/abs/2507.15042)
*Jerry Wang,Fang Yu*

Main category: cs.AI

TL;DR: 通过差分进化优化的对抗性提示后缀在真实世界场景下可以有效攻击基于检索增强生成（RAG）的问答系统。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗性攻击方法受限于梯度依赖或生成后缀的可检测性，本文尝试在黑箱环境下开发更有效且不可检测的方法。

Method: 采用差分进化（DE）方法，优化对抗性提示后缀，以提升错误文档的检索排名，同时引入考虑可读性的后缀构建策略。

Result: 实验表明，基于DE的提示优化对密集和稀疏检索器的攻击成功率与现有方法相当或更优；且生成的后缀在可读性和检测规避能力方面表现优异。

Conclusion: 差分进化是一种有效的对抗性提示优化方法，在保持攻击效果的同时，增强了攻击内容的可读性和不可检测性。

Abstract: Adversarial prompt attacks can significantly alter the reliability of
Retrieval-Augmented Generation (RAG) systems by re-ranking them to produce
incorrect outputs. In this paper, we present a novel method that applies
Differential Evolution (DE) to optimize adversarial prompt suffixes for
RAG-based question answering. Our approach is gradient-free, treating the RAG
pipeline as a black box and evolving a population of candidate suffixes to
maximize the retrieval rank of a targeted incorrect document to be closer to
real world scenarios. We conducted experiments on the BEIR QA datasets to
evaluate attack success at certain retrieval rank thresholds under multiple
retrieving applications. Our results demonstrate that DE-based prompt
optimization attains competitive (and in some cases higher) success rates
compared to GGPP to dense retrievers and PRADA to sparse retrievers, while
using only a small number of tokens (<=5 tokens) in the adversarial suffix.
Furthermore, we introduce a readability-aware suffix construction strategy,
validated by a statistically significant reduction in MLM negative
log-likelihood with Welch's t-test. Through evaluations with a BERT-based
adversarial suffix detector, we show that DE-generated suffixes evade
detection, yielding near-chance detection accuracy.

</details>


### [257] [From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward](https://arxiv.org/abs/2507.15106)
*Xia Xu,Jochen Triesch*

Main category: cs.AI

TL;DR: 引入了一种基于因果推理的内在奖励方法CAIS，用于提高强化学习代理的鲁棒性，特别是在有噪声的生态环境下表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法在噪声场景中效果不佳，难以像婴儿一样稳健地发现因果关系。

Method: 提出CAIS方法，通过测量行动对结果分布的影响程度(即1-Wasserstein距离)，提供能够隔离噪声干扰的因果奖励信号。

Result: 在模拟婴儿-摇铃环境中，CAIS克服了环境噪声，成功识别因果影响并学到正确策略，同时能再现心理学中的“消减爆发”现象。

Conclusion: 因果推理是构建具有稳健感知能力的自主系统的重要机制。

Abstract: While human infants robustly discover their own causal efficacy, standard
reinforcement learning agents remain brittle, as their reliance on
correlation-based rewards fails in noisy, ecologically valid scenarios. To
address this, we introduce the Causal Action Influence Score (CAIS), a novel
intrinsic reward rooted in causal inference. CAIS quantifies an action's
influence by measuring the 1-Wasserstein distance between the learned
distribution of sensory outcomes conditional on that action, $p(h|a)$, and the
baseline outcome distribution, $p(h)$. This divergence provides a robust reward
that isolates the agent's causal impact from confounding environmental noise.
We test our approach in a simulated infant-mobile environment where
correlation-based perceptual rewards fail completely when the mobile is
subjected to external forces. In stark contrast, CAIS enables the agent to
filter this noise, identify its influence, and learn the correct policy.
Furthermore, the high-quality predictive model learned for CAIS allows our
agent, when augmented with a surprise signal, to successfully reproduce the
"extinction burst" phenomenon. We conclude that explicitly inferring causality
is a crucial mechanism for developing a robust sense of agency, offering a
psychologically plausible framework for more adaptive autonomous systems.

</details>


### [258] [Automated planning with ontologies under coherence update semantics](https://arxiv.org/abs/2507.15120)
*Stefan Borgwardt,Duy Nhu,Gabriele Röger*

Main category: cs.AI

TL;DR: 本文提出一种结合DL-Lite本体的自动化规划方法，利用显式输入知识和本体感知的动作效果，在多基准测试中验证其性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何在自动化规划中结合背景知识和本体，解决常见的闭世界语义限制问题。

Method: 提出一种新的规划方法，通过多项式编译将规划问题转化为经典规划，同时利用DL-Lite本体作为背景知识。

Result: 证明了该方法的复杂度与现有工作相当，并通过基准测试评估了系统性能。

Conclusion: 在纳入本体知识的情况下，所提方法在复杂度与性能上均表现优异，提供了传统方法的新替代方案。

Abstract: Standard automated planning employs first-order formulas under closed-world
semantics to achieve a goal with a given set of actions from an initial state.
We follow a line of research that aims to incorporate background knowledge into
automated planning problems, for example, by means of ontologies, which are
usually interpreted under open-world semantics. We present a new approach for
planning with DL-Lite ontologies that combines the advantages of ontology-based
action conditions provided by explicit-input knowledge and action bases (eKABs)
and ontology-aware action effects under the coherence update semantics. We show
that the complexity of the resulting formalism is not higher than that of
previous approaches and provide an implementation via a polynomial compilation
into classical planning. An evaluation of existing and new benchmarks examines
the performance of a planning system on different variants of our compilation.

</details>


### [259] [Clinical Semantic Intelligence (CSI): Emulating the Cognitive Framework of the Expert Clinician for Comprehensive Oral Disease Diagnosis](https://arxiv.org/abs/2507.15140)
*Mohammad Mashayekhi,Sara Ahmadi Majd,Arian AmirAmjadi,Parsa Hosseini*

Main category: cs.AI

TL;DR: 本文开发了一种名为临床语义智能（CSI）的人工智能框架，可诊断118种口腔疾病，通过模拟临床专家的认知过程实现更高效的诊断。


<details>
  <summary>Details</summary>
Motivation: 口腔疾病诊断面临复杂的临床挑战，需要一种可以超越简单模式匹配、模拟专家推理的诊断工具。

Method: CSI框架结合了微调的多模态CLIP模型与专门设计的ChatGLM-6B语言模型，并通过层次化的诊断推理树（HDRT）执行系统化的多步骤鉴别诊断逻辑，分为快速模式和标准模式。

Result: 在431张内部测试集上，快速模式的准确度为73.4%，而基于HDRT的标准模式准确度提高至89.5%，体现了分层推理过程的性能增益。

Conclusion: CSI框架通过模拟专家推理显著提升了口腔疾病的诊断准确度，为临床诊断提供了有效支持。

Abstract: The diagnosis of oral diseases presents a problematic clinical challenge,
characterized by a wide spectrum of pathologies with overlapping
symptomatology. To address this, we developed Clinical Semantic Intelligence
(CSI), a novel artificial intelligence framework that diagnoses 118 different
oral diseases by computationally modeling the cognitive processes of an expert
clinician. Our core hypothesis is that moving beyond simple pattern matching to
emulate expert reasoning is critical to building clinically useful diagnostic
aids.
  CSI's architecture integrates a fine-tuned multimodal CLIP model with a
specialized ChatGLM-6B language model. This system executes a Hierarchical
Diagnostic Reasoning Tree (HDRT), a structured framework that distills the
systematic, multi-step logic of differential diagnosis. The framework operates
in two modes: a Fast Mode for rapid screening and a Standard Mode that
leverages the full HDRT for an interactive and in-depth diagnostic workup.
  To train and validate our system, we curated a primary dataset of 4,310
images, supplemented by an external hold-out set of 176 images for final
validation. A clinically-informed augmentation strategy expanded our training
data to over 30,000 image-text pairs. On a 431-image internal test set, CSI's
Fast Mode achieved an accuracy of 73.4%, which increased to 89.5% with the
HDRT-driven Standard Mode. The performance gain is directly attributable to the
hierarchical reasoning process. Herein, we detail the architectural philosophy,
development, and rigorous evaluation of the CSI framework.

</details>


### [260] [Can We Move Freely in NEOM's The Line? An Agent-Based Simulation of Human Mobility in a Futuristic Smart City](https://arxiv.org/abs/2507.15143)
*Abderaouf Bahi,Amel Ourici*

Main category: cs.AI

TL;DR: 本文探讨了在沙特阿拉伯NEOM项目中的线性城市“The Line”内实现人类自由出行的可行性，通过混合模拟框架分析多模式交通行为，发现AI集成技术可显著提高通行效率和满意度，同时降低能耗和排放。


<details>
  <summary>Details</summary>
Motivation: 研究目标是评估在“线性城市”这一前所未有的城市布局中，居民是否能够实现高效的自由流动，兼顾可持续性和智能技术支撑。

Method: 提出了一个结合多种AI技术的混合模拟框架，包括基于智能体建模、强化学习、监督学习及图神经网络，分析了不同密度和高度场景下的多模式交通行为。

Result: 实验表明，AI集成架构可使平均通勤时间控制在7.8到8.4分钟，满意率超过89%，可达性指数超过91%。去除部分智能模块会显著降低性能，通勤时间增加85%，可达性下降至70%以下。

Conclusion: 研究证明在AI支持的情况下，“The Line”内居民的自由流动不仅概念上可行，且在运营层面具有实现可能性，同时兼顾低能耗和环保特点。

Abstract: This paper investigates the feasibility of human mobility in The Line, a
proposed 170-kilometer linear smart city in NEOM, Saudi Arabia. To assess
whether citizens can move freely within this unprecedented urban topology, we
develop a hybrid simulation framework that integrates agent-based modeling,
reinforcement learning, supervised learning, and graph neural networks. The
simulation captures multi-modal transportation behaviors across 50 vertical
levels and varying density scenarios using both synthetic data and real-world
traces from high-density cities. Our experiments reveal that with the full
AI-integrated architecture, agents achieved an average commute time of 7.8 to
8.4 minutes, a satisfaction rate exceeding 89 percent, and a reachability index
of over 91 percent, even during peak congestion periods. Ablation studies
confirmed that the removal of intelligent modules such as reinforcement
learning or graph neural networks significantly degrades performance, with
commute times increasing by up to 85 percent and reachability falling below 70
percent. Environmental modeling further demonstrated low energy consumption and
minimal CO2 emissions when electric modes are prioritized. The findings suggest
that freedom of movement is not only conceptually achievable in The Line, but
also operationally realistic if supported by adaptive AI systems, sustainable
infrastructure, and real-time feedback loops.

</details>


### [261] [Solving Formal Math Problems by Decomposition and Iterative Reflection](https://arxiv.org/abs/2507.15225)
*Yichi Zhou,Jianqiu Zhao,Yongxin Zhang,Bohan Wang,Siran Wang,Luoxin Chen,Jiahui Wang,Haowei Chen,Allan Jie,Xinbo Zhang,Haocheng Wang,Luong Trung,Rong Ye,Phan Nhat Hoang,Huishuai Zhang,Peng Sun,Hang Li*

Main category: cs.AI

TL;DR: Delta Prover通过结合通用LLM和Lean 4环境，提升了形式化证明的效率，实现了95.9%的miniF2F测试成功率，无需模型专门化。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLM）在智能推理任务中表现出色，但在使用专业语言（如Lean 4）生成形式化证明方面仍存在挑战，这限制了其在复杂定理证明和自动验证中的应用。

Method: 提出Delta Prover框架，结合反射性分解和迭代修复算法，以及基于Lean 4的定制领域专用语言（DSL），通过引导通用LLM与Lean 4证明环境交互，构建形式化证明。

Result: Delta Prover在miniF2F-test测试中取得了95.9%的成功率，超越了需要模型专门化的现有方法。此外，其测试时扩展定律显著优于标准的Best-of-N策略。

Conclusion: 通用LLM在有效的代理结构引导下，具有被充分开发潜力的定理证明能力，这为形式化环境中的强大自动推理提供了一个计算效率高的替代方案。

Abstract: General-purpose Large Language Models (LLMs) have achieved remarkable success
in intelligence, performing comparably to human experts on complex reasoning
tasks such as coding and mathematical reasoning. However, generating formal
proofs in specialized languages like Lean 4 remains a significant challenge for
these models, limiting their application in complex theorem proving and
automated verification. Current approaches typically require specializing
models through fine-tuning on dedicated formal corpora, incurring high costs
for data collection and training. In this work, we introduce \textbf{Delta
Prover}, an agent-based framework that orchestrates the interaction between a
general-purpose LLM and the Lean 4 proof environment. Delta Prover leverages
the reflection and reasoning capabilities of general-purpose LLMs to
interactively construct formal proofs in Lean 4, circumventing the need for
model specialization. At its core, the agent integrates two novel,
interdependent components: an algorithmic framework for reflective
decomposition and iterative proof repair, and a custom Domain-Specific Language
(DSL) built upon Lean 4 for streamlined subproblem management. \textbf{Delta
Prover achieves a state-of-the-art 95.9\% success rate on the miniF2F-test
benchmark, surpassing all existing approaches, including those requiring model
specialization.} Furthermore, Delta Prover exhibits a significantly stronger
test-time scaling law compared to standard Best-of-N proof strategies.
Crucially, our findings demonstrate that general-purpose LLMs, when guided by
an effective agentic structure, possess substantial untapped theorem-proving
capabilities. This presents a computationally efficient alternative to
specialized models for robust automated reasoning in formal environments.

</details>


### [262] [Explainable Artificial Intelligence based Soft Evaluation Indicator for Arc Fault Diagnosis](https://arxiv.org/abs/2507.15239)
*Qianchao Wang,Yuxuan Ding,Chuanzhen Jia,Zhe Li,Yaping Du*

Main category: cs.AI

TL;DR: 提出了一种面向弧故障诊断的软评估指标，并开发了轻量级平衡神经网络，以提高模型的可信性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决在弧故障诊断中AI模型可信性的问题，让诊断结果更加可解释和可靠。

Method: 提出软评估指标结合可解释性人工智能和真实实验数据，并设计轻量神经网络以实现高准确率和平衡的特征提取。

Result: 实验验证了软评估指标在不同数据集和噪声水平下的有效性，使诊断模型变得透明和易于理解。

Conclusion: 通过提出的框架，分析结果更加可信，为弧故障诊断提供了解释性工具，便于实践应用中的可靠决策。

Abstract: Novel AI-based arc fault diagnosis models have demonstrated outstanding
performance in terms of classification accuracy. However, an inherent problem
is whether these models can actually be trusted to find arc faults. In this
light, this work proposes a soft evaluation indicator that explains the outputs
of arc fault diagnosis models, by defining the the correct explanation of arc
faults and leveraging Explainable Artificial Intelligence and real arc fault
experiments. Meanwhile, a lightweight balanced neural network is proposed to
guarantee competitive accuracy and soft feature extraction score. In our
experiments, several traditional machine learning methods and deep learning
methods across two arc fault datasets with different sample times and noise
levels are utilized to test the effectiveness of the soft evaluation indicator.
Through this approach, the arc fault diagnosis models are easy to understand
and trust, allowing practitioners to make informed and trustworthy decisions.

</details>


### [263] [Disentangling Homophily and Heterophily in Multimodal Graph Clustering](https://arxiv.org/abs/2507.15253)
*Zhaochen Guo,Zhixiang Shen,Xuanting Xie,Liangjian Wen,Zhao Kang*

Main category: cs.AI

TL;DR: 本文提出了一个名为DMGC的无监督学习框架，可以在混合邻域模式的真实多模态图上进行有效的聚类分析。


<details>
  <summary>Details</summary>
Motivation: 现有方法尚未深入研究无监督多模态图学习，而多模态图在整合非结构化异质数据和结构化关系上具有巨大潜力。

Method: 提出了一种DMGC框架，将原始混合图分解为两个视图：同质性增强图和异质性感知图，并通过双频融合机制实现对这两个视图的过滤和多模态整合，同时引入自监督对齐目标。

Result: 在多模态和多关系图数据集上的实验表明，DMGC达到了当前最优性能，体现了其在不同场景下的有效性和普适性。

Conclusion: DMGC能够有效处理多模态图中的复杂关系，为无监督学习的多模态图聚类提供了一种新颖且普适的方法。

Abstract: Multimodal graphs, which integrate unstructured heterogeneous data with
structured interconnections, offer substantial real-world utility but remain
insufficiently explored in unsupervised learning. In this work, we initiate the
study of multimodal graph clustering, aiming to bridge this critical gap.
Through empirical analysis, we observe that real-world multimodal graphs often
exhibit hybrid neighborhood patterns, combining both homophilic and
heterophilic relationships. To address this challenge, we propose a novel
framework -- \textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which
decomposes the original hybrid graph into two complementary views: (1) a
homophily-enhanced graph that captures cross-modal class consistency, and (2)
heterophily-aware graphs that preserve modality-specific inter-class
distinctions. We introduce a \emph{Multimodal Dual-frequency Fusion} mechanism
that jointly filters these disentangled graphs through a dual-pass strategy,
enabling effective multimodal integration while mitigating category confusion.
Our self-supervised alignment objectives further guide the learning process
without requiring labels. Extensive experiments on both multimodal and
multi-relational graph datasets demonstrate that DMGC achieves state-of-the-art
performance, highlighting its effectiveness and generalizability across diverse
settings. Our code is available at https://github.com/Uncnbb/DMGC.

</details>


### [264] [IM-Chat: A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry](https://arxiv.org/abs/2507.15268)
*Junhyeong Lee,Joon-Young Kim,Heekyu Kim,Inhyo Lee,Seunghwa Ryu*

Main category: cs.AI

TL;DR: 注塑成型行业利用IM-Chat框架实现知识转移，通过多代理大型语言模型进行领域知识的保留与传递。


<details>
  <summary>Details</summary>
Motivation: 解决由于经验工人退休和多语言障碍导致的注塑成型行业知识难以传递的问题。

Method: 提出IM-Chat框架，结合有限文档知识和基于数据驱动的条件生成器，通过RAG策略和模块化架构确保适应性及多代理任务执行能力。

Result: 在多个模型和任务环境中评估IM-Chat表现，结果表明更强大的模型在复杂任务场景中具有更高的准确性。

Conclusion: IM-Chat证明了多代理LLM系统在工业知识转移和决策支持中的可行性，并提供了可扩展及通用化的方法。

Abstract: The injection molding industry faces critical challenges in preserving and
transferring field knowledge, particularly as experienced workers retire and
multilingual barriers hinder effective communication. This study introduces
IM-Chat, a multi-agent framework based on large language models (LLMs),
designed to facilitate knowledge transfer in injection molding. IM-Chat
integrates both limited documented knowledge (e.g., troubleshooting tables,
manuals) and extensive field data modeled through a data-driven process
condition generator that infers optimal manufacturing settings from
environmental inputs such as temperature and humidity, enabling robust and
context-aware task resolution. By adopting a retrieval-augmented generation
(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat
ensures adaptability without the need for fine-tuning. Performance was assessed
across 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and
GPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance
and correctness, and was further supplemented by automated evaluation using
GPT-4o guided by a domain-adapted instruction prompt. The evaluation results
indicate that more capable models tend to achieve higher accuracy, particularly
in complex, tool-integrated scenarios. Overall, these findings demonstrate the
viability of multi-agent LLM systems for industrial knowledge workflows and
establish IM-Chat as a scalable and generalizable approach to AI-assisted
decision support in manufacturing.

</details>


### [265] [QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI](https://arxiv.org/abs/2507.15330)
*Hammad Atta,Muhammad Zeeshan Baig,Yasir Mehmood,Nadeem Shahzad,Ken Huang,Muhammad Aziz Ul Haq,Muhammad Awais,Kamal Ahmed*

Main category: cs.AI

TL;DR: 本文提出了认知衰退作为自主AI系统中的一种新型脆弱性，并介绍了一种生命周期防护框架来应对。


<details>
  <summary>Details</summary>
Motivation: 旨在解决自主AI系统因内在缺陷导致的失效问题，而非外部威胁。

Method: 提出Qorvex安全AI框架(QSAF)，通过认知衰退生命周期和七个实时控制机制监控并缓解自主AI系统的缺陷。

Result: 构建了首个跨平台防御模型，用于早期检测并应对自主AI系统的疲劳、饥饿及角色崩溃问题。

Conclusion: 认知衰退已被确立为AI系统中的关键漏洞类别，并提出了具有生命周期和实时缓解能力的防御框架。

Abstract: We introduce Cognitive Degradation as a novel vulnerability class in agentic
AI systems. Unlike traditional adversarial external threats such as prompt
injection, these failures originate internally, arising from memory starvation,
planner recursion, context flooding, and output suppression. These systemic
weaknesses lead to silent agent drift, logic collapse, and persistent
hallucinations over time. To address this class of failures, we introduce the
Qorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain
10), a lifecycle-aware defense framework defined by a six-stage cognitive
degradation lifecycle. The framework includes seven runtime controls
(QSAF-BC-001 to BC-007) that monitor agent subsystems in real time and trigger
proactive mitigation through fallback routing, starvation detection, and memory
integrity enforcement. Drawing from cognitive neuroscience, we map agentic
architectures to human analogs, enabling early detection of fatigue,
starvation, and role collapse. By introducing a formal lifecycle and real-time
mitigation controls, this work establishes Cognitive Degradation as a critical
new class of AI system vulnerability and proposes the first cross-platform
defense model for resilient agentic behavior.

</details>


### [266] [One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms](https://arxiv.org/abs/2507.15351)
*Zijian Zhao,Sen Li*

Main category: cs.AI

TL;DR: 文章提出两种新方法解决动态拼车中基于多智能体强化学习的局限性，有效优化接客时间及订单完成数。


<details>
  <summary>Details</summary>
Motivation: 动态拼车面临聚焦于大规模和高不确定性环境下的动态乘客和车辆匹配问题，现有多智能体强化学习模型在大规模应用中表现出不稳定，导致Q值估计和训练偏差严重。

Method: 提出两种新方法：GRPO通过用群体平均回报替代PPO基线，减少了评论员误差和训练偏见；OSPO则基于同质车辆仅利用一步回报训练最优政策。

Result: 实验展示GRPO和OSPO在曼哈顿拼车数据集的多数场景中表现优越，使用简单MLP网络有效优化接客时间及订单完成数。

Conclusion: 通过规避值函数估计问题，文章提出的方法显著改善了动态拼车的效率和性能，提出的新框架具有高潜力。

Abstract: On-demand ride-sharing platforms face the fundamental challenge of
dynamically bundling passengers with diverse origins and destinations and
matching them with vehicles in real time, all under significant uncertainty.
Recently, MARL has emerged as a promising solution for this problem, leveraging
decentralized learning to address the curse of dimensionality caused by the
large number of agents in the ride-hailing market and the resulting expansive
state and action spaces. However, conventional MARL-based ride-sharing
approaches heavily rely on the accurate estimation of Q-values or V-values,
which becomes problematic in large-scale, highly uncertain environments.
Specifically, most of these approaches adopt an independent paradigm,
exacerbating this issue, as each agent treats others as part of the
environment, leading to unstable training and substantial estimation bias in
value functions. To address these challenges, we propose two novel alternative
methods that bypass value function estimation. First, we adapt GRPO to
ride-sharing, replacing the PPO baseline with the group average reward to
eliminate critic estimation errors and reduce training bias. Second, inspired
by GRPO's full utilization of group reward information, we customize the PPO
framework for ride-sharing platforms and show that, under a homogeneous fleet,
the optimal policy can be trained using only one-step rewards - a method we
term One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan
ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior
performance across most scenarios, efficiently optimizing pickup times and the
number of served orders using simple MLP networks.

</details>


### [267] [RAD: Retrieval High-quality Demonstrations to Enhance Decision-making](https://arxiv.org/abs/2507.15356)
*Lu Guo,Yixiang Shan,Zhengbang Zhu,Qifan Liang,Lichang Song,Ting Long,Weinan Zhang,Yi Chang*

Main category: cs.AI

TL;DR: 本研究提出了一种名为RAD的方法，通过结合非参数检索与扩散生成模型改善离线强化学习的效果，解决了稀疏数据集及轨迹间缺乏衔接的问题。


<details>
  <summary>Details</summary>
Motivation: 当前离线强化学习受限于数据集稀疏性及轨迹间过渡点不足，从而在长时间规划中表现较差。

Method: 提出RAD方法，动态检索高回报状态并使用条件引导扩散模型进行规划，使得轨迹拼接更灵活并在遇到分布外状态时提高泛化能力。

Result: 实验表明，与多种基线方法相比，RAD在多个基准测试中表现出色，证明了其有效性。

Conclusion: RAD通过检索和生成技术改进了离线强化学习的规划和泛化能力，为稳定且高效的决策提供了解决方案。

Abstract: Offline reinforcement learning (RL) enables agents to learn policies from
fixed datasets, avoiding costly or unsafe environment interactions. However,
its effectiveness is often limited by dataset sparsity and the lack of
transition overlap between suboptimal and expert trajectories, which makes
long-horizon planning particularly challenging. Prior solutions based on
synthetic data augmentation or trajectory stitching often fail to generalize to
novel states and rely on heuristic stitching points. To address these
challenges, we propose Retrieval High-quAlity Demonstrations (RAD) for
decision-making, which combines non-parametric retrieval with diffusion-based
generative modeling. RAD dynamically retrieves high-return states from the
offline dataset as target states based on state similarity and return
estimation, and plans toward them using a condition-guided diffusion model.
Such retrieval-guided generation enables flexible trajectory stitching and
improves generalization when encountered with underrepresented or
out-of-distribution states. Extensive experiments confirm that RAD achieves
competitive or superior performance compared to baselines across diverse
benchmarks, validating its effectiveness.

</details>


### [268] [Predictive Process Monitoring Using Object-centric Graph Embeddings](https://arxiv.org/abs/2507.15411)
*Wissam Gherissi,Mehdi Acheli,Joyce El Haddad,Daniela Grigori*

Main category: cs.AI

TL;DR: 本文提出了一种模型，结合图注意力网络和LSTM网络，预测未来业务流程中的活动与时间，性能优秀。


<details>
  <summary>Details</summary>
Motivation: 旨在利用对象中心事件日志提高流程预测能力，尤其针对下一步活动和事件时间的精准预测。

Method: 通过结合图注意力网络编码活动及关系，同时使用LSTM网络处理时间依赖性，构建端到端预测模型。

Result: 模型在一个真实和三个合成数据集上评估表现出色，与最先进方法性能相当。

Conclusion: 所提模型有效增强了流程预测能力，表现出对复杂流程的适应性。

Abstract: Object-centric predictive process monitoring explores and utilizes
object-centric event logs to enhance process predictions. The main challenge
lies in extracting relevant information and building effective models. In this
paper, we propose an end-to-end model that predicts future process behavior,
focusing on two tasks: next activity prediction and next event time. The
proposed model employs a graph attention network to encode activities and their
relationships, combined with an LSTM network to handle temporal dependencies.
Evaluated on one reallife and three synthetic event logs, the model
demonstrates competitive performance compared to state-of-the-art methods.

</details>


### [269] [Optimization of Activity Batching Policies in Business Processes](https://arxiv.org/abs/2507.15457)
*Orlenys López-Pintado,Jannis Rosenbaum,Marlon Dumas*

Main category: cs.AI

TL;DR: 本文研究了业务流程中活动批处理政策的优化问题，提出了一种基于Pareto优化的启发式方法，通过元启发式算法迭代更新Pareto前沿以实现最优权衡。


<details>
  <summary>Details</summary>
Motivation: 在业务流程中，批处理活动可以通过减少处理努力和分摊固定成本来降低成本，但也可能导致等待时间增加。因此，需要有效的批处理策略实现等待时间、处理努力和成本之间的最优取舍。

Method: 提出一种基于Pareto优化的启发式方法，通过干预启发式生成替代策略，并使用仿真评估每次干预的影响，这些启发式方法嵌入到诸如爬山法、模拟退火和强化学习等优化元启发式中来改进Pareto前沿。

Result: 通过实验评估，提出的基于干预启发式的方法在收敛性、多样性和循环时间增益方面优于非启发式元启发式基线。

Conclusion: 本文提出了一种新颖的优化方法，通过干预启发式和元启发式算法实现了业务流程中活动批处理政策的最优权衡，展示了其解决复杂批处理优化问题的效果。

Abstract: In business processes, activity batching refers to packing multiple activity
instances for joint execution. Batching allows managers to trade off cost and
processing effort against waiting time. Larger and less frequent batches may
lower costs by reducing processing effort and amortizing fixed costs, but they
create longer waiting times. In contrast, smaller and more frequent batches
reduce waiting times but increase fixed costs and processing effort. A batching
policy defines how activity instances are grouped into batches and when each
batch is activated. This paper addresses the problem of discovering batching
policies that strike optimal trade-offs between waiting time, processing
effort, and cost. The paper proposes a Pareto optimization approach that starts
from a given set (possibly empty) of activity batching policies and generates
alternative policies for each batched activity via intervention heuristics.
Each heuristic identifies an opportunity to improve an activity's batching
policy with respect to a metric (waiting time, processing time, cost, or
resource utilization) and an associated adjustment to the activity's batching
policy (the intervention). The impact of each intervention is evaluated via
simulation. The intervention heuristics are embedded in an optimization
meta-heuristic that triggers interventions to iteratively update the Pareto
front of the interventions identified so far. The paper considers three
meta-heuristics: hill-climbing, simulated annealing, and reinforcement
learning. An experimental evaluation compares the proposed approach based on
intervention heuristics against the same (non-heuristic guided) meta-heuristics
baseline regarding convergence, diversity, and cycle time gain of
Pareto-optimal policies.

</details>


### [270] [Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner](https://arxiv.org/abs/2507.15509)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习的图表域视觉语言模型Chart-R1，通过改进数据生成、链式推理训练和策略优化提升了复杂图表推理能力。


<details>
  <summary>Details</summary>
Motivation: 验证R1-Style方法在更广泛多模态数据上的优越性，解决复杂图表推理问题。

Method: 1. 引入新的程序化数据合成技术生成高质量图表推理数据；2. 两阶段训练策略Chart-COT（链式推理监督）与Chart-RFT（数值敏感强化微调）。

Result: 在开源基准和自建数据集（ChartRQA）上实验表明，Chart-R1显著优于其他图表领域方法，与大型模型性能相当。

Conclusion: Chart-R1通过强化学习微调在图表推理任务中展现了卓越性能，为多模态推理研究提供了新方向。

Abstract: Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based
on reinforcement learning fine-tuning has received widespread attention from
the community. Previous R1-Style methods mainly focus on mathematical reasoning
and code intelligence. It is of great research significance to verify their
advantages on more general multimodal data. Chart is an important multimodal
data type with rich information, which brings important research challenges in
complex reasoning. In this work, we introduce Chart-R1, a chart-domain
vision-language model with reinforcement learning fine-tuning to enable complex
chart reasoning. To support Chart-R1, we first propose a novel programmatic
data synthesis technology to generate high-quality step-by-step chart reasoning
data covering single- and multi-subcharts, which makes up for the lack of
reasoning data in the chart domain. Then we develop a two-stage training
strategy: Chart-COT with step-by-step chain-of-thought supervision, and
Chart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims
to decompose complex chart reasoning tasks into fine-grained, understandable
subtasks through step-by-step supervision, which lays a good foundation for
improving the reasoning level of reinforcement learning. Chart-RFT utilize the
typical group relative policy optimization strategy, in which a relatively soft
reward is adopted for numerical response to emphasize the numerical sensitivity
in the chart domain. We conduct extensive experiments on open-source benchmarks
and self-built chart reasoning dataset (\emph{i.e., ChartRQA}). Experimental
results show that Chart-R1 has significant advantages compared to chart-domain
methods, even comparable to open/closed source large-scale models (\emph{e.g.,
GPT-4o, Claude-3.5}).

</details>


### [271] [HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics](https://arxiv.org/abs/2507.15518)
*Sizhou Chen,Shufan Jiang,Chi Zhang,Xiao-Lei Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: 提出了HAMLET，一个通过多智能体框架进行戏剧创作与在线表演的方法，着重提升交互性和沉浸感。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大型语言模型的戏剧生成方法存在AI缺少主动性、无法与物理环境交互以及需要用户提供详细输入等问题，这降低了交互性和沉浸感。

Method: 开发HAMLET框架，根据简单话题生成叙述蓝图，并通过赋予每位演员自主决策能力实现即兴表演，演员可独立决策，并通过动作改变场景道具状态，这些状态改变会实时影响相关演员的行动决策。

Result: 实验评估显示HAMLET能够创造具有表现力和连贯性的戏剧体验，代码和资源已开源。

Conclusion: HAMLET提高了戏剧表演的交互性和沉浸感，为基于大型语言模型的实时戏剧表演提供了一种新的方法。

Abstract: Creating an immersive and interactive theatrical experience is a long-term
goal in the field of interactive narrative. The emergence of large language
model (LLM) is providing a new path to achieve this goal. However, existing
LLM-based drama generation methods often result in AI agents that lack
initiative and cannot interact with the physical environment. Furthermore,
these methods typically require detailed user input to drive the drama. These
limitations reduce the interactivity and immersion of online real-time
performance. To address the above challenges, we propose HAMLET, a multi-agent
framework focused on drama creation and online performance. Given a simple
topic, the framework generates a narrative blueprint, guiding the subsequent
improvisational performance. During the online performance, each actor is given
an autonomous mind. This means that actors can make independent decisions based
on their own background, goals, and emotional state. In addition to
conversations with other actors, their decisions can also change the state of
scene props through actions such as opening a letter or picking up a weapon.
The change is then broadcast to other related actors, updating what they know
and care about, which in turn influences their next action. To evaluate the
quality of drama performance, we designed an evaluation method to assess three
primary aspects, including character performance, narrative quality, and
interaction experience. The experimental evaluation shows that HAMLET can
create expressive and coherent theatrical experiences. Our code, dataset and
models are available at https://github.com/HAMLET-2025/HAMLET.

</details>


### [272] [LLM world models are mental: Output layer evidence of brittle world model use in LLM mechanical reasoning](https://arxiv.org/abs/2507.15521)
*Cole Robertson,Philip Wolff*

Main category: cs.AI

TL;DR: 研究探索大型语言模型是否构建和操控内部世界模型，结果表明模型对机械问题的表现仅略高于随机水平，并可能利用统计关联而不是深入推理。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨大型语言模型是否仅基于统计关联作出决策，或能够有效构建和操纵内部世界模型。

Method: 使用TikZ渲染的滑轮系统刺激物，适配认知科学方法，对模型进行三个实验：估测机械优势（MA）、辨别功能性系统与假系统，以及比较功能性系统与零力传递系统。

Result: 模型在试验1中表现略高于随机水平，试验2表明模型能区分功能性与假系统(F1=0.8)，但试验3中对更复杂结构连接的推理表现接近随机。

Conclusion: 研究表明LLMs可能操控有限的内部模型，但主要依赖统计关联作出推断。认知科学方法对评估人工智能系统的世界建模能力具有意义。

Abstract: Do large language models (LLMs) construct and manipulate internal world
models, or do they rely solely on statistical associations represented as
output layer token probabilities? We adapt cognitive science methodologies from
human mental models research to test LLMs on pulley system problems using
TikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical
advantage (MA). State-of-the-art models performed marginally but significantly
above chance, and their estimates correlated significantly with ground-truth
MA. Significant correlations between number of pulleys and model estimates
suggest that models employed a pulley counting heuristic, without necessarily
simulating pulley systems to derive precise values. Study 2 tested this by
probing whether LLMs represent global features crucial to MA estimation. Models
evaluated a functionally connected pulley system against a fake system with
randomly placed components. Without explicit cues, models identified the
functional system as having greater MA with F1=0.8, suggesting LLMs could
represent systems well enough to differentiate jumbled from functional systems.
Study 3 built on this by asking LLMs to compare functional systems with matched
systems which were connected up but which transferred no force to the weight;
LLMs identified the functional system with F1=0.46, suggesting random guessing.
Insofar as they may generalize, these findings are compatible with the notion
that LLMs manipulate internal world models, sufficient to exploit statistical
associations between pulley count and MA (Study 1), and to approximately
represent system components' spatial relations (Study 2). However, they may
lack the facility to reason over nuanced structural connectivity (Study 3). We
conclude by advocating the utility of cognitive scientific methods to evaluate
the world-modeling capacities of artificial intelligence systems.

</details>


### [273] [Data-Efficient Safe Policy Improvement Using Parametric Structure](https://arxiv.org/abs/2507.15532)
*Kasper Engelen,Guillermo A. Pérez,Marnix Suilen*

Main category: cs.AI

TL;DR: 本文提出了一些方法，通过参数依赖关系和预处理技术改善了安全策略改进（SPI）的数据效率。基于仿真实验，研究展示了其显著提升效果。


<details>
  <summary>Details</summary>
Motivation: 为了在仅使用行为策略和数据集的情况下实现高置信度且稳定的策略改进，并在环境中利用因果关系和约束提升效率。

Method: 1) 提出一种参数SPI算法，通过已知的分布相关性提高转移动态的估计准确性；2) 利用基于游戏的抽象技术对环境中的冗余动作进行剪枝；3) 使用基于理论可满足性的高级预处理技术识别更多冗余动作。

Result: 实验和消融研究表明，所提技术提高了SPI的数据效率达多个数量级，同时保持了策略改进的可靠性。

Conclusion: 通过整合参数依赖性和优化性预处理技术，本文方法在保持原有可靠性的前提下，显著提升了SPI问题的数据利用效率。

Abstract: Safe policy improvement (SPI) is an offline reinforcement learning problem in
which a new policy that reliably outperforms the behavior policy with high
confidence needs to be computed using only a dataset and the behavior policy.
Markov decision processes (MDPs) are the standard formalism for modeling
environments in SPI. In many applications, additional information in the form
of parametric dependencies between distributions in the transition dynamics is
available. We make SPI more data-efficient by leveraging these dependencies
through three contributions: (1) a parametric SPI algorithm that exploits known
correlations between distributions to more accurately estimate the transition
dynamics using the same amount of data; (2) a preprocessing technique that
prunes redundant actions from the environment through a game-based abstraction;
and (3) a more advanced preprocessing technique, based on satisfiability modulo
theory (SMT) solving, that can identify more actions to prune. Empirical
results and an ablation study show that our techniques increase the data
efficiency of SPI by multiple orders of magnitude while maintaining the same
reliability guarantees.

</details>


### [274] [Metric assessment protocol in the context of answer fluctuation on MCQ tasks](https://arxiv.org/abs/2507.15581)
*Ekaterina Goliakova,Xavier Renard,Marie-Jeanne Lesot,Thibault Laugel,Christophe Marsala,Marcin Detyniecki*

Main category: cs.AI

TL;DR: 本文分析了LLM在多项选择题中的表现评估，发现不同的评估指标对答案波动的影响不同，提出了一种新的评估方法并推荐了一种具备高关联性的指标——最差准确率。


<details>
  <summary>Details</summary>
Motivation: 评估LLM性能时，多项选择题（MCQs）通常被采用。然而，目前方法未对各类评价指标的表现进行全面的研究，而答案波动也使得评估不稳定，因此需要找到更稳定且具有高关联性的评估方法。

Method: 提出了一种协议，通过分析评估指标与答案波动率及原始性能之间的联系，对评估方法进行系统化分析。

Result: 结果表明，现有指标与答案波动率之间有显著联系，即使无需额外的提示变化进行计算。而“最差准确率”这一新指标在该协议中的关联性最高。

Conclusion: “最差准确率”是一种更加稳定可靠的LLM能力评估指标，能够更好地应对答案波动问题。

Abstract: Using multiple-choice questions (MCQs) has become a standard for assessing
LLM capabilities efficiently. A variety of metrics can be employed for this
task. However, previous research has not conducted a thorough assessment of
them. At the same time, MCQ evaluation suffers from answer fluctuation: models
produce different results given slight changes in prompts. We suggest a metric
assessment protocol in which evaluation methodologies are analyzed through
their connection with fluctuation rates, as well as original performance. Our
results show that there is a strong link between existing metrics and the
answer changing, even when computed without any additional prompt variants. A
novel metric, worst accuracy, demonstrates the highest association on the
protocol.

</details>


### [275] [TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft II](https://arxiv.org/abs/2507.15618)
*Weiyu Ma,Jiwen Jiang,Haobo Fu,Haifeng Zhang*

Main category: cs.AI

TL;DR: 提出了一种基于适配器的方法来对《星际争霸II》AI代理进行战术条件调整，能够灵活控制战术行为的同时保持竞争性能。


<details>
  <summary>Details</summary>
Motivation: 现有的《星际争霸II》AI代理虽强大，但无法根据高层次战术指令灵活调整其策略。

Method: 通过在预训练的策略网络（DI-Star）上附加轻量化适配器模块，这些模块根据编码战术偏好的张量进行条件化，并用KL散度约束进行训练，保证政策核心能力同时实现战术变化。

Result: 实验表明该方法能够在保持竞争性能的同时，成功调节代理在攻击性、扩展模式和技术偏好等战术维度上的行为。

Conclusion: 该方法在具有最小计算开销的前提下，实现了复杂实时战略游戏的战术灵活控制和策略定制。

Abstract: We present an adapter-based approach for tactical conditioning of StarCraft
II AI agents. Current agents, while powerful, lack the ability to adapt their
strategies based on high-level tactical directives. Our method freezes a
pre-trained policy network (DI-Star) and attaches lightweight adapter modules
to each action head, conditioned on a tactical tensor that encodes strategic
preferences. By training these adapters with KL divergence constraints, we
ensure the policy maintains core competencies while exhibiting tactical
variations. Experimental results show our approach successfully modulates agent
behavior across tactical dimensions including aggression, expansion patterns,
and technology preferences, while maintaining competitive performance. Our
method enables flexible tactical control with minimal computational overhead,
offering practical strategy customization for complex real-time strategy games.

</details>


### [276] [Agentic AI for autonomous anomaly management in complex systems](https://arxiv.org/abs/2507.15676)
*Reza Vatankhah Barenji,Sina Khoshgoftar*

Main category: cs.AI

TL;DR: 探讨代理型人工智能在复杂系统中自主检测和响应异常的潜力。


<details>
  <summary>Details</summary>
Motivation: 减少对人工的依赖，改进传统异常管理方法。

Method: 采用代理型人工智能技术，聚焦于复杂系统的异常检测与响应。

Result: 代理型人工智能展示出改进异常管理效率和响应能力的潜力。

Conclusion: 该技术有望变革传统异常管理方式，提升自主性和效率。

Abstract: This paper explores the potential of agentic AI in autonomously detecting and
responding to anomalies within complex systems, emphasizing its ability to
transform traditional, human-dependent anomaly management methods.

</details>


### [277] [Towards physician-centered oversight of conversational diagnostic AI](https://arxiv.org/abs/2507.15743)
*Elahe Vedadi,David Barrett,Natalie Harris,Ellery Wulczyn,Shashir Reddy,Roma Ruparel,Mike Schaekermann,Tim Strother,Ryutaro Tanno,Yash Sharma,Jihyeon Lee,Cían Hughes,Dylan Slack,Anil Palepu,Jan Freyberg,Khaled Saab,Valentin Liévin,Wei-Hung Weng,Tao Tu,Yun Liu,Nenad Tomasev,Kavita Kulkarni,S. Sara Mahdavi,Kelvin Guu,Joëlle Barral,Dale R. Webster,James Manyika,Avinatan Hassidim,Katherine Chou,Yossi Matias,Pushmeet Kohli,Adam Rodman,Vivek Natarajan,Alan Karthikesalingam,David Stutz*

Main category: cs.AI

TL;DR: 提出一个名为g-AMIE的多代理系统，用于在医生监督下完成诊断对话和信息采集，并通过实验验证其在效率和决策质量上的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在诊断对话中表现出潜力，但对患者安全的要求使得诊断和治疗成为需持证专业人员进行的监管活动。本研究旨在探索一种有效的AI系统监督框架。

Method: 设计并提出了一个名为g-AMIE的多代理系统，通过守护条款进行信息采集而避免个性化医疗建议，然后将评估结果提供给监督医生以进行异步监督。

Result: 在虚拟结构化临床考试中，g-AMIE在信息采集、案例总结及诊断建议方面优于护士及医生助理，且医生对g-AMIE监督的效率优于传统单独医生咨询模式。

Conclusion: 异步监督是一种可行的AI诊断应用模式，有助于在人类专家监督下提升实际医疗护理质量。

Abstract: Recent work has demonstrated the promise of conversational AI systems for
diagnostic dialogue. However, real-world assurance of patient safety means that
providing individual diagnoses and treatment plans is considered a regulated
activity by licensed professionals. Furthermore, physicians commonly oversee
other team members in such activities, including nurse practitioners (NPs) or
physician assistants/associates (PAs). Inspired by this, we propose a framework
for effective, asynchronous oversight of the Articulate Medical Intelligence
Explorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent
system that performs history taking within guardrails, abstaining from
individualized medical advice. Afterwards, g-AMIE conveys assessments to an
overseeing primary care physician (PCP) in a clinician cockpit interface. The
PCP provides oversight and retains accountability of the clinical decision.
This effectively decouples oversight from intake and can thus happen
asynchronously. In a randomized, blinded virtual Objective Structured Clinical
Examination (OSCE) of text consultations with asynchronous oversight, we
compared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across
60 scenarios, g-AMIE outperformed both groups in performing high-quality
intake, summarizing cases, and proposing diagnoses and management plans for the
overseeing PCP to review. This resulted in higher quality composite decisions.
PCP oversight of g-AMIE was also more time-efficient than standalone PCP
consultations in prior work. While our study does not replicate existing
clinical practices and likely underestimates clinicians' capabilities, our
results demonstrate the promise of asynchronous oversight as a feasible
paradigm for diagnostic AI systems to operate under expert human oversight for
enhancing real-world care.

</details>


### [278] [LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization](https://arxiv.org/abs/2507.15758)
*Xingyu Wu,Yuchen Yan,Shangke Lyu,Linjuan Wu,Yiwen Qiu,Yongliang Shen,Weiming Lu,Jian Shao,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: 提出了一种名为LAPO的新方法，通过强化学习控制模型推理过程中的长度，提高推理效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型推理模型在较简单问题上会生成过多的Token，存在计算资源浪费的问题。

Method: 通过两阶段的强化学习方法，第一阶段发现成功解答的推理长度分布，第二阶段将其嵌入模型推理上下文中，实现对推理深度的灵活控制。

Result: 在数学推理基准测试中，LAPO减少了40.9%的Token使用量，同时提高了2.3%的准确率。

Conclusion: 使用LAPO的模型能根据问题复杂度自主分配计算资源，实现高效而高质量的推理。

Abstract: Large reasoning models have achieved remarkable performance through extended
chain-of-thought sequences, yet this computational freedom leads to excessive
token generation even for simple problems. We present Length-Adaptive Policy
Optimization (LAPO), a novel framework that transforms reasoning length control
from an external constraint into an intrinsic model capability. Unlike existing
approaches that impose rigid limits or rely on post-hoc interventions, LAPO
enables models to internalize an understanding of appropriate reasoning depth
through a two-stage reinforcement learning process. In the first stage, models
learn natural reasoning patterns by discovering the statistical distribution of
successful solution lengths. The second stage leverages these patterns as
meta-cognitive guidance, embedding them directly within the model's reasoning
context to ensure inference-time flexibility. Experiments on mathematical
reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\%
while improving accuracy by 2.3\%. Our analysis reveals that models trained
with LAPO develop emergent abilities to allocate computational resources based
on problem complexity, achieving efficient reasoning without sacrificing
quality.

</details>


### [279] [GasAgent: A Multi-Agent Framework for Automated Gas Optimization in Smart Contracts](https://arxiv.org/abs/2507.15761)
*Jingyi Zheng,Zifan Peng,Yule Liu,Junfeng Wang,Yifan Liao,Wenhan Dong,Xinlei He*

Main category: cs.AI

TL;DR: 研究提出了GasAgent，一个用于智能合约Gas优化的多代理系统，通过自动发现与验证，实现尾端优化，实验表明其高效性与兼容性。


<details>
  <summary>Details</summary>
Motivation: 由于智能合约普遍存在Gas浪费现象，现有解决方案手动发现成本高、效率低且难以拓展，而基于大型语言模型的方法则存在兼容性及冗余问题，需要自动化的新方法。

Method: 提出了一个名为GasAgent的四代理协作系统，包括Seeker、Innovator、Executor和Manager，通过闭环协作实现合约Gas优化，能够自动发现、验证和应用新的优化模式。

Result: 实验分析100个真实合约，其中82个成功优化，平均Gas节省9.97%；评估500个由LLM生成的合约，优化率达79.8%，节省幅度在4.79%-13.93%之间。

Conclusion: GasAgent兼容现有工具，验证了其模块有效性和广泛适用性，展示出其作为LLM辅助智能合约开发优化层的潜力。

Abstract: Smart contracts are trustworthy, immutable, and automatically executed
programs on the blockchain. Their execution requires the Gas mechanism to
ensure efficiency and fairness. However, due to non-optimal coding practices,
many contracts contain Gas waste patterns that need to be optimized. Existing
solutions mostly rely on manual discovery, which is inefficient, costly to
maintain, and difficult to scale. Recent research uses large language models
(LLMs) to explore new Gas waste patterns. However, it struggles to remain
compatible with existing patterns, often produces redundant patterns, and
requires manual validation/rewriting. To address this gap, we present GasAgent,
the first multi-agent system for smart contract Gas optimization that combines
compatibility with existing patterns and automated discovery/validation of new
patterns, enabling end-to-end optimization. GasAgent consists of four
specialized agents, Seeker, Innovator, Executor, and Manager, that collaborate
in a closed loop to identify, validate, and apply Gas-saving improvements.
Experiments on 100 verified real-world contracts demonstrate that GasAgent
successfully optimizes 82 contracts, achieving an average deployment Gas
savings of 9.97%. In addition, our evaluation confirms its compatibility with
existing tools and validates the effectiveness of each module through ablation
studies. To assess broader usability, we further evaluate 500 contracts
generated by five representative LLMs across 10 categories and find that
GasAgent optimizes 79.8% of them, with deployment Gas savings ranging from
4.79% to 13.93%, showing its usability as the optimization layer for
LLM-assisted smart contract development.

</details>


### [280] [A Framework for Analyzing Abnormal Emergence in Service Ecosystems Through LLM-based Agent Intention Mining](https://arxiv.org/abs/2507.15770)
*Yifan Shen,Zihan Zhao,Xiao Xue,Yuwei Guo,Qun Ma,Deyu Zhou,Ming Zhang*

Main category: cs.AI

TL;DR: 论文提出EAMI框架，通过分析智能体之间的动态意图转变，解决复杂服务生态系统中的异常涌现分析问题。


<details>
  <summary>Details</summary>
Motivation: 服务计算、云计算和物联网的发展导致服务生态系统复杂化，需要解决其中智能体复杂交互导致的异常涌现分析问题。现有方法主要集中于微观和静态分析，无法动态揭示智能体意图的因果关系。

Method: 论文提出EAMI框架，采用双视角思想轨迹机制（Inspector Agent和Analysis Agent）提取智能体的意图，通过k-means聚类识别意图的相变点，并使用意图时序涌现图进行动态分析。

Result: 在复杂的线上到线下（O2O）服务系统和斯坦福AI Town实验中验证了EAMI框架的有效性、通用性和效率，并通过消融实验进一步确认。

Conclusion: EAMI框架为服务生态系统中的异常涌现和因果分析提供了一种动态可解释的新范式，相关代码已开源。

Abstract: With the rise of service computing, cloud computing, and IoT, service
ecosystems are becoming increasingly complex. The intricate interactions among
intelligent agents make abnormal emergence analysis challenging, as traditional
causal methods focus on individual trajectories. Large language models offer
new possibilities for Agent-Based Modeling (ABM) through Chain-of-Thought (CoT)
reasoning to reveal agent intentions. However, existing approaches remain
limited to microscopic and static analysis. This paper introduces a framework:
Emergence Analysis based on Multi-Agent Intention (EAMI), which enables dynamic
and interpretable emergence analysis. EAMI first employs a dual-perspective
thought track mechanism, where an Inspector Agent and an Analysis Agent extract
agent intentions under bounded and perfect rationality. Then, k-means
clustering identifies phase transition points in group intentions, followed by
a Intention Temporal Emergence diagram for dynamic analysis. The experiments
validate EAMI in complex online-to-offline (O2O) service system and the
Stanford AI Town experiment, with ablation studies confirming its
effectiveness, generalizability, and efficiency. This framework provides a
novel paradigm for abnormal emergence and causal analysis in service
ecosystems. The code is available at
https://anonymous.4open.science/r/EAMI-B085.

</details>


### [281] [Challenges of Trustworthy Federated Learning: What's Done, Current Trends and Remaining Work](https://arxiv.org/abs/2507.15796)
*Nuria Rodríguez-Barroso,Mario García-Márquez,M. Victoria Luzón,Francisco Herrera*

Main category: cs.AI

TL;DR: 本文探讨了可信人工智能（TAI）框架如何作为指导，系统分析联邦学习（FL）面临的挑战，特别是在满足TAI的各项要求方面。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术应用于敏感和高风险领域，发展可信人工智能（TAI）成为关键目标，而联邦学习（FL）因其隐私保护特点尤为引人关注。

Method: 作者采用TAI框架作为分析结构，分类并详细探讨了FL在满足TAI要求时面临的关键障碍，分析现状、趋势及未来工作。

Result: 本文识别和分类了FL向TAI对齐所遇到的主要挑战，并对解决进展进行了系统总结。

Conclusion: 通过系统梳理，研究展示了FL在满足TAI需求方面的主要难点及未来潜力，为进一步发展提供了方向。

Abstract: In recent years, the development of Trustworthy Artificial Intelligence (TAI)
has emerged as a critical objective in the deployment of AI systems across
sensitive and high-risk domains. TAI frameworks articulate a comprehensive set
of ethical, legal, and technical requirements to ensure that AI technologies
are aligned with human values, rights, and societal expectations. Among the
various AI paradigms, Federated Learning (FL) presents a promising solution to
pressing privacy concerns. However, aligning FL with the rest of the
requirements of TAI presents a series of challenges, most of which arise from
its inherently distributed nature. In this work, we adopt the requirements TAI
as a guiding structure to systematically analyze the challenges of adapting FL
to TAI. Specifically, we classify and examine the key obstacles to aligning FL
with TAI, providing a detailed exploration of what has been done, the trends,
and the remaining work within each of the identified challenges.

</details>


### [282] [Identifying Conditional Causal Effects in MPDAGs](https://arxiv.org/abs/2507.15842)
*Sara LaPlante,Emilija Perković*

Main category: cs.AI

TL;DR: 研究在MPDAG中如何识别条件因果效应，提出了识别公式、广义do计算及完整算法。


<details>
  <summary>Details</summary>
Motivation: 旨在解决在已知MPDAG图条件下，如何有效识别条件因果效应的问题。

Method: 提出了一种识别公式，推广了do计算方法，并设计了一个完整识别算法。

Result: 在MPDAG背景下实现了对条件因果效应的有效识别。

Conclusion: 提供了一个系统性的方法来处理MPDAG条件结构下的因果效应识别难题。

Abstract: We consider identifying a conditional causal effect when a graph is known up
to a maximally oriented partially directed acyclic graph (MPDAG). An MPDAG
represents an equivalence class of graphs that is restricted by background
knowledge and where all variables in the causal model are observed. We provide
three results that address identification in this setting: an identification
formula when the conditioning set is unaffected by treatment, a generalization
of the well-known do calculus to the MPDAG setting, and an algorithm that is
complete for identifying these conditional effects.

</details>


### [283] [Hierarchical Budget Policy Optimization for Adaptive Reasoning](https://arxiv.org/abs/2507.15844)
*Shangke Lyu,Linjuan Wu,Yuchen Yan,Xingyu Wu,Hao Li,Yongliang Shen,Peisheng Jiang,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: 提出一种称为层次化预算策略优化（HBPO）的框架，通过分层预算探索和差异化奖励机制优化推理模型的效率与性能。


<details>
  <summary>Details</summary>
Motivation: 现有的推理模型在处理不同复杂性问题时，采用统一的推理策略，导致计算效率低下。需要一种方法在不牺牲性能的情况下，根据问题复杂度优化推理深度。

Method: 提出HBPO框架，通过分层预算探索将输出样本划分为多个子组，每组具有不同的令牌预算，并采用差异化的奖励机制，激励模型根据任务要求自适应调整计算资源分配。

Result: HBPO在四个推理基准上，平均令牌使用量减少了60.6%，同时准确率提高了3.14%。

Conclusion: 通过合理的分层训练结构，HBPO证明效率与能力可以同时优化，解决了现有方法中效率与性能之间的矛盾。

Abstract: Large reasoning models achieve remarkable performance through extensive
chain-of-thought generation, yet exhibit significant computational inefficiency
by applying uniform reasoning strategies regardless of problem complexity. We
present Hierarchical Budget Policy Optimization (HBPO), a reinforcement
learning framework that enables models to learn problem-specific reasoning
depths without sacrificing capability. HBPO addresses the fundamental challenge
of exploration space collapse in efficiency-oriented training, where penalties
on long output length systematically bias models away from necessary long
reasoning paths. Through hierarchical budget exploration, our approach
partitions rollout samples into multiple subgroups with distinct token budgets,
aiming to enable efficient resource allocation while preventing degradation of
capability. We introduce differentiated reward mechanisms that create
budget-aware incentives aligned with the complexity of the problem, allowing
models to discover natural correspondences between task requirements and
computational effort. Extensive experiments demonstrate that HBPO reduces
average token usage by up to 60.6% while improving accuracy by 3.14% across
four reasoning benchmarks. Unlike existing methods that impose external
constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive
behavior where models automatically adjust reasoning depth based on problem
complexity. Our results suggest that reasoning efficiency and capability are
not inherently conflicting, and can be simultaneously optimized through
appropriately structured hierarchical training that preserves exploration
diversity.

</details>


### [284] [The Other Mind: How Language Models Exhibit Human Temporal Cognition](https://arxiv.org/abs/2507.15851)
*Lingyu Li,Yang Yao,Yixu Wang,Chubo Li,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: 研究发现大型语言模型(LLMs)在时间认知上表现出类似人类的特点，遵循韦伯-费希纳定律，时间感知呈对数压缩特性。分析揭示其内部结构包含时间偏好神经元和非线性时间结构。


<details>
  <summary>Details</summary>
Motivation: 探讨为何LLMs在未明确训练时间认知的情况下表现出人类类似的时间感知方式。

Method: 通过时间相似性判断任务，结合神经元、表示和信息层面多种分析方法，为探究LLMs的时间认知机制提供多角度证据。

Result: 发现LLMs中存在时间偏好的神经元群体，具非线性时间感知规律，且模型训练语料中自带隐含的时间结构支持其内部表征。

Conclusion: 提出LLMs可能形成无法人类直观预测的认知框架，对AI对齐问题提供新方向，强调引导内部构建的重要性。

Abstract: As Large Language Models (LLMs) continue to advance, they exhibit certain
cognitive patterns similar to those of humans that are not directly specified
in training data. This study investigates this phenomenon by focusing on
temporal cognition in LLMs. Leveraging the similarity judgment task, we find
that larger models spontaneously establish a subjective temporal reference
point and adhere to the Weber-Fechner law, whereby the perceived distance
logarithmically compresses as years recede from this reference point. To
uncover the mechanisms behind this behavior, we conducted multiple analyses
across neuronal, representational, and informational levels. We first identify
a set of temporal-preferential neurons and find that this group exhibits
minimal activation at the subjective reference point and implements a
logarithmic coding scheme convergently found in biological systems. Probing
representations of years reveals a hierarchical construction process, where
years evolve from basic numerical values in shallow layers to abstract temporal
orientation in deep layers. Finally, using pre-trained embedding models, we
found that the training corpus itself possesses an inherent, non-linear
temporal structure, which provides the raw material for the model's internal
construction. In discussion, we propose an experientialist perspective for
understanding these findings, where the LLMs' cognition is viewed as a
subjective construction of the external world by its internal representational
system. This nuanced perspective implies the potential emergence of alien
cognitive frameworks that humans cannot intuitively predict, pointing toward a
direction for AI alignment that focuses on guiding internal constructions. Our
code is available at https://TheOtherMind.github.io.

</details>


### [285] [Gemini 2.5 Pro Capable of Winning Gold at IMO 2025](https://arxiv.org/abs/2507.15855)
*Yichen Huang,Lin F. Yang*

Main category: cs.AI

TL;DR: 本文使用谷歌的Gemini 2.5 Pro模型解决IMO 2025问题，成功解决了5道题目。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过优化设计和提示工程改善LLM在奥数问题上的表现。

Method: 采用谷歌Gemini 2.5 Pro模型并结合管道设计和提示工程进行实验。

Result: 在6道IMO 2025题目中，有5道被成功解答（包含某些限制条件）。

Conclusion: 适当的方法可以有效提升强大模型在奥数问题上的表现。

Abstract: The International Mathematical Olympiad (IMO) poses uniquely challenging
problems requiring deep insight, creativity, and formal reasoning. While Large
Language Models (LLMs) perform well on mathematical benchmarks like AIME, they
struggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly
released IMO 2025 problems, avoiding data contamination. With pipeline design
and prompt engineering, 5 (out of 6) problems are solved correctly (up to a
caveat discussed below), highlighting the importance of finding the optimal way
of using powerful models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [286] [Catalyst: a Novel Regularizer for Structured Pruning with Auxiliary Extension of Parameter Space](https://arxiv.org/abs/2507.14170)
*Jaeheun Jung,Donghun Lee*

Main category: cs.LG

TL;DR: 本文提出了一种名为Catalyst Pruning的新方法，通过引入辅助变量的方式构建全新正则化器，在实现公平剪枝的同时，提升模型的鲁棒性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有的结构化剪枝方法如L1或Group Lasso正则化存在两个主要问题：1) 剪枝决策易受到权重大小的偏置影响，导致小权重滤波器更有可能被剪枝；2) 剪枝决策边界缺乏足够的裕量，轻微的扰动即可改变剪枝结果。

Method: 本文通过形式化剪枝对模型性能影响的代数条件，提出了一种基于辅助变量的Catalyst正则化器。该正则化方法旨在零偏差地对滤波器进行公平剪枝，并通过拉大剪枝与保留滤波器权重之间的幅度距离，实现鲁棒的剪枝行为。

Result: 实验结果显示，Catalyst Pruning在多个数据集和模型上都优于现有剪枝方法，不仅具有较高的模型压缩效果，还展现出公平性和鲁棒性的剪枝特性。

Conclusion: Catalyst Pruning通过构造鲁棒公平的剪枝机制有效提升了模型性能和剪枝效果，为深度网络压缩提供了理论和实践上的新视角。

Abstract: Structured pruning aims to reduce the size and computational cost of deep
neural networks by removing entire filters or channels. The traditional
regularizers such as L1 or Group Lasso and its variants lead to
magnitude-biased pruning decisions, such that the filters with small magnitudes
are likely to be pruned. Also, they often entail pruning results with almost
zero margin around pruning decision boundary, such that tiny perturbation in a
filter magnitude can flip the pruning decision. In this paper, we identify the
precise algebraic condition under which pruning operations preserve model
performance, and use the condition to construct a novel regularizer defined in
an extended parameter space via auxiliary catalyst variables. The proposed
Catalyst regularization ensures fair pruning chance for each filters with
theoretically provable zero bias to their magnitude and robust pruning behavior
achieved by wide-margin bifurcation of magnitudes between the preserved and the
pruned filters. The theoretical properties naturally lead to real-world
effectiveness, as shown by empirical validations of Catalyst Pruning algorithm.
Pruning results on various datasets and models are superior to state-of-the-art
filter pruning methods, and at the same time confirm the predicted robust and
fair pruning characteristics of Catalyst pruning.

</details>


### [287] [IPPRO: Importance-based Pruning with PRojective Offset for Magnitude-indifferent Structural Pruning](https://arxiv.org/abs/2507.14171)
*Jaeheun Jung,Jaehyuk Lee,Yeajin Lee,Donghun Lee*

Main category: cs.LG

TL;DR: 提出一种无关于权重大小的新型剪枝方法，通过观察梯度下降中的移动情况来判定冗余权重，以改进结构化剪枝的公平性及效果。


<details>
  <summary>Details</summary>
Motivation: 现有重要性标准常受滤波器大小限制，可能导致重要性评估失衡，提出优化剪枝方法以突破此局限。

Method: 通过将滤波器置于投影空间后观察其梯度变化，提出新的重要性评分（PROscore），用于全新的剪枝策略（IPPRO），摆脱对权重大小的依赖。

Result: 实验数据显示，该方法在保持模型性能的基础上显著降低了剪枝后的性能损失，并在重训练后表现出色。

Conclusion: 挑战了剪枝中“权重大小主导”的传统观点，为重要性评估的理论和实践提供了一种更公平和有效的方案。

Abstract: With the growth of demand on neural network compression methods, the
structured pruning methods including importance-based approach are actively
studied. The magnitude importance and many correlated modern importance
criteria often limit the capacity of pruning decision, since the filters with
larger magnitudes are not likely to be pruned if the smaller one didn't, even
if it is redundant. In this paper, we propose a novel pruning strategy to
challenge this dominating effect of magnitude and provide fair chance to each
filter to be pruned, by placing it on projective space. After that, we observe
the gradient descent movement whether the filters move toward the origin or
not, to measure how the filter is likely to be pruned. This measurement is used
to construct PROscore, a novel importance score for IPPRO, a novel
importance-based structured pruning with magnitude-indifference. Our evaluation
results shows that the proposed importance criteria using the projective space
achieves near-lossless pruning by reducing the performance drop in pruning,
with promising performance after the finetuning. Our work debunks the
``size-matters'' myth in pruning and expands the frontier of importance-based
pruning both theoretically and empirically.

</details>


### [288] [Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI](https://arxiv.org/abs/2507.14172)
*Julien Pourcel,Cédric Colas,Pierre-Yves Oudeyer*

Main category: cs.LG

TL;DR: 该论文提出了一种名为SOAR的方法，将语言模型结合到一个自我改进的进化循环中，以解决编程合成问题，并在ARC-AGI基准上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 许多编程合成任务过于复杂，即使是最先进的语言模型也难以在一次尝试中解决，而现有的基于搜索的进化方法受制于生成模型的固定能力。

Method: SOAR方法结合了进化搜索与语言模型，通过交替进行候选解决方案的抽样与精细化，以及将搜索尝试转化为问题-解决方案对以微调语言模型，逐步改进其能力。

Result: 在ARC-AGI基准上，SOAR通过采样和精细化微调任务之间的正迁移，实现了显著性能提升，从而在公共测试集上解决了52%的问题。

Conclusion: SOAR展示了利用语言模型在进化搜索中的自我改进能力，为解决复杂编程合成任务提供了一种有效的方法。

Abstract: Many program synthesis tasks prove too challenging for even state-of-the-art
language models to solve in single attempts. Search-based evolutionary methods
offer a promising alternative by exploring solution spaces iteratively, but
their effectiveness remain limited by the fixed capabilities of the underlying
generative model.
  We propose SOAR, a method that learns program synthesis by integrating
language models into a self-improving evolutionary loop.
  SOAR alternates between (1) an evolutionary search that uses an LLM to sample
and refine candidate solutions, and (2) a hindsight learning phase that
converts search attempts into valid problem-solution pairs used to fine-tune
the LLM's sampling and refinement capabilities\, -- \,enabling increasingly
effective search in subsequent iterations.
  On the challenging ARC-AGI benchmark, SOAR achieves significant performance
gains across model scales and iterations, leveraging positive transfer between
the sampling and refinement finetuning tasks. These improvements carry over to
test-time adaptation, enabling SOAR to solve 52\% of the public test set. Our
code is open-sourced at: https://github.com/flowersteam/SOAR

</details>


### [289] [Pruning Increases Orderedness in Recurrent Computation](https://arxiv.org/abs/2507.14747)
*Yiding Song*

Main category: cs.LG

TL;DR: 研究将方向性作为人工神经网络的归纳偏差，并探讨通过剪枝技术诱导方向性的可行性。


<details>
  <summary>Details</summary>
Motivation: 研究方向性作为一种归纳偏差在人工神经网络中的作用，受到生物学大脑中循环回路的启发。

Method: 通过一个具备全连接的感知机层进行数学形式化，这相当于一个权重共享的循环神经网络，使用剪枝技术诱导方向性。

Result: 通过不同随机种子的实验，剪枝方案成功增加了神经元之间信息流动的拓扑有序性，同时没有损害性能。

Conclusion: 方向性并非学习的先决条件，但可以作为一种有利的归纳偏差通过梯度下降和稀疏化发现。

Abstract: Inspired by the prevalence of recurrent circuits in biological brains, we
investigate the degree to which directionality is a helpful inductive bias for
artificial neural networks. Taking directionality as topologically-ordered
information flow between neurons, we formalise a perceptron layer with
all-to-all connections (mathematically equivalent to a weight-tied recurrent
neural network) and demonstrate that directionality, a hallmark of modern
feed-forward networks, can be induced rather than hard-wired by applying
appropriate pruning techniques. Across different random seeds our pruning
schemes successfully induce greater topological ordering in information flow
between neurons without compromising performance, suggesting that
directionality is not a prerequisite for learning, but may be an advantageous
inductive bias discoverable by gradient descent and sparsification.

</details>


### [290] [Latent Space Data Fusion Outperforms Early Fusion in Multimodal Mental Health Digital Phenotyping Data](https://arxiv.org/abs/2507.14175)
*Youcef Barkat,Dylan Hamitouche,Deven Parekh,Ivy Guo,David Benrimoh*

Main category: cs.LG

TL;DR: 研究比较了传统随机森林方法和一种结合自编码器和神经网络的模型（CM）在预测抑郁症状方面的表现，发现后者在多模态数据融合中表现更优。


<details>
  <summary>Details</summary>
Motivation: 改进精神疾病的早期检测和个性化干预，探索能有效整合多模态复杂数据的方法。

Method: 利用BRIGHTEN临床试验数据，采用中介融合（隐空间融合）方法，通过自编码器和神经网络建立的结合模型，与随机森林和线性回归进行对比分析，用不同数据组合及时间分割实施实验。

Result: 结合模型（CM）在所有实验设置中表现优于基准模型，MSE为0.4985高于随机森林的0.5305，且R2也得到明显优化；其能有效整合多模态数据，且泛化能力更佳。

Conclusion: 隐空间融合是一种强有力的多模态数据整合方法，未来应研究模型可解释性及临床应用的个体预测能力。

Abstract: Background: Mental illnesses such as depression and anxiety require improved
methods for early detection and personalized intervention. Traditional
predictive models often rely on unimodal data or early fusion strategies that
fail to capture the complex, multimodal nature of psychiatric data. Advanced
integration techniques, such as intermediate (latent space) fusion, may offer
better accuracy and clinical utility. Methods: Using data from the BRIGHTEN
clinical trial, we evaluated intermediate (latent space) fusion for predicting
daily depressive symptoms (PHQ-2 scores). We compared early fusion implemented
with a Random Forest (RF) model and intermediate fusion implemented via a
Combined Model (CM) using autoencoders and a neural network. The dataset
included behavioral (smartphone-based), demographic, and clinical features.
Experiments were conducted across multiple temporal splits and data stream
combinations. Performance was evaluated using mean squared error (MSE) and
coefficient of determination (R2). Results: The CM outperformed both RF and
Linear Regression (LR) baselines across all setups, achieving lower MSE (0.4985
vs. 0.5305 with RF) and higher R2 (0.4695 vs. 0.4356). The RF model showed
signs of overfitting, with a large gap between training and test performance,
while the CM maintained consistent generalization. Performance was best when
integrating all data modalities in the CM (in contradistinction to RF),
underscoring the value of latent space fusion for capturing non-linear
interactions in complex psychiatric datasets. Conclusion: Latent space fusion
offers a robust alternative to traditional fusion methods for prediction with
multimodal mental health data. Future work should explore model
interpretability and individual-level prediction for clinical deployment.

</details>


### [291] [Transforming Datasets to Requested Complexity with Projection-based Many-Objective Genetic Algorithm](https://arxiv.org/abs/2507.15132)
*Joanna Komorniczak*

Main category: cs.LG

TL;DR: 该研究提出一种遗传算法，生成具备不同问题复杂性的合成数据集，用于分类和回归任务。


<details>
  <summary>Details</summary>
Motivation: 研究社区希望使用更高级的合成数据生成器来评估机器学习方法的优劣，提高各种问题复杂性数据集的可用性。

Method: 使用遗传算法，对分类任务的10个复杂性指标和回归任务的4个复杂性指标进行优化，通过线性特征投影实现目标复杂性数据集的生成。

Result: 实验验证了该算法可针对目标复杂性生成不同难度的数据集，且数据复杂性与模型识别性能存在相关性。

Conclusion: 所提算法能生成适应于评估机器学习方法性能的多样化合成数据，有助于问题复杂性分析。

Abstract: The research community continues to seek increasingly more advanced synthetic
data generators to reliably evaluate the strengths and limitations of machine
learning methods. This work aims to increase the availability of datasets
encompassing a diverse range of problem complexities by proposing a genetic
algorithm that optimizes a set of problem complexity measures for
classification and regression tasks towards specific targets. For
classification, a set of 10 complexity measures was used, while for regression
tasks, 4 measures demonstrating promising optimization capabilities were
selected. Experiments confirmed that the proposed genetic algorithm can
generate datasets with varying levels of difficulty by transforming
synthetically created datasets to achieve target complexity values through
linear feature projections. Evaluations involving state-of-the-art classifiers
and regressors revealed a correlation between the complexity of the generated
data and the recognition quality.

</details>


### [292] [Predictive Representativity: Uncovering Racial Bias in AI-based Skin Cancer Detection](https://arxiv.org/abs/2507.14176)
*Andrés Morales-Forero,Lili J. Rueda,Ronald Herrera,Samuel Bassetto,Eric Coatanea*

Main category: cs.LG

TL;DR: 本文探讨人工智能在医疗决策中的偏差问题，提出预测代表性(PR)框架，并在皮肤病分类中进行案例研究，揭示了肤色差异对模型表现的影响。


<details>
  <summary>Details</summary>
Motivation: 关注AI在医疗领域的公平性问题，尤其是在服务历史上被边缘化人群时可能出现的不公正结果。

Method: 提出预测代表性(PR)框架和外部可传递性标准，通过对皮肤病AI分类器的性能分析评估其公平性。使用HAM10000数据集和来自哥伦比亚的BOSQUE测试集，研究了不同肤色人群分类结果的差异。

Result: 皮肤癌分类器在肤色较深人群中的表现显著低于肤色较浅人群，尽管训练数据集已包含肤色比例样本。

Conclusion: 数据集的代表性应从动态、上下文敏感的视角理解，提出后期公平性审计、数据集透明化和包容性模型验证的必要性。

Abstract: Artificial intelligence (AI) systems increasingly inform medical
decision-making, yet concerns about algorithmic bias and inequitable outcomes
persist, particularly for historically marginalized populations. This paper
introduces the concept of Predictive Representativity (PR), a framework of
fairness auditing that shifts the focus from the composition of the data set to
outcomes-level equity. Through a case study in dermatology, we evaluated
AI-based skin cancer classifiers trained on the widely used HAM10000 dataset
and on an independent clinical dataset (BOSQUE Test set) from Colombia. Our
analysis reveals substantial performance disparities by skin phototype, with
classifiers consistently underperforming for individuals with darker skin,
despite proportional sampling in the source data. We argue that
representativity must be understood not as a static feature of datasets but as
a dynamic, context-sensitive property of model predictions. PR operationalizes
this shift by quantifying how reliably models generalize fairness across
subpopulations and deployment contexts. We further propose an External
Transportability Criterion that formalizes the thresholds for fairness
generalization. Our findings highlight the ethical imperative for post-hoc
fairness auditing, transparency in dataset documentation, and inclusive model
validation pipelines. This work offers a scalable tool for diagnosing
structural inequities in AI systems, contributing to discussions on equity,
interpretability, and data justice and fostering a critical re-evaluation of
fairness in data-driven healthcare.

</details>


### [293] [Understanding Two-Layer Neural Networks with Smooth Activation Functions](https://arxiv.org/abs/2507.14177)
*Changcun Huang*

Main category: cs.LG

TL;DR: 本论文研究通过反向传播算法训练的两层神经网络的解决方案，揭示解决方案空间中的“黑箱”。


<details>
  <summary>Details</summary>
Motivation: 探讨两层神经网络的训练过程及其解决方案原理，为解开模型“黑箱”提供理论支持。

Method: 结合泰勒级数展开、节点的严格偏序、平滑样条实现和平滑连续性约束的四大原理，进行理论证明与实验验证。

Result: 证明了在任意输入维度下的通用近似性，同时通过实验验证揭示了解决方案的机制。

Conclusion: 提出的新证明丰富了近似理论，并对神经网络“黑箱”现象提供了理论与实践支持。

Abstract: This paper aims to understand the training solution, which is obtained by the
back-propagation algorithm, of two-layer neural networks whose hidden layer is
composed of the units with smooth activation functions, including the usual
sigmoid type most commonly used before the advent of ReLUs. The mechanism
contains four main principles: construction of Taylor series expansions, strict
partial order of knots, smooth-spline implementation and smooth-continuity
restriction. The universal approximation for arbitrary input dimensionality is
proved and experimental verification is given, through which the mystery of
``black box'' of the solution space is largely revealed. The new proofs
employed also enrich approximation theory.

</details>


### [294] [Feature Bank Enhancement for Distance-based Out-of-Distribution Detection](https://arxiv.org/abs/2507.14178)
*Yuhang Liu,Yuefei Wu,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: 提出了一种简单但有效的特征库增强方法（FBE），通过约束极端特征有效提高了OOD检测性能，在ImageNet-1k和CIFAR-10基准上达到了最新的技术表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基于距离的评分方法由于深度学习特征分布的偏差，容易导致ID样本得分过低，从而影响OOD检测能力。

Method: 提出了特征库增强方法（FBE），利用数据集的统计特性识别并约束极端特征，使得分布内外样本间的距离更远。

Result: 实验表明该方法在ImageNet-1k和CIFAR-10基准上达成了最新的技术水平，并通过理论分析和补充实验进一步验证了方法的有效性。

Conclusion: 通过特征约束提升OOD检测能力的方法，不仅简单高效，还在多个基准上取得了领先的性能表现，显示了其实际应用潜力。

Abstract: Out-of-distribution (OOD) detection is critical to ensuring the reliability
of deep learning applications and has attracted significant attention in recent
years. A rich body of literature has emerged to develop efficient score
functions that assign high scores to in-distribution (ID) samples and low
scores to OOD samples, thereby helping distinguish OOD samples. Among these
methods, distance-based score functions are widely used because of their
efficiency and ease of use. However, deep learning often leads to a biased
distribution of data features, and extreme features are inevitable. These
extreme features make the distance-based methods tend to assign too low scores
to ID samples. This limits the OOD detection capabilities of such methods. To
address this issue, we propose a simple yet effective method, Feature Bank
Enhancement (FBE), that uses statistical characteristics from dataset to
identify and constrain extreme features to the separation boundaries, therapy
making the distance between samples inside and outside the distribution
farther. We conducted experiments on large-scale ImageNet-1k and CIFAR-10
respectively, and the results show that our method achieves state-of-the-art
performance on both benchmark. Additionally, theoretical analysis and
supplementary experiments are conducted to provide more insights into our
method.

</details>


### [295] [A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering](https://arxiv.org/abs/2507.14179)
*Nobel Dhar,Bobin Deng,Md Romyull Islam,Xinyue Zhang,Kazi Fahim Ahmad Nasif,Kun Suo*

Main category: cs.LG

TL;DR: 该论文提出了基于聚类的方法来压缩大语言模型（LLMs）的激活模式，通过对相似的激活模式进行聚类，可以有效减少计算开销并保持模型质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型中存在显著的激活稀疏性，仅部分神经元会被激活。然而，利用这种稀疏性需要可扩展的预测激活模式的方法，目前直接在神经元层面进行预测计算成本过高。

Method: 论文提出了一种基于聚类的激活模式压缩框架，通过将相似的激活模式分组为少量代表性聚类，来简化激活模式的表示。

Result: 该方法实现了79.34%的聚类精度，在保持困惑度（PPL）分数最小退化的情况下优于标准二元聚类方法，并在足够多的聚类数量下可达低至12.49的PPL分数。

Conclusion: 通过预测聚类分配而非单个神经元状态，该方法展示了提升稀疏计算效率的潜力，为未来激活模式预测研究奠定了基础，促进大规模语言模型的高效推理。

Abstract: Large Language Models (LLMs) exhibit significant activation sparsity, where
only a subset of neurons are active for a given input. Although this sparsity
presents opportunities to reduce computational cost, efficiently utilizing it
requires predicting activation patterns in a scalable manner. However, direct
prediction at the neuron level is computationally expensive due to the vast
number of neurons in modern LLMs. To enable efficient prediction and
utilization of activation sparsity, we propose a clustering-based activation
pattern compression framework. Instead of treating each neuron independently,
we group similar activation patterns into a small set of representative
clusters. Our method achieves up to 79.34% clustering precision, outperforming
standard binary clustering approaches while maintaining minimal degradation in
perplexity (PPL) scores. With a sufficiently large number of clusters, our
approach attains a PPL score as low as 12.49, demonstrating its effectiveness
in preserving model quality while reducing computational overhead. By
predicting cluster assignments rather than individual neuron states, future
models can efficiently infer activation patterns from pre-computed centroids.
We detail the clustering algorithm, analyze its effectiveness in capturing
meaningful activation structures, and demonstrate its potential to improve
sparse computation efficiency. This clustering-based formulation serves as a
foundation for future work on activation pattern prediction, paving the way for
efficient inference in large-scale language models.

</details>


### [296] [Digital Twin-Assisted Explainable AI for Robust Beam Prediction in mmWave MIMO Systems](https://arxiv.org/abs/2507.14180)
*Nasir Khan,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil,Sinem Coleri*

Main category: cs.LG

TL;DR: 该论文提出了一种用于毫米波通信的稳健且可解释的深度学习波束对齐引擎，通过减少波束训练开销和提高透明性，实现了近似最优的频谱效率。


<details>
  <summary>Details</summary>
Motivation: 为了适应6G的AI原生视野，毫米波系统需要提高其可解释性和鲁棒性，以建立信任并确保可靠性能。

Method: 利用数字孪生技术生成合成信道数据，结合最小实测数据的迁移学习进行模型优化。引入SHAP方法评估输入特征重要性，并利用DkNN算法提供异常输入的检测能力。

Result: 实验结果表明，该框架减少了70%的实测数据需求以及62%的波束训练开销，同时将异常检测能力提高了8.5倍。

Conclusion: 该工作显著优化了毫米波MIMO系统的波束对齐，并兼顾了稳健性和透明性，为未来6G通信技术的发展奠定了基础。

Abstract: In line with the AI-native 6G vision, explainability and robustness are
crucial for building trust and ensuring reliable performance in millimeter-wave
(mmWave) systems. Efficient beam alignment is essential for initial access, but
deep learning (DL) solutions face challenges, including high data collection
overhead, hardware constraints, lack of explainability, and susceptibility to
adversarial attacks. This paper proposes a robust and explainable DL-based beam
alignment engine (BAE) for mmWave multiple-input multiple output (MIMO)
systems. The BAE uses received signal strength indicator (RSSI) measurements
from wide beams to predict the best narrow beam, reducing the overhead of
exhaustive beam sweeping. To overcome the challenge of real-world data
collection, this work leverages a site-specific digital twin (DT) to generate
synthetic channel data closely resembling real-world environments. A model
refinement via transfer learning is proposed to fine-tune the pre-trained model
residing in the DT with minimal real-world data, effectively bridging
mismatches between the digital replica and real-world environments. To reduce
beam training overhead and enhance transparency, the framework uses deep
Shapley additive explanations (SHAP) to rank input features by importance,
prioritizing key spatial directions and minimizing beam sweeping. It also
incorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a
credibility metric for detecting out-of-distribution inputs and ensuring
robust, transparent decision-making. Experimental results show that the
proposed framework reduces real-world data needs by 70%, beam training overhead
by 62%, and improves outlier detection robustness by up to 8.5x, achieving
near-optimal spectral efficiency and transparent decision making compared to
traditional softmax based DL models.

</details>


### [297] [Semi-Supervised Federated Learning via Dual Contrastive Learning and Soft Labeling for Intelligent Fault Diagnosis](https://arxiv.org/abs/2507.14181)
*Yajiao Dai,Jun Li,Zhen Mei,Yiyang Ni,Shi Jin,Zengxiang Li,Sheng Guo,Wei Xiang*

Main category: cs.LG

TL;DR: 本文提出一种半监督联邦学习框架SSFL-DCSL，通过双重对比损失和软标签解决数据和标签稀缺问题，同时保护用户隐私，表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的智能故障诊断方法需要大量标注数据，这些数据分布在不同客户中，获取成本高且数据分布差异影响模型性能。这需要一种能够充分利用未标记数据且保护隐私的解决方案。

Method: 提出的SSFL-DCSL框架结合了双重对比损失和软标签，能利用客户未标记数据并通过样本权重函数和原型共享缓解数据和模型差异；通过局部和全局对比损失来减小模型偏差。

Result: 实验表明，SSFL-DCSL在复杂任务中尤其是仅有10%标记数据的情况下性能优异，相较先进方法提升1.15%-7.85%。

Conclusion: SSFL-DCSL框架有效解决了数据稀缺和隐私保护问题，同时提升了分布式故障诊断的准确性。

Abstract: Intelligent fault diagnosis (IFD) plays a crucial role in ensuring the safe
operation of industrial machinery and improving production efficiency. However,
traditional supervised deep learning methods require a large amount of training
data and labels, which are often located in different clients. Additionally,
the cost of data labeling is high, making labels difficult to acquire.
Meanwhile, differences in data distribution among clients may also hinder the
model's performance. To tackle these challenges, this paper proposes a
semi-supervised federated learning framework, SSFL-DCSL, which integrates dual
contrastive loss and soft labeling to address data and label scarcity for
distributed clients with few labeled samples while safeguarding user privacy.
It enables representation learning using unlabeled data on the client side and
facilitates joint learning among clients through prototypes, thereby achieving
mutual knowledge sharing and preventing local model divergence. Specifically,
first, a sample weighting function based on the Laplace distribution is
designed to alleviate bias caused by low confidence in pseudo labels during the
semi-supervised training process. Second, a dual contrastive loss is introduced
to mitigate model divergence caused by different data distributions, comprising
local contrastive loss and global contrastive loss. Third, local prototypes are
aggregated on the server with weighted averaging and updated with momentum to
share knowledge among clients. To evaluate the proposed SSFL-DCSL framework,
experiments are conducted on two publicly available datasets and a dataset
collected on motors from the factory. In the most challenging task, where only
10\% of the data are labeled, the proposed SSFL-DCSL can improve accuracy by
1.15% to 7.85% over state-of-the-art methods.

</details>


### [298] [From Bias to Behavior: Learning Bull-Bear Market Dynamics with Contrastive Modeling](https://arxiv.org/abs/2507.14182)
*Xiaotong Luo,Shengda Zhuo,Min Chen,Lichun Li,Ruizhao Lu,Wenqi Fan,Shuqiang Huang,Yin Tang*

Main category: cs.LG

TL;DR: 研究通过分析牛市和熊市对金融市场动态预测的影响，提出一种新的模型B4，统一了价格序列和外部信号的分析。


<details>
  <summary>Details</summary>
Motivation: 动机在于探讨投资者情绪（牛市与熊市）对金融市场动态的影响，以及优化这些市场趋势的预测能力。

Method: 提出Bias to Behavior from Bull-Bear Dynamics模型（B4），将价格序列与上下文信号嵌入共享空间，并通过惯性配对模块和双竞争机制模型化偏差与市场行为的动态关系。

Result: 实验表明，B4模型在预测市场趋势上的表现优于现有方法，并且能够解释偏差、投资者行为和市场动态之间的关系。

Conclusion: B4提供了一种有效且解释性强的方式，捕捉金融市场中行为惯性和偏差驱动的动态特性，可用于更精确的市场趋势预测。

Abstract: Financial markets exhibit highly dynamic and complex behaviors shaped by both
historical price trajectories and exogenous narratives, such as news, policy
interpretations, and social media sentiment. The heterogeneity in these data
and the diverse insight of investors introduce biases that complicate the
modeling of market dynamics. Unlike prior work, this paper explores the
potential of bull and bear regimes in investor-driven market dynamics. Through
empirical analysis on real-world financial datasets, we uncover a dynamic
relationship between bias variation and behavioral adaptation, which enhances
trend prediction under evolving market conditions. To model this mechanism, we
propose the Bias to Behavior from Bull-Bear Dynamics model (B4), a unified
framework that jointly embeds temporal price sequences and external contextual
signals into a shared latent space where opposing bull and bear forces
naturally emerge, forming the foundation for bias representation. Within this
space, an inertial pairing module pairs temporally adjacent samples to preserve
momentum, while the dual competition mechanism contrasts bullish and bearish
embeddings to capture behavioral divergence. Together, these components allow
B4 to model bias-driven asymmetry, behavioral inertia, and market
heterogeneity. Experimental results on real-world financial datasets
demonstrate that our model not only achieves superior performance in predicting
market trends but also provides interpretable insights into the interplay of
biases, investor behaviors, and market dynamics.

</details>


### [299] [LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models](https://arxiv.org/abs/2507.14204)
*Dachuan Shi,Yonggan Fu,Xiangchi Yuan,Zhongzhi Yu,Haoran You,Sixu Li,Xin Dong,Jan Kautz,Pavlo Molchanov,Yingyan,Lin*

Main category: cs.LG

TL;DR: 本文提出了LaCache，一种无需训练的方法，通过优化KV缓存解决LLM在长序列推理中的效率瓶颈和内存不足问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的广泛应用需要强大的长距离建模能力，但随着序列长度的增加，KV缓存效率成为瓶颈，并可能导致内存不足的问题。

Method: 提出了LaCache，包括两个关键创新：1. 梯形KV缓存模式，跨层存储KV对，在固定存储预算下提升模型长距离依赖能力；2. 迭代压缩机制，根据token距离动态压缩旧缓存，释放新token所需空间。

Result: 实验表明，LaCache在各类任务和基准测试中，显著增强了不同LLM的长距离建模能力。

Conclusion: LaCache无需训练，为LLM长距离推理提供了高效解决方案，解决了内存受限和性能提升的双重挑战。

Abstract: Recent advancements in Large Language Models (LLMs) have spurred interest in
numerous applications requiring robust long-range capabilities, essential for
processing extensive input contexts and continuously generating extended
outputs. As sequence lengths increase, the number of Key-Value (KV) pairs in
LLMs escalates, creating a significant efficiency bottleneck. In this paper, we
propose a new KV cache optimization paradigm called LaCache, a training-free
method for efficient and accurate generative inference of LLMs. LaCache enables
LLMs to simultaneously address both of the critical challenges in long-range
modeling: robust long-range capabilities and continuous generation without
running out-of-memory (OOM). Specifically, LaCache integrates two key
innovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only
sequentially (left-to-right within each layer) but also across layers (from
shallow to deep), providing an extended span for capturing long-range
dependencies under a fixed storage budget, thereby boosting long-range
capabilities; and (2) an iterative compaction mechanism that progressively
compresses older caches, freeing up space for new tokens within a fixed cache
size. This token distance-based dynamic compression enables more effective
continuous generation under constrained cache budgets. Experiments across
various tasks, benchmarks, and LLM models consistently validate LaCache's
effectiveness in enhancing LLMs' long-range capabilities. Our code is available
at https://github.com/GATECH-EIC/LaCache.

</details>


### [300] [Developing an AI-Guided Assistant Device for the Deaf and Hearing Impaired](https://arxiv.org/abs/2507.14215)
*Jiayu,Liu*

Main category: cs.LG

TL;DR: 研究开发了一种用于听障人士的深度学习辅助设备，能实时定位和识别声源。


<details>
  <summary>Details</summary>
Motivation: 当前针对听障群体的研究存在技术空白，本研究旨在通过机器学习技术填补这一领域的研究空白，为弱势群体提供更好的支持。

Method: 系统包括三个主要组件：(1) 自定义CNN架构JerryNet，用于九个方向的声源定位；(2) 基于CLAP模型的音频分类模型，用于识别声音类别；(3) 融合音频、视觉和文本的多模态整合模型，实现精准的声源定位。硬件包括麦克风阵列、摄像头和信息显示手环。

Result: JerryNet在声源方向识别中的精准度达到91.1%，CLAP模型在数据集上的分类准确率为98.5%和95%，多模态模型的定位精度cIoU为0.892，AUC为0.658，均优于基线模型。

Conclusion: 该研究展示了用于听障人士的新一代辅助设备的潜力，并为未来该领域的发展奠定了基础。

Abstract: This study aims to develop a deep learning system for an accessibility device
for the deaf or hearing impaired. The device will accurately localize and
identify sound sources in real time. This study will fill an important gap in
current research by leveraging machine learning techniques to target the
underprivileged community. The system includes three main components. 1.
JerryNet: A custom designed CNN architecture that determines the direction of
arrival (DoA) for nine possible directions. 2. Audio Classification: This model
is based on fine-tuning the Contrastive Language-Audio Pretraining (CLAP) model
to identify the exact sound classes only based on audio. 3. Multimodal
integration model: This is an accurate sound localization model that combines
audio, visual, and text data to locate the exact sound sources in the images.
The part consists of two modules, one object detection using Yolov9 to generate
all the bounding boxes of the objects, and an audio visual localization model
to identify the optimal bounding box using complete Intersection over Union
(CIoU). The hardware consists of a four-microphone rectangular formation and a
camera mounted on glasses with a wristband for displaying necessary information
like direction. On a custom collected data set, JerryNet achieved a precision
of 91. 1% for the sound direction, outperforming all the baseline models. The
CLAP model achieved 98.5% and 95% accuracy on custom and AudioSet datasets,
respectively. The audio-visual localization model within component 3 yielded a
cIoU of 0.892 and an AUC of 0.658, surpassing other similar models. There are
many future potentials to this study, paving the way to creating a new
generation of accessibility devices.

</details>


### [301] [Geometry-Aware Active Learning of Pattern Rankings via Choquet-Based Aggregation](https://arxiv.org/abs/2507.14217)
*Tudor Matei Opran,Samir Loudni*

Main category: cs.LG

TL;DR: 通过将非线性效用聚合与几何感知查询选择相结合，该方法有效解决了模式挖掘中的模式爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 解决模式挖掘中的模式爆炸问题，提高用户偏好建模与查询效率。

Method: 采用一种交互式学习框架，运用Choquet积分建模用户偏好，并结合版本空间几何结构进行信息查询定位。

Result: 该方法在UCI数据集上的实验表明，与现有方法ChoquetRank相比，它以更少的用户交互实现更高的排名精度。

Conclusion: 该方法有效提高了模式挖掘过程的效率与准确性，展示了在模式排名问题中的应用潜力。

Abstract: We address the pattern explosion problem in pattern mining by proposing an
interactive learning framework that combines nonlinear utility aggregation with
geometry-aware query selection. Our method models user preferences through a
Choquet integral over multiple interestingness measures and exploits the
geometric structure of the version space to guide the selection of informative
comparisons. A branch-and-bound strategy with tight distance bounds enables
efficient identification of queries near the decision boundary. Experiments on
UCI datasets show that our approach outperforms existing methods such as
ChoquetRank, achieving better ranking accuracy with fewer user interactions.

</details>


### [302] [Artificial Intelligence for Green Hydrogen Yield Prediction and Site Suitability using SHAP-Based Composite Index: Focus on Oman](https://arxiv.org/abs/2507.14219)
*Obumneme Zimuzor Nwafor,Mohammed Abdul Majeed Al Hooti*

Main category: cs.LG

TL;DR: 该研究提出了一种基于人工智能的框架，用于计算绿色氢气产量和选址适宜性指数，精度高达98%。


<details>
  <summary>Details</summary>
Motivation: 为了促进化石燃料的可持续替代，特别是在太阳能资源丰富的干旱地区，需要一种有效方法来确定氢气生产的最优地点。

Method: 研究采用了一个多阶段的人工智能管道框架，包括无监督多变量聚类、监督机器学习分类器和SHAP算法，训练气象、地形及时间数据集。

Result: 结果显示模型预测精度为98%，靠近水源，地势高度和季节性变化是影响绿色氢选址的最重要因素。

Conclusion: 研究为绿色氢气规划提供了一个可复制和扩展的工具，尤其适用于数据稀缺地区，有助于相关决策。

Abstract: As nations seek sustainable alternatives to fossil fuels, green hydrogen has
emerged as a promising strategic pathway toward decarbonisation, particularly
in solar-rich arid regions. However, identifying optimal locations for hydrogen
production requires the integration of complex environmental, atmospheric, and
infrastructural factors, often compounded by limited availability of direct
hydrogen yield data. This study presents a novel Artificial Intelligence (AI)
framework for computing green hydrogen yield and site suitability index using
mean absolute SHAP (SHapley Additive exPlanations) values. This framework
consists of a multi-stage pipeline of unsupervised multi-variable clustering,
supervised machine learning classifier and SHAP algorithm. The pipeline trains
on an integrated meteorological, topographic and temporal dataset and the
results revealed distinct spatial patterns of suitability and relative
influence of the variables. With model predictive accuracy of 98%, the result
also showed that water proximity, elevation and seasonal variation are the most
influential factors determining green hydrogen site suitability in Oman with
mean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively.
Given limited or absence of ground-truth yield data in many countries that have
green hydrogen prospects and ambitions, this study offers an objective and
reproducible alternative to subjective expert weightings, thus allowing the
data to speak for itself and potentially discover novel latent groupings
without pre-imposed assumptions. This study offers industry stakeholders and
policymakers a replicable and scalable tool for green hydrogen infrastructure
planning and other decision making in data-scarce regions.

</details>


### [303] [Domain Generalization via Pareto Optimal Gradient Matching](https://arxiv.org/abs/2507.14227)
*Khoi Do,Duong Nguyen,Nam-Khanh Le,Quoc-Viet Pham,Binh-Son Hua,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 该研究提出了POGM方法，通过最大化梯度内积并限制梯度偏离，实现跨域梯度一致性，实验表明该方法在保证计算效率的同时，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有梯度域泛化方法中梯度波动和计算成本过高的问题。

Method: 提出了POGM方法，通过收集梯度轨迹，使用元学习器独立训练，元更新时最大化梯度内积，同时限制学得梯度偏离经验风险梯度轨迹的距离。

Result: POGM在DomainBed数据集上的实验结果显示出比其他基线方法更好的性能，同时保持了计算效率。

Conclusion: POGM方法在不牺牲计算效率的情况下，成功实现了跨域梯度一致性学习，表现优于现有技术方案。

Abstract: In this study, we address the gradient-based domain generalization problem,
where predictors aim for consistent gradient directions across different
domains. Existing methods have two main challenges. First, minimization of
gradient empirical distance or gradient inner products (GIP) leads to gradient
fluctuations among domains, thereby hindering straightforward learning. Second,
the direct application of gradient learning to the joint loss function can
incur high computation overheads due to second-order derivative approximation.
To tackle these challenges, we propose a new Pareto Optimality Gradient
Matching (POGM) method. In contrast to existing methods that add gradient
matching as regularization, we leverage gradient trajectories as collected data
and apply independent training at the meta-learner. In the meta-update, we
maximize GIP while limiting the learned gradient from deviating too far from
the empirical risk minimization gradient trajectory. By doing so, the aggregate
gradient can incorporate knowledge from all domains without suffering gradient
fluctuation towards any particular domain. Experimental evaluations on datasets
from DomainBed demonstrate competitive results yielded by POGM against other
baselines while achieving computational efficiency.

</details>


### [304] [A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions](https://arxiv.org/abs/2507.14245)
*Hengjie Yu,Kenneth A. Dawson,Haiyun Yang,Shuya Liu,Yan Yan,Yaochu Jin*

Main category: cs.LG

TL;DR: 本研究提出了NanoPro-3M，这是目前最大规模的纳米材料-蛋白质交互数据库，并利用此建立NanoProFormer模型，用于预测纳米材料-蛋白质亲和性。


<details>
  <summary>Details</summary>
Motivation: 随着纳米材料在医学和环保中的潜能待挖掘，理解其与蛋白质的交互至关重要，现有的研究因数据集有限及模型普适性差受到制约。

Method: 研究收集了3.2百万个样本与37,000种蛋白质构成的NanoPro-3M数据库，提出基于多模态表示学习的NanoProFormer模型，可进行泛化预测并处理缺失特征和未见样本。

Result: 实验表明多模态建模优于单一模态方法，模型能捕捉生物冠层形成的关键因素，并能通过零样本推断和微调完成多种下游任务。

Conclusion: 这项研究奠定了高性能泛化预测纳米材料-蛋白质交互的基础，减少了实验依赖，加速了体外研究。

Abstract: Unlocking the potential of nanomaterials in medicine and environmental
science hinges on understanding their interactions with proteins, a complex
decision space where AI is poised to make a transformative impact. However,
progress has been hindered by limited datasets and the restricted
generalizability of existing models. Here, we propose NanoPro-3M, the largest
nanomaterial-protein interaction dataset to date, comprising over 3.2 million
samples and 37,000 unique proteins. Leveraging this, we present NanoProFormer,
a foundational model that predicts nanomaterial-protein affinities through
multimodal representation learning, demonstrating strong generalization,
handling missing features, and unseen nanomaterials or proteins. We show that
multimodal modeling significantly outperforms single-modality approaches and
identifies key determinants of corona formation. Furthermore, we demonstrate
its applicability to a range of downstream tasks through zero-shot inference
and fine-tuning. Together, this work establishes a solid foundation for
high-performance and generalized prediction of nanomaterial-protein interaction
endpoints, reducing experimental reliance and accelerating various in vitro
applications.

</details>


### [305] [Linearized Diffusion Map](https://arxiv.org/abs/2507.14257)
*Julio Candanedo*

Main category: cs.LG

TL;DR: 提出了一种新型线性降维方法——线性化扩散图（LDM），结合了扩散算法的几何直觉与线性嵌入方法的简单性和高效性。


<details>
  <summary>Details</summary>
Motivation: 探索将扩散映射的几何优点和线性降维方法的可解释性与计算效率结合起来的可能性。

Method: 通过线性近似扩散-map kernel构造的线性降维方法LDM，结合了线性和非线性方法的优点，同时分析其在不同数据集上的表现。

Result: LDM较PCA在具有显著流形结构的高维数据集上表现更佳；核矩阵完全正性允许直接应用非负矩阵分解以发现潜在结构。

Conclusion: LDM提供了一种有效的新线性降维技术，在理论与实际应用上具有潜力。

Abstract: We introduce the Linearized Diffusion Map (LDM), a novel linear
dimensionality reduction method constructed via a linear approximation of the
diffusion-map kernel. LDM integrates the geometric intuition of diffusion-based
nonlinear methods with the computational simplicity, efficiency, and
interpretability inherent in linear embeddings such as PCA and classical MDS.
Through comprehensive experiments on synthetic datasets (Swiss roll and
hyperspheres) and real-world benchmarks (MNIST and COIL-20), we illustrate that
LDM captures distinct geometric features of datasets compared to PCA, offering
complementary advantages. Specifically, LDM embeddings outperform PCA in
datasets exhibiting explicit manifold structures, particularly in
high-dimensional regimes, whereas PCA remains preferable in scenarios dominated
by variance or noise. Furthermore, the complete positivity of LDM's kernel
matrix allows direct applicability of Non-negative Matrix Factorization (NMF),
suggesting opportunities for interpretable latent-structure discovery. Our
analysis positions LDM as a valuable new linear dimensionality reduction
technique with promising theoretical and practical extensions.

</details>


### [306] [A Simple "Try Again" Can Elicit Multi-Turn LLM Reasoning](https://arxiv.org/abs/2507.14295)
*Licheng Liu,Zihan Wang,Linjie Li,Chenwei Xu,Yiping Lu,Han Liu,Avirup Sil,Manling Li*

Main category: cs.LG

TL;DR: 多轮问题解决对提升大规模推理模型（LRMs）的能力至关重要。然而，现有强化学习方法主要基于单轮训练，而无法应对多轮推理与上下文反馈修正。本研究提出通过最简化的单义反馈（Unary Feedback）进行多轮强化学习，实验表明改进了多轮推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习训练方法无法有效促进LRMs在多轮问题解决中的反馈反思能力，导致模型重复输出结果，不过适应更复杂的多轮推理场景仍然关键。

Method: 通过引入基于最小单义用户反馈的Unary Feedback as Observation（UFO）训练方法，在多轮强化学习路径中改进模型训练过程。同时优化设计奖励机制以减少修正步骤并增强回答多样性。

Result: 实验表明，该方法在保持单轮推理性能的前提下，将多轮推理准确率提高了最高14%，显著提升模型处理多轮问题及适应反馈的能力。

Conclusion: 通过UFO方法可以有效简化多轮推理中的强化学习配置，既提升了多轮推理能力，又能促进更具反思性的回答生成，在实际场景中有极大的优化潜力。

Abstract: Multi-turn problem solving is critical yet challenging for Large Reasoning
Models (LRMs) to reflect on their reasoning and revise from feedback. Existing
Reinforcement Learning (RL) methods train large reasoning models on a
single-turn paradigm with verifiable rewards. However, we observe that models
trained with existing RL paradigms often lose their ability to solve problems
across multiple turns and struggle to revise answers based on contextual
feedback, leading to repetitive responses. We ask: can LRMs learn to reflect
their answers in a multi-turn context? In this work, we find that training
models with multi-turn RL using only unary feedback (e.g., "Let's try again")
after wrong answers can improve both single-turn performance and multi-turn
reasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement
learning, which uses minimal yet common unary user feedback during iterative
problem solving. It can be easily applied to existing single-turn RL training
setups. Experimental results show that RL training with UFO keeps single-turn
performance and improves multi-turn reasoning accuracy by up to 14%, enabling
language models to better react to feedback in multi-turn problem solving. To
further minimize the number of turns needed for a correct answer while
encouraging diverse reasoning when mistakes occur, we design reward structures
that guide models to produce careful and deliberate answers in each turn. Code:
https://github.com/lichengliu03/unary-feedback

</details>


### [307] [FedStrategist: A Meta-Learning Framework for Adaptive and Robust Aggregation in Federated Learning](https://arxiv.org/abs/2507.14322)
*Md Rafid Haque,Abu Raihan Mostofa Kamal,Md. Azam Hossain*

Main category: cs.LG

TL;DR: 该论文提出FedStrategist，一种基于元学习的框架，通过动态选择聚合规则应对联邦学习中的模型投毒攻击。


<details>
  <summary>Details</summary>
Motivation: 传统防御方法对上下文依赖性强，在适应性攻击者或异构数据环境中表现不佳，需要新的方法提高模型鲁棒性。

Method: 设计了一种轻量的上下文摇臂策略代理，基于实时诊断指标动态选择最佳聚合规则，并通过元学习优化策略。

Result: 实验表明单一静态规则不适用所有场景，而FedStrategist能够在不同环境中学习到更优的防御策略，如对抗“隐形”攻击者和“非鲁棒基线”环境。

Conclusion: 该框架提供了一种切实可行、可分析的方式构建鲁棒且智能的去中心化AI系统，并支持通过风险容忍度参数控制安全性与性能的平衡。

Abstract: Federated Learning (FL) offers a paradigm for privacy-preserving
collaborative AI, but its decentralized nature creates significant
vulnerabilities to model poisoning attacks. While numerous static defenses
exist, their effectiveness is highly context-dependent, often failing against
adaptive adversaries or in heterogeneous data environments. This paper
introduces FedStrategist, a novel meta-learning framework that reframes robust
aggregation as a real-time, cost-aware control problem. We design a lightweight
contextual bandit agent that dynamically selects the optimal aggregation rule
from an arsenal of defenses based on real-time diagnostic metrics. Through
comprehensive experiments, we demonstrate that no single static rule is
universally optimal. We show that our adaptive agent successfully learns
superior policies across diverse scenarios, including a ``Krum-favorable"
environment and against a sophisticated "stealth" adversary designed to
neutralize specific diagnostic signals. Critically, we analyze the paradoxical
scenario where a non-robust baseline achieves high but compromised accuracy,
and demonstrate that our agent learns a conservative policy to prioritize model
integrity. Furthermore, we prove the agent's policy is controllable via a
single "risk tolerance" parameter, allowing practitioners to explicitly manage
the trade-off between performance and security. Our work provides a new,
practical, and analyzable approach to creating resilient and intelligent
decentralized AI systems.

</details>


### [308] [Rethinking Individual Fairness in Deepfake Detection](https://arxiv.org/abs/2507.14326)
*Aryana Hou,Li Lin,Justin Li,Shu Hu*

Main category: cs.LG

TL;DR: 本研究提出解决深度伪造检测领域个体公平性问题的框架，同时提高了公平性和检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的深度伪造检测工具在公平性上存在不足，尤其对于个体公平性问题未得到重视，这可能导致对特定人群存在偏见性结果。

Method: 提出首个通用化框架，可嵌入现有深度伪造检测器中以改善个体公平性并提升泛化能力。

Result: 通过在主流深度伪造数据集上的实验，证明该框架显著提升了个体公平性，同时维持了稳健的检测性能，优于最新的先进方法。

Conclusion: 本研究填补了深度伪造检测领域中个体公平性问题的研究空白，为提高深度伪造检测性能和公平性提供了新思路。

Abstract: Generative AI models have substantially improved the realism of synthetic
media, yet their misuse through sophisticated DeepFakes poses significant
risks. Despite recent advances in deepfake detection, fairness remains
inadequately addressed, enabling deepfake markers to exploit biases against
specific populations. While previous studies have emphasized group-level
fairness, individual fairness (i.e., ensuring similar predictions for similar
individuals) remains largely unexplored. In this work, we identify for the
first time that the original principle of individual fairness fundamentally
fails in the context of deepfake detection, revealing a critical gap previously
unexplored in the literature. To mitigate it, we propose the first
generalizable framework that can be integrated into existing deepfake detectors
to enhance individual fairness and generalization. Extensive experiments
conducted on leading deepfake datasets demonstrate that our approach
significantly improves individual fairness while maintaining robust detection
performance, outperforming state-of-the-art methods. The code is available at
https://github.com/Purdue-M2/Individual-Fairness-Deepfake-Detection.

</details>


### [309] [Development and Deployment of Hybrid ML Models for Critical Heat Flux Prediction in Annulus Geometries](https://arxiv.org/abs/2507.14332)
*Aidan Furlong,Xingang Zhao,Robert Salko,Xu Wu*

Main category: cs.LG

TL;DR: 提出了一种结合机理模型与机器学习的混合模型方法来预测环状几何中的临界热流（CHF），精度显著优于传统经验模型。


<details>
  <summary>Details</summary>
Motivation: 传统经验模型在预测CHF时存在较大误差，而全数据驱动方法缺乏可解释性和鲁棒性，尤其在数据稀缺的情况下，这促使研究者开发基于机器学习的混合模型以提高预测性能。

Method: 利用CTF子通道代码对四种ML模型进行了开发、部署和验证，并以Biasi、Bowring和Katto经验相关模型作为基准，通过来自四个数据集的577个环形实验数据进行训练和测试。

Result: 基准经验相关模型的平均相对误差超过26%，而混合ML模型的平均相对误差降低到3.5%以下，仅少数点超出10%误差范围，显著优于经验模型。

Conclusion: 混合ML模型在预测环状几何中的CHF方面表现出显著优势，为高精度热工水力学计算提供了强有力的工具。

Abstract: Accurate prediction of critical heat flux (CHF) is an essential component of
safety analysis in pressurized and boiling water reactors. To support reliable
prediction of this quantity, several empirical correlations and lookup tables
have been constructed from physical experiments over the past several decades.
With the onset of accessible machine learning (ML) frameworks, multiple
initiatives have been established with the goal of predicting CHF more
accurately than these traditional methods. While purely data-driven surrogate
modeling has been extensively investigated, these approaches lack
interpretability, lack resilience to data scarcity, and have been developed
mostly using data from tube experiments. As a result, bias-correction hybrid
approaches have become increasingly popular, which correct initial
"low-fidelity" estimates provided by deterministic base models by using
ML-predicted residuals. This body of work has mostly considered round tube
geometries; annular geometry-specific ML models have not yet been deployed in
thermal hydraulic codes. This study developed, deployed, and validated four ML
models to predict CHF in annular geometries using the CTF subchannel code.
Three empirical correlation models, Biasi, Bowring, and Katto, were used as
base models for comparison. The ML models were trained and tested using 577
experimental annulus data points from four datasets: Becker, Beus, Janssen, and
Mortimore. Baseline CHF predictions were obtained from the empirical
correlations, with mean relative errors above 26%. The ML-driven models
achieved mean relative errors below 3.5%, with no more than one point exceeding
the 10% error envelope. In all cases, the hybrid ML models significantly
outperformed their empirical counterparts.

</details>


### [310] [Influence Functions for Preference Dataset Pruning](https://arxiv.org/abs/2507.14344)
*Daniel Fein,Gabriela Aranguiz-Dias*

Main category: cs.LG

TL;DR: 该论文探讨了使用影响函数（influence functions）方法检测和筛选语言模型微调训练数据集中的有害示例，同时实验了其在增强微调效果方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 语言模型微调通常需要利用小而可能有噪声的数据集，提高模型生成性能，研究高效剔除有害训练样本的方法有实际意义。

Method: 通过适应TL;DR数据集进行奖励模型训练，运用共轭梯度近似影响函数的方法，筛选有害的训练示例并对数据集进行优化。

Result: 剔除10%的训练样本后，重新训练模型的准确性提高了1.5%。此外，研究还显示梯度相似性在识别有帮助的训练示例时表现优于影响函数。

Conclusion: 影响函数在检测有害训练示例中的表现优异，但梯度相似性在筛选有帮助的示例时更具优势。局部曲率对甄别有害示例更重要。

Abstract: Language models are commonly fine-tuned via reinforcement learning to alter
their behavior or elicit new capabilities. Datasets used for these purposes,
and particularly human preference datasets, are often noisy. The relatively
small size post-training datasets, combined with parameter-efficient
fine-tuning methods, enable the use of influence functions approximations to
detect and prune training examples that are harmful to performance on a
validation set. In this work, we adapt the TL;DR dataset for reward model
training to demonstrate how conjugate-gradient approximated influence functions
can be used to filter datasets. In our experiments, influence function
filtering yields a small retraining accuracy uplift of 1.5% after removing 10%
of training examples. We also show that gradient similarity outperforms
influence functions for detecting helpful training examples. This suggests that
local curvature is important for detecting harmful training examples, but less
so for identifying helpful examples.

</details>


### [311] [Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers](https://arxiv.org/abs/2507.14353)
*Harsh Nilesh Pathak,Randy Paffenroth*

Main category: cs.LG

TL;DR: 提出了一种名为Solo Connection的新的参数高效微调方法，相较于LoRA表现更优且参数需求更低，同时引入了基于同伦理论的训练方法。


<details>
  <summary>Details</summary>
Motivation: 针对现有PEFT方法限制（如LoRA在调整权重矩阵时的效率问题），提出更高效的微调方法，特别适用于深层大型语言模型。

Method: 通过在解码器块级别进行任务特定表示的调整，而非仅仅调整权重矩阵。同时引入了一种基于同伦理论的线性变换，支持平滑和稳定的模型适配。

Result: Solo Connection在自然语言生成基准测试中超过了LoRA，并且与LoRA相比，减少了59%的可训练参数，相较于GPT2的完全微调减少了超过99%的参数。

Conclusion: Solo Connection不仅在性能上优于现有方法，还能大幅减少训练参数量，尤其适用于具有多个解码器块的大型语言模型。

Abstract: Parameter efficient fine tuning (PEFT) is a versatile and extensible approach
for adapting a Large Language Model (LLM) for newer tasks. One of the most
prominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on
adjusting the attention weight matrices within individual decoder blocks of a
Generative Pre trained Transformer (GPT2). In contrast, we introduce Solo
Connection a novel method that adapts the representation at the decoder-block
level rather than modifying individual weight matrices. Not only does Solo
Connection outperform LoRA on E2E natural language generation benchmarks, but
it also reduces the number of trainable parameters by 59% relative to LoRA and
by more than 99% compared to full fine-tuning of GPT2, an early version of
Large Language Models (LLMs). Solo Connection is also motivated by homotopy
theory: we introduce a trainable linear transformation that gradually
interpolates between a zero vector and the task-specific representation,
enabling smooth and stable adaptation over time. While skip connections in the
original 12 layer GPT2 are typically confined to individual decoder blocks,
subsequent GPT2 variants scale up to 48 layers, and even larger language models
can include 128 or more decoder blocks. These expanded architectures underscore
the need to revisit how skip connections are employed during fine-tuning. This
paper focuses on long skip connections that link outputs of different decoder
blocks, potentially enhancing the model's ability to adapt to new tasks while
leveraging pre-trained knowledge.

</details>


### [312] [Incremental Causal Graph Learning for Online Cyberattack Detection in Cyber-Physical Infrastructures](https://arxiv.org/abs/2507.14387)
*Arun Vignesh Malarkkan,Dongjie Wang,Haoyue Bai,Yanjie Fu*

Main category: cs.LG

TL;DR: 提出了INCADET，一个用于实时网络攻击检测的增量因果图学习框架，由三个模块组成，实验表明其在动态攻击场景中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 应对传统实时异常检测方法在高数据方差和类别不平衡导致的高误报率，以及现有离线因果图方法无法适应实时环境的局限性。

Method: 提出了一种名为INCADET的框架，实时更新因果图，包含早期症状检测、增量因果图学习及因果图分类三个模块。

Result: 通过在现实关键基础设施数据集上的实验，证明INCADET在动态攻击场景中的准确性、鲁棒性和适应性优于传统离线因果图方法和深度时间基线方法。

Conclusion: INCADET克服了传统因果图方法无法适应动态数据分布和实时系统中缺乏监督的问题，可作为实时网络攻击检测的有效手段。

Abstract: The escalating threat of cyberattacks on real-time critical infrastructures
poses serious risks to public safety, demanding detection methods that
effectively capture complex system interdependencies and adapt to evolving
attack patterns. Traditional real-time anomaly detection techniques often
suffer from excessive false positives due to their statistical sensitivity to
high data variance and class imbalance. To address these limitations, recent
research has explored modeling causal relationships among system components.
However, prior work mainly focuses on offline causal graph-based approaches
that require static historical data and fail to generalize to real-time
settings. These methods are fundamentally constrained by: (1) their inability
to adapt to dynamic shifts in data distribution without retraining, and (2) the
risk of catastrophic forgetting when lacking timely supervision in live
systems. To overcome these challenges, we propose INCADET, a novel framework
for incremental causal graph learning tailored to real-time cyberattack
detection. INCADET dynamically captures evolving system behavior by
incrementally updating causal graphs across streaming time windows. The
framework comprises three modules: 1) Early Symptom Detection: Detects
transitions in system status using divergence in edge-weight distributions
across sequential causal graphs. 2) Incremental Causal Graph Learning:
Leverages experience replay and edge reinforcement to continually refine causal
structures while preserving prior knowledge. 3) Causal Graph Classification:
Employs Graph Convolutional Networks (GCNs) to classify system status using the
learned causal graphs. Extensive experiments on real-world critical
infrastructure datasets demonstrate that INCADET achieves superior accuracy,
robustness, and adaptability compared to both static causal and deep temporal
baselines in evolving attack scenarios.

</details>


### [313] [It's Not That Simple. An Analysis of Simple Test-Time Scaling](https://arxiv.org/abs/2507.14419)
*Guojun Wu*

Main category: cs.LG

TL;DR: 本论文分析了简单测试时刻缩放方法，发现主要原因在于通过限制最大长度实现的缩放，而非通过“Wait”扩展或长CoT数据微调。


<details>
  <summary>Details</summary>
Motivation: 探讨如何利用简单的测试时刻缩放方法来复制o1类模型的缩放行为，并揭示其中的局限性和潜力。

Method: 分析了限制最大长度缩放和通过追加“Wait”进行扩展缩放的表现，并比较其与训练数据和模型优化之间的关系。

Result: 结果显示，缩放行为主要来自限制最大长度，而不在于其他微调方式；且通过追加“Wait”进行扩展会出现不一致现象。

Conclusion: 简单测试时刻缩放在复制缩放行为上有效，然而对探索提高性能的潜力有限，应注重解锁更高性能目标而非仅复制外观行为。

Abstract: Prior work proposed simple test-time scaling, a method for replicating this
scaling behavior with models distilled from o1-like models by manually
controlling test-time compute: either scaling down by enforcing a maximum
length or scaling up by iteratively appending "Wait" when the model is about to
terminate its generation. This paper presents an analysis of simple test-time
scaling and finds that the scaling behavior is largely attributed to scaling
down by enforcing a maximum length. In contrast, fine-tuning on long CoT data
distilled from o1-like models has no significant impact on scaling behavior,
and scaling up by appending "Wait" leads to inconsistencies, as the model may
oscillate between solutions. A key distinction exists between scaling down by
enforcing a maximum length and scaling up test-time compute in o1-like models,
such as DeepSeek-R1\@. These models are typically allowed to utilize as much
compute as needed, with the only constraint being the model's maximum supported
length. By learning to naturally scale up test-time compute during
reinforcement learning, o1-like models surpass their peak performance when
scaling up. In contrast, simple test-time scaling progressively imposes a lower
upper limit on model performance as it scales down. While replicating the
test-time scaling behavior of o1 models can be straightforward by scaling down,
it is crucial to recognize that the goal of scaling test-time compute is to
unlock higher performance -- beyond what the model could originally achieve --
rather than merely reproducing the appearance of scaling behavior.

</details>


### [314] [Deep RL Dual Sourcing Inventory Management with Supply and Capacity Risk Awareness](https://arxiv.org/abs/2507.14446)
*Feng Liu,Ying Liu,Carson Eisenach*

Main category: cs.LG

TL;DR: 本文提出了一种通过使用预训练深度学习模型模拟和组合随机过程，从而高效应用强化学习解决大规模随机优化问题的方法。


<details>
  <summary>Details</summary>
Motivation: 旨在应对大规模随机优化问题中的复杂性，尤其是供应链优化中的多重约束问题。

Method: 通过分解问题，使用深度学习模块来模拟供应链过程，同时引入约束协调机制预测交叉产品约束下的对偶成本。

Result: 研究表明相比直接建模物理复杂约束，提出的方法在处理大规模实际数据集时性能显著提升。

Conclusion: 证明了分解并利用深度学习模块的方法的实际有效性，同时指出未来研究改进此类模型的开放问题。

Abstract: In this work, we study how to efficiently apply reinforcement learning (RL)
for solving large-scale stochastic optimization problems by leveraging
intervention models. The key of the proposed methodology is to better explore
the solution space by simulating and composing the stochastic processes using
pre-trained deep learning (DL) models. We demonstrate our approach on a
challenging real-world application, the multi-sourcing multi-period inventory
management problem in supply chain optimization. In particular, we employ deep
RL models for learning and forecasting the stochastic supply chain processes
under a range of assumptions. Moreover, we also introduce a constraint
coordination mechanism, designed to forecast dual costs given the
cross-products constraints in the inventory network. We highlight that instead
of directly modeling the complex physical constraints into the RL optimization
problem and solving the stochastic problem as a whole, our approach breaks down
those supply chain processes into scalable and composable DL modules, leading
to improved performance on large real-world datasets. We also outline open
problems for future research to further investigate the efficacy of such
models.

</details>


### [315] [ReDiSC: A Reparameterized Masked Diffusion Model for Scalable Node Classification with Structured Predictions](https://arxiv.org/abs/2507.14484)
*Yule Li,Yifeng Lu,Zhen Wang,Zhewei Wei,Yaliang Li,Bolin Ding*

Main category: cs.LG

TL;DR: ReDiSC通过重新参数化的掩码扩散模型和变分期望最大化框架，提高了结构化节点分类任务的效率和效果，尤其是在大规模数据集上表现出显著优势。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络在节点分类中假设节点标签条件独立性，但这与标签在图上依然存在相关性的直觉观察不符。

Method: 提出ReDiSC模型，该模型采用重新参数化的掩码扩散机制，并通过变分期望最大化框架估计节点标签的联合分布。

Result: ReDiSC显著优于现有基于标签传播和扩散的模型，并能有效处理先前方法在大规模数据集上的计算约束问题。

Conclusion: ReDiSC为结构化节点分类任务提供了高效且具有实际适用性的解决方案，在不同类型图结构上均表现出优越性能。

Abstract: In recent years, graph neural networks (GNN) have achieved unprecedented
successes in node classification tasks. Although GNNs inherently encode
specific inductive biases (e.g., acting as low-pass or high-pass filters), most
existing methods implicitly assume conditional independence among node labels
in their optimization objectives. While this assumption is suitable for
traditional classification tasks such as image recognition, it contradicts the
intuitive observation that node labels in graphs remain correlated, even after
conditioning on the graph structure. To make structured predictions for node
labels, we propose ReDiSC, namely, Reparameterized masked Diffusion model for
Structured node Classification. ReDiSC estimates the joint distribution of node
labels using a reparameterized masked diffusion model, which is learned through
the variational expectation-maximization (EM) framework. Our theoretical
analysis shows the efficiency advantage of ReDiSC in the E-step compared to
DPM-SNC, a state-of-the-art model that relies on a manifold-constrained
diffusion model in continuous domain. Meanwhile, we explicitly link ReDiSC's
M-step objective to popular GNN and label propagation hybrid approaches.
Extensive experiments demonstrate that ReDiSC achieves superior or highly
competitive performance compared to state-of-the-art GNN, label propagation,
and diffusion-based baselines across both homophilic and heterophilic graphs of
varying sizes. Notably, ReDiSC scales effectively to large-scale datasets on
which previous structured diffusion methods fail due to computational
constraints, highlighting its significant practical advantage in structured
node classification tasks.

</details>


### [316] [Federated Reinforcement Learning in Heterogeneous Environments](https://arxiv.org/abs/2507.14487)
*Ukjo Hwang,Songnam Hong*

Main category: cs.LG

TL;DR: 提出了适用于统计异质性环境的联邦强化学习框架FRL-EH，并通过实验验证其优越性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决各地环境统计异质性带来的挑战，同时在尊重隐私的前提下协作学习全球策略。

Method: 设计了新的全局目标函数，提出了FedRQ算法，并通过理论证明其收敛性，同时扩展到连续状态空间环境中。

Result: 实验结果表明，所提算法在异质性环境中表现优越，超过现有最先进FRL算法。

Conclusion: 该方法有效解决了异质环境中的联邦强化学习问题，并实现了表现更为鲁棒的全球策略学习。

Abstract: We investigate a Federated Reinforcement Learning with Environment
Heterogeneity (FRL-EH) framework, where local environments exhibit statistical
heterogeneity. Within this framework, agents collaboratively learn a global
policy by aggregating their collective experiences while preserving the privacy
of their local trajectories. To better reflect real-world scenarios, we
introduce a robust FRL-EH framework by presenting a novel global objective
function. This function is specifically designed to optimize a global policy
that ensures robust performance across heterogeneous local environments and
their plausible perturbations. We propose a tabular FRL algorithm named FedRQ
and theoretically prove its asymptotic convergence to an optimal policy for the
global objective function. Furthermore, we extend FedRQ to environments with
continuous state space through the use of expectile loss, addressing the key
challenge of minimizing a value function over a continuous subset of the state
space. This advancement facilitates the seamless integration of the principles
of FedRQ with various Deep Neural Network (DNN)-based RL algorithms. Extensive
empirical evaluations validate the effectiveness and robustness of our FRL
algorithms across diverse heterogeneous environments, consistently achieving
superior performance over the existing state-of-the-art FRL algorithms.

</details>


### [317] [Glitches in Decision Tree Ensemble Models](https://arxiv.org/abs/2507.14492)
*Satyankar Chandra,Ashutosh Gupta,Kaushik Mallik,Krishna Shankaranarayanan,Namrita Varshney*

Main category: cs.LG

TL;DR: 本文研究了机器学习模型的不可靠行为——glitches，并提出算法检测GBDT模型中的glitches。


<details>
  <summary>Details</summary>
Motivation: 在关键任务中，机器学习模型需要可靠性，但存在决策边界陡峭的模型可能导致输出不一致的问题。作者研究了一个新的不可靠性来源glitches。

Method: 作者提出glitches的正式定义，并通过理论和实验展示了其普遍存在性。同时，针对梯度提升决策树（GBDT），提出了基于MILP的算法进行glitches检测。

Result: 研究证明在深度4的树模型中检测glitches的问题是NP完全的，并通过广泛使用的GBDT基准数据集验证算法的有效性和计算可行性。

Conclusion: 发现了机器学习模型的潜在问题点，并开发了一个算法工具以提升模型的可靠性研究。

Abstract: Many critical decision-making tasks are now delegated to machine-learned
models, and it is imperative that their decisions are trustworthy and reliable,
and their outputs are consistent across similar inputs. We identify a new
source of unreliable behaviors-called glitches-which may significantly impair
the reliability of AI models having steep decision boundaries. Roughly
speaking, glitches are small neighborhoods in the input space where the model's
output abruptly oscillates with respect to small changes in the input. We
provide a formal definition of glitches, and use well-known models and datasets
from the literature to demonstrate that they have widespread existence and
argue they usually indicate potential model inconsistencies in the neighborhood
of where they are found. We proceed to the algorithmic search of glitches for
widely used gradient-boosted decision tree (GBDT) models. We prove that the
problem of detecting glitches is NP-complete for tree ensembles, already for
trees of depth 4. Our glitch-search algorithm for GBDT models uses an MILP
encoding of the problem, and its effectiveness and computational feasibility
are demonstrated on a set of widely used GBDT benchmarks taken from the
literature.

</details>


### [318] [Generative Distribution Distillation](https://arxiv.org/abs/2507.14503)
*Jiequan Cui,Beier Zhu,Qingshan Xu,Xiaogang Xu,Pengguang Chen,Xiaojuan Qi,Bei Yu,Hanwang Zhang,Richang Hong*

Main category: cs.LG

TL;DR: 该论文提出知识蒸馏作为条件生成问题，提出了一种名为GenDD的框架，并在实验中表现卓越。


<details>
  <summary>Details</summary>
Motivation: 探讨知识蒸馏的新方法，以解决高维优化难题及缺乏语义监督的问题。

Method: 提出GenDD框架，同时引入了Split Tokenization策略解决无监督问题，并通过Distribution Contraction将标签监督融入目标中，证明GenDD在梯度层面上作为多任务学习的代理。

Result: 实验表明，GenDD在无监督条件下对比方法有显著提升，并在有监督条件下达到了ImageNet数据集的新SOTA表现。

Conclusion: GenDD框架提供了一种有效的知识蒸馏新方法，在无监督及有监督设置中都具有卓越性能。

Abstract: In this paper, we formulate the knowledge distillation (KD) as a conditional
generative problem and propose the \textit{Generative Distribution Distillation
(GenDD)} framework. A naive \textit{GenDD} baseline encounters two major
challenges: the curse of high-dimensional optimization and the lack of semantic
supervision from labels. To address these issues, we introduce a \textit{Split
Tokenization} strategy, achieving stable and effective unsupervised KD.
Additionally, we develop the \textit{Distribution Contraction} technique to
integrate label supervision into the reconstruction objective. Our theoretical
proof demonstrates that \textit{GenDD} with \textit{Distribution Contraction}
serves as a gradient-level surrogate for multi-task learning, realizing
efficient supervised training without explicit classification loss on
multi-step sampling image representations. To evaluate the effectiveness of our
method, we conduct experiments on balanced, imbalanced, and unlabeled data.
Experimental results show that \textit{GenDD} performs competitively in the
unsupervised setting, significantly surpassing KL baseline by \textbf{16.29\%}
on ImageNet validation set. With label supervision, our ResNet-50 achieves
\textbf{82.28\%} top-1 accuracy on ImageNet in 600 epochs training,
establishing a new state-of-the-art.

</details>


### [319] [SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning](https://arxiv.org/abs/2507.14516)
*Jeyoung Lee,Hochul Kang*

Main category: cs.LG

TL;DR: 提出了一种用于时间序列自监督表示学习的结构感知指标：信号Dice相似性系数(SDSC)，可以更有效地量化信号间的结构一致性并改善表现。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督学习方法多采用基于距离的目标函数（如MSE），但存在对振幅敏感、对波形极性不敏感及尺度不受限等问题，阻碍了语义对齐并降低了解释性。

Method: 提出了SDSC指标，用于量化时间信号间的结构一致性，同时推导了一种基于Heaviside函数的可微分近似以支持梯度优化，并结合MSE提出混合损失公式以保持稳定性和必要的振幅信息。

Result: 通过实验验证，在预测和分类任务中，基于SDSC的预训练在同域和低资源情况下取得了与MSE相当或更好的效果。

Conclusion: 结果表明，信号表示的结构保真度增强了语义表示质量，支持将结构感知指标作为传统基于距离方法的替代方案。

Abstract: We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware
metric function for time series self-supervised representation learning. Most
Self-Supervised Learning (SSL) methods for signals commonly adopt
distance-based objectives such as mean squared error (MSE), which are sensitive
to amplitude, invariant to waveform polarity, and unbounded in scale. These
properties hinder semantic alignment and reduce interpretability. SDSC
addresses this by quantifying structural agreement between temporal signals
based on the intersection of signed amplitudes, derived from the Dice
Similarity Coefficient (DSC).Although SDSC is defined as a structure-aware
metric, it can be used as a loss by subtracting from 1 and applying a
differentiable approximation of the Heaviside function for gradient-based
optimization. A hybrid loss formulation is also proposed to combine SDSC with
MSE, improving stability and preserving amplitude where necessary. Experiments
on forecasting and classification benchmarks demonstrate that SDSC-based
pre-training achieves comparable or improved performance over MSE, particularly
in in-domain and low-resource scenarios. The results suggest that structural
fidelity in signal representations enhances the semantic representation
quality, supporting the consideration of structure-aware metrics as viable
alternatives to conventional distance-based methods.

</details>


### [320] [Positive-Unlabeled Learning for Control Group Construction in Observational Causal Inference](https://arxiv.org/abs/2507.14528)
*Ilias Tsoumas,Dimitrios Bormpoudakis,Vasileios Sitokonstantinou,Athanasios Askitopoulos,Andreas Kalogeras,Charalampos Kontoes,Ioannis Athanasiadis*

Main category: cs.LG

TL;DR: 提出了一种基于正负无标注学习（PU learning）的方法，用于从无标注的数据中识别对照组以展开因果推断。


<details>
  <summary>Details</summary>
Motivation: 因果推断中缺乏显式标记的对照组特别是在观察性研究中是一大挑战，而随机实验又往往代价高昂或难以实现。

Method: 利用正负无标注学习（PU learning）框架，通过已知的处理组数据，从无标注数据集中识别对照组，并基于因果图和模拟生成的合成数据评估方法的有效性，同时应用于实际农业数据进行验证。

Result: PU学习成功识别了无标注数据中的对照组，并估算出的平均处理效应（ATE）接近真实值。

Conclusion: 该方法对因果推断特别是在农业、地球科学等无随机实验条件下的领域具有重要意义，能够启用更多的准实验分析。

Abstract: In causal inference, whether through randomized controlled trials or
observational studies, access to both treated and control units is essential
for estimating the effect of a treatment on an outcome of interest. When
treatment assignment is random, the average treatment effect (ATE) can be
estimated directly by comparing outcomes between groups. In non-randomized
settings, various techniques are employed to adjust for confounding and
approximate the counterfactual scenario to recover an unbiased ATE. A common
challenge, especially in observational studies, is the absence of units clearly
labeled as controls-that is, units known not to have received the treatment. To
address this, we propose positive-unlabeled (PU) learning as a framework for
identifying, with high confidence, control units from a pool of unlabeled ones,
using only the available treated (positive) units. We evaluate this approach
using both simulated and real-world data. We construct a causal graph with
diverse relationships and use it to generate synthetic data under various
scenarios, assessing how reliably the method recovers control groups that allow
estimates of true ATE. We also apply our approach to real-world data on optimal
sowing and fertilizer treatments in sustainable agriculture. Our findings show
that PU learning can successfully identify control (negative) units from
unlabeled data based only on treated units and, through the resulting control
group, estimate an ATE that closely approximates the true value. This work has
important implications for observational causal inference, especially in fields
where randomized experiments are difficult or costly. In domains such as earth,
environmental, and agricultural sciences, it enables a plethora of
quasi-experiments by leveraging available earth observation and climate data,
particularly when treated units are available but control units are lacking.

</details>


### [321] [Kernel Based Maximum Entropy Inverse Reinforcement Learning for Mean-Field Games](https://arxiv.org/abs/2507.14529)
*Berkay Anahtarci,Can Deha Kariksiz,Naci Saldi*

Main category: cs.LG

TL;DR: 该论文研究了用于无限时域平稳平均场博弈的最大因果熵逆强化学习问题，提出了一种基于再生核Hilbert空间的奖励函数估计方法，并通过拉格朗日松弛将问题转化为无约束对数似然最大化，通过梯度上升算法实现优化。


<details>
  <summary>Details</summary>
Motivation: 现有的平均场博弈逆强化学习方法通常限制奖励函数为固定有限基函数的线性组合，难以捕捉复杂和非线性的奖励结构。本研究旨在克服这一局限性，并探讨无限时域情境下的逆强化学习。

Method: 采用再生核Hilbert空间对奖励函数建模，通过拉格朗日松弛将最大因果熵问题转化为无约束对数似然最大化问题，并通过梯度上升算法进行优化。此外，证明了相关软Bellman算子的Fréchet可微性，从而保证了目标函数的光滑性。

Result: 方法在平均场交通路径选择博弈中表现优异，能够准确恢复专家的行为模式。

Conclusion: 提出的方法能够有效处理无限时域平均场博弈中复杂的奖励函数结构，为逆强化学习提供了一个新颖且强大的工具。

Abstract: We consider the maximum causal entropy inverse reinforcement learning problem
for infinite-horizon stationary mean-field games, in which we model the unknown
reward function within a reproducing kernel Hilbert space. This allows the
inference of rich and potentially nonlinear reward structures directly from
expert demonstrations, in contrast to most existing inverse reinforcement
learning approaches for mean-field games that typically restrict the reward
function to a linear combination of a fixed finite set of basis functions. We
also focus on the infinite-horizon cost structure, whereas prior studies
primarily rely on finite-horizon formulations. We introduce a Lagrangian
relaxation to this maximum causal entropy inverse reinforcement learning
problem that enables us to reformulate it as an unconstrained log-likelihood
maximization problem, and obtain a solution \lk{via} a gradient ascent
algorithm. To illustrate the theoretical consistency of the algorithm, we
establish the smoothness of the log-likelihood objective by proving the
Fr\'echet differentiability of the related soft Bellman operators with respect
to the parameters in the reproducing kernel Hilbert space. We demonstrate the
effectiveness of our method on a mean-field traffic routing game, where it
accurately recovers expert behavior.

</details>


### [322] [The Origin of Self-Attention: From Pairwise Affinity Matrices to Transformers](https://arxiv.org/abs/2507.14560)
*Giorgio Roffo*

Main category: cs.LG

TL;DR: 文章研究了自注意力机制的起源及其在不同领域的应用，将其视为基于亲和矩阵的计算方法的特例。


<details>
  <summary>Details</summary>
Motivation: 探索自注意力机制的数学基础及其在不同领域的统一性。

Method: 通过比较自注意力机制和无限特征选择(Inf-FS)在亲和矩阵定义与应用上的异同，揭示其本质联系。

Result: 发现自注意力机制可以视为Inf-FS的一种特例，为其在多领域应用建立了共同的数学框架。

Conclusion: 统一了多种机器学习模型与任务的理论基础，突出了亲和矩阵的重要性及其在信息流控制中的核心作用。

Abstract: The self-attention mechanism, now central to deep learning architectures such
as Transformers, is a modern instance of a more general computational
principle: learning and using pairwise affinity matrices to control how
information flows through a model. This paper traces the conceptual origins of
self-attention across multiple domains, including computer vision, natural
language processing, and graph learning, through their shared reliance on an
affinity matrix, denoted as A. We highlight Infinite Feature Selection (Inf-FS)
as a foundational approach that generalizes the idea of affinity-based
weighting. Unlike the fixed dot-product structure used in Transformers, Inf-FS
defines A either through domain knowledge or by learning, and computes feature
relevance through multi-hop propagation over the affinity graph. From this
perspective, self-attention can be seen as a special case of Inf-FS: it uses a
single-hop affinity computation where A is dynamically built from token
similarities. We argue that the underlying structure, reasoning over pairwise
relationships, is preserved across both approaches, and the key differences lie
in how the affinity matrix is defined and applied. By situating self-attention
within the broader paradigm of affinity-based computation, we unify several
strands of machine learning research and highlight a common mathematical
foundation that underpins diverse models and tasks.

</details>


### [323] [LPS-GNN : Deploying Graph Neural Networks on Graphs with 100-Billion Edges](https://arxiv.org/abs/2507.14570)
*Xu Cheng,Liang Yao,Feng He,Yukuo Cen,Yufei He,Chenhui Zhang,Wenzheng Feng,Hongyun Cai,Jie Tang*

Main category: cs.LG

TL;DR: 论文提出了LPS-GNN框架，能够在单块GPU上高效处理超大规模图，并在真实应用中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有大规模图的图神经网络在执行效率和预测准确性之间难以权衡，尤其是迭代消息传递技术在计算资源和显存需求上表现出巨大挑战。

Method: 设计了一个称为LPS-GNN的框架，包括新的图划分算法LPMetis，以及子图增强策略，优化了学习效率和性能，同时具备兼容性，支持多种图神经网络算法。

Result: LPS-GNN在单GPU上能在10小时内学习1000亿规模的图数据，在用户获取场景下提升13.8%，并在多个指标上优于当前先进方法。在实际应用和公开数据集测试中相比先进模型提升了8.24%至13.89%。

Conclusion: LPS-GNN提供了一种在资源受限环境下高效处理大规模图问题的解决方案，为图神经网络的实际部署奠定了良好基础。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for various graph
mining tasks, yet existing scalable solutions often struggle to balance
execution efficiency with prediction accuracy. These difficulties stem from
iterative message-passing techniques, which place significant computational
demands and require extensive GPU memory, particularly when dealing with the
neighbor explosion issue inherent in large-scale graphs. This paper introduces
a scalable, low-cost, flexible, and efficient GNN framework called LPS-GNN,
which can perform representation learning on 100 billion graphs with a single
GPU in 10 hours and shows a 13.8% improvement in User Acquisition scenarios. We
examine existing graph partitioning methods and design a superior graph
partition algorithm named LPMetis. In particular, LPMetis outperforms current
state-of-the-art (SOTA) approaches on various evaluation metrics. In addition,
our paper proposes a subgraph augmentation strategy to enhance the model's
predictive performance. It exhibits excellent compatibility, allowing the
entire framework to accommodate various GNN algorithms. Successfully deployed
on the Tencent platform, LPS-GNN has been tested on public and real-world
datasets, achieving performance lifts of 8. 24% to 13. 89% over SOTA models in
online applications.

</details>


### [324] [A Transformer-Based Conditional GAN with Multiple Instance Learning for UAV Signal Detection and Classification](https://arxiv.org/abs/2507.14592)
*Haochen Liu,Jia Bi,Xiaomin Wang,Xin Yang,Ling Wang*

Main category: cs.LG

TL;DR: 本研究提出一种结合Transformer和GAN的方法用于无人机（UAV）飞行状态分类，解决现有方法在动态环境中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有无人机飞行状态分类方法在动态环境中缺乏鲁棒性和泛化能力，同时前沿方法需要大量数据和高计算成本。

Method: 提出一种结合Transformer编码器和生成对抗网络（GAN）的框架，通过长距离依赖捕获和数据增强结合多实例学习（MIL），聚焦鉴别性输入段落并减少计算开销。

Result: 该方法在DroneDetect数据集上达到96.5%的精度，在DroneRF数据集上达到98.6%，优于其他SOTA方法，并表现出良好的计算效率及跨平台泛化能力。

Conclusion: 该框架具有真实部署潜力，适用于资源受限环境的实时无人机飞行状态分类应用。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly used in surveillance,
logistics, agriculture, disaster management, and military operations. Accurate
detection and classification of UAV flight states, such as hovering, cruising,
ascending, or transitioning, which are essential for safe and effective
operations. However, conventional time series classification (TSC) methods
often lack robustness and generalization for dynamic UAV environments, while
state of the art(SOTA) models like Transformers and LSTM based architectures
typically require large datasets and entail high computational costs,
especially with high-dimensional data streams. This paper proposes a novel
framework that integrates a Transformer-based Generative Adversarial Network
(GAN) with Multiple Instance Locally Explainable Learning (MILET) to address
these challenges in UAV flight state classification. The Transformer encoder
captures long-range temporal dependencies and complex telemetry dynamics, while
the GAN module augments limited datasets with realistic synthetic samples. MIL
is incorporated to focus attention on the most discriminative input segments,
reducing noise and computational overhead. Experimental results show that the
proposed method achieves superior accuracy 96.5% on the DroneDetect dataset and
98.6% on the DroneRF dataset that outperforming other SOTA approaches. The
framework also demonstrates strong computational efficiency and robust
generalization across diverse UAV platforms and flight states, highlighting its
potential for real-time deployment in resource constrained environments.

</details>


### [325] [$k$-PCA for (non-squared) Euclidean Distances: Polynomial Time Approximation](https://arxiv.org/abs/2507.14631)
*Daniel Greenhut,Dan Feldman*

Main category: cs.LG

TL;DR: 本文提出了一个多项式时间的确定性算法，用于近似点集合的$k$-维子空间中位数，该算法具有平方根$d$的近似因子和输入大小的多项式时间复杂度，并提供了实验和代码可验证性。


<details>
  <summary>Details</summary>
Motivation: 经典的$k$-PCA最小化点到子空间的平方欧几里得距离和，而计算点集合的$k$-维子空间中位数可以更具鲁棒性和稀疏性，解决这一问题面临非凸性和复杂度的挑战。

Method: 设计了一种多项式时间的算法，通过平方根$d$的近似因子在合理时间内近似求解$k$-维子空间中位数问题。算法同样适用于处理其他相关范数问题，如$ell_{2,z}$范数，和处理离群点或稀疏性问题。

Result: 提出的算法首次在计算复杂度和近似因子方面都避免了与$k$指数相关的增长，并在实验中验证了其效果。

Conclusion: 本文算法为低维子空间近似问题提供了有效的解决方案，在理论和实验上均表明其实用性，且开放了代码和结果以供验证。

Abstract: Given an integer $k\geq1$ and a set $P$ of $n$ points in $\REAL^d$, the
classic $k$-PCA (Principle Component Analysis) approximates the affine
\emph{$k$-subspace mean} of $P$, which is the $k$-dimensional affine linear
subspace that minimizes its sum of squared Euclidean distances
($\ell_{2,2}$-norm) over the points of $P$, i.e., the mean of these distances.
The \emph{$k$-subspace median} is the subspace that minimizes its sum of
(non-squared) Euclidean distances ($\ell_{2,1}$-mixed norm), i.e., their
median. The median subspace is usually more sparse and robust to noise/outliers
than the mean, but also much harder to approximate since, unlike the
$\ell_{z,z}$ (non-mixed) norms, it is non-convex for $k<d-1$.
  We provide the first polynomial-time deterministic algorithm whose both
running time and approximation factor are not exponential in $k$. More
precisely, the multiplicative approximation factor is $\sqrt{d}$, and the
running time is polynomial in the size of the input. We expect that our
technique would be useful for many other related problems, such as $\ell_{2,z}$
norm of distances for $z\not \in \br{1,2}$, e.g., $z=\infty$, and handling
outliers/sparsity.
  Open code and experimental results on real-world datasets are also provided.

</details>


### [326] [Rec-AD: An Efficient Computation Framework for FDIA Detection Based on Tensor Train Decomposition and Deep Learning Recommendation Model](https://arxiv.org/abs/2507.14668)
*Yunfeng Li,Junhong Liu,Zhaohui Yang,Guofu Liao,Chuyun Zhang*

Main category: cs.LG

TL;DR: 提出Rec-AD框架，通过张量分解和深度学习推荐模型结合，提高虚假数据注入攻击检测效率。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在智能电网的虚假数据注入攻击检测中广泛应用，但随着系统规模和数据维度的增加，计算和内存负担显著加重，降低了检测效率。

Method: 提出Rec-AD框架，通过嵌入压缩、索引重排和流水线训练机制优化训练和推理效率。完全兼容PyTorch，可直接集成于现有检测系统。

Result: 实验表明，Rec-AD显著提高了计算吞吐量和实时检测性能，缩小了攻击窗口并提高了攻击者成本。

Conclusion: Rec-AD强化了边缘计算能力与可扩展性，为智能电网安全提供了有力的技术支持。

Abstract: Deep learning models have been widely adopted for False Data Injection Attack
(FDIA) detection in smart grids due to their ability to capture unstructured
and sparse features. However, the increasing system scale and data
dimensionality introduce significant computational and memory burdens,
particularly in large-scale industrial datasets, limiting detection efficiency.
To address these issues, this paper proposes Rec-AD, a computationally
efficient framework that integrates Tensor Train decomposition with the Deep
Learning Recommendation Model (DLRM). Rec-AD enhances training and inference
efficiency through embedding compression, optimized data access via index
reordering, and a pipeline training mechanism that reduces memory communication
overhead. Fully compatible with PyTorch, Rec-AD can be integrated into existing
FDIA detection systems without code modifications. Experimental results show
that Rec-AD significantly improves computational throughput and real-time
detection performance, narrowing the attack window and increasing attacker
cost. These advancements strengthen edge computing capabilities and
scalability, providing robust technical support for smart grid security.

</details>


### [327] [Revisiting Graph Contrastive Learning on Anomaly Detection: A Structural Imbalance Perspective](https://arxiv.org/abs/2507.14677)
*Yiming Xu,Zhen Peng,Bin Shi,Xu Hua,Bo Dong,Song Wang,Chen Chen*

Main category: cs.LG

TL;DR: AD-GCL提出了一种面向结构不平衡网络的图对比学习框架，解决现有方法难以检测尾部异常节点的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习模型在异常检测中对整体性能过于重视，而忽略了结构不平衡性的问题，导致无法有效检测尾部异常节点。研究的动机是提高其在不平衡网络中的鲁棒性，满足实际高风险场景需求。

Method: 提出了AD-GCL框架，其采用邻域削减策略过滤头部节点的噪声边，完成伪尾部节点对齐；通过异常引导的邻域补全策略扩大尾部节点的感受野；并引入原图与增强图的视图间一致性损失，提升表示能力。

Result: 实验表明，AD-GCL在多个数据集上对头部和尾部异常节点的检测能力均优于现有方法。

Conclusion: AD-GCL框架增强了在结构不平衡网络中的异常检测能力，验证了其在头节点与尾节点检测中的全面优越性。

Abstract: The superiority of graph contrastive learning (GCL) has prompted its
application to anomaly detection tasks for more powerful risk warning systems.
Unfortunately, existing GCL-based models tend to excessively prioritize overall
detection performance while neglecting robustness to structural imbalance,
which can be problematic for many real-world networks following power-law
degree distributions. Particularly, GCL-based methods may fail to capture tail
anomalies (abnormal nodes with low degrees). This raises concerns about the
security and robustness of current anomaly detection algorithms and therefore
hinders their applicability in a variety of realistic high-risk scenarios. To
the best of our knowledge, research on the robustness of graph anomaly
detection to structural imbalance has received little scrutiny. To address the
above issues, this paper presents a novel GCL-based framework named AD-GCL. It
devises the neighbor pruning strategy to filter noisy edges for head nodes and
facilitate the detection of genuine tail nodes by aligning from head nodes to
forged tail nodes. Moreover, AD-GCL actively explores potential neighbors to
enlarge the receptive field of tail nodes through anomaly-guided neighbor
completion. We further introduce intra- and inter-view consistency loss of the
original and augmentation graph for enhanced representation. The performance
evaluation of the whole, head, and tail nodes on multiple datasets validates
the comprehensive superiority of the proposed AD-GCL in detecting both head
anomalies and tail anomalies.

</details>


### [328] [GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks](https://arxiv.org/abs/2507.14679)
*Zixin Xu,Zhijie Wang,Zhiyuan Pan*

Main category: cs.LG

TL;DR: 提出了一个新的垃圾信息检测框架GCC-Spam，通过字符相似性网络、对比学习和生成对抗网络优化检测效果，在真实数据集上实现了更高的检测率。


<details>
  <summary>Details</summary>
Motivation: 互联网上垃圾文本的快速增长，需要有效检测机制来缓解信息泄漏和社会问题，特别是针对垃圾发送者的对抗性策略以及标注数据稀缺问题。

Method: 1. 通过字符相似性网络提取拼写和语音特征，生成句子嵌入用于分类；2. 使用对比学习优化垃圾与正常文本间的隐空间距离；3. 利用生成对抗网络生成逼真的伪垃圾样本，减少标注数据需求。

Result: 在真实数据集上的实验表明，GCC-Spam模型以更少的标注数据实现了比基线方法更高的检测率。

Conclusion: GCC-Spam框架有效应对了垃圾文本检测中的关键问题，包括对抗攻击和数据稀缺，表现出更高的鲁棒性和准确性。

Abstract: The exponential growth of spam text on the Internet necessitates robust
detection mechanisms to mitigate risks such as information leakage and social
instability. This work addresses two principal challenges: adversarial
strategies employed by spammers and the scarcity of labeled data. We propose a
novel spam-text detection framework GCC-Spam, which integrates three core
innovations. First, a character similarity network captures orthographic and
phonetic features to counter character-obfuscation attacks and furthermore
produces sentence embeddings for downstream classification. Second, contrastive
learning enhances discriminability by optimizing the latent-space distance
between spam and normal texts. Third, a Generative Adversarial Network (GAN)
generates realistic pseudo-spam samples to alleviate data scarcity while
improving model robustness and classification accuracy. Extensive experiments
on real-world datasets demonstrate that our model outperforms baseline
approaches, achieving higher detection rates with significantly fewer labeled
examples.

</details>


### [329] [Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition](https://arxiv.org/abs/2507.14698)
*Xuetao Lin,Tianhao Peng,Peihong Dai,Yu Liang,Wenjun Wu*

Main category: cs.LG

TL;DR: 本文提出SST-CL框架，通过空间-时间变换器与课程学习结合，实现EEG情感识别的高效性与鲁棒性。实验验证框架的优越性能。


<details>
  <summary>Details</summary>
Motivation: 应对EEG情感识别中空间-时间模式整合与情感强度变化适应的挑战。

Method: 开发融合空间-时间变换器的框架SST-CL，包含空间编码器、时间编码器和情感强度意识课程学习。

Result: 在三个基准数据集上的实验显示SOTA性能，验证方法各组成部分的重要性。

Conclusion: SST-CL框架在非平稳性与动态变量处理方面展现出显著优势，为实际应用奠定基础。

Abstract: EEG-based emotion recognition plays an important role in developing adaptive
brain-computer communication systems, yet faces two fundamental challenges in
practical implementations: (1) effective integration of non-stationary
spatial-temporal neural patterns, (2) robust adaptation to dynamic emotional
intensity variations in real-world scenarios. This paper proposes SST-CL, a
novel framework integrating spatial-temporal transformers with curriculum
learning. Our method introduces two core components: a spatial encoder that
models inter-channel relationships and a temporal encoder that captures
multi-scale dependencies through windowed attention mechanisms, enabling
simultaneous extraction of spatial correlations and temporal dynamics from EEG
signals. Complementing this architecture, an intensity-aware curriculum
learning strategy progressively guides training from high-intensity to
low-intensity emotional states through dynamic sample scheduling based on a
dual difficulty assessment. Comprehensive experiments on three benchmark
datasets demonstrate state-of-the-art performance across various emotional
intensity levels, with ablation studies confirming the necessity of both
architectural components and the curriculum learning mechanism.

</details>


### [330] [Fraud is Not Just Rarity: A Causal Prototype Attention Approach to Realistic Synthetic Oversampling](https://arxiv.org/abs/2507.14706)
*Claudio Giusti,Luca Guarnera,Mirko Casu,Sebastiano Battiato*

Main category: cs.LG

TL;DR: 提出了一种新的分类架构CPAC来改善信用卡欺诈检测性能。


<details>
  <summary>Details</summary>
Motivation: 由于信用卡欺诈检测数据存在极端类别不平衡和欺诈模式较难区分的问题，现有方法生成的合成数据导致分类器过于自信，潜在空间聚类效果差。

Method: 提出一种名为CPAC（Causal Prototype Attention Classifier）的新架构，通过基于原型的注意力机制和结合VAE-GAN改进潜在空间结构，同时避免仅进行后处理的样本扩充方案。

Result: CPAC模型在性能指标F1得分达到93.14%，召回率达90.18%，表现优于现有的过采样器（比如SMOTE）及先进生成模型，并改善了潜在空间的聚类分离表现。

Conclusion: 通过分类器驱动的潜在空间处理方法，CPAC大大提升了信用卡欺诈检测的效率和可解释性，提供了一个强大的潜在聚类改进方案。

Abstract: Detecting fraudulent credit card transactions remains a significant
challenge, due to the extreme class imbalance in real-world data and the often
subtle patterns that separate fraud from legitimate activity. Existing research
commonly attempts to address this by generating synthetic samples for the
minority class using approaches such as GANs, VAEs, or hybrid generative
models. However, these techniques, particularly when applied only to
minority-class data, tend to result in overconfident classifiers and poor
latent cluster separation, ultimately limiting real-world detection
performance. In this study, we propose the Causal Prototype Attention
Classifier (CPAC), an interpretable architecture that promotes class-aware
clustering and improved latent space structure through prototype-based
attention mechanisms and we will couple it with the encoder in a VAE-GAN
allowing it to offer a better cluster separation moving beyond post-hoc sample
augmentation. We compared CPAC-augmented models to traditional oversamplers,
such as SMOTE, as well as to state-of-the-art generative models, both with and
without CPAC-based latent classifiers. Our results show that classifier-guided
latent shaping with CPAC delivers superior performance, achieving an F1-score
of 93.14\% percent and recall of 90.18\%, along with improved latent cluster
separation. Further ablation studies and visualizations provide deeper insight
into the benefits and limitations of classifier-driven representation learning
for fraud detection. The codebase for this work will be available at final
submission.

</details>


### [331] [Exploring the Dynamic Scheduling Space of Real-Time Generative AI Applications on Emerging Heterogeneous Systems](https://arxiv.org/abs/2507.14715)
*Rachid Karami,Rajeev Patwari,Hyoukjun Kwon,Ashish Sirasao*

Main category: cs.LG

TL;DR: 研究整合生成式AI，尤其是大型语言模型(LLMs)，到实时多模态应用，并在AMD Ryzen AI SoC上探索调度策略对性能的影响，强调异构调度的关键性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型在实时应用中的需求（如视频会议和游戏）导致新的计算高强度和低延迟工作负载，探索如何最优化异构SoC平台下的任务调度。

Method: 通过AMD Ryzen AI的异构SoC进行特性分析，构建行业案例场景，评估五种调度策略对实时性和大型语言模型性能的影响。

Result: 任务调度显著影响性能（如平均41.7%的截止期违约率差异），需要意识到任务动态和硬件异构性的调度策略。

Conclusion: 动态且任务感知的异构调度是实现高性能本地生成式AI应用的关键。

Abstract: The integration of generative AI models, particularly large language models
(LLMs), into real-time multi-model AI applications such as video conferencing
and gaming is giving rise to a new class of workloads: real-time generative AI
(RTGen). These workloads combine the compute intensity and dynamic execution
patterns of generative models with the stringent latency and concurrency
constraints of real-time inference. To meet the diverse demands of RTGen
workloads, modern edge platforms increasingly adopt heterogeneous
system-on-chip (SoC) architectures that integrate CPUs, GPUs, and NPUs. Despite
the potential of heterogeneous SoC, the scheduling space complexity and
performance implications of RTGen workloads on such platforms remain
underexplored. In this work, we perform a comprehensive characterization of
RTGen workloads on AMD's latest heterogeneous SoC, Ryzen AI. We construct
realistic multi-model scenarios inspired by industry use cases and profile
model performance across all available backends. Using this data, we evaluate
five scheduling policies and their impact on both real-time metrics (e.g.,
deadline violation rate) and LLM performance (e.g., time-to-first-token and
tokens-per-second). Our results show that scheduling decisions significantly
affect workload performance (e.g., leading to a 41.7% difference in deadline
violation rates on average), and highlight the need for scheduling strategies
that are aware of workload dynamics and hardware heterogeneity. Our findings
underscore the importance of workload-aware, dynamic heterogeneous scheduling
in enabling high-performance, on-device RTGen applications.

</details>


### [332] [LeanTree: Accelerating White-Box Proof Search with Factorized States in Lean 4](https://arxiv.org/abs/2507.14722)
*Matěj Kripner,Michal Šustr,Milan Straka*

Main category: cs.LG

TL;DR: 本文介绍了LeanTree，一种面向白盒自动定理证明(ATP)的工具链，改进了现有方法在中间状态利用上的劣势。


<details>
  <summary>Details</summary>
Motivation: 自动定理证明尽管是人工智能的经典问题，但其庞大的状态和动作空间使其仍然具有挑战性。本文旨在解决当前白盒方法未能充分利用中间状态信息的问题。

Method: 提出了一种基于Lean 4语言开发的工具LeanTree，将复杂的证明状态分解为独立的简单分支，同时构建了相关数据集。

Result: 实验表明，与黑盒方法相比，白盒方法在某些场景中性能更佳，同时提供了多种优势如评估简化与上下文精简。

Conclusion: 通过LeanTree，白盒方法在ATP领域中表现出潜在优势，可进一步推动自动定理证明的发展。

Abstract: Automated theorem proving (ATP) has been a classical problem in artificial
intelligence since its inception, yet it remains challenging due to its vast
state and action space. Large language models (LLMs) have recently emerged as a
promising heuristic for ATP, but they lack correctness guarantees and thus
require interaction with a proof verifier. Such interactions typically follow
one of two approaches: black-box interaction, which does not utilize
intermediate proof states, or white-box approaches, which allow for incremental
proof construction and examination of intermediate states. While black-box
approaches have directly benefited from recent LLM advances, white-box methods
have comparatively lagged behind. In this paper, we address this gap by
introducing LeanTree, which consists of (i) a tool built in the Lean 4 language
that factorizes complex proof states into simpler, independent branches, and
(ii) a dataset of these factorized intermediate states. Our white-box tooling
offers several advantages over black-box approaches: it simplifies evaluation,
reduces necessary context, generates richer training data, enables parallel
search across multiple states, supports efficient reuse of states, and provides
feedback in case of errors. Our preliminary results hint that white-box
approaches outperform black-box alternatives in some settings.

</details>


### [333] [Task-Agnostic Continual Prompt Tuning with Gradient-Based Selection and Decoding](https://arxiv.org/abs/2507.14725)
*Anushka Tiwari,Sayantan Pal,Rohini K. Srihari,Kaiyi Ji*

Main category: cs.LG

TL;DR: 提出了一种名为GRID的统一框架解决提示记忆扩展问题和任务无感知推理下的遗忘问题，提升了任务的转移效果与记忆效率。


<details>
  <summary>Details</summary>
Motivation: 现有的Prompt-based持续学习方法在任务无感知推理下存在潜在遗忘问题，同时随着任务增加，提示数量不断增长，难以扩展。

Method: 通过任务感知解码、输入表示任务自动识别、约束解码以及基于梯度的提示选择策略来压缩和聚合提示，减少提示记忆的占用。

Result: 实验表明，GRID在短序列、长序列和反转移基准测试中表现出优异的后向转移能力、竞争性的前向转移能力，并能减少80%的遗忘任务。

Conclusion: GRID框架有效解决了提示记忆扩展和潜在遗忘问题，表现优于一流方法，对持续学习中的大模型适配具有重要意义。

Abstract: Prompt-based continual learning (CL) offers a parameter-efficient way to
adapt large language models (LLMs) across task sequences. However, most
existing methods assume task-aware inference and maintain a growing list of
task-specific prompts, which limits scalability and hides latent forgetting. In
this work, we introduce GRID, a unified framework that addresses two key
limitations: (1) latent forgetting under task-agnostic inference, and (2)
prompt memory explosion as task sequences grow. GRID integrates a task-aware
decoding mechanism that improves backward transfer by leveraging representative
inputs, automatic task identification, and constrained decoding. Additionally,
we propose a gradient-based prompt selection strategy that compresses less
informative prompts into a single aggregated representation, enabling scalable
and memory-efficient lifelong learning. Extensive experiments across
short-sequence, long-sequence, and negative transfer benchmarks show that GRID
significantly improves backward transfer, achieves competitive forward
transfer, and reduces forgotten tasks by up to 80\%, outperforming
state-of-the-art methods on T5 and Flan-T5 backbones.

</details>


### [334] [Balancing Expressivity and Robustness: Constrained Rational Activations for Reinforcement Learning](https://arxiv.org/abs/2507.14736)
*Rafał Surdej,Michał Bortkiewicz,Alex Lewandowski,Mateusz Ostaszewski,Clare Lyle*

Main category: cs.LG

TL;DR: 本文研究了可训练的有理激活函数在强化学习和持续学习场景中的表现，提出了一种约束变体以改善训练稳定性并提高性能。


<details>
  <summary>Details</summary>
Motivation: 探索可训练激活函数在提升网络灵活性和适应性方面的潜力，同时克服它对训练稳定性的潜在负面影响。

Method: 分析可训练的有理激活函数在不同学习场景中的性能，提出一种结构性限制输出幅度的约束变体，并在强化学习与持续学习的基准环境中进行实验验证。

Result: 实验结果表明，该方法在MetaWorld和DMC环境中提高了训练稳定性和性能，在持续学习中改善了长期保留能力，同时凸显了连续控制任务中特有的表达性与可塑性权衡。

Conclusion: 该研究揭示了可训练有理激活函数的表达性与可塑性之间的权衡，并为动态非平稳环境下的鲁棒可适应激活函数设计提供了实用指导。

Abstract: Trainable activation functions, whose parameters are optimized alongside
network weights, offer increased expressivity compared to fixed activation
functions. Specifically, trainable activation functions defined as ratios of
polynomials (rational functions) have been proposed to enhance plasticity in
reinforcement learning. However, their impact on training stability remains
unclear. In this work, we study trainable rational activations in both
reinforcement and continual learning settings. We find that while their
flexibility enhances adaptability, it can also introduce instability, leading
to overestimation in RL and feature collapse in longer continual learning
scenarios. Our main result is demonstrating a trade-off between expressivity
and plasticity in rational activations. To address this, we propose a
constrained variant that structurally limits excessive output scaling while
preserving adaptability. Experiments across MetaWorld and DeepMind Control
Suite (DMC) environments show that our approach improves training stability and
performance. In continual learning benchmarks, including MNIST with reshuffled
labels and Split CIFAR-100, we reveal how different constraints affect the
balance between expressivity and long-term retention. While preliminary
experiments in discrete action domains (e.g., Atari) did not show similar
instability, this suggests that the trade-off is particularly relevant for
continuous control. Together, our findings provide actionable design principles
for robust and adaptable trainable activations in dynamic, non-stationary
environments. Code available at:
https://github.com/special114/rl_rational_plasticity.

</details>


### [335] [Better Training Data Attribution via Better Inverse Hessian-Vector Products](https://arxiv.org/abs/2507.14740)
*Andrew Wang,Elisa Nguyen,Runshi Yang,Juhan Bae,Sheila A. McIlraith,Roger Grosse*

Main category: cs.LG

TL;DR: 本研究提出了一种名为ASTRA的新算法，用于改进高精度逆海森矩阵-向量积（iHVP）的近似，从而提高训练数据归因（TDA）的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 目前梯度驱动的训练数据归因方法中逆海森矩阵-向量积（iHVP）计算难度较高，研究旨在解决其效率低下和难以调节的问题。

Method: 提出ASTRA算法，利用EKFAC预处理器结合Neumann级数迭代进行iHVP的高效准确近似计算，同时减少迭代次数并提升精度。

Result: ASTRA证明了在改进iHVP近似精度情况下，能够显著提升训练数据归因（TDA）的性能。

Conclusion: 改进iHVP近似精度的方法对于提升TDA性能具有重要意义，ASTRA算法为此提供了一种更优解决方案。

Abstract: Training data attribution (TDA) provides insights into which training data is
responsible for a learned model behavior. Gradient-based TDA methods such as
influence functions and unrolled differentiation both involve a computation
that resembles an inverse Hessian-vector product (iHVP), which is difficult to
approximate efficiently. We introduce an algorithm (ASTRA) which uses the
EKFAC-preconditioner on Neumann series iterations to arrive at an accurate iHVP
approximation for TDA. ASTRA is easy to tune, requires fewer iterations than
Neumann series iterations, and is more accurate than EKFAC-based
approximations. Using ASTRA, we show that improving the accuracy of the iHVP
approximation can significantly improve TDA performance.

</details>


### [336] [Beyond the Single-Best Model: Rashomon Partial Dependence Profile for Trustworthy Explanations in AutoML](https://arxiv.org/abs/2507.14744)
*Mustafa Cavus,Jan N. van Rijn,Przemysław Biecek*

Main category: cs.LG

TL;DR: 提出了一种基于Rashomon集合的解释方法，通过聚合接近最优模型的部分依赖轮廓(PDP)来帮助理解模型的variability和uncertainty。


<details>
  <summary>Details</summary>
Motivation: 现有自动化机器学习系统集中于单一模型而忽略了解释的不确定性，这在以人为中心的可解释AI中是一个重要问题。本文旨在结合模型的多样性提供更具可靠性和透明度的解释。

Method: 设计了一种基于Rashomon集合的PDP聚合框架，通过分析多个接近最优模型的数据依赖特性，量化和广泛呈现解释的变化范围，并提出两个量化指标用于评估Rashomon PDP的效果。

Result: 在35个回归数据集上的实验表明，Rashomon PDP覆盖率通常低于70%，说明现有单一模型解释的局限性，同时展现了Rashomon PDP在提高模型解释可靠性上的优势。

Conclusion: Rashomon PDP通过提供多样性和不确定性的解释，提升了高风险领域中的模型透明性和可信度。

Abstract: Automated machine learning systems efficiently streamline model selection but
often focus on a single best-performing model, overlooking explanation
uncertainty, an essential concern in human centered explainable AI. To address
this, we propose a novel framework that incorporates model multiplicity into
explanation generation by aggregating partial dependence profiles (PDP) from a
set of near optimal models, known as the Rashomon set. The resulting Rashomon
PDP captures interpretive variability and highlights areas of disagreement,
providing users with a richer, uncertainty aware view of feature effects. To
evaluate its usefulness, we introduce two quantitative metrics, the coverage
rate and the mean width of confidence intervals, to evaluate the consistency
between the standard PDP and the proposed Rashomon PDP. Experiments on 35
regression datasets from the OpenML CTR23 benchmark suite show that in most
cases, the Rashomon PDP covers less than 70% of the best model's PDP,
underscoring the limitations of single model explanations. Our findings suggest
that Rashomon PDP improves the reliability and trustworthiness of model
interpretations by adding additional information that would otherwise be
neglected. This is particularly useful in high stakes domains where
transparency and confidence are critical.

</details>


### [337] [Sampling from Gaussian Processes: A Tutorial and Applications in Global Sensitivity Analysis and Optimization](https://arxiv.org/abs/2507.14746)
*Bach Do,Nafeezat A. Ajenifuja,Taiwo A. Adebiyi,Ruda Zhang*

Main category: cs.LG

TL;DR: 本文讨论了高保真模拟和物理实验成本高的问题，并提出了高斯过程(GPs)作为代理模型，用以支撑全局敏感性分析(GSA)和优化任务。


<details>
  <summary>Details</summary>
Motivation: 在工程分析和设计中，虽然高保真模拟和物理实验至关重要，但高昂的成本限制了其在GSA和优化中的应用。这推动了使用GPs来降低计算成本并提高决策的可靠性。

Method: 提出了两种从GPs生成后验样本的方法：随机傅里叶特征和路径条件化，并对其在GSA、单目标和多目标优化中的应用进行了详细说明，同时也简要讨论了其它方法。

Result: 通过一系列数值实验，验证了所提出方法在GSA及单目标与多目标优化中的有效性。

Conclusion: 文章展示了基于GPs的采样方法在工程优化中的潜力，拓宽了GPs的应用边界。

Abstract: High-fidelity simulations and physical experiments are essential for
engineering analysis and design. However, their high cost often limits their
applications in two critical tasks: global sensitivity analysis (GSA) and
optimization. This limitation motivates the common use of Gaussian processes
(GPs) as proxy regression models to provide uncertainty-aware predictions based
on a limited number of high-quality observations. GPs naturally enable
efficient sampling strategies that support informed decision-making under
uncertainty by extracting information from a subset of possible functions for
the model of interest. Despite their popularity in machine learning and
statistics communities, sampling from GPs has received little attention in the
community of engineering optimization. In this paper, we present the
formulation and detailed implementation of two notable sampling methods --
random Fourier features and pathwise conditioning -- for generating posterior
samples from GPs. Alternative approaches are briefly described. Importantly, we
detail how the generated samples can be applied in GSA, single-objective
optimization, and multi-objective optimization. We show successful applications
of these sampling methods through a series of numerical examples.

</details>


### [338] [Skill Learning via Policy Diversity Yields Identifiable Representations for Reinforcement Learning](https://arxiv.org/abs/2507.14748)
*Patrik Reizinger,Bálint Mucsányi,Siyuan Guo,Benjamin Eysenbach,Bernhard Schölkopf,Wieland Brendel*

Main category: cs.LG

TL;DR: 本文研究了在强化学习中，基于互信息的技能学习（MISL）如何通过对比继承特征（CSF）方法，恢复环境的真实特征。


<details>
  <summary>Details</summary>
Motivation: 探讨强化学习中MISL方法中表征学习的重要性及其相关理论研究的不足。

Method: 引入对比继承特征（CSF），通过理论分析和实验验证，研究互信息目标和特征多样性对环境特征恢复的作用。

Result: 证明CSF可在线性变换下恢复环境真实特征，并实验证明其在基于状态和像素的任务中的有效性。

Conclusion: CSF提供了RL中表征学习的首次可辨识性保证，有助于深入理解互信息目标和正则项对学习效果的影响。

Abstract: Self-supervised feature learning and pretraining methods in reinforcement
learning (RL) often rely on information-theoretic principles, termed mutual
information skill learning (MISL). These methods aim to learn a representation
of the environment while also incentivizing exploration thereof. However, the
role of the representation and mutual information parametrization in MISL is
not yet well understood theoretically. Our work investigates MISL through the
lens of identifiable representation learning by focusing on the Contrastive
Successor Features (CSF) method. We prove that CSF can provably recover the
environment's ground-truth features up to a linear transformation due to the
inner product parametrization of the features and skill diversity in a
discriminative sense. This first identifiability guarantee for representation
learning in RL also helps explain the implications of different mutual
information objectives and the downsides of entropy regularizers. We
empirically validate our claims in MuJoCo and DeepMind Control and show how CSF
provably recovers the ground-truth features both from states and pixels.

</details>


### [339] [CXR-TFT: Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories](https://arxiv.org/abs/2507.14766)
*Mehak Arora,Ayman Ali,Kaiyuan Wu,Carolyn Davis,Takashi Shimazui,Mahmoud Alwakeel,Victor Moas,Philip Yang,Annette Esper,Rishikesan Kamaleswaran*

Main category: cs.LG

TL;DR: 本研究开发了一种名为CXR-TFT的新框架，将稀疏的胸片成像和放射学报告与高频临床数据结合，用于预测ICU患者胸片发现的动态变化。


<details>
  <summary>Details</summary>
Motivation: 解决CXR诊断方法的局限性，包括其时间动态分析不足及采集不规律等问题，促进危急病患的早期诊断和管理。

Method: 通过将视觉编码器生成的胸片潜在嵌入与按小时记录的临床数据进行时间对齐，使用Transformer模型预测每小时胸片嵌入，基于之前的嵌入和临床测量数据进行训练。

Result: 在20000名ICU患者的回顾性研究中，该方法成功预测异常胸片发现，提前长达12小时发现异常情况。

Conclusion: CXR-TFT提高了对急性病症的早期预测和干预能力，展示了改善危急患者临床结局的潜力。

Abstract: In intensive care units (ICUs), patients with complex clinical conditions
require vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a
vital diagnostic tool, providing insights into clinical trajectories, but their
irregular acquisition limits their utility. Existing tools for CXR
interpretation are constrained by cross-sectional analysis, failing to capture
temporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal
framework that integrates temporally sparse CXR imaging and radiology reports
with high-frequency clinical data, such as vital signs, laboratory values, and
respiratory flow sheets, to predict the trajectory of CXR findings in
critically ill patients. CXR-TFT leverages latent embeddings from a vision
encoder that are temporally aligned with hourly clinical data through
interpolation. A transformer model is then trained to predict CXR embeddings at
each hour, conditioned on previous embeddings and clinical measurements. In a
retrospective study of 20,000 ICU patients, CXR-TFT demonstrated high accuracy
in forecasting abnormal CXR findings up to 12 hours before they became
radiographically evident. This predictive capability in clinical data holds
significant potential for enhancing the management of time-sensitive conditions
like acute respiratory distress syndrome, where early intervention is crucial
and diagnoses are often delayed. By providing distinctive temporal resolution
in prognostic CXR analysis, CXR-TFT offers actionable 'whole patient' insights
that can directly improve clinical outcomes.

</details>


### [340] [Rethinking Memorization Measures and their Implications in Large Language Models](https://arxiv.org/abs/2507.14777)
*Bishwamittra Ghosh,Soumi Das,Qinyuan Wu,Mohammad Aflah Khan,Krishna P. Gummadi,Evimaria Terzi,Deepak Garg*

Main category: cs.LG

TL;DR: 论文探讨了大语言模型(LLM)中的记忆化现象，提出了新的上下文记忆化概念，并发现语言的最优学习无法避免部分记忆化，同时不同记忆化测量方式存在差异。


<details>
  <summary>Details</summary>
Motivation: 研究是否能够在最优语言学习下避免记忆化，并验证现有记忆化对隐私威胁的担忧是否被夸大。

Method: 重新审视已有的记忆化测量方法并提出新的上下文记忆化框架，实验分析了18种LLM，对比不同测量方法在多种语言场景的表现。

Result: 发现记忆化测量方法对字符串排序结果不一致；语言最优学习需要部分训练数据的记忆；改进学习减少上下文与反事实记忆化，但增加基于回忆的记忆化。

Conclusion: 记忆化不可完全避免，但隐私威胁需要结合多种测量方法综合评估。

Abstract: Concerned with privacy threats, memorization in LLMs is often seen as
undesirable, specifically for learning. In this paper, we study whether
memorization can be avoided when optimally learning a language, and whether the
privacy threat posed by memorization is exaggerated or not. To this end, we
re-examine existing privacy-focused measures of memorization, namely
recollection-based and counterfactual memorization, along with a newly proposed
contextual memorization.
  Relating memorization to local over-fitting during learning, contextual
memorization aims to disentangle memorization from the contextual learning
ability of LLMs. Informally, a string is contextually memorized if its
recollection due to training exceeds the optimal contextual recollection, a
learned threshold denoting the best contextual learning without training.
Conceptually, contextual recollection avoids the fallacy of recollection-based
memorization, where any form of high recollection is a sign of memorization.
Theoretically, contextual memorization relates to counterfactual memorization,
but imposes stronger conditions. Memorization measures differ in outcomes and
information requirements.
  Experimenting on 18 LLMs from 6 families and multiple formal languages of
different entropy, we show that (a) memorization measures disagree on
memorization order of varying frequent strings, (b) optimal learning of a
language cannot avoid partial memorization of training strings, and (c)
improved learning decreases contextual and counterfactual memorization but
increases recollection-based memorization. Finally, (d) we revisit existing
reports of memorized strings by recollection that neither pose a privacy threat
nor are contextually or counterfactually memorized.

</details>


### [341] [Omni-Think: Scaling Cross-Domain Generalization in LLMs via Multi-Task RL with Hybrid Rewards](https://arxiv.org/abs/2507.14783)
*Derek Li,Jiaming Zhou,Amirreza Kazemi,Qianyi Sun,Abbas Ghaddar,Mohammad Ali Alomrani,Liheng Ma,Yu Luo,Dong Li,Feng Wen,Jianye Hao,Mark Coates,Yingxue Zhang*

Main category: cs.LG

TL;DR: 引入Omni-Think的统一强化学习框架，通过结合规则验证奖励和生成偏好信号，优化大型语言模型的多任务表现，并展示课程式任务排序的优势。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型训练后方法（如有监督微调）在泛化能力上存在局限，往往偏向记忆而非可传递学习，需要一种提升多任务泛用性的训练方法。

Method: 利用统一强化学习框架Omni-Think，结合规则验证奖励和生成偏好信号，以LLM作为评估判别者，同时采用以结构化到开放式任务为序的课程学习策略。

Result: 实验显示课程式学习比联合训练提升性能5.2%，比模型合并提升9.1%。

Conclusion: 任务感知的采样与混合监督对扩展以强化学习为基础的大型语言模型后训练至关重要，课程学习显著增强泛化性能。

Abstract: The advancement of general-purpose artificial intelligence relies on large
language models (LLMs) that excel across a wide range of tasks, from structured
reasoning to creative generation. However, post-training methods like
Supervised Fine-Tuning (SFT) often struggle with generalization, favoring
memorization over transferable learning. In this work, we introduce Omni-Think,
a unified reinforcement learning (RL) framework that enhances LLM performance
across diverse tasks by combining rule-based verifiable rewards with generative
preference signals via LLM-as-a-Judge evaluations. Our approach enables
consistent optimization across task types and scales RL-based training to
subjective domains. We further investigate training strategies, demonstrating
that a curriculum-based progression that orders tasks from structured to
open-ended improves performance and reduces forgetting. Experimental results
across four domains reveal that curriculum learning improves performance by
5.2\% over joint training and 9.1\% over model merging. These results highlight
the importance of task-aware sampling and hybrid supervision in scaling
RL-based post-training for general-purpose LLMs.

</details>


### [342] [Exploring the In-Context Learning Capabilities of LLMs for Money Laundering Detection in Financial Graphs](https://arxiv.org/abs/2507.14785)
*Erfan Pirmorad*

Main category: cs.LG

TL;DR: 本研究探讨了使用大型语言模型（LLMs）对金融知识图的局部子图进行推理，以识别反洗钱行为中的可疑实体，并生成解释性分析。


<details>
  <summary>Details</summary>
Motivation: 为了应对反洗钱中涉及实体的复杂性和互联性，需要对图结构数据进行推理，作者提出使用LLMs作为推理工具进行分析。

Method: 提出了一个轻量化管道，通过提取兴趣实体周围的k-hop邻域，将其序列化为结构化文本，并通过少量上下文学习提示LLM对其可疑性进行评估和生成解释。

Result: 在合成的反洗钱场景中，LLMs展现出模拟分析师逻辑的能力，能够突出重要迹象并提供连贯的解释。

Conclusion: 这项研究展示了以LLM为基础的图推理在反洗钱领域的潜力，并为解释性、语言驱动的金融犯罪分析奠定了基础。

Abstract: The complexity and interconnectivity of entities involved in money laundering
demand investigative reasoning over graph-structured data. This paper explores
the use of large language models (LLMs) as reasoning engines over localized
subgraphs extracted from a financial knowledge graph. We propose a lightweight
pipeline that retrieves k-hop neighborhoods around entities of interest,
serializes them into structured text, and prompts an LLM via few-shot
in-context learning to assess suspiciousness and generate justifications. Using
synthetic anti-money laundering (AML) scenarios that reflect common laundering
behaviors, we show that LLMs can emulate analyst-style logic, highlight red
flags, and provide coherent explanations. While this study is exploratory, it
illustrates the potential of LLM-based graph reasoning in AML and lays
groundwork for explainable, language-driven financial crime analytics.

</details>


### [343] [Flow Equivariant Recurrent Neural Networks](https://arxiv.org/abs/2507.14793)
*T. Anderson Keller*

Main category: cs.LG

TL;DR: 该研究探讨了顺序模型中引入流不变性的问题，提出了一种新的方法来改进循环神经网络（RNN）在处理时间参数化序列上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习研究中，不变性结构被用于提升模型的泛化能力。然而，该特性主要被应用于静态数据与前馈网络，对时间序列模型的研究仍相对不足。

Method: 研究扩展了不变神经网络理论至时间“流”（一参数李群）的应用，提出了一种设计流不变性循环神经网络的方法，并与传统RNN进行对比实验。

Result: 引入流不变性的模型在训练速度、序列长度泛化、速度泛化等方面均显著优于传统RNN。

Conclusion: 通过首次在时间序列模型中引入时间参数化对称性，该研究为未来开发更加高效的顺序模型开辟了新方向。

Abstract: Data arrives at our senses as a continuous stream, smoothly transforming from
one instant to the next. These smooth transformations can be viewed as
continuous symmetries of the environment that we inhabit, defining equivalence
relations between stimuli over time. In machine learning, neural network
architectures that respect symmetries of their data are called equivariant and
have provable benefits in terms of generalization ability and sample
efficiency. To date, however, equivariance has been considered only for static
transformations and feed-forward networks, limiting its applicability to
sequence models, such as recurrent neural networks (RNNs), and corresponding
time-parameterized sequence transformations. In this work, we extend
equivariant network theory to this regime of `flows' -- one-parameter Lie
subgroups capturing natural transformations over time, such as visual motion.
We begin by showing that standard RNNs are generally not flow equivariant:
their hidden states fail to transform in a geometrically structured manner for
moving stimuli. We then show how flow equivariance can be introduced, and
demonstrate that these models significantly outperform their non-equivariant
counterparts in terms of training speed, length generalization, and velocity
generalization, on both next step prediction and sequence classification. We
present this work as a first step towards building sequence models that respect
the time-parameterized symmetries which govern the world around us.

</details>


### [344] [Subliminal Learning: Language models transmit behavioral traits via hidden signals in data](https://arxiv.org/abs/2507.14805)
*Alex Cloud,Minh Le,James Chua,Jan Betley,Anna Sztyber-Betley,Jacob Hilton,Samuel Marks,Owain Evans*

Main category: cs.LG

TL;DR: 研究发现，语言模型可以通过看似无关的数据传递行为特征（如喜好或偏差）。学生模型会从仅包含数字序列的训练数据中学习教师模型的特性，即便数据经过过滤去除了相关特性信息。


<details>
  <summary>Details</summary>
Motivation: 研究探讨语言模型中一种意想不到的现象，即教师模型通过训练生成的数据传递自身行为特征的机制。

Method: 通过教师模型生成数字序列数据并用于训练学生模型，观察学生是否能学习到教师的特性，并通过理论和实验验证这种机制是否普遍存在。

Result: 即使数据经过过滤，学生模型仍能复制教师模型的行为特征。这种现象对于同样的模型架构有效，但在基础模型不同的情况下无效。

Conclusion: “潜意识学习”是一种普遍存在的现象，这对AI开发提出了新挑战，可能导致意外特质的传播。

Abstract: We study subliminal learning, a surprising phenomenon where language models
transmit behavioral traits via semantically unrelated data. In our main
experiments, a "teacher" model with some trait T (such as liking owls or being
misaligned) generates a dataset consisting solely of number sequences.
Remarkably, a "student" model trained on this dataset learns T. This occurs
even when the data is filtered to remove references to T. We observe the same
effect when training on code or reasoning traces generated by the same teacher
model. However, we do not observe the effect when the teacher and student have
different base models. To help explain our findings, we prove a theoretical
result showing that subliminal learning occurs in all neural networks under
certain conditions, and demonstrate subliminal learning in a simple MLP
classifier. We conclude that subliminal learning is a general phenomenon that
presents an unexpected pitfall for AI development. Distillation could propagate
unintended traits, even when developers try to prevent this via data filtering.

</details>


### [345] [Benchmarking Foundation Models with Multimodal Public Electronic Health Records](https://arxiv.org/abs/2507.14824)
*Kunyu Yu,Rui Yang,Jingchi Liao,Siqi Li,Huitao Li,Irene Li,Yifan Peng,Rishikesan Kamaleswaran,Nan Liu*

Main category: cs.LG

TL;DR: 本文提出了一个针对电子健康记录(EHRs)基础模型的综合基准，以评估其性能、公平性和可解释性，并发现多模态数据整合有助于提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型在处理不同医疗数据模态时的性能并支持公平、可解释的评估方法。

Method: 基于MIMIC-IV公开数据库，设计了标准化数据处理流程，系统比较了8种基础模型（单模态、多模态以及领域特定与通用模型）。

Result: 整合多模态数据可显著提升预测性能，而不会引入额外偏见。

Conclusion: 通过这一基准，有助于开发高效且可信的多模态人工智能系统应用于临床环境。

Abstract: Foundation models have emerged as a powerful approach for processing
electronic health records (EHRs), offering flexibility to handle diverse
medical data modalities. In this study, we present a comprehensive benchmark
that evaluates the performance, fairness, and interpretability of foundation
models, both as unimodal encoders and as multimodal learners, using the
publicly available MIMIC-IV database. To support consistent and reproducible
evaluation, we developed a standardized data processing pipeline that
harmonizes heterogeneous clinical records into an analysis-ready format. We
systematically compared eight foundation models, encompassing both unimodal and
multimodal models, as well as domain-specific and general-purpose variants. Our
findings demonstrate that incorporating multiple data modalities leads to
consistent improvements in predictive performance without introducing
additional bias. Through this benchmark, we aim to support the development of
effective and trustworthy multimodal artificial intelligence (AI) systems for
real-world clinical applications. Our code is available at
https://github.com/nliulab/MIMIC-Multimodal.

</details>


### [346] [eMargin: Revisiting Contrastive Learning with Margin-Based Separation](https://arxiv.org/abs/2507.14828)
*Abdul-Kazeem Shamba,Kerstin Bach,Gavin Taylor*

Main category: cs.LG

TL;DR: 本文探讨了在时序表示学习中将自适应边距引入对比损失函数的效果。


<details>
  <summary>Details</summary>
Motivation: 分析自适应边距是否能够改善相邻但相异的时刻之间的分离，从而提升下游任务表现。

Method: 在对比损失函数中引入基于预定义相似性阈值调整的自适应边距（eMargin），并评估其在三个基准数据集上的聚类和分类性能。

Result: eMargin在无监督聚类指标上优于现有方法，但在线性探测的分类任务中表现不如预期。

Conclusion: 高聚类分数并不总能保证对下游任务有意义的嵌入表示，自适应边距的提升有限。

Abstract: We revisit previous contrastive learning frameworks to investigate the effect
of introducing an adaptive margin into the contrastive loss function for time
series representation learning. Specifically, we explore whether an adaptive
margin (eMargin), adjusted based on a predefined similarity threshold, can
improve the separation between adjacent but dissimilar time steps and
subsequently lead to better performance in downstream tasks. Our study
evaluates the impact of this modification on clustering performance and
classification in three benchmark datasets. Our findings, however, indicate
that achieving high scores on unsupervised clustering metrics does not
necessarily imply that the learned embeddings are meaningful or effective in
downstream tasks. To be specific, eMargin added to InfoNCE consistently
outperforms state-of-the-art baselines in unsupervised clustering metrics, but
struggles to achieve competitive results in downstream classification with
linear probing. The source code is publicly available at
https://github.com/sfi-norwai/eMargin.

</details>


### [347] [The Invisible Leash: Why RLVR May Not Escape Its Origin](https://arxiv.org/abs/2507.14843)
*Fang Wu,Weihao Xuan,Ximing Lu,Zaid Harchaoui,Yejin Choi*

Main category: cs.LG

TL;DR: 本文探讨了强化学习与可验证奖励（RLVR）方法在提升AI解决复杂逻辑任务能力中的潜在局限性。研究通过理论与实证验证了RLVR的作用和边界。


<details>
  <summary>Details</summary>
Motivation: 旨在揭示RLVR是否真正扩展了模型的推理边界，还是仅仅放大了模型已有的高奖励输出以提高精度。

Method: 从理论角度分析RLVR的约束和行为特性，并通过实验探讨其对推理精度和探索能力的影响。

Result: 实验证明RLVR虽然可以提升精度（如pass@1指标），但普遍会缩小解答的探索范围，未能找回基本模型原本可访问的正确答案，表明存在熵-奖励权衡问题。

Conclusion: RLVR存在推理边界上的局限，要打破这些限制可能需要通过未来的算法创新，引入显式的探索机制或混合策略来提升未被充分代表解的概率分布。

Abstract: Recent advances in large reasoning models highlight Reinforcement Learning
with Verifiable Rewards (RLVR) as a promising method for enhancing AI's
capabilities, particularly in solving complex logical tasks. However, it
remains unclear whether RLVR truly expands a model's reasoning boundary or
merely amplifies high-reward outputs that the base model already knows for
improved precision. This study presents a theoretical and empirical
investigation that provides fresh insights into the potential limits of RLVR.
First, we offer a new theoretical perspective that RLVR is constrained by the
base model's support-unable to sample solutions with zero initial
probability-and operates as a conservative reweighting mechanism that may
restrict the discovery of entirely original solutions. We also identify an
entropy-reward tradeoff: while RLVR reliably enhances precision, it may
progressively narrow exploration and potentially overlook correct yet
underrepresented solutions. Extensive empirical experiments validate that while
RLVR consistently improves pass@1, the shrinkage of empirical support generally
outweighs the expansion of empirical support under larger sampling budgets,
failing to recover correct answers that were previously accessible to the base
model. Interestingly, we also observe that while RLVR sometimes increases
token-level entropy, resulting in greater uncertainty at each generation step,
answer-level entropy declines, indicating that these seemingly more uncertain
paths ultimately converge onto a smaller set of distinct answers. Taken
together, these findings reveal potential limits of RLVR in extending reasoning
horizons. Breaking this invisible leash may require future algorithmic
innovations such as explicit exploration mechanisms or hybrid strategies that
seed probability mass into underrepresented solution regions.

</details>


### [348] [Time-Aware Attention for Enhanced Electronic Health Records Modeling](https://arxiv.org/abs/2507.14847)
*Junhan Yu,Zhunyi Feng,Junwei Lu,Tianxi Cai,Doudou Zhou*

Main category: cs.LG

TL;DR: 提出了TALE-EHR，一个基于Transformer的框架，帮助更高效地分析电子健康记录（EHR）。


<details>
  <summary>Details</summary>
Motivation: EHR具有预测患者结果和指导医疗决策的重要价值，但由于数据的异质性和复杂的时间模式，分析难度较高。

Method: 引入TALE-EHR框架，采用时间感知注意力机制来捕捉序列动态，并结合使用预训练大型语言模型生成的标准化代码描述嵌入，以增强临床语义理解。

Result: 实验表明，在MIMIC-IV和PIC数据集上，TALE-EHR在疾病进展预测等任务中表现优于现有的先进基线方法。

Conclusion: 通过显式的连续时间建模和强大的语义表示，TALE-EHR为电子健康记录分析提供了创新的解决方案。

Abstract: Electronic Health Records (EHR) contain valuable clinical information for
predicting patient outcomes and guiding healthcare decisions. However,
effectively modeling Electronic Health Records (EHRs) requires addressing data
heterogeneity and complex temporal patterns. Standard approaches often struggle
with irregular time intervals between clinical events. We propose TALE-EHR, a
Transformer-based framework featuring a novel time-aware attention mechanism
that explicitly models continuous temporal gaps to capture fine-grained
sequence dynamics. To complement this temporal modeling with robust semantics,
TALE-EHR leverages embeddings derived from standardized code descriptions using
a pre-trained Large Language Model (LLM), providing a strong foundation for
understanding clinical concepts. Experiments on the MIMIC-IV and PIC dataset
demonstrate that our approach outperforms state-of-the-art baselines on tasks
such as disease progression forecasting. TALE-EHR underscores the benefit of
integrating explicit, continuous temporal modeling with strong semantic
representations provides a powerful solution for advancing EHR analysis.

</details>


### [349] [Hierarchical Multi-Agent Reinforcement Learning with Control Barrier Functions for Safety-Critical Autonomous Systems](https://arxiv.org/abs/2507.14850)
*H. M. Sabbir Ahmad,Ehsan Sabouni,Alexander Wasilkoff,Param Budhraja,Zijian Guo,Songyuan Zhang,Chuchu Fan,Christos Cassandras,Wenchao Li*

Main category: cs.LG

TL;DR: 提出了一种基于控制屏障函数(CBF)的分层多智能体强化学习(HMARL)方法，用于多智能体安全关键系统中实现安全的策略学习。


<details>
  <summary>Details</summary>
Motivation: 解决在多智能体的安全关键自治系统中，如何同时满足个体的安全要求并实现智能体间的合作完成任务的问题。

Method: 通过分层架构将问题分为两部分：高层学习智能体间的联合合作行为，低层在高层策略的指导下学习安全的个体行为；具体提出基于技能的HMARL-CBF算法，高层学习联合技能政策，低层利用CBF实现技能的安全执行。

Result: 在多个具有冲突路网的复杂场景中验证了方法，其安全性和成功率接近完美（误差在5%以内）并且在所有环境中均提升了总体性能。

Conclusion: 该方法显著改进了现有技术在多智能体安全性和任务完成率上的表现，兼顾了安全和性能。

Abstract: We address the problem of safe policy learning in multi-agent safety-critical
autonomous systems. In such systems, it is necessary for each agent to meet the
safety requirements at all times while also cooperating with other agents to
accomplish the task. Toward this end, we propose a safe Hierarchical
Multi-Agent Reinforcement Learning (HMARL) approach based on Control Barrier
Functions (CBFs). Our proposed hierarchical approach decomposes the overall
reinforcement learning problem into two levels learning joint cooperative
behavior at the higher level and learning safe individual behavior at the lower
or agent level conditioned on the high-level policy. Specifically, we propose a
skill-based HMARL-CBF algorithm in which the higher level problem involves
learning a joint policy over the skills for all the agents and the lower-level
problem involves learning policies to execute the skills safely with CBFs. We
validate our approach on challenging environment scenarios whereby a large
number of agents have to safely navigate through conflicting road networks.
Compared with existing state of the art methods, our approach significantly
improves the safety achieving near perfect (within 5%) success/safety rate
while also improving performance across all the environments.

</details>


### [350] [The Tsetlin Machine Goes Deep: Logical Learning and Reasoning With Graphs](https://arxiv.org/abs/2507.14874)
*Ole-Christoffer Granmo,Youmna Abdelwahab,Per-Arne Andersen,Paul F. A. Clarke,Kunal Dumbre,Ylva Grønninsæter,Vojtech Halenka,Runar Helin,Lei Jiao,Ahmed Khalid,Rebekka Omslandseter,Rupsa Saha,Mayur Shende,Xuan Zhang*

Main category: cs.LG

TL;DR: Graph Tsetlin Machine (GraphTM)通过引入图结构学习深度规则，改进了可解释性和数据利用率，在多个领域优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 提升Tsetlin Machine (TM)在处理图结构数据时的适应能力，并保持高效性和可解释性。

Method: 引入GraphTM，通过消息传递机制在图结构输入中构建嵌套深度规则，增强对子图模式的识别能力。

Result: GraphTM在图像分类、动作共指追踪、推荐系统及病毒基因组序列等任务中表现优异，分别获得明显的精度提升和显著的训练速度加快。

Conclusion: GraphTM在图表示学习中的应用展现了深度规则与TM学习相结合的潜力，为多领域任务提供了效率与表现兼具的新方法。

Abstract: Pattern recognition with concise and flat AND-rules makes the Tsetlin Machine
(TM) both interpretable and efficient, while the power of Tsetlin automata
enables accuracy comparable to deep learning on an increasing number of
datasets. We introduce the Graph Tsetlin Machine (GraphTM) for learning
interpretable deep clauses from graph-structured input. Moving beyond flat,
fixed-length input, the GraphTM gets more versatile, supporting sequences,
grids, relations, and multimodality. Through message passing, the GraphTM
builds nested deep clauses to recognize sub-graph patterns with exponentially
fewer clauses, increasing both interpretability and data utilization. For image
classification, GraphTM preserves interpretability and achieves 3.86%-points
higher accuracy on CIFAR-10 than a convolutional TM. For tracking action
coreference, faced with increasingly challenging tasks, GraphTM outperforms
other reinforcement learning methods by up to 20.6%-points. In recommendation
systems, it tolerates increasing noise to a greater extent than a Graph
Convolutional Neural Network (GCN), e.g., for noise ratio 0.1, GraphTM obtains
accuracy 89.86% compared to GCN's 70.87%. Finally, for viral genome sequence
data, GraphTM is competitive with BiLSTM-CNN and GCN accuracy-wise, training
2.5x faster than GCN. The GraphTM's application to these varied fields
demonstrates how graph representation learning and deep clauses bring new
possibilities for TM learning.

</details>


### [351] [Application-Specific Component-Aware Structured Pruning of Deep Neural Networks via Soft Coefficient Optimization](https://arxiv.org/abs/2507.14882)
*Ganesh Sundaram,Jonas Ulmen,Amjad Haider,Daniel Görges*

Main category: cs.LG

TL;DR: 本文提出了增强的重要性度量框架，在保证任务关键性能的前提下实现DNN模型的有效压缩。


<details>
  <summary>Details</summary>
Motivation: 解决DNN模型压缩中，特别是结构化剪枝对任务特定性能造成影响的问题。

Method: 提出增强的重要性度量框架，采用多种策略确定各组的最优剪枝规模，平衡压缩效果与任务性能。

Result: 在MNIST重建任务的实验中验证了该方法有效地保持任务相关性能，即使显著剪枝后，模型仍然满足应用特定标准。

Conclusion: 通过增强的重要性度量框架，有效实现了压缩与性能的平衡，为DNN模型压缩提供了又一可行方案。

Abstract: Deep neural networks (DNNs) offer significant versatility and performance
benefits, but their widespread adoption is often hindered by high model
complexity and computational demands. Model compression techniques such as
pruning have emerged as promising solutions to these challenges. However, it
remains critical to ensure that application-specific performance
characteristics are preserved during compression. In structured pruning, where
groups of structurally coherent elements are removed, conventional importance
metrics frequently fail to maintain these essential performance attributes. In
this work, we propose an enhanced importance metric framework that not only
reduces model size but also explicitly accounts for application-specific
performance constraints. We employ multiple strategies to determine the optimal
pruning magnitude for each group, ensuring a balance between compression and
task performance. Our approach is evaluated on an autoencoder tasked with
reconstructing MNIST images. Experimental results demonstrate that the proposed
method effectively preserves task-relevant performance, maintaining the model's
usability even after substantial pruning, by satisfying the required
application-specific criteria.

</details>


### [352] [Old Rules in a New Game: Mapping Uncertainty Quantification to Quantum Machine Learning](https://arxiv.org/abs/2507.14919)
*Maximilian Wendlinger,Kilian Tscharke,Pascal Debus*

Main category: cs.LG

TL;DR: 本文研究量子机器学习中的不透明性问题，通过将经典不确定性量化方法引入量子领域，开发并评估了相关技术。


<details>
  <summary>Details</summary>
Motivation: 随着量子机器学习的兴起，其不透明性问题日益凸显，这一问题在传统深度学习中也存在但未解决，因此需要在量子机器学习中引入不确定性量化方法来提升模型的透明性。

Method: 作者以经典不确定性量化研究和量子贝叶斯建模的初步探索为基础，理论开发并实证评估了将经典不确定性量化方法映射到量子机器学习领域的技术。

Result: 研究结果表明，从经典不确定性量化中汲取经验，对新量子机器学习模型的设计过程加入不确定性感知是非常必要的。

Conclusion: 量子机器学习需要结合经典不确定性量化技术，为模型设计融入不确定性以解决不透明性问题，这是进一步发展的关键方向。

Abstract: One of the key obstacles in traditional deep learning is the reduction in
model transparency caused by increasingly intricate model functions, which can
lead to problems such as overfitting and excessive confidence in predictions.
With the advent of quantum machine learning offering possible advances in
computational power and latent space complexity, we notice the same opaque
behavior. Despite significant research in classical contexts, there has been
little advancement in addressing the black-box nature of quantum machine
learning. Consequently, we approach this gap by building upon existing work in
classical uncertainty quantification and initial explorations in quantum
Bayesian modeling to theoretically develop and empirically evaluate techniques
to map classical uncertainty quantification methods to the quantum machine
learning domain. Our findings emphasize the necessity of leveraging classical
insights into uncertainty quantification to include uncertainty awareness in
the process of designing new quantum machine learning models.

</details>


### [353] [FedWCM: Unleashing the Potential of Momentum-based Federated Learning in Long-Tailed Scenarios](https://arxiv.org/abs/2507.14980)
*Tianle Li,Yongzhi Huang,Linshan Jiang,Qipeng Xie,Chang Liu,Wenfeng Du,Lu Wang,Kaishun Wu*

Main category: cs.LG

TL;DR: 本文研究了联邦学习在非IID数据分布下的收敛问题，并提出了一种名为FedWCM的改进方法，通过动态调整动量，解决了长尾分布导致的方向性偏差。


<details>
  <summary>Details</summary>
Motivation: 传统动量法在联邦学习的长尾非IID数据分布中效率较低，存在收敛困难的问题，需一种新方法解决。

Method: 提出FedWCM方法，通过对长尾分布数据中的全局与每轮数据进行动态调整动量，纠正方向性偏差。

Result: FedWCM方法有效解决了非收敛问题，实验表明在分类不平衡与用户异构数据场景下性能优越。

Conclusion: FedWCM显著提升了联邦学习在复杂数据分布下的效率与准确性，是一种处理数据不平衡问题的有效方法。

Abstract: Federated Learning (FL) enables decentralized model training while preserving
data privacy. Despite its benefits, FL faces challenges with non-identically
distributed (non-IID) data, especially in long-tailed scenarios with imbalanced
class samples. Momentum-based FL methods, often used to accelerate FL
convergence, struggle with these distributions, resulting in biased models and
making FL hard to converge. To understand this challenge, we conduct extensive
investigations into this phenomenon, accompanied by a layer-wise analysis of
neural network behavior. Based on these insights, we propose FedWCM, a method
that dynamically adjusts momentum using global and per-round data to correct
directional biases introduced by long-tailed distributions. Extensive
experiments show that FedWCM resolves non-convergence issues and outperforms
existing methods, enhancing FL's efficiency and effectiveness in handling
client heterogeneity and data imbalance.

</details>


### [354] [Clustered Federated Learning for Generalizable FDIA Detection in Smart Grids with Heterogeneous Data](https://arxiv.org/abs/2507.14999)
*Yunfeng Li,Junhong Liu,Zhaohui Yang,Guofu Liao,Chuyun Zhang*

Main category: cs.LG

TL;DR: 本文提出一种名为FedClusAvg的隐私保护联邦学习框架，用于在非独立同分布环境下提高智能电网中错误数据注入攻击的检测。


<details>
  <summary>Details</summary>
Motivation: 当前集中式训练方法在应对隐私风险、数据共享限制以及高传输成本方面存在局限性，尤其在数据非独立同分布的情况下，传统模型的泛化能力显著下降。

Method: 提出了FedClusAvg框架，利用基于簇的分层采样和分层通信（客户端-子服务器-主服务器），结合本地化训练与加权参数聚合，提高模型的泛化能力并降低通信开销。

Result: 实验表明，FedClusAvg在异构数据分布下不仅提高了检测准确率，而且显著减少了通信轮次和带宽消耗。

Conclusion: FedClusAvg提供了在大规模分布式电力系统中进行安全高效FDIA检测的有效解决方案。

Abstract: False Data Injection Attacks (FDIAs) pose severe security risks to smart
grids by manipulating measurement data collected from spatially distributed
devices such as SCADA systems and PMUs. These measurements typically exhibit
Non-Independent and Identically Distributed (Non-IID) characteristics across
different regions, which significantly challenges the generalization ability of
detection models. Traditional centralized training approaches not only face
privacy risks and data sharing constraints but also incur high transmission
costs, limiting their scalability and deployment feasibility. To address these
issues, this paper proposes a privacy-preserving federated learning framework,
termed Federated Cluster Average (FedClusAvg), designed to improve FDIA
detection in Non-IID and resource-constrained environments. FedClusAvg
incorporates cluster-based stratified sampling and hierarchical communication
(client-subserver-server) to enhance model generalization and reduce
communication overhead. By enabling localized training and weighted parameter
aggregation, the algorithm achieves accurate model convergence without
centralizing sensitive data. Experimental results on benchmark smart grid
datasets demonstrate that FedClusAvg not only improves detection accuracy under
heterogeneous data distributions but also significantly reduces communication
rounds and bandwidth consumption. This work provides an effective solution for
secure and efficient FDIA detection in large-scale distributed power systems.

</details>


### [355] [Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback](https://arxiv.org/abs/2507.15066)
*Yiyuan Yang,Zichuan Liu,Lei Song,Kai Ying,Zhiguang Wang,Tom Bamford,Svitlana Vyetrenko,Jiang Bian,Qingsong Wen*

Main category: cs.LG

TL;DR: 本文提出了一种新任务Time-RA，通过引入生成和推理的方式提升时间序列异常检测，并提供了首个针对异常推理的多模态数据集RATs40K。


<details>
  <summary>Details</summary>
Motivation: 目前的时间序列异常检测方法通常只进行二分类检测，缺乏细化分类和推理能力。

Method: 引入了一个新的异常推理任务Time-RA，通过利用大规模语言模型（LLMs）将传统的异常检测从判别任务转化为生成和推理任务。同时，构建了一个名为RATs40K的多模态基准数据集，并采用GPT-4驱动的标注框架进行精细标注。

Result: 通过对LLMs和多模态LLMs的广泛基准测试，展示了现有模型的能力与局限，并指出监督微调的重要性。

Conclusion: 新任务Time-RA和数据集RATs40K为可解释的时间序列异常检测和推理的进一步发展提供了基础。

Abstract: Time series anomaly detection is critical across various domains, yet current
approaches often limit analysis to mere binary anomaly classification without
detailed categorization or further explanatory reasoning. To address these
limitations, we propose a novel task, Time-series Reasoning for Anomaly
(Time-RA) that transforms classical time series anomaly detection from a
discriminative into a generative, reasoning-intensive task leveraging Large
Language Models (LLMs). Also, we introduce the first real-world multimodal
benchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,
comprising approximately 40,000 samples across 10 real-world domains. Each
sample includes numeric time series data, contextual text information, and
visual representations, each annotated with fine-grained categories (14 types
for univariate anomalies and 6 for multivariate anomalies) and structured
explanatory reasoning. We develop a sophisticated annotation framework
utilizing ensemble-generated labels refined through GPT-4-driven feedback,
ensuring accuracy and interpretability. Extensive benchmarking of LLMs and
multimodal LLMs demonstrates the capabilities and limitations of current
models, highlighting the critical role of supervised fine-tuning. Our dataset
and task pave the way for significant advancements in interpretable time series
anomaly detection and reasoning.

</details>


### [356] [ROBAD: Robust Adversary-aware Local-Global Attended Bad Actor Detection Sequential Model](https://arxiv.org/abs/2507.15067)
*Bing He,Mustaque Ahamad,Srijan Kumar*

Main category: cs.LG

TL;DR: 这篇文章提出了一种新型Transformer模型ROBAD，用于检测互联网上的恶意用户，其可抵抗对抗性攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习检测模型对输入变化敏感，难以应对对抗性攻击，因此需要开发具备鲁棒性且能够有效检测恶意用户的模型。

Method: 提出ROBAD模型，该模型利用Transformer结构捕捉用户帖子序列中的局部和全局信息，并通过对抗性学习增强模型的知识。

Result: ROBAD在Yelp和Wikipedia数据集上能够在对抗性攻击下有效检测恶意用户。

Conclusion: ROBAD结合局部和全局信息及对抗性学习，提升了模型的鲁棒性，为恶意用户检测提供了有效解决方案。

Abstract: Detecting bad actors is critical to ensure the safety and integrity of
internet platforms. Several deep learning-based models have been developed to
identify such users. These models should not only accurately detect bad actors,
but also be robust against adversarial attacks that aim to evade detection.
However, past deep learning-based detection models do not meet the robustness
requirement because they are sensitive to even minor changes in the input
sequence. To address this issue, we focus on (1) improving the model
understanding capability and (2) enhancing the model knowledge such that the
model can recognize potential input modifications when making predictions. To
achieve these goals, we create a novel transformer-based classification model,
called ROBAD (RObust adversary-aware local-global attended Bad Actor Detection
model), which uses the sequence of user posts to generate user embedding to
detect bad actors. Particularly, ROBAD first leverages the transformer encoder
block to encode each post bidirectionally, thus building a post embedding to
capture the local information at the post level. Next, it adopts the
transformer decoder block to model the sequential pattern in the post
embeddings by using the attention mechanism, which generates the sequence
embedding to obtain the global information at the sequence level. Finally, to
enrich the knowledge of the model, embeddings of modified sequences by mimicked
attackers are fed into a contrastive-learning-enhanced classification layer for
sequence prediction. In essence, by capturing the local and global information
(i.e., the post and sequence information) and leveraging the mimicked behaviors
of bad actors in training, ROBAD can be robust to adversarial attacks.
Extensive experiments on Yelp and Wikipedia datasets show that ROBAD can
effectively detect bad actors when under state-of-the-art adversarial attacks.

</details>


### [357] [Reinforcement Learning for Flow-Matching Policies](https://arxiv.org/abs/2507.15073)
*Samuel Pfrommer,Yixiao Huang,Somayeh Sojoudi*

Main category: cs.LG

TL;DR: 提出了通过强化学习提高流匹配政策性能的方案，并验证其在模拟任务中的显著改进。


<details>
  <summary>Details</summary>
Motivation: 解决当前流匹配政策由次优策略生成示范数据的问题，并提高机器人任务的效率，尤其是最小时间控制的实现。

Method: 提出了两个方法：奖励加权流匹配（RWFM）和基于奖励代理的群组相对策略优化（GRPO），用于训练和优化流匹配政策。

Result: 在模拟的独轮车动力学任务中，这两种方法显著超过了次优示范策略的性能，GRPO方法尤其实现了50%-85%成本的降低。

Conclusion: 通过引入强化学习方法，改进了流匹配政策的性能，为机器人任务的优化提供了一种新途径。

Abstract: Flow-matching policies have emerged as a powerful paradigm for generalist
robotics. These models are trained to imitate an action chunk, conditioned on
sensor observations and textual instructions. Often, training demonstrations
are generated by a suboptimal policy, such as a human operator. This work
explores training flow-matching policies via reinforcement learning to surpass
the original demonstration policy performance. We particularly note
minimum-time control as a key application and present a simple scheme for
variable-horizon flow-matching planning. We then introduce two families of
approaches: a simple Reward-Weighted Flow Matching (RWFM) scheme and a Group
Relative Policy Optimization (GRPO) approach with a learned reward surrogate.
Our policies are trained on an illustrative suite of simulated unicycle
dynamics tasks, and we show that both approaches dramatically improve upon the
suboptimal demonstrator performance, with the GRPO approach in particular
generally incurring between $50\%$ and $85\%$ less cost than a naive Imitation
Learning Flow Matching (ILFM) approach.

</details>


### [358] [Isotonic Quantile Regression Averaging for uncertainty quantification of electricity price forecasts](https://arxiv.org/abs/2507.15079)
*Arkadiusz Lipiecki,Bartosz Uniejewski*

Main category: cs.LG

TL;DR: 研究提出iQRA方法，通过优化点预测集成，提供电力市场更准确可靠的概率预测。


<details>
  <summary>Details</summary>
Motivation: 解决当前电力市场机器学习预测中缺乏不确定性估计的问题，从而帮助决策者规避风险。

Method: 基于QRA框架，引入随机顺序约束改进预测精度、可靠性及计算成本，提出iQRA方法。

Result: 在德国电力市场预测研究中，iQRA在可靠性和预测精度上优于现行方法，提供更精准的预测区间。

Conclusion: iQRA方法通过等温正则化简化回归问题，提高预测能力，同时提供超参数无关的变量选择方案。

Abstract: Quantifying the uncertainty of forecasting models is essential to assess and
mitigate the risks associated with data-driven decisions, especially in
volatile domains such as electricity markets. Machine learning methods can
provide highly accurate electricity price forecasts, critical for informing the
decisions of market participants. However, these models often lack uncertainty
estimates, which limits the ability of decision makers to avoid unnecessary
risks. In this paper, we propose a novel method for generating probabilistic
forecasts from ensembles of point forecasts, called Isotonic Quantile
Regression Averaging (iQRA). Building on the established framework of Quantile
Regression Averaging (QRA), we introduce stochastic order constraints to
improve forecast accuracy, reliability, and computational costs. In an
extensive forecasting study of the German day-ahead electricity market, we show
that iQRA consistently outperforms state-of-the-art postprocessing methods in
terms of both reliability and sharpness. It produces well-calibrated prediction
intervals across multiple confidence levels, providing superior reliability to
all benchmark methods, particularly coverage-based conformal prediction. In
addition, isotonic regularization decreases the complexity of the quantile
regression problem and offers a hyperparameter-free approach to variable
selection.

</details>


### [359] [Robust Control with Gradient Uncertainty](https://arxiv.org/abs/2507.15082)
*Qian Qi*

Main category: cs.LG

TL;DR: 本文提出了一种面向增强学习等领域的不确定性鲁棒控制扩展方法，核心是引入了对价值函数梯度不确定性的处理。


<details>
  <summary>Details</summary>
Motivation: 当前在增强学习等领域中，由于使用值函数近似方法，会存在值函数梯度的内在不确定性，这对控制策略设计带来了挑战。

Method: 将问题建模为零和动态博弈，提出了一种新的非线性偏微分方程（GU-HJBI）。通过黏性解的排序原则，证明方程的数学良定性。

Result: 证明了传统二次值函数假设在存在梯度不确定性时会失效，提出并验证了非多项式校正值函数及其控制策略的非线性。还提出了GURAC算法，并通过实验验证其在训练稳定性方面的效果。

Conclusion: 该研究扩展了鲁棒控制理论，为处理增强学习等领域的值函数梯度不确定性提供了新路径，在理论和实践中均具有重要意义。

Abstract: We introduce a novel extension to robust control theory that explicitly
addresses uncertainty in the value function's gradient, a form of uncertainty
endemic to applications like reinforcement learning where value functions are
approximated. We formulate a zero-sum dynamic game where an adversary perturbs
both system dynamics and the value function gradient, leading to a new, highly
nonlinear partial differential equation: the Hamilton-Jacobi-Bellman-Isaacs
Equation with Gradient Uncertainty (GU-HJBI). We establish its well-posedness
by proving a comparison principle for its viscosity solutions under a uniform
ellipticity condition. Our analysis of the linear-quadratic (LQ) case yields a
key insight: we prove that the classical quadratic value function assumption
fails for any non-zero gradient uncertainty, fundamentally altering the problem
structure. A formal perturbation analysis characterizes the non-polynomial
correction to the value function and the resulting nonlinearity of the optimal
control law, which we validate with numerical studies. Finally, we bridge
theory to practice by proposing a novel Gradient-Uncertainty-Robust
Actor-Critic (GURAC) algorithm, accompanied by an empirical study demonstrating
its effectiveness in stabilizing training. This work provides a new direction
for robust control, holding significant implications for fields where function
approximation is common, including reinforcement learning and computational
finance.

</details>


### [360] [AnalogFed: Federated Discovery of Analog Circuit Topologies with Generative AI](https://arxiv.org/abs/2507.15104)
*Qiufeng Li,Shu Hong,Jian Gao,Xuan Zhang,Tian Lan,Weidong Cao*

Main category: cs.LG

TL;DR: 该论文提出AnalogFed，通过联邦学习实现跨不同研究者和机构的模拟电路拓扑发现，而无需共享私人数据。


<details>
  <summary>Details</summary>
Motivation: 模拟电路设计非常专有，数据资源分散阻碍了生成式AI在新电路拓扑发现中的应用与进步。

Method: 提出一种名为AnalogFed的联邦学习架构，结合生成模型开发、数据异质性处理和隐私保护技术，支持分布式协作。

Result: 实验结果显示，AnalogFed在隐私保护前提下实现了接近集中式方法的性能，并在拓扑设计方面展现了优秀的效率与可扩展性。

Conclusion: AnalogFed实现了隐私与性能的平衡，展示了生成式AI在模拟设计中的潜力，推动了跨组织的协作创新。

Abstract: Recent breakthroughs in AI/ML offer exciting opportunities to revolutionize
analog design automation through data-driven approaches. In particular,
researchers are increasingly fascinated by harnessing the power of generative
AI to automate the discovery of novel analog circuit topologies. Unlocking the
full potential of generative AI in these data-driven discoveries requires
access to large and diverse datasets.Yet, there is a significant barrier in the
analog domain--Analog circuit design is inherently proprietary, involving not
only confidential circuit structures but also the underlying commercial
semiconductor processes. As a result, current generative AI research is largely
confined to individual researchers who construct small, narrowly focused
private datasets. This fragmentation severely limits collaborative innovation
and impedes progress across the research community. To address these
challenges, we propose AnalogFed. AnalogFed enables collaborative topology
discovery across decentralized clients (e.g., individual researchers or
institutions) without requiring the sharing of raw private data. To make this
vision practical, we introduce a suite of techniques tailored to the unique
challenges of applying FedL in analog design--from generative model development
and data heterogeneity handling to privacy-preserving strategies that ensure
both flexibility and security for circuit designers and semiconductor
manufacturers. Extensive experiments across varying client counts and dataset
sizes demonstrate that AnalogFed achieves performance comparable to centralized
baselines--while maintaining strict data privacy. Specifically, the generative
AI model within AnalogFed achieves state-of-the-art efficiency and scalability
in the design of analog circuit topologies.

</details>


### [361] [Distributional Unlearning: Forgetting Distributions, Not Just Samples](https://arxiv.org/abs/2507.15112)
*Youssef Allouah,Rachid Guerraoui,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 本文提出了一种名为分布式遗忘的框架，用于从模型中移除不需要的信息，通过优化数据样本的移除，减少对目标数据域的残留信号，同时保留其他数据域的性能。


<details>
  <summary>Details</summary>
Motivation: 当前机器遗忘工具主要针对单一样本，但在实际应用中，为满足法律、隐私或质量需求，往往需要删除整个主题领域的数据。现有方法删除数据后仍可能残留信号使得模型可以恢复不需要的领域，因此需要开发一种能够有效删除整个分布的工具。

Method: 提出了一种基于数据分布的模型无关型框架，即分布式遗忘，通过计算Kullback-Leibler散度来衡量移除和保留程度，并设计了一种距离选择规则以满足优化的删除性能。

Result: 在多个实验中（如合成高斯分布数据、评论数据集、短信垃圾邮件和CIFAR-10），该方法比随机删除减少15-72%的删除量，同时对保留数据的性能几乎没有影响。

Conclusion: 分布式遗忘有效减少了删除数据的数量，同时维持了保留域的性能，为满足技术及法律需求的广泛应用场景提供了一种数据高效的方法。

Abstract: Machine unlearning seeks to remove unwanted information from trained models,
initially at the individual-sample level, but increasingly at the level of
entire sub-populations. In many deployments, models must delete whole topical
domains to satisfy privacy, legal, or quality requirements, e.g., removing
several users' posts under GDPR or copyrighted web content. Existing unlearning
tools remain largely sample-oriented, and straightforward point deletion often
leaves enough residual signal for downstream learners to recover the unwanted
domain. We introduce distributional unlearning, a data-centric, model-agnostic
framework that asks: Given examples from an unwanted distribution and a
retained distribution, what is the smallest set of points whose removal makes
the edited dataset far from the unwanted domain yet close to the retained one?
Using Kullback-Leibler divergence to quantify removal and preservation, we
derive the exact Pareto frontier in the Gaussian case and prove that any model
retrained on the edited data incurs log-loss shifts bounded by the divergence
thresholds. We propose a simple distance-based selection rule satisfying these
constraints with a quadratic reduction in deletion budget compared to random
removal. Experiments on synthetic Gaussians, Jigsaw Toxic Comments, SMS spam,
and CIFAR-10 show 15-72% fewer deletions than random, with negligible impact on
retained performance.

</details>


### [362] [Are We Overlooking the Dimensions? Learning Latent Hierarchical Channel Structure for High-Dimensional Time Series Forecasting](https://arxiv.org/abs/2507.15119)
*Juntong Ni,Shiyu Wang,Zewen Liu,Xiaoming Shi,Xinyue Zhong,Zhou Ye,Wei Jin*

Main category: cs.LG

TL;DR: 提出了一种名为U-Cast的新型结构，用于高维时间序列预测，并构建了相关数据集Time-HD。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测方法无法有效处理高维度的频道交互和复杂的层次化相关性。

Method: 提出了U-Cast架构，结合了基于查询的注意力机制来学习隐性频道结构，并在训练中加入全秩正则化以分离高度相关的频道表示。

Result: 实验结果表明，U-Cast在准确性和效率上优于强基线模型，同时构建的Time-HD数据集成为了评估的基准。

Conclusion: U-Cast与Time-HD的数据集为高维时间序列预测研究奠定了坚实基础。

Abstract: Time series forecasting (TSF) is a central problem in time series analysis.
However, as the number of channels in time series datasets scales to the
thousands or more, a scenario we define as High-Dimensional Time Series
Forecasting (HDTSF), it introduces significant new modeling challenges that are
often not the primary focus of traditional TSF research. HDTSF is challenging
because the channel correlation often forms complex and hierarchical patterns.
Existing TSF models either ignore these interactions or fail to scale as
dimensionality grows. To address this issue, we propose U-Cast, a
channel-dependent forecasting architecture that learns latent hierarchical
channel structures with an innovative query-based attention. To disentangle
highly correlated channel representation, U-Cast adds a full-rank
regularization during training. We also release Time-HD, a benchmark of large,
diverse, high-dimensional datasets. Our theory shows that exploiting
cross-channel information lowers forecasting risk, and experiments on Time-HD
demonstrate that U-Cast surpasses strong baselines in both accuracy and
efficiency. Together, U-Cast and Time-HD provide a solid basis for future HDTSF
research.

</details>


### [363] [Constraint-aware Learning of Probabilistic Sequential Models for Multi-Label Classification](https://arxiv.org/abs/2507.15156)
*Mykhailo Buleshnyi,Anna Polova,Zsolt Zombori,Michael Benedikt*

Main category: cs.LG

TL;DR: 研究一种在多标签分类中结合逻辑约束的网络架构，通过结合独立标签分类器与复杂的序列模型实现联合分布预测。


<details>
  <summary>Details</summary>
Motivation: 解决在大型标签集合的多标签分类中，如何利用已知逻辑约束建模标签间相关性和提高性能的问题。

Method: 将各独立标签分类器的输出输入到一个复杂的序列模型中，构建联合分布；同时在训练和推断时利用逻辑约束条件提升模型表现。

Result: 实验验证了该网络架构在训练中利用和推断时执行逻辑约束的能力。

Conclusion: 通过结合逻辑约束与序列模型，提出了一种能够建模标签相关性并提升多标签分类效果的方法。

Abstract: We investigate multi-label classification involving large sets of labels,
where the output labels may be known to satisfy some logical constraints. We
look at an architecture in which classifiers for individual labels are fed into
an expressive sequential model, which produces a joint distribution. One of the
potential advantages for such an expressive model is its ability to modelling
correlations, as can arise from constraints. We empirically demonstrate the
ability of the architecture both to exploit constraints in training and to
enforce constraints at inference time.

</details>


### [364] [Resonant-Tunnelling Diode Reservoir Computing System for Image Recognition](https://arxiv.org/abs/2507.15158)
*A. H. Abbas,Hend Abdel-Ghani,Ivan S. Maksymov*

Main category: cs.LG

TL;DR: 研究提出采用基于共振隧道二极管（RTD）的神经形态计算架构，用于物理储备计算，并在图像识别任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在实时、边缘计算和资源受限环境中的应用增加，需要新型的硬件高效计算模型。

Method: 设计了一种基于RTD的物理储备计算体系结构，理论上进行了公式化，并实施了数值实验。用手写数字分类和Fruit~360数据集进行图像识别任务测试。

Result: 结果显示，该架构在两项图像识别任务中表现出良好的性能，同时符合新一代储备计算的原则——通过确定性的非线性变换替代随机连接。

Conclusion: 基于RTD的神经形态计算架构是一种硬件高效的储备计算模型，适用于资源受限环境的实时任务。

Abstract: As artificial intelligence continues to push into real-time, edge-based and
resource-constrained environments, there is an urgent need for novel,
hardware-efficient computational models. In this study, we present and validate
a neuromorphic computing architecture based on resonant-tunnelling diodes
(RTDs), which exhibit the nonlinear characteristics ideal for physical
reservoir computing (RC). We theoretically formulate and numerically implement
an RTD-based RC system and demonstrate its effectiveness on two image
recognition benchmarks: handwritten digit classification and object recognition
using the Fruit~360 dataset. Our results show that this circuit-level
architecture delivers promising performance while adhering to the principles of
next-generation RC -- eliminating random connectivity in favour of a
deterministic nonlinear transformation of input signals.

</details>


### [365] [Designing User-Centric Metrics for Evaluation of Counterfactual Explanations](https://arxiv.org/abs/2507.15162)
*Firdaus Ahmed Choudhury,Ethan Leicht,Jude Ethan Bislig,Hangzhi Guo,Amulya Yadav*

Main category: cs.LG

TL;DR: 本研究提出一种新模型以改进与用户偏好更贴合的反事实解释（CFEs），解决现有评估指标与用户偏好不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型的决策缺乏透明性，用户难以理解其决策背后的原因，且现有反事实解释（CFEs）的评估指标难以充分体现用户偏好与实际约束。

Method: 通过两项用户研究寻找和验证用户偏好与现有反事实解释方法的对齐情况，并基于研究发现提出了一种新的人本中心两阶段模型AWP，用于预测用户偏好的CFEs。

Result: 用户研究表明，现有的CF评估指标与用户偏好的对齐率仅为63.81%，而提出的AWP模型能以84.37%的准确率预测用户偏好。

Conclusion: 该研究首次基于人类的验证确定反事实解释生成中的个性化成本模型，并强调开发适应性强、用户中心的评估指标的重要性。

Abstract: Machine learning-based decision models are increasingly being used to make
decisions that significantly impact people's lives, but their opaque nature
leaves end users without a clear understanding of why a decision was made.
Counterfactual Explanations (CFEs) have grown in popularity as a means of
offering actionable guidance by identifying the minimum changes in feature
values required to flip a model's prediction to something more desirable.
Unfortunately, most prior research in CFEs relies on artificial evaluation
metrics, such as proximity, which may overlook end-user preferences and
constraints, e.g., the user's perception of effort needed to make certain
feature changes may differ from that of the model designer. To address this
research gap, this paper makes three novel contributions. First, we conduct a
pilot study with 20 crowd-workers on Amazon MTurk to experimentally validate
the alignment of existing CF evaluation metrics with real-world user
preferences. Results show that user-preferred CFEs matched those based on
proximity in only 63.81% of cases, highlighting the limited applicability of
these metrics in real-world settings. Second, inspired by the need to design a
user-informed evaluation metric for CFEs, we conduct a more detailed two-day
user study with 41 participants facing realistic credit application scenarios
to find experimental support for or against three intuitive hypotheses that may
explain how end users evaluate CFEs. Third, based on the findings of this
second study, we propose the AWP model, a novel user-centric, two-stage model
that describes one possible mechanism by which users evaluate and select CFEs.
Our results show that AWP predicts user-preferred CFEs with 84.37% accuracy.
Our study provides the first human-centered validation for personalized cost
models in CFE generation and highlights the need for adaptive, user-centered
evaluation metrics.

</details>


### [366] [Better Models and Algorithms for Learning Ising Models from Dynamics](https://arxiv.org/abs/2507.15173)
*Jason Gaitonde,Ankur Moitra,Elchanan Mossel*

Main category: cs.LG

TL;DR: 研究有效学习真实环境中结构和参数；提出首个在更弱观测模型下高效学习算法，对具有最大度数d的Ising模型，依赖图在时间$\mathsf{poly}(d)\cdot n^2\log n$内恢复，参数则需额外$\widetilde{O}(2^d n)$时间，泛化适用于多种单点可逆马尔可夫链。


<details>
  <summary>Details</summary>
Motivation: 解决目前仅能观察配置更新时的模型学习问题，突破限制并适配更弱的观测条件。

Method: 通过设计算法，首次在仅能观察配置改变的观测条件下学习Ising模型；结合马尔科夫链的更强健性性质，包括对Metropolis链的扩展。

Result: 提出算法能够有效恢复Ising模型的依赖图和参数，其效率与i.i.d设置下最新算法的效率相当。

Conclusion: 在弱观测条件下，本研究成功展示了算法可行性，为实际应用提供新可能性。

Abstract: We study the problem of learning the structure and parameters of the Ising
model, a fundamental model of high-dimensional data, when observing the
evolution of an associated Markov chain. A recent line of work has studied the
natural problem of learning when observing an evolution of the well-known
Glauber dynamics [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,
Gaitonde, Mossel STOC 2024], which provides an arguably more realistic
generative model than the classical i.i.d. setting. However, this prior work
crucially assumes that all site update attempts are observed, \emph{even when
this attempt does not change the configuration}: this strong observation model
is seemingly essential for these approaches. While perhaps possible in
restrictive contexts, this precludes applicability to most realistic settings
where we can observe \emph{only} the stochastic evolution itself, a minimal and
natural assumption for any process we might hope to learn from. However,
designing algorithms that succeed in this more realistic setting has remained
an open problem [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,
Gaitonde, Moitra, Mossel, STOC 2025].
  In this work, we give the first algorithms that efficiently learn the Ising
model in this much more natural observation model that only observes when the
configuration changes. For Ising models with maximum degree $d$, our algorithm
recovers the underlying dependency graph in time $\mathsf{poly}(d)\cdot n^2\log
n$ and then the actual parameters in additional $\widetilde{O}(2^d n)$ time,
which qualitatively matches the state-of-the-art even in the i.i.d. setting in
a much weaker observation model. Our analysis holds more generally for a
broader class of reversible, single-site Markov chains that also includes the
popular Metropolis chain by leveraging more robust properties of reversible
Markov chains.

</details>


### [367] [Joint-Local Grounded Action Transformation for Sim-to-Real Transfer in Multi-Agent Traffic Control](https://arxiv.org/abs/2507.15174)
*Justin Turnau,Longchao Da,Khoa Vo,Ferdous Al Rafi,Shreyas Bachiraju,Tiejin Chen,Hua Wei*

Main category: cs.LG

TL;DR: 本文提出了JL-GAT，将Grounded Action Transformation (GAT)方法应用于多智能体强化学习(MARL)的交通信号控制（TSC）问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 交通信号控制对于管理城市交通流量至关重要，但在现实环境中，基于MARL的TSC策略往往受到模拟与现实差距的影响，导致性能下降。本文的目标是通过改进技术解决这一问题。

Method: 提出一种去中心化的GAT方法，称为JL-GAT，该方法将邻近智能体的信息整合到模型中，在实现现实交通网络所需的可扩展性的同时，捕捉智能体之间的关键交互。

Result: 通过在各种道路网络和模拟的恶劣天气条件下的综合实验以及消融研究，验证了JL-GAT的有效性。

Conclusion: JL-GAT在平衡可扩展性和增强模型对现实交通动态适应能力方面表现突出，为解决MARL在交通信号控制中的模拟至现实应用提供了有效手段。

Abstract: Traffic Signal Control (TSC) is essential for managing urban traffic flow and
reducing congestion. Reinforcement Learning (RL) offers an adaptive method for
TSC by responding to dynamic traffic patterns, with multi-agent RL (MARL)
gaining traction as intersections naturally function as coordinated agents.
However, due to shifts in environmental dynamics, implementing MARL-based TSC
policies in the real world often leads to a significant performance drop, known
as the sim-to-real gap. Grounded Action Transformation (GAT) has successfully
mitigated this gap in single-agent RL for TSC, but real-world traffic networks,
which involve numerous interacting intersections, are better suited to a MARL
framework. In this work, we introduce JL-GAT, an application of GAT to
MARL-based TSC that balances scalability with enhanced grounding capability by
incorporating information from neighboring agents. JL-GAT adopts a
decentralized approach to GAT, allowing for the scalability often required in
real-world traffic networks while still capturing key interactions between
agents. Comprehensive experiments on various road networks under simulated
adverse weather conditions, along with ablation studies, demonstrate the
effectiveness of JL-GAT. The code is publicly available at
https://github.com/DaRL-LibSignal/JL-GAT/.

</details>


### [368] [Feature Construction Using Network Control Theory and Rank Encoding for Graph Machine Learning](https://arxiv.org/abs/2507.15195)
*Anwar Said,Yifan Wei,Ubaid Ullah Ahmad,Mudassir Shabbir,Waseem Abbas,Xenofon Koutsoukos*

Main category: cs.LG

TL;DR: 本文提出利用图中的平均可控性和一种新的等级编码方法，提升图神经网络（GNNs）在社交网络分类任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决社交网络中由于节点特征缺失或隐私限制而导致的GNN性能受限问题。

Method: 提出两种构造节点特征的方法：1) 利用平均可控性和中心性度量构建节点级特征；2) 开发一种新的等级编码方法，将图论度量转化为固定维度特征空间。

Result: 实验显示，平均可控性显著提升了GNN性能。新提出的等级编码方法优于传统的单热编码，将GitHub Stargazers数据集上的GraphSAGE模型的ROC AUC从68.7%提升至73.9%。

Conclusion: 将图论度量（如平均可控性）与等级编码相结合，可有效提升GNN在社交网络任务中的表现。

Abstract: In this article, we utilize the concept of average controllability in graphs,
along with a novel rank encoding method, to enhance the performance of Graph
Neural Networks (GNNs) in social network classification tasks. GNNs have proven
highly effective in various network-based learning applications and require
some form of node features to function. However, their performance is heavily
influenced by the expressiveness of these features. In social networks, node
features are often unavailable due to privacy constraints or the absence of
inherent attributes, making it challenging for GNNs to achieve optimal
performance. To address this limitation, we propose two strategies for
constructing expressive node features. First, we introduce average
controllability along with other centrality metrics (denoted as NCT-EFA) as
node-level metrics that capture critical aspects of network topology. Building
on this, we develop a rank encoding method that transforms average
controllability or any other graph-theoretic metric into a fixed-dimensional
feature space, thereby improving feature representation. We conduct extensive
numerical evaluations using six benchmark GNN models across four social network
datasets to compare different node feature construction methods. Our results
demonstrate that incorporating average controllability into the feature space
significantly improves GNN performance. Moreover, the proposed rank encoding
method outperforms traditional one-hot degree encoding, improving the ROC AUC
from 68.7% to 73.9% using GraphSAGE on the GitHub Stargazers dataset,
underscoring its effectiveness in generating expressive and efficient node
representations.

</details>


### [369] [Long-Short Distance Graph Neural Networks and Improved Curriculum Learning for Emotion Recognition in Conversation](https://arxiv.org/abs/2507.15205)
*Xinran Li,Xiujuan Xu,Jiaqi Qiao*

Main category: cs.LG

TL;DR: 提出了一种名为LSDGNN的多模态图神经网络方法，用于对话中的情感识别，具有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决对话情感识别中挑战性的多模态特征捕获和数据不平衡问题。

Method: 构建基于有向无环图的长短距离图神经网络，结合差异化正则项和双仿射模块以增强特征交互，提出改进的课程学习（ICL）处理数据不平衡。

Result: 实验表明，在IEMOCAP和MELD数据集上，该模型优于现有的基准方法。

Conclusion: LSDGNN方法有效捕获远近对话特征，并通过改进的课程学习提升模型对数据不平衡的鲁棒性，为对话情感识别提供了新思路。

Abstract: Emotion Recognition in Conversation (ERC) is a practical and challenging
task. This paper proposes a novel multimodal approach, the Long-Short Distance
Graph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it
constructs a long-distance graph neural network and a short-distance graph
neural network to obtain multimodal features of distant and nearby utterances,
respectively. To ensure that long- and short-distance features are as distinct
as possible in representation while enabling mutual influence between the two
modules, we employ a Differential Regularizer and incorporate a BiAffine Module
to facilitate feature interaction. In addition, we propose an Improved
Curriculum Learning (ICL) to address the challenge of data imbalance. By
computing the similarity between different emotions to emphasize the shifts in
similar emotions, we design a "weighted emotional shift" metric and develop a
difficulty measurer, enabling a training process that prioritizes learning easy
samples before harder ones. Experimental results on the IEMOCAP and MELD
datasets demonstrate that our model outperforms existing benchmarks.

</details>


### [370] [Exact Reformulation and Optimization for Direct Metric Optimization in Binary Imbalanced Classification](https://arxiv.org/abs/2507.15240)
*Le Peng,Yash Travadi,Chuan He,Ying Cui,Ju Sun*

Main category: cs.LG

TL;DR: 本文研究了在类别不平衡的二分类问题(IC)中，优化精确率和召回率的直接度量优化(DMO)问题，提出了全新的精确约束重构方法，并验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 在类别不平衡分类场景中，传统方法的准确性指标可能会误导，而现有方法通常优化平衡准确率，难以应对特定类别重要性不同或某些指标需要达到固定水平的情况。

Method: 本文通过提出精确约束重构方法，利用精确惩罚法来直接优化精确率和召回率等关键分类指标，代替以往依赖平滑近似的方法。

Result: 通过在多个基准数据集上的实验，验证了本文方法在三种DMO问题(FPOR、FROP、OFBS)中的优越性。

Conclusion: 本文的方法显著优于现有技术，并且提出的精确重构和优化框架(ERO)有望推广到二分类IC及其他领域。

Abstract: For classification with imbalanced class frequencies, i.e., imbalanced
classification (IC), standard accuracy is known to be misleading as a
performance measure. While most existing methods for IC resort to optimizing
balanced accuracy (i.e., the average of class-wise recalls), they fall short in
scenarios where the significance of classes varies or certain metrics should
reach prescribed levels. In this paper, we study two key classification
metrics, precision and recall, under three practical binary IC settings: fix
precision optimize recall (FPOR), fix recall optimize precision (FROP), and
optimize $F_\beta$-score (OFBS). Unlike existing methods that rely on smooth
approximations to deal with the indicator function involved, \textit{we
introduce, for the first time, exact constrained reformulations for these
direct metric optimization (DMO) problems}, which can be effectively solved by
exact penalty methods. Experiment results on multiple benchmark datasets
demonstrate the practical superiority of our approach over the state-of-the-art
methods for the three DMO problems. We also expect our exact reformulation and
optimization (ERO) framework to be applicable to a wide range of DMO problems
for binary IC and beyond. Our code is available at
https://github.com/sun-umn/DMO.

</details>


### [371] [Spatio-Temporal Demand Prediction for Food Delivery Using Attention-Driven Graph Neural Networks](https://arxiv.org/abs/2507.15246)
*Rabia Latief Bhat,Iqra Altaf Gillani*

Main category: cs.LG

TL;DR: 本研究提出了一种基于注意力机制的图神经网络，用于城市食品配送中高精度需求预测。


<details>
  <summary>Details</summary>
Motivation: 解决食品配送中需求预测的难题，优化操作决策。

Method: 构建基于注意力的图神经网络模型，利用空间时序相关性来预测需求；节点表示区域，边表示空间和订单流动关系。

Result: 实验结果表明，该模型在真实食品配送数据上表现优异，预测准确性高。

Conclusion: 模型为城市食品配送中的车队定位、资源分配和调度优化提供了高效、可扩展的解决方案。

Abstract: Accurate demand forecasting is critical for enhancing the efficiency and
responsiveness of food delivery platforms, where spatial heterogeneity and
temporal fluctuations in order volumes directly influence operational
decisions. This paper proposes an attention-based Graph Neural Network
framework that captures spatial-temporal dependencies by modeling the food
delivery environment as a graph. In this graph, nodes represent urban delivery
zones, while edges reflect spatial proximity and inter-regional order flow
patterns derived from historical data. The attention mechanism dynamically
weighs the influence of neighboring zones, enabling the model to focus on the
most contextually relevant areas during prediction. Temporal trends are jointly
learned alongside spatial interactions, allowing the model to adapt to evolving
demand patterns. Extensive experiments on real-world food delivery datasets
demonstrate the superiority of the proposed model in forecasting future order
volumes with high accuracy. The framework offers a scalable and adaptive
solution to support proactive fleet positioning, resource allocation, and
dispatch optimization in urban food delivery operations.

</details>


### [372] [CHORDS: Diffusion Sampling Accelerator with Multi-core Hierarchical ODE Solvers](https://arxiv.org/abs/2507.15260)
*Jiaqi Han,Haotian Ye,Puheng Li,Minkai Xu,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: 提出了一种名为CHORDS的新型加速框架，通过多核并行加速扩散模型采样，同时保证结果质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成质量高，但耗时长，而现有加速方法大多需要重新训练模型或牺牲质量，亟需一种无需训练且通用的加速方法。

Method: 将多核扩散采样视为ODE求解器流水线，设计一种理论支持的核间通信机制，使较慢求解器逐步纠正较快求解器的结果，并应用于CHORDS框架。

Result: 在多种大规模图像和视频扩散模型中，CHORDS实现了显著加速，四核加速可达2.1倍，八核加速达到2.9倍，且无质量损失。

Conclusion: CHORDS奠定了实时、高保真扩散生成的坚实基础，多核并行加速方案展示了广泛的适用性和潜力。

Abstract: Diffusion-based generative models have become dominant generators of
high-fidelity images and videos but remain limited by their computationally
expensive inference procedures. Existing acceleration techniques either require
extensive model retraining or compromise significantly on sample quality. This
paper explores a general, training-free, and model-agnostic acceleration
strategy via multi-core parallelism. Our framework views multi-core diffusion
sampling as an ODE solver pipeline, where slower yet accurate solvers
progressively rectify faster solvers through a theoretically justified
inter-core communication mechanism. This motivates our multi-core training-free
diffusion sampling accelerator, CHORDS, which is compatible with various
diffusion samplers, model architectures, and modalities. Through extensive
experiments, CHORDS significantly accelerates sampling across diverse
large-scale image and video diffusion models, yielding up to 2.1x speedup with
four cores, improving by 50% over baselines, and 2.9x speedup with eight cores,
all without quality degradation. This advancement enables CHORDS to establish a
solid foundation for real-time, high-fidelity diffusion generation.

</details>


### [373] [Temporal Basis Function Models for Closed-Loop Neural Stimulation](https://arxiv.org/abs/2507.15274)
*Matthew J. Bryan,Felix Schwock,Azadeh Yazdan-Shahmorad,Rajesh P N Rao*

Main category: cs.LG

TL;DR: 提出了时间基函数模型（TBFMs）用于闭环神经刺激系统，展示了其在快速训练、低时延和样本有效性上的优越性能，并成功预测和控制神经活动。


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能技术能否为神经疾病如帕金森病的闭环刺激提供个性化治疗或发掘新疗法，同时解决样本效率、训练时间及系统响应时延等翻译性问题。

Method: 提出时间基函数模型（TBFMs），通过研究兴奋性光遗传刺激的单次实验和模拟实验，预测局部场电位（LFPs）变化并驱动神经活动向目标模式靠拢。

Result: TBFMs实现了高效的单次实验预测，在40个实验记录中，数据收集时间为15-20分钟，训练时间为2-4分钟，预测精度与非线性动力系统模型相当，且优于线性状态空间模型。

Conclusion: TBFMs克服了现有动力系统建模方法的弊端，其快速、精确的预测性能为未来开发新型临床用闭环刺激协议铺平了道路。

Abstract: Closed-loop neural stimulation provides novel therapies for neurological
diseases such as Parkinson's disease (PD), but it is not yet clear whether
artificial intelligence (AI) techniques can tailor closed-loop stimulation to
individual patients or identify new therapies. Progress requires us to address
a number of translational issues, including sample efficiency, training time,
and minimizing loop latency such that stimulation may be shaped in response to
changing brain activity. We propose temporal basis function models (TBFMs) to
address these difficulties, and explore this approach in the context of
excitatory optogenetic stimulation. We demonstrate the ability of TBF models to
provide a single-trial, spatiotemporal forward prediction of the effect of
optogenetic stimulation on local field potentials (LFPs) measured in two
non-human primates. We further use simulations to demonstrate the use of TBF
models for closed-loop stimulation, driving neural activity towards target
patterns. The simplicity of TBF models allow them to be sample efficient, rapid
to train (2-4min), and low latency (0.2ms) on desktop CPUs. We demonstrate the
model on 40 sessions of previously published excitatory optogenetic stimulation
data. For each session, the model required 15-20min of data collection to
successfully model the remainder of the session. It achieved a prediction
accuracy comparable to a baseline nonlinear dynamical systems model that
requires hours to train, and superior accuracy to a linear state-space model.
In our simulations, it also successfully allowed a closed-loop stimulator to
control a neural circuit. Our approach begins to bridge the translational gap
between complex AI-based approaches to modeling dynamical systems and the
vision of using such forward prediction models to develop novel, clinically
useful closed-loop stimulation protocols.

</details>


### [374] [Machine Unlearning for Streaming Forgetting](https://arxiv.org/abs/2507.15280)
*Shaofei Shen,Chenhao Zhang,Yawen Zhao,Alina Bialkowski,Weitong Chen,Miao Xu*

Main category: cs.LG

TL;DR: 本文提出一种新的流式遗忘算法，用于解决分布变化下的机器“遗忘”问题，从而在无需访问原始训练数据情况下提高效率与性能。


<details>
  <summary>Details</summary>
Motivation: 现有的机器遗忘方法通常一次性地处理需忘记的数据，但在实际应用中，遗忘请求往往以流式方式出现，现有办法在此情景下效率较低，且性能不足。

Method: 作者提出了一种流式遗忘范式，将遗忘视为分布转变问题，通过估计改变的分布，设计了一种新的流式遗忘算法，实现无需访问原始训练数据的有效流式遗忘。

Result: 理论分析证明了流式遗忘算法的遗忘误差界，并在多个模型和数据集上进行了实验，验证了方法性能。

Conclusion: 本文在流式数据遗忘领域实现了理论与实际的结合，同时克服传统机器遗忘方法的局限，提高了在流式情景中效率与精度，对于实际应用有重要意义。

Abstract: Machine unlearning aims to remove knowledge of the specific training data in
a well-trained model. Currently, machine unlearning methods typically handle
all forgetting data in a single batch, removing the corresponding knowledge all
at once upon request. However, in practical scenarios, requests for data
removal often arise in a streaming manner rather than in a single batch,
leading to reduced efficiency and effectiveness in existing methods. Such
challenges of streaming forgetting have not been the focus of much research. In
this paper, to address the challenges of performance maintenance, efficiency,
and data access brought about by streaming unlearning requests, we introduce a
streaming unlearning paradigm, formalizing the unlearning as a distribution
shift problem. We then estimate the altered distribution and propose a novel
streaming unlearning algorithm to achieve efficient streaming forgetting
without requiring access to the original training data. Theoretical analyses
confirm an $O(\sqrt{T} + V_T)$ error bound on the streaming unlearning regret,
where $V_T$ represents the cumulative total variation in the optimal solution
over $T$ learning rounds. This theoretical guarantee is achieved under mild
conditions without the strong restriction of convex loss function. Experiments
across various models and datasets validate the performance of our proposed
method.

</details>


### [375] [Mixture of Autoencoder Experts Guidance using Unlabeled and Incomplete Data for Exploration in Reinforcement Learning](https://arxiv.org/abs/2507.15287)
*Elias Malomgré,Pieter Simoens*

Main category: cs.LG

TL;DR: 提出一个框架，可以将不完整和不完美的专家示例转化为内在奖励，用于强化学习中的灵活探索和行为指导。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖显式奖励最大化，面临无奖励或者高维状态空间的复杂情形时存在挑战。实际应用中需要从无奖励交互或不完整示例中学习，开发通用智能体以适应现实环境。

Method: 通过将智能体状态与专家数据的相似性转化为内在奖励，结合Autoencoder Experts模型捕捉行为多样性及应对示例信息不完整问题，实现内在动机驱动的灵活探索。

Result: 实验表明，即使在稀疏或不完整的示例环境中，方法可以实现鲁棒探索，并在稀疏和密集奖励环境中表现出色。

Conclusion: 该框架在缺乏最佳数据和需要精确奖励控制的现实情境中，为强化学习提供了一个实用的解决方案。

Abstract: Recent trends in Reinforcement Learning (RL) highlight the need for agents to
learn from reward-free interactions and alternative supervision signals, such
as unlabeled or incomplete demonstrations, rather than relying solely on
explicit reward maximization. Additionally, developing generalist agents that
can adapt efficiently in real-world environments often requires leveraging
these reward-free signals to guide learning and behavior. However, while
intrinsic motivation techniques provide a means for agents to seek out novel or
uncertain states in the absence of explicit rewards, they are often challenged
by dense reward environments or the complexity of high-dimensional state and
action spaces. Furthermore, most existing approaches rely directly on the
unprocessed intrinsic reward signals, which can make it difficult to shape or
control the agent's exploration effectively. We propose a framework that can
effectively utilize expert demonstrations, even when they are incomplete and
imperfect. By applying a mapping function to transform the similarity between
an agent's state and expert data into a shaped intrinsic reward, our method
allows for flexible and targeted exploration of expert-like behaviors. We
employ a Mixture of Autoencoder Experts to capture a diverse range of behaviors
and accommodate missing information in demonstrations. Experiments show our
approach enables robust exploration and strong performance in both sparse and
dense reward environments, even when demonstrations are sparse or incomplete.
This provides a practical framework for RL in realistic settings where optimal
data is unavailable and precise reward control is needed.

</details>


### [376] [Preferential subspace identification (PSID) with forward-backward smoothing](https://arxiv.org/abs/2507.15288)
*Omid G. Sani,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 本文介绍了一种扩展偏好子空间识别(PSID)的方法，加入了对多变量时间序列中滤波和平滑的优化建模能力。


<details>
  <summary>Details</summary>
Motivation: 目前的PSID方法主要关心使用过去的主要信号数据进行预测，但在离线场景中使用并发数据或所有数据可以提高估计性能。

Method: 通过加入一个降维回归步骤来扩展PSID, 实现优化的Kalman滤波更新; 借鉴双滤波器Kalman平滑器的方法，开发正向和反向的PSID平滑算法。

Result: 算法在模拟数据上验证，展示出其能恢复滤波所需的模型参数并达成理想的滤波及平滑解码性能，匹配真实模型性能。

Conclusion: 提出了一种针对双信号场景的优化线性滤波和平滑框架，扩展了分析多变量时间序列动态交互的工具集。

Abstract: System identification methods for multivariate time-series, such as neural
and behavioral recordings, have been used to build models for predicting one
from the other. For example, Preferential Subspace Identification (PSID) builds
a state-space model of a primary time-series (e.g., neural activity) to
optimally predict a secondary time-series (e.g., behavior). However, PSID
focuses on optimal prediction using past primary data, even though in offline
applications, better estimation can be achieved by incorporating concurrent
data (filtering) or all available data (smoothing). Here, we extend PSID to
enable optimal filtering and smoothing. First, we show that the presence of a
secondary signal makes it possible to uniquely identify a model with an optimal
Kalman update step (to enable filtering) from a family of otherwise equivalent
state-space models. Our filtering solution augments PSID with a reduced-rank
regression step that directly learns the optimal gain required for the update
step from data. We refer to this extension of PSID as PSID with filtering.
Second, inspired by two-filter Kalman smoother formulations, we develop a novel
forward-backward PSID smoothing algorithm where we first apply PSID with
filtering and then apply it again in the reverse time direction on the
residuals of the filtered secondary signal. We validate our methods on
simulated data, showing that our approach recovers the ground-truth model
parameters for filtering, and achieves optimal filtering and smoothing decoding
performance of the secondary signal that matches the ideal performance of the
true underlying model. This work provides a principled framework for optimal
linear filtering and smoothing in the two-signal setting, significantly
expanding the toolkit for analyzing dynamic interactions in multivariate
time-series.

</details>


### [377] [Feel-Good Thompson Sampling for Contextual Bandits: a Markov Chain Monte Carlo Showdown](https://arxiv.org/abs/2507.15290)
*Emile Anand,Sarah Liaw*

Main category: cs.LG

TL;DR: 本文探讨了Feel-Good Thompson Sampling（FG-TS）及其变体在解决上下文多臂赌博问题中的表现，特别是在后验分布近似情况下的适用性。


<details>
  <summary>Details</summary>
Motivation: Thompson Sampling在高维环境中的探索能力不足，而FG-TS通过添加乐观偏差克服了这一问题，在线性设定下实现了渐近最小化遗憾的目标。作者探讨其在近似后验分布中的表现以填补现有研究的空白。

Method: 作者通过系统实验研究了FG-TS及其平滑变体（SFG-TS）在11个真实和合成基准下的表现，包括线性和逻辑赌博问题的精确后验分布环境，以及快速但粗糙的随机梯度采样器生成的近似后验分布环境。

Result: 实验表明，后验样本较准确时更大的乐观偏差有助于提高性能，但当采样噪声占主导时反而损害表现。总体上，FG-TS在线性和逻辑赌博问题中表现优于普通TS，但在神经网络赌博问题中表现逊色。

Conclusion: 由于其竞争力和易用性，FG-TS及其变体适合作为现代上下文赌博基准的基线方法。

Abstract: Thompson Sampling (TS) is widely used to address the exploration/exploitation
tradeoff in contextual bandits, yet recent theory shows that it does not
explore aggressively enough in high-dimensional problems. Feel-Good Thompson
Sampling (FG-TS) addresses this by adding an optimism bonus that biases toward
high-reward models, and it achieves the asymptotically minimax-optimal regret
in the linear setting when posteriors are exact. However, its performance with
\emph{approximate} posteriors -- common in large-scale or neural problems --
has not been benchmarked. We provide the first systematic study of FG-TS and
its smoothed variant (SFG-TS) across eleven real-world and synthetic
benchmarks. To evaluate their robustness, we compare performance across
settings with exact posteriors (linear and logistic bandits) to approximate
regimes produced by fast but coarse stochastic-gradient samplers. Ablations
over preconditioning, bonus scale, and prior strength reveal a trade-off:
larger bonuses help when posterior samples are accurate, but hurt when sampling
noise dominates. FG-TS generally outperforms vanilla TS in linear and logistic
bandits, but tends to be weaker in neural bandits. Nevertheless, because FG-TS
and its variants are competitive and easy-to-use, we recommend them as
baselines in modern contextual-bandit benchmarks. Finally, we provide source
code for all our experiments in
https://github.com/SarahLiaw/ctx-bandits-mcmc-showdown.

</details>


### [378] [Universal crystal material property prediction via multi-view geometric fusion in graph transformers](https://arxiv.org/abs/2507.15303)
*Liang Zhang,Kong Chen,Yuen Wu*

Main category: cs.LG

TL;DR: MGT是一个融合多视角图表示的框架，显著提升晶体材料的性能预测精度及多领域可迁移性，其在晶体几何特性捕捉中显示了优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在晶体属性预测中无法有效捕捉复杂的几何和拓扑特性，亟需开发新方法改进性能。

Method: 提出MGT框架，通过融合SE3不变和SO3等变的图表示，并使用轻量化专家路由器自适应调整两者权重，结合多任务自监督预训练提升预测能力。

Result: MGT相比前沿模型错误率降低21%，在多种迁移学习任务中性能提升最高达58%。

Conclusion: MGT能有效支持晶体材料性能预测，具有发现新材料的潜力和应用价值。

Abstract: Accurately and comprehensively representing crystal structures is critical
for advancing machine learning in large-scale crystal materials simulations,
however, effectively capturing and leveraging the intricate geometric and
topological characteristics of crystal structures remains a core, long-standing
challenge for most existing methods in crystal property prediction. Here, we
propose MGT, a multi-view graph transformer framework that synergistically
fuses SE3 invariant and SO3 equivariant graph representations, which
respectively captures rotation-translation invariance and rotation equivariance
in crystal geometries. To strategically incorporate these complementary
geometric representations, we employ a lightweight mixture of experts router in
MGT to adaptively adjust the weight assigned to SE3 and SO3 embeddings based on
the specific target task. Compared with previous state-of-the-art models, MGT
reduces the mean absolute error by up to 21% on crystal property prediction
tasks through multi-task self-supervised pretraining. Ablation experiments and
interpretable investigations confirm the effectiveness of each technique
implemented in our framework. Additionally, in transfer learning scenarios
including crystal catalyst adsorption energy and hybrid perovskite bandgap
prediction, MGT achieves performance improvements of up to 58% over existing
baselines, demonstrating domain-agnostic scalability across diverse application
domains. As evidenced by the above series of studies, we believe that MGT can
serve as useful model for crystal material property prediction, providing a
valuable tool for the discovery of novel materials.

</details>


### [379] [Beyond Model Base Selection: Weaving Knowledge to Master Fine-grained Neural Network Design](https://arxiv.org/abs/2507.15336)
*Jialiang Wang,Hanmo Liu,Shimin Di,Zhili Wang,Jiachuan Wang,Lei Chen,Xiaofang Zhou*

Main category: cs.LG

TL;DR: 该论文提出M-DESIGN，一个用于神经网络优化的模型知识库管道，通过自适应地集成任务元数据框架来改进模型选择流程。


<details>
  <summary>Details</summary>
Motivation: 现有数据库研究在模型选择中缺乏对任务查询和模型架构关系的细粒度研究，难以实现动态优化与提升。

Method: 提出了一个知识编织引擎，将模型优化视作一种基于任务元数据的自适应查询问题，利用图关系知识模式解析数据属性、架构变化和性能差异，并驱动预测查询规划器以适应分布外任务。

Result: 在大量图分析任务和数据集上进行实验，在67,760个候选模型中，M-DESIGN在33个数据任务对中26次选择了最佳模型，且预算有限。

Conclusion: M-DESIGN显著提升了模型优化与选择性能，证明其在数据库系统中引入智能优化机制的潜力。

Abstract: Database systems have recently advocated for embedding machine learning (ML)
capabilities, offering declarative model queries over large, managed model
repositories, thereby circumventing the huge computational overhead of
traditional ML-based algorithms in automated neural network model selection.
Pioneering database studies aim to organize existing benchmark repositories as
model bases (MB), querying them for the model records with the highest
performance estimation metrics for given tasks. However, this static model
selection practice overlooks the fine-grained, evolving relational dependencies
between diverse task queries and model architecture variations, resulting in
suboptimal matches and failing to further refine the model effectively. To fill
the model refinement gap in database research, we propose M-DESIGN, a curated
model knowledge base (MKB) pipeline for mastering neural network refinement by
adaptively weaving prior insights about model architecture modification. First,
we propose a knowledge weaving engine that reframes model refinement as an
adaptive query problem over task metadata. Given a user's task query, M-DESIGN
quickly matches and iteratively refines candidate models by leveraging a
graph-relational knowledge schema that explicitly encodes data properties,
architecture variations, and pairwise performance deltas as joinable relations.
This schema supports fine-grained relational analytics over architecture tweaks
and drives a predictive query planner that can detect and adapt to
out-of-distribution (OOD) tasks. We instantiate M-DESIGN for graph analytics
tasks, where our model knowledge base enriches existing benchmarks with
structured metadata covering 3 graph tasks and 22 graph datasets, contributing
data records of 67,760 graph models. Empirical results demonstrate that
M-DESIGN delivers the optimal model in 26 of 33 data-task pairs within limited
budgets.

</details>


### [380] [Scaling Decentralized Learning with FLock](https://arxiv.org/abs/2507.15349)
*Zehua Cheng,Rui Sun,Jiahao Sun,Yike Guo*

Main category: cs.LG

TL;DR: FLock是一个去中心化框架，结合区块链信任层以改进LLM的协作微调，实现了70B参数模型的首次验证，显著提升了安全性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决中心化控制缺失和大规模计算通信负担问题，填补现有联邦学习在大模型协作调优中的瓶颈。

Method: 提出FLock框架，融合区块链信任层与经济激励机制，替代中心化聚合器，并提供对多领域、去信任环境中LLM安全微调的验证。

Result: FLock框架成功防御后门攻击，降低68%以上的对抗攻击成功率，并提升跨领域模型泛化能力，相较于单独训练的模型表现更优。

Conclusion: FLock验证了在去中心化环境中安全、高效地微调超大规模语言模型的可行性，为安全协作学习提供了新范式。

Abstract: Fine-tuning the large language models (LLMs) are prevented by the deficiency
of centralized control and the massive computing and communication overhead on
the decentralized schemes. While the typical standard federated learning (FL)
supports data privacy, the central server requirement creates a single point of
attack and vulnerability to poisoning attacks. Generalizing the result in this
direction to 70B-parameter models in the heterogeneous, trustless environments
has turned out to be a huge, yet unbroken bottleneck. This paper introduces
FLock, a decentralized framework for secure and efficient collaborative LLM
fine-tuning. Integrating a blockchain-based trust layer with economic
incentives, FLock replaces the central aggregator with a secure, auditable
protocol for cooperation among untrusted parties. We present the first
empirical validation of fine-tuning a 70B LLM in a secure, multi-domain,
decentralized setting. Our experiments show the FLock framework defends against
backdoor poisoning attacks that compromise standard FL optimizers and fosters
synergistic knowledge transfer. The resulting models show a >68% reduction in
adversarial attack success rates. The global model also demonstrates superior
cross-domain generalization, outperforming models trained in isolation on their
own specialized data.

</details>


### [381] [To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models](https://arxiv.org/abs/2507.15381)
*Julia Machnio,Mads Nielsen,Mostafa Mehdipour Ghazi*

Main category: cs.LG

TL;DR: 提出PALM，一个可以用数学模型分析主动学习模型性能的工具，用于预测和比较不同策略下的表现，验证了其在多个数据集上的广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 传统的主动学习评估方法仅关注最终准确性，未能全面反映学习过程动态，因此需要一个统一且可解释的评估工具。

Method: 设计了一种数学模型PALM，通过四个参数（可达准确性、覆盖效率、早期性能和可扩展性）来表征主动学习的轨迹，能够从部分数据预测后续表现。

Result: 在CIFAR-10/100和ImageNet-50/100/200等数据集上对PALM进行了验证，证明其可以有效泛化并且准确预测学习曲线。

Conclusion: PALM不仅揭示了学习效率、数据覆盖和方法扩展性的关键洞察，还为研究和实际应用中的主动学习提供系统化和可复现的评估方法，同时代码已开源以供使用。

Abstract: Active learning (AL) seeks to reduce annotation costs by selecting the most
informative samples for labeling, making it particularly valuable in
resource-constrained settings. However, traditional evaluation methods, which
focus solely on final accuracy, fail to capture the full dynamics of the
learning process. To address this gap, we propose PALM (Performance Analysis of
Active Learning Models), a unified and interpretable mathematical model that
characterizes AL trajectories through four key parameters: achievable accuracy,
coverage efficiency, early-stage performance, and scalability. PALM provides a
predictive description of AL behavior from partial observations, enabling the
estimation of future performance and facilitating principled comparisons across
different strategies. We validate PALM through extensive experiments on
CIFAR-10/100 and ImageNet-50/100/200, covering a wide range of AL methods and
self-supervised embeddings. Our results demonstrate that PALM generalizes
effectively across datasets, budgets, and strategies, accurately predicting
full learning curves from limited labeled data. Importantly, PALM reveals
crucial insights into learning efficiency, data space coverage, and the
scalability of AL methods. By enabling the selection of cost-effective
strategies and predicting performance under tight budget constraints, PALM lays
the basis for more systematic, reproducible, and data-efficient evaluation of
AL in both research and real-world applications. The code is available at:
https://github.com/juliamachnio/PALM.

</details>


### [382] [Learning to Gridize: Segment Physical World by Wireless Communication Channel](https://arxiv.org/abs/2507.15386)
*Juntao Wang,Feng Yin,Tian Ding,Tsung-Hui Chang,Zhi-Quan Luo,Qi Yan*

Main category: cs.LG

TL;DR: 提出了一个创新的框架——Channel Space Gridization (CSG)，首次将信道估计和网格化问题联合优化，显著提高了大规模网络优化的性能。


<details>
  <summary>Details</summary>
Motivation: 现有网格化方法依赖于难以获得的地理位置信息或错误的假设，无法满足信道特性精准划分的需求。

Method: 提出了CSG框架，并设计了名为CSG Autoencoder (CSG-AE)的神经网络，结合物理建模与新的训练机制（PIDA训练方案），实现高效网格划分。

Result: 在合成数据集上的测试显示，CSG-AE的信道角度功率谱估计和聚类质量表现卓越；在真实数据集上，RSRP预测误差显著降低，网格化质量和信道一致性大幅提升。

Conclusion: CSG框架及其训练方案显著推动了大规模网络优化中的网格化技术发展，为实现高效准确的信道特性划分奠定了基础。

Abstract: Gridization, the process of partitioning space into grids where users share
similar channel characteristics, serves as a fundamental prerequisite for
efficient large-scale network optimization. However, existing methods like
Geographical or Beam Space Gridization (GSG or BSG) are limited by reliance on
unavailable location data or the flawed assumption that similar signal
strengths imply similar channel properties. We propose Channel Space
Gridization (CSG), a pioneering framework that unifies channel estimation and
gridization for the first time. Formulated as a joint optimization problem, CSG
uses only beam-level reference signal received power (RSRP) to estimate Channel
Angle Power Spectra (CAPS) and partition samples into grids with homogeneous
channel characteristics. To perform CSG, we develop the CSG Autoencoder
(CSG-AE), featuring a trainable RSRP-to-CAPS encoder, a learnable sparse
codebook quantizer, and a physics-informed decoder based on the Localized
Statistical Channel Model. On recognizing the limitations of naive training
scheme, we propose a novel Pretraining-Initialization-Detached-Asynchronous
(PIDA) training scheme for CSG-AE, ensuring stable and effective training by
systematically addressing the common pitfalls of the naive training paradigm.
Evaluations reveal that CSG-AE excels in CAPS estimation accuracy and
clustering quality on synthetic data. On real-world datasets, it reduces Active
Mean Absolute Error (MAE) by 30\% and Overall MAE by 65\% on RSRP prediction
accuracy compared to salient baselines using the same data, while improving
channel consistency, cluster sizes balance, and active ratio, advancing the
development of gridization for large-scale network optimization.

</details>


### [383] [MAP Estimation with Denoisers: Convergence Rates and Guarantees](https://arxiv.org/abs/2507.15397)
*Scott Pesme,Giacomo Meanti,Michael Arbel,Julien Mairal*

Main category: cs.LG

TL;DR: 研究证明了一种常用算法在对数凸性假设下可以稳定收敛至负对数先验的近端算子，从而为先前的启发式方法提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 许多反问题求解中采用的去噪器被当作预训练网络使用，但缺乏理论上的普遍性解释，需对此探索深入。

Method: 采用一种与实践中常用算法相近的方法，并基于对数凸性假设证明其能收敛至近端算子，同时将其解释为对平滑近端目标的梯度下降。

Result: 证明了先验的对数凸性假设下，相关算法能稳定收敛并提供数学依据支持。

Conclusion: 此研究为众多应用于反问题求解的启发式算法提供了稳健的理论支撑。

Abstract: Denoiser models have become powerful tools for inverse problems, enabling the
use of pretrained networks to approximate the score of a smoothed prior
distribution. These models are often used in heuristic iterative schemes aimed
at solving Maximum a Posteriori (MAP) optimisation problems, where the proximal
operator of the negative log-prior plays a central role. In practice, this
operator is intractable, and practitioners plug in a pretrained denoiser as a
surrogate-despite the lack of general theoretical justification for this
substitution. In this work, we show that a simple algorithm, closely related to
several used in practice, provably converges to the proximal operator under a
log-concavity assumption on the prior $p$. We show that this algorithm can be
interpreted as a gradient descent on smoothed proximal objectives. Our analysis
thus provides a theoretical foundation for a class of empirically successful
but previously heuristic methods.

</details>


### [384] [The calculus of variations of the Transformer on the hyperspherical tangent bundle](https://arxiv.org/abs/2507.15431)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: 提出了使用拉格朗日优化阐释Transformer数学背景的理论框架。


<details>
  <summary>Details</summary>
Motivation: 弥补Transformer数学背景研究的不足，尤其是在变分法与神经网络结合的领域。

Method: 通过计算变分法和拉格朗日优化，引入新框架分析Transformer，并推导其欧拉-拉格朗日方程。

Result: 证明了Transformer可以看作解决某种变分问题的自然求解器，并提出在路径最优损失优化中应用。

Conclusion: 为通过变分法研究Transformer开辟新领域，提出数学理论基础，为未来研究铺路。

Abstract: We offer a theoretical mathematical background to Transformers through
Lagrangian optimization across the token space. The Transformer, as a flow map,
exists in the tangent fiber for each token along the high-dimensional unit
sphere. The circumstance of the hypersphere across the latent data is
reasonable due to the trained diagonal matrix equal to the identity, which has
various empirical justifications. Thus, under the continuum limit of the
dynamics, the latent vectors flow among the tangent bundle. Using these facts,
we devise a mathematical framework for the Transformer through calculus of
variations. We develop a functional and show that the continuous flow map
induced by the Transformer satisfies this functional, therefore the Transformer
can be viewed as a natural solver of a calculus of variations problem. We
invent new scenarios of when our methods are applicable based on loss
optimization with respect to path optimality. We derive the Euler-Lagrange
equation for the Transformer. The variant of the Euler-Lagrange equation we
present has various appearances in literature, but, to our understanding,
oftentimes not foundationally proven or under other specialized cases. Our
overarching proof is new: our techniques are classical and the use of the flow
map object is original. We provide several other relevant results, primarily
ones specific to neural scenarios. In particular, much of our analysis will be
attempting to quantify Transformer data in variational contexts under neural
approximations. Calculus of variations on manifolds is a well-nourished
research area, but for the Transformer specifically, it is uncharted: we lay
the foundation for this area through an introduction to the Lagrangian for the
Transformer.

</details>


### [385] [Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2507.15507)
*Johannes Ackermann,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 本文探讨强化学习从人类反馈（RLHF）方法应用于语言模型（LMs）时的过优化问题，并提出一种称为Off-Policy Corrected Reward Modeling（OCRM）的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法在训练过程中存在奖励模型（RM）随着分布转移变得不准确的问题，导致学习行为偏离人类偏好。

Method: 提出了一种名为OCRM的方法，通过重要性加权对RM进行迭代的离线修正，无需获取新的标签或样本。

Result: 实验验证表明，与现有RLHF方法相比，该方法在摘要及聊天机器人数据集上表现显著更优。

Conclusion: OCRM能够改进奖励模型的准确性，显著优化最终策略，克服标准RLHF方法中的过优化问题。

Abstract: Reinforcement Learning from Human Feedback (RLHF) allows us to train models,
such as language models (LMs), to follow complex human preferences. In RLHF for
LMs, we first train an LM using supervised fine-tuning, sample pairs of
responses, obtain human feedback, and use the resulting data to train a reward
model (RM). RL methods are then used to train the LM to maximize the reward
given by the RM. As training progresses, the responses generated by the LM no
longer resemble the responses seen by the RM during training, leading to the RM
becoming inaccurate. The score given by the RM keeps increasing, but the
learned behavior no longer matches the human preferences. This issue is known
as overoptimization. We investigate overoptimization from the point of view of
distribution shift and show that the shift results in an inconsistent estimate
of the RM parameters, leading to an inconsistent estimate of the policy
gradient. We propose Off-Policy Corrected Reward Modeling (OCRM), which
iteratively off-policy corrects the RM using importance weighting, without
requiring new labels or samples. This results in a more accurate RM, which
empirically leads to an improved final policy. We validate our approach in
experiments with summarization and chatbot datasets and show that it performs
significantly better than standard RLHF methods and baselines. Our
implementation is available at
https://github.com/JohannesAck/OffPolicyCorrectedRewardModeling

</details>


### [386] [An Adaptive Random Fourier Features approach Applied to Learning Stochastic Differential Equations](https://arxiv.org/abs/2507.15442)
*Owen Douglas,Aku Kammonen,Anamika Pandey,Raúl Tempone*

Main category: cs.LG

TL;DR: 本文提出基于自适应随机傅里叶特征（ARFF）的训练算法，用于从快照数据中学习随机微分方程的漂移和扩散部分。


<details>
  <summary>Details</summary>
Motivation: 为了优化随机微分方程在漂移和扩散部分的学习精度和算法收敛速度，提出利用ARFF和Metropolis采样等方法代替传统的Adam优化方法。

Method: 采用基于ARFF的训练算法，包括Metropolis采样和重新采样，并结合从Euler-Maruyama积分推导的基于似然性的损失函数。测试案例包括多项式例子、欠阻尼Langevin动力学、随机SIR模型和随机波动方程。

Result: 在所有测试案例中，ARFF方法在损失最小化和收敛速度上均匹配或优于基于Adam优化的方法。

Conclusion: ARFF方法展示了其在数据驱动的随机动力学建模中的潜力，作为传统优化方法的有力替代方案。

Abstract: This work proposes a training algorithm based on adaptive random Fourier
features (ARFF) with Metropolis sampling and resampling
\cite{kammonen2024adaptiverandomfourierfeatures} for learning drift and
diffusion components of stochastic differential equations from snapshot data.
Specifically, this study considers It\^{o} diffusion processes and a
likelihood-based loss function derived from the Euler-Maruyama integration
introduced in \cite{Dietrich2023} and
\cite{dridi2021learningstochasticdynamicalsystems}.
  This work evaluates the proposed method against benchmark problems presented
in \cite{Dietrich2023}, including polynomial examples, underdamped Langevin
dynamics, a stochastic susceptible-infected-recovered model, and a stochastic
wave equation. Across all cases, the ARFF-based approach matches or surpasses
the performance of conventional Adam-based optimization in both loss
minimization and convergence speed. These results highlight the potential of
ARFF as a compelling alternative for data-driven modeling of stochastic
dynamics.

</details>


### [387] [Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training](https://arxiv.org/abs/2507.15640)
*Kailai Yang,Xiao Liu,Lei Ji,Hao Li,Yeyun Gong,Peng Cheng,Mao Yang*

Main category: cs.LG

TL;DR: 该研究提出了一种称为Data Mixing Agent的新框架，用以优化领域数据的权重分配，从而在持续预训练中平衡源领域与目标领域的表现。


<details>
  <summary>Details</summary>
Motivation: 为了解决在特定领域的持续预训练中，模型性能可能因数据权重分配不当而导致原始能力遗忘的问题。

Method: 提出了Data Mixing Agent框架，该框架基于强化学习，在大量数据混合轨迹及相应反馈中学习权重分配的通用启发式方法。

Result: 在数学推理任务的持续预训练实验中，该方法优于传统基线，在源领域与目标领域的表现平衡上取得了更好效果，并在未见领域与目标模型等方面展现了良好的泛化能力。

Conclusion: Data Mixing Agent 是一种高效、可推广的框架，不仅能够在不同领域间灵活应用，还能够在减少源领域数据的情况下实现优越性能，同时其行为与人类直觉高度一致。

Abstract: Continual pre-training on small-scale task-specific data is an effective
method for improving large language models in new target fields, yet it risks
catastrophic forgetting of their original capabilities. A common solution is to
re-weight training data mixtures from source and target fields on a domain
space to achieve balanced performance. Previous domain reweighting strategies
rely on manual designation with certain heuristics based on human intuition or
empirical results. In this work, we prove that more general heuristics can be
parameterized by proposing Data Mixing Agent, the first model-based, end-to-end
framework that learns to re-weight domains. The agent learns generalizable
heuristics through reinforcement learning on large quantities of data mixing
trajectories with corresponding feedback from an evaluation environment.
Experiments in continual pre-training on math reasoning show that Data Mixing
Agent outperforms strong baselines in achieving balanced performance across
source and target field benchmarks. Furthermore, it generalizes well across
unseen source fields, target models, and domain spaces without retraining.
Direct application to the code generation field also indicates its adaptability
across target domains. Further analysis showcases the agents' well-aligned
heuristics with human intuitions and their efficiency in achieving superior
model performance with less source-field data.

</details>


### [388] [FedMultiEmo: Real-Time Emotion Recognition via Multimodal Federated Learning](https://arxiv.org/abs/2507.15470)
*Baran Can Gül,Suraksha Nadig,Stefanos Tziampazis,Nasser Jazdi,Michael Weyrich*

Main category: cs.LG

TL;DR: FedMultiEmo是一种保护隐私的框架，结合视觉和生理信号，提升车载情绪识别的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了解决视觉方法因光照或遮挡弱化、生理信号个体差异大、隐私敏感数据传输等问题，研究提出一种新方法。

Method: FedMultiEmo通过多模态联邦学习结合视觉与生理信号，使用Raspberry Pi 和 Flower 服务器实现从边缘到云的原型系统，并采用个性化的联邦平均加权机制。

Result: 该系统在FER2013和自定义生理数据集上表现良好，融合模型准确率达87%，与集中式方法相当，且实现了18轮收敛，保障隐私的同时提高了性能。

Conclusion: FedMultiEmo为实时、隐私保护的车载情绪识别提供了一种可行的解决方案，示范了高效的边云协作系统。

Abstract: In-vehicle emotion recognition underpins adaptive driver-assistance systems
and, ultimately, occupant safety. However, practical deployment is hindered by
(i) modality fragility - poor lighting and occlusions degrade vision-based
methods; (ii) physiological variability - heart-rate and skin-conductance
patterns differ across individuals; and (iii) privacy risk - centralized
training requires transmission of sensitive data. To address these challenges,
we present FedMultiEmo, a privacy-preserving framework that fuses two
complementary modalities at the decision level: visual features extracted by a
Convolutional Neural Network from facial images, and physiological cues (heart
rate, electrodermal activity, and skin temperature) classified by a Random
Forest. FedMultiEmo builds on three key elements: (1) a multimodal federated
learning pipeline with majority-vote fusion, (2) an end-to-end edge-to-cloud
prototype on Raspberry Pi clients and a Flower server, and (3) a personalized
Federated Averaging scheme that weights client updates by local data volume.
Evaluated on FER2013 and a custom physiological dataset, the federated
Convolutional Neural Network attains 77% accuracy, the Random Forest 74%, and
their fusion 87%, matching a centralized baseline while keeping all raw data
local. The developed system converges in 18 rounds, with an average round time
of 120 seconds and a per-client memory footprint below 200 MB. These results
indicate that FedMultiEmo offers a practical approach to real-time,
privacy-aware emotion recognition in automotive settings.

</details>


### [389] [An Investigation of Test-time Adaptation for Audio Classification under Background Noise](https://arxiv.org/abs/2507.15523)
*Weichuang Shao,Iman Yi Liao,Tomas Henrique Bode Maul,Tissa Chandesa*

Main category: cs.LG

TL;DR: 本研究探讨域转移问题，提出在背景噪声下通过测试时间适配（TTA）改进音频分类性能，实验结果表明改进版CoNMix方法在领域转移场景下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 为了解决域转移导致的音频分类性能下降问题，尤其是背景噪声引起的领域转移问题。

Method: 采用测试时间适配（TTA），包括TTT、TENT和改进版CoNMix，并在AudioMNIST和SpeechCommands V1数据集上评估这三种方法在不同背景噪声和噪声水平下的性能。

Result: 在领域转移场景下，改进的CoNMix方法取得最佳分类准确性，在AudioMNIST数据集10 dB锻炼单车背景噪声下错误率为5.31%，3 dB流水背景噪声下错误率为12.75%。

Conclusion: 首次结合TTA技术解决音频分类中的域转移问题，并证明改进版CoNMix优于现有方法，适用于背景噪声场景。

Abstract: Domain shift is a prominent problem in Deep Learning, causing a model
pre-trained on a source dataset to suffer significant performance degradation
on test datasets. This research aims to address the issue of audio
classification under domain shift caused by background noise using Test-Time
Adaptation (TTA), a technique that adapts a pre-trained model during testing
using only unlabelled test data before making predictions. We adopt two common
TTA methods, TTT and TENT, and a state-of-the-art method CoNMix, and
investigate their respective performance on two popular audio classification
datasets, AudioMNIST (AM) and SpeechCommands V1 (SC), against different types
of background noise and noise severity levels. The experimental results reveal
that our proposed modified version of CoNMix produced the highest
classification accuracy under domain shift (5.31% error rate under 10 dB
exercise bike background noise and 12.75% error rate under 3 dB running tap
background noise for AM) compared to TTT and TENT. The literature search
provided no evidence of similar works, thereby motivating the work reported
here as the first study to leverage TTA techniques for audio classification
under domain shift.

</details>


### [390] [Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning](https://arxiv.org/abs/2507.15788)
*Sneheel Sarangi,Hanan Salam*

Main category: cs.LG

TL;DR: 研究探讨通过强化学习给予的小型语言模型是否能获得广泛的“心智理论”（ToM）能力，但发现主要是局部过拟合而非真正获得抽象能力。


<details>
  <summary>Details</summary>
Motivation: 探索小型语言模型是否可以通过强化学习方法习得更复杂、更具人类特质的如“心智理论”的社交智能。

Method: 采用强化学习与可验证奖励（RLVR）方法训练模型，并在不同的“心智理论”数据集中包括训练（如HiToM、ExploreToM）和测试（如OpenToM）组合下进行性能评估。

Result: 小型语言模型在分布内（in-distribution）任务上表现提升，但对未见过的任务（out-of-distribution）无法泛化，并且长期训练易导致对统计模式的“猎取”，而非真正的能力提升。

Conclusion: 通过现有方法，小型语言模型难以发展通用的心智理论能力，获得的仅是局限性强的局部性能优化。

Abstract: Recent advancements in large language models (LLMs) have demonstrated
emergent capabilities in complex reasoning, largely spurred by rule-based
Reinforcement Learning (RL) techniques applied during the post-training. This
has raised the question of whether similar methods can instill more nuanced,
human-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This
paper investigates whether small-scale LLMs can acquire a robust and
generalizable ToM capability through RL with verifiable rewards (RLVR). We
conduct a systematic evaluation by training models on various combinations of
prominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for
generalization on held-out datasets (e.g., OpenToM). Our findings indicate that
small LLMs struggle to develop a generic ToM capability. While performance on
in-distribution tasks improves, this capability fails to transfer to unseen ToM
tasks with different characteristics. Furthermore, we demonstrate that
prolonged RL training leads to models ``hacking'' the statistical patterns of
the training datasets, resulting in significant performance gains on in-domain
data but no change, or degradation of performance on out-of-distribution tasks.
This suggests the learned behavior is a form of narrow overfitting rather than
the acquisition of a true, abstract ToM capability.

</details>


### [391] [Data Aware Differentiable Neural Architecture Search for Tiny Keyword Spotting Applications](https://arxiv.org/abs/2507.15545)
*Yujia Shi,Emil Njor,Pablo Martínez-Nuevo,Sven Ewan Shepstone,Xenofon Fafoutis*

Main category: cs.LG

TL;DR: 提出了一种新方法“Data Aware Differentiable Neural Architecture Search”，可同时优化模型架构与数据特性，提高资源利用效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决TinyML系统设计复杂性高的问题，推动其更广泛应用。

Method: 将神经架构搜索的搜索空间扩展至数据配置参数，并联合优化模型架构与输入数据特性。

Result: 初步实验表明，该方法可生成资源节省且高精度的TinyML系统。

Conclusion: 新的联合优化方法证明了在资源受限条件下生成高效TinyML设计的潜力。

Abstract: The success of Machine Learning is increasingly tempered by its significant
resource footprint, driving interest in efficient paradigms like TinyML.
However, the inherent complexity of designing TinyML systems hampers their
broad adoption. To reduce this complexity, we introduce "Data Aware
Differentiable Neural Architecture Search". Unlike conventional Differentiable
Neural Architecture Search, our approach expands the search space to include
data configuration parameters alongside architectural choices. This enables
Data Aware Differentiable Neural Architecture Search to co-optimize model
architecture and input data characteristics, effectively balancing resource
usage and system performance for TinyML applications. Initial results on
keyword spotting demonstrate that this novel approach to TinyML system design
can generate lean but highly accurate systems.

</details>


### [392] [The added value for MRI radiomics and deep-learning for glioblastoma prognostication compared to clinical and molecular information](https://arxiv.org/abs/2507.15548)
*D. Abler,O. Pusterla,A. Joye-Kühnis,N. Andratschke,M. Bach,A. Bink,S. M. Christ,P. Hagmann,B. Pouymayou,E. Pravatà,P. Radojewski,M. Reyes,L. Ruinelli,R. Schaer,B. Stieltjes,G. Treglia,W. Valenzuela,R. Wiest,S. Zoergiebel,M. Guckenberger,S. Tanadini-Lang,A. Depeursinge*

Main category: cs.LG

TL;DR: 本研究评估了传统放射组学（CR）和深度学习（DL）MRI放射组学在多中心队列中预测胶质母细胞瘤预后（<= 6 个月或 > 6 个月生存）的附加价值。


<details>
  <summary>Details</summary>
Motivation: 研究胶质母细胞瘤预后的附加预测能力，特别是在综合考虑影像数据和临床/分子数据的情况下。

Method: 收集1152例胶质母细胞瘤患者的多中心数据，采用常规放射组学（CR）和深度学习（DL）方法开发模型，并评估其在内外部队列上的性能，针对不同数据组合和患者亚组进行了子分析。

Result: 在整个队列分析中，CR组合模型在外部验证中表现最佳（AUC = 0.75），优于仅使用临床数据或仅使用影像数据的模型，但差异较小。DL模型趋势相似，但无统计显著性。在特定亚组中，组合模型的表现未超过仅使用临床数据的模型。CR模型通过额外生存分析表现出影像数据的部分重要性。

Conclusion: 尽管标准CR和DL方法能够确认解剖MRI序列对胶质母细胞瘤预后的预测价值，但其对于人口学预测因素（如年龄与性别）的附加价值有限。

Abstract: Background: Radiomics shows promise in characterizing glioblastoma, but its
added value over clinical and molecular predictors has yet to be proven. This
study assessed the added value of conventional radiomics (CR) and deep learning
(DL) MRI radiomics for glioblastoma prognosis (<= 6 vs > 6 months survival) on
a large multi-center dataset.
  Methods: After patient selection, our curated dataset gathers 1152
glioblastoma (WHO 2016) patients from five Swiss centers and one public source.
It included clinical (age, gender), molecular (MGMT, IDH), and baseline MRI
data (T1, T1 contrast, FLAIR, T2) with tumor regions. CR and DL models were
developed using standard methods and evaluated on internal and external
cohorts. Sub-analyses assessed models with different feature sets
(imaging-only, clinical/molecular-only, combined-features) and patient subsets
(S-1: all patients, S-2: with molecular data, S-3: IDH wildtype).
  Results: The best performance was observed in the full cohort (S-1). In
external validation, the combined-feature CR model achieved an AUC of 0.75,
slightly, but significantly outperforming clinical-only (0.74) and imaging-only
(0.68) models. DL models showed similar trends, though without statistical
significance. In S-2 and S-3, combined models did not outperform clinical-only
models. Exploratory analysis of CR models for overall survival prediction
suggested greater relevance of imaging data: across all subsets,
combined-feature models significantly outperformed clinical-only models, though
with a modest advantage of 2-4 C-index points.
  Conclusions: While confirming the predictive value of anatomical MRI
sequences for glioblastoma prognosis, this multi-center study found standard CR
and DL radiomics approaches offer minimal added value over demographic
predictors such as age and gender.

</details>


### [393] [GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding](https://arxiv.org/abs/2507.15846)
*Fei Tang,Zhangxuan Gu,Zhengxi Lu,Xuyang Liu,Shuheng Shen,Changhua Meng,Wen Wang,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: 本文提出了GUI-G$^2$（GUI Gaussian Grounding Rewards），通过将GUI元素建模为连续的高斯分布，改进了现有强化学习方法在用户界面交互中的稀疏信号问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法将GUI元素视为二元的点击目标，忽略了空间交互中的连续性问题，作者受到人类点击行为的启发，提出了通过高斯分布建模的奖励框架。

Method: GUI-G$^2$框架包括两个机制：高斯点奖励用于精确定位，通过以元素中心为中心的指数衰减分布建模；覆盖奖励用于评估空间对齐，计算预测高斯分布与目标区域的重叠。为了适配不同大小的元素，还引入了自适应方差机制。

Result: 在ScreenSpot、ScreenSpot-v2和ScreenSpot-Pro基准上，GUI-G$^2$性能显著优于现有方法UI-TARS-72B，在ScreenSpot-Pro上提升了24.7%。

Conclusion: 连续建模方法对界面变化具有更强的鲁棒性，并在对新界面布局的泛化上表现更优，为GUI交互任务的空间推理建立了新范式。

Abstract: Graphical User Interface (GUI) grounding maps natural language instructions
to precise interface locations for autonomous interaction. Current
reinforcement learning approaches use binary rewards that treat elements as
hit-or-miss targets, creating sparse signals that ignore the continuous nature
of spatial interactions. Motivated by human clicking behavior that naturally
forms Gaussian distributions centered on target elements, we introduce GUI
Gaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that
models GUI elements as continuous Gaussian distributions across the interface
plane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point
rewards model precise localization through exponentially decaying distributions
centered on element centroids, while coverage rewards assess spatial alignment
by measuring the overlap between predicted Gaussian distributions and target
regions. To handle diverse element scales, we develop an adaptive variance
mechanism that calibrates reward distributions based on element dimensions.
This framework transforms GUI grounding from sparse binary classification to
dense continuous optimization, where Gaussian distributions generate rich
gradient signals that guide models toward optimal interaction positions.
Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro
benchmarks demonstrate that GUI-G$^2$, substantially outperforms
state-of-the-art method UI-TARS-72B, with the most significant improvement of
24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides
superior robustness to interface variations and enhanced generalization to
unseen layouts, establishing a new paradigm for spatial reasoning in GUI
interaction tasks.

</details>


### [394] [PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors](https://arxiv.org/abs/2507.15550)
*Yimeng Chen,Piotr Piȩkos,Mateusz Ostaszewski,Firas Laakom,Jürgen Schmidhuber*

Main category: cs.LG

TL;DR: 此论文提出PhysGym，一个用于评估基于大型语言模型(LLM)的科学推理能力的新基准工具。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门的基准工具评估LLM在科学发现及处理复杂环境的能力，因此需要开发新的评估平台。

Method: 引入了PhysGym，该工具通过控制先验知识水平及复杂环境，评估LLM的科学推理能力，方法包括交互式物理模拟、约束下的数据收集及假设检验。

Result: 研究展示了PhysGym可以通过基准LLM结果区分模型能力，特别是在不同先验知识和任务复杂度下表现的差异。

Conclusion: PhysGym提供了一个系统化工具，能够帮助研究者评估LLM在科学推理及处理复杂问题环境中的能力。

Abstract: Evaluating the scientific discovery capabilities of large language model
based agents, particularly how they cope with varying environmental complexity
and utilize prior knowledge, requires specialized benchmarks currently lacking
in the landscape. To address this gap, we introduce PhysGym, a novel benchmark
suite and simulation platform for rigorously assessing LLM-based scientific
reasoning in interactive physics environments. PhysGym's primary contribution
lies in its sophisticated control over the level of prior knowledge provided to
the agent. This allows researchers to dissect agent performance along axes
including the complexity of the problem and the prior knowledge levels. The
benchmark comprises a suite of interactive simulations, where agents must
actively probe environments, gather data sequentially under constraints and
formulate hypotheses about underlying physical laws. PhysGym provides
standardized evaluation protocols and metrics for assessing hypothesis accuracy
and model fidelity. We demonstrate the benchmark's utility by presenting
results from baseline LLMs, showcasing its ability to differentiate
capabilities based on varying priors and task complexity.

</details>


### [395] [Trade-offs between elective surgery rescheduling and length-of-stay prediction accuracy](https://arxiv.org/abs/2507.15566)
*Pieter Smet,Martina Doneda,Ettore Lanzarone,Giuliana Carello*

Main category: cs.LG

TL;DR: 本文研究了机器学习模型预测患者住院时间（LOS）准确性与重排策略灵活性的关系，并探索在预测误差下如何有效调整住院安排以优化资源利用率。


<details>
  <summary>Details</summary>
Motivation: 患者住院时间预测的准确性直接影响床位资源的使用和计划排程，尤其是当预测与实际相差较大时，可能导致排程不可行。因此，研究如何在预测误差下进行灵活调整以保障资源高效利用至关重要。

Method: 通过模拟方法研究了不同住院时间预测误差情况下的患者重排策略，包括调整入院日期、转移病房和已入院患者流动等，评估其在避免床位溢出和优化资源利用中的效果。

Result: 结果表明，在住院时间预测误差存在的情况下，灵活的重排策略能够显著缓解排程不可行性，同时提高资源利用效率。

Conclusion: 即使住院时间预测存在误差，通过合理的重排策略仍可在资源限制条件下实现床位有效利用，为患者入院排程提供了更稳定可靠的优化途径。

Abstract: The availability of downstream resources plays a critical role in planning
the admission of patients undergoing elective surgery, with inpatient beds
being one of the most crucial resources. When planning patient admissions,
predictions on their length-of-stay (LOS) made by machine learning (ML) models
are used to ensure bed availability. However, the actual LOS for each patient
may differ considerably from the predicted value, potentially making the
schedule infeasible. To address such infeasibilities, rescheduling strategies
that take advantage of operational flexibility can be implemented. For example,
adjustments may include postponing admission dates, relocating patients to
different wards, or even transferring patients who are already admitted. The
common assumption is that more accurate LOS predictions reduce the impact of
rescheduling. However, training ML models that can make such accurate
predictions can be costly. Building on previous work that proposed simulated
\ac{ml} for evaluating data-driven approaches, this paper explores the
relationship between LOS prediction accuracy and rescheduling flexibility
across various corrective policies. Specifically, we examine the most effective
patient rescheduling strategies under LOS prediction errors to prevent bed
overflows while optimizing resource utilization.

</details>


### [396] [On the Role of AI in Managing Satellite Constellations: Insights from the ConstellAI Project](https://arxiv.org/abs/2507.15574)
*Gregory F. Stock,Juan A. Fraire,Holger Hermanns,Jędrzej Mosiężny,Yusra Al-Khazraji,Julio Ramírez Molina,Evridiki V. Ntagiou*

Main category: cs.LG

TL;DR: 本文探讨了AI如何优化卫星超级星座的操作，特别是在数据路由和资源分配中的作用，结果表明AI驱动的算法在灵活性、扩展性和决策过程的普遍适应性上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 卫星星座的快速扩展给网络管理带来了重大挑战，需要创新的管理方式实现高效、可扩展和弹性化的操作。

Method: 文章介绍了使用强化学习（RL）来解决数据路由和资源分配的问题，分别通过学习历史排队延迟来改进端到端延迟和通过优化任务调度有效利用有限资源。

Result: 实验结果表明，RL不仅能与传统方法媲美，还在灵活性、扩展性和决策普遍性方面具有优势，适用多种卫星星座配置和操作情景。

Conclusion: AI能够从根本上改变卫星星座管理格局，提供更具适应性、鲁棒性和成本效益的解决方案，在未来卫星网络管理中显示重大潜力。

Abstract: The rapid expansion of satellite constellations in near-Earth orbits presents
significant challenges in satellite network management, requiring innovative
approaches for efficient, scalable, and resilient operations. This paper
explores the role of Artificial Intelligence (AI) in optimizing the operation
of satellite mega-constellations, drawing from the ConstellAI project funded by
the European Space Agency (ESA). A consortium comprising GMV GmbH, Saarland
University, and Thales Alenia Space collaborates to develop AI-driven
algorithms and demonstrates their effectiveness over traditional methods for
two crucial operational challenges: data routing and resource allocation. In
the routing use case, Reinforcement Learning (RL) is used to improve the
end-to-end latency by learning from historical queuing latency, outperforming
classical shortest path algorithms. For resource allocation, RL optimizes the
scheduling of tasks across constellations, focussing on efficiently using
limited resources such as battery and memory. Both use cases were tested for
multiple satellite constellation configurations and operational scenarios,
resembling the real-life spacecraft operations of communications and Earth
observation satellites. This research demonstrates that RL not only competes
with classical approaches but also offers enhanced flexibility, scalability,
and generalizability in decision-making processes, which is crucial for the
autonomous and intelligent management of satellite fleets. The findings of this
activity suggest that AI can fundamentally alter the landscape of satellite
constellation management by providing more adaptive, robust, and cost-effective
solutions.

</details>


### [397] [We Need to Rethink Benchmarking in Anomaly Detection](https://arxiv.org/abs/2507.15584)
*Philipp Röchner,Simon Klüttermann,Franz Rothlauf,Daniel Schlör*

Main category: cs.LG

TL;DR: 该论文认为异常检测领域的进展停滞，与现有评估方法不足有关，提出需要重新思考异常检测的评估方式。


<details>
  <summary>Details</summary>
Motivation: 目前异常检测算法的评估方法无法充分反映各类应用中异常的多样性，导致算法进展停滞。

Method: 提出了三大改进方向：1. 基于通用分类法识别异常检测场景；2. 对异常检测流程进行端到端及组件级分析；3. 评估算法时需针对场景目标进行有意义地评估。

Result: 暂无直接实验结果，该论文旨在提出新的评估视角与改进方向。

Conclusion: 论文主张重新定义异常检测的评估方法，以应对不同应用场景中多样化的需求，促进异常检测领域的进一步发展。

Abstract: Despite the continuous proposal of new anomaly detection algorithms and
extensive benchmarking efforts, progress seems to stagnate, with only minor
performance differences between established baselines and new algorithms. In
this position paper, we argue that this stagnation is due to limitations in how
we evaluate anomaly detection algorithms. Current benchmarking does not, for
example, sufficiently reflect the diversity of anomalies in applications
ranging from predictive maintenance to scientific discovery. Consequently, we
need to rethink benchmarking in anomaly detection. In our opinion, anomaly
detection should be studied using scenarios that capture the relevant
characteristics of different applications. We identify three key areas for
improvement: First, we need to identify anomaly detection scenarios based on a
common taxonomy. Second, anomaly detection pipelines should be analyzed
end-to-end and by component. Third, evaluating anomaly detection algorithms
should be meaningful regarding the scenario's objectives.

</details>


### [398] [Red-Team Multi-Agent Reinforcement Learning for Emergency Braking Scenario](https://arxiv.org/abs/2507.15587)
*Yinsong Chen,Kaifeng Wang,Xiaoqiang Meng,Xueyuan Li,Zirui Li,Xin Gao*

Main category: cs.LG

TL;DR: 该研究提出了一种通过“红队”多智能体强化学习框架主动挖掘实际问题中的极端情况，提高自动驾驶车辆在安全关键情境下决策安全性的方法。


<details>
  <summary>Details</summary>
Motivation: 当前安全关键场景决策研究中，对角案例的覆盖不足，单纯依赖数据驱动或特定建模方法无法有效应对。

Method: 采用红队多智能体强化学习框架，将能够干扰的背景车辆视为红队，通过探索和主动干扰揭示数据分布之外的极端案例。引入基于约束图表述的马尔可夫决策过程以确保安全规则，同时量化红队对自动驾驶车辆的威胁。

Result: 实验表明，框架显著影响了自动驾驶车辆决策的安全性，能够生成多种极端场景。

Conclusion: 此方法为研究安全关键场景中的极端情况提供了一种新方向。

Abstract: Current research on decision-making in safety-critical scenarios often relies
on inefficient data-driven scenario generation or specific modeling approaches,
which fail to capture corner cases in real-world contexts. To address this
issue, we propose a Red-Team Multi-Agent Reinforcement Learning framework,
where background vehicles with interference capabilities are treated as
red-team agents. Through active interference and exploration, red-team vehicles
can uncover corner cases outside the data distribution. The framework uses a
Constraint Graph Representation Markov Decision Process, ensuring that red-team
vehicles comply with safety rules while continuously disrupting the autonomous
vehicles (AVs). A policy threat zone model is constructed to quantify the
threat posed by red-team vehicles to AVs, inducing more extreme actions to
increase the danger level of the scenario. Experimental results show that the
proposed framework significantly impacts AVs decision-making safety and
generates various corner cases. This method also offers a novel direction for
research in safety-critical scenarios.

</details>


### [399] [Optimal Batch-Size Control for Low-Latency Federated Learning with Device Heterogeneity](https://arxiv.org/abs/2507.15601)
*Huiling Yang,Zhanwei Wang,Kaibin Huang*

Main category: cs.LG

TL;DR: 本论文提出了一种面向通信和计算(C^2)能力差异的新框架，用于优化联邦学习中的批量尺寸控制，以降低端到端学习延迟并保证收敛效果。


<details>
  <summary>Details</summary>
Motivation: 当前6G网络中的联邦学习需要解决高维模型更新计算和传输的开销，以及设备间通信和计算能力差异所导致的问题，以实现低延迟和高性能的学习框架。

Method: 通过收敛性分析揭示了批量尺寸与通信延迟的基本权衡关系，设计了一种计算收敛速度的替代模型，并基于此提出了两种适应不同通信环境的批量尺寸控制策略。

Result: 实验结果表明，提出的批量尺寸控制策略优于未考虑通信和计算能力权衡或设备异构性的传统方法，显著降低了学习延迟。

Conclusion: 该研究提出了一个创新的批量尺寸控制框架，为解决联邦学习低延迟挑战提供了有效方案，并提升了在异构设备环境下的适应性和性能。

Abstract: Federated learning (FL) has emerged as a popular approach for collaborative
machine learning in sixth-generation (6G) networks, primarily due to its
privacy-preserving capabilities. The deployment of FL algorithms is expected to
empower a wide range of Internet-of-Things (IoT) applications, e.g., autonomous
driving, augmented reality, and healthcare. The mission-critical and
time-sensitive nature of these applications necessitates the design of
low-latency FL frameworks that guarantee high learning performance. In
practice, achieving low-latency FL faces two challenges: the overhead of
computing and transmitting high-dimensional model updates, and the
heterogeneity in communication-and-computation (C$^2$) capabilities across
devices. To address these challenges, we propose a novel C$^2$-aware framework
for optimal batch-size control that minimizes end-to-end (E2E) learning latency
while ensuring convergence. The framework is designed to balance a fundamental
C$^2$ tradeoff as revealed through convergence analysis. Specifically,
increasing batch sizes improves the accuracy of gradient estimation in FL and
thus reduces the number of communication rounds required for convergence, but
results in higher per-round latency, and vice versa. The associated problem of
latency minimization is intractable; however, we solve it by designing an
accurate and tractable surrogate for convergence speed, with parameters fitted
to real data. This approach yields two batch-size control strategies tailored
to scenarios with slow and fast fading, while also accommodating device
heterogeneity. Extensive experiments using real datasets demonstrate that the
proposed strategies outperform conventional batch-size adaptation schemes that
do not consider the C$^2$ tradeoff or device heterogeneity.

</details>


### [400] [Accelerating HEC-RAS: A Recurrent Neural Operator for Rapid River Forecasting](https://arxiv.org/abs/2507.15614)
*Edward Holmberg,Pujan Pokhrel,Maximilian Zoch,Elias Ioup,Ken Pathak,Steven Sloan,Kendall Niles,Jay Ratcliff,Maik Flanagin,Christian Guetl,Julian Simeonov,Mahdi Abdelguerfi*

Main category: cs.LG

TL;DR: 本文提出了一种结合GRU和Geo-FNO的深度学习替代模型，用于加速基于HEC-RAS的河流模拟，显著减少了计算时间且保持了高精度。


<details>
  <summary>Details</summary>
Motivation: 物理计算模型如HEC-RAS虽然精度高，但计算效率低，无法满足洪水事件中实时决策的需求，因此需要一种既高效又准确的替代方法。

Method: 提出了一种混合型自回归架构，将GRU用于短期时间动态捕捉，将Geo-FNO模型空间长距离依赖，基于HEC-RAS生成的特征向量完成训练和预测。

Result: 模型在密西西比河流域的测试中取得了显著成果，以0.31英尺的中位绝对误差达到了高精度预测，比传统计算加速了近3.5倍。

Conclusion: 基于数据驱动的深度学习替代方案能够成为传统水力模型的高效替代，促进大规模洪水集合预报的计算可行性。

Abstract: Physics-based solvers like HEC-RAS provide high-fidelity river forecasts but
are too computationally intensive for on-the-fly decision-making during flood
events. The central challenge is to accelerate these simulations without
sacrificing accuracy. This paper introduces a deep learning surrogate that
treats HEC-RAS not as a solver but as a data-generation engine. We propose a
hybrid, auto-regressive architecture that combines a Gated Recurrent Unit (GRU)
to capture short-term temporal dynamics with a Geometry-Aware Fourier Neural
Operator (Geo-FNO) to model long-range spatial dependencies along a river
reach. The model learns underlying physics implicitly from a minimal
eight-channel feature vector encoding dynamic state, static geometry, and
boundary forcings extracted directly from native HEC-RAS files. Trained on 67
reaches of the Mississippi River Basin, the surrogate was evaluated on a
year-long, unseen hold-out simulation. Results show the model achieves a strong
predictive accuracy, with a median absolute stage error of 0.31 feet.
Critically, for a full 67-reach ensemble forecast, our surrogate reduces the
required wall-clock time from 139 minutes to 40 minutes, a speedup of nearly
3.5 times over the traditional solver. The success of this data-driven approach
demonstrates that robust feature engineering can produce a viable, high-speed
replacement for conventional hydraulic models, improving the computational
feasibility of large-scale ensemble flood forecasting.

</details>


### [401] [Towards Explainable Anomaly Detection in Shared Mobility Systems](https://arxiv.org/abs/2507.15643)
*Elnur Isgandarov,Matteo Cederle,Federico Chiariotti,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 该论文提出了一种可解释的异常检测框架，用于共享移动系统中的异常识别。


<details>
  <summary>Details</summary>
Motivation: 提升共享移动系统的运行优化、服务可靠性及用户体验。

Method: 通过整合多源数据（包括共享单车记录、天气条件和公共交通可用性），使用Isolation Forest算法进行无监督异常检测，并结合DIFFI算法实现解释性分析。

Result: 站点级分析提供了对异常的全面理解，发现外部因素（如恶劣天气和有限的交通供应）对异常的影响显著。

Conclusion: 研究有助于改善共享移动运营中的决策过程。

Abstract: Shared mobility systems, such as bike-sharing networks, play a crucial role
in urban transportation. Identifying anomalies in these systems is essential
for optimizing operations, improving service reliability, and enhancing user
experience. This paper presents an interpretable anomaly detection framework
that integrates multi-source data, including bike-sharing trip records, weather
conditions, and public transit availability. The Isolation Forest algorithm is
employed for unsupervised anomaly detection, along with the Depth-based
Isolation Forest Feature Importance (DIFFI) algorithm providing
interpretability. Results show that station-level analysis offers a robust
understanding of anomalies, highlighting the influence of external factors such
as adverse weather and limited transit availability. Our findings contribute to
improving decision-making in shared mobility operations.

</details>


### [402] [GeoHNNs: Geometric Hamiltonian Neural Networks](https://arxiv.org/abs/2507.15678)
*Amine Mohamed Aboussalah,Abdessalam Ed-dib*

Main category: cs.LG

TL;DR: 本文提出了Geometric Hamiltonian Neural Networks (GeoHNN)，通过显式编码物理定律中的几何先验来学习动态。


<details>
  <summary>Details</summary>
Motivation: 目前的机器学习方法往往忽略了物理定律中的几何本质，导致对高维混沌系统的长时间模拟不稳定。

Method: GeoHNN框架结合了惯性几何（通过对称正定矩阵参数化惯性矩阵）和相空间辛几何（通过约束自动编码器保持相空间体积）。

Result: 实验结果表明，GeoHNN在多种物理系统中显著优于现有模型，在长期稳定性、精度和能量守恒性上表现卓越。

Conclusion: 该研究确认了将物理几何嵌入到机器学习模型中的重要性，表明其对构建稳健和可推广的物理世界模型具有必要性。

Abstract: The fundamental laws of physics are intrinsically geometric, dictating the
evolution of systems through principles of symmetry and conservation. While
modern machine learning offers powerful tools for modeling complex dynamics
from data, common methods often ignore this underlying geometric fabric.
Physics-informed neural networks, for instance, can violate fundamental
physical principles, leading to predictions that are unstable over long
periods, particularly for high-dimensional and chaotic systems. Here, we
introduce \textit{Geometric Hamiltonian Neural Networks (GeoHNN)}, a framework
that learns dynamics by explicitly encoding the geometric priors inherent to
physical laws. Our approach enforces two fundamental structures: the Riemannian
geometry of inertia, by parameterizing inertia matrices in their natural
mathematical space of symmetric positive-definite matrices, and the symplectic
geometry of phase space, using a constrained autoencoder to ensure the
preservation of phase space volume in a reduced latent space. We demonstrate
through experiments on systems ranging from coupled oscillators to
high-dimensional deformable objects that GeoHNN significantly outperforms
existing models. It achieves superior long-term stability, accuracy, and energy
conservation, confirming that embedding the geometry of physics is not just a
theoretical appeal but a practical necessity for creating robust and
generalizable models of the physical world.

</details>


### [403] [Explainable Anomaly Detection for Electric Vehicles Charging Stations](https://arxiv.org/abs/2507.15718)
*Matteo Cederle,Andrea Mazzucco,Andrea Demartini,Eugenio Mazza,Eugenia Suriani,Federico Vitti,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 提出了一种结合无监督异常检测和可解释人工智能的方法，用于检测电动车充电基础设施中的异常并分析原因。


<details>
  <summary>Details</summary>
Motivation: 通过异常检测保障电动车充电站的可靠性和效率，并明确异常的根本原因。

Method: 使用 Isolation Forest 进行异常检测，结合 DIFFI 方法分析最重要的异常特征。

Result: 采用实际传感器和充电数据验证了所提出方法的有效性，并在实际工业案例中进行了评估。

Conclusion: 该方法能够有效检测异常并解释异常背后的原因，为电动车充电系统的维护提供支持。

Abstract: Electric vehicles (EV) charging stations are one of the critical
infrastructures needed to support the transition to renewable-energy-based
mobility, but ensuring their reliability and efficiency requires effective
anomaly detection to identify irregularities in charging behavior. However, in
such a productive scenario, it is also crucial to determine the underlying
cause behind the detected anomalies. To achieve this goal, this study
investigates unsupervised anomaly detection techniques for EV charging
infrastructure, integrating eXplainable Artificial Intelligence techniques to
enhance interpretability and uncover root causes of anomalies.
  Using real-world sensors and charging session data, this work applies
Isolation Forest to detect anomalies and employs the Depth-based Isolation
Forest Feature Importance (DIFFI) method to identify the most important
features contributing to such anomalies. The efficacy of the proposed approach
is evaluated in a real industrial case.

</details>


### [404] [Competitive Algorithms for Cooperative Multi-Agent Ski-Rental Problems](https://arxiv.org/abs/2507.15727)
*Xuchuang Wang,Bo Sun,Hedyeh Beyhaghi,John C. S. Lui,Mohammad Hajiesmaili,Adam Wierman*

Main category: cs.LG

TL;DR: 本论文扩展了经典滑雪租赁问题到多代理情况下，分析个人与共享成本的决策模型。


<details>
  <summary>Details</summary>
Motivation: 为了解决多代理决策中的个体与群体成本平衡问题及不确定性带来的影响。

Method: 设计三种竞争比（overall, state-dependent, individual rational），并提出最优的确定性和随机性策略，包括状态感知的阈值函数与分布。

Result: 对称性策略优于非对称性策略，提供了竞争比上下界，并扩展了滑雪租赁模型至多代理场景。

Conclusion: 研究对多代理决策中的不确定性和群体决策有理论和实践意义。

Abstract: This paper introduces a novel multi-agent ski-rental problem that generalizes
the classical ski-rental dilemma to a group setting where agents incur
individual and shared costs. In our model, each agent can either rent at a
fixed daily cost, or purchase a pass at an individual cost, with an additional
third option of a discounted group pass available to all. We consider scenarios
in which agents' active days differ, leading to dynamic states as agents drop
out of the decision process. To address this problem from different
perspectives, we define three distinct competitive ratios: overall,
state-dependent, and individual rational. For each objective, we design and
analyze optimal deterministic and randomized policies. Our deterministic
policies employ state-aware threshold functions that adapt to the dynamic
states, while our randomized policies sample and resample thresholds from
tailored state-aware distributions. The analysis reveals that symmetric
policies, in which all agents use the same threshold, outperform asymmetric
ones. Our results provide competitive ratio upper and lower bounds and extend
classical ski-rental insights to multi-agent settings, highlighting both
theoretical and practical implications for group decision-making under
uncertainty.

</details>


### [405] [Multi-Modal Sensor Fusion for Proactive Blockage Prediction in mmWave Vehicular Networks](https://arxiv.org/abs/2507.15769)
*Ahmad M. Nazar,Abdulkadir Celik,Mohamed Y. Selim,Asmaa Abdallah,Daji Qiao,Ahmed M. Eltawil*

Main category: cs.LG

TL;DR: 该论文提出了一个利用多模态感知（如相机、GPS、LiDAR和雷达）的毫米波通信遮挡预测框架，并通过实验验证了其在动态环境中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 毫米波通信易受动态遮挡物的阻碍，如车辆、行人和基础设施，严重影响通信性能。

Method: 采用一个多模态感知框架，通过相机、GPS、LiDAR和雷达独立处理各传感器数据，并结合软加权集成策略，以提高遮挡预测的准确性。

Result: 相机单独模型取得了97.1%的F1分数（推理时间89.8ms），相机+雷达配置进一步提升至97.2% F1分数（推理时间95.7ms）。

Conclusion: 多模态感知是毫米波遮挡预测的高效方法，为动态环境中的无线通信提供了新方向。

Abstract: Vehicular communication systems operating in the millimeter wave (mmWave)
band are highly susceptible to signal blockage from dynamic obstacles such as
vehicles, pedestrians, and infrastructure. To address this challenge, we
propose a proactive blockage prediction framework that utilizes multi-modal
sensing, including camera, GPS, LiDAR, and radar inputs in an
infrastructure-to-vehicle (I2V) setting. This approach uses modality-specific
deep learning models to process each sensor stream independently and fuses
their outputs using a softmax-weighted ensemble strategy based on validation
performance. Our evaluations, for up to 1.5s in advance, show that the
camera-only model achieves the best standalone trade-off with an F1-score of
97.1% and an inference time of 89.8ms. A camera+radar configuration further
improves accuracy to 97.2% F1 at 95.7ms. Our results display the effectiveness
and efficiency of multi-modal sensing for mmWave blockage prediction and
provide a pathway for proactive wireless communication in dynamic environments.

</details>


### [406] [Deep-Learning Investigation of Vibrational Raman Spectra for Plant-Stress Analysis](https://arxiv.org/abs/2507.15772)
*Anoop C. Patil,Benny Jian Rong Sng,Yu-Wei Chang,Joana B. Pereira,Chua Nam-Hai,Rajani Sarojam,Gajendra Pratap Singh,In-Cheol Jang,Giovanni Volpe*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的自动化方法DIVA，用于分析拉曼光谱检测植物压力，可省去繁琐的手动预处理过程，适合多种压力条件。


<details>
  <summary>Details</summary>
Motivation: 植物压力检测对农业健康及病害早期检测至关重要，传统拉曼分析流程复杂且容易引入偏差，亟需一种高效、自动化的解决方案。

Method: 提出了一种基于变分自编码器(VAE)的自动化深度学习方法DIVA，可以直接处理未经过手动预处理的拉曼光谱，还能精准识别和量化重要光谱特征。

Result: DIVA成功用于检测多种植物压力，包括非生物胁迫（比如遮荫、高光、高温）和生物胁迫（比如细菌感染），实现了高效且准确的分析。

Conclusion: 通过将深度学习与振动光谱技术相结合，DIVA为依靠AI的植物健康监测提供了新的可能性，有助于实现更具韧性和可持续性的农业实践。

Abstract: Detecting stress in plants is crucial for both open-farm and
controlled-environment agriculture. Biomolecules within plants serve as key
stress indicators, offering vital markers for continuous health monitoring and
early disease detection. Raman spectroscopy provides a powerful, non-invasive
means to quantify these biomolecules through their molecular vibrational
signatures. However, traditional Raman analysis relies on customized
data-processing workflows that require fluorescence background removal and
prior identification of Raman peaks of interest-introducing potential biases
and inconsistencies. Here, we introduce DIVA (Deep-learning-based Investigation
of Vibrational Raman spectra for plant-stress Analysis), a fully automated
workflow based on a variational autoencoder. Unlike conventional approaches,
DIVA processes native Raman spectra-including fluorescence backgrounds-without
manual preprocessing, identifying and quantifying significant spectral features
in an unbiased manner. We applied DIVA to detect a range of plant stresses,
including abiotic (shading, high light intensity, high temperature) and biotic
stressors (bacterial infections). By integrating deep learning with vibrational
spectroscopy, DIVA paves the way for AI-driven plant health assessment,
fostering more resilient and sustainable agricultural practices.

</details>


### [407] [Dynamics is what you need for time-series forecasting!](https://arxiv.org/abs/2507.15774)
*Alexis-Raja Brachet,Pierre-Yves Richard,Céline Hudelot*

Main category: cs.LG

TL;DR: 该论文探讨了时间序列预测模型中动态学习的重要性，提出PRO-DYN框架，并通过实验证实了动态模块的重要性和位置对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在时间序列预测任务中仍面临简单模型的挑战，作者推测问题在于模型未能充分学习数据的底层动态规律。

Method: 开发了一个新颖的PRO-DYN命名法，从动态的视角分析现有模型，并通过对多种不同架构模型的系统性和实证研究验证假设。

Result: 实验表明：未充分学习动态的架构表现不佳，动态模块位置设置在模型末端能显著提高表现。

Conclusion: 加入可学习的动态模块并将其定位于作为最终预测器是提升时间序列预测模型性能的关键。

Abstract: While boundaries between data modalities are vanishing, the usual successful
deep models are still challenged by simple ones in the time-series forecasting
task. Our hypothesis is that this task needs models that are able to learn the
data underlying dynamics. We propose to validate it through both systemic and
empirical studies. We develop an original $\texttt{PRO-DYN}$ nomenclature to
analyze existing models through the lens of dynamics. Two observations thus
emerged: $\textbf{1}$. under-performing architectures learn dynamics at most
partially, $\textbf{2}$. the location of the dynamics block at the model end is
of prime importance. We conduct extensive experiments to confirm our
observations on a set of performance-varying models with diverse backbones.
Results support the need to incorporate a learnable dynamics block and its use
as the final predictor.

</details>


### [408] [Graph Attention Specialized Expert Fusion Model for Node Classification: Based on Cora and Pubmed Datasets](https://arxiv.org/abs/2507.15784)
*Zihang Ma,Qitian Yin*

Main category: cs.LG

TL;DR: 论文提出一种增强图神经网络分类效果的方法WR-EFM，提升了分类稳定性和Category 2的准确度。


<details>
  <summary>Details</summary>
Motivation: 观察到在图神经网络节点分类中，不同类别的分类表现差异显著，尤其是Category 2的效果较差，因此需要更优方法提升其性能。

Method: 设计了基于WR距离优化的专家融合模型WR-EFM，其中针对不同类别训练专门的模型，如采用多跳图注意力网络（GAT）来改善Category 2的表现，并通过自适应策略加权融合。

Result: WR-EFM在分类任务上实现了各类别的平衡表现，Category 2准确率提高了5.5%，并显著降低了稳定性系数（CV）。

Conclusion: 通过WR距离指导的融合方法，能够有效捕获复杂的结构模式，对处理类别不平衡的图分类任务提供了一种新范式。

Abstract: Graph node classification is a fundamental task in graph neural networks
(GNNs), aiming to assign predefined class labels to nodes. On the PubMed
citation network dataset, we observe significant classification difficulty
disparities, with Category 2 achieving only 74.4% accuracy in traditional GCN,
7.5% lower than Category 1. To address this, we propose a
Wasserstein-Rubinstein (WR) distance enhanced Expert Fusion Model (WR-EFM),
training specialized GNN models for Categories 0/1 (with layer normalization
and residual connections) and Multi-hop Graph Attention Networks (GAT) for
Category 2. The WR distance metric optimizes representation similarity between
models, particularly focusing on improving Category 2 performance. Our adaptive
fusion strategy dynamically weights models based on category-specific
performance, with Category 2 assigned a GAT weight of 0.8. WR distance further
guides the fusion process by measuring distributional differences between model
representations, enabling more principled integration of complementary
features.
  Experimental results show WR-EFM achieves balanced accuracy across
categories: 77.8% (Category 0), 78.0% (Category 1), and 79.9% (Category 2),
outperforming both single models and standard fusion approaches. The
coefficient of variation (CV) of WR-EFM's category accuracies is 0.013, 77.6%
lower than GCN's 0.058, demonstrating superior stability. Notably, WR-EFM
improves Category 2 accuracy by 5.5% compared to GCN, verifying the
effectiveness of WR-guided fusion in capturing complex structural patterns.
This work provides a novel paradigm for handling class-imbalanced graph
classification tasks. To promote the research community, we release our project
at https://github.com/s010m00n/GASEM4NC.

</details>


### [409] [Federated Split Learning with Improved Communication and Storage Efficiency](https://arxiv.org/abs/2507.15816)
*Yujia Mu,Cong Shen*

Main category: cs.LG

TL;DR: 提出了一种名为CSE-FSL的新方法，解决了联邦分裂学习(FSL)中存在的高通信和存储开销问题。


<details>
  <summary>Details</summary>
Motivation: 联邦分裂学习尽管降低了边缘设备的计算负担，但仍需高通信和存储开销，亟需解决这些瓶颈。

Method: 通过引入辅助网络、减少梯度传输以及选择性发送压缩数据降低通信与存储需求。方法还进行了理论收敛性分析。

Result: 实验证明该方法在实际联邦学习任务中显著减少了通信开销，并保持了性能。

Conclusion: CSE-FSL在降低通信和存储需求方面相比现有方法有显著优势，适合实际应用场景。

Abstract: Federated learning (FL) is one of the popular distributed machine learning
(ML) solutions but incurs significant communication and computation costs at
edge devices. Federated split learning (FSL) can train sub-models in parallel
and reduce the computational burden of edge devices by splitting the model
architecture. However, it still requires a high communication overhead due to
transmitting the smashed data and gradients between clients and the server in
every global round. Furthermore, the server must maintain separate partial
models for every client, leading to a significant storage requirement. To
address these challenges, this paper proposes a novel communication and storage
efficient federated split learning method, termed CSE-FSL, which utilizes an
auxiliary network to locally update the weights of the clients while keeping a
single model at the server, hence avoiding frequent transmissions of gradients
from the server and greatly reducing the storage requirement of the server.
Additionally, a new model update method of transmitting the smashed data in
selected epochs can reduce the amount of smashed data sent from the clients. We
provide a theoretical analysis of CSE-FSL, rigorously guaranteeing its
convergence under non-convex loss functions. The extensive experimental results
further indicate that CSE-FSL achieves a significant communication reduction
over existing FSL solutions using real-world FL tasks.

</details>


### [410] [Multi-Strategy Improved Snake Optimizer Accelerated CNN-LSTM-Attention-Adaboost for Trajectory Prediction](https://arxiv.org/abs/2507.15832)
*Shiyang Li*

Main category: cs.LG

TL;DR: 本文提出了一种结合多策略改进的蛇群优化（SO）算法的混合CNN-LSTM-attention-adaboost神经网络模型，用以改进中长期4D轨迹预测效果。


<details>
  <summary>Details</summary>
Motivation: 现有中长期4D轨迹预测模型存在局限，需开发一种新模型提高预测精度。

Method: 模型结合Adaboost分配弱学习器，利用CNN提取空间特征、LSTM捕捉时序特征和注意力机制捕捉全局特征，同时引入改进SO算法优化模型超参数。

Result: 基于真实ADS-B数据实验表明，该模型比传统优化器（如粒子群、鲸鱼和灰狼算法）表现更优，预测精度提升39.89%。

Conclusion: SO-CLA-adaboost模型显著提高了大规模高维轨迹数据的预测效果，是解决该类问题的有效方法。

Abstract: To address the limitations of medium- and long-term four-dimensional (4D)
trajectory prediction models, this paper proposes a hybrid
CNN-LSTM-attention-adaboost neural network model incorporating a multi-strategy
improved snake-herd optimization (SO) algorithm. The model applies the Adaboost
algorithm to divide multiple weak learners, and each submodel utilizes CNN to
extract spatial features, LSTM to capture temporal features, and attention
mechanism to capture global features comprehensively. The strong learner model,
combined with multiple sub-models, then optimizes the hyperparameters of the
prediction model through the natural selection behavior pattern simulated by
SO. In this study, based on the real ADS-B data from Xi'an to Tianjin, the
comparison experiments and ablation studies of multiple optimizers are carried
out, and a comprehensive test and evaluation analysis is carried out. The
results show that SO-CLA-adaboost outperforms traditional optimizers such as
particle swarm, whale, and gray wolf in handling large-scale high-dimensional
trajectory data. In addition, introducing the full-strategy collaborative
improvement SO algorithm improves the model's prediction accuracy by 39.89%.

</details>


### [411] [Optimizing Canaries for Privacy Auditing with Metagradient Descent](https://arxiv.org/abs/2507.15836)
*Matteo Boglioni,Terrance Liu,Andrew Ilyas,Zhiwei Steven Wu*

Main category: cs.LG

TL;DR: 本文研究了一种称为“黑箱隐私审计”的方法，旨在通过优化审计器的“金丝雀”示例集来提高深度学习模型的隐私参数下界估计。


<details>
  <summary>Details</summary>
Motivation: 当前常用的差分隐私深度学习训练方法（如DP-SGD）存在真实隐私参数难以精确评估的问题，需要开发更加高效的方法改进隐私审计的性能。

Method: 提出了一种基于元梯度优化的技术，用以优化审计器的“金丝雀”示例集，从而提高隐私审计的准确性和效率。

Result: 通过优化后的金丝雀示例，差分隐私图像分类模型的隐私参数的下界能够在某些情况下提高超过2倍。此外，这种优化方法在不同模型及训练方法间具有可迁移性和效率。

Conclusion: 优化审计器的金丝雀示例可以显著改善隐私参数估计的精度，证明了该方法在差分隐私审计场景下的有效性。

Abstract: In this work we study black-box privacy auditing, where the goal is to lower
bound the privacy parameter of a differentially private learning algorithm
using only the algorithm's outputs (i.e., final trained model). For DP-SGD (the
most successful method for training differentially private deep learning
models), the canonical approach auditing uses membership inference-an auditor
comes with a small set of special "canary" examples, inserts a random subset of
them into the training set, and then tries to discern which of their canaries
were included in the training set (typically via a membership inference
attack). The auditor's success rate then provides a lower bound on the privacy
parameters of the learning algorithm. Our main contribution is a method for
optimizing the auditor's canary set to improve privacy auditing, leveraging
recent work on metagradient optimization. Our empirical evaluation demonstrates
that by using such optimized canaries, we can improve empirical lower bounds
for differentially private image classification models by over 2x in certain
instances. Furthermore, we demonstrate that our method is transferable and
efficient: canaries optimized for non-private SGD with a small model
architecture remain effective when auditing larger models trained with DP-SGD.

</details>


### [412] [FASTGEN: Fast and Cost-Effective Synthetic Tabular Data Generation with LLMs](https://arxiv.org/abs/2507.15839)
*Anh Nguyen,Sam Schafft,Nicholas Hale,John Alfaro*

Main category: cs.LG

TL;DR: 本文提出了一种快速且经济高效的方式，利用大语言模型生成现实的表格数据，同时显著降低了生成大规模合成数据的时间和成本。


<details>
  <summary>Details</summary>
Motivation: 现实数据的收集和使用常因成本和稀缺性受到限制，合成数据生成成为解决这一问题的关键方法。

Method: 通过大语言模型推理和编码字段分布，生成可复用的采样脚本，同时将字段自动分类为数值、类别或自由文本类型，从而高效生成多样化和真实感强的数据集。

Result: 实验结果表明，与传统方法相比，该方法在数据多样性和真实性方面更优，并显著降低了生成大量合成数据的负担。

Conclusion: 本方法可加速生产流水线中的测试过程，缩短开发周期，提高系统效率，为研究者和实践者提供了一种可扩展且经济高效的合成数据解决方案。

Abstract: Synthetic data generation has emerged as an invaluable solution in scenarios
where real-world data collection and usage are limited by cost and scarcity.
Large language models (LLMs) have demonstrated remarkable capabilities in
producing high-fidelity, domain-relevant samples across various fields.
However, existing approaches that directly use LLMs to generate each record
individually impose prohibitive time and cost burdens, particularly when large
volumes of synthetic data are required. In this work, we propose a fast,
cost-effective method for realistic tabular data synthesis that leverages LLMs
to infer and encode each field's distribution into a reusable sampling script.
By automatically classifying fields into numerical, categorical, or free-text
types, the LLM generates distribution-based scripts that can efficiently
produce diverse, realistic datasets at scale without continuous model
inference. Experimental results show that our approach outperforms traditional
direct methods in both diversity and data realism, substantially reducing the
burden of high-volume synthetic data generation. We plan to apply this
methodology to accelerate testing in production pipelines, thereby shortening
development cycles and improving overall system efficiency. We believe our
insights and lessons learned will aid researchers and practitioners seeking
scalable, cost-effective solutions for synthetic data generation.

</details>


### [413] [Diffusion Beats Autoregressive in Data-Constrained Settings](https://arxiv.org/abs/2507.15857)
*Mihir Prabhudesai,Menging Wu,Amir Zadeh,Katerina Fragkiadaki,Deepak Pathak*

Main category: cs.LG

TL;DR: 该论文研究了在数据受限的情况下，基于掩码的扩散语言模型相较于自回归（AR）模型的表现，发现扩散模型在数据匮乏但计算资源丰富时表现优越。


<details>
  <summary>Details</summary>
Motivation: 目前AR模型在大规模语言模型上占据主导地位，但扩散模型作为替代方案的潜力尚未被深入探索，特别是在数据有限的情况下。

Method: 系统研究了掩码扩散模型在数据受限条件下的性能，通过分析其训练表现和下游任务效果，与AR模型进行对比，并推导出扩散模型超越AR模型的计算阈值公式。

Result: 当计算资源充足但数据稀缺时，扩散模型相较AR模型表现更优，能够更好地利用重复数据，表现为更低的验证损失和出色的下游任务性能。

Conclusion: 扩散模型通过多样化的令牌排序和预测任务实现隐式数据增强，在数据成为瓶颈时，是AR模型的有力替代方案。

Abstract: Autoregressive (AR) models have long dominated the landscape of large
language models, driving progress across a wide range of tasks. Recently,
diffusion-based language models have emerged as a promising alternative, though
their advantages over AR models remain underexplored. In this paper, we
systematically study masked diffusion models in data-constrained settings-where
training involves repeated passes over limited data-and find that they
significantly outperform AR models when compute is abundant but data is scarce.
Diffusion models make better use of repeated data, achieving lower validation
loss and superior downstream performance. We interpret this advantage as
implicit data augmentation: masked diffusion exposes the model to a diverse
distribution of token orderings and prediction tasks, unlike AR's fixed
left-to-right factorization. We find new scaling laws for diffusion models and
derive a closed-form expression for the critical compute threshold at which
diffusion begins to outperform AR. These results suggest that when data, not
compute, is the bottleneck, diffusion models offer a compelling alternative to
the standard AR paradigm. Our code is available at:
https://diffusion-scaling.github.io.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [414] [APTx Neuron: A Unified Trainable Neuron Architecture Integrating Activation and Computation](https://arxiv.org/abs/2507.14270)
*Ravin Kumar*

Main category: cs.NE

TL;DR: 提出了一种称为APTx Neuron的新型统一神经计算单元，将非线性激活和线性变换集成为一个可训练的表达式。


<details>
  <summary>Details</summary>
Motivation: 希望通过创新的神经元设计，提高计算效率和模型表现能力。

Method: 设计了融合非线性激活和线性变换的功能表达式，并在MNIST数据集上进行了实验验证。

Result: 在MNIST数据集上实现了96.69%的测试准确率，只需20个训练周期和约332K的可训练参数。

Conclusion: APTx Neuron展现了传统神经元无法比拟的表现力和计算效率，为神经元设计和架构提供了一种新范式。

Abstract: We propose the APTx Neuron, a novel, unified neural computation unit that
integrates non-linear activation and linear transformation into a single
trainable expression. The APTx Neuron is derived from the APTx activation
function, thereby eliminating the need for separate activation layers and
making the architecture both computationally efficient and elegant. The
proposed neuron follows the functional form $y = \sum_{i=1}^{n} ((\alpha_i +
\tanh(\beta_i x_i)) \cdot \gamma_i x_i) + \delta$, where all parameters
$\alpha_i$, $\beta_i$, $\gamma_i$, and $\delta$ are trainable. We validate our
APTx Neuron-based architecture on the MNIST dataset, achieving up to 96.69\%
test accuracy in just 20 epochs using approximately 332K trainable parameters.
The results highlight the superior expressiveness and computational efficiency
of the APTx Neuron compared to traditional neurons, pointing toward a new
paradigm in unified neuron design and the architectures built upon it.

</details>


### [415] [Training oscillator Ising machines to assign the dynamic stability of their equilibrium points](https://arxiv.org/abs/2507.14386)
*Yi Cheng,Zongli Lin*

Main category: cs.NE

TL;DR: 本文提出了一种新的神经网络模型，通过适当分配其平衡点（EPs）的稳定性，实现了类似Hopfield网络的关联记忆功能。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是优化Hopfield模型中如何设计神经元之间的耦合权重，以实现既定模式的存储，同时解决现有模型在平衡点动态稳定性问题上的不足。

Method: 提出了振荡器伊辛机（OIM）作为模型，通过引入哈密顿正则化特征值对比法（HRECM）来设计耦合权重，从而为OIM的平衡点赋予适当的稳定性。

Result: 实验结果验证了HRECM方法能够有效地训练OIM的耦合权重，使平衡点稳定性得到优化。

Conclusion: 基于HRECM方法的OIM模型为关联记忆提供了一种新的途径，同时简化了权重设计过程中的动态稳定性考虑。

Abstract: We propose a neural network model, which, with appropriate assignment of the
stability of its equilibrium points (EPs), achieves Hopfield-like associative
memory. The oscillator Ising machine (OIM) is an ideal candidates for such a
model, as all its $0/\pi$ binary EPs are structurally stable with their dynamic
stability tunable by the coupling weights. Traditional Hopfield-based models
store the desired patterns by designing the coupling weights between neurons.
The design of coupling weights should simultaneously take into account both the
existence and the dynamic stability of the EPs for the storage of the desired
patterns. For OIMs, since all $0/\pi$ binary EPs are structurally stable, the
design of the coupling weights needs only to focus on assigning appropriate
stability for the $0/\pi$ binary EPs according to the desired patterns. In this
paper, we establish a connection between the stability and the Hamiltonian
energy of EPs for OIMs, and, based on this connection, provide a
Hamiltonian-Regularized Eigenvalue Contrastive Method (HRECM) to train the
coupling weights of OIMs for assigning appropriate stability to their EPs.
Finally, numerical experiments are performed to validate the effectiveness of
the proposed method.

</details>


### [416] [Analyzing Internal Activity and Robustness of SNNs Across Neuron Parameter Space](https://arxiv.org/abs/2507.14757)
*Szymon Mazurek,Jakub Caputa,Maciej Wielgosz*

Main category: cs.NE

TL;DR: 本文探讨尖峰神经网络（SNNs）在超参数调节对性能的关键影响，通过研究神经元参数空间的操作区域，优化准确性和能耗平衡。


<details>
  <summary>Details</summary>
Motivation: 尖峰神经网络因其能量效率高及生物仿真特性成为人工神经网络的替代选择，但其性能严重依赖于神经元模型参数的优化，这驱动了本文研究。

Method: 通过系统地探索神经元超参数（如膜时间常数和电压阈值），发现有效操作空间并量化其特性，同时分析在鲁棒性和对抗扰动下的表现。

Result: 揭示了在特定操作空间内可实现分类准确性和能量消耗的最佳平衡，超出该区域会引发能量过度消耗或网络沉默，并描述了网络对噪声的响应变化。

Conclusion: 研究提供了调节尖峰神经网络超参数的重要指导方针，增强其鲁棒性及能效，特别适用于类脑计算方案的实施。

Abstract: Spiking Neural Networks (SNNs) offer energy-efficient and biologically
plausible alternatives to traditional artificial neural networks, but their
performance depends critically on the tuning of neuron model parameters. In
this work, we identify and characterize an operational space - a constrained
region in the neuron hyperparameter domain (specifically membrane time constant
tau and voltage threshold vth) - within which the network exhibits meaningful
activity and functional behavior. Operating inside this manifold yields optimal
trade-offs between classification accuracy and spiking activity, while stepping
outside leads to degeneration: either excessive energy use or complete network
silence.
  Through systematic exploration across datasets and architectures, we
visualize and quantify this manifold and identify efficient operating points.
We further assess robustness to adversarial noise, showing that SNNs exhibit
increased spike correlation and internal synchrony when operating outside their
optimal region. These findings highlight the importance of principled
hyperparameter tuning to ensure both task performance and energy efficiency.
Our results offer practical guidelines for deploying robust and efficient SNNs,
particularly in neuromorphic computing scenarios.

</details>


### [417] [DHEvo: Data-Algorithm Based Heuristic Evolution for Generalizable MILP Solving](https://arxiv.org/abs/2507.15615)
*Zhihao Zhang,Siyuan Li,Chenxi Li,Feifan Liu,Mengjing Chen,Kai Li,Tao Zhong,Bo An,Peng Liu*

Main category: cs.NE

TL;DR: 本文提出了一种数据-算法协同演化框架DHEvo，用于提升混合整数线性规划（MILP）中的初始启发式方法在整个问题类上的适应性，实验表明其显著优于手工设计及现有基于LLM的方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的启发式生成方法未能捕捉某问题类中的实例特性，导致在问题类中的泛化能力较差，亟需提升适应性与性能。

Method: 提出数据-算法协同演化框架DHEvo，通过一个以LLM为基础的多代理系统生成数据与代码对，并迭代优化以识别最优的启发式方法。

Result: 实验表明，DHEvo在多个MILP基准上均优于传统的人工设计启发式方法和现有LLM方法，性能得到显著提升。

Conclusion: 通过将实例特性融入到启发式算法生成过程中，DHEvo提供了一种高效泛化的方法，可在问题类中发挥更好的求解性能。

Abstract: Primal heuristics play a critical role in improving the efficiency of mixed
integer programming (MILP) solvers. As large language models (LLMs) have
demonstrated superior code generation abilities, recent MILP works are devoted
to leveraging the evolutionary computation approaches with LLMs to generate
effective primal heuristics. Although the generated heuristics have achieved
better solving performance than the hand-crafted ones with little adaptability,
the advantage of current LLM-based methods is limited to few MILP instances in
one problem class, as they fail to capture the instance characteristics in the
problem class (the MILP instances generated from the same mathematical model
are defined as a problem class). Since MILP instances often differ
significantly in structure and feature distribution, the neglect of their
characteristics in the evolution process results in poor generalization within
the same problem class. To overcome this challenge, we propose a data-algorithm
co-evolution framework (DHEvo) that iteratively selects representative
instances and evolves corresponding heuristics. With the initial instance
distribution, we develop an LLM-based multi-agent system to generate data-code
pairs simultaneously. These data-code pairs are iteratively refined based on
their fitness scores, leading to the identification of the most effective
heuristic over the entire problem class. Extensive experiments across diverse
MILP benchmarks demonstrate that our approach significantly outperforms both
human-designed heuristics and existing LLM-based methods.

</details>


### [418] [TONUS: Neuromorphic human pose estimation for artistic sound co-creation](https://arxiv.org/abs/2507.15734)
*Jules Lecomte,Konrad Zinner,Michael Neumeier,Axel von Arnim*

Main category: cs.NE

TL;DR: 本文提出一种艺术声音装置，利用神经形态体感技术，通过访客的身体动作与机器共同创造声音景观。


<details>
  <summary>Details</summary>
Motivation: 人机交互常因技术性过强而缺乏吸引力，新技术在艺术领域未被充分探索，同时需要更无缝的感知和表现连接。

Method: 设计了一种神经形态的多头人体姿态估计神经传感器，结合尖峰神经网络和神经形态芯片，实现通过微妙身体动作直接控制声音景观和视觉输出。

Result: 访客可以通过这种装置沉浸在声音和神经感知的自我表现中，与能神经思考的机器互动。

Conclusion: 所提出的装置实现了人类和机器间的非侵入式艺术交互，提升人类想象力和艺术潜力的表达。

Abstract: Human machine interaction is a huge source of inspiration in today's media
art and digital design, as machines and humans merge together more and more.
Its place in art reflects its growing applications in industry, such as
robotics. However, those interactions often remains too technical and
machine-driven for people to really engage into. On the artistic side, new
technologies are often not explored in their full potential and lag a bit
behind, so that state-of-the-art research does not make its way up to museums
and exhibitions. Machines should support people's imagination and poetry in a
seamless interface to their body or soul. We propose an artistic sound
installation featuring neuromorphic body sensing to support a direct yet non
intrusive interaction with the visitor with the purpose of creating sound
scapes together with the machine. We design a neuromorphic multihead human pose
estimation neural sensor that shapes sound scapes and visual output with fine
body movement control. In particular, the feature extractor is a spiking neural
network tailored for a dedicated neuromorphic chip. The visitor, immersed in a
sound atmosphere and a neurally processed representation of themselves that
they control, experience the dialogue with a machine that thinks neurally,
similarly to them.

</details>
