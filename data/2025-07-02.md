<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 99]
- [cs.CL](#cs.CL) [Total: 42]
- [cs.AI](#cs.AI) [Total: 19]
- [cs.LG](#cs.LG) [Total: 99]
- [cs.NE](#cs.NE) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Moment Sampling in Video LLMs for Long-Form Video QA](https://arxiv.org/abs/2507.00033)
*Mustafa Chasmai,Gauri Jagatap,Gouthaman KV,Grant Van Horn,Subhransu Maji,Andrea Fanelli*

Main category: cs.CV

TL;DR: 当前视频大语言模型（Video LLMs）在短视频中表现良好，但在长视频的长程推理中表现受限。本文提出了一种新型“时刻采样”方法，利用文本到视频的时刻检索模型指导帧采样过程，以提升长视频问答的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要采用均匀帧采样处理长视频，容易导致关键帧丢失或冗余帧选择，进而降低回答准确性并增加计算成本。

Method: 提出了一种基于文本到视频时刻检索模型的“时刻采样”方法，用于根据问题上下文选择最相关的视频帧，从而优化帧样本选择过程。

Result: 在四个长视频问答数据集和四个顶尖的Video LLMs上进行实验，验证了该方法在提升长视频问答性能上的有效性。

Conclusion: “时刻采样”方法有效提升了长视频问答的性能，是解决长视频建模问题的实用新工具。

Abstract: Recent advancements in video large language models (Video LLMs) have
significantly advanced the field of video question answering (VideoQA). While
existing methods perform well on short videos, they often struggle with
long-range reasoning in longer videos. To scale Video LLMs for longer video
content, frame sub-sampling (selecting frames at regular intervals) is commonly
used. However, this approach is suboptimal, often leading to the loss of
crucial frames or the inclusion of redundant information from multiple similar
frames. Missing key frames impairs the model's ability to answer questions
accurately, while redundant frames lead the model to focus on irrelevant video
segments and increase computational resource consumption. In this paper, we
investigate the use of a general-purpose text-to-video moment retrieval model
to guide the frame sampling process. We propose "moment sampling", a novel,
model-agnostic approach that enables the model to select the most relevant
frames according to the context of the question. Specifically, we employ a
lightweight moment retrieval model to prioritize frame selection. By focusing
on the frames most pertinent to the given question, our method enhances
long-form VideoQA performance in Video LLMs. Through extensive experiments on
four long-form VideoQA datasets, using four state-of-the-art Video LLMs, we
demonstrate the effectiveness of the proposed approach.

</details>


### [2] [Catastrophic Forgetting Mitigation via Discrepancy-Weighted Experience Replay](https://arxiv.org/abs/2507.00042)
*Xinrun Xu,Jianwen Yang,Qiuhong Zhang,Zhanbiao Lian,Zhiming Ding,Shan Jiang*

Main category: cs.CV

TL;DR: 提出了一种名为ER-EMU的算法，用于解决云边协作目标检测中因动态交通环境导致的边缘模型灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 解决动态交通环境中的边缘模型面临的灾难性遗忘问题，尤其是在历史知识仍具价值的情况下，如日/夜周期或高峰时段。

Method: 通过自适应经验重放的边缘模型更新算法ER-EMU结合多核最大均值差异(MK-MMD)与新颖的领域距离度量选择历史经验，配合随机采样策略维护平衡经验库。

Result: 在Bellevue交通视频数据集上，实验显示ER-EMU能在多次日/夜周期中持续提升多种先进云边协作目标检测框架的性能。

Conclusion: ER-EMU有效地在动态环境下减少灾难性遗忘，保留更广泛的历史知识，提高模型一般化能力。

Abstract: Continually adapting edge models in cloud-edge collaborative object detection
for traffic monitoring suffers from catastrophic forgetting, where models lose
previously learned knowledge when adapting to new data distributions. This is
especially problematic in dynamic traffic environments characterised by
periodic variations (e.g., day/night, peak hours), where past knowledge remains
valuable. Existing approaches like experience replay and visual prompts offer
some mitigation, but struggle to effectively prioritize and leverage historical
data for optimal knowledge retention and adaptation. Specifically, simply
storing and replaying all historical data can be inefficient, while treating
all historical experiences as equally important overlooks their varying
relevance to the current domain. This paper proposes ER-EMU, an edge model
update algorithm based on adaptive experience replay, to address these
limitations. ER-EMU utilizes a limited-size experience buffer managed using a
First-In-First-Out (FIFO) principle, and a novel Domain Distance Metric-based
Experience Selection (DDM-ES) algorithm. DDM-ES employs the multi-kernel
maximum mean discrepancy (MK-MMD) to quantify the dissimilarity between target
domains, prioritizing the selection of historical data that is most dissimilar
to the current target domain. This ensures training diversity and facilitates
the retention of knowledge from a wider range of past experiences, while also
preventing overfitting to the new domain. The experience buffer is also updated
using a simple random sampling strategy to maintain a balanced representation
of previous domains. Experiments on the Bellevue traffic video dataset,
involving repeated day/night cycles, demonstrate that ER-EMU consistently
improves the performance of several state-of-the-art cloud-edge collaborative
object detection frameworks.

</details>


### [3] [Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections](https://arxiv.org/abs/2507.00263)
*Vignesh Ram Nithin Kappagantula,Shayan Hassantabar*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法解决VR平台中房间场景发现与分组的问题，同时识别每间卧室组中的床型，以帮助游客更好地了解房产的空间布局。


<details>
  <summary>Details</summary>
Motivation: VR平台中上传了大量未分类的房产图片，影响游客对房产空间布局的理解，尤其在同一类型房间较多时，这个问题尤为突出。

Method: 提出了一种高效计算的机器学习管道，包含监督的房间类型检测模型、重叠检测模型、以及用于分组的聚类算法，并结合MLLM模型将卧室组与床型元数据进行匹配。

Result: 在分别评估管道中各个模型以及整体性能时，该方法的表现显著超越了对比学习和预训练嵌入聚类等已有方法。

Conclusion: 该研究为实时和数据稀缺环境提供了强劲解决方案，提高了房间场景分类的效率与准确性，为VR平台用户体验优化带来新契机。

Abstract: The rapid growth of vacation rental (VR) platforms has led to an increasing
volume of property images, often uploaded without structured categorization.
This lack of organization poses significant challenges for travelers attempting
to understand the spatial layout of a property, particularly when multiple
rooms of the same type are present. To address this issue, we introduce an
effective approach for solving the room scene discovery and grouping problem,
as well as identifying bed types within each bedroom group. This grouping is
valuable for travelers to comprehend the spatial organization, layout, and the
sleeping configuration of the property. We propose a computationally efficient
machine learning pipeline characterized by low latency and the ability to
perform effectively with sample-efficient learning, making it well-suited for
real-time and data-scarce environments. The pipeline integrates a supervised
room-type detection model, a supervised overlap detection model to identify the
overlap similarity between two images, and a clustering algorithm to group the
images of the same space together using the similarity scores. Additionally,
the pipeline maps each bedroom group to the corresponding bed types specified
in the property's metadata, based on the visual content present in the group's
images using a Multi-modal Large Language Model (MLLM) model. We evaluate the
aforementioned models individually and also assess the pipeline in its
entirety, observing strong performance that significantly outperforms
established approaches such as contrastive learning and clustering with
pretrained embeddings.

</details>


### [4] [MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast Representations](https://arxiv.org/abs/2507.00043)
*Mehmet Yigit Avci,Pedro Borges,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: 本文针对临床系统中MRI图像对比度理解的困难，提出一种基于多模态对比学习的方案（MR-CLIP），以自动生成对比度感知表征，且无需依赖手动标签。


<details>
  <summary>Details</summary>
Motivation: 当前MRI图像对比参数缺乏可靠的元数据，且手动标签缺失严重。因此，需要开发自动化且解读友好的方法来解决临床图像处理中的这一问题。

Method: MR-CLIP通过多模态对比学习的方式，将MRI图像与其元数据对齐，生成对比度感知表征。该方法可捕捉跨设备和协议的对比度变化，无需人工标注。

Result: MR-CLIP在多任务（如跨模态检索及对比分类）中表现出色，具有良好的可扩展性和临床应用潜力。

Conclusion: MR-CLIP为MRI图像的对比度分析提供了一个统一和高效的解决方案，简化了数据处理，并推动了临床研究与应用的发展。

Abstract: Accurate interpretation of Magnetic Resonance Imaging scans in clinical
systems is based on a precise understanding of image contrast. This contrast is
primarily governed by acquisition parameters, such as echo time and repetition
time, which are stored in the DICOM metadata. To simplify contrast
identification, broad labels such as T1-weighted or T2-weighted are commonly
used, but these offer only a coarse approximation of the underlying acquisition
settings. In many real-world datasets, such labels are entirely missing,
leaving raw acquisition parameters as the only indicators of contrast. Adding
to this challenge, the available metadata is often incomplete, noisy, or
inconsistent. The lack of reliable and standardized metadata complicates tasks
such as image interpretation, retrieval, and integration into clinical
workflows. Furthermore, robust contrast-aware representations are essential to
enable more advanced clinical applications, such as achieving
modality-invariant representations and data harmonization. To address these
challenges, we propose MR-CLIP, a multimodal contrastive learning framework
that aligns MR images with their DICOM metadata to learn contrast-aware
representations, without relying on manual labels. Trained on a diverse
clinical dataset that spans various scanners and protocols, MR-CLIP captures
contrast variations across acquisitions and within scans, enabling
anatomy-invariant representations. We demonstrate its effectiveness in
cross-modal retrieval and contrast classification, highlighting its scalability
and potential for further clinical applications. The code and weights are
publicly available at https://github.com/myigitavci/MR-CLIP.

</details>


### [5] [HistoART: Histopathology Artifact Detection and Reporting Tool](https://arxiv.org/abs/2507.00044)
*Seyed Kahaki,Alexander R. Webber,Ghada Zamzmi,Adarsh Subbaswamy,Rucha Deshpande,Aldo Badano*

Main category: cs.CV

TL;DR: 此研究提出并比较了三种检测全切片图像（WSI）伪影的方法，包括基础模型、深度学习模型和基于知识的方法，并提供质量评分工具。实验证明基础模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 全切片图像普及后伪影问题影响图像分析的准确性，因此需要开发鲁棒的伪影检测方法。

Method: 引入三种伪影检测方法：基于基础模型、ResNet50深度学习模型和基于知识的手工特征方法，并针对常见六种伪影进行分析。

Result: 基础模型取得了最高的AUROC值0.995，优于ResNet50模型的0.977和手工特征方法的0.940。开发了一个质量评分工具，量化高质量图像及伪影分布可视化。

Conclusion: 基础模型在检测伪影方面表现出色，同时提出的质量评分工具能够为诊断提供实用的可操作性支持。

Abstract: In modern cancer diagnostics, Whole Slide Imaging (WSI) is widely used to
digitize tissue specimens for detailed, high-resolution examination; however,
other diagnostic approaches, such as liquid biopsy and molecular testing, are
also utilized based on the cancer type and clinical context. While WSI has
revolutionized digital histopathology by enabling automated, precise analysis,
it remains vulnerable to artifacts introduced during slide preparation and
scanning. These artifacts can compromise downstream image analysis. To address
this challenge, we propose and compare three robust artifact detection
approaches for WSIs: (1) a foundation model-based approach (FMA) using a
fine-tuned Unified Neural Image (UNI) architecture, (2) a deep learning
approach (DLA) built on a ResNet50 backbone, and (3) a knowledge-based approach
(KBA) leveraging handcrafted features from texture, color, and frequency-based
metrics. The methods target six common artifact types: tissue folds,
out-of-focus regions, air bubbles, tissue damage, marker traces, and blood
contamination. Evaluations were conducted on 50,000+ image patches from diverse
scanners (Hamamatsu, Philips, Leica Aperio AT2) across multiple sites. The FMA
achieved the highest patch-wise AUROC of 0.995 (95% CI [0.994, 0.995]),
outperforming the ResNet50-based method (AUROC: 0.977, 95% CI [0.977, 0.978])
and the KBA (AUROC: 0.940, 95% CI [0.933, 0.946]). To translate detection into
actionable insights, we developed a quality report scorecard that quantifies
high-quality patches and visualizes artifact distributions.

</details>


### [6] [CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning](https://arxiv.org/abs/2507.00045)
*Ming Li,Chenguang Wang,Yijun Liang,Xiyao Wang,Yuhang Zhou,Xiyang Wu,Yuqing Zhang,Ruiyi Zhang,Tianyi Zhou*

Main category: cs.CV

TL;DR: 提出CaughtCheating挑战任务，测试多模态大语言模型在高难度场景下的视觉感知与推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在许多基准测试中表现优异，但缺乏更具挑战性的测试任务以评估其真实能力。新任务反映模型在人类难度场景下的潜能需求。

Method: 通过CaughtCheating任务，提出一系列基于图像的线索检测和推理分析实验，理解现有MLLM在此类任务中的不足和潜力。

Result: 发现在CaughtCheating场景下，当前最强MLLM GPT-o3的表现接近零，暴露出现有模型在检测和整合细微信息方面的短板。

Conclusion: CaughtCheating任务为研究MLLM的高级视觉和推理能力提供了重要线索，为模型朝着人类侦探级别能力进化提供平台。

Abstract: Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have
achieved near-ceiling scores on various existing benchmarks, motivating a
demand for more challenging test tasks. These MLLMs have been reported to excel
in a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their
potential as a detective who can notice minuscule cues in an image and weave
them into coherent, situational explanations, leading to a reliable answer. But
can they match the performance of excellent human detectives? To answer this
question, we investigate some hard scenarios where GPT-o3 can still handle, and
find a common scenario where o3's performance drops to nearly zero, which we
name CaughtCheating. It is inspired by the social media requests that ask
others to detect suspicious clues from photos shared by the poster's partner.
We conduct extensive experiments and analysis to understand why existing MLLMs
lack sufficient capability to solve this kind of task. CaughtCheating provides
a class of challenging visual perception and reasoning tasks with great value
and practical usage. Success in these tasks paves the way for MLLMs to acquire
human-level detective perception and reasoning capabilities.

</details>


### [7] [Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process](https://arxiv.org/abs/2507.00046)
*Akshansh Mishra,Eyob Mesele Sefene,Shivraman Thapliyal*

Main category: cs.CV

TL;DR: 该研究利用进化计算的图像分割方法分析加摩擦搅拌沉积（AFSD）过程中材料的完整性。


<details>
  <summary>Details</summary>
Motivation: 提出一种基于粒子群优化（PSO）的算法，用于在多层AFSD构建中检测缺陷和特征，改善质量监测和优化的能力。

Method: 通过结合梯度幅值分析和距离变换的方式开发新型注意力加权可视化技术，并采用PSO算法自动选择最佳分割阈值。使用多种可视化技术（注意力图和多通道可视化）分析5个不同条件下的AFSD样本。

Result: PSO算法有效地确定了材料界面的最佳分割阈值（156-173），多通道可视化技术成功地合成边界信息、空间关系和材料密度数据，揭示了常规成像方法难以发现的细微材料过渡区及潜在缺陷区域。

Conclusion: 基于注意力的分析方法能准确识别AFSD接头中的不完全粘结区和不均匀区域，为增材制造的工艺优化与质量评估提供了定量指标。

Abstract: This work proposes an evolutionary computing-based image segmentation
approach for analyzing soundness in Additive Friction Stir Deposition (AFSD)
processes. Particle Swarm Optimization (PSO) was employed to determine optimal
segmentation thresholds for detecting defects and features in multilayer AFSD
builds. The methodology integrates gradient magnitude analysis with distance
transforms to create novel attention-weighted visualizations that highlight
critical interface regions. Five AFSD samples processed under different
conditions were analyzed using multiple visualization techniques i.e.
self-attention maps, and multi-channel visualization. These complementary
approaches reveal subtle material transition zones and potential defect regions
which were not readily observable through conventional imaging. The PSO
algorithm automatically identified optimal threshold values (ranging from
156-173) for each sample, enabling precise segmentation of material interfaces.
The multi-channel visualization technique effectively combines boundary
information (red channel), spatial relationships (green channel), and material
density data (blue channel) into cohesive representations that quantify
interface quality. The results demonstrate that attention-based analysis
successfully identifies regions of incomplete bonding and inhomogeneities in
AFSD joints, providing quantitative metrics for process optimization and
quality assessment of additively manufactured components.

</details>


### [8] [AdaDeDup: Adaptive Hybrid Data Pruning for Efficient Large-Scale Object Detection Training](https://arxiv.org/abs/2507.00049)
*Feiyang Kang,Nadine Chang,Maying Shen,Marc T. Law,Rafid Mahmood,Ruoxi Jia,Jose M. Alvarez*

Main category: cs.CV

TL;DR: 本文提出了一个名为AdaDeDup的新型混合框架，通过密度基础的稀疏技术和模型反馈的协作用于数据修剪，显著提升了大型模型训练中的数据效率。


<details>
  <summary>Details</summary>
Motivation: 由于大规模数据集的计算负担和固有冗余，现有的模型训练变得非常挑战，因此需要开发一种高效的数据修剪方法以提高训练效率。

Method: AdaDeDup框架结合了密度基础的修剪方法和基于模型反馈的任务感知机制，采用分区操作和簇内调整，实现对数据的自适应去重与修剪，同时保留关键信息。

Result: 实验表明，AdaDeDup能够在大规模数据集上保持接近模型原始性能的情况下修剪20%的数据，同时在减少性能损耗方面显著优于现有基线方法。

Conclusion: AdaDeDup在提升大规模模型训练数据效率方面具有重要价值，其结合密度和任务感知的创新性方法为数据剪枝提供了新方向。

Abstract: The computational burden and inherent redundancy of large-scale datasets
challenge the training of contemporary machine learning models. Data pruning
offers a solution by selecting smaller, informative subsets, yet existing
methods struggle: density-based approaches can be task-agnostic, while
model-based techniques may introduce redundancy or prove computationally
prohibitive. We introduce Adaptive De-Duplication (AdaDeDup), a novel hybrid
framework that synergistically integrates density-based pruning with
model-informed feedback in a cluster-adaptive manner. AdaDeDup first partitions
data and applies an initial density-based pruning. It then employs a proxy
model to evaluate the impact of this initial pruning within each cluster by
comparing losses on kept versus pruned samples. This task-aware signal
adaptively adjusts cluster-specific pruning thresholds, enabling more
aggressive pruning in redundant clusters while preserving critical data in
informative ones. Extensive experiments on large-scale object detection
benchmarks (Waymo, COCO, nuScenes) using standard models (BEVFormer, Faster
R-CNN) demonstrate AdaDeDup's advantages. It significantly outperforms
prominent baselines, substantially reduces performance degradation (e.g., over
54% versus random sampling on Waymo), and achieves near-original model
performance while pruning 20% of data, highlighting its efficacy in enhancing
data efficiency for large-scale model training. Code is open-sourced.

</details>


### [9] [VSF-Med:A Vulnerability Scoring Framework for Medical Vision-Language Models](https://arxiv.org/abs/2507.00052)
*Binesh Sadanandan,Vahid Behzadan*

Main category: cs.CV

TL;DR: 提出了VSF-Med框架，为医疗视觉语言模型（VLMs）提供了一个全面的脆弱性评估工具，通过超过30,000种对抗性样本进行评测。


<details>
  <summary>Details</summary>
Motivation: 医疗VLMs在医学影像工作流中有着广泛的应用前景，但临床环境下缺乏系统的安全性评估，因此亟需一个综合的框架来评估其潜在的安全风险。

Method: 提出了VSF-Med框架，包括三个新组件：复杂文本提示攻击模板、基于结构相似性（SSIM）的视觉扰动以及通过z分数标准化的八维评分方法，并结合公开数据与开源代码进行实现。

Result: 评估了最先进的VLMs模型，发现例如Llama-3.2-11B-Vision-Instruct在攻击效果持续性上脆弱性显著增加（1.29σ），而GPT-4o在提示注入攻击中的脆弱性有0.28σ增加。

Conclusion: VSF-Med框架实现了医疗VLMs脆弱性评估的标准化和可重现性，为未来的研究提供了工具支持。

Abstract: Vision Language Models (VLMs) hold great promise for streamlining
labour-intensive medical imaging workflows, yet systematic security evaluations
in clinical settings remain scarce. We introduce VSF--Med, an end-to-end
vulnerability-scoring framework for medical VLMs that unites three novel
components: (i) a rich library of sophisticated text-prompt attack templates
targeting emerging threat vectors; (ii) imperceptible visual perturbations
calibrated by structural similarity (SSIM) thresholds to preserve clinical
realism; and (iii) an eight-dimensional rubric evaluated by two independent
judge LLMs, whose raw scores are consolidated via z-score normalization to
yield a 0--32 composite risk metric. Built entirely on publicly available
datasets and accompanied by open-source code, VSF--Med synthesizes over 30,000
adversarial variants from 5,000 radiology images and enables reproducible
benchmarking of any medical VLM with a single command. Our consolidated
analysis reports mean z-score shifts of $0.90\sigma$ for
persistence-of-attack-effects, $0.74\sigma$ for prompt-injection effectiveness,
and $0.63\sigma$ for safety-bypass success across state-of-the-art VLMs.
Notably, Llama-3.2-11B-Vision-Instruct exhibits a peak vulnerability increase
of $1.29\sigma$ for persistence-of-attack-effects, while GPT-4o shows increases
of $0.69\sigma$ for that same vector and $0.28\sigma$ for prompt-injection
attacks.

</details>


### [10] [MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding](https://arxiv.org/abs/2507.00068)
*Ziqi Zhong,Daniel Tang*

Main category: cs.CV

TL;DR: MANTA架构通过将多模态信息转化为结构化文本空间，实现了视觉和听觉输入的统一处理，显著提升长视频问答任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态学习方法通常分别处理各模态，导致表征和推理不一致的问题。

Method: 引入MANTA框架，通过信息论优化对模态语义进行对齐，采用自适应时间同步和分层内容表示，并在数学上证明其在受限条件下的上下文选择最优性。

Result: 在长视频问答任务中，MANTA在整体准确率上提升达22.6%，在30分钟以上视频中提升27.3%，在时间推理和跨模态理解上分别提升23.8%和25.1%。

Conclusion: MANTA通过结构化文本统一多模态表示，提供了新颖的密度估计技术，确立了跨模态表示统一的新基础。

Abstract: While multi-modal learning has advanced significantly, current approaches
often treat modalities separately, creating inconsistencies in representation
and reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization
via Textual Alignment), a theoretically-grounded framework that unifies visual
and auditory inputs into a structured textual space for seamless processing
with large language models. MANTA addresses four key challenges: (1) semantic
alignment across modalities with information-theoretic optimization, (2)
adaptive temporal synchronization for varying information densities, (3)
hierarchical content representation for multi-scale understanding, and (4)
context-aware retrieval of sparse information from long sequences. We formalize
our approach within a rigorous mathematical framework, proving its optimality
for context selection under token constraints. Extensive experiments on the
challenging task of Long Video Question Answering show that MANTA improves
state-of-the-art models by up to 22.6% in overall accuracy, with particularly
significant gains (27.3%) on videos exceeding 30 minutes. Additionally, we
demonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement)
and cross-modal understanding (25.1% improvement). Our framework introduces
novel density estimation techniques for redundancy minimization while
preserving rare signals, establishing new foundations for unifying multimodal
representations through structured text.

</details>


### [11] [An efficient plant disease detection using transfer learning approach](https://arxiv.org/abs/2507.00070)
*Bosubabu Sambana,Hillary Sunday Nnadi,Mohd Anas Wajid,Nwosu Ogochukwu Fidelia,Claudia Camacho-Zuñiga,Henry Dozie Ajuzie,Edeh Michael Onyema*

Main category: cs.CV

TL;DR: 本研究提出了一种基于YOLOv7和YOLOv8的迁移学习系统，用于检测和监控植物疾病，表现出较高的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 农业中植物疾病的早期检测对减少损失和提高生产力至关重要，但目前的手动检测方法效率较低。

Method: 采用YOLOv7和YOLOv8模型，对植物叶片图像数据集进行微调，通过迁移学习实现高效的病害检测。

Result: 模型在mAP、F1分数、精确率和召回率方面分别达到了91.05%、89.40%、91.22%和87.66%。YOLOv8展现出比其他检测方法更优的性能。

Conclusion: 该方法为植物病害的早期自动检测提供了可扩展解决方案，有助于提高作物产量、减少人工监控依赖，并支持可持续农业实践。

Abstract: Plant diseases pose significant challenges to farmers and the agricultural
sector at large. However, early detection of plant diseases is crucial to
mitigating their effects and preventing widespread damage, as outbreaks can
severely impact the productivity and quality of crops. With advancements in
technology, there are increasing opportunities for automating the monitoring
and detection of disease outbreaks in plants. This study proposed a system
designed to identify and monitor plant diseases using a transfer learning
approach. Specifically, the study utilizes YOLOv7 and YOLOv8, two
state-ofthe-art models in the field of object detection. By fine-tuning these
models on a dataset of plant leaf images, the system is able to accurately
detect the presence of Bacteria, Fungi and Viral diseases such as Powdery
Mildew, Angular Leaf Spot, Early blight and Tomato mosaic virus. The model's
performance was evaluated using several metrics, including mean Average
Precision (mAP), F1-score, Precision, and Recall, yielding values of 91.05,
89.40, 91.22, and 87.66, respectively. The result demonstrates the superior
effectiveness and efficiency of YOLOv8 compared to other object detection
methods, highlighting its potential for use in modern agricultural practices.
The approach provides a scalable, automated solution for early any plant
disease detection, contributing to enhanced crop yield, reduced reliance on
manual monitoring, and supporting sustainable agricultural practices.

</details>


### [12] [Diffusion-Based Image Augmentation for Semantic Segmentation in Outdoor Robotics](https://arxiv.org/abs/2507.00153)
*Peter Mortimer,Mirko Maehlisch*

Main category: cs.CV

TL;DR: 提出了用扩散式图像增强与开放词汇语义分割技术改进雪地驾驶中的学习算法。


<details>
  <summary>Details</summary>
Motivation: 针对此前学习算法在异常分布和低代表性环境中的表现不佳，尤其是在因天气和光线变化快速变化的户外场景中表现受限的问题，重点研究雪地驾驶环境问题。

Method: 提出了一种基于扩散图像增强的新方法，利用互联网级数据学习所得的视觉基础模型，结合开放词汇语义分割模型过滤掉可能的幻觉候选增强数据，生成更贴近部署环境的训练数据。

Result: 实验结果表明，该方法在调整训练数据的语义分布并优化模型部署环境表现中效果显著。

Conclusion: 扩散图像增强技术不仅可优化雪地驾驶环境的算法性能，也能推广至沙地、火山地形等其他场景的应用。

Abstract: The performance of leaning-based perception algorithms suffer when deployed
in out-of-distribution and underrepresented environments. Outdoor robots are
particularly susceptible to rapid changes in visual scene appearance due to
dynamic lighting, seasonality and weather effects that lead to scenes
underrepresented in the training data of the learning-based perception system.
In this conceptual paper, we focus on preparing our autonomous vehicle for
deployment in snow-filled environments. We propose a novel method for
diffusion-based image augmentation to more closely represent the deployment
environment in our training data. Diffusion-based image augmentations rely on
the public availability of vision foundation models learned on internet-scale
datasets. The diffusion-based image augmentations allow us to take control over
the semantic distribution of the ground surfaces in the training data and to
fine-tune our model for its deployment environment. We employ open vocabulary
semantic segmentation models to filter out augmentation candidates that contain
hallucinations. We believe that diffusion-based image augmentations can be
extended to many other environments apart from snow surfaces, like sandy
environments and volcanic terrains.

</details>


### [13] [FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion](https://arxiv.org/abs/2507.00162)
*Yu Lu,Yi Yang*

Main category: cs.CV

TL;DR: 提出FreeLong和FreeLong++框架，用于改进长视频生成模型的时间一致性和视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 解决当前视频生成模型在生成长视频时出现的高频失真和质量下降问题。

Method: 通过FreeLong框架结合全局低频特征和局部高频特征，并扩展到FreeLong++，利用多分支设计在不同时间尺度上融合低频到高频特征。

Result: FreeLong++显著提高了长视频生成的时间一致性和视觉保真度，并支持多提示视频生成和可控视频生成。

Conclusion: FreeLong++无需额外训练即可集成到现有视频生成模型中，为实际长视频生成任务提供有效解决方案。

Abstract: Recent advances in video generation models have enabled high-quality short
video generation from text prompts. However, extending these models to longer
videos remains a significant challenge, primarily due to degraded temporal
consistency and visual fidelity. Our preliminary observations show that naively
applying short-video generation models to longer sequences leads to noticeable
quality degradation. Further analysis identifies a systematic trend where
high-frequency components become increasingly distorted as video length grows,
an issue we term high-frequency distortion. To address this, we propose
FreeLong, a training-free framework designed to balance the frequency
distribution of long video features during the denoising process. FreeLong
achieves this by blending global low-frequency features, which capture holistic
semantics across the full video, with local high-frequency features extracted
from short temporal windows to preserve fine details. Building on this,
FreeLong++ extends FreeLong dual-branch design into a multi-branch architecture
with multiple attention branches, each operating at a distinct temporal scale.
By arranging multiple window sizes from global to local, FreeLong++ enables
multi-band frequency fusion from low to high frequencies, ensuring both
semantic continuity and fine-grained motion dynamics across longer video
sequences. Without any additional training, FreeLong++ can be plugged into
existing video generation models (e.g. Wan2.1 and LTX-Video) to produce longer
videos with substantially improved temporal consistency and visual fidelity. We
demonstrate that our approach outperforms previous methods on longer video
generation tasks (e.g. 4x and 8x of native length). It also supports coherent
multi-prompt video generation with smooth scene transitions and enables
controllable video generation using long depth or pose sequences.

</details>


### [14] [SelvaBox: A high-resolution dataset for tropical tree crown detection](https://arxiv.org/abs/2507.00170)
*Hugo Baudchon,Arthur Ouaknine,Martin Weiss,Mélisande Teng,Thomas R. Walla,Antoine Caron-Guay,Christopher Pal,Etienne Laliberté*

Main category: cs.CV

TL;DR: 本文提出了SelvaBox，一种用于高分辨率无人机影像中热带树冠检测的公开数据集，包含超过8.3万个手动标记的树冠，模型在提升检测精度与零样本迁移性能方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前研究热带森林复杂生态系统受到气候变化和人类活动的影响，而现有数据集匮乏限制了稳健模型的开发。

Method: 提出SelvaBox数据集，覆盖三国并包含大量标注数据，结合多分辨率输入方法进行训练和验证，结果与主流方法相比具有竞争力。

Result: 更高分辨率输入提高了检测的准确性，模型在零样本迁移任务中表现优秀，跨多个分辨率联合训练的检测器在多数据集评估中排名靠前。

Conclusion: SelvaBox数据集解决了热带森林检测中数据稀缺问题，显著提升了模型性能，并促进了更多研究可能性，同时公开了数据和代码资源。

Abstract: Detecting individual tree crowns in tropical forests is essential to study
these complex and crucial ecosystems impacted by human interventions and
climate change. However, tropical crowns vary widely in size, structure, and
pattern and are largely overlapping and intertwined, requiring advanced remote
sensing methods applied to high-resolution imagery. Despite growing interest in
tropical tree crown detection, annotated datasets remain scarce, hindering
robust model development. We introduce SelvaBox, the largest open-access
dataset for tropical tree crown detection in high-resolution drone imagery. It
spans three countries and contains more than 83,000 manually labeled crowns -
an order of magnitude larger than all previous tropical forest datasets
combined. Extensive benchmarks on SelvaBox reveal two key findings: (1)
higher-resolution inputs consistently boost detection accuracy; and (2) models
trained exclusively on SelvaBox achieve competitive zero-shot detection
performance on unseen tropical tree crown datasets, matching or exceeding
competing methods. Furthermore, jointly training on SelvaBox and three other
datasets at resolutions from 3 to 10 cm per pixel within a unified
multi-resolution pipeline yields a detector ranking first or second across all
evaluated datasets. Our dataset, code, and pre-trained weights are made public.

</details>


### [15] [Graph-Based Deep Learning for Component Segmentation of Maize Plants](https://arxiv.org/abs/2507.00182)
*J. I. Ruíz,A. Méndez,E. Rodríguez*

Main category: cs.CV

TL;DR: 本研究提出了一种基于图神经网络(GNN)和主成分分析(PCA)的深度学习模型，用于精确农业中通过LiDAR 3D点云数据集识别植物部件，实现了高达80%以上的IoU平均分。


<details>
  <summary>Details</summary>
Motivation: 目前，2D成像、3D重建和卷积神经网络（CNN）在处理3D数据以及识别单个植物部件时存在诸多不足。本研究的目的是构建一种新型深度学习架构，以提高精确农业中植物部件识别的准确性。

Method: 利用图神经网络（GNN）架构，结合主成分分析（PCA）进行特征增强，使用K近邻（KNN）层建立3D点云数据的边缘表示，通过边缘卷积层（Edge-Conv）增强点特征，并最终通过图注意力网络（GAT）分类植物部件。

Result: 实验表明，该方法在分割精度上得到了显著提升，平均IoU超过80%，优于其他基于点云的数据模型。

Conclusion: 本研究提出的方法有效提高了LiDAR 3D点云中植物部件的分割精度，为精确农业应用提供了高效解决方案。

Abstract: In precision agriculture, one of the most important tasks when exploring crop
production is identifying individual plant components. There are several
attempts to accomplish this task by the use of traditional 2D imaging, 3D
reconstructions, and Convolutional Neural Networks (CNN). However, they have
several drawbacks when processing 3D data and identifying individual plant
components. Therefore, in this work, we propose a novel Deep Learning
architecture to detect components of individual plants on Light Detection and
Ranging (LiDAR) 3D Point Cloud (PC) data sets. This architecture is based on
the concept of Graph Neural Networks (GNN), and feature enhancing with
Principal Component Analysis (PCA). For this, each point is taken as a vertex
and by the use of a K-Nearest Neighbors (KNN) layer, the edges are established,
thus representing the 3D PC data set. Subsequently, Edge-Conv layers are used
to further increase the features of each point. Finally, Graph Attention
Networks (GAT) are applied to classify visible phenotypic components of the
plant, such as the leaf, stem, and soil. This study demonstrates that our
graph-based deep learning approach enhances segmentation accuracy for
identifying individual plant components, achieving percentages above 80% in the
IoU average, thus outperforming other existing models based on point clouds.

</details>


### [16] [Computer Vision for Objects used in Group Work: Challenges and Opportunities](https://arxiv.org/abs/2507.00224)
*Changsoo Jung,Sheikh Mannan,Jack Fitzgerald,Nathaniel Blanchard*

Main category: cs.CV

TL;DR: 研究分析了现有技术在教育中对6D姿态估计的不足，尤其是在协作情境中的物体姿态捕捉，并引入了一个新的数据集FiboSB。


<details>
  <summary>Details</summary>
Motivation: 目前教育系统缺乏准确捕捉学生与物体交互的技术，尤其在K-12协作任务中需要解决这一问题。研究提出6D姿态估计可有助于更好理解和关联物体与实体。

Method: 研究提出一个新数据集FiboSB，其中包括多名参与者和手持物体的交互场景，并测试现有4种6D姿态估计方法，优化YOLO11-x以改进模型性能。

Result: 通过微调YOLO11-x实现了mAP_50为0.898的表现，揭示了现有技术在协作任务中的不足，并展示优化潜力。

Conclusion: 本研究提供了新的数据集、基准测试结果及模型错误分析，为改进协作场景中的6D姿态估计技术奠定基础。

Abstract: Interactive and spatially aware technologies are transforming educational
frameworks, particularly in K-12 settings where hands-on exploration fosters
deeper conceptual understanding. However, during collaborative tasks, existing
systems often lack the ability to accurately capture real-world interactions
between students and physical objects. This issue could be addressed with
automatic 6D pose estimation, i.e., estimation of an object's position and
orientation in 3D space from RGB images or videos. For collaborative groups
that interact with physical objects, 6D pose estimates allow AI systems to
relate objects and entities. As part of this work, we introduce FiboSB, a novel
and challenging 6D pose video dataset featuring groups of three participants
solving an interactive task featuring small hand-held cubes and a weight scale.
This setup poses unique challenges for 6D pose because groups are holistically
recorded from a distance in order to capture all participants -- this, coupled
with the small size of the cubes, makes 6D pose estimation inherently
non-trivial. We evaluated four state-of-the-art 6D pose estimation methods on
FiboSB, exposing the limitations of current algorithms on collaborative group
work. An error analysis of these methods reveals that the 6D pose methods'
object detection modules fail. We address this by fine-tuning YOLO11-x for
FiboSB, achieving an overall mAP_50 of 0.898. The dataset, benchmark results,
and analysis of YOLO11-x errors presented here lay the groundwork for
leveraging the estimation of 6D poses in difficult collaborative contexts.

</details>


### [17] [VOCAL: Visual Odometry via ContrAstive Learning](https://arxiv.org/abs/2507.00243)
*Chi-Yao Huang,Zeel Bhatt,Yezhou Yang*

Main category: cs.CV

TL;DR: 提出基于对比学习的视觉里程计新框架VOCAL，提升可解释性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的视觉里程计方法依赖刚性几何假设，存在可解释性不足和缺乏理论基础的问题。

Method: 引入VOCAL框架，将视觉里程计转化为标签排序问题，结合贝叶斯推断与表征学习，优化相机状态相关的特征空间表示。

Result: 在KITTI数据集上进行评估，证明VOCAL提高了方法的可解释性和灵活性。

Conclusion: VOCAL框架推进了视觉里程计领域的通用性和解释性发展。

Abstract: Breakthroughs in visual odometry (VO) have fundamentally reshaped the
landscape of robotics, enabling ultra-precise camera state estimation that is
crucial for modern autonomous systems. Despite these advances, many
learning-based VO techniques rely on rigid geometric assumptions, which often
fall short in interpretability and lack a solid theoretical basis within fully
data-driven frameworks. To overcome these limitations, we introduce VOCAL
(Visual Odometry via ContrAstive Learning), a novel framework that reimagines
VO as a label ranking challenge. By integrating Bayesian inference with a
representation learning framework, VOCAL organizes visual features to mirror
camera states. The ranking mechanism compels similar camera states to converge
into consistent and spatially coherent representations within the latent space.
This strategic alignment not only bolsters the interpretability of the learned
features but also ensures compatibility with multimodal data sources. Extensive
evaluations on the KITTI dataset highlight VOCAL's enhanced interpretability
and flexibility, pushing VO toward more general and explainable spatial
intelligence.

</details>


### [18] [Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition](https://arxiv.org/abs/2507.00248)
*Nikita Nikitin,Eugene Fomin*

Main category: cs.CV

TL;DR: 本文提出了一种新框架用于实时手语识别，使用轻量级DNN并针对有限数据进行了优化。


<details>
  <summary>Details</summary>
Motivation: 解决手语识别中数据稀缺、高计算成本，以及训练与推理环境中帧率差异的问题。

Method: 通过向量化输入参数，如手势形状、掌心方向、运动和位置，同时利用MediaPipe进行关键点提取，并设计小于10MB的DNN架构。

Result: 实现了343个手语识别，准确率达92%，延迟小于10ms，成功集成进‘slait ai’网络应用中。

Conclusion: 轻量化高效的DNN架构在有限数据的情况下依然能够实现高精度实时手语识别，证明了该方法在边缘设备上的可行性。

Abstract: We present a novel framework for real-time sign language recognition using
lightweight DNNs trained on limited data. Our system addresses key challenges
in sign language recognition, including data scarcity, high computational
costs, and discrepancies in frame rates between training and inference
environments. By encoding sign language specific parameters, such as handshape,
palm orientation, movement, and location into vectorized inputs, and leveraging
MediaPipe for landmark extraction, we achieve highly separable input data
representations. Our DNN architecture, optimized for sub 10MB deployment,
enables accurate classification of 343 signs with less than 10ms latency on
edge devices. The data annotation platform 'slait data' facilitates structured
labeling and vector extraction. Our model achieved 92% accuracy in isolated
sign recognition and has been integrated into the 'slait ai' web application,
where it demonstrates stable inference.

</details>


### [19] [GazeTarget360: Towards Gaze Target Estimation in 360-Degree for Robot Perception](https://arxiv.org/abs/2507.00253)
*Zhuangzhuang Dai,Vincent Gbouna Zakka,Luis J. Manso,Chen Li*

Main category: cs.CV

TL;DR: 该研究提出了GazeTarget360系统，以实现从图像中估计360度视线目标，并在未知场景下表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了解决人机交互中目光目标估计的问题，现有方法在移除屏外样本后依托数据驱动，无法处理人物目光离开摄像头的情况。

Method: 设计了GazeTarget360系统，整合了视线检测引擎、预训练视觉编码器和多尺度融合解码器进行360度视线目标估计。

Result: 交叉验证结果表明，GazeTarget360在未知场景中能够准确可靠地预测视线目标。

Conclusion: GazeTarget360是第一个能够从真实相机镜头中高效预测视线目标的系统，并公开了源代码供研究者使用。

Abstract: Enabling robots to understand human gaze target is a crucial step to allow
capabilities in downstream tasks, for example, attention estimation and
movement anticipation in real-world human-robot interactions. Prior works have
addressed the in-frame target localization problem with data-driven approaches
by carefully removing out-of-frame samples. Vision-based gaze estimation
methods, such as OpenFace, do not effectively absorb background information in
images and cannot predict gaze target in situations where subjects look away
from the camera. In this work, we propose a system to address the problem of
360-degree gaze target estimation from an image in generalized visual scenes.
The system, named GazeTarget360, integrates conditional inference engines of an
eye-contact detector, a pre-trained vision encoder, and a multi-scale-fusion
decoder. Cross validation results show that GazeTarget360 can produce accurate
and reliable gaze target predictions in unseen scenarios. This makes a
first-of-its-kind system to predict gaze targets from realistic camera footage
which is highly efficient and deployable. Our source code is made publicly
available at: https://github.com/zdai257/DisengageNet.

</details>


### [20] [VirtualFencer: Generating Fencing Bouts based on Strategies Extracted from In-the-Wild Videos](https://arxiv.org/abs/2507.00261)
*Zhiyin Lin,Purvi Goel,Joy Yun,C. Karen Liu,Joao Pedro Araujo*

Main category: cs.CV

TL;DR: 研究团队开发了VirtualFencer系统，可以从视频中提取击剑运动和策略，并生成逼真的虚拟击剑行为。


<details>
  <summary>Details</summary>
Motivation: 击剑运动涉及多样且具策略性的动作，研究希望通过数据驱动模型分析此多样性和其两人策略交互过程。

Method: 开发VirtualFencer系统，能够在无监督的情况下，从现场视频中提取击剑的3D运动和策略，并生成合理的击剑行为。

Result: 系统可以实现与自身对战、与真实视频中的击剑动作对战，以及与专业选手进行互动对战。

Conclusion: VirtualFencer展示了其在击剑运动数据提取和行为生成中的能力，为击剑策略研究和虚拟互动提供了新工具。

Abstract: Fencing is a sport where athletes engage in diverse yet strategically logical
motions. While most motions fall into a few high-level actions (e.g. step,
lunge, parry), the execution can vary widely-fast vs. slow, large vs. small,
offensive vs. defensive. Moreover, a fencer's actions are informed by a
strategy that often comes in response to the opponent's behavior. This
combination of motion diversity with underlying two-player strategy motivates
the application of data-driven modeling to fencing. We present VirtualFencer, a
system capable of extracting 3D fencing motion and strategy from in-the-wild
video without supervision, and then using that extracted knowledge to generate
realistic fencing behavior. We demonstrate the versatile capabilities of our
system by having it (i) fence against itself (self-play), (ii) fence against a
real fencer's motion from online video, and (iii) fence interactively against a
professional fencer.

</details>


### [21] [Self-Supervised Multiview Xray Matching](https://arxiv.org/abs/2507.00287)
*Mohamad Dabboussi,Malo Huard,Yann Gousseau,Pietro Gori*

Main category: cs.CV

TL;DR: 本研究提出了一种自监督管道，无需人工标注，通过预测合成X射线视图之间的对应关系，提高了多视角骨折分类的性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI方法在分析单一图像方面取得了进展，但在建立多视角X射线图像间的可靠对应关系上仍存在挑战，这一能力对临床诊断至关重要。

Method: 本文提出基于转换器的自监督方法，利用从未标注CT体积中生成的数字重建射线图(DRR)自动生成多对多对应矩阵，并通过这种学习对应关系的策略来预训练实际数据的多视角骨折检测模型。

Result: 实验表明，在合成和真实X射线数据集上融入视图对应关系可提升多视角骨折分类的性能。

Conclusion: 通过使用自监督方法预测多视角X射线视图的对应关系，本文成功改进了骨折分类，并推动了多视角医学图像分析的发展。

Abstract: Accurate interpretation of multi-view radiographs is crucial for diagnosing
fractures, muscular injuries, and other anomalies. While significant advances
have been made in AI-based analysis of single images, current methods often
struggle to establish robust correspondences between different X-ray views, an
essential capability for precise clinical evaluations. In this work, we present
a novel self-supervised pipeline that eliminates the need for manual annotation
by automatically generating a many-to-many correspondence matrix between
synthetic X-ray views. This is achieved using digitally reconstructed
radiographs (DRR), which are automatically derived from unannotated CT volumes.
Our approach incorporates a transformer-based training phase to accurately
predict correspondences across two or more X-ray views. Furthermore, we
demonstrate that learning correspondences among synthetic X-ray views can be
leveraged as a pretraining strategy to enhance automatic multi-view fracture
detection on real data. Extensive evaluations on both synthetic and real X-ray
datasets show that incorporating correspondences improves performance in
multi-view fracture classification.

</details>


### [22] [Reducing Variability of Multiple Instance Learning Methods for Digital Pathology](https://arxiv.org/abs/2507.00292)
*Ali Mammadov,Loïc Le Folgoc,Guillaume Hocquet,Pietro Gori*

Main category: cs.CV

TL;DR: 提出了一种多保真度的模型融合策略，以解决多实例学习（MIL）方法在数字病理学中对全切片图像分类的性能不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 针对多实例学习（MIL）在全切片图像分类中的性能不稳定性问题，其主要来源于权重初始化、批次排序以及学习率，提出优化解决方案。

Method: 提出一种多保真度、模型融合的策略，先训练多个模型几个周期，再基于验证分数选择最稳定和最有前景的模型进行平均。这种方法可以应用于现有的任意MIL模型。

Result: 在两个数据集、三种初始化策略和五种MIL方法上进行了超过2000次实验，验证了该方法在WSI分类任务中的有效性。

Conclusion: 该方法可以减小性能波动性，简化超参数调节，提升可复现性，同时保持计算效率。

Abstract: Digital pathology has revolutionized the field by enabling the digitization
of tissue samples into whole slide images (WSIs). However, the high resolution
and large size of WSIs present significant challenges when it comes to applying
Deep Learning models. As a solution, WSIs are often divided into smaller
patches with a global label (\textit{i.e., diagnostic}) per slide, instead of a
(too) costly pixel-wise annotation. By treating each slide as a bag of patches,
Multiple Instance Learning (MIL) methods have emerged as a suitable solution
for WSI classification. A major drawback of MIL methods is their high
variability in performance across different runs, which can reach up to 10-15
AUC points on the test set, making it difficult to compare different MIL
methods reliably. This variability mainly comes from three factors: i) weight
initialization, ii) batch (shuffling) ordering, iii) and learning rate. To
address that, we introduce a Multi-Fidelity, Model Fusion strategy for MIL
methods. We first train multiple models for a few epochs and average the most
stable and promising ones based on validation scores. This approach can be
applied to any existing MIL model to reduce performance variability. It also
simplifies hyperparameter tuning and improves reproducibility while maintaining
computational efficiency. We extensively validate our approach on WSI
classification tasks using 2 different datasets, 3 initialization strategies
and 5 MIL methods, for a total of more than 2000 experiments.

</details>


### [23] [Beyond Low-Rank Tuning: Model Prior-Guided Rank Allocation for Effective Transfer in Low-Data and Large-Gap Regimes](https://arxiv.org/abs/2507.00327)
*Chuyan Zhang,Kefan Wang,Yun Gu*

Main category: cs.CV

TL;DR: SR-LoRA通过利用预训练权重矩阵的稳定秩，提出了一种高效分配层内低秩的新方法，在跨领域任务中提升了性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA方法由于固定的低秩结构，难以适应存在显著领域差距的场景，而现有提升适应性的方式计算成本高。

Method: 提出了利用预训练权重稳定秩作为层级低秩分配的自然先验的SR-LoRA框架，无需额外的搜索成本即可高效重新分配各层低秩。

Result: 在存在显著领域差距的少样本任务中，SR-LoRA表现优于现有自适应LoRA变体，在性能与效率之间表现出更佳的平衡。

Conclusion: SR-LoRA在降低计算成本的同时显著提升了LoRA的跨领域适应性。

Abstract: Low-Rank Adaptation (LoRA) has proven effective in reducing computational
costs while maintaining performance comparable to fully fine-tuned foundation
models across various tasks. However, its fixed low-rank structure restricts
its adaptability in scenarios with substantial domain gaps, where higher ranks
are often required to capture domain-specific complexities. Current adaptive
LoRA methods attempt to overcome this limitation by dynamically expanding or
selectively allocating ranks, but these approaches frequently depend on
computationally intensive techniques such as iterative pruning, rank searches,
or additional regularization. To address these challenges, we introduce Stable
Rank-Guided Low-Rank Adaptation (SR-LoRA), a novel framework that utilizes the
stable rank of pre-trained weight matrices as a natural prior for layer-wise
rank allocation. By leveraging the stable rank, which reflects the intrinsic
dimensionality of the weights, SR-LoRA enables a principled and efficient
redistribution of ranks across layers, enhancing adaptability without incurring
additional search costs. Empirical evaluations on few-shot tasks with
significant domain gaps show that SR-LoRA consistently outperforms recent
adaptive LoRA variants, achieving a superior trade-off between performance and
efficiency. Our code is available at
https://github.com/EndoluminalSurgicalVision-IMR/SR-LoRA.

</details>


### [24] [MammoTracker: Mask-Guided Lesion Tracking in Temporal Mammograms](https://arxiv.org/abs/2507.00328)
*Xuan Liu,Yinhao Ren,Marc D. Ryser,Lars J. Grimm,Joseph Y. Lo*

Main category: cs.CV

TL;DR: MammoTracker通过三阶段框架提高乳腺癌病灶定位与跟踪的准确性，超越基线模型8%。


<details>
  <summary>Details</summary>
Motivation: 为解决乳腺癌病灶在时间序列乳房X线图像中定位对应问题，并提升计算机辅助诊断系统的效果。

Method: 提出MammoTracker，采用粗到细策略，包含全局搜索、局部搜索和分数细化模块，同时构建新数据集用于模型训练与评估。

Result: 模型平均重叠率为0.455，准确率为0.509，相较基线模型提升8%。

Conclusion: MammoTracker框架在时间序列乳腺X线图像病灶跟踪方面表现优异，有潜力提升乳腺癌诊断效果。

Abstract: Accurate lesion tracking in temporal mammograms is essential for monitoring
breast cancer progression and facilitating early diagnosis. However, automated
lesion correspondence across exams remains a challenges in computer-aided
diagnosis (CAD) systems, limiting their effectiveness. We propose MammoTracker,
a mask-guided lesion tracking framework that automates lesion localization
across consecutively exams. Our approach follows a coarse-to-fine strategy
incorporating three key modules: global search, local search, and score
refinement. To support large-scale training and evaluation, we introduce a new
dataset with curated prior-exam annotations for 730 mass and calcification
cases from the public EMBED mammogram dataset, yielding over 20000 lesion
pairs, making it the largest known resource for temporal lesion tracking in
mammograms. Experimental results demonstrate that MammoTracker achieves 0.455
average overlap and 0.509 accuracy, surpassing baseline models by 8%,
highlighting its potential to enhance CAD-based lesion progression analysis.
Our dataset will be available at
https://gitlab.oit.duke.edu/railabs/LoGroup/mammotracker.

</details>


### [25] [Populate-A-Scene: Affordance-Aware Human Video Generation](https://arxiv.org/abs/2507.00334)
*Mengyi Shan,Zecheng He,Haoyu Ma,Felix Juefei-Xu,Peizhao Zhang,Tingbo Hou,Ching-Yao Chuang*

Main category: cs.CV

TL;DR: 研究将文本生成视频模型转为交互式世界模拟器，通过微调模型实现场景中人与环境的交互预测。


<details>
  <summary>Details</summary>
Motivation: 探索文本到视频模型在感知人与环境交互潜力的能力，并将其用于预测场景中的人类行为。

Method: 在给定场景图像及文本描述的基础上，微调模型插入角色，确保行为一致性、外观协调及场景适应性。研究利用交叉注意力热图，揭示预训练视频模型的内在感知能力。

Result: 无需显式条件（如边框或人体姿势），从单一图像推断视频中人物的行为及插入位置。

Conclusion: 模型能够通过微调实现人类行为一致性预测，同时揭示无需标注数据集的内在感知潜力。

Abstract: Can a video generation model be repurposed as an interactive world simulator?
We explore the affordance perception potential of text-to-video models by
teaching them to predict human-environment interaction. Given a scene image and
a prompt describing human actions, we fine-tune the model to insert a person
into the scene, while ensuring coherent behavior, appearance, harmonization,
and scene affordance. Unlike prior work, we infer human affordance for video
generation (i.e., where to insert a person and how they should behave) from a
single scene image, without explicit conditions like bounding boxes or body
poses. An in-depth study of cross-attention heatmaps demonstrates that we can
uncover the inherent affordance perception of a pre-trained video model without
labeled affordance datasets.

</details>


### [26] [Training for X-Ray Vision: Amodal Segmentation, Amodal Content Completion, and View-Invariant Object Representation from Multi-Camera Video](https://arxiv.org/abs/2507.00339)
*Alexander Moore,Amar Saini,Kylie Cancilla,Doug Poland,Carmen Carrano*

Main category: cs.CV

TL;DR: 该论文提出了名为MOVi-MC-AC的多摄像头多目标视频和遮挡内容数据集，为遮挡分割和遮挡内容补全提供了新数据。


<details>
  <summary>Details</summary>
Motivation: 现有的遮挡分割任务缺乏多摄像头视角下对象上下文的额外信息维度，同时缺乏真实的遮挡内容数据，限制了对象分割、检测和跟踪技术的发展。

Method: 通过模拟家庭场景的多摄像头视频，MOVi-MC-AC提供了涵盖多摄像头视角和遮挡内容的分割和检测标签，数据集中具有一致的跨帧和跨视角对象ID，以及自然遮挡情况下的遮挡内容标签。

Result: MOVi-MC-AC数据集包含约580万个对象实例，这是目前最大规模的遮挡数据集，并首次为遮挡内容提供了真实值标签。

Conclusion: MOVi-MC-AC为计算机视觉任务如对象检测、跟踪和分割提供了丰富的数据支持，开创性地引入了多摄像头视角和遮挡内容任务的数据资源。

Abstract: Amodal segmentation and amodal content completion require using object priors
to estimate occluded masks and features of objects in complex scenes. Until
now, no data has provided an additional dimension for object context: the
possibility of multiple cameras sharing a view of a scene. We introduce
MOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the
largest amodal segmentation and first amodal content dataset to date. Cluttered
scenes of generic household objects are simulated in multi-camera video.
MOVi-MC-AC contributes to the growing literature of object detection, tracking,
and segmentation by including two new contributions to the deep learning for
computer vision world. Multiple Camera (MC) settings where objects can be
identified and tracked between various unique camera perspectives are rare in
both synthetic and real-world video. We introduce a new complexity to synthetic
video by providing consistent object ids for detections and segmentations
between both frames and multiple cameras each with unique features and motion
patterns on a single scene. Amodal Content (AC) is a reconstructive task in
which models predict the appearance of target objects through occlusions. In
the amodal segmentation literature, some datasets have been released with
amodal detection, tracking, and segmentation labels. While other methods rely
on slow cut-and-paste schemes to generate amodal content pseudo-labels, they do
not account for natural occlusions present in the modal masks. MOVi-MC-AC
provides labels for ~5.8 million object instances, setting a new maximum in the
amodal dataset literature, along with being the first to provide ground-truth
amodal content. The full dataset is available at
https://huggingface.co/datasets/Amar-S/MOVi-MC-AC ,

</details>


### [27] [CGEarthEye:A High-Resolution Remote Sensing Vision Foundation Model Based on the Jilin-1 Satellite Constellation](https://arxiv.org/abs/2507.00356)
*Zhiwei Yi,Xin Cheng,Jingyu Ma,Ruifei Zhu,Junwei Tian,Yuanxiu Zhou,Xinge Zhao,Hongzhe Li*

Main category: cs.CV

TL;DR: 本研究提出了一种面向Jilin-1卫星的高分辨率遥感视觉基石模型框架CGEarthEye，并展示其在多项遥感任务中的领先性能。


<details>
  <summary>Details</summary>
Motivation: 解决由于高分辨率遥感数据获取渠道受限，从而限制了遥感视觉基础模型发展的问题。通过利用Jilin-1丰富的亚米级图像资源，推进高分辨率遥感数据的智能分析及应用。

Method: 构建了包含五种不同参数规模的CGEarthEye框架，总计21亿参数；开发了JLSSD数据集（1,500万规模的多时相自监督学习数据集），并采用多样化的对比学习策略（包括季节对比、增强对比、掩码补丁令牌对比）进行预训练。

Result: CGEarthEye在涵盖四类遥感任务的10个基准数据集上实现了SOTA性能；模型在特征可视化、收敛性、参数效率及实际映射应用方面表现出优越性能。

Conclusion: CGEarthEye的卓越表示能力有望推动Jilin-1数据在传统地球观测应用中的广泛和高效应用。

Abstract: Deep learning methods have significantly advanced the development of
intelligent rinterpretation in remote sensing (RS), with foundational model
research based on large-scale pre-training paradigms rapidly reshaping various
domains of Earth Observation (EO). However, compared to the open accessibility
and high spatiotemporal coverage of medium-resolution data, the limited
acquisition channels for ultra-high-resolution optical RS imagery have
constrained the progress of high-resolution remote sensing vision foundation
models (RSVFM). As the world's largest sub-meter-level commercial RS satellite
constellation, the Jilin-1 constellation possesses abundant sub-meter-level
image resources. This study proposes CGEarthEye, a RSVFM framework specifically
designed for Jilin-1 satellite characteristics, comprising five backbones with
different parameter scales with totaling 2.1 billion parameters. To enhance the
representational capacity of the foundation model, we developed JLSSD, the
first 15-million-scale multi-temporal self-supervised learning (SSL) dataset
featuring global coverage with quarterly temporal sampling within a single
year, constructed through multi-level representation clustering and sampling
strategies. The framework integrates seasonal contrast, augmentation-based
contrast, and masked patch token contrastive strategies for pre-training.
Comprehensive evaluations across 10 benchmark datasets covering four typical RS
tasks demonstrate that the CGEarthEye consistently achieves state-of-the-art
(SOTA) performance. Further analysis reveals CGEarthEye's superior
characteristics in feature visualization, model convergence, parameter
efficiency, and practical mapping applications. This study anticipates that the
exceptional representation capabilities of CGEarthEye will facilitate broader
and more efficient applications of Jilin-1 data in traditional EO application.

</details>


### [28] [GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And Dynamic Density Control](https://arxiv.org/abs/2507.00363)
*Xingjun Wang,Lianlei Shan*

Main category: cs.CV

TL;DR: 该论文提出了对3D Gaussian Splatting (3DGS)的改进方法，解决了初始化、优化及密度控制的相关挑战。


<details>
  <summary>Details</summary>
Motivation: 3DGS尽管因其显式的3D高斯表示支持实时渲染，但受限于初始化精度、优化难度和密度控制不足等问题。

Method: 提出了几何引导初始化、表面对齐优化策略及动态自适应密度控制机制，以提升精度与视觉效果。

Result: 该方法在复杂场景中实现了高保真实时渲染，视觉质量显著提升，效果优于或媲美当前最前沿方法。

Conclusion: 通过新的初始化、优化和密度控制机制，该方法改进了3DGS，使其在复杂场景中实现高质量实时渲染。

Abstract: We propose a method to enhance 3D Gaussian Splatting (3DGS)~\cite{Kerbl2023},
addressing challenges in initialization, optimization, and density control.
Gaussian Splatting is an alternative for rendering realistic images while
supporting real-time performance, and it has gained popularity due to its
explicit 3D Gaussian representation. However, 3DGS heavily depends on accurate
initialization and faces difficulties in optimizing unstructured Gaussian
distributions into ordered surfaces, with limited adaptive density control
mechanism proposed so far. Our first key contribution is a geometry-guided
initialization to predict Gaussian parameters, ensuring precise placement and
faster convergence. We then introduce a surface-aligned optimization strategy
to refine Gaussian placement, improving geometric accuracy and aligning with
the surface normals of the scene. Finally, we present a dynamic adaptive
density control mechanism that adjusts Gaussian density based on regional
complexity, for visual fidelity. These innovations enable our method to achieve
high-fidelity real-time rendering and significant improvements in visual
quality, even in complex scenes. Our method demonstrates comparable or superior
results to state-of-the-art methods, rendering high-fidelity images in real
time.

</details>


### [29] [An Improved U-Net Model for Offline handwriting signature denoising](https://arxiv.org/abs/2507.00365)
*Wanghui Xiao*

Main category: cs.CV

TL;DR: 该研究提出了一种基于改进U-net结构的签名字体去噪模型，应用在签名识别系统中。


<details>
  <summary>Details</summary>
Motivation: 签名作为身份识别的重要手段，在司法鉴定、金融交易、商业合同等领域至关重要。但签名样本中混杂的干扰信息对字体识别工作带来严峻挑战。

Method: 通过引入离散小波变换和PCA变换，增强模型的去噪能力；提出基于改进的U-net结构的去噪模型。

Result: 实验结果表明，该模型去噪效果显著优于传统方法，能够有效提升签名图像的清晰度和可读性。

Conclusion: 该研究为签名分析与识别提供了更加可靠的技术支持，并有助于提升系统的鲁棒性。

Abstract: Handwriting signatures, as an important means of identity recognition, are
widely used in multiple fields such as financial transactions, commercial
contracts and personal affairs due to their legal effect and uniqueness. In
forensic science appraisals, the analysis of offline handwriting signatures
requires the appraiser to provide a certain number of signature samples, which
are usually derived from various historical contracts or archival materials.
However, the provided handwriting samples are often mixed with a large amount
of interfering information, which brings severe challenges to handwriting
identification work. This study proposes a signature handwriting denoising
model based on the improved U-net structure, aiming to enhance the robustness
of the signature recognition system. By introducing discrete wavelet transform
and PCA transform, the model's ability to suppress noise has been enhanced. The
experimental results show that this modelis significantly superior to the
traditional methods in denoising effect, can effectively improve the clarity
and readability of the signed images, and provide more reliable technical
support for signature analysis and recognition.

</details>


### [30] [Out-of-Distribution Detection with Adaptive Top-K Logits Integration](https://arxiv.org/abs/2507.00368)
*Hikaru Shijo,Yutaka Yoshihama,Kenichi Yadani,Norifumi Murata*

Main category: cs.CV

TL;DR: 本论文提出了一种名为ATLI(Adaptive Top-k Logits Integration)的新方法，用于改进神经网络的异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络在处理分布外(OOD)样本时容易过度自信，导致潜在安全问题，因此提高OOD检测能力至关重要。

Method: 发现除最大logit外，部分其他logits对OOD检测也有帮助，于是根据每个模型适配性地选择前k个logits并结合最大logit，提出了新的方法ATLI。

Result: 在ImageNet-1K基准测试中，与MaxLogit方法相比，ATLI将错误率(FPR95)降低了6.73%，相较于其他前沿方法进一步降低了2.67%。

Conclusion: ATLI方法通过自适应地整合多个logits来有效提升了神经网络的OOD检测能力，且在实验中表现优异。

Abstract: Neural networks often make overconfident predictions from out-of-distribution
(OOD) samples. Detection of OOD data is therefore crucial to improve the safety
of machine learning. The simplest and most powerful method for OOD detection is
MaxLogit, which uses the model's maximum logit to provide an OOD score. We have
discovered that, in addition to the maximum logit, some other logits are also
useful for OOD detection. Based on this finding, we propose a new method called
ATLI (Adaptive Top-k Logits Integration), which adaptively determines effective
top-k logits that are specific to each model and combines the maximum logit
with the other top-k logits. In this study we evaluate our proposed method
using ImageNet-1K benchmark. Extensive experiments showed our proposed method
to reduce the false positive rate (FPR95) by 6.73% compared to the MaxLogit
approach, and decreased FPR95 by an additional 2.67% compared to other
state-of-the-art methods.

</details>


### [31] [PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance point cloud reconstruction via joint-channel NeRF with multi-view image instance matching](https://arxiv.org/abs/2507.00371)
*Xin Yang,Ruiming Du,Hanyang Huang,Jiayang Xie,Pengyao Xie,Leisen Fang,Ziyue Guo,Nanjun Jiang,Yu Jiang,Haiyan Cen*

Main category: cs.CV

TL;DR: PlantSegNeRF通过多视角RGB图像序列直接生成高精度的植物器官点云，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有植物点云分割技术在分辨率、分割精度及跨物种泛化能力方面仍存在不足。

Method: 提出了PlantSegNeRF方法，结合2D实例分割、多视角实例匹配模块及实例NeRF技术，将隐式场景转换为高精度的植物点云。

Result: PlantSegNeRF在复杂结构数据集中显著超越主流方法，在mPrec、mRec、mCov、mWCov等指标上分别提升11.7%、38.2%、32.2%、25.3%。

Conclusion: PlantSegNeRF扩展了器官级植物表型分析能力，为植物科学领域大规模模型开发提供高质量3D数据。

Abstract: Organ segmentation of plant point clouds is a prerequisite for the
high-resolution and accurate extraction of organ-level phenotypic traits.
Although the fast development of deep learning has boosted much research on
segmentation of plant point clouds, the existing techniques for organ
segmentation still face limitations in resolution, segmentation accuracy, and
generalizability across various plant species. In this study, we proposed a
novel approach called plant segmentation neural radiance fields (PlantSegNeRF),
aiming to directly generate high-precision instance point clouds from
multi-view RGB image sequences for a wide range of plant species. PlantSegNeRF
performed 2D instance segmentation on the multi-view images to generate
instance masks for each organ with a corresponding ID. The multi-view instance
IDs corresponding to the same plant organ were then matched and refined using a
specially designed instance matching module. The instance NeRF was developed to
render an implicit scene, containing color, density, semantic and instance
information. The implicit scene was ultimately converted into high-precision
plant instance point clouds based on the volume density. The results proved
that in semantic segmentation of point clouds, PlantSegNeRF outperformed the
commonly used methods, demonstrating an average improvement of 16.1%, 18.3%,
17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the
second-best results on structurally complex datasets. More importantly,
PlantSegNeRF exhibited significant advantages in plant point cloud instance
segmentation tasks. Across all plant datasets, it achieved average improvements
of 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively.
This study extends the organ-level plant phenotyping and provides a
high-throughput way to supply high-quality 3D data for the development of
large-scale models in plant science.

</details>


### [32] [Efficient Depth- and Spatially-Varying Image Simulation for Defocus Deblur](https://arxiv.org/abs/2507.00372)
*Xinge Yang,Chuong Nguyen,Wenbin Wang,Kaizhang Kang,Wolfgang Heidrich,Xiaoxing Li*

Main category: cs.CV

TL;DR: 提出了一种新的高效可扩展数据集生成方法，解决了因焦外浅景深导致的图像模糊问题，同时优化了深度学习模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现代大孔径固定焦距相机因浅景深导致的图像模糊问题，同时填补现有公开数据集的领域缺口，以实现深度学习模型在实际场景中的优异表现。

Method: 开发了一种不依赖于真实数据微调的合成数据集生成方法，能够同时建模深度相关的散焦和光学像差特性，并兼顾计算复杂性与高质量RGB-D数据集的稀缺性。

Result: 实验结果显示，使用低分辨率合成图像训练的网络能有效泛化到分辨率高达12MP的真实场景图像。

Conclusion: 所提方法能够提升智能眼镜等固定焦距相机在多样场景下的成像效果，具有高效性和实际应用潜力。

Abstract: Modern cameras with large apertures often suffer from a shallow depth of
field, resulting in blurry images of objects outside the focal plane. This
limitation is particularly problematic for fixed-focus cameras, such as those
used in smart glasses, where adding autofocus mechanisms is challenging due to
form factor and power constraints. Due to unmatched optical aberrations and
defocus properties unique to each camera system, deep learning models trained
on existing open-source datasets often face domain gaps and do not perform well
in real-world settings. In this paper, we propose an efficient and scalable
dataset synthesis approach that does not rely on fine-tuning with real-world
data. Our method simultaneously models depth-dependent defocus and spatially
varying optical aberrations, addressing both computational complexity and the
scarcity of high-quality RGB-D datasets. Experimental results demonstrate that
a network trained on our low resolution synthetic images generalizes
effectively to high resolution (12MP) real-world images across diverse scenes.

</details>


### [33] [Customizable ROI-Based Deep Image Compression](https://arxiv.org/abs/2507.00373)
*Ian Jin,Fanxin Xia,Feng Ding,Xinfeng Zhang,Meiqin Liu,Yao Zhao,Weisi Lin,Lili Meng*

Main category: cs.CV

TL;DR: 提出一种可定制的ROI（兴趣区域）深度图像压缩方法，使用文本控制和灵活质量权衡机制，显著提高了压缩的灵活性和用户定制性。


<details>
  <summary>Details</summary>
Motivation: 现有的ROI图像压缩方法通常使用预定义的ROI，难以满足不同用户需求中的个性化定制需求，例如ROI定义和ROI与非ROI区域之间的质量权衡。

Method: 1. 开发了一个文本控制的Mask管理模块（TMA），通过输入语义文本定制ROI； 2. 设计一个可定制的价值赋值机制（CVA），允许用户调整非ROI区域的掩膜程度以控制质量权衡； 3. 提出了潜在掩膜注意模块（LMA），在潜在空间内提取并融合掩膜空间先验和率失真优化先验，优化图像的潜在表示。

Result: 实验结果证明，该方法能够有效支持对ROI定义、掩膜获取以及ROI与非ROI区域之间重建质量权衡的定制需求。

Conclusion: 研究提出了一种灵活、可定制的ROI深度图像压缩方案，能够满足多元化的用户需求，包括可变的ROI定义和灵活的质量权衡机制。

Abstract: Region of Interest (ROI)-based image compression optimizes bit allocation by
prioritizing ROI for higher-quality reconstruction. However, as the users
(including human clients and downstream machine tasks) become more diverse,
ROI-based image compression needs to be customizable to support various
preferences. For example, different users may define distinct ROI or require
different quality trade-offs between ROI and non-ROI. Existing ROI-based image
compression schemes predefine the ROI, making it unchangeable, and lack
effective mechanisms to balance reconstruction quality between ROI and non-ROI.
This work proposes a paradigm for customizable ROI-based deep image
compression. First, we develop a Text-controlled Mask Acquisition (TMA) module,
which allows users to easily customize their ROI for compression by just
inputting the corresponding semantic \emph{text}. It makes the encoder
controlled by text. Second, we design a Customizable Value Assign (CVA)
mechanism, which masks the non-ROI with a changeable extent decided by users
instead of a constant one to manage the reconstruction quality trade-off
between ROI and non-ROI. Finally, we present a Latent Mask Attention (LMA)
module, where the latent spatial prior of the mask and the latent
Rate-Distortion Optimization (RDO) prior of the image are extracted and fused
in the latent space, and further used to optimize the latent representation of
the source image. Experimental results demonstrate that our proposed
customizable ROI-based deep image compression paradigm effectively addresses
the needs of customization for ROI definition and mask acquisition as well as
the reconstruction quality trade-off management between the ROI and non-ROI.

</details>


### [34] [MedDiff-FT: Data-Efficient Diffusion Model Fine-tuning with Structural Guidance for Controllable Medical Image Synthesis](https://arxiv.org/abs/2507.00377)
*Jianhao Xie,Ziang Zhang,Zhenyu Weng,Yuesheng Zhu,Guibo Luo*

Main category: cs.CV

TL;DR: 本文提出了一种名为MedDiff-FT的方法，通过微调扩散基础模型来生成结构依赖性和领域特异性的医学图像，有效解决了高质量数据匮乏的问题。


<details>
  <summary>Details</summary>
Motivation: 当前医学图像分割深度学习受制于高质量训练数据的缺乏，扩散模型虽然能生成合成图像，但受限于对大规模数据集的依赖和图像质量问题。为此提出一种高效且数据敏感的方法，用于生成质量更高的医学图像。

Method: 提出MedDiff-FT方法，在推理过程中采用动态自适应引导掩码确保解剖结构一致性，轻量级随机掩码生成器通过分层随机注入提高多样性，同时通过特征空间指标自动滤除低质量输出并进一步优化。

Result: 在五个医学分割数据集上评估，生成的图像和掩码对提升当前分割方法的Dice分数平均有1%的提高。

Conclusion: 该框架有效平衡了生成质量、多样性和计算效率，为医学数据增强提供了一种实用方案。

Abstract: Recent advancements in deep learning for medical image segmentation are often
limited by the scarcity of high-quality training data.While diffusion models
provide a potential solution by generating synthetic images, their
effectiveness in medical imaging remains constrained due to their reliance on
large-scale medical datasets and the need for higher image quality. To address
these challenges, we present MedDiff-FT, a controllable medical image
generation method that fine-tunes a diffusion foundation model to produce
medical images with structural dependency and domain specificity in a
data-efficient manner. During inference, a dynamic adaptive guiding mask
enforces spatial constraints to ensure anatomically coherent synthesis, while a
lightweight stochastic mask generator enhances diversity through hierarchical
randomness injection. Additionally, an automated quality assessment protocol
filters suboptimal outputs using feature-space metrics, followed by mask
corrosion to refine fidelity. Evaluated on five medical segmentation
datasets,MedDiff-FT's synthetic image-mask pairs improve SOTA method's
segmentation performance by an average of 1% in Dice score. The framework
effectively balances generation quality, diversity, and computational
efficiency, offering a practical solution for medical data augmentation. The
code is available at https://github.com/JianhaoXie1/MedDiff-FT.

</details>


### [35] [Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space](https://arxiv.org/abs/2507.00392)
*Yingping Liang,Yutao Hu,Wenqi Shao,Ying Fu*

Main category: cs.CV

TL;DR: 该论文提出了一种提升2D图像到3D空间的特征匹配框架“Lift to Match (L2M)”，通过在训练中结合3D几何知识和跨视图重建技术，解决现有方法的泛化性问题。


<details>
  <summary>Details</summary>
Motivation: 现有特征匹配方法过于依赖多视角图像，泛化能力不足，同时2D特征编码器难以捕捉3D相关信息。因此，作者提出一种能充分利用单视角图像的数据驱动方法。

Method: 提出了两阶段框架，第一阶段通过多视角图像合成与3D特征表示训练获取具有3D感知的编码器；第二阶段结合新颖视角渲染和大规模合成数据生成训练特征解码器，从而实现鲁棒的特征匹配。

Result: 该方法在多项零样本评估基准上表现出色，展示了其对于跨域泛化的卓越能力。

Conclusion: 提出的L2M框架有效提升了特征匹配的泛化性和鲁棒性，为计算机视觉任务中的特征匹配提供了新思路。

Abstract: Feature matching plays a fundamental role in many computer vision tasks, yet
existing methods heavily rely on scarce and clean multi-view image collections,
which constrains their generalization to diverse and challenging scenarios.
Moreover, conventional feature encoders are typically trained on single-view 2D
images, limiting their capacity to capture 3D-aware correspondences. In this
paper, we propose a novel two-stage framework that lifts 2D images to 3D space,
named as \textbf{Lift to Match (L2M)}, taking full advantage of large-scale and
diverse single-view images. To be specific, in the first stage, we learn a
3D-aware feature encoder using a combination of multi-view image synthesis and
3D feature Gaussian representation, which injects 3D geometry knowledge into
the encoder. In the second stage, a novel-view rendering strategy, combined
with large-scale synthetic data generation from single-view images, is employed
to learn a feature decoder for robust feature matching, thus achieving
generalization across diverse domains. Extensive experiments demonstrate that
our method achieves superior generalization across zero-shot evaluation
benchmarks, highlighting the effectiveness of the proposed framework for robust
feature matching.

</details>


### [36] [Few-shot Classification as Multi-instance Verification: Effective Backbone-agnostic Transfer across Domains](https://arxiv.org/abs/2507.00401)
*Xin Xu,Eibe Frank,Geoffrey Holmes*

Main category: cs.CV

TL;DR: 提出了一种新方法“MIV-head”，在无需微调预训练主干网络的情况下实现跨域少样本学习。


<details>
  <summary>Details</summary>
Motivation: 解决在预训练特征提取网络不可微调情况下的跨域少样本学习问题，这种问题在实际应用中越来越普遍。

Method: 将少样本分类问题建模为多实例验证任务，并设计了一种独立于预训练主干网络的分类头'MIV-head'，通过目标域少样本数据训练完成适配。

Result: 在多种设置下扩展Meta-dataset基准测试中，MIV-head在无需微调网络的情况下，性能与部分微调方法持平甚至更优，并显著降低适配成本。

Conclusion: MIV-head是一种高效、无需微调的跨域少样本学习方法，相比传统分类头方法有显著优势，代码已开源。

Abstract: We investigate cross-domain few-shot learning under the constraint that
fine-tuning of backbones (i.e., feature extractors) is impossible or infeasible
-- a scenario that is increasingly common in practical use cases. Handling the
low-quality and static embeddings produced by frozen, "black-box" backbones
leads to a problem representation of few-shot classification as a series of
multiple instance verification (MIV) tasks. Inspired by this representation, we
introduce a novel approach to few-shot domain adaptation, named the "MIV-head",
akin to a classification head that is agnostic to any pretrained backbone and
computationally efficient. The core components designed for the MIV-head, when
trained on few-shot data from a target domain, collectively yield strong
performance on test data from that domain. Importantly, it does so without
fine-tuning the backbone, and within the "meta-testing" phase. Experimenting
under various settings and on an extension of the Meta-dataset benchmark for
cross-domain few-shot image classification, using representative off-the-shelf
convolutional neural network and vision transformer backbones pretrained on
ImageNet1K, we show that the MIV-head achieves highly competitive accuracy when
compared to state-of-the-art "adapter" (or partially fine-tuning) methods
applied to the same backbones, while incurring substantially lower adaptation
cost. We also find well-known "classification head" approaches lag far behind
in terms of accuracy. Ablation study empirically justifies the core components
of our approach. We share our code at https://github.com/xxweka/MIV-head.

</details>


### [37] [DiGA3D: Coarse-to-Fine Diffusional Propagation of Geometry and Appearance for Versatile 3D Inpainting](https://arxiv.org/abs/2507.00429)
*Jingyi Pan,Dan Xu,Qiong Luo*

Main category: cs.CV

TL;DR: 论文提出了一种统一的3D修复管道DiGA3D，通过利用扩散模型解决多视图一致性和几何一致性问题，提升了修复效果。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有方法多视图修复中单一参照视图的问题，以及2D扩散模型带来的外观和几何一致性问题，作者希望开发一种通用的3D修复框架。

Method: 提出DiGA3D管道，包括多视图选取策略、注意力特征传播机制（AFP）及纹理几何分数蒸馏采样（TG-SDS）损失，逐步改善多视图间一致性及几何准确性。

Result: 实验表明，DiGA3D在多个3D修复任务中表现卓越，能更好地传递外观与几何的一致性。

Conclusion: DiGA3D通过一系列创新的算法设计，成功解决了现有3D修补技术中的多视图外观一致性和几何一致性的问题，验证了其实用价值。

Abstract: Developing a unified pipeline that enables users to remove, re-texture, or
replace objects in a versatile manner is crucial for text-guided 3D inpainting.
However, there are still challenges in performing multiple 3D inpainting tasks
within a unified framework: 1) Single reference inpainting methods lack
robustness when dealing with views that are far from the reference view. 2)
Appearance inconsistency arises when independently inpainting multi-view images
with 2D diffusion priors; 3) Geometry inconsistency limits performance when
there are significant geometric changes in the inpainting regions. To tackle
these challenges, we introduce DiGA3D, a novel and versatile 3D inpainting
pipeline that leverages diffusion models to propagate consistent appearance and
geometry in a coarse-to-fine manner. First, DiGA3D develops a robust strategy
for selecting multiple reference views to reduce errors during propagation.
Next, DiGA3D designs an Attention Feature Propagation (AFP) mechanism that
propagates attention features from the selected reference views to other views
via diffusion models to maintain appearance consistency. Furthermore, DiGA3D
introduces a Texture-Geometry Score Distillation Sampling (TG-SDS) loss to
further improve the geometric consistency of inpainted 3D scenes. Extensive
experiments on multiple 3D inpainting tasks demonstrate the effectiveness of
our method. The project page is available at https://rorisis.github.io/DiGA3D/.

</details>


### [38] [MFH: Marrying Frequency Domain with Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2507.00430)
*Huanxin Yang,Qiwen Wang*

Main category: cs.CV

TL;DR: 本文提出了一种将频域分析融入手写数学表达式识别的新方法（MFH），并展示了其性能提升与DCT结合的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决手写数学表达式识别中因复杂公式与字符布局导致的准确性问题，同时利用频域分析提升模型结构理解能力。

Method: 将离散余弦变换（DCT）应用于手写数学表达式识别，并提出一种结合频域和空间特征的网络方法（MFH）。

Result: 在CROHME 2014/2016/2019测试集上，MFH-CoMER分别取得了61.66%、62.07%、63.72%的准确率，并在多个基线模型上实现性能提升。

Conclusion: 频域信息对于增强公式识别具有显著效果，所提出的方法MFH展示了较高的准确率，具有应用前景。

Abstract: Handwritten mathematical expression recognition (HMER) suffers from complex
formula structures and character layouts in sequence prediction. In this paper,
we incorporate frequency domain analysis into HMER and propose a method that
marries frequency domain with HMER (MFH), leveraging the discrete cosine
transform (DCT). We emphasize the structural analysis assistance of frequency
information for recognizing mathematical formulas. When implemented on various
baseline models, our network exhibits a consistent performance enhancement,
demonstrating the efficacy of frequency domain information. Experiments show
that our MFH-CoMER achieves noteworthy accuracyrates of 61.66%/62.07%/63.72% on
the CROHME 2014/2016/2019 test sets. The source code is available at
https://github.com/Hryxyhe/MFH.

</details>


### [39] [Latent Posterior-Mean Rectified Flow for Higher-Fidelity Perceptual Face Restoration](https://arxiv.org/abs/2507.00447)
*Xin Luo,Menglin Zhang,Yunwei Lan,Tianyu Zhang,Rui Li,Chang Liu,Dong Liu*

Main category: cs.CV

TL;DR: 提出了一种基于变分自编码器（VAE）的Latent-PMRF方法，以在脸部修复的感知质量和保真度之间实现更好的权衡，较PMRF实现显著的性能提升和较快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 针对传统PMRF方法在像素空间中难以与人类感知对齐的问题，探索如何通过潜在空间的建模提高与人类感知一致的修复效果。

Method: 将PMRF重新设计到变分自编码器的潜在空间中，通过对VAE的潜在表示形式进行优化，实现更接近人类感知的目标，且提出了重新设计的高性能VAE模型。

Result: Latent-PMRF在盲脸修复任务中表现优异，相较于现有方法提供了更优的感知-失真权衡，同时其收敛效率提升了5.79倍。

Conclusion: Latent-PMRF显著改进了脸部修复过程中感知质量与失真程度之间的平衡，并通过优化VAE设计提高了效率和性能，为潜在空间建模提供了新思路。

Abstract: The Perception-Distortion tradeoff (PD-tradeoff) theory suggests that face
restoration algorithms must balance perceptual quality and fidelity. To achieve
minimal distortion while maintaining perfect perceptual quality, Posterior-Mean
Rectified Flow (PMRF) proposes a flow based approach where source distribution
is minimum distortion estimations. Although PMRF is shown to be effective, its
pixel-space modeling approach limits its ability to align with human
perception, where human perception is defined as how humans distinguish between
two image distributions. In this work, we propose Latent-PMRF, which
reformulates PMRF in the latent space of a variational autoencoder (VAE),
facilitating better alignment with human perception during optimization. By
defining the source distribution on latent representations of minimum
distortion estimation, we bound the minimum distortion by the VAE's
reconstruction error. Moreover, we reveal the design of VAE is crucial, and our
proposed VAE significantly outperforms existing VAEs in both reconstruction and
restoration. Extensive experiments on blind face restoration demonstrate the
superiority of Latent-PMRF, offering an improved PD-tradeoff compared to
existing methods, along with remarkable convergence efficiency, achieving a
5.79X speedup over PMRF in terms of FID. Our code will be available as
open-source.

</details>


### [40] [ATSTrack: Enhancing Visual-Language Tracking by Aligning Temporal and Spatial Scales](https://arxiv.org/abs/2507.00454)
*Yihao Zhen,Qiang Wang,Yu Qiao,Liangqiong Qu,Huijie Fan*

Main category: cs.CV

TL;DR: ATSTrack通过对视觉和语言输入的时空尺度对齐来提升特征修改效果，解决了视觉和语言信息匹配的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言跟踪中由于目标移动导致的视觉输入和语言描述对齐问题，以及其在时空尺度上的内在差异。

Method: 提出一种新型跟踪方法，按时空对应性分解语言描述并细粒度修改特征，同时引入视觉-语言Token从上一帧中提取修改后的语言信息以优化特征提取。

Result: 实验表明，ATSTrack的性能与现有方法相当。

Conclusion: ATSTrack有效解决了时空尺度差异对视觉语言跟踪的影响，是一种性能可靠的方法。

Abstract: A main challenge of Visual-Language Tracking (VLT) is the misalignment
between visual inputs and language descriptions caused by target movement.
Previous trackers have explored many effective feature modification methods to
preserve more aligned features. However, an important yet unexplored factor
ultimately hinders their capability, which is the inherent differences in the
temporal and spatial scale of information between visual and language inputs.
To address this issue, we propose a novel visual-language tracker that enhances
the effect of feature modification by \textbf{A}ligning \textbf{T}emporal and
\textbf{S}patial scale of different input components, named as
\textbf{ATSTrack}. Specifically, we decompose each language description into
phrases with different attributes based on their temporal and spatial
correspondence with visual inputs, and modify their features in a fine-grained
manner. Moreover, we introduce a Visual-Language token that comprises modified
linguistic information from the previous frame to guide the model to extract
visual features that are more relevant to language description, thereby
reducing the impact caused by the differences in spatial scale. Experimental
results show that our proposed ATSTrack achieves performance comparable to
existing methods. Our code will be released.

</details>


### [41] [Unleashing the Potential of All Test Samples: Mean-Shift Guided Test-Time Adaptation](https://arxiv.org/abs/2507.00462)
*Jizhou Han,Chenhao Ding,SongLin Dong,Yuhang He,Xinyuan Gao,Yihong Gong*

Main category: cs.CV

TL;DR: 提出MS-TTA方法，通过改进特征表示实现更稳定的测试时适应性，比现有的训练自由适应方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决当前Visual-language模型（如CLIP）在测试时分布转移情况下表现差的问题。

Method: 利用单步kNN-Mean-Shift实现特征表示的改进，并通过缓存优化的特征嵌入来增强推理。

Result: 在OOD和跨数据集基准测试中，MS-TTA始终优于现有的方法，在无需额外训练的情况下实现了鲁棒的适应性。

Conclusion: MS-TTA增强了特征紧凑性和类别分离性，进一步借助缓存优化后特征的推理能力，显著提升了模型测试时性能，展现了无训练适应的优势。

Abstract: Visual-language models (VLMs) like CLIP exhibit strong generalization but
struggle with distribution shifts at test time. Existing training-free
test-time adaptation (TTA) methods operate strictly within CLIP's original
feature space, relying on high-confidence samples while overlooking the
potential of low-confidence ones. We propose MS-TTA, a training-free approach
that enhances feature representations beyond CLIP's space using a single-step
k-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA
improves feature compactness and class separability, leading to more stable
adaptation. Additionally, a cache of refined embeddings further enhances
inference by providing Mean Shift enhanced logits. Extensive evaluations on OOD
and cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms
state-of-the-art training-free TTA methods, achieving robust adaptation without
requiring additional training.

</details>


### [42] [Bisecle: Binding and Separation in Continual Learning for Video Language Understanding](https://arxiv.org/abs/2507.00469)
*Yue Tan,Xiaoqian Hu,Hao Xue,Celso De Melo,Flora D. Salim*

Main category: cs.CV

TL;DR: 提出了一种名为Bisecle的新方法，通过模仿人脑海马体的快速绑定和模式分离机制，改善视频-语言模型在视频理解任务中的持续学习能力，并在多个VideoQA基准测试中验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 传统视频理解模型在处理不断变化的数据流和新任务场景时，因计算开销过高，通常只能更新少量参数，而大部分参数保持冻结。此种方式容易导致灾难性遗忘和更新冲突，因此需要一种高效且避免遗忘的持续学习策略。

Method: 提出了一种名为Bisecle的方法，通过多方向监督模块捕获更多跨模态关系，并设计对比提示学习方案以分离任务特定知识，提高记忆存储能力，同时采用绑定和分离机制强化模型保留复杂经验的能力。

Result: 在多个VideoQA基准测试中，Bisecle表现出较好的减少遗忘能力，并显著增强了跨任务的泛化能力。

Conclusion: Bisecle方法模仿了海马体的记忆机制，能够在视频理解任务中实现高效和稳健的持续学习，展现了其性能和潜力。

Abstract: Frontier vision-language models (VLMs) have made remarkable improvements in
video understanding tasks. However, real-world videos typically exist as
continuously evolving data streams (e.g., dynamic scenes captured by wearable
glasses), necessitating models to continually adapt to shifting data
distributions and novel scenarios. Considering the prohibitive computational
costs of fine-tuning models on new tasks, usually, a small subset of parameters
is updated while the bulk of the model remains frozen. This poses new
challenges to existing continual learning frameworks in the context of large
multimodal foundation models, i.e., catastrophic forgetting and update
conflict. While the foundation models struggle with parameter-efficient
continual learning, the hippocampus in the human brain has evolved highly
efficient mechanisms for memory formation and consolidation. Inspired by the
rapid Binding and pattern separation mechanisms in the hippocampus, in this
work, we propose Bisecle for video-language continual learning, where a
multi-directional supervision module is used to capture more cross-modal
relationships and a contrastive prompt learning scheme is designed to isolate
task-specific knowledge to facilitate efficient memory storage. Binding and
separation processes further strengthen the ability of VLMs to retain complex
experiences, enabling robust and efficient continual learning in video
understanding tasks. We perform a thorough evaluation of the proposed Bisecle,
demonstrating its ability to mitigate forgetting and enhance cross-task
generalization on several VideoQA benchmarks.

</details>


### [43] [ARIG: Autoregressive Interactive Head Generation for Real-time Conversations](https://arxiv.org/abs/2507.00472)
*Ying Guo,Xi Liu,Cheng Zhen,Pengfei Yan,Xiaoming Wei*

Main category: cs.CV

TL;DR: 提出ARIG框架，用于实时生成高逼真互动头部动作。


<details>
  <summary>Details</summary>
Motivation: 解决现有生成模型在未来信号获取、行为理解和切换流畅性方面的不足。

Method: 基于自回归框架，采用非量化向量方式进行运动分布预测，引入互动行为理解(IBU)和详细对话状态理解(CSU)。

Result: 实验验证模型有效性，生成结果具备更高的实时性与互动真实性。

Conclusion: ARIG模型能够更出色地满足实时、真实互动头生成的需求。

Abstract: Face-to-face communication, as a common human activity, motivates the
research on interactive head generation. A virtual agent can generate motion
responses with both listening and speaking capabilities based on the audio or
motion signals of the other user and itself. However, previous clip-wise
generation paradigm or explicit listener/speaker generator-switching methods
have limitations in future signal acquisition, contextual behavioral
understanding, and switching smoothness, making it challenging to be real-time
and realistic. In this paper, we propose an autoregressive (AR) based
frame-wise framework called ARIG to realize the real-time generation with
better interaction realism. To achieve real-time generation, we model motion
prediction as a non-vector-quantized AR process. Unlike discrete codebook-index
prediction, we represent motion distribution using diffusion procedure,
achieving more accurate predictions in continuous space. To improve interaction
realism, we emphasize interactive behavior understanding (IBU) and detailed
conversational state understanding (CSU). In IBU, based on dual-track
dual-modal signals, we summarize short-range behaviors through
bidirectional-integrated learning and perform contextual understanding over
long ranges. In CSU, we use voice activity signals and context features of IBU
to understand the various states (interruption, feedback, pause, etc.) that
exist in actual conversations. These serve as conditions for the final
progressive motion prediction. Extensive experiments have verified the
effectiveness of our model.

</details>


### [44] [ADAptation: Reconstruction-based Unsupervised Active Learning for Breast Ultrasound Diagnosis](https://arxiv.org/abs/2507.00474)
*Yaofei Duan,Yuhao Huang,Xin Yang,Luyi Han,Xinyu Xie,Zhiyuan Zhu,Ping He,Ka-Hou Chan,Ligang Cui,Sio-Kei Im,Dong Ni,Tao Tan*

Main category: cs.CV

TL;DR: 本文提出了一种名为ADAptation的无监督主动学习框架，通过将目标域图像转换为源域风格，实现多领域数据池中信息样本的有效选择，显著提高了深度学习模型在跨域分布上的性能表现。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在训练域和测试域分布变化时效果下降，完整的目标域数据收集标注受限于时间与资源，因此探寻低标注成本同时保持性能的方法成为一个关键问题。

Method: 提出了一种框架ADAptation：利用扩散模型消除跨数据集分布差异，将目标域图像风格化为源域；在此基础上，设计了（a）超球约束的对比学习网络实现紧密特征聚类，以及（b）双评分机制平衡样本不确定性和代表性。

Result: 在四个乳腺超声数据集（三个公共数据集和一个内部多中心数据集）上进行实验，并跨五种常见深度学习分类器，结果表明本文方法优于现有的强大AL方法。

Conclusion: 该方法验证了其在临床领域适配中的有效性和广泛适用性，并提供了一种在有限标注预算下实现多领域数据高效学习的解决方案。

Abstract: Deep learning-based diagnostic models often suffer performance drops due to
distribution shifts between training (source) and test (target) domains.
Collecting and labeling sufficient target domain data for model retraining
represents an optimal solution, yet is limited by time and scarce resources.
Active learning (AL) offers an efficient approach to reduce annotation costs
while maintaining performance, but struggles to handle the challenge posed by
distribution variations across different datasets. In this study, we propose a
novel unsupervised Active learning framework for Domain Adaptation, named
ADAptation, which efficiently selects informative samples from multi-domain
data pools under limited annotation budget. As a fundamental step, our method
first utilizes the distribution homogenization capabilities of diffusion models
to bridge cross-dataset gaps by translating target images into source-domain
style. We then introduce two key innovations: (a) a hypersphere-constrained
contrastive learning network for compact feature clustering, and (b) a
dual-scoring mechanism that quantifies and balances sample uncertainty and
representativeness. Extensive experiments on four breast ultrasound datasets
(three public and one in-house/multi-center) across five common deep
classifiers demonstrate that our method surpasses existing strong AL-based
competitors, validating its effectiveness and generalization for clinical
domain adaptation. The code is available at the anonymized link:
https://github.com/miccai25-966/ADAptation.

</details>


### [45] [Just Noticeable Difference for Large Multimodal Models](https://arxiv.org/abs/2507.00490)
*Zijian Chen,Yuan Tian,Yuze Sun,Wei Sun,Zicheng Zhang,Weisi Lin,Guangtao Zhai,Wenjun Zhang*

Main category: cs.CV

TL;DR: 本文研究了当前大规模多模态模型（LMMs）在视觉感知任务中的显著视盲点，并提出了新概念LMM-JND及其研究方法。


<details>
  <summary>Details</summary>
Motivation: 探索LMMs在视觉感知任务中的局限性，解决因其感知缺陷带来的安全问题和效率不足。

Method: 提出LMM-JND概念及其测定流程，构建大规模数据集VPA-JND，并研究LMMs在多种失真类型下的表现差异与通用行为特点。

Result: 发现当前LMMs（如GPT-4o，InternVL2.5系列）在基本视觉比较任务上显著逊于人类表现，揭示了视觉和语言骨干设计对模型视觉敏锐度的关键影响。

Conclusion: LMM-JND是研究LMMs的新视角，并为未来LMMs优化提供指导，同时对安全性问题具有重要意义。

Abstract: Just noticeable difference (JND), the minimum change that the human visual
system (HVS) can perceive, has been studied for decades. Although recent work
has extended this line of research into machine vision, there has been a
scarcity of studies systematically exploring its perceptual boundaries across
multiple tasks and stimulus types, particularly in the current era of rapidly
advancing large multimodal models (LMMs), where studying the multifaceted
capabilities of models has become a mainstream focus. Moreover, the perceptual
defects of LMMs are not investigated thoroughly, resulting in potential
security issues and suboptimal response efficiency. In this paper, we take an
initial attempt and demonstrate that there exist significant visual blind spots
in current LMMs. To systemically quantify this characteristic, we propose a new
concept, {\bf LMM-JND}, together with its determination pipeline. Targeting
uncovering the behavior commonalities in HVS-aligned visual perception tasks,
we delve into several LMM families and construct a large-scale dataset, named
VPA-JND, which contains 21.5k reference images with over 489k stimuli across 12
distortion types, to facilitate LMM-JND studies. VPA-JND exposes areas where
state-of-the-art LMMs, including GPT-4o and the InternVL2.5 series, struggle
with basic comparison queries and fall significantly short of human-level
visual performance. We further explore the effects of vision and language
backbones and find a notable correlation between their design philosophy that
may instruct the future refinement of LMMs for their visual acuity. Together,
our research underscores the significance of LMM-JND as a unique perspective
for studying LMMs, and predictable LMM-JND is crucial for security concerns.
This work will be available at https://github.com/zijianchen98/LMM-JND.

</details>


### [46] [Visual Anagrams Reveal Hidden Differences in Holistic Shape Processing Across Vision Models](https://arxiv.org/abs/2507.00493)
*Fenil R. Doshi,Thomas Fel,Talia Konkle,George Alvarez*

Main category: cs.CV

TL;DR: 该研究通过提出Configural Shape Score (CSS)评估视觉模型的全局配置形状识别能力，发现部分语言对齐和自监督模型表现优异，并揭示高CSS模型依赖于长距离交互和全局编码能力的转变。


<details>
  <summary>Details</summary>
Motivation: 当前视觉模型在形状和纹理表征上存在偏差，研究希望通过更准确的方法来评估和改进模型对全局形状配置的识别能力。

Method: 提出Configural Shape Score (CSS)方法，通过评估模型在Object-Anagram配对图像中的表现来衡量其全局形状配置能力，并对多种模型进行性能测评。

Result: 实验表明，自监督和语言对齐的Transformer（如DINOv2, SigLIP2, EVA-CLIP）在CSS测试中表现出色，并揭示高CSS模型依靠长距离交互和从局部到全局的编码转变。

Conclusion: 为实现更鲁棒且类人化的视觉系统，应构建能同时整合局部纹理和全局形状配置的架构和学习框架，而非单一依赖形状或纹理表征。

Abstract: Humans are able to recognize objects based on both local texture cues and the
configuration of object parts, yet contemporary vision models primarily harvest
local texture cues, yielding brittle, non-compositional features. Work on
shape-vs-texture bias has pitted shape and texture representations in
opposition, measuring shape relative to texture, ignoring the possibility that
models (and humans) can simultaneously rely on both types of cues, and
obscuring the absolute quality of both types of representation. We therefore
recast shape evaluation as a matter of absolute configural competence,
operationalized by the Configural Shape Score (CSS), which (i) measures the
ability to recognize both images in Object-Anagram pairs that preserve local
texture while permuting global part arrangement to depict different object
categories. Across 86 convolutional, transformer, and hybrid models, CSS (ii)
uncovers a broad spectrum of configural sensitivity with fully self-supervised
and language-aligned transformers -- exemplified by DINOv2, SigLIP2 and
EVA-CLIP -- occupying the top end of the CSS spectrum. Mechanistic probes
reveal that (iii) high-CSS networks depend on long-range interactions:
radius-controlled attention masks abolish performance showing a distinctive
U-shaped integration profile, and representational-similarity analyses expose a
mid-depth transition from local to global coding. A BagNet control remains at
chance (iv), ruling out "border-hacking" strategies. Finally, (v) we show that
configural shape score also predicts other shape-dependent evals. Overall, we
propose that the path toward truly robust, generalizable, and human-like vision
systems may not lie in forcing an artificial choice between shape and texture,
but rather in architectural and learning frameworks that seamlessly integrate
both local-texture and global configural shape.

</details>


### [47] [Laplace-Mamba: Laplace Frequency Prior-Guided Mamba-CNN Fusion Network for Image Dehazing](https://arxiv.org/abs/2507.00501)
*Yongzhen Wang,Liangliang Chen,Bingwen Hu,Heng Liu,Xiao-Ping Zhang,Mingqiang Wei*

Main category: cs.CV

TL;DR: 这篇论文介绍了Laplace-Mamba，一个结合拉普拉斯频率先验和混合网络架构的新方法，用于高效图像去雾。


<details>
  <summary>Details</summary>
Motivation: 近期图像修复中，空间状态模型（SSMs）因建模长距离依赖的能力而表现优异，但无法很好地处理局部结构和高维数据，导致细节恢复效果欠佳。

Method: 该研究引入拉普拉斯分解，将图像分解为低频和高频成分，分别用SSMs建模全局纹理和用CNN精细化局部结构。结合混合架构进行处理，提升了计算效率和恢复质量。

Result: 实验表明，Laplace-Mamba在多个基准上优于现有最先进方法，在质量和效率上表现出色。

Conclusion: 该方法通过结合频率分解和双重路径处理，实现了从高效到优质的图像复原，为处理复杂的雾化场景提供了有效解决方案。

Abstract: Recent progress in image restoration has underscored Spatial State Models
(SSMs) as powerful tools for modeling long-range dependencies, owing to their
appealing linear complexity and computational efficiency. However, SSM-based
approaches exhibit limitations in reconstructing localized structures and tend
to be less effective when handling high-dimensional data, frequently resulting
in suboptimal recovery of fine image features. To tackle these challenges, we
introduce Laplace-Mamba, a novel framework that integrates Laplace frequency
prior with a hybrid Mamba-CNN architecture for efficient image dehazing.
Leveraging the Laplace decomposition, the image is disentangled into
low-frequency components capturing global texture and high-frequency components
representing edges and fine details. This decomposition enables specialized
processing via dual parallel pathways: the low-frequency branch employs SSMs
for global context modeling, while the high-frequency branch utilizes CNNs to
refine local structural details, effectively addressing diverse haze scenarios.
Notably, the Laplace transformation facilitates information-preserving
downsampling of low-frequency components in accordance with the Nyquist theory,
thereby significantly improving computational efficiency. Extensive evaluations
across multiple benchmarks demonstrate that our method outperforms
state-of-the-art approaches in both restoration quality and efficiency. The
source code and pretrained models are available at
https://github.com/yz-wang/Laplace-Mamba.

</details>


### [48] [ExPaMoE: An Expandable Parallel Mixture of Experts for Continual Test-Time Adaptation](https://arxiv.org/abs/2507.00502)
*JianChao Zhao,Songlin Dong*

Main category: cs.CV

TL;DR: 本研究提出了一种名为ExPaMoE的新方法，旨在解决持续测试时间适应中的分布变化问题，通过动态扩展专家池、解耦通用和特定领域知识等手段，显著提升了模型在不同场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 当前CTTA方法在应对大规模或非平稳的领域转移时容易出现特征纠缠和灾难性遗忘问题，亟需解决这些局限性。

Method: 提出了ExPaMoE框架，该框架采用可扩展的专家混合架构，通过双分支专家设计解耦领域知识，并利用频域线索实时检测分布变化来动态扩展专家池。

Result: 实验表明，ExPaMoE在多个CTTA场景和基准测试中显著优于现有方法，包括CIFAR-10C、CIFAR-100C、ImageNet-C和语义分割任务（Cityscapes-to-ACDC）。此外，新引入的ImageNet++数据集验证了其在复杂领域演化下的长期适应能力。

Conclusion: ExPaMoE方法具备强大的适应性、可扩展性以及对遗忘的抵抗力，是应对持续测试时间适应问题的有效解决方案。

Abstract: Continual Test-Time Adaptation (CTTA) aims to enable models to adapt
on-the-fly to a stream of unlabeled data under evolving distribution shifts.
However, existing CTTA methods typically rely on shared model parameters across
all domains, making them vulnerable to feature entanglement and catastrophic
forgetting in the presence of large or non-stationary domain shifts. To address
this limitation, we propose \textbf{ExPaMoE}, a novel framework based on an
\emph{Expandable Parallel Mixture-of-Experts} architecture. ExPaMoE decouples
domain-general and domain-specific knowledge via a dual-branch expert design
with token-guided feature separation, and dynamically expands its expert pool
based on a \emph{Spectral-Aware Online Domain Discriminator} (SODD) that
detects distribution changes in real-time using frequency-domain cues.
Extensive experiments demonstrate the superiority of ExPaMoE across diverse
CTTA scenarios. We evaluate our method on standard benchmarks including
CIFAR-10C, CIFAR-100C, ImageNet-C, and Cityscapes-to-ACDC for semantic
segmentation. Additionally, we introduce \textbf{ImageNet++}, a large-scale and
realistic CTTA benchmark built from multiple ImageNet-derived datasets, to
better reflect long-term adaptation under complex domain evolution. ExPaMoE
consistently outperforms prior arts, showing strong robustness, scalability,
and resistance to forgetting.

</details>


### [49] [LLaVA-SP: Enhancing Visual Representation with Visual Spatial Tokens for MLLMs](https://arxiv.org/abs/2507.00505)
*Haoran Lou,Chunxiao Fan,Ziyan Liu,Yuexin Wu,Xinxiang Wang*

Main category: cs.CV

TL;DR: 本文提出了LLaVA-SP，通过增加六个空间视觉token提升视觉表示能力，并在多种多模态任务中性能优于LLaVA-1.5。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型中的视觉编码器（如CLIP-ViT）难以有效建模局部关系，影响对图像细节的理解。

Method: 引入六个空间视觉token，并设计投影器用卷积核从ViT特征派生视觉空间token，通过中心到全局和抽象到具体两种方式模拟视觉空间排序，并使用交叉注意力融合细粒度视觉信息。同时提出两种模型变体（LLaVA-SP-Cropping和LLaVA-SP-Pooling），分别针对细节特征和全局语义优化。

Result: 通过在多种多模态基准任务上的广泛实验，LLaVA-SP在几乎相同的推理时延下显著超越LLaVA-1.5。

Conclusion: LLaVA-SP通过增强局部关系建模和细节理解能力，为多模态语言模型在多任务中提供了更好的性能表现。

Abstract: The architecture of multimodal large language models (MLLMs) commonly
connects a vision encoder, often based on CLIP-ViT, to a large language model.
While CLIP-ViT works well for capturing global image features, it struggles to
model local relationships between adjacent patches, leading to weaker visual
representation, which in turn affects the detailed understanding ability of
MLLMs. To solve this, we propose LLaVA-SP, which \textbf{ only adds six spatial
visual tokens} to the original visual tokens to enhance the visual
representation. Our approach offers three key advantages: 1)We propose a novel
Projector, which uses convolutional kernels to derive visual spatial tokens
from ViT patch features, simulating two visual spatial ordering approaches:
``from central region to global" and ``from abstract to specific". Then, a
cross-attention mechanism is applied to fuse fine-grained visual information,
enriching the overall visual representation. 2) We present two model variants:
LLaVA-SP-Cropping, which focuses on detail features through progressive
cropping, and LLaVA-SP-Pooling, which captures global semantics through
adaptive pooling, enabling the model to handle diverse visual understanding
tasks. 3) Extensive experiments show that LLaVA-SP, fine-tuned with LoRA,
achieves significant performance improvements across various multimodal
benchmarks, outperforming the state-of-the-art LLaVA-1.5 model in multiple
tasks with nearly identical inference latency. The code and models are
available at
\href{https://github.com/CnFaker/LLaVA-SP}{\texttt{https://github.com/CnFaker/LLaVA-SP}}.

</details>


### [50] [SCING:Towards More Efficient and Robust Person Re-Identification through Selective Cross-modal Prompt Tuning](https://arxiv.org/abs/2507.00506)
*Yunfei Xie,Yuxuan Cheng,Juncheng Wu,Haoyu Zhang,Yuyin Zhou,Shoudong Han*

Main category: cs.CV

TL;DR: 提出了Selective Cross-modal Prompt Tuning (SCING)框架，通过跨模态门控机制和随机扰动一致性对齐提高了视觉-语言模型在ReID任务中的跨模态对齐性能，并在多个基准数据集上证明其高效性与优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型用于ReID任务时存在高计算成本和跨模态对齐不足的问题，引出设计高效且鲁棒的简单框架的需求。

Method: 提出SCING框架，包括Selective Visual Prompt Fusion (SVIP)模块和Perturbation-Driven Consistency Alignment (PDCA)双路径训练策略，分别通过门控机制与一致性正则实现跨模态交互和抗扰动对齐。

Result: 在Market1501、DukeMTMC-ReID等多数据集上表现出色，无需重型适配器且推理高效，达到了性能与计算开销的最佳平衡。

Conclusion: SCING框架在保持高效推理的同时提升了ReID任务中的跨模态对齐与鲁棒性，验证了其实用性和优越性能。

Abstract: Recent advancements in adapting vision-language pre-training models like CLIP
for person re-identification (ReID) tasks often rely on complex adapter design
or modality-specific tuning while neglecting cross-modal interaction, leading
to high computational costs or suboptimal alignment. To address these
limitations, we propose a simple yet effective framework named Selective
Cross-modal Prompt Tuning (SCING) that enhances cross-modal alignment and
robustness against real-world perturbations. Our method introduces two key
innovations: Firstly, we proposed Selective Visual Prompt Fusion (SVIP), a
lightweight module that dynamically injects discriminative visual features into
text prompts via a cross-modal gating mechanism. Moreover, the proposed
Perturbation-Driven Consistency Alignment (PDCA) is a dual-path training
strategy that enforces invariant feature alignment under random image
perturbations by regularizing consistency between original and augmented
cross-modal embeddings. Extensive experiments are conducted on several popular
benchmarks covering Market1501, DukeMTMC-ReID, Occluded-Duke, Occluded-REID,
and P-DukeMTMC, which demonstrate the impressive performance of the proposed
method. Notably, our framework eliminates heavy adapters while maintaining
efficient inference, achieving an optimal trade-off between performance and
computational overhead. The code will be released upon acceptance.

</details>


### [51] [Topology-Constrained Learning for Efficient Laparoscopic Liver Landmark Detection](https://arxiv.org/abs/2507.00519)
*Ruize Cui,Jiaan Zhang,Jialun Pei,Kai Wang,Pheng-Ann Heng,Jing Qin*

Main category: cs.CV

TL;DR: 提出了TopoNet框架，通过拓扑约束深度学习提高腹腔镜肝脏地标检测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 减少腹腔镜肝脏手术中因地标检测误差而导致的手术风险，提高自动化检测的稳定性和准确性。

Method: 使用蛇CNN双路径编码器捕获RGB和深度拓扑信息，设计了边界感知拓扑融合模块（BTF）增强边缘感知，并引入拓扑约束损失函数，包括中心线约束和拓扑持久性损失，确保预测与标签的拓扑等价性。

Result: TopoNet在L3D和P2ILF数据集上表现出色，兼具高精度和低计算复杂性。

Conclusion: TopoNet展现了其在腹腔镜肝脏手术领域的临床应用潜力，提供精确且高效的肝脏地标检测方法。

Abstract: Liver landmarks provide crucial anatomical guidance to the surgeon during
laparoscopic liver surgery to minimize surgical risk. However, the tubular
structural properties of landmarks and dynamic intraoperative deformations pose
significant challenges for automatic landmark detection. In this study, we
introduce TopoNet, a novel topology-constrained learning framework for
laparoscopic liver landmark detection. Our framework adopts a snake-CNN
dual-path encoder to simultaneously capture detailed RGB texture information
and depth-informed topological structures. Meanwhile, we propose a
boundary-aware topology fusion (BTF) module, which adaptively merges RGB-D
features to enhance edge perception while preserving global topology.
Additionally, a topological constraint loss function is embedded, which
contains a center-line constraint loss and a topological persistence loss to
ensure homotopy equivalence between predictions and labels. Extensive
experiments on L3D and P2ILF datasets demonstrate that TopoNet achieves
outstanding accuracy and computational complexity, highlighting the potential
for clinical applications in laparoscopic liver surgery. Our code will be
available at https://github.com/cuiruize/TopoNet.

</details>


### [52] [Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving](https://arxiv.org/abs/2507.00525)
*Djamahl Etchegaray,Yuxia Fu,Zi Huang,Yadan Luo*

Main category: cs.CV

TL;DR: 提出Box-QAymo，一个针对自动驾驶中用户指定目标的视觉问答数据集评估基准。通过创新评估协议分析目标对象的空间和时间推理能力，同时揭示现有视觉语言模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型难以真实场景下捕捉用户意图，现有数据集未能评估模型对局部化用户查询的响应能力，亟需一个针对性强的评估基准。

Method: 设计并引入Box-QAymo数据集，支持用户通过绘制边界框指定查询目标，采用分层评估协议，逐步测试模型的基本能力、属性预测、目标动作和时空推理；数据构建中引入负采样、时间一致性及难度平衡以确保数据集质量。

Result: 综合评估显示现有视觉语言模型在感知性问答能力上存在显著不足，无法有效处理用户指定目标查询，凸显模型与真实场景性能的差距。

Conclusion: Box-QAymo为研究开发鲁棒且可解释的自动驾驶系统提供了坚实基础，通过改善人机交互提高系统在真实世界中的表现。

Abstract: Interpretable communication is essential for safe and trustworthy autonomous
driving, yet current vision-language models (VLMs) often operate under
idealized assumptions and struggle to capture user intent in real-world
scenarios. Existing driving-oriented VQA datasets are limited to full-scene
descriptions or waypoint prediction, preventing the assessment of whether VLMs
can respond to localized user-driven queries. We introduce Box-QAymo, a
box-referring dataset and benchmark designed to both evaluate and finetune VLMs
on spatial and temporal reasoning over user-specified objects. Users express
intent by drawing bounding boxes, offering a fast and intuitive interface for
focused queries in complex scenes. Specifically, we propose a hierarchical
evaluation protocol that begins with binary sanity-check questions to assess
basic model capacities, and progresses to (1) attribute prediction for
box-referred objects, (2) motion understanding of target instances, and (3)
spatiotemporal motion reasoning over inter-object dynamics across frames. To
support this, we crowd-sourced fine-grained object classes and visual
attributes that reflect the complexity drivers encounter, and extract object
trajectories to construct temporally grounded QA pairs. Rigorous quality
control through negative sampling, temporal consistency checks, and
difficulty-aware balancing guarantee dataset robustness and diversity. Our
comprehensive evaluation reveals significant limitations in current VLMs when
queried about perception questions, highlighting the gap in achieving
real-world performance. This work provides a foundation for developing more
robust and interpretable autonomous driving systems that can communicate
effectively with users under real-world conditions. Project page and dataset
are available at https://djamahl99.github.io/qaymo-pages/.

</details>


### [53] [Not All Attention Heads Are What You Need: Refining CLIP's Image Representation with Attention Ablation](https://arxiv.org/abs/2507.00537)
*Feng Lin,Marco Chen,Haokui Zhang,Xiaotian Yu,Guangming Lu,Rong Xiao*

Main category: cs.CV

TL;DR: 本文研究CLIP的图像编码器中的注意力头角色，并提出“注意力切除技术（AAT）”来改进表示和提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 虽然CLIP在多种应用中表现出鲁棒性，但某些注意力头可能对最终表示产生负面影响。研究通过探讨这些注意力头的角色，提出可以通过切除这些头来改善性能的可能性。

Method: 提出了“注意力切除技术（AAT）”，通过调整注意力权重抑制特定注意力头的贡献；并整合了两个用于不同场景的策略，在系统上识别和切除负面影响的注意力头。

Result: 实验表明，AAT在跨多个领域的下游任务中一致提升了性能，在跨模态检索任务中，使召回率最高提升了11.1%。

Conclusion: AAT展示了对大规模视觉-语言模型进行高效优化的潜力，同时基本不增加推理开销。

Abstract: This paper studies the role of attention heads in CLIP's image encoder. While
CLIP has exhibited robust performance across diverse applications, we
hypothesize that certain attention heads negatively affect final
representations and that ablating them can improve performance in downstream
tasks. To capitalize on this insight, we propose a simple yet effective method,
called Attention Ablation Technique (AAT), to suppress the contribution of
specific heads by manipulating attention weights. By integrating two
alternative strategies tailored for different application scenarios, AAT
systematically identifies and ablates detrimental attention heads to enhance
representation quality. Experiments demonstrate that AAT consistently improves
downstream task performance across various domains, boosting recall rate by up
to 11.1% on CLIP-family models for cross-modal retrieval. The results highlight
the potential of AAT to effectively refine large-scale vision-language models
with virtually no increase in inference cost.

</details>


### [54] [LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail Conserved Anti-Aliasing](https://arxiv.org/abs/2507.00554)
*Zhenya Yang,Bingchen Gong,Kai Chen,Qi Dou*

Main category: cs.CV

TL;DR: 提出LOD-GS框架，通过动态预测过滤强度提高3D场景渲染质量，并开源了代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的抗锯齿方法用低通滤波，缺乏对采样率敏感，效果有限。

Method: 引入以采样率为输入的基函数，动态调整3D高斯原子的抗锯齿过滤强度，与高斯参数联合优化。

Result: 在公开数据与新建数据集上获取最优渲染质量，有效消除伪影。

Conclusion: LOD-GS框架提高了渲染质量，填补了现有方法中对相机距离影响的忽视。

Abstract: Despite the advancements in quality and efficiency achieved by 3D Gaussian
Splatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent
challenge. Existing approaches primarily rely on low-pass filtering to mitigate
aliasing. However, these methods are not sensitive to the sampling rate, often
resulting in under-filtering and over-smoothing renderings. To address this
limitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework
for Gaussian Splatting, which dynamically predicts the optimal filtering
strength for each 3D Gaussian primitive. Specifically, we introduce a set of
basis functions to each Gaussian, which take the sampling rate as input to
model appearance variations, enabling sampling-rate-sensitive filtering. These
basis function parameters are jointly optimized with the 3D Gaussian in an
end-to-end manner. The sampling rate is influenced by both focal length and
camera distance. However, existing methods and datasets rely solely on
down-sampling to simulate focal length changes for anti-aliasing evaluation,
overlooking the impact of camera distance. To enable a more comprehensive
assessment, we introduce a new synthetic dataset featuring objects rendered at
varying camera distances. Extensive experiments on both public datasets and our
newly collected dataset demonstrate that our method achieves SOTA rendering
quality while effectively eliminating aliasing. The code and dataset have been
open-sourced.

</details>


### [55] [Zero-shot Skeleton-based Action Recognition with Prototype-guided Feature Alignment](https://arxiv.org/abs/2507.00566)
*Kai Zhou,Shuhai Zhang,Zeng You,Jinwu Hu,Mingkui Tan,Fei Liu*

Main category: cs.CV

TL;DR: 本文旨在解决无需见过的骨架动作类别进行分类的难题，通过提出端到端的跨模态对比训练框架和基于原型引导的文本特征对齐策略，显著提升了骨架-文本对齐效果和分类准确率。


<details>
  <summary>Details</summary>
Motivation: 以往关于零样本骨架动作识别的研究在特征的区分性和骨架与文本特征对齐的偏差上存在不足，这些局限性削弱了模型的性能表现。本文旨在解决这些问题。

Method: 本文提出了原型引导的特征对齐范式（PGFA），整合了端到端跨模态对比训练框架以加强骨架-文本对齐，以及优化了分布差异性的基于原型引导的文本特征对齐策略。

Result: 在NTU-60、NTU-120和PKU-MMD数据集上分别达到了22.96%、12.53%和18.54%的绝对准确率提升。

Conclusion: 原型引导的零样本骨架动作识别方法显著提升了对未见类别动作的识别能力，并验证了所提出方法的理论和实验的有效性。

Abstract: Zero-shot skeleton-based action recognition aims to classify unseen
skeleton-based human actions without prior exposure to such categories during
training. This task is extremely challenging due to the difficulty in
generalizing from known to unknown actions. Previous studies typically use
two-stage training: pre-training skeleton encoders on seen action categories
using cross-entropy loss and then aligning pre-extracted skeleton and text
features, enabling knowledge transfer to unseen classes through skeleton-text
alignment and language models' generalization. However, their efficacy is
hindered by 1) insufficient discrimination for skeleton features, as the fixed
skeleton encoder fails to capture necessary alignment information for effective
skeleton-text alignment; 2) the neglect of alignment bias between skeleton and
unseen text features during testing. To this end, we propose a prototype-guided
feature alignment paradigm for zero-shot skeleton-based action recognition,
termed PGFA. Specifically, we develop an end-to-end cross-modal contrastive
training framework to improve skeleton-text alignment, ensuring sufficient
discrimination for skeleton features. Additionally, we introduce a
prototype-guided text feature alignment strategy to mitigate the adverse impact
of the distribution discrepancy during testing. We provide a theoretical
analysis to support our prototype-guided text feature alignment strategy and
empirically evaluate our overall PGFA on three well-known datasets. Compared
with the top competitor SMIE method, our PGFA achieves absolute accuracy
improvements of 22.96%, 12.53%, and 18.54% on the NTU-60, NTU-120, and PKU-MMD
datasets, respectively.

</details>


### [56] [Out-of-distribution detection in 3D applications: a review](https://arxiv.org/abs/2507.00570)
*Zizhao Li,Xueyang Kang,Joseph West,Kourosh Khoshelham*

Main category: cs.CV

TL;DR: 本文探讨在3D应用场景中检测未见样本的能力，提供了OOD检测领域的全面回顾与方法分析。


<details>
  <summary>Details</summary>
Motivation: 解决现有机器学习方法对训练外对象分类不佳的问题，提升AI系统的可靠性与泛化能力。

Method: 研究对象跨越多领域与模态，引入基准数据集和评价指标，并分析基于不确定性与分布距离的OOD检测方法。

Result: 总结了现有方法和研究方向，提出了包括3D视觉集成等新兴研究机会。

Conclusion: 为新研究者提供理论与实践指南，推动开发可靠、安全且健壮的AI系统。

Abstract: The ability to detect objects that are not prevalent in the training set is a
critical capability in many 3D applications, including autonomous driving.
Machine learning methods for object recognition often assume that all object
categories encountered during inference belong to a closed set of classes
present in the training data. This assumption limits generalization to the real
world, as objects not seen during training may be misclassified or entirely
ignored. As part of reliable AI, OOD detection identifies inputs that deviate
significantly from the training distribution. This paper provides a
comprehensive overview of OOD detection within the broader scope of trustworthy
and uncertain AI. We begin with key use cases across diverse domains, introduce
benchmark datasets spanning multiple modalities, and discuss evaluation
metrics. Next, we present a comparative analysis of OOD detection
methodologies, exploring model structures, uncertainty indicators, and
distributional distance taxonomies, alongside uncertainty calibration
techniques. Finally, we highlight promising research directions, including
adversarially robust OOD detection and failure identification, particularly
relevant to 3D applications. The paper offers both theoretical and practical
insights into OOD detection, showcasing emerging research opportunities such as
3D vision integration. These insights help new researchers navigate the field
more effectively, contributing to the development of reliable, safe, and robust
AI systems.

</details>


### [57] [AI-Generated Video Detection via Perceptual Straightening](https://arxiv.org/abs/2507.00583)
*Christian Internò,Robert Geirhos,Markus Olhofer,Sunny Liu,Barbara Hammer,David Klindt*

Main category: cs.CV

TL;DR: 提出了ReStraV，通过分析神经表示域中的几何特性，检测是否为AI生成视频，表现优异。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展导致高度逼真的合成视频出现，现有检测技术在泛化和捕捉时间不一致性方面存在不足。

Method: 采用DINOv2神经模型，通过量化时间曲率和逐步距离评估视频中的几何偏差，并使用这些统计数据训练分类器。

Result: 在VidProM基准测试中取得97.17%的准确率和98.63%的AUROC表现，显著优于现有方法。

Conclusion: ReStraV经济高效，性能优越，为利用神经表示几何检测AI生成视频提供了新视角。

Abstract: The rapid advancement of generative AI enables highly realistic synthetic
videos, posing significant challenges for content authentication and raising
urgent concerns about misuse. Existing detection methods often struggle with
generalization and capturing subtle temporal inconsistencies. We propose
ReStraV(Representation Straightening Video), a novel approach to distinguish
natural from AI-generated videos. Inspired by the "perceptual straightening"
hypothesis -- which suggests real-world video trajectories become more straight
in neural representation domain -- we analyze deviations from this expected
geometric property. Using a pre-trained self-supervised vision transformer
(DINOv2), we quantify the temporal curvature and stepwise distance in the
model's representation domain. We aggregate statistics of these measures for
each video and train a classifier. Our analysis shows that AI-generated videos
exhibit significantly different curvature and distance patterns compared to
real videos. A lightweight classifier achieves state-of-the-art detection
performance (e.g., 97.17% accuracy and 98.63% AUROC on the VidProM benchmark),
substantially outperforming existing image- and video-based methods. ReStraV is
computationally efficient, it is offering a low-cost and effective detection
solution. This work provides new insights into using neural representation
geometry for AI-generated video detection.

</details>


### [58] [Similarity Memory Prior is All You Need for Medical Image Segmentation](https://arxiv.org/abs/2507.00585)
*Tang Hao,Guo ZhiQing,Wang LieJun,Liu Chao*

Main category: cs.CV

TL;DR: 该研究受灵长类视觉皮层细胞对复杂形状识别的启发，提出了一种医疗影像分割的新网络方法Sim-MPNet，并在多个数据集上表现优于其他先进算法。


<details>
  <summary>Details</summary>
Motivation: 受到灵长类视觉皮层细胞识别复杂形状能力的启发，旨在改进医疗图像分割算法的性能。

Method: 设计了一种名为Similarity Memory Prior Network (Sim-MPNet)的网络，提出了动态记忆权重-注意力(DMW-LA)机制，通过原型记忆库的相似性记忆先验进行类别特征匹配和存储，动态更新相似性记忆，辅助网络提取类别特征；并提出双相似性全局内部增强模块(DS-GIM)以进行深层特征分析。

Result: 在四个公开数据集上进行了大量实验，结果表明Sim-MPNet在分割性能上优于其他先进方法。

Conclusion: Sim-MPNet通过结合新颖的相似性记忆机制和全局特征增强模块，有效提升了医疗影像分割的准确性，为领域研究提供了新的方法与方向。

Abstract: In recent years, it has been found that "grandmother cells" in the primary
visual cortex (V1) of macaques can directly recognize visual input with complex
shapes. This inspires us to examine the value of these cells in promoting the
research of medical image segmentation. In this paper, we design a Similarity
Memory Prior Network (Sim-MPNet) for medical image segmentation. Specifically,
we propose a Dynamic Memory Weights-Loss Attention (DMW-LA), which matches and
remembers the category features of specific lesions or organs in medical images
through the similarity memory prior in the prototype memory bank, thus helping
the network to learn subtle texture changes between categories. DMW-LA also
dynamically updates the similarity memory prior in reverse through Weight-Loss
Dynamic (W-LD) update strategy, effectively assisting the network directly
extract category features. In addition, we propose the Double-Similarity Global
Internal Enhancement Module (DS-GIM) to deeply explore the internal differences
in the feature distribution of input data through cosine similarity and
euclidean distance. Extensive experiments on four public datasets show that
Sim-MPNet has better segmentation performance than other state-of-the-art
methods. Our code is available on https://github.com/vpsg-research/Sim-MPNet.

</details>


### [59] [Context-Aware Academic Emotion Dataset and Benchmark](https://arxiv.org/abs/2507.00586)
*Luming Zhao,Jingwen Xuan,Jiamin Lou,Yonghui Yu,Wenwu Yang*

Main category: cs.CV

TL;DR: 论文提出利用面部表情识别学生在学习环境中的学术情感，推出首个多场景学术情感数据集RAER，并提出CLIP-CAER方法结合上下文提示，显著提升情感识别准确率。


<details>
  <summary>Details</summary>
Motivation: 在真实学习场景中，识别学生的学术情感有助于评估学习状态和参与度。然而，与基础情感识别相比，学术情感识别研究较少，主要受限于公开数据集的匮乏。

Method: 提出RAER数据集，包含约2700段视频，覆盖多种自然学习场景并使用多标注方案。设计CLIP-CAER方法，通过CLIP模型结合可学习文本提示，将面部表情和上下文信息进行全面整合。

Result: 实验表明，CLIP-CAER方法在学术情感识别中的表现显著优于现有面部表情识别方法，准确捕捉了学生情感的上下文信息。

Conclusion: RAER是首个结合多样自然学习场景的学术情感数据集，CLIP-CAER方法证明了上下文提示对学术情感识别的重要性，为改善学习行为分析方法提供了新思路。

Abstract: Academic emotion analysis plays a crucial role in evaluating students'
engagement and cognitive states during the learning process. This paper
addresses the challenge of automatically recognizing academic emotions through
facial expressions in real-world learning environments. While significant
progress has been made in facial expression recognition for basic emotions,
academic emotion recognition remains underexplored, largely due to the scarcity
of publicly available datasets. To bridge this gap, we introduce RAER, a novel
dataset comprising approximately 2,700 video clips collected from around 140
students in diverse, natural learning contexts such as classrooms, libraries,
laboratories, and dormitories, covering both classroom sessions and individual
study. Each clip was annotated independently by approximately ten annotators
using two distinct sets of academic emotion labels with varying granularity,
enhancing annotation consistency and reliability. To our knowledge, RAER is the
first dataset capturing diverse natural learning scenarios. Observing that
annotators naturally consider context cues-such as whether a student is looking
at a phone or reading a book-alongside facial expressions, we propose CLIP-CAER
(CLIP-based Context-aware Academic Emotion Recognition). Our method utilizes
learnable text prompts within the vision-language model CLIP to effectively
integrate facial expression and context cues from videos. Experimental results
demonstrate that CLIP-CAER substantially outperforms state-of-the-art
video-based facial expression recognition methods, which are primarily designed
for basic emotions, emphasizing the crucial role of context in accurately
recognizing academic emotions. Project page: https://zgsfer.github.io/CAER

</details>


### [60] [Overtake Detection in Trucks Using CAN Bus Signals: A Comparative Study of Machine Learning Methods](https://arxiv.org/abs/2507.00593)
*Fernando Alonso-Fernandez,Talha Hanif Butt,Prayag Tiwari*

Main category: cs.CV

TL;DR: 研究通过分析卡车CAN总线数据进行超车检测，测试了ANN、RF和SVM分类器，并通过多车数据训练和得分融合策略提高了分类性能，最终实现TNR为93%和TPR为86.5%的准确率。


<details>
  <summary>Details</summary>
Motivation: 超车操作在卡车中对于避免事故和提高交通效率至关重要，因此研究准确预测超车的算法以辅助ADAS的决策。

Method: 通过分析Volvo Group提供的五辆卡车的CAN总线数据，测试ANN、RF和SVM三种分类器，并探讨不同预处理配置对分类性能的影响，同时提出了多车数据训练和得分级融合策略。

Result: 多车辆混合数据训练改善了泛化能力且减少了条件特定的偏差，得分融合策略在大多数情况下实现了更高的分类准确度（TNR=93%，TPR=86.5%）。

Conclusion: 研究验证了多车训练数据和得分融合策略提高了超车检测的性能，为未来基于人工智能的驾驶员行为预测提供了支持。

Abstract: Safe overtaking manoeuvres in trucks are vital for preventing accidents and
ensuring efficient traffic flow. Accurate prediction of such manoeuvres is
essential for Advanced Driver Assistance Systems (ADAS) to make timely and
informed decisions. In this study, we focus on overtake detection using
Controller Area Network (CAN) bus data collected from five in-service trucks
provided by the Volvo Group. We evaluate three common classifiers for vehicle
manoeuvre detection, Artificial Neural Networks (ANN), Random Forest (RF), and
Support Vector Machines (SVM), and analyse how different preprocessing
configurations affect performance. We find that variability in traffic
conditions strongly influences the signal patterns, particularly in the
no-overtake class, affecting classification performance if training data lacks
adequate diversity. Since the data were collected under unconstrained,
real-world conditions, class diversity cannot be guaranteed a priori. However,
training with data from multiple vehicles improves generalisation and reduces
condition-specific bias. Our pertruck analysis also reveals that classification
accuracy, especially for overtakes, depends on the amount of training data per
vehicle. To address this, we apply a score-level fusion strategy, which yields
the best per-truck performance across most cases. Overall, we achieve an
accuracy via fusion of TNR=93% (True Negative Rate) and TPR=86.5% (True
Positive Rate). This research has been part of the BIG FUN project, which
explores how Artificial Intelligence can be applied to logged vehicle data to
understand and predict driver behaviour, particularly in relation to Camera
Monitor Systems (CMS), being introduced as digital replacements for traditional
exterior mirrors.

</details>


### [61] [World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model](https://arxiv.org/abs/2507.00603)
*Yupeng Zheng,Pengxuan Yang,Zebin Xing,Qichao Zhang,Yuhang Zheng,Yinfeng Gao,Pengfei Li,Teng Zhang,Zhongpu Xia,Peng Jia,Dongbin Zhao*

Main category: cs.CV

TL;DR: 提出了一种名为World4Drive的端到端自动驾驶框架，可基于视觉基础模型构建潜在的驾驶世界模型，从而无需感知标注即可实现规划。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶依赖于高昂的感知监督，而构建一个无需感知标注的、具有信息量的驾驶世界模型是一项关键挑战。

Method: 通过视觉基础模型提取场景特征和潜在表示，用于多模态规划轨迹的生成与评估。并引入一个世界模型选择器模块，评估和选择最佳轨迹。以自监督学习的方式对潜在空间的实际观测和预测对齐。

Result: World4Drive在无感知标注的情况下，达到了开放环nuScenes和闭环NavSim基准的最先进性能，分别降低了18.1%的L2误差，46.7%的碰撞率，并加速了3.75倍的训练收敛速度。

Conclusion: World4Drive无需手动标注，证明了通过高效构建潜在世界模型能实现端到端自动驾驶的可行性与优越性能。

Abstract: End-to-end autonomous driving directly generates planning trajectories from
raw sensor data, yet it typically relies on costly perception supervision to
extract scene information. A critical research challenge arises: constructing
an informative driving world model to enable perception annotation-free,
end-to-end planning via self-supervised learning. In this paper, we present
World4Drive, an end-to-end autonomous driving framework that employs vision
foundation models to build latent world models for generating and evaluating
multi-modal planning trajectories. Specifically, World4Drive first extracts
scene features, including driving intention and world latent representations
enriched with spatial-semantic priors provided by vision foundation models. It
then generates multi-modal planning trajectories based on current scene
features and driving intentions and predicts multiple intention-driven future
states within the latent space. Finally, it introduces a world model selector
module to evaluate and select the best trajectory. We achieve perception
annotation-free, end-to-end planning through self-supervised alignment between
actual future observations and predicted observations reconstructed from the
latent space. World4Drive achieves state-of-the-art performance without manual
perception annotations on both the open-loop nuScenes and closed-loop NavSim
benchmarks, demonstrating an 18.1\% relative reduction in L2 error, 46.7% lower
collision rate, and 3.75 faster training convergence. Codes will be accessed at
https://github.com/ucaszyp/World4Drive.

</details>


### [62] [ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2507.00898)
*Zifu Wan,Ce Zhang,Silong Yong,Martin Q. Ma,Simon Stepputtis,Louis-Philippe Morency,Deva Ramanan,Katia Sycara,Yaqi Xie*

Main category: cs.CV

TL;DR: 本文提出了一种名为ONLY的训练无关解码方法，旨在降低大型视觉语言模型（LVLMs）生成过程中幻觉问题的影响。


<details>
  <summary>Details</summary>
Motivation: 通过一种轻量级且高效的单次解码干预方式，解决现有方法需要多次查询导致的实时性问题。

Method: 采用基于文本到视觉熵比的方式，在解码阶段选择性放大关键文本信息，从而改善输出文本质量。无需额外训练，只需一层干预即可实现。

Result: 实验结果表明，ONLY在多个基准测试中均优于现有方法，同时实现了低实现成本和计算成本。

Conclusion: ONLY方法不仅提升了解码效率，还显著降低了LVLMs的幻觉问题，适合于实时应用的推广。

Abstract: Recent Large Vision-Language Models (LVLMs) have introduced a new paradigm
for understanding and reasoning about image input through textual responses.
Although they have achieved remarkable performance across a range of
multi-modal tasks, they face the persistent challenge of hallucination, which
introduces practical weaknesses and raises concerns about their reliable
deployment in real-world applications. Existing work has explored contrastive
decoding approaches to mitigate this issue, where the output of the original
LVLM is compared and contrasted with that of a perturbed version. However,
these methods require two or more queries that slow down LVLM response
generation, making them less suitable for real-time applications. To overcome
this limitation, we propose ONLY, a training-free decoding approach that
requires only a single query and a one-layer intervention during decoding,
enabling efficient real-time deployment. Specifically, we enhance textual
outputs by selectively amplifying crucial textual information using a
text-to-visual entropy ratio for each token. Extensive experimental results
demonstrate that our proposed ONLY consistently outperforms state-of-the-art
methods across various benchmarks while requiring minimal implementation effort
and computational cost. Code is available at https://github.com/zifuwan/ONLY.

</details>


### [63] [De-Simplifying Pseudo Labels to Enhancing Domain Adaptive Object Detection](https://arxiv.org/abs/2507.00608)
*Zehua Fu,Chenguang Liu,Yuyu Chen,Jiaqi Zhou,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: 提出了一种名为De-Simplifying Pseudo Labels (DeSimPL)的新方法，用于解决自标注检测器中低性能的关键原因——简单标签偏差问题，实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前交通和运输场景中的目标检测受限于高质量标注数据的获取困难，因此无监督领域适应（UDA）正成为研究热点，其中自标注方法以其简单性和效率引起关注，但性能有瓶颈。

Method: 提出DeSimPL方法，通过使用实例级记忆库更新伪标签策略、引入对抗样本训练以及自适应加权损失减少简单标签比例，同时避免后期训练中的伪标签误差问题。

Result: 实验表明，DeSimPL有效减少训练中的简单样本比例，大幅提高了自标注检测器的性能，并在四个基准数据集上验证了分析和结论的可靠性。

Conclusion: DeSimPL方法有效改进了传统自标注方法的不足，为无监督领域适应中的目标检测提供了一条新思路，显著提升了性能。

Abstract: Despite its significant success, object detection in traffic and
transportation scenarios requires time-consuming and laborious efforts in
acquiring high-quality labeled data. Therefore, Unsupervised Domain Adaptation
(UDA) for object detection has recently gained increasing research attention.
UDA for object detection has been dominated by domain alignment methods, which
achieve top performance. Recently, self-labeling methods have gained popularity
due to their simplicity and efficiency. In this paper, we investigate the
limitations that prevent self-labeling detectors from achieving commensurate
performance with domain alignment methods. Specifically, we identify the high
proportion of simple samples during training, i.e., the simple-label bias, as
the central cause. We propose a novel approach called De-Simplifying Pseudo
Labels (DeSimPL) to mitigate the issue. DeSimPL utilizes an instance-level
memory bank to implement an innovative pseudo label updating strategy. Then,
adversarial samples are introduced during training to enhance the proportion.
Furthermore, we propose an adaptive weighted loss to avoid the model suffering
from an abundance of false positive pseudo labels in the late training period.
Experimental results demonstrate that DeSimPL effectively reduces the
proportion of simple samples during training, leading to a significant
performance improvement for self-labeling detectors. Extensive experiments
conducted on four benchmarks validate our analysis and conclusions.

</details>


### [64] [UMDATrack: Unified Multi-Domain Adaptive Tracking Under Adverse Weather Conditions](https://arxiv.org/abs/2507.00648)
*Siyuan Yao,Rui Zhu,Ziqi Wang,Wenqi Ren,Yanyang Yan,Xiaochun Cao*

Main category: cs.CV

TL;DR: UMDATrack是一种专为恶劣天气条件设计的统一域自适应视觉跟踪框架，可显著提升目标预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决视觉目标跟踪在恶劣天气条件下受域迁移影响性能大幅下降的问题。

Method: 引入一个可控场景生成器生成多种天气条件下的小规模无标注视频数据，并设计域定制适配器（DCA）和目标感知置信度对齐模块（TCA）以提升模型适应性和域一致性。

Result: UMDATrack在多个极端天气条件下的表现超过现有方法，创造了新的性能基准，验证了方法的有效性。

Conclusion: UMDATrack实现了准确性和适应性的兼顾，在视觉目标跟踪领域树立了新标准。

Abstract: Visual object tracking has gained promising progress in past decades. Most of
the existing approaches focus on learning target representation in
well-conditioned daytime data, while for the unconstrained real-world scenarios
with adverse weather conditions, e.g. nighttime or foggy environment, the
tremendous domain shift leads to significant performance degradation. In this
paper, we propose UMDATrack, which is capable of maintaining high-quality
target state prediction under various adverse weather conditions within a
unified domain adaptation framework. Specifically, we first use a controllable
scenario generator to synthesize a small amount of unlabeled videos (less than
2% frames in source daytime datasets) in multiple weather conditions under the
guidance of different text prompts. Afterwards, we design a simple yet
effective domain-customized adapter (DCA), allowing the target objects'
representation to rapidly adapt to various weather conditions without redundant
model updating. Furthermore, to enhance the localization consistency between
source and target domains, we propose a target-aware confidence alignment
module (TCA) following optimal transport theorem. Extensive experiments
demonstrate that UMDATrack can surpass existing advanced visual trackers and
lead new state-of-the-art performance by a significant margin. Our code is
available at https://github.com/Z-Z188/UMDATrack.

</details>


### [65] [LoD-Loc v2: Aerial Visual Localization over Low Level-of-Detail City Models using Explicit Silhouette Alignment](https://arxiv.org/abs/2507.00659)
*Juelin Zhu,Shuaibang Peng,Long Wang,Hanlin Tan,Yu Liu,Maojun Zhang,Shen Yan*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法LoD-Loc v2，用于低细节级别（LoD1）的城市模型中的空中视觉定位，通过改进的轮廓对齐策略，实现比高细节级别模型更好的定位能力。


<details>
  <summary>Details</summary>
Motivation: 多数现有方法依赖高细节级别的城市模型，但全球大多数模型（及计划构建的）是低细节模型（LoD1），因此需要实现低细节模型的定位功能，以释放无人机在全球城市定位中的潜力。

Method: 提出了一种从粗到精的策略，即在粗阶段通过构建姿态代价体积选择初步姿态；然后在精阶段，通过粒子滤波结合多光束跟踪方法优化姿态估计。整个方法基于建筑轮廓的显式匹配对齐。

Result: 实验表明，LoD-Loc v2在低细节模型上的定位性能首次得以实现，且在所有基准测试中均优于最新方法，包括超越了基于纹理模型的方法。

Conclusion: LoD-Loc v2不仅改进了高细节模型的定位精度，还首次实现了低细节模型的有效定位，扩展了定位算法的收敛范围，具有重要的应用潜力。

Abstract: We propose a novel method for aerial visual localization over low
Level-of-Detail (LoD) city models. Previous wireframe-alignment-based method
LoD-Loc has shown promising localization results leveraging LoD models.
However, LoD-Loc mainly relies on high-LoD (LoD3 or LoD2) city models, but the
majority of available models and those many countries plan to construct
nationwide are low-LoD (LoD1). Consequently, enabling localization on low-LoD
city models could unlock drones' potential for global urban localization. To
address these issues, we introduce LoD-Loc v2, which employs a coarse-to-fine
strategy using explicit silhouette alignment to achieve accurate localization
over low-LoD city models in the air. Specifically, given a query image, LoD-Loc
v2 first applies a building segmentation network to shape building silhouettes.
Then, in the coarse pose selection stage, we construct a pose cost volume by
uniformly sampling pose hypotheses around a prior pose to represent the pose
probability distribution. Each cost of the volume measures the degree of
alignment between the projected and predicted silhouettes. We select the pose
with maximum value as the coarse pose. In the fine pose estimation stage, a
particle filtering method incorporating a multi-beam tracking approach is used
to efficiently explore the hypothesis space and obtain the final pose
estimation. To further facilitate research in this field, we release two
datasets with LoD1 city models covering 10.7 km , along with real RGB queries
and ground-truth pose annotations. Experimental results show that LoD-Loc v2
improves estimation accuracy with high-LoD models and enables localization with
low-LoD models for the first time. Moreover, it outperforms state-of-the-art
baselines by large margins, even surpassing texture-model-based methods, and
broadens the convergence basin to accommodate larger prior errors.

</details>


### [66] [A Unified Transformer-Based Framework with Pretraining For Whole Body Grasping Motion Generation](https://arxiv.org/abs/2507.00676)
*Edward Effendy,Kuan-Wei Tseng,Rei Kawakami*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的全身抓取框架，包括姿态生成和运动填充，提升对象交互的真实感与稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决全身抓取中姿态生成和运动平滑性问题，同时应对数据稀缺的挑战。

Method: 设计三阶段流程：抓取姿态生成、时间填充和平滑，以及用于高分辨率还原的LiftUp Transformer，并进行了广义预训练以提高数据效率。

Result: 在GRAB数据集上表现优于当前最优基线算法，在连贯性、稳定性和视觉真实感上效果突出。

Conclusion: 方法具有优越的性能表现，并且通过模块化设计便于适配其他人体运动应用场景。

Abstract: Accepted in the ICIP 2025
  We present a novel transformer-based framework for whole-body grasping that
addresses both pose generation and motion infilling, enabling realistic and
stable object interactions. Our pipeline comprises three stages: Grasp Pose
Generation for full-body grasp generation, Temporal Infilling for smooth motion
continuity, and a LiftUp Transformer that refines downsampled joints back to
high-resolution markers. To overcome the scarcity of hand-object interaction
data, we introduce a data-efficient Generalized Pretraining stage on large,
diverse motion datasets, yielding robust spatio-temporal representations
transferable to grasping tasks. Experiments on the GRAB dataset show that our
method outperforms state-of-the-art baselines in terms of coherence, stability,
and visual realism. The modular design also supports easy adaptation to other
human-motion applications.

</details>


### [67] [Cage-Based Deformation for Transferable and Undefendable Point Cloud Attack](https://arxiv.org/abs/2507.00690)
*Keke Tang,Ziyong Du,Weilong Peng,Xiaofei Wang,Peican Zhu,Ligang Liu,Zhihong Tian*

Main category: cs.CV

TL;DR: 提出了一种名为CageAttack的框架，通过笼状变形生成自然的对抗性点云，同时在转移性、不可防御性和合理性之间实现了良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有点云的对抗性攻击方法依赖严格的几何约束，限制了转移性与不可防御性；而未结构化的变形方法容易造成不自然的形变，损害合理性。

Method: 利用笼状结构对目标对象进行包裹，并通过更改笼点使点云发生自然变形，从而实现对抗性攻击，确保形变的对象合理性和自然性。

Result: 在三个数据集上的七种3D深度神经网络分类器实验中，CageAttack在转移性、不可防御性和合理性方面超越了现有的最先进方法。

Conclusion: CageAttack能够生成自然的对抗性点云，在性能和合理性之间实现了优良的平衡，为点云对抗性攻击提供了新视角。

Abstract: Adversarial attacks on point clouds often impose strict geometric constraints
to preserve plausibility; however, such constraints inherently limit
transferability and undefendability. While deformation offers an alternative,
existing unstructured approaches may introduce unnatural distortions, making
adversarial point clouds conspicuous and undermining their plausibility. In
this paper, we propose CageAttack, a cage-based deformation framework that
produces natural adversarial point clouds. It first constructs a cage around
the target object, providing a structured basis for smooth, natural-looking
deformation. Perturbations are then applied to the cage vertices, which
seamlessly propagate to the point cloud, ensuring that the resulting
deformations remain intrinsic to the object and preserve plausibility.
Extensive experiments on seven 3D deep neural network classifiers across three
datasets show that CageAttack achieves a superior balance among
transferability, undefendability, and plausibility, outperforming
state-of-the-art methods. Codes will be made public upon acceptance.

</details>


### [68] [Rectifying Magnitude Neglect in Linear Attention](https://arxiv.org/abs/2507.00698)
*Qihang Fan,Huaibo Huang,Yuang Ai,ran He*

Main category: cs.CV

TL;DR: 本文提出了一个新的注意力机制MALA，通过在Linear Attention中引入Query的幅度信息，解决了其性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 解决Linear Attention性能下降以及不充分利用Query幅度信息的问题。

Method: 在Linear Attention的计算中引入Query的幅度信息，来生成更接近Softmax Attention的注意力分布。

Result: MALA在多个任务（如图像分类、目标检测、语义分割等）中均表现出色。

Conclusion: MALA在保持线性复杂度的同时，显著提高了Linear Attention的性能，并且接近Softmax Attention的效果。

Abstract: As the core operator of Transformers, Softmax Attention exhibits excellent
global modeling capabilities. However, its quadratic complexity limits its
applicability to vision tasks. In contrast, Linear Attention shares a similar
formulation with Softmax Attention while achieving linear complexity, enabling
efficient global information modeling. Nevertheless, Linear Attention suffers
from a significant performance degradation compared to standard Softmax
Attention. In this paper, we analyze the underlying causes of this issue based
on the formulation of Linear Attention. We find that, unlike Softmax Attention,
Linear Attention entirely disregards the magnitude information of the Query.
This prevents the attention score distribution from dynamically adapting as the
Query scales. As a result, despite its structural similarity to Softmax
Attention, Linear Attention exhibits a significantly different attention score
distribution. Based on this observation, we propose Magnitude-Aware Linear
Attention (MALA), which modifies the computation of Linear Attention to fully
incorporate the Query's magnitude. This adjustment allows MALA to generate an
attention score distribution that closely resembles Softmax Attention while
exhibiting a more well-balanced structure. We evaluate the effectiveness of
MALA on multiple tasks, including image classification, object detection,
instance segmentation, semantic segmentation, natural language processing,
speech recognition, and image generation. Our MALA achieves strong results on
all of these tasks. Code will be available at https://github.com/qhfan/MALA

</details>


### [69] [BEV-VAE: Multi-view Image Generation with Spatial Consistency for Autonomous Driving](https://arxiv.org/abs/2507.00707)
*Zeming Chen,Hang Zhao*

Main category: cs.CV

TL;DR: BEV-VAE提议用于实现一致性和可控的多视角图像合成，在自动驾驶方面展现出较强的性能。


<details>
  <summary>Details</summary>
Motivation: 多数现有方法缺乏显式的3D建模，本研究提出一种新方法通过结构化表示更好支持自动驾驶场景生成。

Method: BEV-VAE先训练一个多视图变分自编码器以获取紧凑的统一鸟瞰图（BEV）潜空间，再利用潜在扩散变换器进行场景生成，支持可配置的视角生成和3D布局选项。

Result: 在nuScenes和Argoverse 2数据集上进行实验，展现出在3D一致性重建和生成领域的强力性能。

Conclusion: BEV-VAE是一种有效的方法，基于结构化的3D理解解决多视角生成任务，具有潜在应用前景。

Abstract: Multi-view image generation in autonomous driving demands consistent 3D scene
understanding across camera views. Most existing methods treat this problem as
a 2D image set generation task, lacking explicit 3D modeling. However, we argue
that a structured representation is crucial for scene generation, especially
for autonomous driving applications. This paper proposes BEV-VAE for consistent
and controllable view synthesis. BEV-VAE first trains a multi-view image
variational autoencoder for a compact and unified BEV latent space and then
generates the scene with a latent diffusion transformer. BEV-VAE supports
arbitrary view generation given camera configurations, and optionally 3D
layouts. Experiments on nuScenes and Argoverse 2 (AV2) show strong performance
in both 3D consistent reconstruction and generation. The code is available at:
https://github.com/Czm369/bev-vae.

</details>


### [70] [TopoStreamer: Temporal Lane Segment Topology Reasoning in Autonomous Driving](https://arxiv.org/abs/2507.00709)
*Yiming Yang,Yueru Luo,Bingkun He,Hongbin Lin,Suzhong Fu,Chao Yan,Kun Tang,Xinrui Yan,Chao Zheng,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: 本文提出了TopoStreamer，一种端到端时序感知模型，用于车道段拓扑推理，在OpenLane-V2数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有方法在位置嵌入一致性和时序多属性学习上存在限制，影响了道路网络重建的准确性。

Method: 引入流式属性约束、动态车道边界位置编码以及车道段去噪，以增强模型表现和改进车道段拓扑推理。

Result: TopoStreamer在OpenLane-V2数据集上相比最先进方法，在车道段感知任务上提升3.4%mAP，在中心线感知任务上提升2.1%OLS。

Conclusion: TopoStreamer克服了现有方法的局限性，通过改进时序和空间感知能力，实现了车道段拓扑推理的显著进展，有助于自动驾驶系统执行更复杂的操作。

Abstract: Lane segment topology reasoning constructs a comprehensive road network by
capturing the topological relationships between lane segments and their
semantic types. This enables end-to-end autonomous driving systems to perform
road-dependent maneuvers such as turning and lane changing. However, the
limitations in consistent positional embedding and temporal multiple attribute
learning in existing methods hinder accurate roadnet reconstruction. To address
these issues, we propose TopoStreamer, an end-to-end temporal perception model
for lane segment topology reasoning. Specifically, TopoStreamer introduces
three key improvements: streaming attribute constraints, dynamic lane boundary
positional encoding, and lane segment denoising. The streaming attribute
constraints enforce temporal consistency in both centerline and boundary
coordinates, along with their classifications. Meanwhile, dynamic lane boundary
positional encoding enhances the learning of up-to-date positional information
within queries, while lane segment denoising helps capture diverse lane segment
patterns, ultimately improving model performance. Additionally, we assess the
accuracy of existing models using a lane boundary classification metric, which
serves as a crucial measure for lane-changing scenarios in autonomous driving.
On the OpenLane-V2 dataset, TopoStreamer demonstrates significant improvements
over state-of-the-art methods, achieving substantial performance gains of +3.4%
mAP in lane segment perception and +2.1% OLS in centerline perception tasks.

</details>


### [71] [UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement](https://arxiv.org/abs/2507.00721)
*Xiao Zhang,Fei Wei,Yong Wang,Wenda Zhao,Feiyi Li,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 本文提出了UPRE框架，通过优化文本提示和视觉表征，在零样本领域适配（ZSDA）检测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要解决领域分布迁移问题，但忽略了检测任务与依赖人工提示的视觉语言模型（VLM）之间的不匹配。

Method: 提出了统一提示和表征增强（UPRE）框架，引入多视图领域提示和视觉表征增强模块，并采用多级增强策略改善多模态表征对齐和视觉表征多样性。

Result: 在九个基准数据集上进行的实验表明，UPRE框架在ZSDA检测任务上表现优越。

Conclusion: UPRE框架通过联合优化文本提示和视觉表征，较好地解决了ZSDA中的领域适配和任务错位问题，展现了卓越的检测性能。

Abstract: Zero-shot domain adaptation (ZSDA) presents substantial challenges due to the
lack of images in the target domain. Previous approaches leverage
Vision-Language Models (VLMs) to tackle this challenge, exploiting their
zero-shot learning capabilities. However, these methods primarily address
domain distribution shifts and overlook the misalignment between the detection
task and VLMs, which rely on manually crafted prompts. To overcome these
limitations, we propose the unified prompt and representation enhancement
(UPRE) framework, which jointly optimizes both textual prompts and visual
representations. Specifically, our approach introduces a multi-view domain
prompt that combines linguistic domain priors with detection-specific
knowledge, and a visual representation enhancement module that produces domain
style variations. Furthermore, we introduce multi-level enhancement strategies,
including relative domain distance and positive-negative separation, which
align multi-modal representations at the image level and capture diverse visual
representations at the instance level, respectively. Extensive experiments
conducted on nine benchmark datasets demonstrate the superior performance of
our framework in ZSDA detection scenarios. Code is available at
https://github.com/AMAP-ML/UPRE.

</details>


### [72] [Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features](https://arxiv.org/abs/2507.00724)
*Linghui Zhu,Yiming Li,Haiqin Weng,Yan Liu,Tianwei Zhang,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

TL;DR: 该论文提出一种新方法，用于验证个性化模型的拥有权，通过分离相似的公共特征实现改进，并能有效检测多种模型窃取行为。


<details>
  <summary>Details</summary>
Motivation: 针对传统DNN模型的现有防御方法无法有效应对在预训练模型基础上微调的个性化模型面临的盗取风险的现实问题。

Method: 方法主要分为三阶段：第一，使用阴影模型保留受害模型的公共特征，同时破坏其特定于数据集的特征；第二，利用阴影模型与受害模型输出的差异表示数据集特定特征，并训练元分类器以识别可能被窃取的模型；第三，通过假设检验验证模型的拥有权，提升检测的随机性和鲁棒性。

Result: 在基准数据集上的大量实验表明，该方法能有效检测多种形式的模型窃取行为。

Conclusion: 提出的方法为个性化模型的拥有权验证提供了有效且创新的解决方案，克服了现有方法中的安全性和实用性问题。

Abstract: Large vision models achieve remarkable performance in various downstream
tasks, primarily by personalizing pre-trained models through fine-tuning with
private and valuable local data, which makes the personalized model a valuable
intellectual property for its owner. Similar to the era of traditional DNNs,
model stealing attacks also pose significant risks to these personalized
models. However, in this paper, we reveal that most existing defense methods
(developed for traditional DNNs), typically designed for models trained from
scratch, either introduce additional security risks, are prone to misjudgment,
or are even ineffective for fine-tuned models. To alleviate these problems,
this paper proposes a harmless model ownership verification method for
personalized models by decoupling similar common features. In general, our
method consists of three main stages. In the first stage, we create shadow
models that retain common features of the victim model while disrupting
dataset-specific features. We represent the dataset-specific features of the
victim model by the output differences between the shadow and victim models.
After that, a meta-classifier is trained to identify stolen models by
determining whether suspicious models contain the dataset-specific features of
the victim. In the third stage, we conduct model ownership verification by
hypothesis test to mitigate randomness and enhance robustness. Extensive
experiments on benchmark datasets verify the effectiveness of the proposed
method in detecting different types of model stealing simultaneously.

</details>


### [73] [Biorthogonal Tunable Wavelet Unit with Lifting Scheme in Convolutional Neural Network](https://arxiv.org/abs/2507.00739)
*An Le,Hung Nguyen,Sungbal Seo,You-Suk Bae,Truong Nguyen*

Main category: cs.CV

TL;DR: 引入了一种新的双正交可调浪潮单元，提升CNNs的分类和异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 提高CNNs处理图像分类和异常检测的能力，设计更灵活的滤波器。

Method: 通过构建基于提升方案的双正交可调浪潮单元，放松正交性和等长滤波器的限制，并将其整合到ResNet中。

Result: 在CIFAR-10分类准确率提升2.12%，DTD提升9.73%；在MVTec的异常检测中表现超越现有方法。

Conclusion: 提出的方法能够捕获更细粒度的细节，在分类与异常检测性能中显示了其有效性和鲁棒性。

Abstract: This work introduces a novel biorthogonal tunable wavelet unit constructed
using a lifting scheme that relaxes both the orthogonality and equal filter
length constraints, providing greater flexibility in filter design. The
proposed unit enhances convolution, pooling, and downsampling operations,
leading to improved image classification and anomaly detection in convolutional
neural networks (CNN). When integrated into an 18-layer residual neural network
(ResNet-18), the approach improved classification accuracy on CIFAR-10 by 2.12%
and on the Describable Textures Dataset (DTD) by 9.73%, demonstrating its
effectiveness in capturing fine-grained details. Similar improvements were
observed in ResNet-34. For anomaly detection in the hazelnut category of the
MVTec Anomaly Detection dataset, the proposed method achieved competitive and
wellbalanced performance in both segmentation and detection tasks,
outperforming existing approaches in terms of accuracy and robustness.

</details>


### [74] [Improving the Reasoning of Multi-Image Grounding in MLLMs via Reinforcement Learning](https://arxiv.org/abs/2507.00748)
*Bob Zhang,Haoran Li,Tao Zhang,Cilin Yan,Jiayin Cai,Xiaolong Jiang,Yanbin Hao*

Main category: cs.CV

TL;DR: 提出了一种基于强化学习的后训练策略，以改进多模态大语言模型在多图像任务中的推理表现。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在复杂多图像组成及多模态指令任务中的表现下降，揭示了其跨图像推理和泛化的局限性。

Method: 结合链式推理数据初始化、低秩适配的监督微调和规则强化学习策略，以改进跨图像推理性能。

Result: 所提出的方法在实验中实现了显著的性能改进：MIG-Bench提升+9.04%，外域任务基准测试提升+4.98%。

Conclusion: 证明该方法在多图像感知和推理中的有效性与出色的泛化能力。

Abstract: Recently, Multimodal Large Language Models (MLLMs) excel at visual grounding
in single-image scenarios with textual references. However, their performance
degrades when handling real-world applications involving complex multi-image
compositions and multimodal instructions, which reveals limitations in
cross-image reasoning and generalization. To address these challenges, we adopt
a Reinforcement Learning (RL) based post-training strategy to improve the
reasoning performance of MLLMs in multi-image grounding tasks. Our approach
begins with synthesizing high-quality chain-of-thought (CoT) data for
cold-start initialization, followed by supervised fine-tuning (SFT) using
low-rank adaptation (LoRA). The cold-start training stage enables the model to
identify correct solutions. Subsequently, we perform rejection sampling using
the merged SFT model to curate high-quality RL data and leverage rule-based RL
to guide the model toward optimal reasoning paths. Extensive experimental
results demonstrate the effectiveness of our approach, achieving +9.04\%
improvements on MIG-Bench and +4.98\% improvements on several out-of-domain
reasoning grounding benchmarks over the SFT baseline. Furthermore, our approach
exhibits strong generalization in multi-image perception, with gains of +3.1\%
and +2.4\% over the base model on subsets of the BLINK and MMIU benchmarks,
respectively.

</details>


### [75] [Multi-Modal Graph Convolutional Network with Sinusoidal Encoding for Robust Human Action Segmentation](https://arxiv.org/abs/2507.00752)
*Hao Xing,Kai Zhe Boey,Yuankai Wu,Darius Burschka,Gordon Cheng*

Main category: cs.CV

TL;DR: 提出了一种多模态图卷积网络（MMGCN）以减少动作分割中的过分段误差，显著提升了分割精度。


<details>
  <summary>Details</summary>
Motivation: 在协作机器人场景下，精确的动作时间分割对理解活动标签和时间结构至关重要，但由于人类姿态估计与目标检测的噪声容易导致过分段误差。

Method: 提出MMGCN，融合低帧率视觉数据与高帧率运动数据，引入三项创新：三维骨架坐标的连续空间编码、分辨率对齐的时间图融合模块，以及能平滑动作过渡的数据增强技术SmoothLabelMix。

Result: 在Bimanual Actions Dataset基准测试中，方法的动作分割准确性优于现有技术，达到了F1@10: 94.5%和F1@25: 92.8%。

Conclusion: 该方法有效减少了动作序列的过分段现象，增强了时间一致性，为人机协作场景提供了更精准的分割工具。

Abstract: Accurate temporal segmentation of human actions is critical for intelligent
robots in collaborative settings, where a precise understanding of sub-activity
labels and their temporal structure is essential. However, the inherent noise
in both human pose estimation and object detection often leads to
over-segmentation errors, disrupting the coherence of action sequences. To
address this, we propose a Multi-Modal Graph Convolutional Network (MMGCN) that
integrates low-frame-rate (e.g., 1 fps) visual data with high-frame-rate (e.g.,
30 fps) motion data (skeleton and object detections) to mitigate fragmentation.
Our framework introduces three key contributions. First, a sinusoidal encoding
strategy that maps 3D skeleton coordinates into a continuous sin-cos space to
enhance spatial representation robustness. Second, a temporal graph fusion
module that aligns multi-modal inputs with differing resolutions via
hierarchical feature aggregation, Third, inspired by the smooth transitions
inherent to human actions, we design SmoothLabelMix, a data augmentation
technique that mixes input sequences and labels to generate synthetic training
examples with gradual action transitions, enhancing temporal consistency in
predictions and reducing over-segmentation artifacts.
  Extensive experiments on the Bimanual Actions Dataset, a public benchmark for
human-object interaction understanding, demonstrate that our approach
outperforms state-of-the-art methods, especially in action segmentation
accuracy, achieving F1@10: 94.5% and F1@25: 92.8%.

</details>


### [76] [Language-Unlocked ViT (LUViT): Empowering Self-Supervised Vision Transformers with LLMs](https://arxiv.org/abs/2507.00754)
*Selim Kuzucu,Muhammad Ferjad Naeem,Anna Kukleva,Federico Tombari,Bernt Schiele*

Main category: cs.CV

TL;DR: 本文提出LUViT架构，通过联合预训练视觉Transformer和语言模型融合模块，解决视觉任务中的模态不匹配问题，显著提升了下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 为了解决语言模型（LLM）与视觉Transformer（ViT）因模态不同而导致的融合效率低下及训练不稳定的问题，设计一种新方法以更好地利用LLM在视觉任务中的潜力。

Method: 提出LUViT，一种联合预训练策略：(1) 使用遮罩自动编码（MAE）预训练视觉Transformer以获取更丰富的视觉表征；(2) 同时使用MAE目标训练语言模型模块的低秩适配层（LoRA），以实现跨模态协同优化。

Result: LUViT架构在各类视觉任务中显著提升了表现，展示出利用LLM知识进行视觉理解更高效的潜力。

Conclusion: 通过LUViT方法，首次实现了视觉Transformer和语言模型间模态对齐的联合优化，为视觉理解任务提供了更优的解决方案。

Abstract: The integration of Large Language Model (LLMs) blocks with Vision
Transformers (ViTs) holds immense promise for vision-only tasks by leveraging
the rich semantic knowledge and reasoning capabilities of LLMs. However, a
fundamental challenge lies in the inherent modality mismatch between
text-centric pretraining of LLMs and vision-centric training of ViTs. Direct
fusion often fails to fully exploit the LLM's potential and suffers from
unstable finetuning. As a result, LLM blocks are kept frozen while only the
vision components are learned. As a remedy to these challenges, we introduce
Language-Unlocked Vision Transformers (LUViT), a novel approach that bridges
this modality mismatch through a synergistic pre-training strategy. LUViT
co-adapts a ViT backbone and an LLM fusion block by (1) employing Masked
Auto-Encoding (MAE) to pre-train the ViT for richer visual representations, and
(2) concurrently training Low-Rank Adaptation (LoRA) layers within the LLM
block using the MAE objective. This joint optimization guides the ViT to
produce LLM-aligned features and the LLM to effectively interpret visual
information. We demonstrate through extensive experiments that LUViT
significantly improves performance on various downstream vision tasks,
showcasing a more effective and efficient pathway to harness LLM knowledge for
visual understanding.

</details>


### [77] [Towards Open-World Human Action Segmentation Using Graph Convolutional Networks](https://arxiv.org/abs/2507.00756)
*Hao Xing,Kai Zhe Boey,Gordon Cheng*

Main category: cs.CV

TL;DR: 本文提出一种新的框架用于解决开放世界的人体-物体交互动作分割问题，通过设计新颖的网络结构和优化策略，在多个数据集上取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 由于现有方法难以在开放世界场景中对新动作做到良好的泛化，同时收集详尽的动作类别用于训练在实践中不可行，因此需要能检测和分割未分布动作的模型。

Method: 提出了开放世界动作分割任务并设计了一个三部分创新框架：（1）使用增强型金字塔图卷积网络（EPGCN）对时空特征进行强健的上采样；（2）使用基于Mixup的训练方式合成无分布数据；（3）提出新的时间聚类损失用于区分分布内和分布外样本。

Result: 在Bimanual Actions和H2O数据集上，通过多项开放集合评价指标，框架相比现有最优方法在开放集合分割和分布外检测性能上分别取得16.9%和34.6%的相对提升。

Conclusion: 新框架在解决开放世界动作分割问题上表现优异，实验验证了其组件的合理性，为开放世界任务提供了新的可能性。

Abstract: Human-object interaction segmentation is a fundamental task of daily activity
understanding, which plays a crucial role in applications such as assistive
robotics, healthcare, and autonomous systems. Most existing learning-based
methods excel in closed-world action segmentation, they struggle to generalize
to open-world scenarios where novel actions emerge. Collecting exhaustive
action categories for training is impractical due to the dynamic diversity of
human activities, necessitating models that detect and segment
out-of-distribution actions without manual annotation. To address this issue,
we formally define the open-world action segmentation problem and propose a
structured framework for detecting and segmenting unseen actions. Our framework
introduces three key innovations: 1) an Enhanced Pyramid Graph Convolutional
Network (EPGCN) with a novel decoder module for robust spatiotemporal feature
upsampling. 2) Mixup-based training to synthesize out-of-distribution data,
eliminating reliance on manual annotations. 3) A novel Temporal Clustering loss
that groups in-distribution actions while distancing out-of-distribution
samples.
  We evaluate our framework on two challenging human-object interaction
recognition datasets: Bimanual Actions and 2 Hands and Object (H2O) datasets.
Experimental results demonstrate significant improvements over state-of-the-art
action segmentation models across multiple open-set evaluation metrics,
achieving 16.9% and 34.6% relative gains in open-set segmentation (F1@50) and
out-of-distribution detection performances (AUROC), respectively. Additionally,
we conduct an in-depth ablation study to assess the impact of each proposed
component, identifying the optimal framework configuration for open-world
action segmentation.

</details>


### [78] [OptiPrune: Boosting Prompt-Image Consistency with Attention-Guided Noise and Dynamic Token Selection](https://arxiv.org/abs/2507.00789)
*Ziji Lu*

Main category: cs.CV

TL;DR: OptiPrune 是一个框架，通过结合初始噪声优化和基于相似性的token剪枝，实现语义一致性和运行效率的优化。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型面临语义失配和硬件效率瓶颈的问题，且解决方案要么计算开销大，要么语义保真性不足。

Method: 提出了OptiPrune框架，结合了基于注意力引导的噪声优化模块和基于补丁相似性的token剪枝策略以实现优化，并通过混合策略保持高效与语义一致性。

Result: 在多个基准数据集上验证，OptiPrune 在语义一致性方面达到了最新的效果，并显著降低了计算成本。

Conclusion: OptiPrune 提供了一种统一的优化方案，兼顾了噪声优化和剪枝策略，在实现语义一致的同时提升了硬件效率。

Abstract: Text-to-image diffusion models often struggle to achieve accurate semantic
alignment between generated images and text prompts while maintaining
efficiency for deployment on resource-constrained hardware. Existing approaches
either incur substantial computational overhead through noise optimization or
compromise semantic fidelity by aggressively pruning tokens. In this work, we
propose OptiPrune, a unified framework that combines distribution-aware initial
noise optimization with similarity-based token pruning to address both
challenges simultaneously. Specifically, (1) we introduce a distribution-aware
noise optimization module guided by attention scores to steer the initial
latent noise toward semantically meaningful regions, mitigating issues such as
subject neglect and feature entanglement; (2) we design a hardware-efficient
token pruning strategy that selects representative base tokens via patch-wise
similarity, injects randomness to enhance generalization, and recovers pruned
tokens using maximum similarity copying before attention operations. Our method
preserves the Gaussian prior during noise optimization and enables efficient
inference without sacrificing alignment quality. Experiments on benchmark
datasets, including Animal-Animal, demonstrate that OptiPrune achieves
state-of-the-art prompt-image consistency with significantly reduced
computational cost.

</details>


### [79] [LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling](https://arxiv.org/abs/2507.00790)
*Huaqiu Li,Yong Wang,Tongwen Huang,Hailang Huang,Haoqian Wang,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 提出了一种无数据集、通用的低级视觉图像修复方法，通过预训练的潜在扩散模型和循环后验采样实现。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以兼顾任务特定性和通用性，且依赖配对数据集，易受限于闭集问题。

Method: 利用预训练潜在扩散模型，通过加入语义先验、轻量化模块进行输入对齐，以及循环优化后验采样，实现无任务依赖的通用图像修复。

Result: 全面实验表明，该方法优于当前最新技术，表现出较高的有效性与鲁棒性。

Conclusion: 新方法在无需数据集依赖的前提下，实现了通用性与优越修复性能，并将代码和数据公开于GitHub。

Abstract: Unified image restoration is a significantly challenging task in low-level
vision. Existing methods either make tailored designs for specific tasks,
limiting their generalizability across various types of degradation, or rely on
training with paired datasets, thereby suffering from closed-set constraints.
To address these issues, we propose a novel, dataset-free, and unified approach
through recurrent posterior sampling utilizing a pretrained latent diffusion
model. Our method incorporates the multimodal understanding model to provide
sematic priors for the generative model under a task-blind condition.
Furthermore, it utilizes a lightweight module to align the degraded input with
the generated preference of the diffusion model, and employs recurrent
refinement for posterior sampling. Extensive experiments demonstrate that our
method outperforms state-of-the-art methods, validating its effectiveness and
robustness. Our code and data will be available at
https://github.com/AMAP-ML/LD-RPS.

</details>


### [80] [Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters](https://arxiv.org/abs/2507.00792)
*Hendric Voss,Stefan Kopp*

Main category: cs.CV

TL;DR: 提出了一种新的实时逆运动学(IK)求解器，专为生成逼真的人类运动而设计，基于自动微分和TensorFlow即时编译。


<details>
  <summary>Details</summary>
Motivation: 生成精确且逼真的虚拟人体运动对多个领域（如计算机图形学、虚拟环境和机器人学）至关重要。

Method: 利用TensorFlow中的自动微分和即时编译能力，将正向运动学和逆运动学处理为可微操作，解决了误差积累和复杂关节约束等问题。

Result: 在SMPLX骨骼模型上测试，与现有算法（如CCD、FABRIK和IPOPT）相比，表现出快速收敛、低计算开销和更高的成功率。

Conclusion: 该求解器能够实时生成高精度人体运动，改善了现有方法的性能，并提供了代码开源。

Abstract: Generating accurate and realistic virtual human movements in real-time is of
high importance for a variety of applications in computer graphics, interactive
virtual environments, robotics, and biomechanics. This paper introduces a novel
real-time inverse kinematics (IK) solver specifically designed for realistic
human-like movement generation. Leveraging the automatic differentiation and
just-in-time compilation of TensorFlow, the proposed solver efficiently handles
complex articulated human skeletons with high degrees of freedom. By treating
forward and inverse kinematics as differentiable operations, our method
effectively addresses common challenges such as error accumulation and
complicated joint limits in multi-constrained problems, which are critical for
realistic human motion modeling. We demonstrate the solver's effectiveness on
the SMPLX human skeleton model, evaluating its performance against widely used
iterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK,
and the nonlinear optimization algorithm IPOPT. Our experiments cover both
simple end-effector tasks and sophisticated, multi-constrained problems with
realistic joint limits. Results indicate that our IK solver achieves real-time
performance, exhibiting rapid convergence, minimal computational overhead per
iteration, and improved success rates compared to existing methods. The project
code is available at https://github.com/hvoss-techfak/TF-JAX-IK

</details>


### [81] [TRACE: Temporally Reliable Anatomically-Conditioned 3D CT Generation with Enhanced Efficiency](https://arxiv.org/abs/2507.00802)
*Minye Shao,Xingyu Miao,Haoran Duan,Zeyu Wang,Jingkun Chen,Yawen Huang,Xian Wu,Jingjing Deng,Yang Long,Yefeng Zheng*

Main category: cs.CV

TL;DR: 本文提出了TRACE框架，通过二维多模态条件扩散方法生成三维医疗图像，兼顾了计算效率和解剖一致性。


<details>
  <summary>Details</summary>
Motivation: 现有三维医疗图像生成方法在解剖一致性、轴向长度和计算成本方面存在不足，限制了其实用性。

Method: TRACE将二维切片视为视频帧对，通过结合分割的先验和放射性报告来实现解剖对齐，并利用光流技术维持时间一致性。

Result: 实验结果表明，TRACE在计算效率、解剖保真度和时空一致性上表现优异。

Conclusion: TRACE提供了一种高效且解剖一致的解决方案，有助于三维医疗图像的生成和应用。

Abstract: 3D medical image generation is essential for data augmentation and patient
privacy, calling for reliable and efficient models suited for clinical
practice. However, current methods suffer from limited anatomical fidelity,
restricted axial length, and substantial computational cost, placing them
beyond reach for regions with limited resources and infrastructure. We
introduce TRACE, a framework that generates 3D medical images with
spatiotemporal alignment using a 2D multimodal-conditioned diffusion approach.
TRACE models sequential 2D slices as video frame pairs, combining segmentation
priors and radiology reports for anatomical alignment, incorporating optical
flow to sustain temporal coherence. During inference, an overlapping-frame
strategy links frame pairs into a flexible length sequence, reconstructed into
a spatiotemporally and anatomically aligned 3D volume. Experimental results
demonstrate that TRACE effectively balances computational efficiency with
preserving anatomical fidelity and spatiotemporal consistency. Code is
available at: https://github.com/VinyehShaw/TRACE.

</details>


### [82] [CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs](https://arxiv.org/abs/2507.00817)
*Jiaming Zhang,Rui Hu,Qing Guo,Wei Yang Bryan Lim*

Main category: cs.CV

TL;DR: 本文提出了CAVALRY-V，一个针对视频多模态大语言模型（V-MLLMs）的跨模态对抗攻击框架。


<details>
  <summary>Details</summary>
Motivation: 研究V-MLLMs在对抗攻击下的脆弱性，旨在解决复杂的跨模态推理机制、时间依赖性和计算限制问题。

Method: 提出由双目标语义-视觉损失函数和包含大规模预训练与专业微调的两阶段生成框架组成的新方法。

Result: CAVALRY-V在多个视频理解基准测试中显著超过现有攻击方法，对商业系统和开源模型分别实现了22.8%和34.4%的性能提升。

Conclusion: CAVALRY-V展示出作为多模态系统对抗性研究基础方法的潜力。

Abstract: Video Multimodal Large Language Models (V-MLLMs) have shown impressive
capabilities in temporal reasoning and cross-modal understanding, yet their
vulnerability to adversarial attacks remains underexplored due to unique
challenges: complex cross-modal reasoning mechanisms, temporal dependencies,
and computational constraints. We present CAVALRY-V (Cross-modal
Language-Vision Adversarial Yielding for Videos), a novel framework that
directly targets the critical interface between visual perception and language
generation in V-MLLMs. Our approach introduces two key innovations: (1) a
dual-objective semantic-visual loss function that simultaneously disrupts the
model's text generation logits and visual representations to undermine
cross-modal integration, and (2) a computationally efficient two-stage
generator framework that combines large-scale pre-training for cross-model
transferability with specialized fine-tuning for spatiotemporal coherence.
Empirical evaluation on comprehensive video understanding benchmarks
demonstrates that CAVALRY-V significantly outperforms existing attack methods,
achieving 22.8% average improvement over the best baseline attacks on both
commercial systems (GPT-4.1, Gemini 2.0) and open-source models (QwenVL-2.5,
InternVL-2.5, Llava-Video, Aria, MiniCPM-o-2.6). Our framework achieves
flexibility through implicit temporal coherence modeling rather than explicit
regularization, enabling significant performance improvements even on image
understanding (34.4% average gain). This capability demonstrates CAVALRY-V's
potential as a foundational approach for adversarial research across multimodal
systems.

</details>


### [83] [Instant Particle Size Distribution Measurement Using CNNs Trained on Synthetic Data](https://arxiv.org/abs/2507.00822)
*Yasser El Jarida,Youssef Iraqi,Loubna Mekouar*

Main category: cs.CV

TL;DR: 本研究提出了基于深度学习的CNN方法，通过合成粒子图像实现自动化、实时的粒径分布（PSD）测量，避免传统方法的缺点。


<details>
  <summary>Details</summary>
Motivation: 在采矿、制药和化肥制造等工业中，精确的粒径分布测量对产品质量和操作效率至关重要。然而，传统方法手动且耗时，难以应对粒子重叠问题。

Method: 利用Blender生成逼真合成粒子图像，模拟不同工业场景，训练三种CNN架构（ResNet-50、InceptionV3、EfficientNet-B0）以预测关键PSD参数（d10, d50, d90）。

Result: 三种CNN架构的精度相当，EfficientNet-B0在计算效率方面表现最佳，适合实时工业部署。

Conclusion: 利用逼真合成数据集进行CNN训练的效果显著，为工业中粒径分布监测的自动化提供了巨大潜力，研究代码已公开。

Abstract: Accurate particle size distribution (PSD) measurement is important in
industries such as mining, pharmaceuticals, and fertilizer manufacturing,
significantly influencing product quality and operational efficiency.
Traditional PSD methods like sieve analysis and laser diffraction are manual,
time-consuming, and limited by particle overlap. Recent developments in
convolutional neural networks (CNNs) enable automated, real-time PSD estimation
directly from particle images. In this work, we present a CNN-based methodology
trained on realistic synthetic particle imagery generated using Blender's
advanced rendering capabilities. Synthetic data sets using this method can
replicate various industrial scenarios by systematically varying particle
shapes, textures, lighting, and spatial arrangements that closely resemble the
actual configurations. We evaluated three CNN-based architectures, ResNet-50,
InceptionV3, and EfficientNet-B0, for predicting critical PSD parameters (d10,
d50, d90). Results demonstrated comparable accuracy across models, with
EfficientNet-B0 achieving the best computational efficiency suitable for
real-time industrial deployment. This approach shows the effectiveness of
realistic synthetic data for robust CNN training, which offers significant
potential for automated industrial PSD monitoring. The code is released at :
https://github.com/YasserElj/Synthetic-Granular-Gen

</details>


### [84] [High-Frequency Semantics and Geometric Priors for End-to-End Detection Transformers in Challenging UAV Imagery](https://arxiv.org/abs/2507.00825)
*Hongxing Peng,Lide Chen,Hui Zhu,Yan Chen*

Main category: cs.CV

TL;DR: 提出HEGS-DETR，一种针对无人机对象检测优化的实时检测框架，解决了目标尺寸较小、密度高和背景复杂等问题，在VisDrone数据集上性能有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前无人机对象检测面临目标小、密集分布与背景杂乱等挑战，而现有通用算法和新兴端到端框架在处理这类问题上表现有限。

Method: 提出高频增强语义网络（HFESNet）、高效小目标金字塔（ESOP）策略，以及选择性查询回收（SQR）和几何感知位置编码（GAPE）模块，结合检测Transformer框架增强语义提取、目标检测和定位能力。

Result: 在VisDrone数据集上，HEGS-DETR相较基线提升5.1%的AP$_{50}$和3.8%的AP，同时保持实时性并减少4M参数量。

Conclusion: HEGS-DETR框架针对无人机图像进行了全面优化，显著提升了小目标检测性能，且具有很高的实时性和参数效率。

Abstract: Unmanned Aerial Vehicle-based Object Detection (UAV-OD) faces substantial
challenges, including small target sizes, high-density distributions, and
cluttered backgrounds in UAV imagery. Current algorithms often depend on
hand-crafted components like anchor boxes, which demand fine-tuning and exhibit
limited generalization, and Non-Maximum Suppression (NMS), which is
threshold-sensitive and prone to misclassifying dense objects. These generic
architectures thus struggle to adapt to aerial imaging characteristics,
resulting in performance limitations. Moreover, emerging end-to-end frameworks
have yet to effectively mitigate these aerial-specific challenges.To address
these issues, we propose HEGS-DETR, a comprehensively enhanced, real-time
Detection Transformer framework tailored for UAVs. First, we introduce the
High-Frequency Enhanced Semantics Network (HFESNet) as a novel backbone.
HFESNet preserves critical high-frequency spatial details to extract robust
semantic features, thereby improving discriminative capability for small and
occluded targets in complex backgrounds. Second, our Efficient Small Object
Pyramid (ESOP) strategy strategically fuses high-resolution feature maps with
minimal computational overhead, significantly boosting small object detection.
Finally, the proposed Selective Query Recollection (SQR) and Geometry-Aware
Positional Encoding (GAPE) modules enhance the detector's decoder stability and
localization accuracy, effectively optimizing bounding boxes and providing
explicit spatial priors for dense scenes. Experiments on the VisDrone dataset
demonstrate that HEGS-DETR achieves a 5.1\% AP$_{50}$ and 3.8\% AP increase
over the baseline, while maintaining real-time speed and reducing parameter
count by 4M.

</details>


### [85] [Do Echo Top Heights Improve Deep Learning Nowcasts?](https://arxiv.org/abs/2507.00845)
*Peter Pavlík,Marc Schleiss,Anna Bou Ezzeddine,Viera Rozinajová*

Main category: cs.CV

TL;DR: 这篇论文研究了将雷达的回波高度 (ETH) 引入深度学习模型，以改善降雨预测表现，尤其是短期预测。


<details>
  <summary>Details</summary>
Motivation: 为了提升降水短期预测的准确性，并充分利用雷达3D信息，特别是回波高度ETH，作为辅助变量探索其潜在贡献。

Method: 通过一个3D U-Net网络，以雷达回波强度和ETH作为独立输入通道进行处理，分析ETH信息在预测降雨强度中的效果。

Result: 模型在低雨量预测阈值下表现出改善，但在高强度降水下不一致；使用ETH的模型还可能低估降水强度，且增加误差方差。

Conclusion: 尽管ETH能在某些情况下改善预测表现，但它有时会引入更多混淆。研究为评估其他辅助变量对降水短期预测的潜在贡献提供了基础。

Abstract: Precipitation nowcasting -- the short-term prediction of rainfall using
recent radar observations -- is critical for weather-sensitive sectors such as
transportation, agriculture, and disaster mitigation. While recent deep
learning models have shown promise in improving nowcasting skill, most
approaches rely solely on 2D radar reflectivity fields, discarding valuable
vertical information available in the full 3D radar volume. In this work, we
explore the use of Echo Top Height (ETH), a 2D projection indicating the
maximum altitude of radar reflectivity above a given threshold, as an auxiliary
input variable for deep learning-based nowcasting. We examine the relationship
between ETH and radar reflectivity, confirming its relevance for predicting
rainfall intensity. We implement a single-pass 3D U-Net that processes both the
radar reflectivity and ETH as separate input channels. While our models are
able to leverage ETH to improve skill at low rain-rate thresholds, results are
inconsistent at higher intensities and the models with ETH systematically
underestimate precipitation intensity. Three case studies are used to
illustrate how ETH can help in some cases, but also confuse the models and
increase the error variance. Nonetheless, the study serves as a foundation for
critically assessing the potential contribution of additional variables to
nowcasting performance.

</details>


### [86] [UAVD-Mamba: Deformable Token Fusion Vision Mamba for Multimodal UAV Detection](https://arxiv.org/abs/2507.00849)
*Wei Li,Jiaman Tang,Yang Li,Beihao Xia,Ligang Tan,Hongmao Qin*

Main category: cs.CV

TL;DR: 提出一种基于Mamba架构的多模态无人机目标检测框架UAVD-Mamba，解决目标检测中的遮挡、小目标以及不规则形状问题，实验结果表明其超越当前基线模型。


<details>
  <summary>Details</summary>
Motivation: 无人机目标检测在多领域广泛应用，但受限于遮挡、小目标和不规则形状等问题，需要一个强大高效的多模态目标检测方法。

Method: 引入基于Mamba结构的多模态框架UAVD-Mamba，设计变形令牌Mamba块（DTMB），从RGB和红外(IR)模态提取特征，并通过融合模块进行特征融合。利用YOLO系列改进模块优化多尺度目标检测，并引入跨维度注意力机制提升特征判别能力。

Result: 在DroneVehicle数据集上，实验结果表明UAVD-Mamba在mAP指标上比基线OAFA方法提高3.6%。

Conclusion: UAVD-Mamba通过多模态特征的深度融合与优化策略，有效提高无人机目标检测性能，是一个高效且精准的检测框架。

Abstract: Unmanned Aerial Vehicle (UAV) object detection has been widely used in
traffic management, agriculture, emergency rescue, etc. However, it faces
significant challenges, including occlusions, small object sizes, and irregular
shapes. These challenges highlight the necessity for a robust and efficient
multimodal UAV object detection method. Mamba has demonstrated considerable
potential in multimodal image fusion. Leveraging this, we propose UAVD-Mamba, a
multimodal UAV object detection framework based on Mamba architectures. To
improve geometric adaptability, we propose the Deformable Token Mamba Block
(DTMB) to generate deformable tokens by incorporating adaptive patches from
deformable convolutions alongside normal patches from normal convolutions,
which serve as the inputs to the Mamba Block. To optimize the multimodal
feature complementarity, we design two separate DTMBs for the RGB and infrared
(IR) modalities, with the outputs from both DTMBs integrated into the Mamba
Block for feature extraction and into the Fusion Mamba Block for feature
fusion. Additionally, to improve multiscale object detection, especially for
small objects, we stack four DTMBs at different scales to produce multiscale
feature representations, which are then sent to the Detection Neck for Mamba
(DNM). The DNM module, inspired by the YOLO series, includes modifications to
the SPPF and C3K2 of YOLOv11 to better handle the multiscale features. In
particular, we employ cross-enhanced spatial attention before the DTMB and
cross-channel attention after the Fusion Mamba Block to extract more
discriminative features. Experimental results on the DroneVehicle dataset show
that our method outperforms the baseline OAFA method by 3.6% in the mAP metric.
Codes will be released at https://github.com/GreatPlum-hnu/UAVD-Mamba.git.

</details>


### [87] [Robust Component Detection for Flexible Manufacturing: A Deep Learning Approach to Tray-Free Object Recognition under Variable Lighting](https://arxiv.org/abs/2507.00852)
*Fatemeh Sadat Daneshmand*

Main category: cs.CV

TL;DR: 提出了一种基于计算机视觉的系统，实现工业机器人在无刚性定位要求下，可靠检测和抓取笔组件，即使在光照条件变化下，系统也表现出强大的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决工业4.0中灵活制造系统对机器人适应非结构化环境和光照变化的需求。

Method: 采用基于Mask R-CNN的算法，能够实现对任意方向笔组件的检测和抓取，并在ZHAW的完整笔制造生产线上测试其性能。

Result: 在多种光照条件下，检测精度达到95%，减少30%的组装时间，提升制造灵活性。

Conclusion: 通过四种不同光照场景的测试验证了系统的强鲁棒性，表明该方法可应用于实际工业部署。

Abstract: Flexible manufacturing systems in Industry 4.0 require robots capable of
handling objects in unstructured environments without rigid positioning
constraints. This paper presents a computer vision system that enables
industrial robots to detect and grasp pen components in arbitrary orientations
without requiring structured trays, while maintaining robust performance under
varying lighting conditions. We implement and evaluate a Mask R-CNN-based
approach on a complete pen manufacturing line at ZHAW, addressing three
critical challenges: object detection without positional constraints,
robustness to extreme lighting variations, and reliable performance with
cost-effective cameras. Our system achieves 95% detection accuracy across
diverse lighting conditions while eliminating the need for structured component
placement, demonstrating a 30% reduction in setup time and significant
improvement in manufacturing flexibility. The approach is validated through
extensive testing under four distinct lighting scenarios, showing practical
applicability for real-world industrial deployment.

</details>


### [88] [SafeMap: Robust HD Map Construction from Incomplete Observations](https://arxiv.org/abs/2507.00861)
*Xiaoshuai Hao,Lingdong Kong,Rong Yin,Pengwei Wang,Jing Zhang,Yunfeng Diao,Shu Zhao*

Main category: cs.CV

TL;DR: 本文提出了SafeMap框架，在摄像头视角缺失的情况下依然能构建高精度地图。


<details>
  <summary>Details</summary>
Motivation: 解决现有多视角摄像头数据不完整时，地图构建的准确性问题。

Method: 包含两个关键模块：G-PVR模块动态优先考虑重要视角区域，D-BEVC模块校正不完整观测下的鸟瞰图表示。

Result: 在完整和不完整场景下性能均显著优于现有方法。

Conclusion: SafeMap提供了一个易于集成的解决方案，提高了高精度地图生成的鲁棒性和可靠性。

Abstract: Robust high-definition (HD) map construction is vital for autonomous driving,
yet existing methods often struggle with incomplete multi-view camera data.
This paper presents SafeMap, a novel framework specifically designed to secure
accuracy even when certain camera views are missing. SafeMap integrates two key
components: the Gaussian-based Perspective View Reconstruction (G-PVR) module
and the Distillation-based Bird's-Eye-View (BEV) Correction (D-BEVC) module.
G-PVR leverages prior knowledge of view importance to dynamically prioritize
the most informative regions based on the relationships among available camera
views. Furthermore, D-BEVC utilizes panoramic BEV features to correct the BEV
representations derived from incomplete observations. Together, these
components facilitate the end-to-end map reconstruction and robust HD map
generation. SafeMap is easy to implement and integrates seamlessly into
existing systems, offering a plug-and-play solution for enhanced robustness.
Experimental results demonstrate that SafeMap significantly outperforms
previous methods in both complete and incomplete scenarios, highlighting its
superior performance and reliability.

</details>


### [89] [Is Visual in-Context Learning for Compositional Medical Tasks within Reach?](https://arxiv.org/abs/2507.00868)
*Simon Reiß,Zdravko Marinov,Alexander Jaus,Constantin Seibold,M. Saquib Sarfraz,Erik Rodner,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 本文研究视觉上下文学习以实现单一模型同时解决多个任务，并在测试时无需重新训练地适应新任务。


<details>
  <summary>Details</summary>
Motivation: 希望通过训练能够适应任务序列（非单一任务）的视觉上下文学习模型，解决需要多个中间步骤的复杂任务。

Method: 提出了一种使用合成组合任务生成引擎的方法，从任意分割数据集中构建任务序列来训练模型。并探讨了基于掩码的训练目标以优化模型训练。

Result: 研究揭示了视觉上下文学习架构的特性及其限制，特别是代码本的重要性，同时验证了所提出方法在处理组合任务时的有效性。

Conclusion: 本文为多模态任务序列的学习提供了重要见解，但也指出了现存的挑战需进一步解决。

Abstract: In this paper, we explore the potential of visual in-context learning to
enable a single model to handle multiple tasks and adapt to new tasks during
test time without re-training. Unlike previous approaches, our focus is on
training in-context learners to adapt to sequences of tasks, rather than
individual tasks. Our goal is to solve complex tasks that involve multiple
intermediate steps using a single model, allowing users to define entire vision
pipelines flexibly at test time. To achieve this, we first examine the
properties and limitations of visual in-context learning architectures, with a
particular focus on the role of codebooks. We then introduce a novel method for
training in-context learners using a synthetic compositional task generation
engine. This engine bootstraps task sequences from arbitrary segmentation
datasets, enabling the training of visual in-context learners for compositional
tasks. Additionally, we investigate different masking-based training objectives
to gather insights into how to train models better for solving complex,
compositional tasks. Our exploration not only provides important insights
especially for multi-modal medical task sequences but also highlights
challenges that need to be addressed.

</details>


### [90] [GaussianVLM: Scene-centric 3D Vision-Language Models using Language-aligned Gaussian Splats for Embodied Reasoning and Beyond](https://arxiv.org/abs/2507.00886)
*Anna-Maria Halacheva,Jan-Nico Zaech,Xi Wang,Danda Pani Paudel,Luc Van Gool*

Main category: cs.CV

TL;DR: 本文提出了一种场景为中心的3D视觉语言模型，使用语言和任务感知的场景表示，并通过Gaussian splatting技术直接将语言特征嵌入到3D场景中，大幅度提升了跨领域任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉语言模型依赖对象检测器，存在处理瓶颈和分类灵活性不足的问题。研究者希望通过新的方法克服这些局限性。

Method: 使用Gaussian splatting技术，将语言特征直接嵌入每个3D高斯元件中实现早期模态对齐。通过引入双稀疏化单元，根据任务和位置提取稠密表示以生成全局和局部任务相关的稀疏场景标记。

Result: 利用常规RGB图像生成的真实感3D表示，该方法实现了超过现有技术五倍的跨领域性能提升。

Conclusion: 使用语言和任务感知的稀疏化表示的场景为中心3D VLM能够有效提升3D场景理解的性能，特别是在跨领域任务中表现出色。

Abstract: As multimodal language models advance, their application to 3D scene
understanding is a fast-growing frontier, driving the development of 3D
Vision-Language Models (VLMs). Current methods show strong dependence on object
detectors, introducing processing bottlenecks and limitations in taxonomic
flexibility. To address these limitations, we propose a scene-centric 3D VLM
for 3D Gaussian splat scenes that employs language- and task-aware scene
representations. Our approach directly embeds rich linguistic features into the
3D scene representation by associating language with each Gaussian primitive,
achieving early modality alignment. To process the resulting dense
representations, we introduce a dual sparsifier that distills them into
compact, task-relevant tokens via task-guided and location-guided pathways,
producing sparse, task-aware global and local scene tokens. Notably, we present
the first Gaussian splatting-based VLM, leveraging photorealistic 3D
representations derived from standard RGB images, demonstrating strong
generalization: it improves performance of prior 3D VLM five folds, in
out-of-the-domain settings.

</details>


### [91] [Masks make discriminative models great again!](https://arxiv.org/abs/2507.00916)
*Tianshi Cao,Marie-Julie Rakotosaona,Ben Poole,Federico Tombari,Michael Niemeyer*

Main category: cs.CV

TL;DR: 本文提出了Image2GS，一种从单张图像中重建高度真实感3D场景的新方法，显著提高了可见区域的重建质量。


<details>
  <summary>Details</summary>
Motivation: 通过将图像到3D提升问题与补全问题分离，解决现有方法在不可见区域的生成困境，提高重建效率与质量。

Method: 采用优化的3D高斯点控制的可见性遮罩，专注于对图像中可见区域的3D提升，并通过遮罩训练策略来优化模型。

Result: Image2GS比基线方法在可见区域重建质量上有显著提升，并在完整场景评估中与先进的判别模型表现相当。

Conclusion: 将图像到3D提升作为独立问题处理的策略优于传统方法，凸显出专用技术的优势。

Abstract: We present Image2GS, a novel approach that addresses the challenging problem
of reconstructing photorealistic 3D scenes from a single image by focusing
specifically on the image-to-3D lifting component of the reconstruction
process. By decoupling the lifting problem (converting an image to a 3D model
representing what is visible) from the completion problem (hallucinating
content not present in the input), we create a more deterministic task suitable
for discriminative models. Our method employs visibility masks derived from
optimized 3D Gaussian splats to exclude areas not visible from the source view
during training. This masked training strategy significantly improves
reconstruction quality in visible regions compared to strong baselines.
Notably, despite being trained only on masked regions, Image2GS remains
competitive with state-of-the-art discriminative models trained on full target
images when evaluated on complete scenes. Our findings highlight the
fundamental struggle discriminative models face when fitting unseen regions and
demonstrate the advantages of addressing image-to-3D lifting as a distinct
problem with specialized techniques.

</details>


### [92] [MVP: Winning Solution to SMP Challenge 2025 Video Track](https://arxiv.org/abs/2507.00950)
*Liliang Ye,Yunyao Zhang,Yafeng Wu,Yi-Ping Phoebe Chen,Junqing Yu,Wei Yang,Zikai Song*

Main category: cs.CV

TL;DR: 本文提出了一种名为MVP的框架，用于多模态视频流行度预测，在SMP Challenge 2025比赛的Video Track中排名第一。


<details>
  <summary>Details</summary>
Motivation: 通过准确预测社交媒体视频的流行度，为内容推荐、趋势检测和观众互动提供支持，对开发更智能的社交媒体平台有重要意义。

Method: MVP结合了从预训练模型提取的视频深度特征、用户元数据和上下文信息，使用系统化的数据预处理技术（如对数变换及异常值去除），并利用梯度提升回归模型来捕捉多模态模式的复杂关系。

Result: 提出的MVP方法在SMP Challenge 2025的Video Track官方评估中表现最佳，验证了其在视频流行度预测任务中的有效性与可靠性。

Conclusion: MVP是一个高效可靠的多模态视频流行度预测框架，研究结果可为社交平台的视频推荐和用户交互提供借鉴，并且代码已经开源以供研究者使用。

Abstract: Social media platforms serve as central hubs for content dissemination,
opinion expression, and public engagement across diverse modalities. Accurately
predicting the popularity of social media videos enables valuable applications
in content recommendation, trend detection, and audience engagement. In this
paper, we present Multimodal Video Predictor (MVP), our winning solution to the
Video Track of the SMP Challenge 2025. MVP constructs expressive post
representations by integrating deep video features extracted from pretrained
models with user metadata and contextual information. The framework applies
systematic preprocessing techniques, including log-transformations and outlier
removal, to improve model robustness. A gradient-boosted regression model is
trained to capture complex patterns across modalities. Our approach ranked
first in the official evaluation of the Video Track, demonstrating its
effectiveness and reliability for multimodal video popularity prediction on
social platforms. The source code is available at
https://anonymous.4open.science/r/SMPDVideo.

</details>


### [93] [Surgical Neural Radiance Fields from One Image](https://arxiv.org/abs/2507.00969)
*Alberto Neri,Maximilan Fehrentz,Veronica Penza,Leonardo S. Mattos,Nazim Haouchine*

Main category: cs.CV

TL;DR: 该研究提出一种方法，通过结合单张术中图像和术前数据来高效训练NeRF，用于外科手术应用，并取得了高质量的3D重建和视角合成结果。


<details>
  <summary>Details</summary>
Motivation: 由于NeRF需要大量多视角数据，而在术中收集足够数据在时间上不可行，因此目标是提出一种方法利用有限数据实现高效3D重建和视图合成。

Method: 利用术前MRI数据定义相机视点，结合神经风格迁移技术将术中图像样式传递到训练数据集，生成用于单图像NeRF快速训练的数据集。

Result: 通过四例临床脑外科病例评估，实验表明该方法与基于实际手术图像训练的NeRF模型相比有良好的合成一致性，并展示了高结构相似性和纹理保真度。

Conclusion: 证实了在外科场景中通过单张图像成功训练NeRF的可行性，克服了传统多视角方法的限制。

Abstract: Purpose: Neural Radiance Fields (NeRF) offer exceptional capabilities for 3D
reconstruction and view synthesis, yet their reliance on extensive multi-view
data limits their application in surgical intraoperative settings where only
limited data is available. In particular, collecting such extensive data
intraoperatively is impractical due to time constraints. This work addresses
this challenge by leveraging a single intraoperative image and preoperative
data to train NeRF efficiently for surgical scenarios.
  Methods: We leverage preoperative MRI data to define the set of camera
viewpoints and images needed for robust and unobstructed training.
Intraoperatively, the appearance of the surgical image is transferred to the
pre-constructed training set through neural style transfer, specifically
combining WTC2 and STROTSS to prevent over-stylization. This process enables
the creation of a dataset for instant and fast single-image NeRF training.
  Results: The method is evaluated with four clinical neurosurgical cases.
Quantitative comparisons to NeRF models trained on real surgical microscope
images demonstrate strong synthesis agreement, with similarity metrics
indicating high reconstruction fidelity and stylistic alignment. When compared
with ground truth, our method demonstrates high structural similarity,
confirming good reconstruction quality and texture preservation.
  Conclusion: Our approach demonstrates the feasibility of single-image NeRF
training in surgical settings, overcoming the limitations of traditional
multi-view methods.

</details>


### [94] [RTMap: Real-Time Recursive Mapping with Change Detection and Localization](https://arxiv.org/abs/2507.00980)
*Yuheng Du,Sheng Yang,Lingxuan Wang,Zhenghua Hou,Chengying Cai,Zhitao Tan,Mingxia Chen,Shi-Sheng Huang,Qiang Li*

Main category: cs.CV

TL;DR: 提出了一种名为RTMap的新方法，通过持续众包构建多遍HD地图，解决在线HD映射方法中的感知误差、遮挡和多代理观测融合的难题。


<details>
  <summary>Details</summary>
Motivation: 现有的在线HD映射方法无法很好地解决感知误差、交通密集时的遮挡问题以及多代理观测的融合。

Method: 提出了RTMap方法，通过三种核心技术实现：对HD地图元素的不确定性感知空间建模、基于众包地图的概率定位以及检测道路结构变化的实时性。

Result: 实验表明RTMap在优于现有方法的地图质量和定位精度的同时，能增强预测和规划模块的性能，并异步提高地图的新鲜度和准确性。

Conclusion: RTMap有效解决了单遍HD映射方法的限制，提出了一种持续更新的多遍HD地图众包方式，为下游模块提供更加准确和实时的服务。

Abstract: While recent online HD mapping methods relieve burdened offline pipelines and
solve map freshness, they remain limited by perceptual inaccuracies, occlusion
in dense traffic, and an inability to fuse multi-agent observations. We propose
RTMap to enhance these single-traversal methods by persistently crowdsourcing a
multi-traversal HD map as a self-evolutional memory. On onboard agents, RTMap
simultaneously addresses three core challenges in an end-to-end fashion: (1)
Uncertainty-aware positional modeling for HD map elements, (2)
probabilistic-aware localization w.r.t. the crowdsourced prior-map, and (3)
real-time detection for possible road structural changes. Experiments on
several public autonomous driving datasets demonstrate our solid performance on
both the prior-aided map quality and the localization accuracy, demonstrating
our effectiveness of robustly serving downstream prediction and planning
modules while gradually improving the accuracy and freshness of the
crowdsourced prior-map asynchronously. Our source-code will be made publicly
available at https://github.com/CN-ADLab/RTMap (Camera ready version
incorporating reviewer suggestions will be updated soon).

</details>


### [95] [Evaluating Robustness of Monocular Depth Estimation with Procedural Scene Perturbations](https://arxiv.org/abs/2507.00981)
*Jack Nugent,Siyang Wu,Zeyu Ma,Beining Han,Meenal Parakh,Abhishek Joshi,Lingjie Mei,Alexander Raistrick,Xinyuan Li,Jia Deng*

Main category: cs.CV

TL;DR: 近年来单目深度估计有了显著进展，但评估标准多集中于准确性而非稳健性。本文提出PDE新基准，利用程序生成的3D场景测试深度模型对多种扰动的稳健性。


<details>
  <summary>Details</summary>
Motivation: 目前大多数深度估计模型在标准基准上的表现专注于评估准确性，而忽略了模型面对不同环境和变化的稳健性问题。

Method: 通过程序生成3D场景，系统性地引入不同类型的扰动，包括物体、相机、材质和光照等变化，并据此构建新的PDE基准，用于评估深度模型的稳健性。

Result: 分析揭示了当前最先进深度模型在面对不同扰动时的表现，发现了一些具有挑战性的扰动类型。

Conclusion: PDE基准为深度估计提供了一种新的评估框架，有望引导未来在模型稳健性方面的研究。

Abstract: Recent years have witnessed substantial progress on monocular depth
estimation, particularly as measured by the success of large models on standard
benchmarks. However, performance on standard benchmarks does not offer a
complete assessment, because most evaluate accuracy but not robustness. In this
work, we introduce PDE (Procedural Depth Evaluation), a new benchmark which
enables systematic robustness evaluation. PDE uses procedural generation to
create 3D scenes that test robustness to various controlled perturbations,
including object, camera, material and lighting changes. Our analysis yields
interesting findings on what perturbations are challenging for state-of-the-art
depth models, which we hope will inform further research. Code and data are
available at https://github.com/princeton-vl/proc-depth-eval.

</details>


### [96] [UniGlyph: Unified Segmentation-Conditioned Diffusion for Precise Visual Text Synthesis](https://arxiv.org/abs/2507.00992)
*Yuanrui Wang,Cong Han,YafeiLi,Zhipeng Jin,Xiawei Li,SiNan Du,Wen Tao,Yi Yang,shuanglong li,Chun Yuan,Liu Lin*

Main category: cs.CV

TL;DR: 本文提出了一种文本到图像生成的新方法，通过使用丰富的像素级文本掩码作为统一条件输入，以改善描述性文字生成的精确性和风格保留问题。


<details>
  <summary>Details</summary>
Motivation: 当前的文本到图像生成方法在处理视觉文字时存在模糊、语意漂移及风格控制不足等问题。本文旨在解决现有方法在维持字体风格和颜色提示方面的局限，以及模型设计复杂而导致的性能开销和灵活性下降等问题。

Method: 提出了一种分割引导框架：基于一个精细调整的双语分割模型来提取精准的文本掩码；结合扩散模型，增强了自适应字形条件和区域特定损失以提升文本内容和风格的保真度。

Result: 在AnyText基准测试中，方法在中、英文环境下显著超越之前方法。同时，建立了新的GlyphMM和MiniText基准测试，专注于复杂排版的一致性和小区域文本的生成质量。结果表明，新的方法相比现有技术有大幅度提升。

Conclusion: 本文方法在复杂排版保持和小字渲染等方面表现出色，验证了其强大的一般化能力和实用性，表现出高部署潜力。

Abstract: Text-to-image generation has greatly advanced content creation, yet
accurately rendering visual text remains a key challenge due to blurred glyphs,
semantic drift, and limited style control. Existing methods often rely on
pre-rendered glyph images as conditions, but these struggle to retain original
font styles and color cues, necessitating complex multi-branch designs that
increase model overhead and reduce flexibility. To address these issues, we
propose a segmentation-guided framework that uses pixel-level visual text masks
-- rich in glyph shape, color, and spatial detail -- as unified conditional
inputs. Our method introduces two core components: (1) a fine-tuned bilingual
segmentation model for precise text mask extraction, and (2) a streamlined
diffusion model augmented with adaptive glyph conditioning and a
region-specific loss to preserve textual fidelity in both content and style.
Our approach achieves state-of-the-art performance on the AnyText benchmark,
significantly surpassing prior methods in both Chinese and English settings. To
enable more rigorous evaluation, we also introduce two new benchmarks:
GlyphMM-benchmark for testing layout and glyph consistency in complex
typesetting, and MiniText-benchmark for assessing generation quality in
small-scale text regions. Experimental results show that our model outperforms
existing methods by a large margin in both scenarios, particularly excelling at
small text rendering and complex layout preservation, validating its strong
generalization and deployment readiness.

</details>


### [97] [GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning](https://arxiv.org/abs/2507.01006)
*Wenyi Hong,Wenmeng Yu,Xiaotao Gu,Guo Wang,Guobing Gan,Haomiao Tang,Jiale Cheng,Ji Qi,Junhui Ji,Lihang Pan,Shuaiqi Duan,Weihan Wang,Yan Wang,Yean Cheng,Zehai He,Zhe Su,Zhen Yang,Ziyang Pan,Aohan Zeng,Baoxu Wang,Boyan Shi,Changyu Pang,Chenhui Zhang,Da Yin,Fan Yang,Guoqing Chen,Jiazheng Xu,Jiali Chen,Jing Chen,Jinhao Chen,Jinghao Lin,Jinjiang Wang,Junjie Chen,Leqi Lei,Leyi Pan,Mingzhi Zhang,Qinkai Zheng,Sheng Yang,Shi Zhong,Shiyu Huang,Shuyuan Zhao,Siyan Xue,Shangqin Tu,Shengbiao Meng,Tianshu Zhang,Tianwei Luo,Tianxiang Hao,Tianle Gong,Wenkai Li,Wei Jia,Xin Lyu,Xuancheng Huang,Yanling Wang,Yadong Xue,Yanfeng Wang,Yifan An,Yifan Du,Yiming Shi,Yiheng Huang,Yilin Niu,Yuan Wang,Yuanchang Yue,Yuchen Li,Yutao Zhang,Yuxuan Zhang,Zhanxiao Du,Zhenyu Hou,Zhao Xue,Zhengxiao Du,Zihan Wang,Peng Zhang,Debing Liu,Bin Xu,Juanzi Li,Minlie Huang,Yuxiao Dong,Jie Tang*

Main category: cs.CV

TL;DR: 提出了GLM-4.1V-Thinking，一个专注于多模态推理的视图-语言模型，通过RLCS显著提升能力并在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 开发一个通用多模态推理的视图-语言模型，以提升其在各种任务中的性能表现。

Method: 通过大规模预训练开发视觉基础模型，并结合课程采样的强化学习（RLCS）提升模型推理能力。

Result: GLM-4.1V-9B-Thinking在28个基准测试中优于大多数模型，并在长文档理解和STEM推理等任务中表现出色。

Conclusion: GLM-4.1V-9B-Thinking取得了同类模型中的领先表现，具备广泛的通用能力，是开放领域研究的重要进展。

Abstract: We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to
advance general-purpose multimodal reasoning. In this report, we share our key
findings in the development of the reasoning-centric training framework. We
first develop a capable vision foundation model with significant potential
through large-scale pre-training, which arguably sets the upper bound for the
final performance. Reinforcement Learning with Curriculum Sampling (RLCS) then
unlocks the full potential of the model, leading to comprehensive capability
enhancement across a diverse range of tasks, including STEM problem solving,
video understanding, content recognition, coding, grounding, GUI-based agents,
and long document understanding, among others. To facilitate research in this
field, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art
performance among models of comparable size. In a comprehensive evaluation
across 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all
tasks and achieves comparable or even superior performance on 18 benchmarks
relative to the significantly larger Qwen2.5-VL-72B. Notably,
GLM-4.1V-9B-Thinking also demonstrates competitive or superior performance
compared to closed-source models such as GPT-4o on challenging tasks including
long document understanding and STEM reasoning, further underscoring its strong
capabilities. Code, models and more information are released at
https://github.com/THUDM/GLM-4.1V-Thinking.

</details>


### [98] [ShapeEmbed: a self-supervised learning framework for 2D contour quantification](https://arxiv.org/abs/2507.01009)
*Anna Foix Romero,Craig Russell,Alexander Krull,Virginie Uhlmann*

Main category: cs.CV

TL;DR: 提出了一种名为ShapeEmbed的无监督表示学习框架，能够提取对象轮廓，生成对变换不变的形状描述符。


<details>
  <summary>Details</summary>
Motivation: 解决传统形状描述符的局限性，确保提取的测量值对平移、缩放、旋转等几何变换具有不变性。

Method: 使用欧几里得距离矩阵表示二维图像的对象轮廓，通过自监督学习生成变换不变的形状描述符。

Result: ShapeEmbed在自然图像和生物图像的形状分类任务中性能优于现有方法。

Conclusion: 该方法提升了形状分类的准确性，尤其适用于生物成像领域。

Abstract: The shape of objects is an important source of visual information in a wide
range of applications. One of the core challenges of shape quantification is to
ensure that the extracted measurements remain invariant to transformations that
preserve an object's intrinsic geometry, such as changing its size,
orientation, and position in the image. In this work, we introduce ShapeEmbed,
a self-supervised representation learning framework designed to encode the
contour of objects in 2D images, represented as a Euclidean distance matrix,
into a shape descriptor that is invariant to translation, scaling, rotation,
reflection, and point indexing. Our approach overcomes the limitations of
traditional shape descriptors while improving upon existing state-of-the-art
autoencoder-based approaches. We demonstrate that the descriptors learned by
our framework outperform their competitors in shape classification tasks on
natural and biological images. We envision our approach to be of particular
relevance to biological imaging applications.

</details>


### [99] [DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution](https://arxiv.org/abs/2507.01012)
*Zhe Kong,Le Li,Yong Zhang,Feng Gao,Shaoshu Yang,Tao Wang,Kaihao Zhang,Zhuoliang Kang,Xiaoming Wei,Guanying Chen,Wenhan Luo*

Main category: cs.CV

TL;DR: 本论文提出了一种名为DAM-VSR的新方法，通过外观增强和运动控制的分离解决视频超分辨率领域中的细节生成和时间一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视频超分辨率方法在复杂退化场景下难以生成高质量的细节，且无法保证时间一致性。

Method: 提出DAM-VSR框架，采用外观增强和运动控制的分离策略，结合图像超分辨率和视频扩散模型的生成先验，同时引入运动对齐的双向采样策略延伸可处理的视频长度。

Result: DAM-VSR在真实世界数据和AI生成内容上实现了最先进的性能，展现强大的细节生成能力。

Conclusion: DAM-VSR通过外观与运动问题的解耦和策略创新，有效改善了视频超分辨率技术的性能，展现了在实际应用中的潜力。

Abstract: Real-world video super-resolution (VSR) presents significant challenges due
to complex and unpredictable degradations. Although some recent methods utilize
image diffusion models for VSR and have shown improved detail generation
capabilities, they still struggle to produce temporally consistent frames. We
attempt to use Stable Video Diffusion (SVD) combined with ControlNet to address
this issue. However, due to the intrinsic image-animation characteristics of
SVD, it is challenging to generate fine details using only low-quality videos.
To tackle this problem, we propose DAM-VSR, an appearance and motion
disentanglement framework for VSR. This framework disentangles VSR into
appearance enhancement and motion control problems. Specifically, appearance
enhancement is achieved through reference image super-resolution, while motion
control is achieved through video ControlNet. This disentanglement fully
leverages the generative prior of video diffusion models and the detail
generation capabilities of image super-resolution models. Furthermore, equipped
with the proposed motion-aligned bidirectional sampling strategy, DAM-VSR can
conduct VSR on longer input videos. DAM-VSR achieves state-of-the-art
performance on real-world data and AIGC data, demonstrating its powerful detail
generation capabilities.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [100] [Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data](https://arxiv.org/abs/2507.00152)
*Ekaterina Borisova,Fabio Barth,Nils Feldhus,Raia Abu Ahmad,Malte Ostendorff,Pedro Ortiz Suarez,Georg Rehm,Sebastian Möller*

Main category: cs.CL

TL;DR: 本文探索了LLMs在表格数据理解任务中的效果，提出了一个包含3017个表格的TableEval基准，发现LLMs在不同表格模式上的鲁棒性不错，但对科学表格存储数据处理有挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在多种下游任务中表现良好，但对表格结构化数据的处理效率还未深入研究，作者希望探讨其在跨领域和跨模式的表格理解任务中的表现。

Method: 通过比较LLMs在科学和非科学语境中的表现，以及其处理表格的不同表达形式（如图像与文本）来评价。此外，作者还进行了解释性分析，测量上下文使用和输入相关性，并引入了TableEval基准作为评估工具。

Result: 研究发现，LLMs在表格表现形式上的鲁棒性良好，但在科学表格数据处理方面仍存在显著挑战。

Conclusion: LLMs在表格数据中的应用具有潜力，但需要进一步优化以应对更具挑战性的科学表格数据。

Abstract: Tables are among the most widely used tools for representing structured data
in research, business, medicine, and education. Although LLMs demonstrate
strong performance in downstream tasks, their efficiency in processing tabular
data remains underexplored. In this paper, we investigate the effectiveness of
both text-based and multimodal LLMs on table understanding tasks through a
cross-domain and cross-modality evaluation. Specifically, we compare their
performance on tables from scientific vs. non-scientific contexts and examine
their robustness on tables represented as images vs. text. Additionally, we
conduct an interpretability analysis to measure context usage and input
relevance. We also introduce the TableEval benchmark, comprising 3017 tables
from scholarly publications, Wikipedia, and financial reports, where each table
is provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX.
Our findings indicate that while LLMs maintain robustness across table
modalities, they face significant challenges when processing scientific tables.

</details>


### [101] [Prompting as Scientific Inquiry](https://arxiv.org/abs/2507.00163)
*Ari Holtzman,Chenhao Tan*

Main category: cs.CL

TL;DR: 提示工程是用来研究和控制LLMs的主要方法，有着解锁主要能力的重要作用，作者主张将其视为一种行为科学研究，而非简单技术或炼金术。


<details>
  <summary>Details</summary>
Motivation: 探讨提示工程是否应被视为一种科学方法，而非简单的技术手段或猜测行为。

Method: 从提示工程作为研究LLMs行为学视角进行探讨，并对比其与机械解释的研究方式。

Result: 提示工程能够有效地探测LLMs的原生接口行为，并且其重要性不亚于机械可解释性研究。

Conclusion: 提示工程应被视为LLMs科学的一部分，是行为研究的核心方式之一。

Abstract: Prompting is the primary method by which we study and control large language
models. It is also one of the most powerful: nearly every major capability
attributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was
first unlocked through prompting. Yet prompting is rarely treated as science
and is frequently frowned upon as alchemy. We argue that this is a category
error. If we treat LLMs as a new kind of complex and opaque organism that is
trained rather than programmed, then prompting is not a workaround: it is
behavioral science. Mechanistic interpretability peers into the neural
substrate, prompting probes the model in its native interface: language. We
contend that prompting is not inferior, but rather a key component in the
science of LLMs.

</details>


### [102] [LineRetriever: Planning-Aware Observation Reduction for Web Agents](https://arxiv.org/abs/2507.00210)
*Imene Kerboua,Sahar Omidi Shayegan,Megh Thakkar,Xing Han Lù,Massimo Caccia,Véronique Eglin,Alexandre Aussem,Jérémy Espinas,Alexandre Lacoste*

Main category: cs.CL

TL;DR: 引入了一种名为LineRetriever的新方法，用于优化网页导航任务中的检索方式，通过识别与未来导航步骤最相关的观察信息，提高了检索效率和效果。


<details>
  <summary>Details</summary>
Motivation: 目前的语言模型在处理超出上下文限制的网页导航任务时存在困难，传统方法如底层截断或嵌入检索无法有效捕捉与规划相关的关键信息，影响未来行动的预测。

Method: 提出了LineRetriever方法，使用语言模型选择最相关的观察信息行进行检索，注重信息对未来导航行动的价值，而不仅仅依赖语义相似性。

Result: 实验表明，LineRetriever可以在保持性能的同时，减少网页导航过程中每一步的观察信息规模。

Conclusion: LineRetriever在规划相关信息检索方面显示出了显著的优势，为优化网页导航任务中的检索方法提供了新思路。

Abstract: While large language models have demonstrated impressive capabilities in web
navigation tasks, the extensive context of web pages, often represented as DOM
or Accessibility Tree (AxTree) structures, frequently exceeds model context
limits. Current approaches like bottom-up truncation or embedding-based
retrieval lose critical information about page state and action history. This
is particularly problematic for adaptive planning in web agents, where
understanding the current state is essential for determining future actions. We
hypothesize that embedding models lack sufficient capacity to capture
plan-relevant information, especially when retrieving content that supports
future action prediction. This raises a fundamental question: how can retrieval
methods be optimized for adaptive planning in web navigation tasks? In
response, we introduce \textit{LineRetriever}, a novel approach that leverages
a language model to identify and retrieve observation lines most relevant to
future navigation steps. Unlike traditional retrieval methods that focus solely
on semantic similarity, \textit{LineRetriever} explicitly considers the
planning horizon, prioritizing elements that contribute to action prediction.
Our experiments demonstrate that \textit{LineRetriever} can reduce the size of
the observation at each step for the web agent while maintaining consistent
performance within the context limitations.

</details>


### [103] [Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning](https://arxiv.org/abs/2507.00214)
*Mads Henrichsen,Rasmus Krebs*

Main category: cs.CL

TL;DR: 本文提出了一种新的两阶段方法，通过利用大语言模型（LLM）生成解释性推理来改进文本分类任务的性能和解释性。


<details>
  <summary>Details</summary>
Motivation: 传统分类模型缺乏显式推理，可能限制其在性能、鲁棒性和可解释性方面的表现。

Method: 第一阶段，微调 Llama 模型生成一般性推理；第二阶段，利用该生成的推理数据扩展下游模型的训练集，训练模型同时输出推理和情感标签。

Result: 改进的分类器在情感预测准确率上相比基线提升了8.7个百分点。

Conclusion: LLM生成的推理能够丰富训练数据集，从而改善下游NLP任务的性能并提供显式解释性。

Abstract: Standard classification models often map inputs directly to labels without
explicit reasoning, potentially limiting their performance, robustness, and
interpretability. This paper introduces a novel two-stage approach to enhance
text classification by leveraging Large Language Model (LLM)-generated
reasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model
(henceforth Llama-R-Gen) on a general-purpose reasoning dataset
(syvai/reasoning-gen) to generate textual reasoning (R) given a question and
its answer. In the second stage, this generally trained Llama-R-Gen is used
offline to create an augmented training dataset for a downstream generative
model. This downstream model, based on Llama-3.2-1B-Instruct, takes only the
input text (Q) and is trained to output the generated reasoning (R) immediately
followed by the predicted emotion (A). We demonstrate this methodology on the
dair-ai/emotion dataset for emotion classification. Our experiments show that
the generative model trained to output reasoning and the emotion (Classifier
Q->RA) achieves a significant improvement of 8.7 percentage points in accuracy
(for emotion prediction) compared to a baseline generative model trained solely
to output the emotion (Classifier Q->A), highlighting the strong generalization
capabilities of the reasoning generation and the benefit of explicit reasoning
training. This work underscores the potential of LLM-generated reasonings for
creating richer training datasets, thereby improving the performance of diverse
downstream NLP tasks and providing explicit explanations.

</details>


### [104] [Towards Style Alignment in Cross-Cultural Translation](https://arxiv.org/abs/2507.00216)
*Shreya Havaldar,Adam Stein,Eric Wong,Lyle Ungar*

Main category: cs.CL

TL;DR: 研究揭示了大型语言模型（LLMs）在翻译风格中的局限性，并提出了一种新的方法来改善文化背景下的风格对齐问题。


<details>
  <summary>Details</summary>
Motivation: 解决因文化差异导致的风格失配问题，提高语言翻译的准确性和文化适应性。

Method: 提出RASTA方法，通过检索增强和学习的风格概念，使LLMs翻译更符合文化交流规范并对齐风格。

Result: 识别了LLMs在翻译中的风格中性偏向以及在非西方语言中的较差表现，RASTA方法改善了风格传递的准确性。

Conclusion: RASTA方法有效增强了LLMs在不同文化下的翻译风格对齐能力，有助于解决因文化差异引起的交流误解问题。

Abstract: Successful communication depends on the speaker's intended style (i.e., what
the speaker is trying to convey) aligning with the listener's interpreted style
(i.e., what the listener perceives). However, cultural differences often lead
to misalignment between the two; for example, politeness is often lost in
translation. We characterize the ways that LLMs fail to translate style -
biasing translations towards neutrality and performing worse in non-Western
languages. We mitigate these failures with RASTA (Retrieval-Augmented STylistic
Alignment), a method that leverages learned stylistic concepts to encourage LLM
translation to appropriately convey cultural communication norms and align
style.

</details>


### [105] [Linearly Decoding Refused Knowledge in Aligned Language Models](https://arxiv.org/abs/2507.00239)
*Aryan Shrivastava,Ari Holtzman*

Main category: cs.CL

TL;DR: 研究表明，尽管语言模型（LMs）通过指令调优和强化学习进行对齐以拒绝有害请求，但使用“越狱提示”常可绕过这些机制。内部信息在调优后仍然可通过线性探针解码，表明有害信息被抑制而非完全移除。


<details>
  <summary>Details</summary>
Motivation: 探讨通过越狱提示访问的信息在语言模型隐藏状态中是否仍然可被解码，研究是否指令调优真正消除了有害信息。

Method: 通过训练线性探针分析语言模型隐藏状态，评估被拒信息是否可解码，分析探针在不同模型中的迁移性与用途效果。

Result: 发现许多最初被拒绝的有害信息可通过线性探针解码，例如，对一些问题的预测值在皮尔逊相关性中超过0.8；且指令调优后相关信息依旧被模型利用。

Conclusion: 指令调优并未完全消除有害信息，只是抑制了其直接表达，信息在内部表示中依然活动并对下游任务产生影响。

Abstract: Most commonly used language models (LMs) are instruction-tuned and aligned
using a combination of fine-tuning and reinforcement learning, causing them to
refuse users requests deemed harmful by the model. However, jailbreak prompts
can often bypass these refusal mechanisms and elicit harmful responses. In this
work, we study the extent to which information accessed via jailbreak prompts
is decodable using linear probes trained on LM hidden states. We show that a
great deal of initially refused information is linearly decodable. For example,
across models, the response of a jailbroken LM for the average IQ of a country
can be predicted by a linear probe with Pearson correlations exceeding $0.8$.
Surprisingly, we find that probes trained on base models (which do not refuse)
sometimes transfer to their instruction-tuned versions and are capable of
revealing information that jailbreaks decode generatively, suggesting that the
internal representations of many refused properties persist from base LMs
through instruction-tuning. Importantly, we show that this information is not
merely "leftover" in instruction-tuned models, but is actively used by them: we
find that probe-predicted values correlate with LM generated pairwise
comparisons, indicating that the information decoded by our probes align with
suppressed generative behavior that may be expressed more subtly in other
downstream tasks. Overall, our results suggest that instruction-tuning does not
wholly eliminate or even relocate harmful information in representation
space-they merely suppress its direct expression, leaving it both linearly
accessible and indirectly influential in downstream behavior.

</details>


### [106] [The Algebraic Structure of Morphosyntax](https://arxiv.org/abs/2507.00244)
*Isabella Senturia,Matilde Marcolli*

Main category: cs.CL

TL;DR: 此论文提出了一种数学模型来描述形态学与句法之间的接口，强调了形态学的组成特性以及与句法结合的方法。


<details>
  <summary>Details</summary>
Motivation: 理解形态学和句法之间的相互作用，以及如何在数学框架下描述两者的结合。

Method: 利用代数和运算代数的框架，结合形态学和句法树的概念，通过数学模型描述结构形成机制。

Result: 提出了一个能解释形态-句法数据结合的模型，同时重解释了分布式形态学中的一些操作。

Conclusion: 此模型为研究形态与句法界限的灵活性提供了数学工具，从而深化了语言学的理论研究。

Abstract: Within the context of the mathematical formulation of Merge and the Strong
Minimalist Thesis, we present a mathematical model of the morphology-syntax
interface. In this setting, morphology has compositional properties responsible
for word formation, organized into a magma of morphological trees. However,
unlike syntax, we do not have movement within morphology. A coproduct
decomposition exists, but it requires extending the set of morphological trees
beyond those which are generated solely by the magma, to a larger set of
possible morphological inputs to syntactic trees. These participate in the
formation of morphosyntactic trees as an algebra over an operad, and a
correspondence between algebras over an operad. The process of structure
formation for morphosyntactic trees can then be described in terms of this
operadic correspondence that pairs syntactic and morphological data and the
morphology coproduct. We reinterpret in this setting certain operations of
Distributed Morphology as transformation that allow for flexibility in moving
the boundary between syntax and morphology within the morphosyntactic objects.

</details>


### [107] [EfficientXLang: Towards Improving Token Efficiency Through Cross-Lingual Reasoning](https://arxiv.org/abs/2507.00246)
*Sanchit Ahuja,Praneetha Vaddamanu,Barun Patra*

Main category: cs.CL

TL;DR: 研究探讨了非英语语言在语言推理模型中的效率和准确性，与英语相比，非英语语言能减少token使用同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管语言推理模型有显著发展，但大多数研究只关注英语，本研究意在探索非英语语言在推理中的潜力。

Method: 使用三个开源推理模型，跨四个数学数据集以及七种语言评估非英语语言的推理能力，并分析翻译后推理轨迹的表现。

Result: 结果显示，非英语语言推理不仅减少token使用，还保持高准确度，改进程度取决于模型的多语言能力。

Conclusion: 研究强调了多语言推理的重要性与潜力，同时指出强大的多语言基础对提升推理性能的关键作用。

Abstract: Despite recent advances in Language Reasoning Models (LRMs), most research
focuses solely on English, even though many models are pretrained on
multilingual data. In this work, we investigate: Is English the most
token-efficient language for reasoning? We evaluate three open-source RLMs:
DeepSeek R1, Qwen 2.5 and Qwen 3, across four math datasets and seven
typologically diverse languages. We find that reasoning in non-English
languages not only reduces token usage, but also preserves accuracy. These
gains persist even after translating the reasoning traces into English,
suggesting genuine shifts in reasoning behavior rather than surface-level
linguistic effects. The extent of improvement, however, depends on the models
multilingual strength. Our findings motivate a broader view of reasoning in
language models, highlighting the potential of multilingual reasoning and the
importance of strong multilingual foundations. The code for our work can be
found: https://github.com/microsoft/EfficientXLang.

</details>


### [108] [Impact of Fine-Tuning Methods on Memorization in Large Language Models](https://arxiv.org/abs/2507.00258)
*Jie Hou,Chuxiong Wu,Lannan Luo,Qiang Zeng*

Main category: cs.CL

TL;DR: 本文评估了大语言模型用于微调时的隐私风险，发现提示式微调比参数式微调整体显现更低的记忆化风险。


<details>
  <summary>Details</summary>
Motivation: 研究意图在于探讨大语言模型微调过程中记忆化可能引发的隐私风险及其规避方法。

Method: 通过分类流行的微调方法，并利用成员推断攻击（MIAs）分析其对记忆化的影响。

Result: 提示式微调在模型性能上与参数式微调相当，但在隐私泄露风险上表现更优，且其在不同模型规模下保持较低的记忆化风险。

Conclusion: 提示式微调在隐私保护方面优于参数式微调，是更具隐私优势的选择。

Abstract: As the capabilities of pre-trained large language models (LLMs) continue to
advance, the "pre-train and fine-tune" paradigm has become increasingly
mainstream, leading to the development of various fine-tuning methods. However,
the privacy risks arising from memorization during fine-tuning have received
relatively little attention. To address this gap, we categorize popular
fine-tuning approaches and assess their impact on memorization through the lens
of membership inference attacks (MIAs). Our results show that, compared to
parameter-based fine-tuning, prompt-based fine-tuning achieves competitive
performance while exhibiting lower vulnerability to MIAs. Furthermore,
prompt-based methods maintain low memorization regardless of model scale. These
findings suggest that parameter-based fine-tuning is more prone to leaking
private information, whereas prompt-based fine-tuning serves as a more
privacy-preserving option.

</details>


### [109] [Natural language processing for African languages](https://arxiv.org/abs/2507.00297)
*David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 本文研究了低资源语言特别是非洲语言的自然语言处理，探讨了语料质量对单词嵌入的影响，并展示了多语言预训练语言模型在低资源场景中的潜力，提供了21种非洲语言的数据集用于命名实体识别和机器翻译任务。


<details>
  <summary>Details</summary>
Motivation: 解决非洲低资源语言在NLP中的代表性不足问题，并通过高质量语料和多语言PLM改进模型表现。

Method: 分析公开语料中的噪声，构建高质量训练语料；探讨语言模型对低资源语言的适应性；建立21种非洲语言的大规模标注数据集；评估了监督、弱监督、迁移学习方法。

Result: 高质量语料有助于提升语义表示效果；多语言预训练语言模型在未见语言和低资源场景中的表现有明显优势；开发的非洲语言数据集为NLP任务提供了重要支持。

Conclusion: 研究揭示了语料质量对语言模型的重要性，通过创新方法提升了低资源语言的表现，贡献了高质量数据集，推动了非洲语言NLP研究的进展。

Abstract: Recent advances in word embeddings and language models use large-scale,
unlabelled data and self-supervised learning to boost NLP performance.
Multilingual models, often trained on web-sourced data like Wikipedia, face
challenges: few low-resource languages are included, their data is often noisy,
and lack of labeled datasets makes it hard to evaluate performance outside
high-resource languages like English. In this dissertation, we focus on
languages spoken in Sub-Saharan Africa where all the indigenous languages in
this region can be regarded as low-resourced in terms of the availability of
labelled data for NLP tasks and unlabelled data found on the web. We analyse
the noise in the publicly available corpora, and curate a high-quality corpus,
demonstrating that the quality of semantic representations learned in word
embeddings does not only depend on the amount of data but on the quality of
pre-training data. We demonstrate empirically the limitations of word
embeddings, and the opportunities the multilingual pre-trained language model
(PLM) offers especially for languages unseen during pre-training and
low-resource scenarios. We further study how to adapt and specialize
multilingual PLMs to unseen African languages using a small amount of
monolingual texts. To address the under-representation of the African languages
in NLP research, we developed large scale human-annotated labelled datasets for
21 African languages in two impactful NLP tasks: named entity recognition and
machine translation. We conduct an extensive empirical evaluation using
state-of-the-art methods across supervised, weakly-supervised, and transfer
learning settings.

</details>


### [110] [Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones](https://arxiv.org/abs/2507.00322)
*Daking Rai,Samuel Miller,Kevin Moran,Ziyu Yao*

Main category: cs.CL

TL;DR: 本文研究语言模型（LMs）在处理平衡括号等简单句法任务中的错误机制，并提出一种名为RASteer的方法显著提高其性能。


<details>
  <summary>Details</summary>
Motivation: 语言模型在处理代码生成等复杂任务上有显著提升，但在简单句法任务如生成平衡括号时仍然表现不佳，因此研究其错误生成机制并寻求改进方法。

Method: 通过分析不同规模语言模型组件（如注意力头和前馈神经元）的行为，将其分为可靠组件（做出正确预测）和不可靠组件（引入错误预测）。基于此，提出RASteer方法，通过系统性标识和增强可靠组件的贡献来提升模型性能。

Result: RASteer方法显著提高了平衡括号任务的模型表现，将部分模型的准确率从0%提升到接近100%，同时在算术推理任务中也提高了最高约20%的准确率。

Conclusion: 研究表明，通过识别并增强模型中可靠组件的贡献，不仅可以纠正简单句法任务中的错误，还能在其他任务上提供性能提升，验证了RASteer方法的有效性与广泛适用性。

Abstract: Despite remarkable advances in coding capabilities, language models (LMs)
still struggle with simple syntactic tasks such as generating balanced
parentheses. In this study, we investigate the underlying mechanisms behind the
persistence of these errors across LMs of varying sizes (124M-7B) to both
understand and mitigate the errors. Our study reveals that LMs rely on a number
of components (attention heads and FF neurons) that independently make their
own predictions. While some components reliably promote correct answers across
a generalized range of inputs (i.e., implementing "sound mechanisms''), others
are less reliable and introduce noise by promoting incorrect tokens (i.e.,
implementing "faulty mechanisms''). Errors occur when the faulty mechanisms
overshadow the sound ones and dominantly affect the predictions. Motivated by
this insight, we introduce RASteer, a steering method to systematically
identify and increase the contribution of reliable components for improving
model performance. RASteer substantially improves performance on balanced
parentheses tasks, boosting accuracy of some models from $0$% to around $100$%
without impairing the models' general coding ability. We further demonstrate
its broader applicability in arithmetic reasoning tasks, achieving performance
gains of up to around $20$%.

</details>


### [111] [Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios](https://arxiv.org/abs/2507.00330)
*Mohna Chakraborty,Adithya Kulkarni,Qi Li*

Main category: cs.CL

TL;DR: COLDSELECT结合自监督预训练语言模型，通过优化模板和选择具有最优数据多样性的样本及语言标签，有效提升冷启动场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 目前基于Prompt的方法在无标注数据的冷启动场景中，易受到模板、语言标签和实例选择的制约，且现有研究忽视了实例与语言标签之间的依赖性。本文提出一种能够优化数据多样性的方法，解决上述限制。

Method: 本文提出了COLDSELECT方法，将模型词汇表和[MASK]的嵌入映射到统一空间，并采用降维和聚类技术，通过优化数据不确定性和多样性联合选择实例和语言标签。

Result: 实验结果表明，在8个基准数据集上，COLDSELECT在减少不确定性和提升泛化能力上均优于现有基线方法。

Conclusion: COLDSELECT能够有效克服冷启动场景中数据选择和标签依赖性问题，实现数据多样性优化，从而显著增强语言模型的性能。

Abstract: Prompt-based methods leverage the knowledge of pre-trained language models
(PLMs) trained with a masked language modeling (MLM) objective; however, these
methods are sensitive to template, verbalizer, and few-shot instance selection,
particularly in cold-start settings with no labeled data. Existing studies
overlook the dependency between instances and verbalizers, where instance-label
probabilities depend on verbalizer token proximity in the embedding space. To
address this, we propose COLDSELECT, a joint verbalizer and instance selection
approach that models data diversity. COLDSELECT maps PLM vocabulary and
$h_{[MASK]}$ embeddings into a shared space, applying dimensionality reduction
and clustering to ensure efficient and diverse selection. By optimizing for
minimal uncertainty and maximal diversity, COLDSELECT captures data
relationships effectively. Experiments on eight benchmarks demonstrate
COLDSELECT's superiority in reducing uncertainty and enhancing generalization,
outperforming baselines in verbalizer and few-shot instance selection for
cold-start scenarios.

</details>


### [112] [Question Decomposition for Retrieval-Augmented Generation](https://arxiv.org/abs/2507.00355)
*Paul J. L. Ammann,Jonas Golde,Alan Akbik*

Main category: cs.CL

TL;DR: 本文探讨了通过问题分解来增强检索增强生成（RAG）方法，尤其针对多跳问题表现明显提升。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在处理多跳问题时表现不佳，因为相关信息分布在多个文档中，难以有效检索。

Method: 提出一种RAG管道：1) 使用LLM将问题分解为子问题；2)为每个子问题检索相关文档；3)对合并后的候选文档池进行重新排序以提高覆盖率和精度。

Result: 在MultiHop-RAG和HotpotQA数据集上，检索MRR@10提高36.7%，答案准确性F1提升11.6%。

Conclusion: 通过将问题分解与跨编码器重新排序相结合，无需额外训练或专用索引，即可显著提升多跳问题的处理能力。

Abstract: Grounding large language models (LLMs) in verifiable external sources is a
well-established strategy for generating reliable answers. Retrieval-augmented
generation (RAG) is one such approach, particularly effective for tasks like
question answering: it retrieves passages that are semantically related to the
question and then conditions the model on this evidence. However, multi-hop
questions, such as "Which company among NVIDIA, Apple, and Google made the
biggest profit in 2023?," challenge RAG because relevant facts are often
distributed across multiple documents rather than co-occurring in one source,
making it difficult for standard RAG to retrieve sufficient information. To
address this, we propose a RAG pipeline that incorporates question
decomposition: (i) an LLM decomposes the original query into sub-questions,
(ii) passages are retrieved for each sub-question, and (iii) the merged
candidate pool is reranked to improve the coverage and precision of the
retrieved evidence. We show that question decomposition effectively assembles
complementary documents, while reranking reduces noise and promotes the most
relevant passages before answer generation. Although reranking itself is
standard, we show that pairing an off-the-shelf cross-encoder reranker with
LLM-driven question decomposition bridges the retrieval gap on multi-hop
questions and provides a practical, drop-in enhancement, without any extra
training or specialized indexing. We evaluate our approach on the MultiHop-RAG
and HotpotQA, showing gains in retrieval (MRR@10: +36.7%) and answer accuracy
(F1: +11.6%) over standard RAG baselines.

</details>


### [113] [Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics](https://arxiv.org/abs/2507.00380)
*Vojtěch Lanz,Jan Hajič jr*

Main category: cs.CL

TL;DR: 本文探索了格里高利圣咏的旋律分割，提出了基于层级Pitman-Yor语言模型的无监督分割方法，达到了模式分类的最新性能，并探讨了旋律的记忆效率与实际演出用途的关系。


<details>
  <summary>Details</summary>
Motivation: 研究格里高利圣咏旋律中的常用片段及其与记忆效率的关系，以验证“旋律中心化”理论是否能被一种未被发现的旋律分割支持。

Method: 利用层级Pitman-Yor语言模型进行格里高利圣咏旋律的无监督分割，并通过模式分类性能来评估分割的优劣，同时探索记忆效率与模式分类的关系。

Result: 提出的分割方法在模式分类中达到了最新性能，并发现旋律开头和结尾的部分更具公式化，符合演奏实际需求和记忆效率。

Conclusion: 尽管提出的方法取得了很好的性能，但该分割方法与传统音乐学的“旋律中心化”理解不符。

Abstract: The idea that Gregorian melodies are constructed from some vocabulary of
segments has long been a part of chant scholarship. This so-called
"centonisation" theory has received much musicological criticism, but frequent
re-use of certain melodic segments has been observed in chant melodies, and the
intractable number of possible segmentations allowed the option that some
undiscovered segmentation exists that will yet prove the value of
centonisation, and recent empirical results have shown that segmentations can
outperform music-theoretical features in mode classification. Inspired by the
fact that Gregorian chant was memorised, we search for an optimal unsupervised
segmentation of chant melody using nested hierarchical Pitman-Yor language
models. The segmentation we find achieves state-of-the-art performance in mode
classification. Modeling a monk memorising the melodies from one liturgical
manuscript, we then find empirical evidence for the link between mode
classification and memory efficiency, and observe more formulaic areas at the
beginnings and ends of melodies corresponding to the practical role of modality
in performance. However, the resulting segmentations themselves indicate that
even such a memory-optimal segmentation is not what is understood as
centonisation.

</details>


### [114] [Causal Prompting for Implicit Sentiment Analysis with Large Language Models](https://arxiv.org/abs/2507.00389)
*Jing Ren,Wenhao Zhou,Bowen Li,Mujie Liu,Nguyen Linh Dan Le,Jiade Cen,Liping Chen,Ziqi Xu,Xiwei Xu,Xiaodong Li*

Main category: cs.CL

TL;DR: 文章提出了一个名为CAPITAL的因果提示框架，通过引入前门调整优化链式推理（CoT）的效果，用于隐式情感分析 (ISA)。


<details>
  <summary>Details</summary>
Motivation: 现有的隐式情感分析方法依赖于多数表决和链式推理，但可能存在偏差和虚假相关的问题。作者希望通过因果调整解决这些问题，提高模型的推理能力和鲁棒性。

Method: 提出的CAPITAL方法将因果效应分解为输入提示对推理链的影响和推理链对最终输出的影响，并通过对比学习目标优化表示。

Result: 在多个隐式情感分析基准数据集和三种大型语言模型上，CAPITAL在准确性和鲁棒性上均优于强基线方法，尤其是在对抗性条件下表现突出。

Conclusion: CAPITAL不仅在隐式情感分析上提升了性能，还证明了因果推理在语言模型提示中的优势，有助于实现更公平和偏差感知的分析。

Abstract: Implicit Sentiment Analysis (ISA) aims to infer sentiment that is implied
rather than explicitly stated, requiring models to perform deeper reasoning
over subtle contextual cues. While recent prompting-based methods using Large
Language Models (LLMs) have shown promise in ISA, they often rely on majority
voting over chain-of-thought (CoT) reasoning paths without evaluating their
causal validity, making them susceptible to internal biases and spurious
correlations. To address this challenge, we propose CAPITAL, a causal prompting
framework that incorporates front-door adjustment into CoT reasoning. CAPITAL
decomposes the overall causal effect into two components: the influence of the
input prompt on the reasoning chains, and the impact of those chains on the
final output. These components are estimated using encoder-based clustering and
the NWGM approximation, with a contrastive learning objective used to better
align the encoder's representation with the LLM's reasoning space. Experiments
on benchmark ISA datasets with three LLMs demonstrate that CAPITAL consistently
outperforms strong prompting baselines in both accuracy and robustness,
particularly under adversarial conditions. This work offers a principled
approach to integrating causal inference into LLM prompting and highlights its
benefits for bias-aware sentiment reasoning. The source code and case study are
available at: https://github.com/whZ62/CAPITAL.

</details>


### [115] [Beyond Sociodemographic Prompting: Using Supervision to Align LLMs with Human Response Distributions](https://arxiv.org/abs/2507.00439)
*Gauri Kambhatla,Sanjana Gautam,Angela Zhang,Alex Liu,Ravi Srinivasan,Junyi Jessy Li,Matthew Lease*

Main category: cs.CL

TL;DR: 本文探讨了如何通过简单监督提高语言模型与多样化人群的对齐能力。


<details>
  <summary>Details</summary>
Motivation: 准确预测不同人群对主观问题的回答具有重要价值。

Method: 利用简单监督方法，评估语言模型在三个涵盖多种话题数据集上的对齐表现，并报告具体群体间的差异。

Result: 实验结果表明，简单监督显著提升了语言模型对多样化人群的对齐性能。

Conclusion: 研究为如何使用此方法提供了实用指导，并通过开源和多种评测提供了未来研究的基准。

Abstract: The ability to accurately predict how different population groups would
answer subjective questions would have great value. In this work, we show that
use of relatively simple supervision can greatly improve language model
alignment with diverse population groups, as measured over three datasets
spanning various topics. Beyond evaluating average performance, we also report
how alignment varies across specific groups. The simplicity and generality of
our approach promotes easy adoption, while our broad findings provide useful
guidance for when to use or not use our approach in practice. By conducting
evaluation over many LLMs and prompting strategies, along with open-sourcing
our work, we provide a useful benchmark to stimulate future research.

</details>


### [116] [Pitfalls of Evaluating Language Models with Open Benchmarks](https://arxiv.org/abs/2507.00460)
*Md. Najib Hasan,Mohammad Fakhruddin Babar,Souvika Sarkar,Monowar Hasan,Santu Karmaker*

Main category: cs.CL

TL;DR: 该研究揭示开放式基准测试如HELM存在漏洞，通过建立"作弊"模型展示排行榜中的高表现并不等于真实效能。


<details>
  <summary>Details</summary>
Motivation: 探讨开放式基准测试中指标透明性与公平性的问题，揭示潜在漏洞。

Method: 通过构造"作弊"模型（调整后的BART、T5和GPT-2），检验公开基准测试的弱点。

Result: 这些模型尽管泛化能力弱，但在公开基准测试（如HELM）上排名靠前。

Conclusion: 简单双排名不可靠，建议结合私有或动态评估，同时重新思考当前的测试实践。

Abstract: Open Large Language Model (LLM) benchmarks, such as HELM and BIG-bench, offer
standardized, transparent protocols that facilitate the fair comparison,
reproducibility, and iterative advancement of Language Models (LMs). However,
their openness also introduces critical and underexplored pitfalls. This study
exposes these weaknesses by systematically constructing ``cheating'' models --
smaller variants of BART, T5, and GPT-2 fine-tuned directly on public test sets
-- which achieve top rankings on a prominent open, holistic benchmark (HELM)
despite poor generalization and limited practical utility. Our findings
underscore three key insights: \ca high leaderboard performance on open
benchmarks may not always reflect real-world effectiveness; \cb private or
dynamic benchmarks must complement open evaluations to safeguard integrity; and
\cc a fundamental reevaluation of current benchmarking practices is essential
to ensure robust and trustworthy LM assessments.

</details>


### [117] [TeamCMU at Touché: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search](https://arxiv.org/abs/2507.00509)
*To Eun Kim,João Coelho,Gbemileke Onilude,Jai Singh*

Main category: cs.CL

TL;DR: 本文讨论了将广告融入基于LLM和RAG的对话式搜索系统中的生成式响应，提出了一种模块化广告管理管道，通过训练高性能分类器和优化策略实现了无缝广告整合和检测的平衡。


<details>
  <summary>Details</summary>
Motivation: 探索如何在基于LLM和RAG的生成式对话搜索系统中无缝整合广告，同时解决用户信任和透明度的挑战。

Method: 提出一个模块化管道，包括广告重写器与广告分类器，利用合成数据训练分类器，采用监督微调和最佳采样策略实现广告的优化整合。

Result: 实验表明提出的广告分类器性能出色，并通过分类器引导的优化策略，有效提升广告整合的隐蔽性与连贯性。

Conclusion: 研究展示了一个对抗共进化框架，有助于开发更智能的广告感知生成系统和健壮的广告分类器。

Abstract: As conversational search engines increasingly adopt generation-based
paradigms powered by Large Language Models (LLMs) and Retrieval-Augmented
Generation (RAG), the integration of advertisements into generated responses
presents both commercial opportunities and challenges for user experience.
Unlike traditional search, where advertisements are clearly delineated,
generative systems blur the boundary between informational content and
promotional material, raising concerns around transparency and trust. In this
work, we propose a modular pipeline for advertisement management in RAG-based
conversational systems, consisting of an ad-rewriter for seamless ad
integration and a robust ad-classifier for detection. We leverage synthetic
data to train high-performing classifiers, which are then used to guide two
complementary ad-integration strategies: supervised fine-tuning of the
ad-rewriter and a best-of-N sampling approach that selects the least detectable
ad-integrated response among multiple candidates. Our evaluation focuses on two
core questions: the effectiveness of ad classifiers in detecting diverse ad
integration strategies, and the training methods that best support coherent,
minimally intrusive ad insertion. Experimental results show that our
ad-classifier, trained on synthetic advertisement data inspired by marketing
strategies and enhanced through curriculum learning, achieves robust detection
performance. Additionally, we demonstrate that classifier-guided optimization,
through both fine-tuning and best-of-N sampling, significantly improves ad
stealth, enabling more seamless integration. These findings contribute an
adversarial co-evolution framework for developing more sophisticated ad-aware
generative search systems and robust ad classifiers.

</details>


### [118] [NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data](https://arxiv.org/abs/2507.00534)
*Tahir Javed,Kaushal Bhogale,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: 本文介绍Nirantar框架，以评估多语言和多领域ASR中的连续学习，包含22种语言和208个地区的真实数据集，提出新场景（LIDIL），并通过3250小时语音数据对现有方法进行系统性测试，得出当前方法表现不一致的结论。


<details>
  <summary>Details</summary>
Motivation: 在连续学习的研究中，当前依赖于模拟环境的实验反映不了实际应用中的复杂性，因此需要一个真实场景的数据框架以全面评估方法的通用性和有效性。

Method: 开发了一个新框架Nirantar，用自然增量的数据集（22种语言和多个领域的208个地区数据）评估连续学习，涵盖了LIL、DIL和新场景LIDIL，总计3250小时，其中1720小时为新增数据，同时对现方法进行了系统测试。

Result: 测试表明，目前没有单个方法在所有场景下表现始终优异，揭示了现有方法的局限性和必要改进方向。

Conclusion: Nirantar框架提供了一个真实和复杂的基准环境，可以更好地推动多语言和多领域连续学习研究的发展。同时，这一框架的系统性测试结果也表明需要设计更健壮的学习策略。

Abstract: We introduce Nirantar, a comprehensive framework for evaluating continual
learning (CL) in multilingual and multi-domain ASR. Designed to reflect
real-world CL challenges, Nirantar leverages data collected incrementally
across 22 languages and 208 districts in India through natural episodes. This
enables evaluation across Language-Incremental (LIL), Domain-Incremental (DIL),
and the novel Language-Incremental Domain-Incremental Learning (LIDIL)
scenarios. Unlike prior work that relies on simulated episodes, Nirantar
presents dynamic, non-uniform language and domain shifts, making it an ideal
testbed for CL research. With 3250 hours of human-transcribed speech, including
1720 hours newly introduced in this work, our framework enables systematic
benchmarking of CL methods. We evaluate existing approaches and demonstrate
that no single method performs consistently well, underscoring the need for
more robust CL strategies.

</details>


### [119] [Capsule Network-Based Semantic Intent Modeling for Human-Computer Interaction](https://arxiv.org/abs/2507.00540)
*Shixiao Wang,Yifan Zhuang,Runsheng Zhang,Zhijun Song*

Main category: cs.CL

TL;DR: 本文提出了一种基于胶囊网络的用户语义意图建模算法，用于解决人机交互中意图识别精度不足的问题，实验结果表明其优于传统方法及其他深度学习结构。


<details>
  <summary>Details</summary>
Motivation: 旨在解决人机交互中意图识别精度不足的问题，尤其是在复杂语义条件下提高性能。

Method: 通过胶囊网络的向量化结构表示输入文本的语义特征，并使用动态路由机制传递信息，同时结合卷积特征提取模块和基于边距的损失函数优化模型表现。

Result: 实验使用公开数据集进行测试，与多种主流模型比较，提出的方法在精度、F1分数和意图检测率上均表现优异，并分析了动态路由迭代次数对性能的影响。

Conclusion: 本研究提出了一种新型的基于结构化建模的语义建模方法，验证了其在语义建模中的稳定性和有效性，尤其在复杂语义条件下提高了意图识别性能。

Abstract: This paper proposes a user semantic intent modeling algorithm based on
Capsule Networks to address the problem of insufficient accuracy in intent
recognition for human-computer interaction. The method represents semantic
features in input text through a vectorized capsule structure. It uses a
dynamic routing mechanism to transfer information across multiple capsule
layers. This helps capture hierarchical relationships and part-whole structures
between semantic entities more effectively. The model uses a convolutional
feature extraction module as the low-level encoder. After generating initial
semantic capsules, it forms high-level abstract intent representations through
an iterative routing process. To further enhance performance, a margin-based
mechanism is introduced into the loss function. This improves the model's
ability to distinguish between intent classes. Experiments are conducted using
a public natural language understanding dataset. Multiple mainstream models are
used for comparison. Results show that the proposed model outperforms
traditional methods and other deep learning structures in terms of accuracy,
F1-score, and intent detection rate. The study also analyzes the effect of the
number of dynamic routing iterations on model performance. A convergence curve
of the loss function during training is provided. These results verify the
stability and effectiveness of the proposed method in semantic modeling.
Overall, this study presents a new structured modeling approach to improve
intent recognition under complex semantic conditions.

</details>


### [120] [Methodological Rigour in Algorithm Application: An Illustration of Topic Modelling Algorithm](https://arxiv.org/abs/2507.00547)
*Malmi Amadoru*

Main category: cs.CL

TL;DR: 文章讨论了通过结构性主题建模算法提升理论建构研究中的方法学严谨性，并为新的研究者及编辑提供指导。


<details>
  <summary>Details</summary>
Motivation: 高算力算法的崛起为理论发展提供了新的研究机会，但算法的黑箱性和应用中的透明性与严谨性不足带来了方法学挑战。

Method: 通过应用结构性主题建模算法，作者展示了如何在主题建模研究中保证严谨性，并提出一套指导方针。

Result: 这些指导方针不仅适用于主题建模算法，也可通过特定调整应用于其他算法。

Conclusion: 文章为主题建模研究提供了方法学上的贡献，并强调了在计算密集型理论建构研究中的严谨性。

Abstract: The rise of advanced computational algorithms has opened new avenues for
computationally intensive research approaches to theory development. However,
the opacity of these algorithms and lack of transparency and rigour in their
application pose methodological challenges, potentially undermining trust in
research. The discourse on methodological rigour in this new genre of research
is still emerging. Against this backdrop, I attempt to offer guidance on
methodological rigour, particularly in the context of topic modelling
algorithms. By illustrating the application of the structural topic modelling
algorithm and presenting a set of guidelines, I discuss how to ensure rigour in
topic modelling studies. Although the guidelines are for the application of
topic modelling algorithms, they can be applied to other algorithms with
context-specific adjustments. The guidelines are helpful, especially for novice
researchers applying topic modelling, and editors and reviewers handling topic
modelling manuscripts. I contribute to the literature on topic modelling and
join the emerging dialogue on methodological rigour in computationally
intensive theory construction research.

</details>


### [121] [TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification](https://arxiv.org/abs/2507.00579)
*Miriam Anschütz,Ekaterina Gikalo,Niklas Herbster,Georg Groh*

Main category: cs.CL

TL;DR: 探讨解决LLMs中幻觉问题的多语言框架，通过Wiki事实验证与BERT模型识别相结合，取得了多语言领域的竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型中幻觉问题的多语言性缺失，同时增强其信任与广泛应用能力。

Method: 提出了基于检索的维基百科事实验证，与旨在识别幻觉模式的BERT系统微调相结合的两部分流水线方法。

Result: 系统在14种语言中表现优异，其中8种语言排名前十，实现了超出共享任务所涵盖语言范围的多语言支持。

Conclusion: 所提出的多语言幻觉识别系统能提升LLMs输出质量，并具有未来应用和改进潜力。

Abstract: Hallucinations are one of the major problems of LLMs, hindering their
trustworthiness and deployment to wider use cases. However, most of the
research on hallucinations focuses on English data, neglecting the multilingual
nature of LLMs. This paper describes our submission to the SemEval-2025 Task-3
- Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related
Observable Overgeneration Mistakes. We propose a two-part pipeline that
combines retrieval-based fact verification against Wikipedia with a BERT-based
system fine-tuned to identify common hallucination patterns. Our system
achieves competitive results across all languages, reaching top-10 results in
eight languages, including English. Moreover, it supports multiple languages
beyond the fourteen covered by the shared task. This multilingual hallucination
identifier can help to improve LLM outputs and their usefulness in the future.

</details>


### [122] [Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based](https://arxiv.org/abs/2507.00601)
*Shuangquan Lyu,Yingnan Deng,Guiran Liu,Zhen Qi,Ruotong Wang*

Main category: cs.CL

TL;DR: 该论文提出了一个统一框架，结合知识转移模块和参数高效的微调策略，以提高大语言模型在低资源语言情境下的迁移和适应能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在低资源语言场景中有限的迁移和适应能力问题。

Method: 引入知识对齐损失和软提示微调模块，利用冻结策略和提示注入，结合轻量级适配模块，维持模型原有知识的同时快速适应新任务。

Result: 实验结果表明，该方法在多语言跨语言任务（如MLQA，XQuAD，PAWS-X）上表现优异，特别是在极低数据条件下展现显著优势。

Conclusion: 该方法兼具通用性和可扩展性，增强大语言模型特定任务的适配能力，同时保留其通用能力，非常适用于复杂语义建模和多语言处理任务。

Abstract: This paper addresses the limited transfer and adaptation capabilities of
large language models in low-resource language scenarios. It proposes a unified
framework that combines a knowledge transfer module with parameter-efficient
fine-tuning strategies. The method introduces knowledge alignment loss and soft
prompt tuning to guide the model in effectively absorbing the structural
features of target languages or tasks under minimal annotation. This enhances
both generalization performance and training stability. The framework includes
lightweight adaptation modules to reduce computational costs. During training,
it integrates freezing strategies and prompt injection to preserve the model's
original knowledge while enabling quick adaptation to new tasks. The study also
conducts stability analysis experiments and synthetic pseudo-data transfer
experiments to systematically evaluate the method's applicability and
robustness across different low-resource tasks. Experimental results show that
compared with existing multilingual pre-trained models and mainstream transfer
methods, the proposed approach achieves higher performance and stability on
cross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates
particularly strong advantages under extremely data-scarce conditions. The
proposed method offers strong generality and scalability. It enhances
task-specific adaptability while preserving the general capabilities of large
language models. This makes it well-suited for complex semantic modeling and
multilingual processing tasks.

</details>


### [123] [Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies](https://arxiv.org/abs/2507.00606)
*Tao Xiong,Xavier Hu,Wenyan Fan,Shengyu Zhang*

Main category: cs.CL

TL;DR: 提出了一种新的训练框架MoR，通过嵌入多样的推理策略，提高LLMs无需外部提示即可适应多任务的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型依赖手动构造的任务特定提示，限定了适应性和效率。

Method: 设计MoR框架，分为两个阶段：1. 思维链生成阶段，利用模型如GPT-4生成推理链模板；2. SFT数据集构建阶段，与数据集配对进行监督微调。

Result: 实验表明，MoR显著提升了性能，如MoR150在CoT提示下提高了2.2%的精度，相较于基线提升了13.5%。

Conclusion: MoR无需任务特定提示，即可提供通用解决方案，提升LLMs在多任务上的稳健推理能力。

Abstract: Large language models (LLMs) excel in complex tasks through advanced
prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but
their reliance on manually crafted, task-specific prompts limits adaptability
and efficiency. We introduce Mixture of Reasoning (MoR), a training framework
that embeds diverse reasoning strategies into LLMs for autonomous,
task-adaptive reasoning without external prompt engineering. MoR has two
phases: Thought Generation, creating reasoning chain templates with models like
GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets
for supervised fine-tuning.Our experiments show that MoR significantly enhances
performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting
and 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need
for task-specific prompts, offering a generalizable solution for robust
reasoning across diverse tasks.

</details>


### [124] [SAFER: Probing Safety in Reward Models with Sparse Autoencoder](https://arxiv.org/abs/2507.00665)
*Sihang Li,Wei Shi,Ziyuan Xie,Tao Liang,Guojun Ma,Xiang Wang*

Main category: cs.CL

TL;DR: SAFER通过稀疏自编码器解释并改进奖励模型，提升LLM安全性。


<details>
  <summary>Details</summary>
Motivation: 解决当前奖励模型在LLM对齐中不透明的问题。

Method: 利用稀疏自编码器分析奖励模型，提取人类可解释特征，并基于此设计数据投毒和清理策略。

Result: SAFER能够精准地在不影响聊天性能的情况下调整安全对齐效果。

Conclusion: SAFER对于高风险的LLM对齐任务具有解释、审计和优化奖励模型的价值。

Abstract: Reinforcement learning from human feedback (RLHF) is a key paradigm for
aligning large language models (LLMs) with human values, yet the reward models
at its core remain largely opaque. In this work, we present sparse Autoencoder
For Enhanced Reward model (\textbf{SAFER}), a novel framework for interpreting
and improving reward models through mechanistic analysis. Leveraging Sparse
Autoencoders (SAEs), we uncover human-interpretable features in reward model
activations, enabling insight into safety-relevant decision-making. We apply
SAFER to safety-oriented preference datasets and quantify the salience of
individual features by activation differences between chosen and rejected
responses. Using these feature-level signals, we design targeted data poisoning
and denoising strategies. Experiments show that SAFER can precisely degrade or
enhance safety alignment with minimal data modification, without sacrificing
general chat performance. Our approach contributes to interpreting, auditing
and refining reward models in high-stakes LLM alignment tasks. Our codes are
available at https://github.com/xzy-101/SAFER-code. \textit{This paper
discusses topics related to large language model safety and may include
discussions or examples that highlight potential risks or unsafe outcomes.}

</details>


### [125] [Contrasting Cognitive Styles in Vision-Language Models: Holistic Attention in Japanese Versus Analytical Focus in English](https://arxiv.org/abs/2507.00700)
*Ahmed Sabir,Azinovič Gasper,Mengsay Loem,Rajesh Sharma*

Main category: cs.CL

TL;DR: 本研究发现视觉-语言模型(VLMs)反映了训练语言不同文化下的认知模式差异。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉-语言模型是否反映不同文化背景下的视觉信息处理差异。

Method: 通过比较使用日语和英语训练的VLMs生成的图像描述，分析其是否呈现整体和分析倾向的不同。

Result: 结果显示，VLMs不仅内化语言结构特性，还重现训练数据中嵌入的文化行为。

Conclusion: 文化认知可能隐性地影响模型输出，揭示模型具有模拟文化模式的潜力。

Abstract: Cross-cultural research in perception and cognition has shown that
individuals from different cultural backgrounds process visual information in
distinct ways. East Asians, for example, tend to adopt a holistic perspective,
attending to contextual relationships, whereas Westerners often employ an
analytical approach, focusing on individual objects and their attributes. In
this study, we investigate whether Vision-Language Models (VLMs) trained
predominantly on different languages, specifically Japanese and English,
exhibit similar culturally grounded attentional patterns. Using comparative
analysis of image descriptions, we examine whether these models reflect
differences in holistic versus analytic tendencies. Our findings suggest that
VLMs not only internalize the structural properties of language but also
reproduce cultural behaviors embedded in the training data, indicating that
cultural cognition may implicitly shape model outputs.

</details>


### [126] [AI Analyst: Framework and Comprehensive Evaluation of Large Language Models for Financial Time Series Report Generation](https://arxiv.org/abs/2507.00718)
*Elizabeth Fons,Elena Kochkina,Rachneet Kaur,Zhen Zeng,Berowne Hlavaty,Charese Smiley,Svitlana Vyetrenko,Manuela Veloso*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLMs）从时间序列数据生成财务报告的潜力，并提出了相关框架和评估方法。


<details>
  <summary>Details</summary>
Motivation: 研究评估LLMs在财务报告生成中的表现，特别是在生成基于时间序列数据的报告时的能力与局限性。

Method: 构建了一个包含提示工程、模型选择及评估的框架，并引入自动高亮系统分类生成的报告内容，以评估模型的事实基础与推理能力。

Result: 实验结果表明，LLMs能够利用真实的股市指数数据和合成时间序列生成连贯且具有信息含量的财务报告。

Conclusion: LLMs在生成财务报告方面展现了显著潜力，通过对模型进行设计与评估，可进一步提升其在各类财务应用中的使用价值。

Abstract: This paper explores the potential of large language models (LLMs) to generate
financial reports from time series data. We propose a framework encompassing
prompt engineering, model selection, and evaluation. We introduce an automated
highlighting system to categorize information within the generated reports,
differentiating between insights derived directly from time series data,
stemming from financial reasoning, and those reliant on external knowledge.
This approach aids in evaluating the factual grounding and reasoning
capabilities of the models. Our experiments, utilizing both data from the real
stock market indices and synthetic time series, demonstrate the capability of
LLMs to produce coherent and informative financial reports.

</details>


### [127] [LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing](https://arxiv.org/abs/2507.00769)
*Daniel Fein,Sebastian Russo,Violet Xiang,Kabir Jolly,Rafael Rafailov,Nick Haber*

Main category: cs.CL

TL;DR: 本文引入了LitBench，一个针对创意写作验证的标准化基准和数据集，并评测了零样本LLM评审和训练后的奖励模型的表现。


<details>
  <summary>Details</summary>
Motivation: 创意写作缺乏参考标准，现有的零样本模型作为评判依据其可靠性尚不明确，亟需一种稳健的评价方法。

Method: 开发了LitBench基准和数据集，包括2480对验证集和43827对训练语料，评估零样本审查、训练奖励模型，并通过线上人类实验验证其排序结果。

Result: Claude-3.7-Sonnet作为最强零样本评审模型与人类偏好一致性达到73%，而Bradley-Terry和生成奖励模型的准确性达到78%。

Conclusion: 本文提出的LitBench在自动化创意写作系统的评估和优化中表现优异，并与人类偏好保持高度一致性，为后续研究提供了可靠资源。

Abstract: Evaluating creative writing generated by large language models (LLMs) remains
challenging because open-ended narratives lack ground truths. Without
performant automated evaluation methods, off-the-shelf (OTS) language models
are employed as zero-shot judges, yet their reliability is unclear in this
context. In pursuit of robust evaluation for creative writing, we introduce
LitBench, the first standardized benchmark and paired dataset for creative
writing verification, comprising a held-out test set of 2,480 debiased,
human-labeled story comparisons drawn from Reddit and a 43,827-pair training
corpus of human preference labels. Using LitBench, we (i) benchmark zero-shot
LLM judges, (ii) train Bradley Terry and generative reward models, and (iii)
conduct an online human study to validate reward model rankings on newly
LLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the
strongest off-the-shelf judge, reaching 73% agreement with human preferences;
among trained reward models, Bradley-Terry and Generative reward models both
attain an accuracy of 78%, outperforming all off-the-shelf judges. An online
human study further confirms that our trained reward models consistently align
with human preferences in novel LLM-generated stories. We release LitBench and
reward models at
https://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461,
providing a vetted resource for reliable, automated evaluation and optimization
of creative writing systems.

</details>


### [128] [A Diagrammatic Calculus for a Functional Model of Natural Language Semantics](https://arxiv.org/abs/2507.00782)
*Matthieu Pierre Boyer*

Main category: cs.CL

TL;DR: 本文运用函数式编程方法研究自然语言语义，并通过类型和效应系统以及图解演算提升更传统的表述风格。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过函数式编程提升自然语言语义的表现力，克服传统语义表示方式的不足。

Method: 形式化一个基于范畴的类型和效应系统，并构造图解演算来建模句子解析和效应处理。

Result: 高效地计算句子的语义表述，并具备更强的表达能力。

Conclusion: 通过函数式编程的范式和图解工具，可以提升自然语言语义的解析和表示效率。

Abstract: In this paper, we study a functional programming approach to natural language
semantics, allowing us to increase the expressivity of a more traditional
denotation style. We will formalize a category based type and effect system,
and construct a diagrammatic calculus to model parsing and handling of effects,
and use it to efficiently compute the denotations for sentences.

</details>


### [129] [Generative AI and the future of scientometrics: current topics and future questions](https://arxiv.org/abs/2507.00783)
*Benedetto Lepori,Jens Peter Andersen,Karsten Donnay*

Main category: cs.CL

TL;DR: 本文评估生成型人工智能（GenAI）在科学计量学中的应用及其广泛影响，同时探讨其能力与局限性。


<details>
  <summary>Details</summary>
Motivation: 旨在审视生成型人工智能在科学计量学领域中的应用潜力，以及其对知识生产模式的潜在影响。

Method: 提供关于GenAI生成特性和概率性质的概述，分析其在科学计量学上的应用，包括主题标注、引用分析、预测、学者画像及评估研究。进一步探讨GenAI生成科学语言可能对科学计量分析的重要影响。

Result: GenAI在语言生成任务如主题标注中表现出色，但在需要语义稳定性、语用推理或结构化领域知识的任务中存在局限性。并且这些结果可能随着技术进步迅速变化。

Conclusion: 建议对特定任务系统比较不同GenAI模型性能，并通过实证工作和理论反思，理解知识生产演变下的科学计量学模式变化。

Abstract: The aim of this paper is to review the use of GenAI in scientometrics, and to
begin a debate on the broader implications for the field. First, we provide an
introduction on GenAI's generative and probabilistic nature as rooted in
distributional linguistics. And we relate this to the debate on the extent to
which GenAI might be able to mimic human 'reasoning'. Second, we leverage this
distinction for a critical engagement with recent experiments using GenAI in
scientometrics, including topic labelling, the analysis of citation contexts,
predictive applications, scholars' profiling, and research assessment. GenAI
shows promise in tasks where language generation dominates, such as labelling,
but faces limitations in tasks that require stable semantics, pragmatic
reasoning, or structured domain knowledge. However, these results might become
quickly outdated. Our recommendation is, therefore, to always strive to
systematically compare the performance of different GenAI models for specific
tasks. Third, we inquire whether, by generating large amounts of scientific
language, GenAI might have a fundamental impact on our field by affecting
textual characteristics used to measure science, such as authors, words, and
references. We argue that careful empirical work and theoretical reflection
will be essential to remain capable of interpreting the evolving patterns of
knowledge production.

</details>


### [130] [Many LLMs Are More Utilitarian Than One](https://arxiv.org/abs/2507.00814)
*Anita Keshmirian,Razan Baltaji,Babak Hemmatian,Hadi Asghari,Lav R. Varshney*

Main category: cs.CL

TL;DR: 研究探讨大型语言模型(LLMs)在群体合作和独立操作中的道德判断动态及其区别，发现群体中的模型更倾向于接受违反道德规范的行为以实现效用最大化。


<details>
  <summary>Details</summary>
Motivation: 探讨群体中LLMs的道德判断行为是否类似于人类，通过研究这种动态揭示其对AI对齐和多代理设计的影响。

Method: 对六种模型进行了关于道德困境的测试，包括独立推理和群体讨论两种情境，分析两种情景下的行为差异。

Result: 群体化操作的模型表现出比独立操作更高的对违反道德规范的容忍度，部分模型表现出对增加整体幸福的行为的偏好，而另一些则对违背道德规范的行为更加开放。

Conclusion: LLM在群体中的道德判断表现虽然表面上与人类相似，但其驱动机制不同，这对于AI对齐、多代理系统设计和伦理推理具有广泛意义。

Abstract: Moral judgment is integral to large language model (LLM) alignment and social
reasoning. As multi-agent systems gain prominence, it becomes crucial to
understand how LLMs function collectively during collaboration, compared to
individual agents. In human moral judgment, group deliberation leads to a
utilitarian boost: a tendency to endorse norm violations that maximize benefits
for the greatest number of people despite harms. We study whether a similar
dynamic emerges in multi-agent LLM systems. We tested six models on
well-established sets of moral dilemmas across two conditions: (1) Solo, where
models reasoned independently, and (2) Group, where they engaged in multi-turn
discussions in pairs or triads. In personal moral dilemmas, where agents must
decide to directly harm one individual to maximize the utility for others, all
models found moral violations to be more acceptable when part of a group than
individually, similar to human experiments. Some models endorsed actions that
maximized overall well-being, even if they benefited strangers over familiar
individuals. Others became more willing to violate moral norms in groups.
However, while human groups show a similar action bias, the mechanism for their
utilitarian boost differs from LLMs. Whereas the human shift comes from
heightened sensitivity to decision outcomes, LLM groups show either reduced
norm sensitivity or enhanced impartiality. This suggests that while the surface
behavior of LLM collectives mimics human group reasoning, the underlying
drivers differ. We discuss the implications for AI alignment, multi-agent
design, and artificial moral reasoning.

</details>


### [131] [ProxAnn: Use-Oriented Evaluations of Topic Models and Document Clustering](https://arxiv.org/abs/2507.00828)
*Alexander Hoyle,Lorena Calvo-Bartolomé,Jordan Boyd-Graber,Philip Resnik*

Main category: cs.CL

TL;DR: 本文提出一种基于人类实践的新评估协议，用于主题模型和文档聚类的评估，并验证了LLM代理的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法要么与人类偏好不符，要么难以扩展，为解决这一问题，研究提出一个可扩展的评估协议和对应的自动化近似方法。

Method: 设计了一个人类评估的流程，通过分析文本并推断类别；并验证大语言模型(LLM)代理与人工标注的一致性。

Result: 人类标注和LLM代理生成的结果在统计上无法区分，表明LLM代理可以用来替代人工进行自动化评估。

Conclusion: 本文的评估协议和LLM代理能有效反映真实世界中的模型使用情境，为主题模型和文档聚类的自动化评估提供可靠工具。

Abstract: Topic model and document-clustering evaluations either use automated metrics
that align poorly with human preferences or require expert labels that are
intractable to scale. We design a scalable human evaluation protocol and a
corresponding automated approximation that reflect practitioners' real-world
usage of models. Annotators -- or an LLM-based proxy -- review text items
assigned to a topic or cluster, infer a category for the group, then apply that
category to other documents. Using this protocol, we collect extensive
crowdworker annotations of outputs from a diverse set of topic models on two
datasets. We then use these annotations to validate automated proxies, finding
that the best LLM proxies are statistically indistinguishable from a human
annotator and can therefore serve as a reasonable substitute in automated
evaluations. Package, web interface, and data are at
https://github.com/ahoho/proxann

</details>


### [132] [Stylometry recognizes human and LLM-generated texts in short samples](https://arxiv.org/abs/2507.00838)
*Karol Przystalski,Jan K. Argasiński,Iwona Grabska-Gradzińska,Jeremi K. Ochab*

Main category: cs.CL

TL;DR: 该论文探讨了使用文体学方法区分由大型语言模型（LLMs）与人类创作的文本。研究表明，在特定文本类别中，通过分析词汇、语法、句法和标点特征，树模型能够高效区分人类和机器生成的内容。


<details>
  <summary>Details</summary>
Motivation: 动机是解决文本归属、知识产权和人工智能伦理问题，通过探讨如何区分由大型语言模型生成的文本与人类创作的文本。

Method: 作者创建了一个基准数据集，包含人类撰写及不同LLM（如GPT-3.5/4，LLaMa，Orca，Falcon）生成的文本，使用多个文本压缩和改写方法及文体特征分析，借助决策树和LightGBM分类模型进行分类。

Result: 研究结果达到了二分类准确率0.79-1.0（如GPT-4与人类文本在平衡数据集上精确度为0.98），在多分类场景中达到0.87的Matthews相关系数。分析还显示LLMs生成的文本在语法上更规范，过度使用某些词汇。

Conclusion: 结果表明，即使LLM变得更加复杂，也能通过文体特征区分机器生成与人类创造的文本文本，尤其是在特定文本类型中。

Abstract: The paper explores stylometry as a method to distinguish between texts
created by Large Language Models (LLMs) and humans, addressing issues of model
attribution, intellectual property, and ethical AI use. Stylometry has been
used extensively to characterise the style and attribute authorship of texts.
By applying it to LLM-generated texts, we identify their emergent writing
patterns. The paper involves creating a benchmark dataset based on Wikipedia,
with (a) human-written term summaries, (b) texts generated purely by LLMs
(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text
summarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods
(Dipper, T5). The 10-sentence long texts were classified by tree-based models
(decision trees and LightGBM) using human-designed (StyloMetrix) and
n-gram-based (our own pipeline) stylometric features that encode lexical,
grammatical, syntactic, and punctuation patterns. The cross-validated results
reached a performance of up to .87 Matthews correlation coefficient in the
multiclass scenario with 7 classes, and accuracy between .79 and 1. in binary
classification, with the particular example of Wikipedia and GPT-4 reaching up
to .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed
features characteristic of the encyclopaedic text type, individual overused
words, as well as a greater grammatical standardisation of LLMs with respect to
human-written texts. These results show -- crucially, in the context of the
increasingly sophisticated LLMs -- that it is possible to distinguish machine-
from human-generated texts at least for a well-defined text type.

</details>


### [133] [TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation](https://arxiv.org/abs/2507.00875)
*Xi Xuan,King-kui Sin,Yufei Zhou,Chunyu Kit*

Main category: cs.CL

TL;DR: 本文提出TransLaw，一个多智能体框架用于解决香港法律判决翻译的问题。通过Translator, Annotator, 和 Proofreader 三个专用代理的协作，大幅提升翻译的法律语义准确性、风格适切性和结构连贯性，并显著降低翻译成本。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在解决复杂法律领域翻译问题的潜力，并应对香港法律判决翻译中的法律术语、文化内容和语言结构的挑战。

Method: 引入TransLaw框架，包括三个专用代理（Translator, Annotator, Proofreader），协作完成翻译任务，同时支持LLMs的可定制化配置。评估通过13种LLMs和对标人类专家的表现。

Result: TransLaw框架在法律语义准确性、结构连贯性及风格保真度方面优于GPT-4o，但在复杂术语背景化和自然度上仍略逊于人类专家。

Conclusion: TransLaw具备显著的实用性和经济性，虽然在某些细节处理上还需要依赖人类专家，但其表现展示了LLMs在法律翻译领域的巨大潜力，并提供了一套高效、低成本的翻译解决方案。

Abstract: Multi-agent systems empowered by large language models (LLMs) have
demonstrated remarkable capabilities in a wide range of downstream
applications, including machine translation. However, the potential of LLMs in
translating Hong Kong legal judgments remains uncertain due to challenges such
as intricate legal terminology, culturally embedded nuances, and strict
linguistic structures. In this work, we introduce TransLaw, a novel multi-agent
framework implemented for real-world Hong Kong case law translation. It employs
three specialized agents, namely, Translator, Annotator, and Proofreader, to
collaboratively produce translations for high accuracy in legal meaning,
appropriateness in style, and adequate coherence and cohesion in structure.
This framework supports customizable LLM configurations and achieves tremendous
cost reduction compared to professional human translation services. We
evaluated its performance using 13 open-source and commercial LLMs as agents
and obtained interesting findings, including that it surpasses GPT-4o in legal
semantic accuracy, structural coherence, and stylistic fidelity, yet trails
human experts in contextualizing complex terminology and stylistic naturalness.
Our platform website is available at CityUHK, and our bilingual judgment corpus
used for the evaluation is available at Hugging Face.

</details>


### [134] [Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and Scenario Perturbations](https://arxiv.org/abs/2507.00883)
*Aditya Tomar,Nihar Ranjan Sahoo,Ashish Mittal,Rudra Murthy,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本文开发了GSM8K的文化适配版本，以评估数学问题在不同文化背景下的表现差异，并发现模型在文化差异显著时表现较差，尤其是非推理模型。


<details>
  <summary>Details</summary>
Motivation: 探讨数学问题在不同文化背景中的表现，评估现有数学测试数据集的文化普适性。

Method: 通过提示转化和人工验证生成适合五个地区文化背景的GSM8K测试数据，分析六种大模型在不同提示策略下的表现。

Result: 模型在原始西方基准上表现最佳，而在文化适配版本上的表现较差。具备推理能力的模型在适应文化变体时表现更具鲁棒性。

Conclusion: 深度推理能力有助于大语言模型在处理数学任务时克服文化表达差异，现有基准仍需增强文化多样性。

Abstract: Although mathematics is often considered culturally neutral, the way
mathematical problems are presented can carry implicit cultural context.
Existing benchmarks like GSM8K are predominantly rooted in Western norms,
including names, currencies, and everyday scenarios. In this work, we create
culturally adapted variants of the GSM8K test set for five regions Africa,
India, China, Korea, and Japan using prompt-based transformations followed by
manual verification. We evaluate six large language models (LLMs), ranging from
8B to 72B parameters, across five prompting strategies to assess their
robustness to cultural variation in math problem presentation. Our findings
reveal a consistent performance gap: models perform best on the original
US-centric dataset and comparatively worse on culturally adapted versions.
However, models with reasoning capabilities are more resilient to these shifts,
suggesting that deeper reasoning helps bridge cultural presentation gaps in
mathematical tasks

</details>


### [135] [Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check](https://arxiv.org/abs/2507.00885)
*Nicholas Lourie,Michael Y. Hu,Kyunghyun Cho*

Main category: cs.CL

TL;DR: 该研究分析了上下游缩放规律，并发现仅在约39%的情况下表现出接近线性缩放趋势。


<details>
  <summary>Details</summary>
Motivation: 试图探索预训练损失和下游任务性能关系随规模变化的预测准确性。

Method: 对现有的数据进行元分析，分析在不同实验条件下的缩放趋势变化。

Result: 发现仅在少数情况下，性能与缩放规律呈线性关系（约39%），且实验设置的变化可显著影响缩放趋势。

Conclusion: 需研究缩放规律成功的条件，并接受线性趋势偏离的情况以全面建模预训练与任务性能的关系。

Abstract: Downstream scaling laws aim to predict task performance at larger scales from
pretraining losses at smaller scales. Whether this prediction should be
possible is unclear: some works demonstrate that task performance follows clear
linear scaling trends under transformation, whereas others point out
fundamental challenges to downstream scaling laws, such as emergence and
inverse scaling. In this work, we conduct a meta-analysis of existing data on
downstream scaling laws, finding that close fit to linear scaling laws only
occurs in a minority of cases: 39% of the time. Furthermore, seemingly benign
changes to the experimental setting can completely change the scaling trend.
Our analysis underscores the need to understand the conditions under which
scaling laws succeed. To fully model the relationship between pretraining loss
and downstream task performance, we must embrace the cases in which scaling
behavior deviates from linear trends.

</details>


### [136] [MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes](https://arxiv.org/abs/2507.00891)
*Yuheng Wang,Xianhe Tang,Pufeng Huang*

Main category: cs.CL

TL;DR: 本文提出了一个名为MemeCMD的多回合对话数据集，通过自动生成方法结合情境检索表情包来提升对话的多模态表现力。


<details>
  <summary>Details</summary>
Motivation: 现有对话数据集主要是纯文本或人工标注对话，缺乏多模态交互的表现力和上下文细致性。提出新的数据集以解决这一问题。

Method: 结合大规模多模态模型标注的表情包库，设计了一个检索框架和自适应阈值，再利用双代理生成多场景对话。

Result: 实验表明，该框架能有效生成情境相关性高且多样性丰富的表情包互动对话。

Conclusion: MemeCMD数据集为提升多模态对话AI提供了可扩展且保护隐私的资源。

Abstract: Memes are widely used in online social interactions, providing vivid,
intuitive, and often humorous means to express intentions and emotions.
Existing dialogue datasets are predominantly limited to either manually
annotated or pure-text conversations, lacking the expressiveness and contextual
nuance that multimodal interactions provide.To address these challenges, we
introduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue
dataset with contextually retrieved memes. Our dataset combines a large-scale,
MLLM-annotated meme library with dialogues auto-generated by dual agents across
diverse scenarios. We introduce a retrieval framework and adaptive threshold to
ensure contextually relevant, naturally spaced meme usage. Experiments
demonstrate the effectiveness of our approach in generating contextually
appropriate and diverse meme-incorporated dialogues, offering a scalable and
privacy-preserving resource for advancing multimodal conversational AI.

</details>


### [137] [The Cognate Data Bottleneck in Language Phylogenetics](https://arxiv.org/abs/2507.00911)
*Luise Häuser,Alexandros Stamatakis*

Main category: cs.CL

TL;DR: 本文探讨了计算系统发育方法在同源数据应用中的局限性，指出由于数据集不足，当前方法和资源难以生成准确的大数据集用于分析。


<details>
  <summary>Details</summary>
Motivation: 利用计算系统发育方法和机器学习技术挖掘同源数据潜力，但由于现有手动收集的同源数据集规模较小，阻碍了其实际应用。

Method: 通过自动从BabelNet中提取数据集，检验其生成的系统发育树与公认的基准真值树间的一致性。同时探讨了从其他多语言资源提取更多适合数据的可能性。

Result: 实验发现，提取的特征矩阵生成的系统发育树与标准真值树存在显著不一致，说明目前的方法无法从多语言资源中提取足够好的数据。

Conclusion: 现有需要大规模数据集的系统发育分析方法难以用于同源数据，如何将这些方法用于历史语言学仍然是一个开放问题。

Abstract: To fully exploit the potential of computational phylogenetic methods for
cognate data one needs to leverage specific (complex) models an machine
learning-based techniques. However, both approaches require datasets that are
substantially larger than the manually collected cognate data currently
available. To the best of our knowledge, there exists no feasible approach to
automatically generate larger cognate datasets. We substantiate this claim by
automatically extracting datasets from BabelNet, a large multilingual
encyclopedic dictionary. We demonstrate that phylogenetic inferences on the
respective character matrices yield trees that are largely inconsistent with
the established gold standard ground truth trees. We also discuss why we
consider it as being unlikely to be able to extract more suitable character
matrices from other multilingual resources. Phylogenetic data analysis
approaches that require larger datasets can therefore not be applied to cognate
data. Thus, it remains an open question how, and if these computational
approaches can be applied in historical linguistics.

</details>


### [138] [Discourse Heuristics For Paradoxically Moral Self-Correction](https://arxiv.org/abs/2507.00985)
*Guangliang Liu,Zimo Qi,Xitong Zhang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLMs）在进行道德自我纠正能力时面临的两个主要悖论，并提出了通过数据集中的启发式方法来改进这一能力的方案。


<details>
  <summary>Details</summary>
Motivation: 研究旨在分析和解决LLMs在道德自我纠正中效率与深度不足的矛盾，以及其在自我诊断中无法确定道德不一致原因的问题。

Method: 通过分析微调语料中的话语结构，揭示启发式框架的存在，并提出利用高质量剔选数据集中的启发式方法来改进道德自我纠正能力。

Result: 发现道德自我纠正依赖于反映启发式捷径的话语结构，而这些捷径会导致在增强自我纠正与自我诊断能力时出现矛盾和不一致现象。

Conclusion: 通过利用特定设计的数据集中的启发式框架，可提升LLMs的道德自我纠正能力，但这一过程面临从情境化语境与模型扩展中学习的普适性挑战。

Abstract: Moral self-correction has emerged as a promising approach for aligning the
output of Large Language Models (LLMs) with human moral values. However, moral
self-correction techniques are subject to two primary paradoxes. First, despite
empirical and theoretical evidence to support the effectiveness of
self-correction, this LLM capability only operates at a superficial level.
Second, while LLMs possess the capability of self-diagnosing immoral aspects of
their output, they struggle to identify the cause of this moral inconsistency
during their self-correction process. To better understand and address these
paradoxes, we analyze the discourse constructions in fine-tuning corpora
designed to enhance moral self-correction, uncovering the existence of the
heuristics underlying effective constructions. We demonstrate that moral
self-correction relies on discourse constructions that reflect heuristic
shortcuts, and that the presence of these heuristic shortcuts during
self-correction leads to inconsistency when attempting to enhance both
self-correction and self-diagnosis capabilities jointly. Based on our findings,
we propose a solution to improve moral self-correction by leveraging the
heuristics of curated datasets. We also highlight the generalization challenges
of this capability, particularly in terms of learning from situated context and
model scales.

</details>


### [139] [Should We Still Pretrain Encoders with Masked Language Modeling?](https://arxiv.org/abs/2507.00994)
*Hippolyte Gisserot-Boukhlef,Nicolas Boizard,Manuel Faysse,Duarte M. Alves,Emmanuel Malherbe,André F. T. Martins,Céline Hudelot,Pierre Colombo*

Main category: cs.CL

TL;DR: 该论文探讨使用MLM和CLM目标预训练的模型在文本表征任务中的表现，并提出一种两阶段训练策略，结合CLM和MLM的优势。


<details>
  <summary>Details</summary>
Motivation: 探究为何基于CLM的预训练模型在文本表征任务中表现更优，是因其方法本质优越，还是受模型和数据规模等混杂因素驱动。

Method: 进行大规模预训练消融实验，训练30个模型（参数从2.1亿到10亿不等），并完成超过15,000次微调和评估运行。提出一种分阶段的预训练策略，先用CLM然后用MLM。

Result: 发现在文本表示任务中，MLM模型表现更优，但CLM模型数据效率更高且微调稳定性更好。两阶段训练策略能在固定计算预算下实现最佳性能。

Conclusion: 两阶段策略整合CLM和MLM的优点，为更高效训练最佳编码模型提供新路径，并适合现有CLM模型的初始化，减轻计算负担。

Abstract: Learning high-quality text representations is fundamental to a wide range of
NLP tasks. While encoder pretraining has traditionally relied on Masked
Language Modeling (MLM), recent evidence suggests that decoder models
pretrained with Causal Language Modeling (CLM) can be effectively repurposed as
encoders, often surpassing traditional encoders on text representation
benchmarks. However, it remains unclear whether these gains reflect an inherent
advantage of the CLM objective or arise from confounding factors such as model
and data scale. In this paper, we address this question through a series of
large-scale, carefully controlled pretraining ablations, training a total of 30
models ranging from 210 million to 1 billion parameters, and conducting over
15,000 fine-tuning and evaluation runs. We find that while training with MLM
generally yields better performance across text representation tasks,
CLM-trained models are more data-efficient and demonstrate improved fine-tuning
stability. Building on these findings, we experimentally show that a biphasic
training strategy that sequentially applies CLM and then MLM, achieves optimal
performance under a fixed computational training budget. Moreover, we
demonstrate that this strategy becomes more appealing when initializing from
readily available pretrained CLM models (from the existing LLM ecosystem),
reducing the computational burden needed to train best-in-class encoder models.
We release all project artifacts at https://hf.co/MLMvsCLM to foster further
research.

</details>


### [140] [La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America](https://arxiv.org/abs/2507.00999)
*María Grandury,Javier Aula-Blasco,Júlia Falcão,Clémentine Fourrier,Miguel González,Gonzalo Martínez,Gonzalo Santamaría,Rodrigo Agerri,Nuria Aldama,Luis Chiruzzo,Javier Conde,Helena Gómez,Marta Guerrero,Guido Ivetta,Natalia López,Flor Miriam Plaza-del-Arco,María Teresa Martín-Valdivia,Helena Montoro,Carmen Muñoz,Pedro Reviriego,Leire Rosado,Alejandro Vaca,María Estrella Vallecillo-Rodríguez,Jorge Vallego,Irune Zubiaga*

Main category: cs.CL

TL;DR: 本论文提出了一个名为 La Leaderboard 的开源排行榜，用于评估西班牙语及其方言的生成式大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 为了推动西班牙语社区的语言和文化多样性的LLM开发，作者希望建立一个统一的评价标准。

Method: 通过收集66个数据集，包括巴斯克语、加泰罗尼亚语、加利西亚语及不同的西班牙语方言，并对50个模型进行全面评估，同时提出了减少few-shot样本的方法以减轻环境影响并提高研究普遍性。

Result: La Leaderboard 成功整合了语言多样性评估，并提供了50个模型的性能结果，促进了社区对LLM开发的关注。

Conclusion: 此研究为西班牙语及其各种方言的生成模型论证了评价标准，为类似应用于其他语言的开发提供了参考与思路。

Abstract: Leaderboards showcase the current capabilities and limitations of Large
Language Models (LLMs). To motivate the development of LLMs that represent the
linguistic and cultural diversity of the Spanish-speaking community, we present
La Leaderboard, the first open-source leaderboard to evaluate generative LLMs
in languages and language varieties of Spain and Latin America. La Leaderboard
is a community-driven project that aims to establish an evaluation standard for
everyone interested in developing LLMs for the Spanish-speaking community. This
initial version combines 66 datasets in Basque, Catalan, Galician, and
different Spanish varieties, showcasing the evaluation results of 50 models. To
encourage community-driven development of leaderboards in other languages, we
explain our methodology, including guidance on selecting the most suitable
evaluation setup for each downstream task. In particular, we provide a
rationale for using fewer few-shot examples than typically found in the
literature, aiming to reduce environmental impact and facilitate access to
reproducible results for a broader research community.

</details>


### [141] [SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks](https://arxiv.org/abs/2507.01001)
*Yilun Zhao,Kaiyan Zhang,Tiansheng Hu,Sihong Wu,Ronan Le Bras,Taira Anderson,Jonathan Bragg,Joseph Chee Chang,Jesse Dodge,Matt Latzke,Yixin Liu,Charles McGrady,Xiangru Tang,Zihang Wang,Chen Zhao,Hannaneh Hajishirzi,Doug Downey,Arman Cohan*

Main category: cs.CL

TL;DR: SciArena 是一个开放的合作平台，用于科学文献任务中基础模型的评估，采用社区投票模式。


<details>
  <summary>Details</summary>
Motivation: 填补科学文献任务评估的现有空白，利用社区智慧进行开放性评估。

Method: 建立了一个面向样本问答与模型对比的投票平台，并发布了模型的元评估基准。

Result: 平台支持23种模型，收集了研究者投票数据，并验证了其评估的多样性和一致性。

Conclusion: SciArena 提供了新的科学文献评估方式，并推动自动化评估方法的研究。

Abstract: We present SciArena, an open and collaborative platform for evaluating
foundation models on scientific literature tasks. Unlike traditional benchmarks
for scientific literature understanding and synthesis, SciArena engages the
research community directly, following the Chatbot Arena evaluation approach of
community voting on model comparisons. By leveraging collective intelligence,
SciArena offers a community-driven evaluation of model performance on
open-ended scientific tasks that demand literature-grounded, long-form
responses. The platform currently supports 23 open-source and proprietary
foundation models and has collected over 13,000 votes from trusted researchers
across diverse scientific domains. We analyze the data collected so far and
confirm that the submitted questions are diverse, aligned with real-world
literature needs, and that participating researchers demonstrate strong
self-consistency and inter-annotator agreement in their evaluations. We discuss
the results and insights based on the model ranking leaderboard. To further
promote research in building model-based automated evaluation systems for
literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based
on our collected preference data. The benchmark measures the accuracy of models
in judging answer quality by comparing their pairwise assessments with human
votes. Our experiments highlight the benchmark's challenges and emphasize the
need for more reliable automated evaluation methods.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [142] [DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning](https://arxiv.org/abs/2507.00008)
*Hang Wu,Hongkai Chen,Yujun Cai,Chang Liu,Qingwen Ye,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.AI

TL;DR: 本文提出了一种无训练框架DiMo-GUI，用于在GUI中识别自然语言查询，采用独特的动态视觉定位和模态优化策略。


<details>
  <summary>Details</summary>
Motivation: 由于GUI中视觉元素多样性、空间混乱和语言模糊性，传统方法难以有效实现自然语言与GUI的对接。

Method: 方法将GUI分为文本元素和图标元素，通过通用视觉-语言模型分别推理，对模态分离处理。采用分层细化策略，通过动态关注候选重点区域与逐步放大子区域，改善定位结果，无需额外训练或标注。

Result: 在标准GUI基准测试中，DiMo-GUI显著优于基线推理管道，证明了其模态分离结合区域聚焦推理的有效性。

Conclusion: DiMo-GUI通过动态视觉定位和模态优化，克服了GUI定位的棘手难题，为领域理解与相关应用提供了新方向。

Abstract: Grounding natural language queries in graphical user interfaces (GUIs) poses
unique challenges due to the diversity of visual elements, spatial clutter, and
the ambiguity of language. In this paper, we introduce DiMo-GUI, a
training-free framework for GUI grounding that leverages two core strategies:
dynamic visual grounding and modality-aware optimization. Instead of treating
the GUI as a monolithic image, our method splits the input into textual
elements and iconic elements, allowing the model to reason over each modality
independently using general-purpose vision-language models. When predictions
are ambiguous or incorrect, DiMo-GUI dynamically focuses attention by
generating candidate focal regions centered on the model's initial predictions
and incrementally zooms into subregions to refine the grounding result. This
hierarchical refinement process helps disambiguate visually crowded layouts
without the need for additional training or annotations. We evaluate our
approach on standard GUI grounding benchmarks and demonstrate consistent
improvements over baseline inference pipelines, highlighting the effectiveness
of combining modality separation with region-focused reasoning.

</details>


### [143] [TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables](https://arxiv.org/abs/2507.00041)
*Varun Mannam,Fang Wang,Chaochun Liu,Xin Chen*

Main category: cs.AI

TL;DR: 本文提出TalentMine框架，解决传统工具在处理复杂表格数据时语义理解不足的问题，显著提升查询任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的表格提取方法难以保留语义关系，导致信息检索和决策失误。

Method: 提出一种基于LLM的方法，通过多模态推理将提取的表格转化为语义丰富的表示形式，优化检索增强型系统的流程。

Result: 实验表明，TalentMine在查询任务中达到100%的准确率，远超AWS Textract的表现。

Conclusion: 通过系统分析现有方法的语义损失问题，创新性引入了语义增强的表格表示方法，验证了其在人才管理领域应用中的显著优势。

Abstract: In talent management systems, critical information often resides in complex
tabular formats, presenting significant retrieval challenges for conventional
language models. These challenges are pronounced when processing Talent
documentation that requires precise interpretation of tabular relationships for
accurate information retrieval and downstream decision-making. Current table
extraction methods struggle with semantic understanding, resulting in poor
performance when integrated into retrieval-augmented chat applications. This
paper identifies a key bottleneck - while structural table information can be
extracted, the semantic relationships between tabular elements are lost,
causing downstream query failures. To address this, we introduce TalentMine, a
novel LLM-enhanced framework that transforms extracted tables into semantically
enriched representations. Unlike conventional approaches relying on CSV or text
linearization, our method employs specialized multimodal reasoning to preserve
both structural and semantic dimensions of tabular data. Experimental
evaluation across employee benefits document collections demonstrates
TalentMine's superior performance, achieving 100% accuracy in query answering
tasks compared to 0% for standard AWS Textract extraction and 40% for AWS
Textract Visual Q&A capabilities. Our comparative analysis also reveals that
the Claude v3 Haiku model achieves optimal performance for talent management
applications. The key contributions of this work include (1) a systematic
analysis of semantic information loss in current table extraction pipelines,
(2) a novel LLM-based method for semantically enriched table representation,
(3) an efficient integration framework for retrieval-augmented systems as
end-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks
showing substantial improvements across multiple categories.

</details>


### [144] [A collaborative digital twin built on FAIR data and compute infrastructure](https://arxiv.org/abs/2507.00048)
*Thomas M. Deucher,Juan C. Verduzco,Michael Titus,Alejandro Strachan*

Main category: cs.AI

TL;DR: 本文提出了一个基于nanoHUB服务的分布式自动化实验室（SDL）实现，引入了使用FAIR数据管理框架协作开展优化任务的工具。


<details>
  <summary>Details</summary>
Motivation: 通过将机器学习与自动化实验相结合，提升在科学与工程领域中发现和优化任务的效率，并通过FAIR数据基础设施促进研究人员的协作。

Method: 框架采用nanoHUB服务，包含共享中央数据库、在线仿真和结果管理工具。实验数据通过简单的网络接口上传并经过Sim2L处理。在名为ResultsDB的FAIR数据仓库中索引所有输入输出。其中特色之一是以“节俭双胞胎”概念为灵感，使用主动学习进行连续优化。

Result: 研究人员能够使用此系统实时更新机器学习模型并指导后续实验设计，实验可独立执行并通过FAIR机制共享数据。通过工具解决了模型优化中的跨地协作和数据管理问题。

Conclusion: 提出的工具在提升研究效率并降低实验设置复杂性方面表现出广泛适用性，并可以扩展到其他优化问题。

Abstract: The integration of machine learning with automated experimentation in
self-driving laboratories (SDL) offers a powerful approach to accelerate
discovery and optimization tasks in science and engineering applications. When
supported by findable, accessible, interoperable, and reusable (FAIR) data
infrastructure, SDLs with overlapping interests can collaborate more
effectively. This work presents a distributed SDL implementation built on
nanoHUB services for online simulation and FAIR data management. In this
framework, geographically dispersed collaborators conducting independent
optimization tasks contribute raw experimental data to a shared central
database. These researchers can then benefit from analysis tools and machine
learning models that automatically update as additional data become available.
New data points are submitted through a simple web interface and automatically
processed using a nanoHUB Sim2L, which extracts derived quantities and indexes
all inputs and outputs in a FAIR data repository called ResultsDB. A separate
nanoHUB workflow enables sequential optimization using active learning, where
researchers define the optimization objective, and machine learning models are
trained on-the-fly with all existing data, guiding the selection of future
experiments. Inspired by the concept of ``frugal twin", the optimization task
seeks to find the optimal recipe to combine food dyes to achieve the desired
target color. With easily accessible and inexpensive materials, researchers and
students can set up their own experiments, share data with collaborators, and
explore the combination of FAIR data, predictive ML models, and sequential
optimization. The tools introduced are generally applicable and can easily be
extended to other optimization problems.

</details>


### [145] [SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network](https://arxiv.org/abs/2507.00050)
*Devin Y. De Silva,Sandareka Wickramanayake,Dulani Meedeniya,Sanka Rasnayaka*

Main category: cs.AI

TL;DR: 文章提出了一种创新的零样本人体活动识别模型（SEZ-HARN），它不仅能够识别未在训练中遇到的活动，也能利用骨架视频解释其决策过程，并在四个基准数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决当前IMU传感器数据基础的人体活动识别（HAR）面临的数据量不足及现有模型缺乏透明度的问题。

Method: 开发了一个名为SEZ-HARN的自解释零样本人体活动识别网络，该模型可以生成骨架视频解释决策，并在多个基准数据集上与现有方法进行对比测试。

Result: SEZ-HARN在实验中展现出了真实且易理解的解释能力，并在PAMAP2数据集上的零样本预测精度仅比最佳黑箱模型相差3%，在其他数据集上表现也具竞争力。

Conclusion: SEZ-HARN实现了在零样本环境中识别新活动的同时还提供清晰的解释，且性能接近主流黑箱模型，展示了在实际应用中的潜力。

Abstract: Human Activity Recognition (HAR), which uses data from Inertial Measurement
Unit (IMU) sensors, has many practical applications in healthcare and assisted
living environments. However, its use in real-world scenarios has been limited
by the lack of comprehensive IMU-based HAR datasets that cover a wide range of
activities and the lack of transparency in existing HAR models. Zero-shot HAR
(ZS-HAR) overcomes the data limitations, but current models struggle to explain
their decisions, making them less transparent. This paper introduces a novel
IMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity
Recognition Network (SEZ-HARN). It can recognize activities not encountered
during training and provide skeleton videos to explain its decision-making
process. We evaluate the effectiveness of the proposed SEZ-HARN on four
benchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its
performance against three state-of-the-art black-box ZS-HAR models. The
experiment results demonstrate that SEZ-HARN produces realistic and
understandable explanations while achieving competitive Zero-shot recognition
accuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\% of the
best-performing black-box model on PAMAP2 while maintaining comparable
performance on the other three datasets.

</details>


### [146] [Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation](https://arxiv.org/abs/2507.00054)
*Shreyansh Padarha*

Main category: cs.AI

TL;DR: 提出了一种名为AdvDistill的奖励引导数据集蒸馏框架，用于通过奖励机制改进学生模型的表现，特别是在数学和复杂推理任务上。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法让小模型仅复制大模型的输出，限制了其泛化性，特别是在推理任务中耗时且限制较大。

Method: 提出AdvDistill框架，利用基于规则的验证器对教师模型的多重生成进行奖励分配，再基于这些奖励分布训练学生模型。

Result: 通过AdvDistill框架，学生模型在数学和复杂推理方面的性能显著提升。

Conclusion: 引入奖励机制的数据集蒸馏方法可以有效提升学生模型的推理能力和性能。

Abstract: The push to compress and impart the proficiency of Large Language Models
(LLMs) into more deployable and efficient Small Language Models (SLMs) has
benefited from improvements in knowledge distillation (KD) techniques. These
techniques allow a smaller student model to learn from a more capable and
larger teacher model's responses. However, distillation often revolves around
the student model merely copying the teacher's in-distribution responses,
limiting its generalisability. This limitation is amplified on reasoning tasks
and can be computationally expensive. In this study, we propose AdvDistill, a
reward-guided dataset distillation framework. We utilise multiple generations
(responses) from a teacher for each prompt and assign rewards based on
rule-based verifiers. These varying and normally distributed rewards serve as
weights when training student models. Our methods and their subsequent
behavioural analysis demonstrate a significant improvement in student model
performance for mathematical and complex reasoning tasks, showcasing the
efficacy and benefits of incorporating a rewarding mechanism in dataset
distillation processes.

</details>


### [147] [VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems](https://arxiv.org/abs/2507.00079)
*Ethan Smyth,Alessandro Suglia*

Main category: cs.AI

TL;DR: 本文提出了VoyagerVision模型，它是一种多模态模型，能够通过视觉反馈在Minecraft中创建结构，因而增强任务能力和开放性。


<details>
  <summary>Details</summary>
Motivation: 探索通过提供视觉输入给模型以增强其理解空间环境和扩展任务执行能力。

Method: 开发了VoyagerVision多模态模型，该模型基于图像反馈进行任务，尤其是通过Minecraft游戏场景中的截图提供视觉数据。

Result: VoyagerVision在50次迭代中平均创造了2.75个独特结构，并在建筑单元测试中在简单环境中成功率为50%。

Conclusion: VoyagerVision显著提高了任务完成能力，展示了多模态模型在增强开放能力方向上的潜力。

Abstract: Open-endedness is an active field of research in the pursuit of capable
Artificial General Intelligence (AGI), allowing models to pursue tasks of their
own choosing. Simultaneously, recent advancements in Large Language Models
(LLMs) such as GPT-4o [9] have allowed such models to be capable of
interpreting image inputs. Implementations such as OMNI-EPIC [4] have made use
of such features, providing an LLM with pixel data of an agent's POV to parse
the environment and allow it to solve tasks. This paper proposes that providing
these visual inputs to a model gives it greater ability to interpret spatial
environments, and as such, can increase the number of tasks it can successfully
perform, extending its open-ended potential. To this aim, this paper proposes
VoyagerVision -- a multi-modal model capable of creating structures within
Minecraft using screenshots as a form of visual feedback, building on the
foundation of Voyager. VoyagerVision was capable of creating an average of 2.75
unique structures within fifty iterations of the system, as Voyager was
incapable of this, it is an extension in an entirely new direction.
Additionally, in a set of building unit tests VoyagerVision was successful in
half of all attempts in flat worlds, with most failures arising in more complex
structures. Project website is available at
https://esmyth-dev.github.io/VoyagerVision.github.io/

</details>


### [148] [Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models](https://arxiv.org/abs/2507.00092)
*Basab Jha,Firoj Paudel,Ujjwal Puri,Zhang Yuting,Choi Donghyuk,Wang Junhao*

Main category: cs.AI

TL;DR: 本文提出逆向推理是一种新范式，能让大语言模型解释其推理路径。采用元认知结构反向识别关键决策点，从而增强推理透明性和准确性，与现有的大型模型性能相当。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的推理过程当前较为黑箱，缺乏解释能力。通过逆向推理可以更好地理解模型如何选择推理路径，提高小型模型的能力及透明性。

Method: 提出SAGE-nano模型，将推理过程引入逆向反思，基于反向注意力流进行自我解释。同时设计全面的评估框架来验证解释性和推理精度。

Result: SAGE-nano在逻辑推理、数学问题和伦理困境上表现优秀，推理准确度达74.6%，解释质量达92.1%。性能接近Claude等顶级模型。

Conclusion: 逆向推理结合元学习框架提升了小型模型的性能和解释性，为AI系统透明性及AI安全等带来新的可能性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities at
solving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but
their decision-making processes remain somewhat blackbox. We introduce
textbfinverse reasoning, a novel paradigm enabling LLMs to decompose and
explain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a
4-billion-parameter reasoning model, employs a metacognitive structure that
reflects back via attention processes to identify major decision points and
generate explanations of reasoning choices. While typical CoT approaches are
directed towards forward reasoning generation, inverse reasoning provides
insight into why specific reasoning chains were selected over others. Through
thorough testing of logical reasoning puzzles, math problems and ethical
dilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we
demonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy
(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for
its task, and offers performance almost on par with models like Claude-3.5
Sonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for
LLM self-reflection via inverse reasoning, (ii) a novel metalearning framework
to reverse the attention flow, (iii) comprehensive evaluation frameworks for
reasoning transparency, and (iv) evidence that increasing reasoning using
inverse reasoning improves interpretability along with reasoning performance.
Our work creates new avenues for transparent AI systems and closes significant
gaps in AI safety, education, and scientific discovery.

</details>


### [149] [BlackBoxToBlueprint: Extracting Interpretable Logic from Legacy Systems using Reinforcement Learning and Counterfactual Analysis](https://arxiv.org/abs/2507.00180)
*Vidhi Rathore*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习和决策树的新方法，用于从黑盒遗留系统中提取可解释的决策逻辑。


<details>
  <summary>Details</summary>
Motivation: 遗留软件系统的现代化因缺乏文档和原始系统复杂的决策逻辑而面临挑战，传统行为克隆方法无法捕捉系统背后的意图。

Method: 通过强化学习代理探索输入空间并鉴别关键决策边界，将导致输出显著变化的反事实状态过渡进行收集、聚类（使用K-Means），再用决策树提取可解释的决策逻辑。

Result: 在三个不同复杂度的模拟遗留系统中测试，强化学习能够聚焦关键边界并生成准确反映系统核心逻辑的规则。

Conclusion: 该方法为遗留系统迁移中生成规范和测试用例提供了一个有希望的基础。

Abstract: Modernizing legacy software systems is a critical but challenging task, often
hampered by a lack of documentation and understanding of the original system's
intricate decision logic. Traditional approaches like behavioral cloning merely
replicate input-output behavior without capturing the underlying intent. This
paper proposes a novel pipeline to automatically extract interpretable decision
logic from legacy systems treated as black boxes. The approach uses a
Reinforcement Learning (RL) agent to explore the input space and identify
critical decision boundaries by rewarding actions that cause meaningful changes
in the system's output. These counterfactual state transitions, where the
output changes, are collected and clustered using K-Means. Decision trees are
then trained on these clusters to extract human-readable rules that approximate
the system's decision logic near the identified boundaries. I demonstrated the
pipeline's effectiveness on three dummy legacy systems with varying complexity,
including threshold-based, combined-conditional, and non-linear range logic.
Results show that the RL agent successfully focuses exploration on relevant
boundary regions, and the extracted rules accurately reflect the core logic of
the underlying dummy systems, providing a promising foundation for generating
specifications and test cases during legacy migration.

</details>


### [150] [ChatGPT produces more "lazy" thinkers: Evidence of cognitive engagement decline](https://arxiv.org/abs/2507.00181)
*Georgios P. Georgiou*

Main category: cs.AI

TL;DR: 研究探讨了生成型AI（如ChatGPT）在学术写作任务中对学生认知参与的影响。结果表明，使用ChatGPT会降低认知参与水平。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型应用于教育领域，其对深度思考和主动学习的潜在负面影响引发担忧。

Method: 采用实验设计，参与者随机分为AI辅助组（使用ChatGPT）和非辅助组（对照组），完成统一结构的论证写作任务并填写认知参与量表（CES及CES-AI）。

Result: ChatGPT组的认知参与得分显著低于对照组，表明AI辅助可能导致认知卸载。

Conclusion: 研究提示AI在教育中的应用或对学生自我调节学习和深度认知学习造成不利影响，强调需采用促进与AI生成内容积极反思的教学策略。

Abstract: Despite the increasing use of large language models (LLMs) in education,
concerns have emerged about their potential to reduce deep thinking and active
learning. This study investigates the impact of generative artificial
intelligence (AI) tools, specifically ChatGPT, on the cognitive engagement of
students during academic writing tasks. The study employed an experimental
design with participants randomly assigned to either an AI-assisted (ChatGPT)
or a non-assisted (control) condition. Participants completed a structured
argumentative writing task followed by a cognitive engagement scale (CES), the
CES-AI, developed to assess mental effort, attention, deep processing, and
strategic thinking. The results revealed significantly lower cognitive
engagement scores in the ChatGPT group compared to the control group. These
findings suggest that AI assistance may lead to cognitive offloading. The study
contributes to the growing body of literature on the psychological implications
of AI in education and raises important questions about the integration of such
tools into academic practice. It calls for pedagogical strategies that promote
active, reflective engagement with AI-generated content to avoid compromising
self-regulated learning and deep cognitive involvement of students.

</details>


### [151] [Holistic Artificial Intelligence in Medicine; improved performance and explainability](https://arxiv.org/abs/2507.00205)
*Periklis Petridis,Georgios Margaritis,Vasiliki Stoumpou,Dimitris Bertsimas*

Main category: cs.AI

TL;DR: 引入xHAIM，一个结合生成式AI的框架，提升预测性能并增强其解释性。


<details>
  <summary>Details</summary>
Motivation: 解决现有HAIM框架在任务相关性数据应用和可解释性不足的问题。

Method: 通过四个步骤实现：自动识别任务相关数据、生成患者综合摘要、改进预测建模以及通过医学知识提供临床解释。

Result: 在HAIM-MIMIC-MM数据集上，AUC从79.9%提升至90.3%。

Conclusion: xHAIM将AI从黑盒预测工具转变为可解释的决策支持系统，更好地服务于临床需求。

Abstract: With the increasing interest in deploying Artificial Intelligence in
medicine, we previously introduced HAIM (Holistic AI in Medicine), a framework
that fuses multimodal data to solve downstream clinical tasks. However, HAIM
uses data in a task-agnostic manner and lacks explainability. To address these
limitations, we introduce xHAIM (Explainable HAIM), a novel framework
leveraging Generative AI to enhance both prediction and explainability through
four structured steps: (1) automatically identifying task-relevant patient data
across modalities, (2) generating comprehensive patient summaries, (3) using
these summaries for improved predictive modeling, and (4) providing clinical
explanations by linking predictions to patient-specific medical knowledge.
Evaluated on the HAIM-MIMIC-MM dataset, xHAIM improves average AUC from 79.9%
to 90.3% across chest pathology and operative tasks. Importantly, xHAIM
transforms AI from a black-box predictor into an explainable decision support
system, enabling clinicians to interactively trace predictions back to relevant
patient data, bridging AI advancements with clinical utility.

</details>


### [152] [Learning for routing: A guided review of recent developments and future directions](https://arxiv.org/abs/2507.00218)
*Fangting Zhou,Attila Lischka,Balazs Kulcsar,Jiaming Wu,Morteza Haghir Chehreghani,Gilbert Laporte*

Main category: cs.AI

TL;DR: 文章评估了机器学习方法在解决NP难的组合优化问题（如旅行商问题TSP和车辆路径问题VRP）中的应用进展，提出了用ML方法解决这些复杂问题的分类框架。


<details>
  <summary>Details</summary>
Motivation: 由于旅行商问题和车辆路径问题的复杂性，传统的精确算法时间成本过高，而启发式方法无法保证最优解的准确性，引发了引入机器学习的新尝试。

Method: 作者提出了将基于构造和基于改进的两种机器学习方法用作解决这些路径问题的分类模型，并整合了传统运筹学方法与现代ML技术。

Result: 文章构建了一个分类法框架，能够更好地将机器学习技术应用到路径问题中，并为未来解决新型VRP变体提供指导。

Conclusion: 机器学习在优化复杂路由问题方面展现了潜力，研究提出的框架为传统运筹学与机器学习的结合提供了结构化的指导。

Abstract: This paper reviews the current progress in applying machine learning (ML)
tools to solve NP-hard combinatorial optimization problems, with a focus on
routing problems such as the traveling salesman problem (TSP) and the vehicle
routing problem (VRP). Due to the inherent complexity of these problems, exact
algorithms often require excessive computational time to find optimal
solutions, while heuristics can only provide approximate solutions without
guaranteeing optimality. With the recent success of machine learning models,
there is a growing trend in proposing and implementing diverse ML techniques to
enhance the resolution of these challenging routing problems. We propose a
taxonomy categorizing ML-based routing methods into construction-based and
improvement-based approaches, highlighting their applicability to various
problem characteristics. This review aims to integrate traditional OR methods
with state-of-the-art ML techniques, providing a structured framework to guide
future research and address emerging VRP variants.

</details>


### [153] [ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context](https://arxiv.org/abs/2507.00417)
*Joongwon Kim,Anirudh Goyal,Liang Tan,Hannaneh Hajishirzi,Srinivasan Iyer,Tianlu Wang*

Main category: cs.AI

TL;DR: 本文提出了ASTRO，一个结合自我反思、回溯和探索的框架，用以提升语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 目前开源的大型语言模型推理能力的提升多基于已有能力强的模型，而如何提升像Llama 3此类非推理模型的能力尚不明确。

Method: 通过Monte Carlo Tree Search生成合成数据集，将搜索轨迹转化为包含成功与纠错的链式推理表述，并利用强化学习进一步优化模型。

Result: Llama 3模型在MATH-500、AMC 2023和AIME 2024上分别实现了16.0%、26.9%和20.0%的性能提升，尤其是在需要迭代纠正的问题上表现显著提升。

Conclusion: 搜索启发的训练方法为提升开放性大型语言模型的推理能力提供了一个系统且有效的策略。

Abstract: We introduce ASTRO, the "Autoregressive Search-Taught Reasoner", a framework
for training language models to reason like search algorithms, explicitly
leveraging self-reflection, backtracking, and exploration in their outputs.
Recently, training large language models (LLMs) via reinforcement learning (RL)
has led to the advent of reasoning models with greatly enhanced reasoning
capabilities. Open-source replications of reasoning models, while successful,
build upon models that already exhibit strong reasoning capabilities along with
search behavior observed even before RL. As a result, it is yet unclear how to
boost the reasoning capabilities of other non-reasoner models including Llama
3. ASTRO teaches such models to internalize structured search behavior through
a synthetic dataset derived from Monte Carlo Tree Search (MCTS) over
mathematical problem-solving trajectories. By converting search traces into
natural language chain-of-thoughts that capture both successes and recoveries
from failure, ASTRO bootstraps models with a rich prior for exploration during
RL. We finetune our models on these search-derived traces and further improve
performance via RL with verifiable rewards. We apply ASTRO to the Llama 3
family of models and achieve absolute performance gains of 16.0% on MATH-500,
26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon
challenging problems that require iterative correction. Our results demonstrate
that search-inspired training offers a principled way to instill robust
reasoning capabilities into open LLMs.

</details>


### [154] [Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning](https://arxiv.org/abs/2507.00432)
*Maggie Huan,Yuetai Li,Tuney Zheng,Xiaoyu Xu,Seungone Kim,Minxin Du,Radha Poovendran,Graham Neubig,Xiang Yue*

Main category: cs.AI

TL;DR: 大模型在数学推理基准上取得了快速进展，但这些进步并未转化为更广泛的问题解决能力。研究表明，不同的微调方法（如强化学习和监督微调）会显著影响模型的跨领域泛化性能。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在数学推理方面的性能已超越人类，但研究者希望探明这些改进是普遍的还是特定领域的过拟合。

Method: 对20多个经过微调的推理模型进行评估，涵盖数学、科学问答、规划、编程等多个任务；并通过对比实验分析强化学习和监督微调对模型泛化的不同影响。

Result: 强化学习微调的模型在跨领域泛化性上表现更好，而监督微调模型往往丧失了通用能力。具体表现为，监督微调模型在潜在空间表示和分布转移上存在显著变化。

Conclusion: 目前的后训练方法需要重新思考，特别是依赖监督微调数据的策略可能限制了推理模型的进步。

Abstract: Math reasoning has become the poster child of progress in large language
models (LLMs), with new models rapidly surpassing human-level performance on
benchmarks like MATH and AIME. But as math leaderboards improve week by week,
it is worth asking: do these gains reflect broader problem-solving ability or
just narrow overfitting? To answer this question, we evaluate over 20
open-weight reasoning-tuned models across a broad suite of tasks, including
math, scientific QA, agent planning, coding, and standard
instruction-following. We surprisingly find that most models that succeed in
math fail to transfer their gains to other domains. To rigorously study this
phenomenon, we conduct controlled experiments on Qwen3-14B models using
math-only data but different tuning methods. We find that reinforcement
learning (RL)-tuned models generalize well across domains, while supervised
fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space
representation and token-space distribution shift analyses reveal that SFT
induces substantial representation and output drift, while RL preserves
general-domain structure. Our results suggest a need to rethink standard
post-training recipes, particularly the reliance on SFT-distilled data for
advancing reasoning models.

</details>


### [155] [Advancing Local Search in SMT-NRA with MCSAT Integration](https://arxiv.org/abs/2507.00557)
*Tianyi Ding,Haokun Li,Xinpeng Ni,Bican Xia,Tianqi Zhao*

Main category: cs.AI

TL;DR: 本文提出了一种基于非线性实数代数理论的可满足性模块（SMT-NRA）的改进局部搜索方法。


<details>
  <summary>Details</summary>
Motivation: 为提高SMT-NRA问题的求解效率，本文提出了新的方法和框架，以克服传统方法中搜索效率低的问题。

Method: 引入二维单元跳跃（2d-cell-jump）操作；提出扩展的局部搜索框架（2d-LS），结合MCSAT框架来提高搜索效率；实现适用于真实域CDCL搜索的样本单元投影算子，并设计组合MCSAT、2d-LS和OpenCAD的混合框架。

Result: 实验结果表明，改进的局部搜索方法提升了求解性能。

Conclusion: 新方法通过信息交换显著提升了SMT-NRA的搜索效率与性能。

Abstract: In this paper, we advance local search for Satisfiability Modulo the Theory
of Nonlinear Real Arithmetic (SMT-NRA for short). First, we introduce a
two-dimensional cell-jump move, called \emph{$2d$-cell-jump}, generalizing the
key operation, cell-jump, of the local search method for SMT-NRA. Then, we
propose an extended local search framework, named \emph{$2d$-LS} (following the
local search framework, LS, for SMT-NRA), integrating the model constructing
satisfiability calculus (MCSAT) framework to improve search efficiency. To
further improve the efficiency of MCSAT, we implement a recently proposed
technique called \emph{sample-cell projection operator} for MCSAT, which is
well suited for CDCL-style search in the real domain and helps guide the search
away from conflicting states. Finally, we design a hybrid framework for SMT-NRA
combining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through
information exchange. The experimental results demonstrate improvements in
local search performance, highlighting the effectiveness of the proposed
methods.

</details>


### [156] [Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess](https://arxiv.org/abs/2507.00726)
*Dongyoon Hwang,Hojoon Lee,Jaegul Choo,Dongmin Park,Jongho Park*

Main category: cs.AI

TL;DR: 探索使用深度强化学习（RL）提升大型语言模型（LLMs）在国际象棋中的战略推理能力，但发现模型的战略推理能力受预训练模型内部棋类理解不足的限制。


<details>
  <summary>Details</summary>
Motivation: RL在数学推理中的有效性已得到验证，但其在战略推理（如国际象棋）中的潜力鲜有探索，目的是研究RL是否能在国际象棋中开发出LLMs的战略推理能力。

Method: 利用一个在国际象棋上预训练的动作价值网络，为LLM的输出走棋质量提供密集奖励，实质上是一种知识蒸馏方法，并通过实验对比密集奖励与稀疏奖励的效果。

Result: 密集奖励通常优于稀疏二元奖励，但所有模型在能力上均远低于专家水平。

Conclusion: 预训练模型对国际象棋的内部理解不足是限制其能力的主要原因，仅靠RL可能不足以完全克服这一限制。

Abstract: While reinforcement learning (RL) for large language models (LLMs) has shown
promise in mathematical reasoning, strategic reasoning for LLMs using RL
remains largely unexplored. We investigate whether LLMs can develop strategic
reasoning capabilities through RL in chess. To this end, we leverage a
chess-pretrained action-value network to provide dense reward on the LLM's
output move quality, which can be seen as a form of knowledge distillation. Our
experiments show that our distillation-based dense rewards often outperform
sparse binary rewards. However, surprisingly, all models plateau far below
expert levels. We provide SFT and RL ablations on chess reasoning training and
find evidence that this limitation stems from a deficit in the pretrained
models' internal understanding of chess--a deficit which RL alone may not be
able to fully overcome.

</details>


### [157] [A Robust Algorithm for Non-IID Machine Learning Problems with Convergence Analysis](https://arxiv.org/abs/2507.00810)
*Qing Xu,Xiaohua Xuan*

Main category: cs.AI

TL;DR: 利用非平滑优化、二次规划和迭代过程，提出了一种改进的数值算法并验证其收敛性，适用于多领域。


<details>
  <summary>Details</summary>
Motivation: 现有算法可能在求解minimax问题时存在效率或收敛性不足。

Method: 结合非平滑优化、二次规划和迭代过程，提出改进的数值算法。

Result: 提出的算法在一定假设条件下证明了其收敛性。

Conclusion: 改进的数值算法在鲁棒优化和非平衡学习等领域具有广泛应用潜力。

Abstract: In this paper, we propose an improved numerical algorithm for solving minimax
problems based on nonsmooth optimization, quadratic programming and iterative
process. We also provide a rigorous proof of convergence for our algorithm
under some mild assumptions, such as gradient continuity and boundedness. Such
an algorithm can be widely applied in various fields such as robust
optimization, imbalanced learning, etc.

</details>


### [158] [SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents](https://arxiv.org/abs/2507.00841)
*Siyuan Liang,Tianmeng Fang,Zhe Liu,Aishan Liu,Yan Xiao,Jinyuan He,Ee-Chien Chang,Xiaochun Cao*

Main category: cs.AI

TL;DR: 本文探讨了移动多模态代理中的安全问题，旨在构建基于行为序列信息的风险判别机制，并设计了一种基于大语言模型的自动辅助评估方案。


<details>
  <summary>Details</summary>
Motivation: 当前多模态基础模型在智能代理系统中的广泛应用带来了效率和功能的提升，但伴随了潜在的安全威胁，尤其是系统容易被攻击者诱导执行高风险操作，因此亟需针对性安全解决方案。

Method: 通过将行为序列信息纳入风险判别，设计了一种基于大语言模型的自动评估方案，能够在多轮对话和多任务场景中检测潜在风险行为。

Result: 初步验证显示，所提出的方法在识别风险行为方面有一定改善，并可降低代理被“越狱”的概率。

Conclusion: 本研究成果为多模态智能代理系统的安全风险建模与防护提供了有价值的参考。

Abstract: With the wide application of multimodal foundation models in intelligent
agent systems, scenarios such as mobile device control, intelligent assistant
interaction, and multimodal task execution are gradually relying on such large
model-driven agents. However, the related systems are also increasingly exposed
to potential jailbreak risks. Attackers may induce the agents to bypass the
original behavioral constraints through specific inputs, and then trigger
certain risky and sensitive operations, such as modifying settings, executing
unauthorized commands, or impersonating user identities, which brings new
challenges to system security. Existing security measures for intelligent
agents still have limitations when facing complex interactions, especially in
detecting potentially risky behaviors across multiple rounds of conversations
or sequences of tasks. In addition, an efficient and consistent automated
methodology to assist in assessing and determining the impact of such risks is
currently lacking. This work explores the security issues surrounding mobile
multimodal agents, attempts to construct a risk discrimination mechanism by
incorporating behavioral sequence information, and designs an automated
assisted assessment scheme based on a large language model. Through preliminary
validation in several representative high-risk tasks, the results show that the
method can improve the recognition of risky behaviors to some extent and assist
in reducing the probability of agents being jailbroken. We hope that this study
can provide some valuable references for the security risk modeling and
protection of multimodal intelligent agent systems.

</details>


### [159] [Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact](https://arxiv.org/abs/2507.00951)
*Rizwan Qureshi,Ranjan Sapkota,Abbas Shah,Amgad Muneer,Anas Zafar,Ashmal Vayani,Maged Shoman,Abdelrahman B. M. Eldaly,Kai Zhang,Ferhat Sadak,Shaina Raza,Xinqi Fan,Ravid Shwartz-Ziv,Hong Yan,Vinjia Jain,Aman Chadha,Manoj Karkee,Jia Wu,Philip Torr,Seyedali Mirjalili*

Main category: cs.AI

TL;DR: 本文探讨机器是否能够像人类一样在各领域思考、推理和行动，同时综述了人工通用智能（AGI）的发展的跨学科方法。


<details>
  <summary>Details</summary>
Motivation: 研究当前AGI系统的局限性，尤其是对基于令牌预测的依赖以及缺乏真实自主性，并寻求解决这些问题的路径。

Method: 综合分析了多学科领域的研究，包括人工智能、认知神经科学和生成式模型等领域，并介绍了Agentic RAG框架、信息压缩以及与记忆和推理能力结合的模块化设计。

Result: 提出了一种更全面的AGI发展路线，包括从单纯统计学习到目标导向认知的过渡设计，同时指出了实现过程中存在的科学、技术和伦理挑战。

Conclusion: 真正的智能不仅需要规模，还需要记忆与推理的整合，强调模块化、交互性和自我改进以实现适应性行为和广泛领域的通用性。

Abstract: Can machines truly think, reason and act in domains like humans? This
enduring question continues to shape the pursuit of Artificial General
Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,
DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal
fluency and partial reasoning, these systems remain fundamentally limited by
their reliance on token-level prediction and lack of grounded agency. This
paper offers a cross-disciplinary synthesis of AGI development, spanning
artificial intelligence, cognitive neuroscience, psychology, generative models,
and agent-based systems. We analyze the architectural and cognitive foundations
of general intelligence, highlighting the role of modular reasoning, persistent
memory, and multi-agent coordination. In particular, we emphasize the rise of
Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use
to enable more adaptive behavior. We discuss generalization strategies,
including information compression, test-time adaptation, and training-free
methods, as critical pathways toward flexible, domain-agnostic intelligence.
Vision-Language Models (VLMs) are reexamined not just as perception modules but
as evolving interfaces for embodied understanding and collaborative task
completion. We also argue that true intelligence arises not from scale alone
but from the integration of memory and reasoning: an orchestration of modular,
interactive, and self-improving components where compression enables adaptive
behavior. Drawing on advances in neurosymbolic systems, reinforcement learning,
and cognitive scaffolding, we explore how recent architectures begin to bridge
the gap between statistical learning and goal-directed cognition. Finally, we
identify key scientific, technical, and ethical challenges on the path to AGI.

</details>


### [160] [Enhancing LLM Agent Safety via Causal Influence Prompting](https://arxiv.org/abs/2507.00979)
*Dongyoon Hahm,Woogyeol Jin,June Suk Choi,Sungsoo Ahn,Kimin Lee*

Main category: cs.AI

TL;DR: 提出了一种名为CIP的方法，通过因果影响图（CIDs）提高自主代理的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着由大型语言模型驱动的自主代理表现出广泛的潜力，确保其行为安全以防止意外后果非常重要。

Method: 通过因果影响图（CIDs），以三步方法建立、指导并优化代理的决策过程，从而识别并降低决策中的风险。

Result: 实验表明，该方法在代码执行和移动设备控制任务中显著增强了安全性。

Conclusion: CIP方法能有效减少因代理决策引发的风险，提供更安全可靠的行为框架。

Abstract: As autonomous agents powered by large language models (LLMs) continue to
demonstrate potential across various assistive tasks, ensuring their safe and
reliable behavior is crucial for preventing unintended consequences. In this
work, we introduce CIP, a novel technique that leverages causal influence
diagrams (CIDs) to identify and mitigate risks arising from agent
decision-making. CIDs provide a structured representation of cause-and-effect
relationships, enabling agents to anticipate harmful outcomes and make safer
decisions. Our approach consists of three key steps: (1) initializing a CID
based on task specifications to outline the decision-making process, (2)
guiding agent interactions with the environment using the CID, and (3)
iteratively refining the CID based on observed behaviors and outcomes.
Experimental results demonstrate that our method effectively enhances safety in
both code execution and mobile device control tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [161] [Hypertokens: Holographic Associative Memory in Tokenized LLMs](https://arxiv.org/abs/2507.00002)
*Christopher James Augeri*

Main category: cs.LG

TL;DR: 提出一种HDRAM方法，通过象征性记忆框架解决大语言模型中的信息传播问题，显著提升了联想检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理信息时存在精度损失问题，需要一种更有效的内存管理机制来优化信息存取。

Method: 该论文提出了一种名为HDRAM的框架，将Transformer的潜在空间视为扩频信道，通过超标记结合ECC、全息计算和量子启发式搜索技术，改进了键值操作和潜在空间搜索。

Result: HDRAM无需对现有架构进行修改，显著提升了大语言模型的信息恢复和联想检索能力。

Conclusion: 结合经典、全息和量子启发式方法，HDRAM框架为提升大语言模型提供了新的技术方向。

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but suffer from
apparent precision loss, reframed here as information spreading. This reframing
shifts the problem from computational precision to an information-theoretic
communication issue. We address the K:V and V:K memory problem in LLMs by
introducing HDRAM (Holographically Defined Random Access Memory), a symbolic
memory framework treating transformer latent space as a spread-spectrum
channel. Built upon hypertokens, structured symbolic codes integrating
classical error-correcting codes (ECC), holographic computing, and
quantum-inspired search, HDRAM recovers distributed information through
principled despreading. These phase-coherent memory addresses enable efficient
key-value operations and Grover-style search in latent space. By combining ECC
grammar with compressed sensing and Krylov subspace alignment, HDRAM
significantly improves associative retrieval without architectural changes,
demonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can
fortify transformer architectures.

</details>


### [162] [GLU Attention Improve Transformer](https://arxiv.org/abs/2507.00022)
*Zehao Wang*

Main category: cs.LG

TL;DR: 引入了一种名为GLU Attention的注意力机制，通过在注意力值中引入非线性来提升模型性能和收敛速度，同时几乎不增加计算成本。


<details>
  <summary>Details</summary>
Motivation: 提升神经网络性能，通过引入新型注意力机制克服现有方法的局限性，同时保持模型的效率。

Method: 通过设计GLU Attention，将非线性引入注意力机制中，同时确保引入的方法轻量化并能够与现有技术无缝集成。实验覆盖文本和图像领域。

Result: 实验结果表明，GLU Attention在不增加参数的情况下显著提升了模型性能和训练收敛速度。

Conclusion: GLU Attention是一种轻量且高效的注意力机制，可与多种技术联合使用，具有广泛的应用潜力。

Abstract: Gated Linear Units (GLU) have shown great potential in enhancing neural
network performance. In this paper, I introduce a novel attention mechanism
called GLU Attention, which introduces nonlinearity into the values of
Attention. My experiments demonstrate that GLU Attention improves both model
performance and convergence speed across text and vision modalities with zero
additional parameters and negligible computational costs. GLU Attention is
lightweight and can seamlessly integrate with other technologies, such as Flash
Attention, Rotary Position Embedding (RoPE), and various Multi-Head Attention
(MHA) variants such as Grouped-Query Attention (GQA). This project is
open-sourced at github.

</details>


### [163] [Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE](https://arxiv.org/abs/2507.00003)
*Eyhab Al-Masri*

Main category: cs.LG

TL;DR: 本文提出了NeutroSENSE，一个结合中智逻辑的集成框架，提升物联网入侵检测系统的可解释性。在IoT-CAD数据集上的实验显示，其准确率达97%，同时验证了误分类样本的不确定性显著高于正确样本。


<details>
  <summary>Details</summary>
Motivation: 在物联网环境下，入侵检测面临不确定性和复杂性，现有方法在解释性和可信度方面存在不足。本研究旨在提出一种增强解释性和不确定性处理能力的框架。

Method: 通过将Random Forest、XGBoost和Logistic Regression与中智逻辑相结合，框架将预测置信度分解为真（T）、假（F）和不确定性（I）成分。同时，设计了自适应的阈值策略对高不确定性预测进行标记。

Result: 在IoT-CAD数据集上，NeutroSENSE框架获得了97%的准确率，并显示误分类样本的不确定性显著较高（I = 0.62）且正确样本较低（I = 0.24）。

Conclusion: 中智逻辑增强了AI系统的准确性与可解释性，为边缘与雾计算环境中的信任化AI提供了实际依据。

Abstract: This paper presents NeutroSENSE, a neutrosophic-enhanced ensemble framework
for interpretable intrusion detection in IoT environments. By integrating
Random Forest, XGBoost, and Logistic Regression with neutrosophic logic, the
system decomposes prediction confidence into truth (T), falsity (F), and
indeterminacy (I) components, enabling uncertainty quantification and
abstention. Predictions with high indeterminacy are flagged for review using
both global and adaptive, class-specific thresholds. Evaluated on the IoT-CAD
dataset, NeutroSENSE achieved 97% accuracy, while demonstrating that
misclassified samples exhibit significantly higher indeterminacy (I = 0.62)
than correct ones (I = 0.24). The use of indeterminacy as a proxy for
uncertainty enables informed abstention and targeted review-particularly
valuable in edge deployments. Figures and tables validate the correlation
between I-scores and error likelihood, supporting more trustworthy,
human-in-the-loop AI decisions. This work shows that neutrosophic logic
enhances both accuracy and explainability, providing a practical foundation for
trust-aware AI in edge and fog-based IoT security systems.

</details>


### [164] [A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search](https://arxiv.org/abs/2507.00004)
*Austin R. Ellis-Mohr,Anuj K. Nayak,Lav R. Varshney*

Main category: cs.LG

TL;DR: 提出DS3框架，研究推理过程中模型性能与计算成本之间的关系，涵盖多种推理策略及其互依性。


<details>
  <summary>Details</summary>
Motivation: 研究如何在模型推理中实现计算效率与性能的优化，考虑资源限制及复杂推理任务的新挑战。

Method: 提出一个具有普适性的指导框架DS3，将推理建模为技能图的随机遍历，与当前推理策略（如CoT以及ToT）进行理论分析和基于多种推理策略进行对比。

Result: 框架可以在理论上解释诸多实证现象，如计算与准确率的对数关系、推理策略选择随任务变化、推理表现平稳下参数扩展后的涌现行为等，并统一分析BoN、投票等推理现象。

Conclusion: 通过明确训练-推理的相互依赖性，从理论层次加深对语言模型推理的理解，为算法设计和资源利用提供原则性指导。

Abstract: Large language models (LLMs) demand considerable computational, energy, and
financial resources during both training and deployment. While scaling laws for
training have guided much of the field's recent progress, inference costs now
represent a significant and growing component of the overall resource burden,
particularly for reasoning-focused models. Existing characterizations of
compute-optimality that consider model size, dataset size, and inference tokens
in isolation or in fixed combinations risk overlooking more efficient operating
points. We introduce directed stochastic skill search (DS3), a general
framework that represents inference as stochastic traversal over a learned
skill graph. From a simplified yet expressive instantiation, we derive
closed-form expressions for task success and compute cost across a wide range
of inference strategies -- including chain-of-thought (CoT) and tree-of-thought
(ToT) -- enabling comparative analysis as a function of task difficulty and
model capability. To that end, we extend a prior first-principles tripartite
graph framework of LLM training to incorporate inference, and separately bridge
DS3 with empirical methods that characterize LLM scaling behavior. We
theoretically recover empirically observed patterns, including: linear accuracy
scaling with logarithmic compute; variation in preferred inference strategies
as a function of task difficulty and model capability; emergent behavior
elicited by reasoning even when performance plateaus under parameter scaling;
and both best-of-N (BoN) and majority voting behavior captured within a unified
analytical framework. By explicitly characterizing training-inference
interdependencies, our framework deepens theoretical understanding and supports
principled algorithmic design and resource allocation.

</details>


### [165] [Novel RL approach for efficient Elevator Group Control Systems](https://arxiv.org/abs/2507.00011)
*Nathan Vaartjes,Vincent Francois-Lavet*

Main category: cs.LG

TL;DR: 研究提出了一种基于强化学习的电梯群控制系统（EGCS），使用新颖的编码和奖励信号，提高了学习效率并优于传统的基于规则的算法。


<details>
  <summary>Details</summary>
Motivation: 为了减少大型建筑中乘客的旅行时间和能耗，现有的基于启发式或模式检测的调度器难以应对随机性和组合复杂性。

Method: 通过将六部电梯和十五层楼的系统建模为马克夫决策过程，并引入一个端到端的强化学习架构，采用复杂的动作空间编码、连续乘客到达的内步骤建模以及量身定制的奖励信号。

Result: 实验表明，基于强化学习的EGCS在应对流量波动和高度随机的环境中优于传统规则算法。

Conclusion: 新方法能够提高电梯调度的效率与灵活性，为复杂环境中的电梯群控制问题提供了更优的解决方案。

Abstract: Efficient elevator traffic management in large buildings is critical for
minimizing passenger travel times and energy consumption. Because heuristic- or
pattern-detection-based controllers struggle with the stochastic and
combinatorial nature of dispatching, we model the six-elevator, fifteen-floor
system at Vrije Universiteit Amsterdam as a Markov Decision Process and train
an end-to-end Reinforcement Learning (RL) Elevator Group Control System (EGCS).
Key innovations include a novel action space encoding to handle the
combinatorial complexity of elevator dispatching, the introduction of
infra-steps to model continuous passenger arrivals, and a tailored reward
signal to improve learning efficiency. In addition, we explore various ways to
adapt the discounting factor to the infra-step formulation. We investigate RL
architectures based on Dueling Double Deep Q-learning, showing that the
proposed RL-based EGCS adapts to fluctuating traffic patterns, learns from a
highly stochastic environment, and thereby outperforms a traditional rule-based
algorithm.

</details>


### [166] [Towards Undistillable Models by Minimizing Conditional Mutual Information](https://arxiv.org/abs/2507.00012)
*Linfeng Ye,Shayan Mohajer Hamidi,En-hui Yang*

Main category: cs.LG

TL;DR: 本文提出了一种方法，可以使深度神经网络（DNN）无法通过知识蒸馏（KD）被拷贝，其中蒸馏学生模型无法超过独立训练的标签平滑学生模型的性能。


<details>
  <summary>Details</summary>
Motivation: 为了保护深度神经网络的知识产权，研究者希望构建无法被蒸馏的DNN模型。

Method: 提出了一种联合优化交叉熵损失和条件互信息（CMI）的训练方法，即CMIM方法，通过对温度缩放后概率分布的聚合性进行优化，使得模型难以被知识蒸馏复制。

Result: 通过实验验证，CMIM模型对各种现有的知识蒸馏方法均表现为无法被有效蒸馏，同时其分类性能也优于仅通过交叉熵损失训练的模型。

Conclusion: CMIM方法能够有效地保护DNN模型的知识产权，同时在模型预测性能方面也具有优势。

Abstract: A deep neural network (DNN) is said to be undistillable if, when used as a
black-box input-output teacher, it cannot be distilled through knowledge
distillation (KD). In this case, the distilled student (referred to as the
knockoff student) does not outperform a student trained independently with
label smoothing (LS student) in terms of prediction accuracy. To protect
intellectual property of DNNs, it is desirable to build undistillable DNNs. To
this end, it is first observed that an undistillable DNN may have the trait
that each cluster of its output probability distributions in response to all
sample instances with the same label should be highly concentrated to the
extent that each cluster corresponding to each label should ideally collapse
into one probability distribution. Based on this observation and by measuring
the concentration of each cluster in terms of conditional mutual information
(CMI), a new training method called CMI minimized (CMIM) method is proposed,
which trains a DNN by jointly minimizing the conventional cross entropy (CE)
loss and the CMI values of all temperature scaled clusters across the entire
temperature spectrum. The resulting CMIM model is shown, by extensive
experiments, to be undistillable by all tested KD methods existing in the
literature. That is, the knockoff students distilled by these KD methods from
the CMIM model underperform the respective LS students. In addition, the CMIM
model is also shown to performs better than the model trained with the CE loss
alone in terms of their own prediction accuracy.

</details>


### [167] [ST-MTM: Masked Time Series Modeling with Seasonal-Trend Decomposition for Time Series Forecasting](https://arxiv.org/abs/2507.00013)
*Hyunwoo Seo,Chiehyeon Lim*

Main category: cs.LG

TL;DR: 提出了一种通过季节趋势分解的掩码时间序列建模框架，称为ST-MTM，可有效捕捉复杂时间序列的语义信息和时间模式，从而提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的掩码时间序列建模方法难以捕捉复杂语义信息，容易导致学习到数据中的虚假时间模式。

Method: 引入ST-MTM框架，通过季节趋势分解进行时间序列掩码建模，采用新型掩码方法分别针对季节和趋势分量，结合多周期掩码策略和子序列掩码策略，同时利用对比学习增强上下文一致性。

Result: 实验表明，ST-MTM在预测性能上优于现有的掩码建模、对比学习及监督学习方法。

Conclusion: 通过利用时间序列的内在结构和多样性，ST-MTM成功提升了时间序列预测的准确性和鲁棒性，验证了新方法的有效性。

Abstract: Forecasting complex time series is an important yet challenging problem that
involves various industrial applications. Recently, masked time-series modeling
has been proposed to effectively model temporal dependencies for forecasting by
reconstructing masked segments from unmasked ones. However, since the semantic
information in time series is involved in intricate temporal variations
generated by multiple time series components, simply masking a raw time series
ignores the inherent semantic structure, which may cause MTM to learn spurious
temporal patterns present in the raw data. To capture distinct temporal
semantics, we show that masked modeling techniques should address entangled
patterns through a decomposition approach. Specifically, we propose ST-MTM, a
masked time-series modeling framework with seasonal-trend decomposition, which
includes a novel masking method for the seasonal-trend components that
incorporates different temporal variations from each component. ST-MTM uses a
period masking strategy for seasonal components to produce multiple masked
seasonal series based on inherent multi-periodicity and a sub-series masking
strategy for trend components to mask temporal regions that share similar
variations. The proposed masking method presents an effective pre-training task
for learning intricate temporal variations and dependencies. Additionally,
ST-MTM introduces a contrastive learning task to support masked modeling by
enhancing contextual consistency among multiple masked seasonal
representations. Experimental results show that our proposed ST-MTM achieves
consistently superior forecasting performance compared to existing masked
modeling, contrastive learning, and supervised forecasting methods.

</details>


### [168] [SWE-Bench-CL: Continual Learning for Coding Agents](https://arxiv.org/abs/2507.00014)
*Thomas Joshi,Shayan Chowdhury,Fatih Uysal*

Main category: cs.LG

TL;DR: 本文提出了SWE-Bench-CL，这是一个以GitHub问题按时间顺序组织的连续学习基准，测试AI在软件工程中的表现并抵抗遗忘。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码生成方面表现卓越，但在动态且连续变化的软件开发中仍然存在挑战。研究目标是评估AI在任务间知识转移和经验积累能力。

Method: 通过SWE-Bench Verified数据集构建一个新基准（SWE-Bench-CL），结合时间顺序的GitHub问题，并开发特殊指标和评估框架来量化模型性能。

Result: 提出了多项评估指标（如平均准确率、遗忘性和转移能力），并提供了实验对比及公开数据，为评估AI适应性和鲁棒性提供了工具。

Conclusion: SWE-Bench-CL成为一个新的评估平台，以检测和改进AI在动态任务中的连续学习能力。所有代码和数据均公开，为社区提供了进一步开发的基础。

Abstract: Large Language Models (LLMs) have achieved impressive results on static
code-generation benchmarks, but real-world software development unfolds as a
continuous stream of evolving issues, fixes, and feature requests. We introduce
SWE-Bench-CL, a novel continual learning benchmark built on the human-verified
SWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By
organizing GitHub issues into chronologically ordered sequences that reflect
natural repository evolution, SWE-Bench-CL enables direct evaluation of an
agent's ability to accumulate experience, transfer knowledge across tasks, and
resist catastrophic forgetting. We complement the dataset with (i) a
preliminary analysis of inter-task structural similarity and contextual
sensitivity, (ii) an interactive LangGraph-based evaluation framework augmented
with a FAISS-backed semantic memory module, and (iii) a suite of specialized
continual learning metrics -- including average accuracy, forgetting,
forward/backward transfer, tool-use efficiency, and a generalized Composite
Continual Learning Score and CL-F-beta score -- to capture the
stability-plasticity trade-off. We outline a rigorous experimental protocol
comparing memory-enabled and memory-disabled agents across diverse Python
repositories. All code and data are publicly available at
https://github.com/thomasjoshi/agents-never-forget, providing the community
with a reproducible platform for developing more adaptive and robust AI agents
in software engineering.

</details>


### [169] [Vision Transformer with Adversarial Indicator Token against Adversarial Attacks in Radio Signal Classifications](https://arxiv.org/abs/2507.00015)
*Lu Zhang,Sangarapillai Lambotharan,Gan Zheng,Guisheng Liao,Xuekang Liu,Fabio Roli,Carsten Maple*

Main category: cs.LG

TL;DR: 该研究提出一种新颖的基于视觉transformer (ViT) 的自动调制分类防御方法，通过引入对抗性指示(AdvI) token进行对抗攻击检测，实验表明其防御效果优于多种主流方法。


<details>
  <summary>Details</summary>
Motivation: 随着transformer在NLP和计算机视觉领域中的成功，研究者希望将其应用于物联网通信系统中的自动调制分类。然而，现有基于transformer的无线信号分类对对抗攻击存在脆弱性，导致该领域的潜在风险增加。

Method: 本文提出了一种改进的ViT架构，通过引入AdvI token检测对抗攻击，并结合对抗训练方法，将训练时防御与运行时防御统一在一个神经网络模型中，降低了体系结构的复杂性。

Result: 实验证明，本文方法在处理如快速梯度法、投影梯度下降攻击和基本迭代法等白盒攻击场景中效果卓越，且该方法能够增强对潜在对抗扰动的检测能力。

Conclusion: 该研究首次在ViT架构中提出AdvI token，并结合对抗训练方法，展现了一个高效的一体化防御系统，能够克服对抗攻击难题并显著提升分类性能。

Abstract: The remarkable success of transformers across various fields such as natural
language processing and computer vision has paved the way for their
applications in automatic modulation classification, a critical component in
the communication systems of Internet of Things (IoT) devices. However, it has
been observed that transformer-based classification of radio signals is
susceptible to subtle yet sophisticated adversarial attacks. To address this
issue, we have developed a defensive strategy for transformer-based modulation
classification systems to counter such adversarial attacks. In this paper, we
propose a novel vision transformer (ViT) architecture by introducing a new
concept known as adversarial indicator (AdvI) token to detect adversarial
attacks. To the best of our knowledge, this is the first work to propose an
AdvI token in ViT to defend against adversarial attacks. Integrating an
adversarial training method with a detection mechanism using AdvI token, we
combine a training time defense and running time defense in a unified neural
network model, which reduces architectural complexity of the system compared to
detecting adversarial perturbations using separate models. We investigate into
the operational principles of our method by examining the attention mechanism.
We show the proposed AdvI token acts as a crucial element within the ViT,
influencing attention weights and thereby highlighting regions or features in
the input data that are potentially suspicious or anomalous. Through
experimental results, we demonstrate that our approach surpasses several
competitive methods in handling white-box attack scenarios, including those
utilizing the fast gradient method, projected gradient descent attacks and
basic iterative method.

</details>


### [170] [Gradient-based Fine-Tuning through Pre-trained Model Regularization](https://arxiv.org/abs/2507.00016)
*Xuanbo Liu,Liu Liu,Fuxiang Wu,Fusheng Hao,Xianglong Liu*

Main category: cs.LG

TL;DR: 提出一种高效的梯度基础正则化微调方法（GRFT），同时保持效率和性能，显著减少参数更新需求。


<details>
  <summary>Details</summary>
Motivation: 当前预训练模型在下游任务微调中需要大量计算资源和存储。所提出方法旨在优化参数选择和减少存储需求。

Method: 提出了一种梯度和正则化驱动的微调方法，优选梯度平方和最高的权重矩阵行或列进行更新，并结合正则化以提升知识传递。

Result: GRFT 在 FGVC 和 VTAB 数据集上仅需更新 1.22% 和 0.30% 的参数，超越 GPS, Adapter Tuning 和 LoRA 等方法，达到最先进性能。

Conclusion: GRFT 能在保证高效率的同时实现优越的性能，是微调预训练模型领域的重大进展。

Abstract: Large pre-trained models have demonstrated extensive applications across
various fields. However, fine-tuning these models for specific downstream tasks
demands significant computational resources and storage. One fine-tuning
method, gradient-based parameter selection (GPS), focuses on fine-tuning only
the parameters with high gradients in each neuron, thereby reducing the number
of training parameters. Nevertheless, this approach increases computational
resource requirements and storage demands. In this paper, we propose an
efficient gradient-based and regularized fine-tuning method (GRFT) that updates
the rows or columns of the weight matrix. We theoretically demonstrate that the
rows or columns with the highest sum of squared gradients are optimal for
updating. This strategy effectively reduces storage overhead and improves the
efficiency of parameter selection. Additionally, we incorporate regularization
to enhance knowledge transfer from the pre-trained model. GRFT achieves
state-of-the-art performance, surpassing existing methods such as GPS, Adapter
Tuning, and LoRA. Notably, GRFT requires updating only 1.22% and 0.30% of the
total parameters on FGVC and VTAB datasets, respectively, demonstrating its
high efficiency and effectiveness. The source code will be released soon.

</details>


### [171] [Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections](https://arxiv.org/abs/2507.00018)
*Bo Wang,Qinyuan Cheng,Runyu Peng,Rong Bao,Peiji Li,Qipeng Guo,Linyang Li,Zhiyuan Zeng,Yunhua Zhou,Xipeng Qiu*

Main category: cs.LG

TL;DR: 本文提出了一个统一的理论框架，将监督微调(SFT)和偏好学习在大语言模型(LLM)后训练中的作用联系起来，并阐述了它们在最优策略-奖励子空间中的关系。


<details>
  <summary>Details</summary>
Motivation: 旨在解决传统监督微调中KL散度的优化局限性，并改进模型在任务中的指令跟随能力。

Method: 通过严格的数学推导分析现有方法的局限性，提出了学习率降低策略和从f-散度函数派生的新目标函数。

Result: 提出方法在指令任务上的表现得到了显著提升，性能改进幅度可达25%，绝对胜率提升了6%。

Conclusion: 新提出的理论框架和优化方法增强了LLM的后续训练表现，扩展了监督微调与偏好学习之间的关系理论。

Abstract: Post-training processes are essential phases in grounding pre-trained
language models to real-world tasks, with learning from demonstrations or
preference signals playing a crucial role in this adaptation. We present a
unified theoretical framework bridging Supervised Fine-Tuning (SFT) and
preference learning in Large Language Model (LLM) post-training. Through
rigorous mathematical derivation, we demonstrate that both SFT and preference
learning methods like Direct Preference Optimization (DPO) operate within the
same optimal policy-reward subspace, with SFT representing a special case of
implicit reward learning. Our analysis reveals a critical limitation in
conventional SFT: the KL divergence term in distribution matching becomes
constant with respect to the policy during optimization, failing to constrain
model updates. To address this, we propose a simple yet effective learning rate
reduction approach that yields significant performance improvements (up to
\textbf{25\%} relative gain and \textbf{6\%} absolute win rate increase in
instruction following tasks. Additionally, we derive alternative SFT objectives
from various f-divergence functions that preserve the KL term during
optimization, further enhancing post-DPO model performance. Finally, we extend
the theoretical relationship between LLM logits and Q-functions from preference
learning to the SFT context, providing mathematical derivations and
experimental validation.

</details>


### [172] [Quantum Inspired Encoding Strategies for Machine Learning Models: Proposing and Evaluating Instance Level, Global Discrete, and Class Conditional Representations](https://arxiv.org/abs/2507.00019)
*Minati Rath,Hema Date*

Main category: cs.LG

TL;DR: 该研究提出了三种量子启发式数据编码策略（ILS, GDS, CCVS），用于将经典数据转换为适用于纯经典机器学习模型的量子数据，分析其在编码效率和分类任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 设计高效的量子启发式数据编码方法，减少编码时间的同时确保数据编码的正确性，并探讨其对分类性能的影响。

Method: 提出并比较三种编码策略：实例级策略（ILS）、全局离散策略（GDS）和类别条件值策略（CCVS），分别模仿局部量子态、全局统一量子态及类别相关信息量子态，应用在分类任务中评估其表现。

Result: 通过实验分析了编码时间、精度、预测性能及计算成本的权衡，三种策略在不同维度的表现提供优化量子数据转换的新见解。

Conclusion: 这些编码策略为经典机器学习工作流中量子化数据的高效转换与应用提供了新的优化方向。

Abstract: In this study, we propose, evaluate and compare three quantum inspired data
encoding strategies, Instance Level Strategy (ILS), Global Discrete Strategy
(GDS) and Class Conditional Value Strategy (CCVS), for transforming classical
data into quantum data for use in pure classical machine learning models. The
primary objective is to reduce high encoding time while ensuring correct
encoding values and analyzing their impact on classification performance. The
Instance Level Strategy treats each row of dataset independently; mimics local
quantum states. Global Discrete Value Based encoding strategy maps all unique
feature values across the full dataset to quantum states uniformly. In
contrast, the Class conditional Value based encoding strategy encodes unique
values separately for each class, preserving class dependent information.
  We apply these encoding strategies to a classification task and assess their
impact on en-coding efficiency, correctness, model accuracy, and computational
cost. By analyzing the trade offs between encoding time, precision, and
predictive performance, this study provides insights into optimizing quantum
inspired data transformations for classical machine learning workflows.

</details>


### [173] [Variational Autoencoder for Generating Broader-Spectrum prior Proposals in Markov chain Monte Carlo Methods](https://arxiv.org/abs/2507.00020)
*Marcio Borges,Felipe Pereira,Michel Tosin*

Main category: cs.LG

TL;DR: 这项研究利用变分自编码器(VAE)方法改进马尔可夫链蒙特卡罗(McMC)方法，提高其效率与适用性，通过生成更广泛的先验提议来代替传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要已知的协方差函数，而这种信息在实际应用中往往是不可获得的。因此，提出了一种数据驱动的方法，通过VAE框架捕捉更广泛的相关性结构，特别适用于地下流体建模中的贝叶斯反问题。

Method: 利用VAE方法进行参数化表示，并在合成的地下水流反演问题上进行测试，其中使用压力数据估计渗透率场。同时与传统的Karhunen-Loève扩展方法进行对比分析。

Result: 数值实验表明，当相关性长度已知时，VAE参数化方法的准确性可与KLE媲美；当假设的相关性长度偏离真实值时，VAE优于KLE。此外，VAE显著降低了随机维度，提高了计算效率。

Conclusion: 该方法通过结合深度生成模型与McMC方法，在处理高维贝叶斯问题中展现出了更大的灵活性和效率。

Abstract: This study uses a Variational Autoencoder method to enhance the efficiency
and applicability of Markov Chain Monte Carlo (McMC) methods by generating
broader-spectrum prior proposals. Traditional approaches, such as the
Karhunen-Lo\`eve Expansion (KLE), require previous knowledge of the covariance
function, often unavailable in practical applications. The VAE framework
enables a data-driven approach to flexibly capture a broader range of
correlation structures in Bayesian inverse problems, particularly subsurface
flow modeling. The methodology is tested on a synthetic groundwater flow
inversion problem, where pressure data is used to estimate permeability fields.
Numerical experiments demonstrate that the VAE-based parameterization achieves
comparable accuracy to KLE when the correlation length is known and outperforms
KLE when the assumed correlation length deviates from the true value. Moreover,
the VAE approach significantly reduces stochastic dimensionality, improving
computational efficiency. The results suggest that leveraging deep generative
models in McMC methods can lead to more adaptable and efficient Bayesian
inference in high-dimensional problems.

</details>


### [174] [AIMatDesign: Knowledge-Augmented Reinforcement Learning for Inverse Materials Design under Data Scarcity](https://arxiv.org/abs/2507.00024)
*Yeyong Yu,Xilei Bian,Jie Xiong,Xing Wu,Quan Qian*

Main category: cs.LG

TL;DR: 本文提出了AIMatDesign，一种结合强化学习和专家知识的框架，用于高效材料反演设计，超越传统机器学习和强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在高维材料组成空间中的可靠性和整合领域专家知识的能力有限，不足以支持高效的材料反演设计。

Method: AIMatDesign通过差异算法扩充实验数据构建信任经验池，利用大语言模型的自动校正策略提高模型预测可靠性，并结合基于知识的奖励函数引入专家领域规则，提高训练的稳定性和效率。

Result: 实验表明，AIMatDesign在发现效率、收敛速度和成功率上显著优于传统方法，并成功预测和合成了性能与预测值高度匹配的Zr基合金。

Conclusion: AIMatDesign通过结合强化学习和领域专家知识，在材料反演设计领域展示了显著潜力，为闭环材料发现提供了可靠手段。

Abstract: With the growing demand for novel materials, machine learning-driven inverse
design methods face significant challenges in reconciling the high-dimensional
materials composition space with limited experimental data. Existing approaches
suffer from two major limitations: (I) machine learning models often lack
reliability in high-dimensional spaces, leading to prediction biases during the
design process; (II) these models fail to effectively incorporate domain expert
knowledge, limiting their capacity to support knowledge-guided inverse design.
To address these challenges, we introduce AIMatDesign, a reinforcement learning
framework that addresses these limitations by augmenting experimental data
using difference-based algorithms to build a trusted experience pool,
accelerating model convergence. To enhance model reliability, an automated
refinement strategy guided by large language models (LLMs) dynamically corrects
prediction inconsistencies, reinforcing alignment between reward signals and
state value functions. Additionally, a knowledge-based reward function
leverages expert domain rules to improve stability and efficiency during
training. Our experiments demonstrate that AIMatDesign significantly surpasses
traditional machine learning and reinforcement learning methods in discovery
efficiency, convergence speed, and success rates. Among the numerous candidates
proposed by AIMatDesign, experimental synthesis of representative Zr-based
alloys yielded a top-performing BMG with 1.7GPa yield strength and 10.2\%
elongation, closely matching predictions. Moreover, the framework accurately
captured the trend of yield strength variation with composition, demonstrating
its reliability and potential for closed-loop materials discovery.

</details>


### [175] [Generalizing to New Dynamical Systems via Frequency Domain Adaptation](https://arxiv.org/abs/2507.00025)
*Tiexin Qin,Hong Yan,Haoliang Li*

Main category: cs.LG

TL;DR: 本研究提出FNSDA方法，在傅立叶空间进行适应性调整以实现对新动力系统的普适性泛化。实验表明该方法性能优越且参数成本显著降低。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度神经网络的动力建模方法很难对新环境中的相同动力学实现高效泛化。

Method: 提出了一种参数高效的方法——Fourier Neural Simulator for Dynamical Adaptation (FNSDA)，通过在傅立叶空间中分割共享动力学并对每个新环境进行模式调整实现泛化能力。

Result: 通过评估四类典型动力系统，FNSDA方法在保证性能优越或竞争力的同时显著降低了参数开销。

Conclusion: FNSDA能够有效克服现有方法在未见环境下研究动力学泛化问题的局限性，且实现了高效且成本低的适应性建模。

Abstract: Learning the underlying dynamics from data with deep neural networks has
shown remarkable potential in modeling various complex physical dynamics.
However, current approaches are constrained in their ability to make reliable
predictions in a specific domain and struggle with generalizing to unseen
systems that are governed by the same general dynamics but differ in
environmental characteristics. In this work, we formulate a parameter-efficient
method, Fourier Neural Simulator for Dynamical Adaptation (FNSDA), that can
readily generalize to new dynamics via adaptation in the Fourier space.
Specifically, FNSDA identifies the shareable dynamics based on the known
environments using an automatic partition in Fourier modes and learns to adjust
the modes specific for each new environment by conditioning on low-dimensional
latent systematic parameters for efficient generalization. We evaluate our
approach on four representative families of dynamic systems, and the results
show that FNSDA can achieve superior or competitive generalization performance
compared to existing methods with a significantly reduced parameter cost. Our
code is available at https://github.com/WonderSeven/FNSDA.

</details>


### [176] [ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models](https://arxiv.org/abs/2507.00026)
*Jiale Ding,Xiang Zheng,Cong Wang,Wei-Bin Lee,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.LG

TL;DR: 该论文提出了一种名为ROSE的安全评估框架，通过多目标强化学习生成具有上下文和话题多样性的对抗提示，用于更好地评估大型语言模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 当前手工评估方法无法跟上语言模型快速迭代的步伐，且现有自动化方法在话题覆盖和真实场景下表现欠佳，导致安全评估难以深入和全面。

Method: 使用多目标强化学习对对抗性语言模型进行微调，使其能生成话题多样且语境丰富的对抗性提示，提升评估的适应性与现实关联性。

Result: ROSE框架在实验中表现优于现有方法，能够显著提升发现大型语言模型安全漏洞的能力，特别是在综合评估指标上展现显著改进。

Conclusion: ROSE框架为实现更实用和面向现实的语言模型安全评估迈出了重要一步，可更好地识别潜在的安全问题。

Abstract: As Large Language Models (LLMs) are increasingly deployed as black-box
components in real-world applications, evaluating their safety-especially under
adversarial prompting-has become critical. Arguably, effective safety
evaluations should be adaptive, evolving with LLM capabilities, and also cover
a broad spectrum of harmful topics and real-world scenarios to fully expose
potential vulnerabilities. Existing manual safety benchmarks, built on
handcrafted adversarial prompts, are limited by their static nature and the
intensive labor required to update them, making it difficult to keep pace with
rapidly advancing LLMs. In contrast, automated adversarial prompt generation
offers a promising path toward adaptive evaluation. However, current methods
often suffer from insufficient adversarial topic coverage (topic-level
diversity) and weak alignment with real-world contexts. These shortcomings stem
from the exploration-exploitation dilemma in black-box optimization and a lack
of real-world contextualization, resulting in adversarial prompts that are both
topically narrow and scenario-repetitive. To address these issues, we propose
Reality-Oriented Safety Evaluation (ROSE), a novel framework that uses
multi-objective reinforcement learning to fine-tune an adversarial LLM for
generating topically diverse and contextually rich adversarial prompts.
Experiments show that ROSE outperforms existing methods in uncovering safety
vulnerabilities in state-of-the-art LLMs, with notable improvements in
integrated evaluation metrics. We hope ROSE represents a step toward more
practical and reality-oriented safety evaluation of LLMs. WARNING: This paper
contains examples of potentially harmful text.

</details>


### [177] [HiT-JEPA: A Hierarchical Self-supervised Trajectory Embedding Framework for Similarity Computation](https://arxiv.org/abs/2507.00028)
*Lihuan Li,Hao Xue,Shuang Ao,Yang Song,Flora Salim*

Main category: cs.LG

TL;DR: 本文提出了一种名为HiT-JEPA的框架，用于多尺度城市轨迹表示学习，通过三层层次结构结合细粒度细节与全局抽象，解决了现有方法难以同时处理局部和全局信息的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的城市轨迹表示学习方法无法同时很好地捕捉轨迹的细粒度细节和高层次总结，影响了对空间运动模式的分析。

Method: 作者提出了一种三层层次框架HiT-JEPA，用于学习轨迹的多尺度表示；该框架逐步捕捉点级细节、中间模式和高层抽象，从而集成局部动态与全局语义。

Result: 在多个真实数据集上的实验表明，HiT-JEPA的分层设计能生成更丰富的多尺度轨迹表示。

Conclusion: HiT-JEPA有效解决了轨迹表示学习中多尺度信息整合的问题，为轨迹相似性计算提供了更强大的工具。

Abstract: The representation of urban trajectory data plays a critical role in
effectively analyzing spatial movement patterns. Despite considerable progress,
the challenge of designing trajectory representations that can capture diverse
and complementary information remains an open research problem. Existing
methods struggle in incorporating trajectory fine-grained details and
high-level summary in a single model, limiting their ability to attend to both
long-term dependencies while preserving local nuances. To address this, we
propose HiT-JEPA (Hierarchical Interactions of Trajectory Semantics via a Joint
Embedding Predictive Architecture), a unified framework for learning
multi-scale urban trajectory representations across semantic abstraction
levels. HiT-JEPA adopts a three-layer hierarchy that progressively captures
point-level fine-grained details, intermediate patterns, and high-level
trajectory abstractions, enabling the model to integrate both local dynamics
and global semantics in one coherent structure. Extensive experiments on
multiple real-world datasets for trajectory similarity computation show that
HiT-JEPA's hierarchical design yields richer, multi-scale representations. Code
is available at: https://anonymous.4open.science/r/HiT-JEPA.

</details>


### [178] [LoRA-Mixer: Coordinate Modular LoRA Experts Through Serial Attention Routing](https://arxiv.org/abs/2507.00029)
*Wenbing Li,Zikai Song,Hang Zhou,Yunyao Zhang,Junqing Yu,Wei Yang*

Main category: cs.LG

TL;DR: 本文提出了LoRA-Mixer，这是一个模块化、轻量级的低秩适配器（LoRA）与专家混合模型（MoE）结合的框架，可高效适配多任务大语言模型（LLMs）。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在将LoRA与MoE结合时存在参数效率低、任务效果差的问题。

Method: 创新地将注意力模块中线性投影矩阵替换为动态路由的任务特定LoRA专家模块，并引入自适应“特化平衡损失”（SBL）提升专家复用性与路由稳定性。

Result: 在多个基准数据集上表现出显著性能提升，其中在GSM8K、HumanEval 和 MedQA数据集分别提升7.61%、4.88%、3.08%。与最先进方法相比，效果提升1%-2%，参数量仅为其48%。

Conclusion: LoRA-Mixer在高效适配多任务方面表现出色，兼具模型性能和参数效率的优势。

Abstract: Recent efforts to combine low-rank adaptation (LoRA) with mixture-of-experts
(MoE) for adapting large language models (LLMs) to multiple tasks still exhibit
prevailing limitations: they either swap entire attention/feed-forward layers
for switch experts or bolt on parallel expert branches, diluting parameter
efficiency and task fidelity. We propose the LoRA-Mixer, a modular and
lightweight MoE framework that integrates LoRA experts. Our core innovation
lies in replacing the projection matrices of the attention module's
input/output linear layers with dynamically routed, task-specific LoRA experts.
This design ensures seamless compatibility with diverse foundation models,
including transformers and state space models (SSMs), by leveraging their
inherent linear projection structures. The framework supports two operational
paradigms: (1) joint optimization of LoRA experts and routing mechanisms via a
novel hard-soft routing strategy, or (2) direct deployment of pre-trained,
frozen LoRA modules sourced from external repositories. To enable robust router
training with limited data while ensuring stable routing decisions and
maximizing expert reuse, we introduce an adaptive Specialization Balance Loss
(SBL) that jointly optimizes expert balance and task-specific alignment.
Extensive experiments on seven benchmark datasets, including MedQA, CoLA,
SST-2, GSM8K, ARC-E, ARC-C, and HumanEval, demonstrate the effectiveness of
LoRA-Mixer. On datasets such as GSM8K, HumanEval, and MedQA, LoRA-Mixer
achieves significant improvements of 7.61%, 4.88%, and 3.08% over the base
models, respectively. Compared with state-of-the-art methods, LoRA-Mixer
achieves additional improvements of 1.09%, 1.45%, and 1.68%, respectively,
using only 48% of the parameters, demonstrating its efficiency and strong
performance.

</details>


### [179] [Adaptive Action Duration with Contextual Bandits for Deep Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2507.00030)
*Abhishek Verma,Nallarasan V,Balaraman Ravindran*

Main category: cs.LG

TL;DR: 本文提出一种结合上下文多臂老虎机和深度强化学习（DRL）的新方法，以自适应地选择动作持续时间，并在Atari 2600游戏中显示出显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习中动作执行时间尺度的重要性及其在动态任务中的适应性问题。

Method: 将深度Q网络（DQN）与一个上下文多臂老虎机模块结合，主体学习根据状态上下文选择最佳动作重复率。

Result: 实验表明相比于固定动作持续时间基线方法，自适应动作持续时间显著提升了在Atari 2600游戏中的性能。

Conclusion: 本文提出的自适应动作持续时间框架为实时动态任务（例如游戏和机器人领域）中的深度强化学习应用提供了一种可扩展的解决方案。

Abstract: Deep Reinforcement Learning (DRL) has achieved remarkable success in complex
sequential decision-making tasks, such as playing Atari 2600 games and
mastering board games. A critical yet underexplored aspect of DRL is the
temporal scale of action execution. We propose a novel paradigm that integrates
contextual bandits with DRL to adaptively select action durations, enhancing
policy flexibility and computational efficiency. Our approach augments a Deep
Q-Network (DQN) with a contextual bandit module that learns to choose optimal
action repetition rates based on state contexts. Experiments on Atari 2600
games demonstrate significant performance improvements over static duration
baselines, highlighting the efficacy of adaptive temporal abstractions in DRL.
This paradigm offers a scalable solution for real-time applications like gaming
and robotics, where dynamic action durations are critical.

</details>


### [180] [Enhancing Spatio-Temporal Forecasting with Spatial Neighbourhood Fusion:A Case Study on COVID-19 Mobility in Peru](https://arxiv.org/abs/2507.00031)
*Chuan Li,Jiang You,Hassine Moungla,Vincent Gauthier,Miguel Nunez-del-Prado,Hugo Alatrista-Salas*

Main category: cs.LG

TL;DR: 本研究提出了一种轻量级的空间邻域融合技术（SPN），通过结合周边区域信号改善了疫情期间城市区域间的流动性预测。


<details>
  <summary>Details</summary>
Motivation: 通过准确建模人类流动性，来更好地理解疾病传播并提供及时干预。

Method: 利用秘鲁的数字接触追踪应用的大规模时空数据，结合所提的空间邻域融合技术，增强每个区域的特征以改进流动预测。

Result: 实验表明，SPN方法在不同的预测模型中始终提高了性能，测试均方误差（MSE）最多降低了9.85%。

Conclusion: 空间信号的平滑处理为公共卫生危机期间的坚实时空预测提供了简单而有效的解决方案。

Abstract: Accurate modeling of human mobility is critical for understanding epidemic
spread and deploying timely interventions. In this work, we leverage a
large-scale spatio-temporal dataset collected from Peru's national Digital
Contact Tracing (DCT) application during the COVID-19 pandemic to forecast
mobility flows across urban regions. A key challenge lies in the spatial
sparsity of hourly mobility counts across hexagonal grid cells, which limits
the predictive power of conventional time series models. To address this, we
propose a lightweight and model-agnostic Spatial Neighbourhood Fusion (SPN)
technique that augments each cell's features with aggregated signals from its
immediate H3 neighbors. We evaluate this strategy on three forecasting
backbones: NLinear, PatchTST, and K-U-Net, under various historical input
lengths. Experimental results show that SPN consistently improves forecasting
performance, achieving up to 9.85 percent reduction in test MSE. Our findings
demonstrate that spatial smoothing of sparse mobility signals provides a simple
yet effective path toward robust spatio-temporal forecasting during public
health crises.

</details>


### [181] [Data Collection with Non-Uniform Axial Power for Phase II of the OECD/NEA AI/ML Critical Heat Flux Benchmark](https://arxiv.org/abs/2507.00034)
*Reece Bourisaw,Reid McCants,Jean-Marie Le Corre,Anna Iskhakova,Arsen S. Iskhakov*

Main category: cs.LG

TL;DR: 本文收集并数字化了涵盖均匀和非均匀轴向加热条件的CHF数据集，探讨了传统和现代方法在预测CHF中的误差，同时为未来研究提供数据支持。


<details>
  <summary>Details</summary>
Motivation: 研究CHF基准测试，特别是空间变化功率分布下的CHF预测，以提高轻水反应堆安全性。

Method: 通过技术报告提取、插值到一致轴向网格、验证能量平衡并编码为机器可读格式，构建CHF数据集；分析传统方法与现代方法在均匀和非均匀加热下的性能，并使用神经网络校验预测能力。

Result: 传统CHF相关性在均匀加热下误差较大，并在非均匀加热下表现更差；现代表格法有所改进但仍不完美；神经网络仅在均匀条件下性能良好，但未能推广至空间变化场景。

Conclusion: 提供了经过整理的数据集和基线建模结果，为转移学习、不确定性量化和设计优化的研究奠定了基础。

Abstract: Critical heat flux (CHF) marks the onset of boiling crisis in light-water
reactors, defining safe thermal-hydraulic operating limits. To support Phase II
of the OECD/NEA AI/ML CHF benchmark, which introduces spatially varying power
profiles, this work compiles and digitizes a broad CHF dataset covering both
uniform and non-uniform axial heating conditions. Heating profiles were
extracted from technical reports, interpolated onto a consistent axial mesh,
validated via energy-balance checks, and encoded in machine-readable formats
for benchmark compatibility.
  Classical CHF correlations exhibit substantial errors under uniform heating
and degrade markedly when applied to non-uniform profiles, while modern tabular
methods offer improved but still imperfect predictions. A neural network
trained solely on uniform data performs well in that regime but fails to
generalize to spatially varying scenarios, underscoring the need for models
that explicitly incorporate axial power distributions. By providing these
curated datasets and baseline modeling results, this study lays the groundwork
for advanced transfer-learning strategies, rigorous uncertainty quantification,
and design-optimization efforts in the next phase of the CHF benchmark.

</details>


### [182] [IDRIFTNET: Physics-Driven Spatiotemporal Deep Learning for Iceberg Drift Forecasting](https://arxiv.org/abs/2507.00036)
*Rohan Putatunda,Sanjay Purushotham,Ratnaksha Lele,Vandana P. Janeja*

Main category: cs.LG

TL;DR: 提出了一个混合物理驱动的深度学习模型IDRIFTNET，用于预测冰山轨迹，其在有限数据和动态环境下表现出色。


<details>
  <summary>Details</summary>
Motivation: 由于冰山轨迹预测面临时空数据稀缺和复杂的非线性运动特性，目前的深度学习模型未能有效捕捉其动态特性。

Method: 设计了IDRIFTNET模型，将冰山漂移物理的解析公式与增强型残差学习模型相结合，并辅以旋转增强的谱神经网络，能够捕获数据中的全球和局部模式。

Result: IDRIFTNET在两个南极冰山(A23A和B22A)上的终位移误差和平均位移误差均优于现有模型。

Conclusion: IDRIFTNET模型能有效应对数据有限和环境动态条件下的冰山轨迹预测挑战，为冰山轨迹预测提供了更准确的解决方案。

Abstract: Drifting icebergs in the polar oceans play a key role in the Earth's climate
system, impacting freshwater fluxes into the ocean and regional ecosystems
while also posing a challenge to polar navigation. However, accurately
forecasting iceberg trajectories remains a formidable challenge, primarily due
to the scarcity of spatiotemporal data and the complex, nonlinear nature of
iceberg motion, which is also impacted by environmental variables. The iceberg
motion is influenced by multiple dynamic environmental factors, creating a
highly variable system that makes trajectory identification complex. These
limitations hinder the ability of deep learning models to effectively capture
the underlying dynamics and provide reliable predictive outcomes. To address
these challenges, we propose a hybrid IDRIFTNET model, a physics-driven deep
learning model that combines an analytical formulation of iceberg drift
physics, with an augmented residual learning model. The model learns the
pattern of mismatch between the analytical solution and ground-truth
observations, which is combined with a rotate-augmented spectral neural network
that captures both global and local patterns from the data to forecast future
iceberg drift positions. We compare IDRIFTNET model performance with
state-of-the-art models on two Antarctic icebergs: A23A and B22A. Our findings
demonstrate that IDRIFTNET outperforms other models by achieving a lower Final
Displacement Error (FDE) and Average Displacement Error (ADE) across a variety
of time points. These results highlight IDRIFTNET's effectiveness in capturing
the complex, nonlinear drift of icebergs for forecasting iceberg trajectories
under limited data and dynamic environmental conditions.

</details>


### [183] [Model Fusion via Neuron Interpolation](https://arxiv.org/abs/2507.00037)
*Phoomraphee Luenam,Andreas Spanopoulos,Amit Sant,Thomas Hofmann,Sotiris Anagnostidis,Sidak Pal Singh*

Main category: cs.LG

TL;DR: 本文提出了一种以神经元为中心的模型融合算法，能有效融合多个神经网络模型，适用于不同数据分布的情况。


<details>
  <summary>Details</summary>
Motivation: 现有的模型融合方法难以应对因排列不变性、随机初始化或数据分布差异引起的内部表示差异问题。

Method: 通过对父模型的中间神经元进行分组，创建目标表示，并通过子网络进行逼近；算法融合了神经元归因分数，并支持任意层类型的泛化。

Result: 实验表明，在零样本和分布不一致的融合场景中，该方法优于现有技术，在多个标准数据集上取得了优异表现。

Conclusion: 所提出的模型融合算法在性能和适用性上均有显著提升，为处理复杂模型融合任务提供了一种有效方法。

Abstract: Model fusion aims to combine the knowledge of multiple models by creating one
representative model that captures the strengths of all of its parents.
However, this process is non-trivial due to differences in internal
representations, which can stem from permutation invariance, random
initialization, or differently distributed training data. We present a novel,
neuron-centric family of model fusion algorithms designed to integrate multiple
trained neural networks into a single network effectively regardless of
training data distribution. Our algorithms group intermediate neurons of parent
models to create target representations that the fused model approximates with
its corresponding sub-network. Unlike prior approaches, our approach
incorporates neuron attribution scores into the fusion process. Furthermore,
our algorithms can generalize to arbitrary layer types. Experimental results on
various benchmark datasets demonstrate that our algorithms consistently
outperform previous fusion techniques, particularly in zero-shot and non-IID
fusion scenarios. The code is available at
https://github.com/AndrewSpano/neuron-interpolation-model-fusion.

</details>


### [184] [Quality over Quantity: An Effective Large-Scale Data Reduction Strategy Based on Pointwise V-Information](https://arxiv.org/abs/2507.00038)
*Fei Chen,Wenchi Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种基于点对点V信息(PVI)的数据缩减策略，通过识别数据集中最有信息量的实例来提升模型训练效率。


<details>
  <summary>Details</summary>
Motivation: 在数据集规模庞大的情况下，选取最优实例以提高数据质量和训练效率是数据缩减的核心挑战。

Method: 使用PVI量化实例难度，过滤掉低难度实例，并提出一种静态方法。此外，通过将实例按PVI升序排序的渐进学习方式，加速收敛并提升准确率。

Result: 实验表明，移除10%-30%的数据仅导致0.0001%-0.76%的准确率损失，同时渐进学习实现了0.8%的准确率提升。

Conclusion: 该策略在提升模型性能和训练效率方面表现卓越，并成功适用于中文NLP任务中，为跨语言数据缩减提供有益参考。

Abstract: Data reduction plays a vital role in data-centric AI by identifying the most
informative instance within large-scale datasets to enhance model training
efficiency. The core challenge lies in how to select the optimal
instances-rather than the entire datasets-to improve data quality and training
efficiency. In this paper, we propose an effective data reduction strategy
based on Pointwise V-information(PVI). First, we quantify instance difficulty
using PVI and filter out low-difficulty instances enabling a static approach.
Experiments demonstrate that removing 10%-30% of the data preserves the
classifier performance with only a 0.0001% to 0.76% loss in accuracy.Second, we
use a progressive learning approach to training the classifiers on instances
sorted by ascending PVI, accelerating convergence and achieving a 0.8% accuracy
gain over conventional training. Our results suggest that with the effective
data reduction strategy, training a classifier on the selected optimal subset
could enhance the model performance and boost training efficiency. Moreover, we
have transferred the PVI framework, which previously applied only to English
datasets, to diverse Chinese NLP tasks and base models, leading to valuable
insights for cross-lingual data reduction and faster training. The codes are
released at https://github.com/zhouwenchi/DatasetReductionStrategy.

</details>


### [185] [Pattern-Based Graph Classification: Comparison of Quality Measures and Importance of Preprocessing](https://arxiv.org/abs/2507.00039)
*Lucas Potin,Rosa Figueiredo,Vincent Labatut,Christine Largeron*

Main category: cs.LG

TL;DR: 本文探讨了图分类中的质量测量问题，对38种测量方法进行了对比分析，同时提出了一种基于聚类的预处理方法以优化分类性能。


<details>
  <summary>Details</summary>
Motivation: 目前图分类任务中广泛使用的质量测量方法缺乏深入评价，系统地对比分析这些方法的必要性迫切。

Method: 通过理论上的数学属性对38种质量测量方法进行分类，利用公开数据集构建基准测试，提出金标准排名，并进行经验性能对比；此外，还引入了一种聚类预处理方法用于优化分类。

Result: 实验结果表明，聚类预处理方法简化了图模式集合，同时分类性能保持高效。另外，一些文献中常用的质量测量方法并非最佳。

Conclusion: 综合理论分析和实验研究，为研究者和终端用户提供了关于图分类测量方法的实用指南，同时验证了创新预处理方法的效用。

Abstract: Graph classification aims to categorize graphs based on their structural and
attribute features, with applications in diverse fields such as social network
analysis and bioinformatics. Among the methods proposed to solve this task,
those relying on patterns (i.e. subgraphs) provide good explainability, as the
patterns used for classification can be directly interpreted. To identify
meaningful patterns, a standard approach is to use a quality measure, i.e. a
function that evaluates the discriminative power of each pattern. However, the
literature provides tens of such measures, making it difficult to select the
most appropriate for a given application. Only a handful of surveys try to
provide some insight by comparing these measures, and none of them specifically
focuses on graphs. This typically results in the systematic use of the most
widespread measures, without thorough evaluation. To address this issue, we
present a comparative analysis of 38 quality measures from the literature. We
characterize them theoretically, based on four mathematical properties. We
leverage publicly available datasets to constitute a benchmark, and propose a
method to elaborate a gold standard ranking of the patterns. We exploit these
resources to perform an empirical comparison of the measures, both in terms of
pattern ranking and classification performance. Moreover, we propose a
clustering-based preprocessing step, which groups patterns appearing in the
same graphs to enhance classification performance. Our experimental results
demonstrate the effectiveness of this step, reducing the number of patterns to
be processed while achieving comparable performance. Additionally, we show that
some popular measures widely used in the literature are not associated with the
best results.

</details>


### [186] [Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using Knowledge Distillation](https://arxiv.org/abs/2507.00055)
*Varsha Pendyala,Pedro Morgado,William Sethares*

Main category: cs.LG

TL;DR: 本研究提出了一种名为LightweightSER的知识蒸馏框架，通过利用未经标注的音视频数据优化语音情感识别。


<details>
  <summary>Details</summary>
Motivation: 探索如何在不依赖大规模标注数据的情况下，改进语音情感识别性能。

Method: 通过知识蒸馏框架LightweightSER，将大规模教师模型学到的情感信息迁移到轻量化学生模型中。

Result: 实验使用了RAVDESS和CREMA-D数据集，证明LightweightSER减少了对标注数据的依赖。

Conclusion: LightweightSER为语音情感识别提供了一种高效的无标注数据利用方法。

Abstract: Voice interfaces integral to the human-computer interaction systems can
benefit from speech emotion recognition (SER) to customize responses based on
user emotions. Since humans convey emotions through multi-modal audio-visual
cues, developing SER systems using both the modalities is beneficial. However,
collecting a vast amount of labeled data for their development is expensive.
This paper proposes a knowledge distillation framework called LightweightSER
(LiSER) that leverages unlabeled audio-visual data for SER, using large teacher
models built on advanced speech and face representation models. LiSER transfers
knowledge regarding speech emotions and facial expressions from the teacher
models to lightweight student models. Experiments conducted on two benchmark
datasets, RAVDESS and CREMA-D, demonstrate that LiSER can reduce the dependence
on extensive labeled datasets for SER tasks.

</details>


### [187] [Smooth-Distill: A Self-distillation Framework for Multitask Learning with Wearable Sensor Data](https://arxiv.org/abs/2507.00061)
*Hoang-Dieu Vu,Duc-Nghia Tran,Quang-Tu Pham,Hieu H. Pham,Nicolas Vuillerme,Duc-Tan Tran*

Main category: cs.LG

TL;DR: 本文提出Smooth-Distill，一个新的自蒸馏框架，结合了人类活动识别(HAR)和传感器位置检测，使用基于加速度计数据的统一多任务CNN架构(MTL-net)。


<details>
  <summary>Details</summary>
Motivation: 研究旨在打造一个高效、准确的多任务框架，以解决基于加速度计的人类活动识别和设备位置检测，同时降低训练计算开销。

Method: 使用统一的CNN架构MTL-net处理数据，并通过历史平滑版本的自身模型作为教师模式进行训练，从而实现无需独立教师模型的自蒸馏框架。

Result: 实验结果表明，Smooth-Distill在不同评估环境下始终优于其他方法，并显著改善了模型的收敛稳定性与减少过拟合。

Conclusion: Smooth-Distill提供了一种有效的多任务学习解决方案，兼顾了基于加速度计数据的模型准确性和训练效率，同时降低了计算成本。

Abstract: This paper introduces Smooth-Distill, a novel self-distillation framework
designed to simultaneously perform human activity recognition (HAR) and sensor
placement detection using wearable sensor data. The proposed approach utilizes
a unified CNN-based architecture, MTL-net, which processes accelerometer data
and branches into two outputs for each respective task. Unlike conventional
distillation methods that require separate teacher and student models, the
proposed framework utilizes a smoothed, historical version of the model itself
as the teacher, significantly reducing training computational overhead while
maintaining performance benefits. To support this research, we developed a
comprehensive accelerometer-based dataset capturing 12 distinct sleep postures
across three different wearing positions, complementing two existing public
datasets (MHealth and WISDM). Experimental results show that Smooth-Distill
consistently outperforms alternative approaches across different evaluation
scenarios, achieving notable improvements in both human activity recognition
and device placement detection tasks. This method demonstrates enhanced
stability in convergence patterns during training and exhibits reduced
overfitting compared to traditional multitask learning baselines. This
framework contributes to the practical implementation of knowledge distillation
in human activity recognition systems, offering an effective solution for
multitask learning with accelerometer data that balances accuracy and training
efficiency. More broadly, it reduces the computational cost of model training,
which is critical for scenarios requiring frequent model updates or training on
resource-constrained platforms. The code and model are available at
https://github.com/Kuan2vn/smooth\_distill.

</details>


### [188] [Fractional Policy Gradients: Reinforcement Learning with Long-Term Memory](https://arxiv.org/abs/2507.00073)
*Urvi Pawar,Kunal Telangi*

Main category: cs.LG

TL;DR: 本文提出了一种新的强化学习框架Fractional Policy Gradients (FPG)，通过将分数阶微积分应用于策略优化，改进了长期时间建模。


<details>
  <summary>Details</summary>
Motivation: 传统策略梯度方法受马尔可夫假设限制，存在高方差和采样效率低的问题。该研究旨在解决这一局限性，通过引入分数阶微积分建立新的时间相关性建模方法。

Method: 将Caputo分数阶导数应用于梯度重构，提出具有常时间和常内存开销的递归算法，用于计算分数阶时间差分误差。

Result: FPG在样本效率上提高了35-68%，相比现有基准方法方差降低了24-52%。

Conclusion: FPG成功实现了长程依赖的建模，同时保证了计算开销的可控性，为强化学习中的策略优化提供了一种数学上有理论依据的新方法。

Abstract: We propose Fractional Policy Gradients (FPG), a reinforcement learning
framework incorporating fractional calculus for long-term temporal modeling in
policy optimization. Standard policy gradient approaches face limitations from
Markovian assumptions, exhibiting high variance and inefficient sampling. By
reformulating gradients using Caputo fractional derivatives, FPG establishes
power-law temporal correlations between state transitions. We develop an
efficient recursive computation technique for fractional temporal-difference
errors with constant time and memory requirements. Theoretical analysis shows
FPG achieves asymptotic variance reduction of order O(t^(-alpha)) versus
standard policy gradients while preserving convergence. Empirical validation
demonstrates 35-68% sample efficiency gains and 24-52% variance reduction
versus state-of-the-art baselines. This framework provides a mathematically
grounded approach for leveraging long-range dependencies without computational
overhead.

</details>


### [189] [Theoretical Modeling of LLM Self-Improvement Training Dynamics Through Solver-Verifier Gap](https://arxiv.org/abs/2507.00075)
*Yifan Sun,Yushan Liang,Zhen Zhang,Jiaye Teng*

Main category: cs.LG

TL;DR: 研究探索大型语言模型在自我改进过程中的性能变化规律，通过理论建模和实证验证形成模型框架，并研究外部数据对该过程的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨自我改进过程中大型语言模型性能演变的规律及预测最终性能的可能性，同时研究在外部数据有限情况下的数据利用效果。

Method: 通过引入求解器-验证器差距理论模型，对自我改进训练动态进行建模并预测最终自我改进性能，并结合实验验证理论模型的有效性。

Result: 实验证明理论模型在多种语言模型和数据集上的有效性，同时分析有限外部数据的适用性。

Conclusion: 提出了一种理论视角下的自我改进方法动态分析模型，揭示了有限外部数据对最终性能影响不大的规律，提供了预估性能的方法。

Abstract: Self-improvement is among the most prominent techniques within the realm of
large language models (LLM), aiming to enhance the LLM performance without
relying on external data. Despite its significance, generally how LLM
performances evolve during the self-improvement process remains underexplored.
In this paper, we theoretically model the training dynamics of self-improvement
via the concept of solver-verifier gap. This is inspired by the conjecture that
the performance enhancement of self-improvement stems from the gap between
LLM's solver capability and verifier capability. Based on the theoretical
framework, we further introduce how to predict the ultimate power of
self-improvement using only information from the first few training epochs. We
empirically validate the effectiveness of the theoretical model on various LLMs
and datasets. Beyond self-improvement, we extend our analysis to investigate
how external data influences these dynamics within the framework. Notably, we
find that under limited external data regimes, such external data can be
utilized at any stage without significantly affecting final performances, which
accords with the empirical observations.

</details>


### [190] [The language of time: a language model perspective on time-series foundation models](https://arxiv.org/abs/2507.00078)
*Yi Xie,Yun Xiong,Zejian Shi,Hao Niu,Zhengfu Liu*

Main category: cs.LG

TL;DR: 本研究探讨了时间序列基础模型在跨领域任务中的成功和可能的理论解释，阐明其如何从大语言模型中继承高性能表现机制。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决时间序列数据动态系统特性与其基础模型在跨领域任务中成功之间的矛盾。

Method: 通过理论分析与实验研究了基于补丁的时间序列基础模型的表征学习机制和泛化能力，并提出将无序向量表征扩展为潜在概率分布表征的新框架。

Result: 研究表明连续时间序列片段可以成功量化为离散词汇，其统计属性与自然语言的一致，解释了时间序列模型的高效表现。

Conclusion: 本文为理解与改进大规模时间序列基础模型的安全性与可靠性提供了严谨的理论基础。

Abstract: With the rise of large language models, the paradigm of training foundation
models with massive parameter counts on vast datasets has been adopted in
multiple domains to achieve remarkable success. Time series foundation models
represent a significant extension of this paradigm, demonstrating exceptional
expressive power, generalization, and cross-domain transferability. However,
this gives rise to a fundamental paradox: time series data reflect distinct
dynamical systems, making cross-domain transfer intuitively implausible, yet
this is contradicted by the models' empirical success. To resolve this paradox,
this paper investigates, from both theoretical and experimental perspectives,
the representation learning mechanisms and generalization capabilities of
patch-based time series foundation models. We argue that such models are not
merely applying a new architecture but are fundamentally generalizing the
representation paradigm of language models by extending deterministic
vector-based representations to latent probabilistic distributional forms. Our
theoretical analysis supports this framework by demonstrating that continuous
time-series patches can be faithfully quantized into a discrete vocabulary
whose key statistical properties are highly consistent with those of natural
language. This generalization allows time series models to inherit the robust
representation and transfer abilities of large language models, thereby
explaining their superior performance in temporal tasks. Ultimately, our work
provides a rigorous theoretical cornerstone for understanding, evaluating, and
improving the safety and reliability of large-scale time series foundation
models.

</details>


### [191] [Online Meal Detection Based on CGM Data Dynamics](https://arxiv.org/abs/2507.00080)
*Ali Tavasoli,Heman Shakeri*

Main category: cs.LG

TL;DR: 本文提出使用动态模式从CGM数据中提取特征来检测用餐事件，提高了检测准确性和理解度。


<details>
  <summary>Details</summary>
Motivation: 研究饮食对血糖波动的影响，探索通过动态模式捕捉用餐后血糖变化模式的可能性。

Method: 利用CGM数据中的动态模式特性，针对血糖的变化模式提取特征并用于检测用餐事件。

Result: 提出的方法在提高用餐事件检测的准确性和可解释性方面具有显著优势。

Conclusion: 通过动态特征提取，该方法能够跨数据集实现良好的泛化性能，在实际应用中表现可靠。

Abstract: We utilize dynamical modes as features derived from Continuous Glucose
Monitoring (CGM) data to detect meal events. By leveraging the inherent
properties of underlying dynamics, these modes capture key aspects of glucose
variability, enabling the identification of patterns and anomalies associated
with meal consumption. This approach not only improves the accuracy of meal
detection but also enhances the interpretability of the underlying glucose
dynamics. By focusing on dynamical features, our method provides a robust
framework for feature extraction, facilitating generalization across diverse
datasets and ensuring reliable performance in real-world applications. The
proposed technique offers significant advantages over traditional approaches,
improving detection accuracy,

</details>


### [192] [Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission](https://arxiv.org/abs/2507.00082)
*Faranaksadat Solat,Joohyung Lee,Mohamed Seif,Dusit Niyato,H. Vincent Poor*

Main category: cs.LG

TL;DR: FedHLM通过将不确定性感知推论与联邦学习相结合，大幅降低了通信开销，在不显著损失精度的情况下减少了超过95%的LLM请求。


<details>
  <summary>Details</summary>
Motivation: 解决在带宽受限场景中，混合语言模型中由于不确定性预测需要频繁调用LLM导致的通信负担。

Method: 利用联邦学习协作优化动态不确定性阈值，并通过嵌入式令牌表征实现点对点解决方案，减少对LLM的依赖，同时采用分层模型聚合来捕获不确定性模式。

Result: FedHLM在大规模新闻分类任务中显著减少了LLM调用（超过95%），且精度损失可以忽略不计。

Conclusion: FedHLM是一种在边缘AI应用中高效、可扩展的通信优化框架，有助于提升混合语言模型的实用性。

Abstract: Hybrid Language Models (HLMs) combine the low-latency efficiency of Small
Language Models (SLMs) on edge devices with the high accuracy of Large Language
Models (LLMs) on centralized servers. Unlike traditional end-to-end LLM
inference, HLMs reduce latency and communication by invoking LLMs only when
local SLM predictions are uncertain, i.e., when token-level confidence is low
or entropy is high. However, ambiguous or low-confidence predictions still
require frequent offloading to the LLM, leading to significant communication
overhead in bandwidth-constrained settings. To address this, we propose FedHLM,
a communication-efficient HLM framework that integrates uncertainty-aware
inference with Federated Learning (FL). FedHLM's key innovation lies in
collaboratively learning token-level uncertainty thresholds that govern when
LLM assistance is needed. Rather than using static or manually tuned
thresholds, FedHLM employs FL to optimize these thresholds in a
privacy-preserving, distributed manner. Additionally, it leverages
embedding-based token representations for Peer-to-Peer (P2P) resolution,
enabling clients to reuse tokens inferred by semantically similar peers without
engaging the LLM. We further introduce hierarchical model aggregation: edge
servers refine local routing policies through client updates, while
cross-cluster coordination aligns global decision boundaries. This layered
design captures recurring uncertainty patterns, reducing redundant LLM queries.
Experiments on large-scale news classification tasks show that FedHLM reduces
LLM transmissions by over 95 percent with negligible accuracy loss, making it
well-suited for scalable and efficient edge-AI applications.

</details>


### [193] [Strategic Counterfactual Modeling of Deep-Target Airstrike Systems via Intervention-Aware Spatio-Causal Graph Networks](https://arxiv.org/abs/2507.00083)
*Wei Meng*

Main category: cs.LG

TL;DR: 提出了一种名为IA-STGNN的新框架，填补了战术打击行为与战略延迟之间缺乏因果建模的空白，并在多个基准模型中表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前战略级仿真存在关于“韧性—节点压制—谈判窗口”链中中间变量建模的结构性瓶颈，因此需要一个更结构化的因果建模框架。

Method: 提出了IA-STGNN框架，结合图注意力机制、反事实仿真单元和空间干预节点重建，动态仿真打击配置和同步策略。数据由多物理仿真平台产生，并符合NIST SP 800-160标准。

Result: IA-STGNN在多个基准模型上表现出色，相较ST-GNN、GCN-LSTM和XGBoost，在MAE上减少了12.8%，Top-5准确性上提升了18.4%，并改善了因果路径一致性和干预稳定性。

Conclusion: IA-STGNN提升了战略延迟的可解释预测性能，支持核威慑仿真、外交窗口评估以及多策略优化等高层政策建模应用，并提供了结构化透明的AI决策支持机制。

Abstract: This study addresses the lack of structured causal modeling between tactical
strike behavior and strategic delay in current strategic-level simulations,
particularly the structural bottlenecks in capturing intermediate variables
within the "resilience - nodal suppression - negotiation window" chain. We
propose the Intervention-Aware Spatio-Temporal Graph Neural Network (IA-STGNN),
a novel framework that closes the causal loop from tactical input to strategic
delay output. The model integrates graph attention mechanisms, counterfactual
simulation units, and spatial intervention node reconstruction to enable
dynamic simulations of strike configurations and synchronization strategies.
Training data are generated from a multi-physics simulation platform (GEANT4 +
COMSOL) under NIST SP 800-160 standards, ensuring structural traceability and
policy-level validation. Experimental results demonstrate that IA-STGNN
significantly outperforms baseline models (ST-GNN, GCN-LSTM, XGBoost),
achieving a 12.8 percent reduction in MAE and 18.4 percent increase in Top-5
percent accuracy, while improving causal path consistency and intervention
stability. IA-STGNN enables interpretable prediction of strategic delay and
supports applications such as nuclear deterrence simulation, diplomatic window
assessment, and multi-strategy optimization, providing a structured and
transparent AI decision-support mechanism for high-level policy modeling.

</details>


### [194] [A Joint Topology-Data Fusion Graph Network for Robust Traffic Speed Prediction with Data Anomalism](https://arxiv.org/abs/2507.00085)
*Ruiyuan Jiang,Dongyao Jia,Eng Gee Lim,Pengfei Fan,Yuli Zhang,Shangbo Wang*

Main category: cs.LG

TL;DR: 提出了一种名为GFEN的框架，用于提升网络级别的交通速度预测，其综合了空间和时间相关性并动态处理历史数据异常与非平稳性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效处理交通动态的复杂性与非线性问题，且处理历史数据异常与非平稳性的方式缺乏适应性，限制了数据平滑和预测效率。

Method: GFEN引入了拓扑时空图融合技术，通过可训练的方法结合数据分布与网络拓扑的空间和时间相关性，同时利用混合方法动态平滑和缓解异常与非平稳性，其数学框架基于k阶差分，并结合注意力机制的深度学习结构。

Result: 实验表明，GFEN在预测准确性上超越当前先进方法约6.3%，收敛速度几乎为最近混合模型的两倍。

Conclusion: GFEN展现了提升交通预测系统效率的优越潜力，并提供了更高的预测准确性和收敛性。

Abstract: Accurate traffic prediction is essential for Intelligent Transportation
Systems (ITS), yet current methods struggle with the inherent complexity and
non-linearity of traffic dynamics, making it difficult to integrate spatial and
temporal characteristics. Furthermore, existing approaches use static
techniques to address non-stationary and anomalous historical data, which
limits adaptability and undermines data smoothing. To overcome these
challenges, we propose the Graph Fusion Enhanced Network (GFEN), an innovative
framework for network-level traffic speed prediction. GFEN introduces a novel
topological spatiotemporal graph fusion technique that meticulously extracts
and merges spatial and temporal correlations from both data distribution and
network topology using trainable methods, enabling the modeling of multi-scale
spatiotemporal features. Additionally, GFEN employs a hybrid methodology
combining a k-th order difference-based mathematical framework with an
attention-based deep learning structure to adaptively smooth historical
observations and dynamically mitigate data anomalies and non-stationarity.
Extensive experiments demonstrate that GFEN surpasses state-of-the-art methods
by approximately 6.3% in prediction accuracy and exhibits convergence rates
nearly twice as fast as recent hybrid models, confirming its superior
performance and potential to significantly enhance traffic prediction system
efficiency.

</details>


### [195] [pUniFind: a unified large pre-trained deep learning model pushing the limit of mass spectra interpretation](https://arxiv.org/abs/2507.00087)
*Jiale Zhao,Pengzhi Mao,Kaifei Wang,Yiming Li,Yaping Peng,Ranfei Chen,Shuqi Lu,Xiaohong Ji,Jiaxiang Ding,Xin Zhang,Yucheng Liao,Weinan E,Weijie Zhang,Han Wen,Hao Chi*

Main category: cs.LG

TL;DR: 提出了大规模多模态预训练模型pUniFind，在蛋白质组学中显著提高了肽段识别的数量和准确性。


<details>
  <summary>Details</summary>
Motivation: 目前的深度学习模型在质谱数据解析中多为特征提取器，缺乏统一的打分框架。

Method: 通过训练超过1亿的开放搜索光谱数据，pUniFind实现了光谱与肽段的交叉模态预测，并支持去新肽序列推测和超1300种修饰识别。

Result: pUniFind在免疫肽段组学中识别肽段数量提升42.6%，PSM识别效率提升60%，并额外鉴定了38.5%的肽段，包括1,891个基因组相关但未在参考蛋白组中发现的肽段。

Conclusion: pUniFind是一个统一、可扩展的深度学习框架，提升了蛋白质组学分析的敏感性、修饰覆盖率和结果解释性。

Abstract: Deep learning has advanced mass spectrometry data interpretation, yet most
models remain feature extractors rather than unified scoring frameworks. We
present pUniFind, the first large-scale multimodal pre-trained model in
proteomics that integrates end-to-end peptide-spectrum scoring with open,
zero-shot de novo sequencing. Trained on over 100 million open search-derived
spectra, pUniFind aligns spectral and peptide modalities via cross modality
prediction and outperforms traditional engines across diverse datasets,
particularly achieving a 42.6 percent increase in the number of identified
peptides in immunopeptidomics. Supporting over 1,300 modifications, pUniFind
identifies 60 percent more PSMs than existing de novo methods despite a
300-fold larger search space. A deep learning based quality control module
further recovers 38.5 percent additional peptides including 1,891 mapped to the
genome but absent from reference proteomes while preserving full fragment ion
coverage. These results establish a unified, scalable deep learning framework
for proteomic analysis, offering improved sensitivity, modification coverage,
and interpretability.

</details>


### [196] [A new machine learning framework for occupational accidents forecasting with safety inspections integration](https://arxiv.org/abs/2507.00089)
*Aho Yapi,Pierre Latouche,Arnaud Guillin,Yan Bailly*

Main category: cs.LG

TL;DR: 本文提出了一种利用安全检查进行短期职业事故预测的框架，使用二元时间序列进行建模，以预测高风险时间段。


<details>
  <summary>Details</summary>
Motivation: 将常规安全检查数据转化为可操作的信息，帮助决策者优化资源分配和干预措施，以降低职业事故风险。

Method: 使用滑动窗口交叉验证和基于聚合周期指标的评估，训练并比较多种机器学习算法（如逻辑回归、基于树模型、神经网络等），最终证明LSTM网络在预测高风险周期上具有最佳表现。

Result: LSTM网络在预测高风险周期上实现了平衡准确率0.86，表明利用二元时间序列模型解析安全检查数据可有效预测事故风险。

Conclusion: 该方法将常规安全检查数据转化为每周清晰的风险评分，为决策者优化检查重点、干预计划和资源分配提供了实用工具，最大化安全投资的回报并降低事故发生。

Abstract: We propose a generic framework for short-term occupational accident
forecasting that leverages safety inspections and models accident occurrences
as binary time series. The approach generates daily predictions, which are then
aggregated into weekly safety assessments to better inform decision making. To
ensure the reliability and operational applicability of the forecasts, we apply
a sliding-window cross-validation procedure specifically designed for time
series data, combined with an evaluation based on aggregated period-level
metrics. Several machine learning algorithms, including logistic regression,
tree-based models, and neural networks, are trained and systematically compared
within this framework. Unlike the other approaches, the long short-term memory
(LSTM) network outperforms the other approaches and detects the upcoming
high-risk periods with a balanced accuracy of 0.86, confirming the robustness
of our methodology and demonstrating that a binary time series model can
anticipate these critical periods based on safety inspections. The proposed
methodology converts routine safety inspection data into clear weekly risk
scores, detecting the periods when accidents are most likely. Decision-makers
can integrate these scores into their planning tools to classify inspection
priorities, schedule targeted interventions, and funnel resources to the sites
or shifts classified as highest risk, stepping in before incidents occur and
getting the greatest return on safety investments.

</details>


### [197] [Generating Heterogeneous Multi-dimensional Data : A Comparative Study](https://arxiv.org/abs/2507.00090)
*Corbeau Michael,Claeys Emmanuelle,Serrurier Mathieu,Zaraté Pascale*

Main category: cs.LG

TL;DR: 本文比较了多种数据生成方法在消防员干预场景中的应用，包括随机抽样、Tabular VAE、标准GAN、条件表格生成对抗网络和扩散概率模型。


<details>
  <summary>Details</summary>
Motivation: 寻求优化消防员干预资源分配的全球性能，并研究生成方法在模拟不同场景中的作用。

Method: 通过使用多种生成方法生成数据，并使用领域特定指标和标准指标如Wasserstein距离对生成数据进行质量评估。

Result: 评估显示，不同方法在捕捉复杂关系、处理不平衡数据分布、以及符合初始统计分布方面有不同表现。

Conclusion: 对比多种生成方法后，研究为优化消防领域资源分配提供了更好的场景模拟选择和评估标准。

Abstract: Allocation of personnel and material resources is highly sensible in the case
of firefighter interventions. This allocation relies on simulations to
experiment with various scenarios. The main objective of this allocation is the
global optimization of the firefighters response. Data generation is then
mandatory to study various scenarios In this study, we propose to compare
different data generation methods. Methods such as Random Sampling, Tabular
Variational Autoencoders, standard Generative Adversarial Networks, Conditional
Tabular Generative Adversarial Networks and Diffusion Probabilistic Models are
examined to ascertain their efficacy in capturing the intricacies of
firefighter interventions. Traditional evaluation metrics often fall short in
capturing the nuanced requirements of synthetic datasets for real-world
scenarios. To address this gap, an evaluation of synthetic data quality is
conducted using a combination of domain-specific metrics tailored to the
firefighting domain and standard measures such as the Wasserstein distance.
Domain-specific metrics include response time distribution, spatial-temporal
distribution of interventions, and accidents representation. These metrics are
designed to assess data variability, the preservation of fine and complex
correlations and anomalies such as event with a very low occurrence, the
conformity with the initial statistical distribution and the operational
relevance of the synthetic data. The distribution has the particularity of
being highly unbalanced, none of the variables following a Gaussian
distribution, adding complexity to the data generation process.

</details>


### [198] [DFReg: A Physics-Inspired Framework for Global Weight Distribution Regularization in Neural Networks](https://arxiv.org/abs/2507.00101)
*Giovanni Ruggieri*

Main category: cs.LG

TL;DR: 提出DFReg，一种基于物理灵感的深度神经网络正则化方法，利用密度泛函理论改善网络的权重分布。


<details>
  <summary>Details</summary>
Motivation: 目前深度学习正则化方法，如Dropout和L2惩罚，存在局限性，需求更具全局视野的新方法。

Method: 通过密度泛函理论引入函数惩罚项，促进权重的平滑性、多样性和良好分布性，而无需改变网络结构或引入随机扰动。

Result: DFReg以全局方式增强了网络权重配置的规整性。

Conclusion: DFReg是一种独特而有效的正则化方法，为改进深度学习模型提供了新思路。

Abstract: We introduce DFReg, a physics-inspired regularization method for deep neural
networks that operates on the global distribution of weights. Drawing from
Density Functional Theory (DFT), DFReg applies a functional penalty to
encourage smooth, diverse, and well-distributed weight configurations. Unlike
traditional techniques such as Dropout or L2 decay, DFReg imposes global
structural regularity without architectural changes or stochastic
perturbations.

</details>


### [199] [Towards transparent and data-driven fault detection in manufacturing: A case study on univariate, discrete time series](https://arxiv.org/abs/2507.00102)
*Bernd Hofmann,Patrick Bruendl,Huong Giang Nguyen,Joerg Franke*

Main category: cs.LG

TL;DR: 本研究提出了一种结合机器学习与可解释性的新方法，用于工业故障检测，该方法在安全关键性工艺上达到了高准确率并通过专家评估验证了其解释性和相关性。


<details>
  <summary>Details</summary>
Motivation: 现代制造业要求高质量且一致的产品输出，传统质量控制方法在面对数据复杂性和变异性时存在局限性，而纯数据驱动方法的“黑箱”问题则限制了其对工业环境的接受度。因此，需要既能适应生产数据复杂性的，又具备可解释性的工业故障检测方法。

Method: 提出了一种透明的数据驱动型工业故障检测方法，该方法包括监督式多分类机器学习模型、Shapley Additive Explanations（后处理解释方法）以及基于领域的可视化技术。同时，提出了通过定量扰动分析和专家定性评估结合的模型解释评价方法。该系统被应用于针对安全关键性的铆接过程的故障检测。

Result: 本方法在故障检测中达到了95.9%的准确率，说明了模型的高效性，且通过定量选择性分析和专家评估验证了其解释性和相关性。

Conclusion: 这种人本驱动的方法提升了数据驱动故障检测系统的信任度和易解释性，对工业质量控制中的系统设计具有重要意义。

Abstract: Ensuring consistent product quality in modern manufacturing is crucial,
particularly in safety-critical applications. Conventional quality control
approaches, reliant on manually defined thresholds and features, lack
adaptability to the complexity and variability inherent in production data and
necessitate extensive domain expertise. Conversely, data-driven methods, such
as machine learning, demonstrate high detection performance but typically
function as black-box models, thereby limiting their acceptance in industrial
environments where interpretability is paramount. This paper introduces a
methodology for industrial fault detection, which is both data-driven and
transparent. The approach integrates a supervised machine learning model for
multi-class fault classification, Shapley Additive Explanations for post-hoc
interpretability, and a do-main-specific visualisation technique that maps
model explanations to operator-interpretable features. Furthermore, the study
proposes an evaluation methodology that assesses model explanations through
quantitative perturbation analysis and evaluates visualisations by qualitative
expert assessment. The approach was applied to the crimping process, a
safety-critical joining technique, using a dataset of univariate, discrete time
series. The system achieves a fault detection accuracy of 95.9 %, and both
quantitative selectivity analysis and qualitative expert evaluations confirmed
the relevance and inter-pretability of the generated explanations. This
human-centric approach is designed to enhance trust and interpretability in
data-driven fault detection, thereby contributing to applied system design in
industrial quality control.

</details>


### [200] [Graph Neural Networks in Wind Power Forecasting](https://arxiv.org/abs/2507.00105)
*Javier Castellano,Ignacio Villanueva*

Main category: cs.LG

TL;DR: 研究探讨了使用GNN进行风能预测的可行性，并与基于CNN的模型性能进行了对比。


<details>
  <summary>Details</summary>
Motivation: 了解图神经网络（GNN）在风能预测问题上的表现能力，尤其是在与传统卷积神经网络（CNN）基准模型的对比中。

Method: 选择了三个风电设施，利用五年的历史数据以及数值天气预报（NWP）变量作为预测指标，比较了GNN与CNN模型的预测性能。

Result: 某些GNN架构的预测性能可与表现最优的CNN基准模型相媲美。

Conclusion: GNN在风能预测问题上具有应用潜力，与CNN模型相比表现良好。

Abstract: We study the applicability of GNNs to the problem of wind energy forecasting.
We find that certain architectures achieve performance comparable to our best
CNN-based benchmark. The study is conducted on three wind power facilities
using five years of historical data. Numerical Weather Prediction (NWP)
variables were used as predictors, and models were evaluated on a 24 to 36 hour
ahead test horizon.

</details>


### [201] [Text-to-Level Diffusion Models With Various Text Encoders for Super Mario Bros](https://arxiv.org/abs/2507.00184)
*Jacob Schrum,Olivia Kilday,Emilio Salas,Bess Hagan,Reid Williams*

Main category: cs.LG

TL;DR: 研究探索了文本到游戏关卡生成中的扩散模型效果，提出了新的生成策略和评估方式，以提高生成关卡的多样性和可玩性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型用于无条件游戏关卡生成已有研究，但在文本驱动生成中的应用较少，如何改进使生成更实用化成为研究目标。

Method: 通过自动为已有关卡数据集分配描述性字幕，使用简单transformer模型和预训练文本编码器作为生成基础，比较输入输出字幕重合度并评估生成关卡的多样性和可玩性。

Result: 最佳扩散模型使用简单transformer模型进行文本嵌入，训练耗时少于使用复杂文本编码器的模型，且相比无条件扩散模型和竞品方法（如Five-Dollar Model和MarioGPT）结果更好。

Conclusion: 研究证明复杂语言模型并非文本到关卡生成的必要前提，提出的模型更高效且实用，并提供了设计者可使用的用户界面。

Abstract: Recent research shows how diffusion models can unconditionally generate
tile-based game levels, but use of diffusion models for text-to-level
generation is underexplored. There are practical considerations for creating a
usable model: caption/level pairs are needed, as is a text embedding model, and
a way of generating entire playable levels, rather than individual scenes. We
present strategies to automatically assign descriptive captions to an existing
level dataset, and train diffusion models using both pretrained text encoders
and simple transformer models trained from scratch. Captions are automatically
assigned to generated levels so that the degree of overlap between input and
output captions can be compared. We also assess the diversity and playability
of the resulting levels. Results are compared with an unconditional diffusion
model and a generative adversarial network, as well as the text-to-level
approaches Five-Dollar Model and MarioGPT. Notably, the best diffusion model
uses a simple transformer model for text embedding, and takes less time to
train than diffusion models employing more complex text encoders, indicating
that reliance on larger language models is not necessary. We also present a GUI
allowing designers to construct long levels from model-generated scenes.

</details>


### [202] [Beyond Sensor Data: Foundation Models of Behavioral Data from Wearables Improve Health Predictions](https://arxiv.org/abs/2507.00191)
*Eray Erturk,Fahad Kamran,Salar Abbaspourazad,Sean Jewell,Harsh Sharma,Yujie Li,Sinead Williamson,Nicholas J Foti,Joseph Futoma*

Main category: cs.LG

TL;DR: 研究用超过2.5B小时的穿戴数据开发基础模型以改进健康预测，尤其在行为信号如睡眠预测中表现优秀。


<details>
  <summary>Details</summary>
Motivation: 穿戴设备的行为信号与相关生理时间尺度对齐，能提供比低级别传感器数据更有用的信息，但尚未得到充分开发。

Method: 用162K个个体、超过2.5B小时数据设计并优化基础模型架构和标记策略，并通过57项健康任务测试其性能。

Result: 模型在行为驱动型任务中表现出色，如睡眠预测，并通过结合传感器原始数据提升表现。

Conclusion: 研究表明，基础模型需针对穿戴设备特性进行优化，并展示了其推动新型健康应用的潜力。

Abstract: Wearable devices record physiological and behavioral signals that can improve
health predictions. While foundation models are increasingly used for such
predictions, they have been primarily applied to low-level sensor data, despite
behavioral data often being more informative due to their alignment with
physiologically relevant timescales and quantities. We develop foundation
models of such behavioral signals using over 2.5B hours of wearable data from
162K individuals, systematically optimizing architectures and tokenization
strategies for this unique dataset. Evaluated on 57 health-related tasks, our
model shows strong performance across diverse real-world applications including
individual-level classification and time-varying health state prediction. The
model excels in behavior-driven tasks like sleep prediction, and improves
further when combined with representations of raw sensor data. These results
underscore the importance of tailoring foundation model design to wearables and
demonstrate the potential to enable new health applications.

</details>


### [203] [What Makes Local Updates Effective: The Role of Data Heterogeneity and Smoothness](https://arxiv.org/abs/2507.00195)
*Kumar Kshitij Patel*

Main category: cs.LG

TL;DR: 本文深入研究了分布式与联邦优化中的局部更新算法，特别是Local SGD，在数据异质性下的理论表现。


<details>
  <summary>Details</summary>
Motivation: 探讨Local SGD在不同数据异质性假设下是否能优于集中式或小批量方法，并揭示在异质环境中的成功条件。

Method: 建立基于共识误差的分析框架，研究多种局部更新算法的收敛边界与复杂性，并扩展到在线联邦学习中。

Result: 提供了收敛时间更精确的界限以及在线联邦学习下的基本后悔界限，阐明了局部更新具备优势的条件。

Conclusion: 论文不仅展示了Local SGD在异质性环境中的潜在优势，还作为分析此类环境中局部更新算法的理论指南。

Abstract: This thesis contributes to the theoretical understanding of local update
algorithms, especially Local SGD, in distributed and federated optimization
under realistic models of data heterogeneity. A central focus is on the bounded
second-order heterogeneity assumption, which is shown to be both necessary and
sufficient for local updates to outperform centralized or mini-batch methods in
convex and non-convex settings. The thesis establishes tight upper and lower
bounds in several regimes for various local update algorithms and characterizes
the min-max complexity of multiple problem classes. At its core is a
fine-grained consensus-error-based analysis framework that yields sharper
finite-time convergence bounds under third-order smoothness and relaxed
heterogeneity assumptions. The thesis also extends to online federated
learning, providing fundamental regret bounds under both first-order and bandit
feedback. Together, these results clarify when and why local updates offer
provable advantages, and the thesis serves as a self-contained guide for
analyzing Local SGD in heterogeneous environments.

</details>


### [204] [PPFL-RDSN: Privacy-Preserving Federated Learning-based Residual Dense Spatial Networks for Encrypted Lossy Image Reconstruction](https://arxiv.org/abs/2507.00230)
*Peilin He,James Joshi*

Main category: cs.LG

TL;DR: 文章提出了一个名为PPFL-RDSN的框架，通过结合联邦学习、本地差分隐私和模型水印技术，安全地从低分辨率图片重建高质量图像，解决了数据隐私和计算成本问题。


<details>
  <summary>Details</summary>
Motivation: 现有集中式的高质量图像重建方法面临隐私风险（如数据泄漏）及高计算开销的挑战，而在协作环境中需求强烈。

Method: 设计了Privacy-Preserving Federated Learning-based RDSN（PPFL-RDSN）框架，结合联邦学习、本地差分隐私和模型水印技术，在本地设备中保护数据隐私并确保模型真实性。

Result: 实验证明PPFL-RDSN性能达到与先进集中式方法相当的水平，同时降低了计算负担，并有效减少了安全和隐私问题。

Conclusion: PPFL-RDSN为计算机视觉中的安全和隐私保护协作应用提供了切实有效的解决方案。

Abstract: Reconstructing high-quality images from low-resolution inputs using Residual
Dense Spatial Networks (RDSNs) is crucial yet challenging, particularly in
collaborative scenarios where centralized training poses significant privacy
risks, including data leakage and inference attacks, as well as high
computational costs. We propose a novel Privacy-Preserving Federated
Learning-based RDSN (PPFL-RDSN) framework specifically tailored for lossy image
reconstruction. PPFL-RDSN integrates Federated Learning (FL), local
differential privacy, and robust model watermarking techniques, ensuring data
remains secure on local devices, safeguarding sensitive information, and
maintaining model authenticity without revealing underlying data. Empirical
evaluations show that PPFL-RDSN achieves comparable performance to the
state-of-the-art centralized methods while reducing computational burdens, and
effectively mitigates security and privacy vulnerabilities, making it a
practical solution for secure and privacy-preserving collaborative computer
vision applications.

</details>


### [205] [Interpretable AI for Time-Series: Multi-Model Heatmap Fusion with Global Attention and NLP-Generated Explanations](https://arxiv.org/abs/2507.00234)
*Jiztom Kavalakkatt Francis,Matthew J Darr*

Main category: cs.LG

TL;DR: 提出了一种结合ResNet和2D Transformer的热图与输入显著性权重的方法，实现模型解释性的改进，解决了空间-时间错位问题并应用于医疗保健和工业监测领域。


<details>
  <summary>Details</summary>
Motivation: 现有方法在全球上下文捕捉和局部精度上有限，尤其在安全关键领域阻碍了可操作性，需要一种改进的解释框架。

Method: 提出一种新框架，结合ResNet与Transformer的热图，使用全局加权输入显著性实现全空间-时间对齐，同时引入NLP模块将热图转化为领域叙事。

Result: 在PhysioNet数据集上Achieved 94.1%准确度（F1 0.93），在UCI能量应用数据集上减少了RMSE至0.28 kWh（R2 0.95），并在热图语言生成中取得BLEU-4和ROUGE-L指标分别为0.586和0.650。

Conclusion: 此方法解决了解释性技术输出与利益相关者理解之间的鸿沟，为透明且时间同步的决策提供了可扩展解决方案。

Abstract: In this paper, we present a novel framework for enhancing model
interpretability by integrating heatmaps produced separately by ResNet and a
restructured 2D Transformer with globally weighted input saliency. We address
the critical problem of spatial-temporal misalignment in existing
interpretability methods, where convolutional networks fail to capture global
context and Transformers lack localized precision - a limitation that impedes
actionable insights in safety-critical domains like healthcare and industrial
monitoring. Our method merges gradient-weighted activation maps (ResNet) and
Transformer attention rollout into a unified visualization, achieving full
spatial-temporal alignment while preserving real-time performance. Empirical
evaluations on clinical (ECG arrhythmia detection) and industrial (energy
consumption prediction) datasets demonstrate significant improvements: the
hybrid framework achieves 94.1% accuracy (F1 0.93) on the PhysioNet dataset and
reduces regression error to RMSE = 0.28 kWh (R2 = 0.95) on the UCI Energy
Appliance dataset-outperforming standalone ResNet, Transformer, and
InceptionTime baselines by 3.8-12.4%. An NLP module translates fused heatmaps
into domain-specific narratives (e.g., "Elevated ST-segment between 2-4 seconds
suggests myocardial ischemia"), validated via BLEU-4 (0.586) and ROUGE-L
(0.650) scores. By formalizing interpretability as causal fidelity and
spatial-temporal alignment, our approach bridges the gap between technical
outputs and stakeholder understanding, offering a scalable solution for
transparent, time-aware decision-making.

</details>


### [206] [Gym4ReaL: A Suite for Benchmarking Real-World Reinforcement Learning](https://arxiv.org/abs/2507.00257)
*Davide Salaorni,Vincenzo De Paola,Samuele Delpero,Giovanni Dispoto,Paolo Bonetti,Alessio Russo,Giuseppe Calcagno,Francesco Trovò,Matteo Papini,Alberto Maria Metelli,Marco Mussi,Marcello Restelli*

Main category: cs.LG

TL;DR: 本文推出了一个名为Gym4ReaL的环境套件，用于测试和开发能够适应真实世界场景的强化学习算法。


<details>
  <summary>Details</summary>
Motivation: 当前的强化学习基准通常不考虑真实世界的复杂性，如非平稳性和部分可观测，而要将强化学习应用于真实世界，这些挑战至关重要。

Method: 提出了Gym4ReaL，这是一组更贴近真实世界的多样化任务环境，用于测试和改进强化学习算法在现实条件下的表现。

Result: 实验结果表明，标准强化学习算法在这些环境下的表现可以与基于规则的基准竞争，但仍需开发新方法更好地应对复杂任务。

Conclusion: Gym4ReaL提供了一个重要的平台，推动了能够适应真实世界复杂环境的强化学习算法的进一步发展。

Abstract: In recent years, \emph{Reinforcement Learning} (RL) has made remarkable
progress, achieving superhuman performance in a wide range of simulated
environments. As research moves toward deploying RL in real-world applications,
the field faces a new set of challenges inherent to real-world settings, such
as large state-action spaces, non-stationarity, and partial observability.
Despite their importance, these challenges are often underexplored in current
benchmarks, which tend to focus on idealized, fully observable, and stationary
environments, often neglecting to incorporate real-world complexities
explicitly. In this paper, we introduce \texttt{Gym4ReaL}, a comprehensive
suite of realistic environments designed to support the development and
evaluation of RL algorithms that can operate in real-world scenarios. The suite
includes a diverse set of tasks that expose algorithms to a variety of
practical challenges. Our experimental results show that, in these settings,
standard RL algorithms confirm their competitiveness against rule-based
benchmarks, motivating the development of new methods to fully exploit the
potential of RL to tackle the complexities of real-world tasks.

</details>


### [207] [Who Should I Listen To? Adaptive Collaboration in Personalized Federated Learning](https://arxiv.org/abs/2507.00259)
*Amr Abourayya,Jens Kleesiek,Bharat Rao,Michael Kamp*

Main category: cs.LG

TL;DR: 本文提出了一种名为FEDMOSAIC的自适应协作个性化联邦学习方法，通过在共享无标注数据集上的预测交换，实现精细化的信任决策，从而在非均匀数据情境中提升性能。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，由于数据的异质性，现有的个性化方法往往无法优于本地或集中的基线方法。需要一种能够更灵活适应数据结构的协作机制。

Method: 提出了FEDMOSAIC方法，通过在共享的无标签数据集上交换预测，允许客户根据私人数据和公共数据的结果一致性，动态调整损失权重，并根据单个样本的置信度进行协作伪标签贡献。

Result: FEDMOSAIC在多个非独立同分布的场景中优于当前最先进的个性化联邦学习方法，并在标准假设下提供收敛保证。

Conclusion: 在非独立同分布数据场景中，数据感知的协作方式能够显著提升个性化联邦学习的鲁棒性和有效性。

Abstract: Data heterogeneity is a central challenge in federated learning, and
personalized federated learning (PFL) aims to address it by tailoring models to
each client's distribution. Yet many PFL methods fail to outperform local or
centralized baselines, suggesting a mismatch between the collaboration they
enforce and the structure of the data. We propose an approach based on adaptive
collaboration, where clients decide adaptively not only how much to rely on
others, but also whom to trust at the level of individual examples. We
instantiate this principle in FEDMOSAIC, a federated co-training method in
which clients exchange predictions over a shared unlabeled dataset. This
enables fine-grained trust decisions that are difficult to achieve with
parameter sharing alone. Each client adjusts its loss weighting based on the
agreement between private and public data, and contributes to global
pseudo-labels in proportion to its estimated per-example confidence.
Empirically, FEDMOSAIC improves upon state-of-the-art PFL methods across
diverse non-IID settings, and we provide convergence guarantees under standard
assumptions. Our results demonstrate the potential of data-aware collaboration
for robust and effective personalization.

</details>


### [208] [Examining Reject Relations in Stimulus Equivalence Simulations](https://arxiv.org/abs/2507.00265)
*Alexis Carrillo,Asieh Abolpour Mofrad,Anis Yazidi,Moises Betancort*

Main category: cs.LG

TL;DR: 研究探讨了拒绝关系在刺激等价性形成中的作用，比较了人工神经网络与概率学习代理的性能，发现拒绝关系对等价性测试有影响，但神经网络可能更倾向于关联性学习。


<details>
  <summary>Details</summary>
Motivation: 探讨人工神经网络是否能够展示刺激等价性形成，还是其性能仅反映关联性学习。

Method: 使用了前馈神经网络(FFNs)、BERT、GPT等模型，在18种匹配条件下进行MTS模拟，测试了不同训练结构、关系类型与负面比较选择对性能的影响，同时引入概率代理作为对照。

Result: 拒绝关系对代理表现有显著影响，某些情况下带有拒绝关系的负面偏差比较能提高等价性测试的准确率，但与概率模型相似。

Conclusion: 人工神经网络可能更倾向于利用关联性策略，而非真正的刺激等价性，这表明对拒绝关系与严格性标准的明确考虑是必要的。

Abstract: Simulations offer a valuable tool for exploring stimulus equivalence (SE),
yet the potential of reject relations to disrupt the assessment of equivalence
class formation is contentious. This study investigates the role of reject
relations in the acquisition of stimulus equivalence using computational
models. We examined feedforward neural networks (FFNs), bidirectional encoder
representations from transformers (BERT), and generative pre-trained
transformers (GPT) across 18 conditions in matching-to-sample (MTS)
simulations. Conditions varied in training structure (linear series,
one-to-many, and many-to-one), relation type (select-only, reject-only, and
select-reject), and negative comparison selection (standard and biased). A
probabilistic agent served as a benchmark, embodying purely associative
learning. The primary goal was to determine whether artificial neural networks
could demonstrate equivalence class formation or whether their performance
reflected associative learning. Results showed that reject relations influenced
agent performance. While some agents achieved high accuracy on equivalence
tests, particularly with reject relations and biased negative comparisons, this
performance was comparable to the probabilistic agent. These findings suggest
that artificial neural networks, including transformer models, may rely on
associative strategies rather than SE. This underscores the need for careful
consideration of reject relations and more stringent criteria in computational
models of equivalence.

</details>


### [209] [Double Q-learning for Value-based Deep Reinforcement Learning, Revisited](https://arxiv.org/abs/2507.00275)
*Prabhat Nagarajan,Martha White,Marlos C. Machado*

Main category: cs.LG

TL;DR: 文章探讨了如何通过深度双重Q学习(DDQL)在深度强化学习中减少过估计问题，并与Double DQN进行对比分析。结果表明，DDQL减少了过估计，在57款Atari 2600游戏中表现优于Double DQN。


<details>
  <summary>Details</summary>
Motivation: Q学习中普遍存在的过估计问题会影响模型性能，而现有的Double DQN没有完全实现Double Q学习的核心理念，因此需要探索更有效的方法。

Method: 提出并研究了深度双重Q学习(DDQL)算法，与Double DQN对比，分析网络结构、重放比率和小批量采样策略等多个方面。

Result: 实验结果显示，DDQL能够显著减少Q值的过估计问题，并且在57款Atari 2600游戏上平均表现优于Double DQN，且无需额外的超参数。

Conclusion: DDQL不仅在理论上改进了现有方法的不足，并在实践中展示了优于Double DQN的潜力，是一种减少深度强化学习中过估计问题的有效方法。

Abstract: Overestimation is pervasive in reinforcement learning (RL), including in
Q-learning, which forms the algorithmic basis for many value-based deep RL
algorithms. Double Q-learning is an algorithm introduced to address
Q-learning's overestimation by training two Q-functions and using both to
de-correlate action-selection and action-evaluation in bootstrap targets.
Shortly after Q-learning was adapted to deep RL in the form of deep Q-networks
(DQN), Double Q-learning was adapted to deep RL in the form of Double DQN.
However, Double DQN only loosely adapts Double Q-learning, forgoing the
training of two different Q-functions that bootstrap off one another. In this
paper, we study algorithms that adapt this core idea of Double Q-learning for
value-based deep RL. We term such algorithms Deep Double Q-learning (DDQL). Our
aim is to understand whether DDQL exhibits less overestimation than Double DQN
and whether performant instantiations of DDQL exist. We answer both questions
affirmatively, demonstrating that DDQL reduces overestimation and outperforms
Double DQN in aggregate across 57 Atari 2600 games, without requiring
additional hyperparameters. We also study several aspects of DDQL, including
its network architecture, replay ratio, and minibatch sampling strategy.

</details>


### [210] [Structure-preserving Lift & Learn: Scientific machine learning for nonlinear conservative partial differential equations](https://arxiv.org/abs/2507.00301)
*Harsh Sharma,Juan Diego Draxl Giannoni,Boris Kramer*

Main category: cs.LG

TL;DR: 本文提出了一种基于变量提升变换的科学机器学习方法，用于非线性偏微分方程的结构保持降阶建模。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种能够保留偏微分方程非线性特征及守恒定律的降阶建模方法。

Method: 通过能量平衡策略，将非线性提升为等价的二次形式，并结合混合学习算法，导出二次降阶项和线性降阶算子。

Result: 方法用于一维波动方程、二维正弦-戈登方程等，实验中表现出准确性和计算效率均具有竞争力。

Conclusion: 提出的Lift & Learn方法提供了一种在物理约束下进行高效建模的创新工具，适用于非线性偏微分问题。

Abstract: This work presents structure-preserving Lift & Learn, a scientific machine
learning method that employs lifting variable transformations to learn
structure-preserving reduced-order models for nonlinear partial differential
equations (PDEs) with conservation laws. We propose a hybrid learning approach
based on a recently developed energy-quadratization strategy that uses
knowledge of the nonlinearity at the PDE level to derive an equivalent
quadratic lifted system with quadratic system energy. The lifted dynamics
obtained via energy quadratization are linear in the old variables, making
model learning very effective in the lifted setting. Based on the lifted
quadratic PDE model form, the proposed method derives quadratic reduced terms
analytically and then uses those derived terms to formulate a constrained
optimization problem to learn the remaining linear reduced operators in a
structure-preserving way. The proposed hybrid learning approach yields
computationally efficient quadratic reduced-order models that respect the
underlying physics of the high-dimensional problem. We demonstrate the
generalizability of quadratic models learned via the proposed
structure-preserving Lift & Learn method through three numerical examples: the
one-dimensional wave equation with exponential nonlinearity, the
two-dimensional sine-Gordon equation, and the two-dimensional
Klein-Gordon-Zakharov equations. The numerical results show that the proposed
learning approach is competitive with the state-of-the-art structure-preserving
data-driven model reduction method in terms of both accuracy and computational
efficiency.

</details>


### [211] [MamNet: A Novel Hybrid Model for Time-Series Forecasting and Frequency Pattern Analysis in Network Traffic](https://arxiv.org/abs/2507.00304)
*Yujun Zhang,Runlong Li,Xiaoxiang Liang,Xinhao Yang,Tian Su,Bo Liu,Yan Zhou*

Main category: cs.LG

TL;DR: MamNet是一种结合时域建模与频域特征提取的新型网络流量预测与异常检测模型，在多个基准数据集上表现显著优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 监测网络流量的异常波动以检测潜在安全威胁和系统故障，提高网络安全性和流量管理能力。

Method: 模型包含两部分：通过Mamba模块进行时域建模以捕获长期依赖性，并通过傅里叶变换作频域特征提取，然后在特征融合层整合多尺度信息，以增强异常检测能力。

Result: 在UNSW-NB15和CAIDA数据集上，MamNet在准确率、召回率和F1-Score方面均优于主流模型，尤其在复杂流量模式和长期趋势检测中表现出2%-4%的性能提升。

Conclusion: MamNet能有效地跨时间尺度捕捉网络流量异常，适用于网络安全与流量管理的异常检测任务，未来可通过整合外部网络事件信息进一步优化适应性和稳定性。

Abstract: The abnormal fluctuations in network traffic may indicate potential security
threats or system failures. Therefore, efficient network traffic prediction and
anomaly detection methods are crucial for network security and traffic
management. This paper proposes a novel network traffic prediction and anomaly
detection model, MamNet, which integrates time-domain modeling and
frequency-domain feature extraction. The model first captures the long-term
dependencies of network traffic through the Mamba module (time-domain
modeling), and then identifies periodic fluctuations in the traffic using
Fourier Transform (frequency-domain feature extraction). In the feature fusion
layer, multi-scale information is integrated to enhance the model's ability to
detect network traffic anomalies. Experiments conducted on the UNSW-NB15 and
CAIDA datasets demonstrate that MamNet outperforms several recent mainstream
models in terms of accuracy, recall, and F1-Score. Specifically, it achieves an
improvement of approximately 2% to 4% in detection performance for complex
traffic patterns and long-term trend detection. The results indicate that
MamNet effectively captures anomalies in network traffic across different time
scales and is suitable for anomaly detection tasks in network security and
traffic management. Future work could further optimize the model structure by
incorporating external network event information, thereby improving the model's
adaptability and stability in complex network environments.

</details>


### [212] [Open-ended Scientific Discovery via Bayesian Surprise](https://arxiv.org/abs/2507.00310)
*Dhruv Agarwal,Bodhisattwa Prasad Majumder,Reece Adamson,Megha Chakravorty,Satvika Reddy Gavireddy,Aditya Parashar,Harshit Surana,Bhavana Dalvi Mishra,Andrew McCallum,Ashish Sabharwal,Peter Clark*

Main category: cs.LG

TL;DR: 本文提出AutoDS方法，以贝叶斯惊奇驱动开创性的科学探索，通过蒙特卡洛树搜索（MCTS）和渐进扩展策略优化数据驱动发现过程，在21个现实数据集中的表现显示显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的自主科学发现研究多依赖人类指定研究问题，限制了AI在开放性探索中的自主性。现有方法在多样性启发或人为兴趣代理方面存在缺陷，难以高效导航庞大的假设空间。

Method: 提出AutoDS方法，用贝叶斯惊奇量化从语言模型先验到后验知识的改变，利用蒙特卡洛树搜索和渐进搜索策略，其中以“惊奇值”作为奖励函数来高效探索嵌套假设空间。

Result: 在涵盖生物学、经济学、金融学及行为科学的21个数据集上测试，AutoDS在固定预算下比竞争方法多产出5-29%的“惊奇”发现。

Conclusion: AutoDS不仅在量化“惊奇”发现中表现突出，人类评估也表明其发现对领域专家具备真实的惊奇性，为建立开放性自主科学发现系统迈出了重要一步。

Abstract: The promise of autonomous scientific discovery (ASD) hinges not only on
answering questions, but also on knowing which questions to ask. Most recent
works in ASD explore the use of large language models (LLMs) in goal-driven
settings, relying on human-specified research questions to guide hypothesis
generation. However, scientific discovery may be accelerated further by
allowing the AI system to drive exploration by its own criteria. The few
existing approaches in open-ended ASD select hypotheses based on diversity
heuristics or subjective proxies for human interestingness, but the former
struggles to meaningfully navigate the typically vast hypothesis space, and the
latter suffers from imprecise definitions. This paper presents AutoDS -- a
method for open-ended ASD that instead drives scientific exploration using
Bayesian surprise. Here, we quantify the epistemic shift from the LLM's prior
beliefs about a hypothesis to its posterior beliefs after gathering
experimental results. To efficiently explore the space of nested hypotheses,
our method employs a Monte Carlo tree search (MCTS) strategy with progressive
widening using surprisal as the reward function. We evaluate AutoDS in the
setting of data-driven discovery across 21 real-world datasets spanning domains
such as biology, economics, finance, and behavioral science. Our results
demonstrate that under a fixed budget, AutoDS substantially outperforms
competitors by producing 5--29\% more discoveries deemed surprising by the LLM.
Our human evaluation further finds that two-thirds of AutoDS discoveries are
surprising to the domain experts, suggesting this is an important step forward
towards building open-ended ASD systems.

</details>


### [213] [$μ^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation](https://arxiv.org/abs/2507.00316)
*Siyou Li,Pengyao Qin,Huanan Wu,Dong Nie,Arun J. Thirunavukarasu,Juntao Yu,Le Zhang*

Main category: cs.LG

TL;DR: 提出了一个新的多尺度多模态语言模型（$u^2$LLM）用于自动放射学报告生成（RRG），并通过引入$u^2$Tokenizer和DPO优化提高了结果质量，在多种数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决自动放射学报告生成中的复杂信息提取和生成质量评估难题，提高诊断和管理建议的效率和准确性。

Method: 引入多尺度多模态语言模型$u^2$LLM，使用${u}^2$Tokenizer作为中间层，融合多模态特征，并通过GREEN-RedLlama指导的直接偏好优化（DPO）提升报告生成质量。

Result: 在四个CT图像与报告数据集上的实验结果显示，$u^2$LLM的表现优于现有方法。

Conclusion: $u^2$LLM展示了其在有限数据下进行自动报告生成任务的潜力，为临床影像的高效诊断提供了可能的解决方案。

Abstract: Automated radiology report generation (RRG) aims to produce detailed textual
reports from clinical imaging, such as computed tomography (CT) scans, to
improve the accuracy and efficiency of diagnosis and provision of management
advice. RRG is complicated by two key challenges: (1) inherent complexity in
extracting relevant information from imaging data under resource constraints,
and (2) difficulty in objectively evaluating discrepancies between
model-generated and expert-written reports. To address these challenges, we
propose $\mu^2$LLM, a $\underline{\textbf{mu}}$ltiscale
$\underline{\textbf{mu}}$ltimodal large language models for RRG tasks. The
novel ${\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal
features from the multiscale visual tokenizer and the text tokenizer, then
enhances report generation quality through direct preference optimization
(DPO), guided by GREEN-RedLlama. Experimental results on four large CT
image-report medical datasetdemonstrate that our method outperforms existing
approaches, highlighting the potential of our fine-tuned $\mu^2$LLMs on limited
data for RRG tasks.

</details>


### [214] [Exploring Theory-Laden Observations in the Brain Basis of Emotional Experience](https://arxiv.org/abs/2507.00320)
*Christiana Westlin,Ashutosh Singh,Deniz Erdogmus,Georgios Stratis,Lisa Feldman Barrett*

Main category: cs.LG

TL;DR: 本文重新分析一项基于传统情感分类学导向的研究，发现情感类别内存在显著的脑图案差异，而不是原研究所报道的固定映射关系。


<details>
  <summary>Details</summary>
Motivation: 质疑现有情感研究假设，即认为情感类别具有固定的生物和心理学分类学。

Method: 在原研究数据的基础上，按照新的情感类别观点进行再分析，不假设数据的方差结构。

Result: 发现显著的个体脑模式差异，未观察到原研究报告的固定映射关系。

Conclusion: 提出科学研究需要多种分析方法的验证以避免单一假设的误导性结论。

Abstract: In the science of emotion, it is widely assumed that folk emotion categories
form a biological and psychological typology, and studies are routinely
designed and analyzed to identify emotion-specific patterns. This approach
shapes the observations that studies report, ultimately reinforcing the
assumption that guided the investigation. Here, we reanalyzed data from one
such typologically-guided study that reported mappings between individual brain
patterns and group-averaged ratings of 34 emotion categories. Our reanalysis
was guided by an alternative view of emotion categories as populations of
variable, situated instances, and which predicts a priori that there will be
significant variation in brain patterns within a category across instances.
Correspondingly, our analysis made minimal assumptions about the structure of
the variance present in the data. As predicted, we did not observe the original
mappings and instead observed significant variation across individuals. These
findings demonstrate how starting assumptions can ultimately impact scientific
conclusions and suggest that a hypothesis must be supported using multiple
analytic methods before it is taken seriously.

</details>


### [215] [Data-Driven Exploration for a Class of Continuous-Time Linear--Quadratic Reinforcement Learning Problems](https://arxiv.org/abs/2507.00358)
*Yilie Huang,Xun Yu Zhou*

Main category: cs.LG

TL;DR: 研究了一种基于强化学习解决连续时间随机线性-二次(LQ)控制问题的方法，提出了一种自适应探索机制，并在数值实验中显示其相比非自适应方法有更高效率和更优结果。


<details>
  <summary>Details</summary>
Motivation: 传统的探索策略（如固定的或确定性的探索计划）在需要广泛调试且忽略学习进展时效率较低，因此研究更灵活和高效的探索方法具有实际价值。

Method: 提出了一种模型无关的、自适应的数据驱动探索机制，通过对critic调整熵正则化和对actor调整策略方差，动态实现探索过程的优化。

Result: 该方法在灵活性提升的同时保持了与当前最优的模型无关结果一致的次线性后悔界，并在数值实验中显示相比非自适应方法具备更快的收敛速度和更低的后悔值。

Conclusion: 自适应探索机制在提升学习效率和结果性能方面的潜力得到了验证，为强化学习在LQ控制问题中的应用提供了新的思路。

Abstract: We study reinforcement learning (RL) for the same class of continuous-time
stochastic linear--quadratic (LQ) control problems as in
\cite{huang2024sublinear}, where volatilities depend on both states and
controls while states are scalar-valued and running control rewards are absent.
We propose a model-free, data-driven exploration mechanism that adaptively
adjusts entropy regularization by the critic and policy variance by the actor.
Unlike the constant or deterministic exploration schedules employed in
\cite{huang2024sublinear}, which require extensive tuning for implementations
and ignore learning progresses during iterations, our adaptive exploratory
approach boosts learning efficiency with minimal tuning. Despite its
flexibility, our method achieves a sublinear regret bound that matches the
best-known model-free results for this class of LQ problems, which were
previously derived only with fixed exploration schedules. Numerical experiments
demonstrate that adaptive explorations accelerate convergence and improve
regret performance compared to the non-adaptive model-free and model-based
counterparts.

</details>


### [216] [MoNE: Replacing Redundant Experts with Lightweight Novices for Structured Pruning of MoE](https://arxiv.org/abs/2507.00390)
*Geng Zhang,Yuxuan Han,Yuxuan Lou,Wangbo Zhao,Yiqi Zhang,Yang You*

Main category: cs.LG

TL;DR: 提出了一种针对MoE模型的专家裁剪方法，称为MoNE，通过引入轻量级新手替代冗余专家，显著减少内存使用，同时最小化性能下降。


<details>
  <summary>Details</summary>
Motivation: 传统的MoE模型由于需要保留所有专家在内存中，导致内存开销巨大，而现有的结构化裁剪方法在模型架构、校准数据和样本数量等方面表现不稳定。

Method: 通过分析专家的访问频率和输出方差，识别并裁剪不重要的专家，用轻量级新手模型替代它们，保持输出的无偏估计，并设计了一种稳健的专家裁剪算法。

Result: 实验表明，在9个任务的零样本评估中，MoNE在25%和50%的裁剪率下分别提升了2.71和3.61的平均准确率，显著优于其他方法。

Conclusion: MoNE提出了一种新的专家裁剪思路，在减少内存开销的同时，实现了高效、鲁棒且性能稳定的模型压缩，验证了其在多个任务中的优越性和实用性。

Abstract: Mixture-of-Experts (MoE) enables efficient scaling of large language models
by activating only a subset of experts per input token. However, deploying
MoE-based models incurs significant memory overhead due to the need to retain
all experts in memory. While structured pruning is promising to reduce memory
costs, existing methods often show suboptimal performance and unstable
degradation in three dimensions: model architectures, calibration data sources,
and calibration sample sizes. This paper proposes
Mixture-of-Novices-and-Experts (MoNE), a novel expert pruning method that
replaces redundant experts with lightweight novices to achieve effective and
robust model compression. MoNE evaluates expert redundancy based on two
metrics: access frequency and output variance. Experts exhibiting low usage and
stable outputs are pruned and replaced with lightweight novices-unbiased
estimations of their original outputs-minimizing performance degradation.
Extensive experiments demonstrate that MoNE consistently outperforms baseline
methods with minimal accuracy degradation across the three dimensions,
confirming its effectiveness and robustness. Notably, it improves the average
zero shot accuracy across nine downstream tasks by up to 2.71 under 25\%
pruning ratio and 3.61 under 50\% pruning. The code is available at
https://github.com/zxgx/mode-pd.

</details>


### [217] [HelixPipe: Efficient Distributed Training of Long Sequence Transformers with Attention Parallel Pipeline Parallelism](https://arxiv.org/abs/2507.00394)
*Geng Zhang,Shenggan Cheng,Xuanlei Zhao,Ziming Liu,Yang You*

Main category: cs.LG

TL;DR: 提出了一种新的流水线并行方法HelixPipe，用于长序列Transformer模型训练，优化了性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的流水线并行方法在面对Transformer长序列中的二次方注意力计算和高内存开销时表现不佳。

Method: 引入注意力并行分区和两层先进后出的微批量调度，同时使用重新计算机制和分块MLP减少内存碎片化并支持更长序列。

Result: 实验证明，HelixPipe在训练长序列模型时性能和可扩展性超过现有方法，尤其在7B模型及128k序列长度和64个H20 GPU上实现了26%的速度提升。

Conclusion: HelixPipe在吞吐量和可扩展性方面优于现有方法，适合长序列Transformer训练，代码已公开。

Abstract: As transformer sequence lengths grow, existing pipeline parallelisms incur
suboptimal performance due to the quadratic attention computation and the
substantial memory overhead. To relieve these challenges, we propose HelixPipe,
a novel pipeline parallelism for long sequence transformer training. First,
HelixPipe introduces attention parallel partition, which schedules attention
computations of different micro batches across different pipeline stages in
parallel, reducing pipeline bubbles. Second, it employs a two-fold
first-in-last-out micro batch schedule to balance memory usage and overlap
communication with computation. Additionally, HelixPipe utilizes recomputation
without attention and chunked MLP to mitigate fragmentation and enable longer
sequences. Experiments demonstrate that HelixPipe gains increasing advantages
with longer sequence lengths, and outperforms existing methods in throughput
and scalability across varying pipeline sizes, model sizes, and cluster
configurations. Notably, it achieves a 26\% speedup over baseline methods when
training a 7B model with 128k sequence length on 64 H20 GPUs. Code is available
at https://github.com/code-tunnel/Megatron-LM/tree/dev.

</details>


### [218] [Diffusion Disambiguation Models for Partial Label Learning](https://arxiv.org/abs/2507.00411)
*Jinfu Fan,Xiaohui Zhong,Kangrui Ren,Jiangnan Li,Linqing Huang*

Main category: cs.LG

TL;DR: 提出了一种在部分标签学习 (PLL) 场景中使用扩散模型消除标签噪声的方法。


<details>
  <summary>Details</summary>
Motivation: 在实际机器学习中，从含糊标签中学习是一大难题。本研究旨在改进部分标签学习，通过扩散模型生成和去噪过程提高分类性能。

Method: 提出扩散去噪模型 (DDMP)，利用实例与标签间的潜在关联构建伪清洁标签，同时引入过渡矩阵动态估计潜在真值标签。

Result: 通过逐步优化生成的真实标签，提高了分类器性能，实验验证了DDMP在部分标签学习中的有效性。

Conclusion: DDMP在与传统PLL方法比较中表现出优越性，适合多种实际场景中的标签去歧义问题。

Abstract: Learning from ambiguous labels is a long-standing problem in practical
machine learning applications. The purpose of \emph{partial label learning}
(PLL) is to identify the ground-truth label from a set of candidate labels
associated with a given instance. Inspired by the remarkable performance of
diffusion models in various generation tasks, this paper explores their
potential to denoise ambiguous labels through the reverse denoising process.
Therefore, this paper reformulates the label disambiguation problem from the
perspective of generative models, where labels are generated by iteratively
refining initial random guesses. This perspective enables the diffusion model
to learn how label information is generated stochastically. By modeling the
generation uncertainty, we can use the maximum likelihood estimate of the label
for classification inference. However, such ambiguous labels lead to a mismatch
between instance and label, which reduces the quality of generated data. To
address this issue, this paper proposes a \emph{diffusion disambiguation model
for PLL} (DDMP), which first uses the potential complementary information
between instances and labels to construct pseudo-clean labels for initial
diffusion training. Furthermore, a transition-aware matrix is introduced to
estimate the potential ground-truth labels, which are dynamically updated
during the diffusion generation. During training, the ground-truth label is
progressively refined, improving the classifier. Experiments show the advantage
of the DDMP and its suitability for PLL.

</details>


### [219] [Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows](https://arxiv.org/abs/2507.00425)
*Ruixiang Zhang,Shuangfei Zhai,Jiatao Gu,Yizhe Zhang,Huangjie Zheng,Tianrong Chen,Miguel Angel Bautista,Josh Susskind,Navdeep Jaitly*

Main category: cs.LG

TL;DR: 本文提出了一种新的语言建模框架TarFlowLM，从离散的token模型转向连续的潜变量空间，通过基于变换器的自回归归一化流实现这种建模方式。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型依赖离散token、单向上下文及单次解码，局限了某些建模灵活性，因此作者尝试探索新的设计空间。

Method: 采用连续潜变量建模语言，通过变换器的自回归归一化流实现；模型支持全局双向上下文、块式生成以及层次化多次生成，并引入混合基础的耦合变换以捕捉潜变量中的复杂依赖关系。

Result: 语言建模基准测试中，TarFlowLM展示出强大的似然性能，并突出其灵活的建模能力。

Conclusion: 该框架不仅实现了传统离散自回归模型的类似理论基础，还在一定程度上突破了传统语言建模的局限性，提供了新型建模可能性。

Abstract: Autoregressive models have driven remarkable progress in language modeling.
Their foundational reliance on discrete tokens, unidirectional context, and
single-pass decoding, while central to their success, also inspires the
exploration of a design space that could offer new axes of modeling
flexibility. In this work, we explore an alternative paradigm, shifting
language modeling from a discrete token space to a continuous latent space. We
propose a novel framework TarFlowLM, that employs transformer-based
autoregressive normalizing flows to model these continuous representations.
This approach unlocks substantial flexibility, enabling the construction of
models that can capture global bi-directional context through stacked,
alternating-direction autoregressive transformations, support block-wise
generation with flexible token patch sizes, and facilitate a hierarchical
multi-pass generation process. We further propose new mixture-based coupling
transformations designed to capture complex dependencies within the latent
space shaped by discrete data, and demonstrate theoretical connections to
conventional discrete autoregressive models. Extensive experiments on language
modeling benchmarks demonstrate strong likelihood performance and highlight the
flexible modeling capabilities inherent in our framework.

</details>


### [220] [A Recipe for Causal Graph Regression: Confounding Effects Revisited](https://arxiv.org/abs/2507.00440)
*Yujia Yin,Tianyi Qu,Zihao Wang,Yifan Chen*

Main category: cs.LG

TL;DR: 本文研究了解决图因果回归（CGR）问题，通过对现有因果图分类技术进行扩展，提出适用于回归任务的新方法，并通过对比学习实现了因果干预技术推广。


<details>
  <summary>Details</summary>
Motivation: 现有因果图学习（CGL）技术在分类任务中已有较多应用，但面临更具挑战性的回归任务仍然是一个空白。

Method: 通过反思混杂因素在图级回归中的预测作用，并采用对比学习的方法将因果干预技术从分类推广到回归情境。

Result: 实验表明，该方法在多个图OOD基准数据集上具备有效性。

Conclusion: 提出的方法为图回归任务中的因果学习提供了新思路，增强了在OOD场景下的泛化能力。

Abstract: Through recognizing causal subgraphs, causal graph learning (CGL) has risen
to be a promising approach for improving the generalizability of graph neural
networks under out-of-distribution (OOD) scenarios. However, the empirical
successes of CGL techniques are mostly exemplified in classification settings,
while regression tasks, a more challenging setting in graph learning, are
overlooked. We thus devote this work to tackling causal graph regression (CGR);
to this end we reshape the processing of confounding effects in existing CGL
studies, which mainly deal with classification. Specifically, we reflect on the
predictive power of confounders in graph-level regression, and generalize
classification-specific causal intervention techniques to regression through a
lens of contrastive learning. Extensive experiments on graph OOD benchmarks
validate the efficacy of our proposals for CGR. The model implementation and
the code are provided on https://github.com/causal-graph/CGR.

</details>


### [221] [Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention](https://arxiv.org/abs/2507.00449)
*Zhihao Zhan,Jianan Zhao,Zhaocheng Zhu,Jian Tang*

Main category: cs.LG

TL;DR: 论文提出了一种新方法“HAX”，旨在克服以往SSMs在长上下文建模中面临的弱点，并在多种实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: SSMs虽然具有次平方时间复杂度的优势，但在捕捉长距离依赖方面存在缺陷，现有的关联召回等合成任务无法形象化表达现实的长上下文建模问题。

Method: 提出了一个新的任务“联合召回”，证明SSMs在次平方时间复杂度下无法解决此问题，并开发了一种结合上下文依赖稀疏注意力（CDSA）的新方法。此外，提出HAX方法应用于实际语言领域。

Result: HAX在合成和真实长上下文基准测试中均表现优越，超越了SSM和SSM-稀疏注意力的基线方法。

Conclusion: 论文成功填补了SSMs在长上下文建模能力上的缺陷，为NLP领域提供了新型有效的解决方案。

Abstract: Efficient long-context modeling remains a critical challenge for natural
language processing (NLP), as the time complexity of the predominant
Transformer architecture scales quadratically with the sequence length. While
state-space models (SSMs) offer alternative sub-quadratic solutions, they
struggle to capture long-range dependencies effectively. In this work, we focus
on analyzing and improving the long-context modeling capabilities of SSMs. We
show that the widely used synthetic task, associative recall, which requires a
model to recall a value associated with a single key without context,
insufficiently represents the complexities of real-world long-context modeling.
To address this limitation, we extend the associative recall to a novel
synthetic task, \emph{joint recall}, which requires a model to recall the value
associated with a key given in a specified context. Theoretically, we prove
that SSMs do not have the expressiveness to solve multi-query joint recall in
sub-quadratic time complexity. To resolve this issue, we propose a solution
based on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which
has the expressiveness to solve multi-query joint recall with sub-quadratic
computation. To bridge the gap between theoretical analysis and real-world
applications, we propose locality-sensitive Hashing Attention with sparse Key
Selection (HAX), which instantiates the theoretical solution and is further
tailored to natural language domains. Extensive experiments on both synthetic
and real-world long-context benchmarks show that HAX consistently outperforms
SSM baselines and SSMs integrated with context-independent sparse attention
(CISA).

</details>


### [222] [Iterative Distillation for Reward-Guided Fine-Tuning of Diffusion Models in Biomolecular Design](https://arxiv.org/abs/2507.00445)
*Xingyu Su,Xiner Li,Masatoshi Uehara,Sunwoo Kim,Yulai Zhao,Gabriele Scalia,Ehsan Hajiramezanali,Tommaso Biancalani,Degui Zhi,Shuiwang Ji*

Main category: cs.LG

TL;DR: 本文提出了一种迭代蒸馏的微调框架，用于优化扩散模型在分子设计中的奖励导向生成。


<details>
  <summary>Details</summary>
Motivation: 改善扩散模型在生物分子设计中的生成能力，尤其是需要优化非可微奖励函数（如物理模拟或基于科学知识的奖励）的问题。

Method: 通过迭代蒸馏微调框架，包括离策略数据收集、奖励引导的软最优策略模拟以及KL散度最小化模型更新。

Result: 实验证明了该方法在蛋白质、小分子及DNA调控设计任务中的表现效果优异，且在奖励优化方面具有优势。

Conclusion: 此方法相较于现有的基于强化学习的方法提高了训练稳定性和样本效率，适合处理复杂的非可微奖励优化任务。

Abstract: We address the problem of fine-tuning diffusion models for reward-guided
generation in biomolecular design. While diffusion models have proven highly
effective in modeling complex, high-dimensional data distributions, real-world
applications often demand more than high-fidelity generation, requiring
optimization with respect to potentially non-differentiable reward functions
such as physics-based simulation or rewards based on scientific knowledge.
Although RL methods have been explored to fine-tune diffusion models for such
objectives, they often suffer from instability, low sample efficiency, and mode
collapse due to their on-policy nature. In this work, we propose an iterative
distillation-based fine-tuning framework that enables diffusion models to
optimize for arbitrary reward functions. Our method casts the problem as policy
distillation: it collects off-policy data during the roll-in phase, simulates
reward-based soft-optimal policies during roll-out, and updates the model by
minimizing the KL divergence between the simulated soft-optimal policy and the
current model policy. Our off-policy formulation, combined with KL divergence
minimization, enhances training stability and sample efficiency compared to
existing RL-based methods. Empirical results demonstrate the effectiveness and
superior reward optimization of our approach across diverse tasks in protein,
small molecule, and regulatory DNA design.

</details>


### [223] [Best Agent Identification for General Game Playing](https://arxiv.org/abs/2507.00451)
*Matthew Stephenson,Alex Newcombe,Eric Piette,Dennis Soemers*

Main category: cs.LG

TL;DR: 本文提出了一种优化的Wilson评分区间方法（Optimistic-WS），用于在多任务问题中有效地识别表现最好的算法，并显著降低算法的简单遗憾。


<details>
  <summary>Details</summary>
Motivation: 在多任务领域中寻找每个子任务最佳算法的问题复杂，传统的方法可能不够高效或准确。本研究旨在通过改进方法来提高效率和准确性。

Method: 通过将问题建模为多臂老虎机下的最佳臂识别问题，每个老虎机对应一个具体任务，每个臂代表一个算法。提出了一种基于Wilson评分区间的乐观选择方法（Optimistic-WS），按潜在遗憾减少排序选择最优算法。

Result: 在GVGAI和Ludii两种通用游戏框架中，Optimistic-WS方法在有限实验次数范围内表现优异，与先前方法相比，平均简单遗憾显著降低。

Conclusion: Optimistic-WS不仅在通用游戏框架的代理评估中表现出色，也具有在高算法运行时的多任务领域中广泛适用的潜力。

Abstract: We present an efficient and generalised procedure to accurately identify the
best performing algorithm for each sub-task in a multi-problem domain. Our
approach treats this as a set of best arm identification problems for
multi-armed bandits, where each bandit corresponds to a specific task and each
arm corresponds to a specific algorithm or agent. We propose an optimistic
selection process based on the Wilson score interval (Optimistic-WS) that ranks
each arm across all bandits in terms of their potential regret reduction. We
evaluate the performance of Optimistic-WS on two of the most popular general
game domains, the General Video Game AI (GVGAI) framework and the Ludii general
game playing system, with the goal of identifying the highest performing agent
for each game within a limited number of trials. Compared to previous best arm
identification algorithms for multi-armed bandits, our results demonstrate a
substantial performance improvement in terms of average simple regret. This
novel approach can be used to significantly improve the quality and accuracy of
agent evaluation procedures for general game frameworks, as well as other
multi-task domains with high algorithm runtimes.

</details>


### [224] [Recurrent Memory-Augmented Transformers with Chunked Attention for Long-Context Language Modeling](https://arxiv.org/abs/2507.00453)
*Ankit Kashyap*

Main category: cs.LG

TL;DR: 本文提出了一种结合全局注意力、分块局部注意力及FIFO存储机制的Transformer架构，用于高效处理长上下文语言模型中的短期和长期依赖。


<details>
  <summary>Details</summary>
Motivation: 应对长上下文语言建模中短期与长期依赖需高效处理的问题，同时将注意力机制与生物启发的存储机制相结合。

Method: 通过整合全局注意力、分块局部注意力及受生物启发的门控FIFO记忆机制，结合旋转位置编码优化注意力机制，采用PyTorch从零开发透明可扩展的框架。

Result: 模型可以有效处理对话生成、代码补全及文本文档理解等任务，降低传统方法中的计算成本问题。

Conclusion: 该设计展示了一种轻量化、可扩展的Transformer架构，为长上下文语言建模提供了高效的解决方案。

Abstract: We present a Transformer architecture for long-context language modeling that
combines global attention with two biologically inspired components: chunked
local attention and a gated FIFO memory mechanism. This unified attention block
allows the model to efficiently handle both short-range and long-range
dependencies without increasing attention cost quadratically. The memory module
persistently stores past token representations using a gated update mechanism
inspired by recurrent networks. Rotary positional encoding is applied per
attention head to enable directionally disentangled, scale-invariant positional
signals. The architecture is implemented entirely from scratch in PyTorch, with
no reliance on high-level libraries, enabling transparent and modular
experimentation. Our model offers a lightweight and extensible design for tasks
such as dialogue modeling, code completion, and document understanding.

</details>


### [225] [Diversity Conscious Refined Random Forest](https://arxiv.org/abs/2507.00467)
*Sijan Bhattarai,Saurav Bhandari,Girija Bhusal,Saroj Shakya,Tapendra Pandey*

Main category: cs.LG

TL;DR: 该论文提出了一种改进的随机森林（RF）分类器，通过动态选择信息性特征和保留非冗余树来提高分类性能，并在多种基准数据集上实验验证了效果。


<details>
  <summary>Details</summary>
Motivation: 传统随机森林需要大量树和所有输入特征，导致推理成本高和模型冗余问题。研究旨在通过优化训练过程，降低这些成本并提高多样性。

Method: 提出了一种迭代优化的随机森林分类器，包括：逐步去除信息性较弱的特征；解析性地确定新增树的数量；基于相关性聚类去除冗余树。

Result: 在8个包括二分类和多分类的基准数据集上进行实验，与标准随机森林相比，该模型在相同数量树的情况下提升了分类准确性。

Conclusion: 改进的随机森林分类器在减少特征冗余和提高分类准确性方面表现出显著效果，是对现有随机森林方法的有效提升。

Abstract: Random Forest (RF) is a widely used ensemble learning technique known for its
robust classification performance across diverse domains. However, it often
relies on hundreds of trees and all input features, leading to high inference
cost and model redundancy. In this work, our goal is to grow trees dynamically
only on informative features and then enforce maximal diversity by clustering
and retaining uncorrelated trees. Therefore, we propose a Refined Random Forest
Classifier that iteratively refines itself by first removing the least
informative features and then analytically determines how many new trees should
be grown, followed by correlation-based clustering to remove redundant trees.
The classification accuracy of our model was compared against the standard RF
on the same number of trees. Experiments on 8 multiple benchmark datasets,
including binary and multiclass datasets, demonstrate that the proposed model
achieves improved accuracy compared to standard RF.

</details>


### [226] [Posterior Inference in Latent Space for Scalable Constrained Black-box Optimization](https://arxiv.org/abs/2507.00480)
*Kiyoung Om,Kyuil Sim,Taeyoung Yun,Hyeongyu Kang,Jinkyoo Park*

Main category: cs.LG

TL;DR: 本文提出了一种新框架，通过流模型与后验推断优化高维黑箱函数的约束问题，取得了卓越的效果。


<details>
  <summary>Details</summary>
Motivation: 由于约束优化问题中可行解空间难以找到，而且维度灾难使传统贝叶斯优化方法表现不佳，因此提出一种能有效适应多峰约束问题的新方法。

Method: 方法包括两个阶段：1. 使用流模型捕获数据分布，并建立预测函数值和约束违例的代理模型；2. 将候选选择问题视为后验推断问题，通过潜空间实现平滑采样，规避多峰问题。

Result: 实验表明，提出的新方法在多个合成和真实的约束黑箱优化任务中表现优异。

Conclusion: 通过有效的后验推断和流模型训练，新框架能够缓解多模态和维度灾难，提供更高效的解决约束优化问题的工具。

Abstract: Optimizing high-dimensional black-box functions under black-box constraints
is a pervasive task in a wide range of scientific and engineering problems.
These problems are typically harder than unconstrained problems due to
hard-to-find feasible regions. While Bayesian optimization (BO) methods have
been developed to solve such problems, they often struggle with the curse of
dimensionality. Recently, generative model-based approaches have emerged as a
promising alternative for constrained optimization. However, they suffer from
poor scalability and are vulnerable to mode collapse, particularly when the
target distribution is highly multi-modal. In this paper, we propose a new
framework to overcome these challenges. Our method iterates through two stages.
First, we train flow-based models to capture the data distribution and
surrogate models that predict both function values and constraint violations
with uncertainty quantification. Second, we cast the candidate selection
problem as a posterior inference problem to effectively search for promising
candidates that have high objective values while not violating the constraints.
During posterior inference, we find that the posterior distribution is highly
multi-modal and has a large plateau due to constraints, especially when
constraint feedback is given as binary indicators of feasibility. To mitigate
this issue, we amortize the sampling from the posterior distribution in the
latent space of flow-based models, which is much smoother than that in the data
space. We empirically demonstrate that our method achieves superior performance
on various synthetic and real-world constrained black-box optimization tasks.
Our code is publicly available \href{https://github.com/umkiyoung/CiBO}{here}.

</details>


### [227] [PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning](https://arxiv.org/abs/2507.00485)
*Weiran Guo,Guanjun Liu,Ziyuan Zhou,Ling Wang*

Main category: cs.LG

TL;DR: 这篇论文探讨了强化学习中安全强化学习的后门攻击问题，引入了正负动作样本框架，并通过实验展示了这些攻击的可行性和风险。


<details>
  <summary>Details</summary>
Motivation: 随着强化学习越来越多地应用于实际场景，安全问题变得突出，因此研究如何提高安全性或识别潜在风险尤为重要。

Method: 提出了PNAct框架，正动作样本提供参考动作，负动作样本指示需避免的动作，并设计了攻击算法以评估框架的有效性。

Result: 通过实验验证了PNAct后门攻击框架的可行性和威胁，并提供了量化评估指标。

Conclusion: 论文强调了安全强化学习中后门攻击的潜在风险，并通过PNAct框架证明了这些威胁的实现可能性。

Abstract: Reinforcement Learning (RL) is widely used in tasks where agents interact
with an environment to maximize rewards. Building on this foundation, Safe
Reinforcement Learning (Safe RL) incorporates a cost metric alongside the
reward metric, ensuring that agents adhere to safety constraints during
decision-making. In this paper, we identify that Safe RL is vulnerable to
backdoor attacks, which can manipulate agents into performing unsafe actions.
First, we introduce the relevant concepts and evaluation metrics for backdoor
attacks in Safe RL. It is the first attack framework in the Safe RL field that
involves both Positive and Negative Action sample (PNAct) is to implant
backdoors, where positive action samples provide reference actions and negative
action samples indicate actions to be avoided. We theoretically point out the
properties of PNAct and design an attack algorithm. Finally, we conduct
experiments to evaluate the effectiveness of our proposed backdoor attack
framework, evaluating it with the established metrics. This paper highlights
the potential risks associated with Safe RL and underscores the feasibility of
such attacks. Our code and supplementary material are available at
https://github.com/azure-123/PNAct.

</details>


### [228] [Exploring Large Action Sets with Hyperspherical Embeddings using von Mises-Fisher Sampling](https://arxiv.org/abs/2507.00518)
*Walid Bendada,Guillaume Salha-Galvan,Romain Hennequin,Théo Bontempelli,Thomas Bouabça,Tristan Cazenave*

Main category: cs.LG

TL;DR: 本文提出了一种名为von Mises-Fisher exploration (vMF-exp)的增强强化学习探索方法，适用于大规模动作集场景，且具备高扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法（如Boltzmann Exploration）在大规模动作集上的可扩展性问题，并提升强化学习中探索阶段的效率。

Method: 使用von Mises-Fisher分布对状态的嵌入向量进行采样，并探索其最近邻的嵌入表示，从而支持超大规模的动作集探索。

Result: 理论上证明了vMF-exp能够逐渐达到与Boltzmann Exploration相同的动作探索概率水平，并通过仿真数据、公开真实数据，以及全球音乐流媒体服务推荐系统的大规模部署实验验证其核心优势。

Conclusion: vMF-exp是一种比Boltzmann Exploration更具扩展性的大规模动作集探索方法，验证了其在不同应用场景中的有效性。

Abstract: This paper introduces von Mises-Fisher exploration (vMF-exp), a scalable
method for exploring large action sets in reinforcement learning problems where
hyperspherical embedding vectors represent these actions. vMF-exp involves
initially sampling a state embedding representation using a von Mises-Fisher
distribution, then exploring this representation's nearest neighbors, which
scales to virtually unlimited numbers of candidate actions. We show that, under
theoretical assumptions, vMF-exp asymptotically maintains the same probability
of exploring each action as Boltzmann Exploration (B-exp), a popular
alternative that, nonetheless, suffers from scalability issues as it requires
computing softmax values for each action. Consequently, vMF-exp serves as a
scalable alternative to B-exp for exploring large action sets with
hyperspherical embeddings. Experiments on simulated data, real-world public
data, and the successful large-scale deployment of vMF-exp on the recommender
system of a global music streaming service empirically validate the key
properties of the proposed method.

</details>


### [229] [Foundation Models for Clinical Records at Health System Scale](https://arxiv.org/abs/2507.00574)
*Haresh Rengaraj Rajamohan,Xiang Gao,Weicheng Zhu,Shih-Lun Huang,Long Chen,Kyunghyun Cho,Cem M. Deniz,Narges Razavian*

Main category: cs.LG

TL;DR: 该论文提出了一种新的生成性预训练方法，应用于顺序电子健康记录(EHR)数据的下次访问事件预测，并通过无监督的方式实现复杂临床依赖的捕获。


<details>
  <summary>Details</summary>
Motivation: 探索大规模预训练在医疗领域尤其是电子健康记录（EHR）上的潜力，并解决现有模型评估中过多依赖重复事件的不足。

Method: 通过自回归方式生成患者历史中的事件来进行下次访问事件的预测，同时引入重复事件的正则化方法，以及优化EHR中的基础模型评估方式。

Result: 新模型在不进行任务特定微调的情况下，通过零样本预测能够有效预测痴呆和膝骨关节炎的发病几率，其性能与完全微调的掩码预训练Transformer模型相当。

Conclusion: 提出的方法无需昂贵的微调便能建模复杂的临床依赖，可作为处理异构EHR数据的生成式基础模型。

Abstract: Large-scale pretraining has transformed modeling of language and other data
types, but its potential remains underexplored in healthcare with structured
electronic health records (EHRs). We present a novel generative pretraining
strategy for sequential EHR data using next-visit event prediction. Our model
learns to autoregressively generate various tokenized clinical events for the
next visit based on patient history and inherently handles the joint prediction
of heterogeneous data types. Additionally, we introduce regularization on
predicting repeated events and highlight a key pitfall in EHR-based foundation
model evaluations: repeated event tokens can inflate performance metrics when
new onsets are not distinguished from subsequent occurrences. Our model is
evaluated via zero-shot prediction for forecasting dementia and knee
osteoarthritis incidence within 2 and 5 years, and the model performance rivals
a fully fine-tuned masked pretrained Transformer baseline, demonstrating that
our approach captures complex clinical dependencies without requiring costly
task-specific fine-tuning.

</details>


### [230] [Quantum Circuit Structure Optimization for Quantum Reinforcement Learning](https://arxiv.org/abs/2507.00589)
*Seok Bin Son,Joongheon Kim*

Main category: cs.LG

TL;DR: 提出了一种结合量子神经结构搜索（QNAS）与量子强化学习（QRL）的QRL-NAS算法，优化了参数化量子电路（PQC）结构，并实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前量子强化学习中的参数化量子电路结构多由经验确定，优化性尚未验证，亟需一种方法来优化其性能。

Method: 通过结合量子神经网络结构搜索（QNAS）和量子强化学习（QRL），自动优化参数化量子电路（PQC）结构。

Result: 实验结果表明，QRL-NAS算法相比固定电路结构的QRL获得了更高的奖励值。

Conclusion: QRL-NAS验证了结合QNAS和QRL方法在优化PQC结构方面的实用价值和有效性。

Abstract: Reinforcement learning (RL) enables agents to learn optimal policies through
environmental interaction. However, RL suffers from reduced learning efficiency
due to the curse of dimensionality in high-dimensional spaces. Quantum
reinforcement learning (QRL) addresses this issue by leveraging superposition
and entanglement in quantum computing, allowing efficient handling of
high-dimensional problems with fewer resources. QRL combines quantum neural
networks (QNNs) with RL, where the parameterized quantum circuit (PQC) acts as
the core computational module. The PQC performs linear and nonlinear
transformations through gate operations, similar to hidden layers in classical
neural networks. Previous QRL studies, however, have used fixed PQC structures
based on empirical intuition without verifying their optimality. This paper
proposes a QRL-NAS algorithm that integrates quantum neural architecture search
(QNAS) to optimize PQC structures within QRL. Experiments demonstrate that
QRL-NAS achieves higher rewards than QRL with fixed circuits, validating its
effectiveness and practical utility.

</details>


### [231] [Residual Reward Models for Preference-based Reinforcement Learning](https://arxiv.org/abs/2507.00611)
*Chenyang Cao,Miguel Rogel-García,Mohamed Nabail,Xueqian Wang,Nicholas Rhinehart*

Main category: cs.LG

TL;DR: 本文提出Residual Reward Model (RRM)以有效利用先验知识，改进偏好强化学习(PbRL)的方法，加速策略学习。


<details>
  <summary>Details</summary>
Motivation: PbRL需要通过奖励模型训练，但存在收敛速度慢的问题。而当前基于演示和偏好微调奖励模型的方法受限于不同损失函数的组合优化挑战。

Method: 提出了残差奖励模型RRM，将真实奖励分为先验奖励和通过偏好学习的奖励两部分。通过经验评估分别提出适用于状态和图像的RRM版本。

Result: 在Meta-World任务和实际机器人(Franka Panda)任务中，RRM明显提升了学习效率与性能，适用于不同来源的先验奖励。

Conclusion: RRM在偏好学习场景下有效结合先验知识，显著加速策略优化，为任务解决带来了更广泛的适用可能性。

Abstract: Preference-based Reinforcement Learning (PbRL) provides a way to learn
high-performance policies in environments where the reward signal is hard to
specify, avoiding heuristic and time-consuming reward design. However, PbRL can
suffer from slow convergence speed since it requires training in a reward
model. Prior work has proposed learning a reward model from demonstrations and
fine-tuning it using preferences. However, when the model is a neural network,
using different loss functions for pre-training and fine-tuning can pose
challenges to reliable optimization. In this paper, we propose a method to
effectively leverage prior knowledge with a Residual Reward Model (RRM). An RRM
assumes that the true reward of the environment can be split into a sum of two
parts: a prior reward and a learned reward. The prior reward is a term
available before training, for example, a user's ``best guess'' reward
function, or a reward function learned from inverse reinforcement learning
(IRL), and the learned reward is trained with preferences. We introduce
state-based and image-based versions of RRM and evaluate them on several tasks
in the Meta-World environment suite. Experimental results show that our method
substantially improves the performance of a common PbRL method. Our method
achieves performance improvements for a variety of different types of prior
rewards, including proxy rewards, a reward obtained from IRL, and even a
negated version of the proxy reward. We also conduct experiments with a Franka
Panda to show that our method leads to superior performance on a real robot. It
significantly accelerates policy learning for different tasks, achieving
success in fewer steps than the baseline. The videos are presented at
https://sunlighted.github.io/RRM-web/.

</details>


### [232] [Cooperative Sheaf Neural Networks](https://arxiv.org/abs/2507.00647)
*André Ribeiro,Ana Luiza Tenório,Juan Belieni,Amauri H. Souza,Diego Mesquita*

Main category: cs.LG

TL;DR: 本文提出了一种新型的图神经网络模型（CSNNs），其通过在有向图上定义的cellular sheaves解决了已有方法在信息传递上的局限性，表现出更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的sheaf diffusion方法无法实现节点的合作行为（选择传递/接收信息），有必要探索更灵活的信息传播机制。

Method: 引入了有向图上的cellular sheaves，定义其入度和出度拉普拉斯矩阵，设计了新的网络模型Cooperative Sheaf Neural Networks (CSNNs)。

Result: 理论上证明了CSNN的感受野可以灵活选择性地关注远处节点，同时忽略路径上的干扰节点，实验显示其在多个任务上优于现有方法。

Conclusion: CSNN克服了sheaf diffusion的局限性，结合合作性信息传播机制，展现了优越的性能及潜力。

Abstract: Sheaf diffusion has recently emerged as a promising design pattern for graph
representation learning due to its inherent ability to handle heterophilic data
and avoid oversmoothing. Meanwhile, cooperative message passing has also been
proposed as a way to enhance the flexibility of information diffusion by
allowing nodes to independently choose whether to propagate/gather information
from/to neighbors. A natural question ensues: is sheaf diffusion capable of
exhibiting this cooperative behavior? Here, we provide a negative answer to
this question. In particular, we show that existing sheaf diffusion methods
fail to achieve cooperative behavior due to the lack of message directionality.
To circumvent this limitation, we introduce the notion of cellular sheaves over
directed graphs and characterize their in- and out-degree Laplacians. We
leverage our construction to propose Cooperative Sheaf Neural Networks (CSNNs).
Theoretically, we characterize the receptive field of CSNN and show it allows
nodes to selectively attend (listen) to arbitrarily far nodes while ignoring
all others in their path, potentially mitigating oversquashing. Our experiments
show that CSNN presents overall better performance compared to prior art on
sheaf diffusion as well as cooperative graph neural networks.

</details>


### [233] [GANs Secretly Perform Approximate Bayesian Model Selection](https://arxiv.org/abs/2507.00651)
*Maurizio Filippone,Marius P. Linhard*

Main category: cs.LG

TL;DR: 该论文从概率生成模型的角度解释了GANs的成功与局限，并提出了新的正则化与优化方法大大提升性能。


<details>
  <summary>Details</summary>
Motivation: GANs尽管成功广泛，但优化具有挑战性且容易过拟合，亟需理论解释其行为及开发改进方法。

Method: 将GANs解释为具有部分随机性的贝叶斯神经网络，通过这种解释建立本质上与边际似然优化相连的代理优化目标，并据此提出针对平滑损失景观和寻求最小描述长度解的正则化与优化策略。

Result: 通过一系列实验验证了所提方法在多个方面的性能改进，表现出更好的泛化能力和平滑的损失曲面。

Conclusion: 该研究提供了对GANs的深入理论洞见，并提出了有效的优化策略，为未来GANs的改进与发展提供了新的方向。

Abstract: Generative Adversarial Networks (GANs) are popular and successful generative
models. Despite their success, optimization is notoriously challenging and they
require regularization against overfitting. In this work, we explain the
success and limitations of GANs by interpreting them as probabilistic
generative models. This interpretation enables us to view GANs as Bayesian
neural networks with partial stochasticity, allowing us to establish conditions
of universal approximation. We can then cast the adversarial-style optimization
of several variants of GANs as the optimization of a proxy for the marginal
likelihood. Taking advantage of the connection between marginal likelihood
optimization and Occam's razor, we can define regularization and optimization
strategies to smooth the loss landscape and search for solutions with minimum
description length, which are associated with flat minima and good
generalization. The results on a wide range of experiments indicate that these
strategies lead to performance improvements and pave the way to a deeper
understanding of regularization strategies for GANs.

</details>


### [234] [Cognitive Load-Aware Inference: A Neuro-Symbolic Framework for Optimizing the Token Economy of Large Language Models](https://arxiv.org/abs/2507.00653)
*Yilun Zhang*

Main category: cs.LG

TL;DR: 本文提出了认知负荷感知推理（CLAI）框架，用认知负荷理论指导优化大型语言模型推理，显著减少推理开销（最多45%）而不降低精度。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型推理的高计算成本问题，并弥补现有方法缺乏认知理论支撑的不足。

Method: 通过将认知负荷理论中的内在、外在和有助于学习的负荷，转化为可量化的推理指标（$ICL_{LLM}$、$ECL_{LLM}$、$GCL_{LLM}$）。提出CLAI框架，包括CLAI-Prompt方法（零样本引导）和CLAI-Tune方法（模型微调）。

Result: 在复杂推理、长上下文问答和代码生成等任务上，实现了高达45%的token消耗减少，同时保持精度，且CLAI-Tune方法展现了分解问题的能力。

Conclusion: 模拟人脑资源管理策略可构建更高效、稳健的人工智能系统，CLAI框架为未来研究提供了新方向。

Abstract: The escalating computational costs of Large Language Model (LLM) inference
have become a critical barrier to their widespread and sustainable deployment.
While existing optimization strategies are effective, they are predominantly
based on statistical heuristics or architectural modifications, lacking a
guiding cognitive theory to manage the inference process itself. This paper
aims to bridge this gap by introducing a novel paradigm: the Cognitive
Load-Aware Inference (CLAI) framework, which operationalizes principles from
Cognitive Load Theory (CLT) and neuroscience for LLM inference. We formalize
the concepts of Intrinsic Cognitive Load, Extraneous Cognitive Load, and
Germane Cognitive Load into quantifiable LLM metrics ($ICL_{LLM}$, $ECL_{LLM}$,
and $GCL_{LLM}$), thereby reframing the inference process as a cognitive
economics optimization problem: based on the intrinsic complexity of a problem
($ICL_{LLM}$), minimize wasteful computation ($ECL_{LLM}$), and strategically
allocate the token budget to productive reasoning ($GCL_{LLM}$). We propose two
implementation paths: CLAI-Prompt, a zero-shot method that guides a base LLM
through cognitive control steps via a structured meta-prompt, and CLAI-Tune, a
fine-tuned model that internalizes these principles for spontaneous cognitive
economy. Across a range of benchmarks in complex reasoning, long-context
question answering, and code generation, our methods achieve significant
reductions in token consumption (up to 45\%) without sacrificing accuracy.
Furthermore, CLAI-Tune exhibits an emergent ability to autonomously decompose
difficult problems, a key characteristic of human expert cognition. This work
demonstrates that by emulating the brain's resource management strategies, we
can build more efficient, robust, and capable artificial intelligence systems.

</details>


### [235] [Neural Augmented Kalman Filters for Road Network assisted GNSS positioning](https://arxiv.org/abs/2507.00654)
*Hans van Gorp,Davide Belli,Amir Jalalirad,Bence Major*

Main category: cs.LG

TL;DR: 该论文提出了一种利用时间图神经网络 (TGNN) 和卡尔曼滤波 (KF)，通过结合路网数据提高全球导航卫星系统 (GNSS) 在复杂城市环境中的定位精度的方法。


<details>
  <summary>Details</summary>
Motivation: GNSS 在城市密集环境下准确性受限，主要由于多路径和非视距误差，作者希望通过结合路网数据减少这些误差并提高定位精度。

Method: 设计了一个时间图神经网络（TGNN），将路网信息整合到卡尔曼滤波器中。TGNN 预测正确的道路段及其相关的不确定性，用于 KF 的测量更新步骤。

Result: 在实际的 GNSS 数据和开放路网测试中，与仅使用 GNSS 的 KF 方法相比，该方法在复杂场景下的定位误差降低了 29%。

Conclusion: 这是首个基于深度学习、联合使用路网数据和 GNSS 数据进行地面定位的研究。

Abstract: The Global Navigation Satellite System (GNSS) provides critical positioning
information globally, but its accuracy in dense urban environments is often
compromised by multipath and non-line-of-sight errors. Road network data can be
used to reduce the impact of these errors and enhance the accuracy of a
positioning system. Previous works employing road network data are either
limited to offline applications, or rely on Kalman Filter (KF) heuristics with
little flexibility and robustness. We instead propose training a Temporal Graph
Neural Network (TGNN) to integrate road network information into a KF. The TGNN
is designed to predict the correct road segment and its associated uncertainty
to be used in the measurement update step of the KF. We validate our approach
with real-world GNSS data and open-source road networks, observing a 29%
decrease in positioning error for challenging scenarios compared to a GNSS-only
KF. To the best of our knowledge, ours is the first deep learning-based
approach jointly employing road network data and GNSS measurements to determine
the user position on Earth.

</details>


### [236] [Audio-3DVG: Unified Audio - Point Cloud Fusion for 3D Visual Grounding](https://arxiv.org/abs/2507.00669)
*Duc Cao-Dinh,Khai Le-Duc,Anh Dao,Bach Phan Tat,Chris Ngo,Duy M. H. Nguyen,Nguyen X. Khanh,Thanh Nguyen-Tang*

Main category: cs.LG

TL;DR: 本文提出Audio-3DVG框架，通过音频和空间信息的融合提升3D点云中的目标定位能力。


<details>
  <summary>Details</summary>
Motivation: 当前3D点云目标定位领域对语音描述的探索较少，但自动语音识别和语音表示学习的进步提供了新的可能性。

Method: 提出了两个核心组件：多标签分类任务Object Mention Detection用于识别音频中提到的对象；Audio-Guided Attention模块捕捉候选对象与语音提示之间的交互。

Result: 在多项基准数据集上，Audio-3DVG在基于音频的目标定位中达到了最新的性能，并且在某些方面与基于文本的方法竞争。

Conclusion: Audio-3DVG展示了将语音语言引入3D视觉任务的潜力，为领域开辟了新方向。

Abstract: 3D Visual Grounding (3DVG) involves localizing target objects in 3D point
clouds based on natural language. While prior work has made strides using
textual descriptions, leveraging spoken language-known as Audio-based 3D Visual
Grounding-remains underexplored and challenging. Motivated by advances in
automatic speech recognition (ASR) and speech representation learning, we
propose Audio-3DVG, a simple yet effective framework that integrates audio and
spatial information for enhanced grounding. Rather than treating speech as a
monolithic input, we decompose the task into two complementary components.
First, we introduce Object Mention Detection, a multi-label classification task
that explicitly identifies which objects are referred to in the audio, enabling
more structured audio-scene reasoning. Second, we propose an Audio-Guided
Attention module that captures interactions between candidate objects and
relational speech cues, improving target discrimination in cluttered scenes. To
support benchmarking, we synthesize audio descriptions for standard 3DVG
datasets, including ScanRefer, Sr3D, and Nr3D. Experimental results demonstrate
that Audio-3DVG not only achieves new state-of-the-art performance in
audio-based grounding, but also competes with text-based methods-highlighting
the promise of integrating spoken language into 3D vision tasks.

</details>


### [237] [Diffusion Classifier Guidance for Non-robust Classifiers](https://arxiv.org/abs/2507.00687)
*Philipp Vaeth,Dibyanshu Kumar,Benjamin Paassen,Magda Gregorová*

Main category: cs.LG

TL;DR: 扩展了分类器引导以适用于非鲁棒分类器，并提出一种方法使其稳定性提高。


<details>
  <summary>Details</summary>
Motivation: 探索如何使非鲁棒分类器在扩散过程中的噪声条件下更稳定可靠。

Method: 分析分类器对扩散过程噪声的敏感性，使用一步去噪预测和指数移动平均等技术来稳定引导梯度。

Result: 实验表明该方法提高了分类器引导的稳定性，同时保持了样本多样性和视觉质量。

Conclusion: 提出的方法扩展了可用分类器范围，推进了生成模型中的条件采样技术。

Abstract: Classifier guidance is intended to steer a diffusion process such that a
given classifier reliably recognizes the generated data point as a certain
class. However, most classifier guidance approaches are restricted to robust
classifiers, which were specifically trained on the noise of the diffusion
forward process. We extend classifier guidance to work with general,
non-robust, classifiers that were trained without noise. We analyze the
sensitivity of both non-robust and robust classifiers to noise of the diffusion
process on the standard CelebA data set, the specialized SportBalls data set
and the high-dimensional real-world CelebA-HQ data set. Our findings reveal
that non-robust classifiers exhibit significant accuracy degradation under
noisy conditions, leading to unstable guidance gradients. To mitigate these
issues, we propose a method that utilizes one-step denoised image predictions
and implements stabilization techniques inspired by stochastic optimization
methods, such as exponential moving averages. Experimental results demonstrate
that our approach improves the stability of classifier guidance while
maintaining sample diversity and visual quality. This work contributes to
advancing conditional sampling techniques in generative models, enabling a
broader range of classifiers to be used as guidance classifiers.

</details>


### [238] [A Test-Function Approach to Incremental Stability](https://arxiv.org/abs/2507.00695)
*Daniel Pfrommer,Max Simchowitz,Ali Jadbabaie*

Main category: cs.LG

TL;DR: 该论文提出了一种新框架，使用奖励函数作为“测试函数”来分析增量输入-状态稳定性（$\delta$ISS）。


<details>
  <summary>Details</summary>
Motivation: 传统控制理论利用满足时间递减条件的Lyapunov函数，而强化学习则通过指数衰减的奖励函数构造价值函数，两者有根本性不同。

Method: 通过选择Hölder连续奖励函数，建立增量输入-状态稳定性与强化学习价值函数的规则性之间的等价性。

Result: 提出了价值函数与控制系统稳定性的新联系，避免了对Lyapunov方法的依赖。

Conclusion: 价值函数的规则性可用于理解稳定性，开辟了不同于传统Lyapunov方法的稳定性分析新视角。

Abstract: This paper presents a novel framework for analyzing
Incremental-Input-to-State Stability ($\delta$ISS) based on the idea of using
rewards as "test functions." Whereas control theory traditionally deals with
Lyapunov functions that satisfy a time-decrease condition, reinforcement
learning (RL) value functions are constructed by exponentially decaying a
Lipschitz reward function that may be non-smooth and unbounded on both sides.
Thus, these RL-style value functions cannot be directly understood as Lyapunov
certificates. We develop a new equivalence between a variant of incremental
input-to-state stability of a closed-loop system under given a policy, and the
regularity of RL-style value functions under adversarial selection of a
H\"older-continuous reward function. This result highlights that the regularity
of value functions, and their connection to incremental stability, can be
understood in a way that is distinct from the traditional Lyapunov-based
approach to certifying stability in control theory.

</details>


### [239] [SCAWaveNet: A Spatial-Channel Attention-based Network for Global Significant Wave Height Retrieval](https://arxiv.org/abs/2507.00701)
*Chong Zhang,Xichao Liu,Yibing Zhan,Dapeng Tao,Jun Ni*

Main category: cs.LG

TL;DR: 本文提出了一种新型的空间-通道注意力机制网络SCAWaveNet，用于基于四通道CYGNSS数据进行显著波高(SWH)的提取，并显著降低了误差。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的SWH提取方法未充分利用CYGNSS四通道数据的交互信息，存在模型准确性优化空间。

Method: 提出SCAWaveNet模型，将DDMs的每个通道特征建模为独立的注意力头，同时通过一个轻量化的注意力机制融合空间和通道信息，生成最终特征。

Result: 利用ERA5数据，模型平均RMSE为0.438 m；使用NDBC浮标数据，平均RMSE为0.432 m，比现有模型在ERA5和NDBC数据集上的误差分别降低至少3.52%和5.47%。

Conclusion: SCAWaveNet通过创新的注意力机制有效融合空间与通道特征，在显著波高提取任务中优于已有模型，展示了在GNSS数据分析领域的应用潜力。

Abstract: Recent advancements in spaceborne GNSS missions have produced extensive
global datasets, providing a robust basis for deep learning-based significant
wave height (SWH) retrieval. While existing deep learning models predominantly
utilize CYGNSS data with four-channel information, they often adopt
single-channel inputs or simple channel concatenation without leveraging the
benefits of cross-channel information interaction during training. To address
this limitation, a novel spatial-channel attention-based network, namely
SCAWaveNet, is proposed for SWH retrieval. Specifically, features from each
channel of the DDMs are modeled as independent attention heads, enabling the
fusion of spatial and channel-wise information. For auxiliary parameters, a
lightweight attention mechanism is designed to assign weights along the spatial
and channel dimensions. The final feature integrates both spatial and
channel-level characteristics. Model performance is evaluated using
four-channel CYGNSS data. When ERA5 is used as a reference, SCAWaveNet achieves
an average RMSE of 0.438 m. When using buoy data from NDBC, the average RMSE
reaches 0.432 m. Compared to state-of-the-art models, SCAWaveNet reduces the
average RMSE by at least 3.52% on the ERA5 dataset and by 5.47% on the NDBC
buoy observations. The code is available at
https://github.com/Clifx9908/SCAWaveNet.

</details>


### [240] [Large Reasoning Models are not thinking straight: on the unreliability of thinking trajectories](https://arxiv.org/abs/2507.00711)
*Jhouben Cuesta-Ramirez,Samuel Beaussant,Mehdi Mounsif*

Main category: cs.LG

TL;DR: 研究揭示了LLMs在推理任务中可能存在过度思考的问题，即使已提供正确解答，也可能舍弃答案继续生成不必要且错误的推理步骤。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在推理基准测试中表现出色，但其生成的推理链常常冗长且无效，引发了对其推理能力实际改进的质疑。

Method: 对AIME2024数学基准测试中的三个最先进模型进行了实验，分析其在集成和纠正信息时的表现。

Result: 发现模型常常舍弃已提供的正确答案，转而生成过多推理步骤，结果导致错误结论，凸显了模型在稳健性和解释性推理方面的局限。

Conclusion: 当前LLMs在推理任务中的局限性表明，需要新的方法来克服其过度推理问题，以实现更加可靠和可解释的推理能力。

Abstract: Large Language Models (LLMs) trained via Reinforcement Learning (RL) have
recently achieved impressive results on reasoning benchmarks. Yet, growing
evidence shows that these models often generate longer but ineffective chains
of thought (CoTs), calling into question whether benchmark gains reflect real
reasoning improvements. We present new evidence of overthinking, where models
disregard correct solutions even when explicitly provided, instead continuing
to generate unnecessary reasoning steps that often lead to incorrect
conclusions. Experiments on three state-of-the-art models using the AIME2024
math benchmark reveal critical limitations in these models ability to integrate
corrective information, posing new challenges for achieving robust and
interpretable reasoning.

</details>


### [241] [Aleatoric and Epistemic Uncertainty Measures for Ordinal Classification through Binary Reduction](https://arxiv.org/abs/2507.00733)
*Stefan Haas,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 作者提出了一种针对序数分类不确定性的新测量方法，其显著提高了错误检测和分布外检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前多集中于标称分类和回归的研究无法满足序数分类对不确定性量化的需求。

Method: 引入一种基于二元案例熵和方差度量转化的序数分类不确定性新测量方法。

Result: 在多种序数基准数据集上，此方法在误差检测和分布外检测性能上优于传统方法。

Conclusion: 研究强调了在不确定性评估中考虑分类问题序数属性的重要性。

Abstract: Ordinal classification problems, where labels exhibit a natural order, are
prevalent in high-stakes fields such as medicine and finance. Accurate
uncertainty quantification, including the decomposition into aleatoric
(inherent variability) and epistemic (lack of knowledge) components, is crucial
for reliable decision-making. However, existing research has primarily focused
on nominal classification and regression. In this paper, we introduce a novel
class of measures of aleatoric and epistemic uncertainty in ordinal
classification, which is based on a suitable reduction to (entropy- and
variance-based) measures for the binary case. These measures effectively
capture the trade-off in ordinal classification between exact hit-rate and
minimial error distances. We demonstrate the effectiveness of our approach on
various tabular ordinal benchmark datasets using ensembles of gradient-boosted
trees and multi-layer perceptrons for approximate Bayesian inference. Our
method significantly outperforms standard and label-wise entropy and
variance-based measures in error detection, as indicated by misclassification
rates and mean absolute error. Additionally, the ordinal measures show
competitive performance in out-of-distribution (OOD) detection. Our findings
highlight the importance of considering the ordinal nature of classification
problems when assessing uncertainty.

</details>


### [242] [Ordinality in Discrete-level Question Difficulty Estimation: Introducing Balanced DRPS and OrderedLogitNN](https://arxiv.org/abs/2507.00736)
*Arthur Thuy,Ekaterina Loginova,Dries F. Benoit*

Main category: cs.LG

TL;DR: 本文研究了问答难度估计问题，提出了新的衡量指标并改进算法方法，以解决既有研究中忽视任务阶序本质和类别不平衡问题的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有问答难度估计任务的研究中，多数无法有效捕捉难度阶序本质，且评估指标存在类别不平衡的问题，从而影响了任务的性能衡量与比较。

Method: 提出了一种新的评价指标，平衡离散概率评分（DRPS），同时结合序列逻辑回归及神经网络提出OrderedLogitNN方法，并在RACE++和ARC数据集上微调BERT模型进行性能评估。

Result: 实验表明，OrderedLogitNN模型在处理复杂任务时表现更优，平衡离散概率评分（DRPS）能够更客观和全面地对任务进行评价。

Conclusion: 本文为基于离散难度水平的问题估计提供了一个新的稳健和公平评估框架，并为后续研究提供了参考基础。

Abstract: Recent years have seen growing interest in Question Difficulty Estimation
(QDE) using natural language processing techniques. Question difficulty is
often represented using discrete levels, framing the task as ordinal regression
due to the inherent ordering from easiest to hardest. However, the literature
has neglected the ordinal nature of the task, relying on classification or
discretized regression models, with specialized ordinal regression methods
remaining unexplored. Furthermore, evaluation metrics are tightly coupled to
the modeling paradigm, hindering cross-study comparability. While some metrics
fail to account for the ordinal structure of difficulty levels, none adequately
address class imbalance, resulting in biased performance assessments. This
study addresses these limitations by benchmarking three types of model outputs
-- discretized regression, classification, and ordinal regression -- using the
balanced Discrete Ranked Probability Score (DRPS), a novel metric that jointly
captures ordinality and class imbalance. In addition to using popular ordinal
regression methods, we propose OrderedLogitNN, extending the ordered logit
model from econometrics to neural networks. We fine-tune BERT on the RACE++ and
ARC datasets and find that OrderedLogitNN performs considerably better on
complex tasks. The balanced DRPS offers a robust and fair evaluation metric for
discrete-level QDE, providing a principled foundation for future research.

</details>


### [243] [Evaluating LLMs and Prompting Strategies for Automated Hardware Diagnosis from Textual User-Reports](https://arxiv.org/abs/2507.00742)
*Carlos Caminha,Maria de Lourdes M. Silva,Iago C. Chaves,Felipe T. Brito,Victor A. E. Farias,Javam C. Machado*

Main category: cs.LG

TL;DR: 研究评估了29种LLMs对设备故障描述的自动处理能力，结果显示三种模型在性能与资源使用间平衡较佳。


<details>
  <summary>Details</summary>
Motivation: 解决用户对设备故障不明确描述带来的挑战，提升自动测试与用户体验。

Method: 评估27种开源模型和2种专属模型，使用四种提示策略进行推理测试，分析其性能及资源使用情况。

Result: 进行了98,948次推理，最佳f1评分达到0.76，并识别出三种高性能低资源消耗的模型。

Conclusion: LLMs在理解用户故障描述方面具备潜力，特别是在设备终端上的高效推理表现。

Abstract: Computer manufacturers offer platforms for users to describe device faults
using textual reports such as "My screen is flickering". Identifying the faulty
component from the report is essential for automating tests and improving user
experience. However, such reports are often ambiguous and lack detail, making
this task challenging. Large Language Models (LLMs) have shown promise in
addressing such issues. This study evaluates 27 open-source models (1B-72B
parameters) and 2 proprietary LLMs using four prompting strategies: Zero-Shot,
Few-Shot, Chain-of-Thought (CoT), and CoT+Few-Shot (CoT+FS). We conducted
98,948 inferences, processing over 51 million input tokens and generating 13
million output tokens. We achieve f1-score up to 0.76. Results show that three
models offer the best balance between size and performance:
mistral-small-24b-instruct and two smaller models, llama-3.2-1b-instruct and
gemma-2-2b-it, that offer competitive performance with lower VRAM usage,
enabling efficient inference on end-user devices as modern laptops or
smartphones with NPUs.

</details>


### [244] [A Probabilistic Approach to Wildfire Spread Prediction Using a Denoising Diffusion Surrogate Model](https://arxiv.org/abs/2507.00761)
*Wenbo Yu,Anirbit Ghosh,Tobias Sebastian Finn,Rossella Arcucci,Marc Bocquet,Sibo Cheng*

Main category: cs.LG

TL;DR: 提出了一个用于预测野火传播的消噪扩散模型，能够生成具有物理意义的火灾传播分布。


<details>
  <summary>Details</summary>
Motivation: 利用生成式AI模拟复杂自然过程，解决传统模型难以精准预测野火传播的问题。

Method: 采用消噪扩散模型预测野火传播，通过模拟多种可能性反映火灾动态的不确定性。

Result: 模型生成了包含物理意义的火灾传播可能分布，相比传统方法更加可靠。

Conclusion: 该技术有助于开发更智能、快速、可靠的野火预测与决策支持工具，提升火灾风险评估与应急能力。

Abstract: Thanks to recent advances in generative AI, computers can now simulate
realistic and complex natural processes. We apply this capability to predict
how wildfires spread, a task made difficult by the unpredictable nature of fire
and the variety of environmental conditions it depends on. In this study, We
present the first denoising diffusion model for predicting wildfire spread, a
new kind of AI framework that learns to simulate fires not just as one fixed
outcome, but as a range of possible scenarios. By doing so, it accounts for the
inherent uncertainty of wildfire dynamics, a feature that traditional models
typically fail to represent. Unlike deterministic approaches that generate a
single prediction, our model produces ensembles of forecasts that reflect
physically meaningful distributions of where fire might go next. This
technology could help us develop smarter, faster, and more reliable tools for
anticipating wildfire behavior, aiding decision-makers in fire risk assessment
and response planning.

</details>


### [245] [Leveraging Genetic Algorithms for Efficient Demonstration Generation in Real-World Reinforcement Learning Environments](https://arxiv.org/abs/2507.00762)
*Tom Maus,Asma Atamna,Tobias Glasmachers*

Main category: cs.LG

TL;DR: 本文探索了通过遗传算法（GA）生成专家演示、用以增强强化学习（RL）性能的方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习在工业应用中的推广受到样本效率低和学习动态不稳定的限制，因此需要一种更有效的训练方法。

Method: 使用遗传算法生成专家演示数据，将其加入到DQN的回放缓冲中，并用作PPO代理的训练起点以加速学习。

Result: 实验显示，用GA生成的演示数据显著提升了强化学习性能，尤其是PPO代理实现了更高的累积奖励。

Conclusion: 遗传算法与强化学习的结合是非常有前景的混合学习范式，能够提高RL在实际工业环境中的适应能力。

Abstract: Reinforcement Learning (RL) has demonstrated significant potential in certain
real-world industrial applications, yet its broader deployment remains limited
by inherent challenges such as sample inefficiency and unstable learning
dynamics. This study investigates the utilization of Genetic Algorithms (GAs)
as a mechanism for improving RL performance in an industrially inspired sorting
environment. We propose a novel approach in which GA-generated expert
demonstrations are used to enhance policy learning. These demonstrations are
incorporated into a Deep Q-Network (DQN) replay buffer for experience-based
learning and utilized as warm-start trajectories for Proximal Policy
Optimization (PPO) agents to accelerate training convergence. Our experiments
compare standard RL training with rule-based heuristics, brute-force
optimization, and demonstration data, revealing that GA-derived demonstrations
significantly improve RL performance. Notably, PPO agents initialized with
GA-generated data achieved superior cumulative rewards, highlighting the
potential of hybrid learning paradigms, where heuristic search methods
complement data-driven RL. The utilized framework is publicly available and
enables further research into adaptive RL strategies for real-world
applications.

</details>


### [246] [BoltzNCE: Learning Likelihoods for Boltzmann Generation with Stochastic Interpolants and Noise Contrastive Estimation](https://arxiv.org/abs/2507.00846)
*Rishal Aggrwal,Jacky Chen,Nicholas M. Boffi,David Ryan Koes*

Main category: cs.LG

TL;DR: 本文提出了一种方法，通过能量模型训练的似然估计，结合随机插值器，快速、准确地从Boltzmann分布中采样。


<details>
  <summary>Details</summary>
Motivation: 在物理系统（如分子）建模中，从Boltzmann分布中高效采样是一个关键挑战，而传统方法在计算高维分子系统的似然时成本高昂。

Method: 提出一种通过噪声对比估计与得分匹配来学习生成分布似然的能量模型，并结合随机插值技术在先验与生成分布间进行退火优化。

Result: 在苯丙氨酸二肽系统中，方法生成的自由能分布与准确似然方法相当，且显著加速了不同亚稳态间自由能差的估计。

Conclusion: 新方法能够高效学习分布和自由能，特别适用于大规模分子系统，具有重要的实际应用价值。

Abstract: Efficient sampling from the Boltzmann distribution defined by an energy
function is a key challenge in modeling physical systems such as molecules.
Boltzmann Generators tackle this by leveraging Continuous Normalizing Flows
that transform a simple prior into a distribution that can be reweighted to
match the Boltzmann distribution using sample likelihoods. However, obtaining
likelihoods requires computing costly Jacobians during integration, making it
impractical for large molecular systems. To overcome this, we propose learning
the likelihood of the generated distribution via an energy-based model trained
with noise contrastive estimation and score matching. By using stochastic
interpolants to anneal between the prior and generated distributions, we
combine both the objective functions to efficiently learn the density function.
On the alanine dipeptide system, we demonstrate that our method yields free
energy profiles and energy distributions comparable to those obtained with
exact likelihoods. Additionally, we show that free energy differences between
metastable states can be estimated accurately with orders-of-magnitude speedup.

</details>


### [247] [Quantum Approximate Optimization Algorithm for Spatiotemporal Forecasting of HIV Clusters](https://arxiv.org/abs/2507.00848)
*Don Roosan,Saif Nirzhor,Rubayat Khan,Fahmida Hai,Mohammad Rifat Haidar*

Main category: cs.LG

TL;DR: 量子加速的机器学习方法被用来提高HIV流行病学监测的精准性和效率，并揭示关键的因果关系。


<details>
  <summary>Details</summary>
Motivation: 因HIV数据复杂性增加，需要先进计算工具来精确进行群聚检测及预测。

Method: 结合经典聚类算法与量子优化算法，开发混合量子-经典神经网络进行预测，并使用量子贝叶斯网络分析因果关系。

Result: QAOA实现了92%的聚类检测准确度，仅用时1.6秒；混合神经网络的预测准确率达到94%，优于纯经典方法；量子贝叶斯分析指出房屋不稳定性为HIV群聚扩大的关键因素。

Conclusion: 量子增强方法有效提升HIV监测精度与效率，同时为靶向干预和资源优化提供依据。

Abstract: HIV epidemiological data is increasingly complex, requiring advanced
computation for accurate cluster detection and forecasting. We employed
quantum-accelerated machine learning to analyze HIV prevalence at the ZIP-code
level using AIDSVu and synthetic SDoH data for 2022. Our approach compared
classical clustering (DBSCAN, HDBSCAN) with a quantum approximate optimization
algorithm (QAOA), developed a hybrid quantum-classical neural network for HIV
prevalence forecasting, and used quantum Bayesian networks to explore causal
links between SDoH factors and HIV incidence. The QAOA-based method achieved
92% accuracy in cluster detection within 1.6 seconds, outperforming classical
algorithms. Meanwhile, the hybrid quantum-classical neural network predicted
HIV prevalence with 94% accuracy, surpassing a purely classical counterpart.
Quantum Bayesian analysis identified housing instability as a key driver of HIV
cluster emergence and expansion, with stigma exerting a geographically variable
influence. These quantum-enhanced methods deliver greater precision and
efficiency in HIV surveillance while illuminating critical causal pathways.
This work can guide targeted interventions, optimize resource allocation for
PrEP, and address structural inequities fueling HIV transmission.

</details>


### [248] [Aligning Learning and Endogenous Decision-Making](https://arxiv.org/abs/2507.00851)
*Rares Cristian,Pavithra Harsha,Georgia Perakis,Brian Quanz*

Main category: cs.LG

TL;DR: 本论文提出了一种端到端的方法，在存在内生不确定性的情况下训练机器学习模型，提高决策阶段的有效性。


<details>
  <summary>Details</summary>
Motivation: 决策往往受到自身行为的影响，无法获得反事实信息，因此需要通过机器学习来学习此类信息，从而更好地辅助决策。

Method: 引入一种内生不确定性下的端到端方法。此外，提出了一个新的稳健优化变体，通过构造不确定性集合优化决策，保护对最坏预测下的决策。另外，扩展了两阶段随机优化问题至此端到端学习框架中。

Result: 通过定价和库存推荐等多个计算实验，与现有的在线学习、Bandits和离线强化学习方法相比，表现出一致的改进性能。

Conclusion: 论文证明了其稳健优化方法在数据的函数下实现近最优决策的高概率保证，并展示了其在新框架下解决复杂优化问题的能力。

Abstract: Many of the observations we make are biased by our decisions. For instance,
the demand of items is impacted by the prices set, and online checkout choices
are influenced by the assortments presented. The challenge in decision-making
under this setting is the lack of counterfactual information, and the need to
learn it instead. We introduce an end-to-end method under endogenous
uncertainty to train ML models to be aware of their downstream, enabling their
effective use in the decision-making stage. We further introduce a robust
optimization variant that accounts for uncertainty in ML models -- specifically
by constructing uncertainty sets over the space of ML models and optimizing
actions to protect against worst-case predictions. We prove guarantees that
this robust approach can capture near-optimal decisions with high probability
as a function of data. Besides this, we also introduce a new class of two-stage
stochastic optimization problems to the end-to-end learning framework that can
now be addressed through our framework. Here, the first stage is an
information-gathering problem to decide which random variable to poll and gain
information about before making a second-stage decision based off of it. We
present several computational experiments for pricing and inventory
assortment/recommendation problems. We compare against existing methods in
online learning/bandits/offline reinforcement learning and show our approach
has consistent improved performance over these. Just as in the endogenous
setting, the model's prediction also depends on the first-stage decision made.
While this decision does not affect the random variable in this setting, it
does affect the correct point forecast that should be made.

</details>


### [249] [Machine Learning-based Early Detection of Potato Sprouting Using Electrophysiological Signals](https://arxiv.org/abs/2507.00862)
*Davide Andreoletti,Aris Marcolongo,Natasa Sarafijanovic Djukic,Julien Roulet,Stefano Billeter,Andrzej Kurenda,Margot Visse-Mansiaux,Brice Dupuis,Carrol Annette Plummer,Beatrice Paoli,Omran Ayoub*

Main category: cs.LG

TL;DR: 提出了一种基于机器学习的新方法，通过电生理信号预测马铃薯发芽时间，为储存管理提供早期干预依据。


<details>
  <summary>Details</summary>
Motivation: 现有的马铃薯发芽检测方法滞后于形态学变化，导致管理主动性不足。在禁用CIPC后，更昂贵的替代化学物质增加了成本，提出早期预测方法变得尤为重要，从而优化储存效率和资源利用。

Method: 设计了一种基于专有电生理传感器的机器学习方法，从信号的Wavelet域中提取特征，训练监督学习模型进行早期发芽检测，并加入了不确定性评估技术提升预测表现。

Result: 实验结果显示模型能够对部分样本准确预测发芽日期，且全体样本的平均预测误差处于可接受范围内，但最大偏差仍需改进。

Conclusion: 方法在早期发芽检测中显示出潜力，可用于提高储存管理效率，但需进一步改进以降低预测误差。

Abstract: Accurately predicting potato sprouting before the emergence of any visual
signs is critical for effective storage management, as sprouting degrades both
the commercial and nutritional value of tubers. Effective forecasting allows
for the precise application of anti-sprouting chemicals (ASCs), minimizing
waste and reducing costs. This need has become even more pressing following the
ban on Isopropyl N-(3-chlorophenyl) carbamate (CIPC) or Chlorpropham due to
health and environmental concerns, which has led to the adoption of
significantly more expensive alternative ASCs. Existing approaches primarily
rely on visual identification, which only detects sprouting after morphological
changes have occurred, limiting their effectiveness for proactive management. A
reliable early prediction method is therefore essential to enable timely
intervention and improve the efficiency of post-harvest storage strategies,
where early refers to detecting sprouting before any visible signs appear. In
this work, we address the problem of early prediction of potato sprouting. To
this end, we propose a novel machine learning (ML)-based approach that enables
early prediction of potato sprouting using electrophysiological signals
recorded from tubers using proprietary sensors. Our approach preprocesses the
recorded signals, extracts relevant features from the wavelet domain, and
trains supervised ML models for early sprouting detection. Additionally, we
incorporate uncertainty quantification techniques to enhance predictions.
Experimental results demonstrate promising performance in the early detection
of potato sprouting by accurately predicting the exact day of sprouting for a
subset of potatoes and while showing acceptable average error across all
potatoes. Despite promising results, further refinements are necessary to
minimize prediction errors, particularly in reducing the maximum observed
deviations.

</details>


### [250] [NN-Former: Rethinking Graph Structure in Neural Architecture Representation](https://arxiv.org/abs/2507.00880)
*Ruihan Xu,Haokui Zhang,Yaowei Wang,Wei Zeng,Shiliang Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种结合GNN和Transformer的新型预测器，解决了两者在神经网络架构表示中的缺陷，并引入了新的token mixer和channel mixer来改进性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN和Transformer在表示神经网络架构时各有缺陷，GNN无法处理复杂特征，而Transformer在深度网络中泛化能力差，因此需要改进方法更好地表示网络拓扑。

Method: 提出一种结合GNN和Transformer的新型方法，利用新型token mixer（考虑Sibling节点）和channel mixer（基于双向图同构前馈网络）。

Result: 实验结果表明，该方法在准确性和延迟预测方面均表现出色，为学习有向无环图（DAG）拓扑提供了有价值的见解。

Conclusion: 通过改进神经网络拓扑表示，本文所提出的方法优于现有方法，在神经架构搜索领域具有重要应用价值。

Abstract: The growing use of deep learning necessitates efficient network design and
deployment, making neural predictors vital for estimating attributes such as
accuracy and latency. Recently, Graph Neural Networks (GNNs) and transformers
have shown promising performance in representing neural architectures. However,
each of both methods has its disadvantages. GNNs lack the capabilities to
represent complicated features, while transformers face poor generalization
when the depth of architecture grows. To mitigate the above issues, we rethink
neural architecture topology and show that sibling nodes are pivotal while
overlooked in previous research. We thus propose a novel predictor leveraging
the strengths of GNNs and transformers to learn the enhanced topology. We
introduce a novel token mixer that considers siblings, and a new channel mixer
named bidirectional graph isomorphism feed-forward network. Our approach
consistently achieves promising performance in both accuracy and latency
prediction, providing valuable insights for learning Directed Acyclic Graph
(DAG) topology. The code is available at https://github.com/XuRuihan/NNFormer.

</details>


### [251] [TABASCO: A Fast, Simplified Model for Molecular Generation with Improved Physical Quality](https://arxiv.org/abs/2507.00899)
*Carlos Vonessen,Charles Harris,Miruna Cretu,Pietro Liò*

Main category: cs.LG

TL;DR: TABASCO通过标准Transformer架构生成3D分子，比主流模型表现更好且推理速度更快。


<details>
  <summary>Details</summary>
Motivation: 现有的3D分子生成模型有复杂的先验假设，但生成分子的物理可行性仍存在问题。

Method: 提出了TABASCO模型，无需引入等变性或信息传递，采用Transformer架构并将分子视为序列，生成后确定性地重建键。

Result: TABASCO在GEOM-Drugs基准上表现最好，PoseBusters有效性领先，推理速度比最强基线快约10倍，并出现旋转等变性特性。

Conclusion: TABASCO提供了一种简单化、高吞吐的生成模型，为特定任务如基于结构及药效团的药物设计提供了参考。

Abstract: State-of-the-art models for 3D molecular generation are based on significant
inductive biases, SE(3), permutation equivariance to respect symmetry and graph
message-passing networks to capture local chemistry, yet the generated
molecules still struggle with physical plausibility. We introduce TABASCO which
relaxes these assumptions: The model has a standard non-equivariant transformer
architecture, treats atoms in a molecule as sequences and reconstructs bonds
deterministically after generation. The absence of equivariant layers and
message passing allows us to significantly simplify the model architecture and
scale data throughput. On the GEOM-Drugs benchmark TABASCO achieves
state-of-the-art PoseBusters validity and delivers inference roughly 10x faster
than the strongest baseline, while exhibiting emergent rotational equivariance
despite symmetry not being hard-coded. Our work offers a blueprint for training
minimalist, high-throughput generative models suited to specialised tasks such
as structure- and pharmacophore-based drug design. We provide a link to our
implementation at github.com/carlosinator/tabasco.

</details>


### [252] [Privacy-Preserving Quantized Federated Learning with Diverse Precision](https://arxiv.org/abs/2507.00920)
*Dang Qua Nguyen,Morteza Hashemi,Erik Perrins,Sergiy A. Vorobyov,David J. Love,Taejoon Kim*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的随机量化器(SQ)，同时实现差分隐私(DP)和最小量化误差，并结合集群大小优化技术和线性融合方法提高联邦学习的模型聚合准确度。


<details>
  <summary>Details</summary>
Motivation: 改进联邦学习的学习效用，尤其是在隐私保护和设备间量化分辨率异质性的情况下，这两者的兼顾较为困难。

Method: 提出一种新的随机量化器(SQ)，通过设计同时实现差分隐私和量化误差的最小化，同时引入集群大小优化技术和线性融合方法处理量化异质性。

Result: 通过数值模拟验证了所提方法在隐私保护和学习效用方面相较传统LaplaceSQ-FL算法的优势。

Conclusion: 所提方法有效解决了隐私风险和量化异质性带来的挑战，同时改善了联邦学习的整体性能。

Abstract: Federated learning (FL) has emerged as a promising paradigm for distributed
machine learning, enabling collaborative training of a global model across
multiple local devices without requiring them to share raw data. Despite its
advancements, FL is limited by factors such as: (i) privacy risks arising from
the unprotected transmission of local model updates to the fusion center (FC)
and (ii) decreased learning utility caused by heterogeneity in model
quantization resolution across participating devices. Prior work typically
addresses only one of these challenges because maintaining learning utility
under both privacy risks and quantization heterogeneity is a non-trivial task.
In this paper, our aim is therefore to improve the learning utility of a
privacy-preserving FL that allows clusters of devices with different
quantization resolutions to participate in each FL round. Specifically, we
introduce a novel stochastic quantizer (SQ) that is designed to simultaneously
achieve differential privacy (DP) and minimum quantization error. Notably, the
proposed SQ guarantees bounded distortion, unlike other DP approaches. To
address quantization heterogeneity, we introduce a cluster size optimization
technique combined with a linear fusion approach to enhance model aggregation
accuracy. Numerical simulations validate the benefits of our approach in terms
of privacy protection and learning utility compared to the conventional
LaplaceSQ-FL algorithm.

</details>


### [253] [Understanding Generalization in Node and Link Prediction](https://arxiv.org/abs/2507.00927)
*Antonis Vasileiou,Timo Stoll,Christopher Morris*

Main category: cs.LG

TL;DR: 本文分析了消息传递图神经网络(MPNNs)在节点和链接预测任务中的推广能力，提出了一个统一框架，并通过实验支持理论发现。


<details>
  <summary>Details</summary>
Motivation: 研究MPNNs在节点和链接预测中的推广性能，目前对此的理解还较为不足。

Method: 提出了一种统一框架，分析MPNNs在归纳和直推场景下的推广性，采用不同架构参数和损失函数，并量化图结构的影响。

Result: 实验结果支持了理论分析，增强了对MPNNs在节点和链接预测任务中推广能力的理解。

Conclusion: 文章深化了对MPNNs推广性能的理解，并提供一个可应用于更广泛分类任务的推广框架。

Abstract: Using message-passing graph neural networks (MPNNs) for node and link
prediction is crucial in various scientific and industrial domains, which has
led to the development of diverse MPNN architectures. Besides working well in
practical settings, their ability to generalize beyond the training set remains
poorly understood. While some studies have explored MPNNs' generalization in
graph-level prediction tasks, much less attention has been given to node- and
link-level predictions. Existing works often rely on unrealistic i.i.d.\@
assumptions, overlooking possible correlations between nodes or links, and
assuming fixed aggregation and impractical loss functions while neglecting the
influence of graph structure. In this work, we introduce a unified framework to
analyze the generalization properties of MPNNs in inductive and transductive
node and link prediction settings, incorporating diverse architectural
parameters and loss functions and quantifying the influence of graph structure.
Additionally, our proposed generalization framework can be applied beyond
graphs to any classification task under the inductive or transductive setting.
Our empirical study supports our theoretical insights, deepening our
understanding of MPNNs' generalization capabilities in these tasks.

</details>


### [254] [Time Series Foundation Models are Flow Predictors](https://arxiv.org/abs/2507.00945)
*Massimiliano Luca,Ciro Beneduce,Bruno Lepri*

Main category: cs.LG

TL;DR: 研究时间序列基础模型(TSFMs)在群体流量预测中的有效性，发现Moirai和TimesFM在三种真实场景下表现优异，明显优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探讨时间序列基础模型在无需空间信息的情况下能否用于高效、准确的群体流量预测。

Method: 采用严格零样本环境，仅基于时间序列数据对Moirai和TimesFM进行评估，与统计及深度学习基线模型进行对比。

Result: Moirai和TimesFM显著超越现有方法，RMSE降低最多33%，MAE降低最多39%，CPC提高最多49%。

Conclusion: TSFMs在缺乏标注数据或空间背景信息下依旧能够提供准确、可扩展的流量预测，具有实用价值。

Abstract: We investigate the effectiveness of time series foundation models (TSFMs) for
crowd flow prediction, focusing on Moirai and TimesFM. Evaluated on three
real-world mobility datasets-Bike NYC, Taxi Beijing, and Spanish national OD
flows-these models are deployed in a strict zero-shot setting, using only the
temporal evolution of each OD flow and no explicit spatial information. Moirai
and TimesFM outperform both statistical and deep learning baselines, achieving
up to 33% lower RMSE, 39% lower MAE and up to 49% higher CPC compared to
state-of-the-art competitors. Our results highlight the practical value of
TSFMs for accurate, scalable flow prediction, even in scenarios with limited
annotated data or missing spatial context.

</details>


### [255] [Benchmarking the Discovery Engine](https://arxiv.org/abs/2507.00964)
*Jack Foxabbott,Arush Tagade,Andrew Cusick,Robbie McCorkell,Leo McKee-Reid,Jugal Patel,Jamie Rumbelow,Jessica Rumbelow,Zohreh Shams*

Main category: cs.LG

TL;DR: 本文介绍了一种科学发现自动化系统——Discovery Engine，它结合了先进的机器学习和解释技术，能高效地从多样数据集中获得科学见解。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过开发一个综合机器学习和机器学习可解释性的新系统，提升跨领域数据集上的科学洞察力。

Method: 通过比较Discovery Engine与五篇同行评审科学论文中使用的机器学习方法的性能，验证系统表现。这些论文涉及医学、材料科学、社会科学和环境科学等领域。

Result: Discovery Engine的预测性能匹配或优于先前研究，并通过丰富的解释工具提供了更深层次、操作性强的洞察。

Conclusion: 该系统证明了在数据中进行复杂知识发现的潜力，有望成为新的标准化自动化与可解释性科学建模工具。

Abstract: The Discovery Engine is a general purpose automated system for scientific
discovery, which combines machine learning with state-of-the-art ML
interpretability to enable rapid and robust scientific insight across diverse
datasets. In this paper, we benchmark the Discovery Engine against five recent
peer-reviewed scientific publications applying machine learning across
medicine, materials science, social science, and environmental science. In each
case, the Discovery Engine matches or exceeds prior predictive performance
while also generating deeper, more actionable insights through rich
interpretability artefacts. These results demonstrate its potential as a new
standard for automated, interpretable scientific modelling that enables complex
knowledge discovery from data.

</details>


### [256] [Scalable Feature Learning on Huge Knowledge Graphs for Downstream Machine Learning](https://arxiv.org/abs/2507.00965)
*Félix Lefebvre,Gaël Varoquaux*

Main category: cs.LG

TL;DR: SEPAL通过全球对齐和消息传递，在分布式硬件上高效生成知识图谱的嵌入向量，用于下游机器学习任务，结构简单，效果显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前知识图谱嵌入方法主要用于链接预测，且在大型图谱上受限于GPU内存。需要一种能有效扩展至大规模知识图谱，同时优化下游任务表现的方法。

Method: 提出SEPAL方法，通过在小核心实体上优化嵌入，并通过消息传递传播至整个图谱，从而实现全球对齐的高效嵌入生成。

Result: SEPAL在7个大规模知识图谱及46个下游任务中表现优越，显著超越现有方法，并适用于普通硬件处理海量图谱。

Conclusion: SEPAL为大规模知识图谱嵌入生成提供了强大且高可扩展性的方法，在下游任务中显现了巨大潜力，解决了当前方法的局限性。

Abstract: Many machine learning tasks can benefit from external knowledge. Large
knowledge graphs store such knowledge, and embedding methods can be used to
distill it into ready-to-use vector representations for downstream
applications. For this purpose, current models have however two limitations:
they are primarily optimized for link prediction, via local contrastive
learning, and they struggle to scale to the largest graphs due to GPU memory
limits. To address these, we introduce SEPAL: a Scalable Embedding Propagation
ALgorithm for large knowledge graphs designed to produce high-quality
embeddings for downstream tasks at scale. The key idea of SEPAL is to enforce
global embedding alignment by optimizing embeddings only on a small core of
entities, and then propagating them to the rest of the graph via message
passing. We evaluate SEPAL on 7 large-scale knowledge graphs and 46 downstream
machine learning tasks. Our results show that SEPAL significantly outperforms
previous methods on downstream tasks. In addition, SEPAL scales up its base
embedding model, enabling fitting huge knowledge graphs on commodity hardware.

</details>


### [257] [Reasoning as an Adaptive Defense for Safety](https://arxiv.org/abs/2507.00971)
*Taeyoun Kim,Fahim Tajwar,Aditi Raghunathan,Aviral Kumar*

Main category: cs.LG

TL;DR: 提出了一种名为TARS的训练方法，通过自适应推理提高大型语言模型在安全性方面的健壮性。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用自适应分配计算资源的方法训练模型，以在提高任务完成能力的同时增强对安全性漏洞的鲁棒性。

Method: 提出了TARS方法，结合链式推理的训练（SFT）和平衡安全与任务完成的奖励信号，其中包含三个重要设计：一个轻量的SFT阶段、融合不同类型提示的训练数据以及防止推理能力退化的奖励函数。

Result: TARS训练的模型在处理模糊查询时具备自适应计算能力，更好地平衡了安全性与任务完成。同时，这些模型在区分安全与不安全提示上表现更佳，在面对白盒和黑盒攻击时具备更高的鲁棒性。

Conclusion: TARS为对抗模型逃逸和有害请求提供了一种有效且公开的方法。

Abstract: Reasoning methods that adaptively allocate test-time compute have advanced
LLM performance on easy to verify domains such as math and code. In this work,
we study how to utilize this approach to train models that exhibit a degree of
robustness to safety vulnerabilities, and show that doing so can provide
benefits. We build a recipe called $\textit{TARS}$ (Training Adaptive Reasoners
for Safety), a reinforcement learning (RL) approach that trains models to
reason about safety using chain-of-thought traces and a reward signal that
balances safety with task completion. To build TARS, we identify three critical
design choices: (1) a "lightweight" warmstart SFT stage, (2) a mix of harmful,
harmless, and ambiguous prompts to prevent shortcut behaviors such as too many
refusals, and (3) a reward function to prevent degeneration of reasoning
capabilities during training. Models trained with TARS exhibit adaptive
behaviors by spending more compute on ambiguous queries, leading to better
safety-refusal trade-offs. They also internally learn to better distinguish
between safe and unsafe prompts and attain greater robustness to both white-box
(e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our work provides an
effective, open recipe for training LLMs against jailbreaks and harmful
requests by reasoning per prompt.

</details>


### [258] [Description of the Training Process of Neural Networks via Ergodic Theorem : Ghost nodes](https://arxiv.org/abs/2507.01003)
*Eun-Ji Park,Sangwon Yun*

Main category: cs.LG

TL;DR: 提出了一种通过引入幽灵类别扩展和最大Lyapunov指数的计算方法来加速深度学习模型训练的统一框架。


<details>
  <summary>Details</summary>
Motivation: 为了提高深度神经网络训练的效率，并准确区分稳定收敛和统计稳定这两种现象。

Method: 提出了一个运行中估计最大Lyapunov指数的诊断方法，以区分真实收敛与统计稳定；并引入幽灵类别扩展，通过添加额外的输出节点来增强模型的下降方向，帮助优化器绕过早期训练阶段的困难盆地。

Result: 实验表明，幽灵类别扩展减少了逼近误差，并在充足的收敛之后保留模型不变性，同时，扩展的模型路径可降低原始损失。

Conclusion: 该框架在早期训练阶段加速且保留了渐进期行为，为深度学习的训练优化提供了一种新的方法。

Abstract: Recent studies have proposed interpreting the training process from an
ergodic perspective. Building on this foundation we present a unified framework
for understanding and accelerating the training of deep neural networks via
stochastic gradient descent. By analyzing the geometric landscape of the
objective function we introduce a practical diagnostic, the running estimate of
the largest Lyapunov exponent, which provably distinguishes genuine convergence
toward stable minimizers from mere statistical stabilization near saddle
points. We then propose a ghost category extension for standard classifiers
that adds auxiliary ghost output nodes so the model gains extra descent
directions that open a lateral corridor around narrow loss barriers and enable
the optimizer to bypass poor basins during the early training phase. We show
that this extension strictly reduces approximation error and that after
sufficient convergence the ghost dimensions collapse and the extended model's
invariant law coincides with that of the original and there exists a path in
the enlarged parameter space along which the total loss does not increase while
the original loss decreases by an arbitrary margin. Taken together these
results provide a principled architecture level intervention that accelerates
early stage trainability while preserving asymptotic behavior.

</details>


### [259] [ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention](https://arxiv.org/abs/2507.01004)
*Yuhong Chou,Zehao Liu,Ruijie Zhu,Xinyi Wan,Tianjian Li,Congying Chu,Qian Liu,Jibin Wu,Zejun Ma*

Main category: cs.LG

TL;DR: 本文提出ZeCO（零通信开销）的序列并行方法，大幅优化了线性注意力模型在超长序列训练中的性能。


<details>
  <summary>Details</summary>
Motivation: 线性注意力机制适用于超长序列处理，但现有的序列并行方法因通信开销大而成为瓶颈。

Method: 提出了一种新的序列并行方法ZeCO，利用一种名为All-Scan的集体通信原语来减少通信开销。

Result: ZeCO在256个GPU上处理8M序列长度时，比当前最先进方法快60%，同时支持近乎线性的可扩展性。

Conclusion: ZeCO显著提高了长序列训练的通信效率，为下一代LLM处理超长序列奠定了基础。

Abstract: Linear attention mechanisms deliver significant advantages for Large Language
Models (LLMs) by providing linear computational complexity, enabling efficient
processing of ultra-long sequences (e.g., 1M context). However, existing
Sequence Parallelism (SP) methods, essential for distributing these workloads
across devices, become the primary bottleneck due to substantial communication
overhead. In this paper, we introduce ZeCO (Zero Communication Overhead)
sequence parallelism for linear attention models, a new SP method designed to
overcome these limitations and achieve end-to-end near-linear scalability for
long sequence training. For example, training a model with a 1M sequence length
across 64 devices using ZeCO takes roughly the same time as training with an
16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new
collective communication primitive. All-Scan provides each SP rank with
precisely the initial operator state it requires while maintaining a minimal
communication footprint, effectively eliminating communication overhead.
Theoretically, we prove the optimaity of ZeCO, showing that it introduces only
negligible time and space overhead. Empirically, we compare the communication
costs of different sequence parallelism strategies and demonstrate that
All-Scan achieves the fastest communication in SP scenarios. Specifically, on
256 GPUs with an 8M sequence length, ZeCO achieves a 60\% speedup compared to
the current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a
clear path toward efficiently training next-generation LLMs on previously
intractable sequence lengths.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [260] [SwarmFusion: Revolutionizing Disaster Response with Swarm Intelligence and Deep Learning](https://arxiv.org/abs/2507.00005)
*Vasavi Lankipalle*

Main category: cs.NE

TL;DR: SwarmFusion是一种将粒子群优化与卷积神经网络相结合的框架，用于优化实时资源分配和路径规划，在灾害管理中表现卓越。


<details>
  <summary>Details</summary>
Motivation: 应对灾害时需要快速适应混乱环境的决策，该研究旨在提升实时资源分配与路径规划的效率。

Method: 提出了SwarmFusion框架，整合粒子群优化和卷积神经网络，通过处理实时卫星、无人机和传感器数据，提高态势感知和行动效率。

Result: 在基于DisasterSim2025数据集的仿真实验中，灾害响应时间加快40%，幸存者覆盖率达到90%，显著优于基线方法。

Conclusion: 该框架以数据驱动方式为关键时限的灾害管理提供了变革性解决方案，具有广泛的危机场景应用潜力。

Abstract: Disaster response requires rapid, adaptive decision-making in chaotic
environments. SwarmFusion, a novel hybrid framework, integrates particle swarm
optimization with convolutional neural networks to optimize real-time resource
allocation and path planning. By processing live satellite, drone, and sensor
data, SwarmFusion enhances situational awareness and operational efficiency in
flood and wildfire scenarios. Simulations using the DisasterSim2025 dataset
demonstrate up to 40 percentage faster response times and 90 percentage
survivor coverage compared to baseline methods. This scalable, data-driven
approach offers a transformative solution for time-critical disaster
management, with potential applications across diverse crisis scenarios.

</details>


### [261] [A Review on Zeroing Neural Networks](https://arxiv.org/abs/2507.00387)
*Chengze Jiang,Jie Gui,Long Jin,Shuai Li*

Main category: cs.NE

TL;DR: 本文综述了零化神经网络（ZNN）的进展，包括其实现方法、分析理论和实际应用。


<details>
  <summary>Details</summary>
Motivation: 为系统理解零化神经网络领域，审视不同ZNN之间的关系和推导。

Method: 通过文献综述的方式，展示ZNN在实现方法、分析理论及实际应用方面的最新进展。

Result: 整理并归纳了ZNN在时间变化优化、控制问题中的性能及其领域的实际成就。

Conclusion: 对ZNN领域的现状提供了系统性的理解和启示，支持其进一步发展。

Abstract: Zeroing neural networks (ZNNs) have demonstrated outstanding performance on
time-varying optimization and control problems. Nonetheless, few studies are
committed to illustrating the relationship among different ZNNs and the
derivation of them. Therefore, reviewing the advances for a systematical
understanding of this field is desirable. This paper provides a survey of ZNNs'
progress regarding implementing methods, analysis theory, and practical
applications.

</details>


### [262] [Novel Complex-Valued Hopfield Neural Networks with Phase and Magnitude Quantization](https://arxiv.org/abs/2507.00461)
*Garimella Ramamurthy,Marcos Eduardo Valle,Tata Jagannadha Swamy*

Main category: cs.NE

TL;DR: 本文提出了两种新型的复数值Hopfield神经网络，结合了相位和幅度量化，显著增加了状态数量。


<details>
  <summary>Details</summary>
Motivation: 通过引入新的相位和幅度量化模型，扩展复数值Hopfield神经网络的应用领域。

Method: 提出两种复数值HNN：一种基于直角坐标系的天花板激活函数，另一种基于极坐标系的天花板激活函数，二者均包括相位和幅度量化。

Result: 新型复数值HNN通过相位和幅度量化显著增加了状态数量。

Conclusion: 所提出的复数值HNN扩展了状态空间，增强了其应用潜力。

Abstract: This research paper introduces two novel complex-valued Hopfield neural
networks (CvHNNs) that incorporate phase and magnitude quantization. The first
CvHNN employs a ceiling-type activation function that operates on the
rectangular coordinate representation of the complex net contribution. The
second CvHNN similarly incorporates phase and magnitude quantization but
utilizes a ceiling-type activation function based on the polar coordinate
representation of the complex net contribution. The proposed CvHNNs, with their
phase and magnitude quantization, significantly increase the number of states
compared to existing models in the literature, thereby expanding the range of
potential applications for CvHNNs.

</details>


### [263] [High-resolution spatial memory requires grid-cell-like neural codes](https://arxiv.org/abs/2507.00598)
*Madison Cotteret,Christopher J. Kymn,Hugh Greatorex,Martin Ziegler,Elisabetta Chicca,Friedrich T. Sommer*

Main category: cs.NE

TL;DR: 本文探讨了连续吸引子网络（CANs）如何在大脑中通过稀疏分布编码，同时实现稳定性和高分辨率的连续变量记忆。


<details>
  <summary>Details</summary>
Motivation: 传统的连续吸引子网络面临稳定性和分辨率之间的权衡困境，因为其对噪声或异质性等生物系统中常见问题非常敏感。

Method: 作者提出使用基于稀疏二进制分布编码的模型，同时利用具有空间周期性感受野的神经元（类似网格细胞），并结合随机特征嵌入的方法。

Result: 理论分析和模拟结果表明，采用这种编码方式的CANs既能实现高稳定性，又能保持高分辨率，还能支持灵活嵌入任意非线性流形并实现复杂的任务计算。

Conclusion: 该研究为大脑如何以高分辨率和高鲁棒性表示连续变量并灵活进行相关计算提供了新理论。

Abstract: Continuous attractor networks (CANs) are widely used to model how the brain
temporarily retains continuous behavioural variables via persistent recurrent
activity, such as an animal's position in an environment. However, this memory
mechanism is very sensitive to even small imperfections, such as noise or
heterogeneity, which are both common in biological systems. Previous work has
shown that discretising the continuum into a finite set of discrete attractor
states provides robustness to these imperfections, but necessarily reduces the
resolution of the represented variable, creating a dilemma between stability
and resolution. We show that this stability-resolution dilemma is most severe
for CANs using unimodal bump-like codes, as in traditional models. To overcome
this, we investigate sparse binary distributed codes based on random feature
embeddings, in which neurons have spatially-periodic receptive fields. We
demonstrate theoretically and with simulations that such grid-cell-like codes
enable CANs to achieve both high stability and high resolution simultaneously.
The model extends to embedding arbitrary nonlinear manifolds into a CAN, such
as spheres or tori, and generalises linear path integration to integration
along freely-programmable on-manifold vector fields. Together, this work
provides a theory of how the brain could robustly represent continuous
variables with high resolution and perform flexible computations over
task-relevant manifolds.

</details>
